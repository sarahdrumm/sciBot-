A Distributed Learning Framework for
Heterogeneous Data Sources
Srujana Merugu
Joydeep Ghosh
Department of Electrical and Computer Engg .
Department of Electrical and Computer Engg .
The University of Texas at Austin merugu@eceutexasedu
The University of Texas at Austin ghosh@eceutexasedu
ABSTRACT We present a probabilistic model based framework for distributed learning that takes into account privacy restrictions and is applicable to scenarios where the different sites have diverse , possibly overlapping subsets of features . Our framework decouples data privacy issues from knowledge integration issues by requiring the individual sites to share only privacy safe probabilistic models of the local data , which are then integrated to obtain a global probabilistic model based on the union of the features available at all the sites . We provide a mathematical formulation of the model integration problem using the maximum likelihood and maximum entropy principles and describe iterative algorithms that are guaranteed to converge to the optimal solution . For certain commonly occurring special cases involving hierarchically ordered feature sets or conditional independence , we obtain closed form solutions and use these to propose an efficient alternative scheme by recursive decomposition of the model integration problem . To address interpretability concerns , we also present a modified formulation where the global model is assumed to belong to a specified parametric family . Finally , to highlight the generality of our framework , we provide empirical results for various learning tasks such as clustering and classification on different kinds of datasets consisting of continuous vector , categorical and directional attributes . The results show that high quality global models can be obtained without much loss of privacy .
Categories and Subject Descriptors H48 [ Database Management ] : Database Applications | Data Mining ; I26 [ Artificial Intelligence ] : Machine Learning
General Terms Algorithms
Keywords Distributed learning , privacy , heterogeneous data sources , probabilistic models
1 .
INTRODUCTION
Recent advances in data acquisition technology have resulted in the creation of large distributed repositories . However , extracting useful knowledge from such repositories is often challenging due to real world constraints stemming from privacy , proprietary , computational or communication issues . Such restrictions may prevent one from directly integrating the distributed data into a single dataset at a central site . This has led to the emergence of distributed data mining techniques [ 10 , 19 ] that extract high quality information from distributed sources with limited interactions among the data sites . In particular , rising concerns on informational privacy have resulted in an increased focus on privacy preserving distributed data mining techniques [ 14 , 6 , 20 ] . Most of these techniques are applicable only to scenarios where the data is either vertically partitioned ( different sites contain different attributes/ features of a common set of records/objects ) or horizontally partitioned ( objects are distributed amongst different sites , but have the same set of features ) . In real life , however , there are a number of more complex situations where the different sites contain overlapping sets of objects and features , ie , the data is neither vertically nor horizontally partitioned .
In this paper , we focus on privacy preserving learning in a distributed setting1 where the different sites have diverse , possibly overlapping sets of features , and also need not share objects2 . The prototypical application scenario is one in which there are multiple collaborating parties with confidential databases of different , possibly overlapping schemas . The objective is to characterize the entire data using a suitable representation so that learning tasks such as clustering and classification can be readily performed . For example , the collaborating parties could be a group of medical researchers , each owning a database of clinical results . The collective goal in this case is to find patterns and correlations among the various clinical conditions , without compromising the privacy of the patients . The symptoms ( features ) of the patients ( objects ) need not be the same for all the researchers . Figure 1 depicts a possible scenario with three collaborating parties , each with data on a different pair of symptoms .
Our formulation of the distributed learning problem assumes that there exists a ( unknown ) meaningful , underlying distribution that captures the information content in the
1Note that horizontal partitioning is a special case of this setting . 2Objects owned by different sites are treated as different objects , which could result in loss of information in case of overlapping objects . different data sources . The individual data sources provide only partial views that need to be effectively integrated in order to reconstruct the original underlying distribution . Our first attempt [ 16 ] to address this problem involved dividing the learning process into two sub tasks:(a ) learning privacysafe models from the local data , and ( b ) combining the local models effectively to obtain an appropriate \global" model . We also proposed a definition of privacy based on informa
SOURCE X1
Alcoholic
( A )
Cholestrol Level ( B )
0 1 0 . . . 1
160 200 190 .
. . 195
S.No x11 x12 x13
. . . . . x1m 1
SOURCE X2
SOURCE X3
S.No x21 x22 x23
. . . . x2m 2
Cholestrol Level ( B )
Blood Pressure
( C )
195 300 156 .
. . 290
110 140 100 .
. . 150
S.No x31 x32 x33 . .
. . . . x3m 3
Alcoholic
( A )
Blood Pressure
( C )
0 1 0 . . . 1
130 85 136 .
. . 129 l1P ( A,B ) l2P ( B,C )
Pl3 ( A,C )
Pl *(A,B,C )
Figure 1 : Distributed learning scenario with overlapping sets of features . tion theoretic ideas in order to formalize the first sub task . This definition effectively captures the non uniform privacy requirements in most real scenarios and is also applicable to the general setting where the distributed sites have varying schemas . However , the model integration approach in [ 16 ] is applicable only for horizontally partitioned data , wherein all the local and global models correspond to probability distributions on the same domain .
In this paper , we substantially extend our previous work and make three new contributions .
( i ) Using the maximum likelihood principle and the relation between data log likelihood and cross entropy , we formulate the problem of combining probabilistic models based on different sets of features in order to obtain an appropriate joint model . We consider two cases | one in which there are no constraints on the joint model , which is more relevant for discrete domains , and the other in which the joint model is assumed to belong to a particular parametric family .
( ii ) We present algorithms to solve the model integration problem for both the unconstrained and the constrained cases . For discrete domains , we transform the unconstrained model integration problem into a KL divergence projection problem and show that it can be efficiently solved using iterative algorithms . We also demonstrate that the constrained model integration problem is equivalent to optimizing the log likelihood of data generated from the available models and address this problem by generating artificial samples using Monte Carlo Markov Chain ( MCMC ) techniques and then fitting a joint model of the desired parametric form using an EM based algorithm .
( iii ) For certain commonly occurring scenarios involving conditional independence or hierarchically ordered feature sets , we provide closed form solutions to the model integration problem and use these to propose efficient alternatives using recursive decomposition .
Our model integration approach extends the privacy preserving framework proposed in [ 16 ] to a large class of learning tasks that include classification , clustering and semi supervised learning , and to more general scenarios involving distributed sites with diverse schemas . The framework is applicable to a wide range of data types ( both discrete and continuous ) and only requires the individual sites to provide probabilistic models of the local data .
The rest of the paper is organized as follows . Section 2 contains a formal definition of the model integration problem . Sections 3 and 4 describe solutions to the unconstrained and the constrained model integration problems respectively . Section 5 describes a recursive scheme for model integration and presents closed form solutions for certain special scenarios . Section 6 contains experimental results . We present related work in Section 7 and conclude in Section 8 .
Notation : Sets such as fx1 ; ; xng are enumerated as fxign i=1 and an index i running over the set f1 ; ; ng is denoted by [ i]n 1 . Matrices are denoted using upper case letters eg , A , whereas the lower case letters auv denote the matrix elements , and the bold lower case letters , av ; aT u denote the column and row vectors . Transpose of a matrix A is denoted by AT . Feature sets are denoted by F with appropriate subscripts and feature vectors are denoted by the corresponding bold lower case letters . Probability distributions of models are denoted by p with the model name as the subscript , eg , p( ) and the marginal densities corresponding to the feature set F are identified with a superscript , eg , pF .
2 . PROBLEM FORMULATION
Our main objective is to address the distributed learning problem for scenarios where there are no restrictions on the features available at the various sites and the different sites need not share objects . We first consider privacy issues .
2.1 Privacy Issues
There are two main notions of privacy in the existing literature that are applicable to distributed learning scenarios . The first is based on a secure multi party computation [ 21 ] point of view where a computation is considered secure if each collaborating party learns nothing beyond its own input and the final result of the computation . The second notion characterizes the privacy of a computation in terms of the uncertainty [ 1 , 16 ] in recovering the input from the result .
In this paper , we adopt an uncertainty based definition of privacy since it takes into account the information disclosed by the final result . In particular , our framework is based on an information theoretic definition of privacy [ 16 ] where the privacy ( X ; ) of a dataset X with respect to a probabilistic model is quantified in terms of the likelihood of predicting X from the model , ie ,
( X ; ) = ( p(X )),1 :
( 1 )
Using this notion of privacy , it is possible to develop distributed learning techniques based on combining probabilistic models that satisfy the desired privacy constraints . The chief benefits of this approach are that it is scalable for large datasets , does not require the assumption of non colluding parties , and permits the individual parties to have varying privacy requirements . Moreover , it allows the collaborating parties to use proprietary domain knowledge and algorithms in the local learning process .
2.2 Model Integration Problem
We divide the distributed learning problem into two subproblems (i ) learning probabilistic models from the local data while adhering to information theoretic privacy constraints , and ( ii ) integrating the local models effectively to obtain an appropriate \global" model . This separation of privacy and integration issues also allows the individual parties to use their own means of sanitizing the local models during the local learning process , eg , stripping of unique ids . In the current work , we mainly focus on the model integration problem assuming the availability of privacy safe models .
[ i]n
Let fXign
1 where fflign i=1 be n datasets with feature sets fFign bution on the union of all the features , ie , Fc =  n i=1 and i=1 such that jXij = mi ; [ i]n corresponding feature vectors ffign 1 . Let fign i=1 be the local models obtained from these datasets such that the probability distributions fpi gn i=1 closely approximate the true distributions on the corresponding datasets as well as satisfy the local privacy constraints , ie , ( Xi ; i ) i=1 are the desired privacy levels for the fli ; local models3 . The model integration problem involves combining the local models fign i=1 to obtain a \good" global complete model corresponding to a joint probability distrii=1 Fi . For example , in Figure 1 , there are three collaborating parties with feature sets F1 = fA ; Bg , F2 = fB ; Cg , and F3 = fA ; Cg respectively and the goal is to obtain a joint model on the feature set Fc = fA ; B ; Cg . The above formulation encompasses a number of common distributed learning tasks . For example , when all the sites share a class attribute , the distributed classification problem can be posed in terms of learning a joint density on the class labels and all the available features . Similarly , the distributed clustering task can be formulated in terms of learning a joint mixture density on all the features . Semi supervised classification and Bayesian network learning are other examples of learning problems that can be posed in terms of the above formulation .
To concretely formulate the model integration problem , we need to first quantify the quality of the global model . In the absence of any privacy constraints , one could possibly pool all the distributed data and obtain a complete model using the maximum likelihood principle . The quality of any given complete model c can then be measured using the data likelihood or log likelihood , ie ,
QDL(c ) = log pc(Xi )
( 2 ) n i=1
[ i]n
Since a complete model is defined on the feature set Fc , which is the union of the local feature sets fFign i=1 , the data likelihoods pc(Xi ) ; 1 are in fact the incomplete likelihoods obtained by assuming the unavailable feature values as missing data . Now , using the well known relation between loglikelihood and cross entropy [ 4 ] , it can be shown that the incomplete data log likelihood with respect to a complete model is linearly related to the KL divergence or the relative entropy of true distribution on X with respect to the corresponding marginal distribution of the complete model .
Lemma 1 Let X = fxjgm j=1 be a dataset with feature set F such that pX corresponds to the distribution on X and let 3The phrases ’probability density’ and ’probability distributions’ are used interchangeably to mean either the probability density function ( in case of continuous distributions ) or the probability mass function ( in case of discrete distributions ) . pc be any probability distribution defined on a feature set Fc such that F Fc . Then ,
1 m log pc(X ) = ,KL(pX jjpF c ) + H(pX ) where pF c is the marginal distribution of pc on F .
In the above relation , the entropy term is independent of the probability density pc . Therefore , maximizing the average data likelihood is equivalent to minimizing the KLdivergence between the data distribution pX and the appropriate marginal density , ie , the maximum likelihood principle corresponds to a minimum KL divergence principle . Since the local models fign i=1 are obtained from the datasets fXign i=1 , the corresponding probability distributions can be assumed to be reasonable approximations of the true distributions on the datasets . Therefore , using Lemma 1 , we define the quality cost of the global model to be
CKL(c ) = iKL(pi jjpFi c ) ;
( 3 ) n i=1 i=1 mi
; [ i]n n where a lower cost indicates a better model and the weights i = mi 1 are normalized to sum to 1 . Note that due to the privacy constraints , the local models will not exactly correspond to the true distribution on the datasets fXign i , but CKL is a good choice for the cost function since we only have access to the local models and not the original data .
With CKL as the quality measure , the problem of finding the optimal global model essentially involves minimizing the KL divergence between the local models and the appropriate marginal density induced by the global model . Since KLdivergence is strictly convex in both its arguments [ 4 ] , and the marginal densities are linear functions of the complete density function , the overall quality cost function , which is a composition of the two functions , is convex in the complete density pc , but not strictly so . Therefore , the quality cost has a unique minimum value , but there could be multiple models corresponding to the same minimal cost . In order to choose one of these models , we invoke the maximum entropy principle [ 5 ] , which is equivalent to making \no extra assumptions" about the global model . Putting together both the maximum entropy and the minimum KL divergence principles , we can formally state the Model Integration problem as where max pm 2M
H(pm )
M = argmin
CKL(pc ) ; pc 2P(Fc )
( 4 )
( 5 ) and P(Fc ) is the set of all probability distributions over the feature set Fc . Figure 2 shows a pictorial representation of the problem formulation for the distributed scenario of Figure 1 .
Since the KL divergence cost minimization ( 5 ) is a convex optimization problem , the set of minimizers M is also a convex set . Further , since entropy is strictly convex , the overall model integration problem has a unique minimizer .
We consider two model integration scenarios | the first where we require the optimal estimates of the joint distribution for each element in the domain and the second where it is desirable to have an interpretable global model even if it is
LOCAL MODEL 1 P ( A,B ) , l1
1
LOCAL MODEL 2 l2P
( B,C ) , n 2
LOCAL MODEL 3 l3P
( A,C ) , n 3 argmin { KL( ( A,B ) || ( A,B ) ) + KL( ( B,C ) || ( B,C ) )
P 2 cP n 1
Pl 1
Pl c n 3 + KL( ( A,C ) || ( A,C ) ) } l cP Pl 3 n 2 P c
KL−DIVERGENCE MINIMIZATION
M argmax { H( P ( A,B,C ) )} l mP
Me m
ENTROPY
MAXIMIZATION
*P l ( A,B,C )
GLOBAL MODEL
Figure 2 : Model integration in a distributed learning scenario involving different feature sets . less accurate than the optimal one . Unless the optimal solution has a closed form , numerical solution to the exact model integration problem is feasible only for situations where the complete feature vector fc takes a finite number of distinct values . Section 4 addresses the exact model integration problem for discrete domains . To address the second scenario , in Section 5 , we formulate and solve a modified version of the original model integration problem ( Parametric Model Integration ) where the global model is sought from a specified parametric family , eg , mixture of 10 Gaussians .
3 . DISCRETE MODEL INTEGRATION
In this section , we transform the model integration problem for a discrete domain into a projection problem based on KL divergence . Then , we derive properties of this formulation and describe efficient iterative algorithms that are guaranteed to converge to the globally optimal solution . 3.1 KL divergence Projection
Let F be any feature set with corresponding feature vector f . Let ( F ) = ( f ) denote the set of all distinct values taken by the feature vector f . To solve the model integration problem ( 4 ) for a finite discrete domain , we map each discrete distribution p 2 P(F ) , the set of all distributions over F , to a unique point in the probability simplex of dimension d = j(F)j . The basic idea is to enumerate all the elements in ( F ) and represent each model with a non negative + consisting of the probabilities associated with these elements . More precisely , if the elements in ( F ) are ordered such that ( F ) = fwkgd k=1 , then the mapping from the probabilistic model to p is given by vector p 2 d pk = p(f = wk ) ; [ k]d 1 :
( 6 )
Hence , kpk1 = d k=1 p(f = wk ) = 1 .
Using the above representation , every complete distribution pc 2 P(Fc ) maps to a unique vector p in the dc simplex where dc = j(Fc)j . Further , the marginal densities induced on any subset of features F can be shown to be linear projections of the original complete density on appropriate subspaces . Let pF c be the marginal density induced by the complete density pc on the feature set F fl Fc . Let f be the feature vector corresponding to the complement feature set w2( F ) pc(f = w ; f = F = Fc n F . Then , pF c(f = w ) = w ) , or in other words , pF c is obtained by partitioning the set ( Fc ) into j(F)j groups , one for each value of f , and summing up the probabilities associated with each group . The marginal density pF c , therefore , corresponds to a d dimensional vector AF p where AF is the d . dc membership matrix consisting of entries in f0 ; 1g with d = j(F)j and dc = j(Fc)j respectively . Since each element in the original domain ( Fc ) ( represented by columns ) is included within a single element of ( F ) ( represented by the rows ) , each column in AF will have exactly one row entry equal to 1 and the rest equal to 0 .
+ ; di = j(Fi)j ; 1 correspond to the local models pi in the new representation . Then , the quality cost in the KL divergence minimization problem ( 5 ) can be rewritten as
[ i]n iKL(pi jjpFi c ) = iKL(qijjAFi p ) ; ( 7 )
Let qi 2 di n i=1
CKL(c ) = n i=1
Example . Consider the model integration example shown in Figure 1 where F1 = fA ; Bg ; F2 = fA ; Bg , F3 = fA ; Cg and 3 i=1 Fi = fA ; B ; Cg . Let us assume that the features A ; B ; C take two values each so that j(Fc)j = 8 . Then , any complete model c over the features ( A ; B ; C ) can be
Fc =  mapped to a unique p 2 8 p = ( pc(a1 ; b1 ; c1 ) ; pc ( a2 ; b1 ; c1 ) ; pc ( a1 ; b2 ; c1 ) ; pc(a2 ; b2 ; c1 ) ; pc(a1 ; b1 ; c2 ) ; pc ( a2 ; b1 ; c2 ) ; pc(a1 ; b2 ; c2 ) ; pc(a2 ; b2 ; c2 ) )T
+ , where
Similarly , the local models 1 ; 2 ; 3 can be mapped to vectors q1 ; q2 ; and q3 respectively where
1
2
3
7! q1 = ( p1 ( a1 ; b1 ) ; p1 ( a2 ; b1 ) ; ; p1 ( a2 ; b2))T 7! q2 = ( p2 ( b1 ; c1 ) ; p2 ( b2 ; c1 ) ; ; p2 ( b2 ; c2))T 7! q3 = ( p3 ( a1 ; c1 ) ; p3(a2 ; c1 ) ; p3 ( a2 ; c2))T
The projection matrices for obtaining the marginal density over F1 in this case is given by
AF1 =
1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0
0 0 0 1 0 0 0 1 0 0 0 1
Similarly , we can obtain the matrices AF2 and AF3 . Using the linearity property of KL divergence , the cost function in ( 7 ) can be simplified and written as the KL divergence between a single discrete distribution q and Ap where q and A are uniquely determined by the following result .
+ such that kpk1 = 1 and let + and jjqijj1 = 1 . Let AF1 ; ; AFn be non negative matrices such that 1 ; [ i]n AFi is a di . d matrix where di = j(Fi)j ; dc = j(Fc)j and the sum of each column in each matrix AFi equals 1 , ie , kaFi i=1 be non negative
Proposition 1 Let p 2 dc q1 ; ; qn be n vectors such that qi 2 di weights such that n i=1
1 . Further , let fign n i=1 i = 1 . Then , iKL(qijjAFi p ) = KL(qjjAp ) v k1 = 1 ; [ v]dc
1 ; [ i]n where q = [ 1q1
T ; nqn
T ]T , A = [ 1AF1
T
; nAFn T
]T . l l n l l
Using the above result , the KL divergence minimization in the model integration problem can be restated as
( 8 )
KL(qjjAp ) min + ; kpk1 =1 p2 dc where q 2 d0 + and A 2 d0 .dc under the linear constraints p 2 dc are determined by Proposition 1 . Since ( 8 ) involves minimizing a convex function + and kpk1 = 1 , there is a unique minimum value and the set of minimizers is convex . The following result shows that the solution set is in fact the intersection of an affine space with the d simplex .
+ lem ( 8 ) is given by fp 2 dc
Proposition 2 The solution set for the KL projection prob+ jAp = qfig where qfi is a uniquely determined non negative vector such that kqfik1 = 1 ; qfi 1 and AT = 0 . Further , when rank(A ) = rank([A ; q] ) , then qfi = q itself . u+1 ; [ u]d0 u = qu
Proposition 2 provides a closed form for the solution of ( 8 ) for the special cases where rank(A ) = rank([A ; q ] ) or when the null space of A can be easily characterized . It also simplifies the entropy maximization problem , since the set of minimizers M in ( 5 ) is now characterized by linear constraints + , Ap = qfi and kpk1 = 1 . When A is a full column rank matrix ( ie , rank(A ) = dc ) , then there is a unique minimizer , ie , jMj = 1 . In general , the entropy maximization problem is p 2 dc p2 dc max
H(p ) :
( 9 )
+ ; Ap=qfi ; kpk1=1
3.2 Iterative Algorithms
The KL divergence minimization and the entropy maximization problems discussed above are both convex optimization problems with linear constraints and can be solved efficiently using iterative scaling algorithms .
KL Divergence minimization The KL divergence projection problem ( 8 ) has earlier been studied in the context of positron emission tomography [ 3 , 18 ] . It is usually solved using the Expectation Maximization Maximum Likelihood ( EMML ) algorithm ( see Algorithm 1 ) , which is guaranteed to converge to a global minimizer pfi KL , which can then be used to identify the entire solution set KLg . Each iteration of the EMML algorithm requires a computation time that is linear in the size of p and A ( ie , number of non zeros ) .
+ jAp = qfi = Apfi fp 2 dc
A more efficient approach involves reducing the KL divergence projection problem to a form solvable by the Iterative Re weighted Least Squares ( IRLS ) algorithm [ 9 ] . The basic idea is to refine the solution using Newton Raphson like additive updates , which for the KL divergence cost function is given by p(t+1 ) = p(t ) + p(t ) where
AT BA p(t ) = AT h
( 10 ) where B = [ buu0 ] 2 d0 .d0 buu0 = 0 when u 6= u0 ; [ u]d0 ,qu u ;pi u ;pi2 and such that hu = 1 . Since ( 10 ) corresponds to the least squares such that buu =
1 ; [ u0]d0
[ u]d0
1 and h 2 d0 haT minimization problem haT qu
;
( B,1h , A p(t))T B(B,1h , A p(t) ) ; min p(t ) where only B and h change in each iteration , the problem can be solved efficiently using the IRLS algorithm . The computational time of the algorithm is still linear in the size of p and A ( ie , number of non zeros ) , but each iteration only requires computational time that is linear in p .
Entropy maximization The linear constrained entropy maximization problem ( 9 ) has been studied in a number of contexts [ 3 , 5 ] . It is often solved using the Multiplicative Algebraic Reconstruction technique ( MART ) , which is a cyclic minimization process ( see Algorithm 1 ) guaranteed to converge to the optimal + such that + jAp = qfig 6= ; , which is
1 ; [ v]dc
+ and qfi 2 d0 1 ; q 2 d0
+ ; kqk1 = 1 .
Algorithm 1 Discrete Model Integration
+ ; kavk1 = 1 ; [ v]dc
[ u]d0 auv 1 ; the case in ( 9 ) .
Output : pfi = argmax pm 2M distribution for any A 2 d0 .dc 1 and fp 2 dc Input : A 2 d0 .d p2 dc a1 . Initialize p 2 dc pv pv . argmin + ;kpk1=1 until convergence c1 . qfi Ap where M = b1 . repeat auv qu haT
Method : u ;pi
H(pm ) d0 u=1
KL(qjjAp ) .
; [ v]dc 1 fKL divergence Minimization EMMLg
++ randomly such that kpk1 = 1 . fEntropy Maximization MARTg a2 . pv exp(,(AT z0)v , 1 ) ; [ v]dc 1 b2 . repeat qfi u haT until convergence pv pv . u ;pi auv c2 . pfi p for some z0 2 d0
1 ; [ u]d0
+
1
; [ v]dc
4 . PARAMETRIC MODEL INTEGRATION In this section , we formulate a constrained version of the model integration problem , which we call the Parametric Model Integration problem . Then , we show how this problem can be efficiently addressed using sampling techniques and the Expectation Maximization ( EM ) algorithm .
Our formulation of the parametric model integration problem is motivated by the fact that in real scenarios , it is often preferable to obtain a solution that is easy to understand and describe even at the cost of optimality . To incorporate this requirement , we constrain the global model to belong to a specified parametric family G P(Fc ) , eg , mixture of k Gaussians ( k < 10 ) . The KL divergence cost minimization problem for this case can be stated as min c2G
CKL(pc ) :
( 11 ) where G fl P(Fc ) is the specified family of parametric distributions . However , unlike the unconstrained case , ( 11 ) is not necessarily a convex problem since the optimization is over the model parameters and the KL divergence cost is not always convex in these parameters . As a result , there are usually multiple local minima for the KL divergence minimization . Moreover , the set of minimizers may not be a convex set , which makes it more difficult to solve the entropy maximization problem . To make the constrained model integra tion problem tractable , we assume that the parametric family G is chosen so that there is a unique minimizer for the KLdivergence optimization problem , which eliminates the need for solving the maximum entropy problem . However , in general , ( 11 ) is itself a difficult problem to solve and it is only possible to obtain the local minimizers since CKL is not always a convex function of the parameters and G need not be a convex set . A direct solution of ( 11 ) using regular optimization techniques such as gradient descent , NewtonRaphson ’s method , etc . , is computationally infeasible when the local models correspond to continuous probability distributions since only the parameters of the local models are available at the integration . Therefore , we pose an approximate version of the model integration problem by generating artificial samples from the local models . The main idea is to approximate the KL divergence with respect to the local models in terms of the likelihood of the generated data .
Let the datasets f ~Xign i=1 be obtained by sampling from the i=1 . When the dataset sizes ~mi = j ~Xij ; [ i]n local models fign 1 are large ( tend to 1 ) , then the distribution on the datasets f ~X gn i=1 is identical to the corresponding local model distributions . Therefore , from Lemma 1 , it follows that for large sample sizes , the average log likelihood of each dataset ~Xi with respect to any probabilistic model c 2 G is linearly related to the KL divergence between the corresponding i and the appropriate marginal density of c . More specifically , lim
~mi!1
1 ~mi log(pc ( ~Xi ) ) = H(pi ) , KL(pi jjpFi c ) ; [ i]n
1 ; ( 12 ) where H(pi ) is the entropy of the local model i , which is independent of c . Hence , maximizing the log likelihoods of the datasets f ~Xign i=1 is equivalent to minimizing the KLdivergence with respect to the corresponding local models . This observation enables us to pose the constrained model integration problem ( 11 ) as a maximum likelihood parameter estimation problem , which can then be conveniently solved using an Expectation Maximization ( EM ) algorithm .
Algorithm 2 shows the main steps in the process , ie , generating artificial datasets from the local models and learning the maximum likelihood parametric model based on the combination of these datasets . Since the local models are based on a smaller set of features than the global model , the unavailable feature values are modeled as missing and are re estimated during the E step of the algorithm .
The global model a c resulting from the EM algorithm is a local minimizer of the approximate problem and not necessarily the same as the optimal solution of ( 11 ) . However , it is guaranteed to asymptotically converge to a locally optimal solution as the size of ~Xc goes to 1 . In practice , one can use multiple runs of the EM algorithm and pick the best solution among these so that the obtained model is reasonably close to the globally optimal model .
5 . RECURSIVE DECOMPOSITION
In this section , we first describe an alternate scheme for solving the model integration problem by recursively decomposing it into smaller sub problems . Then , we present closed form solutions for certain special distributed learning scenarios , which can be incorporated into the recursive scheme to efficiently solve the model integration problem for more general scenarios .
Algorithm 2 Parametric Model Integration Input : Set of models fign i=1 with weights fign Parametric family G , Global sample size ~mc . i=1 summing to 1 ,
Output : a c ’ argmin c2G
Method : i=1 iKL(pi jjpFi c
) n
1 . Generate ~Xi ; [ i]n sampling such that j ~Xij = i ~mc ; [ i]n 1 .
1 from the local models i ; [ i]n
1 using MCMC
~Xi . Apply EM algorithm to obtain the opti
2 . Let ~Xc = n mal model a i=1 c such that a c = argmax c2G log(pc ( Xc) ) :
5.1 Decomposition Strategy
Solving the model integration problem ( 8 ) can be computationally expensive when j(Fc)j is large since the size of A and p depends on j(Fc)j . However , it is possible to reduce these computational costs when some of the features can be assumed to be independent or conditionally independent given some other features based on domain knowledge or the available local models . The key idea is that the original model integration problem ( 8 ) can be recursively split into smaller model integration problems with closed form solutions . This recursive decomposition process is more efficient than a direct solution since it only estimates the independent parameters required to determine the complete model , which is usually much less than j(Fc))j .
The recursive decomposition procedure is based on the following result that express the optimal solution to the model integration problem in terms of the solutions to smaller problems on subsets of features . a ; pfi
Fia i gn ie , Fa = Fc n Fa . Let Fia = Fi Fa ; Fia = Fi
Proposition 3 Let Fa ; Fa be complementary subsets of Fc , Fa ; [ i]n 1 . a be the optimal solutions for the model integrai=1 i=1 as the model weights . aja(fajfa ) be the optimal solution to the model
Let pfi tion problems based on the local marginal densities fpFia and fp Further , let pfi integration problem involving the conditional densities fp [ i]n ( 4 ) is given by
Fia i ( fajfa)gn i ( fia ) ; 1 . The solution to the original model integration problem i=1 with model weights given by 0 i=1 respectively with fign i = ipFia i gn
( a ) pfi c ( fc ) = pfi a(fa)pfi a(fa ) when Fa and Fa are independent .
( b ) pfi c ( fc ) = pfi a(fa)pfi aja(fajfa ) when Fa and Fa are not in dependent .
The above result outlines a method for reducing the original model integration problem of size ( Fc ) into smaller problems of sizes ( Fa ) and ( Fa ) . When the desired global model exhibits conditional independence , a recursive scheme with judicious choice of Fa and Fa at each stage can result in an efficient solution . For example , when the local models all correspond to naive Bayes classifiers with the same class attribute , but possibly different data attributes , invoking Proposition 3(b ) with Fa as the class attribute and Fa as the remaining attributes enables us to decompose the overall model integration problem into that of integrating the class priors and the class conditional densities for each class . Further , using the naive Bayes assumption and Proposition 3(a ) , the optimal conditional density for each class is identical to the product of the optimal densities of the individual attribute given the class .
Algorithm 3 in [ 15 ] shows a recursive decomposition scheme for model integration . If a given problem has a closed form solution , then the optimal model is computed directly . Otherwise , we look for subsets Fa and Fa that are independent of each other and when such independent splits are available , we invoke Proposition 3(a ) to split the model integration problem . When there are no independent splits , we choose any split Fa and Fa , preferring those in which one of the subsets can be further split into independent sets and invoke Proposition 3(b ) . 5.2 Closed Form solutions
We now describe some special scenarios that admit closed form solutions for the model integration problem .
Identical and disjoint feature sets First , we consider two simple base scenarios corresponding to identical and disjoint feature sets respectively . In case of identical feature sets(ie , horizontally partitioned data ) , the complete feature set Fc = Fi ; [ i]n 1 and the optimal solution to the model integration problem ( 4 ) is given by [ 16 ] nodes , an intermediate node corresponding to the features shared between the two nodes is created such that each intermediate node corresponds to a unique set of features ( ie , no repetition ) . All the nodes are then arranged in the form of a tree such that for each node , all the nodes corresponding to supersets of the node are contained in the sub tree under that node . In general , such an arrangement results in some nodes lying under multiple sub trees as in Figure 3(a ) , ie , there are cycles in the tree . However , for situations where the feature sets can be arranged as a tree without cycles as in Figure 3(b ) , the feature sets are considered to be hierarchically ordered and the unconstrained model integration problem ( 4 ) admits a closed form solution . Note that we have not made any independence assumptions in this setting . More ff
X
Y
( a )
X , Y
Z
           
Y , Z
           
Z,W
( b )
X , Y , W
X , Y
X , Y , Z
X , Y , W
X,U
" " " " " " " " " " " " ! ! ! ! ! ! ! ! ! ! ! !
X , Y , W pfi c ( fc ) = n i=1 ipi(fc ) i=1 i
:
( 13 )
Figure 3 : Figures ( a ) and ( b ) show examples of scenarios that can and cannot be arranged in a hierarchical fashion . The shaded nodes are the leaf nodes and the rest are intermediate ones . pfi
( 14 )
1 ; [ i2]n
In case of disjoint feature sets , the complete feature set Fc = 1 ; i1 6= i2 . Since there is no overlap of features , none of the local models show dependence between any pair of feature sets . Hence , a repeated application of Proposition 3(a ) shows that the optimal solution is just the product distribution , ie ,
  n i=1 Fi where Fi1 Fi2 = ; ; [ i1]n n i=1 c ( fc ) = pi(fi ) :
Hierarchy of feature sets We now look at scenarios where the feature sets of the collaborating parties can be organized in a certain hierarchical fashion . First , consider a flat tree ( depth =1 ) configuration where the feature sets of all the parties have a common overlapping feature set F0 corresponding to the root node . The feature sets of the individual parties are assigned to the leaf nodes of this tree . In order to solve the model integration problem for this setting , we first split it into sub problems corresponding to the complementary sets F0 and F0 = Fc n F0 by invoking Proposition 3(b ) . The sub problems corresponding to F0 and F0 can then be directly addressed using the closed form solutions for identical and disjoint feature sets respectively as shown in the following lemma .
Lemma 2 In problem ( 4 ) , when every pair of collaborating parties have a common set of overlapping features , ie , [ i]n 1 .
1 ; i1 6= i2 and Fi0 = Fi n F0 ;
1 ; [ i2]n
[ i1]n
Then , the optimal solution to ( 4 ) is given by
Fi1 Fi2 = F0 ; c ( fc ) = pfi
( f0 ) n i=1 ipF0 i i=1 i
Fi0 i p
( fi0jf0 ) n i=1
For the general case , we adopt the following procedure to construct the tree . First , the feature set of each collaborating party is assigned to a leaf node and for every pair of
F(Nck1 formally , let F(N ) represent the feature set corresponding to any node N . Let C(N ) denotes the number of immediate children of N and fNck gC(N ) k=1 be the children . Then , k=1 F(Nck ) and the nodes in the tree satisfy F(N ) = C(N )
To address this general scenario , we observe that Lemma 2 enables us to combine the child node distributions to obtain the optimal distribution over the union of the feature sets of all the children . Hence , the optimal distribution over all the feature sets in the tree can be obtained by computing the optimal solution at each sub tree in a bottom up fashion using repeated invocations of Lemma 2 .
) F(Nck2
; [ k2]C(N )
) = F(N ) ;
[ k1]C(N )
; k1 6= k2 .
1
1
6 . EXPERIMENTAL RESULTS
In this section , we provide empirical evidence that high quality global models can be obtained without much loss of privacy using our model integration approach for both discrete and continuous data . We present results on artificial and real datasets that show how various factors such as feature correlation and privacy of local models affect the quality of the global model obtained through our method . 6.1 Datasets
For our experiments , we used both artificial and real datasets consisting of continuous vector , discrete and high dimensional directional attributes . Table 1 shows details of the datasets . The main purpose of the artificial datasets was to facilitate comparison of the global model quality with the true generative model and to study the effect of correlation between features on the effectiveness of our approach . In order to achieve this , we generated five Gaussian datasets from mixtures of 15 spherical Gaussians . To ensure that the datasets had increasing degrees of correlation between the features , the mixture centers for each dataset were obtained by sampling from a Gaussian with the same mean , f but varying covariance matrices ( increasing off diagonal elements ) . The average absolute pairwise correlation values between features for the five datasets are 0:0295 , 0:0869 , 0:1374 , 0:2447 , 0:4171 respectively . We also used two real datasets | Mini 20Newsgroup ( subset of the 20 Newsgroup dataset [ 13 ] ) consisting of high dimensional directional text data , and Dermatology ( from UCI ) consisting of clinical trial results . The datasets can be downloaded from http://lanseceutexasedu/srujana/mr/
6.2 Evaluation Methodology
For each experiment , the relevant dataset was partitioned at random among the various sites . The features were also partitioned so that each site has access to only some features . In case of classification , we hold out a test set and partition only the training set among the sites . Depending on the learning task , we built parametric models from the local data using appropriate EM algorithms or direct maximum likelihood estimation ( MLE ) methods . For continuous data , the local models were used to generate artificial samples , which were then used to learn the global model . For discrete data , the local models were directly used to obtain the global model by solving the appropriate KL projection problem ( 8 ) . We also trained a centralized model by pooling data from all the sites .
For each experiment , we computed the privacy of the local models as well as the quality of the global , centralized and the various local models . For clustering tasks , the quality was quantified using normalized mutual information ( NMI ) and for classification , it was quantified using misclassification error ( ME ) . In case of artificial data , KL divergence ( KL ) with respect to the true model was also computed . Further , results were averaged over multiple runs of the experiments performed using either different sets of features or different datasets in the case of artificial data . Table 2 shows details of the learning algorithms and performance metrics .
6.3 Results on artificial data
First , we performed controlled experiments on the artificial datasets to analyze the behavior of our algorithms for both discrete and parametric model integration . For a fair comparison , the local models were assumed to have the same form as the global and centralized model and the number of artificial samples used for learning the global model ( global sample size ) was chosen to equal the combined size of all the local datasets . Keeping all other factors unchanged , we studied how the quality of the global solution varies with respect to feature correlation , number of overlapping features , privacy of the local models and the global sample size . The results of our experiments are discussed below .
Quality of the global model . Figure 4 shows the quality of the various models for the Gaussian data . In all the cases , the quality of the global model is better than that of the local models and is closer to that of the centralized model .
Quality vs . feature correlation . Figure 4 also shows the variation of the quality of the various models with the correlation between the features and the number of overlapping features . We observe that the difference between the global and the local models is more significant when the average feature correlation is low and there is less overlap between features since in this case , the combined information in the local mod n o i t a m r o n f i l a u t u m d e z i l a m r o N e c n e g r e v d − L K i
0.6
0.5
0.4
0.3
0.2
0.1
0
0
35
30
25
20
15
10
5
0
0
0.2
0.4
Feature correlation
0.2
0.4
Feature correlation
0.6
0.5
0.4
0.3
0
12
10
8
6
4
2
0
0
0.2
0.4
Feature correlation
0.2
0.4
Feature correlation
0.65
0.6
0.55
0.5
0.45
0.4
0
0.02
0.015
0.01
0.005
0
0
0.2
0.4
Feature correlation
0.2
0.4
Feature correlation
Global : o , Centralized : x , Avg−Local : +
Figure 4 : Quality vs . avg . feature correlation for A Gaussian . #clusters = 15 , global sample size = 5000 . Columns correspond to the cases #features = 2,6 and 10 . n o i t a m r o f n i l a u t u m d e z i l a m r o N
0.5
0.4
0.3
0.2
0.1
8.5
8
7.5
7
6.5
6
5.5
5 e c n e g r e v d − L K i
5.2
5
4.8
4.6
4.4
4.2
4 y c a v i r p g o l e g a r e v A
1 5
0 Number of local clusters
15
30
50
4.5 Number of local clusters
1 5 15
30
50
3.8 Number of local clusters
1 5 15
30
50
Global : o , Centralized : x , Avg−Local : +
Figure 5 : Quality vs . privacy for A Gaussian . #features/site=6 and global sample size = 5000 . els is significantly greater than that in the individual models . Further , the model quality shows an initial decrease with increasing feature correlation , but eventually improves . This non monotonic behavior probably arises due to the trade off between the problem difficulty and the amount of information in a given number of samples . When the features are uncorrelated , the amount of information per sample is the highest , but the difficulty level of the problem is also high , whereas for highly correlated features , it is the opposite .
Quality vs . privacy . We also studied the trade off between privacy , and the quality of the global model by varying the resolution of the local models , ie , the number of local clusters in this case . Figure 5 shows the variation of the quality measures and the average log privacy with the local model resolution . We note that as the local model resolution increases , the quality of global model improves while the average log privacy goes down . In particular , the centralized model corresponding to a complete loss of privacy has the highest quality . However , due to the natural structure in data , comparable quality can be obtained with much less resolution and reasonably high privacy .
Quality vs . sample size . The parametric model integration problem is based on an approximation that is exact only when the number of artificial samples used for learning the
Datasets A Gaussian Mini 20 Newsgroup Dermatology
#features #samples #classes/clusters #sites #Features/site #Samples/site 10 1000 30
1000 400 100
600 20
5000 5 2000 5 366 3 Table 1 : Details of datasets
15 20 6
2,6,10
Dataset A Gaussian
Problem clustering
Mini 20 Newsgroup clustering
Dermatology classification
300+66
Train Test split Quality measure
Algorithm
NMI , KL
EM
Other Params #local clusters
( Diagonal Gaussians )
1,5,15,30,50
NMI
ME
EM ( VMF )
Bayesian
#global samples [ 1,3,5,7,9]x1000 #local clusters
1,5,10,20,40 Independence assumptions
Table 2 : Details of experimental setup n o i t a m r o n f i l t a u u m d e z i l a m r o N
0.5
0.48
0.46
0.44
0.42
0.4
0.38
0
6
5.9
5.8
5.7
5.6
5.5
5.4
5.3
5.2
5.1 e c n e g r e v d − L K i
10000
5
0
2000
4000
6000
8000
Global sample size n o i t a m r o n f i l a u t u m d e z i l a m r o N
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
10000
2000
4000
6000
8000
Global sample size
17
16
15
14
13
12 y c a v i r p g o l e g a r e v A
50
11
0
Global Centralized Max−local Avg−local
10
20
30
40
Number of local clusters
10
20
30
40
50
Number of local clusters
Global : o , Centralized : x , Avg−Local : +
Figure 6 : Quality vs . Gaussian . #features/site=6 , #local clusters=15 global sample size for A global model tends to 1 . Hence , we expect the global model quality to improve with increasing sample size . The variation of the global model quality with respect to the global sample size shown in Figure 6 supports this hypothesis . In particular , we observe that the quality of the global model steadily increases and approaches that of the centralized model when the global sample size is the same as the combined size of the local data sources .
6.4 Results on real data
We also conducted experiments on some real life datasets to demonstrate the effectiveness of our model integration techniques for various types of data and applications .
Clustering high dimensional text data . Figure 7 shows the quality of the global model and the average log privacy of the local models on the mini 20Newsgroup data for varying number of local clusters . We observe that the global model is better than even the best local model . Further , for a reasonable number of clusters , the global model is almost as good as the centralized model , while ensuring a high level of privacy .
Classification of discrete medical data . The Dermatology dataset consists of discrete clinical attributes that are relevant for diagnosing skin diseases and hence , classification is the most relevant learning task in this case . To accommodate the large number of attributes , we assume conditional independence of features ( Naive Bayes ) or sets of features ( Partial Conditional Independence ) given the class . Table 3 shows the classification results . The ensemble model is based
Figure 7 : Performance on Mini 20Newsgroup . on averaging the class posteriors of the local models . When there is no compression , the centralized model is identical to the global model since both the models are based on identical conditional independence assumptions , which make the extra information available to the centralized model redundant . As before , there is a trade off between the privacy of the local models and the quality of the global model .
7 . RELATED WORK
Our work is primarily related to four main areas : distributed learning , privacy preserving data mining , information theory and iterative algorithms . In particular , the problem involves distributed learning in a privacy preserving setting while the mathematical formulation is based on informationtheoretic ideas such as maximum likelihood and maximum entropy and the proposed solutions are based on iterative optimization techniques .
As mentioned earlier , there has been a lot of work on distributed learning techniques . However , most of these techniques [ 20 , 10 , 19 ] focus only on horizontally or vertically partitioned data . The work in [ 17 ] considers heterogeneous data sources , but is mainly focused on obtaining generalization error bounds for the distributed classification task . In the current work , we propose a distributed model based learning framework that can simultaneously address a number of learning tasks such as classification , clustering , learning Bayesian networks , etc . , and is applicable to a wide range of distributed scenarios where there are no restrictions on the features available at each site . Further , unlike some earlier parametric model combining techniques [ 7 ] that are restricted to vector data , our framework is based on generative models and applies to a wide range of complex data types encountered in data mining .
Setting
Global Naive Bayes 12.18 % ( with compression ) 1:54 % 10.61 % Naive Bayes 1:54 % 9.78 % 1:23 %
Partial conditional ( independence )
Misclassification Error
Avg . Log privacy
Centralized Ensemble Avg . Local
10.61 % 1:06 % 10.61 % 1:54 % 9.78 % 1:23 %
18.76 % 2:26 % 14.45 % 1:41 % 14.87 % 1:20 %
16.93 % 1:72 % 14.64 % 1:37 % 14.13 % 1:34 %
10.5093 0:5866 8.6068 0:5178 8.2260 0:3704
Table 3 : Classification performance on Dermatology .
In the recent years , there has been considerable work on privacy preserving distributed data mining techniques A survey of these techniques can be found in [ 8 ] . Of these , random perturbation based techniques[2 ] are limited to vector data and there are no theoretical guarantees on the achieved privacy [ 11 ] . In contrast , secure multi party computation based techniques [ 14 , 20 ] do not often capture the privacy requirements of real life scenarios and also involve high computational and communication costs . Our current work extends the framework proposed in [ 16 ] , which is based on an information theoretic notion of privacy and involves sharing only parametric models that satisfy the privacy requirements at each site . The main benefit of our approach is scalability and modularity , which makes it amenable to other privacypreserving transformations such as data swapping , etc .
Our formulation of the model integration problem is based on the maximum likelihood and the maximum entropy principles , which are known to have applications in a wide range of domains [ 4 , 5 ] . For discrete domains , the KL projection problem is closely related to inverse problems in positron emission tomography [ 3 , 18 ] highlighting the fact that the model integration problem can also be viewed as reconstructing the original distribution from noisy partial views . A similar projection problem arises in linear multi variate logistic regression based on multinomial or Poisson models [ 12 ] . However , the order of the arguments q and Ap is reversed in this case , requiring a completely different solution for this case . The solution to the maximum entropy problem is based on iterative projection methods [ 3 ] developed for optimizing convex objective functions associated with Bregman loss functions and in particular , KL divergence . [ 3 ] describes a number of these techniques such as Bregman ’s row action method , MART , SMART , etc . For the KL divergence minimization , we adopt the IRLS algorithm [ 9 ] , which is known to be computationally more efficient , and has in past been applied to problems such as multi variate logistic regression .
8 . CONCLUSION
We proposed a distributed learning framework based on probabilistic models that takes into account privacy constraints , and is applicable to a large class of learning tasks , and to general distributed settings involving diverse schema . In order to achieve this , we formulated the distributed model integration problem using maximum likelihood and maximum entropy principles . We also developed efficient solutions for both discrete and continuous domains , and specialized algorithms for scenarios involving conditional independence assumptions and hierarchically ordered sets . All our algorithms require a computation time that is linear in the size of the local models , thus making our approach scalable for large datasets . Experimental evaluation of our algorithms on various types of data ( both continuous and discrete ) indicates that high quality distributed learning can be performed without much loss of privacy .
9 . ACKNOWLEDGMENTS
We would like to acknowledge support from the NSF under grants IIS 0307792 and IIS 0312471 .
10 . REFERENCES [ 1 ] D . Agrawal and C . C . Aggarwal . On the design and quantification of privacy preserving data mining algorithms . In PODS , pages 247{255 , 2001 .
[ 2 ] R . Agrawal and R . Srikant . Privacy preserving data mining .
In ACM SIGMOD , pages 439{450 , 2000 .
[ 3 ] Y . Censor and S . Zenios . Parallel Optimization : Theory , Algorithms , and Applications . Oxford Univ . Press , 1998 . [ 4 ] T . M . Cover and J . A . Thomas . Elements of Information
Theory . Wiley , 1991 .
[ 5 ] I . Csiszar . Why least squares and maximum entropy ? An axiomatic approach to inference for linear inverse problems . Annals of Stat . , 19(4):2032{2066 , 1991 .
[ 6 ] A . Evfimievski , R . Srikant , R . Agrawal , and J . Gehrke . Privacy preserving mining of association rules . In KDD , pages 217{228 , 2002 .
[ 7 ] U . M . Fayyad , C . Reina , and P . S . Bradley . Initialization of iterative refinement clustering algorithms . In ICML , pages 194{198 , 1998 .
[ 8 ] J . Gehrke . Special issue on privacy and security . SIGKDD
Explorations , 4(2 ) , 2002 .
[ 9 ] P . J . Green . Iteratively re weighted least squares for maximum likelihood estimation . Journal of Royal Statistical society B , 46:149{192 , 1984 .
[ 10 ] E . Johnson and H . Kargupta . Collective , hierarchical clustering from distributed , heterogeneous data . In Large Scale Parallel KDD Systems , LNCS , 1999 .
[ 11 ] H . Kargupta , S . Dutta , Q . Wang , and K . Sivakumar . On privacy preserving properties of random data perturbation techniques . In ICDM , pages 99{107 , 2003 .
[ 12 ] J . Kivinen and M . K . Warmuth . Relative loss bounds for multidimensional regression problems . Machine Learning , 45:301{329 , 2001 .
[ 13 ] K . Lang . 20 newsgroup data . http://kddicsuciedu/databases/20newsgroupshtml
[ 14 ] Y . Lindell and B . Pinkas . Privacy preserving data mining .
Journal of cryptology , 15(3):177{206 , 2002 .
[ 15 ] S . Merugu and J . Ghosh . A distributed learning framework based on probabilistic models . Technical report , University of Texas at Austin , Feb 2005 .
[ 16 ] S . Merugu and J . Ghosh . A privacy sensitive approach to distributed clustering . Pattern Recognition Letters , 26:399{410 , 2005 .
[ 17 ] J . G . S . Ben David and RSchuller A theoretical framework for learning from a pool of disparate data sources . In KDD , pages 443{449 , 2002 .
[ 18 ] L . A . Shepp and YVardi Maximum likelihood reconstruction for emission tomography . IEEE Transactions on medical imaging , 1:113{122 , 1982 .
[ 19 ] A . Strehl and J . Ghosh . Cluster ensembles { a knowledge reuse framework for combining partitionings . JMLR , pages 3:583{617 , 2002 .
[ 20 ] J . Vaidya and C . Clifton . Privacy preserving clustering on vertically partitioned data . In KDD , pages 206{215 , 2003 .
[ 21 ] A . C . Yao . How to generate and exchange secrets . In FOCS , pages 162{167 , 1986 .
