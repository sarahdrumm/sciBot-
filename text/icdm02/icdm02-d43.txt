Mining Motifs in Massive Time Series Databases
Pranav Patel Eamonn Keogh Jessica Lin Stefano Lonardi
University of California Riverside
Computer Science & Engineering Department
Riverside , CA 92521 , USA
{prpatel , eamonn , jessica , stelo}@csucredu
Abstract
The problem of efficiently locating previously known patterns in a time series database ( ie , query by content ) has received much attention and may now largely be regarded as a solved problem . However , from a knowledge discovery viewpoint , a more interesting problem is the enumeration of previously unknown , frequently occurring patterns . We call such patterns “ motifs ” , because of their close analogy to their discrete counterparts in computation biology . An efficient motif discovery algorithm for time series would be useful as a tool for summarizing and visualizing massive time series databases . In addition it could be used as a subroutine in various other data mining tasks , including the discovery of association rules , clustering and classification . In this work we carefully motivate , then introduce , a nontrivial definition of time series motifs . We propose an efficient algorithm to discover them , and we demonstrate the utility and efficiency of our approach on several real world datasets .
1 . Introduction to their discrete counterparts
The problem of efficiently locating previously defined patterns in a time series database ( ie , query by content ) has received much attention and may now be essentially regarded as a solved problem [ 1 , 8 , 13 , 21 , 22 , 23 , 35 , 40 ] . However , from a knowledge discovery viewpoint , a more interesting problem is the detection of previously unknown , frequently occurring patterns . We call such patterns motifs , because of their close analogy in computation biology [ 11 , 16 , 30 , 34 , 36 ] . Figure 1 illustrates an example of a motif discovered in an astronomical database . An efficient motif discovery algorithm for time series would be useful as a tool for summarizing and visualizing massive time series databases . In addition , it could be used as subroutine in various other data mining tasks , for instance : • The discovery of association rules in time series first requires the discovery of motifs ( referred to as “ primitive shapes ” in [ 9 ] and “ frequent patterns ” in [ 18] ) . However the current solution to finding the motifs is either high quality and very expensive , or low quality but cheap [ 9 ] .
• Several researchers have advocated K means clustering of time series databases [ 14 ] , without adequately addressing the question of how to seed the initial points , or how to choose K . Motifs could potentially be used to address both problems . In addition , seeding the algorithm with motifs rather than random points could speed up convergence [ 12 ] .
• Several time series classification algorithms work by constructing typical prototypes of each class [ 24 ] . While this approach works for small datasets , the construction of the prototypes ( which we see as motifs ) requires quadratic time , as is thus unable to scale to massive datasets . In this work we carefully motivate , then introduce a nontrivial definition of time series motifs . We further introduce an efficient algorithm to discover them .
A
B
C
0
500
1000
1500
2000
2500
A B C
0
60
80
20
100 120 Figure 1 : An astronomical time series ( above ) contains 3 near identical subsequences . A “ zoom in ” ( below ) reveals just how similar to each other the 3 subsequences are .
40
The rest of this paper is organized as follows . In Section 2 we formally define the problem at hand and consider related work . In Section 3 we introduce a novel low dimensional discrete representation of time series , and prove that it can be used to obtain a lower bound on the true Euclidean distance . Section 4 introduces our motif finding algorithm , which we experimentally evaluate in Section 5 . In Section 6 we consider related work , and finally in Section 7 we draw some conclusions and highlight directions for future work .
2 . Background and Related Work
The following section is rather dense on terminology and definitions . These are necessary to concretely define the problem at hand , and to explain our proposed solution . We begin with a definition of our data type of interest , time series : Definition 1 . Time Series : A time series T = t1,…,tm is an ordered set of m real valued variables .
Time series can be very long , sometimes containing billions of observations [ 15 ] . We are typically not interested in any of the global properties of a time series ; rather , data miners confine their interest to subsections of the time series [ 1 , 20 , 23 ] , which are called subsequences . Definition 2 . Subsequence : Given a time series T of length m , a subsequence C of T is a sampling of length n < m of contiguous position from T , that is , C = tp,…,tp+n 1 for 1£ p £ m – n + 1 .
A task associated with subsequences is to determine if a given subsequence is similar to other subsequences [ 1 , 2 , 3 , 8 , 13 , 19 , 21 , 22 , 23 , 24 , 25 , 27 , 29 , 35 , 40 ] . This idea is formalized in the definition of a match . Definition 3 . Match : Given a positive real number R ( called range ) and a time series T containing a subsequence C beginning at position p and a subsequence M beginning at q , if D(C , M ) £ R , then M is called a matching subsequence of C .
The first three definitions are summarized in Figure 2 , illustrating a time series of length 500 , and two subsequences of length 128 .
T
M
C 150
50
300
350
250
200
100
0 450 500
Figure 2 : A visual intuition of a time series T ( light line ) , a subsequence C ( bold line ) and a match M ( bold gray line ) For the time being we will ignore the question of what two distance function subsequences match . We will address this in Section 33 to determine whether to use
400
The definition of a match is rather obvious and intuitive ; but it is needed for the definition of a trivial match . One can observe that the best matches to a subsequence ( apart from itself ) tend to be the subsequences that begin just one or two points to the left or the right of the subsequence in question . Figure 3 illustrates the idea .
Trivial M atch
200
350
300
250
100
400
450 500
C 150
Trivial M atch 50 0 Figure 3 : For almost any subsequence C in a time series , the best matches are the trivial subsequences immediately to the left and right of C Intuitively , any definition of motif should exclude the possibility of over counting these trivial matches , which we define more concretely below . Definition 4 . Trivial Match : Given a time series T , containing a subsequence C beginning at position p and a matching subsequence M beginning at q , we say that M is a trivial match to C if either p = q or there does not exist a subsequence M’ beginning at q’ such that D(C , M’ ) > R , and either q < q’< p or p < q’< q .
We can now define the problem of enumerating the K most significant motifs in a time series .
Definition 5 . K Motifs : Given a time series T , a subsequence length n and a range R , the most significant motif in T ( called thereafter 1 Motif ) is the subsequence C1 that has the highest count of non trivial matches ( ties are broken by choosing the motif whose matches have the lower variance ) . The Kth most significant motif in T ( called thereafter K Motif ) is the subsequence CK that has the highest count of non trivial matches , and satisfies D(CK , Ci ) > 2R , for all 1 £ i < K .
Note that this definition forces the set of subsequences in each motif to be mutually exclusive . This is important because otherwise two motifs might share the majority of their elements , and thus be essentially the same .
Having carefully defined the necessary terminology , we now introduce a brute force algorithm to locate 1 motif . The generalization of this algorithm to finding K motifs is obvious and omitted for brevity .
Algorithm Find 1 Motif Brute Force(T,n,R )
1 . 2 . 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . 13 . 14 . 15 . 16 . 17 . best_motif_count_so_far = 0 ; best_motif_location_so_far = null ; for i = 1 to length(T) n + 1 count = 0 ; pointers = null ; for j = 1 to length(T) n + 1 if non_trivial_match(T[i:i+n 1],T[j:j+n 1],R ) count = count + 1 ; pointers = append(pointers,j ) ; end ; end ; if count > best_motif_count_so_far best_motif_count_so_far = count ; best_motif_location_so_far = i ; motif_matches = pointers ; end ; end ; Table 1 : The Find 1 Motif Brute Force algorithm
The algorithm requires O(m2 ) calls to the distance function . Since the Euclidean distance is symmetric [ 22 ] , one could theoretically cut in half the CPU time by storing D(A,B ) and re using the value when it is necessary to find D(B,A ) , however , this would require storing m(m 1)/2 values , which is clearly untenable for even moderately sized datasets . the that is reduced time series
We will introduce our sub quadratic algorithm for finding motifs in Section 4 . Our method requires a discrete representation of in dimensionality and upon which a lower bounding distance measure can be defined . Since no representation in the literature fulfills all these criteria , we will introduce such a representation in the next section . 3 . Dimensionality Reduction and Discretization
Our discretization technique allows a time series of arbitrary length n to be reduced to a string of arbitrary length w , ( w < n , typically w << n ) . The alphabet size is also an arbitrary integer a , where a > 2 .
As an intermediate step between the original time series and our discrete representation of it , we must create a dimensionality reduced version of the data . We will utilize the Piecewise Aggregate Approximation ( PAA ) [ 22 , 40 ] , which we review in the next section .
3.1 Dimensionality Reduction
A time series C of length n can be represented in a w . The ith dimensional space by a vector element of C is calculated by the following equation :
= cC ,1 wc
, c i
= w n j
= n w to reduce i n w c j +i 1)1( the
( 1 )
Simply stated , time series from n dimensions to w dimensions , the data is divided into w equal sized “ frames ” . The mean value of the data falling within a frame is calculated and a vector of these values becomes the data reduced representation . The representation can be visualized as an attempt to approximate the original time series with a linear combination of box basis functions as shown in Figure 4 .
1.5 1 0.5 0 0.5 1 1.5
C
C
0
20
40
60
80
100
120 c0 c1 c2 c3 c4 c5 c6 c7 shown
Figure 4 : The PAA representation can be readily visualized as an attempt to model a sequence with a linear combination of box basis functions . In this case , a sequence of length 128 is reduced to 8 dimensions The PAA dimensionality reduction is intuitive and simple , yet has been sophisticated dimensionality reduction techniques like Fourier transforms and wavelets [ 8 , 22 , 40 ] . In addition it has several advantages over its rivals , including being much faster to compute , and being able to support many different distance functions , including weighted distance functions [ 24 ] , arbitrary Minkowski norms [ 40 ] , and dynamic time warping [ 13 ] . 3.2 Discretization rival more to
Having transformed a time series database into the PAA we can apply a further transformation to obtain a discrete representation . For reasons that will become apparent in Section 4 , we require a discretization technique that will produce symbols with equiprobability . This is easily achieved since normalized time series have a Gaussian distribution . To illustrate this , we extracted subsequences of length 128 from 8 different time series and plotted a normal probability plot of the data as shown in Figure 5 .
Given that the normalized time series have highly Gaussian distribution , we can simply determine the “ breakpoints ” that will produce a equal sized areas under Gaussian curve .
0.999 0.997 0.99 0.98 0.95 0.90
0.75
0.50
0.25
0.10 0.05 0.02 0.01 0.003 0.001 y t i l i b a b o r P
10
0
10
Figure 5 : A normal probability plot of the distribution of values from subsequences of length 128 from 8 different datasets . The highly linear nature of the plot strongly suggests that the data came from a Gaussian distribution .
Definition 6 . Breakpoints : breakpoints are a sorted list of numbers B = b1,…,ba 1 such that the area under a N(0,1 ) Gaussian curve from bi to bi+1 = 1/a ( b0 and ba are defined as ¥ and ¥ , respectively ) .
These breakpoints may be determined by looking them up in a statistical table . For example Table 2 gives the breakpoints for values of a from 3 to 10 .
0
0
5
6
7
3
4
0
9
8
0.67
0.67
10
0.43 0.97
0.43 0.43
0.97 0.43
0.32 0.67 1.15
1.15 0.67 0.32
0.84 0.25 0.25 0.84
1.28 0.84 0.52 0.25
1.07 0.57 0.18 0.18 0.57 1.07 a bi b1 b2 b3 b4 b5 b6 b7 b8 b9 Table 2 : A lookup table that contains the breakpoints that divide a Gaussian distribution in an arbitrary number ( from 3 to 10 ) of equiprobable regions Once
1.22 0.76 0.43 0.14 0.14 0.43 0.76 1.22 the breakpoints have been obtained we can discretize a time series in the following manner . We first obtain a PAA of the time series . All PAA coefficients that are below the smallest breakpoint are mapped to the symbol “ a ” , all coefficients greater than or equal to the smallest breakpoint and less than the second smallest breakpoint are mapped to the symbol “ b ” , etc . Figure 6 illustrates the idea .
0.25 0.52 0.84 1.28
0
1.5 1 0.5 0 0.5 1 1.5 b a 20 a 40
0 c c c b b
60
80
100
120
Figure 6 : A time series is discretized by first obtaining a PAA approximation and then using predetermined breakpoints to map the PAA coefficients into symbols . In the example above , with n = 128 , w = 8 and a = 3 , the time series is mapped to the word baabccbc
Note that in this example the 3 symbols , “ a ” , “ b ” and “ c ” are approximately equiprobable as we desired . We call the concatenation of symbols that represent a subsequence a word . Definition 7 . Word : A subsequence C of length n can be represented as a word as follows . Let alphai denote the ith element of the alphabet , ie , alpha1 = a and alpha2 = b . Then the mapping from a PAA approximation C to a word Cˆ is obtained as follows :
ˆ ,ˆ cC 1
=
ˆ , wc
ˆ c i
= alpha
, j iif b j
1
< c i
£ b j
( 2 )
We have now defined the two representations required for our motif search algorithm ( the PAA representation is merely an intermediate step required to obtain the symbolic representation ) . 3.3 Distance Measures
Having considered various representations of time series data , we can now define distance measures on them . By far the most common distance measure for time series is the Euclidean distance [ 8 , 22 , 23 , 32 , 40 ] . Given two time series Q and C of the same length n , Eq 3 defines their Euclidean distance , and Figure 7.A illustrates a visual intuition of the measure .
( CQD
,
)
”
( q n = i 1 i
) c i
2
( 3 )
If we transform the original subsequences into PAA representations , Q and C , using Eq 1 , we can then obtain a lower bounding approximation of the Euclidean distance between the original subsequences by : ( = q i
( 4 ) This measure is illustrated in Figure 7B A proof that DR( Q , C ) lower bounds the true Euclidean distance appears in [ 22 ] ( an alterative proof appears in [ 40] ) .
CQDR c i
” w i
) n w
)
(
,
1
2
If we further transform the data into the symbolic representation , we can define a MINDIST function that returns the minimum distance between the original time series of two words :
MINDIST
)ˆ,ˆ( CQ
” n w
= w i
1
( dist
) 2)ˆ,ˆ( cq i i
( 5 )
The function resembles Eq 4 except for the fact that the distance between the two PAA coefficients has been replaced with the sub function dist( ) . The dist( ) function can be implemented using a table lookup as illustrated in Table 3 . a b c a 0
0
0.86 b 0
0
0 c 0.86
0
0
Table 3 : A lookup table used by the MINDIST function . This table is for an alphabet of cardinality , ie a = 3 . The distance between two symbols can be read off by examining the corresponding row and column . For example dist(a,b ) = 0 and dist(a,c ) = 086
The value in cell ( r,c ) for any lookup table of can be calculated by the following expression . if b otherwise
£ cell
( 6 ) b c , cr 1 ) ,
,0 max(
= min( cr ) ,
1 cr , r
For a given value of the alphabet size a , the table need only be calculated once , then stored for fast lookup . The MINDIST function can be visualized is Figure 7C
' fi .
1.5
1 0.5 0 0.5 1 1.5 0
1.5 1 0.5 0 0.5 1 1.5 0
C
20
40
60
Q 80
100
120
C
20
40
60
Q 80
100
120
C ˆ
= b aabccbc
Q ˆ
= babcacca
( A )
( B )
( C ) for
Figure 7 : A visual intuition of the three representations discussed in this work , and the distance measures defined on them . A ) The Euclidean distance between two time series can be visualized as the square root of the sum of the squared differences of each pair of corresponding points . B ) The distance measure defined the PAA approximation can be seen as the square root of the sum of the squared differences between each pair of corresponding PAA coefficients , multiplied by the square root of the compression rate . C ) The distance between two symbolic representations of a time series requires looking up the distances between each pair of symbols , squaring them , summing the square root and finally multiplying by the square root of the compression rate .
4 . Efficient Motif Discovery taking them ,
Recall that the brute force motif discovery algorithm introduced Table 1 requires O(m2 ) calculations of the distance function . As previously mentioned , the symmetric property of the Euclidean distance measure could be used to half the number of calculations by storing D(Q,C ) and reusing the value when it is necessary to find D(C,Q ) . In fact , further optimizations would be possible under this scenario . We now give an example of such optimization .
Suppose we are in the innermost loop of the algorithm , attempting to enumerate all possible matches within R = 1 , to a particular subsequence Q . Further suppose that in previous iterations we had already discovered that D(Ca,Cb ) = 2 . As we go through the innermost loop we first calculate the distance D(Q,Ca ) and discover it to be 7 . At this point we should continue on to measure D(Q,Cb ) , but in fact we don’t have to do this calculation! We can use the triangular inequality to discover that D(Q,Cb ) could not be a match to Q . The triangular inequality requires that [ 2 , 22 , 33 ] :
D(Q,Ca ) £ D(Q,Cb ) + D(Ca,Cb ) ( 7 )
Filling in the known values give us
7 £ D(Q,Cb ) + 2 ( 8 )
Rearranging the terms gives us
5 £ D(Q,Cb ) ( 9 ) But since we are only interested in subsequences that are a distance less than 1 unit away , there is no point in determining the exact value of D(Q,Cb ) , which we now know to be at least 5 units away .
The first formalization of this idea for fast searching of nearest neighbors in matrices is generally credited to Burkhard and Keller [ 5 ] . More efficient implementations are possible ; for example , Shasha and Wang [ 33 ] introduced the Approximation Distance Map ( ADM ) algorithm that precomputes an arbitrary set of distances instead of using just one randomly chosen reference point .
For the problem at hand , however , the techniques discussed above seem of little utility , since as previously noted , we are unlikely to have O(m2 ) space in which to store the entire matrix . We propose to use just such a technique as a subroutine in our motif discovery algorithm . Our idea is to create only a small portion of the matrix at a time , and exploit the techniques above to search it . Our contribution comes from the method we use to construct the small matrix . As we will demonstrate , we can use our MINDIST function to create a matrix , much smaller than O(m2 ) , which is guaranteed to contain all the subsequences which are within R of a promising candidate for a motif .
In addition to all the matching sequences to a promising candidate , the small matrix will generally contain some nonmatching subsequences , or “ false hits ” . We use Shasha and Wang ’s ADM algorithm to efficiently separate the true matches to the false hits .
There is a possibility that a promising candidate for a motif will pan out . That is , after searching the small matrix we will discover that most or all of the subsequences don’t match . In this case we will have to construct a new small matrix and continue the search with the next most promising motif . If the new small matrix has any overlap with the previous matrix , we reuse the calculated values rather than recalculating them .
Constructing these small matrices would be of limited utility if their total size added up to O(m2 ) . While this is possible in pathological cases , we can generally search a space much smaller in total size , and still guarantee that we have returned the true best K Motifs .
This , in essence , is the intuition behind our motif discovery algorithm . We will achieve speed up by : •
Searching a set of smaller matrices , whose total size is much less than the naïve O(m2 ) matrix .
• Within the smaller matrices , using ADM to prune away a large fraction of the search space .
We will concretely define our algorithm , which we call through Matrix of Motifs
( Enumeration
EMMA Approximation ) , in the next section . 4.1 The EMMA Algorithm
As before , we only discuss the algorithm for finding the 1Motif . The generalization of the algorithm to finding Kmotifs is obvious and omitted for brevity and clarity of presentation . The pseudocode for the algorithm is introduced in Table 4 . The line numbers in the table are used in the discussion of the algorithm that follows . awCh (
The algorithm begins by sliding a moving window of length n across the time series ( line 4 ) . The hash function h( ) ( line 5 ) , normalizes the time series , converts it to the symbolic representation and computes an address : ·icˆ , i,e . , ord(a ) = 1 , ord(b ) = 2 , and so on . The hash function computes an integer in the range 1 to wa , and a pointer to the subsequence is placed in the corresponding bucket ( line 6 ) .
)1)ˆ( c i Where ord( icˆ ) is the ordinal value of
+=
1 ) , ord w = i 1 a
1
(
, i
( 10 )
Algorithm Find 1 Motif Index(T,n,R,w,a )
1 . 2 . 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . 13 . 14 . 15 . 16 . 17 . 18 . 19 . 20 . 21 . 22 . 23 . 24 . 25 . 26 . best_motif_count = 0 ; best_motif_location = null ; finished = FALSE ; for i = 1 to length(T) n + 1 // Hash pointers hash_val = h(T[i:i+n 1],w,a ) ; // to subsequences append(bucket(hash_val).pointers , i ) ; end ; MPC = address(largest(bucket) ) ; // Find MPC neighborhood = bucket(MPC).pointers ; while not(finished ) for i = 1 to wa // Build neighborhood if MINDIST(MPC , bucket(i ) ) < R // around temp = bucket(i).pointers ; // the MPC neighborhood = append(neighborhood,temp ) end ; end ; // Search neighborhood for motifs [ motif_cntr,count]= ADM(T,neighborhood,R ) ; if count > largest_unexplored_neighborhood best_motif_location = motif_cntr ; best_motif_count = count ; finished = TRUE ; else // Create the next neighborhood to search MPC = address(largest_unexplored(bucket) ) ; neighborhood = bucket(MPC).pointers ; end ; end ;
Table 4 : The Find 1 Motif Index algorithm
At this point we have simply rearranged the data into a hash table with wa addresses , and a total of size O(m ) . This arrangement of the data has the advantage of approximately grouping similar subsequences ( more accurately , pointers to similar subsequences ) together . We can use this information as a heuristic for motif search , since if there is a truly overrepresented pattern in the time series , we should expect that most , if not all , copies of it hashed to the same location . We call the address with the most hits the Most Promising Candidate ( MPC ) ( line 8 ) . We can build a list of all subsequences that mapped to this address ( line 9 ) , but it is possible that some subsequences that hashed to different addresses are also within R of the subsequences contained in MPC . We can use the MINDIST function that we defined in Section 3.3 to determine which addresses could possibly contain such subsequences ( line 12 ) . All such subsequences are added to the list of subsequences that need to be examined in our small matrix ( line 14 ) . We call the contents of a promising address , together with the contents of all the addresses within a MINDIST of R to it , a neighborhood .
At this point we can pass the list of similar subsequences into the ADM subroutine ( line 17 ) . We will elucidate this algorithm later , in Section 42 For the moment we just note that the algorithm will return the best motif from the original MPC subset , with a count of the number of matching subsequences .
If we wish to implement the algorithm as an online algorithm , then at this point we can report the current motif as a tentative answer , before continuing the search . Such “ anytime ” behavior is very desirable in a data mining algorithm [ 7 ] .
Next , a simple test is performed . If the number of matches to the current best so far motif is greater than the largest unexplored neighborhood ( line 18 ) , we are done . We can record the best so far motif as the true best match ( line 19 ) , note the number of matching subsequences ( line 20 ) , and then abandon the search ( line 21 ) .
If the test fails , however , we must set the most promising candidate to be the next largest bucket ( line 23 ) , initialize the new neighborhood with the contents of the bucket ( line 24 ) , and loop back to line 11 , where the full neighborhood is discovered ( lines 13 and 14 ) and the search continues .
For simplicity the pseudocode for the algorithm ignores the following possible optimization , it is possible ( in fact , likely ) , that the neighborhood in one interaction will overlap with the neighborhood in the next . In this case , we can reuse the subset of calculated values from iteration to iteration . 4.2 The ADM Algorithm for the searching
The algorithm we use small neighborhood matrix is a minor modification of the Shasha and Wang ’s ADM algorithm [ 33 ] . The algorithm begins by pre computing an arbitrary set of distances . A matrix ADM is maintained , of which each entry [ i,j ] is either the exact distance between objects i and j ( ie those that are precomputed ) , or the lower bound for the distance between i and j . The algorithm utilizes the property of triangle inequality to find the lower bound distances . Details on how to construct the matrix ADM can be found in [ 33 ] .
After the matrix ADM is constructed , we scan the matrix and compute the actual distance between i and j if ADM[i,j ] is a lower bound that is smaller than R ( because the true distance might be greater than R ) , and omit it if it ’s greater than R . For each object , we keep track of the number of items smaller than R . Finally , the algorithm returns the bestmatching motif ( ie one with the most items within R ) , with a count of number of matching subsequences .
5 . Experimental Evaluation
We begin by showing some motifs discovered in a variety of time series . We deliberately consider time series with very different properties of noise , autocorrelation , stationarity etc . Figure 8 shows the 1 Motif discovered in various datasets , together with a much larger subsequence of the time series to give context . Although the subsequences are normalized [ 22 ] before the unnormalized subsequences for clarity . they match , we show to see testing if
We next turn our attention to evaluating the efficiency of the proposed algorithm . For simplicity we have only considered the problem of speeding up motif search when
120
110 100 90 80 70 0 2 1.9 1.8 1.7 1.6 1.5 1.4 1.3 0
10
20
30
40
40
80
120
160
200
Figure 8 : The 1 Motif discovered in various publicly available datasets . From top to bottom , “ Network ” and “ Burst ” . Details about the datasets are available from the UCR time series data mining archive . The small inset boxes show a subsequence of length 500 to give context to the motif dist dist force calls
= calls times brute number
Euclidean
Euclidean
( 11 ) of times the whole time series fits in main memory ( we intend to address efficient disk based algorithms in future work ) . So we can evaluate the efficiency of the proposed algorithm by simply considering the ratio of how many times the Euclidean distance function must be evaluated by EMMA , over the number of times it must be evaluated by the brute force algorithm described in Table 1 . EMMA efficiency This measure ignores the cost of building the hash table , but this needs be done only once ( even if the user wishes to try several values of R ) , and is in any case linear in m . number of
The efficiency depends on the value of R used in the experiments . In the pathological case of R = ¥ , only one “ small ” matrix would be created , but it would be O(m2 ) , even if we could fit such a large matrix in main memory , the only speed up would come from ADM algorithm . The other pathological case of R = 0 would make our algorithm behave very well , because only a few very small matrices would be created , and the triangular inequality pruning of ADM algorithm would be maximally efficient . Of course , neither of these scenarios is meaningful , the former would result in a Motif with every ( non trivial ) subsequence in the time series included , and the latter case would almost certainly result in no motif being found ( since we are dealing with real numbers ) .
In order to test with realistic values of R we will consider the efficiency achieved when using the values used to create the results shown in Figure 8 . The results are shown in Table 5 .
Network 0.0018
Dataset efficiency Table 5 : The efficiency of the EMMA algorithm on various datasets
Burst 0.0192
These results indicate a one to two order of magnitude speedup over the brute force algorithm .
6 . Related work to
To the best of our knowledge , the problem of finding repeated patterns in time series has not been addressed ( or even formulated ) in the literature .
Several researchers in data mining have addressed the discovery of reoccurring patterns in event streams [ 39 ] , although such data sources are often referred to as time series [ 38 ] . The critical difference is that event streams are sequentially ordered variables that are nominal ( have no natural ordering ) and thus these researchers are concerned with similar subsets , not similar subsequences . Research by Indyk et . al . [ 20 ] , has addressed the problem of finding representative trends in time series , this work is more similar in spirit to our work . However , they only consider trends , not more general patterns , and they only consider locally representative trends , not globally occurring motifs as in our approach .
While there has been enormous interest in efficiently locating previously known patterns in time series [ 1 , 2 , 3 , 8 , 13 , 19 , 22 , 23 , 24 , 27 , 29 , 32 , 35 , 40 ] , our focus on the discovery of previously unknown patterns is more similar to ( and was inspired by ) work in computational biology , which we briefly review below .
In refers the automatic the context of computational biology , “ pattern identification of discovery' ” biologically significant patterns ( or motifs ) by statistical methods . The underlying assumption is that biologically significant words show distinctive distribution patterns within the genomes of various organisms , and therefore they can be distinguished the evolutionary process , living organisms have accumulated certain biases toward or against some specific motifs in their genomes . For instance , highly recurring oligonucleotides are often found in correspondence to regulatory regions or protein binding sites of genes . rare oligonucleotide motifs may be discriminated against due to structural constraints of genomes or specific reservations for global transcription controls . the others . During
Vice versa , from
Pattern discovery in computational biology originated with the work of Rodger Staten [ 34 ] . Along this research line , a multitude of patterns have been variously characterized , and criteria , algorithms and software have been developed in correspondence . We mention a few representatives of this large family of methods , without claiming to be exhaustive : CONSENSUS [ 16 ] , GIBBS SAMPLER [ 26 ] , WINNOWER [ 30 ] , PROJECTION [ 36 ] , VERBUMCULUS [ 4 , 28 ] These methods have been studied from a rigorous statistical viewpoint ( see , eg , [ 31 ] for a review ) and also employed successfully in practice ( see , eg , [ 17 ] and references therein ) .
While there are literally hundreds of papers on discretizing ( symbolizing , tokenizing ) time series [ 2 , 3 , 9 , 13 , 19 , 25 , 27 ] ( see [ 10 ] for an extensive survey ) , and dozens of distance measures defined on these representations , none of the techniques allows a distance measure which lower bounds a distance measure defined on the original time series .
7 . Conclusions
We have formalized the problem of finding repeated patterns in time series , and introduced an algorithm to efficiently locate them1 . In addition , a minor contribution of this paper is to introduce the first discrete representation of time series that allows a lower bounding approximation of the Euclidean distance . This representation may be of independent interest to researchers who use symbolic representations for similarity search [ 3 , 19 , 25 , 27 , 29 ] , change point detection [ 13 ] , and extracting rules from time series [ 9 , 18 ] .
There are several directions in which we intend to extend this work . • As previously noted , we only considered the problem of speeding up main memory search . Techniques for dealing with large disk resident data are highly desirable [ 6 ] .
• On large datasets , the number of returned motifs may be intimidating ; we plan to investigate tools for visualizing and navigating the results of a motif search .
• Our motif search algorithm utilizes the Euclidean metric , and can be trivially modified to use any Minkowski metric [ 40 ] . However , recent work by several authors has suggested that the Euclidean may be inappropriate in some domains [ 21 , 29 ] . We hope to generalize our results to work with other more robust distance measures , such as Dynamic Time Warping [ 29 ] . It may be possible to extend our work to multidimensional time series ( ie , trajectories ) [ 37 ] .
•
8 . References
[ 1 ] Agrawal , R . , Faloutsos , C . & Swami , A . ( 1993 ) . Efficient similarity search in sequence databases . In proceedings of the 4th Int'l Conference on Foundations of Data Organization and Algorithms . Chicago , IL , Oct 13 15 . pp 69 84 .
[ 2 ] Agrawal , R . , Psaila , G . , Wimmers , E . L . & Zait , M . ( 1995 ) . Querying shapes of histories . In proceedings of the 21st Int'l Conference on Very Large Databases . Zurich , Switzerland , Sept 11 15 . pp 502 514 .
[ 3 ] André Jönsson , H . & Badal . D . ( 1997 ) . Using signature files for querying time series data . In proceedings of Principles of Data Mining and Knowledge Discovery , 1st European Symposium . Trondheim , Norway , Jun 24 27 . pp 211 220 .
[ 4 ] Apostolico , A . , Bock , M . E . & Lonardi , S . ( 2002 ) . Monotony of surprise and large scale quest for unusual words ( extended abstract ) . Myers , G . , Hannenhalli , S . , Istrail , S . , Pevzner , P . & Waterman , M . editors . In proceedings of the 6th Int’l Conference on Research in Computational Molecular Biology . Washington , DC , April 18 21 . pp 22 31 .
[ 5 ] Burkhard , W . A . & Keller , R . M . ( 1973 ) . Some approaches to best match file searching . Commun . ACM , April . Vol . 16(4 ) , pp 230 236 .
[ 6 ] Böhm , C . , Braunmüller , B . , Krebs , F . & Kriegel , H . P . ( 2002 ) . Epsilon grid order : An algorithm for the similarity join on massive high dimensional data . In proceedings of ACM SIGMOD Int . Conf . on Management of Data , Santa Barbara .
[ 7 ] Bradley , P . S . , Fayyad , U . M . & Reina , C . A . ( 1998 ) . Scaling clustering algorithms to large databases . In proceedings of the 4th Int’l Conference on Knowledge Discovery and Data Mining . New York , NY , Aug 27 31 . pp 9 15 .
[ 8 ] Chan , K . & Fu , A . W . ( 1999 ) . Efficient time series matching by wavelets . In proceedings of the 15th IEEE Int'l Conference
1 A slightly expanded version of this paper is available by emailing the authors . on Data Engineering . Sydney , Australia , Mar 23 26 . pp 126133 .
[ 9 ] Das , G . , Lin , K . , Mannila , H . , Renganathan , G . & Smyth , P . ( 1998 ) . Rule discovery from time series . In proceedings of the 4th Int'l Conference on Knowledge Discovery and Data Mining . New York , NY , Aug 27 31 . pp 16 22 .
[ 10 ] Daw , C . S . , Finney , C . E . A . & Tracy , E . R . ( 2001 ) . Symbolic analysis of experimental data . Review of Scientific Instruments . To appear .
[ 11 ] Durbin , R . , Eddy , S . , Krogh , A . & Mitchison , G . ( 1998 ) . Biological sequence analysis : probabilistic models of proteins and nucleic acids . Cambridge University Press .
[ 12 ] Fayyad , U . , Reina , C . & . Bradley . P ( 1998 ) . Initialization of iterative refinement clustering algorithms . In Proceedings of the 4th International Conference on Knowledge Discovery and Data Mining . New York , NY , Aug 27 31 . pp 194 198 .
[ 13 ] Ge , X . & Smyth , P . ( 2000 ) . Deformable Markov model templates for time series pattern matching . In proceedings of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . Boston , MA , Aug 20 23 . pp 8190 .
[ 14 ] Goutte , C . Toft , P . , Rostrup , E , Nielsen F.Å & Hansen LK ( 1999 ) . On clustering fMRI time series , NeuroImage , 9(3 ) : pp 298 310 .
[ 15 ] Hegland , M . , Clarke , W . & Kahn , M . ( 2002 ) . Mining the MACHO dataset , Computer Physics Communications , Vol 142(1 3 ) , December 15 . pp . 22 28 .
[ 16 ] Hertz , G . & Stormo , G . ( 1999 ) . Identifying DNA and protein patterns with statistically significant alignments of multiple sequences . Bioinformatics , Vol . 15 , pp 563 577 .
[ 17 ] van Helden , J . , Andre , B . , & Collado Vides , J . ( 1998 ) Extracting regulatory sites from the upstream region of the yeast genes by computational analysis of oligonucleotides . J . Mol . Biol . , Vol . 281 , pp 827 842 .
[ 18 ] Höppner , F . ( 2001 ) . Discovery of temporal patterns learning rules about the qualitative behavior of time series . In Proceedings of the 5th European Conference on Principles and Practice of Knowledge Discovery in Databases . Freiburg , Germany , pp 192 203 .
[ 19 ] Huang , Y . & Yu , P . S . ( 1999 ) . Adaptive query processing for time series data . In proceedings of the 5th Int'l Conference on Knowledge Discovery and Data Mining . San Diego , CA , Aug 15 18 . pp 282 286 .
[ 20 ] Indyk , P . , Koudas , N . & Muthukrishnan , S . ( 2000 ) . Identifying representative trends in massive time series data sets using sketches . In proceedings of the 26th Int'l Conference on Very Large Data Bases . Cairo , Egypt , Sept 10 14 . pp 363 372 .
[ 21 ] Kalpakis , K . , Gada , D . & Puttagunta , V . ( 2001 ) . Distance measures for effective clustering of ARIMA time series . In proceedings of the 2001 IEEE International Conference on Data Mining , San Jose , CA , Nov 29 Dec 2 . pp 273 280 .
[ 22 ] Keogh , E , . Chakrabarti , K , . Pazzani , M . & Mehrotra ( 2000 ) . Dimensionality reduction for fast similarity search in large time series databases . Journal of Knowledge and Information Systems . pp 263 286 .
[ 23 ] Keogh , E . , Chakrabarti , K . , Pazzani , M . & Mehrotra , S . ( 2001 ) . Locally adaptive dimensionality reduction for indexing large time series databases . In proceedings of ACM SIGMOD Conference on Management of Data . Santa Barbara , CA , May 21 24 . pp 151 162 .
[ 24 ] Keogh , E . & Pazzani , M . ( 1998 ) . An enhanced representation of time series which allows fast and accurate classification , clustering and relevance feedback . In proceedings of the 4th
Int'l Conference on Knowledge Discovery and Data Mining . New York , NY , Aug 27 31 . pp 239 241 .
[ 25 ] Koski , A . , Juhola , M . & Meriste , M . ( 1995 ) . Syntactic recognition of ECG signals by attributed finite automata . Pattern Recognition , 28 ( 12 ) , pp . 1927 1940 .
[ 26 ] Lawrence , CE , Altschul , S . F . , Boguski , M . S . , Liu , J . S . , Neuwald , A . F . & Wootton , J . C . ( 1993 ) . Detecting subtle sequence signals : A Gibbs sampling strategy for multiple alignment . Science , Oct . Vol . 262 , pp 208 214 .
[ 27 ] Li , C . , Yu , P . S . & Castelli , V . ( 1998 ) . MALM : a framework for mining sequence database at multiple abstraction levels . In proceedings of the 7th ACM CIKM International Conference on Information and Knowledge Management . Bethesda , MD . pp 267 272 .
[ 28 ] Lonardi , S . ( 2001 ) . Global Detectors of Unusual Words : to Pattern Design , Discovery in Biosequences . PhD thesis , Department of Computer Sciences , Purdue University , August , 2001 .
Implementation , and Applications
[ 29 ] Perng , C . , Wang , H . , Zhang , S . , & Parker , S . ( 2000 ) . Landmarks : a new model for similarity based pattern querying in time series databases . In proceedings of 16th International Conference on Data Engineering .
[ 30 ] Pevzner , P . A . & Sze , S . H . ( 2000 ) . Combinatorial approaches to finding subtle signals in DNA sequences . In proceedings of the 8th International Conference on Intelligent Systems for Molecular Biology . La Jolla , CA , Aug 19 23 . pp 269 278 .
[ 31 ] Reinert , G . , Schbath , S . & Waterman , M . S . ( 2000 ) . Probabilistic and statistical properties of words : An overview . J . Comput . Bio . , Vol . 7 , pp 1 46 .
[ 32 ] Roddick , J . F . , Hornsby , K . & Spiliopoulou , M . ( 2001 ) . An Updated Bibliography of Temporal , Spatial and SpatioIn Post Workshop Temporal Data Mining Research . Proceedings of the International Workshop on Temporal , Spatial and Spatio Temporal Data Mining . Berlin , Springer . Lecture Notes in Artificial Intelligence . 2007 . Roddick , J . F . and Hornsby , K . , Eds . 147 163 .
[ 33 ] Shasha , D . & Wang , T . ( 1990 ) . New techniques for best match retrieval . ACM Trans . on Information Systems , Vol . 8(2 ) . pp 140 158 .
[ 34 ] Staden , R . ( 1989 ) . Methods for discovering novel motifs in nucleic acid sequences . Comput . Appl . Biosci . , Vol . 5(5 ) . pp 293 298 .
[ 35 ] Struzik , Z . R . & Siebes , A . ( 1999 ) . Measuring time series similarity through large singular features revealed with wavelet transformation . In proceedings of the 10th International Workshop on Database & Expert Systems Applications . pp 162 166 .
[ 36 ] Tompa , M . & Buhler , J . ( 2001 ) . Finding motifs using random projections . In proceedings of the 5th Int’l Conference on Computational Molecular Biology . Montreal , Canada , Apr 22 25 . pp 67 74 .
[ 37 ] Vlachos , M . , Kollios , G . & Gunopulos , G . ( 2002 ) . Discovering similar multidimensional trajectories . In proceedings 18th International Conference on Data Engineering . pp 673 684 .
[ 38 ] Wang . W . , Yang , J . and Yu . , P . ( 2001 ) . Meta patterns : revealing hidden periodical patterns . In Proceedings of the 1st IEEE International Conference on Data Mining . pp . 550 557 . [ 39 ] Yang , J . , Yu , P . , Wang , W . and Han . J . ( 2002 ) . Mining long sequential patterns in a noisy environment . In proceedings SIGMOD International . Conference on Management of Data . Madison , WI .
[ 40 ] Yi , B , K . , & Faloutsos , C . ( 2000 ) . Fast time sequence indexing for arbitrary Lp norms . In proceedings of the 26st Intl Conference on Very Large Databases . pp 385 394 .
