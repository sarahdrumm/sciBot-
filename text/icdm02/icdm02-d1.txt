1
1 fweifan,haixun,psyug@usibmcom
IBM TJWatson Research Hawthorne , NY 10532
Wei Fan
1 2
Progressive Modeling
Haixun Wang
Philip S . Yu
Shaw hwa Lo
1
2
3
Salvatore Stolfo
3
Dept . of Statistics , Columbia Univ .
New York , NY 10027 slo@statscolumbiaedu
Dept . of Computer Science , Columbia Univ .
New York , NY 10027 sal@cscolumbiaedu
Abstract
Presently , inductive learning is still performed in a frustrating batch process . The user has little interaction with the system and no control over the final accuracy and training time . If the accuracy of the produced model is too low , all the computing resources are misspent . In this paper , we propose a progressive modeling framework . In progressive modeling , the learning algorithm estimates online both the accuracy of the final model and remaining training time . If the estimated accuracy is far below expectation , the user can terminate training prior to completion without wasting further resources . If the user chooses to complete the learning process , progressive modeling will compute a model with expected accuracy in expected time . We describe one implementation of progressive modeling using ensemble of classifiers .
Keywords : estimation
1 Introduction
Classification is one of the most popular and widely used data mining methods to extract useful information from databases . ISO/IEC is proposing an international standard to be finalized in August 2002 to include four data mining types into database systems ; these include association rules , clustering , regression , and classification . Presently , classification is performed in a “ capricious ” batch mode even for many well known commercial data mining software . An inductive learner is applied to the data ; before the model is completely computed and tested , the accuracy of the final model is not known . Yet , for many inductive learning algorithms , the actual training time is not known prior to learning either . It depends on not only the size of the data and the number of features , but also the combination of feature values that utimately determines the complexity of the model . During this possibly long waiting period , the only interaction between the user and program is to make sure that the program is still running and observe some status reports . If the final accuracy is too low after some long training time ,
Figure 1 . An interactive scenario where both accuracy and remaining training time are estimated all the computing resources become futile . The users either have to repeat the same process using other parameters of the same algorithm , choose a different feature subset , select a completely new algorithm or give up . There are many learners to choose from , a lot of parameters to select for each learner , countless ways to construct features , and exponential ways for feature selection . The unpredictable accuracy , long and hard to predict training time , and endless ways to run an experiment make data mining frustrating even for experts .
1.1 Example of Progressive Modeling
In this paper , we propose a “ progressive modeling ” concept to address the problems of batch mode learning . We illustrate the basic ideas through a cost sensitive example even though the concept is applicable to both cost sensitive and traditional accuracy based problems .
We use a charity donation dataset ( KDDCup 1998 ) that chooses a subset of population to send campaign letters . The cost of a campaign letter is $068 It is only beneficial to send a letter if the solicited person will donate at least $068 As soon as learning starts , the framework begins to compute intermediate models , and report current accuracy as well as estimated final accuracy on a hold out validation set and estimated remaining training time . For costsensitive problem , accuracy is measured in benefits such as dollar amounts . We use the term accuracy to mean traditional accuracy and benefits interchangeably where the meaning is clear from the context . Figure 1 shows a snap
1 that records the benefit received by predicting an example fiers , we use statistical techniques to estimate both the ac classifiers . Their estimates are denoted in lower case , ie ,
Expected Benefit :
Combining individual expected benefits , we have making is a special case of cost sensitive problem , we only discuss the algorithm in the context of cost sensitive deci
( 2 ) We then use optimal decision policy to choose the class label with the maximal expected benefit of class`i to be an instance of class`j . For cost insensitive ( or accuracy based ) problems,8i;b[`i;`i℄=1 and8i6= j;b[`i;`j℄=0 . Since traditional accuracy based decision sion making . Using benefit matrixb[:::℄ , each modelCj will generate an expected benefit or riskej `ijx for every possible class`i . ej `ijx =X`i0b[`i0;`i℄ j `i0jx Assume that we have trainedk modelsfC1;:::;Ckg . Average Expected Benefit : Ek `ijx = jej `ijx k Optimal Decision : k x = argmax`iEk `ijx Assuming that` x is the true label ofx , the accuracy of the ensemble withk classifiers is Ak=Xx2Svb[` x ; k x ℄ For accuracy based problems,Ak is usually normalized into percentage using the size of the validation setjSvj . For costalso have the total time to trainC1 toCk . Tk= the total time to trainfC1;:::;Ckg Next , based on the performance ofk base classicuracy and training time of the ensemble with models . We first summarize some notations.A ;T and complete ensemble , and the remaining training time afterk a , and . An estimate is a range with a mean and by a bar ( ) and the standard deviation is represented by a sigma ( ) . Additionally,d is standard error or the standard The accuracy estimate is based on the probability that`i is the predicted label by the ensemble of classifiers for are the true values to estimate . Respectively , they are the accuracy of the complete ensemble , the training time of the sensitive problems , it is customary to use some units to measure benefits such as dollar amounts . Besides accuracy , we standard deviation . The mean of a symbol is represented
( 1 )
( 3 )
( 4 )
( 5 ) deviation of a sample mean .
2.2 Estimating Accuracy be $14289.5 100.3 with at least 99.7 % confidence . The estimated to be 5.40 0.70 minutes with at least 99.7 % con shot of the new learning process . It displays that the accuracy on the hold out validation set ( total donated charity minus the cost of mailing to both donors and non donors ) for the algorithm using the current intermediate model is $128405 The accuracy of the complete model on the holdout validation set when learning completes is estimated to additional training time to generate the complete model is fidence . This information continuously refreshes whenever a new intermediate model is produced , until the user explicitly terminates the learning or the complete model is generated .
The user may stop the learning process mainly due to the following reasons i ) intermediate model has enough accuracy , ii ) its accuracy is not significantly different from that of the complete model , iii ) the estimated accuracy of the complete model is too low , or iv ) the training time is unexpectedly long . For the example shown in Figure 1 , we would continue , since it is worthwhile to spend 6 more minutes to receive at least $1400 more donation with at least 99.7 % confidence . In this example , we illustrated progressive modeling applied to cost sensitive learning . For costinsensitive learning , the algorithm reports traditional accuracy in place of dollar amounts .
Progressive modeling is significantly more useful than a batch mode learning process , especially for very large dataset . The user can easily experiment with different algorithms , parameters , and feature selections without waiting for a long time for a failure result .
2 Our Approach
2.1 Main Algorithm
We propose an implementation of progressive modeling based on ensembles of classifiers that can be applied to several inductive learning algorithms . The basic idea is to generate a small number of base classifiers to estimate the performance of the entire ensemble when all base classifiers are produced .
Assume that a training setS is partitioned into disjoint subsetsSj with equal size . When the distribution of the or use random sampling without replacement to drawSj . A base level modelCj is trained fromSj . Given an examplex from a validation setSv ( it can be a different dataset or the training set),Cj outputs probabilities for all possible class labels thatx may be an instance of , ie , j `ijx for class label`i . Details on how to calculate j `ijx can be found in [ 5].In addition , we have a benefit matrixb[`i;`j℄ dataset is uniform , each subset can be taken sequentially . Otherwise , we can either completely “ shuffle ” the dataset
2
( 8 )
( 7 )
( 6 )
Since each example is independent , according to multinomial form of central limit theorem ( CLT ) , the total benefit examplex . f x =`ig the probability that `i is the prediction by the ensemble of size Since each class label`i has a probability to be the predicted class , and predicting an instance of class` x as`i receives a benefitb[` x ;`i℄ , the expected accuracy received forx by predicting with base models is ff x =X`ib[` x ;`i℄ f x =`ig with standard deviation of ff x . To calculate the expected accuracy on the validation setSv , we sum up the expected accuracy on each examplex a =Xx2Svff x of the complete model with models is a normal distribua = Xx2Sv ff x 2 sembleA falls within the following range : With confidence ,A 2a a When =3 , the confidence is approximately 997 % Next we discuss how to derive f x =`ig . E `ijx are known , there is only one label , x whose f x =`ig will be 1 , and all other labels will have probability equal to 0 . However,E `ijx are not known , we can only use its estimateEk `ijx measured fromk classifiers to derive f x =`ig . From random sampling theory [ 2],Ek `ijx is an unbiased estimate ofE `ijx 1 f wheref=k d Ek `ijx = Ek `ijx k According to central limit thereon , the true valueE `ijx falls within a normal distribution with mean value of= Ek `ijx and standard deviation of=d Ek `ijx . If Ek `ijx is high , it is more likely forE `ijx to be high , and consequently , for f k x =`ig to be high . For the bels , and compute naive probability 0f x =`ig , Assuming that is an approximate of max`iE `ijx
Using confidence intervals , the accuracy of the complete en time being , we ignore correlation among different class la tion with mean value of Eq[8 ] and standard deviation of with standard error of
, the
( 11 )
( 10 )
( 9 )
If
.
( 12 ) to compensate the error in standard error estimation , we use to the probabilities for other class labels to be the predicted label . When it is more likely for other class labels to be the label . A common method to take correlation into account is to use normalization ,
The reason not to use the maximum itself is that if the associated label is not the predicted label of the complete model , the probability estimate for the true predicted label may be too low . area in the range of [ ; 1 ) is the probability 0f x = `i . 0f x =`ig=Z 1 " 12 z !2#dz 1 2 exp where=d Ek `ijx and=Ek `ijx . Whenk30 , Student t distribution withdf=k . We use the average of the two largestEk `ijx ’s to approximate max`iE `ijx On the other hand , f k x =`i is inversely related predicted label , it will be less likely for`i to be the predicted f x =`ig= 0f x =`ig j 0f x =`jg Thus , we have derived f x =`ig in order to estifor the sampledk models are1 tok . Their mean and standard deviation are and . Then the total training time of classifiers is estimated as With confidence ,T 2 , where 1 f = and = k To find out remaining training time , we simply deduct k from Eq[14 ] . With confidence , 2 , where =  k and = ( 15 ) of the current model ( Ak ) , and the estimated accuracy of the complete model ( a ) as well as estimated remaining training time ( ) . From these statistics , the user decides to
We request the first random sample from the database and train the first model . Then it requests the second random sample and train the second model . From this point on , the user will be updated with estimated accuracy , remaining training time and confidence levels . We have the accuracy
2.3 Progressive Modeling mate the accuracy in Eq[7 ] .
Estimating Training Time Assuming that the training time
( 13 )
( 14 ) continue or terminate . The user normally terminates learning if one of the following Stopping Criteria are met :
3 end end end
Data
Result begin
: benefit matrixb[`i;`j℄ , training setS , validation setSv and :k classifiers partitionS into disjoint subsets of equal size fS1;:::;S g ; trainC1 fromS1 , and1 is the training time ; k 2 ; whilek do trainCk fromSk andk is the training time ; forx2Sv do calculate f =`ig ( Eq[13] ) ; calculateff x and its standard deviation ff x ( Eq[7] ) ; estimate accuracya ( Eq[8 ] and Eq[9 ] ) and remaining training time ( Eq[15] ) ; ifa and satisfy stopping criteria then returnC1;:::;Ck ; endk k 1 ; returnC1;:::;Ck ; ffl The accuracy of the current model is sufficiently high . Assume thatA is the target accuracy . ffl The accuracy of the current model is sufficiently close Formally , a ffl . ffl The estimated accuracy of the final model is too low  a a A , ffl The estimated training time is too long , the user decides to abort . Formally , assume thatT is the target     *T , cancel the Computing base models sequentially has complexity of f . Both the average and standard deviation
Algorithm 1 : Progressive Modeling Based on Averaging Ensemble to that of the complete model . There won’t be significant improvement by training the model to the end .
As a summary of all the important steps of progressive modeling , the complete algorithm is outlined in Algorithm 1 . to be useful . Formally , if stop the learning process . training time , if learning .
2.4 Efficiency can be incrementally updated linearly in the number of examples .
3 Experiment
There are two main issues the accuracy of the ensemble and the precision of estimation . The accuracy and training time of a single model computed from the entire dataset is regarded as the baseline . To study the precision of the estimation methods , we compare the upper and lower error bounds of an estimated value to its true value . We have carefully selected three datasets . They are from real world applications and significant in size . We use each dataset both as a traditional problem that maximizes traditional accuracy as well as a cost sensitive problem that maximizes total benefits . As a cost sensitive problem , the selected datasets differ in the way how the benefit matrices are obtained . ing a charitable donation from an individualx is $0.68 , and the best estimate of the amount thatx will donate isY x . predict:d a e actuald a e actual:d a e predictd a e
The first one is the donation dataset that first appeared in KDDCUP’98 competition . Suppose that the cost of request
Its benefit matrix is :
3.1 Datasets
Y(x ) $068
$0.68
0 0
As a cost sensitive problem , the total benefit is the total amount of received charity minus the cost of mailing . The data has already been divided into a training set and a test set . The training set consists of 95412 records for which it is known whether or not the person made a donation and how much the donation was . The test set contains 96367 records for which similar donation information was not published until after the KDD’98 competition . We used the standard training/test set splits to compare with previous results . The feature subsets were based on the KDD’98 winning submission . To estimate the donation amount , we employed the multiple linear regression method . As suggested in [ 10 ] , to avoid over estimation , we only used those contributions between $0 and $50 . investigate a fraud andy x is the transaction amount , the predict:f a d actualf a d actual:f a d predictf a d y x $90
The second data set is a credit card fraud detection problem . Assuming that there is an overhead $90 to dispute and following is the benefit matrix :
$90
0 0
As a cost sensitive problem , the total benefit is the sum of recovered frauds minus investigation costs . The dataset was sampled from a one year period and contains a total of 5M transaction records . The features record the time of the transaction , merchant type , merchant location , and past payment and transaction history summary . We use data of the last month as test data ( 40038 examples ) and data of pre
4 vious months as training data ( 406009 examples ) . Details about this dataset can be found in [ 9 ] .
The third dataset is the adult dataset from UCI repository . It is a widely used dataset to compare different algorithms on traditional accuracy . For cost sensitive studies , we artificially associate a benefit of $2 to class label F and a benefit of $1 to class label N , as summarized below : predict F predict N actual F actual N
$2 0
0 $1
3.2 Experimental Setup
We use the natural split of training and test sets , so the results can be easily duplicated . The training set contains 32561 entries and the test set contains 16281 records .
2f8;16;32;64;128;256g . The accuracy and estimated
We have selected three learning algorithms , decision tree learner C4.5 , rule builder RIPPER , and naive Bayes learner . We range of partitions , a wide chosen have accuracy is the test dataset .
3.3 Accuracy
For the multiple model , we first discuss the results when the complete multiple model is fully constructed , then present the results of partial multiple model . Each result is the av
Since we study the capability of the new framework for both traditional accuracy based problems as well as costsensitive problems , each dataset is treated both as a traditional and cost sensitive problem . The baseline traditional accuracy and total benefits of the batch mode single model are shown in the two columns under accuracy for traditional accuracy based problem and benefits for cost sensitive problem respectively in Table 1 . These results are the baseline that the multiple model should achieve.1 erage of different multiple models with ranging from 2 been increased by approximately $7,000 $10,000 ; the accuracy has been increased by approximately 1 % 3 % . For that always predict:d a e . This succinct rule will not find any donor to 256 . In Table 2 , the results are shown in two columns under accuracy and benefit . As we compare the respective results in Tables 1 and 2 , the multiple model consistently and significantly beat the accuracy of the single model for all three datasets using all three different inductive learners . The most significant increase in both accuracy and total benefits is for the credit card dataset . The total benefits have
1Please note that we experimented with different parameters for RIPPER on the donation dataset . However , the most specific rule produced by RIPPER contains only one rule that covers 6 donors and one default rule the KDDCUP’98 donation dataset , the total benefit has been increased by $1400 for C4.5 and $250 for NB . and will not receive any donations . However , RIPPER performs reasonably well for the credit card and adult datasets .
5
Donation Credit Card Adult
Donation Credit Card Adult
C4.5
Accuracy Based
Cost sensitive accuracy 94.94 % 87.77 % 84.38 %
RIPPER benefit $13292.7 $733980 $16443
Accuracy Based
Cost sensitive accuracy 94.94 % 90.14 % 84.84 %
NB benefit
$0
$712541 $19725
Accuracy Based
Cost sensitive accuracy 94.94 % 85.46 % 82.86 % benefit $13928 $704285 $16269
Donation Credit Card Adult
We next study the trends of accuracy when the number
Table 1 . Baseline accuracy and total benefits increases , both the accuracy and total tendency show a slow and total benefits for the credit card datasets , and the total benefits for the donation dataset with increasing number of we can see clearly that for the credit card dataset , the multiple model consistently and significantly improve both the accuracy and total benefits over the single model by at least 1 % in accuracy and $40000 in total benefits for all choices of partitions increases . In Figure 2 , we plot the accuracy partitions . C4.5 was the base learner for this study . As of . For the donation dataset , the multiple model boosts the total benefits by at least $1400 . Nonetheless , when decreasing trend . It would be expected that when is exthese include the accuracy of the current modelAk , the true accuracy of the complete modelA and the estimate of the true accuracya with a . If the true value falls within confidence ,A 2a a . Quantitatively , we say an estimate is good if the error bound ( ) is within 5 % k=20 . In Table 3 , we show the average of estimated tions =f8;:::;256g . The true valueA all fall within
The current and estimated final accuracy are continuously updated and reported to the user . The user can terminate the learning based on these statistics . As a summary , tremely large , the results will eventually fall below the baseline . the error range of the estimate with high confidence and the error range is small , the estimate is good . Formally , with of the mean and the confidence is at least 99 % . We chose accuracy of multiple models with different number of parti
3.4 Accuracy Estimation donation dataset with respect to
Figure 2 . Plots of accuracy and total benefits for credit card datasets , and plot of total benefits for
Credit Card Data Accuracy Credit Card Data Accuracy Baseline
0.94
0.92
900000
850000 s t i f
Credit Card Data Total Benefits Credit Card Data Baseline
16000
15500
15000 s t i f
14500
Donation Data Total Benefits Donation Data Baseline y c a r u c c A
0.9
0.88
0.86 e n e B
800000 t l
50
50
250
200
150 a o T
750000
700000
NB
C4.5 benefit benefit accuracy accuracy
Cost sensitive
Cost sensitive
Accuracy Based
100 Number of Partitions
RIPPER Accuracy Based
Donation Credit Card Adult
Donation Credit Card Adult
94.94 0 % 90.37 0.5 % 85.6 0.6 % 94.94 0 % 91.46 0:6 % 86.1 0.4 % 94.94 0 % 88.64 0.3 % 84.94 0.3 %
$14702.9 458 $804964 32250 $16435 150 $0 0 $815612 34730 $19875 390 $14282 530 $798943 23557 $16169 60 up to =256 for all three datasets as shown in Figure 3 . k becomes big . As shown clearly in all three plots , the error bound decreases exponentially . Whenk exceeds 50 ( ap
There are four curves in each plot . The one on the very top and the one on the very bottom are the upper and lower error bounds . The current benefits and estimated total benefits are within the higher and lower error bounds . Current benefits and estimated total benefits are very close especially when
Table 2 . Average accuracy and total benefits by complete multiple model with different number of partitions .
To see how quickly the error range converges with increasing sample size , we draw the entire process to sample
Donation Credit Card Adult the error range .
Accuracy Based
Cost sensitive accuracy benefit proximately 20 % of 256 ) , the error range is already within 5 % of the total benefits of the complete model . If we are satisfied with the accuracy of the current model , we can dis
100 Number of partitions
150 e n e B l t a o T
14000
13500
13000
$0
50
C4.5
Adult
Adult
86.1 %
150
200
250
85.6 %
200
250
12500
12000
$16435
91.46 %
90.37 %
True Val
$815612
$804964
Estimate
Estimate
Estimate
Estimate
Donation
Donation
RIPPER
Credit Card
Credit Card
Cost sensitive
Cost sensitive
Accuracy Based
Accuracy Based
True Val 94.94 %
True Val 94.94 %
100 Number of Partitions
True Val $14702.9
94.94 % 0 % 90.08 % 1.5 % 85.3 % 1.4 % 94.94 % 0 % 91.24 % 0.9 % 85.9 % 1.3 % 94.94 % 0 % 89.01 % 1.2 % 85.3 % 1.5 %
$14913 612 $799876 3212 $16255 142 $0 0 $820012 3742 $19668 258 $14382 120 $797749 4523 $16234 134 partitions , whenk>30 , the current model is usuWhen becomes too big , each dataset becomes trivial and ally within 5 % error range of total benefits by the complete model . For traditional accuracy , the current model is usually within 1 % error bound of the accuracy by the complete model ( detailed results not shown ) . continue the learning process and return the current model . For the three datasets under study and different number of
Table 3 . True accuracy and estimated accuracy .
Next , we discuss an experiment under extreme situations .
True Val 94.94 %
True Val $14282
Accuracy Based
Cost sensitive
Credit Card
Donation
Estimate
Estimate
$798943
84.94 %
88.64 %
$19875
$16169
Adult
NB will not be able to produce an effective model . If the esti
6
18000
16000
14000
12000
10000
8000 s t i f e n e B l a t o T l a t o T
17000
16500
16000
15500
1e+06
800000 s t i f e n e B
600000
1.2e+06
Adult Current Benefits Adult Estimated Total Benefits Upper Err Bound Lower Err Bound
Donation Current Benefits Donation Estimated Final Benefits Lower Err Bound Upper Err Bound
Credit Card Current Benefits Credit Card Estimated Final Benefits Upper Err Bound Lower Err Bound
Figure 3 . Current benefits and estimated final benefits when sampling sizek increases up to =256 for all three datasets . The error range is3 a for 99.7 % confidence . complete model , the user can choose a smaller . We partitioned all three dataset into =1024 partitions . For the servation is that after the sampling sizek exceeds around as small as 25 ( out of =1024 or 0.5% ) , the error bound adult dataset , each partition contains only 32 examples but there are 15 attributes . The estimation results are shown in Figure 4 . The first observation is that the total benefits for donation and adult are much lower than the baseline . This is obviously due to the trivial size of each data partition . The total benefits for the credit card dataset is $750,000 , which is still higher than the baseline of $733980 . The second ob
Online aggregation has been well studied in database community . It estimates the result of an aggregate query such as avg(AGE ) during query processing . One of the most noteworthy work is due to [ 7 ] , which provides an interactive and accurate method to estimate the result of aggregation . One of the earliest work to use data reduction techniques to scale up inductive learning is due to Chan [ 1 ] , in which he builds a tree of classifiers . In BOAT [ 6 ] , Gehrke et al build multiple bootstrapped trees in memory to examine the splitting conditions of a coarse tree . There has been several advances in cost sensitive learning [ 3 ] . MetaCost [ 4 ] takes advantage of purposeful mis labels to maximize total benefits . In [ 8 ] , Provost and Fawcett study the problem on how to make optimal decision when cost is not known precisely . mation methods can effectively detect the inaccuracy of the
4 Related Work
50 200 Sampling Size ( Number of Classifiers )
100
200 Sampling Size ( Number of Classifiers )
150
100
200 Sampling Size ( Number of Classifiers )
150
200000
400000 s t i f e n e B
13500
13000
15000
14500
12500
14000
100
150 l a t o T
250
250
50
0
50
250 becomes small enough , implying that the total benefits by the complete model is very unlikely ( 99.7 % confidence ) to increase . At this point , the user should cancel the learning for both donation and adult datasets . The reason for the “ bumps ” in the adult dataset plot is that each dataset is too small and most decision trees will always predict N most of the time . At the beginning of the sampling , there is no variations or all the trees make the same predictions ; when more trees are introduced , it starts to have some diversities . However , the absolute value of the bumps are less than $50 as compared to $12435 . training time of the multiple model withk=30 classifiers plus the time to classify the test datak times . We then When =256 , using the multiple model not only provide computed ratio of the recorded time of the single and multiple models , called serial improvement . It is the number of times that training the multiple model is faster than training the single model . In Figure 5 , we plot the serial improvement for all three datasets using C4.5 as the base learner .
We recorded both the training time of the batch mode single model plus the time to classify the test data , and the
3.5 Training Efficiency higher accuracy , but the training time is also 80 times faster for credit card , 25 times faster for both adult and donation .
7
5 Conclusion
In this paper , we have demonstrated the need for a progressive and interactive approach of inductive learning where the users can have full control of the learning process . An important feature is the ability to estimate the accuracy of complete model and remaining training time . We have implemented a progressive modeling framework based on averaging ensembles and statistical techniques . One important result of this paper is the derivation of error bounds used in performance estimation . We empirically evaluated our approaches using several inductive learning algorithms . First , we find that the accuracy and training time by the progressive modeling framework maintain or greatly improve over batch mode learning . Second the precision of estimation is high . The error bound is within 5 % of the true value when the model is approximately 25 % 30 % complete .
Based on our studies , we conclude that progressive modeling based on ensemble of classifiers provide an effective
Figure 4 . Current benefits and estimated final estimates when sampling sizek increases up to =1024 for all three datasets . To enlarge the plots whenk is small , we only plot up tok=50 . The error range is3 a for 99.7 % confidence .
25000
20000
15000
10000
5000
0
5000
10000
15000
20000
5
Donation Current Benefits Donation Estimated Final Benefits Upper Err Bound Lower Err Bound
10
15
40 Sampling Size ( Number of Classifiers )
20
25
30
35 s t i f e n e B l a t o T
1e+06
950000
900000
850000
800000
750000
700000
650000
600000
550000
500000
450000
5
45
50
Credit Card Current Benefits Credit Card Estimated Current Benefits Upper Err Bound Lower Err Bound
10
15
40 Sampling Size ( Number of Classifiers )
20
25
30
35 s t i f e n e B l a t o T
12600
12550
12500
12450
12400
12350
12300
45
50
5
10
Adult Current Benefits Adult Estimated Final Benefits Lower Err Bound Upper Err Bound
15
40 Sampling Size ( Number of Classifiers )
20
25
30
35
45
50
Figure 5 . Serial improvement for all three datasets when early stopping is used
30
25
20
15
10
5
0
Donation Data Early Stop Serial Imp
50
100 Number of Partitions
150
200
250 t n e m e v o r p m
I l a i r e S
100
80
60
40
20
0
Credit Card Fraud Data Early Stop Serial Imp
50
100 Number of Partitions
150
200
250 t n e m e v o r p m
I l a i r e S
30
25
20
15
10
5
0
Adult Data Early Stop Serial Imp
50
100 Number of Partitions
150
200
250 s t i f e n e B l a t o T t n e m e v o r p m
I l a i r e S solution to the frustrating process of batch mode learning .
References
[ 1 ] P . Chan . An Extensible Meta learning Approach for Scalable and Accurate Inductive Learning . PhD thesis , Columbia University , Oct 1996 .
[ 2 ] W . G . Cochran . Sampling Techniques . John Wiley and
Sons , 1977 .
[ 3 ] T . Dietterich , D . Margineatu , F . Provost , and P . TurCost Sensitive Learning Workshop ney , editors . ( ICML 00 ) , 2000 .
[ 4 ] P . Domingos . MetaCost : a general method for making classifiers cost sensitive . In Proceedings of Fifth International Conference on Knowledge Discovery and Data Mining ( KDD 99 ) , San Diego , California , 1999 .
[ 5 ] W . Fan , H . Wang , P . S . Yu , and S . Stolfo . A framework for scalable cost sensitive learning based on combining probabilities and benefits . In Second SIAM International Conference on Data Mining ( SDM2002 ) , April 2002 .
[ 6 ] J . Gehrke , V . Ganti , R . Ramakrishnan , and W Y Loh . BOAT optimistic decision tree construction . In Proceedings of ACM SIGMOD International Conference on Management of Data ( SIGMOD 1999 ) , 1999 .
[ 7 ] J . M . Hellerstein , P . J . Haas , and H . J . Wang . Online aggregation . In Proceedings of ACM SIGMOD International Conference on Management of Data ( SIGMOD’97 ) , 1997 .
[ 8 ] F . Provost and T . Fawcett . Robust classification for imprecise environments . Machine Learning , 42:203– 231 , 2000 .
[ 9 ] S . Stolfo , W . Fan , W . Lee , A . Prodromidis , and P . Chan . Credit card fraud detection using metalearning : Issues and initial results . In AAAI 97 Workshop on Fraud Detection and Risk Management , 1997 .
[ 10 ] B . Zadrozny and C . Elkan . Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers . In Proceedings of Eighteenth International Conference on Machine Learning ( ICML’2001 ) , 2001 .
8
