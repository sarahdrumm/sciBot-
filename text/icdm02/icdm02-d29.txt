Adaptive dimension reduction for clustering high dimensional data
Chris Dinga , Xiaofeng Hea , Hongyuan Zhab and Horst D . Simona a NERSC Division , Lawrence Berkeley National Laboratory
University of California , Berkeley , CA 94720 b Department of Computer Science and Engineering
Pennsylvania State University , University Park , PA 16802
{chqding,xhe,hdsimon}@lbl.gov , zha@csepsuedu
Abstract
It is well known that for high dimensional data clustering , standard algorithms such as EM and the K means are often trapped in local minimum . Many initialization methods were proposed to tackle this problem , but with only limited success . In this paper we propose a new approach to resolve this problem by repeated dimension reductions such that K means or EM are performed only in very low dimensions . Cluster membership is utilized as a bridge between the reduced dimensional subspace and the original space , providing flexibility and ease of implementation . Clustering analysis performed on highly overlapped Gaussians , DNA gene expression profiles and internet newsgroups demonstrate the effectiveness of the proposed algorithm .
1
Introduction
In many application areas , such as information retrieval , image processing , computational biology and global climate research , analysis of high dimensional datasets is frequently encountered . For example , in text processing , typical dimension of a word vector is of the size of the vocabulary of a document collection and tens of thousands of words/phrases are used routinely ; in molecular biology , human gene expression profile analysis typically involves thousands of genes ; and in image processing , a typical 2 dim image has 1282 = 16,384 pixels or dimensions .
Developing effective clustering methods to handle high dimensional dataset is a challenging problem . Popular clustering methods such as the K means and EM methods suffer from the well known local minima problem : as iterations proceed , they are often trapped in the local minima in the configuration space , due to the greedy nature of these algorithms . In high dimensional space , the equi potential ( cost function ) surface is very rugged . The iterations almost always get trapped somewhere close to the initial starting configuration . In other words , it is difficult to sample through a large configuration ( parameter ) space . The conventional approach is to do a large number of runs with random initial starts and pick up the best one as the result [ 24 , 26 ] . Besides random starts , there are a number of initialization methods , most of which concentrate on how to intelligently choose the starting configurations ( the K centers ) in order to be as close to the global minima as possible [ 5 , 25 , 22 , 17 ] . However , these approaches are limited by the inherent difficulty of finding global minima in high dimensional space in the first place . Monte Carlo methods are also used [ 28 ] .
In this paper , we propose a new approach to solve this problem . Our approach utilizes the idea of dimension reduction . Dimension reduction is often used in clustering , classification , and many other machine learning and data mining applications . It usually retains the most important dimensions ( attributes ) , removes the noisy dimensions ( irrelevant attributes ) and reduces computational cost .
In most applications , dimension reduction is carried out as a preprocessing step . The selection of the dimensions using principal component analysis ( PCA ) [ 20 , 14 ] through singular value decomposition ( SVD ) [ 15 ] is a popular approach for numerical attributes . In information retrieval , latent semantic indexing uses SVD to project textual documents represented as document vectors [ 7 ] ; SVD is shown to be the optimal solution for a probablistic model for document/word occurrence [ 12 ] . Random projections to subspaces have also been used [ 13 , 6 ] .
In all those applications , however , once the dimensions are selected , they stay fixed during the entire clustering process . The dimension reduction process is de
1 coupled from the clustering process . If the data distribution is far from Gaussian , for example , the dimensions selected using PCA may deviate substantially from the optimal .
Here ( i ) we approach dimension reduction as a dynamic process that should be adaptively adjusted and integrated with the clustering process ; ( ii ) we make effective use of cluster membership as the bridge connecting the clusters defined in the reduced dimensional space ( subspace ) and those defined in the full dimensional space ; ( iii ) using this connection , clusters are discovered in the low dimensional subspace to avoid the curse of dimensionality [ 27 ] and are adaptively re adjusted in the full dimension space for global optimality . This process is repeated until convergence .
In this paper we focus on the K means and EM algorithms using the mixture model of spherical Gaussian components . Using marginal distributions , the gaussian mixtures retain identical model parameters in reduced low dimensional subspace as in the original high dimensional space , providing a theoretical justification for dimension reduction . The objective function for the K means has the same property .
K centroid classification on text are studied via dimension reduction in [ 18 ] , where K centroids are used to define the subspace projection . Dimension reduction in text processing has been extensively studied [ 4 , 12 , 9 , 21 ] . All of above studies use dimension reduction as preprocessing ; while in our approach , dimension reduction is performed adaptively .
In this paper , we consider projection methods in which the new projected dimensions are linear combination of old dimensions . Optimal selection of a subset of existing dimensions ( attributes ) is a substantially different approach . Selection of a subset of attributes in the context of clustering are studied in [ 2 , 1 ] . In the context of classification , subset selection has been extensively studied [ 19 ] .
2 Effective Dimension for
Clustering
Our approach is to perform clustering in low dimensional subspaces . EM , in essence , is fitting a density functional form and is sensitive to local density variations . In the much reduced dimension subspaces , we have a much smoother density [ 27 ] , therefore reducing the chances of being trapped in the local minima .
We may interpret the low dimensional subspace as containing the relevant attributes ( linear combinations of coordinates ) . What is the dimensionality r of the reduced dimension subspace for clustering ? We argue that r ≤ K − 1 based on linear discriminant analysis : Given two gaussian distributions with means µ1 , µ2 , and pooled covariance matrix Σ in d dim space , a point x is classified to belong to class 1 or 2 depending upon xT Σ−1(µ1 − µ2 ) ≥ threshold
For spherical Gaussian , Σ = σ2I ( I is the identity matrix ) , thus the d − 1 subspace perpendicular to the direction µ1 − µ2 does not enter the consideration . The effective dimension for 2 clusters is 1 . For K > 2 , we may consider all pairs of two class classifications , and the dimensions perpendicular to all µi − µj directions ( i 6= j ) are irrelevant . Thus the effective clustering dimensions for the K spherical gaussians are spanned by the K centers µ1 , · · · , µK , for which r = K − 1 . We call the relevant dimensions passing through all K centers the r dim subspace . This conclusion is essentially a geometric point of view . If Euclidean distances are the main factors in clustering , the dimensions perpendicular to the relevant subspace are clearly irrelevant .
The effective dimensionality of the relevant subspace could be less than K − 1 . This happens when the K cluster centers lie in a subspace with dimension r < K − 1 . For example , there could be 5 clusters with all their centroids lie on a 2 dim plane . In summary , the effective dimension for clustering is r ≤ K − 1 .
3 EM in relevant subspace
Our algorithm can be easily and naturally incorporated into Expectation Maximization ( EM ) algorithm [ 8 , 23 ] applied to spherical Gaussian mixtures . The idea is that the irrelevant dimensions can be integrated out , and the resulting marginal distribution follows the same Gaussian mixture functional form . Then we can freely move between the reduced dimension subspace and the original space . In this approach , cluster membership information ( posterior probabilities of the indicator variables ) plays a critical role . Knowing them in the reduceddimension subspace we can directly infer the centers in the original space . We assume the following mixture model p(x ) = π1gd
1(x − µ1 ) + · · · + πKgd
K(x − µK )
( 1 ) where each component is a spherical Gaussian distribution , gd k(x ) =
1
( √2πσk)d exp(−kx − µkk2
2σ2 k
)
2 and x , µk are vectors in d dim space . We denote it as N ( d)(µk , σk ) . Note that the spherical gaussian function has two invariant properties that will be important later on : ( i ) they are invariant under any orthogonal coordinate rotation operation R : x → Rx : k(x|θ ) gd k(Rx|Rθ ) = gd where R is a d × d orthonormal matrix and θ = {µ1 , · · · , µK , σ1 , · · · , σK} are the model parameters . ( ii ) they are invariant under coordinate translation ( shift ) operation L : x → x + ℓ : we have gd k(Lx|Lθ ) = gd k(x|θ )
Given the gaussian mixture model , dimension reduction can be properly studied in a probabilistic framework using marginal distributions . For this reason , we need to split the space into an r dim space which contains all the relevant dimensions ( attributes ) , and an s dim space ( s = d − r ) which contains all the irrelevant dimensions ( noises ) . We split the coordinates into y = RT x = ( Rr , Rs)T x , or more explicitly , y⊥ = RT yk
RT r x s x , ν k
ν ⊥ = RT
RT r µ s µ
( 2 ) where yk , ν k are in r dim relevant space , and y⊥ , ν ⊥ are in s dim subspace of noise , orthogonal to the r dim relevant space . R is the coordinate rotation , such as the coordinate transformation used in PCA , to clearly separate those relevant and noisy dimensions .
The marginal distribution is defined as p(yk ) ≡ Z p(x)Jdy⊥ = Z p(yk , y⊥)Jdy⊥ . where J = det(∂x/∂y ) = det(R ) is the Jacobian related to coordinate transformation . For orthonormal rotations such as U and V in PCA , RT R = RT R = Ik and det(R ) = 1 . Splitting coordinates , we have kx − µk2 d = flflflfl r ( x − µ ) RT s ( x − µ ) flflflfl RT
= kyk − ν kk2 r + ky⊥ − ν ⊥k2 s
2 d
( 3 )
Thus we have gd k(x − µ ) = gr The marginal distribution of gd k(yk − ν k k ) · gs k(x ) becomes k(y⊥ − ν ⊥ k ) .
Z gd k(x − µ)dy⊥ = gr k(yk − ν k ) ,
( 4 ) which is exactly the standard spherical Gaussian in the r dim subspace . For this reason , we simply use y for
3 yk and ν for ν k in the r dim subspace . Therefore we conclude that Theorem 1 . In EM clustering using spherical Gaussian mixture models in d dimensions , after integrating out irrelevant dimensions , the marginal probability becomes p(y ) = π1gr
1(y − ν1 ) + · · · + πKgr
K(y − νK ) ,
( 5 ) exactly the same type of Gaussian distribution as in r dim space . All relevant attributes for clustering are retained in the r dim subspace .
4 Adaptive Dimension Reduction for EM
For real world clustering problems where clusters are not well separated , the r dim subspace initially obtained using PCA does not necessarily coincide with the subspace spanned by the K cluster centers . Therefore , the centers , and cluster memberships , in the usual dimension reduction clustering are not necessarily the correct ( or accurate ) results . One can correct this by adaptively modifying the r dim subspace using the most current clustering results , and do another round of clustering in the modified subspace . One can repeat this process several times to improved the results .
Given a point or a cluster centroid in the r dim subspace , mapping back to the original d dim space is not unique . In fact , there are infinite number of points in the d dim space , all of which project into one point in the r dim subspace ( all points on a vertical line project into a single point in x y plane ) . However , the centers ( or centroids in the K means ) obtained in clustering in the r dim subspace can be uniquely traced back to the original d dim space by using the cluster membership of each data point . This observation is the basis of our ADR EM clustering .
The cluster membership information is contained in the posterior probability hk i , hk i = Pr(ci = k|yi , θ ) the probability of point i belongs to cluster ck given current model ( parameters ) and the evidence ( value of yi ) . EM algorithm is the following : ( i ) initialize model pai } , hk rameters {πk , νk , σk} ; πkgr ber of points belonging to cluster ck : nk = Pi hk Pi hk Pi hk k(y ) ; ( iii ) update : ( 1 ) compute the numi ; ( 2 ) update priors : πk = nk/N ; ( 3 ) update centers : νk = i yi/nk ; i ||yi − µk||2/rnk . Steps ( ii ) and ( iii ) are repeated k(y)/Pk πkgr
( ii ) compute {hk
( 4 ) update covariances :
σk = i = until convergence . Once EM converges , the final cluster information is contained in {hk i } . Using this information , the centers in the original d dim space can be computed as
µk = Xi hk i xi/nk .
CK = ( µ1 , µ2 , · · · , µK ) ,
Once the locations of the K cluster centers in the original d dim space are known , expressed as the d × K matrix ( 6 ) and we can easily find the new r = K − 1 dimensional subspace spanned by these K centers . The new subspace is defined by a set of K − 1 orthonormal vectors Ur = ( u1 , u2 , · · · , ur ) . Note that this orthonormal basis is not unique : any rotation of it is an equally good basis . Here we present two methods to compute the basis . Both have the same O(r2·N ) computational complexity where N is the number of data vectors in the dataset .
4.1 SVD Basis
We compute the singular value decomposition ( SVD ) ℓ . Since the data is cen
[ 15 ] of C as : C = Pℓ uℓλℓvT tered , Pk πkµk = Pi xi/n = 0 , {µ1 · · · µK} are linearly dependent . Therefore C has rank r = K − 1 . The d × r matrix Ur = ( u1 , u2 , · · · , ur ) is the orthogonal basis of the new subspace . The SVD basis has a useful property that it automatically orders the dimensions according to their importance . For example , the last dimension ur is not as important as the first sub dimension u1 , as in principal component analysis .
Now we project the original data into the new subspace using yi = U T r xi and do another round of EM clustering , with the starting cluster centers from the projections νi = U T r µi and information on priors {πk} .
4.2 QR basis
Another way to build the orthogonal basis is to use QR or Gram Schmidt on the K centroids [ 18 ] . Without loss of generality , we let µK be the one with the smallest magnitude , and form a d × ( K − 1 ) matrix
CK−1 = ( µ1 − µK , µ2 − µK , · · · , µK−1 − µK ) .
( 7 )
Note that vectors in Cr are not necessarily orthogonal to each other . We use the Gram Schmidt procedure , or equivalently , the QR factorization in linear algebra ,
Ck−1 = Q P
0 = [ Qr , Q⊥ ] P
0 = QrP , where the d × r matrix Qr = ( q1 , · · · , qr ) is the orthonormal basis for the subspace . P is an r × r upper triangle matrix , containing the projections of components in the Cr basis . This QR basis has the property that qk will be close to the µk centers if they are reasonably orthogonal to each other .
Now we use Qr to project the original data into the new subspace by yi = QT r xi , etc . Note that by construction , no centers can coincide with each other in either SVD or QR basis .
4.3 The complete ADR EM algorithm
The complete Adaptive Dimension Reduction Expectation Maximization ( ADR EM ) algorithm is described as follows .
1 . Preprocessing data to fit better the spherical Gaus sian model . Center the data such that Pi xi = 0 .
Rescale the data such that the variance in every dimension is 1 . Choose appropriate K as input parameter . Choose dimensionality r for the reduced dimension subspace . In general , we recommend r = K − 1 . But r = K or r < K − 1 are also appropriate .
2 . Do the first dimension reduction using PCA or any other methods , including random starts .
3 . Run EM in the r dim subspace to obtain clusters . Use cluster membership to construct cluster centroids in the original space . Check convergence . If yes , go to step 5 .
4 . Compute the new r dim subspace spanned by the K centroids using either SVD or QR basis . Project data into this new subspace . Go to step 3 .
5 . Output results and converting posterior probabilities to discrete indicators . The relevant attributes ( coordinates ) are also identified .
If accurate results are necessary , one may run one final round of EM in the original data space starting with existing parameters ( see section 7 ) .
A key feature of ADR is that no matter how the data are projected and transformed ( shifted , rotated , etc ) in subspaces , once the cluster memberships in the subspace are computed , we can always use them to construct clusters in the original space , no need for bookkeeping of the details of data transformations and/or reductions . One can easily design hybrid schemes of different data projections and use the obtained cluster membership as the bridge between them to form an integrated clustering method .
4
4.4 Relevant dimensions set of centers Cµ = [ µ1 , . . . , µK ] so as to minimize
In general , r = K − 1 is the optimal choice . However , r = K is also a good choice in many cases : ( i ) when using QR basis , the QR can be applied to CK ( cf . Eq 6 ) instead of CK−1 ( cf . Eq 7 ) and obtain K basis vectors ; ( ii ) in either SVD basis or QR basis , we can add one or even more additional basis vectors which are orthogonal to existing basis . These additional basis vectors can be either chosen for a particular emphasis or chosen randomly . Randomly choosing additional basis vector could help to search for broader configuration space , making sure we are not stuck in a local minimum .
Sometimes we can also choose r < K − 1 . Although K centers define a ( K − 1) dim subspace , they can sometimes locate on or near an r dim subspace where r < K − 1 . For example , 4 points in a 3 dim space could lie on a 2 dim plane or even on a 1 dim line . In these cases , C is rank deficient , ie , the rank of C will be less than K − 1 and the singular values in SVD basis will drop to near zero ; we should choose the appropriate r < K − 1 .
Even if C is not rank deficient , we may still set r to be less than K − 1 for computational efficiency and effectiveness . This is especially important if we are dealing with a large and complex dataset and somehow we believe there should be , for example , K = 10 clusters . Due to the curse of dimensionality , 9 dim space may still be too high , so we may set r = 3 and find 10 clusters in 3 dim space where EM or K means are typically more effective . Also in 3 dim space , computation is more efficient ( than in 9 dim ) and the results can be inspected using 3 dim graphics or other visualization tools . In this case , after the best 10 clusters are discovered using r = 3 , we may further refine the results by setting r = 9 and re run the algorithm , using cluster membership as the bridge .
In all the test examples below , we have tested this r < K − 1 ( over reduced ) method and the results are generally the same as the r = K − 1 method . However , we do notice the slower convergence of the EM method .
5 Adaptive Dimension Reduction for K means
The ADR method can also be applied to the K means clustering as well . Given a set of data vectors X = [ x1 , . . . , xn ] , the K means for K clusters seeks to find a
5
Jd(X , Cµ ) =
K
Xk=1 Xi∈ck kxi − µkk2 d .
( 8 )
Each cluster ci is represented by a center µi and consists of the data vectors that are closest to it in Euclidean distance , and the center of a cluster is the centroid of its data vectors . The K means clustering can be viewed as a special case of EM with three simplifications ( i ) σ1 = · · · = σK = σ ; ( ii ) π1 = · · · = πK ; ( iii ) with σ → 0 so that hk i = 1 or 0 .
As before , the key is to find the relevant r dim reduced space , specified by the projection matrix Rr . We have the following . Theorem 2 . Suppose we somehow know the correct rdim relevant subspace defined by Rr . Let Y = RT r X = RT r ( x1 , . . . , xn ) and Cν = [ ν1 , . . . , νK ] be K centroids in r dim subspace . Solve the K means problem in r dim subspace , min Cν
Jr(Y , Cν ) .
1 , . . . , µ∗ i ) obtained to reµ = [ µ∗ K ] in the full µ are the exact optimal so
Use the cluster membership H = ( hk construct the K centers C ∗ dimensional space . Then C ∗ lution to the the full dimension K means problem . Proof . Assume the centroid matrix C ∗ µ are the minimum for K cluster K means in ( 8 ) . Construct projection matrix Rr that spans the subspace for C ∗ µ . Use Gram Schmit procedure to construct Rs such that R = [ Rr , Rs ] is orthonormal matrix . Since Rr spans the subspace for C ∗ k = 0 . Using this fact and Eq ( 2 ) , we have
µ , RT s µ∗ kxi − µ∗ kk2 r c∗ r + kRT k is closest to RT d = kyi − ν ∗ kk2
If we know the final solution C ∗ s xik2 s This indicates that among all K centers , if c∗ k is closest to xi in the d sim space , then RT r xi in the r dim space , independent of Rs . We can write ⊓– Jd(X , C ) = Jr(Y , Cν ) + const . µ , we can easily construct Rr . For any r×r orthonormal matrix S , RrS still spans the correct subspace . In practice we do not know C ∗ µ until after the problem is solved . By Theorem 2 , we only need to find the relevant subspace . Because of the large flexibility in defining Rr , finding the relevant subspace is much easier than finding C ∗ µ directly . This is the usefulness of Theorem 2 . Our adaptive dimension reduction K means is based on the theorem . The complete ADR Kmeans algorithm is identical to ADR EM algorithm in §43
( A ) Cov=(0.25 , 0.25 , 0.25 )
ADR EM are shown below ,
3
2
1
0
−1
−2
−4
−3
−2
−1
0
1
2
( B ) Cov=(1 , 1.44 , 1.96 )
5
4
3
2
1
0
−1
−2
−3
−4
−5
−6
−4
−2
0
2
Figure 1 : Three Gaussian clusters in 4 dim space . ( A ) covariances ( 0.25 , 0.25 , 025 ) ( B ) covariances ( 1 , 1.44 , 196 ) Data points are shown in the first 2 PCA components . From ( A ) to ( B ) , variance increases ( 1/0.5)2 for the 1st cluster ( red squares ) , ( 12/05)2 for the 2nd cluster ( blue circles ) , and ( 14/05)2 for the 3rd cluster ( black triangles ) .
6 Applications
6.1 Highly overlapping Gaussian mixtures
The first example is a 1000 point synthetic dataset of 3 gaussians in 4 dim with centers ( c1 , c2 , c3 ) , listed below
( c1 , c2 , c3 ) =  
0 0 0 1 0 1 0 1
1 1 1 1
 
,
( 9 )
1 , σ2 covariances ( σ2 3 ) = ( 1 , 1.44 , 1.96 ) , and prior distributions ( π1 , π2 , π3 ) = ( 0.25 , 0.35 , 04 ) The 3 gaussians are highly overlapped ( see Fig 1 ) The results of
2 , σ2
( c∗
1 , c∗
2 , c∗
3 ) =  
1.04 0.03 0.06 0.23 1.18 0.99 0.03 1.02 1.32 0.12 1.16 1.00
  quite close to the correct results . Repeated runs show that the method is quite robust . If we run EM directly in 4 dim space , the EM will have difficulty finding the correct clusters . Results change for each different run . After 10 runs , the best results are shown below
( c∗
1 , c∗
2 , c∗
3 ) =  
0.38 0.12 1.44 0.20 0.97 1.76 0.69 0.72 1.96 0.30 0.91 1.38
 
Dimension reduction is essential in this highly overlapped situation .
6.2 DNA Microarray gene expression pro filing
This example is from molecular biology . High density DNA microarray technology can simultaneously monitor the expression level of thousands of genes which determines different pathological states of the same tissue drawn from different patients [ 16 , 3 ] . Here we study gene expression profiles of non Hodgkin lymphoma cancer data from [ 3 ] . Among the 96 samples of 9 phenotypes ( classes ) , we pick the 4 largest classes with a total of 76 samples(see Fig 2 ) : ( 1 ) 46 samples of diffuse large B cell lymphoma ( ◦ ) , ( 2 ) 10 samples of Activated Blood cell B cell ( ▽ ) , ( 3 ) 9 samples of chronic lymphocytic leukemia ( △ ) , ( 4 ) 11 samples of follicular lymphoma ( + ) . Each sample contains expression levels of 4026 genes ( variables ) . The question we ask : could we discover these phenotypes from data directly , without human expertise ?
We use t test statistic criteria to select top 100 genes . The clustering problem is focused on the 76 samples in 100 dim space with K=4 . This is still a high dimensional problem . We use ADM EM algorithm on this dataset , setting r=3 . The clustering result is shown in the following contingency table
T =  
39 · · ·
3 10 · ·
4 · 9 ·
· · · 11
  where T = ( tij ) , tij is the number of data points which are observed to be in cluster i , but was computed via
6 the clustering method to belong to cluster j . The accu racy is 69/76=0.91 % ( accuracy is defined as Pk tkk/N
[ 11] ) . If we perform the clustering directly in the 100dim space , the runs are often trapped in local minimum . The usefulness of PCA on gene expression analysis were noted in [ 10 ] .
8
6
4
2
0
−2
−4
−6 −10
−5
0
5
10
15
Figure 2 : Gene expression profiles for lymphoma cancer dataset . Shown are the data in the first 2 PCA components .
6.3 Internet newsgroups clustering
We use the Internet newsgroups dataset 1 to illustrate the process of adaptive dimension reduction . We use five news groups NG2/NG9/NG10/NG15/NG18 with 50 news articles from each group ( see [ 29 ] for details ) . NG2 : comp.graphics ; NG9 : rec.motorcycles ; NG10 : recsportbaseball ; NG15 : sci.space ; NG18 : talkpoliticsmideast
Words with document frequency less than 3 are removed , and a total of 2731 distinct words are retained . Each document is represented by a vector in this d=2731 dimensional space . We set r = 5 ( relevant dimension is a 5 dim subspace ) . We start with a random initial 5dim subspace . In Table 1 , we list the accuracy and J at the end of each adaptive iteration . Repeated adaptive dimension reduction gradually converges to the correct subspace . As a comparison , we run K means algorithm in the original d=2731 space with the same initial clusters and obtain accuracy 50.40 % and J=22783 This indicates the effectiveness of our adaptive method .
1The newsgroups dataset together with the bow toolkit for processing it can be downloaded from http://wwwcscmuedu /afs/cs/project/theo 11/www/naive bayeshtml
7 iteration
1 2 3 4 5 6 7 8 accuracy min(J ) 235.58 233.11 231.01 229.68 228.19 227.47 227.06 226.86
25.60 34.40 44.80 52.80 60.40 63.60 66.00 67.20
Table 1 : Clustering results of ADM Kmeans algorithm .
7 Discussions
We introduced a new method for clustering high dimensional data using adaptive dimension reductions . The key to the effectiveness of this method lies in ( Theorems 1 and 2 ) that working in the subspace containing true cluster centers is sufficient to find the cluster centers . The subspace containing cluster centers is of dimension K , far smaller than the original dimension in many applications . Adaptive dimension reduction is an effective way to converge to this subspace . Note that finding the subspace is much easier than finding cluster centers directly , due to the flexibility in defining subspace .
Although we concentrate on EM and K means algorithm here , the adaptive dimension reduction approach could be extended to other clustering methods . Using cluster membership as the bridge to connect subspaces of different dimensions makes these extensions easy to implement . For example , one may construct a number of subspaces based on different feature selection methods and apply different clustering methods on these subset of features and move or combine them to satisfy some optimal conditions .
Another interesting subtle point is that although the functional form in d dim space [ cf . Eq ( 1 ) ] is very much the same as that in r dim subspace [ cf . Eq ( 5) ] , the final parameters are not the same : the priors πk differ in the two spaces in the case of highly overlapped clusters with different covariances . The reason is that the probability can not be separated into a product of P(relevant coordinates)*P(irrelevant coordinates ) : p(x ) = p(y , y⊥ ) 6= p(y ) · p(y⊥ ) , even if each mixture component is separable . Therefore , the standard practice of reporting the results directly obtained in the reduced dimension subspace is not accurate enough . For this reason , we suggest the EM in the d dim space be run once using those parameters obtained in the r dim subspace to get more accurate final parameters . Acknowledgments . This work is supported by US Department of Energy ( Office of Science , Office of Ad vanced Scientific Research , MICS Division , and Office of Laboratory Policy and Infrastructure ) under contract DE AC03 76SF00098 .
References
[ 1 ] CC Aggarwal , C . Procopiuc , JL Wolf , PS Yu , and JS Park . Fast algorithms for projected clustering . pages 61–72 , 1999 .
[ 2 ] R . Agrawal , J . Gehrke , D . Gunopulos , and P . Raghavan . Automatic subspace clustering of high dimensional data for data mining applications . pages 94–105 , 1998 .
[ 3 ] AA Alizadeh , MB Eisen , et al . Distinct types of diffuse large B cell lymphoma identified by gene expression profiling . Nature , 403:503–511 , 2000 .
[ 4 ] MW Berry , ST Dumais , and Gavin W . O’Brien . Using linear algebra for intelligent information retrieval . SIAM Review , 37:573–595 , 1995 .
[ 5 ] P . Bradley and U . Fayyad . Refining initial points for k means clustering . In Proc . 15th International Conf . on Machine Learning , 1998 .
[ 6 ] S . Dasgupta . Experiments with random projection . In Proc . 16th Conf . Uncertainty in Artificial Intelligence ( UAI 2000 ) , 2000 .
[ 7 ] S . Deerwester , ST Dumais , TK Landauer , GW Furnas , and RA Harshman . Indexing by latent semantic analysis . J . Amer . Soc . Info . Sci , 41:391– 407 , 1990 .
[ 8 ] AP Dempster , NM Laird , and DB Rubin . Maximum likelihood from incomplete data via em algorithm . J . Royal Stat . Soc . B . , pages 1–38 , 1977 .
[ 9 ] I . Dhillon and D . Modha . Concept decomposition for large sparse text data using clustering . Machine Learning , 42:143–175 , 2001 .
[ 10 ] C . Ding . Analysis of gene expression profiles : class discovery and leaf ordering . In Proc . 6th Int’l Conf . Research in Comp . Mol . Bio.(RECOMB 2002 ) , pages 127–136 , April 2002 .
[ 11 ] C . Ding and I . Dubchak . Multi class protein fold recognition using support vector machines and neural networks . Bioinformatics , 17:349–358 , 2001 .
[ 12 ] CHQ Ding . A similarity based probability model for latent semantic indexing . Proc . 22nd ACM SIGIR Conference , pages 59–65 , Aug . 1999 .
[ 13 ] P . Drineas , A . Frieze , R . Kannan , S . Vempala , and V . Vinay . Clustering in large graphs and matrices . In Proc . 19th ACM SIAM Symposium on Discrete Algorithms , 1999 .
[ 14 ] R . O . Duda , P . E . Hart , and D . G . Stork . Pattern
Classification , 2nd ed . Wiley , 2000 .
[ 15 ] G . Golub and C . Van Loan . Matrix Computations ,
3rd edition . Johns Hopkins , Baltimore , 1996 .
[ 16 ] TR Golub , DK Slonim , P . Tamayo , et al . Molecular classification of cancer : class discovery and class prediction by gene expression monitoring . Science , 286:531–537 , 1999 .
[ 17 ] J . Grim , J Novovicova , P . Pudil , P . Somol , and F . Ferri . Initialization normal mixtures of densities . Proc . Int’l Conf . Pattern Recognition ( ICPR 1998 ) , Dec . 1998 .
[ 18 ] M . Jeon , H . Park , and JB Rosen . Dimension reduction based on centroids and least squares for efficient processing of text data . Proc . SIAM Conf . on Data Mining , 2001 .
[ 19 ] GH John , R . Kohavi , and K . Pfleger . Irrelevant features and the subset selection problem . In Int’l Conf . Machine Learning , pages 121–129 , 1994 .
[ 20 ] IT Jolliffe .
Principal Component Analysis .
Springer Verlag , 1986 .
[ 21 ] G . Karypis and E H Han . Concept indexing : A fast dimensionality reduction algorithm with applications to document retrieval and categorization . Proc . 9th Int’l Conf . Information and Knowledge Management ( CIKM 2000 ) , 2000 .
[ 22 ] R . Kothari and D . Pitts . On finding the number of clusters . Pattern Recognition Letters , 50:405 –416 , 1999 .
[ 23 ] G . McLachlan and T . Krishnan . The EM Algo rithm and Extensions . John Wiley , 1997 .
[ 24 ] M . Meila and D . Heckerman . An experimental comparison of several clustering and initialization methods . Proc . 14th Conf . Uncertainty in Artificial Intelligence ( UAI 98 ) , pages 386–395 , 1998 .
[ 25 ] AW Moore . Very fast em based mixture model clustering using multiresolution kd trees . Proc . Neural Info . Processing Systems ( NIPS 1998 ) , Dec . 1998 .
[ 26 ] J . Pena , J . Lozano , and P . Larranaga . An empirical comparison of four initialization methods for the k means algorithm . Pattern Recognition Letters , 50:1027–1040 , 1999 .
[ 27 ] D . W . Scott . Multivariate Density Estimation : Theory , Practice , and Visualization . John Wiley , 1992 .
[ 28 ] CKI Williams . A MCMC approach to hierarchical mixture modeling . Proc . Neural Info . Processing Systems ( NIPS 2000 ) , Dec . 2000 .
[ 29 ] H . Zha , C . Ding , M . Gu , X . He , and HD Simon . Spectral relaxation for k means clustering . Proc . Neural Info . Processing Systems ( NIPS 2001 ) , Dec . 2001 .
8
