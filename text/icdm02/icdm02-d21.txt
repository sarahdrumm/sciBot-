A Self Organizing Map with Expanding Force for Data Clustering and
Visualization
Wing Ho Shum , Hui Dong Jin , Kwong Sak Leung Department of Computer Science and Engineering
The Chinese University of Hong Kong
Hong Kong fwhshum , hdjin , ksleungg@csecuhkeduhk
Man Leung Wong
Department of Information Systems
Lingnan University
Hong Kong mlwong@lneduhk
Abstract
The Self Organizing Map ( SOM ) is a powerful tool in the exploratory phase of data mining . However , due to the dimensional confiict , the neighborhood preservation cannot always lead to perfect topology preservation . In this paper , we establish an Expanding SOM ( ESOM ) to detect and preserve better topology correspondence between the two spaces . Our experiment results demonstrate that the ESOM constructs better mappings than the classic SOM in terms of both the topological and the quantization errors . Furthermore , clustering results generated by the ESOM are more accurate than those by the SOM .
1 Introduction
The Self Organizing Map ( SOM ) has been proven to be useful as visualization and data exploratory analysis tools [ 6 ] . It maps high dimensional data items onto a lowdimensional grid of neurons . The regular grid can be used as a convenient visualization surface for showing different features of data [ 9 , 12 ] . SOMs have been successfully applied in various areas such as full text and image analysis , and travelling salesman problem [ 3 , 4 , 7 ] .
However , because a SOM maps the data from a highdimensional space to a low dimensional space which is usually 2 dimensional , a dimensional confiict may occur and a perfect topology preserving mapping may not be generated [ 1 , 5 ] . For example , consider the two trained SOMs depicted in Fig 1 , although they preserve good neighborhood relationships , the SOM depicted in Fig 1(b ) folds the neuron string onto data irregularly and loses much topology information in comparison with the SOM shown in Fig 1(a ) .
There are many research efforts to enhance SOMs for visualization and cluster analysis . Most of them focus on how to visualize neurons clearly and classify data [ 2 , 10 , 12 ] .
Figure 1 . Two SOMs from 2›dimensional space to 1›dimension . The connected dots indicate a string of neurons , and other dots indicate data .
Some work has concentrated on better topology preservation . Kirk and Zurada [ 5 ] trained their SOM to minimize the quantization error in the .rst phase and then minimize the topological error in the second phase .
In this paper , we propose a new learning rule to enhance the topology preservation and overcome the irregularity problem . The paper is organized as follows . We introduce our ESOM in Section 2 , followed by its theoretic analysis The visualization and clustering results of the ESOM are presented and compared with the SOM in Section 3 . A conclusion is given in the last section .
2 Expanding SOM
Besides the neighborhood relationship in the SOM , another topology relationship can be detected and preserved during the learning process to achieve a better topology preserving mapping for data visualization . This is a linear ordering relationship based on the distance between data and their center . A neural network can detect and preserve this ordering relationship . If the distance between a data item and the center of all data items is larger , the distance between the corresponding output neuron and the center is also larger .
Our Expanding SOM ( ESOM ) can construct a mapping that preserves both the neighborhood and the ordering relationships . Since this mapping preserves more topology information of the input data , better performance in visualization can be achieved .
We introduce a new learning rule to learn the linear ordering relationship . Different from the SOM , the learning rule of the ESOM has an additional factor , the expanding coef.cient cj(t ) , which is used to push neurons away from the center of all data items during the learning process . In other words , the fiexible neuron network is expanding gradually in our ESOM algorithm . Moreover , the expanding force is speci.ed according to the ordering of the data items . In general , the larger the distance between the corresponding data item and the center , the larger the expanding coef.cient cj(t ) . Consequently , the associated output neuron is pushed away from the center and the ordering of data items is thus preserved in the output neurons . In the following sub sections , the ESOM algorithm will be discussed rst Theoretical analysis of the ESOM algorithm will then be described .
2.1 The ESOM algorithm
The ESOM algorithm consists of 6 steps .
1 . Linearly
~x0 i coordinates transform the
= [ x01i ; x02i; ; x0Di]T ( i = 1; ; N ) of all given data items so that they lie within a sphere SR centered at the origin with radius R ( < 1 ) . Here N is the number of data items , D is the dimensionality of the data set . Hereafter , [ x1i ; x2i; ; xDi]T denotes the new coordinate of ~xi . Let the center of all data items be ~x0 ~x0 C = 1 i and the maximum distance of data from N the data center be Dmax , then
NPi=1
~xi =
R
Dmax~x0 i , ~x0
C for all i :
( 1 )
2 . Set t = 0 , and the initialize weight vectors ~wj(0 ) ( j = 1; , M ) with random values within the above sphere SR where M is the number of output neurons .
3 . Select a data item at say ~xk(t ) = [ x1k ; x2k; ; xDk]T , and feed it to the input neurons . 4 . Find the winning output neuron , say m(t ) , nearest to random ,
~xk(t ) according to the Euclidean metric : m(t ) = arg min j k~xk(t ) , ~wj(t)k :
( 2 )
5 . Train neuron m(t ) and its neighbors by using the follow ing formula :
~wj(t + 1 ) = cj(t ) ~w0 j ( t + 1 ) 4= cj(t)f ~wj ( t )
+ffj(t ) [ ~xk(t ) , ~wj(t)]g
( 3 )
The parameters include : ffl the interim neuron ~w0 j(t + 1 ) , which indicates the position of the excited neuron ~wj ( t ) after moving towards the input data item ~xk(t ) ; ffl the learning parameter ffj(t)(2 [ 0 ; 1] ) , which is speci.ed by a learning rate ffl(t ) and a neighborhood function hj;m(t)((t) ) : ffj(t ) = ffl(t ) . hj;m(t)((t) ) ;
( 4 ) ffl the expanding coef.cient cj(t ) , which is speci.ed according to cj(t ) = [ 1 , 2ffj(t ) ( 1 , ffj(t ) ) j(t) ] , 1 where j(t ) is speci.ed by
2 ;
( 5 ) j(t ) = 1 , h~xk(t ) ; ~wj ( t)i , q(1 , k~xk(t)k2)(1 , k ~wj(t)k2)(6 )
6 . Update the neighbor width parameter ( t ) and the learning parameters ffl(t ) with predetermined decreasing schemes . If the learning loop does not reach a predetermined number , go to Step 3 with t := t + 1 .
The .rst step facilitates the realization of the expanding coef.cient cj(t ) . After the transformation , we can use the norm of a data item k~xk(t)k to represent its distance from the center of the transformed data items since the center is the origin . Thus , the norm k~xk(t)k can indicate the ordering topology in the data space . This ordering will be detected and preserved in k ~wj(t)k through the expanding process . The learning rule de.ned in Eq ( 3 ) is the key point of the proposed ESOM algorithm . Different from the SOM learning rule , it has an additional multiplication factor ( cid:151 ) the expanding coef.cient cj(t ) . It is worth pointing out that , although the expanding coef.cient cj(t ) is relevant to all data items , the calculation of cj(t ) only depends on ffj ( t ) ; ~xk(t ) and ~wj(t ) . If cj(t ) is a constant 1.0 , the ESOM is simpli.ed to a conventional SOM . Since cj(t ) is always greater than or equal to 1.0 , the expanding force pushes the excited neuron away from the center . In other words , the inequality trates the expanding functionality . After moving the excited neuron ~wj(t ) towards the input data ~xk(t ) , as indicated by j(t + 1)flflfl is always true . Fig 2(b ) illus k ~wj ( t + 1)k flflfl ~w0 trend to support the feasibility of the ESOM because it is very dif.cult to prove the convergence of the SOM like networks in high dimensional cases . In fact , it is still one of the long standing open research problems in neural networks [ 8 ] . We will perform a more rigorous convergence analysis in our future research work . In the following theorem , we assume that all input data items are located within the sphere SR and their center coincides with the origin because the preprocessing procedure in Step 1 has been executed .
Theorem 1 Let SR be the closed sphere with radius R ( < 1 ) centered at the origin , f~xk(t ) 2 SRg ( for k = 1; ; N ) be the input data and f ~wj ( t)g ( for j = 1 ; ; M ) be the weight vectors of the ESOM at time t . Then , for any t 0 ; ( i ) . for j 2 f1 ; 2; ; Mg , 1 cj(t ) and ~wj(t ) 2 SR ; that is , p1 , R2
( 8 )
1
; k ~wj(t)k R :
( 9 )
( ii ) . the expanding coef.cient cj(t ) increases with k~xk(t)k when k~xk(t)k k ~wj(t)k .
Proof . ( i ) . We prove Eqs.(8 ) and ( 9 ) together by induction . This is trivially true for t = 0 according to Step 2 of the ESOM algorithm . If we assume that both equations hold for certain t( 0 ) ; then we .nd 1 , j(t ) = h~xk(t ) ; ~wj ( t)i +q(1 , k~xk(t)k2 ) .q(1 , k ~wj ( t)k2 ) DXd=1 dk(t ) +q(1 , k~xk(t)k2)2! dj(t ) +q(1 , k ~wj(t)k2)2! . DXd=1 w2 x2
= 1 :
Similarly ,
1 , j(t ) = h~xk(t ) ; ~wj(t)i +q(1 , k~xk(t)k2 )
,
.q(1 , k ~wj ( t)k2 ) ( k~xk(t)k2 + k ~wj ( t)k2 ) +p(1 , R2 ) .p(1 , R2 ) 1 , 2R2 :
1 2
Figure 2 . A schematic view of two different learning rules . ( a ) . The learning rule for the SOM ; ( b ) . The learning rule for the ESOM . A black disc indicates a data item ; a gray disc indicates a neuron ; a solid line indicates the neighbor relationship on the grid ; a cir› cle indicates the new position of a neuron ; a dashed circle indicates a neuron ’s temporary position ; a dashed arrow indicates a move› ment direction ; and ‘o’ indicates the the cen› ter of data .
~w0j(t + 1 ) , the neuron is then pushed away from the center . So , during the learning process , the fiexible neuron net is expanding in the data space . More interestingly , as the expanding force is speci.ed accordingly to the ordering relationships , distant data items are likely to be mapped to the distant neurons , while data items near the center are likely to be mapped to the neurons in the center of map , therefore , cj(t ) can help us to detect and preserve the ordering relationship .
Researchers have introduced several measures to evaluate the quality of a mapping [ 1 , 13 ] . In this paper , we employ both the quantization error EQ and the topological error ET used in [ 5 , 11 ] to evaluate the mapping obtained by our ESOM . ET is de.ned as the proportion of the data items whose closest and second closest neurons are not adjacent on the grid . The quantization error evaluates how well the weight vectors represent the data set [ 5 , 12 ] . It is speci.ed as follows :
EQ =
1 N
NXk=1 k~xk(t ) , ~wmk ( t)k
( 7 ) where mk is the winner for the data item ~xk(t ) . These two criteria usually confiict with each other in the SOM .
2.2 Theoretical analysis
To show the feasibility of the ESOM , we should verify that the ESOM does generate a map that preserve both the good neighborhood relationship and the ordering relationship . In this paper , we only give a theorem on a one step
Thus ,
0 j(t ) 2R2
On the other hand , for any learning parameter ffj(t ) 2 [ 0 ; 1 ] , the following inequality is true ,
0 ffj(t ) ( 1 , ffj(t ) ) 0:25 :
According to Eq ( 5 ) , we get 1 cj(t ) ing to the ESOM learning rule , we have 1 , k ~wj ( t + 1)k2 =
1p1,R2 : Accord,k ~wj(t ) + ffj(t)(~xk(t ) , ~wj ( t))k2
[ cj(t)],2
( 10 )
. ( cj(t))2 1 =
( cj(t)),2 . ( 1 , ffj(t))q1 , k ~wj ( t)k2 24 +ffj(t)q1 , k~xk(t)k2 ( 1 , ffj(t))p1 , R2 2 +ffj(t)p1 , R2 = 1 , R2 :
2
35
This implies that k ~wj ( t + 1)k R for any j = 1; ; M . Thus , by induction , ~wj ( t ) 2 SR for any j and t . ( ii ) . We rewrite ~xk(t ) and ~wj(t ) as follows , ~xk(t ) = fl . ~exk , ~wj(t ) = r . ~ewj
Here ~exk and ~ewj are two unit vectors , and fl = k~xk(t)k and r = k ~wj ( t)k . According to the assumption that fl r holds . Let
F ( fl ) = h ~wj(t ) ; ~xk(t)i
+q(1 , k ~wj(t)k2)(1 , k~xk(t)k2 ) = fl r ~ewj ; ~exkff +p(1 , fl2)(1 , r2 ) :
( 11 )
1,c,2 j
( t )
According to Eq ( 5 ) , it is obvious that F ( fl ) = 1 , 2ffj ( t)(1,ffj ( t ) ) . F ( fl ) decreases with the expanding coef.cient cj(t ) . So , to justify the increasing property of cj(t ) , it is suf.cient to show that F ( fl ) decreases with fl whenever fl r . A direct calculation shows = r ~ewj ; ~exkff , p1 , fl2p1 , r2 ( 12 ) r , fl 0 :
@F ( fl )
( 13 )
@fl fl
This implies the decreasing property of F ( fl ) on fl when fl r : Theorem 1 ( i ) says that the expanding coef.cient cj(t ) is always larger than or equal to 10 In other words , it always pushes neurons away from the origin . Thus , during learning , the neuron net is expanding . Furthermore , though the expanding force is always greater than or equal to 1.0 , it will never push the output neurons to in.nite locations . In fact , it is restricted by sphere SR in which the data items are located . This point is substantiated by Eq ( 9 ) . This supports the feasibility of our ESOM .
Theorem 1 ( ii ) gives a theoretic support that the ESOM aims to detect and preserve the ordering relationship among the training data items . It points out that the expanding coef.cient cj(t ) , or the expanding force , is different for various data items . The larger the distance between a data item and the center of all data items is , the stronger the expanding force will be on the associated output neuron . Consequently , the neuron will be pushed away from the center .
@fl , flp1,fl2
We now briefiy discuss another interesting trend based on the proof procedure . If ~wj(t ) is far away from ~xk(t)1 , ~ewj ; ~exkff will be very small or even less than 0 . From p1 , r2 0 . In other words ,
Eq ( 12 ) , @F ( fl ) the expanding coef.cient cj(t ) increases with fl which is the distance of the input data item ~xk(t ) from the center . So , the ordering of k~xk(t)k is refiected by the expanding coef.cient cj(t ) and then is learned by ~wj ( t ) . This also explains why the topological error of the ESOM decreases more quickly than that of the SOM at the beginning of learning . A typical example can be found in Fig 4 in Section 3 .
3 Experimental results
3.1 Experimental setting
We examined the ESOM on 3 synthetic data sets and 1 real life data set . All experimental results are compared with those of the SOM in terms of both the quantization and the topological errors . All data sets were preprocessed using the same linear transformation as in Eq ( 1 ) in order to compare results fairly . All experiments were done with the same set of parameters . The initial values of the learning rate ffl , the neighbor width parameter , and the radius R were 0.5 , 0.9 and 0.999 respectively . Both the learning rate ff and the neighbor width parameter were decreased by factor of 0.998 per iteration . Except for the last data set , we used a rectangular grid with 20*20 neurons . All experiments were run for 2000 iterations in total .
The three synthetic data sets are quite interesting in both their special cluster shapes and locations as illustrated in Fig 3 . The conventional clustering algorithms such as Kmeans and Expectation Maximization ( EM ) are unable to identify the clusters . The .rst data set has 3,000 data items in 2 dimensional space with 3 clusters . The most inside cluster looks like a triangle which is surrounded by two strip like clusters . This data set is designed to demonstrate the topology preservation capacity of SOMs , because we
1the case is common at the beginning of learning since the weight vec tor ~wj ( t ) is randomly initialized .
2 e t u b i r t t
A
0.3
0.2
0.1
0
−0.1
−0.2
−0.3
0.5
3 e t u b i r t t
A
0
−0.5 −0.5
0.5
0
3 t e u b i r t t
A
−0.5 −0.5
Data set 1
−0.3
−0.2
−0.1
0
Attribute 1 data set 3
0.1
0.2
0.3
0.5
0.5
0
0
0.5
−0.5
Attribute 2
Attribute 1 data set 2
0
−0.5
0.5
Attribute 2
0
Attribute 1
Figure 3 . Illustration of Data set 1 ( Upper ) , Data set 2 ( Middle ) and Data set 3 ( Lower ) can compare the resultant mapping with the original data set directly . The second data set contains 2,000 data items in 3dimensional space with 2 clusters ( the middle one in Fig 3 ) . The third data set has 3,000 3 dimensional data items ( the lower one in Fig 3 ) . The last data set , Iris , is a benchmark problem and is downloadable from UCI Machine Learning Repository ( wwwicsucsedu/umlearn/ULSummaryhtml ) It has 150 data items in 4 dimensional space with 3 classes . As there are not many data items in the last data set , we use an output grid with 10 * 10 neurons for both algorithms .
3.2 Results for the .rst data set
First , we check how good the mappings the two algorithms can generate in terms of the two criteria given in Subsection 21 Fig 4 illustrates the quantization and the topological errors during a typical run of both algorithms . It is clearly seen that the quantization error decreases gradually as the learning process continues . The quantization error of
Quantization Error of Data Set 1
SOM ESOM
200
400
600
800
1000
1200
1400
1600
1800
2000
Number of Iterations
Topological Error of Data Set 1
SOM ESOM r o r r
E n o i t a z i t n a u Q
0.055
0.05
0.045
0.04
0.035
0.03
0.025
0.02
0.015
0
0.8
0.7
0.6 r o r r
E l
0.5 i a c g o o p o T l
0.4
0.3
0.2
0.1
0
200
400
600
800
1000
1200
1400
1600
1800
2000
Number of Iterations
Figure 4 . The quantization error ( Upper ) and the topological error ( Lower ) during the learn› ing of the ESOM and the SOM . the trained ESOM is 0.017 which is a bit smaller than that of the trained SOM , 0018 During learning , the topological error curve clearly has three stages : decreasing , increasing and convergence . At the very beginning of the training process , the neuron ’s weights are fairly dislike , while some of them even contain remnants of random initial values , thus higher topological errors are got . After several iterations , the topological error decreases dramatically . Because the learning rate ffl is large and the neighborhood function is also large then , the neurons adjacent on the grid may move much closer the input data item together . At this stage , the ESOM can learn the ordering topology of data items very quickly . As shown in Fig 4 , the topological error of the ESOM is much smaller than that of the SOM . Though the topological errors of both algorithms increase later , the ESOM keeps the gain and always has smaller topological error than the SOM . Finally , the topological errors of the ESOM and the SOM are 0.238 and 0.304 respectively . ESOM makes about 20 % improvement on the topological error in comparison with the SOM , and the ESOM gets slightly smaller quantization error than the SOM . Thus , the ESOM can generate better topology preserving maps than the SOM .
Now let us see how well the trained ESOM identi.es the clusters in the .rst data set . Fig 5 illustrates the trained ESOM and SOM in the form of U matrix . The x axis and y axis of the U matrix indicate a neuron ’s position on the grid , and the z axis is the average Euclidean distance of e c n a t s D i
0.2
0.15
0.1
0.05
0 20 e c n a t s D i
0.5
0.4
0.3
0.2
0.1
0 20
U−Matrix of ESOM
15
10
20
15
10
5
5
0
0
U−Matrix of SOM
15
10
20
15
5
0
0
10
5
Quantization Error of Data Set 2
SOM ESOM
200
400
600
800
1000
1200
1400
1600
1800
2000
Number of Iterations
Topological Error of Data Set 2
SOM ESOM r o r r
E n o i t a z i t n a u Q
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0
0.5
0.45
0.4 r o r r
E l
0.35 i a c g o o p o T l
0.3
0.25
0.2
0.15
0
200
400
600
800
1000
1200
1400
1600
1800
2000
Number of Iterations
Figure 5 . U›Matrix of the trained ESOM ( Up› per ) and the trained SOM ( Lower ) .
Figure 7 . The quantization error ( Upper ) and the topological error ( Lower ) in the learning of the ESOM and the SOM .
Scatter Plot of ESOM
0
2
4
6
8
10
12
14
16
18
20
Scatter Plot of SOM
20
18
16
14
12
10
8
6
4
2
0
20
18
16
14
12
10
8
6
4
2
0
0
2
4
6
8
10
12
14
16
18
20
Figure 6 . Scatter plot the trained ESOM ( Up› per ) and the trained SOM ( Lower ) . neurons from its adjacent ones [ 2 ] . A sequence of consecutive peaks can be regarded as a boundary among clusters , while the basins beside are regarded as clusters . There are clearly two sequences of peaks in the ESOM ’s U matrix , which indicate three clusters in the .rst data set . The layer structure in the data set has been clearly illustrated . In contrast , The boundaries in the SOM ’s U matrix is not so clear because some high peaks blur the boundaries . The scatter plots shown in Fig 6 illustrate data clusters on the grid . In the scatter plot , each marker represents a mapping of a data item and the shape of the marker indicates its cluster label . The marker is placed on the winning neuron of the data item . To avoid overlapping , the marker has plotted with a small offset which is speci.ed according to the data item ’s Euclidean distance from the winning neurons . The ESOM maps data items in well organized layers . We can easily .nd the three clusters in its scatter plot which is quite similar with the original data set as shown in Fig 3 . However , as we can see from Fig 6 , the SOM cannot map data very well . The outer cluster in Fig 3 is even separated into three subclusters ( indicated by ‘+’ ) .
3.3 Results for the second data set
Fig 7 shows the quantization and the topological errors during a typical run of both algorithms for the second data set . Though the topological errors of both the ESOM and
U−Matrix of ESOM e c n a t s D i
0.8
0.6
0.4
0.2
0 20 e c n a t s D i
0.5
0.4
0.3
0.2
0.1
0 20
15
10
15
10
5
5
0
0
U−Matrix of SOM
15
10
15
10
5
5
0
0
20
20
Figure 8 . U›Matrix of the trained ESOM ( Up› per ) and the trained SOM ( Lower ) .
Scatter Plot of ESOM
20
18
16
14
12
10
8
6
4
2
0
20
18
16
14
12
10
8
6
4
2
0
0
2
4
6
8
10
12
14
16
18
20
Scatter Plot of SOM
0
2
4
6
8
10
12
14
16
18
20
Figure 9 . Scatter plot the trained ESOM ( Up› per ) and the trained SOM ( Lower ) . the SOM are very close , the ESOM has smaller quantization error than the SOM . Observed from Fig 8 , the U matrix of the ESOM clearly shows two clusters as one cluster is separated by a sequence of peaks . However , the U matrix of the SOM indicates three clusters . An extra cluster has been added at the bottom right corner . This results is further con.rmed by the scatter plots of the two maps as illustrated in Fig 9 . The SOM separates one of the clusters in the original data set into two clusters(represented by ‘’ ) In contrast , the scatter plot of the ESOM clearly shows two clusters where one cluster surrounds the other as in the original data set depicted in Fig 3 .
Table 1 . Average Quantization Errors based on 10 independent runs .
Data Set Name Data Set 1 Data Set 2 Data Set 3 Data Set 4
ESOM 0.01767 0.04753 0.05991 0.02907
SOM 0.01800 0.04832 0.06042 0.02774
Improvement ( % ) 1.80060 1.64508 0.83417 4.81993
Table 2 . Average Topological Errors based on 10 independent runs .
Data Set Name Data Set 1 Data Set 2 Data Set 3 Data Set 4
ESOM 0.24088 0.30215 0.33132 0.33200
SOM 0.26039 0.30555 0.34763 0.33667
Improvement ( % ) 7.49154 1.11241 4.69203 1.38624
Table 3 . Average Execution Times based on 10 independent runs .
Data Set Name Data Set 1 Data Set 2 Data Set 3 Data Set 4
ESOM SOM 2283.2 2371.6 2051.2 2068.9 2872.1 2904.7 28.8 28.5
Difference ( % ) 3.87176 0.86291 1.13506 1.05263
Tables 1 , 2 and 3 show the average quantization errors , topological errors and execution time for all the four data sets , based on 10 independent runs , respectively . We can see that the ESOM has better capability in topology preservation , while the execution times are comparable with those of SOM . However , the ESOM have not much improvement on quantization error yet , because topological error and quantization error are in confiict and it is dif.cult to optimize both of them at the same time . maps . In 2000 IEEE International Conference on Systems , Man , and Cybernetics , volume 4 , pages 2527(cid:150 ) 2532 . IEEE Service Center , 2000 .
[ 6 ] Teuvo Kohonen . Self Organizing Maps . Springer
Verlag , New York , 1997 .
[ 7 ] Teuvo Kohonen , Samuel Kaski , Krista Lagus , Jarkko SalojAvi , Vesa Paatero , and Antti Saarela . Organization of a massive document collection . IEEE Transactions on Neural Networks , Special Issue on Neural Networks for Data Mining and Knowledge Discovery , 11(3):574(cid:150)585 , May 2000 .
[ 8 ] Siming Lin and Jennie Si . Weight value convergence of the SOM algorithm for discrete input . Neural Computation , 10(4):807(cid:150)814 , 1998 .
[ 9 ] Mu Chun Su and Hsiao Te Chang . A new model of self organizing neural networks and its application in data projection . IEEE Transactions on Neural Networks , 12(1):153 ( cid:150)158 , Jan . 2001 .
[ 10 ] Juha Vesanto . SOM based data visualization methods .
Intelligent Data Analysis , 3:111(cid:150)26 , 1999 .
[ 11 ] Juha Vesanto . Neural network tool for data mining : SOM toolbox . In Proceedings of Symposium on Tool Environments and Development Methods for Intelligent Systems ( TOOLMET2000 ) , pages 184(cid:150)196 , Oulu , Finland , 2000 .
[ 12 ] Juha Vesanto and Esa Alhoniemi . Clustering of the IEEE Transactions on Neural self organizing map . Networks , 11(3):586(cid:150)600 , May 2000 .
[ 13 ] Thomas Villmann , Ralf Der , Michael Herrmann , and Thomas M . Martinetz . Topology preservation in selforganizing feature maps : exact de.nition and measurement . IEEE Transactions on Neural Networks , 8(2):256 ( cid:150)266 , March 1997 .
4 Conclusion
In this paper , we have proposed an Expanding SelfOrganizing Map ( ESOM ) to detect and preserve better topology correspondence between the input data space and the output grid . During the learning process of our ESOM , the fiexible neuron net is expanding and the neuron corresponding to a distant data item gets large expanding force . Besides the neighborhood relationship as in the SOM ( SelfOrganizing Map ) , the ESOM can detect and preserve an linear ordering relationship as well . Our experiment results have substantiated that the ESOM constructs better visualization results than the classic SOM in terms of both the topological and the quantization errors . Furthermore , clustering results generated by the ESOM are more accurate than those obtained by the SOM on both synthetic and reallife data sets .
In our future work , we will study different methods to scale up the ESOM algorithm for large data sets and investigate various techniques for visualizing large data sets ef.ciently , and compare the performance of the ESOM with other extensions of SOM .
Acknowledgement
This research was supported by RGC Earmarked Grant for Research CUHK 4212/01E of Hong Kong .
References
[ 1 ] H . U . Bauer , M . Herrmann , and T . Villmann . Neural maps and topographic vector quantization . Neural Networks , 12:659(cid:150)676 , 1999 .
[ 2 ] JAF Costa and ML de Andrade Netto . A new tree structured self organizing map for data analysis . In Proceedings of International Joint Conference on Neural Networks , volume 3 , pages 1931(cid:150)1936 , 2001 .
[ 3 ] Huidong Jin , Kwong Sak Leung , and Man Leung Wong . An integrated self organizing map for the traveling salesman problem . In Nikos Mastorakis , editor , Advances in Neural Networks and Applications , pages 235(cid:150)240 , World Scienti.c and Engineering Society Press , Feb . 2001 .
[ 4 ] Huidong JIN , Kwong Sak Leung , Man Leung Wong , and Zongben Xu . An ef.cient self organizing map designed by genetic algorithms for the traveling salesman problem . Accepted by IEEE Trans . SMC Part B . , May 2002 .
[ 5 ] J . S . Kirk and J . M . Zurada . A two stage algorithm for improved topography preservation in self organizing
