Iterative Clustering of High Dimensional Text Data Augmented by Local Search
Inderjit S . Dhillon⁄ and Yuqiang Guan⁄
Department of Computer Sciences
University of Texas
Austin , TX 78712 1188 , USA inderjit , yguan@csutexasedu
J . Kogan
Department of Mathematics and Statistics University of Maryland Baltimore County
Baltimore , MD 21228 , USA kogan@mathumbcedu
Abstract
The k means algorithm with cosine similarity , also known as the spherical k means algorithm , is a popular method for clustering document collections . However , spherical k means can often yield qualitatively poor results , especially when cluster sizes are small , say 25 30 documents per cluster , where it tends to get stuck at a local maximum far away from the optimal solution . In this paper , we present a local search procedure , which we call ( cid:147).rst variation(cid:148 ) that re.nes a given clustering by incrementally moving data points between clusters , thus achieving a higher objective function value . An enhancement of .rst variation allows a chain of such moves in a KernighanLin fashion and leads to a better local maximum . Combining the enhanced .rst variation with spherical k means yields a powerful ( cid:147)ping pong(cid:148 ) strategy that often qualitatively improves k means clustering and is computationally efcient We present several experimental results to highlight the improvement achieved by our proposed algorithm in clustering high dimensional and sparse text data .
1 . Introduction
Clustering or grouping document collections into conceptually meaningful clusters is a well studied problem . A starting point for applying clustering algorithms to unstructured document collections is to create a vector space model , alternatively known as a bag of words model [ 16 ] . The basic idea is ( a ) to extract unique content bearing words from the set of documents treating these words as features and ( b ) to then represent each document as a vector of certain weighted word frequencies in this feature space . Typically , a large number of words exist in even a moderately sized set of documents where a few thousand words or more are common ; hence the document vectors are very highdimensional . In addition , a single document typically contains only a small fraction of the total number of words in the entire collection ; hence , the document vectors are generally very sparse , ie , contain a lot of zero entries .
⁄This research was supported by NSF Grant No . ACI 0093404 .
The k means algorithm is a popular method for clustering a set of data vectors [ 5 ] . The classical version of kmeans uses squared Euclidean distance , however this distance measure is often inappropriate for its application to document clustering [ 18 ] . An effective measure of similarity between documents , and one that is often used in information retrieval , is cosine similarity , which uses the cosine of the angle between document vectors [ 16 ] . The k means algorithm can be adapted to use the cosine similarity metric to yield the spherical k means algorithm , so named because the algorithm operates on vectors that lie on the unit sphere [ 4 ] . Since it uses cosine similarity , spherical k means exploits the sparsity of document vectors and is highly ef.cient [ 3 ] .
The spherical k means algorithm , similar to the Euclidean algorithm , is a hill climbing procedure and is prone to getting stuck at a local optimum ( .nding the global optimum is NP complete ) . For large document clusters , it has been found to yield good results in practice , ie , the local optimum found yields good conceptual clusters [ 4 , 18 , 3 ] . However , as we show in Section 3 , spherical k means often produces poor results on small and moderately sized clusters where it tends to get stuck in a qualitatively inferior local optimum .
In this paper , we present an algorithm that uses local search to re.ne the clusters produced by spherical k means . Our re.nement algorithm alternates between two phases : ( a ) .rst variation and ( b ) spherical k means itself . A .rstvariation step moves a single document from one cluster to another , thereby increasing the objective function value . A sequence of .rst variation moves allows an escape from a local maximum , so that fresh iterations of spherical kmeans can be applied to further increase the objective function value . This ping pong strategy yields a powerful re.nement algorithm which often qualitatively improves kmeans clustering and is computationally efcient Note that our re.nement algorithm always improves upon the input clustering in terms of the objective function value .
Many variants of k means , such as ( cid:147)batch(cid:148 ) and ( cid:147)incre mental(cid:148 ) versions have been proposed in the literature , see Section 6 for a discussion . The main contribution of this paper is our use of local search for document clustering . Our strategy leads to a ( cid:147)ping pong(cid:148 ) algorithm which alternates between ( cid:147)batch(cid:148 ) k means and .rst variation iterations , thereby harnessing the power of both in terms of improved quality of results and computational speed .
We now give an outline of the paper . In Section 2 , we present the spherical k means algorithm while Section 3 presents scenarios in which this algorithm performs poorly . In Section 4 , we introduce the .rst variation method and present our proposed re.nement algorithm . Experimental results in Section 5 show that our algorithm yields qualitatively better results giving higher objective function values . In Section 6 we discuss related work and .nally , in Section 7 we present our conclusions and future work .
2 . Spherical k means algorithm
We start with some necessary notation . Let d be the number of documents , w be the number of words and let X = fx1 ; x2 ; : : : ; xdg denote the set of non negative document vectors , where each xi 2 Rw and kxik(· kxik2 ) = 1 , ie , each xi lies on the unit sphere . A clustering of the document collection is its partitioning into disjoint subsets …1 ; …2 ; :: : ; …k , ie,Sk j=1 …j = X and …j \ …l = ` , j 6= l : For a cluster … we denote the sumXx2… x by s(… ) . The con cept vector of the cluster … is de.ned by c(… ) = s(… ) ks(…)k
; ie , the concept vector of the cluster is its normalized expectation . We de.ne the ( cid:147)quality(cid:148 ) or ( cid:147)coherence(cid:148 ) of a nonempty cluster … as q ( … ) =Xx2… xT c(… ) = ks(…)k :
( 1 )
We set q(` ) = 0 for convenience . Finally , for a partition f…jgk j=1 we de.ne the objective function to be the sum of the qualities of the k clusters :
Q¡f…jgk j=1¢ = kXj=1 kXj=1 Xx2…j where we have written cj for c(…j ) . The goal is to .nd a clustering that maximizes the value of the above objective function . In what follows we present the spherical k means algorithm which is an iterative process that generates a sequence of partitions f…(t ) l gk Q‡f…(t+1 ) j=1· ‚ Q‡f…(t ) j=1· : gk j gk l=1 , t = 0 ; 1 ; :: : , such that
To emphasize the relationship between successive partitions we shall denote the ( cid:147)next(cid:148 ) partition generated by k means ,
( 2 ) j j l gk j=1 , by nextKM‡f…(t ) gk l=1· . With the above no f…(t+1 ) tation , we now present the spherical k means algorithm . Given a user supplied tolerance tol > 0 do the following : 1 . Start with a partitioning f…(0 ) l gk l=1 and the concept associated with the parti vectors c(0 ) tioning . Set the index of iteration t = 0 .
2 ; : : : ; c(0 )
1 ; c(0 ) k
2 . For each document vector x 2 X .nd the concept vector cl⁄(x ) closest in cosine similarity to x ( unless stated otherwise we break ties arbitrarily ) , ie , xT c(t ) j : l⁄(x ) = arg max j l=1· induced by the old concept vecl gk(t ) l=1 :
Next , compute the new partitioning f…(t+1 ) nextKM‡f…(t ) tors fc(t ) l gk …(t+1 ) l
= fx 2 X : l⁄(x ) = lg ; 1 • l • k : gk l=1 =
( 3 ) l
3 . Compute the new concept vectors corresponding to the partitioning computed in ( 3 ) : c(t+1 ) l l
:
=
) )k s(…(t+1 ) ks(…(t+1 ) l=1·· ¡ Q‡f…(t ) l=1· is l gk(t ) l gk l
4 . If Q‡nextKM‡f…(t ) greater than tol , increment t by 1 and go to step 2 above . Otherwise , stop .
As noted in ( 2 ) , it can be shown that the above algorithm is a gradient ascent scheme , ie , the objective function value increases from one iteration to the next . For details , including a proof , see [ 4 ] . However , like any gradientascent scheme , the spherical k means algorithm is prone to local maxima .
3
Inadequacy of k means We now present some scenarios in which spherical kmeans can get stuck in a qualitatively poor local maximum . Example 3.1 Consider the three unit vectors in R2 :
1 = fx1 ; x2g , …(0 )
Let the initial partition be …(0 ) 2 = fx3g . The value of the objective function for this partition is Q0 = 2 cos ( (cid:181)=2)+1 : When 0 • ( cid:181 ) • …=3 the concept vector c1 = cos((cid:181)=2 ) of the cluster …(0 ) is closest in cosine similarity to vectors x1 and x2 . Hence , an application of spherical kmeans does not change the partition(see .gure below ) .
1
The partition …(1 )
1 = fx1g , …(1 )
2 = fx2 ; x3g , has the objective function value Q1 = 2 cos ( …=4 ¡ ( cid:181)=2 ) + 1 . If ( cid:181 ) = …=3 , then Q0 = 2 cos ( …=6 ) + 1 < 2 cos ( …=12 ) + 1 = Q1 , and so the optimal partition is missed by k means . q ( …j ) = xT cj ; x1 = ( 1 ; 0)T ; x2 = ( cos ( cid:181 ) ; sin ( cid:181))T ; x3 = ( 0 ; 1)T :
X = [ x1 ; x2 ; : : : ; xk2 ] = 266664
K1 K2 I 0 I 0 : : : : : : 0 : : :
: : : Kk : : : 0 0 : : : : : : : : : : : : I
;
377775
In Section 4 , we show that a ( cid:147).rst variation(cid:148 ) iteration corrects the above problem and generates the optimal partition …(1 ) and …(0 ) 2 .
2 = fx2 ; x3g starting from …(0 )
1 = fx1g , …(1 )
1
INITIAL PARTITION
OPTIMAL PARTITION
1.5
1
0.5
0
XXXXXXXXXXXXXXXXXXXX X3
XXXXXXXXXXXXXXXXXXXX X2 q
XXXXXXXXXXXXXXXXXXXX X1
1.5
1
0.5
0
XXXXXXXXXXXXXXXXXXXX X3
XXXXXXXXXXXXXXXXXXXX X2 q
XXXXXXXXXXXXXXXXXXXX X1
−0.5
−0.5
0
0.5
1
1.5
−0.5
−0.5
0
0.5
1
1.5
Example 3.2 Consider the set of vectors arranged as columns of the ( k2 + k ) £ k matrix : where I is the k £ k identity matrix and Kl is the k £ k matrix with entries 1 k in the lth row and 0 ’s elsewhere . Note that all vectors xi 2 Rk2+k have the same norm , jjxijj = p1 + 1=k2 , and they form k natural clusters with xik+j being placed in cluster i + 1 .
The intra cluster and inter cluster dot products are re spectively given by
1 . xT i xj = 1 1 ; : : : ; k , k2 , where ( l ¡ 1)k < i < j • lk and l =
2 . xT i xj = 0 , otherwise .
1+ 1 k2 pk+1 kp1+ 1
As mentioned above , the best clustering would have x1 ; : : : ; xk in the .rst cluster , xk+1 ; : : : ; x2k in the second cluster , and so on . For this optimal clustering , the cosine of any xi with its own concept vector is = q k+1 k2+1 , and 0 with any other concept vector . But suppose we initialize the k clusters as follows : cluster 1 consists of x1 , xk+1 ; : : : ; xk2¡k+1 , cluster 2 consists of x2 ; xk+2 ; : : : ; xk2¡k+2 , and generally cluster l consists of xl ; xk+l ; : : : ; xk2¡k+l , for 1 • l • k . Then it can be seen that spherical k means does not change this initial clustering at all . This is because the cosine of xi , and with its own concept vector is
1+ 1 k2 k2pk+ 1 k
= 1pk
1 k2p1+ 1 k2pk+ 1 k
=
1pk2+1pk with any other concept vec tor . The situation is similar with most other starting partitions . In a set of experiments for k = 5 , we observed that all 100 runs of spherical k means with random initializations stopped without a change in the initial clustering . p1+ 1
Example 3.3 The above behavior is often seen in real life applications . As a concrete example , we took a collection of 30 documents consisting of 10 documents each from the three distinct classes MEDLINE , CISI and CRAN ( see Section 5 for more details ) . These 30 documents contain a total of 1073 words ( after removing stop words ) . Due to this high dimensionality , the cosine similarities between the 30 document vectors are quite low . The average cosine similarity between all documents is 0.025 ; however the average cosine between documents in the same class is 0.294 while the average inter class cosine is 00068 Thus there is a clear separation between intra class cosines and inter class cosines and so it seems that a good clustering algorithm should be easily able to recover the original class structure . However , when we run spherical k means on this data set , there is hardly any movement of document vectors between clusters irrespective of the starting partition ( cid:151 ) indeed we observed that 96 % of 1000 different starting partitions resulted in no movement at all , ie , k means returned a .nal partition that was identical to the initial partition . This behavior is unusual ; in contrast , for large data sets the .rst few iterations of spherical k means typically lead to a lot of movement of data points between clusters [ 4 , 3 ] . However , a closer look reveals the reasons for this failure . Consider a document vector x , and consider an initial cluster …i such that x 62 …i . Due to the small number of data points and the small average cosine similarity between documents , xT ci turns out to be quite small in magnitude for an arbitrary initial partitioning . However , it is instructive to take a closer look at x and its own cluster . If x 2 …l , the cosine similarity xT cl may be broken down into two parts : xT cl =
1 ks(…l)k
+ Xy2…l¡fxg xT y ks(…l)k where the .rst term has 1 in the numerator since xT x = 1 ( due to the contribution of x in cl ) . For data sets that are high dimensional and contain a small number of points , this .rst term itself is typically much larger in magnitude than xT ci , x 62 …i . As a result , a spherical k means iteration retains each document vector in its original cluster . Thus , in this case we start with an arbitrarily poor clustering and the k means algorithm returns this poor clustering as the .nal output . We now see how to overcome this difculty
4 Re.nement Algorithm
This section describes algorithms to ( cid:147).x(cid:148 ) the above problems . 4.1 First Variation De.nition 4.1 A .rst variation of a partition f…lgk partition f…0lgk from a cluster …i of f…lgk existing cluster …j of f…lgk l=1 is a l=1 obtained by removing a single vector x l=1 and assigning this vector to an l=1 . l=1 l=1 ;
We denote the set of all .rst variation partitions of f…lgk l=1¢ . Among all the elements of this set , we by FV¡f…lgk seek to select the ( cid:147)steepest ascent(cid:148 ) partition . The formal de.nition of this partition is given next . l=1¢ is a .rst De.nition 4.2 The partition nextFV¡f…lgk l=1 so that for each .rst variation f…0lgk variation of f…lgk l=1¢ : l=1¢¢ ‚ Q¡f…0lgk
Q¡nextFV¡f…lgk l=1 ; t ‚ 0 ; so that l=1· ; t = 0 ; 1 ; : : : : l=1 = nextFV‡f…(t ) gk l gk
The proposed .rst variation algorithm generates a sequence of partitions f…(t ) l gk
We now pause briefiy to illustrate differences between .rst variation iterations and the spherical k means iterations . To simplify the presentation we consider a two cluster partition fZ ; Yg where Z = fz1 ; : : : ; zng , and Y = fy1 ; : : : ; ymg . Our goal is to examine whether a single vector , say zn , should be removed from Z , and assigned to Y . We denote the potential new clusters by Z¡ and Y+ , ie , Z¡ = fz1 ; : : : ; zn¡1g and Y+ = fy1 ; : : : ; ym ; zng : Note that spherical k means examines the quantity f…(t+1 ) l
¢kmeans = zT n [ c(Y ) ¡ c(Z ) ] :
( 4 )
If ¢kmeans > 0 , then the spherical k means algorithm moves zn from Z to Y . Otherwise zn remains in Z .
Unlike k means , .rst variation computes n¡1Xi=1 i c(Z¡ ) ¡ zT where the quality q is as in ( 1 ) . A straightforward computation shows that
¢ =£q¡Z¡¢ ¡ q(Z)⁄ +£q¡Y+¢ ¡ q(Y)⁄ ; n c(Z)# + i c(Z ) ¡ zT zT i c(Y)# mXi=1 n£c(Y+ ) ¡ c(Y)⁄ + n c(Y+ ) ¡ i £c(Y+ ) ¡ c(Y)⁄ + zT i £c(Z¡ ) ¡ c(Z)⁄ + ¢kmeans ;
¢ ="n¡1Xi=1 " mXi=1 mXi=1 n¡1Xi=1 i c(Y+ ) + zT yT where ¢kmeans is de.ned in ( 4 ) . By the Cauchy Schwarz inequality , we have yT yT zT
=
( 5 ) n¡1Xi=1 n¡1Xi=1 n¡1Xi=1 zT i c(Z ) ; i c(Z¡ ) ‚ zT i £c(Z¡ ) ¡ c(Z)⁄ ‚ 0 : zT and so
For the same reason , mXi=1 yT i £c(Y+ ) ¡ c(Y)⁄ + zT n£c(Y+ ) ¡ c(Y)⁄ ‚ 0 :
These two inequalities along with ( 5 ) imply that :
¢ ‚ ¢kmeans :
The last inequality shows that even when ¢kmeans • 0 and cluster af.liation of zn is not changed by spherical kmeans , the quantity ¢ may still be positive . Thus , while Q¡fZ¡ ; Y+g¢ > Q ( fZ ; Yg ) , the partition fZ¡ ; Y+g will be missed by spherical k means ( see Example 31 ) We now turn to the magnitude of ¢ ¡ ¢kmeans . From ( 5 ) , 0 • ¢ ¡ ¢kmeans = s(Z¡)T£c(Z¡ ) ¡ c(Z)⁄ + s(Y+)T£c(Y+ ) ¡ c(Y)⁄ :(6 ) Since the vectors s(Z¡ ) and c(Z¡ ) are proportional , a large value of c(Z¡ ) ¡ c(Z ) results in a large dot product s(Z¡)T [ c(Z¡ ) ¡ c(Z) ] . A similar argument holds for the second term on the right hand side of ( 6 ) . Hence we can expect a ( cid:147)substantial(cid:148 ) difference between ¢ and ¢kmeans when removing a vector from cluster Z and assigning it to cluster Y ( cid:147)signi.cantly(cid:148 ) changes locations of the corresponding concept vectors . This phenomenon is unlikely to happen when clusters are large . However , .rst variation iterations become effective in the case of small clusters ( clusters of size 100 or less in our experience ) . The ( cid:147)spherical .rst variation(cid:148 ) algorithm is formally presented below . Given a user supplied tolerance tol > 0 do the following : 1 . Start with a partitioning f…(0 ) l gk l=1 . Set the index of iteration t = 0 . i=1 = nextFV(f…(t ) 2 . Generate f…(t+1 ) l gk gk 3 . If Q‡nextFV‡f…(t ) l=1·· ¡ Q‡f…(t ) l=1· is l gk l gk greater than tol , increment t by 1 , and go to step 2 . Otherwise , stop . l=1 ) . l
It is easy to see that a single ( cid:147)spherical .rst variation(cid:148 ) iteration applied to the initial partition of Example 3.1 generates the ( cid:147)optimal partition(cid:148 ) . We now address the time and memory complexity of .rst variation iterations . The computational bottleneck is in computing q(…(t ) j [ fxg ) ¡ q(…(t ) i ¡ fxg ) ¡ q(…(t ) j ) for all the document vectors x 2 X . Note that i · = q‡…(t ) i ¡ fxg· ¡ q‡…(t ) ‡ks(…(t ) i )k2 ¡ 2ks(…(t ) i )kxT c(…(t ) i ) + 1·1=2 i ) and q(…(t )
¡ ks(…(t ) i )k(7 ) j )k:(8 ) and j [ fxg· ¡ q‡…(t ) q‡…(t ) j · = j ) + 1·1=2 ‡ks(…(t ) j )kxT c(…(t ) j )k2 + 2ks(…(t ) ¡ ks(…(t ) We remark that computation of the quantities ks(…(t ) l )k , and xT c(…(t ) l ) , x 2 X , l = 1 ; : : : ; k are needed for iterations of the spherical k means algorithm as well . When these quantities are available , a .rst variation iteration can be completed in O(d + k ) amortized time , where d is the number of document vectors . 4.2 Kernighan›Lin Chains
A .rst variation iteration moves a single vector that maximally increases the objective function value . One way to enhance .rst variation is by expanding the local search to seek a chain of moves instead of just one move . Note that the .rst few moves in this chain might lead to a temporary decrease in objective function value , but overall we seek the chain that leads to a maximal increase . We implement this idea by following the well known Kernighan Lin heuristic for graph partitioning [ 10 ] .
The search for the optimal chain proceeds as follows . We generate a .rst variation move , record the improvement in objective function value , and then ( cid:147)mark(cid:148 ) the moved vector so that it is not moved again in this chain . We repeat this step , say , f times where f is a user de.ned parameter . Finally , we search for that pre.x of moves that leads to the greatest increase in objective function value . This idea is formally described below ( the notation nextFV¡f…lgk l=1 ; U¢ restricts the search for the vector to be moved to the set of unmarked vectors U ) . Given a user supplied tolerance tol > 0 and a number f , do the following : 1 . Start with a partitioning f…(0 ) l gk 2 . Set j = 0 ; U = X and do the following f times : gk l=1= nextFV‡f…(t+j ) l=1 ; U· . l=1 ; U·· ¡ Q‡nextFV‡f…(t+j ) gk l=1· in ObjChange[j ] . gk ment j by 1 .
Mark the vector moved and delete it from U .
Q‡f…(t+j )
( a ) Set f…(t+j+1 ) l=1 . Set the index of iteration t = 0 .
( b ) Record
Incre gk l l l l j=0 ObjChange[j ] j=0 ObjChange[j ] , i =
3 . Set MaxChange = maxiPi and MaxI = arg maxiPi 0 ; 1:: : ; f ¡ 1 . 4 . If MaxChange ‚ tol , increment t by MaxI+1 , and go to step 2 . Otherwise , output the partition f…(t ) l gk and MaxI , and then stop . l=1
Given an initial partitioning f…lgk this KL chain .rst variation algorithm outputs the partition l=1¢ . nextKLFV¡f…lgk
4.3 The Re.nement Algorithm l=1 , tol and f ,
The advantage of ( cid:147)spherical .rst variation(cid:148 ) is that it computes an exact change in the value of the objective function ; however these iterations lead to small increases in the objective function value . On the other hand , k means iterations typically lead to larger increases . To achieve best results we combine these iterations . Our ( cid:147)ping pong(cid:148 ) re.nement algorithm is a two step procedure ( cid:151 ) the .rst step runs spherical k means ; if the .rst step fails the second step runs the Kernighan Lin heuristic outlined in the previous section . The proposed re.nement algorithm is as follows . Given a user supplied tolerance tol > 0 do the following : 1 . Start with a partitioning f…(0 ) l gk 2 . Generate the partition nextKM‡f…(t ) l=1· . l gk l=1· is greater l=1·· ¡ Q‡f…(t ) Q‡nextKM‡f…(t ) l gk l gk l=1 = nextKM‡f…(t ) l=1· , inthan tol , set f…(t+1 ) l gk gk crement t by 1 and repeat step 2 . 3 . Generate the partition nextKLFV‡f…(t ) l=1· . l gk l=1·· ¡ Q‡f…(t ) j=1· is Q‡nextKLFV‡f…(t ) j gk l gk greater increment t by MaxI+1 , l=1· , and go to step 2 . f…(t ) l gk Otherwise , stop . l=1 = nextKLFV‡f…(t ) l=1 . Set the index of iteration t = 0 . than tol , l gk set
If
If l
We emphasize that most of the computations associated with step 3 above have already been performed in step 2 , see ( 7 ) and ( 8 ) . Hence the computational overhead of running a .rst variation iteration just after an iteration of spherical k means is small . Note also that the above algorithm can do no worse than spherical k means . Indeed , as we show below , in many cases the quality of clusters produced is much superior .
5 Experimental Results
In this section , we present experimental results which demonstrate that our proposed algorithm often qualitatively improves k means clustering results . In all our experiments with spherical k means , we tried several initialization schemes varying from random initial partitions to choosing the initial centroids as data vectors that are ( cid:147)maximally(cid:148 ) far apart from each other [ 2 ] .
For our .rst experiment we created the data set described in Example 3.2 setting k = 5 . We observed that all 100 runs of spherical k means with different initializations stopped without changing the initial partition . However , on applying our re.nement algorithm to this initial partitioning and
Table 1 . Confusion matrices for Exam› ple 3.2 ( k = 5 ) .
Table 2 . Confusion matrices for 30 documents with 1073 words .
0 5 0 0 0
5 0 0 0 0
0 …1 0 …2 0 …3 0 …4 5 …5 Obj . fun . value for .nal
0 0 5 0 0
0 0 0 5 0 partition:12.0096
First variation move
…1 …2 …3 …4 …5
2 0 0 0 3
0 2 0 1 2
0 2 1 1 1
0 0 0 4 1
1 1 0 1 2
Obj . fun . value for spherical k means partition:10.8193
12
11
10
9
8
7
6
5
4
3 l e u a v n o i t c n u f e v i t c e b o n j i e s a e r c n i e g a t n e c r e P
2
1
2
4
6
8
10
12
14
# of iterations
Figure 1 . Increase in objective function value for Example 3.2 ( k = 5 , f = 1 ) . setting f = 1 , ie only one .rst variation move is allowed in the KL chain , we were able to recover the optimal clustering in all 100 cases . For a particular run , Table 1 shows the confusion matrices for the initial and .nal partitions . Note that entry(i ; j ) in a confusion matrix gives the number of vectors in cluster i that belong to the true class j ; thus , a diagonal confusion matrix is desirable . Figure 1 shows the percentage increase in objective function value as our algorithm progresses .
For experiments with real life text data , we used the MEDLINE , CISI , and CRANFIELD collections ( available from ftp://ftpcscornelledu/pub/smart ) MEDLINE consists of 1033 abstracts from medical journals , CISI consists of 1460 abstracts from information retrieval papers , while CRANFIELD consists of 1400 abstracts from aerodynamical systems papers .
For our experiments we created three data sets of 30 , 150 , and 300 documents respectively ( see Example 33 ) Each data set was created by an equal sampling of the three collections . After removing stopwords , the document vectors obtained are very high dimensional and sparse . The dimensions for the 30 , 150 and 300 document data sets are 1073 , 3658 and 5577 respectively . In all runs the spherical k means algorithm did not change the initial partition . We then applied our re.nement algorithm to generate the .nal partitions ( using f = 1 ) . These results are summarized in Tables 2 , 3 and 4 by the resulting confusion matrices . In addition , Figures 2 , 3 and 4 plot the percentage increase in objective function values .
…1 …2 …3
5 2 3
1 7 2
2 1 7
…1 …2 …3
9 0 1
1 9 0
0 0 10
Obj . fun . value for spherical Obj . fun . value for .nal k means partition:11.0422 partition:11.9669
First variation move
9
8
7
6
5
4
3
2 l e u a v n o i t c n u f e v i t c e b o j n i e s a e r c n i e g a t n e c r e P
1
1
2
4
6
8
10 # of iterations
12
14
16
Figure 2 . Increase in objective function value for 30 documents ( k = 3 , f = 1 ) .
All the .nal partitions generated have almost diagonal confusion matrices and about 8 % ¡ 25 % higher objective function values , which shows that our ping pong strategy qualitatively improves spherical k means clustering . The confusion matrices in Table 2 and 3 are almost optimal , while the .nal partition generated in Table 4 is qualitatively better than the spherical k means result , but is not optimal . As testi.ed by Figures 2 , 3 and 4 , the signi.cant fraction of the clustering work is done by .rst variation iterations .
To improve the .nal partitioning of the 300 documents and show how KL chain moves lead to a better local optimum , we applied our re.nement algorithm with KL chains to the partitioning of the 300 documents in Table 4 by setting f = 30 . Figure 5 shows that during the .rst 18 moves there is a steady decrease in the objective function value . But from moves 22 to 30 the objective function value increases and becomes greater than the starting value . If we set f < 22 , then the re.nement algorithm will quit without changing the input partition . Figure 5 shows that the .rst KL chain of 30 moves triggers a fresh spherical k means iteration , which substantially increases the objective function value . In the second KL chain , the maximum increase occurs at the 10th .rst variation and all the subsequent moves decrease the objective function value . The resulting confusion matrix shown in Table 5 is almost diagonal .
The following experiment shows that our algorithm is more bene.cial as the number of clusters k increases . For this experiment we used a much larger collection ( cid:151 ) the 20newsgroup data set consists of approximately 20,000 newsgroup postings collected from 20 different usenet newsgroups [ 12 ] . Some of the newsgroups are closely related to
Table 3 . Confusion matrices for 150 docu› ments with 3652 words .
…1 …2 …3
25 5 20
10 18 22
17 10 23
…1 …2 …3
49 1 0
0 49 1
0 0 50
Obj . fun . value for spherical Obj . fun . value for .nal k means partition:28.1934 partition:35.0355
First variation move Spherical k−means move l e u a v n o i t c n u f e v i t c e b o n j i e s a e r c n i t e g a n e c r e P
25
20
15
10
5
0
10
20
30
40 50 # of iterations
60
70
80
90
Figure 3 . Increase in objective function value for 150 documents ( k = 3 , f = 1 ) .
Table 4 . Confusion matrices for 300 docu› ments with 5577 words .
…1 …2 …3
31 26 43
28 36 36
49 34 17
…1 …2 …3
99 1 0
0 100 0
0 19 81
Obj . fun . value for spherical Obj . fun . value for .nal k means partition:52.7753 partition:58.7598 l e u a v n o i t c n u f e v i t c e b o n j i e s a e r c n i e g a t n e c r e P
16
14
12
10
8
6
4
2
0
First variation move Spherical k−means move
10
20
30
40
50
60
70
80
90
100
# of iterations
Figure 4 . Increase in objective function value for 300 documents ( k = 3 , f = 1 ) .
Table 5 . Confusion matrices for the 300 doc› uments . using KL›chains with f = 30 . The objective function values for the .nal partition 595886
99 1 0
0 100 0
0 …1 19 …2 81 …3 Obj . fun . value for
…1 …2 …3
99 1 0
0 100 0
0 0 100 Obj . fun . value for initial partition:58.7598
.nal partition:59.5886
First variation move Spherical k−means move
1.2
1
0.8
0.6
0.4
0.2
0 l e u a v n o i t c n u f e v i t c e b o j n i e g n a h C
−0.2
0
10
20
30
40
# of iterations
50
60
70
Figure 5 . KL›chain moves and change in ob› jective function value for 300 documents ( k = 3 , f = 30 ) .
6
5.5
5
4.5
4
3.5
3
2.5
2
1.5 n o i t c n u f e v i t c e b o j n i e s a e r c n i e g a t n e c r e p
1 20
40
60
80
100
#cluters
120
140
160
180
Figure 6 . The percentage increase in ob› jective function value of our re.nement al› gorithm over spherical k›means on the 20› newsgroup data set ( f = 20 ) . each other ( eg , comp.graphics , composms windowsmisc and compwindowsx ) , while others are unrelated ( eg , alt.atheism , recsportbaseball and scispace ) The headers for each of the messages were removed so that they do not bias clustering results , and 1169 duplicate messages were also taken out of the collection . After removing common stopwords and words that occur in less than 2 documents , the documents are represented by 59534 dimensional vectors , which are more than 99.87 % sparse .
Figure 6 shows the percentage increase in objective function due to the re.nement algorithm over spherical k means as k increases . The main trend is that as k increases , ie , when the average cluster size becomes smaller our re.nement algorithm leads to greater improvement .
6 Related Work
The k means algorithm has been well studied and is one of the most widely used clustering methods [ 5 ] . Some of the important early work is due to Forgy[6 ] and MacQueen[13 ] . In the vector quantization literature , k means clustering is also referred to as the Lloyd Max algorithm[7 ] ; see [ 8 ] for a comprehensive history of quantization and its relations to statistical clustering . Many variants of k means exist ; the version we presented in Section 2 is generally attributed to
Forgy ( see [ 6 , 13 ] ) and is similar to the one given in [ 5 , 1043 ] ; we call this ( cid:147)batch(cid:148 ) k means since the centroids are updated after a batch of points has been reassigned . Another version , which we call ( cid:147)incremental(cid:148 ) k means , randomly selects a single vector x whose re assignment from a cluster …i to a cluster …j leads to a better objective function value ( see [ 5 , 10.8 ] where this incremental algorithm is referred to as ( cid:147)Basic Iterative Minimum Squared Error Clustering(cid:148) ) . Incremental k means is similar to our .rstvariation iterations . The ISODATA algorithm introduces an additional step in each k means iteration , in which the number of clusters is adjusted[1 ] .
However , as we have found , neither the batch nor the incremental version is satisfactory for our purposes . As shown in Section 3 , batch k means can give poor results in high dimensional settings . On the other hand , the incremental version can be computationally expensive . Our main contribution in the paper is the ( cid:147)ping pong(cid:148 ) strategy which exploits the strong points of both batch and incremental kmeans .
Other clustering algorithms use ( cid:147)medoids(cid:148 ) instead of centroids for clustering , for example , the PAM clustering algorithm swaps a single medoid with a non selected object as long as the swap results in an improvement of the quality of the clustering ( see [ 9 , 14] ) . 7 Conclusions and Future Research
In this paper , we have presented a re.nement algorithm that uses local search after completion of spherical k means iterations . The resulting improvements in the quality of clustering can be substantial , especially when the data is highly dimensional and sparse and when clusters contain a small number of data vectors . Note that while many implementations of k means can return empty clusters , our re.nement strategy guarantees that all k clusters will be nonempty .
Future enhancement of our algorithm will allow variable number of clusters at each iteration [ 11 ] . To accomplish this we plan to modify the objective function as follows :
Q!¡k;f…jgk j=1¢ = kXj=1
24Xx2…j xT cj35 + f ( k)! where ! is a scalar parameter and f ( k ) is an increasing function of k . When ! = 0 the trivial partition maximizes the objective function . A negative ! imposes a penalty on the number of clusters , and prevents the trivial partition from becoming the optimal one .
We plan to push this idea further . Instead of keeping ! constant , we plan to select ! at each iteration . The selection will be based on the current partition , and the parameter ! will then become a feedback , and the iterative process can considered to be a discrete control system ( see eg [ 17] ) . We also plan to explore the use of MDL and MML principles to decide on the ( cid:147)right(cid:148 ) number of clusters [ 15 ] .
References
[ 1 ] G . Ball and D . Hall .
ISODATA : a novel method of data analysis and pattern classication Technical report , Stanford Research Institute , Menlo Park , CA , 1965 .
[ 2 ] P . Bradley , U . Fayyad , and C . Reina . Scaling clustering algorithms to large databases . In KDD’03 . AAAI Press , 1998 . [ 3 ] I . S . Dhillon , J . Fan , and Y . Guan . Ef.cient clustering of very large document collections . In Data Mining for Scienti.c and Engineering Applications , pages 357(cid:150)381 . Kluwer Academic Publishers , 2001 .
[ 4 ] I . S . Dhillon and D . S . Modha . Concept decompositions for large sparse text data using clustering . Machine Learning , 42(1):143(cid:150)175 , January 2001 .
[ 5 ] R . O . Duda , P . E . Hart , and D . G . Stork . Pattern Classi.ca tion . John Wiley & Sons , 2nd edition , 2000 .
[ 6 ] E . Forgy . Cluster analysis of multivariate data : Ef.ciency vs . interpretability of classications Biometrics , 21(3):768 , 1965 .
[ 7 ] A . Gersho and R . M . Gray . Vector quantization and signal compression . Kluwer Academic Publishers , 1992 .
[ 8 ] R . M . Gray and D . L . Neuhoff . Quantization . IEEE Trans .
Inform . Theory , 44(6):1(cid:150)63 , 1998 .
[ 9 ] L . Kaufman and P . Rousseeuw . Finding Groups in Data .
Wiley , New York , 1990 .
[ 10 ] B . Kernighan and S . Lin . An ef.cient heuristic procedure for partitioning graphs . The Bell System Technical Journal , 49(2):291(cid:150)307 , 1970 .
[ 11 ] J . Kogan . Means clustering for text data . In MWBerry , editor , Workshop on Text Mining at the 1st SIAM International Conference on Data Mining , pages 47(cid:150)54 , 2001 .
[ 12 ] K . Lang . News Weeder : Learning to .lter netnews .
In Proc . 12th Int’l Conf . Machine Learning , pages 331(cid:150)339 , San Francisco , 1995 .
[ 13 ] J . MacQueen . Some methods for classi.cation and analysis of multivariate observations . In Proceedings of the Fifth Berkeley Symposium on Math . , Stat . and Prob . , pages 281(cid:150 ) 296 , 1967 .
[ 14 ] R . Ng and J . Han . Ef.cient and effective clustering methods for spatial data mining . In Proc . of the 20th Int’l Conf . on Very Large Data Bases ( VLDB ) , pages 144(cid:150)155 , Santiago , Chile , 1994 .
[ 15 ] J . Rissanen . Stochastic Complexity in Statistical Inquiry . Series in Computer Science Vol . 15 . World Scienti.c , Singapore , 1989 .
[ 16 ] G . Salton and M . J . McGill .
Introduction to Modern Re trieval . McGraw Hill Book Company , 1983 .
[ 17 ] E . D . Sontag . Mathematical Control Theory : Deterministic Finite Dimensional Systems . Springer , New York , second edition , 1998 .
[ 18 ] A . Strehl , J . Ghosh , and R . Mooney .
Impact of similarity measures on web page clustering . In AAAI 2000 Workshop on AI for Web Search , pages 58(cid:150)64 , July 2000 .
