Objective Oriented Utility Based Association Mining
Yi Dong Shen
Laboratory of Computer Science
Institute of Software , Chinese Academy of Sciences
Beijing 100080 , P . R . China
Zhong Zhang
School of Computing Science
Simon Fraser University
Burnaby , BC , Canada V5A 1S6
Qiang Yang
Department of Computer Science
Hong Kong University of Science and Technology
Clearwater Bay , Kowloon Hong Kong
Abstract
1 Introduction
The necessity to develop methods for discovering association patterns used to increase business utility of an enterprise has long been recognized in data mining community . This requires modeling specific association patterns that are both statistically ( based on support and confidence ) and semantically ( based on objective utility ) relating to a given objective that a user wants to achieve or is interested in . However , we notice that no such a general model has been reported in the literature . Traditional association mining focuses on deriving correlations among a set of items and their association rules likedia e !bee only tell us that a pattern likefdia e g is statistically related to an item likebee . In this paper , we present a new approach , called Objective Oriented utility based Association ( OOA ) mining , to modeling such association patterns that are explicitly relating to a user ’s objective and its utility . Due to its focus on a user ’s objective and the use of objective utility as key semantic information to measure the usefulness of association patterns , OOA mining differs significantly from existing approaches such as the existing constraint based association mining . We formally define OOA mining and develop an algorithm for mining OOA rules . The algorithm is an enhancement to Apriori with specific mechanisms for handling objective utility . We prove that the utility constraint is neither monotone nor anti monotone nor succinct nor convertible and present a novel pruning strategy based on the utility constraint to improve the efficiency of OOA mining .
@Proceedings of the Second IEEE International Conference on Data Mining , Japan , 2002 .
Observe that most existing approaches to association mining are itemset correlation oriented in the sense that they aim to find out how a set of items are statistically correlated by mining association rules of the form
Association mining is an important problem in data mining . Briefly , given a historical dataset of an application , we derive frequent patterns and association rules from the dataset by using some thresholds , such as a minimum support and a minimum confidence . Since Agrawal ’s pioneer work [ 1 ] , a lot of research has been conducted on association mining . Major achievements include approaches to improving the efficiency of computing the frequent patterns from large datasets [ 2 , 5 ] , approaches to applying constraints to find more interesting patterns [ 3 , 11 , 15 ] , and approaches to eliminating irrelevant association rules by making use of some interestingness measures [ 9 , 14 ] .
1;:: : ; ! 1 ; where , the support of the rule , is the probability of all items 1;:: : ; 1 occurring together , and , the confidence of the rule , is the conditional probability of 1 given the itemsetf 1;:: : ; g . Both and are obAlthough finding correlations of itemsets likedia e ! bee is very important , in many situations people may be specific objective bj that they want to achieve by discov1;:: : ; ! bj ; ; tained simply by counting the frequency of the respective itemsets in a given dataset , and are greater than or equal to the user specified minimum support and minimum confidence , respectively .
( 1 )
( 2 ) more interested in finding out how a set of items support a ering association rules of the form where is the probability that all items 1;:: : ; together with bj hold , is the conditional probability of bj given the itemsetf 1;:: : ; g , and is the utility of the rule , showing to what degree the patternf 1;:: : ; g semantically supports bj . Due to its focus on an objective and semantically support a given objective bj . =f 1;:: : ; g is said to statistically support bj if the support and confidence of the rule ( 2 ) are not below a user specified minimum support and a userspecified minimum confidence , respectively . And is said to semantically support bj if the utility of the rule ( 2 ) is not below a user specified minimum utility . user specified minimum level ( ) . Therefore , OOA the use of objective utility as key semantic information to measure the usefulness of association patterns , we refer to this new type of association mining as Objective Oriented utility based Association ( OOA ) mining , as opposed to traditional Itemset Correlation Oriented Association ( ICOA ) mining .
As a result , all patterns derived in OOA mining must be interesting to an enterprise since when employed , they would increase the ( expected ) utility of the enterprise above the
OOA mining derives patterns that both statistically and Informally , mining has wide applications in many areas where people are looking for objective centered statistical solutions to achieve their goals . For a typical example , in business situations a manager may use OOA mining to discover the best business strategies by specifying his/her objective as “ high profit and low risk of loss . ” Another example is in medical field . A doctor may use OOA mining to find the best treatments for a disease by specifying an objective “ high effectiveness and low side effects . ”
The term utility is commonly used to mean “ the quality of being useful ” and utilities are widely used in decision making processes to express user ’s preferences over decision objects towards decision objectives [ 6 , 13 ] . In decision theory , we have the well known equation “ Decision = probability + utility , ” which says that a decision object is chosen based on its probability and utility . Since association mining can be viewed as a special decision problem where decision objects are patterns , we may well have , correspondingly , an equation “ Interestingness ( of a pattern ) = probability + utility . ” This equation further justifies the necessity and significance of enhancing traditional probability ( support and confidence ) based association mining with objective related utilities .
Since utilities are subjective , they can be acquired from domain experts/users . We would point out , however , that this does not mean we need to acquire a utility for each single item in a dataset . As we will see in Section 3 , it suffices to obtain utilities only for those items in a dataset which are directly related to the given objective . The population of such objective items would be quite small in practical appli cations .
In this paper , we systematically study OOA mining . In Section 3 , we formally define the concepts of objective , support , confidence , and utility under the frame of OOA mining . In Section 4 , we develop an algorithm for mining OOA frequent patterns and rules . The algorithm is based on Apriori , with an enhancement that handles objective utility . Traditional association mining is NP hard , but OOA mining does not seem to be easier . To improve the efficiency of OOA mining , we will present a novel strategy for pruning itemsets based on the support and utility constraints . In Section 5 , we present some experimental results .
2 Related Work
The necessity to develop methods for finding specific patterns which can be used to increase business utility has long been recognized by several researchers [ 7 , 10 , 14 ] . To the best of our knowledge , however , no work on association mining has been reported in the literature which formally models such patterns that are explicitly relating to a user ’s objective and its utility .
Our work is related to but different from existing constrained association mining . Existing constrained association mining , typically represented by the work of Bayardo , Agrawal , and Gounopolos [ 3 ] , Han , Lakshmanan , Ng , Pang and Pei [ 11 , 12 ] , and Srikant , Agrawal and Vu [ 15 ] , takes the formf S!T jCg whereS andT are sets of items andC is a set of constraints on the selection ofS and T . WhenT is not empty , such kind of association minstraintsC is , it always derives asociation rules of the form 1;:: : ; ! 1;:: : ; where both itemsetsf 1;:: : ; g andf 1;:: : ; g satisfyC . Certainly , OOA mining can use f S! bj jC bjg whereC bj is a set of constraints on the selection ofS in terms of the objective bj . Constrained ing item selection , pattern length , set relations ( , , etc. ) , ax S v , i S v , S v , S v and avg S v , whereS is an itemset,v is a real number , and is or ( see [ 12 ] for a summary of types of constraints dis ing belongs to ICOA mining because no matter what con constraints , too . Constrained OOA mining takes the form
OOA mining always derives OOA rules .
Another significant difference between existing constrained association mining and OOA mining is that most exisitng work focuses on SQL style constraints includ cussed in the literature ) . These constraints fall into one of the following four well defined categories : monotone , antimonotone , succinct or convertible . In OOA mining , however , we introduce objective utility as a key constraint . On the one hand , an ( arbitrary ) objective and its utility are difficult , if not impossible , to be formulated using SQL style constraints . On the other hand , the utility constraint is neither monotone nor anti monotone nor succinct nor convert does not . For convenience , we refer to attributes be represented by a logic formula over objective relations
An objective describes anything that we want to achieve or we are interested in . In order to discover patterns in a tal number of records inDB . Finally , for any itemset the function ;DB returns the number of records inDB that are supersets of . datasetDB that support our objective bj , we need first to formulate bj in terms of items ofDB . This can be done by first partitioningDBa into two disjoint subsets : DBa =DB bja [ DB bj a where each attributeA2 DB bja obviously contributes to bj , whereas eachA2 DB bj a inDB bja as objective attributes . LetA be an objective attribute andV its domain . For eachv2V ,A=v is called an objective item or a class of A . We use  a A to denote all classes ofA . Let< be a relation symbol such as=,>,< , etc . For eachv2V , A<v is called an objective relation . An objective can then using the connectives^,_ or : . Formally , we have Definition 1 An objective bj over a datasetDB is a disjunctive normal formC1_:::_C ( 1 ) where eachCi is a conjunctionD1^:::^D ( 1 ) with eachDj being With an objective bj as formulated above , we can then evaluate against a dataset how a pattern =f 1;:: : ; g statistically and semantically supports bj by defining the 1;:: : ; ! bj . In OOA mining , we say an objective bj holds in a record inDB ( or we say supports bj ) if bj is true given . Furthermore , for any itemset = f 1;:: : ; g we say [ f bjg=f 1;:: : ; ; bjg holds in if both bj and all is are true in . We then extend the function ;DB to [ f bjg;DB that returns the number of records inDB in which [ f bjg Definition 2 Let 1;:: : ; ! bj ; ; be an as= f 1;:: : ; ; bjg;DB 100 ; jDBj = f 1;:: : ; ; bjg;DB 100 : f 1;:: : ; g;DB Let bj be an objective andA an objective attribute . Based on bj , the classes ofA can be subjectively classified into three disjoint groups :  a A =  a A [  a   A [  a A where  a A consists of all classes ofA that show positive support for bj ,  a   A of all classes ofA that show negative support for bj , and  a A of all classes ofA that show neither positive nor negative support for bj . Therefore , classes in sociation rule in OOA mining . Then the support and confidence of the rule are respectively given by an objective relation or the negation of an objective relation . support , confidence and utility of the corresponding rule
( 3 )
( 4 ) ible . Therefore , no existing constrained association mining methods are applicable to it . In this work we push the utility constraint deep into OOApriori ( a variant of Apriori ) to prune candicate patterns in order to efficiently derive all OOA rules .
We would point out that although business objectives , such as “ high profit and low risk of loss , ” can be viewed as constraints , such constraints seem to be at a meta level wrt the above mentioned SQL style constraints . Therefore , specific mechanisms are required to represent and handle them . The proposed OOA mining may then be the first such mechanism .
Most recently , Wang , Zhou and Han [ 16 ] and Lin , Yao and Louie [ 8 ] suggested adding values to association rules . The former takes into account the price and quantity of supermaket sales during association mining , while the latter tries to attach a value to every item in a dataset and use the added values to rank association rules . There are three major differences between their approaches and ours . First , we do general objective centered mining by explicitly declaring a user ’s objective and formulating it in a simple , uniform way ( see Section 3 ) . As a result , utilities are assigned only to those items which directly contribute to the objective . Second , we handle both positive and negative utilities , whereas they only consider positive values . Negative utility represents punishment/loss , and it is with negative values that our utility constraints become neither monotone nor anti monotone nor succinct nor convertible . Third , we push the utility constraints into Apriori and use them to prune candidate itemsets . Neither of the above two approaches addressed this .
3 Objective , Support , Confidence , and Utility
Finally , our work is different from existing research on “ interestingness ” [ 9 , 14 ] , which focuses on finding ” interesting patterns ” by matching them against a given set of user ’s beliefs . Informally , a derived association rule is considered ” interesting ” if it conforms to or conflicts with the user ’s beliefs . In contrast , in OOA mining we measure the interestingness of OOA rules in terms of their probabilities as well as their utilities in supporting the user ’s objective . ori algorithm [ 2 ] . A data base or datasetDB is associated with a finite setDBa of attributes . Each attributeAi has a finite domainVi ( continuous attributes can be discretized using methods such as that in [ 4] ) . For eachv2Vi,Ai=v Ak itemset is an itemset withk items.DB consists of a finite set of records/transactions built fromDBa , with each record being a setfA1=v1;:::;A =v g of items where Ai6=Aj for anyi6=j . We usejDBj to denote the to
We assume that readers are familiar with traditional association rule mining , especially with the widely used Apri is called an item . An itemset or a pattern is a set of items . holds . and
 a A will bring bj positive utilities , whereas classes in  a   A bring negative utilities . We then associate each classA=v in  a A or  a   A with a utility A=v ( a real number ) . Since any class in  a A can be considered as a special positive class with a utility0 , we can merge  a A into the positive group . Therefore , in the sequel we always assume that any classA=v belongs to either  a A or  a   A . The groups of positively and negatively supporting classes of a datasetDB for bj are then respectively defined as follows :  a DB = fA=v A=v jA2DB bja andA=v2  a A g and  a   DB =fA=v A=v jA2DB bja and A=v2  a   A g : An OOA itemset ( or OOA pattern ) is a setfA1= v1;:::;A =v g of items withAi2DB bj a Ai6=Aj for anyi6=j . Let be an OOA itemset and a record inDB with . LetC be the set of classes in . The positive utility ( resp . negative utility   ) of for is the sum of the utilities of all positively ( resp . negatively ) supporting classes inC , given by X = A=v2C ^A=v A=v 2  a DB A=v ; X   = A=v2C ^A=v A=v 2  a   DB A=v ; The positive and negative utility ofDB for are then DB = X 2DB^ ;  DB = X 2DB^   : Definition 3 Let 1;:: : ; ! bj ; ; be an association rule with =f 1;:: : ; g an OOA itemset . Let DB = DB    DB . The utility of the rule ( or the itemset ) is given by = DB ;DB Example 1 Let us consider a simplified datasetDB1 about with domainsf1 , 2 , , 5g,f1 , 2 , , 5g andf1 , 2 , 3 , 4g , respectively.R# is not an attribute ofDB1 . perts . The doctor then wants to discover fromDB1 the
It is used to identify records by assigning a unique number to each record . Table 2 shows the degrees of the effectiveness and side effects which are assigned by experienced domain ex medical treatments for a certain disease as shown in Table 1 , where treatment , effectiveness and side effect are attributes
( 5 )
( 6 )
( 7 )
( 9 )
( 8 ) best treatments with high effectiveness and low side effects . Apparently , this is a typical objective oriented utility based mining problem .
Table 1 . A medical datasetDB1 . R# treatment effectiveness side effect
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
1 2 2 2 2 3 3 3 4 4 4 4 5 5 5 5
2 4 4 2 1 4 4 1 5 4 4 3 4 4 4 3
4 2 2 3 3 2 2 4 2 2 2 1 1 1 1 1
Table 2 . Degrees of the effectiveness and side effects . is
4 3 2 1
5 4 3 2 1 side effect very serious serious yet tolerable a little normal
“ high effectiveness with low side effects , ” which divides the set of attributes effectiveness getting much better getting better no obvious effect getting worse getting much worse
The objective bj intoDB bj1a =feffectiveness , side effectg and DB1a DB bj 1a =ftreatmentg . Based on the measurement of the effectiveness and side effects ( Table 2 ) , bj may be ( effectiveness>3)^ ( sideeffect<3 ) . Assume we are given the following groups of  a DB1 =feff=5 1 ;eff=4 0:8 ; eff=3 0 ; id=1 0:6 ; id=2 0 g ,  a   DB1 =feff=1 1 ;eff=2 0:8 ; id=4 0:8 ; id=3 0:4 g : rules of the form “ treatment=k! bj ” wherek is a treatment number , which are composed from the datasetDB1 . positively and negatively supporting classes ( eff stands for effectiveness and sid for side effect ) :
Table 3 shows the supports , confidences and utilities for all formulated by the formula :
Note that the last two rules have quite different utilities for the objective , although their support and confidence are the same . Therefore , “ treatment=5 ” should be the best because it has the highest utility in supporting the objective . rules
4 Mining OOA Rules
4.1 Objective Oriented Apriori
Table 3 . Supports , confidences and utilities . port , minimum confidence and minimum utility , respec bj : ( effectiveness>3)^ ( side effect<3 ) treatment=1! bj 0  1:6 0 treatment=2! bj 50  0:25 12:5 12:5 66  0:066 treatment=3! bj 18:75 75 0:8 treatment=4! bj treatment=5! bj 18:75 75 1:2 Definition 4 LetDB be a dataset and bj an objective . Let , and be a user specified minimum suptively . Let =f 1;:: : ; g be an OOA itemset . is an OOA frequent pattern/itemset inDB if . Let be an OOA frequent pattern . 1;:: : ; ! bj ; ; is an OOA association rule ( OOA rule ) if and . Here , and are as defined in Equations OOA mining is then to derive all OOA rules fromDB . structure using pseudoC itemset =f 1;:: : ; g is internally an instance of the data typedef structf a e ; //store the patternf 1;:: : ; g 1 ; //store ;DB 2 ; //store [ f bjg;DB ; //store DB ( see the formula ( 7 ) )
; //store  DB ( see the formula ( 8 ) )   g ITEMSET ; We use :D to refer to the fieldD of . : 1 , : 2 , : and :   are all initialized to 0 when is created . Moreover , when no confusion would occur , by we refer to its pattern : a e =f 1;:: : ; g . Input : , , , bj andDB . Output:F , the set of OOA frequent itemsets , and
We extend Apriori [ 2 ] to generating OOA frequent patterns and rules by enhancing it with mechanisms for handling objectives and utilities . For convenience , we refer to the extended algorithm as Objective Oriented Apriori ( OOApriori ) .
For the data structure , we associate each OOA itemset with some necessary data fields to record data like counts and utilities . This is done by organizing an itemset into a language . That is , each OOA
Algorithm 1 : Objective Oriented Apriori . type ITEMSET defined as follows :
( 3 ) , ( 4 ) and ( 9 ) , respectively . set int int float float end
15 ) 16 ) 17 ) 18 ) 19 ) 20 ) 21 )
4 ) 5 ) 6 ) 7 ) 8 ) 9 ) 10 ) 11 ) 12 )
AR , the set of OOA rules . function A i i ; ; ; bj;DB 1)AR=F = ; ; 2)k=1 ; 3)Ck=f j is an OOA1 itemset inDBg ; //Part 1 : Collect counts and utilities ofk itemsets for each record inDB for eachk itemset 2Ck if then begin : 1++ ; : = : ; :  = :     ; if bj holds in then : 2++ //Part 2 : Check for frequent patterns ( k ) and rules ( AR ) 13 ) k= ; ; 14 ) for each =f 1;:: : ; kg2Ck if = : 2 jDBj then begin k= k[f g ; = : 2 : 1 ; = :   :   : 1 ; if and then AR=AR[f 1;:: : ; k! bj ; ; g //Part 3 : Generate ( k+1) itemsets 22 ) if k6= ; then begin k++ ; Ck=a i iGe k 1 ; //New candidate itemsets 27 ) returnF =Si i andAR In Algorithm 1 , for eachk1Ck is used to store candidate frequent OOAk itemsets , k to store frequent OOA k itemsets , andAR to store all OOA rules . OOApriori conthe datasetDB and applies each record inDB to countutilities of each candidate itemset inCk . At lines 8 and 9 , and   are as defined in Equations ( 5 ) and dence and utility of each candidate itemset =f 1;:: : ; kg inCk against the three user specified minimums , and to see if is an OOA frequent pattern and 1;:: : ; k! bj is an OOA rule . After all OOA frequentk itemsets and rules have been generated , the third ( k 1) itemsets based on k by calling the following functiona i iGe . This function is borrowed from Apriori functiona i iGe k ing the frequency and computing the positive and negative
( 6 ) . The second part ( lines 13 21 ) checks the support , confi part ( lines 22 26 ) of OOApriori generates new candidate sists of three major parts . The first part ( lines 4 12 ) scans
23 ) 24 ) 25 ) 26 ) end goto 4 ) end end
[ 2 ] .
2 ) 3 ) 4 )
5 ) 6 ) 7 ) end
//Prune itemsets generated , the process goes to the next cycle ( line 25 ) for
Theorem 2 OOApriori is sound and complete in the sense ori will continue this way until no new OOA frequent itemsets can be generated ( line 22 ) .
1)Ck 1= ; ; for each pair of itemsets in k of the form f 1;:: : ; k 1 ; kg andf 1;:: : ; k 1 ; k 1g Ck 1=Ck 1[ff 1;:: : ; k 1gg ; for each 2Ck 1 if somek sub itemset of is not in k then Ck 1=Ck 1 f g ; //Remove fromCk 1 8 ) returnCk 1 After the setCk 1 of new candidate itemsets has been deriving OOA frequent ( k 1) itemsets and rules . OOApriTheorem 1 If =f 1;:: : ; g is an OOA frequent pattern and fl with 6= ; , then is an OOA frequent pattern . that is an OOA frequent itemset if and only if 2F and that 1;:: : ; ! bj ; ; is an OOA rule if and only if it is inAR . =f 1;:: : ; g passes/violates the confidence or the utility constraint , we mean that the OOA rule 1;:: : ; ! bj been identified in the literature [ 11 , 12 ] . LetC be a constraint andS1 andS2 be two arbitrary itemsets . ForS1fl S2,C is anti monotone ifS1 violatingC impliesS2 violatesC , andC is monotone ifS1 satisfyingC impliesS2 satisfiesC . IfC is succinct thenS1 andS2 satisfyingC impliesS1[S2 satisfiesC.C is convertible if there exists an orderR on items such that for any itemsetS satisfying C , every prefix ofS wrtR satisfiesC . OOApriori we can safely delete an itemset from k when cause no frequent patterns will be built from .
Theorem 2 shows the correctness of applying OOApriori to computing OOA frequent itemsets and rules . In this section we develop a pruning strategy to improve its efficiency . Here and throughout , when we say that an OOA itemset its support is below the minimum support ( see line 15 ) beIt turns out , however , that neither the confidence nor the utility constraint for OOA rules is anti monotone .
Theorem 1 assures us that the support constraint for OOA frequent patterns is anti monotone . Therefore , in
4.2 A Pruning Strategy for Mining OOA Rules
Four types of constraints for association mining have passes/violates the constraint . has passed the support constraint but violates either the
The pruning problem is then described as follows : For
OOA frequent itemsets from which no OOA rules would be possibly built . ing mechanism , OOApriori will generate all OOA frequent items , many of which may produce no OOA rules because of the violation of the confidence or the utility constraint .
We present a pruning strategy using the support and utility constraints . To describe the pruning strategy , we add two more data fields to the internal structure of an OOA itemset any itemset in k ( see the OOApriori algorithm ) that confidence or the utility constraint , can we delete from k without missing any OOA rules ? Without any prunLook at the functiona i iGe k again . Since all k 1 itemsets are composed from thek itemsets in k , we need to keep k as small as possible by removing some as shown below : typedef structf a e ; //store the patternf 1;:: : ; g 1 ; //store ;DB 2 ; //store [ f bjg;DB ; //store DB
; //store  DB   2 ; //storejS j   ; //store the least negative utility g ITEMSET ; Here , letS be the set of records inDB in which [ f bjg holds andS be the set of records inS which contain  a DB ) , then the first new field 2 is used to storejS j ( note that the field 2 storesjSj ) and the second new field  is used to store the least negative utility of a record inS S , ie    for any in S S Strategy 1 Remove any OOA itemset =f 1;:: : ; kg :   B  from k if : 2< jDBj and jDBj< , where B = jDBj  : 2 :  . Since is a frequent OOA itemset , there are at least jDBj records injDBj in which [ f bjg holds . When : 2< jDBj , there are at least jDBj  : 2 records inDB in which [ f bjg holds that contain negative classes . Therefore , B >0 is the least negative utility ofDB for and thus is the lower bound of :   . As a result , :   B  of the utility ofDB for . To sum up , this strategy says that an OOA frequent itemset is removable if the upper bound no negative class ( ie , all classes of these records are in set int int float float int float is the upper bound
.
Theorem 3 The utility constraint for OOA rules is neither monotone nor anti monotone nor succinct nor convertible . of its expected utility is below the minimum utility . The following theorem shows that applying this strategy will not miss any OOA rules .
It is easy to push Strategy 1 into the OOApriori algorithm . This is done by replacing lines 14 21 of Algorithm 1 with the following lines :
Theorem 4 Let =f 1;:: : ; kg be an OOA frequent item :   B  set . If : 2< jDBj and jDBj< then there is no OOA itemset =f 1;:: : ; g such that 1;:: : ; ! bj is an OOA rule . 14 ) for each =f 1;:: : ; kg2Ck if = : 2 jDBj then begin k= k[f g ; = : 2 : 1 ; = :   :   : 1 ; if and then AR=AR[f 1;:: : ; k! bj ; ; g ; B = jDBj  : 2 :  ; :   B  if : 2< jDBj and jDBj < then k= k f g //by Strategy 1 datek itemset inCk , if it passes the support constraint then it is added to k ( lines 15 and 16 ) . If it also passes both from is added toAR ( lines 17 20 ) . Otherwise , when itemsets from k from which no OOA rules will be proTheorems 2 and 4 . That is , 1;:: : ; ! bj ; ; is an OOA rule if and only if it is inAR . passes the support constraint but violates either the confidence or the utility constraint , our pruning strategy is applied ( lines 20 1 to 20 5 ) to remove some OOA frequent duced . The correctness of the OOApriori algorithm enhanced with the pruning strategy follows immediately from
15 ) 16 ) 17 ) 18 ) 19 ) 20 ) 20 1 ) 20 2 ) 20 3 ) 20 4 ) 20 5 ) 21 ) the confidence and the utility constraint , an OOA rule built
The above procedure works as follows : For each candi else begin end end
5 Experimental Evaluation
We show the effect of applying our pruning strategy by empirical experiments . We choose the widely used German Credit dataset from the UCI Machine Learning Archive ( ftp://ftpicsuciedu/pub /machine learningdatabases/statlog/german/ ) . This dataset consists of 1000 records ( each record represents a customer ) with 21 attributes such as Status , Duration , Credit history , Purpose , Employment , etc . The last attribute Conclusion classifies a customer as good or bad in terms of his/her credits . The reason we use this dataset in our experiment is that its attributes are semantically easy to understand so that we can flexibly create different objectives from them to test our approach . We build four datasets with different sizes from the 1000 records.DS1 consists of 600 records,DS2 of 700 records ,
Liable people , Foreign and Conclusion , and the objective tomers whose credit is good and who either are not foreign workers or have more than one person being liable to provide maintenance for the credit account . All the remaining eighteen attributes are treated as non objective attributes . The utilities of the major classes of the objective are defined
, andDS4 of 900 records . The objective attributes are bj is defined as ( Conclusion=good)^ ( Liable people=2 _ Foreign=no ) . That is , suppose we are interested in cusin Table 4 where we normalize the utilities into[0;100℄ .  a DB ( 10 ) 2 ( 20 )  a   DB ye ( 10 ) 1 ( 20 ) Let 1 and 2 be the sizes of the two sets of OOA can2  1 2 OOApriori algorithm . On average , they pruned8  9 didate itemsets generated by OOApriori with and without applying Strategy 1 , respectively . We evaluate the effect of applying Strategy 1 to pruning OOA itemsets by demonstrating its itemset reduction rate defined by . Figure 1 shows our experimental results on the itemset reduction rates where we use different minimum utilities while keeping the minimum support and minimum confidence unchanged . The results strongly demonstrate that applying our pruning strategy can greatly improve the efficiency of the g d ( 70 ) bad ( 70 )
Table 4 . Class utilities .
Liable people
Conclusion
Foreign of the candidate itemsets during the mining process . Figure 2 further demonstrates the effectiveness of the pruning strategy , where we use the same minimum confidence and minimum utility while letting the minimum support vary .
25
20
15
10
5
0
DS1 DS2 DS3 DS4
8.12
2.42
5.96
2.15
4.58
3.82 0.67
20.66
18.98 12.81
10.53
12.96
6.22
12.17
5.51
1
2
5
10
15
Minimum Utilities
Figure 1 . The itemset reduction rates against minimum utilities .
6 Conclusions
We have developed a new approach to modeling association patterns . OOA mining discovers patterns that are
35
30
25
20
15
10
5
0
29.20
28.90
25.14
28.24
23.11
20.32
19.05
23.83
23.83
23.11 16.54
19.56
15.79
14.67
14.66
13.55
DS1 DS2 DS3 DS4
20.65
18.50
12.81
10.30
0.50 %
0.60 %
0.70 %
0.80 %
0.90 %
Minimum Supports
Figure 2 . The itemset reduction rates against minimum supports . explicitly relating to a given objective that a user wants to achieve or is interested in and its utility . As a result , all OOA rules derived from a dataset by OOA mining are useful because applying them would increase business utility of an enterprise . This shows a significant difference from traditional association mining .
We developed an algorithm for mining OOA frequent patterns and rules . The algorithm is an enhancement to Apriori with specific mechanisms for handling objective utility . Since the utility constraint is neither anti monotone nor monotone nor succinct nor convertible , finding effective pruning strategies is of great significance . We developed a novel pruning strategy for mining OOA rules by combining the support and utility constraints . As far as we can determine , no similar work has been reported in the literature .
Acknowledgement
Yi Dong Shen is supported in part by Chinese National Natural Science Foundation , Trans Century Training Program Foundation for the Talents by the Chinese Ministry of Education , and Foundations from Chinese Academy of Sciences . Qiang Yang thanks NSERC and IRIS III program for their support .
References
[ 1 ] R . Agrawal , T . Imilienski , and A . Swami . Mining association rules between sets of items in large datasets . In SIGMOD , pages 207–216 , 1993 .
[ 3 ] R . Bayardo , R . Agrawal , and D . Gounopolos . dense
Constraint based rule mining in large , databases . In ICDE , pages 188–197 , 1999 .
[ 4 ] J . Dougherty , R . Kohavi and M . Sahami .
Supervised and unsupervised discretization of continuous features . ICML , 1995 .
[ 5 ] J . Han , J . Pei , and Y . Yin . Mining frequent patterns without candidate generation . In SIGMOD , pages 1– 12 , 2000 .
[ 6 ] R . Howard . Risk preference .
In R . Howard and J . Matheson , eds . Readings in Decision Analysis , pages 429–465 , 1977 .
[ 7 ] J . Kleinberg , C . Papadimitriou , and P . Raghavan . A microeconomic view of data mining . Journal of Data Mining and Knowledge Discovery , 6(1):83–105 , 1998 .
[ 8 ] T . Lin , Y . Yao , and E . Louie . Value added association rules . In PAKDD , pages 328–333 , 2002 .
[ 9 ] B . Liu , W . Hsu , S . Chen , and Y . Ma . Analyzing the subjective interestingness of association rules . IEEE Intellgent Systems , 15:47–55 , 2000 .
[ 10 ] B . Masand and G . Piatetsky Shapiro . A comparison of approaches for maximizing business payoff of prediction models . In KDD , pages 195–201 , 1996 .
[ 11 ] R . Ng , L . Lakshmanan , J . Han , and A . Pang . Exploratory mining and pruning optimizations of constrained association rules . In SIGMOD , pages 13–24 , 1998 .
[ 12 ] J . Pei and J . Han . Constrained frequent pattern mining : a pattern growth view . ACM SIGKDD Explorations ( Special Issue on Constrained Data Mining ) 2(2 ) , 2002 .
[ 13 ] S . Russell and P . Norvig . Artificial Intelligence : A Modern Approach . Englewood Cliffs , NJ : Prentice Hall , 1994 .
[ 14 ] A . Silberschatz and A . Tuzhilin . What makes patterns interesting in knowledge discovery system . IEEE Trans . on Knowledge and Data Engineering , 8:970– 974 , 1996 .
[ 15 ] R . Srikant , Q . Vu , and R . Agrawal . Mining association In KDD , pages 67–73 , rules with item constraints . 1997 .
[ 16 ] K . Wang , S . Zhou and J . Han . Profit mining : from
[ 2 ] R . Agrawal and R . Srikant . Fast algorithm for mining patterns to actions . In EDBT , pages 70 87 , 2002 . association rules . In VLDB , pages 487–499 , 1994 .
