Speed up Iterative Frequent Itemset Mining with Constraint Changes
Gao Cong Bing Liu
School of Computing , National University of Singapore , Singapore 117543
E mail : {conggao , liub}@compnusedusg
Abstract
Mining of frequent itemsets is a fundamental data mining task . Past research has proposed many efficient algorithms for the purpose . Recent work also highlighted the importance of using constraints to focus the mining process to mine only those relevant itemsets . In practice , data mining is often an interactive and iterative process . The user typically changes constraints and runs the mining algorithm many times before satisfied with the final results . This interactive process is very time consuming . Existing mining algorithms are unable to take advantage of this iterative process to use previous mining results to speed up the current mining process . This results in enormous waste in time and in computation . In this paper , we propose an efficient technique to utilize previous mining results to improve the efficiency of current mining when constraints are changed . We first introduce the concept of tree boundary to summarize the useful information available from previous mining . We then show that the tree boundary provides an effective and efficient framework for the new mining . The proposed technique has been implemented in the contexts of two existing frequent itemset mining algorithms , FP tree and Tree Projection . Experiment results on both synthetic and reallife datasets show that the proposed approach achieves dramatic saving in computation .
1 . Introduction
Frequent itemset mining plays an essential role in mining association rules [ 3 ] , correlations , sequential patterns , maximal patterns [ 4 ] , etc . Although many efficient algorithms [ 3 , 16 , 1 , 9 ] have been developed , mining of frequent itemsets remains to be a time consuming process [ 10 ] , especially when the data size is large . To make the matter worse , in most practical applications , the user often needs to run the mining algorithm many times before satisfied with the final results . In each process , the user typically changes some parameters or constraints .
Considering a mining task with only the minimum support constraint ( also called the frequency constraint ) , the user may initially set the minimum support to 5 % and run a mining algorithm . After inspecting the returned results , s/he finds that 5 % is too high . S/he then decides to reduce the minimum support to 3 % and runs the algorithm again . Usually , this process is repeated many times before s/he is satisfied with the final mining results .
This interactive and iterative mining process is very time consuming . Mining the dataset from scratch in each iteration is clearly inefficient because a large portion of the computation from previous mining is repeated in the new mining process . This results in enormous waste in computation and time . So far , limited work has been done to address this problem , and to the best of our knowledge there is still no effective and efficient solution .
In recent years , many constraints ( apart from the traditional support and confidence constraints ) are introduced into frequent itemset mining in order to find only those relevant itemsets [ 13 , 10 , 12 ] . On one hand , these additional constraints give the user more freedom to express his/her preferences . On the other hand , however , it often prolongs the mining process because the user may want to see the results of various combinations of constraint changes by running the mining algorithm more times . This makes mining using previous results for efficiency even more important .
Constraint changes can mean tightening constraints and relaxing constraints . Let us use an example to start the discussion .
Example : Consider that one sets the constraint that the average price of the items in an itemset is less than $100 in the old mining process ( for a market basket problem ) . After inspecting the mining results , one finds that the results are not satisfactory . There are possible two reasons : ( 1 ) $100 is too low ( many useful itemsets may not be discovered ) , and ( 2 ) $100 is too high ( too many itemsets are generated ) . One may wish to change the average price to $150 or to $80 for the new mining . The question is “ can we make use of the results from the old mining to speed up the new mining ? ”
It is straightforward to answer part of the question , ie , when constraints are tightened ( the solution space is reduced ) , eg , when the average price of frequent itemsets is decreased . To obtain the new set of frequent itemsets under the new constraints , we can simply check the frequent itemsets from the old mining to filter out those itemsets that do not satisfy the new constraints . This filtering process is sufficient because the set of new frequent itemsets is only a subset of the old set .
When constraints are relaxed ( the solution space is expanded ) , the problem becomes non trivial as re running the mining algorithm is needed to find those additional frequent itemsets . For instance , in the above example , when the average price of frequent itemsets is increased , more itemsets may be generated . The problem becomes even more complicated when multiple constraints are changed at the same time . The objective of this work is to study how to make use of the previous mining results to speed up re mining when constraints are changed .
In this paper , we propose a novel technique to solve this problem . Using the relaxation of frequency constraint ( the decrease of minimum support ) as an example , we first propose the concept of tree boundary to summarize and to reorganize the previous mining results . We then show that the additional frequent itemsets can be generated in the new mining process by extending only the itemsets on the tree boundary without re generating the frequent itemsets produced in the previous mining ( note that our tree boundary based technique is quite different from the incremental mining approaches based on negative border ) . The proposed technique has been implemented in the contexts of two frequent itemset mining algorithms , FPtree [ 9 ] and Tree Projection [ 1 ] . This results in two augmented itemset mining algorithms RM FP ( re mining using FP tree ) and RM TP ( re mining using Tree Projection ) . Extensive experiments on both synthetic data and real life data show that RM FP and RM TP dramatically outperform FP tree and Tree Projection algorithm respectively . Finally , we also address how the proposed technique can be applied to handle the changes of other types of constraints given in previous studies [ 13 , 10 , 12 ] .
2 . Related work
Frequent itemset mining has been studied extensively in the past eg in [ 3 , 16 , 1 , 9 , 15 , 4 , 5 ] . Most current algorithms are variations of the Apriori algorithm [ 3 ] . They use support based generate and test approach to find all the frequent itemsets . Recently , some tree based algorithms were also proposed , eg , the FP tree algorithm [ 9 ] , which is based on the frequent pattern tree , and Tree Projection algorithm [ 1 ] , which is based on the lexicographic tree . Both algorithms do not strictly follow the Apriori like candidate generate and test approach and were shown to be more efficient than the Apriori algorithm [ 3 ] .
Since [ 13 ] first introduced item constraints to produce only those useful itemsets , many other types of constraints have been integrated into itemset mining algorithms [ 10 , 12 ] . Although many efficient algorithms for mining frequent itemsets with constraints exist , user interaction is at the minimum level . To remedy this situation , [ 10 ] proposes to establish breakpoints in the mining process to accept user feedback to guide the mining . Furthermore , online association rule mining also allows the user to increase minimum support during the mining process [ 2 ] . However , [ 2 ] does not allow decreasing of minimum support . Similarly , the support threshold used in [ 11 ] for incremental and interactive sequence pattern mining can also be increased but not decreased .
The closely related work to ours is the incremental mining , where the concept of negative border ( proposed in [ 16 ] ) is utilized to update the mining results when additional data becomes available [ 14 , 15 , 8 , 11 ] . A negative border consists of all the itemsets that are candidates of the Apriori algorithm that do not have sufficient support . Although the methods in [ 14 , 15 , 8 ] only need one scan of the updated dataset , they could not avoid the disadvantage of negative border , ie , maintaining a negative border is very memory consuming and is not well adapted for very large databases [ 11 ] .
The approach in [ 14 , 15 , 8 ] seemingly can be adapted for handling constraint relaxation . [ 15 ] actually mentions the possibility but no detailed algorithm is proposed . However , one significant shortcoming of the approach is that generating candidates under new constraints using the negative border under old constraints usually result in over generation of a huge number of useless candidates . This makes the approach in [ 14 , 15 , 8 ] impractical for our constraint relaxation problem for large datasets , especially when the minimum support is low . For example , if 105 frequent itemsets are obtained given minimum support of 1 % and 50 1 itemsets become frequent after minimum support is reduced to 0.9 % , the number of candidate itemsets generated using the above approach is ( 250 1)*105 1020 even if we do not consider the expansions of 105 frequent itemsets themselves . This is clearly impractical .
FUP in [ 6 ] is another incremental mining method that follows the Apriori framework . FUP is not for mining with constraint changes . If it is applied to our task , it basically re runs the Apriori algorithm without re counting the supports of those itemsets generated previously ( they still need to be re generated ) . The computation saving is thus very limited , if any , because of some overheads ( see [ 7 ] for more details ) .
3 . Problem statement
Let I be the set of all items , and G be a transaction database . Each transaction in G consists of a subset of items in I . Let S ( ˝ I ) be an itemset . The support of S ( denoted by Support(S ) ) is defined as in [ 3 ] . Given a minimum support MinSup , an itemset S is frequent in G if Support(S ) ‡ MinSup . With a transaction set G and a MinSup , the problem of frequent itemset mining is to find the complete set of frequent itemsets in G .
Constraints can be imposed on both itemset S itself and its attributes ( eg , price , type , etc ) in frequent itemset mining . There are many types of constraints that can be imposed on frequent itemset mining . Four categories of
» constraints : anti monotone , monotone , succinct , and convertible constraints have been effectively integrated into some mining algorithms [ 10 , 12 ] .
Iterative mining of frequent itemsets with constraint changes : Given a transaction database G , the whole process of iterative ( and interactive ) mining of frequent itemsets with constraint changes is captured with the following iterative steps : specify the initial set of constraints SC .
( 1 ) ( 2 ) run the mining algorithm ( 3 ) check the returned results to determine whether they are satisfactory . If so , the mining process ends . Otherwise , the user changes one or more constraints in SC ( including deletion and addition of constraints ) , and the process then goes to ( 2 ) .
( 1 ) and ( 3 ) will not be discussed further in the paper as it is the user ’s responsibility to devise and to change constraints . Our objective is to design a framework for the mining algorithm in ( 2 ) so that it is able to leverage on the mining results from the previous mining iteration to improve the current mining , and consequently speed up the whole data mining process . the efficiency of
Constraint changes : Change of a constraint includes two cases :
( 1 ) Tighten the constraint : The solution space is reduced . For example , when the minimum support is increased .
( 2 ) Relax the constraint : The solution space is expanded . For example , the minimum support is decreased . Constraint changes mean changes to one or several constraints in a set of pre defined constraints . The changes cover deletion or addition of constraints . Adding a new constraint corresponds to tightening the constraint , while deleting an existing constraint corresponds to relaxing the constraint .
As discussed earlier , if a constraint C is tightened to C¢ , the set of itemsets that satisfy the new constraint C¢ is only a subset of the itemsets that satisfy the old constraint C . Thus , the set of itemsets that satisfy C¢ can be obtained by filtering the set of itemsets that satisfy C . The challenge comes when a constraint C is relaxed to C¢ . The set of itemsets that satisfy the old constraint C is only a subset of the itemsets that satisfy the new constraint C¢ . The problem is how to efficiently discover the set of itemsets Fn that satisfy the new constraint C¢ but not the old constraint C . The rest of the paper focuses on this problem . We also study how to utilize the previous mining results to efficiently discover the set of itemsets when multiple constraints are changed at the same time .
4 . The proposed technique
We use the minimum support constraint as an example to present the proposed technique for finding the set of itemsets Fn that satisfy the new but not the old minimum support when the minimum support is reduced ( relaxed ) from one mining process to the next . The relaxation problems of the other constraints can be solved within the proposed framework ( to be discussed in Section 7 ) , although the technical details may vary .
Let MinSupold be the minimum support used in the previous ( or old ) mining , and MinSupnew be the relaxed ( or new ) minimum support . This section first introduces the useful information that can be obtained from the previous mining process using a itemset mining framework . The reason that we use a tree based framework will become clear later . We then describe a method to represent the old information for the purpose of mining under MinSupnew . Next , we present a naïve approach and the proposed technique for discovering the set of itemsets Fn that are frequent under MinSupnew but not MinSupold . tree based
41 Useful information from previous mining
After running a mining algorithm using MinSupold , we find the set of frequent itemsets . One byproduct of the process is the set of itemsets that are checked against MinSupold ( supports are counted ) but are not frequent . Let Lf be the set of frequent itemsets under MinSupold , and Lif be the set of itemsets that are counted , but found infrequent ( the byproduct ) . Although all frequent itemset mining algorithms generate the same set Lf , the set of infrequent itemsets Lif checked in the process varies according to algorithms .
Algorithms , such as those in [ 4 , 1 , 9 ] , do not strictly follow the candidate generation of Apriori like algorithms [ 3 , 16 , 10 ] . Instead , they are based on some kinds of tree . We classify these algorithms as tree based algorithms . Tree based algorithms will count the support of an itemset S = {i1 , i2 , … , ik} if two proper subsets of S , namely S1 = {i1 , … , ik 2 , ik 1} and S2 = {i1,… , ik 2 , ik} , are frequent .
We use tree based mining algorithms as the underlying mining framework of our proposed technique because treebased mining algorithms give us sufficient information , while Apriori like algorithms do not ( see the end of the Section ) . [ 1,9 ] also show that tree based algorithms are actually more efficient in many cases .
As in [ 1 ] , we use a lexicographic tree to represent the set of frequent itemsets Lf . Given the set of items I , it is assumed that a lexicographic order R exists among the items in I . The order R is important for efficiency and for the organization of mining results . We use the notation i £ L j to denote that item i occurs lexicographically earlier than j . Definition 4.1 ( Lexicographic Tree ) A node in a lexicographic tree corresponds to a frequent itemset . The root of the tree corresponds to the null itemset .
We extend Definition 4.1 to also represent those itemsets in Lif with a lexicographic tree . An example lexicographic tree is shown in Figure 1 . Those nodes enclosed in circles are frequent itemsets under MinSupnew but not MinSupold , which are in Fn . Those nodes enclosed by dotted squares are the itemsets in Lif that are not frequent under either MinSupold or MinSupnew . The other nodes are itemsets that are frequent under both MinSupold and MinSupnew . Let P and Q be two itemsets and Q be the parent of P .
Definition 4.2 ( Tree Extensions ) A frequent 1extension of an itemset such that the last item is the contributor to the extension is called a tree extension . The list of tree extensions of a node P is denoted by E(P ) . null
{1}
{2}
{3}
{4}
{5}
{6}
{7}
{8}
{3,4}
{3,5}
{3,6}
{3,7}
{4,5} {4,6} {4,7}
{5,6}
{5,7}
{6,7}
{3,4,6}
{4,5,6}
{4,5,7}
{4,6,7}
In Figure 1 , under MinSupold , the list of tree extensions of node 3 E(3 ) = <4 , 6> .
Definition 4.3 ( Candidate Extensions ) The list of candidate extensions of a node P is defined to be those items in E(Q ) that occur lexicographically after the node P . We denote the list by C(P ) . Note that E(P ) is a subset of C(P ) .
Items in C(P ) are possible frequent extensions of P . Under MinSupold , the tree extensions of null node E(null ) = <3 , 4 , 5 , 6 , 7> ( note that 2 is not frequent under MinSupold ) , and the candidate extensions of node 3 C(3 ) = <4 , 5 , 6 , 7> .
42 Extensions of lexicographic tree
This subsection extends the lexicographic tree with some new conceptions , which will be used in our proposed technique .
Definition 4.4 ( Infrequent Borders ) If a 1 extension i of itemset P is not frequent , i is called an infrequent border . The list of infrequent borders of a node P is denoted by IB(P ) . We have the relationship : IB(P ) = C(P ) E(P ) .
In Figure 1 , under MinSupold , the infrequent borders of node 3 IB(3 ) = <5 , 7> .
Definition 4.5 ( New Tree Extensions ) If itemset P ¨
{i} , i ˛ IB(P ) , becomes frequent after MinSup is reduced from MinSupold to MinSupnew , i is called a new tree extension of node P wrt MinSupnew . The list of new tree extensions of node P wrt MinSupnew is denoted by NTE(P ) .
In Figure 1 , the list of new tree extensions of node 3 wrt MinSupnew NTE(3 ) = <5 , 7> .
For any frequent itemset P ( can be null ) under MinSupold , its tree extensions E(P ) and infrequent borders IB(P ) are stored for mining under MinSupnew . Its new tree extensions NTE(P ) wrt MinSupnew can be obtained by checking the list of infrequent borders of P , IB(P ) . Under MinSupold , the set of tree extensions of all frequent tree nodes makes up Lf , and the set of infrequent borders of all frequent nodes in the tree makes up Lif .
43 A naïve approach
With the two sets Lf and Lif from the mining under MinSupold , we first look at a naïve approach to making use of previous mining results for the new mining . We then present the proposed approach based on tree boundary .
The naïve approach checks all itemsets in Lf and Lif one by one to find the change of their candidate extensions under MinSupnew , and to extend them to obtain the complete set Fn ( in which itemsets are frequent under MinSupnew but not MinSupold ) . Figure 2(a ) shows the children itemsets of null node and the children itemsets of itemset {3} in the naïve approach . To make the figure manageable , we assume that itemset {3 , 8} is frequent under MinSupnew but {4 , 8} , {5 , 8} , {6 , 8} , and {7 , 8} are not . Candidate extensions of each node are shown under the node in Figure 2(a ) . The only saving in the new mining is that we can utilize the count information saved previously for those itemsets in Lf and Lif . the main computation comes from
However , this saving in computation is very limited in a tree based algorithm . Thus , the computation is basically the same as re mining from scratch . In tree based algorithms , the generation of projected transactions for each node . Project transactions for an itemset S are the set of transactions containing S . Tree based algorithms use this subtransaction set for counting support and for all subsequent itemset ( containing S ) generations . This naïve approach still requires the same computation to generate the projected transactions as running a tree based algorithm from scratch . For instance in Figure 2(a ) , we still need to create projected transactions for {3} to count the support for itemset {3 , 8} although the supports of its other children itemsets {3 , 4} , {3 , 5} , {3 , 6} and {3 , 7} are known previously ( the projected transactions for {3} are also used to generate the projected transactions for children itemsets of {3} ) . Similar computation is required for creating projected transactions for {2} , {3} , {4} , {5} , {6} and {7} .
Another shortcoming of the naive approach is that it cannot avoid re generating itemsets in Lf because they need to be extended in the new mining . For example , in Figure 2(a ) , itemsets {3} , {4} , {5} , {6} and {7} still need to be generated to check whether item 8 is in their tree extensions although their supports are already counted in previous mining . null null
{3}
{7} <3,4,5,6,7,8> <4,5,6,7,8> <5,6,7,8> <6,7,8> <7,8> <8> … …
{6}
…
…
{4}
{5}
{8}
{2}
{8}
<3,4,5,6,7,8> <3,4,5,6,7>
…
{2}
…
{3,4}
{3,6} <5,6,7,8> <6,7,8> <7,8>
{3,5}
…
…
…
{3,8}
{3,7} <8> …
   
   
{8,3}
…
{3,5} <4,6,7>
{3,7} <4,6>
… ! ! ! !
" " $# " "
… %'& &)( * + , . % %'& &)( * + , . % %'& &)( * + , . % %'& &)( * + , . %
Based on the above discussion , we see that saving by the naive approach is limited . It is thus not efficient .
44 The proposed approach
Definition 4.6 ( Tree Boundary ) A tree boundary wrt MinSupnew is defined to be the set of itemsets TB = {tb | tb Lif , Support(tb ) ‡ MinSupnew} , where Lif is the set of counted but infrequent itemsets under MinSupold , and Support(tb ) is the support of itemset tb .
For example , the itemsets on the dotted line shown in Figure 1 make up the tree boundary wrt MinSupnew . Itemsets {1} and {3 , 4 , 6} are not in TB although they are in Lif because they are not frequent under MinSupnew .
Our proposed approach discovers the complete set of Fn by extending only the itemsets on the tree boundary . The basic idea is to eliminate the effect of MinSup decrease on itemsets in Lf , ie , no itemset will be extended if it has been extended in previous mining . This is achieved by changing the order of tree extensions of every node ( including the null node ) in Lf ( under MinSupold ) .
Let Sp be null node or any itemset in Lf . Tree extensions of Sp under MinSupnew , denoted by Enew(Sp ) , contains two parts : tree extensions of Sp under MinSupold , Eold(Sp ) , eg , Eold(3 ) = <4 , 6> , and new tree extensions of Sp ( wrt MinSupnew ) ,
NTE(Sp ) , eg , NTE(3 ) = <5 , 7> .
We change the item order of Enew(Sp ) as follows : move items from the new tree extensions , NTE(Sp ) , to the front of the ( old ) tree extensions of Sp under MinSupold , Eold(Sp ) . For example , in Figure 1 , we change the tree extensions of null under MinSupnew from <2 , 3 , 4 , 5 , 6 , 7 , 8> to <2 , 8 , 3 , 4 , 5 , 6 , 7> .
{i} , where i ˛
With the new ordering , for a child itemset of Sp such that Sc = Sp ¨ Lf ) , the candidate extensions of Sc are the same under MinSupold and MinSupnew . For a child itemset of Sp such that Sn = Sp ¨
{i} , where i ˛ NTE(Sp ) , the candidate extensions of Sn consists of :
Eold(Sp ) ( Sc ˛
( 1 ) those items j such that i £ L j , where j ˛
NTE(Sp ) ,
•
•
02143 02143 02143 02143 and ( 2 ) those items j , j ˛
Eold(Sp ) .
Due to the re ordering , candidate extensions of the itemsets in Lf are not affected . For instance , after we change the tree extensions of null node under MinSupnew into <2 , 8 , 3 , 4 , 5 , 6 , 7> , the tree extensions of itemsets {3} , {4} , {5} , {6} and {7} under MinSupnew are the same with those under MinSupold . The tree extensions of itemset {8} become <3 , 4 , 5 , 6 , 7> from ˘ under MinSupold . We compute the projected transactions for itemset {8} to decide whether items 3 , 4 , 5 , 6 , and 7 are tree extensions of {8} . There is no need to compute projected transactions for {3} , {4} , {5} , {6} and {7} ( they were computed in previous mining ) .
Another example is given in Figure 2(b ) , which shows the corresponding part of Figure 2(a ) in our approach . After we change the order of tree extensions of null node , there is no need to extend itemsets {3} , {4} , {5} , {6} and {7} with 8 . We change tree extensions of itemset {3} from <4 , 5 , 6 , 7> to <5 , 7 , 4 , 6> . The candidate extensions of node {3 , 5} are <4 , 6 , 7> . The candidate extensions of node {3 , 7} are <4 , 6> . As a result , we only need to compute projected transactions for itemsets {3 , 5} and {3 , 7} ( which are not computed in previous mining ) while the naïve approach needs to compute projected transactions for itemsets {3 , 4} , {3 , 5} , {3 , 6} and {3 , 7} .
Notice that those itemsets on the tree boundary whose candidate extensions are empty can be removed from the tree boundary , eg , itemsets {4 , 5 , 7} and {5 , 7} in Figure 1 . Let us summarize the advantages of our tree boundary based extension with ordering change .
1 ) Our approach is able to avoid the computation of counting the supports of itemsets in Lf and Lif . We do not re generate the itemsets in Lf to extend them in the new mining process .
2 ) Our approach is able to avoid the generation of projected transactions that were done in previous mining while the naïve approach is unable to .
The ordering change is the key of our technique . It also brings some additional benefits when integrating treebased algorithms with tree boundary . Refer to [ 7 ] .
Now , let us prove the correctness and completeness of tree boundary approach .
˛         / # / # / # / Property 4.1 Given tree boundary TB wrt MinSupnew , extending the itemsets in TB is able to generate the complete set of itemsets Fn ( frequent under MinSupnew but not MinSupold ) .
Interested readers can refer to [ 7 ] for proof . Remark : In Apriori like algorithms , previous mining results under MinSupold do not provide sufficient information to build the tree boundary for re mining under MinSupnew . Moreover , even if we could build a tree boundary , Apriori like algorithms could not be easily modified to extend itemsets on tree boundary to discover Fn .
Interested readers can refer to [ 7 ] for proof .
5 . Tree boundary based re mining itemset mining and
We realized the proposed technique using the FP tree frequent the Tree Projection algorithms . The algorithm using FP tree is called ReMining using FP tree ( in short RM FP ) , and the algorithm using Tree Projection is called RM TP ( Re Mining using Tree Projection ) . Interested readers can refer to [ 7 ] for the algorithms RM FP and RM TP .
6 . Experimental evaluation
This section presents performance comparison of FPtree algorithm with RM FP on both synthetic and real life data sets . The comparison of Tree Projection algorithm with RM TP achieves similar results , and is given in [ 7 ] . All experiments are performed on a 750 Mhz Pentium PC with 512 MB main memory , running on Microsoft Windows 2000 . All the programs are written in Microsoft Visual C++ 60
The synthetic datasets were generated using the procedure described in [ 3 ] . We report experiments results on two synthetic datasets : One is T25I20D200k [ 9 ] with 1K items , which is denoted as D1 . In D1 , the average transaction size and the average maximal potentially frequent itemset size are 25 and 20 respectively . The number of transactions is 200k . The other dataset is T20I6D100k [ 3 ] also with 1K items , denoted as D2 .
We also tested our approaches on two real life datasets obtained from the UC Irvine Machine Learning Database Repository(http://wwwicsuciedu/~mlearn/MLRepositoryhtml ) . One is the Connect 4 dataset the other is the Mushroom dataset .
Figures 3 and 5 show the comparisons of RM FP with FP tree algorithm on datasets D1 and Connect 4 . In the curves for RM FP , the CPU time for each point ( except the first point ) is obtained by running RM FP ( with the value of that point as MinSupnew ) based on the previous mining results under MinSupold just before that point . For example in Figure 3 , the CPU time of RM FP at MinSupnew = 1.75 % is based on the old mining results with MinSupold = 2 % , and the CPU time for RM FP at MinSupnew = 1.5 % is based on the old mining results with MinSupold = 1.75 % , and so on . Note that when MinSupnew of RM FP is the same as MinSupold of the previous mining , eg , at MinSup = 2 % in Figure 3 , the extra running time of RM FP against FP tree shows the overhead of RM FP to output itemsets in Lif . The time is very small as shown in Figures 3 9 . The results on D2 and Mushroom are not shown due to space limitations . Actually , readers can see them based on Figures 7 and 9 .
From Figures 3 and 5 , we observe that RM FP is able to save more than 40 % running time of FP tree in each iteration . The saving is very significant in practice . In fact , RM FP can achieve even better results if the decrease of MinSup is smaller in each iteration as shown in Figure 4 . In Figure 4 , the MinSup is reduced by 10 % each time ( the decrease is smaller than that in Figures 3 and 5 ) . At each point , again RM FP is run based on the mining results of the previous point except for 2 % . In each iteration , we can save more than 70 % of the running time .
More performance curves on datasets D1 , D2 , Mushroom and Connect 4 are given in Figure 6 , 7 , 8 and 9 respectively . In Figure 6 , RM FP was run based on the initial mining results of the FP tree algorithm with MinSupold = 2 % , 1.5 % and 075 % In each case , a few decreased MinSupnew values are used . In Figure 7 , RM FP was run based on the mining results of MinSupold = 2 % , 1 % and 05 % In Figure 8 , RM FP was run based on the mining results of MinSupold = 60 % , 50 % , and 45 % ( we use very high minimum support because the dataset is very dense ) . In Figure 9 , RM FP was run based on the mining results at MinSupold = 2 % , 1 % , and 05 % In each of these figures , we show results with different MinSupnew values .
All the experiments show that RM FP consistently outperforms the FP tree algorithm even when MinSup drops to a very low level from a very high level . Using the same initial ( old ) mining results , we observe that the lower the MinSupnew is in the new mining , the smaller is the percentage of saving in computation . This is clear because the number of frequent itemsets at MinSupnew is much larger than the number of itemsets in Lf from old mining . For example , for D2 , the discovered frequent itemsets at 2 % is 381 while the number at 0.15 % is 558,834 . However , in practice , the user typically will not reduce the MinSup so drastically from one mining process to the next . For example , in most cases , it is quite unlikely that the user uses MinSupold = 2 % first , and then changes it to MinSupnew = 0.15 % suddenly for the next mining . Instead , the decrease each time is usually small as in the cases of Figures 3 , 4 , and 5 .
Note that in Figure 9 , RM FP based on 1 % support takes more time than RM FP based on 2 % support at MinSupnew = 075 % This is because the time used to check previous mining results offsets part of the benefit from utilizing previous mining results when the previous mining results are very large .
M 6 M 6 M 6 M 6
M 6 M 6 M 6 M 6
7 8:9 7 8:9 7 8:9 7 8:9
200 180
160 140 120
100 80 60 40
20 0
FP Tree RM FP 2 % RM FP 1 % RM FP 0.5 %
2
1.5
1
0.75 0.5
0.33
0.2
0.1
Minimum Support( % )
120
100
80
60
40
20
0 100
200
150
100
50
0 100
60
50
40
30
20
10
0
2
FP tree RM FP
FP tree RM FP
30
25
20
15
10
5
0
FP tree RM FP
450
400
350
300
250
200
150
100
50
0
1.75
1.5
1
0.75
0.5
0.33
Minimum Support( % )
2
1.8 1.6 1.5 1.3 1.2
1 Minimum Support( % )
0.9 0.8
60
55
50
48
45
42
40
38
Minimum Support( % )
7 8:9 7 8:9 7 8:9 7 8:9
5 6
60
50
40
30
20
10
0
2
;=< >@ ? ;=< >@ ? ;=< >@ ? ;=< >@ ?
ACB ACB ACB ACB
; 9 ; 9 ; 9 ; 9
D:EFB D:EFB D:EFB D:EFB
G4;=H=6 G4;=H=6 G4;=H=6 G4;=H=6
A:6 A:6 A:6 A:6
A47@I A=JLKNM 6 A47@I A=JLKNM 6 A47@I A=JLKNM 6 A47@I A=JLKNM 6
7 8O9 7 8O9 7 8O9 7 8O9
FP tree RM FP 2 % RM FP 1.5 % RM FP 0.75 %
1.75
1.5
1
0.75
0.5
0.33
Minimum Support( % )
;QP >R ? ;QP >R ? ;QP >R ? ;QP >R ?
ACB ACB ACB ACB
25
20
15
10
5
0
; 9 ; 9 ; 9 ; 9
D:EFB D:EFB D:EFB D:EFB
G4;HR6 G4;HR6 G4;HR6 G4;HR6
A:6 A:6 A:6 A:6
A47RI ARJSKCT A47RI ARJSKCT A47RI ARJSKCT A47RI ARJSKCT
UOHQD V UOHQD V UOHQD V UOHQD V
; 9 W:;:EO9 ; 9 W:;:EO9 ; 9 W:;:EO9 ; 9 W:;:EO9
;OD:U4 ; X M 6 ;OD:U4 ; X M 6 ;OD:U4 ; X ;OD:U4 ; X M 6 M 6
FP tree RM FP 2 % RM FP 1 % RM FP 0.5 %
;=Y >@ ? ;=Y >@ ? ;=Y >@ ? ;=Y >@ ?
ACB ACB ACB ACB
; 9 ; 9 ; 9 ; 9
D:EFB D:EFB D:EFB D:EFB
G4;=H=6 G4;=H=6 G4;=H=6 G4;=H=6
A:6 A:6 A:6 A:6
A47@I A!Z [ O\:\:]:^:_ A47@I A Z [ O\:\:]:^:_ A47@I A A47@I A Z [ O\:\:]:^:_ Z [ O\:\:]:^:_
FP Tree RM FP 60 % RM FP 50 % RM FP 45 %
7 8:9 7 8:9 7 8:9 7 8:9
450
400
350
300
250
200
150
100
50
0
2
1.5
1
0.75 0.5 0.33 0.25 0.15
Minimum Support( % )
60
55
50
48
45
42
40
38
Minimum Support( % )
M e)f4 ; 9 `dM ;=a >Qb cQ` ;=a >Qb c e)f4 ; 9 ;=a >Qb c ;=a >Qb c e)f4 ; 9 e)f4 ; 9
I 9 I 9 I 9 I 9
HQD A4EC;RI ARJSK HQD A4EC;RI ARJSK HQD A4EC;RI ARJSK HQD A4EC;RI ARJSK
M 6 M 6 M 6 M 6
7 8:9 7 8:9 7 8:9 7 8:9
140
;=h >Qb c ;=h >Qb cQ` ;=h >Qb c ;=h >Qb c
`dM e)f4 ; 9 M e)f4 ; 9 M e)f4 ; 9 M e)f4 ; 9
I 9 I 9 I 9 I 9
HQD A4EC;RI ARJjikM 6 HQD A4EC;RI ARJjikM 6 HQD A4EC;RI ARJjikM 6 HQD A4EC;RI ARJjikM 6
7 8:9 7 8:9 7 8:9 7 8:9
;Rl >@b c ;Rl >@b c@` ;Rl >@b c ;Rl >@b c
250
`mM e f4 ; 9 M e f4 ; 9 M e f4 ; 9 M e f4 ; 9
I 9 I 9 I 9 I 9
H@D A4EC;@I A!Z [ O\:\ ]:^O_ H@D A4EC;@I A Z [ O\:\ ]:^O_ H@D A4EC;@I A H@D A4EC;@I A Z [ O\:\ ]:^O_ Z [ O\:\ ]:^O_
FP tree 1 % FP tree 1.5 % RM FP 2% 1 % RM FP 2% 1.5 %
FP tree 0.5 % FP tree 0.75 % RM FP 1% 0.5 % RM FP 1% 0.75 %
300
500
800
1000
Number of transactions(K )
300
500
800
1000
Number of transactions(K )
I 9 I 9 I 9 I 9
`dM e)f4 ; 9 M e)f4 ; 9 M e)f4 ; 9 M e)f4 ; 9
;=n >Qb c ;=n >Qb cQ` ;=n >Qb c ;=n >Qb c
HQD A4EC;RI A=oRp4qOr:st[:[Ou HQD A4EC;RI A oRp4qOr:st[:[Ou HQD A4EC;RI A HQD A4EC;RI A oRp4qOr:st[:[Ou oRp4qOr:st[:[Ou
7 8:9 7 8:9 7 8:9 7 8:9 7 8:9 7 8:9 7 8:9 7 8:9 The scalability experiments are conducted by increasing the number of transactions on dataset D1 . As shown in Figure 10 , both FP tree and RM FP have linear scalability with the number of transactions , but RM FP is more scalable .
M 6 M 6 M 6 M 6
7 . Application to other constraints
This section shows that the proposed approach is also applicable to discovering the set Fn when any other single or multiple constraints are changed . The detailed techniques for handing changes of these constraints differ . We only present methods for dealing with the change of individual constraints and multiple constraints intuitively . Interested readers may refer to our technical report [ 7 ] for additional details and examples .
; KFv >xw ECD V ; KFv >xw ECD V ; KFv >xw ECD V ; KFv >xw ECD V
D y:6 D y:6 D y:6 D y:6 zS{|6 zS{|6 zS{|6 zS{|6
}|B }|B }|B }|B
}4;A:8OH=y4 ; 9 I4g B }4;A:8OH=y4 ; 9 I4g B }4;A:8OH=y4 ; 9 I4g B }4;A:8OH=y4 ; 9 I4g B
D A4UCD:EFB D A4UCD:EFB D A4UCD:EFB D A4UCD:EFB
I A4U I A4U I A4U I A4U
71 Dealing with Individual Constraint Changes
We discuss the methods for discovering the set Fn when a single constraint is changed . Method 1 : Filtering previous mining results
The set Fn can be obtained by filtering previous results in the following two cases : ( 1 ) tightening of a constraint of any kind ; ( 2 ) relaxation of a convertible monotone or monotone constraint . Method 2 : Tree boundary based re mining
This method as discussed in Section 4 applies to the relaxation of a convertible anti monotone or antimonotone constraint although it is a bit different when applying to anti monotone constraint relaxation due to the special property of convertible constraints [ 7 ] . Method 3 : Simpler tree boundary based re mining
Tree boundary in this method is easier to devise than
6 6 V 6 6 6 V 6 6 6 V 6 6 6 V 6 6 6 6 ` ` ` ` P P P P ` ` M M g g g g ` ` g g g g ` ` g g g g ` ` ` ` P P P P ` ` g g g g V 6 B B 9 6 V 6 B B 9 6 V 6 B B 9 6 V 6 B B 9 6 that for Method 2 and usually contains only 1 itemsets . It applies to the relaxation of a succinct and anti monotone constraint , or a succinct and monotone constraint . When one of such constraints is relaxed , it can be dealt with as follows : Let E(null ) be the list of frequent items that satisfy the old constraint . By checking the old mining results , we first find the list of frequent items NTE(null ) that satisfy the new constraint but not the old constraint . Itemsets made of individual items in NTE(null ) make up the tree boundary .
Constraint 1 Constraint 2 Tighten 1&2 Relax 1 tighten 2 Tighten 1 relax 2 Relax 1&2
Succinct & Anti mono .
Succ&Mono
Anti mono .
Monotone
M3
M1
M1
M1
M1
M1&M2
M1&M3
M1&M3 M1&M3 depends depends M1&M3 M1&M3 M1&M3 adapted M3 M1&M2 M2&M3 M1&M3 depends depends
M1&M3 Succ . & Anti . Succ . & Mono . M1 M1&adapted M3 M1&M3 M1 Anti . M1 Mono . M1 Convert . Anti . Convert . Mono . M1 Succ . & Mono . M1 M1 Anti . Mono . M1 – \ M1 Convert . Anti . Convert . Mono . – \ M1 M1 Anti . M1 Mono . M1 Convert . Anti . Convert . Mono . M1 M1 Mono . M1 Convert . Anti . Convert . Mono . M1 Convert . Anti .
M1&M3 M1&M2 M2&M3 M1&M3 – \ M1&M3 – \ M1&M2 – \ M2&M3 – \ M1&M3 – \ M1&M3 adapted M2 M1&M2 violates violates
M1&M2 M1&M2 violates violates
M1&M2 M1&M2
– \ M1 M1&M2
M1 M1 M1
M1&M2
M1 M1
M3
M1
– \ M1 Convertible Anti mono . Convert . Mono . – \ M1 Convert . Mono . Convert . Mono . – \ M1
– \ M1&M2 – \ M1&M2 – \ M1&M2
M1
– \ M1 – \ M1
M1
– \ M2
– \ M1&M2
– \ M1
– \ M1
  x      x      x      x   
~    ~    ~    ~   
72 Dealing with multiple constraint changes
  m                                                
=     =     =     =    
      :        :        :        : 
       
        the methods for discovering Fn when
Although users usually change one constraint at a time to see the effect of the change , it is also possible that multiple constraints are changed at the same time . Table 1 shows two constraints are changed at the same time . Most of the combined cases can be handled by combining the approaches individual constraints . For example , tightening a succinct & antimonotone constraint and relaxing a succinct & monotone constraint requires Method 1 ( handling the tightening ) and 3 ( handling the relaxation ) . Interested readers can refer to [ 7 ] for the meanings of those exceptional cases including “ Adapted ” , “ Violates ” , “ Depends ” and “ – ” . the change of to handling
Finally , when more than two constraints are changed at the same time , they can be handled by combining the methods for their respective changes in consideration of the exceptional cases in table 1 .
8 . Conclusions
Practical data mining is often a highly interactive and
M1
[ 4 ] R . J . Bayardo . Efficiently mining long patterns from reduced minimum support . Experiment iterative process . Users change constraints and run the mining algorithm many times before satisfied with the final results . Current mining algorithms are unable to take advantage of the previous mining results to speed up the new mining process . Motivated by this problem and using the minimum support constraint as an example , this paper first proposed the concept of tree boundary to summarize and reorganize the previous mining results . It then presents an effective and efficient framework for re mining under results the demonstrate is highly effective . Finally , we also show that when any other individual constraint is changed or multiple constraints are changed at the same time , the new set of frequent itemsets can also be mined efficiently using the proposed technique .
References [ 1 ] R . Agarwal , C . Aggarwal , and V . Prasad . A Tree Projection algorithm for generation of frequent itemsets . In J . Parallel and Distributed Computing , 2000 . the proposed technique that
[ 2 ] C . Aggarwal and P . Yu . Online generation of association rules . In Proc . of 14th ICDE , 1998 .
[ 3 ] R . Agrawal and R . Srikant . Fast algorithm for mining association rules . In Proc . of the 20th VLDB , 1994 . database . In Proc . of the SIGMOD , 1998 .
[ 5 ] A . Bykowski , C . Rigotti . A condensed representation to find frequent patterns . In Proc . of PODS , 2001 .
[ 6 ] D . W . Cheung , J . Han , V . Ng , and CY Wong . Maintenance of discovered association rules in large databases : An incremental updating technique . In Proc . of ICDE , 1996
[ 7 ] G . Cong , B . Liu , Interactive mining of frequent itemsets with constraint changes . Technical report , National Univ . of Singapore , 2002 .
[ 8 ] R . Feldman , Y . Aumann , A . Amir , and H . Manila . Efficient algorithm for discovering frequent sets in incremental databases . In 2nd SIGMOD workshop DMKD , 1997 .
[ 9 ] J . Han , J . Pei , and YYin Mining Frequent Patterns without
Candidate Generation . In SIGMOD , 2000 .
[ 10 ] R . Ng , LVS Lakshmanan , J.Han , and APang Exploratory mining and pruning optimizations of constrained association rules . In Proc . of SIGMOD , 1998 .
[ 11 ] S . Parthasarathy , M . J . Zaki , M . Ogihara , and S . Dwarkadas . Incremental and interactive sequence mining . In Proc . of the 8th CIKM , Kansas City , MO , USA , November 1999 .
[ 12 ] J . Pei , J . Han , and LVSLakshmanan Mining frequent itemsets with convertible constraints . In Proc . ICDE , 2001 .
[ 13 ] R . Srikant , Q , Vu , and R . Agrawal . Mining association rules with item constraints . In Proc . of KDD , CA , 1997 .
[ 14 ] S . Thomas , S . Chakravarthy . Incremental mining of constrained associations . In HiPC2000 .
[ 15 ] S . Thomas , S . Bodagala , K . Alsabti , and S . Ranka . An incremental updation of efficient algorithm association rules in large databases . In Proc . KDD , 1997 . the for
[ 16 ] H . Toivonen . Sampling large databases for association rules .
In Proc . of the 22th VLDB , 1996 .
       
