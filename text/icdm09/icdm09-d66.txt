A Sales Forecast Model for the German Automobile
Market Based on Time Series Analysis and Data Mining Methods
Bernhard Brühl1,3 , Marco Hülsmann1 , Detlef Borscheid2 , Christoph M . Friedrich1 , and Dirk Reith1,∗
1 Fraunhofer Institute for Algorithms and Scientific Computing ( SCAI ) , Schloss
Birlinghoven , 53754 Sankt Augustin , Germany
2 BDW Automotive , Maybachstr . 35 , 51381 Leverkusen , Germany
3 Present address : Universität zu Köln , Seminar für ABWL , Risikomanagement und
Versicherungslehre , Kerpener Str . 30 , 50937 Köln , Germany dirkreith@scaifranhoferde
ABSTRACT In this contribution , various sales forecast models for the German automobile market are developed and tested . Our most important criteria for the assessment of these models are the quality of the prediction as well as an easy explicability . Yearly , quarterly and monthly data for newly registered automobiles from 1992 to 2007 serve as the basis for the tests of these models . The time series model used consists of additive components : trend , seasonal , calendar and error component . The three latter components are estimated univariately while the trend component is estimated multivariately by Multiple Linear Regression as well as by a Support Vector Machine . Possible influences which are considered include macro economic and market specific factors . These influences are analysed by a feature selection . We found the non linear model to be superior . Furthermore , the quarterly data provided the most accurate results .
Keywords : Sales Forecast , Time Series Analysis , Data Mining , Automobile Industry .
1 Introduction
Successful corporate management depends on efficient strategic and operative planning . Errors in planning often lead to enormous costs and in some cases also to a loss of reputation . Reliable forecasts make an important contribution to efficient planning . As the automobile industry is one of the most important sectors of the German economy , its development is of utmost interest .
The introduction and the development of mathematical algorithms , combined with the utilization of computers , have increased the reliability of forecasts enormously . Enhanced methods , eg Data Mining , and advanced technology allowing the storage
∗ Corresponding author .
P . Perner ( Ed. ) : ICDM 2009 , LNAI 5633 , pp . 146–160 , 2009 . © Springer Verlag Berlin Heidelberg 2009
A Sales Forecast Model for the German Automobile Market
147 and evaluation of large empirical data sets generate the means of producing more reliable forecasts than ever before . At the same time , the methods have become more complex . However , the explicability of a forecast model is as important as its reliability . Therefore , the main objective of our work is to present a model for sales forecasts which is highly accurate and at the same time easily explicable .
Although Lewandowski [ 1 ] [ 2 ] investigated sales forecast problems in general and concerning the automobile industry in particular in the 1970s , few studies have focused on forecasts concerning the German automobile industry thereafter . Recent publications have only been presented by Dudenhöffer and Borscheid [ 3 ] [ 4 ] , applying time series methods to their forecasts . Another approach has been chosen by Bäck et al . [ 5 ] , who used evolutionary algorithms for their forecasts .
The present contribution pursues new routes by including Data Mining methods . But there are also differences in the time series methods which are applied . A detailed analysis of the past is needed for a reliable forecast of the future . Because of that , one focus of our work is the broad collection and analysis of relevant data . Another focus is the construction and the tests of the used model . The data base of our models consists of the main time series ( registrations of new automobiles ) and the secondary time series , also called exogenous parameters , which should influence the trend of our main time series . To eliminate parameters with insignificant influence on the main time series , a feature selection method is used . These tasks are solved for yearly , monthly and quarterly data . Then the results are analyzed and compared in order to answer the following main questions of this contribution :
1 .
Is it possible to create a model which is easy to explain and which at the same time provides reliable forecasts ?
2 . Which exogenous parameters influence the sales market of the German automobile industry ?
3 . Which collection of data points , yearly , monthly or quarterly data , is the most suitable one ?
2 Data
The main time series comprises the number of registrations of new automobiles in Germany for every time period . Hence , the market sales are represented by the number of registrations of new automobiles , provided by the Federal Motor Transport Authority .
The automobile market in Germany increased extraordinary by the reunification of the two German states in 1990 . This can only be treated as a massive shock event which caused all data prior to 1992 to be discarded . Therefore , we use the yearly , monthly and quarterly registrations of the years 1992 to 2007 . The sales figures of these data are shown in Figures 1 3 and the seasonal pattern of these time series is clearly recognizable in the last two figures .
148
B . Brühl et al .
Main Time Series
( Yearly Data ) s n o i t a r t s g e R w e N i s n o i t a r t s g e R w e N i
4500000
4000000
3500000
3000000
2500000
2 9 9 1
3 9 9 1
4 9 9 1
5 9 9 1
6 9 9 1
7 9 9 1
8 9 9 1
9 9 9 1
0 0 0 2 Year
1 0 0 2
2 0 0 2
3 0 0 2
4 0 0 2
5 0 0 2
6 0 0 2
7 0 0 2
Fig 1 . Registrations of new automobiles from 1992 to 2007 : Yearly data
Main Time Series ( Quarterly Data )
1200000 1100000 1000000 900000 800000 700000 600000
2 9
J
3 9
J
4 9
J
5 9
J
6 9
J
7 9
J
8 9
J
9 9
J
0 0
J
1 0
J
2 0
J
3 0
J
4 0
J
5 0
J
6 0
J
7 0
J
8 0
J
Quarter
Fig 2 . Registrations of new automobiles from 1992 to 2007 : Quarterly data
Main Time Series
( Monthly data ) s n o i t a r t s g e R w e N i
450000 400000 350000 300000 250000 200000 150000
2 9
J
3 9
J
4 9
J
5 9
J
6 9
J
7 9
J
8 9
J
9 9
J
0 0
J
1 0
J
2 0
J
3 0
J
4 0
J
5 0
J
6 0
J
7 0
J
8 0
J
Month
Fig 3 . Registrations of new automobiles from 1992 to 2007 : Monthly data
Our choice of the exogenous parameters fits the reference model for the automobile market given by Lewandowski [ 2 ] . In this model the following properties are considered : a ) Variables of the global ( national ) economy b ) Specific variables of the automobile market c ) Variables of the consumer behavior wrt the changing economic cycle d ) Variables that characterize the influences of credit restrictions or other fiscal measures concerning the demand behaviour in the automobile industry .
Based on this model , the following ten market influencing factors , shown in Table 1 , are chosen [ 6 ] ( exogenous parameters )
A Sales Forecast Model for the German Automobile Market
149 s r e t e m a r a p e s e h t f o n o i t a c i l b u p e h t f o s l a v r e t n i e m i t e h t
, n m u l o c d n o c e s e h t n I
. d e t a c i d n i e r a s r e t e m a r a p s u o n e g o x e e h t
, n m u l o c t s r i f e h t n I
.
1 e l b a T
. s t i n u t n e r e f f i d y b d e s u e r a s r e t e m a r a p e h T
. ”
M
“ n a y b y l h t n o m d n a
, ” Q “ a y b y l r e t r a u q
, ” Y “ a y b d e t o n e d e r a s r e t e m a r a p d e h s i l b u p y l r a e Y
. n e v i g e r a e h t
, d e s u e r a s r e b m u n e t u l o s b a f I
. ” D “ a y b d e t o n e d e b l l i w r e t e m a r a p e h t
, d e s u s i
0 0 0 2 r a e y e c n e r e f e r e h t o t n o i t a i v e d e v i t a l e r e h t t a h t e s a c e h t n I s i s i h T
. ” E “ n a y b d e t o n e d e b l l i w e s e h T
. t i n u o n e v a h d n a s t r e p x e t e k r a m y b d e t u p m o c e r a s r e t e m a r a p o w T
. ” A “ n a y b d e t o n e d e b l l i w r e t e m a r a p n a m r e G e h t
, )
O S F ( e c i f f
O l a c i t s i t a t
S l a r e d e F e h t e r a s e c r u o s a t a d e e r h t e h T
. n e v i g s i e c r u o s a t a d e h t
, n m u l o c h t r o f e h t n I
. n m u l o c d r i h t e h t n i n w o h s a n i y m o n o c e l a n o i t a n a f o h t g n e r t s c i m o n o c e e h t r o f y t i t n a u q a s i
) P D G
( t c u d o r P c i t s e m o D s s o r
G e h T
. y r t n u o c a n i h t i w s e c i v r e s d n a s d o o g l l a f o e u l a v e h t s e r u s a e m t I
. d o i r e p e m i t c i f i c e p s o t d e s u e b d l u o c h c i h w e m o c n i e h t s t n e s e r p e r s d l o h e s u o h e t a v i r p e h t f o e m o c n I l a n o s r e P e l b a l i a v A e h T d n a t i f e n e b l a i c o s y r a t e n o m e h t
, e m o c n i y r a m i r p e h t f o m u s e h t y b d e t a l u c l a c s i t I
. e v a s o t d n a e m u s n o c e r a h c i h w s e c i v r e s d n a s d o o g l l a f o s e c i r p n i t f i h s e g a r e v a e h t s e r u s a e m x e d n I e c i r P r e m u s n o C e h T
. n o i t p m u s n o c r o f s d l o h e s u o h e t a v i r p e h t y b t h g u o b
. s r e f s n a r t s u o u n i t n o c r e h t o d n a s n o i t u b i r t n o c y t i r u c e s l a i c o s
, x a t y t r e p o r p d n a e m o c n i y b d e c u d e r s i m u s s i h T
. s r e f s n a r t s u o u n i t n o c r e h t o e h t d n a k n a B l a r e d e F n a m r e G e h t y b d e h s i l b u p s i h c i h w
, l a t i p a c t i d e r c r o f e t a r e h t s i e t a R t s e r e t n I e h T
. k n a B l a r t n e C n a e p o r u E l i v i c e l b a l i a v a l l a o t e v i t a l e r s n o s r e p d e y o l p m e n u f o e g a t n e c r e p e h t s i e t a R t n e m y o l p m e n U e h T
. r e w o p n a m
. d o i r e p c i f i c e p s a n i s t n e m t s e v n i l a i r e t a m l l a s e d u l c n i d n a m e D t n e m t s e v n I l a i r t s u d n I e h T
. e g r a h C l o r t e P e h t s i l e s e i d r o e n i l o s a g d e d a e l n u e k i l s m r o f l o r t e p t n e r e f f i d e h t f o s e c i r p l l a f o n a e m e h T g n i d n e p s r e m u s n o c r o t n e m t r a p a d l o h e e r f a f o e s u e h t e k i l s e s a h c r u p l a e r n o n s e d u l c n i t I
. n o i t p m u s n o C a s e s a c e s e h t n I
. s e g a t r o h s l a i c n a n i f o t e u d e l p m a x e r o f
, s n o s a e r s u o i r a v r o f d e y a l e d e b d l u o c e s a h c r u p l a i t i n i s i h T
. d e c a l p e r e b o t s d e e n t i h c i h w r e t f a e m i t e f i l t c u d o r p n a e m a e v a h s d o o g l l a
, l a r e n e g n I
. s n o i t a z i n a g r o e t a v i r p f o e t a v i r P d e l l a c s i n o i t p m u s n o c r o f s d l o h e s u o h e t a v i r p e h t f o s e c i v r e s d n a s d o o g e h t f o e s a h c r u p e h T r e i l r a e n a o t d a e l h c i h w s e s a c e r a e r e h t
, t s a r t n o c n I
. s e s i r a
] 2 [ d n a m e D t n e m e c a l p e R t n e t a L d e l l a c o s g n i t s i x e f o s n o i s i v e r
, s e l c i h e v w e n f o n o i t c u d o r t n i e h t f o e c n e u l f n i e h t s r e d i s n o c y c i l o P l e d o M e h T
. d n a m e D t n e m e c a l p e R t n e t a L e h t n i d e r e d i s n o c e r a s n o i t a i r a v e s e h T
. e s a h c r u p l a i t i n i
O S F
O S F
O S F
B F G
O S F
O S F
O S F
O S F
D
D
D
A
A
D
D
D
Q
;
Y
Q
;
Y
M
;
Y
M
M
Q
;
Y
M
;
Y
Q
;
Y
W D B
E
M
;
Q
;
Y n o i t a n a l p x E a t a D e c r u o S r e t e m a r a P t i n U g n i h s i l b u P l a v r e t n I s u o n e g o x E s r e t e m a r a P c i t s e m o D s s o r G t c u d o r P e m o c n I l a n o s r e P e l b a l i a v A e c i r P r e m u s n o C x e d n I e t a R t s e r e t n I t n e m y o l p m e n U e t a R t s e v n I l a i r t s u d n I d n a m e D t n e m e g r a h C l o r t e P n o i t p m u s n o C e t a v i r P t n e m e c a l p e R d n a m e D t n e t a L h t i w s d o i r e p n I
. ] 4 [ t s a p e h t f o a t a d e h t y b d e t a l u c l a c e b l l i w l e v e l e s a b n i a t r e c
A
. s n g i a p m a c d n a s l e d o m e h t
, s n o i t a v o n n i r e w e f h t i w s d o i r e p n I
. l e v e l e s a b e h t e v o b a s e s a e r c n i e u l a v e h t
, s n o i t a v o n n i d e s a e r c n i
. l e v e l e s a b e h t w o l e b s e s a e c e d e u l a v
W D B
E
M
;
Q
;
Y y c i l o P l e d o M
. d e d i v o r p s i s r e t e m a r a p e h t f o n o i t a n a l p x e t r o h s a
, n m u l o c t s a l e h t n I
. )
W D B
( e v i t o m o t u A W D B d n a
, )
B F G
( k n a B l a r e d e F
150
B . Brühl et al .
In Table 1 , it is shown that not all exogenous parameters used are published on a monthly , quarterly , and yearly base . In cases in which the necessary values are not given directly , the following values are taken :
Yearly data analysis : The averages of the Unemployment and Interest Rate of each year are used . Quarterly data analysis : The average of the Unemployment and Interest Rate of each quarter is used . For the parameters Consumer Price Index and Petrol Charge the values of the first months of each quarter are taken .
Monthly data analysis : In the case of the quarterly published parameters , a linear interpolation between the values of two sequential quarters is used .
3 Methodology
3.1 Time Series
Time Series Model In this contribution an additive model with the following components to mimic the time series is applied .
Let xt , t = 1,,T , be the time series observed in the past . Then xt can be written as :
= mx t t
+
( s t
)
+
( p t
)
+ e t
, where mt is the trend component , st the seasonal component ( only for monthly and quarterly data ) , pt is the calendar component ( only for monthly data ) , and et represents the error component .
Seasonal Component For the estimation of the seasonal component there are many standard methods like exponential smoothing [ 7 ] , the ASA II method [ 8 ] , the Census X 11 method [ 9 ] , or the method of Box and Jenkins [ 10 ] . In this contribution , the Phase Average method [ 11 ] is used because it is quite easy to interpret . To get accurate results with this method , the time series must have a constant seasonal pattern over time and it has to be trendless . A constant seasonal pattern is given in our time series . To guarantee the trend freedom , a trend component is estimated univariately and subtracted before the seasonal component is estimated . The latter is done by using a method which is close to the moving average method [ 12 ] . Because of the small given data set , differing from the standard method , the following formula is used to compute the mean m of a period :
= m t
1 t t ∑ = 1 i
= tx i
,,1
T
Trend Component Although a univariate trend estimation would be easier to explain , this route is not followed in this contribution because the assumption that the registrations of new automobiles in Germany are not influenced by any other parameter is not justified . Hence , the most important component , the trend , is estimated multivariately .
A Sales Forecast Model for the German Automobile Market
151
For the linear trend estimation the Multiple Linear Regression ( MLR ) [ 12 ] is used . The Support Vector Machine ( SVM ) with ε Regression and Gaussian kernel [ 13 ] [ 14 ] [ 15 ] is chosen as a representative of a non linear estimation because the SVM has proven to provide suitable results in other industrial projects [ 16 ] [ 17 ] . However , this choice might be altered in future publications .
Calendar Component The calendar component considers the number of working days within a single period . For the estimation of the calendar component pt a method close to a standard method used in [ 1 ] is chosen . At first the auxiliary value p* t is computed by :
=
* p t
A t
N t
− G t
= t
,,1
T where Nt is the mean of the working days in the past of the according period , At the number of working days , and Gt the total number of days of the period t .
Then the absolute values pt of the calendar component is calculated via Linear Re gression using the values xt , t = 1,,T of the main time series and p* Error Component The error component is estimated with the Autoregressive Moving Average Process of order two [ 8 ] . A condition to use this method is the stationarity of the error component . This condition is tested by the Kwiatkowski Phillips Schmidt Shin Test ( KPSSTest ) [ 18 ] . In the cases of non stationarity , it is set to zero . t , t = 1,,T
3.2 Data Pre processing
Time lag In reality , external influencing factors do not always have a direct effect on a time series , but rather this influence is delayed . The method used to assign the time lag is based on a correlation analysis .
Time lag estimation If the value yt of a time series Y has its influence on the time series X in t + s , the time lag of the time series Y is given by the value s . Then the correlation between the main time series X with its values x1,,xT and all of the k secondary time series Yi , i=1,,k , with its values yi T , is computed . Afterwards the secondary time series is shifted by one time unit , ie the value yi t+1 and the correlation between the time series x2,,xT and yi T 1 is assigned . This shifting is repeated up to a pre defined limit . The number of shifts of every highest correlation between the main and a secondary time series is the value of the time lag of this secondary time series . t becomes the value yi
1,,yi
1,yi
Smoothing the exogenous parameters by using the time lag It is assumed that every value yt of an exogenous parameter Y is influenced by its past values . The time lag indicates how many past data points influence the current value . This results in the following method :
Let s be the time lag of Y = y1,,yT , then the current value yt is calculated by the weighted sum
152
B . Brühl et al . y t s j
⎧ = ∑ ⎪ ⎨ ⎪ ⎩
= 1
λλ ) 1(
−
− s j y t
− j
+= s t
,,1
T y t
= t
,,1 s where
)1,0(∈λ is the weighting factor .
Normalisation To achieve comparability between factors which are not weighted in the same way , these factors have to be normalized to a similar range . With that step numerical errors can be tremendously reduced . As normalization method , the z Transformation is applied . It refines the mean value to zero and the standard deviation to one : Let vt be any factor at a particular time t , t∈ T , then the z Transformation is calculated by where v t
, normalized v t
=
)( v
μ− σ )( v
,
)(vμ is the mean and
)(vσ the standard deviation of v .
Feature Selection Methods which are typically used for Feature Selection are the correlation analysis , the Principal Component Analysis ( PCA ) [ 19 ] , the Wrapper Approach [ 20 ] , and the Filter Approach [ 21 ] . Here , the Wrapper Approach with two different regression methods the Multiple Linear Regression and the Support Vector Machine is chosen for dimension reduction . Compared with other methods , this method provides more explicable results even for small data sets . Additionally , forecasts with the PCA are calculated as a reference model for our results . The PCA results are not easily explicable , as the PCA transformed parameters can not be traced back to the original ones . Therefore the results have not been considered for the final solution
4 Evaluation Workflow
The data , ie the main time series and the exogenous parameters , are divided into training and test data . The methods introduced in Chapter 3 are used to generate a model on the training data , which is evaluated by applying it to the test data . The complete evaluation workflow is shown in Figure 4 .
Step 1 : Data Integration : The bundling of all input information to one data source is the first step in the workflow . Thereby , the yearly , quarterly or monthly data ranges from 1992 to 2007 . The initial data is assumed to have the following form :
Main Time Series x t = m t + s t + p t + e t , t = 1 , , T
Secondary Time Series y i t , t = 1 , , T and i = 1 , , k
Step 2 : Data Pre processing : Before the actual analysis , an internal data preprocessing is performed , wherein special effects contaminating the main time series are eliminated . For example , the increase of the German sales tax in 2007 from 16 %
A Sales Forecast Model for the German Automobile Market
153
Data source 1 Data source 1
… …
Data source m Data source m
Data Data
Internal and external Internal and external data pre processing data pre processing
Training data Training data
Test data Test data
Main time series Main time series
Ex . parameters Ex . parameters
Univariate estimation of Univariate estimation of the seasonal component the seasonal component
Ex . parameters Main time series Ex . parameters Main time series
Estimation of the Time lag between every Estimation of the Time lag between every exogenous parameter and the main time series exogenous parameter and the main time series
Multivariate Estimation of the trend Multivariate Estimation of the trend component ( linear and non linear ) component ( linear and non linear )
Univariate Estimation of the Univariate Estimation of the calendar component calendar component
Stationary test and Stationary test and ARMA modeling ARMA modeling
Calculation of the Calculation of the training error training error
ARMA forecast ARMA forecast
Multivariate TrendMultivariate Trendforecast by given forecast by given exogenous parameters exogenous parameters
Forecast of the main time Forecast of the main time series as the sum of trend , series as the sum of trend , ARMA forecast , seasonal ARMA forecast , seasonal and Calendar component and Calendar component
Calculation of the Calculation of the test error test error
Legend Legend
Data integration Data integration
Step 1 Step 1
Data pre processing Data pre processing
Step 2 Step 2
Modeling Modeling Step 3 to 6 Step 3 to 6
Training error Training error
Step 7 Step 7
Forecast Forecast Step 8 Step 8
Test error Test error
Step 9 Step 9
Fig 4 . Evaluation Workflow : First , the data is collected and bundled . After a data preprocessing , it is split into training and test set . The model is built on the training set and the training error is calculated . Then the model is applied to the test data . Thereby , the new registrations for the test time period are predicted and compared with the real values and based on this the test error is calculated . to 19 % led to an expert estimated sales increase of approximately 100.000 automobiles in 2006 . Hence , this number was subtracted in 2006 and added in 2007 . Furthermore , the exogenous parameters were normalized by the z Transformation .
The normalized data are passed on to an external data pre processing procedure . The method used is the Wrapper Approach with an exhaustive search . Since we use a T fold cross validation ( leave one out ) to select the best feature set , it should be noted that we implicitly assumed independency of the parameters . As regression method for the feature evaluation a Linear Regression is applied in the case of linear trend estimation and a Support Vector Machine in the case of non linear trend estimation .
The elimination of the special effects in monthly data is not applicable because the monthly influences can be disregarded .
Step 3 : Seasonal Component : The estimation of the seasonal component is done by the Phase Average method . In this contribution , the seasonal component is estimated before the trend component . The reason is that the Support Vector Machine absorbs a
154
B . Brühl et al . part of the seasonal variation , leading to faulty results . In order to remove the influence of trends on the data , the trend is estimated univariately by using the method presented in section 31 The univariate trend component is subtracted and the seasonal component can be estimated on the revised time series .
For the analysis of yearly data , the seasonal component cannot be computed be cause it measures the seasonal variation within one year . Step 4 : Time Lag and trend component : To analyse monthly and quarterly data , the time lag of each exogenous parameter is calculated by a correlation analysis . The exogenous parameters are smoothed using the estimated time lag . In this case , it is of advantage to limit the time lag because the influence of the exogenous parameters on the main time series is temporary bounded . The limit chosen was one year for monthly and quarterly data . Hence , the time lag is 0 , if it is greater than the limit .
Afterwards , the trend component of the training set is estimated multivariately by Linear Regression ( linear case ) or by a Support Vector Machine ( non linear case ) . To optimize some specific parameters for the Support Vector Machine , a Grid Search algorithm is applied . Thereby , a bootstrapping with ten replications is performed for the evaluation . Step 5 : Calendar component : The calendar component of the training set is estimated univariately by the method presented in section 31
The calendar component is computed only for monthly data because the variation of the working days in the case of yearly and quarterly data can be disregarded .
Step 6 : ARMA Model : In the stationary case the error component is estimated by a second order ARMA process . Otherwise it set to zero ( cf . 31 ) Step 7 : Training Error : The absolute training error is derived from the difference between the original values of the main time series and the values estimated by the model . These values are given by the sum of the values of the trend , seasonal , and calendar component . In the case of a stationary error component , the values estimated by the ARMA model are added . The mean ratio between the absolute training errors and the original values gives the Mean Absolute Percentage Error ( MAPE ) .
Let xi , i=1,,T , be the original time series after the elimination of special effects and zi , i=1,,T , the estimated values . Then , the error functions considered are represented by the following formulas :
Mean Absolute Error
E
MAE
=
1 T
T
∑
= 1 i
− x i z i
Mean Absolute Percentage Error
E
MAPE
=
1 T x i
T
∑
= 1 i z i
− x i
Step 8 : Forecast : The predictions for the test time period are obtained by summing up the corresponding seasonal component , the trend component based on the exogenous parameters of the new time period and the respective multivariate regression method , and the calendar component . In the case of a stationary error component , the values predicted by the ARMA process are added , too .
Step 9 : Test error : The differences of the predictions and the original values of the test set lead to the test errors . Its computation conforms exactly to the computation of the training errors .
A Sales Forecast Model for the German Automobile Market
155
5 Results
The results are obtained from the execution of the workflow presented in chapter 4 .
In a first step , all ten exogenous parameters are used in the model . Secondly , a feature selection is performed by the Wrapper Approach and the same workflow is executed with only the selected parameters . The training period consists either of 14 or 15 years leading to a test set of two years or one year , respectively . In each case , an MLR as well as an SVM is used for the multivariate trend estimation . This leads to eight different evaluation workflows for each type of data collection .
In order to be able to assess the error rates resulting from the evaluation workflows , upper bounds for the training and test errors are required as reference values for the evaluation . Therefore , a Principal Component Analysis ( PCA ) is applied to the original exogenous parameters , and the same evaluation workflow is performed with the PCA transformed parameters . The PCA results are an indicator of what can be expected from the multivariate model . However , in this contribution , the transformed parameters cannot be used , because they are not explicable and the influences of the original parameters cannot be reproduced anymore .
The results of the yearly , monthly , and quarterly data which generate the smallest errors using the PCA transformed exogenous parameters are shown in Table 2 . A training period of 14 years is used . In all cases the SVM gives much better results than the MLR [ 22 ] . To optimize the parameters of the SVM , the Grid search algorithm is applied .
Table 2 . Errors of the best models using the PCA transformed exogenous parameters and training periods of 14 years
Non linear trend estimation ( SVM using PCA transformed exogenous parameters )
Yearly Data Monthly Data Quarterly Data
Parameters
C=8 , γ=0.25 , ε=0.1 C=1 , γ=0.125 , ε=0.1 C=8 , γ=0.25 , ε=0.1
Mean Percentage Training
Error 0.31 % 3.29 % 0.54 %
Mean Percentage Test
Error 2.04 % 11.23 % 4.86 %
5.1 Yearly Model
The Feature Selection for the linear trend approximation showed that the Gross Domestic Product , Unemployment Rate , Price Index , Private Consumption , and Industrial Investment Demand were the only parameters which significantly influenced the main time series . For the non linear trend approximation , the respective parameters were the same , except for the Price Index being exchanged with the Latent Replacement Demand .
Table 3 shows the results of the various models . By comparison with the PCA analysis for yearly data ( see Table 2 ) , one can clearly see that the quality of the linear trend models is inferior . Considering the fact that we face high test errors although we start from low training errors , one can assume that the training set was too small for this specific problem and the model was overfitted . Especially , data points which are not in close proximity to the training set are hard to predict correctly .
156
B . Brühl et al .
Table 3 . Training and test errors in the model using yearly data
All Parameters
Reduced Parameters
Reduced Parameters 14 Years 15 Years 14 Years 15 Years 14 Years 15 Years 14 Years 15 Years 236 653 33 146 0.98 % 7.28 %
207 169 6.31 %
418 335 12.87 %
128 609 3.94 %
35 165 1.05 %
All Parameters
96 802 2.85 %
Mean Training Error
Linear trend estimation ( MLR )
Absolute Percentage
Non linear trend estimation ( SVM with C=16 , γ=0.25 , ε=0.1 )
Absolute Percentage
Mean Training Error
48 823 1.47 %
91 144 2.69 %
3 857 0.12 %
4 270 0.13 %
Mean Test Error
Mean Test Error
All Parameters
Reduced Parameters
Reduced Parameters 14 Years 15 Years 14 Years 15 Years 14 Years 15 Years 14 Years 15 Years 65 136 2.00 %
37 952 1.16 %
51 531 1.54 %
All Parameters
3 762 0.11 %
3 497 0.10 %
In contrast , the results for the non linear models shown in Table 3 have a better quality compared with the PCA analysis . That originates from the saturation effect generated by the regression in conjunction with the Support Vector Machine . It leads to the fact that data points far off can still be reasonably predicted . Another advantage ( and consequence ) of this approach is the fact that a parameter reduction does not severely lower the quality of the predictions down to a threshold value of five parameters . Models with such a low number of parameters offer the chance to easily explain the predictions , which appeals to us .
A general problem , however , is the very limited amount of information that leads to a prediction of , again , limited use ( only annual predictions , no details for shortterm planning ) . Therefore , an obvious next step is to test the model with the best statistics available , ie with monthly data .
5.2 Monthly Model
In this case , the Feature Selection resulted in the following : For the linear trend model , only the parameters Model Policy and Latent Replacement Demand were relevant while in the non linear model , new car registrations were significantly influenced by the Gross Domestic Product , Disposal Personal Income , Interest Rate Model Policy , Latent Replacement Demand , Private Consumption , and Industrial Investment Demand , ie a superset of parameters of the linear case .
The results given in Table 4 are again first compared to the PCA analysis , cf . Table 2 . As for the yearly data , the non linear models are superior to both the results for the PCA analysis and for the linear model . Most accurate predictions can be achieved for the non linear model with all parameters . However , the deviations are deemed too high and are therefore unacceptable for accurate predictions in practice . One reason for this originates from the fact that most parameters are not collected and given monthly , but need to be estimated from their quarterly values . Additionally , the time lag of the parameters can only be roughly estimated and is assumed to be a constant value for reasons of feasibility .
A Sales Forecast Model for the German Automobile Market
157
Table 4 . Training and Test errors in the model used monthly data
All Parameters
All Parameters
11 082 3.95 %
11 832 4.22 %
11 516 4.11 %
35 560 12.86 %
29 767 12.72 %
38 243 13.84 %
Mean Test Error
Reduced Parameters
Mean Training Error
Reduced Parameters 14 Years 15 Years 14 Years 15 Years 14 Years 15 Years 14 Years 15 Years 33 948 10 920 3.92 % 14.24 %
Linear trend estimation ( MLR )
Absolute Percentage
Non linear trend estimation ( SVM with C=4 , γ=0.125 , ε=0.1 )
Absolute Percentage
The shortcomings of the model are most clearly visible in the case of the linear model where most of the parameters have been dropped out . Consequently , models of quarterly data basis have been investigated . They promise to be a suitable compromise between sufficient amount and stability of the provided data .
Reduced Parameters 14 Years 15 Years 14 Years 15 Years 14 Years 15 Years 14 Years 15 Years 28 966 12.25 %
Mean Training Error
Reduced Parameters
Mean Test Error
26 748 11.45 %
21 816 8.42 %
21 992 8.54 %
All Parameters
All Parameters
9 563 3.35 %
9 473 3.35 %
9 975 3.50 %
8 986 3.18 %
5.3 Quarterly Model
In this case , the Feature Selection for the linear trend approximation singled out the following parameters as being significant for the model : Interest Rate , Price Index , Latent Replacement Demand , and Industrial Investment Demand . Surprisingly , all parameters were found to be relevant in the non linear case .
Table 5 . Training and Test errors in the model used quarterly data
Mean Test Error
All Parameters
Reduced Parameters 14 Years 15 Years 14 Years 15 Years 14 Years 15 Years 14 Years 15 Years 32 099 28 815 3.42 % 3.92 %
36 426 4.45 %
50 607 6.21 %
36 278 4.32 %
All Parameters
28 869 3.44 %
Reduced Parameters
Mean Training Error
Linear trend estimation ( MLR )
Absolute Percentage
Non linear trend estimation ( SVM with C=8 , γ=0.125 , ε=0.1 )
Absolute Percentage
Mean Training Error
Reduced Parameters
35 838 4.28 %
68 418 8.31 %
22 316 2.70 %
3 503 0.42 %
3 507 0.42 %
All Parameters
Reduced Parameters 14 Years 15 Years 14 Years 15 Years 14 Years 15 Years 14 Years 15 Years
All Parameters
Mean Test Error
17 035 2.13 %
The results for the training and test errors for the models based on quarterly data are given in Table 5 . Again , the linear model is inferior compared to the PCA ( cf . Table 2 ) and compared to the non linear model . The difference between training and test errors for the linear model with all parameters is still severe . Furthermore , the total error of the linear model with reduced parameters might look small . However , a
158
B . Brühl et al . closer look reveals that this originates only from error cancellation [ 22 ] . Altogether this indicates that the training set is again too small to successfully apply this model for practical use .
The results for the non linear model , in turn , are very satisfying . They are better than the results from the PCA analysis and provide the best absolute test errors of all investigated models . This also indicates that all parameters are meaningful contributions for this kind of macro economic problem . In a previous work , we have shown that a reduction to the six most relevant parameters would more than double the test error [ 22 ] .
5.4 Summary
It can be clearly stated that the SVM provides a superior prediction ( less test errors ) compared to the MLR . This illustrates that the mutual influence of the parameters is essential to achieve accurate forecasts . In order to identify the overall best models , the errors of the training and test sets are accumulated to annual values . The results for the best model based on yearly , monthly , and quarterly data are visualized in Figure 5 .
Absolute Errors , Training Period = 1992 2006 , accumulated to Years Absolute Errors , Training Period = 1992 2006 , accumulated to Years
300000 300000 250000 250000 200000 200000 150000 150000 100000 100000 50000 50000 0 0 s s n n o o i i t t a a r r t t s s i i g g e e R R w w e e N N
2 2 9 9 9 9 1 1
3 3 9 9 9 9 1 1
4 4 9 9 9 9 1 1
5 5 9 9 9 9 1 1
6 6 9 9 9 9 1 1
7 7 9 9 9 9 1 1
8 8 9 9 9 9 1 1
9 9 9 9 9 9 1 1
0 0 0 0 0 0 2 2
1 1 0 0 0 0 2 2
2 2 0 0 0 0 2 2
3 3 0 0 0 0 2 2
4 4 0 0 0 0 2 2
5 5 0 0 0 0 2 2
6 6 0 0 0 0 2 2
7 7 0 0 0 0 2 2
Year Year
Yearly Data Yearly Data
Monthly Data Monthly Data
Quarterly Data Quarterly Data
Fig 5 . Graphical illustration of the absolute errors of the best models for a 15 years training period , cumulated to years : On yearly and monthly data the non linear model with reduced parameters , on quarterly data the non linear model with all parameters
During the training period , the best model for the monthly data is significantly worse compared to both other models . The same holds for the test period . Here , the best quarterly model is significantly superior to the best yearly model , with roughly half of the test error . It can be observed that the quarterly model does not only deliver a minor test error , but at the same time provides higher information content than the yearly model . Both models generate very low errors during the training period , showing again that the set of parameters is well adapted to our problem . The only drawback of the best quarterly model is the fact that all exogenous parameters are necessary , making the model less explicable .
A Sales Forecast Model for the German Automobile Market
159
6 Discussion and Conclusion
Based on the results of Chapter 5 , the three questions mentioned at the beginning of this contribution can now be answered .
1 .
Is it possible to create a model which is easy to interpret and which at the same time provides reliable forecasts ?
To answer this question , a more discriminate approach must be taken . Considering only the used additive model , the answer is “ yes ” , because the additive model has given better results than the Principal Component Analysis .
By looking at the different methods used in our model , answering the question becomes more difficult . Simple and easily explicable univariate estimations are used for the seasonal , calendar and error component but a more difficult multivariate method for the greatest and most important component , the trend . Thereby , the results given by the more easily explicable Multiple Linear Regression are less favorable than the results given by the less explicable Support Vector Machine . But in general , the chosen model is relatively simple and gives satisfying results in consideration of the quality of the forecast .
2 . Which exogenous parameters influence the sales market of the German automobile industry ?
Here , it has to be differentiated between the yearly , monthly and quarterly data . In the yearly model , only a few exogenous parameters are needed to get satisfying results . But it is not possible to generalize the results , because of the very small data set . Also , in the monthly model also less exogenous parameters can be used . But most of the exogenous parameters are not published monthly , so that the exact values of these parameters are not given , leading to inadequate results . However , in the quarterly model , where the highest number of exogenous parameters is explicitly given , a reduction of the exogenous parameters in our tested model is not possible without decreasing the quality of the results .
3 . Which collection of data points , yearly , monthly or quarterly data , is the most suitable one ?
Yearly , monthly , and quarterly data are regarded . The problems in the yearly model are the very small data set and the small information content of the forecast . The problems presented by the monthly model include training and test errors which are much higher than in the yearly and quarterly models . Probable causes for the weakness of the monthly model are the inexact nature of the monthly data since most of the exogenous parameters are not collected monthly . The problems of the yearly model as well as the problems of the monthly model can be solved by using the quarterly model . Therefore , the quarterly model is the superior method , even though no reduction of exogenous parameters is possible in this model .
To conclude , it should be pointed out that forecasts are always plagued by uncertainty . There can be occurrences ( special effects ) in the future , which are not predictable or whose effects can not be assessed . The current financial crisis , which led to lower sales in the year 2008 , is an example for such an occurrence . Because of this fact , forecasts can only be considered as an auxiliary means for corporate management and have to be interpreted with care [ 23 ] .
160
B . Brühl et al .
References
1 . Lewandowski , R . : Prognose und Informationssysteme und ihre Anwendungen . de Gruyter ,
Berlin ( 1974 )
2 . Lewandowski , R . : Prognose und Informationssysteme und ihre Anwendungen Band II . de Gruyter , Berlin ( 1980 )
3 . Dudenhöffer , F . : Prognosemethoden für den PKW Markt : Das Beispiel Dieselfahrzeuge .
In : WISU Wirtschaftsstudium , pp . 1092–1100 ( 2002 )
4 . Dudenhöffer , F . , Borscheid , D . : Automobilmarkt Prognosen : Modelle und Methoden . In : Automotive Management . Strategie und Marketing in der Automobilwirtschaft , pp . 192–202 ( 2004 )
5 . Bäck , T . , Hammel , U . , Lewandowski , R . , Mandischer , M . , Naujoks , B . , Rolf , S . , Schütz , M . , Schwefel , H P , Sprave , J . , Theis , S . : Evolutionary Algorithms : Applications at the Informatik Center Dortmund . In : Genetic Algorithms in Engineering and Computer Science , pp . 175–204 ( 1997 )
6 . Statistisches Landesamt des Freistaates Sachsen , http://wwwstatistiksachsende/21/14_01/ 14_01_definitionen.pdf ( last accessed Feburary 2009 )
7 . Hüttner , M . : Markt und Absatzprognosen . Kohlhammer , Stuttgart ( 1982 ) 8 . Stier , W . : Methoden der Zeitreihenanalyse . Springer , Heidelberg ( 2001 ) 9 . Stier , W . : Verfahren zur Analyse saisonaler Schwankungen in ökonomischen Zeitreihen .
Springer , Heidelberg ( 1980 )
10 . Box , GEP , Jenkins , GM : Time Series Analysis forecasting and control . Holden Day ,
San Francisco ( 1976 )
11 . Leiner , B . : Einführung in die Zeitreihenanalyse . R . Oldenbourg Verlag , München Wien
( 1982 )
12 . Kessler , W . : Multivariate Datenanalyse . Wiley VHC ( 2007 ) 13 . Vapnik , V . : The Nature of Statistical Learning Theory . Springer , Heidelberg ( 1995 ) 14 . Schölkopf , B . , Smola , A . : Learning with Kernels . MIT Press , Cambridge ( 2002 ) 15 . Christianini , N . , Shawe Taylor , J . : An Introduction to Support Vector Machines and other kernel based methods . Cambridge University Press , Cambridge ( 2000 )
16 . Chen , K . , Wang , C . : Support vector regression with genetic algorithms in forecasting tour ism demand . In : Tourism Management , pp . 1–13 ( 2006 )
17 . Trafalis , TB , Ince , H . : Support Vector Machine for Regression and Applications to Financial Forecasting . In : International joint conference on neutral networks , vol . 6 , pp . 348–353 ( 2000 )
18 . Yale School of Public Health , http://publichealthyaleedu/faculty/labs/guan/ Papers%20under%20review/KPSS.pdf ( last accessed Feburary 2009 )
19 . Dunteman , GH : Principal Component Analysis . Sage Publications , Thousand Oaks
( 1989 )
20 . Kohavi , R . , John , GH : Wrappers for feature subset selection . Artificial Intelligence Jour nal , Special Issue on Relevance , 273–324 ( 1997 )
21 . Witten , IH , Frank , E . : Data Mining . Morgan Kaufmann Publishers , San Francisco ( 2005 ) 22 . Brühl , B . : Absatzprognosen für die Automobilindustrie in der Bundesrepublik Deutschland .
Diploma Thesis , University of Cologne ( 2008 )
23 . Taleb , NN : The Fourth Quadrant : A Map of the Limits of Statistics . Edge 257 ( 2008 ) , http://wwwedgeorg ( last accessed April 2009 )
