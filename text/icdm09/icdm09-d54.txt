Fast Online Training of Ramp Loss Support Vector Machines
Zhuang Wang and Slobodan Vucetic
Department of Computer and Information Sciences zhuang@temple.edu and vucetic@isttempleedu
Temple University Philadelphia , USA
Abstract—A fast online algorithm OnlineSVMR for training Ramp Loss Support Vector Machines ( SVMRs ) is proposed . It finds the optimal SVMR for t+1 training examples using SVMR built on t previous examples . The algorithm retains the Karush–Kuhn–Tucker conditions on all previously observed examples . This is achieved by an SMO style incremental learning and decremental unlearning under the ConcaveConvex Procedure framework . Further speedup of training time could be achieved by dropping the requirement of optimality . A variant , called OnlineASVMR , is a greedy approach that approximately optimizes the SVMR objective function and is suitable for online active learning . The proposed algorithms were comprehensively evaluated on 9 large benchmark data sets . The results demonstrate that OnlineSVMR ( 1 ) has the similar computational cost as its offline counterpart ; ( 2 ) outperforms IDSVM , its competing online algorithm that uses hinge loss , in terms of accuracy , model sparsity and training time . The experiments on online active learning show that for a fixed number of label queries OnlineASVMR ( 1 ) achieves consistently better accuracy than QueryAll and competitive accuracy to Greedy approach ; ( 2 ) outperforms the active learning version of IDSVM .
Keywords SVM , CCCP , SMO , ramp loss , online learning
I .
INTRODUCTION
Xx ∈
Online learning is an important learning scenario in which a potentially unlimited stream of training data is presented one example at a time , and can only be seen in a single pass . This is opposed to offline learning where the whole collection of training examples is at hand . The objective of online learning is to learn a mapping f : X→R from a training stream D = {(xt , yt ) , t = 1 , 2} that accurately predicts target variable y given an M dimensional input . For online learning there is the anytime vector requirement that implies the ability to update the prediction model f(x ) after each new example and allows its instant use for prediction . Considering the potentially high stream rates , it becomes very important to update the model in a computationally efficient manner . Typically , online algorithms attempt to update the existing f without retraining it from scratch .
There have been many online algorithms [ 1 , 2 , 5 ] developed for optimization on a regularized hinge loss cost function . IDSVM [ 2 ] is one of the earliest successful solutions . It maintains the optimal Support Vector Machine ( SVM ) [ 16 ] solution after a new example is added . LASVM [ 1 ] and its improved version [ 8 ] have been proposed to retains
−<i ) 1 speed up online SVM training for large scale data by dropping the requirement of optimality . Similarly to their offline counterparts that use hinge loss , their drawback is sensitivity to noise and outliers . This comes from the nature of the hinge loss cost function in which all the noisy examples become the Support Vectors ( SVs ) during training . Moreover , since the SVM training and prediction time grows with the number of SVs , the scalability of hinge loss SVM is poor on noisy data .
) no longer become SVs . .
To address drawbacks of hinge loss , recently , an efficient offline algorithm [ 4 ] based on ConCave Convex Procedure ( CCCP ) [ 17 ] has been proposed for SVMs by using a regularized ramp loss cost function . In this new Ramp Loss SVM ( SVMR ) optimization problem , the noisy examples ( ie xfy ( i In this paper , we propose a fast online modification of SVMR for large scale online learning tasks , called the online Ramp Loss SVM ( OnlineSVMR ) . OnlineSVMR produces the optimal solution for SVMR on t+1 training data using the existing optimal solution on t previous examples and efficiently updating it given the new ( t+1) th example . The algorithm ( KKT ) conditions on all previously observed examples using an SMO style incremental learning and decremental unlearning approach under the CCCP framework . By only maintaining a fixed number of non SVs closest to the decision boundary , the training time of OnlineSVMR could be further improved while retaining competitive accuracy . the Karush–Kuhn–Tucker
Applicability . Online learning algorithms [ 1 , 2 , 5 ] have been combined with other machine learning scenarios ( eg active learning and budget learning ) in many practical applications . Coupled with the higher accuracy and robustness to noise , faster training time , and sparser model , OnlineSVMR could be used as an efficient alternative to these algorithms . To demonstrate the applicability of OnlineSVMR , OnlineSVMR with active learning is also proposed . Moreover , it could be shown that a greedy variant of OnlineSVMR for active learning could be viewed as an approximate optimization process over a regularized ramploss cost function .
II . PRELIMINARIES
A . Zero bias hinge loss SVM and Karush Kuhn Tucker conditions We consider a linear function f(x ) = wTΦ(x ) + b with a bias threshold b fixed at 0 , where w is the weight vector and
Φ is a nonlinear mapping of the attribute space . Zero bias does not lead to loss of generality [ 12 ] , and can lead to the simplification of training [ 6 , 11 , 13 ] . From this point on we assume that form f(x ) = wTΦ(x ) is used .
The SVM [ 16 ] classifier f(x ) is trained from data set D = {(xi , yi ) , i = 1N} , xi ∈R , yi ∈{−1,+1} by optimizing the primal problem
) ) x ( f , y (
H
3
2.5
2
1.5
1
0.5
0
2
Hinge loss
1
0
1
2
) ) x ( f , y (
R
3
2.5
2
1.5
1
0.5
0
2
Ramp loss
1
0
1
2 min
P
( w
)
=
⎛ 1min ⎜ ⎜ 2 ⎝
|| w
||
2
+
C
N
∑
= 1 i yH (
, i xf ( i
) )
⎞ ⎟ ⎟ ⎠
( 1 ) yf(x ) yf(x )
Figure 1 . Hinge loss vs . Ramp loss . yHy ( ' i xfy ( i
Φ
) ) x
)
(
, i i i
−= ∑w
C i
( 5 )
=
− i i
,
) ) xf ( yH ( is the hinge loss where ( see Figure 1 ) and C is a user specified parameter to balance the model structure and training loss . xfy ( i
1max(
)0 ) ,
In the dual formulation of the optimization , the primal i problem is transformed to
Thus , all examples with =i xf yH hinge loss derivative is nonzero ( ie ( ) ) ( ' , i −<i xfy ( This includes all noisy examples with ) 1 . i become SVs as their ) .
To address this drawback of hinge loss , the non convex xfy ( i
1
≤i 1 ) ramp loss max
D
α )( st
⋅≤≤
α0
= max
⎛ ⎜ ⎝ ,C
1
T α1
−
1 2
T
α
Qα
⎞
⎟ ⎠
( 2 ) yR (
, i xf ( i
) )
=
,0
⎧ ⎪ − xfy 1 ( ⎨ i ⎪
,2 ⎩ xfy ( i i ≤−
) , 1 i xfy
( i i
> 1 ) xfy ( i −< ) 1
≤ 1 ) i
( 6 )
)
, ,
( 1 are
Nαα=α the Lagrange multipliers where associated with the primal problem , Qij = yiyjk(xi,xj ) is the element of the Gram kernel matrix Q , and k is the positive definite kernel function satisfying ) .
Note that , unlike the standard SVM dual form , the linear constraint vanished in ( 2 ) because of b = 0 . conditions necessary and sufficient for the optimal solution of ( 2 ) ,
The Karush–Kuhn–Tucker
0=αy T
( KKT ) xxk ( ,
Φ= are
Φ x x
)
(
(
)
T j i j i
= g i
∂ α D )( ∂ α i
−= 1 xfy ( i i
)
< = >
= α 0
;0 i ∈ α C ,0( ;0 ) i = α C
. ;0 i
⎧ ⎪ ⎨ ⎪ ⎩
( 3 )
We call the examples not satisfying ( 3 ) the KKT violators . The resulting SVM classifier can be conveniently represented in the dual form as xf )(
∑= i y i
α i xxk , ( i
.
)
( 4 )
( see Figure 1 ) was introduced in [ 4 ] . Replacing H by R in ( 1 ) , it could be seen that the examples with do not become SVs . The new ramp loss SVM ( SVMR ) formulation then reads as
−<i ) xfy ( i
1 min
P
R
( w
)
=
⎛ 1min ⎜ ⎜ 2 ⎝
|| w
||
2
+
C
N
∑
= 1 i yR (
, i xf ( i
) )
.
⎞ ⎟ ⎟ ⎠
( 7 )
C . ConCave Convex Procedure ( CCCP )
To solve the non convex objective function ( 7 ) , CCCP [ 17 ] is used . CCCP optimizes the non convex problem by solving a sequence of approximate convex problems and has no additional parameters to tune .
To optimize ( 7 ) using CCCP , the non convex objective and concave function is decomposed into convex J parts , which can be rewritten as
( wvexJ
( w
)
) cave
The training examples with Vectors ( SVs ) . B . Ramp loss SVM
0≠iα are called Support min
P
R
( w
)
=
⎛ ⎜ N 1min ⎜ ∑ w yHC xfy || ) ) ( ( i i ⎜ 2 = ffffffffff ffffffffff
( ) i 1 ⎜⎜ ⎝ J
+
|| vex
,
2 i
( 8 )
One practical issue with SVM is its scalability . Recent results [ 15 ] show that the number of SVs scales linearly with the number of training examples . The SV scaling property is in fact a property of the hinge loss function . It could be shown through differentiating ( 1 ) ( and assuming the hinge loss is differentiable at 1 with a smooth approximation ) [ 4 ] that the optimal solution w must satisfy
+
( yR (
N ∑ xfy C ( ) ) i i = ( ffffffffffffff ) i 1 J
, i cave yH (
⎞ ⎟ ) . ⎟ − ) ) i ⎟ ffffffffffffff
⎟⎟ ⎠ xfy ( i
, i initial guess .
CCCP runs in iterations . First , the procedure sets wold with an the procedure approximates Jcave(w ) by its tangent wTJcave'(wold ) , where wold indicates w in the previous iteration , and then optimizes the resulting objective function
In each iteration , new w
= arg
+
T w
∑
∉ Vi w
1min ⎛ ⎜ 2 ⎝ Φ ( y x i
|| w
||
2
+
C
∑
N = i 1 yH (
, i xfy ( i i
) )
⎞ . ) ⎟ ⎠ i
( 9 ) where V is the active subset of training examples with margin larger than −1 , formally ,
V
∀= ,{ xfyi ( i
=
) i y i
( ( w old
T
)
Φ
( x i
) )
−≥
}1
.
( 10 )
There is the convergence guarantee [ 17 ] that the procedure would decrease the non convex objective function in each iteration . The resulting optimization problem ( 9 ) is convex and can be reformulated by its dual form : max st
α VD ( ≤ α0 V
T α1 V
−
1 2
T αQα V V
VV
⎞
⎟ ⎠
( 11 )
)
= max
⎛ ⎜ ⎝ ,C
1
⋅≤
=
− where αV and QVV correspond to the active set examples and α ’s of examples in is set to zero . It is worth noting that the optimization problem ( 11 ) is equivalent to training a standard SVM on V .
VDV
≤ 1| ) xfy ( i i α ∀ , i
Using the CCCP algorithm , the procedure iteratively retrains SVMs on the dynamically updated V until V remains unchanged , and eventually has all the SVs within the ramp region ( ie
⇔≠ |
Cons . Frequently retraining SVM from scratch and reallocating V is computationally expensive even for offline training of SVMR . Directly applying the offline SVMR algorithm [ 4 ] to online learning by retraining each time a new example is observed is impractical . This drawback leads us to the proposed efficient online algorithm that formulates the optimal solution for t+1 examples in terms of the existing solution for t previous examples and efficiently accounting for the new ( t+1) th example .
) .
0 i
III . ONLINE RAMP LOSS SVMS ( ONLINESVMR )
In this section , we describe our proposed online training loss SVM , called OnlineSVMR . algorithm for ramp OnlineSVMR guarantees the optimal solution of ( 7 ) after every new training example is received .
An efficient online modification of CCCP serves as the core of OnlineSVMR . The key of this modification is to retain the KKT conditions on all previously seen examples , while iteratively adding or deleting an example from the t
I
,
∅=I
∅=V
Algorithm 1 : OnlineSVMR 0 . f(x)=0 , 1 . repeat 2 . receive the next example ( xt+1,yt+1 ) ; I y 3 . t 4 . if gt+1 5 . 6 . calculate ααα Δ old 7 . set + + t t 1 1 i ∈∀ ,I update gi using Eq ( 15 ) ; 8 . 9 . repeat 10 . while (
{ +∪= αt+1 = 0 , },1 ]2,0[∈
{ +∪= t }1 t 1+α using Eq ( 14 ) ; new =
VV
−= 1 new + t 1
+ 1
−
;
; g t xf (
+ 1 t
)
;
+ 1 violates Eq ( 3 ) or
Vi ∈∃ Eq ( 13 ) wouldn’t be improved much )
;
− new i * i*α using Eq ( 14 ) ; new =
11 . find a KKT violator i* in V using Eq ( 13 ) ; 12 . calculate ααα Δ old 13 . set i i * * i ∈∀ I , update gi using Eq ( 15 ) ; 14 . 15 . endwhile i ∈∀ 16 . for I −∞∈ Vi ∉ 17 . if gi ( &]2 , ∪= VV }{i 18 . ; +∞ ∈ 19 . elseif gi ,2( & ) = − VV }{i 20 . ; 21 . endif 22 . endfor 23 . for iα new 24 . set k ∈∀ ,I 25 . 26 . endfor 27 . until V is not changed 28 . endif 29.until the end of the stream i∀ , just being removed from V i αα −= old i
0= ; update gk using Eq ( 15 ) ;
Vi ∈ and
Δ active set V ( see ( 10) ) . To achieve this , an SMO style incremental learning and decemental unlearning method is proposed to guarantee the KKT conditions are maintained during the update . An efficient working set selection strategy is designed that minimizes kernel computations and achieves faster convergence . A . Outline of OnlineSVMR ( Algorithm 1 ) this method for
We outline the underlying structure of OnlineSVMR in this section . The details are given in the following sections . OnlineSVMR starts by initializing a zero classifier and creating empty sets I and V . Here , I is a set of all the observed examples and V is a subset of I as defined in ( 10 ) . At time t+1 , after adding the newly received example t+1 into I , initializing αt+1 = 0 , and calculating the gradient gt+1 ( defined in ( 3) ) , the algorithm checks the KKT conditions for this initial assignment of t+1 :
If gt+1 < 0 , then t+1 satisfies the KKT conditions of ( 11 ) and thus the existing solution of ( 7 ) from the first t examples is still optimal after its inclusion .
•
•
If gt+1 > 2 , then t+1 is a KKT violator and is at the noisy region of ramp loss ; thus , it would not change the current V ( according to the definition of V in ( 10 ) ) and the current optimal solution of ( 7 ) is still valid .
]2,0[∈ loss ( ie gt+1
• Otherwise , t+1 is within the linear region of ramp ) , and thus the model is updated . In the event of model update , ( t+1) st example is added to V , αt+1 is calculated and then the KKT conditions for all the observed examples ( set I ) are updated with respect to the obtained αt+1 . The above step ( and , similarly , Steps 14 and 25 ) is important . The updating of KKT conditions for V guarantees the progress of optimization at this step , while the updating for the examples outside V is for a fast relocation of the new V in future steps . Following this , the algorithm updates the solution through online CCCP iterations ( Steps 9 to 27 ) . In the first part ( Steps 10 to 15 ) of this iterative process , ( 11 ) is iteratively optimized on V until the stopping criteria is reached . After that , V is quickly reassigned with respect to Eq ( 10 ) ( Steps 16 to 22 ) . If V is modified , the examples removed from the previous V are decrementally unlearned from the current solution and all KKT conditions are updated accordingly ( Steps 19 to 22 ) . This process ( Steps 9 to 27 ) iterates until V stops changing . Thus , the optimal solution of ( 7 ) is maintained upon addition of a new example . B . Online updating
In this section we give details about the online optimization used in Algorithm 1 . As we already pointed out , the optimization of ( 11 ) is equivalent to training an SVM on V and setting α values of the examples outside V to zero . For the online updating , the goal is to maintain KKT conditions on all the previously seen examples . To achieve this , we propose an SMO style online updating . SMO [ 14 ] is an iterative process where in each iteration the smallest set of examples is selected , and the dual problem is optimized with respect to them . This smallest set is called the working set .
In our case , the working set only involves a single example because the fixed threshold b removes the need for the linear constraint in the dual . Hence , the smallest suboptimization problem when the example i is used as the working set reads as ⎛ ⎜ ⎝
ααα ) i old αQ UUi
1(max
⎞
⎟ ⎠ max
1 2
Q
D
−
− ii i i
( 12 )
,
0 st where U = V−{i} denotes set of all the examples excluding the i th example .
There are many ways to select i . The working set selection is closely related to the optimization progress and the cost of kernel computation . From a greedy view , the most efficient working set in each iteration should be the one that achieves the largest improvement of the dual objective function ( 12 ) . Formally , the optimal selection is obtained as
)
=
α ( i α ≤ i C
≤
( D
= i arg max ∈ Vj
α new ( j
= arg
+
(
1max ⎛ ⎜ 2 ⎝ + Q
∈ Vj g old j
Q jj
−
D
) ( α old ( j
)
)
α old ( j
2
)
−
α new ( j
ααα old j new j old j
)(
− jj
)
2
) ) . )
( 13 )
The computation of ( 13 ) is cheap , merely proportional to the number of KKT violators in V . An appealing part of this procedure over the other popular working set selection methods [ 7 , 8 ] is that it is free of kernel computation and all the values can be directly read from memory if some specific kernels are applied ( eg if RBF kernel is used , then Qii becomes constant ) .
The optimal solution of ( 12 ) leads to update < >
+ +
=
Qg i Qg i
/ /
α new i ii ii
⎧
,0 ⎪⎪ C
, ⎨ ⎪ α old ⎪ i ⎩
Qg / i
+ ii
α old i α old i if if
, otherwise .
,0 C ,
( 14 ) g new k
∂ α D ( α ∂ ( k −= y 1 k = + old g k Δ = new i i
After αi is updated , the KKT conditions of all training examples are updated as well , with respect to the new αi . Specifically , = new new
)
−= 1 fy k
( x k
)
)
( 15 ) i i i k k
) y old thus
Δ+ xxk ( ,
α xxk ( , i ) x f ) ( k α Δ yy i i k − ααα old where . Given an SVM solution on the i current V and a newly added example , iterative execution of Steps 11~14 in Algorithm 1 resolves one KKT violation at a time , ( 11 ) and guaranteeing convergence . the dual objective increasing
Decremental unlearning due to removal of an example from V can be implemented naturally using the same approach as above . For i to be unlearned , we first set αi = 0 and then update the remaining α values in set I using ( 14 ) . We repeat this procedure for every example being removed from V ( Steps 23~26 in Algorithm 1 ) .
Stopping criteria . We use two different stopping criteria for the model optimization ( the condition in Step 10 in Algorithm 1 ) . The first one checks the KKT conditions ( 3 ) . If all gi are less than 10−3 from the desired value the procedure stops . This method is standard , and is also used in SMO [ 14 ] . The second criterion considers the optimization progress ; if the dual objective function could not significantly improve ( eg less than 10 5 ) , as measured by ( 13 ) , then the iterations stop . Using ( 13 ) to estimate progress could significantly reduce the computation cost . These two criteria ( with 10−3 and 10−5 the default setting for our algorithms . C . Improving scalability thresholds ) are
It is possible to modify Algorithm 2 to trade off some accuracy for quicker update time and lower memory consumption . This can be achieved by only maintaining in I only a fixed number of the non SVs that are closest to the decision boundary ( this can be directly measured by |1 − gi| ) , and discarding the remaining ones . The non SVs that are far from the decision boundary are not very likely to become SVs in the future . In practice , this removal step could be executed right after Step 28 in Algorithm 1 as described in Algorithm 2 . There , removal criterion is triggered ( when a predefined maximal number of non SVs is reached ) .
Algorithm 2 : The removal step α = 1 . if |X| > m /* i value*/ 2 . remove ( |X| − m ) non SVs with the largest |1− g| 3 . endif
, m is a predefined
∀= i ,{
}0
X
IV . ONLINESVMR WITH ACTIVE LEARNING AS NON
CONVEX OPTIMIZATION
OnlineSVMR can be modified for online active learning , where labels are queried only for some of the training examples from the stream . Instead of querying labels from all the observed examples , OnlineASVMR ( Algorithm 3 ) only queries examples within the ramp region ( ie ≤xf ) . In other respects , it is identical to OnlineSVMR . | 1|)( The justification comes from the fact that an example outside of the ramp region is not placed in V ( it is placed in I though ) , its α value is set to zero , and the solution remains unchanged . Only if such an example finds itself within the ramp region during the training procedure , it is placed in V and its α value becomes nonzero , and thus the solution is improved . The farther away the new example is from the ramp region the smaller the chance that it will ever be used to update the solution .
More recently , a novel view of active learning as a nonconvex optimization problem was suggested [ 9 ] . There , the perceptron algorithm with such label filtering step is viewed as a stochastic gradient descent over a ramp loss cost function . Following this view , we argue that OnlineASVMR could be considered as a greedy optimization approach that approximately optimizes the regularized ramp loss cost function ( 7 ) . active learning
|
∅=V
3 : OnlineSVMR with
Algorithm ( OnlineASVMR ) ∅=I 0 . f(x)=0 , , 1 . repeat 2 . receive the next unlabeled example xt+1 ; 3 . if 4 . query label yt+1 ; { +∪= VV ; 5 . , }1 , αt+1 = 0 ; xf 6 . ( ) + + t t 1 1 7 . Steps 6 to 27 from Algorithm 1 ; 8 . endif 9 .until the end of the stream
1 ≤ +txf ( 1| ) { +∪= I t −= y 1 + t 1
I g
}1 t
V . EXPERIMENTS
A . Experimental setting
Datasets . We performed experiments on 9 benchmark binary classification data sets summarized in the first column of Table 1 . The multi class data sets were converted to twoclass sets as follows . For the digit dataset USPS we converted the original 10 class problems to binary by representing digits 1 , 2 , 4 , 5 , 7 ( non round digits ) as negative class and digits 3 , 6 , 8 , 9 , 0 ( round digits ) as positive class . For 3 class DNA data set class 3 was separated from the other 2 classes . Class 1 in the 3 class Waveform was treated as negative and the remaining two as positive . For Covertype data the class 2 was treated as positive and the remaining 6 classes as negative . Adult , Banana , Checkerboard and Gauss were originally 2 class data sets . NCheckerboard is a noisy version of Checkerboard where class assignment was switched for 15 % of the randomly selected examples . For both data sets , we used the noise free Checkerboard as the test set . Attributes in all data sets were scaled to mean 0 and standard deviation 1 .
Algorithms . We used two online and one offline algorithm with hinge loss to compare with OnlineSVMR and OnlineASVMR :
• IDSVM [ 2 ] : an online SVM algorithm which is guaranteed to achieve the optimal solution . • Online Passive Aggressive ( PA ) algorithm [ 5 ] : a popular online algorithm based on perceptron style updating . • LibSVM [ 3 ] : one of the most successful offline kernel , tuning .
SVM algorithms with hinge loss . RBF
Hyperparameter k(xi , xj ) = exp(−||xi−xj||2/2δ 2 ) , was used in all experiments . We selected the best hyper parameters C and δ 2 using crossvalidation for all combinations of algorithm and data set . The considered values were C = {0.1 , 1 , 5 , 10 , 50 , 100 , 500} and δ 2 = {M/2−1 , M/20 , M/21 , M/22 , M/24 , M/26} , where M is data dimensionality . B . Illustration on a 2 D dataset the
Let us first illustrate the detailed comparison between the proposed and the previous algorithms on Gauss data set . Gauss is a noisy benchmark dataset [ 10 ] which consists of two overlapping 2 D Gaussian distributions ( see Figure 2a ) OnlineSVMR & OnlineASVMR vs IDSVM . In Figures IDSVM , OnlineSVMR , and OnlineASVMR 2.b d solutions are compared . It can be observed that OnlineSVMR and OnlineASVMR solutions are much sparser than that of IDSVM . As expected , their SVs are all inside the margins , while the SVs of the IDSVM solution are widely distributed and also include the noisy examples outside the margin . In Figure 3.a we show the accuracy as a function of the data stream size for the three algorithms ( IDSVM was terminated after 5,000 seconds of training because it reached the resource limits of our PC ) . It can be seen that OnlineSVMR consistently achieved higher accuracy IDSVM , especially in the initial stages of training ( see the initial points of two curves ) . The accuracy curve of OnlineASVMR than
3
2
1
0
1
2
3
3
3
2
1
0
1
2
3
2
1
0
1
2
3
2
1
0
1
2
2
3
0
1
2 ( a ) Data plot ( b ) IDSVM ( c ) OnlineSVMR ( d ) OnlineASVMR
2
1
2
2
1
1
2
1
2
0
1
2
3
1
0
1
3
3
0
3
3
3
3
3
3
Figure 2 . Plots of solution on Gauss data set . Red circles are SVs , black line is decision boundary , and yellow and cyan lines are positive and negative margins . the closely follows that of OnlineSVMR and reaches it after 10,000 observed examples . This is expected because , as the accuracy of OnlineASVMR increases , there is less of a risk that an unlabeled point will end up within the ramp region . Figure 3.b illustrates that OnlineSVMR and OnlineASVMR learn sparser models than IDSVM .
Computational improvement . First , we show the computational cost for ( 1 ) OnlineSVMR , ( 2 ) offline SVMR [ 4 ] trained on demand , and ( 3 ) the naive online algorithm that repeatedly retrains offline SVMR from scratch in an online setting . To reduce influence of different implementation , the computational cost is expressed as total number of kernel computations . For offline SVMR training , the initial model was trained on a subset of training data to speed up computation , as suggested in [ 4 ] . From Figure 4.a , we can observe that the computational cost of OnlineSVMR is significantly less than the naive approach of retraining SVMR . Interestingly , total cost of OnlineSVMR training is comparable to training a single offline SVMR . This result clearly demonstrates the success of the proposed online solution . In Figure 4.b , we compare the training time of OnlineSVMR and OnlineASVMR with IDSVM . As can be seen , the proposed two algorithms lead to a large reduction in training time over IDSVM due to the sparser models . C . Results on benchmark datasets
The results on 9 benchmark data sets are summarized in c c A
0.82
0.8
0.78
0.76
0.74
0.72
0.7
101
104
IDSVM onlineSVMR onlineASVMR
102 103 data stream length
Table 1 . Mean and standard deviation of 10 repeated experiments for accuracy and number of SVs are reported . If needed , we terminated IDSVM after 5,000 seconds of training .
Accuracy . On the accuracy side , we can observe that OnlineSVMR is significantly more accurate than LibSVM on 5 out of 9 data sets and IDSVM on 4 out of 9 data sets . The largest accuracy improvement ( up to 2 % ) happens on two noisy data sets NCheckerboard and Covertype . The accuracy of OnlineASVMR is very competitive to both IDSVM , LibSVM and OnlineSVMR and is usually within 1 % of their accuracy . Finally , perceptron based PA algorithm is less competitive and it is significantly less accurate than the others on 7 out of 9 data sets .
Sparsity . On the model sparsity side , OnlineASVMR achieves the sparest model on 6 out of 9 data sets . OnlineSVMR is as sparse as OnlineASVMR in most cases , and it obtains the sparsest model on 2 data sets . Both SVMRbased algorithms are much sparser than the hinge loss based IDSVM , LibSVM and PA . In Gauss data set , OnlineSVMR and OnlineASVMR are about 10 times sparser than LibSVM . PA results in the densest models . In the extreme case , for Cover data set , PA includes all the training data as SVs . D . Scalability
In the scalability experiment , we evaluated OnlineSVMR with the removal step ( Algorithm 3 ) . We explored how the
3000
2500
2000
V S #
1500
1000
500
0
0
IDSVM onlineSVMR onlineASVMR
2000
4000
6000
8000
10000
( a ) Accuracy ( b ) Sparsity data stream length
Figure 3 . Comparisons of accuracy and sparsity on Gauss data set n o i t a t u p m o c l e n r e k #
1012
1011
1010
109
108
107
106
105
104
101
SVMR online retrain SVMR onlineSVMR
102 103 data stream length
104
103
102
101
100
10 1
10 2 e m i t
104
10 3
101
IDSVM onlineSVMR onlineASVMR
103 102 data stream length
104
( a ) OnlineSVMR vs SVMR & Naïve online retrain SVMR ( b ) OnlineSVMR & OnlineASVMR vs IDSVM
Figure 4 . Comparison of computational cost as a function of data stream length on Gauss data . parameter m trades off accuracy and scalability . From Figures 5.a and c , it can be seen that the computational cost of OnlineSVMR decreases with m . For aggressive removal with m = 0 , the accuracy loss is significant and should not be recommended ( see Figures 5.b and d ) . Choice of m = 100 is more appropriate , as it results in only a small accuracy loss that decreases with data stream size and still allows significant savings in computational time . If achieving the best accuracy is a critical requirement , m = 1000 seems to be a good choice because accuracy loss in negligible and computational savings are still significant . E . Online active learning experiments
In sections 5.2 and 5.3 we have already observed that OnlineASVMR achieves comparable accuracy and number of
SVs to OnlineSVMR and that it is somewhat faster . In this section , we give detailed results about growth in accuracy of OnlineASVMR as a function of the labeling effort . To gain better its performance with 2 competing active learning approaches implemented on top of OnlineSVMR and IDSVM algorithms : insight , we compared
• QueryAll : ask the label of every observed example . This is identical to the standard online learning and serves as the baseline approach .
• Greedy : ask label of the example with the largest ) from the h most uncertainty ( ie recently observed examples . In our experiments , we used h=5 as proposed before [ 9 ] . Note that when h=1 , Greedy is identical to QueryAll . xf |))( arg min(|
TABLE I .
ACCURACY AND MODEL SPARSITY COMPARISON ON BENCHMARK DATA SETS .
Algorithms Datasets
Adult ( 10000×123 ) Banana ( 4300×2 ) Checkerborad ( 10000×2 ) NCheckerboard ( 10000×2 ) Cover ( 10000×54 ) DNA ( 2000×180 ) Gauss ( 10000×123 ) USPS ( 7291×256 ) Waveform ( 10000×21 )
Acc #SV Acc #SV Acc #SV Acc #SV Acc #SV Acc #SV Acc #SV Acc #SV Acc #SV
LibSVM [ 3 ]
IDSVM [ 2 ]
PA [ 5 ]
OnlineSVMR
OnlineASVMR
840±03 4275±57 900±10 926±17 992±01 455±13 965±03 4679±27 798±04 5164±82 950±00 1080±1 819±03 4183±74 973±00 1126±0 885±02 2629±45
843±02 4093±50 900±08 995±14 991±02 477±17 966±05* 3196±48* 808±04* 3529±22* 952±00 817±0.4 815±06* 2522±76* 973±00 1180±0 897±05 2545±43
823±02 7764±19 887±04 1195±16 971±03 1990±29 899±07 5605±66 815±04 10000±0 938±02 1680±2 76.4±5 4767±47 967±03 2205±18 890±04 8273±19
845±02 2631±68 908±10 713±16 995±00 492±2 986±02 554±19 818±04 3898±69 951±01 796±4 818±02 465±93 975±01 1166±10 894±03 1796±51
845±02 2497±54 899±04 303±11 990±02 438±11 980±02 987±31 808±04 2408±135 952±00 1097±4 818±02 304±27 971±00 1152±14 895±04 1711±41
The highest accuracy and the smallest #SV for each data set are in bold . * means early stopped after 5,000 seconds .
Cover onlineSVMR+Removal,m=0 onlineSVMR+Removal,m=100 onlineSVMR+Removal,m=1000 onlineSVMR
1010
109
108 n o i t a t u p m o c l e n r e k
#
C C A
0.82
0.8
0.78
0.76
0.74
0.72
0.7
Cover onlineSVMR+Removal,m=0 onlineSVMR+Removal,m=100 onlineSVMR+Removal,m=1000 onlineSVMR
103
104
107
1011
1010
109
108
107
106
105
102 n o i t t a u p m o c l e n r e k
#
103
104 data stream length ( a ) ( b ) data stream length
Ncheckerboard
Ncheckerboard onlineSVMR+Removal , m=0 onlineSVMR+Removal , m=100 onlineSVMR+Removal , m=1000 onlineSVMR 103
104
C C A
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
102 onlineSVMR+Removal , m=0 onlineSVMR+Removal , m=100 onlineSVMR+Removal , m=1000 onlineSVMR 103
104
( c ) ( d ) data stream length data stream length
Figure 5 . Influence of number of non SVs on accuracy and computational cost on Ncheckerboard and Cover data sets .
Figure 6 shows results for each of the 9 data sets . The maximal number of labels in each plot is obtained when one of the 5 competing algorithms reaches the end of the data stream .
Let us first discuss performance of 3 active learning algorithms based on OnlineSVMR . The QueryAll strategy is consistently and significantly less accurate than Greedy and OnlineASVMR . OnlineASVMR and Greedy appear to have strengths on different data sets . While Greedy is superior on DNA , USPS , and Waveform data sets , OnlineASVMR is superior and NCheckerboard . Their performance on the remaining 2 data set is similar . These results are interesting because Greedy focuses on examples near the decision boundary , while OnlineASVMR is gentler and labels all examples within the margin . It would be interesting to explore in future work if the size of labeling region can be optimized to each individual data set . on Gauss , Cover , Checkerboard ,
Comparing ramp loss OnlineSVMR and hinge loss IDSVM , it is evident that OnlineSVMR is superior on majority of data sets . This is another proof of the success of the ramp loss measure .
VI . CONCLUSIONS
We presented a fast online algorithm for ramp loss SVM training . The proposed algorithm OnlineSVMR significantly outperforms its competing algorithm IDSVM in terms of accuracy , model sparsity and training time . A variant of OnlineSVMR for active learning could be viewed as an approximate optimization process over a regularized ramp loss cost function . The applicability of OnlineSVMR in active learning scenario has been demonstrated , as it showed the advantages over the competing algorithm . The application of OnlineSVMR to other machine learning scenarios ( eg budget learning ) could be further studied .
ACKNOWLEDGMENT
This work was supported by the US National Science
Foundation under Grant IIS 0546155 . REFERENCES
[ 1 ] Bordes , A . , Ertekin , S . , Wesdon,J and Bottou , L . , “ Fast kernel classifiers for online and active learning ” , JMLR , 2005 .
[ 2 ] Cauwenberghs , G . and Poggio , T . , “ Incremental and decremental support vector machine learning ” , NIPS , 2001 .
[ 3 ] Chang , C C and Lin , C J , LIBSVM : a library for support vector machines , 2001 .
[ 4 ] Collobert , R . , Sinz , F . , Weston , J . and Bottou , L . , “ Trading convexity for scalability ” , ICML , 2006 .
[ 5 ] Crammer , K . , Dekel , O . , Keshet , J . , Shalev Shwartz , S . and Singer ,
Y . “ Online passive aggressive algorithms ” . JMLR , 2006 . checkers
IDSVM+QueryAll IDSVM+Greedy OnlineSVMR+QueryAll OnlineSVMR+Greedy OnlineASVMR
300
400 #labels dna
500
600
700
IDSVM+QueryAll IDSVM+Greedy OnlineSVMR+QueryAll OnlineSVMR+Greedy OnlineASVMR
100
200 #labels
300
400 waveform c c a
0.84
0.83
0.82
0.81
0.8
0.79
0.78
0.77
0.76
0.75
1
0.95
0.9 c c a
0.85
0.8
0.75
0.7
0.82
0.81
0.8
0.79
0.78
0.77
0.76 c c a adult
IDSVM+QueryAll IDSVM+Greedy OnlineSVMR+QueryAll OnlineSVMR+Greedy OnlineASVMR
0
500
1000 #labels
1500
2000
0.9
0.85 c c a
0.8
0.75
0.7
0 banana
IDSVM+QueryAll IDSVM+Greedy OnlineSVMR+QueryAll OnlineSVMR+Greedy OnlineASVMR
1
0.95
0.9
0.85
0.8 c c a
100
200
300
400
500
0.75
100
200
0.8
0.78
0.76
0.74
0.72
0.7
0.68
0.66
0.64 c c a noisycheckers
IDSVM+QueryAll IDSVM+Greedy OnlineSVMR+QueryAll OnlineSVMR+Greedy OnlineASVMR
500
1000
1500
#labels gauss
IDSVM+QueryAll IDSVM+Greedy OnlineSVMR+QueryAll OnlineSVMR+Greedy OnlineASVMR
#labels cover
IDSVM+QueryAll IDSVM+Greedy OnlineSVMR+QueryAll OnlineSVMR+Greedy OnlineASVMR
0.95
0.9
0.85
0.8 c c a
0.75
500
1000 #labels usps
1500
2000
0.95
0.9 c c a
0.85
0.8
0.75
IDSVM+QueryAll IDSVM+Greedy OnlineSVMR+QueryAll OnlineSVMR+Greedy OnlineASVMR
200
400
600
800 1000 1200 1400
#labels
0.7
0.65
0.6
0.55
0
0.9
0.89
0.88
0.87
0.86 c c a
IDSVM+QueryAll IDSVM+Greedy OnlineSVMR+QueryAll OnlineSVMR+Greedy OnlineASVMR
1000 #labels
1500
2000
0.85
500
0.75
100
200
400
500
600
300 #labels
Figure 6 . Accuracy vs . #queried labels for online active learning algorithms on 9 datasets
[ 6 ] Engel , Y . , Mannor , S . and Meir , R . , “ Sparse online greedy support vector regression ” , ECML , 2002 .
[ 7 ] Fan , R E , Chen , P H , and Lin , C J , “ Working set selection using second order information for training SVM ” , JMLR , 2005 .
[ 8 ] Glasmachers , T . and Igel , C . , “ Second order SMO improves SVM online and active Learning ” , Neural Computation , 2008 .
[ 9 ] Guillory , A . , Chastain , E . and Bilmes , J . , “ Active learning as non convex optimization ” , AISTATS , 2009 .
[ 10 ] Haykin , S . , Neural Networks , Prentice Hall . Inc . , 1999 . [ 11 ] Lee , C . , Kim , H . and Jang , M . , “ Fixed threshold SMO for joint constraint learning algorithm of structural SVM ” , SIGIR , 2008 .
[ 12 ] Li , Y . and Long , P . , “ The relaxed online maximum margin algorithm ” , Machine Learning , 2002 .
[ 13 ] Mangasarian , O . L . and Musicant , D . R . , “ Active support vector machine classification ” , NIPS , 2000 .
[ 14 ] Platt , J . C . , “ Fast training of support vector machines using sequential minimal optimization ” , Advances in Kernel Methods Support Vector Learning , MIT Press , 1998 .
[ 15 ] Steinwart , I . , “ Sparseness of support vector machines ” , JMLR , 2003 . [ 16 ] Vapnik , V . N . , Statistical Learning Theory , John Wiley Sons , Inc . ,
1998 .
[ 17 ] Yuille , A . L . and Rangarajan , A . , “ The concave convex procedure
( CCCP ) ” , NIPS , 2002 .
