Responsible Data Releases
Sanguthevar Rajasekaran1 , Ofer Harel2 , Michael Zuba1 ,
Greg Matthews2 , and Robert Aseltine3
1 Department of CSE , University of Connecticut , Storrs , CT
2 Department of Statistics , University of Connecticut , Storrs , CT
3 University of Connecticut Health Center , Farmington , CT
ABSTRACT Data releases to the public should ensure the privacy of individuals involved in the data . Several privacy mechanisms have been proposed in the literature . One such technique is that of data anonymization . For example , synthetic data sets are generated and released . In this paper we analyze the privacy aspects of synthetic data sets . In particular , we introduce a natural notion of privacy and employ it for synthetic data sets .
1 Introduction
When releasing data about a population to the general public it is essential to ensure privacy of individuals in the population . There are two ways of releasing data : 1 ) Interactive Mode : In this case a user will pose a series of queries about the database ; and 2 ) Noninteractive Mode : In this case a “ sanitized ” version of the database is released . We are interested in the noninteractive mode . It would be desirable ( Dalenius [ 2 ] ) if the sanitized database released is such that everything that can be learnt about an individual from this database can also be learnt without accessing the database . Dwork [ 3 ] shows that this form of security cannot be achieved .
The proof is based on the fact that for any version of the sanitized database one could always find relevant auxiliary information such that the database and the auxiliary information together will yield privacy information about an individual . For example , consider a sanitized database that has information about average annual salaries of different nationalities . This information alone is not enough to infer the salary of an individual . On the other hand if the following auxiliary information is available : “ Joe Smith ’s annual salary is $40K more than the average salary of an American ” , then we can infer the salary of Joe Smith ( if we also have access to the sanitized database ) .
Given that the idea of privacy alluded by Dalenius [ 2 ] is not possible , several other less stringent privacy mechanisms have been proposed in the literature . Examples include k anonymity [ 12 ] , l diversity ( see eg , [ 6 ] and [ 1] ) , differential privacy [ 3 ] , probabilistic differential privacy [ 5 ] , etc .
All of these privacy mechanisms seem to have an underlying ( unstated but implicit ) model for the available auxiliary information . For example , they typically state scenarios like : “ Consider the case when information X is available ”
P . Perner ( Ed. ) : ICDM 2009 , LNAI 5633 , pp . 388–400 , 2009 . c Springer Verlag Berlin Heidelberg 2009
Responsible Data Releases
389
( for some specific X ) . The proposed schemes will be good when the information they enumerate is available . Unfortunately it is impossible to enumerate all possible auxiliary information that may be available . Thus a privacy scheme that is applicable independent of the available auxiliary information is desirable . In this paper we propose one such scheme . The basic property that our scheme supports is the following . Let D be a given data set and let S be the corresponding . be the same as D except that it differs from D synthetic data released . Let D . be the corresponding synthetic data . The statistical in only one record and S . will be “ small ” . In other words , a user having access difference between S and S . will not be able to infer ( with a “ reasonable probability ” ) that to only S and S the underlying data for S and S
. are different .
The rest of the paper is organized as follows . In Section 2 we survey some of the existing privacy approaches . In Section 3 we introduce our privacy scheme and illustrate it with two examples . In Section 4 we summarize our algorithms while in section 5 we present some simulation results . Section 6 concludes the paper .
2 Literature Survey
2.1 Synthetic Data Generation
One of the techniques that is being used to release data is with the generation of synthetic data ( see eg , [ 10 ] , [ 9 ] , and [ 7] ) . If D is the database under concern , we think of this as a sample from a population P . It is assumed that n = |D| is much less than |P| . Since P is usually unknown , one generates M synthetic populations P l , 1 ≤ l ≤ M where each P l is drawn from the posterior predictive distribution for P . The size of the population is too large to be released . Thus one typically releases Dl , 1 ≤ l ≤ M , where Dl is a simple random sample from P l of size k ( for some appropriate integer k ) . For a given D , one way of generating the posterior predictive distribution for the corresponding P is to assume that the data follow a multivariate normal distribution with an unknown mean vector µ and an unknown covariance matrix Σ . Call this data releasing scheme Normal Synthetic Data Generation ( NSDG ) .
2.2 Differential Privacy
Differential privacy [ 3 ] captures the added risk of a disclosure when an individual participates in a database . If the risk of a disclosure does not change much whether or not an individual participates in the database , then we can consider this database as having good privacy . A formal definition of differential privacy follows [ 3 ] : Definition 1 . A randomized function F has privacy if P r[F(D1 ) ∈ S ] ≤ e P r[F(D2 ) ∈ S ] for any two databases D1 and D2 that differ by at most one element and for any subset S of the range of F .
390
S . Rajasekaran et al .
The above concept can be extended to group privacy as well . For example , if there are g persons , then the multiplicative factor in the above inequality will become eg ( instead of e ) . The differential privacy is defined with respect to a query f on the database . In general , f : D → Rd ( for some integer d ) . For any such f , its L1 sensitivity is defined as ∆f = maxD1,D2 ||f(D1 ) − f(D2)||1 . ∆f corresponds to the maximum difference in the query response between D1 and D2 . For a query such as : “ How many people have a salary of > $100K ? ” , ∆f ≤ 1 . The scheme of [ 3 ] works best when ∆f is small . The privacy mechanism Kf of [ 3 ] works as follws : It computes f(D ) on any database D and adds a noise that is exponentially distributed with a variance of σ2 ( for some appropriate σ value ) . In particular ,
P r[Kf ( D ) = r ] ∝ exp(−||f(D ) − r||1/σ ) .
( 1 ) This distribution has independent exponentially distributed coordinates . In other words , an exponential noise is added to each coordinate of f(D ) . Theorem 2 . The above mechanism Kf gives ( ∆f /σ) differential privacy . Proof . Applying Equation 1 ( replacing ∝ with equality ) for D1 we get
The following Theorem and Proof are from [ 3 ] :
P r[Kf ( D1 ) = r ] = exp(−||f(D1 ) − r||1/σ ) .
( 2 )
( 3 )
Applying Equation 1 for D2 we get
P r[Kf ( D2 ) = r ] = exp(−||f(D2 ) − r||1/σ ) . From equations 2 and 3 and the triangular inequality we get
P r[Kf ( D1 ) = r ] ≤ P r[Kf ( D2 ) = r ] × exp(||f(D1 ) − f(D2)||1/σ ) .
( 4 ) Since ||f(D1 ) − f(D2)||1 is bounded by ∆f , the theorem holds when S is a singleton . Using the union bound , the theorem can also be proven for any subset S of the range of Kf . ff If Q is a quantity to be estimated in D , one could estimate Q in each synthetic data set Dl , 1 ≤ l ≤ M and using these estimates approximate Q with a normal distribution .
The data privacy scheme has several shortcomings : 1 ) The privacy measure is specific to each query . Given the large number of possible queries , it may be impractical to analyze the privacy for each such possible query ; 2 ) If the same query ( possibly in disguise ) is asked again and again , it is possible to estimate f(D ) for any database D and any query f within a tight interval ; and so on .
It is desirable to come up with a privacy measure that will be queryindependent . In particular , in this paper we are interested in defining the privacy associated with a given data set . We present such a scheme . The scheme we offer assumes a specific way of releasing data . However the technique is general and could be extended to other data releasing schemes as well . In Section 3.3 we present details on the data releasing technique . The other sections introduce our ideas on data privacy .
2.3 Probabilistic Differential Privacy
Responsible Data Releases
391
In [ 5 ] a variant of the differential privacy scheme called probabilistic differential privacy has been proposed . This technique is explained with the example of commuters data . The origins and destinations are captured as k blocks and there is a histogram for each destination block . The histogram for any block d can be thought of as a vector ( n1 , n2 , . . . , nk ) where ni is the number of persons that commute from bock i to block d ( for 1 ≤ i ≤ k ) . The synthetic data corresponding to block d is obtained as a vector ( m1 , m2 , . . . , mk ) such that k i=1 mi = O( The synthetic data is obtained as follows : Some noise ( for example Laplacian ) αi is added to ni , for 1 ≤ i ≤ k . The synthetic data is nothing but a sample ( of size m = O( i=1 ni ) ) from a multinomial distribution with a Dirichlet prior whose parameters are ( n1 + α1 , n2 + α2 , . . . , nk + αk ) . Let α = ( α1 , α2 , . . . , αk ) . To calculate the differential privacy one can obtain using this scheme , we have to compute the maximum value of i=1 ni ) . k k
. 1 , n
. 1 , n
. 2 , . . . , n
P r((m1 , m2 , . . . , mk ) | ( n1 , n2 , . . . , nk ) , α ) P r((m1 , m2 , . . . , mk ) | ( n . k ) , α ) . . . k ) is the histogram corresponding to D Here ( n 2 , . . . , n differ in one point .
. such that D and D . differ in block d , the authors argue that , the worst case value of the above ratio happens when all the m points in the synthetic data fall into block d . The probability of this happening is very low . In this case the synthetic data is highly unrepresentative of the original data . The idea of probabilistic differential privacy is to compute the value of using a probable value of the above ratio ( rather than the worst case maximum value ) .
If D and D
.
As one could see , the probabilistic differential privacy is closely related to differential privacy . In particular , it is dependent on the query under concern .
3 A Novel Scheme for Data Privacy
3.1 Some Observations
Before presenting details on our approach , we enumerate some motivating observations . 1 . We can think of the database D as a sample from the population P . Equivalently , if |D| = n , we can think of D as a collection of n draws from some appropriate probability distribution U ( that models the population P ) .
2 . The released synthetic data is fully characterized by the predictive posterior distribution parameters ( µ and Σ in the normal distribution case ) . In general , whatever synthetic data scheme one may employ , the released data is completely characterized by the parameters of an appropriate probability distribution U . ( that models the data set D ) .
392
S . Rajasekaran et al .
3 . In fact , the synthetic data sets Dl , 1 ≤ l ≤ M released in the synthetic data scheme of [ 10 ] , [ 9 ] , and [ 7 ] are completely characterized by the multivariate normal distribution ( used to model D ) . Note that the posterior distribution U . used to model D is indeed an estimate of U . 4 . What the user learns from Dl , 1 ≤ l ≤ M is at best the multivariate normal distribution that models D . In general the user learns , at best , the parameters of the probability distribution U . that models D . 5 . In summary , the user knows U . and does not know D . Can the user learn any point ( ie , row ) of D using U . alone ? For example , consider a binomial distribution with parameters m and p . Also assume that D is a collection of n draws from this distribution . Can one identify one of the points in D knowing m and p alone ? This may not be possible . What can the user learn from U . ? Let D be a given data set . Let D . be another data set that is a subset of D . One or more points of D are missing in D Let S and S . be the synthetic data sets corresponding to D and D . , respectively . If a user can infer ( with some “ reasonable ” confidence ) that . correspond to two different data sets analyzing only S and S . , S and S then we have leaked some information . It is very much possible for the user ) to infer this using some auxiliary information ( in addition to S and S However , we don’t have any control over the auxiliary information that any user may have access to . In this case , we can call a data release scheme as a . , that S responsible data release if the user cannot infer , using only S and S . correspond to two different data sets . In other words our data release and S . will not scheme should be such that a user who has access only to S and S We be able to detect any significant statistical difference between S and S refer to this property as responsible data release property .
6 . The security of any data release scheme will depend on the type of data that we have as well as the data release technique used . The dependence on data type is very clear . For example , we may have a data set where only one point is ( or a small number of data points are ) far away from the rest of the points . Consider a community C where only one person is of height 7.5 feet and the others are of height less than or equal to 6 feet . Let X be a set of people from C that have a particular disease . Also assume that the person of height 7.5 feet is a member of X . If we employ the NSDG scheme of Section 2.1 ( or any other scheme ) , X should be released only if it has the responsible data release property . In general , for any given dataset and any given data release scheme , the data should be released only if this property holds , as per our new ideas .
3.2 Our New Ideas
The above observations suggest the following technique for releasing data . Cluster the data points ( for example using any hierarchical clustering algorithm ) . The number of clusters could be decided empirically to the one that yields the best security . If the number of clusters is chosen to be very large , then the size of
Responsible Data Releases
393
. 1 , C each cluster will be very small . This is not desirable . If the number of clusters is too small , it will fail to identify the vulnerable points . Let C1 , C2 , . . . , Ck be the resultant clusters . We will not release data corresponding to those clusters that have a ’s mall’ number of points in them since they run the risk of being breached . q , ( q ≤ k ) . What is ’s mall’ is again better decided empirically . Let C be the surviving clusters . For each surviving cluster we will release synthetic data ( using a scheme such as the NSDG protocol ) . For each data released we also compute a risk associated with the release . The risk is a quantification on how much the synthetic data deviates from the responsible data release property . Let C be one of the surviving clusters and assume that we employ the NSDG scheme . Let ¯µ be the mean vector of the posterior distribution and let Σ be the covariance matrix . We are interested in quantifying the risk involved in releasing the data for C using this scheme . In particular , consider the example of quantifying the risk of revealing any one of the records of the database . Note that we will generate synthetic data sets D1 , D2 , . . . , Dn each one of these being a random sample from the posterior distribution .
. 2 , . . . , C
The risk factor for C will depend on how ’homogeneous’ the members of C are . If a member is very unique perhaps this member could be an outlier and hence runs the risk of being detected using the synthetic data sets . Let x be any member . = C −{x} . Our question of interest is : If we were to release data of C and let C . be the mean vector and covariance matrix , respectively . for C Given the synthetic data sets Let D . D1 , D2 , . . . , Dn ; and D n , what is the probability of detecting that these two data sets correspond to two different posterior distributions ? We have to compute the maximum of this probability over all members x of C .
. let ¯µ . and Σ . . n be the synthetic data for C 1 , D
. 2 , . . . , D
. 2 , . . . , D
. 1 , D
Clearly , this probability will depend on the ’distance’ between the two distributions . There are many ways of defining the distance between these two sample data sets . For example , we can estimate the parameters ( mean and covariance ) from the synthetic data sets and compute the distance as one norm between them .
3.3 Synthetic Data Sets Methodology
Using synthetic data sets the analyst would want to make inferences about some population quantity of interest , Q ( eg regression coefficients ) . Q can be estimated by a point estimate q with an associated measure of uncertainty v . The intruder is not interested in the population quantity Q , but in the specific sample quantity q . For each synthetic data set ( 1 , 2 , , M ) , q and v can be estimated , yielding M estimates ( q(l ) , v(l) ) , l = ( 1 , 2 , , M ) . We have to use a slight variant of the scheme described in Raghunathan , et al . [ 9 ] . In particular , we approximate the population parameter , Q|DSyn , by a normal distribution with mean l=l q(l)/M and variance TM = bM + vM where vM = qM = l=1 v(l)/M and l=1(q(l ) − qM )2/(M − 1 ) . The exact variance can be obtained using bM = numerical procedures and evaluating the integrals described in Raghunathan , et al . [ 9 ] .
M
M
M
To solidify the above ideas we provide two examples next .
394
S . Rajasekaran et al .
3.4 An Example
. and σ
. = C − {x} . Also , let µ
Consider the example where each data record has only one ( real valued ) attribute and where we use the NSDG protocol . Let µ and σ be the mean and variance of the posterior distribution for some cluster C . Let x be any member of C and . be the mean and variance of the posterior let C If m synthetic data sets are released for C with n records distribution for C in each , we can use the approximation in section 3.3 to approximate a normal . also we distribution with mean µ = qM and variance σ = TM . Likewise , for C . of size n each . Let the sample means and variances have m synthetic data sets S . s ) , respectively . To simplify the discussion for S and S assume that the pooled variance of C and C . n1+n2−2 . are different ? . are nearly Clearly this will depend on how different µ and µ the same , then we may not be able to detect a statistical difference between the two samples . Any statistical statement is always made with respect to some confidence level . Let P be the confidence level of interest ( P being close to 1 ) . For a given confidence level P and a sample S , we can only estimate µ within an interval .
. what is the probability of inferring that µ and µ
. is equal σp = ( n1−1)TM +(n2−1)T
. . be ( µs , µ s ) and ( σs , σ
. are . If µ and µ
Given S and S
M
Appealing to the central limit theorem ( see eg , [ 11] ) , we can get confidence . intervals for µs and µ s . For example , for large values of mn , we can assume that the distribution of µs is normal with mean µ and variance σp . As a result , it follows that µs−µ can be thought of as the standard normal variate . s−µ is the standard normal variate . Similarly , µ
σp
σp
For the standard normal variate Z ( with mean 0 and variance 1 ) , it is well
( 5 )
( 6 )
( 7 ) known that :
P r[Z ≥ z ] ≤ 1√ 2πz Applying Equation 5 for C , we realize that .
'
1√ 2π(1 − P ) with probability ≥ P . On a similar token we can infer :
µs ∈ µ ± σp
2 ln
. − z2 2 fi
. exp fi
'
. µ s
∈ µ
. ± σp
2 ln
. fi
1√ 2π(1 − P ) with probability ≥ P . ff
2 ln
If A = σp
, then µs is in the interval [ µ − A , µ + A ] and µ . 1√ 2π(1−P ) s . + A ] , both with probability ≥ P . Assume without . − A , µ is in the interval [ µ . ≥ µ ( the other case is similar ) . If µ . loss of generality that µ s falls in the interval > µ ( under the [ µ+ A , µ confidence level P ) . The probability of µ . s falling in this interval can be taken as the risk of releasing C under the confidence level P .
. + A ] then there is a high probability of inferring that µ .
Responsible Data Releases
395 . + A ] is nearly the . − µ) ) ] = . ) ≥ ( A − ( µ is this probability
. Upon simplification , this probability
2
. RHS is nearly equal to
−µ)−(µ
−µ )
2A(µ
2σ2 p
. The probability of µ s falling in the interval [ µ + A , µ − µ ( ) −µ ) . σp . − 1
≥ µ + A ] . P r[µ . s Using
A−(µ −µ ) σp
≥ µ + A ] = P r[(µ . s 5 ,
Equation 2
≥ A−(µ exp
√ fi
σp
1
2
σp s−µ µ
. same as P r[µ s P r ≤ A−(µ −µ ) is ≤ ( 1 − P )
( 1 − P ) σp
A exp
2π
A−(µ −µ ) exp
σp −µ ) σ2 p
.
A(µ
As a result , the following definition of risk can be made .
Definition 3 . In the above example , ( 1 − P ) σ1 the risk of under the confidence level P .
A(µ
A exp
−µ ) σ2 p releasing data C is
3.5 Example of Histograms
Machanavajjhala , et al . [ 5 ] have considered the example of commuting data . Space is divided into grids and each grid block serves both as an origin and a destination . Let the number of grid blocks be k . For every destination d , we have an associated array Ad[1 : k ] , where Ad[i ] is the number of commuters whose origin is i and whose destination is d ( for 1 ≤ i ≤ k ) .
Synthetic data for this example of histograms can be released in a number of ways . In [ 5 ] , noise is added to the data before releasing . They also introduce the idea of probabilistic differential privacy . n1
In this section we consider a different scheme for data release . Like in [ 5 ] we consider the individual destinations separately . From hereon we assume a specific destination without referring to it by name . Let ( n1 , n2 , . . . , nk ) be the histogram corresponding to the destination under concern and let n = i=1 ni . The data released is a collection of m samples ( for some appropriate value of m ) ff . drawn from the multinomial distribution with parameters Our idea of responsible data release suggests that the released data corresponding to D and D . be statistically indistinguishable ( under some confidence level ) where D is the original dataset and D . is a subset of D . As in [ 5 ] consider the example where D . differs from D in exactly one point . Consider a block g in D with i members . Let D . differ from D only in block g . D . has one less member in g than in D . The multinomial distribution for D . n for block g and the multinomial distribution of D will have a probability of i will have the probability of i−1 n for block g . Assume that we release m samples from the multinomial distributions corresponding to the data sets . Let X be the total number of members that fall in block g in the m samples for D . Let Y be the total number of members that fall into g in the m samples for D n , . . . , nk n , n2 n k
Using Chernoff bounds , X ∈ ( 1 ± )m i n with probability ≥ 1 − exp n with probability ≥ 1 − exp m(i−1 ) . 3n
−
2
Likewise , Y ∈ ( 1 ± )m ( i−1 )
−
2 mi 3n
.
396
S . Rajasekaran et al .
Let P be a confidence level of interest . With probability ≥ P , X ∈ ( 1 ± )m i . Similarly , with probability ≥ P , Y ∈ ( 1 ± 1)m ( i−1 ) ff where = n n where 1 = ff
1−P )
3 ln( 1 mi/n 1−P ) 3 ln( 1 m(i−1)/n .
( n
) n i
3n
− fi n , ( 1 + )m i
( ( 1 + 1)m ( i−1 )
If the value of X is in the interval
X ≥ ( 1 + 1)m ( i−1 ) fl2 mi − 1
) , then we may be able to infer that D and D . differ in block g . The probability of this happening . This probability is no more than is no more than P r ( 1 + 1 ) i−1 exp . This probability can be thought of as the risk under confidence level P . If i is large , the above probability is nearly equal to ( 1 − P)i/(i−1 ) . Observation 31 If one desires to keep the value of i large , one could introduce a constant bias to all the blocks . This is in contrast to stochastic noises introduced in the other schemes found in the literature . Observation 32 In the two examples we have seen , D and D . differ in only one point . However , the paradigm and analyses can be extended to general cases where D . is any subset of D .
Observation 33 We have seen only examples where we don’t introduce any noise . However the paradigm is general and can be used also in cases where noises are introduced .
Observation 34 The paradigm we have introduced can be used to compute the risk associated with any data release ( synthetic or otherwise ) . For example , a simple data release scheme is to eliminate some vital attributes ( such as names , social security numbers , etc . ) and release the data with the other attributes . In this case also we can associate a risk with this scheme as follows . Let k be the number of attributes remaining in the released data . One way of associating a risk with this data release is as follows . We can think of the database D released as a sample from a population P . If D has n records , we can think of D as a collection of n draws from some appropriate probability distribution U ( that models the population P ) . One could estimate the parameters of U from D . If D . is any subset of D , one could also estimate the parameters of the distribution corresponding to D Then one could compute the probability that these two parameters correspond to two different data sets . This probability can be used as the risk ( similar to what we computed in Section 34 )
4 Algorithms for Computing Risks of Data Sets
In this section we present psuedocodes for computing risk factors involved in releasing data sets . These psuedocodes basically encode the ideas that have been presented in the previous sections .
Responsible Data Releases
397
4.1 The Example of Section 3.3 Here we consider the example of each data record having only one attribute and the protocol used being NSDG . Consider a data set C that has N records . We assume that there is an algorithm called PD Parameters that takes as input any data set and outputs the parameters of the posterior distribution corresponding to the data set . For the example of Section 3.3 , there are two parameters of interest , namely , the mean µ and standard deviation σ .
Algorithm ComputeRisk1
Let C = {x1 , x2 , . . . , xN} ; Let P be the confidence level ; The synthetic data is of size mn ; Invoke PD Parameters on C and get µ and σ ; ff Risk := 0 ; σp = ( n1−1)TM +(n2−1)T for i := 1 to N do
; A = σp n1+n2−2
2 ln
M
1√ 2π(1−P )
;
. := C − {xi} ;
Let C Invoke PD Parameters on C Risk := max{Risk , ( 1 − P ) σp
. ; . and σ . to get µ } ;
−µ ) A exp σ2 p
A(µ
Output Risk ;
4.2 The General Case In this section we consider a general scenario where for a given data set C , a parameter set P = {p1 , p2 , . . . , pq} is generated , and P is then used to generate and release synthetic data sets . NSDG is an example of this paradigm . In this special case the parameters will be the mean vector and the covariance matrix . If there are m attributes , then the mean vector will be of size m and the covariance matrix will be of size m × m . Here again we assume that there is an algorithm called Get Parameters that takes as input any data set and returns the corresponding parameter set .
Given two parameter sets P1 and P2 , let d(P1 , P2 ) be the distance between them . There are many possible ways of defining this distance . For example , this distance can be the Euclidean distance between these two vectors .
Under the above settings we can make use of the following algorithm .
Algorithm ComputeRisk2
Let C = {x1 , x2 , . . . , xN} ; Invoke Get Parameters on C and get its parameter set P ; Risk := 0 ; for i := 1 to N do
. := C − {xi} ;
Let C . to get its parameter set P Invoke Get Parameters on C .)} ; Risk := max{Risk , d(P , P
. ; P ercentageRisk := 100 ∗ Risk/||P|| ; Output P ercentageRisk ;
398
S . Rajasekaran et al .
5 Experimental Results
We have tested our algorithm ComputeRisk2 with synthetic data sets . In particular , we have used several data sets using the following parameters as variables : number of observations ( or points ) , the number of attributes , mean vector , and . covaraince matrix . For each data set C we also generated another data set C . differ in only one point . In particular , we perturbed the such that C and C If v is the value of one of the attributes in one of the points of C to get C value of C under concern , it is perturbed to different values ( such as v + 5σ , We compute the v + 10σ , etc ) For each such perturbed value we get a new C . to see how different they are ( as a function of the underlying risks of C and C parameters ) .
We have employed the R [ 8 ] function “ mvrnorm ” , which produces one or more samples from a specified multivariate normal distribution . Please note that the length of µ is the number of attributes and Σ is of size p × p where p is the number of attributes . The diagonal elements of Σ will be the variances of each mean and the off diagonals are the covariances between a pair of variables .
For each data set we computed the risk using ComputeRisk2 . Followed by this , we introduced an outlier in the same data set by perturbing one of the attributes by various amounts and calculated the risk again .
The results obtained are shown in the following tables .
Table 1 . Risk factors ( as percentages ) for different data sets ; Number of points =100
Mean σ Attributes R W/O OL R W OL R W OL R W OL ( µ + 5σ ) ( µ + 10σ ) ( µ + 15σ )
µ1 µ2 µ3 µ4 µ1 µ2 µ3 µ4 µ1 µ2 µ3 µ4 µ1 µ2 µ3 µ4
0.5 0.5 0.5 0.5 0.4 0.4 0.4 0.4 0.25 0.25 0.25 0.25 0.1 0.1 0.1 0.1
2 5 10 20 2 5 10 20 2 5 10 20 2 5 10 20
8.81 3.26 3.30 3.04 16.23 3.42 4.88 2.59 22.31 4.52 4.93 3.65 27.12 6.00 5.09 4.33
14.48 7.19 4.22 5.25 17.10 4.54 4.95 4.57 22.27 5.38 6.91 5.49 40.89 3.96 7.05 4.41
18.09 9.89 17.23 8.68 17.36 7.22 10.85 5.89 27.55 6.16 7.86 5.57 49.48 5.26 8.30 4.42
35.12 14.32 16.87 16.66 33.77 12.31 8.59 9.94 30.79 6.19 7.17 6.48 66.87 5.81 5.48 5.68
Table 2 . Risk factors for different data sets ; Number of points =1000
Responsible Data Releases
399
Mean σ Attributes R W/O OL R W OL R W OL R W OL ( µ + 5σ ) ( µ + 10σ ) ( µ + 15σ ) 1.71 % 0.50 % 0.67 % 0.65 % 2.48 % 0.60 % 0.72 % 0.81 % 2.65 % 0.64 % 0.54 % 0.54 % 8.58 % 0.93 % 0.76 % 0.78 %
8.18 % 1.22 % 2.64 % 1.88 % 3.45 % 1.76 % 0.83 % 1.72 % 3.43 % 0.78 % 0.76 % 0.91 % 13.07 % 1.02 % 0.78 % 0.81 %
1.94 % 0.50 % 1.24 % 1.40 % 3.03 % 0.75 % 0.68 % 1.00 % 2.70 % 0.62 % 0.73 % 0.89 % 8.65 % 0.95 % 0.75 % 0.64 %
1.05 % 0.46 % 0.53 % 0.45 % 1.34 % 0.57 % 0.61 % 0.42 % 2.65 % 0.67 % 0.70 % 0.53 % 4.35 % 0.80 % 1.16 % 0.81 %
µ1 µ2 µ3 µ4 µ1 µ2 µ3 µ4 µ1 µ2 µ3 µ4 µ1 µ2 µ3 µ4
0.5 0.5 0.5 0.5 0.4 0.4 0.4 0.4 0.25 0.25 0.25 0.25 0.1 0.1 0.1 0.1
2 5 10 20 2 5 10 20 2 5 10 20 2 5 10 20
In Tables 1 and 2 , µ1 = ( 0 , 0 ) ; µ2 = ( 2,−2.5,−5 , 10 ) ; µ3 = ( 1,−1 , 6,−6 , 8 , 11,−14 , 17 , 20,−24 ) ; and µ4 = ( 2,−2 , 5,−6 , 10 , 14,−12 , 18,−22 , 25 , 27,−30 , 33 , 36,−38 , 40 , 43,−45 , 47,−47 ) . The number of points in the data sets is 100 . R W/O OL stands for “ Risk without outliers ” and R W OL stands for “ Risk with an outlier ” .
As we can see from Table 1 , in general , the risk factor increases significantly when an outlier is present . The exeption is when the value of σ is very small . Even when σ is small , the risk factor increases when the number of attributes is small . When σ is small and the number of attributes is large , even a large deviation in one attribute in one point is not sufficient to leak information . Also , in general , when the deviation is large the risk factor is also large .
This is what one would expect out of any risk computation . In Table 2 also we see results similar to those in Table 1 . The risk factors are in general smaller in Table 2 . This is because the number of points is ten times more . As a result , the influence of one attribute in one point on the entire dataset tends to be smaller . However , as the deviation increases , we see a proportionate increase in the risk factor .
6 Conclusions
In this paper we have introduced a measure of data privacy that is only dependent on the data given ( and is independent of the query ) . This measure overcomes the shortcomings present in the differential privacy measures of [ 3 ] and [ 5 ] . We have illustrated the new paradigm with two examples .
400
S . Rajasekaran et al .
Acknowledgement
This research has been supported in part by an NSF Grant ITR 0326155 .
References
1 . Chen , B . , Lefevre , K . , Ramakrishnan , R . : Privacy skyline : Privacy with multidi mensional adversarial knowledge . Journal of Very Large Databases ( 2007 )
2 . Dalenius , T . : Towards a methodology for statistical disclosure control . Statistisk
Tidskrift 5 , 429–444 ( 1977 )
3 . Dwork , C . : Differential Privacy ( 2008 ) 4 . Horowitz , E . , Sahni , S . , Rajasekaran , S . : Computer Algorithms . Silicon Press
( 2008 )
5 . Machanavajjhala , A . , Kifer , D . , Abowd , J . , Gehrke , J . , Vilhuber , L . : Privacy : Theory meets Practice on the Map . In : Proc . 24th IEEE international Conference on Data Engineering , pp . 277–286 ( 2008 )
6 . Machanavajjhala , A . , Kifer , D . , Gehrke , J . , Venkitasubramaniam , M . : l diversity : Privacy beyond k anonymity . ACM Transactions on Knowledge Discovery from Data 1(1 ) ( 2007 )
7 . Matthews , GJ , Harel , O . , Aseltine , RH : Examining the Robustness of Fully Synthetic Data Techniques for Data with Binary Variables , Technical Report , UConn ( June 2008 )
8 . R Development Core Team . R : A Language and Environment for Statistical Computing . R Foundation for Statistical Computing , Vienna , Austria ( 2007 ) ISBN 3 900051 07 0
9 . Raghunathan , TE , Reiter , J . , Rubin , D . : Multiple imputation for statistical dis closure limitation . Journal of Official.Statistics 19 , 1–16 ( 2003 )
10 . Rubin , DB : Discussion statistical disclosure limitation . Journal of Official Statis tics 9(2 ) ( 1993 )
11 . Snedecor , GW , Cochran , WG : Statistical Methods , 7th edn . The Iowa State
University Press ( 1980 )
12 . Sweeney , L . : k anonymity : a model for protecting privacy . International Journal on
Uncertainty , Fuzziness and Knowledge based Systems 10(5 ) , 557–570 ( 2002 )
Appendix A : Chernoff Bounds
If a random vraiable X is the sum of n iid Bernoulli trials with a success probability of p in each trial , the following equations give us concentration bounds of deviation of X from the expected value of np ( see eg , [ 4] ) . The first equation is more useful for large deviations whereas the other two are useful for small deviations from a large expected value .
P rob(X ≥ m ) ≤ np m m em−np
P rob(X ≤ ( 1 − )pn ) ≤ exp(− 2np/2 ) P rob(X ≥ ( 1 + )np ) ≤ exp(− 2np/3 ) for all 0 < < 1 .
( 8 )
( 9 ) ( 10 )
