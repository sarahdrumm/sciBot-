2009 Ninth IEEE International Conference on Data Mining
Extending Semi supervised Learning Methods for Inductive Transfer Learning
Yuan Shi
School of Software , Sun Yat sen University
Guangzhou , China sycaszsu@gmail.com
Wei Liu
Department of Computer Science , Nanjing University
Nanjing , China lwbiosoft@gmail.com transfer
Abstract—Inductive learning and semi supervised learning are two different branches of machine learning . The former tries to reuse knowledge in labeled out of domain instances while the later attempts to exploit the usefulness of unlabeled in domain instances . In this paper , we bridge the two branches by pointing out that many semi supervised learning methods can be extended for inductive transfer learning , if the step of labeling an unlabeled instance is replaced by re weighting a diff distribution instance . Based on this recognition , we develop a new transfer learning method , namely COITL , by extending the co training method in semi supervised learning . Experimental results reveal that COITL can achieve significantly higher generalization and robustness , compared with two stateof the art methods in inductive transfer learning .
Keywords Inductive learning ; co training transfer learning ; semi supervised
I .
INTRODUCTION
In traditional data mining and machine learning research , it is often assumed that there are sufficient training data . For example , in supervised classification task , the labeled training instances are often assumed to be sufficient to train a good learner that can correctly classify unseen instances . In many practical applications , however , labeled instances are insufficient , because labeling training instances is often expensive , difficult , or time consuming . Meanwhile , unlabeled instances may be relatively easy to obtain , thus can be used to compensate the lack of labeled instances , which motivates a recent machine learning branch semisupervised learning [ 1 ] . Due to its importance in practice and received considerable attention . A number of learning methods , including self training , co training , graph based methods , etc . , have been proposed to effectively learn from both labeled and unlabeled data . semi supervised learning has theory ,
Many machine learning methods , no matter for traditional machine learning or semi supervised learning problems , are under a common assumption : the training and test data are drawn from the same distribution and the same
Zhenzhong Lan
School of Software , Sun Yat sen University
Guangzhou , China lzzsysu@gmail.com
Wei Bi
Department of Computer Science , Sun Yat sen
University , Guangzhou , China biwei@mail2sysueducn feature space . So , when the distribution changes , most learning methods fail to work well . Therefore , human effort is required to re collect a large amount of labeled data to retrain a learner to fit the new distribution . Since the labeling process is often expensive , reducing human effort to label new data attracts increasing attention . This motivates another machine learning branch , namely transfer learning [ 2 ] , which aims to transfer the knowledge in a source domain to solve the learning task in a related target domain , ie the domain we are interested in . The source and target domain have different data distributions or even different feature spaces . For example , in web page classification , we can use transfer learning to adapt a classifier for university pages to the classification task on Facebook.com [ 2 ] . This is because the data distribution between university web and social web pages are quite different , there still exists some common classification knowledge that can be reused to reduce the labeling effort . Even if we still want to classify the university web pages , transfer learning may also help , because the contents on web pages are changing over time , and the training data can be easily outdated ( ie under a different distribution to the current data ) . that although
Transfer learning has been studied for more than one decade , and various approaches have been developed . Most of these approaches can be summarized into two categories , namely instance transfer and feature representation transfer . In instance transfer , training instances in the source domain are re weighted according to their impact on the learning in the target domain [ 3 ] . While feature representation transfer tries to learn a common feature representation across domains to reduce the domain divergence as well as the training error , making knowledge in different domains easy to transfer [ 4 ] .
Besides the learning strategies , different problem settings of transfer learning have also been studied . There are mainly three settings of transfer learning , namely inductive transfer learning , transductive transfer learning and unsupervised transfer learning [ 2 ] . In this paper , we focus on inductive
1550 4786/09 $26.00 © 2009 IEEE DOI 101109/ICDM200975
483 target domain data ( referred transfer learning , in which only a few labeled data in the target domain are available . Besides , we assume that the source domain data are also labeled , just the same as several other works like [ 3 ] , [ 5 ] . In a word , our problem setting assumes that a large amount of source domain data ( referred to as diff distribution data in the following ) and a small amount of to as samedistribution data ) are available . Both the diff distribution and same distribution training data are labeled and under the same feature space . However , we should note that although a few same distribution data are provided , they are still insufficient to train a good learner for the target domain . One intuition to solve the problem is to use the samedistribution data to find out some useful diff distribution instances and reuse them to improve the learning in the target domain , which exactly follows the instance transfer strategy . In fact , several instance based approaches for inductive transfer learning have already been proposed , such as auxiliary data based method [ 5 ] and boosting method for transfer learning [ 3 ] , but their generalization ability and robustness still need to be improved . transfer
It is noteworthy that although inductive transfer learning and semi supervised learning address different problems , they share similarities on their methodologies , thus many approaches for semi supervised learning can be extended to inductive learning setting . To develop better instance based methods for inductive transfer learning , this paper attempts to extend methods in semi supervised learning . The basic idea of the extension is to replace the step of labeling an unlabeled instance by re weighting a diff distribution instance . Specifically , this paper extends the co training method . As an often used semi supervised learning method , co training trains two learners separately on two different views , and uses the predictions of each learner on unlabeled instances to augment the training set of the other learner . Such training style has the advantages of high generalization ability and strong robustness , which has been reported by several research works ( for example , refer to [ 6] ) . In our CO training method for Inductive Transfer Learning ( COITL ) , two base learners also learn in a collaborated way , however , the information that one learner gives to the other is the weight of a diff distribution instance instead of the predicted label of an unlabeled instance . The co training method uses the weighted k Nearest Neighbor ( k NN ) method [ 7 ] as the base learner , and implement two learners by setting different values for the parameter k . Moreover , each k NN learner weights a diff distribution instance according to its influence on the training error . An efficient implementation of COITL is also presented . Experimenting on eight data sets , we show that the proposed method outperforms state of the art approaches in inductive transfer learning , in terms of generalization error and robustness to noise .
To summarize , this paper has mainly two contributions . First , it bridges inductive transfer learning and semisupervised learning , and shows that many semi supervised learning methods can be extended for inductive transfer learning . Second , it develops a new transfer learning method by extending the co training method in semi supervised learning . The rest of this paper is organized as follows . Section II presents the related work . Section III gives the problem statement . Section IV discusses the relationship of the two learning branches . Section V gives the details of COITL . Section VI reports the experimental results . Finally , Section VII concludes .
II . RELATED WORK
A . Instance transfer
Transferring knowledge from instances is intuitively appealing . Although directly reusing the diff distribution data is unfeasible , there are certain parts of the data that can still be reused to benefit the learning in the target domain [ 2 ] . Here we briefly review a few research works with the similar problem setting as ours .
Wu and Dietterich [ 5 ] presented a way of integrating auxiliary training data into k NN as well as support vector machine methods . The methodology is to minimize a weighted sum of two loss functions , one for original training data ( ie same distribution data ) and the other for auxiliary data ( ie diff distribution data ) . Their experiments demonstrated that using auxiliary data can improve the classification performance when the original training data are inadequate . Note that in their work , all diff distribution training data are given the same weight , yet our method sets the weight for each diff distribution instance separately . instance
Liao et al . [ 8 ] introduced an auxiliary variable for each the distribution diff distribution mismatch . Those auxiliary variables are estimated as a byproduct , along with the classifier . They also incorporated a new active learning method to select unlabeled samedistribution instances to be labeled with the help of diffdistribution data . In our method , however , we perform transfer learning only on labeled data .
Dai et al . [ 3 ] proposed a boosting algorithm , namely TrAdaBoost , as an extension of the AdaBoost algorithm . The idea is to use a small number of same distribution instances to find out useful diff distribution instances by iteratively adjusting their weights . In each iteration , a base learner is trained on the weighted training data and used to predict the label of each training instance . Moreover , TrAdaBoost uses the same strategy as AdaBoost to update the weights of incorrectly classified same distribution instances while also adopts a different strategy from AdaBoost to update the weights of misclassified diffdistribution instances . Experimental results verified the high generalization of TrAdaBoost . B . Co training to reflect
The co training method , first proposed by Blum and Mitchell [ 9 ] , presents a novel learning style : two learners are separately trained on two sufficient and redundant
484 the class label , each attribute set views , and the predictions of each learner on unlabeled instances are used to augment the training set of the other one . Here , the two sufficient and redundant views are two attribute sets which satisfy the following two requirements : first , each attribute set is sufficient to train a good learner ; second , given is conditionally independent to the other . Later , Dasgupta et al . [ 10 ] theoretically showed that when there exist two sufficient and redundant views , the co trained learners can reduce their agreement over the unlabeled instances . Although cotraining has been successfully utilized in several domains , such as [ 11 ] and noun phrase identification [ 12 ] , in most application scenarios , the requirement of sufficient and redundant views , could not be met . Therefore , some variants of co training which relax the requirement of sufficient redundancy were proposed . the generalization error by maximizing statistical parsing
Goldman and Zhou [ 13 ] developed a relaxing version of the co training method . Their method employs two different supervised learning algorithms to divide the instance space into a set of equivalence classes , and uses cross validation to help label the unlabeled instances and generate the final hypothesis . Their experimental results demonstrated that although there are no sufficient and redundant views , such co training version can still achieve excellent performance . Recently , Wang and Zhou [ 14 ] theoretically proved that , cotraining can be effective if the learners are diverse , which implies that the sufficient and redundant views are actually used to achieve the diversity of the learners , and they are not necessary if the diversity can be achieved from other ways . For example , in [ 7 ] , the diversity is achieved by setting different parameters for the k NN method , so that even with the same view , ie the same attribute set , co training still outperforms many other methods .
In this paper , we adopt the same co training style in [ 7 ] , that is , our method does not use two views , and achieves the diversity of learners by setting different values for the parameter k in k NN . Clearly , such implementation is more general than the classical version of co training .
III . PROBLEM STATEMENT
Y =
{0,1}
In our problem setting , the data in the source domain and target domain have the same feature space X and label set Y ( ) , but they are under different distributions . The training set T consists of two sets of instances : training sT ) and training data data in the target domain ( denoted by dT ) . All instances in in the source domain ( denoted by dT are labeled . As mentioned above , in this paper , we sT and dT to the would refer diff distribution data set . Moreover , we denote the test set as E , which contains a set of unlabeled instances . sT to the same distribution data set and
Both the same distribution data set sT and the test set E are from the target domain , thus have the same dT and E are from different domains , distribution . While sT thus their distributions are different . Note that the size of is supposed to be relatively small , so that merely training on sT is unable to produce a high performance learner . dT is relatively large , since we assume However , the size of that it is easy to obtain large amounts of source domain dT is shown in data . An illustrative example of Figure 1 . sT and
Figure 1 . An example of the same distribution data set and diff distribution data set
∈ iy s ix s where instance , of
∈ iy d ix d where instance , of
More formally , we define The training set in the target domain : ),,( sT , dT and E as follows .
= = x y , ),( {( s s 1 1 n 1,2 , , ) x y T , s s s 2 2 X i ( is the feature vector of the i th Y∈ is its corresponding label , and n is the size x y , s s n n
)} sT . The training set in the source domain : x ),,( d m y d 2 x d 2
, x d 1
= y T ),( , {( d d 1 = m X i ( ) 1,2 , , is the feature vector of the i th Y∈ is its corresponding label , and m is the size y d m
)}
, dT . The test set ( in the target domain ) :
∈ ix e where vector , and r is the size of the test set . r 1,2 , , )
X i (
=
=
, is the i th instance ’s feature x x { , e e 1 2
, , x e r
E
} dT to learn a function
The objective of inductive transfer learning is to use Y→ , such that f can sT and correctly predict the label of instances in the test set E . IV . BRIDGING INDUCTIVE TRANSFER LEARNING AND
:f X
SEMI SUPERVISED LEARNING
We know that inductive transfer learning and semisupervised learning are two different machine learning branches for solving different learning problems . They are motivated by the observation that in many application areas , labeling new data is expensive , as it requires the effort of experienced human annotators . Hence both branches attempt to explore a large number of auxiliary data to reduce the labeling effort . Their differences are :
485 a . Inductive transfer learning focuses on learning from auxiliary labeled data , while semi supervised learning focuses on learning from auxiliary unlabeled data . b . In inductive transfer learning , the auxiliary and original labeled training data are under different distributions or even different feature spaces , while in semi supervised learning , the auxiliary unlabeled data and original labeled data are drawn from the same distribution .
Considering the above differences , we can see that , although in inductive transfer learning , labels of auxiliary data are available , they cannot be used directly due to the distribution divergence . Therefore , re weighting training data is often adopted to reduce the distribution divergence . While in semi supervised learning , although auxiliary data are unlabeled , their labels can be predicted as they are under the same distribution with the labeled data .
Roughly speaking , in both branches , the auxiliary data can be explored in two levels – instance level and structurelevel . In the instance level , a learning method attempts to select some useful auxiliary instances and add them to the original training set . Typical methods are self training [ 15 ] and co training [ 9 ] [ 13 ] in semi supervised learning , as well as TrAdaBoost [ 3 ] in transfer learning . While in the structure level , a learning method uses the large collection of auxiliary data to help learn the intrinsic structure of the data . Typical methods are harmonic function [ 16 ] and manifold regularization [ 17 ] in semi supervised learning , and Eigen transfer [ 18 ] in transfer learning . In the following , we show the connection between the two branches from the instance level .
It ’s clear that , in the instance level , the key step in inductive transfer learning is to calculate the weight of a diff distribution instance ( we only consider instance transfer here ) , while the key step in semi supervised learning is to calculate the label of an unlabeled instance . The basic idea of extending semi supervised methods for inductive transfer learning is to replace the step of calculating the label of an unlabeled instance with the step of calculating the weight of a diff distribution instance . This idea is intuitive , since if the weights of diff distribution instances are determined , the weighted diff distribution data can be considered under a quite similar distribution to the same distribution data ( this re weighting diffis because , distribution distribution divergence ) . Therefore , after re weighting , a set of labeled instances under the same or at least very similar distribution to the original labeled data are produced , which is equivalent to the result in semi supervised learning after the labels of unlabeled instanced being predicted . Thus , semisupervised learning methods such as self training and cotraining can be easily extended for inductive transfer learning . For instance , co training can be extended so that each learner teaches the other the weights of some diffdistribution instances . We will discuss the details of such method in next section . instance transfer , essentially in data reduces
486
The above thinking points out that , to make the extension feasible , a weighting strategy needs to be designed first . It is noteworthy that the weighting strategy can also be inspired by ideas in semi supervised learning . For instance , in semisupervised learning methods , the confidence of an unlabeled instance is often estimated , and the most confident instance is labeled with priority . Such labeling confidence is functionally similar to the weight in transfer learning , since it points out the usefulness of an auxiliary instance . In our co training method , the weighting strategy can be seen as an extension of the confidence measurement strategy in [ 7 ] .
V . CO TRAINING FOR INDUCTIVE TRANSFER LEARNING
A . Weighting strategy
In our co training method , k NN is chosen to be the base learner , with two considerations . First , due to the iterative property of co training , a base learner needs to be re trained many times . Since k NN is a lazy learner , the training process is just updating its training set , which is very efficient and easy to implement . Second , due to the local property of k NN ( ie the label of an instance is predicted by local neighbors ) , calculating the weight of an instance can be efficient . Therefore , the weighting strategy discussed below is particularly suitable for k NN , yet its underlying idea can be generalized to other learners .
To facilitate our discussion , we first define the neighbor error . Definition . The neighbor error of an instance is the error between its true label and the predicted label by its neighboring instances .
Let ( x y denote an instance whose weight is i iw . Its x , with the i k w , respectively . Therefore , the neighbor i k is calculated according to the k nearest neighbors are denoted by weight error of the instance ( following equation : w w i i 1 2 x x , i i 1 2 x y , i i
, ,
, ,
)
)
,
, i e x i
= | y i k j
− ∑ ∑ ∑
= 1 k j k j j j y w i i w i y w i i j
= 1
= 1 j
| j
( 1 )
∑ / w i gives the
= 1 k j j
Note in the above equation , predicted label of (
, x y by its k neighboring instances . i
) i
The underlying idea of the weighting strategy is to reweight an instance according to its influence on the training error . If the influence is relatively positive , the instance will have a relatively high weight . Let 'T denote the current 'T is set to be the same distribution training set . Initially , sT are set to data set 'T being updated in next be 1 ( we will explain how subsection ) . For each diff distribution instance ( , consider its influence on 'T . Assume that due to the addition of ( instances’ neighbor sT , and the weights of all instances in
, a number of x d i y d i
)
)
,
, x d i y d i goodn
( 2 ) error is reduced while a number of error is increased . If weight of ( good n
)
, x d i y d i n≥ , denoted by n good + n bad n good w d i
= badn and instances’ neighbor , then the goodn
>
0 bad iw , is set as follows : d
0
> badn and equals to goodn
Otherwise , the instance is simply dropped . We can see that in equation ( 2 ) , if , the weight has the goodn badn equals to 0 , the smallest value 0.5 ; if weight has the largest value 1 . The intuition is that if a diffdistribution instance brings more positive influence and less negative influence , its weight will be higher . What should be pointed out is that the neighbor error of an instance is not exactly the same as the training error of an instance , since the neighbor error is a real value while the training error is usually a binary value ( ie correct or not ) . However , using the neighbor error can help evaluate an instance ’s influence more accurately due to its real value property . B . COITL
Let
In this section , the proposed co training method ( COITL ) is discussed in detail . As mentioned in Section II , to make co training effective , two base learners should be diverse . In our implementation of co training , two learners both are k NN , thus the diversity is achieved by setting different values for the parameter k . Since the weighting strategy is based on the neighboring structure , different k values will result in differences on instances’ weights given by two learners . Such setting also brings another profit , as it is usually difficult to decide which k value is better for the learning problem at hand , two learners with different k values might have complementary effects .
1L ,
1T ,
2L denote the two learners , and
2T be their corresponding training set , respectively . Note , in cotraining , each learner is trained on its own training set , and its training set can be updated by the other learner . Initially , 1T and 2T are both set to be the same distribution data set sT , in which the weights of all instances are set to be 1 . In every iterative step , each learner randomly selects an unprocessed diff distribution instance and judges whether the instance be re weighted or just dropped . If the diffdistribution instance needs to be re weighted , then the learner sets the weight for the instance using the weighting strategy discussed in the above subsection , and adds the weighted instance to the training set of the other learner . This training step repeats until all diff distribution instances are processed . Note that in our method , a learner just selects a random diff distribution instance and gives it to the other if it can be re weighted . This implementation differs from the one in semi supervised learning , where usually the most confident instance is selected . We use the random selection just because it is more efficient than selecting the instance
487
1L and with the maximum weight ( analogous to the labeling confidence in semi supervised learning ) . After co training , the hypotheses of
α α + h h 1 1 2 2 1h and are two weighing factors ,
2L should be combined to give the final hypothesis . A simple method is to use the linear combination , that is , constructing a combined hypothesis , where α , 2h are two 2L , respectively . We can hypotheses given by 2e be the training calculate sT . error of , which implies that the
α as follows . Let 1e and 2 2L on the same distribution data set and
αand 1 1L and
1L and
*h by :
Then
α ∈
α =
[ 0,1 ]
= h
1
2
*
α = 1
2 e 2 + e 1 e 2 e 1 + e 1 e 2 weighting factor of a learner is larger if it produces less sT . The procedures of COITL are shown in training error on Table I .
TABLE I .
THE PROCEDURES OF COITL
Algorithm : COITL Input : the same distribution data set distribution data set parameter
1k ,
2k sT ( size = n ) , the diffdT ( size = m ) , the neighboring
Process : s iw ← 1 for i =1 to n end for T← , T
1 Set s
T← T s 2 1T and 2T to be the training sets for two learners 1L and 2L , respectively . The neighboring parameter for 1L and for t = 1 to for
2L are
/ 2m do
2k , respectively .
1k and j =
{1,2}
Randomly select an unprocessed diff distribution x instance ( , d i jT to calculate and n≥ y ) d i goodn goodn dT from badn and > then 0 good
Use if n
= w d i bad n good + n bad n good of ( x d i
, y d i
) iw to d
1jT + iw to d
1jT −
,
,
)
) if j equals to 1 then y with d i x d i x d i y with d i
Add ( else Add ( end if end if end for end for L← , h 2 1 1α and their training error on =
L← 2 2α as the weights of sT . h 1 Set
α α h h 1 1 2 2
+ h
*
Output :
1L and
2L , according to
C . Efficient implementation
)
)
,
)
,
)
) i
≤
−
− i 2 y d i y d i x d i x d i x i 1 is x i 2 i )k
, , set goodn of an instance (
It is noteworthy that the method in [ 7 ] also needs to find a set of instances being influenced by an added instance , ie having the added instance as a neighboring instance . They stated that finding all influenced instances in the whole training time consuming and proposed an approximation strategy . However , in this subsection , we give an efficient implementation of our method which can accurately find all influenced instances with the same time and space complexity as the approximation strategy . Note that in the weighting strategy , to calculate and badn , we need to find out a set of y ) d i . Here we call the set instances influenced by ( x , d i containing a few instances influenced by ( as the Λ ) . In order to efficiently influenced set ( denoted by i process the influenced set , our method needs to store necessary information . For simplicity , let us first consider 'T . the case when there is only one training set , denoted by x y in The size of 'T , i we use an array iA to store the indexes of its k neighbors i ( , . The array is sorted in ascending order 1 x y to each neighbor , , according to the distance between ( i − x D x ( ( )D returns ie , i i k the distance between two instances . The largest neighbor distance , ie , , is named the radius of the neighboring structure ( denoted by ir ) .
. For each instance (
) i ) . Here
O n m+ (
≤ ≤ x− i k
D x ( i
D x ( i
'T is
D x ( i
It ’s easy to see that the space complexity of our method O k n m+ is , since for each instance in the training set , a ( ( k length array is maintained . For discussing the time complexity , we distinguish two operations , namely searching for the influenced set and updating the influenced set . The first operation is invoked when we want to find out the weight of a diff distribution instance , while the second operation is performed when we want to add a weighted diff distribution instance to the training set . Assume that 1c time , and calculating the distance of two instances takes 2c time . Note that in comparing two real values takes c common cases , 1 . When searching for the influenced set , we need first to calculate the distance between the diffdistribution instance to each instance in the training set , which takes 1 time . After that , for each instance in the training set , its distance to the diff distribution instance should be compared with its radius to see whether the instance time . Therefore , the total time complexity for finding the influenced set is + cO n m c O n m cO n m ) ( 1
( Then , let us consider the time complexity for updating the influenced set . Note that the updating operation itself for one diff distribution c ( recall that 1 ) influenced , which c O n m+ ) c O n m+ ) instance
+ + ) takes c>> c>>
) . is
) )
=
)
)
+
(
(
2
2
1
2
,
2
(
2
(
(
(
(
<
)A
( ) contains two sub operations : first , the influenced set for the diff distribution instance should be searched ( because in cotraining , the weighted instance is added to a new training set ) ; second , the arrays of each instance in the influenced set c O n m+ ) should be updated . The first sub operation takes 1 time , as discussed above . For the second sub operation , since each array has been already sorted , updating one array c O k time . Let the maximum size of an influenced takes 2 An , thus , modifying all arrays take set be time . Combing the time complexity of the two sub operations together , the time complexity for updating the influenced set is 1 )A ( . Let us make another assumption , that is , . This assumption makes sense , since usually m is very large while k is relatively small ( in our experiments , m is 1000 , k is no more than 5 , and An equals to 3k ) . Hence the time complexity for updating the influenced is : < + + cOn m cOn m cOn m cOn m cOkn ) ( ) . The equation ) A 1 1 2 c>> c holds because 1 . The above analysis shows that , the time complexity of searching for and updating the influenced set both are 1
+ c O n m c O kn ( ) + n m k ) / ( set + = ) 1 c O n m+ ) c O kn 2
+ + )
+ An
Now we come to the co training setting , in which one learner first re weights a diff distribution instance ( this invokes the operation named searching for the influenced set ) , and adds the weighted instance to the training set of the other learner ( this invokes the operation named updating the influenced set ) . Thus , the time complexity for one learner to teach the other learner once is 1 c O n m+ ) c O n m+ ) their approximation strategy
As mentioned above , in [ 7 ] , an approximation strategy was proposed to process the influenced set . However , the time complexity of is still 1 ( as can be proved ) and the space complexity ( is also the approximation is actually not necessary . Also , as stated in [ 7 ] , in some cases the approximation may bring negative effects . This further demonstrates the advantage of our efficient implementation . the same as ours , which implies that
+
.
.
(
(
(
(
2
2
VI . EXPERIMENTS
A . Datasets and preprocessing
In the experiments , we choose eight datasets , four of which ( Mushroom 1 , Waveform 2 , Magic 3 , Splice 4 ) are directly obtained from UCI Machine Learning Repository , and the other three are generated by adding noise to the four original datasets . The Mushroom dataset has 8124 instances . Each instance has 22 attributes which describe some
1 http://archiveicsuciedu/ml/datasets/Mushroom 2http://archiveicsuciedu/ml/datasets/Waveform+Database+Generator+(Ve rsion+1 ) 3 http://archiveicsuciedu/ml/datasets/MAGIC+Gamma+Telescope 4 http://archiveicsuciedu/ml/datasets/Molecular+Biology+%28Splicejunction+Gene+Sequences%29
488 properties of a mushroom . All instances can be classified into two classes poisonous or non poisonous . We use the mechanism in [ 3 ] to construct the training and test set : for each instance , if its attribute stalk shape is enlarging , then this instance is put into the same distribution or test data set ; otherwise , it is put into the diff distribution data set . In all datasets , we fix the size of the diff distribution data set to be 1000 , the size of the test data set to be 1000 , and the maximum size of the same distribution data set is 500 ( note in experiments , the size of the same distribution data set has several different values ) . To show that the construction mechanism is effective , we can calculate the “ diff test error ” by setting the diff distribution data as the training data and calculate the prediction error on the test data ( the underlying learner is k NN with k = 3 ) . For the Mushroom dataset , the diff test error is 0.659 , which implies the large distribution difference in the same distribution data and the diffdistribution data ( note the test data and the same distribution data are under the same distribution ) . In the experiments , we are also interested in the robustness of a learning method . Here the robustness evaluates its sensitiveness to noisy instances . Therefore , for each of the four datasets , we also construct its corresponding noisy dataset ( for example , the Mushroom Noise dataset corresponds to the Mushroom dataset ) . The mechanism to construct the noisy dataset is to flip a diff distribution instance ’s label with the probability of 0015 Note that we only add noise to the diff distribution data . This makes sense , because usually the samedistribution instances are just a few , which are carefully labeled by humans . The diff distribution data , however , are often large in amounts , which tend to contain noise . The diff test error of the Mushroom Noise dataset is 0.666 , slightly larger than that of the Mushroom dataset .
The Waveform dataset has 5000 instances , each of which contains 21 attributes . All instances can be classified into three classes ; however , in the experiments we only choose instances from the first two classes . For each chosen instance , if its first attribute value is larger than 0.15 and its second attribute value is larger than 0 , it is put into the same distribution or test data set ; otherwise , it is put into the diff distribution data set . The diff test error is 0301 Similarly , we construct its corresponding noisy dataset by introducing labeling noise with the probability of 0015 The diff test error of the Waveform Noise dataset is 0405
The Magic dataset has 19020 instances , each with 10 attributes . All instances belong to two classes . For each instance , if its first attribute value is larger than 100 , it is added to the same distribution or test data set ; otherwise , it is added to the diff distribution data set . The diff test error of the Magic dataset and Magic Noise dataset are 0.123 and 0.204 , respectively .
The Splice dataset has 3190 instances , each containing 60 attributes . Each instance belongs to one of three classes ( ‘EI’ , ‘IE’ and ‘Neither’ ) . Since in this paper , only binary classification problem is addressed , we combine the class ‘EI’ and ‘IE’ to form a single class . Then , for each instance ,
489 if the first attribute value is ‘A’ or ‘G’ , we add it to the same distribution or test data set ; otherwise , we add it to the diff distribution data set . The diff test error of the Splice dataset and Splice Noise dataset are 0.263 and 0.287 , respectively .
Table II lists the information of the eight datasets .
TABLE II .
THE INFORMATION OF THE EIGHT DATASETS dataset
Mushroom
Mushroom Noise
Waveform
Waveform Noise
Magic
Magic Noise
Splice
Splice Noise
# attr . size dT of max size sT of size of E diff test error
22
21
10
60
1000
500
1000
1000
500
1000
1000
500
1000
1000
500
1000
0.659 0.666 0.301 0.405 0.123 0.204 0.263 0.287
Finally , complete content and organizational editing before formatting . Please take note of the following items when proofreading spelling and grammar : B . Experimental results
In the experiments , four learning methods are chosen to be compared . They are k NN , COITL , Aux k NN [ 5 ] and TrAdaBoost [ 3 ] . For k NN , k is set to be 3 . For COITL , the two k values are set to be 3 and 5 , respectively . The parameter settings for Aux k NN and TrAdaBoost follow those proposed in literatures . To test the performance of each learning method in different conditions , the size of the same distribution data varies from 10 to 500 . Table III shows the experimental results ( test error ) on the Mushroom dataset . We can see that , when the size of the samedistribution data set is smaller than 100 , transfer learning is effective , as it reduces the generalization error . When the size of the same distribution data set is larger than 100 , the effects of transfer learning are not apparent .
TABLE III .
EXPERIMENTAL RESULTS ( TEST ERROR ) ON THE
MUSHROOM DATASET
Learning Method kNN COITL
TrAdaBoost Aux kNN size of the same distribution data set
10 0.359 0.274 0.375 0.343
20 0.208 0.120 0.148 0.168
40 0.122 0.087 0.070 0.105
60 0.064 0.054 0.070 0.085
80 0.057 0.054 0.039 0.053
100 0.038 0.033 0.025 0.046
250 0.004 0.004 0.004 0.004
500 0.003 0.003 0.003 0.003
It may be difficult to assess the rank of each learning method ; therefore we provide a meaningful criterion to compare the performance of different methods . The criterion is named the Average Test Error Reduction ( ATER ) , which is the mean of the reduced test error compared to k NN when the size of the same distribution *L ( COITL , Auxdata set varies . Formally , for a learner k NN or TrAdaBoost ) , its ATER is obtained using the following equation :
=
1 8
−
)
∑
8 = i 1
( e k NN
)
ATER L ( * is the test error of k NN and e * L where NNke *Le is the test error *L . As can be seen , the ATER uses k NN as the baseline of to evaluate the performance of other learning methods . Also , it ’s easy to see that the ATER reveals the average generalization ability of a learning method .
TABLE IV .
Learning Method COITL
TrAdaBoost Aux kNN
THE ATER OF EACH LEARNING METHOD ON THE CLEAR
DATASETS .
ATER
Mushroom Waveform 0.028 0.015 0.006
0.100 0.058 0.042
Magic 0.077 0.056 0.037
Splice 0.161 0.065 0.035
Table IV lists the ATER of each learning method on the clear datasets . It is evident that COITL significantly outperforms TrAdaBoost and Aux kNN on all of the four datasets . Also , TrAdaBoost performs better than Aux kNN on each dataset .
Figure 2 shows the performance comparisons of different learning methods on clear datasets . Note that when the number of the same distribution data is quite small ( ie equals to 10 ) , transfer learning methods can significantly improve the test error . For example , in Waveform dataset , the test error is reduced from over 0.4 to about 0.1 when transfer learning is performed . Also note , the curves of the test error are not monotonously descending , because the same distribution data are so insufficient that adding just a few same distribution instances may not help to train a better learner .
( a ) Mushroom
( b ) Waveform
( c ) Magic
( d ) Splice
Figure 2 . Performance comparisons on clear datasets
490
TABLE V .
Learning Method
COITL TrAdaBoost Aux kNN
ATER
MushroomNoise 0.042 0.015 0.017
Waveform
Noise 0.095 0.057 0.022
THE ATER OF EACH LEARNING METHOD ON THE NOISY
DATASETS
MagicNoise 0.043 0.028 0.013
SpliceNoise 0.145 0.057 0.051
Table V lists the ATER of each learning method on the noisy datasets . Again , COITL achieves much higher generalization than the other learning methods . The results also demonstrate the high robustness of COITL . By contrast , the robustness of TrAdaBoost and Aux kNN are worse . Note that although TrAdaBoost performs better than Aux kNN on the Mushroom dataset , it makes larger test error on the Mushroom Noise dataset . Also note , Aux kNN has negative ATER on the Waveform Noise dataset , meaning that its transfer learning brings bad effects . Those observations reflect that TrAdaBoost and Aux kNN are more sensitive to noise than COITL .
Figure 3 shows the performance comparisons of different learning methods on noisy datasets . It is noteworthy that adding noise to the diff distribution data may reduce the test error of the transfer learning methods . For instance , in the learning Mushroom Noise dataset , methods have better performance the Mushroom dataset . This is not strange because those methods always select useful diff distribution instances based on the same distribution data . However , in many cases , the introduction of noisy instances can have bad effects . Comparing Table IV with Table V , we can see that TrAdaBoost and Aux kNN perform worse on the Waveform Noise and Magic Noise datasets than on the Waveform and Magic datasets . that on than the three transfer
( a ) Mushroom Noise
( b ) Waveform Noise
( c ) Magic Noise
( d ) Splice Noise
Figure 3 . Performance comparisons on noisy datasets
491 to generalize well
The outstanding performance of COITL can be roughly explained as follows . First , it utilizes the co training mechanism which helps through maximizing the agreement on the diff distribution data by two learners . Second , the different k values for each kNN allow them to have some compensated effects on different datasets . For example , kNN with a very small value of k tends to perform well when there is little noise in the dataset , while kNN with a relatively large value of k may be more robust on noisy dataset . In our experiments , we found that although k values are sensitive to different datasets , setting them to 3 and 5 may be a robust choice , as it gives good results on each dataset . In practice , one can also use cross validation to choose more proper k values .
VII . CONCLUSION
Inductive transfer learning and semi supervised learning are two different branches of machine learning . In this paper , we bridge them by showing that many semisupervised learning methods can be extended for inductive transfer learning , if the step of labeling of an unlabeled instance is replaced by re weighting a diff distribution instance . Based on this recognition , we develop the COITL algorithm which extends the co training method in semisupervised learning . COITL employs two k NN learners with different values of k . In every learning iteration , each learner re weights a diff distribution instance for the other one , where the weight of the instance is determined according to its influence on the training error . The final prediction is made by linearly combining the predictions of both learners . Moreover , an efficient implementation of COITL with relatively low time complexity is discussed . Experimental results show that , compared with two state ofthe art methods in inductive transfer learning , COITL can have less generalization error and stronger robustness .
There are three research issues to be explored in the future : first , theoretically analyzing the properties of COITL ; second , extending other semi supervised learning approaches , like the graph methods , for inductive transfer learning ; learning approaches for semi supervised learning . The third issue is quite interesting , as we believe it will bring more insightful understanding of the relationship between semi supervised learning and transfer learning . third , extending inductive transfer
ACKNOWLEDGMENT
[ 3 ] Dai W . , Qiang Y . , Xue G . , and Yong Y . , “ Boosting for Transfer Learning ” , In Proceedings of the 24th International Conference on Machine Learning ( ICML ) , 2007 .
[ 4 ] Argyriou , A . , Evgeniou , T . , and Pontil , M . , “ Multitask Feature Learning ” , In Proceedings of the 19th Annual Conference on Neural Information Processing Systems ( NIPS ) , 2007 .
[ 5 ] Wu P . , and Thomas G . D . , “ Improving SVM Accuracy by Training on Auxiliary Data Sources ” , In Proceedings of the 21st International Conference on Machine Learning ( ICML ) , 2004 .
[ 6 ] Sarkar , A . , “ Applying Co training Methods to Statistical Parsing ” , In Proceedings of the 2nd Annual Meeting of the North American Chapter of the Association for Computational Linguistics , 2001 .
[ 7 ] Zhou , Z . H . , and Ming L . , “ Semi supervised Regression with Cotraining Style Algorithms ” , IEEE Transaction on Knowledge and Data Engineering , 2007 , pp . 1479 1493 .
[ 8 ] Liao , X . , Ya X . and Lawrence C . , “ Logistic Regression with An auxiliary Data Source ” , In Proceeding s of the 21st International Conference on Machine Learning ( ICML ) , 2004 .
[ 9 ] Blum , A . and Mitchell , T . , “ Combining Labeled and Unlabeled Data with Co training ” , In Proceedings of the 11th Annual Conference on Computational Learning Theory ( COLT ) , 1998 .
[ 10 ] Dasgupta , S . , Littman , M . , and McAllester , D . , “ PAC Generalization in Neural Information
Bounds for Co training ” , In Advances Processing Systems , MIT Press , 2002 , pp . 375 382 .
[ 11 ] Hwa R . , Osborne , M . , Sarkar , A . , and Steedman , M . , “ Corrected Cotraining for Statistical Parsers ” , In Working Notes of the ICML’03 Workshop on Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining , 2003 .
[ 12 ] Pierce , D . , and Cardie , C . , “ Limitations of Co training for Natural Language Learning from Large Data Sets ” , In Proceedings of Empirical Methods in Natural Language Processing , 2001 .
[ 13 ] Goldman , S . , and Zhou , Y . , “ Enhancing Supervised Learning with International
Unlabeled Data ” , Conference on Machine Learning ( ICML ) , 2000 .
In Proceedings of the 17th
[ 14 ] Wang , W . and Zhou Z . H . , “ Analyzing Co training Style Algorithms ” , In Proceedings of the 18th European Conference on Machine Learning ( ECML ) , 2007 .
[ 15 ] Culp M . and Michailidis G . , “ An Iterative Algorithm for Extending Learners to a Semi supervised Setting . Journal of Computational Graphics and Statistics , 2008 .
[ 16 ] Zhu , X . , Ghahramani , Z . , and Lafferty , J . , “ Semi supervised Learning Using Gaussian Fields and Harmonic Functions ” , In Proceeding s of the 20th International Conference on Machine Learning ( ICML ) , 2003 .
[ 17 ] Belkin , M . , Niyogi , P . , and Sindhwani , V . , “ Manifold Regularization : A Geometric Framework for Learning from Labeled and Unlabeled Examples ” , Journal of Machine Learning Research , 2006 .
[ 18 ] Dai , W . , Jin , O . , Xue , G . , Yang , Q . , and Yu , Y . , “ Eigen Transfer : A Unified Framework for Transfer Learning ” , In Proceedings of the 26th International Conference on Machine Learning ( ICML ) , 2009 .
We thank the anonymous reviewers for their helpful comments .
REFERENCES
[ 1 ] Zhu X . , “ Semi supervised Learning Literature Survey ” , Technical
Report 1530 , University of Wisconsin Madison , 2008 .
[ 2 ] Sinno J . P . , and Qiang Y . , “ A Survey on Transfer Learning ” , Technical Report , Hong Kong University of Science and Technology , 2008 .
492
