Sparse Norm Regularized Reconstructive Coefficients Learning
Bin Liu,Shuo Chen,Mingjie Qian,Changshui Zhang
State Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology(TNList )
{liubin07,chenshuo07}@mailstsinghuaeducn,qianmingjie@gmailcom,zcs@mailtsinghuaeducn
Department of Automation,Tsinghua University,Beijing , PRChina
Abstract—Inspired by the fact that the final decision rule is mainly affected by a small subset of the training samples , ie , Support Vector Machine(SVM ) shows that the decision function relies on the few samples that are on or over the margin . We propose a new framework that explicitly strengthen this intuitive fact by adding an l1 norm regularizer . We give different formulations for our framework in different scenarios , and the experiments show that our framework can not only lead to high sparse solutions but also better performance than traditional methods .
Keywords support vector machine;l1 norm;sparse ;
I . INTRODUCTION
Finding the sparsity of the intrinsic data structure has in machine learning , statistics attracted a lot of interest and signal processing . It is quite common that we get an abundance of data for a particular learning task . For example , the massive text data from the web pages and the easily captured image by our digital cameras . However the valuable information may not increase as the number of the training samples does . For example , Support Vector Machine(SVM ) ( [1 ] ) in the supervised learning scenario has shown that there is only a small number of samples that affect the final decision boundary . In other words , the indices for the samples as support vectors are very sparse .
One method aiming at sparse solution is by utilizing l1norm regularization . Many works have shown that by adding an l1 norm regularizer , one can not only find the intrinsic structure of the data sets , but also improve the performance of the learning machine . For example , in linear regression scenario the famous Lasso regularizer was proposed to do the model selection ( [2] ) . The l1 norm regularizer was also used to do feature selection in SVM ( [3 ] , [ 4 ] , [ 5] ) . [ 6 ] used the characteristics of sparsity in image processing .
In this paper we propose a novel framework for reconstructive coefficients learning with l1 norm regularization term . Our work is based on the assumption that : the number of the training samples that mainly affect the final decision function is very small . Our goal is trying to find the samples that are close enough to the decision boundary no matter it is labeled or unlabeled . We show that our framework can be specialized to many existed supervised and semi supervised sparse methods .
In section 2 , we introduce some related works . In section 3 , We propose our new framework firstly . Then we specialize our framework to fit the supervised and semi supervised scenario . At last we extend our framework by using multikernel functions . We demonstrate our experiment results and analyzing in section 4 , and conclusions and future work is in the last section . Notations : In this paper we assume that the number of labeled samples is , and the number of the unlabeled samples is u . We use N to denote the number of the whole training samples . Specifically , in supervised learning scenario all the samples are labeled , so N = . On the other hand , in semi supervised scenario we not only have the labeled samples but also u unlabeled samples , so N = +u . We use K to denote the kernel matrix constructed by the whole training dataset X = [ x1,··· , x , x+1,··· , x+u ] . We would like to emphasize that the order of the dataset X is fixed . This means that when constructing the kernel matrix K , the first sample are the labeled samples and the rest u samples are unlabeled ones .

··· K(x1 , xN ) ··· K(xN , xN )
 K(x1 , x1 )
K(xN , x1 )
K =
K(x , y ) is the kernel function f : X × X → Y , where X denotes the original space while Y denotes the highdimensional space , ie the Radio Basis Function(RBF ) kernel . Its form is as follows :
K(x , y ) = e
−x−y2 2
σ2
In this paper we use K to denote the first rows of the kernel matrix K . We will introduce some existing methods which are related to our work in this section .
II . RELATED WORKS
A . Sparse Support Vector Machine
Nowadays SVM has been widely used in many fields of machine learning . Its main idea is to construct a classifier which not only minimizes the error on the training dataset , but also maximizes the margin between different classes . Reader can refer to [ 1 ] for detailed formulation .
The Sparse Support Vector Machine(SSVM ) we would like to mention was proposed by [ 5 ] . Its main idea is to constraint the cardinality of the parameter vector w as small as possible , where the cardinality of a vector denotes the number of the non zero element of the vector . It is based on the implicit assumption that only a few number of the features are useful in constructing the goal classifier . The SSVM can be considered as a common SVM training process combining with a feature selection procedure . SSVM is superior to the standard SVM when the training dataset is dirty with noise .
B . Sparse Support Vector Regression
Support Vector Machine(SVM ) can be easily extended for the regression scenario . The regression problem is to find a function f : X → Y that minimize the regularized risk functional ( [7 ] , [ 8 ] , [ 9] ) :
R[f ] := P [ f ] + C
1
L(yi , f(xi) ) ,
( 1 ) where X , Y denotes the spaces of input and output respectively , is the number of the training samples and L(· ) is a loss function ( [4] ) . Usually L(· ) takes the form of hinge loss , the regularization operator P [ · ] is to control the complexity of the model and C is a parameter to control the trade off between the two parts of the objective function R[f ] .
If we choose the linear model then f(x ) = wT x + b , the regression becomes Support Vector Regression(SV R ) . Further more , if we import the kernel trick ( [10 ] ) then the function f becomes f(x ) =
αiK(xi , x ) + b i=1 i=1 i=1
1 i=1 if we use hinge loss then the formulation becomes
R[f ] := α1 +C max(|yi−
αiK(xi , x)−b|− , 0 ) ( 3 )
C . Semi Supervised Learning
Semi supervised Learning(SSL ) is a framework of machine learning that it tries to train a classifier with a small number of labeled data and a huge number of unlabeled data . It is based on the implicit assumption that samples that are close to each other have high probability to belong to the same class . and the optimal solution of w to SVM can be written in the following expression : w = αiΦ(xi ) . The Sparse Support Vector Regression(SSVR)([4 ] ) changes the l2 norm of vector α which used in the standard support vector regression to l1 norm . The formulation ( [4 ] ) is
α∗ = arg min
α where
R[f ] :=
|αi| + C
1
L(yi , f(xi ) )
( 2 ) f(x ) =
αiK(xi , x )
( 8 )
If the abundant unlabeled data lies on manifolds ( [11] ) , even though the number of labeled data is small we still can train a good enough classifier with the help of the manifold information . The general formulation of the Belkin ’s manifold regularization of semi supervised learning is as follows : f∗ = arg min f∈Hk
1
V ( xi , yi , f ) + γAf2
K + γIf2
I ( 4 )
According to Belkin ’s paper[12 ] , f2 I is an appropriate penalty item that reflects the intrinsic structure of the distribution of the data x . The optimal solution of f is i=1 f(x ) =
K(xi , x)αi
( 5 ) i=1 where , u denote the number of labeled data and unlabeled data respectively . K(·,· ) is the kernel function and α is the parameter vector .
III . FRAMEWORK OF NORM REGULARIZED RECONSTRUCTIVE COEFFICIENTS LEARNING
In last section we have reviewed several supervised and semi supervised learning methods . We can see that the optimal solutions of SVM , Belkin ’s framework and SVR are all taking the following form : f(x ) =
αiK(xi , x )
( 6 )
+u
N i=1
Inspired by the idea of the sparse support vector regression and the general equation ( 6 ) used in many formulations , we propose a novel learning framework . We call it “ NormRegularized Reconstructive Coefficients Learning ” , “ NRCL ” for short . Our formulation is as follows :
αp + λ
L(f(xi ) , yi ) + R[f ]
( 7 )
N N i=1 i=1
K(x , y ) is the kernel function . xp denotes the lp norm of x , R[f ] denotes the regularization of function f . L(x , y ) is the loss function . If we use hinge loss for our loss function then the formulation of our framework becomes
α∗ = arg min
α
αp + λ
ξi + R[f ]
( 9 ) i=1 y = Kα + ξ i = [ 1,··· , ] st ξi ≥ 0 , where y is the label vector , and the ith element of y is the label of xi . K is the first rows of the kernel matrix K which is constructed by the data X = [ x1,··· , xN ] . α is i=1 the vector constructed by {αi}N i=1 . ξ is the vector aligned by ξi , i = [ 1,··· , ] . If we use the square loss function , our formulation becomes
α∗ = arg min
α
αp + λy − Kα2
2 + R[f ]
( 10 )
In the following subsections we will mainly analyze our framework in the form of equation ( 10 ) as a special case .
A . Sparse Norm Regularized Reconstructive Coefficients Learning
Our work in this subsection is mainly inspired by the good characteristic of SVM , where the reconstructive coefficients are enforce this characteristic explicitly . This is based on the very sparse , with the nonzero values only for the samples that are near to the decision boundary of the classifier . We thus want to assumption that only a small number of training samples contribute to the construction of the classifier .
In our framework a direct idea is using the l0 norm of vector α in the objective function to make the final coefficients sparse . But to optimize the l0 norm is an NP hard problem . According to [ 5 ] we can optimize the l1 norm of vector α instead . Then the equation ( 7 ) becomes the follows :
α∗ = arg min
α
α1 + λ
L(f(xi ) , yi ) + R[f ]
( 11 )
Also we can rewrite it in the following form when using square loss :
α∗ = arg min
α
α1 + λy − Kα2
2 + R[f ]
( 12 )
Different R[f ] will lead to different formulations of our framework . Inspired by the framework of SSL ( [12 ] ) mentioned above , one assumption is that if the dataset lies on manifolds , we can add a regularized item to guarantee the smoothness of the function on manifolds . And in order to control the complexity of model on RKHS ( Representing Kernel Hilbert Space ) ( [13] ) , we can add another regularized item , then the equation ( 12 ) becomes
α∗ = arg min
α
α1 + λy − Kα2 2 + γAαT Kα + γI αT KLKα
( 13 ) where L is the Laplacian matrix [ 14 ] . We call the above formulation as “ supervised SNRCL ” . The Laplacian matrix is constructed as follows([15] ) : Given the data {xi}N i=1 , first we construct an undirected weighted graph G = {X , E} with vertex set X and the edge set E . The edges are decided by the similarity of the vertexes . If the vertex xi and xj are close to each other , then there is an edge link the two vertexes , for detailed people can refer to paper([15] ) . If we give weights to the edges in the graph , then we can get the similarity matrix W ∈ RN×N . The element of real symmetry matrix W measures the similarity of the vertex pair , which can be computed in different ways([15] ) . At last the Laplacian matrix L can be computed in the following way :
L = D − W where Di,i =
Wj,i
( 14 ) j
B . Multi Class Sparse Norm Regularized Kernel Learning In the above we only talked about the common regression and binary classification scenarios , it is easy for our framework to be extended to the multiple classification problem . Suppose our discriminative function changes to the following form : f = Kα
( 15 ) where α is a N×M matrix , N is the number of the data and M is the number of the classes . The output of the function f is a vector . Thus our framework changes as follows :
α
2 +
α∗ = arg min vec(α)1 + Y − Kα2 γAtr(αT Kα ) + γItr(αT KLKα ) ( 16 ) Here Y becomes a indicate matrix and its size is n × m . Yi,j = 1 only xi belongs to the jth class , otherwise Yi,j = 0 . vec(α ) is the function that expands the matrix α to a vector . tr(α ) is to get the trace of the matrix α .
C . Semi supervised Sparse Norm Regularized Reconstructive Coefficients Learning
Obviously our framework can be easily extended to the semi supervised learning scenario . The formulation is as follows :
α∗ = arg min
α
α1 + λy − Kα2 2 + γAαT Kα + γI αT KLKα
( 17 )
We call it “ semi supervised SNRCL ” for simplicity .
Belkin ’s framework[12 ] is a very effective method when the huge unlabeled data lies on manifold in semi supervised learning . In format our framework is similar to the Belkin ’s framwork , but we add more item α1 in the formulation for we not only want to use the manifold information but also to find the important unlabeled samples which is near the decision boundary , just as the SVM does .
D . Supervised Sparse Norm Regularized Multi Kernel Reconstructive Coefficients Learning
In the above we have only discussed the the decision function in the form of f(x ) =
αiK(xi , x )
( 18 ) i=1 where the kernel K(·,· ) is a single fixed form such as RBF kernel . This limits the flexibility of our framework . A possible extended formulation of our framework is that we use a multiple kernel instead of the single one ( [16] ) . We call it “ Supervised Sparse Norm Regularized Multi Kernel
Reconstructive Coefficients Learning ” . The formulation of the multi kernel function is as follows :
αi(
βjKj(xi , x ) )
( 19 ) m m i=1 j=1 f(x ) = st j=1
βj = 1 ,
βj ≥ 0 where each Kj(·,· ) denotes one of the selected set of different kernel functions . This could be of two situations . One is that all the kernels within the selected set have the same form m ( ie they are all RBF kernels ) but different parameters . The other is that the kernels have different forms and different j=1 βjKj(·,· ) as a new parameters . In fact , we can deem kernel K(·,· ) , then our formulation becomes α2
α∗ = arg min
α1 + λy − K γAαT Kα + γI αT KLKα
2 +
( 20 )
α
 K(x1 , x1 )
K(xN , x1 )
K =

··· K(x1 , xN ) ··· K(xN , xN )
E . Semi supervised Sparse Norm Regularized Multi Kernel Reconstructive Coefficients Learning
It is natural for us to use multiple kernel in sparse kernel semi supervised learning framework . We call it “ Semisupervised Sparse Norm Regularized Multi Kernel Reconstructive Coefficients Learning ” , and the formulation is as follows :
α∗ = arg min
α
α1 + λy − K γAαT Kα + γI αT KLKα
α2
2 +
( 21 ) Also the framework can be used in multi class sparse supervised kernel learning framework with little change for ( 21 ) like ( 16 ) . F . Quadratic Programming for Our Work
The l1 norm in our formulation will significantly decelerate the computational speed of our method . In order to solve our formulation efficienttly , we rewrite αj = uj − vj , where uj , vj ≥ 0 , just as the paper[4 ] . The solution has either uj or vj equal to 0 , which depends on the sign of αj , so in our formulation |αj| = uj + vj . Let u = [ u1,··· , uN ] v = [ v1,··· , vN ] , x = [ u v]T , Then our formulation changes as follows : x∗ = arg min st xi ≥ 0 , x xT Hx + f T x i = [ 1,··· , 2N ]
( 22 )
( cid:181 ) where
H =
K −K −K K
( cid:182 )
K = γAK + γI KLK + λK T
K f1 = [ 1,··· , 1 f = f1 − f2
,−1,··· ,−1 ]T f2 = [ −2λyT K , 2λyT K]T
N
N IV . EXPERIMENTS such as supervised SNRCL with the
We compare the performances of the proposed sparse norm regularized reconstructive coefficients learning methods standard SVM on five UCI data sets and sparse semi supervised norm regularized reconstructive coefficients learning methods such as semi supervised SNRCL , with the Laplacian SVM(LapSVM ) and Laplacian Regularized Least Square Classification(LapRLSC ) on the synthetic Two Moons Dataset and the Columbia Object Image Library Dataset(Coil20 ) .
A . Supervised Experiments
1 ) Data Sets : Our experiments were performed on the five UCI data sets listed in Table 1 . Some of them are of binary classification setting such as ionosphere , parkinson . Others like brown yeast , iris and wine data sets are of multiclass classification setting . For convenience , we only do the binary classification tasks between several selected pairs of classes for the multi class data set . In table 1 the number behind the name of data set means the selected pair of classes for binary classification . All data sets are available at [ 17 ] , and the brown yeast data set is available at [ 18 ] .
2 ) Experimental Setup : We split randomly with 80 % of the data for training and cross validation , and 20 % heldout for testing . For standard SVM , we use RBF kernel for all the data sets , the optimal σ parameter and the optimal C parameter are selected using 5 fold cross validation with 80 % for training and 20 % for validation over the range {2−6 , 2−5,··· , 25 , 26} . For our supervised SNRCL , the graph is constructed with parameters k = 15 , σL = 1000 , and the parameters γA , γI and λ are set by gridsearch from 2−6 to 26 . Finally , the cardinality of the α vector is computed by counting the number of relatively large magnitude elements of the coefficient vector α , ie the number of the elements with |αj| ≥ max(α)· 10−5 . The elements that less than max(α ) · 10−5 are set to zero .
Each method was tested on the held out samples , and the final results reported were averaged over 20 trials of different random splits of the data set . We used the LibSVM([19 ] ) for our standard SVM and our method were trained using standard convex optimization toolbox like CVX([20] ) , MOSEK([21] ) .
3 ) Results : The overall experimental results on five UCI data sets are shown in Table 1 . We can see that in many cases our method supervised SNRCL is superior to the standard SVM . Though the accuracy increase is very low , the cardinality of the coefficients α is far smaller than the standard SVM . In contrast our method had an average sparsity than standard SVM .
B . Semi supervised Experiments we will design a efficient algorithm to solve this problem .
1 ) Synthetic data : Tow Moons dataset : The dataset contains 700 samples with 3 % randomly labeled for each class . We found that by tuning the parameters , both our method and LapRLS can reach 100 % of the predicting accuracy . The first two images in Figure 1 demonstrate the original datset and the randomly labeled samples . Compare the last two images in Figure 1 , we can see that the cardinality of the coefficient vector α of our SNRCL method is sparser than LapRLS . In other words , the number of the samples with non zero coefficients of the LapRLS method is more than that of ours . The right image in Figure 1 shows that the samples with non zero coefficient of the LapRLS method are exactly the labeled instances . However , ours are not totally the labeled samples , while using some of the unlabeled samples . For details , our method not only uses the manifold information by the unlabeled data but also selects the unlabeled samples which are important to our final decision function .
2 ) Columbia Object Recognition : Coil20 is a data set of gray scale images of 20 objects ( [22] ) . For every object there are 72 images , so in Coil20 data set the number of the class is 20 and every class contains 72 instances , and each instance is composed of 1,024 elements . In our experiment we only use 10 classes of them to do our experiment .
We applied LapSVM , LapRLSC and semi supervised SNRCL algorithms to 45 binary classification problems that arise in pairwise classification of Columbia Object Images . For each pairwise classification experiment we randomly labeled 20 % of the samples and the parameters γA , γI , λ are set by grid search from 2−6 to 26 . The final results reported are averaged over 20 trials as Figure 4 shows . It also shows the cardinality of the the coefficient vector α of the applied methods .
From Figure 2 we can see that our method is superior to the LapSVM and LapRLSC for most of the cases . Also our methods has the advantage of lower cardinality of coefficient vector α . In contrast our method had an average sparsity of 200 % than the former two algorithms .
V . CONCLUSION
In this paper we have proposed a novel framework for sparse coefficients learning . In supervised learning scenario we proposed the supervised sparse norm regularized reconstructive coefficient learning method and we compared the standard SVM with our method in five UCI datasets . We found that our method not only increase the predicting accuracy but also get sparser solutions . In semi supervised scenario we proposed the semi supervised SNRCL method . Through several experiments we have found that our method can really find the unlabeled samples that are near the decision boundaries .
The implementation of our methods is quite slow when the number of the training samples is very large . Future work
ACKNOWLEDGMENT
The work is funded by Tsinghua National Laboratory for Information Science and TechnologyTNListCross discipline Foundation .
REFERENCES
[ 1 ] V . Vapnik , The nature of statistical learning theory . springer ,
2000 .
[ 2 ] R . Tibshirani , “ Regression shrinkage and selection via the the Royal Statistical Society . Series B lasso , ” Journal of ( Methodological ) , pp . 267–288 , 1996 .
[ 3 ] K . Bennett and O . Mangasarian , “ Robust linear programming discrimination of two linearly inseparable sets , ” Optimization Methods and Software , vol . 1 , no . 1 , pp . 23–34 , 1992 .
[ 4 ] J . Bi , K . Bennett , M . Embrechts , C . Breneman , and M . Song , “ Dimensionality reduction via sparse support vector machines , ” The Journal of Machine Learning Research , vol . 3 , pp . 1229–1243 , 2003 .
[ 5 ] A . Chan , N . Vasconcelos , and G . Lanckriet , “ Direct convex relaxations of sparse svm , ” in Proceedings of the 24th international conference on Machine learning . ACM New York , NY , USA , 2007 , pp . 145–153 .
[ 6 ] J . Mairal , M . Elad , G . Sapiro et al . , “ Sparse representation for color image restoration , ” IEEE Transactions on Image Processing , vol . 17 , no . 1 , p . 53 , 2008 .
[ 7 ] B . Boser , I . Guyon , and V . Vapnik , “ A training algorithm for optimal margin classifiers , ” in Proceedings of the fifth annual workshop on Computational learning theory . ACM New York , NY , USA , 1992 , pp . 144–152 .
[ 8 ] V . Vapnik , S . Golowich , and A . Smola , “ Support vector method for function approximation , regression estimation , and signal processing , ” Advances in Neural Information Processing Systems 9 , p . 281 , 1997 .
[ 9 ] A . Smola and B . Sch¨olkopf , “ A tutorial on support vector regression , ” Statistics and Computing , vol . 14 , no . 3 , pp . 199– 222 , 2004 .
[ 10 ] B . Scholkopf , “ The kernel trick for distances , ” Advances in Neural Information Processing Systems , pp . 301–307 , 2001 .
[ 11 ] M . Hirsch , C . Pugh , and M . Shub , “ Invariant manifolds , ”
1977 .
[ 12 ] M . Belkin , P . Niyogi , and V . Sindhwani , “ On manifold regularization , ” in Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics ( AISTAT 2005 ) , 2005 .
[ 13 ] R . Rosipal and L . Trejo , “ Kernel partial least squares regression in reproducing kernel hilbert space , ” The Journal of Machine Learning Research , vol . 2 , pp . 97–123 , 2002 .
[ 14 ] B . Mohar , “ The Laplacian spectrum of graphs , ” Graph theory , combinatorics , and applications , vol . 2 , pp . 871–898 , 1991 .
Figure 1 . Results on Two Moons Dataset : The first image(Sequence is from the left side to the right side ) demonstrates the whole rand sampled dataset and the second one shows the labeled samples and the third one illustrates the samples whose coefficients are non zero in LapRLS and the last one gives the results of SNRCL
Figure 2 . Coil20 Experiments Error Rates at Precision Recall BreakEven points for 45 binary classification problems
UCI data sets Wine 12 Wine 13 Wine 23 ionosphere parkinson iris 12 iris 13 iris23 yeast 34 yeast 45 yeast 35
SVM accuracy 0.9208 ± 0.0419 0.9524 ± 0.0593 0.9313 ± 0.0556 0.9431 ± 0.0262 0.8410 ± 0.0606 1.0000 ± 0.0000 1.0000 ± 0.0000 1.0000 ± 0.0000 0.9306 ± 0.0239 0.9086 ± 0.0487 0.9007 ± 0.0404
SNRCL accuracy 0.9338 ± 0.0467 0.9790 ± 0.0307 0.9426 ± 0.0434 0.9123 ± 0.0488 0.8285 ± 0.0487 1.0000 ± 0.0000 1.0000 ± 0.0000 1.0000 ± 0.0000 0.9314 ± 0.0212 0.9219 ± 0.0319 0.9340 ± 0.0383 Table I
SVM α Card 44.5900 ± 28.2403 11.4400 ± 3.2335 24.8800 ± 6.5641 114.6800 ± 33.2860 94.3400 ± 13.8426 3.2400 ± 0.5175 3.2000 ± 0.5345 3.1800 ± 0.3881 67.6600 ± 21.4786 60.6000 ± 18.2130 79.7800 ± 9.9083
SNRCL α Card 36.1600 ± 2.6236 22.3000 ± 1.7871 18.1200 ± 1.4658 24.9200 ± 2.0288 53.7400 ± 8.8475 2.4600 ± 0.6455 2.7200 ± 0.6713 2.6400 ± 0.6928 22.9200 ± 1.7594 9.3800 ± 1.2103 50.7200 ± 2.6189
RESULTS ON FIVE UCI DATA SETS
[ 15 ] D . Cvetkovic , M . Doob , and H . Sachs , Spectra of graphs : Theory and applications . Academic press New York , 1980 .
[ 21 ] A . MOSEK , “ The MOSEK Optimization Tools Version 25
Users Manual and Reference , 2002 . ”
[ 22 ] S . Nene , S . Nayar , and H . Murase , “ Columbia object image library ( coil 20 ) , ” Department of Computer Science , Columbia University , Technical Report CUCS 006 96 .
[ 16 ] S . Sonnenburg , G . Ratsch , and C . Schafer , “ A general and learning algorithm , ” Advances in efficient multiple kernel Neural Information Processing Systems , vol . 18 , p . 1273 , 2006 .
[ 17 ] C . Blake and C . Merz , “ UCI repository of machine learning databases , ” 1998 .
[ 18 ] J . Weston , A . Elisseeff , B . Sch¨olkopf , and M . Tipping , “ Use of the zero norm with linear models and kernel methods , ” The Journal of Machine Learning Research , vol . 3 , pp . 1439– 1461 , 2003 .
[ 19 ] C . Chang and C . Lin , “ LIBSVM : a library for support vector machines , 2001 , ” Software available at http://www . csie . ntu . edu . tw/cjlin/libsvm , 2001 .
[ 20 ] M . Grant , S . Boyd , and Y . Ye , “ CVX : Matlab software for disciplined convex programming , ” web page and software ) . http://stanford . edu/ boyd/cvx .
−15−1−050051152253−15−1−050051152Two Moons Dataset−15−1−050051152253−15−1−050051152Labeled Data Points−15−1−050051152253−15−1−050051152LapRLS samples with non−coefficient in two moon dataset−15−1−050051152253−15−1−050051152SNRCL samples with non−coefficient in two moon dataset51015202530354045000100200300400500600745 Classification ProlemsError RatesLapSVM vs SNRCL lapsvmsnrcl51015202530354045000100200300400500600745 Classification ProlemsError RatesLapRLS vs SNRCL laprlssnrcl5101520253035404540608010012014016045 Classification Prolemsα−CardinalityLapSVM vs SNRCL lapsvmsnrcl5101520253035404540608010012014016045 Classification Prolemsα−CardinalityLapRLS vs SNRCL laprlssnrcl
