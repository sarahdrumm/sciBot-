Semi Supervised Multi Task Learning with Task Regularizations
Fei Wang , Xin Wang , Tao Li
School of Computing and Information Sciences
Florida International University
Miami , FL 33199 USA
{feiwang,xwang009,taoli}@csfiuedu
Abstract—Multi task learning refers to the learning problem of performing inference by jointly considering multiple related tasks . There have already been many research efforts on supervised multi task learning . However , collecting sufficient labeled data for each task is usually time consuming and expensive . In this paper , we consider the semi supervised multitask learning ( SSMTL ) problem , where we are given a small portion of labeled points together with a large pool of unlabeled data within each task . We assume that the different tasks can form some task clusters and the task in the same cluster share similar classifier parameters . The final learning problem is relaxed to a convex one and an efficient gradient descent strategy is proposed . Finally the experimental results on both synthetic and real world data sets are presented to show the effectiveness of our method .
I . INTRODUCTION
In many practical problems , a learning task can usually involve multiple related tasks . Solving those related tasks together is expected to be more advantageous than solving them independently because the knowledge from different tasks can help to improve the generalization ability of the classifier constructed on each individual task . In recent years , the problem of learning multiple related tasks ( usually referred to as multi task learning ( MTL ) ) has been investigated extensively [ 2][3][11][15][4][16 ] , and the related approaches have been applied to a variety of application areas , such as marketing [ 5 ] , computer vision [ 19 ] and bioinformatics[9 ] . Clearly , the exploration of task relationships is at the heart of multi task learning . Many algorithms have been proposed to address this issue . Sheldon [ 18 ] and Kato et al . [ 15 ] assume task relationships are known beforehand ; Argyriou et al . [ 3 ] assumes the data associated with those tasks share some common latent features and derive an optimization strategy to obtain these features ; Evgeniou and Pontil [ 11 ] suppose that the classifiers of all these related tasks share a common part of parameters ; Allenby and Rossi [ 1 ] and Arora et al . [ 5 ] applied a hierarchical Bayesian model with a common prior for the classifier parameters of all tasks .
As another research line , semi supervised learning ( SSL ) [ 8 ] provides an alterative way to improve the classification performances . SSL assumes we are given a partially labeled data set , and it makes inferences by making use of both labeled and unlabeled points [ 2 ] , [ 16 ] . There are also some works that integrate MTL and SSL together , where they assume that there are some labeled and unlabeled points available for each task , and they use all these data samples and the task relationships to perform multi task learning .
Figure 1 . A graphical illustration of learning with task groups , where the large nodes correspond to tasks , and the connected small nodes are the data points within the tasks . The tasks in a dashed line form a task group .
Despite their empirical success and theoretical soundness , one of the major limitations of those traditional methods is that they assume all the tasks should be related with each other . In practice , a more reasonable assumption is that the tasks can form different groups , and the tasks in each group should share similar classification rules ( see Fig 1 for a graphical illustration ) . There are some efforts based on this assumption [ 6 ] , [ 13 ] , however , they are all supervised approaches .
In this paper , we propose a novel semi supervised multitask learning ( SSMTL ) framework , which assumes that we are given a partially labeled set associated with each task . We construct a linear classifier for each task according to the underlying task specific data manifolds . Then all the classification vectors will be integrated together by a K
We further define four concatenated matrices
X = [ X1 , X2 , · · · , Xm ] ∈ Rd×n H = diag(h1 , h2 , · · · , hm ) ∈ Rn×m Y = diag(y1 , y2 , · · · , ym ) ∈ Rn×m G = diag(G1 , G2 , · · · , Gm ) ∈ Rn×n J = diag(J1 , J2 , · · · , Jm ) ∈ Rn×n
( 6 ) ( 7 ) ( 8 ) ( 9 ) ( 10 ) where Gt = X⊤ have t Xt is the Gram matrix for task t . Then we
W = XH
Jloss = kY − JGHk2 F
( 11 ) ( 12 )
A . Intra Task Regularization
In our Semi Supervised Multi Task Learning ( SSMTL ) framework , we propose to explore both the intra task and inter task information to help determine the classification function . In this section we will introduce how to explore the intra task information .
Specifically , within each task t , we first want the classification function to have a better generalization ability , which motivates us to punish the norm of wt , since it relates to the reciprocal of the classification margin [ 20 ] . Thus we can adopt the following regularization term to penalize the norm kXthtk2 = tr(H⊤G⊤H )
( 13 )
C =Xt kwtk2 =Xt means like inter task regularization term which can help explore the task clusters . After some relaxations , we show that our problem is convex and an efficient gradient descent method is derived to solve it . Moreover , we also show that our algorithm can easily be kernelized to nonlinear cases . Finally the experimental results on both synthetic and real world data sets are presented to show the effectiveness of our method .
II . THE ALGORITHM
In the multi task setting , we are given a set of data points i=1 . Each xi is associated with a specific task X = {xi}n t ( t = 1 , 2 , · · · , m ) . We denote by It the set of indices of the data points associated to the task t . For semi supervised multi task learning , we assume that for each task t we have lt labeled points , and we denote the indices of the labeled samples in task t as ILt , the indices of the unlabeled samples we denote its label by yi . For convenience , we assume that in task t as IUt , then It = ILtS IUt . If xi is labeled , ∀i ∈St ILt , yi ∈ {+1 , −1} , and for each task we can learn t x , such that the predicted label a linear classifier ft(x ) = w⊤ of x in task t can be determined by sign(f ( x) ) . We denote by W = [ w1 , w2 , · · · , wm ] ∈ Rd×m the matrix whose columns are the successive vectors we want to estimate .
We use square loss to measure the prediction quality of the linear classifiers , ie , for task t , the prediction loss would be
J t loss = Xi∈ILt
( w⊤ t xi − yi)2
Then the total prediction loss becomes
Jloss =Xt Xi∈ILt
( w⊤ t xi − yi)2
We define the data matrix associated with the t th task as Xt ∈ Rd×nt . From the representor theorem [ 17 ] , we can write wt as a linear combination of the data points in task t , ie , wt =Xi∈It ht ixi = Xtht
( 3 )
1 , ht where ht = [ ht nt]T is the combination coefficient vector . Define the label truncated identity matrix for task t as Jt ∈ Rnt×nt , which is a diagonal matrix with
2 , · · · , ht
Jii =fl 1 , if i ∈ ILt 0 , otherwise
( 4 ) and the initial label vector for task t as yt ∈ Rnt×1 with yt(i ) =fl yi ,
0 , if i ∈ ILt otherwise
( 5 )
( 1 )
( 2 )
Secondly , we want that for each task , the predicted labels should be sufficiently smooth with respect to its corresponding intrinsic data manifold . According to [ 7 ] , such smoothness can be measured by
St = w⊤ t XtLtX⊤ t wt
( 14 ) where Lt ∈ Rnt×nt is the graph Laplacian constructed for task t , whose ( i , j) th entry is [ 10 ]
Lt(i , j ) =fl −sij , if i 6= j dii − sii , otherwise where sij is the similarity between the i th and j th data points associated with the t th task , which can usually be computed by the following Gaussian function
( 15 )
( 16 ) sij = exp − kxt j k2 i − xt 2σ2 t
! where we use xt i to represent the i th data point in task t . Therefore the total smoothness of the classification functions can be measured by t XtLtX⊤ t wt w⊤
S = Xt St =Xt = Xt = trH⊤GLGH t GtLtGtht h⊤
( 17 ) where
L = diag(L1 , L2 , · · · , Lm ) ∈ Rn×n
( 18 ) is the concatenated Laplacian matrix .
Combining Eq ( 13 ) and Eq ( 17 ) we can construct a intra task loss as
Jintra = trH⊤GH + δtrH⊤GLGH
= trH⊤ ( G + δGLG ) H
B . Inter Task Regularization
( 19 )
Besides exploring the information within each task , another important issue is how to explore the task relationships in multi task learning . Some previous works assume that the inter task relationships are known beforehand [ 18][15 ] . In this paper , we will not make such assumption and we will design a paradigm to explore such information automatically . Specifically , we assume there are hidden clusters among those tasks , and the tasks in similar clusters should share similar weight vectors . Using the k means criterion , assuming the tasks can formulate k clusters , we can write the objective
Jinter = kXi=1 XTt∈πi kwt −ewik2 where Tt represents the t th task , πi denotes the i th task cluster . cluster . ewi is the mean classification vector of the i th task
Now we introduce an m × k task cluster indicator matrix
E with its ( i , j) th entry
( 20 )
( 21 )
( 22 )
Let if Ti ∈ πj 0 , otherwise
Eij =fl 1 , M = I − EE⊤E−1
E⊤
Then Eq ( 20 ) can be rewritten as
Jinter = tr(WMW⊤ ) = tr(XHMH⊤X⊤ )
( 23 )
C . Semi Supervised Multi Task Learning with Task Regularizations
Combining all things together , we can derive the following objective for semi supervised multi task learning :
J = Jloss + λ1Jintra + λ2Jinter
= kY − JGHk2
F + λ1trH⊤ ( G + δGLG ) H
( 24 )
+λ2tr(XHMH⊤X⊤ ) restrict the non diagonal elements of H to be zeros , which are thus linear and will not effect the convexity of the problem of minimizing J .
• The constraint on M is a bit more complicated because of its special structure as defined in Eq ( 22 ) and the integer values in E ( see Eq ( 21) ) . To relax it , we observe that ( 1 ) M is idempotent ; ( 2 ) M is symmetric . Therefore M is an orthogonal projector , ie , M and I − M are all positive semi definite ( PSD ) . Moreover , from its definition we can easily see that trM = m−k . Thus we relax M to lie in the following set
M = {M : 0 M I , trM = m − k}
( 25 ) where we use 0 M I to denote that both M and I − M are PSD .
Combining all above things together , we can derive that our semi supervised multi task learning framework aims to solve the following problem minH,M kY − JGHk2
+λ2tr(XHMH⊤X⊤ ) H ∈ H , M ∈ M st
F + λ1trH⊤ ( G + δGLG ) H
( 26 ) where we use H to denote the set of block diagonal matrices taking the form as in Eq ( 7 ) . In this way , we got a problem that is convex in both H and M , in the following section we will introduce an efficient optimization method to solve it .
D . Optimization
A most straightforward way to solve problem ( 26 ) is alternative optimization , which iteratively solves M ( or H ) with H ( or M ) fixed . Specifically , when M is fixed as M = M∗ , we should solve the optimal H by minH J = kY − JGHk2
+λ2tr(XHM∗H⊤X⊤ )
F + λ1trH⊤ ( G + δGLG ) H st H ∈ H
( 27 )
First we ignoring the constraint on H , then since J is convex in H , then the optimal H can be obtained at the zero point of the following gradient
= 2(λ1G + G(J + λ1δL)G)H − GY + λ2X⊤XHM∗
∂J ∂H By setting ∂J /∂H = 0 , we can get that
( λ1G + G(J + λ1δL)G)H + λ2X⊤XHM∗ = GY ( 28 ) which is convex with respect to both H and M . Now we will analyze the constraints on H and M .
• According to the definition of H in Eq ( 7 ) , there are no specific constraints on H except its block diagonal structure . Such structure constraints are equivalent to
This is a generalized Sylvester equation which can be solved with O(n + m)3 [ 14 ] , which is prohibitive when n and m become large . Therefore we give up such idea and apply the simple gradient descent to solve the optimal H for problem ( 26 ) .
Starting with an initial point H0 , the gradient descent
E . Kernelization method successively updates H by
In this section , we will present the kernelized form of our algorithm . Specifically , we assume that there are some nonlinear map φ : Rd → F that maps the data points from the space Rd to some high dimensional ( possibly infinite ) feature space F , and then we will run our task regularization method in F . Let
Φt = [ φ(xt
1 ) , φ(xt
2 ) , · · · , φ(xt nt ) ]
( 38 ) where xt i is the i th data point in the t th task . Then from the representor theorem we know that the classification vector for task t can be written as wφ
( 39 ) t = Φtαt where αt ∈ Rnt×1 is the combination coefficient vector for task t . We denote Kt as the kernel matrix for task t with
Kt(i , j ) = φ(xt i)⊤φ(xt j ) and define two concatenated matrices
φ φ m ) ∈ Rn×m Aφ = diag(α φ 1 , α 2 , · · · , α K = diag(K1 , K2 , · · · , Km ) ∈ Rn×n
Then
F
J φ loss = kY − JKAφk2 J φ intra = tr(Aφ)⊤(K + δKLK)Aφ inter = trΦAφM(Aφ)⊤Φ⊤
J φ where Y ∈ Rn×m , J ∈ Rn×n , L ∈ Rn×n are defined in Eq ( 8 ) , Eq ( 10 ) and Eq ( 18 ) respectively , and Φ = [ Φ1 , Φ2 , · · · , Φm ] is the concatenated data matrix in the feature space . However , in general we cannot compute Φ explicitly . Therefore we can transform J φ inter to
J φ inter = trΦAφM(Aφ)⊤Φ⊤ = trAφM(Aφ)⊤Φ⊤Φ We can define the composite kernel matrix eK ∈ Rn×n as eK =
K1 K12 K21 K2 Km1 Km2
· · · K1m · · · K2m · · · Km
 where Kij ∈ Rni×nj = Φ⊤ i Φj is the kernel matrix between the data points in task i and j . We can perform Choleskey decomposition to K as
K = FF⊤ with F ∈ Rn×n , then
J φ inter = trF⊤AφM(Aφ)⊤F
In this way , we can minimize J = Jloss + λ1Jintra + λ2Jinter to get the optimal Aφ by the same approach as in section II D .
( 40 )
( 41 ) ( 42 )
( 43 )
( 44 )
( 45 )
( 46 )
( 47 )
( 48 )
∂J
∂HfifififiH=Ht−1
Ht = Ht−1 + α
( 29 ) where the superscripts correspond to the numbers of iteration steps , α is the step size , and ∂J /∂H can be computed by Eq ( 28)1 , where M∗ should be computed by
M∗ = arg min M∈M tr(XHMH⊤X⊤ )
( 30 )
Considering the eigenvalue decomposition of M :
M = ΠΛΠ⊤
( 31 ) where Λ = diag(λ1 , λ2 , · · · , λm ) ∈ Rm×m is a diagonal matrix with λi ’s the eigenvalues of M . Then minM∈M tr(XHMH⊤X⊤ )
( 32 )
= min0≤λi≤1,P i λi=m−k,Π∈Om tr(XHVΛV⊤H⊤X⊤ ) where Om is a set of orthogonal matrices in Rm×m . We further consider the singular value decomposition ( SVD ) of XH :
XH = UΣV⊤
( 33 )
Then tr(XHMH⊤X⊤ ) = tr(UΣV⊤ΠΛΠ⊤VΣU ⊤ )
( 34 ) when Π = V , then tr(XHMH⊤X⊤ ) = tr(UΣΛΣU⊤ ) =
λiσ2 i
( 35 ) mXi=1 mXi=1
Therefore min M∈M tr(XHMH⊤X⊤ ) = min 0≤λi≤1 ,
λiσ2 i
( 36 )
P i λi=m−k
Then problem ( 30 ) becomes an eigenvalue optimization problem as i=1 st min{λi} Xm Xm i=1
λiσ2 i
( 37 )
∀ i = 1 , · · · , m , 0 ≤ λi ≤ 1
λi = m − k which is a linear programming problem that can be solved efficiently . After we get the optimal λi ’s , we can get M∗ and proceeds with the gradient calculation . Note that since H has a special block diagonal structure , we only need to update the non zero entries during the gradient descent process .
1In fact , we should compute ∇HJ = ∂HJ + ∂MJ ∂HM , however , the constraints on M makes J non differentiable on M , thus we use ∂HJ instead .
III . EXPERIMENTS
In this section , we will introduce a set of experimental results to show the effectiveness of our method .
A . A Toy Example
We first construct a toy example to evaluate our method . There are totally 6 tasks with binary classes . The data from each class was generated from a Gaussian mixture model ( GMM ) . For Tasks 1 , 2 , and 3 the GMM parameters for class one ( denoted with a blue star ) are drawn from a three component mixture defined as follows . Mixture weights ( three components ) : ( 0.3 , 0.3 , 0.4 ) ; respective two dimensional means : ( 1 , 1 ) , ( 3 , 3 ) and ( 5 , 5 ) ; and re spective covariance matrices Σ1 = 0.3 0.7 3.0 0.0 0.0 3.0 and Σ3 = 3.0 −0.5
0.7 3.0 , Σ2 = 0.3 . Data for class 2 in Tasks 1 , 2 , and 3 are drawn from a single Gaussian with mean ( 2.5 , 1.5 ) and diagonal covariance with symmetric variance 0.5 . For Tasks 4 , 5 and 6 the GMM parameters for class one ( denoted with blue star ) are drawn from a threecomponent mixture defined as follows . The three mixture weights are ( 0.3 , 0.3 , 0.4 ) ; the respective two dimensional means are ( 0.5 , 0.5 ) , ( 3 , 2 ) , and ( 5 , 5 ) and the respective covariances are Σ2 , Σ3 , Σ1 . In Tasks 4 , 5 , 6 the data for class two are drawn from a single Gaussian with mean ( 2 , 3 ) , and diagonal covariance with symmetric variance 05
−0.5 s k s a t x s f i o C U A d e g a r e v a
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
5
10
KSSMTTR MTPNBC LSSMTTR SCMTL STGRF
30
35
15
20
25 number of labeled data
( a ) Averaged AUC vs . number of labeled data s k s a t
1
2
3
4
5
6
Besides our method , we also implemented the MultiTask Parameterized Neighborhood Based Classification ( MTPNBC ) method [ 16 ] , the Supervised Cluster based Multi Task Learning ( SCMTL ) method [ 13 ] and the SingleTask Gaussian Random Field ( STGRF ) [ 21 ] . Note that MTPNBC is a semi supervised multi task learning method with probabilistic classifiers , SCMTL is a supervised multitask learning method , and STGRF is a semi supervised single task learning method . For our Semi Supervised MultiTask Learning with Task Regularization ( SSMTTR ) method , the regularization parameters λ1 and λ2 are all set by 5fold cross validation from the exponential grid 2[−4:1:4 ] . The graph Laplacian for each task is constructed in the same way as in [ 21 ] . For the Kernel SSMTTR ( KSSMTTR ) method , we just use the Gaussian kernel with its width set by 5 fold cross validation from 2[−4:1:4 ] .
Each curve in Figure 2(a ) represents the mean Area Under the ROC Curve ( AUC ) score over 50 independent trials as a function of the number of labeled data , from which we can clearly observe the superiority of our methods . To gain a better understanding of the clustered structure that SSMTTR finds for the six tasks , we plot the Hinton diagram [ 12 ] of the between task sharing matrix found by the Linear SSMTTR ( LSSMTTR ) , averaged over the 50 trials . The ( i , j) th element of sharing matrix is equal to exp(−kwi − wjk2/2 ) , which is represented by a square in the Hinton diagram , with a larger square indicating a larger
1
2
3 tasks
4
5
6
( b ) Hinton diagram
Figure 2 . ( a ) Performance of various multi task learning algorithms on Tasks 1 6 for the data in Figure 1 . The horizontal axis is the number of labeled data in each task . The vertical axis is the AUC averaged over the six tasks and 50 independent trials ( b ) The Hinton diagram of between task sharing found by our linear semi supervised multi task learning method , averaged over 50 independent trials . value of the corresponding element . The Hinton diagram in Figure 2(b ) also shows the agreement of the sharing mechanism of the SSMTTR with the similarity between the tasks .
B . Application in Landmine Detection Based on Airborne Radar Data
We consider the remote sensing problem considered in [ 16 ] , based on data collected from a real landmine field2 . In this problem there are a total of 19 sets of data , collected from various landmine fields by an actual syntheticaperture radar system . Each data point is represented by a 9 dimensional feature vector extracted from the data . The class label is binary ( mine or false mine ) .
2The data set LandmineDatazip is downloaded from http://wwweedukeedu/∼lcarin/
KSSMTTR MTPNBC LSSMTTR SCMTL STGRF
120
140
60 number of labeled points
80
100 s k s a t
9 1 f o C U A d e g a r e v a
0.85
0.8
0.75
0.7
0.65
20
40
( a ) Averaged AUC vs . number of labeled data s k s a t
2
4
6
8
10
12
14
16
18
2
4
6
8
10 tasks
12
14
16
18
( b ) Hinton diagram
Figure 3 . ( a ) Performance of various multi task learning algorithms ; ( b ) The Hinton diagram of between task sharing found by our linear semisupervised multi task learning method , averaged over 100 independent trials .
Each of the 19 data sets defines a task , in which we aim to find landmines with a minimum number of false alarms . Of the 19 data sets , 1 10 are collected at foliated regions and 11 19 are collected at regions that are bare earth or desert . Therefore we expect two dominant clusters of tasks , corresponding to the two different types of ground surface conditions .
We perform 100 independent trials , in each of which we randomly select a subset of data for which labels are assumed available , train the SSMTTR and test the classifiers on the remaining data . The AUC averaged over the 19 tasks is presented in Figure 3(a ) , as a function of the number of labeled data , where each curve represents the mean calculated from the 100 independent trials ( the parameters in our algorithms are also set by cross validation from the same grid as the experiments in last section ) . The results of MTPNBC , SCMTL and STGRF are also reported , which shows that our algorithm outperforms the competitors significantly . The
Hinton diagram of the between task sharing matrix found by the Linear SSMTTR ( LSSMTTR ) , averaged over the 100 trials , is shown in Figure 3(b ) , where the ( i , j) th element of sharing matrix is equal to exp(−kwi − wjk2/2 ) . From the figure we can easily discover the task cluster structure .
IV . CONCLUSIONS
In this paper we propose a novel semi supervised multitask learning algorithm based on task regularizations . Our algorithm assumes that the multiple tasks form several task clusters and the tasks in the same cluster will share similar classification vectors . After relaxations , we show that our algorithm can be cast into a convex optimization problem and can thus be efficiently solved . Finally the experimental results on both toy and real world problems are presented to show the effectiveness of out method .
ACKNOWLEDGEMENT
The work is partially supported by NSF grants NSF grants
DMS 0844513 and CCF 0830659 .
REFERENCES
[ 1 ] G . M . Allenby and P . E . Rossi . Marketing models of Journal of Econometrics , 89(1 consumer heterogeneity . 2):57–78 , 1998 .
[ 2 ] R . K . Ando and T . Zhang . A framework for learning predictive structures from multiple tasks and unlabeled data . Journal of Machine Learning Research , 6:1817–1853 , 2005 .
[ 3 ] A . Argyriou , T . Evgeniou , and M . Pontil . Multi task feature In Advances in Neural Information Processing learning . Systems , pages 41–48 , 2006 .
[ 4 ] A . Argyriou , C . A . Micchelli , M . Pontil , and Y . Ying . A spectral regularization framework for multi task structure learning . In Advances in Neural Information Processing Systems 20 , pages 25–32 . 2008 .
[ 5 ] N . Arora , G . M . Allenby , and J . Ginter . A hierarchical bayes model of primary and secondary demand . Marketing Science , 17(1):29–44 , 1998 .
[ 6 ] B . Bakker and T . Heskes . Task clustering and gating for bayesian multitask learning . Journal of Machine Learning Research , 4:83–89 , 2003 .
[ 7 ] M . Belkin , P . Niyogi , and V . Sindhwani . Manifold regularization : A geometric framework for learning from labeled and unlabeled examples . Journal of Machine Learning Research , 7:2399–2434 , 2006 .
[ 8 ] O . Chapelle , B . Sch¨olkopf , and A . Zien , editors .
Semi
Supervised Learning . MIT Press , Cambridge , MA , 2006 .
[ 9 ] R . Chari , W . W . Lockwood , B . P . Coe , A . Chu , D . Macey , A . Thomson , J . J . Davies , C . Macaulay , and W . L . Lam . Sigma : A system for integrative genomic microarray analysis of cancer genomes . BMC Genomics , 7:324 , 2006 .
[ 10 ] F . R . K . Chung . Spectral Graph Theory ( CBMS Regional Conference Series in Mathematics , No . 92 ) . American Mathematical Society , 1997 .
[ 11 ] T . Evgeniou and M . Pontil . Regularized multi task learning . In Proceedings of the 10th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 109–117 , 2004 .
[ 12 ] G . E . Hinton and T . J . Sejnowski . Learning and relearning in boltzmann machines . Parallel distributed processing : explorations in the microstructure of cognition , 1:282–317 , 1986 .
[ 13 ] L . Jacob , F . Bach , and J P Vert . Clustered multi task In Advances in Neural learning : A convex formulation . Information Processing Systems 21 , 2008 .
[ 14 ] B . K˚agstr¨om and P . Poromaa . Lapack style algorithms and software for solving the generalized sylvester equation and estimating the separation between regular matrix pairs . ACM Trans . Math . Software , 22:78–103 , 1996 .
[ 15 ] T . Kato , H . Kashima , M . Sugiyama , and K . Asai . Multi task In Advances in NIPS 20 , learning via conic programming . pages 737–744 , 2008 .
[ 16 ] Q . Liu , X . Liao , and L . Carin . Semi supervised multitask learning . In NIPS 20 , pages 937–944 . 2008 .
[ 17 ] B . Sch¨olkopf and A . J . Smola .
Learning with Kernels : Support Vector Machines , Regularization , Optimization , and Beyond . MIT Press , Cambridge , MA , USA , 2001 .
[ 18 ] D . Sheldon . Graphical multi task learning . In NIPS Workshop on Structured Input Structured Output , 2008 .
[ 19 ] A . Torralba , K . P . Murphy , and W . T . Freeman . Sharing features : efficient boosting procedures for multiclass object detection . In Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition , volume 2 , pages 762–769 , 2004 .
[ 20 ] V . N . Vapnik . The Nature of Statistical Learning Theory .
Springer Verlag . , New York , NY , USA , 1995 .
[ 21 ] X . Zhu , Z . Ghahramani , and J . Lafferty . Semi supervised learning using gaussian random fields and harmonic functions . In Proceedings of the International Conference on Machine Learning , pages 912–919 , 2003 .
