The Normalized Compression Distance as a
Distance Measure in Entity Identification
Sebastian Klenk sebastianklenk@visuni stuttgartde
Gunther Heidemann ais@visuni stuttgartde
Dennis Thom
Intelligent Systems Department
Stuttgart University Universit¨atsstrasse 38
70569 Stuttgart , Germany
April 8 , 2010
Abstract
The identification of identical entities accross heterogeneous data sources still involves a large amount of manual processing . This is mainly due to the fact that different sources use different data representations in varying semantic contexts . Up to now entity identification requires either the – often manual – unification of different representations , or alternatively the effort of programming tools with specialized interfaces for each representation type . However , for large and sparse databases , which are common eg for medical data , the manual approach becomes infeasible .
We have developed a widely applicable compression based approach that does not rely on structural or semantical unity . The results we have obtained are promising both in recognition precision and performance .
1
Introduction
The integration of different data sources is an ever increasing obstacle in the process of knowledge discovery . The continuous employment of department wide databases and specialized information systems are fostering the scattering of data . At the same time , the increasing computing power makes a
1 more holistic view on data possible and increases the need for data integration .
Health care is an important field of application which suffers from the scattering of data . In hospitals it is common practice to store patient information in distributed and independent database systems . The information system of the hospital administration stores financial and accounting specific information , the laboratory data is stored in lab databases while radiology employs its own problem specific system . But for data mining and statistical analysis , a “ holistic ” view on patient data for is required , so all relevant information must be gathered and integrated for the different data sources . Therefore records or entities from one data source must be linked to the entities of another source . This task is easy as long as all records share a global identifier ( for example a patient identification number ) , but this is not often the case . As a rule , there is no unique way of identification . Alternative ways to identify identical entities , eg a combination of name , birth date and further information , are usually not present in all of the representations and/or are erraneous . So for a successful approach to entity identification we need a method which is fault tolerant and can cope with missing or mixed up information . Further , because of the high number of records in current databases , the method should be computationally inexpensive to allow for high throughput .
We will present an entity identification method based on the Normalized Compression Distance ( NCD ) similarity measure introduced by Cilibrasi and Vit`anyi Cilibrasi and Vitanyi [ 2005 ] which yields not only good performance but has significant advantages ( high robustness to noise and structural heterogeneity ) compared to currently employed methods .
1.1 Entity Matching
The basic procedure when matching identical entities in different data sources is this : First , identify related fields . This step is usually carried out manually . Then calculate for each pair of related fields a measure of similarity and merge the field specific similarities to a general measure that describes the degree of similarity between the two data entities . Within this process several data structural and lexical issues can occur which make the matching difficult . Problems range from data entrance errors or spelling ambiguities to structural differences , where in one data source a data element is encoded in a single field whereas the other source employs several fields . For example , an address can be stored in a single large text field or it may be spread over separate fields for street , city , country , etc . More formally , these
2 problems can be grouped in three categories : lexical heterogeneity , structural heterogeneity and the lack of data quality .
Current research addresses these problems usually independent of each other . First the structure of the data sets is detected and unified , then identical fields are matched . Already in 1969 , Fellegi and Sunter described the need to identify corresponding ” blocks ” and work on these Fellegi and Sunter [ 1969 ] . The overview given by Winkler Winkler [ 2006 ] proposes a related approach , Elmagarmid et . al . Elmagarmid et al . [ 2007 ] focus solely on lexical heterogeneity , similar to Hern´andez and Stolfo Hern´andez and Stolfo [ 1998 ] . Kimball states that 70 % of the work on a data warehouse project goes into the ETL process Kimball and Caserta [ 2004 ] . Much of this work is due to the manual part of entity identification . By contrast , our approach based on the Normalized Compression Distance does not require any manual processing , in particular , the tedious work of field identification can safely be skipped .
For the actual matching of records , most current approaches are either learning based Sarawagi and Bhamidipaty [ 2002 ] , Zhao and Ram [ 2005 ] , Zhao [ 2007 ] , Zhao and Ram [ 2008 ] , Bilenko and Mooney [ 2003 ] or rule based Hern´andez and Stolfo [ 1998 ] , Elmagarmid et al . [ 2007 ] , Navarro [ 2001 ] . Both methods suffer from different kinds of drawbacks . The learning approach requires training data which usually must be prepared manually under great effort . When developing or selecting training data it is of great importance to cover as many true positive cases as possible , because a classification method that is biased towards a certain decision would be of little help . Interestingly , a much greater challenge seems to be the selection of good ” non matching ” samples . Sarawagi and Bhamidipaty circumvent this problem by their active learning procedure that actively generates the training set by presenting samples to the user Sarawagi and Bhamidipaty [ 2002 ] . Distance based methods in general require no or little training ( in case of weighting issues ) . The threshold selection , which we will discuss in more detail later , can be done with the given data . Rule based methods require a strong knowledge of the structure of the data and are not as robust to noise as learning based methods Elmagarmid et al . [ 2007 ] . The method we propose , which is based on the Normalized Compression Distance , is robust to noise as was shown by Cebrian et . al . Cebrian et al . [ 2007 ] , requires no previous learning procedures and is well suited as a distance measure for clustering , classification and similarity matching tasks Heidemann and Ritter [ 2008 ] , Cilibrasi and Vitanyi [ 2005 ] , Amitay et al . [ 2007 ] . The broad range of classification subjects that are covered by almost identical procedures also shows that there is no need for the inclusion of domain specific
3 knowledge which makes this procedure a good candidate for areas where only little is known about the domain itself .
2 Similarity Measurement by Compression
Several concepts for the measurement of string or file similarity by means of compression algorithms have been proposed so far . The one with the most profound theoretical fundament is however the Normalized Compression Distance , formulated by Rudi Cilibrasi and Paul Vit`anyi Cilibrasi and Vitanyi [ 2005 ] .
Although there are many known concepts and methods , that give an operationalization of the resemblance of two given data pieces with specific content , such as image , text or audio files , most of them are highly specialized , complex and work on high level aspects of the data . The idea behind the compression distance is however , to measure closeness of any given files or strings unregarded of their specific content or structure by means of fast and simple algorithms , that work on a very low level . Even though it first seems arbitrary to attend to compression algorithms in means of computing a similarity distance , the astonishing results , that already have been made with the approach , encourage this choice Heidemann and Ritter [ 2008 ] , Cilibrasi and Vitanyi [ 2005 ] , Amitay et al . [ 2007 ] , Casey et al . [ 2008 ] , Alfonseca et al . [ 2006 ] . Furthermore there are several reasons to assume , that compression takes a vital part in the challenge of generalized pattern recognition .
Information Distance
2.1 We define the Kolmogorov complexity K(x ) of a string x ∈ Σ∗ as the length of a shortest Turing machine program , that , given the empty string ǫ as input , generates x on its output tape and then halts . Even though the function , that maps strings x to their complexity K(x ) is evidently not computable . The Kolmogorov complexity can be considered as an idealized measurement of the informational quantity given in a string ( or file ) . Based on this proposal the informational distance of two strings could also be measured by consulting the concept . In doing so , we are talking about Turing machines , that are given the information of one string to resemble the other one . The less different the two strings are , the less complex this task should be and therefore the less the size of the smallest Turing machine , that does the job . To formalize this thought , we define the Normalized
4
Information Distance Li et al . [ 2001 ] of two strings x , y ∈ Σ∗ as
N ID(x , y ) = max {K(x|y ) , K(y|x)} max{K(x ) , K(y)} where K(x|y ) is the conditional Kolmogorov complexity , ie the size of a smallest Turing machine , that , given y as input , generates x on its output tape and then halts . The normalization term max{K(x ) , K(y)} is included to generalize the approach on parameters of variable size and informational quantity . Based on the assumption , that always K(x|y ) ≤ K(x ) the N ID should always take values in [ 0 , 1 ] , with N ID(x , x ) ≈ 0 and N ID(x , ǫ ) ≈ 1 . It can be proven , that the N ID satisfies the metric ( in )equalities . Besides , it has been proven in Li et al . [ 2001 ] , that the N ID minorizes all normalized functions in a wide class of relevant distance functions , meaning , that if two strings are close according to an aspect measured by one of these functions , they are also close according to the N ID .
2.2 Compression Distance
Even if we want to accept the N ID as a generalized measurement for informational distance , there remains the problem of non computability of K(x ) . One obvious real world approximation to the N ID is to see the Turing machines as a maximal possible compression of the string x . Therefore the size of the output C(x ) , that any real world compression algorithm , like LempelZiv or Huffman , produces on input x could be taken to approximate K(x ) in the formula . Furthermore max{K(x|y ) , K(y|x)} could be approximated by C(xy ) − min{C(x ) , C(y)} , where xy is the concatenation of x and y . This way it can be seen how good the compression algorithm can use the information given in one string to better compress the other one and vice versa . By subtraction of min{C(x ) , C(y)} the term approximates the remaining compressed size of the more complex string if the information in the other one was used . Therefore the term should be smaller , the more similar the two strings are . Considered together , we define the Normalized Compression Distance as
N CD(x , y ) =
C(xy ) − min{C(x ) , C(y)} max{C(x ) , C(y)}
Obviously there is no way to prove , that this function approximates , what would be seen as an intuitive distance of file contents , like similarity of pictures or tunes , ie giving an absolute and adequate universal and parameterfree similarity metric . But , it has been proven in Cilibrasi and Vitanyi [ 2005 ] ,
5 that the N CD minorizes relevant distance functions up to an additive term , vanishing with the quality of the approximation of K(x|y ) by the chosen compressor . Besides , as already mentioned , the experimental results , that have been observed with the method , encourage the approach .
3 ETL and Duplicate Detection
Even despite it ’s great age , entity identification , especially in the context of data integration and the Extract Transform Load ( ETL ) process , still is a very active field of research . This can be seen by the great number of recent publications Zhao [ 2007 ] , Zhao and Ram [ 2008 ] , Goiser and Christen [ 2006 ] , Christen [ 2007 ] , Yan et al . [ 2007 ] but also by the books that cover this subject or address related issues Han and Kamber [ 2001 ] , Kimball and Caserta [ 2004 ] . In this section we will present the basic procedures that occur during the ETL process and how the use of the Normalized Compression Distance improves the entire process .
3.1 Data Integration
When it comes to integrating data from different sources there are a number of difficulties that can arise . Biomedical data is a particularly good example . Here structural and lexical elements as well as data quality ( which can be attributed to lexical heterogeneity ) differ to a large degree from one data source to an other . Practically speaking this means that similar data is neither stored in a common structure ( ie in some sources the data is spread over multiple fields whereas in others it is stored in a single one ) nor is it encoded with similar values . These differences can range from different character sets , for example plain ASCII as opposed to UTF 8 where special characters like ¨a or the like will be coded as ae , complex abbreviations where 5th Street , New York could be written as 5th St NY . Detecting these inter field similarities is usually a computationally expensive task and as Christen Christen [ 2007 ] points out the bottleneck of a record linkage system . Commonly used techniques like string matching algorithms require usually O(|σ1| · |σ2| ) comparisons for two strings σ{1|2} Elmagarmid et al . [ 2007 ] . Learning based methods can improve this number but involve training which is in itself an expensive task .
Besides record comparison being expensive , the number of necessary calculation for the whole data set can also become prohibitively large . For two sets A and B an exhaustive search for similar records would require |A| · |B| comparisons which would amount to 1010 for |A| = |B| = 100000
6
Figure 1 : The standard entity matching procedure where the cleaning comes before the matching
Figure 2 : The NCD based entity matching procedure where the matching can be done one the noisy data and the cleaning can use the extra information of entity similarity
One approach to reduce this number is blocking . Thereby the data sets will be grouped in similar blocks depending of the value of certain fields . For example the zip code of the address field could be used to assign records into blocks with identical value . Now the comparison can take place only in blocks which share the same zip code . This of course requires that the field used for blocking is free of errors . A more advanced version of this method is canopies based clustering where very simple distance measures are used to efficiently calculate a number of overlapping clusters that each , in a second step , will be clustered with the detailed and expensive distance measure . This method is , opposed to the blocking scheme , tolerant to noise as the clusters overlap McCallum et al . [ 2000 ] .
3.2 Procedure
The comparison of two strings based on the Normalized Compression Distance can be implemented as a linear algorithm Rodeh et al . [ 1981 ] which improves significantly on the above mentioned values . Even though it is not sufficient to come around blocking or clustering it can increase the performance of a system significantly . Here we will describe an entity identification procedure that employs NCD as a distance measure .
The standard procedure as described by Christen and Goiser Christen and Goiser [ 2007 ] and presented in figure 1 places the cleaning process before blocking and comparison . This is necessary because most string matching and dissimilarity measures are highly domain specific and therefor can only be employed to certain fields . These need to be known in advance and , in case of structural heterogeneity , joined or split to fit the other data sources . In the light of data cleaning this has several drawbacks . First the knowledge about similar records , and thereby about identical values , can not be used
7 during cleaning . Second a preceding data scrubbing might pose the risk of worsening the identification procedure by introducing new , and possibly more severe , errors .
The NCD based approach we propose follows the procedure presented in figure 2 . Here clustering , as the first step , is required for performance reasons on large databases . The second step is already the record comparison . Therefor , for two records ρi and ρj , all fields ( or a relevant subset depending on the size of the record ) of each record are concatenated to form two large string σi and σj . As we will see in section 4 the order of the fields in the string is largely irrelevant . If for both records only a subset of fields is used , the two subsets should have some overlap . The two resulting strings are now compared by the NCD to get the degree N CD(σi , σj ) = dij of similarity between them . Now the knowledge gained about the similarity of two strings can be used to improve the cleaning of the fields . If one knows that fields contain similar values it is much easier to correct possible mistakes than if one is left to guess whether the values differ on purpose or not .
To decide whether two strings are similar enough to represent the same entity is usually being done by comparison with a given threshold t . Is the value of the similarity measure N CD(σi , σj ) < t is less than the threshold , both strings are considered matching . The difficult part is how to chose a t which is small enough to ignore similar , but not identical , entities but also large enough to not miss actual true hits . In terms of information retrieval this can be described as to find an equilibrium between recall and precision Baeza Yates and Ribeiro Neto [ 1999 ] .
Of great help in this situation is that identical entities usually are significantly closer to each other than to all others . This means that the actual neighborhood of an entity – those other entities that are close to it – is sparse . Statistically this can be described as distances that fall below a significance or confidence level Feller [ 1950 ] . Once one knows the statistical properties ( mean µ and variance ν ) of the distances it is easy to calculate a good threshold given the appropriate value c = Φ−1(α ) of the distribution quantile function where α specifies the confidence region . Figure 3 shows the distances of one entity to all matching candidates , the mean value and the confidence regions . There also the spares neighborhood ( the small number of deep spikes ) can be seen very good . t = µ − c√ν
8
5 9 . 0
0 9 . 0
] d n i
, [
X
5 8 . 0
0
50
100
150
200
250
Index
Figure 3 : The distances with mean value and confidence regions for a large noise level of α = 0.6
4 Application
The main intention when developing this method was a fast and easy algorithm which could assist in finding duplicates and identical entities in unstructured , noisy and heterogeneous data . One of the central applications we had in mind was the integration of several large medical databases stored at different locations . These contain patient data stored in various formats , ranging from Microsoft Excel spread sheets to relational and object oriented databases . Most of these systems don’t share a common identifier and the content and structure of the data varies significantly .
4.1 Experiments
To get an impression of the usefulness of the Normalized Compression Distance for this task we prepared two test cases . One which allowed us to vary parameters like record structure or noise level in a controlled environment and a second one which represents the actual task at hand , namely the detection of entities across different tables .
The first test case consists of 241 employee records from the computer science department at the Stuttgart University . The records contains fields like name , room , telephone number and institute . The data is clearly structured and free from syntactic ambiguities . Because of the simple structure
9
0 . 1
8 . 0
6 . 0
4 . 0
2 . 0
0
.
0 e t a r r o r r
E
0.0
0.2
0.4
0.6
0.8
Noise level
Figure 4 : The evolvement of the error rate with increasing noise it is easy to see the influence of changes in structure like a flip of two fields or how noisy data changes the result set . We will therefor use this data set to manually add noise and test the influence thereof .
The second test consisted of two tables . One taken from a tumor center of a Stuttgart hospital ( slightly more than 6000 entities ) and the other one from the local cancer registry ( about 600 entities ) . Both represent data of practicing physicians like name , address or further contact details . They are very heterogenous in structure and don’t share a common identifier . Because data was inserted separately most fields also don’t share common representation schemes such that telephone numbers or street addresses are represented differently even within the same table .
411 Employee Data
In our tests we took the employee data base as reference set and compared it to the same data which was also augmented with various changes in structure and content . First we began to increase the noise level of the data in comparison . This means that , given a noise level α we draw for each letter a random number r ∈ [ 0 , 1 ] and checked whether it was smaller than α . If this was the case we replaced the letter by a randomly chosen letter . For a noise level of α = 0.5 each letter would be replaced with a 50 − 50 chance by a random letter which means that only about half of the string matches the actual string . To see the influence of this we compared for each noise
10 l l
/ a c e R n o s c e r P i i
0 1
.
8
.
0
6 0
.
4 0
.
2
.
0
0 0
. l l
/ a c e R n o s c e r P i i
0 1
.
8
.
0
6
.
0
4
.
0
2 0
.
0
.
0
0.0
0.2
0.4
0.6
0.8
1.0
Noise level
0.0
0.2
0.4
0.6
0.8
1.0
Noise level
Figure 5 : The evolvement of precision and recall with increasing noise
Figure 6 : The evolvement of precision and recall with increasing noise for a matching string with field switches level if the best match actually is the identical string . The results of this first evaluation are plotted in figure 4 ( the solid line ) . Because we are not only interested in the best match but also in all matches we calculated for each noise level the recall R and precision P
R = rm rr and
P = rm m where m is the number of matched entities , rm the number of relevant matched entities and rr all relevant entities . Figure 5 shows how precision and recall decrease with increasing noise .
One major advantage of the Normalized Compression Distance is the resistance to structural changes . To see how the NCD masters the task of identity identification we started flipping and changing field position within the matching string to see how error , precision and recall change by increasing structural heterogeneity . So we repeated the test scenario described above but now we switched the position of fields in the matching string , the reference data was left unchanged . Figure 6 shows the influence of two field switches on precision and recall . In 4 also the error levels for the structural changes are plotted . The solid line represents the error for no structural change , the dashed line involves one field flip and the double dashed line shows the error rate for two field changes . It can be seen that there is only a very small increase in the error rate which is quite interesting if one considers the large change such a field flip poses for the matching string .
11
412 Physician Data
The second data set consists of contact information from physicians in the area of Stuttgart . The two tables , one taken from a tumor center the other one from a cancer registry , represent two non fully overlapping sets . For evaluation we prepared one test set and one reference set . The test set represents the first 100 elements out of the intersection of both sets , taken from the cancer registry . The reference set is the unaltered data from the tumor center . The task was then to find the corresponding element in the reference set that belonged to the data in the test set . This set up was chosen such that it is possible to count the correctly matching entities without having to handle not existing entities ( these would come from the difference of the two sets ) and above all to still be able to interpret the results by visual inspection .
The structure of the data tables is highly heterogeneous . The cancer registry table consists of the following fields in the exact order : Code , CID , Salutation , Name , Field of Work , Street , Town , Status , Family Physician The fields of the tumor center table are : TID , Date edit , User edit , ID SAP , Zip , Country , Town , Title , Street , Name , Salutation , GivenName , Tel , Fax , ID int ,
As can be seen the two tables not only contain a different ordering , there are also fields that share similar content . The field Name from the cancer registry contains the fields Title , Name and GivenName form the tumor center data . To make things worse these fields don’t have to be in this order or can contain further information . There are for example physician that have a foreign title which augments the , in Germany , commonly used Dr . med . or the title is positioned after the given name ( Mr . XXXX YYYY ( Dr . med ) ) Besides all that there are also differences like short names or different separations . Examples are :
• Dres . med . Maier/M¨uller/Schultze vs . D . Schultze & C . M¨uller • CA PD Dr . med . Andrea M¨uller vs . Prof . Andrea Mueller • Dr. Medic/IM Temeschburg Erich Maria Schultze vs . Herr Erich Maria
Schultze
To handle difficulties like these in a classical setting leads to a large deal of manual work . With our method it is possible to reduce the degree
12 e c n a t s D i
5 9 0
.
0 9
.
0
5 8
.
0
0 8 0
. e c n a t s D i
5 9
.
0
0 9 0
.
5 8 0
.
0 8 0
. m : 166
0
1000
2000
3000
Entity
4000
5000
6000
0
1000
2000
3000
Entity
4000
5000
6000 m : 1375
Figure 7 : The distances of the reference data to a select record in the test data set . of human involvement to a minimum . The Idea here is to let the NCD calculate the differences and estimate the best fit ( see Figures 412 ) If this is a miss match still manual labor is necessary but in majority of cases it is not .
In our setting we tried to match the first 100 entities of the test set that existed also in the reference set . There was no preprocessing of any kind . We have only compared the raw data coming from the data tables . Our results showed more that 80 % of the entities were correctly identified . Compared to the number of preprocessing steps needed to acchive similar matching results this is an impressively high number .
5 Conclusions
In this paper we have introduced and evaluated a procedure to match entities with the Normalized Compression Distance . We have presented the rates of error , precision and recall for varying noise and structural heterogeneity levels . The results show that the NCD is capable of correctly identifying almost all of the identical entities , even when confronted with a large level of noise .
We also have tested the NCD based procedure on real world data with a very high level of structural and syntactical heterogeneity . Here the method allows for a significant degree of reduction in manual work when considered with other commonly used methods .
Our tests show that the use of the NCD is especially interesting if one considers its resistance to structural and syntactical changes and its linear
13 computational complexity . This can be of great help in situations where large and unstructured data sets , as is the common case for biomedical or genome related data , must be identified and matched . Especially when it comes to first processing steps the proposed procedure allows for a first review of the data without much hands on labor .
As for further work we are currently working on an integrative software that will be applied to such task and we are positive that the obtained results will ease the work of integration unknown and heterogenous data sources .
References
Manuel Alfonseca , Manuel Cebri´an , and Alfonso Ortega . Testing genetic algorithm recombination strategies and the normalized compression distance for computer generated music . In AIKED’06 : Proceedings of the 5th WSEAS International Conference on Artificial Intelligence , Knowledge Engineering and Data Bases , pages 53–58 , Stevens Point , Wisconsin , USA , 2006 . World Scientific and Engineering Academy and Society ( WSEAS ) . ISBN 111 2222 33 9 .
Einat Amitay , Sivan Yogev , and Elad Yom Tov . Serial sharers : Detecting split identities of web authors . In Benno Stein , Moshe Koppel , and Efstathios Stamatatos , editors , PAN , volume 276 of CEUR Workshop Proceedings . CEUR WS.org , 2007 .
Ricardo Baeza Yates and Berthier Ribeiro Neto . Modern Information Re trieval . Addison Wesley , May 1999 . ISBN 020139829X .
Mikhail Bilenko and Raymond J . Mooney . Adaptive duplicate detection using learnable string similarity measures . In KDD ’03 : Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining , pages 39–48 , New York , NY , USA , 2003 . ACM . ISBN 1 58113 737 0 .
MA Casey , R . Veltkamp , M . Goto , M . Leman , C . Rhodes , and M . Slaney . Content based music information retrieval : Current directions and future challenges . Proceedings of the IEEE , 96(4):668–696 , April 2008 . ISSN 0018 9219 .
M . Cebrian , M . Alfonseca , and A . Ortega . The normalized compression distance is resistant to noise . Information Theory , IEEE Transactions on , 53(5):1895–1900 , May 2007 . ISSN 0018 9448 . doi : 101109/TIT2007 894669 .
14
Peter Christen . A two step classification approach to unsupervised record linkage . In AusDM ’07 : Proceedings of the sixth Australasian conference on Data mining and analytics , pages 111–119 , Darlinghurst , Australia , Australia , 2007 . Australian Computer Society , Inc . ISBN 978 1 92068251 4 .
Peter Christen and Karl Goiser . Quality and complexity measures for data linkage and deduplication . In Fabrice Guillet and Howard J . Hamilton , editors , Quality Measures in Data Mining , volume 43 of Studies in Computational Intelligence , pages 127–151 . Springer , 2007 .
Rudi Cilibrasi and Paul Vitanyi . Clustering by compression . IEEE Trans actions on Information Theory , 51(4 ) , 2005 .
AK Elmagarmid , PG Ipeirotis , and VS Verykios . Duplicate record detection : A survey . Knowledge and Data Engineering , IEEE Transactions on , 19(1):1–16 , Jan . 2007 . ISSN 1041 4347 . doi : 101109/TKDE2007250581
Ivan P . Fellegi and Alan B . Sunter . A theory for record linkage . Journal of the American Statistical Association , 64(328):1183–1210 , 1969 . ISSN 01621459 . URL http://wwwjstororg/stable/2286061
William Feller . An introduction to probability theory and its applications , volume 1 . Wiley , 1950 .
Karl Goiser and Peter Christen . Towards automated record linkage .
In AusDM ’06 : Proceedings of the fifth Australasian conference on Data mining and analystics , pages 23–31 , Darlinghurst , Australia , Australia , 2006 . Australian Computer Society , Inc . ISBN 1 920682 41 4 .
Jiawei Han and Micheline Kamber . Data mining . Morgan Kaufmann Publ . ,
2001 . ISBN 1 55860 489 8 .
G . Heidemann and H . Ritter . On the Contribution of Compression to Visual Pattern Recognition . In Proc . 3rd Int’l Conf . on Comp . Vision Theory and Applications , volume 2 , pages 83–89 , Funchal , Madeira Portugal , 2008 .
Mauricio A . Hern´andez and Salvatore J . Stolfo . Real world data is dirty : Data cleansing and the merge/purge problem . Data Min . Knowl . Discov . , 2(1):9–37 , 1998 . ISSN 1384 5810 . doi : http://dxdoiorg/101023/A : 1009761603038 .
15
Ralph Kimball and Joe Caserta . The Data Warehouse ETL Toolkit : Practical Techniques for Extracting , Cleanin . John Wiley & Sons , 2004 . ISBN 0764567578 .
Ming Li , Xin Chen , Xin Li , Bin Ma , and Paul Vitanyi . The similarity metric , 2001 . URL http://wwwcitebaseorg/abstract?id=oai:arXivorg : cs/0111054 .
Andrew McCallum , Kamal Nigam , and Lyle H . Ungar . Efficient clustering of high dimensional data sets with application to reference matching . In KDD ’00 : Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining , pages 169– 178 , New York , NY , USA , 2000 . ACM . doi : http://doiacmorg/101145/347090347123
ISBN 1 58113 233 6 .
Gonzalo Navarro . A guided tour to approximate string matching . ACM
Comput . Surv . , 33(1):31–88 , 2001 . ISSN 0360 0300 .
Michael Rodeh , Vaughan R . Pratt , and Shimon Even . Linear algorithm for data compression via string matching . J . ACM , 28(1):16–24 , 1981 . ISSN 0004 5411 . doi : http://doiacmorg/101145/322234322237
Sunita Sarawagi and Anuradha Bhamidipaty . Interactive deduplication using active learning . In KDD ’02 : Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , pages 269–278 , New York , NY , USA , 2002 . ACM . ISBN 1 58113 567 X . doi : http://doiacmorg/101145/775047775087
W . E . Winkler . Overview of record linkage and current research directions .
Technical Report RRS2006/02 , US Bureau of the Census , 2006 .
Su Yan , Dongwon Lee , Min Yen Kan , and Lee C . Giles . Adaptive sorted neighborhood methods for efficient record linkage . In JCDL ’07 : Proceedings of the 7th ACM/IEEE CS joint conference on Digital libraries , pages 185–194 , New York , NY , USA , 2007 . ACM . ISBN 978 1 59593 644 8 . doi : http://doiacmorg/101145/12551751255213
Huimin Zhao . Semantic matching across heterogeneous data sources . Commun . ACM , 50(1):45–50 , 2007 . ISSN 0001 0782 . doi : http://doiacmorg/ 101145/11889131188916
Huimin Zhao and Sudha Ram . Entity identification for heterogeneous database integration : a multiple classifier system approach and empirical evaluation . Inf . Syst . , 30(2):119–132 , 2005 . ISSN 0306 4379 .
16
Huimin Zhao and Sudha Ram .
Entity matching across heterogeneous data sources : An approach based on constrained cascade generalization . Data & Knowledge Engineering , 2008 . URL http : //wwwscience directcom/science/article/B6TYX 4SF300D 1/ 1/5ab4808d40aea3bb652437d4492a29ac . Available online 4 May 2008 .
In Press , Corrected Proof ,
17
