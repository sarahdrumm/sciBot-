2009 Ninth IEEE International Conference on Data Mining
Non negative Laplacian Embedding
Dijun Luo , Chris Ding , Heng Huang
Computer Science and Engineering Department
University of Texas at Arlington
Arlington , Texas , 76019 dijunluo@gmailcom , {chqding,heng}@uta.edu
Tao Li
School of Computer Science
Florida International University
Miami,33199 taoli@csfiuedu
Abstract—Laplacian embedding provides a low dimensional representation for a matrix of pairwise similarity data using the eigenvectors of the Laplacian matrix . The true power of Laplacian embedding is that it provides an approximation of the Ratio Cut clustering . However , Ratio Cut clustering requires the solution to be nonnegative . In this paper , we propose a new approach , nonnegative Laplacian embedding , which approximates Ratio Cut clustering in a more direct way than traditional approaches . From the solution of our approach , clustering structures can be read off directly . We also propose an efficient algorithm to optimize the objective function utilized in our approach . Empirical studies on many real world datasets show that our approach leads to more accurate Ratio Cut solution and improves clustering accuracy at the same time .
Keywords Laplacian Embedding ; Non negative Matrix Fac torization ;
I . INTRODUCTION
In many real world tasks in data mining , information retrieval , and machine learning areas , data are represented in high dimensional space , which might intrinsically lie in a very low dimensional one . In addition , many data come in as a matrix of pairwise similarities , such as network data , protein interaction data . Meanwhile , unlabeled data are much easier to be obtained than labeled data . Thus , it is challenging and useful to develop unsupervised approaches to embed high dimensional data into low dimensional one . From the data embedding point of view , there are two categories of embedding approaches . Approaches in the first category are to embed data into a linear space with linear transformation , such as principle component analysis ( PCA ) . These approaches give out robust representations of data in a low dimension ; However , they do not properly embed data which lie on non linear manifold .
The second category approaches embed data in a nonlinear manner . They include IsoMAP [ 25 ] , Local Linear Embedding ( LLE ) [ 22 ] , Local Tangent Space Alignment [ 27 ] , etc . These embeddings have different purposes and objectives . But they can detect the nonlinear manifold where data lie
The above approaches all assume data points are represented by feature vectors ( attributes ) . In this paper , our emphasis is on graph embedding , ie , the relationship among data points are represented by a matrix of pairwise similarities ( which are viewed as edge weights of the graph ) . Laplacian Embedding is one of the most popular graph embedding method .
Laplacian Embedding and the related usage of eigenvectors of graph Laplace matrix is first developed in 1970s . It was called quadratic placement[16 ] of graph nodes in a space . The eigenvectors of graph Laplace matrix are used for graph partitioning and connectivity analysis [ 14 ] . This approach becomes popular in 1990s for circuit layout in VLSI community ( see a review [ 1] ) , and graph partitioning [ 21 ] for domain decomposition , a key problem in distributedmemory computing .
Laplacian Embedding is now very popularly used [ 3 ] mainly due to its relation to graph clustering [ 15 ] , [ 5 ] , [ 24 ] , [ 9 ] . In fact , the eigenvectors of Laplace matrix provides an approximation solution of the Ratio Cut clustering [ 5 ] , and the generalized eigenvectors of the Laplace matrix provides an approximation solution of the Normalized Cut clustering [ 24 ] and min max clustering [ 9 ] . A Difficulty with Eigenvector Embedding
A main difficulty of using eigenvectors of the Laplace matrix to solve multi way clustering problem is that the eigenvectors have mixed sign entries while the clustering indicator vectors ( that these eigenvectors approximate ) are nonnegative . For two way clustering , this is not a problem because a linear Ψ transformation[7 ] of the second eigenvector ( the Fiedler vector ) and the first eigenvector leads to two genuine indicator vectors ( vectors with positive and/or zero entries and each row has only one nonzero entry ) .
Because of this main difficulty , most applications resort to a two step procedure[20 ] : ( 1 ) embedding the graph into the eigenvector space ( Laplace embedding ) and ( 2 ) clustering these embedded points using K means clustering . This procedure provides an approximate solution to the clustering problem . Nonnegative Embedding Provides a Solution
In this paper , we propose a new approach . We propose to perform the Laplace embedding with nonnegative vectors , which can be directly interpreted as cluster membership indicator vectors . As a consequence , the nonnegative embedding also provides a more accurate solution to Ratio Cut
1550 4786/09 $26.00 © 2009 IEEE DOI 101109/ICDM200974
337 clustering problem because the solution indicators now more resembles the desired cluster indicators . We call this new approach , the nonnegative Laplacian Embedding ( NLE ) .
NLE has the following property . It optimizes the Ratio Cut function with enforcing the nonnegative requirements rigorously . With the nonnegative representation of the cluster indicator , the embedding results can be interpreted as the posterior clustering probability . As a result , the cluster membership can be read off from the embedding coordinates immediately ( see §4 ) . Second , our NLE method has soft clustering capability ( see §IX A ) where a data point could be fractionally assigned to several clusters . This capability is especially important for many real life data which come with much noise . For these data , not every data point clearly and uniquely belongs to one cluster ( pattern ) . The soft clustering capability is lacking in standard spectral clustering and Kmeans clustering .
Our approach requires a solver which optimizes a quadratic function with both orthonormal and nonnegative constraints . The feasible domain of such optimization problem is highly non linear and non convex . In this paper , we also propose an efficient algorithm to address this problem . In the remainder of the paper , we first transform the minimization problems of the embedding ( §2 ) and Ratio Cut ( §3 ) into a maximization problem of a well behaved positive definite function ( §4 ) . In order to generalize the problem in §5 , we prove the similarity matrix ( graph definition , matrix ) with mixed signs can also be applied for Laplacian embedding . After that , we present the NLE algorithm ( §6 ) and prove rigorously the correctness and convergence of our algorithm ( §7 ) using the theory of constrained optimization . We illustrate the NLE algorithm and capability using an example of faces images in §9 . In §10 , we perform extensive experiments on five UCI datasets [ 2 ] and AT&T face image dataset [ 23 ] to compare our NLE algorithm to standard spectral approach . We show that our NLE algorithm consistently gives out better objective function value of Laplacian embedding and the Ratio Cut clustering objective . Meanwhile , our NLE method also improves clustering accuracy over the standard spectral approach . Brief Summary of Major Clustering Frameworks
In essence , our line of clustering framework is to show that the Ratio Cut clustering objective can be written as an optimization of quadratic function with nonnegative constraints and orthogonality constraints ; If we retain orthogonality while ignore the nonnegativity , the solution is the standard Laplacian embedding using eigenvectors . This has been the way spectral clustering is developed so far . However , if we retain nonnegativity rigorously and enforce the orthogonality approximately , the solution is the NLE proposed in this paper .
We note that this clustering framework is similar to the K means clustering – PCA – Nonnegative Matrix Factoriza tion(NMF ) [ 17 ] , [ 18 ] framework [ 8 ] . It has been shown [ 26 ] , [ 7 ] , [ 8 ] that K means clustering objective can be written as the maximization of a quadratic function with nonnegativity and orthogonality constraints ; If we retain orthogonality while ignore the nonnegativity , the solution is PCA [ 26 ] , [ 7 ] . However , if we retain nonnegativity rigorously and enforce orthogonality approximately , the solution is NMF[7 ] . Several further developments using NMF for clustering are convex NMF [ 12 ] , orthogonal NMF [ 13 ] , and equivalence between NMF and probabilistic latent semantic indexing [ 10 ] . For recent surveys of NMF see [ 4 ] , [ 19 ] .
II . LAPLACIAN EMBEDDING
We start with a brief introduction to Laplacian embedding . The input data is a matrix W of pairwise similarities among n objects . We view W as the edge weights on a graph with n nodes . The task is to embed the nodes of the graph into 1D space with coordinates ( x1,··· , xn ) . The objective is that if i , j are similar ( ie , wij is large ) , they should be adjacent in embedded space , ie , ( xi − xj)2 should be small . This can be achieved by minimizing [ 16 ]
. ( xi − xj)2wij . xi(D − W )ijxj min x
J(x ) = ij where D = diag(d1,··· , dn ) and di = ij = 2 fi = 2xT ( D − W )x , fi ij(xi − xj)2wij would get xi = 0 fi if there is no constraint on the magnitude of the vector x . i = 1 . The Therefore , we impose the normalization original objective function invariant if we replace xi by xi+ constant . Thus the solution is not unique . To fix this xi = 0 uncertainty , we can adjust the constant such that ( xi is centered around 0 ) . Thus xi have mixed signs . With
The minimization of j Wij . fi i x2
( 1 ) these two constraints : .
. x2 i = 1 , xi = 0 , i i the solution of minimizing the embedding objective is given by the eigenvectors of
( 2 ) The matrix L = D − W is called graph Laplacian . This is because L is a discrete form of the Laplace operator
( D − W )f = λf , ' ff
∇2f(x , y , z ) =
∂2 ∂x2 +
∂2 ∂y2 +
∂2 ∂z2 f(x , y , z )
In mathematical physics , a partial differential operator is not defined unless the boundary condition are specified — different boundary conditions leads to different solutions . The graph Laplacian here is the discretized form of Laplacian operator with the Von Neumann boundary condition , ie , the derivatives along the boundary are zero . ( The discretized
338 form of Laplacian operator with the Dirichlet boundary condition has slightly different form . ) Because of the Von Neumann boundary condition , the solution is invariant wrt an additive constant . As a consequence , the solution contains the constant eigenvector , the first eigenvector with eigenvalue zero . ( see [ 11 ] for details ) . Multi dimensional Embedding This embedding can be generalized to embedding in kdimensional space , with coordinates ri ∈ 'k . Let ||ri − rj|| be the Euclidean distance between nodes i , j . The embedding is obtained by optimizing min R
J(R ) = n . ||ri − rj||2wij n . i,j=1 i ( D − W )ijrj rT = 2 = 2 Tr R(D − W )RT , i,j=1
R ≡ ( r1,··· , rn ) . clustering : divide the nodes of G into K disjoint clusters {Cp} by minimizing the objective function :
. fi
1≤p<q≤K
Jrc = s(Cp , Cq ) s(Cp , Cq )
|Cp| + fi
|Cq|
, fi
( 5 ) i∈Ck where s(Ck , C . ) = j wij . Let hk = {0 , 1}n be an indicator vector for cluster Ck , ie , hk(i ) = 1 , if xi belongs to the cluster Ck ; hk(i ) = 0 , otherwise . They show that wij . di = j∈C .
Theorem 1 : The Ratio Cut objective can be written as , fi
Jrc =
= =
1≤p<q≤K s(Cp,Cq ) fi |Cp| + s(Cp,Cq ) .(D−W )h . hT Tr(H T ( D − W )H ) ,
|Cq|
K .=1
h hT
( 3 ) where H = ( h1/||h1||,··· , hK/||hK|| ) . ratio cut problem becomes
Tr H T ( D − W )H , st H TH = I , min Q
( 6 )
( 7 )
In order to prevent R → 0 , we impose the normalizafi tion constraints RRT = I . To fix the uncertainty due to the shift invariance , we further impose the constraint ri = 0 ( ri is centered around 0 ) . The solution is given by eigenvectors : R = ( f1,··· , fk)T . This is called spectral Laplacian embedding ( spectral means using eigenvectors ) . Let Q = RT ∈ 'n×p , the spectral Laplacian embedding can be formally cast as an optimization problem : Tr QT ( D − W )Q , st QT Q = I .
( 4 ) min Q
III . RATIO CUT SPECTRAL CLUSTERING
The true power of Laplacian embedding is the clustering capability . Here we briefly outline the somehow often neglected , but fundamentally important relationship between the Ratio Cut spectral clustering [ 5 ] and Laplacian embedding . In fact , these two things are identical! i∈A fi fi
In clustering/partitioning a graph , the most popular objective is min cut , which cuts the graph G into A , B the cross cut similarity ( weight ) s(A , B ) = such that j∈B wij is minimized . Without size balancing , the mincut will often cut a very small subgraph out , leading to two highly unbalanced subgraphs . The first solution to this problem is developed in curcuit placement field by Cheng and Wei [ 6 ] who proposed to minimize the following ratio cut objective function min A,B s(A , B ) |A| |B| =
1 |G| s(A , B )
|A| + s(A , B )
|B|
Note |G| is a constant and drops out . Hagen and Kahng[15 ] later show that Fiedler vector ( 2nd eigenvector of the graph Laplacian ) provides an effective solution . Chan et al . [ 5 ] generalized this two way clustering to multi way Ration Cut
339
Chan et al . also discussed the embedding of this function , which is identical to the Laplacian embedding of Eq ( 4 ) with the same orthogonality constraints . Shi and Malik [ 24 ] further developed this into normalized cut clustering . Ding et al , [ 9 ] further developed this into the min max cut clustering . A simple and widely adopted algorithm for solving spectral clustering has two steps : ( 1 ) compute the eigenvectors of L = D − W for Laplacian embedding ; ( 2 ) do K means clustering in the eigenspace to obtain clusters .
The second step is necessary because the eigenvector solution Q has mixed signs and the clusters cannot be identified directly . This is a generic difficulty of multi way spectral clustering .
IV . NONNEGATIVE LAPLACIAN EMBEDDING
In all previous working on spectral clustering , the nonnegativity of the cluster indicator H are ignored . On the other hand , a nonnegative solution by enforcing the constraint H ≥ 0 has two direct benefits : ( 1 ) we can obtain cluster assignments directly . ( 2 ) we obtain more accurate solution because the nonnegative solution resembles the desired cluster indicators .
In this paper , we propose the Nonnegative Laplacian Embedding ( NLE ) approach . In NLE , we rigorously enforce the nonnegativity constraint . The most important benefit of nonnegative embedding is that the cluster membership can be read off from solution Q immediately : xi belongs to the cluster Ck , where k corresponds to the largest component in the i th row of Q , k = arg max 1≤j≤K
Qij .
( 8 )
In fact , we may view the i th row of Q as the posterior probability that object i belongs to different clusters .
Formally , the optimization of Eq ( 4 ) is identical to
Tr[QT ( W − D + σI)Q ] , max st QT Q = I , Q ≥ 0 ,
Q
( 9 ) because the σ term TrQT σIQ = σTrI = σn is a constant . We set σ = λm to be the largest eigenvalue of L = D − W . fin W − D + σI is positive definite , because W − D + σI = k=1(σ−λk)vkvT k . This 2 step transformation ( change min to max and makes the objective positive definite ) makes the optimization as a well behaved problem . The algorithm to solve Eq ( 9 ) will be provided in §6 .
V . LAPLACIAN EMBEDDING WITH MIX SIGNED
SIMILARITY MATRIX
In traditional Laplacian embedding , graph matrix ( ie similarity matrix ) is required to be non negative . Here we show that similarity matrix with mixed sign can also be applied for Laplacian Embedding , as well as NLE . be the positive and negative part of W , respectively : W = W + − W − . For the positive part , we want to minimize the embedding distance so that the instances are similar with each other ,
Let W + and W −
But for the negative part , we maximize the embedding distance so that the instances are dissimilar ,
. i,j
. i,j min x ij(xi − xj)2 . w+ max x ij(xi − xj)2 . w−
We can combine them together by minimizing the difference ,
. min x
. ij − w− ij)(xi − xj)2 =
( w+ wij(xi − xj)2 . i,j i,j
Here we show that the similarity matrix can be shifted by any constant .
Theorem 2 : If q is a non trivial eigenvector of graph Laplacian on similarity W , then q is also an eigenvector of graph Laplacian on similarity W + σE , where σ is any constant and E is a matrix with all entries 1 with proper size .
Obviously the e ( a single column with all ones ) is an eigenvector of any graph Laplacian . The corresponding eigenvalue is 0 . Here , “ by non trivial eigenvector ” , we mean those eigenvectors which are not e . Proof . Since q is a non trivial eigenvector of graph Laplacian on similarity W , ( D − W )q = λq . If the similarity matrix shifts by a constant , W ' = W + σE , then the corresponding graph Laplacian becomes :
L'
= D' − W '
= ( D + nσI ) − ( W + σE ) .
340
Notice that all non trivial eigenvectors are orthogonal to the trivial eigenvector e ,
L'q = [ (D + nσI ) − ( W + σE)]q
= ( D − W )q + nσq − E)q = ( λ + nσ)q ,
( 10 ) which indicates q is also an eigenvector of L'
.
Theorem 2 suggests that for any mix signed similarity matrix , we can add any constant , such that the similarity matrix is nonnegative , without changing the eigenvectors ( ie the embedding results remain the same ) .
VI . SOLVING NLE PROBLEMS
Inspired from NMF algorithms , we solve the NLE problem of Eq ( 9 ) using the similar techniques , see discussions for the relationship with NMF in §VIII . A . NLE algorithm
The algorithm starts with an initial guess Q . It then iteratively updates Q until convergence using the updating rule :
(
Qik ← Qik
[ (W + σI)Q + QΛ−]ik
[ DQ + QΛ+]ik
,
( 11 ) where
Λ = QT ( W + σI − D)Q ,
( 12 ) and Λ+ is the positive part of Λ , and similarly for Λ− .
Notice that the feasible domain of Eq ( 9 ) is non convex , indicating that our algorithm can only reach local solutions . However , we show in empirical study that our algorithm yields much better Ration Cut objective than standard spectral clustering with a statistical analysis over a large number of random trials .
B . Computational complexity analysis
In the typical implementation of NLE algorithm , the computational complexity is O(n2K ) [ the complexity bottleneck is the computation of Λ in Eq ( 12) ] , which is not suitable to large scale problems . However , one can easily incorporate the approximate decompositions such as Nystr¨om decomposition , to reduce the problem to O(nK 2 ) time complexity .
VII . ANALYSIS OF NLE ALGORITHM
In this section , we show the correctness and convergence of our algorithm .
For correctness , we mean that the update yields a correct solution at convergence ; The correctness of our algorithm is assured by the following theorem .
Theorem 3 : Fixed points of Eq ( 11 ) satisfy the KKT condition of the optimization problem of Eq ( 7 ) . Proof . We begin with the Lagrangian L = Tr[QT ( W + σI − D)Q − Λ(QT Q − I ) − ΣQ ] ,
( 13 ) where the Lagrange multiplier Λ enforces the orthogonality condition QT Q = I and the Lagrange multiplier Σ enforces the nonnegativity of Q . The KKT complementary slackness condition ( ∂L/∂Qik)Qik = 0 becomes
[ (W + σI − D)Q − QΛ]ijQij = 0 .
( 14 ) n . k . i=1 p=1
( AS'B)ipS2 ip
S' ip
≥ Tr(ST ASB ) ,
( 19 ) where A , B , S , S' > 0 , A = AT , B = BT . We now find the global maxima of Z(H ) = G(H , ˜H ) . The gradient is
Clearly , a fixed point of the update rule Eq ( 11 ) satisfies
[ (W + σI − D)Q − QΛ]ijQ2 ij = 0 .
∂Z(H , ˜H )
∂Hik
=2
[ (W + σ ) ˜H]ij ˜Hik
Hik
( D ˜H)ikHik
˜Hik
− 2
− 2
( ˜HΛ−)kl ˜Hik
+ 2 Hil ( ˜HΛ+)ikHik
( 20 )
( 21 )
˜Hik
( ˜HΛ−)ik ˜Hik
H 2 il
The second derivative
∂2G(H , ˜H ) ∂Hik∂Hj .
Wik =
= −2Wikδijδk . , [ (W + σ ) ˜H]ij ˜Hik
+
H 2 ik ( D ˜H)ik
˜Hik
+
+
( ˜HΛ+)ik
˜Hik
,
This equation is mathematically identical to Eq ( 14 ) . From Eq ( 14 ) , summing over j , we obtain Λii = [ QT ( W + σI − D)Q]ii . To find the off diagonal elements of α , we ignore the nonnegativity requirement and setting ∂L/∂Q = 0 which leads to Λii . = [ QT ( W + σI − D)Q]ii By combining these two results we obtain Eq ( 9 ) . )– The convergence of our algorithm is assured by the following Theorem .
Theorem 4 : Under the update rule of Eq ( 11 ) , the La grangian function
L = Tr[QT ( W + σI − D)Q − Λ(QT Q − I) ] ,
( 15 ) increases monotonically . Proof of Theorem 4 . We use the auxiliary function approach [ 18 ] . An auxiliary function G(H , ˜H ) of function L(H ) satisfies G(H , H ) = L(H ) , G(H , ˜H ) ≤ L(H ) . We define
H ( t+1 ) = arg max
G(H , H ( t) ) .
( 16 )
H
Then by construction , we have L(H ( t ) ) = Z(H ( t ) , H ( t ) ) ≤ Z(H ( t+1 ) , H ( t ) ) ≤ L(H ( t+1) ) . ( 17 )
This proves that L(H ( t ) ) is monotonically increasing . The key steps in the remainder of the proof are : ( 1 ) Find an appropriate auxiliary function ; ( 2 ) Find the global maxima of the auxiliary function . We write Eq ( 15 ) as L = Tr[QT ( W + σI)Q + Λ−QT Q − QT DQ − Λ+QT Q ] . We can show that one auxiliary function of L is
Z(H , ˜H ) =
( W + σ)ij ˜Hik ˜Hjk(1 + log
. . .
+ ijk ilk
−
HikHjk ˜Hik ˜Hjk
)
HikHil ˜Hik ˜Hil
)
( 18 )
( Λ−)kl ˜Hik ˜Hil(1 + log .
( D ˜H)ikH 2 ik
−
˜Hik ik
( ˜HΛ+)ikH 2 ik
˜Hik ik using the inequality z ≥ 1 + logz , z = HikHjk/ ˜Hik ˜Hjk , and a generic inequality is negative definite . Thus Z(H ) is a concave function in H and has a unique global maximum . This maximum is obtained by setting the first derivative to zero , yielding :
H 2 ik = ˜H 2 ik
[ (W + σ ) ˜H]ij + ( ˜HΛ−)ik
( D ˜H)ik + ( ˜HΛ+)ik
( 22 )
According to Eq ( 16 ) , H ( t+1 ) = H and H ( t ) = ˜H , we see that Eq ( 22 ) is the update rule of Eq ( 11 ) . Thus Eq ( 17 ) )– always holds .
VIII . RELATIONSHIP WITH NMF
The nonnegative Laplacian Embedding is inspired from the idea of NMF . Here we show that these two methods are connected .
Theorem 5 : Eq ( 9 ) is equivalent to the following , ff(W − D + σI ) − QQTff2 , min Q st QT Q = I , Q ≥ 0 ,
( 23 )
Proof . ff(W − D + σI ) − QQTff2 =ffW − D + σIff2 − 2Tr(W − D + σI)QQT + ffQQTff2 Since ffW − D + σIff and ffQQTff2 are constant ( with the constraint QT Q = I ) , Eq ( 23 ) is equivalent to [ −2Tr(W − D + σI)QQT ] , min Q or
TrQT ( W − D + σI)Q , max
Q with the same constraints , which is identical to Eq ( 9 ) . )–
341
Figure 1 . expressions . On the fourth row , ten images come from ten different people .
Face images are selected from AT&T face database . On top three rows ( one person per row ) , each person has ten images with different
IX . ILLUSTRATION EXAMPLE
We illustrate the nonnegative Laplacian embedding using a simple dataset of 30 images from the AT&T face database [ 23 ] ( see the first three row of Fig 1 ) . Each person has 10 images with different expressions . Using the standard way , for each image , we reshape the image to a single vector to represent the image . For this experiment , since the pixel values of the images are non negative , we use the inner product ( wij = xT i xj ) of two images to calculate the similarity ; an advantage of inner product similarity is that there is no adjustable parameter . We start NLE algorithm with random matrix Q , 0 ≤ Q ≤ 1 . We show the NLE embedding results at the 1st , 10 th , 50th and 300 th iterations ( see Fig 2 ) . The objective function value is also shown on y axis . For each checkpoint , we use a 3D plot to show all 30 images ( each image as a point ) with the first , second , and third row of Q as x , y , and z axis . Because we impose both non negative and near orthogonal constraints on Q , all the data points are near the positive part of the axis .
From Fig 2 , we notice that the clustering structure becomes more and more clear as the objective function value increases .
A . Soft clustering capability of NLE
In traditional spectral clustering , a data point must belong to one of the clusters — this is hard clustering . However , such hard clustering sometimes prevents us from detecting delicate cluster structure details in complex data . For example , in Fig 1 , we may add 10 images from other persons ( shown as the bottom row ) to the 30 images on the top . Traditional spectral clustering will assign these 10 images into one of the 3 clusters .
However , these 10 images do not belong to three existing clusters . Ideally , the clustering solution would exhibit this fact . We now demonstrate that this fact is revealed in our NLE approach . Our NLE has the soft clustering capability , ie , the solution Q can be viewed as posterior probability of the object to be assigned to each cluster . The NLE solution Q = [ q1 , q2 , q3 ] is shown in Hinton diagram ( see Fig 3 ) . In the figure the face images index i is sorted as following : i = 1··· 10 for the 10 images shown in 1st row of Fig 1 , i = 11··· 20 for the 10 images shown in 2nd row of Fig 1 , i = 21··· 30 for the 10 images shown in 3rd row of Fig 1 , and i = 31··· 40 for the 10 images shown in 4th row of Fig 1 . We plot the elements of solution Q in rectangles , the size of which denotes the value of the corresponding elements .
We see from Fig 3 that for the first 30 images , one of qk is very pronounced and other components negligible : the cluster distribution/assignment are very clear . For the last 10 images , none of them is clearly clustered into any clusters — indicating the soft clustering nature for these images . These images are outliers in this dataset , and our NLE algorithm can correctly detect them .
342
1
2
3
5
10
15
20
25
30
35
40
Index of Image
Soft clustering of NLE . q1(i ) , q2(i ) , q3(i ) are shown as 3 rows using Hinton diagram of i = 1 · · · 40 ( x axis ) for the 40 images in Fig 1 ,
Figure 3 . where i = 31 · · · 40 correspond to the 10 images in the 4th row of Fig 1 .
90
85
80
75
70
65
60
55
50 e v i t c e b O j
1
0.5
0 0
0.4
0.2
0 0
0.5
1 0
1
0.5
0.4
0.2
0 0
0.5
1 0
1
0.5
0.2
0.4 0
0.4
0.2
0.4
0.2
0 0
0.2
0.4
0.2
45
0
50
100
0.4 0
150
# iteration
200
250
300
Figure 2 . NLE results on the top 30 face images of Figure 1 at different iterations . The objective function values of Tr QT ( W + σI − D)Q are shown on y axis . For each checkpoint , we use a 3D plot to show all 30 images ( each image as a point ) with the first , second , and third row of Q as x , y , and z axis .
X . EXPERIMENTS ON UCI DATASETS
We evaluate the performance of our NLE algorithm in 4 UCI datasets [ 2 ] : Dermatology , Soybean , Vehicle , and Zoo . In experiments , our goal is to compare with the standard spectral approach ( as explained in last paragraph of §3 ) . Therefore , we initialize Q using the clustering solution of the standard spectral clustering : H is set to the cluster indicator and Q0 = H + 0.2 as the starting point . In evaluation , we use clustering accuracy . Suppose we have N = n1 + n2 + ··· + nK data objects ( n1 are known/observed to belong to class F1 , etc ) They are clustered into K clusters . with mk = |Ck| . This forms a contingency table T = ( Tkl ) , where Tkl denotes the number of objects from class Fk and have been clustered k Tkl = ml . into cluster Cl . Clearly , The clustering accuracy is the percentage of objects been correctly clustered : ρ = k Tkk/N . In practice , matching Fk to Cl is obtained by running the Hungarian algorithm for the optimal bipartite matching . l Tkl = nk and fi fi fi
A . Evolution of NLE algorithm
In Figs 4 and 5 , we show NLE evolutions of two typical runs on two UCI ( dermatology and zoo ) datasets . The initial Q are set to be results in spectral clustering as explained above .
We observe that the NLE objective function values increase steadily as iteration proceeds . The clustering accuracy also improves with more iterations . These facts indicate that clustering quality is improved when the objective function value increases .
B . Comparison with spectral clustering
EXPERIMENTAL SETUP DETAILS ON UCI AND AT&T DATASETS
Table I
Dataset Dermatology Glass Soybean Vehicle Zoo AT&T
#sample 366 214 47 846 101 400
#feature 34 9 35 18 106 10304
#class 6 6 4 4 7 40
We perform extensive evaluation of both NLE and spectral clustering on the 5 UCI datasets and the AT&T dataset ( See Table 1 for experimental setup details ) . We note that the standard spectral clustering results on a dataset are not deterministic , because the results of K means on the eigenspace ( the spectral Laplacian embedding ) depend sensitively on
343 x 105
6 e v i t c e b O j
5.5
5
4.5
4
3.5
3
2.5
2
0 0
0.61
0.6
0.59
0.58
0.57
0.56
0.55
0.54 y c a r u c c A
50 50
100 100
150 150
# iteration
200 200
250 250
0.53
300 300
Figure 4 . NLE objective function value and clustering accuracy on dermatology dataset . The accuracy starts from the spectral clustering value and improves with more NLE iterations . x 105
1.7
1.6
1.5
1.4
1.3
1.2
1.1
1
0.9
0.8 e v i t c e b O j
0.7
0 0
50 50
100 100
150 150
# iteration
200 200
250 250
0.7
0.695
0.69
0.685
0.68
0.675 y c a r u c c A
0.67
0.665
0.66
300 300
Figure 5 . NLE objective function value and clustering accuracy on zoo dataset . The accuracy starts from the spectral clustering value and improves with more NLE iterations . the initialization . For this reason , we perform 1024 runs of K means clustering on the eigenspace for each dataset . We also perform 1024 NLE computations and each of them is initialized from the spectral solution . We evaluate the performance as following . Define Best(N ) to be the lowest Ratio Cut objective among N random trials for both of approaches ( spectral clustering and NLE ) . Clearly , Best(N ) improves ( decreases ) as we increase N . The results of experiments for different N are shown in Figure 6 . [ At smaller N , the results are averaged with multiple N interval runs . ] The results are shown in the right of Figures 6 ( a f ) . We compare the clustering accuracy using the same strategy ( shown in left of Figures 6 ( a f) ) . For Ratio Cut objective ,
344 the best ( minimum ) value is subtracted from the original Ratio Cut objective .
In all 6 datasets , NLE results are consistently better than spectral clustering on average , in both terms of Ratio Cup objective and clustering accuracy .
In Table 2 , we show the Ratio Cut objective function value and the corresponding clustering accuracy , picking the best result of the 1024 runs ( here , the best means the lowest Ratio Cut objective function value , because this is an unsupervised learning ) . For all 4 datasets , NLE consistently gives lower ( better ) Ratio Cut objective function value and higher clustering accuracy .
XI . CONCLUSION
In this paper , we propose a Nonnegative Laplacian Embedding ( NLE ) algorithm and prove the correctness and convergence of the algorithm . NLE gives nonnegative embedding results from which clustering structures of data can be read off immediately . A computationally efficient algorithm is developed to solve proposed NLE problems . Moreover , we prove the similarity matrix ( ie graph matrix ) with mixed signs can also be applied for Laplacian embedding .
We demonstrate the cluster assignment advantage and soft clustering capability of NLE algorithm by illustrations on face expression data and extensive experiments on five UCI datasets and one image dataset . Our approach consistently outperforms spectral clustering in terms of both Ratio Cut objective and clustering accuracy . Acknowledgment . This work is supported partially by NSF DMS 0844497 and NSF CCF 0830780 at UTA , and NSF DMS 0844513 and IIS 0546280 at FIU .
REFERENCES
[ 1 ] CJ Alpert and AB Kahng . Recent directions in netlist partitioning : a survey . Integration , the VLSI Journal , 19:1–81 , 1995 .
[ 2 ] A . Asuncion and D . Newman . UCI machine learning repos itory . 2007 .
[ 3 ] M . Belkin and P . Niyogi . Laplacian eigenmaps and spectral techniques for embedding and clustering . NIPS , 2001 .
[ 4 ] M . Berry , M . Browne , A . Langville , P . Pauca , and R . Plemmons . Algorithms and applications for approximate nonnegative matrix factorization . To Appear in Computational Statistics and Data Analysis , 2006 .
[ 5 ] PK Chan , M.Schlag , and JY Zien . Spectral k way ratiocut partitioning and clustering . IEEE Trans . CAD Integrated Circuits and Systems , 13:1088–1096 , 1994 .
[ 6 ] C K Cheng and YA Wei . An improved two way partitioning algorithm with stable performance . IEEE . Trans . on Computed Aided Desgin , 10:1502–1511 , 1991 .
[ 7 ] C . Ding and X . He . K means clustering and principal component analysis . Int’l Conf . Machine Learning ( ICML ) , 2004 .
Objective ( ×103)↓
Clustering Accuracy↑
SpecClus
NLE
SpecClus
NLE
Dataset Dermatology Glass Soybean Vehicle Zoo AT&T
Ave
1.8286 1.0683 0.1409 2.5337 0.6056 15.5898
Best 1.8285 1.0676 0.1409 2.5330 0.6056 15.5896
Ave
1.8255 1.0628 0.1409 2.5326 0.6027 15.5879
Best 1.8244 1.0619 0.1409 2.5324 0.6022 15.5875
Ave
0.8092 0.4387 0.9573 0.3569 0.6160 0.6171
Best 0.8989 0.5421 1.0000 0.4137 0.8713 0.7050
Ave
0.8361 0.4627 0.9960 0.3923 0.8248 0.6874
Best 0.9454 0.5514 1.0000 0.4397 0.9307 0.7825
AVERAGE ( AVG ) AND BEST RATIO CUT OBJECTIVE FUNCTION VALUE AND CLUSTERING ACCURACY OF STANDARD SPECTRAL CLUSTERING ( SPECCLUS ) AND NLE OVER 1024 RANDOM TRIALS . “ ↓ ” MEANS THAT THE LOWER THE BETTER AND “ ↑ ” MEANS THE HIGHER THE BETTER .
Table II
[ 8 ] C . Ding , X . He , and HD Simon . On the equivalence of nonnegative matrix factorization and spectral clustering . Proc . SIAM Data Mining Conf , 2005 .
[ 20 ] AY Ng , MI Jordan , and Y . Weiss . On spectral clustering : Analysis and an algorithm . Proc . Neural Info . Processing Systems ( NIPS 2001 ) , 2001 .
[ 9 ] C . Ding , X . He , H . Zha , M . Gu , and H . Simon . A min max cut algorithm for graph partitioning and data clustering . Proc . IEEE Int’l Conf . Data Mining ( ICDM ) , pages 107–114 , 2001 .
[ 21 ] A . Pothen , H . D . Simon , and K . P . Liou . Partitioning sparse matrices with egenvectors of graph . SIAM Journal of Matrix Anal . Appl . , 11:430–452 , 1990 .
[ 22 ] S . Roweis and L . Saul . Nonlinear dimensionality reduction by locally linear embedding . Science , 290:2323–2326 , 2000 .
[ 23 ] Ferdinando Samaria and Andy Harter .
Parameterisation of a stochastic model for human face identification , 1994 . http://wwwclcamacuk/research/dtg/attarchive/facedatabasehtml
[ 24 ] J . Shi and J . Malik . Normalized cuts and image segmentation . IEEE . Trans . on Pattern Analysis and Machine Intelligence , 22:888–905 , 2000 .
[ 25 ] JB Tenenbaum , V . de Silva , and JC Langford . A global geometric framework for nonlinear dimensionality reduction . Science , 290:2319–2323 , 2000 .
[ 26 ] H . Zha , C . Ding , M . Gu , X . He , and HD Simon . Spectral relaxation for K means clustering . Advances in Neural Information Processing Systems 14 ( NIPS’01 ) , pages 1057– 1064 , 2002 .
[ 27 ] Z . Zhang and Z . Zha . Principal manifolds and nonlinear dimensionality reduction via tangent space alignment . SIAM J . Scientific Computing , 26:313–338 , 2004 .
[ 10 ] C . Ding , T . Li , and W . Peng . Nonnegative matrix factorization and probabilistic latent semantic indexing : Equivalence , chisquare statistic , and a hybrid method . Proc . National Conf . Artificial Intelligence , 2006 .
[ 11 ] Chris Ding , Rong Jin , Tao Li , and Horst D . Simon . A learning framework using green ’s function and kernel regularization with application to recommender system . In KDD , pages 260–269 , 2007 .
[ 12 ] Chris Ding , Tao Li , and Michael I . Jordan . Convex and semi nonnegative matrix factorizations . IEEE Trans . Pattern Analysis and Machine Intelligence , 2009 .
[ 13 ] Chris Ding , Tao Li , Wei Peng , and Haesun Park . Orthogonal nonnegative matrix tri factorizations for clustering . Proc Int’l Conf . on Knowledge Discovery and Data Mining ( KDD 2006 ) , page Accepted by .
[ 14 ] M . Fiedler . Algebraic connectivity of graphs . Czech . Math .
J . , 23:298–305 , 1973 .
[ 15 ] L . Hagen and AB Kahng . New spectral methods for ratio IEEE . Trans . on Computed cut partitioning and clustering . Aided Desgin , 11:1074–1085 , 1992 .
[ 16 ] K . M . Hall . R dimensional quadratic placement algorithm .
Management Science , 17:219–229 , 1971 .
[ 17 ] DD Lee and H . S . Seung . Learning the parts of objects by non negative matrix factorization . Nature , 401:788–791 , 1999 .
[ 18 ] DD Lee and HS Seung . Algorithms for non negative matrix factorization . In Advances in Neural Information Processing Systems 13 , Cambridge , MA , 2001 . MIT Press .
[ 19 ] Tao Li and Chris Ding . The relationships among various In nonnegative matrix factorization methods for clustering . ICDM , pages 362–371 , 2006 .
345
Highest of Accuracy of N Trials ↑
SpecClus NLE 6
Lowest of Ratio Cut of N Trials ↓
2
3
4
5
6
7
2
3
4
5
6
7
2
3
4
5
6
7 t u C o i t a R
4
2
0
8
1
( A ) Dermatology
8
6
4
2 t u C o i t a R
0
8 1 ( B ) Glass 0.01 t u C o i t a R
0
8
1
( C ) Soybean
2
3
4
5
6
7
8
2
3
4
5
6
7
8
2
3
4
5
6
7
8
1
0.5 t u C o i t a R
2
3
4
5
6
7
0
8 1 ( D ) Vehicle
2
3
4
5
6
7
8
4
3
2
1 t u C o i t a R y c a r u c c A y c a r u c c A
0.95
0.9
0.85
1
0.6
0.55
0.5
0.45
0.4
1 y c a r u c c A
1.002
1
0.998
0.996
0.994
1
0.45 y c a r u c c A
0.4
0.35
1
1
0.9
0.8
0.7 y c a r u c c A
1
2
3
4
5
6
7
0.8
0.75
0.7
0.65 y c a r u c c A
1
2
3
4
5
6
7 log2 N
1
0 8 ( E ) Zoo 3 t u C o i t a R
2
1
0
8 1 ( F ) AT&T
2
3
4
5
6
7
8
2
3
4
5
6
7
8 log2 N
Figure 6 . Clustering accuracy ( left ) and Ratio Cut objective ( right ) on six datasets for Spectral Clustering ( SpecClus ) and our method ( NLE ) . For clustering accuracy the higher the better ( “ ↑ ” ) and for Ratio Cut objective the lower the better ( “ ↓ ” ) .
346
