2009 Ninth IEEE International Conference on Data Mining
Inverse Time Dependency in Convex Regularized Learning
Zeyuan Allen Zhu12* , Weizhu Chen2 , Chenguang Zhu23 , Gang Wang2 , Haixun Wang2 , Zheng Chen2
1Fundamental Science Class ,
Department of Physics , Tsinghua University zhuzeyuan@hotmail.com
2Microsoft Research Asia
{v zezhu , wzchen , v chezhu , gawa , haixunw , zhengc}@microsoft.com
3Department of Computer Science and Technology ,
Abstract—In the conventional regularized learning , training time increases as the training set expands . Recent work on L2 linear SVM challenges this common sense by proposing the inverse time dependency on the training set size . In this paper , we first put forward a Primal Gradient Solver ( PGS ) to effectively solve the convex regularized learning problem . This solver is based on the stochastic gradient descent method and the Fenchel conjugate adjustment , employing the well known online strongly convex optimization algorithm with logarithmic regret . We then theoretically prove the inverse dependency property of our PGS , embracing the previous work of the L2 linear SVM as a special case and enable the 𝓵𝒑 norm optimization to run within a bounded sphere , which qualifies more convex loss functions in PGS . We further illustrate this solver logistic regression and regularized least square . Experimental results substantiate the property of the inverse dependency on training data size . in three examples : SVM ,
Keywords – Primal Gradient Solver ; inverse time dependency ; learning ; online convex regularized
Fenchel conjugate ; optimization
I .
INTRODUCTION
In the regularized learning theory , in order to minimize the sum of the regularization part and the loss part , most of the research works are interested in the generalization objective rather than the empirical objective [ 12 ] [ 1 ] . The generalization objective , also known as the stochastic objective , is given with respect to a linear predictor 𝒘∈𝑆 , where 𝑆⊂ℝ is the domain of 𝒘 : 𝐹(cid:3097)(𝒘)=𝜎∙𝑟(𝒘)+𝑙(𝒘 ) =𝜎∙𝑟(𝒘)+𝔼𝜽~(cid:3005)(cid:3046)(cid:3047)[𝑙(〈𝒘,𝜽〉;𝜽 ) ] where 𝑟(𝒘 ) is the regularizer with a positive weight 𝜎 , and 𝑙(〈𝒘,𝜽〉;𝜽 ) is a mapping that calculates the cost or regret by the linear predicting value 〈𝒘,𝜽〉 . The expectation is based on a random selection of the sample 𝜽 over the entire sample distribution 𝐷𝑖𝑠𝑡 . Note that the form 𝜽 is used in order to ensure the generality . As an example , 𝜽 can be in the form of ( 𝒙,𝑦 ) where 𝒙 is a vector of features and 𝑦 is the class identity , adapting ( 1 ) to classifications . The loss function 𝑙 can be for
𝑙(〈𝒘,𝒙〉,𝑦)=max{0,1−𝑦〈𝒘,𝒙〉} example the SVM hinge loss
( 1 )
Practically , an optimization approach for this sort of problem becomes to minimize the empirical objective
1550 4786/09 $26.00 © 2009 IEEE DOI 101109/ICDM200928
667
𝐹(cid:3552)(cid:3097)(𝒘 ) 1 instead , where the average loss over a set of 𝑚
Tsinghua University zcgcs60@gmailcom training samples is used to approximate the generalization loss .
𝐹(cid:3552)(cid:3097)(𝒘)=𝜎∙𝑟(𝒘)+𝑙(cid:4632)(𝒘 ) =𝜎∙𝑟(𝒘)+1𝑚(cid:3533)𝑙(〈𝒘,𝜽𝒊〉;𝜽𝒊 ) ( cid:3040 ) ( cid:2880 )
( 2 ) the and between
The accuracy of a given predictor on some unknown prediction set is strongly associated with equation ( 1 ) . This naturally leads to a two step research work : connect ( 1 ) and ( 2 ) as the step 1 , and effectively solve ( 2 ) as the step 2 . empirical stochastic estimation
Step 1 . Recently , Léon Bottou et al [ 1 ] studied the correlation but unregularized objectives and divided the tradeoff into three parts , namely , and optimization tradeoff . For regularized learning , Karthik
Sridharan et al [ 12 ] stated that 𝐹(cid:3552)(cid:3097)(𝒘 ) converges with a rate of 1/𝑚 to 𝐹(cid:3097)(𝒘 ) for strongly convex objectives . approximation ,
Step 2 . In 2004 , T . Zhang [ 13 ] introduced the stochastic gradient descent ( SGD ) algorithm to solve large scale linear prediction problems . It proves that a constant learning rate will numerically achieve some good accuracy , and states the correlation between SGD and online learning . In 2006 , Hazan et al [ 3 ] introduced a framework with logarithmic regret to solve online strongly convex problems , which is the tightest known regret bound for online optimization . Utilizing this result , Shai Shalev Shwartz et al [ 10 ] proposed an ℓ(cid:2870) norm linear SVM algorithm called PEGASOS . for example , if we get a predictor with accuracy 95 % by accuracy of 95 % , but in less time .
On the basis of the above two steps , Shai Shalev Shwartz et al [ 11 ] presented a surprising result for PEGASOS : assuming the endurable accuracy is given and fixed , the training time has an inverse dependency on the size of the training data , ie the larger the dataset is , the faster the program runs to achieve this given accuracy . He claimed that , training one thousand samples , we can use the extra nine thousand samples to train and get a predictor also with
* This work was done when the first author was visiting Microsoft Research Asia . The first author is supported by the National Innovation Research Project for Undergraduates ( NIRPU ) . throughout this paper we will use ̂ to denote the empirical functions .
1 In order to distinguish between the two – generalized and empirical ,
SUMMARY OF TERMINOLOGY
Generalization objective Regularizer Generalization loss Empirical objective Empirical loss
Temporal objective at iter . 𝑡 Temporal loss at iter . 𝑡
Optimization error
Generalization error
𝐹(cid:3097)(𝑤)=𝜎∙𝑟(𝑤)+𝑙(𝑤 ) 𝑟(𝑤 ) 𝑙(𝒘)=𝔼𝜽~(cid:3005)(cid:3046)(cid:3047)[𝑙(〈𝒘,𝜽〉;𝜽 ) ] 𝐹(cid:3552)(cid:3097)(𝒘)=𝜎∙𝑟(𝒘)+𝑙(cid:4632)(𝒘 ) 𝑙(cid:4632)(𝒘)= ( cid:3040)∑ 𝑙(〈𝒘,𝜽𝒊〉;𝜽𝒊 ) ( cid:3040)(cid:2880 ) 𝑐(cid:3047)(𝒘)=𝜎∙𝑟(𝒘)+𝑔(cid:3047)(𝒘 ) 𝑔(cid:3047)(𝒘)= |(cid:3002)(cid:3295)|∑ 𝑙(〈𝒘,𝜽〉;𝜽 ) 𝜖(cid:3028)(cid:3030)(cid:3030 ) , satisfies 𝜽∈(cid:3002)(cid:3295 ) 𝐹(cid:3552)(cid:3097)(𝐰)≤𝐹(cid:3552)(cid:3097)(𝒘(cid:3549))+𝜖(cid:3028)(cid:3030)(cid:3030 ) 𝜖 , satisfies ∀𝒘𝟎∈𝑆 , 𝑙(𝒘(cid:3557))−𝑙(𝒘𝟎)≤𝜖
Sample Training sample space
The domain of predictor 𝑤
Population optimum Empirical optimum Reference predictor Our generated predictor Average number of nonzero features per sample is Dimension of feature space
TABLE I .
𝜃 𝛹={𝜃,…𝜃(cid:3040)} 𝑆 𝒘∗=argmin𝒘∈(cid:3020)𝐹(cid:3097)(𝒘 ) 𝒘(cid:3549)=argmin(cid:2933)∈S𝐹(cid:3097)(cid:3554)(𝑤 ) 𝒘𝟎 𝒘(cid:3557 ) 𝑑 𝑛
( 3 )
Notice that Shai focuses solely on the ℓ(cid:2870) norm linear SVM problem , partially because the ℓ(cid:2870) norm is naturally a property into more general problems , like ℓ(cid:3043 ) norm , other strongly convex function and the hinge loss in SVM is easy to be handled . However , applying this inverse dependency is to maintain the strong
𝑟(𝒘)=
( PGS ) , which employs the following regularizer :
In this paper , we introduce the Primal Gradient Solver loss functions , or other machine learning algorithms , is very desirable , but it is an under explored research problem .
12(𝑝−1)‖𝒘‖(cid:3043)(cid:2870),𝑝∈(1,2 ] ⁄ arbitrary Lipschitz continuous and convex loss function
Gradient Solver algorithm can achieve the inverse time dependency on the training data size . This conclusion is also verified in the experiments . We summarize the contributions of this paper as below : where the coefficient of 12(𝑝−1 ) convexity of 𝑟(𝒘 ) . At the same time , we consider the 𝑙(〈𝒘,𝜽〉;𝜽 ) . We prove that for a fixed accuracy , our Primal generalizes the state of the art ℓ(cid:2870) SVM result [ 11 ] to ℓ(cid:3043) norm and convex loss functions . Notice that • By bounding S ( the domain of 𝒘 ) , PGS is able to Square Loss is ineligible for 𝑆=ℝ because of its for 𝑆=(cid:3419)𝒘:‖𝒘‖(cid:3043)≤𝐵(cid:3423 ) , where 𝐵 is a constant large enough to embrace the optimal solution of 𝒘∗ in 𝑆 . the generalization the mathematical analysis utilizes a Fenchel conjugate of lacks an explicit expression in most circumstances .
It proposes a Primal Gradient Solver ( PGS ) and proves its inverse dependency property . This work unbounded gradient , but is proved to be acceptable support more loss functions . For example , Least regularizer , which is non trivial , since the
•
•
It firstly demonstrates that both logistic loss and least square loss can be adopted into the proposed solver and achieve the inverse dependency property . Extensive experimental results on two machine learning and regularized least square , substantiate the conclusion . algorithms , regression logistic in this section , we
II . MATH BACKGROUND AND TERMINOLOGY used in this paper in TABLE I . Considering the boundedness
The reminder of this paper is organized as follows . We first provide mathematical backgrounds on convex optimization theory in Section II . Next in Section III , we propose our main result by introducing our Primal Gradient Solver , and analyzing its inverse dependency property . We further demonstrate our solver in SVM , logistic regression and regularized least square in Section IV , and present experimental results in Section V to substantiate our findings . We provide the theoretical proofs of our main theorems in Section VI . We then raise some discussions in Section VII , and conclude the paper in Section VIII .
Next introduce some definitions frequently used in convex optimization , and a proposition to be used later .
Throughout this paper we assume norms to be 𝑝 norms , where 𝑝∈[1,∞)∪{∞} . We also summarize the notations of some vector 𝑥 , we will stick to the expression “ ‖𝑥‖(cid:3043 ) is bounded ” instead of “ 𝑥 is bounded ” for some explicit 𝑝.2 Definition 1 : A function 𝑓:𝑆→ℝ is called 𝐿 Lipschitz continuous wrt a norm ‖.‖ if ∀𝒘𝟏,𝒘𝟐∈𝑆 , |𝑓(𝒘𝟏)−𝑓(𝒘𝟐)|≤𝐿∙‖𝒘𝟏−𝒘𝟐‖ Definition 2 : A function 𝑓:𝑆→𝑅 is called σ strongly convex wrt a norm ‖.‖ if ∀𝒘𝟏,𝒘𝟐∈𝑆,𝛼∈[0,1 ] , 𝑓(𝛼𝒘𝟏+(1−𝛼)𝒘𝟐 ) ≤𝛼𝑓(𝒘𝟏)+(1−𝛼)𝐹(𝒘𝟐 ) −𝜎2𝛼(1−𝛼)‖𝒘𝟏−𝒘𝟐‖ Definition 3 : The Fenchel conjugate of a function 𝑓:𝑆→𝑅 ∀𝑝,𝑞∈[1,∞)∪{∞},∃𝐶∈ℝ,∀𝑥,‖𝑥‖(cid:3043)≤𝐶∙‖𝑥‖(cid:3044 )
2 This is because although in finite dimension , norms are pair wise however , the bounding 𝐶 may hide a constant up to 𝑛 . is defined as : bounded
( 5 )
( 4 )
668
2 .
1 .
( 6 ) and predictor
4 . 5 . 6 . 7 . 8 .
Figure 1 : The Primal Gradient Solver .
INPUT : training sample space 𝛹={𝜃,…𝜃(cid:3040)} 𝑝,𝜎,𝑇,𝑘 INITIALIZE : 𝒘𝟎←0 , 𝝀←0 , 𝑞←1/(1−1/𝑝 ) 3 . FOR 𝑡 = 1,2,…,𝑇 Choose 𝐴(cid:3047)⊂𝛹 satisfying |𝐴(cid:3047)|=𝑘 Set 𝑔(cid:3047)(𝒘)← |(cid:3002)(cid:3295)|∑ 𝑙(〈𝒘,𝜽〉;𝜽 ) Choose 𝝀𝒕∈𝜕𝑔(cid:3047)(𝒘𝒕𝟏 ) 𝜽∈(cid:3002)(cid:3295 ) Let 𝝀←𝝀−𝝀𝒕 Define 𝒘𝒕←𝛻𝑟∗(cid:4672 ) 𝝀((cid:3047))(cid:3097)(cid:4673 ) where 𝑟(𝒘)= ( cid:2870)((cid:3043))‖𝒘‖(cid:3043)(cid:2870 ) 9 . Return a random 𝒘𝒊∈{𝒘𝟏,…𝒘𝑻} as linear 𝑓∗(𝜽)=𝑠𝑢𝑝𝒘∈(cid:3020)〈𝒘,𝜽〉−𝑓(𝒘 ) Example 1 : When 𝑆=ℝ , for 𝑝∈(1,2 ] , the function 𝑓(𝒘)= ( cid:2870)((cid:3043))‖𝒘‖(cid:3043)(cid:2870 ) is 1 strongly convex wrt the ℓ(cid:3043 ) norm , its Fenchel conjugate 𝑓∗(𝜽)= ( cid:2870)((cid:3044))‖𝜽‖(cid:3044)(cid:2870 ) . Here ( cid:3043)+(cid:3044)=1 . Proofs can be found in [ 8 ] [ 2 ] . The strong convexity does not hold for 𝑝>2 . Definition 4 : The dual norm of the ℓ(cid:3043 ) norm ‖𝒙‖(cid:3043)= ( ∑|𝑥|(cid:3043 ) )/(cid:2926 ) is the ℓ(cid:3044 ) norm ‖𝒙‖(cid:3044)=(∑|𝑥|(cid:3044 ) )/(cid:2927 ) if 1/p+ 1/q=1 . As a special case , ‖𝒙‖=∑|𝑥|
‖𝒙‖(cid:2998)=max|𝑥| .
Definition 5 : A vector 𝝀 is a sub gradient of a function 𝑓 at 𝒘 if for all 𝒘(cid:4593)∈𝑆 we have 𝑓(𝒘(cid:4593))−𝑓(𝒘)≥〈𝒘(cid:4593)−𝒘,𝝀〉 . The differential set of 𝑓 at 𝒘 consists of all the sub gradients and is denoted by ∂𝑓(𝒘 ) . When 𝑓 is differentiable at 𝒘 , ∂𝑓(𝒘 ) contains exactly one element ∂𝑓(𝒘)={∇𝑓(𝒘)} . Proposition 1 : If a function 𝑓:𝑆→ℝ is L Lipschitz continuous wrt norm ‖.‖(cid:3043 ) , then ∀𝒘∈𝑆 , the sub gradient at 𝒘 is bounded : ‖𝝀‖(cid:3044)≤𝐿,∀𝝀∈𝜕𝑓(𝒘 ) , where 1/p+ 1/q=1 . continuity , we have for any 𝒘(cid:4593)∈𝑆 , 〈𝒘(cid:4593)−𝒘,𝝀〉≤𝑓(𝒘(cid:4593))−𝑓(𝒘)≤𝐿∙‖𝒘(cid:4593)−𝒘‖(cid:3043 ) By the knowledge of Hölder inequality there exists a 𝒘(cid:4593)∈𝑆 such that 〈𝒘(cid:4593)−𝒘,𝝀〉=‖𝒘(cid:4593)−𝒘‖(cid:3043)‖𝝀‖(cid:3044 ) , and combining the above two we arrive at ‖𝝀‖(cid:3044)≤𝐿 . ∎
Proof : By the definition of differential set and Lipschitz is dual to
III . MAIN RESULT
In this section we first propose a Primal Gradient Solver and state the requirements for the regularizer and the loss function ; we then use two theorems to reveal the inverse time dependency , that is , the required running time decreases as the number of samples increases , when achieving a fixed generalization error .
669
1 . assuming regularized
3 . 4 . 5 . 6 . 7 .
( cid:3040 ) ( cid:2880 )
A . Primal Gradient Solver convex optimization problem , satisfies the following two assumptions : superscript of the form ( j ) to denote the jth coordinate of a vector
In this paper , we concentrate on the loss function that
INPUT : 𝝀,𝑝,𝑆 . Let 𝑛 be the feature dimension . 2 . FOR 𝑖=1,2,…,𝑛 𝑤(cid:3047)()← ( cid:3044)(cid:3436)∑(cid:4698 ) ( cid:3090)((cid:3285 ) ) ( cid:3440)(cid:3118)(cid:3292)∙(cid:4698 ) ( cid:3090)((cid:3285 ) ) ( (cid:3047))(cid:3097)(cid:4698)(cid:3044 ) ( (cid:3047))(cid:3097)(cid:4698)(cid:3044)∙sgn𝜆( ) IF 𝑆=ℝ , RETURN 𝒘𝒕
IF 𝑆=(cid:3419)𝒘:‖𝒘‖(cid:3043)≤𝐵(cid:3423 ) IF ‖𝒘𝒕‖(cid:3043)>𝐵 THEN , 𝒘𝒕← ( cid:3003)‖𝒘𝒕‖(cid:3292)𝒘𝒕 RETURN 𝒘𝒕 ⁄ Figure 2 : Explicit calculation for 𝒘𝒕=∇r∗(𝛌(t+1)σ ) . We use the We first introduce the Primal Gradient Solver for the ℓ(cid:2926 ) 𝑝∈(1,2 ] . By substituting the regularizer ( 3 ) into ( 2 ) , we 𝜎2(𝑝−1)‖𝒘‖(cid:3043)(cid:2870)+1𝑚(cid:3533)𝑙(〈𝒘,𝜽𝒊〉;𝜽𝒊 ) have : 𝐹(cid:3552)(cid:3097)(𝒘)= • Convexity : 𝑙(〈𝒘,𝜽〉;𝜽 ) satisfies the convexity wrt 𝒘 in 𝑆 . Pay attention that we do not require the • Lipschitz Continuity : 𝑙(〈𝒘,𝜽〉;𝜽 ) satisfies 𝐿 Lipschitz continuity wrt 𝒘 and ‖.‖(cid:3043 ) norm in 𝑆 , where 𝐿 is a constant . gradient 𝜕𝒘𝑙(〈𝒘,𝜽〉;𝜽 ) is bounded wrt ‖‖(cid:3044 ) This property the norm parameter 𝑝 , the weight of the regularizer σ , the number of iterations 𝑇 , and a given positive integer 𝑘 . Initially we set 𝒘𝟎=0 and a working vector 𝝀=0 . At iteration t we randomly choose a set 𝐴(cid:3047)⊂Ψ,|𝐴(cid:3047)|=𝑘 , and consider a temporal loss function 𝑔(cid:3047)(𝒘 ) to approximate the empirical loss 𝑙(cid:4632)(𝒘 ) : 𝝀𝒕∈𝜕𝑔𝑡(𝒘𝒕−𝟏 ) , and subtract it from 𝝀 by 𝝀←𝝀−𝝀𝒕 . The next 𝒘𝒕 is calculated according to the gradient of the
𝑔(cid:3047)(𝒘)= 1|𝐴(cid:3047)|(cid:3533)𝑙(〈𝒘,𝜽〉;𝜽 ) 𝜽∈(cid:3002)(cid:3295 ) 𝝀(𝑡+1)𝜎(cid:3440 ) 𝒘𝒕=𝛻𝑟∗(cid:3436 )
Inspired by the work of PEGASOS [ 10 ] , we propose a Primal Gradient Solver ( Figure 1 ) . We take four parameters :
The solver then picks up an arbitrary sub gradient
We notice that with the help of Proposition 1 , the sub
Fenchel conjugate ( see Section II for definition ) : strong convexity here . will be used later .
( 8 )
( 9 )
( 7 )
Running time
Section III.A
𝑇
Theorem 2
Theorem 1
Generalization error
Optimization error
•
•
⁄
Figure 3 : Outline of the proof .
Assume the dimension of the feature space , ie , the these two cases cover most of the circumstances in the applications .
The above process is organized in Figure 1 . In Figure 2 , we write the explicit formula of Equation ( 9 ) for the two
Figure 2 ) . The proof of this can be found in the Appendix , by comparing the results of Corollary 2 and Corollary 3 . directly . The explicit form is shown on Line 3 in Figure 2 . cases 𝑆=ℝ and 𝑆=(cid:3419)𝒘:‖𝒘‖(cid:3043)≤𝐵(cid:3423 ) . We will show that If 𝑆=ℝ , we recall Example 1 in Section II , and calculate the gradient of 𝑟∗(𝜽)=‖𝜽‖(cid:3044)(cid:2870 ) 2(𝑞−1 ) If 𝑆=(cid:3419)𝒘:‖𝒘‖(cid:3043)≤𝐵(cid:3423 ) is bounded , we actually calculate 𝒘𝒕 in the same way , but project it back to the 𝑝 norm sphere 𝑆 if it lies outside 𝑆 ( Line 6 of dimension of 𝒘 , is 𝑛 , and the average number of non zero features per sample is 𝑑 . If the sub gradient 𝜕𝑔(cid:3047)(𝒘 ) can be computed efficiently in 𝑂(𝑑𝑘 ) , the time complexity for the Primal Gradient Solver is 𝑂(cid:3435)𝑇(𝑑𝑘+𝑛)(cid:3439 ) since calculating the gradient of 𝑟∗ costs 𝑂(𝑛 ) , as shown in in Figure 2 . the special case of 𝑝=2 , since the term ( cid:3435)∑… ( cid:3439)(cid:3118)(cid:3292 ) ( (cid:3047))(cid:3097)(cid:3628)(cid:3044)∙sgn𝜆()⇒𝒘𝒕= 𝝀((cid:3047))(cid:3097 ) 𝑤(cid:3047)((cid:2919))=(cid:3628 ) ( cid:3090)((cid:3285 ) ) In this case we no longer need 𝑂(𝑛 ) to calculate ∇𝑟∗ , as we can use a variable to store the coefficient in front of 𝝀 and update it in 𝑂(1 ) time , leaving the overall complexity 𝑂(𝑇𝑑𝑘 ) . Gradient Solver for ℓ(cid:2926 ) regularized convex optimization , and optimization error and the number of iterations 𝑇 , which will Theorem 1 ( To be proved in Section VI.A ) : If 𝑟(𝑤 ) = ( cid:3097)(cid:2870)((cid:3043))‖𝒘‖(cid:3043)(cid:2870 ) , 𝑔(cid:3047)(𝑤)= ( cid:3040)∑ 𝑙(〈𝒘,𝜙(𝜽𝒊)〉;𝜽𝒊 )
Notice that the calculation in Figure 2 gains a speed up in estimated the running time in terms of the number of iterations . Now we state the correlation between the give us a running time in terms of the optimization error ( see Figure 3 ) .
In the previous sub section , we introduced a Primal
B . Inverse Dependency on Training Data Size degenerates to 1 :
( cid:3040)(cid:2880 )
( 10 )
, the loss then satisfies the convexity and Lipschitz continuity , if
( 11 ) theorem ,
Based on the above the endurable logarithmic factors .
Figure 4 : Inverse time dependency with fixed generalization loss the predictor optimized by the Primal Gradient Solver . If the predictor , optimized by our Primal Gradient Solver , the most immediate reflection of its accuracy is the generalization
The following theorem actually bases on Theorem 1 to further give us a correlation between the generalization error and the number of iterations .
∀𝛿∈(0,1 ) , with probability of at least 1−𝛿 over the choices of 𝐴,…𝐴 and the index 𝑖 , we have : 𝐹(cid:3552)(cid:3097)(𝒘𝒊)≤𝐹(cid:3552)(cid:3097)(𝒘(cid:3549))+𝐶log𝑇𝜎𝑇𝛿 optimization error is 𝜖(cid:3028)(cid:3030)(cid:3030 ) , and satisfies 𝐹(cid:3552)(cid:3097)(𝒘𝒊)≤𝐹(cid:3552)(cid:3097)(𝒘(cid:3549))+ 𝜖(cid:3028)(cid:3030)(cid:3030 ) , the algorithm needs 𝑇=𝑂(cid:3560)(cid:4672 ) ( cid:3097)(cid:3083)(cid:3106)(cid:3276)(cid:3278)(cid:3278)(cid:4673 ) iterations ignoring The optimization error 𝜖(cid:3028)(cid:3030)(cid:3030 ) functions as a bridge to the study of the generalization error . We state that if 𝒘(cid:3557 ) is some error 𝜖 . In some other words,∀𝒘𝟎∈𝑆 , 𝑙(𝒘(cid:3557))−𝑙(𝒘𝟎)≤𝜖 . Theorem 2 ( To be proved in Section VI.B ) : Suppose 𝒘(cid:3557 ) is desired error rate 𝜖 obeys 𝑙(𝒘(cid:3557))≤𝑙(𝒘𝟎)+𝜖 ,∀𝒘𝟎∈𝑆 , then 1/𝛿 ‖𝒘𝟎‖(cid:3291)(cid:3118 ) −𝑂(cid:3560)(cid:4672)(cid:3040)(cid:4673)(cid:3442 ) ( cid:2870)(cid:3106)(cid:3118)((cid:3043 ) ) Choosing3 𝑘=1 and integrating ( 12 ) into the complexity • 𝑝=2 , the time complexity is 𝑂(cid:3438 ) 𝒘𝟎(cid:3291)(cid:3118 ) ( cid:3016)(cid:3560)(cid:4672)(cid:3117)(cid:3288)(cid:4673)(cid:3442 ) ( cid:3031)/(cid:3083 ) ( cid:3118)(cid:3354)(cid:3118)((cid:3291)(cid:3127)(cid:3117 ) ) • 𝑝∈(1,2 ) , the time complexity is 𝑂(cid:3438 ) 𝒘𝟎(cid:3291)(cid:3118 ) ( cid:3016)(cid:3560)(cid:4672)(cid:3117)(cid:3288)(cid:4673)(cid:3442 ) /(cid:3083 ) ( cid:3118)(cid:3354)(cid:3118)((cid:3291)(cid:3127)(cid:3117 ) ) from above , decreases as the sample count 𝑚 increases . This special case of 𝑝=2 with the SVM hinge loss . 3 We will discuss how to choose the best 𝑘 in the Section VII . is called the property of inverse time dependency on the training data size . This conclusion confirms the theoretical result in [ 11 ] which proves the inverse dependency in the
As illustrated in Figure 4 , the time complexity derived of the Primal Gradient Solver , we conclude that : the required number of iterations satisfies :
𝑇=𝑂(cid:3438 )
( 12 )
670
We state that this result comes from the perfect wedding of the following two : when the number of training samples increases • We expect a smaller gap between the empirical • We approximate the loss function more accurately objective and the generalization objective . using the random sampling .
IV . APPLICATIONS inequality we deduce that demonstrations of the loss functions .
• The SVM hinge loss : • The Logistic loss : we consider the Lipschitz continuity of the Least Square loss . It can be checked this property does not hold in the entire
The convexity of the three loss functions above and the Lipschitz continuity of first two loss functions can be easily
In this section we utilize the Primal Gradient Solver on three specific loss functions . We first consider the binary
If we consider the regression problem with instance• The Least Square loss : classification problem with instance label pairs 𝜽=(𝒙,𝑦 ) where 𝑦∈{−1,1} , we have the following two famous 𝑙(〈𝒘,𝜽〉;𝜽)=max{0,1−𝑦〈𝒘,𝒙〉} 𝑙(〈𝒘,𝜽〉;𝜽)=log(cid:3435)1+𝑒(cid:3052)〈𝒘,𝒙〉(cid:3439 ) value pairs 𝜽=(𝒙,𝑦 ) where 𝑦∈ℝ , we have 𝑙(〈𝒘,𝜽〉;𝜽)=(〈𝒘,𝒙〉−𝑦)(cid:2870 ) verified mathematically , wrt the entire space 𝑆=ℝ . Now space 𝑆=ℝ , but we may constrain the space to 𝑆= ( cid:3419)𝑤:‖𝑤‖(cid:3043)≤𝐶(cid:3423 ) . For any 𝒘𝟏,𝒘𝟐∈𝑆 , using Hölder's 𝑙(〈𝒘𝟏,𝜽〉;𝜽)−𝑙(〈𝒘𝟐,𝜽〉;𝜽 ) =〈𝒘𝟏−𝒘𝟐,𝒙〉(〈𝒘𝟏,𝒙〉+〈𝒘𝟐,𝒙〉−2𝑦 ) ≤‖𝒘𝟏−𝒘𝟐‖(cid:3043)‖𝒙‖(cid:3044)(cid:3435)2𝐶‖𝒙‖(cid:3044)+2|𝑦|(cid:3439)≤‖𝒘𝟏−𝒘𝟐‖(cid:3043)∙𝐿 is fixed and thus ‖𝒙‖(cid:3044 ) and |𝑦| are naturally bounded . All we 𝒘(cid:3549 ) must lie in 𝑆=(cid:3419)𝒘:‖𝒘‖(cid:3043)≤𝐵(cid:3423 ) . This is because 𝑟(𝒘∗)≤ 𝐹(cid:3097)(𝒘∗)≤𝐹(cid:3097)(0)≤(max|y|)(cid:2870 ) is bounded , where max|y| is the upper bound for |𝑦| . write down 𝝀𝒕 : 𝜆(cid:3047)= |(cid:3002)(cid:3295)|∑ 𝑦∙𝒙 ( 𝒙,(cid:3052))∈(cid:3002)(cid:3295),(cid:3052)〈𝒙,𝒘𝒕〉(cid:2996 ) 𝜆(cid:3047)= |(cid:3002)(cid:3295)|∑ 𝒙 ( cid:3052)∙(cid:3032)(cid:3127)(cid:3300)〈𝒙,𝒘𝒕〉 ( 𝒙,(cid:3052))∈(cid:3002)(cid:3295 ) ( cid:3032)(cid:3127)(cid:3300)〈𝒙,𝒘𝒕〉 2(〈𝒘,𝒙〉−𝑦)𝒙 𝜆(cid:3047)= |(cid:3002)(cid:3295)|∑ ( 𝒙,(cid:3052))∈(cid:3002)(cid:3295 ) restrict 𝑆 to some bounded sphere just like we did for the
Therefore , our solver can be properly adapted to these three loss functions . Note that the Lipschitz continuity of the loss function is an important requirement in the deduction ( see
Considering the algorithmic framework in Figure 1 , we • SVM hinge loss : the last inequality holds for the reason that the sample space left to do is to further verify the empirical optimum solution
Section VI ) . If this requirement is not met , we need to
• Least Square loss :
• Logistic loss :
Least Square loss . We emphasize that the introduction of
Acc>93 % QN LR Acc>94 % PGS LR Acc>94.5 % PGS LR Acc>94 % PGS 1.8LR g n i n i a r t n i s d n o c e S
80 70 60 50 40 30 20 10 0
0 x 100000
8
4 optimal σ .
2 Number of training samples
6 functions to be included as loss functions .
Figure 5 : Running time required to achieve given accuracy on CCAT for bounded 𝑆 enables more kinds of convex and continuous Taking the SVM loss with 𝑝=2 and 𝑆=(cid:3419)𝒘:‖𝒘‖≤ 1/√𝜎} as an example , our solver immediately gives the accuracy bound depends on the boundedness of 𝑆 . However , in 𝑆=ℝ(cid:2924 ) case our algorithm can still run efficiently . It algorithm called PEGASOS [ 10 ] . In that paper the proof of the we used a slightly different Lemma 1 which tells us that even answers the question in footnote 2 of [ 11 ] on why the projection step can be skipped .
V . EXPERIMENTS in
In this section we further strengthen our theoretical result proposed the experimental results . We test our solver in three regularizer loss pairs : ℓ(cid:2870) Logistic , ℓ.(cid:2876) Logistic and ℓ(cid:2870) LeastSquare . We do not use the SVM loss here since its ℓ(cid:2870) norm counterpart the previous section by presenting has been well tested in [ 11 ] . All the following works are conducted on a computer with 2.4 GHz AMD Opteron Processor 852 and 32G RAM . We first introduce the dataset in the experiments : • The binary classification set CCAT retrieved from RCV1 collection [ 5 ] . We used 781,265 samples in training and performed prediction on 23,149 testing samples . A total of 47,236 features are in this dataset and with sparsity 016 % • Three toy binary classification sets with 200,000 samples are used where the number of features is 10 , 20 , and 40 separately . The samples with positive label and with negative label are generated from two Gaussian distributions with different means but the same covariance . Thus , the optimal separating plane is a linear function characterized by a unit vector 𝒘∗ , returns a unit predictor 𝒘 , we will use the error ‖𝒘∗−𝒘‖(cid:2870 ) to verify its correctness . count 𝑚 , we first choose an optimal σ(𝑚 ) according to the
Throughout this section , for a given training sample and can be pre calculated . Assume the program maximal achievable accuracy on the testing set , and then rerun the program to retrieve the required running time to obtain some benchmark accuracies , like 93 % , 94 % , etc . We
671 g n i n i a r t n i s d n o c e S
200
150
100
50
0
2
3
Acc>94.525 % Acc>94.55 % Acc>94.575 % Acc>94.6 % Acc>94.625 % Acc>94.65 %
7 x 100000
4
6 Number of training samples
5
Figure 6 : Inverse dependency experiment of 2 norm logistic regression , on
CCAT dataset
600
450
300
150
0
1.5
2.5
Acc>94.525 % Acc>94.55 % Acc>94.575 % Acc>94.6 % Acc>94.625 % Acc>94.65 % g n i n i a r t n i s d n o c e S s n o i t a r e t i f o r e b m u N
672 g n i n i a r t n i s d n o c e s f o r e b m u N
200
150
100
50
0
1.5
2.5
Acc>94.525 % Acc>94.55 % Acc>94.575 % Acc>94.6 % Acc>94.625 % Acc>94.65 %
3.5
4.5
5.5
Number of training samples
6.5
7.5 x 100000
( PGS ) in Section VII . square , on CCAT dataset
Figure 7 : Inverse dependency experiment of 2 norm regularized least
In the first experiment we compare our Primal Gradient is not scientific [ 4 ] . See further discussion in Section VII for details .
Solver Regression ( LR ) against the L BFGS Quasi Newton ( QN ) method [ 7 ] for LR . The latter has been proved to be superior remark here that choosing a best 𝜎 according to the test data for ℓ(cid:2870 ) and ℓ.(cid:2876 ) regularized Logistic in training large scale ℓ(cid:2870) regularized Logistic Regression by [ 6 ] . In PGS , we choose 𝑘=1 for 𝑝=2 and 𝑘=300 for 𝑝=18∈(1,2 ) The reason for this selection is discussed Solver does not increase as the sample size 𝑚 increases , for both 𝑝=2 and 𝑝=18∈(1,2 ) Although QN can achieve 94.5 % , its running time is above 600 seconds and we ignore of iterations inversely dependent on 𝑚 . However , because each iteration in QN has a time complexity related to 𝑚 , the number of iterations inversely dependent on 𝑚 , the time on 𝑚 . It is the combination of these two properties that total running time of QN still increases . On the contrary , PGS is profited by its stochastic behavior . Not only its it in Figure 5 for the sake of simplicity . It is worth noting that , in the experiment of QN , we also discover the number
As one can see from Figure 5 , except for the QuasiNewton algorithm , the running time of our Primal Gradient an accuracy of the same level as PGS , namely , higher than complexity of a single iteration in PGS is also independent contributes to the final inverse time dependency .
4.5
3.5 Number of training samples
5.5
6.5
7.5 x 100000
Figure 8 : Inverse dependency experiment of 1.8 norm logistic regression , on CCAT dataset
Toy10 Toy20 Toy40
10
0 0 0 0 1 x
7.5
5
2.5
0
0
0.5
1
1.5
Figure 9 : Inverse dependency experiment of 2 norm logistic regression , on
Number of training samples toy dataset
2 x 100000
In the second experiment we check the inverse time dependency for different sets of regularizer loss pairs against both CCAT data and our toy data . We run our program against a set of distinct sample sizes and record the number of seconds required to reach each accuracy benchmark . Due to the randomness of our Primal Gradient Solver we test our program at least 20 times and choose the median . Notice that although Equation ( 12 ) theoretically studies an upper bound in the training time , the decreasing of upper bound does not directly suggest the real time inverse dependency . Nevertheless , the experimental results in Figure 6 , Figure 7 , Figure 8 and Figure 9 all confirm the property in ( 12 ) .
Similar to the first experiment , we set for ℓ(cid:2870) norm 𝑘=1 and for ℓ.(cid:2876 ) norm 𝑘=300 . The median of 20 runs are used the number of iterations required for PGS of ℓ(cid:2870 ) Logistic Regression to train our toy data to achieve an error ‖𝒘∗− 𝒘‖𝟐 of 005 The median of 150 runs are used . We state that independent on the number of training samples 𝑚 , so we use the time complexity at each iteration is constant and for Figure 6 , Figure 7 and Figure 8 . Figure 9 demonstrates the number of iterations to be the y axis for a better illustration in Figure 9 .
In the third experiment , we test our program in CCAT dataset against the optimal solution generated by QuasiNewton algorithm . We run the QN program with sufficient
Regularizer
TABLE II . THE RUNNING TIME AND ACCURACY OF OUR PRIMAL GRADIENT SOLVER USING AN OPTIMAL 𝝈 ON CCAT . ℓ(cid:2870 ) ℓ.(cid:2876 ) ℓ(cid:2870 )
Optimal 𝝈 QN Accuracy
PGS Accuracy 094735±000042 094763±000035 094615±000060
0.94799 0.94808 0.94687 The program has been run 20 times and the accuracy is given by “ median ± standard ” deviation in the table .
LogisticRegression LogisticRegression
55sec 576sec 22sec
PGS Training Time
1E 6 4E 7 2E 5
Least Square
Loss
( 17 )
( 16 )
Line 9 of Figure 1 . the proof of theorem 1 .
We now start to calculate the expected optimization error ,
Now incorporating the Markov inequality , we provide substituting them into ( 16 ) and using the result of Lemma 1 we have
Proposition 1 , the ℓ(cid:3044) norm of 𝝀𝒕∈𝜕𝑔(cid:3047)(𝒘𝒕𝟏 ) is bounded , arriving at our conclusion . ∎ based on the iid selection of subsets 𝐴,…𝐴 and the 𝒘𝒊 in 𝔼[𝜖(cid:3028)(cid:3030)(cid:3030)]=𝔼(cid:3002)(cid:3117),…,(cid:3002)(cid:3269)𝔼(cid:3000)(cid:3000)(cid:3427)𝐹(cid:3552)(cid:3097)(𝒘𝒊)(cid:3431)−𝐹(cid:3552)(cid:3097)(𝒘(cid:3549 ) ) where the empirical optimum 𝒘(cid:3549)=argmin𝐰∈S𝐹(cid:3097)(cid:3553)(𝒘 ) Using a similar technique from [ 10 ] , we state that 𝔼(cid:3002)(cid:3117),…,(cid:3002)(cid:3269)𝔼(cid:3000)(cid:3000)(cid:3427)𝐹(cid:3552)(cid:3097)(𝒘𝒊)(cid:3431)=𝔼(cid:3002)(cid:3117),…,(cid:3002)(cid:3269)𝔼(cid:3000)(cid:3000)[𝑐(𝒘𝒊 ) ] and 𝐹(cid:3552)(cid:3097)(𝒘(cid:3549))=𝔼(cid:3002)(cid:3117),…,(cid:3002)(cid:3269)(cid:3429)1𝑇min𝒘∈(cid:3020)(cid:3533)𝑐(cid:3047)(𝒘 ) ( cid:3433 )
( cid:3047)(cid:2880 ) 𝔼[𝜖(cid:3028)(cid:3030)(cid:3030)]≤𝐶log𝑇𝜎𝑇 Proof of Theorem 1 : The random variable 𝜖(cid:3028)(cid:3030)(cid:3030)=𝐹(cid:3097)(cid:3553)(𝒘𝒊)− 𝐹(cid:3097)(cid:3553)(𝒘(cid:3549))≥0 is non negative , and we have 𝔼[𝜖(cid:3028)(cid:3030)(cid:3030)]≤(cid:3004)(cid:2922)(cid:2925)(cid:2917)(cid:3097 ) , ( cid:4681)∙𝔼[𝜖(cid:3028)(cid:3030)(cid:3030)]𝛿 Pr(cid:4680)𝜖(cid:3028)(cid:3030)(cid:3030)≥𝔼[𝜖(cid:3028)(cid:3030)(cid:3030)]𝛿 ≤𝔼[𝜖(cid:3028)(cid:3030)(cid:3030 ) ] ⇒Pr(cid:3428)𝜖(cid:3028)(cid:3030)(cid:3030)≤𝐶log𝑇𝜎𝑇𝛿 ( cid:3432)≥1−𝛿 1−𝛿 , we have 𝜖(cid:3028)(cid:3030)(cid:3030)≤(cid:3004)(cid:2922)(cid:2925)(cid:2917)(cid:3097)(cid:3083 ) . This immediately gives us the statement . ∎ 𝑙(𝒘(cid:3557))−𝑙(𝒘𝟎 ) =(cid:3435)𝐹(cid:3097)(𝒘(cid:3557))−𝐹(cid:3097)(𝒘∗)(cid:3439)+(cid:3435)𝐹(cid:3097)(𝒘∗)−𝐹(cid:3097)(𝒘𝟎)(cid:3439 ) 𝜎2(𝑝−1)‖𝒘(cid:3557)‖(cid:3043)(cid:2870)+ 𝜎2(𝑝−1)‖𝒘𝟎‖(cid:3043)(cid:2870 ) − here 𝒘(cid:3557 ) is the solution given by our Primal Gradient Solver , optimum 𝒘∗=argmin𝒘∈(cid:3020)𝐹(cid:3097)(𝒘 ) generalization loss 𝑙(𝒘)=𝔼𝜽~(cid:3005)[𝑙(〈𝒘,𝜽〉;𝜽) ] .
B . Proof of Theorem 2 Proof of Theorem 2 : Following [ 12 ] , we decompose the generalization loss into four parts :
The above inequality shows that with probability at least then using the Markov inequality population and
( 19 )
( 18 )
, number of iterations to reach the convergent solution that minimizes the objective ( it takes more than 2 hours ) . Results in TABLE II . indicate that our Primal Gradient Solver can obtain the accuracy on the same level as Quasi Newton , while the training time is within 1 minute for ℓ(cid:2870 ) regularized ones , and within 10 minutes for the ℓ.(cid:2876 ) regularized one .
VI . PROOF OF THE MAIN THEOREMS
In this section we put forward the detailed proofs of the two theorems in Section III.B , using the best known logarithmic regret [ 4 ] for online convex optimization [ 15 ] , and the Oracle inequality in decomposing generalization loss [ 12 ] . A . Proof of Theorem 1
According to ( 8 ) , our temporal objective at iteration 𝑡 is We state that 𝑟(𝒘)=‖w‖(cid:2926)(cid:2870)/2(p−1 ) is 1 strongly convex ( Example 1 ) and a 𝑔(cid:3047)(𝒘 ) is convex according to our requirement to 𝑙 . This suggests 𝑐(cid:3047)(𝒘 ) be σ strongly convex ,
𝑐(cid:3047)(𝒘)=𝜎∙𝑟(𝒘)+𝑔(cid:3047)(𝒘 ) given by
( 13 ) based on the additivity in [ 8 ] .
We next examine the counterpart of our problem in online convex optimization , introduced by [ 15 ] . In such problem , the ultimate purpose is to minimize the regret
𝑟𝑒𝑔𝑟𝑒𝑡≔(cid:3533)𝑐(cid:3047)(𝒘𝒕 )
( cid:3047)(cid:2880 )
−min(cid:2933)∈S(cid:3533)𝑐(cid:3047)(𝒘 )
( cid:3047)(cid:2880 ) algorithm defined in Figure 1 satisfies :
The following lemma gives a bound for the regret of our Primal Gradient Solver ( Figure 1 ) . Its proof can be seen in Theorem 2 in [ 4 ] .
Lemma 1 : Let 𝑐,…𝑐 be a sequence of σ strongly convex functions over some convex domain 𝑆 wrt the some norm ‖.‖(cid:3043 ) . Assume ‖.‖(cid:3044 ) is the dual norm of ‖.‖(cid:3043 ) , then the ( cid:3533)𝑐(cid:3047)(𝒘𝒕 ) −min𝒘∈(cid:3020)(cid:3533)𝑐(cid:3047)(𝒘 )
( cid:3047)(cid:2880 ) ( cid:3047)(cid:2880 ) Corollary 1 : If 𝑐(cid:3047 ) is defined according the requisites of the by ( cid:3004)(cid:2922)(cid:2925)(cid:2917)(cid:3097 ) , where 𝐶 is a constant . Proof : This boundedness is ensured if ‖𝝀𝒕‖(cid:3044)(cid:2870 ) is bounded by constant . Recall the Lipschitz continuity for 𝑙(〈𝒘,𝜽〉;𝜽 ) , which infers the Lipschitz continuity for 𝑔(cid:3047 ) . Based on
≤12(cid:3533)‖𝝀𝒕‖(cid:3044)(cid:2870 )
𝑡𝜎 ( cid:3047)(cid:2880 )
Primal Gradient Solver , the above regret is further bounded
( 15 )
( 14 )
673
The second and third term of equation ( 19 ) is nonpositive , while the first term , the generalization objective difference , can be further bounded by the empirical objective difference according to the main result in [ 12 ] . Combining the results along with the optimization accuracy studied in the previous section ( Theorem 1 ) , we arrive at the following inequality
𝑙(𝒘(cid:3557))−𝑙(𝒘𝟎 ) ≤𝑂(cid:3560)(cid:3436 ) 1𝜎𝑇𝛿(cid:3440)+ If we choose 𝜎=Θ(cid:3561)(cid:4678)(cid:3495)(cid:2870)((cid:3043 ) ) 𝑙(𝒘(cid:3557))−𝑙(𝒘𝟎 ) ≤𝑂(cid:3560)(cid:4684)‖𝒘𝟎‖(cid:3043)(cid:3496 ) Let 𝜖 equal to the right side of this inequality , we immediately arrive at Theorem 2 . ∎
𝜎2(𝑝−1)‖𝒘𝟎‖(cid:3043)(cid:2870)+𝑂(cid:3560)(cid:3436)1𝜎𝑚(cid:3440 ) ‖𝒘𝟎‖(cid:3291)(cid:3118)(cid:3436)(cid:3083)+𝑂(cid:3560)(cid:4672)(cid:3040)(cid:4673)(cid:3440)(cid:4679 ) , the right 12(𝑝−1)(cid:4678)1𝑇𝛿+𝑂(cid:3560)(cid:3436)1𝑚(cid:3440)(cid:4679)(cid:4685 ) hand side is bounded as following :
( 20 )
( 21 )
VII . FURTHER DISCUSSION example : multiple regularizer learning . now are also suitable to be regularizers . In this paper we
( for instance , TABLE II . ) . The reason is still unknown and
In this section we dialectically analyze the limitation of our Primal Gradient Solver and propose some enhancements . We also discuss some problems raised in the previous sections . Why p norm ? In the Primal Gradient Solver , the strong convexity is a core requisite to ensure the convergence rate of 𝟏/𝑻 . However , few strongly convex functions found up to examined the squared 𝒑∈(𝟏,𝟐 ] norms , and experimental results show that 𝒑=𝟏.𝟖 does slightly better than others the choice of 𝒑 may open an interesting field to study , for We notice that 𝒑=𝟏 is not included in this paper for the 𝒓(𝒘)=∑ |𝒘𝒊|𝐥𝐨𝐠|𝒘𝒊| 𝒏𝒊(cid:2880)𝟏 under the assumption that 𝒘 is a linear predictor . When 𝑝=2 , a common technique is to construct a mapping 𝜙 that calculation of 〈𝒘,𝜙(𝒙)〉 needs a traverse on the support vector by〈𝒘,𝜙(𝒙)〉=∑(cid:3435)𝛼𝒦(𝒙𝒊,𝒙)(cid:3439 ) reason that 1 norm itself has a poor convexity . However , 1norm has the often desired property of reducing the number of active features . In [ 8 ] [ 9 ] , Shai proposed a substitute that is strongly convex , which also works well in Primal Gradient Solver with advantages in feature selection . The adoption of Kernel . All the works above are verified maps from the feature space to the Reproducing Kernel Hilbert Space ( RKHS ) space , availing us a non linear separator . We emphasize that our Primal Gradient Solver can be slightly adjusted to cater for this assumption , as the
. However , due to the complexity cost for this inner product , the inverse time dependency property no longer holds . In a counterpart of this paper [ 14 ] we studied the performance of such kernel PGS ,
674 and the result shows that even without the inverse time dependency , the algorithm overwhelms the state of the art in both efficiency and accuracy . Incorporate with a biased term . In our algorithm defined above , we have ignored the biased term in the general loss 𝑙 . biased term to the loss function like log(cid:3435)1+𝑒(cid:3052)〈𝒘,(cid:3109)(𝒙)〉(cid:3029)(cid:3439 ) ,
The most efficient way to compensate for it is to add this and at the same time modify the regularizer to
12(𝑝−1)(cid:3435)‖𝑤‖(cid:3043)(cid:3043)+𝑏(cid:3043)(cid:3439)/(cid:3043 ) optimization problem [ 4 ] . in Section III.A , if the complexity of Primal Gradient Solver keeps the complexity unchanged but boosts the confidence while the time complexity remains the same and the accuracy is raised . used within the analysis . Actually , in each iteration we may use the Chernoff bound to boost the confidence and give a
Doing this allows us to preserve the strong convexity of the regularizer , but runs into a different way as the normal regularizer without this bias term . If we consistently ignore this term in the regularizer , the convergence rate of our solver will reduce to 𝑂(cid:3435)1/√𝑇(cid:3439 ) like a generalized convex The selection of parameter 𝒌 . From the above discussion we can see the number of selected samples 𝑘=|𝐴(cid:3047)| is never better bound than 𝑇=𝑂(cid:3560)(cid:4672 ) ( cid:3097)(cid:3083)(cid:3106)(cid:4673 ) . Both theoretical analysis and experimental results show that in the 𝑝=2 case it is worthless to set 𝑘>1 ; as an alternative , we may choose one single sample each iteration and do 𝑘∙𝑇 iterations in total However , for 𝑝∈(1,2 ) it is not the case . As mentioned is 𝑂(cid:3435)𝑇(𝑑𝑘+𝑛)(cid:3439 ) , we had better choose 𝑘=𝑂(𝑛/𝑑 ) which significantly . For the RCV1 dataset where 𝑛=47,236 and 𝐷≈40 , we may choose 𝑘=300 , which greatly reduces the investigate the influence of 𝑘 more theoretically in the future . The selection of weight 𝝈 . According to Eq ( 12 ) , the running time depends on an unknown vector 𝒘𝟎 that is the optimal predictor in training . Similarly , the choice of 𝜎 also depends on 𝒘𝟎 and we never have such a priori knowledge actually know 𝜎 we cannot run the program at all . Due to of the Primal Gradient Solver that will make 𝜎 self adapted . the ℓ(cid:3043) norm regularized convex learning problems that can is proved to be 𝑂(𝑑/𝜖(cid:3028)(cid:3030)(cid:3030)𝛿𝜎 ) for 𝑝=2 and 𝑂(𝑛/𝜖(cid:3028)(cid:3030)(cid:3030)𝛿𝜎 ) for 𝑝∈(1,2 ) , where 𝛿 is the confidence parameter , 𝜎 is the regularization parameter , 𝑑 is the average number of non deal with any loss satisfying the convexity and Lipschitz continuity , including the famous SVM loss , Logistic loss and Least Square loss . For all of them the expected running time number of required iterations . Experimental results in Section V have confirmed this analysis and we will on how to choose it . A validation set does not work because we are optimizing the running speed and not until we this reason , we are currently working on a modified version
In this paper we analyzed a Primal Gradient Solver for
VIII . SUMMARY zero features for a sample , 𝑛 is the dimension size of the feature space , and 𝜖(cid:3028)(cid:3030)(cid:3030 ) is the desired optimization error . within 10 seconds for 𝑝=2 , and 20 seconds for 𝑝=1.8 ,
Experimental results on CCAT dataset in Reuters Corpus Volume 1 ( RCV1 ) show that our Primal Gradient Solver , for all of the three loss functions , approaches an accuracy of 94 % while the L BFGS Quasi Newton method needs 600 seconds to obtain the same accuracy .
The most important contribution of this paper is that , based on this Primal Gradient Solver , we proved it is not only more efficient than the traditional algorithms , but also endowed with inverse time dependency property on the number of training samples , for a fixed accuracy .
This result , confirmed by the dataset of RCV1 and three toy sets , reminds us that even a linear time algorithm might not theoretically meet the best efficiency . There might exist some algorithm , like our Primal Gradient Solver , whose time complexity is independent on the number of samples 𝑚 , and even inversely dependent on 𝑚 .
ACKNOWLEDGMENT
Zeyuan Allen Zhu wants to thank Shai Shalev Shwartz of Hebrew University for his valuable discussions . The authors also acknowledge Matt Callcut and all four anonymous reviewers for their fruitful comments . REFERENCE
[ 1 ] Léon Bottou and Olivier Bousquet , "The Tradeoffs of Large Scale
Learning," in NIPS , 2007 .
[ 2 ] Stephen Boyd and Lieven Vandenberghe , Convex Optimization , 6th ed . : Cambridge University Press , 2008 .
[ 3 ] Elad Hazan , Adam Kalai , Satyen Kale , and Amit Agarwal , "Logarithmic Regret Algorithms for Online Convex Optimization," in COLT , 2006 .
[ 4 ] Sham Kakade and Shai Shalev Shwartz , "Mind the Duality Gap : Logarithmic regret algorithms for online optimization," in NIPS , 2009 . [ 5 ] David D . Lewis , Yiming Yang , Tony G . Rose , and Fan Li , "RCV1 : A New Benchmark Collection for Text Categorization Research," Journal of Machine Learning Research , vol . 5 , pp . 361 397 , 2004 .
[ 6 ] Thomas P . Minka , "A comparison of numerical optimizers for logistic regression," Microsoft Research , Technical Report 2003 .
[ 7 ] Jorge Nocedal and Stephen J . Wright , Numerical Optimization ,
Chapter 6 7 , 2nd ed . : Springer .
[ 8 ] Shai Shalev Shwartz , "Online Learning : Theory , Algorithms , and applications," The Hebrew University , PhD Thesis 2007 .
[ 9 ] Shai Shalev Shwartz and Yoram Singer , "Logarithmic Regret Algorithms for Strongly Convex Repeated Games," The Hebrew University , Technical Report 2007 .
[ 10 ] Shai Shalev Shwartz , Yoram Singer , and Nathan Srebro , "Pegasos :
Primal Estimated sub GrAdient SOlver for SVM," in ICML , 2007 .
[ 11 ] Shai Shalev Shwartz and Nathan Srebro , "SVM Optimization : Inverse
Dependence on Training Set Size," in ICML , 2008 .
[ 12 ] Karthik Sridharan , Nathan Srebro , and Shai Shalev Shwartz , "Fast
Rates for Regularized Objectives," in NIPS , 2008 .
[ 13 ] Tong Zhang , "Solving Large Scale Linear Prediction Problems Using
Stochastic Gradient Descent Algorithms," in ICML , 2004 .
[ 14 ] Zeyuan Allen Zhu , Weizhu Chen , Gang Wang , Chenguang Zhu , and Zheng Chen , "P packSVM : Parallel Primal grAdient desCent Kernel SVM," in ICDM , 2009 .
675
[ 15 ] Martin Zinkevich , "Online convex programming and generalized infinitesimal gradient ascent," in ICML , 2003 , pp . 928 936 .
( 22 )
APPENDIX and differentiable and
Its proof can be seen from Lemma 6 of [ 9 ] .
Lemma 2 : Let 𝑓 be a closed and strongly convex function to norm ‖.‖ , then 𝑓∗ is over 𝑆⊂ℝ with respect ∇𝑓∗(𝜽)=argmax 〈𝒘,𝜽〉−𝑓(𝒘 ) 𝒘∈(cid:3020 ) Theorem 3 : If 𝑆=(cid:3419)𝒘:‖𝒘‖(cid:3043)≤𝐵(cid:3423 ) and 𝑓(𝒘)=(cid:2870)‖𝒘‖(cid:3043)(cid:2870 ) , let ( cid:3043)+(cid:3044)=1 , then 12‖𝜽‖(cid:3044)(cid:2870 ) , ‖𝜽‖(cid:3044)≤𝐵 𝑓∗(𝜽)=(cid:3422 ) 12‖𝜽‖(cid:3044)(cid:2870)−12(cid:3435)𝐵−‖𝜽‖(cid:3044)(cid:3439)(cid:2870 ) , ‖𝜽‖(cid:3044)>𝐵 ( 23 ) ( cid:3435)∇𝑓∗(𝜽)(cid:3439)=min{𝐵,‖𝜽‖(cid:3044)} 𝜃(cid:3044)/(cid:3043 ) ‖𝜽‖(cid:3044)(cid:3044)/(cid:3043 ) Proof : For any given 𝜽 , using the Hölder's inequality we have 〈𝒘,𝜽〉≤‖𝒘‖(cid:3043)∙‖𝜽‖(cid:3044 ) . Subtracting both sides of them by ( cid:2870)‖𝒘‖(cid:3043)(cid:2870 ) , we have 〈𝒘,𝜽〉−12‖𝒘‖(cid:3043)(cid:2870)≤‖𝒘‖(cid:3043)∙‖𝜽‖(cid:3044)−12‖𝒘‖(cid:3043)(cid:2870 ) =−12(cid:3435)‖𝒘‖(cid:3043)−‖𝜽‖(cid:3044)(cid:3439)(cid:2870)+12‖𝜽‖(cid:3044)(cid:2870 ) and substituting the definition of 𝑓∗ , ie Eq ( 6 ) : 𝑓∗(𝜽)=𝑠𝑢𝑝𝒘∈(cid:3020)〈𝒘,𝜽〉−12‖𝒘‖(cid:3043)(cid:2870 ) ≤𝑠𝑢𝑝𝒘∈(cid:3020)(cid:3436)−12(cid:3435)‖𝒘‖(cid:3043)−‖𝜽‖(cid:3044)(cid:3439)(cid:2870)+12‖𝜽‖(cid:3044)(cid:2870)(cid:3440 ) 12‖𝜽‖(cid:3044)(cid:2870 ) , ‖𝜽‖(cid:3044)≤𝐵 =(cid:3422 ) 12‖𝜽‖(cid:3044)(cid:2870)−12(cid:3435)𝐵−‖𝜽‖(cid:3044)(cid:3439)(cid:2870 ) , ‖𝜽‖(cid:3044)>𝐵 explicit construction of 𝒘 that satisfies the equality sign in Hölder's inequality . For a given norm 𝐶=min{𝐵,‖𝜽‖(cid:3044)} , we construct 𝒘∗ by letting 𝑤∗= ( cid:3004)‖(cid:3087)‖(cid:3292)(cid:3292)/(cid:3291)𝜃(cid:3044)/(cid:3043 ) . It can be easily checked that ‖𝒘∗‖(cid:3043)=𝐶 and 〈𝒘∗,𝜽〉=‖𝒘∗‖(cid:3043)∙‖𝜽‖(cid:3044 ) , and the latter holds because ( (𝑤∗)(cid:3043),…(𝑤∗)(cid:3043 ) ) and ( cid:3435)𝜃(cid:3044),…𝜃(cid:3044)(cid:3439 ) are linear dependent . We remark here that we use 𝑥(cid:3043 ) for the abbreviation of |𝑥|(cid:3043)∙sgn𝑥 . This suffices to prove the
Actually , the equality of Eq ( 26 ) holds , because of an
( 24 )
( 25 )
( 26 ) equality of Eq ( 26 ) , ie Eq ( 23 ) .
2 :
Corollary
Regarding Eq ( 24 ) , we adopt Lemma 2 and see that
∇𝑓∗(𝜽)=argmin𝐰∈S(cid:3435)‖𝒘‖(cid:3043)−‖𝜽‖(cid:3044)(cid:3439)(cid:2870)=𝒘∗ . ∎ and 𝑓(𝒘)= If 𝑆=(cid:3419)𝒘:‖𝒘‖(cid:3043)≤𝐵(cid:3423 ) ( cid:2870)((cid:3043))‖𝒘‖(cid:3043)(cid:2870 ) , let ( cid:3043)+(cid:3044)=1 , then 𝑓∗(𝜽 ) 12(𝑞−1)‖𝜽‖(cid:3044)(cid:2870 ) , ‖(𝑝−1)𝜽‖(cid:3044)≤𝐵 ⎩⎪⎪⎨⎪⎪⎧ 12(𝑞−1)‖𝜽‖(cid:3044)(cid:2870)− = ‖(𝑝−1)𝜽‖(cid:3044)>𝐵 ( 27 ) 12(𝑝−1)(cid:3435)𝐵−‖(𝑝−1)𝜽‖(cid:3044)(cid:3439)(cid:2870 ) , ( cid:3435)∇𝑓∗(𝜽)(cid:3439)=min{𝐵,‖(𝑝−1)𝜽‖(cid:3044)} 𝜃(cid:3044)/(cid:3043 ) ‖𝜽‖(cid:3044)(cid:3044)/(cid:3043 ) Proof : Assume 𝑔(𝒘)=(cid:2870)‖𝒘‖(cid:3043)(cid:2870 ) and we have known 𝑔∗ according to Theorem 3 . Now we calculate 𝑓∗ using 𝑔∗ : 𝑓∗(𝜽)=sup𝒘∈〈𝒘,𝜽〉− 1𝑝−1𝑔(𝒘 ) and we make the calculation :
This immediately gives us Eq ( 27 ) after substituting Eq
( 23 ) and noticing that 𝑝−1= ( cid:3044 ) . Next , regarding Eq ( 28 ) ,
= 1𝑝−1(cid:3436)sup𝒘∈(cid:3020)〈𝒘,(𝑝−1)𝜽〉−𝑔(𝒘)(cid:3440 ) = 1𝑝−1𝑔∗(cid:3435)(𝑝−1)𝜽(cid:3439 ) ( cid:3435)∇𝑓∗(𝜽)(cid:3439)= 1𝑝−1(cid:4672)∇𝜽𝑔∗(cid:3435)(𝑝−1)𝜽(cid:3439)(cid:4673 ) =(cid:4672)∇((cid:3043))𝜽 𝑔∗(cid:3435)(𝑝−1)𝜽(cid:3439)(cid:4673 ) =min(cid:3419)𝐵,‖(𝑝−1)𝜽‖(cid:3044)(cid:3423 ) ( 𝑝−1)(cid:3044)/(cid:3043)𝜃(cid:3044)/(cid:3043 ) ‖(𝑝−1)𝜽‖(cid:3044)(cid:3044)/(cid:3043 ) =min{𝐵,‖(𝑝−1)𝜽‖(cid:3044)} 𝜃(cid:3044)/(cid:3043 ) ‖𝜽‖(cid:3044)(cid:3044)/(cid:3043 ) completes the proof . ∎ Corollary 3 : If 𝑆=ℝ and 𝑓(𝒘)= ( cid:2870)((cid:3043))‖𝒘‖(cid:3043)(cid:2870 ) , then 𝑓∗(𝜽)= ( cid:2870)((cid:3044))‖𝜽‖(cid:3044)(cid:2870 ) and ( cid:3435)∇𝑓∗(𝜽)(cid:3439)= ( cid:3044)‖𝜽‖(cid:3044)(cid:3044)/(cid:3043)∙𝜃(cid:3044)/(cid:3043 ) where the third equality is according to Eq ( 24 ) . This
( 28 )
676
