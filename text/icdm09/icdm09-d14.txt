2009 Ninth IEEE International Conference on Data Mining
A bootstrap approach to eigenvalue correction
Anne Hendrikse , Luuk Spreeuwers and Raymond Veldhuis
Signals and Systems group
EEMCS , University of Twente
Enschede , Netherlands
Email : ajhendrikse@utwentenl
Abstract—Eigenvalue analysis is an important aspect in many data modeling methods . Unfortunately , the eigenvalues of the sample covariance matrix ( sample eigenvalues ) are biased estimates of the eigenvalues of the covariance matrix of the data generating process ( population eigenvalues ) . We present a new method based on bootstrapping to reduce the bias in the sample eigenvalues : the eigenvalue estimates are updated in several iterations , where in each iteration synthetic data is generated to determine how to update the population eigenvalue estimates . Comparison of the bootstrap eigenvalue correction with a state of the art correction method by Karoui shows that depending on the type of population eigenvalue distribution , sometimes the Karoui method performs better and sometimes our bootstrap method .
Index Terms—bootstrapping , eigenvalue correction , general statistical analysis , isotonic tree method
I . INTRODUCTION
Second order statistics are used extensively in data modeling methods . For example , in Principle Component Analysis ( PCA ) ( see [ 1] ) , the second order statistics of high dimensional data are used to find subspaces containing the strongest modes of variation . In Linear Discriminant Analysis ( LDA ) , the ratio of within class and between class variance is used to find the highest discriminating directions ( see [ 2] ) . fi
, operator .
˜x · ˜xT
Σ = E .
When applying these methods , it is usually assumed that the data generating process can be modeled with a multivariate probability function P ( x ) , where x is a multidimensional random variable . It is then assumed that P ( x ) is reasonably characterised by only the mean and second order statistics . The second order statistics of a multidimensional random variable are described by the covariance matrix , given by ˜x = x−E(x ) , where E( ) is the expectancy The covariance matrix Σ can be decomposed as Σ = E·D· ET . Here , E = [ e1 e2 . . . ep ] , with ei the ith eigenvector , and D is a diagonal matrix with the eigenvalues on the diagonal . Often the decomposition results are required instead of Σ . We denote the eigenvectors and eigenvalues of the decomposition of Σ by population eigenvectors and population eigenvalues respectively .
However , since neither P ( x ) nor Σ are known beforehand , an estimate of Σ has to be obtained from a training set . A N−1 X·XT , where commonly used estimator is given by ˆΣ = 1 X is a matrix in which each column consists of the difference of a training sample and the average of the training samples . N is the number of training samples . The decomposition results of ˆΣ are denoted by sample eigenvectors and sample eigenvalues . In a mathematical framework , the column vector consisting of the population eigenvalues is denoted by λ . The column vector consisting of the sample eigenvalues is denoted by l .
A problem of high dimensional training sets is that even though ˆΣ is an unbiased estimate of Σ , l is a biased estimate of λ . The bias becomes significant if the number of samples is of the same order as the dimensionality of the data . The bias has a negative effect on systems using these estimates as is shown for classification systems in [ 3 ] , [ 4 ] for example .
To reduce this negative effect , the sample eigenvalues could be corrected to remove the bias . In this paper we present a bootstrap approach [ 5 ] to eigenvalue correction : our approach iteratively improves eigenvalue estimates without introducing new measurements . Instead , we generate synthetic data in each iteration which we use to update the population eigenvalue estimates .
The system of eigenvalue estimation and correction is schematically represented in Figure 1 . In the figure , F represents the entire procedure of data generation and the sample eigenvalue estimation from this data . If the number of samples and the number of dimensions is large enough , F only introduces a bias on the eigenvalues as will be explained later on . Our aim is to obtain the inverse function F −1 that compensates for the bias in the sample eigenvalues . A method which applies 'F −1 to sample eigenvalues is called a correction method . We add a superscript c to symbols representing results of such a correction method .
F
Ó,ë
X
Ó^ l
^ 1F cë^
Fig 1 . Schematic representation of the bias introduction and bias correction in eigenvalue estimation .
The currently available correction methods can roughly be divided in three categories : regularisation methods , corrections based on Steins loss criterion and corrections based on theoretical descriptions of the bias . The regularisation methods are often empirical and lack a strong theoretical foundation ( See for example [ 6] ) . Correction algorithms based on Stein ’s loss
1550 4786/09 $26.00 © 2009 IEEE DOI 101109/ICDM2009111
818
Authorized licensed use limited to : UNIVERSITEIT TWENTE . Downloaded on January 29 , 2010 at 10:05 from IEEE Xplore . Restrictions apply . criterion actually introduce a new bias to reduce the criterion ( eg [ 7 ] , [ 8] ) .
For the last category bias descriptions are needed . A description of the bias for many data distributions was proved in 1995 [ 9 ] . Karoui derived a correction method based on this relation . This method therefore has a strong theoretical basis as opposed to most regularisation algorithms , it reduces the bias as opposed to Stein ’s loss criterion based methods and it has been shown to reduce bias in real experiments [ 10 ] . We therefore consider this method to be one of the state of the art methods so we compare the performance of our bootstrap correction with the performance of this method .
The overview of the remainder of the article is as follows : we start with an analysis of the eigenvalue bias in section II A . In section II B we introduce the Marˇcenko Pastur equation which describes this bias . A brief description of the Karoui correction is given in section II C . We then describe the bootstrap correction method ( section III ) and compare correction results of the two methods in section IV for a number of synthetic data sets . Section V gives a summary and conclusions .
II . EIGENVALUE BIAS ANALYSIS AND CORRECTION
A . Eigenvalue bias analysis
To find the statistics of estimators often Large Sample Analysis ( LSA ) is performed . In LSA it is assumed that the number of samples grows very large so the statistics of the estimator become a function depending solely on the number of samples , N . In this limit case the sample eigenvalues show no bias . However , for example in biometrics , the number of samples is often in the same order as the number of dimensions p or even lower and LSA cannot be used . Instead in the analysis of the statistics of the sample eigenvalues the N → γ , following limit may be considered : N , p → ∞ while p where γ is some positive constant . Analysis in this limit are denoted as General Statistical Analysis ( GSA ) [ 11 ] . In GSA the sample eigenvalues do have a bias .
Figure 2 demonstrates why the limit in GSA is needed . It shows results of three experiments . For each experiment , the population eigenvalues are chosen uniformly between 0 and 1 . While γ is kept constant at 1 5 , the number of dimensions is set to 4 , 20 and 100 in figures 2a , 2b and 2c respectively . In each figure the population eigenvalue distribution function and the sample eigenvalue distribution functions for 4 repeated experiments are given . Given a set of eigenvalues li , i = 1 . . . p , the corresponding distribution function is given by equation 1 :
Fp(l ) =
1 p u ( l − li )
( 1 ) pff i=1 here u ( l ) is the step function .
Figure 2 shows that the empirical sample distribution functions converge with increasing dimensionality . The bias in the estimates is visible because they converge to a different distribution function than the population distribution function . For low dimensional examples , the bias is only a small part
819 of the error in the estimates . The major part of the error is caused by random fluctuations of Fp(l ) .
B . Marˇcenko Pastur equation
It turns out that under certain conditions a relation between the sample eigenvalues and the population eigenvalues can be given in the GSA limit . The main proofs and conditions on the input data are given in [ 12 ] and [ 9 ] . Here we briefly repeat the main points .
The relation requires the following Stieltjes transform vp ( z ) of a distribution function based on the empirical sample eigenvalue distribution function : −1 z + γ · vp(z ) = ( 1 − γ ) dGp ( l ) l − z
( 2 ) here Gp(l ) is the empirical sample eigenvalue distribution function as given by equation 1 and z ∈ C+ . In the GSA limit , if the population eigenvalue distribution function converges to H∞(λ ) , then Gp(l ) converges weakly to a G∞(l ) such that the relation between the corresponding v∞(z ) and H∞(λ ) is given by equation 3 .
− 1 v∞(z )
= z − γ
λ · dH∞(λ ) 1 + λ v∞(z )
, z ∈ C
+
( 3 )
Because of this relation , reduction of the bias in the sample eigenvalues should be possible .
C . Karoui correction method
The Karoui method is based on the assumption that the number of samples and the number of dimensions is high enough such that the conditions given in section II B are met and equation 3 holds . It is also assumed that the density function h ( λ ) exists . The method approximates h(λ ) by a weighed sum of fixed density functions pi ( λ ) ( see equation 4 ) . In our implementation of the method we used a weighed sum of delta pulses and uniform distributions .
ˆh(λ ) = ai · pi(λ ) , ai = 1 , ai ≥ 0
( 4 ) ff ff
This approximation is then used in equation 3 after substitution of dH ( λ ) with h ( λ ) dλ . The empirical sample eigenvalue density function given by equation 1 is substituted in equation 2 to determine a set of corresponding vp ( z ) and z values . The Karoui method then determines the set of ai values which best satisfies equation 3 for this set of z values . For more details , we refer to [ 10 ] .
III . BOOTSTRAP EIGENVALUE CORRECTION METHOD
DERIVATION
(
The objective of eigenvalue correction is to find λ . However , since λ is unknown , the objective of the bootstrap correction method we propose is to find a ˆλc such that F = F ( λ ) . The general procedure of the method is given schematically in figure 3 . To start the algorithm an initial estimate of λ is needed . We use l as initial estimate . The method then performs a number of iterations , where in each iteration the steps in
ˆλc
Authorized licensed use limited to : UNIVERSITEIT TWENTE . Downloaded on January 29 , 2010 at 10:05 from IEEE Xplore . Restrictions apply .
1
0.5
0 0
F4(l )
F20(l )
1
0.5
1
0.5
F100(l )
0.5
1 l
( a ) 4 dimensions
1.5
0 0
0.5
1 l
( b ) 20 dimensions
1.5
0 0
0.5
1 l
( c ) 100 dimensions
1.5
Fig 2 . Examples of eigenvalue estimation bias toward the GSA limit . The dashed line indicates the population distribution Hp , the four solid lines are the empirical sample distribution Gp . figure 3 are performed starting from the right . In each iteration a synthetic set of white Gaussian distributed data samples Xw,n is generated with the same number of samples and the same dimensionality as the original measurement . These samples are scaled so their population eigenvalues are equal to the current estimate of the population eigenvalues ˆλc :,n , where n is the current iteration index and subscript : indicates the entire column vector . From this synthetic data set the sample covariance matrix ˆΣn is estimated . From this matrix the sample eigenvalues ˆl:,n are determined . If the cost function K = is below a threshold , the sample eigenvalues of the real data and the synthetic data are considered to be equal and therefore the current estimate of the population eigenvalues are used as final estimates of the population eigenvalues . If the cost function is not below the threshold , ˆλc
)))l − ˆl:,n
:,n is updated via update rule :
)))2
ˆλc :,n+1 = ˆλc
:,n − μ · ∂K ∂ ˆλc :,n
( 5 )
The parameter μ determines the size of the adjustment steps taken in the method . After this update a new iteration starts and the previously described steps are repeated .
^ 1F l dK dë
^ l n
^ F
^ Ón
Xw,n
^ Xn
^ c ën
Fig 3 . Schematic representation of the bootstrap eigenvalue correction
To find the derivative in the right hand side of equation 5
820 we can relate this derivative with the derivative ∂ˆl:,n ∂ ˆλc
:,n
( T · ∂ˆl:,n
∂ ˆλc :,n
∝ −
∂K ∂ ˆλc :,n l − ˆl:,n via
( 6 )
:,n
The derivative ∂ˆl:,n ∂ ˆλc can be found using the expression for ˆl:,n in equation 7 , where M is the number of synthetic data samples . Note that we use normal distributed synthetic samples although we do not know the real distribution of the data samples . However , according to the Marˇcenko Pastur equation , including the normal distribution , will have the same relation between sample eigenvalues and population eigenvalues . in the GSA limit many distributions ,
ˆln = diag n · ˆET
( 1
( 1 2 · ˆEn
ˆDc n
2 · w,n · XT
ˆDc n
1 ff M − 1
· Xw,n·
( 7 )
∂ˆl:,n ∂ ˆλc j,n
= diag
Here ˆEn are the eigenvectors of the synthetic data covariance n is a diagonal matrix with ˆλc matrix and ˆDc :,n on the diagonal . From equation 7 we find the derivative ∂ˆl:,n , given element ∂ ˆλc wise in equation 8 .
:,n fi 2 · ˆET n ·
( 1 2 · ˆEn
ˆDc n
( 1 2 · Xw,n · XT fl M − 1 w,n
·
∂ ∂ ˆλc
2 j,n j,n
( 8 )
ˆDc n
ˆDc n
∂ ∂ ˆλc
( − 1 is a matrix with zeros except for element
( 1 {j , j} , which is . Because of this last matrix , the method does not converge if one of the population eigenvalues becomes zero . We solved this problem by using l as initial estimate of λ , but replacing the zero eigenvalues with the lowest non zero eigenvalue . In our experiments no eigenvalues turned to zero during the iterations . Another solution is to use
ˆλc j,n
2
Authorized licensed use limited to : UNIVERSITEIT TWENTE . Downloaded on January 29 , 2010 at 10:05 from IEEE Xplore . Restrictions apply .
ˆλc 1
ˆλc m,1,3
ˆλc 2
ˆλc ˆλc d,1,3 m,1,2
ˆλc 3
ˆλc m,2,3 ˆλc ˆλc d,1,2 m,1,1
ˆλc 4
ˆλc d,2,3
ˆλc d,1,1
ˆλc 5
ˆλc ˆλc m,3,3 m,2,2
ˆλc 6
ˆλc d,3,3
ISOTONIC TREE WITH 6 POPULATION EIGENVALUES .
TABLE I the gradient with respect to standard deviations . However , the first option proved to give more accurate reconstructions and faster convergence .
A . Order preservation in the bootstrap correction method i,n switch value with ˆλc
When updating the eigenvalues without any constraints , oscillations may occur in which ˆλc i+1,n in each iteration . A order preserving algorithm was presented by Stein in [ 13 ] . We used an isotonic tree method , which allows blocks of eigenvalues to have a large change in value simultaneously and the method can also be implemented more efficiently . a the
The tree :
( layer layer m,2k,l+1
/2 and ˆλc isotonic method first builds top layer consists of the current population eigenvalues . Each is based on the difference and the mean lower ( above , or of mean elements of the previous m,2k,l+1 − ˆλc ˆλc ˆλc d,k,l = m,k,l = m,2k−1,l+1 m,2k−1,l+1 + ˆλc ˆλc /2 respectively . Here the first subscript of the left hand side symbol indicates whether the element is a mean ( m ) or a difference ( d ) element , the second subscript is the index of the element in the layer , and the third index is the layer index . So ˆλc d,k,l is the kth difference term on the lth layer and ˆλc m,k,l is the kth mean term on the lth layer . See the example in table I . To reconstruct the ˆλc n only the difference terms in all the layers and the final mean term are needed . d,k,l,2 = ˆλc d,k,l +μdˆλc m,1,1 + μdˆλc m,k,l−1,2 = ˆλc
The derivative in the update rule can be split into a tree a similar manner . The update is then performed per layer starting from the bottom layer . First the average term is updated via m,1,1,2 = ˆλc ˆλc m,1,1 . Then the following two steps are repeated for every layer : first , the difference terms are updated via ˆλc d,k,l . Next the mean terms on layer l−1 are updated via ˆλc d,k/2,l,2 , where the decision of doing an addition or a subtraction is based on whether k is odd or even . For k even , it may happen that ˆλc m,k,l−1,2 . In that case the mean terms k and k + 1 are updated d,k/2,l,2 · via ˆλc d,k/2,l,2+ˆλc ˆλc . The update of the population eigenvalm,1+k/2,l,2−ˆλc ˆλc ues is completed by taking ˆλc m,k,lmax,2 . In case of an odd number of mean elements on a layer , the last mean element is just copied from the lower level . m,k+1,l−1,2 = ˆλc m,k/2,l,2± ˆλc m,k+1,l−1,2 > ˆλc m,k,l−1,2 = ˆλc m,k/2,l,2 + ˆλc n+1,k = ˆλc d,1+k/2,l,2 m,k/2,l,2
IV . EXPERIMENTS AND DISCUSSION
To evaluate the performance of the bootstrap correction we generated a number of synthetic data sets with a number of
821 chosen population eigenvalues set beforehand . We corrected the sample eigenvalues measured from these sets with the bootstrap method and the Karoui method and measured the Levy distance , dL ( F , G ) = inf { > 0 : F ( x − ) − ≤ G ( x ) ≤ F ( x + ) + ,∀ x}
( 9 ) between the corrections and the population eigenvalues . Like Karoui , we calculate a score for a correction result by determining the ratio dL(G , H)/dL( ˆH c , H ) , where ˆH c is the corrected population eigenvalue distribution .
We performed the comparison with 6 different set ups . For most experiments we drew 501 samples from an 100 dimensional normal distribution . The first three experiments have the same set up as the experiments in [ 10 ] : experiment 1 ( identity ) has λk = 1,∀k , experiment 2 ( two cluster ) has λk = 1|k = 1 . . . 50 and λk = 2|k = 51 . . . 100 and experiment 3 ( Toeplitz ) has the eigenvalues of a Toeplitz matrix . In the fourth experiment ( Slope ) λk = 1 + k/100 . In the fifth experiment ( one over n ) we used λk = 1 k , a common model for eigenvalues of facial data [ 6 ] . In the sixth experiment ( undersampled slope ) we draw 201 samples for a 600 dimensional normal distribution with λk = 1 + k/600 .
By repeating the experiments collections of scores for the different set ups are obtained . These score collections are represented by the histograms in figure 4 . A large score indicates that the corrected population eigenvalue distribution and the population eigenvalue distribution are much more alike than the sample eigenvalue distribution and the population eigenvalue distribution . Therefore which ever method has the most density to the right of the histogram provides the best correction according to the Levy distance measure .
In the identity and two cluster experiments ( figures 4a and 4b ) the bootstrap method shows a steady improvement with an average score around 2 . The Karoui correction achieves much higher scores , albeit with a huge spread . In the Toeplitz experiment ( figure 4c ) and the slope experiment ( figure 4d ) , the bootstrap method has significantly higher scores . In the one over n case ( figure 4e ) , the bootstrap method also performs better , however , its scores indicate its estimate has almost the same precision as the sample eigenvalues .
In the undersampled slope case ( figure 4f ) the Bootstrap method still outperforms the Karoui method , however the difference is not as big as in the slope experiment in figure 4d . For a more detailed discussion on the results of this experiment an example repetition is shown in figure 5 , with figure 5a containing the population eigenvalue scree plots and figure 5b contains the sample eigenvalue scree plots .
There are large differences between the population eigenvalues . The Karoui correction models the eigenvalues similar to the spiked population model : almost all eigenvalues are equal except for a few considerably larger ones ( see [ 14 ] and [ 15] ) . It also sets a few population eigenvalues to zero . The bootstrap method sets all the population eigenvalues which were estimated as zero by the sample eigenvalue estimator to
Authorized licensed use limited to : UNIVERSITEIT TWENTE . Downloaded on January 29 , 2010 at 10:05 from IEEE Xplore . Restrictions apply .
1.4
1.2
1
0.8
0.6
0.4
0.2
Bootstrap Karoui
3
2.5
2
1.5
1
0.5
0 0
5
10
15 ( a ) Identity
20
25
0 0
5
10
15
20
( b ) 2 Cluster
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 0
3.5
3
2.5
2
1.5
1
0.5
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 0
1.4
1.2
1
0.8
0.6
0.4
0.2
10
5 ( c ) Toeplitz
15
5
10
( d ) Slope
15
0 0
0.5
2
2.5
0 0
1.5
1 ( e ) 1/f
2
4
6
8
( f ) Undersampled slope
Fig 4 . Histogram of scores of both the bootstrap eigenvalue correction and the Karoui eigenvalue correction .
3.5
3
2.5
2
1.5
1
0.5
0 0
( cid:79 ) Bootstrap ( cid:79 ) Karoui ( cid:79 )
100
200
Eigenvalue index
300
400
( a ) Population eigenvalues
10
8
6
4
2
0 0 l Bootstrap l Karoui l
300
400
100
200
Eigenvalue index ( b ) 2 Cluster
Fig 5 . Example repetition with the undersampled slope population eigenvalues . an equal , non zero value . For the remainder of the eigenvalues a slope is estimated , but with a higher gradient .
Despite the differences in the population eigenvalues , the sample eigenvalue scree plots are almost the same . This shows that the undersampled case is a very difficult problem , which seems to be under determined : multiple population eigenvalue distributions seem to generate the same sample eigenvalue distributions . In practice an equal value for all population eigenvalues in the zero space of the sample eigenvalues is required , since the order of the estimated eigenvectors in the zero space is random . Combining the estimated eigenvectors with different values will introduce arbitrariness . This will lead to arbitrary likelihood estimates for new samples with a component in the sample null space . The bootstrap method corrects all zero sample eigenvalues to almost the same value and seems therefore more applicable to undersampled problems .
We used the Levy distance to be comparable to the tests in [ 10 ] , but the use of the distance is somewhat arbitrary and causes problems in performance comparisons . The distance
822
Authorized licensed use limited to : UNIVERSITEIT TWENTE . Downloaded on January 29 , 2010 at 10:05 from IEEE Xplore . Restrictions apply . valued sample eigenvalues by non zero corrected eigenvalues introduces some arbitrariness , unless the non zero corrected eigenvalues all have the same value .
In the tests we performed we did not focus on the convergence rate of the two methods with respect to the GSA limit . As we described previously , the Karoui correction relies heavily on the Marˇcenko Pastur relation , while the bootstrap method only uses this relation to justify the use of Gaussian distributed synthetic data samples . The Karoui correction may therefore rely more on the GSA limit and have reduced performance for lower dimensional problems .
To reduce random fluctuations in the gradient estimate , the sample covariance matrix of white samples in equation 8 can be constructed using the Marˇcenko Pastur rule [ 16 ] .
REFERENCES
[ 1 ] T . W . Anderson , An introduction to multivariate statistical analysis , 2nd ed . , ser . Wiley series in probability and mathematical statistics . John Wiley & Sons , 1984 .
[ 2 ] K . Fukunaga , Introduction to statistical pattern recognition ( 2nd ed )
San Diego , CA , USA : Academic Press Professional , Inc . , 1990 .
[ 3 ] A . J . Hendrikse , L . J . Spreeuwers , and R . N . J . Veldhuis , “ Eigenvalue correction results in face recognition , ” in 29th Symp . on Information Theory in the Benelux , 2008 , pp . 27–35 .
[ 4 ] P . Xu , G . N . Brock , and R . S . Parrish , “ Modified linear discriminant analysis approaches for classification of high dimensional microarray data , ” Computational Statistics & Data Analysis , vol . 53 , no . 5 , pp . 1674 – 1687 , 2009 .
[ 5 ] A . C . Davison and D . V . Hinkley , Bootstrap Methods and Their Applications , ser . Series in Statistical and Probabilistic Mathematics . Cambridge University Press , January 1997 .
[ 6 ] X . Jiang , B . Mandal , and A . Kot , “ Eigenfeature regularization and extraction in face recognition , ” IEEE Trans . Pattern Anal . Mach . Intell . , vol . 30 , no . 3 , pp . 383–394 , 2008 .
[ 7 ] D . K . Dey and C . Srinivasan , “ Estimation of a covariance matrix under stein ’s loss , ” The Annals of Statistics , vol . 13 , no . 4 , pp . 1581–1591 , 1985 .
[ 8 ] O . Ledoit and M . Wolf , “ Improved estimation of the covariance matrix of stock returns with an application to portfolio selection , ” J . of Empirical Finance , vol . 10 , no . 5 , pp . 603–621 , 2003 .
[ 9 ] J . W . Silverstein , “ Strong convergence of the empirical distribution of eigenvalues of large dimensional random matrices , ” J . Multivar . Anal . , vol . 55 , no . 2 , pp . 331–339 , 1995 .
[ 10 ] N . El Karoui , “ Spectrum estimation for large dimensional covariance matrices using random matrix theory , ” ArXiv Mathematics e prints , september 2006 .
[ 11 ] V . Girko , Theory of Random Determinants . Kluwer , 1990 . [ 12 ] V . A . Marˇcenko and L . A . Pastur , “ Distribution of eigenvalues for some sets of random matrices , ” Mathematics of the USSR Sbornik , vol . 1 , no . 4 , pp . 457–483 , 1967 .
[ 13 ] C . Stein , “ Lectures on the theory of estimation of many parameters , ” J . of Mathematical Sciences , vol . 34 , no . 1 , pp . 1371–1403 , 1986 .
[ 14 ] J . Baik and J . W . Silverstein , “ Eigenvalues of large sample covariance matrices of spiked population models , ” Journal of Multivariate Analysis , vol . 97 , no . 6 , pp . 1382–1408 , 2006 .
[ 15 ] D . Paul , “ Asymptotics of the leading sample eigenvalues for a spiked covariance model , ” Dep . of Statistics , Stanford University , Tech . Rep . , 2004 .
[ 16 ] I . M . Johnstone , “ On the distribution of the largest principle component , ”
Dep . of Statistics , Stanford University , Tech . Rep . , 2000 . is not scale invariant as is demonstrated by the example in figure 6 . Another problem is that the use of a different criterion may very well result in a different ordering of the methods .
1
0.8
0.6
0.4
0.2
0
F G scale small scale large
Fig 6 . Scaling behaviour Levy distance . The dash dotted line and the dotted line indicate the F ( x − ) ± line in case of small scale and in case of large scale respectively . In the small scale , the infimum is determined by the difference on the left , while in the large scale , the infimum is determined by the difference on the right .
It appears that both methods are still biased : the Karoui method seems to be biased toward clusters , what could be explained by the approximation of the population density by the weighed sum of fixed distribution functions . The bootstrap method still over estimates the largest eigenvalues and under estimates the smallest eigenvalues , probably due to inaccuracies in the gradient estimate .
V . CONCLUSIONS AND RECOMMENDATIONS
We introduced the bootstrap correction method , which corrects the bias in sample eigenvalues , and we did a performance comparison between the bootstrap method and the Karoui method . Both methods do improve the estimate of the eigenvalues for most cases , except for the undersampled case . The Karoui correction performs better on clustered densities while the bootstrap method performs better on smooth distributions of the population eigenvalues . Both methods still leave a bias in the eigenvalues after correction : the Karoui correction introduces more clusters , while the bootstrap method still estimates the largest eigenvalues too large , while the smallest eigenvalues are estimated too small .
The comparison of the methods using the Levy distances is still somewhat arbitrarily : the order may change when the scale of the distributions is changed and the ordering by the Levy distance may very well differ from the order found using other measures . In the undersampled case the use of the Levy distance becomes even more problematic : different population eigenvalue distributions seem to lead to the same sample distributions , while the Levy distance between the population distributions can be considerable .
In the undersampled case there is a clear advantage for the bootstrap method : the smallest population eigenvalues which were estimated to be zero with the sample estimator are assigned an equal value . Because the original sample eigenvalues were all zero , the corresponding sample eigenvectors are a random basis in the zero space . Replacing the zero
823
Authorized licensed use limited to : UNIVERSITEIT TWENTE . Downloaded on January 29 , 2010 at 10:05 from IEEE Xplore . Restrictions apply .
