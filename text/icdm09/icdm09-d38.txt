Semi Naive Exploitation of One Dependence Estimators
Nan Li
Yang Yu
Zhi Hua Zhou
National Key Laboratory for Novel Software Technology
Nanjing University , Nanjing 210093 , China
{lin , yuy , zhouzh}@lamdanjueducn
Abstractâ€”It is well known that the key of Bayesian classifier learning is to balance the two important issues , that is , the exploration of attribute dependencies in high orders for ensuring a sufficient flexibility in approximating the ground truth dependencies , and the exploration of low orders for ensuring a stable probability estimate from limited training samples . By allowing one order attribute dependencies , one dependence estimators ( ODEs ) have been shown to be able to approximate the ground truth attribute dependencies whilst keeping the effectiveness of probability estimation , and therefore leading to excellent performance . In previous studies , however , ODEs were exploited in simple ways , such as by averaging , for classification . In this paper , we propose a semi naive exploitation of ODEs that fits a function of ODEs to pursue higher order attribute dependencies . Extensive experiments show that the proposed SNODE approach can achieve better performance than many state of the art Bayesian classifiers .
Keywords Bayesian classifier ; one dependence estimator ; semi naive Bayes
I . INTRODUCTION
In principle , the optimal Bayesian classifier is the optimal way for supervised classification [ 9 ] , which is , however , a theoretical model that requires infinite number of samples for true joint probabilities . Hence , in practice , restricted Bayesian models are used to approximate the optimal Bayesian model . The restricted models are ideally designed to be flexible sufficiently for capturing the ground truth attribute dependencies as well as succinct sufficiently for estimating probabilities effectively . There is , however , a dilemma ; that is , the model should only allow lower order attribute dependencies for obtaining an effective estimate of probabilities from limited samples , yet for capturing the ground truth attribute dependencies , the model should consider higher order dependencies . It is noteworthy that learning the optimal dependencies has been proven to be NP hard [ 5 ] , [ 6 ] .
Consequently , a spectrum of Bayesian learning approaches has been developed , which takes different balances between the complexity of attribute dependencies and the effectiveness of probability estimation . The two extremes of the spectrum are the naive Bayes [ 20 ] and Bayesian network [ 23 ] , [ 16 ] ; the former totally ignores the attribute dependencies while the latter takes the maximum flexibility for approximating the attribute dependencies . Between the two extremes on the spectrum are semi naive Bayesian clas sifiers which try to find proper tradeoffs and have achieved successes [ 26 ] , [ 25 ] , [ 12 ] , [ 11 ] , [ 17 ] , [ 4 ] , [ 33 ] , [ 36 ] , [ 29 ] , [ 32 ] . utilize classifiers which
Among the numerous Bayesian learning approaches , semi naive Bayesian onedependence estimators ( ODEs ) have achieved remarkable performance . By allowing one order dependencies , the probability estimation in ODEs is effective , while the model still has some flexibility for capturing the attribute dependencies . Representative approaches include TAN ( tree augmented naive Bayes ) [ 12 ] , AODE ( averaged one dependence estimators ) [ 29 ] , HNB ( hidden navie Bayes ) [ 32 ] , etc . Note that in previous studies , ODEs were exploited directly in a simple way , eg , simple average of ODEs . It will be interesting to study whether a better performance can be achieved by exploiting the ODEs in other ways such that the resulting model inherits the effectiveness of ODEs while gains more flexibility for modelling higher order dependencies .
In this paper , we propose a framework of semi naive exploitation of ODEs ( SNODE ) , where the ODEs are employed to capture high order attribute dependencies . SNODE approximates the joint probabilities by using functions of ODEs that are fitted according to maximum likelihood estimation . For implementation , generalized additive model ( GAM ) [ 15 ] is employed in this paper , and the function fitting problem is reduced to a constrained concave optimization problem whose global optimal solution can be efficiently obtained . Experiments show that the performance of SNODE is superior to many state of the art Bayesian classifiers . Bias variance decomposition shows that , the success of SNODE mainly owes to its very low bias with slightly increased variance , which validates our proposal of semi naive exploitation of ODEs .
The rest of the paper is organized as follows . In Section II , we introduce the background of Bayesian classifier learning and give a brief review on related work . In Section III , we propose the SNODE approach . Then we report on our experiments in Section IV . Finally , we conclude the paper in Section V .
II . BACKGROUND AND RELATED WORK
Denote ğ’™ = ( ğ‘¥1 , ğ‘¥2,â‹…â‹…â‹… , ğ‘¥ğ‘› ) as an instance , where ğ‘¥ğ‘– is the value of the ğ‘– th attribute , ğ‘ âˆˆ {ğ‘1 , ğ‘2,â‹…â‹…â‹… , ğ‘ğ‘˜} the class where ğ‘˜ is the number of classes , and ğ‘(ğ’™ ) denotes the class of an instance ğ’™ .
The optimal Bayesian classifier provides an optimal way to decide the class of an instance ğ’™ as
ğ‘(ğ’™ ) = arg max
ğ‘
ğ‘ƒ ( ğ‘âˆ£ğ’™ ) = arg max
ğ‘ƒ ( ğ’™ , ğ‘ ) ,
ğ‘ which yields the minimum possible classification error [ 9 ] . However , it is intractable in practical applications , since real world training data is usually insufficient for a reliable estimate of the joint distribution ğ‘ƒ ( ğ’™ , ğ‘ ) . Consequently , approximating ğ‘ƒ ( ğ’™ , ğ‘ ) becomes the central problem in Bayesian classifier learning .
The modification of the optimal Bayesian classifier towards practical applicability forms a spectrum , along which different Bayesian learning approaches utilizing the attribute dependencies in different ways .
One extreme is naive Bayes [ 20 ] . It simply ignores all the attribute dependencies by taking the conditional independence assumption , ie , the attributes are independent given the class , thus it follows that
ğ‘ƒ ( ğ’™ , ğ‘ ) =
ğ‘›âˆ
ğ‘–=1
ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ‘ ) .
Ignoring the dependencies makes the naive Bayes approach efficient , yet the missing of important dependence information sometimes hampers the classification performance seriously .
Another extreme is Bayesian network [ 23 ] , [ 16 ] , which has a strong flexibility for representing and exploiting various attribute dependencies . Suppose that the Bayesian network structure of attributes is known , Bayesian network can encode the joint probability distribution as ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğœ‹ğ‘– , ğ‘ ) ,
ğ‘ƒ ( ğ’™ , ğ‘ ) = ğ‘ƒ ( ğ‘ )
ğ‘›âˆ performance . Representative examples include TAN and SPTAN [ 12 ] , [ 18 ] , AODE and its variants [ 29 ] , [ 34 ] , [ 35 ] , [ 31 ] , HNB [ 32 ] , etc . TAN restricts that each attribute can only depend on one parent in addition to the class , and thus it follows that
ğ‘ƒ ( ğ’™ , ğ‘ ) = ğ‘ƒ ( ğ‘ )
ğ‘›âˆ
ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ‘ğ‘(ğ‘¥ğ‘– ) , ğ‘ ) ,
ğ‘–=1 where ğ‘ğ‘(ğ‘¥ğ‘– ) denotes ğ‘¥ğ‘– â€™s dependent attribute , and is determined in the learning process . In AODE , an ensemble of ODEs is learned and the prediction is produced by aggregating the predictions of all qualified ODEs . In HNB , a hidden parent is created for each attribute which combines the influences from all other attributes .
Some other approaches implicitly restrict the dependencies by localizing naive Bayes classifiers , such as NBTree [ 19 ] which embeds a naive Bayes classifier in the leaves of a pre trained decision tree , and LBR ( lazy Bayesian rule ) [ 36 ] which trains a naive Bayes classifier under a local rule .
The success of semi naive Bayesian classifiers using ODEs suggests that ODEs are on well balance between the ground truth dependencies approximation and the effectiveness of probability estimation . However , in previous approaches , ODEs were used directly in simple ways for classification . In next section , we will present the SNODE method which exploits ODEs in a semi naive way .
III . SNODE
A . The Framework
Suppose that the ground truth dependency structure of the attributes is known . The joint probability distribution can be denoted as
ğ‘ƒ ( ğ’™ , ğ‘ ) = ğ‘ƒ ( ğ‘ )
ğ‘›âˆ
ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ…ğ‘– , ğ‘ ) ,
( 1 )
ğ‘–=1
ğ‘–=1 where ğœ‹ğ‘– denotes the values of the parents of the ğ‘– th attribute excluding the class value ğ‘ for the instance ğ’™ . However , learning the optimal structure , even on restricted structures , is NP hard [ 5 ] , [ 6 ] . Moreover , the data become overly sparse when the dependency order goes high , which causes both the ineffectiveness of probability estimation and inaccurate evaluations in the search of a good structure .
Between the two extremes of the spectrum are semi naive Bayesian classifiers [ 33 ] . Different from totally ignoring the attribute dependencies as naive Bayes or taking maximum flexibility for modelling dependencies as Bayesian network , semi naive Bayesian classifiers try to exploit attribute dependencies in moderate orders . For example , ğ‘˜ DE ( ğ‘˜dependence estimator ) [ 25 ] allows each attribute to depend on at most ğ‘˜ other attributes in addition to the class .
Among numerous semi naive Bayesian classifiers , approaches which utilize ODEs have demonstrated remarkable where ğ…ğ‘– is the attribute values on which the ğ‘– th attribute depends . Based on Eq 1 , the problem of estimating ğ‘ƒ ( ğ’™ , ğ‘ ) can be reduced to estimating the probabilities ğ‘ƒ ( ğ‘ ) and ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ…ğ‘– , ğ‘ ) â€™s . Since ğ‘ƒ ( ğ‘ ) can be easily estimated from the data , the key is to estimate ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ…ğ‘– , ğ‘ ) â€™s . Obviously , ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ…ğ‘– , ğ‘ ) â€™s also encode the dependencies between attributes .
To maintain the effectiveness of probability estimation , we use only ODEs . Consequently , we want to approximate ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ…ğ‘– , ğ‘ ) â€™s from ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ‘¥ğ‘— , ğ‘ ) â€™s ; in other words , we want to model the underlying relationship between ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ…ğ‘– , ğ‘ ) and ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ‘¥ğ‘— , ğ‘ ) â€™s for each different ğ‘– .
For this purpose , we can search in some function space for a function ğ¹ğ‘– by optimizing a certain criterion , such that the inputs of ğ¹ğ‘– are ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ‘¥ğ‘— , ğ‘ ) â€™s and the output is ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ…ğ‘– , ğ‘ ) . The log likelihood and conditional likelihood are commonly used optimization criterions , thus can be used here .
1 ) Formulation : GAM relates the variables ğ‘£ğ‘– â€™s to the objective variable ğ‘œ through the link function ğ‘”(â‹… ) and the smoothing functions ğ‘“ğ‘–(â‹… ) â€™s as ğ‘›âˆ‘
ğ‘”(ğ‘œ ) =
ğœ†ğ‘–ğ‘— ğ‘“ğ‘—(ğ‘£ğ‘– ) ,
ğ‘—=1
Employing GAM to approximate ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ…ğ‘– , ğ‘ ) â€™s from where ğœ†ğ‘–ğ‘— â€™s are model parameters . ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ‘¥ğ‘— , ğ‘ ) â€™s , it becomes ğ‘”(ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ…ğ‘– , ğ‘ ) ) =
ğ‘›âˆ‘
ğœ†ğ‘–ğ‘— ğ‘“ğ‘—(ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ‘¥ğ‘— , ğ‘) ) ,
ğ‘—=1
( 7 ) where ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ…ğ‘– , ğ‘ ) is the objective variable , and ğœ†ğ‘–ğ‘— â€™s are parameters . For simplicity , we use link function ğ‘”(â‹… ) = log(â‹… ) and smoothing function ğ‘“ğ‘—(â‹… ) = log(â‹… ) here . As the consequence , Eq 7 can be rewritten as ) ) ( ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ‘¥ğ‘— , ğ‘ ) .
ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ…ğ‘– , ğ‘ ) = exp
( ğ‘›âˆ‘
ğœ†ğ‘–ğ‘— log
ğ‘—=1
( 8 )
( 9 )
( 10 )
( 11 )
Let ğ’‘ğ‘– = [ ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ‘¥1 , ğ‘),â‹…â‹…â‹… , ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ‘¥ğ‘› , ğ‘)]âŠ¤
. The joint dis tribution can be written as
ğ‘ƒ ( ğ’™ , ğ‘ ) = ğ‘ƒ ( ğ‘ )
ğ‘›âˆ
ğ‘–=1
ğ¹ğ‘–(ğ’‘ğ‘– ) ,
( 2 ) where ğ¹ğ‘– â€™s are functions to be found . Given a training data set ğ’Ÿ , the log likelihood of the model is âˆ‘ ğ‘›âˆ‘
âˆ‘
ğ¿ğ¿(ğ‘­ ) = log ğ‘ƒ ( ğ‘(ğ’™ ) ) + log ğ¹ğ‘–(ğ’‘ğ‘– )
ğ’™âˆˆğ’Ÿ
ğ’™âˆˆğ’Ÿ âˆ‘
ğ’™âˆˆğ’Ÿ log ğ‘ƒ ( ğ‘(ğ’™ ) ) +
ğ‘–=1
ğ‘›âˆ‘
ğ‘–=1
âˆ‘
= where
ğ¿ğ‘–(ğ¹ğ‘– ) ,
( 3 )
ğ¿ğ‘–(ğ¹ğ‘– ) =
( 4 ) ğ‘(ğ’™ ) is the class of an instance ğ’™ , and ğ‘­ = {ğ¹1,â‹…â‹…â‹… , ğ¹ğ‘›} is the set of functions to be determined . log ğ¹ğ‘–(ğ’‘ğ‘– ) ,
ğ’™âˆˆğ’Ÿ
Taking log likelihood as the optimization criterion , the function optimization problem can be formalized as max
{ğ¹1,â‹…â‹…â‹… ,ğ¹ğ‘›} log ğ‘ƒ ( ğ‘(ğ’™ ) ) +
âˆ‘
ğ’™âˆˆğ’Ÿ
ğ‘ ğ‘¡ ğ¹ğ‘– âˆˆ â„±
ğ‘›âˆ‘
ğ¿ğ‘–(ğ¹ğ‘– ) ğ‘– = 1 , 2,â‹…â‹…â‹… , ğ‘›
ğ‘–=1 where â„± is the feasible function space from which the ğ¹ğ‘– â€™s are chosen .
Obviously , in Eq 5 ,
ğ’™âˆˆğ’Ÿ log ğ‘ƒ ( ğ‘(ğ’™ ) ) is a constant , and the objective is linear . Thus , if all ğ¹ğ‘– â€™s are independent , solving Eq 5 is equivalent to solving a series of problems in Eq 6 for different ğ‘– â€™s
âˆ‘
ğ¹ğ‘–âˆˆâ„± ğ¿ğ‘–(ğ¹ğ‘– ) max where ğ¿ğ¿ğ‘–(ğ¹ğ‘– ) is defined in Eq 4 .
Once the functions ğ¹ğ‘– â€™s are obtained by solving the optimization problem in Eq 6 , the joint distribution ğ‘ƒ ( ğ’™ , ğ‘ ) can be computed according to Eq 2 , and the class of instance ğ’™ can be decided .
In SNODE , the functions of ODEs are fitted to gain the model flexibility as that by modelling higher order attribute dependencies . Though maximum likelihood criterion is employed here to fit the functions , other criterions such as minimum square error or conditional likelihood can also be employed .
B . Modelling Higer Order Dependencies with ODEs
Generalized additive model ( GAM ) [ 15 ] is a statistical model that permits the response probability distribution to be any member of the exponential family of distributions , which is also the probability distribution represented by Bayesian networks [ 16 ] . Here , we employ GAM for the implementation of our proposed SNODE framework . Note that it is also possible to use other models , which is an interesting future work .
( 5 )
Considering normalization , the probability distribution can be written as
ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ…ğ‘– , ğ‘ ) =
1
ğ‘†ğ‘–(ğ€ğ‘– ) exp
( ğ€âŠ¤
ğ‘–
) ,
â‹… ğ’ğ‘– where
[ ğ€ğ‘– = [
ğ’ğ‘– = ğ‘†ğ‘–(ğ€ğ‘– ) =
ğœ†ğ‘–1,â‹…â‹…â‹… , ğœ†ğ‘–ğ‘› log
]âŠ¤ , ( ) ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ‘¥1 , ğ‘ ) ,â‹…â‹…â‹… , log âˆ‘ ( â‹… ğ’ğ‘– ğ€âŠ¤
) . exp
ğ‘–
( ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ‘¥ğ‘› , ğ‘ )
)]âŠ¤
,
( 6 )
ğ‘¥â€²
ğ‘–
Now , Eq 8 relates ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ…ğ‘– , ğ‘ ) and ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ‘¥ğ‘— , ğ‘ ) â€™s , and ğ€ = [ ğ€1,â‹…â‹…â‹… , ğ€ğ‘› ] is the parameter with the constraints
ğ‘›âˆ‘
ğ‘—=1
ğœ†ğ‘–ğ‘— = 1 and
0 â‰¤ ğœ†ğ‘–ğ‘— â‰¤ 1 .
2 ) Optimization Problem : Substituting Eq 8 into the optimization problem in Eq 6 , we get the optimization problem to be solved as âˆ‘
( ğ€âŠ¤
ğ‘–
) â‹… ğ’ğ‘– âˆ’ log(ğ‘†ğ‘–(ğ€ğ‘– ) )
( 12 ) max
ğ€ğ‘–
ğ‘ ğ‘¡
ğ’™âˆˆğ’Ÿ ğ‘›âˆ‘
ğœ†ğ‘–ğ‘— = 1 ,
0 â‰¤ ğœ†ğ‘–ğ‘— â‰¤ 1
ğ‘—=1 ğ‘— = 1 , 2 , . . . , ğ‘› where ğ’ğ‘– and ğ‘†ğ‘–(ğ€ğ‘– ) are defined in Eq 10 and 11 , respectively .
Proposition 1 The optimization problem defined in Eq 12 is a constrained concave optimization problem .
â‹„ Training Process Input:ğ’Ÿ Training data set Process : 1 . Estimate ğ‘ƒ ( ğ‘ ) and ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ‘¥ğ‘— , ğ‘ ) from ğ’Ÿ ; 2 . 3 . 4 . 5 .
Initialize ğœ†ğ‘–ğ‘— = 1/ğ‘› , for ğ‘– = 1,â‹…â‹…â‹… , ğ‘›
Solve Eq 12 to get ğ€ğ‘– end for
â‹„ Testing Process
Input :
ğ’™ = ( ğ‘¥1,â‹…â‹…â‹… , ğ‘¥ğ‘› ) Test instances to be classified for all class values ğ‘ for ğ‘– = 1,â‹…â‹…â‹… , ğ‘›
Process : 1 . 2 . 3 . 4 . 5 . 6 . Return ğ‘(ğ’™ ) = ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥ğ‘ ğ‘ƒ ( ğ‘ ) end for end for
Calculate ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ…ğ‘– , ğ‘ ) based on Eq 8
âˆğ‘› ğ‘–=1 ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ…ğ‘– , ğ‘ )
Figure 1 . Pseudo code of the SNODE algorithm
C . Discussion
It is easy to see that our proposed SNODE is a flexible framework . In the current paper , GAM and log likelihood are employed as the model and optimization criterion , respectively . Obviously , other models and criterions can also be utilized .
By using logistic functions , we can get a special case of SNODE which is similar to the well studied log linear model [ 7 ] . This suggests that , it might be possible to derive more insight by connecting the log linear model to seminaive Bayesian classifiers .
Discriminative learning of Bayesian classifiers has atInstead of optimizing the logtached much attention . likelihood , discriminative approaches try to maximize the conditional likelihood and may produce better class probability estimates [ 14 ] , [ 24 ] , [ 3 ] . A discriminant version of SNODE can be developed by maximizing the conditional likelihood ; however , the corresponding optimization problem may be more difficult than the current one , since the conditional likelihood does not decompose into separate terms for each function to be learned .
IV . EXPERIMENTS
Proof : With a simple derivation we can get
A . Settings
âˆ‚2ğ¿ğ‘–(ğ€ğ‘– )
âˆ‚ğ€2 ğ‘–
â‰¤ 0 .
Thus , ğ¿ğ‘–(ğ€ğ‘– ) is a concave function with respect to ğ€ğ‘– . Moreover , the feasible region is convex .
So , the problem in Eq 12 is a constrained concave â–¡ optimization problem .
With the above proposition , the problem in Eq 12 has one unique global optimal solution , and it can be solved efficiently by standard optimization algorithms such as interiorpoint methods [ 2 ] .
(
ğ‘˜ğ‘›2ğ‘£2
3 ) Algorithm : The pseudo code is shown in Figure 1 . At the training time , SNODE forms the tables of the joint variable values and class frequencies from which the probabilities ğ‘ƒ ( ğ‘ ) and ğ‘ƒ ( ğ‘¥ğ‘–âˆ£ğ‘¥ğ‘— , ğ‘ ) are calculated . The space ) , where ğ‘˜ is the complexity of the tables is ğ‘‚ number of classes , ğ‘£ is the average number of values of every attribute , and ğ‘› is the number of attributes . Derivation of the frequencies required to populate these tables is of time complexity ğ‘‚(ğ‘ ğ‘›2 ) , where ğ‘ is the number of training examples . The concave optimization problem is critical to the training process , and its time complexity is ğ‘‚(ğ‘›3 ) [ 22 ] . Therefore , the overall training time required is ğ‘‚(ğ‘›2(ğ‘ + ğ‘›2) ) .
At the testing time , classifying an instance requires the calculation of Eq 8 , and it is of time complexity ğ‘‚(ğ‘˜ğ‘›2 ) .
Thirty five UCI data sets are used in our experiments . These data sets span a broad range of real domains , with sizes ranging from 76 to 20,000 . Some statistics of the data sets are summarized in Table I , where#Inst , #Attr and #Class mean the number of instances , attributes and classes , respectively .
Considering that many Bayesian classifiers could not handle missing values and real values directly , for simplicity , we filled in the missing values by the mode or mean of the corresponding variable , and discretized all the data using the MDL discretization [ 10 ] .
We compare SNODE with several Bayesian classifiers , including the simplest one , naive Bayes ( NB ) , a general Bayesian network learning algorithm , K2 [ 8 ] , and state ofthe art semi naive Bayesian methods including TAN , AODE and HNB . SNODE was implemented in WEKA [ 30 ] , using the MATLAB optimization toolbox [ 27 ] for optimization . The implementations of NB , K2 , TAN , AODE and HNB in WEKA were used here . Laplacian correction was used in probability estimations .
Bias variance decomposition [ 13 ] is helpful for analyzing performance of learning algorithms . It breaks the error into bias and variance . Bias describes the error of the learner in expectation , while variance reflects the sensitivity of the learner to variations in the training samples . We performed bias variance decomposition using the repeated cross validation approach [ 28 ] , which is the default method in WEKA .
DATA SETS USED IN THE EXPERIMENTS .
Table I
Data set artificial auto mpg balance scale balloons clean1 cmc coding colic cylinder bands ecoli glass haberman heart c heart statlog hepatitis house votes 84 hypothyroid ionosphere
# Inst 5,109 398 625 76 476 1,473 20,000 368 540 336 214 306 303 270 155 435 3,772 351
# Attr 8 8 5 5 133 10 16 17 26 7 8 3 12 10 17 17 26 34
# Class 10 4 3 2 2 3 2 2 2 8 7 2 5 2 2 2 4 2
Data set iris kr vs kp led24 led7 machine mfeat mor nursery page blocks segment sick solar flare 2 tic tac toe titanic vehicle waveform 5000 yeast zoo
# Inst 150 3,196 3,200 3,200 209 2,000 12,960 5,473 2,310 3,772 1,066 958 2,201 846 5,000 1,484 101
# Attr 5 37 25 8 8 7 9 11 19 26 12 10 4 19 20 8 17
# Class 3 2 10 10 8 10 5 5 7 2 6 2 2 4 3 10 7
In our experiments , fifty runs of two fold cross validation were executed and divided into 5 groups each contained 10 runs . Bias variance decomposition was performed on each group , and thus the averaged predictive error and a pair of bias/variance were obtained for each group . The mean and standard deviations were recorded , and pairwise ğ‘¡ tests with 95 % significance level were conducted .
B . Results
The error , bias and variance averaged across all data sets for the compared classifiers are presented in Table II . The results of pairwise ğ‘¡ tests are summarized in Table III , where win/tie/loss means that SNODE wins , ties and loses on #win , #tie and #loss number of data sets against the corresponding comparison algorithm , according to pairwise t tests . Detailed information over all data sets can be found in Tables IV , V and VI , respectively .
Figures 2 , 3 and 4 depict the error , bias and variance relative to NB , respectively , where the ğ‘¥ axis shows error ( bias , variance ) of SNODE divided by that of NB , while the ğ‘¦ axis shows that of the compared classifier . Each point in the figures corresponds to one data set . The vertical and horizonal lines ( ie , ğ‘¥ = 1 and ğ‘¦ = 1 ) are used to highlight the performance of NB , and the diagonal lines ( ie , ğ‘¦ = ğ‘¥ ) indicate equal performance of the compared approaches . If the error ( bias , variance ) of SNODE is better than the compared method on a data set , the corresponding point will appear above the diagonal line .
1 ) Comparison with State of the art Methods : It can be observed from Tables II and IV that SNODE achieved the lowest error on much more data sets than the compared algorithms , and it achieved the lowest average error . Concerning pairwise t tests results in Table III , SNODE always has larger counts of wins than losses .
Table II
AVERAGED ERROR , BIAS AND VARIANCE OVER ALL DATA SETS , WHERE THE BEST AND WORST PERFORMANCE IN EACH ROW ARE BOLDED AND
UNDERLINED , RESPECTIVELY .
SNODE AODE HNB 0.1868 0.1879 0.1505 0.1524 0.0363 0.0355
0.1913 0.1605 0.0308
TAN 0.2081 0.1805 0.0275
K2
0.1927 0.1511 0.0416
NB
0.2095 0.1833 0.0273
Error Bias Variance
Table III
WIN/TIE/LOSS COUNTS , WHICH SUMMARIZES THE COMPARISON OF SNODE AGAINST THE CORRESPONDING ALGORITHM ACCORDING TO
PAIRWISE t TESTS WITH 95 % SIGNIFICANCE LEVEL
Error Bias Variance
AODE 16/10/9 25/5/5 7/12/16
HNB 10/19/6 16/12/7 8/19/8
TAN 19/9/7 30/4/1 5/8/22
K2
14/16/5 10/16/9 24/5/6
NB 23/5/7 30/4/1 5/11/19
In Figure 2 , most points appear to the left of the vertical line ğ‘¥ = 1 , indicating that SNODE performs better than NB . Also , the number of points appear above the diagonal line are apparently more than that below the diagonal line , indicating that the error of SNODE error is lower than that of the compared algorithms .
If we divide these algorithms into two groups , ie , extreme group consisting of NB and K2 , and moderate group consisting of SNODE , AODE , HNB and TAN , it can be found that classifiers in the moderate group performs better than those in the extreme group , which supports the motivation of developing Bayesian classifiers with the exploitation of moderate attribute dependencies .
Within the moderate group , it can be found that TAN sometimes has relatively poor performance , such as on house votes 84 , nursery and kr vs kp , while SNODE , AODE
1.2
1
0.8
0.6
/
B N E D O A
0.4
0.4
0.6
0.8
SNODE/NB
1
1.2
/
B N B N H
1
0.8
0.6
0.4
0.4
1.2
1
0.8
0.6
/
B N N A T
1.2
1
0.8
0.6
B N / 2 K
0.6
0.8
SNODE/NB
1
1.2
0.4
0.4
0.6
0.8
SNODE/NB
1
1.2
0.4
0.4
0.6
0.8
SNODE/NB
1
1.2
( a ) SNODE vs . AODE
( b ) SNODE vs . HNB
( c ) SNODE vs . TAN
( d ) SNODE vs . K2
Figure 2 . Comparison of relative ERROR . The ğ‘¥ axis presents error of SNODE divided by that of NB , ğ‘¦ axis presents error of the compared classifier divided by that of NB , and each point corresponds to one data set .
1.2
1
0.8
0.6
/
B N E D O A
1.2
1
0.8
0.6
/
B N B N H
/
B N N A T
1
0.8
0.6
1.2
1
0.8
0.6
B N / 2 K
0.4
0.4
0.6
0.8
1 SNODE/NB
1.2
0.4
0.4
0.6
0.8
1 SNODE/NB
1.2
0.4
0.4
0.6
0.8
1 SNODE/NB
0.4
0.4
0.6
0.8
1 SNODE/NB
1.2
( a ) SNODE vs . AODE
( b ) SNODE vs . HNB
( c ) SNODE vs . TAN
( d ) SNODE vs . K2
Figure 3 . Comparison of relative BIAS . The ğ‘¥ axis presents bias of SNODE divided by that of NB , the ğ‘¦ axis presents bias of the compared classifier divided by that of NB , and each point corresponds to one data set .
3
2.5
2
1.5
1
/
B N E D O A
0.5
0.5
1
2
1.5 SNODE/NB
2.5
3
2.5
2
1.5
1
/
B N B N H
3
0.5
0.5
1
2
1.5 SNODE/NB
2.5
3
2.5
2
1.5
1
/
B N N A T
3
0.5
0.5
1
2
1.5 SNODE/NB
2.5
3
2.5
2
1.5
1
B N / 2 K
3
0.5
0.5
1
2
1.5 SNODE/NB
2.5
3
( a ) SNODE vs . AODE
( b ) SNODE vs . HNB
( c ) SNODE vs . TAN
( d ) SNODE vs . K2
Figure 4 . Comparison of relative VARIANCE . The ğ‘¥ axis presents variance of SNODE divided by that of NB , the ğ‘¦ axis presents variance of the compared classifier divided by that of NB , and each point corresponds to one data set . and HNB , which utilize ODEs , have relatively close performance on most data sets .
2 ) Bias Variance Analysis : It has been shown that the maximum dependency determines the upper bound of representation ability of a Bayesian classifier [ 21 ] ; this implies that , from the viewpoint of version space , the maximum dependency determines the upper bound of the range of the function space . It is known that the larger the version space , the lower the bias , and the larger the variance ; this suggests a connection between bias/variance and the flexibility for dependencies in Bayesian classifiers .
From the comparison of average bias and variance shown in Table II , it can be found that NB was with the highest bias but the lowest variance , K2 exhibited the lowest bias but the highest variance , and TAN , AODE and HNB achieved a tradeoff with bias and variance . Similar behaviors can also be observed in detailed results shown in Tables V and VI . This observation can be explained from the fact that NB ignores all dependencies , K2 tries to exploit all possible dependencies , while TAN , AODE and HNB try to utilize dependencies only in moderate orders .
In Figures 3 and 4 it can be found that , compared with K2 , SNODE achieved comparable bias yet much lower variance ; compared with TAN , AODE and HNB , SNODE exhibited lower bias yet higher variance . This is also verified by pairwise t tests shown in Table III . Hence , we can conclude that the success of SNODE owes much to its low bias .
In Table II it can be seen that SNODE achieved the lowest bias while its variance is still lower than K2 . This may
COMPARISON OF PREDICTIVE ERRORS ( MEANÂ±STD. ) , WHERE THE BEST AND WORST PERFORMANCE IN EACH ROW ARE BOLDED AND UNDERLINED , RESPECTIVELY . THE average ROW PRESENTS THE RESULTS AVERAGED OVER ALL THE 35 DATA SETS ; win/tie/loss ROW SUMMARIZES THE COMPARISON OF SNODE AGAINST THE CORRESPONDING ALGORITHM ACCORDING TO PAIRWISE t TESTS
Table IV
WITH 95 % SIGNIFICANCE LEVEL .
Data set artificial auto mpg balance scale balloons clean1 cmc coding colic cylinder bands ecoli glass haberman heart c heart statlog hepatitis house votes 84 hypothyroid ionosphere iris kr vs kp led24 led7 machine mfeat mor nursery page blocks segment sick solar flare 2 tic tac toe titanic vehicle waveform 5000 yeast zoo average win/tie/loss
SNODE 3709Â±0028 2252Â±0065 2636Â±0027 2622Â±0171 0847Â±0045 4560Â±0028 2444Â±0004 1889Â±0048 2443Â±0054 1667Â±0101 2763Â±0068 2645Â±0052 1762Â±0046 1749Â±0055 1323Â±0086 0494Â±0016 0116Â±0004 0829Â±0018 0680Â±0033 0586Â±0007 2755Â±0014 2649Â±0004 1297Â±0070 3086Â±0034 0583Â±0003 0352Â±0004 0509Â±0008 0242Â±0002 2508Â±0011 2196Â±0072 2113Â±0007 2715Â±0043 1585Â±0004 4100Â±0028 0743Â±0044
0.1868
â€”
AODE
4119Â±0017 2422Â±0061 2659Â±0014 2592Â±0129 0999Â±0043 4799Â±0021 2628Â±0003 1807Â±0014 2348Â±0039 1531Â±0068 2612Â±0032 2625Â±0028 1674Â±0030 1715Â±0031 1416Â±0047 0571Â±0010 0161Â±0002 0824Â±0033 0571Â±0018 0947Â±0006 2757Â±0011 2656Â±0006 1342Â±0046 2997Â±0023 0747Â±0005 0293Â±0006 0551Â±0007 0266Â±0002 2554Â±0016 2504Â±0029 2169Â±0004 2835Â±0054 1397Â±0009 4114Â±0024 0753Â±0066
0.1913 16/10/9
HNB
3554Â±0021 2395Â±0086 2664Â±0018 2663Â±0206 1067Â±0039 4590Â±0026 2450Â±0002 1873Â±0040 2480Â±0061 1756Â±0074 2629Â±0057 2762Â±0063 1736Â±0026 1772Â±0043 1263Â±0069 0561Â±0016 0111Â±0004 0789Â±0024 0663Â±0052 0787Â±0007 2758Â±0010 2655Â±0005 1174Â±0043 3034Â±0036 0605Â±0004 0319Â±0006 0380Â±0005 0248Â±0001 2575Â±0018 2346Â±0044 2104Â±0005 2742Â±0021 1418Â±0007 4103Â±0024 0751Â±0049
0.1879 10/19/6
TAN
4619Â±0016 3042Â±0080 2600Â±0019 2708Â±0143 1356Â±0041 4827Â±0028 2880Â±0005 1941Â±0027 2290Â±0031 1515Â±0056 2825Â±0080 2627Â±0029 1672Â±0072 1652Â±0057 1494Â±0118 0986Â±0009 0158Â±0002 0895Â±0013 0544Â±0027 1270Â±0011 2753Â±0016 2650Â±0006 1401Â±0035 3043Â±0023 0978Â±0010 0634Â±0007 0846Â±0013 0297Â±0002 2605Â±0016 2915Â±0062 2242Â±0013 3747Â±0018 1925Â±0013 4133Â±0022 0753Â±0123
0.2081 19/9/7
K2
4013Â±0015 2395Â±0084 2627Â±0014 2727Â±0226 0921Â±0018 4631Â±0020 2450Â±0003 1927Â±0032 2296Â±0029 1584Â±0045 2730Â±0037 2597Â±0035 1822Â±0021 1815Â±0027 1464Â±0031 0535Â±0006 0090Â±0002 0787Â±0005 0542Â±0017 0766Â±0012 2865Â±0007 2651Â±0002 1294Â±0071 2804Â±0011 0650Â±0002 0403Â±0004 0903Â±0009 0258Â±0002 2528Â±0013 2491Â±0011 2128Â±0013 2760Â±0016 1809Â±0004 4111Â±0028 1090Â±0067
0.1927 14/16/5
NB
4779Â±0012 3127Â±0066 2600Â±0013 2719Â±0210 1479Â±0010 4826Â±0020 2880Â±0003 1932Â±0028 2379Â±0028 1534Â±0068 2866Â±0014 2610Â±0020 1675Â±0022 1651Â±0025 1523Â±0027 0987Â±0007 0180Â±0001 0912Â±0012 0543Â±0015 1273Â±0011 2753Â±0008 2650Â±0002 1496Â±0069 3041Â±0013 0979Â±0002 0652Â±0003 0525Â±0009 0292Â±0002 2603Â±0012 2913Â±0010 2236Â±0015 3757Â±0018 1928Â±0004 4154Â±0030 0876Â±0064
0.2095 23/5/7 suggest that , the utilization of many ODEs is sufficient to obtain a bias as low as that can be obtained by exploiting a full Bayesian structure . Furthermore , it is usually feasible to estimate one dependent estimators accurately in real applications . This validates our purpose of gaining flexibility as that by modelling higher order probabilities based on onedependent estimators .
V . CONCLUSION
In contrast to many previous Bayesian learning methods which utilize ODEs directly for classification in a simple way , in this paper , we present the SNODE framework , a semi naive exploration of ODEs . In SNODE , functions of ODEs are employed to gain the flexibility by modelling higher order attribute dependencies . As a special case , generalized additive model is employed for the implementation , and the function optimization problem is then reduced to an optimization problem whose global optimum can be solved efficiently .
Experiment results show that the proposed SNODE achieves better performance than many state of the art Bayeian classifiers . Bias variance decomposition discloses that , SNODE achieves very low bias ; this validates our proposal of gaining the flexibility as that by modelling higher order attribute dependencies based on ODEs . Moreover , the bias variance decomposition suggests that there is a notable space for reducing the variance of SNODE . It is an interesting future work to improve SNODE by exploring regularization in the maximum likelihood estimation [ 1 ] .
ACKNOWLEDGMENT
This work was supported by the National Science Foundation of China ( 60635030 , 60721002 ) , the Jiangsu Science Foundation ( BK2008018 ) and the Jiangsu 333 High Level Talent Cultivation Program .
COMPARISON OF BIAS ( MEANÂ±STD. ) , WHERE THE BEST AND WORST PERFORMANCE IN EACH ROW ARE BOLDED AND UNDERLINED , RESPECTIVELY . THE average ROW PRESENTS THE RESULTS AVERAGED OVER ALL THE 35 DATA SETS ; win/tie/loss ROW SUMMARIZES
THE COMPARISON OF SNODE AGAINST THE CORRESPONDING ALGORITHM ACCORDING TO PAIRWISEt TESTS WITH 95 %
Table V
SIGNIFICANCE LEVEL .
Data set artificial auto mpg balance scale balloons clean1 cmc coding colic cylinder bands ecoli glass haberman heart c heart statlog hepatitis house votes 84 hypothyroid ionosphere iris kr vs kp led24 led7 machine mfeat mor nursery page blocks segment sick solar flare 2 tic tac toe titanic vehicle waveform 5000 yeast zoo average win / tie / loss
SNODE 2685Â±0021 1736Â±0069 1848Â±0032 1776Â±0157 0631Â±0032 3721Â±0042 2328Â±0003 1580Â±0021 2041Â±0027 1271Â±0057 2318Â±0047 2185Â±0053 1469Â±0033 1591Â±0044 1058Â±0059 0432Â±0017 0079Â±0004 0788Â±0025 0433Â±0041 0503Â±0006 2370Â±0011 2257Â±0019 0754Â±0065 2346Â±0017 0513Â±0001 0314Â±0006 0366Â±0012 0222Â±0003 2106Â±0024 1660Â±0038 2105Â±0002 2116Â±0058 1340Â±0010 3621Â±0008 0158Â±0072
0.1505
â€”
AODE
2973Â±0033 1793Â±0042 1909Â±0031 1845Â±0062 0808Â±0036 4069Â±0041 2516Â±0004 1622Â±0027 1870Â±0043 1261Â±0029 2128Â±0033 2240Â±0038 1547Â±0039 1575Â±0042 1273Â±0084 0537Â±0013 0138Â±0002 0730Â±0047 0541Â±0016 0795Â±0008 2446Â±0010 2294Â±0014 0930Â±0048 2606Â±0026 0684Â±0006 0243Â±0003 0425Â±0009 0250Â±0002 2239Â±0017 2182Â±0033 2034Â±0011 2282Â±0051 1222Â±0008 3753Â±0018 0408Â±0048
0.1605
25 / 5 / 5
HNB
2509Â±0014 1670Â±0089 1907Â±0029 1950Â±0110 0926Â±0056 3809Â±0024 2344Â±0002 1630Â±0033 1741Â±0063 1341Â±0045 1958Â±0031 2180Â±0054 1519Â±0037 1617Â±0037 1019Â±0116 0530Â±0004 0080Â±0002 0713Â±0037 0467Â±0031 0671Â±0009 2446Â±0010 2288Â±0022 0784Â±0038 2341Â±0025 0538Â±0004 0272Â±0009 0280Â±0002 0234Â±0001 2183Â±0013 1927Â±0034 2092Â±0004 2191Â±0017 1248Â±0010 3654Â±0041 0293Â±0067
0.1524
16 / 12 / 7
TAN
3679Â±0040 2448Â±0045 1841Â±0036 1921Â±0097 1247Â±0030 4342Â±0020 2778Â±0009 1772Â±0026 1781Â±0042 1253Â±0016 2252Â±0060 2355Â±0041 1578Â±0056 1549Â±0066 1377Â±0077 0963Â±0011 0135Â±0004 0845Â±0014 0516Â±0044 1126Â±0021 2447Â±0015 2298Â±0017 1022Â±0047 2716Â±0024 0927Â±0010 0564Â±0004 0737Â±0017 0279Â±0003 2294Â±0022 2680Â±0036 2178Â±0010 3224Â±0038 1862Â±0013 3780Â±0018 0409Â±0069
0.1805
30 / 4 / 1
K2
2791Â±0013 1809Â±0058 1837Â±0011 1959Â±0182 0638Â±0018 3680Â±0042 2285Â±0004 1554Â±0022 1603Â±0057 1266Â±0015 2061Â±0050 2245Â±0060 1485Â±0027 1573Â±0025 1081Â±0058 0429Â±0008 0074Â±0003 0680Â±0022 0485Â±0020 0649Â±0010 2308Â±0011 2265Â±0009 0832Â±0067 2379Â±0024 0571Â±0004 0341Â±0009 0374Â±0007 0242Â±0005 2106Â±0017 1820Â±0028 2070Â±0012 1973Â±0043 1292Â±0008 3696Â±0027 0260Â±0080
0.1511
10 / 16 / 9
NB
3795Â±0026 2519Â±0065 1839Â±0014 1955Â±0173 1351Â±0014 4337Â±0043 2778Â±0004 1765Â±0019 1917Â±0047 1263Â±0007 2301Â±0094 2359Â±0057 1583Â±0030 1551Â±0026 1410Â±0058 0964Â±0007 0162Â±0003 0871Â±0022 0521Â±0013 1129Â±0011 2446Â±0012 2298Â±0008 1113Â±0039 2718Â±0023 0927Â±0005 0583Â±0008 0785Â±0009 0277Â±0003 2330Â±0012 2679Â±0030 2182Â±0010 3232Â±0041 1865Â±0009 3783Â±0015 0550Â±0061
0.1833
30 / 4 / 1
REFERENCES
[ 1 ] G . Andrew and J . Gao . Scalable training of ğ¿1 regularized log linear models . In Proceedings of the 24th International Conference on Machine learning , pages 33â€“40 , Corvalis , OR , 2007 .
[ 2 ] S . Boyd and L . Vandenberghe . Convex Optimization . Cam bridge University Press , Cambridge , UK , 2004 .
[ 3 ] J . Burge and T . Lane . Learning class discriminative dynamic bayesian networks . In Proceedings of the 22nd International Conference on Machine learning , pages 97â€“104 , Bonn , Germany , 2005 .
[ 4 ] J . Cerquides and R . L . D . MÂ´antaras . Robust bayesian linear classifier ensembles . In Proceedings of the 16th European Conference on Machine learning , pages 70â€“81 , Porto , Portugal , 2005 .
[ 5 ] D . M . Chickering .
Learning Bayesian networks is NPComplete . In Proceedings of the 5th International Workshop on Artificial Intelligence and Statistics , pages 121â€“130 , Fort Lauderdale , FL , 1996 .
[ 6 ] D . M . Chickering , D . Heckerman , and C . Meek . Largesample learning of Bayesian networks is NP hard . Journal of Machine Learning Research , 5:1287â€“1330 , 2004 .
[ 7 ] R . Christensen . Log Linear Models and Logistic Regression .
Springer , New York , NY , 1997 .
[ 8 ] G . Cooper and E . Herskovits . Bayesian method for the induction of probabilistic networks from data . Machine Learning , 9(4):309â€“347 , 1992 .
[ 9 ] R . O . Duda and P . E . Hart . Pattern Classification and Scene
Analysis . John Wiley & Sons , New York , NY , 1973 .
[ 10 ] U . M . Fayyad and K . B . Irani . Multi interval discretization of continuous valued attributes for classification learning . In
COMPARISON OF VARIANCE ( MEANÂ±STD. ) , WHERE THE BEST AND WORST PERFORMANCE IN EACH ROW ARE BOLDED AND
Table VI
UNDERLINED , RESPECTIVELY . THE average ROW PRESENTS THE RESULTS AVERAGED OVER ALL THE 35 DATA SETS ; win/tie/loss ROW SUMMARIZES THE COMPARISON OF SNODE AGAINST THE CORRESPONDING ALGORITHM ACCORDING TO PAIRWISE t TESTS WITH
95 % SIGNIFICANCE LEVEL .
Data set artificial auto mpg balance scale balloons clean1 cmc coding colic cylinder bands ecoli glass haberman heart c heart statlog hepatitis house votes 84 hypothyroid ionosphere iris kr vs kp led24 led7 machine mfeat mor nursery page blocks segment sick solar flare 2 tic tac toe titanic vehicle waveform 5000 yeast zoo average win / tie / loss
SNODE 1024Â±0024 0515Â±0029 0787Â±0019 0842Â±0074 0215Â±0032 0839Â±0023 0116Â±0002 0308Â±0036 0402Â±0042 0394Â±0096 0444Â±0085 0459Â±0076 0294Â±0045 0157Â±0040 0265Â±0043 0061Â±0024 0037Â±0003 0041Â±0011 0247Â±0047 0084Â±0007 0385Â±0019 0392Â±0019 0543Â±0090 0740Â±0032 0070Â±0003 0038Â±0005 0142Â±0006 0020Â±0002 0401Â±0019 0535Â±0041 0008Â±0007 0599Â±0044 0245Â±0009 0478Â±0036 0584Â±0038
0.0363
â€”
AODE
1146Â±0024 0629Â±0030 0750Â±0026 0745Â±0089 0191Â±0022 0730Â±0033 0112Â±0004 0185Â±0023 0477Â±0011 0269Â±0055 0483Â±0028 0385Â±0022 0127Â±0017 0140Â±0043 0143Â±0057 0034Â±0008 0023Â±0002 0094Â±0024 0030Â±0009 0152Â±0008 0311Â±0006 0362Â±0012 0412Â±0044 0392Â±0013 0064Â±0002 0050Â±0006 0126Â±0007 0016Â±0003 0315Â±0006 0322Â±0028 0136Â±0009 0553Â±0026 0174Â±0009 0361Â±0037 0345Â±0050
0.0308
7 / 12 / 16
HNB
1045Â±0033 0725Â±0027 0756Â±0032 0711Â±0104 0141Â±0025 0781Â±0014 0106Â±0002 0242Â±0027 0739Â±0046 0414Â±0062 0670Â±0068 0581Â±0062 0216Â±0025 0155Â±0024 0243Â±0052 0031Â±0013 0031Â±0003 0076Â±0015 0195Â±0062 0116Â±0009 0312Â±0014 0366Â±0022 0389Â±0028 0694Â±0036 0067Â±0003 0046Â±0004 0099Â±0005 0014Â±0002 0392Â±0015 0419Â±0031 0012Â±0008 0551Â±0024 0170Â±0009 0449Â±0043 0457Â±0068
0.0355
8 / 19 / 8
TAN
0941Â±0044 0593Â±0044 0759Â±0028 0784Â±0053 0108Â±0028 0484Â±0032 0102Â±0009 0168Â±0035 0509Â±0023 0261Â±0040 0572Â±0101 0271Â±0025 0094Â±0026 0103Â±0030 0115Â±0083 0024Â±0011 0023Â±0005 0049Â±0011 0028Â±0027 0144Â±0010 0307Â±0004 0352Â±0016 0378Â±0013 0327Â±0009 0052Â±0004 0070Â±0003 0109Â±0010 0017Â±0002 0311Â±0029 0236Â±0041 0065Â±0022 0523Â±0034 0063Â±0013 0353Â±0024 0344Â±0150
0.0275
5 / 8 / 22
K2
1223Â±0027 0587Â±0060 0790Â±0011 0766Â±0149 0283Â±0012 0951Â±0037 0167Â±0002 0372Â±0011 0693Â±0038 0318Â±0051 0669Â±0033 0353Â±0034 0339Â±0022 0243Â±0022 0383Â±0033 0105Â±0010 0018Â±0003 0107Â±0017 0056Â±0010 0117Â±0006 0557Â±0012 0386Â±0007 0462Â±0035 0425Â±0033 0078Â±0005 0062Â±0009 0150Â±0003 0016Â±0005 0422Â±0012 0670Â±0026 0058Â±0023 0787Â±0038 0517Â±0006 0415Â±0030 0829Â±0043
0.0416
24 / 5 / 6
NB
0984Â±0024 0607Â±0061 0761Â±0014 0761Â±0158 0128Â±0017 0488Â±0042 0102Â±0002 0166Â±0010 0462Â±0027 0270Â±0072 0563Â±0103 0250Â±0050 0091Â±0018 0099Â±0018 0113Â±0041 0023Â±0006 0018Â±0004 0041Â±0014 0021Â±0015 0144Â±0006 0307Â±0013 0351Â±0007 0382Â±0061 0323Â±0034 0052Â±0005 0069Â±0009 0118Â±0004 0015Â±0003 0273Â±0007 0234Â±0028 0053Â±0023 0525Â±0038 0063Â±0007 0370Â±0039 0325Â±0016
0.0273
5 / 11 / 19
Proceedings of the 13th International Joint Conference on Artificial Intelligence , pages 1022â€“1029 , Chambery , France , 1993 .
[ 11 ] E . Frank , M . Hall , and B . Pfahringer . Locally weighted naive bayes . In Proceedings of the 19th Conference on Uncertainty in Artificial Intelligence , pages 249â€“256 , Acapulco , Mexico , 2003 .
[ 12 ] N . Friedman , D . Geiger , and M . Goldszmidt . Bayesian network classifiers . Machine Learning , 29(2 3):131â€“163 , 1997 .
[ 13 ] S . German , E . Bienenstock , and R . Doursat . Neural networks and the bias/variance dilemma . Neural Computation , 4(1):1â€“ 58 , 1992 .
[ 14 ] D . Grossman and P . Domingos . Learning Bayesian network classifiers by maximizing conditional likelihood . In Proceedings of the 21st International Conference on Machine Leanring , pages 361â€“368 , Banff , Canada , 2004 .
[ 15 ] T . Hastie and R . Tibshirani . Generalized Additive Models .
Chapman and Hall , New York , NY , 1990 .
[ 16 ] D . Heckerman . A tutorial on learning with Bayesian netTechnical Report MSR TR 95 06 , Microsoft Re works . search , 1995 .
[ 17 ] Y . Jing , V . PavloviÂ´c , and J . M . Rehg . Efficient discriminative learning of bayesian network classifier via boosted augmented naive bayes . In Proceedings of the 22nd International Conference on Machine learning , pages 369â€“376 , Bonn , Germany , 2005 .
[ 18 ] E . Keogh and M . Pazzani . Learning augmented Bayesian classifiers : A comparison of distribution based and classificationbased approaches . In Proceedings of the 15th International Workshop on Artificial Intelligence and Statistics , pages 225â€“ 230 , Stockholm , Sweden , 1999 .
[ 19 ] R . Kohavi . Scaling up the accuracy of naive Bayes classifiers : In Proceedings of the 2nd ACM
A decision tree hybrid .
[ 34 ] F . Zheng and G . I . Webb . Efficient lazy elimination for averaged one dependence estimators . In Proceedings of the 23rd European Conference on Machine Learning , pages 1113â€“1120 , Pittsburgh , PA , 2007 .
[ 35 ] F . Zheng and G . I . Webb . Finding the right family : Parent and child selection for averaged one dependence estimators . In Proceedings of the 18th European Conference on Machine Learning , pages 490â€“501 , Warsaw , Poland , 2007 .
[ 36 ] Z . Zheng and G . I . Webb . Lazy learning of Bayesian rules .
Machine Learning , 41(1):53â€“84 , 2000 .
SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 202â€“207 , Portland , OR , 1996 .
[ 20 ] P . Langley , W . Iba , and K . Thompson . An analysis of Bayesian classifiers . In Proceedings of the 10th National Conference on Artificial Intelligence , pages 223â€“228 , San Jose , CA , 1992 .
[ 21 ] C . X . Ling and H . Zhang . The representational power of discrete Bayesian networks . Journal of Machine Learning Research , 3:709â€“721 , 2003 .
[ 22 ] S . G . Nash and A . Sofer . On the complexity of a practical interior point method . SIAM Journal on Optimization , 8(3):833â€“849 , 1998 .
[ 23 ] J . Pearl . Probabilistic Reasoning in Intelligent Systems : Networks of Plausible Inference . Morgan Kaufmann , San Francisco , CA , 1988 .
[ 24 ] T . Roos , H . Wettig , P . GrÂ¨unwald , P . MyllymÂ¨aki , and H . Tirri . On discriminative bayesian network classifiers and logistic regression . Machine Learning , 59(3):267â€“296 , 2005 .
[ 25 ] M . Sahami . Learning limited dependence Bayesian classifiers . In Proceedings of the 2nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 335â€“338 , Portland , OR , 1996 .
[ 26 ] M . Singh and G . M . Provan . Efficient learning of selective bayesian network classifiers . In Proceedings of the 13th International Conference on Machine Learning , pages 453â€“ 461 , Bari , Italy , 1996 .
[ 27 ] P . Venkataraman . Applied Optimization with MATLAB Pro gramming . John Wiley & Sons , New York , NY , 2002 .
[ 28 ] G . I . Webb . Multiboosting : A technique for combining Boosting and Wagging . Machine Learning , 40(2):159â€“196 , 2000 .
[ 29 ] G . I . Webb , J . Boughton , and Z . Wang . Not so naive Bayes : Aggregating one dependence estimators . Machine Learning , 58(1):5â€“24 , 2005 .
[ 30 ] I . H . Witten and E . Frank . Data Mining : Practical Machine Learning Tools and Techniques . Morgan Kaufmann , San Francisco , CA , 2nd edition , 2005 .
[ 31 ] Y . Yang , G . I . Webb , J . Cerquidesz , K . Korb , J . Boughton , and K M . Ting . To select or to weigh : A comparative study of linear combination schemes for superparent one dependence estimators . IEEE Transactions on Knowledge and Data Engineering , 9(12):1652â€“1665 , 2007 .
[ 32 ] H . Zhang , L . Jiang , and J . Su . Hidden naive Bayes .
In Proceedings of the 20th National Conference on Artificial Intelligence , pages 919â€“924 , Pittsburgh , PA , 2005 .
[ 33 ] F . Zheng and G . I . Webb . A comparative study of semi naive Bayes methods in classification learning . In Proceedings of the 4th Australasian Data Mining Conference , pages 141â€“ 156 , Sydney , Australia , 2005 .
