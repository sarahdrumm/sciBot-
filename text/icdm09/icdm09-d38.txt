Semi Naive Exploitation of One Dependence Estimators
Nan Li
Yang Yu
Zhi Hua Zhou
National Key Laboratory for Novel Software Technology
Nanjing University , Nanjing 210093 , China
{lin , yuy , zhouzh}@lamdanjueducn
Abstract—It is well known that the key of Bayesian classifier learning is to balance the two important issues , that is , the exploration of attribute dependencies in high orders for ensuring a sufficient flexibility in approximating the ground truth dependencies , and the exploration of low orders for ensuring a stable probability estimate from limited training samples . By allowing one order attribute dependencies , one dependence estimators ( ODEs ) have been shown to be able to approximate the ground truth attribute dependencies whilst keeping the effectiveness of probability estimation , and therefore leading to excellent performance . In previous studies , however , ODEs were exploited in simple ways , such as by averaging , for classification . In this paper , we propose a semi naive exploitation of ODEs that fits a function of ODEs to pursue higher order attribute dependencies . Extensive experiments show that the proposed SNODE approach can achieve better performance than many state of the art Bayesian classifiers .
Keywords Bayesian classifier ; one dependence estimator ; semi naive Bayes
I . INTRODUCTION
In principle , the optimal Bayesian classifier is the optimal way for supervised classification [ 9 ] , which is , however , a theoretical model that requires infinite number of samples for true joint probabilities . Hence , in practice , restricted Bayesian models are used to approximate the optimal Bayesian model . The restricted models are ideally designed to be flexible sufficiently for capturing the ground truth attribute dependencies as well as succinct sufficiently for estimating probabilities effectively . There is , however , a dilemma ; that is , the model should only allow lower order attribute dependencies for obtaining an effective estimate of probabilities from limited samples , yet for capturing the ground truth attribute dependencies , the model should consider higher order dependencies . It is noteworthy that learning the optimal dependencies has been proven to be NP hard [ 5 ] , [ 6 ] .
Consequently , a spectrum of Bayesian learning approaches has been developed , which takes different balances between the complexity of attribute dependencies and the effectiveness of probability estimation . The two extremes of the spectrum are the naive Bayes [ 20 ] and Bayesian network [ 23 ] , [ 16 ] ; the former totally ignores the attribute dependencies while the latter takes the maximum flexibility for approximating the attribute dependencies . Between the two extremes on the spectrum are semi naive Bayesian clas sifiers which try to find proper tradeoffs and have achieved successes [ 26 ] , [ 25 ] , [ 12 ] , [ 11 ] , [ 17 ] , [ 4 ] , [ 33 ] , [ 36 ] , [ 29 ] , [ 32 ] . utilize classifiers which
Among the numerous Bayesian learning approaches , semi naive Bayesian onedependence estimators ( ODEs ) have achieved remarkable performance . By allowing one order dependencies , the probability estimation in ODEs is effective , while the model still has some flexibility for capturing the attribute dependencies . Representative approaches include TAN ( tree augmented naive Bayes ) [ 12 ] , AODE ( averaged one dependence estimators ) [ 29 ] , HNB ( hidden navie Bayes ) [ 32 ] , etc . Note that in previous studies , ODEs were exploited directly in a simple way , eg , simple average of ODEs . It will be interesting to study whether a better performance can be achieved by exploiting the ODEs in other ways such that the resulting model inherits the effectiveness of ODEs while gains more flexibility for modelling higher order dependencies .
In this paper , we propose a framework of semi naive exploitation of ODEs ( SNODE ) , where the ODEs are employed to capture high order attribute dependencies . SNODE approximates the joint probabilities by using functions of ODEs that are fitted according to maximum likelihood estimation . For implementation , generalized additive model ( GAM ) [ 15 ] is employed in this paper , and the function fitting problem is reduced to a constrained concave optimization problem whose global optimal solution can be efficiently obtained . Experiments show that the performance of SNODE is superior to many state of the art Bayesian classifiers . Bias variance decomposition shows that , the success of SNODE mainly owes to its very low bias with slightly increased variance , which validates our proposal of semi naive exploitation of ODEs .
The rest of the paper is organized as follows . In Section II , we introduce the background of Bayesian classifier learning and give a brief review on related work . In Section III , we propose the SNODE approach . Then we report on our experiments in Section IV . Finally , we conclude the paper in Section V .
II . BACKGROUND AND RELATED WORK
Denote 𝒙 = ( 𝑥1 , 𝑥2,⋅⋅⋅ , 𝑥𝑛 ) as an instance , where 𝑥𝑖 is the value of the 𝑖 th attribute , 𝑐 ∈ {𝑐1 , 𝑐2,⋅⋅⋅ , 𝑐𝑘} the class where 𝑘 is the number of classes , and 𝑐(𝒙 ) denotes the class of an instance 𝒙 .
The optimal Bayesian classifier provides an optimal way to decide the class of an instance 𝒙 as
𝑐(𝒙 ) = arg max
𝑐
𝑃 ( 𝑐∣𝒙 ) = arg max
𝑃 ( 𝒙 , 𝑐 ) ,
𝑐 which yields the minimum possible classification error [ 9 ] . However , it is intractable in practical applications , since real world training data is usually insufficient for a reliable estimate of the joint distribution 𝑃 ( 𝒙 , 𝑐 ) . Consequently , approximating 𝑃 ( 𝒙 , 𝑐 ) becomes the central problem in Bayesian classifier learning .
The modification of the optimal Bayesian classifier towards practical applicability forms a spectrum , along which different Bayesian learning approaches utilizing the attribute dependencies in different ways .
One extreme is naive Bayes [ 20 ] . It simply ignores all the attribute dependencies by taking the conditional independence assumption , ie , the attributes are independent given the class , thus it follows that
𝑃 ( 𝒙 , 𝑐 ) =
𝑛∏
𝑖=1
𝑃 ( 𝑥𝑖∣𝑐 ) .
Ignoring the dependencies makes the naive Bayes approach efficient , yet the missing of important dependence information sometimes hampers the classification performance seriously .
Another extreme is Bayesian network [ 23 ] , [ 16 ] , which has a strong flexibility for representing and exploiting various attribute dependencies . Suppose that the Bayesian network structure of attributes is known , Bayesian network can encode the joint probability distribution as 𝑃 ( 𝑥𝑖∣𝜋𝑖 , 𝑐 ) ,
𝑃 ( 𝒙 , 𝑐 ) = 𝑃 ( 𝑐 )
𝑛∏ performance . Representative examples include TAN and SPTAN [ 12 ] , [ 18 ] , AODE and its variants [ 29 ] , [ 34 ] , [ 35 ] , [ 31 ] , HNB [ 32 ] , etc . TAN restricts that each attribute can only depend on one parent in addition to the class , and thus it follows that
𝑃 ( 𝒙 , 𝑐 ) = 𝑃 ( 𝑐 )
𝑛∏
𝑃 ( 𝑥𝑖∣𝑝𝑎(𝑥𝑖 ) , 𝑐 ) ,
𝑖=1 where 𝑝𝑎(𝑥𝑖 ) denotes 𝑥𝑖 ’s dependent attribute , and is determined in the learning process . In AODE , an ensemble of ODEs is learned and the prediction is produced by aggregating the predictions of all qualified ODEs . In HNB , a hidden parent is created for each attribute which combines the influences from all other attributes .
Some other approaches implicitly restrict the dependencies by localizing naive Bayes classifiers , such as NBTree [ 19 ] which embeds a naive Bayes classifier in the leaves of a pre trained decision tree , and LBR ( lazy Bayesian rule ) [ 36 ] which trains a naive Bayes classifier under a local rule .
The success of semi naive Bayesian classifiers using ODEs suggests that ODEs are on well balance between the ground truth dependencies approximation and the effectiveness of probability estimation . However , in previous approaches , ODEs were used directly in simple ways for classification . In next section , we will present the SNODE method which exploits ODEs in a semi naive way .
III . SNODE
A . The Framework
Suppose that the ground truth dependency structure of the attributes is known . The joint probability distribution can be denoted as
𝑃 ( 𝒙 , 𝑐 ) = 𝑃 ( 𝑐 )
𝑛∏
𝑃 ( 𝑥𝑖∣𝝅𝑖 , 𝑐 ) ,
( 1 )
𝑖=1
𝑖=1 where 𝜋𝑖 denotes the values of the parents of the 𝑖 th attribute excluding the class value 𝑐 for the instance 𝒙 . However , learning the optimal structure , even on restricted structures , is NP hard [ 5 ] , [ 6 ] . Moreover , the data become overly sparse when the dependency order goes high , which causes both the ineffectiveness of probability estimation and inaccurate evaluations in the search of a good structure .
Between the two extremes of the spectrum are semi naive Bayesian classifiers [ 33 ] . Different from totally ignoring the attribute dependencies as naive Bayes or taking maximum flexibility for modelling dependencies as Bayesian network , semi naive Bayesian classifiers try to exploit attribute dependencies in moderate orders . For example , 𝑘 DE ( 𝑘dependence estimator ) [ 25 ] allows each attribute to depend on at most 𝑘 other attributes in addition to the class .
Among numerous semi naive Bayesian classifiers , approaches which utilize ODEs have demonstrated remarkable where 𝝅𝑖 is the attribute values on which the 𝑖 th attribute depends . Based on Eq 1 , the problem of estimating 𝑃 ( 𝒙 , 𝑐 ) can be reduced to estimating the probabilities 𝑃 ( 𝑐 ) and 𝑃 ( 𝑥𝑖∣𝝅𝑖 , 𝑐 ) ’s . Since 𝑃 ( 𝑐 ) can be easily estimated from the data , the key is to estimate 𝑃 ( 𝑥𝑖∣𝝅𝑖 , 𝑐 ) ’s . Obviously , 𝑃 ( 𝑥𝑖∣𝝅𝑖 , 𝑐 ) ’s also encode the dependencies between attributes .
To maintain the effectiveness of probability estimation , we use only ODEs . Consequently , we want to approximate 𝑃 ( 𝑥𝑖∣𝝅𝑖 , 𝑐 ) ’s from 𝑃 ( 𝑥𝑖∣𝑥𝑗 , 𝑐 ) ’s ; in other words , we want to model the underlying relationship between 𝑃 ( 𝑥𝑖∣𝝅𝑖 , 𝑐 ) and 𝑃 ( 𝑥𝑖∣𝑥𝑗 , 𝑐 ) ’s for each different 𝑖 .
For this purpose , we can search in some function space for a function 𝐹𝑖 by optimizing a certain criterion , such that the inputs of 𝐹𝑖 are 𝑃 ( 𝑥𝑖∣𝑥𝑗 , 𝑐 ) ’s and the output is 𝑃 ( 𝑥𝑖∣𝝅𝑖 , 𝑐 ) . The log likelihood and conditional likelihood are commonly used optimization criterions , thus can be used here .
1 ) Formulation : GAM relates the variables 𝑣𝑖 ’s to the objective variable 𝑜 through the link function 𝑔(⋅ ) and the smoothing functions 𝑓𝑖(⋅ ) ’s as 𝑛∑
𝑔(𝑜 ) =
𝜆𝑖𝑗 𝑓𝑗(𝑣𝑖 ) ,
𝑗=1
Employing GAM to approximate 𝑃 ( 𝑥𝑖∣𝝅𝑖 , 𝑐 ) ’s from where 𝜆𝑖𝑗 ’s are model parameters . 𝑃 ( 𝑥𝑖∣𝑥𝑗 , 𝑐 ) ’s , it becomes 𝑔(𝑃 ( 𝑥𝑖∣𝝅𝑖 , 𝑐 ) ) =
𝑛∑
𝜆𝑖𝑗 𝑓𝑗(𝑃 ( 𝑥𝑖∣𝑥𝑗 , 𝑐) ) ,
𝑗=1
( 7 ) where 𝑃 ( 𝑥𝑖∣𝝅𝑖 , 𝑐 ) is the objective variable , and 𝜆𝑖𝑗 ’s are parameters . For simplicity , we use link function 𝑔(⋅ ) = log(⋅ ) and smoothing function 𝑓𝑗(⋅ ) = log(⋅ ) here . As the consequence , Eq 7 can be rewritten as ) ) ( 𝑃 ( 𝑥𝑖∣𝑥𝑗 , 𝑐 ) .
𝑃 ( 𝑥𝑖∣𝝅𝑖 , 𝑐 ) = exp
( 𝑛∑
𝜆𝑖𝑗 log
𝑗=1
( 8 )
( 9 )
( 10 )
( 11 )
Let 𝒑𝑖 = [ 𝑃 ( 𝑥𝑖∣𝑥1 , 𝑐),⋅⋅⋅ , 𝑃 ( 𝑥𝑖∣𝑥𝑛 , 𝑐)]⊤
. The joint dis tribution can be written as
𝑃 ( 𝒙 , 𝑐 ) = 𝑃 ( 𝑐 )
𝑛∏
𝑖=1
𝐹𝑖(𝒑𝑖 ) ,
( 2 ) where 𝐹𝑖 ’s are functions to be found . Given a training data set 𝒟 , the log likelihood of the model is ∑ 𝑛∑
∑
𝐿𝐿(𝑭 ) = log 𝑃 ( 𝑐(𝒙 ) ) + log 𝐹𝑖(𝒑𝑖 )
𝒙∈𝒟
𝒙∈𝒟 ∑
𝒙∈𝒟 log 𝑃 ( 𝑐(𝒙 ) ) +
𝑖=1
𝑛∑
𝑖=1
∑
= where
𝐿𝑖(𝐹𝑖 ) ,
( 3 )
𝐿𝑖(𝐹𝑖 ) =
( 4 ) 𝑐(𝒙 ) is the class of an instance 𝒙 , and 𝑭 = {𝐹1,⋅⋅⋅ , 𝐹𝑛} is the set of functions to be determined . log 𝐹𝑖(𝒑𝑖 ) ,
𝒙∈𝒟
Taking log likelihood as the optimization criterion , the function optimization problem can be formalized as max
{𝐹1,⋅⋅⋅ ,𝐹𝑛} log 𝑃 ( 𝑐(𝒙 ) ) +
∑
𝒙∈𝒟
𝑠𝑡 𝐹𝑖 ∈ ℱ
𝑛∑
𝐿𝑖(𝐹𝑖 ) 𝑖 = 1 , 2,⋅⋅⋅ , 𝑛
𝑖=1 where ℱ is the feasible function space from which the 𝐹𝑖 ’s are chosen .
Obviously , in Eq 5 ,
𝒙∈𝒟 log 𝑃 ( 𝑐(𝒙 ) ) is a constant , and the objective is linear . Thus , if all 𝐹𝑖 ’s are independent , solving Eq 5 is equivalent to solving a series of problems in Eq 6 for different 𝑖 ’s
∑
𝐹𝑖∈ℱ 𝐿𝑖(𝐹𝑖 ) max where 𝐿𝐿𝑖(𝐹𝑖 ) is defined in Eq 4 .
Once the functions 𝐹𝑖 ’s are obtained by solving the optimization problem in Eq 6 , the joint distribution 𝑃 ( 𝒙 , 𝑐 ) can be computed according to Eq 2 , and the class of instance 𝒙 can be decided .
In SNODE , the functions of ODEs are fitted to gain the model flexibility as that by modelling higher order attribute dependencies . Though maximum likelihood criterion is employed here to fit the functions , other criterions such as minimum square error or conditional likelihood can also be employed .
B . Modelling Higer Order Dependencies with ODEs
Generalized additive model ( GAM ) [ 15 ] is a statistical model that permits the response probability distribution to be any member of the exponential family of distributions , which is also the probability distribution represented by Bayesian networks [ 16 ] . Here , we employ GAM for the implementation of our proposed SNODE framework . Note that it is also possible to use other models , which is an interesting future work .
( 5 )
Considering normalization , the probability distribution can be written as
𝑃 ( 𝑥𝑖∣𝝅𝑖 , 𝑐 ) =
1
𝑆𝑖(𝝀𝑖 ) exp
( 𝝀⊤
𝑖
) ,
⋅ 𝒍𝑖 where
[ 𝝀𝑖 = [
𝒍𝑖 = 𝑆𝑖(𝝀𝑖 ) =
𝜆𝑖1,⋅⋅⋅ , 𝜆𝑖𝑛 log
]⊤ , ( ) 𝑃 ( 𝑥𝑖∣𝑥1 , 𝑐 ) ,⋅⋅⋅ , log ∑ ( ⋅ 𝒍𝑖 𝝀⊤
) . exp
𝑖
( 𝑃 ( 𝑥𝑖∣𝑥𝑛 , 𝑐 )
)]⊤
,
( 6 )
𝑥′
𝑖
Now , Eq 8 relates 𝑃 ( 𝑥𝑖∣𝝅𝑖 , 𝑐 ) and 𝑃 ( 𝑥𝑖∣𝑥𝑗 , 𝑐 ) ’s , and 𝝀 = [ 𝝀1,⋅⋅⋅ , 𝝀𝑛 ] is the parameter with the constraints
𝑛∑
𝑗=1
𝜆𝑖𝑗 = 1 and
0 ≤ 𝜆𝑖𝑗 ≤ 1 .
2 ) Optimization Problem : Substituting Eq 8 into the optimization problem in Eq 6 , we get the optimization problem to be solved as ∑
( 𝝀⊤
𝑖
) ⋅ 𝒍𝑖 − log(𝑆𝑖(𝝀𝑖 ) )
( 12 ) max
𝝀𝑖
𝑠𝑡
𝒙∈𝒟 𝑛∑
𝜆𝑖𝑗 = 1 ,
0 ≤ 𝜆𝑖𝑗 ≤ 1
𝑗=1 𝑗 = 1 , 2 , . . . , 𝑛 where 𝒍𝑖 and 𝑆𝑖(𝝀𝑖 ) are defined in Eq 10 and 11 , respectively .
Proposition 1 The optimization problem defined in Eq 12 is a constrained concave optimization problem .
⋄ Training Process Input:𝒟 Training data set Process : 1 . Estimate 𝑃 ( 𝑐 ) and 𝑃 ( 𝑥𝑖∣𝑥𝑗 , 𝑐 ) from 𝒟 ; 2 . 3 . 4 . 5 .
Initialize 𝜆𝑖𝑗 = 1/𝑛 , for 𝑖 = 1,⋅⋅⋅ , 𝑛
Solve Eq 12 to get 𝝀𝑖 end for
⋄ Testing Process
Input :
𝒙 = ( 𝑥1,⋅⋅⋅ , 𝑥𝑛 ) Test instances to be classified for all class values 𝑐 for 𝑖 = 1,⋅⋅⋅ , 𝑛
Process : 1 . 2 . 3 . 4 . 5 . 6 . Return 𝑐(𝒙 ) = 𝑎𝑟𝑔𝑚𝑎𝑥𝑐 𝑃 ( 𝑐 ) end for end for
Calculate 𝑃 ( 𝑥𝑖∣𝝅𝑖 , 𝑐 ) based on Eq 8
∏𝑛 𝑖=1 𝑃 ( 𝑥𝑖∣𝝅𝑖 , 𝑐 )
Figure 1 . Pseudo code of the SNODE algorithm
C . Discussion
It is easy to see that our proposed SNODE is a flexible framework . In the current paper , GAM and log likelihood are employed as the model and optimization criterion , respectively . Obviously , other models and criterions can also be utilized .
By using logistic functions , we can get a special case of SNODE which is similar to the well studied log linear model [ 7 ] . This suggests that , it might be possible to derive more insight by connecting the log linear model to seminaive Bayesian classifiers .
Discriminative learning of Bayesian classifiers has atInstead of optimizing the logtached much attention . likelihood , discriminative approaches try to maximize the conditional likelihood and may produce better class probability estimates [ 14 ] , [ 24 ] , [ 3 ] . A discriminant version of SNODE can be developed by maximizing the conditional likelihood ; however , the corresponding optimization problem may be more difficult than the current one , since the conditional likelihood does not decompose into separate terms for each function to be learned .
IV . EXPERIMENTS
Proof : With a simple derivation we can get
A . Settings
∂2𝐿𝑖(𝝀𝑖 )
∂𝝀2 𝑖
≤ 0 .
Thus , 𝐿𝑖(𝝀𝑖 ) is a concave function with respect to 𝝀𝑖 . Moreover , the feasible region is convex .
So , the problem in Eq 12 is a constrained concave □ optimization problem .
With the above proposition , the problem in Eq 12 has one unique global optimal solution , and it can be solved efficiently by standard optimization algorithms such as interiorpoint methods [ 2 ] .
(
𝑘𝑛2𝑣2
3 ) Algorithm : The pseudo code is shown in Figure 1 . At the training time , SNODE forms the tables of the joint variable values and class frequencies from which the probabilities 𝑃 ( 𝑐 ) and 𝑃 ( 𝑥𝑖∣𝑥𝑗 , 𝑐 ) are calculated . The space ) , where 𝑘 is the complexity of the tables is 𝑂 number of classes , 𝑣 is the average number of values of every attribute , and 𝑛 is the number of attributes . Derivation of the frequencies required to populate these tables is of time complexity 𝑂(𝑁 𝑛2 ) , where 𝑁 is the number of training examples . The concave optimization problem is critical to the training process , and its time complexity is 𝑂(𝑛3 ) [ 22 ] . Therefore , the overall training time required is 𝑂(𝑛2(𝑁 + 𝑛2) ) .
At the testing time , classifying an instance requires the calculation of Eq 8 , and it is of time complexity 𝑂(𝑘𝑛2 ) .
Thirty five UCI data sets are used in our experiments . These data sets span a broad range of real domains , with sizes ranging from 76 to 20,000 . Some statistics of the data sets are summarized in Table I , where#Inst , #Attr and #Class mean the number of instances , attributes and classes , respectively .
Considering that many Bayesian classifiers could not handle missing values and real values directly , for simplicity , we filled in the missing values by the mode or mean of the corresponding variable , and discretized all the data using the MDL discretization [ 10 ] .
We compare SNODE with several Bayesian classifiers , including the simplest one , naive Bayes ( NB ) , a general Bayesian network learning algorithm , K2 [ 8 ] , and state ofthe art semi naive Bayesian methods including TAN , AODE and HNB . SNODE was implemented in WEKA [ 30 ] , using the MATLAB optimization toolbox [ 27 ] for optimization . The implementations of NB , K2 , TAN , AODE and HNB in WEKA were used here . Laplacian correction was used in probability estimations .
Bias variance decomposition [ 13 ] is helpful for analyzing performance of learning algorithms . It breaks the error into bias and variance . Bias describes the error of the learner in expectation , while variance reflects the sensitivity of the learner to variations in the training samples . We performed bias variance decomposition using the repeated cross validation approach [ 28 ] , which is the default method in WEKA .
DATA SETS USED IN THE EXPERIMENTS .
Table I
Data set artificial auto mpg balance scale balloons clean1 cmc coding colic cylinder bands ecoli glass haberman heart c heart statlog hepatitis house votes 84 hypothyroid ionosphere
# Inst 5,109 398 625 76 476 1,473 20,000 368 540 336 214 306 303 270 155 435 3,772 351
# Attr 8 8 5 5 133 10 16 17 26 7 8 3 12 10 17 17 26 34
# Class 10 4 3 2 2 3 2 2 2 8 7 2 5 2 2 2 4 2
Data set iris kr vs kp led24 led7 machine mfeat mor nursery page blocks segment sick solar flare 2 tic tac toe titanic vehicle waveform 5000 yeast zoo
# Inst 150 3,196 3,200 3,200 209 2,000 12,960 5,473 2,310 3,772 1,066 958 2,201 846 5,000 1,484 101
# Attr 5 37 25 8 8 7 9 11 19 26 12 10 4 19 20 8 17
# Class 3 2 10 10 8 10 5 5 7 2 6 2 2 4 3 10 7
In our experiments , fifty runs of two fold cross validation were executed and divided into 5 groups each contained 10 runs . Bias variance decomposition was performed on each group , and thus the averaged predictive error and a pair of bias/variance were obtained for each group . The mean and standard deviations were recorded , and pairwise 𝑡 tests with 95 % significance level were conducted .
B . Results
The error , bias and variance averaged across all data sets for the compared classifiers are presented in Table II . The results of pairwise 𝑡 tests are summarized in Table III , where win/tie/loss means that SNODE wins , ties and loses on #win , #tie and #loss number of data sets against the corresponding comparison algorithm , according to pairwise t tests . Detailed information over all data sets can be found in Tables IV , V and VI , respectively .
Figures 2 , 3 and 4 depict the error , bias and variance relative to NB , respectively , where the 𝑥 axis shows error ( bias , variance ) of SNODE divided by that of NB , while the 𝑦 axis shows that of the compared classifier . Each point in the figures corresponds to one data set . The vertical and horizonal lines ( ie , 𝑥 = 1 and 𝑦 = 1 ) are used to highlight the performance of NB , and the diagonal lines ( ie , 𝑦 = 𝑥 ) indicate equal performance of the compared approaches . If the error ( bias , variance ) of SNODE is better than the compared method on a data set , the corresponding point will appear above the diagonal line .
1 ) Comparison with State of the art Methods : It can be observed from Tables II and IV that SNODE achieved the lowest error on much more data sets than the compared algorithms , and it achieved the lowest average error . Concerning pairwise t tests results in Table III , SNODE always has larger counts of wins than losses .
Table II
AVERAGED ERROR , BIAS AND VARIANCE OVER ALL DATA SETS , WHERE THE BEST AND WORST PERFORMANCE IN EACH ROW ARE BOLDED AND
UNDERLINED , RESPECTIVELY .
SNODE AODE HNB 0.1868 0.1879 0.1505 0.1524 0.0363 0.0355
0.1913 0.1605 0.0308
TAN 0.2081 0.1805 0.0275
K2
0.1927 0.1511 0.0416
NB
0.2095 0.1833 0.0273
Error Bias Variance
Table III
WIN/TIE/LOSS COUNTS , WHICH SUMMARIZES THE COMPARISON OF SNODE AGAINST THE CORRESPONDING ALGORITHM ACCORDING TO
PAIRWISE t TESTS WITH 95 % SIGNIFICANCE LEVEL
Error Bias Variance
AODE 16/10/9 25/5/5 7/12/16
HNB 10/19/6 16/12/7 8/19/8
TAN 19/9/7 30/4/1 5/8/22
K2
14/16/5 10/16/9 24/5/6
NB 23/5/7 30/4/1 5/11/19
In Figure 2 , most points appear to the left of the vertical line 𝑥 = 1 , indicating that SNODE performs better than NB . Also , the number of points appear above the diagonal line are apparently more than that below the diagonal line , indicating that the error of SNODE error is lower than that of the compared algorithms .
If we divide these algorithms into two groups , ie , extreme group consisting of NB and K2 , and moderate group consisting of SNODE , AODE , HNB and TAN , it can be found that classifiers in the moderate group performs better than those in the extreme group , which supports the motivation of developing Bayesian classifiers with the exploitation of moderate attribute dependencies .
Within the moderate group , it can be found that TAN sometimes has relatively poor performance , such as on house votes 84 , nursery and kr vs kp , while SNODE , AODE
1.2
1
0.8
0.6
/
B N E D O A
0.4
0.4
0.6
0.8
SNODE/NB
1
1.2
/
B N B N H
1
0.8
0.6
0.4
0.4
1.2
1
0.8
0.6
/
B N N A T
1.2
1
0.8
0.6
B N / 2 K
0.6
0.8
SNODE/NB
1
1.2
0.4
0.4
0.6
0.8
SNODE/NB
1
1.2
0.4
0.4
0.6
0.8
SNODE/NB
1
1.2
( a ) SNODE vs . AODE
( b ) SNODE vs . HNB
( c ) SNODE vs . TAN
( d ) SNODE vs . K2
Figure 2 . Comparison of relative ERROR . The 𝑥 axis presents error of SNODE divided by that of NB , 𝑦 axis presents error of the compared classifier divided by that of NB , and each point corresponds to one data set .
1.2
1
0.8
0.6
/
B N E D O A
1.2
1
0.8
0.6
/
B N B N H
/
B N N A T
1
0.8
0.6
1.2
1
0.8
0.6
B N / 2 K
0.4
0.4
0.6
0.8
1 SNODE/NB
1.2
0.4
0.4
0.6
0.8
1 SNODE/NB
1.2
0.4
0.4
0.6
0.8
1 SNODE/NB
0.4
0.4
0.6
0.8
1 SNODE/NB
1.2
( a ) SNODE vs . AODE
( b ) SNODE vs . HNB
( c ) SNODE vs . TAN
( d ) SNODE vs . K2
Figure 3 . Comparison of relative BIAS . The 𝑥 axis presents bias of SNODE divided by that of NB , the 𝑦 axis presents bias of the compared classifier divided by that of NB , and each point corresponds to one data set .
3
2.5
2
1.5
1
/
B N E D O A
0.5
0.5
1
2
1.5 SNODE/NB
2.5
3
2.5
2
1.5
1
/
B N B N H
3
0.5
0.5
1
2
1.5 SNODE/NB
2.5
3
2.5
2
1.5
1
/
B N N A T
3
0.5
0.5
1
2
1.5 SNODE/NB
2.5
3
2.5
2
1.5
1
B N / 2 K
3
0.5
0.5
1
2
1.5 SNODE/NB
2.5
3
( a ) SNODE vs . AODE
( b ) SNODE vs . HNB
( c ) SNODE vs . TAN
( d ) SNODE vs . K2
Figure 4 . Comparison of relative VARIANCE . The 𝑥 axis presents variance of SNODE divided by that of NB , the 𝑦 axis presents variance of the compared classifier divided by that of NB , and each point corresponds to one data set . and HNB , which utilize ODEs , have relatively close performance on most data sets .
2 ) Bias Variance Analysis : It has been shown that the maximum dependency determines the upper bound of representation ability of a Bayesian classifier [ 21 ] ; this implies that , from the viewpoint of version space , the maximum dependency determines the upper bound of the range of the function space . It is known that the larger the version space , the lower the bias , and the larger the variance ; this suggests a connection between bias/variance and the flexibility for dependencies in Bayesian classifiers .
From the comparison of average bias and variance shown in Table II , it can be found that NB was with the highest bias but the lowest variance , K2 exhibited the lowest bias but the highest variance , and TAN , AODE and HNB achieved a tradeoff with bias and variance . Similar behaviors can also be observed in detailed results shown in Tables V and VI . This observation can be explained from the fact that NB ignores all dependencies , K2 tries to exploit all possible dependencies , while TAN , AODE and HNB try to utilize dependencies only in moderate orders .
In Figures 3 and 4 it can be found that , compared with K2 , SNODE achieved comparable bias yet much lower variance ; compared with TAN , AODE and HNB , SNODE exhibited lower bias yet higher variance . This is also verified by pairwise t tests shown in Table III . Hence , we can conclude that the success of SNODE owes much to its low bias .
In Table II it can be seen that SNODE achieved the lowest bias while its variance is still lower than K2 . This may
COMPARISON OF PREDICTIVE ERRORS ( MEAN±STD. ) , WHERE THE BEST AND WORST PERFORMANCE IN EACH ROW ARE BOLDED AND UNDERLINED , RESPECTIVELY . THE average ROW PRESENTS THE RESULTS AVERAGED OVER ALL THE 35 DATA SETS ; win/tie/loss ROW SUMMARIZES THE COMPARISON OF SNODE AGAINST THE CORRESPONDING ALGORITHM ACCORDING TO PAIRWISE t TESTS
Table IV
WITH 95 % SIGNIFICANCE LEVEL .
Data set artificial auto mpg balance scale balloons clean1 cmc coding colic cylinder bands ecoli glass haberman heart c heart statlog hepatitis house votes 84 hypothyroid ionosphere iris kr vs kp led24 led7 machine mfeat mor nursery page blocks segment sick solar flare 2 tic tac toe titanic vehicle waveform 5000 yeast zoo average win/tie/loss
SNODE 3709±0028 2252±0065 2636±0027 2622±0171 0847±0045 4560±0028 2444±0004 1889±0048 2443±0054 1667±0101 2763±0068 2645±0052 1762±0046 1749±0055 1323±0086 0494±0016 0116±0004 0829±0018 0680±0033 0586±0007 2755±0014 2649±0004 1297±0070 3086±0034 0583±0003 0352±0004 0509±0008 0242±0002 2508±0011 2196±0072 2113±0007 2715±0043 1585±0004 4100±0028 0743±0044
0.1868
—
AODE
4119±0017 2422±0061 2659±0014 2592±0129 0999±0043 4799±0021 2628±0003 1807±0014 2348±0039 1531±0068 2612±0032 2625±0028 1674±0030 1715±0031 1416±0047 0571±0010 0161±0002 0824±0033 0571±0018 0947±0006 2757±0011 2656±0006 1342±0046 2997±0023 0747±0005 0293±0006 0551±0007 0266±0002 2554±0016 2504±0029 2169±0004 2835±0054 1397±0009 4114±0024 0753±0066
0.1913 16/10/9
HNB
3554±0021 2395±0086 2664±0018 2663±0206 1067±0039 4590±0026 2450±0002 1873±0040 2480±0061 1756±0074 2629±0057 2762±0063 1736±0026 1772±0043 1263±0069 0561±0016 0111±0004 0789±0024 0663±0052 0787±0007 2758±0010 2655±0005 1174±0043 3034±0036 0605±0004 0319±0006 0380±0005 0248±0001 2575±0018 2346±0044 2104±0005 2742±0021 1418±0007 4103±0024 0751±0049
0.1879 10/19/6
TAN
4619±0016 3042±0080 2600±0019 2708±0143 1356±0041 4827±0028 2880±0005 1941±0027 2290±0031 1515±0056 2825±0080 2627±0029 1672±0072 1652±0057 1494±0118 0986±0009 0158±0002 0895±0013 0544±0027 1270±0011 2753±0016 2650±0006 1401±0035 3043±0023 0978±0010 0634±0007 0846±0013 0297±0002 2605±0016 2915±0062 2242±0013 3747±0018 1925±0013 4133±0022 0753±0123
0.2081 19/9/7
K2
4013±0015 2395±0084 2627±0014 2727±0226 0921±0018 4631±0020 2450±0003 1927±0032 2296±0029 1584±0045 2730±0037 2597±0035 1822±0021 1815±0027 1464±0031 0535±0006 0090±0002 0787±0005 0542±0017 0766±0012 2865±0007 2651±0002 1294±0071 2804±0011 0650±0002 0403±0004 0903±0009 0258±0002 2528±0013 2491±0011 2128±0013 2760±0016 1809±0004 4111±0028 1090±0067
0.1927 14/16/5
NB
4779±0012 3127±0066 2600±0013 2719±0210 1479±0010 4826±0020 2880±0003 1932±0028 2379±0028 1534±0068 2866±0014 2610±0020 1675±0022 1651±0025 1523±0027 0987±0007 0180±0001 0912±0012 0543±0015 1273±0011 2753±0008 2650±0002 1496±0069 3041±0013 0979±0002 0652±0003 0525±0009 0292±0002 2603±0012 2913±0010 2236±0015 3757±0018 1928±0004 4154±0030 0876±0064
0.2095 23/5/7 suggest that , the utilization of many ODEs is sufficient to obtain a bias as low as that can be obtained by exploiting a full Bayesian structure . Furthermore , it is usually feasible to estimate one dependent estimators accurately in real applications . This validates our purpose of gaining flexibility as that by modelling higher order probabilities based on onedependent estimators .
V . CONCLUSION
In contrast to many previous Bayesian learning methods which utilize ODEs directly for classification in a simple way , in this paper , we present the SNODE framework , a semi naive exploration of ODEs . In SNODE , functions of ODEs are employed to gain the flexibility by modelling higher order attribute dependencies . As a special case , generalized additive model is employed for the implementation , and the function optimization problem is then reduced to an optimization problem whose global optimum can be solved efficiently .
Experiment results show that the proposed SNODE achieves better performance than many state of the art Bayeian classifiers . Bias variance decomposition discloses that , SNODE achieves very low bias ; this validates our proposal of gaining the flexibility as that by modelling higher order attribute dependencies based on ODEs . Moreover , the bias variance decomposition suggests that there is a notable space for reducing the variance of SNODE . It is an interesting future work to improve SNODE by exploring regularization in the maximum likelihood estimation [ 1 ] .
ACKNOWLEDGMENT
This work was supported by the National Science Foundation of China ( 60635030 , 60721002 ) , the Jiangsu Science Foundation ( BK2008018 ) and the Jiangsu 333 High Level Talent Cultivation Program .
COMPARISON OF BIAS ( MEAN±STD. ) , WHERE THE BEST AND WORST PERFORMANCE IN EACH ROW ARE BOLDED AND UNDERLINED , RESPECTIVELY . THE average ROW PRESENTS THE RESULTS AVERAGED OVER ALL THE 35 DATA SETS ; win/tie/loss ROW SUMMARIZES
THE COMPARISON OF SNODE AGAINST THE CORRESPONDING ALGORITHM ACCORDING TO PAIRWISEt TESTS WITH 95 %
Table V
SIGNIFICANCE LEVEL .
Data set artificial auto mpg balance scale balloons clean1 cmc coding colic cylinder bands ecoli glass haberman heart c heart statlog hepatitis house votes 84 hypothyroid ionosphere iris kr vs kp led24 led7 machine mfeat mor nursery page blocks segment sick solar flare 2 tic tac toe titanic vehicle waveform 5000 yeast zoo average win / tie / loss
SNODE 2685±0021 1736±0069 1848±0032 1776±0157 0631±0032 3721±0042 2328±0003 1580±0021 2041±0027 1271±0057 2318±0047 2185±0053 1469±0033 1591±0044 1058±0059 0432±0017 0079±0004 0788±0025 0433±0041 0503±0006 2370±0011 2257±0019 0754±0065 2346±0017 0513±0001 0314±0006 0366±0012 0222±0003 2106±0024 1660±0038 2105±0002 2116±0058 1340±0010 3621±0008 0158±0072
0.1505
—
AODE
2973±0033 1793±0042 1909±0031 1845±0062 0808±0036 4069±0041 2516±0004 1622±0027 1870±0043 1261±0029 2128±0033 2240±0038 1547±0039 1575±0042 1273±0084 0537±0013 0138±0002 0730±0047 0541±0016 0795±0008 2446±0010 2294±0014 0930±0048 2606±0026 0684±0006 0243±0003 0425±0009 0250±0002 2239±0017 2182±0033 2034±0011 2282±0051 1222±0008 3753±0018 0408±0048
0.1605
25 / 5 / 5
HNB
2509±0014 1670±0089 1907±0029 1950±0110 0926±0056 3809±0024 2344±0002 1630±0033 1741±0063 1341±0045 1958±0031 2180±0054 1519±0037 1617±0037 1019±0116 0530±0004 0080±0002 0713±0037 0467±0031 0671±0009 2446±0010 2288±0022 0784±0038 2341±0025 0538±0004 0272±0009 0280±0002 0234±0001 2183±0013 1927±0034 2092±0004 2191±0017 1248±0010 3654±0041 0293±0067
0.1524
16 / 12 / 7
TAN
3679±0040 2448±0045 1841±0036 1921±0097 1247±0030 4342±0020 2778±0009 1772±0026 1781±0042 1253±0016 2252±0060 2355±0041 1578±0056 1549±0066 1377±0077 0963±0011 0135±0004 0845±0014 0516±0044 1126±0021 2447±0015 2298±0017 1022±0047 2716±0024 0927±0010 0564±0004 0737±0017 0279±0003 2294±0022 2680±0036 2178±0010 3224±0038 1862±0013 3780±0018 0409±0069
0.1805
30 / 4 / 1
K2
2791±0013 1809±0058 1837±0011 1959±0182 0638±0018 3680±0042 2285±0004 1554±0022 1603±0057 1266±0015 2061±0050 2245±0060 1485±0027 1573±0025 1081±0058 0429±0008 0074±0003 0680±0022 0485±0020 0649±0010 2308±0011 2265±0009 0832±0067 2379±0024 0571±0004 0341±0009 0374±0007 0242±0005 2106±0017 1820±0028 2070±0012 1973±0043 1292±0008 3696±0027 0260±0080
0.1511
10 / 16 / 9
NB
3795±0026 2519±0065 1839±0014 1955±0173 1351±0014 4337±0043 2778±0004 1765±0019 1917±0047 1263±0007 2301±0094 2359±0057 1583±0030 1551±0026 1410±0058 0964±0007 0162±0003 0871±0022 0521±0013 1129±0011 2446±0012 2298±0008 1113±0039 2718±0023 0927±0005 0583±0008 0785±0009 0277±0003 2330±0012 2679±0030 2182±0010 3232±0041 1865±0009 3783±0015 0550±0061
0.1833
30 / 4 / 1
REFERENCES
[ 1 ] G . Andrew and J . Gao . Scalable training of 𝐿1 regularized log linear models . In Proceedings of the 24th International Conference on Machine learning , pages 33–40 , Corvalis , OR , 2007 .
[ 2 ] S . Boyd and L . Vandenberghe . Convex Optimization . Cam bridge University Press , Cambridge , UK , 2004 .
[ 3 ] J . Burge and T . Lane . Learning class discriminative dynamic bayesian networks . In Proceedings of the 22nd International Conference on Machine learning , pages 97–104 , Bonn , Germany , 2005 .
[ 4 ] J . Cerquides and R . L . D . M´antaras . Robust bayesian linear classifier ensembles . In Proceedings of the 16th European Conference on Machine learning , pages 70–81 , Porto , Portugal , 2005 .
[ 5 ] D . M . Chickering .
Learning Bayesian networks is NPComplete . In Proceedings of the 5th International Workshop on Artificial Intelligence and Statistics , pages 121–130 , Fort Lauderdale , FL , 1996 .
[ 6 ] D . M . Chickering , D . Heckerman , and C . Meek . Largesample learning of Bayesian networks is NP hard . Journal of Machine Learning Research , 5:1287–1330 , 2004 .
[ 7 ] R . Christensen . Log Linear Models and Logistic Regression .
Springer , New York , NY , 1997 .
[ 8 ] G . Cooper and E . Herskovits . Bayesian method for the induction of probabilistic networks from data . Machine Learning , 9(4):309–347 , 1992 .
[ 9 ] R . O . Duda and P . E . Hart . Pattern Classification and Scene
Analysis . John Wiley & Sons , New York , NY , 1973 .
[ 10 ] U . M . Fayyad and K . B . Irani . Multi interval discretization of continuous valued attributes for classification learning . In
COMPARISON OF VARIANCE ( MEAN±STD. ) , WHERE THE BEST AND WORST PERFORMANCE IN EACH ROW ARE BOLDED AND
Table VI
UNDERLINED , RESPECTIVELY . THE average ROW PRESENTS THE RESULTS AVERAGED OVER ALL THE 35 DATA SETS ; win/tie/loss ROW SUMMARIZES THE COMPARISON OF SNODE AGAINST THE CORRESPONDING ALGORITHM ACCORDING TO PAIRWISE t TESTS WITH
95 % SIGNIFICANCE LEVEL .
Data set artificial auto mpg balance scale balloons clean1 cmc coding colic cylinder bands ecoli glass haberman heart c heart statlog hepatitis house votes 84 hypothyroid ionosphere iris kr vs kp led24 led7 machine mfeat mor nursery page blocks segment sick solar flare 2 tic tac toe titanic vehicle waveform 5000 yeast zoo average win / tie / loss
SNODE 1024±0024 0515±0029 0787±0019 0842±0074 0215±0032 0839±0023 0116±0002 0308±0036 0402±0042 0394±0096 0444±0085 0459±0076 0294±0045 0157±0040 0265±0043 0061±0024 0037±0003 0041±0011 0247±0047 0084±0007 0385±0019 0392±0019 0543±0090 0740±0032 0070±0003 0038±0005 0142±0006 0020±0002 0401±0019 0535±0041 0008±0007 0599±0044 0245±0009 0478±0036 0584±0038
0.0363
—
AODE
1146±0024 0629±0030 0750±0026 0745±0089 0191±0022 0730±0033 0112±0004 0185±0023 0477±0011 0269±0055 0483±0028 0385±0022 0127±0017 0140±0043 0143±0057 0034±0008 0023±0002 0094±0024 0030±0009 0152±0008 0311±0006 0362±0012 0412±0044 0392±0013 0064±0002 0050±0006 0126±0007 0016±0003 0315±0006 0322±0028 0136±0009 0553±0026 0174±0009 0361±0037 0345±0050
0.0308
7 / 12 / 16
HNB
1045±0033 0725±0027 0756±0032 0711±0104 0141±0025 0781±0014 0106±0002 0242±0027 0739±0046 0414±0062 0670±0068 0581±0062 0216±0025 0155±0024 0243±0052 0031±0013 0031±0003 0076±0015 0195±0062 0116±0009 0312±0014 0366±0022 0389±0028 0694±0036 0067±0003 0046±0004 0099±0005 0014±0002 0392±0015 0419±0031 0012±0008 0551±0024 0170±0009 0449±0043 0457±0068
0.0355
8 / 19 / 8
TAN
0941±0044 0593±0044 0759±0028 0784±0053 0108±0028 0484±0032 0102±0009 0168±0035 0509±0023 0261±0040 0572±0101 0271±0025 0094±0026 0103±0030 0115±0083 0024±0011 0023±0005 0049±0011 0028±0027 0144±0010 0307±0004 0352±0016 0378±0013 0327±0009 0052±0004 0070±0003 0109±0010 0017±0002 0311±0029 0236±0041 0065±0022 0523±0034 0063±0013 0353±0024 0344±0150
0.0275
5 / 8 / 22
K2
1223±0027 0587±0060 0790±0011 0766±0149 0283±0012 0951±0037 0167±0002 0372±0011 0693±0038 0318±0051 0669±0033 0353±0034 0339±0022 0243±0022 0383±0033 0105±0010 0018±0003 0107±0017 0056±0010 0117±0006 0557±0012 0386±0007 0462±0035 0425±0033 0078±0005 0062±0009 0150±0003 0016±0005 0422±0012 0670±0026 0058±0023 0787±0038 0517±0006 0415±0030 0829±0043
0.0416
24 / 5 / 6
NB
0984±0024 0607±0061 0761±0014 0761±0158 0128±0017 0488±0042 0102±0002 0166±0010 0462±0027 0270±0072 0563±0103 0250±0050 0091±0018 0099±0018 0113±0041 0023±0006 0018±0004 0041±0014 0021±0015 0144±0006 0307±0013 0351±0007 0382±0061 0323±0034 0052±0005 0069±0009 0118±0004 0015±0003 0273±0007 0234±0028 0053±0023 0525±0038 0063±0007 0370±0039 0325±0016
0.0273
5 / 11 / 19
Proceedings of the 13th International Joint Conference on Artificial Intelligence , pages 1022–1029 , Chambery , France , 1993 .
[ 11 ] E . Frank , M . Hall , and B . Pfahringer . Locally weighted naive bayes . In Proceedings of the 19th Conference on Uncertainty in Artificial Intelligence , pages 249–256 , Acapulco , Mexico , 2003 .
[ 12 ] N . Friedman , D . Geiger , and M . Goldszmidt . Bayesian network classifiers . Machine Learning , 29(2 3):131–163 , 1997 .
[ 13 ] S . German , E . Bienenstock , and R . Doursat . Neural networks and the bias/variance dilemma . Neural Computation , 4(1):1– 58 , 1992 .
[ 14 ] D . Grossman and P . Domingos . Learning Bayesian network classifiers by maximizing conditional likelihood . In Proceedings of the 21st International Conference on Machine Leanring , pages 361–368 , Banff , Canada , 2004 .
[ 15 ] T . Hastie and R . Tibshirani . Generalized Additive Models .
Chapman and Hall , New York , NY , 1990 .
[ 16 ] D . Heckerman . A tutorial on learning with Bayesian netTechnical Report MSR TR 95 06 , Microsoft Re works . search , 1995 .
[ 17 ] Y . Jing , V . Pavlovi´c , and J . M . Rehg . Efficient discriminative learning of bayesian network classifier via boosted augmented naive bayes . In Proceedings of the 22nd International Conference on Machine learning , pages 369–376 , Bonn , Germany , 2005 .
[ 18 ] E . Keogh and M . Pazzani . Learning augmented Bayesian classifiers : A comparison of distribution based and classificationbased approaches . In Proceedings of the 15th International Workshop on Artificial Intelligence and Statistics , pages 225– 230 , Stockholm , Sweden , 1999 .
[ 19 ] R . Kohavi . Scaling up the accuracy of naive Bayes classifiers : In Proceedings of the 2nd ACM
A decision tree hybrid .
[ 34 ] F . Zheng and G . I . Webb . Efficient lazy elimination for averaged one dependence estimators . In Proceedings of the 23rd European Conference on Machine Learning , pages 1113–1120 , Pittsburgh , PA , 2007 .
[ 35 ] F . Zheng and G . I . Webb . Finding the right family : Parent and child selection for averaged one dependence estimators . In Proceedings of the 18th European Conference on Machine Learning , pages 490–501 , Warsaw , Poland , 2007 .
[ 36 ] Z . Zheng and G . I . Webb . Lazy learning of Bayesian rules .
Machine Learning , 41(1):53–84 , 2000 .
SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 202–207 , Portland , OR , 1996 .
[ 20 ] P . Langley , W . Iba , and K . Thompson . An analysis of Bayesian classifiers . In Proceedings of the 10th National Conference on Artificial Intelligence , pages 223–228 , San Jose , CA , 1992 .
[ 21 ] C . X . Ling and H . Zhang . The representational power of discrete Bayesian networks . Journal of Machine Learning Research , 3:709–721 , 2003 .
[ 22 ] S . G . Nash and A . Sofer . On the complexity of a practical interior point method . SIAM Journal on Optimization , 8(3):833–849 , 1998 .
[ 23 ] J . Pearl . Probabilistic Reasoning in Intelligent Systems : Networks of Plausible Inference . Morgan Kaufmann , San Francisco , CA , 1988 .
[ 24 ] T . Roos , H . Wettig , P . Gr¨unwald , P . Myllym¨aki , and H . Tirri . On discriminative bayesian network classifiers and logistic regression . Machine Learning , 59(3):267–296 , 2005 .
[ 25 ] M . Sahami . Learning limited dependence Bayesian classifiers . In Proceedings of the 2nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 335–338 , Portland , OR , 1996 .
[ 26 ] M . Singh and G . M . Provan . Efficient learning of selective bayesian network classifiers . In Proceedings of the 13th International Conference on Machine Learning , pages 453– 461 , Bari , Italy , 1996 .
[ 27 ] P . Venkataraman . Applied Optimization with MATLAB Pro gramming . John Wiley & Sons , New York , NY , 2002 .
[ 28 ] G . I . Webb . Multiboosting : A technique for combining Boosting and Wagging . Machine Learning , 40(2):159–196 , 2000 .
[ 29 ] G . I . Webb , J . Boughton , and Z . Wang . Not so naive Bayes : Aggregating one dependence estimators . Machine Learning , 58(1):5–24 , 2005 .
[ 30 ] I . H . Witten and E . Frank . Data Mining : Practical Machine Learning Tools and Techniques . Morgan Kaufmann , San Francisco , CA , 2nd edition , 2005 .
[ 31 ] Y . Yang , G . I . Webb , J . Cerquidesz , K . Korb , J . Boughton , and K M . Ting . To select or to weigh : A comparative study of linear combination schemes for superparent one dependence estimators . IEEE Transactions on Knowledge and Data Engineering , 9(12):1652–1665 , 2007 .
[ 32 ] H . Zhang , L . Jiang , and J . Su . Hidden naive Bayes .
In Proceedings of the 20th National Conference on Artificial Intelligence , pages 919–924 , Pittsburgh , PA , 2005 .
[ 33 ] F . Zheng and G . I . Webb . A comparative study of semi naive Bayes methods in classification learning . In Proceedings of the 4th Australasian Data Mining Conference , pages 141– 156 , Sydney , Australia , 2005 .
