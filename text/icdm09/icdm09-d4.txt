Fine Grain Perturbation for Privacy Preserving Data
Publishing
Rhonda Chaytor , Ke Wang School of Computing Science
Simon Fraser University , BC , Canada
{rlc8 , wangk}@cssfuca
Abstract— Recent work [ 12 ] shows that conventional privacy preserving publishing techniques based on anonymity groups are susceptible to corruption attacks . In a corruption attack , if the sensitive information of any anonymity group member is uncovered , then the remaining group members are at risk . In this study , we abandon anonymity groups and hide sensitive information through perturbation on the sensitive attribute . With each record being perturbed independently , corruption attacks cannot be effectively carried out . Previous anticorruption work did not minimize information loss . This paper proposes to address this issue by allowing fine grain privacy specification . We demonstrate the power of our approach through experiments on real medical and synthetic datasets .
I .
INTRODUCTION
In the field of privacy preserving data publishing ( PPDP ) , a trusted publisher has collected raw personal data , called microdata , and wants to publish it for research purposes . Suppose that a hospital wants to publish the medical microdata in Fig 1 ( a ) to researchers at a medical school . As Disease is a sensitive attribute ( SA ) , the publication must prevent an adversary from inferring the disease of any patient . At the same time , the publication should retain its usefulness for data analysis .
To prepare microdata for publication , unique identifiers like names are first removed ; however , this de identification is not enough to safeguard against privacy attacks . Linking attacks [ 10][11 ] may still occur when an adversary knows a patient ’s unique combination of public attribute values , called quasi identifier ( QI ) attributes . To illustrate , in Fig 1 ( a ) , Name has been removed , and public attributes Age and Sex will be published . Suppose that Name , Age , and Sex appear publicly in other external sources ( eg , voter registration lists ) , such as the one in Fig 1 ( b ) . In this case , an adversary can join the two tables in Fig 1 ( a ) and ( b ) to reveal for example that the only 32 year old female , Eva , has swine flu virus “ H1N1 ” .
A common technique for preventing linking attacks is hiding an individual in an anonymity group . Consider the generalized publication in Fig 1 ( c ) , where “ * ” represents “ any sex ” . It is k anonymous [ 10][11 ] , k = 2 , because for each patient , there are at least k 1 other patients having identical values on the QI attributes . Therefore , even if an adversary knows that Eva is a 32 year old female , he would only be 1/k = 50 % certain that Eva ’s disease is “ H1N1 ” , since it could equally as likely be “ cancer ” .
Patricia Brantingham School of Criminology
Simon Fraser University , BC , Canada pbrantin@sfu.ca
( a ) Medical microdata
( b ) External table
1 2 3 4 5 6 7 8
1 2 3 4 5 6 7 8
Age 21 25 26 28 32 34 36 39
Disease SARS HIV SARS HIV H1N1 cancer H1N1 cancer
Sex M F F M F F M M
Name Andy Beth Carla Doug Eva Fiona Gord Hank
Age 21 25 26 28 32 34 36 39
Sex M F F M F F M M
( c ) Generalization Age
Sex
[ 21 25 ]
[ 26 30 ]
[ 31 35 ]
[ 36 40 ]
*
*
F
M
Disease SARS HIV SARS HIV H1N1 cancer H1N1 cancer
( d ) Perturbed generalization Disease
Age
Sex
[ 21 25 ]
[ 26 30 ]
[ 31 35 ]
[ 36 40 ]
HIV
H1N1
SARS cancer
*
*
F
M
1 2 3 4 5 6 7 8
Figure 1 . Privacy preserving data publication .
A . Motivation
Recent work [ 12 ] shows that anonymized tables are susceptible to corruption attacks . For example , suppose the adversary learns that 34 year old Fiona has “ cancer ” from background knowledge [ 6 ] . Now from the third group in Fig 1 ( c ) , the adversary can deduce with 100 % certainty that Eva ’s disease is “ H1N1 . ” The authors of [ 12 ] propose Perturbed Generalization ( PG ) to prevent corruption attacks . First , they retain a percentage p of SA values and perturb the rest to other values in the domain . Then , they create anonymity groups of size k by generalizing QI attributes . Finally , they sample one perturbed record from each group . Fig 1 ( d ) shows an example of the final result assuming p = 1/3 and k = 2 . In the third group in Fig 1 ( d ) , there is a 1/2 chance that Eva ’s record gets sampled . Now even if Fiona is corrupted , the probability that Eva has “ H1N1 ” is only 1/3 · 1/2 = 1/6 » 17 % .
Given their pioneering work to combat corruption attacks , the focus in [ 12 ] was not on optimizing utility . Specifically , each of perturbation , generalization , and sampling introduces distortion to the data . Generalization loses significant information for aggregate queries [ 14 ] and record sampling makes it difficult to reconstruct the distribution on
SA , which depends on a sufficient number of perturbed records . B . Contributions
The root of corruption attacks is the anonymity group , which is used to hide an individual . Once a group member ’s SA value is corrupted , the remaining group members are at a higher risk . In this work , we abandon the anonymity group approach . Instead , we disguise a record ’s SA value by perturbing it . Unlike PG [ 12 ] , we publish all QI attributes without modification and all perturbed records without sampling . Since each record ’s SA value is perturbed independently , the knowledge of one individual ’s SA value provides no clue about another individual ’s SA value . In this way , corruption attacks are prevented . There is a detailed discussion in Section III . The focus of this paper is on the utility of perturbed records . A key observation motivates our work .
Fine Grain Privacy SA values are not equally sensitive and should be perturbed according to a probability distribution that matches their sensitivity . We extend ( r1 , r2) privacy in the literature [ 3 ] to allow fine grain ( r1i , r2i)privacy for each SA value xi . Informally , this privacy notion limits the posterior probability of inferring the original SAvalue xi ( after seeing the perturbed record ) to r2i whenever prior probability is no more than r1i . We identify the optimal fine grain perturbation operator for a given ( r1i , r2i) privacy requirement for all SA values xi , where pi is the retention probability for xi . In real life applications , the publisher will set the ( r1i , r2i) privacy parameters for each SA value xi based on the perceived sensitivity of xi . We set ( r1i , r2i ) parameters based on the intuition that “ less frequent values are more sensitive ” , which holds in many practical cases . Example 1 : Assume the ( r1i , r2i) privacy requirement for “ SARS ” , “ HIV ” , “ H1N1 ” , and “ cancer ” is ( 1/10 , 1/7 ) , ( 1/10 , 1/4 ) , ( 1/9 , 19/35 ) , and ( 1/8 , 18/25 ) , respectively . Given the data in Fig 1 ( a ) , the uniform and the optimal fine grain perturbation operators are shown in Fig 2 ( a ) and ( b ) ( more details later ) . Each entry ( j , i ) contains the probability that value xi is perturbed to value xj . As shown by the larger retention probabilities along the diagonal , the fine grain operator retains more of the less sensitive values than uniform operator . ( cid:127 )
Not only does it make sense from a privacy point of view to give the highly sensitive SA values more protection , as our results in Section V demonstrate , this strategy also increases utility for ad hoc privacy preserving data publishing tasks .
SARS
HIV
H1N1 cancer
( a ) Uniform
( b ) Fine grain
' ' ' ' ff
1
3
2
2
2
9
9
9
2
9
1
3
2
2
9
9
2
2
1
2
9
9
3
9
2
2
2
9
9
9
1
3 fi .
' ' ' ' ff
1
1
1
4
4
4
1
4
1
1
1
6
2
6
1
6
1
1
1
6
6
2
1
6
1
1
1
6
6
6
1
2 fi .
Figure 2 . Different perturbation operators .
In the rest of the paper , Section II reviews related work , Section III defines the problem we study , Section IV presents our optimal fine grain perturbation algorithm , Section V evaluates these solutions , and Section VI concludes our paper .
II . RELATED WORK
Since Samarati and Sweeney ’s work [ 10][11 ] on privacy preserving data publishing , there have been several proposals of privacy principles , ( eg , l diversity [ 7 ] , tcloseness [ 5] ) . Tao et al . [ 12 ] showed solutions based on these principles are susceptible to corruption attacks because they rely on anonymity groups to hide information . Parallel to Tao et al . [ 12 ] , Rastogi et al . [ 8 ] also adapted perturbation to data publishing , but did not optimize utility in this setting . Using perturbation to disguise sensitive information was first studied in a classical surveying technique called randomized responses [ 13 ] , and more recently used for privacy preserving data mining , where individuals perturb their data before sending it to the publisher , and the publication is intended for a specific data mining task , such as classification [ 2 ] or association rule mining [ 3][9 ] . We consider privacy preserving data publishing , where unmodified data has been collected by a publisher and is published for unknown ad hoc tasks .
Our work shares a similarity with Huang and Du [ 4 ] on finding optimal perturbation operators . In contrast to our approach , they used a notion of optimality that corresponds to non dominance in the privacy utility space and they searched for all non dominated perturbation operators using a genetic algorithm ; we find a single optimal perturbation operator for a given privacy guarantee .
III . PROBLEM
Given the microdata T containing one sensitive attribute ( SA ) and several quasi identifier ( QI ) attributes , we want to find a perturbed version of T for publication , while guaranteeing a notion of privacy and maximizing some utility metric . We assume all attributes in T are categorical . This section provides basic definitions and assumptions necessary to formally define this problem .
SA has a domain {x1,…,xm} of size m . Let |xi| denote the number of records in T having the value xi in SA and |T | denote the number of records in T . The frequency fi of xi in T is equal to |xi| / |T | . To disguise the sensitive values in SA , the publisher releases a perturbed table T* , in which SAvalues are perturbed to other values according to a fixed probability distribution . We use the following convention : xi denotes a SA value in T and yi denotes the same value in T* . Note that xi and yi are the same ( eg , both are “ H1N1 ” ) , but xi is an original value in T and yi is a perturbed value in T* . X denotes a random variable for xi in a record in T and Y denotes a random variable for yi in a record in T* .
A . Perturbation operator
For a record in T with SA value xi , we retain xi with probability pii and perturb xi to value yj with probability pji , j „ i . These probabilities are also denoted by Pr[xi fi yi ] = pii and Pr[xi fi yj ] = pji . A perturbation operator is a perturbation matrix P having the entry ( j , i ) equal to pji , "i , j = 1,…,m . Note j pji = 1 .
We say that P is a fine grain perturbation matrix if pii = pi + qi and pji = qi , j i , where qi = ( 1 pi)/m . In other words , for each record with SA value xi , xi is retained with probability pi and is perturbed to yj with probability qi , where yj is chosen from the domain of SA . A fine grain perturbation matrix looks like
)1( P
= p 2 q 1
' ' ' ' ff p 1
+ q 1 . q 1 q 2 q q 2 + q fi + p q 2 m m m fi . q m
( 1 )
Instead of a uniform retention probability p , each SA value xi has its own retention probability pji = pi + qi . is , that there is record independent ,
Like previous work ( eg , [ 8] ) , we assume that the adversary is no correlation among records . Each perturbed SA value yj is chosen independently at random according to the probability distribution given in a perturbation matrix P . Let r be an original record with xi on SA and let r* be the perturbed record of r with yj on SA . By receiving ri* , the adversary learns something about the original SA value x in ri ; however , the independence assumption implies that all rj* and any knowledge about rj , j „ i , disclose nothing about x of ri and can be ignored in the privacy analysis of ri . Therefore , the corruption attacks discussed earlier are not effective . B . Privacy model
Let r be an original record with SA value xi and let r* be the perturbed version of r with SA value yj . Pr[X = xi ] denotes the prior probability of X = xi in the absence of any knowledge about r ’s SA , and Pr[X = xi | Y = yj ] denotes the posterior probability of X = xi given the perturbed value Y = yj . Since r* and r agree on QI and since QI is public , the change in belief comes from revealing the perturbed value Y = yj . We discuss the impact of QI on privacy in Section IV .
If the prior probability is too high , the adversary already knows X = xi before seeing the published data . For this purpose we borrow ( r1 , r2) privacy from [ 3 ] : the posterior probability is not more than r2 whenever the prior probability is not more than r1 , where r1 < r2 . For example , a ( 1/10 , 1/2) privacy breach occurs when an adversary ’s prior probability is less than 10 % and posterior probability is more than 50 % . If the prior probability is more than 10 % , there is no ( 1/10 , 1/2) privacy breach .
Our observation is that r1 and r2 depend on the sensitivity of a SA value . For example , suppose the less sensitive “ cancer ” requires ( 1/2 , 2/3) privacy and “ HIV ” requires ( 1/10 , 1/7) privacy . Now it is not even possible to have one ( r1 , r2 ) setting : setting r1 1/2 would not enforce ( 1/10 , 1/7) privacy for “ HIV ” ; on the other hand , setting r1 < 1/2 would not enforce ( 1/2 , 2/3) privacy for “ cancer ” if prior probability falls into ( r1 , 1/2 ] . To address this issue , the fine grain privacy below extends the ( r1 , r2) privacy [ 3 ] to allow a different ( r1 , r2 ) for each value in SA . Definition 1 ( Fine grain ( rrrr1i , rrrr2i) privacy ) : Let ( r1i , r2i ) be the privacy requirement for SA value xi . There is an upward ( r1i , r2i) privacy breach with respect to xi ˛ SA if for some yj ˛ SA , Pr[X = xi ] £ r1i and Pr[X = xi | Y = yj ] > r2i . There is a downward ( r2i , r1i) privacy breach with respect to xi ˛ SA if for some yj ˛ SA , Pr[X = xi ] r2i and Pr[X = xi | Y = yj ] < r1i . Here , 0 < r1i < r2i < 1 and Pr[Y = yj ] > 0 . ( cid:157 )
We say that ( r1i , r2i) privacy holds if the above upward and downward privacy breaches are eliminated . C . Data utility
We consider two types of utility in this paper . The first is the standard utility metric used in evaluating perturbation algorithms in data mining , which we call aggregate utility . In many aggregate data mining applications , the distribution of SA values , instead of the exact SA value in a record , is the research target .
Given a perturbed table T* , let O = ( o1,…,om ) be the vector of the observed frequencies of yi in T* , and let F = ( f1,…,fm ) be the vector of the frequencies of xi in T . E(O ) = P×F . We can estimate F by F’ = P 1·O , if P is invertible . F’ is an unbiased estimate of F in the sense that E(F’ ) = P 1 · E(O ) = F . To compute F’ , we can use inverse [ 4 ] or Iterative Bayesian [ 1 ] reconstruction .
Definition 2 ( Aggregate utility ) : Let F = ( f1,…,fm ) be the actual SA value frequencies from an original table T and let F’ = ( f1’,…,fm’ ) be the frequencies estimated from the perturbed table T* , discussed above . The aggregate utility of F’ with respect to F is defined as 1 1/m · i=1m |fi – fi’| . We propose a new utility metric , called record utility , which measures the expected percentage of records in T whose SA values are unchanged in T* . This utility is useful when the truthfulness of data at the record level is important . For example , records may be published for human reading , where a published value differing from the original value is considered an error . Definition 3 ( Record utility ) : Let F = ( f1,…,fm ) be the actual SA value frequencies from an original table T . The record utility is defined as i fi · pii , "i=1,…,m , where pii is the retention probability of an SA value , located along the main diagonal in P .
Record utility is a better utility metric for privacy preserving data publishing because ( a ) it is not instance specific , so it is useful for measuring the quality of M , not just a specific instance T* , ( b ) it measures utility for an ad hoc task , not just an aggregate data mining task and ( c ) we expect it to have a positive impact on aggregate utility , since retaining more SA values helps reconstruct the distribution on SA . We will evaluate this impact experimentally in Section VI . We can now formally define our problem .
Definition ( Optimal Fine Grain Perturbation Problem ) : Given the microdata T and privacy parameters ( r1i , r2i ) for all xi in SA , 0 < r1i < r2i < 1 , i=1,…,m , we want to find an optimal fine grain perturbation matrix P such that " i=1,…,m , ( i ) ( r1i , r2i) privacy holds on any T* generated by P and ( ii ) record utility is maximized under ( i ) . ( cid:134 )
4
IV . OPTIMAL FINE GRAIN PERTURBATION
Fig 3 shows the algorithm for finding the optimal finegrain perturbation operator , called Fine grain . Given the microdata T and privacy parameters ( r1i , r2i ) for each value xi in SA , i = 1,…,m , Fine grain computes the frequencies fi of all SA values xi in Step 1 . In Step 2 , the gi values are computed . These values represent how sensitive the SAvalues are ; a high gi means xi is not very sensitive . In Step 3 , a linear program is solved , which determines the optimal probabilities pi for xi . In Step 4 , a fine grain perturbation operator P is constructed . By rewriting the privacy constraint in Fig 3 as ( m 1 ) × pi + gi × pj £ gi 1 , we have a linear program with a global maximizer . There always exists one solution satisfying the privacy constraints ( ie , pi = 0 , for i = 1,…,m ) . Since 0 < r1i < r2i < 1 , 1 < gi < ¥ , so ( 1 pj)/m can never be zero , ie , P never has zero entries .
Fine grain Input : T , rrrr1i and rrrr2i for all SA values xi Output : optimal fine grain perturbation matrix 1 . Determine SA frequencies fi from T 2 . gi = ( r2i·(1 r1i))/(r1i·(1 r2i) ) , "i=1,…,m 3 . Solve the program for pi : Objective function : max i fi·(pi+(1–pi)/m ) Privacy constraints : =""£ g
,,1
, im
„ p j j i
,
, i i
+ 1( 1( £ p
/ ) mp ="£ i mp i / ) j ,1 i
0
,,1 m
4 . Construct P following ( 1 ) 5 . Return P
Figure 3 . Optimal fine grain perturbation algorithm
Uniform perturbation is a special case of a fine grain perturbation where all probabilities pi , i = 1,…,m , are equal , say to p . Therefore , the main diagonal entries are p + q and all other entries are q = ( 1 p)/m . Also , the privacy constraints in Fig 3 simplify to ( p + ( 1 p)/m ) / q . g , where g = min gi , which corresponds to the most restrictive constraint in Fig 3 . The objective function is maximized when p = ( g 1 ) / ( m 1 + g ) , so , P becomes
)2(
P
=
1 1
+ g m g
' 1 ' ' . ' ff
1
1 1 fi g 1 fi g .
1
( 2 )
Example 2 ( Fine Grain vs . Uniform ) : Continuing from Example 1 , in Step 2 of Fig 3 , gSARS = 1½ , gHIV = 3 , gH1N1 = 9½ , and gcancer = 18 . Let g = min(gSARS , gHIV , gH1N1 , gcancer ) = 1½ . Fig 2 ( a ) gives the uniform perturbation operator with record utility = 1/3 , and Fig 2 ( b ) gives the optimal finegrain perturbation operator returned by Fine grain with record utility = 04375 About a 24 % increase in record utility is due to the increased retention probability on the main diagonal . This is possible because Fine grain perturbs less of the less sensitive SA values . (
We now prove that Fine grain ’s privacy constraints in Fig 3 guarantee ( r1i , r2i) privacy as stated in Definition 1 . In [ 3 ] , a g amplification condition is proposed to guarantee ( r1 , r2) privacy . The idea is to bound the ratio Pr[xk fi y ] / Pr[xi fi y ] by some g value derived from the privacy parameters r1 and r2 . Intuitively , this says that if the probability of any SA value being perturbed to the same value y differs by a factor not more than g , ( r1 , r2) privacy will hold . We now extend this approach to ( r1i , r2i) privacy . The extension is not trivial ; however , the details are omitted due to space constraints .
Definition 5 ( ggggj amplification condition ) : For yj ˛ SA , let gj be a ( finite ) real greater than 1 . We say that a perturbation operator P is at most gj amplifying if
£ gj , "i = 1,…,m , i „ j
( 3 ) p p jj ji
Theorem 1 : Let P be a fine grain operator . Let 0 < r1i < r2i < 1 be the privacy parameters for xi , i = 1,…,m . Assume that P is at most gj amplifying for j = 1,…,m . Revealing “ Y = yj ” causes neither upward ( r1i , r2i) privacy breaches nor downward ( r2i , r1i) privacy breaches if the following condition is satisfied :
2 r r
1 i i
·
1 1
1 i r r
2 i
‡ g i
( cid:127 )
( 4 )
The proof is similar to Statement 1 in [ 3 ] ( details omitted due to space constraints ) . With Theorem 1 , the most relaxed gi amplification condition for ( r1i , r2i) privacy is derived by choosing the maximum gi satisfying ( 4 ) , ie , gi = i
2 r r
1 i
·
1 1 i
1 r r
2 i
( 5 )
So far , we have ignored the QI attributes in the definition of ( r1i , r2i) privacy . One question is whether background knowledge on QI will affect ( r1i , r2i) privacy . Consider ( only ) two SA values , “ lung cancer ” and “ breast cancer ” and suppose the adversary has background knowledge that a male is very unlikely to have “ breast cancer ” . Upon seeing a record in T* with Sex = “ M ” ( a QI attribute ) , the adversary can immediately tell the original SA value for this record is “ lung cancer ” . However , this inference is not due to data publication ; the adversary already knows that any record with Sex = “ M ” has SA = “ lung cancer ” before seeing the perturbed SA value . Thus the impact of QI is on prior probability , not on posterior probability .
In general , we can show that background knowledge on QI does not affect our approach as long as the perturbation operator P is independent of QI , which is true in our case . To see this , we can model any background knowledge on QI by Z = z for an instance z of some random variable Z that depends on the QI value in a record . In Definition 1 , the prior and posterior probabilities are now modified to Pr[X = xi | Z = z ] and Pr[X = xi | Y = yj , Z = z ] , respectively . Therefore , all perturbation probabilities Pr[xi fi yj ] remain the same , so the amplification condition remains unaffected and Theorem 1 still applies . Depending on the background knowledge Z = z ( such as Sex = “ M ” vs . Sex = “ F ” ) , each xi now may have several ( r1i , r2i ) parameters . In this case , we will use the minimum gi value for xi over the Z = z related to xi ( computed by Equation ( 5) ) .
V . EXPERIMENTAL EVALUATION for
In this section , we evaluate the effectiveness of our new strategy improving utility , namely , Fine grain Perturbation . To our knowledge , [ 12 ] is the only work on data publishing that addresses corruption attacks . Their least distorted case is when there is no sampling ( ie , k = 1 ) and no generalization on QI . In this case , their perturbation is exactly the same as Uniform Perturbation . We compare the following algorithms : Uniform – uses Equation ( 2 ) and Fine grain – uses Fig 3 . utility
( using aggregate
We set r1i to the frequency fi , i = 1,…,m , and we use a tolerance parameter q > 1 to set r2i for the posterior probability . If r1i ‡ 1/q , we set gi to a large value so that < 1/q , we set r2i = q there is no privacy concern on xi . If r1i · r1i and compute gi using ( 5 ) . We collect both record utility and iterative Bayesian reconstruction 0 over 10 instances T* ) . Our algorithms are written in C++ and we used Soplex ( http://ziboptzibde ) to solve our linear program . We ran all experiments on a Pentium IV 3.0 GHz PC with 2.0GB of RAM . A .
Dataset descriptions CADR Our first experiment uses the real life Canadian Adverse Drug Reaction ( CADR ) database ( http://wwwhcscgcca/dhp mps/medeff/databasdon/extract_extrait eng php ) . We join the Reactions and Reports tables from this database to obtain a table J with QI attributes report date , gender , age , weight , height , and SA drug reaction . Each record in J has a unique patient Report_id . SA consists of the m = 40 most frequent drug reactions . All records for other drug reactions are discarded . In the remaining records , if a patient has more than one record , we keep the record having the most frequent drug reaction . The dataset size is 95946 .
Zipf We also experimented with a synthetic dataset that has frequencies fi following the Zipfian distribution . For the ith most frequent SA value xi , i = 1,…,m , the frequency fi is given by f d , ,( mi
)
=
/1 =
1 j d i /1 m d j where d determines the decreasing rate of f(i , d , m ) . As in the classic version of Zipf law , we use d = 10 This distribution is more skewed than that of the CADR dataset . We set m = 40 and records to 95946 to match the CADR dataset settings . The frequency distributions for both datasets are given in Fig 4 .
( a ) CADR y c n e u q e r F
0.3 0.2 0.1 0
1 4 7 10 13 16 19 22 25 28 31 34 37 40
SA values in decreasing frequency order
( b ) Zipf y c n e u q e r F
0.3 0.2 0.1 0
1
7 10 13 16 19 22 25 28 31 34 37 40
4 SA values in decreasing frequency order
Figure 4 . Dataset frequency distributions .
B . Uniform vs . fine grain perturbation on CADR
Our goal is to evaluate whether fine grain privacy leads to better retention of data . In this section we consider the real life CADR dataset . The results comparing uniform and fine grain perturbation are shown in Fig 5 ( a ) , with record utility on the left and aggregate utility on the right .
Fine grain perturbation has better record utility than uniform perturbation , with an improvement of about 10 % to 20 % . However , fine grain perturbation has almost the same aggregate utility as uniform perturbation . This is because aggregate utility is more robust to data distortion at the record level . As expected , both utilities improve as the privacy tolerance q increases .
The lower record utility indicates that record utility is more difficult to retain than aggregate utility . This is expected because only unchanged SA values contribute to record utility . Another interesting point is that , although our algorithms optimize record utility , a better record utility translates into a better aggregate utility . C . Uinform vs . fine grain perturbation on Zipf
Again , our goal is to evaluate whether fine grain privacy leads to better retention of data , but in this section we consider the highly skewed Zipf dataset . The results are shown in Fig 5 ( b ) . Note that we use larger q values than in the previous experiment . This is to accommodate privacy settings for extremely small frequencies ( ie , r1 values ) in this skewed dataset . Fine grain perturbation again has better record utility than uniform perturbation and both utilities improve as the privacy tolerance q increases . However , we find an unexpected trend : although fine grain perturbation has a lower aggregate utility than uniform perturbation . y t i l i t u d r o c e r
1 0.8 0.6 0.4 0.2 0 y t i l i t u d r o c e r
1 0.8 0.6 0.4 0.2 0
( a ) CADR y t i l i t u e t a g e r g g a
1.000 0.998 0.996 0.994 0.992 0.990
5
10
15
20 q q q q
( b ) Zipf
5
10
15
20 q q q q y t i l i t u e t a g e r g g a
1.000 0.998 0.996 0.994 0.992 0.990
20
30
40
50
20
30
40
50 q q q q q q q q
Figure 5 . Utility vs . q for fine grain and uniform privacy .
On one hand , we are not surprised , since we optimized our algorithm for record utility , not aggregate utility . A high record utility is a sufficient condition for a high aggregate utility , but it is not a necessary condition . For instance , consider an operator which transforms the original dataset T by randomly swapping the SA values of pairs of records so that every record in T* ends up with another record ’s SAvalue . Since this operator did not alter the distribution of SA values , aggregate utility is maximized , while record utility is very low ( imagine a researcher trying to use such a scrambled publication for ad hoc analysis ) .
On the other hand , it is interesting that a correlation between record and aggregate utility appears to hold for more balanced datasets like CADR , but not for more skewed datasets like Zipf . A key difference in skewed datasets is that low frequency SA values only occur in a small number of records , which makes reconstruction unstable . Moreover , our fine grain algorithm is optimized for record utility , so it prefers the distortion of these lowfrequency SA values ( small number of records overall ) when it is globally optimal to retain more of the highfrequency SA values ( large number of records overall ) . Since data mining tasks generally use aggregate information shared by a large number of records , we do not see this as a negative result ; rather , it highlights an interesting research problem outside the scope of this privacy preserving data publishing paper : how can we optimize utility for aggregate data mining tasks ?
VI . CONCLUSIONS the from
The conventional anonymity group approach for privacy preserving data publishing is susceptible to corruption attacks . We presented a novel anti corruption solution . Fundamentally differing anonymity group approach , we adapted perturbation , previously used for privacy preserving data mining , as the main technique for data publishing . To minimize information loss , we proposed a new perturbation operator called fine grain perturbation . Our studies showed that while our new approach retains better utility for ad hoc tasks , more work is needed in optimizing utility for aggregate data mining tasks , especially for highly skewed datasets . In the future , we plan to extend our approach to preserve privacy of individuals in other types of data , such as graphs , spatial data , and query logs .
ACKNOWLEDGMENT
We would like to thank Dr . Tamon Stephen from the Math Department at Simon Fraser University for his advice on mathematical programming software and our reviewers for their feedback . This research was supported by a Natural Sciences and Engineering Research Council of Canada ( NSERC ) Post Graduate Scholarship and Discovery Grant .
REFERENCES
[ 1 ] R . Agrawal , R . Srikant , and D . Thomas , “ Privacy Preserving OLAP , ”
Proc . SIGMOD 2005 .
[ 2 ] W . Du and Z . Zhan , “ Using Randomized Response Techniques for
Privacy Preserving Data Mining , ” Proc . KDD 2003 .
[ 3 ] A . Evfimievski , J . Gehrke , and R . Srikant , “ Limiting Privacy
Breaches in Privacy Preserving Data Mining , ” Proc . PODS 2003 .
[ 4 ] Z . Huang and W . Du . “ OptRR : Optimizing Randomized Response
Schemes for Privacy Preserving Data Mining , ” Proc . ICDE 2008 .
[ 5 ] N . Li , T . Li , and S . Venkatasubramanian , “ t closeness : Privacy
Beyond k Anonymity and l diversity , ” Proc . ICDE 2007 .
[ 6 ] T . Li and N . Li , “ Modeling and Integrating Background Knowledge , ”
Proc . ICDE 2009 .
[ 7 ] A . Machanavajjhala , and M . Venkitasubramaniam , “ l diversity : Privacy Beyond k anonymity , ” Proc . ICDE 2006 .
J . Gehrke , D . Kifer ,
[ 8 ] V . Rastogi , S . Hong , and D . Suciu , “ The Boundary Between Privacy and Utility in Data Publishing , ” Proc . VLDB 2007 .
[ 9 ] S . Rizvi and J . R . Haritsa , “ Maintaining Data Privacy in Association
Rule Mining , ” Proc . VLDB 2002 .
[ 10 ] P . Samarati , “ Protecting Respondents’ Identities in Microdata
Release , ” TKDE , vol . 13 , no . 6 , pp . 1010–1027 , 2001 .
[ 11 ] L . Sweeney , “ Achieving k anonymity Privacy Protection Using Generalization and Suppression , ” Int’l J . on Uncertainty , Fuzziness and Knowledge based Systems , vol . 10 , no . 5 , pp . 571–588 , 2002 .
[ 12 ] Y . Tao , X . Xiao , J . Li , and D . Zhang , “ On Anti Corruption Privacy
Preserving Publication , ” Proc . ICDE 2008 .
[ 13 ] S . L . Warner , “ Randomized Response : A Survey Technique for Eliminating Evasive Answer Bias . The A . Stat . Assoc . , vol . 60 , no . 309 , pp . 63–69 , 1965 .
[ 14 ] X . Xiao and Y . Tao , “ Anatomy : Simple and effective privacy 2006 . preservation , ”
VLDB
Proc .
