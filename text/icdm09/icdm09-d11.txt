P packSVM : Parallel Primal grAdient desCent Kernel SVM
Zeyuan Allen Zhu12* , Weizhu Chen2 , Gang Wang2 , Chenguang Zhu23 , Zheng Chen2
1 Fundamental Science Class ,
Department of Physics , Tsinghua University zhuzeyuan@hotmail.com
2 Microsoft Research Asia
{v zezhu , wzchen , gawa , v chezhu , zhengc}@microsoft.com
Abstract—It is an extreme challenge to produce a nonlinear SVM classifier on very large scale data . In this paper we describe a novel P packSVM algorithm that can solve the Support Vector Machine ( SVM ) optimization problem with an arbitrary kernel . This algorithm embraces the best known stochastic gradient descent method to optimize the primal objective , and has 𝟏/𝝐 dependency in complexity to obtain a solution of optimization error 𝝐 . The algorithm can be highly parallelized with a special packing strategy , and experiences sub linear speed up with hundreds of processors . We demonstrate that P packSVM achieves accuracy sufficiently close to that of SVM light , and overwhelms the state of the art parallel SVM trainer PSVM in both accuracy and efficiency . As an illustration , our algorithm trains CCAT dataset with 800k samples in 13 minutes and 95 % accuracy , while PSVM needs 5 hours but only has 92 % accuracy . We at last demonstrate the capability of P packSVM on 8 million training samples .
Keywords–parallel ; kernel ; support stochastic gradient descent ; packing strategy vector machine ;
I .
INTRODUCTION
( 1 )
Ψ=(cid:3419)(𝒙,𝑦)(cid:3627)𝒙∈ℝ,𝑦∈{−1,1}(cid:3423)(cid:2880)(cid:3040 )
Since its first introduction by V . Vapnik in 1963 [ 26 ] , support vector machine ( SVM ) has been a widely used supervised learning method for classification , regression [ 6 ] and ranking [ 13 ] problem . Strictly speaking , a set of training data , consisting of 𝑚 samples is given : where 𝒙𝒊 is the feature vector for the ith sample and 𝑦 is either +1 or −1 , indicating the binary class this sample classes , indicating by its normal vector , or predictor 𝒘 , by 𝑓(𝒘)=𝜎2‖𝒘‖(cid:2870)(cid:2870)+1𝑚(cid:3533)max{0,1−𝑦〈𝒘,𝜙(𝒙)〉} where the first term is a 2 norm regularizer 12⁄ ‖𝒘‖(cid:2870)(cid:2870 ) with the regularizer weight 𝜎 , while the second term is the minimizing the following quadratic convex objective , which is also known as the primal SVM objective : belongs to . The soft margin SVM problem [ 26 ] is aiming to find a maximum margin separating hyper plane for the two
( cid:3040 ) ( cid:2880 )
( 2 ) empirical loss function :
3 Department of Computer Science and Technology ,
Tsinghua University zcgcs60@gmailcom
( 3 )
( cid:3040 ) ( cid:2880 )
ℓ(𝒘)=1𝑚(cid:3533)max{0,1−𝑦〈𝒘,𝜙(𝒙)〉} Originally proposed by Aizerman et al [ 1 ] , 𝜙( . ) is the 〈𝜙(𝒙𝒊),𝜙(cid:3435)𝒙𝒋(cid:3439)〉=𝒦(cid:3435)𝒙𝒊,𝒙𝒋(cid:3439 ) for some Mercer kernel 𝒦( , ) . The integration of the kernel enables SVM to produce a non linear predictor , 𝑤 . This is often called non mapping that projects the point from feature space to the Reproducing Kernel Hilbert Space ( RKHS ) , satisfying linear SVM or kernel SVM . For example , a polynomial kernel allows one to model feature conjunctions , while a Gaussian kernel enables us to pick out hyper spheres in features [ 20 ] . Recent works , such as PEGASOS , effectively solved the linear SVM problems [ 24 ] [ 14 ] [ 30 ] ; however , to accelerate the kernel SVM is a very desirable and difficult research problem .
We analyze in this paper a simple stochastic gradient descent ( SGD ) based algorithm that directly optimizes the primal objective ( 2 ) , called packSVM , to solve SVM for an arbitrary kernel . The algorithm embraces a bunch of iterations . At each iteration , it first randomly picks up a single sample from the training sample pool to approximate
ℓ(𝒘 ) , and then calculates the gradient and updates the predictor 𝒘 accordingly . It is worth noting that our proposed [ 24 ] and requires 𝑂(𝑚/𝜎𝜖𝛿 ) in time , where 𝛿 is the confidence parameter and 𝜖 is the optimization error . This means , with probability at least 1−𝛿 we can obtain a predictor 𝒘 that is guaranteed to satisfy 𝑓(𝒘)≤𝑓(𝒘∗)+𝜖 , if 𝑤∗ is the optimal solution . packSVM algorithm embraces the best known learning rate
An important contribution of this paper is that we parallelize the above algorithm with the help of a distributed hash table and our innovative packing strategy . We call our proposed parallel algorithm P packSVM . Notice that it is naturally difficult to parallelize SGD algorithms in hundreds of processors due to their huge communication cost . The packing strategy non trivially reduces the communication cost and allows a sub linear speed up with 512 processors . The time complexity of P packSVM is thus reduced to
𝑂(𝑚/𝜎𝑡𝛿𝑝 ) if using 𝑝 processors . At the same time , PpackSVM uses only 𝑂(𝑚/𝑝 ) space for each processor .
We conduct extensive experiments and show that PpackSVM overwhelms the state of the art PSVM [ 5 ] in both
* This work was done when the first author was visiting Microsoft
Research Asia . accuracy and efficiency , and runs hundreds of times faster than SVM light . Meanwhile , its accuracy is comparable to SVM light . For example , P packSVM trains a CCAT dataset of 800k samples in 761 seconds with a speed up of 295 times on 512 processors ; it trains a CovType dataset of 500k samples in 236 seconds with a speed up of 416 times on 512 processors . The 8 million MNIST8m test set has also been employed and we state that our proposed algorithm is capable of performing well in million scale data set .
This reminder of this paper is organized as follows . We first state the related works of SVM in Section II . Next in Section III , we propose our P packSVM algorithm by introducing its sequential implementation and then move onto the parallel one with introducing the distributed hash and the innovative packing strategy . We also emphasize the differences of our P packSVM with other contemporary works in this section . Experimental results are then provided in Section IV . Finally we leave several enhancements to our algorithm in Section V and conclude our work in Section VI .
II . RELATED WORKS
( 4 )
. We divide the state of the art the Lagrangian dual variable , also known as the support
Historically , the SVM problem has been well studied with the help of the dual objective . The method of Lagrangian multipliers introduces a transformation from the primal objective ( 2 ) into its dual form :
SVM trainers mainly into the following three categories . Interior Point Method ( IPM ) : minimizing the dual objective is a convex Quadratic Programming ( QP ) problem and can be solved via the primal dual Interior Point Method [ 21 ] . The idea of IPM is to incorporate Newton or QuasiNewton methods with the number of iterations proportional min12𝜶𝑸𝜶−𝜶𝟏 , st𝟎≤𝜶≤𝑪,𝒚𝜶=0 where [ 𝑄]=𝑦𝑦𝒦(cid:3435)𝑥,𝑥(cid:3439 ) , and 𝛼∈ℝ(cid:3040 ) is the vector of vector in SVM . The predictor 𝒘 is a superposition of 𝜙(𝑥 ) , namely 𝒘=∑ 𝛼𝑦𝜙(𝒙 ) ( cid:2880 ) to log(1/ϵ ) [ 27 ] , where ϵ is the desired accuracy . However , the memory requirements of IPM are as high as 𝑂(𝑚(cid:2870 ) ) and the computational cost is 𝑂(𝑚(cid:2871 ) ) for each iteration . and Incomplete Cholesky Factorization [ 9 ] ( ICF ) 𝑄≈𝐻𝐻 , where 𝐻 has the dimension 𝑚×𝑚(cid:4593 ) . [ 5 ] empirically showed that 𝑚(cid:4593)=𝑚.(cid:2873 ) gives a good approximation , and thus induced an algorithm with the time complexity of 𝑂(𝑚(cid:2870 ) 𝑝⁄ ) for each iteration and the space requirement of 𝑂(𝑚.(cid:2873)/𝑝 ) , where 𝑝 is the number of processors . To the best of our
Recently , E . Y . Chang et al [ 5 ] proposed an algorithm called PSVM . It enables a parallel implementation of IPM knowledge , E . Y . Chang et al firstly studied the parallel kernel SVM on 500 processors in experiments , and they reported a parallel speed up of up to 169 times with 800k training samples . Sequential Minimal Optimization ( SMO ) : to make SVM more practical , SMO algorithms are developed by decomposing the large QP problem into an inactive part and an active part – a so called “ working set ” . Many open source tools , like Osuna ’s decomposition [ 7 ] , libSVM [ 4 ] and SVM light [ 12 ] , are capable of training as large as several hundred thousand training samples on a single machine .
Attempts to parallel SMO algorithms have also been made . For example , Zanghirati and Zanni [ 28 ] proposed a parallel implementation of SVM light , especially effective for Gaussian kernels ; Cao et al [ 3 ] also parallelized a slightly modified SMO algorithm . For these two papers , the authors conducted experiments on up to 32 processors with 60k training samples , claiming a speed up of approximately 20 times . Stochastic Gradient Descent ( SGD ) : until recently , a growing amount of attention had been paid towards stochastic gradient descent algorithms , in which the gradient is approximated by evaluating on a single training sample . This algorithm has been applied to the primal objective of linear SVM algorithms . T . Zhang [ 29 ] proved that a constant learning rate ( no parameter sweep required ) in SGD will numerically achieve good accuracy , enabling a running time of O(1/ϵ(cid:2870 ) ) for a linear kernel . The algorithm Norma [ 16 ] suggests a learning rate proportional to 1/√𝑡 , where 𝑡 is the aggressively adopted a learning rate of 1/𝜎𝑡 . It turns out this time complexity of 𝑂(𝑚 ) . learning rate is up to now the most efficient [ 24 ] for linear SVM , and even endowed with an inverse time dependency for fixed accuracy [ 25 ] . Notice that these works focus on linear predictors only . Some of them addressed their potential to be extended to kernel SVM , but with an extra iteration . Shai Shalev Shwartz et al [ 24 ] number of
Hush et al [ 10 ] proved that the convergence rate in the primal objective is slow when an algorithm tries to optimize the dual one instead . This applies to the algorithms in the first two categories . Our proposed method falls into the third category , and thus is born with advantages . We incorporate a distributed hash table to enable the parallelism and the packing strategy to facilitate the parallelism . We will show that though in general only the algorithm in the first category can be effectively parallelized , our proposed packing strategy reverses the adversity .
III . THE ALGORITHM
In this section we first adopt the stochastic gradient descent method to the kernel SVM problem , and provide the result of its convergence analysis . Next , we propose its parallel implementation and a special packing strategy . We finally compare our proposed method with other contemporary works . A . Sequential packSVM
In this sub section we describe a sequential stochastic gradient descent ( SGD ) algorithm on the primal SVM objective . With the incorporation of kernels , we call it SpackSVM . We adopt the framework discussed in [ 24 ] , which has the best known learning rate and an additional projection phrase .
Considering the empirical loss ( 3 ) , it averages the hinge loss among all training examples . In the spirit of the SGD algorithm , this empirical loss can be approximated by the
( 5 )
1 . 2 .
4 . 5 . 6 . 7 . 8 .
Figure 1 . S packSVM algorithm
( 2 ) as the following :
5 . 6 . 7 . 8 . 9 .
10 .
Directly adopting the learning rate suggested in [ 24 ] , we hinge loss on a single training sample . Based on this idea , we
INPUT : 𝜎,𝑇 , training sample space 𝛹 INITIALIZE : 𝒘=0 3 . FOR 𝑡 = 1,2,…,𝑇 Randomly pick up ( 𝒙,𝑦)∈𝛹 Predict 𝑦(cid:4593)←〈𝒘,𝜙(𝒙)〉 𝒘←(1−1/𝑡)𝒘 IF 𝑦𝑦(cid:4593)<1 THEN 𝒘←𝒘+(cid:3052)(cid:3097)(cid:3047)𝜙(𝒙 ) 𝒘←min(cid:4676)1,/√(cid:3097)‖𝒘‖(cid:3118 ) ( cid:4677)𝒘 9 . RETURN 𝒘 propose our S packSVM with 𝑇 iterations . At iteration 𝑡∈{1,…𝑇} , it picks up a random example ( cid:3435)𝑥(𝑡),𝑦(𝑡)(cid:3439)∈ Ψ , and approximates the empirical loss ( 3 ) and the objective ℓ(𝑤)≈ℓ(cid:3047)(𝑤)≔max(cid:3419)0,1−𝑦((cid:3047))∙〈𝒘,𝜙(cid:3435)𝒙((cid:3047))(cid:3439)〉(cid:3423 ) 𝑓(𝑤)≈𝑓(cid:3047)(𝑤)≔𝜎2‖𝑤‖(cid:2870)(cid:2870)+ℓ(cid:3047)(𝑤 ) modify the predictor as below in iteration 𝑡 : 𝒘←𝒘−1𝜎𝑡∇𝑓(cid:3047)(𝒘 ) We notice that the operator ∇ does not require the differentiability of function 𝑓(cid:3047 ) , but the existence of its sub∇𝑓(cid:3047)(𝒘)=𝜎𝒘− 𝑦((cid:3047))∙〈𝒘,𝜙(cid:3435)𝒙((cid:3047))(cid:3439)〉≥1 0 , ( cid:4682 ) 𝑦((cid:3047))∙〈𝒘,𝜙(cid:3435)𝒙((cid:3047))(cid:3439)〉<1 𝑦((cid:3047))𝜙(cid:3435)𝒙((cid:3047))(cid:3439 ) , When kernels are introduced , we usually write 𝒘 as a superposition of samples 𝒘=∑ 𝛼𝑦𝜙(𝒙 ) 𝒘←(cid:4672)1−(cid:3047)(cid:4673)𝒘+ 0 , 𝑦((cid:3047))∙〈𝒘,𝜙(cid:3435)𝒙((cid:3047))(cid:3439)〉≥1 ( cid:3421 ) ( cid:3052)(cid:3284)((cid:3295))(cid:3097)(cid:3047 ) ∙𝜙(cid:3435)𝒙((cid:3047))(cid:3439 ) , 𝑦((cid:3047))∙〈𝒘,𝜙(cid:3435)𝒙((cid:3047))(cid:3439)〉<1 After each update to 𝒘 , a projection is applied to help 𝒘 𝒘←min(cid:4682)1,1/√𝜎‖𝒘‖(cid:2870 ) ( cid:4683)𝒘 𝒘=𝑠𝒗 where 𝑠∈ℝ is a scalar that allows Line 6 and 8 of performing scaling we can simply change the value of 𝑠 instead of modifying the coefficients of all the terms in 𝒘 ,
, and the subtraction in ( 6 ) simply consists of an overall shrinking and the addition of at most one term .
Figure 1 to run in a constant time . This is because when
In the implementation of S packSVM , we express to get closer to the optimum [ 24 ] : gradient [ 24 ] [ 23 ] . We write down the sub gradient explicitly :
( cid:3040)(cid:2880 )
( 8 )
( 9 )
( 6 ) and presented in Figure 1 .
Figure 2 . S packSVM pseudo code
. Figure 2 gives the pseudo code of the algorithm
Considering that our objective ( 2 ) is a strongly convex
1 . INPUT : 𝜎,𝑇 , training sample space 𝛹 2 . INITIALIZE : ℋ=∅,𝑠=1,𝑛𝑜𝑟𝑚=0 3 . FOR 𝑡 = 1,2,…,𝑇 4 . Randomly pick up ( 𝒙,𝑦)∈𝛹 𝑦(cid:4593)←𝑠〈𝒗,𝜙(𝒙)〉 by iterating all entries in ℋ 𝑠←(1−1/𝑡)𝑠 IF 𝑦𝑦(cid:4593)<1 THEN 𝑛𝑜𝑟𝑚←𝑛𝑜𝑟𝑚+(cid:2870)(cid:3052)(cid:3097)(cid:3047)∙𝑦(cid:4593)+(cid:4672)(cid:3052)(cid:3097)(cid:3047)(cid:4673)(cid:2870)𝒦(𝒙,𝒙 ) IF key 𝒙 is found in ℋ , THEN add its value by ( cid:3052)(cid:3097)(cid:3047)(cid:3046 ) in ℋ ; ELSE add ℋ a new entry ( cid:4672)𝒙 , ( cid:3052)(cid:3097)(cid:3047)(cid:3046)(cid:4673 ) √(cid:3097)∙(cid:3042)(cid:3045)(cid:3040 ) ; 𝑛𝑜𝑟𝑚←1𝜎⁄ IF 𝑛𝑜𝑟𝑚>1/𝜎 THEN 𝑠←𝑠∙ 11 . RETURN 𝑠𝒗 by iterating all entries in ℋ the adding 𝒘←𝒘+(cid:3052)(cid:3097)(cid:3047)𝜙(𝒙𝒊 ) implies 𝒗←𝒗+ ( cid:3052)(cid:3097)(cid:3047)(cid:3046)𝜙(𝒙𝒊 ) . Besides , a variable 𝑛𝑜𝑟𝑚 is employed to store the up to date value of ‖𝑤‖(cid:2870)(cid:2870 ) , and a hash table ℋis used to store the key value pairs ( 𝑥,𝛽 ) in the representation of 𝒗= ∑𝛽𝜙(𝑥 ) function [ 23 ] with respect to 𝒘 , we follow the convergence following inequality for some constant 𝐶 : −1𝑇min𝒘∈S(cid:3533)𝑓(cid:3047)(𝒘 )
( cid:3047)(cid:2880 ) objective 𝑓(𝒘∗ ) using Markov inequality , while the first that S packSVM requires 𝑇=𝑂(cid:3560)(1/𝜎𝛿𝜖 ) iterations to obtain a predictor 𝒘 , satisfying 𝑓(𝒘)≤𝑓(𝒘∗)+𝜖 with probability at least 1−𝛿 , assuming 𝒘∗ to be the optimal predictor . This suggests a total running time of 𝑂(cid:3560)(𝑚/𝜎𝛿𝜖 ) for S packSVM , entries of ℋ in at most 𝑂(𝑚 ) time . on 1/𝜖 , which is much better than the general SGD many experimental observations , we find the optimal 𝜎 on the same order of 1/𝑚 ( see Appendix ) , and thus the overall samples 𝑚 . In this sub section , we provide the parallel analysis in [ 24 ] , which is a special case of S packSVM when linear kernel is adopted . Due to limited space , we only provide the sketch of the proof , while the details simply follow the idea of [ 24 ] . algorithm [ 29 ] , its sequential behavior does not show a significant superiority in efficiency . This is because from as all commands except Line 5 take a constant running time , while Line 5 needs a complete enumeration through all the
The first observation is that with the strong convexity , we can substitute the main result of [ 15 ] , and arrive at the
1𝑇(cid:3533)𝑓(cid:3047)(𝒘𝒕 )
( cid:3047)(cid:2880 ) term is related to the empirical objective . The final result is time complexity is in square dependence on the number of
The second term above can be related to the optimal
≤𝐶∙lnT𝑇𝜎
Although the complexity of S packSVM depends linearly
B . Parallel packSVM
( 10 )
( 7 ) packSVM , called P packSVM , and show that it has some unique advantages in kernel SVM training . Before going into
5 . 6 . 7 . 8 . 9 . 10 . 11 .
12 .
PROCESSOR 𝑖 1 . INPUT : 𝜎,𝑇 , training sample space 𝛹 2 . INITIALIZE : ℋ=∅ ,𝑠=1,𝑛𝑜𝑟𝑚=0 3 . FOR 𝑡 = 1,2,…,𝑇 4 . All processors pick up the same random ( 𝒙,𝑦)∈𝛹 𝑦(cid:4593)←𝑠〈𝒗𝒊,𝜙(𝒙)〉 by iterating all entries in ℋ Sum up 𝑦(cid:4593)←𝑦(cid:4593 ) via inter processor communication 𝑠←(1−1/𝑡)𝑠 IF 𝑦𝑦(cid:4593)<1 THEN 𝑛𝑜𝑟𝑚←𝑛𝑜𝑟𝑚+(cid:2870)(cid:3052)(cid:3097)(cid:3047)∙𝑦(cid:4593)+(cid:4672)(cid:3052)(cid:3097)(cid:3047)(cid:4673)(cid:2870)𝒦(𝒙,𝒙 ) IF key 𝒙 is found in ℋ THEN add its value by ( cid:3052)(cid:3097)(cid:3047)/𝑠 in ℋ ; THEN Find a least occupied processor 𝑗 and add ℋ a new entry ( cid:4672)𝒙,(cid:3052)(cid:3097)(cid:3047)/𝑠(cid:4673 ) √(cid:3097)∙(cid:3042)(cid:3045)(cid:3040 ) ; 𝑛𝑜𝑟𝑚←1𝜎⁄ IF 𝑛𝑜𝑟𝑚>1/𝜎 THEN 𝑠←𝑠∙ 13 . RETURN 𝑠𝒗 by iterating all entries in ℋ,…𝐻(cid:3043 ) of 〈𝒗,𝜙(𝒙)〉 can be highly parallelized via a distributed storage of the entries ( 𝑥,𝛽 ) in ℋ . detail , we consider the two characteristics related to the parallelism that packSVM embodies : • Merit : A single iteration can be highly parallelized . The sole time consuming process – the calculation
IF no processor reports the existence
Figure 3 . P packSVM pseudo code , without packing
• Defect : Too many iterations exist .
It initiates at least one communication request among all processors in each iteration . The mass communication will slow down the parallel program when the number of processors increases ( This is due to the synchronization overhead ) .
Considering the above two characteristics , we propose a distributed hash table to develop the merit , and a packing strategy to overcome the defect . Distributed Hash Table . We enable a distributed hash table to speed up the bottleneck process in Line 5 of Figure 2 .
Entries in ℋ are averagely divided to all the processors . Suppose the 𝑖th processor saves a subset ℋ=(cid:3419)(cid:3435)𝒙,,𝛽,(cid:3439)(cid:3423)(cid:2880)|ℋ(cid:3284)|⊂ℋ to represent 𝒗=∑𝛽,𝜙(𝒙 , ) inner product 〈𝒗,𝜙(𝑥)〉 can be distributed to all the processors , by each calculating 〈𝒗,𝜙(𝑥)〉= ∑𝛽,𝒦(𝒙,,𝒙 ) the processors check whether the given key 𝒙 exists in the local hash table ℋ . If any of the processors
• Look up & Modification ( Line 9 of Figure 2 ) : all
• Enumeration ( Line 5 of Figure 2 ) : the calculation of communications , like AllReduce in MPI [ 22 ] . and a sum up via inter processor
. Specifically , we explain two important operations : finds the key , it simply updates the value and informs other processors of the existence of the key ; otherwise the new entry is inserted to the leastoccupied processor .
The above parallelization of packSVM , shown in Figure 3 can be experimentally shown to overwhelm many
…
𝒕+𝟏
𝒕 … 𝒕+𝒓−𝟏 Pack the consecutive 𝑟 iterations 𝑦(cid:3047)(cid:4593)=〈𝒘(cid:3047),𝜙(𝒙(cid:3047))〉 𝑦(cid:3047)(cid:4593 ) =∎〈𝒘(cid:3047),𝜙(𝒙(cid:3047))〉+∎𝒦 ( 𝒙(cid:3047),𝒙(cid:3047 ) ) 𝑦(cid:3047)(cid:2870)(cid:4593 ) =∎〈𝒘(cid:3047),𝜙(𝒙(cid:3047)(cid:2870))〉+∎𝒦 ( 𝒙(cid:3047),𝒙(cid:3047)(cid:2870))+∎𝒦 ( 𝒙(cid:3047),𝒙(cid:3047)(cid:2870 ) ) =∎〈𝒘(cid:3047),𝜙(𝒙(cid:3047)(cid:3045))〉+∎𝒦 ( 𝒙(cid:3047),𝒙(cid:3047)(cid:3045))+⋯+∎𝒦(𝒙(cid:3047)(cid:3045)(cid:2870),𝒙(cid:3047)(cid:3045 ) ) 𝑦(cid:3047)(cid:3045 ) ( cid:4593 ) These 𝑟 inner products can be calculated We use ∎ to hide the complex coefficients . via a single communication request
Figure 4 . Packing strategy . iterations into a single one , and thus reduce the number of actually needs no more than two scaling processes and one additional term . For the sake of simplicity , we combine them
𝒘(cid:3047)=𝑎(cid:3047)𝒘(cid:3047)+𝑏(cid:3047)𝜙(𝒙(cid:3047 ) ) contemporary kernel SVM tools and run well on up to hundreds of thousands of training samples . We go one step further by introducing the following packing strategy . bits in communication will not be reduced in our proposed strategy . Nevertheless , the reduction of communication frequency speeds up the algorithm significantly , as to be shown in Section IVC
Packing Strategy . Given an integer 𝑟 ,we aim to pack 𝑟 communications by a factor of 𝑂(𝑟 ) . Notice that the total We use notations 𝒘(cid:3047),𝒙(cid:3047),𝑦(cid:3047 ) to denote the predictor 𝒘 and the random sample ( 𝒙(cid:3047),𝑦(cid:3047 ) ) in the 𝑡(cid:2930)(cid:2918 ) iteration . Considering equation ( 8 ) and ( 9 ) , the calculation from 𝒘(cid:3047 ) to 𝒘(cid:3047 ) and write the recursive formula implicitly , where 𝑎(cid:3047),𝑏(cid:3047 ) are calculated from 𝒘(cid:3047),𝒙(cid:3047),𝑦(cid:3047 ) : In the iteration 𝑡 , we need to calculate 𝑦(cid:3047)(cid:4593)=〈𝒘(cid:3047),𝜙(𝒙(cid:3047))〉 , but 𝒘(cid:3047 ) is dependent on the previous iteration , since 𝑎(cid:3047 ) and 𝑏(cid:3047 ) can only be calculated in iteration 𝑡−1 . At first glance , this ends . Next , we will show how to calculate 𝑦(cid:3047)(cid:4593),…𝑦(cid:3047)(cid:3045 ) ( cid:4593 ) terms of 𝑤(cid:3047),𝜙(𝑥(cid:3047)),…𝜙(𝑥 ) , for 𝑦(cid:4593)=〈𝒘,𝜙(𝒙)〉 to 𝑖=𝑡…𝑡+𝑟−1 , and hide those complex coefficients . One can see that although coefficients ∎ are unknown at the iteration 𝑡 , we can pre calculate the time consuming part 〈𝒘𝒕,𝜙(𝒙𝒊)〉 for 𝑖=𝑡,…,𝑡+𝑟−1 all together at iteration 𝑡 . Besides , the pair wise values 𝒦(𝒙𝒊,𝒙𝒋 ) for 𝑡≤𝑖<𝑗≤𝑡+ 𝑟−1 can also be pre processed in a distributed manner . This MPI . We summarize our packing algorithm for 𝑟 consecutive iterations 𝑡,…,𝑡+𝑟−1 as follows : suggests it is unrealistic to calculate an iteration before the previous one all needs two communication requests like AllReduce in
As illustrated in Figure 4 , we expand the formula of simultaneously .
( 11 )
• updated offline ( without communication ) :
• Pre calculate 𝑦(cid:4593)=〈𝒘𝒕,𝜙(𝒙𝒊)〉 for 𝑖=𝑡…𝑡+𝑟−1 • Pre calculate 𝒦(𝒙𝒊,𝒙𝒋 ) for 𝑡≤𝑖<𝑗≤𝑡+𝑟−1 Iterate 𝑖 through 𝑡 to 𝑡+𝑟−1 and process the 𝑖 th iteration as before . Whenever iteration 𝑖 is finished , ,…𝑦(cid:3047)(cid:3045 ) 𝑎,𝑏 can be calculated and 𝑦(cid:4593 ) ( cid:4593 ) 𝑦(cid:4593 ) ←𝑎𝑦(cid:4593 ) +𝑏𝒦(cid:3435)𝒙𝒊𝒋,𝒙𝒊(cid:3439 ) • Update the distributed hash table ℋ after all 𝑟 𝑟 that it is not the larger the better . As one may see from the pre calculation of 𝒦(𝒙𝒊,𝒙𝒋 ) , it needs 𝑂(𝑟(cid:2870)𝑑/𝑝 ) in time , assuming 𝑑 to be the feature dimension . If this time exceeds that 𝑟=100 is a good parameter in Section IVC iterations finish , by communicating to confirm the existing entries , and then add new entries to the least occupied processors .
We provide the pseudo code of P packSVM with the packing strategy in Appendix . We remark on the coefficient the communication cost saved by the packing strategy , the acceleration will be undermined . We will practically show are
C . Comparisons
After introducing our algorithm , P packSVM ( based on SGD method ) , we are ready to compare it with other sequential or parallel trainers mentioned in Section II , for the large scale kernel SVM training . Accuracy . The prediction accuracy is associated with two factors : how well we optimize the objective and how well the objective is related to the accuracy . Since we are only considering the SVM trainers , we ignore the latter and only pay attention here the former – the optimization error . iterations increases . However ,
First of all , IPM and SMO algorithms both focus on the dual objective , but Hush et al [ 10 ] proved that this dual approach converges slowly in the desired primal objective . On the contrary , our P packSVM directly optimizes on the primal . If the algorithm terminates early , the optimization on the primal produce better solution than on the dual . Secondly , we consider the optimization effectiveness – how fast each algorithm converges to its own objective . IPM and SMO algorithms do well in this aspect , since the predictor always goes closer to the optimal in every step . SGD algorithms do not have such property and the accuracy fluctuate as the number of the strong convexity [ 23 ] [ 24 ] ensures that P packSVM achieves good accuracy , as we analyzed in Section IIIA Speed on a single machine . SGD algorithms are the fastest for linear SVM [ 25 ] , and SMO algorithms are generally believed the fastest for non linear kernels [ 12 ] , while IPM algorithms fall far behind . Few of the papers substantially address the incorporation of kernels in SGD , because before the introduction of the best known learning rate in [ 24 ] , SGD algorithms like [ 16 ] take a much longer time than SMO . We will show in Section IV.A that our SGD algorithm , PpackSVM , can achieve similar efficiency as SMO on a single machine . Parallel speed up . Regarding the parallel capability , we need to consider the following two factors :
• The communication cost . the
With increasing number of processors , communications start to become the bottleneck , so the algorithm that invokes fewer communications iterations is logarithmic to 1/𝜖 [ 27 ] , while SMO and of communications requests by a factor of 𝑟 . This
SGD both experience a large number of iterations . In this paper we turn the tide and reduce the number shows its superiority . Under such a magnitude , IPM algorithms take the lead , for their number of makes our P packSVM highly parallelized . • The parallel efficiency Amdahl ’s law . [ 2 ]
The law states that a small portion of the program that cannot be parallelized will limit the overall speed up . In the view point of Amdahl ’s law , IPM algorithms are the most difficult to be parallelized , due to its complex matrix operations . In PSVM [ 5 ] there exists a small scale Cholesky factorization in each iteration that cannot be parallelized , which becomes the bottleneck as to be shown in Section IVC SMO algorithms are relatively easier but need modification , like [ 3 ] . Our proposed P packSVM has highly parallelized each iteration , except for only a constant number of commands , and thus attains the potential to reach high scalability .
Compare with PSVM . We pay special attention to the comparisons with our well matched adversary PSVM . Firstly , in order to achieve an endurable speed , PSVM forces an approximation to the kernel matrix . This approximation , by Incomplete Cholesky Factorization , lacks theoretical error bounds . We empirically show in the next section that this decomposition is not accurate enough in many datasets . On the contrary , though in stochastic manner , the theoretical convergence analysis on P packSVM guarantees good accuracy . Secondly , as previously stated , PSVM optimizes the dual objective while our P packSVM directly optimizes on the primal . Thirdly , the parallel speed up of PSVM cannot achieve the height of P packSVM , due to Amdahl ’s law mentioned above . Fourthly , the memory requirement for
PSVM is as high as 𝑂(𝑚.(cid:2873)/𝑝 ) , while P packSVM uses only 𝑂(𝑚/𝑝 ) for each processor , making memory no longer a bottleneck for the algorithm .
IV . EXPERIMENTS
In this section we perform experiments on training sets varying in size from 1,000 to 8,000,000 samples . We use 144 equally configured machines in our data center , where each machine is equipped with two 2.5GHz Intel Xeon CPUs with a total of eight cores and a memory of 16GB . We use the Message Passing Interface ( MPI ) as our parallel platform [ 22 ] . We first introduce the binary classification datasets in the experiments : • CCAT dataset , retrieved from RCV1 collection [ 18 ] . The samples are scaled by the author and have a sparsity of 016 % • CovType dataset , prepared by J . T Y Kwok [ 17 ] . No normalization has been performed on this dataset , and it has 54 features in total .
TABLE I .
COMPARISONS ON THE TRAINING TIME . #PROCESSORS IS ONLY APPLIES TO PSVM AND P PACKSVM .
#samples(train/test ) #features
#processors
SVM light
Data set Splice1 Adult3 Web3
CovType3 CCAT3
RCV1 All3 MNIST8m2
1,000 / 2,175 32,561 / 16,281 49,749 / 14,951 522,910 / 58,102 781,265 / 23,149 781,265 / 23,149 8,000,000 / 10,000
60 123 300 54
47,236 47,236 784
8 128 128 256 256 256 512
0.3s 1103s 2483s 280101s 219744s 3819441s
PSVM P pack 1 P pack 1.5 P pack 2 0.6s 12s 17s 748s 18173s 74888s4
4s 12s 19s 864s 2739s 79686s 145248s
2s 5s 8s 321s 918s 32363s 12880s
3s 8s 14s 574s 1741s 55323s 41866s
1 We used 𝑇=10𝑚,15𝑚,20𝑚 for P pack 1 / 1.5 / 2 resp . , and 𝑚(cid:4593)=0.1𝑚 for PSVM . 2 We used 𝑇=𝑚/8,𝑚/4,𝑚/2 for P pack 1 / 1.5 / 2 resp . In this set , both SVM light and PSVM fail to run within ten days . 3 For the rest of the datasets , we used 𝑇=𝑚,1.5𝑚,2𝑚 for P pack 1 / 1.5 / 2 resp . 4 We forced to use m(cid:4593)=m.(cid:2872 ) instead of m.(cid:2873 ) to reduce PSVM ’s running time . and thus we choose 𝑚(cid:4593)=0.1𝑚 for PSVM and 𝑇= 10𝑚,15𝑚,20𝑚 for P packSVM . small training set splice , neither PSVM nor P packSVM can achieve reasonable accuracy under the above configurations ,
• Splice / Web / Adult prepared by the libSVM project team [ 8 ] . They are three relatively small datasets . • RCV1 All , the entire 103 categories in RCV1 topics collection . This is a multi label problem and we consider it as 103 binary classifications and add the correct / incorrect predictions together to verify the accuracy . • Class 2 in the MNIST8m dataset , prepared by the libSVM project team [ 8 ] . This set contains 8.1 million samples and was generated [ 19 ] by performing careful elastic deformation of the original MNIST training set . We use the scaled version , with values in [ 0,1 ] .
We use the first 8 million samples as training data , and prepare two sets of testing data : the last 100,000 samples in MNIST8m , and the 10,000 samples in the original MNIST testing set . the Gaussian 𝑟𝑏𝑓 kernel
𝒦(𝑥,𝑥(cid:2870))=exp(−𝑟𝑏𝑓∙‖𝑥−𝑥(cid:2870)‖(cid:2870)(cid:2870 ) )
For convenience , throughout the experiments we stick to though our proposed algorithm can deal with arbitrary kernels like the polynomial kernel , Laplacian kernel , etc . A . Performance Test
In the first experiment we compare the running time and the accuracy of our proposed P packSVM against two stateof the art SVM trainers : SVM light [ 12 ] and PSVM [ 5 ] . For SVM light we use its default convergence parameters . For PSVM we set a gap threshold and the residual ( primal & dual ) threshold to 0.1 , and an upper limit of 1000 iterations .
Unless otherwise state , we adopt the suggested 𝑚(cid:4593)=𝑚.(cid:2873 ) For all of the test sets , we choose the best selected 𝜎(𝐶= 1/𝑚𝜎 in SVM light ) and 𝑟𝑏𝑓 for the Gaussian kernel ( see 𝑇=𝑚,1.5𝑚,2𝑚 as three different iteration limits , and approximation ( see Section II ) , which was claimed to balance the accuracy and the efficiency in [ 5 ] .
Appendix for a detailed configuration ) . These parameters are equally set to the three trainers . In our P packSVM , we set notate them as P pack 1 , P pack 1.5 and P pack 2 . The program runs three times and the mean accuracy , mean number of support vectors and mean training time are calculated . We give special regard to the fact that for the
Considering the training time in TABLE I . Our poposed method is undoubtedly the fastest for large scale learning . Notice that the column “ #processors ” applies to both PSVM and P packSVM , while SVM light is a sequential SVM trainer . We conclude that P packSVM is hundreds of times faster than SVM light and several times faster than PSVM for large datasets like CovType and CCAT . Notice that , by performing simple multiplication , one can see even in a single machine , our proposed P packSVM may achieve a similar speed as SVM light for large scale data .
The number of support vectors in our model is the smallest among the three ( Figure 5 ) , partially because the number of iterations is limited and some samples are not selected in the entire execution of P packSVM . The accuracy report in Figure 6 demonstrates that our proposed method can get accuracy very close to SVM light ’s , and overwhelm the state of the art trainer PSVM on datasets except CovType . We remark here that the approximation – incomplete Cholesky decomposition – makes PSVM not accurate enough for datasets with large rank kernel matrices , like CCAT . RCV1 All . We test on a sequential of 103 labels in RCV1 , and add the number of correct / incorrect instances together . We pay special attention to this test because most of the labels are extremely biased ( number of negative samples dominate ) . Results in Figure 7 show that our proposed PpackSVM can handle this situation successfully . In sharp contrast , PSVM receives no more than 50 % in the F1 measure [ 11 ] ( we use 𝑚(cid:4593)=𝑚.(cid:2872 ) to make PSVM stop in did not spend extra time choosing the best fit 𝜎 and 𝑟𝑏𝑓 ( see several days ) . MNIST8m . We emphasize that our proposed method can run against the very large scale dataset MNIST8m with 8 million training samples . For the lack of computing resources , we
Appendix ) . To the best of our knowledge , no generic kernel SVM trainer has claimed its success on training this dataset . [ 19 ] used the invariance property of MNIST8m and achieved an accuracy of 99.33 % for all 10 classes in 8 days ( predicting
CCAT
CoverType
Web
Adult
Splice
1
F1
Accuracy
P pack 2 2 P pack 1 1.5 P pack 1 1 PSVM SVMLig ght
100
10000
1000000
Figu ure 5 . Comparison ns on # support vec ctors .
P pack 2 P pack 1.5 P pack 1 PSVM SVMLight
0.7
0.75
0 0.8
0.85
0.9 9
0.95
1
Figure 6
6 . Comparisons on the accuracy .
CCAT
CovType
Web
Adult
Splice
0.96 0.95 0.94 0.93 0.92
2 P pack 1.5 P pack P pack 1 PSVM SVMLi ight
1
10 umber of iterations T T re 8 . CCAT accura acy with 𝑇
20 x
100000
0
Nu of 𝑟=100 is for 𝑟= =1,10 in speed eighth
Figur
We running speed u can be time in What i both C obtain a CovTyp PSVM for both our pro much alike to d . our scalability summarize o onds is shown g time in seco illustrated in F up values are i seen that with hout our packi the number of ncreases when s worse , the sp peed up does n vType . With t CCAT and Cov 295 times and a speed up of 2 pe respectively y , with 512 pro el speed up of gains a paralle h CCAT and C CovType . Thus oposed P packS SVM in efficien V . ENHANCE V e dialectically this section we propose some packSVM , and erm . P packSV VM does not in e naive integra ective ( 2 ) . Th in a theoretic al challenge o ause the linear strong convexit a slightly differ strong convexi ginal objective arizer and resul ime increase . ion . Our algori at without a du asurement of t sist on calculat will appear in that the aver m samples can b ve breaks the s is to enable a to ensure the s ose to the orig this new regula 2 ) in running ti rgence Criteri on . It is true tha an explicit mea gap . If we ins ra factor of 𝑚 w e a substitute le 1000 random
In t our P p Bias te the obj result i
𝑇=𝑂(cid:3560)( ( 1/𝜎𝛿𝜖 ) , beca 𝑏(cid:2870))/2 t objectiv to this stay clo tested t ( about 2 Conver criterio lacks a duality an extr propose exampl the single proc cessor task below : the . , and the igure 10 . It he training xceeds 256 . 0 times for acking , we CCAT and he contrary , n 180 times far behind test results b in TABLE II igure 9 and Fi ing strategy , th f processors ex not exceed 100 the help of pa 416 times for ocessors . On th f no more than s , PSVM falls ncy for all valu EMENT analyze the lim mitation of . enhancements ias term in ncorporate a b ation of a vari of the conver r dependency o ty [ 24 ] . The be rent regularize ity , and at the . In the exper lts show a cons ues of 𝑝 . iable 𝑏 will on 𝑏 in the er ( ‖𝑤‖(cid:2870)(cid:2870)+ same time riment , we stant factor rgence rate est solution ithm lacks a co ual view of the the convergenc ting the primal the time comp rage hinge los be calculated ( t onvergence problem it ce , like the l objective , plexity . We ss over for this needs
0
0.2 o𝑇 𝑇=𝑚/8,𝑚/4 on the original
0.4 Figure 7 . Compar
4,𝑚/2 and ob l MNIST test s o 𝑇 the num
9.57 % on the d an accuracy 0 samples in m shown in TAB r results show t e training . ce Test nd experiment ,
9 99.54 % and 99 te esting set , and th he last 100,00 r running time is c competitor , our o on million scale B . Convergenc B In the secon w with respect to e experiment is p prediction accu s shown in Figur a around 200,00 T This chart also o of iterations on w with a stochast p packSVM with Test C . Scalability C In the third d experiment w nd CCAT , and on CovType an o ferent number o ti ime with a diff We define the 1 128 , 256 , 512 . as the following g : a speed_ conducted on uracy is calcula re 8 , the accur 0 iterations ( 1 helps the user n request . We tic method , th respect to 𝑇 is
_up =(cid:2930)(cid:2919)(cid:2923)(cid:2915 ) ( cid:2916)(cid:2925)(cid:2928 ) ( cid:2876 ) ( cid:2930)(cid:2919)(cid:2923)(cid:2915 ) ( cid:2916)(cid:2925)(cid:2928 ) ( cid:2926 ) ( cid:2930)(cid:2919)(cid:2923)(cid:2915 ) ( cid:2916)(cid:2925)(cid:2928 ) ( cid:2876 ) ( cid:2926)(cid:2928)(cid:2925)(cid:2913)(cid:2915)(cid:2929 ) ( cid:2930)(cid:2919)(cid:2923)(cid:2915 ) ( cid:2916)(cid:2925)(cid:2928 ) ( cid:2926 ) ( cid:2926)(cid:2928)(cid:2925 ) speed_up =
We use th n numerator in th p packSVM exp p processors . For r running time to he 8 processor he above equat perience close r P packSVM w o be the basel
0.6
0.8 risons on RCV1 Al ll . set ) . In our ex btained an accu class 2 of the of 100 % , 100 % minst8m as the BLE I . Althoug the competenc xperiment , we uracy of 99.49 e original MNI % and 100 % e testing set . T gh without a go ce of P packSV set 9 % , IST for The ood VM he accuracy cur we analyze th tions spent . T mber of iterat n the CCAT dataset and t 000 iterations . ated every 50,0 94.5 % only af racy exceeds 9 256 processor 1 minute for ecide the numb r to actively de lthough equipp notice that al he stability of our proposed t . s still sufficient rve The the As fter rs ) . ber ped P parallel speed we run PSVM d measure the of processors 𝑝 ( cid:2926)(cid:2928)(cid:2925)(cid:2913)(cid:2915)(cid:2929)(cid:2929)(cid:2925)(cid:2928)(cid:2929)×8 ( cid:2926)(cid:2928)(cid:2925)(cid:2913)(cid:2915)(cid:2929)(cid:2929)(cid:2925)(cid:2928)(cid:2929 ) ( cid:2925)(cid:2913)(cid:2915)(cid:2929)(cid:2929)(cid:2925)(cid:2928)(cid:2929 ) ×8 ( cid:2929)(cid:2929)(cid:2925)(cid:2928)(cid:2929),(cid:2928)(cid:2880 ) line for 𝑟=1 , r results as th tion ) , since bo e to linear spe we use the 8 pr
𝑝= 8 , 16 , 32 , 6 and P packSV elapsed traini
up measureme
VM ing 64 , ent
( PSVM )
( P packSVM ) he baseline ( t th PSVM and eed up below
8 00 rocessor 𝑟=1 10 , because o the P one
SCALABILITY TEST FOR PSVM AND P PACKSVM . ( MEAN TIME OF THREE RUNS , 𝑇=𝑛 )
𝒑=𝟑𝟐 𝒑=𝟔𝟒 𝒑=𝟏𝟐𝟖 𝒑=𝟐𝟓𝟔 𝒑=𝟓𝟏𝟐
𝒑=𝟏𝟔
𝒑=𝟖
278780s 30599s 29308s 28061s 14294s 13224s 12895s 12267s
138246s 15976s 14734s 13838s 7099s 6374s 6014s 5710s
71933s 8528s 7386s 6953s 5626s 3144s 2728s 2611s
46989s 4793s 3631s 3307s 2866s 1529s 959s 924s
31235s 2928s 1930s 1552s 1342s 1144s 544s 514s
18313s 2570s 1122s 917s 934s 1128s 390s 316s
12917s 3282s 1265s 761s 1587s 1346s 389s 236s
TABLE II .
PSVM
Algorithm
Data set CCAT CCAT CCAT CCAT CovType CovType CovType
P packSVM , 𝑟=1 P packSVM , 𝑟=10 P packSVM , 𝑟=100 P packSVM , 𝑟=1 P packSVM , 𝑟=10 CovType P packSVM , 𝑟=100
PSVM p u d e e p S l e l l a r a P
600 500 400 300 200 100 0
0
200 400 Number of processors Figure 9 . CovType speed up .
600
P pack r=1 P pack r=10 P pack r=100 PSVM Linear
600 500 400 300 200 100 0 p u d e e p S l e l l a r a P no extra effort since our algorithm in Figure 1 already calculates the hinge loss for random examples ) , and if this value is numerically stable enough , the program can automatically stop . Experimental results show that the iteration limit 𝑇 is around the sample size 𝑚 . slightly change ∇𝑓(cid:3047 ) in Equation ( 7 ) . We have shown that the
Extension to other loss . We emphasize that our proposed algorithm can be easily generalized to convex loss functions , other than the hinge loss . For example , L2 Kernel Logistic Regression can be similarly solved where we only need to convergence rate still holds in a counterpart of this paper [ 30 ] . We will perform this research in our further work .
VI . CONCLUSION
This paper analyzes a stochastic gradient descent method that optimizes the primal SVM objectives for arbitrary kernels . Parallel implementation is provided by introducing a distributed hash table and the innovative packing strategy . The proposed algorithm , P packSVM , averagely distributes the support vector to all processors , and the 𝑟 consecutive gradient descent steps can be packed with constant times of communications requests . We emphasize that this packing strategy compensates for the defect of SGD – large communication cost , and is effective in increasing the parallel speed up .
We conduct extensive experiments on benchmark datasets that vary in size , in sparsity and in the number of features . Extensive experimental results show that our proposed algorithm can run much faster than the state of theart parallel SVM trainer PSVM [ 5 ] , and hundreds of times faster than the sequential trainer SVM light . For example , PpackSVM trains CovType with 500k samples in 4 minutes
P pack r=1 P pack r=10 P pack r=100 PSVM Linear
0
200 Number of processors
400
600
Figure 10 . CCAT speed up . and CCAT with 800k samples in 13 minutes . We emphasize that P packSVM attains accuracy that is sufficiently close to SVM light , and prevails over that of PSVM . ACKNOWLEDGMENT
Zeyuan Allen Zhu wants to thank Shai Shalev Shwartz of Hebrew University for his valuable discussions , Teng Gao from Tsinghua University for his construction and maintenance on our parallel platform , and Zhijie Ren from Peking University for her comparable experiments on PSVM . Zeyuan Allen Zhu is partially supported by the National Innovation Research Project for Undergraduates ( NIRPU ) .
The authors also acknowledge Matt Callcut and all three anonymous reviewers for their fruitful comments .
REFERENCES
[ 1 ] Mark A . Aizerman , Emmanuel M . Braverman , and Lev I . Rozonoér , "Theoretical foundations of the potential function method in pattern recognition learning," Automation and Remote Control 25 , 1964 .
[ 2 ] Gene Amdahl , "Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities," in AFIPS , 1967 , pp . 483 485 .
[ 3 ] Li Juan Cao et al . , "Parallel sequential minimal optimization for the training of support vector machines," IEEE Transactions on Neural Networks , vol . 17 , no . 4 , pp . 1039 1049 , July 2006 .
[ 4 ] Chih Chung Chang and Chih Jen Lin . ( 2001 ) LIBSVM : a library for support vector machines . http://wwwcsientuedutw/~cjlin/libsvm
[ 5 ] Edward Y . Chang , Kaihua Zhu , Hao Wang , and Hongjie Bai , "PSVM : Parallelizing Support Vector Machines on Distributed Computers," in NIPS , 2007 , Software available at http://codegooglecom/p/psvm
[ 6 ] Nello Cristianini and John Shawe Taylor , An introduction to support vector machines . : Cambridge University Press , 2000 .
[ 7 ] Osuna Edgar , Robert Freund , and Federico Girosi , "An improved algorithm for support vector machines," in IEEE Signal Processing Society Workshop , 1997 .
Support Vector Machines using Selective Sampling," in Large Scale Kernel Machines . : MIT Press , 2007 , pp . 301 320 .
[ 8 ] Rong En Fan . LIBSVM Data : Classification , Regression , and Multi label . [ Online ] . http://wwwcsientuedutw/~cjlin/libsvmtools/datasets/
[ 9 ] Shai Fine and Katya Scheinberg , "Efficient SVM Training Using
Low Rank Kernel Representations," JMLR , pp . 243 264 , 2001 .
[ 10 ] Don Hush , Patrick Kelly , Clint Scovel , and Ingo Steinwart , "QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines," JMLR , vol . 7 , pp . 733 769 , 2006 .
[ 11 ] Thorsten Joachims , Learning to Classify Text Using Support Vector
Machines . : Kluwer Academic Publisher , 2002 .
[ 12 ] Thorsten Joachims , "Making large scale SVM learning practical," in Advances in Kernel Methods Support Vector Learning . : MIT Press , 1998 .
[ 13 ] Thorsten Joachims , "Optimizing search engines using clickthrough data," in SIGKDD , 2002 .
[ 14 ] Thorsten Joachims , "Training Linear SVMs in Linear Time," in KDD ,
2006 .
[ 15 ] Sham Kakade and Shai Shalev Shwartz , "Mind the Duality Gap : Logarithmic regret algorithms for online optimization," in NIPS , 2009 . [ 16 ] Jyrki Kivinen , Alexander J . Smola , and Robert C . Williamson , "Online Learning with Kernels," in IEEE Transactions on Signal Processing , 2004 .
[ 17 ] James Tin Yau Kwok . CovType classification data . [ Online ] . http://wwwcsusthk/~jamesk/data/forestzip
[ 18 ] David D . Lewis , Yiming Yang , Tony G . Rose , and Fan Li , "RCV1 : A New Benchmark Collection for Text Categorization Research," Journal of Machine Learning Research , vol . 5 , pp . 361 397 , 2004 .
[ 19 ] Gaëlle Loosli , Stéphane Canu , and Léon Bottou , "Training Invariant
[ 20 ] Christopher D . Manning , Prabhakar Raghavan , and Hinrich Schütze , Introduction to Information Retrieval . : Cambridge University Press , 2008 .
[ 21 ] Sanjay Mehrotra , "On the Implementation of a Primal Dual Interior Point Method," SIAM Journal on Optimization , vol . 2 , no . 4 , pp . 575601 , November 1992 .
[ 22 ] MPI Documents . [ Online ] . http://wwwmpi forumorg/docs/ [ 23 ] Shai Shalev Shwartz , "Online Learning : Theory , Algorithms , and applications," The Hebrew University , PhD Thesis 2007 .
[ 24 ] Shai Shalev Shwartz , Yoram Singer , and Nathan Srebro , "Pegasos :
Primal Estimated sub GrAdient SOlver for SVM," in ICML , 2007 .
[ 25 ] Shai Shalev Shwartz and Nathan Srebro , "SVM Optimization : Inverse
Dependence on Training Set Size," in ICML , 2008 .
[ 26 ] Vladimir Vapnik , The Nature of Statistical Learning Theory . :
Springer Verlag , 1995 .
[ 27 ] Stephen J . Wright , Primal dual interior point methods . : SIAM , 1997 . [ 28 ] Gaetano Zanghirati and Luca Zanni , "A parallel solver for large quadratic programs in training support vector machines," Parallel Computing , vol . 29 , no . 4 , April 2003 .
[ 29 ] Tong Zhang , "Solving Large Scale Linear Prediction Problems Using
Stochastic Gradient Descent Algorithms," in ICML , 2004 .
[ 30 ] Zeyuan Allen Zhu et al . , "Inverse Time Dependency in Regularized
Learning," in ICDM , 2009 .
5 . 6 .
APPENDIX
PROCESSOR 𝑖 1 . INPUT : 𝜎,𝑇,𝑟 , training sample space 𝛹 2 . INITIALIZE : ℋ=∅ ,𝑠=1,𝑛𝑜𝑟𝑚=0 3 . FOR 𝑡 = 1,2,…,𝑇/𝑟 4 . Randomly pick up 𝑟 samples ( 𝒙,𝑦)…(𝒙(cid:3045),𝑦(cid:3045))∈𝛹 . Ensure all processors receive the same samples . FOR 𝑘 = 1,…𝑟 DO 𝑦,(cid:3038)(cid:4593)←𝑠〈𝒗𝒌,𝜙(𝒙𝒊)〉 by iterating all entries in ℋ 7 . Communicate with other processors to get 𝑦(cid:3038)(cid:4593)=∑𝑦,(cid:3038)′ 8 . Calculate 𝑝𝑎𝑖𝑟,=𝒦(𝑥,𝑥 ) in distribution
𝐿𝑜𝑐𝑎𝑙𝑆𝑒𝑡←∅ FOR 𝑘=1,…𝑟 DO 𝑠←(1−1/𝑡)𝑠 FOR 𝑙=𝑘+1…𝑟 DO 𝑦(cid:3039)(cid:4593)←(1−1/𝑡)𝑦(cid:3039)(cid:4593 ) IF 𝑦(cid:3038)𝑦(cid:3038)(cid:4593)<1 THEN 𝑛𝑜𝑟𝑚←𝑛𝑜𝑟𝑚+(cid:2870)(cid:3052)(cid:3286)(cid:3097)(cid:3047)∙𝑦(cid:3038)(cid:4593)+(cid:4672)𝑦𝑘𝜎𝑡(cid:4673)2𝑝𝑎𝑖𝑟𝑘,𝑘 𝐿𝑜𝑐𝑎𝑙𝑆𝑒𝑡←𝐿𝑜𝑐𝑎𝑙𝑆𝑒𝑡∪(cid:4676)(𝒙𝒌,𝑦𝑘𝜎𝑡/𝑠)(cid:4677 ) For 𝑙=𝑘+1…𝑟 DO 𝑦(cid:3039)(cid:4593)←𝑦(cid:3039)(cid:4593)+(cid:3052)(cid:3286)(cid:3097)(cid:3047)∙𝑝𝑎𝑖𝑟(cid:3038),(cid:3039 ) IF 𝑛𝑜𝑟𝑚>1/𝜎 THEN √(cid:3097)∙(cid:3042)(cid:3045)(cid:3040 ) ; 𝑛𝑜𝑟𝑚←1𝜎⁄ 𝑠←𝑠∙ FOR 𝑙=𝑘+1…𝑟 DO 𝑦(cid:3039)(cid:4593)←(1−1/𝑡)𝑦(cid:3039)(cid:4593 )
20 . Update ℋ according to 𝐿𝑜𝑐𝑎𝑙𝑆𝑒𝑡 , for those elements reported not existed in ℋ…ℋ(cid:3043 ) , 21 . RETURN 𝑠𝒗𝒊 by iterating all entries in ℋ,…ℋ(cid:3043 )
9 . 10 . 11 . 12 . 13 . 14 . 15 . 16 . 17 . 18 . 19 . add them to the least occupied processors .
Figure 11 . P packSVM pseudo code , with packing strategy
THE PARAMETERS USED IN EXPERIMENTS .
𝝈
0.001 0.0001 0.00001 0.000005 0.00001 0.00001 0.000001
TABLE III .
𝑪=𝟏/𝒎𝝈
1
0.307116 2.010091 0.382475 0.127998 0.127998 0.123457
Data set splice adult web
CovType CCAT
RCV1 All MNIST8m
𝒓𝒃𝒇
0.01 1 1
0.002
1 1 1
( PSVM ) rank_ratio=𝒎(cid:4593)/𝒎 00055418(𝑚(cid:4593)=𝑚(cid:2873 ) ) 000448341(𝑚(cid:4593)=𝑚(cid:2873 ) ) 000138289(𝑚(cid:4593)=𝑚(cid:2873 ) ) 000113136(𝑚(cid:4593)=𝑚(cid:2873 ) ) 0000291287(𝑚(cid:4593)=𝑚(cid:2872 ) )
0.1
𝒓
( P packSVM )
100 100 100 100 100 100 100
