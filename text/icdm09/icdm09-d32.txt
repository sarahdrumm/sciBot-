Discovering Excitatory Networks from Discrete Event Streams with Applications to Neuronal Spike Train Analysis
Debprakash Patnaik
Department of Computer Science
Virginia Tech , VA 24061 , USA
Email : patnaik@vt.edu
Srivatsan Laxman Microsoft Research
Bangalore 560080 , India
Email : slaxman@microsoft.com
Naren Ramakrishnan
Department of Computer Science
Virginia Tech , VA 24061 , USA
Email : naren@vt.edu
Abstract—Mining temporal network models from discrete event streams is an important problem with applications in computational neuroscience , physical plant diagnostics , and human computer interaction modeling . We focus in this paper on temporal models representable as excitatory networks where all connections are stimulative , rather than inhibitive . Through this emphasis on excitatory networks , we show how they can be learned by creating bridges to frequent episode mining . Specifically , we show that frequent episodes help identify nodes with high mutual information relationships and which can be summarized into a dynamic Bayesian network ( DBN ) . To demonstrate the practical feasibility of our approach , we show how excitatory networks can be inferred from both mathematical models of spiking neurons as well as real neuroscience datasets .
Keywords Frequent Episodes ; Dynamic Bayesian Network ; Computational Neuroscience ; Spike train analysis ; Temporal Data Mining
I . INTRODUCTION
Discrete event streams are prevalent in many applications , such as neuronal spike train analysis , physical plants , and human computer interaction modeling . In all these domains , we are given occurrences of events of interest over a time course and the goal is to identify trends and behaviors that serve discriminatory or descriptive purposes .
A Multi Electrode Array ( MEA ) records spiking action potentials from an ensemble of neurons which after various pre processing steps , yields a spike train dataset providing real time , dynamic , perspectives into brain function ( see Fig 1 ) . Identifying sequences ( eg , cascades ) of firing neurons , determining their characteristic delays , and reconstructing the functional connectivity of neuronal circuits are key problems of interest . This provides critical insights into the cellular activity recorded in the neuronal tissue .
Similar motivations arise in other domains as well . In physical plants the discrete event stream denotes diagnostic and prognostic codes from stations in an assembly line and the goal is to uncover temporal connections between codes emitted from different stations . In human computer interaction modeling , the event stream denotes actions taken by users over a period of time and the goal is to capture aspects such as user intent and interaction strategy by understanding causative chains of connections between actions .
Beyond uncovering structural patterns from discrete events , we seek to go further , and actually uncover a generative temporal process model for the data . In particular , our aim is to infer dynamic Bayesian networks ( DBNs ) which encode conditional independencies as well as temporal influences and which are also interpretable patterns in their own right . We focus exclusively on excitatory networks where the connections are stimulative rather than inhibitory in nature ( eg , ‘event A stimulates the occurrence of event B 5ms later which goes on to stimulate event C 3ms beyond.’ ) This constitutes a large class of networks with relevance in multiple domains , including neuroscience .
Our main contributions are three fold : 1 ) New model class of excitatory networks : Learning Bayesian networks ( dynamic or not ) is a hard problem and to obtain theoretical guarantees we typically have to place restrictions on network structure , eg , assume the BN has a tree structure as done in the ChowLiu algorithm . Our focus on excitatory networks places restrictions on the nature of the conditional probability tables ( CPT ) instead of network structure and we show how this leads to a tractable formulation .
2 ) New methods for learning DBNs : We demonstrate that DBNs can be learnt by creating bridges to frequent episode mining literature . In particular , the focus on excitatory networks allows us to relate frequent episodes to parent sets for nodes with high mutual information . This enables us to predominantly apply fast algorithms for episode mining , while relating them to probabilistic notions suitable for characterizing DBNs .
3 ) New applications to spike train analysis : We demonstrate a successful application of our methodologies to analyzing neuronal spike train data , both from mathematical models of spiking neurons and from real cortical tissue .
The paper is organized as follows . Sec II gives a brief overview of DBNs and Sec III presents our formalism for modeling event streams using DBNs . Sec IV defines excitatory networks and develops the theoretical basis for efficiently learning such networks . Sec V introduces fixeddelay episodes and relates frequencies of such episodes with marginal probabilities of a DBN . Our learning algorithm is presented in Sec VI , experimental results in Sec VII and conclusions in Sec VIII .
Figure 1 . A multi electrode array ( MEA ; left ) produces a spiking event stream of action potentials ( middle top ) . Mining cascaded firings ( middle bottom ) in the event stream helps uncover excitatory circuits ( right ) in the data .
II . BAYESIAN NETWORKS : STATIC AND DYNAMIC Formal mathematical notions are presented in the next section , but here we wish to provide some background context to past research in Bayesian networks ( BNs ) . As is well known , BNs use directed acyclic graphs to encode probabilistic notions of conditional independence , such as that a node is conditionally independent of its non descendants given its parents ( for more details , see [ 1] ) . The earliest known work for learning BNs is the Chow Liu algorithm [ 2 ] . It showed that , if we restricted the structure of the BN to a tree , then the optimal BN can be computed using a minimum spanning tree algorithm . It also established the tractability of BN inference for this class of graphs .
More recent work , by Williamson [ 3 ] , generalizes the Chow Liu algorithm to show how ( discrete ) distributions can be approximated using the same general ingredients as the Chow Liu approach , namely mutual information quantities between random variables . Meila [ 4 ] presents an accelerated algorithm that is targeted toward sparse datasets of high dimensionality . The approximation thread for general BN inference is perhaps best exemplified by Friedman ’s sparse candidate algorithm [ 5 ] that presents various greedy approaches to learn ( suboptimal ) BNs .
DBNs are a relatively newer development and best examples of them can be found in specific state space and dynamic modeling contexts , such as HMMs . In contrast to their static counterparts , exact and efficient inference for general classes of DBNs has not been studied well .
III . MODELING EVENT STREAMS USING DBNS
Consider a finite alphabet , E = {A1 , . . . , AM} , of event types ( or symbols ) . Let s = ( E1 , τ1 ) , ( E2 , τ2 ) , . . . , ( En , τn ) denote a data stream of n events over E . Each Ei , i = 1 , . . . , n , is a symbol from E . Each τi , i = 1 , . . . , n , takes values from the set of positive integers . The events in s are ordered according to their times of occurrence , τi+1 ≥ τi , i = 1 , . . . , ( n− 1 ) . The time of occurrence of the last event in s , is denoted by τn = T . We model the data stream , s , as a realization of a discrete time random process
X(t ) , t = 1 , . . . , T ; X(t ) = [ X1(t)X2(t)··· XM ( t) ] , where Xj(t ) is an indicator variable for the occurrence of event type , Aj ∈ E , at time t . Thus , for j = 1 , . . . , M and t = 1 , . . . , T , we will have Xj(t ) = 1 if ( Aj , t ) ∈ s , and Xj(t ) = 0 otherwise . Each Xj(t ) is referred to as the eventindicator random variable for event type , Aj , at time t . Example 1 : The following is an example event sequence of n = 7 events over an alphabet , E = {A , B , C , . . . , Z} , of M = 26 event types : ( A , 2 ) , ( B , 3 ) , ( D , 3 ) , ( B , 5 ) , ( C , 9 ) , ( A , 10 ) , ( D , 12 ) ( 1 ) The maximum time tick is given by T = 12 . Each X(t ) , t = 1 , . . . , 12 , is a vector of M = 26 indicator random variables . Since there are no events at time t = 0 in the example sequence ( 1 ) , we have X(1 ) = 0 . At time t = 2 , we will have X(2 ) = [ 1000··· 0 ] . Similarly , X(3 ) = [ 0101··· 0 ] , and so on .
A DBN [ 6 ] is a DAG with nodes representing random variables and arcs representing conditional dependency relationships . We model the random process X(t ) ( or equivalently , the event stream s ) , as the output of a DBN . Each event indicator , Xj(t ) , t = 1 , . . . , T and j = 1 , . . . M , corresponds to a node in the network , and is assigned a set of parents , which is denoted as π(Xj(t ) ) ( or simply πj(t) ) . A parent child relationship is represented by an arc ( from parent to child ) in the DAG . In a DBN , nodes are conditionally independent of their non descendants given their parents . The joint probability distribution of X(t ) under the DBN model , can be factorized as a product of P [ Xj(t)| πj(t ) ] for various j , t . In this paper we restrict the class of DBNs using the following two constraints :
A1
A2
[ Time bounded causality ] For user defined parameter , W > 0 , the set , πj(t ) , of parents for the node , Xj(t ) , is a subset of event indicators out of the W length history at time tick , t , ie πj(t ) ⊆ {Xk(τ ) : 1 ≤ k ≤ M , ( t − W ) ≤ τ < t} . [ Translation invariance ] If πj(t ) = {Xj1(t1 ) , . . . , Xj(t)} is an size parent set of Xj(t ) for some t > W , then for any other Xj(t ) , t > W , its parent set , πj(t ) , is simply a time shifted version of πj(t ) , and is given by πj(t ) = {Xj1(t1 + δ ) , . . . , Xj ( t + δ)} , where δ = ( t − t ) .
While A1 limits the range of influence of a random variable , Xk(τ ) , to variables within ( a user defined ) W time ticks of τ , A2 is a structural constraint that allows parent child relationships to depend only on relative ( rather than absolute ) time stamps of random variables . Further , we also assume that the underlying data generation model is stationary , so that joint statistics can be estimated using frequency counts of suitably defined temporal patterns in the data .
A3
[ Stationarity ] For every set of event indicators , Xj1 ( t1 ) , . . . , Xj(t ) , and for every time shift δ , we have P [ Xj1 ( t1 ) , . . . , Xj(t ) ] = P [ Xj1(t1+δ ) , . . . , Xj(t + δ) ] .
Learning network structure involves learning the map , πj(t ) , for each Xj(t ) , j = 1 , . . . , M and t > W . Let I[Xj(t ) ; πj(t ) ] denotes the mutual information between Xj(t ) and its parents , πj(t ) . DBN structure learning can be posed as a problem of approximating the data distribution , P [ · ] , by a DBN distribution , Q[· ] . Let DKL(P||Q ) denote the KL divergence between P [ · ] and Q[· ] . Using A1 , A2 and A3 , and following the lines of [ 2 ] , [ 3 ] , it is possible to show that for parent sets with sufficiently high mutual information to Xj(t ) , DKL(P||Q ) will be concomitantly lower [ 7 ] . In other words , a good DBN based approximation of the underlying stochastics ( ie one with small KL divergence ) can be achieved by picking , for each node , a parent set whose corresponding mutual information exceeds a userdefined threshold .
However , picking such sets with high mutual information ( while yields a good approximation ) falls short of unearthing useful dependencies among the random variables . This is because mutual information is non decreasing as more random variables are added to a parent set ( leading to a fullyconnected network always being optimal ) . For a parent set to be interesting , it should not only exhibit sufficient correlation ( or mutual information ) with the corresponding child node , but should also successfully encode the conditional independencies among random variables in the system . This can be done by checking if , conditioned on a candidate parentset , the mutual information between the corresponding childnode and all its non descendants is always close to zero . ( We provide more details later in Sec VI C ) .
IV . EXCITATORY NETWORKS
The structure learning approach described in Sec III is applicable to any general DBN that satisfies A1 and A2 . In this paper , we focus on a further specialized class of networks , called excitatory networks , where only certain kinds of conditional dependencies among nodes are permitted . In general , each event type has some baseline propensity in the data which is small and less than 0.5 ( This corresponds to a sparse data assumption ) . A collection , Π , of random variables is said to have an excitatory influence on an event type , A ∈ E , if occurrence of events corresponding to the variables in Π , increases the propensity of A to greater than 05 We define an excitatory network as one in which nodes can only exert excitatory influences on one another . For example , in an excitatory network it is possible that “ if B , C and D occur ( say ) 2 time ticks apart , the probability of A increases . ” By contrast , in excitatory networks , it is not possible to model relationships like “ when A does not occur , the probability of B occurring 3 time ticks later increases . ” Similarly , excitatory networks cannot model inhibitory relationships like “ when A occurs , the probability of B occurring 3 time ticks later decreases . ” Excitatory networks are natural in neuroscience , where one is interested in unearthing conditional dependency relationships among neuron spiking patterns . Several regions in the brain are known to exhibit predominantly excitatory relationships [ 8 ] and our model is targeted toward unearthing these .
EXAMPLE OF A CONDITIONAL PROBABILITY TABLE IN AN EXCITATORY
Table I
NETWORK .
Π
XB XC XD 0 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1
0 0 1 1 0 0 1 1
P [ XA = 1 | aj ] < 1 1 ≥ 2 2 ≥ 3 ≥ , 1 , 2 4 ≥ 5 ≥ , 1 , 4 6 ≥ , 2 , 4 2 , j∀j φ > , 1 a0 a1 a2 a3 a4 a5 a6 a7(a∗ )
The excitatory assumption manifests as a set of constraints on the conditional probability tables associated with the DBN . Consider a node1 XA ( an indicator variable for eventtype A ∈ E ) and let Π denote a parent set for XA . In excitatory networks , the probability that A occurs , conditioned on the occurrence of all the events associated with Π , should be at least as high as the corresponding conditional probability when only some ( though not all ) of the events of Π occur . Further , the probability of A is less than 0.5 when none of the events of Π occur and greater than 0.5 when all the events of Π occur . For example , let Π = {XB , XC , XD} . The conditional probability table for A given Π is shown in Table I . The different conditioning contexts come about by the occurrence or otherwise of each of the events in Π . These are denoted by aj , j = 0 , . . . , 7 . So while a0 represents the all zero assignment ( ie none of the events B , C or D occur ) , a7 ( or a∗ ) denotes the all ones assignment ( ie all the events B , C and D occur ) . The last column of the table lists the corresponding conditional probabilities along with the associated excitatory constraints . The baseline propensity
1To facilitate simple exposition , time stamps of random variables are dropped from the notation in this discussion . of A is denoted by and the only constraint on it is that 2 . Conditioned on the occurrence of it must be less than 1 any event of Π , the propensity of A can only increase , and hence , j ≥ ∀j and φ ≥ . Similarly , when both C and D occur , the probability must be at least as high as that when either C or D occurred alone ( ie we must have 3 ≥ 1 and 3 ≥ 2 ) . Finally , for the all ones case , denoted by Π = a∗ , 2 and must the conditional probability must be greater than 1 also satisfy φ ≥ j ∀j .
In the context of DBN structure learning , excitatory networks ensure that event types will occur frequently after their respective parents ( with suitable delays ) . This will allow us to estimate DBN structure using frequent pattern discovery algorithms ( which have been a mainstay in data mining for many years ) . We now have a simple necessary condition on the probability ( or frequency ) of parent sets in an excitatory network . Theorem 4.1 : Let XA denote a node in the Dynamic Bayesian Network corresponding to the event type A ∈ E . Let Π denote a parent set with excitatory influence on A . Let ∗ be an upper bound for conditional probabilities P [ XA = 1 | Π = a ] for all a = a∗ ( ie for all but the all ones assignment in Π ) . If the mutual information I[XA ; Π ] exceeds ϑ(> 0 ) , then the joint probability of an occurrence of A along with all events of Π satisfies P [ XA = 1 , Π = a∗ ] ≥ PminΦmin , where
( 2 )
Pmin = Φmin = h−1
P [ XA = 1 ] − ∗
1 − ∗ min h(P [ XA = 1 ] ) − ϑ
1 ,
Pmin
( 3 ) and where h(· ) denotes the binary entropy function h(q ) = −q log q − ( 1− q ) log(1− q ) , 0 < q < 1 and h−1[· ] denotes its pre image greater than 1 2 . Proof : Under the excitatory model we have P [ XA = 1|Π = a∗ ] > P [ XA = 1 | Π = a ] ∀a = a∗ . First we apply ∗ to terms in the expression for P [ XA = 1 ] :
P [ XA = 1 ] = P [ Π = a∗
]P [ XA = 1 | Π = a∗
]
+ ≤ P [ Π = a∗
P [ Π = a]P [ XA = 1 | Π = a ] ] ) ∗
] + ( 1 − P [ Π = a∗ a=a∗
This gives us P [ Π = a∗ ] ≥ Pmin ( see Fig 2 ) . Next , since we are given that mutual information I[XA ; Π ] exceeds ϑ , the corresponding conditional entropy must satisfy : ]h(P [ XA = 1 | Π = a∗
H[XA | Π ] = P [ Π = a∗
] )
P [ Π = a]h(P [ XA = 1 | Π = a ] )
+ < H[XA ] − ϑ = h(P [ XA = 1 ] ) − ϑ a=a∗
Every term in the expression for H[XA | Π ] is non negative , and hence , each term ( including the first one ) must be less than ( h(P [ XA = 1 ] ) − ϑ ) . Using ( P [ Π = a∗ ] ≥ Pmin ) in
Figure 2 . An illustration of the results obtained in Theorem 41 x axis of the plot is the range of P [ XA = 1 ] and y axis is the corresponding range of entropy H[XA ] . For the required mutual information criteria , the conditional entropy must lie below H[XA ] − ϑ and on line A . At the boundary condition [ φ = 1 ] , Line A splits Line B in the ratio Pmin : ( 1 − Pmin ) . This gives the expression for Pmin . the above inequality and observing that P [ XA = 1| Π = a∗ ] must be greater than 0.5 for an excitatory network , we now get ( P [ XA = 1 | Π = a∗ ] > Φmin ) . This completes the proof .
V . FIXED DELAY EPISODES
In the framework of frequent episode discovery [ 9 ] the data is a single long stream of events over a finite alphabet ( cf . Sec III and Example 1 ) . An node ( serial ) episode , α , is defined as a tuple , ( Vα , <α , gα ) , where Vα = {v1 , . . . , v} denotes a collection of nodes , <α denotes a total order2 such that vi <α vi+1 , i = 1 , . . . , ( − 1 ) . If gα(vj ) = Aij , Aij ∈ E , j = 1 , . . . , , we use the graphical notation ( Ai1 → ··· → Ai ) to represent α . An occurrence of α in event stream , s = ( E1 , τ1 ) , ( E2 , τ2 ) , . . . , ( En , τn ) , is a map h : Vα → {1 , . . . , n} such that ( i ) Eh(vj ) = g(vj ) ∀vj ∈ Vα , and ( ii ) for all vi <α vj in Vα , the times of occurrence of the ith and jth events in the occurrence satisfy τh(vi ) ≤ τh(vj ) in s . Example 2 : Consider a 3 node episode α = ( Vα , <α , gα ) , such that , Vα = {v1 , v2 , v3} , v1 <α v2 , v2 <α v3 and v1 <α v3 , and gα(v1 ) = A , gα(v2 ) = B and gα(v3 ) = C . The graphical representation for this episode is α = ( A → B → C ) , indicating that in every occurrence of α , an event of type A must appear before an event of type B , and the B must appear before an event of type C . For example , in sequence ( 1 ) , the subsequence ( A , 1 ) , ( B , 3 ) , ( C , 9 ) constitutes an occurrence of ( A → B → C ) . For this 2In general , <α can be any partial order over Vα . We focus on only total orders here and show how multiple such total orders can be used to model DBNs of arbitrary arity . In [ 9 ] , such total orders are referred to as serial episodes .
Curve B : H[X] θCurve A : H[XA]h(ε*)h(φ)ε*φH[XA]P[XA=1]H[XA ] θθ1Pmin1 PminLine BLine ALine C occurrence , the corresponding h map is given by , h(v1 ) = 1 , h(v2 ) = 2 and h(v3 ) = 5 .
There are many ways to incorporate explicit time constraints in episode occurrences like the windows width constraint of [ 9 ] . Episodes with inter event gap constraints were introduced in [ 10 ] . For example , the framework of [ 10 ] can express the temporal pattern “ B must follow A within 5 time ticks and C must follow B within 10 time ticks . ” Such a pattern is represented using the graphical notation , [ 0–10]−→ C ) . In this paper , we use a simple ( A sub case of the inter event gap constraints , in the form of fixed inter event time delays . For example , ( A 5→ B 10→ C ) represents a fixed delay episode , every occurrence of which must comprise an A , followed by a B exactly 5 time ticks later , which in turn is followed by a C exactly 10 time ticks later .
[ 0–5]−→ B
Definition 5.1 : An node fixed delay episode is defined as a pair , ( α,D ) , where α = ( Vα , <α , gα ) is the usual ( serial ) episode of [ 9 ] , and D = ( δ1 , . . . , δ−1 ) is a sequence of ( − 1 ) non negative delays . Every occurrence , h , of the fixed delay episode in an event sequence smust satisfy the inter event constraints , δi = ( τh(vi+1 ) − τh(vi) ) , i = δ1−→ ··· δ−1−→ Aj ) is the graphi1 , . . . , ( − 1 ) . ( Aj1 cal notation for inter event episode , ( α,D ) , where Aji = gα(vi ) , i = 1 , . . . , .
Definition 5.2 : Two occurrences , h1 and h2 , of a fixeddelay episode , ( α,D ) , are said to be distinct , if they do not share any events in the data stream , s . Given a user defined , W > 0 , frequency of ( α,D ) in s , denoted fs(α,D , W ) , is defined as the total number of distinct occurrences of ( α,D ) in s that terminate strictly after W . In general , counting distinct occurrences of episodes suffers from computational inefficiencies [ 11 ] . ( Each occurrence of an episode ( A → B → C ) is a substring that looks like A ∗ B ∗ C , where ∗ denotes a variable length don’t care , and hence , counting all distinct occurrences in the data stream can require memory of the same order as the data sequence which typically runs very long ) . However , in case of fixed delay episodes , it is easy to track distinct occurrences efficiently . For example , when counting frequency of ( A 3−→ B 5−→ C ) , if we encounter an A at time t , to recognize an occurrence involving this A we only need to check for a B at time ( t + 3 ) and for a C at time ( t + 8 ) . In addition to being attractive from an efficiency point of view , we show next in Sec V A that the distinct occurrences based frequency count for fixed delay episodes will allow us to interpret relative frequencies as probabilities of DBN marginals . ( Note that the W in Definition 5.2 is same as length of the history window used in the constraint A1 . Skipping occurrences terminating in the first W timeticks makes it easy to normalize the frequency count into a probability measure ) .
A . Marginals from episode frequencies
: j = 1 , . . . , M ;
Definition 5.3 : Let {Xj(t )
In this section , we describe how to compute mutual information from the frequency counts of fixed delay episodes . For this , every subset of event indicators in the network is associated with a fixed delay episode . t = 1 , . . . , T} denote the collection of event indicators used to model event stream , s = ( E1 , τ1 ) , . . . ( En , τn ) , over alphabet , E = {A1 , . . . , AM} . Consider an size subset , X = {Xj1(t1 ) , . . . , Xj(t)} , of these indicators , and without loss of generality , assume t1 ≤ ··· ≤ t . Define the ( − 1 ) inter event delays in X as follows : δj = ( tj+1 − tj ) , j = 1 , . . . , ( − 1 ) . The fixed delay episode , ( α(X ),D(X ) ) , that is associated with the subset , X , of event indicators is defined by α(X ) = ( Aj1 → ··· → Aj ) , and D(X ) = {δ1 , . . . , δ−1} . In graphical notation , the fixed delay episode associated with X can be represented as follows :
( α(X ),D(X ) ) = ( Aj1
δ1→ ··· δ−1→ Aj )
( 4 )
For computing mutual information we need the marginals of various subsets of event indicators in the network . Given a subset like X = {Xj1(t1 ) , . . . , Xj(t)} , we need estimates for probabilities of the form , P [ Xj1(t1 ) = a1 , . . . , Xj ( t ) = a ] , where aj ∈ {0 , 1} , j = 1 , . . . , . The fixeddelay episode , ( α(X ),D(X ) ) , that is associated with X is given by Definition 5.3 and its frequency in the data stream , s , is denoted by fs(α(X ),D(X ) , W ) ( as per Definition 5.2 ) where W denotes length of history window as per A1 . Since an occurrence of the fixed delay episode , ( α(X ),D(X ) ) , can terminate in each of the ( T − W ) time ticks in s , the probability of an all ones assignment for the random variables in X is given by :
P [ Xj1(t1 ) = 1 , . . . , Xj(t ) = 1 ] = fs(α(X ),D(X ) , W )
T − W
( 5 ) in [ 12 ] ,
For all other assignments ( ie for assignments that are not all ones ) we use inclusion exclusion to obtain corresponding probabilities . Inclusion exclusion has been used before in data mining , eg , to obtain exact or approximate frequency counts for arbitrary boolean queries using only counts of frequent itemsets in the data . In our case , counting distinct occurrences of fixed delay episodes facilitates use of the inclusion exclusion formula for obtaining the probabilities needed for computing mutual information of different candidate parent sets . Consider the set , X = {Xj1 ( t1 ) , . . . , Xj(t)} , of event indicators , and let A = ( a1 , . . . , a ) , aj ∈ {0 , 1} , j = 1 , . . . , , be an assignment for the event indicators in X . Let U ⊂ X denote the subset of indicators out of X for which corresponding assignments ( in A ) are 1 ’s , i . e . U = {Xjk ∈ X : k st
Procedure 1 Overall Procedure Input : Alphabet E , event stream s = ( E1 , τ1 ) , . . . , ( En , τn = T ) , length W of history window , conditional probability upper bound ∗ , mutual information threshold , ϑ network )
Output : DBN structure ( parent set for each node in the 1 : for all A ∈ E do 2 : XA := event indicator of A at any time t > W 3 : 4 :
Set fmin = ( T − W )PminΦmin , using Eqs . ( 2) (3 ) Obtain set , C , of fixed delay episodes ending in A , with frequencies greater than fmin ( cf . Sec VI B , Procedure 2 ) for all fixed delay episodes ( α,D ) ∈ C do
X(α,D ) := event indicators corresponding to ( α,D ) Compute mutual information I[XA ; X(α,D ) ] Remove ( α,D ) from C if I[XA ; X(α,D ) ] < ϑ
Prune C using conditional mutual information criteria to distinguish direct from indirect influences ( cf . Sec VI C ) Return ( as parent set for XA ) event indicators corresponding to episodes in C
5 : 6 : 7 : 8 : 9 :
10 : ak = 1 in A , 1 ≤ k ≤ } . Inclusion exclusion is used to compute the probabilities as follows :
P [ Xj1 = a1 , . . . , Xj = a ] ( −1 )
=
|Y\U| fs(Y )
T − W
( 6 )
Y st
U ⊆ Y ⊆ X where fs(Y ) is short hand for fs(α(Y),D(Y ) , W ) , the frequency ( cf . Definition 5.2 ) of the fixed delay episode , ( α(Y),D(Y) ) .
VI . ALGORITHMS
A . Overall approach
In Secs . III V , we developed the formalism for learning an optimal DBN structure from event streams by using distinct occurrences based counts of fixed delay episodes to compute the DBN marginal probabilities . The top level algorithm ( cf . Sec III ) for discovering the network is to fix any time t > W , to consider each Xj(t ) , j = 1 , . . . , M , in turn , and to find its set of parents in the network . Due to the translation invariance assumption A2 , we need to do this only once for each event type in the alphabet . The algorithm is outlined in Procedure 1 . For each A ∈ E , we first compute the minimum frequency for episodes ending in A based on the relationship between mutual information and joint probabilities as per Theorem 4.1 ( line 3 , Procedure 1 ) . Then we use a pattern growth approach ( see Procedure 2 ) to discover all patterns terminating in A
Procedure 2 pattern grow(α,D,L(α,D ) ) Input : node episode ( α,D ) = ( Aj1
δ1→ ··· δ−1→ Aj ) and event sequence s = ( E1 , τ1 ) , . . . , ( En , τn = T ) , Length of history window W , Frequency threshold fmin . for δ = 0 to ∆ do
1 : ∆ = W − span(α,D ) 2 : for all A ∈ E do 3 : 4 : 5 : 6 : 7 : 8 : continue if δ = 0 and ( Aj1 > A or = 1 ) then ( α,D ) = A δ→ α ; L(α,D ) = {} ; fs(α,D ) = 0 for all τi ∈ L(α,D ) do if ∃(Ej , τj ) such that Ej = A and τi − τj = δ then
9 : 10 : 11 : 12 : 13 : 14 :
Increment fs(α,D ) L(α,D ) = L(α,D ) ∪ {τj} if fs(α,D ) ≥ fmin then Add ( α,D ) to output set C if span(α,D ) ≤ W then pattern grow(α,D,L(α,D ) )
( line 4 , Procedure 1 ) . Each frequent pattern corresponds to a set of event indicators ( line 6 , Procedure 1 ) . The mutual information between this set of indicators and the node XA is computed using exclusion exclusion formula and only sets for which this mutual information exceeds ϑ are retained as candidate parent sets ( lines 5 8 , Procedure 1 ) . Finally , we prune out candidate parent sets which have only indirect influences on A and return the final parent sets for nodes corresponding to event type A ( lines 9 10 , Procedure 1 ) . This pruning step is based on some conditional mutual information criteria ( to be described later in Sec VI C ) .
B . Discovering fixed delay episodes
We employ a pattern growth algorithm ( Procedure 2 ) for mining frequent fixed delay episodes because , unlike Apriori style algorithms , pattern growth procedures allow use of different frequency thresholds for episodes ending in different alphabets . This is needed in our case , since , in general , Theorem 4.1 prescribes different frequency thresholds for nodes in the network corresponding to different alphabets . The recursive procedure is invoked with ( α,D ) = ( A , φ ) and frequency threshold fmin = ( T − W )Pminφmin ( Recall that in the main loop of Procedure 1 , we look for parents of nodes corresponding to event type A ∈ E ) . The pattern growth algorithm listed in Procedure 2 takes as input , an episode ( α,D ) , a set of start times L(α,D ) , and the event sequence s . L(α,D ) is a set of time stamps τi such that there is an occurrence of ( α,D ) starting at τi in s . For example , if at level 1 we have ( α,D ) = ( C , φ ) , then L(C,φ ) = {1 , 4 , 5 , 8 , 9} in the event sequence s shown in Fig 3 . The algorithm obtains counts for all episodes like ( α,D )
Figure 3 . An event sequence showing 3 distinct occurrences of the episode A
1→ B
2→ C . generated by extending ( α,D ) eg B 1→ C , . . . , A 5→ C etc . For an episode say ( α,D ) = B 2→ C , the count is obtained by looking for occurrences of event B at times τj = τi − 2 where τi ∈ L(C,φ ) . In the example such B ’s at τj ∈ L B 2→C = {2 , 3 , 6} . The number of such occurrences ( = 3 ) gives the count of B 2→ C . At every step the algorithm tries to grow an episode with count fs > fmin otherwise stops .
C . Conditional MI criteria
The final step in determining the parents of a node XA involves testing of some conditional mutual information criteria ( cf . line 10 , Procedure 1 ) . The input to the step is a set C of ( frequent ) fixed delay episodes ending in A . Each episode in C is associated with a set of event indicators whose mutual information with XA exceeds ϑ . Consider two such sets Y and Z , each having sufficient mutual information with XA . Our conditional mutual information criterion is : remove Y from the set of candidate parents ( of XA ) , if I[XA ; Y | Z ] = 03 . We repeat this test for every pair of episodes in C . To understand the utility of this criterion , there are two cases to consider : ( i ) either Y ⊂ Z or Z ⊂ Y , and ( ii ) both Y ⊂ Z and Z ⊂ Y . In the first case , our conditional mutual criterion will ensure that we pick the larger set as a parent only if it brings more information about XA than the smaller set . In the second case , we are interested in eliminating sets which have a high mutual information with XA because of indirect influences . For example , if the network were such that C excites B and B excites A , then XC can have high mutual information with XA , but we do not want to report XC as a parent of XA , since C influences A only through B . Our conditional mutual information criterion will detect this and eliminate XC from the set of candidate parents ( of XA ) because it will detect I[XA ; XC | XB ] = 0 .
VII . EXPERIMENTAL RESULTS
We present results on data gathered from both mathematical models of spiking neurons as well as real neuroscience datasets .
A . Neuronal network model
The approach here is to model each neuron as an inhomogeneous Poisson process whose firing rate is a function
3We use a small threshold parameter to ascertain this equality . j ijl of the input received by the neuron is recent past [ 13 ] :
λi(t ) =
λ
1 + exp(−Ii(t ) + δ )
( 7 )
Eq ( 7 ) gives the firing rate of the ith neuron at time t . The network inter connect allowed by this model gives it the amount of sophistication required for simulating higherorder interactions . More importantly , the model allows for variable delays which mimic the delays in conduction pathways of real neurons .
Ii(t ) =
βijYj(t−τij )+ . . .+
βijlYj(t−τij ) . . . Yl(t−τil )
( 8 ) In Eq ( 8 ) , Yj(t−τij ) is the indicator of the event of a spike on jth neuron τij time earlier and the β(.)s are the weight parameters for the interactions . The higher order terms in the input contribute to the firing rate only when the ith neuron received inputs from all the neurons in the term with corresponding delays . With suitable choices of parameters β(. ) , one can simulate a wide range of networks . B . Types of Networks
In this section we demonstrate the effectiveness of our approach in unearthing different types of networks . Each of these networks was simulated by setting up the appropriate inter connections , of suitable order , in our mathematical model .
Causative chains and higher order structures : A higherorder chain is one where parent sets are not restricted to be of cardinality one . In the example network of Fig 4(a ) , there are four disconnected components with two of them having cycles . ( Recall this would be ‘illegal’ in a static Bayesian network formulation . ) Also the component consisting of nodes 18 , 19 , 20 , 21 exhibits higher order interactions . The node 20 fires with high probability when node 18 has fired 4 ms before and node 18 has fired 5 ms before . Similarly node 21 is activated by node 18 , 19 , 20 firing at respective delays . The complete network consists of 100 nodes ( with the remainder of the nodes firing independently ) . Spike train data is generated for runs of 60 sec using the multi neuronal simulator . The base firing rate for neurons is set at 20Hz and the activation probability of a child node ( ie the conditional probability that the child node fires given its parents ) is varied form 0.6 to 0.8 ( by suitably selecting βij ’s ) . Our algorithm reports good precision and recall over a range of ϑ ( 0.05 ≤ ϑ ≤ 0.5 ) and ∗ ( 0.02 ≤ ∗ ≤ 004 ) For lower values of ( simulation ) conditional probability , recall gradually drops but precision remains high ( 100% ) . Details are shown in Table II . Overlapping causative chains : The graph shown in Fig 4(a ) ( b ) has two chains 0 → 1 → 2 → 3 and 12 → 1 → 13 → 14 → 15 → which share the node 1 . Here 1 can be independently excited by 0 or 12 . Also 2 is activated by 0 , 1 together and 13 is activated by 13 , 1 .
12345678910ABC Table II
RESULTS FOR NETWORK SHOWN IN FIG . 4(A ) FOR VARYING
CONDITIONAL PROBABILITY ( USED IN GENERATION ) AND MIN . MI ϑ ; BASE FIRING RATE = 20HZ [ PROBABILITY=0.02 IN 1MS
BINS ] ; BASE RATE THRESHOLD ∗ = 03
Cond . prob .6 .6 .6 .6 .9 .9 .9 .9 .9
ϑ
.05 .075 .1 .25 .05 .075 .1 .25 .5
Time ( sec ) 7.5 7.4 7.1 2.8 41.8 42.6 42.0 30.0 26.0
Recall ( % ) 69.6 60.9 4.3 0.0 100 87.0 69.6 47.8 0.0
Precision ( % ) 100 100 100 100 92.0 95.2 100 100 100
Thus a firing event on 0 excites the chain 0 → 1 → 2 → 3 where as a firing event on 12 excites the other chain . This shows one possible way in which neurons can participate in several different circuits at the same time ( eg polychronous circuits [ 14] ) . Depending on the stimulus sequence , the same neurons can participate in different cascade firing events ( encoding completely unrelated pieces of information ) . For each node , our formulation reports multiple sets of nodes that satisfy the minimum mutual information ∗ , thus unearthing 0 and 12 as two sets activating 1 . 0 , 1 and 12 , 1 are also found to be the parent sets of 2 and 3 respectively ( with high precision and recall ) .
Syn fire chains : Another important pattern often reported in neuronal spike train data is that of synfire chains . This consists of groups of synchronously firing neurons strung together repeating over time . In [ 10 ] , it was noted that discovering such patterns required a combination of serial and parallel episode mining . But the DBN approach applies more naturally to mining such network structures . Again we are able to find the structure for a wide range of parameters . For larger histories or influence windows W , many combinations of nodes are frequent , slowing down the mining process . ( For instance , network 4(c ) takes 180 sec to mine as compared to 60 sec for network 4(a ) , on a dual core 3GHz Windows Vista computer with 3GB RAM . )
Polychronous circuits : Groups of neurons that fire in a time locked manner with respect to each other are referred to as polychronous groups . This notion was introduced in [ 14 ] and gives rise to an important class of patterns . Once again , our DBN formulation is a natural fit for discovering such groups from spike train data . A polychronous circuit is shown in Fig 4(d ) . We are also able to discover overlapping polychronous circuits ( where different sets of nodes can excite the same node ) . For relatively deep networks ( having nodes with long ancestry ) recall drops mainly because the nodes lower down in the graph are not excited sufficiently often ( and hence do not meet the mutual information threshold ) . Detailed results are listed in Table III .
Table III
RESULTS FOR NETWORK SHOWN IN FIG . 4(D ) FOR VARYING BASE RATE THRESHOLD ∗ AND MIN . MI ϑ ; BASE FIRING RATE
= 20HZ AND COND . PROB . = 09
∗
0.005 0.01 0.03 0.06 0.1 0.03 0.03 0.03 0.03 0.03
ϑ
0.04 0.04 0.04 0.04 0.04 0.05 0.075 0.1 0.25 0.5
Time ( in sec ) 56.0 56.3 45.8 0.9 0.9 44.2 6.1 6.0 1.2 0.8
Recall ( % ) 80.0 80.0 66.7 0.0 0.0 66.7 26.7 13.3 0.0 0.0
Precision ( % ) 66.7 66.7 66.7 100.0 100.0 66.7 80.0 100.0 100.0 100.0
C . Scalability
The scalability of our approach with respect to data length and number of variables is shown in Fig 5(a ) . Here four different networks with 50 , 75 , 100 and 125 variables respectively were simulated for time durations ranging from 20 sec to 120 sec . The base firing rate of all the networks was fixed at 20 Hz . In each network 40 % of the nodes were chosen to have upto three parents . The parameters of the DBN mining algorithm were chosen such that recall and precision are both high ( > 80% ) . It can be seen in the figures that for a network with 125 variables , the total run time is of the order of few minutes along with recall > 80 % and precision at almost 100 % .
Another way to study scalability is wrt the density of the network , defined as the ratio of the number of nodes that are descendants for some other node to the total number of nodes in the network . Fig 5(b ) shows the time taken for mining DBNs when the density is varied from 0.1 to 0.6 , averaged over 36 datasets . We observe near linear growth in time taken and the absolute figures can be improved using native implementation ( currently our algorithms are implemented in Python ) .
( a ) Varying data length in sec
( b ) Varying network density
Figure 5 . Plot of total time taken for DBN discovery
204060801001200200400600800Duration of event sequence ( in sec)Time taken ( in sec)50−Nodes75−Nodes100−Nodes125−Nodes0102030405060100200300400500600DensityTime taken ( in sec)50−Nodes75−Nodes100−Nodes125−Nodes ( a ) Higher order causative chains
( b ) causative chains
Overlapping
( c ) Syn fire Chains
( d ) Polychronous Circuits
Figure 4 . Four classes of DBNs investigated in our experiments .
D . Sensitivity
Finally , we discuss the sensitivity of the DBN mining algorithm to the parameters ϑ , ∗ and cond . mutual information threshold ( used to check for MI=zero ) . To obtain precision recall curves for our algorithm applied to data sequences with different characteristics , we vary the parameter ϑ in the range [ 004 006 ] and repeat for different cond . mutual information threshold values . The data sequence for this experiment is generated from the multi neuronal simulator using different settings of base firing rate , conditional probability , number of nodes in the network , and the density of the network .
The set of precision recall curves are shown in Fig 6 . The general trends observed here show that for a range of settings of the conditional probability , base firing rates , and network topology , both high precision and high recall can be obtained . As the stringency of the conditional mutual information threshold is increased ( compare Fig 6 ( top ) to Fig 6 ( bottom ) , we observe a deterioration of performance only wrt the base rate threshold .
E . Cortical cultures
Multi electrode arrays provide high throughput recordings of the spiking activity in neuronal tissue and are hence rich sources of event data where events correspond to specific neurons being activated . We use data from dissociated cortical cultures gathered by Steve Potter ’s laboratory at Georgia Tech [ 15 ] which gathered data over several days . The mining is done with mutual information threshold ϑ = 0.001 with DBN search parameter ∗ = 002
In order to establish the significance of the networks discovered we run our algorithm on several surrogate spike trains generated by replacing the neuron labels of spikes in the real data with randomly chosen labels . These surrogates break the temporal correlations in the data and yet preserve the overall summary statistics . No network structure was found in such surrogate sequences . We are currently in the process of characterizing and interpreting the usefulness of
Figure 7 . recording on day 35 of culture 2 1 [ 15 ] .
DBN structure discovered from first 15 min of spike train such networks found in real data . An example network is shown in Fig 7 , reflecting the sustained bursts observed in this culture by Wagenaar et al [ 15 ] .
VIII . DISCUSSION
Our work marries frequent pattern mining with probabilistic modeling for analyzing discrete event stream datasets . DBNs provide a formal probabilistic basis to model relationships between time indexed random variables but are intractable to learn in the general case . Conversely , frequent episode mining is scalable to large datasets but does not exhibit the rigorous probabilistic interpretations that are the mainstay of the graphical models literature . We have presented the beginnings of research to relate these two diverse threads and demonstrated its potential to mine excitatory networks with applications in spike train analysis .
Two key directions of future work are being explored . The excitatory assumption as modeled here posits an order over the entries of the conditional probability table but does not impose strict distinctions of magnitude over these entries . This suggests that , besides the conditional independencies inferred by our approach , there could potentially
Figure 6 . Precision recall curves for different parameter values in the DBN mining algorithm . be additional ‘structural’ constraints masquerading inside the conditional probability tables . We seek to tease out these relationships further . A second , more open , question is whether there are other useful classes of DBNs that have both practical relevance ( like excitatory circuits ) and which also can be tractably inferred using sufficient statistics of the form studied here .
ACKNOWLEDGMENT
This work is supported in part by General Motors Research , NSF grant CNS 0615181 , and ICTAS , Virginia Tech . Authors thank V . Raajay and P . S . Sastry of Indian Institute of Science , Bangalore , for many useful discussions and for access to the neuronal spike train simulator of [ 13 ] .
REFERENCES
[ 1 ] M . I . Jordan , Ed . , Learning in Graphical Models . MIT Press ,
1998 .
[ 2 ] C . Chow and C . Liu , “ Approximating discrete probability distributions with dependence trees , ” IEEE Transactions on Information Theory , vol . 14 , no . 3 , pp . 462–467 , May 1968 .
[ 3 ] J . Williamson , “ Approximating discrete probability distributions with bayesian networks , ” in Proc . Intl . Conf . on AI in Science & Technology , Tasmania , 2000 , pp . 16–20 .
[ 4 ] M . Meila , “ An accelerated chow and liu algorithm : Fitting tree distributions to high dimensional sparse data , ” in Proc . ICML’99 , 1999 , pp . 249–257 .
[ 5 ] N . Friedman , K . Murphy , and S . Russell , “ Learning the structure of dynamic probabilistic networks , ” in Proc . UAI’98 . Morgan Kaufmann , 1998 , pp . 139–147 .
[ 6 ] K . Murphy , “ Dynamic Bayesian Networks : representation , inference and learning , ” PhD dissertation , University of California , Berkeley , CA , USA , 2002 .
[ 7 ] D . Patnaik , S . Laxman , and N . Ramakrishnan , “ Inferring dynamic bayesian networks using frequent episode mining , ” CoRR , vol . abs/0904.2160 , 2009 .
[ 8 ] F . Rieke , D . Warland , R . Steveninck , and W . Bialek , Spikes :
Exploring the Neural Code . The MIT Press , 1999 .
[ 9 ] H . Mannila , H . Toivonen , and A . Verkamo , “ Discovery of frequent episodes in event sequences , ” Data Mining and Knowledge Discovery , vol . 1 , no . 3 , pp . 259–289 , 1997 .
[ 10 ] D . Patnaik , P . S . Sastry , and K . P . Unnikrishnan , “ Inferring neuronal network connectivity from spike data : A temporal data mining approach , ” Scientific Programming , vol . 16 , no . 1 , pp . 49–77 , January 2007 .
[ 11 ] S . Laxman , “ Discovering frequent episodes : Fast algorithms , connections with hmms and generalizations , ” PhD dissertation , IISc , Bangalore , India , September 2006 .
[ 12 ] J . K . Seppanen , “ Using and extending itemsets in data mining : Query approximation , dense itemsets and tiles , ” PhD dissertation , Helsinki University of Technology , 2006 .
[ 13 ] V . Raajay , “ Frequent episode mining and multi neuronal spike train data analysis , ” Master ’s thesis , IISc , Bangalore , 2009 .
[ 14 ] E . M .
Izhikevich , “ Polychronization : Computation with spikes , ” Neural Comput . , vol . 18 , no . 2 , pp . 245–282 , 2006 .
[ 15 ] D . A . Wagenaar , J . Pine , and S . M . Potter , “ An extremely rich repertoire of bursting patterns during the development of cortical cultures , ” BMC Neuroscience , 2006 .
020406080100020406080100PrecisionRecallPrecisionminimum MI = 0.04 − 006Cond MI threshold = 1e−04Density02040608020406080100020406080100PrecisionRecallPrecisionminimum MI = 0.04 − 006Cond MI threshold = 0001Density02040608020406080100020406080100PrecisionRecallPrecisionminimum MI = 0.04 − 006Cond MI threshold = 1e−04N5075100125020406080100020406080100PrecisionRecallPrecisionminimum MI = 0.04 − 006Cond MI threshold = 0.001N5075100125020406080100020406080100PrecisionRecallPrecisionminimum MI = 0.04 − 006Cond MI threshold = 1e−04BaseRate001002004006020406080100020406080100PrecisionRecallPrecisionminimum MI = 0.04 − 006Cond MI threshold = 0001BaseRate001002004006020406080100020406080100PrecisionRecallPrecisionminimum MI = 0.04 − 006Cond MI threshold = 1e−04CondProb060809020406080100020406080100PrecisionRecallPrecisionminimum MI = 0.04 − 006Cond MI threshold = 0001CondProb060809
