Kernel Conditional Quantile Estimation via Reduction Revisited
Novi Quadrianto∗ , Kristian Kersting† , Mark D . Reid∗ , Tib´erio S . Caetano∗ and Wray L . Buntine∗
∗SML , NICTA & RSISE , ANU
Canberra ACT , Australia
Email : {firstnamelastname}@nictacomau
†Fraunhofer IAIS
Sankt Augustin , Germany
Email : {firstnamelastname}@iaisfraunhoferde
Abstract—Quantile regression refers to the process of estimating the quantiles of a conditional distribution and has many important applications within econometrics and data mining , among other domains . In this paper , we show how to estimate these conditional quantile functions within a Bayes risk minimization framework using a Gaussian process prior . The resulting non parametric probabilistic model is easy to implement and allows non crossing quantile functions to be enforced . Moreover , it can directly be used in combination with tools and extensions of standard Gaussian Processes such as principled hyperparameter estimation , sparsification , and quantile regression with input dependent noise rates . No existing approach enjoys all of these desirable properties . Experiments on benchmark datasets show that our method is competitive with state of the art approaches .
Keywords Regression ; Quantile Regression ; Gaussian Pro cesses ;
I . INTRODUCTION
In most regression studies , we are typically interested in inferring a real valued function whose values correspond to the mean of response variables conditioned on the explanatory variables . The application of this conditional mean regression is ubiquitous . There are , however , many important applications where we are interested in estimating either the median or other quantiles such as estimating the potential amount of money a customer can spend on a product rather than his/her expected spending [ 1 ] . This is called quantile regression and was introduced by Koenker and Bassett [ 2 ] . The unobservable nature of quantiles means that their prediction is a challenging task . If we had a model p(y|x ) of the conditional distribution of response variables y conditioned on the explanatory variables x , however , their prediction would be much simpler as quantile estimation essentially involves slicing this distribution at a certain quantile level . This slicing operation is a convex optimization problem . Although we are reducing a hard quantile estimation problem to yet another hard problem , ie distribution modeling , the latter is a well studied subject in machine learning in particular Gaussian processes . At first glance , the usage of Gaussian process distribution modeling for learning problems such as classification or regression might violate Vapnik ’s paradigm of estimating only the relevant parameters directly [ 3 ] .
This paradigm is in favor of estimating latent functions while sidestepping distribution modeling . However , there have been several studies that show superiority of Gaussian processes based methods to infer flexible latent functions [ 4 ] , [ 5 ] . Our reduction approach is similar in spirit to the Langford ’s et al . [ 6 ] method in reducing quantile estimation problem to series of classification problems .
Therefore , we propose in this paper to estimate conditional quantile functions within a Gaussian process model [ 7 ] . The well known advantage of using such type of model over non Bayesian models is that of having an explicit probabilistic formulation . This allows us to have a principled way of performing model selection , as well as a predictive posterior probability distribution over response variables . In terms of quantile estimation , the latter is particularly useful when we have censored or missing response variables [ 8 ] . From a practical point of view , our estimator can be easily sparsified , therefore being able to handle large datasets , and can take input dependent noise into account . More importantly , our derived quantile estimator has the desirable property that the estimated conditional functions at different quantiles can never cross or overlap each other . Quantile crossing occurs because each conditional quantile function is independently estimated , and it has traditionally been one of the challenging problems in the field [ 8 ] , [ 9 ] . To our knowledge , our quantile estimator is the first that enjoys both sparsifiable and non crossing properties while being competitive with state of the art alternatives as we will show in our experiments . Our contributions : framework using a Gaussian process prior .
− A quantile estimator within a Bayes risk minimization − A specific example of Bayes quantile estimator which enjoys non parametric , probabilistic model , principled learning of free parameters , sparse approximation , heteroscedasticity and enforced non crossing constraint . inputdependent noise which allows jointly learning the free parameters of the latent and observed processes . − A theoretical analysis of our proposed estimator in
− A novel Gaussian processes treatment of term of regret transform bound .
II . RELATED WORK
Most of existing work focuses on estimating each conditional quantile function separately . A standard technique for conditional quantile estimation is based on a linear model [ 2 ] . In this model , the τ th conditional quantile function of y given x is assumed to be a linear function of the vector of regressors , ie qτ ( x ) := x , β(τ ) , where β(τ ) is a vector of coefficients dependent on τ . Estimation of coefficients is done by minimizing the pinball loss function . It is shown that the minimization can be reformulated as a linear programming problem and can be solved efficiently with interior point techniques [ 8 ] .
The assumption of a linear relationship between the regressors and the conditional quantile function is quite restrictive . Takeuchi et al . [ 10 ] propose a nonparametric approach to quantile regression based on kernel methods . The dual of a regularized version of pinball loss minimization is solved via standard quadratic programming techniques . Several extensions to incorporate commonly desired constraints such as non crossing constraints and a monotonicity constraint are also discussed . This method provides state of the art performance .
Langford et al . [ 6 ] show that the quantile regression problem can be reduced to a series of classification problems such that a small average error rate on the classification problems leads to a provably accurate estimate of the conditional quantile . The estimation of τ th conditional quantile function is first reduced to a set of importance weighted binary classification problems . This problem is further reduced to ubiquitous unweighted binary classification problem via rejection sampling . This method is computationally efficient thus is able to handle large datasets .
The closest work to Quantile Gaussian processes is the work of Yu and Moyeed [ 11 ] . They introduce a Bayesian approach for quantile estimation based on the linear model . An asymmetric Laplace distribution is used as a likelihood function , p(y|x , β(τ) ) , and the prior on the coefficients is chosen to be improper uniform , ( p(β(τ ) ) ∝ 1 ) . Although the prior is improper , they proved that the posterior distribution will be proper . A Markov chain Monte Carlo ( MCMC ) method is used to infer this posterior distribution , p(β(τ)|x , y ) . Finally , the posterior mean is used for quantile ie qτ ( x ) := x , βMAP(τ ) . We focus on a estimation , different notion of prior distribution , usage of any likelihood function , and a different procedure for quantile estimation . Precisely these differences allow us to derive the desirable properties mentioned in the introduction .
III . QUANTILE ESTIMATION AS AN OPTIMIZATION
PROBLEM
Given m observed data points D = {(xi , yi)}m i=1 , where yi ∈ Y ( the set of outputs ) and xi ∈ X ( the set of regressors or inputs ) , the goal of quantile regression is to
Figure 1 . The pinball loss function . infer a conditional quantile function qτ ( x ) from observed data points . Definition 1 ( Conditional Quantile ) : Let τ ∈ ( 0 , 1 ) . The conditional quantile qτ ( x ) for a pair of random variables ( x , y ) ∈ X × R is defined as the function qτ : X → R for which pointwise qτ ( x ) is the infimum over q for which Pr(y ≤ qτ|x ) = τ . minimization of m
The idea behind quantile regression arises from the observation that minimizing the 1 loss function yields the median . The symmetry of the 1 loss function implies that i=1 |yi − q| must give an equal number of yi − q terms lying on either side of zero . Koenker and Bassett [ 2 ] generalize this idea to obtain a quantile regression estimator by tilting the loss function . This loss function is given in Figure 1 and is known as a pinball loss function ,
τ ξ ,
Lτ ( ξ ) :=
( τ − 1)ξ , if ξ ≥ 0 if ξ < 0 .
In this paper , our goal is to estimate the latent quantile function qτ ( x ) in a Bayesian framework with a Gaussian Process prior , which we will develop in the next section .
IV . BAYESIAN FRAMEWORK
Assuming we can estimate the conditional distribution p(y|x ) , the Bayes quantile estimator is found by minimizing expected value of the pinball loss function , ie q(opt ) τ = argmin qτ
Lτ ( y − qτ )p(y|x)dy = argmin qτ where Rτ ( qτ ) := Ep(y|x)[Lτ ( y − qτ ) ] is the Bayes risk .
Lemma 2 : The Bayes risk in Equation ( 1 ) is convex in
Rτ ( qτ ) ( 1 ) qτ .
Proof : The Bayes risk is a convex combination of convex loss functions , which must itself be convex . The subsequent sections will deal with modeling the conditional distribution p(y|x ) . We will describe a model based on Gaussian processes framework . Before introducing the model , let us briefly review Gaussian processes .
Gaussian Process Prior :
In the Gaussian process framework , the output yi at input location xi is assumed to be a corrupted version of a latent function q(xi ) , ie yi = q(xi ) + i where i is the noise term . A Gaussian
Lτ(ξ)τ−1τξ00 process can be used to define a prior distribution over these latent functions [ 7 ] , q ∼ GP(m(x ) , k(x , x) ) , where m(x ) is the mean function ( assumed to be zero ) and the covariance k(x , x ) between functions at input x and x is defined by Mercer kernel functions [ 7 ] .
Likelihood for Quantile Regression : In the Bayesian setting , there is a distinction between the likelihood function and the loss function . The likelihood defines the probability of observing the noisy outputs given the latent functions , whereas the loss function measures the regret of making a specific decision . We can in fact define any likelihood function to model the data . For the purpose of this paper , we give a specific example for the likelihood function where we choose to believe that the noise term is independent and normally distributed , i ∼ N ( 0 , σ2 n is the noise variance . n ) where σ2
Predictive Distribution : Choosing a Gaussian likelihood leads to tractable Bayesian inference , ie the standard Gaussian process conditional mean regression [ 7 ] . Thus , the predictive distribution over latent functions is given as q∗|x∗ , X , Y ∼ N ( µ∗ , σ 2∗ ) with the moments as follows µ∗ = k∗T ( σ2 nI + K)−1Y 2∗ = k(x∗ , x∗ ) − k∗T ( σ2 σ
( 2 ) ( 3 ) In these equations , we have K ∈ Rm×m , Kij = k(xi , xj ) and k∗ ∈ Rm×1 , k∗ i = k(x∗ , xi ) . Here k denotes covariance function . The predictive distribution over output y∗ is also normally distributed , ie y∗|x∗ , X , Y ∼ N ( µ∗ , σ n := σ2∗ ) . nI + K)−1k∗ .
Quantile Estimator : Under the assumption that the true conditional distribution p over y is Gaussian with mean µ and variance σ2 , we can evaluate the risk in ( 1 ) for a given quantile estimate q1 :
Rτ ( q ) = ( µ − q).τ − Φµ,σ2(q)fi + σφµ,σ2(q )
2∗ + σ2
( 4 )
Proposition 3 ( Quantile Estimator ) : The empirical soluτ to ( 1 ) using the predictive distribution y∗|x∗ , X , Y ∼ tion q∗ N ( µ∗ , σ2∗ ) is given by the zero of the following function : fτ ( q ) = Φµ∗,σ2∗(q ) − τ . convex , the p(y∗|x∗ , X , Y )
∂qτ{ Lτ ( y∗ − qτ )p(y∗|x∗ , X , Y )dy∗} = 0 . objective Since ( global ) minimizer N ( µ∗ , σ2∗ ) is of Rτ ( . ) with by is function
Proof : given the
=
Thus , the τ th quantile estimate is given by q∗ τ = µ∗ + σ∗Φ−1(τ ) .
( 5 )
Remark : In literatures , ( 5 ) is known as a location scale model . Several methods have been proposed to estimate both location and scale functions simultaneously ( cf [ 8] ) . This mean µ and variance σ2 and Φµ,σ2 ( z ) = z
1φµ,σ2 ( x ) denotes the density at x of the Gaussian random variable with −∞ φµ,σ2 ( x)dx denotes the
CDF . Φ(z ) := Φ0,1(z ) is the standard Gaussian CDF . is a special case of our framework with a specific choice of likelihood function .
Corollary 4 ( Non Crossing Estimator ) : For
Our estimator carries several advantages . The first is that our estimator inherently enforces a non crossing constraint . Estimation of several conditional quantile functions can cause two or more estimated functions to cross or overlap . This is due to each conditional quantile function being independently estimated . This phenomenon should not happen as the true quantile functions are defined to be non crossing . p(y∗|x∗ , X , Y ) is independent of τ , estimator is a monotone increasing function of τ . the Bayes quantile Proof : Provided p(y∗|x∗ , X , Y ) is independent of τ and has finite density this is immediate from the fact that the inverse CDF is monotonically increasing . There have been several approaches addressing the noncrossing constraint . He [ 9 ] transformed the non crossing constraint into a positivity constraint , however , this might not be desirable from the non parametric point of view . Takeuchi et al . [ 10 ] imposed the non crossing constraint as linear constraints , however , this means that every adjacent pair of conditional quantile functions should be computed when multiple quantiles are needed . Recently , Shim et al . [ 12 ] used a location–scale model and estimated both location and scale functions simultaneously via SVM . It is shown that the proposed method works slightly better than the method of [ 10 ] but offers conceptual simplicity since it estimates the location and scale functions simultaneously .
Secondly , by approximating the predictive distribution over quantile functions with conditional mean Gaussian process regression , we have a large pool of sparse approximation methods at our disposal . Several approximative models , such as subset of regressors , subset of datapoints , projected process , and Bayesian committee machine have been proposed for Gaussian process regression in order to deal with the high time and storage requirements for large training datasets . Many of the approximations use a subset I , |I| = n , of datapoints ( the support set ) from the full training set D , |D| = m , see eg [ 7 ] for more details . Finally , we can elegantly deal with input dependent noise as shown below .
V . HETEROSCEDASTIC QUANTILE ESTIMATION
In many real world problems , the local noise rates are important features of data distributions and hence of conditional quantiles that have to be modeled accurately . Our Bayesian approach allows a simple but elegant solution to handle this locally varying noise : use heteroscedastic Gaussian processes instead of standard ones .
In contrast to the standard Gaussian process approaches discussed so far , we now do not assume a constant noise level n(x ) at location x but place a prior over it . More precisely , an independent Gaussian process is used to model the logarithms of the noise levels , denoted as z(x ) = log(n(x) ) . This noise process is governed by a different estimating the true distribution can potentially make large changes to a quantile location .
σ4
σ2
Proof :
σ2
2π and
=
σ4 fififi d second µ−q σ2 φµ,σ2(q )
The proof of this theorem relies on the Lipschitz continuity of several functions related to Gaussian densities which we establish in the following lemma . Lemma 6 : Upper bounds on the Lipschitz constants for the functions q → φµ,σ2(q ) and q → ( µ − q)Φµ,σ2(q ) are σ−2 and σ−1 , respectively . derivatives first The are φ of φµ,σ2(q ) µ,σ2(q ) and µ,σ2(q ) = ( µ−q)2−σ2 φ φµ,σ2(q ) . Thus , the maximal/minimal values for φ µ,σ2(q ) = 0 . That is , when q = µ ± σ . Thus , for all q ∈ R , we have µ,σ2(q)| ≤ |φ |φ < σ−2 . Similarly , the first and second derivatives of ( µ− q)Φµ,σ2 are ( µ−q)2−σ2 φµ,σ2(q ) . Thus , fififi ≤ or µ − q = ±√ its first derivative is maximal/minimal at either q = µ 2πσ2 max(1 , 2e−3/2 ) < σ−1 and proves the lemma . 3σ . Substituting these solutions back
µ,σ2(q ) occur when φ µ,σ2(µ ± σ)| = σ 1√ √ 2πσ2 = 1 σ2 φµ,σ2(q ) and ( µ−q)3−3σ2(µ−q ) into the first derivative gives 1√ dq ( µ − q)Φµ,σ2(q )
Proof : [ Theorem 5 ] The regret under the assumption that p is the true point distribution is given by ( 4 ) for q and qopt as ∆Rτ ( q ) = τ(qopt − q ) + σ[φµ,σ2(q ) − φµ,σ2(qopt ) ] + ( µ − qopt)Φµ,σ2(qopt ) − ( µ − q)Φµ,σ2(q ) . Letting Γµ,σ2(q ) := ( µ − q)Φµ2 ( q ) and by the Lipschitz conditions of Lemma 6 we can write , |∆Rτ ( q)| ≤ τ|qopt − q| + σ|φµ,σ2(q)− φµ,σ2(qopt)| +|Γµ,σ2(qopt)− Γµ,σ2(q)| = ( τ + σ−1)|qopt − q| . We now note that , by equation ( 5 ) that |qopt − q| ≤ |µ − µ∗| + |Φ−1(τ)||σ − σ∗| , where ( µ , σ ) and ( µ∗ , σ∗ ) are the moments for the true and predictive distribution , respectively . Thus , it is sufficient to bound the difference in means and variances between these distributions . We now make use of our assumption that KL(p∗||p ) < . The KL divergence for two Gaussians is the well known 2σ2 + ( µ−µ∗)2 expression , KL(p∗||p ) = ln + σ2∗ 2 . Since σ∗ and µ∗ can be chosen independently , we note that the upper bound implies both |µ− µ∗| < σ 2 and ln(σ/σ∗ ) + 2(σ2∗/2σ2 − 1 ) < . The well known bound ln(x ) ≤ x − 1 for all x > 0 can be rearranged to give ln(x ) ≥ 1 − 1 x and σ2 −1 ) = ( σ−σ∗)2 2( σ2∗ so > ln( σ √ . 2σ2 Thus , it is also the case that |σ − σ∗| < σ 2 . Combining all these bounds proves the result .
σ2 −1 ) ≥ 1− σ∗
2σ2 − 1
σ
σ∗ )+ 1
σ + 1
2( σ2∗
σ∗
√
1
σ covariance function kz , parameterized by θz . The locations ¯X = {¯x1 , ¯x2 , . . . , ¯xl} for the ” training ” noise levels ¯Z = {¯z1 , ¯z2 , . . . , ¯zl} can be chosen differently from the ones used for the noise free process .
Since the noise rates zi at locations x1 , x2 , . . . , xm are now independent latent variables in the combined regression model , the predictive distribution for y∗ changes to p(y∗|x∗,D , θ ) = p ( y∗|x∗ , z∗,D , Z , θ ) · p,z∗ , Z|x∗ , X , ¯X , ¯Z , θz dz∗dZ , where Z denotes the pre the original dicted ( logarithmized ) noise levels at the original locations X . Given ( z∗ , Z ) , the prediction p ( y∗|x∗ , z∗,D , Z ) is Gaussian with mean and variance as defined by ( 2 ) and ( 3 ) n × I with the diagonal replacing the constant noise level σ2 matrix diag(exp(Z) ) . The problematic term is p(z∗ , Z|x∗ , X , ¯X , ¯Z ) . It makes the integral difficult to handle analytically . Instead , we seek for the solution using the most probable noise estimates , ie , p(y∗|x∗,D , θ ) ≈ p ( y∗|x∗ , z∗ , X , Y , Z ) where ( z∗ , Z ) are the mean predictions of the latent noise process . To jointly estimate θ , θz , and ¯Z from data , we seek an MAP solution that maximizes log p(Z|y , X , ¯X , θ ) = log p(y|X , Z , θ ) + log p(Z|X , ¯X , θ ) + const . , where ( overloading notation ) θ now also includes θz and ¯Z . One may now find the gradient of this objective function with respect to the hyperparameters θ and employ it within a gradient based optimization to find the corresponding solution .
VI . REGRET TRANSFORM BOUND
Our approach to solving the quantile estimation problem can be thought of as a reduction . The problem we really wish to solve is quantile estimation ( Problem A ) but the problem we actually solve is a Gaussian Process regression ( Problem B ) and then we use this to get a quantile estimation . A natural question in this sort of reductions is : given a measure of how well we solve Problem B , what can we say about how well will we solve Problem A ?
Questions like this are typically answered in terms of regrets . The regret of a τ th quantile point estimate q under a true point distribution p(y|x ) is ∆Rτ ( q ) := Rτ ( q ) − Rτ ( qopt ) , where qopt is the best τ th quantile estimate in ( 1 ) under p . Theorem 5 : Suppose p∗ = N ( µ∗ , σ2∗ ) is a predictive distribution at the point x∗ and the true point distribution is p = N ( µ , σ2 ) . Then , if KL(p∗||p ) ≤ , the regret of the corresponding τ th quantile estimator q∗
∆Rτ ( q ) ≤
√
τ satisfies 2 ( τ σ + 1)(|Φ−1(τ)| + 1 ) .
( 6 )
VII . EXPERIMENTAL EVALUATION
√
This bound depends not only on how well the true normal term ) but also on the distribution can be estimated ( the quantile being estimated ( τ ) and the variance of the true distribution ( σ ) . These dependencies are quite natural . If the true distribution is spread out a small error in estimating it can lead to large errors in its quantiles . Also , since there is little mass in the highest and lowest quantiles , an error in
Our intention here is to investigate to which extent the performance of the Quantile GP is comparable to state ofthe art quantile estimation approaches .
A . Synthetic Data
In this experiment , we are interested to analyze the quality of our estimator under the condition of known noise
0.1
0.25
QSVM 0.0822 HQGP 0.0621
0.0641 0.0410
QSVM 0.0987 HQGP 0.5286
0.2465 0.2938
τ 0.5
Example 1
0.0274 0.0306
Example 2
0.5090 0.2398
0.75
0.9
0.0238 0.0379
0.0937 0.0563
0.8044 0.4793
0.9393 0.9927
Table I
ABSOLUTE LOSS COMPARISON ON EXAMPLE 1 AND EXAMPLE 2 . QSVM : QUANTILE SVM , [ 10 ] ; HQGP : HETEROSCEDASTIC QUANTILE
GP , IE OUR APPROACH . distribution . We focus on two cases , namely the Gaussian noise case and the Chi squared noise case :
4
2.1−x
, and noise , ξ ∼ χ2
ξ ( τ ) with Φξ( . ) is given as ΦN ( 0,1)( . ) or Φχ2
Example 1 ( Heteroscedastic Gaussian Noise ) We generate 100 samples from the following stochastic process : x ∼ U(−1 , 1 ) and y = µ(x ) + σ(x)ξ with µ(x ) = sinc(x ) , σ(x ) = 0.1 exp(1 − x ) , and noise , ξ ∼ N ( 0 , 1 ) . Example 2 ( Heteroscedastic Chi squared Noise ) We generate 200 samples from the following stochastic process : x ∼ U(0 , 2 ) and y = µ(x ) + σ(x)ξ with ( 1 ) − 2 . µ(x ) = sin(2πx ) , σ(x ) = For the case of known noise distribution , the true quantile values can be computed simply via inverse cumulative = µ(x ) + distribution function of noise density , ie qtrue ( .)−2 σ(x)Φ−1 for Example 1 or 2 , respectively . The absolute errors for each estimated quantile regression functions are given in Table I . For comparison , we contrast the performance of our method with SVM based quantile estimator [ 10 ] . It is of no surprise that our estimator shows superior performance in Gaussian noise corrupted data while falls short in Chisquared noise model case . In the latter case , our estimator tries to approximate a single right tail Chi squared density with a double tail Gaussian density and thus the produced estimates suffer badly in the lower quantile regime . However , in the real data , the noise model miss specification is less apparent and as it is shown in the next section our quantile estimator delivers competitive performance with state ofthe art approaches . This is partly due to the competitive advantage of Gaussian processes based estimator to be a superior mean predictor .
( 1 )
τ
B . Real Data
We are interested to assess the effectiveness of our quantile estimator in comparison to the linear model by [ 2 ] , the SVM based approach by [ 10 ] , and the learning reduction based approach by [ 6 ] for several real datasets . We implemented our approach in Matlab using [ 7 ] GPML Toolbox .
0.1
Dataset Antigen
Mcycle
A 0293±0105 0123±0033 B 0122±0031 C D 0116±0021 Weather A 0291±0034 0067±0015 B 0075±0011 C D 0057±0010 A 0396±0080 0090±0012 B 0094±0011 C D 0092±0025 0079±0019 E A 0328±0028 0122±0017 B 0121±0020 C D 0135±0014 0123±0017 E A 0283±0038 B 0108±0014 C 0104±0016 F
Calif . Housing
BMD
†
τ 0.5
0264±0050 0249±0033 0266±0021 0255±0028 0293±0024 0218±0034 0176±0028 0097±0015 0389±0019 0202±0019 0190±0015 0186±0018 0187±0021 0325±00340 0306±0039 0311±0041 0310±0045 0309±0045 0225±0009 0263±0023 0272±0018
†
0.9
0292±0087 0128±0018 0131±0015 0126±0015 0301±0045 0118±0013 0123±0011 0068±0017 0387±0056 0085±0008 0083±0010 0089±0010 0070±0016 0324±0073 0152±0025 0154±0027 0168±0030 0153±0027 0254±0068 0167±0006 0175±0024
†
Table II
PINBALL LOSS COMPARISON : 5 FOLD CROSS VALIDATION ERRORS ± STD . THE BEST RESULT IS IN BOLDFACE . A : LINEAR , [ 2 ] ; B : QUANTILE SVM , [ 10 ] ; C : REDUCTION TO CLASSIFICATIONS , [ 6 ] ; D : QUANTILE GP ; E : HETEROSCEDASTIC QUANTILE GP ; F : SPARSE QUANTILE GP . † :
PROGRAM FAILS ( LARGE DATASET ) .
Datasets : We used three regression datasets from the UCI repository ( Antigen 972 , Weather 238 and Motorcycle 133 ) ; one dataset from the Elements of Statistical Learning Book ( BMD 485 ) ; and one dataset from StatLib repository ( California Housing 20640 ) . We normalized all datasets to have zero mean and unit standard deviation for each coordinate [ 10 ] .
Model Selection :
In our approach , we use squared exponential covariance function . Gaussian RBF Kernel is used for Quantile SVM with the kernel width and regularization parameters fitted with the trick described in [ 10 ] . For learning the reduction method , Expectation Propagation ( EP ) approximation of Gaussian process classification [ 7 ] with squared exponential covariance function is used as base classifier learners . We fix the number of classifiers at 100 for all datasets [ 6 ] . For the linear model , there is no parameter to be tuned .
Sparse Approximation : As stated in Section IV , our estimator can be easily sparsified . This relies on advances of sparse approximation methods for conditional mean Gaussian process regression . In our experiments , we use the projected process ( PP ) [ 7 ] . We assess the performance of
2Number of observations .
( a ) Takeuchi et al .
( b ) Langford et al .
( c ) Quantile GP
( d ) Heteroscedastic QGP
Illustration of conditional quantile analysis for Silverman ’s motorcycle dataset via Quantile SVM ( Takeuchi et al. ) , Reduction ( Langford et al. ) , Figure 2 . Quantile GP , and Heteroscedastic QGP . The dataset exhibits heteroscedasticity . Non crossing constraint is not enforced in Reduction and enforceable in Quantile SVM via additional linear constraints and is an inherent property of Quantile GP . this sparse approximation in the California Housing dataset . Input dependent noise : We optimized the hyperparameters of both the noise free and the noise process jointly using a scaled conjugate gradient approach . As locations ¯X of the latent noise process , we selected 10 points linearly spaced in the bounding interval of the given input location .
Results : We run the linear model , SVM , learning reduction , and Gaussian process methods for 3 different quantile values for each dataset , ie at 0.1 , 0.5 and 09 For Quantile SVM , we perform nested 5 fold cross validation . There is no need to perform nested cross validation for our approach , learning reduction approach , and the linear model as the free parameters are selected via log evidence , EP approximation of log evidence or there is no free parameter , respectively . The 5 fold cross validation results are summarized in Table II . Arguably , our estimator performs on par with ( if not exceeding ) state of the art SVM and learning reduction based method . Noticeably , our approach has a competitive advantage over Quantile SVM for large datasets where the later approach might fail due to high memory and computational time requirements .
An illustration of the estimated quantile regression functions via SVM , learning reduction , and Gaussian process methods on the Motorcycle dataset is given in Figure 2 .
VIII . CONCLUSIONS AND FUTURE RESEARCH
We tackled the quantile estimation problem by modeling the conditional distribution and subsequently slicing the distribution at the respective quantile level to get the estimate of the latent quantile function . This approach is preferable when multiple quantile regression functions are needed and captures rather well the characteristics of the datasets .
In this paper , we have focussed on the specific example of Bayes quantile estimator , ie with Gaussian likelihood function . While this offers several appealing properties , the framework is by no means restricted to this . Our proposed Bayes quantile estimator offers two design parameters : likelihood function ( for robustness , a heavier tail distribution function might be more preferable ) and non parametric CDF ( estimation of CDF directly from the data via residuals / errors ) .
ACKNOWLEDGMENT
The authors would like to thank Alex Smola , Marcus Hutter , and Marconi Barbosa for useful comments and suggestions . NICTA is funded through the Australian Government ’s Backing Australia ’s Ability initiative , in part through the ARC . The research was supported by the Pascal Network and the Fraunhofer ATTRACT fellowship STREAM .
REFERENCES
[ 1 ] C . Perlich , S . Rosset , R . D . Lawrence , and B . Zadrozny , “ High quantile modeling for customer wallet estimation and other applications , ” in KDD ’07 . ACM , 2007 , pp . 977–985 .
[ 2 ] R . Koenker and G . Bassett , “ Regression quantiles , ” Econo metrica , vol . 46 , no . 1 , pp . 33–50 , 1978 .
[ 3 ] V . Vapnik , Estimation of Dependences Based on Empirical
Data . Springer , 1982 .
[ 4 ] C . Rasmussen , “ Evaluation of Gaussian processes and other methods for non linear regression , ” PhD dissertation , Department of Computer Science , University of Toronto , 1996 , http://wwwkybmpgde/publications/pss/ps2304ps
[ 5 ] H . Nickisch and C . E . Rasmussen , “ Approximations for binary gaussian process clasification , ” JMLR , vol . 9 , pp . 2035– 2078 , 2008 .
[ 6 ] J . Langford , R . Oliveira , and B . Zadrozny , “ Predicting conditional quantiles via reduction to classification , ” in UAI , 2006 .
[ 7 ] C . E . Rasmussen and C . K . I . Williams , Gaussian Processes for Machine Learning . Cambridge , MA : MIT Press , 2006 .
[ 8 ] R . Koenker , Quantile Regression .
Press , 2005 .
Cambridge University
[ 9 ] X . He , “ Quantile curves without crossing , ” The American
Statistician , vol . 51 , no . 2 , pp . 186–192 , may 1997 .
[ 10 ] I . Takeuchi , Q . V . Le , T . Sears , and A . J . Smola , “ Nonparametric quantile estimation , ” J . Mach . Learn . Res . , vol . 7 , 2006 .
[ 11 ] K . Yu and R . A . Moyeed , “ Bayesian quantile regression , ” Statistics & Probability Letters , vol . 54 , pp . 437–447 , 2001 .
[ 12 ] J . Shim , C . Hwang , and K . H . Seok , “ Non crossing quantile regression via doubly penalized kernel machine , ” in Computational Statistics , vol . 24 , 2009 , pp . 83–94 .
