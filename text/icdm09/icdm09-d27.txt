2009 Ninth IEEE International Conference on Data Mining
Two Heads Better Than One : Metric+Active Learning and Its Applications for IT
Service Classification
Fei Wang1 , Jimeng Sun2 , Tao Li1 , Nikos Anerousis2
1School of Computing and Information Sciences , Florida International University , Miami , FL 33199
2IBM T . J . Watson Research Center , Service Department , Yorktown Heights , NY 10598 0218
Abstract—Large IT service providers track service requests is and their execution through problem/change tickets . It important to classify the tickets based on the problem/change description in order to understand service quality and to optimize service processes . However , two challenges exist in solving this classification problem : 1 ) ticket descriptions from different classes are of highly diverse characteristics , which invalidates most standard distance metrics ; 2 ) it is very expensive to obtain high quality labeled data .
To address these challenges , we develop two seemingly independent methods 1 ) Discriminative Neighborhood Metric Learning ( DNML ) and 2 ) Active Learning with Median Selection ( ALMS ) , both of which are , however , based on the same core technique : iterated representative selection . A case study on real IT service classification application is presented to demonstrate the effectiveness and efficiency of our proposed methods .
I . INTRODUCTION
In IT outsourcing environments , every day thousands of problem and change requests are generated on a diverse set of issues related to all kinds of software and hardware . Those requests need to be resolved correctly and quickly in order to meet service level agreements ( SLAs ) . Under this environment , when devices fail or applications need to be upgraded , the person recovering such failures or applying the patch is likely to be sitting thousand miles from the affected component . The reliance on outsourcing of technology support has fundamentally shift the dependencies between participating organizations .
In such a complex environment , all service requests are handled and tracked through tickets by various problem & change management systems . A ticket is opened with certain symptom description , and routed to appropriate Service Matter Experts ( SMEs ) for resolution . The solution is documented when the ticket is closed . The job of SMEs is to resolve tickets quickly and correctly in order to meet SLAs . Another important job role is Quality Analysts ( QAs ) , whose responsibility is to analyze recent tickets in order to identify the opportunity for the service quality improvement . For example , a frequent password resets on one system may be due to the inconsistent password period in that system . Instead of resetting the password all the times , the password period should be adjusted properly . Or sometimes one patch or fix for a particular server should also be applied to
1550 4786/09 $26.00 © 2009 IEEE DOI 101109/ICDM2009103
1022 all servers with the same configuration , instead of creating multiple tickets with exactly the same work order on each of those systems .
It requires great understanding of the current ticket distribution for QAs to identify any opportunity for optimization . In another word , it is important to classify those tickets based on their description and resolution accurately in a timely manner . In this paper , we address the ticket classification problem that lies in the core of IT service quality improvement with significant practical impact and great challenges : • Standard distance metrics do not apply due to diverse characteristics of raw features . More specifically , the ticket description and solution are highly diverse and noisy . Since different SMEs can describe the same problem quite differently and the descriptions are typically short and noisy . Also depending on the types of problems , the description can vary significantly .
• There are almost no high quality labeled data . Tickets are handled by SMEs , who often do not have the incentive or ability to classify the tickets accurately , due to heavy workload and incomplete information . On the other hand , QAs , who have the right ability and incentive , do not have cycle to manually label all the tickets .
To address these two challenges , we propose a novel hybrid approach that leverage both active learning and metric learning . The contributions of this paper are the following : • We propose Discriminative Neighborhood Metric Learning ( DNML ) that learns a domain specific distance metric using the overall data distribution and a small set of labeled data .
• We propose Active Learning with Median Selection ( ALMS ) that progressively selects the representative data points that need to be labeled , which is naturally a multi class algorithm .
• We combine metric and active learning steps into a unified process over the data . Moreover , our algorithm can automatically detect the number of classes contained in the data set .
• We demonstrate the effectiveness and efficiency of DNML and ALMS over several datasets compared to several existing methods . because of its homogeneous assumption of all the feature dimensions . Therefore many researchers propose to learn a Mahalanobis distance which measures the distance between data xi and xj by
. dm(xi , x ) =
( xi − xj )fiC(xi − xj )
( 2 ) where C ∈ Rd×d is a positive semi definite covariance matrix used to incorporate the correlations of different feature dimensions . In this paper , we consider to learn a low rank covariance matrix C , such that with the learned distance metric , the within class compactness and betweenclass scatterness are maximized .
Different from linear discriminant analysis [ 1 ] , which seeks for a discriminant subspace in a global sense , our algorithm aims to learn a distance metric with enhanced local discriminability . To define such local discriminability , we first introduce the definition of two types of neighborhoods [ 8 ] :
Definition 1 : Homogeneous Neighborhood . The homoi , is the |N o i |i | is the geneous neighborhood of xi , denoted as N o nearest data points of xi with the same label , |N o size of N o i .
Definition 2 : Heterogeneous Neighborhood . The heti , is the |N e i |i | is the erogeneous neighborhood of xi , denoted as N e nearest data points of xi with different labels , |N e size of N e i . Based on the above two definitions , we can define the local compactness of point xi as fi fi
( 3 )
( 4 )
Ci = m(xi , xj ) d2 j:xj ∈N o i and the local scatter ness of point xi as
Si = m(xi , xk ) d2 k:xk ∈N e i
Then the local discriminability of the data set X with respect to the distance metric dm can be defined as
' '
=
' i Ci' i Si
J = j:xj ∈N o i k:xk ∈N e i
( xi − xj)fiC(xi − xj ) ( xi − xk)fiC(xi − xk )
( 5 )
The goal of our algorithm is to minimize J , which is equivalent to minimize the local compactness and maximize the local scatterness simultaneously . Fig 2 provides an intuitive graphical illustration of the theme behind our algorithm .
However , the minimization of J in Eq ( 5 ) to get an optimal C is not an easy task as there are d(d + 1)/2 variables to solve provided that C is symmetric . Recall that we require C to be a low rank matrix and C is positive semi definite , then by incomplete Choleskey factorization , we can decompose C to
C = WW
( 6 ) where W ∈ Rd×r and r is the rank of C . In this way , we only need to solve W instead of solving the entire C . fi
Figure 1 . the labeled points , and the gray blobs correspond to unlabeled points .
The basic algorithm flowchart , where the red blobs represent
The rest of the paper is organized as follows : Section II presents the methods for metric and active learning . Section III demonstrates the practical case study of the proposed methods on IT ticket classification application . Finally , Section V concludes .
II . METRIC+ACTIVE LEARNING
In this section we will introduce our algorithm in detail .
First we give an overview of our algorithm . A . Algorithm Overview
The basic procedure of our algorithm is to iterate the following procedure :
• Learn a distance metric from the labeled data set , and then classify the unlabeled data points using the nearest neighbor classifier with the learned metric . We call the data points in the same class a cluster .
• Select the median from each cluster . For each cluster ( whose labels Xi , we partition it into a labeled set X L are given initially ) and an unlabeled set X U ( whose labels are predicted by the nearest neighbor classifier ) . Then the median for Xi is defined as i i mi = arg min x∈X U i
.x − ci.2
( 1 ) where ci is the mean of Xi .
• Add the selected points into the labeled set . Fig 1 shows the graphical view of the basic algorithm flowchart . B . Discriminative Neighborhood Metric Learning
As we know that a good distance metric plays a central role in many data mining and machine learning algorithms ( eg , Nearest Neighbor classifier , k means algorithm ) . Usually the Euclidean distance cannot satisfy our requirements
1023 the data set . Despite the theoretical soundness and empirical success of TED , there are still some limitations :
• Although the name suggests TED is transductive , it does make use of any label information contained in the data set . In fact , TED just uses the whole data set ( include labeled and unlabeled ) to select the k most representative data , such that the linear reconstruction loss of the whole data set using the selected points are minimized . In this sense , TED is an unsupervised method .
• As the authors analyzed in [ 11 ] , TED tends to select the data points with large norms ( where the authors analyzed that these points are hard to predict ) . However , these selected points lie on the border of the data distribution area , and those data points could be outliers that would mislead the classification process .
Based on the above analysis , we propose to ( 1 ) make use of the label information ; ( 2 ) select the representative points locally . Specifically , we first learn a distance metric using our DNML method introduced in last section and then apply the nearest neighbor classifier to classify the unlabeled points . In this way , the whole data set is partitioned into several classes , and for each class , we just select the median point as defined in Eq ( 1 ) .
4
3
2
1
0
−1
−2
−3 −4 class1 class2 class3 class4 selected
−2
0
2
4
4
3
2
1
0
−1
−2
−3 −4 class1 class2 class3 class4 selected
−2
0
2
4
( a ) TED
( b ) Local Median
Figure 3 . Active learning results . ( a ) shows the results of transductive experimental design ; ( b ) shows the results of our local median selection method .
Fig 3 illustrates a toy example on the difference between TED and our local median selection method . The data set here is generated from 4 Gaussians , and each Gaussian contains 100 data points . We treat each Gaussian as a class . Initially we randomly label 10 % of the data points and use TED to select 4 most representative data points shown as black triangles in Fig 3(a ) , from which we can see that these points all lie on the border of these Gaussians . Fig 3(b ) shows the results of our local median selection method , where we first apply DNML to learn a proper distance metric from the labeled points and then use such metric to classify the whole data set , and finally we select one median from each class . From the figure we observe that the selected points are representative for each Gaussian .
An issue that is worth mentioning here is that our algorithm in fact can be viewed as an approximated version of
( a ) The neighborhoods of xi with Euclidean distance
( b ) The neighborhoods of xi with the learned distance
Figure 2 . The homogeneous and heterogeneous neighborhoods of xi with the regular Euclidean and learned Mahalanobis distance metrics . The blob with dark green in the middle of the circle is xi , and the blobs with green and blue correspond to the points in N o i . The goal of DNML is to learn a distance metric that pulls the points in N o towards xi while push the points in N e i away from xi . i and N e i
DECOMPOSED NEWTOWN’S PROCEDURE FOR SOLVING THE TRACE
Table I
RATIO PROBLEM
Input : Matrices MC and MS , precision ε , dimension d Output : Trace Ratio value λ∗ and matrix W∗ Procedure :
1 . Initialize λ0 = 0 , t = 0 2 . Do eigenvalue decomposition to MC − λtMS 3 . Let ( βk(λ ) , wk(λ ) ) be the k th eigenvalue , eigenvector pair obtained from step 2 , define the first order Taylor expansion ˆβk(λ ) = βk(λ ) + βfi ( λt)(λ − λt ) , where ( λt ) = wk(λt)'MS wk(λt ) βfi k
4 . Define ˆfd(λ ) to be the sum of the smallest d ˆβk(λ ) , k solve ˆdd(λ ) = 0 and set the root to be λt+1
5 . If |λt+1 − λt| < , go to step 6 ; otherwise t = t + 1 , go to step 2
6 . Output λ∗ = λ , and W∗ to be the eigenvectors wrt the smallest d eigenvalues of MC − MS
Combining Eq ( 6 ) and Eq ( 5 ) , we can derive the following optimization problem minW tr(W tr(W fi fi
MCW ) MSW )
( 7 ) where
MC =
MS = fi fi fi fi i i
( xi − xj)(xi − xj)fi
( 8 ) j:xj ∈N o i
( xi − xk)(xi − xk)fi ( 9 ) k:xk ∈N e i are the compactness and scatterness matrices respectively . Therefore , our problem ( 7 ) becomes a trace quotient minimization problem , and we can make use of the decomposed Newtown ’s method [ 3 ] .
C . Active Learning with Median Selection
Recently a novel active learning method called Transductive Experimental Design ( TED ) [ 11 ] is proposed , which aims to select the k most representative points contained in
1024
THE METRIC+ACTIVE LEARNING ALGORITHM
Table II
Inputs : Training data , |N o
| , precision , dimension d ,
| , |N e i i number of iteration steps T
Outputs : The selected points and learned W∗ Procedure : for t=1:T
1 . Construct MS , MC from the training data 2 . Learn a proper distance metric 3 . Count the number of classes k in the training data , apply the learned metric to classify the unlabeled data using the nearest neighbor classifier
4 . Select the median in each class and add them into the training data pool end local TED , where we first partition the data set into several local regions using the learned distance metric , and then select exactly ONE representative point in each region . As data mean is the most representative point for a set of data in the sense of Euclidean loss , we select the median which is closest to the data mean from the candidate set .
The whole algorithm procedure is summarized in Table
II .
III . TICKET CLASSIFICATION : A CASE STUDY
In this section we present the detailed experimental results on applying our proposed active learning scheme for ticket classification . First we will describe the basic characteristics of the data set .
A . The Data Set
There are totally 4182 tickets from 27 classes . We use the bag of words features , which results in a 3882 dimensional space . After eliminating the duplicate and null tickets , there are 2222 tickets remained . The class distribution is shown in Fig 4(a ) , from which we can observe that the classes are highly imbalanced and there are many rare classes with only a few data points . We identify a class as “ rare class ” if and only if the number of data points contained in it is less than 20 . In our experiments , we eliminate those “ rare classes ” , which results in a data set of size 2161 from 15 classes , and the class distribution is shown in Fig 4(b ) .
Besides rare classes , we also observe that the data set is highly sparse and there are also a set of “ rare features ” . The original feature distribution is shown in Fig 5(a ) , where we accumulated the times that each feature appears in each class . We identify a feature as a “ rare feature ” if and only if its total appearance times in the data set is less than 10 . After eliminating those rare features , we obtain a data set with 669 features and its distribution is shown in Fig 5(b ) . Finally , we also eliminated the data with only these rare features , which makes the final data set containing 2130 tickets with 669 features .
1025 s t i n o p t a a d f o r e b m u n
600
500
400
300
200
100
0 0
5
10
15 class index
20
25
30
( a ) Original data s t i n o p t a a d f o r e b m u n
600
500
400
300
200
100
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class index
( b ) Data distribution after eliminating the rare classes
Figure 4 . Class distribution of original data and the data with no rare classes x e d n i s s a c l
5
10
15
500
1000
1500
2000 feature index
2500
3000
3500
( a ) Original data feature distribution x e d n i s s a c l
5
10
15
100
200
300 feature index
400
500
600
( b ) Data feature distribution with no rare features
60
40
20
60
40
20
Figure 5 . Feature distribution of the original data and the data with rare feature eliminated .
B . Distance Metric Learning
In this part of experiments , we first test the effectiveness of our DNML algorithm on the ticket data set , where we use our algorithm to obtain a distance metric , and then use such distance metric to perform nearest neighbor classification and get the final classification accuracy . Such procedure is repeated 50 times and we report the average classification accuracies and standard deviations as in Fig 6 . The size of homogeneous and heterogeneous neighborhoods are set to 3 manually , and the rank of the covariance matrix C is set to 400 . From the figure we observe the superiority of our metric learning method . Specifically , with the learned metric , our DNML method clearly outperforms the original NN method , which validates that DNML can learn a better distance metric .
As there are two sets of parameters in our DNML method , one is the rank r of the covariance matrix C , the other is the sizes of the homogeneous neighborhood No and heterogeneous neighborhood Ne ( denoted as no and ne ) . Therefore we also conducted a set of experiments to test the sensitivity of DNML with respect to those parameters .
Fig 7 shows how the algorithm performance varies with respect to the rank of the covariance matrix C , where we randomly label half of the tickets as the training set , and the remaining tickets are used for testing . We set the sizes of No and Ne to be 3 . The results in Fig 7 are summarized over 50 independent runs . From the figure we can see that
0.95
0.9
0.85 y c a r u c c a
DNML NN NB RLS SVM n o i t a c i f i s s a c l
0.8
0.75
0.7
20
30 60 percentage of labeled tickets ( % )
40
50 i | and not that sensitive with respect to the variation of |N o i | . From Fig 8 we can see that when the neighborhood |N e sizes are small , the algorithm performance is better than that when the neighborhood sizes are large . This is possibly because the distribution of the data set is complicated and data in different classes are highly overlapped , then when we enlarge the neighborhoods to include more data points , the learned distance metric might be corrupted by some noisy points , which will make the final classification results inaccurate .
70
Figure 6 . Classification accuracy comparison with different supervised learning methods . The x axis represents the percentage of randomly labeled tickets , and the y axis denotes the averaged classification accuracy . the final ticket classification results are stable with respect to the choice of the rank of the covariance matrix , except the cases when the rank is too small ( ie , ¡10 in our case ) , since in those cases too much information will be lost . When the rank becomes too large , some noise contained in the data set could be retained , therefore the performance of our algorithm would go down a little , and the choices of r ∈ [ 200 , 400 ] are all reasonable . y c a r u c c a n o i t a c i f i s s a c l
0.87
0.86
0.85
0.84
0.83
0.82
0.81
0.8
0.79 0
100
200
500 rank of the covariance matrix
300
400
600
Figure 7 . The sensitivity of the performance of our algorithm with respect | = 3 , and to the rank r of the covariance matrix C . We set |N o half of the data set are labeled as the training data .
| = |N e i i i and N e i and N e
We also test the sensitivity of our algorithm with respect i , the results of to the choices of the sizes of N o which are shown in Fig 8 , where the x axis and y axis i , and the z axis denotes correspond to the sizes of N o the classification accuracy averaged over 50 independent runs . Here we assume that the sizes homogeneous and heterogeneous neighborhoods are the same for all the data points . For each run , we randomly label 50 % of the tickets as training data , and the rest as testing data . From Fig 8 we can clearly see that the whole surface of z = f ( x , y ) is flat , which means that the performance of our algorithm is y c a r u c c A
0.85
0.84
0.83 0
5 e| |Ni
10
15
15
0
5 o| |Ni
10
Figure 8 . The sensitivity of the performance of our algorithm with respect to the choices of |N o | , and half of the data set are labeled as the training data .
| and |N e i i
C . Integrated Active Learning and Distance Metric Learning
In our implementation , we initially label 20 % of the data set , and then apply the various active learning methods . Since there are totally 15 classes in the ticket data set , for each method , we select 15 points from the unlabeled set in each round . For all the approaches that use DNML , we set |No| = |Ne| = 3 , and the rank of the covariance matrix is set to 400 . Fig 9 illustrates the results of these algorithms summarized over 50 independent runs , where the x axis represents the percentage of selected points , and the y axis denotes the averaged classification accuracy as well as the standard deviation . From the figure we can clearly see that with our DNML+LMED method , the classification accuracy will ascend faster compared to other methods .
IV . RELATED WORKS
In this section we will briefly review some previous works that are closely related to our metric+active learning method . A . Distance Metric Learning
Distance metric learning plays a central role in real world applications . According to [ 10 ] , these approaches can mainly be categorized into two classes : unsupervised and supervised . Here we mainly review the supervised methods , which learn distance metric from the data set with some supervised information . Usually the information takes the
1026 y c a r u c c a n o i t a c i f i s s a c l
1
0.9
0.8
0.7
0.6
0.5
0.4 0
5
DNML+LMED LMED DNML+Rand DNML+TED 35
10
30 percentage of actively labeled tickets ( % )
15
20
25
Figure 9 . The classification accuracy vs . number of selected tickets plot . The x axis represents the percentage of labeled tickets , and the y axis represents the classification accuracy averaged over 50 independent runs . form of pairwise constraints , which indicating whether a pair of data points belong to the same class ( usually referred to as must link constraints ) or different classes ( cannotlink ) constraints . Then these algorithms aim to learn a proper distance metric under which the data with must link constraints are as compact as possible , while the data with cannot link constraints are far apart from each other . Some typical approaches include the side information method [ 9 ] , Relevant Component Analysis ( RCA ) [ 5 ] , and Discriminant Component Analysis ( DCA ) [ 2 ] . Our Discriminant Neighborhood Metric Learning ( DNML ) method can also be viewed as a supervised method , however , we make use of the labeled data together with their labels , which is different from those pairwise constraints .
B . Active Learning
In many real world problems , we face the problems when unlabeled data are abundant but labeling data is expensive to obtain ( eg , in text classification , it is expensive and time consuming to ask the users to label the documents manually , however , it is quite easy to obtain a large amount of unlabeled documents by crawling the web ) . In such a scenario the learning algorithm can actively query the user/teacher for labels . This type of iterative supervised learning is called active learning . Since the learner chooses the examples , the number of examples to learn a concept can often be much lower than the number required in normal supervised learning .
Two classical active learning algorithms are Tong and Koller ’s Simple SVM algorithm [ 6 ] and Seung et al . ’s Query By Committee ( QBC ) algorithm [ 4 ] . However , the simple SVM algorithm is coupled with the Support Vector Machine ( SVM ) classifier [ 7 ] and is only applicable to two class problems . For the QBC algorithm , we need to construct a committee of models that represent different regions of the version space and have some measure of disagreement among committee members , which is usually difficult for real world applications . Recently , Yu et al . [ 11 ] proposed
1027 another active learning algorithm called Transductive Experimental Design ( TED ) , which aims to find some most representative points that can optimally reconstruct the whole data set in the sense of Euclidean sense . Our median selection strategy introduced in this paper is similar to TED , and we have analyzed in section II C the superiority of our algorithm .
V . CONCLUSIONS
We present a novel metric+active learning method for IT service ticket classification in this paper . Our method combines both the strengths of metric learning and active learning methods . Finally the experimental results on both benchmark and real ticket data sets are presented to demonstrate the effectiveness of the proposed method .
ACKNOWLEDGEMENT
The work is partially supported by NSF CAREER Award
IIS 0546280 and a 2008 IBM Faculty Award .
REFERENCES
[ 1 ] K . Fukunaga . Introduction to Statistical Pattern Recognition .
Academic Press , San Diego , California , 1990 .
[ 2 ] S . Hoi , W . Liu , M . Lyu , and W . Ma . Learning distance metrics with contextual constraints for image retrieval . In Proceedings of CVPR2006 , 2006 .
[ 3 ] Y . Jia , F . Nie , and C . Zhang . Trace ratio problem revisited .
IEEE Transactions on Neural Networks , 2009 .
[ 4 ] H . S . Seung , M . Opper , and H . Sompolinsky . Query by committee . In Proceedings of COLT , pages 287–294 , 1992 .
[ 5 ] N . Shental , T . Hertz , D . Weinshall , and M . Pavel . Adjustment learning and relevant component analysis . In Proceedings of ECCV , pages 776–790 , 2002 .
[ 6 ] S . Tong and D . Koller . Support vector machine active learning with applications to text classification . Journal of Machine Learning Research , 2:45–66 , 2001 .
[ 7 ] V . N . Vapnik .
The nature of statistical learning theory . Springer Verlag New York , Inc . , New York , NY , USA , 1995 .
[ 8 ] F . Wang and C . Zhang . Feature extraction by maximizing the neighborhood margin . In Proceedings of CVPR , 2007 .
[ 9 ] E . P . Xing , A . Y . Ng , M . I . Jordan , and S . Russell . Distance metric learning , with application to clustering with sideinformation . In Advances in Neural Information Processing Systems , volume 15 , pages 505–512 , 2003 .
[ 10 ] L . Yang . Distance metric learning : A comprehensive survey .
Technical report , Michgan State University , 2006 .
[ 11 ] K . Yu , J . Bi , and V . Tresp . Active learning via transductive experimental design . In Proceedings of ICML , pages 1081– 1088 , 2006 .
