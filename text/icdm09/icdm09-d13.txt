Audio Classification of Bird Species : a Statistical Manifold Approach
Forrest Briggs , Raviv Raich , and Xiaoli Z . Fern
School of EECS , Oregon State University , Corvallis , OR 97331 5501 {briggsf , raich , xfern}@eecsoregonstateedu
Abstract
Our goal is to automatically identify which species of bird is present in an audio recording using supervised learning . Devising effective algorithms for bird species classification is a preliminary step toward extracting useful ecological data from recordings collected in the field . We propose a probabilistic model for audio features within a short interval of time , then derive its Bayes risk minimizing classifier , and show that it is closely approximated by a nearest neighbor classifier using Kullback Leibler divergence to compare histograms of features . We note that feature histograms can be viewed as points on a statistical manifold , and KL divergence approximates geodesic distances defined by the Fisher information metric on such manifolds . Motivated by this fact , we propose the use of another approximation to the Fisher information metric , namely the Hellinger metric . The proposed classifiers achieve over 90 % accuracy on a data set containing six species of bird , and outperform support vector machines .
1 Introduction
Our goal is to develop algorithms that can predict which species of bird is present in an audio recording , by learning from a collection of labeled examples . Such algorithms will serve as part of a system to automatically collect bird species presence/absence data , which will provide valuable ecological information for species distribution modeling and conservation planning . Existing bird species distribution data are collected by manual surveys , which are labor intensive , and require observers trained in bird recognition [ 2 ] . Automated bird population surveys could provide vast amounts of useful data for species distribution modeling , while requiring less effort and expense than human surveys . Other applications of classifying bird sounds include reducing plane crashes caused by collisions with birds [ 5 ] , and audio classification in general .
Sounds that birds make have a grammatical structure ; two important levels of organization in this structure are songs and syllables . Syllables are single distinct utterances by a bird and serve as the basic building blocks of bird song
Figure 1 . The spectrogram for a one second portion of a recording of a Swainson ’s Thrush . Darker areas indicate higher energy at the corresponding frequency .
[ 3 ] . A song consists of a series of syllables arranged in a particular pattern . In this study , our goal is to classify bird species from an interval of sound ( containing one or more syllables ) , which roughly corresponds to the song level of organization .
Audio classification systems typically begin by extracting acoustic features from audio signals . Such features often pertain to individual frames ( ie , very short segments of signal ) . For example , one commonly used feature is the spectrum of a signal frame , which describes the intensity of ( a short segment of ) the signal as a function of frequency . To apply many standard algorithms for classification , it is necessary to represent a sound , which contains multiple frames , using a fixed length vector . To construct such a fixed length feature vector to describe a sound as a whole , a common approach is to first identify interesting frames by segmentation , compute features for those frames , then take the average of the features over all frames [ 13 , 23 , 29 ] . For example , in the context of bird species recognition , a recent work by Fagerlund [ 13 ] ( current state of the art ) averages framelevel features and applies support vector machines ( SVMs ) . Rather than averaging frame level features , we represent their distributions using histograms ( bag of codewords ) defined by a ‘codebook’ of clustered frame level features . Codebook based representations have been successfully applied in computer vision [ 7 , 31 , 19 ] , and have also recently achieved success in music genre classification [ 26 ] .
000510246810Time ( Seconds)Frequency ( kHz ) Our main contribution in this paper is to establish a theoretical framework that connects nearest neighbors classifiers using histograms of features , Bayesian risk minimization , and geodesics on statistical manifolds . In particular , • We propose a probability model for audio , then follow a Bayesian approach to derive the risk minimizing classifier for this model . The Bayes classifier is closely approximated by a nearest neighbor classifier using Kullback Leibler ( KL ) divergence to compare feature histograms ( Sec 3 ) ;
• We explain that not only do Kullback Leibler and the related Hellinger distance follow from a Bayesian probability model , but in the limit of a nearestneighbor classifier , they can be thought of as approximations to geodesics using the Fisher information metric on statistical manifolds of histograms ( Sec 4 ) ;
• We experimentally compare the accuracy of nearestneighbors using L1 , L2 , KL and Hellinger distances , and SVMs , with averages and histograms of framelevel features , on a data set consisting of 413 thirtysecond intervals of sound from six species of bird . Results indicate that classifiers using histograms of frame level features outperform those using averages , and that with a manifold geodesic distance between histograms , nearest neighbor can outperform SVM . Several classifiers achieve over 90 % accuracy ( Sec 5 ) .
2 Background and Related Work
We review data representation for audio classification , and related work on species identification from bird sounds .
2.1 Data Representation
Our goal is to classify a recording of bird sound as one of several species . A critical initial step toward this goal is to extract meaningful features to describe an interval of sound . This section presents our approach to constructing feature vectors to describe such intervals .
211 Basics : Signals and Spectrograms
Audio signals consists of a time series of samples , which we denote as s(t ) . It is often easier to recognize patterns in an audio signal when samples are converted to a frequency domain spectrogram using the Fast Fourier Transform ( FFT ) [ 3 ] , ( see Fig 1 for an example spectrogram ) .
To compute a spectrogram , samples in a sound are divided into overlapping frames ( Fig 2 ) , each of which contains a fixed number of consecutive samples . The FFT is applied to each frame to obtain the complex Fourier coefficients . The magnitudes of these coefficients are called the is made up of Figure 2 . An audio signal samples , which are divided into overlapping frames . frame ’s magnitude spectrum and represent the intensity of the sound during that frame at different frequencies . A spectrogram is a plot of the spectrum for each frame in a signal .
212 Frame Level Features
Many features for audio classification describe individual frames of a signal . In this section , we describe three features that we used in our experiments . Spectrum Density . The magnitude spectrum of a frame can be normalized to form a probability distribution . If the magnitude spectrum is ( |c1| , . . . ,|cl| ) , where l is the numf(i ) = |ci|P |ci| . We can directly use ( f(1 ) , f(2 ) , . . . , f(l ) ) ber of elements in a spectrum , then the spectrum density is as the feature vector describing a frame . Mean Frequency and Bandwidth . Consider the spectrogram shown in Fig 1 ; each vertical slice represents the spectrum of one frame of sound . Bird sounds are usually concentrated at a few frequencies ; we can see this phenomenon as horizontal strips in the spectrogram . This suggests that it is possible to condense the information contained in the spectrum density into just two values : the mean frequency and the bandwidth of the spectrum . The mean frequency of a frame indicates the vertical position of the strip , while bandwidth describes the width of the strip .
Specifically , the mean frequency is fc = xf(x)dx , and
( x − fc)2f(x)dx . bandwidth is BW =
Mel Frequency Cepstral Coefficients . Mel frequency cepstral coefficients [ 9 ] ( MFCCs ) are one of the most widely used features for audio classification . The idea is to first compute Mel frequency coefficients ( MFCs ) , which are like the magnitude spectrum , but in units of mels rather than Hz ( mels correspond more closely with human perception of pitch [ 30] ) . MFCs are computed by applying a collection of triangular filters to the magnitude spectrum ; the MFCs are the response of each filter . The filters are evenly spaced in the mel scale . MFCCs are the result of applying the discrete cosine transform ( DCT ) to the log of the MFCs .
213 Aggregating Frame Level Features
An interval contains a large number of frames , which can be aggregated to produce a single fixed length feature vector . s(1)s(2)s(129)s(256)s(384)Frame 1Frame 2 Codebook Feature Histograms . The simple binning approach does not work for higher dimensional frame level features such as spectra or MFCCs — we would need an infeasible number of bins to cover these high dimensional spaces . Instead , we take a ‘codebook’ approach [ 26 ] to constructing histograms for high dimensional features , which amounts to using non uniform bins . A codebook is a collection of k codewords , each of which is a feature vector that is considered as representative in the feature space . There is one bin associated with each codeword . Given an interval ( ie , a set of frames each described by a feature vector ) and a codebook , to compute a feature for the interval , assign each frame to its closest codeword , then count the number of frames assigned to each codeword . The vector of counts , normalized by dividing by their sum , gives the final feature vector , which is a histogram .
2.2 Related Work
Bird species can be classified using features extracted from audio recordings . A common approach to bird species classification is to identify distinct syllables , then construct feature vectors for those syllables and apply a standard classifier such as nearest neighbor or support vector machines to predict the species for each syllable [ 29 , 13 , 16 , 12 , 23 , 22 , 27 ] . Song level species prediction has also been investigated using Hidden Markov Models [ 21 , 29 ] , Gaussian Mixture Models [ 29 ] , based on comparisons of syllable pair histograms [ 28 ] , or nearest neighbor classifiers using a feature constructed by aggregating syllable features [ 23 ] .
To classify syllables or songs , most prior work relies on segmentation of the input audio into syllables [ 29 ] . As such , the classifier accuracy can be strongly dependent upon accurate segmentation [ 12 ] . A standard approach to segmentation is to compute the energy of each frame , then adaptively compute a threshold that separates syllables from background noise [ 29 , 13 , 25 , 25 ] . It is difficult to obtain reliable segmentation using this method in recordings with low signal to noise ratio . In this paper , we use a simple approach to detect a set of interesting frames within the signal that correspond to bird sound , and do not require that they precisely match syllable time boundaries ( Sec 5 ) .
Audio classification in general has been widely studied , with applications to human speech and music being the most common . Our work is closely related to recent work by Seyerlehner et al . [ 26 ] on music genre identification . They follow a codebook approach to constructing audio feature histograms ( Sec 213 ) , and use a nearestneighbor classifier with L1 distance to classify these features . However , it is not obvious why a nearest neighbor classifier is ideal for classifying histograms of features , or which distance measures are the best for comparing histograms . In this paper , we show that the Bayes optimal classifier for a probability model for audio is closely related
Figure 3 . 2D histograms of frame mean frequency and bandwidth from two different intervals of audio recordings of the Downy Woodpecker .
A common approach that has been used in syllable classification is to average frame level features [ 13 , 23 , 29 ] . However , by averaging , significant information about the distribution of features is lost , which can be problematic when the distribution of features in an interval is multimodal . For example , Fig 3(b ) shows the distribution of the features ( mean frequency and bandwidth ) of the frames from a 30 second recording of a downy woodpecker ( approximated by a 5000 bin histogram ) . In this case , the distribution is clearly multimodal and its mean will actually be in an area of relatively low probability , making it a poor representation for the overall distribution . We observed that such multimodality is common for bird sound . This observation suggests that aggregation schemes that can capture multimodality in feature distributions may be more successful than averages ( our experimental results support this idea ; Sec 56 ) Inspired by the use of codebooks for image classification [ 7 , 31 , 19 ] , and recent work in music genre classification [ 26 ] , we consider aggregating frame level features by representing their distributions with histograms . Low Dimensional Feature Histograms . Given an interval ( ie , a set of frames ) , each of which is described by a d dimensional feature vector , a natural way to represent the interval is to use the probability distribution of features in this interval . This distribution can be approximated by a d dimensional histogram , where dimension i is discretized i=1 ki bins . Note that since the total number of bins grows exponentially with d , this method can only be applied for small values of d . The vector of frequencies for each histogram bin can be used as a feature vector for classification . into ki bins , leading to a total ofd
0510001020304050Mean Frequency ( kHz)Bandwidth ( kHz)(a ) Downy Woodpecker , Pecking0510001020304050Mean Frequency ( kHz)Bandwidth ( kHz)(b ) Downy Woodpecker , Song Figure 4 . The plate diagram for the IntervalIID model .
Table 1 . Notations
Description class label ( bird species ) number of interesting frames in an interval frame feature histogram parametrization ith test frame feature vector ith training frame feature vector for the k training interval frame feature vector collection for an interval X = [ x1 , . . . , xn ] frame feature vector collection for the kth training interval class label associated with the kth training interval number of training intervals number of training intervals from class m class prior probability class conditional histogram probability interval conditional framefeature probability interval conditional features probability class conditional features probability
Variable m n θ xi xt ik
X
X t k yt k
K Km P ( m ) p(θ|m ) px|θ(x|θ ) pX|θ(X|θ ) pX|m(X|m ) to nearest neighbor classifiers using histograms of features with appropriate distance measures .
3 Probability Model for Sound
In the following section , we present a theoretical justification for the frame level feature histogram representation through a probability model , namely the IntervalIID model , and show that the corresponding Bayes riskminimizing classifier can be approximated by a nearestneighbor classifier with KL divergence .
3.1 The Interval IID model
The Interval IID model follows the graphical representation in Fig 4 . The model suggests that to generate an interval , we first determine its class label m based on the class prior p(m ) . Given m , we then generate an intervalspecific parameterization θ based on p(θ|m ) , which parameterizes the the frame feature distribution px|θ(x|θ ) of that interval . Given θ , we then generate n independent and identically distritbuted ( iid ) frame feature vectors xi based on px|θ(x|θ ) ( thus the name Interval IID , ie , frames are iid within an interval ) .
Given an observed interval represented by its collection of frame features X = [ x1 , x2 , . . . , xn ] , using Bayes rule , we write its class conditional probability as pX|m(X|m ) = pX|θ(X|θ)p(θ|m)dµ(θ ) ,
( 1 ) where θ is a parametrization determining the intervalconditional feature distribution px|θ(x|θ ) and m denotes the class label . Here we marginalized over the intervalconditional features probability model pX|θ(X|θ ) according to the class conditional histogram parametrization distribution p(θ|m ) . As the Interval IID model name suggests , conditioned on θ , the frame level features are assumed iid , and hence pX|θ(X|θ ) can be written as a product of the marginal distributions of each frame level feature : n i=1 pX|θ(X|θ ) = px|θ(xi|θ ) ,
( 2 ) where xi denotes the feature vector of the ith frame . Substituting ( 2 ) into ( 1 ) , the class conditional model for the Interval IID model is given by pX|m(X|m ) = px|θ(xi|θ)p(θ|m)dµ(θ ) .
( 3 ) n i=1 n
The integral wrt θ here applies to the product of marginal probabilities . By writing p as elog p and replacing the integral with the expectation notation , ( 3 ) becomes pX|m(X|m ) = Eθ
( 4 ) To express pX|m(X|m ) in ( 4 ) in terms of the KullbackLeibler ( KL ) divergence , start by introducing the following terms :
.e i=1 log px|θ(xi|θ)|mfi . Pn log px|θ(xi|θ ) ,
θ
θ∗ = arg max n ˜H(θ∗ ) = − 1 n n i=1
D(θ∗ , θ ) = log
1 n i=1
1 n log px|θ(xi|θ∗ ) , px|θ(xi|θ∗ ) px|θ(xi|θ )
( 5 )
( 6 )
( 7 ) i=1 ing ( 5 7 ) and the observation thatn
Note that θ∗ is the maximum likelihood estimator of θ . Usi=1 log px|θ(xi|θ ) = −n( ˜H(θ∗ ) + D(θ∗ , θ) ) , we rewrite ( 4 ) as pX|m(X|m ) = e−n ˜H(θ∗)Eθ|m
( 8 ) By the definition of θ∗ in ( 5 ) , we have that D(θ∗ , θ ) ≥ 0 for all θ and is zero for θ = θ∗ . We proceed with
e−nD(θ∗,θ)fi
KmΘnXm L the specific case in which the features are discretized into L non intersecting bins defined by the sets Al . Hence , we represent the class conditional distribution of framelevel features using histograms . Each frame level feature xi can fall into one of the histogram bins {A1 , . . . , AL} with probability {θ1 , . . . , θL} , respectively . The vector θ = [ θ1 , . . . , θL]T is a probability mass function ( or a his togram ) , ie , θl = 1 and θl ≥ 0 . The interval conditional probability model for a frame level feature is given by px|θ(x|θ ) =
θI(x∈Al ) l
,
( 9 ) l=1 where I(· ) is the indicator function which takes the value one if its argument is true and zero otherwise . We would like to point out that when px|θ(·|θ ) is given by ( 9 ) and p(θ|m ) is the Dirichlet distribution , then ( 3 ) becomes the Dirichlet Multinomial model , which is also referred to as Polya distribution [ 24 ] or the Dirichlet compound multinomial ( DCM ) model [ 11 ] . This model is often used as a topic model in text document classification . One criticism concerning the choice of Dirichlet prior is the limited capability of representing multimodal priors [ 34 ] . Our experience with bird sounds suggests that the probability model p(θ|m ) is indeed multimodal ; as Fig 3 shows , frame level feature histograms for the same species differ between intervals .
For px|θ(x|θ ) given by ( 9 ) , we have
θ∗ l = ˆpl ,
˜H(θ∗ ) = H(θ∗ ) D(θ∗ , θ ) = Dkl(θ∗θ ) ,
( 10 ) ( 11 ) n ( 12 ) i=1 I(xi ∈ Al ) is the lth empiriwhere ˆpl = 1 n cal histogram bin probability estimate based on the observed feature collection X = [ x1 , x2 , . . . , xn ] , H(p ) = l=1 pl log pl is the entropy associated with a multinomial parameterized by p ( the vector of bin probabilities of a histogram ) , and
−L
Dkl(θ∗θ ) =
L l=1 l log θ∗ θ∗ l θl is the Kullback Leibler ( KL ) divergence between a multinomial parameterized by θ∗ and another parameterized by θ . Substituting ( 10) (12 ) into ( 8 ) , we have pX|m(X|m ) = e−nH( ˆp)Eθ
( 13 ) This form for pX|m(X|m ) acts as the likelihood component in the Bayes risk minimizing classifier in the following section . Moreover , it highlights the role of the KL divergence in optimal Bayesian classification for the problem at hand .
.e−nDkl( ˆpθ)|mfi .
M
3.2 Bayes Risk Minimizing Classifier
We start with a brief review of the Bayes risk minimization approach to classification [ 14 ] . The probability of error for a given classification rule ˆm(X ) is
P r(error ) =
P ( ˆm(X ) = m|m)P ( m ) .
( 14 ) m=1
The classification rule that minimizes the error in ( 14 ) is
ˆm = arg max m pm|X(m|X ) .
( 15 )
This rule is also referred to as maximum a posteriori ( MAP ) , as it assigns a decision based on the highest class probability given the set of observations . Using Bayes rule pm|X(m|X ) = pX|m(X|m)P ( m)/pX(X ) and the fact that pX(X ) is constant wrt to the class variable m , yields an equivalent form to the MAP classifier : log P ( m ) + log pX|m(X|m ) . ( 16 ) ˆm = arg max m After replacing the likelihood PX|m(X|m ) with ( 13 ) , the MAP classification rule ( 16 ) for the Interval IID model in ( 3 ) is
ˆm = arg max m
+ log Eθ log P ( m )
exp,−nDkl(ˆpflflθ) |mfi
( 17 )
Note that since H(ˆp ) in ( 13 ) is independent of m , it is not incorporated into ( 17 ) . It is equivalent to replace the maximization with minimization and divide by n
.e−nDkl( ˆpθ)|mfiP ( m )
Eθ log
ˆm = arg min . ( 18 ) m With no exact knowledge about P ( m ) and the PDF p(θ|m ) used to compute the expectation Eθ[·|m ] , we propose estimating these quantities from the training data .
− 1 n
3.3 Training
1k , xt
2k , . . . , xt
To describe the training process , we start by explainEach interval k ing the format of the training data . k = in the training data contains n training features X t [ xt nk ] and is associated with a class label yt k . We assume that K training intervals are available , ie , K ) . We denote training vari(X t ables using the superscript t notation . and E[·|m ] through their sample estimates
To train the classification rule in ( 18 ) , we replace P ( m )
2 ) , . . . , ( X t
1 ) , ( X t
K , yt
2 , yt
1 , yt
ˆm = arg min m
−1 n log k = m)e−nDkl( ˆpˆθ(k ) )
I(yt
, ( 19 )
K k=1 where k is the interval number , K is the total number of training intervals , yt k is the class label for the kth training interval , and ˆθ(k ) is a histogram estimated from the kth training interval given by n i=1
ˆθ(k ) l =
1 n
I(xt ik ∈ Al ) ,
( 20 ) where xt With a slight abuse of notations , we rewrite ( 19 ) as ik is the ith feature vector from the kth interval .
Km e−nDkl( ˆpˆθ(k,m ) )
,
( 21 )
ˆm = arg min m
− 1 n log k=1 where the ˆθ(k,m ) ’s are the sorted version of the ˆθ(k)s from class m such that Dkl(ˆpˆθ(1,m ) ) ≤ Dkl(ˆpˆθ(2,m ) ) ≤ . . . Dkl(ˆpˆθ(Km,m) ) , and Km is the number of training intervals for the mth class . Using the ordered training class histograms , we reorganize ( 21 ) as Dkl(ˆpˆθ(1,m ) )
ˆm = arg min m e−n(Dkl( ˆpˆθ(i,m))−Dkl( ˆpˆθ(1,m) ) )
( 22 ) nm
1 +
.
− 1 n log i=2
We refer to ( 22 ) as the Interval IID MAP classifier . While equivalent to ( 21 ) , ( 22 ) provides insight into the relation between Bayes risk minimization , nearest neighbor classifiers , and manifold geodesics . Identifying the training intervals with their feature histograms , and the test interval with its feature histogram , the first term on the RHS of ( 22 ) is a KL divergence based nearest neighbor rule in histogram space . Note that if the KL distance to points other than the first nearest neighbor Dkl(ˆpˆθ(i,m ) ) is sufficiently larger than the distance to the first nearest neighbor Dkl(ˆpˆθ(1,m ) ) then the second term on the RHS of ( 22 ) becomes negligible , and ( 22 ) is simply a nearest neighbor classifier using KL divergence .
4 Nearest Neighbors on Statistical Manifolds
The connection between optimal Bayes classification and the histogram KL nearest neighbor rule leads us to extend the approach to nearest neighbor classification on histograms . Note that a collection of probability models ( i.e , histograms ) can be regarded as a manifold . Denote a model by p(X|θ ) or in short by p(·|θ ) . The collection of models given by
M =(p(·|θ ) | θ ∈ Θ ∈ Rd ) ,
( 23 ) is a d dimensional statistical manifold if there exist a oneto one smooth mapping between θ to p(·|θ ) . In the geometric approach to statistical models [ 20 ] , one can measure the geodesic distance between two histograms by using the Fisher information metric ( FIM ) as the Riemannian metric
DF ( p(·|θ ) , p(·|θ ) ) = min θ(· ) , θ(0)=θ , θ(l)=θ
˙θ(t)TI(θ(t ) ) ˙θ(t)dt , ( 24 ) l
0 where I(θ ) is the Fisher information matrix given by d log p(x|θ ) d log p(x|θ )
Iij(θ ) = E dθi dθj
.
( 25 )
The FIM is considered a natural metric for statistical manifolds as it reflect the capability to discriminate between probability models from their samples .
To generalize the nearest neighbor approach discussed in the previous section in the context of statistical manifolds , we consider a geodesic nearest neighbor rule using DF ( p(·|θ ) , p(·|θ ) ) defined in ( 24 ) . As the precise form of the manifold is unavailable , an exact computation of the geodesic distance DF ( p , p ) is impossible . Since the nearest neighbor approach prompts us to calculate short geodesic distances , local approximations of DF ( p , p ) can be used instead . For two close probability models p → p it is known [ 20 ] that2Dkl(pp ) → DF ( p , p ) . The KL di vergence provides a computable approximation to the FIM manifold geodesic distance .
Note that other approximations for the FIM are available ( eg , certain Ali Silvey divergences , and specifically , Hellinger divergence ) . In this paper , we use the Hellinger divergence given by
DH(p , q)2 =(
√ pi − √ qi)2 ,
( 26 ) which is a metric as opposed to the KL divergence . The approximation of the FIM using Hellinger distance for close models is 2DH(p , p ) → DF ( p , p ) [ 20 ] .
For the purpose of comparison , we experimentally evaluate nearest neighbor classifiers using L1 and L2 distances as well as KL and Hellinger . L2 is the standard Euclidean distance , which is widely used , but not theoretically justified for the comparison of probability distributions . L1 is fairly common for comparing probability distributions . It is a member of the Ali Silvey family , but due to non differentiability , it is not an approximation to the FIM . However , it is related to Hellinger by the inequal2 DH(p , q)2 ≤ DL1(p , q ) ≤ DH(p , q ) [ 8 ] . This relaity , 1 tion between L1 and Hellinger hints at why classifiers using these distances achieve similar results ( Sec 56 )
5 Experiments
In this section , we describe the experimental setup used to measure the accuracy of the proposed methods for bird species classification , and to compare with SVMs [ 13 ] . We consider various frame level features ( mean frequency and bandwidth , MFCCs , and spectral density ) , interval level features ( averages vs . histograms ) , and metrics for nearestneighbor classification ( L1 , L2 , KL , and Hellinger ) . We also empirically verify that a nearest neighbor classifier using Kullback Leibler closely approximates the Interval IID MAP classifier ( 22 ) as suggested in Sec 3 .
5.1 Data
We have 1.13 GB of recordings from the Cornell Macaulay library , of 6 species : Black Throated Blue Warbler , Hermit Warbler , Downy Woodpecker , Swainson ’s Thrush , Western Tanager , and Winter Wren . All of these recordings are at least 30 seconds long , and most are less than 10 minutes . We divide each recording into intervals of 30 seconds , resulting in 413 intervals . Our goal is to classify these intervals according to species .
The recordings were collected over several decades , mainly in the western United States . Most are made using a directional microphone in the field . The amount of noise in the recordings varies widely . In addition to static and wind , some recordings contain cars sounds , human speech , and other non bird sounds . We manually removed most portions of sound with human voices . Although each recording is labeled with just one species , some recordings contain multiple birds , sometimes of different species ; usually the loudest bird present corresponds to the label for the recording . The sampling frequency for all recordings is 44.1 kHz . The audio data is stored as mono channel WAV files .
5.2 Preprocessing
Section 2.1 covers the process of converting a sequence of samples from an audio interval into interval level features . We proceed by further elaborating on the specific details of our experimental setup .
When dividing a signal into frames , we use 256 samples per frame , and successive frames overlap by 50 % . To reduce noise and decrease processing time in later stages , we discard the lowest 8 and highest 64 elements of each frame ’s spectrum , leaving 56 elements from the original 128 ( equivalent to removing all sound below 1.378 kHz and above 10.852 kHz ) .
Instead of syllables , we detect a subset of interesting frames ( which are more likely to contain bird sound ) in an interval . To find these interesting frames , we compute the total magnitude of each frame,|ci| , and retain only the
10 % of frames with highest total magnitude in all subsequent calculations . Note that the total magnitude is similar to , but not the same as the energy of a frame.1
1Parseval ’s theorem states that the energy of a frame can be computed
Our implementation of MFCCs ( Sec 212 ) , is based on the description provided by Ganchev et al . [ 15 ] of the MFCCs computed in the Cambridge Hidden Markov Models Toolkit ( for MATLAB ) , known as HTK [ 33 ] . We use 24 filters,2 resulting in 24 MFCs , then take only the first 12 elements of the output of the DCT as the frame level feature . For constructing 2D histograms , we divide the range of values for mean frequency and bandwidth into square bins 100 Hz wide , with 100 bins on the mean frequency axis , and 50 bins for the bandwidth axis ( for a total of 50 × 100 = 5000 bins , covering a range of 0 Hz to 10,000 Hz for fc and 0 Hz to 5000 Hz for BW ) . There is one element in the feature vector for each histogram bin , so this representation results in a 5000 dimensional feature vector.3
5.3 Clustering for Codebooks
For constructing codebooks , we apply the k means++ clustering algorithm [ 1 ] to the frame level features from a training data set . Note that there are several hundredthousand frames to cluster in our data set . To speed up codebook construction , we follow a two staged clustering proceedure suggested by Seyerlehner et al . [ 26 ] . In particular , we first cluster features within each 30 second interval , then cluster the resulting cluster centers to obtain the final codewords . In the first stage of clustering , the feature vectors are either the spectrum density , or MFCCs for the interesting frames . In the second stage , the examples are the cluster centers from the first stage . We use k = 10 clusters for the first stage and k = 100 for the second . Thus , the final interval level features constructed using this method are 100 dimensional . In our preliminary experiments , this approach to clustering yielded an order of magnitude speedup over clustering all frame level features at once , because the first stage of clustering does not need to be repeated in each fold of cross validation .
5.4 Classifiers
There are many combinations of frame level features and methods of aggregating them . The combinations we consider in this study are : averages of fc and BW , spectrum
X |ci|2 , were ci is the ith FFT from its spectrum via the formula E = coefficient .
2In our implementation , the filters span a range of frequencies from flow = 1000Hz to fhigh = 22050Hz . Following an exact implementation of the filters described by Ganchev et al . [ 15 ] , we got aliased triangle filters because some were narrower than a single spectrum bin , which caused artifacts in the MFCs . To fix this problem , we numerically integrate the triangle filter function over the range of each bin . Many other implementations of MFCCs work with lower sampling frequencies [ 15 ] , so we suspect this problem is related to working with sound sampled at 44.1 kHz , as well as our choice of values for flow and fhigh .
3We apply Laplace smoothing to the histogram estimation by starting with a count of 1 for each bin .
Representation Classifier Frame Feature Average fc , BW Average fc , BW Average MFCCs Average MFCCs MFCCs Average Spectrum Density Average Spectrum Density Average Spectrum Density Average 2D Histogram fc , BW 2D Histogram fc , BW 2D Histogram fc , BW 2D Histogram fc , BW 2D Histogram fc , BW Codebook MFCCs Codebook MFCCs Codebook MFCCs Codebook MFCCs MFCCs Codebook Spectrum Density Codebook Spectrum Density Codebook Spectrum Density Codebook Spectrum Density Codebook Spectrum Density Codebook
NN L1 NN L2 NN L1 NN L2 SVM NN L1 NN L2 SVM Interval IID MAP NN Kullback Leibler NN Hellinger NN L1 NN L2 NN L1 NN L2 NN Kullback Leibler NN Hellinger SVM NN L1 NN L2 NN Kullback Leibler NN Hellinger SVM
% correct 42.85 42.85 81.11 81.11 84.50 79.42 81.35 84.75 87.40 87.40 88.13 86.44 83.05 84.41 ± .89 83.49 ± .62 85.42 ± .62 86.59 ± .50 87.17 ± .58 92.54 ± .44 88.28 ± .99 90.70 ± .40 92.10 ± .27 88.14 ± .58
Table 2 . The accuracy of each classifier in predicting bird species based on 413 thirty second intervals of sound . NN means nearest neighbor . The values listed for classifiers using a codebook are average accuracy over 5 trials , ± average deviation . Our proposed methods are listed in bold . density , and MFCCs , 2D histograms of fc and BW , and codebook histograms of spectrum density and MFCCs .
Using the above features extracted from the data described in Sec 5.1 , we compare several classification algorithms : nearest neighbor with L1 , L2 , KL and Hellinger distances , and the Interval IID MAP classifier proposed in Sec 3.2 ( 22 ) , as well as support vector machines . Of these classifiers , Interval IID Map , KL and Hellinger are our proposed methods , and the others are included for comparison .
541 Support Vector Machines
Support vector machines [ 6 ] ( SVMs ) are a family of algorithms for supervised classification that find a linear decision boundary by maximizing the margin between two classes . In cases where linear classification is insufficient , the kernel trick is applied to non linearly project features into a higher dimensional space where linear separability is possible . The implementation of SVMs that we used is WLSVM [ 10 ] , which integrates LIBSVM [ 4 ] into the Weka [ 32 ] machine learning system . Following Fagerlund [ 13 ] , and the recommendations of Hsu , Chang and Lin [ 17 ] , we use a radial basis function kernel , and optimize the SVM parameter C and the kernel parameter γ , by grid search . We evaluate the SVM at all combinations of C and γ in {10−1 , 100 , 101 , 102} , and report the best accuracy achieved with any set of parameters . To handle multiple classes ( in our case , species ) , LIBSVM use the one againstone voting scheme [ 18 ] .
5.5 Cross Validation and Multiple Trials
To measure the accuracy of the proposed classifiers , we use them to predict the species in each of 413 thirty second intervals of sound . Each classifier is trained using all of the intervals that do not come from the same recording as the interval being classified ( the data set consists of longer recordings that are split into intervals ) . We use this setup so the classifier must identify species without already having example recordings of the individual bird being classified . Fagerlund [ 13 ] used a similar ‘individual independent’ setup for cross validation .
Classifiers that use a codebook to construct feature histograms depend on a randomized clustering algorithm . To account for the randomness , we ran five trials with different random seeds , and report average accuracy , ± average deviation .
5.6 Results
Table 2 lists the accuracy of each classifier on the species recognition problem . We make the following key observations .
Hellinger ) outperform SVM . We want to emphasize that unlike SVMs , which require significant parameter tuning , the proposed methods also offer additional advantages in terms of their simplicity and scalability , making them more usable in practice .
• Regardless of which frame level features we use , histograms of features achieved better accuracy than averages . One possible explanation of this result is that feature distributions may be multimodal ( Fig 3(b) ) , so the mean alone may not be enough to discriminate between distributions from different species .
• Using the 2D histogram of fc and BW , the IntervalIID MAP classifier ( 22 ) produced identical results to a nearest neighbor classifier with KL divergence . This result confirms our theoretical argument that a nearest neighbor classifier using KL divergence is a close approximation to the Interval IID MAP classifier . Accordingly , we recommend using the more efficient nearest neighbor with KL as opposed to ( 22 ) , when audio data is believed to be generated as in the IntervalIID model .
• Comparing different distance functions when using histograms , we observe that L1 , Hellinger and KL were generally more accurate than using L2 , with the performance of Hellinger being the most robust across different settings . Interestingly , while L1 is not an approximation to the FIM , its performance is highly competitive to KL and Hellinger . For histograms of spectrum density , L1 slightly outperformed Hellinger ( although not statistically significantly ) . This is possibly due to the close relationship between L1 , Hellinger and KL , as explained in Sec 4 . Note that MFCCs are essentially a compressed version of the spectrum ( from 56 elements to 12 ) , so it is not surprising that classifiers using them are slightly less accurate than those using spectrum density .
• Despite their relative simplicity , classifiers using 2D histograms of mean frequency and bandwidth provide remarkably accurate predictions . Being able to visualize a 2D histogram as an image provides insight into the structure of bird sound ( for example , we can see that interval feature histograms may be multimodal ) . • Finally , we note that the proposed methods achieved accuracy similar to or better than SVMs . In particular , using codebook histograms of MFCCs , SVMs are slightly more accurate than a nearest neighbor classifier with Hellinger , although the difference is not statistically significant . On codebook histograms of spectrum density , nearest neighbor classifiers using statistical divergence measures ( ie L1 , Kullback Leibler and
6 Conclusion and Future Work
In this paper , we addressed the problem of bird species classification from audio recordings . Following a Bayesian approach to classification , we introduced the interval IID model to describe the distribution of feature vectors within an interval consisting of frames , and derived the corresponding MAP classifier . The MAP classifier suggests aggregating features into histograms and using KL nearest neighbor to classify . This connection to nearest neighbor classification on statistical manifolds led us to extended the classifier by proposing different metrics ( eg , Hellinger ) . To use the MAP classifier with high dimensional framelevel features , we employ codebook histograms .
Our study suggests that 1 ) using histograms of framelevel features in an audio classifier can produce better results than using averaged frame level features 2 ) nearestneighbor classifiers using Kullback Leibler and Hellinger distance to compare feature histograms results are competive with state of the art method such as SVM and 3 ) metrics appropriate for histograms such as Hellinger , KL , and L1 perform better than the Euclidean L2 metric .
The classifiers in this study make predictions from intervals based on the collection of frames within the interval . A common alternative is to instead focus on individual syllables . We are working on an experimental survey of methods for classifying bird species from syllables , as well as probability models that are specialized for this purpose .
The experiments and algorithms presented here are a preliminary step toward analyzing a large ( terabyte scale ) data set of bird sounds that our collaborators collected in field conditions , using an array of omnidirectional microphones . We intend to apply algorithms for bird species classification to these recordings to extract information about patterns of bird activity at an unprecedented spatial and temporal resolution .
7 Acknowledgments
This work is partially funded by the IGERT program through National Science Foundation grant No . DGE 0333257 , as well as the College of Engineering , Oregon State University . We would also like to thank Dr . Matthew Betts at Oregon State University for his help in acquiring the bird sound data .
References
[ 1 ] D . Arthur and S . Vassilvitskii . the advantages of careful seeding . In SODA ’07 : Proceedings of the eighteenth annual ACM SIAM symposium on Discrete algorithms , pages 1027–1035 , Philadelphia , PA , USA , 2007 . Society for Industrial and Applied Mathematics . k means++ :
[ 2 ] M . G . Betts , A . Diamond , G . Forbes , M A Villard , and J . Gunn . The importance of spatial autocorrelation , extent and resolution in predicting forest bird occurrence . Ecological Modelling , 191(2):197 – 224 , 2006 .
[ 3 ] C . Catchpole , P . Slater , and N . Mann . Bird song : biological themes and variations . Cambridge Univ Pr , 2003 .
[ 4 ] C C Chang and C J Lin . LIBSVM : a library for support vector machines , 2001 .
[ 5 ] Z . Chen and R . C . Maher . Semi automatic classification of bird vocalizations using spectral peak tracks . J Acoust Soc Am , 120(5 Pt 1):2974–2984 , November 2006 .
[ 6 ] C . Cortes and V . Vapnik . Support vector networks . In Ma chine Learning , pages 273–297 , 1995 .
[ 7 ] G . Csurka , C . Dance , L . Fan , J . Willamowski , and C . Bray . Visual categorization with bags of keypoints . In Workshop on Statistical Learning in Computer Vision , ECCV , volume 1 , page 22 , 2004 .
[ 8 ] A . DasGupta . Asymptotic theory of statistics and probability . International Statistical Review , 77(1):160–161 , 04 2009 .
[ 9 ] S . B . Davis and P . Mermelstein . Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences . Readings in speech recognition , pages 65–74 , 1990 .
[ 10 ] Y . EL Manzalawy and V . Honavar . WLSVM : Integrating
LibSVM into Weka Environment , 2005 .
[ 11 ] C . Elkan . Clustering documents with an exponential family approximation of the Dirichlet compound multinomial distribution . In Proceedings of the 23rd international conference on Machine learning , pages 289–296 , 2006 .
[ 12 ] S . Fagerlund . Automatic Recognition of Bird Species by their Sounds . PhD thesis , Helsinki University of Technology , 2004 .
[ 13 ] S . Fagerlund . Bird species recognition using support vector machines . EURASIP Journal on Advances in Signal Processing , 2007 , 2007 .
[ 14 ] K . Fukunaga .
Introduction to Statistical Pattern Recogni tion . Academic Press , 1990 .
[ 15 ] T . Ganchev , N . Fakotakis , and G . Kokkinakis . Comparative evaluation of various mfcc implementations on the speaker verification task . In in Proc . of the SPECOM 2005 , pages 191–194 , 2005 .
[ 16 ] A . H¨arm¨a . Automatic identification of bird species based on sinusoidal modeling of syllables . In Acoustics , Speech , and Signal Processing , 2003 . Proceedings . ( ICASSP ’03 ) . 2003 IEEE International Conference on , volume 5 , pages V–545– 8 vol.5 , 2003 .
[ 17 ] C . Hsu , C . Chang , C . Lin , et al . A practical guide to support vector classification , 2003 .
[ 18 ] C . Hsu and C . Lin . A comparison of methods for multiclass support vector machines . IEEE Transactions on Neural Networks , 13(2):415–425 , 2002 .
[ 19 ] F . Jurie and B . Triggs . Creating efficient codebooks for viIn Tenth IEEE International Conference sual recognition . on Computer Vision , 2005 . ICCV 2005 , volume 1 , 2005 .
[ 20 ] R . Kass and P . Vos . Geometrical foundations of asymptotic inference . Wiley Interscience , 1997 .
[ 21 ] J . A . Kogan and D . Margoliash . Automated recognition of bird song elements from continuous recordings using dynamic time warping and hidden markov models : A comparative study . The Journal of the Acoustical Society of America , 103(4):2185–2196 , 1998 .
[ 22 ] C . Kwan , G . Mei , X . Zhao , Z . Ren , R . Xu , V . Stanford , C . Rochet , J . Aube , and K . Ho . Bird classification algorithms : Theory and experimental results . In IEEE International Conference on Acoustics , Speech , and Signal Processing , volume 5 , pages 289–292 , 2004 .
[ 23 ] C H Lee , Y K Lee , and R Z Huang . Automatic recognition of bird songs using cepstral coefficients . Journal of Information Technology and Applications , 1(1):17 – 23 , 2006 . [ 24 ] T . Minka . Estimating a Dirichlet distribution . Unpublished paper available at http://research . microsoft . com/ minka , 2003 .
[ 25 ] A . Selin , J . Turunen , and J . Tanttu . Wavelets in recognition of bird sounds . EURASIP Journal on Advances in Signal Processing , 2007:1–9 , 2007 .
[ 26 ] C . Seyerlehner , G . Widmer , and P . Knees . Frame Level Audio Similarity A Codebook Approach . In Proceedings of the 11th International Conference on Digital Audio Effects ( DAFx 08 ) , Espoo , Finland , September 1–4 2008 .
[ 27 ] P . Somervuo and A . H¨arm¨a . Analyzing bird song syllables on the self organizing map . In Proceedings of the Workshop on Self Organizing Maps ( WSOM’03 ) , Kitakyushu , Japan , Sept . 2003 .
[ 28 ] P . Somervuo and A . Harma . Bird song recognition based on syllable pair histograms . In Proceedings of IEEE International Conference on Acoustics , Speech , and Signal Processing ( ICASSP04 , volume 5 , pages 825–828 , 2004 .
[ 29 ] P . Somervuo , A . H¨arm¨a , and S . Fagerlund . Parametric representations of bird sounds for automatic species recognition . In IEEE Transactinos on Audio , Speed , and Language Processing , volume 14 . IEEE Press , 2006 .
[ 30 ] J . Volkmann , S . S . Stevens , and E . B . Newman . A scale for the measurement of the psychological magnitude pitch . The Journal of the Acoustical Society of America , 8(3):208–208 , 1937 .
[ 31 ] J . Winn , A . Criminisi , and T . Minka . Object categorization by learned universal visual dictionary . In Tenth IEEE International Conference on Computer Vision , 2005 . ICCV 2005 , volume 2 , 2005 .
[ 32 ] I . Witten and E . Frank . Data mining : practical machine learning tools and techniques with Java implementations . ACM SIGMOD Record , 31(1):76–77 , 2002 .
[ 33 ] S . Young . The hidden markov model toolkit . Entropic Cam bridge Research Laboratory , Ltd , 2:2–44 , 1995 .
[ 34 ] K . Yu , S . Yu , and V . Tresp . Dirichlet enhanced latent semantic analysis . In Conference in Artificial Intelligence and Statistics , 2005 .
