Bayesian Overlapping Subspace Clustering
Qiang Fu
Arindam Banerjee
Dept . of Computer Science & Engineering
Dept . of Computer Science & Engineering
University of Minnesota , Twin Cities
University of Minnesota , Twin Cities qifu@csumnedu banerjee@csumnedu
Abstract
Given a data matrix , the problem of finding dense/uniform sub blocks in the matrix is becoming important in several applications . The problem is inherently combinatorial since the uniform sub blocks may involve arbitrary subsets of rows and columns and may even be overlapping . While there are a few existing methods based on co clustering or subspace clustering , they typically rely on local search heuristics and in general do not have a systematic model for such data . We present a Bayesian Overlapping Subspace Clustering ( BOSC ) model which is a hierarchical generative model for matrices with potentially overlapping uniform sub block structures . The BOSC model can also handle matrices with missing entries . We propose an EM style algorithm based on approximate inference using Gibbs sampling and parameter estimation using coordinate descent for the BOSC model . Through experiments on both simulated and real datasets , we demonstrate that the proposed algorithm outperforms the state of the art .
1
Introduction
Several datasets are represented in the form of a matrix , where each row represents an object and each column represents a feature . The problem of finding dense/uniform sub blocks in a given data matrix has emerged as an important unsupervised learning task . A dense/uniform sub block consists of a subset of instances that have similar feature values for a subset of features . Such a problem is inherently combinatorial and has been investigated in the context of subspace clustering [ 2 , 15 ] , projected clustering [ 1 , 18 ] and co clustering [ 8 , 11 , 17 ] . The problem is important in a variety of applications . For example , in gene expression data analysis , one would like to find a set of genes which coexpress under a set of experimental conditions . In a recommendation system , a uniform sub block indicates a group of users who have similar ratings for a group of movies .
10
20
30
40
50
60
70
80
90
100
10
20
30
40
50
60
70
80
90
100
10
20
30
40
50
60
70
80
90
100
10
20
30
40
50
60
70
80
90
100
( a ) Raw Data
( b ) Ideal Output
Figure 1 . An example problem : ( a ) Raw data with latent overlapping co clustering structure , ( b ) Ideal output from an algorithm , where rows and columns have been permuted to reveal the structure discovered .
While progress has been made in the development of subspace clustering and co clustering algorithms , the existing formulations often lack the flexibility needed to solve the problem of finding uniform sub blocks . In the current context , the desiderata can be captured by the following three requirements . First , the sub blocks may overlap , so that some entries may belong to more than one subblock . For example , in gene expression analysis , a gene can have multiple functions and hence co express with different groups under different experimental conditions . Most clustering/co clustering formulations are not designed to discover overlapping clusters . Second , not all rows and columns may be a part of a sub block , and the formulation has to be flexible enough to allow that . Most existing clustering/co clustering formulations assume that all points belong to some cluster/co cluster , and the corresponding algorithms have no capacity to identify background noise automatically . Finally , the matrix may have missing entries . In practice , one often imputes the missing values with row/column statistics or has a heuristic work around . Ideally , we want the model formulation to be able to work with sparse matrices and in fact use sparsity to a computational advantage . To better understand the task the algorithm should perform , we give an example in Figure 1 . Figure 1(a ) shows a simple input data matrix with two overlapping dense blocks and background noise and Figure 1(b ) is the ideal output of the algorithm .
In this paper , we present a BOSC model which can find potentially overlapping sub blocks , automatically de tect background noise and naturally handle matrices with missing entries . For inference and parameter estimation , we propose an EM style algorithm .
The rest of the paper is organized as follows : We propose the BOSC model in Section 2 and present an EM style algorithm to learn the sub block assignments in Section 3 . The experimental results on both simulated and real datasets are presented in Section 4 . We conclude in Section 5 .
2 Bayesian Overlapping Subspace Cluster ing Model
1 ( [j]k
The proposed Bayesian Overlapping Subspace Clustering model assumes that the number of sub blocks k is given . Each sub block is modeled using a parametric distribution 1 ≡ j = 1 , . . . , k ) from any suitable expop(·|θj ) , [ j]k nential family . The noise entries are modeled using another distribution p(·|θk+1 ) from the same family . However , the generative model for the observed data matrix is rather different from traditional mixture models [ 12 ] as well as the more recent mixed membership models such as LDA [ 3 , 7 ] . Suppose the data matrix X has m rows and n columns , possibly with several missing entries . The main idea behind the proposed model is as follows : Each row u and each column v respectively have k dimensional latent bit vectors zu r and zv c which indicate their sub block memberships . The sub block membership for any entry xuv in the matrix is obtained by an element wise ( Hadamard ) product of the corr fi zv responding row and column bit vectors , ie , z = zu c . Given the sub block membership z and the sub block distributions , the actual observation xuv is assumed to be generated by a multiplicative mixture model [ 9 , 4 ] so that p(xuv|zu fik j=1 pj(xuv|θj)zj c , Θ ) = r , zv
. c(z )
1 p(xuv|θk+1 ) if z '= 0 , otherwise , r , zv r fizv c , Θ ) is a valid distribution . If z = zu
( 1 ) where c(z ) is a normalization factor to guarantee that p(·|zu c = 0 , the all zeros vector , then xuv is assumed to be generated from the noise component p(·|θk+1 ) . In the sequel , we will use r fi zv c = 0 ] to denote the indicator of this event . Figure 1 [ zu shows an example of a matrix generated from such a model with two dense blocks . The Hadamard product ensures that the matrix has uniform/dense sub blocks with possible overlaps while treating certain rows/columns as noise .
Since it can be tricky to work directly with latent bit vectors , we introduce suitable Bayesian priors on the sub block memberships . the prothere are k Beta distribuposed model assumes that tions Beta(αj 1 corresponding to the rows and k Beta distributions Beta(αj 1 corresponding to the columns . Let πu,j denote the Bernoulli parameter sampled r from Beta(αj r , βj r ) for row u and sub block j where [ u]m 1
In particular , r ) , [ j]k c ) , [ j]k r , βj c , βj k
β r
α r k
β c
α c mk
π r z r nk
π c z c k+1
θ
NE z x
Figure 2 . Bayesian Overlapping Subspace Clustering model . z = zr . zc . N E is the number of non missing entries in the matrix .
1 . Similarly , let πv,j and [ j]k denote the Bernoulli paramec ter sampled from Beta(αj c , βj c ) for column v and sub block j , where [ v]n 1 . The Beta Bernoulli distributions are assumed to be the priors for the latent row and column membership vectors zu
1 and [ j]k r and zv c .
The proposed model is shown as a plate diagram in Fig ure 2 . In particular , the generative process is as follows :
1 . For each co cluster [ j]k 1 :
( a ) For each row u , [ u]m 1 :
( i ) sample πu,j ( ii ) sample zu,j r ∼ Beta(αj r ) , r ∼ Bernoulli(πu,j r , βj r
) . c , βj
( i ) sample πv,j ( ii ) sample zv,j
( b ) For each column v , [ v]n 1 : c ∼ Beta(αj c ) , c ∼ Bernoulli(πv,j c ) . . fik j=1 p(xu,v|θj , zu,j p(xu,v|θk+1 ) r .zv c ) c(zu
1 r sample xuv ∼
2 . For each ( non missing ) matrix entry xuv , [ u]m
1 [ v]n 1 ,
, zv,j c ) c '= 0 , r fi zv if zu otherwise .
Since only the observed entries in the matrix are assumed to be generated by the above process , the model naturally handles matrices with missing values .
3 Analysis and Algorithm
Let Πr and Πc be m× k and n× k latent matrices which have the Bernoulli parameters for each row and column , Zr and Zc be m× k and n× k binary matrices that have the latent row and column sub block assignments for each row and column . For convenience of notation , let ςuv be an indicator variable for observed entries in the matrix , ie , ςuv = 1 if entry xuv is not missing , and 0 otherwise . Then the joint distribution over all observed and latent variables is given by p(X , Zr , Zc , Πr , Πc|αr , βr , αc , βc , Θ ) = p(Πr|αr , βr)p(Πc|αc , βc)p(Zr|Πr)p(Zc|Πc )
( 2 ) p(X|Θ , Zr , Zc ) . m' p(X|Θ , Zr , Zc ) = n' u=1 v=1
Since the observations are statistically independent given Zr , Zc , we have p(xuv|Θ , zu r , zv c)ςuv .
( 3 )
Marginalizing over all latent variables , the conditional probability of generating the matrix X given the parameters ( αr , βr , αc , βc , Θ ) is given by ff p(X|αr , βr , αc , βc , Θ ) = p(X , Zr , Zc , Πr , Πc|αr , βr , αc , βc , Θ)dΠrdΠc .
( 4 )
Πr,Πc
Zr,Zc
3.1
Inference
In the the given the of
E step , goal the the ( αr , βr , αc , βc , Θ ) , expectation paramethe model is to esters timate log likelihood E[log p(X , Zr , Zc|αr , βr , αc , βc , Θ ) ] where exto the posterior probability pectation is with respect p(Zr , Zc|X , αr , βr , αc , βc , Θ ) . We use Gibbs sampling In particular , we to approximate the expectation [ 7 , 5 ] . compute the conditional probabilities of each row variable zu,j and construct a Markov chain r based on the conditional probabilities . On convergence , the chain will draw samples from the posterior joint distribution of ( Zr , Zc ) , which in turn can be used to get an approximate estimate of the expected log likelihood . and column variable zv,j c denotes the binary matrix Zr excluding zu,j
, r the conditional probability of zu,j r = 1 is given by :
−(u,j ) If Z r p(zu,j r = 1|Z−(u,j ) ∝ p(X|Zr , Zc , Θ)p(zu,j where p(X|Zr , Zc , Θ ) is as in ( 3 ) and
, Zc , X , Θ ) r r = 1|Z−(u,j ) r
) , r = 1|Z−(u,j ) p(zu,j r r = 1|πu,j p(zu,j r
)p(πu,j r
)dπu,j r ff 1
) =
0
=
αj r αj r + βj r
.
Now ,
, Zc , X , Θ ) r , zq c , Θ)ςpq .
αj r r + βj αj r
, c , Θ)ςuq .
αj r αj r + βj r
, p(zu,j r q=1 p=1 n' r = 1|Z−(u,j ) ∝ m' p(xp,q|zp ∝ n' ∝ n' p(xu,q|zu r , zq q=1 p(xu,q|θj)zq,j c q=1
.p(xu,q|θk+1)[zu c(zu r fi zq c ) r .zq c =0 ]
( 6 )
( 7 )
( 8 )
,
.
αj r r + βj αj r ( 9 )
( ςuq r where ( 8 ) follows since the probability of generating the entries in the rows except u does not depend on the value of zu,j , and ( 9 ) follows since whether the entry xu,q belongs r to sub blocks other than j does not play a role in deciding the value of zu,j in the product term other than the overall normalization term c(zu The probability of zu,j r = 0|Z−(u,j ) ∝ n' r fi zq c ) . r = 0 can be derived similarly as
, Zc , X , Θ ) p(xu,q|θk+1)[zu r .zq r fi zq c )
βj r αj r + βj r
( ςuq p(zu,j c(zu
( 10 ) c =0 ]
.
. r q=1
Πr and Πc can be analytically integrated out in ( 4 ) because of conjugacy : they are generated by Beta distributions which are conjugate priors to Bernoulli distributions which generate Zr and Zc . Thus ( 4 ) does not depend on Πr and Πc . It is also important to note the conditional probability of observing X as in ( 4 ) is not the product of the conditional probability of observing each entry , ie , p(X|αr , βr,αc , βc , Θ ) n'
( 5 ) p(xu,v|αr , βr , αc , βc , Θ)ςuv . m'
'= u=1 v=1
The equality does not hold because the entries in the matrix are not conditionally independent given the parameters ( αr , βr , αc , βc , Θ ) . According to the generative process , r and zv zu c are sampled only once for each row and column , so that the observations in the same row/column get coupled . This is a crucial departure from several related mixture models which assume the joint probability of all observations to be simply a product of the marginal probabilities . Given the entire matrix X , the learning task is to infer the joint posterior distribution of ( Zr , Zc ) and compute the model parameters ( αfi c , Θfi ) which maximize log p(X|αr , βr , αc , βc , Θ ) . We can then draw samples from the posterior distribution and compute the dense block assignment for each entry . A general approach for the learning task is to use expectation maximization ( EM ) algorithm [ 13 ] . However , direct calculation of log p(X|αr , βr , αc , βc , Θ ) is intractable , indicating that a direct application of EM is not possible . In this section , we propose an EM like algorithm alternating between approximate inference and optimal parameter estimation to tackle the learning task . r , βfi c , βfi r , αfi
The conditional probabilities for the other binary assignment variables can be similarly derived .
The true underlying posterior distribution of ( Zr , Zc ) may have multiple modes . To prevent the sampling algorithm from getting stuck in local modes , we modify the Gibbs sampler using simulated annealing [ 10 ] . Given a temperature parameter T , the sampling is done following p(T )(zu,j r = 0|··· ) = p(T )(zu,j r = 1|··· ) =
T p(zu,j r = 0|··· ) 1 p(zu,j r = 0|··· ) 1 r = 0|··· ) 1 T + p(zu,j r = 1|··· ) 1 T + p(zu,j
T p(zu,j p(zu,j r = 1|··· ) 1
T r = 1|··· ) 1
T
When T is high , the probability distribution is almost uniform , and when T is low , more emphasis is given to high probability states . In practice , we start with a relatively high T and gradually decrease T to 1 , when the sampling distribution is exactly the posterior distribution of Zr and Zc .
The sampling is run for enough iterations till it converges . Then we sample from the stationary distribution ( with suitable gaps ) to obtain N independent and identically distributed samples of ( Zr , Zc ) , where N is a pre)N defined large number . From the samples , the expectation of the log likelihood can be empirically estimated s=1 log p(X , Zr,s , Zc,s|αr , βr , αc , βc , Θ ) , where as : 1 N Zr,s and Zc,s correspond to the sth samples .
3.2 Estimation c , βfi r , βfi r , αfi
In M step , we estimate ( αfi c , Θfi ) which maximizes the expectation . Note that , given Zr and Zc , each entry in the matrix is statistically independent of each other . So the parameter estimation problem can be formulated as maximizing the following expected log likelihood objective function : log p(X , Zr,s , Zc,s|αr , αc , Θ )
L(αr , βr , αc , βc , Θ ) s=1 s=1
N
N N log p(Zr,s|αr ) + N log p(X|Zr,s , Zc,s , Θ ) m N k n m N log p(zu,j r,s |αj u=1 s=1 s=1 s=1 j=1 r ) +
=
=
=
+
+
ςuv log p(xu,v|zu r,s , zv c,s , Θ ) .
( 11 ) s=1 u=1 v=1
The optimization can be broken into two independent parts—over the parameters ( αr , βr , αc , βc ) of the Beta log p(Zc,s|αc )
N n k s=1 v=1 j=1 log p(zv,j c,s|αj c ) distributions , and over the natural parameters Θ of the exponential family distributions . The parameter update method is similar to the one in [ 4 ] . Due to the space constraint , we omit the details .
4 Experimental Results
,
.
In this section , we present experimental results on simulated datasets , a microarray gene expression dataset and a movie recommendation dataset . First , we introduce some additional notation to be used in this section : Tstart denotes the initial temperature parameter in simulated annealing , fT < 1 denotes the multiplicative factor by which the temperature goes down every IT iterations and N is the number of samples drawn from the stationary distribution .
Since we obtain several samples from the Markov chain after it converges , the final row and column sub block assignments are decided by the following appraoch : if a row/column belongs to a sub block in more than half of the samples , we consider the row/column belongs to that corresponding sub block .
4.1 Simulated Datasets
We do experiment on four simulated datasets . The first two datasets are easy to visualize and both datasets are in the form of a 200 × 100 matrix , whose entries are initially sampled from a background noise distribution . For the first dataset D1 , we introduce 3 non overlapping uniform blocks ( normally distributed with different means ) to replace certain sub blocks in the matrix ( Figure 3(a) ) . The actual dataset is obtained by randomly permuting the rows and columns , so that the block structure is not apparent ( Figure 3(b) ) . For the second dataset D2 , we introduce 4 mildly overlapping dense blocks where the overlapping entries are generated from the multiplicative model in ( 1 ) ( Figure 3(c) ) . As before , the actual dataset is obtained by a random row/column permutation ( Figure 3(d) ) . The other two simulated datasets have larger number of sub blocks , one with 10 blocks and the other with 15 blocks . We do not provide label information to STATPC on these two datasets . We compare the performance of the proposed algorithm to a state of the art subspace clustering algorithm called STATPC [ 15 ] and an overlapping clustering algorithm [ 4 ] , which we call MMM . STATPC finds non redundant and statistically ( overlapping ) regions in high dimensional data . MMM finds overlapping clusters and automatically detects the noisy data points . To make the three algorithms comparable , MMM is used to cluster the entries in the matrix , instead of the rows . STATPC can make use of the row cluster labels if available—we give it substantial advantage by providing the true cluster labels for all the rows . For D1 ,
20
40
60
80
100
120
140
160
180
200
10
20
30
40
50
60
70
80
90
100
20
40
60
80
100
120
140
160
180
200
10
20
30
40
50
60
70
80
90
100
( a ) Original Dataset 1
( b ) Permutated Dataset 1
20
40
60
80
100
120
140
160
180
200
10
20
30
40
50
60
70
80
90
100
20
40
60
80
100
120
140
160
180
200
10
20
30
40
50
60
70
80
90
100
( c ) Original Dataset 2
( d ) Permutated Dataset 2
Figure 3 . Two Simulation Datasets .
Accuracy Dataset 1 Dataset 2 Dataset 3 Dataset 4 0.5723 BOSC 0.1286 STATPC MMM 0.3549
0.8959 0.4786 0.5287
0.6813 0.2670 0.4560
0.7767 0.5888
1
Table 1 . Clustering accuracy of BOSC , STATPC and MMM on four simulation datasets . since there is no overlap , we provide the true cluster label . For D2 , we provide the true cluster labels for the nonoverlapping rows , and one of the true cluster labels for the overlapping rows . On the contrary , the proposed algorithm does not use any form of supervision . In particular , we run kmeans on the matrix entries to get the initial estimate of the component parameter values for BOSC and MMM—in this case , means and standard deviations of each Gaussian component . However , kmeans does not capture the structure of the matrix , because it rarely assigns entries to the correct sub blocks . The noise component is initialized with the mean and standard deviation across all entries in the matrix . We use Tstart = 10 , fT = 0.67 , IT = 50 and N = 50 . We quantitatively measure the performance by calculating the clustering accuracy ( Table 1 ) . In particular , suppose c1 is the number of ground truth sub blocks , c2 is the number of output sub blocks , and aij , [ i]c1 1 is the number of entries from the ith ground truth block that are also in the jth output block . The clustering accuracy is defined as :
1 [ j]c2 j=1 maxi aij j=1 aij i=1
.
)c2 )c1
)c2
Clustering Accuracy =
4.2 Microarray Gene Expression Dataset
The microarray gene expression dataset we use consists of 4062 genes and 215 experimental conditions [ 14 ] . We first select 1000 genes that have the highest variance of expressions over the 215 conditions . We run our algorithm on this 1000 × 215 matrix and want to find subsets of genes which highly co express under subsets of conditions . The number of sub blocks is set to be 30 . The annealing parameters are set as follows : Tstart = 500 , fT = 0.67 , IT = 75
Algorithms Number of Blocks with Significant Enrichment
BOSC Plaid MOC BOC 10
13
10
8
Table 2 . BOSC finds more dense blocks with significant enrichment . and N = 100 . As a strong baseline , we use the the Plaid biclustering algorithm [ 11 ] which has been extensively used for gene expression analysis . The Plaid algorithm finds overlapping dense regions in gene expression datasets . We also compare our algorithm with a model based overlapping co clustering ( MOC ) algorithm [ 16 ] and the state of the art Bayesian co clustering ( BOC ) algorithm [ 17 ] .
To evaluate whether the dense blocks identified are meaningful from a biological perspective , we check if the genes in each dense block show significant enrichment for known biological process annotations . We make use of Gene Ontology Term Finder1 online tool , which searches for shared annotations given a set of genes and computes an associated p value . The p value measures the probability of observing a group of genes to be annotated with a certain annotation purely by chance . If genes in a dense block indeed correspond to known biological processes , we would expect a low p value . We consider an annotation to be significant if the p value associated with it is less than 10−4 . We initialize all algorithms by running kmeans on matrix entries . For co clustering algorithms , we try different combinations of row/colum cluster numbers and report the best results . The BOC algorithm estimates for each row/column the probability of belonging to each row/column cluster . We consider that a row/column belongs to a row/column cluster if it has the highest probability on that row/column cluster . The result is listed in Table 2 . BOSC identifies 24 blocks of which 13 have significant enrichment . Most of the other identified blocks have p values that are of the order of 10−4 . In contrast , among the 30 ‘layers’ found by Plaid , only 8 have significant enrichment . Among the 30 co clusters found by MOC , 10 have significant enrichment . BOC also finds 10 blocks with significant enrichment .
4.3 MovieLens Dataset
MovieLens2 is a movie rating dataset with 100,000 ratings from 943 users on 1682 movies . The ratings are on a 1 5 scale . We work with a subset with 568 users who submitted at least 50 ratings and 603 movies which have at least 50 ratings . The resulting matrix has 73544 ratings and 79 % missing entries . Since different users may have different standards and ratings can be very personal , we z score the ratings submitted by each user .
If we treat each genre as a cluster , the MovieLens dataset has naturally overlapping cluster structure , because each
1http://wwwyeastgenomeorg/cgi bin/GO/goTermFinderpl 2http://wwwgrouplensorg/taxonomy/term/14
Dataset1 Precision Recall F measure
BOSC 0.7722 0.6050 0.6784
BOC 0.7727 0.3335 0.4659
Table 3 . The performance of BOSC and BOC on the first dataset with animation , children ’s and comedy movies .
Dataset2 Precision Recall F measure
BOSC 0.6496
1
0.7876
BOC 0.6643 0.5567 0.6058 case in several problems . We would like to investigate nonparametric priors , such as the Indian Buffet Process [ 6 ] , to relax this assumption . Second , in spite of the effectiveness of the proposed algorithm , it is computationally slow for large datasets . In future , we would like to investigate faster approximate inference algorithms for the BOSC model .
Acknowledgements : The research was supported by NSF grant IIS 0812183 .
Table 4 . The performance of BOSC and BOC on the second dataset with thriller , action and adventure movies .
References movie may have several genres . Following the methodology in [ 16 ] , we then cerate 2 subsets from the dataset we use above . The first dataset contains 220 movies from 3 genres : animation , children ’s and comedy . The second dataset contains 225 movies from 3 genres : thriller , action and adventure . We run the BOSC algorithm with k = 20 on these 2 datasets trying to discover genres based on the belief that similarity in the user ratings gives an indication about whether the movies are in related genres . Since the BOC algorithm [ 17 ] can handle datasets with missing entries , we use it as the baseline . We initialize two algorithms by running kmeans on the matrix entries . The annealing parameters are the same as those in the gene expression experiment . We compare pairwise precision , recall and F measure over movies . Two movies are in a pair if they belong to the same cluster/genre . Precision , recall and F measure are calculated as follows :
Precision =
Recall =
Number of Correctly Identified Pairs
Number of Identified Pairs
Number of Correctly Identified Pairs
,
,
Number of Pairs in the Original Dataset
F measure = 2 × Precision × Recall
.
Precision + Recall
For the BOC algorithm , we try different combinations of the number of row clusters and column clusters and report the best result . The result is listed in Table 3 and 4 . Two algorithms have comparable performance on precision , but BOSC has higher recall and F score .
5 Conclusions
In this paper , we have presented a systematic hierarchical generative model and corresponding algorithms for discovering uniform sub blocks in a given data matrix . Preliminary empirical evidence goes significantly in favor of the proposed algorithm . Perhaps more importantly , the BOSC model introduces a substantially novel way to approach the problem . There are at least two important future research directions . First , the BOSC model assumes that the number of co clusters k is known , which is typically not the
[ 1 ] C . C . Aggarwal , J . L . Wolf , P . S . Yu , C . M . Procopiuc , and J . S . Park . Fast algorithms for projected clustering . SIGMOD , 1999 .
[ 2 ] R . Agrawal , J . Gehrke , D . Gunopulos , and P . Raghavan . Automatic subspace clustering of high dimensional data for data mining applications . SIGMOD , 1998 .
[ 3 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent dirichlet allocation . JMLR , 3:993–1022 , 2003 .
[ 4 ] Q . Fu and A . Banerjee . Multiplicative mixture models for overlapping clustering . ICDM , 2008 .
[ 5 ] S . Geman and D . Geman . Stochastic relaxation , Gibbs distributions , and the Bayesian restoration of images . PAMI , 6:721–741 , 1984 .
[ 6 ] T . Griffiths and Z . Ghahramani . Infinite latent feature mod els and the Indian buffet process . NIPS , 2005
[ 7 ] T . L . Griffiths and M . Steyvers . Finding scientific topics .
PNAS , 101(1):5228–5225 , 2004 .
[ 8 ] J . A . Hartigan . Direct clustering of a data matrix . JASA ,
67(337):123–129 , 1972 .
[ 9 ] K . A . Heller and Z . Ghahramani . A nonparametric bayesian approach to modeling overlapping clusters . AISTATS , 2007 . [ 10 ] S . Kirkpatrick , C . D . Gelatt , and M . P . Vecchi . Optimization by Simulated Annealing . Science , 220:671–680 , 1983 .
[ 11 ] L . Lazzeroni and A . Owen . Plaid models for gene expression data . Statistica Sinica , 12:61–86 , 2002 .
[ 12 ] G . Mclachlan and D . Peel . Finite Mixture Models . Wiley Series in Probability and Statistics . Wiley Interscience , 2000 . [ 13 ] G . J . McLachlan and T . Krishnan . The EM algorithm and
Extensions . Wiley Interscience , 1996 .
[ 14 ] S . Mnaimneh , A . P . Davierwala , J . Haynes , J . Moffat , WT Peng , W . Zhang , X . Yang , J . Pootoolal , G . Chua , and A . Lopez . Exploration of essential gene functions via titratable promoter alleles . Cell , 118(1):31–44 , 2004 .
[ 15 ] G . Moise and J . Sander . Finding non redundant , statistically significant regions in high dimensional data : a novel approach to projected and subspace clustering . SIGKDD , 2008 .
[ 16 ] M . Shafie and E . Milios . Model based overlapping co clustering . SDM , 2006 .
[ 17 ] H . Shan and A . Banerjee . Bayesian co clustering . ICDM ,
2008 .
[ 18 ] K . Y . Yip , D . W . Cheung , and M . K . Ng . Harp : A practical projected clustering algorithm . TKDE , 16(11):1387–1397 , 2004 .
