Learning the Shared Subspace for Multi Task Clustering and Transductive Transfer
Classification
Quanquan Gu and Jie Zhou
State Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology ( TNList )
Department of Automation , Tsinghua University , Beijing , China 100084 gqq03@mailstsinghuaeducn , jzhou@tsinghuaeducn the underlying relation and treat
Abstract—There are many clustering tasks which are closely related in the real world , eg clustering the web pages of different universities . However , existing clustering approaches neglect these clustering tasks either individually or simply together . In this paper , we will study a novel clustering paradigm , namely multi task clustering , which performs multiple related clustering tasks together and utilizes the relation of these tasks to enhance the clustering performance . We aim to learn a subspace shared by all the tasks , through which the knowledge of the tasks can be transferred to each other . The objective of our approach consists of two parts : ( 1 ) Within task clustering : clustering the data of each task in its input space individually ; and ( 2 ) Crosstask clustering : simultaneous learning the shared subspace and clustering the data of all the tasks together . We will show that it can be solved by alternating minimization , and its convergence is theoretically guaranteed . Furthermore , we will show that given the labels of one task , our multi task clustering method can be extended to transductive transfer classification ( aka cross domain classification , domain adaption ) . Experiments on several cross domain text data sets demonstrate that the proposed multi task clustering outperforms traditional singletask clustering methods greatly . And the transductive transfer classification method is comparable to or even better than several existing transductive transfer classification approaches . Keywords multi task clustering ; transductive transfer classification ; multi task learning ; transfer learning ; cross domain classification ; domain adaption
I . INTRODUCTION
Clustering has a long history in the machine learning literature . It aims to partition data points into groups , so that the data points in the same group are relatively similar , while the data points in different groups are relatively dissimilar . In the past decades , incorporating prior knowledge into clustering has witnessed increasing interest , eg semisupervised clustering [ 1 ] [ 2 ] [ 3 ] [ 4 ] [ 5 ] and co clustering [ 6 ] [ 7 ] [ 8 ] [ 9 ] . However , all these methods are limited to a single task , where iid assumption of the data samples holds . We refer them as single task clustering .
There are many different but related data sets in real applications . For example , we have web pages from 4 universities , eg Cornell , Texas , Wisconsin and Washington . And we are going to cluster the web pages of each university into 7 categories , eg student , faculty , staff , department , course , project and the other . In this scenario , clustering the web pages of each university can be seen as a task . Our intuition tells us that the 4 clustering tasks are related , since the sources and contents of their data are similar . However , the distributions of their data should be different , since different universities exhibit different features . Imagine that if we have limited web pages in one university , typical clustering methods may fail to discover the correct clusters . In this case , one may argue to use the web pages from the other universities as an auxiliary data to inform the correct clusters . However , simply combining them together followed with traditional single task clustering approach does not necessarily lead to performance improvement , because their distributions are different , which violates the iid assumption in single task clustering . To address this problem , new clustering paradigm is imperative , which can utilize the relation of different tasks to enhance clustering as well as overcome the non iid problem .
In this paper , based on the observations mentioned above , we will study a novel clustering paradigm , namely multitask clustering , which can exploit the knowledge shared by multiple related tasks . It falls in the field of multitask learning [ 10 ] [ 11 ] [ 12 ] [ 13 ] [ 14 ] [ 15 ] , which says learning multiple related tasks together may achieve better performance than learning these tasks individually , provided that we can exploit the underlying relation . The assumption of our multi task clustering is that there is a common underlying subspace shared by the multiple related tasks . This underlying subspace can be seen as a new feature representation , in which the data distributions of the related tasks are close to each other . Hence single task clustering algorithm can be applied in this shared subspace . Similar assumption has also been made in several multi task classification approaches [ 11 ] [ 13 ] [ 14 ] [ 15 ] . Based on the above assumption , we propose a multi task clustering method . It aims to learn a subspace shared by all the tasks , through which the knowledge of one task can be transferred to another . And the objective of our approach consists of two parts : ( 1 ) Within task clustering : clustering the data of each task in its input space individually ; and ( 2 ) Crosstask clustering : simultaneous learning the shared subspace the tasks . Our approach and clustering the data of all not only utilizes the knowledge in each individual task as traditional clustering method does , but also make use of the knowledge shared by the related tasks which may benefit the clustering performance . We will show that it can be solved via alternating minimization , and its convergence is theoretically guaranteed . To the best of our knowledge , this is the first work addressing multi task clustering .
Furthermore , we will show that provided with the labels of one task , our multi task clustering method turns out to be a transductive transfer classification method ( aka crossdomain classification or domain adaption ) , in which the label of a source task ( in domain ) is available as prior knowledge , and we aim to utilize this prior knowledge to predict the labels of the data in a related target task ( out of domain ) . Experiments on several cross domain text data sets demonstrate that the proposed multi task clustering method outperforms traditional single task clustering methods greatly . And the transductive transfer classification method is comparable to or even better than several existing transductive transfer classification approaches .
The remainder of this paper is organized as follows . In Section II , we will review some related works . In Section III we will propose the multi task clustering algorithm . In Section IV , we will extend the multi task clustering method to transfer clustering setting . Experiments on text data sets are demonstrated in Section V . Finally , we draw a conclusion in Section VI and point out the future works .
II . RELATED WORKS
In this section , we will review some works related with ours .
A . Multi Task Learning
Empirical work has shown that learning multiple related tasks from data simultaneously can be advantageous in terms of predictive performance , relative to learning these tasks independently . This motivates multi task learning [ 10 ] [ 11 ] [ 12 ] [ 13 ] [ 14 ] [ 15 ] . However , existing multi task learning methods all tackle classification , in which each task has both labeled and unlabeled data , and the goal is to predict the class labels of unlabeled data in each task by utilizing within task and cross task knowledge . In this paper , we consider multi task clustering , where the data in each task are all unlabeled , and it aims at predicting the cluster labels of the data in each task .
B . Transfer Learning
Transfer learning [ 16 ] [ 17 ] is closely related with multitask learning . It tackles the transfer of knowledge across tasks , domains , categories and distributions that are similar but not the same . In this paper , we refer task and domain as the same thing . Transfer learning is closely related with multi task learning , with the difference that in multi task learning , the learner focuses on enhancing the performance of all the tasks , while in transfer learning , the learner only focuses on improving the performance of a so called target task ( out of domain ) by using the knowledge from a socalled source task ( in domain ) . Transfer learning can be categorized as ( 1 ) inductive transfer : there are a few labeled data in the target task , while there are a large amount of labeled [ 18 ] [ 19 ] [ 20 ] or unlabeled [ 21 ] data in the source task , ( 2 ) transductive transfer : there are no labeled data in the target task , while there are large amount of labeled data in the source task [ 22 ] [ 23 ] [ 24 ] [ 25 ] , this is also called cross domain classification or domain adaption , and ( 3 ) Unsupervised transfer : there are no labeled data in the target task , while there are large amount of unlabeled data in the source task [ 26 ] . In our study , we focus on transductive transfer classification , which belongs to the second category .
C . Clustering with Background and Prior Knowledge
Improving clustering performance using the background and prior knowledge has witnessed increasing interest in the past decade . One direction is co clustering [ 6 ] [ 7 ] [ 8 ] [ 9 ] , which clusters the data and features simultaneously to enhance the clustering performance . Another direction is semi supervised clustering [ 1 ] [ 2 ] [ 3 ] [ 4 ] [ 5 ] , which incorporates pairwise constraints , eg must link and cannotlink constraints , to assist clustering . Both co clustering and semi supervised clustering use either the background or prior knowledge within a single task . However , multi task clustering exploits both in task and out of task knowledge .
D . Semi Supervised Learning
In many practical machine learning problems , the acquisition of sufficient labeled data is often expensive and/or time consuming . On the contrary , in many cases , large number of unlabeled data are far easier to obtain . Consequently , semisupervised learning [ 27 ] [ 28 ] [ 29 ] , which aims to learn from both labeled and unlabeled data points , has received significant attention in the past decade . Semi supervised learning is different from transductive transfer classification . In semisupervised learning , the labeled and unlabeled samples are drawn from the same task , so their distributions are the same . However , in transductive transfer classification , the labeled samples are from the source task , while the unlabeled samples are from the target task . So their distributions are different .
III . MULTI TASK CLUSTERING
In this section , we first present the problem setting of multi task clustering . Then we propose a multi task clustering method and the optimization algorithm , followed with its convergence analysis .
A . Problem Formulation
1 , x(k )
2 , . . . , x(k )
Suppose we are given m clustering tasks , each with a nk } ∈ set of data points , ie X ( k ) = {x(k ) Rd , 1 ≤ k ≤ m , where nk is the number of data points in the k th task . The goal of multi task clustering is to partition the data set X ( k ) of each task into c clusters {C(k ) j }c j=1 . Note that we assume the dimensionality of the feature vector of all the tasks is the same , ie d . It is appropriate since we could augment the feature vectors of all the tasks to make the dimensionality same . In fact , the bagof words document representation used in our experiments implicitly does the augmentation . Moreover , we assume that the number of clusters in each task is the same , ie c1 = c2 = . . . = cm = c , which is also assumed in existing multi task learning literature .
B . Objective
Let us consider the case of single task clustering first . Take the k th task for example . We are going to partition the k th data set into c clusters . The classical K means algorithm achieves this goal by minimizing the following objective
Jst =
||x(k ) i − m(k ) j
||2 2 ,
( 1 ) j
( k ) j=1 i ∈C(k ) x where ||·||2 is 2 norm and m(k ) in the k th task . If we define M(k ) = [ m(k ) Rd×c , then Eq ( 1 ) can be rewritten as j is the mean of cluster C(k ) ] ∈
1 , . . . , m(k ) c j
Jst = ||X(k ) − M(k)P(k)T||2 F , st P(k ) ∈ {0 , 1}nk×c
1 , . . . , x(k )
( 2 ) nk ] , 1 ≤ k ≤ m , || · ||F is where X(k ) = [ x(k ) Frobenius norm and P(k ) ∈ {0 , 1}nk×c is called partition matrix , which represents the clustering assignment , such that P(k ) ij = 0 otherwise . This is also known as hard clustering , ie the cluster assignment is binary . belongs to cluster C(k ) ij = 1 if x(k ) and P(k ) j i c subspace . Then it is formulated as minimizing
Jmt = λ
||X(k ) − M(k)P(k)T||2
F m k=1 m
||WT X(k ) − MP(k)T||2
+ ( 1 − λ ) st WT W = I , P(k ) ∈ {0 , 1}nk×c k=1
F
( 3 ) where λ ∈ [ 0 , 1 ] is a regularization parameter balancing the clustering in the input space and the clustering in the shared subspace , I is an identity matrix , and M = [ m1 , . . . , mc ] ∈ Rm×c with mj is the mean of cluster Cj of all the tasks in the shared subspace .
The objective in Eq ( 3 ) consists of two terms . The first term is Within task clustering , which includes k independent k means clustering of each task in the input space . The second term is Cross task clustering , which simultaneously learns the shared subspace and clusters the data of all the tasks together in the shared subspace . It is worth noting that the second term is similar with clustering the data of all the tasks together via Adaptive Subspace Iteration ( ASI ) clustering method [ 30 ] . The first term and the second term are intertwined through the partition matrices . When letting λ = 1 , Eq ( 3 ) degenerates to m independent K means clustering . And When letting λ = 0 , Eq ( 3 ) turns out to be clustering data of all the tasks via ASI . In general case , the more related the tasks are , the smaller λ we will set .
By its definition , the elements in P(k ) can only take binary values , which makes the minimization in Eq ( 3 ) very difficult , therefore we relax P(k ) into nonnegative continuous domain . Then the objective of multi task clustering in Eq ( 3 ) turns out to be
Jmt = λ
||X(k ) − M(k)P(k)T||2
F m k=1 m
||WT X(k ) − MP(k)T||2
F
+ ( 1 − λ ) st WT W = I , P(k ) ≥ 0 . k=1
( 4 )
When it comes to multi task clustering setting , we are going to learn a shared subspace , which is obtained by an orthonormal projection W ∈ Rd×l , across all the related tasks , in which we perform all the clustering tasks together . This shared subspace can be seen as a new feature space , in which the data distribution from all the tasks are similar with each other . As a result , we can cluster them together in this shared subspace using traditional single task clustering algorithm , ie K means . Furthermore , we add a constraint that the clustering result of each task in the shared subspace is the same as that in the input subspace , which intertwines the clustering in the input space and clustering in the shared
We call Eq ( 4 ) Learning the Shared Subspace for Multi Task Clustering ( LSSMTC ) .
C . Optimization is with respect
As we see , minimizing Eq ( 4 ) to M(k ) , P(k ) , W and M . And we cannot give a closed form solution . In the following , we will present an alternating minimization algorithm to optimize the objective . In other words , we will optimize the objective with respect to one variable when fixing the other variables .
1 ) Computation of M : Given W and P(k ) , optimizing
4 ) Computation of W : Given M , M(k ) , P(k ) , optimizing
Eq ( 4 ) with respect to M is equivalent to optimizing
Eq ( 4 ) with respect to W is equivalent to optimizing
J1 =
||WT X(k ) − MP(k)T||2
F
J4 =
||WT X(k ) − MP(k)T||2
F m m k=1 and P
( 5 )
=
( 6 )
= ||WT X − MPT||2 F [ X(1 ) , . . . , X(m ) ] = where X [ P(1)T , . . . , P(m)T ]T .
Setting ∂J1
∂M = 0 , we obtain
M = WT XP(PT P)−1 .
2 ) Computation of M(k ) : Given P(k ) , optimizing Eq ( 4 ) with respect to M(k ) is equivalent to optimizing
J2 = ||X(k ) − M(k)P(k)T||2
F
Setting ∂J2
∂M(k ) = 0 , we obtain
M(k ) = X(k)P(k)(P(k)T P(k))−1 .
( 7 )
( 8 )
3 ) Computation of P(k ) : Given W , M , M(k ) , optimizing
Eq ( 4 ) with respect to P(k ) is equivalent to optimizing
J3 = λ||X(k ) − M(k)P(k)T||2
F
F
+ ( 1 − λ)||WT X(k ) − MP(k)T||2 st P(k ) ≥ 0 ,
( 9 ) For the constraint P(k ) ≥ 0 , we cannot get a closedform solution of P(k ) . In the following , we will present an iterative solution . We introduce the Lagrangian multiplier γ ∈ Rnk×c , and the Lagrangian function is L(P(k ) ) = λ||X(k ) − M(k)P(k)T||2
+ ( 1 − λ)||WT X(k ) − MP(k)T||2 − tr(γP(k)T )
F
F
( 10 )
Setting ∂L(P(k ) )
∂P(k ) = 0 , we obtain
γ = −2A + 2P(k)B
( 11 ) where A = λX(k)T M(k ) + ( 1 − λ)X(k)T WM and B = λM(k)T M(k ) + ( 1 − λ)MT M .
Using the Karush Kuhn Tucker condition [ 31 ] γijP(k ) ij =
0 , we get
[ −A + P(k)B]ijP(k ) ij = 0
( 12 )
[ A− + P(k)B+ − A+ − P(k)B−]ijP(k )
Introduce A = A+ − A− and B = B+ − B− where A+ ( |Aij|+Aij)/2 and A− ij = ij = ( |Aij|−Aij)/2 [ 32 ] , we obtain ( 13 ) ij = 0
Eq ( 13 ) leads to the following updating formula [ A+ + P(k)B−]ij [ A− + P(k)B+]ij ij ← P(k ) P(k ) ij
( 14 ) k=1
= ||WT X − MPT||2 st WT W = I
F
( 15 )
= where X [ P(1)T , . . . , P(m)T ]T .
= Substituting M = WT XP(PT P)−1 into the above
[ X(1 ) , . . . , X(m ) ] and P equation , we obtain
J5 = tr(WT X(I − P(PT P)−1PT )XT W ) st WT W = I
( 16 )
It is easy to show that the optimal W minimizing Eq ( 16 ) is composed of the eigenvectors of X(I− P(PT P)−1PT )XT corresponding to the l smallest eigenvalues .
In summary , we present
Eq ( 4 ) in Algorithm 1 . the algorithm of optimizing
Algorithm 1 Learning the Shared Subspace for Multi Task Clustering ( LSSMTC ) the dimensionality of the k=1 ,
Input:m tasks , {X(k)}m shared subspace l , maximum number of iterations T ; Output:Partitions P(k ) ∈ Rn×c , 1 ≤ k ≤ m ; Initialize t = 0 and P(k ) , 1 ≤ k ≤ m using K means ; Initialize W ∈ Rd×l using any orthonormal matrix . while not convergent and t ≤ T do
M = WT ( XP)(PT P)−1 ; for k = 1 To m do
[ A++P(k)B−]ij [ A−+P(k)B+]ij
Compute M(k ) = X(k)P(k)(P(k)T P(k))−1 ; Update ij ← P(k ) P(k ) end for Compute Wij by eigen decomposition of X(I − P(PT P)−1PT )XT ; t = t + 1 end while ij
;
D . Convergence Analysis
In the following , we will investigate the convergence of Algorithm 1 . We use the auxiliary function approach [ 33 ] to analyze the multiplicative updating formulas . Here we first introduce the definition of auxiliary function [ 33 ] . Definition III1 [ 33 ] Z(h , h ) is an auxiliary function for F ( h ) if the conditions
Z(h , h ) ≥ F ( h ) , Z(h , h ) = F ( h ) , are satisfied .
Lemma III2 [ 33 ] If Z is an auxiliary function for F , then F is non increasing under the update h(t+1 ) = arg min h
Z(h , h(t ) )
Proof : F ( h(t+1 ) ) ≤ Z(h(t+1 ) , h(t ) ) ≤ Z(h(t ) , h(t ) ) =
F ( h(t ) ) Lemma III3 [ 32 ] For any nonnegative matrices A ∈ Rn×n , B ∈ Rk×k , S ∈ Rn×k,S ∈ Rn×k , and A , B are symmetric , then the following inequality holds n k i=1 p=1
Theorem III4 Let
( ASB)ipS2 ip
S ip
≥ tr(ST ASB )
J(P(k ) ) = tr(P(k)BP(k)T − 2AP(k)T )
( 17 )
Then the following function
Z(P(k ) , P(k ) ) =
−
( P(k)B+)ijP(k)2 ij ij
P(k ) ij P(k ) jkP(k ) B− ik ( 1 + log ij P(k ) P(k ) ij P(k ) P(k ) ik ik
)
− 2 ijP(k ) A+
+ 2
A− ij
P(k ) ij P(k ) ij
) ij ( 1 + log ij + P(k)2 P(k)2 2P(k ) ij ij ij ijk ij ij is an auxiliary function for J(P(k) ) . Furthermore , it is a convex function in P(k ) and its global minimum is
P(k ) ij = P(k ) ij
[ A+ + P(k)B−]ij [ A− + P(k)B+]ij
( 18 )
Proof : Please see Appendix .
Theorem III5 Updating P(k ) using Algorithm 1 will monotonically decrease the value of the objective in Eq ( 4 ) , the objective is invariant under the updating if and only if P(k ) is at a stationary point .
Proof : By Lemma III.2 and Theorem III.4 , we can get that J(P(k)0 ) = Z(P(k)0 , P(k)0 ) ≥ Z(P(k)1 , P(k)0 ) ≥ J(P(k)1 ) ≥ . . . So J(P(k ) ) is monotonically decreasing . Since J(P(k ) ) is obviously bounded below , we prove this theorem .
In addition to Theorem III.5 , since the computation of W in Eq ( 16 ) also monotonically decreases the value of the objective in Eq ( 4 ) , Algorithm 1 is guaranteed to converge .
IV . TRANSDUCTIVE TRANSFER CLASSIFICATION
2 , . . . , x(k )
In this section , we will show that given the labels of one task , the proposed multi task clustering method turns out to be a transductive transfer classification method . For simplicity , we consider the 2 tasks case , X ( k ) = {x(k ) nk } , k = 1 , 2 , where nk is the number 1 , x(k ) of data points in the k th task . Without loss of generality , we assume the label of the 1st task is given , and we are going to predict the labels of the data in the 2nd task . This problem is exactly transductive transfer classification , which is also known as domain adaption or cross domain classification . We call the 1st task source task ( in domain ) , and the 2nd task target task ( out of domain ) . Denote X(k ) = [ x(k ) nk ] , k = 1 , 2 . Again , we assume that the number of classes in each task is the same , ie c1 = c2 = c . Note that it is trivial to generalize our transductive transfer classification method from 1 source task to more than 1 source task .
1 , . . . , x(k )
Based on the above discussion , our multi task clustering method can be extended to transductive transfer classification as follows ,
Jtc = λ||X(2 ) − M(2)P(2)T||2
F
2
||WT X(k ) − MP(k)T||2
+ ( 1 − λ ) st WT W = I , P(2 ) ∈ {0 , 1}n2×c k=1
F
( 19 ) where the first term is clustering in the input space of the target task , the second and the third term are clustering of the source and the target task together in the shared subspace , λ ∈ [ 0 , 1 ] is a regularization parameter balancing the clustering in the input space and the clustering in the shared subspace . It should be noted that P(1 ) in Eq ( 19 ) is a constant since the label of the source task has been known as prior knowledge .
Again , we relax P(2 ) into nonnegative continuous domain . Then the objective of transductive transfer classification in Eq ( 19 ) turns out to be
Jtc = λ||X(2 ) − M(2)P(2)T||2
F
2
||WT X(k ) − MP(k)T||2
F
+ ( 1 − λ ) st WT W = I , P(2 ) ≥ 0 , k=1
( 20 )
We call Eq ( 20 ) Learning the Shared Subspace for Transductive Transfer Classification ( LSSTTC ) .
Since the optimization of Eq ( 20 ) is very similar with that of Eq ( 4 ) , we omit the derivation of the optimization algorithm , and present it in Algorithm 2 directly .
The convergence of Algorithm 2 is also theoretically guaranteed . The proof of convergence is very similar with that of Algorithm 1 .
Algorithm 2 Learning the Shared Subspace for Transductive Transfer Classification ( LSSTTC )
Input:X(1 ) , P(1 ) , X(2 ) , the dimensionality of the shared subspace l , maximum number of iterations T ; Output:Partitions P(2 ) ∈ Rn×c ; Initialize P(2 ) using K means ; Initialize W using any orthonormal matrix . while not convergent and t ≤ T do Compute M = WT XP(PT P)−1 ; Compute M(2 ) = WT X(2)P(2)(P(2)T P(2))−1 ; ij ← P(2 ) Update P(2 ) Compute Wij by eigen decomposition of X(I − P(PT P)−1PT )XT ;
[ A++P(2)B−]ij [ A−+P(2)B+]ij ij
; end while
V . EXPERIMENTS
In our experiments , we will evaluate the proposed meth ods on several cross domain text data sets .
A . Evaluation Metrics
To evaluate the clustering results , we adopt the performance measures used in [ 34 ] . These performance measures are the standard measures widely used for clustering .
Clustering Accuracy : Clustering Accuracy discovers the one to one relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class . Clustering Accuracy is defined as follows : n
Acc = i=1 δ(map(ri ) , li ) n
,
( 21 ) where ri denotes the cluster label of xi , and li denotes the true class label , n is the total number of documents , δ(x , y ) is the delta function that equals one if x = y and equals zero otherwise , and map(ri ) is the permutation mapping function that maps each cluster label ri to the equivalent label from the data set .
Normalized Mutual Information : The second measure is the Normalized Mutual Information ( NMI ) , which is used for determining the quality of clusters . Given a clustering result , the NMI is estimated by c c c i=1
( i=1 ni log ni c j=1 ni,j log ni,j ni ˆnj n )( j=1 ˆnj log ˆnj n )
N M I =
,
( 22 ) where ni denotes the number of data contained in the cluster Ci(1 ≤ i ≤ c ) , ˆnj is the number of data belonging to the Lj(1 ≤ j ≤ c ) , and ni,j denotes the number of data that are in the intersection between the cluster Ci and the class Lj . The larger the NMI is , the better the clustering result will be .
To evaluate the classification results , we use the classifi cation accuracy .
B . Data Sets
In order to evaluate the proposed methods , we use 2 text data sets , which are widely used in cross domain classification literature [ 22 ] [ 23 ] [ 25 ] .
WebKB1 The WebKB data set contains webpages gathered from university computer science departments ( Cornell , Texas , Washington , Wisconsin ) . There are about 8280 documents and they are divided into 7 categories , and we choose student , faculty , course and project these four most populous entity representation categories for clustering , named WebKB4 . We consider clustering the web pages of each university as one task . Therefore , we have 4 tasks .
20Newsgroup2 The 20 Newsgroups is a collection of approximately 20000 newsgroup documents , partitioned across 20 different newsgroups nearly evenly . We generate 2 crossdomain data sets , ie RecvsTalk and CompvsSci , for evaluating multi task clustering and transductive transfer classification methods . In detail , two top categories are chosen , one as positive and the other as negative . Then the data are split based on sub categories . The task is defined as top category classification . The splitting ensures the data in different tasks are related but different , since they are drawn from the same top category but different sub categories . The detailed constitutions of the 2 data sets are summarized in Table I .
CONSTITUTION OF THE 2 DATA SETS GENERATED FROM 20NEWSGROUP
Table I
Data set
RecvsTalk
CompvsSci
Task id Task 1 Task 2 Task 1 Task 2
Class 1 rec.autos recsportbaseball composms windowsmisc compsysmachardware
Class 2 talkpoliticsguns talkpoliticsmideast sci.crypt sci.space
Table.II summarizes the characteristics of the 3 data sets used in this experiment .
Table II
DESCRIPTION OF THE DATA SETS
Data set
WebKB4
RecvsTalk
CompvsSci
Task id Task 1 Task 2 Task 3 Task 4 Task 1 Task 2 Task 1 Task 2
#Sample
#Feature
#Class
227 250 248 304 1844 1545 1875 1827
2000 2000 2000 2000 2000 2000 2000 2000
4 4 4 4 2 2 2 2
C . Experiment 1 : Multi Task Clustering
In this experiment , we study multi task clustering . We assume that the labels of all the tasks in each data set
1http://wwwcscmuedu/afs/cscmuedu/project/theo 20/www/data/ 2http://peoplecsailmitedu/jrennie/20Newsgroups/ are unknown . We compare the proposed multi task clustering method with typical single task clustering methods , eg Kmeans ( KM ) , Principal Component Analysis ( PCA)+Kmeans ( PCAKM ) , Normalized Cut ( NCut ) [ 35 ] and adaptive subspace iteration ( ASI ) [ 30 ] . Note that Kmeans can be seen as a special case of the proposed method with λ = 1 . We also present the experimental results of clustering the data of all the tasks together using Kmeans , PCA+Kmeans , NCut and ASI . Note that clustering the data of all the tasks together via ASI corresponds to the proposed method with λ = 0 . the reduced dimension of PCA is set
1 ) Methods & Parameter Settings : We set the number of clusters equal to the true number of classes for all the clustering algorithms . For NCut , the scale parameter of Gaussian kernel is set by the grid {10−3 , 10−2 , 10−1 , 1 , 10 , 102 , 103} . For PCAKM , to least 95 % of the the minimal number that preserves at information . For LSSMTC , we set l by searching the grid {2 , 22 , 23 , 24} . And the regularization parameter λ is set by searching the grid {0.25 , 0.5 , 075} Under each parameter setting , we repeat clustering 5 times , and the mean result as well as the standard deviation is computed . We report the mean and standard deviation result corresponding to the best parameter setting for each method to compare with each other . Since our algorithm is iterative , in our experiments , we prescribe the maximum number of iterations as T = 20 .
2 ) Clustering Results : We repeat each experiment 5 times , and the average results are shown in Table III , Table IV and Table V .
” All ” refers to clustering the data of all the tasks together . We can see that LSSMTC indeed improves the clustering result , and outperforms Kmeans greatly , which is its singletask degeneration . This improvement owes to exploiting the relation among the tasks by learning the shared subspace . In Task 2 of WebKB4 data set , NCut achieves better clustering result than our method . This is because NCut considers the geometric structure in the data , which is suitable for data sampled from manifold , while our method does not take this into account .
In addition , it is worthwhile noticing that although our method involves combining all the tasks together and doing dimensionality reduction , it far exceeds these simple operations . As we see , simply clustering the data of all the tasks together does not necessarily improve the clustering result , because the data distributions of different tasks are different , and combining the data together directly will violate the iid assumption in single task clustering . Moreover , the clustering result of doing dimensionality reduction followed with clustering is also not as good as LSSMTC , because it treats learning the subspace and clustering independently , while learning the subspace and clustering could benefit from each other .
D . Experiment 2 : Transductive Transfer Classification
In this experiment , we study transductive transfer classification . We do experiments on any two tasks of each data set . One task is used as source task , in which the class labels of all the data are known . The other is used as target task , where the class labels of all the data are unknown and to be predicted . We compare the proposed transductive transfer classification method with support vector machine ( SVM ) [ 36 ] , three semi supervised learning methods , ie Gaussian Field Harmonic Function ( GFHF ) [ 27 ] , Learning with Local and Global Consistency ( LLGC ) [ 28 ] and transductive SVM ( TSVM ) [ 29 ] . We also compare it with several existing transductive transfer classification methods , Co Clustering based Classification ( CoCC ) [ 22 ] and Cross Domain Spectral Classification ( CDSC ) [ 23 ] .
0 , 2−1σ2
0 , 2−2σ2
1 ) Methods & Parameter Settings : For SVM , TSVM , CDSC , since they are designed originally for binary classification , we address the multi class classification via 1vs rest strategy . For SVM , it is trained on the source task , and tested on the target task . For TSVM , GFHF , LLGC and our method , they are trained using both labeled ( source task ) and unlabeled ( target task ) data , and are tested on the unlabeled data . SVM is implemented by LibSVM3 [ 37 ] , while TSVM is implemented by SVMLight6.01 4 , and linear kernel is used . The implementation of GFHF is the same as in [ 27 ] . The width of the Gaussian similarity is set via the 0} , where σ0 grid {2−3σ2 is the mean distance between any two samples in the training set . And the size of neighborhood is searched by the grid {5 , 10 , 50 , 80 , n − 1} . The implementation of LLGC is the same as in [ 28 ] , in which the width of the Gaussian similarity and the size of neighborhood are also determined the same as that in GFHF , and the regularization parameter is set by searching the grid {0.1 , 1 , 10 , 100} . The implementation and parameter settings of CoCC and CDSC are the same as that in their papers . For LSSTTC , we set l by searching the grid {100 , 200 , . . . , 900 , 1000} . And the regularization parameter λ is set by searching the grid {0.25 , 0.5 , 075} Under each parameter setting , we repeat LSSTTC 5 times , and the mean result is computed .
0 , σ2
0 , 2σ2
0 , 22σ2
0 , 23σ2
2 ) Classification Results : The classification results are reported in Table VI , Table VII and Table VIII . It is obvious that the proposed transductive transfer classification method outperforms traditional single task classification methods , eg SVM , TSVM , GFHF and LLGC , greatly on most transfer settings . This improvement is due to the prior knowledge , ie label information , in the related source task which is transferred to the target task by our method . It is also comparable to or even better than existing transductive transfer classification methods , eg CoCC and CDSC . Note that in Task 4 → Task 1 and Task 4 → Task 2 settings of
3http://wwwcsientuedutw/ cjlin/libsvm/ 4http://svmlightjoachimsorg/
Table III
CLUSTERING RESULTS ON WEBKB4
Method
KM
PCAKM
NCut ASI
All KM
All PCAKM
All NCut All ASI LSSMTC
Task 1
Task 2
Task 3
Task 4
Acc
05784±00996 05938±01006 04907±00188 05119±00612 05476±00837 05912±00841 05683±00000 05731±00581 06247±00336
NMI
02760±00753 03085±00822 02816±00342 02374±00365 01846±00736 02318±00907 02505±00000 01209±00515 03369±00144
Acc
05670±00697 05616±00595 06720±00000 05512±00613 05944±00447 05952±00565 05920±00000 05384±00232 06304±00364
NMI
02552±00551 02446±00534 03632±00000 02947±00438 03178±00514 02059±00182 02721±00000 02296±00122 03416±00101
Acc
05671±00903 06105±00659 05282±00000 06097±00569 05980±00914 05718±00789 04960±00000 05044±00790 06677±00408
NMI
02814±00603 03224±00588 03466±00000 02933±00265 02147±01255 01413±01206 02340±00000 02965±00879 03552±00147
Acc
06770±00773 06882±00824 06079±00015 06418±00254 05898±01158 06753±01060 05132±00000 06234±00845 07329±00333
NMI
03552±00949 04187±00975 02555±00068 03591±00157 03108±01135 03812±01094 02620±00000 03363±01297 04240±00096
CLUSTERING RESULTS ON RECVSTALK
Table IV
Task 1
Task 2
Acc
Method
KM
PCAKM
NCut ASI
06467±00382 06757±00015 06779±00000 06303±00607 06551±00382 All PCAKM 06765±00348 06866±00000 All NCut 06241±00585 All ASI 08433±00804 LSSMTC
All KM
NMI
01884±00307 02122±00020 02216±00000 01311±01012 01742±00160 01796±00184 02604±00000 01133±00660 04306±00582
Acc
06454±00967 06344±01131 06887±00000 06170±01464 05898±00271 06061±00262 06188±00000 05683±00376 07895±00827
NMI
01568±01379 01416±01602 02122±00000 01401±01893 00558±00443 00770±00381 00783±00000 00295±00357 03473±00835
CLUSTERING RESULTS ON COMPVSSCI
Table V
Task 1
Task 2
Acc
Method
KM
PCAKM
NCut ASI
06130±00202 06073±00190 06683±00000 07404±01615 06656±00727 All PCAKM 06659±00726 06352±00000 All NCut 06241±00585 All ASI 08801±00076 LSSMTC
All KM
NMI
01727±00228 01661±00214 02327±00000 03444±02041 02330±00924 02334±00922 01941±00000 01133±00660 05376±00155
Acc
06716±00000 06716±00000 06678±00000 06657±00021 05407±00211 05409±00212 05506±00000 05683±00376 08016±00614
NMI
02087±00000 02087±00000 01694±00000 01282±00098 00532±00191 00536±00193 00627±00000 00295±00357 03347±01407
WebKB4 data set , ” negative transfer ” [ 17 ] occurred , where transfer learning lowers the learning performance .
VI . CONCLUSIONS AND FUTURE WORKS
The contribution of this paper includes the following aspects . First of all , we initiate a novel clustering paradigm , ie multi task clustering , which utilizes the relation among multiple clustering tasks and outperforms traditional singletask clustering methods greatly . As far as we know , this is the first work addressing multi task clustering . Secondly , we extend our multi task clustering method to transductive transfer classification , which is comparable to or even better than existing methods .
In our future work , we will extend our method to take into account geometric structure as in [ 34 ] [ 38 ] .
Doctoral Program of Higher Education . We would like to thank the anonymous reviewers for their helpful comments . And we especially thank one of the anonymous reviewers for pointing out a recent work [ 39 ] which also considers exploring the relation among multiple unsupervised domains .
APPENDIX
PROOF OF THEOREM III.4
Proof : We rewrite Eq ( 17 ) as L(P(k ) ) = tr(P(k)B+P(k)T − P(k)B−P(k)T
− 2A+P(k)T + 2A−P(k)T )
ACKNOWLEDGMENT
By Lemma III.3 , we have
This work was supported by the National Natural Science Foundation of China ( No.60721003 , No.60673106 and No.60573062 ) and the Specialized Research Fund for the tr(P(k)B+P(k)T ) ≤ ij
( P(k)B+)ijP(k)2 ij
P(k ) ij
CLASSIFICATION RESULTS ON WEBKB4
Table VI
Source Task 1 Task 1 Task 1 Task 2 Task 2 Task 2 Task 3 Task 3 Task 3 Task 4 Task 4 Task 4
Target Task 2 Task 3 Task 4 Task 1 Task 3 Task 4 Task 1 Task 2 Task 4 Task 1 Task 2 Task 3
SVM 0.6280 0.6371 0.7138 0.5639 0.5645 0.6382 0.6344 0.5920 0.7237 0.7269 0.6360 0.5887
TSVM GFHF 0.5920 0.6320 0.7137 0.6492 0.7303 0.7467 0.5639 0.6167 0.5202 0.5960 0.5132 0.6842 0.6035 0.6564 0.5920 0.6800 0.7304 0.5132 0.6608 0.7313 0.6420 0.5920 0.5960 0.5202
LLGC 0.6520 0.7379 0.7993 0.6432 0.5403 0.7171 0.6960 0.6520 0.7368 0.7357 0.6320 0.5726
CoCC 0.6400 0.7258 0.7895 0.6520 0.6573 0.7237 0.6916 0.6760 0.7796 0.6740 0.6200 0.6976
CDSC 0.6760 0.7339 0.8092 0.6828 0.7218 0.7336 0.7048 0.6840 0.8257 0.7093 0.6360 0.7177
LSSTTC 0.7024 0.7419 0.8125 0.6745 0.7661 0.7513 0.7022 0.6960 0.8414 0.6493 0.6072 0.7339
CLASSIFICATION RESULTS ON RECVSTALK
Table VII
Source Task 1 Task 2
Target Task 2 Task 1
SVM 0.7605 0.7310
TSVM GFHF 0.8220 0.8395 0.7988 0.7039
LLGC 0.7625 0.7055
CoCC 0.8544 0.8574
CDSC 0.8628 0.8829
LSSTTC 0.8841 0.9170
CLASSIFICATION RESULTS ON COMPVSSCI
Table VIII
Source Task 1 Task 2
Target Task 2 Task 1
SVM 0.6902 0.7803
TSVM GFHF 0.6825 0.8336 0.8864 0.8955
LLGC 0.7170 0.8800
CoCC 0.9063 0.8960
CDSC 0.9196 0.9003
LSSTTC 0.9489 0.9056
Moreover , by the inequality a ≤ ( a2+b2 )
,∀a , b > 0 , we have ij
2b ij
A− ij ij + P(k)2 P(k)2 2P(k ) ij ij tr(A−P(k)T ) =
A− ijP(k ) ij ≤
To obtain the lower bound for the remaining terms , we use the inequality that z ≥ 1 + log z,∀z > 0 , then tr(A+P(k)T ) ≥ ijP(k ) A+ ij ( 1 + log ij tr(P(k)B−P(k)T ) ij P(k ) jkP(k ) B− ik ( 1 + log
≥ ijk
P(k ) ij P(k ) P(k ) ij P(k ) ik ik
P(k ) ij P(k ) ij
)
)
By summing over all the bounds , we can get Z(P(k ) , P(k) ) , which obviously satisfies ( 1 ) Z(P(k ) , P(k ) ) ≥ Jmt(P(k) ) ; ( 2)Z(P(k ) , P(k ) ) = Jmt(P(k ) ) To find the minimum of Z(P(k ) , P(k) ) , we take the Hessian matrix of Z(P(k ) , P(k ) ) ∂2Z(P(k ) , P(k ) )
2(P(k)B− + A+)ijP(k ) ij
∂P(k ) ij ∂P(k ) kl
= δikδjl(
P(k)2 2(P(k)B+ + A−)ij ij
)
+
P(k ) ij by setting ∂Z(P(k),P(k ) ) which we can get Eq ( 18 ) .
( k ) ij
∂P
= 0 and solving for P(k ) , from
REFERENCES
[ 1 ] K . Wagstaff , C . Cardie , S . Rogers , and S . Schr¨odl , “ Constrained k means clustering with background knowledge , ” in ICML , 2001 , pp . 577–584 .
[ 2 ] S . Basu , M . Bilenko , and R . J . Mooney , “ A probabilistic framework for semi supervised clustering , ” in KDD , 2004 , pp . 59–68 .
[ 3 ] B . Kulis , S . Basu , I . S . Dhillon , and R . J . Mooney , “ Semisupervised graph clustering : a kernel approach , ” in ICML , 2005 , pp . 457–464 .
[ 4 ] T . Li , C . Ding , and M . I . Jordan , “ Solving consensus and semi supervised clustering problems using nonnegative matrix factorization , ” in ICDM , 2007 , pp . 577–582 .
[ 5 ] F . Wang , T . Li , and C . Zhang , “ Semi supervised clustering via matrix factorization , ” in SDM , 2008 , pp . 1–12 .
[ 6 ] I . S . Dhillon , “ Co clustering documents and words using bipartite spectral graph partitioning , ” in KDD , 2001 , pp . 269– 274 .
[ 7 ] I . S . Dhillon , S . Mallela , and D . S . Modha , “ Information theoretic co clustering , ” in KDD , 2003 , pp . 89–98 . which is a diagonal matrix with positive diagonal elements . Thus Z(P(k ) , P(k ) ) is a convex function of P(k ) . Therefore , we can obtain the global minimum of Z(P(k ) , P(k ) )
[ 8 ] C . H . Q . Ding , T . Li , W . Peng , and H . Park , “ Orthogonal nonnegative matrix t factorizations for clustering , ” in KDD , 2006 , pp . 126–135 .
[ 9 ] Q . Gu and J . Zhou , “ Co clustering on manifolds , ” in KDD ,
2009 , pp . 359–368 .
[ 10 ] R . Caruana , “ Multitask learning , ” Machine Learning , vol . 28 , no . 1 , pp . 41–75 , 1997 .
[ 11 ] T . Evgeniou and M . Pontil , “ Regularized multi–task learning , ” in KDD , 2004 , pp . 109–117 .
[ 12 ] C . A . Micchelli and M . Pontil , “ Kernels for multi–task learning , ” in NIPS , 2004 .
[ 13 ] R . K . Ando and T . Zhang , “ A framework for learning predictive structures from multiple tasks and unlabeled data , ” Journal of Machine Learning Research , vol . 6 , pp . 1817– 1853 , 2005 .
[ 14 ] A . Argyriou , T . Evgeniou , and M . Pontil , “ Multi task feature learning , ” in NIPS , 2006 , pp . 41–48 .
[ 15 ] J . Chen , L . Tang , J . Liu , and J . Ye , “ A convex formulation for learning shared structures from multiple tasks , ” in ICML , 2009 , p . 18 .
[ 16 ] S .
J . Pan on and Q . Yang , transfer “ A survey and learning , ” Department Computer Engineering , Hong Kong University Science and Technology , Hong Kong , China , Tech . Rep . HKUST CS08 08 , November 2008 . [ Online ] . Available : http://wwwcseusthk/ sinnopan/publications/TLsurvey 0822.pdf
Science of of
[ 17 ] W . Dai , O . Jin , G R Xue , Q . Yang , and Y . Yu , “ Eigentransfer : a unified framework for transfer learning , ” in ICML , 2009 , p . 25 .
[ 18 ] P . Wu and T . G . Dietterich , “ Improving svm accuracy by training on auxiliary data sources , ” in ICML , 2004 .
[ 27 ] X . Zhu , Z . Ghahramani , and J . D . Lafferty , “ Semi supervised learning using gaussian fields and harmonic functions , ” in ICML , 2003 , pp . 912–919 .
[ 28 ] D . Zhou , O . Bousquet , T . N . Lal , J . Weston , and B . Sch¨olkopf , “ Learning with local and global consistency , ” in NIPS , 2003 .
[ 29 ] T . Joachims , “ Transductive inference for text classification using support vector machines , ” in ICML , 1999 , pp . 200–209 .
[ 30 ] T . Li , S . Ma , and M . Ogihara , “ Document clustering via adaptive subspace iteration , ” in SIGIR , 2004 , pp . 218–225 .
[ 31 ] S . Boyd and L . Vandenberghe , Convex optimization . Cam bridge : Cambridge University Press , 2004 .
[ 32 ] C . H . Ding , T . Li , and M . I . Jordan , “ Convex and seminonnegative matrix factorizations , ” IEEE Transactions on Pattern Analysis and Machine Intelligence , vol . 99 , no . 1 , 2008 .
[ 33 ] D . D . Lee and H . S . Seung , “ Algorithms for non negative matrix factorization , ” in NIPS , 2000 , pp . 556–562 .
[ 34 ] D . Cai , X . He , X . Wu , and J . Han , “ Non negative matrix factorization on manifold , ” in ICDM , 2008 , pp . 63–72 .
[ 35 ] J . Shi and J . Malik , “ Normalized cuts and image segmentation , ” IEEE Trans . Pattern Anal . Mach . Intell . , vol . 22 , no . 8 , pp . 888–905 , 2000 .
[ 36 ] V . Vapnik , Statistical Learning Theory . Wiley , 1998 .
[ 37 ] R E Fan , P H Chen , and C J Lin , “ Working set selection using second order information for training support vector machines , ” Journal of Machine Learning Research , vol . 6 , pp . 1889–1918 , 2005 .
[ 19 ] X . Liao , Y . Xue , and L . Carin , “ Logistic regression with an auxiliary data source , ” in ICML , 2005 , pp . 505–512 .
[ 38 ] Q . Gu and J . Zhou , “ Local learning regularized nonnegative matrix factorization , ” in IJCAI , 2009 , pp . 1046–1051 .
[ 39 ] J . Gao , W . Fan , Y . Sun , and J . Han , “ Heterogeneous source consensus learning via decision propagation and negotiation , ” in KDD , 2009 , pp . 339–348 .
[ 20 ] W . Dai , Q . Yang , G R Xue , and Y . Yu , “ Boosting for transfer learning , ” in ICML , 2007 , pp . 193–200 .
[ 21 ] R . Raina , A . Battle , H . Lee , B . Packer , and A . Y . Ng , “ Selftaught learning : transfer learning from unlabeled data , ” in ICML , 2007 , pp . 759–766 .
[ 22 ] W . Dai , G R Xue , Q . Yang , and Y . Yu , “ Co clustering based classification for out of domain documents , ” in KDD , 2007 , pp . 210–219 .
[ 23 ] X . Ling , W . Dai , G R Xue , Q . Yang , and Y . Yu , “ Spectral domain transfer learning , ” in KDD , 2008 , pp . 488–496 .
[ 24 ] S . J . Pan , J . T . Kwok , and Q . Yang , “ Transfer learning via dimensionality reduction , ” in AAAI , 2008 , pp . 677–682 .
[ 25 ] J . Gao , W . Fan , J . Jiang , and J . Han , “ Knowledge transfer via multiple model local structure mapping , ” in KDD , 2008 , pp . 283–291 .
[ 26 ] W . Dai , Q . Yang , G R Xue , and Y . Yu , “ Self taught cluster ing , ” in ICML , 2008 , pp . 200–207 .
