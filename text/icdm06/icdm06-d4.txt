GraphRank : Statistical Modeling and Mining of Significant Subgraphs in the Feature Space
Huahai He
Ambuj K . Singh
Department of Computer Science
University of California , Santa Barbara
Santa Barbara , CA 93106 , USA {huahai , ambuj}@csucsbedu
Abstract
We propose a technique for evaluating the statistical significance of frequent subgraphs in a database . A graph is represented by a feature vector that is a histogram over a set of basis elements . The set of basis elements is chosen based on domain knowledge and consists generally of vertices , edges , or small graphs . A given subgraph is transformed to a feature vector and the significance of the subgraph is computed by considering the significance of occurrence of the corresponding vector . The probability of occurrence of the vector in a random vector is computed based on the prior probability of the basis elements . This is then used to obtain a probability distribution on the support of the vector in a database of random vectors . The statistical significance of the vector/subgraph is then defined as the p value of its observed support . We develop efficient methods for computing p values and lower bounds . A simplified model is further proposed to improve the efficiency . We also address the problem of feature vector mining , a generalization of itemset mining where counts are associated with items and the goal is to find significant sub vectors . We present an algorithm that explores closed frequent sub vectors to find significant ones . Experimental results show that the proposed techniques are effective , efficient , and useful for ranking frequent subgraphs by their statistical significance .
1
Introduction
Recent advances in science and technology have generated a large amount of complex data . As a powerful abstract data type , graphs are often used to represent these complex data . In the database community , graph models have been used for schema matching [ 1 ] , web documents , multimedia [ 2 ] , and social networks [ 3 ] . In biology , graphs have been used to represent molecular structures , protein
3D structures [ 4 ] , and protein interaction networks [ 5 ] .
Mining structured patterns in a collection of graphs is useful for understanding the intrinsic characteristics of scientific data . In drug development , frequent pattern mining can reveal conserved substructures in a category of medically effective chemical compounds [ 6 ] . In studies of protein interaction networks , conserved patterns in multiple species reveal cellular machinery [ 5 ] . In the analysis of protein structures , the presence of conserved subgraphs in protein contact maps can reveal evolutionarily significant patterns of chemical bonds and interactions [ 4 ] .
A number of techniques have been developed to find frequent subgraphs [ 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 ] in a transactional database , ie , a large collection of graphs . However , the usefulness of frequent subgraph mining is limited by two factors :
1 . Not all frequent subgraphs are statistically significant .
2 . There is no way to rank the frequent subgraphs . This hinders the identification of subgraphs of real interest , especially when the number of discovered frequent subgraphs is large .
For illustrative purposes , consider a sample graph database shown in Figure 1 and some frequent subgraphs shown in Figure 2 . The support of a subgraph is the number of graphs that contain the subgraph . A subgraph is frequent if its support is above a given threshold . Neither the support nor the size of a subgraph is sufficient to measure the statistical significance of a subgraph , and to rank the listed subgraphs .
1.1 Our Approach
In this paper , we propose a technique for computing the statistical significance of frequent subgraphs , and show that frequent subgraphs can be effectively ranked by this measure .
1
A
A
A
A
B
B
B
C
B
B
C
B
B
C
B
C
C
G1
G2
G3
G4
G5
Figure 1 . A sample graph database
Subgraph
Structure
Support g1 g2 g3 g4 g5
A
B
A
A
A
A
B
B
B
B
B
B
B
C
B
C
4 {G1 , G2 , G3 , G4}
3 {G1 , G2 , G4}
Which subgraph is the most statistically significant ?
2 {G1 , G2}
2 {G2 , G4}
1 {G2}
Figure 2 . Frequent subgraphs and their supports
Given a subgraph g and its observed support µ0 , its statistical significance is defined as the probability that g occurs in a database of random graphs with a support µ ≥ µ0 , namely the p value of g . In this way , we can compute the p values of all frequent subgraphs discovered by existing subgraph mining techniques , rank them by p values , and/or remove insignificant ones . This would greatly improve the quality of the mining results .
The main challenge of the above procedure is how to estimate the probability that a subgraph occurs in a random graph . As graphs have flexible structures , it is difficult to estimate such probability directly in the graph space ( Note that the problem of determining whether a graph is a subgraph of another is NP complete ) . Milo et al [ 15 ] adopted a simulation approach : generate many random graphs while maintaining some empirical measures such as degree of vertices , number of edges , and then count the ones that contain the subgraph . However , this approach is neither scalable to a large collection of graphs nor precise for computing and comparing small p values .
We address the above challenge by transforming graphs into a feature space . First , we use domain knowledge to define a set of basis elements such as vertices , edges , or small subgraphs . A graph is simply regarded as a collection or a histogram of basis elements ; this defines its feature vector . Then , we approximate the question of significance of a subgraph by considering the significance of its feature vector in the feature space . This is a simpler problem that admits closed form solutions . Although structural information of a graph is lost in the feature space , statistics on the basis elements are still captured . As shown by the experimental results , this approximation is suitable for the discovery of significant subgraphs .
Figures 3 and 4 outline our approach . In the first phase ( Figure 3 ) , we obtain frequent subgraphs from a target graph database using existing graph mining techniques , and transform them into feature vectors . In the second phase ( Figures 4 ) , we compute the probability that feature vector x of a subgraph g occurs in a random vector , and use this probability to compute the probability distribution on x ’s support in a random database . The statistical significance of g is then computed as the p value of its observed support in the target database .
Target Graph
DB
Graph mining
Frequent subgraphs
Transform
Feature vectors
Figure 3 . Representation of graphs as feature vectors
Probability that x occurs in a random vector
Feature vector x
Probability distribution of x ’s support in a database of random vectors
A subgraph g
Support of g in target DB
How significant is g ?
P value of g
Figure 4 . Computation of p value of a frequent subgraph
In the second half of the paper , we address the problem of feature vector mining , a simplified version of graph mining . Vector ( aka histogram and multiset ) mining is an important generalization of frequent itemset mining . We develop ClosedVect , an algorithm that explores closed subvectors to find significant ones . We prove that ClosedVect is optimal in terms of the number of search states .
We validate the quality of our technique through experiments on chemical compounds and synthetic graphs . In particular , we find that a specific subgraph , neither largest nor
2 most frequent , turns out to be the largest common subgraph in a specific class of medically effective compounds . This finding validates the practical usefulness of our approach . We also demonstrate the efficiency of the computational methods and the feature vector mining algorithm .
The main contributions of our work are as follows :
1 . We propose a technique for computing the p values of frequent subgraphs , and show that frequent subgraph can be ranked by this measure . Efficient methods are developed for computing p values and lower bounds .
2 . We address the problem of feature vector mining , and present an algorithm for mining significant closed subvectors . This is an important problem in its own right .
The remainder of the paper is organized as follows . Section 2 discusses how to represent graphs as feature vectors . Section 3 presents the probabilistic model . Section 4 describes methods for computing p values and lower bounds . Section 5 describes a simplified probabilistic model . Section 6 describes feature vector mining . Experimental results are reported in Section 7 . Section 8 discusses related work . We conclude with a brief discussion in Section 9 .
2 Representing Graphs as Feature Vectors
We view a graph as a collection of basis elements B = {ˆb1 , , ˆbm} . These basis elements can be vertices , edges , or small graphs . Each basis element ˆbi is associated with a prior probability θi . We first discuss how to select basis elements and transform graphs into feature vectors .
2.1 Feature Selection
The selection of basis elements is application dependent and may require domain knowledge . A basic approach is to select all types of vertices or edges as features . This approach can be done efficiently and the meaning is clear : we evaluate the statistical significance of a subgraph by looking at how its vertices or edges are distributed . The drawback of this approach is that it does not capture any structural information of graphs .
For other graphs such as chemical compounds , one may choose small graphs such as Benzene rings . In this case , the number of available elements may grow dramatically , and these small graphs may overlap structurally . Thus , selecting a representative subset would be more appropriate . The following criteria for selection can be used : 1 ) frequency : frequent basis elements are more representative of graphs ; 2 ) size : large basis elements carry more structural information ( but would be less frequent ) ; 3 ) structural overlap : overlapping basis elements are relatively not independent ;
4 ) Co occurrence : basis elements that frequently occur together are relatively not independent .
Generally , it is computationally difficult to select the optimal subset of basis elements . One may simply use a greedy approach [ 16 ] : choose the kth best element according to its benefit gained ( eg , frequency ) and its relevance ( eg , overlap , covariance ) to the previously selected k − 1 basis elements : b1 = arg max b
{w1f req(b ) + w2size(b)} bk = arg max b
{w1f req(b ) + w2size(b ) k−1X sim(bi , b ) − w4 k − 1 k−1X i=1 cov(bi , b)} ,
( 1 ) w3
− k − 1 i=1 k = 2 , , m where w1 − w4 are weighting factors , f req(b ) is the frequency of b , size(b ) is the size of b , sim(bi , b ) is the overlap between bi and b , and cov(bi , b ) is the covariance between bi and b . All terms are normalized to [ 0 , 1 ] . The procedure repeats until m features are selected .
For the sample database shown in Figure 1 , we use all kinds of edges as the basis , ie , B={A B , A C , B B , B C , C C} . The prior probabilities are empirically computed using their frequency in the database , ie , θ = 17 , 2 ( 6
17 , 3
17 , 1
17 , 5
17 ) .
2.2 Transforming Graphs into Feature Vectors
After a basis is selected , we transform ( sub)graphs into feature vectors . We denote a feature vector by x = ( x1 , , xm ) , where xi counts the frequency of feature ˆbi in the graph . The size of x is defined as |x| = P xi . Vector x is a sub vector of vector y ( and y a super vector of x ) if xi ≤ yi for i = 1 , , m , and is denoted by x ⊆ y . The floor of two vectors x and y is a vector v where vi = min(xi , yi ) for i = 1 , , m . The definition extends to a group of vectors . The ceiling of a group of vectors is defined analogously .
For the sample subgraphs shown in Figure 2 , Table 1 shows their corresponding feature vectors . g1 g2 g3 g4 g5
A B A C B B B C C C 1 1 2 1 2
0 0 0 0 0
0 0 0 1 1
0 0 0 0 0
0 1 1 1 1
Table 1 . Feature vectors of the subgraphs in Figure 2
3
3 Probabilistic Model
In this section , we model the probability with which a feature vector x ( corresponding to a subgraph ) occurs in a random vector ( corresponding to a random graph ) obtained using prior probabilities on the basis elements , and the probability distribution of x ’s support in a database of random vectors . Statistical significance is obtained by comparison to its observed support .
3.1 Probability of occurrence of a feature vector in a random vector
We regard the basis B as a set of m distinct events , one for every basis element , where basis element ˆbi is associated with its prior probability θi . A feature vector of a certain size ℓ is thus regarded as an outcome of ℓ independent trials . Given a feature vector y = ( y1 , , ym ) , |y| = ℓ , the probability that y is observed in ℓ trials can be modeled by a multinomial distribution :
Q(y ) =
ℓ!
Q yi! mY i=1
θyi i ,
( 2 )
In other words , Eqn . ( 2 ) gives the probability of observing y in a random vector of size ℓ .
Let x be the feature vector of a subgraph g . Then , the probability that x occurs in a random vector of size ℓ is a cumulative mass function ( cmf ) of Eqn . ( 2 ) :
P ( x ; ℓ ) = X ystyi≥xi,|y|=ℓ
Q(y )
( 3 )
In other words , this is the probability that x occurs in a random vector of size ℓ . The size constraint ℓ is reasonable : the larger a random vector , the more likely that x will occur in the vector .
For example , the feature vector of subgraph g3 in Figure 2 is x = ( 2 , 0 , 1 , 0 , 0 ) . The probability that x occurs in a random vector of size 3 is P ( x ; 3 ) = 0066
The computation of Eqn . ( 3 ) is not trivial when m and ℓ are large . We will discuss an efficient way to compute Eqn . ( 3 ) in Section 4 .
3.2 Probability distribution of a feature vector ’s support in a database of random vectors
Now we consider the support of x in the context of a database of random vectors . This support is a random variable that follows a probability distribution . Since we are assessing the significance of x in a given target database , the random database should have the same number of vectors as the target database , and vectors in the random database should have similar sizes as those in the target database .
Let n be the number of vectors in the target database , we summarize the sizes of the vectors by ℓ = ( ℓ1 , , ℓd ) and n = ( n1 , , nd ) , where ni is the number of vectors of size ℓi , and P ni = n . If we regard a random vector as a trial , and the occurrence of x in the vector as a “ success ” . Then , the database of random vectors corresponds to n trials , and the support of x corresponds to the number of successes in n trials . If the sizes of the vectors were identical , say ℓ , then the support can be modeled as a binomial random variable , with parameters n and P ( x ; ℓ ) . When the sizes are distinct , each size will correspond to one binomial random variable with parameters ni and P ( x ; ℓi ) . Then , the support of x is the sum of the binomial random variables : the probability of x ’s support being equal to µ is given by
R(µ ; x , ℓ , n ) =
( 4 ) bino(tj ; ni , P ( x ; ℓi ) ) dX P tj =µ t )pt(1−p)n−t is the binomial probwhere bino(t ; n , p ) = ( n ability distribution . In other words , the jth binomial contributes tj successes , with the sum of them equal to µ . All possible combinations of tj give the total probability of observing µ .
For the sample database of Figure 1 , a random database would have ℓ = ( 3 , 4 ) and n = ( 3 , 2 ) . Figure 5 plots the probability distribution of subgraph g3 ’s support in the random database . y t i l i b a b o r P
0.6
0.5
0.4
0.3
0.2
0.1
0 p value = 0.09
=µ
0
2
0
1
3
2 Support µ
4
5
Figure 5 . Probability distribution of g3 ’s support and its p value
We will discuss an efficient method of computing
Eqn . ( 4 ) in Section 4 .
3.3 Statistical Significance of a Feature Vector
Let µ0 be the observed support in the target database . Then , the p value , ie , the probability of observing a support of at least µ0 in the random database is given by
R(µ ≥ µ0 ; x , ℓ , n ) = nX
µ=µ0
R(µ ; x , ℓ , n ) .
( 5 )
4
We denote Eqn . ( 5 ) by p value(x , µ0 ) . The smaller the pvalue , the more statistically significant is the feature vector . For example , g3 ’s observed support is 2 . Its p value is x2 in the second half of the vector , provided that the sum of the two halves is equal to ℓ . The recurrence relation is given by shown as the shaded part in Figure 5 .
The p value has the following monotonicity properties :
1 . Given two vectors x1 and x2 , if x1 ⊆ x2 , then p value(x1 , µ0)≥ p value(x2 , µ0 ) for any µ0 .
2 . Given two supports µ1 and µ2 , if µ1 ≤ µ2 , then p value(x , µ1)≥ p value(x , µ2 ) for any x .
A frequent pattern is closed if none of its super patterns has the same support as the pattern . According to the monotonicity properties , the p value of a non closed pattern is greater than or equal to that of its closed super pattern . Thus , we can consider only closed sub vectors/subgraphs . Now , we are ready to answer the question regarding significance raised in Figure 2 . The p value of each subgraph is computed and shown in Table 2 . Their expected supports are computed as well . Among the subgraphs listed in Figure 2 , g3 has the smallest p value . Thus , we can claim that g3 is the most statistically significant ( though it is neither the largest nor the most frequent ) . g1 g2 g3 g4 g5
µ 3.84 1.65 0.55 0.85 0.16
µ 0 4 3 2 2 1 p value
0.67 0.20 0.09 0.20 0.15
Table 2 . P values of the subgraphs in Figure 2 ; subgraph g3 has the smallest p value .
P ( x ; ℓ ) =
ℓ−|x2|X t=|x1|
( ℓ t)P ( x1 ; t)P ( x2 ; ℓ − t )
( 6 )
The splitting is done recursively until a single bin xi is reached , where P ( xi ; ℓ ) = θℓ i . The recurrence form requires an array for P ( x1 ; t ) and P ( x2 ; ℓ − t ) respectively . Thus , at each recurrence step we need to compute an array P ( x ; s ) for s = |x| , , ℓ .
The time complexity of the above recurrence is analyzed as follows . Let f ( m , ℓ ) be the time to compute array P ( x ; s ) for s = |x| , , ℓ , then it takes 2f ( m 2 , ℓ ) to compute the arrays of the sub units , and ( ℓ − |x|)2 to compute the array of the current unit . Thus , f ( m , ℓ ) = O(2f ( m 2 , ℓ ) + ( ℓ − |x|)2 ) . Solving the equation yields the time complexity of O(m(ℓ − |x|)2 ) . Value m can be further reduced to the number of non zero bins in x .
4.2 Computation of Sum of Binomials
The sum of binomial distributions ( Eqn . ( 4 ) ) can also be computed using a divide and conquer scheme . To observe µ in the d binomials , one observes t in the first d 2 binomials and µ − t in the rest binomials for all t = 0 , , µ . The recurrence is given by
R(µ ; x , ℓ , n ) =
µX t=0
R(t ; x , ℓ1 , n1)R(µ − t ; x , ℓ2 , n2 ) ( 7 ) where hℓ1 , n1i and hℓ2 , n2i correspond to the first and the second half of the d binomials , respectively . Analogous to Subsection 4.1 , the time complexity can be computed to be O(dµ2 ) .
4 Computation of P values and Lower
Bounds
4.3 Lower Bound to P value
In this section , we present efficient methods to compute Eqn . ( 3 ) and Eqn . ( 4 ) , which would take exponential time using naive approaches . Lower bounds are also developed for fast estimation .
4.1 Computation of P ( x ; ℓ )
We develop a divide and conquer scheme for the computation of P ( x ; ℓ ) , the probability that x occurs in a vector of size ℓ ( Eqn . ( 3) ) . The idea is to split x into two halves : 2 +1 , , xm ) , and then take x1 = ( x1 , , x m the convolution of x1 and x2 . In other words , the probability of observing x in a vector of size ℓ is equal to the probability of observing x1 in the first half , and observing
2 ) and x2 = ( x m
For faster estimation , we develop a lower bound to the pvalue . The lower bound to the p value is obtained through a lower bound to P ( x ; ℓ ) . Theorem 1 shows how this is done . Theorem 1 . Let x and y be two vectors , if P ( x ; ℓ ) ≤ P ( y ; ℓ ) for ∀ℓ , then p value(x , µ ) ≤ p value(y , µ ) for ∀µ .
Next , we get a lower bound to P ( x ; ℓ ) by decoupling the multinomial into a product of binomials . Theorem 2 . ( Lower bound to P ( x ; ℓ ) )
P ( x ; ℓ ) ≥ mY aiX i=1 t=xi
( ai t )θt i(1 − θi)ai−t
= mY i=1
I(θi ; xi , ai − xi + 1 )
( 8 )
5 where ai = ℓ − Pi−1 regularized Beta function . t=1 xt , and I(θi ; xi , ai − xi + 1 ) is the
Proof . Let us start with the simple case where x has two bins x1 and x2 . We rewrite P ( x ; ℓ ) in the form of P ( y1 ≥ x1 , y2 ≥ x2 ; ℓ ) , ie , the probability of observing at least x1 in the first bin and at least x2 in the second bin in ℓ trials . Note that P ( y1 ≥ x1 , y2 ≥ x2 ; ℓ ) = P ( y1 ≥ x1 ; ℓ)P ( y2 ≥ x2 ; ℓ|y1 ≥ x1 ; ℓ ) . For P ( y2 ≥ x2 ; ℓ|y1 ≥ x1 ; ℓ ) , we can always ensure the condition y1 ≥ x1 by reserving x1 trials from the ℓ trials . Thus , P ( y2 ≥ x2 ; ℓ|y1 ≥ x1 ; ℓ ) ≥ P ( y2 ≥ x2 ; ℓ − x1 ) . Hence P ( y1 ≥ x1 , y2 ≥ x2 ; ℓ ) ≥ P ( y1 ≥ x1 ; ℓ)P ( y2 ≥ x2 ; ℓ − x1 ) . When we extend the above reasoning to more than two bins , we obtain the product of sums in Eqn . ( 8 ) . Equivalence to the regularized Beta function is known from the statistics literature1 .
Theorem 3 gives an upper bound to P ( x ; ℓ ) in an analo gous manner .
Theorem 3 . ( Upper bound to P ( x ; ℓ ) )
Under the simplified model , we compute the p value as follows .
1 . Empirically obtain the prior probabilities P ( Yi ≥ j ) for every basis elementˆbi and every j ( up to the maximum possible value ) . For example , element ˆb1 = “ A B ” occurs twice ( G1 and G2 ) in the sample database , thus P ( Y1 ≥ 2 ) = 2 5 . Similarly , P ( Y1 ≥ 1 ) = 4 5 , P ( Y1 ≥ 0 ) = 1 , P ( Y3 ≥ 1 ) = 3
5 , etc .
2 . Compute bP ( x ) using Eqn . ( 11 ) . For subgraph g3 , x = ( 2 , 0 , 1 , 0 , 0 ) . Thus bP ( x ) = P ( Y1 ≥ 2 ) × P ( Y3 ≥ 1 ) = 2
5 × 3
25 . 5 = 6
3 . Compute the p value of x by Pn
µ0 bino(µ ; n , bP ( x) ) , or equivalently by the regularized Beta function I(bP ( x ) ; µ0 , n ) . When both nbP ( x ) and n(1 − bP ( x ) ) are large , the binomial distribution can be approximated by a normal distribution .
P ( x ; ℓ ) ≤ mY
ℓX i=1 t=xi
( ℓ t)θt i(1 − θi)ℓ−t
6 Feature Vector Mining
= mY i=1
I(θi ; xi , ℓ − xi + 1 )
( 9 )
5 A Simplified Model
The computation of p values and lower bounds as illustrated in the previous section does not scale to very large databases . In this section , we present a simplified model in which the computation of p values is much more efficient . In our previous model , we had a constraint on the size of random vectors . Our first simplification is to relax this constraint , and consider the probability that a feature vector occurs in a random vector of arbitrary size . The probability can be written as
P ( x ) = P ( Y1 ≥ x1 , , Ym ≥ xm )
( 10 )
Further , if we assume that different types of basis elements are orthogonal , then the above joint probability can be decoupled into a product of probabilities : bP ( x ) = mY i=1
P ( Yi ≥ xi )
( 11 ) where P ( Yi ≥ xi ) is the probability that element ˆbi occurs at least xi times in a random vector .
Since bP ( x ) is fixed , the support of x in a database of random vectors can be modeled by a single binomial distribution , with parameters n and bP ( x ) .
1http://mathworldwolframcom/BinomialDistributionhtml
As frequent subgraphs are represented as feature vectors and evaluated for statistical significance , an interesting question arises : can we directly search top K significant sub vectors , or sub vectors above a significance threshold ? To our best knowledge , the problem of feature vector mining has not been addressed before . Feature vector mining is important in two aspects . First , feature vectors , also known as histograms and multisets , are common ways to summarize complex data . As a result , feature vector patterns are profiles of structured patterns , and feature vector mining can work as a foundation of structured pattern mining . Second , feature vector mining is an important generalization of the well studied frequent itemset mining : each item is now associated with a count instead of a boolean value .
We develop ClosedVect , an algorithm that explores frequent closed sub vectors to find significant ones . The algorithm consists of two phases : exploring closed sub vectors and evaluating the significance of a closed sub vector .
6.1 Exploring Closed Sub Vectors
Alg . 1 outlines the phase of exploring closed sub vectors . The algorithm explores sub vectors in a bottom up , depthfirst manner . At each search state , the algorithm “ jumps ” to a future state that has an immediately smaller supporting set along a branch ( line 3 ) . The corresponding sub vector is then promoted as the floor of the supporting set ( line 6 ) . To prevent duplicates , each state is associated with a beginning position b . Any future state must extend at a position greater than or equal to b . All extensions starting at the same
6 position are placed along the same search branch . If an extension designated at position i results in a starting position of less than i , then it must be a duplicate extension ( lines 7 8 ) .
The evaluation phase ( line 1 ) computes the p value of a sub vector and reports top K significant ones . Lines 9 10 estimate a lower bound on the p value of the super vectors of x′ and prune it if this bound is too high . The evaluation phase and the pruning will be discussed in Subsection 6.2 and 63
Alg . 1 ClosedVect(x , S , b ) x : current sub vector ; S : supporting set of x , ie , feature vectors in the database that contain x ; b : beginning position at which bins can be extended ; continue ;
1 : Eval(x , |S| ) ; 2 : for i := b to m do 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : continue ; continue ;
ClosedVect(x′ , S′ , i ) ;
S′ ← {Y | Y ∈ S , Yi > xi} ; if |S′| <minSupport then x′ := f loor(S′ ) ; if ∃j < i such that x′ j > xj then if p value(ceiling(S′ ) , |S′| ) ≥ maxP value then
Figure 6 shows a running example of Alg 1 . The underlined numbers denote the beginning position b of each state . Duplicate search states are pruned by examining the search order . For example , an extension to state “ 2 3 2 ” at position “ 3 ” leads to a supporting set “ {h1 , h3} ” , of which the floor is “ 3 4 2 ” . However , this extension violates the search order and is pruned ( lines 7 8 ) . h1 : 4 5 6 h2 : 3 2 4 h3 : 3 4 2 h4 : 2 3 3
2 2 2
{h1 , h2 , h3 , h4}
3 2 2
{h1 , h2 , h3}
2 3 2
{h1 , h3 , h4}
X
4 5 6 {h1}
3 4 2 {h1 , h3}
3 2 4 {h1 , h2}
3 4 2
{h1 , h3}
2 3 3 {h1 , h4}
2 2 3
{h1 , h2 , h4}
X
3 2 4
{h1 , h2}
X
4 5 6 {h1}
X
4 5 6 {h1}
Figure 6 . A running example of ClosedVect
Now , we show the correctness and efficiency of algorithm ClosedVect . We say that an algorithm is complete if it explores all desired answers . It is compact if every search
7 state finds at least one distinct answer . It is duplicate free if it does not extend duplicate search states nor generate duplicate answers .
First , we state a lemma that establishes the use of floor of supporting sets when exploring closed sub vectors .
Lemma 1 . For any closed sub vector x and its supporting set S , x = f loor(S ) .
Theorem 4 . ( Correctness and Efficiency of ClosedVect ) Algorithm ClosedVect explores closed and only closed sub vectors . It is complete , compact , and duplicate free .
Proof . 1 ) completeness : For any closed sub vector x , let S be its supporting set , then x = f loor(S ) . We show that x can be found in a search state . Starting from the root state r , let i be the first bin such that f loor(S)i > ri , then S must be a subset of the supporting set at extension i . By induction on the number of bins , the supporting set will eventually shrink to S , hence x is found . 2 ) compactness : By construction , each state in the search tree corresponds to a closed subvector . 3 ) duplicate free : According to the search order , each search state can be uniquely located in the search tree .
In other words , ClosedVect is optimal in terms of the number of search states because every search state corresponds to a distinct closed sub vector .
6.2 Evaluating Closed Frequent Sub Vectors
Next , we describe the evaluation phase of our feature mining algorithm . Alg . 2 outlines the evaluation phase . A priority queue is used to maintain the answer set , ie , top K significant sub vectors found so far . The p value threshold maxPvalue is the p value of the K th significant sub vector found so far . To evaluate a candidate sub vector , the lower bound to the p value ( Eqn . ( 8 ) ) is examined before the computation of exact p values ( Eqn . ( 5) ) . If they are both less than maxPvalue , then both the priority queue and maxPvalue are updated .
Alg . 2 Eval(x , µ0 ) x : a sub vector ; µ0 : support of x ; P Q : Priority queue for top K answers ; if p value lowerbound(x , µ0)<maxPvalue then if p value(x , µ0)< maxPvalue then Insert hp value(x , µ0 ) , xi into PQ ; if |P Q| > K then
Pop an item from PQ ; maxPvalue:=PQtoppvalue ;
To search top K significant sub vectors , one sets the initial maxPvalue as 1 ; to search sub vectors above a significance threshold , one sets K = +∞ .
6.3 Lower Bound of P values of Super Vectors
Next , we study how to compute a lower bound to the p values of all super vectors of a given sub vector . This is used to prune unnecessary extensions in algorithm ClosedVect ( lines 9 10 ) . There are two approaches to computing this lower bound . The first approach computes the ceiling of the supporting set and uses it to bound the p value .
Theorem 5 . Let x and u be two vectors and x ⊂ u , then for any y subject to x ⊆ y ⊆ u , p value(y , support(y ) ) ≥ p value(u , support(x) ) .
Proof . The proof follows from the monotonicity property of the p value .
The second approach constructs the most skewed supervector of a certain size from x , and uses it to bound the p value of all super vectors of the same size . The following lemma allows us to incrementally skew a vector .
Lemma 2 . Assuming θ1 ≤ θ2 ≤ ≤ θm . Let x=(x1 , , xi , , xj , , xm ) .
( 1 ) If xi ≥ xj , let x′ = ( x1 , , xi +1 , , xj −1 , , xm ) , then P ( x′ ; ℓ ) ≤ P ( x ; ℓ ) . In this case , we increment xi and decrement xj .
( 2 ) If xi < xj , let x′ = ( x1 , , xj , , xi , , xm) . , then
P ( x′ ; ℓ ) ≤ P ( x ; ℓ ) . In this case , we switch xi and xj .
Proof . See Appendix A .
In the following theorem , we estimate a lower bound to the p value of a super vector of constant size |x| + δ by first skewing x to x′ using Lemma 2 , and then adding δ to bins with the smallest prior probabilities .
Theorem 6 . Assuming θ1 ≤ θ2 ≤ ≤ θm . Let u = ( u1 , , um ) be the ceiling of super vectors , u1 ≥ u2 ≥ ≥ um . Given x , x ⊆ u , sort x′ is in non increasing order : x′ = ( x′ m . Given δ > 0 , construct ym from x′ as follows : fill x′ 1 up to u1 , then x′ up to u2 , , and so on until δ is used up . Then , for any y subject to x ⊆ y ⊆ u and |y| = |x|+δ , P ( y ; ℓ ) ≥ P ( ym ; ℓ ) .
2 ≥ ≥ x′
1 , , x′ m ) , x′
1 ≥ x′
2
Proof . According to Lemma 2 , y can be iteratively transformed into ym with P ( y ; ℓ ) non increasing .
Theorems 5 and 6 are intended to prune search states where the closed sub vectors are large . They can be especially effective if large sub vectors are not significant .
7 Experimental Results
In this section , we report experimental results that validate the quality and efficiency of the proposed techniques . The experiments are divided into two parts : 1 ) Validation of the quality of our probabilistic model , and 2 ) Performance evaluation of the feature vector mining algorithm as well as the p value computation .
Three datasets are used in our experiments . The first dataset is the DTP AIDS Antiviral Screen chemical compound dataset from NCI/NIH2 . The compounds have been classified into three categories according to their AIDS antiviral activities . We focus on the category of confirmed active ( CA ) which contains 422 chemical compounds . On average , each graph has 40 vertices and 42 edges . The second dataset is synthetic graphs for recall tests . The third dataset is a web page visits dataset .
The p value computation and the feature vector mining algorithm were implemented in Java using Sun JDK 150 The regularized Beta function ( Eqn . ( 8 ) ) was computed using Apache ’s Commons Math Library3 . All experiments were performed on an Intel 2.8GHz , 1G memory running MS Windows XP Professional .
We use CloseGraph [ 10 ] to find frequent closed subgraphs . We compare the p value ranking with a simple ranking approach based on size . To our best knowledge , there are no other methods that evaluate the statistical significance of frequent subgraphs in a graph database . Thus , comparative assessments to other statistical methods are not presented .
7.1 Evaluating the Quality of the Results
711 Chemical Compound Graphs
We demonstrate the practical usefulness of our method on the chemical compound dataset . Two sets of basis elements are generated to transform subgraphs into feature vectors . The first set of basis elements consists of all different edges ( 39 in total ) , namely 1 edge basis . For the second set of basis elements , we consider all possible subgraphs containing 3 edges ( 322 in total ) , and select 30 of them using the greedy approach discussed in Section 2 . We call this the 3 edge basis . For each case , we compute the prior probabilities using their frequency in the background dataset ( containing around 42,000 compounds ) .
Using CloseGraph [ 10 ] with minimum support minSup=5 % , 7879 closed subgraphs are generated 4 . For each of them , we compute its p value using the two bases and the two models for p value computation ( exact and simplified ) .
2http://dtpncinihgov/ 3http://jakartaapacheorg/commons/math/ 4the results are different from [ 10 ] since aromatic bonds are not spe cially treated .
8
100
10−10
10−20
10−30
10−40
10−50
10−60
10−70 l e u a v − P
10−80
100
101
102
Rank
103
100
10−50 l e u a v − P
10−100
10−150
100
10−50
10−100
10−150
10−200 l e u a v − P
CA CM
104
10−200
100
101
102
Rank
Exact model Simplified model
103
104
10−250
100
101
Exact model Simplified model
103
104
102
Rank
( a ) 1 edge basis
( b ) 3 edge basis
Figure 9 . P value vs . rank with different feature bases and models
Figure 7 . P value vs . validation rank with cross
Figure 7 shows the p values of the subgraphs vs . their ranks using 3 edge basis and the exact model . To crossvalidate the significance of the subgraphs , we also compute their p values in the category of confirmed moderately active ( CM ) for comparison . As shown in the figure , the pvalues of the discovered subgraphs are much smaller than they would be in the context of the CM category . Further , a large number of the subgraphs are statistically insignificant . Using a p value cutoff , say 0.01 , we are able to reduce the number of discovered subgraphs by one order of magnitude .
N
O
O
N
N+
N
N
O
O
Figure 8 . The most significant subgraph in CA
Figure 8 shows the most significant subgraph found in our results ( the unlabeled nodes are C atoms ) . It is ranked 1st in both the exact and simplified model using the 3 edge basis . This subgraph has 19 edges and 15 % support . We found that this subgraph is the largest common subgraph in the chemical class of Azido Pyrimidines 5 . The AZT compound ( NSC 602670 ) , a super graph of this subgraph , has an extra edge on the left hexagon and 12 % support . It is ranked 3rd in the exact model and 2nd in the simplified model . The compound has been widely used for HIV inhibition . The findings validate the practical usefulness of our approach .
Figure 9 shows the p values of the subgraphs vs . their ranks using different feature bases and different models for the computation of p values . The p values in the simplified
5http://dtpncinihgov/docs/aids/searches/listhtml model are smaller than those in the exact model . The underlying reason is the stronger assumption by the simplified model that different basis elements are totally independent , whereas in the exact model , they are constrained by the size of random graphs . Nevertheless , the rankings by the two models are more or less consistent . Under the 3 edge basis , for example , the top 10 subgraphs of the exact model show up in the top 30 subgraphs of the simplified model .
We compare our ranking approach with a naive ranking approach : rank by size ( in case of tie , rank by support ) . Table 3 shows the rankings of some special subgraphs : the most significant subgraph ( AZT* ) , the largest subgraph , and Benzene ( a ring with six carbons ) . There is no current scientific evidence regarding the importance of the largest subgraph . As shown in the table , ranking by p value is much more appropriate than the ranking by size . And the most significant subgraph is not necessarily the largest or the most frequent subgraph .
Subgraph Support
Size
AZT* Largest Benzene
15 % 5 % 70 %
19 34 6
Rank by p value simpl .
3 edge basis strict 1st 914th 886th
1 edge basis strict 40th 752nd simpl . 69th 1st 142nd 751st 1424th 6820th 1875th
Rank by size 428th 1st
6969th
Table 3 . Ranking by different approaches
712 Recall Tests on Synthetic graphs
We also verify the quality of our method through recall tests on synthetic graphs . The procedure of recall tests is illustrated in Figure 10 . The basic idea is to embed some significant subgraphs into a synthetic database , and then see how they can be recalled through p value ranking . The tests are “ supervised ” in that all prior knowledge , such as basis elements and significant subgraphs , are known in advance . We generate synthetic graphs as follows . Let LV and LE be the label set of vertices and edges respectively . The size of a graph is measured by the number of edges . First , we
9
Building blocks , B
Generate
Significant
Subgraphs , A
Graph
Database , D
Graph Mining
Compare A and A’
Top K Significant
Subgraphs , A’
Figure 10 . Recall tests on synthetic graphs generate a set of building blocks ( also the basis elements ) B = {ˆb1 , , ˆbm} . Each building block is a tiny subgraph generated by randomly adding an edge to the subgraph until it reaches a given fixed size zB . Then , we generate k significant subgraphs A = {A1 , , Ak} using the building blocks . Each significant subgraph is generated by randomly inserting a building block into the subgraph until it reaches size zA , which has a Poisson distribution . Next , we use B and A to generate the database graphs . Each database graph has a probability of PA of selecting a significant subgraph from A . Then , the building blocks are randomly selected and inserted into the database graph until it reaches size zG , which has a Poisson distribution .
Next , we use our technique to discover the frequent subgraphs , compute their p values , and see how they are ranked .
In our experiments , we fix |LV | = |LE| = 10 , zB = 3 , m = 100 , zA = 10 , k = 5 , |B| = 100 , zG = 30 , |D| = 1000 , and PA = 0.6 ∼ 10
100
10−50
10−100 l e u a v − p
10−150
10−200
10−250
10−300
P =0.6 A P =0.9 A
0
10
20
30
40
50
60
70
80
Figure 11 . P value vs . rank
Figure 11 shows the p value vs . rank for PA=0.6 and 0.9 respectively . On average , the p values for PA = 0.9 are smaller than those for PA = 06 This is because for PA = 0.9 , database graphs contains more significant subgraphs .
Table 4 shows the rankings of the subgraphs in A . All significant subgraphs in A have been discovered and ranked at very high positions .
10
PA=0.6 PA=0.6 PA=0.9 PA=0.9
Rankings of significant subgraphs Rankings of significant subgraphs 0 , 1 , 2 , 11 , 23 0 , 1 , 2 , 11 , 23
0 , 1 , 2 , 11 , 13 0 , 1 , 2 , 11 , 13
Table 4 . Rankings of subgraphs in A
7.2 Computation Costs of P values and Lower
Bounds
We evaluate the computation costs and lower bounds of p values , as well as tightness of lower bounds ( Section 4 ) using random data . The scenario is set up as follows . The size of the database is 1000 ; the sizes of database vectors randomly range from 50 to 300 ; the number of distinct sizes of database vectors is 100 , ie , there are 100 binomial distributions ; the number of dimensions of vectors ranges from 5 to 100 with an interval of 5 ; for each number of dimensions , we randomly generate 50 sub vectors of size 30 , compute their p values and lower bounds , and average the running times .
Figure 12(a ) shows the running time for a single p value computation in the number of dimensions . ‘Accurate’ refers to the computation of exact p values ; ‘Lower bound’ refers to that of lower bounds ( Eqn . ( 8) ) . The running time for exact computation increases when the number of dimensions increases . The time complexity is actually linear in m′ , the number of non zero bins in the sub vectors . The running time for the lower bound computations is much less than that of the accurate computation .
Accurate Lower bound
150
100
50
) c e s i l l i m
( e m T i
0
−50
−100
−150
−200
−250
−300 l e u a v − P
Lower bound Accurate Upper bound
0 0
20
40 60 Dimensions
80
100
−350 5
10
15
20
25
Size of sub−histogram
30
35
40
45
50
( a ) Running time
( b ) Lower/upper bounds
Figure 12 . Computation costs and bounds of p values
To evaluate the tightness of the lower bound and upper bound , we gradually grow a sub vector starting with size 5 until size 50 , and compute the p value and lower/upper bounds . This procedure fits the typical depth first feature vector mining scenario .
Figure 12(b ) shows the lower bound and upper bound to the p value . Both the lower bound and the upper bound are close to the exact p value in orders of magnitude . Thus , they can be effectively used to estimate the exact p value .
7.3 Feature Vector Mining
731 Chemical Compound Graphs
We evaluate the performance of algorithm ClosedVect ( Section 6 ) using the chemical compound dataset which contains 422 graphs . The graphs are transformed into feature vectors using the 3 edge basis . We run the algorithm using the exact model , the simplified model , and without p value evaluation . The experimental settings are : minSupport=5∼25 % ; K=+∞ ; maxPvalue=1 , 001
Figure 13(a ) shows the running time of ClosedVect wrt minSupport . As expected , the running time decreases with higher support thresholds . Also , the running time without p value evaluation is only in seconds . This demonstrates the high efficiency of ClosedVect in exploring closed sub vectors . With p value computation , the simplified model adds a little amount of overhead . The exact model takes much longer in the computation of p values . Actually , the computation time of a single p value in the exact model is less than one second . It is the large number of closed sub vectors that lead to the high running time .
Figure 13(b ) shows the number of closed sub vectors wrt minSupport under the exact model . With the maximum p value threshold set at 0.01 , the number of closed sub vectors is reduced by one order of magnitude .
Table 5 shows the first few categories and the prior probabilities of being visited at least once ( in the context of the simplified model ) .
Category
Prob . of ≥ 1 visit frontpage
0.316 news 0.177 tech 0.123 local 0.123
Table 5 . Page categories and prior probabilities
We use the simplified model to evaluate statistical significance . The experimental settings are : minSupport=1∼9 % ; K=+∞ ; maxPvalue=1,001
Figure 14(a ) shows the running time wrt minSupport . As shown in the figure , the ClosedVect algorithm is very efficient and scalable to large datasets ( nearly 1 million records ) . Figure 14(b ) shows the number of closed subvectors wrt minSupport . The maximum p value threshold effectively reduces the number of discovered results .
102
101
) c e s ( e m i i t g n n n u R simpl . model w/o Eval( )
103
102
101 s r o t c e v − b u s d e s o c f o
# l p−value <= 1 p−value <= 0.01
103
) c e s ( e m i i t g n n n u R
102
101
100 5 exact model simpl . model w/o Eval( )
10
15 minSupport ( % )
20
25
105
104
103 s r o t c e v − b u s d e s o c f o
# l
102 5
10
15 minSupport ( % )
20
25 p−value <= 1 p−value <= 0.01
100 1
2
3
4 6 minSupport ( % )
5
7
8
9
100 1
2
3
4 6 minSupport ( % )
5
7
8
9
( a ) Running time
( b ) # of Closed Sub vectors
Figure 14 . ClosedVect on MSNBC page visits data
( a ) Running time
( b ) # of closed sub vectors
Figure 13 . ClosedVect on chemical compounds
732 MSNBC Page Visits Data
We also run the ClosedVect algorithm on the MSNBC page visits dataset . The dataset is available in the UCI KDD archive repository6 . It records the page visits of msnbc.com on a specific day . The dataset consists of 989,818 records , each of which is a sequence of page categories visited by a user . There are 17 page categories . Thus , each record in the data set is a vector of size 17 and we are interested in finding the sub vectors that denote significant visit patterns .
6http://kddicsuciedu/databases/msnbc/msnbchtml
11
A preliminary study of the results shows that the most significant sub vectors are those with skewed distributions and at least two non zero bins . For example , a discovered sub vector with a high statistical significance was one in which users visited the frontpage seven times and news once ; the corresponding support was only 11 % In contrast , another pattern in which users visited the frontpage eight times was not statistically significant , even though its support was 21 %
8 Related Work
Graph mining has been an active research topic recently . In the area of mining frequent subgraphs from a transactional graph database , Inokuchi et al . [ 7 ] addressed the problem using an Apriori approach . Kuramochi and Karypis [ 8 ] proposed FSG , an Apriori based approach to frequent subgraph discovery . Yan and Han [ 9 ] proposed gSpan that efficiently explores frequent subgraphs . Their later work [ 10 ] searched closed frequent subgraphs . Huan et al . [ 11 ] explored frequent subgraphs using a canonical adjacency matrix representation of graphs . Their later work [ 12 ] searched maximal frequent subgraphs . Vanetik et al . [ 13 ] proposed an Apriori approach using paths as building blocks . Their later work [ 14 ] addressed partially labeled graph patterns . In the area of mining frequent subtrees , Zaki [ 17 ] developed an algorithm to find frequent subtrees in a forest . Chi et al . [ 18 ] presented an index technique for free trees and applied it to frequent subtree mining . All these techniques focus on finding frequent subgraphs or subtrees . Statistical significance of the frequent patterns was not addressed .
Milo et al . [ 15 ] identified network motifs in complex networks . They defined network motifs as graph patterns that appear significantly more frequently than those in randomized networks . However , their method deals with a single large graph , whereas our model deals with a large collection of graphs . Moreover , they computed the p value by simulation : they generated a number of randomized networks , and counted the number of networks that contained the subgraph with a support at least the observed support . This approach cannot compute p values with high accuracy because the generation of N randomized networks can never yield a non zero p value of less than 1/N . In contrast , our method is deterministic and computes accurate p values .
In the study of large graphs such as the Internet , random graph models [ 19 ] are used to describe the graph topology . Faloutsos et al . [ 20 ] showed that degrees of nodes of the Internet follow a power law distribution . Albert et al . [ 21 ] showed the small world phenomena of the world wide web . Leskovec et al . [ 22 ] observed how graphs evolve over time in terms of densities and diameters etc . Whereas these studies pertain to properties of the graph topology , we target the discovery of recurring subgraphs in a collection of graphs .
In an approach to interestingness measurement , Bayardo and Agrawal [ 23 ] proposed to mine an optimal set of rules according to a partial order defined using both rule support and confidence . Jaroszewicz and Simovici [ 24 ] defined the interestingness of frequent itemsets as the difference between the support from data and the support estimated from a background Bayesian network . Amer Yahia et al . [ 25 ] proposed scoring methods based on both structure and content . The scoring methods are used for ranking answers to XML queries .
In the area of frequent itemset mining , Srikant and Agrawal [ 26 ] addressed the problem of mining quantitative association rules . Han et al . [ 27 ] developed an algorithm for mining top K frequent closed patterns .
9 Conclusions
Statistical significance and ranking are useful in the postprocessing of data mining results . In this paper , we proposed a probabilistic model for frequent subgraphs , and show that frequent subgraphs can be effectively ranked by their p values . By representing graphs in the feature space , we derived an exact model which leads to a closed form solution for the p values . Efficient methods were developed for computing the p values and lower bounds . A simplified model was further proposed to improve efficiency . We also addressed the problem of feature vector mining , and developed an algorithm that efficiently searches significant closed sub vectors . Experimental results validated the quality , performance , and practical usefulness of the presented techniques .
Although presented in the context of graphs , the proposed techniques are generic and can be applied to mining of other complex data , such as trees . Future directions are integration of the significance measurement and graph mining techniques , incorporation of feature dependency into the probabilistic model , and development of better approximations and lower bounds .
Acknowledgements : We would like to thank Xifeng Yan and Jiawei Han for providing the code of CloseGraph . The work was supported in part by NSF grants IIS 0612327 and DBI 0213903 .
References
[ 1 ] E . Rahm and P . Bernstein . A survey of approaches to automatic schema matching . VLDB J . 10(4 ) : 334 350 ( 2001 ) .
[ 2 ] S . Berretti , A . D . Bimbo , and E . Vicario . Efficient matching and indexing of graph models in contentbased retrieval . In IEEE Trans . on Pattern Analysis and Machine Intelligence , volume 23 , 2001 .
[ 3 ] S . White and P . Smyth . Algorithms for estimating rel ative importance in networks . In KDD , 2003 .
[ 4 ] J . Hu , X . Shen , Y . Shao , C . Bystroff , and M . J . Zaki .
Mining protein contact maps . In BIOKDD , 2002 .
[ 5 ] R . Sharan , S . Suthram , R . M . Kelley , T . Kuhn , S . McCuine , P . Uetz , T . Sittler , R . M . Karp , and T . Ideker . Conserved patterns of protein interaction in multiple species . In Proc Natl Acad Sci , 2005 .
[ 6 ] S . Kramer , L . D . Raedt , and C . Helma . Molecular feature mining in HIV data . In KDD , 2001 .
[ 7 ] A . Inokuchi , T . Washio , and H . Motoda . An aprioribased algorithm for mining frequent substructures
12
[ 25 ] S . Amer Yahia , N . Koudas , A . Marian , D . Srivastava , and D . Toman . Structure and content scoring for XML . In VLDB , 2005 .
[ 26 ] R . Srikant and R . Agrawal . Mining quantitative assoIn SIGMOD , ciation rules in large relational tables . 1996 .
[ 27 ] J . Han , J . Wang , Y . Lu , and P . Tzvetkov . Mining top K frequent closed patterns without minimum support . In ICDM , 2002 . from graph data . Knowledge Discovery , pages 13–23 , 2000 .
In Principles of Data Mining and
[ 8 ] M . Kuramochi and G . Karypis . Frequent subgraph dis covery . In ICDM , pages 313–320 , 2001 .
[ 9 ] X . Yan and J . Han . gSpan : Graph based substructure pattern mining . In ICDM , 2002 .
[ 10 ] X . Yan and J . Han . CloseGraph : Mining closed fre quent graph patterns . In KDD , 2003 .
[ 11 ] J . Huan , W . Wang , and J . Prins . Efficient mining of frequent subgraph in the presence of isomorphism . In ICDM , 2003 .
[ 12 ] J . Huan , W . Wang , J . Prins , and J . Yang . SPIN : Mining maximal frequent subgraphs from graph databases . In KDD , 2004 .
[ 13 ] N . Vanetik , E . Gudes , and S . E . Shimony . Computing frequent graph patterns from semistructured data . In Proceedings of ICDM , 2002 .
[ 14 ] N . Vanetik and E . Gudes . Mining frequent labeled and partially labeled graph patterns . In ICDE , 2004 .
[ 15 ] R . Milo , S . Shen Orr , S . Itzkovitz , N . Kashtan , D . Chklovskii , and U . Alon . Network motifs : Simple building blocks of complex networks . Science , October 2002 .
[ 16 ] S . Theodoridis and K . Koutroumbas . Pattern Recognition , chapter 5 , pages 181–183 . Academic press , second edition , 2003 .
[ 17 ] M . J . Zaki . Efficiently mining frequent trees in a for est . In KDD , 2002 .
[ 18 ] Y . Chi , Y . Yang , and R . R . Muntz . Indexing and min ing free trees . In ICDM , 2003 .
[ 19 ] P . Erdos and A . Renyi . On random graphs . In Publ .
Math . , pages 290–297 , 1959 .
[ 20 ] M . Faloutsos , P . Faloutsos , and C . Faloutsos . On In power law relationships of the internet topology . SIGCOMM , 1999 .
[ 21 ] R . Albert , H . Jeong , and A L Barabasi . Diameter of the world wide web . In Nature , pages 401:130–131 , 1999 .
[ 22 ] J . Leskovec , J . Kleinberg , and C . Faloutsos . Graphs over time : Densification laws , shrinking diameters and possible explanations . In KDD , 2005 .
[ 23 ] R . J . Bayardo and R . Agrawal . Mining the most inter esting rules . In KDD , 1999 .
[ 24 ] S . Jaroszewicz and D . A . Simovici .
Interestingness of frequent itemsets using bayesian networks as background knowledge . In KDD , 2004 .
13
APPENDIX
A Proof of Lemma 2
Proof . ( 1 ) According to Eqn . ( 6 ) , let x1 = ( xi , xj ) and 1 = ( xi + 1 , xj − 1 ) . It is sufficient if we can show that x′ P ( x1 ; s ) ≥ P ( x′
1 ; s ) for all s ≥ xi + xj .
P ( x1 ; s ) = s−xjX t=xi t )θt ( s iθs−t j
P ( x′
1 ; s ) = s−xj +1X t=xi+1
( s t )θt iθs−t j
P ( x1 ; s ) − P ( x′ i θs−xi xi )θxi
= ( s j
1 , s )
− ( s xj −1)θs−xj +1 i
θxj −1 j
Since xi ≥ xj and s ≥ xi + xj , we get ( s Since θi ≤ θj and xi ≤ s − xj + 1 , we get θxi θs−xj +1
. Thus , P ( x1 ; s ) − P ( x′
θxj −1
1 ; s ) ≥ 0 . xi ) ≥ ( s xj −1 ) . ≥ i θs−xi j i j
( 2 ) Let x1 = ( xi , xj ) , x′
1 = ( xj , xi ) ,
P ( x1 , s ) =
P ( x′
1 , s ) = s−xjX t=xi s−xiX t=xj
( s t )θt iθs−t j
( s t )θt iθs−t j
Since xi < xj and s ≥ xi + xj , let b = min(s − xj , xj ) , then
P ( x1 , s ) − P ( x′
1 , s )
=
= bX t=xi bX t=xi
( s t )θt iθs−t j − s−xiX t=s−b
( s t )θt iθs−t j
( s t )θt iθs−t j − bX t=xi t )θs−t ( s i
θt j
It can be verified that t ≤ s − t when xi ≤ t ≤ b . It follows that θt j . Thus , P ( x1 , s ) − P ( x′ θt j ≥ θs−t
1 , s ) ≥ 0 . iθs−t i
14
