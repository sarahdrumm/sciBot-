P3C : A Robust Projected Clustering Algorithm
Gabriela Moise
J¨org Sander
Martin Ester
Dept . of Computing Science
Dept . of Computing Science
School of Computing Science
University of Alberta gabi@csualbertaca
University of Alberta joerg@csualbertaca
Simon Fraser University ester@cssfuca
Abstract
Projected clustering has emerged as a possible solution to the challenges associated with clustering in high dimensional data . A projected cluster is a subset of points together with a subset of attributes , such that the cluster points project onto a small range of values in each of these attributes , and are uniformly distributed in the remaining attributes . Existing algorithms for projected clustering rely on parameters whose appropriate values are difficult to set by the user , or are unable to identify projected clusters with few relevant attributes .
In this paper , we present a robust algorithm for projected clustering that can effectively discover projected clusters in the data while minimizing the number of parameters required as input . In contrast to all previous approaches , our algorithm can discover , under very general conditions , the true number of projected clusters . We show through an extensive experimental evaluation that our algorithm : ( 1 ) significantly outperforms existing algorithms for projected clustering in terms of accuracy ; ( 2 ) is effective in detecting very low dimensional projected clusters embedded in high dimensional spaces ; ( 3 ) is effective in detecting clusters with varying orientation in their relevant subspaces ; ( 4 ) is scalable with respect to large data sets and high number of dimensions .
1
Introduction
Projected clustering has been mainly motivated by seminal research showing that , as the dimensionality increases , the farthest neighbor of a point is expected to be almost as close as its nearest neighbor for a wide range of data distributions and distance functions [ 6 ] . Due to this lack of contrast in distances , the concept of proximity , and subsequently the concept of a “ cluster ” , are seriously challenged in high dimensional spaces . At the same time , irrelevant attributes are as important a motivation as the number of attributes for projected clustering . Even in data sets with moderate dimensionality , clusters may exist in subspaces , which are defined as subsets of attributes . The irrelevant attributes may in fact “ hide ” the clusters by making two objects that belong to the same cluster look as dissimilar as an arbitrary pair of objects . Furthermore , data objects may cluster differently in varying subspaces .
Traditional feature selection techniques are not effective in this scenario , because they may remove attributes that are relevant for some clusters and it may not be possible to recover those clusters in the remaining attributes [ 10 ] . Global feature transformation techniques ( eg , PCA ) , preserve to some extent the information from irrelevant attributes , and they may thus be unable to identify clusters that exist in different subspaces [ 10 ] .
Projected clustering assumes that meaningful structure can be detected only when data is projected onto subspaces of lower dimensionality . Virtually all existing projected clustering algorithms ( PROCLUS [ 1 ] , DOC/FASTDOC [ 11 ] , HARP [ 14 ] , SSPC [ 15 ] , EPCH [ 9 ] ) assume , explicitly or implicitly , the following definition of a projected cluster .
Definition 1 . Given a database D of d dimensional points . A projected cluster is defined as a pair ( Xi , Yi ) , where ( 1 ) Xi is a subset of D , and ( 2 ) Yi is a subset of attributes so that the projection of the points in Xi along each attribute a ∈ Yi has a small variance , compared to the variance of the whole data set on a , and ( 3 ) the points in Xi are uniformly distributed along every other attribute not in Yi . For a projected cluster ( Xi , Yi ) , the attributes in Yi are called the “ relevant ” attributes for Xi , whereas the remaining attributes are called “ irrelevant ” attributes for Xi . The data model in projected clustering assumes that the data consists of k projected clusters , {(Xi , Yi)}i=1,k 1 , and a set of outliers , O , where {X1 , . . . , Xk , O} form a partition of D . The subsets of attributes {Yi}i=1,k may not be disjoint and they may have different cardinalities . The outliers O are assumed to be uniformly distributed throughout the space . The projected clustering problem is to detect k projected clusters in the data , plus possibly a set of outliers .
1Notation i = 1 , k denotes all integers i between 1 and k .
Definition 1 states that the relevant attributes Yi of a projected cluster ( Xi , Yi ) are a subset of the data attributes . Such projected clusters are easily interpretable by the user because the original attributes of the data set have specific meaning in real life applications . ORCLUS [ 2 ] generalizes projected clusters ( Xi , Yi ) by assuming that Yi is an arbitrary set of orthogonal vectors .
Projected clustering is related to subspace clustering [ 3 ] in that both detect clusters of objects that exist in subspaces of a data set . In contrast to projected clustering , subspace clustering detects clusters of objects in all subspaces of a data set and tends to produce a large number of overlapping clusters . Related problems have been addressed in the bi clustering community [ 8 ] , where ( sub)sets of objects are considered similar if they follow similar “ rise and fall ” patterns across a ( sub)set of attributes .
The performance of existing projected clustering algorithms depends greatly on ( 1 ) a series of parameters whose appropriate values are difficult to anticipate by the users ( eg , the true number of projected clusters or the average dimensionality of subspaces where clusters exist ) , or ( 2 ) the computation of k initial clusters , which is typically performed in full dimensional space based on various heuristics . The performance of the algorithms that fall within the second category depends on how well the initial clusters approximate projected clusters in the data . These algorithms are likely to be less effective in the practically most interesting case of projected clusters with very few relevant attributes , because the members of such clusters are likely to have low similarity in full dimensional space .
In this paper , we propose an algorithm for mining projected clusters , called P3C ( Projected Clustering via Cluster Cores ) with the following properties . • P3C effectively discovers the projected clusters in the data while being remarkably robust to the only parameter that it takes as input . Setting this parameter requires little prior knowledge about the data , and , in contrast to all previous approaches , there is no need to provide the number of projected clusters as input , since our algorithm can discover , under very general conditions , the true number of projected clusters . • P3C effectively discovers very low dimensional projected clusters embedded in high dimensional spaces . • P3C effectively discovers clusters with varying orien• P3C is scalable with respect to large data sets and high tation in their relevant subspaces . number of dimensions .
P3C is comprised of several steps . First , regions corresponding to projections of clusters onto single attributes are computed . Second , cluster cores are identified by spatial areas that ( 1 ) are described by a combination of the detected regions and ( 2 ) contain an unexpectedly large number of points . Third , cluster cores are iteratively refined into pro a2
S6
S5
S4
S1
C1
S3
C2
S2 a1
Figure 1 : Overlapping true p signatures jected clusters . Finally , the outliers are identified , and the relevant attributes for each cluster are determined .
The remainder of the paper is organized as follows . Section 2 introduces preliminary definitions . Section 3 describes our algorithm . Section 4 presents an extensive experimental evaluation of P3C . Section 5 reviews work relevant for the this paper . Section 6 concludes the paper .
2 Preliminary Definitions
To present our algorithm for finding projected clusters , we introduce the following notation and definitions . Let D = ( xij)i=1,n,j=1,d be a data set of n ddimensional data objects . Let A = {a1 , . . . , ad} be the set of all attributes of the objects in D . We can assume , without restricting the generality , that all attributes have normalized values , ie , ( xij)i=1,n,j=1,d ∈ [ 0 , 1 ] . An interval S = [ vl , vu ] on attribute aj is defined as all real values x on aj so that vl ≤ x ≤ vu . The width of interval S is defined as width(S ) := vu − vl . The attribute of an interval S is denoted by attr(S ) , ie , attr(S ) = aj , if S ⊆ aj . In figure 1 , S1 , S2 , and S3 are intervals on attribute a1 , S4 , S5 and S6 are intervals on attribute a2 , attr(S1 ) = attr(S2 ) = attr(S3 ) = a1 , and attr(S4 ) = attr(S5 ) = attr(S6 ) = a2 . To ease the presentation , we specify the attribute of an interval only when it is necessary .
Let S be an interval on attribute aj . The support set of S , denoted by SuppSet(S ) , represents the set of database objects that belong to S , ie , SuppSet(S ) := {x ∈ D|x.aj ∈ S} . The support of S , denoted by Supp(S ) , is the cardinality of its support set , ie , Supp(S ) := |SuppSet(S)| . A p signature S is defined as a set S = {S1 , . . . , Sp} of p intervals on some ( sub)set of p distinct attributes } ( ji ∈ {1 , . . . , d} ) , where attr(Si ) = aji . Si {aj1 , . . . , ajp is also called the projection of S onto attribute aji , i = 1 , p . For example , in figure 1 , S = {S3 , S4} is a 2 signature , where S3 is the projection of S onto attribute a1 , and S4 is the projection of S onto attribute a2 . {S3 , S1} is not a 2 signature , because S3 and S1 are intervals on the same attribute a1 .
The support set of a p signature S = {S1 , . . . , Sp} , denoted by SuppSet(S ) , represents the set of database objects that are contained in the support sets of all intervals in S , ie , SuppSet(S ) := {x ∈ D|x ∈ p i=1 SuppSet(Si)} . The support of a p signature S , denoted by Supp(S ) , is the cardinality of its support set , ie , Supp(S ) := |SuppSet(S)| . A true p signature ˜S of a projected cluster ( Xi , Yi ) , Yi = {a1 , . . . , ap} , is a p signature {S1 , . . . , Sp} , where Si is the smallest interval on attribute ai that contains the projections onto ai of all the points in Xi , i = 1 , p . Figure 1 illustrates two projected clusters , C1 , and C2 , both having a1 and a2 as the only relevant attributes . The true p signature of C1 is the 2 signature {S1 , S6} , and the true p signature of C2 is the 2 signature {S2 , S4} .
Since an attribute may be relevant to more than one projected cluster , true p signatures may overlap , ie , they may contain overlapping intervals . In figure 1 , C1 and C2 have overlapping true p signatures , since intervals S1 and S2 overlap on attribute a1 . We assume that true p signatures can overlap as long as they are not nested within each other . True p signatures ˜S and ˜R are nested if for every interval Si in ˜S , there is an interval Sj in ˜R so that Si ⊆ Sj .
3 Algorithm P3C
P3C is based on the idea that if the true p signatures of projected clusters were known , then clusters can be immediately computed as the support sets of the true p signatures . Since the true p signatures are not known , P3C computes in two steps a set of p signatures that match or approximate well the true p signatures of projected clusters in the data . First , on every attribute , intervals that match or approximate well projections of true p signatures onto that attribute are computed ( section 31 ) Second , the challenge is to determine which intervals actually represent the same true psignature . P3C addresses this challenge by aggregating the computed intervals into cluster cores . Roughly speaking , a cluster core consists of a p signature S and its support set SuppSet(S ) , so that the p signature S approximates a true p signature ˜S of a projected cluster C , and a large fraction of the points in SuppSet(S ) belongs to C ( section 32 )
For the example in figure 1 , P3C first computes the interval S3 on attribute a1 that approximates the projections of the true 2 signatures {S1 , S6} and {S2 , S4} onto attribute a1 , and intervals S5 and S4 that approximate/match the projections of the same true 2 signatures onto attribute a2 . Second , P3C aggregates these intervals into two cluster cores , ie , {S3 , S4} and {S3 , S5} , which can be regarded as approximations of the two projected clusters in the data .
Cluster cores may include in their support sets additional points that do not belong to the projected clusters that they approximate . This happens when the intervals are wider than the projections of true p signatures that they approx
In figure 1 , interval S3 is wider than interval S2 , imate . and thus , the support set of cluster core {S3 , S4} includes points that do not belong to cluster C2 . On the other hand , cluster cores may not include completely in their support sets the projected clusters that they approximate . This is the case when the intervals are tighter than the projections of true p signatures that they approximate . In figure 1 , interval S5 is tighter than interval S6 , and thus the support set of cluster core {S3 , S5} does not include all points of cluster C1 . Thus , in order to compute the projected clusters , the supports sets of cluster cores are iteratively refined ( section 33 ) Finally , outliers are detected ( section 3.4 ) , and relevant attributes for each cluster are determined ( section 35 )
3.1 Projections of true p signatures
This section describes how P3C computes , for each attribute , intervals that match or approximate well projections of true p signatures onto that attribute .
An attribute that is irrelevant for all projected clusters exhibits , by definition 1 , uniform distribution . In contrast , an attribute that is relevant for at least one projected cluster will exhibit in general a non uniform distribution , because it contains one or more intervals with unusual high support corresponding to projections of clusters onto that attribute . Note that theoretically an attribute could exhibit uniform distribution , even though it is relevant for several projected clusters . This is the case when projected clusters are constructed in such a way that their projections on a specific attribute have equal support , and thus they form a uniform histogram . In such cases , it may still be possible to recover p signatures of the involved clusters , which are incomplete , but can be later refined unless projected clusters are constructed in such a way that all their relevant attributes look uniform . However , it is assumed that these situations are not common in typical applications for projected clustering .
We need to identify attributes with uniform distribution , and , for the non uniform attributes , to identify intervals with unusual high support . For this task , the Chi square goodness of fit test [ 13 ] is employed . Each attribute is divided into the same number of equi sized bins . Sturge ’s rule [ 13 ] suggests that the number of bins should be equal to '1 + log2(n)ff , where n is the number of data objects . For every bin in every attribute , its support is computed . The Chi square test statistic sums , over all bins in an attribute , the squared difference between the bin support and the average bin support , normalized by the average bin support . Based on the Chi square statistic , the uniform attributes are determined at a confidence level of α = 0001 The confidence level α does not act as a parameter of our method . α is set to one of the standard values used in statistical hypothesis testing : the value 0.001 signifies that the probability of declaring an attribute non uniform when in fact the attribute is uniform is very small , ie , less than 0001
On the attributes deemed non uniform , the bin with the largest support is marked . The remaining un marked bins are tested again using the Chi square test for uniform distribution . If the Chi square test indicates that the un marked bins “ look ” uniform , then we stop . Otherwise , the bin with the second largest support is marked . Then , we repeat testing the remaining un marked bins for the uniform distribution and marking bins in decreasing order of support , until the current set of un marked bins satisfies the Chi square test for uniform distribution . At this point , intervals are computed by merging adjacent marked bins . The process of marking bins is linear in the number of bins .
The computed intervals may be wider or tighter than the projections of true p signatures that they approximate . Overlapping true p signatures may lead to the former case ( eg , intervals S1 and S2 are approximated by interval S3 ) . An example of the latter case is an interval that approximates the projection of a true p signature onto an attribute where the cluster is normally distributed . In this case , the interval may only capture the most dense region of the projection ( eg , interval S5 on attribute a2 ) .
3.2 Cluster cores
In figure 1 , the computed intervals form only two possible 2 signatures , {S3 , S5} and {S3 , S4} , which actually represent the two projected clusters C1 and C2 . However , in practical applications , the number of possible p signatures that can be constructed from the set of computed intervals is large . The challenge is to determine which p signatures do in fact represent projected clusters . This section describes how P3C addresses this challenge . } be a ( p + 1)Let S be a p signature . Let R = S ∪{S signature composed of S and an interval S that is not in S . Assuming that S is a subset of some true t signature T also belongs ( t > p ) , we could ask the question whether S does belong to T , the support Supp(R ) of to T . When S R is likely to have a larger value than in the case when S does not belong to T , because , in the former case , Supp(R ) should include a large fraction of the projected cluster with } signature T . Clearly , the support Supp(R ) of R = S ∪{S is equal to the number of points in SuppSet(S ) that also . Therefore , we want to compute how many belong to S points in SuppSet(S ) are expected to belong to S in the case when S does not belong to T .
The points in SuppSet(S ) are mainly points of a projected cluster with signature T , and interval S does not belong to T . In this case , under the assumption that points in SuppSet(S ) are uniformly distributed in the attribute of , the expected number of points in SuppSet(S ) interval S ) . The folthat also belong to S lowing definition formally introduces the notion of expected is proportional to width(S support of a ( p + 1) signature R = S ∪{S a p signature S obtained by adding interval S
} with respect to to S .
} Definition 2 . Let S be a p signature . Let R = S ∪{S be a ( p + 1) signature composed of S and interval S ( S not in S ) . The expected support of the ( p + 1) signature R }|S ) , given the p signature S , denoted by ESupp(R = S ∪{S is defined as :
ESupp(R = S ∪{S
:= Supp(S ) * width(S
}|S )
)
We consider that if the actual support Supp(R ) of R is significantly larger than the expected support ESupp(R = S }|S ) of R given S , then this is evidence that S ∪{S belongs to the same true t signature as S . We need a quantitative way of deciding when the observed support Supp(R ) of R = S ∪{S } is significantly }|S ) of larger than the expected support ESupp(R = S ∪{S R given S . For this task , we employ the Poisson probability density function P oisson(v , E ) of observing v occurrences of a certain event within a time interval/spatial region , given the expected number E of random occurrences per time interval/spatial region [ 13 ] :
P oisson(v , E ) := exp(−E)∗Ev v! where exp stands for the exponential function . In our case , we measure the probability of observing a certain number of points ( ie , Supp(R ) ) within a spatial region , given the expected number of points within this spatial region ( ie , }|S) ) , under a random process that uniESupp(R = S ∪{S formly distributes the points in SuppSet(S ) onto the attribute of S
.
We call an observed support significantly larger than an expected support , if the observed support is larger than the expected support , and the Poisson probability of observing the support given the expected support is smaller than a certain value , which we call the Poisson threshold .
The Poisson probability quantifies how likely the observed support Supp(R ) of R is with respect to the expected }|S ) of R given S : the less likely support ESupp(R = S ∪{S repthe observed support , the stronger the evidence that S resents the same projected cluster as S . The Poisson threshold is the only “ parameter ” required by P3C . The Poisson threshold is different from typical parameters used by clustering algorithms ( such as the number of clusters ) in that it requires little prior knowledge about the data . The Poisson threshold signifies the error probability that the user is willing to accept . Concretely , the value 1.0E − 20 for the Poisson threshold signifies that the probability of declaring that represents the same projected cluster as S , when in fact S this is not true , is very small , ie , less than 1.0E − 20 . This is why higher values for the Poisson threshold like 1.0E−1 are not useful . On the other hand , a very small value for the Poisson threshold would result in failing to recognize that represents the same projected cluster as S , when in fact S this is true . The robustness of P3C to the Poisson threshold is studied empirically in section 4 ( see figure 2 ) . Intuitively , a p signature S = {S1 , . . . , Sp} represents a projected cluster C if S consists of ( 1 ) only and ( 2 ) all intervals that represent cluster C . The first condition is equivalent to requesting that for any q signature Q ⊆ S ( q = 1 , p − 1 ) , and any interval S ∈ S \ Q , there is evrepresents the same projected cluster as Q . idence that S The second condition is equivalent to requesting that S is not in S , there is no evmaximal , ie , for any interval S represents the same projected cluster as S . idence that S Formally , a cluster core can be defined as following . Definition 3 . A p signature S = {S1 , . . . , Sp} together with its support set SuppSet(S ) is called a cluster core , if : 1 . For any q signature Q ⊆ S , q = 1 , p − 1 , and any in
∈ S \ Q , it holds that :
} ) > ESupp ( Q ∪{S
}|Q ) , and } ) , ESupp(Q ∪{S
}|Q ) ) < terval S Supp(Q ∪{S Poisson(Supp(Q ∪{S Poisson threshold 2 . For any interval S Supp(S ∪{S Poisson(Supp(S ∪{S Poisson threshold
} ) ≤ ESupp ( S ∪{S not in S , it holds that }|S ) , or } ) , ESupp(S ∪{S
}|S ) ) ≥
Condition 1 in definition 3 is equivalent to requesting , for any q signature Q ⊆ S ( q = 1 , p − 1 ) , and any interval } ) is significantly larger than ∈ S \ Q , that Supp(Q ∪{S S }|Q ) . Condition 2 in definition 3 is equivaESupp ( Q ∪{S not in S , that Supp(S lent to requesting , for any interval S } ) is not significantly larger than ESupp ( S ∪{S }|S ) . ∪{S Condition 1 in definition 3 is anti monotonic , in the sense that , given a p signature S that satisfies condition 1 , any sub signature of S also satisfies condition 1 . This fact motivates an Apriori like generation of p signatures that satisfy condition 1 . Condition 1 acts like the support test in frequent itemsets generation [ 4 ] : a signature consisting of ( q + 1 ) intervals will not be generated if any of its subsignatures consisting of q intervals does not satisfy condition 1 . p signatures that satisfy condition 1 are generated , and the ones that are “ maximal ” in the sense of condition 2 are reported as cluster cores .
3.3 Computing projected clusters
Let k be the number of cluster cores constructed according to section 32 The support sets of these cluster cores may not necessarily contain all and only the points of the projected clusters that the cluster cores approximate , depending on the accuracy of the intervals computed in section 31 In this section , we discuss how P3C refines the k cluster cores into k projected clusters .
The refinement of k cluster cores into k projected clusters is performed in a subspace of ( reduced ) dimension of the original d dimensional data , containing all ality d attributes that were deemed non uniform according to the analysis presented in section 31
The support sets of the k cluster cores are not necessarily disjoint , because they may contain , in addition to the members of the clusters approximated by the cluster cores , outlier objects and/or other clusters’ members that have the signatures of other cluster cores . The membership of data points to cluster cores can be described through a fuzzy membership matrix M = ( mil)i=1,n,l=1,k , where mil denotes the membership of object i to cluster core l ; it is defined as follows : mil = 0 , if data point i does not belong to the support set of any cluster core ; mil is equal to the fraction of clusters cores that contain data point i in their support set , if i is in the support set of cluster core l .
We want to compute for each data point its probability of belonging to each projected cluster using the Expectation Maximization ( EM ) algorithm [ 7 ] . For this purpose , we will initialize EM with the fuzzy membership matrix M . Since the fuzzy membership matrix M contains unassigned data points , ie , data points with membership 0 everywhere , we first assign these points to one of the k cluster cores .
In the case of projected clusters , by definition 1 , cluster members project closely to cluster means on the directions with the least spread . Thus , cluster members have shorter Mahalanobis distances to cluster means than noncluster members . Provided that the support set of a cluster core mainly consists of members of the projected cluster C approximated by the cluster core , data points with short Mahalanobis distance to the mean of the support set are highly likely to be members of C . Based on these considerations , unassigned data points are assigned to the “ closest ” cluster core in terms of Mahalanobis distances to means of support sets of cluster cores .
Once all unassigned points have been assigned to cluster cores , the fuzzy membership matrix M is equivalent to a fuzzy partition of the data points into k projected clusters . EM computes data points’ probabilities of belonging to projected clusters based on Mahalanobis distances between data points and the means of projected clusters . Therefore , cluster members have higher probabilities of belonging to their clusters than non cluster members . EM is considered to converge when the means of the projected clusters remain unchanged between two consecutive iterations . Typically , when starting with cluster cores , it takes only 5 to 10 iterations until convergence , since the cluster cores typically approximate well projected clusters in the data .
The output of EM is a matrix of probabilities that gives for each data point its probability of belonging to each projected cluster . Since the data model in projected clustering assumes disjoint projected clusters , we convert the matrix of probabilities produced by EM into a hard membership matrix by assigning each data point to the most probable pro jected cluster . Interestingly , our method can also be used to discover overlapping clusters . In this respect , P3C positions itself between projected and subspace clustering .
3.4 Outlier Detection
Although each data point has been assigned to a projected cluster in section 3.3 , the data set may contain outlier points that need to be identified . We use a standard technique for multivariate outlier detection [ 12 ] . The Mahalanobis distances between data points and the means of the projected clusters to which they belong are compared to dethe critical value of the Chi square distribution with d grees of freedom at a confidence level of α = 0001 The confidence level α signifies that the probability of failing to recognize a true outlier is less than 0001 Data points with Mahalanobis distances larger than this critical value are declared outliers .
3.5 Relevant Attributes Detection
Once the cluster members have been identified , the relevant attributes for each projected cluster can be determined . The relevant attributes of a projected cluster include the attributes of the intervals that make up the p signature of the cluster core based on which this cluster has been computed . As discussed in section 3.1 , an attribute may be considered uniform although it may be relevant for several projected clusters . To cover these rather rare cases too , we test , for each projected cluster , using the Chi square test , whether its members are uniformly distributed in the attributes initially deemed uniform . When the members of a projected cluster are not uniformly distributed in one of the attributes initially considered uniform , then those attributes are included in the attributes considered relevant for the projected cluster . Finally , the p signatures of projected clusters can be refined by computing for each relevant attribute the smallest interval that the cluster members project onto .
4 Experimental Evaluation
The experiments reported in this section were conducted on a Linux machine with 3 GHz CPU and 2 GB RAM .
Synthetic Data . Synthetic data sets were generated as described in [ 1 ] , [ 2 ] , with n = 10 , 000 data points , d = 100 attributes , 5 clusters with sizes 15 % to 25 % of n , and 5 % of n outliers . The performance of P3C is studied based on the following criteria :
1 . Distribution of cluster points in the relevant subspace : uniform versus normal .
2 . Projected clusters having an equal number of relevant attributes versus projected clusters having different numbers of relevant attributes .
F1 Cluster points
F1 Relevant Dim l e u a v 1 F
1
0.95
0.9
1.00E
10
1.00E
20
1.00E
30
1.00E
1.00E
1.00E
1.00E
1.00E
80
1.00E
90
1.00E100
60
50
40 70 Poisson threshold
Figure 2 : P3C ’s sensitivity to the Poisson threshold
3 . Projected clusters with axis parallel orientation versus projected clusters with arbitrary orientation .
Combining these three criteria results in 8 categories of data sets . A data set in the category “ Uniform Equal Parallel ” is a data set for which the cluster points are uniformly distributed in the relevant subspace , the number of relevant attributes for each projected cluster is equal , and the eigenvectors of each projected cluster ’s covariance matrix are parallel to the coordinate axes . In each category , we generated data sets with average cluster dimensionality 2 % , 4 % , 6 % , 8 % , 10 % , 15 % , and 20 % of the data dimensionality d . In total , 56 synthetic data sets have been generated .
For data sets where cluster points are normally distributed in their relevant subspace , we ensured that the variance of cluster members on individual relevant attributes is between 1 % and 10 % of the variance of all data points when uniformly distributed on an attribute . Various amounts of overlap were introduced among the signatures of projected clusters , ie , the larger the average cluster dimensionality , the higher the chance for the overlap between signatures .
Real Data . We have tested P3C on two real world data sets . The first data set is the colon cancer data set of Alon et al . [ 5 ] that measures the expression level of 40 tumor and 22 normal colon tissue samples in 2000 human genes . The task is to discover projected clusters using samples as data objects and genes as attributes . This task is challenging due to the data sparsity ( ie , 62 data points in 2000 attributes ) , but of practical importance . A relevant attribute of a projected cluster represents a gene that has similar values in the samples that belong to the projected cluster . Provided that a projected cluster contains mainly tumor or normal samples , the relevant attributes are potential indicators for the presence , respectively absence , of colon cancer .
Projected clusters may exist in data sets with moderate dimensionality when some of the attribute are irrelevant . The second data set is the Boston housing data 2 , which consists of 12 numerical attributes of 506 suburbs of Boston .
2http://wwwicsuciedu/ mlearn/MLRepository.html
Since this data set is not labeled , we apply clustering in an exploratory fashion , and report interesting findings .
Experimental setup . We evaluate the performance of P3C against the following competing algorithms for projected clustering 3 : PROCLUS [ 1 ] , FASTOC [ 11 ] , HARP [ 14 ] , SSPC [ 15 ] , and ORCLUS [ 2 ] .
P3C requires only one parameter setting , namely the Poisson threshold . P3C does not require the user to set the target number of clusters ; instead , it discovers a certain number of clusters by itself . In contrast , all competing algorithms require the user to specify the target number of clusters .
On synthetic data , we have run the competing algorithms with the target number of clusters equal to the true number of projected clusters . PROCLUS and ORCLUS require the average cluster dimensionality as a parameter , which was set to the true average cluster dimensionality . HARP requires the maximum percentage of outliers as a parameter , which was set to the true percentage of outliers . For FASTDOC and SSPC , several reasonable values for their parameters were tried , and we report results for the parameter settings that consistently produced the best accuracy . SSPC was run without any semi supervision . Except HARP , all competing algorithms are non deterministic ; thus each of them is run five times , and the results are averaged .
On the colon cancer data , we have run the competing algorithms with the target number of clusters equal to the number of classes ( ie , 2 ) . Multiple values were tried for the other parameters required by the competing algorithms , and the results with the best accuracy are reported . Since this data set contains no points labeled as outliers , the outlier removal option of all algorithms was disabled .
On the housing data , since it has no labels , the evaluation of the competing algorithms is cumbersome . The reason is that the performance of the competing algorithms is dependent on a large number of required parameters , including critical ones such as the number of clusters and the average cluster dimensionality . Under these circumstances , we apply only P3C on the second real data set .
Performance measures . We refer to true clusters as input clusters , and to found clusters as output clusters . On synthetic data , cluster labels and relevant attributes for each cluster are known . On the colon cancer data , only the cluster labels are known . We use an F 1 value to measure the clustering accuracy . For each output cluster i , we determine the input cluster ji with which it shares the largest number of data points . The precision of output cluster i is defined as the number of data points common to i and ji divided by the total number of data points in i . The recall of output
3We intended to compare with EPCH [ 9 ] too , but after consulting with its authors , and using the original implementation , we could not find a parameter setting that produces results with reasonable accuracy on our synthetic data sets . cluster i is defined as the number of data points common to i and ji divided by the total number of data points in ji . The F 1 value of output cluster i is the harmonic mean of its precision and recall . The F 1 value of a clustering solution is obtained by averaging the F 1 values of all its output clusters . Similarly , we use an F 1 value to measure the accuracy of found relevant attributes based on the matching between output and input clusters .
Sensitivity analysis . We have studied the sensitivity of P3C to the Poisson threshold . Figure 2 illustrates the accuracy of P3C measured using the two F 1 values introduced above for one of our synthetic data sets as the Poisson threshold is progressively decreased from 1.0E − 10 to 10E−100 We observe that P3C is remarkably robust with respect to the Poisson threshold . Similar results have been obtained on all our synthetic data sets , but are omitted due to space limitations . Consequently , we have set the Poisson threshold at 1.0E − 20 .
Accuracy results . On synthetic data , in all the performed experiments , the number of clusters discovered by P3C equals the true number of projected clusters in the data . Figures 3 to 10 show the accuracies of the compared algorithms as a function of increasing average cluster dimensionality for the 8 categories of data sets . We observe that P3C significantly and consistently outperforms the competing projected clustering algorithms , both in terms of clustering accuracy and in terms of accuracy of the found relevant attributes .
The difference in performance between P3C and previous methods is particularly large for data sets that contain very low dimensional projected clusters embedded in high dimensional spaces . Even in these difficult cases P3C shows very high accuracies , in contrast to the modest accuracies obtained by the competing algorithms . As the average cluster dimensionality increases , the accuracy of the competing algorithms increases as well .
Our experiments indicate that P3C effectively discovers projected clusters with varying orientation in their relevant subspaces . The accuracy of P3C on data sets where projected clusters have axis parallel orientation is as high as the accuracy of P3C on data sets where projected clusters have arbitrary orientation .
The accuracy of P3C on data sets where projected clusters are uniformly distributed in their relevant subspaces is slightly higher than the accuracy of P3C on data sets where projected clusters are normally distributed in their relevant subspaces . The reason is that projections of clusters onto their relevant attributes can be approximated more faithfully by the computed intervals for clusters in the former category than for clusters in the latter category .
The number of relevant attributes for projected clusters does not have an impact on the performance of P3C . This is to be expected , since P3C does not use in any way the
P3C
1.00
0.80
0.60
0.40
0.20
0.00
P3C
1.00
0.80
0.60
0.40
0.20
0.00
P3C
1.00
0.80
0.60
0.40
0.20
0.00
P3C
1.00
0.80
0.60
0.40
0.20
0.00 i s t n o p r e t s u C l l e u a v 1 F i s t n o p r e t s u C l l e u a v 1 F i s t n o p r e t s u C l l e u a v 1 F i s t n o p r e t s u C l l e u a v 1 F
SSPC
PROCLUS
HARP
FASTDOC
ORCLUS
P3C
SSPC
PROCLUS
HARP
FASTDOC
ORCLUS r t t
A t n a v e e R l l e u a v 1 F
1.00
0.80
0.60
0.40
0.20
0.00
2 %
6 %
4 % Average Cluster Dimensionality
10 %
8 %
15 %
20 %
2 %
6 %
4 % Average Cluster Dimensionality
10 %
8 %
15 %
20 %
Figure 3 : Category Uniform Equal Parallel
SSPC
PROCLUS
HARP
FASTDOC
ORCLUS
P3C
SSPC
PROCLUS
HARP
FASTDOC
ORCLUS r t t
A t n a v e e R l l e u a v
1 F
1.00
0.80
0.60
0.40
0.20
0.00
2 %
6 %
4 % Average Cluster Dimensionality
10 %
8 %
15 %
20 %
2 %
6 %
4 % Average Cluster Dimensionality
10 %
8 %
15 %
20 %
Figure 4 : Category Uniform Equal NonParallel
SSPC
PROCLUS
HARP
FASTDOC
ORCLUS
P3C
SSPC
PROCLUS
HARP
FASTDOC
ORCLUS r t t
A t n a v e e R l l e u a v 1 F
1.00
0.80
0.60
0.40
0.20
0.00
2 %
6 %
4 % Average Cluster Dimensionality
10 %
8 %
15 %
20 %
2 %
6 %
4 % Average Cluster Dimensionality
10 %
8 %
15 %
20 %
Figure 5 : Category Normal Equal Parallel
SSPC
PROCLUS
HARP
FASTDOC
ORCLUS
P3C
SSPC
PROCLUS
HARP
FASTDOC
ORCLUS r t t
A t n a v e e R l l e u a v 1 F
1.00
0.80
0.60
0.40
0.20
0.00
2 %
6 %
4 % Average Cluster Dimensionality
10 %
8 %
15 %
20 %
2 %
4 %
6 %
8 %
10 %
15 %
20 %
Average Cluster Dimensionality
Figure 6 : Category Normal Equal NonParallel average cluster dimensionality . Interestingly , the accuracy of the found relevant attributes is 100 % in all experiments . On the colon cancer data , P3C discovers 2 projected clusters . P3C obtains the highest clustering accuracy ( 67% ) , followed by HARP ( 55 % ) and SSPC ( 53% ) , whereas the accuracies of the other projected clustering algorithms are significantly lower on this data set : FASTDOC and PROCLUS obtain 43 % accuracy , and ORCLUS obtains 35 % . The dimensionality of these 2 projected clusters in 11 , which is much smaller than the dimensionality of the data set ( ie , 2000 ) . This indicates that only a relatively small fraction of genes out of the total number of genes may be relevant for distinguishing between cancer and normal tissues , as also noted in previous work [ 5 ] . The biological significance of the genes selected as relevant is yet to be determined .
On the housing data , P3C discovers 2 projected clusters , which exist in subspaces of dimensionality 4 . The first projected cluster contains suburbs that are similar in terms of residential land , crime rate , pollution and property tax . The second projected cluster contains suburbs that are similar in terms of business land , size , distance to employment centers , and property tax . This data set illustrates that projected clusters can exist in data sets with a moderate number of attributes when some of these attributes are irrelevant . To verify that the members of the 2 projected clusters are not close in full dimensional space , we have run KMeans ( k = 2 ) several times . In all runs , the members of the projected clusters discovered by P3C are distributed between the clusters found by KMeans , which indicate that full dimensional clustering cannot reproduce the same clusters .
Robustness to outliers . Data sets with n = 10 , 000 , d = 100 , 5 clusters , average cluster dimensionality 4 , and different percentages of outliers were generated . Figure 11 shows the accuracies of the compared algorithms as a function of increasing percentages of outliers . P3C , as well as the competing algorithms , are robust in the presence of outliers . The clustering accuracy of P3C decreases only slightly as more outliers are introduced . Even when the percentage of outliers in the data is as high as 25 % , P3C still obtains a clustering accuracy of 86 % . The accuracy of the found relevant attributes of P3C remains 100 % with increasing percentages of outliers .
Scalability experiments . In all scalability figures , the time is represented on a log scale .
Figure 12 shows scalability results for data sets with d = 10 , 2 clusters , 5 % outliers , average cluster dimensionality 2 , and increasing database sizes . The scalability of P3C with respect to database size is comparable to the scalability of the fastest projected clustering algorithms .
Figure 13 shows scalability results for data sets with n = 10 , 000 , 2 clusters , 5 % outliers , average cluster dimensionality 2 , and increasing database dimensionalities . P3C is relatively unaffected by increasing data dimension
P3C
SSPC
PROCLUS
HARP
FASTDOC
ORCLUS
4
3
2
1
0
) c e s n i e m i t ( g o l
10,000
100,000
500,000
1,000,000
Database Size
Figure 12 : Scalability with increasing database size ality , because attributes with uniform distributions are not involved in the computation of cluster cores .
Figure 14 shows scalability results for data sets with n = 10 , 000 , d = 100 , 5 clusters , 5 % outliers , and increasing average cluster dimensionalities . The running time of P3C increases with increasing average cluster dimensionality , due to the increased complexity of p signatures generation . However , as the average cluster dimensionality increases , clusters become increasingly detectable in full dimensional space . P3C has comparable running times to the other projected clustering algorithms at low average cluster dimensionality , which is the critical cases that “ fulldimensional ” clustering algorithms cannot deal with .
In summary , P3C consistently and significantly outperforms existing projected clustering algorithms in terms of clustering accuracy and accuracy of the found relevant attributes , while being as efficient as the fastest of these algorithms on data sets with low dimensional projected clusters .
5 Related Work
PROCLUS [ 1 ] is essentially a k medoid algorithm adapted to projected clustering . A main difference to the standard k medoid algorithm is that initial clusters around the medoids have to be computed as basis for the simultaneous selection of relevant attributes . The performance of PROCLUS crucially depends on 2 required input parameters ( k the desired number of projected clusters , and l the average cluster dimensionality ) , whose appropriate values are difficult to guess . Another weakness is the strong dependency on the initial clustering which is hard to determine since it is performed in full dimensional space where the “ true ” distances will be distorted by noisy attributes .
ORCLUS [ 2 ] is a generalization of PROCLUS that can discover clusters in arbitrary sets of orthogonal vectors . The quality of a projected cluster is defined as the sum of the variances of the cluster members along the projected attributes . Therefore , in order to identify the projection in
P3C
1.00
0.80
0.60
0.40
0.20
0.00
P3C
1.00
0.80
0.60
0.40
0.20
0.00
P3C
1.00
0.80
0.60
0.40
0.20
0.00
P3C
1.00
0.80
0.60
0.40
0.20
0.00 i s t n o p r e t s u C l l e u a v 1 F i s t n o p r e t s u C l l e u a v
1 F i s t n o p r e t s u C l l e u a v 1 F i s t n o p r e t s u C l l e u a v 1 F
SSPC
PROCLUS
HARP
FASTDOC
ORCLUS
P3C
SSPC
PROCLUS
HARP
FASTDOC
ORCLUS r t t
A t n a v e e R l l e u a v 1 F
1.00
0.80
0.60
0.40
0.20
0.00
2 %
6 %
4 % Average Cluster Dimensionality
10 %
8 %
15 %
20 %
2 %
4 %
6 %
8 %
10 %
15 %
20 %
Average Cluster Dimensionality
Figure 7 : Category Uniform Different Parallel
SSPC
PROCLUS
HARP
FASTDOC
ORCLUS
P3C
SSPC
PROCLUS
HARP
FASTDOC
ORCLUS r t t
A t n a v e e R l l e u a v 1 F
1.00
0.80
0.60
0.40
0.20
0.00
2 %
6 %
4 % Average Cluster Dimensionality
10 %
8 %
15 %
20 %
2 %
6 %
4 % Average Cluster Dimensionality
10 %
8 %
15 %
20 %
Figure 8 : Category Uniform Different NonParallel
SSPC
PROCLUS
HARP
FASTDOC
ORCLUS
P3C
SSPC
PROCLUS
HARP
FASTDOC
ORCLUS r t t
A t n a v e l e R e u l a v
1 F
1.00
0.80
0.60
0.40
0.20
0.00
2 %
4 %
6 %
8 %
10 %
15 %
20 %
2 %
Average Cluster Dimensionality
6 %
4 % Average Cluster Dimensionality
10 %
8 %
15 %
20 %
Figure 9 : Category Normal Different Parallel
SSPC
PROCLUS
HARP
FASTDOC
ORCLUS
P3C
SSPC
PROCLUS
HARP
FASTDOC
ORCLUS r t t
A t n a v e e R l l e u a v 1 F
1.00
0.80
0.60
0.40
0.20
0.00
2 %
6 %
4 % Average Cluster Dimensionality
10 %
8 %
15 %
20 %
2 %
6 %
4 % Average Cluster Dimensionality
10 %
8 %
15 %
20 %
Figure 10 : Category Normal Different NonParallel
P3C
1.00
0.80
0.60
0.40
0.20
0.00 i s t n o P r e t s u C l l e u a v 1 F
SSPC
PROCLUS
HARP
FASTDOC
ORCLUS
P3C
SSPC
PROCLUS
HARP
FASTDOC
ORCLUS r t t
A t n a v e e R l l e u a v 1 F
1.00
0.80
0.60
0.40
0.20
0.00
0 %
5 %
10 %
15 %
20 %
25 %
0 %
5 %
10 %
15 %
20 %
25 %
Percentage Outliers
Percentage Outliers
P3C
SSPC
PROCLUS
HARP
FASTDOC
ORCLUS
P3C
SSPC
PROCLUS
HARP
FASTDOC
ORCLUS
Figure 11 : Robustness to Noise
5
4
3
2
1
0
) c e s n i e m i t ( g o l
4
3
2
1
0
) c e s n i e m i t ( g o l
100
200
500
1,000
Database Dimensionality
2 %
4 %
6 %
8 %
10 %
15 %
20 %
Average Cluster Dimensionality
Figure 13 : Scalability with increasing database dim
Figure 14 : Scalability with increasing avg . cluster dim which a set of points cluster “ best ” according to the quality measure , ORCLUS selects the eigenvectors corresponding to the smallest eigenvalues of the covariance matrix of the given set of points . The parameter l is used to decide how many such eigenvectors to select . While ORCLUS can find significantly more general clusters , it inherits the weaknesses of PROCLUS discussed above .
DOC [ 11 ] defines a projected cluster as a pair ( C , D ) , where C is a subset of points , and D is a subset of attributes , such that C contains at least a fraction α of the total number of points , and D consists of all the attributes on which the projection of C is contained within a segment of length w . DOC defines the function µ to measure the quality of a projected cluster as µ(|C|,|D| ) = |C| ∗ ( 1/β)|D| where β is a user specified parameter that controls the trade off between the number of data points and the number of relevant attributes in a projected cluster . DOC computes one projected cluster at a time , optimizing its quality using a randomized algorithm with certain quality guarantees . In order to reduce the time complexity of DOC , its authors introduce a variant , called FASTDOC , which uses three heuristics to reduce the search time . Similar to PROCLUS , the performance of DOC is sensitive to the choice of the input parameters , whose values are difficult to determine for real life data sets . In addition , the assumption that a projected cluster is a hyper cube of same side length in all attributes may not be appropriate in real applications .
HARP [ 14 ] is an agglomerative , hierarchical clustering algorithm that starts by placing each data object in a cluster . Two clusters are allowed to merge if the resulting cluster has dmin or more relevant attributes , and an attribute is selected as relevant for the merged cluster if a given relevance score is greater than Rmin . dmin and Rmin are two internal thresholds that start at some harsh values so that only objects belonging to the same real cluster are likely to be merged . Subsequently , as the clusters increase in size , and the relevant attributes are more reliably determined , the two thresholds are progressively decreased , until they reach some base values or a certain number of clusters has been obtained . HARP avoids some of the problems of the previous approaches , such as the computation of initial clusters , or the usage of parameters whose values are difficult to set . However , HARP inherits the drawbacks of hierarchical clustering algorithms , in particular the lack of backtracking in the clustering process and the quadratic runtime complexity which makes it not scalable to large data sets .
Yip et al . proposes the algorithmSSPC [ 15 ] , similar in structure to PROCLUS , and whose performance can be improved by the use of domain knowledge in the form of labeled objects and/or labeled attributes . The algorithm uses an objective function based on the relevance score of HARP [ 14 ] . The quality of a clustering solution is the sum of the qualities of each individual cluster , and the quality of an individual cluster is the sum of the relevance scores of the cluster ’s relevant attributes . The performance of SSPC depends on a user defined parameter that controls tation of the comparing algorithms for projected clustering . This research was supported by the Alberta Ingenuity Fund and the iCORE Circle of Research Excellence .
References
[ 1 ] C . C . Aggarwal , C . Procopiuc , J . L . Wolf , P . S . Yu , and J . S . Park . Fast algorithms for projected clustering . In SIGMOD , 1999 .
[ 2 ] C . C . Aggarwal and P . S . Yu . Finding generalized projected clusters in high dimensional spaces . In SIGMOD , 2000 .
[ 3 ] R . Agrawal , J . Gehrke , D . Gunopulos , and P . Raghavan . Automatic subspace clustering of high dimensional data for data mining applications . In SIGMOD , 1998 .
[ 4 ] R . Agrawal and R . Srikan . Fast Algorithms for Mining As sociation Rules . In VLDB , 1994 .
[ 5 ] U . Alon , N . Barkai , D . Notterman , K . Gish , S . Ybarra , D . Mack , and A . J . Levine . Broad patterns of gene expression revealed by clustering of tumor and normal colon tissues probed by oligonucleotide arrays . PNAS , 96:6745– 6750 , 1999 .
[ 6 ] K . Beyer , J . Goldstein , R . Ramakrishnan , and U . Shaft . When is nearest neighbor meaningful ? LNCS , 1540:217– 235 , 1999 .
[ 7 ] A . Dempster , N . M . Laird , and D . B . Rubin . Maximum likelihood for incomplete data via the EM algorithm . J . R . Stat . Soc . , 39:1–38 , 1977 .
[ 8 ] S . C . Madeira and A . J . Oliveira . Biclustering algorithms for biological data analysis : a survey . IEEE TCBB , 1(1):24–45 , 2004 .
[ 9 ] E . Ng , A . Fu , and R . Wong . Projective clustering by his tograms . IEEE TKDE , 17(3):369–383 , 2005 .
[ 10 ] L . Parsons , E . Haque , and H . Liu . Subspace clustering for high dimensional data : a review . SIGKDD Explorations Newsletter , 6(1):90–105 , 2004 .
[ 11 ] C . M . Procopiuc , M . Jones , P . K . Agarwal , and T . M . Murali . A Monte Carlo algorithm for fast projective clustering . In SIGMOD , 2002 .
[ 12 ] P . J . Rousseeuw and B . C . V . Zomeren . Unmasking multivariate outliers and leverage points . J . Amer . Stat . Assoc . , 85(411):633–651 , 1990 .
[ 13 ] G . W . Snedecor and W . G . Cochran . Statistical Methods .
Iowa State University Press , 1989 .
[ 14 ] K . Y . Yip , D . W . Cheung , and M . K . Ng . HARP : A practical projected clustering algorithm . IEEE TKDE , 16(11):1387– 1397 , 2004 .
[ 15 ] K . Y . Yip , D . W . Cheung , and M . K . Ng . On discovery of extremely low dimensional clusters using semi supervised projected clustering . In ICDE , 2005 . the relevance scores of attributes . SSPC can find projected clusters with moderately low dimensionality whereas most other methods fail due to an initialization based on the fulldimensional space .
EPCH [ 9 ] computes low dimensional histograms ( 1D or 2D ) , and “ dense ” regions are identified in each histogram , based on iteratively lowering a threshold that depends on a user specified parameter . For each data object , a “ signature ” is derived , which consists of the identifiers of the dense regions the data object belongs to . The similarity between two objects is measured by the matching coefficient of their signatures in which zero entries in both signatures are ignored . Objects are grouped in decreasing order of similarity until at most a user specified number of clusters is obtained . EPCH differs from our method both in how the computation of low dimensional projections of projected clusters is performed , and in how these projections are used to recover projected clusters . In particular , dense regions from different attributes are not combined into higher dimensional regions , but used to measure the similarity of pairs of objects . In addition , the performance of EPCH is sensitive to the values of its parameters .
6 Conclusions
Projected clustering is motivated by data sets with a large number of attributes or with irrelevant attributes . Existing projected clustering algorithms crucially depend on user parameters whose appropriate values are often difficult to anticipate , and are unable to discover low dimensional projected clusters . In this paper , we address these drawbacks through the novel , robust projected clustering algorithm P3C . P3C is based on the computation of so called cluster cores . Cluster cores are defined as regions of the data space containing an unexpectedly high number of points , forming cores of actual projected clusters . Cluster cores are generated in an Apriori like fashion , and subsequently refined into projected clusters . Lastly , outliers are removed and the relevant cluster attributes are detected . Our experimental evaluation on numerous synthetic data sets and two real data sets demonstrates that P3C can indeed discover projected clusters , including clusters in very low dimensional subspaces , and clusters with varying orientation , distribution or number of relevant attributes , while being robust to the only required parameter . P3C consistently outperforms the state of the art methods in terms of accuracy , and it is robust to noise . In addition , our algorithm scales well with respect to large data sets and high number of dimensions .
As future work , we will investigate the extension of P3C for categorical data .
Acknowledgments . We would like to thank Kevin Yip from Yale University for providing us with the implemen
