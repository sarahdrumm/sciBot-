A Data Mining Approach for Capacity Building of Stakeholders in Integrated
Flood Management
Peter Owotokiξ , Nataša Manojlović§ , Friedrich Mayer Lindenbergξ , Erik Pasche§
ξ Institute for Computer Technology , § Institute for River and Coastal Engineering
ξ§ Technische Universitaet Hamburg Harburg ( TUHH )
{owotoki , natasa.manojlovic , mayer lindenberg , pasche}@tu harburg.de
Abstract
New approaches to managing flood events are increasingly of more relevance due to recent widespread floods and the presumed changes in the climate . These approaches fall under the integrated flood management ( IFM ) banner and focus not only on flood prevention , but on flood resilience . This paper introduces an application ( FLORETO ) for IFM that utilizes the data mining approach , in a web based three tier system , devoted to the capacity building of stakeholders as a micro scale resilience strategy of IFM . The intelligent models , which constitute the business logic in FLORETO , are used to match the input parameters or design criteria , describing properties prone to flooding , to technically justifiable flood mitigation measures . Datasets from the German city of Kellinghusen were collected and intelligent models were built . Satisfactory results have been obtained , which shows the promise of this data mining approach and opens the door for its application for IFM in other regions .
1 . Introduction and Description of the Integrated Flood Management Domain
Over the past years the world has witnessed a growing probability and extent of floods , posing a risk to health and wellbeing and causing huge damage to properties and personal belongings . This problem gains even more importance since the climate change favours extreme weather situations and flooding becomes increasingly likely . The recent flood events ( Europe , 2006 , New Orleans , 200 , Asia 2004 ) showed that the existing conventional flood protection systems do not guarantee sufficient protection level of people and properties . Those conventional strategies , such as dikes and walls , that manage the risk of flooding by mitigating the probability of flooding , are very cost intensive and the financial resources usually can not be provided ad hoc . Also , improving ( raising ) dikes causes restrictions of the natural dynamics of a river system and spoils landscape qualities . New strategies to cope with flooding have to be developed and implemented to adapt the communities to climate change in an adequate way . the the objectives of
The governmental institutions worldwide are trying to redefine their flood policies . In Europe , the proposal for the “ Directive of the European Parliament and of the council on the assessment and management of floods ” {SEC(2006 ) 66} was released . This Directive extends the Water Framework Directive and interdisciplinary cross border , catchments based approach in flood management . Driven by the EU initiative for integrated flood management , many countries adapted or released new water policies to cope with floods . For example , in Germany those regulations are brought forward within the Flood Control Act ( FCA ) . [ 1 ]
( WDF ) , promoting coherent
These new integrative strategies present a new approach to manage the risk of flooding . Instead of mitigating probability of flooding , the preference is given to strategies that aim at mitigating their impact . Those strategies are called flood resilience strategies or non structural flood mitigation ( table 1 ) and are a substantial part of the Integrated Flood Management ( IFM ) . flood mitigation related to Flood Probability Assessment , Flood Impact Assessment , Non Structural Flood Mitigation and Flood Risk Management Planning .
IFM covers overall activities for
The term resilience originates from ecology and can be defined as the ability of a system to resist the perturbation or the speed at which the system recovers after being disturbed . The underlying idea behind flood resilience management is to foster the ability of areas to recover after they had been prone to flood . It implies that in a flood resilience policy flood damages have to be minimized and normal life has to return as soon as possible after flood . [ 2 ] .
Table 1 :
Overview of the Flood Resilience Strategies [ 1 ]
NonStructural Flood Mitigation ( Flood Resilience )
Capacity building of stakeholder s Contingency planning Financial Incentives
Land Control
Use
Raising Risk Awareness Adaptation the Environment of
Build
Emergency planning Flood warning Insurance Reimbursement Damage compensation Spatial Planning Building Restrictions
Regulations and
This paradigm shift in flood management from “ fighting flood ” to “ living with floods ” triggers a demand for more interaction among the experts of different fields such as spatial planning , social sciences , structural engineering or geology , but also for the active involvement of the public in integrated flood management . The public is not a passive receptor any more , it becomes an important stakeholder of flood management and has to be actively involved in decision making as decisions made influence the way of living in flood prone areas . But also , the citizens in flood prone areas can reduce the flood risk of their properties by protecting them in an adequate way .
Capacity building of stakeholders is a resilience strategy devoted to promote the importance of the concept of “ living with floods ” on the micro scale level , focusing on individual flood mitigation measures and increasing public awareness of flood hazard so that the people are able to adjust their homes by applying appropriate flood mitigation measures .
2 . The Need for a Capacity Building System and Specification of the Matching Problem summarized under
The mitigation measures applied to the existing buildings with the aim to reduce flood damage potential can be term retrofitting . Retrofitting can be defined as [ a combination of adjustments or addition to features of existing structures , that are intended to eliminate or reduce the possibility of flood damage [ 3 ] . By applying flood retrofitting measures , the resilience of the buildings can be improved either by preventing flood the the inventory above water entering the building or by applying waterproof materials and elevating the expected flood level . The main resilience strategies for the existing buildings are dry proofing and wet proofing [ 4 ] , often combined with using of easily removable inventory in lower parts of the building . The main flood retrofitting strategies are depicted in table 2 . In the case of dry proofing , floodwater is kept out of the building . Those techniques are generally more costintensive than the wet proofing ones and always carry a certain failure risk , as the stability of the building can be jeopardized through the increased water pressure .
By applying wet proofing measures floodwater is not prevented from entering the building , but the building is adapted to flooding in a way that potential damage is reduced by applying water resistant building material or adapting the occupancy of the building ( eg basement is not used for living ) . to the the main characteristics of
Those strategies have some shortcomings when applied solely . Most of the deficiencies are related to the limitations of the current construction technologies , design and materials , that are , to lesser or greater impact from flood . An extent , subjected overview of flood retrofitting strategies is depicted in table 3 . Before any decision is made , a selection of technically justifiable and logical solutions is to be performed based on the following general design criteria : a ) building type ( description of fabric and inventory ) b ) distance to the water bodies c ) availability of resources , d ) e ) parameters describing the flood event , f ) g ) stability issue ( owner description ) logistical parameters ,
Additionally , an important issue of a capacity building system is how to reach stakeholders and in which form they should get required knowledge on flood mitigation issues . environmental
Having recognized this importance of the active involvement of the stakeholders in flood management , many agencies or governmental institutions publish material or offer consultancy to inform citizens about possible mitigation measures for their homes . Taking advantage of the internet as a powerful communication media , a considerable part of provided information is available via internet and is usually presented in the form of pdf files or html text.A list in wwwtuhhde/wb/floreto examined web of sites is given
Overview of the main flood mitigation ( retrofitting ) strategies
Table 2 : strategy measure main characteristics techniques available
Dryproofing
Sealing water resistant concrete “ Weisse Wanne ”
Shielding
The building is sealed ie the external walls are used to hold back the flood water polymer bituminous seal “ Schwarze Wanne “ protection ( sealing)of the openings installing non return valves ( anti flooding device ) within the private sewage system as a protection from the sewerage backflow
The floodwater does not reach the building itself . Barriers are installed at some distance from the building or a group of properties flood barriers , flood skirts
Controlled flooding of building using
Wetproofing water resistant materials adapting occupancy building the the of is reduced
Flood damage potential of a by building applying water resistant materials . Due to economic or technical reasons , the lower parts building ( basement ) is partly flooded and not used for living . At the same time , the ground floor can be dry proofed . the of eg water resistant paints and coating , lime based plaster , plasters of synthetic resin , mineral fibre , insulation tiles , oil based paints raising the inventory , heating tanks and electrical appliances above the expected flood level
Elevating inventory the easily removable Using pieces of inventory in lover parts of the building eg removable furniture ( eg with wheels ) , rugs rather than fitted carpets
Web sites usually provide the public with the information whom to contact or who carries the responsibility for certain flood related issues . Some of the websites go a step forward and try to use animation tools such as simulations of a building getting flooded . Those animations are supported by , in some cases , detailed explanations of the mitigation measures that can be applied . But a capacity building system that would integrate both , hazard stakeholders and appropriate mitigation measures for each single case on the microscale level , is not available
21 FLORETO Platform awareness of
FLORETO is a system for capacity building of stakeholders in form of a web based advisory tool on a microscale level tailored to the user ’s own property data that at the same time :
Enables determination of suitable retrofitting measure(s ) which improve flood resilience of single households and consequently of the whole area . As a final output of the system , the user gets different flood retrofitting scenarios for his own property , based on the cost benefit analysis ( CBA ) . to
Improves flood hazard awareness of flood management and helps the stakeholders by providing them with the information related them understanding the main terms and concepts relevant for flood protection and mitigation . Facilitates decision making on the microscale level and enables continuous exchange of information between the experts and the stakeholders by giving the stakeholders enough information to make their decisions and at the same time retrieving the feed back of the preferable solutions and use it as an input for the spatial planning and further flood management decision making process .
FLORETO targets the public ( especially the population potentially affected by flood ) as well as the experts and decision makers eg local authorities . Development of FLORETO was initiated within the EU funded FLOWS project ( wwwflowsnu ) , that focuses on finding innovative flood management solutions the communities to the climate change , considering both , technical and social aspects . response of adequate for an
The system has a client/server architecture consisting of three well defined and separate processes ( three tier design ) , each running on a different platform , as depicted in figure 1 ( link : http://floretowbtu harburgde from October address : natasamanojlovic@tuhhde ) available contact
2006 ,
Figure 1 : Architecture of FLORETO
The three tier design has many advantages , the chief one being the modularity of such a system , enabling modification and replacement of one tier without affecting the other ones . The tiers discuss in more detail are as follows :
1 ) User interface ( UI ) , which runs on the user's computer with the browser ( the client )
2 ) The functional ( business logic ) module . This is the brain of FLORETO , it matches users inputs representing the design criteria of their properties to the food mitigation measures that protect them in the event of a flood . The selected technically justifiable measures , are processed and evaluated to yield a the cost benefit analysis ( CBA ) . As benefit , the damage reduction in case of the selected combination of measures is defined . Costs include all expenses for applying the selected measure ( material , labour and maintenance costs ) .
3 ) A database management system ( DBMS ) that stores the data required by the middle tier . This tier runs on a second server called the database server . The database used for FloReTo design is an open source database system MySQL .
The database is optimized for different input modules , for users and for the experts ( data as results for polls , interviews etc )
22 Matching Input Parameters to Measures
The heart of the cost benefit analysis ( CBA ) for FLORETO is the matching of the input parameters describing the stakeholders’ property to the measures that must be implemented in order to protect the properties in the event of a flood .
The input parameters consist of vectors of categorical and/or numerical attributes , which constitute the design criteria describing the property and the flood water parameters of the specific location . The input parameters are further represented as X with the complete set of all possible design ∞X , and the cardinality X criteria represented as the number of attributes of being input parameters . For the example of the final Kellinghusen location dataset used for the experiments in this the paper :
X kellinghue
12=sen
.
The class parameter is made up of technically justifiable mitigation measures that are needed to protect the property in the event of a flood . The set Y constitutes a categorical enumeration of all possible measures . Matching from input parameters to measures is realized with a mapping function M defined below : ,
XMy There are many possible matching functions M , the challenge and task of this paper is to find a M which returns the most appropriate flood the design criteria or
∞∈
( 1 )
Yy
∈
)
:
= optimal
(
X
X mitigation measures for the stakeholders . Different alternatives for realizing
M optimal are now explored .
3 . Alternative Solutions for Matching
The mapping of input parameters to the technically is a knowledge justifiable mitigation measures intensive process . Two alternatives to realize this where considered . The first approach is based on eliciting and elucidating the knowledge of flood into rules that are management experts then incorporated into an expert system . The second approach is a data mining approach using computational intelligence models that learn from data to derive the rules or patterns needed for the matching function .
31 Rules Based or Expert System
Expert systems are used to solve knowledge intensive technical problems The first successful deployment of expert systems happened about four decades ago with the development at Stanford of DENDRAL by Lederberg , Feigenbaum , Buchanan et al . ; and MYCIN developed by Shortliffe [ 5 ] . Since then they have been widely and successfully deployed in various domains ranging from medical diagnosis to chemical spectroscopy , to oil exploration and configuration of computer components etc [ 6 ] . of the implementation , each expert system consists of 3 major parts ie the user interface , the knowledge base and the inference engine .
Irrespective complexity the of
The FLORETO framework readily maps to these three components of an expert system . The data layer of FLORETO maps to the physical storage of the rules set or the knowledge base of an expert system . The Business Logic layer of FLORETO maps to the inference engine which matches users input parameters the appropriate measures . The representation used in the knowledge base also belongs to the business logic layer of FLORETO . The equivalent of an expert system ’s user interface in FLORETO is provided by the web client used for interaction with end users to collect input parameters , display the flood mitigation measures and other recommendations from the system . to technical problems ,
Despite this obviously appropriate and applicable mapping of the matching function of FLORETO to the basic components of an expert system and despite the success of expert systems for solving knowledge intensive some major shortcomings limit the realization of FLORETO ’s matching function as an expert system .
311 The Knowledge Acquisition Bottleneck . This is a well studied and well documented bottleneck of expert systems . the knowledge elicitation problem and the knowledge representation problem [ 7 ] , [ 8 ] , [ 9 ] . The Knowledge Elicitation Problem : To build the knowledge base for expert systems , first genuine experts need to be identified and then they must be
It consists of
Problem : laborious and time consuming , the machine . The to be adapted or available and willing to share their expertise with the knowledge engineer . This is not always possible . When they are willing they are often unable to easily and accurately and explicitly express their implicit knowledge as rules for a machine . Sometimes experts have conflicting opinions about aspects of their knowledge . All of these define the challenge of eliciting knowledge from experts . The process is often very for example MYCIN with only 400 rules required 100 man hours for its development . The Knowledge Representation knowledge elicited from the experts has to be translated into a form that can be stored in and understood by transcribed knowledge must then be validated for inconsistencies . This process is inherently uncertain due to leak of knowledge in the transcription from experts to machine form . There is no guarantee that the knowledge elicited is exactly the same as that transferred into the system . Many expert systems store knowledge as production rules using predicate logic , but experts do not always think in rigid crisp logic . So to minimize knowledge leak the style of the expert needs to the knowledge representation method knowledge representation must be selected to accommodate the expert ’s style of eliciting knowledge . Different methods used for knowledge representation have included rules in crisp and fuzzy logic , frames , semantic networks , belief networks , decision trees , exemplars and objects .
312 The Robust learning limitation Bottleneck . An expert system is only as good as its knowledge base . It has no way of learning new concepts by itself . In order to update its knowledge base , the same experts , or where they are not available new experts , must be approached ; the latest knowledge of the experts must be elicited again and transcribed into the machine . Every time confronted with the knowledge elicitation bottleneck . This is a considerable disadvantage in a domain where the initial knowledge is incomplete or the knowledge turnover is high . The novelty of the FLORETO platform addressing problems in the infant but rapidly developing IFM domain implies that the knowledge turnover will be rather high . Finding the right experts , with knowledge on the design criteria for properties in a particular area and these properties , is not guaranteed for every adaptation of FLORETO to that particular area . Hence the expert system or rule based approach is not the right one for the right measures to protect the
FLORETO , considering that also in the absence of experts it might still be possible to collect historical data of flood events .
32 Data Mining and Computational Intelligence Models Based System
The disadvantages , described above , of using an expert system for the FLORETO platform can be mitigated by using computational intelligence ( CI ) models in a data mining process to construct the optimal mapping function for matching the input parameters describing the design criteria of the properties to the damage mitigating measures in the event of a flood . This approach incorporates the knowledge of experts in the constructed CI model but overcomes the knowledge acquisition bottleneck by supplementing any available expert knowledge with more objective knowledge extracted from databases . This process of knowledge discovery from databases [ 10 ] with the use of CI models also overcomes the robust learning problem , because the models are able to autonomously learn new concepts and patterns when presented with new datasets containing the design criteria of the properties and measures that were used in previous flood events .
The learning theory [ 11 ] presupposes that by training a CI model with a representative sample of the input space
X representa tive
∞⊂ X
, so that it becomes the
M optimal for function ( 1 ) , the model will unseen generalize to unseen members of the input space X optimally at predicting appropriate measures : and still perform representa
X
X
=
− tive
∞
MMif
= optimal for
XMy
=
( representa tive
)
( 2 ) unseen optimal
(
)
=
= for
MM
XMy then This assumption in expression ( 2 ) is at the heart of building intelligent models with training data in the data mining process and later validating them with test sets which were previously unseen by the models . The $performance of the trained models on the test set is then extrapolated to real life scenarios which are also input parameters that were previously unseen by the models .
A CI model encompasses algorithms and learning methods from the fields of artificial neural networks ( ANN ) , computing , evolutionary computing and machine learning [ 12 ] . fuzzy logic
Scores of CI models have been developed and figure 2 lists some of these , especially ANN and machine learning models which are more widely used for such mapping as in equation ( 1 ) . Deciding on which model to use is a major technical challenge ( ie the model selection problem ) which is still being studied in the machine learning and the computational intelligence community . One approach is to pre select different models based on the experience and expertise at hand and to train each of these models with the training dataset . All trained pre selected models are then applied on the validation dataset and the model with the most satisfactory performance here is chosen for deployment in the real life application scenario . The performance measure could be the ability to correctly predict the class or in this particular IFM domain , the measures of the validation set . Other constraints which can influence the model selection are the clarity or comprehensibility of the results from the models , the compactness of the knowledge representation in the models etc .
4 . Data Mining Process the
The data mining process involves many steps [ 13 ] such as understanding the domain and specification of the problem , preparation of data , application of computational intelligence models on the data , and analysis of the specification of the matching problem have already been described above , now we proceed with the process of acquiring the data .
41 Data Acquisition results . The domain and
Data acquisition represents one of the most delicate tasks of decision making in microscale flood management . Required data , that are referred to as design criteria , is usually not available beforehand and has to be acquired either by on site acquisition done by the experts or by interviewing the affected citizens . Acquiring data in such a manner requires considerable effort and soft skills to overcome any subjective perception of flood data . The main problems of this data acquisition are : 1 . difficulty to reach people some people reject providing information about their own properties reliability of acquired data a ) data obtained by the citizens usually do not represent the actual state of their properties . The common reasons are : lack of knowledge on technical reactions when reporting on flooding to their own belongings or suspiciousness to talk about their properties with the third party . issues , emotional
2 . b ) as there is no standardized procedure of data collection , each expert has his own way of assessing the design criteria which sometimes leads to different results . At the moment , there is an initiative EU wide to develop standards for flood data collection . legal aspects the way data is collected must not infringe protection of data privacy .
3 .
Figure 3 Flooding of Kellinghusen , February ,
2002
Data from the German city of Kellinghusen has been collected and used for the case study in this paper wherein data mining approach was used to develop a tool for flood mitigation strategies on the micro scale level .
Kellinghusen is located in the state of SchleswigHolstein , Germany ( population 8.024 30 . June 2005 ) . Due to its proximity to the river Stör and the given hydraulic and hydrologic conditions , the urban fabric of the city has always been prone to flooding . Recent flood events in 1995 , 1997 , and especially in February 2002 , showed the vulnerability of the existing flood defense strategies .
The flood event of 2002 , when the river Stör overtopped its banks reaching the water level of 3,66 m above the sea level , statistically corresponds to the event that occurs once in 30 years ( HQ30 ) . It severely affected more than 100 properties , mostly in the old city quarters . The impact of river flooding was followed by through unprotected basements , resulting in further increase in damage . After the flood event , a thorough study was performed by the Department of River and Coastal Engineering , TUHH , with the aim to assess the structure of the urban fabric and direct damage to it . the groundwater seepage
This dataset is referred to as the initial dataset of Kellinghusen . The analysis of data mining test results is given in table 5 . It is limited only to the properties affected by the HQ 30 that does not fully cover the impact in case of any higher flood event ( eg HQ 100 ) . Considering the area ( ie the properties ) that would be affected in case of HQ100 is an important planning step for development of an appropriate flood mitigation strategy on microscale level .
Table 3 :
Description of the Final Kellinghusen Dataset
Nr .
Description
Type
1
2
3 4
5 6 7 8
9
Basement wall External Base
Categorical
Basement wall Internal Base
Categorical
Basement wall Internal Coating Basement wall Internal WallCoverings
Basement Floor Floor base Basement Floor Covering Basement doors Basement Inventory MovableAssets Services Sewerage System inBasement
Categorical Categorical
Categorical Categorical Categorical Categorical
Binary
10 Services Electrical Appliances
Categorical
Location of the main fuse 11 Services Heating System 12 Flood Water Parameters Flood
Categorical Numeric
Depth
13 CLASS_ATTRIBUTE Measures Categorical
Therefore , within the case study presented in this paper , the initial dataset was extended to buildings potentially affected by flooding with the objective to acquire enough data to generalize matching of design criteria to the technically justifiable solutions for flood mitigation . Further , this matching function is to be implemented in the FLORETO Platform .
For the scope of the study presented in this paper , only residential area ( without industry and public infrastructure ) was considered and design criteria relevant for decision making ( matching ) in the case of Kellinghusen , are selected ( table 4 ) . into integrated
Based on the hydraulic and hydrologic analysis , flood water would mostly affect the lower parts of the buildings . In a few cases , the ground floor is affected . In those cases , the method of shielding is considered and the CLASS_ATTRIBUTEMeasures ( table 2 ) . As depicted in table 3 , the basement is described with detailed description of walls , floors and ceiling , which implicitly takes into account the pathways of flooding and stability issue . Services and fittings , building openings , as well as the heating system are considered for refining the mitigation strategies .
42 Experiment with Models
A description of the experiments performed with intelligent models to solve the matching problems follows first with a description of the datasets , the computational intelligence models used , the test methodology and the results .
421 The Dataset . Two Kellinghusen datasets were collected for the experiments .
The original Kellinghusen dataset
( HQ30 ) consisted of 102 examples each with 15 attributes describing the parameters of the estate and one class attribute representing a label for the measures needed to protect the estate in the event of a flood . All but the 15th non class attribute were categorical attributes . The initial results ( discussed in section 4.3 ) from this dataset was unpromising , hence new and more refined data had to be collected – see section ( 41 )
The final Kellinghusen dataset consisted of 210 examples with 12 non classes and 1 class attribute with 23 distinct values . The description of the final Kellinghusen dataset is presented in table 4 above .
422 The Models . Four models were used to learn what underlying patterns exist within the datasets to predict the appropriate flood mitigating measures for the specific property defined by the design criteria in the non class attributes . The methods used are : a multilayer perceptron neural network with back propagation learning [ 14 ] [ 15 ] , the nested generalized exemplar method [ 16 ] [ 17 ] [ 18 ] [ 19 ] , Bayesian or belief networks [ 20 ] [ 21 ] , and an implementation of the C4.5 decision tree [ 22 ] [ 23 ] [ 24 ] . Multilayer Perceptron with Back Propagation Learning ( MLP ) : Artificial neural networks mimic the information processing capabilities at the center of intelligence in humans and other animals – the nervous system . The learning in the interconnected neurons ( or perceptrons ) is stored in the synaptic weights between these neurons . It is proven that a MLP can approximate any function by converging its weights using as back propagation learning . Also a MLP provides robust learning even in a noisy domain . Nested Generalized Exemplars Method ( NGE ) : It belongs to the nearest neighbor class of instance based learning method that classifies new examples by using a distance measure to find the similarity between data points in memory . The new example to be classified is assigned the class of the nearest example previously stored in memory . In the case of NGE we not only apply the class of the nearest such methodologies exemplar in memory but also create generalizations as axis parallel hyper rectangles . These generalized exemplars are then used for the classification . Due to these generalizations The NGE method is attractive where a more compact representation is needed in comparison to the widely used k nearest neighbour ( widely used instance based learning method ) . Also the dimensions of the hyper rectangle can be map to logic rules thereby improving the comprehensibility of the NGE method . Bayesian or Belief Networks ( BayesNet ) : Bayesian networks or Bayesian belief networks are based on the Bayesian learning theory , which uses the well known Bayes theorem ( 3 ) to assign input parameters to the class ( or measure ) with the maximum posterior probability .
Pr
XYob
(
|
Pr
)
=
YXob
(
| Pr
Pr* ) ( )
Xob
)( Yob
( 3 ) its ancestors tables different
The probability distribution is constructed from the dataset using the Bayesian network structure , which is a network of nodes that constitute a directed acyclic graph . Each node , given its parents , is assumed to be independent of ie conditionally independent . Additionally each node contains conditional probability tables . Because of the large numbers of possible network structures and conditional probability search methods have been developed for construction an optimal network from the dataset . The default K2 hill climbing search algorithms in WEKA were used for this experiment .
Decision Tree ( J48 ) : J48 is Weka ’s implementation of Ross Quinlan ’s C4.5 algorithm which is an improvement on tree algorithm . A decision tree uses a graphical tree to represent knowledge . All leaves ( end nodes ) of the tree correspond to classes ( mitigation measures in this experiment ) whereas other nodes correspond to values of the non class attributes . A path from root to leaf corresponds to a classification rule . At every node , in a decision tree , a test is performed to find the leading attribute based on possible information gain . The node is then split according to the values of the leading attribute . This process of node splitting continues until stopping criterions , well defined for the specific implementation of the decision tree , are met . C4.5 ’s improvement to ID3 includes the abilities to deal with numeric attributes , handle missing values , leading attribute the basic decision improved
ID3 the selection method and tree compactness due to post pruning .
423 The Methodology . The models used and the experiments were conducted using WEKA [ 25 ] , an open platform for machine learning . Two approaches for training and validation were used with each of the methods on both datasets – the split and cross validation methods of validation . SPLIT : Here the dataset is split into a training set and a test set . After training the test set is used to validate the model by comparing the output from the data to the known class of the test set example . The accuracy of such comparison the predictive accuracy of the model K Fold Cross Validation : The dataset is split into k approximately equal parts . K 1 parts of these are used for training and the final part is used for testing . This is repeated k times until each of the k sets has been used for testing exactly once . The predictive accuracy of the model is taken as the mean from all k tests .
43 Analyses of Results is
The results of our experiments are shown in tables
4 and 5 below .
The results from the initial Kellinghusen dataset showed there weren’t enough samples in the dataset for good generalization properties . One reason for this unsatisfactory result could be that enough data important measures was not representative of collected . Another to insignificant attributes . reason could be due
Table 4 :
Results of tests on initial Kellinghusen Dataset
Predictive Accuracy
Model
Split
NGE
MLP BayesNet J48
70 % Average
60 % 45.24 48.39 46.81
42.86 45.16 44.01 42.86 48.39 45.62 40.48 45.16 42.82
10 fold
CV
32.04
34.95 38.83 38.83
Armed with this experience from the initial experiment in addition to utilizing the know how of experts ( see section 4.1 ) , new data was collected with the results in table 6 below .
Table 5 :
Results of tests on Final Kellinghusen Dataset
Model
Predictive Accuracy
60 %
77.38 NGE 72.62 MLP BayesNet 69.05 76.19 J48
Split 70 % Average 71.42 74.4 65.08 68.85 58.73 63.89 77.78 76.98
10 fold
CV 82.86 79.05 74.28 82.86
The final Kellinghusen dataset showed greatly improved generalization properties . Considering the underlying distribution of the class attributes and that there are 23 distinct class attribute values , this result is satisfactory for an initial deployment of the matching function of FLORETO .
5 . Conclusions and Outlook function for to technically justifiable
The matching a data mining the FLORETO Platform , to help stakeholders match parameters of design criteria flood mitigation measures , has been described and realized in process using different computational intelligence models . Different datasets from Kellinghusen were acquired . The CI Models were applied with satisfactory generalization results discovered on the final Kellinghusen dataset . The models with the better performance the decision tree ( J48 ) and the NGE method have the added advantage of using that can be easily transformed into rules that are easier to comprehend by humans . This is not possible with the MLP model for example . The realization of such comprehensible rules is of additional benefit for the stakeholder building function of FLORETO , because the rules serve as justification for the selected measures and this justification helps to improve human interaction with the system and their confidence in the results . representations
The results obtained although satisfactory still leave room for improvement . This can be realized by collecting more data , refining the design criteria ( attributes ) which is achieved by identifying and eliminating irrelevant attributes . Also measures can be refined to be more specific . this work will
With improvement of the Kellinghusen results , further development of involve adapting the business logic of FLORETO to cover other regions by similarly collecting data and mining it considering to adjust the design criteria and the measures to the new locations . for
6 . References
[ 1 ] E . Pasche , D . Kraus , and N . Manojlovic , “ A WebBased Strategy Integrated Flood Management ” , Proceedings , Hydroinformatic Conference 2006 , Nice ( in preparation ) [ 2 ] E . Pasche , TR Geissler , “ New Strategies of Damage Reduction in Urban Areas Prone to Flood ” , Proceedings in : Urban Flood Management A . Szöllösi Nagy , C . Zevenbergen , 2004 . [ 3 ] FEMA , Engineering Principles and Practices of Retrofitting Flood Prone residential Structures , 2005 . [ 4 ] Environment Agency , Flood products A guide for homeowners , 2004 . [ 5 ] BG Buchanan and EH Shortliffe , Rule Based Expert Systems , Addison Wesley . 1984 . [ 6 ] F . Hayes Roth , D . A . Waterman , and D . B . Lenat , Building Expert Systems , Addison Wesley , Reading , MA , 1983 . [ 7]M . Welbank . , A Review of Knowledge Acquisition Techniques for Expert Systems , British Telecom , Research Laboratories Technical Report , Ipswich England 1983 . [ 8 ] EA Feigenbaum , The Art of Artificial Intelligence : Themes and Case Studies Knowledge Engineering , In Intern . Joint Conf . of Artificial Intellig . , pages 1014 1029 , 1977 . [ 9 ] E . Santos Jr . and D . O . Banks , Acquiring Consistent Knowledge , Technical Report AFIT/EN/TR96 01 , Air Force Institute of Technology , January 1996 . [ 10 ] G . Piatetsky Shapiro , Workshop on Knowledge Discovery in Real Databases , In International Conference of Artificial Intelligence , 1989 . [ 11 ] H J . Zimmermann , G . Tselentis , M . Van Someren , G . Dounias ( Eds. ) , Advances in Computational Intelligence and Learning : Methods and Applications , Kluwer Academic Publishers , 2002 . [ 12 ] BC Craenen and AE Eiben , Computational Intelligence , Vrije Universiteit Amsterdam , Faculty of Exact Sciences , 2002 .
[ 13 ] P . Cabena , P . Hadjinian , R . Stadler , J . Verhees , and A . Zanasi , Discovering Data Mining : From Concepts to Implementation , Perentice Hall , 1998 . [ 14 ] F . Rosenblatt , “ The Perceptron : A Probabilistic Model for Information Storage and Organization in The Brain “ , Psychological Review , 65:386 408 , 1958 . [ 15 ] D . E . Rumelhart , G . E . Hinton and R . J Williams , “ Learning Representations by Back Propagation Errors ” , Nature , 323:533—536 , 1986 . TM Cover and PE Hart . Nearest Neighbor Pattern Classification . IEEE Transactions on Information Theory , 13(1 ) , 21—27 , 1967 . [ 16 ] DW Aha , D . Kibler , and MK Albert . InstanceBased Learning Algorithms . Machine Learning , 6(1 ) : 3766 , 1991 . [ 17 ] DW Aha and D . Kibler , Learning Representative Exemplars of Concepts ; An Initial Study . In Proceedings of the 4th International Workshop on Machine Learning , pages 24 29 , CA , June 1987 . [ 18 ] S . Cost and S . Salzberg , “ A Weighted Nearest Neighbour Algorithm for Learning with Symbolic Features ” , Machine Learning , 10:57 78 , 1993 . [ 19 ] S . Salzberg , Learning with Nested Generalized Examplars , Kluwer Academic Publishers , 1990 . [ 20 ] W . Buntime , „Learning Classification Rules using Bayes “ In Proceedings of the Sixth International Workshop on Machine Learning : pages 94 98 , Ithaca NY , 1989 . [ 21 ] Finn V . Jensen , FV V . Jensen and F . V . Jensen , „Introduction to Bayesian Networks “ , Springer Verlag Inc . , New York , 1996 . [ 22 ] R . Quinlan , "Induction of decision trees" , Machine Learning , 1986,Vol.1 No.1 , pp . 81 106 . [ 23 ] JR Quinlan . Decision Trees and Decision making . IEEE Transactions on Systems , Man and Cybernetics , 20(2 ) : 339 346 , March/April 1990 . [ 24 ] R . Quinlan , C4.5 : Programs for Machine Learning , Morgan Kaufmann Publishers , San Mateo , CA , 1993 . [ 25 ] Ian H . Witten and Eibe Frank "Data Mining : Practical machine learning tools and techniques" , 2nd Edition , Morgan 2005 .
Kaufmann ,
San
Francisco ,
