Turning Clusters into Patterns : Rectangle based Discriminative Data Description
Byron J . Gao
Martin Ester
School of Computing Science , Simon Fraser University , Canada bgao@cssfuca ester@cssfuca
Abstract
The ultimate goal of data mining is to extract knowledge from massive data . Knowledge is ideally represented as human comprehensible patterns from which end users can gain intuitions and insights . Yet not all data mining methods produce such readily understandable knowledge , eg , most clustering algorithms output sets of points as clusters . In this paper , we perform a systematic study of cluster description that generates interpretable patterns from clusters . We introduce and analyze novel description formats leading to more expressive power , motivate and define novel description problems specifying different trade offs between interpretability and accuracy . We also present effective heuristic algorithms together with their empirical evaluations .
1 . Introduction
The ultimate goal of data mining is to discover useful knowledge , ideally represented as human comprehensible patterns , in large databases . Clustering is one of the major data mining tasks , grouping objects together into clusters that exhibit internal cohesion and external isolation . Unfortunately , most clustering methods simply represent clusters as sets of points and do not generalize them into patterns that provide interpretability , intuitions , and insights .
So far , the database and data mining literature lacks systematic study of cluster description that transforms clusters into human understandable patterns . For numerical data , hyper rectangles generalize multi dimensional points , and a standard approach in database systems is to describe a set of points with a set of isothetic hyper rectangles [ 1 , 16 , 18 ] . Due to the property of being axis parallel , such rectangles can be specified in an intuitive manner ; eg , “ 3.80 ≤ GPA ≤ 4.33 and 0.1 ≤ visual acuity ≤ 0.5 and 0 ≤ minutes in gym per week ≤ 30 ” intuitively describes a group of “ nerds ” .
Patterns are models with generalization capacity , as well as templates that can be used to make or to generate things . The rectangle based expressions are interpretable models ; as another practical application , they can also be used as search conditions in SELECT query statements to retrieve ( generate ) cluster contents , supporting query based iterative mining [ 13 ] and interactive exploration of clusters .
−
±
±
−
To be understandable , cluster descriptions should appear short in length and simple in format . Sum of Rectangles ( SOR ) , simply taking the union of a set of rectangles , has been the canonical format for cluster descriptions in the database literature . However , this relatively restricted format may produce unnecessarily lengthy descriptions . We introduce two novel description formats , leading to more expressive power yet still simple enough to be intuitively format describes a cluster as understandable . The SOR the difference of its bounding box and a SOR description forof the non cluster points within the box . The kSOR mat allows describing different parts of a cluster separately , descriptions . We prove that using either SOR or SOR based description language is equivalently exthe kSOR pressive to the ( most general ) propositional language [ 18 ] . Meanwhile , cluster descriptions should cover cluster contents accurately , which conflicts with the goal of minimizing description length . The Pareto front for the bicriteria problem of optimizing description accuracy and length , as illustrated in Figure 3 , offers the best trade offs between accuracy and interpretability for a given format . To solve the bicriteria problem , we introduce the novel Maximum Description Accuracy ( MDA ) problem with the objective of maximizing description accuracy at a given description length . The optimal solutions to the MDA problems with different length specifications up to a maximal length constitute the Pareto front . The maximal length to specify ( 20 in Figure 3 ) is determined by the optimal solution to the Minimum Description Length ( MDL ) problem , which aims at finding some shortest perfectly accurate description that covers a cluster completely and exclusively . Previous research only considered the MDL problem ; however , perfectly accurate descriptions can become very lengthy and hard to interpret for arbitrary shape clusters . The MDA problem allows trading accuracy for interpretability so that users can zoom in and out to view the clusters .
The description problems are NP hard . We present heuristic algorithms Learn2Cover for the MDL problem to approximate the maximal length , and starting from which DesTree for the MDA problems to iteratively build the socalled description trees approximating the Pareto front . The , can resulting descriptions , in the format of SOR or SOR descriptions with at be transformed into shorter kSOR least the same accuracy by FindClans , taking advantage of the exceeding expressive power of kSOR descriptions .
−
±
±
±
− and kSOR
Contributions . ( 1 ) Introduction and analysis of novel de , providing enhanced scription formats , SOR expressive power . ( 2 ) Definition and investigation of a novel description problem , MDA , allowing trading accuracy for interpretability . ( 3 ) Presentation and evaluation of effective description heuristics , Learn2Cover , DesTree and FindClans , approximating the Pareto front .
Related work . [ 1 ] studies grid data and defines a cluster as a set of connected dense cells . Their proposed Greedy Growth heuristic constructs an exact covering of a cluster with maximal isothetic rectangles . In the heuristic , a yetuncovered dense cell is arbitrarily chosen to grow as much as possible along an arbitrarily chosen dimension and continue with other dimensions until a hyper rectangle is obtained . A greedy approach is then used to remove redundancy from the set of obtained rectangles . This special case of cluster description is related to the problem of covering rectilinear polygons with axis parallel rectangles [ 11 ] , which is NP complete [ 17 ] , no polynomial time approximation scheme [ 3 ] , and usually studied in 2 dimensional space in the computational geometry community ( eg , [ 15] ) .
[ 16 ] also studies grid data but generalizes the description problem studied in [ 1 ] by allowing covering some “ don’t care ” cells to reduce the cardinality of the set of rectangles . Their proposed Algorithm BP bases on Greedy Growth to generate the initial set of rectangles , then performs greedy pairwise merges of rectangles without covering undesired cells and with limited “ don’t care ” cells for use .
Greedy Growth and BP explicitly work on cluster description . However , despite the grid data limitation , they address the MDL problem solely while our focus is on the more useful and practical MDA problem . In addition , they only use the SOR format while we study and apply novel formats with more expressive power .
Similar to [ 1 ] and [ 16 ] , [ 18 ] is motivated by database applications too but with a focus on the theoretical formulation and analysis of concise descriptions . [ 18 ] also formally defines the general MDL problem for given language ( LMDL ) and proves its NP completeness . As the initial work of this study , [ 9 ] discusses cluster description formats , problems and algorithms at the introductory level . [ 10 ] extends the description problem to the classification problem .
Axis parallel decision trees [ 5 ] can be related to cluster description technically as they provide feasible solutions to the MDL and MDA problems even if with different ob jectives . Consider a closed rectangular instance space , the leaf nodes of a decision tree correspond to a set of isothetic rectangles forming a partition of the training data ( and the instance space ) . Stipulating rectangles to be disjoint , decision tree methods can be considered addressing a partitioning problem ( with the additional constraint of partitioning the instance space ) while cluster description is essentially a covering problem allowing overlapping . Partitioning problems are “ easier ” than covering problems in the sense that they have a smaller search space . Algorithms for a partitioning problem usually work too for the associated covering problem but typically generate larger covers . In decision tree induction , the preference for shorter trees coincides with the preference for shorter description length in the MDL problem . As in the MDA problem , decision tree pruning allows trading accuracy ( on training data ) for shorter trees , and the technique can be applied to generate feasible solutions for the MDA problems .
Indirectly related work in the theory community exists . [ 6 ] studies the red blue set cover problem . Given a set of red and blue elements and a family which is a subset of the power set of the element set , find a subfamily of given cardinality that covers all the blue elements and the minimum number of red elements . [ 7 ] studies the maximum box problem . Given two finite sets of points , find a hyperrectangle that covers the maximum number of points from one designated set and none from the other . Both problems are NP hard and related to the MDA problem ( with precision at fixed recall of 1 and recall at fixed precision of 1 as the accuracy measures respectively , see §3.1 ) , except that the former is given an alphabet ( family ) and restricted to the SOR format , and the latter is limited to use one rectangle . The above discussed research more or less roots in the classical minimum set cover and maximum coverage problems . The former attempts to select as few as possible subsets from a given family such that each element in any subset of the family is covered ; the latter , a close relative , attempts to select k subsets from the family such that their union has the maximum cardinality . The simple greedy algorithm , iteratively picking the subset that covers the maximum number of uncovered elements , approximates the two NP hard problems within ( 1+ln n ) [ 14 ] and ( 1− 1 e ) [ 12 ] respectively . The ratios are optimal unless NP is constrained in quasi polynomial time [ 8 ] . The minimum set cover and maximum coverage problems are related to the MDL and MDA ( with recall at fixed precision of 1 as the accuracy measure ) problems respectively except that they are given an alphabet ( family ) and restricted to the SOR format .
Organization of the paper . In Section 2 we introduce and analyze the description formats . In Section 3 we formalize the description problems . We present heuristic algorithms in Section 4 , report empirical evaluations in Section 5 , and conclude the paper in Section 6 .
2 . Alphabets , formats , and languages
In this section , we study alphabets , formats , and languages for cluster descriptions in depth so as to gain insights into the description problems with different given formats .
21 Preliminaries
Given a finite set of multi dimensional points U as the universe , and a set of isothetic hyper rectangles Σ as the alphabet , each of which is a symbol and subset of U containing points covered by the corresponding rectangle . A description format F allows certain Boolean set expressions over the alphabet . All such expressions constitute the description language L with each expression E ∈ L being a possible description for a given subset C ⊆ U . The vocabulary for E , VE , is the set of symbols in E . We summarize the notations used and to be used for easy lookup . D : data space ; D = D1 × D2 × × Dd U : data set ; U ⊆ D R : rectangle or the set of points it covers ; R ⊆ U Σ : alphabet ; a set of symbols with each as a rectangle VE : vocabulary of expression E ; set of symbols used in E ||E|| : length of expression E ; ||E|| = |VE| Bu : bounding box for u ; u is a set of points or rectangles C : set of points in a given cluster ; C ⊆ U − = BC − C − : set of points in BC but not in C ; C C EF,Σ : expression in format F with vocabulary in Σ LF,Σ : language comprising all EF,Σ expressions
Note that R ( E ) is overloaded to denote a rectangle ( expression ) or the set of points it covers ( describes ) . The distinction should be made clear by the context . Σ is often left unspecified in EF,Σ and LF,Σ when assuming some default alphabet ( to be discussed shortly ) . “ + ” , “ · ” , “ – ” and “ ¬ ” are used to denote Boolean set operators union , intersection , difference and complement .
Two descriptions E1 and E2 are equivalent , denoted by E1 = E2 , if they cover the same set of points . Logical equivalence implies equivalence but not vice versa . ||E|| indicates the interpretability of E for a given format . There are two simple ways of defining ||E|| , absolute length and relative length . Absolute length is the total number of occurrences of symbols in E ; relative length is the cardinality of VE . Neither alone captures the interpretability of E perfectly . The former overestimates the repeated symbols ; the latter underestimates the repeated symbols . The two converge if the repeated symbols are few , which we expect to be the case for cluster descriptions . We define ||E|| to be the relative length of E for the ease of analysis .
Description accuracy is another important measure for the goodness of E , which we will discuss in more details in section 3 . For the use of this section , we conservatively define the “ more accurate than ” relationship for descriptions .
−
.
− ⊆ E2·C
−| ≤ |E2 · C
A description for C is more accurate than the other if it covers more points from C and less points from C Definition 2.1 ( more accurate than ) Given E1 and E2 as descriptions for a cluster C , we say E1 is more accurate than E2 , denoted by E1 ≥accu E2 , if |E1 · C| ≥ |E2 · C| and |E1 · C
−| . − ) ⇒ E1 ≥accu E2 . ( E1·C ⊇ E2·C)∧(E1·C A description problem , viewed as searching , is to search good descriptions that optimize some objective function in a given description language . A more general description language implies more expressive power . Language L1 is more general than language L2 if L1 ⊇ L2 . To characterize expressive power more precisely , we define the “ more expressive than ” relationship for languages . L1 is more expressive than L2 if for any description of cluster C in L2 , there is a shorter and more accurate description in L1 . Definition 2.2 ( more expressive than ) Given two description languages L1 , L2 and a cluster C , we say L1 is more expressive than L2 , denoted by L1 ≥exp L2 , if for any description E2 ∈ L2 , there exists some description E1 ∈ L1 with ||E1|| ≤ ||E2|| and E1 ≥accu E2 with respect to C . Also , L1 =exp L2 if ( L1 ≥exp L2 ) ∧ ( L2 ≥exp L1 ) . A more expressive language is guaranteed to contain “ better ” expressions with respect to length and accuracy . Certainly , L1 ⊇ L2 ⇒ L1 ≥exp L2 . Yet to restrict the search space , we do not want languages to be unnecessarily general . This concern carries on through the following discussions on alphabets and formats , by which languages are specified .
22 Description alphabets
Unlike the set cover problem , alphabet Σ is not explicitly given in cluster description . Assuming given Σ , a description problem , MDA or MDL , can be considered searching through a given language L for the optimal expression . We call such problems L problems ; in particular , L MDA or LMDL . The L problems , to be detailed shortly , are variants of the set cover problem . We brief alphabets mainly for the purpose of gaining insights into the description problems by relating their L problems to the classical set cover problem . Σ is potentially an infinite set since for a set of points , there are an infinite number of covering rectangles with each being a candidate symbol . A simple finite alphabet can be defined as the set of bounding boxes for the subsets of BC , ie , Σmost = {BS | S ⊆ BC} . The alphabet is finite since there is a unique bounding box for a set of points . Σmost is the most general alphabet relevant to the task of describing C ; however , it can be unnecessarily general for some L problem with a mismatched format . It is desirable to have some most specific sufficiently general alphabets .
Let f(L p ) denote the feasible region of an L problem Lp ; certainly , f(L p ) ⊆ L . We say Σ is sufficiently general for LF,Σ p if LF,Σ ≥exp f(LF,Σmost p ) , which roughly means Σ does not lose to Σmost if used in the L p case . On top of being sufficiently general , Σ is most specific if removing any element from it , Σ would not be sufficiently general anymore . In the following , we define some alphabets with this desireable property .
Σpure = {BS | BS · C pure = {BS | BS · C = ∅ ∧ S ⊆ C − Σ Σmix = {BS | S ⊆ C ∧ S )= ∅} mix = {BS | S ⊆ C − ∧ S )= ∅} − Σ
− = ∅ ∧ S ⊆ C ∧ S )= ∅} − ∧ S )= ∅}
Σpure ( Σ −
− pure ) contains pure rectangles covering points − from C ( C mix ) , however , can ) only . Symbols in Σmix ( Σ − to co exist . Apbe mixed allowing points from C and C parently , Σpure ⊆ Σmix ⊆ Σmost and Σ pure ⊆ Σ mix ⊆ − − Σmost . Multiple subsets may match to the same bounding box , eg , B13 = B123 in Figure 1 , where B13 is short for B{1,3} and so on . The figure illustrates Σpure and Σmix .
Examples of some L problems with matching alphapure MDL , bet and format are LSOR,Σpure MDL , LSOR−,Σ− MDA . In addition , we LSOR,Σmix MDA and LSOR−,Σ− − define Σk = Σmix + Σ pure , then LkSOR±,Σk MDL and LkSOR±,Σk MDA are also such examples . Due to the page limit , we omit further explanations . mix
Such a desirable Σ is assumed given by default if it is left unspecified in EF,Σ or LF,Σ . Although these default alphabets are made specific , they can still be prohibitively large ( eg , O(|2C| ) and not scalable to real problems . Therefore , it is practically infeasible to generate Σ and apply existing set cover approximations on description problems .
23 Description formats and languages
We require descriptions to be interpretable . For descriptions to be interpretable , the description format has to have a simple and clean structure . Sum of Rectangles ( SOR ) , denoting the union of a set of rectangles , serves this purpose well and has been the canonical format for cluster descriptions in the literature ( eg , [ 1 , 16] ) . For better interpretability , we also want descriptions to be as short as possible . To minimize the description length of SOR descriptions has been the common description problem .
Nevertheless , there is a trade off between our preferences for simpler formats and shorter description length . Simple formats such as SOR may restrict the search space too much leading to languages with low expressive power . On the other hand , if a description format allows arbitrary Boolean operations over a given alphabet , we certainly have the most general and expressive language containing the shortest descriptions , but such descriptions are likely hard to
B13 B123
1
B12
4
2
B23
3
C = {1 , 2 , 3} C– = {4}
( cid:166)pure = {B1 , B2 , B3 , B23} ( cid:166)mix = {B1 , B2 , B3 , B12 , B13 , B23}
C2
R1’
C1
SOR : 5 SOR–
: 4 kSOR±
: 3
EkSOR±(C ) = ESOR(C1 ) + ESOR–(C2 ) = BC
2 – R1’ )
1 + ( BC
Figure 1 . Alphabets .
Figure 2 . Formats . comprehend due to their complexity in format despite their succinctness in length . Moreover , not well structured complex formats bring difficulties in manipulation of symbols and design of efficient and effective searching strategies .
−
−
−
−
−
.
±
− and kSOR
Clearly , we require description languages with high expressive power yet in intuitively understandable formats . In the following , we explore several alternative description formats beyond SOR , in particular , SOR
. While SOR takes the form of R1 + R2 + + Rl , a description , in describing C , takes the set difference
SOR between BC and a SOR description for C Definition 2.3 ( SOR SOR sion in the form of BC − ESOR(C a SOR description for C ± description ) Given a cluster C , a description for C , ESOR−(C ) , is a Boolean expres− ) is
In addition , a SOR
. description for C , ESOR±(C ) , is an expression in the form of either ESOR(C ) or ESOR−(C ) . Clearly , LSOR± ⊇ LSOR and LSOR± ≥exp LSOR . In describing a cluster C , SOR and SOR descriptions together nicely cover two situations where C is easier is easier to describe . Different data distrito describe or C butions , which are usually not known in advance , favor different formats . In Figure 2 , consider C2 as a single cluster deto be described with perfect accuracy , certainly SOR scriptions are favored . The shortest ESOR(C2 ) has length 4 whereas the shortest ESOR−(C2 ) has length 2 .
− ) , where ESOR(C
−
SOR descriptions have a structure as simple and clean as SOR descriptions . In addition , the added BC draws a big picture of cluster C and contributes to interpretability in a positive way . The two formats together also allow us to view C from two different angles . Note that the special rectangle BC is required for the format , it is not included in the default alphabets either counted in ||E|| for simplicity . descriptions generally serve well for the purpose of describing compact and distinctive clusters . Nevertheless , arbitrary shape clusters are not uncommon and for such applications , we may want to further increase the expressive power of languages by allowing less restrictive formats . For example , if some parts of cluster C favor SOR and some
SOR
±
−
−
−
−
±
±
±
k
, then SOR descriptions .
Clearly , kSOR i=1 Ci = C . is too restrictive to other parts favor SOR consider different parts separately . Instead , it can only provide a global treatment for C . To overcome this disadvantage , we introduce kSOR ±
Definition 2.4 ( kSOR description ) Given a cluster C , a description for C , EkSOR±(C ) , is a Boolean exkSOR pression in the form of ESOR±(C1 ) + ESOR±(C2 ) + + ESOR±(Ck ) , where ± descriptions generalize SOR descriptions by allowing different parts of C to be described separately ; and the latter one is a special case of the former one with k = 1 . In Figure 2 , C1 favors SOR whereas . The shortest ESOR(C ) and ESOR−(C ) C2 favors SOR is able to provide have length 5 and 4 respectively . kSOR local treatments for C1 and C2 separately and the shortest EkSOR±(C ) has length 3 . In many situations kSOR can be found much more effective than other simpler formats ; but how expressive is LkSOR± precisely ? In the following , we compare it with the propositional language , the most general description language we consider ( as in [ 18] ) .
±
±
−
±
Note that for case 1 and 2 , we have supposed R0 ∈ Σk , which may not hold since Σk is not intersection closed . As a simple counter example , the intersection of two bounding boxes may not be a bounding box . However , we note that j is to describe R0 · C = for the two cases , the purpose of E 0 = BC0 ∈ Σk . As expressions of length 1 C0 ⊆ C , thus R 0· C0 = R0· C0)∧ 0 ≥accu R0 because ( R describing C0 , R 0 · C 0 ⊆ R0 · C − − 0 ) . We replace R0 with R ( R j to get j || = ||E j|| . Then , Ej can j ≥accu E j , then E E ± description with be re written as a more accurate SOR length ≤ ||Ej|| . Wo do this for every disjoint of EDN F , ∈ ± description E then E can be re written as a kSOR || ≤ ||E|| and E LkSOR±,Σk with ||E ≥accu E , which ≥exp LP,Σk . implies LkSOR±,Σk j and ||E
0 in E
Theorem 2.6 does not hold if description length ||E|| is defined as the absolute length , in which case E = ( R1 + = ( R1− R2)− R3 in LP has length 3 but the equivalent E R3 ) + ( R2 − R3 ) in LkSOR± has length 4 . Nevertheless , the general conclusion persists , that is , LkSOR± is a very expressive language close or equal to LP .
Despite its exceptional expressive power , the kSOR
±
Definition 2.5 ( propositional language ) Given Σ as the alphabet , LP,Σ is the propositional language comprising expressions allowing usual set operations of union , intersection and difference over Σ .
¬
=exp LP,Σk ⇒ LP,Σk
Theorem 2.6 LkSOR±,Σk ⊇ LkSOR±,Σk
≥exp LP,Σk .
Proof LP,Σk we only need to prove LkSOR±,Σk
≥exp LkSOR±,Σk ; Consider any E ∈ LP,Σk . Since E can be re written as EDN F in disjunctive normal form with the same set of vocabulary , thus ||E|| = ||EDN F|| and E = EDN F . Each disjunct in EDN F is a conjunction of literals and each literal R where R ∈ Σk . Consider any distakes the form of R or junct Ej in EDN F , since axis parallel rectangles are intersection closed , Ej can be re written as E j , which takes one j = R0 · j = R0 ; ( 2 ) E of the following three forms : ( 1 ) E Rx · ¬ Ry · · ¬ Ry · · ¬ Rx· ¬ ¬ Rz . In all Rz ; ( 3 ) E j|| ≤ ||Ej|| and E the three cases ||E j = Ej . For case 3 , due to the generalized De Morgan ’s law , j = ¬ Rz = ¬(Rx + Ry + + Rz ) = E BC −(Rx + Ry + + Rz ) , which is a SOR description . For case 1 and 2 , we suppose R0 ∈ Σk . Then for case 1 , j is a SOR description . For case 2 , due to the generalized E j = R0 · ¬(Rx + Ry + + Rz ) = De Morgan ’s law , E R0 − ( Rx + Ry + + Rz ) , which is a SOR description . description with length ≤ ||Ej|| . We do this for every disjunct descripof EDN F , then E can be re written as a kSOR = E , tion E which implies LkSOR±,Σk
Then , Ej can be re written as an equivalent SOR
± || ≤ ||E|| and E
∈ LkSOR±,Σk with ||E
≥exp LP,Σk .
Ry · · ¬
Rx · ¬ j = ¬
±
−
− format is very simple and conceptually clear , allowing only format . It is also wellone level of nesting as the SOR structured to ease the design of searching strategies .
−
Assuming some given default alphabet , previous research studied cluster description as searching the shortest expression in LSOR,Σpure , the simplest and least expressive language we discussed in this section . We study the same problem but considering other more expressive languages LSOR± and LkSOR± , and our main focus is on the problem of finding the best trade offs between accuracy and interpretability , as to be introduced in the following section .
3 . Cluster description problems
A description problem is to find a description for a cluster in a given format that optimizes some objective . In this section , we introduce cluster description problems with different objective measures .
31 Objective measures
We want to describe a given cluster C with good interpretability and accuracy . Simple formats and shorter descriptions lead to improved interpretability . We have studied alternative description formats that are intuitively comprehensible . Within a given description format , description length is the proper objective measure for interpretability .
In addition to interpretability , the objective of minimizing description length can also be motivated from a “ data compression ” point of view . There are many situations when we need to retrieve the original cluster records ; eg , to send promotion brochures to a targeted class of customers , to perform statistical analysis , or in a query based iterative mining environment as advocated by [ 13 ] , to resume the mining process from stored temporary or partial results . Cluster descriptions provide a neat and standalone way of “ storing and retrieving ” cluster contents . In DBMS systems , an isothetic rectangle can be specified by a Boolean search condition such as 1 ≤ D1 ≤ 10∧ ∧5 ≤ Dd ≤ 50 . A cluster description is then a compound search condition for the points in the cluster , which can be used in the WHERE clause of a SELECT query statement to retrieve the cluster contents entirely . In this scenario , the cluster description process resembles encoding and the cluster retrieval process resembles decoding . The compression ratio for cluster description E can be roughly defined as |E| / ( ||E|| × 2 ) , as each rectangle takes twice as much space as each point . The goal of large compression ratio leads to the objective of minimizing description length . Meanwhile , shorter length also speeds up the retrieval process by saving condition checking time [ 18 ] .
Accuracy is another important measure for the goodness of cluster descriptions . An accurate description should cover many points in the cluster and few points not in the cluster . To precisely characterize description accuracy , we borrow some notations from the information retrieval community [ 2 ] and define recall and precision for a description E of cluster C . recall = |E · C| / |C| precision = |E · C| / |E|
If we only consider recall , the bounding box BC could make a perfectly accurate description ; if we only consider precision , any single point in C would do the same . The F measure considers both recall and precision and is the harmonic mean of the two . f =
2 × recall × precision recall + precision
A perfectly accurate description with f = 1 has recall = − 1 and precision = 1 . In a perfectly accurate SOR or SOR description , all rectangles are pure in the sense that they contain same class points only .
The F measure does not fit situations where users want to specify constraints on either recall or precision . We introduce two additional measures to provide this flexibility . The first is recall at fixed precision ; often , we want to fix precision at 1 . The second is precision at fixed recall ; often , we want to fix recall at 1 . The measures can be found useful in many situations . If we can afford to lose points , then we can in C much more than to include points in C choose recall at fixed precision of 1 to sacrifice recall and protect precision as in the maximum box problem [ 7 ] ; in the opposite situation , we can choose precision at fixed recall of 1 as in the red blue set cover problem [ 6 ] .
− y c a r u c c A
1
0.8
0.6
0.4
0.2
0 feasible region of MDL
Pareto front approximation to Pareto front feasible region of the bicriteria problem = union of feasible regions of MDA problems
Length
0
5
10
15
20
25
Figure 3 . Accuracy vs . length .
32 MDA problem and MDL problem
Description length and accuracy are two conflicting objective measures that cannot be optimized simultaneously . The Pareto front for the bicriteria problem , as illustrated in Figure 3 , offers the best trade offs between accuracy and interpretability for a given format . To solve the bicriteria problem and obtain the best trade offs , we introduce the novel Maximum Description Accuracy ( MDA ) problem .
Definition 3.1 ( Maximum Description Accuracy problem ) Given a cluster C , a description format F , an integer l , and an accuracy measure , find a Boolean expression E in format F with ||E|| ≤ l such that the accuracy measure is maximized .
The optimal solutions to the MDA problems with different length specifications up to a maximal length constitute the Pareto front . The vertical lines in Figure 3 illustrate the feasible regions of the MDA problems , whose union is the feasible region of the bicriteria problem . The maximal length is the length of some shortest description with perfect accuracy , which is 20 in Figure 3 . It is pointless to specify larger lengths as the best accuracy has been achieved . To determine this maximal length , we define the Minimum Description Length ( MDL ) problem , whose objective is to find some shortest description that covers a given cluster completely and exclusively , ie , with f = 1 .
Definition 3.2 ( Minimum Description Length problem ) Given a cluster C and a description format F , find a Boolean expression E in format F with minimum length such that ( E · C = C ) ∧ ( E · C
− = ∅ ) .
The optimal solution to the MDL problem gives the maximal length to specify in solving the MDA problems . The feasible region of the MDL problem is also illustrated in Figure 3 . Previous research only considered the MDL problem with SOR as the description format . However , in practice , perfectly accurate descriptions can become lengthy and hard to interpret for arbitrary shape clusters . The MDA problem allows trading accuracy for interpretability so that users can zoom in and out to view the clusters . From a “ data
Algorithm : Learn2Cover preprocessing( ) ; // sort BC along Ds foreach ( ox ∈ BC ) { // processed in sorted order if ( ox ∈ C ) cover( , ox , − cover( −
) ; , ox , ) ; } else
Procedure : cover( , ox , − foreach ( R ∈ − ) {
) if ( cost(R , ox ) == 0 ) close R ; } foreach ( R ∈ && R is not closed ) { if ( cost(R , ox ) == 0 ) { extend R to cover ox ; return ; }} foreach ( R ∈ ) { // processed in ascending order of cost if ( no violation against − ) { expand R to cover ox ; return ; }} insert( , Rnew ) ; // ox was not covered . Rnew = ox
1 2 3 4 5 6
1 2 3
4 5 6 7
8 9 10 11
12 compression ” point of view , description requiring perfect accuracy resembles lossless compression while description allowing lower accuracy resembles lossy compression .
Assuming some given default alphabets as previously discussed , the L problems , easier than their counterparts , can be related to some variants of the set cover problem . In particular , LSOR,Σpure MDL corresponds to the minimum set cover problem , which is known to be NP hard . Other LMDL problems are harder in the sense that they have larger search spaces with more general languages .
Given the recall at fixed precision of 1 accuracy measure , the LSOR,Σpure MDA problem corresponds to the maximum coverage problem , which is known to be NP hard . Given the precision at fixed recall of 1 accuracy measure , the LSOR,Σpure MDA problem corresponds to the red blue set cover problem , which is also NP hard [ 6 ] . Given the F measure , the decision version of the LSOR,Σpure MDA problem is reducible to the decision problem of either of the two . Other L MDA problems are harder in the sense that they have larger search spaces .
As we have seen , the cluster description problems , with different format and accuracy measure specifications as discussed , are all NP hard . We present efficient and effective heuristic algorithms for these problems in the next section .
4 . Description algorithms
In this section , we present three heuristic algorithms . Learn2Cover solves the MDL problem approximating the maximal length . Starting with the output of Learn2Cover , DesTree iteratively builds the so called description tree for the MDA problems approximating the Pareto front . FindClans transforms the output descriptions from DesTree into descriptions without reducing the accuracy . shorter kSOR
±
41 Learn2Cover
−
Given a cluster C , Learn2Cover returns a description E format with f = 1 . For for C in either SOR or SOR this purpose , it suffices to learn a set of pure rectangles covering C and a set of pure rectangles − − covering C completely . Learn2Cover is carefully designed such that and − are learned simultaneously in a single run ; besides , the extra learning of − does not come as a cost but rather a boost to the running time .
Sketch of Learn2Cover . To better explain the main ideas , we give the pseudocode for the simplified Learn2Cover and its major procedure cover( ) in the following .
In preprocessing( ) , the bounding box BC is determined ; the points in BC are normalized against BC and sorted along a selected dimension Ds . Ties are broken arbitrarily .
. The two situations are symmetric .
Initially = ∅ and −
− Suppose ox ∈ C , procedure cover( , ox , −
At the moment we suppose there are no mixed ties involving points from different classes . In general , the choice of Ds does not have a significant impact if the data is not abnormally sparse ; thus Ds can be arbitrarily chosen . Nevertheless , Learn2Cover offers an option to choose Ds with the maximum variance , which makes the algorithm more robust against some rare , malicious cases such as large mixed tie groups . Learn2Cover is deterministic once Ds is chosen . = ∅ . Let ox be the next point from BC in the sorted order to be processed . cover( , ox , − ) , ox , ) is called upon depending on ox ∈ C or cover( − or C ) chooses a non closed R ∈ covering no points covered by rectangles in − and with the minimum covering cost with respect to ox to expand and cover ox . Otherwise , a new rectangle Rnew minimally covering ox will be created and added to ( line 12 ) . A rectangle is closed if it cannot be expanded to cover any further point without causing a covering violation , ie , covering points from the other class ( lines 1 , 2 , 3 ) . Violation checking can be expensive ; therefore , we always calculate cost(R , ox ) first . If there is a non closed R with cost(R , ox ) = 0 , we need to extend R only along Ds to cover ox , in which case violation checking is unnecessary ( lines 4 , 5 , 6 , 7 ) . Otherwise rectangles are considered in ascending order of cost(R , ox ) for violation checking . The first qualified rectangle will be used to cover ox ( lines 8 , 9 , 10 , 11 ) . In the following we discuss covering cost and covering violation in more details .
R2
R1
Ds
R3
R2
R1
R5
R3
A ox
B
R4
R6
( a ) Choice of rectangles
( b ) Demonstration run
Figure 4 . Learn2Cover .
Covering cost and choice of rectangles . The behavior of Learn2Cover largely depends on how the covering cost cost(R , ox ) is defined , ie , the cost of R to cover point ox . This cost should estimate the reduction of the potential of R to cover further points after ox . Intuitively , we want to choose R with the minimum increased volume , so that rectangles can keep maximal potential for future expansions without incurring covering violations .
Yet there are more issues to be concerned beyond this basic principle . First , when calculating the increased volume for R , we should not consider Ds . Since points are sorted on Ds and processed in the sorted order , the extension of R along Ds is the distance it has to travel to cover further points after ox . To keep R short on Ds does not help to keep its potential for future expansions . In Figure 4(a ) , if we considered Ds , R1 would have the biggest increased volume and not be chosen to cover ox . But this saved space would be part of the expanded R1 in covering any point after ox , and whether R1 had been expanded already to cover ox or not would not make a difference . This suggests that the increased volume of R1 with respect to ox should be 0 , ignoring Ds in the calculation .
Second , if the expanded R has a length of 0 or close to 0 in any dimension , its volume and increased volume will be 0 or close to 0 , which makes R a favorable choice . But R may have traveled far along some other dimensions to cover ox ; its expansion potential would thus be limited . In Figure 4(a ) , both R1 and R3 require the same increased volume of 0 to cover ox since R1 has to be extended only along Ds and R3 has a length of 0 in one of its dimensions . However , R3 has to travel far along some dimensions other than Ds whereas R1 does not . Moreover , R2 does not have the increased volume of 0 , but it seems not to be a worse choice than R3 because ox is more local to R2 than R3 . Therefore , cost needs to fix the illusion of 0 increased volume and take into account the locality of rectangles .
In the following , we propose a definition for cost(R , ox ) with these issues in consideration . Let lj(R ) denote the length of rectangle R along dimension Dj , and R denote the expanded R in covering ox .
. j=1d,j=s lj(R ) vol(R ) = ) − vol(R))1/(d−1 ) aveIncV ol(R , ox ) = ( vol(R ) − lj(R)|2)1/2 dist(R , ox ) = ( cost(R , ox ) = aveIncV ol(R , ox ) + dist(R , ox ) j=1d,j=s |lj(R fi
Ds is ignored if we project R and ox onto the subspace D\Ds . vol(R ) is the volume of the projected R ; aveIncV ol can be viewed as the increased volume averaged on each dimension ; dist is precisely the Euclidean distance from the projected ox to the projected R . cost is the sum of aveIncV ol and dist , ie , we assign equal weights to both components of cost . According to this definition of cost(R , ox ) , the choices in Figure 4(a ) would be R1 , R2 , and R3 in priority order .
Sometimes there are no good rectangles available , in which case forcing greedy expansions may deteriorate the overall performance . Learn2Cover provides an expansion control parameter to limit the maximum distance each dimension can travel at each expansion . Since data is normalized , the default choice of 0.5 means that each expansion cannot exceed half of the span of BC in each dimension . The parameter is user specified but not sensitive . Without expansion control , Learn2Cover works generally well ; but it may help in cases of extremely sparse or malicious datasets . Figure 4(b ) is a real run of Learn2Cover on a toy dataset . respecDark and light points denote points in C and C tively . Rectangles are numbered in ascending order of their creation time . Note that on processing A , a better choice of R3 was made while R4 was also available . R3 had cost of 0 with Ds ignored . If R4 had been chosen to cover A , it would have been closed before covering B .
−
Covering violation and correctness . Learn2Cover is required to output pure rectangles , any covering of inter class points will be considered as a violation .
BP considers all inter class points in violation checking . In Learn2Cover , since points are processed in the sorted order , the only points that could lead to violations in the expansion of Ri ∈ ( − ) are currently processed points in Rj ∈ − ( ) that overlaps with the expanded Ri . Thus and − , the sets of rectangles to be learned , also exist to help each other in violation checking to dramatically reduce the number of points in consideration . A simple auxiliary data structure is maintained to avoid the possible performance deterioration in the presence of extremely dense and big rectangles . We omit the details due to the page limit . Learn2Cover outputs pure rectangle collections and − respectively . We covering every point in C and C examine procedure cover( ) to argue the correctness . From the definition of cost , cost(R , ox ) = 0 if and only if the projected ox is in the projected R . In such case , if R ∈ ( − −(C ) , R will not be able to cover any
) and ox ∈ C
−
− causing a violation , o
If R ∈ ( − further point without covering ox , which causes a violation ) and and R will thus be closed ( line 3 ) . ox ∈ C ( C ) , R can be extended along Ds to cover ox as ( line 6 ) without causing any violation . If there existed must have been processed before o ) = 0 , which would have caused R to ox with cost(R , o be closed . In line 10 , R is expanded to cover ox after violation checking . In line 12 , Rnew is a degenerate rectangle covering only one point ox . Thus , upon completion of Learn2Cover , each ox ∈ BC is covered without violation . In the sketch of Learn2Cover , we have assumed there were no mixed ties involving points from different classes . This case may happen and even frequently on grid data . Mixed tying points may cause covering violation to one another . Learn2Cover identifies mixed tie groups in the preprocessing( ) step , and then some extra checking is performed on processing ox belonging to a mixed tie group .
42 DesTree
The MDA problem is to find a description E with a userspecified length l for cluster C maximizing a given accuracy measure . Our algorithm DesTree takes the output from Learn2Cover , or − whose cardinality approximates the maximal length , iteratively builds a so called description tree approximating the Pareto front .
Description trees are tree structures resembling dendrograms to provide overviews on alternative trade off descriptions of different lengths . Accordingly , DesTree resembles agglomerative hierarchical clustering to iteratively merge child nodes into parent nodes until a single node is left . Each node in a description tree represents a rectangle ; and a normal merge operation produces a parent node that is the bounding box of its child nodes . The tree grows bottom up along a series of merge operations .
−
−
− leading to a SOR
Each horizontal cut in the tree defines a set of rectangles . For the so called C description tree , a cut set constitutes the vocabulary for a SOR description of C ; for the so called description tree , a cut set constitutes the vocabulary for C description a SOR description of C of C . The cardinality of a cut set , the description length , equals to the number of links being cut . Each cut offers an alternative trade off between description length and accuracy . The higher in the tree we cut , the shorter the length and the lower the accuracy . − Consider a SOR ( SOR
) description , merging two rectangles into their bounding box may cause precision ( recall ) to decrease ; removing a rectangle may cause recall ( precision ) to decrease . Both operations trade the accuracy measure , say f , for shorter length and we want to consider both . To integrate the removal operation in building description trees , we add a symbolic node , the empty set ∅ , into the leaf nodes and define the merge operator as follows .
Φ
R6
Φ cut
2 = {R
5 , R
3}
R5
R1
R2
R3
R4
Φ
3 , R
4} cut
5 , R 1 = {R cut 0 = R ( R– ) input : R ( R– )
Figure 5 . Description tree .
Definition 4.1 ( merge ) Ri merge Rj = Rparent = ( 1 ) bounding box for Ri and Rj if Ri )= ∅ and Rj )= ∅ ; ( 2 ) ∅ otherwise
−
DesTree is a greedy approach starting from the input leaf nodes , a set of pure rectangles or − generated by Learn2Cover , building the tree in a bottom up fashion . Pairwise merge operations are performed iteratively , and the merging criterion is the biggest resulting accuracy mea description tree are sure . The C description tree and C built separately in the same fashion . Figure 5 exemplifies a description tree . R1 ∼ R4 are the input rectangles . R1 and R2 are chosen for the first merge to give R5 . The second merge of R4 and ∅ results in the removal of R4 . R6 , the parent node of R5 and R3 , merges with ∅ to give the symbolic root . The lowest cut cut0 is or − description . Take cut2 as an example . For a C description tree , cut2 corresponds to ESOR(C ) = R5 + R3 ; for a C description tree , it corresponds to ESOR−(C ) = BC − ( R5 + R3 ) . Description trees are not necessarily binary . A merge could result in more rectangles fully contained in the parent rectangle . Nonetheless , the merging criterion discourages branchy trees and Figure 5 is a typical example .
. Each cut corresponds to a SOR or SOR
−
−
The merging process can be simplified for some accuracy measures . Given recall at fixed precision of 1 , for the C description tree , only merge operation ( 2 ) ( the removal operation ) needs to be considered , and the root is always ∅ ; description tree , only merge operation ( 1 ) ( the for the C normal merge operation ) needs to be considered , and the root is always BC− . For both cases , precision is guaranteed to be 1 and recall reduces along the merging process .
−
Given precision at fixed recall of 1 , for the C description tree , only merge operation ( 1 ) needs to be considered , and the root is always BC ; for the C description tree , only merge operation ( 2 ) needs to be considered , and the root is always ∅ . For both cases , recall is guaranteed to be 1 and precision reduces along the merging process .
−
We can easily prove the accuracy measure , recall at fixed precision of 1 or precision at fixed recall of 1 , reduces monotonically along the merging process in DesTree . With respect to the F measure , though evident in experiments , it is non trivial to construct a proof of the same property .
43 FindClans
FindClans takes as input a cut ( denoted by T in the following ) from a description tree representing a SOR or description with SOR shorter length and equal or better accuracy . description , outputs a kSOR
−
±
−
The algorithm is based on the concept of clan . Let SORV be a SOR description with vocabulary V , eg , SORV = R1 + R2 for V = {R1 , R2} . Intuitively , a clan N ⊆ T is a group of rectangles that dominate ( densely populate ) a local region , so that by replacing them as a whole , SORN can be rewritten as a shorter and more accurate description for the targeted points in the region . SOR − ) Definition 4.2 ( clan ) Given T as a cut from a C ( C | > 1 and description tree , N ⊆ T is a clan if |N| − |N BN − SORN ≥accu SORN in describing SORN · C ( SORN · C − ) , where BN is the bounding box of N and N a set of rectangles associated with N called the replacement of N . We also refer to |N| − |N
| − 1 as Nscore
−
−
) if T is from a C ( C
Note that the purpose of SORN is to describe SORN · C ( SORN · C ) description tree . N.score is the possible length reduction offered by a single clan N since SORN will be replaced by BN − SORN . Two clans N1 and N2 are disjoint if N1 ∩ N2 = ∅ . For a set of mutually disjoint clans , Clans , the total length reduction is at least
Ni.score where Ni ∈ Clans . fi
Figure 6 uses the example in Figure 2 and illustrates how a clan can help to rewrite a SOR description represented by T into a shorter kSOR description .
±
−
−
±
If T is from a C
description tree , the input SOR
Suppose we have found Clans for T and T is from a C description tree , it is straightforward to rewrite the input SOR description ESOR(C ) = SORT into a kSOR description as illustrated in Figure 6 . For each N ∈ Clans , we simply replace SORN in SORT by the shorter and more accurate ( BN − SORN ) . description is ESOR−(C ) = BC − SORT . For each N ∈ Clans , we replace SORN in SORT by BN and add back SORN . As an example , let ESOR−(C ) = BC − SORT = BC − ( R1 + R2 + R3 + R4 + R5 ) . Suppose we have a clan N = {R2 , R3 , R4 , R5} with replacement N 1} , then EkSOR±(C ) = BC−(R1+BN )+R 1 . The length reduction = T.score = 4−1−1 = ||ESOR−(C)||−||EkSOR±(C)|| = 6 − 4 = 2 . It is easy to verify that after all such replacements , the resulting EkSOR±(C ) is shorter and more accurate than ESOR(C ) or ESOR−(C ) with respect to any accuracy measure we discussed .
= {R
1 2 3 4 5 6
1 2 3 4 5 6
7 8 9 10
R2
R4
R1’
R5
R2’
R1
R3’
R3
R = {R1 , R2 , R3 , R4 , R5} R– = {R1’ , R2’ , R3’}
T = R = {R1 , R2 , R3 , R4 , R5} ESOR(C ) = R1 + R2 + R3 + R4 + R5 N = {R2 , R3 , R4 , R5} ⊆ T N’ = {R1’} BN = bounding box for N N.score = |N| – |N’| – 1 = 4 – 1 – 1 = 2 EkSOR±(C ) = R1 + ( BN − R1’ ) length reduction = ||EkSOR±(C)|| – ||ESOR(C)|| = 2
Figure 6 . Clan helps in length reduction .
−
− ) since N
( SORN ·C covering BN ·C N contains pure rectangles completely ( BN ·C ) . Thus given a candidate clan N , | > 1 . is uniquely determined and N is a clan if |N|−|N Algorithm Findclans , as presented in the following , continues to call procedure findAClan( ) to find a clan N in the updated T and insert it into Clans . findAClan( ) first checks each pair of rectangles in T and finds theN with the highest score . bestN is used to keep track of the best stage of theN , which continues to grow greedily one more R ∈ ( T − theN ) at a time resulting in the largest score increase , until no more rectangles available . bestN is returned if it is a clan ; otherwise , N U LL .
Algorithm : FindClans ( T ) Clans ← ∅ ; N ← f indAClan(T ) ; while ( N ! = N U LL ) { insert(Clans , N ) ; T ← T − N ; N ← f indAClan(T ) ; }
7 return Clans ;
Procedure : findAClan(T ) find theN ; // consider each pair in T bestN ← theN ; while ( theN ⊆ T ) { grow theN ; // consider each R ∈ ( T − theN ) if ( theN.score > bestN.score ) bestN ← theN ; } if ( bestN.score ≥ 1 ) return bestN ; else return N U LL ;
5 . Experimental results
All we need is to find Clans . To simplify the task , , the replacement of N , to contain each recwe define N ( ) overlapping with BN if T is from tangle R ∈ − − ) description tree . Then , it is guaranteed that a C ( C BN − SORN ≥accu SORN in describing SORN · C
We experimentally evaluated and compared our methods against CART ( Salford Systems , Version 5.0 ) and BP [ 16 ] . While decision tree classifiers , as argued in related work , can be applied to the MDA and MDL problems ,
BP addresses the MDL problem only . We implemented Learn2Cover , DesTree , FindClans , and BP . For BP , we also implemented Greedy Growth and a synthetic grid data generator . To make our experiments reproducible , real datasets from the UCI repository [ 4 ] with numerical attributes and without missing values were used , where data records with the same class label were treated as a cluster . Note that , in the broad sense , a cluster can be used to represent an arbitrary class of labeled data that require discriminative generalization . The notion of rectangle can be extended ( but not implemented in this version ) to tolerate categorical attributes ; eg , to use sets instead of intervals . Rectangles do not provide generalization on the categorical attributes .
51 Comparisons with CART
To approximate the Pareto front , our approach starts with applying Learn2Cover for the MDL problem , then DesTree for the MDA problems to build description trees . Decision tree classifiers provide feasible solutions to both the MDL and MDA problems . We compared Learn2Cover and DesTree with CART on UCI datasets . In each experiment we described one class of points C , considering all points from other classes within BC , the bounding box for C , as C
−
.
Learn2Cover vs . CART . In each experiment , BC was fed to Learn2Cover and CART . The CART parameters were set such that a complete tree without misclassifications could be built . The entropy method was used for tree splits . dataset wine iono iris
Table 1 . Learn2Cover vs . CART . |C−| Learn2Cover CART dim |C| cls 5 / 5 12 71 13 2 7 / 6 11 225 34 g 18 5 / 5 4 50 vir 35 / 20 1588 88 10 4 blocks 48 / 49 4974 329 10 2 blocks 144 / 174 783 463 8 cyt yeast 164 / 170 429 8 939 nuc yeast 1342 2549 8 313 / 317 I abalone 454 / 449 1307 2693 abalone F 8 abalone M 8 1528 2626 510 / 488
2 / 1 3 / 2 3 / 3 31 / 12 26 / 19 69 / 97 74 / 87 165 / 179 214 / 259 251 / 278
FindClans
–0 –0 –0 –6 –7 –13 –16 –37 –62 –54
−
−
For each dataset , Table 1 presents the class label , the di , and the results . mensionality , the cardinalities of C and C For Learn2Cover and CART , a/b denotes the cardinalities respecof the two sets of rectangles covering C and C tively . For CART , a and b correspond to the numbers of leaf nodes of the two classes . For Learn2Cover , they correspond to | | and | −| . The smallest a or b is highlighted in bold font . We observe that , on average , Learn2Cover needs only half of the description length required by CART . Results from other UCI datasets as well as synthetic datasets are not presented . However , the above observation holds consistently through all the experiments .
1 f
0.9
0.8
0.7
0.6
0.5 yeast(cyt ) DesTree blocks(2 ) DesTree yeast(cyt ) CART blocks(2 ) CART
0
10
20
30
40
50
60
70
Figure 7 . DesTree vs . CART . l
80
The output of Learn2Cover was further fed into FindClans for additional length reduction . In Table 1 , −c denotes the additional reduction achieved by FindClans comparing to the shortest length ( in bold font ) . The effectiveness of FindClans depends on the size and distribution of the input data ; bigger and more complex datasets are likely to exhibit more clans , leading to more length reduction . We observe that , FindClans further reduces the shortest description length by about 20 % on average and up to 50 % .
DesTree vs . CART . In each experiment , DesTree took as input or − returned by Learn2Cover . For CART , the complete tree without misclassifications was pruned step by step . In each step , the misclassifications for both classes were counted to calculate the F measure . The number was also of rectangles covering the target class C or C recorded as the description length .
−
Figure 7 demonstrates the results of DesTree ( Cdescription tree ) and CART ( target class C ) , for clarity , on only two of the UCI datasets used in Table 1 . f and l denote the F measure and description length respectively . As expected , for both methods , f decreases monotonically with decreasing l . However , the DesTree results clearly dominate the ones from CART . For each l , DesTree achieves a significantly higher f , and each f a significantly smaller l . Again , this observation holds consistently for all other experiments not presented , including ones on other UCI description datasets and synthetic datasets , as well as C tree experiments on both data types .
−
52 Comparisons with BP
While the focus of our study is on the MDA problem , BP addresses the MDL problem only . In this series of experiments , we compared Learn2Cover with BP on synthetic datasets as BP works on grid data . Our data generator follows exactly what [ 16 ] does for BP . It takes as input the dimensionality , the number of intervals of each dimension , and the density . Dense cells are randomly generated in a grid with specified density , then one of them is randomly selected to grow a connected dense cell set as cluster C , the rest of the dense cells in BC constitute C
−
.
In our experiments with BP , we did not limit the number of “ don’t care ” cells for use and allowed BP to find the best possible results . Since BP only generates one set of rectangles , we used from Learn2Cover for the comparison . We studied the averaged percentage length reduction compared to BP for varying dataset size and dimensionality . We observed that in all cases Learn2Cover clearly outperformed BP , gaining 20 % to 50 % length reduction . As a general tendency , the reduction increased with increasing complexity of data . FindClans further improved the results , gaining an additional 25 % length reduction on average .
BP starts with the maximal rectangles generated by Greedy Growth in a greedy manner . The “ don’t care ” cells may come too late to be helpful , as illustrated in Figure 8 .
1 4 7
2 5 8
3 6 9
C : {3 , 4 , 5 , 6 , 7} ; C– : { 1 , 9} ; “ don’t care ” cells : 2 , 8 Greedy Growth : R47 + R456 + R36 BP : the same as Greedy Growth since any pairwise merge of rectangles would cause a violation covering 1 or 9 Learn2Cover : R2356 + R4578
Figure 8 . A typical case in BP .
Runtime was not a major concern of this study . We did not integrate the X tree index , on which BP relies to reduce the violation checking time . Without indexing for both , we observed that Learn2Cover ran faster than BP by one to two orders of magnitude . Recall that Learn2Cover can significantly reduce the number of points in consideration for violation checking . If we assume constant number of rectangles , Learn2Cover has a worst case runtime of O(|BC|2 ) . DesTree and FindClans also have quadratic worst case runtime in the number of input rectangles , O(| |2 ) or O(| −|2 ) for DesTree and O(|T|2 ) for FindClans respectively . In particular , for DesTree , the accuracy calculation results of all possible pairwise merges of rectangles can be reused in each iteration , recalculation is needed only for the resulting parent rectangle , which takes linear time . For FindClans , in each call of procedure findAClan( ) , the results from “ find theN ” ( line 1 ) can be reused .
6 . Conclusions
In this paper , we systematically studied rectangle based discriminative data generalization in the context of cluster description , which transforms clusters into patterns and provides the possibility of obtaining human comprehensible knowledge from clusters . In particular , we introduced and ± , analyzed novel description formats , SOR providing enhanced expressive power ; we defined the novel Maximum Description Accuracy ( MDA ) problem , allowing users to specify different trade offs between interpretability and accuracy ; we also presented heuristic algorithms together with their experimental evaluations . and kSOR
−
The concept of cluster in our study , in the narrow sense , is the output from clustering algorithms . Our study is motivated from and can find most applications in describing such clusters . In the broad sense , a cluster can be used to represent an arbitrary class of labeled data that require discriminative generalization .
Last but not least , cluster descriptions are patterns which can be stored as tuples in a relational table , so that a clustering and its associated clusters become queriable data mining objects . Therefore , this research can serve as a first step for integrating clustering into the framework of inductive databases [ 13 ] , a paradigm for query based “ secondgeneration ” database mining systems .
References
[ 1 ] R . Agrawal , J . Gehrke , D.Gunopulos , and PPaghavan Automatic subspace clustering of high dimensional data for data mining applications . In SIGMOD , 1998 .
[ 2 ] R . Baeza Yates and B . Ribeiro Neto . Modern Information
Retrieval . Addison Wesley Longman : Harlow , UK , 1999 .
[ 3 ] P . Berman and B . Dasgupta . Approximating rectilinear poly gon cover problems . In Algorithmica , 1997 .
[ 4 ] C . Blake and C . Merz . UCI repository of machine learning databases . 1998 .
[ 5 ] L . Breiman , J . Friedman , R . Olshen , and C . Stone . Classifi cation and Regression Trees . Wadsworth , 1984 .
[ 6 ] R . Carr , S . Doddi , G . Konjevod , and M . Marathe . On the red blue set cover problem . In SODA , 2000 .
[ 7 ] J . Eckstein , P . Hammer , Y . Liu , M . Nediak , and B . Simeone . The maximum box problem and its application to data analysis . In Comput . Optim . Appl . , volume 23 , pages 285– 298 , 2002 .
[ 8 ] U . Feige . A threshold of lnn for approximating set cover . In
Journal of the ACM , volume 45(4 ) , pages 634–652 , 1998 .
[ 9 ] B . Gao and M . Ester . Cluster description formats , problems , and algorithms . In SDM , 2006 .
[ 10 ] B . Gao and M . Ester . Right of inference : Nearest rectangle learning revisited . In ECML , 2006 .
[ 11 ] M . Garey and D . Johnson . Computers and Intractability : A guide to the Theory of NP completeness . WH Freeman : New York , 1979 .
[ 12 ] D . S . Hochbaum . Approximation algorithms for NP hard problems . PWS Publishing Company : Boston , 1997 .
[ 13 ] T . Imielinski and H . Mannila . A database perspective on knowledge discovery . In Communications of the ACM , volume 39(11 ) , pages 58–64 , 1996 .
[ 14 ] D . Johnson . Approximation algorithms for combinatorial problems . In J . Comput . System Sci . , 1974 .
[ 15 ] V . A . Kumar and H . Ramesh . Covering rectilinear polygons with axis parallel rectangles . In STOC , 1999 .
[ 16 ] L . Lakshmanan , R . Ng , C . Wang , X . Zhou , and T . Johnson . The generalized MDL approach for summarization . In VLDB , 2002 .
[ 17 ] W . Masek . Some NP complete set covering problems . man uscript , MIT , Cambridge , MA , 1979 .
[ 18 ] A . Mendelzon and K . Pu . Concise descriptions of subsets of structured sets . In PODS , 2003 .
