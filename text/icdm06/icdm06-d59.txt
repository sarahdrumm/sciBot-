A Balanced Ensemble Approach to Weighting Classi.ers for
Text Classi.cation
Gabriel Pui Cheong Fung1 , Jeffrey Xu Yu1 , Haixun Wang2 , David W . Cheung3 , Huan Liu4
1 The Chinese University of Hong Kong , Hong Kong , China , fpcfung ; yug@se:cuhk:edu:hk
2 IBM T . J . Watson Research Center , New York , USA , haixun@us:ibm:com 3 The University of Hong Kong , Hong Kong , China , dcheung@cs:hku:hk
4 Arizona State University , Arizona , USA , hliu@asu:edu
Abstract
This paper studies the problem of constructing an effective heterogeneous ensemble classi.er for text classication One major challenge of this problem is to formulate a good combination function , which combines the decisions of the individual classi.ers in the ensemble . We show that the classi.cation performance is affected by three weight components and they should be included in deriving an effective combination function . They are : ( 1 ) Global effectiveness , which measures the effectiveness of a member classi.er in classifying a set of unseen documents ; ( 2 ) Local effectiveness , which measures the effectiveness of a member classi.er in classifying the particular domain of an unseen document ; and ( 3 ) Decision con.dence , which describes how con.dent a classi.er is when making a decision when classifying a speci.c unseen document . We propose a new balanced combination function , called Dynamic Classi.er Weighting ( DCW ) , that incorporates the aforementioned three components . The empirical study demonstrates that the new combination function is highly effective for text classication
1 Introduction
Let U be a set of unseen documents and C be a set of prede.ned categories . Automated text classi.cation is the process of labeling U with C , such that every d 2 U will be assigned to some of the categories in C . Note that d can be assigned to none of the categories in C . If the number of categories in C is more than two ( jC j > 2 ) , it is a multilabel text classi.cation problem . Since every multi label text classi.cation problem can be transformed to a binarylabel text classi.cation problem , we focus on the binary problem in this paper ( jC j = 2 ) . Let c 2 C . Binary label text classi.cation is to construct a binary classi.er , denoted by F( ) , for c such that :
F(d ) =(1
,1 if f ( d ) > 0 ; otherwise ;
( 1 )
1 where F(d ) = 1 indicates that d belongs to c and F(d ) = f ( ) 2 ´ is a ,1 indicates that d does not belong to it . decision function . Every classi.er , Fi , has its own decision function , fi( ) . If there are m different classi.ers , there will be m different decision functions . The goal of constructing a binary classi.er , F( ) , is to approximate the unknown true target function ffF( ) , so that F( ) and ffF( ) are coincident as much as possible [ 17 ] .
In order to improve the effectiveness , ensemble classi.ers ( aka classi.er committee ) were proposed [ 1 , 3 , 5 , 6 , 7 , 8 , 9 , 15 , 16 , 17 , 18 , 19 ] . An ensemble classi.er is constructed by grouping a number of member classiers If the decisions of the member classi.ers are combined properly , the ensemble is robust and effective . There are two kinds of ensemble classi.ers : homogeneous and heterogeneous .
A homogeneous ensemble classi.er contains m binary classi.ers in which all classi.ers are constructed by the same learning algorithm . Bagging and boosting [ 19 ] are two common techniques [ 1 , 15 , 16 , 18 ] .
A heterogeneous ensemble classi.er contains m binary classi.ers in which all classi.ers are constructed by different learning algorithms ( eg , one SVM classi.er and one kNN classi.er are grouped together ) [ 19 ] . The individual decisions of the classi.ers in the ensemble are combined ( eg , through stacking [ 19] ) :
Q(d ) =(1 if g,F1(d);F2(d ) ; : : : ;Fm(d) > 0 ;
,1 otherwise ;
( 2 ) where Q( ) is an ensemble classi.er ; g( ) is a combination function that combines the outputs of all Fi( ) . The effectiveness of the ensemble classi.er , Q( ) , depends on the effectiveness of g( ) . In this paper , we concentrate on analyzing heterogeneous ensemble classiers Our problem is thus to examine how to formulate a good g( ) .
Four widely used g( ) are : ( 1 ) Majority voting ( MV ) [ 8 , 9 ] ; ( 2 ) Weighted linear combination ( WLC ) [ 7 ] ; ( 3 ) Dynamic classi.ers selection ( DCS ) [ 3 , 8 , 6 , 5 ] ; and ( 4 ) Adaptive classi.ers combination ( ACC ) [ 8 , 9 ] . Except for MV , the other three functions assign different weights to the classi.ers in the ensemble . The bigger the weight , the more ef accurate ( effective ) than Naive Bayes ( NB ) [ 20] ) . Although it does not imply all of the decisions made by SVM must be superior than NB , it does imply that we should value the judgment of SVM higher than that of NB in general . In this paper , we term this kind of effectiveness as global effectiveness of a classi.er , denoted by ai ( Eg aSVM > aNB ) . ai gives us good insight about how to weight the classi.ers in an ensemble . Intuitively , if we construct an ensemble classi.er by grouping Fa( ) and Fb( ) together , where aa > ab , then we should value Fa( ) higher than Fb( ) .
Yet , a globally effective classi.er may sometimes perform poorly on some speci.c dataset ( domain ) . As an example , consider two classi.ers , SVM and NB . According to the benchmark Reuters21578 , the micro F1 scores for SVM and NB are respectively 0.860 and 0788 Unfortunately , the F1 score for SVM when classifying Retail ( Retail fl Reuters21578 ) is 0.0 , but it is 0.667 for NB . As a result , an effective classi.er may not always perform well in all domains ( eg , SVM performs poorly in Retail ) . This can be further illustrated in Figure 1 . The two ovals , A and B , represent two different domains . Oval A covers over the decision boundary , whereas Oval B resides in the lower triangle . All of the documents within the domain of Oval A are aligned near the decision boundary . An unseen document that belongs to this domain may easily be classi.ed wrongly . On the other hand , the documents within the domain of Oval B are well separated by the decision boundary . An unseen document that belongs to this domain will most likely be classi.ed correctly . So , the effectiveness of the classi.er also relies on the domain of the unseen data . We term this kind of effectiveness as local effectiveness of the classi.er , denoted by bi . bi helps us to adjust the weights of the classi.ers in the ensemble . If the ai of Fi is very high but it is not effective in classifying the domain of the unseen document , we should re consider its effectiveness .
For every decision a classi.er makes , one may ask how con.dent the classi.er is about the decision ? Consider the two unseen documents , document 1 and document 2 , in the same domain ( Oval B ) in Figure 1 . While both document 1 and document 2 reside near the boarder of their domain , document 2 locates closer to the decision boundary ( the dashed line ) whereas document 1 locates far away from it . Since both document 1 and document 2 belong to the same domain , the local effectiveness of the classi.er upon them are the same . Yet , the con.dence in making a correct decision for document 1 should be higher than that of document 2 , as document 1 is further away from the decision boundary ( d1 > d2 ) . In this paper , we term it as decision condence It is estimated according to the distance between the unseen document and the decision boundary .
We summarize the needs for the above components as follows : if we ignore ai , over .tting may result as we neglect the combined infiuence of all domains . If we ignore
Figure 1 . Illustration of local effectiveness and decision condence fective is that classier In MV , all classi.ers in the ensemble are equally weighted . It can end up with a wrong decision if the minority votes are signicant WLC assigns static weights to the classi.ers based on their performance on a validation data . However , a generally well performed classi.er can perform poorly in some speci.c domains . For instance , the micro F1 scores of SVM and Naive Bayes ( NB ) for the benchmark Reuters21578are respectively 0:860 and 0:788 . In this sense , SVM excels NB . Yet , for the categories Potatoand Retailin Reuters21578 , the F1 scores for NB are both 0.667 , but are both 0.0 for SVM . DCS and ACC weight the classi.ers by partitioning the validation data ( domain speci.c ) , they do not combine the classi.ers’ decisions , but select one of the classi.ers from the ensemble and rely on it solely . We will show in the experiments that this will lead to inferior results .
In this paper , we propose a new combination function called Dynamic Classi.ers Weighting ( DCW ) . We consider three components when combining classi.ers : ( 1 ) Global Effectiveness , which is the effectiveness of a classi.er in an ensemble when it classi.es a set of unseen documents ; ( 2 ) Local effectiveness , which is the effectiveness of a classi.er in an ensemble when it classi.es the particular domain of the unseen document ; and ( 3 ) Decision con.dence , which is the con.dence of a classi.er in making a decision of the ensemble for a speci.c unseen document .
2 Motivations
Let F1( );F2( ) ; : : : ;Fm( ) be m different binary classi.ers and f1( ) , f2( ) ; : : : ; fm( ) be their corresponding decision functions . Conceptually , Fi( ) divides the entire domain into two parts according to fi( ) . Figure 1 illustrates this idea . The dashed lines are the decision boundaries . If the unseen document , d , falls into the upper ( lower ) triangle , it would be labeled as positive ( negative ) . Usually , if d is further away from the decision boundary , the decision of d by Fi(d ) is more condent
Every classi.er has different effectiveness . For instance , Support Vectors Machine ( SVM ) is being regarded as more
2 bi , over generalization may ensue as it relies on the domain where the unseen document appears . ai and bi do not measure the classi.er ’s decision con.dence , gi is proposed as it indicates how much con.dence a classi.er has when it classi.es the unseen documents .
3 Dynamic Classi.ers Weighting ( DCW )
In the previous section , we have explained why the three weight components ( ai , bi and gi ) are helpful in constructing an effective combination function , g( ) . We now describe how they are estimated and how they are combined in an ensemble classier ai is the effectiveness of the classi.er when we use it to classify a set of unseen documents . During the training phase , although we do not have a set of labeled unseen documents , we can estimate ai from the training data , D : we estimate ai by 10 folded cross validation . While our experience suggested that estimating the effectiveness of a classi.er based on cross validation would always yield an optimistic result than evaluating it from the unseen data , this would not be a problem in our situation , as we are not targeting for evaluating the real global effectiveness of the classi.ers , but aiming at obtaining the relative global effectiveness . We normalize ai such that 0 < ai < 1 and ( cid:229)m i=1 ai = 1 . bi is the effectiveness of the classi.er when we use it to classify the domain of the unseen document , d . For an unseen document , we would never know what the true domain of d is . As above , we can only estimate its domain according to the training data , D . Let D be a subset of documents in the training data , ie , D D . We can .nd the domain of the unseen document , d , by using D , to extract the documents in D that are similar to d . Accordingly , the extraction of D is based on a nearest neighbor strategy . We extract the top n documents that are most similar to d from D . The value n can be readily obtained through a validation dataset . The similarities among these n documents are measured by the cosine coef.cient [ 13 ] . Since D is a subset of the training data ( D D ) , we will know precisely the labels of those documents that appear in D . We estimate bi by evaluating D using the F1 score . bi is normalized such that 0 < bi < 1 and ( cid:229)m gi is a measure about how con.dent the classi.er is when it makes a decision upon d . From Eq ( 1 ) , the classi.cation decision of the classi.er , Fi( ) , is based on the decision function , fi( ) . For most cases , if not all , the higher the magnitude of fi( ) , the more con.dent are their decisions . Consequently , we can compute gi by using the decision function , fi( ) . Unfortunately , the range of fi( ) varies among different algorithms . For example , Fi( ) may have fi( ) in the range of [ ,1;1 ] , whereas F j( ) may have another f j( ) in the range of ( ,¥ ; +¥ ) . Since different decision functions have different ranges , a direct comparison among them is i=1 bi = 1 . inappropriate . We solve the problem as follows : Let D be the domain of the unseen document . D is obtained by the technique described previously . We compute gi as follows : gi =fifififi
( cid:181)i = fi(d ) ( cid:181)i 1 jDj ( cid:229 ) fifififi d02D
; fi(d0 ) ;
( 3 )
( 4 ) where ( cid:181)i is the average con.dence of the decisions made by fi( ) among the documents in D . Since D D , we can presume that ( cid:181)i is non zero . When gi > 1 , fi(d ) , has more than average con.dence to make a correct classi.cation on d , where d will be far away from the decision boundary ( eg , document 1 in Figure 1 ) . When gi < 1 , the decision function , fi(d ) , has less than average con.dence to make a correct classi.cation on d , where d will be closer to the decision boundary ( eg , document 2 in Figure 1 ) . We normalize gi such that 0 < gi < 1 and ( cid:229)m
We now present how ai , bi and gi are combined . Assume In the most that there are m classi.ers in the ensemble . simplest form , the combination function , g( ) is : i=1 gi = 1 . g( ) = m
( cid:229 ) i decisioni ;
( 5 ) where decisioni = Fi(d ) 2 f1; 1g ( Eq ( eq:c) ) . Here , all classi.ers in the ensemble are equally weighted ( ie MV ) . In DCW , since a con.dence ( gi ) is associated with each decisioni , therefore : g( ) = m
( cid:229 ) i decisioni . gi :
( 6 )
Yet , even for a con.dent decision , we need to review whether the classi.er , which makes this decision , is effective in the ensemble . Consequently : g( ) = m
( cid:229 ) i decisioni . gi . effectivenessi :
( 7 )
Since there are two kinds of effectiveness for each of the classi.er ( ai and bi ) , we have : g( ) = m
( cid:229 ) i Fi(d ) . ai . bi . gi ;
( 8 )
4 Experimental Study
The purpose of the experiments is twofold . ( 1 ) We want to examine how effective the Dynamic Classi.ers Weighting ( DCW ) is , when it is compared with the other kinds of heterogeneous ensemble classiers As such , we implemented four existing ensemble classi.ers for comparison :
3
No .
Combination
1 2 3 4 5 6 7 8 9 10 11
S+N S+R S+K R+N K+N K+R S+K+R S+K+N S+R+N K+R+N S+K+R+N
MV WLC 0.874 ( cid:150 ) 0.883 ( cid:150 ) 0.862 ( cid:150 ) ( cid:150 ) 0.833 0.824 ( cid:150 ) 0.825 ( cid:150 ) 0.879 0.874 0.872 0.825 0.851
0.872 0.855 0.852 0.857
( cid:150 )
DCS 0.859 0.862 0.862 0.831 0.827 0.832 0.862 0.859 0.861 0.837 0.861
ACC 0.852 0.874 0.843 0.821 0.820 0.821 0.876 0.865 0.856 0.823 0.859
DCW 0.876 0.885 0.863 0.834 0.829 0.831 0.882 0.873 0.874 0.830 0.853
Reuters21578
Newsgroup20
MV WLC 0.817 ( cid:150 ) 0.800 ( cid:150 ) 0.813 ( cid:150 ) ( cid:150 ) 0.762 0.780 ( cid:150 ) 0.762 ( cid:150 ) 0.815 0.819 0.815 0.782 0.720
0.776 0.783 0.777 0.775
( cid:150 )
DCS 0.761 0.762 0.763 0.738 0.746 0.764 0.763 0.762 0.761 0.750 0.762
ACC 0.794 0.793 0.780 0.759 0.769 0.760 0.812 0.809 0.801 0.775 0.761
DCW 0.817 0.800 0.815 0.765 0.780 0.765 0.816 0.821 0.815 0.784 0.763
Table 1 . The results of the micro F1 for different ensemble classiers
Majority voting ( MV ) [ 8 , 9 ] , Weighted linear combination ( WLC ) [ 7 ] , Dynamic classi.ers selection ( DCS ) [ 3 , 8 , 6 , 5 ] , and Adaptive classi.ers combination ( ACC ) [ 8 , 9 ] . We report the results in Section 41 ( 2 ) We want to understand how signi.cant the results are whenever one of the ensemble classi.ers outperforms the others . As such , we performed a pairwise signi.cant test in Section 42
In the experiments , two benchmarks are used : Reuters21578 and Newsgroup20 . For Reuters21578 , we separate the dataset into training data and testing data using the ModApte split [ 2 ] . For Newsgroup20 , for each of the categories , we randomly select 80 % of the postings as training data , and the remaining as testing data .
For the data preprocessing , punctuation , numbers , web page addresses , and email addresses are removed . All features are stemmed and converted to lower cases , and are weighted using the standard tf idf schema [ 14 ] . Features that appear in only one document are ignored . All features are ranked based on the NGL Coef.cient[12 ] , and the top X features are selected . This X is tuned for different classi.ers and for different benchmarks .
For creating the ensemble classi.ers , different combinations of four kinds of classi.ers are used : ( 1 ) Support Vectors Machine ( SVM ) ; ( 2 ) k Nearest Neighbor ( kNN ) ; ( 3 ) Rocchio ( ROC ) ; ( 4 ) Naive Bayes ( NB ) . Their default settings are as follows : For SVM , we use linear kernel with C = 1:0 . No feature selection is required [ 4 ] . For kNN , we set k = 50 and select 2,750 and 4,900 features for Reuters21578and Newsgroup20 . For ROC , we implement the version in [ 11 ] and selects 2,750 and 7,500 features for Reuters21578 and Newsgroup20 . For NB , we implement the multinomial version [ 10 ] and selects 2,750 and 9,500 features for Reuters21578and Newsgroup20 .
4.1 Effectiveness Analysis
Table 1 shows the results of the micro F1 score for all ensemble classi.ers ( MV , WLC , DCS , ACC and DCW ) when they are created using different combinations of the binary
4 classi.ers for both benchmarks . The left most column denotes which of the binary classi.ers are used for creating the corresponding ensemble classier We use S , K , R and N to denote SVM , kNN , Rocchio and Naive Bayes , respectively . For example , S+K+R represents an ensemble classi.er which is comprised of SVM , kNN and Rocchio . Note that MV cannot be created if the number of binary classi.ers in the ensemble is an even number , hence the ( cid:147)(cid:150)(cid:148 ) entries in Table 1 .
At the .rst glance , the results are promising . DCW , the proposed approach , dominates over all other approaches when they are being created using the same set of binary classiers Similar results are obtained when we use the macro F1 score . The only case where DCW performs inferior is case 6 when DCW is created by kNN and Rocchio ( K+R ) , meanwhile it is evaluated using Reuters21578 . Its micro F1 is 0.831 , which is 0.001 lower than DCS ( Dynamic Classi.ers Weighting ) . Nevertheless , such a difference can be negligibled .
Concerning DCW , the best combination of binary classi.ers in the ensemble is SVM and Rocchio ( case 2 ) for Reuters21578 . The micro F1 score is 0:885 . It is also the best results obtained among all of the ensemble classi.ers that we have evaluated . For Newsgroup20the best result is obtained by comprising SVM , kNN and Rocchio together ( case 8 ) . The micro F1 score is 0:821 . It is also the best result obtained among all approaches .
For MV , its philosophy is to take the majority agreement among the binary classi.ers in the ensemble . Hence , the number of binary classi.ers must be an odd number . So we can only create MV using three different binary classiers Interestingly , all combinations perform similarly . best combination the for Concerning WLC , Reuters21578 ( case 2 ) , its micro F1 score is 0.883 , which is higher than all ensemble classi.ers ( except DCW ) . For Newsgroup20 , similar observations are made , where its best combination is case 8 . Although the idea of WLC is very simple ( cid:150 ) assigns static weights to the classi.ers in the ensemble according to their global effectiveness and its score combines them linearly ( cid:150 ) it performs surprisingly well . Another interesting .nding is that when SVM is included in the ensemble , the effectiveness of WLC would be increased dramatically . This suggests that the choice of the classi.ers in WLC is particularly important . best micro F1
Concerning DCS , for Reuters21578 ( case 2 ) is 0:862 only . It is far lag behind all the other approaches . For Newsgroup20 , none of the F1 score is higher than 0:77 . We believe that the reasons of why DCS performs poorly are because : ( 1 ) It does not combine the classi.ers’ decisions . Rather , it selects one of the classi.er in the ensemble and relies on it completely . ( 2 ) It neither pays attention to the global effectiveness of the classi.ers nor the decision condence
ACC performs slightly better than DCS . This may be because the decision strategy for ACC is more sophisticated that DCS . The best ensembles for Reuters21578and Newsgroup20are both case 7 . However , these results are all inferior than both WLC and our DCW .
4.2 Signi.cant Test
In this section , we conduct a pairwise comparison among them using the signi.cant test [ 20 ] . Given two classi.ers , FA( ) and FB( ) , the signi.cant test determines whether FA( ) performs better than FB( ) based on the errors that FA( ) and FB( ) made . Let N be the total number of the unseen documents , and ai = f0;1g ( bi = f0;1g ) indicate whether FA( ) ( FB( ) ) makes a correct classi.cation upon the ith unseen document . ai = 0 means FA( ) makes an incorrect classi.cation whereas ai = 1 means FA( ) makes a correct one . Similar de.nition is also applied to bi . Let da be the number of times that FA( ) performs better than FB( ) , and db be the number of times that FB( ) performs better than FA( ) . In this test , the null hypothesis is that both classi.ers perform the same ( H0 : da = db ) . The alternative is that FA( ) and FB( ) performs differently ( H1 : da 6= db ) . Table 2 shows the results of comparing the performance of DCW with the other ensemble classiers A * B means A performs signi.cantly better than B ( P Value 0:01 ) . A > B means A performs slightly better than B . A B means no evidence indicates A and B has any differences in terms of the errors they made . A summary is given below :
Reuters21578 : fDCW , WLCg > fMV , ACCg * DCS Newsgroup20 : DCW > WLC > ACC * MV * DCS
5 Conclusions
In order to formulate an effective combination function for heterogeneous ensemble classi.er , three weight components are necessary : Global Effectiveness , Local Effectiveness , and Decision Condence We compare DCW with
B
A MV WLC DCS MV ACC MV DCW MV WLC DCS ACC WLC DCW WLC ACC DCS DCW DCS ACC DCW
Reuters21578
Newsgroup20
< *
* >
<
*
* >
<
Table 2 . Results of the signi.cant test . four other kinds of heterogeneous ensemble classi.ers using two benchmarks . The results indicated that DCW can effectively balance the contributions of the three components and outperforms the existing approaches .
References
[ 1 ] W . W . Cohen and Y . Singer . Context sensitive learning methods for text categorization . ACM Transactions on
Information Systems ( TOIS ) , 17(2):141(cid:150)173 , 1999 .
[ 2 ] F . Debole and F . Sebastiani . An analysis of the relative hardness of Reuters 21578 subsets . Journal of the
American Society for Information Science and Technology , 56(6):584(cid:150)596 , 2004 .
[ 3 ] G . Giacinto and F . Roli . Adaptive selection of image classiers In Proceedings of the 9th International Confer ence on Image Analysis and Processing ( ICIAP’97 ) , pages 38(cid:150)45 , Florence , Italy , 1997 .
[ 4 ] T . Joachims . Text categorization with support vector machines : Learning with many relevant features . In Proceedings of 10th European Conference on Machine Learning ( ECML’98 ) , pages 137(cid:150)142 , Chemnitz , Germany , 1998 .
[ 5 ] K . B . Kevin Woods , W . Philip Kegelmeyer . Combination of multiple classi.ers using local accuracy estimates .
IEEE Transactions on Pattern Analysis and Machine Intelligence ( TPAMI ) , 19(4):405(cid:150)410 , 1997 .
[ 6 ] W . Lam and K Y Lai . A meta learning approach for text categorization . In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval ( SIGIR’01 ) , pages 303(cid:150)309 , New Orleans , Louisiana , USA , 2001 .
[ 7 ] L . S . Larkey and W . B . Croft . Combining classi.ers in text categorization . In Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval ( SIGIR’96 ) , pages 289(cid:150)297 , Zurich , Switzerland , 1996 .
[ 8 ] Y . H . Li and A . K . Jain . Classi.cation of text documents . The Computer Journal , 41(8):537(cid:150)546 , 1998 .
[ 9 ] R . Liere and P . Tadepalli . Active learning with committees for text categorization .
In Proceedings of 14th
National Conference on Arti.cial Intelligence ( AAAI’97 ) , pages 591(cid:150)596 , Providence , Rhode Island , 1997 .
[ 10 ] A . McCallum and K . Nigam . A Comparison of Event Models for Naive Bayes Text Classication In The 15th National Conference on Arti.cial Intelligence ( AAAI’98 ) Workshop on Learning for Text Categorization , 1998 .
[ 11 ] A . Moschitti . A study on optimal parameter tuning for rocchio text classier In Proceedings of the 25th European
Conference on Information Retrieval Research ( ECIR’03 ) , pages 420(cid:150)435 , Pisa , Italy , 2003 .
[ 12 ] H . T . Ng , W . B . Goh , and K . L . Low . Feature selection , perception learning , and a usability case study for text categorization . In Proceedings of the 20th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval ( SIGIR’97 ) , pages 67(cid:150)73 , Philadelphia , PA , USA , 1997 .
[ 13 ] E . Rasmussen . Clustering algorithm . In W . B . Freakes and R . Baeza Yates , editors , Information Retrieval Data
Structures & Algorithms , pages 419(cid:150)442 . Prentice Hall PTR , 1992 .
[ 14 ] G . Salton and C . Buckley . Term weighting approaches in automatic text retrieval . Information Processing and
Management ( IPM ) , 24(5):513(cid:150)523 , 1988 .
[ 15 ] R . E . Schapire and Y . Singer . BoosTexter : a boosting based system for text categorization . Machine Learning ,
39(2(cid:150)3):135(cid:150)168 , 2000 .
[ 16 ] R . E . Schapire , Y . Singer , and A . Singhal . Boosting and Rocchio applied to text ltering In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval ( SIGIR’98 ) , pages 215(cid:150)223 , Melbourne , Australia , 1998 .
[ 17 ] F . Seabastiani . Machine learning in automated text categorization . ACM Computing Surveys , 34(1):1(cid:150)47 , 2002 .
[ 18 ] S . M . Weiss , C . Apte , F . J . Damerau , D . E . Johnson , F . J . Oles , T . Goetz , and T . Hampp . Maximizing text mining performance . IEEE Intelligent Systems , 14(4):63(cid:150)69 , 1999 .
[ 19 ]
I . H . Witten and E . Frank . Data Mining : Practical Machine Learning Tools and Techniques . Morgan Kaufmann , second edition , 2005 .
[ 20 ] Y . Yang and X . Liu . A re examination of text categorization methods .
In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval ( SIGIR’99 ) , pages 42(cid:150)49 , Berkeley , California , USA , 1999 .
5
