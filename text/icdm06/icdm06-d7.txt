Boosting Kernel Models for Regression
Ping Sun and Xin Yao
School of Computer Science University of Birmingham
{psun,xyao}@csbhamacuk
Abstract
This paper proposes a general boosting framework for combining multiple kernel models in the context of both classification and regression problems . Our main approach is built on the idea of gradient boosting together with a new regularization scheme and aims at reducing the cubic complexity of training kernel models . We focus mainly on using the proposed boosting framework to combine kernel ridge regression ( KRR ) models for regression tasks . Numerical experiments on four large scale data sets have shown that boosting multiple small KRR models is superior to training a single large KRR model on both improving generalization performance and reducing computational requirements .
1
Introduction
The emergence of kernel based methods originated from the success of support vector machines ( SVM ) in pattern recognition [ 1 ] . Afterwards , a number of powerful kernel methods , eg , kernel ridge regression ( KRR ) [ 2 ] , kernel principle component analysis ( KPCA ) [ 14 ] , kernel fisher discriminant ( KFD ) [ 10 ] and kernel logistic regression ( KLR ) [ 23 ] were proposed and have shown practical relevance for both supervised and unsupervised problems . Here we just consider the supervised problem , which can be formalized as the problem of inferring a function y = f ( x ) from a given traing dataset D = {(x1 , y1 ) , . . . , ( xN , yN )} . When y ∈ R is continuous , the problem is regression , whereas in classification problems , y is categorical ( eg , binary , y ∈ {−1 , 1} ) . Generally , kernel based methods [ 15 , 23 ] can be interpreted as the variational problem of finding the function f that minimizes the functional min f ∈H
H[f ] =
1 N
V ( yi , f ( xi ) ) +
λ 2kfk2
H
( 1 )
N
Xi=1 where V ( ·,· ) is a loss function , H is the Reproducing Kernel Hilbert Spaces ( RKHS ) generated by the kernel k(·,· ) and λ is a parameter that trades off the two terms . The first term assesses the quality of the prediction f ( xi ) for the observed target yi . The second one is called the regularization term and represents smoothness assumptions on f . The solution to the problem ( 1 ) was given by the well known representer theorem [ 13 ] which shows that each minimizer f ∈ H of H[f ] has the form of Xi=1
αik(xi , x ) . f ( x ) =
( 2 )
N
Substituting f ( x ) for ( 1 ) and using the property of RKHS hk(xi,· ) , k(xj,·)i = k(xi , xj ) , we obtain min
α
H(α ) =
1 N
N
Xi=1
V ( yi , ( Kα)i ) +
λ 2
α⊤Kα ,
( 3 ) where α = [ α1 , , αN ]⊤ is the weight parameter , K is the so called kernel matrix of size N × N and ( K)ij = k(xi , xj ) , i , j = 1 , N By defining different types of loss functions , the problem ( 3 ) corresponds to different kernel models . For example , soft margin loss function V ( yi , f ( xi ) ) = max{0 , 1 − yif ( xi)}2 leads to the primal formulation of SVM , logistic loss function results in KLR and KRR can be obtained by replacing V ( ·,· ) with a simple squared loss 1 . It is noted that different kernel models require different techniques to find the optimal coefficients α . For training KRR and KLR models , it requires solving a linear system of equations once or repeatedly . And the dual formulation of SVM is a quadratic programming ( QP ) problem with linear constraints . But a common drawback of all these training algorithms is that the computational cost tends to scale with the cube of the number of training examples ( ie , O(N3 ) ) and the memory requirements grow as the square , which is impractical for large scale datasets . To overcome these limitations , numerous speed up training algorithms
1Note that KRR was known as Regularization Networks ( RN ) in the theory of regularization [ 5 ] . Additionally , KRR is also equivalent to the MAP estimate of Gaussian Process Regression ( GPR ) [ 9 , 19 ] . with widely different motivations for each of them ( especially for SVM ) have been studied extensively in the literature , for example [ 11 , 21 , 3 , 4 , 8 , 23 , 9 ] .
One general approach to scale up these kernel models to large scale datasets is that of divide and conquer strategy . The basic idea is that the dataset is divided up into M small subsets , ie , {Dj , j = 1 , , M} , and M sub models are derived from the individual sets . The final prediction is generated by an ensemble model ¯f ( x ) which combine predictions of all M individual models , that is particular , we demonstrate how to combine multiple KRR models with the proposed framework . Compared to previous combining methods mentioned above , our work is simple to use , does not require any critical parameters and can be applied to boost different kinds of kernel based models . More important , our boosting approach could achieve better prediction results than a single kernel model on the whole training data , also with significantly lower computational requirements .
¯f ( x ) =
M
Xj=1 cjfj(x ) ,
( 4 )
2 Boosting Kernel Models
2.1 Gradient Boosting where fj(x ) is the j th sub model trained on Dj and cj is the corresponding weight coefficient . Since the training of individual model on smaller dataset can be very fast , the overall learning cost of M sub models together with the combining effort are still below the cost of training a single model on the whole dataset . The difficulty of this approach is how to nicely combine the individual results if we assume that small training subsets are randomly generated from the whole dataset .
The methods for weighting individual sub models appeared in the kernel learning community can be categorized into two classes . The first one is Mixture of Experts like approach which dynamically adjust the combining coefficients involved in the composite model . For example , in [ 4 ] , the authors proposed a parallel mixture of SVMs which combine their individual outputs using a trained MLP neural networks . Another representative example is Bayesian Committee Machine ( BCM ) [ 20 ] which was proposed to scale up the training of Bayesian kernel models . It weights each sub model by the inverse covariance of its prediction . A good performance of BCM requires that M subsets be pairwisely independent .
The second class of approaches is linearly weighting schemes which do not change the combining coefficients in the phase of testing . The naive example is simple averaging ( AV ) but it seems not suitable for combining stable kernel models to our best experience . A better idea [ 22 ] is to find an optimal combination based on optimiz ing an objective function under the constraints Pj cj = 1 and cj ≥ 0 , j = 1 , , M . Pavlov et . al . [ 11 ] extended the Adaboost technique to combine SVMs for classification problems . They claimed that making an ensemble of few SVMs , each trained on subsets between 2− 4 % of the original data set , results into algorithms whose performance is comparable to a standard SVM trained on the whole dataset . In this paper , we propose a general framework to nicely combine multiple kernel models for both classification and regression problems . Our work is based on the ideas of gradient boosting attached by a new regularization scheme . In is a general
Gradient boosting [ 6 ] framework to strengthen the weak base learners . In our case , the ‘weakness’ comes from learning kernel models on a subset of a given training set . Following the pioneering work by Friedman [ 7 ] , the boosting procedure can be generally viewed as a forward stagewise search for a good additive model . This is done by searching , at each iteration , for the base learner which gives the largest reduction in the loss denoted by L(y , ¯f ) 2 , and changing its coefficients accordingly , where ¯f denotes an ensemble model . The essential steps of a boosting procedure can be summarised as follows [ 7 ] :
1 . ¯f0(x ) = 0 ;
2 . For j = 1 : M do :
( a ) ( cj , fj(x ) ) = arg minc∗ j ( x ) j ,f ∗ i=1 L(yi , ¯fj−1(xi ) + c∗ j f ∗ ( b ) ¯fj(x ) = ¯fj−1(x ) + cjfj(x )
PN j ( xi ) )
3 . EndFor
4 . ¯f ( x ) = ¯fM ( x ) =PM j=1 cjfj(x ) .
Figure 1 . A general boosting framework
Here f ∗ j ( x ) is the initial base learner derived from the data subset Dj , fj(x ) is optimized one by boosting stage 2(a ) at the j th iteration and cj denotes the corresponding coefficient in the ensemble model ¯f ( x ) . The loss L(y , ¯f ) can be any differential function which corresponds to different boosting algorithms . The most prominent example is Adaboost which just employ the exponential loss L(y , ¯f ) = exp{−y ¯f} . Other examples include Logitboost with the ( minus ) binomial log likelihood L(y , ¯f ) =
2Note that , in the context of boosting , we use L(· , · ) denote the loss function instead of V ( · , · ) as done in kernel models .
2
2 ( y − ¯f )2 . log(1 + exp{−y ¯f} ) and L2boost with the simple squared loss L(y , ¯f ) = 1 In contrast to the objective function ( 3 ) used in kernel models , we note that boosting framework in Figure 1 does not include a regularization term . Actually , boosting utilizes an empirical method , ie , so called “ shrinkage ” in statistics , to avoid the resulting overfit problem by a large number of base learners . This can be accomplished by replacing stage 2(b ) in Figure 1 with
¯fj(x ) = ¯fj−1(x ) + ( ν · cj)fj(x ) ,
( 5 ) where 0 < ν ≤ 1 is a shrinkage factor . This can greatly improve the generalization performance of the algorithm [ 6 ] . The cost paid for better performance is increased computation since a large number of base learners are required . This conflicts with our expected objective of reducing computational cost . Another potential alternative to regularization in the context of boosting is introducing a penaly term in the objective , for example , the commonly used norm 2 penalty j , where µ > 0 is a trade off parameter . The explicit shortcoming of this strategy lies in introducing an additional parameter µ . No doubt , it will increase the computational burden of combining procedure for determining µ .
µP c2
2.2 A new regularization for boosting
We introduce a different regularization term which is similar to the one employed by kernel models . The idea was inspired by the sparse formulation of kernel models ( 3 ) , where the “ sparseness ” means that some entries of the solution α are exactly zeros . Let αM = [ αi1 , , αiM ] denote all the non zero entries of α indexed by IM = {i1 , , iM} which correponds to the set of so called basis vectors , the sparse formulation of ( 3 ) is given by the optimization problem min αM
H(αM ) =
1 N
N
Xi=1
V ( yi , ( KM αM )i)+
λ 2
α⊤
M KMM αM and the resulting sparse model has a form of f ( x ) =
M
Xj=1
αij k(˜xj , x ) .
( 6 )
( 7 )
Xj=1
D
Xd=1
Now we imagine f ( x ) in ( 7 ) to be an ensemble model
M
¯f ( x ) = cjfj(x ) ,
( 8 ) and now each term in the expansion is a base learner which consists of multiple kernel terms not just one as in ( 7 ) , that is , fj(x ) = aj dk(˜xj d , x ) ,
( 9 ) where D is the number of kernel terms 3 involved in the submodel fj(x ) , {˜xj d , d = 1 , , D} ⊆ Dj are basis vectors and {aj d , d = 1 , , D} are weight parameters . Accordingly , the matrices KM and KMM in sparse formulation ( 3 ) , denoted by F and Q respectively in the context of ensemble model , evolve as
D
Xd=1
Fij = and aj dk(xi , ˜xj d ) , i = 1 , , N ; j = 1 , , M ( 10 )
D
D daj ′ aj d′ k(˜xj d , ˜xj ′ d′ ) , j , j ′ = 1 , , M .
( 11 )
Qjj ′ =
Xd=1
Xd′=1
Finally , we can obtain the following objective function for combining multiple kernel models :
¯H(c ) = min c
1 N
L(yi , ( F c)i ) +
µ 2 c⊤Qc ,
( 12 )
N
Xi=1 where c = [ c1 , . . . , cM ]⊤ is the vector of weighting M individual models built on different training subsets , µ is a regularization parameter , the components of matrices F and Q have been defined by ( 10 ) and ( 11 ) , respectively . Since our ensemble objective function ( 12 ) keeps the same form as the objective ( 6 ) of one single kernel model , it is reasonable to set the parameter µ = λ . By now , we have successfully introduced a new type of regularization term for constructing an anti overfit ensemble model . We name the boosting technique with this new regularization method as “ kernelboosting ” .
In line with the Friedman ’s boosting framework , we propose our new general boosting framework for combining kernel models , which is described in Figure 2 . Note that , in stage 2(b ) , we have defined αj = [ αj D]⊤ , cj−1 = [ c1 , , cj−1]⊤ ,
1 , , αj where {˜xj , xij , j = 1 , , M} are selected basis vectors , KM is a N × M matrix of the kernel function k(·,· ) between all the training examples and selected basis vectors , ie , {(KM )ij = k(xi , ˜xj ) , i = 1 , , N ; j = 1 , , M} and KMM is the M × M kernel matrix of evaluating basis vectors on the kernel function , ie , {(KM,M )jj ′ = k(˜xj , ˜xj ′ ) , j , j ′ = 1 , , M} .
3 fj =  d , x1 ) d , x2 ) dk(˜xj dk(˜xj dk(˜xj d , xN )
Pd αj Pd αj Pd αj
 N ×1
,
( 13 )
3For brevity , we have assumed that all base learners have the same number of kernel terms or basis vectors .
1 . ¯f0(x ) = 0 and r0 = y ;
2 . For j = 1 : M do :
( a ) Generate the j th initial base learner f ∗ dk(˜xj the targets of Dj are replaced with the current residual rj−1 . j fj ) + µ j ( x ) =PD i=1 L(yi , Fj−1cj−1 + c∗
( b ) ( cj , aj ) = arg minc∗ d=1 αj d , x ) from the subset Dj . Note that j ) . 2 cj−1 j ⊤Qj−1 qj jcj−1 q⊤ j q∗ c∗ c∗ j ,αj(PN
( c ) Compute the optimized base learner fj(x ) =PD ( d ) ¯fj(x ) = ¯fj−1(x ) + cjfj(x ) and rj = rj−1 − cjfj . d=1 aj dk(˜xj d , x ) .
3 . EndFor
4 . ¯f ( x ) = ¯fM ( x ) =PM j=1 cjfj(x ) .
Figure 2 . A framework for boosting kernel models qj =  and dαj dαj d αj
PdPd′ α1 PdPd′ α2 PdPd′ αj−1 j =Xd Xd′ q∗ d′ k(˜x1 d′ k(˜x2 d , ˜xj d , ˜xj d′ ) d′ ) d′k(˜xj−1 d
, ˜xj d′ )
 (j−1)×1 dαj αj d′k(˜xj d , ˜xj d′ ) ,
( 14 )
( 15 ) j are all dependent on the and these three quantities fj , qj , q∗ parameters αj d , d = 1 , , D . The step 2(a ) aims to generate an initial base learner from a random training subset and the current residual . The key step 2(b ) can be efficiently solved by any gradient based optimization algorithm .
In the next section , we will demonstrate how to apply this framework to combine multiple KRR models in the context of regression , but the main procedure is applicable to boost other kernel models .
3 Boosting Kernel Ridge Regression
In this section , we apply the framework of boosting kernel models described in the last section to KRR models for regression tasks . We firstly review some training algorithms to KRR on a single dataset . Secondly we discuss how to efficiently implement the step 2(b ) in Figure 2 when boosting KRR models .
3.1 Solving single KRR
The problem of KRR is to minimize which is equivalent to solve the linear system of equations : ( K + λI)α = y , which requires O(N3 ) time and O(N2 ) memory . A feasible scheme to large data is to find a sparse estimate of α , ie , solving min αD
H(αD ) =
1 2ky − KDαDk2 +
λ 2
α⊤
DKDDαD ,
( 17 ) where we have changed the subscript M in ( 6 ) to D in order to highlight that M denotes the size of ensemble model and D is the size of each base learner . The optimal solution to sparse KRR ( 17 ) has a form of αD = ΣDK ⊤ Dy where DKD + λKDD)−1 . It is obvious that solving ΣD = ( K ⊤ such a sparse solution only cost O(ND2 ) time and O(ND ) memory . But the trouble is how to pick up a good subset of basis vectors , which is a combinatorial problem in theory and has a crucial effect on generalization performance .
It has been shown that forward selection algorithms [ 17 , 9 , 19 ] , which choose basis vector from all the remaining basis candidates one by one , could achieve good results in generalization while keeping the overall complexity to O(ND2 ) . The key step to forward approaches is the use of a criterion for deciding which training example should be chosen as basis vector at each iteration . A good criterion could achieve a good balance between computational cost and predictive performance . One path to achieve this is the criteria [ 9 ]
∆i =
1 2 with i ( y − KDαD ) − λ˜k⊤ [ k⊤ i αD]2
λki,i + k⊤ i ki
,
( 18 ) min
α
H(α ) =
1 2ky − Kαk2 +
λ 2
α⊤Kα ,
( 16 ) ki = [ k(x1 , xi ) , , k(xN , xi)]⊤ , ki,i = k(xi , xi ) , ˜ki = [ k(˜x1 , xi ) , , k(˜xD , xi)]⊤ ,
4 which measures the score of the training instance xi to be selected in the current iteration . This evaluating method requires O(N ) for evaluating one instance . At each iteration , if we compute this score for only κ candidates from all the remaining instances , then the complexity for basis selections is O(κND ) till D basis vectors are included . Typically , the value of κ is set to 59 [ 17 ] or the predefined number D of selected basis vectors [ 9 ] . The linear scaling in N makes this selection scheme viable for large problems .
Another criterion is info gain method [ 16 ] which is always inferior to the criteria ( 18 ) for generalization on large datasets [ 9 ] . But this method evaluates the score of adding one case just in O(1 ) time , which makes this algorithm almost as fast as using random selection . To reduce the computational cost to a great extent , our boosting procedure use this fast approach to initialize the base learner f ∗ j ( x ) ( see step 2(a ) in Figure 2 ) .
In the experimental section , the algorithm with the criteria ( 18 ) running on the whole training data will be compared empirically to our ensemble method both in computational time and generalization performance .
3.2 Details of boosting KRR
We are going to boost a set of regression models , so the loss function L(y , ¯f ) in our ensemble objective ( 12 ) will be replaced with the squared function . Correspondingly , the step 2(b ) can be further detailed as
¯H(c∗ j ,αj)= min j ,αjfl 1 c∗
2
+
2flflrj−1−c∗ j fjflfl j ⊤Qj−1 qj 2cj−1 q⊤ j c∗ q∗
µ jcj−1 j ) , c∗
( 19 ) where rj−1 = y − Fj−1cj−1 is the residual . Since the condition for optimality of c∗ j is j , αj )
∂ ¯H(c∗ ∂c∗ j we can get
= c∗ j ( µq∗ j +f ⊤ j fj)+[µq⊤ j cj−1−f ⊤ j rj−1 ] = 0 , c∗ j = j rj−1 − µq⊤ f ⊤ j + f ⊤ j fj
µq∗ j cj−1
.
( 20 )
After simplification of some notations , we can show that the problem ( 19 ) is equivalent to
¯H(αj)= ¯Hj−1−( 1
2 j cj−1]2 j rj−1−µq⊤ [ f ⊤ j +f ⊤ j fj
µq∗
) .
( 21 ) d , d = 1 , , D , can be easily where
˙fj =
∂fj ∂αj d
, ˙qj =
∂qj ∂αj d
, ˙q∗ j =
∂q∗ j ∂αj d
.
It can be noted that the optimization problem ( 21 ) involves a quadratic function wrt αj and thus the global optimum can be reached in at most D iterations by conjugate gradient ( CG ) method .
Another issue step 2(d ) is how to compute the optimal weight cj from old weight cj−1 and new entry cj . In our general framework for boosting kernel models , we simply use cj = [ cj−1⊤ , cj]⊤ with no changes for cj−1 . In fact , another better way is to update the weights of all the base learners included so far . In the context of boosting KRR models , this can be done very efficiently in O(NM2 ) by a recursive formula ( see Appendix for details ) . The final version of boosting KRR models can now be summarized as follows :
1 . Initialization :
( a ) size of training subset ( eg , 2000 ) ; ( b ) number D of selected basis vectors for each base learner ( eg , D = 200 ) ;
( c ) maximal number M of base learners to be boosted ( eg , M = 150 ) ;
( d ) iteration number of CG method ( eg , 50 ) ;
2 . ¯f0(x ) = 0 and r0 = y ;
3 . For j = 1 : M do
( a ) Generate a random subset Dj from training data and rj−1 , then produce the j th base learner by employing info gain method ;
( b ) Obtain the optimal solution aj to ( 21 ) by the CG method ;
( c ) Update cj−1 , rj−1 to cj , rj , respectively , based on ( 27 ) ;
( d ) Compute the optimized base learner fj(x ) = d=1 aj dk(˜xj d , x ) .
PD
4 . EndFor j=1 cM j fj(x ) .
5 . Output the ensemble model ¯f ( x ) =PM
The major computational cost incurred in our boosting approach is the step of optimizing the base learner ( eg the stage 3(b) ) , at each iteration . Note that evaluating the gradient information ˙fj , ˙qj , ˙q∗ j need O(ND ) time . So the overall complexity would be O(NDM ) time and O(ND ) memory . We set the size D of each base learner to 200 and the size M of ensemble model to 150 in this paper . Unlike single KRR model on the whole dataset , it costs O(ND2 ) time and O(ND ) memory but where D is significantly larger than 200 especially for large scale datasets .
The derivative of ( 21 ) wrt αj obtained , that is ∂ ¯H(∂αj )
=
1 2
αj d j [ 2( ˙f ⊤ c∗ j rj−1−µ ˙q⊤ j cj−1)−c∗ j ( µ ˙q∗ j +2f ⊤ j
˙fj ) ]
( 22 )
5
4 Numerical Experiments
In this section , we empirically demonstrate that the proposed boosting appoach for combining multiple KRR models could achieve better generalization performance and lower computational requirements simultaneously when compared to training a single KRR with ( 18 ) . The following data sets are used in the next evaluations :
1 . Outaouais : The data was used in the “ Evaluating Predictive Uncertainty Challenge ” but its background information is not available 4 .
2 . Kin40k : This dataset represents the forward dynamics of an 8 link all revolute robot arm , the task is to predict the distance of the end effector from a target , given the twist angles of the 8 links as features 5 .
3 . Sarcos : The task is related to learn the inverse dynamics of a seven degrees of freedom SARCOS anthropomorphic robot arm 6 .
4 . Census house : The task is to predict the median price of the house based on some certain demographic information 7 .
Some properties of these data sets are shown in Table 1 .
Table 1 . Properties of four datasets
Data set Outaouais Kin40k Sarcos Census house
#training #testing #dimension 20,000 36,000 44,484 20,000
9,000 4,000 4,449 2,784
37 8 21 138 error
1
Ntest Pi
To evaluate normalized mean square yi−f ( xi ) Var(y ) , where Ntest prediction performance , we utilize ( NMSE ) given by is the number of test examples and Var(y ) is the standard deviation of training targets . The algorithms presented in this section are coded in Matlab 7.0 and all the numerical experiments are conducted on a 2G Pentium 4 machine with 512M RAM , running Windows XP .
For the first three datasets , the squared exponential ker nel function was used [ 9 ] , k(xi , xj ; θ ) = θ0 exp −
1 2
L
Xl=1
θl(xi,l − xj,l)2! + θb ,
( 23 )
4http://predictkybtuebingenmpgde/datasets/ 5http://idafirstfraunhoferde/˜anton/data/ 6http://wwwgaussianprocessorg/gpml/data/ 7http://wwwcsusthk/˜jamesk/data/censuszip where θ0 , θl , θb > 0 are hyperparameters , L is the dimension and xi,l is the l th entry of the input xi . Since Gaussian process regression ( GPR ) model can be regarded as a Bayesian interpretation of KRR [ 12 ] , we can maximize the marginal likelihood of GPR on a random training subset ( eg , 1000 examples ) for optimal settings of the hyperparameters θ0 , θl , θb and regularization parameter λ . This task can be done by routines of the well known NETLAB software 8 . Since Census house is a high dimensional data , optimizing much more hyperparameters will take a very long time . So we simply use the Gaussian kernel exp(−kx − x′k2/θ ) , with θ = 1 i,j=1 kxi − xjk2 . The associated λ parameter is tuned on a small validation dataset .
N PN
For the proposed boosting ensemble approach , we fix the size of training subset on 2000 , the size of each learner D = 200 and the upper limit of ensemble model size M = 150 for all datasets considered . When implementing the forward selection algorithm with the criteria ( 18 ) on the whole data , we vary the maximal allowed number of selected basis vectors for different datasets due to the limit of memory size . Generally , the more basis vectors chosen the better generalization performance we could achieve . The parameter κ involved in the selection process is set to 59 in order to further save the storage space for training a single KRR model .
To remove the randomness involved in the algorithms , the results reported below are averaged on 10 independant runs . Figure 3 6 illustrate the results of test NMSE as a function of CPU time by learning a single model and boosted ensembles on four large scale datasets , respectively . It is clear that training multiple smaller KRR models by our proposed approach consistently achieve much better generalization performance than training a single KRR model if the same CPU time is gone . Moreover , we can note that the curves for single KRR models terminate quite earlier than ensemble models . This is because that the programs cannot proceed due to an out of memory error at this point . Table 2 shows the maximal allowed number of selected basis vectors for single KRR on four datasets . If considering that the setting of this number for each base learner in boosted ensembles is just 200 , we can understand why our ensembles has the advantage of requiring less memory than single large model .
We also summarize the best results of test NMSE obtained by single model and boosted ensembles under the parameter settings described above in Table 3 . For all the datasets considered , ensembles is significantly better than single KRR models on generalization performance , which is dicided by a p value threshold of 0.001 in the paired ttest . Table 4 reports comparisons of CPU time elapsed when
8It is available at http://wwwncrgastonacuk/netlab/ indexphp
6
Table 2 . The maximal allowed number of selected basis vectors for single KRR on four datasets
Data set Outaouais Kin40k Sarcos Census house
# selected basis
1,160 680 560 1,040
0.25
0.2
0.15
0.1
0.05
E S M N t s e T
Single model Boosted ensembles
Single model Boosted ensembles
0
0
200
400
600 800 Time ( seconds )
1000
1200
1400
Figure 4 . NMSE comparisons of single model and ensembles for the Kin40k dataset as a function of CPU time .
0.7
0.6
0.5
0.4
0.3
0.2
0.1
E S M N t s e T
0
0
500
1000
1500
Time ( seconds )
2000
2500
Figure 3 . NMSE comparisons of single model and ensembles for the Outaouais dataset as a function of CPU time . single model and ensembles reach the same predictive accuracy . It can be seen that boosted ensembles is at least two times faster than training single KRR model . The number in parentheses reflects the number of added base learners in ensembles .
Table 3 . Comparisons of best NMSE obtained by single model and ensembles .
Data set Outaouais Kin40k Sarcos Census house single KRR 00686±00013 00280±00006 00181±00001 00751±00003 boosted ensembles 00463±00005 00176±00002 00155±00001 00738±00001
5 Conclusions
We presented a general boosting framework to combine multiple kernel models where each one is initialized from an individual training subset . The proposed framework
E S M N t s e T
0.09
0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0
Single model Boosted ensembles
200
400
600
800
1000
1200
1400
1600
1800
Time ( seconds )
Figure 5 . NMSE comparisons of single model and ensembles for the Sarcos dataset as a function of CPU time . was employed to boost multiple KRR models for regression tasks . The resulting ensembles is empirically compared to the state of the art algorithm for learning a single KRR model on three large scale regression problems . The results support that we still benefit a lot from ensembling multiple kernel models at least for regression even though the base learners are stable .
In the near future , we will investigate the performance of applying the proposed boosting framework to large scale classification problems . Another interesting direction is to extend our work to Multiple Kernel Learning ( MKL ) [ 18 ] which aims to address the issue of model selection and heterogeneous property involved .
7
0.24
0.22
0.2
0.18
E S M N t s e T
0.16
0.14
0.12
0.1
0.08
0.06
0
Single model Boosted ensembles
500
1000
1500
Time ( seconds )
2000
2500
3000
Figure 6 . NMSE comparisons of single model and ensembles for the Census house dataset as a function of CPU time .
Table 4 . Comparisons of CPU time when single model and ensembles reach the same accuracy .
Data set Outaouais Kin40k Sarcos Census house single KRR boosted ensembles
1540 983 1144 2083
820 ( 40 ) 264 ( 30 ) 485 ( 45 ) 672 ( 30 )
Appendix
1 . The gradients of fj , qj , q∗ j wrt αj d , d = 1 , , D can which can be evaluated in O(ND ) complexity .
2 . In order to update cj−1 , rj−1 to cj , rj , we define the following notations . Let Lj be the factor of Cholesky and factorisation : LjL⊤ j = Qj , let Gj = FjL−⊤ j be detailed as
˙fj =
∂fj ∂αj d
=  Pd′ α1 Pd′ α2 Pd′ αj−1 = 2Xd′
˙qj =
∂qj ∂αj d
˙q∗ j =
= 
∂q∗ j ∂αj d k(˜xj d , x1 ) k(˜xj d , x2 ) d , xN ) k(˜xj
  d′k(˜x1 d′k(˜x2 d′ , ˜xj d ) d′ , ˜xj d ) d′ k(˜xj−1 d′
, ˜xj d ) d′ k(xj αj d′ , xj d ) ,
( 24 )
( 25 )
( 26 )
 
Mj be the factor of another Cholesky decomposition : MjM ⊤ j Gj + λIdj ) , we have j = ( G⊤ cj = L−⊤ j
( MjM ⊤ j )−1G⊤ j y and rj = y − Fjcj .
The involved recursive steps can be summarized : lj = L−1 j−1qj , l∗ j =qq∗ j − l⊤ j lj , gj = fj − Gj−1lj l∗ j
, mj = M −1 j−1(G⊤ j−1gj ) , η = M −⊤ j−1mj , dj = gj−Gj−1η , b = d⊤ j y , c = d⊤ j gj , m∗ j = √λ + c , cj = b l∗ j ( λ + c )
, cj =cj−1 − cj[L−⊤ j−1(lj + l∗ cj j η ) ]
, rj = rj−1−
, bdj λ + c ( 27 ) and finally
Lj =Lj−1 0 l⊤ j j , Mj =Mj−1 0 j m∗ m⊤ j , Gj = [ Gj−1 gj ] . l∗
Since the matrices Lj and Mj are low triangular , the multiplication of their inverse and a vector can be computed very efficiently .
References
[ 1 ] C . Burges . A tutorial on support vector machines for pattern recognition . Data Mining and Knowledge Discovery , 2(2):121–167 , 1998 .
[ 2 ] V . V . C . Saunders , A . Gammermann . Ridge regression learning algorithm in dual variables . In J . Shavlik , editor , ICML1998 , San Francisco , CA , 1998 . Morgan Kaufmann .
[ 3 ] A . Choudhury , P . B . Nair , and A . J . Keane . A data parallel approach for large scale gaussian process modeling . In Proceedings of SDM 2002 , 2002 .
[ 4 ] R . Collobert , S . Bengio , and Y . Bengio . A parallel mixture of svms for very large scale problems . Neural Computation , 14(5 ) , 2002 .
[ 5 ] T . Evgeniou , M . Pontil , and T . Poggio . Regularization networks and support vector machines . In Advances in Large Margin Classifiers , 2000 .
[ 6 ] J . H . Friedman . Greedy function approximation : A gradient boosting machine . Annals of Statistics , 29(5):1189–1232 , 2001 .
[ 7 ] J . H . Friedman , T . Hastie , and R . Tibshirani . Additive logistic regression : a statistical view of boosting . Annals of Statistics , 28(2):337–407 , 2000 .
[ 8 ] H . P . Graf , E . Cosatto , L . Bottou , I . Dourdanovic , and V . Vapnik . Parallel support vector machines : The cascade svm . In NIPS 17 . MIT Press , 2005 .
[ 9 ] S . S . Keerthi and W . Chu . A matching pursuit approach to sparse Gaussian process regression . In NIPS 18 . MIT Press , 2006 .
8
[ 10 ] S . Mika . Kernel Fisher Discriminants . PhD thesis , Univer sity of Technology , Berlin , Germany , October 2002 .
[ 11 ] D . Pavlov , J . Mao , and B . Dom . Scaling up support vector machines using boosting algorithm . In Proceedings of ICPR 2000 , 2000 .
[ 12 ] C . E . Rasmussen and C . K . I . Williams . Gaussian Processes for Machine Learning . The MIT Press , 2006 .
[ 13 ] B . Scholkopf , R . Herbrich , and A . J . Smola . A generalized representer theorem . In Proceedings of the 14th Annual Conference on Computational Learning Theory , pages 416– 426 , 2001 .
[ 14 ] B . Scholkopf , A . Smola , and K R Muller . Nonlinear component analysis as a kernel eigenvalue problem . Neural Computation , 10:1299–1319 , 1998 .
[ 15 ] B . Scholkopf and A . J . Smola . Learning with Kernels : Support Vector Machines , Regularization , Optimization , and Beyond . The MIT Press , 2002 .
[ 16 ] M . Seeger , C . K . I . Williams , and N . D . Lawrence . Fast forward selection to speed up sparse gaussian process regression . In Ninth International Workshop on Artificial Intelligence and Statistics , Key West , Florida , 2003 .
[ 17 ] A . J . Smola and B . Sch¨okopf . Sparse greedy matrix approximation for machine learning . In ICML 2000 , pages 911– 918 , 2000 .
[ 18 ] S . Sonnenburg , G . Ratsch , and C . Schafer . A general and efficient multiple kernel learning algorithm . In NIPS 18 . MIT Press , 2006 .
[ 19 ] P . Sun and X . Yao . Greedy forward selection algorithms to sparse Gaussian Process Regression . In Proceedings of IJCNN 2006 , 2006 . to appear .
[ 20 ] V . Tresp . A bayesian committee machine . Neural Compu tation , 12:2719–2741 , 2000 .
[ 21 ] V . Tresp .
Scaling kernel based systems to large data sets . Data Mining and Knowledge Discovery , 5(3):197–211 , 2001 .
[ 22 ] M . Yamana , H . Nakahara , M . Pontil , and S . Amari . On different ensembles of kernel machines . In Proceedings of ESANN 2003 , 2003 .
[ 23 ] J . Zhu and T . Hastie . Kernel logistic regression and the import vector machine . Journal of Computational and Graphical Statistics , 14(1):185–205 , 2005 .
9
