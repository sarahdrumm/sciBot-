A Simple Yet Effective Data Clustering Algorithm∗
Soujanya Vadapalli
Satyanarayana R Valluri
Kamalakar Karlapalem
Center for Data Engineering , IIIT , Hyderabad , INDIA
{soujanya , satya , kamal}@iiitacin
Abstract
In this paper , we use a simple concept based on kreverse nearest neighbor digraphs , to develop a framework RECORD for clustering and outlier detection . We developed three algorithms ( i ) RECORD algorithm ( requires one parameter ) , ( ii ) Agglomerative RECORD algorithm ( no parameters required ) and ( iii ) Stability based RECORD algorithm ( no parameters required ) . Our experimental results with published datasets , synthetic and real life datasets show that RECORD not only handles noisy data , but also identifies the relevant clusters . Our results are as good as ( if not better than ) the results got from other algorithms .
1 . Introduction
Clustering partitions a dataset into highly dissimilar groups of similar points [ 5 ] . The problem of outlier detection is closely related to clustering and deals with identifying anomalous observations in the data . Earliest clustering and outlier detection algorithms have been devised based on objective criteria and prior assumptions on data distributions . Recent approaches are developed based on concepts like density , k nearest neighbor ( kNN ) graphs and shared nearest neighbors .
The definition of clusters and outliers depends very much on the domain of the dataset . But for sake of clarity , general definitions quoted in the literature are given below : A cluster is a set of similar points that are highly dissimilar with other points in the dataset . An outlier or a noise point is an observation which appears to be inconsistent with the remainder of the data [ 1 ] . Aggarwal and Yu [ 1 ] note that outliers may be considered as noise points lying outside a set of defined clusters .
The problem of identifying clusters becomes more challenging when : ( i ) there is noise , ( ii ) clusters are arbitrary shaped and ( iii ) clusters have different densities . Generally , existing clustering techniques work well in the absence of noise . When there is noise in the dataset , the clusters
∗
This work was made possible by the grant from The Boeing Company . identified by these techniques include the surrounding noise points too . Presence of noise disrupts the process of clustering ; noise needs to be separated from the dataset to enhance the quality of the clustering results . Density based techniques address the issue of noise well but require user given thresholds to identify the noise points .
In this paper , a simple yet effective solution for clustering and outlier detection has been proposed : RECORD REverse nearest neighbor based Clustering and OutlieR Detection . The algorithms proposed in this paper , first segregate noise points that might disrupt clustering . Clusters are identified based on reverse nearest neighbors and their mutual reachability with other points in the dataset . Our technique is based on this simple concept but its results are as good as ( and in some cases better than ) the results of other algorithms such as CURE , DBSCAN , CHAMELEON and SNN . Therefore , any application that finds results of these algorithms satisfactory can as well use our algorithm . Our algorithm does not require any user input parameters . Some of the results displayed in this paper show the effectiveness of our algorithms on various datasets . Though the notion of reverse nearest neighbors has been around and has been used for various applications , the very specific concept of ‘utilizing the number of reverse nearest neighbors for a data point’ for clustering and outlier detection has not been studied to the best of our knowledge . More details and results are given in the extended version of the paper [ 10 ] .
11 Related Work
Density based techniques like DBSCAN [ 4 ] , OPTICS [ 2 ] consider the density around each point to demarcate boundaries and identify the core cluster points . The close cluster points in a single neighborhood are then merged . OPTICS does not generate a clustering solution ; instead generates an augmented ordering of the points . But to understand these plots , the user should have a minimum knowledge of the algorithm , which precludes its usage by novice users . Finding the appropriate parameters for DBSCAN and identifying cluster boundaries in OPTICS are challenges to the user .
1
Proceedings of the Sixth International Conference on Data Mining ( ICDM'06)0 7695 2701 9/06 $20.00 © 2006 Graph based approaches : To address the problem of clustering , k nearest neighbors ( kNNs ) are used to identify k most similar points around each point and by way of conditional merging , clusters are generated . There are various variants of kNN clustering and they differ at the conditional merging part of the solution . For a given point p , its kNNs If the distance between p and any of the are found out . points in kNN(p ) set ( say q ) is less than , then point q is merged into the cluster of p . This algorithm requires tuning of and k values to get clusters .
Tarjan [ 9 ] proposed a method to construct strongly connected components from a given directed graph where each edge is associated with a weight ( usually distance between points ) . This method is highly sensitive to the presence of noise and cannot handle clusters of different densities .
CHAMELEON [ 7 ] involves two phases : identifying small tight clusters built from k nearest neighbor graph and merging these clusters using measures of inter connectivity and relative closeness . CHAMELEON can find clusters of different shapes and sizes , but it requires the parameters for relative inter connectivity and relative closeness to be finetuned .
The Shared Nearest Neighbor approach [ 3 ] ( first proposed in [ 6] ) , defines a similarity measure between a pair of points based on the number of nearest neighbors they share . Using this similarity measure , a notion of density is defined based on the sum of the similarities of neighbors of a point . Points with high density and similarity are formed into clusters and points with low density are eliminated as noise . This method requires three parameters : k ( to build kNN graphs ) , link strength threshold ( to identify dense and outlier points ) and link weight threshold ( to remove edges with have less weights ) .
12 Our Contributions
We developed algorithms for both cluster identification and outlier detection . The concept of k reverse nearest neighbors and strongly connected components are used to ( i ) derive clearly distinguishable clusters and ( ii ) identify and remove outliers during the process of clustering . We provided agglomerative and stability based clustering algorithms based on the k reverse nearest neighbors framework and evaluated these algorithms on various published datasets ( synthetic and real life datasets ) .
2 . RECORD Framework
The problem of finding reverse nearest neighbors has received attention in the last few years due to its relevance in applications involving decision support , resource allocation and profile based marketing . The RNN problem has been introduced in the database setting by Korn and Muthukrishnan [ 8 ] . In this paper , we demonstrate the application of RNNs to address the problem of clustering and outlier detection .
21 Notation and Definitions
The following is the notation used in this paper . X : d dimensional dataset , n : size of dataset C : set of clusters , c : number of clusters O : set of outlier points xi , xj : i th , j th points in X p , q : any two points in X dij : distance between two points ( xi , xj ) kNN(p ) : k nearest neighbor set of point p kRNN(p ) : k reverse nearest neighbor set of p . A point q belongs to kRNN(p ) iff p ∈ kNN(q ) . kRNNG : k reverse nearest neighbor graph kRNN≥kG : sub graph of kRNNG having only core points ( defined below ) kRNN<kG : sub graph of kRNNG having only outlier points ( defined below ) SCC : Strongly connected component Ci , Cj : i th cluster , j th cluster SCC<k : Set of SCCs obtained from kRNN<kG SCC≥k : Set of SCCs obtained from kRNN≥kG The brief problem statement is : Given X , ( |X|=n ) having c clusters and O ⊂ X , the set of outlier points , the clustering algorithm must find the c clusters and all outlier points in O . k nearest neighbor set kNN(xj ) is defined as {xi|dji < kth nearest distance of xj} . For a given point xj , after sorting all the distances from xj to the remaining points in X , the k th smallest distance is the k th nearest distance of xj . k reverse nearest neighbor set kRNN(xi ) is defined as kRNN(xi ) = {xj|xj ∈ X xi ∈ kNN(xj)} , the set of all points xj that consider xi as their k nearest neighbor .
22 Properties of RNNs
Lemma 1 : The kRNN graph of a dataset is the transpose of kNN graph and vice versa .
Following are properties of kRNNs and the kRNN graph : 1 . If q ∈ kRNN(p ) , then p need not ∈ kRNN(q ) . 2 . Each point in the dataset could have varying number of elements in its kRNN set , including zero .
Note that while in the case of kNNs , for a given k value , each point in the dataset will have at least k nearest neighbors ( > k NNs in case of ties ) . As per point 2 above , kRNN set of a point could have zero or more elements . The concept of kRNNs is used to capture the neighborhood of a point , as the kRNNs define the influence around a point .
2
Proceedings of the Sixth International Conference on Data Mining ( ICDM'06)0 7695 2701 9/06 $20.00 © 2006 The kRNN set of point p gives the set of points that consider p as their k nearest , for a given value of k . If the point p has higher number of kRNNs than another point q , then p has a denser neighborhood than q . Lesser the number of kRNNs , the farther apart are the points in the dataset to p . The value of k determines the neighborhood cover around a point .
Also , the rate at which the cardinality of kRNN set increases as k tends to n is not the same for all points in the dataset . At a given k , a point x need not have any additional kRNNs , but at k+1 it could have added one or more points in its ( k+1)RNN set than kRNNs . If the cardinality of the kRNNs set for all values of k from 1 to n is plotted , the curve would have some parts where the number of kRNNs remain the same and other parts which have upward slopes indicating the increase in the number of kRNNs .
Based on the above observations , we analyzed the kRNN plots for various types of points ( a ) core point ( point amidst dense set of points ) , ( b ) outlier point and ( c ) boundary point ( point that usually lies on the boundary of the cluster ) . For each of these points , their respective kRNN plots are analyzed and based on the analysis , the definitions for core point , outlier point and boundary point are given below . Detailed analysis and kRNN plots are given in [ 10 ] . kRNN points : |kRNNs| ≥ k . An outlier point is a point that has less than k number of kRNNs : |kRNNs| < k . Lesser the number of kRNNs , the more distant it is from its neighbors .
A core point is defined as a point that has at least k
A boundary point is an outlier , classified to be boundary point , if majority of its kRNNs belong to one cluster .
Boundary point vs . outlier point : A boundary point is one that is usually near the boundaries of a cluster or amidst more than one cluster . Based on our experiments on many datasets [ 11 , 7 ] , we found that an outlier point has |kRNNs| < k and most of its kRNNs are outliers . But for a boundary point , its |kRNNs| < k and most of its kRNNs are core points . In this paper , for all practical purposes , if majority of kRNNs of a point are outliers , the point is an outlier ; otherwise , it is a boundary point .
The three algorithms for data clustering : RECORD algorithm , Agglomerative RECORD algorithm and Stabilitybased RECORD algorithm , are described in the following subsections .
23 RECORD Algorithm
Input : Dataset X , k ( number of RNNs required per point ) ; Output : Clusters C , Outliers O ; The RECORD algorithm involves the following steps : 1 . Initialization : Create an array of integers rnncount to store the number of kRNNs for each point . Create a kRNN Graph and initialize it to φ . 2 . kRNN Computation : Generate the distance matrix based on the distance function dij . Calculate the kRNN sets of each point as follows : ( i ) For each point p , identify the k nearest points kNN(p ) . For every point q ∈ kNN(p ) , increment rnncount(q ) by 1 . ( ii ) Add the directed edge ( q ∈ kNN(p ) , p ) to the kRNN graph . 3 . Outlier detection : For each point p , the number of points in kRNN(p ) is checked . If it is less than k , the point is identified as outlier . The point p and all out going edges from p are removed from the kRNNG graph . The experimental results show that this definition of outliers eliminates most of the noise and outlier points . After removing all outlier points from the graph , the modified graph is represented as kRNNG≥k . The complement of this graph is kRNNG kRNNG≥k = kRNNG<k and this graph includes only outlier points and their corresponding out going edges . Cluster Identification : Eliminating outliers from 4 . kRNNG results in kRNNG≥k graph . Every point in kRNNG≥k will have at least k outgoing edges . The clusters are now computed based on the kRNNG≥k .
A Strongly Connected Component(SCC ) of a digraph partitions the vertices into subsets wherein all points in a subset are mutually reachable .
Each SCC in the kRNNG≥k graph becomes a cluster as each SCC follows two rules : ( i ) each member of the SCC is accepted as k nearest by at least k points and ( ii ) every possible pair of members in the SCC are mutually reachable depicting the cohesiveness of the graph .
SCCs are also computed on kRNNG<k ( sub graph kRNNG containing only outliers ) . The SCCs obtained from this graph are clusters of noise points and are very sparse . The SCCs so obtained are used later on to identify stable clustering solution ( discussed in section 26 ) 5 . Local outlier incorporation The outlier detection technique discussed above could be stringent . Along with the actual outliers , some of the boundary cluster points and some internal cluster points are also identified as outliers . Due to this , the clusters generated are highly dense and incomplete . To avoid such elimination , the outlier points are classified into their nearest cluster . Nearest cluster is the closest majority cluster among kRNN points of each outlier . The idea is to classify an outlier point to a cluster if at least k/d of its kRNNs belong to one particular cluster . Otherwise , the point is retained as outlier . The fraction k/d is used to factor in the sparsity of data points that arises in case of high dimensionality . This fraction has worked well for the empirical studies conducted on various datasets .
The total complexity of the algorithm is O(n2 ) , if the similarity matrix also is to be computed . If the matrix is already given , then the complexity is O(nk log n+n)+Θ(n+ e ) . k is the neighborhood cover parameter and e is the num
3
Proceedings of the Sixth International Conference on Data Mining ( ICDM'06)0 7695 2701 9/06 $20.00 © 2006 ber of edges in the kRNNG≥k ( kRNNG after eliminating outlier points and their corresponding edges ) .
24 Agglomerative RECORD Algorithm ( ARA )
In this algorithm , the clusters are generated for only selected values of k , not for all k=1 to n . The algorithm starts with k = 1 and the next value of k is chosen such that there is a merge between two clusters ( if possible ) . The algorithm is as follows : For k=1 , generate the graph and clusters , C = {C1 , C2 , . . . , Cl} using kRNN clustering algorithm . If the number of clusters is 1 , then the agglomerative algorithm is terminated . Otherwise , a two dimensional matrix jump of size | C | × | C | is created . Each cell jump[Ci][Cj ] , ∀Ci , Cj ∈ C , i = j maintains the minimum value of k at which the clusters Ci and Cj will merge . This is computed as the maximum of the two values , kij and kji . kij(kji ) is the value of k for which there exists a point p ∈ Cj ( ∈ Ci ) and a point q ∈ Ci ( ∈ Cj ) , such that p belongs to kijRNN(q)(kjiRNN(q) ) . The jumps are calculated for every pair of clusters . The next value of k is set to min(jump[Ci][Cj]),∀i , j . The clusters are calculated again using the new value of k and the process is repeated till only one cluster is obtained .
For most practical purposes with dense and well separated clusters , this algorithm computes very few levels of hierarchical results . The complexity is closer to O(cn2 ) , c number of clusters in the dataset , rather than O(n3 ) .
25 Stability based RECORD Algorithm ( SRA )
Stability based RECORD Algorithm generates the most stable clustering result as its output . The kRNNG is split into two subgraphs kRNNG<k and kRNNG≥k . SCCs are computed for both the subgraphs . Now as k increases , |SCC<k| and |SCC≥k| are checked . After some value of k these numbers do not change . This implies no new clusters are formed and no variation in noise behavior is found . For all the empirical results , when the number of SCCs ( both kRNNG≥k and kRNNG<k ) remained constant for at least three consecutive values of k , the clustering results for the corresponding k values gave good results . This also helps finding the right value of k . If required , the user can vary k around this stabilized k to evaluate the clustering solutions . This algorithm does not need any parameters .
Figure 1 shows the stability plot for CHAMELEON dataset . The number of clusters obtained from kRNNG≥k ( green plot ) and kRNNG<k ( blue plot ) graphs are observed from k to k + l ( for some l > 0 and l ≤ 3 ) , that is , for some consecutive values of k . The first occurrence of stable clustering result is at k=59 with the number of clusters at 6 . The clustering result is shown in Figure 2 .
Figure 1 . Stability plots for CHAMELEON dataset
Figure 2 . SRA Result dataset for CHAMELEON
3 . Conclusions
In this paper , we proposed effective clustering algorithms based on reverse nearest neighbors . RECORD ( REverse nearest neighbor based Clustering and OutlieR Detection ) applies outlier detection , followed by either agglomerative RECORD algorithm or stability based RECORD algorithm with local outlier incorporation . RECORD , by using reverse nearest neighbors , captures density aspects and distinguishes between core points , boundary points and outlier points without any user input . This concept has been shown to be powerful in finding clusters and detecting outliers . More importantly , we do not need any parameter for the RECORD algorithms ( ARA , SRA ) . Agglomerative RECORD algorithm gives multiple clustering solu
4
Proceedings of the Sixth International Conference on Data Mining ( ICDM'06)0 7695 2701 9/06 $20.00 © 2006 DBSCAN
Chameleon Results
SNN Results
ARA Results
Figure 3 . Comparison of Agglomerative RECORD algorithm with DBSCAN , Chameleon and Shared Nearest Neighbor approach
[ 5 ] A . K . Jain and R . C . Dubes . Algorithms for clustering data .
Prentice Hall , Englewood Cliffs , NJ , 1988 .
[ 6 ] R . A . Jarvis and E . A . Patrick . Clustering using a similarity mesure based on shared nearest neighbors . IEEE Transactions on Computers , C 22(11 ) , 1973 .
[ 7 ] G . Karypis , E . H . Han , and V . Kumar . Chameleon : Hierarchical clustering using dynamic modeling . IEEE Computer 32(8 ) , 1999 .
[ 8 ] F . Korn and S . Muthukrishnan . Influence sets based on re verse nearest neighbor queries . SIGMOD , 2000 .
[ 9 ] R . E . Tarjan . An improved algorithm for hierarchical clustering using strong components . Information Processing Letters 17 , 1983 .
[ 10 ] S . Vadapalli , S . Valluri , and K . Karlapalem . Record : A simple yet effective clustering algorithm using reverse nearest neighbors . Technical Report , International Institute of Information Technology Hyderabad , 2006 .
[ 11 ] J . R . Vennam and S . Vadapalli . Syndeca : A tool to generate synthetic datasets for evaluation of clustering algorithms . COMAD , 2005 . tions of dataset for exploratory data mining . Stability based RECORD algorithm gives one clustering solution of dataset for quick results .
Our experimental results on published datasets ( see Figure 3 ) , synthetic data sets ( SYNDECA ) and real life data set like Intrusion Detection have shown convincing results on its validity and efficiency . Our experimental studies show that this simple concept caters to algorithms that work as well if not better than more complicated algorithms proposed in literature [ 3 , 4 ] . More details are given in [ 10 ] .
References
[ 1 ] C . C . Aggarwal and P . S . Yu . Outlier detection for high dimensional data . SIGMOD , 2001 .
[ 2 ] M . Ankerst , M . M . Breunig , H . P . Kriegel , and J . Sander . Optics : Ordering points to identify the clustering structure . SIGMOD , 1999 .
[ 3 ] L . Ertoz , M . Steinbach , and V . Kumar . Finding clusters of different sizes , shapes , and densities in noisy , high dimensional data . SDM , 2003 .
[ 4 ] M . Ester , H . P . Kriegel , J . Sander , and X . Xu . A densitybased algorithm for discovering clusters in large spatial databases with noise . KDD , 1996 .
5
Proceedings of the Sixth International Conference on Data Mining ( ICDM'06)0 7695 2701 9/06 $20.00 © 2006
