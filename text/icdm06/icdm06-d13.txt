The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering
Tao Li
Chris Ding
School of Computer Science
Florida International University
Lawrence Berkeley Nat’l Lab
University of California
Miami , FL 33199 taoli@csfiuedu
Berkeley , CA 94720 chqding@lbl.gov
Abstract
The nonnegative matrix factorization ( NMF ) has been shown recently to be useful for clustering . Various extensions of NMF have also been proposed . In this paper we present an overview and theoretically analyze the relationships among them . In addition , we clarify previously unaddressed issues , such as NMF normalization , cluster posterior probabilty , and NMF algoritm convergence rate . Experiments are also conducted to empirically evaluate and compare various factorization methods .
Keywords : matrix factorization , simultaneous cluster ing , NMF normalization , NMF convergence rate
1 . Introduction
The nonnegative matrix factorization ( NMF ) has been shown recently to be useful for many applications in environment , pattern recognition , multimedia , text mining , and DNA gene expressions [ 9 , 26 , 29 , 32 ] . NMF can be traced back to 1970s ( Notes from G . Golub ) and is studied extensively by Paatero [ 29 ] . The work of Lee and Seung [ 24 , 25 ] brought much attention to NMF in machine learning and data mining fields . A very recent theoretical analysis [ 12 ] shows the equivalence of NMF and spectral clustering and K means clustering . Various extensions and variations of NMF have been proposed recently [ 13 , 14 , 15 , 23 , 27 , 3 , 30 , 33 ] .
Despite significant research progress in this area , few attempts have been made to establish the connections between various factorization methods while highlighting their differences . In this paper , we aim to provide a comparative study on matrix factorization for clustering . We present an overview and summary on various matrix factorization algorithms and theoretically analyze the relationships among them . Experiments were also conducted to empirically evaluate and compare various factorization methods . In particu lar , our study tries to address the following important questions for matrix factorizations :
• What are the available forms of matrix factorizations for clustering ?
• What are the relations among the matrix factorizations as well as the existing clustering methods ?
• How to interpret the cluster posterior obtained from matrix factorizations ?
• What are the benefits of simultaneous clustering ?
• How to evaluate simultaneous clustering ?
• How to choose different factorization methods ?
We expect our study would provide insightful guidance on matrix factorization research for clustering . The rest of the paper is organized as follows : Section 2 summarizes various matrix factorizations ; Section 3 illustrates the differences among various factorizations using examples ; Section 4 introduces the computation algorithms for various factorization methods ; Section 5 studies the relationships of various matrix factorization methods ; Section 6 discusses the normalization method for eliminating the uncertainty in NMF solutions ; Section 7 explains when and why the simultaneous clustering is preferred and presents strategies for evaluating simultaneous clustering ; Section 8 shows the experimental results for empirically comparing various matrix factorization methods ; and finally Section 9 concludes .
2 . Different Matrix Factorizations
In general , matrix factorization algorithms attempt to find the subspace in which the majority of the data points lie .
Let the input data matrix X = ( x1 , · · · ,xn ) contain the collection of n data column vectors . Generally , we factorize X into two matrices ,
X ≈ FGT ,
( 1 ) where X ∈ Rp×n , F ∈ Rp×k and G ∈ Rn×k . Generally , the rank of matrices F,G is much lower than the rank of X ( ie , k ≪ min(p,n) ) . Here we provide an overview on related matrix factorization methods :
1 . SVD : The classic matrix factorization is Principal Component Analysis ( PCA ) which uses the singular value decomposition [ ? , 17 ] , X ≈ US V T , where we allow U,V to have mixed signs ; the input data could have mixed signs . absorbing S into U , we can write
5 . Tri Factorization : To simultaneously cluster the rows and the columns of the input data matrix X , we consider the following nonnegative 3 factor decomposition [ 15 ]
X ≈ FSGT .
( 2 )
Note that S provides additional degrees of freedom such that the low rank matrix representation remains accurate while F gives row clusters and G gives column clusters . More precisely , we solve
SVD : X± ≈ U±V±
2 . NMF : When the input data is nonnegative , and we restrict F and G to be nonnegative . The standard NMF can be written as
NMF : X+ ≈ F+G+ using an intuitive notation for X,F,G ≥ 0 .
3 . Semi NMF : When the input data has mixed signs , we can restrict G to be nonnegative while placing no restriction on the signs of F . This is called semiNMF [ 13 ] : semi NMF : X± ≈ F±G+ .
Semi NMF can be motivated by K means clustering . Let F = ( f1 , · · · ,fk ) be the cluster centroids obtained via K means clustering . Let G be the cluster indicators : ie , gki = 1 if xi belongs to cluster ck ; gki = 0 otherwise . The K means clustering objective can be written as
JK−means = n(cid:229 )
K(cid:229 ) i=1 k=1 gikkxi − fkk2 = kX − FGT k2 , where k · k is Frobenius norm1 . Semi NMF can be thought as a soft clustering by relaxing the element of g from binary to continuous nonnegative values .
4 . Convex NMF : In general , the basis vectors
F = ( f1 , · · · ,fk ) can be anything in a large space , in particular , a space that contains the space spanned by the columns of X = ( x1 , · · · ,xn ) . In order for the vectors F to capture the notion of cluster centroids , we restrict them to lie within the space spanned by the columns of X , ie , fl = w1lx1 + · · · + wnlxn = Xwl,or F = XW .
Furthermore , we restrict fl as a convex combination wil ≥ 0 of the data points . We call this restricted form of factorization as Convex NMF . Convex NMF applies to both nonnegative and mixed sign input data .
1Without specifying , all norms in this paper are Frobenius norm . min
F≥0,G≥0,S≥0 kX − FSGTk2 , st F T F = I , GT G = I . ( 3 ) This form gives a good framework for simultaneously clustering the rows and columns of X [ 35 ] .
An important special case is that the input X contains a matrix of pairwise similarities : X = X T = W . In this case , F = G = H . We optimize the symmetric NMF : min
W ≥0,S≥0 kX − HSHT k2 , st HT H = I .
6 . Kernel NMF : Consider a mapping xi → f ( xi ) , or X → f ( X ) = ( f ( x1 ) , · · · ,f ( xn) ) .
A standard NMF or Semi NMF like f ( X ) ≈ FGT would be difficult since F,G will depends explicitly on the mapping function f ( · ) . However , Convex NMF provides a nice possibility : f ( X ) ≈ f ( X)W GT .
It is easy to see that the minimization objective ||f ( X ) − f ( X)W GT ||2 = Tr[f ( X)T f ( X ) − 2GT f T ( X)f ( X)W
+W T f T ( X)f ( X)W GT G ] depends only on the kernel K = f T ( X)f ( X ) . This kernel extension of NMF is similar to kernel PCA and kernel K means .
In summary , various factorizations differ by the restrictions on the matrix forms and signs , we write them collectively as follows :
SVD : NMF : Semi NMF : Convex NMF : Kernel NMF : Tri Factorization : Symmetric NMF :
X± ≈ U±V± X+ ≈ F+GT + X± ≈ F±GT + X± ≈ X±W+GT + f ( X± ) ≈ f ( X±)W+GT X+ ≈ F+S+GT + W+ ≈ H+S+HT +
+
3 . Illustration Examples
3.2 A Mixed sign Example
In this section , we use examples to illustrate the differ ence among various NMF methods .
3.1 A Nonnegative Example
In the section , we use a nonnegative example to illustrate the differences in NMF and Tri Factorization .
The input data matrix X is


0.185 0.326 0.761 2.799 2.375 2.970 2.585 0.508 0.380 0.884 2.134 2.374 2.342 2.524 0.452 0.887 0.457 2.065 2.484 2.253 2.163 1.486 1.843 1.858 0.566 0.103 0.417 0.269 1.496 1.806 1.610 0.612 0.158 0.560 0.784


One can see that the first 3 columns should be one cluster and the last 4 columns should be another cluster .
The computed basis vectors F for NMF and Tri
Factorization are :
Fnm f =
0.0403 0.0889 0.1033 0.3882 0.3794 16.83


0.3695 0.3149 0.2945 0.0002 0.0210 30.64


, FTri =
0.0000 0.0215 0.0320 0.4773 0.4692 2.6172


0.3704 0.3228 0.3068 0.0000 0.0000 3.4036


Basis vectors are normalized to 1 in L2 norm ( norms are given at the bottom line ) for comparison purpose . We can see that F leads to the correct row clustering results : the first three row are in one cluster and the bottom 2 rows are in another cluster .
The matrices G for NMF and Tri Factorization are listed as follows :
Gnm f =
0.234 0.287 0.259 0.080 0.012 0.063 0.065 0.006 0.014 0.040 0.223 0.238 0.244 0.236
GTri =
0.270 0.335 0.333 0.034 0.000 0.009 0.020 0.000 0.000 0.000 0.239 0.248 0.264 0.250 .
In both factorizations , G leads to the correct clustering the first three columns are in one cluster and the results : remaining columns are in another cluster . Note that
STri− f actor =4.3626
1.4824
1.0136
8.4000 .
In this section , we give an example to illustrate the differences in SVD , semi NMF and convex NMF . The input data matrix is 
4.8 8.0 1.3 3.9 −5.5 −8.5 −3.9 −5.5 1.5 8.2 −7.2 −8.7 −7.9 −5.2 6.5 7.4 3.8 4.7 −7.3 −1.8 −2.1 6.2
1.8 6.9 1.6 8.3
6.4 2.7
7.5 6.8
3.2 4.8

X =
5.0
7.1
5.2


One can see that the first 3 columns should be one cluster and the last 4 columns should be another cluster .
The computed basis vectors F are :
Fsvd =
0.50 −0.41 0.21 0.35 0.32 0.66 −0.28 0.72 −0.43 −0.28 25.5 15.6


,


Fsemi =
0.05 0.27 0.40 −0.40 0.70 −0.72 0.08 0.30 0.49 −0.51 20.3 23.0


, Fconv =


0.31 0.53 0.42 −0.30 0.56 −0.57 0.41 0.49 0.36 −0.41 31.0 39.3


,


Basis vectors are normalized to 1 in L2 norm ( norms are given at the bottom line ) for comparison purpose .
The matrix G are listed as follows :
Gsvd =0.25
0.50
0.05 0.60
0.22 −0.45 −0.44 −0.46 −0.52 0.43
0.30 −0.12
0.01
0.84
0.31 1.16 0.36
0.02
0
0.29
0.12
0.54 0.11
0.89 0.53
0.77 1.03
0.36 0.77
0.14 0.60
0.31 0.06
Gsemi =0.61 Gconv =0.31 Both semi NMF and convex NMF give the correct clustering results . However , convex NMF gives sharper cluster indicators , while semi NMF gives a soft clustering . The residual values , the level of low rank approximations , are
0.02 0.31
0.27
0.30
0
0
0 kX − FGT k = 027940,027944,030877 , for SVD , semi NMF , and convex NMF respectively . We see that semi NMF has a good quality approximation close to SVD .
4 . Algorithms for Various Matrix Factorization
Methods
It absorbs the different scales of X , FTri− f actor and GTri− f actor and thus FTri− f actor provides row clusters ( ie , the attribute clusters ) .
The algorithms for matrix factorizations are generally iterative updating procedures : updating one factor while fixing the other factors . The algorithms for various matrix
Factorizations
NMF
Semi NMF
Convex NMF
Tri Factorization
Kernel NMF
Updating Rules ( XG)ik Fik ← Fik ( FGT G)ik ( XT F ) jk G jk ← G jk ( GFT F ) jk F = XG(GT G)−1
Gik ← Gikr ( XT F)+
( XT F)− ik +[G(FT F)− ]ik ik +[G(FT F)+ ]ik
Gik ← Gikr [ (XT X)+W ]ik+[GW T ( XT X)−W ]ik Wik ← Wikr [ (XT X)+ G]ik+[(XT X)−W GT G]ik
[ (XT X)−W ]ik+[GW T ( XT X)+W ]ik
[ (XT X)− G]ik+[(XT X)+W GT G]ik
G jk ← G jkr ( XT FS ) jk Fik ← Fikq ( XGST )ik Sik ← Sikq ( FT XG)ik
( FT FSGT G)ik replace X T X by hf ( X)T f ( X)i in Convex NMF
( GGT XT FS ) jk
( FFT XGST )ik
Table 1 . Updating rules for different matrix factorizations . ik = ( |Aik| − Aik)/2 . factorizations are summarized in Table 1 . In the table , we separate the positive and negative parts of a matrix A as ik = ( |Aik| + Aik)/2 , A− A+ In the Literature there is some question [ 16 ] on whether Lee Seung algorithm converge to a local minima . However , it is easy to show that at convergence , the solution satisfy the well known KKT complementarity condition in the theory of constrained optimization , which is ,
( XG − FGT G)ikFik = 0 , ( X T F − GFT F ) jkG jk = 0 ,
( 4 ) ik = F ∗ for the objective J = ||X − FGT ||2 . For example , at conik(XG∗)ik/ ( F ∗G∗T G∗)ik which is identical vergence , F ∗ to first condition in Eq ( 4 ) . Therefore , Lee Seung algorithm does converge to a local minima according to KKT theory . It has been proved that at convergence , solutions of all algorithms listed in Table 1 satisfy the KKT conditions in their respective cases .
G and view the updating algorithms as
We can let Q =F mapping Q ( t+1 ) = M(Q ∗ ) . The objectives for all cases in Table 1 have been proved to ( t+1 ) ) ≤ J(Q be non increasing , J(Q ( t) ) . Following Xu & Jordan [ 31 ] , we expand2 Q ≃ M(Q ∗ ) + ( ¶ M/¶ ∗ ) . Therefore ,
( t) ) . At convergence , Q
∗ = M(Q
)(Q − Q kQ
( t+1 ) − Q
∗k ≤ k
¶ M k · kQ
( t ) − Q
∗k under appropriate matrix norm . 6= 0 . Thus these updating algorithms have first order convergence rate , same as the EM algorithm [ 31 ] .
In general , ¶ M/¶
2Nonnegativity constraint need be enforced .
5 . Relations Among Various Factorizations
In this section , we theoretically analyze the relationships among various matrix factorization methods .
51 NMF and K means Clustering
Lee and Seung [ 24 ] emphasizes the difference between NMF and vector quantization ( which is K means clustering ) . Later experiments [ 22 , 26 ] empirically show that NMF has clear clustering effects . Theoretically , NMF is inherently related to kernel K means clustering . Theorem 1 . Orthogonal NMF , min
F≥0,G≥0 kX − FGTk2 , st GT G = I .
( 5 ) is equivalent to K means clustering . This theorem has been previously proved [ 12 ] with additional normalization conditions . Here we give a more general proof , which will generalize to bi orthogonality . Proof . We write J = ||X − FGT ||2 = Tr(X T X − 2FT XG + F T F ) . The zero gradient condition ¶ J/¶ F = −2XG + 2F = 0 gives F = XG . Thus J = Tr(X T X − GT X T XG ) . Since Tr(X T X ) is a constant , the optimization problem becomes
Tr(GT X T XG ) st GT G = I .
( 6 ) min G≥0
According to Theorem 2 below , this is identical to K means clustering . ' We note that Theorem 1 holds even if X and F are not nonnegative , ie , X and F have mixed sign entries . Theorem 2[11 , 34 ] . The K means clustering
J = kxi − fkk2 k=1 i∈Ck
( 7 )
Q ¶ Q Q k ( cid:229 ) ( cid:229 ) where fk is the cluster centroid of the k th cluster , and more generally , the Kernel K means with mapping xi → f ( xi )
Jf = k=1 i∈Ck kf ( xi ) − ¯f k||2
( 8 ) where ¯f k is the centroid in the feature space . This can be solved via the optimization problem max
GT G=I , G≥0
Tr(GTW G ) ,
( 9 ) soft versions of K means clustering . From NMF/semi NMF , and to convex NMF , the successive restrictions make them different levels of soft clustering .
This situation is similar to the mixture of Gaussian generalization of K means . K means is a mixture of spherical Gaussians with same variance . The first step is to generalize to spherical Gaussians with individual variance . The second step is to generalize to Gaussians with individual full covariance matrix , etc . Each generalization have more model parameters and fits the data better . where G are the cluster indicators and Wi j = f ( xi)Tf ( x j ) is the kernel . For K means , f ( xi ) = xi , Wi j = xT
NMF has clustering capabilities which is generally better than the K means . In fact , PCA is effectively doing K means clustering [ 11 , 34 ] . Let G be the cluster indicators for the k clusters then ( 1 ) GGT ≃ VV T ; ( ii ) the principal directions , UU T , project data points into the subspace spanned by the k cluster centroids . i x j .
52 NMF , Semi NMF , Convex NMF and
Kernel NMF
In fact , NMF , semi NMF , convex NMF and kernel NMF all have K means clustering interpretations when the factor G is orthogonal . Being orthogonal and nonnegative , implies each row of G has only one nonnegative elements , ie , G is a bona fide cluster indicator . We have Theorem 3 . G orthogonal NMF , semi NMF , convex NMF and Kernel NMF is identical to relaxed K means clustering . Proof . For NMF , semi NMF and convex NMF , we first eliminate F . The objective is J = kX − FGT k2 = Tr(X T X − 2X T FGT + FF T ) . Setting ¶ J/¶ F = 0 , we obtain F = XG . Thus we obtain
J = Tr(X T X − GT X T XG ) .
For Kernel NMF , we have
J = kf ( X ) − f ( X)WGT k2 = Tr(K − GT KW +W T KW ) , where K is the kernel . Setting ¶ J/¶ W = 0 , we have KG = KW . Thus
J = Tr(X T X − GT KG ) .
In all the above cases , the first term are constant and are ignored . The minimization problem thus becomes
Tr(GT KG ) , max GT G=I where K is either a linear kernel X T X or hf ( X),f ( X)i . It is known [ 34 ] that this is identical to ( kernel ) K means clustering . ⊓– In the definitions of NMF , semi NMF , convex NMF , G is not restricted to be orthogonal ; these NMF varieties are
53 Tri Factorization
First , we emphasize the role of orthogonality in TriFactorization 3 Considering the unconstrained 3 factor NMF min
F≥0,G≥0,S≥0 kX − FSGTk2 ,
( 10 ) we note that this 3 factor NMF can be reduced to the unconstrained 2 factor NMF by mapping F ← FS . Another way to say this is that the degree of freedom of FSGT is the same as FGT .
Therefore , 3 factor NMF is interesting only when it can not be transformed into 2 factor NMF . This happens when certain constraints are applied to the 3 factor NMF . However , not all constrained 3 factor NMF differ from their 2factor NMF counterpart . For example , the following 1 sided orthogonal 3 factor NMF min
F≥0,G≥0,S≥0 kX − FSGTk2 , st FT F = I
( 11 ) is no different from its 2 factor counterpart , because the mapping F ← FS reduces one to another . It is clear that min
F≥0,G≥0,S≥0 kX − FSGTk2 , st FT F = I , GT G = I .
( 12 ) has no corresponding 2 factor counterpart . This is a genuine new factorization , which we call 3 factor NMF . The update rules are given in Table 1 .
An important special case is that the input X contains a matrix of pairwise similarities : X = X T = W . In this case , F = G = H . We optimize the symmetric NMF : min
W ≥0,S≥0 kX − HSHT k2 , st HT H = I .
( 13 )
When the orthogonality of HT H = I is enforced , we can use the update rules of Tri Factorization of Eq ( 12 ) with appropriate substitutions . When HT H = I is not enforced , the update rules are :
Sik ← Sik
( H TW H)ik
( HTHSHTH)ik
.
( 14 )
3Sometimes we also use 3 factor NMF to represent Tri Factorization . k ( cid:229 ) ( cid:229 ) Hik ← Hik1 − b + b
( W HS)ik
( HSHTHS)ik .
( 15 ) where 0 < b ≤ 1 . In practice , we find b = 1/2 is a good choice .
54 NMF and PLSI
Here we show that NMF is related to another relevant unsupervised learning method : Probabilistic Latent Semantic Indexing ( PLSI ) . So far , the cost function we used for computing NMF is the sum of squared errors , ||X − FGT ||2 . Another cost function KL divergence :
JNMF KL = m(cid:229 ) i=1 n(cid:229 ) j=1 Xi jlog
Xi j
( FGT )i j
− Xi j + ( FGT )i j ( 16 )
Probabilistic Latent Semantic Indexing ( PLSI ) is a unsupervised learning method based on statistical latent class models and has been successfully applied to document clustering [ 21 ] . ( PLSI is further developed into a more comprehensive Latent Dirichlet Allocation model [ 5] . )
PLSI maximize the likelihood
A way to resolve this is to assume NMF follows a certain distribution . We can think of the rectangular input data X as a word document matrix and perform a PLSI type probabilistic decomposition . as in Eq 18 , where zk is the latent cluster variable , and the probability factors follow the probability normalization m(cid:229 ) p(wi|zk ) = 1 , n(cid:229 ) p(d j|zk ) = 1 ,
K(cid:229 ) p(zk ) = ( cid:229 )
Xi j = 1 . j=1 i=1 We assume the data is normalized such that ( cid:229 ) this , the cluster posterior probability for column d j is then i j Xi j = 1 . With k=1 i j p(zk|d j ) = p(d j|zk)p(zk)/p(d j ) ( cid:181 ) p(zk)p(d j|zk ) .
Translating to F,G , the equivalent probabilistic decomposition is
X = FGT = ( FD−1
F )(DF DG)(GD−1 where DF = diag(eT F ) , DG = diag(eT G ) , and m(cid:229 )
( GD−1
( FD−1
G ) jk = 1 ,
F )ik = 1 ,
K(cid:229 ) n(cid:229 )
G )T , j=1 k=1
( DF DG)kk = 1 .
JPLSI = m(cid:229 ) n(cid:229 ) i=1 j=1
X(wi,d j)logP(wi,d j )
( 17 ) i=1 where the joint occurrence probability is factorized ( ie , parameterized or approximated ) as
P(wi,d j ) = ( cid:229 ) = ( cid:229 ) k k
P(wi,d j|zk)P(zk )
P(wi|zk)P(d j|zk)P(zk ) , assuming that wi and d j are conditionally independent given zk . Proposition 1 . Objective function of PLSI is identical to the objective function of NMF , ie , JPLSI = −JNMF KL + constant . The proposition can be easily proved by setting ( FGT )i j = P(wi,d j ) . Therefore , the NMF update algorithm and the EM algorithm in training PLSI are alternative methods to optimize the same objective function [ 14 ] .
6 . Normalization of Nonnegative Matrix Fac torizations
In this section , we try to interpret the cluster posterior obtained form matrix factorization .
Given a solution ( F,G ) of NMF : X = FGT , it is usually assumed that G is the cluster posterior and thus Gik gives the posterior probability that xi belongs to the k th column cluster . However , the NMF solutions are not unique . Suppose ( F,G ) is solution of NMF . There exist many matrices ( A,B ) such that ABT = I , FA ≥ 0 , GB ≥ 0 . Thus ( FA,GB ) is also the solution with the same residue kX − FGTk .
Thus for standard NMF , the cluster posterior probability for column xi is
NMF : p(zk|xi ) ( cid:181 )
[ (DF DG)(GD−1
G )T ]T ik = ( GDF )ik
( 18 )
For Convex NMF , the centroid interpretation of F = XW implies W should have a L1 normalization . Thus we write
X = XWGT = ( XW D−1
W )(DW DG)(GD−1
G )T .
Therefore , the cluster posterior probability for column xi is
Convex NMF : p(zk|xi ) ( cid:181 )
( GDW )ik , DW = diag(eTW ) .
Semi NMF does not have a probability interpretation because F could be negative signs . For this reason , the L2 normalization is most natural . Let F = ( f1 , · · · ,fk ) and ZF = diag(||f1|| , · · · , ||fk|| ) . We write
X = FGT = ( FZ−1
F )(ZF DG)(GD−1
G )T .
Thus the cluster posterior probability is
Semi NMF : p(zk|xi ) ( cid:181 )
( GZF )ik .
7 . Simultaneous Clustering
Consider the nonnegative Tri Factorization X ≃ FSGT . For the objective of the function approximation , we optimize min
F≥0,G≥0,S≥0 kX − FSGTk2 , st F T F = I , GT G = I . p×n + , F ∈ R p×k + and S ∈ Rk×ℓ
+ and G ∈ Rn×ℓ We note X ∈ R + . This allows the number of row cluster ( k ) differ from the number of column cluster ( ℓ ) . In most cases , we set k = ℓ . This form gives a good framework for simultaneously clustering the rows and columns of X . Recently , simultaneous clustering has been extensively studied [ 10 , 8 , 2 , 7 , 28 ] . However , two questions are still largely unaddressed in the literature : we have
P(wi,d j ) = ( cid:229 ) = ( cid:229 ) k,l k,l
P(wi,d j| fk,gl)P( fk,gl )
P(wi| fk)P(d j|gl)P( fk,gl ) .
( 19 )
Here , P(wi| fk ) corresponds to F , P(d j|gl ) to G and P( fk,gl ) to S .
• Why do we prefer the simultaneous clustering to single side clustering ?
72 An Illustrative Example
• How to evaluate the simultaneous clustering ?
In this section , we attempt to provide our insights for the above questions .
71 Why Simultaneous Clustering ?
First , simultaneous clustering is preferred for applications in high dimensional spaces . Most clustering algorithms do not work efficiently in high dimensional spaces due to the curse of dimensionality . It has been shown that in a high dimensional space , the distance between every pair of points is almost the same for a wide variety of data distributions and distance functions [ 4 ] . Many feature selection techniques have been applied to reduce the dimensionality of the space . However , as demonstrated in [ 1 ] , the correlations among the dimensions are often specific to data locality ; in other words , some data points are correlated with a given set of features and others are correlated with respect to different features . As pointed out in [ 20 ] , all methods that overcome the dimensionality problems use a metric for measuring neighborhoods , which is often implicit and/or adaptive . Simultaneous clustering performs an implicit feature selection at each iteration and provides an adaptive metric for measuring the neighborhood .
Second , simultaneous clustering is preferred when there is an association relationship between the data and the features ( ie , the columns and the rows ) . A case is the binary data . A distinctive characteristic of the binary data is that the features ( attributes ) they include have the same nature as the data they intend to account for : both are binary . Another case is block diagonal clustering where both data points and features have the same number of clusters . In this case , after appropriate permutation of the rows and columns , the cluster structure takes the form of a block diagonal matrix [ 18 ] .
It should be noted that simultaneous clustering can also be interpreted using a probabilistic view similar to the PLSI model . Instead of assuming that the variables wi and d j are conditionally independent given zk in Eq 18 , we assume that the variable wi only depends on its cluster variable fk and the variable d j only depends on its cluster variable gl in the probabilistic model of simultaneous clustering . Therefore
The example is based on a simple dataset which contains six system log messages from two different situations : Start and Create .
After removing stop words and words only appear once , we get the binary document term matrix as shown in Table 2 . For this example , using one side clustering , eg , k means , it usually does get perfect clustering results . However , using simultaneous clustering , we could correctly obtain the message clusters . The reason is that using simultaneous clustering , in the iteration process , we could adaptively measure the distance between the data points : if the words have similar distributions across multiple clusters , it can be treated as outliers and does not contribute to the distance computation . In the example , Term 3 and 4 ( ie , column 3 and 4 ) can be thought as feature noises as they have similar distributions across multiple clusters .
Terms/Messages
T1 T2 T3 T4 T5 T6
S1 1 1 0 0 0 1
S2 1 1 1 1 0 0
S3 C1 C2 C3 1 1 1 0 0 1 0 1 1 0 0 1
0 1 1 1 1 1
0 0 1 1 1 1
Table 2 . Log message example : The 6 terms are start , application , version , service , create , temporary respectively .
73 Evaluate Simultaneous Clustering
Simultaneous clustering performs clustering of row and column clustering simultaneously , where the factor F is the cluster indicator for words ( ie , rows ) . Quantitatively , we can view the i th row of the cluster indicator F as the posterior probability that word i belongs to each of the K word clusters . We can assign a word to the cluster that has the largest probability value . However , row clustering has no clear a prior labels to compare with . For example , for document clustering , we usually have labels for each document class and we have no label information about word clusters .
Here we provide a systematic way for analyzing and evaluating the clustering of rows ( ie,words ) Let this row of F be ( p1 , · · · , pk ) , which has been normalized to ( cid:229 ) k pk = 1 . Suppose a word has a posterior distribution of
( 0.93 , 0.01 , 0.04 , · · · ,0.02 ) ; it is obvious that this word is cleanly clustered into one cluster . We say this word has a 1 peak distribution . Suppose another word has a posterior distribution of
( 0.52 , 0.46 , 0.01 , · · · ,0.01 ) ; obviously this word is clustered into two clusters . We say this word has a 2 peak distribution . In general , we wish to characterize each word as belonging to 1 peak , 2 peak , 3peak etc . For K word clusters , we set K prototype distributions :
( 1,0 , · · · ,0 ) , (
1 2
,
1 2
, · · · ,0 ) , · · · , (
1 K
, · · · ,
1 K
) .
For each word , we assign it to the closest prototype distribution based on the Euclidean distance , allowing all possible permutations of the clusters . For example , ( 1,0,0 , · · · ,0 ) is equivalent to ( 0,1,0 , · · · ,0 ) . In practice , we first sort the row such that the components decrease from the left to the right , and then assign it to the closest prototype . Generally speaking , the less peaks of the posterior distribution of the word , the more unique content of the word has . This multi peak distribution approach provides the capability of evaluating row ( eg , word ) clusterings and enables the systematic analysis of word content .
8 . Experiments
In this section , experiments are conducted to empirically compare the clustering results of various NMF algorithms . In our experiments , documents are represented using the binary vector space model where each document is a binary vector in the term space . Our comparative experimental study includes the following six methods : K means , NMF , Semi NMF , Convex NMF , Tri Factorization , and PLSI .
81 Datasets
Datasets CSTR
WebKB4 Reuters WebACE
Log
# documents
# class
476 4199 2,900 2,340 1367
4 4 10 20 9
Table 3 . Document Datasets Descriptions .
WebKB The WebKB dataset contains webpages gathered from university computer science departments . There are about 8280 documents and they are divided into 7 categories : student , faculty , staff , course , project , department and other . The raw text is about 27MB . Among these 7 categories , student , faculty , course and project are four most populous entity representing categories . The associated subset is typically called WebKB4 .
Reuters
The Reuters 21578 Text Categorization Test collection contains documents collected from the Reuters newswire in 1987 . It is a standard text categorization benchmark and contains 135 categories . In our experiments , we use a subset of the data collection which includes the 10 most frequent categories among the 135 topics and we call it Reuters top 10 .
WebACE
The K dataset was from WebACE project and has been used for document clustering [ 6 , 19 ] . The Kdataset contains 2340 documents consisting news articles from Reuters new service via the Web in October 1997 . These documents are divided into 20 classes .
Log The log data used in our experiments are collected from several different machines with different operating systems using logdump2td ( NT data collection tool ) developed at IBM TJ Watson Research Center . The data in the log files describe the status of each component and record system operational changes , such as the starting and stopping of services , detection of network applications , software configuration modifications , and software execution errors .
To pre process the datasets , we remove the stop words using a standard stop list , all HTML tags are skipped and all header fields except subject and organization of the posted articles are ignored . In all our experiments , we first select the top 1000 words by mutual information with class labels .
82 Result Analysis
We use a variety of datasets , most of which are frequently used in the information retrieval research . Table 3 summarizes the characteristics of the datasets .
CSTR This is the dataset of the abstracts of technical reports ( TRs ) published in the Department of Computer Science at a research university . The dataset contained 476 abstracts , which were divided into four research areas : Natural Language Processing(NLP ) , Robotics/Vision , Systems , and Theory .
The above document datasets are standard labeled corpora widely used in the information retrieval literature . We view the labels of the datasets as the objective knowledge on the structure of the datasets . We use accuracy as the clustering performance measure . Accuracy discovers the one toone relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class . It sums up the whole matching de
Datasets/Methods K Means 0.4256 0.3888 0.4448 0.4001 0.6876
WebKB4 Reuters WebACE
CSTR
Log
NMF 0.5713 0.4418 0.4947 0.4761 0.7805
Semi NMF Convex NMF Tri Factorization
0.5628 0.4578 0.4867 0.4162 0.7385
0.5340 0.4658 0.4789 0.4089 0.7257
0.604 0.483 0.554 0.510 0.801
PLSI 0.587 0.503 0.4870 0.4890 0.778
Table 4 . Clustering Accuracy . Each entry is the clustering accuracy of the column method on the corresponding row dataset . The results obtained by averaging 5 trials . gree between all pair class clusters . Accuracy can be represented as : served from the table that NMF and PLSI usually lead to similar clustering results .
Accuracy = Max( ( cid:229 )
Ck,Lm
T ( Ck,Lm))/N ,
( 20 ) where Ck denotes the k th cluster , and Lm is the m th class . T ( Ck,Lm ) is the number of entities which belong to class m are assigned to cluster k . Accuracy computes the maximum sum of T ( Ck,Lm ) for all pairs of clusters and classes , and these pairs have no overlaps . The greater accuracy means the better clustering performance .
Figure 1 . Clustering Accuracy Comparison
The experimental results are shown in Table 4 and Figure 1 . From the experimental comparisons , we observe that :
• NMF like algorithms generally outperform K mean clustering algorithms . As we showed in Section 5.4 , NMF is equivalent to soft K means and the soft relaxation improves clustering performance .
• On most of the datasets , NMF gives somewhat better accuracy than semi NMF and convex NMF . The differences are modest , however , suggesting that the more highly constrained semi NMF and convex NMF may be worthwhile options if interpretability is viewed as a goal of the data analysis .
• The experimental comparisons empirically verify the equivalence between NMF and PLSI . It can be ob
• Tri Factorization generally is better than K means and NMF like algorithms on most of the datasets . The document datasets are of high dimension . Tri Factorization provides a good framework for simultaneously clustering the rows and columns . Simultaneous clustering performs an implicit feature selection at each iteration , provides an adaptive metric for measuring the neighborhood , and thus tends to yield better clustering results .
• As we discussed in Section 7 , Tri Factorization enables simultaneous clustering of rows and columns and the multi peak distribution evaluation approach enables the systematic analysis of word content . We take a closer look at the Log dataset and obtain the words in 1 peak , 2 peak , 3 peak and 4 peak categories respectively . The raw log files contain a free format ASCII description of the event . We can derive meaningful common situations ( ie , row clustering ) from the word cluster results . For example , situation start can be described by 1 peak words such as start , and service , and 2 peak words such as version . The situation configure can be described by 1 peak words such as configuration , twopeak words such as product , and 3 peak words such as professional . To summarize , the word clustering is capable of distinguishing the contents of words . The results of peak words are consistent with what we would expect from a systematic content analysis .
9 . Summary
In this paper we provide a comparative study on ( nonnegative ) matrix factorization for clustering . Attempts have been made to establish the relations among various matrix factorization methods while highlighting their difference . Previously unaddressed yet important questions such as the interpretation and normalization of cluster posterior , convergence issues , and the benefits and evaluation of simultaneous clustering have also been studied . We expect our study could provide insightful guidances on matrix factorization research for clustering . In particular , the extensive research and experiments show that NMF provides a new paradigm for unsupervised learning . Acknowledgments . We would like to Dr . Shenghuo Zhu for useful comments and discussion . T . Li is partially supported by a IBM Faculty Research Award , and the NSF CAREER Award IIS 0546280 . C . Ding is partially supported by the US Dept of Energy , Office of Science .
References
[ 1 ] C . C . Aggarwal , J . L . Wolf , P . S . Yu , C . Procopiuc , and J . S . Park . Fast algorithms for projected clustering . In Proc . SIGMOD’99 , pages 61–72 . ACM Press , 1999 .
[ 2 ] D . Baier , W . Gaul , and M . Schader . Two mode overlapping clustering with applications to simultaneous benefit segmentation and market structuring . In R . Klar and O . Opitz , editors , Classification and Knowledge Organization , pages 577–566 . Springer , 1997 .
[ 3 ] M . Berry , M . Browne , A . Langville , P . Pauca , and R . Plemmons . Algorithms and applications for approximate nonnegative matrix factorization . To Appear in Computational Statistics and Data Analysis , 2006 .
[ 4 ] K . S . Beyer , J . Goldstein , R . Ramakrishnan , and U . Shaft . When is nearest neighbor meaningful ? In Procs Int’l Conf . on Database Theory ( ICDT’99 ) , pages 217–235 . 1999 .
[ 5 ] D . Blei , A . Ng , and M . Jordan . Latent dirichlet allocation . Journal of Machine Learning Research , 3:993–1022 , 2003 . [ 6 ] D . Boley . Principal direction divisive partitioning . Data min ing and knowledge discovery , 2:325–344 , 1998 .
[ 7 ] W . Castillo and J . Trejos . Two mode partitioning : Review of methods and application and tabu search . In K . Jajuga , A . Sokolowski , and H H Bock , editors , Classification , Clustering and Data Analysis , pages 43–51 . Springer , 2002 . [ 8 ] H . Cho , I . S . Dhillon , Y . Guan , and S . Sra . Minimum sumIn squared residue co clustering of gene experssion data . Proceedings of the SIAM Data Mining Conference , 2004 .
[ 9 ] M . Cooper and J . Foote . Summarizing video using nonnegative similarity matrix factorization . In Proc . IEEE Workshop on Multimedia Signal Processing , pages 25–28 , 2002 . InformationIn Proc . KDD 2003 , pages 89–98 ,
[ 10 ] I . S . Dhillon , S . Mallela , and S . S . Modha . theoretic co clustering . 2003 .
[ 11 ] C . Ding and X . He . K means clustering and principal compo nent analysis . Int’l Conf . Machine Learning ( ICML ) , 2004 .
[ 12 ] C . Ding , X . He , and H . Simon . On the equivalence of nonnegative matrix factorization and spectral clustering . Proc . SIAM Data Mining Conf , 2005 .
[ 13 ] C . Ding , T . Li , and M . Jordan . Convex and semi nonnegative matrix factorizations for clustering and low dimension representation . Technical Report LBNL 60428 , Lawrence Berkeley National Laboratory , 2006 .
[ 14 ] C . Ding , T . Li , and W . Peng . Nonnegative matrix factorization and probabilistic latent semantic indexing : Equivalence , chi square statistic , and a hybrid method . In Proc . of National Conf . on Artificial Intelligence ( AAAI 06 ) , 2006 .
[ 15 ] C . Ding , T . Li , W . Peng , and H . Park . Orthogonal nonnegative matrix tri factorizations for clustering . In Proc SIGKDD Int’l Conf . on Knowledge Discovery and Data Mining , 2006 .
[ 16 ] EF Gonzales and Y . Zhang . Accelarating ithe Lee Seung algorithms for nonnegative matrix factorization . Dept . of Comp . and Applied Math . , Rice University Tech Report , 2005 .
[ 17 ] G . Golub and C . Van Loan . Matrix Computations , 3rd edi tion . Johns Hopkins , Baltimore , 1996 .
[ 18 ] G . Govaert . Simultaneous clustering of rows and columns .
Control and Cybernetics , 24(4):437–458 , 1995 .
[ 19 ] E . Han , D . Boley , M . Gini , R . Gross , K . Hastings , G . Karypis , V . Kumar , B . Mobasher , and J . Moore . WebACE : A web agent for document categorization and exploration . In Proc . Int’l Conf . on Autonomous Agents ( Agents’98 ) 1998 .
[ 20 ] T . Hastie , R . Tibshirani , and J . Friedman . The Elements of Statistical Learning : Data Mining , Inference , Prediction . Springer , 2001 .
[ 21 ] T . Hofmann . Probabilistic latent semantic analysis . In Proceedings of the 15th Annual Conference on Uncertainty in Artificial Intelligence ( UAI 99 ) , pages 289–296 , 1999 .
[ 22 ] P . O . Hoyer . Non negative matrix factorization with sparseness constraints . J . Machine Learning Research , 5:1457– 1469 , 2004 .
[ 23 ] F . D . La Torre and T . Kanade . Discriminative cluster analysis . In Proc . Int’l Conf . on Machine Learning ( ICML 2006 ) , 2006 .
[ 24 ] D . Lee and H . S . Seung . Learning the parts of objects by non negative matrix factorization . Nature , 401:788–791 , 1999 .
[ 25 ] D . Lee and H . S . Seung . Algorithms for non negatvie matrix factorization . Advances in Neural Information Processing Systems 13 , 2001 .
[ 26 ] S . Li , X . Hou , H . Zhang , and Q . Cheng . Learning spatially localized , parts based representation . In Proce IEEE Computer Vision and Pattern Recognition , pages 207–212 , 2001 . [ 27 ] B . Long , Z . Zhang , and P . Yu . Co clustering by block value decomposition . In KDD ’05 , pages 635–640 , 2005 .
[ 28 ] V . Maurizio . Double k means clustering for simultaneous classification of objects and variables . In S . Borra , R . Rocci , M . Vichi , and M . Schader , editors , Advances in Classification and Data Analysis , pages 43–52 . Springer , 2001 .
[ 29 ] P . Paatero and U . Tapper . Positive matrix factorization : A non negative factor model with optimal utilization of error estimates of data values . Environmetrics , 5:111–126 , 1994 . [ 30 ] F . Sha , L . Saul , and D . Lee . Multiplicative updates for nonnegative quadratic programming in support vector machines . In Advances in Neural Information Processing Systems 15 , pages 1041–1048 . 2003 .
[ 31 ] L . Xu and MI Jordan . On convergence properties of the em algorithm for gaussian mixtures . Neural Computation , pages 129–151 , 1996 .
[ 32 ] W . Xu , X . Liu , and Y . Gong . Document clustering based on non negative matrix factorization . In SIGIR’03 ) , pages 267– 273 , 2003 .
[ 33 ] D . Zeimpekis and E . Gallopoulos . CLSI : A flexible approximation scheme from clustered term document matrices . Proc . SIAM Data Mining Conf , pages 631–635 , 2005 .
[ 34 ] H . Zha , C . Ding , M . Gu , X . He , and H . Simon . Spectral relaxation for K means clustering . Advances in Neural Information Processing Systems 14 ( NIPS’01 ) .
[ 35 ] H . Zha , X . He , C . Ding , M . Gu , and H . Simon . Bipartite graph partitioning and data clustering . Proc . Int’l Conf . Information and Knowledge Management ( CIKM 2001 ) , 2001 .
