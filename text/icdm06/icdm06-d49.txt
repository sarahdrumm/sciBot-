Probabilistic Enhanced Mapping with the Generative Tabular Model
Rodolphe Priam
Mohamed Nadif
LMA UMR 6086 CNRS/Universit´e de Poitiers
Site Noron , 8 Rue Archim`ede , 79000 Niort
CRIP5 , Universit´e Paris Descartes , 45 rue des Saints P`eres , 75006 Paris
Abstract
Visualization of the massive datasets needs new methods which are able to quickly and easily reveal their contents . The projection of the data cloud is an interesting paradigm in spite of its difficulty to be explored when data plots are too numerous . So we study a new way to show a bidimensional projection from a multidimensional data cloud : our generative model constructs a tabular view of the projected cloud . We are able to show the high densities areas by their non equidistributed discretization . This approach is an alternative to the self organizing map when a projection does already exist . The resulting pixel views of a dataset are illustrated by projecting a data sample of real images : it becomes possible to observe how are laid out the class labels or the frequencies of a group of modalities without being lost because of a zoom enlarging change for instance . The conclusion gives perspectives to this original promising point of view to get a readable projection for a statistical data analysis of large data samples .
1 Introduction
Increasing number of nonlinear methods M as [ 15 , 3 , 14 , 7 , 8 , 6 ] are proposed in the literature to project a data sample {xi}i=I i=1 , demonstrating that the data cloud projection {yi = M(xi)}i=I i=1 paradigm is a good way to see the structure of the distribution of a dataset . An alternative to this point of view is the self organizing maps or SOM [ 10 ] which construct a tabular projection of the data cloud , with neighbour cells on the table enough similar in the data space to preserve the topology . This way to see data is certainly more readable than a cloud of numerous points on the plane , where points are often hard to see while the shape of the projected distribution appears . For this reason , some zoom tools are generally needed , but can be a way to be lost on the data projection when one has to change the scale of the enlarging . As these new methods are certainly projecting data in a complementary way , and that the tabular view is easily understood , we aim in this paper to add a tabular view for any 2D data cloud by an alternative approach to the selforganizing map . The SOM solution for getting the tabular view would be to construct a self organizing map over the bidimensional projection . To construct a tabular view of a data set , the self organizing map algorithms are clustering methods with their center vectors constrained with an imaginary regular mesh . Each node of this mesh corresponds to one class center , and the aim of the algorithm is to train the center vectors such as nearer they are on the mesh , nearer they are in the data space . This strong hypothesis is only true for parts of the mesh because the data distribution can be complex in practice . So a SOM algorithm constructs well organized areas separated by a frontier which reveals that these areas are not connected in the data space . To show a tabular view of the projected values yi , self organizing maps are an appealing solution : in 2D , they give a discretized view of the distribution shape with zoom capabilities over high density areas . Several algorithms already exist like the GTM or Generative Topological Mapping [ 2 ] for instance . This model is a Gaussian mixture [ 13 ] with centers constrained to lay on a discretized surface , and constituting the knot of this discrete surface . In our case , we are interesting in showing the tabbled data as they are laid out in the plane , as proposed in the tabular model : it is a new constrained Gaussian mixture where means are lying on linear rows and columns which are free to move to show area with high density . Our study permits to construct readable tables from multimedia corpuses . When cells or classes are numerous like in the case of the massive datasets , a Pixel Based [ 9 ] approach is necessary , carrying interesting and synthetic information . In this paper , we present the model , we give the obtained pixel maps for an image dataset , then we explain how visual datamining is beneficial , and finally we conclude with discussion and some perspectives .
2 The Generative Tabular Model
In this section , we suppose that a mapping M was used to obtain the bidimensional coordinates yi = ( yis)s=1,2 which are processed to get a tabular view of the projected data cloud . The basis of the algorithm is the Gaussian mixture model which extends the K means [ 12 ] method to a probabilistic expression with a classifying hidden variable . A Gaussian mixture density is written P ( yi ) = 1≤k≤K πkG(yi ; mk , σ ) with K factors , components , or clusters , where the k th G density is a normal density with the mean mk and the spherical variance parameter σ . The parameter πk is the probability that an observation belongs to the k th component , it therefore corresponds to the proportion of the k th cluster . The log likelihood of the observed data D assumed to be an iid sample {yi}i=I i=1 from  the probability distribution with density P ( yi ) is given by :  ,
L(θ|D ) =
πk G(yi ; mk , σ )
 
.
. log
1≤i≤I
1≤k≤K where θ = ( m1 , m2,··· , mK , π1 , π2,··· , πK−1 , σ ) . Inference of this model is done by maximizing the loglikelihood which is intractable in an exact closed form solution . The EM algorithm [ 4 ] is applied to this problem by using the likelihood completed by the knowledge of the partition Z = {Z1,Z2,··· ,ZK} , L(θ , Z|D ) =
πziG(yi ; mzi , σ )
)
1≤i≤I where zi the latent variable of which the unknown value is in K = {1 , 2 ··· , K} .
It proceeds iteratively in two steps , E ( for expectation ) and M ( for maximization ) , in maximizing the conditional expectation of L(θ , Z|D ) , given a previous current estimate ff θ(t ) : Q(θ|θ(t ) ) =
πkG(yi ; mk , σ ) q(t ) k ( yi ) log
.
.
1≤i≤I
1≤k≤K where q(t ) k ( yi ) =
1≤ ≤K π(t )
π(t ) k G(yi ; m(t ) k , σ(t ) ) G(yi ; m(t )
, σ(t ) ) denotes the conditional probability , given y and θ(t ) , that yi arises from the mixture component with density G(yi ; m(t ) k , σ(t) ) . Each iteration of EM uses the following steps :
• E step : compute the conditional expectation of L(θ , Z|D ) ; in the mixture case this step reduces to the computation of the conditional density q(t ) k ( yi ) .
• M step : compute θ(t+1 ) maximizing Q(θ|θ(t ) ) which leads to :
π(t+1 ) k
=
1 I
.
1≤i≤I q(t ) k ( yi ) ,
1≤i≤I q(t ) . . 1≤i≤I q(t )
1≤i≤I
1≤k≤K m(t+1 )
= k fi
1 I
, k ( yi)yi k ( yi ) k ( yi)||yi − m(t+1 ) q(t ) k
||2 . and σ(t+1 ) = fl ffi mk,1 mk,2
In the following paragraph , we constraint the mk vectors to get a matrix representation . We construct a regular mesh with K1 columns and K2 rows , such as k is indiced by k = K1 × ( k2 − 1 ) + k1 , ( k1 = 1 , . . . , K1 ; k2 = 1 , . . . , K2 ) , and K = K1K2 . Let ’s have :
1≤ ≤k exp(u ,s )
,
1≤ ≤Ks with mk,s = exp(u ,s ) + 1 mk = where s ∈ {1 , 2} , and uk,s is a real unknown parameter which can be obtained in maximizing Q(θ|θ(t) ) . Our parametrization is such as 0 ≤ mk,s ≤ 1 to get simpler expressions . Note that , the additive constant 1 in mk,s can be replaced by any positive real , and the sum over the numerator induces a fixed topological direction for the corresponding probabilities : a topology driven variant of the classical soft max [ 1 ] parameterization . Finally , we normalize the range of the vector components by calculating ( yis − mini(yis))/(maxi(yis ) − mini(yis ) ) as a new yis . In this case , with q(t ) ( k1,k2)(yi ) and πk = 1/K roughly speaking , to put more centers in high density areas , we propose the new marginal sensitive criterion : Qm(θ|θ(t ) ) ≡ k ( yi ) = q(t ) . . . ks ( yi)(yis − mk,s)2 , q(t )
1≤s≤2
1≤i≤I
1≤k≤Ks ks ( yi ) = q(t ) k,s(yi ) is the marginal of q(t ) where q(t ) ( k1,k2)(yi ) over k1 if s = 2 , and k2 if s = 1 . A closed form for maximizing this quantity does not exist yet , so we use a gradient ascent to calculate : m(t+1 ) = argmaxmQm(θ|θ(t) ) .
.
.
By derivating the criterion , we get the gradient vector
Dm(t ) with the u ,s derivative components Dm ,s
( t ) : k,s − yis)(δ ≤k − m(t ) q(t ) ks ( yi)(m(t )
1≤k≤Ks k,s)∆m(t ) 1≤i≤I with δ ≤k is one if ff ≤ k and zero else , and with ∆m(t ) ,s − m(t ) ( m(t ) gradient step :
−1,s ) where m0,s = 0 . Finally , we end to the
=
,s
,s
, u(t+1 ) ,s = u(t )
,s − ρ(t ) Dm ,s
( t ) , for a well chosen ρ(t ) decreasing function to get a converging value for our u ,s parameters . A Newton Raphson step by calculating the Hessian is an alternative ; its expression is not given here . Finally , iterating calculus of the u ,s and σ converges towards a maximum ( local ) . We note with a hat the final parameters . The resulting centers , in the data space , are easily found from the ˆmk vectors by the inverse preceding translation , homothety , and a possible rotation when needed , as explained at the end of the paper .
3 Application to visualization
We illustrate the tabular view on 500 images from the dataset1 of 2000 images from handwritten digits digitalized in binary images . The projecting method is here the Locally Linear Embedding or LLE mapping [ 14 ] . This method is very efficient when dealing with images database which contains classes of objects with similar shapes because locally linear relations exist . We get the resulting projection or yi sample in Figure 1 . Our model is a complementary
3
2.5
2
1.5
1
0.5
0
−0.5
−1
−1.5 −3
4 4 4 4
1 1 1 11 1 111 11 1 11 1 11 1 1 11 1 11 11 111 1 1 1 1 4 44 44 4 4 4 1 4 4 4 4 44 4 4 44 4 4 4 4 4 4 4 4 4 44 4 4 4 4 4 4 4 4 4 4 4 4
4 4
4
4
1
1 1
6
1
1
1
1 1
4
1
1
1
1
6
1
7
3
1
9
7
9 9 9 2 0 2 2
7 7 77777 77 77 7 7 7 777 7 7 77 7 7 7 7 7 7 7 7 7 7 77 7 7 7 77 7 7 7 9 7 7 7 7 9 9 9 9 7 9 7 9 1 9 9 9 9 99 1 9 7 3 9 9 9 9 9 9 99 9 9 2 9 2 9 9 9 99 2 9 9 9 9 9 99 2 9 9 9 5 2 9 2 2 2 9 2 2 5 5 9 5 5 2 9 2 2 9 2 2 2 5 2 5 5 2 5 5 3 2 22 2 3 2 5 3 2 3 2 5 3 333 3 2 5 22 2 2 2 2 22 3 22 222 3 2 8 2 5 33 2 3 3 2 3 3 3 2 8 333 333 333 5 33333333 33 2 5 8 2 33 33 8 33 33 33 5 8 3 55 6 8 5 5 8 8 555555 55 555 2 8 0 5 555 5 8 5 8 8 5 5 55 5 8 5 8 5 8 5 8 5 8 5 888888 8 8 888 888 88888 8 5 0000000000 0 8 5 6 0000000000000000 0000000000000 000 88 00000 88 8 8 8888 8 6 8 6 6 6 6 666 66 6 6 6 66 666 6 6666666666 66 666666 6 6 6 666 66 6
9
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
Figure 1 . The LLE projection of the 500 digits with 6 neighbours . approach to the existing solutions in visualization , with the strong perspective to ameliorate their own definitions thanks to the help of the local density of the data cloud . One is able to explore the data cloud on the plane without taking care of the exact relative plot positions or local density change which should have induced a possible annoying variation of the mental map . When the number of rows and columns grows up , we need a new way to see how classes are organized among the cells because the table is too large . A pixel view is then very natural and brings us to a synthetic and possibly evolutive or interactive visualization . We give the location of the ten classes over the discretized plane in Figure 2 : each cluster has a rectangular shape and its size is sensitive to the local density . Here a high density induces a smaller shape than for a low density which permits more summarization on the following global
1ftp://ftpicsuciedu/pub/machine learning databases/mfeat/
Figure 2 . The 500 digits projection with the rectangular clusters from the Tabular Model .
0
5
1
6
2
7
3
8
4
9
Figure 3 . The 10 bitmaps for the 10 digits 0 9 . picture of the classes . Each class is showed separately by a map of bits on the well defined visualization in Figure 3 : every rectangular clusters from Figure 2 are reduced to the same size near few pixels only , and for each class a bitmap is drawn . For a given class , its bitmap shows the new reduced cluster with colour filling if any datum which have the class label is inside the corresponding original cluster . We illustrate the pixel view paradigm : the bitmaps show in a synthetic fashion the relations , for LLE , between the 10 classes of digits . A potentially interactive application is to select an area from the original 2D projection and then discover from the pixel maps the corresponding classes intensities with gray level .
We are able too to zoom high densities areas by the following proposed formula , with Φ the Gaussian cumulative distribution function and Ps the marginal of P on y3−s : ffl yis ffl 1 y∗ is =
0 Ps(ys)dys Ps(ys)dys
0
= 1≤k≤K Φ( yis− ˆmk
1− ˆmk
ˆσ
1≤k≤K Φ(
ˆσ
)−Φ( )−Φ(
− ˆmk ˆσ ) − ˆmk ˆσ )
This formula is based on increasing entropy of the coordinates of the projection in Figure 4 by adding space between points which are the most numerous , ie when density of
8
7
6
5
4
3
2
1
1
2
3
4
5
6
7
8
9
10
Figure 4 . The LLE projection with the classes .
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 5 . The probabilistic zoom result . the projection is high . This is a continuous alternative to the discrete mean projection of the Generative Topological Mapping . Here by comparing the Figure 5 with the Figure 4 , we see where the data is the best clustered by our method , ie where the zoom is the most efficient : this approach enhances the visualization because we are able to see more data plots after the probabilistic transformation .
The pixel view is able to show the location of the classes over the maps , without the need to memorize some preceding map or superpose complex continuous bivariate densities as it would be the case with the classical solution of the smoothed visualisation of the original projected data cloud . Some additional information can even be put over the pixelbase level . For instance , considering one unique map we can provide several colours for the class inside only one cell . It is also possible to replace the label classes by the values of a group of variables . This solution is very interesting for textmining where contextual meaning exists among words for instance .
4 Discussion and perspectives
We have proposed a new way to explore a data projection by a new generative model which constructs a table from a cloud which is generally difficult to read when data are too numerous . Our method has several outstanding properties which permit it to carry out several interesting operations over the projection and to get a final interpretable representation .
When the data sample is large , a good initialization and supra linear estimation by a Newton algorithm are necessary to get a quick and help convergence to avoid local minima of the likelihood . The initialization is easily driven from a classical percentiles discretization along rows and columns . We suppose that we have the 2 × K percentile values hks . Then initial values for uls are written : u(0 ) s = log•
( GT s Gs )
−1GT s hs ff
, is the log log• the componentwise is column vector with uks function , where u(0 ) compos nents , hs is the column vector with hks as components , and the matrix Gs has for k st row the vector [ 1 − h1s , 1 − hks , ··· , 1 − hks , −hks , ··· , −hks ] with ks components ( 1 − hks ) followed by Ks − ks components hks . for
When the variance data is not very well oriented compared to the axes of the reference mark , it makes sens to introduce a rotation to get most of the inertia for the tabular view . So we propose here an ad hoc rotation over the data sample : we add a matrix transformation such as we replace yi by ¯y + W ( yi − ¯y ) where W is a R 2×2 matrix and ¯y is the empirical ( sample ) mean of the vectors yi . The gradient is updated by replacing yis by ¯ys + ss . ¯yis . where ¯yis = ( yis − ¯ys ) . A more elegant and general transformation is the true rotation matrix W ( t ) , written , for the rotation angle α(t ) , and an implicit proportional scaling β for ¯yis : s . w(t ) fl
W ( t ) = cos α(t ) −sin α(t ) cos α(t ) sin α(t ) ffi
.
So , we get the 1 st derivative Dα ( t ) and 2 nd derivative ( t ) formulas which provide the Newton Raphson update
Hα step :
α(t+1 ) = α(t ) − Dα
( t)/Hα
( t ) , ending to the sought rotation . The complete enhanced model shares interesting properties with several approaches from data analysis .
The tabular model is similar to a discretized [ 5 ] linear principal component [ 11 ] from the projected data cloud
[ 4 ] A . Dempster , N . Laird , and D . Rubin . Maximum likelihood from incomplete data via the em algorithm . J . Royal Statist . Soc . Ser . B . , 39 , 1977 .
[ 5 ] J . C . Fu and L . Wang . A random discretization based monte carlo sampling method and its application . Methodology and Computing in Applied Probability , 4:5–25 , 2002 .
[ 6 ] X . He , D . Cai , and W . Min . Statistical and computational analysis of locality preserving projection . In The 22nd International Conference on Machine Learning ( ICML2005 ) , 2005 .
[ 7 ] T . Hofmann .
Probmap a probabilistic approach for Intell . Data Anal . , mapping large document collections . 4(2):149–164 , 2000 .
[ 8 ] A . Kaban and M . Girolami . A combined latent class and trait model for the analysis and visualization of discrete data . IEEE Transactions on Pattern Analysis and Machine Intelligence , 23(8):859–872 , 2001 .
[ 9 ] D . A . Keim . Information visualization and visual data mining . IEEE Trans . on visualization and computer graphics , 7(1 ) , 2002 .
[ 10 ] T . Kohonen . Self organizing maps . Springer , 1997 . [ 11 ] L . Lebart , A . Morineau , and K . Warwick . Multivariate De scriptive Statistical Analysis . J . Wiley , 1984 .
[ 12 ] J . MacQueen . Some methods for classification and analysis In 5th Berkeley Symp . Math . of multivariate observations . Stat . and Proba . , volume 1 , pages 281–296 , 1967 .
[ 13 ] G . McLachlan and D . Peel . Finite Mixture Models . John
Wiley and Sons Inc . , 2000 .
[ 14 ] S . T . Roweis and L . K . Saul .
Nonlinear dimensionScience , ality reduction by locally linear embedding . 290(5500):2323–2326 , 2000 .
[ 15 ] J . Sammon . A nonlinear mapping for data structure analyIEEE Transactions on Computers , 5(18C):401–409 , sis . may 1969 . which gives almost the same rotation : when the centers are enough numerous , and with an unsmoothing affectation of the data vectors to classes . The corresponding solution is similar to the PCA projection because : ||yi − [ ¯y + ˆW −1( ˆmˆzi − ¯y)]||2 =
||¯yi − ˆW −1 ¯ˆmˆzi||2
.
. i i is then minimal . The method is connected also to a discretization approach and a density histogram construction for bivariate continuous data : we obtain two new discrete variables , along rows and columns , with interval choice sensitive to the marginal density of the bivariate projection . We are able to see the data cloud in an easier and quicker readable way . Roughly speaking , our method is similar to a PCA rotation preceding by a bivariate discretization near a percentile based one which can be used to initialize the parameters . This is an alternative to an equally spaced cut of the plane to provide us most of the visual information to show . The pixel based visualization is introduced to show the final maps and to allow their comparison all together . The method is independent of the projection choice to obtain the 2D plots . It is an alternative to the self organizing map over the original dataset , when a bidimensional projection yet exists by any nonlinear method .
Moreover , a self organizing map could have been used to analyse the 2D projection instead of the tabular model . The difference is the risk to loose the exact plot locations over the original projection . This alternative approach is complementary to our work and demands to be studied in the future . Some additive constraints can be used to enhance the tabular contents . It seems that the use of the density of a projected data cloud is under used in the current tools for visualizing massive datasets . We believe that such information can be further used instead of the classical 3D continuous bivariate density in particular . Several extensions of our approach are possible too , like new ways to see the local density of the data cloud , new ways to zoom a part of the plane , and to take advantage of the pixel paradigm . Finally , the Generative Tabular Model extended to a 3D clustering with a data projection embedded inside the Gaussian distributions is an interesting perspective for visual datamining .
References
[ 1 ] C . M . Bishop . Neural Networks for Pattern Recognition .
Clarendon Press , 1995 .
[ 2 ] C . M . Bishop , M . Svens´en , and C . K . I . Williams . GTM : A principles alternative to self organizing map . Advances in Neuronal Processing System 9 , MIT Press , 1997 .
[ 3 ] P . Demartines and J . Herault . Curvilinear component analysis : A self organizing neural network for nonlinear mapping of data sets . IEEE Transactions on Neural Networks , 8(1):148–154 , 1997 .
