Optimal Segmentation Using Tree Models
Robert Gwaderafi , Aristides Gionis , and Heikki Mannila
HIIT , Basic Research Unit
Helsinki University of Technology and University of Helsinki
Finland
April 11 , 2007
Abstract
Sequence data are abundant in application areas such as computational biology , environmental sciences , and telecommunications . Many real life sequences have a strong segmental structure , with segments of different complexities . In this paper we study the description of sequence segments using variable length Markov chains ( VLMCs ) , also known as tree models . We discover the segment boundaries of a sequence and at the same time we compute a VLMC for each segment . We use the Bayesian Information Criterion ( BIC ) and a variant of the Minimum Description Length ( MDL ) Principle that uses the Krichevsky Trofimov ( KT ) code length to select the number of segments of a sequence . On DNA data the method selects segments that closely correspond to the annotated regions of the genes .
Keywords : sequence segmentation ; MDL ; DNA segmentation ; sequence data mining
1
Introduction
We consider the problem of segmenting a sequence of symbols into contiguous homogeneous segments . The segmentation problem has many applications in areas such as computational biology , environmental sciences , and context recognition in mobile devices , see , eg , [ 4 , 13 , 12 ] . The problem has been widely studied in different fields , for instance , in statistics it is known under the name change point detection .
Many segmentation algorithms have been proposed in the data mining community , ranging from on line to offline , from heuristic to optimal ( typically involving a dynamic programming approach ) , and from combinatorial to probabilistic , see , eg , [ 14 , 12 ] .
We consider the sequence segmentation problem as a model selection process where we fit a variable length Markov chain ( VLMC ) [ 5 ] to segments of fiContact author . Email : gwadera@cishutfi
1 the input sequence . We use the Bayesian Information Criteria ( BIC ) and a variant of the Minimum Description Length ( MDL ) Principle that uses the Krichevsky Trofimov ( KT ) code length [ 7 ] for : ( i ) fitting an optimal VLMC for each segment ; and ( ii ) determining the optimal number of segments to partition the sequence .
A d order VLMC is a Markov chain ( MC ) whose contexts ( memory ) are allowed to be of variable length . Such reduced models are also called tree models , since they can be conveniently represented by a context tree . The tree can range from a full tree in the case of an ordinary d order full MC to an empty tree in the case of a 0 MC . The variable length memory of VLMCs has the potential of capturing complex phenomena that are present in real life sequences . VLMCs provide a sparse representation of a sequence by reducing the number of parameters to be estimated . This flexibility of VLMCs is useful for the segmentation task : we can fit high order models to segments to maximize the likelihood , without being penalized for an exponential increase in the number of parameters ( as in the case of ordinary MCs ) .
The fundamental question is whether the increased modeling power of VLMCs with respect to MCs really translates into a better segmentation performance . As it turns out VLMCs can provide more accurate segmentations than MCs and are also capable of recognizing partition points in cases where MCs fail .
We fit an optimal VLMC for each segment of the sequence in order to discover segments of differing contextual regularities corresponding to different tree structures , where an optimal tree is a trade off between maximizing the maximum likelihood of the segment and the tree complexity .
The challenges in our approach are the following : ( i ) fitting an optimal VLMC to data is a non trivial task because it involves selecting among an exponential number of trees ; ( ii ) many real sources have short segments and the algorithm has to fit VLMCs from sparse data ; and ( iii ) the standard dynamic programming algorithm has a quadratic time complexity .
We solve task ( i ) by adapting known pruning algorithms of VLMC to be used with the BIC and KT criteria . We address ( ii ) by using VLMCs of variable order ( maximum depth of the tree ) with respect to segment length such that we fit taller trees to longer segments and shorter trees to shorter segments . Finally , we solve ( iii ) by applying numerous optimization techniques .
We conducted experiments on synthetic data , on a variety of DNA sequences and on natural language text sources . The results show that the method selects gene segments that closely correspond to the currently known ( annotated ) gene regions . Our segmentation system , called TreeSegment , and the data sources used in experiments are available at the authorsâ€™ web pages .
The rest of this paper is organized as follows .
In Section 2 we introduce the notion of tree models . Section 3 presents the details of the algorithm . In Section 4 we present our experimental results . Section 5 reviews related work and Section 6 is a short conclusion .
2
2 Tree models
2.1 Basic definitions
Let A = fa1 ; a2 ; : : : ; amg be an alphabet of cardinality m = jAj . Let sn 1 = s1s2 : : : sn be a string over A and let sj i = sisi+1 : : : sj . A concatenation of strings u and v is denoted by uv . A string v is a suffix of s if there exists w such that s = wv .
Let Sn
1 = [ S1 ; S2 ; : : : ; Sn ] be a stationary ergodic stochastic process over 1 =
1 = [ s1 ; s2 ; : : : ; sn ] is its realization and P ( sn
1 ) = P ( Sn alphabet A , where sn sn 1 ) .
A context tree T is a set of strings such that no string c 2 T is a suffix of another string c0 2 T . Each string c = cd 1 2 T can be visualized as a path from the root to a leaf consisting of d edges labeled by symbols cdcd,1 : : : c1 . We use to denote the empty context that corresponds to the root of the context tree . Given a context tree T , a parameter assignment ( T ) assigns to each suffix c0 of a context c 2 T ( including to c itself ) a vector of conditional probabilities
( c0 ) = [ P ( a1jc0 ) ; P ( a2jc0 ) ; : : : ; P ( amjc0) ] , where Pa2A P ( ajc0 ) = 1 .
We assign a probability P ( sjT ; ( T ) ) for the input sequence s = sn the context tree T and the probability assignments ( T ) by
1 given
P ( sjT ; ( T ) ) =
P ( si+1jci ) ;
( 1 ) n,1
Yi=0 where ci is the longest suffix of si
1 that belongs to T .
A pair ( T ; ( T ) ) is called a tree model or a d VLMC if the longest string in T has length d . In the case that all strings c 2 T are of length d and jT j = md the tree model is called d MC ( full d order Markov chain model ) . Given a tree model ( T ; ( T ) ) and an input sequence sn 1 , we can compute the probability of observing the sequence sn
1 given the model using ( 1 ) .
Example . Consider a binary context tree T = f1 ; 00 ; 10g and let ( 0 ) = [ 0:6 ; 0:4 ] , ( 1 ) = [ 0:1 ; 0:9 ] , ( 00 ) = [ 0:5 ; 0:5 ] , ( 10 ) = [ 0:3 ; 0:7 ] and ( ) = [ 0:9 ; 0:1 ] . Then from ( 1 ) for a sequence s = 101000 we obtain P ( s ) = P ( 1j ) P ( 0j1) P ( 1j10) P ( 0j1) P ( 0j10) P ( 0j00 ) = 0:1 0:1 0:7 0:1 0:1 0:3 0:5 = 105 1000000 . In the case that the tree T is known , but the parameter vector ( T ) is unknown , one can compute the maximum likelihood estimator ( MLE ) of the parameter vector denoted ^(T ) . In particular , the MLE for a conditional probability P ( ajc ) is :
^P ( ajc ) =
Nn(c ; a ) Nn(c )
; where Nn(c ; a ) and Nn(c ) = Pa2A Nn(c ; a ) are the number of occurrences of
1 , respectively . For a given input sequence sn
1 we also use
1 j^(T ) ) to denote the maximum likelihood of sn 1 . strings ca and c in sn M L = P ( sn
For a tree T we define d(T ) to be the length of the longest context . If the tree T is understood then we use d instead . We also use T jd to denote the tree that is a truncation of T to depth d .
3
A
C
G
T
TGCA
A C G T
A C G T A C G T
A
C
G
T fC ; Gg
T
A fC ; G ; T g
Figure 1 : An example of a 2 MC ( top ) and a 2 VLMC ( bottom )
In practice , given an input sequence s , a d VLMC is built using a two stage process . First a context tree of depth d is built from the sequence . Then the tree is pruned to obtain a variable depth context tree that corresponds to a contextual regularity in the input sequence . Figure 1 shows an example of a 2 MC ( top ) and a 2 VLMC ( bottom ) for an alphabet of size four . In the 2VLMC case , the leaves fC ; GgA and fC ; G ; T gC , that are connected using a dashed edge to their parents , represent virtual nodes . A virtual node is created when a parent node loses between 2 and m , 1 children nodes as a result of pruning . The idea of virtual nodes is that they represent the context of the pruned children by merging the pruned children contexts together . Thus , the node fC ; GgA represents contexts CA and GA . Clearly , we do not create a virtual node if there is only one pruned child , as this would not change the total number of children .
We use MT = fM ( T ; ( T ) ) : ( T ) 2 .(T )g to denote a set of all models sharing the same tree T ; here .(T ) is the set of all valid parameter assignments to T . If the input sequence has been generated by a VLMC , we denote by T0 the generating tree . Accordingly , 0 is the generating parameter vector and d0 is the depth of the generating tree .
A k segmentation of a sequence is a partition of the sequence in k consecutive segments . A d VLMC k segmentation is a segmentation by fitting a d VLMC and having k segments . We also use the term d VLMC segmentation to mean a segmentation that selects an optimal number of segments given a bound on the number of segments . We use the term BIC or KT segmentation with any of the above to specify the scoring function .
As an information criterion for model selection , we use a variant of the MDL principle that uses the KT code length . The MDL principle says that the best model of the process given the observed sequence is the one that gives the shortest description of the sequence , where the model itself is also a part of the description . For VLMCs , MDL has the following general form
M DLT ( sn
1 ) = LC(sn
1 jM ( T ; ( T ) ) ) + LC(T ) ;
4 where LC( ) is a real valued binary code length function of uniquely decodable binary code . Thus , LC(sn 1 jM ( T ; ( T ) ) ) is the code length of the data given the model and LC(T ) is the code length of the tree .
2.2 The Bayesian Information Criterion ( BIC ) and the
Krichevski Trofimov probability ( KT )
In this section we review the known criteria for selecting an optimal context tree , where an optimal context tree is a trade off between maximizing the maximum likelihood of the segment and the tree complexity .
BIC
For a d MC the BIC has the following form [ 7 ]
BICd(sn
1 ) = , log2(M Ld(sn
1 ) ) +
2 For a d VLMC the BIC has the following form [ 7 ]
( m , 1)md log(n ) :
( 2 ) where
BICT ( sn
1 ) = , log2(M LT ( sn
1 ) ) +
( m , 1)jT j
2 log(n ) ;
M LT ( sn
1 ) = P ( sd
1 ) Xc2T ;a2A
Nn(c ) : Nn(c ; a ) Nn(c ; a )
( 3 )
( 4 )
Thus , an optimal context tree of depth up to D , with respect to BIC , is defined as follows :
^TBIC(sn
1 ) = min
T ;d(T )D
( BICT ( sn
1 ) ) :
( 5 )
In coding terms BIC corresponds to the two stage coding [ 10 , 8 ] . The likelihood terms M Ld and M LT correspond to the code length of the data given the model while the additional terms ( m,1)md log(n ) correspond to the length of encoding of the parameters . In statistical terms , BIC has an interpretation as a maximum likelihood method . The first term measures the goodness of fit of the tree T to sn 1 , and the second term is the penalty term equal to the number of free parameters , which prevents BIC from overfitting . log(n ) and ( m,1)jT j
2
2
KT
The KT probability [ 15 ] for a 0 MC binary sequence sn probability of sn by the Dirichlet distribution D(u ) with parameters u = [ 1 and it can be expressed as
1 is defined as the average 1 over all possible parameter assignments p = 2 [ 0 ; 1 ] weighted 2 ] ( Jeffrey â€™s prior )
2 ; 1
KT0(sn
1 ) =Z 1
0 pNn(0)(1 , p)n,Nn(0)D(pju)dp ;
( 6 )
5 where Nn(0 ) is the number of zeros in sn 1 . In terms of Bayesian statistics , KT0 corresponds to the marginal likelihood [ 17 ] of sn 1 . Equation ( 6 ) can be generalized to a multi alphabet case ( m > 2 ) by using the multinomial distribution in place of the binomial . It can be shown [ 15 ] that the integral has an exact solution
KT0(sn
1 ) = Qa2A[,(Nn(a ) + 1 ,n + jAj 2
2 ) ]
:
( 7 )
The choice of prior parameter u in ( 6 ) is dictated by asymptotic properties [ 1 ] and has an effect as pseudo counts in ( 7 ) .
For VLMCs , KT can be expressed using the fact that all symbols corre1 , i.e , 1 corresponding sponding to the same context c 2 T form a memoryless subsequence of sn P ( sn to context c . This leads to the following expression :
1 jc denotes a subsequence of sn
1 jc ) , where sn
1 ) =Qc2T KT0(sn
KTT ( sn
1 ) =
1 jAjk Yc2T ;Nn(c)1
KT0(sn
1 jc ) :
( 8 )
KT is a minimizer of the worst case average redundancy Rn(T ) for the model class determined by context tree T , where Rn(T ) = . jT j(m,1 )
In coding terms , KT corresponds to the mixture coding which consists of the encoding of sn 1 and the encoding of the tree [ 10 , 8 ] . Thus , the KT MDL estimator of an optimal context tree of depth up to D is defined as follows [ 27 ] : log ( n ) [ 15 ] .
2
^TKT ( sn
1 ) = min
T ;d(T )D
( , log2(KTT ( sn
1 ) ) + LC(T ) ) :
( 9 )
In statistical terms , KT is a mixture distribution that measures the goodness of fit of the tree T to sn 1 in terms of the average probability in the model class MT . In [ 26 ] a simple code is given that describes a context tree T using LC(T ) = mjT j,1 m,1 bits .
We now are ready to define the problem of optimal sequence segmentation using tree models .
2.3 Definition of the problem of optimal sequence segmen tation using tree models
The problem of optimal sequence segmentation using tree models can be stated as follows . Given : ffl s = [ s1 ; s2 ; : : : ; sn ] : a finite sequence of categorical data over an alphabet
A , where sj i = sisi+1 : : : sj over A . ffl K : the maximal number of segments . ffl D : the maximal depth of the VLMC .
6 ffl costsj i : a cost function for segment sj i . find a vector of partition points I = [ i1 ; i2 ; : : : ; ik ] , 1 k K , 1 such that
I=arg min
[ i1;i2;:::;ik ]8< : k
Xj=0 costs jj+1,1 ij
;
+ k B9= ; where 1 k K , 1 , i0 = 1 , ik+1 = n and B is a border insertion penalty .
The cost function is either or
( sj i ) i = BIC ^TBIC costsj i = , log2KT ^TKT
( sj costsj i ) + LC( ^TKT )
( 10 )
( 11 ) depending on the corresponding tree model selection method used . In experiments we used the size of the tree as the code length of the tree , ie , we used LC( ^TKT ) = j ^TKT j .
2.4 Pruning the tree
Because it is infeasible in practice to enumerate all possible trees of a given maximum depth local search methods such as the Context algorithm [ 21 ] and the Context Tree Maximization ( CTM ) algorithm [ 27 ] have to be used . The algorithms Context and CTM work in two stages . They first build a context tree of depth d and then they recursively prune the tree starting from the leaves and proceeding bottom up .
Algorithm Context
Algorithm Context prunes a context tree as follows [ 5 ] .
For every parent node w the algorithm considers every child node uw and marks it for pruning if Nn(uw ) < m or uw < K(n ) , where
uw = Xa2A
Nn(a ; uw ) log2 ^P ( ajuw ) ^P ( ajw ) !
( 12 ) and K(n ) is a user defined threshold . If all children of w were marked for pruning then they are pruned and w becomes terminal . If at least two children were marked for pruning but there is at least one non marked child then the marked nodes are merged to create a virtual node , which represents the needed pruned contexts . In [ 21 , 22 ] the minimization of the stochastic complexity is used in place of Equation ( 12 ) as a pruning criterion .
7
The Context Tree Maximization ( CTM )
We now present the original version of the CTM algorithm [ 27 ] . CTM finds a tree maximizing ( 8 ) by a local optimization in a recursive bottom up way . For each node v in the tree CTM assigns two values : the maximum KT contribution to ( 8 ) of the contexts in the subtree rooted at v called KTmax(v ) ; and an indicator Imax(v ) that marks nodes to be included in the maximizing tree . The algorithm proceeds bottom up as follows :
1 . if v is a leaf node then KTmax(v ) = KT0(sn
1 jv )
2 . if v is an internal node then
KTmax(v ) = maxfKT0(sn
1 jv ) ; Ya2A
KTmax(av)g
( 13 )
3 . If KT0(sn
1 jv ) <Qa2A KTmax(av ) then Imax(v ) = 1 else Imax(v ) = 0 .
After having visited all nodes , KTmax(root ) contains the maximized probability and ^T0 has been marked by the indicators . To reconstruct ^T0 one has to KT ^T0 recursively read them off top down starting from the root . In terms of pruning the value of Imax(v ) has the following meaning : if Imax(v ) = 0 then all children are pruned at once ( they are not part of ^T0 ) ; while if Imax(v ) = 1 then the children are not pruned ( they are a part of ^T0 ) . Comparing to Context , CTM has to visit all nodes in the tree .
3 TreeSegment : Segmentation using Tree Mod els
In this section we present the TreeSegment algorithm that solves the problem of optimal segmentation using tree models as defined in Section 23 We start with discussing our pruning criteria .
3.1 Pruning according to Context algorithm to minimize the BIC of the tree
In this section we give a precise derivation for the threshold K(n ) of the Context algorithm that locally minimizes ( 3 ) . There is a consensus in the literature [ 25 , 5 ] that K(n ) should be of the form C log(n ) ; here we give a derivation for the value of C in detail .
We start with an example , illustrated in Figure 2 , which shows a tree rooted at a node w for alphabet A = fA ; C ; G ; T g . The tree undergoes a pruning scenario according to the Context algorithm . For simplicity we assume in this example that the virtual node is created after the first node is pruned . We number the trees ( 1) (5 ) from the left to the right . Tree ( 1 ) shows the situation before the pruning algorithm starts . Tree ( 2 ) shows the situation after pruning
8 w w w w w
A C G T fAg
C G T
( 1 )
( 2 )
G T fA ; Cg ( 3 ) fA ; C ; Gg
T
( 4 )
( 5 )
Figure 2 : VLMC pruning node Aw to w , which results in creating a virtual node fAgw . Tree ( 3 ) shows the situation after pruning Cw to w , which results in updating the virtual node to represent context fA ; Cgw . Tree ( 4 ) shows the situation after pruning node Gw to w , which results in updating the virtual node to represent context fA ; C ; Ggw . Finally , tree ( 5 ) shows the situation after the last node T w has been pruned to w and w becomes terminal . Thus , in the presented scenario , the size of the tree jT j changes from m to 1 even though it does not decrease strictly monotonically after every pruning operation because of the need to create the virtual node . For simplicity we assume that every pruning operation decreases jT j by one . Let Tu be the tree T after the terminal node u has been pruned including a possible creation of a new virtual node .
Thus , we want to prune uw to w if and only if BICTu(sn
1 ) < BICT ( sn
1 ) , where jT j , jTuj = 1 , which leads to log2 M LT ( sn
1 ) M LTu(sn
1 ) <
( m , 1 )
2 log2(n ) and from ( 12 ) we have
Nn(a ; uw ) log2 ^P ( ajuw )
^P ( ajw ) ! <
( m , 1 )
2 log2(n )
Xa2A which finally gives us
K(n ) =
( m , 1 )
2 log2(n ) :
3.2 Pruning according to CTM algorithm to minimize KT
MDL of the tree
We modify the CTM algorithm ( 13 ) to locally minimize the KT MDL score ( 9 ) as follows :
M DL(KTmax(v))=minf , log2 ( KT0(sn
1 jv ) ) + 1 ;
Xa2A , log2 ( KTmax(av ) ) + j ^T0(av)jg ;
( 14 ) where j ^T0(av)j is the size ( number of contexts ) of the optimal subtree rooted at node av . Thus , while ( 13 ) searches for a tree that maximizes the KT probability
9
( 14 ) searches for a tree that minimizes the corresponding KT MDL score . As in ( 13 ) ( 14 ) considers a local pairwise decision : parent versus children , i.e , whether the children should be part of the optimal tree or not that corresponds to pruning them off . Comparing to ( 13 ) ( 14 ) favors pruning the children by containing the term corresponding to the code length of the subtree rooted at v that is 1 bit if the children are pruned versus Pa2A j ^T0(av)j bits if the children are part of the optimal tree . As a result of it ( 14 ) produces a sparser tree than ( 13 ) .
We also implemented a refinement of criterion ( 14 ) that considers all valid subsets of children for pruning instead of the two subsets consisting of all children ( the parent node ) versus none of the children . The criterion is as follows :
M DL(KTmax(v))=min
X f , log2 ( KT0(sn
1 jv ) ) + 1 ;
Xa2A,X , log2 ( KTmax(av ) ) + j ^T0(av)j
, log2 ( KT0(sn
1 jfX gv ) ) + 1g ;
( 15 ) where : X is a subset of children , where jX j = 0 ; 2 ; : : : ; jAj , 1 ; v is the parent node ; fX gv is the virtual node ; and av is a child node .
Because of the need to consider all subsets of children criterion ( 15 ) is efficient only for small alphabet sizes and we found it useful for DNA in order to obtain a finer fitting of trees to sn
1 than by using criterion ( 14 ) .
3.3 The segmentation algorithm
In this section we present the details of the algorithm TreeSegment . The standard optimal segmentation algorithm can be expressed by the following dynamic programming equation , due to Bellman [ 2 ] :
C[k ; i ] = min k,1ji fC[k , 1 ; j , 1 ] + W [ j ; i]g :
In the above equation , C[k ; i ] is the optimal k segmentation cost of the prefix si 1 and W [ j ; i ] is the cost function ( score ) of the segment si j , that is either the BIC ( 10 ) or the KT ( 11 ) score as defined in Section 23 Computing W [ j ; i ] in j leads to overall O(n3 ) running time proportional to the length of the segment si time , which is impractical for real life sequences .
However , Algorithm 1 achieves a linear speedup by computing W [ j ; i ] in constant time ( for a fixed alphabet size and depth of the tree ) . The main idea of the speedup is that for a fixed ending position i , a fixed depth tree T1 is being built ( inductively ) starting at position i and proceeding backward for j = i ; i , 1 ; : : : ; 0 . Thus , for every pair ( j ; i ) T1 contains counts of all context strings that occur in segment si j and for the next starting position j , 1 T1 can be updated in constant time , since only one new context has to be added to it . After each updating of T1 it is copied to T2 , which is pruned to obtain the score for si j . Clearly , the cost of copying T1 to T2 and pruning of T2 is proportional to the size jT1j , which is also constant for fixed values of the parameters D and
10 m . Since the computation of W [ j ; i ] can be done in a constant time for all pairs ( j ; i ) , the overall running time of algorithm TreeSegment is ( n2 ) The space complexity of the algorithm is ( Kn )
We also find it very effective to compute the score C[k ; i ] for values of i that are a multiple of a parameter . Using this modification , we obtain a suboptimal solution , but the running time of the Algorithm is .(( n
)2 ) .
Algorithm 1 : Algorithm TreeSegment
Input : A , n , K , sn Output : I = [ i1 ; i2 ; : : : ; ik0 ] begin
1 , D ,
1
2
3
4
5
6
7
8
9
10
11
12
13
14 for i = 1 ; i n= ; i = i + do
T1:init( ) for j = i ; j 0 ; j = j , do j, +1 ) ;
T1:add(si N = i , ( j , + 1 ) + 1 ; if N < m then
W [ j ] = 1 ; else
Dmax(N ) = max0dD ( d logm(N ) ) ; T2 = T1jDmax(N ) ; T2:prune( ) ; W [ j ] = score(T2 ) ; end end for k = 2 ; k K ; k = k + 1 do
C[k ; i ] = mink,1j<i C[k , 1 ; j ] + W [ j + 1]g ; end end BackT rack( ) ; end
3.4 The maximum depth of the tree
Since we need to fit optimal trees to segments of varying length bounding the maximum depth of the tree is of a particular importance in TreeSegment for the following reasons : ( i ) it decreases the probability of overfitting while estimating the tree from a short sequence ; and ( ii ) it reduces the unnecessary computational complexity of estimating a deeper tree . The only problem with the bound is that it may increases the probability of underestimation by restricting the context length . We use Dmax(n ) logm(n ) as a bound , which follows from the fact that the if we assume that an occurrence of every context in a given position in the sequence is equally likely with probability 1 md then the length of the sequence has to be at least md to guarantee that on average every context occurs at least once .
11
3.5 Border insertion penalties
The border insertion penalty can be understood in terms of the Hidden Markov Model ( HMM ) as a transition probability between hidden states of the generating source , where segments correspond to the hidden states . Also , in MDL terms each partition point should be treated as an additional parameter and penalized appropriately . Based on our extensive experiments we selected the following penalties for the BIC and the KT scoring methods : BBIC = ( K , 1 ) log2(n ) and
BKT = PK k=2 log2 n k,1 , where K is the total number of segments . Clearly ,
BBIC follows from the BIC as a parameter penalty . BKT follows from MDL by observing that to encode the following partition points we need proportionally fewer bits , i.e , we need roughly log2(n ) bits for the first point , log2( n 2 ) for the second and so on .
2
4 Experiments
To evaluate results of segmentations obtained by TreeSegment we used the following distance measure Dseg(A ; B ) :
Dseg(A ; B ) = maxfD(A ; B ) ; D(B ; A)g ;
( 16 ) where
D(A ; B ) =
1 m Xa2A min b2Bfl d(a ; b ) n ; and A and B represent the sets of partition points of two segmentations . The measure D(A ; B ) captures the distance of each segmentation point in A to the closest segmentation point in B on average . The distance is measured as a fraction of the total length of the sequence . So , the measure D(A ; B ) takes values between 0 and 1 , where the value 0 means that the two segmentations A and B are identical , while the value 1 can be obtained only for segmentations with one segmentation points ( and being at opposite ends ) . Thus , we compute the measures Dseg(BIC ; SOU RCE ) and Dseg(KT ; SOU RCE ) , where SOU RCE , BIC , and KT are the sets of partition points corresponding to a known segmentation of the source , the BIC segmentation , and the KT segmentation , respectively .
We conducted our experiments using the following sources : synthetic data in Section 4.1 , DNA in Section 4.2 and text data in Section 43
4.1 Generated data
In this section we use a synthetic sequence to show the advantage of VLMCs over MCs in segmentation . In short , VLMCs are advantages over MCs because if segments were generated using VLMCs then MCs may miss the partition points . This follows from the fact that by using tree models we can fit high order models to segments to maximize the likelihood , without being penalized
12 for an exponential increase in the number of parameters ( as in the case of ordinary MCs ) .
As an illustration of such a case consider the following example synthetic sequence S12 of length 2n over the alphabet A = fA ; C ; G ; T g that was composed of two segments S1 and S2 of length n each generated as follows : ffl S2 was generated from a 2 VLMC model M2(T2 ; ( T2) ) , where tree T2 has only 5 nodes at depth 2 ; and ffl S1 was generated from a 1 MC M1(T1 ; ( T1) ) , where T1 = T2j1 and ( T1 ) = ( T2j1) ) , where T2j1 is a truncation of tree T2 upto depth 1 . Thus , M1 and M2 are identical in terms of 1 MC and 0 MC .
The models generating S are presented in Figure 3 . Thus , since M1 and M2 are
A
C
G
T
A
C
G
T
T1 = T2 j1
1 = 2 ( T1 )
A C fG ; T g fA ; G ; T gC
M1 ( T1 ; 1 )
M2 ( T2 ; 2 )
S1
S2
1 n
2n
Figure 3 : An example sequence S12 that requires VLMC segmentation to discover the partition point . Segment S1 was generated from model M1(T1 ; ( T1 ) ) that is a truncation upto depth 1 of model M2(T2 ; ( T2 ) ) that generated segment S2 . identical in terms of 1 MC and 0 MC MC segmentation methods will select M1 as the underlying model for the whole sequence S to avoid being penalized for a full 2 MC for S2 , ie , for useless parameters corresponding to the non existent contexts in the generating tree T2 . But a VLMC segmentation method will properly select T2 for S2 without paying for the the non existent contexts and therefore it will properly select two segments with the partition point in the middle of S .
We now present figures that show the behavior of the optimal segmentation cost ( 3.3 ) for the 2 segmentation ( k = 2 ) of S12 as a function of the partition point j for the BIC 2 MC , BIC 2 VLMC and KT 2 VLMC segmentation methods . Each figure in this section consists of 3 subplots , where each subplot presents a quantity of interest plotted as a function of the partition point in the 2 segmentation of S12 . Also , the upper two subplots depict quantities that when added up are equal to the quantity in the third subplot .
Figure 4 presents the BIC 2 MC case . We present this 2 segmentation case for comparison with the VLMC methods since we know that the algorithm
13
400
200
0
3200
3100
3000
2900
3260
3240
3220
3200
2âˆ’MC BICâˆ’penalty
200
400
600
800
1000
1200
1400
1600
1800
2âˆ’MC ML
200
400
600
800
1000
1200
1400
1600
1800
2âˆ’MC BIC
200
400
600
800 1200 Partition point for K=2
1000
1400
1600
1800
Figure 4 : BIC 2 MC method and the corresponding optimal segmentation cost for the 2 segmentation of the synthetic sequence S12 as a function of the partition point . Top : the BIC penalty . Middle : the minus maximum likelihood . Bottom : the BIC score . selected 1 segmentation using 1 MC instead of the 2 segmentation . The plots of the BIC penalty and ML are \choppy" because the model selection machinery is very poor in this case since there are only 3 trees ( 2 MC , 1 MC and 0 MC ) available to fit to both segments . Also , this figure reveals the fundamental fact of the BIC segmentation namely the contribution of the penalty is not uniform with respect to the partition point placement . The reason is that the total BICpenalty for the whole sequence is equal to mk1 ( m,1 ) log2(n , x ) where x is the partition point . Then clearly if k1 = k2 then the factor log2 ( x(n , x ) ) is the source of problem since x(n , x ) is a square function of x with the maximum at x = n 2 . log2(x ) + mk2 ( m,1 )
Figure 5 presents the BIC 2 VLMC method . The area under the BICpenalty curve is smaller for the 2 VLMC comparing to the 2 MC since here the BIC charges for only the relevant contexts . This example clearly shows the superiority of VLMCs over MCs in segmentation .
2
2
Figure 6 show the results for the KT 2 VLMC method and in particular it compares the MDL KT score ( 11 ) with the ML . A comparison of Figures 5 and 6 reveals that KT gives a more uniform penalty then BIC but at the expense of a flatter score characteristic .
4.2 DNA sequences
In this section we present the results of experiments on DNA sequences . In particular , we consider DNA sequences containing genes and their flanking regions .
14
200
150
100
50
3150
3100
3050
3000
3250
3200
3150
2âˆ’VLMC BICâˆ’penalty
200
400
600
800
1000
1200
1400
1600
1800
2âˆ’VLMC ML
200
400
600
800
1000
1200
1400
1600
1800
2âˆ’VLMC BIC
200
400
600
800 1200 Partition point for K=2
1000
1400
1600
1800
Figure 5 : BIC 2 VLMC method and the corresponding optimal segmentation cost for the 2 segmentation of the synthetic sequence S12 as a function of the partition point . Top : the BIC penalty . Middle : the minus maximum likelihood . Bottom : the BIC score .
We distinguish the following structural regions in DNA [ 6 , 28 ] : ffl 5â€™ UTR and 3â€™ UTR ( untranslated regions ) ffl CDS ( coding region ) on the directed and complementary strand ffl intron ffl 5â€™ flanking and 3â€™ flanking regions , where CDSs and UTRs are part of an exon . Thus , we consider a total of seven functional regions to be segmented by TreeSegment .
In our experiments , we start with studying viral genomes in Section 421 , and then we consider eukaryotic genes in Section 422 We obtained our viral gene sequences from http://wwwncbinlmnihgov and the eukaryotic gene sequences from http://wwwensemblorg
421 Viral genomes
The first experiment tests whether TreeSegment discovers any tree structure variation in the genomes with the simplest possible structure . For this purpose we selected complete viral genomes from a subset of ssRNA positive strand viruses that contain exactly 3 segments : UTR5â€™ , CDS and UTR3â€™ and segmented them . Our results revealed the following facts : ( i ) for every genome TreeSegment delineates at most 3 segments , where for most of the genes it delineates exactly 3 segments ; ( ii ) every CDS segment corresponds to a tree of
15
120
100
80
60
3150
3100
3050
3000
3200
3150
3100
2âˆ’VLMC MDL(KT)âˆ’ML
200
400
600
800
1000
1200
1400
1600
1800
2âˆ’VLMC ML
200
400
600
800
1000
1200
1400
1600
1800
2âˆ’VLMC MDL(KT )
200
400
600
800 1200 Partition point for K=2
1000
1400
1600
1800
Figure 6 : KT 2 VLMC method and the corresponding optimal segmentation cost for the 2 segmentation of the synthetic sequence S12 as a function of the partition point . Top : the KT MDL score minus the maximum likelihood . Middle : the minus maximum likelihood . Bottom : the KT MDL score . depth 1 D 2 ; ( iii ) the UTR segments correspond to trees of depth D = 0 ; and ( iv ) all the CDS segments have a common subtree of depth D = 1 consisting of contexts C and T .
We now show detailed results for Wisteria vein mosaic virus genome ( accession point NC 007216 ) that is a member of the family of ssRNA positive strand viruses . Figure 7 shows segmentation results and Figure 8 shows a tree built from the CDS region .
In the following experiment we segmented Bacteriophage lambda virus genome that has a long history of being used as test sequence for demonstrating new segmentation techniques [ 4 , 16 ] . The sequence contains mostly overlapping CDS segments , where there is a subsequence containing complementary strand CDS segments . Figure 9 shows results for Bacteriophage lambda that are consistent with [ 4 , 16 ] .
422 Eukaryotic genes
We now consider a more difficult task of segmenting eukaryotic genes for which the TreeSegment needs to be capable of discovering differences in tree structures between introns and exons . Therefore , we first investigate those differences . For this purpose , we repeated the following experiment for many eukaryotic organisms . We first scanned the respective genomes and then extracted the corresponding exons and introns to separate sequences . Then we fitted context trees for those two kinds of sequences using algorithm Context . As an example we present results for Caenorhabditis elegans genome in Table 1 .
16
Figure 7 : Segmentations obtained for Wisteria vein mosaic virus genome . Top : annotated sequence . Middle : segmentation obtained by BIC VLMC method . Bottom : segmentation obtained by KT VLMC method . The numbers above each segment denote the depths of the corresponding trees .
A
C
G
T fA ; C ; Gg
T
Figure 8 : CDS region of Wisteria vein mosaic virus NC 007216 .
Table 1 : Comparison of intron and exon trees for C . elegans .
Tree properties jT j D Tree intron 23 3 2 8 exon
Base content
C
0.168 0.232
G
0.164 0.239
A
0.337 0.286
T
0.327 0.242
17
Figure 9 : Segmentations obtained for Bacteriophage lambda genome . Top : annotated sequence . Middle : segmentation obtained by BIC VLMC method . Bottom : segmentation obtained by KT VLMC method . The numbers above each segment denote the depths of the corresponding trees .
The results show that there is a structural difference between the intron and exon trees , where the intron tree is bigger ( jT j = 23 versus jT j = 3 ) and ( D = 3 versus D = 2 ) . Also the exon sequence has a higher CG content while the intron sequence has a higher AT content [ 9 ] .
Given the discovered differences in tree structures , we segmented 10 example genes . The results are presented in Table 2 . By comparing the Dseg distance measure for BIC and KT we can conclude that both methods perform comparably .
Below we present details of segmentations form Table 2 . To check whether TreeSegment recognizes partition points between flanking regions and exons we segmented first Drosophila melanogaster gene CG10045 RA , and then we segmented a sequence composed of that gene and flanking regions of length 1000 . The results are shown in in Figure 10 , where GENE=[UTR5â€™ , Intron12 , CDS2 , UTR3â€™ ] and in Figure 11 , where GENE=[Flanking5â€™ , UTR5â€™ , Intron12 , CDS2 , UTR3â€™ , Flnaking3â€™ ] . Clearly , after adding the flanking regions the origins of the first exon and the second exon have been properly recognized by the BIC and KT methods . Figure 12 shows segmentation of Drosophila melanogaster gene CG5407 RA .
Figure 13 shows segmentation of Caenorhabditis elegans gene F33E113 The gene structure is as follows : GENE=[CDS1 , Intron12 , CDS2 , Intron23 , CDS3 , Intron34 , CDS4 , Intron45 and CDS5 ] . The BIC method merged two regions : Intron23 , CDS3 into one region while the KT method recognized all gene regions . Figure 14 shows segmentation of Caenorhabditis elegans gene Y50D4C3
18
Table 2 : Summary of segmentation results for DNA using BIC and KT segmentation methods , where Dseg is the distance measure and k is the number of segments .
Gene
BIC
KT
Name Wisteria vein Bacteriophage lambda Drosophila melanogaster CG10045 RA CG10045 RA+flanking CG5407 RA Caenorhabditis elegans F33E11.3 Y50D4C.3 Tetraodon nigroviridis GSTENT00014173001 Homo Sapiens ENST00000246662 Pan troglodytes Intron7 k 3 8
4 6 6
9 9
7
Dseg 0.0005 0.0149
0.0144 0.0309 0.0383
0.0243 0.0874 k 3 5
3 6 7
8 4
Dseg 0.0065 0.0150
0.0114 0.0291 0.0341
0.0034 0.0336
0.0316
13
0.0567
14
0.0348
10
0.0590
6
0.0944
4
0.0946 k 2 5
3 5 10
10 12
8
8
4
The gene structure is as follows : GENE=[CDS1 , Intron12 , CDS2 , Intron23 , CDS3 , Intron34 , CDS4 , Intron45 , CDS5 ] . The BIC method merged 7 consecutive regions starting from CDS2 while the KT method merged only 4 regions from CDS2 to Intron34 . Also the KT method produced more segments than the annotated segmentation , while the BIC method produced fewer segments .
Figure 15 shows segmentation of Tetraodon nigroviridis gene GSTENT00014173001 for which the BIC method seems to have recognized all gene segments while the KT method merged three gene regions . Also unlike in the case of gene Y50D4C.3 here the BIC method produced more segments than the KT method .
Figure 16 shows segmentation of Homo Sapiens gene ENST00000246662 . Figure 17 presents segmentation of intron7 of Pan troglodytes ( chimpanzee ) alpha fetoprotein precursor ( AFP ) gene , where intron7 is know to contain distinct homogeneous segments [ 20 ] . The results are consistent with [ 20 ] .
Concluding , the presented experiments on DNA sequences reveal two main properties of TreeSegment : ( i ) it tends to recognize boundaries exon intron in cases where the corresponding segments are appropriately long ; and ( ii ) it tends to merge consecutive smaller heterogeneous segments into one larger segment . Property ( i ) follows from the fact that long segments enable building appropriately large trees that may better fit to the segments in order to discover a finer difference between them . Property ( ii ) follows from the fact that the MDL criteria employed in TreeSegment favor a simpler model that spans a larger region instead of creating smaller segments to represent local fluctuations of the probabilistic behavior of the sequence .
19
Figure 10 : Segmentations obtained for Drosophila melanogaster gene CG10045RA . Top : annotated sequence . Middle : segmentation obtained by BIC VLMC method . Bottom : segmentation obtained by KT VLMC method . The numbers above each segment denote the depths of the corresponding trees .
4.3 Text
In this section we experiment with natural language text corpus . In Section 431 we segment a multilingual text and in Section 432 we experiment with corrupted context separation . We converted the original texts to an alphabet of cardinality 27 , which is a set consisting of the English alphabet and the space character . Every space character in the converted text corresponds to a sequence of white spaces between normal characters in the original text .
431 Multilingual text separation
In this experiment we constructed a text sequence composed from three segments : German , English and French translations of Chapter 11 of Robur the Conqueror by Jules Verne . We obtained the text data from http://www . gutenbergorg In order to convert the German and the French texts to the English alphabet we performed the following conversions . In the case of the German text we converted the sharp s to \ss" and the umlauts to \ae" , \oe" and \ue" , respectively . In the case of the French text we stripped off all accents . Figure 18 presents results of the segmentation that show a very accurate separation between the languages . As the figure shows both the BIC and the KT methods selected 1 VLMCs for each segment . As it turns out 0 MC can also separate the texts however not as accurately as 1 VLMC .
20
Figure 11 : Segmentations obtained for Drosophila melanogaster gene CG10045RA + flanking regions . Top : annotated sequence . Middle : segmentation obtained by BIC VLMC method . Bottom : segmentation obtained by KT VLMC method . The numbers above each segment denote the depths of the corresponding trees .
432 Corrupted context separation
In this experiment we constructed a text sequence composed from three segments : a scientific text segment of length 5000 , a reversed version of the first segment and a copy of the first segment . The purpose of this experiment was to test whether the algorithm was able to discover the corrupted context segment . This experiment presents the following two challenges for the algorithm : ( i ) all three segments have the same symbol composition such that the whole sequence is homogeneous in terms of 0 MC ; and ( ii ) since the outer two segments are identical they correspond the same context tree making the tree prevalent across the sequence . As it turns out 1 MC is enough to separate the segments . Figure 19 presents results that show an almost perfect separation .
5 Related work
Tree models and the algorithm Context were introduced by Rissanen in [ 21 ] . Consistency results for tree models were provided by Weinberger et al . in [ 25 ] and by B(cid:127)uhlmann and Wyner in [ 5 ] who also defined the term VLMC . The CTM was introduced by Wilems et al . in [ 26 , 27 ] . In [ 15 ] Krichevsky and Trofimov introduced KT and derived its asymptotic properties . In [ 1 ] Barron , Rissanen and Yu presented a comprehensive review of theoretical results on MDL in the context of coding and modeling . The Bayesian Information Criterion ( BIC )
21
Figure 12 : Segmentations obtained for Drosophila melanogaster gene CG5407RA . Top : annotated sequence . Middle : segmentation obtained by BIC VLMC method . Bottom : segmentation obtained by KT VLMC method . The numbers above each segment denote the depths of the corresponding trees . was introduced by Schwarz in [ 23 ] . In [ 7 ] Ciszar and Talata proved consistency results for BIC and KT as estimators of the optimal context tree .
Segmentation algorithms have been central in the analysis of genomic sequences . In [ 17 ] Liu and Lawrence presented a Bayesian approach to DNA segmentation by assuming a 0 MC model and using the KT probability . The optimal number of segments was selected using Bayesian inference . Makeev et al [ 18 ] studied a Bayesian approach to DNA segmentation by extending the idea from [ 17 ] by using heuristic border insertion penalties and filtration of boundaries .
Orlov et al . [ 19 ] presented a method for recognizing functional DNA sites and segmenting genomes . They developed a program \Complexity" for computing a context tree of a DNA sequence using the stochastic complexity [ 21 , 22 ] as a pruning criterion . Using their program they analyzed DNA sequences of various functional classes ( coding , non coding and regulatory ) and discovered that the DNA structure can be represented by trees .
The problem of DNA segmentation by model selection was posed by Li [ 16 ] , where he considered a greedy top down divide and conquer 0 MC segmentation approach to segment DNA by using the BIC and the Akaike information criterion but he did not consider gene segmentation . Also in [ 24 ] Szpankowski used 0 MC and the Shanon Jensen distance to segment DNA .
As far as gene segmentation the distinctive statistical properties of gene functional regions are well documented in the literature , eg , see [ 9 , 28 , 11 ] . In particular the detection of genes has been based on the non uniform codon
22
Figure 13 : Segmentations obtained for Caenorhabditis elegans gene F33E113 Top : annotated sequence . Middle : segmentation obtained by BIC VLMC method . Bottom : segmentation obtained by KT VLMC method . The numbers above each segment denote the depths of the corresponding trees . usage in protein coding segments and has been modeled by non uniform Markov models and HMMs [ 6 ] . Bernaola et al . [ 3 ] proposed using entropic segmentation for finding borders between coding and non coding DNA regions .
6 Conclusions
We presented a segmentation method that uses tree models to partition the input sequence into segments of differing contextual regularities that correspond to differnt tree structures . The MDL principle is used to guide the segmentation process by deciding the optimal tree model in each segment and by deciding the overall number of segments . In our experiments we demonstrated that VLMCs can provide more accurate segmentations than MCs and are also capable of recognizing partition points in cases where MCs fail . In the experiments on DNA we showed usefulness of our method for gene segmentation .
References
[ 1 ] A . Barron , J . Rissanen , and B . Yu . The minimum description length principle in coding and modeling . IEEE Transactions on Information Theory , 44(6):2743{2760 , 1998 .
[ 2 ] R . Bellman . On the approximation of curves by line segments using dy namic programming . Communications of the ACM , 4(6):284 , 1961 .
23
Figure 14 : Segmentations obtained for Caenorhabditis elegans gene Y50D4C3 Top : annotated sequence . Middle : segmentation obtained by BIC VLMC method . Bottom : segmentation obtained by KT VLMC method . The numbers above each segment denote the depths of the corresponding trees .
[ 3 ] P . Bernaola Galvan , I . Grosse , P . Carpena , J . Oliver , R . Roman Roland , and H . Stanley . Finding borders between coding and noncoding dna regions by an entropic segmentation method . Physical Review Letters , 85(6):1342{ 1345 , 2000 .
[ 4 ] J . Braun and H . Muller . Statistical methods for dna sequence segmentation .
Statistical Science , 13(2):142{162 , 1998 .
[ 5 ] P . B(cid:127)uhlmann and A . Wyner . Variable length Markov chains . Annals of
Statistics , 27:480{513 , 1999 .
[ 6 ] Ch . Burge and S . Karlin . Prediction of complete gene structures in human genomic dna . Journal of Molecular Biology , 268:78{94 , 1997 .
[ 7 ] I . Csiszar and Z . Talata . Context tree estimation for not necessarily finite memory processes , via bic and mdl . IEEE Transactions on Information Theory , 52(3):1007{1016 , 2006 .
[ 8 ] P . Gr(cid:127)unwald . A Tutorial introduction to the minimum description length principle . In : Advances in Minimum Description Length : Theory and Applications . MIT Press , 2005 .
[ 9 ] R . Guigo and J . Fickett . Distinctive sequence features in protein coding genic non coding , and intergenic human dna . Journal of Molecular Biology , 253:51{60 , 1995 .
24
Segmentations obtained for Tetraodon nigroviridis
Figure 15 : gene GSTENT00014173001 . Top : annotated sequence . Middle : segmentation obtained by BIC VLMC method . Bottom : segmentation obtained by KT VLMC method . The numbers above each segment denote the depths of the corresponding trees .
[ 10 ] M . Hansen and B . Yu . Model selection and the principle of minimum description length . Journal of the American Statistical Association , 96(454):746{774 , 2001 .
[ 11 ] H . Herzel and I . Grosse . Correlations in dna sequences : The role of protein coding segments . Physical Review Letters , 55(1):800{810 , 1997 .
[ 12 ] H . Mannila J . Tikanmki J . Himberg , K . Korpiaho and H . Toivonen . Time series segmentation for context recognition in mobile devices . In First IEEE International Conference on Data Mining , pages 203{210 , October 2001 .
[ 13 ] Ath . Kehagias . A hidden markov model segmentation procedure for hydrological and environmental time series . Stochastic Environmental Research and Risk Assessment ( SERRA ) , 18(2):117{130 , April 2004 .
[ 14 ] Eamonn J . Keogh , Selina Chu , David Hart , and Michael J . Pazzani . An online algorithm for segmenting time series . In ICDM , pages 289{296 , 2001 .
[ 15 ] R . Krichevsky and V . Trofimov . The performance of universal encoding .
IEEE Transactions on Information Theory , IT 27(2):199{207 , 1981 .
[ 16 ] Wentian Li . DNA segmentation as a model selection process . In International Conference on Research in Computational Molecular Biology , pages 204{210 , 2001 .
25
Figure 16 : Segmentations obtained for Homo Sapiens gene ENST00000246662 . Top : annotated sequence . Middle : segmentation obtained by BIC VLMC method . Bottom : segmentation obtained by KT VLMC method . The numbers above each segment denote the depths of the corresponding trees .
[ 17 ] S . Liu and C . Lawrence . Bayesian inference of biopolymer models . Bioin formatic , 15:38{52 , 1999 .
[ 18 ] V . Makeev , V . Ramensky , M . Gelfand , M . Roytberg , and V . Tumanyan . Bayesian approach to dna segmentation into regions with different average nucleotide composition . Lecture Notes in Computer Science , 2066:54{73 , 2000 . Computational Biology .
[ 19 ] Y . Orlov , V . Potapov , and V . Filipov . Recognizing functional dna sites and segmenting genomes using the program "complexity" . In Proceedings of BGRS 2002 , volume 3 , pages 244{247 . Novosibirsk Insititute of Cytology and Genetics Press , 2002 .
[ 20 ] D . Henderson R . Boys and D . Wilkinson . Detecting homogeneous segments in dna sequences by using hidden markov models . Applied Statistics , 49(2):269{285 , 2000 .
[ 21 ] J . Rissanen . A universal data compression system . IEEE Transactions on
Information Theory , IT 29(5):656{664 , 1983 .
[ 22 ] J . Rissanen . Fast universal coding with context models . IEEE Transactions on Information Theory , 45(4):1065{1071 , 1999 .
[ 23 ] Gideon Schwarz . Estimating the dimension of a model . The Annals of
Statistics , 7(2):461{464 , 1978 .
26
Figure 17 : Intron7 from Pan troglodytes alpha fetoprotein precursor ( AFP ) gene ( accession U21916 ) . Top : known regions . Middle : segmentation obtained by BIC VLMC method . Bottom : segmentation obtained by KT VLMC method . The numbers above each segment denote the depths of the corresponding trees .
[ 24 ] W . Szpankowski , W . Ren , and L . Szpankowski . An optimal DNA segmentation based on the MDL principle . In IEEE Computer Society Bioinformatics Conference , pages 541{546 , 2003 .
[ 25 ] M . Weinberger , J . Rissanen , and M . Feder . A universal finite memory source . IEEE Transactions on Information Theory , 41(3):643{652 , 1995 .
[ 26 ] F . Willems , Y . Shtarkov , and T . Tjalkens . The context tree weighting method : Basic properties . IEEE Transactions on Information Theory , IT41:653{664 , 1995 .
[ 27 ] F . Willems , Y . Shtarkov , and T . Tjalkens . Context tree maximizing . In
Conference on Information Sciences and Systems , pages 7{12 , 2000 .
[ 28 ] M . Zhang . Statistical features of human exons and their flanking regions .
Human Molecular Genetics , 7(5):919{932 , 1998 .
27
Figure 18 : Multilingual text separation : German , English and French . Top : segments corresponding to the three languages . Middle : segmentation obtained by BIC VLMC method . Bottom : segmentation obtained by KT VLMC method . The numbers above each segment denote the depths of the corresponding trees .
Figure 19 : Corrupted context separation . Top : original segment . Middle : segmentation obtained by BIC VLMC method . Bottom : segmentation obtained by KT VLMC method . The numbers above each segment denote the depths of the corresponding trees .
28
