Star Structured High Order Heterogeneous Data
Co clustering based on Consistent Information Theory
Bin Gao Tie Yan Liu Wei Ying Ma
Microsoft Research Asia
4F , Sigma Center , No . 49 , Zhichun Road
Beijing , 100080 , P . R . China
{bingao , tyliu , wyma}@microsoft.com two the co clustering of
In the past decades , researchers have proposed many types of technologies on heterogeneous data ( denoted by pair wise co clustering ) . Representative work includes information bottleneck coclustering [ 6][10 ] , bipartite spectral graph partitioning [ 4 ] , and information theoretic co clustering [ 1][5 ] . According to [ 5 ] , the information theoretic method is much more scalable and efficient than the other methods . In this particular method , a two dimensional co occurrence table , which describes the relations among data objects , was viewed as an empirical joint probability distribution of two discrete random variables . Accordingly , the coclustering problem was posed as an optimization problem in co clustering maximized the mutual information [ 3 ] between the clustered random variables subject to constraints on the number of row and column clusters in the co occurrence table . Besides the above work on pair wise co clustering , researchers also made some efforts on the co clustering of multiple types of objects ( denoted by high order coclustering ) . For example , Wang et al [ 11 ] proposed an iterative method named ReCoM to cluster multi type interrelated Web objects . the optimal information theory :
Abstract
Heterogeneous object co clustering has become an important research topic in data mining . In early years of this research , people mainly worked on two types of heterogeneous data ( denoted by pair wise co clustering ) ; while recently more and more attention was paid to multiple types of heterogeneous data ( denoted by highorder co clustering ) . In this paper , we studied the highorder co clustering of objects with star structured interrelationship , ie , there is a central type of objects that connects the other types of objects . Actually , this case could be a very good model for many real world applications , such as the co clustering of Web images , their low level visual features , and the surrounding text . We used a tripartite graph to represent the interrelationships among different objects , and proposed a consistent information theory which generates an effective algorithm to obtain the co clusters of different types of objects . Experiments on a Web image show that our proposed algorithm is a better choice compared with previous work on heterogeneous object co clustering .
1 . Introduction
Homogeneous data clustering has been extensively studied in the literature . However , in real applications nowadays , people often need to deal with the co clustering of heterogeneous data objects . For example , in order to conduct personalized search , one may need to co cluster three types of objects in the click through log of Web search engines , ie , Web users , issued queries , and clicked pages . One may also need to co cluster Web images , their low level visual features , and surrounding text for a friendly user interface of an image search engine . And one may need to co cluster papers , authors , conferences , and journals in order to mine communities in the research society . In the above examples , not only features of the objects , but also their inter relationships are heterogeneous . In such cases , homogeneous clustering technologies cannot work well any longer . in
In this paper , we would like to study a special topology of star structured high order heterogeneous data as shown in Figure 1(a ) In this structure , there is a central type of objects that connects the other types of objects . This case could be a very good abstraction for many real world the co clustering of authors , applications , such as conferences , papers , and keywords academic publication systems ( corresponding to Figure 1(a)(ii ) , where paper is the central data type ) , in order to identify that a certain group of authors usually write papers of a certain series of topics using a certain list of keywords , and submit them to a certain kind of conferences . To solve the problem of star structured high order co clustering , we designed an algorithm named consistent bipartite graph co partitioning ( CBGC ) based on the consistency theory proposed in [ 7][8 ] . Although experiments showed this algorithm is very effective , it is not very efficient in largescale datasets because CBGC is solved by semi definite programming ( SDP ) , which is time consuming in largescale cases .
Proceedings of the Sixth International Conference on Data Mining ( ICDM'06)0 7695 2701 9/06 $20.00 © 2006
( i )
( ii )
( iii )
( a )
( b )
Figure 1 . ( a ) Star structured high order heterogeneous data . ( b ) The tripartite graph of heterogeneous objects . to solve high order co clustering
The motivation of this paper is to develop a more the aforementioned starefficient method problem . For structured simplicity , we only focus on the case as shown in Figure 1(a)(i ) In order to get a high order co clustering algorithm with both high effectiveness and speed , we extend the information theoretic co clustering algorithm [ 5 ] to the high order case , once again based on the consistency theory . Similar to [ 7][8 ] , we use a tripartite graph to represent the inter relationships among different types of objects . Figure 1.(b ) shows a tripartite graph which consists of three types of heterogeneous objects : X={x1 , x2,… , xm} , Y={y1 , y2,… , yn} , and Z={z1 , z2,… , zl} . We model the co clustering of X , Y , and Z as the consistent fusion of two pair wise co clustering subproblems . That is , we look for such two partitions for the sub problems of X Y co clustering and Z Y co clustering , provided that their clustering results on the central type Y are the same . Then for each sub problem , we adopt the information theoretic co clustering algorithm to get the desirable clusters , and adjust these clusters by considering the clustering sub problem . Experiments on a Web image dataset show that this method is almost as effective as the CBGC method , while it is much faster . In this regard , this method is more suitable co clustering applications .
2 . Problem formulation results of large scale the other real world for
In the following two sections , we will mainly discuss how to extend the information theoretic co clustering method to handle the star structured high order cases . First of all , we will introduce a probability model to represent inter relationship among heterogeneous objects , which is an extension of that used in [ 4 ] . the
Let X , Y , and Z be discrete random variables that take values from the sets {x1 , , xm} , {y1 , , yn} , and {z1 , , zl} , representing the three types of objects . Denote the joint probability distributions between X and Y , and between Z and Y as the m n× matrix p1(X,Y ) and the l n× matrix p2(Z,Y ) . Our target is to cluster X , Y , and Z into r , s , and t disjoint ( or hard ) clusters simultaneously . , Suppose the clusters of X , Y , and Z are
1ˆ { , , }r x
ˆ x
, we are actually seeking the
( ) i C x 1
:{ , , } { , , }
→ x m
1ˆ x
,
ˆ x r . In
ˆ y
, and
1ˆ { , , }s y maps CX , CY , and CZ , ie ,
1ˆ { , , }t z
ˆ z
X ( ) iii C z 1 , ˆ
=
Z
ˆ y s
1ˆ y
Y
Y
X
)
(
,
=
=
ˆ z t y n
→
( )
:{ , , }
Y C Y
X C X
Z C Z
:{ , , } { , , }
1ˆ { , , } z z l , and ˆ
( ) ii C y → 1 brief , we denote ˆ Based on the above notations , we have the following definitions . Definition 1 . We refer to the tuple ( CX , CY ) as a coclustering . Definition 2 . We refer to the star structured triple ( CX , CY , CZ ) as a consistent co clustering , where CY is the mapping corresponding to the central data type .
.
)
(
Z
A traditional and fundamental quantity that measures the amount of information that random variable X contains about Y ( and vice versa ) is the mutual information I(X,Y ) [ 3 ] . As shown in Definition 3 , similar to [ 5 ] , the resultant loss in mutual information is adopted in this paper to judge the quality of a co clustering . Definition 3 . An optimal co clustering minimizes
) I X Y
(
,
ˆ I X Y
ˆ
(
,
−
) subject to the constraints on the number of row and column clusters in the probability matrix
1( p X Y ,
)
, where denotes the mutual information , ie ,
( , ) I ⋅ ⋅ = ∑ ∑
) x
( I X Y
, p x y 1
( ,
) log
( p x y 1
( ,
) ( y p x p y 1
( )
( ) )
1
Similarly , we have ( , I Z Y
= ∑ ∑
)
2 z y p z y
( ,
) log
( p z y
( ,
2
) ( p z p y
( )
( ) )
2
2
)
. ( 1 )
)
. ( 2 )
It was proved in [ 5 ] that the loss of mutual information could be obtained by calculating a Kullback Leibler ( KL ) [ 3 ] divergence . For the star structured high order coclustering , we can have the similar conclusion as follows after some deductions . Lemma 1 . For a fixed consistent co clustering ( CX , CY , CZ ) , we can write the loss in mutual information as
I X Y
(
,
)
−
ˆ I X Y
ˆ
(
,
)
=
D p X Y
(
(
,
1
) || q X Y 1
(
,
) )
, ( 3 )
I Z Y
(
,
)
−
ˆ I Z Y
ˆ
(
,
)
= where
D ⋅
( || ) ⋅ denotes
D p Z Y
(
(
,
) || q Z Y
(
,
) )
, ( 4 )
2 the Kullback Leibler
2
( KL ) divergence , also known as relative entropy , and q1(X,Y ) and q2(Z,Y ) are distributions of the following forms : ˆ y
∈ , ( 5 )
ˆ ˆ ( , )
ˆ , x y
( |
∈
) ,
=
ˆ
ˆ
)
(
|
( , ) q x y 1 ( , ) q z y
= p x y p x x p y y where x 1 p z y p z z p y y where z
ˆ ( | )
ˆ ˆ ( , )
) ,
ˆ
(
|
1
1
2
2
2
2 According to the consistency theory [ 7][8 ] , we divide the original X Y Z co clustering problem into two subproblems : X Y co clustering and Z Y co clustering , with the constraints that their clustering results on the central type Y are exactly the same and the overall partitioning is optimal under a certain objective function . A simple but
∈
ˆ , z y
∈ . ( 6 )
ˆ y
Proceedings of the Sixth International Conference on Data Mining ( ICDM'06)0 7695 2701 9/06 $20.00 © 2006 feasible objective function could just be the linear combination of the two KL divergences in ( 3 ) and ( 4 ) . Definition 4 . The objective function of the consistent coclustering on ( CX , CY , CZ ) is defined as ) )
D p X Y
F X Y Z
α=
) ||
(
(
(
(
)
,
,
,
,
+ −
( 1
) α
1 D p Z Y
(
(
,
2 q X Y 1 ,
) ) ,
) || q Z Y where
(
0
<
α
< . ( 7 )
1
2
In the right hand side of the above formula , the first term stands for the objective function for the sub problem of X Y co clustering , while the second one stands for that of Y Z co clustering . Parameter α is a weighting factor determining which local bipartite graph we trust more .
3 . Consistent information theoretic coclustering
According to [ 5 ] , we have the following proposition and lemma after some extensions . Proposition 1 . For the sub problems of X Y co clustering and Z Y co clustering in a fixed ( or hard ) consistent coclustering ( CX , CY , CZ ) , there holds D p X Y q X Y
ˆ ˆ , ) ) D p X Y X Y q X Y X Y
ˆ ˆ , ) ||
, ( 8 )
, ) ||
, ) )
=
(
(
(
(
(
(
,
,
,
, computing row clusters and column clusters . Furthermore , it can be proved that this interactive process can gradually maximize the mutual information between the clustered random variables in a reinforcing manner . To solve the overall consistent co clustering problem , we first use the above idea to get the solution of each sub problem , and then determine the clustering result for the central type of object Y by minimizing the objective function as defined in Definition 4 , based on the foregoing clustering results of X and Z . This process can be conducted in an iterative way , until the co clustering results become stable . More the Consistent Information specifically , we propose Theoretic co clustering algorithm ( CIT ) as shown in Table 1 star structured high order heterogeneous data co clustering problem . solve the to
Table 1 . The CIT algorithm .
ALGORITHM CIT ( p1 , p2 , r , s , t , α , &CX , &CY , &CZ )
Input : p1 : the joint probability distributions of X and Y ; p2 : the joint probability distributions of Z and Y ; r : the desired cluster number of X ; s : the desired cluster number of Y ; t : the desired cluster number of Z .
D p Z Y q Z Y
( , ) ||
( , ) )
(
=
ˆ ˆ , ) ) D p Z Y Z Y q Z Y Z Y
ˆ ˆ , ) ||
( ,
( ,
(
,
,
. ( 9 )
Output : The mapping functions CX , CY and CZ .
1
2
1
2
1
2
1
2
Lemma 2 . For either of the sub problems of X Y coclustering and Z Y co clustering in a fixed ( or hard ) consistent co clustering ( CX , CY , CZ ) , the loss in mutual information can be expressed as ( i ) a weighted sum of the relative entropies between row distributions and “ rowlumped ” distributions , or as ( ii ) a weighted sum of the relative entropies between column distributions and “ column lumped ” distributions , that is ,
) || q X Y X Y 1
) )
(
,
,
,
ˆ
ˆ
1 . Initialization : Set i=0 . Start with some initial partition functions
( 0 )
C C and
,X
( 0 ) Y
( 0 )
ZC . Compute
ˆ ( 0 ) q X Y q X X q Y Y 1
ˆ ˆ , ) ,
( 0 ) 1
( 0 ) 1
) ,
ˆ
(
(
(
|
|
) ,
( 0 ) q Z Y q Z Z q Y Y ) , 2
( , ) ,
( |
( 0 ) 2
( 0 ) 2
) ,
(
|
ˆ ˆ
ˆ
ˆ and distributions
( 0 ) q 1
ˆ ( Y x
|
) , 1
≤ ≤ and
ˆ x r
( 0 ) ˆ q Y z 2
| ),1
(
≤ ≤ t
ˆ z using
( ) i ˆ ( | ) q y x 1
=
( ) i ˆ ˆ ( | ) q y y q y x 1
ˆ ( | )
( ) i 1
,
( ) i ˆ ( | ) q y z 2
=
( ) i ˆ ˆ ( | ) q y y q y z 2
ˆ ( | )
( ) i 2
.
2 . Compute X clusters . For each x , find its new cluster index as
C
+
( 1 ) i X
( ) x
= arg min
ˆ x
D p Y x
(
(
|
1
) ||
( ) i q 1
ˆ ( Y x
|
) )
, p x D p Y x 1
( )
(
(
|
1
) ||
ˆ q Y x 1
(
|
) )
, ( 10 ) resolving ties arbitrarily . Let
( 1 ) i + = C Y
( ) i C Y
.
) || q X Y X Y 1
) )
(
,
,
,
ˆ
ˆ p y D p X y 1
( )
(
(
|
1
) ||
ˆ q X y 1
(
|
) )
. ( 11 )
3 . Compute distributions ( 1 )
+ i q 1
ˆ ˆ ( 1 ) i ( , ) , X Y q 1
+
(
ˆ
( 1 ) i X X q 1
) ,
|
+
ˆ ( | ) Y Y
ˆ y
: y C y
(
Y
)
=
ˆ y and the distributions
+ iq
( 1 ) 1
(
ˆ X y
|
) , 1
≤ ≤ using
ˆ y s
) ||
) ) q Z Y Z Y
(
,
,
,
ˆ
ˆ
2 p z D p Y z
( )
(
(
|
2
) ||
ˆ q Y z
(
|
2
) ||
) ) q Z Y Z Y
(
,
,
,
ˆ
ˆ
2
) )
, ( 12 )
+
( 1 ) i q 1
ˆ ( | ) x y
=
+
( 1 ) i q 1
( 1 ) i ˆ ( | ) x x q 1
+
ˆ ˆ ( | ) x y
.
4 . Compute Y clusters . For each y , find its new cluster index as
+
( 2 ) i C Y
( ) y
= arg min
ˆ y p X y 1
(
|
) ||
+
( 1 ) i q 1
(
ˆ ) X y
| p y D p Z y
( )
(
(
|
2
) ||
ˆ q Z y
(
|
2
) )
. ( 13 ) resolving ties arbitrarily . Let
+
2 )
C
( i X
=
C
+
( 1 ) i X
.
1
,
(
ˆ
ˆ ( , D p X Y X Y = ∑ ∑
,
ˆ x
( ) : x C x
X
=
ˆ x
1
,
(
ˆ
ˆ ( , D p X Y X Y = ∑ ∑
,
2
,
(
ˆ
ˆ ( , D p Z Y Z Y = ∑ ∑
,
2
ˆ z
( ) : z C z
Z
=
ˆ z
2
,
(
ˆ
ˆ ( , D p Z Y Z Y = ∑ ∑
,
2
ˆ y
: y C y
(
Y
)
=
ˆ y
Lemma 2 shows that in either of the sub problems , we can express the objective function solely in terms of the row clustering , or in terms of the column clustering . Then , for example , in X Y co clustering , we can define the q Y x as a row cluster prototype , and distribution 1
ˆ
)
(
| similarly , the distribution q X y as a column cluster 1
(
)
|
ˆ prototype . Based on this intuition , the co clustering of each sub problem can be calculated by iteratively
5 . Compute distributions
( q 1 i
+
2 )
(
ˆ ( X Y q 1
) ,
ˆ
, i
+
2 )
(
ˆ
( X X q 1
) ,
| i
+
2 )
ˆ ( Y Y
|
) and the distributions
+
2 ) iq
( 1
ˆ ( Y x
|
) , 1
≤ ≤ using r
ˆ x
( ) i q 1
(
ˆ y x
|
)
=
( ) i q 1
(
( ) i y y q 1
ˆ
)
|
(
ˆ ˆ y x
|
)
.
6 . If the number of the process loop of Steps 2~5 exceeds the scheduled value , or the change in objective function value of the X Y sub problem , that is ,
Proceedings of the Sixth International Conference on Data Mining ( ICDM'06)0 7695 2701 9/06 $20.00 © 2006 D p X Y q X Y
, ) ||
, ) )
(
(
(
1
( ) i 1
−
( 2 ) i D p X Y q 1
, ) ||
(
(
+
1
(
X Y
, ) )
, is small , go to Step 7 ; otherwise , go to Step 2 .
7 . Compute Z clusters . For each z , find its new cluster index as
C
+
( 1 ) i Z
( ) z
= arg min
ˆ z
D p Y z
(
(
|
2
) || q
( ) i 2
ˆ ( Y z
|
) )
, resolving ties arbitrarily . Let
( 1 ) i + = C Y
( ) i C Y
.
8 . Compute distributions ( 1 )
+ i q 2
(
ˆ
ˆ ( 1 ) i Z Y q 2
) ,
,
+
(
ˆ
( 1 ) i Z Z q 2
) ,
|
+
ˆ ( Y Y
|
) and the distributions
+ iq
( 1 ) 2
(
ˆ Z y
|
) , 1
≤ ≤ using n
ˆ y q
+
( 1 ) i 2
(
ˆ z y
|
)
= q
+
( 1 ) i 2
( z z q
)
ˆ
|
+
( 1 ) i 2
(
ˆ ˆ z y
|
)
.
9 . Compute Y clusters . For each y , find its new cluster index as
+
( 2 ) i C Y
( ) y
= arg min
ˆ y
D p Z y
(
(
|
2
) ||
+
( 1 ) i q 2
(
ˆ Z y
|
) ) resolving ties arbitrarily . Let
+
2 )
C
( i Z
=
C
+
( 1 ) i Z
.
10 . Compute distributions ( i q 2
+
2 )
(
ˆ ˆ ( i , Z Y q 2
) ,
+
2 )
(
ˆ
( i Z Z q 2
) ,
|
+
2 )
ˆ ( Y Y
|
) and the distributions
+
2 ) iq
( 2
ˆ ( Y z
|
) , 1
≤ ≤ using
ˆ z t q
( ) i 2
(
ˆ y z
|
)
= q
( ) i 2
(
( ) i y y q 2
ˆ
)
|
(
ˆ ˆ y z
|
)
.
11 . If the number of the process loop of Steps 7~10 exceeds the scheduled value , or the change in objective function value of the Z Y sub problem , that is ,
( 2 ) i D p Z Y q Z Y D p Z Y q 2
( , ) ||
( , ) ||
( , ) )
( ) i 2
−
(
(
+
2
2
( , ) ) Z Y
, is small , go to Step 12 ; otherwise , go to Step 7 .
12 . Compute Y clusters under the concept of consistency . For each y , find its new cluster index as
+
( 2 ) i C Y
( ) y
= arg min [ ˆ y
α p y D p X y 1
( )
(
(
|
1
) ||
+
( 1 ) i q 1
(
ˆ X y
|
) )
( 14 )
+ −
( 1
) α p y D p Z y
( )
(
(
|
) ||
2
2
+
( 1 ) i q 2
(
ˆ Z y
|
) ) ] resolving ties arbitrarily ( 0
1α<
< ) . Let
+
2 )
C
( i X
=
C
+
( 1 ) i X and
+
2 )
C
( i Z
+
1 )
=
C
( i Z
.
13 . Compute distributions
ˆ ( 2 ) i + ( | ) , q X Y q X X q Y Y 1
ˆ ˆ ( , ) ,
( | ) ,
( 2 ) i + 1
( 2 ) i + 1
ˆ
ˆ ( 2 ) i +
( | ) q ZY q Z Z q Y Y 2
ˆ ( | ) ,
ˆ ˆ ( , ) ,
( 2 ) i + 2
( 2 ) i + 2 and distributions
+
2 ) iq
( 1
ˆ ( Y x
| ) , 1
≤ ≤ and
ˆ x r
+ iq
( 2 ) 2
ˆ ( Y z
| ),1
≤ ≤ t
ˆ z using
( ) i ˆ ( | ) q y x 1
=
( ) i ˆ ˆ ( | ) q y y q y x 1
ˆ ( | )
( ) i 1
,
( ) i ˆ ˆ ( | ) q y z q y y q y z 2
ˆ ( | )
( | )
2ˆ
( ) i 2
=
( ) i
.
14 . Stop and return
C
X
=
2 )
C +
( i X
,
C Y
= i
(
C +
Y
2 ) and
C
Z
= i
(
C +
Z
2 ) if the change in objective function value , that is ,
 α 
D p X Y
(
(
,
1
) ||
( ) i q X Y 1
(
,
) )
−
( 2 ) i D p X Y q 1
) ||
(
(
,
+
1
(
X Y
,
) )
 
+ −
( 1
) α
 
D p Z Y q Z Y
( ,
( ,
) ||
(
2
( ) i 2
) )
−
D p Z Y
( ,
(
2
) ||
+
( 2 ) i q 2
( , Z Y
 ) ) ,  where 0
1α<
< is small ; else , set i=i+2 and go to step 2 .
Overall speaking , at the beginning of each iteration of the CIT algorithm , we calculate the clusters of X and Y through the sub problem of X Y information theoretic co clustering ( see steps 2~6 ) . Actually , instead of processing this sub problem till it converges , we stop it after a few iterations and output the clustering of X . This is because that the aim of the above operation is to obtain a relatively good initial clustering of X for calculating the clustering of Y , rather than to get an accurate clustering of X . Similarly , the clusters of Z and Y are calculated through the sub problem of Z Y information theoretic co clustering and the clustering of Z is outputted after several iterations ( see steps 7~11 ) . Then , the new clusters of Y are calculated under the concept of consistency , by minimizing the loss function ( 14 ) ( defined in Definition 4 ) . After that , the clusters of X , Y , and Z are all updated ( Steps 12~14 ) . This iterative process stops when the objective function no longer decreases .
Note that the convergence of this algorithm can be well proved . That is , this algorithm can monotonically decrease the objective function as given in Definition 4 , and terminate in a finite number of iterations . We omitted the details of the proof due to the space restriction .
4 . Experimental results
Web image clustering is a technology to help users digest the large amount of online visual information . Many traditional methods on image clustering only used either the low level visual features inside the images or the surrounding text in the corresponding Web pages . Considering that these two kinds of information are complementary , one can expect better clustering results if we are able to utilize both of them in an effective way . Low level visual features , images , and surrounding text can just make up a star structure where images are the central type of objects . Therefore , they can be well represented by the tripartite graph as shown in Figure 1.(b ) and thus be solved by the CIT algorithm . In this section , we would like to show some evaluation results on this task .
41 Data preparation
The image data used in our experiments were crawled from the Photography Museums and Galleries of the Yahoo! Directory . Images and their surrounding texts were extracted from the crawled Web pages . After removing some low quality data , the remaining 17,000 images were assigned to 48 categories manually .
In our experiment , we randomly selected 10 categories of images from this dataset , the names of which are listed in Table 3 . We extracted 530 dimension color and texture features in total as the low level visual representation of the images ( See Table 2 ) to build the {visual feature} byimage matrix A . To generate the {term in surrounding text} by image matrix B , we removed the stop words and
Proceedings of the Sixth International Conference on Data Mining ( ICDM'06)0 7695 2701 9/06 $20.00 © 2006 then the remaining words were regarded as textual representations of the images in our experiments . The dimensionality of the textual features ranges from several hundreds to more than one thousand , changing with different subset of images .
Table 2 . The low level features extracted from images .
Feature category
Feature Name
Dimensions
Color
Texture
Color Histogram Features Color Moment Features Color Coherence Features Tamura Texture Features
Wavelet Features [ 2 ]
MRSAR [ 9 ]
256
9
128 18 104 15
42 Average performance
We set parameter α = 0.4 and report the clustering performance for all possible pairs of categories in the selected image dataset . We plot the comparison between CIT accuracy and CBGC accuracy in Figure 2 , each point in which represents a category pair . We can see that most of the points fall in the upper side of the diagonal , indicating that the CIT algorithm outperforms the CBGC method in most cases . We also plot the comparison between CIT time cost and CBGC time cost in Figure 3 , which shows definitely that the proposed algorithm is much more efficient than the CBGC algorithm . ( In Figure 3 , the indexes on the horizontal axis were a random permutation of all possible pairs of categories in the selected image dataset . ) To sum up , the CIT algorithm is a better solution to the high order co clustering problem .
1
0.9
0.8
0.7
0.6
0.5 y c a r u c c A T C
I
0.4
0.4
0.5
0.7
0.6 0.8 CBGC Accuracy
0.9
1
Figure 2 . Accuracy comparison .
30
25
20
15
10
5
) s d n o c e s ( t s o C e m T i
0
0
5
10
CBGC
CIT
35
40
45
20
15 30 Index of Category Pairs
25
5 . Conclusions
In this paper , we proposed a novel algorithm based on the consistent information theory for co clustering highorder heterogeneous data . This algorithm can be regarded as another realization of the consistency theory , and can also be regarded as an extension of the informationtheoretic co clustering algorithm . Experiments showed that it is a good choice for the co clustering of multi type inter related data objects , in terms of both efficiency and effectiveness .
7 . References
[ 1 ] Banerjee , A . , Dhillon , I . S . , Ghosh , J . , Merugu , S . , and Modha , D . S . A generalized maximum entropy approach to Bregman co clustering and matrix approximations . In Proceedings of ACM SIGKDD’04 , pages : 509 514 , 2004 .
[ 2 ] Chang , T . , and Kuo , CCJ Texture analysis and classification with tree structured wavelet transform . IEEE Transactions on Image Processing , vol . 2 , no . 4 , pages : 429 441 , 1993 .
[ 3 ] Cover , T . and Thomas , J . Elements of Information Theory .
John Wiley & Sons , New York , USA , 1991 .
[ 4 ] Dhillon , IS Co clustering documents and words using bipartite spectral graph partitioning . In Proceedings of ACM SIGKDD’01 , pages : 269 274 , 2001 .
[ 5 ] Dhillon , IS , Mallela , S . , and Modha , DS InformationIn Proceedings of ACM theoretic SIGKDD’03 , pages : 89 98 , 2003 . co clustering .
[ 6 ] El Yaniv , R . , and Souroujon , O . Iterative double clustering In for unsupervised and Proceedings of ECML’01 , pages : 121 132 , 2001 . semi supervised learning .
[ 7 ] Gao , B . , Liu , T . Qin , T . Zheng , X . Cheng , Q . and Ma , W . Web image clustering by consistent utilization of visual features and surrounding texts . In Proceedings of ACM Multimedia’05 , pages : 112 121 , 2005 .
[ 8 ] Gao , B . , Liu , T . , Zheng , X . , Cheng , Q . , and Ma , W . starConsistent bipartite graph co partitioning structured high order heterogeneous data co clustering . In Proceedings of ACM SIGKDD’05 , pages : 41 50 , 2005 . for
[ 9 ] Mao , J . C . , and Jain , A . K . Texture classification and simultaneous segmentation autoregressive models . Pattern Recognition , vol . 25 , no . 2 , pages : 173 188 , 1992 . using multiresolution
[ 10 ] Slonim , N . , and Tishby , N . Document clustering using word clusters via the information bottleneck method . In Proceedings of ACM SIGIR’00 , pages : 208 215 , 2000 .
[ 11 ] Wang , J . , Zeng , H . , Chen , Z . , Lu , H . , Tao , L . , and Ma , W . ReCoM : reinforcement clustering of multi type interrelated data objects . In Proceedings of ACM SIGIR’03 , pages : 274 281 , 2003 .
Figure 3 . Time cost comparison .
Proceedings of the Sixth International Conference on Data Mining ( ICDM'06)0 7695 2701 9/06 $20.00 © 2006
