Global and Componentwise Extrapolation for Accelerating Data Mining from
Large Incomplete Data Sets with the EM Algorithm
Chun Nan Hsu
Han Shen Huang
Bo Hou Yang
{chunnan,hanshen,ericyang}@iissinicaedutw
Institute of Information Science
Academia Sinica
Nankang , Taipei , Taiwan
Abstract
The Expectation Maximization ( EM ) algorithm is one of the most popular algorithms for data mining from incomplete data . However , when applied to large data sets with a large proportion of missing data , the EM algorithm may converge slowly . The triple jump extrapolation method can effectively accelerate the EM algorithm by substantially reducing the number of iterations required for EM to converge . There are two options for the triple jump method , global extrapolation ( TJEM ) and componentwise extrapolation ( CTJEM ) . We tried these two methods for a variety of probabilistic models and found that in general , global extraplolation yields a better performance , but there are cases where componentwise extrapolation yields very high speedup . In this paper , we investigate when componentwise extrapolation should be preferred . We conclude that , when the Jacobian of the EM mapping is diagonal or block diagonal , CTJEM should be preferred . We show how to determine whether a Jacobian is diagonal or block diagonal and experimentally confirm our claim . In particular , we show that CTJEM is especially effective for the semi supervised Bayesian classifier model given a highly sparse data set .
1 . Introduction
The Expectation Maximization ( EM ) algorithm [ 4 ] is one of the most popular algorithms for data mining from incomplete data . Given an incomplete data set , the EM algorithm iteratively searches for the best parameter vector θ∗ that maximizes the log likelihood of the data . However , when applied to large data sets with a large number of parameters to estimate , the EM algorithm may converge slowly . If the data sets also contain a large proportion of missing data or there are a large number of hidden variables in the model , the convergence of EM can be even slower .
Aitken ’s acceleration is one of the most commonly used method to speed up fixed point iteration methods [ 2 ] . Since the EM algorithm can be considered as a fixed point iteration method , we can apply Aitken ’s acceleration to accelerate the EM algorithm [ 9 , 10 ] .
However , the multivariate version of Aitken ’s acceleration requires to compute or approximate the Jacobian of the EM mapping matrix , which can be intractable . Many variants of Aitken ’s acceleration have been proposed to approximate Aitken ’s acceleration as an extrapolation method . One of the methods is the triple jump extrapolation method ( TJEM ) [ 7 , 5 , 15 ] . The idea is to estimate the extrapolation rate by considering the previous two estimates of the parameter vectors . The triple jump extrapolation method can effectively accelerate the EM algorithm by substantially reducing the number of iterations required for the EM algorithm to converge . Another benefit of the triple jump method is that it can be easily integrated with existing EM packages for any probabilistic model . We can even integrate the triple jump method with other extrapolationbased acceleration methods , such as the parameterized EM ( pEM ) [ 1 ] and the adaptive overrelaxed EM ( aEM ) [ 14 ] , to further accelerate the convergence [ 6 ] .
The triple jump method can extrapolate the parameter vector with one extrapolation rates for different dimensions . We refer to the former approach as global extrapolation and the latter as componentwise extrapolation . The componentwise extrapolation of the EM algorithm is referred to as the componentwise triple jump EM algorithm ( CTJEM ) . Hesterberg [ 5 ] proposed a global extrapolation method , while Huang et al . [ 7 ] described a componentwise extrapolaion method , though in that method many dimensions can be extrapolated together as a sub vector . We tried these two methods for a variety of probabilistic models with synthesized data and found that in general , global extraplolation yields a better performance , but there are cases where componentwise extrapolation yields very high speedup . In some cases , one triple jump can reach the local maximum .
We investigate when componentwise extrapolation should be preferred . We conclude that , when the Jacobian of the EM mapping is diagonal or block diagonal , CTJEM should be preferred . Previously , Schafer [ 16 ] also suggested the same , but he did not formally justify this claim and how to determine when the rates are different . In this paper , we demonstrate how to determine whether a Jacobian is diagonal or block diagonal and experimentally confirm our claim .
2 . Aitken ’s Acceleration for EM
Suppose we want to use the EM algorithm to build a probabilistic model with a l dimensional parameter vector θ from an incomplete data set D = ( Dobs,Dmis ) , where Dobs denoted the observed values and Dmis denotes the missing values . Now let d = [ y1 , y2,··· , yn]T be the data set with all missing values in D imputed ( ie , filled in by some estimation method . ) and f(d|θ ) be the probability density of d given θ , then
Ld(θ ) = log f(D = d|θ ) is the log likelihood of d while
˜Ld(θ ) = log fdobs(Dobs = dobs|θ ) = log f(D = d|θ)dDmis
( 1 )
( 2 )
( 3 ) is the log likelihood of the observed data dobs . The maximum likelihood principle states that the best parameter vector is the one that maximizes the log likelihood of the observed data . However , it is usually difficult to derive a closed form solution for the integral for the observed data likelihood with complex probabilistic model . The EM algorithm solves this problem by iteratively imputing the missing data and searching for θ∗ that maximizes the expected complete data likelihood . Let θ be in the space Ω and θ(t ) ∈ Ω be the result of the tth EM iteration , t = 0 , 1 , 2 , . . Then the EM algorithm defines a mapping M : Ω → Ω such that θ(t+1 ) = M(θ(t) ) . If M is continuous and θ(t ) converges to a local optimum θ∗ , then θ∗ = M(θ∗ ) . Therefore , the EM algorithm is equivalent to solving θ∗ by the fixed point iteration [ 2 ] . The multivariate version of Aitken ’s acceleration can be derived as follows [ 10 ] . Suppose that when t → ∞ , θt → θ∗ . Then we can express θ∗ as
∞ h=1
By applying a linear Taylor expansion of M(θ(t+h−1 ) ) around θ(t+h−2 ) , we have
θ(t+h ) − θ(t+h−1 )
≈ J(θ∗)(θ(t+h−1 ) − θ(t+h−2) ) ,
( 5 )
θ∗ = θ(t ) +
( θ(t+h ) − θ(t+h−1) ) .
( 4 )
Since where J is the Jacobian matrix of M . Note that J(θ(t+h−2 ) ) can be approximated by J(θ∗ ) near the convergence point . Repeatedly applying ( 5 ) in ( 4 ) gives
θ∗ ≈ θ(t ) +
J(θ∗)h(θ(t ) − θ(t−1 ) )
∞ h=1
= θ(t ) + ( I − J(θ∗))−1(θ(t )
( 6 ) when all eig(J(θ∗ ) ) are between 0 and 1 . In Equation 6 , we replace θ(t+1 ) with θ(t ) EM to emphasize that θ(t+1 ) is obtained by applying an EM mapping to θ(t ) here .
EM − θ(t) ) ,
The multivariate version of Aitken ’s acceleration requires to compute or approximate the Jacobian of the EM mapping matrix . From [ 4 ] , we know that the Jacobian of the EM algorithm is given by
J = I − IobsI−1 c where I is the l × l identity matrix , − ∂2[log f(D|θ ) ]
Ic = E
∂θ∂θT
( 7 )
( cid:175)(cid:175)(cid:175)(cid:175)(cid:175 )
θ=θ∗ is the Fisher ’s information of the expected complete data ,
,
( cid:175)(cid:175)(cid:175)(cid:175)(cid:175)Dobs , θ ( cid:175)(cid:175)(cid:175)(cid:175)(cid:175 )
θ=θ∗
Iobs = − ∂2 ˜Ld(θ )
∂θ∂θT is the Fisher ’s information of the observed data . Fisher ’s information measures how flat the likelihood surface is . Computing Fisher ’s information can be intractable for complex models with a high dimensional parameter space .
In addition to the complexity of computing the Jacobian matrix , Aitken ’s acceleration also has the drawbacks including that it may not always converge and may be numerically unstable [ 8 ] .
3 . Triple Jump Extrapolation
The triple jump extrapolation method approximates the largest eigenvalue of the Jacobian matrix . The eigendecomposition of J is
J(θ∗ ) = Qdiag(λ1 , . . . , λn)Q−1 = QΛQ−1 , where columns of Q are the eigenvectors . Therefore , ( I − J(θ∗))−1 =
Q [ I − Λ ] Q−1(cid:164)−1 ( cid:163 )
= Qdiag(
1
1 − λ1
, . . . ,
1
1 − λn
)Q−1 .
θ∗ ≈ θ(t ) + ( I − J(θ∗))−1(θ(t )
EM − θ(t ) ) Q−1θ(t ) + [ I − Λ]−1 Q−1(θ(t )
= Q
EM − θ(t ) )
,
Q−1θ∗ ≈ Q−1θ(t ) + [ I − Λ]−1 Q−1(θ(t ) EM − θ(t)e ) .
θ∗e ≈ θ(t)e + [ I − Λ]−1 ( θ(t)e
EM − θ(t ) )
The superscript e in θe denotes that it is a transformed parameter vector in the eigenspace . We can derive the Aitken ’s acceleration along the direction of the i th dimension as i ≈ θ(t)e θ∗e i +
1
1 − λi
EM i − θ(t)e ( θ(t)e i
) .
( 8 )
Let ϕ(t ) = θ(t ) − θ∗ denote the difference between current estimated parameter vector to the local maximum . The global rate of convergence of the EM algorithm is defined as the ratio :
R = lim t→∞ R(t ) ≡ lim t→∞
ϕ(t+1 ) ϕ(t )
( 9 )
Dempster et al . [ 4 ] have shown that R = λmax , the largest eigenvalue of J . Thus , instead of computing the Jacobian , we can simplify Aitken ’s acceleration for EM by replacing every eigenvalue λi with a single value γ(t ) such that γ(t ) is an approximation of λmax at the t th iteration . That is ,
θ(t+1 ) = θ(t ) + ( 1 − γ(t))−1(θ(t )
EM − θ(t) ) .
( 10 )
We can estimate γ(t ) as follows . From Equation ( 5 ) ,
J(θ∗)(θ(t ) − θ(t−1 ) ) ≈ θt
EM − θ(t ) .
Suppose γ(t ) is an exact approximation of J(θ∗ ) , then
γ(t)(θ(t ) − θ(t−1 ) ) = θt
EM − θ(t ) . i − θ(t−1)e has the same direction as θ(t)e
EM i− Since eig(J(θ∗ ) ) is greater than or equal to zero [ 4 ] , θ(t)e θ(t)e . To ensure i i that our extrapolation for each θ(t+1)e is along the same direction as Aitken ’s acceleration , we need γ(t ) ≥ 0 and our estimation of γ(t ) is thus defined by : EM − θ(t ) θ(t ) − θ(t−1 ) . i
γ(t ) ≡ θt
( 11 )
To obtain θ(t+1 ) by ( 10 ) and ( 11 ) , we need to apply the EM algorithm to obtain θ(t ) from θ(t−1 ) and apply again to obtain θ(t ) EM . Because this is similar to the hop , step and jump phases in triple jump , Huang et al . [ 7 ] named this method the Triple Jump Acceleration .
In the case that the parameter space has only one dimension , Equation ( 11 ) provides an exact approximation of J(θ∗ ) = M(θ∗ ) . When the convergence is slow , we will have M ≈ 1 and γ(t ) ≈ 1 , too . In that case , 1/(1 − γ(t ) ) will be very large and provide a large acceleration . In a multi dimensional case , the convergence rate is determined by the largest eig(J(θ∗) ) . When the eigenvalue is close to one , the convergence will be slow . Aitken ’s acceleration can provide a large acceleration when we have a good approximation of λmax but may also cause numerical insatiability when λmax ≈ 1 . Since γ(t ) ≤ λmax , the triple jump extrapolation is numerically more stable than directly using the eigenvalues . The Aitken ’s acceleration does not guarantee to reach θ∗ directly from θ(t ) because it is based on the assumption that θ(t ) is within the neighborhood of θ∗ . When θ(t ) is not close enough to θ∗ , the extrapolation jumps to a θ(t+1 ) that might fail to improve the likelihood . Salakhutdinov et al . [ 14 ] showed that with the extrapolation ratio within a certain interval , EM with extrapolation is guaranteed to converge . However , since the ratio in such an interval is too small , the speedup will not be significant . Therefore , they proposed another method called adaptive overrelaxed EM ( aEM ) , which switches back to vanilla EM during the search if the new data likelihood is not increased . In this way , the data likelihood will monotonically increase and aEM is guaranteed to converge . We can apply their idea to come up with a variant of EM with the triple jump extrapolation that is guaranteed to converge .
4 . Experimental Results with TJEM
This section reports the experimental evaluation of the triple jump accelerated EM algorithm ( TJEM ) to demonstrate the effectiveness of TJEM . The results reported here are different from those in [ 7 ] , where we reported the results of applying triple jump to sub vectors , while here we report the results of applying global triple jump . Previously , Hesterberg [ 5 ] also performed experiments for the same purpose , but he only used a quite simple probabilistic model with a two dimensional parameter vector .
We compared the numbers of iterations required to converge for vanilla EM and TJEM to evaluate their performance . More specifically , the number of iterations is the number of times that an E step is executed , which is the most costly operation in EM for the probabilistic models used in our experiments and is proportional to the CPU time required to converge .
We synthesized data sets for the following models : • Hidden Markov Models ( HMM ) : we used five state , 20 symbol HMMs with randomly initialized parameter vectors to generate training data sets . Each data set contains 500 sequences of an alphabet of 100 symbols . • Bayesian networks ( BN ) : we used the ALARM model [ 3 ] . We randomly synthesized 2,000 examples for each experimental data set .
• Mixture of Gaussians ( MoG ) : we used MoG with Gaussian components that overlapped with one another . We sampled 2,000 cases for each data set using five equal weight Gaussians with means at {(0 , 0 ) , ( 0 , 1 ) , ( 1 , 0 ) , ( 0,−1 ) , ( −1 , 0)} and variances 08
• Semisupervised Bayesian classifier ( SB ) : We used a Bayesian classifier that classifies instances with 100 10 valued discrete features into 5 categories . 3,000 training cases were generated with 90 % labels unknown and skewed missing features .
Figure 1 illustrates that TJEM almost always converges faster than EM . For the data sets of each model , we use one scattered plot to show the required convergence iterations . The coordination of each data point is the iterations of TJEM ( the X axis ) and EM ( the Y axis ) for the same data set . Thus , there are 100 data points in each plot . A data point lays in the upper triangle if TJEM converges faster , and in the lower triangle if EM is faster . We can see that in the 400 learning tasks of all the four models , TJEM converges faster for 392 times , and slower only for eight times . We also compared the likelihood of the convergent parameter vectors by EM and TJEM . In Figure 1 , a circle means that TJEM ( the X axis algorithm ) converges at a parameter vector with a higher likelihood , while a box indicates that EM ( the Y axis algorithm ) yields a higher likelihood . The size of a data point shows the difference between their likelihoods . A small point means that the difference is less than 10−5 , a medium one between 10−3 and 10−5 , and a large one more than 10−3 . We found that TJEM converges with a higher likelihood 60 times for HMM , 90 times for BN , 83 times for MoG , and 60 times in SB . All figures are out of 100 trials for each model . Therefore , TJEM not only accelerates the EM algorithm but also often improves the data likelihood of the learned models .
5 . Global and Componentwise Extrapolation
It is also possible to approximate λi in each dimension , or divide the parameter space into subspaces and use Equation ( 11 ) to obtain an approximation for each subspace , as reported in [ 7 ] . In this section , we investigate the conditions when componentwise extrapolation is preferred .
Componentwise extrapolation may accelerate the convergence more effectively than global extrapolation when components of the parameter vector converge at different rates with the EM algorithm [ 16 ] . Recall that the global rate of convergence R of EM is defined in Equation ( 9 ) . Now let ϕ(t ) i = θ(t ) i denote the componentwise difference . The i th componentwise rate of convergence is defined as i − θ∗
Ri = lim t→∞ R(t ) i ≡ lim t→∞
ϕ(t+1 ) i ϕ(t ) i
.
( 12 )
When Ri = R for all component i , global extrapolation is more appropriate than componentwise extrapolation , and vice versa . The global rate of convergence R is known to be the largest eigenvalue of the Jacobian [ 4 ] . Ri is also one of the eigenvalues but due to eigen transformation , Ri is not necessarily the i th eigenvalue . The following Lemma is helpful for us to understand why R and Ri are eigenvalues and which eigenvalue corresponds to Ri . Lemma 1 The l × l Jacobian matrix J can be decomposed into a linear combination of its eigenvalues
J =
λjujvT j = λ1u1vT
1 + λ2u2vT
2 + ··· + λkukvT k , where 1 > λ1 > ··· > λk > 0 are k(≤ l ) distinct eigenvalues of J , uj , vj ( j = 1,··· , k ) form the bases of the j th eigenvector spaces for J and J T , respectively . Moreover , k j=1
J t = jujvT λt j .
( 13 ) j=1
Proof Since J is a real valued square matrix and can be decomposed as J = QΛQ−1 . Let Q = [ u1 ,··· , ul ] and
Q−1 = [ vT
1 , vT
2 ,··· , vT l ]T
Equation ( 13 ) follows immediately from [ 16 ] , page 293 .
With this Lemma , Meng and Rubin [ 11 ] showed that the global rate of convergence R for EM is the largest eigenvalue and gave the sufficient and necessary condition of when the componentwise rate Ri = R . We restate the proof of their findings less formally here .
A Taylor expansion of ϕ(t ) and from Lemma 1 , we have k k j=1
ϕ(t ) =
λt jujvT j ϕ(0 ) .
( 14 )
That is , the difference between the t th estimate θ(t ) to the local maximum θ∗ is a linear combination of the eigenvalues of J . Now , consider the i th component θi of the parameter vector and the j th largest eigenvalue λj of J . The contribution of λj to θi is

ϕ(0 ) i
 = λt j

 . wij j · [ ujvT λt j ]· ϕ(0 ) = λt j · [ ujvT j ]·
( 15 ) j is a matrix defining the eigen transformation Note that ujvT j maps the difference of the i th of the j th eigenvalue . ujvT to wij . If wij = 0 , then λj contributes to component ϕ(0 ) the convergence of θi and the convergence rate for the i th component is at least as slow as λj . If for any component i , we have wi1 = 0 , that is , the mapping result of the largest eigenvalue λ1 is nonzero , then i
( a ) Training HMM with TJEM and EM
( b ) Training ALARM with TJEM and EM
( c ) Training MoG with TJEM and EM
( d ) Training SB with TJEM and EM
Figure 1 . Scattered plots that compare the TJEM and EM algorithms . TJEM converges faster and achieves a better likelihood in almost all trials . the global rate of convergence is at least as slow as the largest eigenvalue λ1 . That is , R = λ1 . If for a given component i , we have wi1 = 0 , then the componentwise rate of convergence for the i th component is as slow as the global rate of convergence . That is , Ri = R . Otherwise , the componentwise rate of convergence is different from the global rate . Meng and Rubin [ 11 ] proved this by following the definitions of the componentwise rate and global rate of convergence with Lemma 1 . Corollary 2 Ri = R if wi1 = eT is the i th column of the identity matrix Id .
1 ϕ(0 ) = 0 , where ei i u1vT
An obvious case that makes wi1 = 0 is when ϕ(0 ) is a zero vector . That is , our initial value is exactly the local maximum , which is unlikely to happen . Since wi1 is the
1 and ϕ(0 ) , inner product of the i th row of the matrix u1vT wi1 = 0 if they are orthogonal . This is unlikely , too . A 1 is a zero more possible case is that the i th row of u1vT vector .
When J = QΛQ−1 is a diagonal matrix , Q and Q−1 will be diagonal , too . As a result , ujvT j will be singular . That is , some of their rows will be zero vectors and thus makes Ri = R . Therefore , we can conclude that when J is a diagonal matrix , we have Ri = R , and we should apply componentwise triple jump extrapolation . More precisely , we should also require that J is not only diagonal but also not proportional to the identity matrix so that all eigenvalues are distinct . Similarly , if J is block diagonal , ujvT j will also be singular and lead to the same consequences . We summarize our conclusion with the following claim :
0500100015002000250030003500400005001000150020002500300035004000HMM # of TJEM Iteration # of EM Iteration050100150200250300050100150200250300Alarm # of TJEM Iteration # of EM Iteration010002000300040005000600070008000900010000010002000300040005000600070008000900010000GMM # of TJEM Iteration # of EM Iteration050100150200250300350400450500050100150200250300350400450500Semisupervised # of TJEM Iteration # of EM Iteration Claim 3 When the Jacobian J of an EM algorithm application is diagonal , or block diagonal , componentwise triple jump extrapolation may accelerate the convergence faster than global triple jump extrapolation and vice versa .
6 . Case Studies
In this section , we review two simple mixture of Gaussian models whose Jacobians were derived in previous work . One of them is diagonal and the other is not . Then we consider the Bayesian Network models and investigate when their Jacobians are ( block ) diagonal . We also present experimental results that verify our claims .
61 Mixtures of Gaussians
Our first example is from Meng and Rubin [ 11 ] . We have a set of one dimension data Dobs = X = {xi|i = 1 , 2 , . . .} from the following distribution : fex1(X|µ , σ2 ) = ( 1 − π)N(µ , σ2 ) + πN(µ , σ2/λ ) .
That is , the data set comes from a mixture of two Gaussians with the same mean but different variances . Assuming that we know the mixture ratio π and constant λ , then our parameter vector is θ = ( µ , σ2 ) . We can estimate the parameter vector from data by the EM algorithm by creating a missing , unobservable variable Q ∈ {1 , λ} that assigns membership of an observed variable X . Therefore , our complete , augmented data set is D = {(xi , qi)|i = 1 , 2 , . . }
We can use Equation ( 7 ) to compute the Fisher ’s information of the observed and missing data by differentiating the log likelihood of the data twice to determine whether the Jacobian of this model is diagonal . If both information matrices , Ic and Iobs , are diagonal , then the Jacobian will be diagonal , too . Though this model is simple , its Jacobian is still quite complex . Nevertheless , Meng and Rubin [ 11 ] showed that in this case , the Jacobian is a 2 × 2 diagonal matrix and empirically show that the componentwise rate of convergence is different .
Interestingly , with a different parameter vector , another one dimensional Mixtures of Gaussian model from [ 9 ] has a Jacobian that is not diagonal . In this case , the parameter vector is θ = ( µ0 , µ1 , π ) with the variance known to be σ2 = 1 . The distribution for the observed data is fex2(X|µ0 , µ1 , π ) = ( 1 − π)N(µ0 , 1 ) + πN(µ1 , 1 ) .
We introduce an additional unobserved membership assignment variable Q ∈ {0 , 1} for the augmented complete data set . In this case , Louis [ 9 ] showed that Ic is diagonal , while Iobs is not , though it is symmetric . Thus J is not diagonal . We then applied the global and componentwise triple jump extrapolation to these simple models . We synthesized a data set with 10,000 data points for both models and ran different EM variants to compare their rate of convergence . We found that for the first model , componentwise triple jump can accelerate the convergence more than global triple jump , while for the second model , global triple jump converges faster . For both models , both triple jump methods converge faster than vanilla EM . Figure 2 plot the curves of convergence of this experiment . The result is consistent with our prediction .
62 Bayesian Networks
We now consider a more practical model , the Bayesian Networks , to determine when its Jacobian is diagonal . The EM algorithm is applied to train a Bayesian Network model when we have latent variables whose values are not observable or when some of the values of variables in the training data are missing . The Jacobian of the EM algorithm for the Bayesian network can be obtained from Equation ( 7 ) . Since our purpose is only to determine if the Jacobian is diagonal , there is no need to obtain the entire Jacobian matrix . In fact , if we can show that the Fisher ’s information Iobs and Ic are ( block ) diagonal , then the Jacobian must be ( block ) diagonal as well . Therefore , our plan here is to determine if the off diagonal elements of Iobs and Ic are zero . For Iobs , these off diagonal elements are the second partial derivatives of the log likelihood of data with respect to two different parameters . For Ic , these elements are the expectation of the second partial derivatives of the complete data log likelihood . Since if the second partial derivatives are zero , their expected values must be zero , too , there is no need to obtain the expectation . Thus , it suffices to show just the second partial derivatives with respect to two different parameters to determine if the Jacobian is diagonal . A Bayesian network consists of a set of variables X = {Xi|i = 1 , 2 , . . .} , the graph structure of the variables , and their conditional probability tables . Suppose we have a variable Xi whose parent nodes include a set of variables denoted by Ui . The conditional probability table for a variable Xi consists of entries of the form wijk ≡ Pr(Xi = xik|Ui = uij ) to denote the probability that Xi has its k th possible value xik under the condition that its parent Ui has the j th combination of values , uij . Since wijk denotes the probability , to ensure that wijk is in [ 0 , 1 ] during the training process , a common technique usually used in practice is applying softmax reparameterization : wijk = eθijk k eθijk
Therefore , the parameters that we want to estimate from data using the EM algorithm are the set θ = {θijk|i , j , k = 1 , 2 , . . }
Figure 2 . ( Left ) Convergence rate comparison of EM,TJEM,CTJEM for the example model given in [ 11 ] , ( right ) Convergence rate comparison of EM,TJEM,CTJEM for the example model given in [ 9 ] .
The training data for the Bayesian network is a set D = { . . . , y , . . .} where y = { . . . , Xi = xik , . . .} is a set of variable value pairs . Some of the variable ’s value may be missing either because in that particular case , its value is not available , or because the variable is a latent variable . Many algorithms available for the Bayesian network allow us to efficiently compute the conditional probability
Pθ(y ) = Pθ(ymis|yobs)Pθ(yobs ) , the probability of unknown variable values given known variable values and the set of parameters . The Bayesian network allows us to factorize any conditional probability given the values of a subset of variables into an expression of wijk , the entries in the conditional probability table [ 13 ] . To discuss Iobs and Ic , we start by considering the second order partial derivatives of the log likelihood L(θ ) . We suppose that each training example is drawn independently so that
L(θ ) = log Pθ(y ) .
∂
∂θijk
∂ y
∂θijk
We start from Lemma 4 which summarizes that will be frequently used :
∂
∂θijk wijk
Lemma 4 For Bayesian networks with softmax reparameterization , the derivative of wijk with respect to θijk is :

∂wijk ∂θijk
=
0
: i = i or j = j : ( i , j ) = ( i , j ) and k = k
−wijkwijk wijk(1 − wijk ) : ( i , j , k ) = ( i , j , k )
( 16 )
Then , Lemma 5 describes the first order derivative of
P ( y ) , and when the derivative is zero .
Lemma 5 For Bayesian networks with softmax reparameterization , the derivative of P ( y ) with respect to θijk is :
∂
∂θijk
P ( y ) = P ( xik , uij , y ) − wijkP ( uij , y ) ,
( 17 ) that is , and the derivative must be 0 if uij d separates [ 12 ] the observations in y − {Ui = uij} and Xi , if P ( Xi|uij , y ) = P ( Xi|uij ) . Lemma 5 implies that Xi must not be initiated in y to satisfy the d separation condition . Intuitively , if Xi is initiated as xik in y , y and Xi will never be conditionally independent given uij because P ( Xi|uij , y ) is equal to 1 if Xi = xik and equal to 0 otherwise . Based on Lemma 5 , Theorem 6 shows the conditions in which ∂2 log P ( y ) ∂θijk ∂θijk
= 0 . is 0 if any of the following holds :
Theorem 6 ∂2 log P ( y ) ∂θijk ∂θijk 1 . Lemma 5 holds , or 2 . uij d separates the observations in y∪{uij , xik} and
Xi .
The first condition is straightforward because the derivative of zero is still zero . We can obtain the second condition by expanding y to y ∪ {uij , xik} and applying Lemma 5 to θijk again . As for other non zero second order derivatives , we discuss the situation that {uij , xik} ⊂ y , and ( i , j ) = ( i , j ) in Theorem 7 . Theorem 7 If uij and xik are observed in y , ∂2 log P ( y ) , ∂θijk ∂θijk the second order derivative of two parameters with the same i , j , is :
( cid:189 ) −wijk(1 − wijk):if k = k
∂2 log P ( y ) ∂θijk ∂θijk
= wijkwijk
:if k = k .
234567891011−17923−17923−17923−17923−17923−17923x 104IterationLog−likelihoodGMMEMTJEMCTJEM050100150200250300350400450−14185−14185−14185−14185−14185−14185−14185−14185−14185−14185−14185x 104IterationLog−likelihoodGMMEMTJEMCTJEM Moreover , the derivatives with other ( i , j , k ) ’s are zero . Now we consider Ic , the Fisher information of a complete data set . From Theorem 7 , we can arrange θijk so that ∂2 log P ( y ) is a block diagonal matrix , implying that Ic = E( ∂2 log P ( D ) Theorem 8 Ic is a block diagonal matrix in which each element in each block Bij is :
) is also a block diagonal matrix .
∂θ2
∂θ2
( cid:182 )
( cid:181 )
E
∂2 log P ( D ) ∂θijk∂θijl
If Iobs is also block diagonal with the same block layout , J will also be a block diagonal matrix . Then , we can apply componentwise triple jump to each block and estimate the maximal eigenvalues for each block .
63 Semi Supervised Bayesian Classifier
We verify our theoretical analysis with experiments on semi supervised Bayesian classifiers . A Bayesian classifier consists of a cluster random variable C and a set of feature random variables F1 , . . . , FN . There are N links from C to each Fn . The model assumes that the feature random variables are conditionally independent given C . Now we discuss Iobs of the model , which is simpler than general Bayesian networks in that every feature nodes share the same parent node . C and Fn might contain missing values . Theorem 9 describes some properties of Iobs . Theorem 9 classifiers is zero if any of the following is satisfied : log P ( y ) for Iobs of Bayesian
∂θijk ∂θijk
∂2
1 . Xi is a feature variable and is not observed , or
2 . Xi is a feature variable and is not observed .
∂2
∂θijk ∂θijk log P ( y ) for Iobs of Bayesian Corollary 10 classifiers can be nonzero if any of the following is satisfied : 1 . i = i and Xi is a class variable , 2 . Xi and Xi are feature and class variables and the feature variable is observed , or
3 . Xi and Xi are feature variables or the same feature variable and are observed . From Corollary 10 , whether Iobs is block diagonal matrix or not is related to the missing rates . First , we consider the case that the missing rate is low . For example , we suppose that the features are all observed . From Theorem 9 , no element is guaranteed to be zero in ∂2 log P ( y ) so that
∂θ2 is unlikely to be a block diagonal matrix . y
∂2 log P ( y )
∂θ2
However , if the missing rate is high , Iobs is much more close to a block diagonal matrix . An extreme example is that only one feature is observed in every training example . From Theorem 9 , most values outside the block diagonal area is zero . Thus , componentwise TJEM is more likely to outperform global TJEM under such circumstances .
We performed experiments to verify the influence of missing rates of training data on the convergence rate of CTJEM and TJEM . We use Bayesian classifiers with 20 feature variables . All the features and class variables have five possible values . We randomly initialized the parameters of Bayesian classifiers and synthesized 10,000 examples with 50 % and 90 % missing rates . Then , we ran EM , TJEM , and CTJEM to train new classifiers .
Figure 3 and shows an example of the learning task with 50 % and 90 % missing values in data sets . The EM algorithm took 40 iterations to converge in the less sparse data set , and 837 in highly sparse . The TJEM algorithm accelerated vanilla EM by reducing the number of elapsed iterations to 28 and 351 . The CTJEM algorithm took advantage of the block diagonal property described in the previous section , and further accelerated the highly sparse case by converging in only 114 iterations . However , when the missing rate is 50 % , Iobs is less close to a block diagonal matrix and CTJEM fails to outperform the TJEM algorithm . Therefore , for a semi supervised Bayesian classifier model with a large number of missing data , componentwise extrapolation should be preferred .
7 . Conclusion
In this paper , we claim that , when the componentwise rate of convergence is different from the global rate of convergence , componentwise extrapolation should be preferred . We show that the componentwise rate and the global rate of convergence are different if the Jacobian of the EM mapping is diagonal or block diagonal . Our results suggest that when considering accelerating the EM algorithm with the triple jump method , we should try TJEM first . If TJEM does not provide satisfactory speedup , we can check the off diagonal elements of the Jacobian to determine whether CTJEM may produce a better speedup .
Acknowledgements
We thank Dr . Chen Hsin Chen for his comments . This research is supported in part by the National Science Council , Taiwan , under Grant No . NSC95 3112 B 001 017 .
References
[ 1 ] E . Bauer , D . Koller , and Y . Singer . Update rules for parameter estimation in Bayesian networks . In Proc . of the 13th
Figure 3 . Training Bayesian classifiers with data sets containing 50 % ( left ) and 90 % ( right ) missing values . The data sets reflect our consideration whether Iobs is close to block diagonal ( right ) or not ( left ) . CTJEM outperformed TJEM when Iobs is close to block diagonal .
Conference on Uncertainty in Artificial Intelligence , pages 3–13 , 1997 .
[ 2 ] R . L . Burden and D . Faires . Numerical Analysis . PWS
KENT Pub Co . , 1988 .
[ 3 ] G . F . Cooper and E . Herskovits . A Bayesian method for the induction of probabilistic networks from data . Machine Learning , 9:309–347 , 1992 .
[ 4 ] A . Dempster , N . Laird , and D . Rubin . Maximum likelihood from incomplete data via the EM algorithm . Journal of the Royal Statistical Society , Series B , 39(1):1–38 , 1977 .
[ 5 ] T . Hesterberg . Staggered aitken acceleration for EM . In Proceedings of the Statistical Computing Section of the American Statistical Association , Minneapolis , Minnesota , USA , August 2005 .
[ 6 ] H S Huang , B H Yang , and C N Hsu . Triple jump aiken accceleration for EM algorithm and its extrapolation based variants . In preparation .
[ 7 ] H S Huang , B H Yang , and C N Hsu . Triple jump acceleration for the EM algorithm . In Proceedings of the Fifth IEEE International Conference on Data Mining , pages 649– 652 , 2005 .
[ 8 ] M . Jamshidian and R . I . Jennrich . Acceleration of the EM algorithm by using quasi newton methods . Journal of the Royal Statistical Society , , Series B , 59(3):569–587 , 1997 .
[ 9 ] T . A . Louis . Finding the observed information matrix when using the EM algorithm . Journal of the Royal Statistical Society , Series B , 44:226–233 , 1982 . [ 10 ] G . J . McLachlan and T . Krishnan .
The EM Algorithm and Extensions . Wiley Series in Probability and Statistics . Wiley Interscience , 1997 .
[ 11 ] X L Meng and D . B . Rubin . On the global and componentwise rates of convergence of the EM algorithm . Linear Algebra and Its Applications , 199:413–425 , 1994 .
[ 12 ] J . Pearl . Probabilistic Reasoning in Intelligent Systems : Networks of Plausible Inference . Morgan Kaufmann , 1988 .
[ 13 ] S . Russell , J . Binder , D . Koller , and K . Kanazawa . Local learning in probabilistic networks with hidden variables . In In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence , pages 1146–1152 , 1995 .
[ 14 ] R . Salakhutdinov and S . Roweis . Adaptive overrelaxed bound optimization methods . In Proceedings of the Twentieth International Conference on Machine Learning , pages 664–671 , 2003 .
[ 15 ] J . L . Schafer . Analysis of Incomplete Multivariate Data .
Chapman and Hall , New York , 1997 .
[ 16 ] S . R . Searle . Matrix Algebra Useful For Statistics . Wiley ,
New York , 1982 .
Appendix
Proof of Lemma 4 :
∂wijk ∂θijk
∂wijk ∂θijk
(
= − eθijk eθijk k eθijk )2 eθijk k eθijk − = = wijk(1 − wijk ) .
( cid:181 )
= −wijkwijk
( cid:182)2 eθijk k eθijk
Proof of Lemma 5 :
Russell et . al [ 13 ] derived k
∂
∂θijk
P ( y ) =
P ( uij)P ( y|uij , xik ) ∂ ∂θijk wijk .
( 18 )
0510152025303540−17−165−16−155x 105IterationLog−likelihoodBayesian Classifier , Missing rate= 50%EMTJEMCTJEM0100200300400500600700800900−324−3238−3236−3234−3232−323−3228−3226−3224−3222−322x 104IterationLog−likelihoodBayesian Classifier , Missing rate= 90%EMTJEMCTJEM Based on Lemma 4 , Equation ( 18 ) can be further simplified :
∂
∂θijk
P ( y ) = P ( uij)P ( y|uij , xik ) ∂ ∂θijk wijk +
P ( uij)P ( y|uij , xik ) ∂ ∂θijk wijk = P ( uij)P ( y|uij , xik)wijk(1 − wijk ) − k=k
P ( uij)P ( y|uij , xik)wijkwijk
= P ( uij)P ( y|uij , xik)wijk −
P ( uij)P ( y|uij , xik)wijk k=k wijk k
= P ( uij , xik , y ) − wijk = P ( uij , xik , y ) − wijkP ( uij , y ) . k
P ( uij , xik , y )
( 19 ) When uij d separates the observations in y−{uij} and Xi , Equation ( 19 ) can be rewritten as :
P ( uij , xik , y ) − wijkP ( uij , y )
= P ( xik|uij , y)P ( uij , y ) − wijkP ( uij , y ) = wijkP ( uij , y ) − wijkP ( uij , y ) = 0 .
Proof of Theorem 6 :
The first condition is straightforward . The second condition can also be proved by Lemma 5 . From Equation ( 17 ) , we have
=
=
∂2
∂θijk ∂θijk
1
∂
P ( y )
∂θijk
1
∂
∂θijk
P ( y ) −wijk
∂
∂θijk
P ( uij , y ) . log P ( y )
( P ( uij , xik , y ) − wijkP ( uij , y ) )
P ( uij , xik , y ) − P ( uij , y )
∂
∂θijk wijk
( 20 )
We can consider y = y ∪ {uij , xik} as another observed training example , and uij d separates y and ∂θi jk P ( uij , xik , y ) , xik . By Lemma 5 , we obtain that the first term of the above equation , Similarly , ∂θij k P ( uij , y ) = 0 . Besides , Lemma 4 describes that ∂θij k wijk = 0 here . Therefore , Equation ( 20 ) is also 0 under the second condition . is 0 .
∂
∂
∂
Proof of Theorem 7 :
We start from
∂2 log P ( y ) ∂θijk ∂θijk
= ∂
∂θijk
( P ( uij , xik|y ) − wijkP ( uij|y ) ) . log f(D|θ ) =
=
Therefore , ∂ log f(D|θ )
∂θijk eθijk . ijk
˜p(wijk)(log
˜p(wijk)θijk − ijk
= ˜p(wijk ) − ∂
= ˜p(wijk ) − ij k eθijk k eθijk )
˜p(wij ) log eθijk k eθijk . = k , we have ijk ; for θijk , ijk = ijk ,
˜P ( wij ) log ∂θijk
˜p(wij ) · ij ij k eθijk
If uij and xik are exactly the observed values in y , P ( uij , xik|y ) and P ( uij|y ) are 1 and the above equation becomes :
∂2 log P ( y ) ∂θijk∂θijk
= ∂
∂θijk
( 1 − wijk ) .
From Lemma 4 , −wijk(1 − wijk ) if k = k , is wijkwijk if k is 0 otherwise . the second order partial derivative is = k , and
Proof of Theorem 8 : Let ˜p(w ) denote the number of times that the predicate in w occurs in the data set D . For example , suppose wijk = Pr(Xi = xik|Ui = uij ) , then ˜p(wijk ) is the number of times that Xi = xik|Ui = uij occurs in D .
θijk
= eθijk ·
For θijk , i = i , j = j , k = k or k ∂2 log f ( y|θ ) ∂θijk ∂θijk ∂2 log f ( y|θ ) ∂θijk ∂θijk Proof of Theorem 9 :
= 0 . k e
θ
In the first condition , if Xi is not observed , Xi is dseparated with y by the cluster node . Based on the first condition in Theorem 6 , In the second condition , if Xi is not observed , Xi is dseparated with {y , xik} by the cluster node . Based on the log P ( y ) = 0 . second condition in Theorem 6 , log P ( y ) = 0 .
∂θij k ∂θijk
∂2
∂2
∂θi j k ∂θijk
Proof of Corollary 10 :
The three conditions are the complement of Theorem 9 . The class variable and feature variables are probabilistically dependent because there are direct links between the class and feature variables . Therefore , we know from Lemma 5 that the derivative of log P ( y ) with respect to the parameters of the class variable is not guaranteed to be zero because y cannot be d separated from the class variable . Accordingly , the first is true and the second conditions can be easily verified . The third condition is true because Xi and Xi are not independent of y .
