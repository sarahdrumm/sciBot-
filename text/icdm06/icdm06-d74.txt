Efficient Clustering of Uncertain Data
Wang Kay Ngai , Ben Kao , Chun Kit Chui
Dept . of Computer Science The University of Hong Kong wkngai,kao,ckchui@cshkuhk
Michael Chau
School of Business
The University of Hong Kong mchau@businesshkuhk
Reynold Cheng
Department of Computing
Hong Kong Polytechnic University csckcheng@comppolyueduhk
Kevin Y . Yip
Dept . of Computer Science
Yale University yuklapyip@yaleedu
Abstract
We study the problem of clustering data objects whose locations are uncertain . A data object is represented by an uncertainty region over which a probability density function ( pdf ) is defined . One method to cluster uncertain objects of this sort is to apply the UK means algorithm , which is based on the traditional K means algorithm . In UK means , an object is assigned to the cluster whose representative has the smallest expected distance to the object . For arbitrary pdf , calculating the expected distance between an object and a cluster representative requires expensive integration computation . We study various pruning methods to avoid such expensive expected distance calculation .
1 . Introduction
In many applications , data contains inherent uncertainty . A number of factors contribute to the uncertainty such as the random nature of the physical data generation and collection process , measurement error , and data staling . As an example , consider the problem of clustering mobile devices continuously according to the periodic updates of their locations . One application of the clustering is the selection of a device as the leader for each cluster . A leader ’s role is to collect data ( such as location data ) from its cluster members and to communicate with a server or a base station with batched updates . In this way , most communications are short ranged messages between cluster members and their leaders . In this example , we observe that data , as received by a server , contains the following type of uncertainty :
This research is supported by Hong Kong Research Grants Council grant HKU 7134/06E .
• The physical devices that determine vehicle locations are accurate only up to a certain precision .
• The current locations can only be estimated based on the last reported values . That is , data is always stale .
As another example , an animal tracking device might report the location of an animal periodically . One can compile the location data into a histogram that indicates the relative probability of the animal being located at a particular region . The whereabouts of the animal can then be represented by a probability density function ( pdf ) . Applying clustering on the data can reveal interesting insights on the relationship and social behaviors of the animals .
In this paper we study the problem of clustering objects with multi dimensional uncertainty . In particular , an object is not a simple point in space , but is represented by an uncertainty region over which a pdf is defined . Formally , we consider a set of n objects oi , 1 ≤ i ≤ n in an m dimensional space . Object oi is represented by a pdf fi : IRm → IR that specifies the density of each possible location of object oi . The methods to be discussed in this paper do not rely on any special forms of fi , but only require that for each object oi , there is a finite region Ai such that ∀x /∈ Ai , fi(x ) = 0 . This requirement allows each object to be approximated by a finite bounding box .
The goal of clustering is to group the objects into k clusters , such that the total expected distance from the objects to a representative point of their corresponding clusters is minimized . Going back to our mobile device example again , if peer to peer transmission cost is proportional to the transmission distance , then the above goal is equivalent to minimizing the expected transmission cost . So , suppose ci represents the cluster to which object oi belongs , and pci is the representative point of cluster ci , we want to find the values of pci ’s and ci ’s such that the objective function
Proceedings of the Sixth International Conference on Data Mining ( ICDM'06)0 7695 2701 9/06 $20.00 © 2006 i=1 ED(oi , pci ) = Pn Pn i=1(R fi(x)d(x , pci)dx ) is mini mized , where ED is the expected distance based on a metric d ( eg , Euclidean distance ) .
In real applications , there are practical ways to specify the pdfs . For example , measurement errors can be modeled by Gaussian distributions with the means and variances estimated from sample data with known real locations obtained by some external means [ 19 ] . Also , uncertainty due to data staling can be modeled according to the last reported location and velocity and the vehicle ’s properties ( eg , maximum speed ) [ 19 ] . Assuming the pdfs fi to be non zero only over a finite region is thus reasonable since the density functions drop super linearly ( eg , exponentially for Gaussian distributions ) so that the locations outside a certain region have negligible contribution to expected distances .
We have discussed in a separate study the importance of considering data uncertainty explicitly in clustering with a focus on the quality of the clustering results [ 2 ] . And an algorithm called UK means was proposed . The algorithm was applied to moving object uncertainty and was shown to improve accuracy of the clusters formed . Uniform distribution was assumed for the uncertainty associated with the data used . While the method can be generalized to other distributions , there are significant efficiency issues that have not been addressed . This paper , however , focuses on efficiency issues . Given a fixed clustering process , we study ways to reduce the running time . Efficiency is of particular importance in real time applications such as the one about mobile devices . It is also important when a large amount of object data is involved .
Traditional clustering process often requires the definition of a distance metric . For example , in K means clustering [ 14 ] , an object o is assigned to a cluster c such that the distance between o and a representative of c is the smallest among all clusters . We remark that with multi dimensional uncertainty , an object is no longer a single point in space but is represented by a pdf over an uncertainty region . The definition of distance thus has to be revisited . In this paper we choose to use expected distance as the distance measure . As we will explain , the expected distance metric not only provides an intuitive way of handling uncertainty , but also enables the development of efficient clustering algorithms . For arbitrary pdfs , computing the expected distance of an uncertain object and a cluster representative requires very expensive integration operations , which are often hundreds or even thousands of times more expensive than a simple distance computation . As we will discuss later in this paper , traditional clustering techniques have to be refined so that the algorithms become computationally feasible . One of the major contributions of this paper is about pruning techniques that can significantly reduce the number of expected distance calculations in the clustering process .
The rest of this paper is organized as follows . Section 2 discusses some related work on uncertain data mining and in particular uncertain data clustering . We discuss some issues that are not dealt with in previous studies and how we tackle them . Section 3 describes a basic clustering algorithm for uncertain data based on K means . We discuss the performance bottleneck of the algorithm , and describe a simple method for speeding up the clustering process . In Sections 4 and 5 we propose four methods for further improving performance . Section 6 demonstrates the effectiveness of the methods by extensive experiments . Section 7 discusses some observations and concludes the study .
2 . Related work
There has been significant research interest in data uncertainty management in recent years . Data uncertainty has been broadly classified into two types . The first type concerns existential uncertainty . For example , a tuple in a relational database could be associated with a probability value that indicates the confidence of its presence [ 16 ] . This “ probabilistic database model ” has been applied to semistructured data and XML [ 10 ] . The second type concerns value uncertainty . Under this type , a data item is modeled as a closed region which bounds its possible values , together with a pdf of its value [ 3 , 19 ] . This model can be used to quantify the imprecision of location and sensor data in a constantly evolving environment , as in our moving vehicles example . For value uncertainty , most work has been devoted to “ imprecise queries , ” which provide probabilistic guarantees over correctness of answers . For example , in [ 4 ] , indexing solutions for range queries over uncertain data have been proposed . The same authors also proposed solutions for aggregate queries such as nearestneighbor queries in [ 3 ] . Notice that all these works have applied the study of uncertain data management to simple database queries , instead of to the relatively more complicated data analysis and mining problems .
Data clustering is one of the most studied areas in data mining research . Depending on application , there are several goals of clustering : to identify the ( locally ) most probable values of the model parameters [ 5 ] ( eg , means of Gaussian mixtures ) , to minimize a certain cost function ( eg the total within cluster squared distance to centroid [ 14] ) , or to identify high density connected regions [ 8 ] ( eg , areas with high population density ) . The current study falls into the second category .
On the topic of clustering uncertain data , Hamdan and Govaert have addressed the problem of fitting mixture densities to uncertain data for clustering using a modified Expectation Maximization ( EM ) algorithm [ 9 ] . They supposed that data observed were the sampling results from a distribution mixture and aimed to find the maximum like
Proceedings of the Sixth International Conference on Data Mining ( ICDM'06)0 7695 2701 9/06 $20.00 © 2006 lihood estimation of the mixture model parameters . The algorithm is customized for EM and thus cannot be readily applied to other clustering situations . Clustering on interval data has also been studied . Different distance measures , like city block distance or Minkowski distance , have been used in measuring the similarity between two intervals [ 11 ] . The pdfs of the intervals are not taken into account in most of these metrics . Another related area of research is fuzzy clustering , which has been long studied in fuzzy logic [ 17 ] . In fuzzy clustering , a cluster is represented by a fuzzy subset of a set of objects . Each object has a “ degree of belongingness ” for each cluster . In other words , an object can belong to more than one cluster , each with a different degree . The fuzzy c means algorithm is one of the most widely used fuzzy clustering methods [ 6 ] . Different fuzzy clustering methods have been applied on normal or fuzzy data to produce fuzzy clusters [ 18 ] . While their work focuses on creating fuzzy clusters ( ie , each object can belong to more than one cluster with different degrees ) , our work is developed for hard clustering based on the uncertainty model of objects , in which each object can only belong to one cluster .
Recently , there have been studies on density based clustering of uncertain data . The FDBSCAN [ 12 ] and FOPTICS [ 13 ] algorithms are based on DBSCAN [ 8 ] and OPTICS [ 1 ] , respectively . Instead of identifying regions with high density , these algorithms identify regions with high expected density , based on the pdfs of the objects .
3 . Basic algorithm and min max dist pruning
In this section we describe two algorithms for clustering uncertain data . The first one is called UK means ( which stands for Uncertain K means ) [ 2 ] . UK means basically follows the well known K means algorithm except that it uses expected distance when determining which cluster an object should be assigned to . The second algorithm uses the idea of min max distance pruning in UK means with the objective of reducing the number of expected distance calculations .
UK means starts by randomly selecting k points as cluster representatives . Each object oi is then assigned to the cluster whose representative pj has the smallest expected distance from oi ( ED(oi , pj ) ) among all clusters . After the assignment , cluster representatives are recomputed as the mean of the centers of mass of the assigned objects . The two steps form an iteration , which is repeated until the convergence of the objective function . the computation of the integral R fi(x)d(x , pj)dx , where
Computing the expected distance , ED(oi , pj ) , requires fi(x ) is the probability density of a point x in the uncertainty region of oi , and d(x , pj ) is the distance between x and pj . In practice , the pdf is approximated by dividing the uncertainty region into a number of grids . The probability density of a sample point in each grid is recorded . To calculate the integral , we calculate the distance of each sample to pj , and then approximate the integral by finding the sum of the distances , weighted by the corresponding probability density of the sample points . For accuracy , thousands of samples are needed . Expected distance calculation is thus a computationally expensive operation .
Not only is expected distance expensive to compute , it is also one of the most frequently executed operations . This is because an expected distance is calculated between each object and cluster representative pair for each iteration . That is to say , if UK means iterates t times for a set of n objects to form k clusters , UK means would compute a total of nkt expected distances .
( a ) An object with a small uncertain region .
( b ) An object with a large uncertain region .
Figure 1 . The min max dist pruning method .
To improve efficiency , a pruning technique based on the concept of min max distance can be used to avoid unnecessary expected distance calculations . The basic idea is to use inexpensive distance calculations to identify cluster representatives that cannot be the closest one to an object . Hence , the expected distances between those representatives and the object need not be computed . More specifically , for each object oi , we define a minimum bounding rectangle ( MBR ) outside which the object has zero ( or negligible ) probability of occurrence . Now , for each cluster representative pj , we compute the minimum distance ( MinDist ij ) and maximum distance ( MaxDist ij ) between pj and the MBR of oi ( see Figure 1a ) . Among all the maximum distances , the smallest one is called the min max distance ˆdi between oi and the cluster representatives . For example , in Figure 1a , we have MinDist i1 = 1 , MinDist i2 = 5 , MinDist i3 = 7 , MaxDist i1 = 6 , MaxDist i2 = 11 and MaxDisti3 = 14 . Since p1 gives the smallest maximum distance , we have ˆdi = 6 . One can show that any cluster representative pj whose minimum distance MinDist ij is larger than ˆdi cannot be the one with the smallest expected distance from oi . That is ,
ED(oi , pj ) ≥ ED(oi , pj∗ ) ,
( 1 ) where pj∗ is a cluster representative with the smallest maximum distance ( ie , MaxDist ij∗ = ˆdi ) . For example , in Fig p1oip2p316511714p1oip2p318513614Proceedings of the Sixth International Conference on Data Mining ( ICDM'06)0 7695 2701 9/06 $20.00 © 2006 ure 1a , since MinDist i3 = 7 , which is larger than ˆdi = 6 , p3 cannot be the representative closest to oi and thus the expected distance ED(oi , p3 ) needs not be computed .
For those cluster representatives that cannot be pruned , their expected distances from object oi are calculated . Object oi is then assigned to the one with the smallest expected distance . Note that during this process , whenever an expected distance ED(oi , pj ) of a cluster representative pj is computed , we can refine ˆdi to min( ˆdi , ED(oi , pj) ) . Any remaining cluster representative pj0 can be pruned if MinDist ij0 > ˆdi . This potentially reduces the number of expected distance calculations further . We call the above method the min max dist pruning method .
The min max dist pruning method is effective in saving expected distance calculations of cluster representatives that are much farther away from the closest one . However , it suffers when the MBR gives poor distance estimates . This occurs when the uncertainty region of an object is large . For example , in Figure 1b , the maximum distance of p1 is increased to 8 due to a larger MBR . The min max distance ˆdi is now 8 . This new min max distance is no longer smaller than MinDist i3 , and thus representative p3 cannot be pruned and ED(oi , p3 ) needs to be computed .
Note that the pruning effectiveness of min max dist pruning relies on the MinDist and the MaxDist bounds . The closer they are to the true expected distance , the more effective is the pruning . We will describe several methods for achieving tighter bounds in the following two sections .
4 . Improving distance bounds
In [ 15 ] , the triangle inequality is applied to derive an upper and lower bound of a distance between two points in order to reduce many distance computations between data points in a hierarchical clustering process of conventional data . In this section we extend such idea to derive a good upper and lower bound of an expected distance between a data object and a cluster representative point in order to reduce expected distance computations in UK means . Let y be a point called an anchor point . Since the distance function d is a metric , by the triangle inequality , we can derive the following upper bound for the expected distance between an object oi and a cluster representative pj :
ED(oi , pj ) ≤ ED(oi , y ) + d(y , pj ) .
( 2 )
If the expected distance ED(oi , y ) between the object oi and the anchor point y is pre computed , then by Inequality 2 , an upper bound of the expected distance ED(oi , pj ) can be computed using only one inexpensive distance calculation between y and pj . We call this upper bound estimation the Upre method ( for “ Upper bound estimation based on Pre computed Expected Distances ” ) .
Method Upre can be integrated into the min max dist pruning strategy easily . Recall that with the basic min maxdist pruning , we estimate an upper bound of ED(oi , pj ) by finding the maximum distance between a cluster representative pj and the MBR of an object oi . Let us call this bound UMBR,ij . With Upre , we simply compare the upper bound as determined by Inequality 2 with UMBR,ij and take the smaller one as MaxDist ij . The rest of min max dist pruning is the same as what we have described in Section 3 .
We note that since the upper bound from Inequality 2 is used as MaxDist ij only if it gives a tighter bound , ˆdi is always at least as good ( ie , small ) as the value obtained without using the Upre method , and is potentially better . As a consequence , potentially more cluster representatives will have their MinDist ’s > ˆdi and thus more of them can be pruned , leading to a more efficient algorithm .
Moreover , the use of Upre is not restricted to one anchor point y . One can use multiple anchor points and pick the one that gives the best MaxDist ij upper bound . The tradeoff is that the more anchor points used , the tighter is the bound ( and hence a higher pruning potential ) at the expense of a higher pre computation cost . We will elaborate on this point further later in this section .
Similarly , a lower bound for ED(oi , pj ) can be derived :
ED(oi , pj ) ≥ |d(y , pj ) − ED(oi , y)|
( 3 )
Again , if the expected distance ED(oi , y ) between object oi and anchor point y is pre computed , then a lower bound of ED(oi , pj ) can be determined by an inexpensive computation of d(y , pj ) . We call this lower bound estimation method Lpre . Method Lpre can be incorporated into min max dist pruning in a way similar to how we use Method Upre . When determining MinDist ij of a cluster representative pj and an object oi , we compare the lower bound |d(y , pj)− ED(oi , y)| against the minimum distance between pj and the MBR of oi . The larger value is taken as MinDist ij . Since the lower bound MinDist ij is potentially tighter ( ie , larger ) , a cluster representative pj is more likely be pruned due to MinDist ij > ˆdi .
While Methods Upre and Lpre can potentially increase the pruning power , they come with a cost . If r anchor points are used , a total of nr expected distances need to be precomputed . The two methods would induce an overall saving only if the total number of additional representatives being pruned in all iterations is larger than nr . It is therefore important to choose a set of anchor points that has sufficient pruning power , but is small enough to avoid large overhead . Let us revisit the upper bound derived by Method Upre ( Inequality 2 ) . To make the upper bound as tight as possible , we want both ED(oi , y ) and d(y , pj ) to be small . We now show that the anchor point y that minimizes ED(oi , y ) must lie inside the MBR of oi . Suppose that a point y is
Proceedings of the Sixth International Conference on Data Mining ( ICDM'06)0 7695 2701 9/06 $20.00 © 2006 outside the MBR of oi and y∗ is the point on the boundary of the MBR that is closest to y . There are two possible cases , either the line y∗y is perpendicular to an edge of the MBR ( Figure 2a ) , or y∗ is at a corner of the MBR ( Figure 2b ) . Consider any point x inside the uncertainty region of oi . We observe that if x lies on the line y∗y ( ie , x = z ) , then d(x , y ) > d(x , y∗ ) ; Otherwise , d(x , y ) = = d(x , y∗ ) . Therefore in all cases , d(x,z ) cos 6 yxz ED(oi , y∗ ) , which proves that the point y that minimizes ED(oi , y ) must be within the MBR of oi .
ED(oi , y ) = R fi(x)d(x , y)dx > R fi(x)d(x , y∗)dx =
> d(x,z ) cos 6 y∗xz
( a )
( b )
Figure 2 . Proving that the point minimizing the expected distance to an object must be within its MBR .
Unfortunately , for a general pdf , there are no simple ways to compute the exact location of the point that minimizes the expected distance , so a reasonable scheme is to pick various anchor points within the MBR of oi . Another consideration is that we also want d(y , pj ) to be small . However , the location of pj is not known in advance at the time when the anchor points are picked and their expected distances from oi are pre computed . A reasonable option is thus to pick anchor points on the different sides of the object ’s MBR . Depending on the number of anchor points allowed , we have chosen several reasonable sets of anchor points . In a one point scheme , the center of the MBR is picked . In a five point scheme , the center of the four faces of the MBR are also used as it is guaranteed that for any representative outside the MBR , at least one of the four points would be closer to the representative than the center does . In a nine point scheme , the four corners of the MBR are also used to cater for cases in which the representative is closer to a corner than to the four faces . Figure 3 illustrates the choice of anchor points .
The above schemes are also reasonable choices for Method Lpre . This is because when a cluster representative pj is outside the MBR of oi , we want ED(oi , y ) to be small and d(y , pj ) to be large ( see Inequality 3 ) . The former goal is achieved by picking anchor points within the MBR of oi , and the latter goal is achieved by trying anchor points
( a ) One point scheme ( b ) Five point scheme ( c ) Nine point scheme
Figure 3 . Anchor point selection schemes . at different sides of the MBR . In Section 6 we will show empirically the cost and benefits of the various schemes of anchor point selection .
Finally , we remark that Methods Upre and Lpre are especially useful in applications for which some of the objects change their pdfs and/or locations while some others do not , and that updates of the clustering results are needed periodically . In this scenario , the cost of pre computing the expected distances of static objects can be amortized over the multiple executions of the clustering algorithm . Some of the pre computation overhead can thus be ignored .
5 . Reusing expected distance calculations
Let us consider Inequality 2 again . In Section 4 , we discussed why we picked anchor points within the MBR of an object oi . The idea was to make the term ED(oi , y ) as small as possible so that the upper bound was tight . Inequality 2 suggests that another way of making the bound tight is to make d(y , pj ) as small as possible . That is to say , we want an anchor point to be as close to the cluster representative pj as possible . Now the question is which point y is close to pj while its expected distance to oi , namely , ED(oi , y ) is readily available ? A reasonable answer is the location of the representative of cluster j in the previous iteration of the clustering process . Such idea is used in [ 7 ] to derive a lower bound of a distance between a data point and a cluster representative point in K means clustering of conventional data . In this section we extend the idea to derive both upper and lower bounds of the expected distances . Consider a cluster j whose representative is pj during a particular iteration of the clustering process . Let p0 j be the updated representative of cluster j in the next iteration . We note that the distance between pj and p0 j ) is likely to be small . This is especially true during the later iterations of the clustering process when cluster representatives shift by small distances only . j , ie , d(pj , p0
Given an object oi , recall that min max dist pruning determines if a cluster representative pj could be pruned by comparing MinDist ij against the min max dist threshold yy*xzy*xzyoioioiProceedings of the Sixth International Conference on Data Mining ( ICDM'06)0 7695 2701 9/06 $20.00 © 2006 ˆdi . There are two cases : 1 . MinDist ij > ˆdi .
In this case , pj is pruned and ED(oi , pj ) is not calculated . In the next iteration , we compare MinDist ij0 against ( an updated but likely similar ) ˆdi . Since pj and p0 j are likely to be close to each other , MinDist ij0 is likely to be similar in value to MinDist ij . Hence , p0 j is likely to be pruned by minmax dist pruning in the next iteration .
2 . MinDist ij ≤ ˆdi .
In this case , pj cannot be pruned and min max dist pruning will calculate ED(oi , pj ) . In the next iteration , min max dist pruning needs an upper bound of ED(oi , p0 j ) . This upper bound can be easily obtained by taking pj as the anchor point for Inequality 2 .
For Case ( 2 ) , triangle inequality gives
ED(oi , p0 j ) ≤ ED(oi , pj ) + d(pj , p0 j )
( 4 )
Since ED(oi , pj ) was calculated in the previous iteration , an upper bound of ED(oi , p0 j ) can be obtained by an inexpensive distance calculation d(pj , p0 j ) . We call this method Ucs , which stands for “ upper bound estimation based on Cluster Shift . ”
Likewise , a good lower bound of ED(oi , p0 j ) can also be obtained by taking pj as the anchor point :
ED(oi , p0 j ) ≥ |ED(oi , pj ) − d(pj , p0 j)|
( 5 )
We call this method Lcs .
A big advantage of Methods Ucs and Lcs is that they require no pre computation of expected distances that was needed by Methods Upre and Lpre .
The four methods Upre , Lpre , Ucs and Lcs are all independent of each other . One can choose to apply any combination of the four methods . For notational convenience , we concatenate the methods’ names to represent strategies that combine a number of such methods . So , for example , Upre Lpre Lcs represents a method that combines Method Upre , Method Lpre and Method Lcs together . In general , Upre and Lpre provide effective pruning during early iterations of the clustering process when a relatively large number of objects are still migrating among multiple clusters . Methods Ucs and Lcs , on the other hand , give significant pruning during late iterations when cluster representatives shift by only small distances across iterations .
6 . Experiments
As we have explained , expected distance calculations are the performance bottleneck of the algorithms . Therefore , we measure the effectiveness of the pruning methods based on the average number of expected distance calculated per
Table 1 . Parameters and baseline values .
Parameter Description n k d s number of objects number of clusters maximum length of an MBR ’s side number of sample points
Value 20,000 49 10 196 object per iteration in a clustering process . We denote this number by NED . Note that under the brute force implementation of UK means , during each iteration , the algorithm calculates the expected distance of an object to every single cluster representative . Therefore , NED = k for the brute force algorithm , where k is the number of clusters . The value k is thus the baseline reference when we discuss the effectiveness of the other methods .
61 Settings
In our experiment , we evaluate the pruning methods on two types of datasets : one generated with intrinsic cluster patterns , the other without . As we will see later , the general observations that we draw about the relative performance of the pruning methods do not differ significantly across these two models . In this paper we focus on the latter case . Data generation follows the following procedure . All objects are to be located in a 100× 100 2D space . Each object is first represented by an MBR with random side lengths . For datasets without cluster patterns , the MBRs are simply randomly positioned in the space . For datasets with cluster patterns , k points are chosen randomly as the cluster centers such that the distance between any two of them is at √ least 100 . Then the MBRs of the objects are divided into k k groups , each assigned to one cluster center . For each MBR assigned to a cluster center , its center position is randomly chosen from all the positions within a distance of 100√ from the cluster center . In this way , each object assigned to a cluster is likely to be closer to the center of its cluster than to those of any other clusters , hence producing some natural cluster patterns . k
2
√
During clustering , expected distances are computed based on discrete sample points of the pdfs . Therefore , the pdf of an object is specified by the probabilities at a finite number of discrete sample points in its MBR . For each object , we divide its MBR into a s grid , where s is the number of samples used to approximate the pdf . A probability is randomly generated for each cell of the grid . The cell probabilities are then normalized so that they sum to 1 . In our experiments we study how the following factors affect the algorithm ’s performance , namely , n : the number of objects to be clustered ; k : the number of clusters ; d : the s×√
Proceedings of the Sixth International Conference on Data Mining ( ICDM'06)0 7695 2701 9/06 $20.00 © 2006 maximum length of a side of an MBR ; and s : the number of sample points used in representing an object ’s pdf . Table 6.1 shows the baseline setting of these parameters . In the experiments , we perform sensitivity study on these parameters , varying the value of one parameter at a time .
For datasets without cluster patterns , the initial cluster representatives are picked uniformly from the 2D space . For datasets with cluster patterns , the initial cluster representatives are set as the centers of mass of some randomly chosen objects .
For each set of parameter values , the clustering process is executed 50 times and the average result is reported . We compare NED of the brute force UK means algorithm , the basic min max dist pruning algorithm , and various combinations of our four pruning strategies .
All our codes are written in Java 15 The experiments are run on Windows machines with an Intel 3.2GHz Pentium 4 processor and 1024MB of memory .
62 Results
We first investigate the effectiveness of the pruning methods with different dataset sizes ( n ) . Figure 4 shows NED under different strategies as n varies from 1,000 to 30,000 objects . The data objects in this experiment are generated without cluster patterns .
Figure 4 . NED vs . n .
In the figure , “ ALL ” refers to the use of all four pruning methods , and the bracketed number refers to the number of anchor points used in Upre and Lpre . The nine point scheme was used by default if not otherwise specified . All curves that involve Upre or Lpre except the one labeled “ All ( 9pts , precomp . excluded ) ” include the overhead of precomputing the expected distances for the anchor points .
Recall that k = 49 in this experiment ( baseline setting , see Table 61 ) Therefore , the brute force UK means algorithm computes 49 expected distances per object per iteration . From Figure 4 , we see that min max dist pruning reduces NED to slightly less than 1.4 , ie , about 97 % of the expected distance computations are pruned . Using 9 point pre computation for Upre Lpre ( the ‘ff’ line ) further improves the effectiveness of min max dist pruning by a factor of 2 when n is large . Method Ucs Lcs ( the ‘4’ line ) , which uses bounds based on the clusterrepresentative shift triangle inequality , gives the best performance . In terms of pruning effectiveness , the figure shows that Ucs Lcs is 4.5 to 12 times more effective than the basic min max dist pruning . For example , when n = 20 , 000 , NED is only 012 Algorithms that use all four pruning methods ( the “ ALL ” curves ) perform better than Upre Lpre , but are seen to be less effective than Ucs Lcs . This is mainly due to the pre computation overheads . Comparing the three “ ALL ” curves , we see that the saving made by the use of more anchor points cannot compensate for the extra overhead induced . On the other hand , if the precomputations are discounted from the cost model such as for those cases in which the pre computed anchor point expected distances are reused over and over again across multiple clustering exercises , then the combination of all 4 methods ( the ‘+’ line ) gives the most effective pruning . As shown in Figure 4 , the pruning effectiveness is about 11 to 24 times better than basic min max dist . This leads to a very significant performance improvement over all others . Finally , we have also tried more than nine anchor points , but they did not give any significant further improvement . The results are thus not shown in the graph .
From Figure 4 , we see that the four methods become more effective ( ie , their values of NED become smaller ) as n , the number of objects , increases . We note that with more objects , the number of iterations executed by the algorithms becomes larger before the clusters stabilize . A higher number of iterations favors Upre and Lpre due to the amortization of the pre computation overheads . It also favors Ucs and Lcs since the cluster representatives move only mildly in later iterations of the algorithm .
To further illustrate this point , we analyze the number of expected distance calculations performed in each iteration of a clustering process . Figure 5a shows a typical breakdown of the expected distance calculations across iterations for a dataset with 1,000 objects . The figure shows , for example , that using all 4 pruning methods with 5 anchor points , the algorithm computes an average of 0.8 expected distances per object during iteration 1 . From Figure 5 , we see that the number of expected distance calculated by basic min max dist and Upre Lpre stay relatively stable across the iterations . This number , however , drops rapidly across iterations when Ucs and Lcs are used . During the late iterations , cluster representatives shift only slightly and Ucs Lcs registers a very significant pruning result .
We further study the behavior of each of the four pruning methods individually . The results are shown in Figure 5b .
002040608112140100002000030000nAverage number of expected distancecalculationsmin max dist onlyUpreLpreUcsLcsALL ( 9pts)ALL ( 5pts)ALL ( 1pt)ALL ( 9pts , precomp.excluded)Proceedings of the Sixth International Conference on Data Mining ( ICDM'06)0 7695 2701 9/06 $20.00 © 2006 Figure 6 shows the results . Comparing Figure 6 and Figure 4 , we observe that the general trends of the curves remain the same as in the case when data are generated without cluster patterns . We note that NED values are generally smaller in Figure 6 than those in Figure 4 . This is because if data objects follow certain cluster patterns , then objects tend to get drawn towards their respective clusters . As a consequence , there will be fewer objects that are “ close ” to multiple cluster representatives . It is thus easier to prune irrelevant representatives . Pruning effectiveness is therefore slightly higher .
Figure 6 . NED vs . n ( on data with cluster patterns ) .
Our next experiment studies the effectiveness of the pruning methods when the number of clusters ( k ) varies from 9 to 81 . Figure 7 shows the results when 20,000 objects are clustered . Since the number of expected distance calculations should increase with k , instead of reporting the absolute number of calculations , we report the values as a percentage of the number required by the brute force UKmeans algorithm .
Figure 7 . Average number of expected distance calculations vs . k .
( a )
( b )
Figure 5 . Number of expected distance calculations in each iteration number ( n = 1 , 000 ) .
From the figure , we see that Lpre offers only slightly better pruning than basic min max dist . Also , among the four pruning methods , when applied alone , Upre is the most effective . The lower bound estimated by Inequality 3 is therefore not much tighter than the minimum distance from an object ’s MBR to a cluster representative ( see Figure 1 ) . Comparing the curve for Upre in Figure 5(b ) to that for Upre Lpre in Figure 5(a ) , we see that adding the pruning method Lpre to Upre achieves only a small gain . The effect of Lpre and Upre is somewhat additive to each other .
Methods Ucs and Lcs , however , are synergetic to each other . From Figure 5(b ) , we see that Ucs and Lcs achieve similar pruning effectiveness . However , when combined , they achieve a tremendous pruning effectiveness . This is especially so towards the later iterations . The synergetic effect of Ucs and Lcs indicates that we need good estimations for both an upper bound and a lower bound when we apply the cluster shift method . On the other hand , for the precomputation method , an accurate upper bound estimation is more important .
We have also studied the effectiveness of the pruning methods when the objects exhibit intrinsic cluster patterns .
00204060811214024681012IterationNumber of expected distancecalculations per objectmin max dist onlyUpreLpreUcsLcsALL ( 9pts)ALL ( 5pts)ALL ( 1pt)00204060811214024681012IterationNumber of expected distancecalculations per objectmin max distonlyUpreLpreUcsLcs002040608112140100002000030000nAverage number of expected distancecalculationsmin max dist onlyUpreLpreUcsLcsALL ( 9pts)ALL ( 5pts)ALL ( 1pt)ALL ( 9pts , precomp.excluded)0123456020406080kAverage number of expected distancecalculations ( percentage of the brute force approach)min max dist onlyUpreLpreUcsLcsALL ( 9pts)ALL ( 5pts)ALL ( 1pt)ALL ( 9pts , precomp.excluded)Proceedings of the Sixth International Conference on Data Mining ( ICDM'06)0 7695 2701 9/06 $20.00 © 2006 From the figure , again we see that the four pruning methods significantly outperform basic min max dist pruning . In general , a larger k gives a more effective pruning ( relative to the brute force UK means algorithm ) . This is because with more clusters , there are simply more candidates to be pruned . Method Upre Lpre again is about twice as effective as the basic min max dist pruning . Also , without the pre computation overheads , Ucs Lcs continues to be the best strategy . If overheads are discounted , then using all 4 pruning methods gives a very impressive pruning effectiveness . The ratios between the min max dist curve and each of the other curves remain virtually constant for most values of k , except when k is very small . This is because under small k , the overhead of Upre and Lpre becomes relatively significant . Overall , the figure shows that the pruning methods are extremely effective over a broad range of values of k .
( a ) including brute force UK means .
( b ) pruning methods only .
Figure 9 . Time vs . s .
Figure 8 . NED vs . d .
The next experiment studies the effect of d , the maximum length of an MBR ’s edge . Figure 8 shows how NED changes as d varies from 1 to 20 units . From the figure , we see that all curves rise with d . This is because a larger d gives larger uncertainty regions of objects , which leads to more data uncertainty . As we have explained through the illustration shown in Figure 1 , a bigger bounding box results in less effective pruning , which is reflected by the curves in Figure 8 . An interesting observation from the figure is that when d is very small , Upre and Lpre are not more effective than the basic min max dist pruning algorithm . This is because with a very small MBR , an expected distance estimated using an anchor point is not much different from those estimated using the minimum and maximum distances to the MBR . In comparison , Ucs and Lcs are relatively more effective . Except for extremely small values of d , all combinations of the four pruning methods are highly effective and they provide substantial improvement to the basic min max dist pruning .
Finally we study the relationship between the actual running time and the number of sample points s used to represent the pdf of an object . Recall that a larger s implies a more expensive expected distance calculation . So , for large s values , we expect that the more effective is a pruning method , the faster is the algorithm . We use a dataset with 10,000 objects without cluster patterns for this study . We vary s from 196 to 1,225 . The average results of 10 clustering runs are shown in Figure 9 .
Figure 9a compares the execution times of all approaches , including that of the brute force UK means algorithm ( which does not perform any pruning ) . It is clear that for all values of s , a substantial amount of time is saved by the pruning methods . Even for the smallest value of s ( 196 ) , applying any kinds of pruning still reduces the running time by at least a factor of ten . We remark that in practice , s should be much larger than what we have shown in the graph ( such as in the order of 104 − 105 ) such that the pdf ’s are accurately represented . Hence , brute force UKmeans is not feasible computationally .
Figure 9b shows only the curves that involve pruning . From the figure , we see that all curves go up with s . This is because a larger s implies a higher computational cost in calculating an expected distance . Among all the curves , min max dist pruning rises most rapidly with s . This is because min max dist is the least effective in pruning and therefore it is the most sensitive to s . If s is extremely
005115225305101520dAverage number of expected distancecalculationsmin max dist onlyUpreLpreUcsLcsALL ( 9pts)ALL ( 5pts)ALL ( 1pt)ALL ( 9pts , precomp.excluded)0200040006000800010000020040060080010001200sClustering time ( seconds)min max dist onlyUpreLpreUcsLcsALL ( 9pts)ALL ( 5pts)ALL ( 1pt)ALL ( 9pts , precomp.excluded)Brute force050010001500200025000200040006000800010000sClustering time ( seconds)min max dist onlyUpreLpreUcsLcsALL ( 9pts)ALL ( 5pts)ALL ( 1pt)ALL ( 9pts , precomp.excluded)Proceedings of the Sixth International Conference on Data Mining ( ICDM'06)0 7695 2701 9/06 $20.00 © 2006 small ( eg , 196 ) , computing expected distances are not that expensive and min max dist performs very well . The precomputation based methods give no performance gain due to the various overheads involved . Therefore using all four pruning methods does not run faster than using basic minmax dist pruning only . Yet , Ucs Lcs and ALL(1pt ) are still amongst the best strategies , as they save expected distance calculations while not introducing too much computational overhead .
When s is moderate ( eg , 3,025 ) , the cost of expected distance computation is major and it dominates the algorithms’ execution times . The basic min max dist pruning method is outperformed by any combination of the four pruning methods . The relative performance of the different methods now follow almost the same trend as that shown in Figure 4 .
7 . Concluding remarks
In this paper we studied the problem of clustering uncertain objects with the uncertainty regions defined by pdfs . For an accurate representation , at least thousands of sample points should be used to approximate an object ’s pdf . When applying the UK means algorithm to cluster uncertain objects , a large number of expected distances have to be calculated . We explained why expected distance computations are expensive and thus argued that effective pruning techniques are necessary for a computationally feasible clustering algorithm .
We described the basic min max dist pruning method and showed that it was fairly effective in pruning expected distance computations . To further improve performance , we derived four bound estimation methods . We conducted extensive experimental study evaluating those four pruning methods . Our results showed that Ucs and Lcs are very effective , especially when they work together . In some experiment setting , Ucs Lcs was a dozen times more effective than basic min max dist in terms of pruning effectiveness . Method Upre Lpre , which is based on precomputation of anchor points’ expected distances , also performed very well . The pre computation overheads , however , made Upre Lpre second best to Ucs and Lcs . The four pruning methods are independent of each other and can be combined to achieve an even higher pruning effectiveness . Pruning is at its full strength when all four are applied and if the pre computation overhead could be discounted . A factor of 24 times more effective in pruning than min max dist was registered in some of the experiments .
References
[ 1 ] M . Ankerst , M . M . Breunig , H P Kriegel , and J . Sander . OPTICS : Ordering points to identify the clustering structure .
In Proc . of ACM SIGMOD Conference , 1999 .
[ 2 ] M . Chau , R . Cheng , B . Kao , and J . Ng . Uncertain data mining : An example in clustering location data . In PacificAsia Conference on Knowledge Discovery and Data Mining , 2005 .
[ 3 ] R . Cheng , D . Kalashnikov , and S . Prabhakar . Querying imprecise data in moving object environments . IEEE TKDE , 16(9):1112–1127 , 2004 .
[ 4 ] R . Cheng , X . Xia , S . Prabhakar , R . Shah , and J . Vitter . Efficient indexing methods for probabilistic threshold queries over uncertain data . In Proc . of VLDB Conference , 2004 .
[ 5 ] A . P . Dempster , N . M . Laird , and D . B . Rubin . Maximum likelihood for incomplete data via the EM algorithm . Journal of the Royal Statistical Society , Series B , 39:1–38 , 1977 . [ 6 ] J . C . Dunn . A fuzzy relative of the ISODATA process and its use in detecting compact well separated clusters . Journal of Cybernetics , 3:32–57 , 1973 .
[ 7 ] C . Elkan . Using the triangle inequality to accelerate k means . In Proc . of ICML Conference , 2003 .
[ 8 ] M . Ester , H P Kriegel , J . Sander , and X . Xu . A densitybased algorithm for discovering clusters in large spatial In Proc . of ACM SIGKDD Conferdatabases with noise . ence , 1996 .
[ 9 ] H . Hamdan and G . Govaert . Mixture model clustering of In Proc . of IEEE ICFS Conference , pages uncertain data . 879–884 , 2005 .
[ 10 ] E . Hung , L . Getoor , and V . S . Subrahmanian . PXML : A In probabilistic semistructured data model and algebra . Proc . of IEEE ICDE Conference , 2003 .
[ 11 ] M . Ichino and H . Yaguchi . Generalized minkowski metIEEE TSMC , rics for mixed feature type data analysis . 24(4):698V–708 , 1994 .
[ 12 ] H P Kriegel and M . Pfeifle . Density based clustering of uncertain data . In Proc . of ACM SIGKDD Conference , 2005 . [ 13 ] H P Kriegel and M . Pfeifle . Hierarchical density based clustering of uncertain data . In Proc . of IEEE ICDM Conference , 2005 .
[ 14 ] J . B . MacQueen . Some methods for classification and analysis of multivariate observations . In 5th Berkeley Symposium on Mathematical Statistics and Probability , 1967 .
[ 15 ] M . Nanni . Speeding up hierarchical agglomerative clustering in presence of expensive metrics . In Pacific Asia Conference on Knowledge Discovery and Data Mining , pages 378–387 , 2005 .
[ 16 ] N . D . Nilesh and D . Suciu . Efficient query evaluation on probabilistic databases . In Proc . of VLDB Conference , pages 864–875 , 2004 .
[ 17 ] E . H . Ruspini . A new approach to clustering . Information
Control , 15(1):22–32 , 1969 .
[ 18 ] M . Sato , Y . Sato , and L . Jain . Fuzzy Clustering Models and
Applications . Physica Verlag , Heidelberg , 1997 .
[ 19 ] O . Wolfson , P . Sistla , S . Chamberlain , and Y . Yesha . Updating and querying databases that track mobile units . Distributed and Parallel Databases , 7(3 ) , 1999 .
Proceedings of the Sixth International Conference on Data Mining ( ICDM'06)0 7695 2701 9/06 $20.00 © 2006
