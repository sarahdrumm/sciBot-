Deep Learning to Hash with Multiple Representations
Yoonseop Kang1 , Saehoon Kim1 , Seungjin Choi1,2,3 1 Department of Computer Science and Engineering ,
2 Division of IT Convergence Engineering ,
3 Department of Creative IT Excellence Engineering ,
Pohang University of Science and Technology , Pohang 790 784 , Korea
Email : {e0en,kshkawa,seungjin}@postechackr
Abstract—Hashing seeks an embedding of high dimensional objects into a similarity preserving low dimensional Hamming space such that similar objects are indexed by binary codes with small Hamming distances . A variety of hashing methods have been developed , but most of them resort to a single view ( representation ) of data . However , objects are often described by multiple representations . For instance , images are described by a few different visual descriptors ( such as SIFT , GIST , and HOG ) , so it is desirable to incorporate multiple representations into hashing , leading to multi view hashing . In this paper we present a deep network for multi view hashing , referred to as deep multi view hashing , where each layer of hidden nodes is composed of view specific and shared hidden nodes , in order to learn individual and shared hidden spaces from multiple views of data . Numerical experiments on image datasets demonstrate the useful behavior of our deep multi view hashing ( DMVH ) , compared to recently proposed multi modal deep network as well as existing shallow models of hashing .
Keywords deep learning ; harmonium ; hashing ; multi view learning ; restricted Boltzmann machines ;
I . INTRODUCTION
Similarity search is a core problem in various areas such as machine learning , information retrieval , and data mining to name a few . For example , content based image retrieval ( CBIR ) takes an image as a query and returns its nearest neighbors , computing similarity between visual descriptors ( features ) of the query and of images in database . A naive solution to nearest neighbor search is linear scan , but this approach is not scalable in practical applications . Approximate nearest neighbor search such as tree based methods , is successful for low dimensional data , but its performance is not satisfactory for high dimensional data and does not guarantee faster search compared to linear scan [ 1 ] . Most of visual descriptors , such as SIFT [ 2 ] , GIST [ 3 ] , and HOG [ 4 ] , constitute high dimensional vectors , so tree based space partition approach is not preferred in CBIR applications . for to methods
Hashing refers embedding highdimensional data into a low dimensional Hamming space such that similar objects are indexed by binary codes with small Hamming distances . Hashing can be categorized into data independent and data dependent methods . A notable data independent method is locality sensitive hashing ( LSH ) [ 1 ] , [ 5 ] where random projections followed by rounding are used to generate binary codes . The performance of LSH is not satisfactory when short binary codes are used [ 6 ] . Datadependent hashing methods learn binary codes from a set of data to compactly index them . Learning hash functions can be done in unsupervised [ 7 ] , supervised [ 8 ] , or semisupervised [ 9 ] , [ 10 ] manner . Notable data dependent hashing methods include : ( 1 ) spectral hashing [ 7 ] where a subset of eigenvectors of the Laplacian of the similarity graph is rounded to determine binary codes ; ( 2 ) semantic hashing [ 8 ] where multi layer networks are used to learn a non linear mapping between input data and binary code bits .
Most of existing data dependent hashing methods exploit only single representation of objects to learn hash functions , referred to as single view hashing . In practice , however , images are often represented by different visual descriptors such as GIST [ 3 ] , HOG [ 4 ] , SIFT [ 2 ] , and so on . These visual descriptors have their own characteristics and could be complementary to each other ’s strength and weakness . Thus , it is desirable to incorporate these heterogenous visual descriptors into learning hash functions , leading to multiview hashing . Recently spectral hashing was extended to multi view hashing where a linear sum of view specific similarity matrices [ 11 ] was exploited . This is limited to a shallow model and considers linear hash functions followed by quantization to determine binary codes .
In this paper we present a deep network with a set of viewspecific hidden nodes and a set of shared hidden nodes , that yields binary codes at its top layer nodes . There has been recent work on deep networks to handle multi modal data [ 12 ] or designed for multi view learning [ 13 ] . However , to our best knowledge , deep learning has not been exploited for multi view hashing yet . This paper presents the first work on multi view hashing using deep networks .
II . RELATED WORK
We briefly review spectral hashing [ 7 ] and its multiview extension [ 11 ] Suppose that we are given a set of N instances with K different views , {x(1 ) i=1 , }N where x(k ) ∈ RDk correspond to visual descriptors . Then , the view specific data matrix is defined as X ( k ) = [ x(k ) ∈ {−1 , +1}M a binary code of length M associated with
N ] ∈ RDk ×N . We denote by y(k )
1 , . . . , x(k )
, . . . , x(K ) i i i i i i x(k ) . Then the binary code matrix is given by Y ( k ) = , . . . , y(k ) [ y(k ) N ] ∈ RM ×N . For single view hashing , the binary code matrix is represented by Y ( without the superscript ) and for multi view hashing an integrated binary code matrix is denoted by Y ∗ = {y∗ i=1 ∈ RM ×N which is expected to capture the average similarities between instances across views . i }N
Spectral hashing [ 7 ] determines similarity preserving compact binary codes by enforcing the average Hamming distance between similar neighbors to be minimized , and also requiring the codes of length M to be uncorrelated and balanced . Thus , spectral hashing involves the following optimization : arg min
Y subject to
NXi=1
NXj=1
Si,jkyi − yjk2 2 ,
Y ∈ {−1 , +1}M ×N ,
Y 1N = 0 ,
1 N
Y Y ⊤ = I M ,
( 1 ) where Si,j is the similarity between instances xi and xj , kyik2 is the Euclidean norm of binary code vector yi , I M ∈ RM ×M denotes the identity matrix , and 1N ∈ RN is the vector of all ones . The problem is relaxed by discarding binary constraints yi ∈ {+1 , −1}M , so that rounding a subset of eigenvectors of the graph Laplacian of the similarity graph leads to binary codes for spectral hashing .
CHMIS AW [ 11 ] takes a linear sum of view specific i,j = Pk Si,j , which is plugged into the similarities S∗ spectral hashing framework ( 1 ) . For out of sample extension in CHMIS AW , binary codes of unseen examples are determined by a convex combination of linear hash k=1 βk = 1 . Embedding matrices W ( k ) and mixing coefficients βk are estimated by solving a regularized regression problem . Y ∗ and {W ( k ) , βk}K k=1 are determined in an alternative fashion . functions , ie , PK i with PK k=1 βkW ( k)⊤x(k )
III . DEEP MULTI VIEW HASHING
In this section we present the main contribution , deep multi view hashing ( DMVH ) , as shown in Fig 1 , which has a few unique features that highlight advantages : ( 1 ) Existing methods for multi view hashing are limited to shallow models , while DMVH is expected to benefit from deep architecture ; ( 2 ) Semantic hashing [ 8 ] builds a deep belief network for hashing , but it is limited to single view , while DMVH is able to manage multiple views ; ( 3 ) DMVH consists of shared hidden nodes as well as view specific hidden nodes to capture both shared and view specific characteristics , while most of existing multi view learning methods , including canonical correlation analysis [ 14 ] and dual wing harmonium ( DWH ) [ 15 ] , assume that all views are completely correlated , which are captured by shared hidden nodes . Similar idea was used in shallow models [ 16 ] , [ 17 ] and deep networks [ 13 ] , but to our best knowledge it was not exploited for hashing yet .
( c )
( b )
( a )
Figure 1 . Graphical model for 4 layer DMVH .
A . Model
The 4 layer DMVH model is shown in Fig 1 , where : ( a ) The bottom layer represents the visible nodes x(1 ) and x(2 ) with two different views , both of which are connected to 1 and each of which is connected the shared hidden nodes h corresponding view specific hidden nodes h(1),1 and h(2),1 in the first hidden layer ; ( b ) In the second hidden layer , the 2 also further capture the common shared hidden nodes h characteristics across hidden nodes in the first layer and the view specific hidden nodes h(1),2 and h(2),2 are connected to corresponding hidden nodes in the first hidden layer ; ( c ) The hidden nodes y∗ in the top layer , which are connected to all the hidden nodes in the second hidden layer , represent the integrated binary code associated with x(1 ) and x(2 ) .
In order to handle both continuous and discrete variables , we choose the independent marginal distributions for visible nodes x(k ) , view specific hidden nodes h(k ) , and shared hidden nodes h , from the exponential family , as in [ 18 ] : p(x(k ) ) = Yi p(h(k ) ) = Yj p(h ) = Yj expnXi,a expnXj,b expnXj,b i j i
) − A(k )
( {ξ(k ) i,a ( x(k ) i,a f ( k ) ξ(k )
λ(k ) j,b g(k ) i,a })o , j,b })o , λj,bgj,b(hj ) − Bj({λj,b})o ,
) − B(k ) j,b ( h(k ) j
( {λ(k ) i,a } , {λ(k ) j,b } , {gj,b} are sufficient statistics , and {A(k ) j,b } , {λj,b} are natural parameters , {f ( k ) i } , {B(k ) where {ξ(k ) i,a } , {g(k ) j } , {Bj} are log partition functions . We illustrate random fields to describe each inter layer model , denoted by ( a ) , ( b ) , ( c ) in Fig 1 , which are used to compute log likelihoods .
1 ) Model ( a ) : We assume Bernoulli distributions over hidden nodes : p(h(k),1 ) =Yj =Yj ) =Yj =Yj p(h
1
Bern(h(k),1 j
| σ(λ(k),1 j
) ) expnλ(k),1 j h(k),1 j
Bern(h
1 j | σ(λ
1 j ) )
− log1 + expnλ(k),1 j oo , expnλ
1 1 j h j − log1 + expnλ
1 joo , j
= {λ(k),1 j,b } and λ where σ(· ) is the logistic function and the natural parameters 1 1 λ(k),1 j,b} become log odds . j = {λ Sufficient statistics are given by h(k),1 and h distributions with unit variance for real valued descriptors and Bernoulli distributions for binary valued descriptors :
) ) j ) ) . For visible nodes , we use Gaussian
= ( g(k),1
( h(k),1 j,b
1 j j
1 j,b(h j = ( g1 p(x(k ) ) =( Qi Bern(x(k ) Qi N ( x(k ) i i
| σ(ξ(k ) , 1 ) , i
| ξ(k ) i
) ) , for binary , for real valued .
Gaussian distribution N ( x(k ) , 1 ) is specified by two pairs of natural parameters and sufficient statistics , and a log partition function :
| ξ(k ) i i
2 ) Model ( b ) : DMVH improves the limited representation power of ’Model ( a)’ by stacking an additional hidden layer on it . The second hidden layer is also composed of 2 and view specific hidden nodes shared hidden nodes h h(k),2 , which are connected to hidden nodes in the first hidden layer , in order to form higher level representation ( see ( Fig 1 (b) ) .
Assuming Bernoulli distributions for all visible and hid den nodes , the joint distribution is given by
2
, {h(k),2} , h ) h(k),1
λ(k),1 i i
+Xi
1 i h
λ
1 p({h(k),1} , h
∝ expnXk,i + Xj + Xi,j
2 j h
U
λ
2 j + Xk,i,j jo .
2 1 2 i,j h i h
λ(k),2 j h(k),2 j
1 i +Xk,j + Xk,i,j
W ( k),2 i,j h(k),1 i h(k),2 j
U ( k),2 i,j h(k),1 i
2 h j
We can repeatedly stack this structure in the middle of DMVH to build a desired number of layers . For example , stacking twice gives us 5 layer DMVH and omitting this structure results in 3 layer DMVH .
3 ) Model ( c ) : The top two layers in DMVH combine high level representations {h(1),2 , . . . , h(K),2 , h } into a set of integrated hash codes y∗ ( Fig 1 (c) ) . Since hash codes are j ( which is also hidden ) is assumed binary , the output node y∗ to follow Bernoulli distribution with mean σ(ψj ) :
2 i,a } = [ ξ(k ) i
, −1/2]⊤ , {f ( k )
( {ξ(k ) i,a } ) = −((ξ(k ) i i,a } = [ x(k ) i )2 + log 2π)/2 .
, ( x(k ) i
)2]⊤ , p(y∗ ) = Yj
Bern(y∗ j | σ(ψj) ) .
{ξ(k ) A(k ) i
With these marginal distributions , we couple visible and hidden variables in the log domain by introducing quadratic interaction terms , leading to the following random filed : p({x(k)} , {h(k),1} , h
1
) ξ(k ) i,a f ( k ) i,a ( x(k ) i
∝ expnXk,i,a + Xk,i,a,j
) +Xj
1 1 λ j h j +Xk,j
λ(k),1 j h(k),1 j
W ( k),1 i,a,j f ( k ) i,a ( x(k ) i
)h(k),1 j
U ( k),1 i,a,j f ( k ) i,a ( x(k ) i
+ Xk,i,a,j
1 )h jo .
2 Then , the joint distribution over ( {h(k),2} , h by
, y∗ ) is given
2 p({h(k),2} , h
, y∗ )
∝ expnXk,i +Xk,i,j
λ(k),2 i
2 i h
λ
ψjy∗ j i h(k),2
+Xi j +Xi,j
2 i +Xj jo . i,j h(k),2 V ( k ) i y∗
V i,jh
2 i y∗
Since the model is a bipartite graph , between layer conditional distributions are represented as products of distributions of individual nodes , leading to the conditional distributions given by i,a f ( k ) i,a ( xi ) − A(k ) i i j j j
1 p(h p(x(k ) p(h(k),1
1 |{x(k)} , h
1 |{h(k),1} , h
) ∝ expnXa eξ(k ) ) ∝ expneλ(k),1 j |{x(k)} , {h(k),1} ) ∝ expnfλ i,a + Pj W ( k),1 where eξ(k ) +Pi,a W ( k),1 eλ(k),1 Pi,a,k U ( k),1 i,a = ξ(k ) i,a,j f ( k ) i,a,j f ( k ) i,a ( x(k )
= λ(k),1
1 j h(k),1 j j i j h(k),1
− B i,a,j h(k ) i,a ( x(k ) i j j
1
1
− B(k),1 i,a })o , ( {eξ(k ) })o , ( {eλ(k),1 j })o . j ( {fλ j + Pj U ( k),1 ) , and fλ i,a,j h 1 j +
1 j = λ
1 j ,
( 2 )
) are shifted parameters .
All these three models together constitute the 4 layer DMVH model . Our DMVH model is different from existing multi view deep networks [ 12 ] , [ 13 ] in many aspects . While existing models learn high level features separately for each view and fuse them in the top layer , DMVH learns viewspecific and shared representations across views in every layers to capture the partial correlations in multi view data .
B . Training
Training procedure of DMVH is similar to those of other deep networks . We first pre train each layer in a greedy , layer by layer approach . We start by training parameters for the bottom two layers , then fix the learned parameters . Then we repeat the process for the higher two layers , until we reach the top layer . We train each two layers of DMVH by maximizing their expected log likelihood L derived from the joint distribution . For example , the expected log likelihood of the bottom two layers of DMVH is as below : objective function of nonlinear neighborhood component analysis ( NCA ) [ 20 ] with backpropagation algorithm :
L = hlog p({x(k)})i+
=logP{h(k),1
1 p({x(k)}|{h(k),1} , h
},h
1
)*+
, where h·i+ is expectation over data distribution . Derivation of likelihood of other layers is done in a similar way . integration over
As exact calculation of the gradients of L requires the model distribution summation or 1 p({x(k)} , {h(k),1} , h ) , we need to do some approximation . Contrastive divergence [ 19 ] learning approximates model distribution by initializing visible nodes with data distribution , and then alternatively sampling from conditional distributions of visible and hidden nodes derived in ( 2 ) . The gradient of L over the parameters of bottom two layers of DMVH is as follows : ∂L/∂W ( k),1
)i+ − hf ( k ) i,a B(k),1′
)i− , j j j
( eλ(k),1
1 j )i− ,
( fλ ( eλ(k),1 j
1 j )i+ − hf ( k ) i,a B
′
1 j i,a,j = hf ( k ) i,a,j = hf ( k ) i,a = hf ( k )
∂L/∂U ( k),1 ∂L/∂ξ(k ) ∂L/∂λ(k),1 j
′ j
1 j i,a B(k),1′ i,a B i,a i+ − hf ( k )
( eλ(k),1 ( fλ ( eλ(k),1 ( fλ
1 j )i+ − hB i,a i− ,
1 j j j
= hB(k),1′
)i+ − hB(k),1′ j
)i− ,
′
′
1 j
∂L/∂λ
1 j )i− ,
1 j = hB
( fλ where h·i− is expectation over ( approximated ) model i,a ( x(k ) stands for f ( k ) distribution , f ( k ) = 1 1 /∂λ(k),1 ∂B(k),1 j are derivatives of logj /∂λ partition functions over parameters .
) , and B(k),1′ and B
= ∂B
1 j i,a j j j i
′
We also add additional penalty terms to the log likelihood separate shared and view specific features and enforce sparse hidden node activation . First , an orthogonalization term ensures that shared and view specific hidden nodes do not contain similar features . We use a pairwise sum of cosine distances between columns of view specific features W ( k),1 and shared features U ( k),1 :
L(k ) orth = Xi,j w
( k),1⊤ i
( k),1 u j kw(k),1 i kku(k),1 j k
.
In addition , we enforce sparse activation of hidden nodes by setting their activation rate ρ to a low value for numerical stability . For the output nodes , we set the activation rate to 0.5 make sure each hash bit bisects the whole dataset :
L(k ) sparsity =Pj(ρ − E[h(k),1 =Pj(ρ − E[h j
Lsparsity
|{x(l)}])2 ,
1 j |{x(k)}])2 ,
By summing the expected likelihood , weight orthogonalization terms and sparsity penalty , we get the final objective function for pre training .
LDM V H = L −Xk
( αL(k ) orth + βL(k ) sparsity ) − βLsparsity .
Training other layers is done in a similar way . After pretraining each layers of DMVH , we fine tune DMVH using
LN CA = Xi,j∈Ci pi,j = Xi,j∈Ci exp(−d(y∗ i , y∗
Pk6=i exp(−d(y∗ j ) ) i , y∗
. k ) ) i and y∗
Nonlinear NCA maximizes the probability pi,j that hash codes y∗ j with the same class Ci are close to each other . This objective function is suitable for supervised hashing as it maps hash codes of instances with same labels to have small Hamming distances .
IV . EXPERIMENTS
In this section , we performed image retrieval with hash codes learned by various hashing methods to investigate the usefulness of DMVH . Caltech 256 and NUS WIDELite dataset were used for our experiments . Caltech 256 [ 21 ] has 30,607 images with 256 classes . We extracted HoG [ 4 ] and GIST [ 3 ] descriptors from the dataset . 1000 samples were used as test querys , and the remaining were used for training . When building deep models for hashing this dataset , Gaussian distribution was assumed for both HOG and GIST descriptors . NUS WIDE Lite [ 22 ] contains 27,807+27,808 ( training+test ) images with tag annotations and 6 image descriptors . We used 5 of 6 image descriptors ( excluding bag of visual words ) and tag annotations . The dataset also consists of 81 different ’concepts’ , and these are used as labels for NCA fine tuning . Gaussian distribution was assumed for 5 image descriptors , and Bernoulli distribution was chosen for tag annotations .
We compared existing deep networks including model for multi modal deep learning ( MMDL ) and DWH with DMVH . We trained every model with layer wise greedy pre training followed by fine tuning with NCA objective function ( 3 ) . We pre defined the number of nodes in DMVHs used in experiments . In 4 layer DMVHs , the second layer had 512+64 ( shared+view specific ) hidden nodes . The third layer had 256+32 nodes . Then the top layer had node with the same number of hash code length . In 3 layer DMVHs , the middle layer had 512+64 nodes . For a fair comparison , we made sure that MMDL had similar number of parameters by assigning more hidden nodes for MMDL than DMVH . All deep models were trained with the same training parameters1 .
We also compared DMVH with CHMIS AW , a shallow , multi view hashing method based on spectral decomposition . We trained all hashing methods for hash code lengths {8 , 16 , 32 , 64 , 128} .
After training the models , we retrieved images from the training set whose hash code was within a hamming distance
1Models were trained for 50 epochs with batch size 100 . Learning rate were set to 0.1 , with momentum of 09 The parameters for orthogonalization penalty α and sparsity term β were set to α = 0.0001 and β = 01 For numerical stability , we set learning rate to 0.001 for pre training bottom layers of deep networks . i i n o s c e r p e g a r e v A
0.09
0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0
20
40
60
80
Number of hash bits
DMVH 4layer DMVH 3layer MMDL 4layer MMDL 3layer DWH CHMIS−AW
0.4
0.38
0.36
0.34
0.32
0.3 i i n o s c e r p e g a r e v A
100
120
20
40
60
80
Number of hash bits
DMVH 4layer DMVH 3layer MMDL 4layer MMDL 3layer DWH CHMIS−AW
100
120
( a ) Average precision on Caltech 256 dataset
( b ) Average precision on NUS WIDE Lite dataset e v r u c l l i i a c e r − n o s c e r p r e d n u a e r A
0.04
0.035
0.03
0.025
0.02
0.015
0.01
0.005
20
40
60
80
Number of hash bits e v r u c l l i i a c e r − n o s c e r p r e d n u
0.32
0.31
0.3
0.29
0.28 a e r A
0.27
0.26
20
40
DMVH 4layer DMVH 3layer MMDL 4layer MMDL 3layer DWH CHMIS−AW
100
120
60
80
Number of hash bits
DMVH 4layer DMVH 3layer MMDL 4layer MMDL 3layer DWH CHMIS−AW
100
120
( c ) AU PRC on Caltech 256 dataset
( d ) AU PRC on NUS WIDE Lite dataset
Figure 2 . Average precision ( hamming radius=2 ) and area under precision recall curve ( AU PRC ) measured for hashing algorithms including DMVH , MMDL , DWH , and CHMIS AW on Caltech 256 and NUS WIDE Lite datasets .
2 from the hash code of the query from test set . We increased the hamming distance boundary until we get more than 100 retrieved samples . After that , we calculated the precision by using labels as ground truth . The area under precisionrecall curve was also computed for each hash code length . To increase the recall level , we increased hamming distance boundary used for retrieval .
On Caltech 256 dataset , DMVH models achieved the highest precision on every size of hash codes , followed by single view deep network which uses more connection weights than it . DMVH scored the largest area under precision recall curve also . DMVH also outperformed spectral hashing based shallow multi view hashing models , while still outperforming multimodal deep learning model . In both datasets , 4 layer DMVH gave better result than 3layer model , empirically showing that the model is benefited from the advantage of using deep architecture for multi view hashing . In contrast , adding more layer gave worse result on MMDL model ( Fig 2 , 3 ) .
V . CONCLUSIONS
In this paper , we have proposed a deep network model for learning hash functions from partially correlated multi view data . Each layer of the proposed model separates correlated and uncorrelated information , and propagates the higherlevel shared representation generated by aggregating its inputs as well as the view specific , uncorrelated information to upper layers . Then the model combines its outputs to
Query
Retrieved images
Figure 3 . Qualitative comparison of 4 layer DMVH and 2 layer DWH on Caltech 256 dataset . For 5 query images ( leftmost column ) , we retrieved 10 images using DMVH ( upper rows ) and DWH ( lower rows ) . Retrieved images with the same label as the quary images were marked with red border . generate a set of hash functions that effectively represents the partial correlation of multi view inputs . By analyzing connection weights and hash codes learned from real world image datasets , we have demonstrated that our model is capable of separating correlated and uncorrelated information on real world datasets , and generates more semantically meaningful hash codes than shallow ones . Moreover , our model has shown beneficial over the existing deep models and multi view hashing algorithms based on spectral hashing on image retrieval experiments on image datasets with multiple descriptors .
Acknowledgments : This work was supported by NIPAMSRA Creative IT/SW Research Project , NIPA ITRC Support Program ( NIPA 2012 H0301 12 3002 ) , the Converging Research Center Program funded by the Ministry of Education , Science , and Technology ( 2012K001343 ) , MKE NIPA ” IT Consilience Creative Program ” ( C1515 1121 0003 ) , and NRF World Class University Program ( R31 10100 ) .
REFERENCES
[ 1 ] A . Gionis , P . Indyk , and R . Motawani , “ Similarity search in high dimensions via hashing , ” in Proceedings of the International Conference on Very Large Data Bases ( VLDB ) , 1999 .
[ 2 ] D . G . Lowe , “ Distinctive image features from scale invariant keypoints , ” International Journal of Computer Vision , vol . 60 , no . 2 , pp . 91–110 , 2004 .
[ 3 ] A . Oliva and A . Torralba , “ Modeling the shape of the scene : A holistic representation of the spatial envelope , ” International Journal of Computer Vision , vol . 42 , no . 3 , pp . 145–175 , 2001 .
[ 4 ] N . Dalal and B . Triggs , “ Histograms of oriented gradients for human detection , ” in Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition ( CVPR ) , San Diego , CA , 2005 .
[ 5 ] M . Datar , N . Immorlica , P . Indyk , and V . Mirrokni , “ Locality sensitive hashing scheme based on p stable distributions , ” in Proceedings of the Annual ACM Symposium on Computational Geometry ( SoCG ) , 2004 .
[ 6 ] A . Torralba , R . Fergus , and Y . Weiss , “ Small codes and large image databases for recognition , ” in Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition ( CVPR ) , Anchorage , Alaska , 2008 .
[ 7 ] Y . Weiss , A . Torralba , and R . Fergus , “ Spectral hashing , ” in Advances in Neural Information Processing Systems ( NIPS ) , vol . 20 . MIT Press , 2008 .
[ 8 ] R . Salakhutdinov and G . Hinton , “ Semantic hashing , ” in Proceeding of the SIGIR Workshop on Information Retrieval and Applications of Graphical Models , 2007 .
[ 9 ] J . Wang , S . Kumar , and S . F . Chang , “ Semi supervised hashing for scalable image retrieval , ” in Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition ( CVPR ) , San Francisco , CA , 2010 .
[ 10 ] S . Kim and S . Choi , “ Semi supervised discriminant hashing , ” in Proceedings of the IEEE International Conference on Data Mining ( ICDM ) , Vancouver , Canada , 2011 .
[ 11 ] D . Zhang , F . Wang , and L . Si , “ Composite hashing with multiple information sources , ” in Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval ( SIGIR ) , Beijing , China , 2011 .
[ 12 ] J . Ngiam , A . Khosla , M . Kim , J . Nam , H . Lee , and A . Y . Ng , “ Multimodal deep learning , ” in Proceedings of the International Conference on Machine Learning ( ICML ) , Bellevue , WA , 2011 .
[ 13 ] Y . Kang and S . Choi , “ Restricted deep belief networks for multi view learning , ” in Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases ( ECML PKDD ) , Athens , Greece , 2011 .
[ 14 ] D . R . Hardoon , S . Szedmak , and J . Shawe Taylor , “ Canonical correlation analysis : An overview with applications to learning methods , ” Neural Computation , vol . 16 , pp . 2639–2664 , 2004 .
[ 15 ] E . P . Xing , R . Yan , and A . G . Hauptmann , “ Mining associated text and images with dual wing harmonium , ” in Proceedings of the Annual Conference on Uncertainty in Artificial Intelligence ( UAI ) , Edinburgh , UK , 2005 .
[ 16 ] H . Lee and S . Choi , “ Group nonnegative matrix factorization for EEG classification , ” in Proceedings of the International Conference on Artificial Intelligence and Statistics ( AISTATS ) , Clearwater Beach , Florida , 2009 .
[ 17 ] M . Salzmann , C . H . Ek , R . Urtasun , and T . Darrell , “ Factorized orthogonal latent spaces , ” in Proceedings of the International Conference on Artificial Intelligence and Statistics ( AISTATS ) , Sardinia , Italy , 2010 .
[ 18 ] M . Welling , M . Rosen Zvi , and G . Hinton , “ Exponential family harmoniums with an application to information retrieval , ” in Advances in Neural Information Processing Systems ( NIPS ) , vol . 17 . MIT Press , 2005 .
[ 19 ] G . E . Hinton , “ Training products of experts by minimizing contrastive divergence , ” Neural Computation , vol . 14 , no . 8 , pp . 1771–1800 , 2002 .
[ 20 ] R . Salakhutdinov and G . Hinton , “ Learning a nonlinear embedding by preserving class neighbourhood structure , ” in Proceedings of the International Conference on Artificial Intelligence and Statistics ( AISTATS ) , San Juan , Puerto Rico , 2007 .
[ 21 ] G . Griffin , A . Holub , and P . Perona , “ Caltech 256 object category dataset , ” Caltech , Tech . Rep . , 2007 .
[ 22 ] T . S . Chua , J . Tang , R . Hong , H . Li , Z . Luo , and Y . Zheng , “ NUS WIDE : a real world web image database from national university of singapore , ” in Proceedings of the ACM International Conference on Image and Video Retrieval ( CIVR ) , Santorini , Greece , 2009 .
