2012 IEEE 12th International Conference on Data Mining 2012 IEEE 12th International Conference on Data Mining
Self Training with Selection by Rejection
Yan Zhou , Murat Kantarcioglu , and Bhavani Thuraisingham
Department of Computer Science
University of Texas at Dallas
{yan.zhou2 , muratk , bhavanithuraisingham}@utdallasedu
Richardson , TX 75080
Abstract—Practical machine learning and data mining problems often face shortage of labeled training data . Self training algorithms are among the earliest attempts of using unlabeled data to enhance learning . Traditional self training algorithms label unlabeled data on which classifiers trained on limited training data have the highest confidence . In this paper , a selftraining algorithm that decreases the disagreement region of hypotheses is presented . The algorithm supplements the training set with self labeled instances . Only instances that greatly reduce the disagreement region of hypotheses are labeled and added to the training set . Empirical results demonstrate that the proposed self training algorithm can effectively improve classification performance .
Keywords semi supervised learning , self training
I . INTRODUCTION
Semi supervised learning ( SSL ) techniques are developed for learning tasks where the amount of labeled data is insufficient for learning a hypothesis with good generality . The common assumption is that labeled data and unlabeled data are independent and identically distributed , and some unlabeled data can be mapped to a specific label with a high likelihood in a predefined framework . Frameworks differ in how unlabeled data is blended in with labeled data . Most frameworks fall into the following categories : confidencebased , model based , graph based , and multi view based [ 1 ] . A detailed survey of semi supervised learning techniques can be found in Zhu [ 2 ] . In this paper , we focus on one of the simplest confidence based SSL techniques , self training . Self training [ 3 ] , [ 4 ] , also known as self teaching , is one of the earliest techniques using both labeled and unlabeled data to improve learning . Given a set of labeled data L and unlabeled data U , self training proceeds as follows : train a classifier h using L , and classify U with h ; select a subset . ⊂ U for which h has the highest confidence scores ; U . from U . Repeat the process add U until the algorithm converges . In this paper , we consider semi supervised learning as a search problem . The goal is to find subsets of unlabeled data and their proper labels to compensate for the lack of labeled data . Inclusion of the selected unlabeled data would effectively correct the current decision boundary . Unlabeled data improves learning if : 1 ) it supplies additional information on the true decision boundary ; 2 ) it retains the overall data distribution ; and 3 )
. to L and remove U
1550 4786 2012 1550 4786 2012 US Government Work Not Protected by US Copyright US Government Work Not Protected by US Copyright DOI 101109/ICDM201256 DOI 101109/ICDM201256
586 795
2 . ) it misguides the training process and causes costly maladjustment of the decision boundary , which in turn backfires and causes a significant increase in misclassification of the labeled data .
Case 2 A biased selection of unlabeled data favors one particular class of data . This happens when the classifier used to label the unlabeled data is biased . In this case , overfitting becomes more likely . Case 3 An overwhelming amount of noise is introduced to the training set during the process of expansion . If unbounded noise is introduced , eventually classification of labeled data will become no better than random guessing . As a result , adding unlabeled data would actually hurt the classifier ’s performance . it introduces bounded noise . Without the information of the true labels , it is difficult to foretell if a selected pool of unlabeled instances hold the aforementioned properties . We find it helpful to examine the opposite aspect of using unlabeled data—cases where the above properties are violated and therefore labeling unlabeled data is not rewarding . Case 1 Selected unlabeled data does not add more information on the decision boundary . There are two possible consequences when the selection is used for training : 1 . ) there is very little impact on the classification of given labeled data , yet the classification accuracy deteriorates in general ;
Therefore if the inclusion of an unlabeled data pool has a marginal impact on classification of labeled data , it is not clear whether we are approaching the true decision boundary . On the other hand , if the addition of unlabeled data leads to much greater misclassification of the labeled data , we can safely conclude that the unlabeled data with the current assigned labels are harmful , either because they are incorrectly labeled or they have been sampled disproportionately . Thus we need to assign different labels to the examples or rebalance the data distribution in the selection . This is the foundation of our self training algorithm .
With a classifier h , we let h label a subset of the unlabeled data , but invert the labels assigned by h . In other words , if h predicts “ + ” for an unlabeled instance X , we assign “ – ” to X as its label in a binary classification problem . In multi classification problems , if h predicts . for an unlabeled instance X , we assign . to X where . is any legitimate label but We then train a new classifier on the training set that includes the newly labeled unlabeled data . If the new classifier holds its classification accuracy on the labeled data , we select a different subset from the unlabeled pool and restart the process . If , on the other hand , the new classifier fails largely on the labeled data , we restore the labels of the selected unlabeled instances assigned by h and add the selection to the training set . We repeat the process until we can no longer identify such a subset of unlabeled data to add to the training set . We refer to this technique as selection by rejection . This proof by contradiction style of data selection sets our algorithm apart from other semi supervised learning algorithms .
In the rest of the paper , we first present the algorithm of our self training technique in Section II . In Section III , we present the theoretical foundation of this technique . We demonstrate the empirical results in Section IV and discuss the related work in Section V . Finally , we conclude our work in Section VI .
II . SELF TRAINING ALGORITHM
We now discuss the design of our self training algorithm . Given a set of labeled data L and a set of unlabeled data U , a classifier h is trained on L initially . h is used to label all the unlabeled data U . The algorithm then randomly selects and assesses N subsets U1 , . . . , UN of U each of which , if included for training , is likely to improve classification . One of the subsets will be selected and added to the training set . h is retrained on the expanded training set . The process repeats until stopping criteria are met . Detailed algorithm is given in Figure 1 . There are several issues we have to resolve : First , how to obtain a set of unlabeled data for which the current classifier is likely to provide correct labels ? Second , how to find the most effective subset among the N subsets ? And finally , when does the algorithm halt ?
A . Candidate Unlabeled Subsets
In theory if we sample enough data , the sample noise introduced as a result of misclassification by a given hypothesis would not shadow the benefits of an augmented labeled data set [ 5 ] . In practice it is nearly impossible to estimate how much is enough for two reasons : 1 ) we do not know exactly the size of the hypothesis space |H| ; 2 ) there are often not enough samples ( including the unlabeled ones ) for us to draw enough samples to compensate for the noise . Therefore we need better “ engineering ” for data selection to ensure low sampling noise . The brute force approach would explore each possible combination of unlabeled data in the search space . Like many other brute force approaches , its computational intractability calls for a better search strategy that can greatly reduce the search effort . A good starting point would be sets of unlabeled data each of which is highly likely labeled correctly by the classifier . For each unlabeled data point , we estimate its distance to the current decision boundary . An unlabeled data point that is at least δ distance away from the decision boundary would be considered as a good candidate . The larger the value of δ , the more likely the assigned label is correct . It has been shown that error variance is greatest near decision boundary [ 6 ] . Therefore , δ must be large enough to avoid noise caused by great uncertainty in the vicinity of the decision boundary . On the other hand , an overly conservative δ would bias toward outliers . In a binary classification problem , a data point x is considered to be on the decision boundary if p+ p− = 1 where p+ is the probability of x being positive , and p− is the probability of x being negative . To facilitate multiclass classification problems , we estimate the distance to a decision boundary as the negation of the entropy of class probabilities ( line 9 in Algorithm 1 ) .
In each round of self training we randomly sample N subsets of unlabeled data from U that are δ distance away from the current decision boundary ( lines 11–14 in Algorithm 1 ) . As self training progress to the next round , the unlabeled data set U is updated using random sampling with replacement according to the weight of each unlabeled data point . Unlabeled instances that have been selected for training in previous rounds will have weights that have decayed exponentially ( lines 23–24 in Algorithm 1 ) . The higher the decay rate , the less likely the same instance would be added to the training set more than once . Next , we discuss how to choose the most influential subset from the N subsets of preselected unlabeled data .
B . Selection by Transformation
There are two reasons that our algorithm begins with multiple candidate subsets in each round . First , not all unlabeled data , despite correct labeling , is helpful in improving supervised learning . This phenomenon has been emphasized and extensively researched in the field of active learning . Second , unlabeled data points that are “ far away ” from the decision boundary could be outliers . Therefore , simply adding unlabeled data that is situated in the classifier ’s high confidence zone is not always a good idea . To find a set of unlabeled data that is truly informative , we resort to our aforementioned selection by rejection strategy .
In essence , we transform the problem of selecting a set of unlabeled data to the problem of rejecting the same set of unlabeled data with its class membership inverted . Let ˆh denote the classifier trained on L ∪ U , where U has been labeled using the classifier trained on the current labeled data set ( line 15 in Algorithm 1 ) . As discussed in the previous section , N subsets of unlabeled data are randomly selected with older samples decay in weight exponentially in each round . For each of the preselected N subsets of unlabeled . data Uj ( j = 1 , . . . , N ) , we create a subset U j by inverting . the label .j to . j for each data point in Uj , where .j is the label predicted by the classifier trained on the labeled data j ∈ L\{.j} and L denotes the class space . A classifier . set , .
587796 unlabeled data , N is the number of pre selected subsets at the beginning of each round , and finally δ , the distance threshold , is set to the median of distances of all unlabeled data to the current decision boundary . Selection of the values of the constants is discussed in a later section .
Input : L , L0—a set of labeled data
U—a set of unlabeled data T —a set of test data c—knob constant , α—decay rate
2 : w[xi ] = 1.0
Output : Predictive accuracy on T 1 : for all i such that xi ∈ U do 3 : w is the average weight over all xi ∈ U 4 : repeat 5 : 6 :
Train classifier h on L Uδ = ∅ // unlabeled data δ distance away from decision boundary for all i such that xi ∈ U do h predicts label arg maxfik∈fi p(.k|xi ) for xi d[xi ] =
∀fik∈fi p(.k|xi ) · log p(.k|xi ) fid // Median of d—distances to decision
.
δ = boundary for all i such that xi ∈ U do if d[xi ] > δ then Uδ = Uδ ∪ {xi}
Randomly sample ( with weight decay ) N subsets U1 , . . . , UN from Uδ Train classifier ˆh on L ∪ U for j = 1 to N do j = ∅ . U for all i such that xk ∈ Uj do . k =SETLABEL(xk , . x . U j = U j ∪ {x . k} .
Train classifier hj on L ∪ U Ur = arg maxUj Δdj(ˆh , hj , L0 ) for all i such that xi ∈ Ur do w[xi ] = w[xi ] · α k ∈ \{k} ) . j ∪ ( U − Uj ) . hj is trained on each L∪U j∪(U−Uj ) . ˆh and hj|∀j∈[1,N ] are . used to estimate Δdj—the maximum disagreement between ˆh and hj|∀j∈[1,N ] on L0—the original labeled data plus unlabeled data points added with high confidence ( line 27 in Algorithm 1 ) . The theoretical implication of Δdj and its computation ( given in Equation 1 ) will be discussed in the next section . The subset Ur ∈ {U1 , . . . , UN} that maximizes Δdj is “ rejected ” . Lines 16–22 in Algorithm 1 describe this process . After reverting the labels of data in Ur , all instances in Ur will be added to the training set for training a new classifier ( line 26 in Algorithm 1 ) . By adding more informative data to the training set in each round , we gradually guide the decision boundary to approach the true decision boundary .
Theoretically , optimal subsets can be identified only when N approaches positive infinity . In practice , N needs to be large enough to include subsets whose membership inversion would cause significant increase in misclassification of the originally labeled data . It is not surprising that our algorithm does not require a very large N since the pre selection step ensures that most pre selected unlabeled data , if mislabeled , will cause drastic classification performance drop . This is because the selected data points are located in areas where class membership is less uncertain .
C . Stopping Criteria
The last issue we need to resolve is when the algorithm should stop . We let the algorithm continue running until there is no unlabeled data that carries significant boundary information left . In each round , we resample the unlabeled data set U . As more and more unlabeled data points that satisfy the distance requirement have been labeled at least once , the average weight w of data points in U will decline at an increasingly slower pace . In the extreme case where all unlabeled data points are equally important ( informative ) and are labeled all at once , the average rate will drop to α—the rate of weight decay—after the first round of selflabeling and the algorithm can halt at that point . In our algorithm , we check if w < c · α ( line 29 in Algorithm 1 ) , where c , a positive constant , is the knob that controls how soon the algorithm terminates . A larger c terminates the algorithm sooner than a smaller c .
To summarize we present the following nutshell version of our self training algorithm . First , we assign labels to the unlabeled data with the current classifier ; next we randomly select , as many as processing time permits , N subsets of unlabeled data that are δ distance away from the decision boundary ; we select from the N subsets the most effective subset to add to the training set using selection by rejection . The process repeats until no more qualified unlabeled data has not been added to the training set .
There are several constants presented in the proposed algorithm . c is the knob constant that determines how soon the algorithm halts , α is the decay rate for the weights of
7 : 8 : 9 : 10 :
11 : 12 : 13 : 14 :
15 : 16 : 17 : 18 : 19 : 20 : 21 : 22 : 23 : 24 : 25 : 26 : 27 : 28 :
Update w L = L ∪ Ur L0 = L0 ∪ {x ∈ Ur|h highly conf ident f or x} Resample U with replacement based on w
29 : until w < c · α 30 : h 31 : return 1 − error(h
∗
∗
= arg minh error(h , L0 )
, T )
Figure 1 . Self Training
III . THEORETICAL ANALYSIS
This section presents the theoretical justification of the self training algorithm . We first define a sampling process where there is a noisy oracle that increases the mislabeling rate p by introducing additional random noise to the
588797 classification noise η . The action of the oracle models the label inversion process in our self training algorithm . We show that regardless of η , our mislabeling rate p can be confined to less than 1 2 , thus we can learn an good hypothesis ( Theorem 3.5 and 3.8 ) [ 5 ] . Naturally it follows that the probability of learning an good hypothesis increases as we turn off the noise oracle ( Theorem 36 ) In other words , we show that using the sampling process in our self training algorithm , we can learn an good hypothesis with a monotonically increasing probability for any arbitrary ∈ ( 0 , 1 2 ) . We also argue that by using label inversion and minimizing the disagreement region , we can select unlabeled examples that are not only more likely labeled correctly but more informative compared to the rest in the unlabeled data set . Next , we formally define the sampling process .
Given a set of labeled data L and a set of unlabeled data U , a hypothesis h is formulated on L . Let η = errorD(h ) denote the error of the hypothesis h under distribution D . We define the following sampling process for our self training algorithm :
Randomly draw M samples from D and let them pass through an oracle h before being added to the training set . With probability η = errorD(h ) the oracle returns an incorrectly labeled example ( x , . ) , and with probability 1−η it returns a correctly labeled example ( x , ) In addition , each example x has to pass through a noise oracle that randomly inverts the decision of h from . to . or from . to . with a probability of ηf .
A hypothesis h is good if it has an error bounded by with a bounded confidence δ , that is ,
P r[error(h ) > ] < δ .
In other words , the probability that h has an error greater than is less than δ . A hypothesis is bad if its error rate is greater than . We now prove that with the above sampling process we can learn an good hypothesis with a monotonically increasing probability for any arbitrary ∈ ( 0 , 1
2 ) in each round of our self training algorithm .
Let p be the probability that an example x is mislabeled . Therefore , p is the probability that h correctly labels x but the noise oracle inverts the label to the incorrect one , plus the probability that h mislabels x and the noise oracle does not invert the label . Thus , p = η(1 − ηf ) + ( 1 − η)ηf . We first consider the case where 0 < η < 1 2 .
Lemma 3.1 : If 0 < η < 1 follows that 0 < p < 1 2 .
Proof :
2 , choose an ηf ∈ ( 0 , 1
2 ) . It p = η(1 − ηf ) + ( 1 − η)ηf
= η(1 − 2ηf ) + ηf ( 1 − 2ηf ) + ηf <
1 2
=
1 2
Lemma 3.2 : If 0 < η < 1
2 , then η < p < 1 2 . Proof : According to Lemma 3.1 , p < 1 2 . We only need to prove that η < p . p − η = η(1 − ηf ) + ( 1 − η)ηf − η
= ηf − 2ηηf = ηf ( 1 − 2η ) > 0
Let m be the number of samples drawn from D , and |H| be the size of a finite set of hypotheses .
Theorem 3.3 :
[ 5 ] Given a mislabeling rate p < 1
2 ( See
Lemma 3.1 ) , if we draw a sample of size 2|H| δ
η2(1 − 2p)2 ln( m ≥
2
) from D , the probability that an η bad hypothesis minimizes disagreement with the sample is at most δ . We simple substitute p for the upper bound of the noise rate and η for in the noisy PAC model . = η
( 1− ηf ) + ( 1− η . )ηf . . good If a sample S of size m is sufficient for learning an η hypothesis with the mislabeling rate p , S is sufficient for . good hypothesis with the mislabeling rate p learning an η
Lemma 3.4 : Let η
. < η and p
.
.
Proof : p = η(1 − ηf ) + ( 1 − η)ηf
= η(1 − 2ηf ) + ηf 2 , therefore 1− 2ηf > 0 . Thus p decreases Given that ηf < 1 . < p . According to Themonotonically with η . Therefore , p orem 3.3 , it requires a smaller sample to achieve the same classification error with a lower mislabeling rate . Therefore , . good hypothesis with if m is large enough to learn an η . good the mislabeling rate p , m is sufficient to learn an η hypothesis when the mislabeling rate is reduced to p 2 ) < η and δ ∈ ( 0 , 1 ) , let n = log2( 1 n , our self training algorithm outputs a hypothesis with errD(h ) ≤ with probability of at least 1− δ after n rounds . The number of samples drawn in each round i is
Theorem 3.5 : For any ∈ ( 0 , 1
)( and k = ( η )
1
M =
2
( kiη)2(1 − 2p)2 ln(
2|H| δ
) .
Proof : According to Theorem 3.3 and Lemma 3.4 , in each self training round i , we draw M samples to reduce errD(hi ) by a factor of 1 k . After n rounds of self training , we reduce errD(h ) to knη = η η = . The sample complexity is
O(ln(
1
)
1
2(1 − 2p)2 ln(
589798
2|H| δ
) ) .
M sets the bound of the number of examples we need to draw in each round in order to reduce the learning error by a fixed factor of k . So far we have proved that with a sample mislabeling rate of p our self training algorithm outputs an good hypothesis after n rounds of drawing M samples . We now introduce our selection by rejection sampling technique to the self training procedure . Let Li be the set of labeled data in the ith round of our self training procedure . Let H = i} be a finite set of N hypotheses . Each {h1 , h2 i ∈ H is learned from Li ∪ Uj of M samples randomly hj drawn from D ( M is given in Theorem 35 ) Examples in Uj are labeled by hi but with ηf probability the labels are inverted by the noise oracle . From Theorem 3.5 we know that for each hj
, . . . , hN i i i
( x ) = h ∗
( x ) ] ≤ k i
η
∗
) = P rD[hj d(hj ∗ is the target concept with errD(h
) = 0 . In other where h words , all the hypotheses in H are at least as good as hi . Now , suppose we turn off the noise oracle and repeat the above process . Let ˆH be a set of N hypotheses in which each hypothesis is learned from Li∪Uj—the same set of samples drawn from U but with ηf = 0 . According to Lemma 3.2 and Theorem 3.5 , ∗ i d(ˆh j , h j(x ) = h ∗
( x ) ] ≤ k i ) = P rD[ˆh
η . i i ∈ H ∗ , h i
Let
Δdj = max p∈{1,,N} d(hp i
∗ , h
) − d(ˆh
∗ i j , h
) .
The quantity Δdj consists of two parts : i|h ∗ i|h ∗ i i j , hj i
)
( 1 )
, hv j , hj j , hj i|h ∗
) + Δd(hu
Δdj = Δd(ˆh i|h ∗ where Δd(ˆhi ) denotes the disagreement between hyi|h ∗ ∗ and Δd(hu i with respect to h potheses ˆhi j and hj ) , hv i ∈ H i ∈ H and hv denotes the disagreement between hu ∗ where u , v ∈ {1 , . . . , N} and u = v . with respect to h In other words , Δd(ˆhi ) assesses whether the labels inverted by the noise oracle are causing the decision boundary to drift away from the true decision boundary ; and ) signifies whether one set of samples drawn Δd(hu from U is more informative than the other . The former has a natural affinity with semi supervised learning where the goal is to draw samples from the unlabeled data set to enhance the labeled data set without introducing too much noise ; while the latter demonstrates the idea of active learning in which the goal is to draw the most informative samples to minimize ∗ and hypotheses in the the disagreement region between h version space . Let i|h ∗
, hv i is more likely that
“ reject ” . The unlabeled examples in U have two properties : 1 . ) it they are labeled correctly by the oracle—because inverting the labels causes the decision boundary to move farther away from the true decision boundary ( that is , larger Δd(ˆhi ) ) ; 2 . ) they are located in the maximum disagreement region between two hypotheses in H . Therefore , examples in U are not only more likely labeled correctly by the oracle but more informative compared to examples in Uj = U . i|h ∗ j , hj
In practice , we do not know M—the bound of the samples to be drawn in the ith round . Instead , we just use all available unlabeled data to train ˆhi j , that is , ˆh in Algorithm 1 . Also ∗ needs ∗ is unknown . In theory h notice that in Equation 1 , h to be known to compute this difference , which is impossible in general . We resolve the difficulty by estimating the difference on L0—the original labeled data plus data labeled with high confidence . Although this estimate is rough , it is sufficient to manifest the deterioration in classification performance when the labels are incorrectly inverted . Theorem 3.6 : In each self training round i , U ∈ {U1 , . . . , UN} is selected as a result of selection by rejection . Let Li+1 = Li ∪ U where U is labeled by hi with ηf = 0 . The probability that we learn an bad hypothesis hi+1 in the next round is monotonically decreasing , that is ,
∗ P r[d(hi+1 , h
∗ ) > ] < P r[d(hi , h
) > ] .
Sketch of Proof According to Theorem 3.5 , U ∈ {U1 , . . . , UN} is sampled to monotonically reduce errD(hi ) to a fraction k of errD(hi ) in the presence of sample noise that consists of classification noise introduced by hi and random noise introduced by the noise oracle . When forming the training set for round i + 1 , the noise oracle is turned off ( because inverted labels are restored ) and the sample noise is the sole contribution from hi . Therefore the proof follows naturally from Lemma 3.4 and Theorem 35 .
So far we have only considered the case where 0 < η < 1 2 . When η > 1 2 , we simply increase the noise rate in the 2 < ηf ≤ 1 . In addition , all the unlabeled noise model to 1 examples in U ( after being labeled by hi ) need to have their labels inverted before they are added to the training set . Similar lemmas and theorems are developed as follows . 2 < ηf ≤ 1 . Lemma 3.7 : if 1 According to Lemma 3.7 , when η = errorD(h ) > 1
2 < η ≤ 1 , p ∈ ( 0 , 1
2 , we need to increase the noise rate of the noise oracle so that 2 . Let η = 1 − η . the combined error p is bounded below 1 2 and a mislabeling rate
Theorem 3.8 : Given 1 ≥ η > 1 2 ( See Lemma 3.7 ) , if we draw a sample of size
2 ) if 1 p < 1 m ≥
2
( η)2(1 − 2p)2 ln(
2|H| δ
)
U = arg max
Uj∈{U1,,UN} Δdj be the set of unlabeled samples drawn from D that maximizes Δdj . Thus U ( with inverted labels ) is our target to from D , the probability that an η bad hypothesis minimizes disagreement with the sample is at most δ .
Theorem 3.9 : In each self training round i , let U f be the set of unlabeled examples selected ( through rejection ) . Let
590799
Li+1 = Li ∪ U f where U f is labeled by hi but having each label inverted . The probability that hi+1 is an bad hypothesis decreases monotonically in the next round , that ∗ is , P r[d(hi+1 , h
∗ ) > ] < P r[d(hi , h
) > ] .
Theorem 3.9 states that even when errorD(hi ) > 1 2 , self training is still effective if we let hi label the selected unlabeled samples but having all the labels inverted afterwards . For multi class problems , we can rank the class labels according to the prior distribution , and have a label inverted to the one with the highest rank other than itself .
IV . EXPERIMENTAL RESULTS
We tested our self training algorithm on 17 UCI [ 7 ] data sets , 13 of which are binary classification problems , and 4 of which are multi class classification problems . We chose as our baseline classifier the logistic regression algorithm in the latest distribution of Weka [ 8 ] . We also tested the standard self training algorithm with logistic regression as the baseline and the state of the art semi supervised support vector machine algorithm on the same data sets for comparison . In the binary classification cases , our self training algorithm improved over all data sets , and it clearly outperformed the standard self training algorithm [ 3 ] and the SVMLight [ 9 ] implementation with LOQO solver [ 10 ] , [ 11 ] in most cases . In addition , the SVM method seemed to be less consistent compared to the other two algorithms . In the cases of multiclass classification , our self training algorithm again improved over all data sets . It outperformed the standard selftraining algorithm in all four cases and the univerSVM [ 12 ] implementation of the semi supervised SVM method in two out of four cases . Parameters in all SVM implementations were carefully selected for each data set as suggested so that the algorithm would converge . Parameter values in various ranges were tried . Little variations were observed whenever the algorithm converges .
A . Binary Classification
We selected 13 binary classification problems from the UCI repository . Table I shows the number of labeled , unlabeled , and test instances of each data set . For small data sets , we reserve at most 10 % of the data as labeled data , the rest is used as test data . For very large data sets , including sick , mushroom , and kr vs kp , a very small percentage ( 5 % , 0.5 % , 1 % , respectively ) of data was set for training . Unlabeled data was selected as a subset of each test set . Unlabeled data and test data are the same for small data sets . For large data sets , 20 % of the test data is used as unlabeled data .
Table II shows the classification results when the initial labeled data is stratified . It presents in order the predictive accuracy of the logistic regression algorithm ( LR ) when only the labeled data is used for training , the standard self training algorithm , our proposed self training algorithm , and the semi supervised SVM method . Our self training results improved over all data sets . In addition , it output
STATISTICS OF THE SELECTED UCI DATA SETS .
Table I
Labeled Data set 44 Vote 21 Sonar 189 Sick Mushroom 51 6 Labor 32 Kr vs kp 36 Ionosphere 16 Hepatitis Heart stat 27 77 Diabetes 100 Credit g 69 Credit a Breast w 70
Unlabeled 391 187 716 1614 51 632 315 139 243 691 900 621 629
Test 391 187 3583 8073 51 3164 315 139 243 691 900 621 629 better results than both of the other two algorithms in 10 out of 13 cases . The semi supervised SVM improved on 8 of the 13 data sets , but failed to improve over the rest of the 5 data sets . The proposed self training algorithm demonstrated better results than the SVM method on 10 of the 13 data sets . The standard self training algorithms improved on 6 of the 13 data sets . It was outperformed by our algorithm in 11 out of the 13 cases . The advantage of our self training algorithm becomes more obvious when the initial labeled data is not stratified , as shown in Table III . Our self training algorithm again improved over all data sets , while the SVM method improved on 5 data sets and the standard self training improved on 7 data sets . Our self training algorithm performed better than SVM and the standard self training algorithm on 12 of the 13 data sets . All the results reported in this paper are the average over 100 runs . Since the semi supervised SVM always labels data , for fair comparison we excluded cases where no performance change was observed simply because no data or very little data was labeled . Constants are set as follows : N = 10 , c = 4 , and α = 0125 We also tried other constant values . Our observations are as follows : large N values slow down the algorithm without much reward . Since each unlabeled subset is carefully selected to avoid the high uncertainty region near the decision boundary , the advantage of large Ns is trivialized . Large c values lead to more rounds of labeling and thus more noise that eventually will hurt the classification accuracy . Large α values reduce the chance of sample replacement , making successive labeling rounds more dependent on the preceding ones . When the data set is small , we prefer a small α value .
For the four data sets that have a very low sample todimension ratio , we tested again with a higher labeledto unlabeled data ratio ( > 4 ) . The results are shown in Table IV and Table V . Our self training algorithm improved significantly over all four data sets and demonstrated clear advantage over the other two algorithms . The standard selftraining algorithm had marginal improvement on all four data sets . The SVM method showed less improvement on all
591800
CLASSIFICATION ACCURACIES ON 13 UCI DATA SETS WHEN THE INITIAL LABELED DATA IS STRATIFIED . THE FIRST COLUMN IS FOR LOGISTIC
REGRESSION WHEN ONLY LABELED DATA IS USED ; THE SECOND COLUMN IS THE STANDARD SELF TRAINING ALGORITHM ( STD SELF TRAINING ) ,
THE THIRD COLUMN IS FOR OUR SELF TRAINING ALGORITHM , AND THE LAST ONE IS FOR THE SEMI SUPERVISED SVM METHOD ( SS SVM ) .
Table II
Data set Vote Sonar Sick Mushroom Labor Kr vs kp Ionosphere Hepatitis Heart statlog Diabetes Credit g Credit a Breast w
Labeled Only 0932±0022 0657±0056 0956±0007 0937±0024 0683±0115 0818±0069 0806±0034 0769±0051 0757±0044 0741±0022 0696±0022 0829±0023 0942±0017
Std Self Training 0.951 ±0.098 0.638 ±0.097 0.954 ±0.094 0.930 ±0.097 0.785 ±0.102 0.795 ±0.124 0.809 ±0.090 0.772 ±0.102 0.744 ±0.089 0.739 ±0.077 0.700 ±0.073 0.849 ±0.088 0.936 ±0.097
Self Training 0953±0015 0696±0051 0961±0006 0962±0016 0777±0110 0858±0063 0833±0027 0814±0030 0786±0035 0752±0019 0714±0018 0849±0014 0952±0012
SS SVM 0948±0019 0704±0052 0894±0016 0947±0022 0804±0100 0665±0132 0816±0032 0810±0054 0767±0018 0736±0023 0679±0023 0826±0016 0966±0005
CLASSIFICATION ACCURACIES ON 13 UCI DATA SETS WHEN THE INITIAL LABELED DATA IS NOT STRATIFIED . THE FIRST COLUMN IS FOR LOGISTIC REGRESSION WHEN ONLY LABELED DATA IS USED ; THE SECOND COLUMN IS FOR THE STANDARD SELF TRAINING ALGORITHM , THE THIRD COLUMN
IS FOR SELF TRAINING , AND THE LAST COLUMN IS FOR THE SEMI SUPERVISED SVM METHOD ( SS SVM ) .
Table III
Data set Vote Sonar Sick Mushroom Labor Kr vs kp Ionosphere Hepatitis Heart statlog Diabetes Credit g Credit a Breast w
Labeled Only 0931±0027 0640±0069 0955±0008 0928±0031 0592±0180 0799±0073 0804±0039 0763±0051 0751±0043 0742±0018 0694±0019 0829±0022 0939±0020
Std Self Training 0951±0097 0.618 ±0.098 0953±0096 0924±0100 0.770 ±0.119 0809±0120 0808±0093 0773±0102 0740±0087 0734±0080 0703±0073 0848±0087 0937±0097
Self Training 0954±0011 0683±0064 0959±0007 0958±0019 0708±0152 0841±0065 0825±0033 0807±0033 0777±0043 0755±0015 0714±0014 0850±0014 0952±0012
SS SVM 0913±0040 0679±0061 0907±0042 0923±0032 0719±0123 0537±0051 0805±0046 0650±0083 0717±0067 0655±0054 0675±0026 0830±0027 0950±0021 data sets , and when the initial labeled data was not stratified , the SVM method had either no improvement or marginal improvement on the four data sets .
B . Multi class Classification
We also tested on four multi classification data sets from the UCI repository . The four data sets are zoo , waveform5000 , vowel , and vehicle . For each data set , we allocated 10 % for training , and the rest for testing . Unlabeled data is again selected as a subset of each test set . Table VI shows the classification results of the four settings—labeled only , the standard self training algorithm , our self training algorithm , and the SVM method—when the labeled data set is stratified . Our self training algorithm successfully improved over all data sets . It also outperformed the SVM method on two data sets . The SVM method improved the classification performance on two data sets , but largely hurt the performance on the other two data sets . The standard self training algorithm did not improve on any of the data sets . When the initial labeled data was not stratified , we observed similar results as shown in Table VII .
V . RELATED WORK
Hearst [ 13 ] and Yarowsky [ 3 ] use self training for resolving word sense disambiguation . Riloff et al . [ 14 ] use self training through multi level bootstrapping for selecting extraction patterns . They also use self training to identify subjective nouns [ 4 ] . Rosenberg et al . [ 15 ] build a selftraining model for object detection from images . The basic assumption these techniques have developed upon is that an unlabeled instance can be labeled for training if the underlying classifier has high confidence in its prediction . Our self training algorithm gradually improves a classifier through a similar boot strapping style of self teaching . However , we do not consider all the unlabeled data that the classifier has confidently labeled equally informative . Some of the unlabeled data could be outliers , some others could be correctly labeled but carries very little additional information on the true decision boundary . Our self training algorithm employs a random selection procedure that rules out these two cases as we discussed earlier .
A group of probabilistic approaches have also been proposed for using unlabeled data in supervised learning . Unlabeled data , due to its large amount , can be used to identify
592801
CLASSIFICATION ACCURACIES ON 4 SMALL UCI DATA SETS WHEN THE INITIAL LABELED DATA IS STRATIFIED .
Table IV
Data set Sonar Labor Ionosphere Hepatitis
Labeled Only 0756±0084 0831±0069 0869±0044 0817±0077
Std Self Training 0.747 ±0.120 0.892 ±0.125 0.871 ±0.104 0.828 ±0.114
Self Training 0814±0078 0935±0062 0903±0040 0885±0071
SS SVM 0782±0082 0908±0075 0896±0046 0843±0093
CLASSIFICATION ACCURACIES ON 4 UCI DATA SETS WITH LOW SAMPLE TO DIMENSION RATIOS WHEN THE INITIAL LABELED DATA IS NOT
Table V
STRATIFIED .
Data set Sonar Labor Ionosphere Hepatitis
Labeled Only 0742±0073 0836±0077 0868±0051 0812±0079
Std Self Training 0.746 ±0.119 0.880 ±0.123 0.871 ±0.103 0.826 ±0.115
Self Training 0802±0067 0931±0065 0899±0047 0882±0073
SS SVM 0722±0104 0829±0111 0883±0051 0827±0086
CLASSIFICATION ACCURACIES ON 4 MULTI CLASS UCI DATA SETS WHEN THE INITIAL LABELED DATA IS STRATIFIED .
Table VI
Data set Zoo Waveform 5000 Vowel Vehicle
Labeled Only 0677±0072 0851±0006 0507±0043 0676±0029
Std Self Training 0638±0141 0850±0086 0449±0060 0656±0077
Self Training 0740±0059 0855±0005 0526±0036 0695±0025
SS SVM 0843±0047 0794±0013 0263±0037 0720±0022
CLASSIFICATION ACCURACIES ON 4 MULTI CLASS UCI DATA SETS WHEN THE INITIAL LABELED DATA IS NOT STRATIFIED .
Table VII
Data set Zoo Waveform 5000 Vowel Vehicle
Labeled Only 0633±0127 0850±0006 0492±0040 0660±0036
Std Self Training 0595±0172 0849±0085 0444±0059 0647±0075
Self Training 0707±0103 0855±0005 0510±0037 0683±0030
TSVM 0787±0083 0793±0012 0262±0036 0718±0021 the mixture components of a generative model . Nigam et al . [ 16 ] apply EM with mixture models for using unlabeled data . They demonstrate promising results in the field of text classification . However , when the mixture model assumption is incorrect , mixture model based semi supervised learning can in fact hurt classification performance [ 17 ] . Several techniques have been proposed to address this issue [ 18 ] , [ 19 ] .
Graph based semi supervised learning methods treat labeled and unlabeled data as graph nodes . The edge between a pair of nodes entails similarity between the two corresponding data points . Blum and Chawala [ 20 ] propose a graph mincut solution to semi supervised learning . Discussions on other graph based approaches , including direct Markov random fields , Gaussian random fields and harmonic functions , manifold regularization , and many others can be found in Zhu and Goldberg ’s work [ 1 ] .
Blum and Mitchell present co training [ 21 ] that takes advantage of data sets that have two independent and redundant views . The algorithm improves supervised learning by letting two classifiers trained on the two different views label data for each other . Some other co training style semisupervised learning techniques employ multiple learners on single view data sets [ 22 ] , [ 23 ] .
In this paper , we focus on self training and therefore only compare our algorithm to the self training SSL algorithms .
VI . CONCLUSIONS
We present a robust self training algorithm that improves learning by selecting and labeling more informative unlabeled data . A random selection procedure , namely selection by rejection , is presented to guide the search for informative unlabeled data subsets . The experiment results demonstrate that our proposed algorithm is in general more reliable and more effective than the standard self training algorithm and the semi supervised SVM method . Its classification performance is also more consistent across all datasets than the other two methods .
ACKNOWLEDGMENT
This work was partially supported by The Air Force Office of Scientific Research MURI Grant FA 9550 08 10265 and Grant FA9550 12 1 0082 , National Institutes of Health Grant 1R01LM009989 , National Science Foundation ( NSF ) Grant Career CNS 0845803 , NSF Grants CNS0964350 , CNS 1016343 , CNS 1111529 , and CNS 1228198 , Army Research Office Grant 58345 CS .
593802
REFERENCES
[ 1 ] X . Zhu and A . Goldberg , Introduction to Semi Supervised
Learning . Morgan and Claypool Publishers , 2009 .
[ 2 ] X . Zhu , “ Semi supervised learning literature survey , ” 2006 . [ Online ] . Available : http://wwwcswiscedu/∼jerryzhu/pub/ ssl survey.pdf
[ 16 ] K . Nigam , A . K . McCallum , S . Thrun , and T . Mitchell , “ Text classification from labeled and unlabeled documents using em , ” Machine Learning , vol . 39 , pp . 103–134 , 2000 .
[ 17 ] F . G . Cozman , I . Cohen , M . C . Cirelo , and E . Politcnica , “ Semi supervised learning of mixture models , ” in ICML 03 , 20th International Conference on Machine Learning , 2003 , pp . 99–106 .
[ 3 ] D . Yarowsky , “ Unsupervised word sense disambiguation rivalling supervised methods , ” in Proceedings of the Thirtythird Annual Meeting of the Association for Computational Linguistics , 1995 , pp . 189–196 .
[ 18 ] B . Shahshahani and D . Landgrebe , “ The effect of unlabeled samples in reducing the small sample size problem and mitigating the hughes phenomenon , ” IEEE Trans . On Geoscience and Remote Sensing , vol . 32 , pp . 1087–1095 , 1994 .
[ 19 ] D . Miller and H . Uyar , “ A mixture of experts classifier with learning based on both labelled and unlabelled data , ” in NIPS 9 . MIT Press , 1997 , pp . 571–577 .
[ 20 ] A . Blum and S . Chawla , “ Learning from labeled and unlabeled data using graph mincuts , ” in Proceedings of the Eighteenth International Conference on Machine Learning , 2001 , pp . 19–26 .
[ 21 ] A . Blum and T . Mitchell , “ Combining labeled and unlabeled data with co training , ” in Proceedings of the Eleventh Annual Conference on Computational Learning Theory , 1998 , pp . 92–100 .
[ 22 ] S . Goldman and Y . Zhou , “ Enhancing supervised learning with unlabeled data , ” in Proceedings of the Seventeenth International Conference on Machine Learning , 2000 , pp . 327–334 .
[ 23 ] Z H Zhou and M . Li , “ Semi supervised regression with co training , ” in Proceedings of the 19th international joint conference on Artificial intelligence , 2005 , pp . 908–913 .
[ 4 ] E . Riloff , J . Wiebe , and T . Wilson , “ Learning subjective nouns using extraction pattern bootstrapping , ” in Proceedings of the seventh conference on Natural language learning at HLTNAACL 2003 Volume 4 , ser . CONLL ’03 . Stroudsburg , PA , USA : Association for Computational Linguistics , 2003 , pp . 25–32 .
[ 5 ] D . Angluin and P . Laird , “ Learning from noisy examples , ”
Machine Learning , vol . 2 , pp . 343–370 , 1988 .
[ 6 ] J M Park and Y H Hu , “ On line learning for active pattern recognition , ” IEEE Signal Processing Letters , vol . 3 , pp . 301– 303 , 1996 .
[ 7 ] C . J . Merz and P . M . Murphy , “ UCI repository of machine learning databases , ” 1998 .
[ 8 ] M . Hall , E . Frank , G . Holmes , B . Pfahringer , P . Reutemann , and I . Witten , “ The weka data mining software : An update , ” SIGKDD Explorations , vol . 11 , no . 1 , 2009 .
[ 9 ] K . Scheinberg , “ An efficient implementation of an active set method for svms , ” Journal of Machine Learning Research , vol . 7 , pp . 2237–2257 , 2006 .
[ 10 ] R . Vanderbei , “ Loqo : An interior point code for quadratic programming , ” Princeton University , Tech . Rep . SOR 9415 , 1998 .
[ 11 ] A . Smola , “ pr loqo optimizer , ” http://wwwkernel machines org/code/prloqotargz
[ 12 ] F . Sinz , R . Collobert ,
J . Weston , and B . L . , “ Universvm : Support vector machine with large scale cccp functionality , ” http://wwwkybtuebingenmpgde/bs/people/fabee/ universvmhtml
[ 13 ] M . A . Hearst , “ Noun homograph disambiguation using local context in large text corpora , ” in University of Waterloo , 1991 , pp . 1–22 .
[ 14 ] E . Riloff and R . Jones , “ Learning dictionaries for information extraction by multi level bootstrapping , ” in Proceedings of the Sixteenth National Conference on Artificial Intelligence , 1999 , pp . 474–479 .
[ 15 ] C . Rosenberg , M . Hebert , and H . Schneiderman , “ Semisupervised self training of object detection models , ” in The Seventh IEEE Workshop on Applications of Computer Vision , 2005 .
594803
