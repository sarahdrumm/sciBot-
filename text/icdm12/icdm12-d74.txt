2012 IEEE 12th International Conference on Data Mining 2012 IEEE 12th International Conference on Data Mining
High Performance Offline & Online Distributed
Collaborative Filtering
Ankur Narang , Abhinav Srivastava
IBM India Research Laboratory
New Delhi , India
( annarang,abhin122)@inibmcom
Naga Praveen Kumar Katta
Princeton University New Jersey , USA nkatta@csprincetonedu
Abstract—Big data analytics is a hot research area both in academia and industry . It envisages processing massive amounts of data at high rates to generate new insights leading to positive impact ( for both users and providers ) of industries such as Ecommerce , Telecom , Finance , Life Sciences and so forth . We consider collaborative filtering ( CF ) and Clustering algorithms that are key fundamental analytics kernels that help in achieving these aims . High throughput CF and co clustering on highly sparse and massive datasets , along with a high prediction accuracy , is a computationally challenging problem . In this paper , we present a novel hierarchical design for soft real time ( less than 1 minute . ) distributed co clustering based collaborative filtering algorithm . We study both the online and offline variants of this algorithm . Theoretical analysis of the time complexity of our algorithm proves the efficacy of our approach . Further , we present the impact of load balancing based optimizations on multi core cluster architectures . Using the Netflix dataset(900M training ratings with replication ) as well as the Yahoo KDD Cup(2.3B training ratings with replication ) datasets , we demonstrate the performance and scalability of our algorithm on a large multi core cluster architecture . In offline mode , our distributed algorithm demonstrates around 4× better performance ( on Blue Gene/P ) as compared to the best prior work , along with high accuracy . In online mode , we demonstrated around 3× better performance compared to baseline MPI implementation . To the best of our knowledge , our algorithm provides the best known online and offline performance and scalability results with high accuracy on multi core cluster architectures .
I . INTRODUCTION
Collaborative filtering ( CF ) is a subfield of machine learning that aims at creating algorithms to predict user preferences based on past user behavior in purchasing or rating of items [ 1 ] , [ 2 ] . Here , the input is a set of known item preferences per user , typically in the form of a user item ratings matrix . This user item ratings matrix is typically sparse . The collaborative filtering problem is to find the unknown preferences of a user for a specific item , ie an unknown entry in the ratings matrix , using the underlying collaborative behavior of the user item preferences . Collaborative filtering based recommender systems are very important in e commerce applications . They help people more easily find items that they would like to purchase [ 3 ] . This enhances the user experience which typically leads to improvements in sales and revenue . Further , scientific disciplines such as Computational Biology and Personalized medicine ( risk stratification ) stand to gain immensely from CF [ 4 ] , [ 5 ] . CF systems are also increasingly important in dealing with information overload since they can lead users to information that others like them have found useful . With massive amounts of data ( terabytes to petabytes ) and high data rates in Telecom ( around 6B Call Data Records per day ) , Finance and other industries , there is a strong need to deliver soft real time training for CF as it will lead to further enhance the quality of experience of customers along with
1550 4786/12 $26.00 © 2012 IEEE 1550 4786/12 $26.00 © 2012 IEEE DOI 101109/ICDM2012128 DOI 101109/ICDM2012128
925 549 increase in revenue for the provider .
Typical approaches for CF include matrix factorization based techniques , correlation based techniques , co clustering based techniques , and concept decomposition based techniques [ 6 ] . Matrix factorization [ 7 ] and correlation [ 8 ] based techniques are computationally expensive hence cannot deliver soft real time CF . Further , in matrix factorization based approaches , updates to the input ratings matrix leads to non local changes which leads to higher computational cost for online CF . Concept Decomposition based technique [ 6 ] perform spherical k means followed by least squares based approximation of the original matrix . This work presents only sequential performance ( 13.5 minutes ) for training of the full Netflix dataset which is far from being considered soft real time . Coclustering based techniques [ 9 ] , [ 10 ] have better scalability but have not been optimized to deliver high throughput on massive data sets . Daruru et al . in [ 10 ] presented dataflow parallelism based co clustering implementation which did not scale beyond 8 cores due to cache miss and in memory lookup overheads . CF over highly sparse data sets leads to lower compute utilization due to load imbalance . For large scale distributed environment ( 256 nodes and beyond ) , load imbalance can dominate the overall performance and the communication cost becomes worse with increasing size of the cluster , leading to performance degradation . Thus , high computational demand and low parallel efficiency due to cache misses and load imbalance are the key challenges that need to be addressed to achieve high throughput distributed collaborative filtering on highly sparse massive data sets .
In order to optimize the parallel performance , achieve high parallel efficiency and give soft real time ( ˜1min ) guarantees on massive datasets , we designed a novel hierarchical approach for distributed co clustering . Specifically , this paper makes the following key contributions :
• We present a novel distributed hierarchical approach for both offline and online co clustering based collaborative filtering . Performance optimizations including load balancing and computation communication overlap have been incorporated for high throughput and soft real time ( less than 1 min . ) performance over highly sparse massive data sets .
• Analytical parallel time complexity analysis , establishes theoretically that our hierarchical design leads to performance gain of order O(log(π ) ) ( where π is number of row and column partitions of the input matrix ) over the best prior approach [ 11 ] .
• We demonstrate soft real time performance for both offline and online mode collaborative filtering using the Netflix Prize and Yahoo KDD Cup datasets on a 4096 node multi core cluster architecture ( Blue Gene/P 1 ) . For offline mode , we achieved a training time ( using Idivergence and C6 , Section III ) of around 9.38s with the full Netflix dataset and prediction time of 2.8s on 1.4M ratings with RMSE ( Root Mean Square Error ) of 0.87 ± 002 This is around 4× better than the best prior distributed algorithm [ 11 ] for the same dataset . For 900M ratings using Netflix dataset and 2.3B ratings using Yahoo KDD Cup dataset , we show soft real time performance along with high accuracy . For online mode , our algorithm delivers around 3× better performance compared to baseline MPI implementation .
II . RELATED WORK
Co clustering and collaborative filtering ( CF ) are fundamental data mining kernels used in many application domains such as Information Retrieval [ 12 ] , Telecom [ 13 ] , Financial markets , Life Sciences [ 4 ] . Hassan et al . in [ 5 ] evaluate the context of a specific clinical challenge , ie , risk stratification following acute coronary syndrome ( ACS ) . On over 4,500 patients , this research shows that CF outperforms traditional classification methods such as logistic regression ( LR ) and support vector machines ( SVMs ) for predicting both sudden cardiac death and recurrent myocardial infarction within one year of the index event .
Banerjee et al . in
[ 14 ] consider multiway clustering of a single tensor or a group of tensors over heterogeneous relational data , using Bregman ( Bregman divergence models a broad family of information loss functions that includes squared Euclidean distance , KL divergence , I divergence ) coclustering based alternate minimization algorithm and shows its advantages in the domains of social networks , e commerce using movie recommendation data as well as newsgroup articles . We optimize the Bregman co clustering algorithm [ 15 ] ( based on alternate minimization ) for distributed systems . Our novel hierarchical approach will also improve the distributed performance of the multi way clustering algorithm over heterogeneous relational tensor data .
Typical CF techniques are based on correlation criteria [ 8 ] and matrix factorization [ 7 ] . The correlation based techniques use similarity measures such as Pearson correlation and cosine similarity to determine a neighborhood of like minded users for each user and then predict the user ’s rating for a product as a weighted average of ratings of the neighbors . Correlationbased techniques are computationally very expensive as the correlation between every pair of users needs to be computed during the training phase . Further , they have much reduced coverage since they cannot detect item synonymy . The matrix factorization approaches include Singular Value Decomposition ( SVD [ 16 ] ) and Non Negative Matrix Factorization ( NNMF ) based [ 7 ] filtering techniques . They predict the unknown ratings based on a low rank approximation of the original ratings matrix . The missing values in the original matrix are filled using average values of the rows or columns . However , the training component of these techniques is computationally intensive , which makes them impractical to have frequent re training . Incremental versions of SVD based on folding in and exact rank 1 updates [ 17 ] partially alleviate this problem . But , since the effects of small updates are not localized , the update operations are not very efficient .
George et al in [ 9 ] studies a special case of the weighted Bregman co clustering algorithm . The co clustering problem is formulated as a matrix approximation problem with nonuniform weights on the input matrix elements . As in the case of SVD and NNMF , the co clustering algorithm also optimizes the approximation error of a low parameter reconstruction of the ratings matrix . However , unlike SVD and NNMF , the effects of changes in the ratings matrix are localized which makes it possible to have efficient incremental updates . This work presents parallel algorithm design based on co clustering . It compares the performance of the algorithm against matrix factorization and correlation based approaches on the MovieLens 2 and BookCrossing dataset [ 18 ] ( 269392 explicit rating(1 10 ) from 47034 users on 133438 books ) . We consider soft real time CF framework using hierarchical parallel coclustering optimized for multi core clusters using pipelined parallelism and computation communication overlap . We deliver scalable performance over much large data on multicore clusters .
Daruru et al . in
[ 10 ] use a dataflow parallelism based framework to study performance vs . accuracy trade offs of coclustering based CF . However , it doesn’t consider re training time for incremental input changes . Further , the parallel implementation does not scale well beyond 8 cores due to cache miss and in memory lookup overheads . We demonstrate parallel scalable performance on 4096 nodes of Blue Gene/P and 7× to 10× better training time and better prediction time . Further , while none of the prior work aims at massive scale performance , we provide theoretical and empirical analysis to demonstrate this scale of performance of our distributed algorithm . Hsu et al.in [ 19 ] study IO scalable co clustering by mapping a significant fraction of computations performed by the Bregman co clustering algorithm to an on line analytical processing ( OLAP ) engine . Kwon et al . in [ 20 ] study the scalability of basic MPI based implementation of coclustering . We deliver more than one order of magnitude higher performance compared to this work , by performing communication and load balancing optimizations along with novel hierarchical design for multi core clusters .
Ampazis in [ 6 ] presents results of collaborative filtering using Concept decomposition based approach . It has been empirically established by Dhillon et al . in [ 21 ] that the approximation power ( when measured using the Frobenius norm ) of concept decompositions is comparable to the best possible approximations by truncated SVDs [ 22 ] . However,this paper presents the results of a sequential concept decomposition based algorithm that takes 135mins training time for the full Netflix data , which is very high when looking at soft real time performance . Narang et al . [ 23 ] presents a parallel CF algorithm using concept in decomposition on 32 code SMP architecture . It achieves 64s total training time for Netflix data . Using multicore clusters , we deliver around two order of magnitude improvement in training time compared to the sequential concept decomposition technique [ 6 ] and around one of magnitude improvement compared to the parallel concept decomposition technique [ 23 ] . Narang et al . [ 11 ] presents a flat distributed co clustering algorithm where all the processors in the system participate in each iteration of the co clustering algorithm , and both OpenMP and MPI ( hybrid approach ) are used to exploit both intra node and inter node parallelism available in Blue Gene/P . Using the Netflix dataset it demonstrates the performance
( 100M ratings ) ,
1wwwibmcom/bluegene
2http://wwwgrouplensorg/data/ 100K ratings(1 5 ) 943 users , 1682 movies
926550 and scalability of the algorithm on 1024 node Blue Gene/P system : with training time of around 6s on the full Netflix dataset ( with Euclidean divergence).In this paper , we present a hierarchical approach for distributed co clustering along with load balancing optimizations leading to around average 4× improvement in performance as compared to [ 11 ] on Yahoo KDD Cup and Netflix datasets . Theoretical analysis of our hierarchical algorithm firmly establishes the performance gain of O(log(π ) ) ( where , π is the number of partitions of rows and columns of the input matrix ) as compared to the flat algorithm in [ 11 ] . Further , we present detailed performance comparison with much larger data as compared to [ 11 ] .
III . BACKGROUND AND NOTATION
In this paper , we deal with partitional co clustering where all the rows and columns are partitioned into disjoint row and column clusters respectively . We consider a general framework for addressing this problem that considerably expands the scope and applicability of the co clustering methodology . As part of this generalization , we view partitional co clustering as a lossy data compression problem [ 15 ] where , given a specified number of rows and column clusters , one attempts to retain as much information as possible about the original data matrix in terms of statistics based on the co clustering [ 24 ] . The main idea is that a reconstruction based on co clustering should result in the same set of user specified statistics as the original matrix .
Let k and l be the number of row and column clusters respectively then a k ∗ l partitional co clustering is defined as a pair of functions : ρ : 1 , . . . , m fi−→ 1 , , k ; and , γ : 1 , . . . , n fi−→ 1 , , l . Let ˆU and ˆV be random variables that take values in 1 , , k and 1 , , l such that ˆU = ρ(U ) and ˆV = γ(V ) . Let , ˆZ = [ ˆzuv ] ∈ Sm×n be an approximation of the data matrix Z such that ˆZ depends only upon a given co clustering ( ρ , γ ) and certain summary statistics derived from co clustering . Let ˆZ be a ( U,V) measurable random variable that takes values in this approximate matrix ˆZ following w , ie , p( ˆZ(U , V ) = ˆzuv ) = wuv . Then , the goodness of the underlying co clustering can be measured in terms of the expected distortion between Z and ˆZ , that is , m . n .
E[dφ(Z , ˆZ ) ] = wuvdφ(zuv , ˆzuv ) = dΦw ( Z , ˆZ )
( 1 ) u=1 v=1 where Φw : Sm×n fi−→ R is a is a separable convex function induced on the matrices such that the Bregman divergence ( dΦ( ) ) between any pair of matrices is the weighted sum of the element wise Bregman divergences corresponding to the convex function φ . From the matrix approximation viewpoint , the above quantity is simply the weighted element wise distortion between the given matrix Z and the approximation ˆZ . The co clustering problem is then to find ( ρ , γ ) such that ( 1 ) is minimized .
Now we consider two important convex functions that satisfy the Bregman divergence criteria and are hence studied in this paper .
• I Divergence : Given z ∈ R+ , let φ(z ) = zlogz − z . For z1 , z2 ∈ R , dφ(z1 , z2 ) = z1log(z1/z2 ) − ( z1 − z2 ) .
• Squared Euclidean distance : Given z ∈ R , φ(z ) = z2 . For z1 , z2 ∈ R , dφ(z1 , z2 ) = ( z1 − z2)2 . let
Given a co clustering ( ρ , γ ) , Banerjee et al . [ 15 ] discuss six co clustering bases where each co clustering basis preserves certain summary statistics on the original matrix . It also proves that the possible co clustering bases ( C1 . . . C6 ) form a hierarchical order in the number of cluster summary statistics they preserve . The co clustering basis C6 preserves all the summaries preserved by the other co clustering bases and hence is considered the most general among the bases . In this paper we discuss the partitioning co cluster algorithms for the basis C6 . For co clustering basis C6 and Euclideandivergence objective , the matrix approximation is given by : ˆAij = ACOC = . . gj ) , where , ARC .)=h Aij .)=h Wij .)=h Ai . . . . j .)=h Wi . the basis C6 is as shown in Algorithm 1 where the approximation matrix ˆA for various co clustering bases can be obtained from [ 15 ] . For Euclidean divergence , Step 2b . and 2c . of Algorithm 1 use dφ(Aij , ˆAij ) = ( Aij − ˆAij)2 . For I divergence , Step 2b . and 2c . of Algorithm 1 use dφ(Aij , ˆAij ) = Aij
. )=g Ai . . |ρ(i j i .)=g Wi . .|ρ(i j i SCOC ACOC gh = W COC The
. )=g .)=g algorithm for ih = SCC =
W CC ih . |ρ(i i . |ρ(i i
∗ log( ˆAij/Aij ) − Aij + ˆAij gj = . and sequential update
. |γ(j j .|γ(j j . |γ(j j .|γ(j gh + ( ACC ih
. . . .
− ARC
SRC gj W RC gj
; ACC
. . ih
=
. j gh gh j
Algorithm 1 Sequential Static Training via Co Clustering Input : Ratings Matrix A , Non zeros matrix W , No . of row clusters l , No . of column clusters k .
Output : Locally optimal co clustering ( ρ,γ ) and averages
ACOC,ARC,ACC,AR and AC .
Method : 1 . Randomly initialize ( ρ,γ ) while RMSE value is converging do gj ,ACC ih ,AR and AC
2a . Compute averages ACOC,ARC where 1 ≤ g ≤ k and 1 ≤ h ≤ l . 2b . Update row cluster assignments ρ(i ) = argmin 1≤g≤k fi n j=1 Wijdφ(Aij , ˆAij ) , 1 ≤ i ≤ m
2c . Update column cluster assignments γ(i ) = argmin 1≤h≤l i=1 Wijdφ(Aij , ˆAij ) , 1 ≤ j ≤ n m fi end
A . Distributed Flat Coclustering Algorithm
In the above sequential algorithm ( Algorithm 1 ) , we notice two important steps a ) Calculating the matrix averages , and , b ) updating the row and column cluster assignments . Further , given the matrix averages , row and column cluster updates can be done independently , and row updates themselves can be done in parallel.The flat distributed algorithm [ 11 ] leverages this inherent data parallelism . As one can see , this algorithm needs three MPI collectives calls : 1 ) To communicate Row/Column memberships , 2 ) To communicate Row/Column cluster averages and 3 ) To communicate cocluster averages . Since , the input ratings matrix is uniformly partitioned across all available processors , the algorithm can support very large matrices and hence has strong memory scalability . However , as the number of processor increases the collectives across all the
927551 processors can become a bottleneck to the strong scalability for performance . In this paper we consider a hierarchical algorithm which reduces both communication and computation cost on multi core cluster architecture while maintaining similar accuracy .
IV . HIERARCHICAL COCLUSTERING ALGORITHM
In this section , we present the detailed algorithmic design of our novel hierarchical co clustering algorithm . The original input ( users*items ) ratings matrix is divided into certain number of row and column partitions . Each partition is assigned to a set of nodes in the cluster architecture . The hierarchical algorithm runs from bottom to top along a computation tree ( Fig 2 ) as follows . First , flat parallel co clustering is run in each partition independently . The number of row and column clusters chosen is smaller compared to that specified in the input . Then , for each partition , the row and column clusters generated are merged with the adjacent partition . This gives the next level row and column clusters . At this higher level , flat parallel co clustering is then run independently in each partition . Then again , the resulting row and column clusters at this level are merged to generate the next higher level row and column clusters . This forms a computation tree ( Fig 2 ) of execution . The alternate flat co clustering and row/column cluster merge continue up the computation tree until the full matrix is obtained as a single partition ( at the highest level in the tree ) and finally flat parallel co clustering is run here with the number of row and column clusters as specified in the input .
This hierarchical design helps in improving the overall time of the co clustering algorithm without loss in accuracy of CF . At the lower levels of the computation tree , faster co clustering iterations with smaller number of row and column clusters take place . This reduces the computation time . Moreover , MPI collectives like MPI Allreduce and MPI Allgather are usually costly in nature when used over large number of nodes in the system ( as in the flat algorithm [ 11] ) . However , in the hierarchical algorithm , these collectives occur in smaller subsets of nodes ( smaller communication topologies ) and hence the communication cost is reduced . Thus , the hierarchical design results in lower computation as well as communication time . Further , row and column clusters at one level , after merge , result in good quality seed clusters for co clustering at the next level . So , in the same number of iterations as a pure flat co clustering algorithm [ 11 ] , one can converge to similar high quality co clustering for the hierarchical algorithm . Hence , the hierarchical algorithm provides a better trade off point for speed vs accuracy as compared to the flat algorithm .
Fig 1 and Fig 2 illustrate the hierarchical algorithm in detail . Here , the input ratings matrix A is partitioned into 4 ∗ 4 = 16 partitions ( πr = 4 , πc = 4 ) . At level 0 ( leaf level of the computation tree , Fig 2 ) , first each partition , Π0 i,j ( 1 ≤ i ≤ 4 , 1 ≤ j ≤ 4 ) , performs a certain number of flat co clustering iterations on its corresponding sub matrix , i,j , independently and in parallel using the G0 processors A0 allocated to it . Each partition generates , k/4 row clusters and l/4 column clusters . Then , pairs of adjacent partitions ( for instance partition Π0 2,1 ) , merge their row and column clusters respectively , to generate k/2 row clusters and l/4 column clusters at level 1 . Since , the underlying submatrices of the adjacent partitions are concatenated along the rows , this step is called as row folding step ( See Fig 1 and Step 3 in Algorithm 2 ) . Then , at level 1 , each partition , Π1 i,j
1,1 and partition Π0
( with 1 ≤ i ≤ 2 and 1 ≤ j ≤ 4 ) , independently runs flat co clustering iterations on the sub matrix , A0 i,j , with k/2 row clusters and l/4 column clusters . The updated row and column clusters of adjacent partitions are merged to generate k row clusters and l/4 column clusters at the next level 2 ( another row fold step ) . These two row fold steps for the corresponding sub matrices are illustrated in Fig 1 . These are followed by two column fold steps . At level 2 , each partition , Π2 i,j ( i == 1 , 1 ≤ j ≤ 4 ) independently runs flat co clustering iterations on i,j , to update the k row clusters and l/4 the sub matrix , A2 column clusters . Then , each pair of adjacent partitions merges the row and column clusters to generate k new row clusters and l/2 column clusters . These , form the seed row and column clusters for level 3 . After , the flat co clustering iterations at level 3 , the k row clusters and l/2 column clusters of the two partitions at this level , are merged to generate k row clusters and l column clusters at level 4 . These clusters are then refined by final set of flat co clustering iterations . This gives us the full matrix with k row and l column clusters . For exact details refer Algorithm 2 .
While merging row and column clusters of one level to generate row and column clusters of the next level , one needs ensure low merge compute and communication time while at the same time generating good quality starting seed clusters for the next level . In order to achieve this , we use maximum weight bi partite matching across two sets of clusters . During row folds , the number of row clusters simply doubles hence , no merge is required . While , the number of column clusters remains the same at the next level . Hence , using the number of overlapping columns as the weight of the edge connecting two column clusters , we perform maximum weight bi partite matching algorithm to quickly merge the column clusters . This merging operation requires an additional MPI Allreduce operation to communicate the cluster memberships from one partition to the other .
The row and column merging ( folding ) usually happens alternatively to reduce the bias towards row or column clusters . However , to minimize data transfer volume for mitigating load imbalance , one might choose a particular sequence of row or column folds/merge . We leave a detailed study of this effect to future work .
Ϭ Ϭ
Ϭ Ϭ
ϭ͕ϭ ϭ͕ϭ
Ϯ͕ϭ Ϯ͕ϭ
Ϭ Ϭ
Ϭ Ϭ
ϭ͕Ϯ Ϭ ϭ͕Ϯ Ϭ Ϭ Ϭ
ϭ͕ϯ Ϭ ϭ͕ϯ Ϭ Ϭ Ϭ
Ϯ͕ϯ Ϯ͕ϯ
Ϯ͕Ϯ Ϯ͕Ϯ
Ϭ Ϭ
Ϭ Ϭ
Ϭ Ϭ ϯ͕ϭ ϯ͕ϭ ϰ͕ϭ Ϭ ϰ͕ϭ Ϭ
ϯ͕Ϯ ϯ͕Ϯ
ϰ͕Ϯ ϰ͕Ϯ
Ϭ Ϭ
Ϭ Ϭ
Ϭ Ϭ ϯ͕ϯ ϯ͕ϯ ϰ͕ϯ Ϭ ϰ͕ϯ Ϭ
ϭ͕ϰ ϭ͕ϰ
Ϯ͕ϰ Ϯ͕ϰ
ϯ͕ϰ ϯ͕ϰ
ϰ͕ϰ ϰ͕ϰ
&ůĂƚ ŽͲĐůƵƐƚĞƌŝŶŐ &ůĂƚ ŽͲĐůƵƐƚĞƌŝŶŐ ŝŶ ĞĂĐŚ ƉĂƌƚŝƚŝŽŶ ŝŶ ĞĂĐŚ ƉĂƌƚŝƚŝŽŶ
ĂĐŚ ƉĂƌƚŝƚŝŽŶ ŚĂƐ Dͬϰ ĂĐŚ ƉĂƌƚŝƚŝŽŶ ŚĂƐ Dͬϰ ZŽǁƐ͕ Eͬϰ ŽůƵŵŶƐ Θ Wͬϭϲ ZŽǁƐ͕ Eͬϰ ŽůƵŵŶƐ Θ Wͬϭϲ EŽĚĞƐ EŽĚĞƐ
DĞƌŐŝŶŐ ůŽŶŐ ZŽǁƐ DĞƌŐŝŶŐ ůŽŶŐ ZŽǁƐ
DĞƌŐŝŶŐ ůŽŶŐ ZŽǁƐ DĞƌŐŝŶŐ ůŽŶŐ ZŽǁƐ
ĂĐŚ ĐĞůů ŚĂƐ DͬϮ ZŽǁƐ͕ ĂĐŚ ĐĞůů ŚĂƐ DͬϮ ZŽǁƐ͕ Eͬϰ ŽůƵŵŶƐ Θ Wͬϴ EŽĚĞƐ Eͬϰ ŽůƵŵŶƐ Θ Wͬϴ EŽĚĞƐ
&ůĂƚ ŽͲĐůƵƐƚĞƌŝŶŐ &ůĂƚ ŽͲĐůƵƐƚĞƌŝŶŐ ŝŶ ĞĂĐŚ ƉĂƌƚŝƚŝŽŶ ŝŶ ĞĂĐŚ ƉĂƌƚŝƚŝŽŶ
ϭ ϭ
ϭ͕ϭ ϭ͕ϭ
ϭ ϭ
ϭ͕Ϯ ϭ͕Ϯ
ϭ ϭ
ϭ͕ϯ ϭ ϭ͕ϯ ϭ
ϭ͕ϰ ϭ͕ϰ
ĂĐŚ ĐĞůů ŚĂƐ D ZŽǁƐ͕ Eͬϰ ĂĐŚ ĐĞůů ŚĂƐ D ZŽǁƐ͕ Eͬϰ ŽůƵŵŶƐ Θ Wͬϰ EŽĚĞƐ ŽůƵŵŶƐ Θ Wͬϰ EŽĚĞƐ
ϭ ϭ
DĞƌŐŝŶŐ ůŽŶŐ ŽůƵŵŶƐ DĞƌŐŝŶŐ ůŽŶŐ ŽůƵŵŶƐ
Ϯ͕ϭ ϭ Ϯ͕ϭ ϭ
Ϯ͕Ϯ ϭ Ϯ͕Ϯ ϭ
Ϯ͕ϯ ϭ Ϯ͕ϯ ϭ
Ϯ͕ϰ Ϯ͕ϰ
Ϯ Ϯ
ϭ͕ϭ ϭ͕ϭ
Ϯ Ϯ
ϭ͕Ϯ Ϯ ϭ͕Ϯ Ϯ
ϭ͕ϯ Ϯ ϭ͕ϯ Ϯ
ϭ͕ϰ ϭ͕ϰ
&ůĂƚ ŽͲĐůƵƐƚĞƌŝŶŐ &ůĂƚ ŽͲĐůƵƐƚĞƌŝŶŐ ŝŶ ĞĂĐŚ ƉĂƌƚŝƚŝŽŶ ŝŶ ĞĂĐŚ ƉĂƌƚŝƚŝŽŶ
ĂĐŚ ĐĞůů ŚĂƐ D ZŽǁƐ͕ EͬϮ ĂĐŚ ĐĞůů ŚĂƐ D ZŽǁƐ͕ EͬϮ ŽůƵŵŶƐ Θ WͬϰϮEŽĚĞƐ ŽůƵŵŶƐ Θ WͬϰϮEŽĚĞƐ
ϰ ϰ
ϭ͕ϭ ϭ͕ϭ
&ůĂƚ Ž ĐůƵƐƚĞƌŝŶŐ &ůĂƚ Ž ĐůƵƐƚĞƌŝŶŐ ŝŶ ĞĂĐŚ ƉĂƌƚŝƚŝŽŶ ŝŶ ĞĂĐŚ ƉĂƌƚŝƚŝŽŶ
ϯ ϯ
ϭ͕ϭ ϭ͕ϭ
ϯ ϯ
ϭ͕Ϯ ϭ͕Ϯ
ĂĐŚ ĐĞůů ŚĂƐ D ZŽǁƐ͕ E ĂĐŚ ĐĞůů ŚĂƐ D ZŽǁƐ͕ E ŽůƵŵŶƐ Θ W EŽĚĞƐ ŽůƵŵŶƐ Θ W EŽĚĞƐ
DĞƌŐŝŶŐ ůŽŶŐ ŽůƵŵŶƐ DĞƌŐŝŶŐ ůŽŶŐ ŽůƵŵŶƐ
Fig 1 . Hierarchical Co clustering : Matrix Row/Column Folding
928552
Algorithm 2 Distributed Hierarchical Co Clustering Input : Ratings Matrix A , Non zeros matrix W , No . of row clusters l , No . of column clusters k . Output : Locally optimal co clustering ( ρ,γ ) and averages ACOC,ARC,ACC,AR and AC . Method : Let A be divided into πr row partitions and πc column partitions . Then the hierarchical algorithm proceeds with log(πr ) row folds first and then with log(πc ) column folds . Initialize x = 0 and y = 0 . while ( x++ ) < ( log(πr ) ) do i,j reads only the m ˙2x
πr
× n πc submatrix Ax i,j of A where 0 ≤ i < πr
2x and i,j iteratively calculates a ( k.2x/πr , l/πc ) locally optimum coclustering ( ρx i,j,γx i,j ) for the submatrix
1 . In the current iteration , each partition Πx 0 ≤ j < πc . 2 . Each partition Πx Ax i,j 3 . Fold along rows : Partition Πx 1a . ρx 1b . γx
2i,j and ρx 2i,j and γx
2i,j merges with partition Πx
2i+1,j in the following manner to form Πx+1 i,j
.
2i+1,j together form k.2x+1/πr new row clusters ρx+1 2i+1,j merge using maximum bi partite matching to form l/πc new column clusters γx+1 i,j i,j end while ( y++ ) < ( log(πc ) ) do i,j iteratively calculates a ( k , l.2y/πc ) locally optimum coclustering ( ρy i,j reads only the m × n.2y
πc submatrix Ay i,j of A where i = 0 and 0 ≤ j < πc 2y . i,j ) for the submatrix Ay i,j,γy i,j
1 . In the current iteration , each partition Πy 2 . Each partition Πy 3 . Fold along columns : Partition Πy 1a . ρy 1b . γy i,2j and ρy i,2j and γy i,2j merges with partition Πy i,2j+1 in the following manner to form Πy+1 i,j . i,2j+1 merge using maximum weight bi partite matching form k new row clusters ρy+1 i,2j+1 together form l.2y+1/πc new column clusters γy+1 i,j i,j end
Fig 2 . Hierarchical Co clustering Computation Tree
TABLE I NOTATION
Symbol
P0 c
( m , n ) s
( k , l ) ( m/k ) n/l B0 S0
Definition Total number of nodes for computation Number of threads ( cores ) per node Number of rows and columns in the input matrix Sparsity factor of the matrix Number of row and column clusters Average number of rows per row cluster Average number of columns per column cluster Interconnect Bandwidth for AllReduce/Allgather Setup cost for AllReduce/Allgather
V . TIME COMPLEXITY ANALYSIS
In this section , we establish theoretically , the performance and scalability advantage of our optimized distributed hierarchical algorithm . We look at performance of the hierarchical algorithm as compared to the flat algorithm . Refer notation given in Table V .
The time complexity analysis of the flat distributed co
929553 clustering algorithm using MPI + OpenMP ( hybrid ) is given in detail in [ 11 ] . This approach is similar to the MPI only flat algorithm , but additionally exploits communication and compute overlap using multi core cluster architectures . Thus , the overall time complexity for the flat hybrid distributed coclustering algorithm , per iteration , is given by :
Th(m , n,P0 , k , l ) = mn/P0 ∗ c + 2 ∗ ( mn/B0 ) ∗ log(P0 )
+ S0 + 3 ∗ mns ∗ ( k + l)/(P0 ∗ c ) )
( 2 )
A . Analysis of Hierarchical Algorithm
For the parallel hierarchical co clustering algorithm , we consider 2 way merge at each level , ie a binary tree ( for sake of simplicity ) with Z levels of computation . In case of the binary tree , the base level , l0 , has 2Z partitions each of size G0 nodes ( processors ) such that G0 = P0/(mn ) . At each level , lz , z ∈ [ 0Z − 1 ] , the kfi row clusters and lfi column clusters from two previous level partitions are merged to form a new initial set of kfifi row clusters and lfifi column clusters for the next level partition . In the hierarchical computation , first the row to row cluster assignment and the column to column cluster assignment iterations are performed at a level , lz . The time required for these iterations depends upon the size of the sub matrix handled by each partition at that level , the number of clusters kfi and lfi , as well as the number of nodes in the partition at that level . The total number of levels in the binary tree of hierarchical computations is given by :
Z = log(πr ) + log(πc )
( 3 )
We consider separately , the cost for iterations at each level and the merge overhead to go from one level to the next . For sake of simplicity , we assume that all row folds ( with levels referred to as x , x ∈ [ 0 log(πr ) − 1 ] ) happen before the col folds ( with levels referred to as y , y ∈ [ 0 log(πc ) − 1] ) .
The cost of iterations during the row fold at each level ,
T(f lat)(m.2x/πr , n/πc , G0.2x , kx , l/πc ) ,
( 4 ) where , kx = k.2x/πr ( flat hybrid equation ) . Similarly , the cost of iterations during col fold at each level , referred to here as ,
T(f lat)(m , n.2y/πc , P0.2y/πc , k , ly ) ,
( 5 ) where P0 = G0 ∗ πr ∗ πcandly = l2y/πc For merge compute and communication cost , let us consider row fold based merge between two partitions of level , x , to create a new partition at level , x + 1 , and its initial row and column clusters . Here , communication takes place between the nodes of the two partitions at level x to share the row cluster and column cluster mapping . The time for this is given by : O(S0 + 2(kx + lπc ) ∗ log(2xG0)/B0 ) Then the nodes perform maximum weight bipartite matching between row clusters of the two partitions and also between column clusters of the two partitions . Since , this matching effort is equally distributed across the nodes ( and cores within the nodes ) of the two partitions , this compute time is given by : O((kx +l/πc)/(G0 ∗c) ) . Once , the merge happens , the assignment of rows to the row clusters and columns to the column clusters is done by each node . The time for this is given by : O((1/2G0.c ) ∗ ( (m.2x/πr ) + n/πc) ) . Let α = log(πr ) and β = log(πc ) . The merge time for row based merge between two partitions at level,x , 0 ≤ x ≤ ( log(πr ) − 1 ) , is given by ( assuming compute time dominates ) :
T(r merge)(m.2x/πr,n/πc , G0.2x ) =
( kx + l/πc )
( G0 ∗ c )
+ (
1
2G0.c
∗ ( m.2x
πr
+ n πc
) )
( 6 )
Similarly , the merge time for column based merge between two partitions at level,α + y , 0 ≤ y ≤ ( β − 1 ) , is given by ( assuming compute time dominates ) :
T(c merge)(m,n.2y/πc , G0πr2y ) =
( G0 ∗ πr2yc ) ( (1/2G0.πr , 2y.c ) ∗ ( m + ( n.2y)/πc ) )
+
( 7 )
( k + ly )
The total time in the hierarchical computation is given by the time for all row folds Trow f old plus the time for all column folds Tcol f old .
T(hier ) = Trow f old + Tcol f old log(πr −1 ) .
Trow f old = O(
(
Tcol f old = O( x=0 log(πc−1 ) . x=0
( k/πr.2x + l/πc)m/πrn/πc2xs
P0.2x πr .πc
) )
( k + l.2x πc
) . mns
P0
)
The total time over all row folds is given by :
− 1 )
+ k.(πr πr
Trow f old = O((
πc Similarly using Tcol f old can be written as : − 1 ) l.(πc
Tcol f old = O((k . log(πc ) + l . log(πr )
πc
( 8 )
) . mns
P0
)
( 9 )
) . mns
P0
)
( 10 )
930554
Substituting the expression for Trow f old , Tcol f old from equation ( 9 ) , and simplifying equation ( 10 ) , and assuming the communication cost is low , we get :
Thier = O(((k . log(πc ) + l.(πc
− 1 )
)
+ (
− 1 ) k.(πr πr
+
πc l . log(πr )
πc
( 11 )
) ) . mns
P0
)
Now combining results from ( 11 ) & ( 3 ) and making k=l=C , πr=πc=π we get Thier as : ( 2.(1 − C
π + 1) ) . mns
P0
)
( 12 )
Thier = O(
π ) + C.( log(π ) 2 . log(π )
Hence by doing similar replacement in Tf lat as above we get :
Tf lat = O(
Thier Tf lat
= O(
)
Cmns
P0 1 log(π )
)
( 13 )
Equation ( 13 ) demonstrates that the distributed hierarchical algorithm performs better than the distributed flat algorithm . In real experiments , the compute and communication merge overheads lead to lesser gain . One can use the above performance model ( equation ( 11 ) ) to compute the optimal values of πr , πc and Z . We skip this analysis for brevity . VI . LOAD BALANCING OPTIMIZATION
Since the input matrix is highly sparse , one needs to perform load balancing to achieve the maximum parallel efficiency on large scale parallel systems . We model the load balancing problem as an Integer Linear Program ( ILP ) for both flat and hierarchical distributed algorithms . This can be used for both static load balancing as well as dynamic load balancing in case of online co clustering / collaborative filtering algorithms . In the distributed flat algorithm , we need to ensure that each processor has equal compute load based on the rows and columns assigned to that processor . Formally , this problem is related to the k partition problem that is known to be NP hard . However , approximation algorithms can be used obtain a good load balanced data distribution for the flat distributed CF algorithm . We employed greedy row and column movement heuristic to ensure good balancing for the flat algorithm . The flat load balancing algorithm works in iterations . In each iteration the total row and column load on each processor , CLp is computed and using all reduce this information is obtained at each processor . Then , a matching is computed between processors with heavy loads and processors with light load . After this , the processor with high load sends a certain number of heavy rows and columns to its matched processor with low load . The selection of rows and columns to send is made to ensure that these two matched processors end up with similar load after their communication . These iterations are repeated till the overall load imbalance in the system is below a certain threshold .
In the hierarchical algorithm one needs to ensure load balance across the partitions at each level of the computation hierarchy . Performing this forward looking load balancing for all levels in the beginning ( at the leaf level ) itself will ensure high parallel efficiency at all levels of execution . This can be viewed as a multi level k partitioning problem . At each level , the problem is similar ( with a small difference ) to the flat case , ie k partition problem ) . This multi level k partition problem is NP hard since it a generalization of the k partition problem . Further , our problem has additional constraints which makes it computationally challenging . We use similar heuristic as for the flat algorithm at levels close to the leaf of the tree since it at these levels that the load balance leads to severe impact on performance .
A . Online Hierarchical Co clustering Algorithm
,ARC gh
Algorithm 3 presents the hierarchical online distributed co clustering algorithm . In the online algorithm , the row and column clusters at various levels are updated on the hierarchical computation tree , as a collection of row/column updates in the input matrix come along . The clusters at the top most level and the corresponding averages ACOC ih at that level are hence maintained which can be used for the purpose of prediction later . The key challenge in the online hierarchical algorithm , is to calculate the changed co clustering for the partition Πi,j y+1 at level y + 1 , using the updates at the level y in an efficient manner along the hierarchical computation tree . In order to acheive this , the changes in clusters at level y are propagated to level y + 1 while utilizing the history of the previous clustering at level y + 1 . This reduces the number of cluster assignment iterations to reach an optimal co clustering at level y + 1 . gj ,ACC
The propagation of changes from level y to level y + 1 is done in the following manner ( refer Algorithm 3 . Let us assume we have the optimal local clustering for the partitions at level y . We need to propagate the information in this changed assignment of row clusters ( say Ky ) to the old row clusters at level y + 1 , Ky+1 . Now , we take each row r at level y that is affected by the updates to the ratings in that partition and hence changed to a cluster Ry in Ky . Now , the assignment of r in Ky+1 is determined as follows . Find that cluster Ry+1 ∈ Ky+1 such that Ry has a maximal match with Ry+1 ( in terms of number of rows assigned to them ) , more than any other row cluster in Ky+1 . Now at level y + 1 , the row r is assigned to this row cluster Ry+1 initially . In this fashion , one round of initial row assignment updates is done for all the changed rows in the partition Πi,j y+1 by propagating the changes from the previous level while using the history at this level . Then , a few rounds of flat row cluster assignment update iterations are run to reduce the error in the divergence function chosen . For reducing the run time further , these iterations at lower levels of the hierarchical tree ( closer to the leaves ) can be skipped , since the number of changes within a partition at a lower level maybe so less that just a reassignment of clusters by propagating the cluster updates is enough to maintain the clusters and ensure good accuracy . In this case , we will significantly reduce the algorithm execution time while not loosing much on the accuracy of cluster assignments at lower levels . Further , successive chunks ( collections ) of updates can proceed in parallel by carefully , allocating and multiplexing the cores in each node to process a chunk ( collection ) of updates . Thus , at any point of time , multiple updates can proceed in parallel up ( from leaf to the root ) along the hierarchical computation tree in a pipelined fashion . This pipelined parallelism leads to soft real time online CF performance by enabling higher utilization of the underlying compute nodes in the system .
931555
VII . RESULTS & ANALYSIS
The hybrid flat and hierarchical distributed algorithms were both implemented using MPI and OpenMP . The Netflix Prize dataset ( 100M training ratings and 1.5M validation ratings on a scale of 15 , over 480K users and 17K movies ) , and , Yahoo KDD Cup ( 252M training ratings and 4M validation ratings on a scale of 1100 , over 1M users and 624K songs ) datasets were used to evaluate and compare the performance and scalability of these distributed algorithms . Platform : The experiments were performed on the Blue gene/P ( MPP ) architecture . Each node in Blue Gene/P is a quad core chip with frequency of 850 MHz having 2 GB of DRAM , 32 KB of L1 instruction and data caches per core , 2KB prefetch buffer ( L2 cache ) and 8MB of L3 cache . Blue Gene/P has 3D torus interconnect with 3.4 Gbps bandwidth in each of the six directions per node along with separate collective and global barrier networks . MPI was used across the nodes for communication while within each node OpenMP was used to parallelize the computation and communication amongst the four cores .
For all the experiments , we obtained RMSE in the range 0.87 ± 0.02 on the Netflix validation data and RMSE in the range 26 ± 4 on the Yahoo KDD Cup data . The sequential implementation 1 and the flat distributed algorithm [ 11 ] obtains similar accuracy for both datasets . Below , k refers to the number of row clusters ( k = 16 for Netflix , k = 20 for Yahoo KDD Cup ) generated while l refers to the number of column clusters ( l = 16 for Netflix , l = 20 for Yahoo KDD Cup ) generated . For all the experiments we used the C6 constraints ( refer section III ) .
A . Scalability Analysis
We present the strong , weak and data scalability analysis including the training phase and the prediction phase for Idivergence with Netflix dataset and for Euclidean divergence with Yahoo KDD Cup dataset .
1 ) Strong Scalability : Fig 3(a ) compares the strong scalability curves of the hierarchical algorithm and the flat algorithm . The hierarchical algorithm with load balancing ( Hier lb ) has better performance of around 2× ( 77s vs 144s at 64 nodes ) to 4× ( 9.38s vs 38.2s at 4096 nodes ) over the flat algorithm . This gap increases with increasing number of nodes as the hierarchical algorithm has better load balance across the nodes along with lower communication time , while achieving the same accuracy as flat ( 0.87 ± 0.02 RMSE ) . This is a very desirable property for massive scale analytics and comes from the novel hierarchical design of our algorithm . This also demonstrates soft real time training ( 9.38s ) performance for the full Netflix dataset even with the computationally expensive I divergence objective . In the hierarchical algorithm , as the number of nodes increases by 64× , from 64 to 4096 , the time decreases by 8.2× ( from 77s to 938s ) The prediction time was 0.7s for 1.4M ratings . This gives an average prediction time of 0.5μs per rating using 4K nodes . Fig 4(a ) illustrates the performance gain of the hierarchical algorithm over the flat algorithm for Euclidean divergence with the Yahoo KDD Cup dataset . The hierarchical algorithm consistently performs better than the flat by around 4× ( 61s vs 267s at 64 nodes and 11.85s vs 51s at 4096 nodes ) . This also demonstrates soft realtime training performance ( 13.28s ) for the full Yahoo KDD Cup data . Because of the fundamental advantage of lesser overall compute requirement and lesser load imbalance and communication cost ( while giving the same accuracy 26 ± 4
Algorithm 3 Hierarchical Online Distributed Coclustering update algorithm Input : Original Matrix A , Updated Ratings Matrix U , Previous Hierarchical Co clusters ( ρi,j
( 1 ≤ y ≤ Y0 , 1 ≤ i ≤ m1 , 1 ≤ j ≤ n1 ) and averages ACOC gh
,ARC gj ,ACC ,ARC ih at level Y0 y ,γi,j y ) for each partition Πi,j y y ,γi,j y ) and averages ACOC
Output : Updated optimal co clustering ( ρi,j Method : 1 . Before starting the iterations , update the m × n matrix A with changes from U . 2 . Following the hierarchical structure , at ( 1 ≤ i ≤ m1 , 1 ≤ j ≤ n1 ) , as shown in the figure gets m level y , Partition Πi,j i ) and n y columns ( i.e , a m × n n1 submatrix AC Each node p in this partition gets m level y , to each partition Πi,j coclustered into k while ( y + + ) ≤ Y0 do
G0∗n1 columns where G0 is the number of nodes in the partition . At any n1 submatrix Ai,j is visible/read by the nodes in the partition . This matrix is m1 row clusters and l n1 column clusters . y 2 ( and n1 = 2 m1 rows ( i.e , a m j ) where m1 = 2
× n submatrix AR m1∗G0 rows and ih at level Y0 . y only the m gj ,ACC
) y 2 .
× n m1 m1 gh n1 n y y y y ,γi,j or ρ2i+1,j y merges with partition Π2i+1,j in the following manner to form Πi,j y ) for the updated submatrix Ai,j at each partition Πi,j y at level y does a maximal match of that cluster with one of the y+1 at level y + 1 and joins it , eventually leading to k1 changed Row clusters ρi,j
3 . Update the locally optimum coclustering ( ρi,j 4 . If y is odd , Fold along rows : Partition Π2i,j 1a . Each row r that updated its cluster in ρ2i,j old clusters ρi,j 1b . Each column c that updates its cluster in γ2i,j the old clusters γi,j 1c . Some flat row and column update iterations are run if y < H before proceeding to the next level and making m1 = m1 2 else If y is even , Fold along Columns : Partition Πi,2j 1a . Each row r that updated its cluster in ρi,2j old clusters ρi,j 1b . Each column c that updates its cluster in γi,2j the old clusters γi,j 1c . Some flat row and column update iterations are run y < H before proceeding to the next level and making n1 = 1 2 y+1 at level y + 1 and joins it , eventually leading to l1 changed Column clusters γi,j y+1 at level y + 1 and joins it , eventually leading to l1 changed Column clusters γi,j y+1 at level y + 1 and joins it , eventually leading to k1 changed Row clusters ρi,j at level y does a maximal match of that cluster with one of the at level y does a maximal match of that cluster with one of at level y does a maximal match of that cluster with one of in the following manner to form Πi,j y merges with partition Πi,2j+1 or γi,2j+1 or γ2i+1,j or ρi,2j+1 y+1 . y+1 . y+1 . y+1 . y+1 . y+1 . y y y y y y y end
Strong Scalability ( Netflix , I div ) k=16 , l=16
Weak Scalability ( Netflix , I div ) k=16 , l=16
Data Scalability ( Netflix , I Divergence ) : k=16,l=16 i
) s ( e m T g n n i a r T i
160
140
120
100
80
60
40
20
0
Flat hybrid Hier lb i
) s ( e m T g n n i a r T i
120
100
80
60
40
20
0
64
128
256
512
1024 2048
4096
Number of Nodes
( a )
64
128
256
512
1024
2048
4096
Number of Nodes
( b )
Flat hybrid Hier lb
) s ( e m T i
45 40 35 30 25 20 15 10 5 0
39.4
30.3
20.63
Sscal mpi Sscal hier lb
10.47
3.65
5.66
2.34
9.38
6.75
4.52
12.50 % 25 %
50 %
75 %
100 %
Data Size : % of full Netflix
( c )
Fig 3 . Netflix ( I div/C6 ) :
( a ) Strong Scalability .
( b ) Weak Scalability .
( c ) Data Scalability
RMSE ) as compared to the flat algorithm , the hierarchical algorithm achieves better performance and hence is ideally suited for massive scale analytics . The prediction time was 3.2s for 4M ratings . This gives an average prediction time of 0.8μs per rating . The parallel efficiency here is lower than the Netflix data since the Yahoo data has much higher sparsity and hence load imbalance , but it can be further improved by fine tuning the load balance further as well as optimizing the merge phase in the hierarchical algorithm .
2 ) Weak Scalability : Fig 3(b ) compares the weak scalability curves for hierarchical algorithm and the flat algorithm , using I divergence based co clustering with C6 constraints . As the number of nodes ( P0 ) increases from 64 to 4096 and the training data increases from 6.25 % to 400 % ( 400M ratings ) of the full Netflix dataset ( with k = 16 , l = 16 ) , the total training time for the hierarchical algorithm increases by around 8.7× ( 4.5s to 39s ) , while that for the flat algorithm increases by 11.87× ( 9s to 107s ) , thus demonstrating better weak scalability . Further , the hierarchical algorithm performs consistently better compared to the flat algorithm , around 2× ( 4.5s vs 9s ) with 64 nodes and 2.7× ( 39s vs 107s ) at 4096 nodes . Fig 4(b ) demonstrates the weak scalability of the hierarchical algorithm for Euclidean divergence with Yahoo KDD Cup dataset : with 64× increase in the data ( 16.25M to 1B ratings ) and number of nodes ( 64 to 4096 ) , the training time only increases by 3.5× ( 8.15s to 285 ) Further , the hybrid algorithm performs consistently better than the flat algorithm , 3.9× ( 8.15s vs 32s ) at 64 nodes and 2.9× ( 28.5s
932556
) s ( e m T i g n i i n a r T
Strong Scalability ( Yahoo KDD Cup , Euclidean
Div ) : k=20,l=20
Flat hybrid
Hier lb
300
250
200
150
100
50
0
64
128
256
512 1024 2048 4096
Num ber of Nodes
( a )
) s ( e m T i i g n n a r T i
Weak Scalability ( Yahoo KDD Cup , Euclidean
90
80
70
60
50
40
30
20
10
0
Div ) : k=20,l=20
Data Scalability ( Yahoo KDD Cup , Euclidean Div ) : k=20,l=20
Flat hybrid Hier lb
Flat hybrid
Hier lb
250
200
150
100
50
0
) s ( e m T i g n i i n a r T
64
128
256
512 1024 2048 4096
Num ber of Nodes
( b )
65M 130M 260M 520M 1B 2.3B 4.6B
Number of Ratings
( c )
Fig 4 . Yahoo KDD ( Euclidean/C6 ) :
( a ) Strong Scalability .
( b ) Weak Scalability .
( c ) Data Scalability vs 82.4s ) at 4096 nodes .
3 ) Data Scalability : Fig 3(c ) compares the data scalability curves of the hierarchical algorithm and the flat algorithm . As the training data increases from 13M to 900M ( using replication of Netflix dataset ) , while P0 = 4096 , the training time for the hierarchical algorithm increases by 34× ( 1.87s to 64s ) which is much lesser than that of the flat algorithm increases by 48× . This demonstrates better than linear data scalability of the hierarchical algorithm and better data scalability over the flat algorithm . Further , the hierarchical performs consistently better than the flat algorithm , 2.24× at 13M ratings ( 1.87s vs 4.2s ) and 3.2× at 900M ratings ( 64s vs 203s ) . Moreover , this gap increases with increasing input size of the data , that makes the hierarchical algorithm attractive for massive scale data . Fig 4(c ) compares the data scalability curves for the hierarchical and the flat algorithm on the Yahoo KDD Cup dataset ( with P0 = 4096 , Euclidean divergence/C6 ) . The hierarchical algorithm demonstrates better than linear data scalability ( 21× increase in time with 64× increase in data from 65M ratings to 4.6B ratings ) . It performs better than the flat algorithm by 3.15× at 65M ratings ( 3.8s vs 12s ) and 2.4× at 4.6B ratings ( 80s vs 194s ) . On 1B as well as for 2.3B ratings , the hierarchical algorithm achieves soft real time performance , 25s and 47s respectively .
B . Detailed Scalability Comparison
In this section we present detailed comparison of the gains obtained by the hierarchical algorithm and load balancing . Fig 5(a ) presents the curves for strong scalability for the flat hybrid algorithm , the hybrid flat load balanced algorithm and the hierarchical load balanced algorithm . The hybrid flat load balanced algorithm performs around 2× better than the flat hybrid algorithm and this gap increases with increasing number of nodes . This is because at P0 = 64 , the flat hybrid algorithm is able to utilize the 4 threads per node efficiently , while also being able to effectively overlap computation with communication . However , at higher values of P0 , the load imbalance problem dominates its overall throughput . Hence , its performance degrades wrt the load balanced flat algorithm by 2× at P0 = 1024 , and its speedup is only 2.9× over 16× increase in the number of nodes . The hybrid flat load balanced algorithm eliminates this problem by making sure that each node roughly processes the same number of entries . Hence , the hybrid flat load balanced algorithm , achieves 3.6× speedup over 16× increase in the number of nodes . The hierarchical algorithm further demonstrates an additional 2× performance over the flat load balanced algorithm at P0 = 1024 and an improvement in speedup to 7.2× with 16× increase in the number of nodes .
Fig 5(b ) presents the comparison curves for weak scalability . Here , the hybrid flat algorithm incurs 5.6× increase in time with 16× increase in data and number of nodes , and the flat load balanced algorithm incurs 3.95× increase in time ; while the hierarchical algorithm incurs only 2.6× increase in time . This can be attributed to better efficiency in the hierarchical algorithm as compared to the flat algorithm even with load balance . Further , the hierarchical algorithm has consistently superior performance over the hybrid flat load balanced algorithm by around 2× ( at 1024 nodes ) ; while the flat load balanced algorithm has around 2× performance over the flat hybrid algorithm owing to its better work distribution amongst the nodes . Fig 5(c ) presents the comparison curves for data scalability . The hybrid flat load balanced algorithm achieves gain to 1.8× at P0 = 64 and 2.15× at P0 = 1024 over the flat hybrid algorithm . Further , the hybrid flat load balanced algorithm improves the overall data scalability over the flat hybrid algorithm ( 6.6× increase in time overall vs 14.6× for flat hybrid ) . The hierarchical algorithm further improves the data scalability by achieving only 3.2× overall increase in time with 16× increase in data size .
C . Online Algorithm : Performance Analysis
Fig 6(a ) illustrates the performance gain of the hierarchical online algorithm over the baseline MPI online algorithm for Euclidean divergence with 4 % change in data . The hierarchical online algorithm performs better than the baseline by around 3.64× to 7× . This also demonstrates soft real time online training performance ( 2.81s ) of our algorithm . The parallel efficiency can be further improved here , by having better load balance . Further , the hierarchical algorithm involves node to node communication during the merge phase . This leads to increase in the communication time , leading to decrease in parallel efficiency . Fig 6(b ) illustrates the weak scalability of the online hierarchical algorithm for Euclidean divergence with 4 % incremental change in the Netflix dataset . Here , with 16× increase in the data ( matrix size ) and number of nodes , the training time remains pretty much the same . Further , the hierarchical online algorithm performs consistently better than the online baseline algorithm by around 4× . Fig 6(c ) compares the data scalability curves for the online hierarchical and the online baseline algorithm with P0 = 1024 , Euclidean divergence/C6 and 4 % incremental change in the Netflix dataset . The online hierarchical algorithm demonstrates
933557
) s ( e m T i g n i n i a r T
160 140 120 100 80 60 40 20 0
Strong Scalability Comparison ( Netflix , I div ) : k=16,l=16
64
128
256
512
1024
Number of Nodes
( a )
Sscal hier lb
Sscal flat ompmpi Sscal flat lb
) s ( e m T i
60
50
40
30
20
10
0
Weak Scalability Comparison ( Netflix , I div ) : k=16,l=16
Data Scalability Comparison ( Netflix , I div ) : k=16,l=16
Wscal hier lb
Wscal flat ompmpi Wscal flat lb
) s ( e m T i
70
60 50
40 30
20 10
0
Dscal flat mpi
Dscal hier lb
Dscal flat ompmpi Dscal flat lb
64
128
256
512
1024
Number of Nodes
( b )
12.50 % 25 %
50 %
75 %
100 %
Data Size : % of full Netflix
( c )
Fig 5 . Detailed Comparison(Netflix ) :
( a ) Strong Scalability .
( b ) Weak Scalability .
( c ) Data Scalability
^ƚƌŽŶŐ ^ĐĂůĂďŝůŝƚǇ ;KŶůŝŶĞ͕ ƵĐůŝĚĞĂŶ ŝǀͬ͘ ϲͿ tĞĂŬ ^ĐĂůĂďŝůŝƚǇ ;KŶůŝŶĞ͕ ƵĐůŝĚĞĂŶ ŝǀͬ͘ ϲͿ
ĂƚĂ ^ĐĂůĂďŝůŝƚǇ ;KŶůŝŶĞ͕ ƵĐůŝĚĞĂŶ ŝǀͬ͘ ϲͿ
ϳϬ
ϲϬ
ϱϬ
ϰϬ
ϯϬ
ϮϬ
ϭϬ
Ϭ
Ϳ Ɛ ;
Ğ ŵ d
ŝ
ϲϮ͘ϵϱ
ϯϴ͘Ϭϳ
Ϯϯ͘ϰϰϲ
ϴ͘ϴϯ
ϱ͘ϰϯ
ϰ͘Ϭϭ
ϯ͘ϭ
ϭϱ͘ϯϯ
ϭϬ͘Ϯϰ
Ϯ͘ϴϭ
ϲϰ
ϭϮϴ
Ϯϱϲ
ϱϭϮ
ϭϬϮϰ
EƵŵďĞƌ ŽĨ EŽĚĞƐ ( a )
ŚŝĞƌ
ŵƉŝͲĨůĂƚ
Ϳ Ɛ ;
Ğ ŵ d
ŝ
ϭϴ ϭϲ ϭϰ ϭϮ ϭϬ ϴ ϲ ϰ Ϯ Ϭ
ϭϱ͘ϳϮ
ϭϰ͘ϲϳ
ϭϱ͘ϯϳ
ϭϲ͘ϱϱ
ϯ͘ϵ
ϯ͘ϵϱ
ϯ͘ϵ
ϯ͘ϵϵ
ϭϮ͘ϱ
Ϯϱ
ϱϬ
ϭϬϬ
й ŽĨ &Ƶůů DĂƚƌŝǆ ^ŝǌĞ ( b )
,ŝĞƌĂƌŝĐŚĂů
&ůĂƚ
Ϳ Ɛ ;
Ğ ŵ d
ŝ
ϭϬ ϵ ϴ ϳ ϲ ϱ ϰ ϯ Ϯ ϭ Ϭ
ϴ͘ϵ
ϯ͘ϵ
ϰϴϬŬ
ϲ͘Ϯϭϴ
ϴ͘ϯϲϵ
ϳ͘ϰϴϲ
ϭ͘ϵϮ
ϲϬŬ
Ϯ͘ϳϳ
ϯ͘ϰ
ϭϮϬŬ ϮϰϬŬ EƵŵďĞƌ ŽĨ ZŽǁƐ ( c )
Fig 6 . Netflix Online(Euclidean/C6 ) :
( a ) Strong Scalability .
( b ) Weak Scalability .
( c ) Data Scalability
,ŝĞƌĂƌŝĐŚĂů
&ůĂƚ linear data scalability ( 2× time increase with 8× increase in data ( number of rows ) ) and performs better than the baseline algorithm by around 2.3× to 324×
VIII . CONCLUSIONS & FUTURE WORK
Real time co clustering and collaborative filtering with high prediction accuracy are computationally challenging problems . We have presented a novel hierarchical approach for both offline and online distributed co clustering and collaborative filtering along with theoretical analysis of parallel time complexity . Soft real time performance and superior scalability of our algorithm has been demonstrated experimentally using Netlfix and Yahoo KDD Cup datasets on multi core cluster architecture . Our hierarchical algorithm outperforms all known prior results for collaborative filtering while maintaining high accuracy . In future , we intend to investigate theoretical analysis of convergence for this algorithm .
REFERENCES
[ 1 ] P . Resnick and H . R . Varian , “ Recommender systems introduction to special section , ” Comm . ACM , vol . 40 , no . 3 , pp . 56–58 , 1997 .
[ 2 ] B . M . Sarwar , G . Karypis , J . A . Konstan , and J . Riedl , “ Analysis of recommendation algorithms for e commerce , ” in ACM Conference on Electronic Commerce , 2000 , pp . 158–167 .
[ 3 ] J . B . Schafer , J . A . Konstan , and J . Riedi , “ Recommender systems in e commerce , ” in ACM Conference on Electronic Commerce , 1999 , pp . 158–166 .
[ 4 ] R . G . Pensa and J F Boulicaut , “ Constrained co clustering of gene expression data , ” in SDM , 2008 , pp . 25–36 .
[ 5 ] S . Hassan and Z . Syed , “ From netflix to heart attacks : collaborative filtering in medical datasets , ” in International Health Informatics Symposium ( IHI ) , 2010 , pp . 128–134 .
[ 6 ] N . Ampazis , “ Collaborative filtering via concept decomposition on the netflix dataset , ” in ECAI , 2008 , pp . 143–175 .
[ 7 ] N . Srebro and T . Jaakkola , “ Weighted low rank approximation , ” in Twentieth International Conference on Machine Learning , 2003 , pp . 720–728 .
[ 8 ] J . S . Breese , D . Heckerman , and C . Kadie , “ Empirical analysis of predictive algorithms for collaborative filtering , ” in Fourteenth International Conference on Uncertainty in Artificial Intelligence , 1998 , pp . 43–52 .
934558
[ 9 ] T . George and S . Merugu , “ A scalable collaborative filtering framework based on co clustering , ” in Fifth International Conference on Data Mining , 2005 , pp . 625–628 .
[ 10 ] S . Daruru , N . M . Marin , M . Walker , and J . Ghosh , “ Pervasive parallelism in data mining : dataflow solution to co clustering large and sparse netflix data , ” in 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2009 , pp . 1115–1124 .
[ 11 ] A . Narang , A . Srivastava , and P . Katta , “ Distributed scalable collabora tive filtering algorithm , ” in EuroPar 2011 , France , 2011 .
[ 12 ] K . Kummamuru , A . Dhawale , and R . Krishnapuram , “ Fuzzy coclustering of documents and keywords , ” in IEEE International Conference on Fuzzy Systems , 2003 .
[ 13 ] A . de Spindler , M . C . Norrie , M . Grossniklaus , and B . Signer , “ Spatiotemporal proximity as a basis for collaborative filtering in mobile environments , ” in UMICS , 2006 .
[ 14 ] A . Banerjee , S . Basu , and S . Merugu , “ Multi way clustering on relation graphs , ” in SDM , 2007 .
[ 15 ] A . Banerjee , I . Dhillon , J . Ghosh , S . Merugu , and D . S . Modha , “ A generalized maximum entropy approach to bregman co clustering and matrix approximation , ” Journal of Machine Learning Research , vol . 8 , no . 1 , pp . 1919 – 1986 , Aug . 2007 .
[ 16 ] B . Sarwar , G . Karypis , J . Konstan , and J . Reidl , “ Application of dimensionality reduction in recommender systems : a case study , ” in WebKDD Workshop , 2000 .
[ 17 ] M . Brand , “ Fast online svd revisions for lightweight recommender systems , ” in SIAM International Conference on Data Mining , 2003 , pp . 37–48 .
[ 18 ] C . N . Ziegler , S . M . McNee , J . A . Konstan , and G . Lausen , “ Improving recommendation lists through topic diversification , ” in Fourteenth International World Wide Web Conference , 2005 .
[ 19 ] K W Hsu , A . Banerjee , and J . Srivastava , “ I/o scalable bregman co clustering , ” in Proceedings of the 12th Pacific Asia conference on Advances in knowledge discovery and data mining , 2008 .
[ 20 ] B . Kwon and H . Cho , “ Scalable co clustering algorithms , ” Algorithms and Architectures for Parallel Processing , Lecture Notes in Computer Science , vol . 6081 , pp . 32–43 , 2010 .
[ 21 ] I . S . Dhillon and D . S . Modha , “ Concept decompositions for large sparse text data using clustering , ” in Machine Learning , 1999 , pp . 143–175 .
[ 22 ] G . H . Golub and C . F . V . Loan , Matrix computations . Baltimore , MD ,
USA : The Johns Hopkins University Press , 1996 .
[ 23 ] A . Narang , R . Gupta , V . Garg , and A . Joshi , “ Highly scalable parallel collaborative filtering algorithm , ” in IEEE International Conference on High Performance Computing , Goa , India , 2010 .
[ 24 ] I . Dhillon , S . Mallela , and D . Modha , “ Information theoretic cothe 9th International Conference on clustering , ” in Proceedings of Knowledge Discovery and Data Mining , 2003 , pp . 89–98 .
