Unsupervised Multi Class Regularized
Least Squares Classification
Tapio Pahikkala , Antti Airola
University of Turku
Turku Centre for Computer Science
Email : {tapio.pahikkala , anttiairola}@utufi
Turku , Finland
Fabian Gieseke , Oliver Kramer
Carl von Ossietzky Universit¨at Oldenburg
Computer Science Department
Email : {f.gieseke , oliverkramer}@uni oldenburgde
Oldenburg , Germany
Abstract—Regularized least squares classification is one of the most promising alternatives to standard support vector machines , with the desirable property of closed form solutions that can be obtained analytically , and efficiently . While the supervised , and mostly binary case has received tremendous attention in recent years , unsupervised multi class settings have not yet been considered . In this work we present an efficient implementation for the unsupervised extension of the multi class regularized least squares classification framework , which is , to the best of the authors’ knowledge , the first one in the literature addressing this task . The resulting kernel based framework efficiently combines steepest descent strategies with powerful meta heuristics for avoiding local minima . The computational efficiency of the overall approach is ensured through the application of matrix algebra shortcuts that render efficient updates of the intermediate candidate solutions possible . Our experimental evaluation indicates the potential of the novel method , and demonstrates its superior clustering performance over a variety of competing methods on real world data sets .
Index Terms—Unsupervised Learning , Multi Class Regularized Least Squares Classification , Maximum Margin Clustering
I . INTRODUCTION
Unsupervised learning belongs to the most important tasks at the beginning of each data mining process : In an early phase , no labeled data at all are given , and the task consists in extracting reasonable information based on the patterns only . Various unsupervised learning tasks like clustering or dimensionality reduction have been proposed in the literature over the years [ 1 ] . In this work we concentrate on clustering , which plays a central role in a variety of real world applications in computer vision , information retrieval , marketing , and many other fields [ 2 ] . Roughly speaking , clustering techniques aim at grouping objects into clusters , so that objects with similar characteristics belong to the same cluster , and those with different properties to different ones .
In recent years , the supervised learning method known as the support vector machine ( SVM ) [ 3 ] , [ 4 ] , and related regularized learning schemes have been extended to unsupervised learning settings , in most cases under the name maximum margin clustering ( MMC ) [ 5 ] . These extensions aim at finding a partition of the unlabeled patterns into classes , so that a subsequent application of the underlying supervised model yields the overall best result . In general , these unsupervised extensions induce combinatorial or non convex optimization tasks that are difficult to address . However , since the obtained models have been proven to outperform standard clustering techniques in many experimental analyses , they have received considerable attention during the recent years .
Xu et al . [ 5 ] were among the first ones who formalized the extension of SVMs to unsupervised learning scenarios . Their optimization approach is based on reformulating the original combinatorial task as semidefinite programming problem [ 6 ] , which can then be addressed via standard solvers . An extension of this framework is provided by Valizadegan and Jin [ 7 ] . Their approach is based on semidefinite programming . In contrast to Xu et al . [ 5 ] , they show how to reduce the number of involved optimization variables , thus showing how to reduce the computational runtime . A recent local search approach for the linear case is given by Zhao et al . [ 8 ] . Their optimization framework is based on a combination of recently proposed cutting plane schemes and concave convex procedures . Similar ideas have also been presented by Li et al . [ 9 ] .
An alternative approach for the binary case is suggested by Zhang et al . [ 10 ] . Basically , their simple but surprisingly effective approach is based on iteratively applying a support vector machine model to improve kind of an “ initial guess ” that is obtained via an auxiliary clustering framework . One of the key ingredients of their framework , however , is the replacement of the original hinge loss by the ε insensitive or the square loss . As pointed out by Zhang et al . [ 10 ] , the resulting models “ can more easily get out of a poor solution ” . These ideas are extended by Gieseke et al . [ 11 ] , who propose matrix based update strategies that can be used to significantly speed up stochastic search frameworks . In line with the approach of Zhang et al . [ 10 ] , they resort to the square loss in this context .
Contribution . While there exists a significant body of research on extending supervised regularized classifiers to clustering under the framework of maximum margin clustering , almost all of the work has concentrated on the binary case . The exception is the cutting plane multi class method of Zhao et al . [ 12 ] . While Xu et al . [ 13 ] also formalize a method for the multi class setting , their method has a runtime complexity of O(n7 ) for n patterns , which becomes impractical for realworld problems .
In this work we extend the concept of supervised onevs all multi class regularized least squares classification [ 14 ] to unsupervised learning settings . As reported by Zhang et al . [ 10 ] and Gieseke et al . [ 11 ] , the square loss depicts a very reasonable choice in the context of such clustering settings and offers desirable computational shortcuts for optimization strategies that address the resulting combinatorial tasks . The particular contribution provided in this work is twofold :
1 ) Firstly , we show how to enhance simple steepest descent strategies by means of a powerful meta heuristic that effectively avoids local minima with a suboptimal clustering performance . While being a seemingly simple modification , we demonstrate that this minor yet crucial adaptation provides major improvements to the clustering accuracy compared to straightforward stochastic search and steepest descent implementations .
2 ) Secondly , in line with the work of Gieseke et al . [ 11 ] , we provide computational shortcuts for assessing the quality of the intermediate clustering candidate solutions . As we show , these shortcuts render function calls possible to be conducted in O(1 ) time , which paves the way for an exhaustive search in the large combinatorial search space .
The meta heuristic ( which we call a shaking strategy ) is an important algorithmic ingredient for the unsupervised one vsall extension which we address . We experimentally analyze our approach on various data sets ; the results demonstrate that our approach is capable of yielding better clustering accuracies than conventional techniques in most cases.1
II . MATHEMATICAL BACKGROUND
In this section we provide the mathematical notations and the mathematical background related to the general concept of regularized kernel methods [ 3 ] , [ 4 ] , which encompasses the regularized least squares classification framework and support vector machines as a special case . We start from the standard supervised setting and proceed to re formalize the central concepts for the unsupervised learning setting . To simplify the notation , we denote the set {1 , . . . , n} of natural numbers the set of all n × m matrices with real by [ n ] . Further , coefficients are denoted by Rn×m . Given a particular matrix M ∈ Rn×m , we denote its element in the i th row and j th column by Mi,j . For two index sets R = {i1 , . . . , ir} ⊆ [ n ] and S = {k1 , . . . , ks} ⊆ [ m ] , we use MR,S to denote the sub matrix that only contains the rows and the columns of M that are indexed by R and S . Finally , we use the shorthand MR,[m ] = MR , and use yi to denote the i th coordinate of a vector y ∈ Rn .
1It is worth pointing out that , so far , no publicly available implementation can be found in the literature that takes care of the interesting multi class maximum margin principle , and we consider the approach presented in this work to be a valuable candidate for such difficult multi class clustering settings . Our implementation will be made available as part of the RLScore software library at http://staffcsutufi/∼aatapa/software/RLScore/
A . Binary Classification Scenarios
We start by depicting the binary cases , for both supervised and unsupervised learning settings . The multi class scenarios that are central for the work at hand are described afterwards . 1 ) Supervised Regularized Kernel Methods : Regularized least squares and support vector machines can be seen as a special case of so called regularized kernel methods [ 3 ] , [ 4 ] . We briefly define these settings and then show how to extend the corresponding supervised models to unsupervised ( multiclass ) learning settings . Let X be an arbitrary set and let k : X × X → R be a kernel function that can be seen as a similarity measure for the elements in this space . For a given labeled training set T = {(x1 , y1 ) , . . . , ( xn , yn)} ⊂ X × Y with Y = {−1 , +1} , the regularized risk minimization problem is defined as
L,yi , f ( xi) + λ||f||2Hk
,
( 1 ) n i=1 argmin f∈Hk where f is the prediction function ( also called model ) that maps a given data pattern to a real valued prediction and · Hk is a norm in a reproducing kernel Hilbert space Hk induced by the kernel function k . The disagreement between the predictions and the true labels is measured via a loss function L : Y × R → [ 0,∞ ) that gives rise to the empirical risk , which , in turn , measures how well the prediction function training patterns . The regularization parameter fits to all λ ∈ R+ determines the trade off between the first term of the task ( 1 ) and the complexity of the prediction function f . Two prominent representatives of this family of regularization methods are support vector machines and the concept of regularized least squares classification [ 15 ] . The first one stems from the use of the hinge loss L,y , f ( x) = max(0 , 1− L,y , f ( x) =,y − f ( x) 2 . By the representer theorem [ 16 ] , yf ( x) ) , whereas the latter one is based on the square loss any solution f∗ ∈ Hk of the task ( 1 ) has the form f∗(· ) = aik(xi,· )
( 2 ) i=1 with appropriate coefficients a = ( a1 , . . . , an)T ∈ Rn . Hence , by plugging in the square loss into the objective and by using = aTKa [ 4 ] with kernel matrix K ∈ Rn×n consisting ||f∗||2Hk of entries Ki,j = k(xi , xj ) , we can rewrite the task at hand as argmin a∈Rn
J(a ) with
J(a ) = ( y − Ka)T(y − Ka ) + λaTKa .
( 3 )
( 4 )
The objective J(a ) of the above optimization task is convex and differentiable with respect to a ∈ Rn . Thus a global min∂a J(a ) = 0 . imizer can analytically be obtained by enforcing ∂ One can therefore obtain an optimal solution via a∗ = Gy
( 5 ) n with
G = ( K + λI )
−1 and where I ∈ Rn×n is the identity matrix [ 15 ] . Although the resulting models are , in general , not sparse ( as it is often the case for standard support vector machines ) , the above closedform solution is a desirable property [ 4 ] , [ 15 ] . As we will see below , this is especially the case in the context of the considered clustering scenarios .
2 ) Unsupervised Least Squares Extension : As pointed out by Zhang et al . [ 10 ] , the square loss depicts an ideal candidate for the maximum margin principle from a practical point of view . Further , the above closed form solution can also be used to greatly speed up the computations induced by a variety of search strategies [ 11 ] . The direct extension of the supervised regularized kernel methods ( 1 ) to the unsupervised case for a given unlabeled training set T = {x1 , . . . , xn} ⊂ X has the form [ 5 ] :
L,yi , f ( xi) + λ||f||2Hk n i=1 argmin y∈{−1,+1}n , f∈Hk
Hence , the difficult part is the additional integer optimization variable y ∈ {−1 , +1}n that encodes the partition of the given unlabeled patterns . To avoid trivial solutions , some form of a balancing constraint is usually added for such clustering settings , which is of the form fififififi 1 n n i=1 fififififi < ε max(0 , yi ) − bc with user defined parameters bc ∈ [ 0 , 1 ] and ε ∈ R+ . By again considering the square loss and by substituting ( 5 ) back into ( 4 ) , we obtain argmin y∈{−1,+1}n
F ( y )
( 6 ) with
F ( y ) = ( y − KGy)T(y − KGy ) + λyTGKGy
( 7 ) as resulting optimization task for the case of the square loss .
B . Multi Class Classification Scenarios
We are now ready to address the multi class learning settings that are the basis of the optimization schemes derived in this work . Like above , we start by outlining the supervised models followed by their unsupervised extensions .
1 ) Supervised Multi Class Extension : In the literature , several ways to extend the concept of support vector machines and their variants to multi class settings have been proposed . As reported by Rifkin and Klautau [ 14 ] the so called oneversus all multi class classification settings depicts a valuable candidate for such learning scenarios , and we follow this line of research for the unsupervised case . In such multi class supervised settings , we are given a training set T = {(x1 , c1 ) , . . . , ( xn , cn)} ⊂ X × C with C = {1 , . . . ,|C|} as set of all possible class labels . In a nutshell , one aims at deriving models f1 , . . . , f|C| such that a new pattern x ∈ X is assigned to the class whose associated model is the most confident . A variety of different objectives ( and loss functions ) have been proposed in the literature [ 14 ] . In the following , we consider extensions of the supervised models ( 1 ) to the multi class case having the form argmin f1,,f|C|∈Hk where the loss function L can be defined via a binary encoding of the class memberships : Let c ∈ Cn be the vector containing the class labels of the training examples . Further , let
, i=1 h=1 n |C| L,ci , fh(xi) + λ||fh||2Hk −1  + 2
δcj hej ∈ {−1 , +1}n n j=1
−1 ph(c ) =
( 8 ) be a binary vector defined for each class h ∈ C , where δ is the Kronecker delta ( ie , we have δcj h = 1 if cj = h and δcj h = 0 otherwise ) , and ej is the j th standard basis vector of Rn . Hence , the i th component of vector ph(c ) equals +1 in case the i th training pattern belongs to the class h , and equals −1 otherwise . Based on these definitions , one can formulate the loss in the multi class setting for the square loss as + λ||fh||2Hk
,ph(c)i − fh(xi) 2 n
|C| argmin
. f1,,f|C|∈Hk h=1 i=1
Hence , the goal of the learning process is the search of binary valued prediction functions f1 , . . . , f|C| that minimize the above risk . It can therefore be seen as training |C| binary models independently . As pointed out above , such frameworks have been shown to work as well as other sophisticated multiclass schemes for such supervised settings , see Rifkin and Klautau [ 14 ] .
2 ) Unsupervised Least Squares Extension : Exactly as for the binary case , one can extend the multi class framework depicted above to unsupervised settings by considering the class membership vector c ∈ Cn as additional optimization variable . For the square loss , this leads to the following optimization task : n
|C| h=1 i=1
,ph(c)i − fh(xi) 2
+ λ||fh||2Hk
. argmin c∈Cn f1,,f|C|∈Hk
Hence , one is again given a mixed integer programming problem ; the key problem is to find an appropriate assignment for the integer variable c such that the induced |C| supervised binary classification tasks yield the overall best results . Note that , while the objectives seem to be independent from each other , they interact via the vector c , since changing the class membership of a single training instance leads to the modification of two of the induced classification models f1 , . . . , f|C| . The optimization problem for the unsupervised extension of the one vs all multi class framework can be re written in the form argmin c∈Cn
Q(c )
( 9 ) with
|C| h=1
Q(c ) =
F ( ph(c) ) ,
( 10 ) where F is defined via ( 7 ) . Hence , the unsupervised multiclass extension can be considered as task of finding an appropriate local minimum for the objective function Q(c ) . Note that additional constraints can ( and should ) be used to enforce appropriate ratios of the cluster sizes . In the next section , we propose an efficient algorithm for finding accurate clustering solutions that aim at minimizing the above objective subject to such cluster constraints , along with computational shortcuts for assessing the quality of intermediate candidate solutions .
III . ALGORITHMIC FRAMEWORK
In this section we describe the basic ideas behind the proposed algorithm without getting into the computational details . Due to the discrete nature of the clustering problem , we employ direct optimization methods for searching appropriate labels for the data points . In the literature , this type of methods are often referred to as hill climbing algorithms ( see eg Russel and Norvig [ 17] ) .
There are different variants of hill climbing , such as stochastic and steepest hill climbing . In addition , there are so called meta algorithms that are built on top of the hill climbing algorithms , such as climbing with random restarts , etc . Here , we focus mainly on the idea of the steepest descent hill climbing , in which all closest neighbors of the current solution are compared , and the current solution is replaced with the neighbor having the lowest value of the objective function . In our case , the set of closest neighbors consists of label vectors that differ from the current solution only by one entry . In addition , we propose a meta algorithm we call shaking that uses the idea of steepest descent so that it is less likely to get stuck to local minima with inferior clustering performance than the basic steepest descent search . For convenience , we use S(c , j , d ) , where c ∈ Cn , j ∈ {1 , . . . , n} , and d ∈ C , to denote the value of the objective function Q defined in ( 10 ) for a cluster label vector , whose entries are equal to those of c except that the label of the jth data point has been switched from cj to d . This allows us to denote the search directions in the space of cluster label vectors so that each direction corresponds to switching the cluster label of a single data point . Armed with the above notation , and a vector of initial cluster assignments c ∈ Cn for n data points , we next consider the search algorithms for solving the clustering problems . b ←True for j = 1 , . . . , n do d ← argmind∈C S(c , j , d ) if cj = d then cj ← d b ←False
Algorithm 1 STOCHASTIC DESCENT 1 : Initialize c ∈ Cn randomly 2 : loop 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : end if 13 : 14 : end loop end for if b then break end if
Stop if local optimum found
Fig 1 . Stochastic Descent j , d ← argminj∈{1,,n},d∈C S(c , j , d ) if cj = d then
Algorithm 2 STEEPEST DESCENT 1 : Initialize c ∈ Cn randomly 2 : loop 3 : 4 : 5 : 6 : 7 : end if 8 : 9 : end loop break cj ← d else
Fig 2 . Steepest Descent al . [ 11 ] . In our experiments , we use a similar algorithm modified for multi class clustering as a baseline method . The modification can take advantage of the computational shortcuts presented in Appendix , and it is therefore computationally as efficient as the other methods proposed in this paper .
Another framework is the basic steepest descent search ( see Algorithm 2 ) for the multi class clustering problem . The idea is that during each iteration the algorithm finds a pair ( j , d ) , where j is the index of the data point , for which switching the cluster label would decrease the value of the objective function the most and d is the corresponding new cluster label . The algorithm stops when a local minimum is found , that is , switching the cluster assignment of a single data point will not decrease the objective value .
A . Basic Descent Strategies
B . Avoiding Local Minima via Shaking
One of the most straightforward approaches is the so called stochastic hill climbing , in which the algorithm simply goes trough the data points one at a time and switches its current class label to another one if it decreases the objective value , and stops when a local optimum is found . This type of algorithms were proposed for binary clustering by Gieseke et
Both the stochastic and steepest descent methods can easily get stuck in local minima corresponding to inferior clusterings of the data ; the existence and severity of this problem is confirmed in our experiments . For this reason , we propose to improve the steepest descent algorithm with a simple , but surprisingly effective trick ( see Algorithm 3 ) . for d ∈ C do α ← n for j = 1 , . . . , α do
Algorithm 3 STEEPEST DESCENT WITH SHAKING 1 : Initialize c ∈ Cn randomly 2 : for i = 0 , . . . , s do 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : end for
2i|C| + n|C| − |{h | ch = d}| j ← argminj∈{1,,n},d=cj S(c , j , d ) cj ← d end for end for
Fig 3 . Steepest Descent with Shaking
1 ) General Idea :
Instead of traversing the search space exactly towards the steepest descent direction , the algorithm iterates through the clusters and each cluster at a time claims a number of points from the other clusters . The point the cluster d claims next is determined by the steepest descent direction . That is , the point for which switching the cluster label to d would decrease the objective value the most ( or increase the least ) , is assigned to the cluster d .
2 ) Exchanging Class Labels : The number |C| − |{h | ch = d}| n 2i|C| +
α = n
( 11 ) of points claimed by the clusters firstly depends on the round of the outer loop . During the first iterations , all clusters claim a large number of points , but the number decreases exponentially with respect to the iteration index of the loop . This dependence is encoded in the first term of ( 11 ) via the exponential factor in the nominator . To prevent the smaller clusters from disappearing completely , the number of points claimed by a cluster also depends on the number of points already assigned to the cluster in question , that is , the small clusters claim more points than the large ones . This behavior is ensured by the sum of the second and third terms of ( 11 ) , which is positive for smaller than average size clusters and negative for the larger ones . Note that this definition of α is just an ad hoc heuristic and better ones may be designed , for example , if we have a prior knowledge about the cluster sizes . Nevertheless , as we show in the experiments , even this simple approach performs considerably better than the stochastic hill climbing and the basic steepest descent , since it avoids many of the inferior local minima into which the basic approaches get stuck .
Intuitively ,
3 ) Shaking Meta Heuristic : the idea of this approach can be considered as “ shaking ” the solution so that the local minima can be avoided . In the beginning , the solution is shaken profusely , but the intensity quickly decreases with the rounds of the outer loop . Because of the exponential decrease , the number s of shakings can be set to a small constant . In our experiments , we always use the value s = 20 . The behavior of the proposed method is demonstrated in Figure 4 with a toy example consisting of three Gaussian clusters . First , the patterns are labeled randomly . Then , each cluster claims points from other clusters at a time and the number of points claimed decreases during each iteration .
C . Efficient Optimization via Matrix Based Updates
As observed above , all considered search algorithms can be formulated in terms of the operation S(c , j , d ) that computes the objective value for a certain neighbor of the label vector c . In particular , we note that the shaking heuristic requires roughly O(s|C|n2 ) calls of S(c , j , d ) .
A naive approach for computing it would require retraining the classifiers from scratch each time the operation is used , which would clearly be computationally infeasible except for very small data sets . For example , the computational complexity of training support vector machine classifiers with non linear kernels is O(n3 ) in the worst case , and hence the overall complexity of running the shaking heuristic together with those would be of order O(s|C|n5 ) .
The next theorem characterizes one of the main contributions of this paper . This offers massive runtime savings compared to the naive implementation mentioned above . Theorem 1 . The computational complexity of Algorithm 3 is
O((s|C| + r)n2 ) , where r is the rank of the kernel matrix .
Proof sketch : For the sake of exposition , we defer the lengthy proof to the Appendix . The general idea is that given a certain amount of time spent in the preprocessing phase , one can test each possible flip of a coordinate in O(1 ) time , which is far less compared to a naive implementation that would take O(n2 ) time per flip . Algorithm 3 performs O(s|C|n2 ) calls for the function S(c , j , d ) , each with O(1 ) cost , and the initialization of the caches requires O(n2r ) time .
As we show in the experimental evaluation , both the shaking framework as well as the computational shortcuts are important algorithmic ingredients to address the challenging combinatorial task induced by the unsupervised multi class extension of the maximum margin principle considered in this work .
IV . EXPERIMENTS
In the experiments we evaluate the accuracy of the proposed unsupervised multi class regularized least squares algorithm ( UMC RLS ) on several real world and synthetic data sets with several baseline methods .
A . Methods
As baselines we employ the cutting plane multi class MMC method ( CPMMC ) [ 12 ] , K means clustering initialized with kmeans++ [ 18 ] , Gaussian mixture models fitted using expectation maximization with full covariance structure ( GMM ) [ 19 ] , and spectral clustering using 10 nearest neighbors graph as similarity graph ( SC ) [ 20 ] . Additionally , to demonstrate the importance of the shaking heuristic , we also provide results
Fig 4 . Demonstration of the shaking meta algorithm on three Gaussian clusters . The first image contains the initial random assignments of the cluster labels . The next three images correspond to the first round of the outermost loop of during which each class at a time claims a large number of data points . During the following rounds , the classes keep claiming data points but the number of points claimed decreases exponentially with the index of the outermost loop . for simpler variants of the proposed method , where the combinatorial search over the cluster assignments is based either on pure stochastic search ( Stoc RLS ) or on the direction of steepest descent without the shaking heuristic ( Steep RLS ) . As discussed above , the stochastic variant extends the binary MMC method of [ 11 ] to the multi class setting .
UMC RLS , Steep RLS , Stoc RLS , and CPMMC are implemented using the Python Numpy and Scipy libraries . Further , CPMMC implementation uses the CVXOPT optimization library for solving the quadratic programs arising during training the method . The method was originally formulated only for the linear case , but as suggested by [ 12 ] , the method can be straightforwardly kernelized by running it on a feature representation generated by eigen decomposing the kernel matrix ( see , eg , [ 21 ] for a detailed discussion ) . The im plementation , which is based on the primal formulation of the optimization problem , does not scale to large problem sizes , in effect restricting our comparison to problems with few hundred training examples , and few clusters at most . Zhao et al . [ 12 ] note that one can derive a dual version of the method with much better scalability , but neither technical details nor a corresponding implementation are provided . The K means , GMM and SC implementations are from the scikitlearn machine learning library2 .
B . Data Sets
We perform experiments on eight tasks that represent a wide variety of application domains ( see Table I ) . Iris , Letter and USPS are standard benchmarks from the UCI repository . From
2http://scikit learn.org
051015202505101520250510152025051015202505101520250510152025051015202505101520250510152025051015202505101520250510152025051015202505101520250510152025051015202505101520250510152025051015202505101520250510152025051015202505101520250510152025 COMPARISON OF THE CLUSTERING METHODS . MEAN ARI AND THE ONE STANDARD DEVIATION BASED ON TEN REPETITIONS ARE PROVIDED .
TABLE I dataset Coil Coil 1 4 Coil 5 8 Iris Letter Moons USPS 1 4 USPS 5 8 size 1440 288 288 150 500 500 500 500 classes
SC
20 4 4 3 4 2 4 4
0.74 ± 0.01 0.70 ± 0.00 0.74 ± 0.06 0.43 ± 0.00 0.38 ± 0.00 0.99 ± 0.00 0.66 ± 0.00 0.93 ± 0.00
GMM
0.55 ± 0.01 0.50 ± 0.00 0.61 ± 0.03 0.96 ± 0.00 0.45 ± 0.00 0.78 ± 0.00 0.48 ± 0.00 0.64 ± 0.00
K means 0.58 ± 0.02 0.49 ± 0.00 0.60 ± 0.04 0.70 ± 0.09 0.43 ± 0.00 0.65 ± 0.00 0.61 ± 0.01 0.67 ± 0.00
CPMMC
0.47 ± 0.08 0.62 ± 0.05 0.69 ± 0.08 0.39 ± 0.10 0.69 ± 0.45 0.59 ± 0.19 0.57 ± 0.11
Stoc RLS 0.31 ± 0.10 0.29 ± 0.11 0.35 ± 0.06 0.45 ± 0.25 0.19 ± 0.12 0.21 ± 0.35 0.38 ± 0.12 0.26 ± 0.12
Steep RLS 0.43 ± 0.07 0.29 ± 0.09 0.37 ± 0.10 0.56 ± 0.13 0.20 ± 0.12 0.07 ± 0.05 0.38 ± 0.08 0.34 ± 0.14
UMC RLS 0.84 ± 0.06 1.00 ± 0.00 0.97 ± 0.09 0.96 ± 0.00 0.46 ± 0.09 1.00 ± 0.00 0.85 ± 0.02 0.91 ± 0.02
Letter , we choose the first 4 classes in the data . We split the USPS to two tasks , USPS 1 4 and USPS 5 8 , which both contain 4 classes from the original data . We use the full COIL image recognition data set [ 22 ] , as well as two subsets , COIL 1 4 and COIL 5 8 . Moons is a well known artificial benchmark data set with non linear structure . Letter and USPS data sets are sub sampled so that they have at most 500 examples , in order to allow for running the experiments for CPMMC . For the full COIL data set we do not present results for CPMMC , as the implementation does not scale to the considered number of patterns , and clusters .
C . Clustering Performance
We estimate the clustering performance of the compared methods using the Adjusted Rand Index ( ARI ) [ 23 ] . After parameter selection , each method is run 10 times , with mean ARI of the repetitions being used to represent the final performance . All kernel methods employ a Gaussian kernel in our experiments .
Similarly to the setups of [ 12 ] , [ 24 ] , we choose the regularization parameter λ and kernel width σ for the RLSbased methods an CPMMC using grid search . In preliminary experiments we noticed that the methods tended to favor small regularization parameter values , therefore λ is chosen from grid {2−10 , 2−9 . . . 2−1} . Kernel width σ is chosen from the grid {0.1σ0 , 0.2σ0 , . . . , σ0} , where σ0 is the maximum distance between two data points in the data set . For each tested parameter , the performance is computed as the mean over 10 repetitions of clustering with different random initializations . Following [ 12 ] , [ 24 ] , we set the error tolerance parameters α and for CPMMC both to 001 The parameter l of CPMMC was set to 10 , based on preliminary experiments .
Table I presents the mean ARI with standard deviations for the considered methods and data , with the results for best performing methods highlighted in each row . On seven of the eight considered data sets UMC RLS either outperforms all the other methods , or shares the place of best performing method with one other baseline method . On the USPS 5 8 data UMCRLS is the second best performing method . SC and GMM are the most competitive baselines . SC outperforms the other methods on USPS 5 8 data , and is the only baseline method also able to solve the Moons problem . GMM performs as well as UMC RLS on Iris , and almost as well on Letter . K means and CPMMC are not competitive with UMC RLS , but can still
COMPARISON OF THE CLUSTERING METHODS . THE MAXIMUM ARI OUT
OF 10 REPETITIONS IS PROVIDED .
TABLE II dataset Coil Coil 1 4 Coil 5 8 Iris Letter Moons USPS 1 4 USPS 5 8
SC
GMM K means CPMMC Stoc RLS
Steep RLS UMC RLS
0.75 0.70 0.79 0.43 0.38 0.99 0.66 0.93
0.57 0.50 0.65 0.96 0.45 0.78 0.48 0.65
0.62 0.49 0.65 0.62 0.43 0.65 0.62 0.68
0.56 0.67 0.76 0.49 0.99 0.92 0.68
0.43 0.50 0.42 0.85 0.37 0.95 0.51 0.51
0.58 0.43 0.51 0.92 0.36 0.14 0.52 0.57
1.00 1.00 1.00 0.96 0.57 1.0 0.88 0.93 on some of the data sets outperform either SC or GMM . The mean clustering performances of Stoc RLS and Steep RLS are very poor . On most runs CPMMC , Stoc RLS and Steep RLS seem to get stuck in bad local minima . Thus , the shaking heuristic implemented by UMC RLS proves to be crucial in order achieving stable and good performance .
Next , we compare the methods based on the maximum ARI achieved out of the final ten runs . The experiment allows us to estimate whether the performance of some of the considered methods could be significantly improved by using random restarts based meta heuristics . The results are presented in Table II . In this setting , CPMMC becomes competitive with the SC and GMM methods , even outperforming all the methods on USPS1 data sets . Stoc RLS and Steep RLS results are also greatly improved compared to mean results . Still , even if we compare the maximum ARI out of 10 repetitions for the other methods ( Table II ) to the mean ARI out of 10 repetitions for UMC RLS ( Table I ) UMC RLS still appears to be the overall best performing method .
To conclude , the experimental results suggest the proposed UMC RLS method often achieves high clustering performance on real world problems , and seems to represent currently the state of the art among multi class MMC type of methods . Further , the results highlight the importance of the shaking heuristic , as otherwise the combinatorial search will typically not lead to a good clustering solution . that
D . Runtime
Finally , we explore also experimentally the scalability of the proposed method . In Figure 5 we have plotted the number of steepest descent steps executed while running UMC RLS for varying sized random subsets of the COIL dataset with 20 clusters . From the plot it can be seen that the number of steps required grows linearly in the size of the data set , as can be expected from the complexity considerations . Thus the experiment further verifies that the steepest descent search can be executed efficiently for the proposed method . The computational bottleneck remains the computation of the eigen decomposition of the kernel matrix needed during initialization . Here , kernel matrix approximation techniques could be of great benefit , in case one needs to scale the method to very large data sets .
V . DISCUSSION AND FUTURE WORK
As shown by the experimental results , the proposed shaking heuristic provides considerably better results than the simple greedy approach relying on the steepest descent directions only . Still , the heuristic was the first non trivial one we tested , and hence it is of very ad hoc nature . We expect that far better heuristics can be produced if we can encode prior knowledge about the classification problem into it , as is often possible in practical problems . Heuristics could also be designed for other variations of unsupervised classification , such as learning with partial class memberships , for example [ 25 ] .
The main computational bottleneck of the proposed algorithm is computing the eigen decomposition of the kernel matrix , whose time complexity is cubic with respect to data set size in the worst case . For large data sets , a standard practise in kernel based learning is to employ sparse kernel matrix approximation techniques , such as the well know Nystr¨om method [ 26 ] , which will decrease the complexity of performing the decomposition to linear , usually without considerably harming the classification performance . Another bottleneck is due to the linear cost of a single steepest descent step , which causes the overall time complexity to become quadratic if the amount of steps also grows linearly with respect to data set size , as is usually the case with the shaking heuristic . To remedy this , we intend to develop such variations of the method and the heuristic that , instead of doing global steepest descent steps with linear cost , would employ the steps on small local subsets of the data at the time so that the step costs would scale with the subset sizes rather than the overall data set size . This would also reduce the memory size of the cache matrices required by the method ( see Appendix ) , since the caches would have to be constructed for the subsets only and reconstructed when the subset would be changed . If the size of the subsets can be fixed to a small constant , the overall computational and memory complexities of the method will be linear in the overall data set size . This requires considerably more sophisticated heuristics that also take care of changing the local subsets when needed .
The performance of kernel based learning methods depends considerably on the hyper parameters values , such as those of the regularization and kernel parameters . However , tuning the values properly is very challenging in unsupervised learning . There exists several methods for measuring the cluster validity that do not depend on the class labels of the data points but work in completely unsupervised fashion [ 27 ] , [ 28 ] . In the
Fig 5 . Number of steepest descent steps required by UMC RLS as a function of the data set size . future , we also intend to investigate the potential of these methods for setting the hype parameter values .
VI . CONCLUSION
In this work we proposed a multi class extension of the binary maximum margin principle . Our framework is based on the least squares variant of the original problem formulation , which has been experimentally proven to be a valuable candidate for such clustering settings , see , eg , Zhang et al . [ 10 ] or Gieseke et al . [ 11 ] . So far , only little work has been done related to the interesting extension of these schemes to the multi class case though . This is the focus of the work at hand dealing with the unsupervised extension of the corresponding one vs all multi class setting .
The key contributions provided in this work are ( 1 ) a carefully designed steepest descent strategy , and ( 2 ) its extremely efficient implementation . The former contribution is based on a new shaking strategy that effectively avoids getting trapped in bad local optima during early stages of the optimization process . The latter contribution is based on a series of nontrivial matrix based update steps that take care of the intermediate optimization tasks induced by the global shaking steepest descent framework . The experimental evaluation takes into account a variety of real world data sets , and the clustering accuracy of our approach is compared to the ones of stateof the art methods . The results demonstrate the superior performance of the proposed framework and , hence , indicates that the unsupervised regularized least squares approach is a promising clustering variant , given that one addresses the induced combinatorial optimization task appropriately .
APPENDIX
In Section III we gave an intuitive description of the proposed search algorithm , and pretended a claim about its overall computational complexity . Here , we show in detail how the claimed complexity can be achieved . The consideration can be divided into the following three fundamental parts :
A . Initialization of cache memories : Before starting the actual search , certain cache memories have to be initialized that the subsequent parts will take advantage of .
B . Computation of the steepest descent directions : The proposed search algorithm computes these directions before deciding , for which data point the cluster label should be switched next .
1002003004005006007008009001000data set size05001000150020002500300035004000steepest descent steps C . Update of the caches : whenever a cluster label of a data point is switched , the cache memories have to be updated in order to maintain the ability to compute the steepest descent directions efficiently .
We go through these phases one by one before summarizing them in the proof of Theorem 1 . A . Initialization of Cache Memories
First , we reformulate the objective function of the regularized least squares framework . Let K = VΛVT be the eigen decomposition of the kernel matrix , let Λ = ( Λ + λI)−1 , and let F ( y ) be defined as in ( 7 ) . We can prove the following : Lemma 1 .
F ( y ) = 1 − yTVΛΛVTy
Proof : Using standard linear algebra techniques , we ob tain the following decomposition F ( y ) = ( y − KGy)T(y − KGy ) + λyTGKGy
= yTV
= yTV
= yT ( I − KG − GK + GKKG + λGKG ) y = yTV
I − 2ΛΛ + Λ2Λ2 + λΛΛ2
I + ( −2I +ΛΛ + λΛ)ΛΛ
I + ( −2I +Λ(Λ + λI))ΛΛ I + ( −2I + I)ΛΛ
I − ΛΛ = 1 − yTVΛΛVTy .
= yTV
VTy
VTy
= yTV
VTy
VTy
VTy
Given the kernel matrix containing all pairwise kernel evaluations between the training data points , the computation of the compact decomposition , in which only the eigen vectors corresponding to the nonzero eigenvalues are computed , requires O(r2n ) time , where r is the rank of the kernel matrix.3 It is worth pointing out that the eigen decomposition of the kernel matrix is often used to turn the kernel based clustering setting into a linear one ( as is done in the multiclass MMC experiments by , eg , Zhao et al . [ 12] ) ; it therefore forms a common computational bottleneck for the kernel based competitors considered in our experimental evaluation . Assumption 1 . Assume that we are given the compact eigen decomposition of the kernel matrix K = VΛVT , where V ∈ Rn×r , Λ ∈ Rr×r , and r is the rank of the kernel matrix , as well as an initial vector of class labels c ∈ Cn . In the initialization phase , we prepare the following cache memories which are updated whenever the vector of class labels is changed :
• The n × n matrix
R = VΛΛVT ,
( 12 )
• the vectors Rpc(c),∀c ∈ C , 3In the compact decomposition , the matrix V ∈ Rn×r contains the r eigenvectors and Λ ∈ Rr×r is a diagonal matrix containing the r nonzero eigenvalues .
• as well as the values Q(c ) and F ( ph(c)),∀h ∈ C . The computational complexity of the initialization phase is dominated by the computation of R , which can be done in O(rn2 ) time given the compact decomposition of the kernel matrix of rank r .
B . Computation of the Steepest Descent Directions
The next lemma concerns the efficient computation of S(c , j , d ) , given that certain intermediate results have already been computed and cached . Its proof also encompasses implementation details of the search algorithms that take advantage of the computational short cuts . Lemma 2 . Assume that we have cache memories given in Assumption 1 available . Then , the value of S(c , j , d ) can be computed in a constant time .
Proof : Let y ∈ {−1 , 1}n and let us denote ˆy = y−2yjej , that is , y and ˆy are two ±1 valued vectors differing from each other only by the jth entry . Moreover , we denote t = Ry and {j} = {1 , . . . , n} \ {j} . Then , continuing from ( 12 ) , if we already know F ( y ) , R , and t , the value of F for ˆy can be computed from
F ( ˆy ) = 1 − ˆyTRˆy = 1 − ˆyjRj,j ˆyj − 2ˆyjRj,{j}y{j} − yT = 1 − yjRj,jyj − 2ˆyjRj,{j}y{j} − yT = 1 − yTRy − 2(ˆyj − yj)TRj,{j}y{j} = F ( y ) − 2(ˆyj − yj)T(tj − Rj,jyj ) = F ( y ) + 4yj(tj − Rj,jyj ) = F ( y ) + 4yjtj − 4Rj,j .
{j}R{j},{j}y{j} {j}R{j},{j}y{j}
Moreover , we observe that we can define a simple formula for calculating the difference in the objective values if a single entry of the ±1 valued vector is flipped as follows : D(y , j ) = F ( ˆy ) − F ( y ) = 4yjtj − 4Rj,j .
( 13 )
Putting together ( 8 ) , ( 10 ) , and ( 13 ) , S(c , j , d ) can be for mally written as
S(c , j , d ) = Q(c ) + D(pcj ( c ) , j ) + D(pd(c ) , j ) .
The formula contains the objective value adjustments of both the old cluster cj , and the new one d of the aggregate objective function ( 10 ) that allows the cluster assignments of a single data point to have only one positive entry . Thus , given the assumptions about cached intermediate results , we can calculate the objective value change caused by switching a single entry of the cluster label vector c in O(1 ) time .
C . Updates of the Caches
After the steepest descent direction is found , and the cluster label of the corresponding patterns is switched , part of the cache memories given in Assumption 1 have to be updated accordingly in order to maintain the ability to perform fast searches . As shown in the following lemma , the update operation does now slow down the computation of the steepest descent directions . Lemma 3 . The cache memories given in Assumption 1 can be updated in O(n ) time after the cluster label of a single pattern is switched .
Proof : Given that Rpcj ( c ) is stored in memory , the vector
R(pcj ( c ) − 2ej ) can be obtained from
R(pcj ( c ) − 2ej ) = Rpcj ( c ) − 2(Rj)T in O(n ) time . The vector R(pd(c ) + 2ej ) can be computed analogously . The values Q(c ) , F ( pcj ( c) ) , and F ( pd(c ) ) are obtained in a constant time as implied by the proof of Theorem 2 . The matrix R does not depend on c and , thus , does not have to be updated .
D . Runtime Proof
Theorem 1 .
Putting everything together , we arrive to the proof of
Proof of Theorem 1 : As shown in Lemma 2 , the evaluation of S(c , j , d ) can be performed in constant time by taking advantage of the cache memories defined in Assumption 1 . Since there are |C| clusters and n data points , finding the steepest descent direction requires O(|C|n ) time . Lemma 3 in turn shows that the cache memories can be updated according to the steepest descent direction in O(n ) , but this is dominated by the time required for finding the next steepest descent direction . Altogether , Algorithm 3 performs O(s|C|n2 ) calls for the function S(c , j , d ) , each with a constant time complexity , and the initialization of the caches requires O(n2r ) time . The proof follows .
ACKNOWLEDGMENT
This work has been supported in part by the Academy of Finland ( Tapio Pahikkala , grant 134020 ) and by funds of the Deutsche Forschungsgemeinschaft ( DFG ) ( Fabian Gieseke , grant KR 3695 ) . The authors would like to thank the anonymous reviewers for valuable comments and suggestions on an early version of this work .
REFERENCES
[ 1 ] T . Hastie , R . Tibshirani , and J . Friedman , The Elements of Statistical
Learning , 2nd ed . Springer , 2009 .
[ 2 ] A . Jain and R . Dubes , Algorithms for clustering data .
Prentice Hall ,
1988 .
[ 3 ] B . Sch¨olkopf and A . J . Smola , Learning with Kernels : Support Vector Machines , Regularization , Optimization , and Beyond . Cambridge , MA , USA : MIT Press , 2001 .
[ 4 ] I . Steinwart and A . Christmann , Support Vector Machines . New York ,
NY , USA : Springer Verlag , 2008 .
[ 5 ] L . Xu , J . Neufeld , B . Larson , and D . Schuurmans , “ Maximum margin clustering , ” in Advances in Neural Information Processing Systems 17 , L . K . Saul , Y . Weiss , and L . Bottou , Eds . MIT Press , 2005 , pp . 1537– 1544 .
[ 6 ] S . Boyd and L . Vandenberghe , Convex Optimization .
Cambridge
University Press , 2004 .
[ 7 ] H . Valizadegan and R . Jin , “ Generalized maximum margin clustering and unsupervised kernel learning , ” in Advances in Neural Information Processing Systems 19 , B . Sch¨olkopf , J . C . Platt , and T . Hoffman , Eds . MIT Press , 2007 , pp . 1417–1424 .
[ 8 ] B . Zhao , F . Wang , and C . Zhang , “ Efficient maximum margin clustering via cutting plane algorithm , ” in Proceedings of the SIAM International Conference on Data Mining . Society for Industrial and Applied Mathematics , 2008 , pp . 751–762 .
[ 9 ] Y F Li , I . Tsang , J . Kwok , and Z H Zhou , “ Tighter and convex maximum margin clustering , ” in Proceedings of the 12th International Conference on Artificial Intelligence and Statistics , ser . JMLR : Workshop and Conference Proceedings , D . van Dyk and M . Welling , Eds . , vol . 5 .
JMLR , 2009 , pp . 344–351 .
[ 10 ] K . Zhang , I . Tsang , and J . Kwok , “ Maximum margin clustering made practical , ” in Proceedings of the 24th International Conference on Machine Learning , ser . ACM International Conference Proceeding Series , Z . Ghahramani , Ed . , vol . 227 . ACM , 2007 , pp . 1119–1126 .
[ 11 ] F . Gieseke , T . Pahikkala , and O . Kramer , “ Fast evolutionary maximum margin clustering , ” in Proceedings of the 26th International Conference on Machine Learning , ser . ACM International Conference Proceeding Series , L . Bottou and M . Littman , Eds . , vol . 382 . ACM , June 2009 , pp . 361–368 .
[ 12 ] B . Zhao , F . Wang , and C . Zhang , “ Efficient multiclass maximum margin clustering , ” in Proceedings of the 25th international conference on Machine learning , ser . ACM International Conference Proceeding Series , W . W . Cohen , A . McCallum , and S . T . Roweis , Eds . , vol . 307 . ACM , 2008 , pp . 1248–1255 .
[ 13 ] L . Xu and D . Schuurmans , “ Unsupervised and semi supervised multiclass support vector machines , ” in Proceedings of the 20th national conference on Artificial intelligence , A . Cohn , Ed . AAAI Press , 2005 , pp . 904–910 .
[ 14 ] R . Rifkin and A . Klautau , “ In defense of one vs all classification , ”
Journal of Machine Learning Research , vol . 5 , pp . 101–141 , 2004 .
[ 15 ] R . Rifkin , G . Yeo , and T . Poggio , “ Regularized least squares classification , ” in Advances in Learning Theory : Methods , Models and Applications , ser . NATO Science Series III : Computer and System Sciences , J . Suykens , G . Horvath , S . Basu , C . Micchelli , and J . Vandewalle , Eds . Amsterdam : IOS Press , 2003 , vol . 190 , ch . 7 , pp . 131–154 .
[ 16 ] G . Kimeldorf and G . Wahba , “ Some results on Tchebycheffian spline functions , ” Journal of Mathematical Analysis and Applications , vol . 33 , no . 1 , pp . 82–95 , 1971 .
[ 17 ] S . Russell and P . Norvig , Artificial Intelligence : A Modern Approach .
Prentice Hall , 1995 .
[ 18 ] D . Arthur and S . Vassilvitskii , “ k means++ : the advantages of careful seeding , ” in Proceedings of the eighteenth annual ACM SIAM symposium on Discrete algorithms , N . Bansal , K . Pruhs , and C . Stein , Eds . Philadelphia , PA , USA : Society for Industrial and Applied Mathematics , 2007 , pp . 1027–1035 .
[ 19 ] A . P . Dempster , N . M . Laird , and D . B . Rubin , “ Maximum likelihood from incomplete data via the EM algorithm , ” Journal of the Royal Statistical Society , Series B , vol . 39 , no . 1 , pp . 1–38 , 1977 .
[ 20 ] J . Shi and J . Malik , “ Normalized cuts and image segmentation , ” IEEE Transactions on Pattern Analysis and Machine Intelligence , vol . 22 , no . 8 , pp . 888–905 , Aug . 2000 .
[ 21 ] B . Sch¨olkopf , S . Mika , C . Burges , P . Knirsch , K R M¨uller , G . R¨atsch , and A . Smola , “ Input space versus feature space in kernel based methods , ” IEEE Transactions On Neural Networks , vol . 10 , no . 5 , pp . 1000– 1017 , 1999 .
[ 22 ] S . Nene , S . Nayar , and H . Murase , “ Columbia object image library ( coil100 ) , ” Department of Computer Science , Columbia University , Tech . Rep . , 1996 .
[ 23 ] L . Hubert and P . Arabie , “ Comparing partitions , ” Journal of Classifica tion , vol . 2 , no . 1 , pp . 193–218 , 1985 .
[ 24 ] F . Wang , B . Zhao , and C . Zhang , “ Linear time maximum margin clustering , ” IEEE Transactions on Neural Networks , vol . 21 , no . 2 , pp . 319–332 , 2010 .
[ 25 ] W . Waegeman , J . Verwaeren , B . Slabbinck , and B . De Baets , “ Supervised learning algorithms for multi class classification problems with partial class memberships , ” Fuzzy Sets and Systems , vol . 184 , no . 1 , pp . 106–125 , 2011 .
[ 26 ] C . K . I . Williams and M . Seeger , “ Using the nystr¨om method to speed up kernel machines , ” in Advances in Neural Information Processing Systems 13 , T . K . Leen , T . G . Dietterich , and V . Tresp , Eds . MIT Press , 2001 , pp . 682–688 .
[ 27 ] M . Halkidi , Y . Batistakis , and M . Vazirgiannis , “ On clustering validation techniques , ” J . Intell . Inf . Syst . , vol . 17 , no . 2 3 , pp . 107–145 , Dec . 2001 . [ 28 ] Q . Zhao , “ Cluster validity in clustering methods , ” PhD dissertation ,
University of Eastern Finland , 2012 .
