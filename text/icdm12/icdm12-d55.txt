Effective and Robust Mining of Temporal Subspace Clusters
Hardy Kremer
Thomas Seidl
Stephan G¨unnemann Arne Held RWTH Aachen University , Germany
{kremer , guennemann , held , seidl}@csrwth aachende
Abstract—Mining temporal multivariate data by clustering is an important research topic . In today ’s complex data , interesting patterns are often neither bound to the whole dimensional nor temporal extent of the data domain . This challenge is met by temporal subspace clustering methods . Their effectiveness , however , is impeded by aspects unavoidable in real world data : Misalignments between time series , for example caused by out of sync sensors , and measurement errors . Under these conditions , existing temporal subspace clustering approaches miss the patterns contained in the data .
In this paper , we propose a novel clustering method that mines temporal subspace clusters reflected by sets of objects and relevant intervals . We enable flexible handling of misaligned time series by adaptively shifting time series in the time domain , and we achieve robustness to measurement errors by allowing certain fractions of deviating values in each relevant point in time . We show the effectiveness of our method in experiments on real and synthetic data .
I .
INTRODUCTION
Clustering of temporal data is a major area of data mining research [ 6 ] , [ 9 ] , [ 18 ] , [ 25 ] , [ 26 ] . Temporal data reflect the changing state of an observed system over time . Examples are financial ratios , engine readings in the automotive industry , patient monitoring , audio data , interaction sequences in robotics , and climate simulation models . The data are often multivariate , with each attribute representing a distinct aspect of the observed objects .
Clustering approaches mine for unknown patterns in temporal databases by grouping time series based on their similarity . Clusters of time series correspond to groups of objects having a similar evolution over time , and detected clusters represent these evolutions .
Multivariate ( as well as univariate ) time series belong to the class of high dimensional data . Mining high dimensional data is challenging , since not all of the dimensions of a data object are relevant for the mining task at hand . Accordingly , methods were developed that analyze substructures of the data . In the classification domain , for example , there are methods for vector data [ 5 ] , graphs [ 17 ] , and time series [ 27 ] . These approaches use dimensional subsets , subgraphs , and subsequences to improve classification accuracy . In the domain of clustering high dimensional data , vector data can be effectively handled by using subspace clustering [ 16 ] , [ 20 ] , where each cluster has an individual set of relevant dimensions . These approaches can be applied
Figure 1 . Multivariate temporal pattern : The cluster ( containing 4 time series ) is bound to the relevant intervals ( marked in red ) in dimensions 2 & 3 . for clustering of time series ( cf . Sec II ) ; however , they use sets of ( disconnected ) dimensions , and thus the natural correlation between subsequent points in time is ignored . If subspace clustering is extended for handling temporal aspects , clustering quality can be increased [ 14 ] . The approach in [ 14 ] is based on relevant intervals ( ie , subsequences in which clustered time series are similar ) . It is designed for multivariate time series data and allows individual sets of relevant intervals in each dimension . An exemplary cluster is illustrated in Fig 1 .
The applicability of subspace clustering on real world time series data , however , is questionable . In real data , aspects like misalignments between time series and deviating values are unavoidable . In the following , we explain why existing subspace clustering algorithms and their extensions for temporal data are the wrong choice in these scenarios . Misalignments between time series that contain similar patterns are common in time series datasets . Reasons could be that sensors were out of sync , or that the same phenomena occurred slightly shifted in time . Fig 2 shows a real example , a plot of all classes of the Spoken Arabic Digit dataset from the UCI Machine Learning Repository [ 8 ] . ( It only shows a sampled set of all time series . ) In this plot , equal colors mark the same class . The plot shows that time series of some classes are out of sync to each other . An effective clustering algorithm has to be able to compensate for these misalignments and thus place the time series in the correct clusters . Generally , misalignments are well known in time series similarity research , where several distance measures were introduced to cope with this problem . Most of them are based on the idea of finding a new alignment between the compared time series to reduce the dissimilarity of the corresponding values . Existing subspace clustering approaches , however , are unable to compare different dimensions . While this is reasonable for standard vector data , where each dimension represents a distinct aspect , is an essential requirement for comparing time series . In the it dimension 2dimension 3timedimension 1 Figure 2 . Spoken Arabic Digit dataset : plot of the first dimension of all classes .
Figure 3 . Spoken Arabic Digit dataset : sampled plot of the 11th dimension of one class . worst case , subspace clustering approaches exclude time series from clusters because the misalignment obfuscates the similarity to the other time series contained in the clusters . Measurement errors , or more generally , deviating values , are another aspect of real world data . Fig 3 shows the 11th dimension of a class of the Spoken Arabic Digit dataset . All time series show a similar behavior , but there are points in time in which some of them highly deviate . Subspace clustering approaches would either omit these time series from the cluster or they would consider the corresponding point in time as irrelevant , possible losing a pattern that discriminates the cluster from others . The reason is that subspace clustering assumes that dimensions are either relevant or irrelevant for a whole cluster , and there is no possibility that a relevant dimension of a cluster could contain some noise .
In this paper , we propose a novel approach for clustering of multivariate time series going beyond the concept of temporal coherent time series subspace clusters [ 14 ] . We enable effective handling of misalignments by individually shifting of time series such that a better matching in a cluster ’s relevant intervals is obtained . We achieve robustness wrt deviating values by allowing a certain fraction of deviating values per point in time in relevant intervals . Since our problem is NP hard , we propose an algorithm for our model that delivers approximative clusterings of high quality .
II . RELATED WORK
Clustering of temporal data can roughly be divided into clustering of an incoming data stream , called stream clustering [ 15 ] , and clustering of static databases , this paper ’s topic . Our method is related to two static clustering research areas : time series clustering and subspace clustering . In the experiments , we compare to methods from both areas .
Time Series Clustering . There is much research on clustering of univariate time series data , and we suggest the surveys in [ 9 ] , [ 18 ] . Early work on multivariate time series clustering in [ 19 ] uses clustering to identify outliers . Multivariate clustering methods based on statistical features were introduced in [ 6 ] , [ 25 ] , [ 26 ] . There are no concepts in these approaches to discovery patterns hidden in parts of the dimensional or temporal extents of time series .
Most clustering approaches are based on an underlying similarity measure between time series . In the area of similarity search , there are noise robust similarity measures based on partial comparison and local shifting , as eg Longest Common Subsequences ( LCSS ) [ 24 ] . Measures like LCSS or Dynamic Time Warping ( DTW ) are called elastic .
An overview of elastic measures is given in [ 7 ] . From the different elastic measures , LCSS is most suitable for our requirements , because it allows for partial comparison and shifting on the time axis . We combined k Medoid with LCSS and use it as a competing approach in the experiments .
Subspace Clustering ( 2D ) and Triclustering ( 3D ) . Subspace clustering [ 1 ] , [ 2 ] , [ 21 ] , [ 28 ] was introduced for highdimensional ( non temporal ) vector data , where clusters are hidden in individual subsets of dimensions . Surveys can be found in [ 16 ] , [ 20 ] . Since subspace clustering is simultaneous clustering of the objects and dimensions of a dataset , it is also known as 2D clustering . When subspace clustering is applied to 3D data ( objects , dimensions , points in time ) , the time series of the individual dimensions are concatenated to obtain a 2D space ( objects , concatenated time series ) . The concatenated time series are thus interpreted as high dimensional vector data . While subspace clustering is good for handling noisy data and excluding irrelevant points in time , there are problems when subspace clustering is applied to temporal data : Subspace clustering cannot exploit the natural correlation between subsequent points in time , ie temporal coherence is lost .
Accordingly , for temporal ( 3D ) data , Triclustering was introduced [ 12 ] , [ 23 ] , [ 29 ] , which simultaneously cluster objects , dimensions , and points in time . Another type of Triclustering approaches are for clustering two related datasets together [ 4 ] , [ 11 ] , which is a different problem than the one tackled in this paper . In general , Triclustering approaches can only find block shaped clusters : A cluster is defined by a set of objects , dimensions , and points in time [ 29 ] or intervals [ 12 ] , [ 23 ] ; therefore , the points in time or intervals hold for all objects and dimensions of a cluster .
In [ 14 ] , a first approach for mining temporal coherent subspace clusters is introduced , where each dimension has different , independent relevant intervals . While being well suited for some synthetic scenarios , this approach as well as the other 3D and 2D clustering approaches , are not suited for real world data , as it was explained in the introduction : All of them are directly based on the principle of subspace clustering , and thus cannot compare different points in time , ie there is no possibility of compensating misalignments between time axis , and have substantial problems when coping with deviating values that only occur in individual time series .
In this paper , we will substantially improve the paradigm of mining temporal subspace clusters ( Sec III A ) by introducing effective concepts for misalignment compensation ( Sec III B ) and handling of deviating values ( Sec III C ) .
III . ROBUST , EFFECTIVE CLUSTERING OF MULTIVARIATE
TIME SERIES DATA
We introduce our novel approach in four steps . We start by formalizing the basic definition of an interval based cluster model in Section III A , allowing for clusters that are neither bound to the full dimensional nor temporal extent of the data domain . Based on this we present a cluster model in Section III B that enables individual shifts of time series , maximizing the similarity between the time series contained in a single cluster . In Section III C we increase the robustness of this approach by introducing a method for noise handling . In Section III D we discuss how a clustering , ie a cluster set , is obtained . A dataset can contain an abundance of possible clusters satisfying the conditions of the final cluster model . We are interested in a partitioning clustering that gives a precise and concise description of the dataset : we formalize a clustering that realizes a trade off between the number and the compactness of clusters .
Basic definitions . As input we assume a databaseDB of multivariate time series . A time series o ∈ DB is defined as o=(o[1 ] , . . . , o[Tmax] ) , with o[t]∈ RDmax . o[t , d]∈ R refers to the value at point in time t∈T and in dimension d∈D.D={1 , . . . , Dmax} denotes the set of all dimensions , andT ={1 , . . . , Tmax} denotes the set of all points in time .
There are data sets in which the contained time series have different length . Generally , our method can handle such data by design , as we will later explain . In that case , Tmax is the length of the longest time series . Alternatively , one could scale all time series to the same length . A . Interval Based Subspace Cluster
In multivariate time series data , patterns can exist in dimensional or temporal subsets of the data domain . Triclustering approaches [ 12 ] , [ 23 ] , [ 29 ] cope with such data by defining clusters by tuples(O , S , T ) : The objects O show a similar behavior in subspace S⊆D and temporal subset T ⊆T . A drawback is that each selected dimension d∈ S has to be relevant for each point in time t∈ T . In [ 14 ] a more flexible concept was introduced ; in this section ( III A ) we provide a more general reformulation of this concept , laying the foundation for the following sections . In this concept , per dimension an individual set of intervals is selected , in which the time series are similar in . In the following , such individual ( relevant ) intervals are denoted as cluster intervals .
Definition 1 : A cluster interval is bound to a specific dimension and is defined by the start and end position in that dimension . That is , a cluster interval I is a tuple
I∶=(Id , Istart , Iend ) with Id∈D , Istart∈T , Iend∈T , and Istart< Iend . Accordingly , is : Ipt∶={t∈T   Istart≤ t≤ Iend} . In the following , ● the set of all points in time contained in an interval I the tuple(I , t ) consisting of an Interval I and a point in time t with t∈ Ipt is denoted as an interval point .
Ilength∶= Iend− Istart+ 1= Ipt  ● the length of an interval I is defined by : Since Istart< Iend , it holds that Ilength> 1 ; that is , we only consider intervals that actually represent a temporal extent and not just a single isolated point in time , in which the values could be similar by chance .
A time series subspace cluster is a set of time series combined with a set of cluster intervals , in which these time series are similar . The combination of the individual cluster intervals has to ensure temporal coherence of the cluster [ 14 ] .
Definition 2 : A time series subspace cluster C ∶= ( CO , CI ) is defined by a time series set CO ⊆ DB with  CO > 1 , interval set CI∶={I 1 , . . . , I m} , and conditions : d= I j ∀I i , I j∈ CI with i≠ j and I i d∶ pt=࢝ . pt∩ I j ● The intervals per dimension are overlap free , ie of points in time over all dimensions Cext∶= m ● The cluster is temporal coherent , ie the combined set i=1I i ∃ t∈T such that{t , . . . , t+ Cext − 1}= Cext . ● All interval points fulfill a cluster property : ∀I∈ CI∶∀t∈ Ipt∶∀oi , oj∈ CO∶(oi , oj)∈ compactId a single connected interval : pt is
I i t ie , the cluster has to be compact in all interval points . The compactness is defined in the following definition . Overlap free intervals are required for the clustering process , in which time series are compared with respect to the cluster intervals of a specific cluster ( cf . Sec III D , Def . 9 ) . If tuples would be used several times , their influence would not be equal and some could dominate the similarity comparison . The cluster property defines how the similarity between time series in the cluster is measured and how similar they need to be for inclusion in the same cluster ( for a specific cluster interval ) . We use compactness as cluster property because it has shown to be effective in other subspace clustering methods [ 21 ] .
Definition 3 : The compactness relation for a point in time t and dimension d is based on the maximum norm :
( oi , oj)∈ compactd t ⇔ oi[t , d]− oj[t , d ] ≤ w with compactness parameter w . We analyze the influence of the compactness parameter in the experiments .
The crucial part in our cluster definition is the cluster property . In the following steps , we replace this property to obtain a more effective and robust time series subspace clustering technique .
B . Effective Compensation for Misalignments
A problem of real world time series datasets is the possible misalignment between time series pairs ; often these time series are very similar but when clustered , the misalignments cause the time series to be placed in different clusters . An effective clustering algorithm has to compensate for these
Figure 4 . Spoken Arabic Digit data : plot of one cluster resulting from our approach ( 9th dim ) left : with compensated misalignments . right : original data . ( Non cluster intervals are faded out . The cluster intervals in both figures are based on the shift based clustering on the left side . ) misalignments in the decision whether two time series are clustered together . Fullspace algorithms like k Medoid can compensate by using an elastic distance function like DTW or LCSS . Our approach , however , is based on subspace clustering . Until now , no subspace approach exists that can compare different dimensions in the clustering process . This is perfectly reasonable for non temporal vector data , but a drawback for time series clustering : in the worst case , a time series is lost for a cluster because the misalignment obfuscates the similarity to the other time series contained in the cluster ; if the time series would be included in the cluster it would no longer be compact .
Our novel approach combines the idea of subspace clustering with the idea of maximizing the similarity of time series in each cluster by individual shifts on the time axis . More concretely , in the clustering process , we shift the time series of a cluster on the time axis such that the compactness in the cluster ’s current cluster intervals is maximized . The result of our approach for the Spoken Arab Digit is shown in Fig 4 , where the 9th dimension of one of the found clusters is plotted . The left side shows the alignment on which the clustering is based : misalignments are compensated by individual shifts . The right side shows the same set of time series , but without the compensation . In this scenario , the cluster is much less compact in the cluster intervals . Accordingly , without the compensation , it is likely that not all of these time series would have been clustered together . In the following , we formalize our method . Each time series of a cluster is shifted by an individual number of points in time , achieving compactness in a cluster .
Definition 4 : The shift of a time series o is denoted ∆o∈ ∆ , where ∆ is the set of all possible shifts ∆∶={s∈ N  −δmax≤ s≤+δmax} for a maximal shift of δmax .
The maximal shift parameter can be compared to the bandwidth constraint of DTW . It was shown [ 22 ] that too much bandwidth can result in decreased quality of data mining results . We made similar observations , and we evaluate the influence of δmax in the experiments .
The aforementioned compactness relation ( cf . Def . 3 ) is no longer applicable . We introduce a new compactness relation in which time series are shifted before the compactness is computed .
Figure 5 . Illustration of shift concept in one dimension . Left : original time series . Right : time series a & b are shifted to achieve compactness . The corresponding cluster intervals are marked in red .
Definition 5 : The shift based compactness relation for a point in time t and a dimension d is defined by
( oi , oj , ∆i , ∆j)∈ compactd t ⇔ oi[t+∆i , d]−oj[t+∆j , d ] ≤ w based cluster property iff with compactness parameter w . A set of time series combined with a set of cluster intervals is a cluster , if it is compact in all its interval points . In our new , shift based approach , this is fulfilled if there exists at least one set of time series shifts such that the cluster is compact in all its interval points .
Definition 6 : A cluster C∶=(CO , CI ) fulfills the shift∃(∆1 , . . . , ∆ CO )∈ ∆ CO ∶∀I∈ CI∶∀t∈ Ipt∶∀oi , oj∈ CO∶ ( oi , oj , ∆i , ∆j)∈ compactId t ∨ ( t+∆i)~∈T ∨ ( t+∆j)~∈T ( eg , ( t+∆i ) ~∈ T ) we do not require compactness , as domainT at which no value exists for this time series ; we explained below . When a time series is shifted or time series of different lengths are clustered , there are points in time of the temporal ie , there needs to be a set of shifts such that the cluster is compact in the cluster ’s interval points . In specific cases denote these points in time as invalid . For example , the time series b in Fig 5 has no values for the last points in time of the right cluster interval .
These cases only occur at the bounds of the temporal domain . If caused by shifting of time series , then they are even limited by the maximal allowed shift . It is unlikely that all time series have the same invalid positions ; accordingly , these areas can contain relevant patterns in the remaining ( valid ) time series . A complete exclusion of cluster intervals covering these areas could therefore result losing cluster intervals that discriminate the cluster from others . Our cluster model explicitly allows such cluster intervals and excludes only those object pairs from the compactness verification that have invalid positions . Relationship to elastic similarity measures . Fullspace clustering approaches can be combined with elastic distance measures like LCSS ( cf . Sec II ) . When two time series are compared by an elastic measure , this time series pair has an individual alignment of the points in time . This is in contrast to our approach , where we have one individual shift per single time series ( Def . 6:∃(∆1 , . . . , ∆ CO  ) . . .∀oi , oj . . . ) .
Accordingly , one could expect that using individual shifts
1Tmaxabc1Tmaxabc ∀oi , oj . . .∃(∆oi , ∆oj ) . . . ) . This is true for fullspace clus for each pair of time series would be more effective ( ie , tering , but not for temporal subspace clustering . By allowing individual shifts per time series pair , these two time series are shifted only wrt each other ( and not wrt all time series from the cluster ) such that an arbitrary pattern from these two time series is used to achieve compactness in the cluster intervals . This is repeated for every pair , and thus in each pair a different pattern could be used . Accordingly , the cluster no longer represents a specific pattern , which is a contradiction to the idea of clustering .
We implemented variants of our approach using measures like LCSS to compensate misalignments . However , results based on these more complex ( and less efficient ) distance measures did not show the same effectiveness as the shiftbased method .
The presented model compensates for misalignments between similar time series in a cluster . In the following , we present an improvement that makes it more robust with respect to deviating values in single instances of time .
C . Robust Handling of Deviating Values
Deviating values , as for example caused by measurement errors , are unavoidable in real datasets . Values are called deviating , if they only occur in a few time series of a cluster and strongly deviate from the distribution of the other time series’ values . Subspace clustering algorithms exclude dimensions ( or intervals in TimeSC [ 14 ] ) for whole clusters , ie for all contained time series . The dimensions are either relevant or non relevant . There is no concept of relevant dimensions that contain some deviating values . Since our model so far is based on subspace clustering , it either has to remove a time series with a deviating value from the cluster , or the corresponding position in the cluster interval has to be considered non relevant , splitting up the interval into two subintervals . In the worst case , the cluster loses a pattern that discriminates it from other clusters .
Technically , this is attributable to the cluster property in Def . 6 , which considers interval points as non compact in case of deviating values . In the following , we extend Def . 6 such that a small number of deviating values does no longer force a relevant point in time to be categorized as nonrelevant . Concretely , the improved model allows for a certain fraction of deviating values per point in time in the cluster intervals .
In the new cluster property , we assume that the object values in a cluster ’s interval point ( I , t ) folt , σ2 ) , where µId N(µId low a normal distribution with a variance based on the compactness parameter w . Concretely , we have a normal distribution is the center of cluster interval I at point in time t , and the standard deviation σ corresponds to t and deviation robust cluster property iff
2 . The above figure illustrates this . We define an interval w point to be compact , if the probability of its compactness is larger than the probability that two arbitrary points x , y∈ N(µId t , σ2 ) are compact , ie x− y ≤ w . Definition 7 : A cluster C ∶=(CO , CI ) fulfills the shift ∃(∆1 , . . . , ∆ CO )∈ ∆ CO ∶∀I∈I∶∀t∈ Ipt∶ P ( oi , oj , ∆i , ∆j)∈ compactId   oi , oj∈ valid , oi≠ oj t ,(w~2)2 ) , x≠ y ≥ P  x− y ≤ w   x , y∈N(µId with valid∶={oi∈ CO  t+ ∆i∈T} being the set of objects valid in interval point(I , t ) . The probability of an interval point(I , t ) to be compact is the ratio of compact valid object pairs in(I , t ) to the number of all valid object pairs  valid  in ( I , t ) . The probability t , σ2 ) fulfill x− y ≤ w is independent of that x , y∈N(µId t . Interestingly , in case of σ = w the actual µId 2 it is also independent of w . When analytically solved , we obtain a probability of about 85 % .
2 t
Accordingly , Def . 7 holds if there is a set of shifts such that in all interval points compactness is achieved , with an allowed violation in 15 % of the time series pairs .
This completes our new cluster model . In the following , we introduce the definition of a corresponding cluster set .
D . Clustering Model the patterns in the analyzed datasetDB . We are interested
In this section we formalize a set of clusters representing in a partitioning that gives a precise and concise description of the dataset . Accordingly , we have a trade off between generalization ( number of clusters ) and precision ( compactness of single clusters ) . We realize this trade off with the costs of a clustering . The costs are the sum of the average cluster diameters ( defined in Def . 10 below ) , reflecting both the number of clusters and the cluster compactness .
Definition 8 : The costs of a clustering
M∶={C 1 , . . . , C n} are cost(M)∶= Q
C∈MCluster(C )
The lower the costs are , the better is the trade off between the number of clusters and the cluster diameters . The costs are not monotonic in the number of clusters : by transfering time series from the clusters {C 1 , . . . , C n} to ∶= an additional cluster C n+1 , the costs of clustering M′ {C 1 , . . . , C n , C n+1} with object sets{C 1Oࢨ C n+1O , . . . , C nOࢨ C n+1O , C n+1O } can be larger ( the additional cluster outweighs smaller cluster diameters ) , smaller ( the additional cluster is counterbalanced by smaller cluster diameters ) , or even the same as for M .
The average diameter of a cluster reflects its compactness and is based on the average pairwise distance of the contained time series . This distance is constrained to the cluster intervals and uses the shifted time series . dIt2/w2/ww Definition 9 : The normalized shift distance between time series oi , oj for a cluster C and its valid interval points intp∶={(I , t)  I∈ CI , t∈ Ipt,(t+∆i)∈T , ( t+∆j)∈T} oi[t+∆i , Id]−oj[t+∆j , Id ] dC(oi , oj , ∆i , ∆j)∶= 1 intp  Q(I,t)∈intp is defined as
Based on this distance , we define the cluster diameter which is used for the clustering costs in Def . 8 .
1  CO  wise time series shift distance in that cluster :
Definition 10 : The cluster diameter is the average pair
Cluster(C)∶= dC(oi , oj , ∆i , ∆j ) ( ∆1,,∆ CO )∈∆C with ∆C⊆ ∆ CO  , all shift sets that yield a fulfilled cluster  ∆C ≥ 1 holds . The minimal cluster size in Def . 2 (  CO > 1 ) ensures the normalization factor is not 0 , ie  CO  ≥ 1 . property according to Def . 7 . Since C is a valid cluster ,
Q oi,oj∈CO oi≠oj min
2
We know ( Def . 6 & 7 ) that there is at least one shift set such that C is compact in its cluster intervals . In the final clustering result , however , we are searching for a set of maximally compact clusters . Accordingly , for determining the cluster diameter , we minimize over all shifts sets that yield a compact cluster , reducing the clustering costs in Def . 8 .
Definition 11 : The cost minimizing clustering wrt our
With the cluster diameter , the definition of the clustering costs is complete . The optimal clustering is defined by demanding three conditions . cluster model is a set of clusters M∶={C 1 , . . . , C n} with ( 1 . ) each C ∈ M is a valid cluster according to our ( 2 . ) complete , disjoint object coverage:äC∈M CO=DB . For all clusterings M′ fulfilling conditions ( 1 . ) & ( 2. ) : costs(M)≤ costs(M′ ) .
( 3 . ) minimal clustering costs : definition .
2
We chose to realize a partitioning clustering approach , since the real world time series data we analyzed were of this kind . However , in future work , we plan to extend our model to handle outliers and overlap between clusters .
With this definition of a clustering , the formalization of our novel clustering model for multivariate time series is complete . The determination of the clustering in Def . 11 is an optimization problem . As for other partitioning clustering approaches , it can be proven that the problem is NP hard [ 3 ] .
IV . ALGORITHM
We present an efficient algorithm that determines an approximate solution wrt our proposed clustering model . Algorithm 1 is the corresponding pseudocode . In the algorithm , each cluster C is represented by a representative
Cp∈ CO . The cluster intervals are specified with respect to
Cp ; that is , wlog we assume that ∆Cp= 0 . The remaining time series COࢨ{Cp} are possibly shifted . The current set of shifts for a cluster C is denoted as ∆C .
Our algorithm is divided into three phases . The first phase in which a superset of is the cluster generation phase , the final clustering is generated . The following refinement phase eliminates redundant clusters from this set . In the last phase , the final assignment of the time series to the clusters takes place . In the next three paragraphs these phases are explained . The representatives , the cluster intervals , and the shifts are constantly updated during the three phases . The last two paragraphs of this section explain how these recomputations are achieved .
Phase 1 : Cluster generation . In this first phase ( l . 219 ) , the objective is to find a set of clusters , or more concretely cluster representatives , corresponding to a superset of the actual clustering result . The cluster representatives are generated iteratively , and during the iterations the already obtained representatives are simultaneously refined . The representatives are maximally different ( l . 4 ) , such that different patterns in the data are represented . At the beginning , the representatives are very rough approximations of the final clusters . While generating more clusters , we simultaneously enlarge the already generated clusters by adding more time series to them ( l . 10 17 ) . The added time series are drawn from the whole databaseDB ( independently representative , ie min∆o∈∆ dC(o , Cp , ∆o , ∆Cp = 0 ) . By for each cluster ) , and the order they are added ( l . 11 ) is based on the normalized shift distance ( Def . 9 ) to the cluster using this distance , we ensure a minimal enlargement of the cluster diameter ( Def . 10 ) and thus low clustering costs ( Def . 8 , 11 ) . By drawing the time series independently for each cluster , we ensure that time series assigned to a wrong cluster can still be added to the correct one .
We want to find overlap free clusters ( Def . 11 ) . Accordingly , we need to avoid clusters whose cluster representatives are based on overlapping object sets . In case of overlap between two clusters , we discard the smaller cluster ( l . 14 ) . Time series are added in batches ( l . 12 ) , and the batch size is constantly increased with each iteration of this phase ( l . 18 ) . This reflects that right after their generation , cluster representatives are unstable and potentially undergo many changes ; adding large batches of time series based on a wrong representative has to be avoided . The initial batch size of 3 was empirically determined .
By adding time series to a cluster , the cluster representative , shifts , and cluster intervals are refined . Since the recomputations of representatives and shifts , however , are costly , recomputations of more established clusters are done in larger intervals . This is done by an exponential function ( l . 15 ) , ensuring a recomputation each time the cluster size
CO has reached a power of 2 ( 2 , 4 , 8 , ) Accordingly , larger and thus more stable cluster are less often refined and a substantial runtime improvement is achieved . Since it is a less expensive operation , the intervals of a cluster are adapted after each batch insert ( l . 17 ) .
The objective of phase 1 is to generate a superset of the final clusters . Termination of our algorithm is guaranteed , since the cluster generation phase is stopped when all cluster candidates have been probed or when the number of clusters in M converges ( l . 19 ) . The latter is reasonable , since with a sufficient number of iterations , the number of cluster candidates showing a difference to the already existing cluster representatives decreases substantially . Accordingly , overlap between established clusters and newly generated ones is highly probable , and new ones will be instantly discarded . Technically , this convergence is assumed when the cumulated increase in clusters over the last 10 iterations reaches 0 . Fig 6 shows the number of clusters and the cumulated increase for the Spoken Arabic Digit dataset . In this example , the cluster generation phase terminates after 38 iterations . The figure shows that if more iterations would be performed , there would be no increase in the cluster number . Phase 2 : Refinement . In many datasets , the obtained representatives are a good reflection of the final clusters . In some datasets , however , multiple clusters are generated that only represent parts of a single cluster . This is also the case in Fig 6 : the dataset has 10 clusters , but 20 are generated . The problem in these datasets is that while there is a high inter cluster dissimilarity , there is also some intra cluster dissimilarity . Dependent on the initial cluster representatives , two or more clusters are generated for single clusters . The objective of the refinement phase is to retain only one cluster instead of the multiple clusters . To detect these cases , we use a similar technique as in the generation phase . Independently for each cluster ( obtained in the generation phase and reset to CO={Cp} , with the most recent representative Cp ) , we add time series fromDB according to the normalized shift distance . If sufficiently many time series are added to the multiple clusters that should be a single cluster , the probability increases that the same time series are added to these clusters ; that is , the difference between the corresponding representatives makes no longer a difference . To obtain an equal weighting , the number of time series added to each cluster is the same ( maxSize ) , and is the maximal obtained cluster size from the cluster generation phase . Afterwards , for each pair of clusters sharing more than half of the objects , one is discarded . In the final assignment phase , the remaining cluster will absorb the time series from the discarded cluster . The refinement phase is completed by a recomputation of the representatives , shifts , and intervals .
Figure 6 . Convergence of main phase on Spoken Arabic Digit dataset . Our algorithm stops after 38 iterations .
3 4
5 6
7 8 9 10 11 12 13 14
15 16
17
18 else
//Iterative cluster generation :
1 M←࢝ ; DBcand←DB ; batchSize= 3 2 repeat //new cluster C with Cp , CO if ( M≠࢝ ) then Cp← maxo∈DBcand{∑Cj∈M L1(o , C j p)} Cp← random(DBcand ) M.add(C ) ; DBcand.remove(Cp ) CO←{Cp} C.adaptIntervals( ) //here : fullspace foreach C j∈ M do //grow clusters C j.orderT imeSeriesCandidates( ) C j.addT imeSeriesCandidatesbatchSize( ) while ∃C i , C j∈ M , i≠ j and C iO∩ C jO > 1 do discardSmallerCluster(C i , C j ) if ( ऄld CO अ− ldऄ CO − batchSizeअ> 0 ) then C j.recomputeShif ts( ) C j.adaptIntervals(85 % ) batchSize++ 19 until (  M  converges ∨DBcand=࢝ ) 20 foreach C∈ M do C.F illU pClusters(maxSize ) ( ∃C i , C j , i≠ j with C iO∩ C jO > C iOࢨ C jO  ) do randomlyDiscardCluster(C i , C j ) 24 foreach C∈ M do C.recomputeShif ts( ) C.adaptIntervals( ) 27 return generateP artitioningClustering(M )
25 26 //Final Object Assignment :
//shift recomputation at exponential intervals of the cluster size :
21 22 while 23
//Refinement phase :
Algorithm 1 : Algorithm for approximate solutions .
Phase 3 : Final assignment . Our approach generates a partitioning clustering . In this last step , the cluster object sets each o∈DB is assigned to the best fitting cluster represenCO are reset . Then , based on the normalized shift distance , tative . Accordingly , a disjoint and complete assignment of time series to clusters is realized .
Shift and Representative Recomputation . ( l . 16,25 ) At cluster initialization and during the growing phase , it is unlikely that the best representative is chosen . The best cluster representative is a time series to which the other time series in the cluster need a minimal shift and that is close to the cluster ’s center . A representative not positioned near the center can absorb time series belonging to other clusters in the growing phase . A representative that does not minimize the shifts can force time series to be shifted near the maximal shift , which decreases the potential to absorb time series in the growing phase that retain compactness in the cluster intervals . Accordingly , based on the distribution of shifts to the current representative , we chose a new representative that potentially minimizes the shift
‐50510152025111213141516171number of clustersiterationsnumber of clusterscumulated increase (10 iter.)stop  {o∈ CO   ∆o =ऄ
 COࢨ{Cp }∑∆o∈∆C ∆oअ} , which contains
1 to this new representative . Concretely , we obtain the set all time series whose shifts are similar to the average shift of the cluster . From this set , we chose the time series that is most similar ( L1 distance ) to the cluster ’s center as a new representative , avoiding a representative that could absorb time series from other clusters . After obtaining the new representative , the shifts of the cluster are recomputed wrt the new representative . In the algorithm , this recomputation is followed by interval recomputation and time series candidate reordering ( based on new shifts & intervals ) .
Interval Recomputation . ( l . 9,17,26 ) When additional time series are added to a cluster or the cluster ’s representative and the corresponding shifts were adapted , the cluster intervals need to be recomputed to better reflect the cluster . We obtain the new cluster intervals by verifying for each tuple of dimension d and point in time t whether 85 % of the valid object pairs at t in d are compact ( cf . Def . 7 ) , enabling the robust handling of deviating values . The compact tuples form the adapted cluster intervals . Cluster intervals that prevent temporal coherence ( cf . Def . 2 ) are discarded ( in case of several possibilities , the larger intervals are kept ) . When a new cluster is generated , it only consists of its representative . In this case , the cluster intervals span all dimensions and all points in time ( l . 9 ) .
V . EXPERIMENTS
We evaluate our method , which is denoted as Robust Time Series Subspace Clustering ( RTSC ) , in comparison to six competing approaches . From fullspace clustering we chose a kMeans that uses statistical features to cope with multivariate time series [ 25 ] , and kMedoid , where we use LCSS [ 24 ] as a distance measure to allow for partial comparison . From 2D clustering , we included Proclus [ 1 ] and MineClus [ 28 ] . From 3D clustering , we included MIC [ 23 ] , and TimeSC [ 14 ] . We also compared to TriCluster [ 29 ] ( from the author ’s webpage ) , but it either delivered no result or an accuracy below 3 % . The used synthetic data was too large for MineClus , but we could include it in nearly all real data experiments ; this issue is discussed in the paragraph on efficiency . The implementation of MIC , which was provided by the authors , did crash in some experiments . We evaluated our approach on five real world data sets , described in the corresponding paragraph . If not stated otherwise , we use the following settings for the synthetic data generator : The dataspace has an extent of[−100,+100 ] ,
Tmax=200 , Dmax=10 , cluster length is 100 , the number of relevant dimensions per cluster is 5 , the number of clusters is 10 , the number of time series per cluster is 50 . The first time series of a cluster is generated by random walk in the cluster intervals . The remaining time series are then added with a variance of 30 . Afterwards , shift and deviating values are added randomly . The maximal shift is 5 , and there are 600 deviating values per cluster ( distributed over the interval points ) . In our approach , we use w=30 for the compactness parameter , corresponding to 15 % of the dataspace ’s extent . The maximal shift is set to δmax=15 ; this choice is discussed in the next paragraph . The parameters of the approaches were tuned to obtain highest quality . The experiments were performed on AMD Opteron servers with 2.2GHz and 256GB RAM . The clustering quality in the experiments on synthetic data was measured with the CE and the E4SC measure , both being extensions of the F1 measure such that subspaces are considered [ 10 ] . Since they showed comparable results , we only included the E4SC measure . The ground truth of the real world data does not specify the data ’s subspaces ; we use the F1 measure [ 10 ] to measure the clustering quality in these experiments . The usage of the F1 measure for clustering is explained in [ 10 ] . All values are averages of three runs . of w is important . However , the figure shows that there is a wide range in which the clustering qualities are stable ( in
Influence of model parameters . We start by analyzing the influence of the two parameters of our clustering model . Fig 7(a ) shows the robustness of our approach wrt compactness parameter w . As previously mentioned , the synthetic data generator uses w= 30 . Obviously , the choice this case : 20≤ w≤ 40 ) , making the parameter determination maximal quality is obtained at δmax= 10 . Afterwards , the easier for the user . Fig 7(b ) shows results for a fixed dataset ( time series are shifted by maximal 10 points in time ) for a varying δmax . As expected , the clustering quality increases if we allow more shift in the clustering model , until the clustering quality is stable for a wide range until it starts to decrease . This illustrates that the choice of δmax is not critical as long as it is not chosen too small . However , the figure shows on the secondary axis that large δmax also incur high runtimes . We chose to use δmax=15 as standard .
Effectiveness on synthetic data . We compare our method to the competing ones with respect to several aspects . Fig 8(a ) shows how the approaches cope with a varying time series length . The plot shows that all approaches show stable clustering qualities . Our new approach outperforms the competing solutions by more than 30 % . From the other subspace clustering results , Proclus shows the best results , followed by TimeSC and MIC . The low performance of MIC is surprising , as MIC is designed for temporal data . The large gap from our approach to Proclus can be explained by the temporal nature of the data which Proclus cannot
( a ) w
( b ) δmax
Figure 7 . Evaluation of cluster model parameters .
0255075100391521273339455157clustering quality [%]03060901200255075100010203040506070runtime [s]clustering quality [%]clustering qualityruntime ( a ) time series length
( b ) relevant dim . per cluster
( a ) number of clusters
( b ) number of dimensions
Figure 8 . Effectiveness on synthetic data .
Figure 10 . Efficiency on synthetic data ( log . scale ) . handle . The low quality obtained by TimeSC is caused by misalignments and deviating values , as we will show later . The fullspace approaches deliver clusterings of low quality . Surprisingly , the LCSS based kMedoid cannot counterbalance the cluster intervals and misalignments in the data . An experiment in which we analyzed an increasing number of dimensions showed very similar ratios .
In Fig 8(b ) , we analyze the influence of varying relevant dimensions per cluster . The results of our method and the other subspace clustering approaches are very similar to the experiment in Fig 8(a ) . As expected , the clustering qualities of the fullspace approaches improve with an increasing number of relevant dimensions . An experiment with a varying number of relevant points in time showed similar results .
The influence of shift in the data is analyzed in Fig 9(a ) . The plot shows that with increasing shift , the clustering quality of TimeSC constantly decreases , while our approach can compensate these misalignments until a shift of 15 is reached . Afterwards , the clustering quality yielded by our approach also decreases . The influence of an increasing number of deviating values per cluster is analyzed in Fig 9(b ) . As for misalignments in Fig 9(a ) , our approach is robust their number grows very large . Both experiments indicate why in the previous experiments TimeSC performs much worse than Proclus : it is very prone to misalignments and deviating values , which occurred in the synthetic data in Fig 8 . The other approaches obtain low quality clusterings as in the previous experiments , caused by the data ’s temporal subspace aspects . to deviating values until
Efficiency on synthetic data . In the experiments in Fig 10 , we increase the number of clusters and dimensions . Both experiments show that our algorithm is much faster than Proclus , MIC , and MineClus . Note that we obtained the values for MineClus on downscaled ( Tmax=50,Dmax=6 , 10 time series per cluster ) versions of the datasets ; the real runtimes are much higher . The obtained clustering quality of Mineclus are below the ones achieved by Proclus . Only TimeSC and the kMeans variant are faster than our algorithm ; their effectiveness , however , is much worse as we demonstrated before .
Real world data . We selected three multivariate datasets that seemed suitable for our approach ( eg , due to misalignments and/or deviating values ; cf . Fig 1 , 2 , 4 ) from the UCI Machine Learning Repository [ 8 ] . The datasets are Spoken
Arabic Digit ( Dmax= 13 , Tmax= 93, DB = 8 , 800 , 10 clusters ) , Japanese vowels ( Dmax= 12 , Tmax=25, DB =640 , 9  DB =164 , 5 clusters ) . Fig 12 shows the results . In all clusters ) , and Robot Execution Failures ( Dmax=6 , Tmax=15 , three datasets , our approach outperforms the competing approaches by a substantial margin , showing the effectiveness of our introduced concepts . Our approach is not limited to multivariate data , as Fig 11 illustrates . We evaluate our approach on the univariate Trace [ 13 ] and the Mallet [ 27 ] datasets . Again , our approach clearly outperforms the competing solutions .
VI . CONCLUSION
We proposed a novel model for temporal subspace clustering , in which time series are individually shifted to compensate for misalignments between them . Our model achieves robustness to deviating values by allowing a certain fraction of deviating values in each relevant point in time . These novel concepts go beyond existing subspace clustering algorithms , which cannot compare different dimensions and only distinguish between relevant and non relevant points in time . The concepts enable our method to discover patterns missed by other approaches . Since our model is NP hard , we proposed an algorithm that yields approximate solutions . It is both effective and efficient , as we showed in the experiments . Acknowledgment . This work has been supported by the UMIC Research Centre , RWTH Aachen University , Germany .
( a ) maximal shift in dataset
( b ) deviating values per cluster
( a ) Trace
( b ) Mallet
Figure 9 .
Influence of shift and deviating values .
Figure 11 . Real world data ( univariate ) .
02550751002004006008001,000clustering quality [%]RTSCProclusTimeSCkMedoidMICkMeans025507510013579clustering quality [%]RTSCProclusTimeSCKmedoidMICkMeans025507510005101520clustering quality [%]RTSCProclusTimeSCkMedoidMICkMeans025507510006001,2001,8002,400clustering quality [%]RTSCProclusTimeSCkMedoidMICkMeans110010,0001,000,0001015202530runtime [s]RTSCProclusTimeSCkMedoidMICkMeans110010,0001,000,0001014182226runtime [s]RTSCProclusTimeSCkMedoidMICkMeans020406080100F1 Measure [%]020406080100F1 Measure [ % ] ( a ) Spoken Arabic Digit
( b ) Japanese Vowels
( c ) Robot Execution Failures
Figure 12 . Real world data ( multivariate ) . ( In some cases , MIC & MineClus did not finish . )
REFERENCES
[ 1 ] C . C . Aggarwal , C . M . Procopiuc , J . L . Wolf , P . S . Yu , and J . S . Park . Fast algorithms for projected clustering . In ACM SIGMOD , pages 61–72 , 1999 .
[ 2 ] R . Agrawal , J . Gehrke , D . Gunopulos , and P . Raghavan . Automatic subspace clustering of high dimensional data for data mining applications . In ACM SIGMOD , pages 94–105 , 1998 .
[ 3 ] D . Aloise , A . Deshpande , P . Hansen , and P . Popat . NPhardness of euclidean sum of squares clustering . Machine Learning ( ML ) , 75(2):245–248 , 2009 .
[ 4 ] F . Alqadah and R . Bhatnagar . An effective algorithm for In ACM mining 3 clusters in vertically partitioned data . CIKM , pages 1103–1112 , 2008 .
[ 5 ] I . Assent , R . Krieger , P . Welter , J . Herbers , and T . Seidl . SubClass : Classification of multidimensional noisy data using subspace clusters . In PAKDD , pages 40–52 . Springer , 2008 . [ 6 ] T . Dasu , D . Swayne , and D . Poole . Grouping Multivariate Time Series : A Case Study . In IEEE ICDM Workshops , pages 25–32 , 2005 .
[ 7 ] H . Ding , G . Trajcevski , P . Scheuermann , X . Wang , and E . J . Keogh . Querying and mining of time series data : experimental comparison of representations and distance measures . PVLDB , 1(2):1542–1552 , 2008 .
[ 8 ] A . Frank and A . Asuncion . UCI machine learning repository . http://archiveicsuciedu/ml , 2010 .
[ 9 ] T . Fu . A review on time series data mining . Engineering
Applications of AI , 24(1):164–181 , 2011 .
[ 10 ] S . G¨unnemann , I . F¨arber , E . M¨uller , I . Assent , and T . Seidl . External evaluation measures for subspace clustering . In ACM CIKM , pages 1363–1372 , 2011 .
[ 11 ] Z . Hu and R . Bhatnagar . Algorithm for discovering lowvariance 3 clusters from real valued datasets . In IEEE ICDM , pages 236–245 , 2010 .
[ 12 ] D . Jiang , J . Pei , M . Ramanathan , C . Tang , and A . Zhang . Mining coherent gene clusters from gene sample time microarray data . In ACM SIGKDD , pages 430–439 , 2004 .
[ 13 ] E . J . Keogh , Q . Zhu , B . Hu , Y . Hao , X . Xi , L . Wei , and C . Ratanamahatana . The UCR time series classification/clustering homepage . , 2011 .
[ 14 ] H . Kremer , S . G¨unnemann , A . Held , and T . Seidl . Mining of temporal coherent subspace clusters in multivariate time series databases . In PAKDD , pages 444–455 . Springer , 2012 .
[ 15 ] H . Kremer , P . Kranen , T . Jansen , T . Seidl , A . Bifet , G . Holmes , and B . Pfahringer . An effective evaluation In ACM measure for clustering on evolving data streams . SIGKDD , pages 868–876 , 2011 .
[ 16 ] H P Kriegel , P . Kr¨oger , and A . Zimek . Clustering highdimensional data : A survey on subspace clustering , patternbased clustering , and correlation clustering . ACM TKDD , 3(1):1–58 , 2009 .
[ 17 ] T . Kudo , E . Maeda , and Y . Matsumoto . An application of boosting to graph classification . NIPS , 17:729–736 , 2004 .
[ 18 ] T . W . Liao . Clustering of time series data a survey . Pattern
Recognition , 38(11):1857–1874 , 2005 .
[ 19 ] T . Oates . Identifying distinctive subsequences in multivariate time series by clustering . In ACM SIGKDD , pages 322–326 , 1999 .
[ 20 ] L . Parsons , E . Haque , and H . Liu . Subspace clustering for high dimensional data : a review . ACM SIGKDD Explorations , 6(1):90–105 , 2004 .
[ 21 ] C . M . Procopiuc , M . Jones , P . K . Agarwal , and T . M . Murali . In
A monte carlo algorithm for fast projective clustering . ACM SIGMOD , pages 418–427 , 2002 .
[ 22 ] C . A . Ratanamahatana and E . Keogh . Three myths about In SIAM SDM , pages dynamic time warping data mining . 506–510 , 2005 .
[ 23 ] K . Sim , Z . Aung , and V . Gopalkrishnan . Discovering corIn related subspace clusters in 3D continuous valued data . IEEE ICDM , pages 471–480 , 2010 .
[ 24 ] M . Vlachos , D . Gunopulos , and G . Kollios . Discovering similar multidimensional trajectories . In IEEE ICDE , pages 673–684 , 2002 .
[ 25 ] X . Wang , A . Wirth , and L . Wang . Structure based statistical In IEEE features and multivariate time series clustering . ICDM , pages 351–360 , 2007 .
[ 26 ] E . H . C . Wu and P . L . H . Yu . Independent component analysis for clustering multivariate time series data . In ADMA , pages 474–482 . Springer , 2005 .
[ 27 ] L . Ye and E . J . Keogh . Time series shapelets : a new primitive for data mining . In ACM SIGKDD , pages 947–956 , 2009 .
[ 28 ] M . L . Yiu and N . Mamoulis . Frequent pattern based iterative projected clustering . In IEEE ICDM , pages 689–692 , 2003 . [ 29 ] L . Zhao and M . J . Zaki . TriCluster : An effective algorithm for mining coherent clusters in 3D microarray data . In ACM SIGMOD , pages 694–705 , 2005 .
020406080100F1 Measure [%]020406080100F1 Measure [%]020406080100F1 Measure [ % ]
