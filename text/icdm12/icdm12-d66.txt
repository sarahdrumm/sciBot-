2012 IEEE 12th International Conference on Data Mining 2012 IEEE 12th International Conference on Data Mining
Analysis of Temporal High Dimensional Gene Expression Data for Identifying
Informative Biomarker Candidates
Department of Computer and Information Science
Center for Data Analytics and Biomedical Informatics
Department of Computer and Information Science
Center for Data Analytics and Biomedical Informatics
Qiang Lou
Temple University Philadelphia , USA qianglou@temple.edu
Zoran Obradovic
Temple University Philadelphia , USA zoranobradovic@templeedu is based on well established machine
Abstract—Identifying informative biomarkers from a large pool of candidates is the key step for accurate prediction of an individual ’s health status . In clinical applications traditional static feature selection methods that flatten the temporal data cannot be directly applied since the patient ’s observed clinical condition is a temporal multivariate time series where different variables can capture various stages of temporal change in the patient ’s health status . In this study , in order to identify informative genes in temporal microarray data , a margin based feature selection filter is proposed . The proposed learning method techniques without any assumptions about the data distribution . The objective function of temporal margin based feature selection is defined to maximize each subject's temporal margin in its own relevant subspace . In the objective function , the uncertainty in calculating nearest neighbors is taken into account by considering the change in feature weights in each iteration . A fixed point gradient descent method is proposed to solve the formulated objective function . The experimental results on both synthetic and real data provide evidence that the proposed method can identify more informative features than the alternatives that flatten the temporal data in advance .
Keywords high dimensional ; temporal data ; selection ; margin ; multivariate time series data feature
I .
INTRODUCTION
The major challenges in analyzing microarray data is dealing with small sample high dimensional data where the number of biomarkers used as features is typically much larger than the number of labeled subjects . Performing feature selection methods as a preprocessing step to identify informative biomarkers is a common way to address this problem . It is often followed by a classification method on selected genes to predict the health status of an individual .
However , there is often interest in the analysis of dynamic biological processes with data from DNA gene expression microarray chips instead of analyzing static gene expression data . In order to predict an individual's health status , it is very helpful to analyze such high dimensional gene expression data that varies with time . Besides the traditional challenge of curse of dimensionality , another challenge of analyzing dynamic biological processes is that
1550 4786/12 $26.00 © 2012 IEEE 1550 4786/12 $26.00 © 2012 IEEE DOI 101109/ICDM201292 DOI 101109/ICDM201292
380 996 the data gathered is temporal . Therefore , the data records for each individual are multivariate time series . However , traditional feature selection methods cannot handle such multivariate time series data . The most straightforward method is to apply some techniques to flatten the temporal data , and then perform traditional feature selection methods in the flattened data .
In this study , we proposed a feature selection filter that can directly select informative features from temporal highdimensional biomarkers . We defined a temporal margin for each subject based on a measure of distance between two multivariate time series data from two different subjects . The objective function of the proposed selection method is to maximize each subject's temporal margin in its own relevant subspace . We also take into account the uncertainty in calculating nearest neighbors because the feature weights change in each iteration , and it is hard to calculate nearest neighbors for a multivariate time series data . We applied fixed point gradient ascent to solve the optimization problem and get the optimal weight for each gene . Genes with large weights are selected to build the prediction model to predict the health status of each individual . The experimental results show that our method outperforms the alternatives , which apply traditional feature selection methods after flattening the temporal multivariate gene expression data . Convergence theorem of the proposed method is also presented .
II . RELATED WORK is
Feature selection methods can be broadly categorized into filtering models [ 1 ] and wrapper models [ 2 ] . Filtering methods separate the feature selection from the learning process , whereas wrapper methods combine them . The main drawback of wrapper methods their computational inefficiency .
There are three widely used kinds of filtering methods . In [ 3 , 5 ] a margin based method is proposed as a featureweighting algorithm that is a new interpretation of a RELIEF based method [ 4 ] . The method in [ 5 ] is an online algorithm that solves a convex optimization problem with a margin based objective function .
Markov Blanket based methods [ 1 , 6 , 7 ] perform feature selection by searching an optimal set of features using Markov Blanket approximation . The method proposed at [ 6 ] is approximates the Markov Blanket by applying a GrowShrink process and then removing the feature whose Markov Blanket can be found in the rest of features . Method [ 7 ] searches Markov Blanket after learning the structure of the Bayesian network .
Dependence estimation based methods use the HilbertSchmidt Independence Criterion as a measure of dependence between the features and the labels [ 8 ] . The key idea in this method that good features should maximize such dependence . However , all these methods assume that the data is static without varying on time . They cannot be applied in temporal gene expression data that is the main problem of this study .
Several feature learning methods [ 9 , 10 ] have recently been proposed to handle the temporal gene expression data , without imputing missing values in advance . However , those two methods are different from the proposed method in this study , since those methods treat the records for an individual at different time steps independently , which will result in loss of temporal information among the data . All those works project the data to another space and learn features from the new space ( factors or principal component ) . Those methods are actually methods for dimension reduction , rather than feature selection . Due to this , we will not compare our method with them in this study .
The method proposed in this study extends our feature ranking method addressing the similar problem [ 13 ] . Previously , we measured nearest neighbors in the original space and did not update them while updating the feature weights . Both aspects are generalized here . In addition , our method proposed in [ 13 ] directly applies gradient descent optimization , which cannot guarantee convergence to a global solution , whereas a globally optimized solution is guaranteed when using the methods proposed here .
III . PROPOSED METHOD } i
× iTn
)(r
Let i YXD i
}1,1{ −∈iY
, = N ,,1 ×ℜ∈X Ti n i be the data set with N = ±×ℜ⊂ { 1 represents n observed biomarkers individuals . ( eg gene expression data ) for individual i measured at Ti represents the class label ( eg health time steps . iX be the rth column of Xi that status ) for individual i . Let corresponds n biomarkers measured at time tr .
We will first define the measure of distance between multivariate time series data of two subjects , and then present the temporal margin based on the distance measure as well as the objective function of proposed feature selection method the corresponding optimization problem . A . Measure Distance Among Multivariate Time Series algorithm solving and for
Given Xi , and Xj corresponding the observed biomarkers measured at different time steps for individual i and individual j , respectively , the distance ( we call Temporal distance , represented as Tdist ) between two multivariate time series Xi and Xj is defined as : to
(
XX
, i
=
) j
1 × TT i
T j
T i
¦¦ j r
= 1
= 1 s d
(
XX r )( i
,
( 1 ) s )( j
)
Tdist
)(r where Ti and Tj are the number of time steps of individual i and individual j , respectively ; iX is the vector consists of biomarkers measured at time steps r for individual i ; iX is the vector of biomarkers measured at time steps s for individual j ; for any two vectors v and z , function d(v , z ) can be any kind of distance function . To keep the notation simple , we defined as the Manhattan distance between two vectors . B . Maximize Temporal Margin With Uncertainty zvd ),(
)(s
)
(
(
=
X
X
Tdist ( nearmiss
Given an instance , the margin of a hypothesis is the distance between the hypothesis and the closest hypothesis that assigns an alternative label [ 4 ] . For a given instance Xi , we find two nearest neighbors for Xi , one with the same class label ( called nearhit ) , and the other with different class label ( called nearmiss ) . The hypothesis margin of a given instance Xi in data set D is defined as :
1 L D i 2
In margin based feature selection , we scale the feature by assigning a non negative weight vector w , and then choose the features with large weights that maximize the margin . One idea is to then calculate the margin in weighted feature space rather than the original feature space , since the nearest neighbor in the original feature space can be completely different from the one in the weighted feature space . Therefore , we define the instance margin for each instance Xi from D in a weighted feature space as :
( 2 )
, i − nearhit
Tdist
) ) )
X
X
X
) )
(
(
(
, i i i d
(
X r ( ) i
, nearmiss
(
X i
) s ( )
| w
)
−
1 × T T 2 i
T i
T H
¦¦
H r
= 1
= 1 s d
(
X r ( ) i
, nearhit
(
X i
) s ( )
| w
)
( 4 )
= i w ȕ T where TM and TH are the number of time steps of nearmiss(Xi ) and nearhit(Xi ) , respectively ; for each instance Xi , the corresponding T ¦¦ M iȕ is defined as : nearmiss
X
X
=
−
ȕ s ( )
(
)
T i
|
| i i r ( ) i
1 × T T 2 i
M r
= 1
= 1 s
( 5 )
−
1 × T T 2 i
X ( | ⋅ is the element wise absolute operator . | nearhit r ( ) i
X
H r
= 1
= 1
| s
T i
T H
¦¦
− where
381997 s ( )
) i
|
X i
, nearmiss ( wX
| )
) i nearhit
Tdist
(
X
, i
( wX
| ) i
( 3 )
) )
ρ D
|
(
) wX i
=
1 2
Tdist ( ( − which is equivalent to : T i ρ D
X w
=
¦¦
T M
(
)
| i
1 × T T 2 i
M r
= 1
= 1 s
One possible problem may exist in the current definition of instance margin . The nearest neighbors we calculate for each instance might not be the real nearest neighbors , since we calculate the nearest neighbor for each instance in the weighted space that changes each time when the weights get updated . To solve this problem , we take into account the uncertainty of calculating nearest neighbors when calculating instance margins . We calculate the uncertainty of each instance being the nearest neighbor of xn . The uncertainty is evaluated by standard Gaussian kernel estimation with kernel width of ı . Specifically , we define the uncertainty that an instance xi with the same class label as xn can be the nearest hit neighbor of xn as :
U nearhit
( x x w i
, n
|
=
) d
(
X r ( ) i
,(
X
) n s ( )
| w
) /
σ ) d
(
X r ( ) j
,(
X n s ( )
)
| w
) /
σ )
¦
T T i M
¦¦
M r exp( exp(
1 × T T 2 i 1 × T T 2 = M i r 1 ≠ ≤ ≤ i N i n y , , i ≤ ≤ = j N y y ,
= = s 1 1 T T i M
¦¦
= s 1 = y n j where 1 and 1 ( 6 ) Similarly , the uncertainty that an instance xi with a different class label from xn can be the nearest miss neighbor of xn is defined as : n j
U nearmiss
( x x w i
, n
|
=
) d
(
X r ( ) i
,(
X
) n s ( )
| w
) /
σ ) d
(
X r ( ) j
,(
X n s ( )
)
| w
) /
σ )
T T i H
¦¦
= = s 1 1 T T i H
¦¦
= 1
= 1
H r
¦ exp(
1 × T T 2 i 1 × T T 2 H i ≠ ≤ ≤ i N y y , i ≠ ≤ ≤ j N y y , exp( j r s n n j where 1 and 1 ( 7 ) Please note that distance in equations ( 6 ) and ( 7 ) denotes the distance between xn and xi in weighted space determined by weight vector w . Finally , by checking the uncertainty of each instance to be the nearest neighbor of xn , we define our final temporal margin with uncertainty as the expectation of the instance margin of xn , which can be written as : x w w E E ( ρ n D where
=
)
T
|
β n
− r ( ) i nearmiss
(
X s ( )
) i
| n n
|
|
|
⋅
⋅
,
, y n n
=
)
(
)
(
−
=
U
U
X nearhit
E
β n nearmiss x x w i x x w i
¦ i when y
, i
≠ y ¦ i when y , i
T T i M ¦¦ = = s r 1 1 T T i H ¦¦ | = = r s 1 1 As we mentioned before , our r ( ) ( 8 ) i temporal margin incorporates the uncertainty in calculating two nearest neighbors ( Eȕn ) .
1 × T T 2 i M 1 × T T 2 H i
We already define the instance margin for each subject Xn . Therefore , we can define the temporal margin of the entire data D that has N subjects as the sum of all instance margins , which can be written as : nearhit
X
X s ( )
−
(
)
| i
|
( w
0
≥ w
Eρ D
N ¦ = n 1 x w n
) subject to
( 10 ) max
We followed logistic regression formulation framework . In order to avoid huge values in weight vector w , we add a . Therefore , we can rewrite normalization condition the optimization problem as :
|| θ≤w
||
1 min w
N ¦ = n 1 log(1 exp(
+
−
Eρ D
( x w n
|
) ) subject to w
≥
0,|| w
≤
|| 1
θ ( 11 )
The above formulation is called nonnegative garrote . We can rewrite the formulation as :
)
0
≥
+
−
+ w w
|| 1 wE
N ¦ = n 1 log(1 exp(
β λ || n
( 12 ) min w subject to
For each solution to ( 12 ) , there is a parameter ș , corresponding to the obtained Ȝ in ( 12 ) , which gives the same solution in ( 11 ) . Formulation ( 12 ) is actually the 1A regularization . The benefits of optimization problem with 1A penalty have been well studied [ 12 ] and it is adding the 1A penalty can effectively handle sparse data shown that the and huge amounts of irrelevant features . C . Feature Selection Algorithm
In this section we will introduce our feature selection method , which solves the optimization problem introduced in Section 32 As we can see from ( 12 ) , the optimization problem is convex if Eȕn is fixed . Fox a fixed Eȕn , ( 12 ) is a constrained convex optimization problem . However , it cannot be directly solved by gradient descent because of the nonnegative constraints on w . To handle this problem , we introduce a mapping function : f
: w
→ u
, where w i ( )
= u
2 i ( ) ,
∀ = i
1 , 2 ,
M
( 13 )
Therefore , the formulation ( 12 ) can be rewritten as : min w
By taking the derivative with respect to u , we obtain the
( 14 )
β λ || n log(1 exp(
N ¦ = n 1 wE
|| 2 u
+
−
+
)
2 following updated rule for u :
382998
ρ D
| w
=
E
ρ D
( wx n
|
)
( 9 )
( new
) u
( old
)
= u
−
N
¦ n
= 1
The feature weights can be learned by solving an optimization problem that maximizes the uncertainty margin of data D . This optimization problem can be represented as : exp(
−
2 u E j
N ¦ αλ = n 1 ( N ¦ + n
−
1
= 1
M ¦ j
= 1 M ¦ j
= 1 exp(
−
)
β n
⊗ u
)
2 u E j
)
β n
( 15 ) where Į is learning rate , ⊗ is the Hadamard product , and nβE is defined as :
=
E
β n
¦ i when y
, i
− n
≠ y ¦ i when y , i
U nearmiss
( x x w i n
,
|
⋅
)
X
− r ( ) i nearmiss
(
X s ( )
) i
|
U nearhit
(
= y n x x w i n
,
|
|
X r ( ) i
− nearhit
(
X s ( )
) i
|
1 × T T 2 i M 1 × T T 2 H i
⋅
|
T T i M ¦¦ = = s r 1 1 T T i H ¦¦ = = s r 1 1
)
However , Eȕn is determined by w so that ( 14 ) is not a convex problem . We use a fixed point EM algorithm to find the optimal w . The proposed algorithm for Margin based Feature Selection in Temporal Microarray data ( we call it MSTM ) is shown in Table 1 .
The MSTM algorithm starts by initializing the values of w to be 1 . With such initialization , we can estimate the sn and Eȕn for each instance xn . Then , in each iteration , the weights vector w is updated by solving the optimization problem ( 14 ) with estimated values of sn and Eȕn in the previous iteration . We repeat the iteration until convergence . The MSTM algorithm requires pre defined kernel width ı and a regularization parameter Ȝ . We applied cross validation to select the values of parameters .
TABLE I .
MSTM FEATURE SELECTION METHOD
Input : data set D = {(xi , yi)}i=1,N kernel width ı regularization parameter Ȝ Output : feature weights w Initialization : set w(0)=1 , t = 1 Do Calculate Eȕn Update u(t ) using updated rule in equationfi(15 ) Update w(t ) using u(t ) using equation ( 13 ) fit = t + 1 Until convergence
To prove convergence of MSTM algorithm we will use
( t ) using w(t 1 ) and equation ( 8 ) the following theorem .
1
1 r n −
+ ≤ ) 1
Theorem 1 ( Contraction Mapping Theorem ) . Let T : XĺX be a contraction mapping on a complete metric space X . The sequence generated by xn=T(xn 1 ) for n = 1 , 2,3,…converges to unique limit x* , where x* is the fixed point of T ( T(x*)=x* ) . In other words , there is a nonnegative real number r<1 such that d x x ( ) , 0 d x x ( * , n r Proof : See [ 12 ] . Based on this theorem we prove the following : Theorem 2 . There exists ı0 such that for any ı > ı0 the MSTM algorithm converges to a fixed unique solution w* when initial feature weights w(0 ) are nonnegative .
The proof is following a similar schema as in [ 3 ] . Details are omitted for lack of space . The complexity of the MSTM algorithm is O(TN2M ) where T is the total number of iterations , N is the number of instances , and M is the number of features . Our experimental results show that the algorithm converges in a small number of iterations ( less than 40 ) . Therefore , the complexity of MSTM algorithm in real application is about O(N2M ) . Note that the MSTM algorithm is linear to the number of features , so the proposed method can handle a huge number of features .
IV . EXPERIMENTS
To characterize the proposed algorithm , we conducted large scale experiments on both synthetic and 2 real flu data sets [ 9 , 10 ] . All experiments of this study were performed on a PC with 3 GB of memory . We compared our proposed MSTM algorithm in temporal gene expression data with four traditional feature selection methods ( the method proposed in [ 12 ] that we call BAHSIC , SIMBA [ 4 ] , Relief [ 11 ] and FST [ 13 ] ) after flattening temporal multivariate data into one single matrix .
For the prediction method , we apply Nearest Neighbor classifier on all features and select features by different feature selection methods . We compare results on both synthetic data and real data . A . Results on Synthetic Data t 10
)
We generate synthetic data simulating 20 subjects . Each subject has 50 dimensional records at 20 different time steps . Each subject i is generated according the following process . We first generate 50 dimensional random data Xi for subject i at time step 1 . Label Yi is complete decided by the first four . We then features following generate records for subject i at other time steps using is the Gaussian formula :
, where
ε+ iY
X
X
X
X
∨
∧
∨
=
=
(
)
)
( i 1
4
2 i
3 i i
X
+ t )1( i
X
+ t )1( i
,0ȃ(~ noise that is also a function of time steps .
ε
The results on synthetic data are shown in Figure 1 and Table II . Figure 1 shows the feature weights for each feature learned by our proposed MSTM and three alternatives . It clearly shows that our method assigns significantly larger weights to the first four features used to decide the Label than to most other features . Moreover , our method applied L1 regularization so that the feature weights learned are sparse ( most of feature weights are tend to zero ) .
Table II shows the results comparing our method to three alternatives ( three alternatives are applied after flattening the temporal data ) . We choose the top 4 features selected by each method , and compare the number of features correctly selected among these top 4 features . Top 4 features means 4 features with biggest feature weight . We can see from Table II that our method included all 4 informative features in the top 4 features , whereas SIMBA hits 3 , FST and Relief hits only 2 . Our method outperforms alternatives on this synthetic data . We didn't apply BAHSIC on the synthetic data because BAHSIC is not feature weighting method .
TABLE II .
NUMBER OF CORRECTLY SELECTED FEATURES AMONG
TOP 4 FEATURES FST 2
Relief
2
Simba MSTM
3
4
# correct features
383999 s t i h g e W e r u t a e F s t i h g e W e r u t a e F
0.06
0.04
0.02
0
0
0.06
0.04
0.02
0
0
RELIEF
FST s t i h g e W e r u t a e F
10
20
30
40
50
Index of Feature
SIMBA s t i h g e W e r u
0.06
0.04
0.02
0
0
0.2
0.15
0.1
10
20
30
40
50
Index of Feature
MSTM t a e F
0.05
0
0
10
20
30
40
50
Index of Feature
10
20
30
40
50
Index of Feature
Figure 1 . Feature weights learned on synthetic data .
B . Results on Two Real Flu Datasets
We first describe the data sets [ 9 , 10 ] used in this section . Two challenge studies were performed with two groups of healthy human volunteers . One of these groups was exposed to H3N2 virus , and the other was exposed to H1N1 virus . The H1N1 and H3N2 studies were performed independently , with different subjects . In summary , H3N2 data consists of records for 17 subjects collected at 16 different time steps . H1N1 data consists of records for 24 subjects collected at 16 different time steps . For H3N2 and H1N1 gene expression data , the same 12,023 genes are considered for analysis for each subject at each time step .
For the feature selection and learning prediction process , we apply leave one out schema because of the low number of subjects in both data sets . To avoid overfitting , in each iteration of leave one out schema , the training set is used to perform feature selection and learn the prediction model , and the one test subject is only touched in prediction process . We applied a Nearest Neighbor classifier to build the prediction model because it is easy to perform on multivariate temporal gene expression data sets .
The results on H3N3 and H1N1 data sets are listed in Table III and Table IV . Since the H1N1 data set is imbalanced data ( 8 negative subjects and 16 positive subjects ) . We report sensitivity , specificity , and balanced accuracy to evaluate the results from all methods . The
3841000 is balanced accuracy the average of sensitivity and specificity . Balanced accuracy tends to decrease the chance that the classifier takes advantage of an imbalanced test set .
The classification results on H3N3 and H1N1 are shown at the top sub table of Table III and Table IV . We repeat experiments 20 times and report the mean ± std values for classification results ( sensitivity , specificity , and balanced accuracy ) . We can see there that the accuracy of the predictor built on the features selected by our proposed MSTM method outperforms all alternatives including the predictor built on all features . This proves that our MSTM method selects more accurate features .
Number of Selected features . The number of selected features from different methods is shown at the bottom subtable of Table III and Table IV . Our MSTM method can automatically select the optimal feature set by eliminating features with weight zero . MSTM selected 55 genes out of 12,023 features on H3N3 , and 27 genes out of 12,023 features on H1N1 . However , FST , Simba , BAHSIC and Relief cannot select the optimal feature set automatically , since they are all feature ranking methods . We report the number of top features where we get the highest accuracy for these three methods . The number of selected features is listed at the bottom of Table III and Table IV . Our method forces the weights of most irrelevant features to be zero , and it therefore selects much fewer features than the alternatives .
TABLE III . RESULTS ON H3N2 DATA
Sensitivity
Specificity
All feature 0.667 ± 0
BAHSIC
Relief
Simba
FST
0.735 ± 0.202
0.875 ± 0.063
0.882 ± 0.073
1.000 ± 0
MSTM 1.000 ± 0
0.811 ± 0
0.582 ± 0.056
0.778 ± 0.118
0.763 ± 0.053
0.889 ± 0.130
0.922 ± 0.150
Balanced_Accuracy
0.771 ± 0
0.659 ± 0.129
0.826 ± 0.065
0.823 ± 0.063
0.944 ± 0.064
0.961 ± 0.084
( a ) Classification Accuracy ( mean ± std )
BAHSIC
217
Relief 154
Simba 135
FST 50
MSTM
55
( b ) Number of Selected Features
TABLE IV . RESULTS ON H1N1 DATA
Sensitivity
Specificity
Balanced_Accuracy
All feature 0.938 ± 0
0.125 ± 0
0.531 ± 0
BAHSIC
0.806 ± 0.052
Relief 1.000 ± 0
Simba
FST
0.948 ± 0.003
1.000 ± 0
0.405 ± 0.131
0.500 ± 0.132
0.605 ± 0.163
0.750 ± 0.151
0.606 ± 0.092
0.750 ± 0.074
0.777 ± 0.085
0.875 ± 0.101
MSTM 1.000 ± 0
0.801 ± 0.131 0.901 ± 0.065
( a ) Classification Accuracy ( mean ± std )
BAHSIC
346
Relief 121
Simba 141
FST 43
MSTM
27
( b ) Number of Selected Features
V . CONCLUSION
[ 5 ]
Sun , Y . , Todorovic , S . and Goodison , S . 2009 . Local Learning Based Feature Selection for High Dimensional Data Analysis . IEEE Transactions on Pattern Analysis and Machine Learning
[ 6 ] Margaritis , D . and Thrun , S . "Bayesian Network Induction via Local
[ 7 ]
[ 8 ]
Neighborhoods" , In Neural Information Processing System , 1999 . Shen , J . , Li , L . and Wong , W . "Markov Blanket Feature Selection for Support Vector Machine" In Proc . of AAAI Conference on Artificial Intelligence ( AAAI 08 ) . 2008 , Song , L , Smola , A , Gretton , A . and Borgwardt , KL "A dependence maximization view of clustering" , In Proceedings of the 24th International Conference on Machine Learning , Corvallis , OR , 2007 .
We proposed a margin based feature selection filter that can directly select a few informative genes from temporal high dimensional gene expressions . For each subject , we define a temporal margin based on a measure of distance between two multivariate time series from other subjects . We take into account the uncertainty in calculating nearest neighbors by considering the updated weights in each iteration . The objective function of the proposed selection method is to maximize each subject's temporal margin in its own relevant subspace . The optimal weight for each feature is learned by solving this optimization problem .
ACKNOWLEDGMENT
This work was supported in part by DARPA grant DARPA N66001 11 1 4183 negotiated by SSC Pacific ( to ZO ) .
REFERENCES
[ 1 ] Yu , L . , and Liu , H . 2003 . "Feature Selection for High dimensional In 20th
Data , A Fast Correlation based Filter Solution." International Conference on Machine Learning , pp . 856 863
[ 2 ] Kohave , R . and John , G . 1997 . "Wrappers for Feature Subset
Selection." Artificial Intelligence , vol 1 2 , pp . 273 324
[ 3 ] Lou , Q , and Obradovic , Z . “ Margin Based Feature Selection in Incomplete Data , ” . In Proc . Of 26th AAAI Conference on Artificial Intelligence ( AAAI 12 ) . July 2012 , Toronto , Ontario , Canada
[ 4 ] Gilad Bachrach , R . , Freund , Y . , Bartlett , P . L . and Lee , W . S . 2004 . Margin Based Feature Selecgtion theory and algorithms . In 21st International Conference on Machine Learning , pp . 43 50
3851001
[ 9 ] Chen , B . , Chen , M . , Paisley , J . , Zass , A . , Woods , C . , Ginsburg , GS,Lucas , J . , Dunson , D . , and Carin , L . “ Bayesian Inference of the Number of Factors in Gene expression Analysis : Application to Human Virus Challenge Studies , ” , BMC bioinformatics , 2010 .
[ 10 ] Chen , M . , Zaas , A . , Woods , C . , Ginsburg , GS , Lucas , J . , Dunson , D . , and Carin , L . “ Predicting Viral Infection From HighDimensional Biomarkers Trajectories ” Journal of the American Statistical Association , vol . 106 , No . 496 . December 2011 .
[ 11 ] Sun , J . and Li , J Iterative RELIEF for Feature Weighting . In 23rd
International Conference on Machine Learning , 2006 , Pittsb
[ 12 ] Song , L . , Smola , A . , Gretton , A . Borgwardt , KM , and Bedo , J . "Supervised Feature Selection via Dependence Estimation" , International Conference on Machine Learning . 2007 , pp . 856 863 .
[ 13 ] Lou , Q and Obradovic , Z . "Prediciting Viral Infection by Selecting Informative Biomarkers From Temporal High Dimensional Gene Expression Data" IEEE International Confenece on Bioinformatics and Biomedicine , October 2012 , Philadelphia , USA
