Bounded Matrix Low Rank Approximation
Ramakrishnan Kannan , Mariya Ishteva , Haesun Park School of Computational Science and Engineering
Georgia Institute of Technology
Email:rkannan@gatech.edu and mishteva,hpark@ccgatechedu
Abstract—Matrix lower rank approximations such as nonnegative matrix factorization ( NMF ) have been successfully used to solve many data mining tasks . In this paper , we propose a new matrix lower rank approximation called Bounded Matrix Low Rank Approximation ( BMA ) which imposes a lower and an upper bound on every element of a lower rank matrix that best approximates a given matrix with missing elements . This new approximation models many real world problems , such as recommender systems , and performs better than other methods , such as singular value decompositions ( SVD ) or NMF . We present an efficient algorithm to solve BMA based on coordinate descent method . BMA is different from NMF as it imposes bounds on the approximation itself rather than on each of the low rank factors . We show that our algorithm is scalable for large matrices with missing elements on multi core systems with low memory . We present substantial experimental results illustrating that the proposed method outperforms the state of the art algorithms for recommender systems such as Stochastic Gradient Descent , Alternating Least Squares with regularization , SVD++ , Bias SVD on real world data sets such as Jester , Movielens , Book crossing , Online dating and Netflix . Keywords low rank approximation , recommender systems , bound constraints , matrix factorization , block coordinate descent method , scalable algorithm , block
I . MOTIVATION
In a matrix low rank approximation , given a matrix R ∈ Rn×m , and a lower rank k < rank(R ) , we find two matrices P ∈ Rn×k and Q ∈ Rk×m such that R is well approximated by PQ , ie , R ≈ PQ . Low rank approximations vary depending on the constraints imposed on the factors as well as the measure for the difference between R and PQ . Low rank approximation draws huge interest in the data mining and machine learning community , for it ’s ability to address many foundational challenges in this area . A few prominent techniques of machine learning that use low rank approximation are principal component analysis , factor analysis , latent semantic analysis , non negative matrix factorization , etc .
One of the most important low rank approximation is based on singular value decompositions ( SVD ) [ 9 ] . Low rank approximation using SVD has many applications . For example , an image can be compressed by taking the low row rank approximation of its matrix representation using SVD . Similarly , in text mining – latent semantic indexing , is for document retrieval/dimensionality reduction of a termdocument matrix using SVD [ 6 ] . The other applications include event detection in streaming data , visualization of document corpus etc .
Over the last decade , NMF has emerged as another important low rank approximation technique , where the low rank factor matrices are constrained to have only non negative elements . It has received enormous attention and has been successfully applied to a broad range of important problems in areas including text mining , computer vision , community detection in social networks , visualization , bioinformatics etc [ 18][11 ] .
In this paper , we propose a new type of low rank approximation where the elements of the low rank matrix are bounded – that is , its elements are within a given range which we call as Bounded Matrix Low Rank Approximation(BMA ) . BMA is different from NMF in that it imposes both upper and lower bounds on its product PQ rather than each of the low rank factors P and Q . The goal is to obtain a lower rank approximation PQ of a given input matrix R , where the elements of PQ and R are bounded . There are various real world applications that benefit from this approximation – a well known application being the Recommender System . Currently , the factorization techniques for recommender system , do not use the information that ratings always belong to a range , during the factorization process . Instead , the range is only used to truncate the values of the approximated low rank matrix . In case of recommender system , the input matrix apart from being range bound , also has many missing elements . In general , approximately only 1 or 2 % of all matrix elements are known .
We are designing a Bounded Matrix Low Rank Approximation ( BMA ) that imposes bounds on a low rank matrix that is the best approximate for an input matrix with missing elements.1 The design considerations are – ( 1 ) Simple implementation ( 2 ) Scalable to work with any large matrices and ( 3 ) Easy parameter tuning with no hyper parameters .
Formally , the BMA problem for an input matrix R is defined as2
1Throughout this paper , we might use rows and users , columns and items , reduced rank and hidden latent features , values and ratings interchangeably and appropriately chosen for better understanding of the idea .
2Here afterwards , if the inequality is between a vector/matrix and scalar , every element in the vector/matrix should satisfy the inequality against the scalar . min P,Q subject to
M · ∗(R − PQ)2
F rmin ≤ PQ ≤ rmax .
( 1 )
In the case of input matrix with missing elements , the low rank matrix is approximated only against the known elements of the input matrix . Hence , during error computation the filter matrix M , includes only the corresponding elements of the low rank PQ for which the values are known . Thus , M has ‘1’ everywhere for input matrix R with all known elements . However , in the case of recommender system , the matrix M has zero for each of the missing elements of R .
Traditionally , regularization is used to control the low rank factors P and Q from taking larger values . However , this does not guarantee the value of the product PQ to be in the given range . We also experimentally show introducing the bounds on the product of PQ outperforms the low rank approximation algorithms with regularization .
We use Block Coordinate Descent ( BCD ) framework [ 2 ] to solve the Problem ( 1 ) as it satisfies the design considerations discussed above . Also , the BCD method provides fine grained control of choosing every element in the low rank factors .
II . NOTATIONS
In this paper , we choose notations consistent with those in the machine learning literature . A lowercase/uppercase such as x or X , is used to denote a scalar ; a boldface lowercase letter , such as x , is used to denote a vector ; a boldface uppercase letter , such as X , is used to denote a matrix . Indices typically start from 1 . When a matrix X j denotes its jth is given , xi denotes its ith column , y row and xij or X(i , j ) denote its ( i , j)th element . For a vector i , x(i ) means vector i indexes into the elements of vector x . That is , for x = [ 1 , 4 , 7 , 8 , 10 ] and i = [ 1 , 3 , 5 ] , x(i ) = [ 1 , 7 , 10 ] . We have also borrowed certain notations from matrix manipulation scripts such as Matlab/Octave . For example , the max(x ) returns the maximal element x ∈ x and max(X ) returns a vector of maximal elements from each column x ∈ X .
For reader ’s convenience , the Table I is the summary of the notations used in the paper .
III . RELATED WORK
In the literature , two important low rank approximations are NMF and SVD . In this paper , we introduce BMA and its application in recommender systems . Initially , for the input matrix R ∈ R+ , we explored the possibility of modelling this problem as a non negative matrix factorization using BCD . Block coordinate descent has been a commonly used to solve NMF . We briefly survey and explain NMF using BCD framework in Section IV A .
Ratings matrix with missing ratings as 0 and ratings bounded within [ rmin , rmax ]
R ∈ Rn×m M ∈ {0 , 1}n×m Indicator matrix with missing ratings as 0 n m k P ∈ Rn×k Q ∈ Rk×m px ∈ Rn×1 x ∈ R1×m q rmax > 1 rmin u i ·∗ ·/ A( : , i ) A(i , : ) β memsize(v )
Number of users Number of items Reduced rank User feature matrix Item feature matrix x th column vector of P = ( p1,··· , pk ) x th row vector of Q = ( q1,··· , qk ) Maximal rating/upper bound Minimal rating/lower bound A user An item Element wise matrix multiplication Element wise matrix division ith column of the matrix A ith row of the matrix A Data structure in memory factor Returns the approximate memory of a variable v =number of elements in v*size of each element*β Mean of all known ratings in R Bias of all users u Bias of all items i Table I : Notations
µ g ∈ Rn h ∈ Rm
In this section , we explain the important milestones in matrix factorization for recommender systems . Most of these milestones are achieved because of the Netflix competition ( http://wwwnetflixprizecom/ ) where the winners were awarded 1 million US Dollars as grand prize .
Funk [ 7 ] first proposed matrix factorization for recommender system based on SVD and is commonly called as the Stochastic Gradient Descent ( SGD ) . Paterek [ 23 ] improved SGD as a combination of the baseline estimate and matrix factorization . Koren , the member of winning team of Netflix prize improved the results with his remarkable contributions in this area . Koren [ 15 ] proposed a baseline estimate based on mean rating , user–movie bias , combined with matrix factorization and called it Bias SVD . In SVD++ [ 15 ] , he extends this Bias SVD with implicit ratings and considered only the relevant neighbourhood items during matrix factorization . The Netflix dataset also provided the time of rating . However most of the techniques did not include time in their model . Koren [ 16 ] proposes time svd++ , where he extended his previous SVD++ model to include the time . So far , all the techniques discussed here , in matrix factorization for recommender systems are based on SVD and used gradient descent to solve the problem . Alternatively , Zhou etal , [ 26 ] used alternating least squares with regularization ( ALSWR ) . Apart from these directions , there had been other approaches such as Bayesian tensor factorization [ 25 ] , Bayesian probabilistic modelling [ 24 ] , graphical modelling of the recommender system problem [ 21 ] and weighted low rank approximation with zero weights for the missing values [ 22 ] . A detailed survey and overview of matrix factorization for recommender systems is discussed in [ 17 ] .
A . Our Contributions
Given the above background , we highlight the contributions of this paper . In this paper , we propose a novel matrix lower rank approximation called Bounded Matrix Low Rank Approximation ( BMA ) which imposes a lower and an upper bound on every element of a lower rank matrix that best approximates a given matrix with missing elements . We solve the BMA using block coordinate descent method.3 We also ensure it works seamlessly for recommender system . From this perspective , this is the first work that uses the block coordinate descent method and experiment BMA for recommender systems . We present the details of the algorithm with supporting technical details and a scalable version of the naive algorithm . Also , we test our algorithm on recommender system datasets and compare against state of the art algorithms SGD , SVD++ , ALSWR , Bias SVD .
IV . FOUNDATIONS
In the case of low rank approximation using NMF , the low rank factor matrices are constrained to have only nonnegative elements . However , in the case of BMA , we constrain the elements of their product with an upper and lower bound rather than each of the two low rank factor matrices . In this section , we explain the BCD framework for NMF and subsequently explain using BCD to solve BMA .
A . NMF and Block Coordinate Descent
In this section , we will see relevant foundation for using
BCD framework to solve NMF .
Consider a constrained non linear optimization problem as follows : min f ( x ) subject to x ∈ X ,
( 2 ) where X is a closed convex subset of Rn . An important assumption to be exploited in the BCD method is that set X is represented by a Cartesian product : X = X1 × ··· × Xm ,
RNj satisfying n = m
( 3 ) where Xj , j = 1,··· , m , is a closed convex subset of j=1 Nj . Accordingly , vector x is partitioned as x = ( x1,··· , xm ) so that xj ∈ Xj for j = 1,··· , m . The BCD method solves for xj fixing is , all other subvectors of x in a cyclic manner . That if x(i ) = ( x(i ) m ) is given as the current iterate at the ith step , iterate x(i+1 ) = ( x(i+1 ) m ) block by block , according to the solution of the following subproblem : j ← argmin x(i+1 ) ξ∈Xj j+1,··· , x(i ) m ) . ( 4 ) Also known as a non linear Gauss Seidel method [ 2 ] , this algorithm updates one block each time , always using the the algorithm generates the next ,··· , x(i+1 )
1 ,··· , x(i )
,··· , x(i+1 ) j−1 , ξ , x(i )
1 f ( x(i+1 )
1
3There could be other ways to solve this problem . most recently updated values of other blocks x˜j , ˜j = j . This is important since it ensures that after each update the objective function value does not increase . For a sequence
( x(i ) ) where each x(i ) is generated by the BCD method , the following property holds .
Theorem 1 : Suppose f is continuously differentiable in X = X1 × ··· × Xm , where Xj , j = 1,··· , m , are closed convex sets . Furthermore , suppose that for all j and i , the minimum of
1 min ξ∈Xj f ( x(i+1 ) j−1 , ξ , x(i )
,··· , x(i+1 ) is uniquely attained . Let ( x(i ) ) be the sequence generated Then , every limit point of(x(i ) ) is a stationary point . The by the block coordinate descent method as in Eq ( 4 ) . j+1,··· , x(i ) m ) uniqueness of the minimum is not required when m is two .
The proof of this theorem for an arbitrary number of blocks is shown in Bertsekas [ 2 ] . For a non convex optimization problem , most algorithms only guarantee the stationarity of a limit point [ 19 ] .
When applying the BCD method to a constrained nonlinear programming problem , it is critical to wisely choose a partition of X , whose Cartesian product constitutes X . An important criterion is whether the subproblems in Eq ( 4 ) are efficiently solvable : For example , if the solutions of subproblems appear in a closed form , each update can be computed fast . In addition , it is worth checking how the solutions of subproblems depend on each other . The BCD method requires that the most recent values need to be used for each subproblem in Eq ( 4 ) . When the solutions of subproblems depend on each other , they have to be computed sequentially to make use of the most recent values ; if solutions for some blocks are independent from each other , however , simultaneous computation of them would be possible . We discuss how different choices of partitions lead to different NMF algorithms . Three cases of partitions are discussed below .
1 ) BCD with Two Matrix Blocks ANLS Method : For convenience , we assume all the elements of the input matrix are known and hence ignoring the M from the discussion . The most natural partitioning of the variables is the two biggest blocks P and Q representing the entire matrix . In this case , following the BCD method in Eq ( 4 ) , we take turns solving
P ← arg min P≥0 f ( P , Q ) and Q ← arg min Q≥0 f ( P , Q ) .
( 5 )
Since the subproblems are the nonnegativity constrained least squares ( NLS ) problems , the two block BCD method has been called the alternating nonnegative least square ( ANLS ) framework [ 19 ] , [ 12 ] , [ 14 ] .
2 ) BCD with 2k Vector Blocks HALS/RRI Method : Let us now partition the unknowns into 2k blocks in which each block is a column of P or a row of Q . In this case , it is easier to consider the objective function in the following form : f ( p1,··· , pk , q
1 ,··· , q pjqT j 2 F ,
( 6 ) j=1 where P = [ p1,··· pk ] ∈ Rn×k ∈ Rk×m + . The form in Eq ( 6 ) represents that R is approximated by the sum of k rank one matrices . and Q = [ q1,··· , qk ]
+
Following the BCD scheme , we can minimize f by iteratively solving k ) = R − k pi ← arg min pi≥0 for i = 1,··· , k , and i ← arg min i ≥0 q q for i = 1,··· , k . f ( p1,··· , pk , q
1 ,··· , q k ) f ( p1,··· , pk , q
1 ,··· , q k )
The 2K block BCD algorithm has been studied as Hierarchical Alternating Least Squares ( HALS ) proposed by Cichocki et.al [ 5 ] , [ 4 ] and independently by Ho [ 10 ] as rankone residue iteration ( RRI ) .
3 ) BCD with k(n + m ) Scalar Blocks : We can also partition the variables with the smallest k(m + n ) element blocks of scalars , where every element of P and Q is considered as a block in the context of Theorem 1 . To this end , it helps to write the objective function as a quadratic function of scalar pij or qij assuming all other elements in P and Q are fixed :
2 + const ,
( 7a )
˜k
) − pijq j2 pi˜kq p˜kq˜kj ) − piqij2
2 + const ,
( 7b ) f ( pij ) = ( r i − f ( qij ) = ( rj −
˜k=j
˜k=i where r i and rj denote the ith row and the jth column of R , respectively . Kim , He and Park , [ 13 ] discuss about NMF using BCD method .
B . Bounded Matrix Low Rank Approximation
The building blocks of BMA are column vectors px and x of the matrix P and Q respectively . In this row vectors q section , we discuss the idea behind finding these vectors x ∈ px and q x such that all the elements in T + pxq [ rmin , rmax ] and the error M · ∗(R − PQ)2 F is reduced . Here , T = k pjq j .
The Problem ( 1 ) can be equivalently represented with a j=1,j=x set of rank one matrices pxq x as
M · ∗(R − T − pxq x)2 ∀x = [ 1 , k ]
F min px,qx subject to
( 8 )
Thus , we take turns in solving px and q x ≤ rmax T + pxq x ≥ rmin T + pxq x . That is , assume we know px and find q x and vice versa . Here afterwards , in the entire section we assume fixing column px and x . Without loss of generality , all the discussions finding row q x fixing px hold good for the other pertaining to finding q scenario of finding px fixing q
There are different orders of updates of vector blocks x . when solving the Problem ( 8 ) . For example ,
1 → ··· → pk → q k p1 → q and p1 → ··· → pk → q
1 → ··· → q k .
( 9 )
( 10 )
Kim , He and Park [ 13 ] prove that equation ( 6 ) satisfies the formulation of BCD method . Equation ( 6 ) when extended with the matrix M becomes equation ( 8 ) . Here , the matrix M is like a filter matrix that defines the elements of ( R − T − pxq x ) to be included for the norm computation . Thus , the Problem ( 8 ) is similar to Problem ( 6 ) and we can solve by applying 2k block BCD to update px and q x iteratively , although equation ( 8 ) appears not to satisfy the BCD requirements directly . We focus on scalar block case , since by imposing the lower and upper bounds to the lower rank matrix , the problem can be more easily understood .
Also , according to BCD , if the solutions of the elements in a block are independent , they can be computed simulx , i = j , are taneously . Here , the elements qxi , qxj ∈ q independent of each other . Hence , the problem of finding row q to solving the x fixing column px is equivalent following problem
M( : , i ) · ∗((R − T)( : , i ) − pxqxi)2
F min qxi ∀i = [ 1 , m ] , ∀x = [ 1 , k ]
( 11 ) subject to
To construct the row vector q
T( : , i ) + pxqxi ≤ rmax T( : , i ) + pxqxi ≥ rmin x , we use k(n + m ) scalar blocks based on the problem formulation ( 11 ) . Theorem 3 identifies these best elements that construct q x . Given T , R and assume we are fixing column px , we find the row vector x = ( qx1 , qx2,··· , qxm ) for the Problem ( 11 ) . For this , let q us understand the boundary values of qxi by defining two bounding vectors one for each above and below .
The lower bound vector l ∈ Rn and an Definition 1 : upper bound vector u ∈ Rn for a given px and T that bounds the qxi is defined as , rmin − T(j , i ) rmax − T(j , i ) −∞ pjx > 0,∀j ∈ [ 1 , n ] pjx < 0,∀j ∈ [ 1 , n ] otherwise lj = pjx pjx and , uj = pjx rmax − T(j , i ) rmin − T(j , i ) ∞ pjx
, pjx > 0,∀j ∈ [ 1 , n ] pjx < 0,∀j ∈ [ 1 , n ] otherwise
 
It is important to observe that the defined l and u are for a given px and T to bound qxi . Alternatively , if we are solving px for a given T and qx , the above function correspondingly represent the possible lower and upper bounds for pix , where l , u ∈ Rm . Theorem 2 : Given R , T , px , the qxi is always bounded as max(l ) ≤ qxi ≤ min(u ) . xi /∈ [ rmin , rmax ] .
Proof : It is easy to see that if qxi < max(l ) or qxi > min(u ) , then T( : , i ) + pxq
Here , it is imperative to note that if qxi , results in T( : xi /∈ [ rmin , rmax ] , this implies that qxi is either , i ) + pxq less than the max(l ) or greater than the min(u ) . It cannot be any other inequality .
Given the boundary values of qxi , Theorem 3 defines the solution to the Problem ( 11 ) . Theorem 3 : Given T , R , px , l and u , the unique solution qxi to least squares Problem ( 11 ) is given as qxi = max(l ) : if qxi < max(l ) min(u ) : if qxi > min(u ) ( [M( : , i ) · ∗(R − T)( : , i ) ] px)/(M( : , i ) · ∗px2 F )
: otherwise
Proof :
Out of Boundary : qxi < max(l ) & qxi > min(u ) Under this circumstance , the best value for qxi is either max(l ) or min(u ) . We can prove this by contradiction . Let us assume there exists a ˜qxi = max(l ) + δ ; δ > 0 that is optimal to the Problem ( 11 ) for qxi < max(l ) . However , for qxi = max(l ) < ˜qxi is still a feasible solution for the Problem ( 11 ) . Also there does not exist a feasible solution that is less than max(l ) , because the Problem ( 11 ) is quadratic in qxi . Hence for qxi < max(l ) , the optimal value for the Problem ( 11 ) is max(l ) . In similar direction we can show that optimal value of qxi is min(u ) for qxi > min(u ) .

Within Boundary : max(l ) ≤ qxi ≤ min(u ) The derivative of the objective function in the optimization Problem ( 11 ) results as 2M( : , i ) . ∗ px2
F qxi − 2[M( : , i ) . ∗ ( R − T)( : , i ) ]
V . IMPLEMENTATIONS px .
Now we have necessary tools to construct the algorithm . The BMA has three major functions . ( 1 ) Initialization , ( 2 ) Stopping Criteria and ( 3 ) Find the low rank factors P and Q . The initialization and the stopping criteria are explained in further detail in the later sections . For the time being , let us understand that we need two initial matrices P and Q , such that PQ ∈ [ rmin , rmax ] . And we need a stopping criteria for terminating the algorithm , when the constructed matrices P and Q provide a good representation of the given matrix R .
In the case of BMA algorithm , since multiple elements can be updated independently , we reorganize the scalar block BCD into 2k vector blocks . The BMA algorithm is presented to the readers as Algorithm 1 .
Algorithm 1 works very well and yields low rank factorized matrices P and Q for a given matrix R such that PQ ∈ [ rmin , rmax ] . However , when applied for very large scale matrices , such as recommender systems , it can only be run on machines with a large amount of memory . We address scaling the algorithm in multi core systems and machines with low memory in the next section . A . Scalable Bounded Matrix Low Rank Approximation
In this section , we address the issue of scaling the algorithm for large matrices with missing elements . The two important aspects of making the algorithm run for large matrices are running time and memory . We discuss the parallel implementation of the algorithm , which we refer to as Parallel Bounded Matrix Low Rank Approximation . Subsequently , we also discuss a method called Block Bounded Matrix Low Rank Approximation , which will outline the details of executing the algorithm for large matrices in low memory systems . Let us start this section by discussing Parallel Bounded Matrix Low Rank Approximation .
1 ) Parallel Bounded Matrix Low Rank Approximation : In the case of BCD method , the solutions of sub problems that depend on each other have to be computed sequentially to make use of the most recent values . However , if solutions for some blocks are independent from each other , simultaneous computation of them would be possible . We can observe that , according to Theorem 3 , every element qxi , qxj ∈ q x , i = j is independent of each other . We are leveraging this characteristic to parallelize the for loop in the Algorithm 1 . Now a days all the commercial processors have multiple cores . Hence , we can parallelize find qxi across multiple cores . We are not presenting the algorithm , as it is trivial to change the for in step 12 and step 20 of Algorithm 1 to parallel for . input : Matrix R ∈ Rn×m , rmin , rmax > 1 , reduced output : Matrix P ∈ Rn×k and Q ∈ Rk×m // Rand initialization of P , Q . rank k
1 Initialize P , Q as a nonnegative random matrix ; rmax − 1
// modify random PQ such that // PQ ∈ [ rmin , rmax ] // maxelement of PQ without first // column of P and first row of Q maxElement
2 maxElement = max(P( : , 2 : end ) ∗ Q(2 : end , :) ) ; 3 α ← 4 P ← α · P ; 5 Q ← α · Q ; 6 P( : , 1 ) ← 1 ; 7 Q(1 , : ) ← 1 ; 8 M ← ComputeRatedBinaryMatrix(R ) ; 9 while stopping criteria not met do 10 for x ← 1 to k do
;
T ← k j ; pjq for i ← 1 to m do
11
12
13 14
15 16 17
18 19
20
21 22
23
24 25
26 27
Definition 1 j=1,j=x // Find vector l , u ∈ Rn as in l ← LowerBounds(rmin , rmax , T , i , px ) ; u ← UpperBounds(rmin , rmax , T , i , px ) ;
// Find vector q x fixing px as qxi ← FindElement(px , M , R , T , i , x ) ; if qxi < max(l ) then on Theorem 3 qxi ← max(l ) ; qxi ← min(u ) ; else if qxi > min(u ) then for i ← 1 to n do
Definition 1
// Find vector l , u ∈ Rm as in l ← LowerBounds(rmin , rmax , T , i , q x ) ; u ← UpperBounds(rmin , rmax , T , i , q // Find vector px fixing q pix ← FindElement(q if pix < max(l ) then pix ← max(l ) ; pix ← min(u ) ; else if pix > min(u ) then x ) ; x as on Theorem 3 x , M
, i , x ) ;
, T
, R
Algorithm 1 : Bounded Matrix Low Rank Approximation ( BMA )
It is obvious to see that the T at Step 11 on Algorithm 1 requires the largest amount of memory . Also the function F indElement in step 15 takes a sizeable amount of memory . Hence it is not possible to run the algorithm for large matrices – with rows and columns in the scale of 100,000 ’s , in machines with low memory . Hence we propose the next algorithm Block BMA .
2 ) Block Bounded Matrix Low Rank Approximation : To help better understanding of this section , let us define β – a data structure in memory factor . That is , maintaining a floating scalar as single , double , sparse matrix with one element or full matrix with one element takes different amount of memory . This is because of the datastructure that is used to represent the number in the memory . Typically , in Matlab , the data structure in memory factor β for full matrix is around 10 . Similarly , in Java , the β factor for maintaining a number in an ArrayList is around 8 . Let , memsize(v ) be the function that returns the approximate memory size of a variable v . Generally , memsize(v ) = number of elements in v * size of each element * β . Consider an example of maintaining 1000 floating point numbers on an ArrayList of a Java program . The approximate memory would be 1000*4*8 = 32000 bytes ≈ 32MB against 4MB in actual because of the factor β=8 .
As discussed earlier , for most of the real world large datasets such as Netflix , Yahoo music , online dating , book crossing , etc . , it is impossible to keep the entire matrix T in memory . Also notice that , according to Theorem 3 and Definition 1 , we need only the i th column of T to compute qxi . The block size of qxi to find in one core of the machine is dependent on the size of T and F indElements that fits in the memory .
On the one hand , partition qx to fit the maximum possible T and F indElements in the entire memory of the system . On the other hand , create very small partitions such that , we can give every core some amount of work so that we don’t under utilize the processing capacity of the system . The disadvantage of the former , is that only one core is used . However , there is a significant communication overhead . Figure 1 gives the pictorial view of the Block Bounded Matrix Low Rank Approximation . in the latter case , of the
We blocks number determined
= memsize(full(R)+other variables of FindElement)/(system memory * number of d cores ) . The full(R ) is non sparse representation and d ≤ number of cores available in the system . Typically for most of the datasets , we achieved minimum running time when we used 4 cores and number is , we find 1/16 th of q of blocks to be 16 . That x concurrently on 4 cores .
For the convenience of the readers , we have presented the Block BMA as Algorithm 2 . We describe the algorithm only to find the partial vector of q x given px . To find more than one element , Algorithm 1 is modified such that the vectors l , u , px are matrices L , U , Pblk respectively , in Algorithm
2 . The described Algorithm 2 replaces the steps 11 – 19 in Algorithm 1 . The initialization and the stopping criteria for Algorithm 2 is similar to those of Algorithm 1 . We also include the necessary steps to handle numerical errors as part of Algorithm 2 . x , current q input : Matrix R ∈ Rn×m , set of indices i , current px , output : Partial vector qx of requested indices i // ratings of input indices i x , rmin , rmax
1 Rblk ← R( : , i ) ; 2 Mblk ← ComputeRatedBinaryMatrix(Rblk ) ; 3 Pblk ← Replicate(px , size(i) ) ; numerical errors
// save qx of input indices i for blk ← qx(i ) ; // Tblk ∈ n × size(i ) of input indices i
4 q
5 Tblk ← k blk ; pjq j=1,j=x
// Find matrix L , U ∈ Rn×size(i ) as in
Definition 1
6 L ← LowerBounds(rmin , rmax , T , i , px ) ; 7 U ← UpperBounds(rmin , rmax , T , i , px ) ;
// Find vector qblk fixing px as in
Theorem 3
8 qblk = ( [Mblk·∗(Rblk−Tblk ) ]
Pblk)/(Mblk·∗Pblk2 F )
; // indices of qblk that are not within bounds
// case 1 & 2 numerical errors blk ≈ max(L)]or[q
9 idxlb ← f ind(qblk < max(L ) ) ; 10 idxub ← f ind(qblk > min(U ) ) ; blk ≈ min(U ] ) 11 idxcase1 ← f ind([q 12 idxcase2 ← f ind([max(L ) ≈ min(U)]or[max(L ) > 13 idxdontchange ← idxcase1 ∪ idxcase2 ; // set appropriate values of min(U) ] ) ;
; qblk /∈ [ max(L ) , min(U ) ]
14 qblk(idxlb \ idxdontchange ) ← max(L)(idxlb \ idxdontchange ) ; 15 qblk(idxub \ idxdontchange ) ← min(U)(idxub \ idxdontchange ) ; 16 qblk(idxdontchange ) ← q
Algorithm 2 : Block BMA blk(idxdontchange ) ;
In the next section , we discuss tuning various parameters in the algorithm for improving the accuracy for prediction tasks . B . Parameter Tuning
The BMA has many types of applications . One important application of the BMA is prediction of the missing elements in the matrix . For example , in the case of recommender systems the missing ratings are provided as ground truth in the form of a test data . The dot product of P(u , : ) and Q( : , i ) gives the missing rating of a ( u , i ) pair . In such cases , the accuracy of the algorithm is determined by the Root Mean Square Error ( RMSE ) of the predicted ratings against
Figure 1 : Block Bounded Matrix Low Rank Approximation the ground truth . It is unimportant how good the algorithm converges for a given rank k .
This section discusses ways to improve the RMSE of the predictions against the missing ratings by tuning the parameters of the BMA algorithm .
1 ) Initialization : The BMA algorithm can converge to different points depending on the initialization . In Algorithm 1 , we explained about random initialization such that PQ ∈ [ rmin , rmax ] . In general , it should give good results .
However , in the case of recommender system , we tuned this initialization for better results . According to Koren [ 15 ] , one good baseline estimate for a missing rating ( u , i ) is µ + gu + hi , where µ is the average of the known ratings , and gu and hi are the bias of user u and item i , respectively . We initialized P and Q such that PQ(u , i ) = µ + gu + hi as mentioned below

P = and

µ k − 2
µ k − 2
µ k − 2
µ k − 2
··· ··· g1 gn
1
1

Q =

1
1
1 1 h1 h2
··· ··· ···
1
1 hm
That is , let the first k − 2 columns of P be
, P( : , k− 1 ) = g and P( : , k ) = 1 . Let all the k− 1 rows of Q be . We call this as baseline initialization 1 ’s and Q(k , : ) = h
µ k − 2 and expected to work better than random initializations for certain type of and vice versa .
2 ) Reduced Rank k :
In the case of regular low rank approximation with all known elements , the higher the k ; the closer the low rank approximation to the input matrix [ 13 ] . However , in the case of predicting with the low rank factors , a good k depends on the nature of the dataset . Even though , for a higher k , the low rank approximation is closer to the known rating of the input R , the RMSE on the test data may be poor . In the Table III , we can observe the behaviour of RMSE on the test data against k . Most of the time , a good k is determined by trial and error for the prediction problem . 3 ) Stopping Criteria C : The stopping criteria defines the goodness of the low rank approximation for the given matrix and the task for which the low rank factors are used . The two common stopping criteria are – ( 1 ) For a given rank k , the product of the low rank factors should be closer to the known ratings of the matrix and ( 2 ) The low rank factors should perform the prediction task on a smaller validation set which has the same distribution of the test set . The former is common when all the elements of R are known and the latter for recommender system .
Let us formally define the stopping criteria C1 for the
M · ∗(R − PQ)2
F numRatings in R did not change in former case as the successive iterations at a given floating point precision like 1e 5 . Since the function is monotonically decreasing , for every iteration , we find P and Q that improve the solution to the Problem ( 1 ) . Otherwise , we terminate .
The stopping criteria C2 for the latter case is the increase
M · ∗(V − PQ)2
F numRatings in V of
, for some validation matrix V which has the same distribution of the test matrix , between successive iterations . Here , M is for the validation matrix V . This stopping criteria has diminishing effect as the number of iterations increase . Hence , we also check whether
M · ∗(V − PQ)2 did not change in the successive
F numRatings in V iterations at a given floating point precision like 1e 5 .
We can show that , for the above stopping criterion C1 and
C2 , the Algorithm 1 terminates for any input matrix R .
Theorem 4 : Algorithm 1 terminates for both the stopping criterion C1 and C2 . Proof : Case 1 : Termination with C1 : It is trivial that the R−TF is monotonically decreasing for every px and x . Hence at the end of completely finding all the vectors of q P and Q , M.∗(R−PQ)F would have decreased . In other words , we find only those px and q x that reduce the RMSE between the known ratings in the rating matrix and the corresponding factors of P and Q . Alternatively , if the algorithm could not find P and Q due to convergence , the M.∗ ( R− PQ)F does not change . Hence , the algorithm terminates .
Dataset
Rows Columns
Jester Movielens Dating Book crossing
73421 71567 135359 278858
100 10681 168791 271379
Ratings Density Ratings Range [ 10,10 ] [ 1,5 ] [ 1,10 ] [ 1,10 ]
( millions ) 4.1 10 17.3 1.1
0.5584 0.0131 0.0007 0.00001
Table II : Datasets for experimentation
Case 2 : Termination with C2 : It is trivial because at the end of the iteration , we terminate if the RMSE on the validation set has either increased or decreased marginally .
VI . EXPERIMENTATION
In the experimentation , we take an important application of the BMA – recommender system . The entire experimentation was conducted with Algorithm 2 in various systems with memory as lows as 16GB .
One of the major challenges during experimentation is numerical errors . The numerical errors might result in T + x /∈ [ rmin , rmax ] . The two fundamental questions to pxq solve the numerical errors are – ( 1 ) How to identify the occurrence of a numerical error ? and ( 2 ) What is the best possible value to choose in the case of a numerical error ? Let us start with addressing the former question of potential numerical errors that arise in the BMA Algorithm 1 . It is important to understand that if we are well within bounds , ie , if max(l ) < qxi < min(u ) , we are not essentially impacted by the numerical errors . It is critical only when qxi is out of boundary , that is , qxi < max(l ) or qxi > min(u ) and approximately closer to the boundary discussed as in ( Case A and Case B ) . For discussion let us assume we are improving the old value of q xi to qxi such that we minimize x)2 the error M · ∗(R − T − pxq
F . xi ≈ min(u ) : xi ≈ max(l ) or q to saying q is already optimal for This is equivalent the given px and T and there is no further improvement xi ≈ qxi and it is better to possible . Under this scenario , if q retain q xi irrespective of the new qxi found .
Case B : max(l ) ≈ min(u ) or max(l ) > min(u ) :
Case A : q xi
According to Theorem 2 , we know that max(l ) < min(u ) . Hence if max(l ) > min(u ) , it is only because of the numerical errors . In all the above cases during numerical errors , we are better off retaining the old value q xi against the new value qxi . We have explained the Algorithm 2 – Block BMA considering the numerical errors .
We experimented this Algorithm 2 among varied bounds in very large matrix sizes taken from the real world datasets . The datasets used for our experiments include the movielens 10 million [ 1 ] , jester [ 8 ] , book crossing [ 27 ] and online dating dataset [ 3 ] . The characteristics of the dataset are presented in Table II .
We have chosen Root Mean Square Error ( RMSE ) – a defacto metric for recommender systems . We compare the RMSE of BMA with baseline initialization ( BMA– Baseline ) and BMA with random initialization ( BMA– Random ) against the other algorithms on all the datasets . The algorithms used for comparison are ALSWR ( alternating least squares with regularization ) [ 26 ] , SGD [ 7 ] , SVD++ [ 15 ] and Bias SVD [ 15 ] and its implementation in Graphlab(http://graphlab.org/ ) [ 20 ] software package . We implemented our algorithm in Matlab and used parallel computing toolbox for parallelizing across multiple cores . For parameter tuning , we varied the number of reduced rank k and tried different initial matrices for our algorithm to compare against all the algorithms . Many algorithms do not support the stopping criteria C1 and hence , to be consistent , we used the stopping criteria C2 .
For every k , every dataset was randomly partitioned into 85 % training , 5 % validation and 10 % test data . We run all the algorithms on these partitions and computed their RMSE scores . We repeated each experiment 5 times and reported their RMSE scores in the Table III , where each resulting value is the average of the RMSE scores on randomly chosen test set on 5 runs . The Table III summarizes the RMSE comparison of all the algorithms .
The Algorithm 1 consistently outperformed existing state of the art algorithms . One of the main reason for the consistent performance is the absence of hyper parameters . In the case of machine learning algorithms , there are many parameters that need to be tuned for performance . Even though the algorithms perform the best when provided with the right parameters , identifying these parameters are formidable challenge – usually by trial and error methods . For example , in Table III , we can observe that the BiasSVD an algorithm without hyper parameters performed better than its extension SVD++ with default parameters in many cases . The BMA algorithm without hyper parameters performed well in real world datasets , albeit a BMA with hyper parameters and right parametric values would have performed better .
Recently , there has been a surge in interest to understand the temporal impact on the ratings . Time svd++ [ 16 ] is one such algorithm that leverages the time of rating to improve prediction accuracy . Also , the most celebrated dataset in the recommender system community is Netflix dataset , because of the prize money and first massive dataset for recommender system that was publicly made available . In Netflix dataset there were 17770 users who rated 480189 movies in a scale of [ 1,5 ] . There were totally 100,480,507 ratings in training set and 1,408,342 ratings in validation set . All the algorithms listed above were invented to address Netflix challenge . Even though the book crossing dataset [ 27 ] is bigger than the Netflix , we felt the paper is not complete without experimenting on Netflix and comparing against time SVD++ . However , the major challenge is that Netflix
Algorithm BMA Baseline BMA Random ALSWR SVD++ SGD Bias SVD time svd++ SVD++ Test time SVD++ Test k = 100 0.9287 0.8777 1.5663 1.5135 1.2997 1.3354 1.1868 0.8924 0.8805 Table IV : RMSE Comparison of BMA with other algorithms on Netflix k = 50 0.9405 0.9405 1.5664 1.5235 1.2997 1.3662 1.1884 0.8952 0.8824 k = 10 0.9521 0.9883 1.5663 1.6319 1.2997 1.3920 1.1800 0.9131 0.8971 k = 20 0.9533 0.9569 1.5663 1.5453 1.2997 1.3882 1.1829 0.9032 0.8891 dataset has been withdrawn from the internet and its test data is no more available . Hence , we extracted a small sample of 5 % from the training data as validation set and tested the algorithm against the validation set that was supplied as part of the training package . We performed this experiment and results are presented in Table IV . For better comparison we also present the original Netflix test scores for SVD++ and time SVD++ algorithms from [ 16 ] . These are labeled as SVD++ Test and time SVD++ Test respectively . Our BMA algorithm out performed all the algorithm in Netflix dataset when tested on the validation set supplied as part of the Netflix training package .
VII . CONCLUSION
In this paper , we presented a new low rank approximation called Bounded Matrix Low Rank Approximation ( BMA ) which imposed a lower and upper bound on every element of a lower rank matrix that best approximates a given matrix with missing elements . We also presented substantial experimental results on real world datasets illustrating that our proposed method outperformed the state of the art algorithms in recommender systems . We would like to extend BMA to tensors , such that it can use time information of the ratings during factorization . During our experimentation , we observed linear scale up for Algorithm 2 in Matlab . However , the other algorithms from Graphlab are implemented in C/C++ and take lesser clock time . A C/C++ implementation of Algorithm 2 would be a good starting point to compare the running time performance against the other state of the art algorithms . Also , we want to experiment BMA in datasets of different nature other than those from recommender systems .
ACKNOWLEDGEMENT
This work is supported in part by the National Science Foundation grants CCF 0732318 and CCF 0808863 . Any opinions , findings , and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundations .
Dataset k
BMA
BMA ALSWR SVD++
SGD Bias SVD
5.7170 5.6752 5.6689 1.2386 1.2371 1.2381 3.9082 3.9144 3.9123 5.1772 5.1719 5.1785
5.8261 5.7862 5.7956 1.2329 1.2317 1.2324 3.9052 3.9115 3.9096 3.9466 3.9645 3.9492
Jester Jester Jester
10 20 50 Movielens10M 10 Movielens10M 20 Movielens10M 50 10 20 50 10 20 50 Table III : RMSE Comparison of Algorithms on Real World Datasets
Baseline Random 4.6289 4.7339 4.7180 0.8974 0.8931 0.8932 2.1625 2.1617 2.1642 2.8137 2.4652 2.1269
Dating Dating Dating Book Crossing Book Crossing Book Crossing
5.6423 5.6579 5.6713 1.5166 1.5158 1.5162 3.8581 3.8643 3.8606 4.7131 4.7212 4.7168
4.3320 4.3664 4.5046 0.8531 0.8526 0.8553 1.9309 1.9337 1.9434 1.9355 1.9315 1.9405
5.5371 5.5466 5.5437 1.4248 1.4196 1.4204 4.1902 4.1868 4.1764 4.7315 4.6762 4.6918
REFERENCES
[ 1 ] Movielens dataset . http://movielensumnedu cessed 6 June 2012 ] .
[ Online ; ac
[ 2 ] D . P . Bertsekas . Nonlinear Programming . Athena Scientific ,
Belmont , MA , 1999 .
[ 3 ] L . Brozovsky and V . Petricek . Recommender system for online dating service . In Proceedings of Conference Znalosti 2007 , Ostrava , 2007 . VSB .
[ 4 ] A . Cichocki and A H Phan . local algorithms for large scale nonnegative matrix and tensor factorizations . IEICE Transactions on Fundamentals of Electronics , Communications and Computer Sciences , E92 A:708–721 , 2009 .
Fast
[ 5 ] A . Cichocki , R . Zdunek , and S . Amari . Hierarchical als algorithms for nonnegative matrix and 3d tensor factorization . LNCS , 4666:169–176 , 2007 .
[ 6 ] S . Deerwester , S . T . Dumais , G . W . Furnas , T . K . Landauer , and R . Harshman . Indexing by latent semantic analysis . Journal of the American Society for Information Science , 41:391–407 , 1990 .
[ 7 ] S . Funk . Stochastic gradient descent . http://sifter.org/∼simon/ [ Online ; accessed 6 June journal/20061211.html , 2006 . 2012 ] .
[ 8 ] K . Goldberg .
Jester collaborative filtering dataset . http:// goldbergberkeleyedu/jester data/ , 2003 . [ Online ; accessed 6June 2012 ] .
[ 14 ] J . Kim and H . Park . Fast nonnegative matrix factorization : An active set like method and comparisons . SIAM J . Scientific Computing , 33(6):3261–3281 , 2011 .
[ 15 ] Y . Koren . Factorization meets the neighborhood : a multiIn KDD , pages 426– faceted collaborative filtering model . 434 , 2008 .
[ 16 ] Y . Koren . Collaborative filtering with temporal dynamics . In
KDD , page 447 , 2009 .
[ 17 ] Y . Koren , R . Bell , and C . Volinsky . Matrix Factorization Techniques for Recommender Systems . Computer , 42(8):30– 37 , Aug . 2009 .
[ 18 ] D . Kuang , H . Park , and C . H . Q . Ding . Symmetric nonnegative matrix factorization for graph clustering . In SDM , pages 106–117 , 2012 .
[ 19 ] C . J . Lin . Projected Gradient Methods for Nonnegative Matrix Factorization . Neural Comput . , 19(10):2756–2779 , Oct . 2007 .
[ 20 ] Y . Low , J . Gonzalez , A . Kyrola , D . Bickson , C . Guestrin , and J . M . Hellerstein . Graphlab : A new parallel framework for machine learning . In UAI , 2010 .
[ 21 ] L . W . Mackey , D . Weiss , and M . I . Jordan . Mixed member ship matrix factorization . In ICML , pages 711–718 , 2010 .
[ 22 ] I . Markovsky . Algorithms and literate programs for weighted low rank approximation with missing data . In J . Levesley , A . Iske , and E . Georgoulis , editors , Approximation Algorithms for Complex Systems , number 18296 , pages 255– 273 . Springer Verlag , 2011 . Chapter : 12 .
[ 9 ] G . H . Golub and C . F . Van Loan . Matrix Computations . The
Johns Hopkins University Press , 3rd edition , 1996 .
[ 23 ] A . Paterek . Improving regularized singular value decomposition for collaborative filtering . In KDD , pages 39–42 , 2007 .
[ 10 ] N D Ho , P . V . Dooren , and V . D . Blondel . Descent methods for nonnegative matrix factorization . CoRR , abs/0801.3199 , 2008 .
[ 24 ] R . Salakhutdinov and A . Mnih . Bayesian probabilistic matrix In ICML , factorization using markov chain monte carlo . pages 880–887 , 2008 .
[ 11 ] H . Kim and H . Park . Sparse non negative matrix factorizations via alternating non negativity constrained least squares for microarray data analysis . Bioinformatics , 23(12):1495– 1502 , 2007 .
[ 25 ] L . Xiong , X . Chen , T K Huang , J . G . Schneider , and J . G . Carbonell . Temporal collaborative filtering with bayesian In SDM , pages 211–222 , probabilistic tensor factorization . 2010 .
[ 12 ] H . Kim and H . Park . Nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method . SIAM Journal on Matrix Analysis and Applications , 30(2):713–730 , 2008 .
[ 26 ] Y . Zhou , D . Wilkinson , R . Schreiber , and R . Pan . Largescale Parallel Collaborative Filtering for the Netflix Prize . Algorithmic Aspects in Information and Management , 5034:337–348 , 2008 .
[ 13 ] J . Kim , Y . He , and H . Park . Algorithms for nonnegative matrix and tensor factorizations : A unified view based on block coordinate descent framework . Under Review , 2012 .
[ 27 ] C N Ziegler , S . M . McNee , J . A . Konstan , and G . Lausen . Improving recommendation lists through topic diversification . In WWW , pages 22–32 , 2005 .
