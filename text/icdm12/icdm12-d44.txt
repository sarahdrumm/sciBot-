Towards Active Learning on Graphs : An Error Bound Minimization Approach
Quanquan Gu and Jiawei Han
Department of Computer Science
University of Illinois at Urbana Champaign
Urbana , IL , 61820 qgu3@illinois.edu , hanj@csuiucedu
Abstract—Active learning on graphs has received increasing interest in the past years . In this paper , we propose a nonadaptive active learning approach on graphs , based on generalization error bound minimization . In particular , we present a data dependent error bound for a graph based learning method , namely learning with local and global consistency ( LLGC ) . We show that the empirical transductive Rademacher complexity of the function class for LLGC provides a natural criterion for active learning . The resulting active learning approach is to select a subset of nodes on a graph such that the empirical transductive Rademacher complexity of LLGC is minimized . We propose a simple yet effective sequential optimization algorithm to solve it . Experiments on benchmark datasets show that the proposed method outperforms the stateof the art active learning methods on graphs .
Keywords Active Learning ; Graph ; Generalization Error
Bound ; Sequential Optimization
I . INTRODUCTION
In many practical machine learning problems , the acquisition of labeled data is often expensive and/or time consuming . This motivates Active Learning [ 7 ] , which attempts to select the most informative data points for labeling to reduce the labeling cost . Traditional active learning methods [ 15 ] [ 16 ] [ 12 ] focus on the data which are represented by vectors . However , in many real life applications , the data are represented by a graph , eg , bibliographic networks . Moreover , the data which are represented by vectors can be transformed into a graph by standard techniques widely used in graphbased semi supervised learning [ 18 ] [ 17 ] . Therefore , active learning on graphs is an alternative of practical interest to traditional active learning and has received increasing attention .
Depending on whether there is an interaction between the learner and the oracle , active learning can be roughly categorized into two families . One is adaptive active learning , such as SVM active learning [ 15 ] , and agnostic active learning [ 1 ] , which is able to use previous labels to determine the next point to label . The other family is nonadaptive active learning , which is appealing because it is able to select a batch of data points without training a classifier . For example , optimal experimental design methods [ 16 ] [ 12 ] have been used for nonadaptive active learning . In our study , we mainly focus on nonadaptive active learning .
In recent years , many active learning methods on graphs have been proposed , motivated by different criteria of “ informative ” data . For example , [ 11 ] derived a deterministic error bound for Minimum Cut based semi supervised learning approach [ 4 ] , which shows that the prediction error is small if the graph cut size is large . This suggests a label selection method to choose the labeled nodes to maximize the graph cut size . Therefore , they proposed a heuristic algorithm to maximize the graph cut for active learning . [ 10 ] generalized the error bound in [ 11 ] by replacing the graph cut with an arbitrary symmetric submodular function , and also proposed an improved algorithm to maximize the graph cut using submodular function maximization technique . [ 11 ] proposed a probabilistic error bound that motivates an active learning method , which first clusters the graph and then randomly chooses a node in each cluster . [ 13 ] proposed to select the most informative nodes by minimizing the prediction variance of Gaussian Filed and Harmonic Function ( GFHF ) [ 18 ] . All the active learning methods mentioned above are non adaptive . Another line of research [ 5 ] [ 3 ] has considered adaptive active learning , where the labels for the nodes of a graph are queried and predicted in an iterative way .
In this paper , we aim to develop a non adaptive active learning method on graphs , which theoretically guarantees a good generalization performance . To achieve this goal , it is natural to consider the generalization error of a specific classifier on graphs . In particular , we choose Learning with Local and Global Consistency ( LLGC ) [ 17 ] as the classifier on graphs , because it is comparable to or even better than Minimum Cut ( MinCut ) [ 4 ] and GFHF [ 18 ] . We present a data dependent generalization error bound for LLGC using the tool of transductive Rademacher Complexity [ 8 ] , which is an extension of inductive Rademacher Complexity [ 2 ] and measures the richness of a class of real valued functions with respect to a probability distribution . We show that the empirical transductive Rademacher complexity is a good surrogate for active learning on graphs . Thus we propose to actively select the nodes by minimizing the empirical transductive Rademacher complexity of LLGC on a graph . The resulting active learning method is a combinatorial optimization problem . In order to optimize it effectively , we present a sequential optimization algorithm . It is worth noting that our proposed active learning method tends to result in small generalization error for LLGC1 . Experiments on benchmark datasets show that the proposed method outperforms the state of the art active learning methods on graphs .
The remainder of this paper is organized as follows . In Section II , we present a generalization error bound for LLGC . In Section III , we present a criterion for active learning and its optimization algorithm . The experiments are demonstrated in Section IV . Finally , we draw conclusions and point out some future work in Section V .
CONSISTENCY
II . ANALYSIS OF LEARNING WITH LOCAL AND GLOBAL Given a weighted graph G = ( V;E ) , where each node vi ∈ V corresponds to a data point xi , and the weight Wij of edge eij ∈ E reflects the affinity between i th node and the jth node . W ∈ Rn.n is called adjacency matrix of the graph . For undirected graphs , W is symmetric , while for directed graphs , W is asymmetric . In the setting of classification , some of the nodes on the graph are labeled , ie , yi ∈ {±1} , while the remainder are unlabeled , ie , yi = 0 . And our goal is to predict the labels of those unlabeled nodes .
A . Review of LLGC
There exist bunches of graph based ( semi supervised ) learning methods , eg , Minimum Cut ( MinCut ) [ 4 ] , Gaussian Field and Harmonic Function ( GFHF ) [ 18 ] and Learning with Local and Global Consistency ( LLGC ) [ 17 ] . In this paper , we focus on LLGC because it is the state of the art method and amenable to theoretical analysis .
In order to preserve the topological properties of a graph , LLGC [ 17 ] assumes that if two nodes xi and xj are connected in the graph , then the labels of these two nodes tend to be similar to each other . Given a symmetric adjacency matrix W ∈ Rn.n of the graph , and let f ( xi ) be the label of node xi produced by a classifier f , the above assumption can be mathematically formulated as : n∑ i;j=1
1 2
− fj√
Djj
( fi√ Dii
)2Wij = f T Lf ;
( 1 ) n
∑ where fi is a shorthand for f ( xi ) , f = [ f1 ; : : : ; fn]T , D is a diagonal matrix , called degree matrix , with Dii = j=1 Wij , L = I − D 2 is the normalized graph Laplacian [ 6 ] , and I is an identity matrix of appropriate size . Eq ( 1 ) is called Graph Regularization . Intuitively , the objective function incurs a heavy penalty if neighboring points xi and xj are mapped far apart .
2 WD
, 1
, 1
In the setting of binary classification , LLGC pursues a function f by minimizing the following criterion
||f − y||2
2 + f T Lf ; min f
( 2 )
1Although it is designed based on LLGC , we will show that it also works very well for GFHF by experiments . where > 0 is a regularization parameter , which controls the balance between the loss and label smoothness . y is the label vector with y = [ y1 ; y2 ; : : : ; yn]T . B . Generalization Error Bound for LLGC
In this subsection , we derive a generalization error bound for LLGC using the tool of transductive Rademacher complexity for general function classes [ 8 ] . Definition 1 . [ 8 ] For a fixed sample set S = {x1 ; : : : ; xn} generated by a distribution DX on a set X and a real valued function class F with domain X , the empirical transductive ] Rademacher complexity of F is the random variable
[
^Rl+u(F ) = (
1 l
+
)E
1 u if ( xi )
;
( 3 ) l+u∑ sup f2F i=1
 1
: wp 1 − 2p ; ] [ where l + u = n , and = ( 1 ; : : : ; n)T are independent random variables such that −1 0 wp p wp p i =
( 4 )
( 5 ) where 0 ≤ p ≤ 1 is
2 . The transductive Rademacher complexity Rl+u(F ) = Ex Note that for the case p = 1
^Rl+u(F ) 2 and l = u , the transductive Rademacher complexity coincides with the standard inductive definition [ 2 ] up to a normalization factor 1 u . We set p = lu n2 in the following derivation .
Intuitively speaking , transductive Rademacher complexity measures the richness of a class of real valued functions with respect to a probability distribution . Theorem 2 . [ 8 ] Fix ffi ∈ ( 0 ; 1 ) , and let F be a class of functions mapping from X ×Y to [ 0 ; 1 ] . Let c0 = and Q = 1 i=1 , with probability 1− ffi over random draws of a subsample of size l , every f ∈ F satisfies
√ u . For any fixed sample set {(xi ; yi)}n l + 1 l + 1
32 ln(4e )
3 err(f ) ≤ ^err(f ) + ^Rl+u(F )
√
√
+ c0Q min(l ; u ) +
2Q ln(1=ffi ) ;
( 6 ) where err(f ) is the expected error on the unlabeled data , and ^err(f ) is the empirical error on the labeled data .
The above error bound is quite general and applicable to various transductive learning algorithms if an empirical transductive Rademacher complexity ^Rl+u(F ) of the function class F can be found efficiently . It also implies that in order to prove the generalization error bound for LLGC , it is sufficient to give an estimation of the empirical transductive Rademacher complexity for the following function class . Definition 3 . The function class of LLGC is Fl = {f =
,1y;||y||2 ≤ √ l} .
( L + I )
Note that therefore , ∥y∥2 ≤ √ there are l l . labeled data with yi ∈ {±1} ,
In the following , we present two theorems , which serve as the theoretical foundation of our proposed method in this paper . Theorem 4 . The empirical transductive Rademacher complexity of the function class Fl is upper bounded as
√
^Rl+u(Fl ) ≤ tr ( (L + I),2 ) ;
2 u
( 7 ) where I is an identity matrix .
Proof : The empirical Rademacher complexity of the function class Fl is computed as
]
]
 yT ( L + I )
,1 sup f :jjyjj2p l
^Rl+u(Fl ) )E
+
1 l
= (
[ [
1 u
1 u
1 u
≤ (
1 l
+
+
1 l
≤ ( √ ≤ (
1 l
=
2 u l sup
vuut n∑ f :jjyjj2p √ lE i;j=1
)E √ ) √ ) n2 tr ( (L + I),2 )
+ l
2lu
1 u tr ( (L + I),2 ) ;
||y||2||(L + I )
,1||2 ij ( (L + I),2)ij to the labeled set L , LUU denotes the principal submatrix corresponding to the unlabeled set U , and LLU denotes the submatrix which interrelates the labeled set L with unlabeled set U . A . Objective Function
√ min(l ; u ) +
From Theorem 5 , we can see that the expected error on the unlabeled data is upper bounded by the empirical √ error on the labeled data plus the empirical transductive Rademacher complexity ^Rl+u(Fl ) and the confidence term 2Q ln(1=ffi ) . It is easy to check that , c0Q the larger the number of labeled samples ( l ) is , the tighter the bound will be . In other words , the expected error on the unlabeled data will be approximated by the empirical error more accurately . Ideally we should minimize the expected error on the unlabeled data by jointly minimizing the empirical error on the labeled data and ^Rl+u(Fl ) . However , in the setting of non adaptive active learning on a graph , we do not know the label of a given node until we select this node . That means we cannot estimate ^err(f ) before we label the nodes and train a classifier . Hence the only term we can control is the empirical transductive Rademacher complexity . In other words , minimizing ^Rl+u(Fl ) is a surrogate to guarantee small expected error . Therefore , we present an active learning criterion by minimizing the upper bound of empirical transductive Rademacher complexity for LLGC as follows ,
(
)
,2
{ defined as
Sij = where we ignore the constant scalers and square root symbol . Note that LLL is computed based on the selected l samples , ie , L .
The above optimization problem is a combinatorial optimization problem . Finding the global optimal solution is NPhard . Motivated by the success of sequential minimization algorithm in some existing experimental design approaches [ 16 ] [ 12 ] [ 13 ] , we present a simple yet effective sequential optimization algorithm as follows . B . Sequential Optimization
We introduce a selection matrix S ∈ Rn.l , which is if xi is selected as the j th point in L otherwise :
1 ; 0 ;
It is easy to check that each column of S has one and only one 1 , each row has at most one 1 , and ST S = I . The constraint set for S can be defined as
S = {S|S ∈ {0 ; 1}n.l ; ST 1 = 1 ; S1 ≤ 1}
( 11 ) Then LLL can be represented by ST LS . Therefore ,
= {S|S ∈ {0 ; 1}n.l ; ST S = I} ; )
Eq ( 10 ) can be equivalently written as ,2
(
( ST LS + I )
:
( 12 ) arg minSflS tr
( 8 ) arg minLflV tr
( LLL + I )
:
( 10 ) where the first inequality holds due to the Cauchy Schwarz ’s inequality and the third inequality holds due to the Jensen ’s inequality .
Using Theorem 2 and Theorem 4 , we obtain the following
√ generalization error bound for LLGC . Theorem 5 . Fix ffi ∈ ( 0 ; 1 ) , and let F be a class of functions mapping from X ×Y to [ 0 ; 1 ] . Let c0 = and Q = u . For any fixed sample {(xi ; yi)}n i=1 , with probability l + 1 1 − ffi over random draws of a subsample of size l , every f ∈ F satisfies
32 ln(4e )
3
1
√
√ err(f ) ≤ ^err(f ) + tr ( (L + I),2 )
2 u
√
+ c0Q min(l ; u ) +
2Q ln(1=ffi ) :
III . ACTIVE LEARNING VIA ERROR BOUND
MINIMIZATION
( 9 )
The generic problem of non adaptive active learning on graphs is as follows . Given a weighted graph G = ( V;E ) , V is the pool of candidate nodes , our goal is to find a subset L ⊂ V , which contains the most informative l nodes , namely active set or labeled set . Let U = V \ L be the unlabeled set . Given a graph Laplacian matrix L associated with the graph , LLL denotes the principal submatrix corresponding
( ( ( (
Suppose the eigen decomposition of L is UfiUT where U is consisted of the eigenvectors , and fi = diag(1 ; : : : ; n ) is a diagonal matrix whose diagonal elements are eigenvalues . We have
( tr
( ST LS + I )
,2
)
)
,2
,2
= tr = tr = tr = tr
( ST UfiUT S + I ) ( ST U(fi + I)UT S ) ( ST UUT S ) ( ST U,UT S + I )
)
,1
,1
) )
( ) ( 1 + 1)2 − 1 ; : : : ; ( n + 1)2 − 1 where = diag diag fact that ST UUT S = I .
( 1 + 1)2 ; : : : ; ( n + 1)2
( 13 ) , , = , and we use the
Using the Woodbury matrix identity [ 9 ] , we have
)
(
,1 + UT SST U )
,1UT S :
( 14 )
Hence
,1
)
,1
( ST U,UT S + I )
= I − ST U( , ( ( = l − tr = l − tr(( , tr
( ST U,UT S + I )
ST U( , ,1 + UT SST U ) ,1 ,1 ) )
,1 + UT SST U − ,
( ,
(
,1 + UT SST U )
,1UT S
)
)
( ,
,1 + UT SST U )
= l − n + tr ( 15 ) ,1 is a diagonal matrix whose i th diagonal ele2 . Therefore , the optimization problem ( i+1)2,1
,1 ,
,1 where , ment is in Eq ( 10 ) is equivalent to
1
; arg minLflV tr
,1 + UTLUL )
,1 ,
,1
( ,
;
( 16 )
(
) where UL = ST U is a submatrix of U . More specially , UL consists of the rows in U which corresponds to the selected nodes . ,1 . Suppose k nodes have been selected , Let H0 = , ∈ Rkn Let denoted by Lk , which correspond to ULk ) ULk , then the ( k + 1) th node can be Hk = , selected by solving the following optimization problem
,1 + UTLk
( tr
( Hk + uiuT i )
,1 ,
,1
;
( 17 ) ik+1 = arg min iflV=Lk where ui is the transpose of the i th row of U ( thus a column vector ) .
By using the Sherman Morrison formula [ 9 ] , we have
( Hk + uiuT i )
,1 = H
,1 k
− H
,1 k uiuT i H
,1 i H k ,1 k ui
1 + uT
:
( 18 )
( Hk + uiuT i ) ,1 ) − uT ,1 k ,
,1 ,1 , ,1 i H k , 1 + uT
,1 k ui
,1H ,1 i H k ui
= tr(H
:
( 19 )
2Since the smallest eigenvalue of graph Laplacian is 0 , ,
,1 is illdefined . In our implementation , we resolve this problem by replacing the zero eigenvalue with a sufficient small value , eg , 1e , 6 .
(
Therefore , tr
)
,1 k , the optimization
,1 k ,
,1 ) is a constant given H
Since tr(H problem in Eq ( 17 ) is equivalent to ,1 i H k , 1 + uT ik+1 = arg max iflV=Lk uT
,1 k ui
,1H ,1 i H k ui ,1 k+1 can be updated ,1 k , by using the Sherman Morrison formula
( 20 )
:
Once the ( k+1) th node is selected , H based on H again ,
,1 k+1 = H
,1 k
H
− H
,1 k uik+1uT H
H
,1 ik+1 k ,1 k uik+1 ik+1
1 + uT
:
( 21 )
,1 k+1 is updated by matrix ( vector ) multiplication this
Note that H and addition , rather than matrix inverse . Therefore , process is efficient .
In summary , we present the whole algorithm for active learning on graphs in Algorithm 1 .
Algorithm 1 Active Learning on Graphs via Generalization Error Bound Minimization ( Bound )
2
, 1
, 1
Input : Adjacency matrix W , number of nodes to select l , regularization parameter ; Compute L = I − D 2 WD Perform eigen decomposition L = UfiUT Initialize H0 = , for k = 0 → l − 1 do Compute ik+1 = arg maxiflV=Lk Update Lk+1 = Lk ∪ {ik+1} − H Update H i H ,1 ,1 k uik+1 uT ik+1 k ,1 1+uT k uik+1 H
,1 , L0 = ∅
,1 k+1 = H
,1H ,1 k ui
,1 k fi 1+uT
,1 k ui
,1 k i H uT
H
; ik+1 end for
C . Complexity Analysis
The computational complexity of Algorithm 1 includes is eigen decomposition of the two parts . The first part adjacency matrix W . For a graph whose average node degree is k , the Lanczos algorithm [ 9 ] can be used to efficiently compute the eigenvectors of the eigen problem within O(tn2k ) , where t is the number of iterations in Lanczos . The second part is the sequential optimization algorithm , whose complexity is O(n2l ) where l is the number of selected nodes , ie , |L| . Hence the total time complexity is O(n2(tk+l) ) , which is applicable to mediumscale graphs .
IV . EXPERIMENTS
In this section , we evaluate the proposed method on realworld datasets , and compare it with the state of the art active learning methods on graphs . Recall that the input of active learning methods on graphs is an adjacency matrix .
A . Datasets
C . Experimental Setup
In our experiments , we use three real world benchmark datasets to evaluate the active learning methods . Cora contains the abstracts and references of about 34 ; 000 research papers from the computer science community . The task is to classify each paper into one of the subfields of data structure ( DS ) , hardware and architecture ( HA ) , machine learning ( ML ) , and programming language ( PL ) , based on the citation relation between the papers . We only use the link information of this dataset . We choose DS and PL subsets to form two datasets . For each dataset , the largest connected component of the graph is used . Since the adjacency matrix of the Cora dataset is directed , we symmetrize it by max(W ; WT ) . Coauthor is an undirected co author graph data extracted from the DBLP3 database in four areas : machine learning , data mining , information retrieval and database . It contains a total of 1711 authors , each of which is represented by a node . The edge between each pair of authors is weighted by the number of papers they co authored . Each class contains about 400 authors . This graph is already connected .
B . Methods & Parameter Settings
To demonstrate the effectiveness of our proposed method , we compare the following active learning approaches . ffl Random Sampling ( Random ) uniformly selects nodes from the candidate set . It is the simplest baseline for active learning . ffl Variance Minimization ( VM ) [ 13 ] is a recently proposed method , which is motivated by GFHF and minimizing the prediction variance . ffl METIS [ 11 ] : it uses the METIS clustering method [ 14 ] to divide a graph into l clusters , and randomly chooses one data point from each cluster . ffl ) Maximization ( ) Max ) [ 10 ] : it is solved by submodular function maximization , which performs better than the heuristic optimization algorithm proposed in [ 11 ] . ffl Generalization Error Bound Minimization ( Bound ) is our proposed method . It is motivated by Theorem 5 . There is one parameter tunable . Throughout our experiments , we simply fix = 0:01 .
After selecting the nodes by active learning , we train a classifier on the graph to do classification . In our experiments , we tried three classifiers : LLGC , GFHF and MinCut . The reason why we tried these three classifiers is obvious , because the proposed active learning method is built upon LLGC , VM is motivated by GFHF and ) Max is designed for MinCut . There is a parameter for LLGC , ie , , which is tuned by 3 fold cross validation on the selected labeled set over the grid {0:01 ; 0:1 ; 1 ; 10 ; 100} .
3wwwinformatikuni trierde/ ley/db/
In order to randomize the experiments , in each run of experiments , we restrict the pool of the candidate nodes to be selected from a random sampling of 50 % of the total nodes . The random split was repeated 10 times . For each dataset , we let the active learning methods incrementally choose {10 ; 20 ; : : : ; 160} nodes from the training set to label . We evaluate different active learning methods combined with different classifiers . We compute the mean classification accuracy on all the unlabeled nodes , that is , the unselected nodes in the pool plus the remaining 50 % nodes .
D . Classification Results
The experimental results evaluated on the unlabeled data are shown in Figures 1 and 2 . In all subfigures , the horizontal axis represents the number of labeled nodes , while the vertical axis is the averaged classification accuracy over 10 runs . The experimental result of MinCut is much worse than LLGC and GFHF for all the active learning methods , because it usually results in a very unbalanced classification . Therefore , we omit its result .
From Figures 1 and 2 , we observe that the proposed method ( Bound ) consistently outperforms other methods in most cases using either LLGC or GFHF . It is appealing because even though our method is built upon the error bound minimization of LLGC , it is also much better than other methods using GFHF . But note that our method using LLGC achieves marginally better performance than using GFHF . On the Cora datasets , when the number of labeled nodes is small , eg , less than 30 , our method and ) Max usually perform the best . On the other cases , our method is much better than the second best method . The superior performance of our method is attributed to its theoretical foundation , which guarantees that the classifier can achieve small generalization error on the unlabeled data .
VM is usually worse than random sampling . The reason is that minimizing the prediction variance does not guarantee the quality of predictions on the unlabeled data .
The performance of METIS is usually comparable to or even better than that of ) Max . Although METIS has a solid theoretical foundation , the corresponding criterion is so difficult that we have to solve it by a clustering algorithm [ 14 ] followed by a heuristic sampling , which sacrifices its performance .
In summary , our method together with LLGC is the most promising combination , which is consistent with our theory .
V . CONCLUSIONS AND FUTURE WORK
The main contributions of this paper are : ( 1 ) We present a generalization error bound for LLGC ; ( 2 ) we present an active learning criterion for graph data via minimizing the empirical transductive Rademacher complexity of LLGC ; and ( 3 ) we present a simple algorithm to optimize the active
( a ) Cora ( DS )
( b ) Cora ( PL )
( c ) Coauthor
Figure 1 . Comparison of active learning methods on ( a ) Cora ( DS ) ; ( b ) Cora ( PL ) ; and ( c ) Coauthor using LLGC evaluated on all the unlabeled data .
( a ) Cora ( DS )
( b ) Cora ( PL )
( c ) Coauthor
Figure 2 . Comparison of active learning methods on ( a ) Cora ( DS ) ; ( b ) Cora ( PL ) ; and ( c ) Coauthor using GFHF evaluated on all the unlabeled data . learning criterion . In the future , we plan to develop a more scalable algorithm to solve Eq ( 10 ) .
ACKNOWLEDGEMENT
The work was supported in part by US National Science Foundation grants IIS 0905215 , CNS 0931975 , the US Army Research Laboratory under Cooperative Agreement No . W911NF 09 2 0053 ( NS CTA ) , the US Air Force Office of Scientific Research MURI award FA9550 08 1 0265 , and MIAS , a DHS IDS Center for Multimodal Information Access and Synthesis at UIUC .
REFERENCES
[ 1 ] M F Balcan , A . Beygelzimer , and J . Langford . Agnostic active learning . In ICML , pages 65–72 , 2006 .
[ 2 ] P . L . Bartlett and S . Mendelson . Rademacher and gaussian complexities : Risk bounds and structural results . Journal of Machine Learning Research , 3:463–482 , 2002 .
[ 3 ] M . Bilgic , L . Mihalkova , and L . Getoor . Active learning for networked data . In ICML , pages 79–86 , 2010 .
[ 4 ] A . Blum and S . Chawla . Learning from labeled and unlabeled data using graph mincuts . In ICML , pages 19–26 , 2001 .
[ 5 ] N . Cesa Bianchi , C . Gentile , F . Vitale , and G . Zappella . Active learning on trees and graphs . In COLT , pages 320– 332 , 2010 .
[ 6 ] F . R . K . Chung . Spectral Graph Theory . American Mathe matical Society , February 1997 .
[ 7 ] D . A . Cohn , L . E . Atlas , and R . E . Ladner . Improving generalization with active learning . Machine Learning , 15(2):201– 221 , 1994 .
[ 8 ] R . El Yaniv and D . Pechyony . Transductive rademacher complexity and its applications . J . Artif . Intell . Res . ( JAIR ) , 35:193–234 , 2009 .
[ 9 ] G . H . Golub and C . F . V . Loan . Matrix computations ( 3rd ed ) Johns Hopkins University Press , 1996 .
[ 10 ] A . Guillory and J . Bilmes . Active semi supervised learning using submodular functions . In UAI , pages 274–282 , 2011 . [ 11 ] A . Guillory and J . A . Bilmes . Label selection on graphs . In
NIPS , pages 691–699 , 2009 .
[ 12 ] X . He , W . Min , D . Cai , and K . Zhou . Laplacian optimal design for image retrieval . In SIGIR , pages 119–126 , 2007 . [ 13 ] M . Ji and J . Han . A variance minimization criterion to active learning on graphs . AISTATS , pages 556–564 , 2012 .
[ 14 ] G . Karypis and V . Kumar . A fast and high quality multilevel scheme for partitioning irregular graphs . SIAM J . Sci . Comput . , 20(1):359–392 , Dec . 1998 .
[ 15 ] S . Tong and D . Koller . Support vector machine active learning with applications to text classification . In ICML , pages 999– 1006 , 2000 .
[ 16 ] K . Yu , J . Bi , and V . Tresp . Active learning via transductive experimental design . In ICML , pages 1081–1088 , 2006 .
[ 17 ] D . Zhou , O . Bousquet , T . N . Lal , J . Weston , and B . Sch¨olkopf . Learning with local and global consistency . In NIPS , 2003 . [ 18 ] X . Zhu , Z . Ghahramani , and J . D . Lafferty . Semi supervised In learning using gaussian fields and harmonic functions . ICML , pages 912–919 , 2003 .
20406080100120140160404550556065707580#Labeled dataAccuracy RandomVMMETISY MaxBound204060801001201401602025303540455055606570#Labeled dataAccuracy RandomVMMETISY MaxBound2040608010012014016040455055606570#Labeled dataAccuracy RandomVMMETISY MaxBound20406080100120140160404550556065707580#Labeled dataAccuracy RandomVMMETISY MaxBound20406080100120140160203040506070#Labeled dataAccuracy RandomVMMETISY MaxBound204060801001201401603540455055606570#Labeled dataAccuracy RandomVMMETISY MaxBound
