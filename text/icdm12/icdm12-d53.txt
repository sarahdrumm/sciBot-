Learning Target Predictive Function without Target Labels
School of Computer Engineering , Nanyang Technological University , Singapore , 639798
Chun Wei Seah , Ivor Wai Hung Tsang , Yew Soon Ong , Qi Mao Email : {seah0116,IvorTsang,ASYSOng,qmao1}@ntuedusg
Abstract—In the absence of the labeled samples in a domain referred to as target domain , Domain Adaptation ( DA ) techniques come in handy . Generally , DA techniques assume there are available source domains that share similar predictive function with the target domain . Two core challenges of DA typically arise , variance that exists between source and target domains , and the inherent source hypothesis bias . In this paper , we first propose a Stability Transfer criterion for selecting relevant source domains with low variance . With this criterion , we introduce a TARget learning Assisted by Source Classifier Adaptation ( TARASCA ) method to address the two core challenges that have impeded the performances of DA techniques . To verify the robustness of TARASCA , extensive experimental studies are carried out with comparison to several state of the art DA methods on the real world Sentiment and Newsgroups datasets , where various settings for the class ratios of the source and target domains are considered .
I . INTRODUCTION
In the situation when a new domain , referred to as target domain , has limited labeled data or even no labeled sample on hand , Domain Adaptation ( DA ) techniques come in handy . Generally , DA techniques assume there are available source domains that share similar predictive function with the target domain [ 1 ] , [ 2 ] .
A natural question for DA is how to select relevant source domains for predicting the target data since source and target domains have their own unique distributions . Although , a classifier that well fits for the source domains , referred to as source classifier , can be used to predict the class labels of the target data , the accuracy performance may fail miserably on the target data . This is due to the change in data distribution P(x ) between the source and target data . Here we refer this distribution difference to as variance [ 3 ] . In particular , when variance is low , one can confidently use a stable source classifier1 to robustly predict the target data . However , due to the absence of labeled data in the target domain , the common practice of using standard cross validation approaches to choose a stable source classifier [ 4 ] does not work here anymore . Apart from this , another core challenge of DA is that a predictive function trained on the source samples steers towards the hypotheses and the class distribution of the source domains . Taking Support Vector Machines ( SVM ) as an example in the context of DA [ 5 ] , [ 6 ] , its classifier learns the kernel expansion and the bias term based on
1A source classifier with the lowest estimated variance on the target domain source labeled samples or their re weighted samples instead on the target samples . Thus the predictive function in this form of classifier lies in a hypothesis space different from that of the target domain , leading to deteriorating prediction performance on unseen target data , especially when the class distributions of the source and target domains differ significantly from each other [ 7 ] , [ 8 ] . We uniformly refer this phenomenon to as source hypothesis bias .
Taking this cue , we propose TARget learning Assisted by Source Classifier Adaptation ( TARASCA ) method to address the two challenges in the absence of target labeled data , but with many source domains and target unlabeled data at hand .
II . RELATED WORK
Among the early works of DA , [ 9 ] considered a strict assumption on the source and target domains that share a common joint distribution of the input data x and the corresponding label y . Alternatively , by leveraging from multiple source domains , [ 6 ] formulates the target predictive function by averaging multiple SVM classifiers learned from each individual domain , is known as Multiple Convex Combinations ( MCC ) . to arrive at what
Nevertheless , the true joint distributions of the target and source domains typically differ , hence [ 5 ] relaxes the assumption of having common joint distributions by relaxing the marginal ( input data ) distributions to be different . To rectify the importance of each source sample , the authors introduced the Kernel Mean Matching ( KMM ) method according to the Maximum Mean Discrepancy ( MMD ) criterion [ 10 ] , which estimates the marginal distribution difference of the source and target domains using their unlabeled data ; the re weighted source samples are subsequently used to train a classifier for predicting the target unseen data . A plethora of DA approaches [ 11 ] , [ 12 ] , [ 13 ] , [ 14 ] , [ 15 ] , [ 16 ] have also attempted to learn the target predictive function from the source samples . For example , [ 13 ] described a Transfer Component Analysis ( TCA ) method that re weights the features of the predictive function using the target unlabeled data based on the MMD criterion .
From a survey of the literature of related DA works , a predictive function is usually defined in the form of :
∑ bK(xs f ( x ) =
( 1 ) where X s represents the data from the source domain , i denotes the bK( . , . ) is an enhanced kernel function , αs i , x ) + bs
∈X s
αs i xs i i , x ) and bs is a bias parameter . From ( 1 ) , existing DA methods generally focus on learning predictive weight of bK(xs a kernel function bK that embeds well the source and target X s with the learned bK . Thus , source hypothesis bias still data , and then train a classifier using the source samples inevitably arises when the classifier is mainly trained using X s which has class distribution different from that of the target domain . In contrast , the proposed TARASCA focuses on learning the class labels of the target unlabeled data , and its kernel expansion and the bias term is learned based on the target unlabeled data only .
Most recently , [ 17 ] , [ 7 ] , [ 18 ] , [ 19 ] directly learn the prediction on the target unlabeled data . However , their methods are designed for transductive learning , ie , labeling the class labels of the given target unlabeled data , but cannot be used for unseen data . In contrast , the proposed TARASCA can be used for classifying unseen data .
III . LEARNING TARGET PREDICTIVE FUNCTION
WITHOUT TARGET LABELS
′ ) with fl fl Cf
∗ as follows : ∗ − 1 2
∥w∥2
′
In this paper , the task of Domain Adaptation ( DA ) is to learn a target predictive function f that exploits the kernel expansion of a given set of unlabeled data XU belonging to the target domain for predicting the label of any unseen target data x . The core symbols used throughout the rest of this paper are listed in Table I . In order to learn f from a set of unlabeled data XU , we ∗ of ideal confidence assume that there exists a vector fl ∗ , values of class labels for the unlabeled dataset . Using fl we can learn a target predictive function f ( x ) = w ϕ(x ) by maximally aligning f for each target unlabeled data ( ie , f = [ f ( x1 ) , , f ( xu ) ]
′
2 w max
∥w∥2
However , in practice , fl
( 2 ) 2 is a regularizer to avoid overfitting and C is where 1 2 the tradeoff parameter for controlling the complexity of f . ∗ is unknown . To address this , we ∗ from the set of precomputed source propose to estimate fl classifiers . Figure 1 depicts the learning process of the target predictive function from the kernel expansion of XU assisted by source classifiers adaptation . First , a Stability Transfer criterion is defined for the selection of source domains with low variance as detailed in Section III A . Then in Section III B , the weights of the kernel expansion defined on XU of the target domain are directly learned such that the inherent source hypothesis bias in the predictive function can be minimized . Note that this is in stark contrast to existing methods , which attempt to learn the weight of the kernel expansion defined on source data as given by ( 1 ) .
A . Selecting relevant source domains with low variance
According to [ 20 ] , even when the estimated generalization error bound is small , some source classifiers may exhibit poor prediction accuracy on the target domain since the
Symbol
Definition m Total number of source domains S
Table I
Symbol Definition
∑ m! m i=1 i!(m−i)!
Number of unique combinations form using m source domains , S = Number of labeled data in the sth combined source domain . the ith labeled data in sth domain The set of unlabeled data in target domain the ith data of XU u number of unlabeled data exists in XU return ,1 if a < 0 , otherwise +1 . defines a vector of u elements with all ones superscript fls=[f s(x1 ) ; :: : ; f s(xu ) ] where f s is defined in ( 1 ) on the sth combined source domain . the ith bias of f s( : ) projects on XU where i 2 f1 ; :: : ; Zg(see Algorithm 1 ) i = fls , bs fls denotes the transpose of a vector
′
′ i ns xs XU i xi u sign(a ) 1 ′ fls bs i fls i
Figure 1 . TARget learning Assisted by Source Classifier Adaptation true distribution of the target domain is unknown [ 21 ] . Typically , an ideal target prediction should have low variance and minimal source hypothesis bias . To achieve this , we first analyze the target generalization error based on the source and target data . We define the joint distributions of the sth source domain and the target domain as P s and P t , respectively . Similarly , the marginal distributions of the sth source domain and the target domains as Ds and Dt , respectively . Then , expected errors of the sth source domain ϵs(h ) = E(x;y)∼P sI[h(x ) ̸= y ] and the target domain ϵt(h ) = E(x;y)∼P tI[h(x ) ̸= y ] where hypothesis h : x → {−1 , 1} , and I[ . ] returns 1 if the predicate holds , otherwise a zero is returned . Note , I[h(x ) ̸= y| is considered as zero one loss function . In addition , given two hypotheses , h1 and h2 , we define ϵt(h1 , h2 ) = Ex∼DtI[h1(x ) ̸= h2(x) ] . Theorem 1 . Let a hypothesis space be H . Then , from [ 20 ] , for a hypothesis h , the target generalization error bound is : ( 3 )
ϵt(h ) ≤ ϵt(h ∗
∗ ) + ϵt(h , h dH∆H(Ds,Dt ) + λs 1 2 ∗ ) , h
≤ ϵs(h ) + ∗ ∗ where λs = ϵs(h ) + ϵt(h ) + ) and dH∆H(Ds,Dt ) = 2supA∈A|PDs ( A ) − PDt(A)| ∗ ϵt(h as the H∆H distance between Ds and Dt . Here , A is a collection of subsets of {x : x ∈ X , h1(x ) ̸= h2(x)} for some h1 , h2 ∈ H and |.| is an absolute operator . Generally , DA methods assume that λs in ( 4 ) is reasonably
∗ = arg minhfi∈H ϵs(h
( 4 )
)
Step 1 : Select relevant source domains with low varianceUnlabeleddataStep 2:Learn the target predictive function with low source hypothesis biasUnlabeled dataStabilityTransferCriterionTarget Predictive FunctionSourcedomains with low varianceSourceDomain 1SourceDomain mUnlabeled data in Target DomainUnseendata in Target DomainTarget unseen data xf(x ) small [ 21 ] , which means that there exists a hypothesis ∗ that can classify well on the both source and target h domains . Based on this assumption , if the H∆H distance dH∆H(Ds,Dt ) between a source domain Ds and the target domain Dt is small enough , then a hypothesis h ( classifier ) that minimizes the expected error on the source domain Ds , should attain good generalization on the target domain Dt . Therefore , instead of choosing a source classifier directly , we should firstly choose a source domain close to the target domain , which motivates the following Stability Transfer criterion . Definition 2 . Stability Transfer : Stability Transfer is defined as leveraging a source domain having low variance ( i.e , small dH∆H(Ds,Dt ) ) to learn a robust predictive function f for the target domain . To achieve stability transfer , based on Definition 2 , we need to choose a source domain Ds whose dH∆H(Ds,Dt ) is sufficiently small . Hereby , we denote Ds as a stable source domain . However , estimating dH∆H(Ds,Dt ) is non trivial , [ 22 ] gives a bound on dH∆H(Ds,Dt ) as : Theorem 3 . Let H be a hypothesis space of VCdimension v . When U s and U t are unlabeled samples of √ size η each , drawn from Ds and Dt , respectively , then with probability at least 1 − δ dH∆H(Ds;Dt ) dH∆H(U s;U t ) + 4 Later , estimate dH∆H(U s,U t ) by training a linear classifier , ψ , using a standard hinge loss and assigning U s with a negative class label and U t with a positive class label . Since larger size of η contributes to a lower value of upper bound for dH∆H(Ds,Dt ) in Theorem 3 , η is set as max(n1 , , ns , , nS , u ) ∀s = 1 , , S . Next , U s and U t are formed by sampling η number of data from each source domain and the target domain , respectively . Then dH∆H(U s,U t ) is estimated by :
( 1/2η)(Ix∈U s [ ψ(x ) = −1 ] + Ix∈U t[ψ(x ) = 1] ) .
( 6 ) In order to have a stable estimate of ( 6 ) , q number of independent runs are carried out and the average result is used . In our experiment , q is set as 10 . Intuitively , a lower value of ( 6 ) means the source and target domains are less separable , thus the H∆H distance between the source and target domains is deemed to be smaller . Then we denote the chosen source domain that has the smallest estimated value of ( 6 ) ( among all the S source domains ) as the pth source domain . empirically vlog(2 ) + log 4 ffi proposed
[ 20 ]
( 5 ) to
:
B . Learning the target predictive function with minimal source hypothesis bias
According to Theorem 1 , a predictive function that minimizes the expected error on the pth source domain should attain good generalization performance on the target domain Dt . However , as discussed in the Introduction , even a predictive function is trained from a stable source domain , it still steers towards the hypotheses and the class distribution of the source domains , and the source hypothesis bias will inevitably arise . To address such bias , we propose to use source classifiers2 from a stable source domain to give predictive values flp on the target unlabeled data XU first , and then learn a target predictive function f on directly on XU using the objective in ( 2 ) . As a result , learning a target predictive function f based on the target unlabeled data becomes feasible .
′
However , the class distribution of the target domain is still unknown . Hence , in order to address source hypothesis bias , we need to design f to be robust to various possible class ratios of XU using the following objective : flp i f w max
( 7 ) min ∈fl p flp ′ . This objective is motivated i where f = [ f ( x1 ) , , f ( xu ) ] by the worst case strategy [ 23 ] . Specifically , given a pth source classifier that gives the predictive values flp of XU , ( 7 ) maximally aligns f to flp i represents a unique class ratio of XU . Since the class ratio of the target domain is unknown , the ideal class ratio of XU using flp flp i , so f is learned through may appear to be arg minfl p the worst case condition ( ie , minflp i = flp−bp i where bp
Together with the regularizer , we learn a target predictive flp i ) . f f
′
′ i i
{
′
}
ϕ(x ) as follows : fls i f
′ function f ( x ) = w
∥w∥2
− 1 2 i
2 w
C
∈fi max min ∀s∈ℑ;fl s
Based on the minimax theory , ( 8 ) can be reformulated as :
( 8 ) where ℑ = {1 , , S} and fi is a set of predictive values of XU that are given by S source classifiers , which will be described in Section III C . Since one can use the Stability Transfer measure proposed in Section III A to choose multiple stable source domains , we provide a more general form in ( 8 ) that uses all s ∈ ℑ , instead of just showing the pth source domain . In the experiment , for simplicity , only the pth source domain is used in ( 8 ) . min;w −Cθ + 1 ||w||2 θ ≤ f 2 fls ∑ ∑ ∑ i , ∀s = 1 , , S and ∀i = 1 , , Z . ∑ Then , the dual form of ( 9 ) is written as follows : maxfi − 1 ≥ 0 , ∀s = 1 , , S and ∀i = 1 , , Z , ( 10 ) where βs is the Lagrangian multiplier . It is worth noting i that ( 10 ) can be solved by means of quadratic programming . Upon solving , the predictive output is derived as : u i=1 αiK(xi , x )
Z i βq i;j=1 βs j i = C , βs i u r;v=1 K(xr , xv)γs
∑
∑
ϕ(x ) = f ( x ) = w Z j=1 βs j γs where αi = Then , the class label of x is defined as sign(f ( x) ) .
′ ji and K(xi , x ) = ϕ(xi )
S s=1
( 11 ) ϕ(x ) .
∑
∑
Z i=1 βs
2 S s=1
S s;q=1 irγq
( 9 ) st s.t jv
′
′
2
2When a source classifier performs poorly on the pth source domain based on empirical error , another source classifier can be chosen . For simplicity , this situation is not being considered .
′
Algorithm 1 Generating predictive values on the target unlabeled data from source classifiers 1 : Inputs : XU , F ( a set of source classifiers ) , ( defines the class ratio of the lower bound and upper bound of positive samples in XU ) fls = [ f s(x1 ) ; :: : ; f s(xu ) ] //impose the class ratio constraint by sorting indexes=sort(fls ) in descending order i = 0 ; for q = u to u(1 , ) do
2 : Outputs : fi ( a set of predictive values for XU ) 3 : for all f s 2 F do 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : i = i + 1 ; // defines a bias based on the qth indexes of the XU i = f s(xindexes(q) ) ; bs // let the first q indexes of the predictive values of the samples as positive and the rest as negative i = fls , bs i ; fls fi = fi [ fls i ;
13 : 14 : 15 : 16 : end for 17 : return fi end for
∑
From ( 11 ) , the predictive function is formulated using the kernel expansion defined on XU . Since TARASCA directly learns the weights αi of the kernel expansion of XU of the target domain , the source hypothesis bias of the predictive function is minimized ( in contrast to existing DA methods that learn the weight of the kernel expansion defined on each source sample as shown in ( 1) ) . In addition , the bias parameter of the predictive function is implicitly learned from XU when fls j is generated using Algorithm 1 ( will be described in the next subsection ) . C . Generating prediction on XU with various class ratios A classifier can be trained for each individual source domain and also for each unique combination of 2 , 3 , , ( m− 1 ) source domains , until all m source domain are combined . These unique combinations give rise to S = i!(m−i)! classifiers being formed . However , each of the sth classifier normally includes a bias bs in its predictive function , taking i , x)+bs where ns SVM for example , f s(x ) = is the number of labeled samples in the sth combined source domain , αs i , x ) with the sign of αs i . However , the bias bs is learned from the source domain , which typically do not represent the class distribution of the target domain . Hence , we propose to learn the bias from XU by imposing a balance constraint using a parameter σ to control the class ratio of positive samples in XU : resulting in Z = u − 2σu ways of imposing the class ratio constraint . For example , with σ = 0.3 and u = 100 , then 30 ≤ 1 ( sign(f s(XU ) ) + 1)/2 ≤ 70 implies the number of positive samples in XU is at least 30 and at most 70 . This class ratio constraint can be implicitly fulfilled by sorting
( sign(f s(XU ) ) + 1)/2 ≤ u(1 − σ ) , i indicating the sign for the label of xs i denotes the weight for K(xs uσ ≤ 1
∑ ns i αs i K(xs m i=1 m!
′
′ the decision values of XU produced by each sth classifier , as described in Algorithm 1 . Notice that diverse forms of precomputed classifiers can be trained using SVM , Transductive SVM [ 24 ] , Gaussian Process , or other supervised , semi supervised and DA methods . For simplicity , we only consider SVM in the present study .
IV . EXPERIMENTAL STUDY
In this section , we first describe the evaluation measures being used in the experimental study and then introduce the DA experimental datasets , the state of the art algorithms considered here for comparison study and the settings of the parameters in the algorithms . In order to evaluate the performance of testing data with imbalanced class ratio , Area under the ROC Curve ( AUC ) measure is commonly used [ 25 ] and is adopted in the experimental study .
A . Settings of Experimental Datasets
In practice , the true class distribution of the target domain is unknown . Hence , we analyze the effects of imbalanced class ratio in the source and target domains towards different learning algorithms , by defining a term known as the Target Positive Class Ratio ( TPCR ) to denote the number of positive samples in the target domain . For example , a TPCR of 0.7 in a set of 1000 target samples has 700 positive samples and the rest are negative samples . In the experimental study , TPCR values of 0.3 , 0.5 and 0.7 are investigated . Similarly , the term Source Positive Class Ratio ( SPCR ) is also used to denote the number of positive samples in the source domain . In the experimental study , the robustness of different stateof the art algorithms for different SPCR values of 0.2 , 0.4 , 0.6 and 0.8 are investigated . To accommodate the TPCR settings , each of the target unlabeled and target testing datasets has 600 samples randomly chosen . While the sample size of the source labeled dataset is assumed to be lesser than that of the target unlabeled dataset so only 100 samples are randomly chosen for each source labeled dataset . Each task is repeated 10 times and the average AUC is reported .
In the experimental study , the datasets are pre processed by extracting single and bi terms , removing stopwords , and normalizing and stemming each feature . Consequently , each feature of the sample is represented by its respective tf idf value ; thus , a linear kernel is employed .
1 ) Settings of Newsgroups Dataset : Newsgroups Dataset consists of three main categories : comp , rec , and sci . The subcategories of each main category are grouped into a target domain and three source domains according to their lexicographical order and three tasks are formed : comp vs . rec , comp vs . sci and rec vs . sci . These datasets are commonly used in DA papers [ 26 ] .
2 ) Settings of Real world Sentiment Dataset : A common real world application of transfer learning problem is Sentiment prediction . Four Sentiment domains from Amazon.com are provided by [ 27 ] : Book , DVDs , Electronics , and Kitchen appliances . In the dataset , 1 star and 2 star ratings form the negative samples and 4 star and 5 star ratings made up the positive samples . For each task , one domain is posed as the target domain while the rest as related source domains .
B . Setting of the state of the art algorithms
In the present study , the state of the art algorithms used to investigate on each task ( one target and three source domains ) are briefly described as follows : 1 ) 1S SVMBest : A SVM is trained on each source domain and the best AUC among the three SVM classifiers is reported as 1S SVMBest . 2 ) 2S SVMBest : [ 6 ] proposed a DA approach that linearly combines various number of source domains and then trains them using SVM . Among the three source domains , each unique pair is trained using SVM and the classifier with the best AUC is then reported as 2S SVMBest . 3 ) MCC : Multiple Convex Combinations [ 6 ] uses all source domains to train a SVM . 4 ) KMMBest : Kernel Mean Matching re weight each of the source samples based to address the marginal distribution differences between a single source domain and a target domain . Then , a weighted SVM is trained on the weighted source samples . One KMM is trained for each source domain and the best AUC among the classifiers is reported . 5 ) TCABest : Transfer Component Analysis learns a set of transfer components and then a SVM is trained on the source domain using the discovered feature map [ 13].One TCA is trained for each source domain and the best AUC among the classifiers is reported . 6 ) TARASCA : The proposed method , TARget learning Assisted by Source Classifier Adaptation , as described in Figure 1 .
The parameters of all methods are configured by means of k fold cross source domains validation [ 28 ] , which is an extension of the standard k fold cross validation for DA . Here , k is the number of source domains with each kth partition representing a source domain . In addition , σ is fixed as 0.3 in TARASCA .
V . AUC RESULTS ON NEWSGROUPS AND SENTIMENT From Figure 2 , the Sub Figures among each task ( row ) are observed to exhibit similar trends in AUC for every methods , thus showing the consistency of AUC in evaluating different class ratios of testing sets . Note , due to space constraint , only selected datasets are shown in Figure 2 . In general , most algorithms have their AUC results peak at either SPCR values of 0.4 or 0.6 , and their performances deteriorate at both extreme ends of SPCR , except for the proposed method TARASCA . In Figure 2(b ) , KMM is notable to degrade the performance as compared to 1S SVMBest for some SPCR values . Furthermore , in SubFigures 2(c and d ) , KMM fails to work in all settings considered . This implies that reweighting the importance of each source sample hurts the performance as compared to treating all samples equally , when the assumption on source and target domains sharing similar predictive distribution does not hold well . On the other hand , since TCA learns a set of transfer components , in general , it is observed to perform better than 1S SVMBest in Figure 2 . This is mainly due to the transfer components that consider the target unlabeled data . However , the performance of TCA deteriorates when the SPCR value approaches either extreme ends since the learned predictive function is biased towards the source domain . On the other hand , TARASCA addresses the challenges of both the KMM and TCA significantly well with robust AUC results reported across the entire range of TPCR and SPCR settings .
Furthermore , in Figure 2 , TARASCA is observed to outperform 2S SVMBest and MCC ; this is mainly due to TARASCA automatically selects source classifier whereas 2S SVMBest and MCC are both ensemble methods that treat each source domain equally ( no selection of source classifier ) . In SubFigures 2(a ) , 1S SVMBest reports much higher AUC results than both 2S SVMBest and MCC . This implies TARASCA is capable of selecting useful source domain under the situation where some source domains may hurt the target performance .
TARASCA generally emerges as superior to all other methods in terms of AUC as reported in Figure 2 . This is mainly due to the design of TARASCA such that the kernel is defined on the target samples in its predictive function for allowing the kernel expansion of the target domain to be exploited ( as opposed to all other methods with their predictive functions form using the kernel defined on the source samples ) . In addition , TARASCA learns the weights of kernel defined on the target samples using the stable source classifier ( as opposed to KMM which learns the weight of source samples using the target samples ) ; thus , TARASCA addresses the challenges of both the variance and source hypothesis bias through its designs of the target predictive function that is robust to the class ratio of the target domain .
VI . CONCLUSION
In this paper , two core challenges of DA typically are identified , variance that exists between source and target domains and the inherent source hypothesis bias . To address these core challenges , we introduce a TARget learning Assisted by Source Classifier Adaptation ( TARASCA ) method . In the experimental study , TARASCA emerges as superior to several state of the art DA methods . Thus , TARASCA serves as an indispensable tool to address the two core challenges in their designs of the target predictive function . In addition , unlike standard cross validation methods , which need plenty of labeled data for model selection , the proposed Stability Transfer criterion uses target unlabeled data to select a stable model which serves as a tool to speedup any potential algorithms . c e r
. s v p m o c i c s
. s v c e r s c i n o r t c e l E s e c n a i l p p A n e h c t i
K
( a1 )
( a2 )
( b1 )
( b2 )
( c1 )
( c2 )
( d1 )
TPCR=0.3
( d2 )
TPCR=0.5
Figure 2 . AUC results of various approaches perform across different target positive class ratio ( TPCR ) .
ACKNOWLEDGEMENT
This research was in part supported by Singapore NTU A* SERC Grant ( 112 172 0013 ) and Multi plAtform Game Innovation Centre ( MAGIC ) . MAGIC is supported by the Interactive Digital Media Programme Office ( IDMPO ) hosted by the Media Development Authority of Singapore . IDMPO was established in 2006 under the mandate of the National Research Foundation to deepen Singapores research capabilities in interactive digital media ( IDM ) , fuel innovation and shape the future of media .
[ 1 ] K . Crammer , M . Kearns , and J . Wortman , “ Learning from
Multiple Sources , ” JMLR , vol . 9 , pp . 1757–1774 , 2008 .
[ 2 ] S . B . David , J . Blitzer , K . Crammer , A . Kulesza , F . Pereira , and J . W . Vaughan , “ A theory of learning from different domains , ” ML , vol . 79 , pp . 151–175 , 2010 .
[ 3 ] T . G . Dietterich and E . B . Kong , “ Machine learning bias , statistical bias , and statistical variance of decision tree algorithms , ” ML , vol . 255 , pp . 0–13 , 1995 .
[ 4 ] E . Zhong , W . Fan , Q . Yang , O . Verscheure , and J . Ren , “ Cross Validation Framework to Choose amongst Models and Datasets for Transfer Learning , ” in ECML/PKDD , 2010 .
[ 5 ] J . Huang , A . Smola , A . Gretton , K . M . Borgwardt , and B . Sch¨olkopf , “ Correcting Sample Selection Bias by Unlabeled Data , ” in NIPS , 2006 .
[ 6 ] G . Schweikert , C . Widmer , B . Sch¨olkopf , and G . R¨atsch , “ An Empirical Analysis of Domain Adaptation Algorithm for Genomic Sequence Analysis , ” in NIPS , 2009 .
[ 7 ] L . Bruzzone and M . Marconcini , “ Domain Adaptation Problems : A DASVM Classification Technique and a Circular Validation Strategy , ” IEEE TPAMI , vol . 32 , no . 5 , pp . 770– 787 , 2010 .
[ 8 ] C W Seah , I . W . Tsang , Y S Ong , and K K Lee , “ Predictive Distribution Matching SVM for Multi domain Learning , ” in ECML/PKDD , 2010 .
[ 9 ] P . Wu and T . G . Dietterich , “ Improving SVM Accuracy by
Training on Auxiliary Data Sources , ” in ICML , 2004 .
[ 10 ] A . Gretton , K . Borgwardt , M . Rasch , B . Scholkopf , and A . Smola , “ A kernel method for the two sample problem , ” in NIPS , 2007 .
[ 11 ] J . Blitzer , R . McDonald , and F . Pereira , “ Domain Adaptation with Structural Correspondence Learning , ” in EMNLP , 2006 . [ 12 ] H . Daum´e III , “ Frustratingly easy domain adaptation , ” in
ACL , 2007 .
REFERENCES
[ 13 ] S . J . Pan , I . Tsang , J . Kwok , and Q . Yang , “ Domain Adaptation via Transfer Component Analysis , ” IEEE TNN , vol . 22 , pp . 199 – 210 , 2011 .
[ 14 ] S . J . Pan , X . Ni , J . Sun , Q . Yang , and Z . Chen , “ Cross Domain Sentiment Classification via Spectral Feature Alignment , ” in WWW , 2010 .
[ 15 ] X . Gao , X . Wang , X . Li , and D . Tao , “ Transfer latent variable model based on divergence analysis , ” Pattern Recognition , vol . 44 , no . 10 11 , pp . 2358–2366 , 2011 .
[ 16 ] B . Geng , D . Tao , and C . Xu , “ Daml : Domain adaptation metric learning , ” IEEE TIP , vol . 20 , no . 10 , pp . 2980–2989 , 2011 . matching for transduction , ” in NIPS , 2009 .
[ 17 ] N . Quadrianto , J . Petterson , and A . Smola , “ Distribution
[ 18 ] C W Seah , I T Tsang , and Y S Ong , “ Healing sample selection bias by source classifier selection , ” in IEEE ICDM , 2011 .
[ 19 ] E . Morvant , A . Habrard , and S . Ayache , “ Sparse domain adaptation in projection spaces based on good similarity functions , ” in IEEE ICDM , 2011 .
[ 20 ] J . Blitzer , K . Crammer , A . Kulesza , F . Pereira , and J . Wortman , “ Learning bounds for domain adaptation , ” in NIPS , 2007 .
[ 21 ] S . Ben David , T . Luu , T . Lu , and D . Pal , “ Impossibility
Theorems for Domain Adaptation , ” in AI Statistics , 2010 .
[ 22 ] S . Ben David , J . Blitzer , K . Crammer , and F . Pereira , “ Analysis of representations for domain adaptation , ” in NIPS , 2006 . [ 23 ] Y F Li and Z H Zhou , “ Towards making unlabeled data never hurt , ” in ICML , 2011 .
[ 24 ] T . Joachims , “ Transductive Inference for Text Classification using Support Vector Machines , ” in ICML , 1999 .
[ 25 ] N . V . Chawla , N . Japkowicz , and A . Kotcz , “ Editorial : special issue on learning from imbalanced data sets , ” SIGKDD Explorations Newsletter , vol . 6 , pp . 1–6 , 2004 .
[ 26 ] S . J . Pan and Q . Yang , “ A Survey on Transfer Learning , ”
IEEE TKDE , vol . 22 , pp . 1345–1359 , 2010 .
[ 27 ] J . Blitzer , M . Dredze , and F . Pereira , “ Biographies , Bollywood , Boom boxes and Blenders : Domain Adaptation for Sentiment Classification , ” in ACL , 2007 .
[ 28 ] J . Jiang and C . Zhai , “ A two stage approach to domain adaptation for statistical classifiers , ” in ACM CIKM , 2007 .
1S SVMBest2S SVMBestMCCKMMBestTCABestTARASCA020406086065707580859095100AUCSPCR020406086065707580859095100AUCSPCR020406086065707580859095100AUCSPCR020406086065707580859095100AUCSPCR02040608606570758085AUCSPCR02040608606570758085AUCSPCR02040608606570758085AUCSPCR02040608606570758085AUCSPCR
