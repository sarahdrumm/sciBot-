Efficient Episode Mining of Dynamic Event Streams
Debprakash Patnaik∗ , Srivatsan Laxman† , Badrish Chandramouli‡ and Naren Ramakrishnan§
∗Amazon.com , Seattle , WA 98109 , USA ; E mail : patnaikd@amazon.com †Microsoft Research , Bangalore , India 560080 ; E mail : slaxman@microsoft.com ‡Microsoft Research , Redmond , WA 98052 ; E mail : badrishc@microsoft.com
§Department of Computer Science , Virginia Tech , Blacksburg , VA 24061 ; E mail : naren@vt.edu
Abstract—Discovering frequent episodes over event sequences is an important data mining problem . Existing methods typically require multiple passes over the data , rendering them unsuitable for streaming contexts . We present the first streaming algorithm for mining frequent episodes over a window of recent events in the stream . We derive approximation guarantees for our algorithm in terms of : ( i ) the separation of frequent episodes from infrequent ones , and ( ii ) the rate of change of stream characteristics . Our parameterization of the problem provides a new sweet spot in the tradeoff between making distributional assumptions over the stream and algorithmic efficiencies of mining . We illustrate how this yields significant benefits when mining practical streams from neuroscience and telecommunications logs .
Index Terms—Event Sequences ; Data Streams ; Frequent Episodes ; Pattern Discovery ; Streaming Algorithms ; Approximation Algorithms
I . INTRODUCTION
Application contexts in telecommunications , neuroscience , and intelligence analysis feature massive data streams [ 1 ] with ‘firehose’ like rates of arrival . In many cases , we need to analyze such streams at speeds comparable to their generation rate . In neuroscience , one goal is to track spike trains from multi electrode arrays [ 2 ] with a view to identify cascading circuits of neuronal firing patterns . In telecommunications , network traffic and call logs must be analyzed on a continual basis to detect attacks or other malicious activity . The common theme in all these scenarios is the need to mine episodes ( ie , a succession of events occurring frequently , but not necessarily consecutively [ 3 ] ) from dynamic and evolving streams .
Algorithms for pattern mining over streams have become increasingly popular over the recent past [ 4]–[7 ] . Manku and Motwani [ 4 ] introduced a lossy counting algorithm for approximate frequency counting over streams , with no assumptions on the stream . Their focus on a worst case setting often leads to stringent threshold requirements . At the other extreme , algorithms such as [ 5 ] provide significant efficiencies in mining but make strong assumptions such as iid distribution of symbols in a stream .
In the course of analyzing some real world datasets , we were motivated to develop new methods as existing methods are unable to process streams at the rate and quality guarantees desired ( see Sec VI for some examples ) . Furthermore , established stream mining algorithms are almost entirely focused on itemset mining ( and , modulo a few isolated exceptions , just the counting phase of it ) whereas we are interested in mining general episodes .
Our specific contributions are as follows : • We present the first algorithm for mining episodes in a stream . Unlike prior streaming algorithms that focus almost exclusively on counting , we provide solutions for both candidate generation and counting over a stream .
• Devoid of any statistical assumptions on the stream ( eg , independence or otherwise ) , we develop a novel error characterization for streaming episodes by identifying and tracking two key properties of the stream , viz . maximum rate of change and top k separation . We demonstrate how the use of these two properties enables novel algorithmic optimizations , such as the idea of borders to amortize work as the stream is tracked .
• Although our work is geared towards episode mining , we adopt a black box model of an episode mining algorithm . In other words , our approach can encapsulate and wrap around any pattern discovery algorithm to enable it to accommodate streaming data . This significantly generalizes the scope and applicability of our approach as a general methodology to streamify existing pattern discovery algorithms .
• We demonstrate successful applications in neuroscience and telecommunications log analysis , and illustrate significant benefits in runtime , memory usage , and the scales of data that can be mined . We compare against episodemining adaptations of two typical algorithms [ 5 ] from streaming itemsets literature .
II . PRELIMINARIES
In the framework of frequent episodes [ 3 ] , an event sequence is denoted as ( e1 , τ1 ) , . . . , ( en , τn ) , where ( ei , τi ) represents the ith event ; ei is drawn from a finite alphabet E of symbols ( called event types ) and τi denotes the time stamp of the ith event , with τi+1 ≥ τi , i = 1 , . . . , ( n − 1 ) . An node episode α is defined by a triple α = ( Vα , <α , gα ) , where Vα = {v1 , . . . , v} is a collection of nodes , <α is a partial order over Vα and gα : Vα → E is a map that assigns an eventtype gα(v ) to each node v ∈ Vα . An occurrence of an episode α is a map h : Vα → {1 , . . . , n} such that eh(v ) = gα(v ) for all v ∈ Vα and for all pairs of nodes v , v ∈ Vα such that v <α v the map h ensures that τh(v ) < τh(v ) . Two occurrences of an episode are non overlapped [ 8 ] if no event corresponding to one appears in between the events corresponding to the other . The maximum number of non overlapped occurrences of an episode is defined as its frequency in the event sequence .
Fig 1 . A sliding window model for episode mining over event streams : Bs is the most recent batch of events that arrived in the stream and Ws is the window of interest over which the user wants to determine the set of frequent episodes .
The task in frequent episode discovery is to find all episodes whose frequency exceeds a user defined threshold . Aprioristyle level wise algorithms [ 3 ] , [ 8 ] are typically applicable in this setting . An important variant is top k episode mining ( see [ 9 ] for definitions in the itemsets mining context ) , where , rather than a frequency threshold , the user supplies the number of most frequent episodes needed .
Definition 1 ( Top k episodes of size ) : The set of top k episodes of size is defined as the collection of all node episodes with frequency greater than or equal to the frequency f k of the kth most frequent node episode in the given event sequence . The number of top k node episodes can exceed k , although the number of node episodes with frequencies strictly greater than f k is at most ( k − 1 ) . In general , top k mining can be difficult to solve without knowledge of a good lower bound for f k ; for relatively short event sequences the following simple solution works well enough : start mining at a high threshold and progressively lower the threshold until the desired number of top patterns are returned .
III . PROBLEM STATEMENT
The data available ( referred to as an event stream ) is in the form of a potentially infinite sequence of events :
D = ( e1 , τ1 ) , ( e2 , τ2 ) , . . . , ( ei , τi ) , . . . , ( en , τn ) , . . .
( 1 )
Our goal is to find all episodes that were frequent in the recent past ; for this , we consider a sliding window model1 for the window of interest . In this model , the user wants to determine episodes that were frequent over a ( historical ) window of fixed size terminating at the current time tick . As new events arrive in the stream , the user ’s window of interest shifts , and the data mining task is to next report the frequent episodes in the new window of interest .
We consider the case where the window of interest is very large and cannot be stored and processed in memory . This straightaway precludes the use of standard multi pass algorithms for frequent episode discovery over the window of interest . We organize the events in the stream into smaller batches such that at any given time only the latest incoming batch is stored and processed in memory . This is illustrated in Fig 1 . The current window of interest is denoted by Ws and the most recent batch , Bs , consists of events in D that occurred between times ( s − 1)Tb and sTb where Tb is the time span of each batch and s is the batch number ( s = 1 , 2 , . . . ) .
1Other models such as the landmark and time fading models have also been studied [ 7 ] but we do not consider them here .
Fig 2 . Batch frequencies in Example 1 .
TABLE I
WINDOW FREQUENCIES IN Example 1 .
Episode Window Freq
ABCD
MNOP
EFGH
WXYZ
35
34
25
24
IJKL 23
PQRS
19
The frequency of an episode α in a batch Bs is referred to as its batch frequency f s(α ) . The current window of interest , Ws , consists of m consecutive batches ending in batch Bs , ie
Ws = Bs−m+1 , Bs−m+2 , . . . , Bs
( 2 )
Bj∈Ws f j(α ) .
Definition 2 ( Window Frequency ) : The frequency of an episode α over window Ws , referred to as its window frequency and denoted by f Ws ( α ) , is defined as the sum of batch frequencies of α in Ws . Thus , if f j(α ) denotes the batch frequency of α in batch Bj , then the window frequency of α is given by f Ws ( α ) =
In summary , we are given an event stream ( D ) , a time span for batches ( Tb ) , the number of consecutive batches that constitute the current window of interest ( m ) , the desired size of frequent episodes ( ) , the desired number of most frequent episodes ( k ) and the problem is to discover the top k episodes in the current window without actually having the entire window in memory . Problem 1 ( Streaming Top k Mining ) : For each new batch , Bs , of events in the stream , find all node episodes in the corresponding window of interest , Ws , whose window frequencies are greater than or equal to the window frequency , s , of kth most frequent node episode in Ws . f k Example 1 ( Window Top k v/s Batch Top k ) : Let W be a window of four batches B1 through B4 . The episodes in each batch with corresponding batch frequencies are listed in Fig 2 . The corresponding window frequencies ( sum of each episodes’ batch frequencies ) are listed in Table I . The top 2 episodes in B1 are ( PQRS ) and ( WXYZ ) . Similarly ( EFGH ) and ( IJKL ) are the top 2 episodes in B2 , and so on . ( ABCD ) and ( MNOP ) have the highest window frequencies but never appear in the top 2 of any batch – these episodes would ‘fly below the radar’ and go undetected if we considered only the top 2 episodes in every batch as candidates for the top 2 episodes over W . This example can be easily generalized to any number of batches and any k .
Example 1 highlights the main challenge in the streaming top k mining problem : we can only store/process the most recent batch of events in the window of interest and the batchwise top k may not contain sufficient information to compute the top k over the entire window . It is obviously not possible to track all episodes ( both frequent and infrequent ) in every batch since the pattern space is typically very large . This brings us to the question of which episodes to track in
Time Window Batch Events BsWsPattern Count W X Y Z 12 P Q R S 10 A B C D 8 M N O P 8 E F G H 0 I J K L 0 Pattern Count E F G H 15 I J K L 12 M N O P 10 A B C D 9 P Q R S 0 W X Y Z 0 Pattern Count W X Y Z 12 E F G H 10 A B C D 10 M N O P 8 P Q R S 0 I J K L 0 Pattern Count I J K L 11 P Q R S 9 M N O P 8 A B C D 8 E F G H 0 W X Y Z 0 B1 B2 B3 B4 every batch – how deep must we search within each batch for episodes that have potential to become top k over the window ? We develop the formalism needed to answer this question .
IV . PERSISTENCE AND TOP k APPROXIMATION
We identify two important properties of the underlying event stream which influence the design and analysis of our algorithms . These are stated in Definitions 3 & 4 below .
Definition 3 ( Maximum Rate of Change , ∆ ) : Maximum rate of change ∆(> 0 ) is defined as the maximum change in batch frequency of any episode , α , across any pair of consecutive batches , Bs and Bs+1 , ie , ∀α , s , we have
|f s+1(α ) − f s(α)| ≤ ∆ .
( 3 )
Intuitively , ∆ controls the extent of change from one batch to the next . While it is trivially bounded by the number of events arriving per batch , it is often much smaller in practice . Definition 4 ( Top k Separation of ( ϕ , ) ) : A batch Bs of events is said to have a top k separation of ( ϕ , ) , ϕ ≥ 0 , ≥ 0 , if it contains at most ( 1 + )k episodes with batch frequencies of ( f s k is the batch frequency of the kth most frequent episode in Bs and ∆ is the maximum rate of change . k − ϕ∆ ) or more , where f s
This is essentially a measure of how well separated the frequencies of the top k episodes are relative to the rest of the episodes . We expect to see roughly k episodes with batch s and the separation is considered frequencies of at least f k to be high ( or good ) if lowering the threshold from f k s to k − ϕ∆ ) only brings in very few additional episodes , ie ( f s remains small as ϕ increases . Top k separation of any batch Bs is characterized by , not one but , several pairs of ( ϕ , ) since ϕ and are functionally related : is typically close to zero if ϕ = 0 , while we have k roughly the size of the class s . Note that is a nonof size episodes ( minus k ) if ϕ∆ ≥ f k decreasing function of ϕ and that top k separation is measured relative to the maximum rate of change ∆ .
We now use the maximum rate of change property to design efficient streaming algorithms for top k episode mining and show that top k separation plays a pivotal role in determining the quality of approximation that our algorithms achieve .
Lemma 1 : The batch frequencies of the kth most frequent episodes in any pair of consecutive batches cannot differ by more than the maximum rate of change ∆ , ie , for every batch Bs , we must have
|f s+1 k − f s k| ≤ ∆ .
( 4 )
The above lemma follows directly from : ( i ) there are at least k , and ( ii ) the batch k episodes with frequencies no less than f s frequency of any episode can increase or decrease by no more than ∆ when going from one batch to the next .
Our next observation is that if the batch frequency of an episode is known relative to f s k in the current batch Bs , we can bound its frequency in any later batch Bs+r . Lemma 2 : Consider two batches , Bs and Bs+r , r ∈ Z , located r batches away from each other . Under a maximum rate of change of ∆ the batch frequency of any episode α in Bs+r must satisfy the following : k − 2|r|∆ k + 2|r|∆
If f s(α ) ≥ f s If f s(α ) < f s k , then f s+r(α ) ≥ f s+r k , then f s+r(α ) < f s+r
1 ) 2 ) Detailed proofs can be found in [ 10 ] . Lemma 2 gives us a way to track episodes that have potential to be in the topk of future batches . This is an important property which our algorithm exploits and we recorded this as a remark below . Remark 1 : The top k episodes of batch , Bs+r , r ∈ Z , must have batch frequencies of at least ( f s k − 2|r|∆ ) in batch Bs . Specifically , the top k episodes of Bs+1 must have batch frequencies of at least ( f s
The maximum rate of change property leads to a necessary condition , in the form of a minimum batch wise frequency , for an episode α to be in the top k over a window Ws .
( α ) ≥ ( f s
Theorem 1 ( Exact Top k over Ws ) : An episode , α , can be a top k episode over window Ws only if its batch frequencies satisfy f s k − 2(m − 1)∆ ) ∀Bs ∈ Ws .
Proof : Consider an episode β for which f s k − 2(m − 1)∆ ) in batch Bs ∈ Ws . Let α be any top k episode of Bs . In any other batch Bp ∈ Ws , we have ( α ) − |p − s|∆ k − |p − s|∆ f p(α ) ≥ f s ≥ f s
( β ) < ( f s
( 5 ) k − 2∆ ) in Bs . and f p(β ) ≤ f s
< ( f s
( β ) + |p − s|∆ k − 2(m − 1)∆ ) + |p − s|∆ Applying |p − s| ≤ ( m − 1 ) to the above , we get k − ( m − 1)∆ > f p(β ) f p(α ) ≥ f s
( 6 )
( 7 )
This implies f Ws(β ) < f Ws(α ) for every top k episode α of Bs . Since there are at least k top k episodes in Bs , β cannot be a top k episode over the window Ws .
Based on Theorem 1 we have the following simple algorithm for obtaining the top k episodes over a window : Use a traditional level wise approach to find all episodes with a batch frequency of at least ( f k 1 − 2(m − 1)∆ ) in the first batch ( B1 ) , accumulate their corresponding batch frequencies over all m batches of Ws and report the episodes with the k highest window frequencies over Ws . This approach is guaranteed to return the exact top k episodes over Ws . In order to report the top k over the next sliding window Ws+1 , we need to consider all episodes with batch frequency of at 2 − 2(m − 1)∆ ) in the second batch and track them least ( f k over all batches of Ws+1 , and so on . Thus , an exact solution to Problem 1 would require running a level wise episode mining algorithm in every batch , Bs , s = 1 , 2 , . . . , with a frequency threshold of ( f k A . Class of ( v , k) Persistent Episodes s − 2(m − 1)∆ ) .
Theorem 1 characterizes the minimum batchwise computation needed in order to obtain the exact top k episodes over a sliding window . This is effective when ∆ and m are small ( compared to f k s ) . However , the batchwise frequency thresholds can become very low in other settings , making the processing time per batch as well as the number of episodes to track over the window to become impractically high . To address this issue , we introduce a new class of episodes called ( v , k) persistent episodes which can be computed efficiently by employing higher batchwise thresholds . Further , we show that these episodes can be used to approximate the true top k episodes over the window and the quality of approximation is characterized in terms of the top k separation property ( cf . Definition 4 ) .
Definition 5 ( (v , k) Persistent Episode ) : An episode is said to be ( v , k) persistent over window Ws if it is a top k episode in at least v batches of Ws .
Problem 2 ( Mining ( v , k) Persistent Episodes ) : For each new batch , Bs , of events in the stream , find all node ( v , k) persistent episodes in the corresponding window of interest , Ws .
Theorem 2 : An episode , α , can be ( v , k) persistent over the window Ws only if its batch frequencies satisfy f s ( α ) ≥ k − 2(m − v)∆ ) for every batch Bs ∈ Ws . ( f s Proof : Let α be ( v , k) persistent over Ws and let Vα denote the set of batches in Ws in which α is in the topk . For any Bq /∈ Vα there exists Bp(q ) ∈ Vα that is nearest to Bq . Since |Vα| ≥ v , we must have |p(q ) − q| ≤ ( m − v ) . Applying Lemma 2 we then get f q(α ) ≥ f q k − 2(m− v)∆ for all Bq /∈ Vα . Theorem 2 gives us the necessary conditions for computing all ( v , k) persistent episodes over sliding windows in the stream . The batchwise threshold required for ( v , k) persistent episodes depends on the parameter v . For v = 1 , the threshold coincides with the threshold for exact top k in Theorem 1 . The threshold increases linearly with v and is highest at v = m ( when the batchwise threshold is same as the corresponding batchwise top k frequency ) .
The algorithm for discovering ( v , k) persistent episodes follows the same general lines as the one described earlier for exact top k mining , only that we now apply higher batchwise thresholds : For each new batch , Bs , entering the stream , use a standard level wise episode mining algorithm to find all episodes with batch frequency of at least ( f k s −2(m−v)∆ ) . We provide more details of our algorithm later in Sec V . First , we investigate the quality of approximation of top k that ( v , k)persistent episodes offer and show that the number of errors is closely related to the degree of top k separation .
1 ) Top k Approximation : The main idea here is that , under a maximum rate of change ∆ and a top k separation of ( ϕ , ) , there cannot be too many distinct episodes which are not ( v , k) persistent while still having sufficiently high window frequencies . To this end , we first compute a lower bound ( fL ) on the window frequencies of ( v , k) persistent episodes and an upper bound ( fU ) on the window frequencies of episodes that are not ( v , k) persistent ( cf . Lemmas 3 & 4 ) .
Lemma 3 : If episode α is ( v , k) persistent over a window , Ws , then its window frequency , f Ws(α ) , must satisfy the
Bp∈Vα
Bp∈Vα following lower bound : k − ( m − v)(m − v + 1)∆ def= fL f s f Ws ( α ) ≥
Bs
( 8 )
Proof : Consider episode α that is ( v , k) persistent over Ws and let Vα denote the batches of Ws in which α is in the top k . The window frequency of α can be written as f Ws(α ) = f p(α ) + f q(α )
≥
= f p k +
Bq∈Ws\Vα
Bq∈Ws\Vα f q k − 2|p(q ) − q|∆ 2|p(q ) − q|∆ ( 9 ) where Bp(q ) ∈ Vα denotes the batch nearest Bq where α is in the top k . Since |Ws \ Vα| ≤ ( m − v ) , we must have
|p(q ) − q| ≤ ( 1 + 2 + ··· + ( m − v ) ) f s k −
Bq∈Ws\Vα
Bs∈Ws
Bq∈Ws\Vα
=
1 2
( m − v)(m − v + 1 )
( 10 )
Putting together ( 9 ) and ( 10 ) gives us the lemma . Similar arguments give us the next lemma about the maximum frequency of episodes that are not ( v , k) persistent ( Full proofs are available in [ 10] ) .
Lemma 4 : If episode β is not ( v , k) persistent over a window , Ws , then its window frequency , f Ws(β ) , must satisfy the following upper bound : f Ws ( β ) < k + v(v + 1)∆ def= fU f s
( 11 )
Bs
It turns out that fU > fL ∀v , 1 ≤ v ≤ m , and hence there is always a possibility for some episodes which are not ( v , k)persistent to end up with higher window frequencies than one or more ( v , k) persistent episodes . We observed a specific instance of this kind of ‘mixing’ in our motivating example as well ( cf . Example 1 ) . This brings us to the top k separation property that we introduced in Definition 4 . Intuitively , if there is sufficient separation of the top k episodes from the rest of the episodes in every batch , then we would expect to see very little mixing . As we shall see , this separation need not occur exactly at kth most frequent episode in every batch , somewhere close to it is sufficient to achieve a good top k approximation . s − ϕ∆ , f k
Definition 6 ( Band Gap Episodes , Gϕ ) : In any batch Bs ∈ Ws , the half open frequency interval [ f k s ) is called the band gap of Bs . The corresponding set , Gϕ , of band gap episodes over the window Ws , is defined as the collection of all episodes with batch frequencies in the band gap of at least one Bs ∈ Ws . The main feature of Gϕ is that , if ϕ is large enough , then the only episodes which are not ( v , k) persistent but that can still mix with ( v , k) persistent episodes are those belonging
Lemma 5 : If ϕ
2 > max{1 , ( 1 − v to Gϕ . This is stated formally in the next lemma . The proof , omitted here , can be found in [ 10 ] . m )(m − v + 1)} , then any episode β that is not ( v , k) persistent over Ws , can have f Ws ( β ) ≥ fL only if β ∈ Gϕ . The number of episodes in Gϕ is controlled by the topk separation property , and since many of the non persistent episodes which can mix with persistent ones must spend not one , but several batches in the band gap , the number of unique episodes that can cause such errors is bounded . Theorem 3 is our main result about quality of top k approximation that ( v , k) persistence can achieve .
µ
ϕ
Theorem 3 ( Quality of Top k Approximation ) : Let km 2 ( √1 + 2mϕ − 1)} . 2 , 1 every batch Bs ∈ Ws have a top k separation of ( ϕ , ) with m )(m − v + 1)} . Let P denote the 2 > max{1 , ( 1 − v set of all ( v , k) persistent episodes over Ws . If |P| ≥ k , then the top k episodes over Ws can be determined from episodes , where P with an error of no more than µ = min{m − v + 1 , ϕ Proof : By top k separation , we have a maximum of ( 1 + )k episodes in any batch Bs ∈ Ws , with batch frequencies greater than or equal to f k s − ϕ∆ . Since at least k of these must belong to the top k of the Bs , there are no more than k episodes that can belong to the band gap of Bs . Thus , there can be no more than a total of km episodes over all m batches of Ws that can belong to Gϕ . Consider any β /∈ P with f Ws(β ) ≥ fL – these are the only episodes whose window frequencies can exceed that of any α ∈ P ( since fL is the minimum window frequency of any α ) . If µ denotes the minimum number of batches in which
β belongs to the band gap , then there can be at most such distinct β . Thus , if |P| ≥ k , we can determine the set of top k episodes over Ws with error no more than episodes . km km
µ
µ
There are now two cases to consider to determine µ : ( i ) β is in the top k of some batch , and ( ii ) β is not in the top k of any batch .
2 s − 2t∆ < f k
ϕ∆ ) is , ϕ
. Since β /∈ P , β is below the top k in at least
Case ( i ) : Let β be in the top k of Bs ∈ Ws . Let Bs ∈ Ws be t batches away from Bs . Using Lemma 2 we get f s ( β ) ≥ s − 2t∆ . The minimum t for which ( f k f k s − ( m− v + 1 ) batches . Hence β stays in the band gap of at least min{m − v + 1 , ϕ Case ( ii ) : Let VG denote the set of batches in Ws where β lies in the band gap and let |VG| = g . Since β does not belong to top k of any batch , it must stay below the band gap in all the ( m − g ) batches of ( Ws \ VG ) . Since ∆ is the maximum rate of change , the window frequency of β can be written as follows :
2 } batches of Ws .
Bp∈VG f p(β ) + f p(β ) +
Bq∈Ws\VG
Bp∈VG
Bq∈Ws\VG f q(β ) q − ϕ∆ ) ( 12 ) ( f k f Ws(β ) =
<
Let Bq(p ) denote the batch in Ws \ VG that is nearest to Bp ∈
VG . Then we have : f p(β ) ≤ fq(p)(β ) + |p −q(p)|∆ < f kq(p ) − ϕ∆ + |p −q(p)|∆ p − ϕ∆ + 2|p −q(p)|∆
< f k
( 12 ) we get where the second inequality holds because β is below the band gap in Bq(p ) and ( 13 ) follows from Lemma 1 . Using ( 13 ) in 2|p −q(p)|∆ f k s − mϕ∆ + f Ws(β ) <
( 13 )
Bs∈Ws
Bs∈Ws
Bs∈Ws
<
=
Bp∈VG f k s − mϕ∆ + 2(1 + 2 + ··· + g)∆ s − mϕ∆ + g(g + 1)∆ = UB ( 14 ) f k
The smallest g for which ( f Ws(β ) ≥ fL ) is feasible can be obtained by setting UB ≥ fL . Since ϕ m )(m− v + 1 ) ,
UB ≥ fL implies mϕ∆
2 > ( 1− v f k s − mϕ∆ + g(g + 1)∆ > f k s −
2
Bs∈Ws Solving for g , we get g ≥ 1 ( i ) and ( ii ) , we get µ = min{m−v +1 , ϕ
Bs∈Ws 2 ( √1 + 2mϕ−1 ) . Combining cases 2 ( √1 + 2mϕ−1)} . 2 , 1
Theorem 3 shows the relationship between the extent of topk separation required and quality of top k approximation that can be obtained through ( v , k) persistent episodes . In general , µ ( which is minimum of three factors ) increases with ϕ 2 until the latter starts to dominate the other two factors , namely , 2 ( √1 + 2mϕ−1 ) . The theorem also brings out ( m−v+1 ) and 1 the tension between the persistence parameter v and the quality of approximation . At smaller values of v , the algorithm mines ‘deeper’ within each batch and so we expect fewer errors with respect to the true top k epispodes . On the other hand , deeper mining within batches is computationally more intensive , with the required effort approaching that of exact top k mining as v approaches 1 .
Finally , we use Theorem 3 to derive error bounds for three special cases ; first for v = 1 , when the batchwise threshold is same as that for exact top k mining as per Theorem 1 ; second for v = m , when the batchwise threshold is simply the batch frequency of the kth most frequent episode in the batch ; ff , when the batchwise threshold lies and third , for v =' m+1 midway between the thresholds of the first two cases . ( Proofs are detailed in [ 10] ) .
Corollary 1 : Let every batch Bs ∈ Ws have a top k separation of ( ϕ , ) and let Ws contain at least m ≥ 2 batches . Let P denote the set of all ( v , k) persistent episodes over Ws . If we have |P| ≥ k , then the maximum error rate in the top k episodes derived from P , for three different choices of v , is given by :
2 km m−1 for v = 1 , if ϕ 1 ) 2 ) ( km ) for v = m , if ϕ
2 > ( m − 1 ) 2 > 1 i size episodes is determined by combining frequent ( i − 1)size episodes from the previous level . Then the data is scanned for determining frequencies of the candidates , from which the frequent i size episodes are obtained . We note that all candidate episodes that are not frequent constitute the negative border of the frequent lattice [ 11 ] . This is because , a candidate is generated only when all its subepisodes are frequent . The usual approach is to discard the border . For our purposes , the border contains information required to identify changes in the frequent episodes from one batch to the next2 . s and Bi
The pseudocode for incrementally mining frequent episodes in batches is listed in Algorithm 1 . The inputs to the algorithm are : ( i ) Number k of top episodes desired , ( ii ) New incoming batch Bs of events in the stream , ( iii ) Lattice of frequent ( F∗s−1 ) and border episodes ( B∗s−1 ) from previous batch , and ( iv ) threshold parameter θ . Frequent and border episodes of size i , with respect to frequency threshold f s k − θ , are denoted s respectively ( F∗s and B∗s denote the by the sets F i corresponding sets for all episode sizes ) . For the first batch B1 ( lines 1–3 ) the top k episodes are found by a brute force method , ie , by mining with a progressively lower frequency threshold until at least k episodes of size are found . Once f 1 k is determined , F∗1 and B∗1 are obtained using an Apriori procedure . For subsequent batches , we do not need a brute force k . Based on Remark 1 , if θ ≥ 2∆ , method to determine f s s−1 from batch Bs−1 contains every potential top k episode F of the next batch Bs . Therefore simply updating the counts of s−1 in the new batch Bs and picking the kth all episodes in F highest frequency gives f s k ( lines 4–6 ) . To update the lattice of frequent and border episodes ( lines 7–24 ) the procedure starts from the bottom ( size 1 episodes ) . The data is scanned to determine the frequency of new candidates together with the frequent and border episodes from the lattice ( line 11 ) . In the first level ( episodes of size 1 ) , the candidate set is empty . s−1 that continue to be After counting , the episodes from F s ( lines 13–14 ) . But frequent in the new batch are added to F if an episode is no longer frequent it is marked as a border set and all its super episodes are deleted ( lines 15–17 ) . This ensures that only border episodes are retained in the lattice . In the border and new candidate sets , any episode that is found to new ( lines 18–21 ) . The be frequent is added to both F i remaining infrequent episodes belong to the border because , otherwise , they would have at least one infrequent subepisode and would have been deleted at a previous level ; hence , these infrequent episodes are added to B Finally , the candidate generation step ( line 24 ) is required to fill out the missing parts of the frequent lattice . We want to avoid a full blown candidate generation . Note that if an episode is frequent in Bs−1 and Bs then all its subepisodes are also frequent in both Bs and Bs−1 . Any new episode ( ∈ F s−1 ) that turns frequent in Bs , therefore , must have at least one subepisode that was not frequent in Bs−1 2Border sets were employed by [ 11 ] for efficient mining of dynamic databases . Multiple passes over older data are needed for any new frequent itemsets , which is not feasible in a streaming context . s ( lines 22–23 ) . s and F i s−1 ∪ B
Fig 3 . The set of frequent episodes can be incrementally updated as new batches arrive .
4 km2 for v =' m+1 ff , if ϕ m−1 m+1
2
2
2 m m2−1
2 > 1
3 ) Using v = 1 we make roughly k errors by considering only persistent episodes for the final output , while the same batchwise threshold can give us the exact top k as per Theorem 1 . On the other hand , at v = m , the batchwise thresholds are higher ( the algorithm will run faster ) but the number of errors grows linearly with m . Note that the ( ϕ , ) separation needed for v = 1 is much higher than for v = m . The case lies in between , with roughly 4 k errors under of v = m+1 reasonable separation conditions .
2
V . ALGORITHM
In this section we present an efficient algorithm for incrementally mining episodes with frequency at least ( f s k − θ ) in batch Bs , for each batch in the stream . The threshold parameter θ is set to 2(m − v)∆ for mining ( v , k) persistent episodes ( see Theorem 2 ) and to 2(m − 1)∆ for exact top k mining ( see Theorem 1 ) . A trivial ( brute force ) algorithm to find all episodes with k − θ ) in Bs is as follows : Apply frequency greater than ( f s an Apriori based episode mining algorithm ( eg [ 3 ] , [ 8 ] ) on batch Bs . If the number of episodes of size is less than k , the support threshold is decreased and the mining repeated until at least k size episodes are found . At this point f s k is known . The mining process is then repeated once more with k − θ ) . Doing this entire procedure the frequency threshold ( f s for every new batch is expensive and wasteful . After seeing the first batch of the data , whenever a new batch arrives we have information about the episodes that were frequent in the previous batch . This can be exploited to incrementally and efficiently update the set of frequent episodes in the new batch . The intuition is that the frequencies of most episodes do not change much from one batch to the next . As a result only a small number of previously frequent episodes fall below the new support threshold in the new batch ; similarly , some new episodes may become frequent . This is illustrated in Figure 3 . In order to efficiently find these sets of episodes , we need to maintain additional information that allows us to avoid fullblown candidate generation in each batch . We show that this state information is a by product of an Apriori based algorithm and therefore any extra processing is unnecessary .
Frequent episodes are discovered level wise , in ascending order of episode size . An Apriori procedure alternates between counting and candidate generation . First a set Ci of candidate patterns patterns Decreasing frequency BsBs−1fs−1kfsk( ) ( + ) After a new batch arrives New frequent episodes Old episodes no longer frequent F￿s−1F￿sfs−1k−θfsk−θ Algorithm 1 Persistent episode miner : Mine top k v persistent episodes . Input : Number k of top episodes ; New batch Bs of events ; s−1,B∗
Current lattice of frequent & border episodes ( F∗ Threshold parameter θ ( set to 2(m− v)∆ for ( v , k) persistence , 2(m − 1)∆ for exact top k ) s−1 ) ; s ,B∗ s ) after Bs k − θ ) s−1 , Bs ) k ( based on episodes in F s ← φ s−1 ∪ Bi k ( brute force ) 1 ,B∗
1 ) ← Apriori(B1 , , f 1
Determine f 1 Compute ( F∗ CountFrequency(F Determine f s s−1 ) Initialize C1 ← φ ( new candidates of size 1 ) for i ← 1 , . . . , do
Output : Lattice of frequent and border episodes ( F∗ 1 : if s = 1 then 2 : 3 : 4 : else 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : 17 : 18 : 19 : 20 : 21 : 22 : 23 : 24 : 25 : return ( F∗ s ← φ , Bi Initialize F i new ← φ ( new frequent episodes of size i ) Initialize F i CountFrequency(F i for α ∈ F i s−1 do if f s(α ) ≥ f s Update F i Update Bi Delete super episodes of α from ( F∗ k − θ then s ← F i s ← Bi s−1 ∪ Ci do k − θ then s ← F i new ← F i s ← Bi if f s(α ) ≥ f s Update F i Update F i Update Bi s ,B∗ s )
Ci+1 ← GenerateCandidate(F i s ∪ {α} s ∪ {α} s−1 ∪ Ci , Bs ) s ∪ {α} new ∪ {α} s ∪ {α} s−1,B∗ s−1 ) for α ∈ Bi else else new,F i s ) new|.|F i new| |F i s| ) instead of O(|F i new . but is frequent in Bs . All such episodes are listed in F i Hence the candidate generation step ( line 24 ) for the next level generates only candidates with atleast one subepisode new . This reduces the number of candidates generated at ∈ F i each level without compromising completeness of the results . The space and time complexity of candidate generation is now O(|F i s|2 ) and in most practical s| . Later in the experiments section , we cases |F i show how border sets help our algorithm run very fast . For a window Ws ending in the batch Bs , the set of output episodes can be obtained by picking the top k most frequent s . Each episode also maintains a list episodes from the set F that stores its batch wise counts is last m batches . The window frequency is obtained by adding these entries together . The output episodes are listed in decreasing order of their window counts .
Example 2 : In this example we illustrate the procedure for incrementally updating the frequent episodes lattice as a new batch Bs is processed ( see Figure 4 ) . Figure 4(A ) shows the lattice of frequent and border episodes found in the batch Bs−1 ( The figure does not show all the subepisodes at each level , only some of them ) . ABCD is a 4 size frequent episode in the lattice . In the new batch Bs , the episode ABCD is no longer frequent . The episode CDXY appears as a new frequent episode . The episode lattice in Bs is shown in Figure 4(B ) .
In the new batch Bs , AB falls out of the frequent set . AB
Fig 4 . frequent and border episodes in Bs−1 .
Incremental lattice update for the next batch Bs given the lattice of now becomes the new border and all its super episodes namely ABC , BCD and ABCD are deleted from the lattice .
At level 2 , the border episode XY turns frequent in Bs . This allows us to generate DXY as a new 3 size candidate . At level 3 , DXY is also found to be frequent and is combined with CDX which is also frequent in Bs to generate CDXY as a 4 size candidate . Finally at level 4 , CDXY is found to be frequent . This shows that border sets can be used to fill out the parts of the episode lattice that become frequent in the new data .
A . Experimental Setup
VI . RESULTS
We show experimental results on one synthetic and two real data streams , from two very different domains : experimental neuroscience and telecom networks . In neuroscience , we consider ( 1 ) synthetic neuronal spike trains based on mathematical models of interconnected neurons , with each neuron modeled as an inhomogeneous Poisson process [ 8 ] , and ( 2 ) real neuronal spiking activity from dissociated cortical cultures , recorded using multi electrode arrays at Steve Potter ’s lab , Georgia Tech [ 12 ] . The third kind of data we consider are call data records from a large European telecom network . Each record describes whether the associated call was voice or data , an international or local call , in network or out of network , and of long or short duration . Pattern discovery over such data can uncover hidden , previously unknown , trends in usage patterns , evolving risk of customer churn , etc . . The data length in terms of number of events and the alphabet size of each of these datasets is shown in Table II(a ) . Table II(b ) gives the list of parameters and the values used in experiments that we do not explicitly mention in the text .
As mentioned earlier , we are not aware of any streaming algorithms that directly address top k episode mining over sliding windows of data . For our experiments , we compare persistent episode miner against two methods from itemsets mining literature [ 5 ] ( after adapting them for episodes ) : one that fixes batchwise thresholds based on Chernoff bounds under an iid assumption over the event stream , and the second based on a sliding window version of the Lossy Counting
A B C D AB BC CD ABC BCD ABCD X Y DX XY CDX Bs−1AX BX E H … … A B C D AB BC CD ABC BCD ABCD X Y DX XY CDX DXY CDXY AX BX E H … … BsLevel : 2 Level : 1 Level : 3 Level : 4 Level : 2 Level : 1 Level : 3 Level : 4 ( A ) ( B ) Border sets Border sets TABLE II
EXPERIMENTAL SETUP .
( a ) Datasets used in the experiments .
Dataset Call logs Neuroscience Synthetic
Alphabet size 8 58 515
Data length 42,030,149 12,070,634 25,295,474
AGGREGATE PERFORMANCE COMPARISON OF DIFFERENT ALGORITHMS .
TABLE III
( a ) Dataset : Neuroscience , Size of episodes = 4
Top k Miner Chernoff bound based Lossy counting based Persistent episode
F Score 92.0 92.0 97.23
Runtime ( sec ) Memory ( MB ) 456.0 208.25 217.64
251.39 158.5 64.51
( b ) Parameter set up .
( b ) Dataset : Call logs , Size of episodes = 6
Value
Parameter k ( in Top k episodes ) Number of batches in a window , m Batch size ( as number of events ) Error parameter ( applicable to Chernoff based and Lossy counting methods ) Parameter v ( applicable to Persistence miner ) m/2
25 10 106 0.001 algorithm [ 4 ] . We modify the support threshold computation using chernoff bound for episodes since the total number itemsets is bounded by the size of the largest of distinct transaction while for episodes it is the alphabet size that determines this .
Estimating ∆ dynamically : ∆ is a critical parameter for our persistent episode miner . But unfortunately the choice of the correct value is highly data dependent and the characteristics of the data can change over time . One predetermined value of ∆ cannot be provided in any intuitive way . Therefore we estimate ∆ from the frequencies of size episodes in consecutive batches by computing the change in frequency of episodes and using the 75th percentile as the estimate . We avoid using the maximum change as it tends to be noisy .
B . Quality of top k mining
We present aggregate comparisons of the three competing methods in Table III . These datasets provide different levels of difficulty for the mining algorithms . Tables III(a ) & III(c ) shows high f scores3 for the synthetic and real neuroscience data for all methods ( Our method performs best in both cases ) . On the other hand we find that all methods report very low f scores on the call logs data ( see Table III(b) ) . The characteristics of this data does not allow one to infer window top k from batches ( using limited computation ) . But our proposed method nearly doubles the f score with identical memory and CPU usage on this real dataset . It may be noteworthy to mention that the competing methods reported close to 100 % accuracies but they do not perform that well on more realistic datasets . In case of the synthetic data ( see Table III(c ) the characteristics are very similar to that of the neuroscience dataset .
C . Computation efficiency comparisons
Table III shows that we do better than both competing methods in most cases ( and never significantly worse than either ) with respect to time and memory .
3fscore = 2 · precision·recall precision+recall
Top k Miner Chernoff bound based Lossy counting based Persistent episode
F Score 32.17 24.18 49.47
Runtime ( sec ) Memory ( MB ) 11.87 3.29 3.34
66.14 56.87 67.7
( c ) Dataset : Synthetic data , Size of episodes = 4
Top k Miner Chernoff bound based Lossy counting based Persistent episode
F Score 92.7 92.7 96.2
Runtime ( sec ) Memory ( MB ) 14.91 6.96 4.98
43.1 32.0 34.43
1 ) Effect of parameters on performance : In Figure 5 , we see all three methods outperform the reference method ( the standard multi pass apriori based miner ) by at least an order of magnitude in terms of both run times and memory .
In Figure 5(a) (c ) , we show the effect of increasing k on all the methods . The accuracy of Lossy counting algorithm drops with increase in k , while that of Chernoff based method and Persistence miner remain unchanged . Persistence miner has lower runtimes for all choices of k while having comparable memory footprint as the other two methods .
With increasing window size ( m = 5 , 10 , 15 and batch − size = 106 events ) , we observe better f scores for Persistence miner but this increase is not significant enough and can be caused by data characterisitcs alone . The runtimes and memory of Persistence miner remain nearly constant . This is important for streaming algorithms as the runtimes and memory of the standard multi pass algorithm increases ( roughly ) linearly with window size . new|.|F i
2 ) Utility of Border Sets : For episodes with slowly changing frequency we show in Section V that using bordersets to incrementally update the frequent episodes lattice results in an order complexity of O(|F i s| ) instead of s|2 ) for candidate generation and in most practical cases O(|F i s| . Table IV demonstrates the speed up in runtime |F i new| |F i achieved by using border set . We implemented two versions of our Persistence miner . In one we utilize border sets to incrementally update the lattice where as in other we rebuild the frequent lattice from scratch in every new batch . The same batch wise frequency thresholds used as dictated by Theorem 2 . We run the experiment on our call logs dataset and for various parameter settings of our algorithm we observe a speed up of ≈ 4× resulting from use of border sets . D . Adapting to Dynamic data
In this experiment we show that Persistence miner adapts faster than the competing algorithms to changes in underlying data characteristics . We demonstrate this using synthetic data generated using the multi neuronal simulator based [ 8 ] .
( a ) F Score vs . k
( b ) Runtime vs . k
( c ) Memory vs . k
( d ) F Score vs . m
( e ) Runtime vs . m
( f ) Memory vs . m
Fig 5 . Performance with different parameters ( Dataset : Call logs )
TABLE IV
UTILITY OF BORDER SET .
( a ) Dataset : Call logs , Size of episodes = 6 , Parameter v = m/2
Window size 5 10 15
Runtime ( border set ) 2.48 3.13 3.47
( no
Runtime border set ) 12.95 11.41 13.76
Memory ( border set ) 67.55 67.7 67.82
Memory ( no border set ) 66.21 66.67 67.02
( b ) Dataset : Call logs , Size of episodes = 6 , window size m = 10
Parameter v 0 5 10
Runtime ( border set ) 2.78 3.13 3.21
( no
Runtime border set ) 11.98 11.41 10.85
Memory ( border set ) 67.7 67.7 67.69
Memory ( no border set ) 66.67 66.67 57.5
The simulation model was adapted to update the connection strengths dymanically while generating synthetic data . This allowed us to change the top k episodes slowly over the length of simulated data . We embedded 25 randomly picked patterns with time varying arrival rates . Figure 6(a ) shows the performance of the different methods in terms of f score computed after arrival of each new batch of events but for topk episodes in the window . The ground truth is again the output of the standard multi pass apriori algorithm that is allowed access to the entire window of events . The f score curves of the both the competing methods almost always below that of the Persistence miner . While the runtimes for Persistence miner are always lower than those of the competing methods ( see Figure 6(b ) . Lossy counting based methods is the slowest at error parameter set to 00001
The main reason of better tracking in the case of Persistence miner is that the output of the algorithm filters out all non(v , k ) persistent episodes . This acts in favor of Persistence miner as the patterns most likely to gather sufficient support to be in the top k are also likely to be persistent . The use of border sets in Persistence miner explains the lower runtimes .
( a ) Tracking top k f score comparison
( b ) Tracking top k runtime
Fig 6 . Comparison of different methods in tracking top k episodes over dynamically changing event stream ( Parameters used : k = 25 , m = 10 , Persistence miner : v=0.5 , alg 6,7 : error = 1.0e 4 )
E . Correlation of f score with theoretical error
Can we compute theoretical errors ( data dependent quantity ) and guess how well we perform ? This could be invaluable in a real world streaming data setting .
In this experiment we try to establish the usefulness of the theoretical analysis proposed in the paper . The main power of the theory is to predict the error in the output set at the end of each batch . Unlike other methods we compute the error bounds
010203040506070802550100F scoreTop kChernoff bound basedLossy counting basedPersistence miner0204060801002550100Runtime ( sec)Top kStandard Multi pass minerChernoff bound basedLossy counting basedPersistence miner01002003004005006007002550100Memory Usage ( MB)Top kStandard Multi passChernoff bound basedLossy counting basedPersistence miner01020304050607051015F scoreWindow size ( m batches)Chernoff bound basedLossy counting basedPersistence miner05010015020025030035040051015Runtime ( sec)Window size ( m batches)Standard Multi passChernoff bound basedLossy counting basedPersistence miner010020030040050060070080051015Memory Usage ( MB)Window size ( m batches)Standard Multi passChernoff bound basedLossy counting basedPersistence miner75808590951001050510152025F ScoreSliding window positionChernoff bound basedLossy counting basedPersistence miner05101520253035400510152025Runtime ( sec)Sliding window positionChernoff bound basedLossy counting basedPersistence miner using the data characteristics and is dynamically updated as new data arrives . The error guarantees of both Lossy counting and Chernoff based methods are static .
In Figure 7 , we plot the error bound using Theorem 3 and the f score computed with respect to the reference method ( standard multi pass apriori ) in a 2D histogram . According to the theory different pairs of ( φ , ) output a different error bound in every batch . In our experiment we pick the smallest error bound in the alloable range of φ and corresponding in each batch and plot it with the corresponding f score . The histogram is expected to show negative correlation between f score and our error predicted error bound ie the predicted error for high f score top k results should be low and vise versa . The correlation is not very evident in the plot . The histogram shows higher density in the left top part of the plot , which is a mild indication that high f score has a corresponding low error predicition .
VIII . CONCLUSIONS
While episode mining in offline multi pass scenarios has been researched [ 3 ] , [ 8 ] , this paper is the first to explore ways of doing both counting and candidate generation efficiently in a streaming setting . We have presented algorithms that can operate at as high frequency thresholds as possible and yet give certain guarantees about frequent output patterns . One of our directions of future work is to further the idea of automatic ‘streamification’ of algorithms whereby black box mining algorithms that operate in levelwise fashion can exploit the approximation guarantees and border set efficiencies introduced here . We also aim to increase the expressiveness of our episode mining formulation ; for instance , by combining episode mining with itemsets we will be able to mine more generalized partial order episodes in a stream .
REFERENCES
[ 1 ] S . Muthukrishnan , “ Data streams : Algorithms and applications , ” Foundations and Trends in Theoretical Computer Science , vol . 1 , no . 2 , 2005 . [ 2 ] D . Patnaik et al . , “ Discovering excitatory networks from discrete event streams with applications to neuronal spike train analysis , ” in Proc . of The 9th IEEE Intl . Conf . on Data Mining ( ICDM ) , 2009 .
[ 3 ] H . Mannila et al . , “ Discovery of frequent episodes in event sequences , ”
Data Mining and Knowl . Dsc . , vol . 1 , no . 3 , pp . 259–289 , 1997 .
[ 4 ] G . S . Manku and R . Motwani , “ Approximate frequency counts over data streams , ” in Proc . of the 28th Intl . Conf on Very Large Data Bases ( VLDB ) , 2002 , pp . 346–357 .
[ 5 ] R . C W Wong and A . W C Fu , “ Mining top k frequent itemsets from data streams , ” Data Mining and Knowl . Dsc . , vol . 13 , pp . 193–217 , 2006 .
[ 6 ] T . Calders et al . , “ Mining frequent itemsets in a stream , ” in Proc . of the
7th IEEE Intl . Conf . on Data Mining ( ICDM ) , 2007 , pp . 83–92 .
[ 7 ] J . Cheng et al . , “ A survey on algorithms for mining frequent itemsets over data streams , ” Knowl . and Info . Systems , vol . 16 , no . 1 , pp . 1–27 , 2008 .
[ 8 ] A . Achar et al . , “ Discovering injective episodes with general partial orders , ” Data Mining and Knowl . Dsc . , vol . 25 , no . 1 , pp . 67–108 , 2012 . [ 9 ] J . Wang et al . , “ TFP : An efficient algorithm for mining top k frequent closed itemsets , ” IEEE Trans . on Knowledge and Data Engg . , vol . 17 , no . 5 , pp . 652–664 , 2005 .
[ 10 ] D . Patnaik et al . , “ Streaming algorithms for pattern discovery over dynamically changing event sequences , ” CoRR , vol . abs/1205.4477 , 2012 .
[ 11 ] Y . Aumann et al . , “ Borders : An efficient algorithm for association generation in dynamic databases , ” J . of Intl . Info . Syst . ( JIIS ) , no . 1 , pp . 61–73 , 1999 .
[ 12 ] D . A . Wagenaar et al . , “ An extremely rich repertoire of bursting patterns during the development of cortical cultures , ” BMC Neuroscience , 2006 . [ 13 ] R . M . Karp et al . , “ A simple algorithm for finding frequent elements in streams and bags , ” ACM Trans . Database Syst . , vol . 28 , pp . 51–55 , 2003 .
[ 14 ] J . H . Chang and W . S . Lee , “ Finding recent frequent itemsets adaptively over online data streams , ” in Proc . of the 9th ACM SIGKDD Intl . Conf . on Knowledge discovery and data mining ( KDD ) , 2003 , pp . 487–492 . [ 15 ] —— , “ A sliding window method for finding recently frequent itemsets over online data streams , ” J . Inf . Sci . Eng . , vol . 20 , no . 4 , pp . 753–762 , 2004 .
[ 16 ] R . Jin and G . Agrawal , “ An algorithm for in core frequent itemset mining on streaming data , ” in Proc . of the 5th IEEE Intl . Conf . on Data Mining ( ICDM ) , 2005 , pp . 210–217 .
[ 17 ] H . T . Lam et al . , “ Online discovery of top k similar motifs in time series data , ” in SIAM Intl . Conf . of Data mining , 2011 , pp . 1004–1015 .
[ 18 ] L . Mendes , B . Ding , and J . Han , “ Stream sequential pattern mining with precise error bounds , ” in Proc . of the 8th IEEE Intl . Conf . on Data Mining ( ICDM ) , 2008 , pp . 941–946 .
[ 19 ] J . Pei et al . , “ Prefixspan : Mining sequential patterns efficiently by prefixprojected pattern growth , ” in Proc . of the 17th Intl . Conf . on Data Engg . ( ICDE ) , 2001 , pp . 215– .
Fig 7 .
2D Histogram of predicted error vs . f score . ( Dataset : Call logs )
VII . RELATED WORK
The literature for streaming algorithms for pattern discovery is dominated by techniques from the frequent itemset mining context [ 4]–[7 ] , [ 13]–[17 ] , and we are unaware of any algorithms for episode mining over event streams .
Manku and Motwani [ 4 ] proposed the lossy counting Algorithm for approximate frequency counting in the landmark model ( ie , all events seen until the current time constitute the window of interest ) . One of its main drawbacks is that the algorithm must essentially operate at a threshold of to provide error guarantees , which is impractical in many realworld scenarios . Karp et al . [ 13 ] also propose a one pass streaming algorithm for finding frequent items , and these ideas were extended to itemsets by Jin and Agrawal [ 16 ] , but all these methods require even more space than lossy counting . Mendes et al . [ 18 ] extend the pattern growth algorithm ( PrefixSpan ) [ 19 ] for mining sequential patterns to incorporate the idea of lossy counting . Chang and Lee [ 15 ] and Wong and Fu [ 5 ] extend lossy counting to sliding windows and topk setting , respectively . New frequency measures for itemsets over streams have also been proposed ( eg , Calders et al . [ 6 ] Lam et al . [ 17 ] ) but these methods are heavily specialized toward the itemset context and it is not obvious how to extend them to accommodate episodes in a manner that supports both candidate generation and counting .
02004006008001000120020406080Predicted ErrorF−Score
