Appears in Proceedings of the IEEE International Conference on Data Mining ( ICDM ) , 2012 .
Scaling Inference for Markov Logic via Dual Decomposition
Ce Zhang Department of Computer Sciences
Christopher R´e
Jude Shavlik
Feng Niu
University of Wisconsin Madison , USA
{leonn , czhang , chrisre , shavlik}@cswiscedu
Abstract—Markov logic is a knowledge representation language that allows one to specify large graphical models . However , the resulting large graphical models can make inference for Markov logic a computationally challenging problem . Recently , dual decomposition ( DD ) has become a popular approach for scalable inference on graphical models . We study how to apply DD to scale up inference in Markov logic .
A standard approach for DD first partitions a graphical model into multiple tree structured subproblems . We apply this approach to Markov logic and show that DD can outperform prior inference approaches . Nevertheless , we observe that the standard approach for DD is suboptimal as it does not exploit the rich structure often present in the Markov logic program . Thus , we describe a novel decomposition strategy that partitions a Markov logic program into parts based on its structure . A crucial advantage of our approach is that we can use specialized algorithms for portions of the input problem – some of which have been studied for decades , eg , coreference resolution . Empirically , we show that our program level decomposition approach outperforms both non decomposition and graphical model based decomposition approaches to Markov logic inference on several data mining tasks .
I . INTRODUCTION
Markov logic [ 10 ] is a knowledge representation language that uses weighted first order logic to specify graphical models . It has been applied to a wide range of applications , including many in information extraction and data mining [ 9 ] . The resulting graphical models can be huge ( hundreds of millions of nodes or more in our applications ) , and so a key technical challenge is the scalability and performance of Markov logic inference .
Semantically , a Markov logic program , or Markov logic network ( MLN ) , specifies a graphical model called a Markov random field ( MRF ) . Thus , one approach to improving the scalability of MLNs is to apply inference techniques from the graphical model literature . One such technique is dual decomposition [ 15 ] , which has recently been applied to MRF inference [ 1 ] , [ 2 ] , [ 4 ] , [ 14 ] . In addition to increased scalability , dual decomposition also offers the possibility of dual certificates1 that can bound the distance of a solution from optimality , eg , for maximum a posteriori ( MAP ) inference .
There are two steps in applying dual decomposition : ( 1 ) decompose the inference problem into multiple parts , and
1Namely , an lower ( resp . upper ) bound to a minimization ( resp . maxi mization ) problem .
( 2 ) iteratively combine solutions from individual parts . The intuition is that the individual parts will be more tractable than the whole problem – so much so that the improved performance of individual parts will compensate for the overhead of many iterations of repeated inference . Following the literature [ 2 ] , [ 4 ] , we first implement MRF level decomposition and compare it with state of the art MLN inference algorithms . On simpler MLN generated MRFs , MRFlevel decomposition can achieve competitive performance compared to monolithic inference ( ie , running a generic MLN inference algorithm without decomposition ) . On more complex MLN generated MRFs , the performance and quality of both monolithic inference and MRF decomposition approaches may be suboptimal .
Our key observation is that MRF level decomposition strategies in step ( 1 ) ignore valuable structural hints that often occur in MLN programs . For example , a large Markov logic program may have several “ subroutines ” that each perform a standard , well studied task such as coreference resolution or labeling , eg , with a conditional random field ( CRF ) [ 6 ] . In contrast to traditional approaches that either use a generic MLN inference algorithm or decompose into semantically meaningless parts like trees , our idea is to exploit this information so that we may use existing , specialized algorithms for such individual subtasks . For example , we may choose to solve a labeling task with a CRF and so use the Viterbi algorithm [ 6 ] . Importantly , even if we use different algorithms for each part , dual decomposition preserves the joint inference property – ie , different parts are not solved in isolation . The hope is to achieve higher efficiency and quality than monolithic approaches .
To illustrate this idea , we describe several such correspondences between MLN structure and specialized inference algorithms , including logistic regression and linearchain conditional random fields . A user may declare ( or an algorithm may detect ) which of these specialized algorithms to use on an individual part of the MLN program . We call our prototype system FELIX .
Experimentally , we validate that our ( MRF level and program level ) dual decomposition approaches have superior performance than prior MLN inference approaches on several data mining tasks . Our main results are that ( 1 ) on simple MLNs taken from the literature , MRF level decomposition outperforms monolithic inference , and FELIX has competitive efficiency and quality compared to monolithic inference and MRF level decomposition ; and ( 2 ) on more complex MLNs ( also taken from the literature ) , FELIX achieves substantially higher efficiency and quality than monolithic inference and MRF level decomposition .
II . RELATED WORK
Dual Decomposition : Dual decomposition is a classic and general technique in optimization that decomposes an objective function into multiple smaller subproblems ; in turn , these subproblems communicate to optimize a global objective via Lagrange multipliers [ 15 ] . Recently , dual decomposition has been applied to inference in graphical models such as MRFs . In the master slave scheme [ 3 ] , [ 4 ] , the MAP solution from each subproblem is communicated and the Lagrange multipliers are updated with the projected gradient method at each iteration . Our prototype implementation of FELIX uses the master slave scheme . It is future work to adapt the closely related tree reweighted ( TRW ) algorithms [ 2 ] , [ 14 ] that decompose an MRF into a convex combination of spanning trees .
MLN Inference : There has been extensive research interest in techniques to improve the scalability and performance of MLN inference . For example , we have studied how to prune MRFs based on logical rules in MLNs [ 11 ] , and use relational databases to improve the speed and scalability of constructing MRFs [ 7 ] . Our implementation builds on the open source TUFFY system [ 7 ] , which implements many of the above techniques .
III . BACKGROUND
We first walk through an MLN program that extracts affiliation relationships between people and organizations from text . We then briefly review dual decomposition . A . Markov Logic
An MLN consists of three parts : schema , evidence , and rules . To tell an MLN inference system what data will be provided or generated , the user provides a schema , ie , definitions of a set of relations ( or equivalently , predicates ) . The truth values for some relations are given ; such relations are called the evidence . In the schema of Figure 1 , the first three relations are evidence relations . In addition to evidence , there are also relations whose content we do not know ; they are called query relations . A user may ask for predictions on some or all query relations .
In addition to schema and evidence , we also provide a set of MLN rules that encode our knowledge about the correlations and constraints over the relations . An MLN rule is a first order logic formula associated with a real number ( or infinity ) called a weight . Infinite weighted rules are called hard rules , which means that they must hold in any prediction that the MLN system makes . In contrast , rules with finite weights are soft rules : a positive weight indicates confidence in the rule ’s correctness .
Semantics : An MLN program defines a probability distribution over possible worlds . Formally , we first fix a schema   ( as in Figure 1 ) and a domain D . Given as input a set of formulae ¯F = F1 , . . . , FN with weights w1 , . . . , wN , they define a probability distribution over possible worlds as follows . Given a formula Fk with free variables ¯x = ( x1,··· , xm ) , for each ¯d 2 Dm we create a new formula g ¯d called a ground formula where g ¯d denotes the result of substituting each variable xi of Fk with di . We assign the weight wk to g ¯d . Denote by G = ( ¯g , w ) the set of all such weighted ground formulae of ¯F . Essentially , G forms an MRF over a set of Boolean random variables ( representing the truth value of each possible ground tuple ) . Let w be a function that maps each ground formula to its assigned weight . Fix an MLN ¯F , then for any possible world ( instance ) I , we say a ground formula g is violated if w(g ) > 0 and g is false in I , or if w(g ) < 0 and g is true in I . We denote the set of ground formulae violated in a world I as V ( I ) . The cost of the world I is costMLN(I ) = Xg2V ( I )
|w(g)| .
( 1 )
Through costMLN , an MLN defines a probability distribution over all instances :
Pr[I ] = Z 1 exp{ costMLN(I)} , where Z is a normalizing constant .
Inference : We focus on MAP inference that finds a most likely world , ie , a world with the lowest cost . MAP inference is essentially a mathematical optimization problem that is intractable , and so existing MLN systems implement generic algorithms for inference.2
B . Dual Decomposition
We illustrate the basic idea of dual decomposition with an example . Consider the problem of minimizing a real valued function f ( x1 , x2 , x3 ) . Suppose that f can be written as f ( x1 , x2 , x3 ) = f1(x1 , x2 ) + f2(x2 , x3 ) .
Further suppose that we have black boxes to solve f1 and f2 ( plus linear terms ) . To apply these black boxes to minimize f we need to cope with the fact that f1 and f2 share the variable x2 . Following dual decomposition , we can rewrite minx1,x2,x3 f ( x1 , x2 , x3 ) into the form min x1,x21,x22,x3 f1(x1 , x21 ) + f2(x22 , x3 ) st x21 = x22 , where we essentially make two copies of x2 and enforce that they are identical . The significance of such rewriting is that we can apply Lagrangian relaxation to the equality constraint to decompose the formula into two independent
2FELIX also supports marginal inference using dual decomposition . pSimHard(per1 , per2 ) homepage(per , page ) faculty(org , per ) ⇤affil(per , org ) ⇤pCoref(per1 , per2 ) Schema faculty(‘MIT’ , ‘Chomsky’ ) homepage(‘Joe’ , ‘Doc201’ ) · · ·
Evidence rule weight ( F1 ) +1 pCoref(p , p ) ( F2 ) +1 pCoref(p1 , p2 ) => pCoref(p2 , p1 ) +1 pCoref(x , y ) , pCoref(y , z ) => pCoref(x , z ) ( F3 ) 6 ( F4 ) 8 ( F5 ) pSimHard(p1 , p2 ) => pCoref(p1 , p2 ) faculty(o , p ) => affil(p , o )
Rules
Figure 1 . An example MLN program that performs two tasks : 1 . discover affiliation relationships between people and organizations ( affil ) , 2 . resolve coreference among people mentions ( pCoref ) . The remaining three relations are evidence relations . pieces . To do this , we introduce a scalar variable   2 R ( called a Lagrange multiplier ) and define g(  ) =
Tasks
λ Γ1
Task : Classification
Master$
λ Γ2
Task : Generic min x1,x21,x22,x3 f1(x1 , x21 ) + f2(x22 , x3 ) +  (x21   x22 ) .
Relations
GoodNews$
Happy1$
Happy2$
BadNews$
Sad$
For any   , we have g(  )  minx1,x2,x3 f ( x1 , x2 , x3).3 Thus , the tightest bound is to maximize g(  ) ; the problem max  g(  ) is a dual problem for the problem on f . If the optimal solution of this dual problem is feasible ( here , x21 = x22 ) , then the dual optimal solution is also an optimum of the original program [ 15 , p . 168 ] .
The key benefit of this relaxation is that , instead of a single problem on f , we can now compute g(  ) by solving two independent problems ( each problem is grouped by parentheses ) that may be easier to solve : g(  ) = min x1,x21 + min x22,x3
( f1(x1 , x21 ) +  x21 ) ( f2(x22 , x3 )    x22 ) .
To compute max  g(  ) , we can use standard techniques such as projected subgradient [ 15 , p . 174 ] . Notice that dual decomposition can be used for MLN inference if xi are truth values of ground tuples and one defines f to be costMLN(I ) . Decomposition Choices : The dual decomposition technique leaves open the question of how to decompose a function f . We need to answer this question if we want to apply dual decomposition to MLNs .
IV . DUAL DECOMPOSITION FOR MLNS
The two approaches that we implement for dual decomposition work at different levels of abstraction : at the MRF level or at the MLN program level . Still , the two methods are similar : and both pass messages in a master slave scheme , and produce a MAP solution after inference in a similar way . As a result , we are able to implement both approaches on top of the TUFFY [ 7 ] system . We describe each step of the process in turn : decomposition ( Section IV A ) , masterslave message passing ( Section IV B ) , and production of the solution ( Section IV C ) .
Figure 2 . A program level decomposition for Example 1 . Shaded boxes are evidence relations . Solid arrows are data flow ; dash arrows are control .
A . Decomposition
In decomposition , we partition the input structure and set up auxiliary structures to support message passing . For MRF level decomposition , we partition an MRF into multiple trees that are linked via auxiliary singleton factors ; for program level decomposition , we partition an MLN into subprograms that are linked via auxiliary singleton rules .
MRF level Decomposition : The input to the decomposition algorithm is an MRF which is the result of grounding an input MLN . The MRF is represented as a factor graph ( ie , a bipartite graph between ground tuple nodes and ground formula factors).4 Following Komodakis et al . [ 3 ] , [ 4 ] and Wainwright et al . [ 14 ] , we decompose the MRF into a collection of trees ( smaller factor graphs with disjoint factors ) that cover this factor graph , ie , each node in the factor graph is present in one or more trees.5 Nodes that are in multiple trees may take conflicting values ; to allow messages to be passed in the next step , we create a special singleton factor for each copy of a shared node . the input the program level :
Program level Decomposition : In contrast , FELIX performs decomposition at is the ( first order ) rule set of an MLN program , and FELIX partitions it into multiple tasks each of which can be solved with different algorithms . In our program level approach , we share at ie , an entire relation is shared or not . This choice allows FELIX to use a relational database management system ( RDBMS ) for all data movement , which can be formulated as SQL queries ; in turn , this allows us to ignore low level issues like memory management . We illustrate the idea with an example . Example 1 Consider a simple MLN which we call   : the granularity of relations ,
1 1 5
GoodNews(p ) => Happy(p ) BadNews(p ) => Sad(p ) Happy(p ) <=> ¬Sad(p )
 1  2  3
3The search space of LHS is a superset of RHS , therefore we always have LHS  RHS . One can always take x21 = x22 = x3 in the minimization , and the value of the two object functions are equal .
4This representation allows for non pairwise MRFs . 5We use a greedy algorithm to find these trees . We describe more details in the full version [ 8 ] . where GoodNews and BadNews are evidence and the other two relations are queries . Consider the decomposition  1 = { 1} and  2 = { 2 ,  3} .  1 and  2 share the relation Happy , so we create two copies of this relation : Happy1 and Happy2 , one for each subprogram . We introduce Lagrange multipliers  p , one for each possible ground tuple Happy(p ) . We thereby obtain a new program :
1  p 1 5 p
GoodNews(p ) => Happy1(p ) Happy1(p ) BadNews(p ) => Sad(p ) Happy2(p ) <=> ¬Sad(p ) Happy2(p )
 01 '1  2  03 '2 where each 'i represents a set of singleton rules , one for each value of p ( ie , for each specific person in a given testbed ) . This program contains two subprograms , 1 = { 01 , '1} and 2 = { 2 ,  03 , '2} , that can be solved independently with any inference algorithm .
As illustrated in Figure 2 , the output of our decomposition method is a bipartite graph between a set of subprograms and a set of relations . In a program level approach , FELIX attaches an inference algorithm to each subprogram ; we call this pairing of algorithm and subprogram a task . We discuss how to select decompositions and assign algorithms in Section V .
B . Message Passing
We apply the master slave message passing scheme [ 3 ] , [ 4 ] for both MRF level and program level decomposition . The master slave approach alternates between two steps : ( 1 ) perform inference on each part independently to obtain each part ’s predictions on shared variables , and ( 2 ) a process called the Master examines the ( possibly conflicting ) predictions and sends messages in the form of Lagrange multipliers to each task . The reader can consult [ 3 ] , [ 4 ] for details.6
C . Producing the Final Solution
When inference stops , for some variables all of their copies might not have the same value . The last step is to choose a final solution based on solutions from the decomposed parts .
MRF level Decomposition : To obtain a solution to the original MLN from individual solutions on each tree , we use the heuristic proposed by Komogorov et al . [ 2 ] ( and used in Komodakis et al . [ 4 ] ) that sequentially fixes shared variable values based on max product messages in individual trees . Program level Decomposition : If a shared relation is not subject to any hard rules , FELIX takes majority votes from the predictions of related tasks . ( If all copies of this relation have converged , the votes would be unanimous . ) To ensure that hard rules in the input MLN program are
6For MRF level decomposition , we can perform exact running the max product algorithm [ 5 ] on each component . inference by not violated in the final output , we insist that for any query relation r , all hard rules involving r ( if any ) be assigned to a single task , and that the final value of r be taken from this task.7 This guarantees that the final output is a possible world for   ( provided that the hard rules are satisfiable ) .
V . SPECIALIZED TASKS
With program level decomposition , we can use different algorithms for different subprograms . FELIX uses specialized algorithms to handle tasks , which can be more efficient than generic MLN inference . As an existence proof , we describe several tasks that are common in text processing.8 Classification : Classification is a fundamental statistical problem and ubiquitous in applications ; it arises in Markov logic as a query predicate R(x , y ) with hard rules like
R(x , y1 ) ^ y1 6= y2 => ¬R(x , y2 ) , which mandates that each object ( represented by a possible value of x ) can only be assigned at most one label ( represented by a possible value of y ) .
If the only query relation in a subprogram  i is R and R is mentioned at most once in each rule in  i ( except the rule above ) , then  i is essentially a logistic regression ( LR ) classification model . The inference problem for LR is trivial given model parameters ( here rule weights ) and feature values ( here ground formulae ) .
Suppose there are N objects and K labels . Then the memory requirement for LR is O(N K ) . On the other hand , it would require N K
2  factors to represent the above rule in an MRF . For tasks such as entity linking ( eg , mapping textual mentions to Wikipedia entities ) , the value of K could be in the millions .
Other Tasks : FELIX also supports other tasks , including ( 1 ) Segmentation and ( 2 ) Coreference . Segmentation encodes linear chain CRF models [ 6 ] that contain both unigram and bigram features . Coreference takes a set of N strings and decides which strings represent the same real world entity . FELIX implements the Viterbi algorithm [ 6 ] for segmentation and Singh et al . ’s algorithm [ 12 ] for coreference.9
VI . EXPERIMENTS
Our main hypotheses are that ( 1 ) MRF level decomposition can outperform monolithic inference , and ( 2 ) FELIX can outperform both MRF level decomposition and monolithic inference . We also validate that specialized algorithms indeed have higher efficiency and quality than generic MLN inference algorithms or MRF decomposition .
7This policy might result in cascaded subtasks . More sophisticated policies are an interesting future direction .
8We describe how FELIX automatically detects these tasks given an MLN program in the full version [ 8 ] .
9We will describe these tasks in more details in the full version [ 8 ] .
# Relations in MLN 17 18 2 7 7
IE IERJ NER KBP KBP+
# MLN Rules
34 357 4 6 6
# Evidence Tuples 150K 150K 10K 4.3M 240M
# MRF Factors 137K 564K 1.7M 20M 64B
DB Size
11MB 44MB 134MB 1.6GB 5.1TB
Table I
DATASET SIZES .
Datasets and MLNs : We use two publicly available MLN testbeds from ALCHEMY ’s website.10 In addition , we create an MLN program for named entity recognition based on skip chain CRFs [ 13 ] , and an MLN program that we developed for TAC KBP , a knowledge base population challenge.11 Table I shows some statistics of these datasets . We describe the MLN testbeds from ALCHEMY ’s website : ( 1 ) IE , where one performs segmentation on Cora citations using unigram and bigram features ( see the “ Isolated ” program [ 9] ) ; ( 2 ) IERJ , where one performs joint segmentation and entity resolution on Cora citations ( see the “ Jnt SegER ” program [ 9] ) . The last two are ( 3 ) NER , where one performs named entity recognition on a dataset with 10K tokens using the skip chain CRF model [ 13 ] ( encoded in three MLN rules ) ; and ( 4 ) KBP , which is an implementation of the TAC KBP ( knowledge base population ) challenge using an MLN that performs entity linking ( mapping textual mentions to Wikipedia entities ) , slot filling ( mapping cooccurring mentions to a set of possible relationships ) , entitylevel knowledge base population , and fact verification from an existing partial knowledge base .
Among those datasets , IE , and NER are simpler programs as they only have unigram and sparse bigram rules and classification constraints with very few labels ; IERJ , and KBP are more complex programs as they involve transitivity rules and classification constraints with many labels . To test the scalability of FELIX , we also run the KBP program on a 1.8M doc TAC KBP corpus ( “ KBP+ ” ) .
Experimental Setup : We run TUFFY [ 7 ] and ALCHEMY as state of the art monolithic MLN inference systems . We implement MRF level decomposition and program level decomposition ( ie , FELIX ) approaches on top of the opensource TUFFY system.12 As TUFFY has similar or superior performance to ALCHEMY on each dataset , here we use TUFFY as a representative for state of the art MLN inference and report ALCHEMY ’s performance in the technical report version of this paper . We use the following labels for these three approaches : ( 1 ) TUFFY , in which TUFFY performs RDBMS based grounding and uses WalkSAT as its inference algorithm ; ( 2 ) TREE , in which we replace TUFFY ’s WalkSAT algorithm with tree based MRF level dual decomposition as described in Section IV ; and ( 3 ) FELIX , in which we implement program level dual decomposition as described in Sections IV and V . Table II lists FELIX ’s decomposition scheme for each dataset .
CRF
Coref WalkSAT
0 1 0 0
1 1 1 1
LR 1 1 0 2
IE IERJ NER KBP
0 0 1 0 Table II
FELIX .
NUMBER OF TASKS OF EACH TYPE IN THE DECOMPOSITIONS USED BY
All three approaches are implemented in Java and use PostgreSQL 904 as the underlying database system . Unless specified otherwise , all experiments are run on a RHEL 6.1 server with four 2.00GHz Intel Xeon CPUs ( 40 total physical cores plus hyperthreading ) and 256 GB of RAM .
A . Overall Efficiency and Quality
We validate that TREE can outperform TUFFY and that FELIX in turn outperforms both TREE and TUFFY on complex programs . To support these claims , we compare the efficiency and quality of all three approaches on the datasets listed above . We run each system on each dataset for 5000 seconds ( except for the largest dataset KBP , for which we run 5 hours ) , and plot the MLN cost against runtime .
From Figure 3 we see that , on the two simpler programs ( ie , IE and NER ) , all three approaches were able to converge to about the same result quality . Although TREE and TUFFY have similar performance on IE , on NER the TREE approach obtains a low cost solution within two minutes whereas TUFFY takes more than one hour to reach comparable quality . FELIX has slower convergence behavior than TUFFY and TREE on IE , but it does converge to a similar solution . We note that alternate step size rules may improve FELIX ’s performance on these datasets .
On the more complex programs ( ie , IERJ and KBP ) , FELIX achieves dramatically better performance compared to TUFFY and TREE : while TUFFY and TREE fail to find a feasible solution ( ie , a solution with finite cost ) after 5000 seconds on each of these datasets , FELIX converges to a feasible solution within minutes . There are complex structures in these MLNs ; eg , transitivity for entity resolution and uniqueness constraints for entity linking in KBP . TUFFY and TREE were not able to find feasible solutions that satisfy such complex constraints , and the corresponding curves were obtained by replacing hard rules with a “ softened ” version with weight 100 . Still , we see that the results from both TUFFY and TREE are substantially worse than FELIX . We conclude that overall the FELIX approach is able to achieve significantly higher efficiency and quality than TUFFY and TREE approaches .
10http://alchemycswashingtonedu/ 11http://nlpcsqccunyedu/kbp/2010/ 12http://researchcswiscedu/hazy/tuffy
Scalability : To test scalability , we also run FELIX on the large KBP+ dataset with a parallel RDBMS ( from Greenplum Inc ) This MLN converges within a few iterations ;
$ t s o c
40000"
30000"
20000"
10000"
0"
Felix
Tuffy
Tree
IE$
160000"
120000"
80000"
40000"
0"
0"
1000"
2000"
3000"
4000"
5000"
6000"
!me$(sec)$
IERJ$
Tuffy*
Tree*
Felix
0"
1000"
2000"
3000"
4000"
5000"
6000"
!me$(sec)$
2000000"
1500000"
1000000"
500000"
0"
Tuffy*
Tree
NER$
Felix
0"
1000"
2000"
3000"
4000"
5000"
6000"
!me$(sec)$
600000"
450000"
300000"
150000"
0"
0"
Tuffy*
KBP$
Felix
Tree ran out of memory
5000"
10000"
15000"
20000"
!me$(sec)$
Figure 3 . High level performance results of various approaches to MLN inference . For each dataset and each system , we plot a time cost curve . Labels ending with an asterisk indicate that some points in the corresponding curves correspond to infeasible solutions ( ie , solutions with infinite cost ) , and the curves were obtained by “ softening ” hard rules to have weight 100 .
Task
CRF
System FELIX TUFFY TREE ALCHEMY
Final 35 s 186 s 911 s 760 s
Cost 4.6e5 6.4e5 4.7e5 1.4e6
F1 0.90 0.14 0.16 0.10
Initial 35 s 148 s 150 s 740 s Table III
PERFORMANCE AND QUALITY COMPARISON ON INDIVIDUAL TASKS . “ INITIAL ” ( RESP . “ FINAL ” ) IS THE TIME WHEN A SYSTEM PRODUCED THE FIRST ( RESP . CONVERGED ) RESULT . FOR TUFFY , THE HARD RULES
WERE SOFTENED BECAUSE OTHERWISE NO SOLUTION WAS FOUND . an iteration takes about five hours in FELIX . TUFFY and ALCHEMY failed to run on this dataset .
B . Specialized Tasks
We next validate that the ability to integrate specialized tasks into MLN inference is key to FELIX ’s higher efficiency and quality . To do this , we demonstrate that FELIX ’s specialized algorithms outperform generic MLN inference algorithms in both quality and efficiency when solving specialized tasks . To evaluate this claim , we run FELIX , TUFFY , TREE , and ALCHEMY on three MLN programs encoding a CRF task.13 We measure application quality ( F1 scores ) on a subset of the CoNLL 2000 chunking dataset.14 As shown in Table III , while it always takes less than a minute for FELIX on CRF , the other approaches take much longer . Moreover , FELIX has the best inference quality ( ie , cost ) and application quality ( ie , F1 ) .
VII . CONCLUSION
We study how to apply dual decomposition to Markov logic inference . We find that MRF level decomposition empirically outperforms traditional , monolithic approaches to MLN inference on some programs . However , MRF level decomposition ignores valuable structural hints in Markov logic programs . Thus , we propose an alternative decomposition strategy that partitions an MLN program into high level tasks ( eg , classification ) that can be solved with specialized algorithms . On several datasets , we empirically show that our program level decomposition approach outperforms both monolithic inference and MRF level decomposition approaches to MLN inference .
13We leave similar experiments on LR and CC to the full version [ 8 ] . 14http://wwwcntsuaacbe/conll2000/chunking/
VIII . ACKNOWLEDGEMENTS
We gratefully acknowledge the support of the DARPA Machine Reading Program under prime contract no . FA8750 09 C 0181 . CR is also generously supported by NSF CAREER award under IIS 1054009 , ONR award N000141210041 , and gifts or research awards from Google , Greenplum , and Oracle . The opinions and conclusions in this paper are not necessarily those of our sponsors .
REFERENCES
[ 1 ] V . Jojic , S . Gould , and D . Koller . Accelerated dual decom position for map inference . In ICML , 2010 .
[ 2 ] V . Kolmogorov . Convergent tree reweighted message passing for energy minimization . PAMI , 2006 .
[ 3 ] N . Komodakis and N . Paragios . Beyond pairwise energies : Efficient optimization for higher order MRFs . In CVPR , 2009 . [ 4 ] N . Komodakis , N . Paragios , and G . Tziritas . MRF optimizaIn tion via dual decomposition : Message passing revisited . ICCV , 2007 .
[ 5 ] F . Kschischang , B . Frey , and H . Loeliger . Factor graphs and the sum product algorithm . IEEE Transactions on Information Theory , 47(2):498–519 , 2001 .
[ 6 ] J . Lafferty , A . McCallum , and F . Pereira . Conditional random fields : Probabilistic models for segmenting and labeling sequence data . In ICML , 2001 .
[ 7 ] F . Niu , C . R´e , A . Doan , and J . Shavlik . Tuffy : Scaling up statistical inference in Markov logic networks using an RDBMS . In VLDB 2011 .
[ 8 ] F . Niu , C . Zhang , C . R´e , and J . Shavlik . Scaling inference for Markov logic via dual decomposition . Technical report , University of Wisconsin Madison , 2012 .
[ 9 ] H . Poon and P . Domingos . extraction . In AAAI , 2007 .
Joint inference in information
[ 10 ] M . Richardson and P . Domingos . Markov logic networks .
Machine Learning , 2006 .
[ 11 ] J . Shavlik and S . Natarajan . Speeding up inference in Markov logic networks by preprocessing to reduce the size of the resulting grounded network . In IJCAI , 2009 .
[ 12 ] S . Singh , A . Subramanya , F . Pereira , and A . McCallum . Large scale cross document coreference using distributed inference and hierarchical models . In ACL HLT , 2011 .
[ 13 ] C . Sutton and A . McCallum . Collective segmentation and labeling of distant entities in information extraction . Technical Report 04 49 , University of Massachusetts , 2004 .
[ 14 ] M . Wainwright , T . Jaakkola , and A . Willsky . MAP estimation via agreement on trees : Message passing and linear programming . IEEE Transactions on Information Theory , 2005 .
[ 15 ] L . Wolsey . Integer Programming . Wiley , 1998 .
