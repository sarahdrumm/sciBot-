Hashing with Generalized Nystr¨om Approximation
Jeong Min Yun1 , Saehoon Kim1 , Seungjin Choi1,2,3 1 Department of Computer Science and Engineering ,
2 Division of IT Convergence Engineering ,
Pohang University of Science and Technology , Pohang 790 784 , Korea
3 Department of Creative IT Excellence Engineering , Email : {azida,kshkawa,seungjin}@postechackr
Abstract—Hashing , which involves learning binary codes to embed high dimensional data into a similarity preserving low dimensional Hamming space , is often formulated as linear dimensionality reduction followed by binary quantization . Linear dimensionality reduction , based on maximum variance formulation , requires leading eigenvectors of data covariance or graph Laplacian matrix . Computing leading singular vectors or eigenvectors in the case of high dimension and large sample size , is a main bottleneck in most of data driven hashing methods . In this paper we address the use of generalized Nystr¨om method where a subset of rows and columns are used to approximately compute leading singular vectors of the data matrix , in order to improve the scalability of hashing methods in the case of high dimensional data with large sample size . Especially we validate the useful behavior of generalized Nystr¨om approximation with uniform sampling , in the case of a recentlydeveloped hashing method based on principal component analysis ( PCA ) followed by an iterative quantization , referred to as PCA+ITQ , developed by Gong and Lazebnik . We compare the performance of generalized Nystr¨om approximation with uniform and non uniform sampling , to the full singular value decomposition ( SVD ) method , confirming that the uniform sampling improves the computational and space complexities dramatically , while the performance is not much sacrificed . In addition we present low rank approximation error bounds for generalized Nystr¨om approximation with uniform sampling , which is not a trivial extension of available results on the nonuniform sampling case .
Keywords CUR decomposition ;
Nystr¨om approximation ; uniform sampling ; pseudoskeleton hashing ; generalized approximation ;
I . INTRODUCTION for to methods
Hashing refers embedding highdimensional data into a low dimensional Hamming space such that similar data points are indexed by binary codes with small Hamming distances [ 1 ] , [ 2 ] . A variety of hashing methods have been developed , since hashing has been shown to be better suited to approximate similarity search , especially for high dimensional data , compared to tree structureexploited methods [ 3 ] . An earlier work on notable dataindependent method is locality sensitive hashing ( LSH ) [ 1 ] where random projections followed by rounding are used to generate binary codes such that two objects in database within a smaller distance are shown to have a higher probability of collision . However , for successful performance in real world applications , LSH usually requires longer binary codes or multiple hash tables [ 1 ] .
Data dependent hashing , which is also known as ’learning to hash’ seeks a similarity preserving embedding into a Hamming space , where compact binary codes for efficient indexing are learned from a set of training examples . Spectral hashing ( SH ) [ 2 ] is a representative unsupervised hashing method , where a subset of eigenvectors of graph Laplacian matrix is rounded to determine binary codes . Semantic hashing [ 4 ] uses multi layers of restricted Boltzman machines to learn a non linear mapping between input data and binary code bits in supervised manner . Semi supervised learning , which uses both plenty of unlabeled examples and small number of labeled examples , has also been applied to hashing [ 5 ] , [ 6 ] , where hash function is mainly learned from labeled data while unlabeled data are used for regularization . Many data dependent hashing methods employ linear dimensionality reduction as an initial step , in order to project high dimensional data into a low dimensional subspace , which often requires the computation of leading eigenvectors of data covariance matrix [ 7 ] or graph Laplacian matrix [ 2 ] . Low dimensional projection is followed by binary quantization to yield compact binary codes . Subspace computed by principal component analysis ( PCA ) was shown to be a promising solution to low dimensional projection of hashing in information theoretical perspective [ 7 ] .
One of state of the arts along this direction , which is also of our interest in this paper , is PCA+ITQ [ 8 ] , where one project data points into principal subspace and determine binary codes by iteratively minimizing the quantization error of mapping the projected data to the vertices of a zerocentered binary hypercube . The scalability of PCA+ITQ is limited by the computational and space complexities required to compute leading singular vectors of the data matrix . For instance , in the case of a dataset which contains hundreds of thousands of samples with each sample represented by hundreds of attributes , all data points are fit in 24 Gbytes memory , so full SVD is affordable . However , for larger scale dataset , such as Holidays + Flickr1M dataset [ 9 ] which contains a million of samples with each sample corresponding to ten thousand dimensional vector , about 90 Gbytes is required to store all data points . A usual standard machine with 32 Gbytes memory cannot afford to use the full SVD . PCA+ITQ becomes quickly impractical as the size of dataset grows , although hashing has been developed to handle large scale data . To overcome this limitation , we employ a sampling based method to approximately compute leading singular vectors of a large scale data matrix using a subset of columns and rows .
Sampling based methods are attractive and powerful techniques for approximately computing SVD or spectral decomposition , since they operate on a small block of the original matrix . Column sampling method [ 10 ] approximates the SVD of a rectangular matrix by using the SVD of a small block of sampled columns . Nystr¨om method [ 11 ] provides a low rank approximation of a symmetric positive semidefinite matrix , making use of the SVD of the intersection of sampled columns with the corresponding rows of the original matrix . Recent studies in [ 12 ] reveal the similarities and relative advantages between these two methods . We refer to the extension of Nystr¨om method to a rectangular matrix as generalized Nystr¨om method , which is also known as pseudoskeleton approximation [ 13 ] or CUR decomposition [ 14 ] , [ 15 ] , [ 16 ] . It uses only a subset of columns and rows to approximate a rectangular matrix .
The sampling strategy is an important component of sampling based methods . Nystr¨om method with uniform sampling without replacement has shown to be quite efficient both in time and space in practice and its performance bound on approximation error was derived [ 17 ] . Nystr¨om method with non uniform sampling , where columns are sampled from a fixed distribution was also studied in [ 18 ] . The performance of CUR decomposition with non uniform sampling was analyzed in [ 14 ] , [ 15 ] , [ 16 ] . In this paper we present bounds on approximation error , for generalized Nystr¨om method , when used with uniform sampling without replacement . This analysis is motivated by seminar work on CUR decomposition [ 14 ] , [ 15 ] , [ 16 ] , but our results are not trivial extension of existing work .
The main contribution of this paper is two fold : ( 1 ) the use of generalized Nystr¨om method in PCA+ITQ to improve the scalability in both time and space ; ( 2 ) approximation error bound for generalized Nystr¨om method with uniform sampling . Experiments on several widely used benchmark datasets demonstrate that the performance of generalized Nystr¨om approximation with uniform sampling is not much sacrificed in PCA+ITQ , while improving the computational and space complexities dramatically over the full SVD as well as non uniform sampling method .
II . GENERALIZED NYSTR ¨OM APPROXIMATION
A . Notation
Given X ∈ R SVD of X is denoted by X = U X ΣX V . U X,kΣX,kV . m×n and assuming wlog that m ≤ n , the X , where X k = X,k corresponds to rank k approximation using
SVD . It is known that X k = arg minrank( .X)≤k 'X −.X'F . k = X − X k = X,k , and the Moore Penrose pseudoinverse X U . X . We denote xi as the i th column vector
We define a complementary matrix X⊥ U⊥ X,kV ⊥ . X,kΣ⊥ X + = V X Σ+ of X , and xj as the j th row vector of X . fi n/cSC ∈ R
We also use the following sampling matrices [ 15 ] : Let c and r be the number of sampled columns and rows fi respectively , then a column sampling matrix is defined by SC = n×c , where [ SC]ij = 1 if xi is selected m/rSR ∈ in the j th trial , 0 otherwise . Similarly , SR = r×m can be defined . Using these , we define three different scaled sub matrices of X : C = XSC ∈ R m×c , R = SRX ∈ R r×n , and W = SRXSC ∈ R r×c .
R
B . Generalized Nystr¨om Method
WG,k
G,kC .
Given a symmetric positive semi definite ( SPSD ) matrix G ∈ R n×n , the standard Nystr¨om method [ 11 ] is popular as an approximate algorithm of eigenvectors and eigenvalues of G . It firstly builds two submatrices of G : C G = GSC and of G as follows : G ≈ 'Gk = C GW + W G = S . C GSC , then constructs a k rank approximation eigenvectors and eigenvalues are 'U G = C GU WG,kΣ+ G . Approximate and 'ΣG = ΣWG,k respectively . For an arbitrary matrix X ∈ R m×n , we consider a direct generalization of the standard Nystr¨om method [ 13 ] like this : With C = XSC , R = SRX , and W = SRXSC , X ≈ffX k = CW + WkU W,kR , tors , and singular values are 'U X = CV W,kΣ+ W,k , 'V X = W,k , and 'ΣX = ΣW,k , respectively . From this , you can see that 'U X k R ≈ X . rows of X , the computation times of 'U X , 'V X , and 'ΣX Without considering the sampling time of columns and are O(mck + min{c2r , cr2} ) , O(nrk + min{c2r , cr2} ) , and O(min{c2r , cr2} ) respectively . The reconstruction of X requires O(mnk + min{c2r , cr2} ) time . For the sampling time , if we use the uniform sampling without replacement strategy , the algorithm takes additional O(m + n ) time , whereas it needs O(mn ) additional time with the nonuniform sampling strategy . where approximate left singular vectors , right singular vecR.U W,kΣ+
. X = CW + k R = CV W,kΣ+
( 1 )
'ΣX
'V
C . Previous Error Bound Analysis of Generalized Nystr¨om Generalized Nystr¨om method is designed to approximate some components of X k = U X,kΣX,kV . 'X −ffX k'F after forming a low rank approximation ffX k X,k , and its quality is usually measured by the low rank approximation error : form ffX k : If only U X,k is approximated as 'U X , we form ffX k as 'U X using the approximated components . Based on how many components of X k are approximated , there are two ways to
. X X , which is an approximate projection of
'U
X onto the subspace spanned by 'U X 1 . We call this ‘matrix projection’ [ 12 ] . If all three components are available , ffX k can be formed by 'U X rithm only uses 'U X , so our theoretical justification about
. X , and we refer this to ‘matrix reconstruction’ . As you will see in Section III B , the algo
'ΣX
'V its quality is based on ’matrix projection’ .
For matrix projection , there are two theoretical studies about the low rank approximation error bound with the Frobenius norm [ 10 ] , [ 14 ] . Both methods basically sample rows and columns based on the non uniform distribution , and show that the resulted ffX k satisfies
'X −ffX k'F ≤ 'X − X k'F + 'X'F
( 2 ) with high probability . The main difference of those is the required number of rows and columns to achieve the above bound . [ 10 ] requires Ω(k2/ 4 ) , and [ 14 ] requires Ω(max{k4/c3 , k2/ 4} ) .
III . HASHING WITH GENERALIZED NYSTR ¨OM
In this section , we explain why principal subspace is useful for data dependent hashing , and suggest the generalized Nystr¨om approximation scheme , especially with uniform sampling to handle large scale high dimensional data .
A . PCA based Hashing Given a data matrix X = [ x1 ··· xn ] ∈ R m×n , hashing m → {+1,−1}k with aims to find a hash function h : R k << m , such that for all i , j ∈ {1 , . . . , n} , Hamming distance between h(xi ) and h(xj ) is small if xi and xj are semantically similar . We use hi(· ) to represent an ith entry of the vector h(· ) , and define Y = [ y ··· yn ] ∈ {+1,−1}k×n , where yi = h(xi )
1
To produce compact binary codes , data dependent hashing mostly requires two conditions in which both are designed to maximize the information from the hash bits [ 2 ] : i=1
• Balanced condition , hj(xi ) = 0 , is a solution of an entropy maximization problem of the j th hash bit ( max H[hj(x) ] ) [ 7 ] .
• Uncorrelated condition , n−1Y Y .
= I , enforces that the hash bits are uncorrelated each other , so that there is no redundancy between the hash bits . n
However , finding a hash function which satisfies the above conditions is an NP hard problem [ 2 ] , so some kind of relaxation of them is needed for practical use . With a restriction of the form of the hash function ; for all j ∈ {1 , . . . , k} , hj(x ) = sgn(u . j x ) where uj ∈ R m and sgn(z ) = z|z| , [ 7 ] shows that the balanced condition is lower bounded by the scaled variance of the projected data ; max H[hj(x ) ] ≥ c · var[u . j x ] . And by relaxing the uncorrelated condition to the orthogonal constraint as
1We call this an ‘approximate’ projection since .U
2 X = .U X , the definition of the projection , may not be satisfied depending on the approximation algorithms .
U.U = I , where U = [ u1 ··· uk ] , they suggest the following optimization problem for the hash function learning : var[u . j xi ] , where U.U = I .
( 3 ) n k arg max
U i=1 j=1
This problem is equivalent to PCA , and the binary codes are obtained by applying sgn(· ) to each entry of the projected data UX However , applying sgn(· ) directly to this PCAprojection is problematic in practice . In the relaxed optimization problem , just a summation of variance of hash bits is maximized , but the actually good hash functions should have enough information ( variance ) for each hash bit . For the PCA of real world datasets , only top few eigenvectors have a large variance , so that most hash functions are obtained from the directions that have a small variance ; as a result , the performance is degraded as the code length increases [ 5 ] , [ 8 ] . To avoid this situation , [ 5 ] adds a penalty term to the objective function for the orthogonality instead of the constraint . [ 8 ] proposes an iterative quantization ( ITQ ) method , which iteratively minimizes the quantization error of sgn(· ) by rotating principal components .
Algorithm 1 PCA + ITQ with generalized Nystr¨om approximation ( uniform sampling ) Input : training data is X = [ x1 ··· xn ] ∈ R m×n , test data m , and binary code length is k . c and r are the is x ∈ R number of sampled columns and rows respectively . fi fi
Output : binary code y associated with x 1 : Sample c columns of X uniformly at xt1
, . . . , xtc , and form C = n c [ xt1 random as
··· xtc ] . random as . ··· ( ctr ) .
]
. m
. r [ (ct1 ) ct1 , . . . , ctr , then W =
2 : Sample r rows of C uniformly at 4 : Approximate left singular vectors 'U X = CV W,kΣ+ 3 : Apply SVD to W and obtain W k = U W,kΣW,kV . 0 is a random rotation matrix . 'U , where Y iX.'U X is decomposed by 8 : Qi = MM Y i = sgn(Q . i−1 SVD as M Ω.M
5 : Q 6 : for i = 1 , . . . , 50 do 7 :
. X X ) .
W,k . W,k .
.
.
9 : end for 10 : Return k bit binary code : y = sgn(Q .
50
. X x ) .
'U
B . PCA + ITQ with Generalized Nystr¨om Approximation
Although PCA + ITQ provides a promising solution in many cases , PCA is very time consuming for a large scale high dimensional dataset . So we suggest the generalized Nystr¨om method with uniform sampling to approximate PCA , whose time complexity is only linear at m . Because the eigenvectors of XX . is equivalent to the left singular vectors of X , we use the approximate left singular vector of X from the generalized Nystr¨om method as an eigenvector approximation of XX . uniform sampling case is shown in Algorithm 1 .
. The overall procedure for the
IV . EXPERIMENTS
In this experiment , we compare our generalized Nystr¨om with uniform sampling + ITQ ( Uniform + ITQ ) to full PCA ( full PCA + ITQ ) , generalized Nystr¨om with non uniform sampling ( Non Uniform + ITQ ) , and random projection + ITQ ( Random + ITQ ) . For full PCA , we compute the data covariance and extract the top k eigenvectors . For generalized Nystr¨om with non uniform sampling , we implement the ConstantTimeSVD algorithm in [ 14 ] . For random projection , we use the standard normal random matrix .
We use three widely used benchmark datasets : CIFAR10 [ 19 ] , NUS WIDE [ 20 ] , and Holidays + Flickr1M [ 9 ] . Although CIFAR 10 ( 60K × 1584 ) and NUS WIDE ( 270K × 634 ) are small scale so that full PCA could be performed very fast , we include them to check that Uniform + ITQ even works in the small scale . Holidays + Flickr1M consists of 1M images crawled from Flickr2 ( Flickr1M ) and 500 queries with a few number of ground truth images for each ( Holidays ) . We extract 12,800 dimensional vectors for each image using VLAD descriptor [ 9 ] . All experiments are conducted five times using Intel i7 with 32 Gbytes memory . Matrix operations are implemented by Matlab , and disk I/O operation ( data loading ) and the selection algorithm for nonuniform sampling are implemented by C++ . For evaluation , we compute mean average precision ( mAP ) as in [ 8 ] . For Holidays + Flickr1M , we compute mAP in the following way since they provide only the small number of groundtruth matches of each queries ( usually 2 or 3 ) . For each query , we retrieve 100 candidates ( or slightly more if there is a tie ) using hash bits , and compute an average precision from the re sorted candidates ; re sorting is based on the projected data matrix from each method , since sorting with the original features needs additional disk I/O operation .
Fig 1 represents mAP comparison over hash bits of various hashing methods with fixed sampling ratios , 5 % for samples and 30 % for features . We observe that approximate principle components , from both Uniform + ITQ and NonUniform + ITQ , are enough to learn an effective hash function , using only a small subset of samples and features . We also observe that the hashing performance of Uniform + ITQ is definitely superior to Random + ITQ since random projection does not approximate principle components . Fig 2 compares mAP and the computation time over various sampling ratios on Holidays + Flickr1M dataset . As you can see in ( a ) and ( b ) , mAP of Uniform + ITQ and NonUniform + ITQ become comparable to full PCA + ITQ with the sampling ratios larger than 5 % for samples and 30 % for features . In ( c ) and ( d ) , we see that the computation time of Uniform + ITQ is at least twice faster than full PCA
2http://wwwflickrcom/
0.35
0.3
0.25
0.2 full PCA + ITQ Non Uniform + ITQ Uniform + ITQ Random + ITQ
0.35
0.3
0.25
0.2 full PCA + ITQ Non Uniform + ITQ Uniform + ITQ Random + ITQ i i n o s c e r p e g a r e v a n a e m i i n o s c e r p e g a r e v a n a e m
0.51
2 sampling ratio ( % ) of samples
5
10
2
10 sampling ratio ( % ) of features
20
30
( a ) full PCA + ITQ Non Uniform + ITQ Uniform + ITQ Random + ITQ
6000
5000
) c e s ( e m i t
4000
3000
2000
( b ) full PCA + ITQ Non Uniform + ITQ Uniform + ITQ Random + ITQ
6000
5000
) c e s ( e m i t
4000
3000
2000
1000
0.51
2 sampling ratio ( % ) of samples
5
10
1000 2
10
20
30 sampling ratio ( % ) of features
40
40
( c )
( d )
Figure 2 . mAP ( the first row ) and the computation time ( the second row ) over various ratios ( % ) of samples and features on Holidays + Flickr1M . Each hashing methods use 256 hash bits . The sampling ratio of features is 30 % in the left column , and that of samples is 5 % in the right column . and Non Uniform + ITQ , and as fast as random projection . Specifically with 256 hash bits , full PCA + ITQ takes about 4,000 sec for PCA , 1,500 sec for data projection , and 400 sec for ITQ . Note that data projection and ITQ are also needed for other methods . In case of 5 % samples , Uniform + ITQ takes about 250 sec for PCA approximation , while Non Uniform + ITQ needs additional time for the sampling probability computation ( about 1,100 sec ) and the selection algorithm ( about 520 sec ) .
V . APPROXIMATION ERROR BOUND FOR GENERALIZED
NYSTR ¨OM WITH UNIFORM SAMPLING
Here , we provide the approximation error bound for matrix projection of X when the generalized Nystr¨om method with uniform sampling without replacement is applied .
A . Concentration Bound for Matrix Multiplication
To derive our bound , we use the concentration bound for approximate matrix multiplication [ 17 ] : Theorem 1 . ( Theorem 2 of [ 17] ) . Given X ∈ R and Y ∈ R ( δ ∈ ( 0 , 1 ) , α(u , v ) = m×n C Y . Let 1−1/(2 max{u,v} ) , and η = n×p , define C = XSC , R = S . u+v−1/2 uv
1
2 log(2/δ)α(c,n−c ) c
. Then , the following holds 'XY − CR'F ≤ ( 1 + η)n√ 'xi' max ( with probability at least 1 − δ . If Y = X . log(2/δ)α(c,n−c ) inequality satisfies with η = . max c j i c
'yj' ,
( 4 )
, the above
We also use the spectral norm version of Theorem 1 : i i n o s c e r p e g a r e v a n a e m
0.18
0.16
0.14
0.12
0.1
16 32
64 mAP for CIFAR 10 mAP for NUS WIDE mAP for Holidays + Flickr1M i i n o s c e r p e g a r e v a n a e m full PCA + ITQ Non Uniform + ITQ Uniform + ITQ Random + ITQ
0.34
0.33
0.32
0.31
0.3
0.29 i i n o s c e r p e g a r e v a n a e m full PCA + ITQ Non Uniform + ITQ Uniform + ITQ Random + ITQ
128 number of hash bits
256
16
32
64
48 number of hash bits
128
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0 16 32
64 full PCA + ITQ Non Uniform + ITQ Uniform + ITQ Random + ITQ
128 number of hash bits
256
Figure 1 . mAP comparison of various hashing methods . For Non Uniform + ITQ and Uniform + ITQ , we fix the sampling ratios of samples and features to 5 % and 30 % respectively . For all figures , standard deviation ( n = 5 ) are represented as error bars .
(
Corollary 1 . Let δ ∈ ( 0 , 1 ) , η = with probability at least 1 − δ ,
2 log(2/δ)α(c,n−c ) c
. Then ,
'XY − CR'F ≤ ( 1 + η)n√ c
'X'2'Y '2 ,
( 5 )
B . Modification of Generalized Nystr¨om Approximation
We add a simple truncation step to the original method , to ideas from the ConstantTimeSVD algorithm in [ 14 ] , bound W + k . For γ > 0 , We first compute SVD of W = min{k,|{i : as W k = U W,kΣW,kV W,k , and let kff i ( W k ) ≥ γ'C'1'C'∞}|} . Then , a modified generalized σ2 X ≈ffX k = CW + Nystr¨om approximation of A is k . R = CV W,k . Σ+
W,k . U .
W,k . R .
( 6 )
C . Main Theorem c r
(
The following theorem is our main result .
( m×n . Let δ ∈ ( 0 , 1 ) , Theorem 2 . Given a matrix X ∈ R 2 log(4/δ)α(r,m−r ) log(4/δ)α(c,n−c ) η1 = , and η2 = . For 1 and r ≥ 1/(γ2 2 1 , 2 > 0 , if c ≥ 4k/ 2 replacement produces 'U X which satisfy 2 ) , then the modified Generalized Nystr¨om method with uniform sampling without 'X − 'U X 'U X X'F ≤ 'X − X k'F + fi n 1(1 + η1 ) maxi 'xi' + {m 2(1 + η2 ) + E}'X'2 k − kff . with prob . at least 1 − δ , where E = 'U X X'F Direct proof is not easy , so we divide 'X −'U X into two subproblems using the triangle inequality as 'X − . X X'F , and derive an upper k X'F +'CC + CC + bound for each as follows . 1 ) Upper Bound for 'X − CC + k X'F : If we use Theorem 1 ( concentration bound ) and Theorem 2 of [ 14 ] , we can directly derive the following low rank approximation error bound from the fact that CC + Theorem 3 . Given a matrix X ∈ R η = m×n . Let δ ∈ ( 0 , 1 ) and . For > 0 , if c ≥ 4k/ 2 , then with k X−'U X k X = U C,kU .
√ 2(C⊥ k ( F √ γ(C(2 log(2/δ)α(c,n−c )
C,kX :
(
'U
√
+ c
'xi' . n ( 1 + η ) max i
'U . X X'F : probability at least 1 − δ , 'X − CC + fi k X'F ≤ 'X − X k'F + k X − 'U X 2 ) Upper Bound for 'CC + 'U k X − 'U X . X X'F 'C.C − W .W'F + E
Lemma 1 . 'CC +
)
≤
1
γ'C'2
2 where E = E(C , γ , k , kff
√ 2(C⊥ k ( F √ γ(C(2 Proof : Let Y = ΣCV . C V W,k . Σ+
ΣC,kV .
C,kV W,k . Σ+ = 'U C,kU . ≤ 'U C,kU .
+
( 7 )
) =
'X'2 , √ k − kff . 'U k X − 'U X W,k . and Y k = . X X'F C X'F C,kX − U CY Y U C,k − U CY Y U C'F'X'2 , ff fi
W,k Then , 'CC +
F
C
C,k
C,k fi fi
+ tr
F = tr
A.A
( 8 ) where ( 8 ) follows from submultiplicity for Schatten p norms . Since for any A and B , 'A'2 , tr [ AB ] = tr [ BA ] , and tr [ A + B ] = tr [ A ] + tr [ B ] , fi ff ff C,k − U CY Y U C'2 'U C,kU . ff fi ff − tr U C,kU . U C,kY kY U = tr ff fi ff fi −tr U CY Y .Y Y U k U . U CY Y . ff fiffi fi ff fl = tr [ I k ] − 2tr Y . Y .Y Y .Y k Y k fi ff tr [ I k . ] − 2tr Y .Y Y .Y Y .Y + tr +(k − kff Y ⊥ . k Y ⊥ ) + 2tr = 'I k . − Y .Y '2 k '2 F + ( k − kff F + 2'Y ⊥ (
Remaining derivation for 'I k . − Y .Y '2 identical to Lemma 2 of [ 14 ] , then lemma follows . Lemma 2 . Let δ ∈ ( 0 , 1 ) , η = . For > 0 , if r ≥ 1/(γ2 2 ) , then with probability at least 1 − δ , ( 10 )
'C.C − W .W'F ≤ m ( 1 + η ) .
F and 'Y ⊥ k '2
2 log(2/δ)α(r,m−r )
( 9 ) F is
+ tr
) .
=
1
C k r
γ'C'2
2
Proof : By applying Corollary 1 with δ , we get 'C.C − W .W'F ≤ m(1 + η)√
'C'2 r
.
2
[ 7 ] J . Wang , S . Kumar , and S . F . Chang , “ Sequential projection learning for hashing with compact codes , ” in Proceedings of the International Conference on Machine Learning ( ICML ) , Haifa , Israel , 2010 .
( 11 )
Lemma immediately follows from our choice of r .
By combining Theorem 3 , Lemma 1 and 2 , we get Theorem 2 by assigning 2/δ for each events , Theorem 3 and Lemma 2 , and applying the union bound .
VI . CONCLUSIONS
In this paper , we proposed a scalable learning to hash algorithm , in which the generalized Nystr¨om method with uniform sampling is applied to PCA + ITQ [ 8 ] . With the large scale high dimensional dataset , Holidays + Flickr1M [ 9 ] , we showed that by only using 5 % of samples and 30 % of features , the method achieves comparable performance to the case of full PCA as well as non uniform sampling , while the overall computation time is reduced to less than half . We also provided the first theoretical bound analysis of the generalized Nystr¨om method with uniform sampling .
ACKNOWLEDGMENTS
This work was supported by NIPA MSRA Creative IT/SW Research Project , NIPA ITRC Support Program ( NIPA 2012 H0301 12 3002 ) , National Research Foundation ( NRF ) of Korea ( 2012 0005032 ) , MKE and NIPA ” IT Consilience Creative Program ” ( C1515 1121 0003 ) , and NRF World Class University Program ( R31 10100 ) .
REFERENCES
[ 1 ] A . Gionis , P . Indyk , and R . Motawani , “ Similarity search in high dimensions via hashing , ” in Proceedings of the International Conference on Very Large Data Bases ( VLDB ) , 1999 .
[ 2 ] Y . Weiss , A . Torralba , and R . Fergus , “ Spectral hashing , ” in Advances in Neural Information Processing Systems ( NIPS ) , vol . 20 . MIT Press , 2008 .
[ 3 ] S . Arya , D . M . Mount , N . S . Netanyahu , R . Silverman , and A . Y . Wu , “ An optimal algorithm for approximate nearest neighbor searching , ” Journal of the ACM , vol . 45 , no . 6 , pp . 891–923 , 1998 .
[ 4 ] R . Salakhutdinov and G . Hinton , “ Semantic hashing , ” in Proceeding of the SIGIR Workshop on Information Retrieval and Applications of Graphical Models , 2007 .
[ 5 ] J . Wang , S . Kumar , and S . F . Chang , “ Semi supervised hashing for scalable image retrieval , ” in Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition ( CVPR ) , San Francisco , CA , 2010 .
[ 6 ] S . Kim and S . Choi , “ Semi supervised discriminant hashing , ” in Proceedings of the IEEE International Conference on Data Mining ( ICDM ) , Vancouver , Canada , 2011 .
[ 8 ] Y . Gong and S . Lazebnik , “ Iterative quantization : A procrustean approach to learning binary codes , ” in Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition ( CVPR ) , Colorado Springs , CO , 2011 .
[ 9 ] H . J´egou , M . Douze , C . Schmid , and P . P´erez , “ Aggregating local descriptors into a compact image representation , ” in Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition ( CVPR ) , San Francisco , CA , 2010 .
[ 10 ] A . Frieze , R . Kannan , and S . Vempala , “ Fast Monte Carlo algorithms for finding low rank approximations , ” Journal of the ACM , vol . 51 , no . 6 , pp . 1025–1041 , 2004 .
[ 11 ] C . K . I . Williams and M . Seeger , “ Using the Nystr¨om method to speed up kernel machines , ” in Advances in Neural Information Processing Systems ( NIPS ) , vol . 13 . MIT Press , 2001 .
[ 12 ] S . Kumar , M . Mohri , and A . Talwalkar , “ On samplingbased approximate spectral decomposition , ” in Proceedings of the International Conference on Machine Learning ( ICML ) , Montreal , Canada , 2009 .
[ 13 ] S . A . Goreinov , E . E . Tyrtyshnikov , and N . L . Zamarashkin , “ A theory of pseudoskeleton approximations , ” Linear Algebra and Its Applications , vol . 261 , pp . 1–21 , 1997 .
[ 14 ] P . Drineas , R . Kannan , and M . W . Mahoney , “ Fast Monte Carlo algorithms for matrices II : Computing a low rank approximation to a matrix , ” SIAM Journal on Computing , vol . 36 , no . 1 , pp . 158–183 , 2006 .
[ 15 ] —— , “ Fast Monte Carlo algorithms for matrices III : Computing a compressed approximate matrix decomposition , ” SIAM Journal on Computing , vol . 36 , no . 1 , pp . 184–206 , 2006 .
[ 16 ] P . Drineas , M . W . Mahoney , and S . Muthukrishnan , “ Relativeerror CUR matrix decompositions , ” SIAM Journal on Matrix Analysis and Applications , vol . 30 , no . 2 , pp . 844–881 , 2008 .
[ 17 ] S . Kumar , M . Mohri , and A . Talwalkar , “ Sampling techniques for the Nystr¨om method , ” in Proceedings of the International Conference on Artificial Intelligence and Statistics ( AISTATS ) , Clearwater Beach , FL , 2009 .
[ 18 ] P . Drineas and M . W . Mahoney , “ On the Nystr¨om method for approximating a gram matrix for improved kernel based learning , ” Journal of Machine Learning Research , vol . 6 , pp . 2153–2175 , 2005 .
[ 19 ] A . Krizhevsky and G . E . Hinton , “ Learning multiple layers of features from tiny images , ” Computer Science Department , University of Toronto , Tech . Rep . , 2009 .
[ 20 ] T . S . Chua , J . Tang , R . Hong , H . Li , Z . Luo , and Y . Zheng , “ NUS WIDE : a real world web image database from national university of singapore , ” in Proceedings of the ACM International Conference on Image and Video Retrieval ( CIVR ) , Santorini , Greece , 2009 .
