Sparse Group Selection on Fused Lasso Components for Identifying Group specific
DNA Copy Number Variations
Ze Tian∗ , Huanan Zhang∗ and Rui Kuang∗
∗Department of Computer Science and Engineering
University of Minnesota Twin Cities , Minneapolis , Minnesota 55455–0213
Email : tianze@csumnedu , huanan@csumnedu , kuang@csumnedu
Abstract—Detecting DNA copy number variations ( CNVs ) from arrayCGH or genotyping array data to correlate with cancer outcomes is crucial for understanding the molecular mechanisms underlying cancer . Previous methods either focus on detecting CNVs in each individual patient sample or common CNVs across all the patient samples . These methods ignore the discrepancies introduced by the heterogeneity in the patient samples , which implies that common CNVs might only be shared within some groups of samples instead of all samples . In this paper , we propose a latent feature model that couples sparse sample group selection with fused lasso on CNV components to identify group specific CNVs . Assuming a given group structure on patient samples by clinical information , sparse group selection on fused lasso ( SGS FL ) identifies the optimal latent CNV components , each of which is specific to the samples in one or several groups . The group selection for each CNV component is determined dynamically by an adaptive algorithm to achieve a desired sparsity . Simulation results show that SGS FL can more accurately identify the latent CNV components when there is a reliable underlying group structure in the samples . In the experiments on arrayCGH breast cancer and bladder cancer datasets , SGS FL detected CNV regions that are more relevant to cancer , and provided latent feature weights that can be used for better sample classification .
Keywords sparse group learning ; fused lasso ; group lasso ;
DNA copy number variations ;
I . INTRODUCTION
There are normally two copies of each gene in the doublestranded DNAs of human genome . Alterations of the DNAs can lead to a different number of copies of the genes in the DNA . If a DNA region is deleted in one or both strands , there will be a fewer number of copies of the genes and on the contrary if a DNA region is duplicated , there will be a larger number of copies of the genes . These events of amplification or deletion of a large DNA segment on chromosomes are called DNA copy number variations ( CNVs ) . It has been confirmed in many recent studies that chromosomal aberrations of DNA copy numbers , rearrangement and structures have association with cancer and other diseases [ 1 ] . Among the aberrations , DNA copy number variations ( CNVs ) are believed to play an important role in tumorigenesis [ 2 ] .
DNA CNVs can be measured by comparative genomic hybridization ( CGH ) , which compares the copy number of a differentially labeled case sample with a normal reference DNA . ArrayCGH technology based on DNA microarray can currently allow genome wide identification of CNV regions by CGH measuring at a number of sampled locations at different resolutions [ 3 ] . ArrayCGH data provide important information of candidate cancer loci for the classification of patients and discovery of molecular mechanisms of cancers [ 4 ] . Thus , one of the main tasks of arrayCGH data analysis is to identify the CNVs represented as amplified and deleted regions on the chromosomes and correlate them with diseases . These regions are expected to encode important structural genomic variations related to the diseases .
II . RELATED WORK
Since CNV data are series of log intensity ratios at the sampled locations ( probes ) , the adjacent probe locations are more likely to be associated in the same CNV event . For single sample CNV detection , fused lasso appears to be a promising model [ 5 ] . In the fused lasso , L1 norm is used in the penalty term to smooth the data by encouraging sparsity of the data and also the sparsity of the change points . Specifically , the method finds the segmented series β by solving the optimization problem
( yi − βi)2 i j |βj| ≤ s1 and
ˆβ = argmin
β subject to j |βj − βj+1| ≤ s2 , where yi is the log2 ratio measurement of the ith probe and βi is the corresponding value after smoothing . Here , s1 controls the overall sparsity of CNV regions ( the number of nonzero values in β ) and s2 controls the number of CNV alterations ( the number of change points between the adjacent probes ) . By examining the non zero values in β , CNVs can be identified for the sample . The procedure is repeated for all the samples and the resulted segmented series are aggregated to report the identified common CNVs .
For multi sample CNV detection , all samples are analyzed simultaneously in one optimization framework to identify the amplification or deletion regions shared across all samples . For example , for p copy number profiles of length n , n−1 i=1
[ 6 ] and [ 7 ] proposed the following optimization problem
Y − U2 + λ min
U∈Rn×p
Ui+1,• − Ui,• , where Y is the n×p CNV profile matrix , U is the de noised segmentation approximating Y and Ui,• is the ith row of U . A fast group least angle regression ( LARS ) algorithm can be applied to solve the optimization framework approximately to detect shared change points from the multiple CNV profiles . Since the change points are detected from all profiles in the framework , it is expected to be more accurate than detecting change points independently from each CNV profile .
Under the same motivation that CNVs are usually shared by multiple samples , instead of approximating the profile matrix Y by a segmentation matrix of the same size , another more advanced modeling is to detect the shared CNVs as latent fused features by low rank matrix factorization decomposed from Y . For example , the widely used dimensionality reduction method principal component analysis ( PCA ) can decompose Y into orthogonal principle components . The projection of Y to a low dimensional space obtains coefficients of the principle components to preserve the variance . However , practically it is not feasible to interpret the principle components as CNVs since the principle components cannot be explained as CNV patterns without fusing the adjacent features with lasso .
More recently , a Fused Lasso Latent Feature Model ( FLLat ) was proposed by [ 8 ] for detecting latent CNV components . For the profile matrix Y with S samples and L probes , FLLat decomposes it as a weighted sum of a fixed number of latent feature components , which are smoothed by fused lasso . The corresponding optimization problem for FLLat is
S
L
Yls − J s=1 l=1 j=1 min Γ,Θ
2 J
J
L
|Γlj| j=1 l=1
|Γlj − Γl−1,j|
+ λ1
L
ΓljΘjs
+λ2 j=1 l=2 subject toS s=1 Θ2 js ≤ 1 for each j , where J is the number of latent features and Yls is the log intensity ratio of the lth probe for the sth sample . This model minimizes the sum of the square errors as well as the fused lasso penalties on the latent feature Γ . It is clear that the model does not assume any structure on the weights of the latent fused lasso components Θ .
III . METHOD
A . Motivation
It is well acknowledged that there is often groupinformation on the samples in cancer structured prior
Figure 1 . Outline of SGS FL . The arrayCGH data is factorized into the coefficient matrix and K CNV components . After the factorization , each CNV sample can be reconstructed by the sum of the components weighted by the coefficients . The fused lasso penalty on each component encourage the step function pattern to model real CNV events . The patient samples are divided into L groups . For each component , only the samples in the selected groups will have nonzero weights . For example , only samples in group 1 have nonzero weights for component 1 and only samples in group 1 and 2 have nonzero weights for component 3 . The group selection enforces the sparseness of the coefficients by groups . genomic datasets , which accounts for the heterogeneities among the patients . For example , the samples can be grouped by different tumor grades or stages , or by survival and metastatic status . The samples in each or some of the groups might be associated with CNV components that are only associated with the samples in the group(s ) . It is actually known that samples with different phenotypes also show different frequencies of CNVs . For example , low and medium grade tumors of bladder cancer generally contain few changes [ 9 ] . Thus , it is more biologically interesting to identify CNV patterns for the samples under the groupstructure given by prior information . To achieve this objective , we propose sparse group selection on fused lasso components ( SGS FL ) for integrating group information on the fused lasso components . SGS FL assumes a group structure on the component coefficients and attempts to select only a small number of groups for each component . SGS FL also requires that the coefficients of latent features to be non negative for better distinguishing CNVs as regions with amplifications or deletions . The outline of SGS FL is given in Fig 1 .
B . Notation
We denote the arrayCGH profiles on a chromosome as a m × n matrix Y where m is the number of samples and n is the number of probes . Yij is the log2 intensity ratio measured for the ith sample on the jth probe . Assume that the probes are ordered by their positions on the chromosome . The objective is to decompose Y into a m × K matrix X and a K × n matrix W such that 1 F is minimized where K is the number of the components of
2 Y − XW2
ArrayCGH Data Coefficients Fused Lasso Components Sample Probe Location Group 1 Group 2 ( cid:1709 ) Group L = Component 1 Component 2 Component 3 ( cid:1710 ) Component K Group 1 ( cid:1710 ) Group 2 ( cid:1709 ) ( cid:1709 ) ( cid:1709 ) Group L ( cid:3400 ) Component 1 Component 2 Component 3 ( cid:1709 ) ( cid:1709 ) Component K weighted sum of latent features latent features . Here W are the components of the latent features and X are the coefficients of the components for all samples . Each sample Yi,• is reconstructed by the linear k Xik · Wk,• . We further assume that the m samples are categorized into L disjoint groups as G = {g1 , g2 , . . . , gL} where gl ⊂ {1 , 2 , . . . , m} is the set of indexes of all samples in the lth group .
C . Regularization Framework
We propose the following optimization problem to minimize reconstruction error and fused lasso under the group selection constraints :
K min X,W
1 2
Y − XW2
F + λ1
|Wk,•|
K n
+λ2 k=1
|Wkj − Wk,j−1|
( 1 ) subject to k=1 j=2 blkXgl,k = 0 for l = 1 , . . . , L and k = 1 , . . . , K Xik ≥ 0 for i = 1 , . . . , m and k = 1 , . . . , K
X 2 ik = 1 for k = 1 , . . . , K m i=1 where blk is a binary indicator variable of selecting the lth group for the kth latent feature component and Xgl,k is a sub vector of X•,k with indexes i ∈ gl . If the m samples are divided into L strictly non overlapping groups , X can be rearranged into a matrix of L × K vectors as
 Xg1,1
XgL,1
X =
 .
··· Xg1,K ··· XgL,K
blk is introduced for group selection on each fused lasso component Wk,• . Specifically , if blk = 1 , all the coefficients of Xgl,k , the kth latent feature component from the lth group , need to be 0 ; otherwise if blk = 0 , the coefficients of Xgl,k can be any nonnegative weights . In other words , group l is selected for component k if blk = 0 . The blk acts as a gating of the weights Xgl,k on the component Wk,• and Wk,• is only specific to the samples in the selected groups ( with blk = 0 ) . In the optimization problem , blks are chosen dynamically in each iteration of the optimization algorithm according to a fixed global parameter r ∈ [ 0 , 1 ] . D . Sparse Group Selection
Given the parameter r ∈ [ 0 , 1 ] and the current weights and components , blks are determined for each latent feature component separately . We first define variance factors v(k ) = ( Y − X•,=kW=k,•)W k,• where X•,=k is the matrix after removing the kth column from X and W=k,• is the matrix after removing the kth row from W . Each v(k ) evaluates the importance of the component Wk,• to the reconstruction error . The larger the v(k ) , the more important the Wk,• . Then , the role of a group l to the component k is evaluated as flflflv(k ) flflfl |gl|Wk,•2 , gl
γ(k ) l =
( 2 ) where |gl| is the cardinality of gl and v(k ) is the sub vector of v(k ) with indexes in gl . The importance vector for group l is normalized by the size of group l and the 2 norm of component Wk,• . Then , we can sort the L groups by γ(k ) q2 ≥ ··· ≥ γ(k ) such that γ(k ) qL . Based on the ranking of the groups , blks are calculated by q1 ≥ γ(k ) gl l if l = 1 or l−1 qs /L s=1 γ(k ) s=1 γ(k ) qs < r fl 0 bql,k =
1 otherwise .
( 3 ) For each component Wk,• , at least one group is selected and additional groups are selected based on their importance proportional to the total importance . More intuition of group selection by the ranking of γ(k ) ql and its connection to group lasso are discussed in section III F .
E . Alternative Optimization
Eqn . ( 1 ) can be solved with alternative optimization to get an empirical solution . We alternate between fixing W and solving for X and vice versa until both W and X do not change anymore in the iterations . The complete SGSFL algorithm is described in Fig 2 . W is initialized with the first K principle components of Y computed by PCA ( line 1 ) . Then , we solve X column by column given W ( line 3 14 ) and solve W row by row given X ( line 1519 ) . The algorithm iterates until both X and W converge . Specifically , for the kth column of X , we first compute {blk} as in Eqn . ( 3 ) ( line 5 11 ) , and then update X•,k by solving the following sub optimization problem on line 12 :
1 2
Y − XW2
F
( 4 ) min X•,k subject to blkXgl,k = 0 , Xik ≥ 0 and m ik = 1 . Eqn . ( 4 ) can be solved with standard quadratic optimization techniques . This procedure is repeated iteratively for each column of X until X does not change anymore . Similarly , for each row of W , we solve the sub optimization problem to update Wk,• , i=1 X 2
1 2 min Wk,•
Y − XW2
F + λ1|Wk,•| + λ2
|Wkj − Wk,j−1| , ( 5 ) which is the fused lasso problem . We used the package provided by [ 10 ] in our implementation to solve Eqn . ( 5 ) on line 17 . j=2 n
Input : arrayCGH data Y ∈ Rm×n , the number of latent features K ∈ Z+ , the group sparsity controlling parameter r ∈ [ 0 , 1 ] , the parameters λ1 , λ2 ∈ R+ for lasso and fused lasso penalties . Output : The non negative coefficient matrix X ∈ Rm×K , the latent feature matrix W ∈ RK×n .
1 : Initialize W as the first K principle components of Y and X as 0 . repeat
2 : repeat 3 : 4 : 5 : 6 :
L and it is not possible to choose a global parameter suitable for all the latent feature components . Moreover , Eqn . ( 6 ) does not include the fused lasso penalty , which is necessary for the CNV problem .
Now we examine the following group lasso problem similar to Eqn . ( 6 ) ,
1 2
Y − XW2 pl Xgl,k2 , l=1 min X•,k where pl = |gl| is the weight of the lth group . Note
F + γ that only X•,k are variables in this problem and all other columns of X are fixed . For this non overlapping group lasso problem , there exists a γ(k ) for each group gl such that when γ ≥ γ(k ) , the optimal solution for Xgl,k is zero ; l when γ < γ(k ) , the optimal solution for Xgl,k is nonzero [ 12 ] and actually , l l flflflv(k ) flflfl |gl|Wk,•2 gl
γ(k ) l = l where v(k ) gl exactly the γ(k ) is defined the same as in Eqn . ( 2 ) . Thus , it is that we used to compute {blk} in Eqn(3 ) In summary , instead of using group lasso to get sparse X directly , SGS FL first applies the group lasso setting with the parameter r to adaptively determine {blk} . Then , blks are used to compute whether Xgl,k should be zero or nonzero in the optimization . Thus , r controls the sparsity of X through {blk} . Empirically , using r for adaptive sparse group selection instead of solving a group lasso problem directly for a fixed global parameter γ is more reliable and stable for CNV data analysis . for k = 1 , . . . , K do
√
|gl|Wk,•2 v(k ) ← ( Y − X•,=kW=k,•)W k,• for l = 1 , . . . , L do l ← v(k ) gl γ(k ) end for for l = 1 , . . . , L do
 0 blkXgl,k = 0 , Xik ≥ 0 andm end for X•,k ← argminX•,k s∈{l|γ(k ) otherwise . l >γ(k ) blk =
1
1 if l = argmaxs γ(k ) or l } γ(k ) s
2 Y − XW2 i=1 X 2
F subject to ik = 1 s /L s=1 γ(k ) s < r end for until X does not change repeat for k = 1 , . . . , K do
Wk,• ← argminWk,• λ1|Wk,•| + λ2 n j=2 |Wkj − Wk,j−1|
2 Y − XW2
1
F +
7 :
8 : 9 :
10 :
11 : 12 :
13 : 14 : 15 : 16 : 17 : end for
18 : 19 : 20 : until X and W do not change until W does not change
Figure 2 . SGS FL algorithm
IV . SIMULATION flflflH ( b)flflfl1,q
B b=1
F . Relation to Group Lasso
[ 11 ] proposed a group sparsity regularization method to introduce the group structured prior knowledge for nonnegative matrix factorization . The objective function of their approach is min
W≥0,H≥0
1 2
Y − W H2
F + αW2
F + β
( 6 ) where Y is the original data matrix and the coefficient matrix H is divided into B groups as {H ( 1),··· , H ( B)} by prior knowledge . The motivation of the regularization term on {H ( b)} is that samples in the same group are expected to share the same sparsity patterns in their latent factor representation . Eqn . ( 6 ) uses a global parameter β on group lasso to enforce the group sparsity . However , it does not fit in the problem of CNV detection since the magnitude of latent features ( log ratio intensities ) can be in very different scales ,
In the simulation , we compare SGS FL with FLLat on the performance of learning the latent components and the coefficients from an artificial CNV dataset . We also tested the effect of the group sparsity parameter r and evaluated the scalability and the convergency characteristics of SGS FL .
A . Data Generation
We first generated simulated latent CNV components W and coefficients X with a sparse group structure , and then constructed a CNV dataset Y = XW + Ξ , where Ξ are IID gaussian noises , as illustrated in Fig 3 . The simulated arrayCGH dataset contains 150 samples with 300 probes . There are 5 latent components , each of which contains 4 independent events of copy number gain or loss , shown in Fig 3(A ) . The 150 samples are equally divided into 3 groups with 50 samples in each group and the corresponding relation between the 3 groups and the 5 components is shown in Fig 3(B ) . The group prior is shown in Fig 3(C ) . Errors are introduced in the prior information as a certain percentage of misplaced samples in each group . As shown
( A ) Simulated W
( B ) Simulated X
( C ) Group information
( D ) Simulated Y
Figure 3 . Simulation data . ( A ) Each latent CNV component contains four randomly generated copy number gains/losses . ( B ) The samples are divided into 3 groups of equal size . The coefficients are nonzero only between a group and its corresponding components . ( C ) Errors are introduced into the prior groups , ie a certain percentage of samples are misplaced into the wrong group . ( D ) The noisy simulated CNV dataset . There is no observable pattern although the data is constructed from the sample groups and the latent CNV components . in Fig 3(C ) , each prior group contains samples from all the three true groups although majority of the samples are from only one group . Given the W and X , the dataset Y = XW + Ξ is shown in Fig 3(D ) . Y is a very noisy dataset . k means clustering with k = 3 on the dataset results in error rate above 50 % . The objective is to recover W and X from the noisy data Y with SGS FL and FLLat .
B . Performance of Recovering W and X
Bayesian Information Criterion ( BIC ) [ 8 ] is used to determine the hyper parameters λ1 and λ2 for applying SGSFL and FLLat . The group sparsity parameter r is set to 05 Fig 4(A ) shows the learned hidden components X . Clearly , the X learned by SGS FL preserves the group structure and is more similar to the original X , compared with the X learned by FLLat . Guided by the prior group information , for each component the coefficients are learned only for the samples in the selected groups . The coefficients in the unselected groups are all zero . For example , for first latent component , only group 1 and 3 are chosen . Fig 4(B ) shows the learned latent components . SGS FL successfully recovered the 5 latent components with a lowest correlation 0.70 with the original component , while FLLat detected two wrong latent components that are completely different . In the FLLat method , the mistakes in the components can be matched with the wrong weights in X : column 4 and column 5 in the X in Fig 4(A ) do not capture any group relations and thus , the corresponding components are derived to support the wrong samples . On the contrary , the X learned
( A )
( B )
Figure 4 . Performance of recovering the latent components and the weight coefficients with SGS FL and FLLat . ( A ) The learned coefficient matrices . ( B ) The learned latent components . The red numbers above each component is the Pearson correlation coefficient between the component and its corresponding original latent component . by SGS FL preserves the group structures and the correct samples are used to derive the components . Note that since we introduced noise in Y and errors in the prior group , the X by SGS FL is not perfectly sparse in the coefficients of the samples in the unselected groups .
C . Controlling Group Sparsity by Parameter r
Selecting appropriate group sparsity with r is important for the performance of SGS FL . We tested SGS FL with different r ∈ [ 0 , 1 ] with step size 0.05 and plot the accuracy of the learned X and W by calculating the Pearson correlation with the original ones in Fig 5 . As expected that the group selection changes by steps ( Eqn . 3 ) and the performance of SGS FL only changes when group selection changes . Thus , SGS FL performs the same in a certain range of r until reaching an increase or a decrease in the number of selected groups . It is interesting that in the range r ∈ [ 0.45 , 0.65 ] , X and W are most accurately recovered , and when r is
050100150200250300−101050100150200250300−101050100150200250300−101050100150200250300−101050100150200250300−1010255075100125150123sampleprior group0100200300−100100946330100200300−100100868140100200300−100100841920100200300−100100862550100200300−10010070020100200300−100100760230100200300−100100774030100200300−1001000630190100200300−100100700460100200300−100100082947SGS−FLFLLat ( A )
( B )
Figure 6 . Running time and convergency of SGS FL . ( A ) Running time under different m ( # of samples ) and n ( # of probes ) . ( B ) Convergency of X and W .
Figure 7 . The coefficient matrices learned from the breast cancer data by FLLat and SGS FL . The sample profile missing the tumor grade information is put in the first row and the other samples are ordered by tumor grade 1 3 from top to bottom . The groups are separated by red horizontal lines . The K columns of X are sorted in descending order by the magnitude of the corresponding latent features ( ie Wi,•2 ) . define three groups of samples . There is one additional sample missing the clinical information of tumor grade which was also included in the study . Note that SGS FL allows additional samples that are not assigned to any group . The number of latent feature K was chosen as the number of principle components that explain at least 80 % variation of the data . The hyper parameters λ1 and λ2 were chosen by Bayesian Information Criterion ( BIC ) suggested by [ 8 ] . The r parameter for SGS FL was set to 0.5 to learn a coefficient matrix with moderate sparsity .
Figure 5 . Accuracy of the learned X and W at different group sparsities . Different group sparsity parameter r is tested . The x axis is the Pearson correlation between the learned X and the original X , and the y axis is the correlation between the learned W and the original W . Each star represents the accuracy of X and W , labeled by the corresponding r parameter . too small or too large , the correlations are lower . This is consistent with our hypothesis : a small r leads to insufficient group selection for the components and a large r may lead to unnecessary group selection and thus an overly dense X that overfits Y . It is clear that in a reasonable range of sparsity , SGS FL performs well and SGS FL also performs better than FLLat in most of the choices of r .
D . Scalability and Convergence
In real arrayCGH datasets , the number of probes can be as many as several millions . In Fig 6 , we analyze the running time and the convergency of SGS FL on simulated datasets of different sample sizes and different numbers of probes . In the left plot of Fig 6(A ) , we fixed the number of probes to be 300 and vary the sample size ; in the right plot , we fixed the sample size to be 150 and vary the number of probes . In both cases , SGS FL scales linearly with the log of the sizes . In Fig 6(B ) , SGS FL clearly converges within tens of iterations . The results suggest a good scalability to large datasets by SGS FL .
V . EXPERIMENTS ON BREAST CANCER DATA
To directly compare SGS FL with FLLat , we followed the experiment setup in [ 8 ] . SGS FL and FLLat were applied to chromosome 8 and 17 of a breast cancer arrayCGH data from [ 13 ] to identify CNV regions for cancer relevance .
A . Breast Cancer Data
The breast cancer data contains profiles of 44 predominantly advanced primary breast tumors with 241 mapped human genes from chromosome 8 and 382 mapped human genes from chromosome 17 . Among the 44 profiles , 5 are in tumor grade 1 , 21 in grade 2 and 17 in grade 3 . This prior clinical information was used in our model to
0203040506070404505055060650707508X similarityW similarity[0−035]04[045−065]070075[08−10 ] SGS−FLFLLat102103104105100101102103size of samplesrunning time(s)running time under different m101102103104105100101102103104size of probesrunning time(s)running time under different n05101520253035005115225convergency of Xiteration #norm of difference of X05101520253035020406080100120convergency of Witeration #norm of difference of W CLASSIFICATION ACCURACIES IN LEAVE ONE OUT CROSS VALIDATION
WITH BEST RESULTS FOR EACH METHOD BOLD .
Table I chromosome 8 PCA FLLat SGS FL chromosome 17 PCA FLLat SGS FL Tumor Grade k=1 k=3 k=5 k=7
0.727 0.659 0.705
0.750 0.591 0.773 0.727
0.795 0.659 0.795
0.795 0.682 0.750 0.727
0.795 0.614 0.750
0.818 0.659 0.705 0.727
0.682 0.636 0.773
0.750 0.750 0.750 0.727 it is reasonable to state between tumor grade 1 and 3 , assume that some samples with tumor grade 2 are more similar to samples with tumor grade 1 and some are more similar to samples with tumor grade 3 . The results can be easily explained by the coefficient matrix in Fig 7 . The components can only be shared by samples in group 1 and group 2 , or by samples in group 2 and group 3 , and never shared by samples in group 1 and group 3 . This result strongly support the hypothesis that CNVs correlate with the tumor grade . The clustering result generated by FLLat in Fig 8 and [ 8 ] also showed there are three distinct groups of samples . However , it is not clear why these samples were clustered together since their tumor grades are different . C . Sample Classification
To check whether the coefficient matrix X is also consistent with other clinical information , we also designed a binary classification problem of separating samples into two groups with another clinical variable ‘Tumor size’ ( T1&T2 vs . T3&T4 ) . Tumor grades were used as prior group information by SGS FL and the ‘Tumor size’ variable is the target variable for classification . We run a leave one out cross validation with k nearest neighbor ( KNN ) classifier on the coefficient matrices learned by PCA , FLLat and SGS FL from chromosome 8 and 17 . The number of latent features are fixed to be the number of principle components that explain 80 % variance for all the three methods . The classification accuracies by the three methods with different KNN parameters are reported in Tab . I . It is not surprising that PCA achieved the best performance since PCA preserves the most variance of the data without constraints on obtaining interpretable coefficients . Nevertheless , in the table the result by SGS FL is comparable to PCA and much better than the result by FLLat . It suggests that by using relevant prior information , SGS FL can obtain both interpretable CNV components and informative coefficients for classification . It is worth noting that if only the tumor grade information is used by KNN in the leave one out cross validation , the accuracy is 0.727 for any choice of k for the KNN classifier . This result further implies that the better classification performance of SGS FL is not solely due to the relevant prior information in tumor grade .
Figure 8 . Hierarchical clustering results of breast cancer samples . The samples are labeled by their tumor grade , G1 , G2 or G3 .
B . Analysis of the Coefficient Matrices
We compare the coefficient matrices learned from FLLat and SGS FL for chromosome 17 in Fig 7 . The samples are ordered by tumor grade and the three groups are separated by red horizontal lines . In the coefficient matrix learned by FLLat , there is hardly any group structure of samples and the relation between the samples and the latent features seems arbitrary . In other words , there is no subset of samples with similar tumor grade by which a latent feature is shared . The coefficients learned by SGS FL show clear group structures . When the coefficients are all zeros in one group , it implies that the CNVs identified from the corresponding latent feature are not associated with that group . For example , the first two latent features are only shared by the groups of tumor grade 2 and 3 ; the third latent feature is only shared by the group of tumor grade 3 and the last latent feature is only shared by the group of tumor grade 1 . The submatrix of the last group is denser than those of the first and the second groups , which implies that the samples with tumor grade 3 are sharing more CNVs than the samples in the other two groups .
We also performed hierarchical clustering on the two coefficient matrices . Cosine similarity was used as the similarity metric in the clustering to obtain similar clustering results reported in [ 8 ] . The clustering results are shown in Fig 8 . Since the tumor grade information is incorporated in SGS FL , the generated hierarchical structures are more biologically meaningful . For example , there are three large clusters : the first cluster contains samples with tumor grade 1 and 2 , the second cluster contains samples with tumor grade 2 and 3 , and the third cluster contains samples with tumor grade 3 except the additional sample missing the tumor grade information . Since tumor grade 2 is an intermediate
02040608NORWAY 26 G2NORWAY 101 G2NORWAY 57 G3NORWAY 47 G?STANFORD A G3NORWAY 53 G2NORWAY 61 G3STANFORD 2 G3NORWAY 17 G1NORWAY 18 G2NORWAY 20 G1STANFORD 31 G2STANFORD 38 G2STANFORD 40 G2STANFORD 34 G1NORWAY 65 G3NORWAY 41 G3NORWAY 16 G2NORWAY 27 G1STANFORD 39 G2NORWAY 48 G3NORWAY 112 G1STANFORD 4 G2NORWAY 10 G2NORWAY 11 G2STANFORD 21 G2NORWAY 104 G3STANFORD 24 G2STANFORD 17 G2NORWAY 14 G2NORWAY 12 G2NORWAY 109 G3NORWAY 56 G2NORWAY 19 G3NORWAY 15 G3STANFORD 16 G3STANFORD 23 G3NORWAY 7 G2STANFORD 14 G2NORWAY 39 G2NORWAY 100 G3STANFORD 35 G3NORWAY 102 G3NORWAY 111 G3Clustering by FLLat002040608STANFORD 17 G2STANFORD 21 G2STANFORD 31 G2STANFORD 24 G2NORWAY 10 G2NORWAY 11 G2NORWAY 7 G2NORWAY 112 G1STANFORD 34 G1NORWAY 12 G2NORWAY 16 G2NORWAY 14 G2STANFORD 4 G2STANFORD 40 G2NORWAY 18 G2STANFORD 14 G2NORWAY 17 G1NORWAY 20 G1NORWAY 27 G1NORWAY 26 G2NORWAY 101 G2NORWAY 53 G2NORWAY 39 G2STANFORD 39 G2NORWAY 100 G3STANFORD 35 G3STANFORD 38 G2NORWAY 56 G2NORWAY 15 G3STANFORD 16 G3NORWAY 19 G3NORWAY 48 G3STANFORD 23 G3NORWAY 104 G3NORWAY 41 G3NORWAY 65 G3NORWAY 102 G3NORWAY 111 G3NORWAY 47 G?NORWAY 61 G3NORWAY 57 G3NORWAY 109 G3STANFORD A G3STANFORD 2 G3Clustering by SGS−FL RANKING OF THE 33 KNOWN AMPLIFIED GENES IN THE BLADDER
CANCER DATA . THE BEST RANK OF EACH GENE IS BOLD .
Table II
Chromosome
2
6
8
Gene CPSF3 ADAM17 YWHAQ TAF1B UNQ5830
KLF11 RRM2 CAP2
FAM8A1 NUP153 KIF13A NHLRC1
AOF1 DEK
IBRDC2
ID4
OACT1 E2F3
CDKAL1
SOX4 PRL
COX6C POLR2K SPAG1 RNF19
MGC39715
NCALD RRM2B ODF1 KLF10 FLJ45248 ATP6V1C1
BAALC
Na¨ıve
3 3 3 8 13 1 1 53 28 28 28 79 117 117 117 66 23 10 3 20 15 65 69 69 69 76 6 52 138 185 162 46 68
FLLat
1 1 1 4 5 5 8 59 61 61 62 66 67 67 67 31 13 1 3 16 22 49 51 51 51 53 7 10 63 64 65 58 60
SGS FL
1 1 1 4 5 5 8 52 53 53 54 58 59 59 59 30 14 1 3 16 22 45 47 47 47 49 37 39 71 76 77 35 41
VI . EXPERIMENTS ON BLADDER CANCER DATA
We also applied SGS FL and FLLat to test a bladder cancer arrayCGH data from [ 9 ] to identify CNVs relevant to bladder cancer . This dataset contains 38 urothelial carcinomas with whole genome tiling resolution array CGH and high density expression profiling . We still used tumor grade as the prior information to separate samples into 3 groups {G1,G2,G3} and set r = 0.5 for SGS FL . The parameters k , λ1 and λ2 were selected in the same way as in the previous experiment . [ 9 ] reported genomic amplifications of 47 genes at regions 2p25 , 6p22 and 8q22 in “ Additional file 4 ” , so we focused our study on these chromosomes . There are 1938 probes on chromosome 2 ; 1801 probes on chromosome 6 ; and 1091 probes on chromosome 8 . 33 of the 47 genes are annotated in the dataset . Both FLLat and SGS FL identified the 33 known amplified genes and ranked them in the top 100 probes . We also compared the methods with the na¨ıve approach which ranks the genes simply based on the sum i |Yij| ) . The 33 genes and their corresponding ranks are listed in Tab . II . Compared with FLLat , SGS FL ranked 16 genes better and 6 genes worse . The result suggests that the prior information in tumor grade helps rank the cancer relevant CNVs higher . of their magnitude in the original data ( ie
Figure 9 . Top ranked 50 genes by FLLat and SGS FL on chromosome 17 . Green denotes ‘gain’ status and red denotes ‘loss’ status . Genes with most different ranks by FLLat and SGS FL are label by their gene symbols .
D . Analysis of CNV Components their magnitude in all latent features ( ie
We next compared the latent features learned by FLLat and SGS FL . We ranked the identified probes by the sum of k |Wkj| ) . The probes without gene names were excluded in this analysis . We took the top 50 genes ranked by FLLat and SGS FL , and plot their ranks in Fig 9 . FLLat and SGS FL have consensus on the ranks of many of the genes . We focus on the genes which have a difference larger than 3 in the ranks by the two methods . There are several interesting examples . NGFR was demonstrated as a marker to identify myoepithelial cells in preinvasive lesions and myoepithelial differentiation in breast carcinomas [ 14 ] . SPOP can mediate the Breast cancer metastasis suppressor 1 ( BRMS1 ) and is important for breast cancer progression [ 15 ] . PIP5K2B ( PIP4K2B ) is a known amplified gene in breast cancer [ 16 ] . DLX4 ( BP1 ) negatively regulates BRCA1 in sporadic breast cancer [ 17 ] . NR1D1 is a survival factor for breast cancer [ 18 ] . ITGB4 is a prognostic marker for breast cancer [ 19 ] . All the genes ranked better by SGS FL seem to be relevant to breast cancer . However , for the genes ranked higher by FLLat such as ZNF207 , PCTP and SCYA3L1 , there is no literature suggesting associations between the genes and breast cancer . Possibly , these genes might be involved in some frequent CNVs instead of CNVs specific to breast cancer .
Finally , we also compared the ranking of the known cancer genes in Cancer Gene Census1 on chromosome 8 and 17 in Fig 10 . Overall , most of the known cancer genes were ranked better by SGS FL . The result implies that the identified CNVs by SGS FL are more likely to be associated with breast cancer .
1http://wwwsangeracuk/genetics/CGP/Census/
0510152025303540455005101520253035404550SUPT6HZNF207SCYA3L1SCYA3PIP5K2BTHRANR1D1IFI35NGFRSPOPDLX4PCTPITGB4Top 50 Genes on Chromosome 17Rank by FLLatRank by SGS−FL Cancer Genes on Chromosome 8
Cancer Genes on Chromosome 17
Figure 10 . Ranking of known cancer genes on chromosome 8 and 17 . Green denotes ‘gain’ status and red denotes ‘loss’ status . On average , SGS FL ranked the cancer genes on chromosome 8 and 17 better than FLLat .
Compared with the na¨ıve method , SGS FL ranked 25 genes better and 7 genes worse . The result suggests that the learned latent features is more reliable than the original data for identifying cancer relevant CNVs .
VII . CONCLUSIONS
In general , discovering CNVs across multiple samples is more accurate than single sample analysis . To analyze multiple samples of probe series , it is important to consider both the similarity and the heterogeneity among the samples . Existing methods such as FLLat ignore the fact that patient samples with different phenotypes show different frequencies and patterns of CNVs in their genotyping . These methods tend to miss the CNVs specific to subsets of samples . To the best of our knowledge , SGS FL is the first model that considers the prior information on sample groups in CNV identification . SGS FL constructs a latent feature model to identify CNVs and learn the sample groups sharing the CNVs simultaneously by integrating fused lasso to smooth CNV patterns and adaptive sparse group selection to identify the group specificity of the CNVs . The simulations and experiments on real cancer arrayCGH datasets suggest that with the relevant sample group information , SGS FL can more accurately identify cancer relevant CNV regions and a more informative representation of CNV data as coefficients on the CNV components .
ACKNOWLEDGMENT
This work is supported by NSF grant #IIS1117153 . The authors gratefully thank Christina Leslie and Jieping Ye for helpful discussions .
REFERENCES
[ 1 ] L . Feuk , A . Carson , and S . Scherer , “ Structural variation in the human genome , ” Nature Reviews Genetics , vol . 7 , no . 2 , pp . 85–97 , FEB 2006 .
[ 2 ] R . Redon et al . , “ Global variation in copy number in the human genome , ” Nature , vol . 444 , no . 7118 , pp . 444–454 , NOV 23 2006 .
[ 3 ] N . P . Carter , “ Methods and strategies for analyzing copy number variation using DNA microarrays , ” Nature Genetics , vol . 39 , no . 7 , pp . S16–S21 , JUL 2007 .
[ 4 ] N . H . Sykes et al . , “ Copy number variation and association analysis of SHANK3 as a candidate gene for autism in the IMGSAC collection , ” European Journal Of Human Genetics , vol . 17 , no . 10 , pp . 1347–1353 , OCT 2009 .
[ 5 ] R . Tibshirani and P . Wang , “ Spatial smoothing and hot spot detection for CGH data using the fused lasso , ” Biostatistics , vol . 9 , no . 1 , pp . 18–29 , JAN 2008 .
[ 6 ] J P Vert and K . Bleakley , “ Fast detection of multiple changepoints shared by many signals using group lars , ” in Advances in Neural Information Processing Systems 23 , J . Lafferty , C . K . I . Williams , J . Shawe Taylor , R . Zemel , and A . Culotta , Eds . , 2010 , pp . 2343–2351 .
[ 7 ] K . Bleakley and J P Vert , “ The group fused lasso for multiple change point detection , ” 2011 .
[ 8 ] G . Nowak , T . Hastie , J . R . Pollack , and R . Tibshirani , “ A fused lasso latent feature model for analyzing multi sample aCGH data , ” Biostatistics , vol . 12 , no . 4 , pp . 776–791 , OCT 2011 .
[ 9 ] M . Heidenblad et al . , “ Tiling resolution array CGH and high density expression profiling of urothelial carcinomas delineate genomic amplicons and candidate target genes specific for advanced tumors , ” BMC Medical Genomics , vol . 1 , JAN 31 2008 .
020406080100120140020406080100120140EXT1FGFR1MYCNCOA2NDRG1PCM1WRNRank by FLLatRank by SGS−FL020406080100120140160180200020406080100120140160180200BRCA1CD79BCLTCCOL1A1DDX5ERBB2HLFMAP2K4MLLT6PRKAR1ARAB5EPTP53Rank by FLLatRank by SGS−FL [ 17 ] B . J . Kluk et al . , “ BP1 , an Isoform of DLX4 Homeoprotein , Negatively Regulates BRCA1 in Sporadic Breast Cancer , ” International Journal of Biological Sciences , vol . 6 , no . 5 , pp . 513–524 , 2010 .
[ 18 ] A . Kourtidis et al . , “ An RNA Interference Screen Identifies Metabolic Regulators NR1D1 and PBP as Novel Survival Factors for Breast Cancer Cells with the ERBB2 Signature , ” Cancer Research , vol . 70 , no . 5 , pp . 1783–1792 , MAR 1 2010 .
[ 19 ] A . Brendle et al . , “ Polymorphisms in predicted microRNAbinding sites in integrin genes and breast cancer : ITGB4 as prognostic marker , ” Carcinogenesis , vol . 29 , no . 7 , pp . 1394– 1399 , JUL 2008 .
[ 10 ] J . Liu , S . Ji , and J . Ye , SLEP : Sparse Learning with Efficient Projections , Arizona State University , 2009 . [ Online ] . Available : http://wwwpublicasuedu/∼jye02/Software/SLEP
[ 11 ] J . Kim , R . Monteiro , and H . Park , “ Group sparsity in nonnegative matrix factorization , ” in Proceedings of the 12th SIAM International Conference on Data Mining , 2012 , p . 851 .
[ 12 ] F . Bach , R . Jenatton , J . Mairal , and G . Obozinski , “ Convex optimization with sparsity inducing norms . ”
[ 13 ] J . Pollack et al . , “ Microarray analysis reveals a major direct role of DNA copy number alteration in the transcriptional program of human breast tumors , ” Proceedings of the National Academy of Sciences of the United States of America , vol . 99 , no . 20 , pp . 12 963–12 968 , OCT 1 2002 .
[ 14 ] J . Reis et al . , “ Distribution and significance of nerve growth factor receptor ( NGFR/p75(NTR ) ) in normal , benign and malignant breast tissue , ” Modern Pathology , vol . 19 , no . 2 , pp . 307–319 , FEB 2006 .
[ 15 ] B . Kim et al . , “ Breast cancer metastasis suppressor 1 ( BRMS1 ) is destabilized by the Cul3 SPOP E3 ubiquitin ligase complex , ” Biochemical And Biophysical Research Communications , vol . 415 , no . 4 , pp . 720–726 , DEC 2 2011 .
[ 16 ] S . Luoh , N . Venkatesan , and R . Tripathi , “ Overexpression of the amplified Pip4k2 beta gene from 17q11 12 in breast cancer cells confers proliferation advantage , ” Oncogene , vol . 23 , no . 7 , pp . 1354–1363 , FEB 19 2004 .
