2012 IEEE 12th International Conference on Data Mining 2012 IEEE 12th International Conference on Data Mining
Fast Kernel Sparse Representation Approaches for Classification
School of Computer Science , University of Windsor , Windsor , Ontario , Canada
Yifeng Li and Alioune Ngom {li11112c , angom}@uwindsor.ca
Abstract—Sparse representation involves two relevant procedures sparse coding and dictionary learning . Learning a dictionary from data provides a concise knowledge representation . Learning a dictionary in a higher feature space might allow a better representation of a signal . However , it is usually computationally expensive to learn a dictionary if the numbers of training data and(or ) dimensions are very large using existing algorithms . In this paper , we propose a kernel dictionary learning framework for three models . We reveal that the optimization has dimension free and parallel properties . We devise fast active set algorithms for this framework . We investigated their performance on classification . Experimental results show that our kernel sparse representation approaches can obtain better accuracy than their linear counterparts . Furthermore , our active set algorithms are faster than the existing interior point and proximal algorithms .
Keywords kernel sparse representation ; dictionary learning ; sparse coding ; 𝑙1 regularization ; non negative least squares
I . INTRODUCTION
Sparse representation ( SR ) involves sparse coding and dictionary learning [ 1 ] . The definition of sparse coding ( SC ) is that a data point 𝒃 is represented by a sparse linear combination of fixed dictionary atoms 𝑨 . Mathematically , a 𝑙1 regularized SC can be min
𝒙
1 2
∥𝒃 − 𝑨𝒙∥2
2 + 𝜆∥𝒙∥1 ,
( 1 ) where 𝑨 is called dictionary . Each column of 𝑨 is called an atom . 𝒙 is the sparse coefficient vector . 𝜆 is a parameter to control the tradeoff between reconstruction error and sparsity . Fast algorithms , such as the interior point method by [ 2 ] and the proximal method by [ 3 ] , have been proposed for the 𝑙1 regularized SC . Using 𝒙 , we can interpret the signal 𝒃 differently in various applications . Learning a dictionary from training data is coined dictionary learning ( DL ) . If the number of atoms is greater than the dimension of the dictionary , then the dictionary is overcomplete . If it is less , then the dictionary is undercomplete .
SR has been applied in machine learning for face recognition [ 4 ] . This is called SR classification ( SRC ) approach . Sound performance has been reported . However , this approach pools all training samples as a redundant dictionary . Therefore it is not efficient for large data . In fact , this is a lazy learning approach . That is there is no training step , all training samples are directly used in the prediction step . Thus it is not applicable in real time applications . Moreover , the number of dimensions does practically affect the speed of optimization in many implementations , for example , [ 2 ] . If we map the training samples into a higher dimensional space by a proper mapping function , and learn the dictionary in this feature space , we expect a better linear representation of a unknown sample . The linearity corresponds to the nonlinearity in the original space . As far as we know in this direction , there are two kernel SR ( KSR ) approaches . One was proposed in [ 5 ] and [ 6 ] . The approach maps all training samples into a high feature space and then reduces the dimensionality . DL is not considered by this approach . Another approach was proposed in [ 7 ] . This approach is not efficient because the dictionary is learned over the training sample iteratively . In each iteration , updating the sparse coefficients for a training sample is a SC problem . Therefore many SC problems have to be solved separably . Moreover , the issue encountered in [ 7 ] is that the dictionary learned in the feature space is difficult to be represented as the feature space is intractable . The KSR based classification approaches are basically kernel feature extraction approaches . The sparse coefficient vector of each sample is actually the representation in the feature space corresponding to the original sample . Generally speaking , any classifier can be employed to learn over the training representations and predict the class labels of new samples .
In this paper , we propose a generic kernel framework of DL . We reveal that the optimization of DL is dimensionfree and parallel . We propose fast active set optimization algorithms . We focus on their application for classification . The rest of this paper is organized as below . In section II , we present our framework for linear SR models . In Section III , active set algorithms are proposed . The kernel extension is described in Section IV . After that , experimental results are compared . Finally , we draw our conclusion .
II . REVISED LINEAR SPARSE REPRESENTATION
In this section , in order to pave the road to kernel SR , we present our modified linear SR models and the generic optimization framework for them . We shall show , in Section IV , that they can be nicely extended to kernel versions . We first give our 𝑙1 regularized linear SR model and then propose the optimization framework for it . After that the non negative model and 𝑙1 regularized non negative model are given . The unique features of our framework are that i ) the dictionary atoms are constrained to have unit norm ; and
1550 4786/12 $26.00 © 2012 IEEE 1550 4786/12 $26.00 © 2012 IEEE DOI 101109/ICDM2012133 DOI 101109/ICDM2012133
733 966 ii ) the inner product of dictionary , rather than the dictionary itself , is directly updated .
A . 𝑙1 Regularized Model and Optimization Framework
The problem of 𝑙1 regularized least squares ( 𝑙1LS ) in Equation 1 is convex but non smooth . Global minima can be found due to its convexity . In SC , the dictionary 𝑨 is fixed . Let ’s take classification as an example . Suppose 𝑨 is a pool of training samples with class labels . A new sample can be regressed by a sparse linear combination of training samples . The training samples with nonzeros coefficients provide us an interpretation to predict the unknown sample [ 4 ] . However , a loosely collected and fairly redundant dictionary 𝑨 is computationally inefficient . Therefore , we might need to learn a concise dictionary from training data if we have no prior knowledge , which is the DL task described below . Suppose 𝑫𝑚×𝑛 is the training data with samples in columns , a dictionary 𝑨𝑚×𝑘 is to be learned to let each training sample be a sparse linear combination of its columns , that is 𝒅𝑖 ≈ 𝑨𝒚𝑖 where 𝒚𝑖 is the sparse coefficient vector of sample 𝒅𝑖 . 𝑨 is also called basis matrix , and 𝒀 is called coefficient matrix . There are generally two 𝑙1 regularized SR models mentioned in literature : min 𝑨,𝒀
1 2
∥𝑫 − 𝑨𝒀 ∥2
𝐹 + 𝜆∥𝒀 ∥1 st diag(𝑨T𝑨 ) ≤ 1 ,
( 2 ) where diag(𝑴 ) is defined as a column vector accommodating the diagonal elements of square matrix 𝑴 , and 2 trace(𝑨T𝑨 ) + 𝜆∥𝒀 ∥1 .
∥𝑫 − 𝑨𝒀 ∥2 min 𝑨,𝒀
𝐹 +
( 3 )
1 2
𝛼
The constraint in Equation 2 and second term in Equation 3 can suppress the norm of dictionary atoms . The purpose of this is to avoid the invariance of the reconstructive term .
Our model is min 𝑨,𝒀
1 2
∥𝑫 − 𝑨𝒀 ∥2
𝐹 + 𝜆∥𝒀 ∥1 st diag(𝑨T𝑨 ) = 1
( 4 ) where diag(𝑨T𝑨 ) = 1 means the dictionary atoms need to have unit 𝑙2 norm . The difference between this model with Equations 2 and 3 is that we restrict the dictionary atoms to the surface of unit ball . The advantages are that i ) the constraint is easier to satisfy given the analytical solution of the objective ( compared with Equation 2 ) ; and ii ) it involves less parameters ( compared with Equation 3 ) and therefore makes model selection easier . However , as Equations 2 and 3 , this model is not convex either . We often use alternating updating algorithms which usually converge to local minima [ 8 ] . The general algorithmic procedure is in the following . In one step , 𝒀 is updated while fixing 𝑨 as expressed by
1 2 min 𝒀
∥𝑫 − 𝑨𝒀 ∥2
𝐹 + 𝜆∥𝒀 ∥1 .
( 5 )
This is a batch of 𝑛 single 𝑙1LS problems in Equation 1 . We call this multiple 𝑙1LS problem . In the next step , 𝒀 is fixed , and the unnormalized 𝑨 is updated by solving :
∥𝑫 − 𝑨𝒀 ∥2 𝐹 .
1 2 min 𝑨
( 6 ) It is well known that the analytical solution is 𝑨 = 𝑫𝒀 † , where 𝒀 † is Moore Penrose pseudoinverse . After that , 𝑨 is normalized to have columns of unit 𝑙2 norm . This is to fulfill diag(𝑨T𝑨 ) = 1 . The difference between this and others , for example the dictionary updating step in [ 3 ] , is that i ) analytical solution can be obtained by ours , while an iterative block coordinate descent is done over each atom in [ 3 ] ; and ii ) we only update the inner product , while it is not so in [ 3 ] . Therefore this framework has the advantages of fast computation and ease of kernelization . The alternating updating is done iteratively until termination criteria are met . One of the issues in traditional SR models is that the dictionary atoms are updated explicitly and separately . The explicit update makes it challenging to extend to kernel version as this update is intractable in higher ( even infinite ) feature space . And the separate update is computationally expensive . Therefore , our idea is to treat the dictionary as a whole 2 way variable and only to update its inner product . Note that solving Equation 5 only needs the inner product 𝑯 = 𝑨T𝑨 , since it is essentially quadratic programming ( QP ) ( see Section III ) , and some applications only directly make use of coefficient matrix while do not analyze the dictionary directly . Therefore , we only need to learn 𝑯 instead of 𝑨 . Indeed , we have 𝑯 = 𝒀 †T𝑫T𝑫𝒀 † . Therefore , we can , fortunately , find that updating 𝑯 only needs the inner product 𝑲 = 𝑫T𝑫 and 𝒀 . This is very useful when kernelizing the model . The computational advantage of updating the inner product of basis matrix over updating the basis matrix directly is that the former is dimension free . The aforementioned constraint diag(𝑨T𝑨 ) = 1 can be √ realized by the normalization of 𝑯 , which is straightfor√ diag(𝑯)diag(𝑯)T , where ./ ward . We have 𝑯 = 𝑯./ 𝑴 takes the element wise denotes element wise division , square root of matrix 𝑴 . According to the above derivation , we have the framework of solving the 𝑙1 regularized DL problem as illustrated in Algorithm 1 .
For 𝑝 new samples 𝑩𝑚×𝑝 , they can be coded by a sparse linear combination of the dictionary atoms . This task is min 𝑿
1 2
∥𝑩 − 𝑨𝑿∥2
𝐹 + 𝜆∥𝑿∥1 .
( 7 )
Obviously , the SC task includes 𝑝 single 𝑙1LS problems . The multiple 𝑙1LS and single 𝑙1LS problems are in fact QP problems . We will show their optimization in Section III .
B . Non Negative Model
Non negativity can also result in sparsity . A typical example is linear programming . Also , in machine learning , the normal vector of hyperplane in support vector machine
734967
Algorithm 1 𝑙1 Regularized Dictionary Learning 2∥𝑫−𝑨𝒀 ∥2
1
𝐹 +𝜆∥𝒀 ∥1
Task : solve 𝑓 ( 𝑨 , 𝒀 ) = min𝑨,𝒀 st diag(𝑨T𝑨 ) = 1 Input : 𝑲 = 𝑫T𝑫 , dictionary size 𝑘 , 𝜆 Output : 𝑯 = 𝑨T𝑨 , 𝒀 initialize 𝒀 and 𝑯 = 𝑨T𝑨 randomly ; 𝑟𝑝𝑟𝑒𝑣 = 𝐼𝑛𝑓 ; for 𝑖 = 1 : 𝑚𝑎𝑥𝐼𝑡𝑒𝑟 do update 𝒀 by solving the 𝑙1LS problem in Equation 5 ; √ update 𝑯 = 𝒀 †T𝑫T𝑫𝒀 † ; normalize 𝑯 by 𝑯 = 𝑯./ if 𝑖 == 𝑚𝑎𝑥𝐼𝑡𝑒𝑟 or 𝑖 mod 𝑙 = 0 then
𝑑𝑖𝑎𝑔(𝑯)𝑑𝑖𝑎𝑔(𝑯)T ;
% check termination conditions every 𝑙 iterations 𝑟𝑐𝑢𝑟 = 𝑓 ( 𝑨 , 𝒀 ) ; % current objective value if 𝑟𝑝𝑟𝑒𝑣 − 𝑟𝑐𝑢𝑟 ≤ 𝜖 or 𝑟𝑐𝑢𝑟 ≤ 𝜖 then break ; end if 𝑟𝑝𝑟𝑒𝑣 = 𝑟𝑐𝑢𝑟 ; end if end for
( SVM ) is a non negative linear combination of training samples . Furthermore , Non negative matrix factorization ( NMF ) [ 8 ] decomposes a non negative matrix into non negative basis and coefficient matrices . Therefore , instead of using 𝑙1 regularization , we can use non negativity to constrain LS in order to obtain sparse coefficients . In SC , we can also assume a new signal is a non negative combination of dictionary atoms in some scenes . This SC task is min
𝒙
1 2
∥𝒃 − 𝑨𝒙∥2
2 st 𝒙 ≥ 0 .
( 8 )
We call this non negative least squares ( NNLS ) [ 9 ] based SC . An algorithm solving this NNLS problem is given later in Section III . Through replacing 𝑙1 regularization with nonnegativity , our corresponding NNLS DL task is min 𝑨,𝒀
1 2
∥𝑫 − 𝑨𝒀 ∥2
𝐹 st diag(𝑨T𝑨 ) = 1 , 𝒀 ≥ 0 .
( 9 )
We do not constrain the signs of 𝑫 and 𝑨 as in semi NMF [ 10 ] . It is also convenient to be kernelized , because the signs of images in feature space is intractable . Keeping 𝑨 fixed , updating 𝒀 is a multiple NNLS problem : min 𝒀
1 2
∥𝑫 − 𝑨𝒀 ∥2
𝐹 st 𝒀 ≥ 0 .
( 10 )
Updating 𝑨 while fixing 𝒀 is simply a LS problem as in Equation 6 . Because this model is essentially a semiNMF , we therefore can say that NMF is a DL method in the words of SR . By replacing the 𝑙1LS solver in Algorithm 1 with a NNLS solver ( given next section ) , the framework can optimize the NNLS DL . The SC task for 𝑝 new samples
𝑩𝑚×𝑝 is 𝑝 single NNLS ( Equation 8 ) problems : min 𝑿
1 2
∥𝑩 − 𝑨𝑿∥2
𝐹 st 𝑿 ≥ 0 .
( 11 )
C . 𝑙1 Regularized Non Negative Model
If we combine 𝑙1 regularization and non negativity to gether , then we have the 𝑙1NNLS model . Its DL task is min 𝑨,𝒀
1 2
∥𝑫 − 𝑨𝒀 ∥2
𝐹 + 𝜆∥𝒀 ∥1st diag(𝑨T𝑨 ) = 1 , 𝒀 ≥ 0 .
( 12 )
If we replace the SC in Algorithm 1 by a 𝑙1NNLS solver ( Section III ) , then we have the framework of 𝑙1NNLS model .
The SC task for 𝑝 new samples ( 𝑩𝑚×𝑝 ) is min 𝑿
1 2
∥𝑩 − 𝑨𝑿∥2
𝐹 + 𝜆∥𝑿∥1 st𝑿 ≥ 0 .
( 13 )
III . OPTIMIZATION
Equation 7 can be formulated into multiple 𝑙1QP problem :
𝒙T
𝑖 𝑯𝒙𝑖 + 𝒄T
𝑖 𝒙𝑖 + 𝜆∥𝒙𝑖∥1 ,
1 2
( 14 ) where 𝑿 is of size 𝑘 × 𝑝 , Hessian 𝑯 𝑘×𝑘 = 𝑨T𝑨 , 𝒄𝑖 = −𝑨T𝒃𝑖 . Equation 14 is convex but not differentiable . It is equivalent to the following smooth constrained QP :
𝑝∑
𝑖=1 min 𝑿
𝑝∑
𝑖=1 min 𝑿,𝑼
𝒙T
𝑖 𝑯𝒙𝑖 + 𝒄T
𝑖 𝒙𝑖 + 𝝀T𝒖𝑖 st − 𝑼 ≤ 𝑿 ≤ 𝑼 ( 15 )
1 2 where 𝝀 ∈ ℝ 𝑛 + is a vector containing 𝑛 𝜆 ’s , and 𝑼 has the same size as 𝑿 . Solving this task is equivalent to solve 𝑝 single 𝑙1QP problems , each of which is min 𝒙,𝒖
1 2
𝒙T𝑯𝒙 + 𝒄T𝒙 + 𝝀T𝒖 , st − 𝒖 ≤ 𝒙 ≤ 𝒖 ,
( 16 ) where 𝒙 is a 𝑘 length vector . However , we shall see that the time complexity of multiple 𝑙1QP can far less than the summation of 𝑝 single 𝑙1QP problems .
Equations 11 and 13 can be solved by formulating them to the following multiple NNQP problem :
𝑝∑
𝑖=1 min 𝑿
𝒙T
𝑖 𝑯𝒙𝑖 + 𝒄T
𝑖 𝒙𝑖 , st 𝑿 ≥ 0 ,
1 2
( 17 ) where 𝑯 = 𝑨T𝑨 , 𝒄𝑖 = −𝑨T𝒃𝑖 for Equation 11 , and 𝒄𝑖 = 𝜆 − 𝑨T𝒃𝑖 for Equation 13 .
A . Active Set Methods
We now propose our active set algorithms for single 𝑙1QP and NNQP problems . The active set algorithms for multiple 𝑙1QP and NNQP problems will be proposed in Section III B . It is well known that active set algorithms usually have linear convergence rate [ 11 ] .
735968
1 ) Active Set 𝑙1QP Algorithm : Equation 16 can be for mulated to
[
[ min 𝒙,𝒖
𝑠𝑡
[ 𝒙T , 𝒖T ]
𝑯 0𝑘×𝑘 ][ 0𝑘×𝑘 0𝑘×𝑘 𝒙 𝒖
𝑰 𝑘×𝑘 −𝑰 𝑘×𝑘 −𝑰 𝑘×𝑘 −𝑰 𝑘×𝑘
][ ]
]
𝒙 𝒖 ≤ 0 ,
+ 𝒄T𝒙 + 𝝀T𝒖
( 18 ) where 𝑰 is an identity matrix . Obviously , the Hessian in this problem is positive semi definite . A general active set algorithm for constrained QP is provided in [ 12 ] . The main idea is that a working set is updated iteratively until it meets the true active set . In each iteration , a new solution 𝒙𝑘 to the QP constrained only by the current working set is obtained . If the update step 𝒑𝑘 = 𝒙𝑘 − 𝒙𝑘−1 is zero , then Lagrangian multipliers of the current active inequalities are computed . If all these Lagrangian multipliers corresponding to the working set are non negative , then the algorithm terminates with an optimal solution . Otherwise , an active inequality is dropped from the current working set . If the update step 𝒑𝑘 is not zero , then a update length 𝛼 is computed using the inequality of the current passive set . The new solution is updated as 𝒙𝑘 = 𝒙𝑘−1 + 𝛼𝒑𝑘 . If 𝛼 < 1 , then a blocking inequality is added to the working set .
To solve our specific problem efficiently , we have to modify the general method , because i ) our constraint is sparse , for the 𝑖 th constraint , we have 𝑥𝑖 − 𝑢𝑖 ≤ 0 ( if 𝑖 ≤ 𝑛 ) or −𝑥𝑖 − 𝑢𝑖 ≤ 0 ( if 𝑖 > 𝑛 ) ; and ii ) when 𝑢𝑖 is not constrained in the current working set , the QP constrained by the working set is unbounded , therefore it is not necessary to solve this problem to obtained 𝒑𝑘 . Therefore , we have to make four modifications for our specific problem . First , we require that the working set is complete . That is all the variables in 𝒖 must be constrained when computing the current update step . ( And therefore all variables in 𝒙 are also constrained . ) For example , if 𝑛 = 3 , a working set {1 , 2 , 6} is complete as all variables , 𝑥1 , 𝑥2 , 𝑥3 , 𝑢1 , 𝑢2 , 𝑢3 , are constrained , while {1 , 2 , 4} is not complete , as 𝑢3 ( and 𝑥3 ) is not constrained . Second , the update step of the variables that are constrained once in the working set are computed by solving the equality constrained QP . The variables constrained twice are zeros . In the example above , suppose the current working set is {1 , 2 , 4 , 6} , then 𝑥2 , 𝑥3 , 𝑢2 , 𝑢3 are computed by the constrained QP , while 𝑥1 and 𝑢1 are directly set to zeros . This is because the only value satisfying the constraint −𝑢1 ≤ 𝑥1 ≤ 𝑢1 is 𝑥1 = 𝑢1 = 0 . Third , the scale of equality constrained QP is half of the original algorithm . In the above example , we do not need to solve the equality constrained QP with four variables . In fact we only need two variables by setting 𝑢2 = −𝑥2 and 𝑢3 = 𝑥3 . Forth , once a constraint is dropped from the working set and it becomes incomplete , other inequalities must be immediately added to it until it is complete .
2 ) Active Set NNQP Algoritm : Now , we present the active set algorithm for NNQP . This problem is easier to solve than 𝑙1QP . Our algorithm is obtained via revising the NNLS active set algorithm by [ 9 ] . Due to page limit , we omit the details . The main idea is that a constraint is added to the current active set if it has the minimum negative Lagrangian multiplier , and a constraint is removed from the current active set if it obtains negative solution . This procedure is repeated until the true active set is found . The solution corresponding to the active set is set to zero , while the rest is the analytical solution to unconstrained QP .
B . Methods for Multiple 𝑙1QP and NNQP
If we use interior point and proximal methods to solve the multiple 𝑙1QP ( Equation 15 ) and NNQP ( Equation 17 ) problems , it is difficult to compute the individual single problems in parallel and share computations . Therefore , the time complexity of the multiple problem is the summation of that of single problems . However , the multiple problem can be much more efficiently solved by active set algorithm . The single active set algorithms can be solved in parallel by sharing the computation of systems of linear equations . At each iteration , single problems , having the same active ( working ) set , have the same systems of linear equations to compute . These systems of linear equations can be solved once only . For a large number of 𝑛 ( number of training samples ) and 𝑝 ( number of unknown samples ) and small 𝑘 ( number of dictionary atoms ) , active set algorithms have dramatic computational advantage over interior point and proximal methods unless interior point and proximal methods could have a scheme of share computations among individual problems . Moreover , the interior points method for Equation 16 does not allow 𝑢2 𝑖 must be always greater than 𝑥2 𝑖 = 𝑥2 is 𝑖 naturally possible when the 𝑖 th constraint is active , that is 𝑢𝑖 = 𝑥𝑖 = 0 . Active set algorithms do allow this situation . Therefore , active set methods are more precise than interiorpoint methods . Furthermore , 𝑘 ( the size of learned dictionary ) is usually small or median . It is well known that activeset algorithm is usually the most efficient one for small and median LP and QP problems . ( Simplex is an activeset algorithm for LP . ) Therefore , active set algorithms are good choices to solve multiple 𝑙1QP and NNQP problems . However , in the case of very large 𝑘 , active set algorithm would loose its merits because it becomes expensive to search the true active set and the probability that two single problems have the same active set becomes rare .
𝑖 and 𝑢2 𝑖 due to feasibility . But 𝑢2
𝑖 = 𝑥2
IV . KERNEL SPARSE REPRESENTATION
Based on the revised linear models in Section II and the optimization in Section III , we are now ready to propose the kernel versions . The 𝑙1LS based kernel DL and SC are respectively expressed in the following equations :
∥𝜙(𝑫 ) − 𝑨𝜙𝒀 ∥2
𝐹 + 𝜆∥𝒀 ∥1st diag(𝑨T
𝜙𝑨𝜙 ) = 1 , ( 19 ) min 𝑨𝜙,𝒀
1 2
736969 and
∥𝜙(𝑩 ) − 𝑨𝜙𝑿∥2
𝐹 + 𝜆∥𝑿∥1 ,
1 2 min 𝑿
( 20 ) where 𝜙(∙ ) is a mapping function . 𝑨𝜙 is the dictionary in feature space . The NNLS and 𝑙1NNLS based SR are defined analogically . These two non negative DL methods can be actually viewed as variants of kernel NMF and kernel sparse NMF , respectively . Recall that the optimizations of our three linear models , in Section II , only require inner products of signals ( 𝑫T𝑫 , 𝑫T𝑩 , and 𝑩T𝑩 ) . Therefore , they can be easily extended to kernel versions by replacing these inner products by kernel matrices , is 𝐾(𝑫 , 𝑫 ) = 𝜙(𝑫)T𝜙(𝑫 ) , 𝐾(𝑫 , 𝑩 ) = 𝜙(𝑫)T𝜙(𝑩 ) , and 𝐾(𝑩 , 𝑩 ) = 𝜙(𝑩)T𝜙(𝑩 ) . that
V . EXPERIMENT
The linear and kernel models , optimized by our activeset algorithms , are tested on three undercomplete and three overcomplete datasets ( see Table I ) . Linear SVM is used after feature extraction using SR . We compared our models with NNLS and 𝑙1LS [ 13 ] classifiers as well as 𝑘 NN . NNLS and 𝑙1LS classifiers lazily use all training samples as dictionary . When only using SC , the nearest subspace rule [ 4 ] is used to assign class label based on the sparse code . Radial basis function was used as kernel . The parameters of the methods were selected by grid search . 4 fold crossvalidation ( CV ) were employed to split data into training set and test set . We set 𝑚𝑎𝑥𝐼𝑡𝑒𝑟 = 200 and 𝜖 = 10−4 in Algorithm 1 . 4 fold CV was run 20 times and the average result were recorded for fair comparison .
KSR−NNLS KSR−l1NNLS KSR−l1LS SR−NNLS SR−l1NNLS SR−l1LS NNLS l1LS KNN
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55 y c a r u c c A
0.5
Colon
Breast
Undercomplete Data
Prostate
Figure 1 . Classification Performance on High Dimensional Data . on Wave2 due to large sample size . First , all sparse methods obtained better accuracies than 𝑘NN . Second , though all methods on Iris have similar accuracy , all the SR approaches , but SR 𝑙1LS , obtained significantly better result than 𝑘 NN on Wine . Also , all kernel methods yielded better results than the linear ones on Wine . This corroborates that our kernel methods can outperform the linear ones in some cases .
KSR−NNLS KSR−l1NNLS KSR−l1LS SR−NNLS SR−l1NNLS SR−l1LS NNLS l1LS KNN
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55 y c a r u c c A
Table I
DATASETS
0.5
Iris
Wine
Overcomplete Data
Wave2
DATASET COLON [ 14 ] BREAST [ 15 ] PROSTATE [ 16 ] IRIS [ 17 ] WINE [ 17 ] WAVE2 [ 17 ]
#CLASS#FEATURE
2 5 2 3 3 3
2000 13582 15154
4 13 40
#SAMPLE 40+22=62 47+25=158 253+69=322 3 × 50=150 59+71+48=178 1692+1653+1655=5000
The average accuracies on high dimensional datasets are reported in Figure 1 . Prefix “ SR ” indicates that both DL and SC are employed . “ KSR ” denotes the kernel version . NNLS and 𝑙1LS without prefix means only NNLS and 𝑙1LS SC are respectively applied . Firstly , we can see that on all three datasets , the SR approaches are much better than 𝑘 NN . Secondly , the SR approaches using linear and kernel DL and SC outperform NNLS and 𝑙1LS on Colon and Breast . This convinces us that dictionary learning is necessary to improve performance for some undercomplete data . Thirdly , the performance of kernel approaches just slightly outperform the linear counterparts .
The accuracy on low dimensional data is compared in Figure 2 . The active set based NNLS and 𝑙1LS were not tested
Figure 2 . Classification Performance on Low Dimensional Data .
We also compared the efficiency of our active set algorithms with the interior point [ 2 ] and proximal [ 3 ] algorithms on Colon , which is the smallest sized data in our experiment , and Wave2 which is the largest . For fair competition , we use the same dictionary size for a dataset . On Colon , dictionary with 8 atoms gave the best accuracy . The dictionary size of Wave2 is set to 9 based on the fact that there are three classes and every wave is generated by a linear combination of 2 or 3 basis waves . The average computing time over 20 runs of 4 fold CV is shown in Figures 3 and 4 . The logarithm of base 2 was taken on time in second to have clear comparison . Firstly , on small data , we can see that our active set algorithms are faster than interior point and proximal methods in the context of DL . For example , the computation of KSR 𝑙1LS using interior point and proximal algorithms is as 26 and 22 times , respectively , as that using active set algorithm . The SC ( without DL ) using active set algorithm took similar time with proximal algorithm , and ran much faster than interior point algorithm . Secondly , on large data as shown in Figure 4 , the complexity of interior point
737970 and proximal methods are also as around 28 and 22 times , respectively , as that of active set algorithm . This proves the superiorities of active set method discussed in Section III B .
) e m T ( i g o l
2
) e m T ( i g o l
2
6
4
2
0
−2
−4
−6
14
12
10
8
6
4
2
0
KSR−NNLS KSR−l1NNLS KSR−l1LS NNLS l1LS
Active−Set
Proximal Method
Interior−Point
Figure 3 . Computing Time on Colon .
KSR−l1LS l1LS
Active−Set
Proximal Method
Interior−Point
Figure 4 . Computing Time on Wave2 .
VI . CONCLUSION AND FUTURE WORKS
In this paper we propose a generic DL framework for three SR models . We reveal that its optimization only needs inner products of samples . Under this framework , the linear models can be easily kernelized . We tailor active set based sparse coding algorithms and accelerate the computation of multiple sparse coding problem by sharing the computation among single problems . Numerical experiments show that our kernel approaches can sometimes obtain higher accuracy than the linear ones and the SRs without DL . Computational comparison shows that our active set algorithm is more efficient than interior point and proximal methods . There are many problems unsolved in this field . First , model selection might be solved by optimization . Second , kernel structured dictionary learning is also an interesting direction . Third , new optimization algorithms are required when the dictionary size is very large .
ACKNOWLEDGMENT
This research has been supported by IEEE CIS Summer Research Grant 2010 , OGS 2011 2013 , and NSERC #RGPIN228117 2011 .
738971
REFERENCES
[ 1 ] M . Elad , Sparse and Redundant Representations . New York :
Springer , 2010 .
[ 2 ] S . J . Kim , K . Koh , M . Lustig , S . Boyd , and D . Gorinevsky , “ An interior point method for large scale 𝑙1 regularized least squares , ” IEEE J . Selected Topics in Signal Processing , vol . 1 , no . 4 , pp . 606–617 , 2007 .
[ 3 ] R . Jenatton , J . Mairal , G . Obozinski , and F . Bach , “ Proximal methods for hierarchical sparse coding , ” JMLR , vol . 12 , no . 2011 , pp . 2297–2334 , 2011 .
[ 4 ] J . Wright , A . Yang , A . Ganesh , S . S . Sastry , and Y . Ma , “ Robust face recognition via sparse representation , ” TPAMI , vol . 31 , no . 2 , pp . 210–227 , 2009 .
[ 5 ] L . Zhang , W . D . Zhou , P . C . Chang , J . Liu , Z . Yan , T . Wang , and F . Z . Li , “ Kernel sparse representation based classifier , ” IEEE Transactions on Signal Processing , vol . 60 , no . 4 , pp . 1684 – 1695 , 2012 .
[ 6 ] J . Yin , X . Liu , Z . Jin , and W . Yang , “ Kernel sparse representation based classification , ” Neurocmputing , vol . 77 , pp . 120–128 , 2012 .
[ 7 ] S . Gao , I . W . H . Tsang , and L . T . Chia , “ Kernel sparse representation for image classification and face recognition , ” in ECCV . Springer , 2010 , pp . 1–14 .
[ 8 ] D . D . Lee and S . Seung , “ Learning the parts of objects by non negative matrix factorization , ” Nature , vol . 401 , pp . 788– 791 , 1999 .
[ 9 ] C . L . Lawson and R . J . Hanson , Solving Least Squares
Problems . Piladelphia : SIAM , 1995 .
[ 10 ] C . Ding , T . Li , and M . I . Jordan , “ Convex and seminonnegative matrix factorizations , ” TPAMI , vol . 32 , no . 1 , pp . 45–55 , 2010 .
[ 11 ] E . McCall , “ Performance results of the simplex algorithm for a set of real world linear programming models , ” Communications of the ACM , vol . 25 , no . 3 , pp . 207–212 , 1982 .
[ 12 ] J . Nocedal and S . J . Wright , Numerical Optimization , 2nd ed .
New York : Springer , 2006 .
[ 13 ] X . Hang and F X Wu , “ Sparse representation for classification of tumors using gene expression data , ” J . Biomedicine and Biotechnology , vol . 2009 , 2009 .
[ 14 ] U . Alon , “ Broad patterns of gene expression revealed by clustering of tumor and normal colon tissues probed by oligonucleotide arrays , ” PNAS , vol . 96 , no . 12 , pp . 6745– 6750 , 1999 .
[ 15 ] Z . Hu , “ The molecular portraits of breast tumors are conserved across microarray platforms , ” BMC Genomics , vol . 7 , p . 96 , 2006 .
[ 16 ] E . I . Petricoin , “ Serum proteomic patterns for detection of prostate cancer , ” J . National Cancer Institute , vol . 94 , no . 20 , pp . 1576–1578 , 2002 .
[ 17 ] A . Frank and A . Asuncion , “ Uci machine learning repository , ” University of California , Irvine , California , Tech . Rep . , 2010 .
