2012 IEEE 12th International Conference on Data Mining 2012 IEEE 12th International Conference on Data Mining
Multi Task Semi Supervised Semantic Feature Learning for Classification
Changying Du
â€ Â§
â€  , Fuzhen Zhuang
â€  , Qing He and Zhongzhi Shi
â€ 
The Key Laboratory of Intelligent Information Processing , Institute of Computing Technology
â€ 
Â§
University of Chinese Academy of Sciences , Beijing 100049 , China
Chinese Academy of Sciences , Beijing 100190 , China Email : {ducy , zhuangfz , heq , shizz}@icsictaccn
Abstractâ€”Multi task learning has proven to be useful to boost the learning of multiple related but different tasks . Meanwhile , latent semantic models such as LSA and NMF are popular and effective methods to extract discriminative semantic features of high dimensional dyadic data . In this paper , we present a method to combine these two techniques together by introducing a new matrix tri factorization based formulation for semi supervised latent semantic learning , which can incorporate labeled information into traditional unsupervised learning of latent semantics . Our inspiration for multi task semantic feature learning comes from two facts , ie , 1 ) multiple tasks generally share a set of common latent semantics ; and 2 ) a semantic usually has a stable indication of categories no matter which task it is from . Thus to make multiple tasks learn from each other we wish to share the associations between categories and those common semantics among tasks . Along this line , we propose a novel joint Nonnegative matrix tri factorization framework with the aforesaid associations shared among tasks in the form of a semantic category relation matrix . Our new formulation for multi task learning can simultaneously learn ( 1 ) discriminative semantic features of each task ; ( 2 ) predictive structure and categories of unlabeled data in each task ; ( 3 ) common semantics shared among tasks and specific semantics exclusive to each task . We give alternating iterative algorithm to optimize our objective and theoretically show its convergence . Finally extensive experiments on text data along with the comparison with various baselines and three state of the art multi task learning algorithms demonstrate the effectiveness of our method .
Keywords multi task learning ; semantic feature learning ; joint nonnegative matrix tri semi supervised learning ; factorization ; text classification ;
I . INTRODUCTION
Multi task learning ( MTL ) refers to learning multiple related tasks together so as to improve the performance of each task relative to learning them separately . Over the past decade , MTL has attracted more and more attention in the community of machine learning and data mining , and has been applied to many important areas including computer vision [ 1 ] , [ 2 ] , natural language processing [ 3 ] , and bioinformatics [ 4 ] , [ 5 ] . As well known , what is crucial in MTL is the knowledge sharing scheme among tasks . Typical approaches in the literature can be roughly categorized into two kinds . One kind are the model based approaches which can be further divided into two forms , ie , placing a common prior on the model parameters of each task in hierarchical
Bayesian models [ 6 ] , [ 7 ] , [ 8 ] , [ 9 ] , [ 10 ] and explicitly sharing some model parameters or model structure among tasks [ 3 ] . So far , model based methods have mainly focused on discriminative models such as linear regression and logistic regression . The other kind are the feature based approaches which share a set of common features among tasks in original input space or the transformed feature space [ 11 ] , [ 12 ] , [ 13 ] , [ 5 ] , [ 1 ] . To learn new features , most previous work have used a common feature transformation for all tasks on the original input data . While having the convenience to be formulated , this may be not the best choice in some problems , eg , in multi task news classification , sports news about baseball may have rather different key words from the news about hockey since different sports have different terms . In such cases , simply applying the same feature transformation for all tasks will either not help much or lose much useful information , and one may most want to extract the semantic features in each task .
On the other hand , latent semantic models such as LSA [ 14 ] , NMF [ 15 ] and LDA [ 16 ] have provided an effective way to learn the semantic features of dyadic data in unsupervised or supervised manner [ 17 ] , [ 18 ] . While unsupervised latent semantic models are popular for dimensionality reduction , their supervised counterparts are similar in spirit to supervised dimensionality reduction [ 19 ] , which aims at finding a low dimensional representation of data such that the supervision information can be well fitted by some predictive model that will be used to predict unlabeled data . However , single task supervised feature learning may be not reliable when supervision information is not enough . To improve reliability , we may combine the ideas of supervised and unsupervised semantic models and yield a semisupervised semantic learning model . Besides , by means of multi task learning , we can learn multiple such problems jointly to further improve the performance of each problem . Motivated by these aspects , in this paper , we combine multi task learning , latent semantic learning and semisupervised learning , yielding a novel multi task semantic feature learning method with few labeled and abundant unlabeled data in each task . Our main inspirations come from multi task text classification where despite of the different marginal distributions among different tasks in raw word feature space , there may be some common semantic topics
1550 4786/12 $26.00 Â© 2012 IEEE 1550 4786/12 $26.00 Â© 2012 IEEE DOI 101109/ICDM201215 DOI 101109/ICDM201215
11 191 shared among tasks , and the associations between those common semantic topics and text categories may remain stable across different tasks . With these stable semanticcategory associations being exploited for the knowledge sharing among tasks1 , multiple tasks may learn from each other . To formulate our learning framework , first we provide a new matrix tri factorization based formulation for semisupervised latent semantic learning , which can incorporate labeled information into the traditional unsupervised learning of latent semantics and can combine the finding of discriminative semantic features , the learning of predictive structure and the predicting of unlabeled data in a unified optimization framework . Then we extend this single task formulation to the multi task scenario , by sharing the associations between categories and those common semantics among tasks in the form of a semantic category relation matrix . Our new formulation for multi task semantic feature learning essentially is a joint Nonnegative matrix trifactorization based optimization which can simultaneously learn ( 1 ) discriminative semantic features of each task ; ( 2 ) predictive structure and categories of unlabeled data in each task ; ( 3 ) common semantics shared among tasks and specific semantics exclusive to each task .
Compared with typical model based approaches that either directly share some model parameters or place a common prior on the parameters , our method not only shares a part of model parameters but also automatically learns the correspondingly shared features that may have different forms in different tasks .
There also exist key differences between our method and typical multi task feature selection/learning ( MTFS/L ) . First , typical MTFS only seeks to find a set of common features in the original input space [ 13 ] , [ 5 ] , while our method finds common features in the more meaningful semantic space ; second , typical MTFL [ 13 ] , [ 1 ] enforces a common transformation for all tasks on the original input data , while our method allows the common semantic features learned among tasks to have different forms in terms of the multinomial distributions over words ; third , most previous studies on MTFS/L mainly use the labeled data , while our method can naturally incorporate usually abundant unlabeled samples in each task for semantic feature learning .
We give alternating iterative algorithm for the optimization of our objective and theoretically show its convergence . Then we construct 144 4 task learning problems from the widely used 20 Newsgroups data set to evaluate our algorithm . Experimental results demonstrate our method can provide superior generalization performance compared with various baselines and several state of the art multi task learning algorithms .
The remainder is organized as follows . Section II states
1These associations shared among tasks essentially are a part of model parameters in the latent semantic space of each task , but here the model is not discriminative , thus differs from typical model based MTL methods . the notations and briefly covers the necessary preliminaries . Then in Section III we propose our multi task semantic feature learning algorithm by first introducing a matrix trifactorization based formulation for semi supervised latent semantic learning . The experimental results are demonstrated in Section IV and related works are given in Section V . Finally we conclude the paper in Section VI .
II . PROBLEM SPECIFICATION AND PRELIMINARIES
A . Problem Specification and Notations Assume we have ğ‘‡ classification learning tasks indexed by ğ‘– = 1,â‹…â‹…â‹… , ğ‘‡ , each of which with few labeled data ğ’Ÿğ‘–ğ¿ = {(xğ‘–,ğ‘› , ğ‘¦ğ‘–,ğ‘›)}ğ‘ğ‘–ğ¿ ğ‘›=1 but sufficient unlabeled data ğ’Ÿğ‘–ğ‘ˆ = {xğ‘–,ğ‘›}ğ‘ğ‘–ğ‘ˆ ğ‘›=1 . Our objective is to learn these ğ‘‡ tasks together with the hope that they will help each other in the learning process . The learning performance is measured by the averaged classification accuracy over all tasks , and a multi task learning algorithm is evaluated by its performance improvements over single task learning ( learning each task independently ) and pooling ( simply combine all labeled and unlabeled data in each task together ) . We denote the set of real numbers and the set of nonnegative real numbers by â„ and â„+ respectively . ğ‘‹ğ‘– âˆˆ â„ ğ‘€Ã—ğ‘ğ‘– + is the word document matrix2 in the ğ‘– th task , with ğ‘ğ‘– = ğ‘ğ‘–ğ¿ + ğ‘ğ‘–ğ‘ˆ being the number of total documents in the ğ‘– th task and ğ‘€ being the number of vocabulary words . ğ¾ is the number of total semantic topics in each task , while ğ” is the number of shared common topics among tasks . ğ¶ denotes the number of data categories in each task . ğ‘‹ğ‘ğ‘ indicates the ğ‘ th row and the ğ‘ th column element of matrix ğ‘‹ , while âˆ¥ â‹… âˆ¥ and Tr(â‹… ) denote the Frobenius norm and the trace of a matrix respectively .
B . Nonnegative Matrix Factorization ( NMF )
As shown in [ 15 ] , NMF can learn semantic features of text . It simply aims to factorize a data matrix ğ‘‹ into two nonnegative factor matrices ğ¹ and ğº :
ğ‘‹ â‰ˆ ğ¹ ğº , ğ¹ â‰¥ 0 , ğº â‰¥ 0 , where ğ‘‹ âˆˆ â„ ğ‘€Ã—ğ‘ ğ‘€ features , ğ¹ âˆˆ â„ + ğº âˆˆ â„ semantic space spanned by the columns of ğ¹ . contains ğ‘ data samples in terms of represents ğ¾ latent semantics and contains the coordinates of each sample in the
ğ‘€Ã—ğ¾ +
ğ¾Ã—ğ‘ +
The nonnegativity constraints makes the learned semantics in NMF more interpretable than that in LSA which uses the Singular Value Decomposition ( SVD ) technique .
As two factor NMF is restrictive sometimes , Ding et al . studied Nonnegative Matrix Tri Factorization ( NMTF ) [ 20 ] :
ğ‘‹ â‰ˆ ğ¹ ğ‘†ğº , ğ¹ â‰¥ 0 , ğ‘† â‰¥ 0 , ğº â‰¥ 0 .
NMTF can have many interpretations . In this paper , we will endow ğº with the label information of data , and the
2In the sequel , we only take text data as example though our method can address any dyadic data .
12192 labels of training data will be fixed during the optimization . Accordingly , we endow ğ‘† with the semantic category relations , which can be properly shared among tasks .
III . MULTI TASK SEMI SUPERVISED SEMANTIC
FEATURE LEARNING FOR CLASSIFICATION
A . Semi Supervised Semantic Feature Learning for Classification with NMTF
Let us begin with a semi supervised classification setting of each learning task , ie , there are a portion of labeled data and large amount of unlabeled data in each task . We formulate this problem for the ğ‘– th task as follows : min
ğ¹ğ‘–,ğ‘†ğ‘–,ğºğ‘–â‰¥0
âˆ¥ğ‘‹ğ‘– âˆ’ ğ¹ğ‘–ğ‘†ğ‘–[ğºğ‘–ğ¿ ğºğ‘–ğ‘ˆ ]âˆ¥2
( 1 ) where
âˆ™ ğ‘‹ğ‘– = [ ğ‘‹ğ‘–ğ¿ ğ‘‹ğ‘–ğ‘ˆ ] âˆˆ â„ matrix in the ğ‘– th task , with ğ‘‹ğ‘–ğ¿ âˆˆ â„ labeled data and ğ‘‹ğ‘–ğ‘ˆ âˆˆ â„ is the word document being the the unlabeled data ; contains the latent semantic topics in the
ğ‘€Ã—ğ‘ğ‘– + ğ‘€Ã—ğ‘ğ‘–ğ‘ˆ +
ğ‘€Ã—ğ‘ğ‘–ğ¿ +
ğ‘– th task , which can be seen as semantic features ;
ğ‘€Ã—ğ¾ + ğ¾Ã—ğ¶ +
âˆ™ ğ¹ğ‘– âˆˆ â„ âˆ™ ğ‘†ğ‘– âˆˆ â„ âˆ™ ğºğ‘– = [ ğºğ‘–ğ¿ ğºğ‘–ğ‘ˆ ] âˆˆ â„ the ğ‘– th task ; is the semantic category relation matrix in
ğ¶Ã—ğ‘ğ‘– + contains the category information of data matrix ğ‘‹ğ‘– . Those supervision information is injected into ğºğ‘–ğ¿ 3 and fixed during the optimization . ğºğ‘–ğ‘ˆ contains the predicted category information of unlabeled data ğ‘‹ğ‘–ğ‘ˆ 4 , which can be initialized by a supervised learner trained on those few labeled data ;
For sake of brevity , we refer this formulation as SemiSupervised semantic Feature Learning for Classification ( S3FLC ) in the sequel . Note that S3FLC can not only incorporate labeled information into the traditional unsupervised learning of latent semantics , but also combine the finding of discriminative latent semantics , the learning of predictive structure and the predicting of unlabeled data in a unified optimization framework . Zhuang et al . [ 21 ] achieved fairly good transfer learning results by using a special case of this formulation to model the fully labeled source domain data .
B . The Proposed Multi Task Learning Formulation is from , so we may split
As stated in the introduction , multiple related tasks generally shares a set of common semantics , and a semantic usually has a stable meaning of categories no matter which task it the semantic category relation matrix of each task into two parts : one part is for those common semantics and will be shared across tasks , while the other part corresponds to the specific topics of each task . Along this line of thoughts , we jointly factorize
3If the ğ‘› th sample in ğ‘‹ğ‘–ğ¿ belongs to category ğ‘ , then ( ğºğ‘–ğ¿)ğ‘ğ‘› = 1 , and for any ğ‘â€² âˆˆ {1 , â‹… â‹… â‹… , ğ¶} , ğ‘â€² âˆ•= ğ‘ , ( ğºğ‘–ğ¿)ğ‘â€²ğ‘› = 0 .
4The ğ‘› th sample in ğ‘‹ğ‘–ğ‘ˆ belongs to category ğ‘ = arg maxğ‘(ğºğ‘–ğ‘ˆ )ğ‘ğ‘› .
ğ‘‡ word document matrices ğ‘‹ğ‘– , ğ‘– = 1,â‹…â‹…â‹… , ğ‘‡ as follows , with the hope that this will pass knowledge via the common semantic category associations ( contained in ğ‘† ) from some â€œ confident â€ tasks to those unconfident ones :
[ ğºğ‘–ğ¿ ğºğ‘–ğ‘ˆ ]âˆ¥2
( 2 ) min
ğ¹ğ‘–,ğ‘†,ğ‘…ğ‘–,ğºğ‘–â‰¥0
âˆ¥ğ‘‹ğ‘– âˆ’ [ ğ¹ğ‘–ğ‘  ğ¹ğ‘–ğ‘Ÿ ]
ğ‘‡âˆ‘
ğ‘–=1
[
]
ğ‘† ğ‘…ğ‘– where
ğ‘€Ã—ğ” +
ğ‘€Ã—ğ¾ +
âˆ™ ğ‘‹ğ‘– , and ğºğ‘– = [ ğºğ‘–ğ¿ ğºğ‘–ğ‘ˆ ] have the same meanings as âˆ™ ğ¹ğ‘– = [ ğ¹ğ‘–ğ‘  ğ¹ğ‘–ğ‘Ÿ ] âˆˆ â„ those in the single task S3FLC introduced above ; contains the semantic topics in the ğ‘– th task , with ğ¹ğ‘–ğ‘  âˆˆ â„ , ğ‘– = 1,â‹…â‹…â‹… , ğ‘‡ containing the common semantic topics among tasks and ğ¹ğ‘–ğ‘Ÿ âˆˆ â„ ğ‘€Ã—(ğ¾âˆ’ğ” ) containing the specific topics of + the ğ‘– th task . ğ¾Ã—ğ¶ is the semantic category relation matrix with ğ‘† âˆˆ â„ + containing the shared semantic category associations among tasks and ğ‘…ğ‘– âˆˆ ( ğ¾âˆ’ğ”)Ã—ğ¶ containing associations between the ğ‘– th +
âˆ™ Ëœğ‘†ğ‘– = [ ğ‘† ğ‘…ğ‘–]ğ‘‡ âˆˆ â„
â„ task â€™s specific topics and text categories ;
ğ”Ã—ğ¶ +
Note that , ( a ) common semantic topics among tasks mean that for any 1 â‰¤ ğ‘– , ğ‘— â‰¤ ğ‘‡ , ğ‘– âˆ•= ğ‘— , ğ¹ğ‘–ğ‘  and ğ¹ğ‘—ğ‘  are semanticly corresponding to each other but there is no need to satisfy ğ¹ğ‘–ğ‘  = ğ¹ğ‘—ğ‘  , which is to say , several corresponding topics have the same semantic relations to text categories , but they may have different forms in terms of the multinomial topic distributions over words ; ( b ) the number ğ” of shared topics can be adjusted to accommodate to the relatedness of tasks . For closely related tasks we may use a large ğ” even equal to the total topics ğ¾ , and for loosely related tasks we may use a small ğ” even equal to zero which is equivalent to no knowledge being shared ; ( c ) we can employ a supervised learner ie , Logistic Regression to pre predict those unlabeled data and initialize ğºğ‘–ğ‘ˆ using these originally predicted probabilities . This pre predicting process can be done either in the pooling manner or in single task learning manner ; ( d ) since we have learned ğ¹ğ‘– and Ëœğ‘†ğ‘– during this transductive process , new data that not in the original factorization of ğ‘‹ğ‘– can also be predicted by factorizing them according to ( 2 ) with ğ¹ğ‘– and Ëœğ‘†ğ‘– initialized to the original factorization results .
In what follows , we detail on how to solve the joint optimization problem in Eq ( 2 ) .
C . Optimization
The objective function ğ’ª in Eq ( 2 ) is not convex in all its variables , thus , it is unrealistic to find the global minima for it . In the following , we provide an alternating iterative algorithm to obtain the local optima of ( 2 ) . Ëœğ‘†ğ‘–ğºğ‘–âˆ¥2 , ğ‘– = 1,â‹…â‹…â‹… , ğ‘‡ , then after some
Let ğ·ğ‘– = âˆ¥ğ‘‹ğ‘– âˆ’ ğ¹ğ‘– simple algebra we have5
5Here we used the properties Tr(ğ´ + ğµ ) = Tr(ğ´ ) + Tr(ğµ ) and
Tr(ğ´ğµ ) = Tr(ğµğ´ ) .
13193
ğ·ğ‘– = Tr((ğ‘‹ğ‘– âˆ’ ğ¹ğ‘–
Ëœğ‘†ğ‘–ğºğ‘–)ğ‘‡ ( ğ‘‹ğ‘– âˆ’ ğ¹ğ‘–
Ëœğ‘†ğ‘–ğºğ‘– ) )
= Tr(ğ‘‹ ğ‘‡
= Tr(ğ‘‹ ğ‘‡
ğ‘– ğ‘‹ğ‘– âˆ’ 2ğ‘‹ ğ‘‡ ğ‘– ğ‘‹ğ‘– âˆ’ 2ğ‘‹ ğ‘‡
Ëœğ‘†ğ‘–ğºğ‘– + ( ğ¹ğ‘– ğ‘– ğ¹ğ‘– ğ‘– ğ¹ğ‘–ğ‘ ğ‘†ğºğ‘– âˆ’ 2ğ‘‹ ğ‘‡
Ëœğ‘†ğ‘–ğºğ‘–)ğ‘‡ ğ¹ğ‘– ğ‘– ğ¹ğ‘–ğ‘Ÿğ‘…ğ‘–ğºğ‘–
Ëœğ‘†ğ‘–ğºğ‘– )
+(ğ¹ğ‘–ğ‘ ğ‘†ğºğ‘–)ğ‘‡ ğ¹ğ‘–ğ‘ ğ‘†ğºğ‘– + 2(ğ¹ğ‘–ğ‘ ğ‘†ğºğ‘–)ğ‘‡ ğ¹ğ‘–ğ‘Ÿğ‘…ğ‘–ğºğ‘– +(ğ¹ğ‘–ğ‘Ÿğ‘…ğ‘–ğºğ‘–)ğ‘‡ ğ¹ğ‘–ğ‘Ÿğ‘…ğ‘–ğºğ‘–
( 3 ) The partial derivatives of ğ·ğ‘– are ( we assume ğºğ‘–ğ¿ is fixed , and the matrix variables are ğ¹ğ‘– , ğºğ‘–ğ‘ˆ , ğ‘…ğ‘– and ğ‘† ) Ëœğ‘†ğ‘–ğºğ‘–( Ëœğ‘†ğ‘–ğºğ‘–)ğ‘‡ âˆ’ 2ğ‘‹ğ‘–( Ëœğ‘†ğ‘–ğºğ‘–)ğ‘‡
= 2ğ¹ğ‘–
( 4 )
âˆ‚ğ·ğ‘– âˆ‚ğ¹ğ‘–
= 2(ğ¹ğ‘–
Ëœğ‘†ğ‘–)ğ‘‡ ğ¹ğ‘–
Ëœğ‘†ğ‘–ğºğ‘–ğ‘ˆ âˆ’ 2(ğ¹ğ‘–
âˆ‚ğ·ğ‘– âˆ‚ğºğ‘–ğ‘ˆ
âˆ‚ğ·ğ‘– âˆ‚ğ‘…ğ‘–
= 2ğ¹ ğ‘‡
ğ‘–ğ‘Ÿ ğ¹ğ‘–ğ‘ ğ‘†ğºğ‘–ğºğ‘‡
ğ‘– + 2ğ¹ ğ‘‡
ğ‘–ğ‘Ÿ ğ¹ğ‘–ğ‘Ÿğ‘…ğ‘–ğºğ‘–ğºğ‘‡
( 5 )
ğ‘–ğ‘Ÿ ğ‘‹ğ‘–ğºğ‘‡ ğ‘– ( 6 )
Ëœğ‘†ğ‘–)ğ‘‡ ğ‘‹ğ‘–ğ‘ˆ ğ‘– âˆ’ 2ğ¹ ğ‘‡ ğ‘– âˆ’ 2ğ¹ ğ‘‡
ğ‘– + 2ğ¹ ğ‘‡
âˆ‚ğ·ğ‘– âˆ‚ğ‘† = 2ğ¹ ğ‘‡
ğ‘–ğ‘  ğ¹ğ‘–ğ‘ ğ‘†ğºğ‘–ğºğ‘‡
ğ‘–ğ‘  ğ¹ğ‘–ğ‘Ÿğ‘…ğ‘–ğºğ‘–ğºğ‘‡
ğ‘–ğ‘  ğ‘‹ğ‘–ğºğ‘‡ ğ‘– ( 7 ) Now we consider the updating of ğ¹ğ‘– when ğ¹ğ‘— , ğºğ‘– , ğ‘…ğ‘– and ğ‘† are fixed , where ğ‘– , ğ‘— = 1,â‹…â‹…â‹… , ğ‘‡ , ğ‘— âˆ•= ğ‘– . For this problem , Eq ( 2 ) is equivalent to the following optimization :
âˆ¥ğ‘‹ğ‘– âˆ’ ğ¹ğ‘–
Ëœğ‘†ğ‘–ğºğ‘–âˆ¥2 min ğ¹ğ‘–â‰¥0
( 8 )
We can solve this constrained quadratic programming problem in many ways , eg , the Reduced Gradient method and the Interior Point method . However , we find these methods either are complex to implement or converge very slow . Here we make use of the multiplicative updating method [ 22 ] , [ 20 ] which is free from selecting the step size that is needed in gradient based methods . By using the KKT complementarity condition of ( 8 ) and note ( 4) ) , we have
Ëœğ‘†ğ‘–ğºğ‘–( Ëœğ‘†ğ‘–ğºğ‘–)ğ‘‡ âˆ’ 2ğ‘‹ğ‘–( Ëœğ‘†ğ‘–ğºğ‘–)ğ‘‡ )ğ‘ğ‘(ğ¹ğ‘–)ğ‘ğ‘ = 0
( 2ğ¹ğ‘– which is equivalent to
Ëœğ‘†ğ‘–ğºğ‘–( Ëœğ‘†ğ‘–ğºğ‘–)ğ‘‡ âˆ’ 2ğ‘‹ğ‘–( Ëœğ‘†ğ‘–ğºğ‘–)ğ‘‡ )ğ‘ğ‘(ğ¹ğ‘–)2
( 2ğ¹ğ‘–
ğ‘ğ‘ = 0
Solving this equation leads to the following updating rule of ğ¹ğ‘– :
( ğ¹ğ‘–)ğ‘ğ‘ â† ( ğ¹ğ‘–)ğ‘ğ‘ â‹…
( ğ‘‹ğ‘–( Ëœğ‘†ğ‘–ğºğ‘–)ğ‘‡ )ğ‘ğ‘ Ëœğ‘†ğ‘–ğºğ‘–( Ëœğ‘†ğ‘–ğºğ‘–)ğ‘‡ )ğ‘ğ‘
( ğ¹ğ‘–
( 11 )
Similarly we can get the following multiplicative update equations of other matrix variables in Eq ( 2 ) :
âˆš
âˆš
( ğºğ‘–ğ‘ˆ )ğ‘ğ‘ â† ( ğºğ‘–ğ‘ˆ )ğ‘ğ‘ â‹… âˆš
( ğ‘…ğ‘–)ğ‘ğ‘ â† ( ğ‘…ğ‘–)ğ‘ğ‘ â‹…
( (ğ¹ğ‘–
Ëœğ‘†ğ‘–)ğ‘‡ ğ‘‹ğ‘–ğ‘ˆ )ğ‘ğ‘
( (ğ¹ğ‘–
Ëœğ‘†ğ‘–)ğ‘‡ ğ¹ğ‘–
Ëœğ‘†ğ‘–ğºğ‘–ğ‘ˆ )ğ‘ğ‘
( ğ¹ ğ‘‡ ğ‘–ğ‘Ÿğ¹ğ‘–ğ‘ ğ‘†ğºğ‘–ğºğ‘‡
ğ‘–ğ‘Ÿğ‘‹ğ‘–ğºğ‘‡ ğ‘– +ğ¹ ğ‘‡
( ğ¹ ğ‘‡
ğ‘– )ğ‘ğ‘
ğ‘–ğ‘Ÿğ¹ğ‘–ğ‘Ÿğ‘…ğ‘–ğºğ‘–ğºğ‘‡
ğ‘– )ğ‘ğ‘
( ğ‘†)ğ‘ğ‘ â† ( ğ‘†)ğ‘ğ‘ â‹…
ğ‘‡âˆ‘
(
ğ‘–=1
ğ¹ ğ‘‡
ğ‘–ğ‘ ğ‘‹ğ‘–ğºğ‘‡
ğ‘– )ğ‘ğ‘
( ğ¹ ğ‘‡
ğ‘–ğ‘ ğ¹ğ‘–ğ‘ ğ‘†ğºğ‘–ğºğ‘‡
ğ‘– +ğ¹ ğ‘‡
ğ‘–ğ‘ ğ¹ğ‘–ğ‘Ÿğ‘…ğ‘–ğºğ‘–ğºğ‘‡
ğ‘– ))ğ‘ğ‘
ğ‘‡âˆ‘
(
ğ‘–=1
â·
For these updating rules we have the following nonincreasing theorem , which is proved in a later subsection using the auxiliary function method . Theorem 1 . The objective function ğ’ª in Eq ( 2 ) is nonincreasing under the updates of ğ¹ğ‘– , ğºğ‘–ğ‘ˆ , ğ‘…ğ‘– and ğ‘† by using Eqs . ( 11 ) , ( 12 ) , ( 13 ) , ( 14 ) for each task ğ‘– .
Finally , Theorem 2 gives an alternating iterative solution
Theorem 2 . Iteratively updating ğ¹ğ‘– , ğºğ‘–ğ‘ˆ , ğ‘…ğ‘– , to our optimization problem ( 2 ) . ğ‘– = 1,â‹…â‹…â‹… , ğ‘‡ , and ğ‘† will converge to a local minimum of ( 2 ) . Proof : Since ğ’ª obviously has the lower bound zero and is nonincreasing in each round iteration , so this iterating process will converge to a local minimum of ( 2 ) finally .
D . Learning Algorithm
Based on Theorem 2 , we develop our new multi task learning algorithm which is summarized in Algorithm 1 . To initialize the common topics ğ¹ğ‘–ğ‘  , ğ‘– = 1,â‹…â‹…â‹… , ğ‘‡ , we the data in each task and conduct PLSA6 combine all to ğ” ; All elements of on them with topic number set ğ¹ğ‘–ğ‘Ÿ , ğ‘– = 1,â‹…â‹…â‹… , ğ‘‡ , and ğ‘† are initialized to constant 1 , while the elements of ğ‘…ğ‘– , ğ‘– = 1,â‹…â‹…â‹… , ğ‘‡ are randomly initialized as real numbers in ( 0 , 1 ) . Then ğ¹ğ‘– , ğ‘– = 1,â‹…â‹…â‹… , ğ‘‡ are rownormalized while Ëœğ‘†ğ‘– , ğ‘– = 1,â‹…â‹…â‹… , ğ‘‡ are column normalized to provide better initialization . We fix ğºğ‘–ğ¿ to the true label information of ğ‘‹ğ‘–ğ¿ , and initialize ğºğ‘–ğ‘ˆ , ğ‘– = 1,â‹…â‹…â‹… , ğ‘‡ using the originally predicted category probabilities by pooling Logistic Regression .
+
+
Algorithm 1 Multi Task Semi Supervised Semantic Feature Learning for Classification ( MTS3FLC ) Input : the word document matrix ğ‘‹ğ‘– âˆˆ ğ‘…ğ‘€Ã—ğ‘ğ‘– information of ğ‘‹ğ‘–ğ¿ âˆˆ â„ task , the shared topic number ğ” , and the maximal iterating number ğ‘šğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿ . Output : classification results of unlabeled data . in each task , the true label in each task , the total topic number ğ¾ in each
ğ¶Ã—ğ‘ğ‘–ğ¿
, ğº(0 )
ğ‘(ğ‘‹ğ‘–)ğ‘ğ‘ = 1 . for ğ‘¡ = 1 to ğ‘‡ do
, ğ‘†(0 ) and ğ‘…(0 )
Update ğ‘…ğ‘– by Eq ( 13 ) .
Initialize the matrix variables as ğ¹ ( 0 ) for ğ‘–ğ‘¡ğ‘’ğ‘Ÿ = 1 to ğ‘šğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿ do
1 ) Normalize ğ‘‹ğ‘– , ğ‘– = 1 , â‹… â‹… â‹… , ğ‘‡ by columns , ie , let 2 ) 3 ) 4 ) 5 ) 6 ) 7 ) 8 ) 9 ) 10 ) 11 ) end for 12 ) For any ğ‘› = 1 , â‹… â‹… â‹… , ğ‘ğ‘–ğ‘ˆ , ğ‘– = 1 , â‹… â‹… â‹… , ğ‘‡ , classify the ğ‘› th unlabeled end for Update ğ‘† by Eq ( 14 ) . for ğ‘¡ = 1 to ğ‘‡ do
Update ğ¹ğ‘– and ğºğ‘–ğ‘ˆ by Eqs . ( 11 ) , ( 12 ) . end for
âˆ‘
.
ğ‘–
ğ‘–
ğ‘– sample in task ğ‘– , ie , ( ğ‘‹ğ‘–ğ‘ˆ )â‹…ğ‘› into category ğ‘ = arg maxğ‘(ğºğ‘–ğ‘ˆ )ğ‘ğ‘› . word number and document number is O(ğ‘€ â‹…âˆ‘ğ‘‡
For each iteration of MTS3FLC , the time complexity wrt
ğ‘–=1 ğ‘ğ‘– ) .
E . Convergence analysis
In this subsection we give a proof of Theorem 1 . The proof methods for each matrix variable are similar , so we only take ğ¹ğ‘– as example . Consider the updating of ğ¹ğ‘– when ğ¹ğ‘— , ğºğ‘– , ğ‘…ğ‘– and ğ‘† are fixed , where ğ‘– , ğ‘— = 1,â‹…â‹…â‹… , ğ‘‡ , ğ‘— âˆ•=
( 9 )
( 10 )
( 12 )
( 13 )
( 14 )
6http://wwwkybtuebingenmpgde/bs/people/pgehler/code/
14194
ğ‘– . For this problem , Eq ( 2 ) is equivalent to Eq ( 8 ) . To prove the nonincreasing property of the updating Eq ( 11 ) , we make use of the auxiliary function [ 22 ] method which can be described as follows . function of ğ’¯ ( ğ‘Œ ) if it satisfies
Definition 1 . A function ğ»(ğ‘Œ , Ëœğ‘Œ ) is called an auxiliary
ğ»(ğ‘Œ , Ëœğ‘Œ ) â‰¥ ğ’¯ ( ğ‘Œ ) , ğ»(ğ‘Œ , ğ‘Œ ) = ğ’¯ ( ğ‘Œ )
( 15 )
( 16 ) for any ğ‘Œ , Ëœğ‘Œ .
Define
ğ‘Œ ( ğ‘¡+1 ) = arg min
ğ‘Œ
ğ»(ğ‘Œ , ğ‘Œ ( ğ‘¡) ) .
Then , according to Definition 1 we have ğ’¯ ( ğ‘Œ ( ğ‘¡ ) ) = ğ»(ğ‘Œ ( ğ‘¡ ) , ğ‘Œ ( ğ‘¡ ) ) â‰¥ ğ»(ğ‘Œ ( ğ‘¡+1 ) , ğ‘Œ ( ğ‘¡ ) ) â‰¥ ğ’¯ ( ğ‘Œ ( ğ‘¡+1 ) ) ( 17 ) which means that ğ’¯ ( ğ‘Œ ) is non increasing under the update rule of Equation ( 20 ) .
Now ignoring unrelated terms in ğ·ğ‘– of Eq ( 3 ) , we write
ğ·ğ‘–(ğ¹ğ‘– ) = Tr(âˆ’2ğ‘‹ ğ‘‡
ğ‘– ğ¹ğ‘–
Ëœğ‘†ğºğ‘– + ( ğ¹ğ‘–
Ëœğ‘†ğºğ‘–)ğ‘‡ ğ¹ğ‘–
Ëœğ‘†ğºğ‘– ) ,
â€²
ğ»(ğ¹ğ‘– , ğ¹ and construct an auxiliary function of ğ·ğ‘–(ğ¹ğ‘– ) as follow : â€² ğ‘– )ğ‘ğ‘(1 + log ( ğ¹ğ‘–)ğ‘ğ‘ â€² ğ‘– )ğ‘ğ‘ ( ğ¹ 2 ğ‘– )ğ‘ğ‘ â€² ( ğ¹ ğ‘– )ğ‘ğ‘
âˆ‘ ğ‘– ) = âˆ’2 âˆ‘ +
Ëœğ‘†ğ‘‡ )ğ‘ğ‘(ğ¹
Ëœğ‘†ğ‘‡ )ğ‘ğ‘
( ğ‘‹ğ‘–ğºğ‘–
ğ‘ğ‘ ( ğ¹
Ëœğ‘†ğºğ‘‡
ğ‘– ğºğ‘–
â€² ğ‘–
( ğ¹
)
ğ‘ğ‘
â€²
â€² ğ‘– the equality ğ»(ğ¹ğ‘– , ğ¹
( 18 ) â€² Obviously , when ğ¹ğ‘– = ğ¹ ğ‘– ) = ğ·ğ‘–(ğ¹ğ‘– ) â€² holds . By comparing each term in ğ»(ğ¹ğ‘– , ğ¹ ğ‘– ) and ğ·ğ‘–(ğ¹ğ‘– ) , ğ‘– ) â‰¥ ğ·ğ‘–(ğ¹ğ‘– ) holds . we can also show the inequality ğ»(ğ¹ğ‘– , ğ¹ â€² Eg , the first term in ğ»(ğ¹ğ‘– , ğ¹ ğ‘– ) is always smaller than the first term in ğ·ğ‘–(ğ¹ğ‘– ) , because of the inequality ğ‘§ â‰¥ 1 + ğ‘™ğ‘œğ‘”(ğ‘§ ) , âˆ€ğ‘§ > 0 . The second term in ğ»(ğ¹ğ‘– , ğ¹ â€² ğ‘– ) is always bigger than the second term in ğ·ğ‘–(ğ¹ğ‘– ) , due to the following Proposition7 . ğ‘˜Ã—ğ‘˜ + , + , ğ‘†â€² âˆˆ â„ ğ‘† âˆˆ â„ ğ‘›Ã—ğ‘˜ ğ‘›Ã—ğ‘˜ + , and ğ´ , ğµ are symmetric , the following inequality holds ğ‘›âˆ‘
Proposition 1 . For any matrices ğ´ âˆˆ â„
+ , ğµ âˆˆ â„ ğ‘›Ã—ğ‘›
ğ‘˜âˆ‘
( ğ´ğ‘†â€²ğµ)ğ‘–ğ‘ğ‘†2
ğ‘–ğ‘
â‰¥ Tr(ğ‘†ğ‘‡ ğ´ğ‘†ğµ )
( 19 )
ğ‘–=1
ğ‘=1
ğ‘†â€²
ğ‘–ğ‘
Then , we minimize ğ»(ğ¹ğ‘– , ğ¹
â€² ğ‘– ) while fixing ğ¹
â€² ğ‘– . This can be achieved by letting
â€² ğ‘– )
0 = âˆ‚ğ»(ğ¹ğ‘–,ğ¹ âˆ‚(ğ¹ğ‘–)ğ‘ğ‘ = âˆ’2(ğ‘‹ğ‘–( Ëœğ‘†ğ‘–ğºğ‘–)ğ‘‡ )ğ‘ğ‘
â€² ğ‘– )ğ‘ğ‘ ( ğ¹ ( ğ¹ğ‘–)ğ‘ğ‘
+ 2(ğ¹
â€² ğ‘–
Ëœğ‘†ğ‘–ğºğ‘–( Ëœğ‘†ğ‘–ğºğ‘–)ğ‘‡ )ğ‘ğ‘
Solving for ( ğ¹ğ‘–)ğ‘ğ‘ , we get ğ‘– )ğ‘ğ‘ â‹…
( ğ¹ğ‘–)ğ‘ğ‘ = ( ğ¹
â€²
âˆš
( ğ‘‹ğ‘–( Ëœğ‘†ğ‘–ğºğ‘–)ğ‘‡ )ğ‘ğ‘ â€² Ëœğ‘†ğ‘–ğºğ‘–( Ëœğ‘†ğ‘–ğºğ‘–)ğ‘‡ )ğ‘ğ‘ ( ğ¹ ğ‘–
( ğ¹ğ‘–)ğ‘ğ‘ â€² ğ‘– )ğ‘ğ‘ ( ğ¹ ( 20 )
( 21 )
Thus , we have proved the objective function is nonincreasing under the updating rule ( 11 ) .
IV . EXPERIMENTS
In this section , our algorithm is evaluated on the widely used 20 Newsgroups data set ( Section V A will detail on how this data set can be used to construct many 4 task learning problems ) . We compare our algorithm with various baselines and three state of the art multi task learning algorithms . We also visualize the learned semantic topics and investigate the parameter effect of our algorithm .
A . Data Preparation
20 Newsgroups8 is a widely used benchmark data set for text classification , which has approximately 20,000 newsgroup documents that are partitioned evenly into twenty different newsgroups . Since some of the newsgroups are very closely related , a part of these twenty newsgroups are further grouped into four top categories , eg , the top category sci contains four subcategories sci.crypt , sci.electronics , sci.med and scispace All the four top categories and their subcategories are listed in Table I .
FOUR TOP CATEGORIES AND THEIR SUBCATEGORIES IN 20 NEWS .
Table I
Top Categories
Subcategories comp rec sci talk comp.graphics , composms windowsmisc , compsys{ibmpchardware , mac.hardware} rec.{autos , motorcycles} , recsport{baseball , hockey} talkpolitics{guns , mideast , misc} , talkreligionmisc sci.{crypt , electronics , med , space}
Now we detail on how to re organize this corpus to be used to evaluate our multi task learning algorithm . Here we focus on 2 class classification although our algorithm can naturally deal with multi class classification problem . It is easy to see that we can select any two of the four top categories sci , talk , comp and rec to construct 2 class classification data set . For the combination sci vs . talk , we construct a 2 class learning task by selecting one subcategory from the top category sci as the positive samples and one subcategory from talk as the negative samples . Since there are four subcategories in each top category , we can get an 4 task learning problem from the data set sci vs . talk by selecting subcategories in the order in Table 1 . The constructed 4 task classification problem is suitable for multi task learning due to the facts that 1 ) the data in each of the four tasks are drawn from different distributions since they are from different subcategories ; 2 ) the four tasks are related to each other since the positive ( negative ) samples in each task are from the same top categories . Similarly , we can also construct 4 task learning problems from the combinations comp vs . talk , rec vs . talk , sci vs . rec , sci vs . comp , and rec vs . comp . Therefore , we can totally get six orderly constructed 4 task learning problems from 20 Newsgroups . We can also randomly select subcategories
7Due to space limitation , we refer the reader to [ 20 ] for the proof of it .
8http://peoplecsailmitedu/jrennie/20Newsgroups/
15195 when construct 4 task classification problems from any two top categories . There are totally ğ´4 4 = 24 such randomly constructed 4 task problems for each pair of top categories . We will first present results on the six orderly constructed problems then report the average results over all randomly constructed problems . For all documents from the four top categories in 20 Newsgroups , we used ğ‘¡ğ‘“ â‹… ğ‘–ğ‘‘ğ‘“ weighting scheme to represent them and all words with document frequency less than 25 were removed resulting in a vocabulary with 8198 terms .
B . Compared Algorithms and Evaluation Metric
We compare our multi task learning algorithm MTS3FLC with the following algorithms :
âˆ™ Five baselines , ie , the single task supervised classification algorithm Logistic Regression ( STL LG ) , the single task semi supervised classification algorithm Transductive Support Vector Machine ( STL TSVM ) , the single task S3FLC introduced in this paper ( STLS3FLC ) , the supervised pooling ( Pooling LG ) and the semi supervised pooling ( Pooling TSVM ) ;
âˆ™ Multi task feature learning ( MTFL ) [ 13 ] : shares a set of common features among tasks ;
âˆ™ Group multi task feature learning ( GMTFL ) [ 1 ] : learns the task grouping structure ( with pre specified number of groups ) and encourages the tasks within each group to share features by an integer programming ;
âˆ™ Bayesian multi task learning with gaussian process priors ( BMTLGP ) [ 10 ] : assumes the latent function of each task follows a common GP prior that integrates the information coming from both the data and the tasks . Our performance evaluation metric is the averaged pre diction accuracy over all tasks .
C . Implementation Details
We adopt Minka â€™s LG package9 and SVMğ‘™ğ‘–ğ‘”â„ğ‘¡10 for the implementation of our baselines LG and TSVM .
The parameter settings of all the compared MTL algorithms follow the instructions in their original papers and are carefully tuned on our data set .
For MTS3FLC , after some preliminary investigation we find its performance is not very sensitive to the total topic number ğ¾ in each task when ğ¾ chooses values in [ 20 , 100 ] , so we set ğ¾ = 20 for all our experiments .
For single task S3FLC which is the special case of MTS3FLC when ğ” = 0 , we initialize the topic matrix ğ¹ğ‘– by the word clustering results of ğ‘‹ğ‘– by PLSA ( with topic number set to ğ¾ ) in each task , and initialize the to the constant 1/ğ¾ . elements of the relation matrix ğ‘†ğ‘– ğºğ‘–ğ‘ˆ , ğ‘– = 1,â‹…â‹…â‹… , ğ‘‡ are initialized by single task Logistic Regression since we assume we are learning each task
9http://researchmicrosoftcom/âˆ¼minka/papers/logreg/ 10http://svmlightjoachimsorg/
16196 independently . Other initializations of variables are the same as that in MTS3FLC .
The maximal iterating number is set to ğ‘šğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿ = 20 in all experiments of MTS3FLC and S3FLC as we empirically find they converge very quickly .
D . Experimental Results
1 ) Comparison on the 20 Newsgroups Data :
In this subsection the proposed algorithm MTS3FLC is compared with various baselines and three state of the art multi task learning algorithms . The shared topic number in MTS3FLC is set to ğ” = 5 for all experiments conducted here .
We first evaluate these algorithms on the six orderly constructed 4 task learning problems . For each of these six problems we vary the number of labeled samples in each task from 2 to 20 , since as well known the merits of multitask learning are more apparent when few data per task are available . For clarity , we present the comparison results between MTS3FLC and the five baselines and the results between MTS3FLC and other MTL algorithms separately . The former are shown in Figure 1 while the latter in Figure 2 , and all these results are averaged over 30 independent trials ( in each trial we randomly sample the labeled data and test on the remaining data ) . From Figure 1 we can see that owing to the exploration of unlabeled data and the elaborate knowledge sharing scheme , MTS3FLC can provide very good predictive performance , and can substantially outperform the single task learning methods STL LG , STLTSVM and STL S3FLC as well as the pooling methods Pooling LG and Pooling TSVM on all six problems .
For the results in Figure 2 , we directly applied MTFL11 and BMTLGP12 on the original 8198 dimensional bag ofwords data described in Section IV A while PCA was used to reduce the dimension to 500 ( as the author did in their paper ) for GMTFL13 on each of the six problems . The linear covariance function was used in BMTLGP ( according to the author â€™s advice ) since the high dimensional text data makes the ARD covariance function infeasible . After careful investigation , the group number and the parameter ğ›¾ in GMTFL were set to 2 and 10 respectively , while the parameter ğ›¾ in MTFL was set to 001 From the results , we can clearly see that MTS3FLC is consistently superior to all the three compared MTL algorithms on all six problems .
Then we conduct comparison experiments on all the 24 randomly constructed 4 task learning problems for each pair of top categories from 20 Newsgroups . There are totally 144 such problems since we have ğ¶ 2 4 = 6 pairs of top categories . The experimental setting for each algorithm are the same as above , but here we only vary the number of labeled samples per task among {2 , 10 , 20} because so many 4 task text classification problems are very time consuming . We
11http://tticuchicagoedu/âˆ¼argyriou/code/indexhtml/ 12http://homepagesinfedacuk/gsanguin/softwarehtml/ 13http://www scfuscedu/âˆ¼zkang/GoupMTLCodezip
1
0.9
0.8
0.7
0.6
0.5 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
0.4
2
4
1
0.9
0.8
0.7
0.6
0.5 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
0.4
2
4
MTS3FLC STL LG STL TSVM STL S3FLC Pooling LG
Pooling TSVM
6
14 Number of labeled samples in each task
10
8
20
( a ) comp vs . rec
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
0.5
2
4
1
0.9
0.8
0.7
0.6
0.5 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
6
14 Number of labeled samples in each task
10
8
20
0.4
2
4
1
0.9
0.8
0.7
0.6
0.5 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
20
0.4
2
4
6
14 Number of labeled samples in each task
10
8
6
14 Number of labeled samples in each task
10
8
( b ) comp vs . talk
( c ) sci vs . rec
1
0.9
0.8
0.7
0.6
0.5 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
20
0.4
2
4
6
14 Number of labeled samples in each task
10
8
6
14 Number of labeled samples in each task
10
8
20
20
( d ) rec vs . talk
( e ) sci vs . talk
( f ) sci vs . comp
Figure 1 . Comparison with various baselines on the six orderly constructed 4 task classification problems from 20 Newsgroups . repeat the experiment 5 times ( with labeled data randomly sampled every time ) for each problem , and compute the mean accuracy . Finally the averaged results over all 24 problems for each pair of top categories are listed in Table II , from which we see obvious advantage of MTS3FLC again .
2 ) Parameter Effect : Our preliminary experiments show that the performance of MTS3FLC is not sensitive to the total topic number ğ¾ when ğ¾ chooses values in [ 20 , 100 ] , so here we mainly investigate the shared topic number ğ” how affect the performance of MTS3FLC . To this end , we fix ğ¾ = 20 and vary ğ” in the interval [ 0 , ğ¾ ] . The averaged results over 30 repetitions on the six orderly constructed 4 task learning problems from 20 Newsgroups are shown in Table III ( for comparison we also listed the results of LG and Pooling LG ) , where we set the number of labeled samples per task to ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘ ğ‘–ğ‘§ğ‘’ = 6 for the upper three problems and ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘ ğ‘–ğ‘§ğ‘’ = 14 for the lower three problems to study whether the parameter effects of ğ” are identical under different training sizes . From Table III we can see MTS3FLC generally performs well in a wide range of ğ” , eg , on the combination sci vs . rec MTS3FLC achieves the accuracy over 95 % for arbitrary 0 < ğ” â‰¤ ğ¾ , which means that we can choose ğ” freely and boldly when we have no prior information of the task relatedness ( if we have prior information , we may use a large ğ” even equal to ğ¾ for closely related tasks , and a small ğ” even equal to zero for loosely related tasks ) . We can also see that MTS3FLC with medium large ğ” performs better than both MTS3FLC with too small ğ” and MTS3FLC with very large ğ” . Eg , MTS3FLC with ğ” = 5 generally performs well than MTS3FLC with ğ” = 3 and MTS3FLC with ğ” = 13 .
3 ) Visualization of The Learned Semantic Topics : To show the capability of MTS3FLC to automatically find the common semantics shared among tasks and specific semantics exclusive to each task , we choose 20 most probable key words to express each topic according to the word clustering information ğ¹ğ‘– , ğ‘– = 1,â‹…â‹…â‹… , ğ‘‡ . Table IV lists one common topic among the four tasks and one specific topic of each task on the orderly constructed 4 task learning problem from comp vs . rec . Here , we set the number of shared topics to ğ” = 5 and the number of labeled samples per task to ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘ ğ‘–ğ‘§ğ‘’ = 10 . From these results , it can be seen that MTS3FLC can effectively capture the common semantic topics among tasks using same or different key words about the topic , eg , Topic 1 is a common topic among tasks discussing â€œ computer â€ ( The associations between such topic and the categories are shared among tasks for multitask classification ) . Moreover , MTS3FLC can also find the specific topics of each task which are generally expressed by different key words , eg , Topic 2 in Table IV is a specific
17197
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
2
4
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
0.55
2
4
6
14 Number of labeled samples in each task
10
8
( a ) comp vs . rec
6
14 Number of labeled samples in each task
10
8
MTS3FLC BMTLGP GMTFL MTFL
20
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
0.55
2
4
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
20
0.55
2
4
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
20
0.55
2
4
6
14 Number of labeled samples in each task
10
8
6
14 Number of labeled samples in each task
10
8
( b ) comp vs . talk
( c ) sci vs . rec
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
20
0.55
2
4
6
14 Number of labeled samples in each task
10
8
6
14 Number of labeled samples in each task
10
8
20
20
( d ) rec vs . talk
( e ) sci vs . talk
( f ) sci vs . comp
Figure 2 . Comparison with state of the art MTL algorithms on the six orderly constructed 4 task classification problems from 20 Newsgroups . topic of each task which is discussing â€œ graphics â€ , â€œ bike â€ , â€œ baseball â€ and â€œ hockey â€ in each task respectively .
V . RELATED WORK
Multi task learning [ 23 ] has been the focus in the community of machine learning and data mining during the past ten years , and many algorithms on it have been proposed . In [ 13 ] , Argyriou et al . proposed a multi task feature learning method which learns a shared sparse representation for multiple related tasks . They show their ğ‘™1,2 norm based formulation is equivalent to a convex optimization problem and can be solved efficiently . Later , Zhang et al . [ 5 ] proposed a probabilistic interpretation of the general multi task feature selection problem using ğ‘™1,2 or ğ‘™1,âˆ norm . Recently , a group multi task feature learning method is proposed in [ 1 ] , which assumes the tasks exist in groups and the tasks within each group share features . Our model is related to these works since we are also learning features for each task . However , our model is more flexible to capture the semantic features that exist in many dyadic data including text , image and gene expression . Besides , usually abundant unlabeled samples in each task is naturally incorporated in our model , which makes feature learning more reliable .
Since we have used unlabeled data during the learning phase , our algorithm should be categorized into semisupervised multi task learning [ 9 ] , [ 4 ] . Though as two major directions to improve supervised learning in case of labeled samples are insufficient , semi supervised learning and multitask learning have been investigated extensively respectively , few works have combined these two lines together . While the manifold properties of unlabeled data are well exploited in [ 9 ] , our method utilizes unlabeled data to learn not only the more robust predictive structures but also the common and specific semantic features of each task .
Our model is also related to a transfer learning model named MTrick [ 21 ] , which shares the associations between categories and all semantics in source domain and target domain . Though multiple related tasks ( domains ) typically share semantics , treating all semantics as the shared ones may be not appropriate . In fact , our experiments on 20 Newsgroups data show the best number of shared semantics among tasks even less than a half of the total topic number . the idea of distinguishing common semantic topics from specific ones is also considered in a recent work called Group Matrix Factorization ( GMF ) [ 24 ] , which aims at scaling up topic modeling approaches and assumes there exist class specific topics for each text class as well as shared topics across all classes . However , GMF only simply supposed that a shared topic has the same word distribution in each class , which differs from the semantic corresponding idea considered in this paper .
Besides ,
18198
COMPARISON ON THE 144 RANDOMLY CONSTRUCTED 4 TASK CLASSIFICATION PROBLEMS FROM 20NEWSGROUPS . THE LISTED ACCURACIES ( % ) FOR EACH PAIR OF TOP CATEGORIES ARE THE MEAN PERFORMANCE ON ITS 24 CORRESPONDING PROBLEMS , WHERE THE PERFORMANCE ON EACH PROBLEM IS AVERAGED OVER 5 INDEPENDENT TRIALS . IN EACH PROBLEM , THE NUMBER OF LABELED SAMPLES PER TASK IS VARIED AMONG 2 , 10
AND 20 . P LG REPRESENTS POOLING LG , AND P TSVM REPRESENTS POOLING TSVM .
Table II trainsize
2 10 20 2 10 20 2 10 20 2 10 20 2 10 20 2 10 20
LG 70.64 86.66 91.09 70.82 87.43 91.99 65.69 82.72 88.55 68.70 84.22 89.35 65.01 80.20 85.47 64.59 78.91 84.59
TSVM S3FCL 91.22 71.43 93.25 93.77 94.58 95.26 85.92 70.70 87.75 91.05 93.49 90.66 87.87 64.60 94.04 86.80 95.28 94.58 61.73 85.06 89.54 85.10 90.24 87.78 77.87 58.12 86.11 74.74 82.04 86.37 79.28 60.67 85.61 81.25 88.77 87.98 comp vs . rec comp vs . talk sci vs . rec rec vs . talk sci vs . talk sci vs . comp
P LG P TSVM MTFL GMTFL BMTLGP MTS3FLC 73.98 87.67 90.74 78.17 91.46 94.00 64.66 78.26 84.86 69.53 83.30 88.24 65.30 79.31 83.28 65.67 77.40 82.38
95.46 96.74 96.95 93.10 96.33 96.50 90.24 95.64 96.63 92.28 95.85 96.25 86.31 92.34 93.25 86.48 88.74 90.13
67.08 89.60 92.33 68.86 85.11 88.74 51.41 60.60 80.00 59.74 66.70 71.39 58.56 64.32 68.05 53.34 68.42 76.64
66.87 89.64 92.96 70.05 91.64 94.70 60.62 80.65 87.34 64.53 83.49 89.63 58.80 76.43 84.21 61.15 77.47 84.81
64.98 82.33 88.76 64.49 81.69 89.33 62.64 78.94 86.68 66.14 80.94 86.96 62.12 77.88 83.90 61.02 75.21 82.79
68.77 88.46 93.27 69.53 88.34 94.14 65.54 84.92 91.98 67.72 86.76 92.37 64.60 82.19 87.84 64.17 81.62 87.80
PARAMETER EFFECT OF MTS3FLC ON THE SIX ORDERLY CONSTRUCTED 4 TASK LEARNING PROBLEMS FROM 20 NEWSGROUPS . THE TOTAL TOPIC NUMBER IS FIXED TO ğ¾ = 20 WHEN ğ” IS VARIED IN THE INTERVAL [ 0 , ğ¾ ] . THE NUMBER OF LABELED SAMPLES PER TASK IS SET TO ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘ ğ‘–ğ‘§ğ‘’ = 6
FOR THE UPPER THREE PROBLEMS AND ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘ ğ‘–ğ‘§ğ‘’ = 14 FOR THE LOWER THREE PROBLEMS . ALL RESULTS ( % ) ARE AVERAGED OVER 30 RUNS .
Table III comp vs . rec comp vs . talk sci vs . rec rec vs . talk sci vs . talk sci vs . comp
Average
LG 81.28 82.58 77.74 86.89 82.91 82.76 82.36
P LG 82.06 87.80 75.20 85.9 81.66 79.13 81.96
ğ”=0 92.80 91.43 91.50 91.39 85.99 85.49 89.77
ğ”=3 96.56 94.86 95.12 95.82 91.28 92.87 94.42
ğ”=5 96.59 95.60 95.22 96.10 92.06 92.86 94.74
ğ”=7 96.47 94.47 95.17 95.36 91.52 92.62 94.27
ğ”=10 96.33 94.36 95.81 94.50 91.17 92.82 94.17
ğ”=13 95.89 94.30 95.46 94.49 91.14 91.82 93.85
ğ”=16 95.65 94.10 95.04 94.57 90.05 91.43 93.47
ğ”=18 95.81 93.87 95.30 94.56 89.82 91.67 93.51
ğ”=20 95.00 92.81 95.11 94.42 89.17 90.76 92.88
VI . CONCLUSIONS AND FUTURE WORK
In this paper , we have proposed a novel multi task semantic feature learning algorithm MTS3FLC with few labeled and abundant unlabeled data in each task . The algorithm shares the associations between categories and common semantics among tasks and formulates its objective as a joint Nonnegative matrix tri factorization based optimization which can simultaneously learn the topic correspondences among tasks and predict the categories of unlabeled data . We have developed alternating iterative algorithm to optimize our objective and theoretically showed its convergence . Extensive experiments on text data demonstrate our method can provide superior generalization performance compared with various baselines and several state of the art multi task learning algorithms .
Though we only studied text classification in this paper , MTS3FLC is also applicable to many other learning problems , eg , image classification in computer vision , where we have many â€œ visual words â€ as the raw features , based on which we can also induce higher level semantic features .
For the future work , one challenging but important issue is to automatically resolve the appropriate number of shared semantics among tasks according to their relatedness . Luckily , the recent progress in nonparametric Bayesian has provided us a possible way to achieve this . Another issue that matters is to combine graph regularization with our framework to exploit the geometric structure of data . Since close samples tend to have the same label , predicting labels smoothly may further improve performance . We will report these results in a longer version of the paper .
ACKNOWLEDGMENT
This work is supported by the National Natural Science Foundation of China ( No . 61175052 , 60975039 , 61203297 , 60933004 , 61035003 ) , National High tech R&D Program of China ( 863 Program ) ( No . 2012AA011003 ) , and National Program on Key Basic Research Project ( 973 Program ) ( No . 2013CB329502 ) .
19199
THE COMMON TOPICS AMONG TASKS AND SPECIFIC TOPIC OF EACH TASK ON THE ORDERLY CONSTRUCTED 4 TASK LEARNING PROBLEM FROM comp vs . rec . TASK 1 : comp.graphics vs . rec.autos ; TASK 2 : composms windowsmisc vs . rec.motorcycles ; TASK 3 : compsysibmpchardware vs . recsportbaseball ; TASK 4 : compsysmachardware vs . recsporthockey
Table IV
Topic 1
( Common topic )
Topic 2
( Specific topic )
Task 1
Task 2
Task 3
Task 4
Task 1
Task 2
Task 3
Task 4 graphics,gif,thanks,format,files,tiff,file,image,program,bit , color,images,pcx,advance,hi,vesa,vga,xv,ftp,card windows,dos,file,os,drivers,driver,mouse,files,nt,card , printer,thanks,program,window,version,ini,ms,win,memory,apps scsi,ide,drive,controller,card,mb,dx,bus,drives,isa , pc,vlb,disk,dos,bios,mhz,system,motherboard,floppy,port mac,apple,lc,monitor,mhz,drive,centris,simms,quadra,mb , scsi,card,iisi,duo,thanks,test,video,fpu,lciii,cd graphics,thanks,please,looking,where,format,image,any,find,anyone , advance,can,mail,me,comp,hi,files,polygon,file,am bike,dod,shaft,com,ride,my,motorcycle,bmw,you,drive , bikes,riding,honda,rider,ca,dog,on,do,your,nec mattingly,he,career,njit,tesla,don,baseball,game,edu,year , games,team,his,cubs,runs,was,hit,pitcher,sox,last game,hockey,espn,team,he,games,go,nhl,bruins,was , wings,win,ca,season,detroit,play,pens,gm,players,they
REFERENCES
[ 1 ] Z . Kang , K . Grauman , and F . Sha , â€œ Learning with whom to share in multi task feature learning , â€ in ICML , 2011 .
[ 2 ] A . Torralba , K . Murphy , and W . Freeman , â€œ Sharing visual features for multiclass and multiview object detection , â€ IEEE Trans . on PAMI , vol . 29 , no . 5 , pp . 854â€“869 , 2007 .
[ 3 ] R . Ando and T . Zhang , â€œ A framework for learning predictive structures from multiple tasks and unlabeled data , â€ JMLR , vol . 6 , pp . 1817â€“1853 , 2005 .
[ 4 ] Y . Qi , O . Tastan , J . Carbonell , J . Klein Seetharaman , and J . Weston , â€œ Semi supervised multi task learning for predicting interactions between hiv 1 and human proteins , â€ Bioinformatics , vol . 26 , no . 18 , pp . i645â€“i652 , 2010 .
[ 5 ] Y . Zhang , D . Yeung , and Q . Xu , â€œ Probabilistic multi task feature selection , â€ in NIPS , vol . 23 , 2010 , pp . 2559â€“2567 .
[ 6 ] B . Bakker and T . Heskes , â€œ Task clustering and gating for bayesian multitask learning , â€ JMLR , vol . 4 , pp . 83â€“99 , 2003 .
[ 7 ] K . Yu , V . Tresp , and A . Schwaighofer , â€œ Learning gaussian processes from multiple tasks , â€ in ICML , 2005 , pp . 1012â€“ 1019 .
[ 8 ] Y . Xue , X . Liao , L . Carin , and B . Krishnapuram , â€œ Multitask learning for classification with dirichlet process priors , â€ JMLR , vol . 8 , pp . 35â€“63 , 2007 .
[ 9 ] Q . Liu , X . Liao , H . Li , J . Stack , and L . Carin , â€œ Semisupervised multitask learning , â€ IEEE Trans . on PAMI , vol . 31 , no . 6 , pp . 1074â€“1086 , 2009 .
[ 10 ] G . Skolidis and G . Sanguinetti , â€œ Bayesian multitask classification with gaussian process priors , â€ IEEE Trans . on Neural Networks , no . 99 , pp . 2011â€“2021 , 2011 .
[ 11 ] T . Jebara , â€œ Multi task feature and kernel selection for svms , â€ in ICML , 2004 .
[ 12 ] T . Evgeniou , C . Micchelli , and M . Pontil , â€œ Learning multiple tasks with kernel methods , â€ JMLR , vol . 6 , pp . 615â€“637 , 2006 .
[ 13 ] A . Argyriou , T . Evgeniou , and M . Pontil , â€œ Multi task feature learning , â€ in NIPS , 2007 .
[ 14 ] S . Deerwester , S . Dumais , G . Furnas , T . Landauer , and R . Harshman , â€œ Indexing by latent semantic analysis , â€ Journal of the American society for information science , vol . 41 , no . 6 , pp . 391â€“407 , 1990 .
[ 15 ] D . Lee , H . Seung et al . , â€œ Learning the parts of objects by nonnegative matrix factorization , â€ Nature , vol . 401 , no . 6755 , pp . 788â€“791 , 1999 .
[ 16 ] D . Blei , A . Ng , and M . Jordan , â€œ Latent dirichlet allocation , â€
JMLR , vol . 3 , pp . 993â€“1022 , 2003 .
[ 17 ] S . Chakraborti , R . Mukras , R . Lothian , N . Wiratunga , S . Watt , and D . Harper , â€œ Supervised latent semantic indexing using adaptive sprinkling , â€ in IJCAI , vol . 7 , 2007 , pp . 1582â€“1587 .
[ 18 ] D . Blei and J . McAuliffe , â€œ Supervised topic models , â€ in NIPS ,
2007 , pp . 121â€“128 .
[ 19 ] I . Rish , G . Grabarnik , G . Cecchi , F . Pereira , and G . Gordon , â€œ Closed form supervised dimensionality reduction with generalized linear models , â€ in ICML , 2008 , pp . 832â€“839 .
[ 20 ] C . Ding , T . Li , W . Peng , and H . Park , â€œ Orthogonal nonnegative matrix tri factorizations for clustering , â€ in SIGKDD . ACM , 2006 , pp . 126â€“135 .
[ 21 ] F . Zhuang , P . Luo , H . Xiong , Q . He , Y . Xiong , and Z . Shi , â€œ Exploiting associations between word clusters and document classes for cross domain text categorization ? â€ Statistical Analysis and Data Mining , vol . 4 , no . 1 , pp . 100â€“114 , 2011 .
[ 22 ] D . Lee and H . Seung , â€œ Algorithms for non negative matrix factorization , â€ in NIPS , vol . 13 , 2001 .
[ 23 ] R . Caruana , â€œ Multitask learning , â€ Machine Learning , vol . 28 , no . 1 , pp . 41â€“75 , 1997 .
[ 24 ] Q . Wang , Z . Cao , J . Xu , and H . Li , â€œ Group matrix factorization for scalable topic modeling , â€ in SIGIR . ACM , 2012 , pp . 375â€“384 .
20200
