2012 IEEE 12th International Conference on Data Mining 2012 IEEE 12th International Conference on Data Mining
Multi Task Semi Supervised Semantic Feature Learning for Classification
Changying Du
†§
† , Fuzhen Zhuang
† , Qing He and Zhongzhi Shi
†
The Key Laboratory of Intelligent Information Processing , Institute of Computing Technology
†
§
University of Chinese Academy of Sciences , Beijing 100049 , China
Chinese Academy of Sciences , Beijing 100190 , China Email : {ducy , zhuangfz , heq , shizz}@icsictaccn
Abstract—Multi task learning has proven to be useful to boost the learning of multiple related but different tasks . Meanwhile , latent semantic models such as LSA and NMF are popular and effective methods to extract discriminative semantic features of high dimensional dyadic data . In this paper , we present a method to combine these two techniques together by introducing a new matrix tri factorization based formulation for semi supervised latent semantic learning , which can incorporate labeled information into traditional unsupervised learning of latent semantics . Our inspiration for multi task semantic feature learning comes from two facts , ie , 1 ) multiple tasks generally share a set of common latent semantics ; and 2 ) a semantic usually has a stable indication of categories no matter which task it is from . Thus to make multiple tasks learn from each other we wish to share the associations between categories and those common semantics among tasks . Along this line , we propose a novel joint Nonnegative matrix tri factorization framework with the aforesaid associations shared among tasks in the form of a semantic category relation matrix . Our new formulation for multi task learning can simultaneously learn ( 1 ) discriminative semantic features of each task ; ( 2 ) predictive structure and categories of unlabeled data in each task ; ( 3 ) common semantics shared among tasks and specific semantics exclusive to each task . We give alternating iterative algorithm to optimize our objective and theoretically show its convergence . Finally extensive experiments on text data along with the comparison with various baselines and three state of the art multi task learning algorithms demonstrate the effectiveness of our method .
Keywords multi task learning ; semantic feature learning ; joint nonnegative matrix tri semi supervised learning ; factorization ; text classification ;
I . INTRODUCTION
Multi task learning ( MTL ) refers to learning multiple related tasks together so as to improve the performance of each task relative to learning them separately . Over the past decade , MTL has attracted more and more attention in the community of machine learning and data mining , and has been applied to many important areas including computer vision [ 1 ] , [ 2 ] , natural language processing [ 3 ] , and bioinformatics [ 4 ] , [ 5 ] . As well known , what is crucial in MTL is the knowledge sharing scheme among tasks . Typical approaches in the literature can be roughly categorized into two kinds . One kind are the model based approaches which can be further divided into two forms , ie , placing a common prior on the model parameters of each task in hierarchical
Bayesian models [ 6 ] , [ 7 ] , [ 8 ] , [ 9 ] , [ 10 ] and explicitly sharing some model parameters or model structure among tasks [ 3 ] . So far , model based methods have mainly focused on discriminative models such as linear regression and logistic regression . The other kind are the feature based approaches which share a set of common features among tasks in original input space or the transformed feature space [ 11 ] , [ 12 ] , [ 13 ] , [ 5 ] , [ 1 ] . To learn new features , most previous work have used a common feature transformation for all tasks on the original input data . While having the convenience to be formulated , this may be not the best choice in some problems , eg , in multi task news classification , sports news about baseball may have rather different key words from the news about hockey since different sports have different terms . In such cases , simply applying the same feature transformation for all tasks will either not help much or lose much useful information , and one may most want to extract the semantic features in each task .
On the other hand , latent semantic models such as LSA [ 14 ] , NMF [ 15 ] and LDA [ 16 ] have provided an effective way to learn the semantic features of dyadic data in unsupervised or supervised manner [ 17 ] , [ 18 ] . While unsupervised latent semantic models are popular for dimensionality reduction , their supervised counterparts are similar in spirit to supervised dimensionality reduction [ 19 ] , which aims at finding a low dimensional representation of data such that the supervision information can be well fitted by some predictive model that will be used to predict unlabeled data . However , single task supervised feature learning may be not reliable when supervision information is not enough . To improve reliability , we may combine the ideas of supervised and unsupervised semantic models and yield a semisupervised semantic learning model . Besides , by means of multi task learning , we can learn multiple such problems jointly to further improve the performance of each problem . Motivated by these aspects , in this paper , we combine multi task learning , latent semantic learning and semisupervised learning , yielding a novel multi task semantic feature learning method with few labeled and abundant unlabeled data in each task . Our main inspirations come from multi task text classification where despite of the different marginal distributions among different tasks in raw word feature space , there may be some common semantic topics
1550 4786/12 $26.00 © 2012 IEEE 1550 4786/12 $26.00 © 2012 IEEE DOI 101109/ICDM201215 DOI 101109/ICDM201215
11 191 shared among tasks , and the associations between those common semantic topics and text categories may remain stable across different tasks . With these stable semanticcategory associations being exploited for the knowledge sharing among tasks1 , multiple tasks may learn from each other . To formulate our learning framework , first we provide a new matrix tri factorization based formulation for semisupervised latent semantic learning , which can incorporate labeled information into the traditional unsupervised learning of latent semantics and can combine the finding of discriminative semantic features , the learning of predictive structure and the predicting of unlabeled data in a unified optimization framework . Then we extend this single task formulation to the multi task scenario , by sharing the associations between categories and those common semantics among tasks in the form of a semantic category relation matrix . Our new formulation for multi task semantic feature learning essentially is a joint Nonnegative matrix trifactorization based optimization which can simultaneously learn ( 1 ) discriminative semantic features of each task ; ( 2 ) predictive structure and categories of unlabeled data in each task ; ( 3 ) common semantics shared among tasks and specific semantics exclusive to each task .
Compared with typical model based approaches that either directly share some model parameters or place a common prior on the parameters , our method not only shares a part of model parameters but also automatically learns the correspondingly shared features that may have different forms in different tasks .
There also exist key differences between our method and typical multi task feature selection/learning ( MTFS/L ) . First , typical MTFS only seeks to find a set of common features in the original input space [ 13 ] , [ 5 ] , while our method finds common features in the more meaningful semantic space ; second , typical MTFL [ 13 ] , [ 1 ] enforces a common transformation for all tasks on the original input data , while our method allows the common semantic features learned among tasks to have different forms in terms of the multinomial distributions over words ; third , most previous studies on MTFS/L mainly use the labeled data , while our method can naturally incorporate usually abundant unlabeled samples in each task for semantic feature learning .
We give alternating iterative algorithm for the optimization of our objective and theoretically show its convergence . Then we construct 144 4 task learning problems from the widely used 20 Newsgroups data set to evaluate our algorithm . Experimental results demonstrate our method can provide superior generalization performance compared with various baselines and several state of the art multi task learning algorithms .
The remainder is organized as follows . Section II states
1These associations shared among tasks essentially are a part of model parameters in the latent semantic space of each task , but here the model is not discriminative , thus differs from typical model based MTL methods . the notations and briefly covers the necessary preliminaries . Then in Section III we propose our multi task semantic feature learning algorithm by first introducing a matrix trifactorization based formulation for semi supervised latent semantic learning . The experimental results are demonstrated in Section IV and related works are given in Section V . Finally we conclude the paper in Section VI .
II . PROBLEM SPECIFICATION AND PRELIMINARIES
A . Problem Specification and Notations Assume we have 𝑇 classification learning tasks indexed by 𝑖 = 1,⋅⋅⋅ , 𝑇 , each of which with few labeled data 𝒟𝑖𝐿 = {(x𝑖,𝑛 , 𝑦𝑖,𝑛)}𝑁𝑖𝐿 𝑛=1 but sufficient unlabeled data 𝒟𝑖𝑈 = {x𝑖,𝑛}𝑁𝑖𝑈 𝑛=1 . Our objective is to learn these 𝑇 tasks together with the hope that they will help each other in the learning process . The learning performance is measured by the averaged classification accuracy over all tasks , and a multi task learning algorithm is evaluated by its performance improvements over single task learning ( learning each task independently ) and pooling ( simply combine all labeled and unlabeled data in each task together ) . We denote the set of real numbers and the set of nonnegative real numbers by ℝ and ℝ+ respectively . 𝑋𝑖 ∈ ℝ 𝑀×𝑁𝑖 + is the word document matrix2 in the 𝑖 th task , with 𝑁𝑖 = 𝑁𝑖𝐿 + 𝑁𝑖𝑈 being the number of total documents in the 𝑖 th task and 𝑀 being the number of vocabulary words . 𝐾 is the number of total semantic topics in each task , while 𝔎 is the number of shared common topics among tasks . 𝐶 denotes the number of data categories in each task . 𝑋𝑎𝑏 indicates the 𝑎 th row and the 𝑏 th column element of matrix 𝑋 , while ∥ ⋅ ∥ and Tr(⋅ ) denote the Frobenius norm and the trace of a matrix respectively .
B . Nonnegative Matrix Factorization ( NMF )
As shown in [ 15 ] , NMF can learn semantic features of text . It simply aims to factorize a data matrix 𝑋 into two nonnegative factor matrices 𝐹 and 𝐺 :
𝑋 ≈ 𝐹 𝐺 , 𝐹 ≥ 0 , 𝐺 ≥ 0 , where 𝑋 ∈ ℝ 𝑀×𝑁 𝑀 features , 𝐹 ∈ ℝ + 𝐺 ∈ ℝ semantic space spanned by the columns of 𝐹 . contains 𝑁 data samples in terms of represents 𝐾 latent semantics and contains the coordinates of each sample in the
𝑀×𝐾 +
𝐾×𝑁 +
The nonnegativity constraints makes the learned semantics in NMF more interpretable than that in LSA which uses the Singular Value Decomposition ( SVD ) technique .
As two factor NMF is restrictive sometimes , Ding et al . studied Nonnegative Matrix Tri Factorization ( NMTF ) [ 20 ] :
𝑋 ≈ 𝐹 𝑆𝐺 , 𝐹 ≥ 0 , 𝑆 ≥ 0 , 𝐺 ≥ 0 .
NMTF can have many interpretations . In this paper , we will endow 𝐺 with the label information of data , and the
2In the sequel , we only take text data as example though our method can address any dyadic data .
12192 labels of training data will be fixed during the optimization . Accordingly , we endow 𝑆 with the semantic category relations , which can be properly shared among tasks .
III . MULTI TASK SEMI SUPERVISED SEMANTIC
FEATURE LEARNING FOR CLASSIFICATION
A . Semi Supervised Semantic Feature Learning for Classification with NMTF
Let us begin with a semi supervised classification setting of each learning task , ie , there are a portion of labeled data and large amount of unlabeled data in each task . We formulate this problem for the 𝑖 th task as follows : min
𝐹𝑖,𝑆𝑖,𝐺𝑖≥0
∥𝑋𝑖 − 𝐹𝑖𝑆𝑖[𝐺𝑖𝐿 𝐺𝑖𝑈 ]∥2
( 1 ) where
∙ 𝑋𝑖 = [ 𝑋𝑖𝐿 𝑋𝑖𝑈 ] ∈ ℝ matrix in the 𝑖 th task , with 𝑋𝑖𝐿 ∈ ℝ labeled data and 𝑋𝑖𝑈 ∈ ℝ is the word document being the the unlabeled data ; contains the latent semantic topics in the
𝑀×𝑁𝑖 + 𝑀×𝑁𝑖𝑈 +
𝑀×𝑁𝑖𝐿 +
𝑖 th task , which can be seen as semantic features ;
𝑀×𝐾 + 𝐾×𝐶 +
∙ 𝐹𝑖 ∈ ℝ ∙ 𝑆𝑖 ∈ ℝ ∙ 𝐺𝑖 = [ 𝐺𝑖𝐿 𝐺𝑖𝑈 ] ∈ ℝ the 𝑖 th task ; is the semantic category relation matrix in
𝐶×𝑁𝑖 + contains the category information of data matrix 𝑋𝑖 . Those supervision information is injected into 𝐺𝑖𝐿 3 and fixed during the optimization . 𝐺𝑖𝑈 contains the predicted category information of unlabeled data 𝑋𝑖𝑈 4 , which can be initialized by a supervised learner trained on those few labeled data ;
For sake of brevity , we refer this formulation as SemiSupervised semantic Feature Learning for Classification ( S3FLC ) in the sequel . Note that S3FLC can not only incorporate labeled information into the traditional unsupervised learning of latent semantics , but also combine the finding of discriminative latent semantics , the learning of predictive structure and the predicting of unlabeled data in a unified optimization framework . Zhuang et al . [ 21 ] achieved fairly good transfer learning results by using a special case of this formulation to model the fully labeled source domain data .
B . The Proposed Multi Task Learning Formulation is from , so we may split
As stated in the introduction , multiple related tasks generally shares a set of common semantics , and a semantic usually has a stable meaning of categories no matter which task it the semantic category relation matrix of each task into two parts : one part is for those common semantics and will be shared across tasks , while the other part corresponds to the specific topics of each task . Along this line of thoughts , we jointly factorize
3If the 𝑛 th sample in 𝑋𝑖𝐿 belongs to category 𝑐 , then ( 𝐺𝑖𝐿)𝑐𝑛 = 1 , and for any 𝑐′ ∈ {1 , ⋅ ⋅ ⋅ , 𝐶} , 𝑐′ ∕= 𝑐 , ( 𝐺𝑖𝐿)𝑐′𝑛 = 0 .
4The 𝑛 th sample in 𝑋𝑖𝑈 belongs to category 𝑐 = arg max𝑏(𝐺𝑖𝑈 )𝑏𝑛 .
𝑇 word document matrices 𝑋𝑖 , 𝑖 = 1,⋅⋅⋅ , 𝑇 as follows , with the hope that this will pass knowledge via the common semantic category associations ( contained in 𝑆 ) from some “ confident ” tasks to those unconfident ones :
[ 𝐺𝑖𝐿 𝐺𝑖𝑈 ]∥2
( 2 ) min
𝐹𝑖,𝑆,𝑅𝑖,𝐺𝑖≥0
∥𝑋𝑖 − [ 𝐹𝑖𝑠 𝐹𝑖𝑟 ]
𝑇∑
𝑖=1
[
]
𝑆 𝑅𝑖 where
𝑀×𝔎 +
𝑀×𝐾 +
∙ 𝑋𝑖 , and 𝐺𝑖 = [ 𝐺𝑖𝐿 𝐺𝑖𝑈 ] have the same meanings as ∙ 𝐹𝑖 = [ 𝐹𝑖𝑠 𝐹𝑖𝑟 ] ∈ ℝ those in the single task S3FLC introduced above ; contains the semantic topics in the 𝑖 th task , with 𝐹𝑖𝑠 ∈ ℝ , 𝑖 = 1,⋅⋅⋅ , 𝑇 containing the common semantic topics among tasks and 𝐹𝑖𝑟 ∈ ℝ 𝑀×(𝐾−𝔎 ) containing the specific topics of + the 𝑖 th task . 𝐾×𝐶 is the semantic category relation matrix with 𝑆 ∈ ℝ + containing the shared semantic category associations among tasks and 𝑅𝑖 ∈ ( 𝐾−𝔎)×𝐶 containing associations between the 𝑖 th +
∙ ˜𝑆𝑖 = [ 𝑆 𝑅𝑖]𝑇 ∈ ℝ
ℝ task ’s specific topics and text categories ;
𝔎×𝐶 +
Note that , ( a ) common semantic topics among tasks mean that for any 1 ≤ 𝑖 , 𝑗 ≤ 𝑇 , 𝑖 ∕= 𝑗 , 𝐹𝑖𝑠 and 𝐹𝑗𝑠 are semanticly corresponding to each other but there is no need to satisfy 𝐹𝑖𝑠 = 𝐹𝑗𝑠 , which is to say , several corresponding topics have the same semantic relations to text categories , but they may have different forms in terms of the multinomial topic distributions over words ; ( b ) the number 𝔎 of shared topics can be adjusted to accommodate to the relatedness of tasks . For closely related tasks we may use a large 𝔎 even equal to the total topics 𝐾 , and for loosely related tasks we may use a small 𝔎 even equal to zero which is equivalent to no knowledge being shared ; ( c ) we can employ a supervised learner ie , Logistic Regression to pre predict those unlabeled data and initialize 𝐺𝑖𝑈 using these originally predicted probabilities . This pre predicting process can be done either in the pooling manner or in single task learning manner ; ( d ) since we have learned 𝐹𝑖 and ˜𝑆𝑖 during this transductive process , new data that not in the original factorization of 𝑋𝑖 can also be predicted by factorizing them according to ( 2 ) with 𝐹𝑖 and ˜𝑆𝑖 initialized to the original factorization results .
In what follows , we detail on how to solve the joint optimization problem in Eq ( 2 ) .
C . Optimization
The objective function 𝒪 in Eq ( 2 ) is not convex in all its variables , thus , it is unrealistic to find the global minima for it . In the following , we provide an alternating iterative algorithm to obtain the local optima of ( 2 ) . ˜𝑆𝑖𝐺𝑖∥2 , 𝑖 = 1,⋅⋅⋅ , 𝑇 , then after some
Let 𝐷𝑖 = ∥𝑋𝑖 − 𝐹𝑖 simple algebra we have5
5Here we used the properties Tr(𝐴 + 𝐵 ) = Tr(𝐴 ) + Tr(𝐵 ) and
Tr(𝐴𝐵 ) = Tr(𝐵𝐴 ) .
13193
𝐷𝑖 = Tr((𝑋𝑖 − 𝐹𝑖
˜𝑆𝑖𝐺𝑖)𝑇 ( 𝑋𝑖 − 𝐹𝑖
˜𝑆𝑖𝐺𝑖 ) )
= Tr(𝑋 𝑇
= Tr(𝑋 𝑇
𝑖 𝑋𝑖 − 2𝑋 𝑇 𝑖 𝑋𝑖 − 2𝑋 𝑇
˜𝑆𝑖𝐺𝑖 + ( 𝐹𝑖 𝑖 𝐹𝑖 𝑖 𝐹𝑖𝑠𝑆𝐺𝑖 − 2𝑋 𝑇
˜𝑆𝑖𝐺𝑖)𝑇 𝐹𝑖 𝑖 𝐹𝑖𝑟𝑅𝑖𝐺𝑖
˜𝑆𝑖𝐺𝑖 )
+(𝐹𝑖𝑠𝑆𝐺𝑖)𝑇 𝐹𝑖𝑠𝑆𝐺𝑖 + 2(𝐹𝑖𝑠𝑆𝐺𝑖)𝑇 𝐹𝑖𝑟𝑅𝑖𝐺𝑖 +(𝐹𝑖𝑟𝑅𝑖𝐺𝑖)𝑇 𝐹𝑖𝑟𝑅𝑖𝐺𝑖
( 3 ) The partial derivatives of 𝐷𝑖 are ( we assume 𝐺𝑖𝐿 is fixed , and the matrix variables are 𝐹𝑖 , 𝐺𝑖𝑈 , 𝑅𝑖 and 𝑆 ) ˜𝑆𝑖𝐺𝑖( ˜𝑆𝑖𝐺𝑖)𝑇 − 2𝑋𝑖( ˜𝑆𝑖𝐺𝑖)𝑇
= 2𝐹𝑖
( 4 )
∂𝐷𝑖 ∂𝐹𝑖
= 2(𝐹𝑖
˜𝑆𝑖)𝑇 𝐹𝑖
˜𝑆𝑖𝐺𝑖𝑈 − 2(𝐹𝑖
∂𝐷𝑖 ∂𝐺𝑖𝑈
∂𝐷𝑖 ∂𝑅𝑖
= 2𝐹 𝑇
𝑖𝑟 𝐹𝑖𝑠𝑆𝐺𝑖𝐺𝑇
𝑖 + 2𝐹 𝑇
𝑖𝑟 𝐹𝑖𝑟𝑅𝑖𝐺𝑖𝐺𝑇
( 5 )
𝑖𝑟 𝑋𝑖𝐺𝑇 𝑖 ( 6 )
˜𝑆𝑖)𝑇 𝑋𝑖𝑈 𝑖 − 2𝐹 𝑇 𝑖 − 2𝐹 𝑇
𝑖 + 2𝐹 𝑇
∂𝐷𝑖 ∂𝑆 = 2𝐹 𝑇
𝑖𝑠 𝐹𝑖𝑠𝑆𝐺𝑖𝐺𝑇
𝑖𝑠 𝐹𝑖𝑟𝑅𝑖𝐺𝑖𝐺𝑇
𝑖𝑠 𝑋𝑖𝐺𝑇 𝑖 ( 7 ) Now we consider the updating of 𝐹𝑖 when 𝐹𝑗 , 𝐺𝑖 , 𝑅𝑖 and 𝑆 are fixed , where 𝑖 , 𝑗 = 1,⋅⋅⋅ , 𝑇 , 𝑗 ∕= 𝑖 . For this problem , Eq ( 2 ) is equivalent to the following optimization :
∥𝑋𝑖 − 𝐹𝑖
˜𝑆𝑖𝐺𝑖∥2 min 𝐹𝑖≥0
( 8 )
We can solve this constrained quadratic programming problem in many ways , eg , the Reduced Gradient method and the Interior Point method . However , we find these methods either are complex to implement or converge very slow . Here we make use of the multiplicative updating method [ 22 ] , [ 20 ] which is free from selecting the step size that is needed in gradient based methods . By using the KKT complementarity condition of ( 8 ) and note ( 4) ) , we have
˜𝑆𝑖𝐺𝑖( ˜𝑆𝑖𝐺𝑖)𝑇 − 2𝑋𝑖( ˜𝑆𝑖𝐺𝑖)𝑇 )𝑎𝑏(𝐹𝑖)𝑎𝑏 = 0
( 2𝐹𝑖 which is equivalent to
˜𝑆𝑖𝐺𝑖( ˜𝑆𝑖𝐺𝑖)𝑇 − 2𝑋𝑖( ˜𝑆𝑖𝐺𝑖)𝑇 )𝑎𝑏(𝐹𝑖)2
( 2𝐹𝑖
𝑎𝑏 = 0
Solving this equation leads to the following updating rule of 𝐹𝑖 :
( 𝐹𝑖)𝑎𝑏 ← ( 𝐹𝑖)𝑎𝑏 ⋅
( 𝑋𝑖( ˜𝑆𝑖𝐺𝑖)𝑇 )𝑎𝑏 ˜𝑆𝑖𝐺𝑖( ˜𝑆𝑖𝐺𝑖)𝑇 )𝑎𝑏
( 𝐹𝑖
( 11 )
Similarly we can get the following multiplicative update equations of other matrix variables in Eq ( 2 ) :
√
√
( 𝐺𝑖𝑈 )𝑎𝑏 ← ( 𝐺𝑖𝑈 )𝑎𝑏 ⋅ √
( 𝑅𝑖)𝑎𝑏 ← ( 𝑅𝑖)𝑎𝑏 ⋅
( (𝐹𝑖
˜𝑆𝑖)𝑇 𝑋𝑖𝑈 )𝑎𝑏
( (𝐹𝑖
˜𝑆𝑖)𝑇 𝐹𝑖
˜𝑆𝑖𝐺𝑖𝑈 )𝑎𝑏
( 𝐹 𝑇 𝑖𝑟𝐹𝑖𝑠𝑆𝐺𝑖𝐺𝑇
𝑖𝑟𝑋𝑖𝐺𝑇 𝑖 +𝐹 𝑇
( 𝐹 𝑇
𝑖 )𝑎𝑏
𝑖𝑟𝐹𝑖𝑟𝑅𝑖𝐺𝑖𝐺𝑇
𝑖 )𝑎𝑏
( 𝑆)𝑎𝑏 ← ( 𝑆)𝑎𝑏 ⋅
𝑇∑
(
𝑖=1
𝐹 𝑇
𝑖𝑠𝑋𝑖𝐺𝑇
𝑖 )𝑎𝑏
( 𝐹 𝑇
𝑖𝑠𝐹𝑖𝑠𝑆𝐺𝑖𝐺𝑇
𝑖 +𝐹 𝑇
𝑖𝑠𝐹𝑖𝑟𝑅𝑖𝐺𝑖𝐺𝑇
𝑖 ))𝑎𝑏
𝑇∑
(
𝑖=1
⎷
For these updating rules we have the following nonincreasing theorem , which is proved in a later subsection using the auxiliary function method . Theorem 1 . The objective function 𝒪 in Eq ( 2 ) is nonincreasing under the updates of 𝐹𝑖 , 𝐺𝑖𝑈 , 𝑅𝑖 and 𝑆 by using Eqs . ( 11 ) , ( 12 ) , ( 13 ) , ( 14 ) for each task 𝑖 .
Finally , Theorem 2 gives an alternating iterative solution
Theorem 2 . Iteratively updating 𝐹𝑖 , 𝐺𝑖𝑈 , 𝑅𝑖 , to our optimization problem ( 2 ) . 𝑖 = 1,⋅⋅⋅ , 𝑇 , and 𝑆 will converge to a local minimum of ( 2 ) . Proof : Since 𝒪 obviously has the lower bound zero and is nonincreasing in each round iteration , so this iterating process will converge to a local minimum of ( 2 ) finally .
D . Learning Algorithm
Based on Theorem 2 , we develop our new multi task learning algorithm which is summarized in Algorithm 1 . To initialize the common topics 𝐹𝑖𝑠 , 𝑖 = 1,⋅⋅⋅ , 𝑇 , we the data in each task and conduct PLSA6 combine all to 𝔎 ; All elements of on them with topic number set 𝐹𝑖𝑟 , 𝑖 = 1,⋅⋅⋅ , 𝑇 , and 𝑆 are initialized to constant 1 , while the elements of 𝑅𝑖 , 𝑖 = 1,⋅⋅⋅ , 𝑇 are randomly initialized as real numbers in ( 0 , 1 ) . Then 𝐹𝑖 , 𝑖 = 1,⋅⋅⋅ , 𝑇 are rownormalized while ˜𝑆𝑖 , 𝑖 = 1,⋅⋅⋅ , 𝑇 are column normalized to provide better initialization . We fix 𝐺𝑖𝐿 to the true label information of 𝑋𝑖𝐿 , and initialize 𝐺𝑖𝑈 , 𝑖 = 1,⋅⋅⋅ , 𝑇 using the originally predicted category probabilities by pooling Logistic Regression .
+
+
Algorithm 1 Multi Task Semi Supervised Semantic Feature Learning for Classification ( MTS3FLC ) Input : the word document matrix 𝑋𝑖 ∈ 𝑅𝑀×𝑁𝑖 information of 𝑋𝑖𝐿 ∈ ℝ task , the shared topic number 𝔎 , and the maximal iterating number 𝑚𝑎𝑥𝐼𝑡𝑒𝑟 . Output : classification results of unlabeled data . in each task , the true label in each task , the total topic number 𝐾 in each
𝐶×𝑁𝑖𝐿
, 𝐺(0 )
𝑎(𝑋𝑖)𝑎𝑏 = 1 . for 𝑡 = 1 to 𝑇 do
, 𝑆(0 ) and 𝑅(0 )
Update 𝑅𝑖 by Eq ( 13 ) .
Initialize the matrix variables as 𝐹 ( 0 ) for 𝑖𝑡𝑒𝑟 = 1 to 𝑚𝑎𝑥𝐼𝑡𝑒𝑟 do
1 ) Normalize 𝑋𝑖 , 𝑖 = 1 , ⋅ ⋅ ⋅ , 𝑇 by columns , ie , let 2 ) 3 ) 4 ) 5 ) 6 ) 7 ) 8 ) 9 ) 10 ) 11 ) end for 12 ) For any 𝑛 = 1 , ⋅ ⋅ ⋅ , 𝑁𝑖𝑈 , 𝑖 = 1 , ⋅ ⋅ ⋅ , 𝑇 , classify the 𝑛 th unlabeled end for Update 𝑆 by Eq ( 14 ) . for 𝑡 = 1 to 𝑇 do
Update 𝐹𝑖 and 𝐺𝑖𝑈 by Eqs . ( 11 ) , ( 12 ) . end for
∑
.
𝑖
𝑖
𝑖 sample in task 𝑖 , ie , ( 𝑋𝑖𝑈 )⋅𝑛 into category 𝑐 = arg max𝑏(𝐺𝑖𝑈 )𝑏𝑛 . word number and document number is O(𝑀 ⋅∑𝑇
For each iteration of MTS3FLC , the time complexity wrt
𝑖=1 𝑁𝑖 ) .
E . Convergence analysis
In this subsection we give a proof of Theorem 1 . The proof methods for each matrix variable are similar , so we only take 𝐹𝑖 as example . Consider the updating of 𝐹𝑖 when 𝐹𝑗 , 𝐺𝑖 , 𝑅𝑖 and 𝑆 are fixed , where 𝑖 , 𝑗 = 1,⋅⋅⋅ , 𝑇 , 𝑗 ∕=
( 9 )
( 10 )
( 12 )
( 13 )
( 14 )
6http://wwwkybtuebingenmpgde/bs/people/pgehler/code/
14194
𝑖 . For this problem , Eq ( 2 ) is equivalent to Eq ( 8 ) . To prove the nonincreasing property of the updating Eq ( 11 ) , we make use of the auxiliary function [ 22 ] method which can be described as follows . function of 𝒯 ( 𝑌 ) if it satisfies
Definition 1 . A function 𝐻(𝑌 , ˜𝑌 ) is called an auxiliary
𝐻(𝑌 , ˜𝑌 ) ≥ 𝒯 ( 𝑌 ) , 𝐻(𝑌 , 𝑌 ) = 𝒯 ( 𝑌 )
( 15 )
( 16 ) for any 𝑌 , ˜𝑌 .
Define
𝑌 ( 𝑡+1 ) = arg min
𝑌
𝐻(𝑌 , 𝑌 ( 𝑡) ) .
Then , according to Definition 1 we have 𝒯 ( 𝑌 ( 𝑡 ) ) = 𝐻(𝑌 ( 𝑡 ) , 𝑌 ( 𝑡 ) ) ≥ 𝐻(𝑌 ( 𝑡+1 ) , 𝑌 ( 𝑡 ) ) ≥ 𝒯 ( 𝑌 ( 𝑡+1 ) ) ( 17 ) which means that 𝒯 ( 𝑌 ) is non increasing under the update rule of Equation ( 20 ) .
Now ignoring unrelated terms in 𝐷𝑖 of Eq ( 3 ) , we write
𝐷𝑖(𝐹𝑖 ) = Tr(−2𝑋 𝑇
𝑖 𝐹𝑖
˜𝑆𝐺𝑖 + ( 𝐹𝑖
˜𝑆𝐺𝑖)𝑇 𝐹𝑖
˜𝑆𝐺𝑖 ) ,
′
𝐻(𝐹𝑖 , 𝐹 and construct an auxiliary function of 𝐷𝑖(𝐹𝑖 ) as follow : ′ 𝑖 )𝑎𝑏(1 + log ( 𝐹𝑖)𝑎𝑏 ′ 𝑖 )𝑎𝑏 ( 𝐹 2 𝑖 )𝑎𝑏 ′ ( 𝐹 𝑖 )𝑎𝑏
∑ 𝑖 ) = −2 ∑ +
˜𝑆𝑇 )𝑎𝑏(𝐹
˜𝑆𝑇 )𝑎𝑏
( 𝑋𝑖𝐺𝑖
𝑎𝑏 ( 𝐹
˜𝑆𝐺𝑇
𝑖 𝐺𝑖
′ 𝑖
( 𝐹
)
𝑎𝑏
′
′ 𝑖 the equality 𝐻(𝐹𝑖 , 𝐹
( 18 ) ′ Obviously , when 𝐹𝑖 = 𝐹 𝑖 ) = 𝐷𝑖(𝐹𝑖 ) ′ holds . By comparing each term in 𝐻(𝐹𝑖 , 𝐹 𝑖 ) and 𝐷𝑖(𝐹𝑖 ) , 𝑖 ) ≥ 𝐷𝑖(𝐹𝑖 ) holds . we can also show the inequality 𝐻(𝐹𝑖 , 𝐹 ′ Eg , the first term in 𝐻(𝐹𝑖 , 𝐹 𝑖 ) is always smaller than the first term in 𝐷𝑖(𝐹𝑖 ) , because of the inequality 𝑧 ≥ 1 + 𝑙𝑜𝑔(𝑧 ) , ∀𝑧 > 0 . The second term in 𝐻(𝐹𝑖 , 𝐹 ′ 𝑖 ) is always bigger than the second term in 𝐷𝑖(𝐹𝑖 ) , due to the following Proposition7 . 𝑘×𝑘 + , + , 𝑆′ ∈ ℝ 𝑆 ∈ ℝ 𝑛×𝑘 𝑛×𝑘 + , and 𝐴 , 𝐵 are symmetric , the following inequality holds 𝑛∑
Proposition 1 . For any matrices 𝐴 ∈ ℝ
+ , 𝐵 ∈ ℝ 𝑛×𝑛
𝑘∑
( 𝐴𝑆′𝐵)𝑖𝑝𝑆2
𝑖𝑝
≥ Tr(𝑆𝑇 𝐴𝑆𝐵 )
( 19 )
𝑖=1
𝑝=1
𝑆′
𝑖𝑝
Then , we minimize 𝐻(𝐹𝑖 , 𝐹
′ 𝑖 ) while fixing 𝐹
′ 𝑖 . This can be achieved by letting
′ 𝑖 )
0 = ∂𝐻(𝐹𝑖,𝐹 ∂(𝐹𝑖)𝑎𝑏 = −2(𝑋𝑖( ˜𝑆𝑖𝐺𝑖)𝑇 )𝑎𝑏
′ 𝑖 )𝑎𝑏 ( 𝐹 ( 𝐹𝑖)𝑎𝑏
+ 2(𝐹
′ 𝑖
˜𝑆𝑖𝐺𝑖( ˜𝑆𝑖𝐺𝑖)𝑇 )𝑎𝑏
Solving for ( 𝐹𝑖)𝑎𝑏 , we get 𝑖 )𝑎𝑏 ⋅
( 𝐹𝑖)𝑎𝑏 = ( 𝐹
′
√
( 𝑋𝑖( ˜𝑆𝑖𝐺𝑖)𝑇 )𝑎𝑏 ′ ˜𝑆𝑖𝐺𝑖( ˜𝑆𝑖𝐺𝑖)𝑇 )𝑎𝑏 ( 𝐹 𝑖
( 𝐹𝑖)𝑎𝑏 ′ 𝑖 )𝑎𝑏 ( 𝐹 ( 20 )
( 21 )
Thus , we have proved the objective function is nonincreasing under the updating rule ( 11 ) .
IV . EXPERIMENTS
In this section , our algorithm is evaluated on the widely used 20 Newsgroups data set ( Section V A will detail on how this data set can be used to construct many 4 task learning problems ) . We compare our algorithm with various baselines and three state of the art multi task learning algorithms . We also visualize the learned semantic topics and investigate the parameter effect of our algorithm .
A . Data Preparation
20 Newsgroups8 is a widely used benchmark data set for text classification , which has approximately 20,000 newsgroup documents that are partitioned evenly into twenty different newsgroups . Since some of the newsgroups are very closely related , a part of these twenty newsgroups are further grouped into four top categories , eg , the top category sci contains four subcategories sci.crypt , sci.electronics , sci.med and scispace All the four top categories and their subcategories are listed in Table I .
FOUR TOP CATEGORIES AND THEIR SUBCATEGORIES IN 20 NEWS .
Table I
Top Categories
Subcategories comp rec sci talk comp.graphics , composms windowsmisc , compsys{ibmpchardware , mac.hardware} rec.{autos , motorcycles} , recsport{baseball , hockey} talkpolitics{guns , mideast , misc} , talkreligionmisc sci.{crypt , electronics , med , space}
Now we detail on how to re organize this corpus to be used to evaluate our multi task learning algorithm . Here we focus on 2 class classification although our algorithm can naturally deal with multi class classification problem . It is easy to see that we can select any two of the four top categories sci , talk , comp and rec to construct 2 class classification data set . For the combination sci vs . talk , we construct a 2 class learning task by selecting one subcategory from the top category sci as the positive samples and one subcategory from talk as the negative samples . Since there are four subcategories in each top category , we can get an 4 task learning problem from the data set sci vs . talk by selecting subcategories in the order in Table 1 . The constructed 4 task classification problem is suitable for multi task learning due to the facts that 1 ) the data in each of the four tasks are drawn from different distributions since they are from different subcategories ; 2 ) the four tasks are related to each other since the positive ( negative ) samples in each task are from the same top categories . Similarly , we can also construct 4 task learning problems from the combinations comp vs . talk , rec vs . talk , sci vs . rec , sci vs . comp , and rec vs . comp . Therefore , we can totally get six orderly constructed 4 task learning problems from 20 Newsgroups . We can also randomly select subcategories
7Due to space limitation , we refer the reader to [ 20 ] for the proof of it .
8http://peoplecsailmitedu/jrennie/20Newsgroups/
15195 when construct 4 task classification problems from any two top categories . There are totally 𝐴4 4 = 24 such randomly constructed 4 task problems for each pair of top categories . We will first present results on the six orderly constructed problems then report the average results over all randomly constructed problems . For all documents from the four top categories in 20 Newsgroups , we used 𝑡𝑓 ⋅ 𝑖𝑑𝑓 weighting scheme to represent them and all words with document frequency less than 25 were removed resulting in a vocabulary with 8198 terms .
B . Compared Algorithms and Evaluation Metric
We compare our multi task learning algorithm MTS3FLC with the following algorithms :
∙ Five baselines , ie , the single task supervised classification algorithm Logistic Regression ( STL LG ) , the single task semi supervised classification algorithm Transductive Support Vector Machine ( STL TSVM ) , the single task S3FLC introduced in this paper ( STLS3FLC ) , the supervised pooling ( Pooling LG ) and the semi supervised pooling ( Pooling TSVM ) ;
∙ Multi task feature learning ( MTFL ) [ 13 ] : shares a set of common features among tasks ;
∙ Group multi task feature learning ( GMTFL ) [ 1 ] : learns the task grouping structure ( with pre specified number of groups ) and encourages the tasks within each group to share features by an integer programming ;
∙ Bayesian multi task learning with gaussian process priors ( BMTLGP ) [ 10 ] : assumes the latent function of each task follows a common GP prior that integrates the information coming from both the data and the tasks . Our performance evaluation metric is the averaged pre diction accuracy over all tasks .
C . Implementation Details
We adopt Minka ’s LG package9 and SVM𝑙𝑖𝑔ℎ𝑡10 for the implementation of our baselines LG and TSVM .
The parameter settings of all the compared MTL algorithms follow the instructions in their original papers and are carefully tuned on our data set .
For MTS3FLC , after some preliminary investigation we find its performance is not very sensitive to the total topic number 𝐾 in each task when 𝐾 chooses values in [ 20 , 100 ] , so we set 𝐾 = 20 for all our experiments .
For single task S3FLC which is the special case of MTS3FLC when 𝔎 = 0 , we initialize the topic matrix 𝐹𝑖 by the word clustering results of 𝑋𝑖 by PLSA ( with topic number set to 𝐾 ) in each task , and initialize the to the constant 1/𝐾 . elements of the relation matrix 𝑆𝑖 𝐺𝑖𝑈 , 𝑖 = 1,⋅⋅⋅ , 𝑇 are initialized by single task Logistic Regression since we assume we are learning each task
9http://researchmicrosoftcom/∼minka/papers/logreg/ 10http://svmlightjoachimsorg/
16196 independently . Other initializations of variables are the same as that in MTS3FLC .
The maximal iterating number is set to 𝑚𝑎𝑥𝐼𝑡𝑒𝑟 = 20 in all experiments of MTS3FLC and S3FLC as we empirically find they converge very quickly .
D . Experimental Results
1 ) Comparison on the 20 Newsgroups Data :
In this subsection the proposed algorithm MTS3FLC is compared with various baselines and three state of the art multi task learning algorithms . The shared topic number in MTS3FLC is set to 𝔎 = 5 for all experiments conducted here .
We first evaluate these algorithms on the six orderly constructed 4 task learning problems . For each of these six problems we vary the number of labeled samples in each task from 2 to 20 , since as well known the merits of multitask learning are more apparent when few data per task are available . For clarity , we present the comparison results between MTS3FLC and the five baselines and the results between MTS3FLC and other MTL algorithms separately . The former are shown in Figure 1 while the latter in Figure 2 , and all these results are averaged over 30 independent trials ( in each trial we randomly sample the labeled data and test on the remaining data ) . From Figure 1 we can see that owing to the exploration of unlabeled data and the elaborate knowledge sharing scheme , MTS3FLC can provide very good predictive performance , and can substantially outperform the single task learning methods STL LG , STLTSVM and STL S3FLC as well as the pooling methods Pooling LG and Pooling TSVM on all six problems .
For the results in Figure 2 , we directly applied MTFL11 and BMTLGP12 on the original 8198 dimensional bag ofwords data described in Section IV A while PCA was used to reduce the dimension to 500 ( as the author did in their paper ) for GMTFL13 on each of the six problems . The linear covariance function was used in BMTLGP ( according to the author ’s advice ) since the high dimensional text data makes the ARD covariance function infeasible . After careful investigation , the group number and the parameter 𝛾 in GMTFL were set to 2 and 10 respectively , while the parameter 𝛾 in MTFL was set to 001 From the results , we can clearly see that MTS3FLC is consistently superior to all the three compared MTL algorithms on all six problems .
Then we conduct comparison experiments on all the 24 randomly constructed 4 task learning problems for each pair of top categories from 20 Newsgroups . There are totally 144 such problems since we have 𝐶 2 4 = 6 pairs of top categories . The experimental setting for each algorithm are the same as above , but here we only vary the number of labeled samples per task among {2 , 10 , 20} because so many 4 task text classification problems are very time consuming . We
11http://tticuchicagoedu/∼argyriou/code/indexhtml/ 12http://homepagesinfedacuk/gsanguin/softwarehtml/ 13http://www scfuscedu/∼zkang/GoupMTLCodezip
1
0.9
0.8
0.7
0.6
0.5 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
0.4
2
4
1
0.9
0.8
0.7
0.6
0.5 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
0.4
2
4
MTS3FLC STL LG STL TSVM STL S3FLC Pooling LG
Pooling TSVM
6
14 Number of labeled samples in each task
10
8
20
( a ) comp vs . rec
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
0.5
2
4
1
0.9
0.8
0.7
0.6
0.5 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
6
14 Number of labeled samples in each task
10
8
20
0.4
2
4
1
0.9
0.8
0.7
0.6
0.5 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
20
0.4
2
4
6
14 Number of labeled samples in each task
10
8
6
14 Number of labeled samples in each task
10
8
( b ) comp vs . talk
( c ) sci vs . rec
1
0.9
0.8
0.7
0.6
0.5 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
20
0.4
2
4
6
14 Number of labeled samples in each task
10
8
6
14 Number of labeled samples in each task
10
8
20
20
( d ) rec vs . talk
( e ) sci vs . talk
( f ) sci vs . comp
Figure 1 . Comparison with various baselines on the six orderly constructed 4 task classification problems from 20 Newsgroups . repeat the experiment 5 times ( with labeled data randomly sampled every time ) for each problem , and compute the mean accuracy . Finally the averaged results over all 24 problems for each pair of top categories are listed in Table II , from which we see obvious advantage of MTS3FLC again .
2 ) Parameter Effect : Our preliminary experiments show that the performance of MTS3FLC is not sensitive to the total topic number 𝐾 when 𝐾 chooses values in [ 20 , 100 ] , so here we mainly investigate the shared topic number 𝔎 how affect the performance of MTS3FLC . To this end , we fix 𝐾 = 20 and vary 𝔎 in the interval [ 0 , 𝐾 ] . The averaged results over 30 repetitions on the six orderly constructed 4 task learning problems from 20 Newsgroups are shown in Table III ( for comparison we also listed the results of LG and Pooling LG ) , where we set the number of labeled samples per task to 𝑡𝑟𝑎𝑖𝑛𝑠𝑖𝑧𝑒 = 6 for the upper three problems and 𝑡𝑟𝑎𝑖𝑛𝑠𝑖𝑧𝑒 = 14 for the lower three problems to study whether the parameter effects of 𝔎 are identical under different training sizes . From Table III we can see MTS3FLC generally performs well in a wide range of 𝔎 , eg , on the combination sci vs . rec MTS3FLC achieves the accuracy over 95 % for arbitrary 0 < 𝔎 ≤ 𝐾 , which means that we can choose 𝔎 freely and boldly when we have no prior information of the task relatedness ( if we have prior information , we may use a large 𝔎 even equal to 𝐾 for closely related tasks , and a small 𝔎 even equal to zero for loosely related tasks ) . We can also see that MTS3FLC with medium large 𝔎 performs better than both MTS3FLC with too small 𝔎 and MTS3FLC with very large 𝔎 . Eg , MTS3FLC with 𝔎 = 5 generally performs well than MTS3FLC with 𝔎 = 3 and MTS3FLC with 𝔎 = 13 .
3 ) Visualization of The Learned Semantic Topics : To show the capability of MTS3FLC to automatically find the common semantics shared among tasks and specific semantics exclusive to each task , we choose 20 most probable key words to express each topic according to the word clustering information 𝐹𝑖 , 𝑖 = 1,⋅⋅⋅ , 𝑇 . Table IV lists one common topic among the four tasks and one specific topic of each task on the orderly constructed 4 task learning problem from comp vs . rec . Here , we set the number of shared topics to 𝔎 = 5 and the number of labeled samples per task to 𝑡𝑟𝑎𝑖𝑛𝑠𝑖𝑧𝑒 = 10 . From these results , it can be seen that MTS3FLC can effectively capture the common semantic topics among tasks using same or different key words about the topic , eg , Topic 1 is a common topic among tasks discussing “ computer ” ( The associations between such topic and the categories are shared among tasks for multitask classification ) . Moreover , MTS3FLC can also find the specific topics of each task which are generally expressed by different key words , eg , Topic 2 in Table IV is a specific
17197
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
2
4
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
0.55
2
4
6
14 Number of labeled samples in each task
10
8
( a ) comp vs . rec
6
14 Number of labeled samples in each task
10
8
MTS3FLC BMTLGP GMTFL MTFL
20
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
0.55
2
4
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
20
0.55
2
4
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
20
0.55
2
4
6
14 Number of labeled samples in each task
10
8
6
14 Number of labeled samples in each task
10
8
( b ) comp vs . talk
( c ) sci vs . rec
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6 s k s a t r u o f r e v o y c a r u c c a d e g a r e v A
20
0.55
2
4
6
14 Number of labeled samples in each task
10
8
6
14 Number of labeled samples in each task
10
8
20
20
( d ) rec vs . talk
( e ) sci vs . talk
( f ) sci vs . comp
Figure 2 . Comparison with state of the art MTL algorithms on the six orderly constructed 4 task classification problems from 20 Newsgroups . topic of each task which is discussing “ graphics ” , “ bike ” , “ baseball ” and “ hockey ” in each task respectively .
V . RELATED WORK
Multi task learning [ 23 ] has been the focus in the community of machine learning and data mining during the past ten years , and many algorithms on it have been proposed . In [ 13 ] , Argyriou et al . proposed a multi task feature learning method which learns a shared sparse representation for multiple related tasks . They show their 𝑙1,2 norm based formulation is equivalent to a convex optimization problem and can be solved efficiently . Later , Zhang et al . [ 5 ] proposed a probabilistic interpretation of the general multi task feature selection problem using 𝑙1,2 or 𝑙1,∞ norm . Recently , a group multi task feature learning method is proposed in [ 1 ] , which assumes the tasks exist in groups and the tasks within each group share features . Our model is related to these works since we are also learning features for each task . However , our model is more flexible to capture the semantic features that exist in many dyadic data including text , image and gene expression . Besides , usually abundant unlabeled samples in each task is naturally incorporated in our model , which makes feature learning more reliable .
Since we have used unlabeled data during the learning phase , our algorithm should be categorized into semisupervised multi task learning [ 9 ] , [ 4 ] . Though as two major directions to improve supervised learning in case of labeled samples are insufficient , semi supervised learning and multitask learning have been investigated extensively respectively , few works have combined these two lines together . While the manifold properties of unlabeled data are well exploited in [ 9 ] , our method utilizes unlabeled data to learn not only the more robust predictive structures but also the common and specific semantic features of each task .
Our model is also related to a transfer learning model named MTrick [ 21 ] , which shares the associations between categories and all semantics in source domain and target domain . Though multiple related tasks ( domains ) typically share semantics , treating all semantics as the shared ones may be not appropriate . In fact , our experiments on 20 Newsgroups data show the best number of shared semantics among tasks even less than a half of the total topic number . the idea of distinguishing common semantic topics from specific ones is also considered in a recent work called Group Matrix Factorization ( GMF ) [ 24 ] , which aims at scaling up topic modeling approaches and assumes there exist class specific topics for each text class as well as shared topics across all classes . However , GMF only simply supposed that a shared topic has the same word distribution in each class , which differs from the semantic corresponding idea considered in this paper .
Besides ,
18198
COMPARISON ON THE 144 RANDOMLY CONSTRUCTED 4 TASK CLASSIFICATION PROBLEMS FROM 20NEWSGROUPS . THE LISTED ACCURACIES ( % ) FOR EACH PAIR OF TOP CATEGORIES ARE THE MEAN PERFORMANCE ON ITS 24 CORRESPONDING PROBLEMS , WHERE THE PERFORMANCE ON EACH PROBLEM IS AVERAGED OVER 5 INDEPENDENT TRIALS . IN EACH PROBLEM , THE NUMBER OF LABELED SAMPLES PER TASK IS VARIED AMONG 2 , 10
AND 20 . P LG REPRESENTS POOLING LG , AND P TSVM REPRESENTS POOLING TSVM .
Table II trainsize
2 10 20 2 10 20 2 10 20 2 10 20 2 10 20 2 10 20
LG 70.64 86.66 91.09 70.82 87.43 91.99 65.69 82.72 88.55 68.70 84.22 89.35 65.01 80.20 85.47 64.59 78.91 84.59
TSVM S3FCL 91.22 71.43 93.25 93.77 94.58 95.26 85.92 70.70 87.75 91.05 93.49 90.66 87.87 64.60 94.04 86.80 95.28 94.58 61.73 85.06 89.54 85.10 90.24 87.78 77.87 58.12 86.11 74.74 82.04 86.37 79.28 60.67 85.61 81.25 88.77 87.98 comp vs . rec comp vs . talk sci vs . rec rec vs . talk sci vs . talk sci vs . comp
P LG P TSVM MTFL GMTFL BMTLGP MTS3FLC 73.98 87.67 90.74 78.17 91.46 94.00 64.66 78.26 84.86 69.53 83.30 88.24 65.30 79.31 83.28 65.67 77.40 82.38
95.46 96.74 96.95 93.10 96.33 96.50 90.24 95.64 96.63 92.28 95.85 96.25 86.31 92.34 93.25 86.48 88.74 90.13
67.08 89.60 92.33 68.86 85.11 88.74 51.41 60.60 80.00 59.74 66.70 71.39 58.56 64.32 68.05 53.34 68.42 76.64
66.87 89.64 92.96 70.05 91.64 94.70 60.62 80.65 87.34 64.53 83.49 89.63 58.80 76.43 84.21 61.15 77.47 84.81
64.98 82.33 88.76 64.49 81.69 89.33 62.64 78.94 86.68 66.14 80.94 86.96 62.12 77.88 83.90 61.02 75.21 82.79
68.77 88.46 93.27 69.53 88.34 94.14 65.54 84.92 91.98 67.72 86.76 92.37 64.60 82.19 87.84 64.17 81.62 87.80
PARAMETER EFFECT OF MTS3FLC ON THE SIX ORDERLY CONSTRUCTED 4 TASK LEARNING PROBLEMS FROM 20 NEWSGROUPS . THE TOTAL TOPIC NUMBER IS FIXED TO 𝐾 = 20 WHEN 𝔎 IS VARIED IN THE INTERVAL [ 0 , 𝐾 ] . THE NUMBER OF LABELED SAMPLES PER TASK IS SET TO 𝑡𝑟𝑎𝑖𝑛𝑠𝑖𝑧𝑒 = 6
FOR THE UPPER THREE PROBLEMS AND 𝑡𝑟𝑎𝑖𝑛𝑠𝑖𝑧𝑒 = 14 FOR THE LOWER THREE PROBLEMS . ALL RESULTS ( % ) ARE AVERAGED OVER 30 RUNS .
Table III comp vs . rec comp vs . talk sci vs . rec rec vs . talk sci vs . talk sci vs . comp
Average
LG 81.28 82.58 77.74 86.89 82.91 82.76 82.36
P LG 82.06 87.80 75.20 85.9 81.66 79.13 81.96
𝔎=0 92.80 91.43 91.50 91.39 85.99 85.49 89.77
𝔎=3 96.56 94.86 95.12 95.82 91.28 92.87 94.42
𝔎=5 96.59 95.60 95.22 96.10 92.06 92.86 94.74
𝔎=7 96.47 94.47 95.17 95.36 91.52 92.62 94.27
𝔎=10 96.33 94.36 95.81 94.50 91.17 92.82 94.17
𝔎=13 95.89 94.30 95.46 94.49 91.14 91.82 93.85
𝔎=16 95.65 94.10 95.04 94.57 90.05 91.43 93.47
𝔎=18 95.81 93.87 95.30 94.56 89.82 91.67 93.51
𝔎=20 95.00 92.81 95.11 94.42 89.17 90.76 92.88
VI . CONCLUSIONS AND FUTURE WORK
In this paper , we have proposed a novel multi task semantic feature learning algorithm MTS3FLC with few labeled and abundant unlabeled data in each task . The algorithm shares the associations between categories and common semantics among tasks and formulates its objective as a joint Nonnegative matrix tri factorization based optimization which can simultaneously learn the topic correspondences among tasks and predict the categories of unlabeled data . We have developed alternating iterative algorithm to optimize our objective and theoretically showed its convergence . Extensive experiments on text data demonstrate our method can provide superior generalization performance compared with various baselines and several state of the art multi task learning algorithms .
Though we only studied text classification in this paper , MTS3FLC is also applicable to many other learning problems , eg , image classification in computer vision , where we have many “ visual words ” as the raw features , based on which we can also induce higher level semantic features .
For the future work , one challenging but important issue is to automatically resolve the appropriate number of shared semantics among tasks according to their relatedness . Luckily , the recent progress in nonparametric Bayesian has provided us a possible way to achieve this . Another issue that matters is to combine graph regularization with our framework to exploit the geometric structure of data . Since close samples tend to have the same label , predicting labels smoothly may further improve performance . We will report these results in a longer version of the paper .
ACKNOWLEDGMENT
This work is supported by the National Natural Science Foundation of China ( No . 61175052 , 60975039 , 61203297 , 60933004 , 61035003 ) , National High tech R&D Program of China ( 863 Program ) ( No . 2012AA011003 ) , and National Program on Key Basic Research Project ( 973 Program ) ( No . 2013CB329502 ) .
19199
THE COMMON TOPICS AMONG TASKS AND SPECIFIC TOPIC OF EACH TASK ON THE ORDERLY CONSTRUCTED 4 TASK LEARNING PROBLEM FROM comp vs . rec . TASK 1 : comp.graphics vs . rec.autos ; TASK 2 : composms windowsmisc vs . rec.motorcycles ; TASK 3 : compsysibmpchardware vs . recsportbaseball ; TASK 4 : compsysmachardware vs . recsporthockey
Table IV
Topic 1
( Common topic )
Topic 2
( Specific topic )
Task 1
Task 2
Task 3
Task 4
Task 1
Task 2
Task 3
Task 4 graphics,gif,thanks,format,files,tiff,file,image,program,bit , color,images,pcx,advance,hi,vesa,vga,xv,ftp,card windows,dos,file,os,drivers,driver,mouse,files,nt,card , printer,thanks,program,window,version,ini,ms,win,memory,apps scsi,ide,drive,controller,card,mb,dx,bus,drives,isa , pc,vlb,disk,dos,bios,mhz,system,motherboard,floppy,port mac,apple,lc,monitor,mhz,drive,centris,simms,quadra,mb , scsi,card,iisi,duo,thanks,test,video,fpu,lciii,cd graphics,thanks,please,looking,where,format,image,any,find,anyone , advance,can,mail,me,comp,hi,files,polygon,file,am bike,dod,shaft,com,ride,my,motorcycle,bmw,you,drive , bikes,riding,honda,rider,ca,dog,on,do,your,nec mattingly,he,career,njit,tesla,don,baseball,game,edu,year , games,team,his,cubs,runs,was,hit,pitcher,sox,last game,hockey,espn,team,he,games,go,nhl,bruins,was , wings,win,ca,season,detroit,play,pens,gm,players,they
REFERENCES
[ 1 ] Z . Kang , K . Grauman , and F . Sha , “ Learning with whom to share in multi task feature learning , ” in ICML , 2011 .
[ 2 ] A . Torralba , K . Murphy , and W . Freeman , “ Sharing visual features for multiclass and multiview object detection , ” IEEE Trans . on PAMI , vol . 29 , no . 5 , pp . 854–869 , 2007 .
[ 3 ] R . Ando and T . Zhang , “ A framework for learning predictive structures from multiple tasks and unlabeled data , ” JMLR , vol . 6 , pp . 1817–1853 , 2005 .
[ 4 ] Y . Qi , O . Tastan , J . Carbonell , J . Klein Seetharaman , and J . Weston , “ Semi supervised multi task learning for predicting interactions between hiv 1 and human proteins , ” Bioinformatics , vol . 26 , no . 18 , pp . i645–i652 , 2010 .
[ 5 ] Y . Zhang , D . Yeung , and Q . Xu , “ Probabilistic multi task feature selection , ” in NIPS , vol . 23 , 2010 , pp . 2559–2567 .
[ 6 ] B . Bakker and T . Heskes , “ Task clustering and gating for bayesian multitask learning , ” JMLR , vol . 4 , pp . 83–99 , 2003 .
[ 7 ] K . Yu , V . Tresp , and A . Schwaighofer , “ Learning gaussian processes from multiple tasks , ” in ICML , 2005 , pp . 1012– 1019 .
[ 8 ] Y . Xue , X . Liao , L . Carin , and B . Krishnapuram , “ Multitask learning for classification with dirichlet process priors , ” JMLR , vol . 8 , pp . 35–63 , 2007 .
[ 9 ] Q . Liu , X . Liao , H . Li , J . Stack , and L . Carin , “ Semisupervised multitask learning , ” IEEE Trans . on PAMI , vol . 31 , no . 6 , pp . 1074–1086 , 2009 .
[ 10 ] G . Skolidis and G . Sanguinetti , “ Bayesian multitask classification with gaussian process priors , ” IEEE Trans . on Neural Networks , no . 99 , pp . 2011–2021 , 2011 .
[ 11 ] T . Jebara , “ Multi task feature and kernel selection for svms , ” in ICML , 2004 .
[ 12 ] T . Evgeniou , C . Micchelli , and M . Pontil , “ Learning multiple tasks with kernel methods , ” JMLR , vol . 6 , pp . 615–637 , 2006 .
[ 13 ] A . Argyriou , T . Evgeniou , and M . Pontil , “ Multi task feature learning , ” in NIPS , 2007 .
[ 14 ] S . Deerwester , S . Dumais , G . Furnas , T . Landauer , and R . Harshman , “ Indexing by latent semantic analysis , ” Journal of the American society for information science , vol . 41 , no . 6 , pp . 391–407 , 1990 .
[ 15 ] D . Lee , H . Seung et al . , “ Learning the parts of objects by nonnegative matrix factorization , ” Nature , vol . 401 , no . 6755 , pp . 788–791 , 1999 .
[ 16 ] D . Blei , A . Ng , and M . Jordan , “ Latent dirichlet allocation , ”
JMLR , vol . 3 , pp . 993–1022 , 2003 .
[ 17 ] S . Chakraborti , R . Mukras , R . Lothian , N . Wiratunga , S . Watt , and D . Harper , “ Supervised latent semantic indexing using adaptive sprinkling , ” in IJCAI , vol . 7 , 2007 , pp . 1582–1587 .
[ 18 ] D . Blei and J . McAuliffe , “ Supervised topic models , ” in NIPS ,
2007 , pp . 121–128 .
[ 19 ] I . Rish , G . Grabarnik , G . Cecchi , F . Pereira , and G . Gordon , “ Closed form supervised dimensionality reduction with generalized linear models , ” in ICML , 2008 , pp . 832–839 .
[ 20 ] C . Ding , T . Li , W . Peng , and H . Park , “ Orthogonal nonnegative matrix tri factorizations for clustering , ” in SIGKDD . ACM , 2006 , pp . 126–135 .
[ 21 ] F . Zhuang , P . Luo , H . Xiong , Q . He , Y . Xiong , and Z . Shi , “ Exploiting associations between word clusters and document classes for cross domain text categorization ? ” Statistical Analysis and Data Mining , vol . 4 , no . 1 , pp . 100–114 , 2011 .
[ 22 ] D . Lee and H . Seung , “ Algorithms for non negative matrix factorization , ” in NIPS , vol . 13 , 2001 .
[ 23 ] R . Caruana , “ Multitask learning , ” Machine Learning , vol . 28 , no . 1 , pp . 41–75 , 1997 .
[ 24 ] Q . Wang , Z . Cao , J . Xu , and H . Li , “ Group matrix factorization for scalable topic modeling , ” in SIGIR . ACM , 2012 , pp . 375–384 .
20200
