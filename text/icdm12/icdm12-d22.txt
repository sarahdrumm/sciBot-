An AdaBoost Algorithm for Multiclass Semi Supervised Learning
Jafar Tanha , Maarten van Someren , Hamideh Afsarmanesh
Informatics Institute , University of Amsterdam
Science Park 904 , 1098 XH Amsterdam , Netherlands
Email : {j.tanha , mwvanSomeren , hafsarmanesh}@uvanl
Abstract—We present an algorithm for multiclass SemiSupervised learning which is learning from a limited amount of labeled data and plenty of unlabeled data . Existing semisupervised algorithms use approaches such as one versus all to convert the multiclass problem to several binary classification problems which is not optimal . We propose a multiclass semisupervised boosting algorithm that solves multiclass classification problems directly . The algorithm is based on a novel multiclass loss function consisting of the margin cost on labeled data and two regularization terms on labeled and unlabeled data . Experimental results on a number of UCI datasets show that the proposed algorithm performs better than the stateof the art boosting algorithms for multiclass semi supervised learning .
Keywords Semi Supervised Learning ; boosting ; multiclass classification
I . INTRODUCTION
In many applications of Machine Learning , for example in classification of images , texts , and multimedia , it is difficult to obtain values of the target variable , while unlabeled examples are plentiful . Semi supervised learning algorithms use not only the labeled data but also the unlabeled data to build a classifier . One approach to semi supervised learning is to extend a boosting algorithm , which is one of the most successful meta classifiers for supervised learning [ 4 ] . Boosting uses a base learner to construct a series of weak classifiers . Each classifier is assigned a weight and when a termination criterion is reached , then the final hypothesis is constructed as a weighted combination of the classifiers from the boosting . In supervised learning , boosting uses the prediction error to determine the weights of training data and the weights for the classifiers .
Recently , boosting has been extended to semi supervised learning . Examples are ASSEMBLE [ 1 ] and MarginBoost [ 3 ] . They use the pseudo margin for unlabeled data in order to improving the classification performance using unlabeled examples . ASSEMBLE and MarginBoost rely only on the classifier predictions to assign the “ pseudo labels ” to the unlabeled data , which is not always optimal [ 7 ] . An advantage of this approach is that it can easily be applied to multiclass classification , see [ 9 ] . A different idea is to not estimate the pseudo margin from the predictions of the base learner . Instead , beside the margin on the labeled data , also the “ consistency ” is maximized . Consistency is a form of smoothness . Class labels are consistent if they are equal for data that are similar . For this the calculation of the weights for data and for classifiers in the boosting algorithm must be modified . This is done in SemiBoost [ 7 ] and RegBoost [ 2 ] . Experiments show that this idea is more effective than the approach based on pseudo margin or confidence , see [ 2 ] and [ 7 ] . However , the solutions for the weights of data and classifiers in the boosting algorithm only exist for binary classification problems . For a multiclass semi supervised learning problem , the only available algorithms , beside supervised learning of the labeled data only , are now those based on pseudo margin or the use of a binary method that minimizes error on the labeled data and consistency over the unlabeled data . This must then be used in a one versus all or similar meta algorithm to handle multiclass classification problems . This approach can have various problems , such as imbalanced class frequencies , increased complexity , no guarantee to obtain an optimal joint classifier , and different scales for the outputs of generated binary classifiers which complicates combining them , see [ 6 ] and [ 8 ] . Recently , in [ 11 ] a new boosting method is used for semi supervised multiclass learning which uses similarity between predictions and data . It maps labels to n dimensional space but this mapping may not lead to train a classifier that minimizes the margin cost properly .
In this paper we present a boosting algorithm for multiclass semi supervised learning which minimizes both the empirical error on the labeled data and the inconsistency over labeled and unlabeled data , named MultiSemiAdaBoost . This generalizes the SemiBoost and RegBoost algorithms from binary to multiclass classification using a coding scheme . Our proposed method uses the margin on the labeled data , the similarity among labeled and unlabeled data , and the similarity among unlabeled data in the loss function . We give a formal definition of this loss function and derive functions for the weights of classifiers and unlabeled data by minimizing an upper bound on the objective function . We then compare the performance of the algorithm to the state of the art algorithms . The results show that our algorithm gives the best results .
This paper is organized as follows : section II formalizes the setting and the loss function , section III derives the weights for the boosting algorithm , sections IV and V present the experiments and the results and section VI draws the main conclusions . fl 1
II . MULTICLASS SEMI SUPERVISED LEARNING
In this section , we define the multiclass setting and a loss function for multiclass semi supervised learning . A . Multiclass setting
In multiclass semi supervised learning for the labeled points Xl= ( x1 , x2 , , xl ) labels {1 , , K} are provided , and for the unlabeled points Xu = ( xl+1 , xl+2 , , xl+u ) , the labels are not known . In the multiclass setting , let Yi ∈ Rk define a K dimensional vector with all entries equal to − 1 K−1 except a 1 in position i . Each class label i ∈ {1 , , K} is then mapped into a vector Yi . Each entry yi,j is thus defined as follows : if i = j if i = j
( 1 ) yi,j =
−1 K−1 dimensional vectors such that for each Yi ∈ Y ,K where K is the number of classes and Y is the set of Kj=1 yi,j = 0 . We use the above notation to formulate the loss function for multiclass semi supervised learning . This coding was used by Zhu et . al [ 12 ] for supervised Multiclass AdaBoost algorithm , named SAMME .
For our algorithm we also need a ( symmetric ) similarity matrix S = [ Si,j]n×n , where Si,j = Sj,i is the similarity between the points xi and xj . Slu = [ Si,j]nl×nu denotes the similarity matrix of the labeled and unlabeled data and Suu = [ Si,j]nu×nu of the unlabeled data . Our algorithm is a “ meta learner ” that uses a supervised learning algorithm as base learner . We assume that the labeled and unlabeled data are drawn independently from the same data distribution . In applications of semi supervised learning normally l u , where l is the number of labeled data and u is the number of unlabeled data . B . Loss Function for Multiclass Semi Supervised Boosting The loss functions of the ASSEMBLE and MarginBoost algorithms for semi supervised boosting are the sum of empirical loss on the labeled data and on the unlabeled data . Our approach is based on a different idea . Unlike ASSEMBLE and MarginBoost , instead of a term based on pseudo margin for the unlabeled data , we add terms for consistency over the labeled and unlabeled data to the loss function . If examples that are similar , in terms of the similarity matrix , are assigned different classes then this adds a penalty to the loss function . This gives three components for the loss function : ( 1 ) the empirical loss on the labeled data , ( 2 ) the consistency among labeled and unlabeled data , and ( 3 ) the consistency among unlabeled data . Combining them results in a minimization problem with the following objective function : F ( Y , H ) = C1Fl(Y , H)+C2Flu(Y , Slu , H)+C3Fuu(Suu , H )
( 2 ) where C1 , C2 , and C3 are the weights for the three components . To derive the algorithm it is mathematically convenient to use an exponential loss function for margin cost on the labeled data ( as in multiclass AdaBoost [ 12 ] ) which gives :
Fl(Y , H ) = exp(− 1 K
Yi.H(xi ) )
( 3 ) nl i=1 where H( . ) is a multiclass ensemble classifier . i=1 j=1
In ( 2 ) , Flu(Y , Slu , H ) measures the inconsistency between the similarity information and the classifier predictions on labeled and unlabeled data as follows : −1 K − 1
Flu(Y , Slu , H ) =
Slu(xi , xj)exp( nl nu
Yi.H(xj ) )
( 4 ) where nl and nu are the number of labeled and unlabeled data respectively .
The third term of ( 2 ) measures the inconsistency between the unlabeled examples in terms of the similarity information and the classifier predictions . We use the harmonic function to define Fuu(Suu , H ) . This is a popular way to define the inconsistency in many graph based methods , for example [ 13 ] .
Fuu(Suu , H ) =
Suu(xi , xj)e
K−1 ( H(xi)−H(xj ) ) .
1
−→ 1 i,j∈nu
( 5 ) The resulting loss function is presented as the following optimization problem : minimize F(Y , S , H ) subject to H1(xi ) + + Hk(xi ) = 0 , H(xi ) = Yi , i = 1 , 2 , , nl
( 6 )
The above objective function is non linear , which makes it difficult to solve . One valuable point in that function is that it uses an exponential function and since exp(x ) is convex , thus it can be shown that F ( S , Y , H ) maintains the convexity . Maintaining the convexity of the objective function requires that the kernel or similarity metrics be positive semi definite . As mentioned before , exp(x ) is a convex function and the similarity function S(xi , xj ) is nonnegative for each i , j . Hence , the function F ( S , Y , H ) is a convex function .
III . THE MULTICLASS SEMI SUPERVISED BOOSTING
ALGORITHM
In this section we derive the Multi SemiAdaBoost algorithm . Given a set of labeled and unlabeled data , a similarity metric , and a supervised ( multiclass ) learning algorithm , Multi SemiAdaBoost effectively minimizes the objective function ( 2 ) . In Multi SemiAdaBoost , a series of classifiers is constructed for the same classification task by applying a supervised base learner to varying sets of ( pseudo )labeled training data . Each classifier gets a weight .
When a termination condition is reached , the weighted combination of classifiers becomes the final hypothesis . The algorithm starts with the labeled data and in each iteration adds some unlabeled data that received a “ pseudo label ” from the hypothesis constructed in the previous iteration . For this algorithm we need to find the weight for the classifiers and the confidence for the predictions . For this , let H t(x ) : X −→ Rk define the linear combination of classification models after t iterations . At the t th iteration , H t(x ) is computed as : the following :
F1 = C1 i=1 nl exp , − 1
βt k∈l i − ht i,j∈nu
( ht
K − 1
+ C3 exp
Slu(xi , xj)exp(
Yi.(H t−1 i + βtht
K −1 K − 1 Suu(xi , xj)exp
H t−1 j
.ek)exp(
1
K − 1 j).ek nl nu i=1 j=1 k∈l i) + C2
−βt K − 1 ht j.ek)δ(Yi , ek ) i − H t−1 ( H t−1
).ek j
H t(x ) = H t−1(x ) + βtht(x )
Subject to :
( 7 )
H t
1(xi ) + + H t k(xi ) = 0
( 10 )
( 11 ) ht i.ek )
( 12 )
( 13 )
H t(xi ) = Yi f or i = 1 , , nl where δ(Yi , ek ) is :
δ(Yi , ek ) = fl 1
0 if i = k if i = k
F1 is an upper bound on F and minimizing F1 will also minimize F . H t−1(xi ) and ht
Proof : To simplify the notation we write H t−1 i for ht(xi ) , see appendix .
For now assume that C1 = C2 = C3 = 1 . As mentioned before , ( 10 ) is a nonlinear optimization problem . To solve this problem and also to find a criterion for assigning “ pseudo labels ” to the unlabeled examples we use Proposition 2 . for
Proposition 2 : Minimizing F1 is equivalent to minimizing i the following objective function F2 :
Wiexp(
−βt K
Yi.ht i ) +
Pi,kexp(
−βt K − 1 nu i=1 k∈l
Wi = exp(
−1 K
Yi.H t−1 i
)
Slu(xi , xj)e( −1
K−1 H t−1 i
.ek)δ(Yi , ek )
, 1
Suu(xi , xj)e
K−1 ( H t−1 j −H t−1 i
).ek
1
K−1 e
( 14 ) where βt ∈ R is the weight of the base classifier ht(x ) , and ht(x ) is determined by P ( x ) , which is a multiclass base classifier . Let also ht(x ) denote a classifier as follows :
∀x ∈ RP ht(x ) : X → Y
( 8 )
Two main approaches to solve the optimization problem ( 6 ) are : gradient descent [ 2 ] and bound optimization [ 7 ] . A difficulty of the gradient descent method is to determine the step size at each iteration . We use the bound optimization approach to derive the boosting algorithm for ( 6 ) , which automatically determines the step size at each iteration of boosting process .
At t th iteration the optimization problem ( 2 ) is derived by substituting ( 7 ) in ( 6 ) : i=1 exp(− 1 K
Yi.,H t−1(xi ) + βtht(xi ) nl K−1 Yi.,H t−1(xj )+βtht(xj )nl nu
,(H t−1(xi)+βtht(xi))−(H t−1(xj )+βtht(xj )) .ek
Slu(xi , xj)e
Suu(xi , xj ) i,j∈nu
− 1 j=1 i=1 k∈l
F = C1
+ C2
+ C3
1
K−1 e
( 9 ) nl
F2 = i=1 where and
Pi,k =
+ nl nu j=1 j=1
To solve ( 9 ) , we first simplify it and then with reformulation we derive a criterion to assign weights to data , which are in propositions 1 2 . Finally , we use proposition 3 to find optimal class labels and β as weight for the new classifier ht(x ) . For this , we use a standard basis vector ek to represent class membership . The resulting reformulation is in preposition 1 .
Proposition 1 : Minimizing ( 9 ) is equivalent to minimizing
Proof : See appendix .
In the optimization problem ( 12 ) , Pi,k can be used for weighting the unlabeled examples and Wi is used for weighting labeled data . According to these criteria the algorithm decides whether to select a sample for the new training set or not . The expression in ( 12 ) is in terms of βt and ht(x ) and hence make it difficult to solve . The following proposition simplifies it by using a bound .
( K − 1)2
β =
Proposition 3 : The class label for unlabeled example xi that minimizes F2 is :
ˆYi = arg max and the weight for that will be w the optimal value for β that minimizes F2 is :
( 15 ) ( Pi,k ) i = max|Pi,k| . Meanwhile k
K i∈nu i∈nu log(K − 1 ) k∈l Pi,kδ(ht k∈l Pi,kδ(ht i.ek , Pi = k ) + i.ek , Pi = k ) + i∈nl ht i=Yi i∈nl i=Yi ht
Wi
Wi
( 16 )
+ log where Pi ≡ P ( xi ) was defined in ( 7 ) .
Proof : See appendix .
Now , let t denote the weighted error made by the classifier ht . Then the t will be as follows : i∈nu k∈l Pi,kδ(hi.ek , Pi = k ) + k∈l Pi,k + i∈nu i∈nl
Wi t = i∈nl i=Yi ht
Wi
( 17 )
( 18 )
Replacing ( 17 ) in ( 16 ) leads to the following βt : 1 − t t log(K − 1 ) + log(
( K − 1)2
βt =
K
) which is a generalization of the weighting factor of Multiclass AdaBoost [ 12 ] . Our formulation can thus be seen as a generalization of AdaBoost to multiclass semi supervised data .
A . Multi SemiAdaBoost Algorithm
Based on the previous discussion , we can provide the details of the algorithm . In Algorithm 1 , first the values of
Algorithm 1 Multi SemiAdaBoost
L , U , S , H 0(x ) = 0 ; L , U : Labeled and Unlabeled data ; S : Similarity Matrix ; H 0(x ) : Ensemble of Classifiers ; t ← 1 ; while ( βt > 0 ) and ( t < M ) do// M is the number of iterations for each xi ∈ L do for each xi ∈ U do
Compute Wi for labeled example xi based on ( 13 )
Compute Pi,k for unlabeled example xi based on ( 14 ) Assign pseudo labels for unlabeled data based on ( 15 ) Normalize the weights of labeled and unlabeled examples Sample a set of high confidence examples from data Build a new classifier ht(x ) Compute the weights and βt using ( 16 ) Update H t ← H t−1 + βtht ; t ← t + 1 end while Output : Generate final hypothesis based on the weights and classifiers
Pi,k and Wi are computed for each unlabeled and labeled example . In order to compute the value of Pi,k , we use the similarity information among data and the classifier prediction and Wi is computed as in multiclass AdaBoost
( SAMME ) . Then ( 15 ) is used to assign a “ pseudo label ” to the unlabeled example and the value of ( 15 ) becomes the weight . Next , a set of high confidence newly labeled data besides the labeled data are used based on the weights for training a new classifier , called ht(x ) . The algorithm then uses the result of Proposition ( 2 ) to sample data which will lead to a decrease of the value of the objective function . This new set is consistent with two criteria , classifier prediction and similarity between examples . The boosting process is repeated until it reaches a stopping condition . After finishing , the final hypothesis is the weighted combination of the generated classifiers . We use βt ≤ 0 and a fix number of iterations as stopping conditions .
IV . EXPERIMENTS
In the experiments we compare Multi SemiAdaBoost ( MSAB ) with several other algorithms . One comparison is with using the base learner on the labeled data only and a second is with ( mutliclass ) AdaBoost ( SAMME [ 12 ] ) using the same base learner . The purpose is to evaluate if MSAB exploits the information in the unlabeled data . We also include comparisons with the state of the art algorithms for semi supervised boosting , in particular ASSEMBLE , SemiMultiAdaBoost [ 9 ] , RegBoost [ 2 ] , and SemiBoost [ 7 ] . Like MSAB , SemiBoost and RegBoost use smoothness regularization but they are limited to binary classification . For comparison , we use the one vs all method to handle the multiclass classification problem with RegBoost and SemiBoost . In our experiments , we use WEKA implementation of the classifiers with default parameter settings [ 5 ] .
Two main steps in Multi SemiAdaBoost are : ( i ) the similarity between examples and ( ii ) sample from the unlabeled examples . There are different distance based approaches to compute the similarity between the data . In this paper we employ the Radial Basis Function ( RBF ) which is used effectively as similarity metric in many domains . For two examples xi and xj the RBF similarity is computed as :
S(xi , xj ) = exp(−xi − xj2
2
σ2
)
( 19 ) where σ is the scale parameter which plays a major role in the performance of the kernel , and should be carefully tuned to the problem [ 13 ] .
In the sampling process only the high confidence data points must be selected for training . Finding the best selection is a difficult problem [ 10 ] . On one hand , selecting a small set of newly labeled examples might lead to slow convergence , and on the other hand , selecting a large set of newly labeled examples may include some poor examples . One possible solution for that is to use a threshold or even a fixed number which is optimized through the training process . Sampling data for labeling is based on the following criterion : for confidence
Pd(xi ) = nu ˆYi − max{Yi,k|Yi,k = ˆYi , k = 1 , , K} i=1( ˆYi − max{Yi,k|Yi,k = ˆYi , k = 1 , , K} ) ( 20 ) where ˆYi is computed from ( 15 ) . Pd(xi ) is viewed as the probability distribution of classes of the example xi , which amounts to a measure of confidence . For the labeled data we use Wi in ( 13 ) as weight :
Winl i=1 Wi
Pd(xi ) =
( 21 ) where xi ∈ L . We use learning from weighed examples as in AdaBoost and in each iteration we select the top 15 % of the unlabeled data based on the weights and add them to the training set .
A . Supervised Base Learner
As mentioned earlier , we assume that the base learner as a black box in the algorithm . It means that the proposed algorithm does not need to know the inner process of the base learner . However , the performance of the method depends on the base learner . We experiment with a Decision Tree learner ( J48 , the Java implementation of C4.5 decision tree classifier ) and Naive Bayes as base classifiers .
B . UCI datasets
We use the UCI datasets for assessing the proposed algorithms . Recently several UCI datasets have been extensively used for evaluating semi supervised learning methods [ 10 ] , [ 7 ] , and [ 9 ] . Sixteen benchmark datasets from the UCI data repository are used in our experiments . For each dataset , 30 percent of the data are kept as test set , and the rest is used as training data . Training data in each experiment are first partitioned into 90 percent unlabeled data and 10 percent labeled data , keeping the class proportions in all sets similar to the original data set . We run each experiment 10 times with different subsets of training and testing data . The results reported refer to the test set .
V . RESULTS
Tables I and II give the results of all experiments . The first column shows the specification of datasets , such as name , number of examples and classes . The second and third columns in these Tables give the performance of supervised multiclass base classifiers ( DT for Decision Trees and Naive Bayes ) and multiclass Adaboost meta classifier using the classifiers as base learner . The columns ASSEMBLE , SMBoost , RegBoost , and SemiBoost show the performance of four semi supervised boosting algorithms ASSEMBLE , SemiMultiAdaBoost , RegBoost , and SemiBoost respectively .
MSAB and Supervised Learning
Multi SemiAdaBoost significantly improves the performance of supervised learning with different base classifiers for nearly all the datasets . Using statistical t test , we observed that Multi SemiAdaBoost improves the performance of J48 and Naive Bayes base classifiers on 16 out of 16 datasets . The results show that Multi SemiAdaBoost is also better than multiclass AdaBoost meta classifier trained using only labeled data . We also observe that the classification models generated by using Multi SemiAdaBoost are relatively more stable than J48 base classifier because of lower standard deviation in classification accuracy .
MSAB and Semi Supervised Boosting Algorithms
Table I and II show that Multi SemiAdaBoost gives better results than all four semi supervised algorithms ASSEMBLE , SemiMultiAdaBoost , RegBoost , and SemiBoost on 14 ( for J48 ) or 13 ( for Naive Bayes ) out of 16 datasets . The improvement of MASB in most of the used datasets are significant and it outperforms the ASSEMBlE and SMBoost on 15 out of 16 datasets , when the base classifier is J48 . The same results can be seen with Naive Bayes classifier .
There are datasets where the proposed algorithms may not significantly improve the performance of the base classifiers . In these cases the supervised algorithm outperforms all the semi supervised algorithms , for example in Tables II the AdaBoost meta classifier performs the same as the proposed methods on Car , Cmc , and Diabetes datasets and outperforms the other methods . These kinds of results emphasize that the unlabeled examples do not guarantee that they always are useful and improve the performance .
VI . CONCLUSION AND DISCUSSION
We presented a boosting algorithm for multiclass semisupervised learning with a novel loss function involving the empirical error on labeled data and the consistency between classifier predictions and similarity information . Specifically we derived the weights for the data and for the classifiers . The resulting algorithm is shown to perform better than existing algorithms for multiclass problems . MultiSemiAdaBoost can use any multiclass supervised learning algorithm as base classifier .
An open problem for future work is how to efficiently find and tune a good similarity function . Another issue is the encoding for the multiclass setting . The encoding that is used here is not the only possibility and it seems interesting to exploit other encoding schemes .
REFERENCES
[ 1 ] K . Bennett , A . Demiriz , and R . Maclin . Exploiting unlabeled data in ensemble methods . In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , pages 289–296 . ACM , 2002 .
Table I : The classification accuracy ( ± s.d ) with 10 % labeled examples and J48 as base learner
DataSets ( #Samples,#Classes ) Balance(625,3 ) Car(1728,4 ) Cmc(1473,3 ) Dermatology(366,6 ) Diabetes(768,2 ) Ecoli(336,8 ) Glass(214,6 ) Iris(150,3 ) Liver(345,2 ) Optdigits(1409,10 ) Sonar(208,2 ) Soybean(686,19 ) Vowel(990,11 ) Wave(5000,3 ) Wine(178,3 ) Zoo(101,7 )
DT
Supervised Learning SAMME 7475±31 78.34±2 4489±42 7699±50 7097±25 6682±85 5094±61 7113±89 5693±76 7652±34 5987±69 4783±46 4257±26 7804±08 6736±98 5666±81
7072±55 7693±16 4203±38 7300±41 6881±50 6588±99 4979±96 7113±89 5486±75 6326±24 6008±92 5100±44 3592±47 7010±17 6736±98 6619±52
ASSEMBLE 7801±35 7897±16 4069±32 8728±42 7198±45 7554±40 5284±71 7708±63 5103±57 7942±26 5798±90 7137±25 2652±32 7695±12 9192±44 8523±42
Semi Supervised Learning
SMBoost 7773±31 7288±13 3904±31 8777±31 6978±50 7741±77 5462±95 8869±81 5117±28 8771±14 605±81 6851±20 2670±25 7553±17 9327±53 8523±42
RegBoost 7534±41 7840±13 4789±29 8710±39 7150±41 7943±38 6021±44 914±62 6201±28 7641±21 7223±51 6710±25 4431±27 7782±13 9410±49 8794±54
SemiBoost 7656±37 7859±10 4846±22 8668±39 7320±27 7741±46 6155±46 9345±56 6246±34 7465±13 7442±23 6149±34 4794±30 7782±13 9473±48 8714±55
MSAB 8190±20 8040±17 4926±27 8837±22 7496±26 8598±34 6638±34 9464±44 6327±41 8028±25 7368±50 7106±24 5702±30 7915±11 9649±33 9000±47
Table II : The classification accuracy ( ± s.d ) with 10 % labeled examples and Naive Bayes as base learner
DataSets Balance Car Cmc Dermatology Diabetes Ecoli Glass Iris Liver Optdigits Sonar Soybean Vowel Wave Wine Zoo
Supervised Learning Naive Bayes 7588±35 775±19 4698±14 8152±74 7345±11 8168±33 50±7.0 8125±55 5539±55 7204±19 6441±71 7000±14 4752±34 8044±14 8456±58 8777±40
SAMME 7671±25 7851±05 4805±13 8152±74 7305±15 7869±63 5021±67 8095±66 5504±72 7057±17 6323±51 6844±71 4802±20 8091±19 8456±58 8777±40
ASSEMBLE 7977±20 7764±19 4735±31 8694±75 7364±16 813±19 4558±99 8749±34 5274±41 846±10 6264±76 7685±12 2582±33 8035±11 9438±22 8944±25
[ 2 ] K . Chen and S . Wang . Semi supervised learning via regularized boosting working on multiple semi supervised assumptions . Pattern Analysis and Machine Intelligence , IEEE Transactions on , 33(1):129–143 , 2011 .
[ 3 ] F . dAlch´e Buc , Y . Grandvalet , and C . Ambroise . Semiinformation supervised marginboost . Advances in neural processing systems , 14:553–560 , 2002 .
[ 4 ] Y . Freund and R . E . Schapire . Experiments with a new boosting algorithm . In ICML , pages 148–156 , 1996 .
[ 5 ] M . Hall , E . Frank , G . Holmes , B . Pfahringer , P . Reutemann , and I . H . Witten . The weka data mining software : an update . SIGKDD Explor . Newsl . , 11:10–18 , November 2009 .
[ 6 ] R . Jin and J . Zhang . Multi class learning by smoothed boosting . Machine learning , 67(3):207–227 , 2007 .
[ 7 ] P . Mallapragada , R . Jin , A . Jain , and Y . Liu . Semiboost : Boosting for semi supervised learning . Pattern Analysis and Machine Intelligence , IEEE Transactions on , 31(11):2000– 2014 , 2009 .
Semi Supervised Learning
SMBoost 7998±25 8042±25 4694±22 8525±45 7509±24 7607±80 5126±87 9107±59 5522±88 8735±20 6558±56 7307±39 3278±35 7994±07 9368±29 9400±27
RegBoost 7824±35 7912±95 482±21 8644±33 7212±31 80±2.7 5701±70 8732±47 5506±70 8054±31 655±51 6777±11 4734±27 8101±17 9369±30 8952±38
SemiBoost 7736±41 7866±21 4909±14 8644±39 7490±19 8074±28 5651±71 9047±63 5575±63 7795±25 7094±58 5777±11 445±22 8183±13 9228±26 8952±38
MSAB 8209±32 8075±18 5111±17 90±2.4 7529±28 8392±32 6008±34 9345±47 6017±57 8300±20 7058±65 743±29 5253±30 8343±06 9517±29 9111±34
[ 8 ] M . Saberian and N . Vasconcelos . Multiclass boosting : Theory In Proc . Neural Information Processing and algorithms . Systems ( NIPS ) , 2011 .
[ 9 ] E . Song , D . Huang , G . Ma , and C . Hung . Semi supervised multi class adaboost by exploiting unlabeled data . Expert Systems with Applications , 2010 .
[ 10 ] J . Tanha , M . van Someren , Disagreement based co training . Intelligence Conference on , pages 803–810 . IEEE , 2011 . and H . Afsarmanesh . In Tools with Artificial IEEE International
( ICTAI ) ,
2011
23rd
[ 11 ] H . Valizadegan , R . Jin , and A . Jain . Semi supervised boosting for multi class classification . Machine Learning and Knowledge Discovery in Databases , pages 522–537 , 2008 .
[ 12 ] J . Zhu , S . Rosset , H . Zou , and T . Hastie . Multi class adaboost .
Statistics and its Interface , 2,:349–360 , 2009 .
[ 13 ] X . Zhu , Z . Ghahramani , and J . D . Lafferty . Semi supervised In learning using gaussian fields and harmonic functions . ICML , pages 912–919 , 2003 .
APPENDIX
Proposition 1 :
Proof : Let the function Yi.ek be equal to 1 , if a data point with Yi belongs to class k . Then ( 9 ) can be rewritten as follows :
Yi.,H t−1(xi ) + βtht(xi) ) min F = C1 exp(− 1 K i=1 nl nu nl i,j∈nu j=1 i=1 k∈l
+ C2
+ C3
1
K−1 e
Suu(xi , xj )
( H t−1 i +βtht i)−(H t−1 j +βtht j )
.ek arranging terms then results in : exp(− 1 K
)exp(− βt K nl i
Yi.H t−1 −βt K − 1 i=1 exp( ht j.ek )
Slu(xi , xj )
Yi.ht i )
H t−1 j
.ek)δ(Yi , ek ) i=1
F1 ≤ nl nu k∈l −1 K − 1 exp( j=1
+ i∈nu
1
K − 1
( H t−1
).ek)e( i j − H t−1 −βt
Pi,kexp
K − 1 nu i=1 k∈l
Yi.ht i)+
F1 ≤ nl
As a results , it gives : −βt K
Wiexp( i=1
Slu(xi , xj)e− 1
K−1 k∈l(Yi.ek)(H t−1 j +βtht j ).ek
+
Suu(xi , xj)exp(
1
K−1 )
( 27 ) ht i.ek
( 28 )
( 29 )
Using the following inequality in ( 9 ) :
( c1c2cn )
1 n ≤ c1 + c2 + + cn n
( 22 ) where
( 23 ) and and then replacing the value of Yi results in the ( 10 ) , where δ is defined in ( 11 ) .
Preposition 2 :
Proof : According to the formulation that we mentioned K−1 ≤ βth(xi).ek ≤ 1 , then multiplying it with earlier , −1 K−1 and using the exponential function gives the following inequality :
1
−1 ( K−1)2 ≤ e e
βt
K−1 h(xi ) ≤ e
1
K−1
( 24 )
Using ( 24 ) for decomposing the third term of ( ? ? ) leads to the following expression :
βt
K−1 ( h(xi)−h(xj )).ek ≤ e e
1
K−1 e
−βt K−1 h(xj ).ek
( 25 )
Replacing the inequality ( 25 ) in the last term of ( ? ? ) gives : exp(− 1 K i=1
F1 ≤ nl nu nl
−βt K − 1 exp( k∈l j=1 i=1
+
+ i,j∈nu
1
K−1 e e k∈l −βt K−1 ht j .ek
Yi.H t−1 i
Yi.ht i )
)exp(− βt K −1 K − 1
H t−1 j
.ek )
Slu(xi , xj)exp( ht j.ek)δ(Yi , ek )
Suu(xi , xj)exp
1
K − 1
( H t−1 i − H t−1 j
).ek
( 26 )
Factoring out the common term e
−βt K−1 h(xj ).ek and re
Wi = exp(
−1 K
Yi.H t−1 i
)
Slu(xi , xj)e( −1
K−1 H t−1 i
.ek)δ(Yi , ek )
, 1
Suu(xi , xj)e
K−1 ( H t−1 j −H t−1 i
).ek
1
K−1 e
( 30 ) nl nu j=1 j=1
Pi,k =
+
Preposition 3 :
Proof : We use the following inequality to decompose the elements of F2 : ∀x ∈ [ −1 , 1 ] exp(βx ) ≤ exp(β)+exp(−β)+βx−1 , Replacing the inequality ( 31 ) in ( 12 ) gives :
( 31 )
F2 ≤ nl nu i=1
+
Wi(e i=1 k∈l
βt
K − 1 ) − nl K−1 − 1 ) − nu
Wi(
βt K i=1
βt
−βt K + e
Pi,k(e
−βt K−1 + e
Yi.ht i ) k∈l
Pi,k ht i.ek
βt K − 1 ( 32 )
Then , re arranging the above inequality gives : i=1 nu
Pi,k(e
−βt K−1 + e
βt
K−1 − 1 )
−βt K + e
Wi(e
Wi(
βt K
Yi.ht i ) +
βt
K − 1 ) + nu i=1 k∈l i=1
Pi,k k∈l βt K − 1 ht i.ek
F2 ≤ nl − nl i=1 i=1
The first term in inequality ( 33 ) is independent of hi . Hence , to minimize F2 , finding the examples with the largest Pi,k and Wi is sufficient at each iteration of the boosting process .
( 33 )
We assign the value Pi,k of selected examples as weight i = max|Pi,k| . To for the newly labeled examples , hence w prove the second part of the proposition , we expand F2 as follows :
Wiexp( i ) +
Pi,kexp( k∈l nu i=1
−βt Yi.ht K −βt K − 1
−βt K − 1 βt
Wiexp(
) +
Wiexp(
( K − 1)2 ) i∈nl i=Yi ht )δ(ht
−βt K − 1 βt
Pi,kexp( i.ek , Pi = k )
Pi,kexp
( K − 1)2
δ(ht i.ek , Pi = k ) i=1 nl nu nu i∈nl ht i=Yi k∈l i=1 i=1 k∈l
F2 =
=
+
+ ht i.ek )
( 34 )
( 35 ) where δ is defined as :
δ(ht i.ek , T ) = fl 1
0 if T is true otherwise
Differentiating the above equation wrt β and equating it to zero , gives : −1 ∂F2 K − 1 ∂β
−β K − 1 exp(
Wi
=
)
1
( K − 1)2 exp(
β
( K − 1)2 )
−1 K − 1
−β K − 1
) exp(
1
( K − 1)2 exp(
βt
( K − 1)2 ) i∈nl ht i=Yi nu i=1 nu k∈l
Wi i∈nl i=Yi ht Pi,kδ(ht i=1 k∈l
+
+
+
Simplifying the above equation results in ( 16 ) . i.ek , Pi = k ) )
Pi,kδ(ht i.ek , Pi = k ) ) = 0
( 36 )
