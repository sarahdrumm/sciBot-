Predicting Directed Links using Nondiagonal Matrix Decompositions
J´erˆome Kunegis
J¨org Fliege
University of Koblenz–Landau , Germany
The University of Southampton , UK kunegis@uni koblenz.de jfliege@sotonacuk
Abstract—We present a method for trust prediction based on nondiagonal decompositions of the asymmetric adjacency matrix of a directed network . The method we propose is based on a nondiagonal decomposition into directed components ( DEDICOM ) , which we use to learn the coefficients of a matrix polynomial of the network ’s adjacency matrix . We show that our method can be used to compute better lowrank approximations to a polynomial of a network ’s adjacency matrix than using the singular value decomposition , and that a higher precision can be achieved at the task of predicting directed links than by undirected or bipartite methods .
Keywords trust ; decomposition into directed components
I . INTRODUCTION
In online social networks , a trust relationship is a unidirectional connection between two persons that denotes the trust of one person in another . Trust relationships are found for instance on dedicated sites such as the product rating site Epinions [ 1 ] . In trust networks , a commonly studied problem is the problem of trust prediction . From a machine learning perspective , the trust prediction problem is a link prediction problem on a social graph with directed edges . The classical link prediction problem , as studied for instance in [ 2 ] applies to undirected networks . Instead , what needs to be addressed in social recommender systems is the directed link prediction problem . To do that , we will use spectral graph theory and consider decompositions of that matrix .
While network mining for undirected networks is a welldeveloped area , the same cannot be said of directed networks . For instance , the directed link prediction problem is itself rarely addressed , let alone using algebraic methods . Thus , our contribution in this paper is a structural trust prediction algorithm , based on a little known family of matrix decompositions , the decomposition into directed components ( DEDICOM ) , which we combine with a novel technique to learn a trust prediction function , and show that the resulting trust predictions outperform other structural trust prediction methods . Our evaluation is performed on five trust network datasets .
II . BACKGROUND
A directed network consists of vertices connected by edges . We denote a directed network as G = ( V , E ) where V is the set of vertices and E is the set of directed edges .
An edge connecting the vertices i , j ∈ V will be denoted ( i , j ) . A trust network is then a directed network in which nodes are persons and edges represent a trust relationship . A major method for solving the link prediction problem is by representing a network as a matrix , which is then manipulated to compute link prediction values . Algebraic graph theory uses ( among others ) the adjacency matrix A to study a graph , defined as Aij = 1 when ( i , j ) is an edge and Aij = 0 otherwise . In the general case of undirected networks , the adjacency matrix A is symmetric , and we can consider its eigenvalue decomposition A = UΛUT , in which U is an orthogonal matrix and Λ is a diagonal matrix . The reason one often considers the eigenvalue decomposition of the adjacency matrix is that allows to compute a power of the adjacency matrix as Ak = UΛkUT , which can be used to define trust prediction methods such as triangle closing , rank reduction , the matrix exponential and the Neumann kernel . This useful property is not found in other decomposition such as nonnegative matrix factorization and probabilistic latent semantic analysis .
A . Symmetric Case
If , in a given directed network with adjacency matrix A , edge directions are not important , we may map the network to an undirected network . The resulting graph is undirected and its adjacency matrix equals A+AT . This new adjacency matrix is normal and thus has the eigenvalue decomposition A + AT = UΛUT , in which the matrix Λ is diagonal and U is orthogonal . Many link prediction functions can be constructed from the eigenvalue decomposition using socalled spectral transformations [ 3 ] . Given the eigenvalue decomposition A = UΛUT , a spectral transformation of A is a function f that can be written as f ( A ) = Uf ( Λ)UT , such as the matrix exponential eA . In particular , any power sum p(A ) can be written as p(A ) = p(UΛUT ) = Up(Λ)UT .
This can be exploited to learn a link prediction function p using curve fitting [ 3 ] . Taking the set of edges in a network , we split it into two disjoint sets . The first set represents known edges , and the second set represents edges that we want to predict . Let both edge sets be represented by the adjacency matrices A and B . We now want to find a power sum p mapping A to B . Using the eigenvalue decomposition A = UΛUT , we can write p(A ) = p(UΛUT ) . Since for every k we can write Ak = ( UΛUT)k = UΛkUT , it follows that p(A ) = Up(Λ)UT and finally p(Λ ) = UTBU .
Since p(Λ ) is necessarily a diagonal matrix we can ignore the nondiagonal entries of UTBU . Therefore , a good power sum p is one that maps the eigenvalues Λkk to the diagonal elements ( UTBU)kk . Finding such a p is a one dimensional curve fitting problem whose size equals the rank of the eigenvalue decomposition , and can thus be solved efficiently .
B . Bipartite Case
The bipartite double cover of a directed graph is an undirected bipartite graph that has twice as many nodes and the same number of edges as the directed graph .
Formally , the directed graph G = ( V , E ) has the bipartite double cover H = ( V × {1} ∪ V × {2} , F ) such that
F = {((i , 1 ) , ( j , 2 ) ) | ( i , j ) ∈ E} .
If A is the adjacency matrix of G , then bip(A ) =
A
AT
¯U is the adjacency matrix of H . We can interpret A as the biadjacency matrix of a bipartite graph whose adjacency matrix is bip(A ) . The eigenvalue decomposition of bip(A ) is then equivalent to the singular value decomposition of A . Given the singular value decomposition A = UΣVT , the eigenvalue decomposition of bip(A ) is given by
+Σ −Σ
¯U
¯U ¯V − ¯V
T
√ with ¯U = U/ 2 . In this decomposition , each singular value σ corresponds to the eigenvalue pair {±σ} . Odd powers of bip(A ) then have the form ( AAT)kA bip(A)2k+1 =
,
( ATA)kAT
¯U ¯V − ¯V 2 and ¯V = V/
√ bip(A ) = where the alternating power ( AAT)kA can be explained by the fact that in the bipartite double cover , a path will follow edges from one vertex set to the other in alternating directions . The same learning method as for the eigenvalue decomposition can be used with the singular value decomposition of the asymmetric matrix A . However , it will not find power sums but instead alternating power sums [ 4 ] . For instance taking the cube of the singular values in the decomposition A = UΣVT leads to the following alternating power of A : UΣ3VT = AATA , which can be interpreted as the act of following an edge in the forward direction , then following an edge in the backward direction , and the following an edge in the forward direction again . In this way , every odd power series applied to the singular values gives the corresponding alternating power of the adjacency matrix . Thus , instead of using the matrix exponential , one may instead use its odd component , the hyperbolic sine [ 4 ] .
III . RELATED ASYMMETRIC MATRIX DECOMPOSITIONS To compute link prediction functions of the asymmetric adjacency matrix A , the methods described in the previous section cannot be applied because they either ignore the direction of edges or reduce the network to its bipartite double cover . Instead , this section reviews simple but ultimately ineffective methods for achieving this : Direct computations of link prediction functions and nonorthogonal eigenvalue decompositions , including personalized PageRank . An effective method for computing link prediction functions of the asymmetric adjacency matrix is given in the next section .
A . Direct Computation of the Matrix Exponential
Instead of using matrix decompositions , we may think of computing a link prediction function directly . For instance , since the matrix exponential is a valid link prediction function , we may try to compute it directly . An overview of methods for achieving this is given in [ 5 ] . In short , the known methods are not scalable to large , sparse matrices , since the exponential will be dense . Therefore , any scalable computation of the matrix exponential must result in a lowrank or otherwise compressed form . According to the state of the art in the cited paper , this can only be achieved by actually computing a matrix decomposition .
B . Nonorthogonal Eigenvalue Decomposition
The adjacency matrix A of an undirected graph is not symmetric in the general case , and therefore the normal eigenvalue decomposition A = UΛUT is not guaranteed to exist . However , as long as A is diagonalizable , we can write A = UΛU−1 , in which U is a matrix whose columns have unit norm ( but are not necessarily orthogonal ) and Λ contains complex eigenvalues .
This model works well as long as the asymmetric matrix A is diagonalizable . In practice , this is not the case . For instance , if a network is acyclic , then all the eigenvalues of A are zero , and the eigenvalue decomposition is not defined . Even though networks such as scientific citation networks are not acyclic , because pairs of papers may cite each other . However , the networks are almost acyclic , and accordingly their eigenvalues are small and their eigenvalue decomposition cannot be used . Another problem with the nonorthogonal eigenvalue decomposition are complex eigenvalues . Because they are complex , it is hard to find a spectral transformation mapping them to new , real eigenvalues , since
, all usual link prediction functions map complex values to complex values . Thus , while the nonorthogonal eigenvalue decomposition can be used in theory to compute spectral transformations of directed matrices , this works badly in practice .
C . Personalized PageRank diagonal degree matrix D defined as Dii =
A personalized variant of PageRank can be defined that gives , for each pair of nodes in a directed network , a score which can be used for link prediction [ 6 ] . Using the j Aij , we can define the right stochastic transition probability matrix as P = D−1A to give the trust prediction function ( I − βP)−1P = P + βP + β2P2 + . . . with 0 < β < 1 . To compute this power sum , we now have the same problem as with the computation of power sums of A itself . Since the matrix P is not symmetric , its eigenvalue decompositions is complex in the general case , and is not suited for the computation of matrix powers .
IV . PROPOSED METHOD
We now present our new method for predicting directed links in trust networks . Our method is based on DEDICOM , a decomposition type of asymmetric matrices , and adds to it a novel way of learning a transformation of it that can be used for link prediction . DEDICOM ( Decomposition into Directed Components [ 7 ] ) refers to a class of matrix decompositions of the form A = UXUT in which the matrix U is orthogonal and the matrix X is not necessarily diagonal . The idea behind DEDICOM is to map all asymmetry of A into the central matrix X , and leave the outer factors U and UT symmetric in the decomposition . This has the advantage that powers of A can be computed easily . Note also that if a decomposition A = YXYT is given in which Y is not orthogonal , it can be transformed to orthogonal form by using the QR decomposition Y = QR , giving A = Q(RXRT)QT , which is in the desired form . DEDICOM was originally developed in 1979 to study directed relations in social groups [ 7 ] . Individual DEDICOM algorithms are described in [ 8 ] , [ 9 ] , [ 10 ] .
A . Computation of DEDICOM
Unlike eigenvalue problems , there is no single simple algorithm for DEDICOM that returns a globally optimal result . Instead , several algorithms are used in the literature , each giving different results that are local optima to the underlying optimization problem .
A useful property of the singular value decomposition is that a truncation of it gives the best possible rank r approximation to the original matrix for all r . Let A ∈ Rn×n be the asymmetric adjacency matrix of a directed graph , and A = UΣVT its singular value decomposition . The following problem seeks a rank r approximation of A : flflA − XYTflflF min
X,Y∈Rn×r
An optimal solution to this problem is given by
X = U• 1:rΣ1:r 1:r , Y = V• 1:r .
The ordering of the singular values in Σ must be chosen such that the largest singular values are kept , to give the best rank r approximation . On the other hand , decompositions into directed components are not as well behaved . In fact , a full rank DEDICOM can be trivially written as
A = UXUT , X = ΣVTU , in which A = UΣVT is the singular value decomposition of A , ie , a decomposition such that U and V are orthogonal matrices and Σ is a diagonal matrix . This construction suggests that a DEDICOM can be computed based on the singular value decomposition . However , a truncation of this decomposition is not the best rank r approximation to A , because in the general case we have the inequality
Σ1:r 1:rVT• 1:rU• 1:r = ( ΣVTU)1:r 1:r .
This is true because we can split the right into the sum
( ΣVTU)1:r 1:r = Σ1:r •(VTU)• 1:r
= Σ1:r 1:r(VTU)1:r 1:r + Σ1:r ( r+1):n(VTU)(r+1):n 1:r , whose right side is not zero in the general case . In fact , since the central matrix X is not diagonal , it is not even evident which of the latent dimensions should be kept . For these reasons , decompositions into directed components must be computed for individual values of r separately .
Left and Right Closed form Solutions : Approximate solutions to DEDICOM can be obtained from the rank r singular value decomposition A = UΣVT [ 7 ] :
A = U(ΣVTU)UT A = V(VTUΣ)VT
We will call these the left and right closed form DEDICOMs ( LEFT and RIGHT ) .
Symmetric Closed form Solution : A closed form solution is given by first computing the rank r singular value decomposition A = UΣVT , and then computing the rankr eigenvalue decomposition of the following symmetric matrix [ 7 ] :
UΣUT + VΣVT = QΛQT
The matrix Q is then used to construct a decomposition into directed components :
A = QXQT , X = QTAQ
We will call this the symmetric closed form solution ( CLO ) , because it is based on the symmetric eigenvalue decomposition .
Iterative Solution : Given a matrix A ∈ Rn×n and a rank 1 ≤ r ≤ n , an optimal decomposition into directed components is given by solutions to the following minimization problem . flflA − UXUTflflF min
U∈Rn×r,X∈Rr×r
This problem can be solved in various ways using iterative algorithms [ 7 ] , [ 8 ] , [ 9 ] . These algorithms are in alternating least squares form , ie they choose a U and a X and then alternatively find a new U and a new X that minimizes the Frobenius norm .
In our method , we used the algorithm described in [ 8 ] , as given in Algorithm 1 . In this algorithm , svd(A ) returns the rank r singular value decomposition of A , and qr(U ) orthogonalizes the columns of U . The term 2aDFU can be omitted to give faster convergence , although the resulting iteration is not guaranteed to converge monotonically anymore . In practice , we found that omitting the term did not make convergence faster . Therefore , all subsequent tests are computed with the term . The precision parameter ε determines the precision of the computed decomposition . We used ε = 10−7 in our experiments .
Input : matrix A ∈ Rn×n , rank 1 ≤ r < n Output : matrices U ∈ Rn×r , X ∈ Rr×r
U , D , V ← svd(A , r ) a ← D11 repeat
U ← AUUTATU + ATUUTAU + 2aDFU U ← qr(U ) D ← D D ← UTAU until D − DF ≤ ε/n
Algorithm 1 : The iterative algorithm ( ITER ) to compute the decomposition into directed components .
Table II
THE NETWORKS USED IN OUR EVALUATION . THE RANK r IS USED IN ALL MATRIX DECOMPOSITIONS OF EACH NETWORK . D : INCLUDES
DISTRUST EDGES . T : INCLUDES EDGE ARRIVAL TIMES .
Network Advogato DBLP Email Epinions Slashdot Twitter
[ 11 ] [ 12 ] [ 13 ] [ 1 ] [ 14 ] [ 15 ] [ 16 ] Wikipedia
Vertices 6,535 12,591 265,214 131,828 79,120 465,017 8,297
Edges 51,397 49,793 420,045 841,372 515,581 835,423 107,071 r 25 14 10 11 11 10 19
Type Trust Citation Communication Trust D T Trust D Trust Trust D T matrix exponential . Let the polynomial p have degree d with coefficients ai : p(X ) = a0I + a1X + a2X2 + . . . + adXd
Thus we have a0I + a1X + a2X2 + . . . + adXd = UTBU .
In vectorized form , we get the linear system
 vec(I )
··· vec(X ) vec(Xd )
 = vec(UTBU ) ,
[ a0a1 . . . ad ] in which vec(X ) transforms the matrix X into a row vector . Since the powers of X and UTBU are known , this is a linear system of n2 equations and d + 1 variables . This system is over specified , and can be solved using the usual methods for systems of linear equations . In analogy with the diagonal case , we can find the best weights ak by solving the following minimization problem : flflflflfl d k=0 akXk
− UTBU flflflflflF
B . Nondiagonal Spectral Transformations min a∈R{0,,d}
In the case of the eigenvalue and singular value decompositions , we have seen that spectral transformations can be learned by curve fitting . In the case of the nonorthogonal eigenvalue decomposition A = UΛU−1 we can use the same technique to learn a power series . In this case however , eigenvalues are complex and special care must be taken .
A given decomposition into directed components A = UXUT can be used to compute a power series p of A as p(A ) = Up(X)UT . Here , X is a full r × r matrix . For the values of r for which a DEDICOM can reasonably computed ( < 100 ) , any power series such as the exponential of X can be easily computed . In order to learn an optimal function p(A ) , we must thus solve p(X ) = UTBU . Since X is not diagonal , the problem does not reduce to a onedimensional curve fitting problem . As a solution , we propose the following method .
Our method can be applied to learn a polynomial p , and does not generalize to other power series such as the
The result are the weights ak of a link prediction function .
V . EVALUATION
Table I gives the list of link prediction functions we use in the evaluations . We use the networks given in Table II for evaluation . All networks are available at konectunikoblenzde
A . Approximation of Prediction Functions
As mentioned before , the best rank r approximation to a given matrix is given by a truncation of its singular value decomposition . In this regard the singular value decomposition is optimal . However , finding low rank approximations of a network ’s adjacency matrix is not a typical task . Instead , a typical task is to compute functions of the adjacency matrix to predict links , given by power series of the adjacency matrix , such as the matrix exponential . If we now evaluate various matrix decompositions at the task of
SPECTRAL TRUST PREDICTION METHODS USED IN THE EVALUATION .
Table I
Name
Decomposition
Spectral transformation
SYM SVD COMP PPR LEFT RIGHT CLO ITER
Eigenvalue decomposition A + AT = UΛUT Singular value decomposition A = UΣVT Complex eigenvalue decomposition A = UΛU−1 D−1A = UΛU−1 Personalized PageRank Left closed form DEDICOM A = U(ΣVTU)UT Right closed form DEDICOM A = V(VTUΣ)VT Symmetric closed form DEDICOM A = UXUT Iterative DEDICOM A = UXUT
Polynomial [ 3 ] Polynomial [ 3 ] Polynomial [ 3 ] ( 1 − βλ)−1λ [ 6 ] Nondiagonal polynomial ( Section IV B ) Nondiagonal polynomial ( Section IV B ) Nondiagonal polynomial ( Section IV B ) Nondiagonal polynomial ( Section IV B ) approximating the exponential of a given matrix , we find that the DEDICOMs perform better than the singular value decomposition . Assume that we want to approximate a link prediction function of A such as the matrix exponential . We are thus solving flfleαA − XYTflflF . min
X,Y∈Rn×r
In the case where A is symmetric , the eigenvalue decomposition and the singular value decomposition coincide , and we can use the eigenvalue decomposition A = UΛUT to write eαA = eαUΛUT = UeαΛUT which itself is the eigenvalue decomposition of eαA and therefore a solution to Equation ( 1 ) is given by
X = UeαΛ , Y = U .
If A is asymmetric however the two decompositions do not coincide . The reason for this is that it is wrong in the general case that eA = UeΣVT when A = UΣVT is the singular value decomposition of A . Instead , we must look for decompositions of the form A = UXU−1 . Since the nonorthogonal eigenvalue decomposition and the DEDICOM type decompositions are of this type , we may use them both . Figure 1 shows the root mean squared error on the approximation to a graph kernel of the asymmetric adjacency matrix of Advogato for the decompositions given in Table I . As expected the singular value decomposition gives the best approximation to A itself . For the exponential of A however , the singular value decomposition is very imprecise and the other decompositions are better . The smallest error is achieved by the iterative DEDICOM .
B . Trust Prediction
We evaluate the performance of our method on the task of trust prediction . Given a directed trust network , the task we are implementing is to predict which directed edges will be added in the future . Given a directed trust network G = ( V , E ) , we split its edge set E into a training set Etraining and a test set Etest . Then , we compute the largest weakly connected component in the training set . All nodes that are not in it are then removed from both the training and the test set . The training set is then split into a source Esource
( a ) Adjacency matrix A
( b ) Matrix exponential e0.1A
Figure 1 . prediction function e0.1A for the Advogato trust network .
Approximation of the adjacency matrix A and of the link and a target set Etarget of edges , which are used to learn the different link prediction functions . When the edges in the networks have known arrival times , we compute the split such that the source set contains edge arrived earlier than the target set , and both of these arrived earlier than the test set . For networks that are unweighted , we augment the test set by a set of vertex pairs ( i , j ) for which ( i , j ) is not an edge . As a comparison with non spectral methods , we include preferential attachment and the friend of a friend count . Let din(i ) and dout(i ) be the indegree and outdegree of vertex i . Then , the preferential attachment prediction score for the vertex pair ( i , j ) is given by dout(i)din(j ) . The friend of a friend count is defined as the number of directed paths of length two between any two nodes .
The other methods are based on matrix decompositions . Let UYVT be one of the matrix decompositions given in Table I , in which U may be equal to V . Then predictions using the corresponding method are computed as Up(Y)VT . In the case of the nonorthogonal eigenvalue decomposition ( COMP ) , we use the real part of the resulting complex prediction values . For the decompositions SYM , SVD , COMP , we use the method described in [ 3 ] to learn a polynomial . For personalized PageRank ( PPR ) , we use the spectral transformation ( 1−βλ)−1λ , using the method of [ 3 ] to learn β . For the DEDICOM methods , we use the method described in Section IV B . The link prediction accuracy is
SVD05101520250660680707207407607808082084Decomposition rank ( r)Root mean squared error ( RMSE)05101520251751818519195220545x 10−45Decomposition rank ( r)Root mean squared error ( RMSE ) ALL AVERAGE PRECISION VALUES . THE BEST METHOD FOR EACH NETWORK IS HIGHLIGHTED IN BOLD .
Table III
Network Advogato DBLP Email Epinions Slashdot Twitter Wikipedia
PREF 0.8988 0.9406 0.9330 0.7885 0.6826 0.9849 0.7699
FOAF 0.8547 0.6873 0.8547 0.8869 0.8565 0.5199 0.8484
SYM 0.8765 0.9483 0.9971 0.8789 0.9044 0.9776 0.8535
SVD 0.8348 0.9594 0.8098 0.8408 0.8429 0.9169 0.8250
COMP 0.7806 0.5945 0.9106 0.8150 0.8232 0.3415 0.7806
PPR 0.7138 0.5058 0.8133 0.8350 0.8249 0.5143 0.7973
LEFT 0.8785 0.8288 0.9242 0.8174 0.8361 0.3378 0.8427
RIGHT 0.8917 0.8013 0.9546 0.8635 0.8429 0.8169 0.7801
CLO 0.9290 0.9501 0.9850 0.8644 0.8844 0.9859 0.8540
ITER 0.9343 0.9162 0.9840 0.8903 0.9013 0.9107 0.8453 measured using the average precision in the following way . We sort all vertex pairs ( i , j ) in the test set by decreasing link prediction scores , and then compute the average precision , based on whether a pair ( i , j ) represents an edge or a nonedge . The results of the evaluation are given in Table III .
1 ) Discussion : As expected , the naive methods LEFT and RIGHT do not perform well . The closed form method CLO and the iterative solution ITER both perform well for different trust datasets . For the Slashdot network , the symmetric eigenvalue decomposition SYM performs better , indicating that Slashdot is more a friendship network than a trust network . The complex eigenvalue decompositions ( COMP and PPR ) do not give accurate results in any network . We interpret this as being due to the complex value of the eigenvalues , which cannot be accurately mapped to other complex values . There are exceptions to these observations : In the Twitter network , the singular value decomposition SVD works better . We interpret this as an indication that the Twitter follower graph has a rather bipartite structure . This is in line with previous results describing Twitter as news media rather than a social network [ 17 ] .
2 ) Other Network Types : In addition to trust networks , two other important types of directed networks exist : citation networks and communication networks . In citation networks such as DBLP , the bipartite approach is best . In communication networks such as the Email dataset , the symmetric approach is best . An explanation of why DEDICOM works in trust networks and not in citation and communication networks is that trust networks display actual directed transitivity in their edge structure . On the other hand , citation networks follow the co citation models , which leads to a bipartite structure , and communication networks are essentially symmetric . A conclusion of these results is that we are able to recommend our method for link prediction in trust networks . The resulting trust prediction algorithm gives better prediction values that either the symmetric , bipartite or complex variant .
Acknowledgments : We thank Yoshio Takane , Tamara G . Kolda , Charles Van Loan and Matthias Thimm . The research was funded by the European Community ’s Seventh Frame Programme under the project ROBUST .
REFERENCES
[ 1 ] P . Massa and P . Avesani , “ Controversial users demand local trust metrics : an experimental study on epinions.com com munity , ” in AAAI , 2005 , pp . 121–126 .
[ 2 ] D . Liben Nowell and J . Kleinberg , “ The link prediction problem for social networks , ” in CIKM , 2003 , pp . 556–559 . [ 3 ] J . Kunegis and A . Lommatzsch , “ Learning spectral graph transformations for link prediction , ” in ICML , 2009 , pp . 561– 568 .
[ 4 ] J . Kunegis , E . W . De Luca , and S . Albayrak , “ The link prediction problem in bipartite networks , ” in IPMU , 2010 , pp . 380–389 .
[ 5 ] C . Moler and C . Van Loan , “ Nineteen dubious ways to compute the exponential of a matrix , twenty five years later , ” SIAM Review , vol . 45 , no . 1 , pp . 1–46 , 2003 .
[ 6 ] F . E . Walter , S . Battiston , and F . Schweitzer , “ Personalised and dynamic trust in social networks , ” in RecSys , 2009 , pp . 197–204 .
[ 7 ] R . A . Harshman , “ Models for analysis of asymmetrical relationships among n objects or stimuli , ” in Proc . First Meeting of for Mathematical Psychology , 1978 . the Psychometric Soc . and the Soc .
[ 8 ] H . A . L . Kliers , J . M . F . ten Berge , Y . Takane , and J . de Leeuw , “ A generalization of Takane ’s algorithm for DEDICOM , ” Psychometrika , vol . 55 , no . 1 , pp . 151–158 , 1990 .
[ 9 ] Y . Takane and Z . Zhang , “ Algorithms for DEDICOM : Acceleration , deceleration , or neither ? ” J . of Chemometrics , vol . 23 , pp . 364–370 , 2009 .
[ 10 ] R . A . Harshman , P . E . Green , Y . Wind , and M . E . Lundy , “ A model for the analysis of asymmetric data in marketing research , ” Marketing Science , vol . 1 , no . 2 , pp . 205–242 , 1982 .
[ 11 ] D . Stewart , “ Social status in an open source community , ” American Sociological Rev . , vol . 70 , no . 5 , pp . 823–842 , 2005 . [ 12 ] M . Ley , “ The DBLP computer science bibliography : Evolution , research issues , perspectives , ” in Proc . Int . Symp . on String Processing and Information Retrieval , 2002 , pp . 1–10 . [ 13 ] J . Leskovec , J . Kleinberg , and C . Faloutsos , “ Graph evolution : Densification and shrinking diameters , ” ACM Trans . Knowledge Discovery from Data , vol . 1 , no . 1 , pp . 1–40 , 2007 .
[ 14 ] J . Kunegis , A . Lommatzsch , and C . Bauckhage , “ The Slashdot Zoo : Mining a social network with negative edges , ” in WWW , 2009 , pp . 741–750 .
[ 15 ] M . D . Choudhury , Y R Lin , H . Sundaram , K . S . Candan , L . Xie , and A . Kelliher , “ How does the data sampling strategy impact the discovery of information diffusion in social media ? ” in ICWSM , 2010 , pp . 34–41 .
[ 16 ] J . Leskovec , D . Huttenlocher , and J . Kleinberg , “ Governance in social media : A case study of the Wikipedia promotion process , ” in ICWSM , 2010 , pp . 98–105 .
[ 17 ] H . Kwak , C . Lee , H . Park , and S . Moon , “ What is Twitter , a social network or a news media ? ” in WWW , 2010 , pp . 591– 600 .
