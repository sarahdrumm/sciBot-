Dimensionality Reduction on Heterogeneous Feature Space
∗ Computer Science Department , University of Illinois at Chicago
Xiaoxiao Shi∗ and Philip Yu∗†
{xshi9 , psyu}@uic.com
† King Abdulaziz University , Jeddah , Saudi Arabia
Abstract—Combining correlated data sources may help improve the learning performance of a given task . For example , in recommendation problems , one can combine ( 1 ) user profile database ( eg genders , age , etc. ) , ( 2 ) users’ log data ( eg , clickthrough data , purchasing records , etc. ) , and ( 3 ) users’ social network ( useful in social targeting ) to build a recommendation model . All these data sources provide informative but heterogeneous features . For instance , user profile database usually has nominal features reflecting users’ background , log data provides term based features about users’ historical behaviors , and social network database has graph relational features . Given multiple heterogeneous data sources , one important challenge is to find a unified feature subspace that captures the knowledge from all sources . To this aim , we propose a principle of collective component analysis ( CoCA ) , in order to handle dimensionality reduction across a mixture of vector based features and graph relational features . The CoCA principle is to find a feature subspace with maximal variance under two constraints . First , there should be consensus among the projections from different feature spaces . Second , the similarity between connected data ( in any of the network databases ) should be maximized . The optimal solution is obtained by solving an eigenvalue problem . Moreover , we discuss how to use prior knowledge to distinguish informative data sources , and optimally weight them in CoCA . Since there is no previous model that can be directly applied to solve the problem , we devised a straightforward comparison method by performing dimension reduction on the concatenation of the data sources . Three sets of experiments show that CoCA substantially outperforms the comparison method . Keywords component ; formatting ; style ; styling ;
I . INTRODUCTION
Multiple related data sources may be available for a given task . An example is user oriented recommendation system . For this task , related data sources can be ( 1 ) user profile database ( as shown in Fig 1(a) ) , ( 2 ) users’ log data ( as shown in Fig 1(b) ) , and ( 3 ) users’ social connections with other users ( as shown in Fig 1(c) ) . Each single data source may not be informative enough to build an accurate model , but the combination of them may provide important knowledge . For instance , in Fig 1(a ) , given that we observe several users’ opinions on the new iPhone , what is Lily ’s opinion about iPhone ? Note that it is difficult to infer the answer just from any one of the databases ( Fig 1(a ) , Fig 1(b ) or Fig 1(c) ) . However , if we combine the knowledge from all three data sources , it can be inferred that Lily has a similar background/behavior to Bob ; thus , she may like iPhone as Bob likes . These mixture types of data are rich in information , and it is desirable to build learning models by using the heterogeneous features collectively . We call this framework heterogeneous learning . More specifically , the task can have ( 1 ) multiple vector based data sources , and ( 2 ) multiple graph relational data sources . The aim is to devise learning algorithms to make good use of all types of features collectively .
Note that one essential challenge of heterogeneous learning is to conduct dimensionality reduction , in order to find a unified feature subspace that captures the information from all sources . However , this is a challenging task . First , it is not clear how to find only one ( unified ) feature subspace , given the heterogeneous features from multiple sources . Second , it is not obvious how to reduce dimensions with multiple graph relational features ( eg , Fig 1(c) ) , and for an even more daunting task with a mixture of vector based features and relational features . To solve the problem , we propose a principle Collective Component Analysis ( CoCA ) . The CoCA principle is to find a feature subspace to maximize the variance of the projected data with the following two constraints .
1 ) First , all vector based feature spaces have to be projected onto the same reduced feature space consensually . For example , Fig 1(a ) and Fig 1(b ) provide two sets of heterogeneous vector based features . They both have to agree on and to be projected to a common feature subspace as Fig 1(d ) , such that their original data structures ( eg , data similarity ) are preserved .
2 ) Second , if there are relational features ( as in Fig 1(c) ) , the connected data are preferred to be similar in the reduced feature space . For example , Lily and Tom do not have connections in the graph Fig 1(c ) . Hence , as in the unified feature subspace Fig 1(d ) , Lily and Bob are more similar than Lily and Tom .
We first analyze the CoCA principle under two special cases . They are cases where ( 1 ) there are multiple vector based data sources ( as shown in Fig 1(a ) and Fig 1(b) ) , and ( 2 ) there are multiple graph relational data sources ( as shown in Fig 1(c) ) . We then combine the two cases , and discuss the situation where we have a mixture of vector based features and relational features . A corresponding optimization problem is devised , and the final solution is obtained via the
( a ) User profile . Will Lily like iPhone ? In this data source , Lily is similar to Bob and Angeli .
( b ) Browsing behaviors
( c ) Social network
( d ) A unified feature subspace
Figure 1 . Heterogeneous feature spaces in computational advertising . It is difficult to analyze Lily ’s opinion on iPhone if we just consider only one of the data sources ( Fig 1(a ) , Fig 1(b ) , or Fig 1(c) ) . However , if we combine the information from all three sources , we can infer Lily ’s opinion based on her similarity ( background , behavior , social connection ) to Bob . A unified feature subspace generated from this example is given in Fig 1(d ) . eigenvectors of certain matrix . Furthermore , given multiple sources , we design a quadratic programming problem to identify sources that are more informative , and optimally weight them to learn a better projection . This is an important step since some of the data sources may contain substantial noise , and it is desirable to reduce their effect . Empirical studies include three sets of real world datasets ranging from handwritten numbers recognition , document classification and terrorist attack detection . Since there was no previous method that can solve the same problem , we devised a straightforward comparison model that performed dimension reduction on the concatenation of the data sources . In all experiments , CoCA substantially outperforms the comparison method by as much as 50 % . In summary , the proposed CoCA model has the following properties .
• Firstly , it finds a unified feature subspace from multiple vector based data sources and multiple graph relational datasets .
• Secondly , it is a robust model since a quadratic programming framework is incorporated to reduce the effect of noise .
• Thirdly , it is an efficient model . It has the same algorithm complexity as traditional dimension reduction algorithms such as PCA , even though ( 1 ) heterogeneous data sources are considered , and ( 2 ) a quadratic programming framework is used . More details can be found at Section III E .
II . PROBLEM FORMULATION
In this section , we first formally define heterogeneous learning , and then introduce the CoCA principle for dimensionality reduction . In heterogeneous learning , the data can be described in heterogeneous feature spaces from multiple sources . Note that in this paper , we mainly study two categories of heterogeneous features : vector based features and graph relational features . For traditional vector based i ∈ Rdj to denote the features , we use column vector x(j ) i th data in the j th source ( or the j th feature space ) whose dimension is dj . In a matrix form , we denote X(j ) ∈ Rdj×m as the set of data in the j th feature space where m is the sample size . Different from vector based features , graph relational features describe the relationships among the data . In other words , they are graphs representing similarity of the data , and the adjacency matrices of the graphs are considered as relational features . Specifically , we denote M(k ) ∈ Rm×m as the adjacency matrix of the k th graph where m is the sample size . Note that the matrix M can be a “ soft ” adjacency matrix such that M(k)(i , j ) is the degree of similarity between the i th data and the j th data in the k th graph . It is large ( closed to 1 ) if the two data are strongly correlated ; otherwise it is 0 . We assume that the features from the same data source are from the same feature space , and hence each data source has a corresponding feature space . Thus , heterogeneous learning is a machine learning scenario where the data have heterogeneous feature spaces . For example , we have p data sources providing vectorbased features X(1 ) , ··· , X(p ) and q data sources providing relational features M(1 ) , ··· , M(q ) . The aim is to derive learning models ( classification or clustering ) by collectively using the p + q feature spaces . up to p
One innate challenge of heterogeneous learning is dimensionality reduction . First , dimensionality reduction is a necessary step to increase the learning accuracy and efficiency . Let us consider a straightforward strategy of utilizing the heterogeneous features by directly joining the p + q sources together . In such a case , the dimension goes i di + qm , which is approximately p + q times of the original dimension ( if assuming that the data size is the same as the feature size ) . In a linear classification model , it means that the number of variables increases by p + q times . Given a fixed number of labeled examples , it may easily encounter the under fitting problem . In other words , the number of labeled examples is far from enough to determine the increasing number of model variables . Hence , it is necessary to reduce its overall dimension . Second , dimensionality reduction is a necessary step to reduce noise . This is because different sources may have different data qualities . Some may contain substantial noise that hurt the
IPHONELikeNameCountryNeutralBobLilyAngeliTomAgeDislike31343558USAUSAUSACanda?FriendsBussinessPartnersColleaguesLilyTomBobNameBobLilyAngeliTomProjected_Feature050408 20 Table I
SYMBOL DEFINITION
Definition
Symbol X(j ) = [ x(j ) M(k ) ∈ Rm×m u(j ) Φ
1 , x(j )
2 ,··· , x(j ) m ] ∈ Rdj×m The dataset in the j th source where x(j ) i ∈ Rdj is the i th data .
The adjacency matrix of the k th relational graph . Projection basis for the j th source ( the j th feature space ) . The projected data with reduced dimension . learning performance . Hence , dimensionality reduction is also a necessary step to dig out informative features to reduce noise . However , it is a very challenging task . The first challenge is how to find a unified reduced feature subspace based on multiple feature spaces . The second challenge is how to use the graph relational features in finding the feature subspace .
We devise a CoCA principle for unsupervised dimensionality reduction of heterogeneous data . The idea is derived from PCA , which aims at maximizing the variance of the projected data to maximize the data separation . In addition , we introduce two more constraints . The first is to force all heterogeneous feature spaces to be projected onto the same subspace . The second is to maximize the similarity of the data with connections as reflected by the relational features . Similar to PCA , the data are normalized to have zero mean , and the aim is to find the orthogonal linear projection basis u(j ) for all heterogeneous feature spaces ( j = 1 , 2,··· ) . The general objective function can be written as follows : max u(1),u(2),··· γ(u ) + c(u ) + αχ(u )
( 1 ) where γ(u ) is the variance of the projected data , which ,··· ]T 1 . depends on the projection basis u = [ u(1)T The second term c(u ) is the similarity among connected data . In other words , if the i th data and the j th data are connected in any of the graph data sources , they should also be similar in the projected space .
, u(2)T
The last term χ(u ) forces the projections from different data sources to be similar . More specifically , if the projections of the i th and j th data sources are Φi and Φj respectively , then the difference between Φi and Φj should be very small . For example , suppose we have two objects and two data sources , denote the projections learned from the two data sources are ˆΦ1 and ˆΦ2 respectively . Further assume that ˆΦ1 = [ 0 , 1]T and ˆΦ2 = [ 1 , 0]T . If we maximize the similarity of the two projected spaces via χ(u ) , we will get the final projections as Φ1 = ˆΦ1 and Φ2 = ˆΦ2 ˆI where ˆI = . As such , both feature spaces are projected into [ 0 , 1]T . It is equally likely to be both projected to [ 1 , 0]T , but the structure ( separation ) of the data is the same . Hence , the term χ(u ) tries to find a consensus feature subspace
0 1
1 0
1The dependency of γ( ) on the data is obvious , and we do not write it explicitly for simplicity . n j=1 across all data sources . Moreover , α is a parameter to control how strongly we want the data to be projected onto a unified subspace . Note that by solving Eq 1 , we can obtain multiple projections u(j ) where j = 1 , 2,··· ( one for each feature space ) . According to the CoCA principle , all these projections are similar to each other . Hence , the final projection is the expected value of the feature subspace :
Φ = E(XT u ) =
1 n
X(j)T u(j )
( 2 ) where n is the number of data sources ( feature spaces ) , and X(j)T u(j ) is the projected space of the j th data source , and Φ ∈ Rm×d is the desired unified feature subspace with reduced dimension d .
III . COLLECTIVE COMPONENT ANALYSIS
In the last section , we introduce collective component analysis principle for unsupervised heterogeneous dimensionality reduction , which is described in Eq 1 . In this section , for the sake of clarity , we first study the CoCA principle on two special cases where ( 1 ) all sources provide only vector based features , and where ( 2 ) all sources provide only graph relational features . We then combine the two cases , and derive a general CoCA algorithm for vector based features and relational features simultaneously . The most important challenges of the problem can be summarized as follows :
• How should we formulate the term χ(u ) to force the similarity of the projected spaces ?
• How should we solve the optimization in Eq 1 , espe cially given that it contains multiple objectives ?
• How should we reduce the impact of the noisy data sources ?
A . CoCA on multiple vector based feature spaces .
We first consider the problem of finding a consensus feature subspace across multiple sources with only vectorbased features . Without loss of generality , we first discuss the case where the desired dimension is one . DEFINITION 1 : Given p data sources with different feature spaces X(1 ) , X(2 ) , ··· , X(p ) , the aim is to find a feature subspace that ( 1 ) maximizes the variance of the projected data , and that ( 2 ) maximizes the “ agreement ” of u(j))2 max u(1),··· ,u(p )
X(j)X(j)T u(j )
The optimization problem in Eq 4 can be written in a matrix form as follows : p j=1
+ α
ωju(j)T p p u(h )
( 6 ) u(j)T
X(j)X(h)T j=1 h=1 st u(j ) = 1 where j = 1,··· , p
Note that at the moment , we set the weight ωj to be 1 p . In Section III D , a separate method is introduced to obtain the optimal ωj with some prior knowledge . Hence , the unknown variables in Eq 6 are the projection bases u(1),··· , u(p ) . We further rewrite the optimization problem into a more compact form : max u uT Xu st u(j ) = 1 where j = 1,··· , p
( 7 ) where u = [ u(1)T
,··· , u(p)T
]T ∈ R(d1+···+dp)×1 and
αX(1)X(2)T ω2X(2)X(2)T
···
αX(p)X(2)T
αX(1)X(p)T αX(2)X(p)T
··· ··· ··· ··· ωpX(p)X(p)T ( 8 )
···

LEMMA 1 : The optimization problem in Eq 3 is equiv
To solve Eq 7 , we first form its Lagrangian formula all projections : max u(1),··· ,u(p ) m p p i=1
ωj
T
( x(j ) i m p j=1
− α
T
( x(j ) i u(j ) ) − ( x(h ) i
T u(h))2 j=1 h=1 i=1 st u(j ) = 1 where j = 1,··· , p
( 3 )
T i i i
T u(j ) = x(h ) where x(j ) u(j ) is the length of the projection of x(j ) on u(j ) , and ∗ is the Euclidean norm to ensure the agreement of projections from different data sources . In other words , although the same object ( eg , xi ) has different features from different feature spaces ( eg , x(j ) and x(h ) ) , it has only one projected value in the final subspace ( ie , to force x(j ) u(h) ) . Furthermore , ωj is the weight of importance of the j th data source ( or the j th feature space ) . Ideally , we should give higher weights for informative sources . At the moment , we set all weights to be equal . In Section III D , a separate method is introduced to obtain optimal weights to better use the informative sources . Note that although the optimization problem in Eq 3 is straightforward , it is not easy to solve . We derive an equivalent optimization problem as follows :
T i i i alent to the following optimization problem : max u(1),··· ,u(p ) m p p i=1
ωj p j=1
+ α i m
T
( x(j ) u(j))2
T u(j))T ( x(h ) i
T u(h ) )
( 4 )
( x(j ) i j=1 h=1 i=1 st u(j ) = 1 where j = 1,··· , p
Proof : The difference between Eq 3 and Eq 4 is the last term in Eq 3 and the last term in Eq 4 . We next prove that they are equivalent . Note that i
− ( x(j ) = − ( x(j ) + 2(x(j )
T i i
T u(j ) ) − ( x(h ) u(j))T ( x(j ) T u(j))T ( x(h ) i T
T i i
T u(h))2 u(j ) ) − ( x(h ) u(h ) ) i
T u(h))T ( x(h ) i
T u(h ) )
( 5 )
Note that the first two terms can be incorporated into the first term in Eq 3 , and the last term is controlled by the parameter α . Thus , maximizing Eq 3 is equivalent to maximize Eq 4 .
···
αX(2)X(1)T
 ω1X(1)X(1)T p
αX(p)X(1)T uT Xu− λ j=1
X = max u
( u(j)T u(j)− 1 ) = uT Xu− λuT u + λp
( 9 ) where λ is the Lagrangian multiplier associated with the constraints . It is well known that the eigenvector corresponding to the largest eigenvalue of X maximizes Eq 9 ( eg , in [ 7] ) . The final projected data is Φ = 1 u(j ) p as discussed in Eq 2 . Note that although we analyze the case where the desired dimension is one , it is not difficult to extend it to multiple dimensions by using the eigenvectors of the top d eigenvalues . j=1 X(j)T p
B . CoCA on multiple relational feature spaces .
For relational features , the aim is to find a subspace that maximizes the similarity among the data with connections . In other words , we maximize the second term in Eq 1 . Note that since there are no vector based features , we do not have to find the linear projection basis u as in Section III A . Instead , we can find the projected data Φ directly . The principle is : if data a and data b are connected , they should have similar projected values Φ(a ) and Φ(b ) . Also note that since the projected data Φ are normalized to have zero mean as in PCA , maximizing the similarity of Φ(a ) and Φ(b ) is approximately equivalent to maximizing their product Φ(a)· q m
T m max
˜ωj j=1 a=1 b=1 st Φ = 1
, in order to ensure that they have the same sign . Hence , the optimization problem is as follows :
Φ(b )
M(j)(a , b ) · Φ(a ) ·
Φ(b )
T
( 10 ) where where X is as defined in Eq 8 of Section III A and

˜X =
 Z1 Zi = X(i ) q
0 ··· 0
0 Z2 ··· 0
··· 0 ··· 0 ··· ··· ··· Zp
˜ωtM(t )
X(i)T
( 14 ) where M(j)(a , b ) is large if data a and data b have strong similarity in the j th relational graph ( eg , equals to 1 if connected , 0 otherwise ) , and Φ(a ) , Φ(b ) are the projected features of a and b respectively . Moreover , ˜ωj is the weight of importance of the j th source . In Section III D , a separate method is introduced to learn the best ˜ωj from the dataset . We rewrite Eq 10 in a matrix form as follows : q
˜ωjM(j ) j=1
ΦT
( 11 ) q max
˜ωjΦM(j)ΦT = Φ j=1 st Φ = 1 eigenvalues of the matrix sumq
C . A general CoCA algorithm .
Hence , the optimal Φ is the set of eigenvectors of the top d j=1 ˜ωjM(j ) .
Section III A and Section III B introduce two special cases of the CoCA principle . In this section , we consider the general scenario where we have a mixture of vectorbased features and relational features . The objective function contains three components as discussed in Eq 1 . The first term maximizes the variance of the projected data . The second term is to ensure a common feature subspace as in Section III A . The last term is to maximize the similarity of the data with connections as in Section III B . Hence , the objective function can be written as follows : m i=1
ωj max u(1),··· ,u(p ) p p p m q p h=1 j=1 j=1 i=1
( x(j ) i
+α
+
T
( x(j ) i u(j))2
T
T i u(h ) ) u(j))(x(h )
˜ωtM(t)(a , b ) ·,x(j ) a j=1 a b t=1 st u(j ) = 1 where j = 1,··· , p u(j) T · x(j ) b
T
T u(j )
( 12 ) where the first term is to maximize the variance , and the third term is to maximize the similarity of connected data , and the second term is to ensure the multiple feature spaces to be projected to the same feature subspace . In a matrix form , it can be written as : max u(1),··· ,u(p ) uT Xu + uT ˜Xu st u(j ) = 1 where j = 1,··· , p
( 13 ) t
T u(j ) . p
Hence , the optimal solution is given by the eigenvectors of the top d eigenvalues of the matrix X + ˆX . The output is the expected value of the projections Φ = 1 p j=1 ( X(j ) + ˆX(j ) )
It is important to note that Eq 12 is a general solution , and it includes Eq 7 in Section III A and Eq 10 in Section III B as special cases . In the experiment , we use Eq 12 to generate the reduced feature spaces since it can handle all three cases : ( 1 ) learning with only vector based features , ( 2 ) learning with only relational features , and ( 3 ) learning with both vector based features and relational features . D . Weight learning . q
In Eq 12 , we set the weights ωj = 1 p and ˜ωt = 1 where 1 ≤ j ≤ p and 1 ≤ t ≤ q . In other words , all sources providing vector based features have the same weight ; all sources with relational features have the equal weight . However , in some cases , some of the sources may be more informative than the others . In this situation , a better strategy is to increase the weights of the informative sources , in order to obtain a more informative feature subspace . In the following , we study how to obtain the optimal weights with the “ must link ” and “ cannot link ” constraints as in semisupervised learning [ 21 ] . We first introduce how to learn optimal weights for data sources with vector based features . The general idea is to first use PCA to obtain projections Φ(k ) = X(k)T u(k ) individually for each source where u(k ) is given by PCA , and then find which feature subspaces Φ(k ) can better satisfy the constraints . To do so , we first denote a m × m constraint matrix C as :
1 if the i th and j th data should have similar projected values 0 no preference −1 if the i th and j th data should have dissimilar projected values
C(i , j ) =
 E(k)(i , j ) = |C(i , j)| ×
( 15 ) In other words , C(i , j ) = 1 if it is a “ must link ” constraint ; C(i , j ) = −1 if it is a “ cannot link ” constraint ; otherwise it is 0 . We further define a truncated similarity matrix E(k ) for the k th source such that
Φ(k)(i)Φ(k)(j )
( 16 ) where |∗| takes the absolute value , and hence |C(i , j)| = 1 iff the i th data and the j th data have certain constraint , regardless it is a “ must link ” or a “ cannot link ” . Furthermore ,
T
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
( 18 ) end end
Φ(k ) = X(k)T u(k ) is the reduced subspace from the k th source given by PCA . Hence , if the i th data and the j th data do not have any constraint , E(k)(i , j ) = 0 ; otherwise
E(k)(i , j ) =,Φ(k)(i) ,Φ(k)(j) T
( 17 )
We call E(k ) “ truncated similarity matrix ” since it only shows the similarity of the data with constraints . Moreover , since Φ(k ) has zero mean as in PCA , E(k)(i , j ) > 0 iff the ith and the j th data have similar projected values of the same sign , and E(k)(i , j ) < 0 iff they are with opposite signs . The ideal truncated similarity matrix should have the same values as the constraint matrix . Hence , given the truncated similarity matrices E(k ) from all sources where k = 1 , 2,··· , the objective function is to find a linear combination strategy that best satisfies the constraint matrix C :
ωkE(k ) − C2
F
ω1,ω2,··· p st p min k=1 k=1
ωk = 1 , ωk ≥ 0 for k = 1,··· , p where ∗2 F is the Frobenius norm . The sources with better truncated similarity matrices are learned to have higher weights , and we use these weights in the general CoCA algorithm Eq 12 . We next solve Eq 18 .
THEOREM 1 : The optimization in Eq 18 is equivalent to the following quadratic programming problem :
ωT ( K)ω − 2vT ω min st p
ω k=1
ωk = 1 , ωk ≥ 0 for k = 1,··· , p
( 19 ) where K is a p× p matrix , and v is a p× 1 vector such that
( E(i))T E(j )
K(i , j ) = tr v(i ) = tr(CE(i ) )
Proof : The optimization function in Eq 18 can be written as k=1 p ωkE(k ) − C2 , p ωkE(k) T , p ωkE(k )
− 2CT , p ωkE(k) + CT C k=1 k=1
F
=tr k=1
=ωT ( K)ω − 2vT ω + tr(CT C )
Note that tr(CT C ) is a constant . Hence , the optimization in Eq 18 is equivalent to that Eq 19 . The quadratic programming problem in Eq 19 can be conveniently solved with standard solvers such as MATLAB . Note that if the given heterogeneous features are relational features M(1 ) , M(2),··· , we can replace Φ(k)Φ(k)T by M(k ) in Eq 17 with some modifications on M(i ) . We have
Input : p sources with vector based features : X(1 ) , X(2),··· , X(p ) , q sources with relational features : M(1 ) , M(2),··· , M(q ) where either p or q can be zero , the parameter α and the desired dimension d , and the constraint matrix C ( optional ) .
Output : The dataset with reduced dimension Φ . if the constraint matrix C is available then
Learn ω ’s and ˜ω ’s as in Eq 19 ; else if p = 0 then
Set ω ’s to be 0 and ˜ω ’s to be 1 q ; else if q = 0 then
Set ω ’s to be 1 p and ˜ω ’s to be 0 ; else
Set ω ’s to be 1 p and ˜ω ’s to be 1 q ; end Construct the matrix X as in Eq 8 ; Construct the matrix ˜X as in Eq 14 ; Find the eigenvectors u of the top d eigenvalues of the matrix X + ˆX ; Construct Φ with u as in Eq 2 .
17 Algorithm 1 : A general algorithm with the CoCA principle to set M(k)(i , j ) = −1 if the i th and the j th data is disconnected . This modification is to reflect the “ cannotlink ” constraints when forming E(k ) . All the formulas and properties still hold for relational features .
E . Algorithm flow and complexity analysis
The CoCA approach is described in Algorithm 1 . It takes the data from various sources as input , and its aim is to construct a projected feature subspace with reduced dimensions . The algorithm has an optional input , which is the constraint matrix C . If C is available , the CoCA algorithm can take advantage of C to learn the optimal weights ( ω ’s ) as in Eq 19 ; otherwise , CoCA assigns equal weights to the sources to learn the target feature subspace . It is important to mention that although the CoCA algorithm applies quadratic programming to obtain the optimal weights , this step can actually be finished in constant time . This is because the complexity of the quadratic programming solver ( eg , MATLAB ) is O(u3 ) where u is the number of unknown parameters , which is equal to the number of sources . In practice , the number of sources is usually not large , and it can be viewed as a constant . Hence , the quadratic programming step can take constant time to finish . The most expensive step of the algorithm is to get the eigenvectors . The complexity of the whole algorithm is
O,dIN where d is the number of eigenvectors desired , I zero entries of the matrix . In the worst case , N = ( as X + ˆX in Eq 12 where is the number of Lanczos iteration steps , and N is the nonj dj)2 j dj is the total dimension of all feature spaces .
IV . EXPERIMENTS
In this section , we analyze the proposed model Collective Component Analysis ( CoCA ) on three sets of datasets with heterogeneous features .
A . Comparison Approaches .
Since there was no previous model that can be directly used to handle the same problem , we compared CoCA with a straightforward strategy . The comparison strategy is to directly join all features together . In other words , given the sources with vector based features X(1 ) , ··· , X(p ) and sources with relational features M(1 ) , ··· , M(q ) , the joined features can be represented as follows :
X = [ X(1)T
,··· , X(p)T
, M(1)T
,··· , M(q)T
]T
( 20 )
Hence , traditional dimensionality reduction methods can be applied on the joined features to obtain a reduced space . Note that CoCA is a general principle to handle heterogeneous features . The principle can be incorporated by most of the unsupervised dimensionality reduction models by adding the projection consensus and graph constraints . In this paper , we take PCA as an example . In other words , the proposed CoCA model adopts PCA ’s strategy to maximize the variance of the data but with the CoCA principle , and PCA was used as baseline that run on the joined dataset ( Eq 20 ) .
B . Evaluation Strategy .
Note that the output of CoCA is the dataset with a reduced feature space . In order to evaluate the quality of the reduced feature space , we performed clustering on the output , and the clustering result was evaluated by normalized mutual information ( NMI ) . Note that NMI equals to zero when clustering is random , and it is close to one when the clustering result is good . Furthermore , k means was chosen as the based clustering algorithm since it is efficient and widely used in practice . Note that k means is sensitive to initial seed selection . Hence , we run k means 10 times on each parameter setting , and report the summarized NMI with mean value and standard deviation . It is also important to note that we propose two versions of CoCA , which are different in how to obtain the weights of the sources . In other words , we have two versions of CoCA depending on whether the optional constraint matrix C in Algorithm 1 is available or not . If the constraint matrix C is not available , we directly assign equal weights to the sources ; otherwise we learn the optimal weights as in Section III D . In the experiment , we
Figure 2 . Handwritten numbers recognition . denote the CoCA without the constraint matrix C as “ CoCA ( equal weights ) ” and the one with C as “ CoCA ( unequal weights ) ” . To construct constraint matrix C , we sampled 30 % data and generated “ must links ” among the data with the same class label , and “ cannot links ” among the data with different labels . We report the results with increasing number of desired dimensions .
C . Handwritten dutch numbers recognition .
The first dataset contains 2000 handwritten numerals ( “ 0 ” – “ 9 ” ) extracted from a collection of Dutch utility maps [ 8 ] . The handwritten numbers are scanned and digitized as binary images . They are represented in terms of the following six feature spaces ( six sources ) with different vector based features : ( 1 ) 76 Fourier coefficients of the character shapes , ( 2 ) 216 profile correlations , ( 3 ) 64 KarhunenLove coefficients , ( 4 ) 240 pixel averages in 2 × 3 windows , ( 5 ) 47 Zernike moments , and ( 6 ) 6 morphological features . All these features are conventional vector based features but in different feature spaces . The aim is to find the optimal projection with reduced dimensions collectively with the six different sources . As mentioned before , we use two versions of CoCA to find the reduced feature spaces . One is to assign equal weights to the sources , and the other is to learn the optimal weights from prior knowledge . The results are presented in Fig 2 . As it can be observed , CoCA outperforms the comparison method substantially . For example , when the dimension is around 8 , the NMI obtained from the two versions of CoCA are around 0.65 while that of the comparison method is only around 045 This shows that CoCA performs better than the intuitive strategy which directly uses the joined features . Between the two versions of CoCA , the unequal weight solution is noticeably better , especially when the dimension is low . This is because some prior knowledge ( “ must links ” and “ cannot links ” ) is used to emphasize those informative sources and reduce noise .
D . Citeseer document dataset .
In this experiment , we studied high dimensional text data . The dataset [ 3 ] is originally from Citeseer with 3312 scientific papers under six different topics : Agents , AI , DB , IR , ML , and HCI . It includes two types of features . One is the vector based features that describe the papers with “ bag of words ” representation . In other words , each
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 2 4 6 8 10 12 14NMIDimensionsCoCA ( equal weights)CoCA ( uneuqal weights)Modified PCA Figure 3 . Citeseer dataset .
Figure 4 . Terrorist attack detection dataset . feature represents one word and the feature value reflects the presence or absence of the word in the document . The relational features come from the citations that link papers together if they have citation relations , which are converted into an undirected graph . Note that in order to examine the weight learning algorithm described in Section III D , we split the vector based “ bag of words ” representation into five different subspaces . This was done by first calculating the mutual information of each feature ( word ) with respect to the class labels ( topics ) , and then sorting them in descending order . We then split the sorted features into five groups where the first group contains the top 20 % of features with the largest mutual information , and the second group is the following 20 % of features , and so on . Furthermore , we randomly generated additional sets of vector based features and relational features by generating matrices with binary entry values ( 0 or 1 ) randomly . As a result , we have 8 sources which contain 6 sets of vector based features and 2 sets of relational features . Note that some of the sources are highly correlated with the class label ( informative ) , while others are very noisy . The weight learning algorithm discussed in Section III D is supposed to be able to identify the informative sources .
The results summarized in 10 runs are presented in Fig 3 with the changing value of dimensions . Since we were dealing with a document dataset that has over 3000 features , we thus extended the maximal dimension up to 100 . As it can be observed , the CoCA with unequal weights has the best performance while the CoCA with equal weights ranks the second . Both approaches beat the comparison modified PCA substantially and significantly . It shows that the na¨ıve strategy is far from enough to build an accurate learner . Instead , CoCA can collectively take different sources into consideration , and obtain a better reduced feature space . Furthermore , some prior knowledge can help CoCA get better weights of the sources , and the results are further improved .
E . Terrorist attack detection .
The third dataset is about terrorist attack [ 3 ] that consists of 1293 different attacks in one of the six labels indicating the type of the attack : arson , bombing , kidnapping , NBCR attack , weapon attack and other attack . Each attack is
( a ) Classification capability
( b ) Parameter sensitivity
Figure 6 . Classification capability and parameter sensitivity analysis described by a binary value vector of attributes whose entries indicate the absence or presence of a feature . There is a total of 106 distinct vector based features , along with two sets of relational features . One set connects the attacks from the same location , and the other connects the attacks if they are planned by the same organization . As in the previous experiment , we split the vector based features into five different groups , and randomly generated one additional set of vector based features and one additional set of relational features . In summary , this dataset contains features from 9 sources including 6 sets of vector based features and 3 sets of relational features .
The results with the changing value of dimensions are presented in Fig 4 . It is clear that the equal weight CoCA beats the modified PCA by as much as 50 % , and the weighted CoCA can beat the comparison model by as much as three times in NMI . Furthermore , when the number of features reaches 15 , the proposed model gets the best performance . Note that selecting the optimal dimension is a nontrivial model selection problem , but it is beyond the scope of the present paper . In practice , one can apply crossvalidation or validation set to obtain the best dimension . F . Discussion
In this section , we aim at analyzing CoCA more in detail in order to answer the following three questions :
1 ) How does the reduced feature space look like geomet rically ?
2 ) Can the data with reduced dimensions work well on other learners ( such as different classifiers in classification problem ) ?
3 ) How does the parameter affect the algorithm ?
Fig 5 plots a subset of the hand written number data in certain feature spaces . The black squares represent the data
0.4 0.5 0.6 0.7 0.8 0.9 1 10 20 30 40 50 60 70 80 90 100NMIDimensionsCoCA ( equal weights)CoCA ( uneuqal weights)Modified PCA 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10 20 30 40 50 60 70 80 90 100NMIDimensionsCoCA ( equal weights)CoCA ( uneuqal weights)Modified PCASVMC45NBNngeLogisticS3VM04045050550606507PCACoCAAccuracy00102030405060708102030405060708090100NMI CoCA(unequalweights ) ( a )
( b )
( c )
( d )
( e )
( f )
( g ) Projected feature space
Figure 5 . Samples of the projected data : fis are the data of the same class , while Xs are the data of another class . belonging to one class , while the red crosses are the data of another class . Specifically , Fig 5(a)∼Fig 5(f ) are the feature spaces from the six sources , and the shown dimensions are the ones that are most correlated with the class labels . In other words , in each source , the two best features ( evaluated by mutual information ) were chosen to show in Fig 5 . It can be clearly observed that some feature spaces are relatively informative ( eg , Fig 5(a ) and Fig 5(c ) help separate part of the data ) , while some are quite noisy ( eg , Fig 5(e) ) . However , the final reduced feature space in Fig 5(g ) best captures the characteristics of the dataset by taking all sources into consideration .
Note that in the experiment section , we evaluated the data with reduced dimensions via clustering algorithm k means . It is also interesting to see how the low dimensional data performs on classification problems . Hence , we used the 10 classes hand written number recognition dataset as an example to analyze the classification capability of the lowdimensional data . To isolate the effect of prior knowledge , we considered the equal weight CoCA . The modified PCA was again used as a comparison method . Several classifiers were trained on the dataset with reduced dimensions and we compared their average accuracy with 10 fold cross validation . The results are shown in Fig 6(a ) with 6 different classifiers SVM , C4.5 , Naive Bayes , Nearest Neighbor , Logistic Regression and the semi supervised SVM . All six experiments show that CoCA better captures the characteristics of the data and obtains a better classification accuracy . For example , when the accuracy given by the modified PCA is only 55 % with SVM , the accuracy of CoCA reaches 65 % , which is 18 % better .
In the proposed algorithm , we have one parameter α that controls how much we want the different feature spaces to be projected onto the same subspace . We use the terrorist dataset as an example and plot the results with different values of α as shown in Fig 6(b ) . It can be observed that when α is small , the result does not look good because the heterogeneous feature spaces are not bounded to find a consensus subspace . When α is around 60 , the result tends to be stable , and we choose α to be 60 in the experiment . However , when α is larger than 75 , the performance drops a little because the learning model encounters overfitting by forcing all feature spaces ( including the noisy ones ) “ vote ” for the consensus subspace . In practice , one can use cross validation or directly use the root mean square error ( the objective function of k means ) as a criterion to choose α . In the experiment , we chose α = 60 .
V . RELATED WORK
There are several areas of related works that we built upon . First , multi view learning ( eg , [ 2 ] , [ 16 ] , [ 15 ] , [ 9 ] , [ 13 ] , [ 12 ] ) is proposed to learn from instances which have multiple views in different feature spaces . For example , in [ 15 ] , a framework is proposed to reconcile the clustering results from different views . In [ 6 ] , a term called consensus learning is proposed . The general idea is to perform learning on each heterogeneous feature space independently and then ensemble the results . Furthermore , [ 1 ] proposes an approach to combine different data sources for recommendation systems . There are mainly two differences between our work with these previous approaches . First , so far as we know , most of the previous works do not consider vector based features and relational features simultaneously . Second , most of these models aim at tackling classification or clustering problem . In this paper , we instead aim at solving dimensionality reduction problem especially for heterogeneous learning , to compress the features from multiple sources .
Another area of related work is collective classification ( eg , [ 17 ] , [ 5 ] ) that aims at predicting the class label from a network . Its key idea is to combine the supervision knowledge from traditional vector based feature vectors , as well as the linkage information from the network . It has been applied to various applications such as part of speech tagging [ 14 ] , classification of hypertext documents using hyperlinks [ 18 ] , etc . Most of these works study the case when there is only one vector based feature space and only one relational feature space , and the focus is how to combine the two . Different from traditional collective classification framework , we consider multiple vector based features and multiple relational features simultaneously . Furthermore , we solve dimensionality reduction problem instead of classification problem since “ the curse of dimension ” is more likely to happen in heterogeneous learning ( increasing number of features from multiple sources ) .
The proposed model is also related to dimensionality reduction and kernel learning ( eg , [ 20 ] , [ 19 ] , [ 10] ) . One of the surveys can be found at [ 4 ] . There are two differences of the current work with previous works . First , most of the previous works do not consider vector based features and
−005000501015−002−0010001002−15−10−50510−20−1001020−1−05005−04−0200204−5051015−005000501015−1−05005−1−050051−004−0020002004006−005000501015−10−5051015−10−50510 graph relational features simultaneously . Second , most of approaches treat all the data sources the same , while the proposed CoCA has a schema to identify informative data source from the noisy ones . Robustness is a very important property of CoCA since the multiple data sources may contain substantial unobserved noise .
Specifically , the work in [ 11 ] also attacks dimensionality reduction on “ heterogeneous ” data . However , it is totally different from the present work . The objective of [ 11 ] is to design an implementation for PCA in the distributed computing environment and the term “ heterogeneous data ” refers to the data stored in different computers . The present work instead aims at developing an algorithm on a new learning schema called heterogeneous learning where “ heterogeneous ” refers to the different feature spaces of the objects , and they can have different physical meanings .
VI . CONCLUSION
In this paper , we study the problem of unsupervised dimensionality reduction given multiple sets of vector based features and multiple sets of graph relational features . We propose a CoCA principle to solve the problem by finding a reduced feature subspace that maximizes the variance of the data , with two constraints . First , all the heterogeneous feature spaces have to be consensually projected on the same subspace . Second , if relational features are available , the data with connections are preferred to be similar in the projected space . An optimization problem is derived from the CoCA principle , and the solution is obtained by solving an eigenvalue problem . Furthermore , we extend the method by using prior knowledge to identify better data sources and construct informative feature subspaces . Three sets of experiments were performed to evaluate CoCA . It can be clearly observed that the proposed CoCA model outperforms the comparison algorithm by as much as 50 % , and the weighted CoCA beats the comparison model even more ( by as much as three times in NMI ) . We also analyze the performance of CoCA on classification problems in the discussion section , where we can observe an 18 % gain over comparison model .
Acknowledgement This work is supported in part by NSF through grants IIS 0905215 , CNS 1115234 , IIS 0914934 , DBI 0960443 , and OISE 1129076 , US Department of Army through grant W911NF 12 1 0066 , Google Mobile 2014 Program and KAU grant .
REFERENCES
[ 1 ] D . Agarwal , B . Chen , and B . Long . Localized factor models for multi context recommendation . In KDD , pages 609–617 , 2011 .
[ 2 ] A . Blum and T . M . Mitchell . unlabeled data with co training . 1998 .
Combining labeled and In COLT , pages 92–100 ,
[ 3 ] Collective Classification Dataset . http://wwwcsumdedu/projects/linqs/projects/lbc/indexhtml
[ 4 ] M . Dash and H . Liu . Dimensionality reduction .
In Wiley Encyclopedia of Computer Science and Engineering , 2008 .
[ 5 ] H . Eldardiry and J . Neville . Across model collective ensem ble classification . In AAAI , 2011 .
[ 6 ] J . Gao , W . Fan , Y . Sun , and J . Han . Heterogeneous source consensus learning via decision propagation and negotiation . In KDD , pages 339–348 , 2009 .
[ 7 ] G . Golub and C . V . Loan . Matrix computation . The Johns
Hopkins University Press Baltimore , 1996 .
[ 8 ] Handwritten Numerals Dataset . http://archiveicsuciedu/ml/datasets/multiple+features
[ 9 ] David R . Hardoon , S´andor Szedm´ak , and John Shawe Taylor . Canonical correlation analysis : An overview with application to learning methods . Neural Computation , 16(12):2639–2664 , 2004 .
[ 10 ] Hideitsu Hino and Noboru Murata . A conditional entropy minimization criterion for dimensionality reduction and multiple kernel learning . Neural Computation , 22(11):2887–2923 , 2010 .
[ 11 ] H . Kargupta , W . Huang , K . Sivakumar , B . Park , and S . Wang . Collective principal component analysis from distributed , heterogeneous data . In PKDD , pages 452–457 , 2000 .
[ 12 ] Hans Peter Kriegel , Peter Kunath , Alexey Pryakhin , and Matthias Schubert . Muse : Multi represented similarity estimation . In ICDE , pages 1340–1342 , 2008 .
[ 13 ] Abhishek Kumar , Piyush Rai , and Hal Daum´e III . CoIn NIPS , pages regularized multi view spectral clustering . 1413–1421 , 2011 .
[ 14 ] J . D . Lafferty , A . McCallum , and F . C . N . Pereira . Conditional random fields : Probabilistic models for segmenting and labeling sequence data . In Proceedings of the International Conference on Machine Learning , 2001 .
[ 15 ] B . Long , P . S . Yu , and Z . Zhang . A general model for multiple view unsupervised learning . In SDM , pages 822–833 , 2008 .
[ 16 ] K . Nigam and R . Ghani . Analyzing the effectiveness and applicability of co training . In CIKM , pages 86–93 , 2000 .
[ 17 ] P . Sen , G . Namata , M . Bilgic , L . Getoor , B . Gallagher , and T . Eliassi Rad . Collective classification in network data . AI Magazine , 29(3):93–106 , 2008 .
[ 18 ] B . Taskar , P . Abbeel , and D . Koller . Discriminative probIn Proceedings of the abilistic models for relational data . Annual Conference on Uncertainty in Artificial Intelligence , 2002 .
[ 19 ] X . Wang , C . Pal , and A . McCallum . Generalized component analysis for text with heterogeneous attributes . In KDD , pages 794–803 , 2007 .
[ 20 ] Z . Zhao and H . Liu . Spectral feature selection for supervised and unsupervised learning . In ICML , pages 1151–1157 , 2007 .
[ 21 ] X . Zhu . Semi supervised learning with graphs . Technical report , Carnegie Mellon University , 2005 .
