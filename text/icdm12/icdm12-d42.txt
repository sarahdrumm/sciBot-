2012 IEEE 12th International Conference on Data Mining 2012 IEEE 12th International Conference on Data Mining
Robust Matrix Completion via Joint Schatten 𝑝 Norm and ℓ𝑝 Norm Minimization
∗ Department of Computer Science and Engineering
, Heng Huang
, Hua Wang
∗ and Chris Ding
†
∗
, Xiao Cai
∗
Feiping Nie ∗
†
Email : feipingnie@gmail.com , xiaocai@mavsutaedu , {heng,chqding}@uta.edu
University of Texas , Arlington , USA
Department of Electrical Engineering & Computer Science
Colorado School of Mines , Golden , Colorado 80401
Email : huawangcs@gmail.com
Abstract—The low rank matrix completion problem is a fundamental machine learning problem with many important applications . The standard low rank matrix completion methods relax the rank minimization problem by the trace norm minimization . However , this relaxation may make the solution seriously deviate from the original solution . Meanwhile , most completion methods minimize the squared prediction errors on the observed entries , which is sensitive to outliers . In this paper , we propose a new robust matrix completion method to address these two problems . The joint Schatten 𝑝 norm and ℓ𝑝 norm are used to better approximate the rank minimization problem and enhance the robustness to outliers . The extensive experiments are performed on both synthetic data and real world applications in collaborative filtering and social network link prediction . All empirical results show our new method outperforms the standard matrix completion methods .
Keywords recommendation system ; matrix completion ; low rank matrix recovery ; optimization ;
I . INTRODUCTION
The prediction of the incomplete observations of an evolving matrix is a challenge of interest in many machine learning applications [ 1 ] , [ 2 ] , [ 3 ] , such as friendship prediction in social network , rating value estimation in recommendation system and collaborative filtering , link prediction in proteinprotein interaction network . All these problem can be seen as a special case of matrix completion where the goal is to impute the missing entries of the data matrix . As one emerging technique of compressive sensing , the problem of matrix completion has been extensively studied on both theory and algorithms [ 4 ] , [ 5 ] , [ 6 ] , [ 7 ] , [ 8 ] , [ 2 ] , and also became popular after the recent concluded million dollar Netflix competition .
The matrix completion methods assume that the values in the data matrix ( graph ) are correlated and the rank of the data matrix is low . The missing entries can be recovered using the observed entries by minimizing the rank of the data matrix , which is an NP hard problem . Instead of solving such an NP hard problem , the researchers minimize the trace norm ( the sum of the singular values of the data matrix ) as the convex relaxation of the rank function . Many recent research has been focusing on solving such trace norm minimization problem [ 9 ] , [ 10 ] , [ 11 ] , [ 12 ] , [ 7 ] . Meanwhile , instead of strictly keeping the values of the observed entries , the recent research work relaxed it to minimize the prediction errors ( using squared error function ) on the observed entries .
Although the trace norm minimization based matrix completion objective is a convex problem with global solution , the relaxation may make the solution seriously deviate from the original solution . It is desired to solve a better approximation of the rank minimization problem without introducing much computational cost . In this paper , we reformulate the matrix completion problem using the Schatten 𝑝 norm . When 𝑝 → 0 , our new objective can approximate the rank minimization better than the trace norm . Moreover , to improve the robustness of matrix completion method , we introduce the ℓ𝑝 norm ( 0 < 𝑝 ≤ 1 ) error function for the prediction errors on the observed entries . Thus , our new objective minimizes the joint Schatten 𝑝 norm and ℓ𝑝norm ( 0 < 𝑝 ≤ 1 ) . When 𝑝 → 0 , our objective is more robust and effective than the standard matrix completion methods , which is a special case of our objective when 𝑝 = 1 . Although our objective function is not a convex problem ( when 𝑝 < 1 ) , we derive an efficient algorithm based on the Alternating Direction Method . With extensive experiments we observe that under a large number of random initializations , our new non convex objective can always find a better convergency result for the matrix completion without introducing much extra computational cost . We evaluate our new method using both synthetic and real world data sets . Six benchmark data sets from collaborative filtering and social network link prediction applications are utilized in our validations . All empirical results show our new robust matrix completion method outperforms the standard missing value prediction approaches . In summary , we highlight the main contributions of this paper as follows : 1 , We propose a new and reasonable objective function for the robust matrix completion task . 2 , Optimizing the objective function is a non trivial problem , we derive an optimization algorithm to solve this problem . 3 , We derive the optimal solution to the problem ( 20 ) , which generalizes a famous soft thresholding result in [ 8 ] , and can be used in many other Schatten 𝑝 norm minimization problems .
1550 4786/12 $26.00 © 2012 IEEE 1550 4786/12 $26.00 © 2012 IEEE DOI 101109/ICDM2012160 DOI 101109/ICDM2012160
1065 566
II . A NEW ROBUST MATRIX COMPLETION use the same 𝑝 for two norms to avoid one more parameter ) :
∑𝑛 𝑖 ∣𝑣𝑖∣𝑝 )
A . Definitions of ℓ𝑝 Norm and Schatten 𝑝 Norm The ℓ𝑝 norm1 ( 0 < 𝑝 < ∞ ) of a vector 𝑣 ∈ ℝ defined as ∥𝑣∥𝑝 = ( ∑𝑛 of 𝑣 . Thus the 𝑝 norm of a vector 𝑣 ∈ ℝ 𝑖 ∣𝑣𝑖∣𝑝 . is ∥𝑣∥𝑝 𝑋 ∈ ℝ
𝑛×1 is 𝑝 , where 𝑣𝑖 is the 𝑖 th element 𝑛×1 to the power 𝑝 The extended Schatten 𝑝 norm ( 0 < 𝑝 < ∞ ) of a matrix
𝑝 = 𝑛×𝑚 is defined as
1
⎛ ⎝min{𝑛,𝑚}∑
⎞ ⎠ 1
𝑝
𝑖=1
∥𝑋∥𝑆𝑝
=
𝜎𝑝 𝑖
,
( 1 ) where 𝜎𝑖 is the 𝑖 th singular value of 𝑋 . Thus the Schatten 𝑝 norm of a matrix 𝑋 ∈ ℝ
𝑛×𝑚 to the power 𝑝 is min{𝑛,𝑚}∑
∥𝑋∥𝑝
𝑆𝑝
=
𝜎𝑝 𝑖 .
( 2 )
𝑖=1
When 𝑝 = 1 , the Schatten 1 norm is the trace norm or nuclear norm . If we define 00 = 0 , then when 𝑝 = 0 , Eq ( 2 ) is the rank of 𝑋 .
B . Robust Matrix Completion Objective
∑ We denote 𝑋Ω = {𝑋𝑖𝑗 ∣(𝑖 , 𝑗 ) ∈ Ω} , and ∥𝑋Ω∥𝑝 𝑝 = ( 𝑖,𝑗)∈Ω ∣𝑋𝑖𝑗∣𝑝 . Suppose we are given the observed values 𝐷Ω = {𝐷𝑖𝑗 ∣(𝑖 , 𝑗 ) ∈ Ω} in a matrix 𝐷 , the matrix completion task is to predict the unobserved values in the matrix 𝐷 . The general rank minimization problem solves the following problem :
∥𝑋Ω − 𝐷Ω∥2 min 𝑋
2 + 𝛾rank(𝑋 ) ,
( 3 )
This problem is NP hard due to the rank function in the objective . In practice , the rank is relaxed to the Schatten 1 norm and then we solve the following relaxed problem :
∥𝑋Ω − 𝐷Ω∥2
2 + 𝛾 ∥𝑋∥𝑆1 min 𝑋
.
( 4 )
However , the relaxation may make the solution deviate seriously from the original solution . Meanwhile , the used squared error is sensitive to outliers . When 𝑝 → 0 , the Schatten 𝑝 norm ∥𝑋∥𝑝 𝑆𝑝 will approximate the rank of 𝑋 [ 13 ] . In this paper , we replace the ∥𝑋∥𝑆1 by ∥𝑋∥𝑝 𝑆𝑝 , the value of 𝑝 can be selected from ( 0 , 1 ] . When 𝑝 is set to a value smaller than 1 , then the resulted problem will better approximate the original problem . We also use the ℓ𝑝 norm ( 0 < 𝑝 ≤ 1 ) as the error function to improve the robustness to outliers in given data [ 14 ] , and propose to solve the following robust matrix completion problem ( we 1When 𝑝 ≥ 1 , ∥v∥𝑝 = 𝑝 strictly defines a norm that satisfies the three norm conditions , while it defines a quasinorm when 0 < 𝑝 < 1 . The quasinorm extends the standard norm in the sense that it replaces the triangle inequality by ∥x + y∥𝑝 ≤ 𝐾 for some 𝐾 > 1 . Because the mathematical formulations and derivations in this paper equally apply to both norm and quasinorm , we do not differentiate these two concepts for notation brevity .
𝑖=1 ∣𝑣𝑖∣𝑝 ) 1
∥x∥𝑝 + ∥y∥𝑝
( ∑𝑛
(
)
𝐽 = ∥𝑋Ω − 𝐷Ω∥𝑝
𝑝 + 𝛾 ∥𝑋∥𝑝
𝑆𝑝 min 𝑋
.
( 5 )
III . PROPOSED ALGORITHM
Solving the problem in Eq ( 5 ) is challenge since both of the terms in Eq ( 5 ) are non smooth and the Schatten 𝑝 norm is somewhat intractable . We use the Augmented Lagrangian Method [ 15 ] , [ 16 ] , [ 17 ] to solve this problem , and focus on the solutions to the related subproblems .
A . Brief Description of Augmented Lagrangian Method
Consider the constrained optimization problem : min
ℎ(𝑋)=0
𝑓 ( 𝑋 )
( 6 )
The algorithm using the Augmented Lagrangian Method ( ALM ) to solve the problem ( 6 ) is described in Algorithm 1
It has been proved that under some rather general conditions , the Algorithm 1 converges Q linearly to the optimal solution [ 17 ] . This property makes ALM very attractive .
Set 1 < 𝜌 < 2 . Initialize 𝜇 > 0 , Λ ; while not converge do 1 . Update 𝑋 by min 𝑋 2 . Update Λ by Λ = Λ + 𝜇ℎ(𝑋 ) ; 3 . Update 𝜇 by 𝜇 = 𝜌𝜇 ;
𝑓 ( 𝑋 ) + 𝜇 2
( ((ℎ(𝑋 ) + 1
𝜇 Λ
( ((2
𝐹
; end while
Algorithm 1 : Algorithm to solve the problem ( 6 ) .
B . Solving Problem ( 5 ) Using ALM
We equivalently rewritten Problem ( 5 ) as : 𝑝 + 𝛾 ∥𝑍∥𝑝
∥𝐸Ω∥𝑝
𝑋,𝐸Ω=𝑋Ω−𝐷Ω,𝑋=𝑍 min
𝑆𝑝
.
( 7 )
According to step 1 in Algorithm 1 , we need to solve the following problem :
∥𝐸Ω∥𝑝 min
𝑋,𝐸Ω,𝑍
𝑆𝑝
𝑝 + 𝛾 ∥𝑍∥𝑝
( (((𝐸Ω − ( 𝑋Ω − 𝐷Ω ) + ( (((2 ( (((𝑋 − 𝑍 +
Σ
.
1 𝜇
𝐹
+
+
𝜇 2
𝜇 2
( (((2
𝐹
1 𝜇
ΛΩ
( 8 )
An accurate , joint minimization with respect to 𝑋 , 𝐸Ω , 𝑍 is difficult and costly , we can use the Alternating Direction Method ( ADM ) [ 18 ] to solve this problem . Specifically , we optimize the problem with respect to one variable when fix
1066567
Set 1 < 𝜌 < 2 . Initialize 𝜇 > 0 , ΛΩ , Σ , 𝐸Ω , 𝑍 ; while not converge do
1 . Update 𝑋 by Eq ( 10 ) ; 2 . Update 𝐸Ω by the optimal solution to problem ( 11 ) ; 3 . Update 𝑍 by the optimal solution to problem ( 12 ) ; 4 . Update ΛΩ by ΛΩ = ΛΩ + 𝜇(𝐸Ω − 𝑋Ω + 𝐷Ω ) , Update Σ by Σ = Σ + 𝜇(𝑋 − 𝑍 ) ; 5 . Update 𝜇 by 𝜇 = 𝜌𝜇 ; end while
Algorithm 2 : Algorithm to solve the problem ( 5 ) . the other two variables , which result in the following three subproblem .
When fix 𝐸Ω , 𝑍 , the problem ( 8 ) is simplified to the following problem :
∥𝑋Ω − 𝑀Ω∥2 min 𝑋
𝐹 + ∥𝑋 − 𝑁∥2 𝐹 , 𝜇 ΛΩ ) and 𝑁 = ( 𝑍− 1 where 𝑀Ω = ( 𝐸Ω +𝐷Ω + 1 𝜇 Σ ) Denote 𝑋 ¯Ω = {𝑋𝑖𝑗 ∣(𝑖 , 𝑗 ) /∈ Ω} , the optimal solution to problem ( 9 ) can be easily obtained by
( 9 )
𝑋Ω =
𝑀Ω + 𝑁Ω
2
, 𝑋 ¯Ω = 𝑁 ¯Ω
( 10 )
When fix 𝑋 , 𝑍 , the problem ( 8 ) is simplified to the following problem :
Note that ℎ(𝑥 ) is a quadratic equation in one variable , its convexity can be easily analyzed . Denote a constant 𝑣 as
(
𝜈 =
𝜆𝑝(1 − 𝑝 )
2
) 1
2−𝑝
.
( 16 )
⎧⎨ ⎩
The optimal solution to problem ( 13 ) can be obtained by
𝑔(𝜈 ) ≥ 0 , 𝑔(−𝜈 ) ≤ 0 𝑥∗ = 0 𝑔(𝜈 ) < 0 , 𝑔(−𝜈 ) ≤ 0 𝑥∗ = arg min𝑥∈{0,𝑥1} ℎ(𝑥 ) 𝑔(𝜈 ) ≥ 0 , 𝑔(−𝜈 ) > 0 𝑥∗ = arg min𝑥∈{0,𝑥2} ℎ(𝑥 ) 𝑔(𝜈 ) < 0 , 𝑔(−𝜈 ) > 0 𝑥∗ = arg min𝑥∈{0,𝑥1,𝑥2} ℎ(𝑥 ) ( 17 ) where 𝑥1 = 0 and 𝑥2 = 0 the root of 𝑔(𝑥 ) = 0 , which can be easily obtained with Newton method initialized at 2𝜈 and −2𝜈 , respectively .
∥𝐸Ω − 𝐻Ω∥2
𝐹 +
1 2
∥𝐸Ω∥𝑝 𝑝 ,
1 𝜇 min 𝐸Ω where 𝐻Ω = 𝑋Ω − 𝐷Ω − 1
𝜇 ΛΩ
When fix 𝑋 , 𝐸Ω , the problem ( 8 ) is simplified to the following problem : min
𝑍
1 2
∥𝑍 − 𝐺∥2
𝐹 +
∥𝑍∥𝑝
𝑆𝑝
,
𝛾 𝜇
( 12 ) where 𝐺 = 𝑋 + 1
𝜇 Σ
The detailed algorithm to solve the problem in Eq ( 5 ) is described in Algorithm 2 .
Subsequently , we derive the optimal solution to subprob lems in Eq ( 11 ) and ( 12 ) , respectively .
C . Solving the Subproblem ( 11 )
Note that the elements {𝑋𝑖𝑗 ∣(𝑖 , 𝑗 ) ∈ Ω} in subproblem ( 11 ) can be decoupled . For each element , we only need to solve the following problem : min
𝑥
1 2
( 𝑥 − 𝑎)2 + 𝜆∣𝑥∣𝑝
( 13 )
Denote the objective function in the problem ( 13 ) by ℎ(𝑥 ) , ie ,
ℎ(𝑥 ) =
( 𝑥 − 𝑎)2 + 𝜆∣𝑥∣𝑝 .
1 2
We can see that the gradient of ℎ(𝑥 ) is
𝑔(𝑥 ) = ℎ′(𝑥 ) = 𝑥 − 𝑎 + 𝜆𝑝∣𝑥∣𝑝−1 sgn(𝑥 ) .
( 14 )
( 15 )
1067568
( 11 )
Similarly , consider the following problem which will be used later : min 𝑥≥0
1 2
( 𝑥 − 𝑎)2 + 𝜆∣𝑥∣𝑝 ,
( 18 )
{ the optimal solution to problem ( 18 ) can be obtained by
𝑔(𝜈 ) ≥ 0 𝑥∗ = 0 𝑔(𝜈 ) < 0 𝑥∗ = arg min𝑥∈{0,𝑥1} ℎ(𝑥 )
( 19 )
D . Solving the Subproblem ( 12 )
We rewrite the subproblem ( 12 ) as follows to simplify the notation . min 𝑋
1 2
∥𝑋 − 𝐴∥2
𝐹 + 𝜆∥𝑋∥𝑝
𝑆𝑝
( 20 )
Denote the SVD of 𝑋 by 𝑋 = 𝑈 Δ𝑉 𝑇 . We prove in the Appendix that for the optimal solution 𝑋 , 𝑈 and 𝑉 are the left and right singular vector of 𝐴 respectively , and the 𝑖 th singular value 𝛿𝑖 is the optimal solution of the following problem :
1 2 min 𝛿𝑖≥0
( 𝛿𝑖 − 𝑎𝑖)2 + 𝜆𝛿𝑝
𝑖
( 21 ) where 𝑎𝑖 is the 𝑖 th singular value of 𝐴 . The optimal solution to the problem ( 21 ) can be obtained according to Eq ( 19 ) . It is interesting to see that when 𝑝 = 1 , the derived solution is exactly the same as in [ 8 ] , and our result extend the result in [ 8 ] to the case of 0 < 𝑝 < 1 .
IV . EXPERIMENTS
In this section , we empirically evaluate the proposed method in both the matrix completion task on synthetic data and two real world applications of collaborative filtering and link discovery in social networks .
For simplicity , the regularization parameter 𝛾 in Eq ( 5 ) is set to 1 in all our experiments .
A . Numerical Results on Synthetic Data
To demonstrate the practical applicability of the proposed method for recovering low rank matrices from their entries , we first perform the following numerical experiments . Following [ 4 ] , for each ( 𝑛 , 𝑟 , 𝑞 ) triplet , where 𝑛 ( we set 𝑚 = 𝑛 ) is the matrix dimension , 𝑟 is the predetermined rank , and 𝑞 is the number of known entries , we experiment with the following procedures . We generate 𝑀 = 𝑀𝐿𝑀 𝑇 𝑅 as suggested in [ 4 ] , where 𝑀𝐿 and 𝑀𝑅 are 𝑛 × 𝑟 matrices with iid standard Gaussian entries . We then select a subset Ω of 𝑞 elements uniformly at random from {(𝑖 , 𝑗 ) : 𝑖 = 1 , . . . , 𝑛 , 𝑗 = 1 , . . . , 𝑛} as known entries , and our goal is to recover the rest entries of 𝑀 given the incomplete input matrix .
The stopping criterion we use for our algorithm in all our experiments is as follows :
( ( ( (𝑋 ( 𝑘 ) − 𝑋 ( 𝑘−1 ) max ( ∥𝑋 𝑘∥𝐹 , 1 )
𝐹
≤ Tol ,
( 22 ) where Tol is a moderately small number . In our experiments , we set Tol = 10−4 .
We measure the accuracy of the computed solution 𝑋sol of our algorithm by the relative error ( RE ) [ 9 ] , which is widely used metric in matrix completion and defined by :
RE := ∥𝑋sol − 𝑀∥𝐹 /∥𝑀∥𝐹 ,
( 23 ) where 𝑀 is the original matrix created in the above process . Study of parameter 𝑝 . Because 𝑝 in Eq ( 5 ) is the most important parameter of the proposed method , we first investigate its impact on our model . We vary the value of 𝑝 in the range of {0.1 , 0.2 . . . , 1} , and perform incomplete matrix recovery as described above . For each value of 𝑝 , we repeat the experiment for 50 times and report the average relative error in Figure 1 , from which we can see that the matrix recovery performance increases when the value of 𝑝 decreases . This result clearly justifies the usefulness of the proposed method to introduce 𝑝 ( < 1) norm in the proposed objective . Upon this preliminary result , unless otherwise specified , we will set 𝑝 = 0.1 in all subsequent experiments .
[ 12 ] and Accelerated Proximal Gradient singular value thresholding ( APG ) method [ 9 ] , which are the most recent methods and have demonstrated superior performances . We implement these two methods using the codes published by the respective authors , and setup their parameters using the same settings as in [ 9 ] . In order for a fair comparison , we perform our experiments using the procedures described above with the same ( 𝑛 , 𝑟 , 𝑞 ) triplet settings as in [ 9 ] . For each triplet setting , we repeat the experiment for 50 times and report the average performance in Table I . The average number of iterations ( denoted as iter ) is also reported in Table I , as well as the ratio ( denoted by 𝑞/𝑑𝑟 ) between the number of known entries and the degree freedom of an 𝑛×𝑛 matrix of rank 𝑟 . Following [ 8 ] , the degree of an 𝑛×𝑛 matrix of rank 𝑟 depends on 𝑑𝑟 = 𝑟 ( 2𝑛 − 𝑟 ) degrees of freedom . As can be seen , 𝑞 is selected to be 3 , 4 and 5 times of the degrees of freedom of the corresponding input matrices .
From Table I , we can see that the proposed method achieves more accurate matrix recovery than those delivered by the two compared methods . Moreover , our method uses substantially less iterations than the other two methods . These results clearly demonstrate the effectiveness of the proposed method in incomplete matrix recovery in terms of both quality and speed .
Comparison with other matrix completion methods on noisy data . Besides performing matrix completion on noiseless data , we also evaluate the proposed method on noisy data . Following [ 9 ] , given a matrix 𝑀 created by the aforementioned procedures , we corrupt it by a noise matrix 𝑁 whose element are iid standard Gaussian variables . Then we carry out the same procedures as before for matrix and 𝑛𝑓 is a completion on 𝑀 + 𝜎𝑁 , where 𝜎 = 𝑛𝑓 given noise factor . We set 𝑛𝑓 = 01 Same as before , the experiment for each ( 𝑛 , 𝑟 , 𝑞 ) triplet setting is repeated for 50 times , and the average results are reported in Table II .
∥𝑀∥𝐹 ∥𝑁∥𝐹
Again , the proposed method performs the best . Most importantly , the relative errors of our method are smaller than the noise level ( 𝑛𝑓 = 0.1 ) , which is consistent with ( or even more accurate than ) the theoretical results established in [ 19 ] and further confirm the correctness of our method .
B . Improved Collaborative Filtering by Our Method
Collaborative filtering is an important topic in data mining and has been widely used in recommendation system , which aims to predict unknown users’ opinions to a set of items upon those known and is often formalized as a matrix completion problem [ 20 ] . In this section , we evaluate the proposed method in the task of collaborative filtering .
Comparison with other matrix completion methods on noiseless data . In order to demonstrate the effectiveness of the proposed method , we compare the performance of the proposed method against the following two matrix completion methods : Fixed Point Continuation ( FPC ) method
Data sets . We perform our experiments using the following data sets .
The MovieLens data contains 10,000,054 ratings and 95,580 tags applied to 10,681 movies by 71,567 users of the online movie recommender service MovieLens , which
1068569
MATRIX COMPLETION PERFORMANCES OF THE COMPARED METHODS ON NOISELESS DATA .
Table I
Unknown 𝑀
𝑛/𝑟
100/10 200/10 500/10
𝑞
𝑞/𝑑𝑟
5666 15665 49471
3 4 5 iter
439 496 491
FPC relative error
1.08e 3 4.66e 4 5.92e 4
APG relative error
1.59e 4 1.19e 4 9.86e 5
Our method relative error
7.47e 5 6.17e 5 5.34e 5 iter
26 25 27 iter
78 74 76
Table II
MATRIX COMPLETION PERFORMANCES OF THE COMPARED METHODS ON NOISY DATA .
Unknown 𝑀
𝑛/𝑟
100/10 200/10 500/10
𝑞
𝑞/𝑑𝑟
5666 15665 49471
3 4 5 iter
442 486 488
FPC relative error
2.45e 2 6.61e 3 8.81e 3
APG relative error
2.36e 3 3.21e 3 2.21e 3
Our method relative error
6.39e 4 5.15e 4 4.92e 4 iter
24 23 21 iter
81 77 73
Table III
PERFORMANCE OF THE COMPARED METHODS MEASURED BY NMAE IN COLLABORATIVE FILTERING . TOP : 20 % RATINGS ARE KNOWN AS TRAINING
SAMPLES ; BOTTOM : 50 % RATINGS ARE KNOWN AS TRAINING SAMPLES .
Data movie 100K movie 1M movie 10M Epinion movie 100K movie 1M movie 10M Epinion
FPC
2.49e 01 2.53e 01 2.38e 01 3.15e 01
2.09e 01 2.13e 01 1.98e 01 2.45e 01
APG
1.94e 01 1.96e 01 1.89e 01 2.37e 01
1.74e 01 1.84e 01 1.77e 01 2.13e 01
PMF
2.26e 01 2.32e 01 2.21e 01 2.75e 01
2.16e 01 2.22e 01 2.11e 01 2.55e 01
WNMF
2.31e 01 2.42e 01 2.36e 01 3.07e 01
2.04e 01 2.02e 01 1.95e 01 2.48e 01
Our ( 𝑝 = 1 ) Our ( 𝑝 = 0.1 )
1.81e 01 1.89e 01 1.78e 01 2.23e 01
1.66e 01 1.71e 01 1.53e 01 2.03e 01
8.92e 2 9.04e 2 8.09e 2 1.75e 1 8.14e 2 8.36e 2 7.94e 2 1.84e 1 has been filtered and refined by GroupLens lab2 as three data sets with the following characteristics ( 1 ) movie 100K : 100,000 ratings for 1682 movies by 943 users ; ( 2 ) movie1M : 1 million ratings for 3900 movies by 6040 users ; ( 3 ) movie 10M : 10 million ratings for 10681 movies by 71567 users .
In addition , we also experiment with Epinion data3 . In Epinion.com , users can assign products or reviewers integer ratings . These ratings and reviews will influence future users when they are deciding whether a product is worth buying or a movie is worth watching . The data set contains 2671
2http://wwwgrouplensorg/ 3http://wwwtrustletorg/wiki/DownloadedEpinionsdataset users and 1375 items with 75308 ratings .
Evaluation metric . In collaborative filtering , some entries of the input matrix are missing , therefore we cannot compute the relative error of the estimated output matrix as we did in Section IV A . Instead , we compute the Normalized Mean Absolute Error ( NMAE ) as in [ 12 ] , [ 21 ] :
∑
( 𝑖,𝑗)∈Γ ∣𝑀𝑖𝑗 − 𝑋𝑖𝑗∣ ∣Γ∣ ( 𝑟max − 𝑟min )
NMAE =
,
( 24 ) where 𝑀𝑖𝑗 denotes the rating given by user 𝑖 to item 𝑗 , 𝑋𝑖𝑗 denotes the predicted rating given by user 𝑖 to item 𝑗 , and 𝑟max and 𝑟min are the upper and lower bounds of the ratings . Because the user ratings in all the data sets range from 1 to
1069570
10x 10−5
)
E R
( r o r r
E e v i t l a e R
9
8
7
6 0
0.2
0.4 p
0.6
0.8
1
Figure 1 . Matrix recovery performance of the proposed method with different values of 𝑝 .
5 , we have 𝑟min = 1 and 𝑟max = 5 . Experimental results . For each data set , we randomly select 20 % and 50 % ratings as known samples , and our task is to recovery the rest ratings from the incomplete input matrices . Besides comparing to the two matrix completion methods used in Section IV A , we also compare the results of our method against two state of the art collaborative filter methods : Probabilistic Matrix Factorization ( PMF ) method and Weighted Nonnegative Matrix Factorization ( WNMF ) method . The former uses probabilistic model , while the latter is devised by extending nonnegative matrix factorization . Both of them have reported promising empirical results . We implement our method for two different settings of 𝑝 = 1 and 𝑝 = 01 For each data set , we run each compared method for 20 times and report the average results in Table III .
The results in Table III show that our method consistently outperforms the compared methods , sometimes very significantly , which provide one more concrete evidence to support the advantage of the proposed method . Moreover , as can be seen in Table III , the results of our method when 𝑝 = 0.1 is much better than those when 𝑝 = 1 . This observation is in accordance with our theoretical analysis in that , the smaller the value of 𝑝 is , the better the Schatten 𝑝 norm approximates the matrix rank ; and the smaller the vale of 𝑝 is , the more robust of our loss function is against outliers .
C . Improved Link Discovery on Social Networks by Our Method
Link discovery on social graphs , which explores the relationships between users , plays a central role in understanding the structure of related social communities . Because most users on a social network only know a very small fraction of users and tag even fewer explicitly , the resulted social graphs are sparse and link discovery is necessary to mine more useful information to better understand a community . In this section , we evaluate the proposed matrix completion method by exploring link discovery problem on social networks .
Data sets . We evaluate the performance of our method using the Wikipedia 2 [ 22 ] and Slashdot 3 [ 23 ] data set . The former contains more than 7,000 users with 103,000 trust links and the latter contains about 80,000 users with 900,000 trust links . The link coverage of these two graphs are as low as 0.21 % and 0.01 % , therefore they are very sparse and skewed due the domination of the non interacting user pairs . To alleviate the data skewness for fair comparison , we select top 2,000 highest degree users from each data set for experiments .
Experimental setups . The goal of our method is to infer the unobservable links in the network . However , due to the lack of ground truth , we have to hide existing links to simulate missing one . In this paper , we emulate to hide 90 %
1070571
PERFORMANCE COMPARISON FOR THE TASK ON LINK DISCOVERY ON SOCIAL NETWORKS
Table IV
Method
CN SVD FPC APG Our ( 𝑝 = 1 ) Our ( 𝑝 = 0.1 )
Precision
0.071 0.088 0.107 0.114 0.135 0.142
Wikipedia 2
Slashdot 3
Recall 0.205 0.211 0.244 0.251 0.301 0.322
Precision
0.058 0.064 0.084 0.089 0.101 0.112
Recall 0.149 0.166 0.189 0.194 0.224 0.245 entries and do the imputation based on the remaining 10 % available information . The reason we hide large percentage of entries is to simulate the fact that most users from social web sites such as Facebook and Linkedin , according to our observation , only explicitly express trust and distrust to a small fraction of peer users considering total number of users .
In order to make prediction , we need a threshold . Empirically , we select as the mean of the available entries as the threshold to convert the prediction into the binary matrix .
Experimental results . Besides comparing the matrix completion methods as in previous subsections , we also compare our method to two link prediction methods which are widely used in the studies of social networks , including common neighbors ( CN ) method [ 24 ] and SVD method [ 25 ] . The settings of the matrix completion methods including ours are same as before . For SVD method , we fine tune the rank by searching the grid of {100 , 200 , . . . , 1 , 000} .
We evaluate the compared methods two standard performance metrics broadly used in statistical learning , including precision and recall . The results of the compared methods on the two data sets are reported in Table IV . A first glance at Table IV shows that the proposed methods again is superior to other compared methods , which demonstrate their effectiveness in the task of link discovery on social networks . Moreover , when 𝑝 = 0.1 , our method achieved better results than those when 𝑝 = 1 , which once again validate the usage of small 𝑝 for matrix completion .
V . CONCLUSIONS
In this paper , we proposed a new robust matrix completion method using joint Schatten 𝑝 norm and ℓ𝑝 norm ( 0 < 𝑝 ≤ 1 ) . When 𝑝 → 0 , the Schatten 𝑝 norm based objective can approximate the rank minimization problem much better than the standard trace norm minimization to achieve better matrix completion results . The ℓ𝑝 norm based error function enhances the robustness of the proposed objective . Both Schatten 𝑝 norm and ℓ𝑝 norm are nonsmooth terms . To solve this difficult optimization problem ,
1071572 we derive the algorithm based on the Alternating Direction Method . Extensive experiments show that under arbitrarily random initializations , our new method can always get better matrix completion results without introducing much extra computational cost . The extensive experiments were performed on both synthetic and real world applications ( collaborative filtering and social network link prediction ) data . All empirical results demonstrate the effectiveness of the proposed approach .
APPENDIX
Denote 𝜎(𝐴 ) as the ordered eigenvalue matrix of 𝐴 , ie a diagonal matrix with the diagonal elements being the ordered eigenvalues of 𝐴 . We have the following two results : two matrices Theorem 1 ( von Neumann ) : For 𝑚×𝑛 , 𝑡𝑟(𝐴𝑇 𝐵 ) ≤ 𝑡𝑟(𝜎(𝐴)𝑇 𝜎(𝐵) ) . 𝐴 , 𝐵 ∈ ℝ Corollary 1 : For any orthonormal matrices 𝑄 , 𝑅 ∈ 𝑛×𝑛 , and any diagonal matrices Σ , Λ ∈ ℝ 𝑛×𝑛 , where the ℝ diagonal elements of Σ , Λ are ordered with the same order , we have 𝑡𝑟(Σ𝑄Λ𝑅 ) ≤ 𝑡𝑟(ΣΛ ) . any
In the following , we suppose all the eigenvalue matrices in compact SVD are the ordered eigenvalue matrix with the same order . ∥𝑋∥𝑝 problem ( 20 ) by 𝑓 ( 𝑋 ) , ie ,
Suppose the compact SVD of 𝑋 is 𝑋 = 𝑈 Δ𝑉 𝑇 , then = 𝑡𝑟Δ𝑝 . Denote the objective function in the
𝑆𝑝
𝑓 ( 𝑋 ) = 𝑓 ( 𝑈 , Δ , 𝑉 ) =
1 2
( (𝑈 Δ𝑉 𝑇 − 𝐴 ( (2
+ 𝜆𝑡𝑟(Δ𝑝 ) ( 25 )
𝐹
As 𝑈 and 𝑉 are the left and right singular vectors of 𝑋 respectively , they are both orthogonal matrices , ie , 𝑈 𝑇 𝑈 = 𝐼 and 𝑉 𝑇 𝑉 = 𝐼 . Denote the Lagrangian function of the problem ( 20 ) by ℒ(𝑈 , Δ , 𝑉 , 𝜂 , 𝜔 ) = 𝑓 ( 𝑈 , Δ , 𝑉 ) − 𝑡𝑟(𝜂𝑇 ( 𝑈 𝑇 𝑈 − 𝐼 ) ) ( 26 ) By setting the derivative of ℒ(𝑈 , Δ , 𝑉 , 𝜂 , 𝜔 ) with respect to 𝑈 and 𝑉 respectively , we have the following two equations :
−𝑡𝑟(𝜔𝑇 ( 𝑉 𝑇 𝑉 − 𝐼) ) .
𝐴𝑉 Δ − 2𝑈 𝜂𝑇 = 0
( 27 )
𝐴𝑇 𝑈 Δ − 2𝑉 𝜔𝑇 = 0
According to Eq ( 28 ) , we have
𝐴𝐴𝑇 𝑈 Δ − 2𝐴𝑉 𝜔𝑇 = 0 ⇒ 𝐴𝐴𝑇 𝑈 Δ − 4𝑈 𝜂𝑇 Δ−1𝜔𝑇 = 0 ⇒ 𝐴𝐴𝑇 𝑈 = 4𝑈 𝜂𝑇 Δ−1𝜔𝑇 Δ−1 ⇒ 𝑈 𝑇 𝐴𝐴𝑇 𝑈 = 4𝜂𝑇 Δ−1𝜔𝑇 Δ−1
( 28 )
( 29 ) ( 30 ) ( 31 ) where Eq ( 29 ) holds according to Eq ( 27 ) . From Eq ( 31 ) we know 4𝜂𝑇 Δ−1𝜔𝑇 Δ−1 must be symmetrical . Suppose the eigenvalue decomposition 4𝜂𝑇 Δ−1𝜔𝑇 Δ−1 = 𝑄Π𝑄𝑇 , then according to Eq ( 30 ) we have
𝐴𝐴𝑇 𝑈 = 𝑈 𝑄Π𝑄𝑇 ⇒ 𝐴𝐴𝑇 𝑈 𝑄 = 𝑈 𝑄Π
( 32 )
Therefore , 𝑈 𝑄 should be the left singular vectors of 𝐴 .
𝐹 = 𝑡𝑟(Δ𝑇 Δ ) + 𝑡𝑟(Σ𝑇 Σ ) − 2𝑡𝑟(𝑋 𝑇 𝐴 ) ≥ then ∥𝑋 − 𝐴∥2 𝑡𝑟(Δ𝑇 Δ ) + 𝑡𝑟(Σ𝑇 Σ)− 2𝑡𝑟(Δ𝑇 Σ ) = ∥Δ − Σ∥2 𝐹 , where the inequality holds based on Theorem 1 . Therefore minimizing 𝑓 ( 𝑈 , Δ , 𝑉 ) is reduced to minimizing Eq ( 38 ) .
ACKNOWLEDGMENT
This research was partially supported by NSF CCF 0830780 , NSF DMS 0915228 , NSF CCF 0917274 , NSF IIS 1117965 .
REFERENCES
[ 1 ] N . Srebro , J . Rennie , and T . Jaakkola , “ Maximummargin matrix factorization , ” NIPS , vol . 17 , pp . 1329–1336 , 2004 .
[ 2 ] J . Rennie and N . Srebro , “ Fast maximum margin matrix factorization for collaborative prediction , ” ICML , 2005 .
On the other hand , According to Eq ( 27 ) , we have
𝐴𝑇 𝐴𝑉 Δ − 2𝐴𝑇 𝑈 𝜂𝑇 = 0 ⇒ 𝐴𝑇 𝐴𝑉 Δ − 4𝑉 𝜔𝑇 Δ−1𝜂𝑇 = 0 ⇒ 𝐴𝑇 𝐴𝑉 = 4𝑉 𝜔𝑇 Δ−1𝜂𝑇 Δ−1 ⇒ 𝑉 𝑇 𝐴𝑇 𝐴𝑉 = 4𝜔𝑇 Δ−1𝜂𝑇 Δ−1
[ 3 ] J . Abernethy , F . Bach , T . Evgeniou , and J . P . Vert , “ A new approach to collaborative filtering : Operator estimation with spectral regularization , ” JMLR , vol . 10 , pp . 803–826 , 2009 .
[ 4 ] E . Cand`es and B . Recht , “ Exact matrix completion via convex optimization , ” Foundations of Computational Mathematics , vol . 9 , no . 6 , pp . 717–772 , 2009 .
( 33 ) ( 34 ) ( 35 ) where Eq ( 33 ) holds according to Eq ( 28 ) . From Eq ( 35 ) we know 4𝜔𝑇 Δ−1𝜂𝑇 Δ−1 must be symmetrical . Suppose the eigenvalue decomposition 4𝜔𝑇 Δ−1𝜂𝑇 Δ−1 = 𝑅Π𝑅𝑇 , then according to Eq ( 34 ) we have
𝐴𝑇 𝐴𝑉 = 𝑉 𝑅Π𝑅𝑇 ⇒ 𝐴𝑇 𝐴𝑉 𝑅 = 𝑉 𝑅Π
( 36 )
Therefore , 𝑉 𝑅 should be the right singular vectors of 𝐴 . then we have
Denote the compact SVD of 𝐴 by 𝐴 = 𝑈 𝑄Π1𝑅𝑇 𝑉 𝑇 , ∑ ≥ ∑ ∑
( (𝑈 Δ𝑉 𝑇 − 𝑈 𝑄Π1𝑅𝑇 𝑉 𝑇 𝑖 ) − 𝑇 𝑟(Δ𝑄Π1𝑅𝑇 ) 𝑖 ) − 𝑇 𝑟(ΔΠ1 ) 𝑖 − 1 2 𝑎2 𝑖 )
( (2 + 𝜆𝑡𝑟Δ𝑝
𝑖
𝑖
𝑓 ( 𝑈 , Δ , 𝑉 ) = 1 2 𝑖 + 𝜆𝛿𝑝 ( 1 2 𝛿2 = 𝑖 + 𝜆𝛿𝑝 ( 1 2 𝛿2 2 ( 𝛿𝑖 − 𝑎𝑖)2 + 𝜆𝛿𝑝 ( 1
=
𝑖
( 37 ) where 𝛿𝑖 ≥ 0 is the 𝑖 th diagonal element of Δ , 𝑎𝑖 is the 𝑖 th diagonal element of Π1 , and the inequality holds according to Corollary 1 . Therefore , minimizing 𝑓 ( 𝑈 , Δ , 𝑉 ) is reduced to minimizing the following problem :
( 𝛿𝑖 − 𝑎𝑖)2 + 𝜆𝛿𝑝 𝑖 ,
1 2
( 38 )
∑ min 𝛿𝑖≥0
𝑖 which can be solved by solving the problem ( 21 ) for each 𝑖 .
It is worth to mentioning that this work and the above proof were finished one year ago , now we find a more concise proof as follows . Suppose the SVD of 𝑋 and 𝐴 are 𝑋 = 𝑈 Δ𝑉 𝑇 and 𝐴 = 𝑄Σ𝑅𝑇 respectively , where Δ , Σ are ordered eigenvalue matrices with the same order .
1072573
[ 5 ] E . J . Candes and T . Tao , “ The power of convex relaxation : Near optimal matrix completion , ” IEEE Trans . Inform . Theory , vol . 56 , no . 5 , pp . 2053–2080 , 2009 .
[ 6 ] B . Recht , M . Fazel , and P . A . Parrilo , “ Guaranteed minimumrank solutions of linear matrix equations via nuclear norm minimization , ” SIAM Review , vol . 52 , no . 3 , pp . 471–501 , 2010 .
[ 7 ] R . Mazumder , T . Hastie , and R . Tibshirani , “ Spectral regularization algorithms for learning large incomplete matrices , ” submitted to JMLR , 2009 .
[ 8 ] J F Cai , E . J . Candes , and Z . Shen , “ A singular value thresholding algorithm for matrix completion , ” SIAM J . on Optimization , vol . 20 , no . 4 , pp . 1956–1982 , 2008 .
[ 9 ] K . Toh and S . Yun , “ An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems , ” Pacific Journal of Optimization , vol . 6 , pp . 615– 640 , 2010 .
[ 10 ] S . Ji and Y . Ye , “ An accelerated gradient method for trace norm minimization , ” ICML , 2009 .
[ 11 ] Y J Liu , D . Sun , and K C Toh , “ An implementable proximal point algorithmic framework for nuclear norm minimization , ” Optimization Online , 2009 .
[ 12 ] S . Ma , D . Goldfarb , and L . Chen , “ Fixed point and bregman iterative methods for matrix rank minimization , ” Mathematical Programming , 2009 .
[ 13 ] F . Nie , H . Huang , and C . H . Q . Ding , “ Low rank matrix recovery via efficient schatten p norm minimization , ” in AAAI , 2012 .
[ 14 ] F . Nie , H . Huang , X . Cai , and C . Ding , “ Efficient and robust feature selection via joint ℓ2,1 norms minimization , ” in NIPS , 2010 .
[ 15 ] M . J . D . Powell , A method for nonlinear constraints in In R . Fletcher , editor , Optimization . minimization problems . Academic Press , London and New York , 1969 .
[ 16 ] M . R . Hestenes , “ Multiplier and gradient methods , ” Journal of Optimization Theory and Applications , vol . 4 , pp . 303–320 , 1969 .
[ 17 ] D . P . Bertsekas , Constrained optimization and lagrange mul tiplier methods . Athena Scientific , 1996 .
[ 18 ] D . Gabay and B . Mercier , “ A dual algorithm for the solution of nonlinear variational problems via finite element approximation , ” Computers & Mathematics with Applications , vol . 2 , no . 1 , pp . 17–40 , 1969 .
[ 19 ] E . Candes and Y . Plan , “ Matrix completion with noise , ” Arxiv preprint arXiv:0903.3131 , 2009 .
[ 20 ] R . Salakhutdinov and A . Mnih , “ Probabilistic matrix factor ization , ” in NIPS , 2008 .
[ 21 ] Q . Gu , J . Zhou , and C . Ding , “ Collaborative filtering : Weighted nonnegative matrix factorization incorporating user and item graphs , ” in SDM , 2010 .
[ 22 ] J . Leskovec , D . Huttenlocher , and J . Kleinberg , “ Predicting positive and negative links in online social networks , ” in WWW , 2010 .
[ 23 ] J . Leskovec , K . Lang , A . Dasgupta , and M . Mahoney , “ Community structure in large networks : Natural cluster sizes and the absence of large well defined clusters , ” Internet Mathematics , vol . 6 , no . 1 , pp . 29–123 , 2009 .
[ 24 ] M . Newman , “ Clustering and preferential attachment in growing networks , ” Physical Review E , vol . 64 , no . 2 , p . 025102 , 2001 .
[ 25 ] D . Billsus and M . Pazzani , “ Learning collaborative informa tion filters , ” in ICML , 1998 .
1073574
