Semi Supervised Sparse Metric Learning Using
Alternating Linearization Optimization
Wei Liu
Shiqian Ma
Columbia University New York , NY , USA wl2223@columbia.edu
Columbia University New York , NY , USA sm2756@columbia.edu
Dacheng Tao
Nanyang Technological
University Singapore dctao@ntuedusg
Jianzhuang Liu
The Chinese University of
Hong Kong
Shenzhen Institutes of Advanced Technology Chinese Academy of
Sciences , China jzliu@iecuhkeduhk
Peng Liu
Barclays Capital
New York , NY , USA liup1024@gmail.com
ABSTRACT In plenty of scenarios , data can be represented as vectors and then mathematically abstracted as points in a Euclidean space . Because a great number of machine learning and data mining applications need proximity measures over data , a simple and universal distance metric is desirable , and metric learning methods have been explored to produce sensible distance measures consistent with data relationship . However , most existing methods suffer from limited labeled data and expensive training . In this paper , we address these two issues through employing abundant unlabeled data and pursuing sparsity of metrics , resulting in a novel metric learning approach called semi supervised sparse metric learning . Two important contributions of our approach are : 1 ) it propagates scarce prior affinities between data to the global scope and incorporates the full affinities into the metric learning ; and 2 ) it uses an efficient alternating linearization method to directly optimize the sparse metric . Compared with conventional methods , ours can effectively take advantage of semi supervision and automatically discover the sparse metric structure underlying input data patterns . We demonstrate the efficacy of the proposed approach with extensive experiments carried out on six datasets , obtaining clear performance gains over the state of the arts .
Categories and Subject Descriptors H28 [ Database Management ] : Database ApplicationsData Mining ; H.3 [ Information Storage and Retrieval ] : Information Search and Retrieval
General Terms Algorithms
Keywords Metric learning , semi supervised sparse metric learning , sparse inverse covariance estimation , alternating linearization
1 .
INTRODUCTION
Vectored data frequently occur in a variety of fields , which are easy to handle since they can be mathematically abstracted as points residing in a Euclidean space . An appropriate distance metric in this space spanned by input data vectors is quite demanding for a great number of applications including classification , clustering and retrieval . Two most commonly used distance metrics are Euclidean distance and Mahalanobis distance , of which the former is independent of the input data while the latter is related to second order statistics of the input data . In practice , we need to seek distance metrics suitable for the requirements of different tasks .
In the context of classification , distance metrics are frequently applied in concert with kNN classifiers . As such , the goal of metric learning towards kNN classification is to keep the distances among nearby points as small as possible and push the differently labeled neighbors out of the neighborhood of any of these points . Neighbourhood Components Analysis ( NCA ) [ 11 ] and its seminal work Maximally Collapsing Metric Learning ( MCML ) [ 10 ] emphasize that the target metric should support tight neighborhoods and even reach zero distances within all neighborhoods . Like the notion of SVMs , Large Margin Nearest Neighbor classification ( LMNN ) [ 28 ] not only narrows the distance gap within all neighborhoods , but also maximizes the soft margin of distances over each neighborhood .
As for clustering , metric learning usually cooperates with constrained clustering , namely , semi supervised clustering [ 26][3][15 ] where some background knowledge about data proximities is given beforehand . Specifically , two kinds of
1139 pairwise links , ie , must links and cannot links , are given . The must links indicate that two data points must be in the same cluster , while the cannot links require that two data points not be grouped into the same cluster . Therefore , the purpose of metric learning applied to semi supervised clustering is to minimize the distances associated with mustlinks and simultaneously maximize the distances associated with cannot links . There have been some works [ 29][4][8][25 ] which are engaged in learning metrics towards better clusterings .
In the field of content based image retrieval ( CBIR ) , choosing appropriate distance metrics plays a key role in establishing effective systems . Regular CBIR systems usually adopt the Euclidean distance measure for images represented in a vector form . Unfortunately , Euclidean distance is generally not effective enough in retrieving relevant images . A main reason stems from the well known semantic gap between low level visual features and high level semantic concepts [ 24 ] . The commonly used relevance feedback scheme [ 23 ] may remedy the semantic gap issue , which produces , aided by users , a set of pairwise constraints about relevance ( similarity ) or irrelevance ( dissimilarity ) between two images . These constraints along with involved image examples are called log data . Then the key to CBIR is to find an effective way of utilizing the log data in relevance feedback so that the semantic gap can be successfully reduced . A lot of ways have been studied to utilize the log data to boost the performance of CBIR . In particular , one can use a metric learning technique devoted to semi supervised clustering for tackling CBIR since these relevance constraints are essentially must links and cannot links . The recent works [ 2][14][18 ] have recommended learning proper distance metrics for image retrieval tasks .
In this paper , we pose metric learning under the semisupervised setting where only a few pairwise constraints including similar and dissimilar exist and most data instances are not involved in such constraints . We propose a novel metric learning technique called semi supervised sparse metric learning ( S3ML ) . The major features of S3ML include : 1 ) it is capable of propagating scarce pairwise constraints to all data pairs ; 2 ) it generates a sparse metric matrix which coincides with the sparse spirit of feature correlations in the high dimensional feature space ; and 3 ) it is quite efficient by using the alternating linearization method in contrast to existing metric learning approaches using expensive optimizations such as semidefinite programming . The proposed S3ML technique has widespread applicability without being limited to particular backgrounds . Quantitative experiments are performed for classification and retrieval tasks , uncovering the effectiveness and efficiency of S3ML .
The remainder of this paper is arranged as follows . Section 2 reviews the related work on recent metric learning . Section 3 describes and addresses the semi supervised metric learning problem . Section 4 presents the S3ML algorithm by using the alternating linearization optimization method . Section 5 validates the efficacy of the proposed S3ML through extensive experiments . Section 6 includes conclusions .
2 . RELATED WORK
In recent years , there are some emerging research interests in learning data representations in some intrinsic lowdimensional space embedded in the ambient high dimensional space such that regular Euclidean distance is more meaning ful in the low dimensional space . The early efforts are learning linear representations by Principal Component Analysis ( PCA ) and learning nonlinear representations by manifold learning . However , these methods are unsupervised and loosely related to the distance outcome . This paper investigates distance metric learning which is vital to a lot of machine learning and data mining applications . The recent metric learning research can be classified into three main categories as follows .
2.1 Supervised Metric Learning
The first category is supervised metric learning approaches for classification where distance metrics are usually learned from training data associated with explicit class labels . The representative techniques include Neighbourhood Components Analysis ( NCA ) [ 11 ] , Maximally Collapsing Metric Learning ( MCML ) [ 10 ] , and metric learning for Large Margin Nearest Neighbor classification ( LMNN ) [ 28 ] . Nevertheless , the performance of these supervised approaches rests highly on the amount of labeled data that are often practically difficult and expensive to gather . Moreover , all of them request nontrivial optimizations such as semidefinite programming [ 5 ] , which is inefficient for real world datasets .
2.2 Weakly Supervised Metric Learning
Our work is closer to the second category of weakly supervised metric learning which learns distance metrics from pairwise constraints present in input data , or known as side information [ 29 ] . It is manifest that such side information is weaker than exact label information . In detail , each constraint indicates whether two data points are relevant ( similar ) or irrelevant ( dissimilar ) in a particular learning task . A well known metric learning method with these constraints was proposed by Xing et al . [ 29 ] , which casts the learning task into a convex optimization problem and applies the generated solution to data clustering . Following this work , there are several emerging metric learning techniques in the “ weakly supervised ” direction . For instance , Relevance Component Analysis ( RCA ) learns a global linear transformation by exploiting only the equivalent ( relevant ) constraints [ 2 ] . Recently , Information Theoretic Metric Learning ( ITML ) [ 8][7 ] expresses the weakly supervised metric learning problem as a Bregman optimization problem where the pairwise constraints are treated as inequality constraints .
2.3 Sparse Metric Learning
In [ 21 ] , to speedup the training time of metric learning , ℓ1 regularization is incorporated into the original non sparse metric learning objective , resulting in a much faster learning procedure : Sparse Distance Metric Learning ( SDML ) . Besides , the sparsity of desirable metric matrices makes sense since the Mahalanobis matrix is nearly sparse under the high dimensional data space . This sparsity spirit stems from the weak correlations among different feature dimensions in the high dimensional feature space because most distinct features are measured by distinct mechanisms and relatively independent of each other . In another perspective , [ 22][30 ] attempt to learn a low rank sparse metric matrix by inducing sparsity to a low rank linear mapping whose outer product constitutes the sparse metric . Nonetheless , learning lowrank sparse metrics often implicates complex optimization
1140 which has a large computational cost and is sensitive to the choice of the low rank as well .
3 . SEMI SUPERVISED METRIC LEARNING
In this section , we investigate the semi supervised metric learning problem [ 14 ] which takes scarce pairwise constraints as input and , importantly , exploits abundant unlabeled data that are not involved in these constraints . Consequently , semi supervised metric learning supplements the aforementioned weakly supervised metric learning due to the merit of accessing unlabeled data . Different from standard semi supervised learning [ 31 ] , semi supervised metric learning does not need any class labels , so it can be applied to a broad spectrum of applications such as semi supervised classification , semi supervised clustering , relevance feedback based information retrieval , etc .
3.1 Problem Description
Assume that we are given a set of n data points X = i=1 ⊆ Rm and two sets of pairwise constraints among
{xi}n these data points :
S = {(i , j ) | xi and xj are judged to be similar} D = {(i , j ) | xi and xj are judged to be dissimilar} ,
( 1 ) where S is the set of similar pairwise constraints , and D is the set of dissimilar pairwise constraints . Each pairwise constraint ( i , j ) indicates if two data points xi and xj are relevant or irrelevant judged by users under some application context . Note that it is not necessary for all the points in X to be involved in S or D .
For any pair of points xi and xj , let d(xi , xj ) denote the distance between them . To compute the distance , let M ∈ Rm×m be a symmetric metric matrix . We can then express the distance measure as follows : dM ( xi , xj ) = kxi − xjkM =p(xi − xj)⊤M ( xi − xj ) . ( 2 )
In practice , the symmetric matrix M is a valid metric if and only if it satisfies the non negativity and the triangle inequality conditions . In other words , M must be positive semidefinite , ie , M 0 . Generally , the matrix M parameterizes a family of Mahalanobis distances on the vector space Rm . As an extreme case , when setting M to be identity matrix I ∈ Rm×m , the distance in eq . ( 2 ) becomes the common Euclidean distance .
Definition 1 . The goal of semi supervised metric learning is to learn an optimal symmetric matrix M ∈ Rm×m from a collection of data points X on a vector space Rm together with a set of similar pairwise constraints S and a set of dissimilar pairwise constraints D , which can be formulated as the following optimization prototype : min M 0 g(M , X , S , D )
( 3 ) where M is maintained to be positive semidefinite and g(· ) is a proper objective function defined over the given data and constraints .
Given the above definition , the strategy to attack metric learning is to first design an appropriate objective function g and then seek an efficient algorithm to minimize it . In the following , we discuss some principles for formulating reasonable optimization models . Importantly , we emphasize that it is critical for solving real world metric learning problems to avoid overfitting .
Min Max Principle . One common principle for metric learning is to minimize the distances among the data points with similar constraints and meanwhile to maximize the distances among the data points with dissimilar constraints . We refer to it as a min max principle . Many existing metric learning works such as [ 29][28][14 ] can be interpreted via this min max principle . Immediately , we can define g simply based on this principle : g(M ) = X(i,j)∈S kxi − xjk2
M − γ X(i,j)∈D kxi − xjk2
M ,
( 4 ) where γ > 0 is the trade off parameter . Although the above objective function has been shown effective for some clustering tasks , it may be unsuitable for the applications in which the pairwise constraints are scarce and most data instances are not involved in any constraint . As a matter of fact , optimizing the above g is likely to overfit the two limited constraint sets S and D . Hence , we should incorporate all data in X into the design of g .
To facilitate the following derivations , we assume that there exits a linear mapping U ⊤ : Rm → Rr ( U = [ u1 , . . . , ur ] ∈ Rm×r ) such that M = U U ⊤ . We require that u1 , . . . , ur be linearly independent so that r is the rank of the metric matrix M . Then the distance under M between two inputs can be computed as : kxi − xjkM =p(xi − xj)⊤M ( xi − xj ) =p(xi − xj)⊤U U ⊤(xi − xj ) =flflfl U ⊤(xi − xj)flflfl
.
Actually , the target metric M is usually low rank in highdimensional data spaces [ 7 ] . Thus , we may pursue the subspace U equivalently .
Affinity Preserving Principle . To remedy overfitting , we aim at taking full advantage of unlabeled data that are demonstrated to be quite beneficial to the semi supervised learning problem . Due to this consideration , we define g based on the notion of affinity preserving [ 18 ] . Given the collection of n data points X including the data involved in the given constraints and the unlabeled data , we need an affinity matrix W ∈ Rn×n on X such that each entry Wij measures the strength of affinity between data pair xi and xj . The larger Wij , the closer xi and xj . Through absorbing all data information X = [ x1 , · · · , xn ] guided by the affinity matrix W , we formulate g as follows : g(M , X , S , D ) n
1 2 r
1 2 n r
2
=
= kxi − xjk2
M Wij =
U ⊤(xi − xj)flflfl
Xi,j=1 Xd=1 = trU ⊤XLX ⊤U = trXLX ⊤U U ⊤ = tr(XLX ⊤M ) ,
Xi,j=1flflfl Xd=1 d X(D − W )X ⊤ud = u⊤ d XLX ⊤ud
Wij u⊤
( 5 ) where tr(· ) stands for the trace operator , and D is a diagonal matrix whose diagonal elements equal the sums of the row j=1 Wij . The matrix L = D − W entries of W , ie , Dii =Pn is known as the graph Laplacian [ 31 ] .
1141 Laplacian Regularized Metric Learning ( LRML ) [ 14 ] and SDML [ 21 ] follow the same affinity preserving principle . Under supervised settings , it is quite simple to obtain the affinity matrix W by setting Wij = 1 if ( i , j ) ∈ S and Wij = −1 if ( i , j ) ∈ D . However , under semi supervised settings , there is no intuitive way to acquire W since S and D often contain very few data instances . As a weak measure of affinities , LRML defines W in an unsupervised fashion , ie , Wij = 1 if xi is among k nearest neighbors of xj or vice versa . Such a definition for W fails to absorb the pairwise constraints and thus does not take full advantage of semi supervision .
3.2 Affinity Propagation
We aim at designing better affinity matrices for semisupervised settings through integrating the min max principle and the affinity preserving principle . Based on the weak affinities revealed by k NN search , we intend to propagate the strong ( definitely correct ) affinities revealed by the given pairwise constraints to the global scope of the data . Let us define a neighborhood indicator matrix P ∈ Rn×n on X :
Then , we build a new reliable affinity matrix by symmetrizing the converged affinity matrix W ∗ and removing unreliable affinities , that is
W = W ∗ + W ∗⊤
2
||≥θ
,
( 10 ) in which the operator ⌊S⌋||≥θ zeros out the entries of S whose absolute values are smaller than 0 < θ < 1 .
In summary , we are capable of learning a better affinity matrix provided with strong affinities , ie , real constraints . The effect of learning affinity matrices subject to the known pairwise constraints is enhancing the generalization and robustness capabilities of the affinity preserving function g formulated in eq . ( 5 ) . The learned affinities between all data pairs lead to better generalization characteristics of g than using only strong affinities or only weak affinities . Compared with our parallel work [ 18 ] which employs similarity ( positive affinity ) propagation to learn non sparse metrics , the proposed affinity propagation in this paper discloses more hidden relevances and irrelevances among data .
Pij =( 1 k 0 ,
, j ∈ Ni otherwise
( 6 )
3.3 Log Determinant Divergence where Ni denotes the index list composed of k nearest neighbors of data point xi using the Euclidean distance . Note that P is asymmetric and provides weak affinities .
To enable metric learning techniques to work for practical applications , we should shrink the distances between as many similar pairs as possible and enlarge the distances between as many dissimilar pairs as possible . Although the affinity preserving function g has incorporated all unlabeled data , it does not emphasize the distances between “ real ” similar or dissimilar data pairs . Since the two real constraint sets S and D are available , we desire to propagate the limited real constraints to all data pairs via the found neighborhoods P using the Euclidean distance . Specifically , we learn an affinity matrix W such that Wij reflects the extent of propagated affinity between data pair ( xi , xj ) . ii = 1 for any i , W 0 ij = −1 for any ( i , j ) ∈ D , and W 0
Let us begin with an initial affinity matrix W 0 ∈ Rn×n where we set W 0 ij = 1 for any ( i , j ) ∈ S , W 0 ij = 0 otherwise . Clearly , W 0 represents the strong affinities . If we consider ±1 entries in W 0 as signed energies , our intent is to propagate the energies in W 0 to its 0 entries . The propagation path coincides with the neighborhood structure at each data point , so we pose the affinity propagation criterion as the locally linear energy mixture , ie ,
W ( t+1 ) i .
= ( 1 − α)W ( 0 ) i . + α
PijW ( t ) j . ,
( 7 ) n
Xj=1 where W ( t ) i . denotes the ith row of W ( t ) and t = 0 , 1 , 2 , · · · is the time stamp . We write the matrix form of eq . ( 7 ) as
W ( t+1 ) = ( 1 − α)W ( 0 ) + αP W ( t ) ,
( 8 ) where 0 < α < 1 is the trade off parameter and P is like the transition probability matrix widely used in Markov random walk models . Because 0 < α < 1 and the eigenvalues of P are in [ −1 , 1 ] , the limit W ∗ = limt→∞ W ( t ) exists . It suffices to solve the limit as
W ∗ = ( 1 − α)(I − αP )−1W ( 0 ) .
( 9 )
So far , we can give a general formula for semi supervised metric learning as the minimization of the log determinant Bregman divergence Dℓd(M , M0 ) = tr(M M −1 0 ) − log det(M M −1 0 ) − m [ 8 ] between a given metric matrix M0 ∈ Rm×m and the desirable metric matrix M regularized by the affinitypreserving function g(M , X , S , D ) = tr(XLX ⊤M ) prescribed in subsection 3.1 , that is
0 M ) − log det M + βtr(XLX ⊤M ) tr(M −1 min M st M 0
( 11 ) where β > 0 is the regularization parameter and the graph Laplacian L is computed based on the learned affinity matrix in eq . ( 10 ) . This optimization problem is convex , but it is not easy to solve efficiently . In the next section , we propose a fast algorithm to solve it with ℓ1 regularization .
4 . SPARSE METRIC OPTIMIZATION
We adopt the following notations throughout this section . We use Sm + to denote the set of positive semidefinite matrices in dimensions m × m . For matrix M , kM k0 represents the number of components which are nonzeros in M ; kM k1 is the sum of absolute values of all components in M ; and the matrix inner product operator is hM , Ai = tr(M ⊤A ) .
4.1 Sparse Inverse Covariance Estimation
Let us start with the hot statistical problem Sparse Inverse Covariance Estimation ( SICE ) [ 1][9 ] . To estimate the sparse inverse covariance matrix , one common approach is to penalize its maximum likelihood function by a cardinality term . Thus the maximum likelihood estimate of the sparse inverse covariance matrix reduces to the following optimization problem : max M ∈Sm + log det M − hΣ , M i − ρkM k0 ,
( 12 ) where Σ is the empirical covariance matrix , kM k0 is a penalty term to enforce the solution to be sparse , and ρ > 0 is the trade off parameter to balance the maximum likelihood and the sparsity . This problem is combinatorial in essence and
1142 thus numerically intractable . A common approach to overcome this difficulty is to replace the cardinality norm kM k0 by its tightest convex relaxation , ℓ1 norm kM k1 ( see [ 13] ) , which results in the following convex optimization problem : max M ∈Sm + log det M − hΣ , M i − ρkM k1 , or equivalently , the following minimization problem : min M ∈Sm +
− log det M + hΣ , M i + ρkM k1 .
( 13 )
If we regarded Σ = M −1 0 + βXLX ⊤ , the semi supervised metric learning problem formulated in eq . ( 11 ) could be converted to the SICE problem by introducing the ℓ1 norm . Therefore , we call the process of optimizing eq . ( 13 ) as semisupervised sparse metric learning ( S3ML ) .
Note that eq . ( 13 ) can be rewritten as min M ∈Sm + max kU k∞≤ρ
− log det M + hΣ + U , M i .
( 14 )
Thus the dual problem of eq . ( 13 ) is given by exchanging the order of max and min in eq . ( 14 ) , ie , max kU k∞≤ρ min M ∈Sm +
− log det M + hΣ + U , M i , which is equivalent to max st log det Z + m kZ − Σk∞ ≤ ρ .
( 15 )
We will see in the next subsection that our algorithm easily constructs a feasible solution to the primal problem eq . ( 13 ) and a feasible solution to the dual problem eq . ( 15 ) . Thus we can easily compute the duality gap and use it to determine when to terminate the algorithm and claim optimality .
4.2 Alternating Linearization Method
If we define f ( M ) := − log det M + hΣ , M i
( 16 ) and h(M ) := ρkM k1 , then eq . ( 13 ) can be viewed as minimizing the sum of two convex functions f and h : min f ( M ) + h(M ) .
( 17 )
We can leverage the alternating linearization method ( ALM ) proposed in [ 12 ] to solve eq . ( 17 ) . ALM requires the two functions f and h to be both in the class C 1,1 ( C 1,1 contains differentiable functions whose gradients are Lipschitz continuous ) , which means their gradients are Lipschitz continuous . So we need to smooth the ℓ1 term h(M ) first . One way to smooth the ℓ1 function h(M ) is to apply Nesterov ’s smoothing technique [ 20 ] . We use hσ(M ) to denote a smoothed approximation to h(M ) with a smoothness parameter σ . According to the Nesterov ’s technique , hσ(M ) is given by hσ(M ) := max
U
{hU , M i −
σ 2 kU k2
F : kU k∞ ≤ ρ} ,
( 18 ) in which k.kF denotes the Frobenius norm . It is easy to check that U ∗ := min{ρ , max{M/σ , −ρ}} is the optimal solution to eq . ( 18 ) . According to Theorem 1 in [ 20 ] , the gradient of hσ(M ) is given by ∇hσ(M ) := U ∗ and is Lipschitz continuous with constant L(hσ ) = 1/σ . ALM which can solve the smoothed SICE problem
Algorithm 1 ALM for SICE
Input : M 0 = Y 0 , i = 0 . repeat solve M i+1
:= arg minM f ( M ) + hσ(Y i ) + 1 2µ := arg minY f ( M i+1 ) + kM − Y ik2 F ;
1 2µ kY − M i+1k2
F + hσ(Y ) ; solve
Y i+1
∇hσ(Y i ) , M − Y iff + ∇f ( M i+1 ) , Y − M i+1ff + i = i + 1 ; until M i converges Output : M = M i . is described in Algorithm 1 where µ := 1/ max{L(f ) , L(hσ)} . It is easy to see that in the i th iteration of Algorithm 1 , ∇hσ(M i ) + Σ is a feasible solution to the dual problem eq . ( 15 ) , so we can conveniently compute the duality gap between this dual feasible solution and the primal feasible solution M i . We terminate Algorithm 1 once the duality gap achieves the desired accuracy .
Remark 2 . There are four advantages of our ALM algorithm over the block coordinate descent algorithm in [ 1 ] and [ 9 ] . First , we consider the primal problem eq . ( 17 ) where the ℓ1 term is involved , so ALM will preserve the sparsity of the desired metric matrix M . However , the block coordinate descent algorithm in [ 1][9 ] was proposed for the dual problem eq . ( 15 ) where the desired matrix M is obtained by inverting the optimal dual solution . Such a matrix inversion usually results in a dense matrix due to the floating point errors encountered in inverting a matrix . Second , the matrix M in Algorithm 1 is obtained by solving the subproblem where the log det(M ) term is involved , so M is always positive definite throughout Algorithm 1 . Third , both subproblems in Algorithm 1 have closed form solutions and thereby can be solved substantially efficiently . Finally , there are no iteration complexity results about the block coordinate descent algorithm in [ 1][9 ] . In contrast , we can obtain an iteration complexity bound for ALM as we will show below .
When we apply ALM to the smoothed SICE problem eq . ( 19 ) , we do get an iteration complexity bound . However , the complexity results in [ 12 ] do not apply to Algorithm 1 right away . To apply the complexity results in [ 12 ] , we need to modify Algorithm 1 a little bit . First , although the function hσ is in the class C 1,1 , the function f is not in the class C 1,1 since log det M is not in C 1,1 on Sm + . However , restricted on the convex set {M : M ˆλI} with ˆλ > 0 , f ( M ) is in C 1,1 , ie , its gradient ∇f ( M ) = −M −1 + Σ is Lipschitz continuous with Lipschitz constant L(f ) = ˆλ−2 . According to Proposition 3.1 in [ 19 ] , the optimal solution M ∗ to eq . ( 13 ) satisfies M ∗ λI where λ = 1/(kΣk + mρ ) and kΣk denotes the largest eigenvalue of matrix Σ . We denote the domain in which f ( M ) is in C 1,1 by C , ie , C := {M ∈ Sm : M λI} . Thus we can impose constraint M ∈ C to the subproblem with respect to M in Algorithm 1 to guarantee that f is in the class C 1,1 . Second , Y i might be not positive definite and f ( Y i ) is thus not well defined . To this end , we need to also impose the constraint Y ∈ C to the subproblem with respect to Y in Algorithm 1 to guarantee that f ( Y i ) is well defined . Before addressing our main complexity result , we need to min f ( M ) + hσ(M )
( 19 ) define the terminology ǫ optimal solution first .
1143 Definition 3 . Suppose x∗ is an optimal solution to the problem
( 20 ) where C ⊂ Rn is a convex set . x ∈ C is an ǫ optimal solution to eq . ( 20 ) if f ( x ) − f ( x∗ ) ≤ ǫ holds . min{f ( x ) : x ∈ C} ,
We have the following result about the relation between an approximate solution to eq . ( 19 ) and an approximate solution to eq . ( 17 ) .
Theorem 4 . For any ǫ > 0 , we let σ := m2ρ2 . Suppose Mσ is an ǫ/2 optimal solution to eq . ( 19 ) , then Mσ is an ǫ optimal solution to eq . ( 17 ) .
ǫ
Proof . It is easy to verify that ( see equation ( 2.7 ) in
[ 20] ) : hσ(M ) ≤ h(M ) ≤ hσ(M ) + σDh , ∀M ∈ Rm×m ,
( 21 )
2 kM k : kM k∞ ≤ ρ} = 1
2 m2ρ2 . Suppose where Dh := max{ 1 M ∗ is an optimal solution to eq . ( 17 ) and M ∗ σ is an optimal solution to eq . ( 19 ) , then we exploit the inequalities in eq . ( 21 ) to derive f ( Mσ ) + h(Mσ ) − f ( M ∗ ) − h(M ∗ )
≤f ( Mσ ) + hσ(Mσ ) + σDh − f ( M ∗ ) − hσ(M ∗ ) ≤f ( Mσ ) + hσ(Mσ ) + σDh − f ( M ∗ σ ) − hσ(M ∗ σ )
+ σDh
≤
ǫ 2 =ǫ , where the third inequality is due to the fact that Mσ is an ǫ/2 optimal solution to eq . ( 19 ) and the last equality is due to σ = m2ρ2 . This completes the proof .
ǫ
Now we are ready to give the iteration complexity bound for ALM .
Theorem 5 . By imposing constraints M ∈ C and Y ∈ C in the two subproblems in Algorithm 1 , Algorithm 1 returns an ǫ optimal solution to eq . ( 17 ) in O(1/ǫ2 ) iterations .
Proof . From Theorem 4.2 and discussions in section 5.3 in [ 12 ] , after imposing constraints M ∈ C and Y ∈ C in the two subproblems in Algorithm 1 , Algorithm 1 returns an ǫ/2 optimal solution to eq . ( 19 ) in O(
) iterations . If
ǫ we choose σ = m2ρ2 , then incorporating Theorem 4 we conclude that Algorithm 1 returns an ǫ optimal solution to
1 σǫ eq . ( 17 ) in O(
1 ǫ2 ) iterations .
Now we show how to optimize the two subproblems in Algorithm 1 . The first order optimality condition for the M subproblem in Algorithm 1 is
∇f ( M ) + ∇hσ(Y i ) + ( M − Y i)/µ = 0 .
( 22 )
Since ∇f ( M ) = −M −1 +Σ , it is easy to verify that M i+1 := V diag(γ)V ⊤ satisfies eq . ( 22 ) and is thus optimal to the M subproblem in Algorithm 1 , where V diag(d)V ⊤ is the eigenvalue decomposition of Y i − µ(Σ + ∇hσ(Y i ) ) and
γj = ( dj +qd2 j + 4µ)/2 , j = 1 , . . . , m .
( 23 )
Table 1 : Dataset Information : the numbers of features , samples and classes .
Dataset
# Features # Samples # Classes
Iris Wine
BCancer
Car
MSRA MM NUS WIDE
4 13 30 6
225 225
150 178 569 1728 10,000 269,648
3 3 2 4 50 81
Note that γj is always strictly positive since µ > 0 and matrix M i is hence always positive definite . If the constraint M ∈ C is imposed in the M subproblem in Algorithm 1 , the only change in the optimal solution is changing eq . ( 23 ) to
γj = max{λ , ( dj +qd2 j + 4µ)/2} , j = 1 , . . . , m .
The first order optimality condition for the Y subproblem is
∇f ( M i+1 ) + ( Y − M i+1)/µ + ∇hσ(Y ) = 0 .
( 24 )
Since ∇hσ(Y ) = min{ρ , max{Y /σ , −ρ}} , it is easy to verify that
Y i+1 =M i+1 − µ∇f ( M i+1)−
µ min{ρ , max{
M i+1 − µ∇f ( M i+1 )
σ + µ
, −ρ}}
( 25 ) satisfies eq . ( 24 ) and is thus optimal to the Y subproblem in Algorithm 1 . In summary , the M subproblem is corresponding to an eigenvalue decomposition and the Y subproblem is a trivial projection operation . Consequently , solving M subproblems dominates the computational complexity since solving Y subproblems is much cheaper compared with the computational effort on solving M subproblems .
4.3 The S3ML Algorithm
Given a set of n data points X = {xi}n i=1 ⊆ Rm with a similar constraint set S and a dissimilar constraint set D , an integer k , four real valued parameters 0 < α < 1 , θ > 0 , β > 0 and ρ > 0 , and input metric matrix M0 ( identity matrix or inverse covariance matrix ) , we summarize the proposed semi supervised sparse metric learning ( S3ML ) algorithm in Interestingly , the proposed S3ML algorithm Algorithm 2 . can also work under supervised settings when simply setting W = W 0 , so S3ML is appropriate for various metric learning problems .
5 . EXPERIMENTS
In this section , we compare the proposed semi supervised sparse metric learning ( S3ML ) algorithm with several existing state of the art metric learning algorithms on six datasets including four benchmark UCI datasets and two real world image datasets . Table 1 describes fundamental information about these datasets .
5.1 Compared Methods
The compared methods include : Euclidean : the baseline denoted as “ EU ” in short . Mahalanobis : a standard Mahalanobis metric denoted as “ Mah ” in short . Specifically , the metric matrix A = Cov−1 where Cov is the sample covariance matrix .
1144 Algorithm 2
S3ML
Input : Three sets X , S , D , an integer k , four real valued parameters 0 < α < 1 , θ > 0 , β > 0 and ρ > 0 , and input metric matrix M0 .
1 . k NN Search : Construct a neighborhood indicator matrix P ∈ Rn×n upon all n samples in X = [ x1 , · · · , xn ]
1 k
, if j ∈ Ni
0 , otherwise
Pij =  in which Ni denotes the set consisting of the indexes of k nearest neighbors of xi . 2 . Affinity Propagation : Set an initial affinity matrix W 0 ∈ Rn×n by W 0 ij = 1 for ∀(i , j ) ∈ S , W 0 ij = 0 otherwise . Calculate W ∗ = ( 1 − α)(I − αP )−1W ( 0 ) and the final affinity matrix is ii = 1 for ∀i ∈ {1 , · · · , n} , W 0 ij = −1 for ∀(i , j ) ∈ D , and W 0
W ∗ ij + W ∗ ji
2
, if
|W ∗ ij + W ∗ ji|
2
≥ θ
0 , otherwise .
Wij = 
Calculate the matrix T = XLX ⊤ where L = D − W and D = diag(W 1 ) . 3 . Alternating Linearization Method : Set Σ = M −1 0 +βT . Solve the following optimization problem using − log det M + hΣ , M i + ρkM k1 . Algorithm 1 : minM ∈Sm
+
Output : The sparse metric matrix M .
LMNN [ 28 ] : Large Margin Nearest Neighbor which works under supervised settings where each sample has an exact class label .
ITML [ 8 ] : Information Theoretic Metric Learning which works under pairwise relevance constraints but does not explicitly engage the unlabeled data .
SDML [ 21 ] : Sparse Distance Metric Learning which works under pairwise relevance constraints to produce sparse metrics but does not explicitly engage the unlabeled data .
S2ML : the supervised counterpart of S3ML with W = W 0 , which works under pairwise relevance constraints to produce sparse metrics and does not engage the unlabeled data .
LRML [ 14 ] : Laplacian Regularized Metric Learning which works under pairwise relevance constraints and explicitly engages the unlabeled data .
S3ML : our semi supervised sparse metric learning method which works under pairwise relevance constraints to produce sparse metrics and explicitly engages the unlabeled data .
To sum up , the compared distance metrics include two standard unsupervised metrics , four ( weakly ) supervised metrics LMNN , ITML , SDML and S2ML , as well as two semisupervised metrics LRML and S3ML . For the implementation of affinity propagation concerned in S3ML , we obtain the full affinity matrix in eq . ( 10 ) by setting k = 6 , α = 0.5 , and θ = 0.01 for all datasets . To run ALM for both of S2ML and S3ML , we fix the smoothing parameter σ = 10−6 for all datasets . We tune two regularization parameters β ( for affinity preserving ) and ρ ( for sparsity ) to the best values on each dataset .
Table 2 : Comparisons of classification error rates ( % ) on UCI datasets .
Compared Methods
Iris Wine Breast Cancer
EU Mah
LMNN ITML SDML S2ML LRML S3ML
8.45 10.67 7.06 6.34 4.55 4.27 3.26 2.10
25.34 30.25 12.67 22.19 10.78 9.62 7.71 7.20
15.69 14.32 10.82 12.61 7.14 7.56 6.26 2.52
Car
19.80 22.25 15.86 13.25 10.74 9.65 8.46 6.13
5.2 Benchmark UCI Datasets
We apply eight distance metrics listed above on four UCI datasets : Iris , Wine , Breast Cancer and Car , where we randomly choose 5 % examples from each class as labeled data and treat the other examples as unlabeled data . We evaluate kNN ( k=1 ) classification performance in terms of error rates on unlabeled data . For each dataset , we repeat the evaluation process with the 8 algorithms 50 times , and take the average error rates for comparison . Table 2 reports the average error rates for all compared methods . In this group of experiments , we let the inverse covariance matrix be the initial metric matrix that is fed to ITML , SDML , S2ML and S3ML .
In contrast to EU , Mah , LMNN , ITML , SDML , S2ML and LRML , the proposed S3ML achieves the lowest average error rates across all these datasets . The significant improvement of S3ML over S2ML demonstrates that the “ semi supervised ” scenario makes sense and that the proposed affinity propagation trick used in S3ML effectively utilizes the information of unlabeled data . While the computational efficiency of S2ML is comparable to the recently proposed sparse metric learning method SDML that applies the block coordinate descent algorithm to solve the related SICE , S2ML achieves better average classification performance on three datasets , and more importantly , SDML cannot guarantee to produce truly sparse metrics in bounded iterations .
5.3 Image Datasets
We conduct the experiments for image retrieval tasks on two image datasets . One is the MSRA MM dataset [ 27 ] which consists of 10,000 images and labeled with 50 concepts . The other is the NUS WIDE dataset [ 6 ] which consists of 269,648 images with 81 concepts labeled .
In this group of experiments , we set the identity matrix to be the initial metric matrix used for ITML , SDML and S3ML . Since the supervised metric learning approach LMNN requires explicit class labels , it is unsuitable for image retrieval . Therefore , we only compare six methods ( excluding LMNN and S2ML ) . We construct a subset for each image concept by selecting 2500 images in which 500 are labeled as relevant and 2000 as irrelevant from the entire datasets . Then , we randomly select 20 % samples to form the similar and dissimilar pair sets . We perform 20 random trials and show the averaged performance . The visual features we used in this experiment is the 225 dimentional block wise color moments extracted over 5 ∗ 5 fixed grid partitions with each block described by 9 dimensional feature . The performance
1145 i i n o s c e r P e g a r e v A
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
Ten Concepts in MSRA−MM
EU Mah ITML LRML SDML S3ML
1
2
3
4
5
6
7
Animal
Apple
Athlete
Baby
Baseball
Basketball Beach
8
Bird
9
Boat
10
Building
Figure 1 : The MSRA MM dataset . Average precisions for 10 concepts ( classes ) with the initial 20 % labeling rate .
Table 3 : Comparisons of Mean APs ( MAPs ) ( % ) on image datasets .
Compared MSRA MM NUS WIDE Methods
EU Mah ITML SDML LRML S3ML
14.59 12.89 16.17 16.21 22.45 27.30
26.57 21.63 26.98 28.82 30.19 39.87 is measured by the widely used non interpolated Average Precision ( AP ) which averages the precision values obtained when each relevant image occurs . We average the APs over all concepts in each dataset to get the Mean AP ( MAP ) for overall performance measurement .
Table 3 lists MAPs of six methods in comparison . We also show APs for ten concepts in each dataset in Fig 1 and Fig 2 , respectively . We can observe that S3ML consistently achieves the highest accuracy for the most of the concepts . Both MAPs and APs results show that the proposed S3ML is very promising for handling image retrieval tasks .
6 . CONCLUSIONS
This paper proposes a semi supervised sparse metric learning ( S3ML ) algorithm which works under a few of the pairwise similar and dissimilar constraints and produces favorable sparse metrics . In contrast to previous metric learning techniques , the proposed S3ML can employ unlabeled data in a principled way , ie , affinity propagation , that assigns reliable affinities to all data pairs through propagating prior strong affinities . Observing that the existing sparse metric learning methods did not optimize the sparse metrics explicitly , we apply the alternating linearization method to optimize the sparse metrics directly . This alternating linearization method has appealing computational and theoretical properties . Extensive experiments have been conducted to evaluate classification and retrieval performance using the sparse metrics offered by S3ML . The promising results show that S3ML is superior to the state of the arts .
Suppose data do not have a vector form and are merely measured via some kernel function , then nonlinear metrics are needed . Our latest work [ 16 ] actually learned a nonlinear metric for semi supervised classification . Another concern is that if the data dimension is substantially large any metric learning method will be computationally expensive and even prohibitive . Motivated by our earlier work [ 17 ] , it is possible to embed subspace learning into sparse metric learning so that S3ML is able to work on very high dimensional data such as microarray data and text data .
7 . ACKNOWLEDGMENTS
This work was supported by NTU NAP Grant with project number M58020010 and the Open Project Program of the State Key Lab of CAD&CG ( Grant No . A1006 ) , Zhejiang University . This work was also supported by grants from Natural Science Foundation of China ( No . 60975029 ) and Shenzhen Bureau of Science Technology&Information , China ( No . JC200903180635A ) .
References [ 1 ] O . Banerjee , L . E . Ghaoui , and A . d’Aspremont . Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data . Journal of Machine Learning Research , 9:485–516 , 2008 .
[ 2 ] A . Bar Hillel , T . Hertz , N . Shental , and D . Weinshall . Learning a mahalanobis metric from equivalence constraints . JMLR , 6:937–965 , 2005 .
[ 3 ] S . Basu , M . Bilenko , and R . Mooney . A probabilistic frame work for semi supervised clustering . In Proc . KDD , 2004 .
[ 4 ] M . Bilenko , S . Basu , and R . Mooney . Integrating constraints and metric learning in semi supervised clustering . In Proc . ICML , 2004 .
1146 i i n o s c e r P e g a r e v A
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Ten Concepts on NUS−WIDE
EU Mah ITML LRML SDML S3ML
1
Train
2
Tree
3
4
5
6
7
8
9
Valley
Vehicle
Water
Waterfall Wedding Whales Window
10
Zebra
Figure 2 : The NUS WIDE dataset . Average precisions for 10 concepts ( classes ) with the initial 20 % labeling rate .
[ 5 ] S . Boyd and L . Vandenberge . Convex Optimization . Cam
[ 19 ] Z . Lu . Smooth optimization approach for sparse covariance bridge University Press , Cambridge , UK , 2003 . selection . SIAM J . Optim . , 19(4):1807–1827 , 2009 .
[ 6 ] T S Chua , J . Tang , R . Hong , H . Li , Z . Luo , and Y . Zheng . Nus wide : A real world web image database from national university of singapore . In Proc . CIVR , 2009 .
[ 7 ] J . V . Davis and I . S . Dhillon . Structured metric learning for high dimensional problems . In Proc . KDD , 2008 .
[ 8 ] J . V . Davis , B . Kulis , P . Jain , S . Sra , and I . S . Dhillon . Information theoretic metric learning . In Proc . ICML , 2007 .
[ 9 ] J . Friedman , T . Hastie , and R . Tibshirani . Sparse inverse covariance estimation with the graphical lasso . Biostatistics , 8(1):1–10 , 2007 .
[ 10 ] A . Globerson and S . Roweis . Metric learning by collapsing classes . In NIPS 18 , 2006 .
[ 11 ] J . Goldberger , S . Roweis , and R . Salakhutdinov . Neighbour hood components analysis . In NIPS 17 , 2005 .
[ 12 ] D . Goldfarb and S . Ma . Fast alternating linearization methods for minimizing the sum of two convex functions . Technical report , Department of IEOR , Columbia University , 2009 . Preprint available at http://arxivorg/abs/09124571
[ 13 ] J B Hiriart Urruty and C . Lemar´echal . Convex Analysis and Minimization Algorithms II : Advanced Theory and Bundle Methods . Springer Verlag , New York , 1993 .
[ 14 ] S . C . Hoi , W . Liu , and S F Chang . Semi supervised distance In Proc . metric learning for collaborative image retrieval . CVPR , 2008 .
[ 20 ] Y . E . Nesterov . Smooth minimization for non smooth func tions . Math . Program . Ser . A , 103:127–152 , 2005 .
[ 21 ] G J Qi , J . Tang , Z J Zha , T S Chua , and H J Zhang . An efficient sparse metric learning in high dimensional space via ℓ1 penalized log determinant regularization . In Proc . ICML , 2009 .
[ 22 ] R . Rosales and G . Fung . Learning sparse metrics via linear programming . In Proc . KDD , 2006 .
[ 23 ] Y . Rui , T . S . Huang , M . Ortega , and S . Mehrotra . Relevance feedback : A powerful tool in interactive content based image retrieval . IEEE Trans . on Circuits and Systems for Video Technology , 8(5):644–655 , 1998 .
[ 24 ] A . W . M . Smeulders , M . Worring , S . Santini , A . Gupta , and R . Jain . Content based image retrieval at the end of the early years . IEEE Trans . PAMI , 22(12):1349–1380 , 2000 .
[ 25 ] W . Tang , H . Xiong , S . Zhong , and J . Wu . Enhancing semisupervised clustering : A feature projection perspective . In Proc . KDD , 2007 .
[ 26 ] K . Wagstaff , C . Cardie , S . Rogers , and S . Schroedl . Constrained k means clustering with background knowledge . In Proc . ICML , 2001 .
[ 27 ] M . Wang , L . Yang , and X S Hua . Msra mm : Bridging research and industrial societies for multimedia information retrieval . Technical report , Miscosoft Research Asia , 2009 . No . MSR TR 2009 30 .
[ 15 ] B . Kulis , S . Basu , I . Dhillon , and R . Mooney .
Semisupervised graph clustering : A kernel approach . Machine Learning , 74(1):1–22 , 2009 .
[ 28 ] K . Q . Weinberger and L . K . Saul . Distance metric learning for large margin nearest neighbor classification . Journal of Machine Learning Research , 10:207–244 , 2009 .
[ 16 ] W . Liu , B . Qian , J . Cui , and J . Liu . Spectral kernel learning for semi supervised classification . In Proc . IJCAI , 2009 .
[ 17 ] W . Liu , D . Tao , and J . Liu . Transductive component anal ysis . In Proc . IEEE ICDM , 2008 .
[ 18 ] W . Liu , X . Tian , D . Tao , and J . Liu . Constrained metric In Proc . AAAI , learning via distance gap maximization . 2010 .
[ 29 ] E . P . Xing , A . Y . Ng , M . I . Jordan , and S . Russell . Distance metric learning with application to clustering with side information . In NIPS 15 , 2003 .
[ 30 ] Y . Ying , K . Huang , and C . Campbell . Sparse metric learning via smooth optimization . In NIPS 22 , 2010 .
[ 31 ] X . Zhu . Semi supervied learning literature survey . Technical report , University of Wisconsin Madison , 2008 .
1147
