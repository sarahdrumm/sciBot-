Large Linear Classification When Data Cannot Fit In
Memory
Hsiang Fu Yu
Dept . of Computer Science National Taiwan University
Taipei 106 , Taiwan
Cho Jui Hsieh
Dept . of Computer Science National Taiwan University
Taipei 106 , Taiwan b93107@csientuedutw b92085@csientuedutw
Kai Wei Chang
Dept . of Computer Science National Taiwan University
Taipei 106 , Taiwan b92084@csientuedutw
Chih Jen Lin
Dept . of Computer Science National Taiwan University
Taipei 106 , Taiwan cjlin@csientuedutw
ABSTRACT Recent advances in linear classification have shown that for applications such as document classification , the training can be extremely efficient . However , most of the existing training methods are designed by assuming that data can be stored in the computer memory . These methods cannot be easily applied to data larger than the memory capacity due to the random access to the disk . We propose and analyze a block minimization framework for data larger than the memory size . At each step a block of data is loaded from the disk and handled by certain learning methods . We investigate two implementations of the proposed framework for primal and dual SVMs , respectively . As data cannot fit in memory , many design considerations are very different from those for traditional algorithms . Experiments using data sets 20 times larger than the memory demonstrate the effectiveness of the proposed method .
Categories and Subject Descriptors I52 [ Pattern Recognition ] : Design Methodology—Classifier design and evaluation
General Terms Algorithms , Performance , Experimentation
1 .
INTRODUCTION
Linear classification1 is useful in many applications , but training large scale data remains an important research is
1By linear classification we mean that data remain in the input space and kernel methods are not used .
Figure 1 : Data size versus training time on a machine with 1GB memory . sue . For example , a category of PASCAL Large Scale Learning Challenge2 at ICML 2008 compares linear SVM implementations . The competition evaluates the time after data have been loaded into the memory , but many participants find that loading time costs more . Thus some have concerns about the evaluation.3 This result indicates a landscape shift in large scale linear classification because time spent on reading/writing between memory and disk becomes the bottleneck . Existing training algorithms often need to iteratively access data , so without enough memory , the training time will be huge . To see how serious the situation is , Figure 1 presents the running time by applying an efficient linear classification package LIBLINEAR [ 1 ] to train data with different scales on a computer with 1 GB memory . Clearly , the time grows sharply when the data size is beyond the memory capacity .
We model the training time to contain two parts : training time = time to run data in memory + time to access data from disk .
( 1 )
Traditional training algorithms , assuming that the second
2http://largescalefirstfraunhoferde/workshop 3http://hunch.net/?p=330 part is negligible , focus on the first part by minimizing the number of CPU operations . Linear classification , especially when applied to document classification , is in a situation that the second part may be more significant . Recent advances on linear classification ( eg , [ 2 , 3 , 4 , 5 ] ) have shown that training one million instances takes only a few seconds ( without counting the loading time ) . Therefore , some have said that linear classification is essentially a solved problem if the memory is enough . However , handling data beyond the memory capacity remains a challenging research issue . According to [ 6 ] , existing approaches to handle large data can be roughly categorized to two types . The first approach solves problems in distributed systems by parallelizing batch training algorithms ( eg , [ 7 , 8] ) . However , not only writing programs on a distributed system is difficult , but also the data communication/synchronization may cause significant overheads . The second approach considers online learning algorithms . Since data may be used only once , this type of approaches can effectively handle the memory issue . However , even with an online setting , an implementation over a distributed environment is still complicated ; see the discussion in Section 2.1 of [ 9 ] . Existing implementations ( including those in large Internet companies ) may lack important functions such as evaluations by different criteria , parameter selection , or feature selection .
This paper aims to construct large linear classifiers for ordinary users . We consider one assumption and one requirement :
• Assumption : Data cannot be stored in memory , but can be stored in the disk of one computer . Moreover , sub sampling data to fit in memory causes lower accuracy .
• Requirement : The method must be simple so that support for multi class classification , parameter selection and other functions can be easily done .
If sub sampling does not downgrade the accuracy , some ( eg , [ 10 ] ) have proposed approaches to select important instances by reading data from disk only once .
In this work , we discuss a simple and effective block minimization framework for applications satisfying the above assumption . We focus on batch learning though extensions to online or incremental/decremental learning are straightforward . While many existing online learning studies claim to handle data beyond the memory capacity , most of them conduct simulations with enough memory and check the number of passes to access data ( eg , [ 3 , 4] ) . In contrast , we conduct experiments in a real environment without enough memory . An earlier linear SVM study [ 11 ] has specifically addressed the situation that data are stored in disk , but it assumes that the number of features is much smaller than data points . Our approach allows a large number of features , a situation often occurred for document data sets .
This paper is organized as follows . In Section 2 , we consider SVM as our linear classifier and propose a block minimization framework . Two implementations of the proposed framework for primal and dual SVM problems are respectively in Sections 3 and 4 . Techniques to minimize the training time modeled in ( 1 ) are in Section 5 . Section 6 discusses the implementation of cross validation , multi class classification , and incremental/decremental settings . We show experiments in Section 7 and give conclusions in Section 8 .
Algorithm 1 A block minimization framework for linear SVM files accordingly .
1 . Split {1 , . . . , l} to B1 , . . . , Bm and store data into m 2 . Set initial α or w 3 . For k = 1 , 2 , . . .
( outer iteration )
For j = 1 , . . . , m ( inner iteration ) 31 Read xr , ∀r ∈ Bj from disk 32 Conduct operations on {xr | r ∈ Bj} 33 Update α or w
2 . BLOCK MINIMIZATION FOR LINEAR
SVMS
We consider linear SVM in this work because it is one of the most used linear classifiers . Given a data set {(xi , yi)}l i=1 , xi ∈ Rn , yi ∈ {−1 , +1} , SVM solves the following unconstrained optimization problem:4 min w
1 2 wT w + C l
X i=1 max(1 − yiwT xi , 0 ) ,
( 2 ) where C > 0 is a penalty parameter . This formulation considers L1 loss , though our approach can be easily extended to L2 loss . Problem ( 2 ) is often referred to as the primal form of SVM . One may instead solve its dual problem : min
α subject to
1 2 f ( α ) =
αT Qα − eT α 0 ≤ αi ≤ C , i = 1 , . . . , l ,
( 3 ) where e = [ 1 , . . . , 1]T and Qij = yiyj xT i xj .
As data cannot fit in memory , the training method must avoid random accesses of data . In Figure 1 , LIBLINEAR randomly accesses one instance at a time , so frequent moves of the disk head result in lengthy running time . A viable method must satisfy the following conditions :
1 . Each optimization step reads a continuous chunk of training data .
2 . The optimization procedure converges toward the optimum even though each step uses only a subset of training data .
3 . The number of optimization steps ( iterations ) should not be too large . Otherwise , the same data point may be accessed from the disk too many times .
Obtaining a method having all these properties is not easy . We will propose methods to achieve them to a certain degree .
In unconstrained optimization , block minimization is a classical method ( eg , [ 12 , Chapter 27 ] ) Each step of this method updates a block of variables , but here we need a connection to data . Let {B1 , . . . , Bm} be a partition of all data indices {1 , . . . , l} . According to the memory capacity , we can decide the block size so that instances associated with Bj can fit in memory . These m blocks , stored as m files , are loaded when needed . Then at each step , we conduct some operations using one block of data , and update w or α according to if the primal or the dual problem is considered . We assume that w or α can be stored in memory . The
4The standard SVM comes with a bias term b . Here we do not consider this term for the simplicity . block minimization framework is summarized in Algorithm 1 . We refer to the step of working on a single block as an inner iteration , while the m steps of going over all blocks as an outer iteration . Algorithm 1 can be applied on both the primal form ( 2 ) and the dual form ( 3 ) . We show two implementations in Sections 3 and 4 , respectively .
We discuss some implementation considerations for Algorithm 1 . For the convenience , assume B1 , . . . , Bm have a similar size |B| = l/m . The total cost of Algorithm 1 is
( Tm(|B| ) + Td(|B| ) ) × l
|B| × #outer iters ,
( 4 ) where
• Tm(|B| ) is the cost of operations at each inner itera tion , and
• Td(|B| ) is the cost to read a block of data from disk . These two terms respectively correspond to the two parts in ( 1 ) for modeling the training time .
Many studies have applied block minimization to train SVM or other machine learning problems , but we might be the first to consider it in the disk level . Indeed the major approach to train nonlinear SVM ( ie , SVM with nonlinear kernels ) has been block minimization , which is often called decomposition methods in the SVM community . We discuss the difference between ours and existing studies in two aspects :
• variable selection for each block , and • block size .
Existing SVM packages assume data in memory , so they can use flexible ways to select each Bj . They do not restrict B1 , . . . , Bm to be a split of {1 , . . . , l} . Moreover , to decide indices of one single Bj , they may access the whole set , an impossible situation for us . We are more confined here as data associated with each Bj must be pre stored in a file before running Algorithm 1 .
Regarding the block size , we now go back to analyze ( 4 ) . If data are all in memory , Td(|B| ) = 0 . For Tm(|B| ) , people observe that if |B| linearly increases , then
|B| ր , Tm(|B| ) ր , and #outer iters ց .
( 5 )
Tm(|B| ) is generally more than linear to |B| , so Tm(|B| ) × l/|B| is increasing along with |B| . In contrast , the #outeriters may not decrease as quick . Therefore , nearly all existing SVM packages use a small |B| . For example , |B| = 2 in LIBSVM [ 13 ] and 10 in SVMlight [ 14 ] . With Td(|B| ) > 0 , the situation is now very different . At each outer iteration , the cost is
Tm(|B| ) × l |B|
+ Td(|B| ) ×
. l |B|
( 6 )
The second term is for reading l instances . As reading each block of data takes some initial time , a smaller number of blocks reduces the cost . Hence the second term in ( 6 ) is a decreasing function of |B| . While the first term is increasing following the earlier discussion , as reading data from the disk is slow , the second term is likely to dominate . Therefore , contrary to existing SVM software , in our case the block size should not be too small . We will investigate this issue by experiments in Section 7 .
Algorithm 2 An implementation of Algorithm 1 for solving dual SVM We only show details of steps 3.2 and 3.3 :
3.2 Exactly or approximately solve the sub problem
( 7 ) to obtain d∗ Bj 3.3 αBj ← αBj + d∗ Bj Update w by ( 10 )
The remaining issue is to decide operations at each inner iteration . The second and the third conditions mentioned earlier in this section should be considered . We discuss two implementations in the next two sections .
3 . SOLVING DUAL SVM BY LIBLINEAR
FOR EACH BLOCK
A nice property of the SVM dual problem ( 3 ) is that each variable corresponds to a training instance . Thus we can easily devise an implementation of Algorithm 1 by updating a block of variables at a time . Assume ¯Bj = {1 , . . . , l}\Bj , at each inner iteration we solve the following sub problem . min dBj f ( α + d )
( 7 ) subject to d ¯Bj = 0 and 0 ≤ αi + di ≤ C , ∀i ∈ Bj .
That is , we change αBj , while fix α ¯Bj . We then update αBj using the solution of ( 7 ) . Then Algorithm 1 reduces to the standard block minimization procedure , so the convergence to the optimal function value of ( 3 ) holds [ 12 , Proposition 271 ]
We must ensure that at each inner iteration , only one block of data is needed . With the constraint d ¯Bj = 0 in ( 7 ) , dT
Bj QBj Bj
1 2 f ( α + d ) = dBj + ( QBj ,:α − eBj )T dBj + f ( α ) , ( 8 ) where QBj , : is a sub matrix of Q including elements Qri , r ∈ Bj , i = 1 , . . . , l . Clearly , QBj , : in ( 8 ) involves all training data , a situation violating the requirement in Algorithm 1 . Fortunately , by maintaining w = l
X i=1
αiyixi ,
( 9 ) we have
Qr,:α − 1 = yrwT xr − 1 , ∀r ∈ Bj .
Therefore , if w is available in memory , only instances associated with the block Bj are needed . To maintain w , if d∗ Bj is an optimal solution of ( 7 ) , we consider ( 9 ) and use w ← w + X r∈Bj d∗ ryr xr .
( 10 )
This operation again needs only the block Bj . The procedure is summarized in Algorithm 2 .
For solving the sub problem ( 7 ) , as all the information is available in the memory , any bound constrained optimization method can be applied . We consider LIBLINEAR [ 1 ] , which implements a coordinate descent method ( ie , block minimization with a single element in each block ) . Then Algorithm 2 becomes a two level block minimization method . The two level setting had been used before for SVM or other applications ( eg , [ 15 , 16 , 17] ) , but ours might be the first to associate the inner level with memory and the outer level with disk .
Algorithm 2 converges if each sub problem is exactly solved .
Practically we often obtain an approximate solution by imposing a stopping criterion . We then address two issues :
1 . The stopping criterion for solving the sub problem must be satisfied after a finite number of operations , so we can move on to the next sub problem .
2 . We need to prove the convergence .
Next we show that these two issues can be resolved if using LIBLINEAR for solving the sub problem . Let {αk} be the sequence generated by Algorithm 2 , where k is the index of outer iterations . As each outer iteration contains m inner iterations , we can further consider a sequence
{αk,j}∞,m+1 k=1,j=1 with αk,1 = αk and αk,m+1 = αk+1 .
From αk,j to αk,j+1 , LIBLINEAR coordinate wisely updates variables in Bj to approximately solve the sub problem ( 7 ) and we let tk,j be the number of updates .
If the coordinate descent updates satisfy certain condi tions , we can prove the convergence of {αk,j} :
Theorem 1 If applying a coordinate descent method to solve ( 7 ) with the following properties :
1 . each αi , i ∈ Bj is updated at least once , and 2 . {tk,j} is uniformly bounded , then {αk,j} generated by Algorithm 2 globally converges to an optimal solution α∗ . The convergence rate is at least linear : there are 0 < µ < 1 and an iteration k0 such that f ( αk+1 ) − f ( α∗ ) ≤ µf ( αk ) − f ( α∗ ) , ∀k ≥ k0 .
The proof is in appendix . With Theorem 1 , the condition 2 mentioned in the beginning of Section 2 holds . For condition 3 on the convergence speed , block minimization does have fast convergence rates . However , for problems like document classification , some ( eg , [ 5 ] ) have shown that we do not need many iterations to get a reasonable model . Though [ 5 ] differs from us by restricting |B| = 1 , we hope to enjoy the same property of not needing many iterations . Experiments in Section 7 confirm that for some document data this property holds .
Next we discuss various ways to fulfill the two properties in Theorem 1 . 3.1 Loosely Solving the Sub problem
A simple setting to satisfy Theorem 1 ’s two properties is to go through all variables in Bj a fixed number of times . Then not only tkj is uniformly bounded , but also the finite termination for solving each sub problem holds . A small number of passes to go through Bj means that we very loosely solve the sub problem ( 7 ) . While the cost per block is cheaper , the number of outer iterations may be large . Through experiments in Section 7 , we discuss how the number of passes affects the running time . A special case is to go through all αi , i ∈ Bj exactly once . Then Algorithm 2 becomes a standard ( one level ) coordinate descent method , though data are loaded by a block wise setting .
For each pass to go through data in one block , we can sequentially update variables in Bj . However , using a random permutations of Bj ’s elements as the order for update usually leads to faster convergence in practice . 3.2 Accurately Solving the Sub problem
Alternatively , we can accurately solve the sub problem . The cost per inner iteration is higher , but the number of outer iterations may be reduced . As an upper bound on the number of iterations does not reveal how accurate the solution is , most optimization software consider the gradient information . We check the setting in LIBLINEAR . Its gradientbased stopping condition ( details shown in appendix ) guarantees the finite termination in solving each sub problem ( 7 ) . Thus the procedure can move on to the next sub problem without problem . Regarding the convergence , to use Theorem 1 , we must show that {tk,j} is uniformly bounded : Theorem 2 If coordinate descent steps with LIBLINEAR ’s stopping condition are used to solve ( 7 ) , then Algorithm 2 either terminates in a finite number of outer iterations or tk,j ≤ 2|Bj| ∀j after k is large enough .
Therefore , if LIBLINEAR is used to solve ( 7 ) , then Theorem 1 implies the convergence .
4 . SOLVING PRIMAL SVM BY PEGASOS
FOR EACH BLOCK
Instead of solving the dual problem , in this section we check if the framework in Algorithm 1 can be used to solve the primal problem . Since the primal variable w does not correspond to data instances , we cannot use a standard block minimization setting to have a sub problem like ( 7 ) . In contrast , existing stochastic gradient descent methods possess a nice property that at each step only certain data are used . In this section , we study how Pegasos [ 3 ] can by used for implementing an Algorithm 1 .
Pegasos considers a scaled form of the primal SVM prob lem : min w
1 2lC wT w +
1 l l
X i=1 max(1 − yiwT xi , 0 ) ,
At the tth update , Pegasos chooses a block of data B and updates the primal variable w by a stochastic gradient descent step :
¯w = w − ηt∇t ,
( 11 ) where ηt = lC/t is the learning rate , ∇t is the sub gradient ( 12 ) yixi ,
∇t =
1 lC w −
1
|B| X i∈B+ and B+ ≡ {i ∈ B | yiwT xi < 1} . Then Pegasos obtains w by scaling ¯w :
√lC k ¯wk w ← min(1 ,
) ¯w .
( 13 )
Clearly we can directly consider Bj in Algorithm 1 as the set B in the above update . Alternatively , we can conduct several Pegasos updates on a partition of Bj . Algorithm 3 gives details of the procedure . Here we consider two settings for an inner iteration :
Algorithm 3 An implementation of Algorithm 1 for solving primal SVM . Each inner iteration is by Pegasos files accordingly .
1 . Split {1 , . . . , l} to B1 , . . . , Bm and store data into m 2 . t = 0 and initial w = 0 . 3 . For k = 1 , 2 , . . .
Algorithm 4 Splitting data into blocks • Decide m and create m empty files . • For i = 1 , . . .
1 . Convert xi to a binary format ¯xi . 2 . Randomly choose a number j ∈ {1 , . . . , m} . 3 . Append ¯xi into the end of the jth file .
For j = 1 , . . . , m
31 Find a partition of Bj : B1 32 For r = 1 , . . . , ¯r j , . . . , B ¯r j . j as B to conduct the update ( 11 )
( 13 ) .
• Use Br • t ← t + 1
1 . Using one Pegasos update on the whole block Bj .
2 . Splitting Bj to |Bj| sets , where each one contains an element in Bj and then conducting |Bj| Pegasos updates .
For the convergence , though Algorithm 3 is a special case of Pegasos , we cannot apply its convergence proof [ 3 , Corollary 1 ] , which requires that all data {x1 , . . . , xl} are used at each update . However , empirically we observe that Algorithm 3 converges without problems .
5 . TECHNIQUES TO REDUCE THE TRAIN
ING TIME
Many techniques have been proposed to make block minimization faster . However , these techniques may not be suitable here as they are designed by assuming that all data are in memory . Based on the complexity analysis in ( 6 ) , in this section we propose three techniques to speed up Algorithm 1 . One technique effectively shortens Td(|B| ) , while the other two aim at reducing the number of iterations . 5.1 Data Compression
The loading time Td(|B| ) is a bottleneck of Algorithm 1 due to the slow disk access . Except some initial cost , Td(|B| ) is proportional to the length of data . Hence we can consider a compression strategy to reduce the loading time of each block . However , this strategy introduces two additional costs : the compression time in the beginning of Algorithm 1 and the decompression time when a block is loaded . The former is minor as we only do it once . For the latter , we must ensure that the loading time saved is more than the decompression time . The balance between compression speed and ratio has been well studied in the area of backup and networking tools [ 18 ] . We choose a widely used compression library zlib for our implementation.5 Experiments in Section 7 show that the compression strategy effectively reduces the training time .
Because of using compression techniques , all blocks are stored in a binary format instead of a plain text form . 5.2 Random Permutation of Sub problems
In Algorithm 1 , we sequentially work on blocks B1 , B2 , . . . , Bm . We can consider other ways such as a permutation of blocks to decide the order of sub problems . In LIBLINEAR ’s coordinate descent implementation , the authors randomly
5http://wwwzlibnet permute all variables at each iteration and report faster convergence . We adopt a permutation strategy here as the loading time is similar regardless of the order of sub problems . 5.3 Split of Data
An important step of Algorithm 1 is to split training data to m files . We need a careful design as data cannot be loaded into memory . To begin , we find the size of data and decide the value m based on the memory size . This step does not have to go through the whole data set as the operating system provides information such as file sizes . Then we can sequentially read data instances and save them to m files . However , data in the same class are often stored together in the training set , so we may get a block of data with the same label . This situation clearly causes slow convergence . Thus for each instance being read , we randomly decide which file it should be saved to . Algorithm 4 summarizes our procedure . It goes through data only once .
6 . OTHER FUNCTIONALITY
A learning system only able to solve an optimization problem ( 3 ) is not practically useful . Other functions such as cross validation ( for parameter selection ) or multi class classification are very important . We discuss how to implement these functions based on the design in Section 2 . 6.1 Cross Validation
Assume we conduct v fold cross validation . Due to the use of m blocks , a straightforward implementation is to split m blocks to v groups . Each time one group of blocks is used for validation , while all remaining groups are for training . However , the loading time is v times more than training a single model . To save the disk accessing time , a more complicated implementation is to train v models together . For example , if v = 3 , we split each block Bj to three parts B1 j , B2 j , j . Then ∪m and B3 j ) is the training set to validate ∪m j=1B3 j . We maintain three vectors w1 , w2 , and w3 . Each time when Bj is loaded , we solve three sub problems to update w vectors . This implementation effectively saves the data loading time , but the memory must be enough to store v vectors w1 , . . . , wv . 6.2 Multi class Classification j ∪ B2 j=1(B1
Existing multi class approaches either train several two class problems ( eg , one against one and one against the rest ) or solve one single optimization problem ( eg , [ 19] ) . Take one against the rest for a K class problem as an example . We train K classifiers , where each one separates a class from the rest . Similar to the situation in cross validation , the disk access time is K times more if we sequentially train K models . Using the same technique , we split each blocks Bj to B1 j according to the class information , Then K subproblems are solved to update vectors w1 , . . . , wK . Finally we obtain K models simultaneously . The one against one j , . . . , BK
Table 1 : Data statistics : We assume a sparse storage . Each non zero feature value needs 12 bytes ( 4 bytes for the feature index , and 8 bytes for the value ) . However , this 12 byte structure consumes 16 bytes on a 64 bit machine due to data structure alignment .
Data set yahoo korea webspam epsilon l 460,554 350,000 500,000 n 3,052,939 16,609,143 2,000
#nonzeros Memory ( Bytes ) 2,502,986,496 156,436,656 1,304,697,446 20,875,159,136 16,000,000,000 1,000,000,000 approach is less suitable as it needs K(K − 1)/2 vectors for w , which may consume too much memory . For one againstthe rest and the approach in [ 19 ] , they both need K vectors . 6.3 Incremental/ Decremental Setting
Many practical applications retrain a model after collecting enough new data . Our approach can be extended to this scenario . We make a reasonable assumption that each time several blocks are added or removed . Using LIBLINEAR to solve the dual form as an example , to possibly save the number of iterations , we can reuse the vector w obtained earlier . Algorithm 2 maintains w = Pl i=1 yiαixi , so the new initial w can be w ← w + X yiαixi − X yiαixi . i:xi being added i:xi being removed
( 14 ) For data being added , αi is simply set to zero , but for data being removed , their corresponding αi are not available . To use ( 14 ) , we must store α . That is , before and after solving each sub problem , Algorithm 2 reads and saves α from/to disk .
If solving the primal problem by Pegasos for each block , Algorithm 3 can be directly applied for incremental or decremental settings .
7 . EXPERIMENTS
In this section , we conduct experiments to analyze the performance of the proposed approach . We also investigate several implementation issues discussed in Section 5 . 7.1 Data and Experimental Environment
We consider two document data sets yahoo korea6 and webspam , and an artificial data epsilon.7 Table 1 summarizes the data statistics .
We randomly split 4/5 data for training and 1/5 for testing . All feature vectors are instance wisely scaled to unitlength ( ie , kxik = 1 , ∀i ) . For epsilon , each feature of the training set is normalized to have mean zero and variance one , and the testing set is modified according to the same scaling factors . This feature wise scaling is conducted before the instance wise scaling . The value C in ( 2 ) is set to one . We conduct experiments on a 64 bit machine with 1GB RAM . Due to the space consumed by the operating system , the real memory capacity we can use is 895MB . 7.2 A Related Method
For the comparison we include another method StreamSVM [ 20 ] , which performs only a single pass over data . The method initiates with a single data point . When a new data point is read , it checks whether the point is contained in
6This data set is not publicly available 7webspam and at be http://largescalefirstfraunhoferde/instructions/ downloaded epsilon can a ball enclosing past data . If so , it continues to next data point . If not , it updates the center and radius of the ball to cover the point . Because this method is very different from our approach , we omit its details here . 7.3 Training Time and Testing Accuracy
We compare the following methods : • BLOCK L N : Algorithm 2 with LIBLINEAR to solve each sub problem . LIBLINEAR goes through the block of data N rounds , where we consider N = 1 , 10 , and 20 .
• BLOCK L D : Algorithm 2 with LIBLINEAR to solve each sub problem . LIBLINEAR ’s default stopping condition is adopted .
• BLOCK P B : Algorithm 3 with ¯r = 1 . That is , we apply one Pegasos update on the whole block .
• BLOCK P I : Algorithm 3 with ¯r = |Bj| . That is , we apply |Bj| Pegasos updates , each of which uses an individual data instance .
• LIBLINEAR : The standard LIBLINEAR without any modification to handle the situation if data cannot fit in memory . • StreamSVM
For all methods under the framework of Algorithms 1 , the number of blocks is 5 for yahoo korea , 40 for webspam and 30 for epsilon . We make sure that no other jobs are running on the same machine and report wall clock time in all experiments . We include all data loading time and , for Algorithm 1 , the initial time to split and compress data into blocks . It takes around 228 seconds to split yahoo korea , 1,594 seconds to split webspam and 1,237 seconds to split epsilon . For LIBLINEAR , the loading time for yahoo korea is 103 seconds , 829 seconds for webspam and 560 seconds for epsilon .
Figure 2 presents two results :
1 . Training time versus the relative difference to the op timum f P ( w ) − f P ( w∗ ) f P ( w∗ ) fifififi
, fifififi where f P is the primal objective function in ( 2 ) and w∗ is the optimal solution . Since w∗ is not really available , we spend enough training time to get a reference solution .
2 . Training time versus the difference to the best testing accuracy
( acc∗ − acc(w ) ) × 100 % , where acc(w ) is the testing accuracy using the model w and acc∗ is the best testing accuracy among all methods .
( a ) yahoo korea : Relative difference to optimum .
( b ) yahoo korea : Difference to the best accuracy .
( c ) webspam : Relative difference to optimum .
( d ) webspam : Difference to the best accuracy .
( e ) epsilon : Relative difference to optimum .
( f ) epsilon : Difference to the best accuracy .
Figure 2 : This table shows the relative function value difference to the minimum and the accuracy difference to the best testing accuracy . Time ( in seconds ) is log scaled . The blue dotted vertical line indicates time spent by Algorithms 1 based methods for the initial split of data to blocks . StreamSVM goes through data only once , so we present only one accuracy value . Note that in Figure 2(f ) , the curve of BLOCK L D is not connected , where the missing point corresponds to the best accuracy .
Figure 3 : Effectiveness of two implementation techniques : raw : no random assignment in the initial data splitting . perm : a random order of blocks at each outer iteration . BLOCK L D is used .
Figure 4 : Convergence speed of using different m ( number of blocks ) . BLOCK L D is used .
Clearly , LIBLINEAR suffers from slow disk swapping due to the random access of data . For Algorithm 1 based methods , BLOCK L ∗ methods ( using LIBLINEAR ) are faster than BLOCK P ∗ ( using Pegasos ) methods . The reason seems to be that for BLOCK P ∗ , the information of each block is underutilized . In particular , BLOCK P B suffers from very slow convergence as for each block it conducts only one very simple update . However , it may not be always needed to use the block of data in an exhaustive way . For example , in Figure 2(a ) , BLOCK L 1 ( for each block LIBLINEAR goes through all data only once ) is slightly faster than BLOCK LD ( for each block running LIBLINEAR with the default stopping condition ) . Nevertheless , as reading each block from the disk is expensive , in general we should make proper efforts to use it . For StreamSVM , because of passing data only once , its accuracy is lower than others .
Note that the objective values of BLOCK P ∗ methods may not be decreasing as Pegasos does not have this property . All BLOCK ∗ methods except BLOCK P B needs around four iterations to achieve reasonable accuracy values . This number of iterations is small , so we do not need to read the training set many times .
7.4 Initial Block Split and Random Permuta tion of Sub problems
Section 5 proposes randomly assigning data to blocks in the beginning of Algorithm 1 . It also suggests that a random order of B1 , . . . , Bm at each iteration is useful . We are interested in their effectiveness . Figure 3 presents the result of running BLOCK L D on webspam . We assume the worst situation that data of the same class are grouped together in the input file . If data are not randomly split to blocks , clearly the convergence is very slow . Further , the random permutation of blocks at each iteration slightly improves the training time .
7.5 Block Size
In Figure 4 , we present the training speed of BLOCKL D by using various block sizes ( equivalently , numbers of blocks ) . The data webspam is considered . The training time of using m = 40 blocks is smaller than that of m = 400 or 1000 . This result is consistent with the discussion in Section 2 . When the number of blocks is smaller ( ie , larger block size ) , from ( 5 ) , the cost of operations on each block increases . However , as we read less files , the total time is shorter . Furthermore , the initial split time is longer as m increases . Therefore , contrary to traditional SVM software which use small block sizes , now for each inner iteration we should consider a large block . We do not check m = 20 because the memory is not enough to load a block of data . 7.6 Data Compression
We check if compressing each block saves time . By running 10 outer iterations of BLOCK L D on the training set of webspam with m = 40 , the implementation with compression takes 3,230 seconds , but without compression needs 4,660 seconds . Thus the compression technique is very useful .
8 . DISCUSSION AND CONCLUSIONS
The discussion in Section 6 shows that implementing cross validation or multi class classification may require extra memory space and some modifications of Algorithm 1 . Thus constructing a complete learning tool is certainly more complicated than implementing Algorithm 1 . There are many new and challenging future research issues .
In summary , we propose and analyze a block minimization method for large linear classification when data cannot fit in memory . Experiments show that the proposed method can effectively handle data 20 times larger than the memory size .
Our code is available at http://wwwcsientuedutw/~cjlin/liblinear/exphtml
9 . ACKNOWLEDGMENTS
The authors thank anonymous reviewers for helpful comments . This work was supported in part by the National Science Council of Taiwan via the grant 98 2221 E 002 136MY3 .
10 . REFERENCES
[ 1 ] R E Fan , K W Chang , C J Hsieh , X R Wang , and C J Lin , “ LIBLINEAR : A library for large linear classification , ” JMLR , vol . 9 , pp . 1871–1874 , 2008 .
[ 2 ] T . Joachims , “ Training linear SVMs in linear time , ” in
ACM KDD , 2006 .
[ 3 ] S . Shalev Shwartz , Y . Singer , and N . Srebro , “ Pegasos : primal estimated sub gradient solver for SVM , ” in ICML , 2007 .
[ 4 ] L . Bottou , “ Stochastic gradient descent examples , ”
2007 . http://leonbottouorg/projects/sgd
[ 5 ] C J Hsieh , K W Chang , C J Lin , S . S . Keerthi , and S . Sundararajan , “ A dual coordinate descent method for large scale linear SVM , ” in ICML , 2008 .
[ 6 ] J . Langford , L . Li , and T . Zhang , “ Sparse online learning via truncated gradient , ” JMLR , vol . 10 , pp . 771–801 , 2009 .
[ 7 ] E . Chang , K . Zhu , H . Wang , H . Bai , J . Li , Z . Qiu , and
H . Cui , “ Parallelizing support vector machines on distributed computers , ” in NIPS 21 , 2007 .
[ 8 ] Z . A . Zhu , W . Chen , G . Wang , C . Zhu , and Z . Chen , “ P packSVM : Parallel primal gradient descent kernel SVM , ” in ICDM , 2009 .
[ 9 ] J . Langford , A . J . Smola , and M . Zinkevich , “ Slow learners are fast , ” in NIPS , 2009 .
[ 10 ] H . Yu , J . Yang , and J . Han , “ Classifying large data sets using SVMs with hierarchical clusters , ” in ACM KDD , 2003 .
[ 11 ] M . Ferris and T . Munson , “ Interior point methods for massive support vector machines , ” SIAM Journal on Optimization , vol . 13 , no . 3 , pp . 783–804 , 2003 .
[ 12 ] D . P . Bertsekas , Nonlinear Programming . Belmont ,
MA 02178 9998 : Athena Scientific , second ed . , 1999 .
[ 13 ] C C Chang and C J Lin , LIBSVM : a library for support vector machines , 2001 . Software available at http://wwwcsientuedutw/~cjlin/libsvm [ 14 ] T . Joachims , “ Making large scale SVM learning practical , ” in Advances in Kernel Methods Support Vector Learning , MIT Press , 1998 .
[ 15 ] R . Memisevic , “ Dual optimization of conditional probability models , ” tech . rep . , Department of Computer Science , University of Toronto , 2006 .
[ 16 ] F . P´erez Cruz , A . R . Figueiras Vidal , and
A . Art´es Rodr´ıguez , “ Double chunking for solving SVMs for very large datasets , ” in Proceedings of Learning 2004 , Spain , 2004 .
[ 17 ] S . R¨uping , “ mySVM another one of those support vector machines , ” 2000 . Software available at http://www aicsunidortmundde/SOFTWARE/MYSVM/
[ 18 ] K . G . Morse , Jr . , “ Compression tools compared , ”
Linux Journal , 2005 .
[ 19 ] K . Crammer and Y . Singer , “ On the learnability and design of output codes for multiclass problems , ” in COLT , 2000 .
[ 20 ] P . Rai , H . Daum´e III , and S . Venkatasubramanian ,
“ Streamed learning : One pass SVMs , ” in IJCAI , 2009 .
[ 21 ] Z Q Luo and P . Tseng , “ On the convergence of coordinate descent method for convex differentiable minimization , ” J . Optim . Theory Appl . , vol . 72 , no . 1 , pp . 7–35 , 1992 .
APPENDIX Proof of Theorem 1 If each sub problem involves a finite number of coordinate descent updates , then Algorithm 1 can be regarded as a coordinate descent method . We apply Theorem 2.1 of [ 21 ] to obtain the convergence results . The theorem requires that ( 3 ) satisfies certain conditions and in the coordinate descent method there is an integer t such that every αi is iterated at least once every t successive updates ( called almost cyclic rule in [ 21] ) . Following the same analysis in the proof of [ 5 , Theorem 1 ] , ( 3 ) satisfies the required conditions . Moreover , the two properties on tj,k imply the almost cyclic rule . Hence both global and linear convergence results are obtained .
Proof of Theorem 2 To begin , we discuss the stopping condition of LIBLINEAR . Each run of LIBLINEAR to solve a sub problem generates {αk,j,v | v = 1 , . . . , tk,j + 1} with
αk,j = αk,j,1 and αk,j+1 = αk,j,tk,j +1 .
We further let ij,v denote the index of the variable being updated by αk,j,v+1 = αk,j,v + d∗eij,v , where d∗ is the optimal solution of min d f ( αk,j,v + deij,v ) subject to 0 ≤ αk,j,v ij,v
+ d ≤ C , ( 15 ) and eij,v is an indicator vector for the ( ij,v)th element . All tk,j updates can be further separated to several rounds , where each one goes through all elements in Bj . LIBLINEAR checks the following stopping condition in the end of each round : v∈a round∇P ij,v f ( αk,j,v ) ≤ ǫ , ( 16 ) max where ǫ is a tolerance and ∇P f ( α ) is the projected gradient : ij,v f ( αk,j,v ) − min v∈a round∇P
∇P i f ( α ) =
 
∇if ( α ) max(0 , ∇if ( α ) ) min(0 , ∇if ( α ) ) if 0 < αi < C , if αi = C , if αi = 0 .
( 17 )
The reason that LIBLINEAR considers ( 16 ) is that from the optimality condition , α∗ is optimal if and only if ∇P f ( α∗ ) = 0 . Next we prove the theorem by showing that for all j =
1 , . . . , m there exists kj such that
∀k ≥ kj , tk,j ≤ 2|Bj| .
( 18 )
Suppose that ( 18 ) does not hold . We can find a j and a sub sequence R ⊂ {1 , 2 , . . .} such that tk,j > 2|Bj| , ∀k ∈ R .
( 19 ) Since {αk,j | k ∈ R} are in a compact set , we further consider a sub sequence M ⊂ R such that {αk,j | k ∈ M} converges to a limit point ¯α . Let σ ≡ mini Qii . Following the explanation in [ 5 , Theorem 1 ] , we only need to analyze indices with Qii > 0 . Therefore , σ > 0 . Lemma 2 of [ 5 ] shows that f ( αk,j,v ) − f ( αk,j,v+1 ) ≥ ∀v = 1 , . . . , 2|Bj| .
σ 2 kαk,j,v − αk,j,v+1k2 ,
( 20 )
In the second round , assume αi is not changed again until the v′th update . From ( 29 ) and ( 23) (25 ) , we have or or
|∇if ( αk,j,v′
)| ≤
ǫ 4
,
∇if ( αk,j,v′
) ≥ −
ǫ 4 and αk,j,v′ i
= 0 ,
∇if ( αk,j,v′
) ≤
ǫ 4 and αk,j,v′ i
= C .
( 30 )
( 31 )
( 32 )
Using ( 30) (32 ) , the projected gradient defined in ( 17 ) satisfies i ( αk,j,v′
|∇P
)| ≤
ǫ 4
.
This result holds for all i ∈ Bj . Therefore , max v∈2nd round∇P ij,v ( αk,j,v ) − min ǫ ǫ ) = 4 − ( − 2
< ǫ .
ǫ 4 v∈2nd round∇P ij,v ( αk,j,v )
≤
Thus ( 16 ) is valid in the second round . Then tk,j = 2|Bj| violates ( 19 ) . Hence ( 18 ) holds and the theorem is obtained .
The sequence {f ( αk ) | k = 1 , . . .} is decreasing and bounded below as the feasible region is compact . Hence f ( αk,j,v ) − f ( αk,j,v+1 ) = 0 , lim k→∞
( 21 )
∀v = 1 , . . . , 2|Bj| .
Using ( 21 ) and taking the limit on both sides of ( 20 ) , we have lim k∈M,k→∞
αk,j,2|Bj |+1 = lim k∈M,k→∞
αk,j,2|Bj | = · · ·
= lim k∈M,k→∞
αk,j,1 = ¯α .
From the continuity of ∇f ( α ) and ( 22 ) , we have lim k∈M,k→∞ ∇f ( αk,j,v ) = ∇f ( ¯α ) , ∀v = 1 , . . . , 2|Bj| . Hence there are ǫ and ¯k such that ∀k ∈ M with k ≥ ¯k
|∇if ( αk,j,v)| ≤ ∇if ( αk,j,v ) ≥ ∇if ( αk,j,v ) ≤ −
ǫ 4 3ǫ 4 3ǫ 4 if ∇if ( ¯α ) = 0 , if ∇if ( ¯α ) > 0 , if ∇if ( ¯α ) < 0 ,
( 22 )
( 23 )
( 24 )
( 25 ) for any i ∈ Bj , v ≤ 2|Bj| . When we update αk,j,v to αk,j,v+1 by changing the ith element ( ie , i = ij,v ) in the first round , the optimality condition for ( 15 ) implies that one of the following three situations occurs :
∇if ( αk,j,v+1 ) = 0 , ∇if ( αk,j,v+1 ) > 0 and αk,j,v+1 ∇if ( αk,j,v+1 ) < 0 and αk,j,v+1 i i
= 0 ,
= C .
( 26 )
( 27 )
( 28 )
From ( 23) (25 ) , we have that i satisfies
( 26 ) ( 27 ) ( 28 )
 
⇒
 
∇if ( ¯α ) = 0 ∇if ( ¯α ) ≥ 0 ∇if ( ¯α ) ≤ 0
.
( 29 )
