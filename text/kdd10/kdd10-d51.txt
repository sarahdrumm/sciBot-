Discriminative Topic Modeling based on Manifold Learning
Seungil Huh
Carnegie Mellon University
5000 Forbes Ave .
Pittsburgh , PA seungilh@cscmuedu
Stephen E . Fienberg Carnegie Mellon University
5000 Forbes Ave .
Pittsburgh , PA fienberg@statcmuedu
ABSTRACT Topic modeling has been popularly used for data analysis in various domains including text documents . Previous topic models , such as probabilistic Latent Semantic Analysis ( pLSA ) and Latent Dirichlet Allocation ( LDA ) , have shown impressive success in discovering low rank hidden structures for modeling text documents . These models , however , do not take into account the manifold structure of data , which is generally informative for the non linear dimensionality reduction mapping . More recent models , namely Laplacian PLSI ( LapPLSI ) and Locally consistent Topic Model ( LTM ) , have incorporated the local manifold structure into topic models and have shown the resulting benefits . But these approaches fall short of the full discriminating power of manifold learning as they only enhance the proximity between the low rank representations of neighboring pairs without any consideration for non neighboring pairs . In this paper , we propose Discriminative Topic Model ( DTM ) that separates non neighboring pairs from each other in addition to bringing neighboring pairs closer together , thereby preserving the global manifold structure as well as improving the local consistency . We also present a novel model fitting algorithm based on the generalized EM and the concept of Pareto improvement . As a result , DTM achieves higher classification performance in a semi supervised setting by effectively exposing the manifold structure of data . We provide empirical evidence on text corpora to demonstrate the success of DTM in terms of classification accuracy and robustness to parameters compared to state of the art techniques .
Categories and Subject Descriptors H28 [ Database Application ] : Data mining ; I70 [ Document and Text Processing ] : General General Terms Algorithms Keywords Topic modeling , Dimensionality reduction , Document classification , Semi supervised learning
1 .
INTRODUCTION
Topic models are based on the notion that each data component ( eg , a document ) can be represented by a mixture of basic components ( or topics ) . In text analysis , topic models typically adopt the bag of words assumption that ignores the information from the ordering of words . Each document in a given corpus is thus represented by a histogram containing the occurrence of words . The histogram is modeled by a distribution over a certain number of topics , each of which is a distribution over words in the vocabulary . By learning the distributions , a corresponding low rank representation of the high dimensional histogram can be obtained for each document . Topic models , such as probabilistic Latent Semantic Analysis ( pLSA ) [ 11 ] and Latent Dirichlet Allocation ( LDA ) [ 4 ] have shown impressive empirical success by improving classification accuracy through the discovery of low rank hidden structures . In addition , these models provide probabilistic interpretations of the generative process of data .
According to recent research [ 17 , 14 , 2 ] , data from texts or images are often found to be placed on a low rank non linear manifold within the high dimensional space of the original data . Therefore , learning the manifold structure can provide better dimensionality reduction mapping and visualization . Based on this assumption , several topic models were recently developed , namely , Laplacian Probabilistic Latent Semantic Indexing ( LapPLSI ) [ 6 ] and Locally consistent Topic Modeling ( LTM ) [ 7 ] . Both of the topic models increase the proximity between the probability distributions of the data pairs with favorable relationships ( ie , within class pairs or neighbors in manifolds ) by adding the proximity as a regularization term to the log likelihood function of pLSA . As a result , these models obtain probabilistic distributions concentrated around the manifold and show higher accuracy than pLSA and LDA for text clustering and classification tasks . However , LapPLSI and LTM fall short of the full discriminating power of manifold learning because the global manifold structure is often not well preserved only by enhancing the proximity between favorable pairs . The unfavorable relationships ( ie , between class pairs or non neighbors in manifolds ) between data pairs should also be considered .
In this work , we propose a new topic model to focus more on discriminating power , which we refer to as Discriminative Topic Model ( DTM ) . In order to address real world problems in a semi supervised setting ( ie , using a small amount of labeled data with a large amount of unlabeled data ) , DTM maintains the local consistency by considering the manifold structure of data as do LapPLSI and LTM . However , DTM
653 explicitly aims not only to increase the proximity between the probability distributions of the data pairs with favorable relationships , but also to increase the separability between those of the data pairs with unfavorable relationships . Due to the effectiveness of this more refined manifold learning formulation , DTM also preserves the global manifold structure , showing better performance in real word document classification tasks than the previous approaches . We also present an efficient algorithm to solve the proposed regularized loglikelihood maximization problem based on the generalized Expectation Maximization algorithm [ 10 ] and the concept of Pareto improvement [ 1 ] . Our model fitting algorithm does not require the regularization parameter to which the classification performance can be sensitive . We offer empirical evidence on two real world text corpora ( 20 newsgroups and Yahoo! News K series ) and demonstrate the superiority of DTM to state of the art techniques .
The remainder of this paper is organized as follows . Section 2 provides the background and Section 3 overviews previous works . We then formulate DTM and describe how to fit the proposed model in Section 4 . The experimental results with discussions are presented in Section 5 , followed by conclusions in Section 6 .
2 . BACKGROUND AND NOTATIONS
We begin by describing the two basic components of our method : probabilistic Latent Semantic Analysis ( pLSA ) [ 11 ] as a topic model and Laplacian Eigenmaps [ 2 ] as a manifold learning algorithm .
2.1 Probabilistic Latent Semantic Analysis
One of the most well known and fundamental topic models is probabilistic Latent Semantic Analysis ( pLSA ) [ 11 ] . Evolved from Latent Semantic Indexing ( LSA ) [ 9 ] , pLSA defines a proper generative model based on a solid statistical foundation .
Suppose that we have a corpus that consists of N documents {d1 , d2 , · · · , dN } with words from a vocabulary containing M words {w1 , w2 , · · · , wM } . In pLSA , the occurrence of a word w in a particular document d is associated with one of K unobserved topic variables {z1 , z2 , · · · , zK} . More formally , pLSA can be defined by the following generative process :
• select a document d with probability P ( d )
• pick a latent class z with probability P ( z|d )
• generate a word w with probability P ( w|z )
By summing out the latent variable z , the joint probability of an observed pair ( d , w ) can be computed as
P ( d , w ) = P ( d)P ( w|d )
= P ( d )
K
Xk=1
P ( w|zk)P ( zk|d )
( 1 ) where n(d , w ) denotes the number of times word w occurred in document d . Following the likelihood principle , one can determine P ( w|z ) and P ( z|d ) by maximizing the relevant part of Eq ( 2 ) :
L =
N
M
Xi=1
Xj=1 n(di , wj ) log
K
Xk=1
2.2 Laplacian Eigenmaps
P ( wj|zk)P ( zk|di )
( 3 )
Traditional manifold learning algorithms [ 17 , 14 , 2 ] have given way to recent graph based semi supervised learning algorithms [ 19 , 18 , 3 ] . The goal of manifold learning is to recover the structure of a given dataset by non linear mapping into a low dimensional space . As a manifold learning algorithm , Laplacian Eigenmaps [ 2 ] was developed based on spectral graph theory [ 8 ] .
Suppose that we have N data points {u1 , u2 , · · · , uN } , each of which is an M × 1 vector . From the nearest neighbor graph of these data points , we define a local similarity matrix W that contains favorable pair wise relationships among them :
Wij =(1 ,
0 , otherwise if ui ∈ Nr(uj ) or uj ∈ Nr(ui )
( 4 ) where Nr(u ) is the set of the r nearest neighbors of u .
Let xi , which is a K × 1 vector , be a low rank representation of ui on the manifold ( ie , K ≪ N ) . Intuitively , if two data points ui and uj are close to each other in the original space , the corresponding low rank representations xi and xj should also lie near each other . From this intuition , Laplacian Eigenmaps minimize the following objective function :
Wij||xi − xj||2
( 5 )
N
Xi,j=1
This optimization problem , however , produces trivial solutions x1 = x2 = · · · = xN . To avoid these outcomes , we also need to somehow maintain unfavorable relationships between data points . For example , the original Laplacian Eigenmaps [ 2 ] impose the constraint , XDX T = I where X is the matrix , the i th column of which is xi and D is a diagonal matrix such that Dii =Pn
3 . PREVIOUS WORKS j=1 Wij
Cai et al . recently proposed two topic models , Laplacian pLSI ( LapPLSI ) [ 6 ] and Locally consistent Topic Modeling ( LTM ) [ 7 ] , which use manifold structure information based on pLSA . To formalize these models , the objective of the Laplacian Eigenmaps is added as a regularization term to the original log likelihood of pLSA . In LapPLSI , the Euclidean distance is adopted to measure the proximity between two probability distributions :
Based on this joint probability , we can calculate the loglikelihood as
−
˜L =
N
M
Xi=1
Xj=1 n(di , wj ) logP ( di )
K
Xk=1
P ( wj|zk)P ( zk|di )
( 2 ) n(di , wj ) log
P ( wj|zk)P ( zk|di )
K
Xk=1
N
M
L =
Xi=1
Xj=1 Xk=1
K
λ 2
N
Xi,j=1
Wij,P ( zk|di ) − P ( zk|dj) 2
( 6 ) where λ is the regularization parameter and W is an N × N matrix measuring the local similarity of document pairs based on word occurrences .
654 In LTM , Kullback Leibler Divergence is used instead of is as follows : the Euclidean distance :
N
M
K
L =
−
Xk=1 n(di , wj ) log
P ( wj|zk)P ( zk|di )
Xj=1 Xi=1 WijD,P ( z|di)||P ( z|dj) + D,P ( z|dj)||P ( z|di)Xi,j=1
λ 2
( 7 )
N
By discovering the local neighborhood structure , these two models show more discriminating power than pLSA and LDA for document clustering and classification tasks .
However , both of the models fall short of the full discriminating power of Laplacian Eigenmaps because the global manifold structure is often not well preserved only by enhancing the proximity between favorable pairs without maintaining or increasing the separability between unfavorable pairs . In addition , these models are limited in that their performance depends on the regularization parameter λ ; futhermore , it is unclear how to appropriately determine its value .
4 . DISCRIMINATIVE TOPIC MODEL
In this section , we formalize our proposed model , named Discriminative Topic Model ( DTM ) . We also present an algorithm to solve the proposed regularized log likelihood maximization problem based on the generalized Expectation Maximization ( EM ) algorithm [ 10 ] and the concept of Pareto improvement [ 1 ] .
4.1 Regularized Model
When increasing the local consistency in manifold learning of data , we also need to maintain or increase the separability of the low rank probability distributions of documents whose word occurrences are not close to each other . More formally , we need to minimize the proximity of the probability distributions of favorable pairs , expressed by
N
K
Xi,j=1
Xk=1
Wij,P ( zk|di ) − P ( zk|dj) 2
( 8 )
Simultaneously , we need to maintain or maximize the separability of the probability distributions of unfavorable pairs , which can be expressed by
N
M
K
L = n(di , wj ) log
P ( wj|zk)P ( zk|di )
Xk=1
Xi=1 Xj=1 + λ PN i,j=1PK PN i,j=1PK k=1,P ( zk|di ) − P ( zk|dj) 2 k=1 Wij,P ( zk|di ) − P ( zk|dj) 2
( 12 ) where λ is the regularization parameter . Although this parameter is presented , due to the nature of our model fitting algorithm , this parameter need not be considered as we will elaborate in the following section .
4.2 Model Fitting
When a probabilistic model involves unobserved latent variables , the EM algorithm is generally used for the maximum likelihood estimation of the model . Here we use the generalized EM algorithm which in the M step finds parameters that “ improve ” the expected value of the log likelihood function rather than “ maximizing ” it . For more details , see [ 10 ] .
Let φ = {P ( wj|zk)} and θ = {P ( zk|di)} , which are parameters of DTM . Thus , M K + KN parameters are needed to be estimated , which is the same as pLSA .
E step : The E step of DTM is exactly the same as that of pLSA [ 11 ] . By applying Bayes’ formula , we compute posterior probabilities .
PK
P ( zk|di , wj ) =
P ( wj|zk)P ( zk|di ) k′=1 P ( wj|zk′ )P ( zk′ |di )
( 13 )
M step : In the M step of DTM , we improve the expected value of the log likelihood function which is
Q(φ , θ ) = Q1(φ , θ ) + λQ2(θ )
( 14 )
N
M
K
=
Xi=1
P ( zk|di , wj ) log[P ( wj|zk)P ( zk|di ) ] n(di , wj )
Xk=1 Xj=1 k=1,P ( zk|di ) − P ( zk|dj) 2 i,j=1PK + λ PN k=1 Wij,P ( zk|di ) − P ( zk|dj) 2 PN i,j=1PK
The M step re estimation equations for φ are exactly the same as those of pLSA because the regularization term of DTM does not include P ( wj|zk ) .
( 9 )
P ( wj|zk ) = i=1 n(di , wj)P ( zk|di , wj ) i=1 n(di , wj ′ )P ( zk|di , wj ′ )
( 15 )
PN PM j ′=1PN
N
K
Xi,j=1
Xk=1
( 1 − Wij),P ( zk|di ) − P ( zk|dj) 2
Putting these two objectives together , we maximize the following objective function : which is equivalent to k=1(1 − Wij),P ( zk|di ) − P ( zk|dj) 2 i,j=1PK PN k=1 Wij,P ( zk|di ) − P ( zk|dj) 2 PN i,j=1PK k=1(1 − Wij),P ( zk|di ) − P ( zk|dj) 2 i,j=1PK PN k=1 Wij,P ( zk|di ) − P ( zk|dj) 2 PN i,j=1PK k=1,P ( zk|di ) − P ( zk|dj) 2 = PN i,j=1PK k=1 Wij,P ( zk|di ) − P ( zk|dj) 2 i,j=1PK PN
( 10 )
+ 1
( 11 )
Our regularized model is regularized with this term to learn the manifold structure , in addition to adopting the generative process of pLSA . Thus , the log likelihood of our model
Before describing the M step re estimation algorithm for θ , we introduce the concept of Pareto improvement [ 1 ] , based on which we propose our algorithm .
Pareto improvement is defined as a change from one status to another that can improve at least one objective without worsening any other objectives . More formally , in our problem , an update θ(t ) → θ(t+1 ) is a Pareto improvement if either of the following two conditions is satisfied .
1 . Q1(φ , θ(t+1 ) ) > Q1(φ , θ(t ) ) and Q2(θ(t+1 ) ) ≥ Q2(θ(t ) ) 2 . Q1(φ , θ(t+1 ) ) ≥ Q1(φ , θ(t ) ) and Q2(θ(t+1 ) ) > Q2(θ(t ) )
Based on the concepts of generalized EM and Pareto improvement , we re estimate θ by 1 ) increasing Q(φ , θ ) rather than maximizing it and 2 ) increasing at least one of Q1(φ , θ ) and Q2(θ ) without decreasing the other .
655 One advantage of Pareto improvement is that Q(φ , θ ) is improved regardless of the regularization parameter λ whose value affects the performance of previous models , and , yet is hard to determine appropriately .
In order to present a re estimating algorithm for θ to increase Q(φ , θ ) based on Pareto improvement , we first propose re estimating equations to increase each of Q1(φ , θ ) and Q2(θ ) .
Theorem 1 . If θ(t+1 ) is computed from θ(t ) by applying the following re estimation equations
P ( zk|di ) = PM j=1 n(di , wj)P ( zk|di , wj )
( 16 ) j=1 n(di , wj )
PM then Q1(φ , θ ) monotonically increases when θ moves from θ(t ) to θ(t+1 ) along the line with fixed φ .
Proof . Q1(φ , θ ) is the expected value of the log likelihood function of pLSA and Eq ( 16 ) is the re estimation equations for P ( zk|di ) of pLSA ; thus , θ(t+1 ) maximizes Q1(φ , θ ) when φ is fixed . Since Q1(φ , θ ) is a concave function of θ and θ(t+1 ) is the maximum solution of Q1(φ , θ ) , Q1(φ , θ ) monotonically increases when θ moves from θ(t ) to θ(t+1 ) along the line .
Theorem 2 . Let α be the estimated value of the regularization term under the current estimates of the parameters : ie ,
α = PN i,j=1PK PN i,j=1PK k=1,P ( zk|di ) − P ( zk|dj) 2 k=1 Wij,P ( zk|di ) − P ( zk|dj) 2
And we define β for topic id p and document id i as
( 17 )
β = min N P ( zp|di ) + αPN j=1 P ( zp|dj ) + αDiiP ( zp|di ) j=1 Wij P ( zp|dj )
,
1
P ( zp|di)!
( 18 ) Then , Q2(θ ) is nondecreasing by the following re estimation equations for [ P ( z1|di ) , P ( z2|di ) , · · · , P ( zK|di) ] :
PN
P ( zk|di ) =(βP ( zp|di ) ,
1−βP ( zp|di ) 1−P ( zp|di ) P ( zk|di ) , if k = p otherwise
( 19 )
Proof . See Appendix A .
It is worth mentioning that the minimum operator is inserted in Eq ( 18 ) to ensure that [ P ( z1|di ) , · · · , P ( zK|di ) ] becomes a probability distribution after re estimation . It can k=1 P ( zk|di ) = 1 and ∀k , P ( zk|di ) ≥ be easily verified thatPK
0 after the re estimation .
The re estimation equations in Theorem 2 can be simplified and parallelized by matrix computation . See Appendix B for more details .
Now we propose our re estimating algorithm for θ . Let the current parameters be θ0 . In order to maximize the discriminating power , we first compute θ1 by repeatedly applying Eq ( 19 ) to θ0 with all possible pairs of ( topic id , document id ) . Theorem 2 guarantees that Q2(θ1 ) ≥ Q2(θ0 ) . We then test whether Q1(φ , θ1 ) ≥ Q1(φ , θ0 ) , and if it is true , re estimating for θ is done by setting θ = θ1 .
If Q1(φ , θ1 ) < Q1(φ , θ0 ) , θ is re estimated through the local search from θ1 as follows . θ2 is computed from θ1 by applying the E step in Eq ( 13 ) and the pLSA M step in Eq ( 16 ) . Theorem 1 ensures that Q1(φ , θ ) monotonically
Algorithm 1 Model fitting for DTM Input : n(di , wj ) : word occurrences in each document , N : # of documents , M : size of vocabulary , K : # of topics , W : similarity matrix , γ : step size , MI : max # of iterations . Output : φ = {P ( wj|zk)} and θ = {P ( zk|di)} .
1 : Randomly initialize φ and θ . 2 : t ← 0 3 : while t < MI do 4 : E STEP : 5 : 6 : M STEP : 7 : 8 : 9 : 10 : 11 :
Re estimate φ as in Eq ( 15 ) . θ1 ← θ for p = 1 to K do for i = 1 to N do end for if Q1(φ , θ1 ) ≥ Q1(φ , θ ) then
Re estimate θ by θ ← θ1 end for else
12 : 13 : 14 : 15 : 16 : 17 : 18 : 19 : 20 : 21 : 22 : 23 : 24 : 25 : end if 26 : t ← t + 1 27 : 28 : end while end if
Compute P ( zk|di , wj ) using φ and θ as in Eq ( 13 ) . update P ( z1|di ) , · · · , P ( zK|di ) in θ1 with topic id p and document id i as in Eq ( 19 ) .
Compute P ( zk|di , wj ) using φ and θ1 as in Eq ( 13 ) . Compute θ2 from θ1 as in Eq ( 16 ) . θ3 ← θ1 , s ← 0 repeat
θ3 ← θ3 + γ(θ2 − θ1 ) , s ← s + γ until Q1(φ , θ3 ) ≥ Q1(φ , θ ) or s + γ > 1 if Q1(φ , θ3 ) ≥ Q1(φ , θ ) and Q2(θ3 ) ≥ Q2(θ ) then
Re estimate θ by θ ← θ3 increases when θ moves from θ1 to θ2 along the line . Thus , θ3 is initially set as θ1 and iterate the following update until Q1(φ , θ3 ) ≥ Q1(φ , θ0 )
θ3 = θ3 + γ(θ2 − θ1 )
( 20 ) where γ is the step parameter such that 0 < γ ≪ 1 .
We then test whether Q2(θ3 ) ≥ Q2(θ0 ) . If it is true , reestimating for θ is done by setting θ = θ3 . Otherwise , we keep θ as θ0 without updating in the M step and continue to the next E step . Our model fitting algorithm is summarized in Algorithm 1 .
It is worth discussing the role of the step parameter γ in Eq ( 20 ) . In some sense , γ plays a role in controlling the balance of the log likelihood term Q1(φ , θ ) and the regularization term Q2(θ ) ; the balance control is originally the role of the regularization parameter λ in Eq ( 14 ) . If γ is small , θ tends to be relatively close to θ1 ; thus , the gap between two Q1(φ , θ ) in the consecutive iterations tends to be small , which leads to relatively large Q1(φ , θ ) and small Q2(θ ) in the end of the fitting . Similarly , if γ is large , we can expect relatively small Q1(φ , θ ) and large Q2(θ ) in the end of the fitting . Though γ influences the final value of Q1 and Q2 , classification performance is not sensitive to γ as we will empirically show in the following section .
656 5 . EXPERIMENTS
In this section , we evaluate the proposed DTM on the two widely used text corpora , 20 newsgroups and Yahoo! News K series , in document classification .
5.1 Datasets and Experimental setup
The 20 newsgroups corpus is a collection of approximately
20,000 newsgroup documents , partitioned almost evenly across 20 different newsgroups [ 12 ] . The preprocessed version was downloaded from R . F . Corrˆea ’s webpage1 ; this version includes 8,156 distinct words and is divided into a training set and a test set . Among the documents in the training set , we randomly select 100 documents from each category for each test run so that 2,000 documents are used for classification . Yahoo! News K series is a collection of 2340 news articles belonging to one of 20 different categories [ 5 ] . The preprocessed version including 8,104 distinct words was downloaded from D . L . Boley ’s webpage2 . Among all documents , we select the documents belonging to the category “ Entertainment ” and its sub categories , using 1389 documents with 15 categories for every test run ; these categories have varying sizes ranging from 278 to 9 .
We evaluate the performance of DTM and provide comparison against previous topic models ( including LapPLSI and LTM ) and other traditional dimension reduction algorithms :
• Probabilistic Latent Semantic Analysis ( pLSA ) [ 11 ]
• Latent Dirichlet Allocation ( LDA ) [ 4 ]
• Laplacian Probabilistic Latent Semantic Indexing ( Lap
PLSI ) [ 6 ]
• Locally consistent Topic Modeling ( LTM ) [ 7 ]
• Principal Component Analysis ( PCA )
• Non negative Matrix Factorization ( NMF ) [ 13 ] .
Additionally , the approach using raw word histograms without any dimension reduction is tested .
To address real world problems in a semi supervised setting , we randomly select a small number of documents ( one of 1 , 3 , 5 , and 10 ) from each category as labeled data ; the rest are considered to be unlabeled data . For each approach , we explore several numbers of topics or dimensionalities of the embedding space . For classification , a linearkernel Support Vector Machine ( SVM ) is trained on the lowdimensional representations ( ie , P ( zk|di ) for topic models ) . After 20 test runs , the average of the accuracies is reported .
5.2 Implementation Details
The tf idf weight scheme [ 15 ] is first applied to the word occurrences . The histogram intersection is then computed to measure the similarity of two documents after L1 normalization for each document . More formally , the similarity of two documents di and dj is calculated as
M
Xk=1 min n(di , wk ) n(di )
, n(dj , wk ) n(dj )
( 21 ) where n(d , w ) is the number of occurrences of word w and n(d ) is the total number of words in document d ; ie , n(d ) = 1 http://sitesgooglecom/site/renatocorrea02/textcategorizationdatasets/ http://www userscsumnedu/∼boley/ftp/PDDPdata/
2
PM k=1 n(d , wk ) . We found that this histogram intersection is as effective as the Euclidean distance in discovering the nearest neighbors of each document , which are in the same category .
We additionally utilize class label information when constructing the similarity matrix W , as described in the previous work [ 7 ] . More specifically , after generating a r nearest neighbor graph in an unsupervised manner , we add edges between documents belonging to the same category and remove edges between documents belonging to different categories .
In our experiments , we set the number of the nearest neighbors r as 10 , and the step parameter γ as 01 Although we chose these parameters , the classification performance is not sensitive to these parameters as we will show later in this section .
For performance comparison , we implemented the other approaches as follows . For pLSA , the source codes were downloaded from Peter Gehler ’s code and dataset page3 . For LDA , Matlab Topic Modeling Toolbox 1324 was used . For LapPLSI and LTM , the source codes were downloaded from the author ’s webpage5 . We directly implemented the other two methods : PCA and NMF . The regularization parameters of LapPLSI and LTM were tuned to produce the best performance among 1 , 10 , 100 , and 1000 . All the other parameter settings and implementation details were set to be identical to DTM .
5.3 Results and Discussions
Figures 1 and 2 demonstrate that DTM consistently outperforms all other approaches , including the most recently proposed LapPLSI and LTM , in terms of classification accuracy . From these results , we can conclude that DTM is more successful in exposing the manifold structure inherent in 20 newsgroups and Yahoo! News K series corpora . LapPLSI and LTM do not show such capabilities because they are not effective in preserving the global manifold , which is expected to be found in both the corpora since they comprise groups of highly related categories .
Among previous approaches , LapPLSI and LTM show higher performance than pLSA and LDA , as expected . Although LapPLSI and LTM do not reach the full discriminating power of manifold learning , they can still find a low rank nonlinear embedding space to which documents are mapped . On the other hand , pLSA and LDA , which do not adopt any regularization for manifold learning , cannot find such a nonlinear embedding space . The performance of pLSA decreases as the number of topics increases beyond a certain point ; it is well known that pLSA is prone to overfitting due to the large number of parameters which grows proportionally with data size . PCA and NMF also demonstrate similar tendencies on both of the corpora .
Figure 3 shows that DTM is insensitive to the variation in the two parameters utilized by the model : the number of the nearest neighbors r and the step size γ . Additionally , in contrast to LapPLSI and LTM , the regularization parameter is not needed in DTM . Therefore , DTM is negligibly affected by parameter changes .
3
4
5 http://wwwkybmpgde/bs/people/pgehler/code/indexhtml http://psiexpssuciedu/research/programs data/toolbox.htm http://wwwzjucadcgcn/dengcai/LapPLSA/indexhtml
657 )
%
( y c a r u c c a
)
%
( y c a r u c c a
)
%
( y c a r u c c a
)
%
( y c a r u c c a number of labeled documents = 1 number of labeled documents = 3
50
45
40
35
30
25
20
15
10
65
60
55
50
45
40
35
30
25
10 20 30
50
70
100 number of topics ( reduced dimensionality ) number of labeled documents = 5
10 20 30
50
70
100 number of topics ( reduced dimensionality )
)
%
( y c a r u c c a
)
%
( y c a r u c c a
60
55
50
45
40
35
30
25
20
70
65
60
55
50
45
40
35
30
10 20 30
50
70
100 number of topics ( reduced dimensionality ) number of labeled documents = 10
10 20 30
50
70
100 number of topics ( reduced dimensionality )
DTM LTM LapPLSI LDA pLSA PCA NMF words
Figure 1 : Classification performance on 20 newsgroups ( best viewed in color ) number of labeled documents = 1 number of labeled documents = 3
30
25
20
15
10
45
40
35
30
25
20
15
10
5
10
20
30
50 number of topics ( reduced dimensionality ) number of labeled documents = 5
5
10
20
30
50 number of topics ( reduced dimensionality )
)
%
( y c a r u c c a
)
%
( y c a r u c c a
40
35
30
25
20
15
10
50
45
40
35
30
25
20
15
10
5
10
20
30
50 number of topics ( reduced dimensionality ) number of labeled documents = 10
5
10
20
30
50 number of topics ( reduced dimensionality )
DTM LTM LapPLSI LDA pLSA PCA NMF words
Figure 2 : Classification performance on Yahoo! News K series ( best viewed in color )
658 )
%
( y c a r u c c a
65
60
55
50
45
40
35
30
25
5
10
15
20 number of the nearest neighbors
)
%
( y c a r u c c a
65
60
55
50
45
40
35
30
25
0.05
0.10
0.15
0.20 step size
DTM LTM LapPLSI LDA pLSA PCA NMF words
Figure 3 : Classification performance of DTM as the parameters are varied ( 5 labeled data for each category and 100 topics on 20 newsgroups , best viewed in color )
6 . CONCLUSIONS
In this paper , we have proposed a topic model that incorporates the information from manifold structures of data by considering unfavorable relationships in addition to favorable ones ; the former are ignored in previous work . We have also presented an efficient model fitting algorithm , based on generalized EM and Pareto improvement , which enables reliable discovery of the low rank hidden structures by minimizing the sensitivity to parameters . We empirically demonstrated that our approach ourperforms previous topic models in terms of classification accuracy in a semi supervised setting on two text corpora .
7 . ACKNOWLEDGMENTS
Seungil Huh is partially supported by a Samsung Fellowship . We thank the anonymous reviewers for their insightful feedback and Yoongu Kim for providing helpful comments for the final manuscript .
8 . REFERENCES
[ 1 ] N . Barr . Economics of the welfare state . Oxford
University Press , New York , USA , 2004 .
[ 2 ] M . Belkin and P . Niyogi . Laplacian eigenmaps and spectral techniques for embedding and clustering . In Advances in Neural Information Processing Systems , volume 14 , pages 586–691 , 2001 .
[ 3 ] M . Belkin , P . Niyogi , and V . Sindhwani . Mainfold regularization : A geometric framework for learning from examples . Journal of Machine Learning , 7:2399–2434 , 2006 .
[ 4 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent dirichlet allocation . Journal of Machine Learning , 3:993–1022 , 2003 .
[ 5 ] D . L . Boley . Principal direction divisive partitioning . In Data Mining and Knowledge Discovery , volume 2 , pages 325–344 , 1998 .
[ 6 ] D . Cai , Q . Mei , J . Han , and C . Zhai . Modeling hidden topics on document manifold . In Proceedings of the ACM conference on Information and knowledge management , pages 911–920 , 2008 .
[ 7 ] D . Cai , X . Wang , and X . He . Probabilistic dyadic data analysis with local and global consistency . In Proceedings of the International Conference on Machine Learning , pages 105–112 , 2009 .
[ 8 ] F . R . K . Chung . Spectral graph theory . American
Mathematical Society , 1997 .
[ 9 ] S . C . Deerwester , S . T . Dumais , T . K . Landauer , G . W . Furnas , and R . A . Harshman . Indexing by latent semantic analysis . Journal of the American Society of Information Science , 41:391–407 , 1990 .
[ 10 ] A . P . Dempster , N . M . Laird , and D . B . Rubin .
Maximum likelihood from incomplete data via the em algorithm . Journal of Royal Statistical Society . Series B ( Methodological ) , 39:1–38 , 1977 .
[ 11 ] T . Hofmann . Probabilistic latent semantic indexing . In
Proceedings of International Conference on Research and Development in Information Retrieval , pages 50–57 , 1999 .
[ 12 ] Home page for 20 newsgroups data set . http://peoplecsailmitedu/jrennie/20Newsgroups/
[ 13 ] D . D . Lee and H . S . Seung . Algorithms for non negative matrix factorization . In Advances in Neural Information Processing Systems , pages 556–562 , 2000 .
[ 14 ] S . Roweis and L . K . Saul . Nonlinear dimensionality reduction by locally linear embedding . Science , 290:2323–2326 , 2000 .
[ 15 ] G . Salton and C . Buckley . Term weighting approaches in automatic text retrieval . Information Processing and Management , 24:513–523 , 1988 .
[ 16 ] F . Sha , L . K . Saul , and D . D . Lee . Multiplicative updates for nonnegative quadratic programming in support vector machines . In Advances in Neural Information Processing Systems , pages 1041–1048 , 2003 .
[ 17 ] J . B . Tenenbaum , V . de Silva , and J . C . Langford . A global geometric framework for nonlinear dimensionality reduction . Science , 290:2319–2323 , 2000 .
[ 18 ] D . Zhou , O . Bousquet , T . N . Lal , J . Weston , and
B . Sch¨olkopf . Learning with local and global consistency . In Advances in Neural Information Processing Systems , volume 14 , pages 321–328 , 2003 .
[ 19 ] X . Zhu , Z . Ghahramani , and J . D . Lafferty .
Semi supervised learning using gaussian fields and harmonic functions . In Proceedings of the International Conference on Machine Learning , pages 912–919 , 2003 .
659 We define G as an auxiliary function of F ( τ ) by replacing the second order derivative in the Taylor series expansion of F ( τ ) at τ = 1 .
N
∂τ
∂F ( τ )
( τ − 1 )
G(τ , 1 ) =F ( 1 ) + fififififiτ =1 P ( zp|dj ) + αDiiP ( zp|di)(τ − 1)2 ( 30 ) Since G(τ , 1)− F ( τ ) = −c,PN j=1 P ( zp|dj ) + N P ( zp|di) (τ −
1)2 ≤ 0 , G is an auxiliary function of F . Solving ∂G(τ,1 ) ∂τ = 0 yields ˆτ in Eq ( 24 ) , which minimizes G(τ , 1 ) because G(τ , 1 ) is concave with respect to τ . Therefore , by Lemma 1 ,
− c
Xj=1
R(˜θ(t+1 ) ) = F ( ˆτ ) ≥ G(ˆτ , 1 ) ≥ G(1 , 1 ) = F ( 1 ) = R(θ(t ) )
( 31 )
Lemma 3 . R(θ ) is nondecreasing by the updates in Eq ( 19 ) with β in Eq ( 18 ) .
Proof . For any µ such that 0 ≤ µ ≤ 1 ,
G(1 , 1 ) = ( 1−µ)G(1 , 1)+µG(1 , 1 ) ≤ ( 1−µ)G(1 , 1)+µG(ˆτ , 1 ) ( 32 )
Since G(τ , 1 ) is concave ,
( 1 − µ)G(1 , 1 ) + µG(ˆτ , 1 ) ≤ G((1 − µ ) + µˆτ , 1 )
( 33 )
Thus , G(1 , 1 ) ≤ G(ν , 1 ) for any ν that is placed between 1 and ˆτ ( either 1 ≤ ν ≤ ˆτ or ˆτ ≤ ν ≤ 1 ) .
Let θ(t+1 ) be obtained by applying the updates in Eq ( 19 ) to θ(t ) . Since β is always placed between 1 and ˆτ ,
R(θ(t+1 ) ) = F ( β ) ≥ G(β , 1 ) ≥ G(1 , 1 ) = F ( 1 ) = R(θ(t ) )
( 34 )
Proof of Theorem 2 Proof . Since α = Q2(θ(t) ) , R(θ(t ) ) = 0 . By Lemma 3 ,
N
N
K
K
− α
Xi,j=1
R(θ(t+1 ) ) =
Xk=1,P ( zk|di ) − P ( zk|dj) 2fififififiθ=θ(t+1 ) Xi,j=1 Wij,P ( zk|di)(t+1 ) − P ( zk|dj) 2fififififiθ=θ(t+1 ) Xk=1 k=1,P ( zk|di ) − P ( zk|dj) 2fifiθ=θ(t+1 ) i,j=1PK Q2(θ(t+1 ) ) = PN k=1 Wij,P ( zk|di ) − P ( zk|dj) 2fifiθ=θ(t+1 ) PN i,j=1PK
≥ α = Q2(θ(t ) )
Therefore ,
( 35 )
( 36 )
≥ 0
APPENDIX A . Proof of Theorem 2 We reintroduce the concept of auxiliary function [ 13 , 16 ] .
Definition 1 . G(x , x′ ) is an auxiliary function for F ( x ) if the two following conditions are satisfied .
G(x , x′ ) ≤ F ( x ) , G(x , x ) = F ( x )
( 22 )
This definition is useful with the following Lemma .
Lemma 1 . If G(x , x′ ) is an auxiliary function , then F ( x ) is non increasing under the update xt+1 = arg max x
G(x , x′ )
( 23 )
Proof . F ( xt+1 ) ≥ G(xt+1 , xt ) ≥ G(xt , xt ) = F ( xt ) .
We define ˆτ for topic id p and document id i as
ˆτ = and also define
N
R(θ ) = j=1 Wij P ( zp|dj ) j=1 P ( zp|dj ) + αDiiP ( zp|di )
K
N P ( zp|di ) + αPN PN Xk=1,P ( zk|di ) − P ( zk|dj) 2 Xi,j=1
Xk=1
K
N
Wij,P ( zk|di ) − P ( zk|dj) 2
Xi,j=1
− α
( 24 )
( 25 )
Lemma 2 . R(θ ) is nondecreasing after re estimation of [ P ( z1|di ) , P ( z2|di ) , · · · , P ( zK|di ) ] by the following equations with τ = ˆτ .
P ( zk|di ) =(τ P ( zp|di ) ,
1−τ P ( zp|di ) 1−P ( zp|di ) P ( zk|di ) , if k = p otherwise
( 26 )
Proof . Let F ( τ ) be the value of R(θ ) at θ = ˜θ(t+1 ) that is obtained by applying the update in Eq ( 26 ) to the current parameters θ(t ) = {P ( z|d)} . Then ,
∂F ( τ )
∂τ
= 2
N
Xj=1
( 1 − αWij)",τ P ( zp|di ) − P ( zp|dj) P ( zp|di ) 1 − P ( zp|di)# P ( zk|di ) − P ( zk|dj ) P ( zp|di )
1 − P ( zp|di )
−Xk6=p 1 − τ P ( zp|di ) Since Pk6=p P ( zk|d ) = 1 − P ( zp|d ) ,
∂F ( τ )
∂τ
N
Xj=1
P ( zp|dj ) − α
= 2c,N P ( zp|di ) − αDiiP ( zp|di) τ − 2c N Xj=1
WijP ( zp|dj ) 1−P ( zp|di) and Dii =PN ∂τ 2 = 2c,N P ( zp|di ) − αDiiP ( zp|di )
∂2F ( τ ) where c =,P ( zp|di ) + P ( zp|di )
In addition , the second order derivative of F ( τ ) is j=1 Wij .
( 27 )
( 28 )
( 29 )
660 B . Matrix Formulation of Re estimation Equations in Theorem 2 Let P be a matrix such that Pij = P ( zi|dj ) .
Since L is a sparse matrix , α can be efficiently computed . Now we reformalize β as a matrix form . For topic id p and document id i ,
N
K
Xi,j=1
Xk=1
Wij(Pki − Pkj)2 = T r(P LP T )
( 37 ) where L = D − W , which is the graph Laplacian . In the same way ,
N
K
βpi = min ( N P + αP W )pi
N + αP D)pi
( P 1N 1T
,
1
Ppi!
( 40 )
Considering all the documents with the topic id p , we define
~βp = min ( N P + αP W )p
N + αP D)p
( P 1N 1T
,
1T N
Pp!
( 41 )
Xk=1,Pki − Pkj 2 = T r,P ( N IN − 1N 1T
N )P T
Xi,j=1
= N T r(P P T ) − ( P 1N )T ( P 1N ) where Xp is the p th row of matrix X and division is elementwise . Finally , for the topic id p , we obtain the following update for P .
( 38 ) where IN is the N × N identity matrix and 1N is an N by 1 vector with all ones .
From Eqs . ( 37 ) and ( 38 ) ,
α =
N T r(P P T ) − ( P 1N )T ( P 1N )
T r(P LP T )
( 39 )
Pk =(~βp ⊗ Pk ,
1T N −~βp⊗Pp 1T N −Pp if k = p
⊗ Pk , otherwise
( 42 ) where ⊗ denotes element wise multiplication .
661
