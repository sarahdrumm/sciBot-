Probably the Best Itemsets
Nikolaj Tatti
ADReM , University of Antwerp , Antwerpen , Belgium nikolajtatti@uaacbe
ABSTRACT One of the main current challenges in itemset mining is to discover a small set of high quality itemsets . In this paper we propose a new and general approach for measuring the quality of itemsets . The method is solidly founded in Bayesian statistics and decreases monotonically , allowing for efficient discovery of all interesting itemsets . The measure is defined by connecting statistical models and collections of itemsets . This allows us to score individual itemsets with the probability of them occuring in random models built on the data . As a concrete example of this framework we use exponential models . This class of models possesses many desirable properties . Most importantly , Occam ’s razor in Bayesian model selection provides a defence for the pattern explosion . As general exponential models are infeasible in practice , we use decomposable models ; a large sub class for which the measure is solvable . For the actual computation of the score we sample models from the posterior distribution using an MCMC approach .
Experimentation on our method demonstrates the measure works in practice and results in interpretable and insightful itemsets for both synthetic and real world data .
Categories and Subject Descriptors H28 [ Database management ] : Database applications— Data mining ; G.3 [ Probability and Statistics ] : Markov processes
General Terms Algorithms , Theory
Keywords Itemset mining , exponential models , decomposable models , junction trees , Bayesian model selection , MCMC
1 .
INTRODUCTION
Discovering frequent itemsets is one of the most active fields in data mining . As a measure of quality , frequency possesses a lot of positive properties : it is easy to interpret and as it decreases monotonically there exist efficient algorithms for discovering large collections of frequent itemsets [ 2 ] . However , frequency also has serious drawbacks . A frequent itemset may be uninteresting if its elevated frequency is caused by frequent singletons . On the other hand , some non frequent itemsets could be interesting . Another drawback is the problem of pattern explosion when mining with a low threshold .
Many different quality measures have been suggested to overcome the mentioned problems ( see Section 5 for a more detailed discussion ) . Usually these measures compare the observed frequency to some expected value derived , for example , from the independence model . Using such measures we may obtain better results . However , these approaches still suffer from pattern explosion . To point out the problem , assume that two items , say a and b are correlated , and hence are considered significant . Then any itemset containing a and b will be also considered significant .
Example 1 . Assume a dataset with K items a1 , . . . , aK such that a1 and a2 always yield identical value and the rest of the items are independently distributed . Assume that we apply some statistical method to evaluate the significance of itemsets by using the independence model as the ground truth . If an itemset X contains a1a2 , its frequency will be higher than then the estimate of the independence model . Hence , given enough data , the P value of the statistical test will go to 0 , and we will conclude that the itemset X is interesting . Consequently we will find 2K−2 interesting itemsets . In this work we approach the problem of defining quality measure from a novel point of view . We construct a connection between itemsets and statistical models and use this connection to define a new quality measure for itemsets . To motivate this approach further , let us consider the following example .
Example 2 . Consider a binary dataset with 5 items , say a1 , . . . , a5 , generated from the independence model . We argue that if we know that the data comes from the independence model , then the only interesting itemsets are the singletons . The reasoning behind this claim is that the frequencies of singletons correspond exactly to the column margins , the parameters of the independence model . Once we know the singleton frequencies , there is nothing left in the data that would be statistically interesting .
Let us consider a more complicated example . Assume that data is generated from a Chow Liu tree model [ 4 ] , say p(A ) = p(a1)p(a2 | a1)p(a3 | a1)p(a4 | a2)p(a5 | a4 ) .
Again , if we know that data is generated from this model , then we argue that the interesting itemsets are a1 , a2 , a3 , a4 , a5 , a1a2 , a1a3 , a2a4 , and a4a5 . The reasoning here is the same as with the independence model . If we know the frequencies of these itemsets we can derive the parameters of the distribution . For example , p(a2 = 1 | a1 = 1 ) = fr ( a1a2 ) /fr ( a1 ) .
Let us now demonstrate that this approach will produce much smaller and more meaningful output than the method given in Example 1 .
Example 3 . Consider the data given in Example 1 . To fully describe the data we only need to know the frequencies of the singletons and the fact that a1 and a2 are identical . This information can be expressed by outputting the frequencies of singleton itemsets and the frequency of itemset a1a2 . This will give us K + 1 interesting patterns in total .
Our approach is to extend the idea pitched in the preceding example to a general itemset mining framework . In the example we knew which model generated the data , in practice , we typically do not . To solve this we will use the Bayesian approach , and instead of considering just one specific model , we will consider a large collection of models , namely exponential models . A virtue of these models is that we can naturally connect each model to certain itemsets . A model M has a posterior probability P ( M | D ) , that is , how probable is the model given the data . The score of a single itemset then is just the probability of it being a parameter of a random model given the data . This setup fits perfectly the given example . If we have strong evidence that data is coming from the independence model , say M , then the posterior probability P ( M | D ) will be close to 1 , and the posterior probability of any other model will be close to 0 . Since the independence model is connected to the singletons , the score for singletons will be 1 and the score for any other itemset will be close to 0 .
Interestingly , using statistical models for defining significant itemsets provides an approach to the problem of the pattern set explosion ( see Section 3.2 for more technical details ) . Bayesian model selection has an in built Occam ’s razor , favoring simple models over complex ones . Our connection between models and itemsets is such that simple models will correspond to small collections of itemsets . In result , only a small collection of itemsets will be considered interesting , unless the data provides sufficient amount of evidence .
Our contribution in the paper is two fold . First , we introduce a general framework of using statistical models for scoring itemsets in Section 2 . Secondly , we provide an example of this framework in Section 3 by using exponential models and provide solid theoretical evidence that our choices are well founded . We provide the sampling algorithm in Section 4 . We discuss related work in Section 5 and present our experiments in Section 6 . Finally , we conclude our work with Section 7 . The proofs are given in Appendix [ 19 ] . The implementation is provided for research purposes1 .
1http://adremuaacbe/implementations
2 . SIGNIFICANCE OF ITEMSETS BY STA
TISTICAL MODELS
As we discussed in the introduction , our goal is to define a quality measure for itemsets using statistical models . In this section we provide a general framework for such a score . We will define the actual models in the next section .
We begin with some preliminary definitions and notations . In our setup a binary dataset is a collection of N transactions , binary vectors of length K . We assume that these vectors are independently generated from some unknown distribution . Such a dataset can be easily represented by a binary matrix of size N × K . By an attribute ai we mean a Bernoulli random variable corresponding to the ith column of the data . We denote the set of attributes by A = {a1 , . . . , aK} . An itemset X is simply a subset of A . Given an itemset X = {ai1 , . . . , aiL} and a transaction t we denote by tX = ( ti1 , . . . , tiL ) the projection of t into X . We say that t covers X if all elements in tX are equal to 1 . We say that a collection of itemsets F is downward closed if for each member X ∈ F any sub itemset is also included . This property plays a crucial point in mining frequent patterns since it allows effective candidate pruning in level wise approach and branch pruning in a DFS approach .
Our next step is to define the correspondence between statistical models and families of itemsets . Assume that we have a set M of statistical models for the data . We will discuss in later sections what specific models we are interested in , but for the moment , we will keep our discussion on a high level . Each model M ∈ M has a posterior probability P ( M | D ) , that is , how probable the model M is given data D . To link the models to families of itemsets we assume that we have a function fam that identifies a model M with a downward closed family of itemsets . As we will see later on , there is one particular natural choice for such a function . Now that we have our models that are connected to certain families of itemsets , we are ready to define a score for individual itemsets . The score for an itemset X is the posterior probability of X being a member in a family or itemsets , sc(X ) =
P ( M | D ) .
( 1 )
X∈fam(M ) ,
M∈M
The motivation for such score is as follows . If we are sure that some particular model M is the correct model for D , then the posterior probability for that model will be close to 1 and the posterior probabilities for other models will be close to 0 . Consequently , the score for an itemset X will be close to 1 if X ∈ fam(M ) , and 0 otherwise .
Naturally , the pivotal choice of this score lies in the mapping fam . Such a mapping needs to be statistically wellfounded , and especially the size of an itemset family should be reflected in the complexity of the corresponding model . We will see in the following section that a particular choice for the model family and mapping fam has these properties , and leads to certain important qualities .
Proposition 4 . The score decreases monotonically , that is , X ⊆ Y implies sc(X ) ≥ sc(Y ) .
Proof . We are allowing fam only to map on downward
X closed families . Hence the inequality
X
P ( M | D ) ≥ X
X∈fam(M ) ,
M∈M
Y ∈fam(M ) ,
M∈M
P ( M | D ) holds , and consequently we have sc(X ) ≥ sc(Y ) . This completes the proof .
3 . EXPONENTIAL MODELS In this section we will make our framework more concrete by providing a specific set M of statistical models and the function fam identifying the models with families of itemsets . We will first give the definition of the models and the mapping . After this , we point out the main properties of our model and justify our choices . However , as , it turns out that computing the score for these models is infeasible , so instead , we solve this problem by considering decomposable models . 3.1 Definition of the models
Models of exponential form have been studied exhaustively in statistics , and have been shown to have good theoretical and practical properties . In our case , using exponential models provide a natural way of describing the dependencies between the variables . In fact , the exponential model class contains many natural models such as , the independence model , the Chow Liu tree model , and the discrete Gaussian model . Finally , such models have been used successfully for predicting itemset frequencies [ 14 ] and ranking itemsets [ 18 ] . In order to define our model , let F be a downward closed family of itemsets containing all singletons . For an itemset X ∈ F we define an indicator function SX ( t ) mapping a transaction t into binary value . If the transaction t covers X , then SX ( t ) = 1 , and 0 otherwise . We define an exponential model M associated with F to be the collection of distributions having the exponential form
P ( A = t | M , r ) = exp rX SX ( t )
,
!
X
X∈F where rX is a parameter , a real value , for an itemset X . Model M also contains all the distributions that can be obtained as a limit of the distribution having the exponential form . This technicality is needed to handle distributions with zero probabilities . Since the indicator function S∅(t ) is equal to 1 for any t , the parameter r∅ acts like a normalization constant . The rest of the parameters form a parameter vector r of length |F| − 1 . Naturally , we set F = fam(M ) . Example 5 . Assume that F consists only of singleton itemsets . Then the corresponding model has the form
KX i=1
!
KY i exp r∅ + rai Sai ( t )
= exp(r∅ ) exp(rai Sai ( t ) ) .
( 2 ) Since Sai ( t ) depends only on ti , the model is actually the independence model . The other extreme is when F consists of all itemsets . Then we can show that the corresponding model contains all possible distributions , that is , the model is in fact the parameter free model .
As an intermediate example , the tree model in Example 2 is also an exponential model with a corresponding family {a1 , . . . , a5} ∪ {a1a2 , a2a3 , a2a4 , a4a5} . The intuition behind the model is that when an itemset X is an element of F , then the dependencies between the items in X are considered important in the corresponding model .
For example , if F consists only of singletons , then there are no important correlations , hence the corresponding model should be the independence model . On the other hand , in the tree model given in Example 2 the important correlations are the parent child item pairs , namely , a1a2 , a2a3 , a2a4 , a4a5 . These are exactly , the itemsets ( along with the singletons ) that correspond to the model .
Our choice for the models is particularly good since the complexity of models reflects the size of itemset family . Since Bayesian approach has an in built tendency to punish complex families ( we will see this in Section 3.2 ) , we are punishing large families of itemsets . If the data states that simple models are sufficient , then the probability of complex models will be low , and consequently the score for large itemsets will also be low . In other words , we casted the problem of pattern set explosion into a model overfitting problem and used Occam ’s razor to punish the complex models! 3.2 Computing the Model Now that we have defined our model M , our next step is to compute the posterior probability P ( M | D ) . That is , the probability of M given the data set D . We select the model prior P ( M ) to be uniform . Recall that in Eq 2 a model M has a set of parameters , that is , to pinpoint a single distribution in M we need a set of parameters r . Following the Bayesian approach to compute P ( M | D ) we need to marginalize out the nuisance parameters r , P ( M | D ) =
P ( M , r | D ) ∝
P ( t | M , r ) .
P ( r | M )
Y
Z
Z r r t∈D
In the general case , this integral is too complex to solve analytically so we employ the popular BIC estimate [ 15 ] , P ( M | D ) ≈ C × P ( D | M , r
− log |D||F| − 1
) × exp
∗
,
„
«
2
( 3 ) where C is a constant and r∗ is the maximum likelihood estimate of the model parameters . This estimate is correct when |D| approaches infinity [ 15 ] . So instead of computing a complex integral our challenge is to discover the maximum likelihood estimate r∗ and compute the likelihood of the data . Unfortunately , using such model is an NP hard problem ( see , for example , [ 17] ) . We will remedy this problem in Section 3.4 by considering a large subclass of exponential models for which the maximum likelihood can be easily computed . 3.3
Justifications for Exponential Model
In this section we will provide strong theoretical justification for our choices and show that our score fulfills the goals we set in the introduction .
We saw in Example 2 that if the data comes from the independence model , then we only need the frequencies of the singleton itemsets to completely explain the underlying model . The next theorem shows that this holds in general case .
Theorem 6 . Assume that data D is generated from a distribution p that comes from an exponential model M . Let F = fam(M ) be the family of itemsets . We can derive the maximum likelihood estimate from the frequencies of F . Moreover , as the number of transactions goes to infinity , we can derive the true distribution p from the frequencies of F .
The preceding theorem showed that fam(M ) is sufficient family of itemsets in order to derive the correct true distribution . The next theorem shows that we favor small families : if the data can be explained with a simpler model , that is , using less itemsets , then the simpler model will be chosen and , consequently , redundant itemsets will have a low score .
Theorem 7 . Assume that data D is generated from a distribution p that comes from a model M . Assume also that if any other model , say M , contains this distribution , then |fam(M)| > |fam(M )| . Then the following holds : as the number of data points in D goes into infinity , sc(X ) = 1 if X ∈ fam(M ) , otherwise sc(X ) = 0 . 3.4 Decomposable Models
We saw in Section 3.2 that in practice we cannot compute In this section the score for general exponential models . we study a subclass of exponential models , for which we can easily compute the needed score . Roughly speaking , a decomposable model is an exponential model where the corresponding maximal itemsets can be arranged to a specific tree , called junction tree . By considering only decomposable models we obviously will lose some models , for example , the discrete Gaussian model , that is , a model corresponding to all itemsets of size 1 and 2 is not decomposable . On the other hand , many interesting and practically relevant models are decomposable , for example Chow Liu trees . Finally , these models are closely related to Bayesian networks and Markov Random Fields ( see [ 5 ] for more details ) . To define a decomposable model , let F be a downward closed family of itemsets . We write G = max(F ) to be the set of maximal itemsets from F . Assume that we can build a tree T using itemsets from G as nodes with the following property : If X , Y ∈ G have a common item , say a , then X and Y are connected in T ( by a unique path ) and every itemset along that path contains a . If this property holds for T , then T is called junction tree and F is decomposable . We will use E(T ) to denote the edges of the tree .
Not all families have junction trees and some families may have multiple junction trees . a2a3 a1a2 a2a4 a4a5 a1a2 a2a3a4 a4a5
( b ) Decomposable family F2 after merge
( a ) Decomposable family F1 of itemsets a1a2 a2a3a4 a3a4a5
( c ) Decomposable family F3 after the second merge
Figure 1 : Figure 1(a ) shows that the itemset family given in Example 2 is decomposable . Figure 1(b ) shows the junction tree for the family after Merge({a2} , a3 , a4 ) and Figure 1(c ) shows the junction tree after Merge({a4} , a3 , a5 ) .
Example 8 . Let F1 be the family of itemsets connected to the Chow Liu model given in Example 2 . The maximal itemsets F1 are {a1a2 , a2a3 , a2a4 , a4a5} . Figure 1(a ) shows a junction tree for this family , making the family decomposable . On the other hand , family {a1a2 , a1a3 , a2a3} is not decomposable since there is no junction tree for this family .
The most important property of decomposable families is that we can compute the maximum likelihood efficiently . We first define the entropy of an itemset X , denoted by H(X ) , as
H(X ) = − X qD(X = t ) log qD(X = t ) , t∈{0,1}|X| where qD is the empirical distribution of the data .
Theorem 9 . Let F be a decomposable family and let T be its junction tree . The maximum log likelihood is equal to − log P ( D | M , r∗ ) H(X ∩ Y ) .
H(X ) − X
X
|D|
=
X∈max(F )
( X,Y ) ∈E(T )
Example 10 . Assume that our model space M consists only of two models , namely the tree model M1 given in Example 2 and the independence model , which we denote M2 . Assume also that we have a dataset with 9 transactions , t1 = ( 1 , 0 , 0 , 0 , 0 ) , t2 = ( 1 , 1 , 0 , 0 , 1 ) , t3 = ( 0 , 0 , 0 , 0 , 0 ) , t4 = ( 1 , 1 , 1 , 1 , 1 ) , t5 = ( 1 , 1 , 0 , 1 , 1 ) , t6 = ( 0 , 1 , 1 , 0 , 0 ) , t7 = ( 0 , 0 , 1 , 0 , 0 ) , t8 = ( 0 , 0 , 1 , 0 , 1 ) , t9 = ( 0 , 1 , 1 , 1 , 1 ) . To compute the probabilities P ( M1 | D ) and P ( M2 | D ) , we need to know the entropies of certain itemsets
H(a1 ) = H(a2 ) = H(a3 ) = H(a5 ) = 0.68 , H(a4 ) = 0.64 ,
H(a1a2 ) = 1.31 , H(a2a3 ) = 1.37 , H(a2a4 ) = H(a4a5 ) = 106
The log likelihood of the independence model is equal to log P ( D | M2 , r
∗
) = −9(4 × 0.68 + 064 )
∗
We use the junction tree given in Figure 1(a ) and Theorem 9 to compute the log likelihood of M1 , ) = −9(131+137+2×106−2×068−064 ) log P ( D | M1 , r Note that |fam(M2)| = 6 and |fam(M1)| = 10 . Thus Eq 3 implies that P ( M1 | D ) ∝ P ( D | M1 , r −14 , P ( M2 | D ) ∝ P ( D | M2 , r −14 . We get the final probabilities by noticing that P ( M1 | D ) + P ( M2 | D ) = 1 so that we have P ( M1 | D ) = 0.72 and P ( M2 | D ) = 028 Consequently , the scores for itemsets are equal to sc(a1a2 ) = sc(a2a3 ) = sc(a2a4 ) = sc(a4a5 ) = 0.72 , sc(ai ) = 1 , for i = 1 , . . . , 5 , and sc(X ) = 0 otherwise .
) exp(−9/2 log 9 ) = 6.27 × 10 ) exp(−5/2 log 9 ) = 2.43 × 10
∗ ∗
4 . SAMPLING MODELS
Now that we have means for computing the posterior probability of a single decomposable model , our next step is to compute the score of an itemset namely , the sum in Eq 1 . The problem is that this sum has an exponential number of terms , and hence we cannot solve by enumerating all possible families . We approach this problem from a different point of view . Instead of computing the score for each itemset individually , we will divide our mining method into two steps :
1 . Sample random decomposable models from the poste rior distribution P ( M | D ) .
2 . Estimate the true score of an itemset by computing the number of sampled families of itemsets in which the itemset occurs .
4.1 Moving from One Model to Another
In order to sample we will use a MCMC approach by modifying the current decomposable family by two possible operations , namely
• Merge : Select two maximal itemsets , say X and Y . Let S = X∩Y . Since X and Y are maximal , X−S = ∅ and Y − S = ∅ . Select x ∈ X − S and y ∈ Y − S . Add a new itemset S ∪ {x , y} into the family F along with all possible sub itemsets . We will use notation Merge(S , x , y ) to denote this operation .
• Split : Select an itemset X ∈ max(F ) . Select two items x , y ∈ X . Delete X and all sub itemsets containing x and y simultaneously . We will denote this operation by Split(X , x , y ) .
Naturally , not all splits and merges are legal , since some operations may result in a family that is not decomposable , or even downward closed .
Example 11 . The family F2 given in Figure 1(b ) is obtained from the family F1 given in Figure 1(a ) by performing Merge({a2} , a3 , a4 ) . Moreover , F3 ( Figure 1(c ) ) is obtained from F2 by performing Merge({a4} , a3 , a5 ) . Conversely , we can go back by performing Split(a3a4a5 , a3 , a5 ) first and Split(a2a3a4 , a3 , a4 ) second .
The next theorem tells us which splits are legal . Theorem 12 . Let F be decomposable family and let X ∈ max(F ) and let x , y ∈ X . Then the resulting family after a split operation Split(X , x , y ) is decomposable if and only , there are no other maximal itemsets in F containing x and y simultaneously .
Example 13 . All possible split combinations are legal in families F1 and F2 given in Figure 1(a ) and Figure 1(b ) . However , for F3 given in Figure 1(c ) Split(a2a3a4 , a3 , a4 ) is illegal since a3a4a5 contains a3 and a4 . Similarly , the operation Split(a3a4a5 , a3 , a4 ) is illegal .
In order to identify legal merges , we will need some additional structures . Let F be a downward closed family and let G = max(F ) be its maximal itemsets . Let S be an itemset . We construct a reduced family , denoted by rf ( F ; S ) with the following procedure . Let us first define
X = {X − S | X ∈ G , S X} .
To obtain the reduced family rf ( F ; S ) from X , assume there are two itemsets X , Y ∈ X such that X ∩ Y = ∅ . We remove these two sets from X and replace them with X ∪ Y . This is continued until no such replacements are possible . We ignore any reduced family that contains 0 or 1 itemsets . The reason for this will be seen in Theorem 15 , which implies that such families will not induce any legal merges .
Example 14 . The non trivial reduced families of the family given in Figure 1(a ) are rf ( F1 ; a2 ) = {a1 , a3 , a4} and rf ( F1 ; a4 ) = {a2 , a5} . Similarly , the reduced families for the family given in Figure 1(b ) are rf ( F2 ; a2 ) = {a1 , a3a4} , and rf ( F2 ; a4 ) = {a2a3 , a5} . Finally , the reduced families for the family given in Figure 1(c ) are rf ( F3 ; a2 ) = {a1 , a3a4} and rf ( F3 ; a3a4 ) = {a2 , a5} .
The next theorem tells us when Merge(S , x , y ) is legal . Theorem 15 . Let F be decomposable family . A merge operation is legal , that is , F is still decomposable after adding Z = S ∪{x , y} if and only if there are sets V , W ∈ rf ( F ; S ) , V = W , such that x ∈ V and y ∈ W .
Example 16 . Family F2 in Figure 1(b ) is obtained from the family F1 in Figure 1(a ) by Merge(a2 , a3a4 ) . This is legal operation since rf ( F1 ; a2 ) = {a1 , a3 , a4} . Similarly , merge transforming F2 to F3 is legal since rf ( F2 ; a4 ) = {a2a3 , a5} . However , this merge would not be legal in F1 since we do not have a3 in rf ( F1 ; a4 ) . 4.2 MCMC Sampling Algorithm
Sampling requires a proposal distribution Q(M | M ) . Let M be a current model . We denote the number of legal operations , either a split or a merge , by d(M ) . Let M be a model obtained by sampling uniformly one of the legal operations and applying it to M . The probability of reaching M from M with a single step is Q(M | M ) = 1/d(M ) . Similarly , the probability of reaching M from M with a single step is Q(M | M ) = 1/d(M ) . Consequently , if we sample u uniformly from the interval [ 0 , 1 ] and accept the step moving from M into M if and only if u is smaller than
P ( M | D)Q(M | M ) P ( M | D)Q(M | M )
=
P ( M | D)d(M ) P ( M | D)d(M )
,
( 4 ) then the limit distribution of the MCMC will be the posterior distribution P ( M | D ) provided that the MCMC chain is ergodic . The next theorem shows that this is the case .
Theorem 17 . Any decomposable model M can be reached from any other model M by a sequence of legal operations .
Our first step is to compute the ratio of the models given in Eq 4 . To do that we will use the BIC estimate given in Eq 3 and Theorem 9 . Let us first define a function gain(X , x , y ) =
|D|(H(X ) − H(X − x ) − H(X − y ) + H(X − {x , y}) ) , where X is an itemset and x , y ∈ X are items .
Theorem 18 . Let M be a decomposable model and let M = Split(X , x , y ; M ) be a model obtained by a legal split . Let A be the BIC estimate of P ( M | D ) and let B be the BIC estimate of P ( M | D ) . Then gain(X , x , y ) − log |D|2 Similarly , if M = Merge(S , x , y ; M ) , then
|X|−3 ” “ “ −gain(S ∪ {x , y} , x , y ) + log |D|2
B/A = exp
B/A = exp
.
|S|−1 ”
.
To compute the gain we need the entropies for 4 itemsets . Let X be an itemset . To compute H(X ) we first order the transactions in D such that the values corresponding to X are in lexicographical order . This is done with a radix sort Sort(D , X ) given in Algorithm 1 . This sort is done in O(|D||X| ) time . After the data is sorted we can easily compute the entropy with a single data scan : Set e = 0 and p = 0 . If the values of X of the current transaction is equal to the previous transaction we increase p by 1/|D| , otherwise we add −p log p to e and set p to 1/|D| . Once the scan is finished , e will be equal to H(X ) . The pseudo code for computing the entropy is given in Algorithm 2 .
Algorithm 1 : Sort(D , X ) . Routine for sorting the transactions . Used by Entropy as a pre step for computing the entropy . 1 if X = ∅ or D = ∅ then return D ; 2 ai ← first item in X ; 3 D0 ← {t ∈ D | ti = 0} ; D1 ← {t ∈ D | ti = 1} ; 4 D0 ← Sort(D0 , X − ai ) ; D1 ← Sort(D1 , X − ai ) ; 5 return D0 concatenated with D1 .
Algorithm 2 : Entropy(D , X ) . Computes the entropy of X from the dataset D . 1 Sort(D , X ) ; 2 e ← 0 ; p ← 0 ; 3 u ← first transaction in D ; 4 foreach t ∈ D do 5 6 7 8 if uX = tX then e ← e − p log p ; u ← t ; p ← 1/|D| ; p ← p + 1/|D| ; else
9 10 11 e ← e − p log p ; 12 return e ;
Our final step is to compute d(M ) and actually sample the operations . To do that we first write sd ( M ) for the number of possible Split operations and let sd ( M , X ) be the number of possible Split operations using itemset X . Similarly , we write md ( M , S ) for the number of legal merges using S and also md ( M ) for the amount of legal merges in total . Given a maximal itemset X we build an occurrence table , which we denote by st(X ) , of size |X| × |X| . For x , y ∈ X , the entry of the table st(X , x , y ) is the number of maximal itemsets containing x and y . If st(X , x , y ) = 1 , then Theorem 12 states that Split(X , x , y ) is legal . Consequently , to sample a split operation we first select a maximal itemset weighted by sd ( M , X ) /sd ( M ) . Once X is selected we select uniformly one legal pair ( x , y ) .
To sample legal merges , recall that Merge(S , x , y ) involves selecting two maximal itemsets X and Y such that S = X ∩ Y , x ∈ X − S , and y ∈ Y − S . Instead of selecting these itemsets , we will directly sample an itemset S and then select two items x and y . This sampling will work only if two legal merges Merge(S1 , x1 , y1 ) and Merge(S2 , x2 , y2 ) result in two different outcomes whenever S1 = S2 .
Theorem 19 . Let S1 and S2 be two different itemsets and let x1 , y1 /∈ S1 , and x2 , y2 /∈ S2 be items . Assume that Merge(Si , xi , yi ) is a legal merge for i = 1 , 2 . Define Zi = Si ∪ {xi , yi} for i = 1 , 2 . Then Z1 = Z2 .
The construction of a reduced family states that , if V , W ∈ rf ( F ; S ) , V = W , then V ∩ W = ∅ . It follows from Theorem 15 that
X md ( M , S ) =
|V ||W| .
V,W∈rf ( F;S )
V =W
To sample a merge we first sample an itemset S weighted by md ( M , S ) /md ( M ) . Once S is selected , we sample two different itemsets V , W ∈ rf ( F ; S ) ( weighted by |V | and |W| ) . Finally , we sample x ∈ V and y ∈ W .
Sampling S for a merge operation is feasible only if the number of reduced families for which the merge degree is larger than zero is small .
Theorem 20 . Let K be the number of items . There are at most K maximal itemsets . There are at most K − 1 itemsets for which the degree md ( M,· ) > 0 .
Pseudo code for a sampling step is given in Algorithm 3 .
Algorithm 3 : MCMC step for sampling decomposable models . 1 u ← random integer between 1 and d(M ) ; 2 if u ≤ sd ( M ) then 3 4
Sample X from max(F ) weighted by sd ( M , X ) ; Sample x , y ∈ X such that X is the only maximal itemset containing both x and y ; M ← Split(X , x , y ; M ) ; g ← gain(X , x , y ) − log |D|2|X|−3 ;
5 6
Sample S weighted by md ( M , S ) ; Sample V ∈ rf ( F ; S ) weighted by |V | ; Sample W ∈ rf ( F ; S ) , V = W , weighted by |W| ; Sample x ∈ V and y ∈ W ; M ← Merge(S , x , y ; M ) ; g ← −gain(S ∪ {x , y} , x , y ) + log |D|2|S|−1 ;
7 else 8 9 10 11 12 13 14 z ← random real number from [ 0 , 1 ] ; 15 if z ≤ exp(g ) d(M ) d(M ) then return M ; 16 else return M ;
4.3 Speeding Up the Sampling
We have demonstrated what structures we need to compute so that we can sample legal operations . After a sample , we can reconstruct these structures from scratch . In this section we show how to optimize the sampling by constructing the structures incrementally using Algorithms 4–7 . First of all , we store only maximal itemsets of F . Theorem 20 states that there can be only K such sets , hence split and merge operations can be done efficiently .
During a split or a merge , we need to update what split operations are legal after the split . We do this by updating an occurrence table st(X ) . An update takes O(|X|2 ) time . The next theorem shows which maximal itemsets we need to update for legal split operations after a merge .
Theorem 21 . Let F be a downward closed family of itemsets and let G be the family after performing Merge(S , x , y ) . Let Y be a maximal itemset in max(F)∩max(G ) . Then legal split operations using Y remain unchanged during the merge unless Y is the unique itemset among maximal itemsets in F containing either S ∪ {x} or S ∪ {y} .
The following theorem tells us how reduced families should be updated after a merge operation . To ease the notation , let us denote by link ( F , S , x ) the unique itemset ( if such exists ) in rf ( F ; S ) containing x .
Theorem 22 . Let F be a downward closed family of itemsets and let G be the family after performing Merge(S , x , y ) . Then the reduced families are updated as follows :
1 . Itemsets link ( F , S , x ) and link ( F , S , y ) in rf ( F ; S ) are merged into one itemset in rf ( G ; S ) .
2 . Itemset {x} is added into rf ( G ; S ∪ {y} ) . Itemset {y} is added into rf ( G ; S ∪ {x} ) .
3 . Let T S and let z ∈ S− T . The itemset containing z in rf ( F ; T ∪ {y} ) is augmented with item x . Similarly , itemset containing z in rf ( F ; T ∪ {x} ) is augmented with item y .
4 . Otherwise , rf ( F ; T ) = rf ( G ; T ) or md ( F ; T ) = 0 and md ( G ; T ) = 0 .
Theorems 21 and 22 only covered the updates during merges . Since Split(S ∪ {x , y} , x , y ) and Merge(S , x , y ) are opposite operations we can derive the needed updates for splits from the preceding theorems .
Corollary 23
( of Theorem 21 ) . Let F be a downward closed family of itemsets and let G be the family after performing Split(X , x , y ) . Let Y be a maximal itemset in max(F ) ∩ max(G ) . Then legal split operations using Y remain unchanged during the merge unless Y is the unique itemset among maximal itemsets in G containing either X − {x} or X − {y} .
Corollary 24
( of Theorem 22 ) . Let F be a downward closed family of itemsets and let G be the family after performing Split(X , x , y ) . Let S = X − {x , y} . Then the reduced families are updated as follows :
1 . Itemset containing {x , y} in rf ( F ; S ) is split into two parts , link ( G , S , x ) and link ( G , S , y ) .
2 . Itemset {x} is removed from rf ( G ; S ∪ {y} ) . Itemset
{y} is removed from rf ( G ; S ∪ {x} ) .
3 . Let T S and let z ∈ S − T . Item x is removed from the itemset containing z in rf ( F ; T ∪ {y} ) . Similarly , item y is removed from the itemset containing z in rf ( F ; T ∪ {x} ) .
4 . Otherwise , rf ( F ; T ) = rf ( G ; T ) or md ( F ; T ) = 0 and md ( G ; T ) = 0 .
We keep in memory only those families that have positive merge degree . Theorem 20 tells us that there are only K − 1 such families . By studying the code in the update algorithm we see that , except in two cases , the update of a family is either a insertion/deletion of an element into an itemset or a merge of two itemsets . The first complex case is given on Line 2 in MergeSide which corresponds to Case set , and this is done in O(P
2 in Theorem 22 . The problem is that this family may have contained only itemset before the merge , hence we did not store it . Consequently , we need to recreate the missing itemX∈F X ) time . The second case occurs on Line 2 in SplitSide . This corresponds to the case where we need to break the itemset W ∈ rf ( S ) containing x and y apart during a split ( Case 1 in Corollary 24 ) . This is done by constructing the new sets from scratch . The construction needs O(|W|K ( |W| + M ) ) time , where M is the size of largest itemset in F .
Algorithm 4 : SplitUpdate(X , x , y ) . Routine for updating the structures during Split(X , x , y ) . 1 Update F ; 2 Remove link ( S , x ) from rf ( S ) ; 3 S ← X − {x , y} ; 4 SplitSide(X , x , y , S ) ; SplitSide(X , y , x , S ) ;
Algorithm 5 : Subroutine SplitSide(X , a , b , S ) used by SplitUpdate . 1 Z ← S ∪ {a} ; 2 while changes do 3
Z ← Z ∪ {X ∈ max(F ) ; S Z ∩ X} ;
4 Add Z into rf ( S ) ; 5 if rf ( S ∪ {a} ) exists then Remove {b} from rf ( S ∪ {a} ) ; 6 7 for T S , rf ( T ∪ {a} ) exists do 8 9 10 if there is unique Z ∈ max(F ) st S ∪ {a} Z then 11 z ← ( any ) item in S − T ; Remove b from link ( S , z ) ;
Update st(Z ) ;
Algorithm 6 : MergeUpdate(S , x , y ) . Routine for updating the structures during Merge(S , x , y ) .
1 Merge link ( S , x ) and link ( S , y ) in rf ( S ) ; 2 A ← itemset in max(F ) such that S ∪ {x} ⊆ A ; 3 B ← itemset in max(F ) such that S ∪ {y} ⊆ B ; 4 Build st(S ∪ {x , y} ) from st(A ) and st(B ) ; 5 MergeSide(S , x , y ) ; MergeSide(S , y , x ) ; 6 Update F ;
Algorithm 7 : Subroutine MergeSide(S , a , b ) used by MergeUpdate . 1 U ← S ∪ {a} ; 2 if U /∈ max(F ) and rf ( U ) does not exists then 3 rf ( U ) ←nS o
U⊆X∈max(F ) X
; z ← ( any ) item in S − T ; Augment link ( S , z ) with b ;
4 Add b into rf ( U ) ; 5 for T S , rf ( T ∪ {a} ) exists do 6 7 8 if there is unique Z ∈ max(F ) st U Z then 9
Update st(Z ) ;
5 . RELATED WORK
Many quality measures have been suggested for itemsets . A major part of these measures are based on how much the itemset deviates from some null hypothesis . For example , itemset measures that use the independence model as background knowledge have been suggested in [ 1 , 3 ] . More flexible models have been proposed , such as , comparing itemsets against graphical models [ 11 ] and local Maximum Entropy models [ 12 , 18 ] . In addition , mining itemsets with low entropy has been suggested in [ 9 ] .
Our main theoretical advantage over these approaches is that we look at the itemsets as a whole collection . For example , consider that we discover that item a and b deviate greatly from the null hypothesis . Then any itemset containing both a and b will also be deemed interesting . The reason for this is that these methods are not adopting to the discovered fact that a and b are correlated , but instead they continue to use the same null hypothesis . We , on the other hand , avoid this problem by considering models : if itemset ab is found interesting that information is added into the statistical model . If this new model then explains bigger itemsets containing a and b , then we have no reason to add these itemsets , into the model , and hence such itemsets will not be considered interesting .
The idea of mining a pattern set as a whole in order to reduce the number of patterns is not new . For example , pattern reduction techniques based on minimum description length principle has been suggested [ 16 , 21 , 10 ] . Discovering decomposable models have been studied in [ 20 ] . In addition , a framework that incrementally adopts to the patterns approved by the user has been suggested in [ 8 ] . Our main advantage is that these methods require already discovered itemset collection as an input , which can be substantially large for low thresholds . We , on the other hand , skip this step and define the significance for itemsets such that we can mine the patterns directly .
6 . EXPERIMENTS
In this section we present our empirical evaluation of the measure . We first describe the datasets and the setup for the experiments , then present the results with synthetic datasets and finally the results with real world datasets . 6.1 Setup for the Experiments
We used 2×3 synthetic datasets and 3 real world datasets . The first three synthetic datasets , called Ind , contained 15 independent items and 100 , 103 , and 104 transactions , respectively . We set the frequency for the individual items to be 01 The next three synthetic datasets , called Path , also contained 15 items . In these datasets , an item ai were generated from the previous one with P ( ai = 1 | ai−1 = 1 ) = P ( ai = 0 | ai−1 = 0 ) = 075 The probability of the first item was set to 05 We set the number of transactions for these datasets to 100 , 103 , and 104 , respectively .
Our first real world dataset Paleo2 contains information of species fossils found in specific paleontological sites in Europe [ 7 ] . The dataset Courses contains the enrollment records of students taking courses at the Department of Computer Science of the University of Helsinki . Finally , our last dataset is Dna is DNA copy number amplification data collection of human neoplasms [ 13 ] . We used 100 first
2NOW public release 030717 available from [ 7 ] . items from this data and removed empty transactions . The basic characteristics of the datasets are given in Table 1 .
For each data we sampled the models from the posterior distribution using techniques described in Section 34 We used singleton model as a starting point and did 5000 restarts . The number of required MCMC steps is hard to predictm since the structure of the state space of decomposable models is complex . Further it also depends on the actual data . Hence , we settle for heuristic : for each restart we perform 100K log K MCMC steps , where K is the number of items . Doing so we obtained N = 5000 random models for each dataset . The execution times for sampling are given in Table 1 . Let {F1 , . . . ,FN} be the discovered models . We estimated the itemset score sc(X ) ≈ |{Fi | X ∈ Fi}|/N and mined interesting itemsets using a simple depth first approach .
Name
Ind Path Dna Paleo Courses
|D| 100–104 100–104 1160 501 3506
K # of steps time
15 15 100 139 90
4 063 4 063 46 052 68 590 40 499
4m–2h 7m–3.5h 6h 5h 8.5h
Table 1 : Basic characteristics of the datasets . The fourth column contains the number of sample steps and the last column is the execution time .
6.2 Synthetic datasets
Our main purpose for the experiments with synthetic datasets is to demonstrate how the score behaves as a function of number of data points . To this end , we plotted the number of significant itemsets , that is itemsets whose score was higher than the threshold σ , as a function of the threshold σ . The results are shown in Figures 2(a ) and 2(b ) .
Ideally , for Ind , the dataset with independent variables we should have only 15 significant itemsets , that is , the singletons , for any σ > 0 . Similarly , for Path we should have 15 + 14 = 29 itemsets , the singletons and the pairs of form aiai+1 . We can see from Figures 2(a ) and 2(b ) that as we increase the number of transactions in data , the number of significant itemsets approaches these ideal cases , as predicted by Theorem 7 . The convergence to the ideal case is faster in Path than in Ind . The reason for this can be explained by the curse of dimensionality . In Ind we have 15 × 14/2 = 105 combinations of pairs of items . There is a high probability that some of these item pairs appear to be correlated . On the other hand , for Path , let us assume that we have the correct model . That is , the singletons and the 14 pairs aiai+1 . The only valid itemsets of size 3 that we can add to this model are of the form aiai+1ai+2 . There are only 13 of such sets , hence the probability of finding such itemset important is much lower . Interestingly , in Path we actually benefit from the fact that we are using decomposable models instead of general exponential models . 6.3 Use cases with real world datasets
Our first experiment with real world data is to study the number of significant itemsets as a function of the threshold σ . Figure 2(c ) shows the number of significant itemsets for all three datasets . We see that the number of significant itemsets increases faster than for the synthetic datasets as
( a ) Ind
( b ) Path
( c ) Real datasets
Figure 2 : Number of significant itemsets as a function of the threshold . The smallest threshold used for Ind and Path is 001 The smallest threshold used for real datasets is 005 the threshold decreases . The main reason for this difference , is that with real world datasets we have more items and less transactions . This is seen especially in the Paleo dataset for which the number of significant itemsets increases steeply between the interval 04–10 when compared to Dna and Courses .
Our next experiment is to compare the score against baselines , namely , the frequency fr ( X ) and entropy H(X ) . These comparisons are given in Figures 3(a ) and 3(b ) . In addition , we computed the correlation coefficients ( given in Table 2 ) . From results , we see that sc(X ) has a positive correlation with frequency and a negative correlation with entropy . The correlation with entropy is expected , since low entropy implies that the empirical distribution of an itemset is different than the uniform distribution . Hence , using the frequency of such an itemset should improve the model and consequently the itemset is considered interesting .
Data fr ( X ) H(X ) index diff .
Dna Paleo Courses
0.16 0.45 0.18
0.27 0.16 0.35
0.28 0.17 0.02
Table 2 : Correlations of sc(X ) and baselines .
Both datasets Paleo and Dna have a natural order between the items , for example , for Paleo dataset it is the era when particular species was extant . Our next experiment is to test whether this order is represented in the discovered patterns . In our experiments , the items in these datasets were ordered using this natural order . Let X = aiaj be an itemset of size 2 . In Figure 3(c ) , we plotted j − i as a function of sc(X ) . We used only itemsets of size 2 , since larger itemsets have inherently smaller scores and larger difference between the indices . In addition , we compute the correlation coefficients ( given in Table 2 ) . From the results we see that for Paleo and Dna the more significant itemsets tend to have a smaller difference between their indices . On the other hand , we do not discover any significant correlation in Courses .
Finally , we report some of the discovered patterns from Courses . The 4 most significant itemsets of size 2 are ( Computer Architectures , Performance Analysis ) with a score of 0.95 , ( Design & Analysis of Algorithms , Principles of Functional Programming ) scoring 0.94 , ( Database Systems II , In formation Storage ) scoring 0.94 , ( Three concepts : probability , Machine Learning ) scoring 092
7 . CONCLUSIONS
In this paper we introduced a novel and general approach for ranking itemsets . The idea behind the approach is to connect statistical models and collections of itemsets . This connection enables us to define the score of an itemset as the probability of itemset occurring in a random model . Doing so , we transformed the problem of mining patterns into a more classical problem of modeling .
As a concrete example of the framework , we used exponential models . These models have many important theoretical and practical properties . The connection with itemsets is natural and the Occam ’s razor inherent to these models can be used against the pattern explosion problem . Our experiments support the theoretical results and demonstrate that the measure works in practice .
8 . ACKNOWLEDGMENTS
Nikolaj Tatti is funded by FWO postdoctoral mandate .
9 . REFERENCES [ 1 ] C . C . Aggarwal and P . S . Yu . A new framework for itemset generation . In Proceedings of the 17th ACM SIGACT SIGMOD SIGART symposium on Principles of database systems ( PODS 1998 ) , pages 18–24 . ACM Press , 1998 .
[ 2 ] R . Agrawal , H . Mannila , R . Srikant , H . Toivonen , and
A . I . Verkamo . Fast discovery of association rules . In Advances in Knowledge Discovery and Data Mining , pages 307–328 . AAAI/MIT Press , 1996 .
[ 3 ] S . Brin , R . Motwani , and C . Silverstein . Beyond market baskets : Generalizing association rules to correlations . In SIGMOD 1997 , Proceedings ACM SIGMOD International Conference on Management of Data , pages 265–276 . ACM Press , May 1997 . [ 4 ] C . Chow and C . Liu . Approximating discrete probability distributions with dependence trees . IEEE Transactions on Inf . Theory , 14(3):462–467 , 1968 .
[ 5 ] R . G . Cowell , A . P . Dawid , S . L . Lauritzen , and D . J .
Spiegelhalter . Probabilistic Networks and Expert Systems . Statistics for Engineering and Information Science . Springer Verlag , 1999 .
001020406081threshold15406080100120significantitemsets102points103points104points001020406081threshold20253035404550556063significantitemsets102points103points104points005020406081threshold91200400600800100012001294significantitemsetsDnaPaleoCourses [ 6 ] I . Csisz´ar . I divergence geometry of probability distributions and minimization problems . The Annals of Probability , 3(1):146–158 , Feb . 1975 .
[ 7 ] M . Fortelius , A . Gionis , J . Jernvall , and H . Mannila .
Spectral ordering and biochronology of european fossil mammals . Paleobiology , 32(2):206–214 , March 2006 . [ 8 ] S . Hanhij¨arvi , M . Ojala , N . Vuokko , K . Puolam¨aki ,
N . Tatti , and H . Mannila . Tell me something I don’t know : randomization strategies for iterative data mining . In Knowledge Discovery and Data Mining ( KDD 2009 ) , pages 379–388 , 2009 .
[ 9 ] H . Heikinheimo , J . K . Sepp¨anen , E . Hinkkanen ,
H . Mannila , and T . Mielik¨ainen . Finding low entropy sets and trees from binary data . In Knowledge discovery and data mining ( KDD 2007 ) , pages 350–359 , 2007 .
[ 10 ] H . Heikinheimo , J . Vreeken , A . Siebes , and
H . Mannila . Low entropy set selection . In Proceedings of the SIAM International Conference on Data Mining ( SDM 2009 ) , pages 569–580 , 2009 .
[ 11 ] S . Jaroszewicz and D . A . Simovici . Interestingness of frequent itemsets using bayesian networks as background knowledge . In Knowledge discovery and data mining ( KDD 2004 ) , pages 178–186 , 2004 .
[ 12 ] R . Meo . Theory of dependence values . ACM Trans .
Database Syst . , 25(3):380–406 , 2000 .
[ 13 ] S . Myllykangas , J . Himberg , T . B¨ohling , B . Nagy ,
J . Hollm´en , and S . Knuutila . Dna copy number amplification profiling of human neoplasms . Oncogene , 25(55):7324–7332 , Nov . 2006 .
[ 14 ] D . Pavlov , H . Mannila , and P . Smyth . Beyond independence : Probabilistic models for query approximation on binary transaction data . IEEE TKDE , 15(6):1409–1421 , 2003 .
[ 15 ] G . Schwarz . Estimating the dimension of a model .
Annals of Statistics , 6(2):461–464 , 1978 .
[ 16 ] A . Siebes , J . Vreeken , and M . van Leeuwen . Item sets that compress . In Proceedings of the SIAM International Conference on Data Mining ( SDM 2006 ) , 2006 .
[ 17 ] N . Tatti . Computational complexity of queries based on itemsets . IPL , pages 183–187 , June 2006 .
[ 18 ] N . Tatti . Maximum entropy based significance of itemsets . KAIS , 17(1):57–77 , 2008 .
[ 19 ] N . Tatti . Probably the best itemsets . Technical Report
2010/06 , University of Antwerp , 2010 . http://adremuaacbe/publications
[ 20 ] N . Tatti and H . Heikinheimo . Decomposable families of itemsets . In Knowledge Discovery in Databases : PKDD 2008 , pages 472–487 , 2008 .
[ 21 ] N . Tatti and J . Vreeken . Finding good itemsets by packing data . In Proceedings of the 8th IEEE International Conference on Data Mining ( ICDM 2008 ) , pages 588–597 , 2008 .
[ 22 ] A . Wald . Tests of statistical hypotheses concerning several parameters when the number of observations is large . Transactions of the American Mathematical Society , 52(3):426–482 , 1943 .
Proof of Theorem 6 . We will prove the theorem using the well known connection between maximum entropy principle and exponential models . Let qD be the empirical distribution of the data . Let P be the collection of distributions ,
P = {p | p(X = 1 ) = fr ( X ) = qD(X = 1 ) , X ∈ F} , that is , a distribution in P has the same itemset frequencies for F as the data . Assume for the time being that all entries in qD are positive . Then , a well known theorem [ 6 ] states that the unique distribution , say p∗ , maximizing the entropy among P belongs to the exponential model M such that fam(M ) = F . Let the corresponding parameters for the model be r∗ . We need to show that p∗ is actually a maximum likelihood distribution . To see this , let q ∈ M be another distribution from the model and let r be its parameters . A straightforward calculus reveals that p∗(t ) q(t )
= |D|X p∗(D ) q(D ) p∗(t ) q(t ) qD(t ) log log
= log t∈D
X = |D|X = |D|X = |D|X t∈Ω t∈Ω t∈Ω
X X
X∈F
X∈F ∗ t∈Ω qD(X = 1)(r
X − rX ) ∗
∗ p
( X = 1)(r
X − rX ) ∗ p
( t ) log p∗(t ) q(t )
= |D|KL(p
∗q ) ≥ 0 , where KL(pq ) is the Kullback Leibler divergence between p∗ and q . The inequality shows that r∗ has the highest likelihood . If qD has zero probabilities , then we can consider a sequence n → 0 as n → ∞ . Define qn = ( 1− n)qD + n2−K . Let p∗ n be the maximal entropy distribution computed from qn . Let p∗ be the maximal entropy distribution computed from qn and let rn = ( 1 − n)p∗ + n2−K . Since rn has the same frequencies than qn for each X ∈ F , it follows n ) ≥ H(rn ) . Since rn converges to p∗ , we have that H(p∗ n ) ≥ H(p∗ ) . Any converging subsequence of that lim inf H(p∗ n will converge to a distribution in P , and hence having p∗ the entropy H(p∗ ) . Since maximal entropy distribution is unique , the sequence p∗ n converges to the unique value , p∗ . Following the same calculus as above we can show that p∗ is the maximal likelihood distribution
X t∈Ω qD(t ) log p∗(t ) q(t )
= lim n→∞
= lim n→∞
= lim n→∞ t∈Ω
X X X X t∈Ω t∈Ω t∈Ω ∗ qn(t ) log p∗ n(t ) q(t )
X X
X∈F qn(X = 1)(r
X − rX ) ∗
∗ n(X = 1)(r
X − rX ) ∗ p p
X∈F ∗ n(t ) log p∗(t ) q(t )
= lim n→∞
X t∈Ω
= p
( t ) log p∗ n(t ) q(t )
= KL(p
∗q ) ≥ 0 .
APPENDIX A . PROOFS FOR SECTION 3
Finally , as the number of transactions goes into infinity r∗ converges to the true parameters of the model and p∗ converges into true underlying distribution p .
To prove Theorem 7 we will need the following lemma whose prove can be found for example in [ 22 ] .
Lemma 25 . Assume that D with N data points comes from a model M with parameters r . Let rN be the maximal likelihood estimate . Then log P ( D | M , rN ) − log P ( D | M , r ) converges uniformly to χ2 distribution with d ≤ |F|−1 number degrees of freedom .
Proof of Theorem 7 . Let M
F = fam(M ) . We need to show that
= M be a model and
P ( M | D ) P ( M | D )
→ 0 as the number of data points goes to infinity . We will use the BIC estimate given in Eq 3 to prove the theorem . Let N = |D| . Let us define pN to be the maximal likelihood distribution from M and qN to be the maximal likelihood distribution in M . Let us write AN = log qN ( D ) − log pN ( D ) and
BN = log N ( |F| −˛˛F˛˛)/2 .
Then asymptotically log P ( M
| D ) − log P ( M | D ) = AN + BN .
Let A be the limit of AN and B be the limit of BN . Assume that p /∈ M . Let q ∈ M be the maximum likelihood distribution as N → ∞ . Since the exponential models are closed sets q cannot be p . A straightforward calculation reveals that N−1AN → −KL(pq ) < 0 . In addition , N−1BN → 0 . Thus N−1(AN + BN ) approaches a negative value , and so A + B = −∞ . Assume that p ∈ M , then the conditions imply that |F| > |F| so that B = −∞ . Lemma 25 implies that A is a difference of two χ2 distributions . More importantly , since convergence in Lemma 25 is uniform we can compute the limit inside the probability so that the probability
P ( AN + BN > σ ) → P ( A + B > σ ) = P ( A > ∞ ) = 0 for any σ ∈ R . Hence AN +BN approaches −∞ . This proves the theorem .
Proof of Theorem 9 . Let p(A ) = P ( A | M , r ) be the distribution from a model M with parameters r . To ease the notation , let us define G = max(F ) and S = {X ∩ Y | ( X , Y ) ∈ E(T )} .
A classic result states that we can decompose p to factors ,
Q Q p(A = t ) =
X∈G p(X = tX ) S∈S p(S = tS )
, where tX is a projection of a binary vector t to the variables of X . Let q(A ) be the empirical distribution . Let p be the maximum likelihood distribution . The proof of Theorem 6 states that p(X = 1 ) = q(X = 1 ) for any itemset X ∈ F . Using the exclusion inclusion rules and the fact that F is downward closed we can show that p(X = t ) = qD(X = t ) for every X ∈ F and every possible binary vector t of length |X| . Using the equality
X t∈D tX tX log p(X = tX ) =|D|X =|D|X Q X Q = − |D|X p(A = t ) =
Y t∈D log
X∈G
= − |D|H(X ) log t∈D we see that the log likelihood is equal to q(X = tX ) log p(X = tX ) q(X = tX ) log q(X = tX )
X∈G p(X = tX ) S∈S p(S = tS )
H(X ) + |D|X
H(S ) .
S∈S
This proves the theorem .
B . PROOFS FOR SECTION 4
We will need the following technical lemma for several subsequent proofs .
Lemma 26 . Let X1 , . . . , XN ∈ max(F ) with N ≥ 3 . Define Si = Xi ∩ Xi+1 and SN = XN ∩ X1 . If each Si has a unique item ( when compared to other Si ) , then F is not decomposable .
Proof of Theorem 12 . Let us assume that X is the only itemset containing x and y simultaneously . Let T be a junction tree . The adjacent itemsets of X either miss x or y or both . Thus we can remove X and replace it X − x and X − y , connected to each other , the adjacent itemsets of X can be connected to either X − x or to X − y . If X − x is not maximal , that is there is an itemset Z ⊃ X − x , then we can remove X − x and connect all the edges going to X − x to Z . We can repeat this for X − y as well . The resulting tree is a junction tree .
To prove the other direction , assume that there is an another itemset Y containing x and y , simultaneously . There must be an item z ∈ X such that z /∈ Y . Then itemsets Y , X − x , and X − y satisfy the requirements of Lemma 26 , hence the new family is not decomposable .
Proof of Theorem 15 . Let T be a junction tree . Assume that V and W exist . By construction of rf ( F ; S ) it follows that there must be two itemsets X , Y ∈ F such that S = X ∩ Y , x ∈ X , and y ∈ Y . If X and Y are adjacent in T , we can add Z between X and Y ( possibly removing X and Y if they are no longer maximal ) , and T still remains a junction tree .
If X and Y are not adjacent , then there is a path P connecting them . There must be itemsets Pi and Pi+1 along that path such that Pi ∩ Pi+1 = S , otherwise X and Y would have been merged during the construction of reduced family , which is a contradiction . If we remove edge ( Pi , Pi+1 ) and add edge ( X , Y ) we will obtain an alternative junction tree for F . It is straightforward to see that this tree is truly a junction tree . But in this tree X and Y are adjacent , so we can add Z between them .
To prove the other direction , assume that adding Z is a legal merge . By definition it immediately implies that there are sets V , W ∈ rf ( F ; S ) such that x ∈ V and y ∈ W . We need to show that V = W . Let H be the family resulted from the merge . Let U be a junction tree for H . Let X ∈ max(F ) be an itemset such that S ∪ {x} ⊆ X and , similarly , let Y ∈ max(F ) such that S ∪ {y} ⊆ Y . We may assume that X = S ∪{x} and Y = S ∪{y} , otherwise the proof is trivial . Hence , X , Y ∈ max(H ) Let P be a path in U from X to Y . Let R be a path in U from Z to X and let Pe be the first entry in path R that also occurs in P . We must have Z∩X ⊆ Pe . The path from Z to Y must also contain Pe , and so Z∩Y ⊆ Pe . This implies that Z = ( Z∩X)∪(Z∩Y ) ⊆ Pe . Since Z is a maximal set , we must have Z = Pe . Itemset Z is the only maximal itemset in H that contains x and y simultaneously . Otherwise , Theorem 12 states that Split(Z , x , y ) is illegal in H so Merge(S , x , y ) is illegal in F . We can remove Z = Pe from U , attach Pe−1 and Pe+1 to each other , and connect the rest adjacent nodes either to Pe−1 or to Pe+1 depending whether these nodes have x or y as a member . The outcome , say R , is a junction tree for F .
If V = W , then there must be a sequence X = ( T1 , . . . , TN ) = Y ∈ F such that S Ti ∩ Ti+1 . Now consider a path O in R visiting each Ti in turn . Since R is a junction tree we must have S Oi∩Oi+1 . Since X = T1 and Y = TN path O must use the edge ( Pe−1 , Pe+1 ) . But we must have Pe−1 ∩ Pe+1 = S , otherwise U would not have been a junction tree . This contradiction implies that V = W .
Proof of Theorem 17 . We will prove the theorem by showing that all models can be brought by legal splits to the indepenence model . Since a split Split(X , x , y ) and a merge Merge(X − {x , y} , x , y ) , are the opposite operations we can reach model M from M by first reaching the independence model with splits and then reaching M from the independence model by merges . Let T be a junction tree for F = fam(M ) and let X ∈ max(F ) be a maximal itemset such that X is the leaf clique in a junction tree T with |X| ≥ 2 . If no such X exist , then M is in fact the independence model . Let Y be the maximal itemset into which X is connected in T . There must be an item x ∈ X such that x is not contained in any other maximal itemset of F . To prove this , assume otherwise . Then for each item z there is a maximal itemset Z that contains z . The path from X to X must go through Y so the running intersection property implies that x ∈ Y . But this implies that X ⊆ Y which contradicts the maximality of X . Let y ∈ X such that x = y . Theorem 12 now states that Split(X , x , y ) is a legal operation . We can now iteratively apply this step until we reach the independence model . This proves the theorem .
Proof of Theorem 18 . For notational convinience , let
H(X ∩ Y ) and b(M ) =
X
H(X ) .
X∈max(F ) us write a(M ) =
X
( X,Y ) ∈E(T )
Theorem 9 and Eq 3 implies that and log A = |D|(a(M ) − b(M ) ) − log |D|(|F| − 1 ) log B = |D|`a(M )´ − log |D|(|F| − 1 )
) − b(M
2
.
2
Let us write d = |D|(a(M ) − b(M ) − a(M ) + b(M ) ) . We will first show that d = gain(X , x , y ) . Let us denote S =
X − {x , y} , U = X − x , and V = X − y . Consider the possibility that U is not maximal in F , in such case there must be a maximal itemset Q ∈ F such that U ⊂ Q and that Q is adjacent to X with a separator U . Similarly , if V is not maximal in F , then there is a maximal itemset P ∈ F such that V ⊂ P and that P is adjacent to X with a separator V . By studying the first part of the proof of Theorem 12 we see that there are four possible cases depending whether U and/or V is maximal set in F . The split operations are
X −→ P −→ X −→
SU
SU
SQ
VX
UQ
U
Q
V
X
P
−→
SQ
V
P
V
P
( Case 1 )
( Case 2 )
( Case 3 )
( Case 4 )
In all cases , the difference d is equal to H(X ) + H(S ) − H(V ) − H(U ) = gain(X , x , y ) . For example , in Case 2 , the term H(X ) is in b(M ) but not in b(M ) , similarly the term H(U ) is in b(M ) but not in b(M ) , consequently b(M ) − b(M ) = H(X)− H(U ) . Similarly , a(M )− a(M ) = H(V )− H(S ) . Thus d = gain(X , x , y ) for Case 2 , and similar observations show that d = gain(X , x , y ) holds also for other cases . To complete the proof we need to show that |F| − |F| = 2|X|−2 . During a split , itemsets containing x and y are removed , but there are exactly 2|X|−2 such itemsets . We can now combine these results log B − log A = d − log |D|
2
( |F| −˛˛F˛˛ )
= gain(X , x , y ) − log |D|2
|X|−3 , which proves the theorem .
Proof of Theorem 19 . Let Xi and Yi be the maximal itemsets such that Xi ∩ Yi = Si , xi ∈ Xi and yi ∈ Yi . These sets must exist since adding Zi is a legal merge .
Assume that Z1 = Z2 . This implies that either all items x1 , y1 , x2 , and y2 are unique or two of them are the same . Assume the latter case and assume that x1 = x2 . Then we must have y2 ∈ S1 − S2 and y1 ∈ S2 − S1 . We have x1 ∈ X1 ∩ X2 , y1 ∈ X2 ∩ Y1 , and y2 ∈ Y1 ∩ X1 . Hence the conditions in Lemma 26 hold and F is not decomposable . Assume now that all four items are unique . This implies that x1 , y1 ∈ S2 − S1 and y2 ∈ S1 − S2 . Again we have x1 ∈ X1 ∩ X2 , y1 ∈ X2 ∩ Y1 , and y2 ∈ Y1 ∩ X1 so Lemma 26 implies that F is not decomposable .
Proof of Theorem 20 . Let S be an itemset . We will show that md ( M , S ) > 0 only if S is a separator , that is , S = X ∩ Y for two adjacent itemsets in a junction tree . The theorem will follow from this , since a junction tree contains at most K nodes and hence at most K − 1 edges . To prove the result , assume that md ( M , S ) > 0 . This implies that there are two sets , say U , V ∈ rf ( F , S ) . Let x ∈ U and y ∈ V . This implies that there are ( at least ) two sets X and Y such that S ∪ {x} ⊂ X and S ∪ {y} ⊂ Y . Let P = ( P1 , . . . , PL ) be a path from X to Y in the junction tree . The running intersection property implies that
S ⊆ Pi ∩ Pi+1 If there is no i such that S = Pi ∩ Pi+1 , then X and Y should have been joined together during the construction of rf ( F , S ) . In other words , there must be a set in rf ( F , S ) containing x and y . Hence , there are adjacent itemsets Pi and Pi+1 such that S = Pi ∩ Pi+1 . This proves the theorem .
Proof of Theorem 21 . Adding X = S ∪ {x , y} into F can only make splits illegal . Assume that Split(Y , a , b ) becomes illegal after adding X . Theorem 12 implies that a , b ∈ X . We must have {a , b} ⊆ S since otherwise the split would not be legal in the original family . Assume that a = x and let W ∈ max(F ) is the maximal itemset containing {x} ∪ S ( such itemset exists because of the definition of merge ) . Note that , b = y , so b ∈ S and {a , b} ∈ W . If there is another maximal itemset , say Z ∈ max(F ) , such that {x , b} = {a , b} ⊂ Z , then the split Split(Y , x , b ) is not legal in the original family . This proves the theorem .
To see Claim 4 let T be an itemset .
Proof of Theorem 22 . Let Z = S ∪ {x , y} . This itemset is the only maximal itemset in G that contains x and y simultaneously . The definition of reduced family now implies immediately Claims 2 and 3 . To prove Claim 1 let V , W ∈ rf ( F ; S ) such that x ∈ V and y ∈ W . These must be separate sets because of Theorem 19 . The set Z now connects these sets into one in rf ( G ; S ) . If x , y ∈ T , then rf ( G ; T ) has only one set and rf ( if amG ; T ) has zero sets . If x ∈ T and y /∈ T , then T − S = ∅ , otherwise T is covered by Claim 2 or 3 . In such case , the construction of rf ( G ; T ) does not use Z , and hence remains unchanged . Assume that x , y /∈ T . Let X , Y ∈ max(F ) be the itemsets such that S ∪ {x} ⊆ X and S ∪ {y} ⊆ Y . If T − S = ∅ , then Z is not used . Assume now that S T . In this case the items of X − T and Y − T are already joined into one itemset , hence rf ( G ; T ) remains unchanged .
