Boosting with Structure Information in the Functional
Space : an Application to Graph Classification
Department of Electrical Engineering and Computer Science
Hongliang Fei , Jun Huan
University of Kansas
Lawrence , KS 66047 7621 , USA {hfei , jhuan}@ittckuedu
ABSTRACT Boosting is a very successful classification algorithm that produces a linear combination of “ weak ” classifiers ( aka base learners ) to obtain high quality classification models . In this paper we propose a new boosting algorithm where base learners have structure relationships in the functional space . Though such relationships are generic , our work is particularly motivated by the emerging topic of pattern based classification for semi structured data including graphs . Towards an efficient incorporation of the structure information , we have designed a general model where we use an undirected graph to capture the relationship of subgraph based base learners . In our method , we combine both L1 norm and Laplacian based L2 norm penalty with Logit loss function of Logit Boost . In this approach , we enforce model sparsity and smoothness in the functional space spanned by the basis functions . We have derived efficient optimization algorithms based on coordinate decent for the new boosting formulation and theoretically prove that it exhibits a natural grouping effect for nearby spatial or overlapping features . Using comprehensive experimental study , we have demonstrated the effectiveness of the proposed learning methods .
Categories and Subject Descriptors H28 [ Database Management ] : Database ApplicationsData Mining
General Terms Algorithms , Experimentation
Keywords L1 Regularization , Graph Classification , Semi Structured Data , Boosting , Feature Selection
1 .
INTRODUCTION
Boosting is a very successful classification algorithm that produces a linear combination of “ weak ” classifiers ( aka
Figure 1 : Three subgraph features in three graphs . Dashed edge means that the two nodes are connected by a path with varying length >1 . base learners ) to obtain high quality classification models [ 7 , 9 , 34 , 35 ] . Recently , the boosting algorithm has been successfully extended to tasks such as multi class classification [ 23 ] , multi label classification [ 39 ] , cost sensitive learning [ 26 ] , semi supervised learning [ 43 ] , manifold learning [ 24 ] , classification with missing value [ 12 ] , and transfer learning [ 3 ] among others .
In this paper we propose a new boosting algorithm where base learners have structure relationships in the functional space . Our work is particularly motivated by the emerging topic of pattern based classification for semi structure data including graphs [ 16 , 31 , 36 , 38 , 40 ] . For example , Kudo et al . [ 20 ] recently applied boosting to graph classification using subgraphs as base learners and showed the connection of graph boosting to support vector machine with the R convolution kernel . Nowozin et al . [ 28 , 32 ] combined subgraph mining and graph boosting for classifying graphs representing images .
Though graph boosting has demonstrated promising results , the limitations of the current algorithms are that they totally ignore the structure relationships among subgraph base learners and hence may not provide the optimal results for graph classification . We illustrate the point with the following example :
Consider the three labeled graphs G1 , G2 , G3 and three subgraph features F1 , F2 , F3 shown in Figure 1 . Suppose that the class labels for graphs G1 , G2 , G3 are Y = [ 1 , 1,−1]T . We may construct three base learners h1(G ) , h2(G ) and h3(G ) in the format hi(G ) = 1 if Fi ⊆ G and hi(G ) = −1 otherwise ( i ∈ {1 , 2 , 3} ) . These decision rules are derived based on a majority voting of subgraph coverage on positively and negatively labeled graphs .
ABCDEFF1F2F3G1G2G3AGCBHDAHCBGDEKCFJDEFEFd1d2 Considering a boosting algorithm that iteratively selects base classifiers to build ensemble models , since h1 is perfectly correlated with class labels as evaluated on the three training samples , h1 will be selected first . h2 and h3 produce the same prediction for all the graphs in the training data set and hence may be perceived to have the same discriminative power . This is not true in this example . Subgraph F1 and F2 occur in every positive graph sample and are clustered with a consistent relative spatial position . F3 occurs in every graph , but in contrast to F1 and F2 , it has quite different spatial distribution as compared to F1 and F2 and hence we consider F3 as a spurious pattern . Once F1 is selected , we argue that we should select F2 rather F3 to build more stable and interpretable classification models . However , current boosting methods are not designed to perform such model selection since the structure relationships of base learners are not considered in any case .
The spatial relationship is special cases of possible relationships of base learners . Another example is the partial overlapping relationship . We call the possible information regarding to the relationships of base learners as structure relationships . Here we hypothesize that the structure relationship of subgraph features carries important information regarding the importance of the base learners in boosting . Towards an efficient incorporation of such information , we design a general model where we use an undirected graph to capture the relationship of subgraph based base learners . We combined L1 norm and Laplacian based L2 norm penalty with Logit loss function of Logit Boost [ 9 ] . In this approach , we enforce model sparsity and smoothness in the functional space spanned by the basis functions . We derive efficient optimization algorithms based on coordinate decent for the new boosting formulation and theoretically prove that it exhibits a natural grouping effect for nearby spatial or overlapping features . Using comprehensive experimental study and comparing with the state of the art , we have demonstrated the effectiveness of the proposed learning method .
We believe the new formalization is applicable to a variety of boosting applications where ( i ) base learners have a known structure relationship and ( ii ) the optimal ensemble of base learner functions is sparse in the functional space . The proposed method can be naturally extended to other semi structured data such as sequences and trees where patterns such as frequent subsequences and frequent subtrees are widely used for classification [ 21 ] .
1.1 Related Work
Subgraph based supervised learning on graphs has recently attracted extensive research interest [ 16 , 31 , 36 , 38 , 40 ] . For example , Yan et . al [ 40 ] proposed Leap algorithm with two concepts : structural leap search and frequency descending to reduce search space and mine informative patterns faster than previous methods . However , LEAP only considers individual pattern rather than a set of patterns [ 16 ] . Moreover , the discriminate power of a pattern is evaluated entirely on the occurrence information of the pattern and misses interaction among patterns . gPLS [ 31 ] applies partial least square regression to graph mining and performs feature selection and classifier construction simultaneously , but the model interpretability is low due to the use of latent variables [ 16 ] . In addition , the structure relationship among features is neglected . COM [ 16 ] is a newly proposed method that mines co occurrence rules . COM is prone to giving high number of false positives and fails to consider the structure information among features as well .
Recently , a significant amount of progress has been made on developing supervised learning algorithms for feature selection from data with structured features [ 4 , 15 , 18 , 22 , 33 , 37 , 41 , 42 ] . In these models , features may be naturally partitioned into groups [ 4 , 15 , 41 ] or ordered in some meaningful way , such as a chain [ 18 , 37 ] , a tree [ 42 ] or a graph [ 22 , 33 ] . These approaches demonstrate the importance of incorporating prior structure information among features to build highly accurate and interpretable models . However , all these algorithms handles vector data and hence are not applicable to graphs .
Though subgraph based feature selection on graph data has been studied for a long time , none of the existing method considers the structure relationships among subgraph features and hence may not provide the optimal results for graph classification . The objective of this paper is to incorporate the structural information on features into learning and build a more accurate and interpretable graph boosting model .
The rest of the paper is organized as following . In Section 2 , we provide some preliminaries on Boosting . In Section 3 we show our details of our methodology . In Section 4 we apply the algorithm and theory developed in Section 3 to graph data and graph classification . In Section 5 we present the experimental study of our algorithm , followed by a conclusion and a discussion of the future work . 2 . PRELIMINARIES
Given training instances T = {xi , yi}n
We use the following notations throughout the rest of the paper . We use lowercase letters to represent scalar values , lower case letters with an arrow to represent vectors ( eg ⃗β ) , uppercase letters to represent matrices , {λ , λ1 , λ2} to represent Lagrange multiplier , and uppercase calligraphic letters to represent sets . Unless state otherwise , all vectors in this paper are column vectors . xi ∈ X , we construct a set of base learners H = {hj : X 7→ {−1 , +1} , j = 1··· p} . In this paper , we do not assume any type of X ; it may be a vector space , or simply a set . The objective of boosting is to train a composite binary classifier with weight vector ⃗β taking the form of p h⃗β(xi ) = sgn( j=1 βjhj(xi ) ) such that the following empirical loss function ℓ(X , ⃗y ; ⃗β ) is minimized .
∑ i=1 where yi ∈ {−1 , +1} ,
L(X ,Y , ⃗β ) = where l is a loss function . l(yi , h⃗β(xi ) ) i=1
( 1 )
AdaBoost [ 8 ] takes the exponential loss function : n∑ p∑ p∑ l(yi , h⃗β(xi ) = exp(−yi
βjhj(xi) ) )
( 2 ) j=1 and LogitBoost [ 9 ] takes the logit loss function : l(yi , h⃗β(xi ) = log(1 + exp(−yi
βjhj(xi) ) )
( 3 ) j=1
Duchi et . al [ 4 ] modified AdaBoost by imposing L1/L2 or L1/L∞ penalty on weight vectors in a multi task learning decision function on the sample x . For a fixed training data set , we denote all the predicted labels for the training data using functions in H as an n by p matrix H , where n is the sample size , p is the number of base learners . Hi,j = hj(xi ) is the label given by base learner hj ∈ H on the training sample xi . We call H “ object prediction ” matrix . We use Hi to denote ith row of object prediction matrix H ( the predictions of all the base learners on the sample xi ) and H.j to represent jth column of H ( the predictions of hj on the training data ) . We use the following Lemma to show that the minimizer of the expected loss function J(F ) = E(log(1+exp ( −yF ( x)) ) ) is the symmetric logistic transform of P ( y = 1|x ) . Lemma 31 E(log(1 + exp ( −yF ( x)) ) ) is minimized at F ( x ) = log( P ( y=1|x ) 1 −F ( x ) and P ( y = −1|x ) = Proof . Since E imposes expectation over the joint distribution of y and x , we have E(log(1 + exp−yF ( x) ) ) = P ( y = 1|x ) log(1 + exp ( −yF ( x) ) ) + P ( y = −1|x ) log(1 + exp ( yF ( x)) ) . Then it is sufficient to minimize J(F ) by computing the first derivative with respect to F ( x ) : ∂J(F ) ∂F ( x ) = − P ( y=1|x)e . The result follows by setting the derivative to zero .
P ( y=−1|x ) ) . Hence P ( y = 1|x ) =
−F ( x ) + P ( y=−1|x)eF ( x )
1+eF ( x ) .
1+eF ( x )
−F ( x )
1+e
1+e
1
With Lemma 3.1 , the structurally regularized boosting can be interpreted as logistic regression with the same regularization function . Let y = ( y + 1)/2 , taking values of 0 , 1 , and parameterize the binomial probabilities by P ( y = 1|x ) = p(x ) = 1 −F ( x ) , it is sufficient to derive that the logit loss function is equivalent to negative binomial loglikelihood :
1+e
∗
( 4 )
∗ lb(y
, p(x ) ) = −[y log(p(x ) ) + ( 1 − y = log(1 + exp ( −yF ( x) ) )
∗
∗
)log(1 − p(x) ) ] framework . However , they neglect the structure information among base learners . We consider a simple yet effective modification to Logit Boost [ 9 ] that incorporates a composite penalty with L1 and L2 regularization encoding the structural information among base learners on the weight vector , which is detailed in the following section .
∑
3 . BOOSTING WITH STRUCTURE INFORMATION IN THE FUNCTIONAL SPACE We capture the structure relationships among base learners as an undirected graph G , whose nodes correspond to the set of p base learners . Edges in the graph G are weighted , with wi,j indicating the “ closeness ” between the two features and 0 indicating that the two features have no relationship . We call the graph G “ feature graph ” and explore approaches for building a feature graph in Section 42
We incorporate the priori domain knowledge by adding i,j wi,j(βi − βj)2 in a a Tikhonov regularization factor 1 convex fitness function ℓ(X , ⃗y ; ⃗β ) to enforce that the fea2 ture coefficients vary smoothly for neighboring features . The Tikhonov regularization factor could be conveniently written in matrix format ⃗βT L⃗β where L is the Laplacian of G given by : L = D − W . W is the p by p edge weight matrix W = ( wi,j)p i,j=1 , and D is the density matrix of W , defined
{ ∑ i,j=1 where di,j = as D = ( di,j)p To avoid having any feature “ dominate ” the penalization function , we use the normalized Laplacian L following [ 2 ] to normalize the weight of each feature , where the elements of L are defined by
0 if i = j otherwise p k=1 Wi,k
 1 − wi,j/di,i √
−wi,j/ 0 di,idj,j
Li,j = if i = j and di,i ̸= 0 if i and j are adjacent otherwise
Tikhonov regularization does not lead to the sparsity of the model . To obtain a sparse solution , we add the L1 norm of ⃗β to the convex function ℓ(X , ⃗y ; ⃗β ) . Specifically , we seek to identify a vector ⃗β that minimizes the following loss function :
( 5 ) where λ1 > 0 , λ2 > 0 , ||.||1 is L1 norm . In our implemen g(X , ⃗y ; ⃗β ) = ℓ(X , ⃗y ; ⃗β ) + λ1||⃗β||1 + p∑
λ2 ⃗βTL⃗β tation , we use the logitloss [ 9 ] : n∑
1 2
ℓ(X , ⃗y ; ⃗β ) = log(1 + exp ( −yi
βjhj(xi) ) )
( 6 ) i=1 j
The major challenge in fitting the model described in Equation ( 5,6 ) to data is to estimate the parameter ⃗β efficiently and accurately . In the following subsection , we provide the optimization algorithm . 3.1 Optimization Algorithm
We discuss the optimization algorithm for Equation ( 5 ) bellow . We first show that the structurally regularized boosting with logit loss function can be interpreted as an additive logistic regression with the same regularization in the functional space spanned by base learners . We then provide the optimization algorithm based on coordinated decent to solve the equivalent regularized logistic regression towards the p base learners . For simplicity , let F ( x ) = j βjhj(x ) be the
∑
By plug p(x ) into ( 7 ) , we can reduce ( 7 ) to lb(y log(1 + eF ( x ) ) − y negative binomial log likelihood with y
∗
∗
∑
( 7 ) , p(x ) ) = F ( x ) . Now we rewrite ( 5 ) in terms of
∑ ∑ ∑ n i=1[log(1 + exp ( j βjhj(xi ) ] + λ1||⃗β||1 + 1 2 λ2 ⃗βTL⃗β y i=1[log ( 1 + exp ( Hi ⃗β ) ) − y ∗ i Hi ⃗β]+ λ1||⃗β||1 + 1
∗ j βjhj(xi)))−
2 λ2 ⃗βTL⃗β
∗ i n
: p p g(X , ⃗y ; ⃗β ) =
=
( 8 ) After transforming logit loss to negative binomial loglikelihood , we followed the general framework of coordinated decent algorithm proposed in [ 10 ] recently proposed by Friedman et al . for L1 norm regularized logistic regression . Their approach relies on the connection between the Newton ’s method for optimizing logistic regression and the least square formulation . The Newton ’s method amounts to using Taylor expansion , up to a quadratic function , to approximate the logit function . In this way , applying Newton ’s method can be viewed as solving a series of least squares problem ( also called iterative reweighted least squares fitting [ 10] ) . ˜⃗β to negaApplying Taylor ’s expansion at current estimate tive log likelihood function ( 7 ) , we have the reweighted least square problem : wi(zi − Hi ⃗β)2 + C(
˜⃗β )
( 9 ) lQ(⃗β ) = − n∑ i=1 i − ˜p(xi))/(˜p(xi)(1 − ˜p(⃗xi)) ) , wi = ∗ ˜⃗β + ( y where zi = Hi ˜p(xi)(1 − ˜p(xi ) ) and C( ˜⃗β ) is a constant .
In the remaining discussion , we show an extension of Friedman ’s work to solve a reweighted least square fitting ( 9 ) with Laplacian weighted L2 and L1 norm regularization . To handle the new mixture penalty , we derive a modified coordinate descent scheme in Lemma 3.2 extending the work presented in [ 10 ] . Lemma 32 Suppose that the data set contains n observations and p predictors , with the response vector Y = ( y1,··· , yn)T and the data matrix X = ( ⃗x1,··· , ⃗xn)T . We also as∑ sume that the predictors are standardized and the response ij = 1 and is centered so that for all j , n i=1 yi = 0 . The Lagrange form of the network constrained n i=1 xij = 0 ,
∑
∑ i=1 x2 objective function ( with least squares fitness function ) is : n n
( 10 )
( j))−λ2
L(λ1 , λ2 , ⃗β ) = 1 1
∑ i=1 xij(yi− ˜yi
∑ The coordinate wise update has the form ( for each βj ) : ˆβj = Ljk ˆβk , λ1)/(1+λ2Ljj ) where S( l̸=j xil ˆβl is the fitted response value excluding the ( j ) = ˜yi contribution from xij and S(z , γ ) = sign(z)(|z| − γ)+ is the soft thresholding operator where :
2 ( Y − X ⃗β)T ( Y − X ⃗β)+ 2 λ2 ⃗βTL⃗β + λ1||⃗β||1 ∑  z − γ if z > 0 and γ < |z| if z < 0 and γ < |z| if γ ≥ |z| sign(z)(|z| − γ)+ = z + γ 0 p k̸=j
Suppose that we have estimates of ˆβl for l ̸= j and we wish to partially optimize the objective function with respect to βj . We would like to compute the gradient at βj = ˆβj , which only exists if ˆβj ̸= 0 . If ˆβj > 0 , then the gradient for equation 10 is given by
∂L(λ1,λ2,⃗β )
∂βj k̸=j xik ˆβk − xijβj)+
Ljk ˆβk + λ2Ljjβj + λ1 n i=1 xij(yi −∑ = −∑ ∑ ∑p ( j))−λ2 1+λ2Ljj
^βk−λ1 p k̸=j
Ljk k̸=j
λ2 n i=1 xij ( yi− ~yi
( 11 ) Since X is standardized , by setting 11 to 0 , we obtain βj =
∑
. A similar closed form exists for ˆβj < 0 . Combining two cases we will get Lemma 32
We notice that our solution is not constrained in L1 and L2 penalty , but can be extended to L∞ , which recently attracted research interest [ 4 ] , since L∞ norm is differentiable everywhere except singular points ( ⃗β = 0 ) [ 45 ] . We summarize what is discussed previously in the algorithm called LPGB . Given the training data T = {X , ⃗y} , the n by p object prediction matrix H = {hi,j} = {hj(xi)} constructed from base learners , regularization parameters λ1 , λ2 and convergence parameter ϵ , our algorithm iteratively solves ( 8 ) . Here we transform ⃗y to ⃗y∗ using 0/1 to represent the outcome and p(x ) = P ( y = 1|x ) = P ( y = 1|x ) =
1/(1 + exp ( −∑ p j=1 βjhj(x)) ) .
∗
As evaluated in our experimental study in Section 5 , the regularized LPGB algorithm usually has better classification performance and are insensitive to outliers and class label noises , comparing to the unregularized gBoosting [ 20 ] . We believe that these advantages are contributed to the capability of LPGB to select clustered base learners in the functional space . We call this phenomenon the “ grouping effect ”
ˆ⃗β(0 ) = ⃗0 ;
Algorithm 1 LPGB(λ1 , λ2 , H , ⃗y∗ , M axIteration , ϵ ) 1 : Initialize 2 : for i=1 to MaxIteration do 3 : 4 : Use the coordinate descent method in lemma 3.2 to solve the reweighted least squares problem with mixture penalty and obtain the updated ⃗β(i ) ; if || ˆ⃗β(i ) − ˆ⃗β(i−1)||1 ≤ ϵ then
Compute the quadratic approximation for ( 7 ) ;
Break ;
5 : 6 : 7 : end if 8 : end for 9 : return
ˆ⃗β =
ˆ⃗β(i ) ; and we provide theorems to explain the “ group effect ” below . Our proof is similar to that presented in [ 22 ] where we consider a simple case of two base learners that are linked . We show that the related L2 regularization ensures that the difference of the estimated coefficients have an upper bound based on the sample size and the regularization coefficients . 3.2 Grouping Effect
We derive an upper bound of the difference of coefficients between two neighboring features . Motivated from a similar proof in [ 22 ] where a linear regression framework with L1 and L2 regularization , we study the special case in which only two features are connected to each other in the feature graph .
Theorem 33 Give training data T = {xi , yi}n i=1 where xi ∈ X and fixed scalars λ1 , λ2 and let ˆ⃗β(λ1 , λ2 ) be the optimal solution to ( 8 ) , we suppose that ˆβi(λ1 , λ2 ) ˆβj(λ1 , λ2 ) > 0 , and the two features Fi and Fj are only linked to each other on the feature graph . Define Dλ1,λ2 ( i , j ) = | ˆβi(λ1 , λ2)− ˆβj(λ1 , λ2)| , then Dλ1,λ2 ( i , j ) ≤ 2n/λ2 .
Proof . Since
ˆ⃗β(λ1 , λ2 ) is the optimal solution to ( 8 ) , = 0 if ˆβk(λ1 , λ2 ) ̸=
|
ˆ⃗β(λ1 , λ2 ) satisfies ∂g(λ1,λ2,⃗β ) 0 . More specifically , for ˆβi and ˆβj , we have
^⃗β(λ1,λ2 )
∂βk
⃗β= wu,i
∑ −H T .i ( ⃗y∗ − ⃗p(X ) ) + λ1sgn( ˆβi ) + λ2 ˆβi −λ2 u̸=i ∑ −H T .j ( ⃗y∗ − ⃗p(X ) ) + λ1sgn( ˆβj ) + λ2 ˆβj −λ2
ˆβu√ ˆβv√ du,udi,i wv,i
= 0
= 0 v̸=j dv,vdj,j
( 12 )
( 13 ) where ⃗p(X ) = 1/(1 + exp ( −H ⃗β) ) , ⃗y∗ = ( ⃗y + ⃗1)/2 and H is the object prediction matrix . Subtracting ( 12 ) from ( 13)and taking the absolute value with the assumption that di,i = dj,j = wi,j and sgn( ˆβi ) = sgn( ˆβj ) gives .i − H T .j|| ⃗y∗ − ⃗p(X )| λ2
( 14 ) Now consider the worst case , for any x ∈ X hi(x ) ̸= hj(x ) , also | ⃗y∗ − ⃗p(X )| ≤ ⃗1 , hence Dλ1,λ2 ( i , j ) = | ˆβi − ˆβj| ≤ ⃗2T ·⃗1 = 2n/λ2
| ˆβi − ˆβj| =
|H T
λ2
{ hi(x ) =
ˆy −ˆy if Fi ⊆ x otherwise n∑
The upper bound of Dλ1,λ2 ( i , j ) provides two insights of our method : 1 ) smoothness : the coefficients of neighboring base learners are close to each other due to the L2 norm regularized feature graph Laplacian penalty term . 2 ) Grouping effect : Once a base learner is selected , its spatially neighboring base learners will be more likely selected . Thus our boosting algorithm can select groups of spatially neighboring base learners .
4 . APPLICATION TO GRAPH DATA
We show how to apply the LPGB algorithm to graph clas sification bellow . 4.1 Base Learner Construction
In our model , we use frequent subgraphs as features and construct base learners ( decision stamps ) from these features . Given training data {X , ⃗y} and a set of frequent subgraphs , the decision stamp classifier for subgraph Fi is given by :
The prediction ˆy for Fi given training data X is found by :
ˆy = arg max y∈{±1} j=1 yjhi(xj )
This criteria is to perform a majority voting to obtain prediction of the decision stamp based on the percentage of positive ( or negative ) graphs where the feature occurs . gBoosting [ 20 ] uses a similar strategy to construct base learners . 4.2 Feature Graph Construction
One challenge of processing graph data is that there is no natural approach to define the structure relationship of base learners . We notice a few recent studies that are moving towards the direction of defining the relationship among features in graphs and sets . For example in the recently defined graph Graphlet Spectrum kernel [ 19 ] , the spatial relationship of graph feature ( called graphlets ) are explored in an algebraic framework for measuring the structure similarity of graph adjacency matrices . In addition , recently developed association net uses a graph model to represent a set of association rules [ 29 ] . However , these work could not be directly applied in our current framework since the graphlet spectrum method models the spatial relationship of graphlet in an implicit approach and the association rule net only explore the overlapping relationship of features .
Here we adopted our previous work [ 5 , 6 ] to construct feature graphs . In [ 5 ] , we formalize a concept which we called “ feature consistency map ” . A feature consistency map is a undirected graph in which each node represents a feature and each edge encodes the spatial consistency relationship between two features . We measure the minimum distance between two features using the average shortest path connecting a node in one feature to a node in the other feature . We compute the variance of the minimal distance between the occurrences of the two subgraphs in the training data . If the variance is bellow a threshold , we consider the two features are in a consistent spatial relationship . In our experiment study , we adopt the feature consistency map as an approach to construct a feature graph .
In addition , we also explored the possibility of evaluating the structure overlapping relationship of features as did in [ 6 ] . Towards that end , we compute a kernel function for the set of features . A graph kernel function is a positive semi definite function that maps graphs to a Hilbert space in order to evaluate the similarity of graphs in the space . Many kernel functions have been designed for graphs and we use the random walk based Marginalized Graph kernel function [ 17 ] to compute the kernel function for the set of subgraph features . We convert such kernel matrix to a feature graph where nodes are features and edges are labeled with the inner product ( as evaluated with a graph kernel function ) of the two features . To avoid a complete connected graph , we use a threshold . If the inner product between two features is less than a threshold , we set the weight of the edge to zero ( and hence canceling the edge ) . The aforementioned approach provides another way to construct a feature graph .
5 . EXPERIMENTAL STUDY
We have performed a rigorous evaluation of our algorithm in terms of modeling accuracy and feature selection performance using 6 Protein structure data sets , obtained from [ 16 ] . We implemented a prototype of our method in Matlab . We have compared our method with state of the art methods including Support Vector Machine Recursive Feature Elimination ( SVM RFE ) [ 11 ] , gBoosting [ 20 ] , graph partial least square regression ( gPLS ) [ 31 ] , graph classification based Pattern Co occurrence ( COM ) [ 16 ] . We obtained the SVM RFE executable along with the spider machine learning toolbox from http://wwwkybtuebingenmpgde/ bs/people/spider/ . For gBoosting , we use the gboost toolbox [ 30 ] . We obtained gPLS and COM directly from the original authors of the methods . All the experiments were conducted on a PC with a 2.8Ghz duo core CPU and 3GB memory . 5.1 Data sets
To evaluate our methods , we utilized 6 protein structure graph data sets that were originally studied in [ 16 ] . Each data set is a set of geometric graphs representing a set of three dimensional protein structures . Nodes in such graphs represent amino acids in a protein structure and are labeled with the amino acid type . Edges represent the pairwise Euclidian distance of amino acids ( defined between Cα atoms ) and are labeled with the discretized distances .
Graphs in the data sets are labeled . Positive samples are sampled from a selected protein family . Negative samples are randomly sampled from the Protein Data Bank . On average a graph contains 250 nodes and 1600 edges . Proteinstructure graphs are much larger than chemical structure graphs , which usually contain about hundreds of nodes and thousands of edges , and contain much large number of patterns . Working with protein structure graphs are hence more challenging for constructing sparse predictive models .
In Table 1 , we summarize the characteristics of the 6 protein structure graph data sets . For each data set , we list the data set index , the related protein family ID in the SCOP database [ 27 ] , the description of the protein family , the number of positive samples and the number of negative samples . See [ 16 ] for a comprehensive description of the data collection process .
Table 1 : Data set : the symbol of the data set . P : total number of positive samples , N : total number of negative samples
Data set
SCOP ID
Family Name
P1 P2 P3 P4 P5 P6
48623 52592 48942 56437 56251 88854
Vertebrate phospholipase A2
G proteins
C1 set domains
C type lectin domains Proteasome subunits
Protein kinases , catalytic subunit
P 29 33 38 38 35 41
N 29 33 38 38 35 41
5.2 Experimental Protocol
We use standard cross validation to generate training and testing data sets . We apply FFSM [ 14 ] to generating frequent subgraphs from the training data set with min sup = 0.30 and with subgraph size between 2 and 6 . Such subgraphs are used as feature for feature based classification(eg SVM , SVM RFE ) or as base learners for boosting based classification including gBoosting and our methods .
For SVM RFE , we encode each graph sample as a binary feature vector , indexed by the mined subgraphs , with values indicate the presence ( 1 ) or absence ( 0 ) of the related features . We perform feature selection using SVM RFE and use LibSVM [ 1 ] with linear kernel to construct the best model . We use 5 fold cross validation in the training data set to select important parameter C for SVM .
For COM , we set tp = 0.3 and tn = 0 as proposed in [ 16 ] , where tp is the minimal positive frequency for a classification rule and tn represents the maximal negative frequency permitted . For gPLS , we use min sup = 0.3 and examine the combinations of n = {2 , 4 , 8 , 16} and k = {2 , 4 , 8 , 16} for optimal setting . For gBoosting , we also set min sup = 0.30 and search the optimal parameter µ ( misclassification cost ) in the range of {0.04 , 0.06 , . . . , 0.18 , 020} All the parameter selection are based on another 5 fold cross validation on the training data only .
For our own methods , we utilize two approaches to model the spatial correlation of base learners ( ie subgraphs ) . The first approach , LPGBK , is to construct a kernel function for the subgraphs , utilizing the the Marginalized kernel [ 17 ] . The second approach , LPGBCMP , is to construct the feature consistency map , as investigated in [ 5 ] . We fix max var = 1 for feature consistency map building threshold and δ = 0.25 for overlapping threshold . Empirical study shows that there is no significant change if we change these two parameters within a wide range . Further details of the two spatial correlation computation methods can be found in [ 5 , 6 ] .
Below we summarize the model construction and model evaluation .
Model Construction . For each data set , we partition the data set into 5 folds to perform 5 fold cross validation ( CV ) with 4 folds for training and 1 fold for testing . We use another 5 fold CV on the training data set to select the optimal parameters for each method . We then generate a single model from the entire training set with the selected parameters and apply the model to the testing data set for prediction .
Model Comparison . For model comparison , we collect the sensitivity ( TP/(TP+FN) ) , specificity ( TN/(TP+FP ) ) and accuracy ( (TP+TN)/S ) of the trained model , where
TP stands for true positive , FP stands for false positive , TN stands for true negative , FN stands for false negative , and S stands for the total number of samples . All the values reported are collected from the testing data set only and are averaged across 10 replicates of the 5 fold cross validation in a total of 50 experiments . 5.3 Classification Performance
In this subsection , we show the performance of our methods compared with SVM RFE , gPLS , gBoosting and COM . The accuracy is shown in Fig 2 . Since the standard deviation is around 2% 5 % for all these methods , we do not list it here .
Figure 2 : Accuracy comparison of on 6 data sets .
In Fig 2 , we observe that the accuracy of all these methods has the same trend with different data sets . gBoosting and gPLS have comparable performance in the 6 data sets . SVM RFE outputs gBoosting , gPLS , and COM in three out of six data sets and have comparable performance for the rest . Comparing two versions of our methods , LPGBCMP outperforms LPGBK on all data sets . In fact LPGBCMP performs best among all the evaluated data sets though the margin may be small for 3 data sets when compared with SVM RFE .
To better understand the accuracy differences , we plot the average sensitivity and average specificity of all methods in Fig 3 . It is clear that COM provides the best sensitivity among the majority of data sets . COM utilizes a rule based classification algorithm where it classifies a graph sample as positive if a co occurrence pattern rule is satisfied . This algorithm is not specific enough , as compared to other methods ( shown in the right panel of Fig 3 ) . Interesting enough , all boosting based methods , including gBoosting , LPGBCMP , and LPGBK , have very high specificity comparing to the rest of the methods . Overall , the regularized boosting methods such as LPGBCMP and LPGBK seem to have a good compromise between specificity and sensitivity . This observation provides experimental evidence supporting our hypothesis that structure information among base classifier should be considered in order to build a highly accurate predictive model for semi structure data such as graphs . 5.4 Grouping Selection Effect and Stable Spa tial Distribution
To evaluate the capability of the LPGBCMP algorithm for selecting grouped base learners , we visualize the spatial distribution of selected base learners in original graphs . By
00000100020003000400050006000700080009001000P1P2P3P4P5P6AccuracyData SetAverage AccuracyLPGBCMPSVM_RFELPGBKgPLSgBoostingCOM Figure 3 : Left : Sensitivity comparison . Right : Specificity comparison ranking the base learners by the learned coefficients , we select the top three features for LPGBCMP and gBoosting . We plot the embedding of the three subgraphs in two proteins : protein 1EGI and protein 1H8U belonging to the same protein family in Fig 5 . We rotate the protein structures and highlight the occurrence of the features with circles for a better demonstration .
In Fig 5 , the upper row shows the spatial distribution of the top three features for LPGBCMP in two proteins and the lower row shows the distribution for the top three features from gBoosting . Each column uses the same protein for demonstration . From Fig 5 , we observe that F1 , F2 and F3 from LPGBCMP have a consistent spatial distribution on the two proteins . F1 and F2 are clustered and both are close to F3 . In contrast , features from gBoosting do not have a stable spatial distribution in the two proteins . The observation supports our claim that our method can select grouped features with stable spatial distribution among the graph data .
5.5 Method Robustness
A common concern with boosting is that the method is usually sensitive to outliers and errors in the training data set due to the exponential loss function . We use logit loss function that is less sensitive to outliers . However , as claimed in [ 25 ] , any convex loss function may degenerate to random guess with a certain level of random classification noise . L2 regularization in linear regression has been shown to stabilize the learning function [ 13 ] . In our algorithm design , we used the Laplacian based L2 regularization and this may reduce the boosting algorithms’ sensitivity to outliers and random classification noise . To test the robustness of our method experimentally , we singled out the P4 data set and performed 5 fold cross validation with class label errors . In particular , for each fold , we change certain percentage of the class labels in the training data , train a model with changed training data , and apply trained model to normal test data . In Fig 4 , we report the average accuracy with error rate ranging from 0 % to 25 % for LPGBCMP , gPLS , gBoosting , SVM RFE and COM .
From Fig 4 , a clear trend is that the accuracy of all methods decreases as more errors are introduced in the training data set . There is a sharp deceasing from 0 to 5 % for SVM RFE and COM . The regularized boosting method remains the best over all the settings , even though the performance gain is not significant . From the test , we conclude that LPGBCMP is at least as sensitive ( if not less ) to noises
Figure 4 : Average accuracy with different percentage of flipped training labels
Figure 6 : Average accuracy with different max var as other classifiers including SVM and partial least square based methods .
In addition , we evaluate the robustness of the regularized boosting algorithm by changing different parameter values . Among the parameters that may affect the performance of the regularized boosting algorithm , we test the parameter max var , which is used to derive the feature consistency map . With a large value of max var , the edge number of feature consistency map increases and with smaller value of max var , the edge number of feature consistency map decreases .
Fig 6 indicates average accuracy on 5 fold cross validation for each value of threshold max var from 0.5 to 8 . From the result , we observe that the accuracy remains stable within a relatively wide range of threshold and the best accuracy can be obtained around 1 to 2 . Furthermore , the relation
( cid:36)(cid:72)(cid:85)(cid:74)(cid:72)fi(cid:72)(cid:86)(cid:76)(cid:87)(cid:76)(cid:76)(cid:87)(cid:92)(cid:87)fi(cid:72)(cid:87)(cid:72)(cid:86)(cid:76)(cid:87)(cid:76)(cid:76)(cid:87)(cid:92)(cid:42)(cid:42)(cid:74)(cid:74)(cid:86)(cid:87)(cid:76)(cid:74)(cid:50)(cid:36)(cid:72)(cid:85)(cid:74)(cid:72)fi(cid:72)(cid:76)(cid:73)(cid:76)(cid:76)(cid:87)(cid:92)(cid:87)fi(cid:72)(cid:87)(cid:72)(cid:76)(cid:73)(cid:76)(cid:76)(cid:87)(cid:92)(cid:42)(cid:42)(cid:74)(cid:74)(cid:86)(cid:87)(cid:76)(cid:74)(cid:50)051015202505055060650707508085090951PercentageAverage Accuracy LPGBCMPgPLSgBoostingSVM_RFECOMfl(cid:66)(cid:85)(cid:36)(cid:85)(cid:92)(cid:42 ) Figure 5 : Top Left : Spatial distributions of the top 3 features from LPGBCMP in protein 1EGI . Top Right : Spatial distributions of the same 3 features from LPGBCMP in protein 1H8U . Lower Left : Spatial distributions of the top 3 features from gBoosting in protein 1EGI . Lower Right : Spatial distributions of the same 3 features from gBoosting in protein 1H8U . ship between the performance and parameter mar var is revealed . When max var is quite small , the structure information among features is ignored and our method will degenerate to regular logit boosting with elastic net regularization [ 44 ] ; when max var is large , the feature graph will be a complete graph and our method may possibly introduce less discriminative features hence undermine the performance .
Overall , the regularized boosting method is effective and achieves good accuracy within a wide range parameters and a certain number of outliers . 6 . CONCLUSIONS
In this paper , we presented a novel boosting algorithm that considered the structure relationship of base learners in the functional space . We model the structure relationship as an undirected graph and incorporate such information by introducing a L2 norm regularized graph Laplacian to standard boosting formalization . Though the new algorithm may be applied to many applications , we specifically focus on constructing supervised graph learning models in this paper . Using a comprehensive experimental study with protein structure graphs and comparing with current state of the art , we demonstrate that the new algorithm selects clustered features with stable spatial relationship , and achieves better predictive performance . In the future we plan to extend the current work to constructing boosting algorithms for other types of data such as trees and sequences , where the base learners have a structure relationship in the functional space . Acknowledgments This work has been supported by an Office of Naval Research award ( N00014 07 1 1042 ) and the National Science Foundation under Grant No . 0845951 .
7 . REFERENCES [ 1 ] C . Chang and C . Lin . Libsvm : a library for support vector machines , 2001 . Software available at http://wwwcsientuedutw/ cjlin/libsvm .
[ 2 ] F . Chung . Spectral graph theory . CBMS Reginal
Conferences Series , 92 , 1997 .
[ 3 ] W . Dai , Q . Yang , G . rong Xue , and Y . Yu . Boosting for transfer learning . In International Conference on Machine Learning , 2007 .
[ 4 ] J . Duchi and Y . Singer . Boosting with structural sparsity . In ICML ’09 : Proceedings of the 26th Annual International Conference on Machine Learning , pages 297–304 , 2009 .
[ 5 ] H . Fei and J . Huan . Structure feature selection for graph classification . In Proc . ACM 17th Conference on Information and Knowledge Management , 2008 .
[ 6 ] H . Fei and J . Huan . L2 norm regularized feature kernel regression for graph data . In CIKM ’09 : Proceeding of the 18th ACM conference on Information and knowledge management , pages 593–600 , 2009 .
[ 7 ] Y . Freund . Boosting a weak learning algorithm by majority . Information and Computation , 121:256–285 , 1995 .
[ 8 ] Y . Freund and R . Shapire . A decision theoretic generalization of on line learning and an application to boosting . In Proceedings of the Second European Conference on Computational Learning Theory , 1995 .
[ 9 ] J . Friedman , T . Hastie , and R . Tibshirani . Additive logistic regression : a statistical view of boosting . Annals of Statistics , 28(2):337–407 , 2000 .
[ 10 ] J . Friedman , T . Hastie , and R . Tibshirani .
Regularization paths for generalized linear models via coordinate descent . The Annals of Applied Statistics , page to be appeared , 2009 .
F1F2F3F1F2F3F1F2F3F1F2F3 [ 11 ] I . Guyon , J . Weston , S . Barnhill , and V . Vapnik . Gene selection for cancer classification using support vector machines . Machine Learning , 46:389–422 , 2002 January .
[ 28 ] S . Nowozin , K . Tsuda , T . Uno , T . Kudo , and
G . Bakir . Weighted substructure mining for image analysis . In Computer Vision and Pattern Recognition 2007 . CVPR ’07 , pages 1–8 , 2007 .
[ 12 ] G . Haffari , Y . Wang , S . Wang , G . Mori , and F . Jiao .
[ 29 ] G . Pandey , S . Chawla , S . Poon , B . Arunasalam , and
Boosting with incomplete information . In International Conference on Machine Learning , 2008 .
[ 13 ] T . Hastie , R . Tibshirani , and J . Friedman . The
Elements of Statistical Learning . Springer Verlag , 2009 .
[ 14 ] J . Huan , W . Wang , and J . Prins . Efficient mining of frequent subgraph in the presence of isomorphism . In Proceedings of the 3rd IEEE International Conference on Data Mining ( ICDM ) , pages 549–552 , 2003 .
[ 15 ] L . Jacob , G . Obozinski , and J P Vert . Group lasso with overlap and graph lasso . In ICML ’09 : Proceedings of the 26th Annual International Conference on Machine Learning , pages 433–440 , 2009 .
[ 16 ] N . Jin , C . Young , and W . Wang . Graph classification based on pattern co occurrence . In CIKM ’09 : Proceeding of the 18th ACM conference on Information and knowledge management , pages 573–582 , 2009 .
[ 17 ] H . Kashima , K . Tsuda , and A . Inokuchi . Marginalized kernels between labeled graphs . In Proc . of the Twentieth Int . Conf . on Machine Learning ( ICML ) , 2003 .
[ 18 ] S . Kim and E . P . Xing . Structured feature selection in highdimensional space via block regularized regression . In In Proceedings of the 24th International Conference on Conference on Uncertainty in Artificial Intelligence , 2008 .
[ 19 ] R . I . Kondor , N . Shervashidze , and K . M . Borgwardt .
The graphlet spectrum . In ICML09 , volume 382 , page 67 . ACM , 2009 .
[ 20 ] T . Kudo , E . Maeda , and Y . Matsumoto . An application of boosting to graph classification . In NIPS , 2004 .
[ 21 ] C . Leslie , E . Eskin , and W . Noble . The spectrum kernel : a string kernel for svm protein classification . In Pac Symp Biocomput , pages 564–75 , 2002 .
[ 22 ] C . Li and H . Li . Newwork constrained regularization and variable selection for analysis of genomic data . Bioinformatics , 24(9):1175–1182 , 2008 .
[ 23 ] P . Li . Adaptive base class boost for multi class classification . In International Conference on Machine Learning , 2008 .
[ 24 ] N . Loeff , D . Forsyth , and D . Ramachandran .
Manifoldboost : Stagewise function approximation for fully , semiand un supervised learning . In International Conference on Machine Learning , 2008 . [ 25 ] P . M . Long and R . A . Servedio . Random classification noise defeats all convex potential boosters . In International Conference on Machine Learning , pages 608–615 , 2008 .
[ 26 ] A . C . Lozano and N . Abe . Multi class cost sensitive boosting with p norm loss functions . In ACM SIGKDD International Conference on Knowledge Discovery and Data mining , 2008 .
[ 27 ] A . Murzin , S . Brenner , T . Hubbard , and C . Chothia . SCOP : a structural classification of proteins database for the investigation of sequences and structures . Journal of Molecular Biology , 247:536–40 , 1995 .
J . G . Davis . Association rules network : Definition and applications . Stat . Anal . Data Min . , 1(4):260–279 , 2009 .
[ 30 ] H . Saigo and et . al . gboost : Graph learning toolbox for matlab , 2007 . http://wwwkybtuebingenmpg de/bs/people/nowozin/gboost/ .
[ 31 ] H . Saigo , N . Kr¨amer , and K . Tsuda . Partial least squares regression for graph mining . In Proc . SIGKDD08 , 2008 .
[ 32 ] H . Saigo , S . Nowozin , T . Kadowaki , T . Kudo , and
K . Tsuda . gboost : a mathematical programming approach to graph classification and regression . Journal of Machine Learning , 75(1):69–89 , 2009 .
[ 33 ] T . Sandler , P . P . Talukdar , and L . H . Ungar .
Regularized learning with networks of features . In NIPS08 , 2008 .
[ 34 ] R . Schapire . The strength of weak learnability .
Machine Learning , 5:197–227 , 1990 .
[ 35 ] R . Schapire and Y . Singer . Improved boosting algorithms using confidence rated predictions . Machine Learning , 37:297–336 , 1999 .
[ 36 ] M . Thoma , H . Cheng , A . Gretton , J . Han , H P
Kriegel , A . J . Smola , L . Song , P . S . Yu , X . Yan , and K . M . Borgwardt . Near optimal supervised feature selection among frequent subgraphs . In Proccedings of the 2009 SIAM Conference on Data Mining ( SDM 2009 ) , pages 1076–1087 . Philadelphia , PA , USA , 2009 .
[ 37 ] R . Tibshirani , M . Saunders , S . Rosset , J . Zhu , and K . Knight . Sparsity and smoothness via the fused lasso . J . R . Statist . Soc . , 67(1)):91–108 , 2005 .
[ 38 ] K . Tsuda . Entire regularization paths for graph data .
In ICML07 , 2007 .
[ 39 ] R . Yan , J . Tesic , and J . R . Smith . Model shared subspace boosting for multi label classification . In ACM SIGKDD International Conference on Knowledge Discovery and Data mining , pages 834–843 , 2007 .
[ 40 ] X . Yan , H . Cheng , J . Han , and P . Yu . Mining significant graph patterns by leap search . In Proceedings of the 2008 ACM SIGMOD international conference on Management of data , pages 433–444 . ACM , 2008 .
[ 41 ] M . Yuan and Y . Lin . Model selection and estimation in regression with grouped variables . Journal of the Royal Statistical Society , Series B , 68:49–67 , 2006 .
[ 42 ] P . Zhao and B . Yu . Grouped and hierarchical model selection through composite absolute penalties . Annals of Statistics , 2006 .
[ 43 ] L . Zheng , S . Wang , C . hoon Lee , and Y . Liu .
Information theoretic regularization for semi supervised boosting . In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2009 .
[ 44 ] H . Zou and T . Hastie . Regularization and variable selection via the elastic net . Journal of the Royal Statistical Society B , 67:301–320 , 2005 . [ 45 ] H . Zou and M . Yuan . F∞ norm support vector machine . Statistica Sinica , 18:379–398 , 2008 .
