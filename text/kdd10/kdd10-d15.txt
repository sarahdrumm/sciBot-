An Efficient Algorithm for a Class of
Fused Lasso Problems
Jun Liu
Arizona State University
Tempe , AZ 85287 jliu@asuedu
Lei Yuan
Arizona State University
Tempe , AZ 85287 leiyuan@asuedu
Jieping Ye
Arizona State University
Tempe , AZ 85287 jiepingye@asuedu
ABSTRACT The fused Lasso penalty enforces sparsity in both the coefficients and their successive differences , which is desirable for applications with features ordered in some meaningful way . The resulting problem is , however , challenging to solve , as the fused Lasso penalty is both non smooth and non separable . Existing algorithms have high computational complexity and do not scale to large size problems . In this paper , we propose an Efficient Fused Lasso Algorithm ( EFLA ) for optimizing this class of problems . One key building block in the proposed EFLA is the Fused Lasso Signal Approximator ( FLSA ) . To efficiently solve FLSA , we propose to reformulate it as the problem of finding an “ appropriate" subgradient of the fused penalty at the minimizer , and develop a Subgradient Finding Algorithm ( SFA ) . We further design a restart technique to accelerate the convergence of SFA , by exploiting the special “ structures" of both the original and the reformulated FLSA problems . Our empirical evaluations show that , both SFA and EFLA significantly outperform existing solvers . We also demonstrate several applications of the fused Lasso .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications Data Mining
General Terms Algorithms
Keywords Fused Lasso , ℓ1 regularization , restart , subgradient
1 .
INTRODUCTION
The fused Lasso penalty introduced in [ 30 ] can yield a solution that has sparsity in both the coefficients and their successive differences . It has found applications in comparative genomic hybridization [ 25 , 31 ] , prostate cancer analysis [ 30 ] , image denoising [ 8 ] , and time varying networks [ 1 ] , where features can be ordered in some meaningful way . Some properties of the fused Lasso have been established in [ 26 ] .
In this paper , we focus on optimizing the following class of op timization problems with the fused Lasso penalty : min x∈Rn h(x ) = loss(x ) + fl(x ) ,
( 1 ) where loss(x ) is a given smooth and convex loss ( eg , the least squares loss ) defined on a set of training samples , and fl(x ) = λ1 n
X i=1
|xi| + λ2 n
X i=2
|xi − xi−1|
( 2 ) is the fused Lasso penalty with the nonnegative λ1 and λ2 . The problem in ( 1 ) is challenging to solve , as the fused Lasso penalty is both non smooth and non separable .
Existing algorithms reformulate ( 1 ) as the equivalent constrained smooth optimization problem by introducing additional variables and constraints , and then apply the standard solver for optimization . Let n denote the sample dimensionality . Tibshirani et al . proposed the fused Lasso with the least squares loss [ 30 ] . They derived a smooth reformulation by introducing 4n auxiliary variables , linear constraints of the type : a ≤ Ay ≤ b ( the matrix A is of size ( 2n + 2 ) × 5n with 11n − 1 non zero elements ) , and 4n non negative constraints , and then solved the reformulated problem by the SQOPT1 package . Ahmed and Xing proposed to solve the fused Lasso penalized logistic regression by introducing 2n auxiliary variables and 4n inequality constraints [ 1 ] , and then solved the reformulated problem by the CVX2 optimization package [ 10 ] . However , the constrained smooth reformulation usually does not scale well with n , due to the large number of auxiliary variables and inequality constraints introduced . Indeed , it was pointed out in [ 30 ] that , “ one difficulty in using the fused Lasso is the computational speed" , and “ when n > 2000 and m > 200 ( m denotes the number of samples ) , speed could become a practical limitation" .
In this paper , we develop an Efficient Fused Lasso Algorithm ( EFLA ) by treating the objective function of ( 1 ) as a composite function with the smooth part loss(· ) and the other non smooth part fl(· ) . One appealing feature of EFLA is that it makes use of the special structure of ( 1 ) for achieving a convergence rate of O(1/k2 ) for k iterations , which is optimal for the first order black box methods . Note that , when directly applying the black box first order method for solving the non smooth problem ( 1 ) , one can only achieve a convergence rate of O(1/√k ) , much slower than O(1/k2 ) .
In the proposed EFLA , a key building block ( in each iteration ) is the proximal operator [ 19 ] associated with the nonsmooth fused Lasso penalty fl(· ) ( corresponding to a pair of λ1 and λ2 ) , which 1tomoptcom/tomlab/products/snopt/solvers/SQOPTphp 2stanford.edu/∼boyd/cvx is also called the Fused Lasso Signal Approximator [ 8 , FLSA ] . Existing approaches [ 8 , 12 ] for solving FLSA usually employ a path technique that requires exactly following the path for computing the solution corresponding to the desired pair of λ1 and λ2 . Despite their advantage in obtaining the whole path solutions , they might not be efficient for our proposed EFLA , which needs the computation of FLSA corresponding to a pair of λ1 and λ2 only in each iteration . In addition , it is hard for them to incorporate the “ warm" start technique for further improving the efficiency .
To efficiently solve FLSA corresponding to a pair of λ1 and λ2 , we propose to reformulate it as the problem of finding an “ appropriate" subgradient of the fused penalty at the minimizer , and develop a Subgradient Finding Algorithm ( SFA ) . We further design a restart technique for accelerating the convergence of SFA , by exploiting the special “ structures" of the original and the reformulated FLSA problems . When used as a building block in EFLA , SFA is shown to converge within dozens of iterations for problems of size up to 107 using the “ cold" start ; with the “ warm" start , SFA usually converges within 10 iterations . Our empirical evaluations show that , both SFA and EFLA significantly outperform the existing solvers . We also demonstrate several applications of the fused Lasso .
Notations : k·k1 , k·k , and k·k∞ denote the ℓ1 , ℓ2 , and ℓ∞ norm , respectively . R ∈ R(n−1)×n is a sparse matrix defined as :
Rij =
8>< > :
−1 j = i , i = 1 , 2 , . . . , n − 1 1 j = i + 1 , i = 1 , 2 , . . . , n − 1 0 otherwise .
We can rewrite the fused Lasso penalty in ( 2 ) as fl(x ) = λ1kxk1 + λ2kRxk1 .
( 3 ) Let sgn(· ) and SGN(· ) be the operators defined in the componentwise fashion : if t > 0 , sgn(t ) = 1 , SGN(t ) = {1} ; if t < 0 , sgn(t ) = −1 , SGN(t ) = {−1} ; and if t = 0 , sgn(t ) = 0 , SGN(t ) = [ −1 , 1 ] . Pλ2 ( x ) is an operator that projects each element of x onto the interval [ −λ2 , λ2 ] . Let e ∈ Rn be a vector composed of 1 ’s . Denote [ 1 : n ] as the set of indices from 1 to n .
2 . THE PROPOSED EFFICIENT FUSED
LASSO ALGORITHM
We review several categories of first order methods that can be applied to optimizing the composite function ( 1 ) in Section 2.1 , present our proposed algorithm in Section 2.2 , and discuss the key building block—FLSA in Section 23 2.1 Composite Function Optimization Subgradient Descent ( SD ) When treating h(x ) as the general non smooth convex function , we can apply the subgradient descent [ 20 , 21 ] , which can achieve a convergence rate of O(1/√k ) for k iterations . However , SD has the following two disadvantages : 1 ) the convergence is slow ; and 2 ) the iterates of SD are very rarely at the points of non differentiability [ 6 ] , thus it might not achieve the desirable sparse solution ( which is usually at the point of nondifferentiability ) within a limited number of iterations .
Friedman et al . [ 8 ] derived a modified CD for solving FLSA— a special case of ( 1 ) , and discussed its extension for solving the general fused Lasso ; however , as explicitly mentioned in [ 8 , Section 3 , page 310 ] , the resulting algorithm is not guaranteed to give the exact solution .
Nesterov ’s Method Nesterov ’s method [ 20 , 21 ] is an optimal firstorder black box method for smooth convex optimization , achieving a convergence rate of O(1/k2 ) . In the recent studies [ 2 , 22 ] , the Nesterov ’s method is extended to solve the composite function composed of one smooth part and the other non smooth part . The resulting algorithm can achieve the optimal convergence rate of O(1/k2 ) , at the expense that the proximal operator [ 19 ] associated with the non smooth part needs to be solved at each iteration . For the problem in ( 1 ) , the associated proximal operator is the FLSA . The Nesterov ’s method has been applied to solve various sparse learning formulations [ 2 , 13 , 16 , 17 , 18 , 22 , 32 ] .
Forward Looking Subgradient ( FOLOS ) The FOrward LOoking Subgradient [ 6 ] was proposed for optimizing the composite function . FOLOS is a forward backward splitting method . It can be applied for both online and batch learning . For bach learning , the convergence rates of O(1/√k ) and O(1/k ) were established for the general convex and the smooth convex loss functions .
Regularized Dual Averaging ( RDA ) The Regularized Dual Averaging [ 35 ] was proposed for solving the regularized composite function , based on the dual averaging method proposed in [ 23 ] . RDA was designed for stochastic learning and online learning , and the convergence rates of O(1/√k ) and O(ln k/k ) were established for the general convex and the strongly convex regularization .
In this paper , we consider solving ( 1 ) in the batch learning setting , and propose to apply the Nesterov ’s method due to its fast convergence rate . Note that , one can develop the online learning algorithms for ( 1 ) using algorithms such as FOLOS and RDA , where FLSA is also a key building block . The efficient computation of FLSA will be discussed in Section 3 .
2.2 The Efficient Fused Lasso Algorithm
We first construct the following model for approximating the composite function h(· ) at the point x : L hL,x(y ) = [ loss(x ) + hloss′(x ) , y − xi ] + fl(y ) + 2 ky − xk2 , ( 4 ) where L > 0 . In the model hL,x(y ) , we apply the first order Taylor expansion at the point x ( including all terms in the square bracket ) for the smooth function loss(· ) , and directly put the nonsmooth penalty fl(· ) into the model . The regularization term L 2 ky− xk2 prevents y from walking far away from x , thus the model can be a good approximation to h(y ) in the neighborhood of x . With the model ( 4 ) , we can develop the following gradient de scent like method for solving ( 1 ) : xi+1 = arg min y hLi,xi ( y )
( 5 )
Coordinate Descent ( CD ) Coordinate descent [ 33 ] and its recent extension—coordinate gradient descent [ 34 ] are applicable for optimizing the non differentiable composite function . Convergence results have been established , when the non differentiable part is separable [ 33 , 34 ] ; and CD was applied for solving Lasso in [ 8 ] . However , when applied for solving ( 1 ) , CD may not converge to the desirable solution , as the fused Lasso penalty is non separable . for some properly chosen {Li} . It has been shown in [ 2 , 21 ] that , the scheme in ( 5 ) can yield a convergence rate of O(1/k ) , and can be further accelerated to O(1/k2 ) .
The accelerated method can be derived using the “ estimate sequence" [ 21 , 22 ] , which is quite involved . To make the presentation relatively easy to follow , we use the scheme provided in [ 2 , 20 ] to present the Nesterov ’s method for solving ( 1 ) .
The Nesterov ’s method is based on two sequences {xi} and {si} in which {xi} is the sequence of approximate solutions , and {si} is the sequence of search points . The search point si is the affine combination of xi−1 and xi as si = xi + βi(xi − xi−1 ) ,
( 6 ) where βi is a properly chosen coefficient . The approximate solution xi+1 is computed as the minimizer of hLi,si ( y ) : xi+1 = arg min y hLi,si ( y ) ,
( 7 ) where Li is determined by the line search according to the ArmijoGoldstein rule so that Li should be appropriate for si .
The algorithm for solving ( 1 ) is presented in Algorithm 1 . Following the proof given in [ 20 , 2 ] , we can establish the following global convergence result : h(xk+1 ) − h(x∗ ) ≤
2 max(2 ˜L , L0)kx0 − x∗k2
( k + 1)2
,
( 8 ) where x∗ is an optimal solution to ( 1 ) , and ˜L is the Lipschitz continuous gradient of the smooth convex loss function loss(· ) . Algorithm 1 The Efficient Fused Lasso Algorithm ( EFLA ) Input : λ1 ≥ 0 , λ2 ≥ 0 , L0 > 0 , x0 , k Output : xk+1 1 : Initialize x1 = x0 , α−1 = 0 , α0 = 1 , and L = L0 . 2 : for i = 1 to k do 3 : 4 :
Set βi = αi−2−1 αi−1 Find the smallest L = Li−1 , 2Li−1 , . . . such that
, si = xi + βi(xi − xi−1 ) h(xi+1 ) ≤ hL,si ( xi+1 ) , where xi+1 = arg miny hL,si ( y ) 1+√1+4α2 Set Li = L and αi+1 = i
2
5 : 6 : end for
In Algorithm 1 , a key building block is the problem ( 7 ) , which is the fused Lasso signal approximator to be discussed in the next subsection . 2.3 Fused Lasso Signal Approximator
The Fused Lasso Signal Approximator ( FLSA ) solves the fol lowing problem : min x∈Rn f λ1 λ2
( x ) ≡
1 2kx − vk2 + λ1kxk1 + λ2kRxk1 ,
( 9 )
2kx − vk2 . which is a special case of ( 1 ) by setting loss(x ) = 1 Note that , FLSA in ( 9 ) is essentially the proximal operator [ 11 , 14 , 19 ] associated with the fused Lasso penalty fl(x ) .
Let where ∂f λ1 λ2 subdifferential of f λ1 λ2
( x∗ ) denotes the subdifferential of f λ1 λ2
( · ) at x∗ . The
( · ) can be computed as :
∂f λ1 λ2
( x ) = x − v + λ1SGN(x ) + λ2RTSGN(Rx ) .
( 13 )
Using the subgradient technique , it has been shown in [ 8 ] that , the minimizer of the problem ( 9 ) for any value of ( λ1 , λ2 ) can be obtained by a simple soft thresholding of the solution obtained for ( 0 , λ2 ) , as stated in Theorem 1 . We provide an alternative and simplified proof using the technique of subdifferential ; and this simplified proof also motivates our proposed method in Section 3 .
THEOREM 1 . For any λ1 , λ2 ≥ 0 , we have
πλ1 λ2
( v ) = sgn(π0
λ2 ( v ) ) ⊙ max(|π0
λ2 ( v)| − λ1 , 0 ) .
( 14 )
Proof : We first analyze the optimality condition for the solution π0 λ2 ( v ) . According to ( 12 ) and ( 13 ) , there exists z∗ ∈ λ2SGN(Rπ0
λ2(v ) )
( 15 ) such that π0
λ2 ( v ) = v − RTz∗ . Let x = sgn(π0
λ2 ( v ) ) ⊙ max(|π0 λ2 ( v ) ) ⊙ min(|π0 g = sgn(π0
λ2 ( v)| − λ1 , 0 ) ,
λ2 ( v)| , λ1 ) .
We can easily verify x − v + g + RTz∗ = 0 and g ∈ λ1SGN(x ) . Utilizing the definition of x , the special structure of R that each of its row has two nonzero elements −1 and 1 , and ( 15 ) , we can verify z∗ ∈ λ2SGN(Rx ) . Therefore ,
0 = x − v + g + RTz∗ ∈ ∂f λ1
λ2
( x ) .
It follows from the optimality condition ( 12 ) that ( 14 ) holds .
' Theorem 1 implies that , it suffices to solve ( 9 ) with λ1 = 0 . For discussion convenience , we omit the superscript to indicate that λ1 = 0 . The proof of Theorem 1 implies that , πλ2 ( v ) can be analytically solved as πλ2 ( v ) = v − RTz∗ , provided that an appropriate z∗ ∈ λ2SGN(Rπλ2 ( v ) ) can be found . Interestingly , z∗ is unique , as shown in the following analysis . It follows from πλ2 ( v ) = v − RTz∗ that z∗ is a solution to the linear system RRTz = Rv − Rπλ2 ( v ) . Since RRT is positive definite ( see the discussion in Section 3.1 ) and πλ2 ( v ) is unique , we conclude that z∗ is unique . The above discussion motivates us to solve ( 9 ) via finding the appropriate and unique z∗ .
In the next section , we shall show that z∗ can be efficiently computed by a special quadratic programming problem with the bound constraint . As RTz∗ is a subgradient of the fused penalty λ2kRxk1 at the minimizer , we term our proposed method as the Subgradient Finding Algorithm ( SFA ) . Note that , SFA is our main technical contribution in this paper .
πλ1 λ2
( v ) = arg min x f λ1 λ2
( x ) .
We can easily verify that arg min y hLi,si ( y ) = πλ1/Li λ2/Li
( si − loss′(si)/Li ) .
Thus the building block ( 7 ) in Algorithm 1 can be solved by FLSA in ( 9 ) . In the sequel , we present its efficient computation .
The objective function f λ1 λ2
( · ) is strictly convex , thus it admits a unique minimizer , denoted by x∗ . The optimality condition [ 21 , Theorem 3.15 , Chapter 3 ] requires that
0 ∈ ∂f λ1
λ2
( x∗ ) ,
( 12 )
( 10 )
( 11 )
3 . THE SUBGRADIENT FINDING
ALGORITHM
In this section , we discuss solving ( 9 ) with λ1 = 0 , ie , min x∈Rn fλ2 ( x ) ≡
1 2kx − vk2 + λ2kRxk1 .
( 16 )
Introducing the dual variable z ∈ Rn−1 , we can reformulate ( 16 ) as the following equivalent min max problem : min x∈Rn max kzk∞≤λ2
φ(x , z ) ≡
1 2kx − vk2 + hRx , zi .
( 17 )
This is a saddle point problem , and the existence of the saddle point is ensured by the well known Von Neumann Lemma [ 20 ] , as φ(x , z ) is differentiable , convex in x , and concave in z .
Exchanging min and max and setting the derivative of φ(x , z ) with regard to x to zero , we have x = v − RTz .
( 18 )
Plugging ( 18 ) into ( 17 ) , we obtain the following optimization problem with regard to z : min kzk∞≤λ2
ψ(z ) ≡ −φ(v − RTz , z ) =
1 2kRTzk2 − hRTz , vi . ( 19 ) It follows from ( 18 ) that , once z∗ , the minimizer of ( 19 ) , is found , we can analytically obtain πλ2 ( v ) ; and this coincides with the relationship πλ2 ( v ) = v − RTz∗ shown in the proof of Theorem 1 . In the sequel , we discuss the efficient optimization of the bound constrained quadratic programming problem ( 19 ) . To this end , we exploit the special “ structures" of ( 16 ) , ( 17 ) and ( 19 ) , and develop a novel restart technique for the fast convergence .
The rest of this section is organized as follows : we present the optimality condition for ( 19 ) in Section 3.1 , derive the maximal value of λ2 in Section 3.2 , present the proposed SFA in Section 3.3 , compute the duality gap of the solution in Section 3.4 , develop a restart technique for accelerating the convergence in Section 3.5 , and provide further discussions in Section 36 3.1 The Optimality Condition The Hessian of ψ(· ) can be computed as
RRT =
0
BBBBB@
2 −1 −1
2 −1
−1
2
.
0
0
. . −1
−1
2
1
CCCCCA
,
( 20 ) which is an ( n − 1 ) × ( n − 1 ) tridiagonal matrix . The n − 1 eigenvalues of the Hessian RRT can be analytically computed as 2 − 2 cos(iπ/n ) , i = 1 , 2 , . . . , n − 1 . Therefore , the Hessian is positive definite , and the minimizer of ( 19 ) is unique . According to [ 21 , Theorem 225 , Chapter 2 ] , we have that z∗ , satisfying kz∗k∞ ≤ λ2 , is a minimizer of ( 19 ) if and only if hz − z∗ , ψ′(z∗)i ≥ 0,∀z : kzk∞ ≤ λ2 .
( 21 )
The optimality condition leads to an important relationship between the minimizer and its gradient , as summarized in the following lemma ( this lemma shall help derive the restart technique to be discussed in Section 3.5 ) :
LEMMA 1 . Let g∗ = ψ′(z∗ ) . We have : 1 ) if g∗ z∗ i = −λ2 ; 2 ) if g∗ i = λ2 ; and 3 ) if |z∗ g∗ i = 0 . 3.2 Computing the Maximal Value for λ2 i < 0 , then z∗ i > 0 , then i | < λ2 , then
When λ2 → ∞ , the problem ( 19 ) becomes an unconstrained optimization problem . Intuitively , there exists a λmax ( the maximal value for λ2 ) , over which the problem ( 19 ) has the same solution . The following theorem shows how to compute λmax
.
2
2
THEOREM 2 . The linear system
RRTz = Rv has a unique solution , denoted by ˆz . Let λmax 2 = kˆzk∞ .
( 22 )
( 23 )
For any λ2 ≥ λmax ( 16 ) can be analytically computed as :
2
, the solution of ( 19 ) is ˆz , and the solution of
πλ2 ( v ) = he , vie/n .
( 24 )
2
2
2
.
When λ2 ≥ λmax
, we can easily verify that kˆzk∞ = λmax
Proof : As RRT , the Hessian of ψ(· ) , is positive definite , the linear system ( 22 ) has a unique solution , denoted by ˆz . For any λ2 ≥ 2 ≤ λ2 and ψ′(ˆz ) = λmax RRTˆz − Rv = 0 . It follows from the optimality condition ( 21 ) that ˆz is the optimal solution of ( 19 ) for any λ2 ≥ λmax , we have πλ2 ( v ) = v − RTˆz from ( 18 ) . It follows that 1 ) Rπλ2 ( v ) = R(v − RTˆz ) = −ψ′(ˆz ) = 0 , and 2 ) eTπλ2 ( v ) = eTv − eTRTˆz = eTv , where the last equality holds as Re = 0 . Thus ( 24 ) holds for any λ2 ≥ λmax ' The linear system ( 22 ) can be efficiently computed in O(n ) time by using the special tridiagonal structure of the matrix RRT . One well known algorithm for solving the general tridiagonal systems of equations is the Thomas algorithm [ 29 ] , which consumes approximately 3n additions and 5n multiplications . In addition , when considering the special form of RRT in ( 20 ) , we can apply the Rose algorithm [ 27 ] , which costs only n multiplications and 3n − 5 additions , as can be easily observed from Algorithm 2 . Evans [ 7 ] presented an algorithm similar to the Rose algorithm , and proved that its rounding error is bounded .
.
2
Algorithm 2 The Rose Algorithm Input : u ∈ R(n−1)×1 Output : ˆz ∈ R(n−1)×1 satisfying RRTˆz = u 1 : Compute the scalar s = −n−1 Pn−1 j=1 j × uj 2 : Compute ˆzj sequentially using ˆzn−1 = un−1 + s and ˆzj =
ˆzj+1 + uj , j = n − 2 , . . . , 1
3 : Obtain ˆzj sequentially using ˆzj = ˆzj + ˆzj−1 , j = 2 , . . . , n− 1
To solve ( 19 ) , we can first compute ˆz , the solution to the linear If λ2 ≥ λmax system ( 22 ) by Algorithm 2 , and obtain λmax , ˆz is the solution of ( 19 ) ; otherwise , we apply the algorithm to be discussed in the subsequent subsections for 0 ≤ λ2 < λmax . We note that , Algorithm 2 shall also be used in the restart technique to be discussed in Section 35
.
2
2
2
3.3 SFA Via Gradient Descent
In the literature , there have been quite a few algorithms for solving the bound constrained optimization problem like ( 19 ) ; eg , [ 4 , 5 , 15 ] and the references therein . However , these algorithms are either for the general quadratic programming or the general optimization .
In this paper , we propose to apply the gradient descent [ 21 ] , and present the algorithm in Algorithm 3 . According to [ 21 , Chapter 224 ] , Algorithm 3 converges linearly as kzk − z∗k2 ≤ ( 1 −
µ L
)kkz0 − z∗k2 ,
( 25 ) where L = 2−2 cos(π(n−1)/n ) and µ = 2−2 cos(π/n ) are the largest and the smallest eigenvalues of the Hessian RRT , respectively . Algorithm 3 can be further accelerated with the Nesterov ’s method ; and we denote the resulting algorithm as SFAN .
Our proposed SFAG can be significantly accelerated with the restart technique ( to be discussed in Section 3.5 ) , by exploiting the special “ structures" of the original and the reformulated problems . Before presenting the restart technique , we show in the next subsection how to compute the duality gap for checking the convergence of the algorithm .
2
Algorithm 3 SFA via Gradient Descent ( SFAG ) Input : v ∈ Rn×1 , 0 ≤ λ2 < λmax Output : zk ∈ R(n−1)×1 1 : Set L = 2 − 2 cos(π(n − 1)/n ) 2 : for i = 1 to k do 3 : 4 : 5 : end for
Compute gi = ψ′(zi ) = RRTzi − Rv Set zi+1 = Pλ2 ( zi − gi/L )
, z0 ∈ R(n−1)×1 , k
3.4 Computing the Duality Gap
In optimizing ( 19 ) via SFAG ( see Algorithm 3 ) , SFAN ( the accelerated version of SFAG via the Nesterov ’s method ) , and SFAR ( SFA with the restart technique to be discussed in the next subsection ) , it would be desirable that we can check the convergence of the algorithm based on the “ goodness" of the approximate solution . To this end , we propose to compute the duality gap for the min max optimization problem ( 17 ) , as both ( 16 ) and ( 19 ) are its resulting problems by eliminating the variable z or x .
Let ˜z be an appropriate solution computed by SFAG ( or SFAN and SFAR ) . Note that , we have k˜zk∞ ≤ λ2 from Step 4 of Algorithm 3 . Let ˜x = v − RT˜z be the appropriate solution computed by ( 18 ) . We can define the duality gap for ( 17 ) at ( ˜x , ˜z ) as : gap(˜x , ˜z ) = max z:kzk∞≤λ2
φ(˜x , z ) − min x
φ(x , ˜z ) .
( 26 )
The following theorem shows that , gap(˜x , ˜z ) can be computed using ˜z only , and it measures the “ goodness" of the solutions ˜z and ˜x for ψ(· ) and fλ2 ( · ) , respectively .
THEOREM 3 . The duality gap in ( 26 ) can be computed as : gap(˜x , ˜z ) = λ2kψ′(˜z)k1 + h˜z , ψ′(˜z)i .
In addition , we have
ψ(˜z ) − ψ(z∗ ) ≤ gap(˜x , ˜z ) , fλ2 ( ˜x ) − fλ2 ( x∗ ) ≤ gap(˜x , ˜z ) .
( 27 )
( 28 )
( 29 )
Proof : From ( 16 19 ) , we can establish the following relationships :
−ψ(˜z ) = φ(˜x , ˜z ) = min x
φ(x , ˜z ) ≤ φ(x∗ , ˜z ) ,
φ(x∗ , ˜z ) ≤ max z:kzk∞≤λ2
φ(x∗ , z ) = φ(x∗ , z∗ ) = −ψ(z∗ ) , fλ2 ( x∗ ) = φ(x∗ , z∗ ) = min x
φ(x , z∗ ) ≤ φ(˜x , z∗ ) ,
φ(˜x , z∗ ) ≤ max z:kzk∞≤λ2
φ(˜x , z ) = fλ2 ( ˜x ) .
From ( 26 33 ) , we can write the duality gap as :
( 30 )
( 31 )
( 32 )
( 33 ) gap(˜x , ˜z ) = fλ2 ( ˜x ) − φ(˜x , ˜z ) = λ2kR˜xk1 − h˜z , R˜xi
= λ2kψ′(˜z)k1 + h˜z , ψ′(˜z)i ,
( 34 ) where the last equality follows from ψ′(˜z ) = RRT˜z − Rv and ˜x = v − R˜z . The inequalities ( 28 ) and ( 29 ) can be easily verified using ( 26 33 ) . This completes the proof . 3.5 SFA via the Restart Technique
'
Although Algorithm 3 has a linear convergence rate , it may converge slowly for large n , due to the high condition number L µ . For example , when n = 102 , 103 , 104 and 105 , we have L µ ≈
4 × 103 , 4 × 105 , 4 × 107 and 4 × 109 , respectively . In this subsection , we propose to make use of the special structures of ( 16 ) , ( 17 ) , and ( 19 ) to restart SFAG for fast convergence ; and we call the resulting method as SFAR ( SFA via the restart technique ) . Figure 1 illustrates SFAG , SFAN and SFAR for solving ( 19 ) . From this figure , we can clearly observe that , the restart technique can significantly accelerate the convergence and yield the exact solution using much fewer iterations than SFAG and SFAN .
Our proposed restart technique is based on the so called support set3 , motivated by Lemma 1 . Specifically , for any kzk∞ ≤ λ2 , we define its support set as : S(z ) = {i ∈ [ 1 : n− 1 ] : |zi| = λ2 , zigi < 0 , g = ψ′(z)} . ( 35 ) When S is nonempty , we denote the j th largest element in the set S by sj , j = 1 , 2 , . . . , |S| . It is clear that 1 ≤ s1 and s|S| ≤ n− 1 . For discussion convenience , we let s0 = 0 and s|S|+1 = n . We note that the following discussion also holds for the case when S is empty . With s0 , s1 , . . . , s|S|+1 , we can partition the indices in [ 1 : n ] into |S| + 1 non overlapping groups :
Gj = {i : sj−1 + 1 ≤ i ≤ sj} , 1 ≤ j ≤ |S| + 1 .
( 36 )
Let eGj and vGj denote the j th group of e and v corresponding to the indices in Gj , respectively .
Based on the support set S , we define the mapping x = ω(z ) as : xi = heGj , vGji − zsj−1 + zsj
|Gj|
, i ∈ Gj ,
( 37 ) where j = 1 , 2 , . . . , |S| + 1 and we have assumed z0 = zn = 0 for presentation convenience .
LEMMA 2 . For any kzk∞ ≤ λ2 and i ∈ S(z ) , we have : 1 ) if zi = λ2 , then vi+1 > vi ; and 2 ) if zi = −λ2 , then vi+1 < vi . Proof Let g = ψ′(z ) . For discussion convenience , we add z0 = zn = 0 into the ( n − 1) dimensional vector z . We have gi = −zi−1 + 2zi− zi+1− ( vi+1− vi ) . If follows from the definition of S(z ) in ( 35 ) that , 1 ) if zi = λ2 , then gi = −zi−1 + 2λ2 − zi+1 − ( vi+1 − vi ) < 0 , which leads to vi+1 > vi ; and 2 ) if zi = −λ2 , then gi = zi−1 − 2λ2 − zi+1 − ( vi+1 − vi ) > 0 , which leads to vi+1 < vi . Lemma 2 shows that , for any i ∈ S(z ) , the sign of zi is indeed determined by vi+1 − vi . Next , we show that the optimal solution to ( 16 ) can be exactly recovered using the support set S(z∗ ) only .
'
THEOREM 4 . Let z∗ be the minimizer of ( 19 ) . Then x∗ , the minimizer of ( 16 ) , satisfies x∗ = ω(z∗ ) .
( 38 ) i = x∗
Proof Let g∗ = ψ′(z∗ ) . It follows from Lemma 1 and the definition of S in ( 35 ) that , g∗ i = 0 , ∀i /∈ S . Based on the relationship Rx∗ = −ψ′(z∗ ) = −g∗ , we have x∗ i′ ,∀i , i′ ∈ Gj . Let the matrix R be partitioned into |S| + 1 non overlapping blocks as R = [ RG1 , RG2 , . . . , RG|S|+1 ] . From x∗ = v − RTz∗ , we have ( 39 )
Gj z∗,∀j . We can easily get ( 38 ) by left multiplying eT ( 39 ) , using ( 37 ) , and incorporating the fact that x∗ i , i′ ∈ Gj . This completes the proof . 3We call S(z ) the support set due to the following two reasons : 1 ) as shall be shown in Theorem 4 , S(z∗ ) supports the exact recovery of x∗ for ( 16 ) , and 2 ) S(z ) directly induces the mapping ω : z → x in ( 37 ) .
Gj to both sides of i′ for all '
Gj = vGj − RT x∗ i = x∗
)
1 + p a g
(
0 1 g o l
: p a g y t i l a u D
4
3.5
3
2.5
2
1.5
1
0.5
0
0 n=105 , λ =0.5 2
SFA G SFA N SFA R
)
1 + p a g
(
0 1 g o l
: p a g y t i l a u D
50
100
Iteration i
150
200
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0
0 n=105 , λ =1 2
SFA G SFA N SFA R
50
100
Iteration i
150
200
Figure 1 : Illustration of SFAG , SFAN , and SFAR for solving ( 19 ) ( see the experimental setting in Section 41 ) SFAG , SFAN and SFAR denote the proposed SFA via the gradient descent , the Nesterov ’s method and the restart technique , respectively . The duality gaps reported in this figure are in the logarithmic scale . In 200 iterations , SFAG ( SFAN ) achieves the duality gaps 0.01 ( 0.001 ) , 4.02 ( 0.22 ) for λ2 = 0.5 and 1 , respectively . SFAR achieves the exact solution in 8 and 9 iterations for λ2 = 0.5 and 1 , respectively .
Theorem 4 offers an alternative way for computing x∗ from z∗ , which is quite different from ( 18 ) . More specifically , ( 18 ) requires that all the entries in z∗ are known ; while ( 38 ) says that , x∗ can be exactly computed , if the support set S(z∗ ) is known ( we have used Lemma 2 ) . In other words , if we have an appropriate solution ˜z 6= z∗ , we can still exactly obtain x∗ = ω(˜z ) , provided that S(˜z ) = S(z∗ ) . Intuitively , this shows that , for a given appropriate solution ˜z 6= z∗ , x = ω(˜z ) can be a much better approximate solution than ˜x = v − RT˜z for optimizing fλ2 ( · ) , provided that S(˜z ) is close to S(z∗ ) . In our proposed restart technique , with x = ω(˜z ) , we compute a restart point z0 using the relationship x = v − RTz0 . Here , z0 can be easily computed by solving the linear system RRTz0 = Rv−Rx . We present the proposed restart technique in Algorithm 4 , where Step 4 ensures that z0 is feasible for ( 19 ) .
Algorithm 4 The Restart Technique Input : Rv ∈ R(n−1)×1 , 0 ≤ λ2 < λmax Output : Restart point z0 ∈ R(n−1)×1 1 : Compute the support set S(˜z ) according to ( 35 ) 2 : Compute x = ω(˜z ) according to ( 37 ) 3 : Calculate z0 as the solution to RRTz0 = Rv − Rx 4 : Set z0 = Pλ2 ( z0 )
, ˜z ∈ R(n−1)×1
2
We illustrate the proposed Subgradient Finding Algorithm via the restart technique ( SFAR ) in Figure 2 , from which we can see that , SFAR recursively calls SFAG ( Algorithm 3 ) and the restart technique ( Algorithm 4 ) . For the SFAG block in the proposed SFAR , we set the number of iteration(s ) k = 1 , as it yields the best performance in our experiments . It is clear that the per iteration cost of SFAR is O(n ) . 3.6 Discussion
We summarize our methodology for solving ( 16 ) as follows . We first make use of the dual of the ℓ1 norm to rewrite the primal problem ( 16 ) as the equivalent saddle point problem ( 17 ) . By using the relationship between the primal and dual variables in ( 18 ) , we obtain the dual problem ( 19 ) , which is a bound constrained quadratic programming problem and can be solved in linear time by the firstorder methods such as gradient descent . To further accelerate the optimization of ( 19 ) , we propose a restart technique . The underly z0
SFAG
( Algorithm 3 ) z0
Linear System ( Algorithm 2 ) z x
Restart
( Algorithm 4 ) gap(z)(cid:148)tol ?
Y z is the solution
N
Support Set S(z ) x=(cid:550)(z )
Figure 2 : The flow chart of the proposed SFAR ( Subgradient Finding Algorithm via the restart technique ) . SFAR recursively calls SFAG ( Algorithm 3 ) and the restart technique ( Algorithm 4 ) until the duality gap is within a pre specified precision parameter tol ( set to 10−12 in our experiments ) . ing motivation is that , at a given appropriate solution ˜z , if we can obtain a better appropriate point z0 , the optimization can be greatly accelerated with the restart of z0 . For the problem discussed in this paper , z0 is obtained by exploiting the “ structures" of the primal problem ( 16 ) , the saddle point problem ( 17 ) , and the dual problem ( 19 ) . Such a restart technique can potentially be used for other problems , when utilizing the problem “ structures" in a nice way .
Next , we compare our proposed SFA with the modified CD ( mCD ) proposed in [ 8 ] and the path algorithm ( pathFLSA ) proposed in [ 12 ] . First , both mCD and pathFLSA focus on solving the original formulation ( 16 ) ; while our proposed SFA focuses on solving the dual problem ( 19 ) utilizing the special structures of ( 16 ) , ( 17 ) and ( 19 ) . Second , in solving πλ2 ( v ) , both mCD and pathFLSA need to start from λ = 0 and then increase λ according to certain strategies until λ = λ2 to obtain the path solutions ; while our proposed SFA directly solves ( 19 ) corresponding to the given λ2 . We note that , for EFLA in Algorithm 1 , we need the efficient computation of πλ2 ( v ) corresponding to a given λ2 rather than the path solutions ; and this is also the case for the the online learning algorithms ( eg , FOLOS [ 6 ] and RDA [ 35 ] ) for solving ( 1 ) . Third , the starting point of our proposed SFA is quite flexible , thus it can benefit from the “ warm" start technique , when used as a building block in EFLA ( note that , the solution of FLSA in the previous EFLA iteration can potentially be close to that of FLSA in the next iteration ; and Figure 4 illustrates the benefit of the “ warm" start ) ; while in solving
πλ2 ( v ) , both mCD and pathFLSA need to exactly follow the solution path thus they cannot benefit from the “ warm" start technique ( note that the solution of FLSA in the previous EFLA iteration is not necessarily on the path of the next one ) .
4 . EXPERIMENT
We first demonstrate the efficiency of the proposed SFA for solving FLSA in Section 4.1 , and then the proposed EFLA in Section 42 All experiments were carried out on an Intel(R)Core(TM)2 Duo CPU ( E6850 ) 2.99GHZ processor . The source codes , included in the SLEP package [ 18 ] , are available online4 .
4.1 Performance of the Proposed SFA Illustration of the Proposed SFA We generate a vector v ∈ Rn with n = 105 . The entries in v are randomly drawn from the standard normal distribution . By applying Theorem 2 , we get λmax 2 = 300.2 with Algorithm 2 . All the algorithms start from the origin .
We first compare SFAR with SFAG and SFAN , and present the results in Figure 1 . For SFAG ( Algorithm 3 ) and its accelerated version via the Nesterov ’s method—SFAN , we run them for 200 iterations ; and for SFAR ( depicted in Figure 2 ) , we terminate the algorithm until the duality gap is zero . From Figure 1 , we can observe that : 1 ) SFAN converges faster than SFAG ; 2 ) the duality gaps of SFAG and SFAN are not very small after 200 iterations , especially for large λ2 ; and 3 ) SFAR achieves the exact solution within 8 and 9 iterations for λ2 = 0.5 and 1 , respectively , and thus significantly outperforms both SFAG and SFAN . We attribute the superior performance of SFAR to the restart technique using the special structures of ( 16 ) , ( 17 ) and ( 19 ) .
Next , we further explore the performance of SFAR under different values of λ2=0.1 , 0.5 , 1 , 2 , 3 , 5 , 10 , 20 , 200 . In Figure 3 , we report the duality gap and the number of elements in the support set during the iterations . We can observe from this figure that : 1 ) SFAR converges within dozens of iterations , and 2 ) the number of elements in the support set decreases with an increasing λ2 . Comparison with the Other Algorithms Before comparing the proposed SFA with the other algorithms , we first discuss the efficiency of the existing solvers for FLSA . Friedman et al . [ 8 ] showed that their proposed modified CD ( mCD ) outperforms the general solver SQOPT by factors of 50 up to 300 or more . Höefling [ 12 ] showed that his pathFLSA5 is over 100 times faster than the general solver CVX . In addition , pathFLSA is significantly faster than mCD for problems with size up to 106 . Therefore , in this paper , we compare our proposed SFAR with pathFLSA .
, where λmax
We try problems of sizes n = 102 , 103 , 104 , 105 , 106 and 107 . For each n , we generate 100 input vectors v ∈ Rn , whose entries are randomly drawn from the standard normal distribution , and set λ2 = r × λmax is computed according to Theorem 2 , and r = 10−3 , 10−2 , 10−1 and 1 . For our proposed SFAR , we set the origin as the starting point . We report the average results over 100 runs in Table 1 , from which we can observe that , 1 ) our proposed SFAR is much more efficient than pathFLSA for solving FLSA corresponding to a given parameter λ2 , 2 ) our proposed algorithm converges within dozens of iterations even for problems of size 107 , and 3 ) with an increasing value of λ2 = r × λmax , both the number of iterations and the computational time for SFAR increase , as the starting point ( the origin ) is much farther away from the solution for the larger λ2 than that of the small ones .
2
2
2
We would like to emphasize the following two points . First ,
4wwwpublicasuedu/~jye02/Software/SLEP 5cranr projectorg/web/packages/flsa n=105 , duality gap
5
10 Iteration i
15
20 n=105 , number of elements in the support set
λ =0.1 2 λ =0.5 2 λ =1 2 λ =2 2 λ =3 2 λ =5 2 λ =10 2 λ =20 2 λ =200 2
λ =0.1 2 λ =0.5 2 λ =1 2 λ =2 2 λ =3 2 λ =5 2 λ =10 2 λ =20 2 λ =200 2
)
1 + p a g
(
0 1 g o l
: p a g y t i l a u D
7
6
5
4
3
2
1
0
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
)
1 +
|
S
|
(
0 1 g o l
: t e s t r o p p u s e h t n i s t n e m e e l f o r e b m u N
0
5
10 Iteration i
15
20
Figure 3 : Illustration of SFAR for solving ( 19 ) under different values of λ2 = 0.1 , 0.5 , 1 , 2 , 3 , 5 , 10 , 20 , 200 in terms of the duality gap and the number of elements in the support set ( shown in the logarithmic scale ) . In this experiment , λmax = 3002 The number of iterations consumed by SFAR are 5 , 8 , 9 , 12 , 13 , 16 , 19 , 22 and 13 , respectively ; and the number of elements in the support set of the minimizer are 88882 , 51507 , 26970 , 10986 , 5857 , 2492 , 731 , 166 and 1 , respectively .
2 when the objective is to compute the path solutions for FLSA , pathFLSA should be a better choice than SFAR , as it is specialized for the path solutions ; however , when used as a building block in EFLA , we only need to solve FLSA corresponding to a given λ2 , and thus SFAR is a better choice . Second , when used as the building block in EFLA , SFAR can achieve much better practical performance than what reported in Table 1 , as we can apply the “ warm" start technique , ie , using the solution obtained from the previous EFLA iteration as the “ warm" start for the latter ; and we have observed in our experiments that , the average SFAR iterations is usually within 10 ( see the following experiments ) . However , we note that , neither mCD nor pathFLSA can benefit from the “ warm" start technique ( see the discussion in Section 36 )
SFAR as A Building Block for EFLA with the “ Warm" Start To evaluate the efficiency of SFAR in EFLA , we apply it for solving the following fused Lasso problem : min x
1 2kAx − bk2 + λ1kxk1 + λ2kRxk1 .
( 40 )
Here , A ∈ Rm×n is a random matrix whose entries are drawn from the normal distribution with zero mean and unit variance , b = A˜x + ǫ is the response , ˜x ∈ Rn×1 is a vector whose en
Table 1 : Comparison of SFAR and pathFLSA under different values of n and λ2 . We set λ2 = r × λmax , where r is chosen from {10−3 , 10−2 , 10−1 , 1} . “ SFAR Iteration" and “ |S|" denote the number of iterations consumed by SFAR , and the number of elements in the support set of the minimizer , respectively . “ SFAR Time" and “ pathFLSA Time" denote the computation time ( in seconds ) consumed by SFAR and pathFLSA , respectively . The reported results are averaged over 100 simulations . “ " denotes that , pathFLSA does not run successfully in our study ; note that , the computational time reported in [ 12 ] for the same problem size is 108 seconds .
2 n
SFAR Time
SFAR Time
SFAR Iteration ( |S| ) pathFLSA Time SFAR Iteration ( |S| ) pathFLSA Time SFAR Iteration ( |S| ) pathFLSA Time SFAR Iteration ( |S| ) pathFLSA Time
SFAR Time
SFAR Time
10−3
10−2
10−1
1
102 1 ( 98 ) 2.9×10−5 5.9×10−4 2 ( 90 ) 3.0×10−5 6.1×10−4 5 ( 34 ) 4.1×10−5 6.4×10−4 1.7×10−5 4.8×10−4
0 ( 0 )
103
2 ( 969 ) 1.6×10−4 6.1×10−3 4 ( 714 ) 2.6×10−4 6.1×10−3 9 ( 85 ) 3.5×10−4 6.1×10−3 4.8×10−5 6.1×10−3
0 ( 0 )
104
3 ( 9025 ) 2.1×10−3 6.2×10−2 9 ( 3363 ) 4.2×10−3 6.1×10−2 16 ( 113 ) 5.1×10−3 6.1×10−2 3.5×10−4 6.2×10−2
0 ( 0 )
0.76
7 ( 70058 ) 4.8×10−2 14 ( 7394 ) 5.9×10−2 24 ( 119 ) 8.7×10−2
0.76
0.76 0 ( 0 )
1.5 9.7 0 ( 0 )
20
0 ( 0 ) 0.53
5.0×10−3
0.77
5.4×10−2
9.7
105
106
107
10 ( 324617 )
15 ( 782902 )
0.72 9.6
9.2
20 ( 10789 )
29 ( 13092 )
1.1 9.6
14
32 ( 127 )
42 ( 139 )
Table 2 : Computational time ( seconds ) and average number of iterations when applying SFAR as a building block for EFLA with the “ warm" start . n
SFAR average iterations
EFLA time SFAR time pathFLSA time
103 1.4 0.35 0.13 6.1
104 2.6 5.8 1.5 61
105 3.5 67 21 760 tries are drawn from the normal distribution with zero mean and unit variance , and ǫ is the noise vector whose entries are drawn from the normal distribution with zero mean and variance equal to 001 We set m = 100 , λ1 = λ2 = 0.01 , and try different values of n = 103 , 104 and 105 .
We run EFLA for 1,000 iterations , and report the results in Table 2 . We can observe from the second row of this table that , the average number of iterations consumed by SFAR is very small ( within 10 ) . In Figure 4 , we further report the number of SFAR iterations with increasing EFLA iterations . We observe a similar trend . We attribute the small number of iterations to the usage of the “ warm" start , ie , to solve SFAR , we use the solution z in the previous EFLA iteration as the “ warm" start for the successive one .
In rows 3 and 4 of Table 2 , we report the the computational time consumed by EFLA and SFAR , from which we can observe that SFAR consumes about 1/3 of the total computational time for the above experiments . If we apply pathFLSA [ 12 ] for fulfilling the same task as SFAR , it consumes much more computational time , as shown in the last row of Table 2 ; and this shall make the EFLA much slower than that with SFAR . From the results in Tables 1 and 2 , we can conclude that our proposed SFAR is much more efficient than pathFLSA for solving FLSA ( which acts as a building block in the proposed EFLA ) corresponding to a given parameter , especially when the “ warm" start technique is applied . 4.2 Performance of the Proposed EFLA
We apply the proposed EFLA to several real world applications , with the least squares loss . Specifically , we solve the fused Lasso ( 40 ) , where each row of A denotes a sample , and each row of b contains the corresponding class label information .
R s n o i t a r e t i
A F S f o r e b m u N
6
5.5
5
4.5
4
3.5
3
2.5
2
1.5
1 0 n=104 n=105
7
6.5
6
5.5
5
4.5
4
3.5
3
2.5
R s n o i t a r e t i
A F S f o r e b m u N
200
400 600 EFLA Iteration
800
1000
2 0
200
400 600 EFLA Iteration
800
1000
Figure 4 : Number of SFAR iterations when used as a building block for EFLA with the “ warm" start .
Data Description We conduct experiments on the following three data sets : ArrayCGH [ 28 ] , Prostate Cancer [ 24 ] , and Leukemias [ 9 ] . The ArrayCGH ( Array comparative genomic hybridization ) [ 28 ] is a micro array based technology that can detect genomic copy number variation ( CNV ) at different locations along the genome . Each measurement ( feature ) is the log ratio of CNV , and adjacent features correspond to adjacent locations along the genome . This data set contains ArrayCGH profiles of 57 bladder tumor samples , and each profile contains 2,385 measurements . Here we consider the tumor grade classification problem , with 12 samples of Grade 1 and 45 samples of higher grades ( 2 or 3 ) .
The Prostate Cancer [ 24 ] data set used in our experiments is based on the protein mass spectrometry , where the features are indexed by many time of flight values . Time of flight is related to the mass over charge ratio m/z of the constituent proteins in the blood . The data set contains 15,154 measurements of 132 patients , including 63 healthy and 69 with prostate cancer .
The Leukemias [ 3 , 9 ] is a DNA microarray data set . It contains 7,129 genes and 72 samples : 47 of acute lymphocytic leukaemia and 25 of acute myelogenous leukaemia . Unlike the ArrayCGH and Prostate Cancer data sets , the features in Leukemias have no prespecified order [ 30 ] . We follow [ 30 ] to reorder the features in the data , using the binary hierarchical clustering ; and we call the resulting data set as “ Leukemias Reordered" .
EFLA warm start
EFLA Prostate
CVX
EFLA arrayCGH
103
102
101
) c e s ( e m i t
U P C
EFLA warm start
CVX
104
) c e s ( e m i t
U P C
103
102
101
100
1.0e−01
3.2e−03
λ 2
1.0e−04
100
1.0e−01
1.0e−03
λ 2
1.0e−05
CVX
EFLA Leu r
EFLA warm start
103
102
101
) c e s ( e m i t
U P C
CVX
EFLA
EFLA warm start
Leu
103
) c e s ( e m i t
U P C
102
101
100
1.0e+00
3.2e−02
λ 2
1.0e−03
100
1.0e+00
3.2e−02
λ 2
1.0e−03
Figure 5 : Computational time ( seconds ) on ArrayCGH ( top left ) , Prostate Cancer ( top right ) , Leukemias ( bottom left ) , and Leukemias Reordered ( bottom right).The x axis denotes the different values of λ2 , while the y axis represents the total computational time ( seconds ) corresponding the given λ2 . For illustration convenience , the y axis is plotted in the logarithmic scale .
Computational Efficiency of EFLA We compare our proposed EFLA with the CVX optimization package [ 10 ] , for solving the fused Lasso ( 40 ) . In the comparison , we terminate EFLA till it achieves an objective function value less than or equal to that of CVX . The parameters λ1 and λ2 are specified by a 9 × 9 grid sampled using the logarithmic scale from the parameter space . We report the computational time ( seconds ) in Figure 5 , where the x axis denotes the different values of λ2 , and the y axis represents the total computational time ( seconds ) corresponding to the given λ2 . For a given λ2 , we can solve ( 40 ) by applying the solution corresponding to the large λ1 as the “ warm" start to the smaller one ; and this is the so called “ warm" start technique widely employed in the literature [ 8 , 16 ] . From Figure 5 , we can observe that EFLA is one or two orders of magnitude faster than the standard solver CVX , especially for larger λ2 . In addition , the “ warm" start helps improve the efficiency . The superior efficiency of EFLA attributes to the following reasons : 1 ) EFLA directly solves the composite function ( 40 ) utilizing the composite structure , while CVX is a general solver optimizing the smooth reformulation of ( 40 ) by introducing many additional variables and constraints ; 2 ) EFLA enjoys the optimal convergence rate for the first order black box methods ; 3 ) the SFAR developed in Section 3 can efficiently solve FLSA , the key building block of the proposed EFLA ( see Tables 1 & 2 ) .
Classification Performance We follow [ 25 ] to report the leaveone out performance of the fused Lasso ( via EFLA ) . The parameters λ1 and λ2 are specified by a 9 × 9 grid sampled using the logarithmic scale from the parameter space . The classification er
Table 3 : The best leave one out accuracy ( % ) on different data sets by Fused Lasso and Lasso .
Array CGH
Prostate Cancer
Leukemias
Leukemias Reordered
Fused Lasso Lasso 82 % 98 % 94 % 94 %
88 % 98 % 96 % 97 % rors corresponding to different parameter values are visualized by a heat map in Figure 6 . We also report in Table 3 the best leaveone out accuracy ( % ) on different data sets by Fused Lasso and Lasso ; and we can observe that Fused Lasso can achieve comparable or better classification performance than Lasso , benefited by the additional fused penalty kRxk1 . We refer the readers to [ 30 ] for detailed comparison between Lasso and Fused Lasso , and [ 25 , 31 ] for the biological interpretation .
5 . CONCLUSION
In this paper , we consider solving the class of problems with the fused Lasso penalty , leading to a class of non smooth and nonseparable optimization problems . By treating its objective function as the composite function ( composed of one smooth part and the other non smooth part ) , we propose to apply the Nesterov ’s method to develop the Efficient Fused Lasso Algorithm ( EFLA ) , achieving the optimal convergence rate of O(1/k2 ) . In the proposed EFLA , a key building block in each iteration is FLSA , for which we propose an efficient Subgradient Finding Algorithm ( SFA ) , equipped with a restart technique for fast convergence . When used as a building block in EFLA , SFA is shown to converge within 10 iterations with the “ warm" start . Our empirical evaluations show that , both SFA and EFLA significantly outperform the existing solvers , thus making it applicable for large scale problems .
We plan to apply the proposed EFLA for the construction of time varying networks [ 1 ] , and other applications with either spatially or temporally ordered features . In addition , we plan to develop the online and stochastic versions of EFLA using FOLOS [ 6 ] and RDA [ 35 ] , where the proposed SFA again acts as a building block . Finally , we plan to extend our methodology to multidimensional fused Lasso , where the features form more complex graph structures , eg , the two dimensional fused Lasso [ 8 ] .
6 . ACKNOWLEDGEMENT
This work was supported by NSF IIS 0612069 , IIS 0812551 , CCF 0811790 , NGA HM1582 08 1 0016 , NSFC 60905035 , and the Office of the Director of National Intelligence ( ODNI ) , Intelligence Advanced Research Projects Activity ( IARPA ) , through the US Army .
7 . REFERENCES [ 1 ] A . Ahmed and E . P . Xing . Recovering time varying networks of dependencies in social and biological studies . Proceedings of the National Academy of Sciences , 106(29):11878–11883 , 2009 .
[ 2 ] A . Beck and M . Teboulle . A fast iterative shrinkage thresholding algorithm for linear inverse problems . SIAM Journal on Imaging Sciences , 2(1):183–202 , 2009 .
[ 3 ] C . Chang and C . Lin . Libsvm : a library for support vector machines . http://wwwcsientuedutw/ cjlin/libsvm , 2001 .
[ 4 ] Z . Dostál . Box constrained quadratic programming with proportioning and projections . SIAM Journal on Optimization , 7(3):871–887 , 1997 .
1.0e−02 arrayCGH
1.8e−03
2
λ
3.2e−04
0
0
5.6e−04
λ 1
Leu
7.5e−03
1.0e−01
1.0e−03
1.8e−04
2
λ
3.2e−05
0
0
5.6e−03
λ 1
7.5e−02
1.0e+00
20
18
16
14
12
10
8
45
40
35
30
25
20
15
10
5
1.0e−03
Prostate
1.8e−04
2
λ
3.2e−05
0
0
1.0e−04
λ 1
Leu r
3.2e−03
1.0e−01
1.0e−03
1.8e−04
2
λ
3.2e−05
0
0
5.6e−03
λ 1
7.5e−02
1.0e+00
60
50
40
30
20
10
45
40
35
30
25
20
15
10
5
Figure 6 : The Leave one out test error of the Fused Lasso ( via EFLA ) on ArrayCGH ( top left ) , Prostate Cancer ( top right ) , Leukemias ( bottom left ) , and Leukemias Reordered ( bottom right ) . For each plot , the x axis corresponds to λ1 , while the y axis corresponds to λ2 . The color indicates the number of leave one out errors , with dark blue indicating more errors and red indicating fewer errors .
[ 5 ] Z . Dostál and J . Schöberl . Minimizing quadratic functions subject to bound constraints with the rate of convergence and finite termination . Computational Optimization and Applications , 30(1):23–43 , 2005 .
[ 6 ] J . Duchi and Y . Singer . Online and batch learning using forward backward splitting . Journal of Machine Learning Research , 2009 .
[ 7 ] D . J . Evans . An algorithm for the solution of certain tridiagonal systems of linear equations . The Computer Journal , 15(4):356–359 , 1972 .
[ 8 ] J . Friedman , T . Hastie , H . Höfling , and R . Tibshirani . Pathwise coordinate optimization . Annals of Applied Statistics , 1(2):302–332 , 2007 .
[ 9 ] T . R . Golub and et al . Molecular classification of cancer : Class discovery and class prediction by gene expression monitoring . Science , 286(5439):531–537 , 1999 .
[ 10 ] M . Grant and S . Boyd . CVX : Matlab software for disciplined convex programming , 2009 .
[ 11 ] J . Hiriart Urruty and C . Lemaréchal . Convex Analysis and
Minimization Algorithms I & II . Springer Verlag , Berlin , 1993 .
[ 12 ] H . Höfling . A path algorithm for the fused lasso signal approximator . arXiv , 2009 .
[ 13 ] S . Ji and J . Ye . An accelerated gradient method for trace norm minimization . In International Conference on Machine Learning , 2009 .
[ 14 ] C . Lemaréchal and C . Sagastizábal . Practical aspects of the moreau yosida regularization I : Theoretical properties . SIAM Journal on Optimization , 7(2):367–385 , 1997 .
[ 15 ] C . J . Lin and J . J . Moré . Newton ’s method for large bound constrained optimization problems . SIAM Journal on Optimization , 9(4):1100–1127 , 1999 .
[ 16 ] J . Liu , J . Chen , and J . Ye . Large scale sparse logistic regression . In ACM SIGKDD International Conference On Knowledge Discovery and Data Mining , 2009 .
[ 17 ] J . Liu , S . Ji , and J . Ye . Multi task feature learning via efficient
ℓ2,1 norm minimization . In Uncertainty in Artificial Intelligence , 2009 .
[ 18 ] J . Liu , S . Ji , and J . Ye . SLEP : Sparse Learning with Efficient
Projections . Arizona State University , 2009 .
[ 19 ] J J Moreau . Proximité et dualité dans un espace hilbertien . Bull .
Soc . Math . France , 93:273–299 , 1965 .
[ 20 ] A . Nemirovski . Efficient methods in convex programming . Lecture
Notes , 1994 .
[ 21 ] Y . Nesterov . Introductory Lectures on Convex Optimization : A Basic
Course . Kluwer Academic Publishers , 2003 .
[ 22 ] Y . Nesterov . Gradient methods for minimizing composite objective function . CORE Discussion Paper , 2007 .
[ 23 ] Y . Nesterov . Primal dual subgradient methods for convex problems .
Mathematical Programming , 120(1):221–259 , 2009 .
[ 24 ] E . Petricoin and et al . Serum proteomic patterns for detection of prostate cancer . Journal of National Cancer Institute , 94(20):1576–1578 , 2002 .
[ 25 ] F . Rapaport , E . Barillot , and J . Vert . Classification of arrayCGH data using fused svm . Bioinformatics , 24(13):i375–i382 , 2008 .
[ 26 ] A . Rinaldo . Properties and refinements of the fused lasso . Annals of
Statistics , 37(5B):2922–2952 , 2009 .
[ 27 ] D . J . Rose . An algorithm for solving a special class of tridiagonal systems of linear equations . Communications of the ACM , 12(4):234–236 , 1969 .
[ 28 ] N . Stransky and et al . Regional copy number independent deregulation of transcription in cancer . Nature Genetics , 38(12):1386–1396 , 2006 .
[ 29 ] L . H . Thomas . Elliptic Problems in Linear Difference Equations over a Network . Waston Scientific Computing Laboratory , Columbia , NY , 1949 .
[ 30 ] R . Tibshirani , M . Saunders , S . Rosset , J . Zhu , and K . Knight .
Sparsity and smoothness via the fused lasso . Journal Of The Royal Statistical Society Series B , 67(1):91–108 , 2005 .
[ 31 ] R . Tibshirani and P . Wang . Spatial smoothing and hot spot detection for CGH data using the fused lasso . Biostatistics , 9(1):18–29 , 2008 . [ 32 ] K . Toh and S . Yun . An accelerated proximal gradient algorithm for nuclear norm regularized least squares problems . Pacific Journal of Optimization , to appear , 2009 .
[ 33 ] P . Tseng . Convergence of block coordinate descent method for nondifferentiable minimization . Journal of Optimization Theory and Applications , 109:474–494 , 2001 .
[ 34 ] P . Tseng and S . Yun . A coordinate gradient descent method for nonsmooth separable minimization . Mathematical Programming , 117(1):387–423 , 2009 .
[ 35 ] L . Xiao . Dual averaging methods for regularized stochastic learning and online optimization . In Advances in Neural Information Processing Systems , 2009 .
