Multi Label Learning by Exploiting Label Dependency
1 School of Computer Science and Technology ,
Max Planck Institute for Biological Cybernetics
2 National Key Laboratory for Novel Software kzhang@tuebingenmpgde
Min Ling Zhang 1;2
Southeast University Nanjing 210096 , China
Technology , Nanjing University
Nanjing 210093 , China zhangmlseu@gmailcom
Kun Zhang
72076 Tübingen
Germany
ABSTRACT In multi label learning , each training example is associated with a set of labels and the task is to predict the proper label set for the unseen example . Due to the tremendous ( exponential ) number of possible label sets , the task of learning from multi label examples is rather challenging . Therefore , the key to successful multi label learning is how to effectively exploit correlations between different labels to facilitate the learning process . In this paper , we propose to use a Bayesian network structure to efficiently encode the conditional dependencies of the labels as well as the feature set , with the feature set as the common parent of all labels . To make it practical , we give an approximate yet efficient procedure to find such a network structure . With the help of this network , multi label learning is decomposed into a series of single label classification problems , where a classifier is constructed for each label by incorporating its parental labels as additional features . Label sets of unseen examples are predicted recursively according to the label ordering given by the network . Extensive experiments on a broad range of data sets validate the effectiveness of our approach against other well established methods .
Categories and Subject Descriptors I26 [ Computing Methodologies ] : Learning—concept learning , induction
General Terms Algorithms
1 .
INTRODUCTION
Traditional supervised learning works under the singlelabel scenario , each example is associated with one single label characterizing its property . However , in many real world applications , objects are usually associated with ie multiple labels simultaneously . To name a few , in text categorization , each document may belong to several topics , such as Shanghai World Expo , economics and even volunteers [ 14 , 19 ] ; In bioinformatics , each gene may be associated with a number of functional classes , such as metabolism , transcription and protein synthesis [ 7 ] ; In automatic video annotation , each video clip may be related to several semantic classes , such as urban and building [ 16 ] . In multi label learning , each example in the training set is represented by a feature vector and associated with a set of labels . The task is then to predict the label sets of unseen examples through analyzing training examples with known label sets .
Formally , learning from multi label examples corresponds to find a mapping from the space of features to the space of label sets , ie the power set of all labels . Therefore , when there is large or even moderate number of labels , the task of multi label learning would become rather challenging due to the tremendous ( exponential ) number of possible label sets . To cope with this issue , it is deemed that the correlations between different labels should be exploited to facilitate multi label learning [ 21 , 23 ] . For example , the probability of an image be annotated with label Africa would be high if we know it has labels lion and grassland ; a document is unlikely to be labeled as politics if we know it is related to entertainment . Thus , effective exploitation of correlation information among different labels is crucial for the success of any multi label learning system .
Roughly speaking , existing strategies to multi label learning problems can be characterized into the following categories based on the order of correlations considered by the system : • First order approaches : The task of multi label learning is tackled by considering decomposing it into a number of independent binary classification problems , one for each possible label [ 1 , 4 , 5 , 29 ] . • Second order approaches : The task of multi label learning is tackled by considering the pairwise relations between labels , such as the ranking between the proper label and the improper label of an example [ 7 , 8 , 19 , 28 ] , or the interaction between any pair of labels [ 9 , 16 , 24 , 30 ] . • High order approaches : The task of multi label learning is tackled by considering the high order relations between labels , such as the full order style of imposing all other labels’ influences on each label in an indirect manner [ 3 , 10 , 11 , 25 ] , or the random style of combining an ensemble of classifiers each addressing correlations among a random subset of labels [ 17 , 18 , 22 ] .
First order approaches simply ignore the correlations between different labels and this may weaken the generalization abilities of these approaches . For the latter two strategies however , their model complexities are usually high due to the exploitation of label combinations . Furthermore , the generality of these two strategies is also limited : a ) Secondorder approaches may suffer from the fact that the correlations between different labels would possibly go beyond second order . b ) The full order approaches may not work well when certain structures exist among labels ( eg label subgroups ) , while the random approaches may not work well due to their randomness in addressing label correlations .
In this paper , we aim to address the label correlations in an effective yet computational efficient way . Specifically , a novel approach named Lead ( multi label Learning by Exploiting lAbel Dependency ) is proposed to learn from multilabel examples .
At first , a Bayesian network ( or directed acyclic graph , DAG ) is built to characterize the joint probability of all labels conditioned on the feature set , such that correlations among labels are explicitly expressed through their dependency relations represented by the DAG structure . After that , a binary classifier is learned for each label by treating its parental labels in the DAG as additional input features . Finally , the label sets of unseen examples are predicted by reasoning with the identified Bayesian network together with the learned binary classifiers .
In contrast to other multi label learning approaches , Lead bears the following advantages through employing Bayesian network : 1 ) The underlying structure inherent in the label space is explicitly expressed in a compact way , which offers a promising opportunity to gain further insights on the concerned learning problem ; 2 ) It is capable of addressing arbitrary order of label correlations , where the order of dependency is “ controlled ” by the number of parents of each label ; 3 ) The model complexity is linear to the number of possible labels ( one binary classifier per label ) , and making predictions for unseen example is straightforward with respect to the Bayesian network and the learned classifiers . Extensive experiments across a broad range of multi label data sets show that Lead achieves highly competitive performance to the well established first order , second order as well as high order approaches .
The rest of this paper is organized as follows . Section 2 presents the Lead approach . Section 3 reports our experimental results . Finally , Section 4 concludes .
Y
2 . THE LEAD APPROACH Let X = Rd be the d dimensional input space and Y = {1 ; 2 ; : : : ; q} be finite set of q possible labels . Given a multilabel training set D = {(xi ; Yi)| 1 ≤ i ≤ m} , where xi ∈ X is a feature vector and Yi ⊆ Y is the set of labels associated with xi , the goal of multi label learning is to learn a function h : X → 2 from D which maps each unseen example to a set of proper labels . From the Bayesian point of view , this problem can be reduced to model the conditional joint distribution of P ( y|x ) , where x ∈ X is the feature vector while y = ( y1 ; y2 ; : : : ; yq ) ∈ {0 ; 1}q is a binary label vector indicating whether x is associated with the k th label ( yk = 1 ) or not ( yk = 0 ) . As reviewed in Section 1 , previous approaches tackle the problem of modeling P ( y|x ) in various ways . First order approaches solve the problem by decomposing it into a num
( a )
( b )
Figure 1 : The structures used to encode the conditional dependencies/independencies of the labels . ( a ) A loyal Bayesian network representation , where all labels have common cause x , and we need to identify the links between yk given x . ( b ) A simplified version , where we first eliminate the effects of x on all labels and find the errors ek , and then exploit the Bayesian network of the errors ek .
′ ber of independent tasks through modeling P ( yk|x ) ( 1 ≤ k ≤ q ) ; Second order approaches solve the problem by considering interactions between a pair of labels through modeling P ( (yk ; yk′ )|x ) ( k ̸= k ) ; High order approaches solve the problem by addressing correlations between a subset of labels through modeling P ( (yk1 ; yk2 ; : : : ; ykq′ )|x ) ( q ′ ≤ q ) . Our goal is to find a simple and efficient way to improve the performance of multi label learning by exploiting the label dependencies . In this section we present the basic idea and procedure of such an approach . 2.1 Basic Idea Mathematically , multi label learning aims to model and predict p(y|x ) . Our objective is to make use of the conditional dependencies among the labels yk ( 1 ≤ k ≤ q ) such that for each example we can better predict their combination . The problem is how to find and make use of such conditional dependencies in an efficient way . To this end , we adopt the Bayesian network [ 13 ] as a compact manner to encode the label dependencies ; for simplicity of the representation , we assume that the joint distribution of the labels yk and the feature set X factorizes according to some Bayesian network structure , or directed acyclic graph . Note that in multi label learning , all labels inherently depend on the feature set , therefore , x is the common parent of all labels . Consequently , we have q∏ p(y|x ) = p(yk|pak ; x ) ;
( 1 ) k=1 where pak denotes the set of parents of the label yk , excluding the inherent parent x . In this way , the multi label classification problem is decomposed into a series of smallscale single label classification problems .
Fig 1 ( a ) describes the relations among all labels yk , and the feature set x ( note that the links among yk are not given since they are to be found ) . From this figure one can see that there are two types of dependencies among the labels . One is due to the common parent , ie , the feature set x ; because of its effect , labels become dependent even if they are conditionally independent given x . The other is the direct dependencies of the labels . One should be aware that the links among yk given in Fig 1 ( a ) may be very different from those implied by the conditional dependencies of yk without considering the effect of x ; in fact , the effect of the y1y2y3 ?xe1e2e3 ? common parent x makes learning the relations between yk complicate .
Generally speaking , there exist two kinds of approaches to Bayesian network structure learning [ 13 ] . One is constraintbased , and the other is score based . Constraint based approaches exploit ( conditional ) independence relations between the variables to construct the causal structure . When performing conditional independence tests in such approaches , one usually assumes that the variables are either discrete or jointly Gaussian with linear relations.1 Score based approaches view a Bayesian network as specifying a statistical model and then address learning as a model selection problem ; they find the Bayesian network structure which maximizes a score function reflecting the goodness of fit and complexity of the model . In our problem , the labels are binary while the features are usually continuous . Moreover , there are usually a large number of features , and the effect of the features on the labels are significantly nonlinear . Consequently , both kinds of approaches mentioned above would encounter difficulties in learning the structure shown in Fig 1 ( a ) . 2.2 A Practical Approach 221 DAG ’s on Errors : To Eliminate the Effect of
Features
We then aim to develop a simplified procedure to identify the links between the labels in Fig 1 ( a ) , with the help of certain reasonable assumptions . To facilitate the following analysis , we consider the binary classification problem as a special case of the nonlinear regression problem : y = f ( x ) + e ;
( 2 ) where y denotes the target variable , x the set of predictors , and e the noise . The following proposition shows the relationship between maximizing the data likelihood of this model and minimizing the mutual information between x and the estimate of e.2
Proposition 1 . Consider the nonlinear regression model Eq 2 , where f is smooth function . Given the examples {xi ; yi}N i=1 , fitting the above model with maximum likelihood is equivalent to minimizing the mutual information between x and the estimate of e .
For two different classification problems exploiting the same feature set , the following proposition holds straightforwardly .
Proposition 2 . Suppose that we have two classification problems with the same attributes : y1 = f1(x ) + e1 and y2 = f2(x ) + e2 :
( 3 )
If ( 1 ) both e1 and e2 are independent from x , and ( 2 ) e1 and e2 are also independent from each other , then y1 and y2 are conditionally independent given x .
As an extension of Proposition 1 , Condition ( 1 ) in Proposition 2 , which states that both e1 and e2 are independent from x , approximately holds . Consequently , roughly speaking , y1 and y2 are conditionally independent given the feature set x if and only if e1 is independent from e2 .
In other words , here we reasonably assume that the effect of x is “ separable ” : we can first eliminate the influences of x in all labels , and then discover the conditional independencies among yk ( conditioned on x ) by analyzing the errors . The assumption may not always hold rigorously . However , it provides a greatly simplified manner to identify the links between yk in presence of the common parent x in the network Fig 1 ( a ) . 222 Procedure of LEAD We can then find the links between yk in the network Fig 1 ( a ) in the following way . We first eliminate the effects of the feature set x on all labels by constructing classifiers for all labels and finding the corresponding errors . Then , we find the Bayesian network structure of the errors ek and treat it as an approximate of that of the labels with x as the common parent . Fig 1 ( b ) illustrates this idea . With this Bayesian network , we then find pak for each label yk in Eq 1 . In our approach , we make use of the links in the Bayesian network structure by directly incorporating pak into the “ feature set ” when constructing the classifier for yk . Our proposed approach consists of the following four steps .
1 . Construct the classifiers for all labels independently .
This produces the error ek for each label yk ( Eq 2 ) . 2 . Learn the Bayesian network structure G of ek ; 1 ≤ k ≤ q .
Proof of this proposition is given in the Appendix . We view classification as an extreme case of nonlinear regression : in classification , y denotes the target class label ( 0 or 1 ) , f involves threshold functions , and the error e , which is discrete , may be 0 , 1 , or 1 . e = 1 ( 1 ) means that the example , which actually came from class 1 ( 0 ) , is classified to class 0 ( 1 ) . 1We note that recently , in the causal discovery scenario , a constraint based method was proposed to find the network structure between a moderate number of continuous variables with nonlinear relations [ 27 ] . In principle it can be easily extended to solve our problem ; however , due to the computational loads , it is not feasible if the number of labels is large ( say , larger then 20 ) . 2Mutual information is a canonical measure of dependence [ 6 ] . The mutual information amongst a set of varii=1 H(vi ) − ables v1 ; v2 ; :: : ; vn is defined as I(v1 ; :: : ; vn ) = H(v1 ; :: : ; vn ) , where H(· ) denotes the entropy . Mutual information is always non negative , and is zero if and only if the involved variables are mutually independent .
∑ n
3 . For each label yk , construct the new classifier Ck by incorporating pak implied in the network G into the feature set .
4 . For testing data , recursively predict yk with the clas sifier Ck and the feature set x ordering of the labels implied in G .
∪cpak according to the
223 On Bayesian Network Learning In Step 2 we need to choose suitable techniques for Bayesian network structure learning . Over 50 software packages are listed in [ 15 ] for different applications of Bayesian networks . We used the BDAGL ( Bayesian DAG learning ) package,3 which implemented the dynamic programming based algorithm for computing the marginal posterior probability of every edge in a Bayesian network [ 12 ] . This algorithm takes O(q2q ) both in time and space , where q is the number of 3http://wwwcsubcca/~murphyk/Software/BDAGL/indexhtml variables . It is very efficient when q is small , and is limited to about 20 variables . ( In practice , it takes about 5 seconds for 10 variables to about 5 minutes for 20 variables . ) When the number of variables is larger than 20 , we resorted to the Banjo ( Bayesian ANalysis with Java Objects ) package [ 20 ] . This package performs approximate maximum a posterior ( MAP ) structure learning using simulated annealing and hill climbing for searching , and is suitable to analyze large data sets . When using it , one needs to specify the maximum running time and some other necessary parameters , and it will finally report the best network found .
3 . EXPERIMENTS 3.1 Evaluation Metrics
Performance evaluation in multi label learning is much more complicated than traditional single label learning , as each example is associated with multiple labels simultaneously . One straightforward solution is to calculate the classical single label metric ( such as precision , recall and Fmeasure ) on each possible label independently , and then combine the metric value from each label through micro or macro averaging [ 23 ] . However , this intuitive way of evaluation fails to directly address the correlations between different labels of each example .
In this paper , five popular metrics specially designed for multi label learning [ 19 , 23 ] are used , ie hamming loss , oneerror , coverage , ranking loss and average precision . Given a multi label data set S = {(xi ; Yi)|1 ≤ i ≤ p} , the five metrics are defined as below . Here , h(xi ) returns a set of proper labels of xi ; h(xi ; y ) returns a real value indicating the confidence for y to be a proper label of xi ; rankh(xi ; y ) returns the rank of y derived from h(xi ; y ) .
• Hamming loss : hlossS ( h ) =
1 p
1|Y||h(xi)∆Yi|
( 4 )
Here ∆ denotes the symmetric difference between two sets . The hamming loss evaluates how many times an examplelabel pair is misclassified .
• One error : one errorS ( h ) =
1 p
[ [ [ arg max y∈Y h(xi ; y ) ] =∈ Yi ] ]
( 5 )
Here for predicate , [ [ ] ] equals 1 if holds and 0 otherwise . The one error evaluates how many times the top ranked label is not in the set of proper labels of the example .
• Coverage : coverageS ( h ) =
1 p max y∈Yi i=1 rankh(xi ; y ) − 1
( 6 )
The coverage evaluates how many steps are need , on average , to move down the label list in order to cover all the proper labels of the example .
• Ranking loss : rlossS ( h ) =
1 p
1|Yi|| ¯Yi| · |Ri| ; where
Ri = {(y1 ; y2)|h(xi ; y1 ) ≤ h(x ; y2 ) ; ( y1 ; y2 ) ∈ Yi × ¯Yi} ( 7 ) p∑ i=1 p∑ i=1 p∑ p∑ i=1
Here ¯Yi denotes the complementary set of Yi in Y . The ranking loss evaluates the average fraction of label pairs that are misordered for the example .
• Average precision : avgprecS ( h ) =
1 p
Pi = {y
′|rankh(xi ; y
|Pi|
1|Yi| · rankh(xi ; y ) ) ) ≤ rankh(xi ; y ) ; y
′
; where
′ ∈ Yi}
( 8 ) p∑ i=1
The average precision evaluates the average fraction of proper labels ranked above a particular label y ∈ Yi .
For the first four metrics , the smaller the value the better the performance . For average precision , on the other hand , the larger the value the better the performance . Furthermore , we choose to normalize the coverage metric ( Eq 6 ) by |Y| so that all the five metrics vary between [ 0 1 ] . 3.2 Data Sets
A total of fourteen multi label data sets are collected for experiments in this paper , whose characteristics are summarized in Table 1 . Given a multi label data set S = {(xi ; Yi)| 1 ≤ i ≤ p} , we use |S| , dim(S ) , L(S ) , F ( S ) to represent the ∑ number of examples , number of features , number of possible In addition , several labels , and feature type respectively . multi label statistics [ 18 , 23 ] are also shown in the Table : a ) Label cardinality LCard(S ) = 1 |Yi| , which meab ) Label density LDen(|S| ) = LCard(S ) L(S ) , which normalizes c ) Distinct label sets DL(S ) = |{Y |∃ x : ( x ; Y ) ∈ S}| , which counts the number of distinct label combinations appeared in the data set ; d ) Proportion of distinct label sets P DL(S ) = DL(S ) |S| which normalizes DL(S ) by the number of examples . sures the average number of labels per example ; LCard(S ) by the number of possible labels ; p i=1 p
,
As shown in Table 1 , seven regular scale data sets ( first part ) as well as seven large scale data sets ( second part ) are included whose sizes are roughly ordered by |S| . In addition , dimensionality reduction is performed on rcv1 ( subset 1 ) to rcv1 ( subset 5 ) as well as tmc2007 , where the top 2 % features with highest document frequency [ 26 ] are retained . To the best of our knowledge , few works on multi label learning have conducted experimental evaluation across such broad range of data sets . One notable exception is [ 18 ] where a total of 12 data sets ( 6 regular scale , 6 large scale ) are considered . Further details on these data sets are available at different sites.4
Intuitively , for the data whose underlying joint label dependence could be well represented by a DAG with the feature vector as a common parent , learning with Lead would give excellent performance . 3.3 Experimental Results
In this paper , we compare Lead with several state of theart multi label learning methods , including two first order approaches Bsvm [ 1 ] and Ml knn [ 29 ] , one second order approach Bp mll [ 28 ] and one high order approach Ecc [ 18 ] . For fair comparison , Libsvm ( with linear kernel ) [ 2 ] is employed as the base classifier for Lead , Bsvm and Ecc .
Furthermore , parameters suggested in respective literatures are used for the compared algorithms : For Bsvm , 4http://mlkdcsdauthgr/multilabelhtml , http://wwwcswaikatoacnz/~jmr30/
Table 1 : Characteristics of the experimental data sets .
LCard(S ) LDen(S ) DL(S ) P DL(S ) Domain music biology
Data set emotions genbase medical enron image scene yeast rcv1 ( subset 1 ) rcv1 ( subset 2 ) rcv1 ( subset 3 ) rcv1 ( subset 4 ) rcv1 ( subset 5 ) bibtex tmc2007
|S| 593 662 978 1702 2000 2407 2417 6000 6000 6000 6000 6000 7395 28596
72 dim(S ) L(S ) 6 27 45 53 5 6 14 101 101 101 101 101 159 22
1185 1449 1001 294 294 103 944 944 944 944 944 1836 981
F ( S ) numeric nominal nominal nominal numeric numeric numeric numeric numeric numeric numeric numeric nominal nominal
1.869 1.252 1.245 3.378 1.236 1.074 4.237 2.880 2.634 2.614 2.484 2.642 2.402 2.158
0.311 0.046 0.028 0.064 0.247 0.179 0.303 0.029 0.026 0.026 0.025 0.026 0.015 0.098
27 32 94 753 20 15 198 1028 954 939 816 946 2856 1341
0.046 0.048 0.096 0.442 0.010 0.006 0.082 0.171 0.159 0.157 0.136 0.158 0.386 0.047 text text media media biology text text text text text text text
Table 2 : Performance ( mean±std . ) of each algorithm in terms of hamming loss . •/◦ indicates whether LEAD is statistically superior/inferior to the compared algorithm ( pairwise t test at 5 % significance level ) .
Data Set emotions genbase medical enron image scene yeast rcv1 ( subset 1 ) rcv1 ( subset 2 ) rcv1 ( subset 3 ) rcv1 ( subset 4 ) rcv1 ( subset 5 ) bibtex tmc2007
Lead
0197±0024 0001±0001 0010±0001 0050±0003 0173±0011 0098±0005 0202±0011 0027±0001 0023±0001 0023±0001 0020±0001 0023±0001 0013±0001 0063±0001
Bsvm
0199±0022 0001±0001 0010±0001 0060±0003• 0176±0007 0104±0006 0199±0010 0026±0001◦ 0023±0001 0023±0001 0020±0001 0023±0001 0016±0001• 0063±0001
Algorithm Ml knn 0194±0013 0005±0002• 0016±0002• 0052±0002• 0170±0008 0084±0008◦ 0195±0011◦ 0027±0001• 0024±0001• 0023±0001 0021±0001• 0024±0001• 0014±0001• 0073±0001•
Bp mll
0219±0021• 0004±0002• 0019±0002• 0052±0003• 0253±0024• 0282±0014• 0205±0010• 0033±0001• 0028±0001• 0028±0001• 0025±0001• 0029±0001• 0016±0001• 0098±0006•
Ecc
0192±0021 0001±0001 0010±0001 0055±0004• 0180±0015• 0096±0010◦ 0208±0010• 0033±0003• 0029±0002• 0029±0003• 0025±0002• 0028±0002• 0016±0001• 0064±0001• models are learned via the cross training strategy [ 1 ] ; For Ml knn , the number of nearest neighbors considered is set to 10 and Euclidean distance is used as the distance measure [ 29 ] ; For Bp mll , the number of hidden neurons is set to 20 % of the dimensionality and the number of training epochs is set to 100 [ 28 ] ; For Ecc , the ensemble size is set to 10 and sampling ratio is set to 67 % [ 18 ] .
Ten fold cross validation is performed on each experimental data set , where Tables 2 to 6 report the detailed results in terms of different evaluation metrics . On each data set , the mean metric value as well as the standard deviation of each algorithm is recorded . Furthermore , to statistically measure the significance of performance difference , pairwise t tests at 5 % significance level are conducted between the algorithms . Specifically , whenever Lead achieves significantly better/worse performance than the compared algorithm on any data set , a win/loss is counted and a maker •/◦ is shown in the Table . Otherwise , a tie is counted and no marker is given . The resulting win/tie/loss counts for Lead against the compared algorithms are summarized in Tables 7 and 8 , grouped by |S| and L(S ) respectively . As shown in Table 7 , for data sets with regular number of examples ( |S| < 5000 ) , Lead is significantly superior to the compared algorithms in 31:4 % ( Bsvm ) , 31:4 % ( Ml knn ) , 68:6 % ( Bp mll ) and 54:3 % ( Ecc ) cases , and is inferior to them in much less 0:0 % ( Bsvm ) , 17:1 % ( Mlknn ) , 8:6 % ( Bp mll ) and 17:1 % ( Ecc ) cases ; Furthermore , for data sets with large number of examples ( |S| > 5000 ) , Lead is significantly superior to the compared algorithms in 57:1 % ( Bsvm ) , 97:1 % ( Ml knn ) , 91:4 % ( Bp mll ) and 82:9 % ( Ecc ) cases , and is inferior to them in much less 5:7 % ( Bsvm ) , 0:0 % ( Ml knn ) , 8:6 % ( Bp mll ) and 5:7 % ( Ecc ) cases . These results indicate that Lead is highly competitive to the state of the art approaches , especially on data sets with large number of examples . As shown in Table 8 , for data sets with regular number of labels ( L(S ) < 50 ) , Lead is significantly superior to the compared algorithms in 17:1 % ( Bsvm ) , 34:3 % ( Ml
Table 3 : Performance ( mean±std . ) of each algorithm in terms of one error . •/◦ indicates whether LEAD is statistically superior/inferior to the compared algorithm ( pairwise t test at 5 % significance level ) .
Data Set emotions genbase medical enron image scene yeast rcv1 ( subset 1 ) rcv1 ( subset 2 ) rcv1 ( subset 3 ) rcv1 ( subset 4 ) rcv1 ( subset 5 ) bibtex tmc2007
Lead
0248±0071 0002±0005 0139±0044 0283±0041 0313±0026 0264±0024 0235±0025 0435±0016 0411±0016 0421±0014 0358±0019 0404±0022 0404±0013 0226±0011
Bsvm
0253±0070 0002±0005 0151±0054 0308±0050• 0314±0021 0250±0027 0230±0023 0396±0013◦ 0407±0018 0477±0127 0391±0082 0432±0090 0444±0011• 0225±0010
Algorithm Ml knn 0263±0067 0009±0011 0252±0045• 0313±0035 0320±0026 0219±0029◦ 0228±0029 0548±0018• 0521±0018• 0519±0024• 0457±0022• 0499±0029• 0589±0019• 0308±0012•
Bp mll
0318±0057• 0000±0000 0327±0057• 0237±0038◦ 0600±0079• 0821±0031• 0235±0030 0714±0017• 0619±0020• 0639±0017• 0625±0020• 0718±0019• 0431±0024• 0444±0050•
Ecc
0216±0085 0000±0000 0099±0034◦ 0212±0026◦ 0289±0026◦ 0226±0034◦ 0176±0022◦ 0441±0028 0413±0030 0428±0039 0377±0027• 0408±0044 0341±0022◦ 0176±0009◦
Table 4 : Performance ( mean±std . ) of each algorithm in terms of coverage . •/◦ indicates whether LEAD is statistically superior/inferior to the compared algorithm ( pairwise t test at 5 % significance level ) .
Data Set emotions genbase medical enron image scene yeast rcv1 ( subset 1 ) rcv1 ( subset 2 ) rcv1 ( subset 3 ) rcv1 ( subset 4 ) rcv1 ( subset 5 ) bibtex tmc2007
Lead
0292±0022 0019±0015 0039±0017 0232±0016 0184±0007 0087±0007 0455±0019 0124±0006 0108±0007 0112±0006 0095±0008 0106±0007 0159±0007 0135±0002
Bsvm
0295±0027 0011±0005 0047±0011• 0425±0037• 0189±0021 0089±0009 0514±0018• 0219±0008• 0206±0010• 0207±0010• 0187±0010• 0200±0011• 0226±0010• 0135±0003
Algorithm Ml knn 0300±0019 0021±0013 0060±0025• 0247±0014• 0194±0020 0078±0010◦ 0447±0014 0219±0010• 0203±0012• 0202±0010• 0176±0007• 0198±0010• 0340±0008• 0183±0004•
Bp mll
0300±0022 0025±0012 0047±0024• 0204±0012◦ 0343±0029• 0374±0024• 0456±0019 0222±0010• 0250±0010• 0262±0005• 0245±0010• 0229±0008• 0096±0005◦ 0268±0021•
Ecc
0322±0022• 0013±0007 0071±0023• 0387±0032• 0199±0020• 0091±0008• 0516±0015• 0353±0018• 0350±0018• 0340±0015• 0302±0016• 0342±0013• 0347±0011• 0239±0008• knn ) , 80:0 % ( Bp mll ) and 54:3 % ( Ecc ) cases , and is inferior to them in much less 0:0 % ( Bsvm ) , 17:1 % ( Ml knn ) , 0:0 % ( Bp mll ) and 17:1 % ( Ecc ) cases ; Furthermore , for data sets with large number of labels ( L(S ) > 50 ) , Lead is significantly superior to the compared algorithms in 71:4 % ( Bsvm ) , 97:1 % ( Ml knn ) , 80:0 % ( Bp mll ) and 82:9 % ( Ecc ) cases , and is inferior to them in much less 5:7 % ( Bsvm ) , 0:0 % ( Ml knn ) , 11:4 % ( Bp mll ) and 5:7 % ( Ecc ) cases . In general , correlations among labels would be complex when the label space becomes larger . Therefore , it is very attracting that Lead gains greater advantages over the comparing algorithms when there is large number class labels , which validates Lead ’s effectiveness in exploiting label dependency to facilitate multi label learning .
4 . CONCLUSION
In this paper , a novel approach to multi label learning is proposed by exploiting the dependencies among labels . Specifically , Bayesian networks are employed to represent the joint distribution of the label space conditioned on the feature space , which is capable of modeling arbitrary order of label correlations . We present an efficient way to approximately find such networks , by working on the classification errors of all labels , instead of of the original labels . The learning system involves a complexity linear in the number of possible labels . Experiments over a broad range of data sets show that our method is highly comparable to the stateof the art approaches , especially on learning tasks with large number of labels as well as examples . Due to its accuracy and efficiency , Lead is expected to be a practically appealing multi label learning method for large scale problems .
In the future , we will explore if there exist better ways to identify , encode , and make use of the conditional depen
Table 5 : Performance ( mean±std . ) of each algorithm in terms of ranking loss . •/◦ indicates whether LEAD is statistically superior/inferior to the compared algorithm ( pairwise t test at 5 % significance level ) .
Data Set emotions genbase medical enron image scene yeast rcv1 ( subset 1 ) rcv1 ( subset 2 ) rcv1 ( subset 3 ) rcv1 ( subset 4 ) rcv1 ( subset 5 ) bibtex tmc2007
Lead
0154±0029 0005±0008 0024±0016 0084±0008 0164±0018 0087±0009 0172±0015 0051±0003 0046±0003 0049±0002 0040±0003 0043±0003 0086±0005 0055±0002
Bsvm
0156±0034 0001±0002 0032±0012• 0180±0022• 0169±0019 0089±0011 0200±0013• 0097±0004• 0096±0005• 0097±0006• 0091±0004• 0091±0008• 0127±0006• 0054±0002
Algorithm Ml knn 0163±0022 0006±0006 0042±0021• 0093±0007• 0175±0019 0076±0012◦ 0166±0015 0105±0005• 0100±0007• 0100±0006• 0083±0005• 0095±0005• 0209±0006• 0089±0003•
Bp mll
0173±0020• 0008±0006 0032±0018• 0068±0006 0366±0037• 0434±0026• 0171±0015 0115±0006• 0152±0007• 0166±0002• 0155±0006• 0118±0004• 0051±0003◦ 0147±0015•
Ecc
0233±0040• 0008±0008 0098±0032• 0241±0025• 0245±0024• 0135±0013• 0285±0022• 0382±0025• 0377±0031• 0368±0020• 0317±0026• 0369±0025• 0411±0013• 0179±0006•
Table 6 : Performance ( mean±std . ) of each algorithm in terms of average precision . •/◦ indicates whether LEAD is statistically superior/inferior to the compared algorithm ( pairwise t test at 5 % significance level ) .
Data Set emotions genbase medical enron image scene yeast rcv1 ( subset 1 ) rcv1 ( subset 2 ) rcv1 ( subset 3 ) rcv1 ( subset 4 ) rcv1 ( subset 5 ) bibtex tmc2007
Lead
0811±0035 0994±0008 0890±0037 0663±0022 0799±0017 0848±0014 0761±0020 0600±0009 0641±0010 0629±0011 0683±0012 0642±0016 0537±0009 0802±0005
Bsvm
0807±0037 0998±0004 0871±0047• 0591±0035• 0796±0015 0849±0016 0749±0019• 0588±0008• 0612±0011• 0576±0054• 0635±0036• 0600±0047• 0516±0010• 0804±0005
Algorithm Ml knn 0799±0031 0989±0010• 0806±0036• 0626±0022• 0792±0017 0869±0017◦ 0765±0021 0478±0011• 0513±0012• 0523±0013• 0575±0016• 0530±0019• 0350±0011• 0726±0007•
Bp mll
0779±0027• 0988±0010• 0782±0042• 0705±0025◦ 0601±0040• 0445±0018• 0754±0020• 0388±0011• 0389±0011• 0388±0009• 0407±0014• 0391±0005• 0557±0013◦ 0603±0031•
Ecc
0796±0042• 0994±0006 0872±0033• 0640±0025• 0794±0016 0852±0016 0728±0019• 0475±0020• 0498±0014• 0499±0018• 0558±0017• 0507±0028• 0512±0013• 0768±0005• dencies of the labels with the feature set as the common parent .
5 . ACKNOWLEDGMENTS
The authors wish to thank the anonymous reviewers for their invaluable comments . This work is supported by the National Science Foundation of China ( 60805022 ) , PhD Programs Foundation of Ministry of Education of China for Young Faculties ( 200802941009 ) , Open Foundation of National Key Laboratory for Novel Software Technology of China ( KFKT2008B12 ) .
6 . REFERENCES [ 1 ] M . R . Boutell , J . Luo , X . Shen , and C . M . Brown .
Learning multi label scene classification . Pattern Recognition , 37(9):1757–1771 , 2004 .
[ 2 ] C C Chang and C J Lin . LIBSVM : A library for support vector machines , 2001 . Software available at http://wwwcsientuedutw/˜cjlin/libsvm [ 3 ] W . Cheng and E . H¨ullermeier . Combining instance based learning and logistic regression for multilabel classification . Machine Learning , 76(2 3):211–225 , 2009 .
[ 4 ] A . Clare and R . D . King . Knowledge discovery in multi label phenotype data . In L . D . Raedt and A . Siebes , editors , Lecture Notes in Computer Science 2168 , pages 42–53 . Springer , Berlin , 2001 .
[ 5 ] F . D . Comit´e , R . Gilleron , and M . Tommasi . Learning multi label altenating decision tree from texts and data . In P . Perner and A . Rosenfeld , editors , Lecture Notes in Computer Science 2734 , pages 35–49 . Springer , Berlin , 2003 .
[ 6 ] T . M . Cover and J . A . Thomas . Elements of
Table 7 : The win/tie/loss results ( grouped by |S| ) for LEAD against the compared algorithms in terms of different evaluation metrics .
Lead against
Evaluation Metric hamming loss one error coverage ranking loss average precision
Bsvm jSj < 5000 1/6/0 1/6/0 3/4/0 3/4/0 3/4/0 jSj > 5000 1/5/1 1/5/1 6/1/0 6/1/0 6/1/0
Ml knn
Bp mll jSj < 5000 3/2/2 1/5/1 2/4/1 2/4/1 3/3/1 jSj > 5000 6/1/0 7/0/0 7/0/0 7/0/0 7/0/0 jSj < 5000 7/0/0 4/2/1 3/3/1 4/3/0 6/0/1 jSj > 5000 7/0/0 7/0/0 6/0/1 6/0/1 6/0/1
Ecc jSj < 5000 3/3/1 0/2/5 6/1/0 6/1/0 4/3/0 jSj > 5000 7/0/0 1/4/2 7/0/0 7/0/0 7/0/0
In Total
11/24/0
20/13/2
11/18/6
34/1/0
24/8/3
32/0/3
19/10/6
29/4/2
Table 8 : The win/tie/loss results ( grouped by L(S ) ) for LEAD against the compared algorithms in terms of different evaluation metrics .
Lead against
Evaluation Metric hamming loss one error coverage ranking loss average precision
Bsvm
L(S ) < 50 L(S ) > 50 2/4/1 0/7/0 2/4/1 0/7/0 7/0/0 2/5/0 7/0/0 2/5/0 2/5/0 7/0/0
Ml knn
L(S ) < 50 L(S ) > 50 6/1/0 3/2/2 7/0/0 2/4/1 7/0/0 2/4/1 7/0/0 2/4/1 3/3/1 7/0/0
Bp mll
L(S ) < 50 L(S ) > 50 7/0/0 7/0/0 6/0/1 5/2/0 5/2/0 4/3/0 5/1/1 5/2/0 7/0/0 5/0/2
Ecc
L(S ) < 50 L(S ) > 50 7/0/0 3/3/1 1/4/2 0/2/5 7/0/0 6/1/0 7/0/0 6/1/0 4/3/0 7/0/0
In Total
6/29/0
25/8/2
12/17/6
34/1/0
28/7/0
28/3/4
19/10/6
29/4/2
Information Theory . Wiley Interscience , New York , NY , 1991 .
[ 7 ] A . Elisseeff and J . Weston . A kernel method for multi labelled classification . In T . G . Dietterich , S . Becker , and Z . Ghahramani , editors , Advances in Neural Information Processing Systems 14 , pages 681–687 . MIT Press , Cambridge , MA , 2002 .
[ 8 ] J . F¨urnkranz , E . H¨ullermeier , E . L . Menc´ıa , and
K . Brinker . Multilabel classification via calibrated label ranking . Machine Learning , 73(2):133–153 , 2008 . [ 9 ] N . Ghamrawi and A . McCallum . Collective multi label classification . In Proceedings of the 14th ACM International Conference on Information and Knowledge Management , pages 195–200 , Bremen , Germany , 2005 .
[ 10 ] S . Godbole and S . Sarawagi . Discriminative methods for multi labeled classification . In H . Dai , R . Srikant , and C . Zhang , editors , Lecture Notes in Artificial Intelligence 3056 , pages 22–30 . Springer , Berlin , 2004 .
[ 11 ] S . Ji , L . Tang , S . Yu , and J . Ye . Extracting shared subspace for multi label classification . In Proceedings of the 14th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 381–389 , Las Vegas , NV , 2008 .
[ 14 ] A . McCallum . Multi label text classification with a mixture model trained by EM . In Working Notes of the AAAI’99 Workshop on Text Learning , Orlando , FL , 1999 .
[ 15 ] K . Murphy . Software packages for graphical models / bayesian networks . International Society for Bayesian Analysis , 2007 .
[ 16 ] G J Qi , X S Hua , Y . Rui , J . Tang , T . Mei , and
H J Zhang . Correlative multi label video annotation . In Proceedings of the 15th ACM International Conference on Multimedia , pages 17–26 , Augsburg , Germany , 2007 .
[ 17 ] J . Read , B . Pfahringer , and G . Holmes . Multi label classification using ensembles of pruned sets . In Proceedings of the 9th IEEE International Conference on Data Mining , pages 995–1000 , Pisa , Italy , 2008 .
[ 18 ] J . Read , B . Pfahringer , G . Holmes , and E . Frank .
Classifier chains for multi label classification . In W . Buntine , M . Grobelnik , and J . Shawe Taylor , editors , Lecture Notes in Artificial Intelligence 5782 , pages 254–269 . Springer , Berlin , 2009 .
[ 19 ] R . E . Schapire and Y . Singer . Boostexter : a boosting based system for text categorization . Machine Learning , 39(2/3):135–168 , 2000 .
[ 12 ] M . Koivisto . Advances in exact bayesian structure
[ 20 ] V . Smith , J . Yu , T . Smulders , A . Hartemink , and discovery in bayesian networks . In Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence ( UAI 2006 ) , pages 241–248 . AUAI Press , 2006 .
[ 13 ] D . Koller and N . Friedman . Probabilistic Graphical
Models : Principles and Techniques . MIT Press , Cambridge , MA , 2009 .
E . Jarvis . Computational inference of neural information flow networks . PLoS Computational Biology , 2:1436–1449 , 2006 .
[ 21 ] G . Tsoumakas , I . Katakis , and I . Vlahavas . Mining multi label data . In O . Maimon and L . Rokach , editors , Data Mining and Knowledge Discovery Handbook . Springer , Berlin , 2010 .
ˆe = y − ˆf ( x ) , the Jacobian matrix in that transformation is
)
(
(
(
)
T
@x @x @ ^e @x
J =
@x @y @ ^e @y
=
I 0 −( @ ^f @x )T 1
)
; where I denotes the identity matrix and 0 denotes the vector of zeros . Clearly , one can see that the determinant of J is |J| = 1 . Consequently , we have p(x ; ˆe ) = p(x ; y)=|J| = p(x ; y ) . That is , the joint entropy of ( x ; ˆe ) is H(x ; ˆe ) = −E{log p(x ; ˆe)} = −E{log p(x ; y)} = H(x ; y ) : Mutual information between x and ˆe is then I(x ; ˆe ) = H(x ) + H(ˆe ) − H(x ; ˆe ) = H(x ) + H(ˆe ) − H(x ; y ) : ∑
As the first and third terms in the above quantity do not depend on ˆf , minimizing I(x ; ˆe ) is then equivalent to minimizi=1 log pe(yi − ing H(ˆe ) , or maximizing ˆf ( xi) ) , which is exactly the log likelihood given in Eq 9 . One can then see that maximum likelihood is equivalent to minimizing the mutual information between x and ˆe .
N i=1 log pe(ˆei ) =
∑
N
[ 22 ] G . Tsoumakas and I . Vlahavas . Random k labelsets : an ensemble method for multilabel classification . In J . N . Kok , J . Koronacki , R . L . de Mantaras , S . Matwin , D . Mladeniˇc , and A . Skowron , editors , Lecture Notes in Artificial Intelligence 4701 , pages 406–417 . Springer , Berlin , 2007 .
[ 23 ] G . Tsoumakas , M L Zhang , and Z H Zhou . Tutorial on learning from multi label data [ http://www.ecml pkdd2009net/wp content/uploads/2009/08/learningfrom multi label datapdf ] In ECML/PKDD 2009 , Bled , Slovenia , 2009 .
[ 24 ] N . Ueda and K . Saito . Parametric mixture models for multi label text . In S . Becker , S . Thrun , and K . Obermayer , editors , Advances in Neural Information Processing Systems 15 , pages 721–728 . MIT Press , Cambridge , MA , 2003 .
[ 25 ] R . Yan , J . Teˇsi´c , and J . R . Smith . Model shared subspace boosting for multi label classification . In Proceedings of the 13th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 834–843 , San Jose , CA , 2007 .
[ 26 ] Y . Yang and J . O . Pedersen . A comparative study on feature selection in text categorization . In Proceedings of the 14th International Conference on Machine Learning , pages 412–420 , Nashville , TN , 1997 . [ 27 ] K . Zhang and A . Hyv¨arinen . Acyclic causality discovery with additive noise : An information theoretical perspective . In Proc . European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases ( ECML PKDD ) 2009 , pages 570–585 , Bled , Slovenia , 2009 .
[ 28 ] M L Zhang and Z H Zhou . Multilabel neural networks with applications to functional genomics and text categorization . IEEE Transactions on Knowledge and Data Engineering , 18(10):1338–1351 , 2006 . [ 29 ] M L Zhang and Z H Zhou . ML kNN : A lazy learning approach to multi label learning . Pattern Recognition , 40(7):2038–2048 , 2007 .
[ 30 ] S . Zhu , X . Ji , W . Xu , and Y . Gong . Multi labelled classification using maximum entropy method . In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 274–281 , Salvador , Brazil , 2005 .
APPENDIX A . PROOF OF PROPOSITION 1
Proof . Denote by ˆe and ˆf the estimate of e and f , respectively . Suppose that the density of the noise , pe , is given ( which may be adaptively estimated from data or fixed to a reasonable prior distribution ) . The maximum likelihood estimate of f and e is obtained by maximizing the data loglikelihood
N∑ l = i=1
N∑ i=1 log p(yi|xi ) = log pe(yi − ˆf ( xi) ) :
( 9 )
Now let us see how the above quantity is related to I(x ; ˆe ) , the mutual information between x and the estimate of e . Consider the transformation from ( x ; y)T to ( x ; ˆe)T . As
