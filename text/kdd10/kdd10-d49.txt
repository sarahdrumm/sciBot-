Discovering Frequent Subgraphs over Uncertain Graph
Databases under Probabilistic Semantics
Zhaonian Zou znzou@hiteducn
Hong Gao honggao@hiteducn
Jianzhong Li lijzh@hiteducn
School of Computer Science and Technology
Harbin Institute of Technology
92 West Da Zhi Street , Harbin , Heilongjiang 150001 , China
ABSTRACT Frequent subgraph mining has been extensively studied on certain graph data . However , uncertainties are inherently accompanied with graph data in practice , and there is very few work on mining uncertain graph data . This paper investigates frequent subgraph mining on uncertain graphs under probabilistic semantics . Specifically , a measure called ϕ frequent probability is introduced to evaluate the degree of recurrence of subgraphs . Given a set of uncertain graphs and two numbers 0 < ϕ , τ < 1 , the goal is to quickly find all subgraphs with ϕ frequent probability at least τ . Due to the NP hardness of the problem , an approximate mining algorithm is proposed for this problem . Let 0 < δ < 1 be a parameter . The algorithm guarantees to find any frequent subgraph S with probability at least , where s is the number of edges of S . In addition , it is thoroughly discussed how to set δ to guarantee the overall approximation quality of the algorithm . The extensive experiments on real uncertain graph data verify that the algorithm is efficient and that the mining results have very high quality . fis
1−δ 2
.
Categories and Subject Descriptors H28 [ Database Applications ] : [ data mining ]
General Terms Algorithms , Performance , Theory
Keywords Uncertain graph , frequent subgraph , probabilistic semantics
1 .
INTRODUCTION
Graphs are general data structures for representing complicated relationships between objects , which have seen wide applications in bioinformatics , social networks and spatial databases , etc . Large amount of data represented by graphs , also known as graph data , call for intelligent tools to analyze and understand them . Frequent subgraph mining [ 9 ] is one of the powerful tools to study the structures of graph data , especially , the recurring substructures . More precisely , the mining task is formulated as follows . Given a set D of graphs and a number 0 < ϕ < 1 , find all subgraphs that occur in at least ϕ · |D| graphs in D . The percentage of graphs in D that contain a subgraph S is called the support of S .
Recent researches [ 4,13,14 ] have shown that uncertainties are inherent in graph data , in particular , the structures of graphs are uncertain . In the data uncertainty models used in [ 4 , 13 ] , each edge of a graph is associated with a number indicating the probability of the edge existing in reality , and the existence of edges is mutually independent . Such graph is called an uncertain graph [ 13 ] . An uncertain graph essentially represents a probability distribution over all of the certain graphs in the forms of which the uncertain graph may actually exist . Each of these certain graphs is called an implicated graph [ 13 ] . Zou et al . [ 13 ] studied frequent subgraph mining on uncertain graph data . A set of uncertain graphs D = {G1 , G2 , . . . , Gn} represents a probability distribution over a family F of certain graph sets . Each set of certain graphs D . = {G . is an implicated graph of Gi ∈ D for 1 ≤ i ≤ n . For a certain subgraph S , the degree of recurrence of S in D is measured by the expected value of the supports of S in all certain graph sets in F , called the expected support of S . Thus , the frequent subgraph mining problem on uncertain graph data is defined in [ 13 ] as follows . Given a set D of uncertain graphs and a number 0 < γ < 1 , find all subgraphs with expected support at least γ . n} ∈ F satisfies that G .
2 , . . . , G . i ∈ D .
1 , G .
Motivated by some recent work [ 2,11 ] on frequent itemset mining on probabilistic data , this paper investigates frequent subgraph mining on uncertain graph data based on semantics different from that adopted by [ 13 ] . Again , a set D of uncertain graphs represents a probability distribution over a family F of certain graph sets . However , the degree of recurrence of a certain subgraph S in D is measured by the probability that S has support at least ϕ across all certain graph sets in F . Such probability is called ϕ frequent probability in this paper . Therefore , the problem to be solved in this paper can be defined as follows . Given a set D of uncertain graphs and two numbers 0 < ϕ , τ < 1 , find all subgraphs with ϕ frequent probability at least τ .
Though the difference between the two semantics for mining uncertain data has been argued in [ 11 ] from the aspect of mining results , there is a lack of explanation of the difference from the angle of application areas . In short , frequent
633 subgraph mining under expected semantics [ 13 ] is more suitable for investigating structural patterns in uncertain graph data , which behaves more like an exploratory tool ; whereas , frequent subgraph mining under probabilistic semantics in this paper is more suitable for extracting features from uncertain graph data . Supposing a subgraph having support at least ϕ can be a feature in a set of certain graphs , then ϕ frequent probability perfectly captures the probability of a subgraph being a feature in a set of uncertain graphs , but expected support does not . On the contrary , expected support characterizes the expected degree of recurrence of a subgraph , but ϕ frequent probability does not .
Although the problem to be solved in this paper follows the same semantics as the frequent itemset mining problems in [ 2 , 11 ] , the algorithms presented in [ 2 , 11 ] can not be extended to our problem . This is because it can be decided in polynomial time whether an itemset is frequent or not [ 2,11 ] ; however , it is #P hard to compute the ϕ frequent probability of a subgraph as will be proved later in this paper .
The problem is also proved to be a NP hard problem . Hence , instead of discovering all strictly frequent subgraphs , we find an ε approximate set of frequent subgraphs including all frequent subgraphs and a fraction of infrequent subgraphs but with ϕ frequent probability at least τ −ε , where 0 < ε < τ is a small error tolerance . In other words , all subgraphs with ϕ frequent probability at least τ must be output , but all subgraphs with ϕ frequent probability less than τ − ε need not to be output .
This paper proposes an approximate mining algorithm to find such an ε approximate set of frequent subgraphs . Let 0 < δ < 1 be a pre specified parameter . The algorithm guarantees to discover any frequent subgraph S with probability at least
, where s is the number of edges of S . fis
.
1−δ 2
The algorithm is developed based on the well known gSpan algorithm [ 9 ] . First , all subgraphs are encoded into their minimum DFS codes and are organized into a search tree according to the lexicographic order of minimum DFS codes [ 9 ] . Then , the search tree is traversed in depth first order to find an ε approximate set of frequent subgraphs . In particular , for each visited subgraph S , instead of computing the exact ϕ frequent probability of S , it is determined whether the ϕfrequent probability of S is certainly not less than τ − ε and is probably greater than or equal to τ using an approximation algorithm . The approximation algorithm is very simple and fails with probability at most δ . If the answer from the approximation algorithm is “ yes ” , then output S and continue the search , otherwise stop searching the descendants of S since the ϕ frequent probability of any supergraph of S is definitely no greater than τ . The theoretical analysis shows that to obtain any frequent subgraph with probability at least 1−Δ , δ should be at most 1 − 2 · ( 1 − Δ)1/fimax , where max is the maximum number of edges of frequent subgraphs . In addition , extensive experiments were performed on real uncertain graph data to evaluate the practical performance and the approximation quality of the proposed algorithm . The experimental results verify that the algorithm is very efficient and accurate . The main contributions of this paper are as follows . • This paper formally defines the problem of mining frequent subgraphs on uncertain graph data under probabilistic semantics .
• It is rigorously proved that the mining problem is NP hard and that it is #P hard to compute the ϕ frequent probability of a subgraph .
.
1−δ 2
• An approximate mining algorithm is proposed to find an ε approximate set of frequent subgraphs . The algorithm guarantees to discover any frequent subgraph S , where s is the number with probability at least of edges of S . fis
• It is thoroughly discussed how to set parameter δ to guarantee the overall approximation quality of the proposed algorithm .
• Extensive experiments were carried out on real uncertain graph data to evaluate the performance and the approximation quality of the proposed algorithm .
The rest of this paper is organized as follows . Section 2 reviews the related work . Section 3 introduces the model of uncertain graph data and defines the frequent subgraph mining problem . Section 4 formally proves the inherent computational complexity of the problem . Section 5 proposes the mining algorithm and analyzes the performance guarantees of the algorithm . Section 6 points out how to set parameters to ensure the overall approximation quality of the algorithm . Section 7 shows the experimental results on real uncertain graph data . Finally , Section 8 concludes this paper .
2 . RELATED WORK
A number of algorithms have been proposed for mining frequent subgraphs on certain graph data , which have been surveyed in [ 1 ] . All these algorithms can not handle uncertainties inherent in graph data . However , some state of theart techniques adopted by the algorithms such as minimum DFS codes [ 9 ] for representing subgraphs and the right most extension technique [ 9 ] for extending subgraphs can also be used in our work because knowledge to be discovered in this paper is actually certain subgraphs .
Kimmig and Raedt [ 6 ] cast pattern mining problems in the context of logic programming , particularly in ProbLog , a probabilistic Prolog system . Due to the powerful expression capability , ProbLog can represent uncertainties of itemsets , sequences , trees or graphs . Their appealing method is an integration of multi relational data mining and inductive logic programming . However , due to the data representation in ProbLog , operations on graphs such as subgraph isomorphism testings are implemented by clause reductions , which become inefficient on large sized graphs .
Zou et al . [ 13 ] studied frequent subgraph mining on uncertain graph data independently . They proposed expected support to evaluate the significance of subgraphs . In particular , the expected support of a subgraph S is the expected value of the supports of S in all implicated graph databases . It differs from ϕ frequent probability proposed in this paper in the following way . Even if the support of S can be less than ϕ in a specific implicated graph database , it still contributes to the expected support of S but doesn’t contribute to the ϕ frequent probability of S .
Although the problem in this paper follows the same semantics as the frequent itemset mining problems in [ 2 , 11 ] , the algorithms proposed in [ 2,11 ] can not be extended to our problem . Except for the difference in data type , another important reason is that it can be decided in polynomial time whether an itemset is frequent or not ; however , it is #P hard to compute the ϕ frequent probability of a subgraph .
634 3 . PROBLEM DEFINITION 3.1 Uncertain Graphs
Let us first extend the model of uncertain graphs recently proposed in [ 13 ] to the following one that considers uncertainties of both vertices and edges .
Definition 1 . An uncertain graph is a system G = ( V , E , Σ , LV , LE , PV , PE ) , where ( V , E ) is an undirected graph , V is the set of vertices , E is the set of edges , Σ is a set of labels , LV : V → Σ is a function assigning labels to the vertices , LE : E → Σ is a function assigning labels to the edges , PV : V → [ 0 , 1 ] is a function assigning existence probability values to the vertices , and PE : E → [ 0 , 1 ] is a function assigning conditional existence probability values to the edges given their endpoints . The existence probability , PV ( v ) , of a vertex v ∈ V is the probability of v existing in practice . The conditional existence probability , PE(e|u , v ) , of an edge e = ( u , v ) ∈ E is the probability of e existing between vertices u and v on the condition that both u and v exist in practice . Thus , a labeled graph in traditional graph mining [ 9 ] , which is called certain graph in this paper , is a special uncertain graph with existence probability values of 1 on all vertices and conditional existence probability values of 1 on all edges .
Similar to the statement in [ 13 ] , an uncertain graph essentially represents a set of certain graphs implicated by it , each of which is a possible structure in the form of which the uncertain graph might be present in practice .
Definition 2 . An uncertain graph G = ( V , E , Σ , LV , LE , V , L . PV , PE ) implicates a certain graph G . E ) , denoted by G ⇒ G . . ⊆ Σ , V = LV |V . , and L . L . ) is the , and L|X denotes the set of edges with both endpoints in V . function obtained by restricting function L to domain X .
= ( V . , E . , Σ . , L . , if V . ⊆ V , E . ⊆ E ∩ ( V . × V . ) , Σ E = LE|E . , where E ∩ ( V . × V .
This paper assumes that the existence probabilities of vertices and the conditional existence probabilities of edges of an uncertain graph are mutually independent , respectively . Based on this assumption , the probability of an uncertain graph G = ( V , E , Σ , LV , LE , PV , PE ) implicating a certain graph G . Pr(G ⇒ G .
( 1 − PV ( v ) )
= ( V . , E . , Σ
E ) is ·
V , L .
PV ( v )
) =
. , L .
' ff ' ff ' v∈V .
·
· e=(u,v)∈E .
' ff v∈V \V . PE(e|u , v ) ff e=(u,v)∈E∩(V .×V )\E
( 1 − PE(e|u , v ) )
,
( 1 ) where E ∩ ( V . × V . in V .
.
) is the set of edges with both endpoints
Let Imp(G ) denote the set of all implicated graphs of an uncertain graph G . It is easy to verify that ff
⎛⎝ |V | ) i=1
|V | i
⎞⎠
|Imp(G)| = O i(i−1)/2 2
Theorem 1 . For an uncertain graph G , the function given in Equation 1 defines a probability distribution over Imp(G ) . 3.2 Uncertain Graph Databases
Let us proceed to extend the model of uncertain graph databases proposed in [ 13 ] to the following one that takes trivial implicated graphs into account .
An uncertain graph database is a set of uncertain graphs . It essentially represents a set of implicated graph databases . Definition 3 . An uncertain graph database D = {G1 , G2 , 1 , G . 2 , m} if m ≤ n , and there is an injection σ : {1 , 2 , . . . , m}
. . . , Gn} implicates a certain graph database D . . . . , G . → {1 , 2 , . . . , n} such that Gσ(i ) ⇒ G .
= {G . i for 1 ≤ i ≤ m .
Note that , in Definition 3 , we have m ≤ n because every uncertain graph in D generally has a non zero probability to implicate a trivial certain graph ∅ , ie , a certain graph with no vertices and no edges . Since trivial graphs do not contain useful knowledge , they should be eliminated from implicated graph databases . This is the main difference from the model of uncertain graph databases proposed in [ 13 ] .
This paper assumes that the uncertain graphs in an uncertain graph database are mutually independent . Based on this assumption , the probability of an uncertain graph database D = {G1 , G2 , . . . , Gn} implicating a certain graph database D .
2 , . . . , G .
= {G .
1 , G . m} is mff ff
Pr(D ⇒ D .
) =
·
Pr(Gσ(i ) ⇒ G . ff i ) i=1 i∈{1,2,,n}\{σ(x)|1≤x≤m} ff
,
( 2 )
Pr(Gi ⇒ ∅ ) where σ is an injection from {1 , 2 , . . . , m} to {1 , 2 , . . . , n} such that Gσ(i ) ⇒ G . i for 1 ≤ i ≤ m , and ∅ denotes a trivial graph . Let Imp(D ) denote the set of all implicated graph dataffi bases of an uncertain graph database D . Based on the independence assumption , it is easy to know that |Imp(D)| = G∈D |Imp(G)| . Using the arguments in the proof of The orem 2 in [ 15 ] , we can prove the theorem below .
Theorem 2 . For an uncertain graph database D , the function given in Equation 2 defines a probability distribution over Imp(D ) . 3.3 Frequent Subgraph Mining such that
E ) , denoted by G ) G .
Definition 4 . A certain graph G = ( V , E , Σ , LV , LE ) is subgraph isomorphic to another certain graph G . = ( V . , E . , V , L . . , L . Σ , if there exists an injection f : V → V . ( 1 ) LV ( v ) = L . ( 2 ) ( f ( u ) , f ( v ) ) ∈ E . ( 3 ) LE((u , v ) ) = L . Injection f is called a subgraph isomorphism from G to G .
E((f ( u ) , f ( v) ) ) for any ( u , v ) ∈ E .
V ( f ( v ) ) for any v ∈ V , for any ( u , v ) ∈ E ,
. by counting the number of implicated graphs of a fullyconnected uncertain graph . Due to arguments similar to the proof of Theorem 1 in [ 15 ] , we have the following theorem .
Probabilistic frequent subgraphs . As in traditional frequent subgraph mining , this paper also considers mining
635 connected subgraphs . If not otherwise specified , a subgraph refers to a connected subgraph in the rest of the paper .
Let D be an uncertain graph database , Imp(D ) be the set of all implicated graph databases of D , and S be a certain subgraph . Because each implicated graph database D . ∈ Imp(D ) is actually a certain graph database , the traditional definition of support [ 7 ] applies to D . , that is , the support of S in D . is sup(S ; D .
) =
|{G . ∈ D.| S ) G.}|
|D.|
.
( 3 )
)
Thus , the probability that the support of S is no less than 0 < ϕ < 1 across all implicated graph databases of D is
Pr(D ⇒ D .
) ,
( 4 )
Pr(S ; D , ϕ ) =
D.∈Imp(D ) , sup(S;D.)≥ϕ ) is the probability of D implicating D . where Pr(D ⇒ D . as given in Equation 2 . In the rest of the paper , the probability given in Equation 4 is called the ϕ frequent probability of S in D . When D and ϕ are explicit from the context , Pr(S ; D , ϕ ) can be simply written as Pr(S ) . A subgraph S is called ( ϕ , τ ) probabilistic frequent if the ϕ frequent probability of S is no less than a user specified confidence threshold 0 < τ < 1 . When ϕ and τ are clear from the context , S can be simply called a frequent subgraph .
Problem statement . The problem of mining frequent subgraphs in an uncertain graph database under probabilistic semantics can be formalized as follows .
Input : an uncertain graph database D , a support threshold
0 < ϕ < 1 , and a confidence threshold 0 < τ < 1 .
Output : all ( ϕ , τ ) probabilistic frequent subgraphs in D .
4 .
INHERENT COMPUTATIONAL COMPLEXITY OF THE PROBLEM
This section formally proves the inherent computational complexity of the problem of mining frequent subgraphs in an uncertain graph database . The following proofs use the complexity class #P for enumeration problems [ 8 ] .
Theorem 3 . It is #P hard to compute the ϕ frequent prob ability of a subgraph S in an uncertain graph database D .
Proof . We prove the theorem by reducing an arbitrary instance of the #P complete problem of counting the number of assignments satisfying a monotone k DNF formula F [ 8 ] to an instance of the current problem in polynomial time . Here , let F = C1 ∨ C2 ∨ ··· ∨ Cm contain m clauses over n variables x1 , x2 , . . . , xn . Each clause Ci is in the form of l1 ∧ l2 ∧ ··· ∧ lk , where k is a constant , and each literal lj is an un negated variable in {x1 , x2 , . . . , xn} . Without loss of generality , we assume that a variable occurs in a clause at most once . The reduction is carried out as follows . Firstly , construct an uncertain graph database D = {G} , where G is a bipartite uncertain graph . The vertex set of G is U ∪ V , where U = {c1 , c2 , . . . , cm} and V = {v1 , v2 , . . . , vn} . All of the vertices in U are labeled by α and have existence probabilities of 1 , and all of the vertices in V are labeled by β and have existence probabilities of 1/2 . There is an edge between vertices ci and vj if and only if clause Ci contains variable xj . All of the edges of G are labeled by γ and have conditional existence probabilities of 1 .
Secondly , create a certain subgraph S . The vertex set of S is {c , v1 , v2 , . . . , vn} , where c is labeled by α , and v1 , v2 , . . . , vn are labeled by β . There is an edge labeled by γ connecting vertex c with each of v1 , v2 , . . . , vn .
Thirdly , let ϕ = 1 . The correspondence between the number of assignments satisfying F and the ϕ frequent probability , Pr(S ; D , ϕ ) , of S in D can be established due to the following arguments .
( 1 ) A truth assignment π 1 to 1 corresponds to an implicated graph database D . π of D such that variable xi is assigned true in π if and only if vertex vi is contained in the only certain graph in D . π .
( 2 ) A truth assignment π satisfies F if and only if S has support at least ϕ in the implicated graph database D .
π that π 1 to 1 corresponds to .
Subsequently , the number of truth assignments satisfying F is 2n · Pr(S ; D , ϕ ) . The reduction is completed .
Theorem 4 . It is #P hard to count the number of fre quent subgraphs in an uncertain graph database .
Proof . An instance of the #P hard problem of counting the number of frequent subgraphs with support at least ϕ in a certain graph database D [ 10 ] can be trivially reduced to an instance of the current problem by specifying the input uncertain graph database to be D , the support threshold to be ϕ and the confidence threshold τ = 1 . This is because D is a special uncertain graph database , and a subgraph has support at least ϕ in D if and only if its ϕ frequent probability in D is 1 . Thus , the theorem holds .
5 . APPROXIMATE MINING ALGORITHM Due to the hardness results proved in Section 4 , it can not be expected to discover all strictly frequent subgraphs in polynomial time unless P = NP . Instead , an approximate mining algorithm is proposed to find a broader set of subgraphs including all frequent subgraphs and a fraction of infrequent subgraphs but with ϕ frequent probability at least τ − ε , where 0 < ε < τ is an error tolerance . In other words , all subgraphs with ϕ frequent probability at least τ must be output , but all subgraphs with ϕ frequent probability less than τ − ε need not to be output . It is not meant to discover all subgraphs with ϕ frequent probability at least τ − ε ; or otherwise , the problem would become even harder . Suppose a subgraph S has ϕ frequent probability at least τ − ε . If it is definitely known that the ϕ frequent probability of S is less than τ , then S need not to appear in the mining results . 5.1 High Level Description
The approximate mining algorithm takes as input an uncertain graph database D , a support threshold ϕ , a confidence threshold τ , and an error tolerance ε . The procedure of the algorithm can be briefly outlined as follows .
( S1 ) First , organize all subgraphs in D into a search tree , where nodes represent subgraphs , and each node ( subgraph ) is subgraph isomorphic to all its children , if it has any , and has one less edge than all of them .
( S2 ) Then , examine the subgraphs in the search tree in depth first order . For each examined subgraph S , determine in polynomial time whether S has ϕ frequent
636 probability at least τ −ε and probably at least τ . If the answer is “ yes ” , then output S and proceed to examine the descendants of S in depth first order . Otherwise , all descendants of S are certainly infrequent and can be pruned due to the following property of ϕ frequent probability .
Lemma 1
( Apriori property ) . For two subgraphs S and S .
, if S ) S .
, then Pr(S ) ≥ Pr(S .
) .
Proof . The lemma can be readily proved from Equa tion 4 and the Apriori property of support [ 7 ] .
The algorithm terminates while no subgraphs are left un examined . Next , we clarify the details of each step .
Building search tree of subgraphs . Because subgraphs are actually certain graphs , all subgraphs in D can be encoded into minimum DFS codes , the state of the art canonical graph coding scheme developed for frequent subgraph mining [ 9 ] . Informally speaking , the minimum DFS code of a subgraph is a sequence that is prior to all other sequential representations of the isomorphic subgraphs according to the lexicographic order of DFS codes . For more details on minimum DFS codes , please refer to [ 9 ] .
Then , the search tree of subgraphs can be constructed . The nodes are all subgraphs in D . The root is the trivial subgraph ∅ . The parent parent(S ) of each subgraph S ( S ffi= ∅ ) satisfies that the minimum DFS code of parent(S ) is the longest prefix of that of S .
Depth first search on search tree . Note that we do not materialize the search tree in memory but enumerate subgraphs in the search tree in depth first order . Particularly , for each subgraph S , all its children are generated as follows . First , perform right most extension [ 9 ] to S , obtaining a set of subgraphs , each of which contains S and has one more edge than S . Then , for each right most extended subgraph , if the minimum DFS code of S is a prefix of that of S . S . , then S . is not a child of S . Please refer to [ 9 ] for more details on right most extension . is a child of S , otherwise S .
Verifying visited subgraphs . For each subgraph S visited in the search , it need to be verified whether S can be output as a result or not using the following method . First , approximate the ϕ frequent probability , Pr(S ) , of S by an interval [ pl , pu ] such that Pr(S ) ∈ [ pl , pu ] and that |pu−pl| ≤ ε . Then , apply the following rules . ( R1 ) If pl ≥ τ −ε and pu ≥ τ , then it is certain that Pr(S ) ≥ τ − ε and is probable that Pr(S ) ≥ τ , thus S can be output as a result .
( R2 ) If pu < τ , then Pr(S ) < τ , thus S is not frequent .
Section 5.2 presents the details of the algorithm for ap proximating Pr(S ) by such interval [ pl , pu ] . 5.2 Approximating ϕ Frequent Probability
This subsection proposes an algorithm for approximating the ϕ frequent probability , Pr(S ) , of a subgraph S by an interval [ pl , pu ] such that |pu−pl| ≤ ε and Pr(S ) ∈ [ pl , pu ] . Let 0 < δ < 1 be a parameter . The algorithm fails to produce such an interval with probability at most δ . This subsection first develops a dynamic programming based method to exactly compute Pr(S ) in exponential time and then extends it to produce desired interval [ pl , pu ] with probability at least 1 − δ in polynomial time . 521 Dynamic Programming Let us begin with a concept originally introduced in [ 13 ] . We say “ a subgraph S occurs in an uncertain graph G ” if S is subgraph isomorphic to at least one implicated graph of G . Naturally , the probability of S occurring in G is Pr(G ⇒ G .
Pr(S )U G ) =
)
( 5 )
) ,
G.∈Imp(G ) , SG where Imp(G ) is the set of all implicated graphs of G , and Pr(G ⇒ G . as given in Equation 1 .
) is the probability of G implicating G .
With the concept given above , the ϕ frequent probability of a subgraph S can be exactly computed via dynamic programming . Suppose the uncertain graphs in D are indexed from 1 to n , ie , D = {G1 , G2 , . . . , Gn} . Let T [ 0n , 0n , 0n ] be a three dimensional array with subscript ranging from 0 to n in each dimension . Each element T [ i , j , k ] of T stores the probability that , across all implicated graph databases of {G1 , G2 , . . . , Gk} ⊆ D , ( 1 ) the number of certain graphs in the implicated graph database is i + j , and
( 2 ) subgraph S is subgraph isomorphic to i certain graphs and is not subgraph isomorphic to j certain graphs in this implicated graph database .
All elements of T can be computed using the following recursive equation . Basically , T [ 0 , 0 , 0 ] = 1 , and T [ i , j , k ] = 0 if i + j > k . For other cases , T [ i , j , k ] can be computed by Equation 6 , where Pr(Gk ⇒ ∅ ) is the probability of Gk implicating a trivial certain graph ∅ , and Pr(S )U Gk ) is the probability of S occurring in Gk as given in Equation 5 . It is apparent that the ϕ frequent probability of S can be computed from T by n )
) min{((1−ϕ)i/ϕ ) , n−i}
Pr(S ) =
T [ i , j , n ] . i=1
( 7 ) Supposing that the exact values of Pr(Gi ⇒ ∅ ) and Pr(S )U Gi ) for 1 ≤ i ≤ n are given as input , the procedure of the dynamic programming is presented in Figure 2 . Obviously , the time complexity of the DP procedure is Θ(n3 ) . j=0
Procedure DP 1 . create array T [ 0n , 0n , 0n ] 2 . T [ 0 , 0 , 0 ] ← 1 3 . for k ← 1 to n do for j ← 0 to k do for i ← 0 to k − j do
4 . 5 . 6 . 7 . compute Pr(S ) using Equation 7 8 . return Pr(S ) compute T [ i , j , k ] using Equation 6
Figure 2 : The dynamic programming procedure DP .
Remark 1 . A similar dynamic programming method has previously been adopted by [ 11 ] to find frequent items over probabilistic data , in which the counterpart of Pr(S )U G )
637 ⎧⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎩
T [ i , j , k ] =
Pr(Gk ⇒ ∅ ) · T [ i , j , k − 1 ] Pr(Gk ⇒ ∅ ) · T [ i , j , k − 1 ] + Pr(S )U Gk ) · T [ i − 1 , j , k − 1 ] Pr(Gk ⇒ ∅ ) · T [ i , j , k − 1 ] +(1 − Pr(Gk ⇒ ∅ ) − Pr(S )U Gk ) ) · T [ i , j − 1 , k − 1 ] Pr(Gk ⇒ ∅ ) · T [ i , j , k − 1 ] + Pr(S )U Gk ) · T [ i − 1 , j , k − 1 ] +(1 − Pr(Gk ⇒ ∅ ) − Pr(S )U Gk ) ) · T [ i , j − 1 , k − 1 ] if i = 0 and j = 0 , if i > 0 and j = 0 , if i = 0 and j > 0 , if i > 0 and j > 0 .
( 6 )
Figure 1 : Recursive equation for computing T [ i , j , k ] , where 0 ≤ i + j ≤ k and 1 ≤ k ≤ n . in the recursive equation is the probability that an item is contained in an x tuple . Actually , the probability of an item being contained in an x tuple can be evaluated in polynomial time . Unfortunately , the complexity result is negative for computing Pr(S )U G ) , which is shown in the following important theorem .
Theorem 5 . It is #P hard to compute the probability , Pr(S )U G ) , of a subgraph S occurring in an uncertain graph G .
Proof . The proof is very similar to the proof of Theo rem 1 in [ 13 ] , so it is omitted here .
Although the time complexity of the DP procedure is O(n3 ) , it is computationally prohibitive to evaluate the required input of the DP procedure . 522 Approximation by Intervals The dynamic programming based method is extended to an approximation algorithm that produce an interval [ pl , pu ] in polynomial time such that |pu − pl| ≤ ε and that Pr(S ) ∈ [ pl , pu ] with probability at least 1 − δ , where 0 < δ < 1 . To this end , an algorithm for estimating Pr(S )U G ) with high accuracy is proposed and then is combined with the dynamic programming procedure DP to get the approximation algorithm for computing [ pl , pu ] . We clarify the details of each step as follows . Estimation of Pr(S )U G ) . The fundamental idea of the algorithm for estimating Pr(S )U G ) is to transform the problem of computing Pr(S )U G ) to the problem of computing the probability of a DNF formula F being satisfied by a randomly and independently chosen truth assignment to the variables of F .
Given an uncertain graph G and a subgraph S , let us construct a DNF formula F first . Let G denote the certain graph obtained by removing uncertainties from G . As a side product of enumerating the children of the parent of S in the search tree , all subgraph isomorphisms from S to G have been obtained . Based on all these subgraph isomorphisms , all embeddings , M1 , M2 , . . . , Mm , of S in G can be obtained . Each embedding Mi is a subgraph of G that S is subgraph isomorphic to . The DNF formula F therefore can be constructed as follows .
Step 1 . Assign a distinct variable to each vertex contained in M1 , M2 , . . . , Mm . The probability of the variable being assigned true is equal to the existence probability of the vertex in G .
Step 2 . Assign a distinct variable to each edge contained in M1 , M2 , . . . , Mm . The probability of the variable be ing assigned true is equal to the conditional existence probability of the edge in G .
Step 3 . For each embedding Mi , construct a clause Ci by conjuncting all variables assigned to the vertices and edges in Mi .
Step 4 . Build F = C1 ∨ C2 ∨ ··· ∨ Cm .
Let Pr(F ) denote the probability of F being satisfied by a randomly and independently chosen truth assignment to the variables of F . It is easy to prove the following lemma .
Lemma 2 . Pr(S )U G ) = Pr(F ) .
The Monte Carlo algorithm proposed in [ 5 ] can be applied to estimate Pr(F ) within absolute error ε with probability at least 1 − δ for arbitrary 0 < ε , δ < 1 .
4 .
2 . assign variable xe to e assign variable xv to v
Procedure Estimate 1 . for all vertex v ∈ V ( M1 ) ∪ V ( M2 ) ∪ ··· ∪ V ( Mm ) do 3 . for all edge e ∈ E(M1 ) ∪ E(M2 ) ∪ ··· ∪ E(Mm ) do 5 . for i ← 1 to m do 7 . F ← C1 ∨ C2 ∨ ··· ∨ Cm 8 . estimate Pr(F ) using the Monte Carlo algorithm [ 5 ] 9 . return the estimated value of Pr(F )
Ci ← . fi ∧ . v∈V ( Mi ) xv e∈E(Mi ) xe fi
6 .
Figure 3 : The algorithm for estimating Pr(S )U G ) .
δ fi
δ
.
.
Figure 3 shows the procedure Estimate of the algorithm for estimating Pr(S )U G ) . Let s be the number of edges of S . Since |V ( Mi)| ≤ |E(Mi)| = s for all 1 ≤ i ≤ n , the fi DNF formula F can be constructed in O(ms ) time . Since F consists of m clauses and each clause contains O(s ) varim2s ε2 log 2 ables , the Monte Carlo algorithm completes in O time [ 5 ] . Thus , the time complexity of the Estimate procedure is O
. We have the following theorem . estimated by Pr(S )U G ) such that Theorem 6 . For any 0 < ε , δ < 1 , Pr(S )U G ) can be Pr(S )U G ) −Pr(S )U G ) fi .
≥ ε m2s ε2 log 2 time , where s is the number of edges of S , in O and m is the number of embeddings of S in the certain graph G obtained by removing uncertainties from G . m2s ε2 log 2
≤ δ
Pr
δ
638 Approximation algorithm . Based on Theorem 6 , an approximation algorithm for computing [ pl , pr ] can be developed . Given an uncertain graph database D = {G1 , G2 , . . . , Gn} , a subgraph S , an error tolerance 0 < ε < 1 and a failure probability tolerance 0 < δ < 1 , the algorithm works in a very simple and elegant way as follows . Step 1 . Compute Pr(Gi ⇒ ∅ ) by Equation 1 for 1 ≤ i ≤ n .
Step 2 . Estimate Pr(S )U Gi ) by Pr(S )U Gi ) within Pr(Gi ⇒ ∅ ) and Pr(S )U Gi ) for 1 ≤ i ≤ n as input . Let Pr(S ) be the output of DP .
Step 4 . Return [ pl , pu ] = [ Pr(S ) − ε/2,Pr(S ) + ε/2 ] .
2n with probability at least ( 1 − δ)1/n
Step 3 . Call the dynamic programming procedure DP with absolute error ε for 1 ≤ i ≤ n .
Since the value of Pr(Gi ⇒ ∅ ) does not depend on the specific input subgraph S , the values of Pr(Gi ⇒ ∅ ) for all 1 ≤ i ≤ n can be computed just at the beginning of the mining algorithm and are reused in all subsequent calls of this approximation algorithm for all input subgraphs . The procedure Approximate of this approximation algorithm is presented in Figure 4 .
Procedure Approximate 1 . for i ← 1 to n do
2 . Pr(S )U Gi ) ← the estimated value of Pr(S )U Gi ) 3 . Pr(S ) ← the output of the DP procedure using Pr(Gi ⇒ ∅ ) andPr(S )U Gi ) for 1 ≤ i ≤ n as input . 4 . return [ Pr(S ) − ε/2,Pr(S ) + ε/2 ]
Figure 4 : The algorithm for approximating Pr(S ) .
The time complexity of the Approximate procedure is
'
O
4m2n3s
ε2 log
2
1 − ( 1 − δ)1/n
, where s is the number of edges of S , and m is the maximum number of embeddings of S in the uncertain graphs in D . The expression of the time complexity can be simplified as follows . Let f ( x ) = 1 − ( 1 − x)1/n be a real function , where 0 ≤ x ≤ 1 and n ∈ N . By Taylor ’s expansion at x = 0 , f ( x ) = f ( 0 ) + f .
= 0 + x n +
( 0 ) · x + 2n ( 1 − 1 1 f ( ξ ) 2
· x2 n )(1 − ξ )
1 n
−2x2 , where 0 ≤ ξ ≤ x . The Lagrange remainder is nonnegative , so f ( x ) ≥ x/n . At x = δ , we have 1 − ( 1 − δ)1/n ≥ δ/n . Therefore , the time complexity of the Approximate procedure can be simplified to
'
O
4m2n3s
ε2 log
2n δ
.
523 Theoretical Guarantees It is obvious that the output [ pl , pu ] of the Approximate procedure satisfies |pr − pl| ≤ ε . The rest of this subsection n ) n )
)
) proves that the Approximate procedure fails with probability at most δ . database D = {G1 , G2 , . . . , Gn} with Pr(S )U Gj ) for 1 ≤ Pr(S )U Gj ) is the estimated value of Pr(S )U Gj ) within
For ease of proof , let DP ( D , i ) denote the output of the dynamic programming procedure DP over uncertain graph j ≤ i and Pr(S )U Gj ) for i + 1 ≤ j ≤ n as input , where 2n with probability at least ( 1 − δ)1/n . We absolute error ε have the following crucial lemma .
Lemma 3 . For 1 ≤ i ≤ n ,
|DP ( D , i− 1)− DP ( D , i)| ≤ | Pr(S )U Gi)−Pr(S )U Gi)| .
Proof . Let DP . be the dynamic programming procedure that is the same as DP except that in line 7 the equation for computing the final result is replaced by min(((1−ϕ)(i+1)/ϕ ) , n−i )
T [ i , j , n ] . i=1 j=0
Also , let DP be the dynamic programming procedure that is the same as DP except that in line 7 the equation for computing the final result is substituted with min(((1−ϕ)i/ϕ)−1 , n−i )
T [ i , j , n ] . i=1
Furthermore , let D .
Pr(S )U Gi ) , and pi =Pr(S )U Gi ) . We have
= D \ {Gi} , qi = Pr(Gi ⇒ ∅ ) , pi = j=0
DP ( D , i − 1 ) = qi · DP ( D . , i − 1 ) + pi · DP .
( D . , i − 1 ) and
+ ( 1 − qi − pi ) · DP
( D . , i − 1 ) ,
DP ( D , i ) = qi · DP ( D . , i − 1 ) +pi · DP .
+ ( 1 − qi −pi ) · DP
( D . , i − 1 ) .
( D . , i − 1 )
By simple mathematics ,
|DP ( D , i − 1 ) − DP ( D , i)|
= |pi −pi| · |DP .
( D . , i − 1 ) − DP and DP
( D . , i − 1)| .
Since the outputs of DP .
|DP ( D , i − 1 ) − DP ( D , i)| ≤ |pi −pi| · 1 = |pi −pi| . Recall that Pr(S ) is the estimated value of Pr(S ) com
Thus , the lemma holds . are both in [ 0 , 1 ] , puted in line 3 of the Approximate procedure . We have the following theorem .
DP ( D , n ) . Then ,
.
2
Theorem 7 . With probability at least 1 − δ ,
Proof . We observe that Pr(S ) = DP ( D , 0 ) andPr(S ) = | Pr(S ) −Pr(S)| = |DP ( D , 0 ) − DP ( D , n)|
( DP ( D , i − 1 ) − DP ( D , i ) )
| Pr(S ) −Pr(S)| ≤ ε n ) ≤ n )
|DP ( D , i − 1 ) − DP ( D , i)| . i=1
= i=1
639 By Lemma 3 ,
|DP ( D , i − 1 ) − DP ( D , i)| with probability at least ( 1 − δ)1/n . Thus ,
≤ | Pr(S )U Gi ) −Pr(S )U Gi)| ≤ ε | Pr(S ) −Pr(S)| ≤ n · ε fin
2n =
.
ε
2
. = 1 − δ .
( 1 − δ)1/n
2n
. fis with probability at least
Consequently , the Approximate procedure fails with prob ability at most δ .
6 . SETTING PARAMETERS
Let the input parameter δ of the Approximate procedure be set to the same value for all input subgraphs S . This section discusses how to set δ to guarantee the overall quality of approximation of the mining algorithm .
Note that , in most cases , it is expected to have τ > 0.5 >
ϕ . Then , we have the following crucial theorem .
1−δ 2
Theorem 8 . The probability of a frequent subgraph S be , where s is ing output as a mining result is at least the number of edges of S . Proof . Let S0 , S1 , . . . , Ss be the subgraphs on the path from the root to S in the search tree , where S0 = ∅ , Ss = S , and Si−1 is the parent of Si for 1 ≤ i ≤ s . Due to the Apriori property of ϕ frequent probability , ie , Lemma 1 , all of S0 , S1 , . . . , Ss−1 are also frequent . Furthermore , since subgraphs are visited in depth first order , S is output as a mining result if and only if all of S0 , S1 , . . . , Ss are output as mining results . Mathematically ,
Pr(S is output ) = Pr(S0 is output )
Pr(Si is output| S0 , S1 , . . . , Si−1 are output ) .
Particularly , Pr(S0 is output ) = 1 , and for 1 ≤ i ≤ s , Pr(Si is output| S0 , S1 , . . . , Si−1 are output ) i=1
· sff = Pr(Pr(Si ) ≥ ϕ − ε/2 ) = Pr(Pr(Si ) −Pr(Si ) ≤ Pr(Si ) − ϕ + ε/2 ) . ≥ Pr(Pr(Si ) −Pr(Si ) ≤ ε/2 ) '
≥ ( 1 − δ)/2 . s
Pr(Si is output| S0 , S1 , . . . , Si−1 are output )
Due to the fact that Pr(Si ) ≥ τ > ϕ and to Theorem 7 ,
Subsequently ,
Pr(S is output ) ≥
1 − δ 2
.
The theorem thus holds .
Let max be the maximum number of edges of a frequent subgraph , which can either be estimated by sampling approaches [ 3 ] or be specified as a constraint on mining results [ 12 ] . From Theorem 8 , we have the following corollary .
Corollary 1 . To ensure that the probability of any frequent subgraph being output is at least 1 − Δ , the parameter δ should be at most 1 − 2 · ( 1 − Δ)1/fimax .
Table 1 : Summary of uncertain graph database . avg(PE ) organism
.
S . pombe ( fission yeast )
D . melanogaster ( fruit fly ) M . musculus ( house mouse )
R . norvegicus ( rat )
A . thaliana ( thale cress )
C . elegans ( worm )
|V | 162 3751 199 130 513 514
|E| 300 7384 286 178 1168 960
0.148 0.456 0.413 0.374 0.444 0.190
7 . EXPERIMENTS
Extensive experiments were performed to evaluate the efficiency of the proposed algorithm and the quality of mining results . The experimental results are shown in this section . 7.1 Experimental Settings
We experimented on a real uncertain graph database that was obtained by integrating the BioGRID database1 with the STRING database2 . The real uncertain graph database contains the protein protein interaction ( PPI ) networks of six organisms in the BioGRID database . A PPI network is an uncertain graph where vertices represent proteins , edges represent interactions between proteins , the labels of vertices are the COG functional annotations of proteins3 provided by the STRING database , the existence probabilities of all vertices are 1 , and the conditional existence probabilities of edges are provided by the STRING database . The summary of the uncertain graph database is shown in Table 1 , where |V | indicates the number of vertices , |E| the number of edges , and avg(PE ) the average value of the conditional existence probabilities of edges .
The proposed algorithm was implemented in C on Linux . All experiments were performed on an IBM X3950 server with 2.4GHz Xeon E7440 CPU and 8GB of RAM . The operating system installed on the server is CentOS . 7.2 Experimental Results 721 Experimental Results on Time Efficiency We first performed experiments to test the execution time of the algorithm with respect to support threshold ϕ , confidence threshold τ and parameters ε and δ . The experimental results are presented and explained as follows .
Execution time vs . ϕ . Figure 5(a ) shows the execution time of the algorithm while ϕ varies from 0.2 to 1 , τ = 0.9 , ε = 0.05 and δ = 005 It can be seen that the execution time decreases significantly as ϕ gets larger . This is because the number of frequent subgraphs reduces quickly as ϕ becomes larger , leading to the decrease in the number of visited subgraphs as shown in Figure 5(b ) .
Execution time vs . τ . Figure 6(a ) shows the execution time of the algorithm while τ varies from 0.6 to 1 , ϕ = 0.2 , ε = 0.05 and δ = 005 The execution time decreases quickly as τ increases . The reason is that fewer subgraphs will have ϕ frequent probability beyond τ as τ becomes larger , resulting in less subgraphs to be visited by the algorithm as shown in Figure 6(b ) .
1
2
3 http://thebiogrid.org http://string db.org http://wwwncbinihgov/COG
640 250
200
150
100
) c e s ( e m i t n o i t u c e x e visited output
25
×
20
) 0 0 1
( s h p a r g b u s f o
15
10 ure 8(a ) illustrates the box plot of the execution time as δ varies from 0.05 to 0.5 , ϕ = 0.2 , τ = 0.9 and ε = 005 The execution time statistically decreases as δ becomes larger . This is because the number of visited subgraphs does not vary significantly as shown in Figure 8(b ) but the time for computing the ϕ frequent probability of each subgraph is proportional to log(2n/δ ) as analyzed in Section 522
50
0 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 threshold φ r e b m u n
5
0 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 threshold φ
( a ) Execution time .
( b ) Number of subgraphs .
Figure 5 : Impact of threshold ϕ on time efficiency .
5
) c e s
0 0 1
4
×
(
3 e m i t
×
) c e s
0 0 1
( e m i t n o i t u c e x e
16 14 12 10 8 6 4 2 0 0.6
6
)
3
0 1
5
×
( s h p a r g b u s f o
4
3
2 r e 1 b m u n
0 0.6
0.7
0.8 threshold τ
0.9
1
0.7
0.8 threshold τ
0.9
1
( a ) Execution time .
( b ) Number of subgraphs .
Figure 6 : Impact of threshold τ on time efficiency .
Execution time vs . ε . Figure 7(a ) shows the execution time of the algorithm as ε varies from 0.025 to 0.2 , ϕ = 0.2 , τ = 0.9 and δ = 005 We can see that the execution time decreases substantially as ε increases . This is due to the following facts . The execution time of the algorithm is dominated by the time for computing the ϕ frequent probabilities of all visited subgraphs . Although the number of visited subgraphs increases as ε gets larger as shown in Figure 7(b ) , the time for computing the ϕ frequent probability of each subgraph is inversely proportional to ε2 as analyzed in Section 522 , thus decreasing the overall time for computing the ϕ frequent probabilities of all visited subgraphs .
10
) c e s
0 0 1
×
( e m i t n o i t u c e x e
8
6
4
2
0
0
0.05
0.1 threshold ε
0.15
0.2
30
25
20
15
10
×
) 0 0 1
( s h p a r g b u s f o r e b m u n visited output
5
0
0
0.05
0.1 threshold ε
0.15
0.2
( a ) Execution time .
( b ) Number of subgraphs .
Figure 7 : Impact of parameter ε on time efficiency .
Execution time vs . δ . Unlike in the previous experiments with respect to ϕ , τ and ε , we can not figure out the relationship between the execution time and δ when running the algorithm only once for each tested value of δ . Hence , we ran the algorithm 30 times for each tested value of δ . Fig visited output
2 n o i t u c e x e
1
0
0.1
0.3
0.2 threshold δ
0.4
0.5
20 visited output
25
) 0 0 1
×
( s h p a r g b u s f o r e b m u n
2.1
1.8
0
0.1
0.3
0.2 threshold δ
0.4
0.5
( a ) Execution time .
( b ) Number of subgraphs .
Figure 8 : Impact of parameter δ on time efficiency .
722 Experimental Results on Quality of Approxi mation
We also evaluated the quality of approximation of the algorithm by testing the precision and recall of mining results . Precision is the proportion of frequent subgraphs in all output subgraphs , and recall is the proportion of output subgraphs in all frequent subgraphs .
Since it is computationally prohibitive to find all strictly frequent subgraphs , we instead use the set of subgraphs obtained for ϕ = 0.2 , τ = 0.9 , ε = 0.02 and δ = 0.001 as all frequent subgraphs with respect to ϕ = 0.2 and τ = 09
The experimental results with respect to parameters ε and
δ are presented and analyzed as follows .
Precision vs . ε . Figure 9 shows the precision of the mining results while ε varies from 0.025 to 0.2 , ϕ = 0.2 , τ = 0.9 and δ = 005 We observe that the precision is over 90 % for ε ≤ 0.05 and goes down as ε gets larger . This is because with the increasing of ε , the number of infrequent subgraphs with ϕ frequent probability within [ τ − ε , τ ) increases , resulting in more infrequent subgraphs been output as results as illustrated in the #OIS row in Table 2 . Moreover , since δ is fixed , the probability of a frequent subgraph being output is also fixed and is at least as proved in Theorem 8 . Thus , the number of output frequent subgraphs is stable and is independent of ε as shown in the #OFS row in Table 2 . The precision is equal to #OFS/(#OFS + #OIS ) , thus it decreases as ε becomes larger . fis
1−δ 2
.
Recall vs . ε . By the arguments above , we know that the number of output frequent subgraphs #OFS is stable and is independent of ε . Moreover , for ϕ = 0.2 and τ = 0.9 , the number of frequent subgraphs is 184 . Since the recall of the mining results is equal to #OFS/184 , we have that the recall does not depend on ε and is over 94 % .
Precision vs . δ . In this experiment , we ran the algorithm 30 times for each tested value of δ . Figure 10(a ) shows the box plot of the precision of the mining results while δ varies from 0.05 to 0.5 , ϕ = 0.2 , ε = 005 It can be seen that
641 ε
#OFS #OIS
0.025 184 8
0.05 181 16
0.075 183 25
0.1 176 33
0.125 176 34
0.15 175 39
0.175 174 49
0.2 180 50
Table 2 : Number of output frequent subgraphs ( #OFS ) and number output infrequent subgraphs ( #OIS ) with respect to threshold ε .
100
)
%
( n o i s i c e r p
95
90
85
80
75
0
0.05
0.1 threshold ε
0.15
0.2
Figure 9 : Impact of parameter ε on precision . the precision is as high as 90 % and does not depend on δ statistically . This is due to the following reasons . Since ε is fixed , the number of infrequent subgraphs with ϕ frequent probability in [ τ − ε , τ ) is also fixed . Although δ varies , the ratio of the probability of a specific frequent subgraph being output to the probability of a specific infrequent subgraph with ϕ frequent probability in [ τ − ε , τ ) being output is a constant . Therefore , the precision of the mining results is stable and is statistically independent of δ .
)
%
( n o i s i c e r p
95 94 93 92 91 90 89 88 87
0
)
%
( l l a c e r
100 99 98 97 96 95 94 93
0
0.1
0.3
0.2 threshold δ ( a ) Precision .
0.4
0.5
0.1 0.2 0.3 0.4 0.5 threshold δ ( b ) Recall .
Figure 10 : Impact of δ on precision and recall .
. fis
Recall vs . δ . By Theorem 8 , the probability of a frequent subgraph being output as a result is at least , thus the recall of the mining results should decrease as δ increases . However , Figure 10(b ) does not show this trend . The reason is that the Monte Carlo algorithm [ 5 ] used in the Estimate procedure for estimating the probability of a DNF formula being satisfied has much higher accuracy in practice than its theoretical lower bound used in the proof of Theorem 8 .
1−δ 2
8 . CONCLUSIONS
In this paper , an approximate mining algorithm has been developed for efficiently and accurately mining frequent subgraphs over an uncertain graph database under probabilistic semantics . The algorithm guarantees to find any frequent subgraph with a provably high probability by carefully setting parameter δ using a systematic method . The extensive experiments on the real uncertain graph database verify that the algorithm is practically efficient and that the mining results have very high precision and recall .
Acknowledgements The research work in this paper was in part supported by the NSF of China under Grant No . 60773063 , the NSFC RGC of China under Grant No . 60831160525 , the National Grand Fundamental Research 973 Program of China under Grant No . 2006CB303000 , and the NSF of China under Grant No . 60903017 .
9 . REFERENCES [ 1 ] C . C . Aggarwal and H . Wang . Managing and Mining
Graph Data . Springer , 2010 .
[ 2 ] T . Bernecker , H P Kriegel , M . Renz , F . Verhein , and
A . Z¨ufle . Probabilistic frequent itemset mining in uncertain databases . In KDD , pages 119–128 , 2009 .
[ 3 ] M . A . Hasan and M . J . Zaki . Output space sampling for graph patterns . PVLDB , 2(1):730–741 , 2009 . [ 4 ] P . Hintsanen and H . Toivonen . Finding reliable subgraphs from large probabilistic graphs . DMKD , 17(1):3–23 , 2008 .
[ 5 ] R . M . Karp and M . Luby . Monte Carlo algorithms for enumeration and reliability problems . In FOCS , pages 56–64 , 1983 .
[ 6 ] A . Kimmig and L . D . Raedt . Local query mining in a probabilistic prolog . In IJCAI , pages 1095–1100 , 2009 . [ 7 ] M . Kuramochi and G . Karypis . An efficient algorithm for discovering frequent subgraphs . IEEE Trans . Knowl . Data Eng . , 16(9):1038–1051 , 2004 .
[ 8 ] L . G . Valiant . The complexity of enumeration and reliability problems . SIAM J . Comput . , 8(3):410–421 , 1979 .
[ 9 ] X . Yan and J . Han . gSpan : Graph based substructure pattern mining . In ICDM , pages 721–724 , 2002 .
[ 10 ] G . Yang . The complexity of mining maximal frequent itemsets and maximal frequent patterns . In KDD , pages 344–353 , 2004 .
[ 11 ] Q . Zhang , F . Li , and K . Yi . Finding frequent items in probabilistic data . In SIGMOD , pages 819–832 , 2008 .
[ 12 ] F . Zhu , X . Yan , J . Han , and P . S . Yu . gPrune : A constraint pushing framework for graph pattern mining . In PAKDD , pages 388–400 , 2007 .
[ 13 ] Z . Zou , J . Li , H . Gao , and S . Zhang . Frequent subgraph pattern mining on uncertain graph data . In CIKM , pages 583–592 , 2009 .
[ 14 ] Z . Zou , J . Li , H . Gao , and S . Zhang . Finding top k maximal cliques in an uncertain graph . In ICDE , pages 649–652 , 2010 .
[ 15 ] Z . Zou , J . Li , H . Gao , and S . Zhang . Mining frequent subgraph patterns from uncertain graph data . TKDE , preprint , 2010 .
642
