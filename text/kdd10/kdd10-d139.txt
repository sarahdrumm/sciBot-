Semantic Relation Extraction With Kernels Over Typed
Dependency Trees
Frank Reichartz
Gerhard Paass
Hannes Korte Fraunhofer IAIS
Schloss Birlinghoven St . Augustin , Germany
{frank.reichartz | hannes.korte | gerhardpaass}@iaisfraunhoferde
ABSTRACT An important step for understanding the semantic content of text is the extraction of semantic relations between entities in natural language documents . Automatic extraction techniques have to be able to identify different versions of the same relation which usually may be expressed in a great variety of ways . Therefore these techniques benefit from taking into account many syntactic and semantic features , especially parse trees generated by automatic sentence parsers . Typed dependency parse trees are edge and node labeled parse trees whose labels and topology contains valuable semantic clues . This information can be exploited for relation extraction by the use of kernels over structured data for classification . In this paper we present new tree kernels for relation extraction over typed dependency parse trees . On a public benchmark data set we are able to demonstrate a significant improvement in terms of relation extraction quality of our new kernels over other state of the art kernels .
Categories and Subject Descriptors I27 [ Artificial Intelligence ] : Natural Language Processing—text analysis
General Terms Algorithms , Design
Keywords Relation Extraction , Information Extraction , Text Mining
1 .
INTRODUCTION
Often the semantic content of natural language sentences may be described by relations between entities . For example the sentence “ Google buys AdMob for $750 Million in stock ” can approximately be described by the relation Acquire(Google,AdMob ) . Note , that this relation can be expressed in various other ways , eg as “ Google plans to ac quire AdMob ” or “ Google purchase of AdMob gets closer antitrust review ” .
Classification methods are a way to detect such variant verbalizations . The automatic extraction of the semantic content of text is important for many practical problems like the automatic gathering of relationships between entites from large text corpora for the contruction of knowledge bases . This problem is especially relevant as , for example , investment firms and banks are naturally interested in automatically acquired knowledge from news ticker articles . In a first step possible relation arguments are extracted by entity recognition methods performing mention detection , named entity recognition[21 ] and co reference resolution . In a second step a classifier decides for each pair of possible arguments whether the sentence expresses the target relation between the two arguments . It is obvious that many syntactic and semantic properties of a sentence have to be evaluated to detect relations . Parse trees provide extensive information on syntactic structure and can be automatically generated by parsers . Recently tree kernels have been developed which operate on parse trees for classification purposes in kernel based classifiers . Those kernels allow to compare large ( often exponential , or in some cases , infinite ) numbers of characteristics of trees in polynomial time .
There are several variants of syntactic parse trees[15 ] . In contrast to phrase grammar parse trees having nonterminal structural nodes , dependency parse trees only consist of word nodes , therefore allowing for a bijective mapping between words in the sentence and the nodes in the tree . Their links characterize directly the syntactic dependency of words and may either be untyped or typed with edge labels providing additional information about the syntactic and semantic function of a link . While tree kernels for relation extraction have been proposed for phrase grammar parse trees [ 29 ] and untyped dependency parse trees [ 7 , 22 , 25 ] there are currently no kernels available taking into account subtree similarity in node labeled and edge labeled trees , eg typed dependency parse trees .
In this paper we describe a new approach for relation extraction based on kernels for typed dependency trees which deeply integrates the edge labels in the kernel computations . On a public benchmark data set we show that on average the new kernels outperform other kernels by 2.4 % F Score on coarse grained relations which is an error reduction of 75 % On some fine grained relations our new kernels outperform other kernels by up to 9.4 % F Score and reduce the error by up to 279 % The results provide strong evidence that
773 To(a ) : a1
5 : president advmod nsubj cop det prep a2
1 : Recently a3
2 : Obama a4
3 : became a5
4 : the
To(b ) : b1
5 : CEO nsubj cop advmod det prep b2
1 : Ballmer b3 2 : is b4
3 : still b5
4 : the b6
6 : of pobj b7
7 : Microsoft a6 6 : of pobj a7
8 : USA det a8
7 : the
Figure 1 : The typed dependency trees of two example sentences . The nodes are labeled with the position of the word in the sentence and the word itself . The edges are labeled with the type of the dependency . A thick border marks an entity mention . the usage of our typed dependency tree kernels for relation extraction significantly improves the extraction quality .
The remainder of the paper is organized as follows . We start by giving a short overview of the related work and outline typed dependency parse trees . After introducing our notation we define and elaborate our novel typed dependency tree kernel . Subsequently we evaluate the kernel and compare it to previous approaches using a public benchmark data set for training and testing . Finally we summarize the results and give a conclusion .
2 . RELATED WORK
An early approach to relation extraction is based on patterns [ 12 ] , usually expressed as regular expressions . The underlying hypothesis assumes that terms sharing similar linguistic contexts are connected by similar semantic relations . The manual identification of these patterns inspired other approaches in which the initial set of patterns is used to seed a bootstrapping process that automatically acquires additional patterns for relations [ 1 ] . However , the meaning of the extracted relations is often quite vague . Various authors follow related approaches , eg using frequent itemset mining to extract word patterns [ 3 ] . Others take into account the sequential structure of sentences , eg by conditional random fields [ 5 ] . These authors annotate words with rich features , eg part of speech information , and tag the words of the relation arguments by labels . The model is trained using manually annotated data and can , for instance , successfully predict semantic relations between diseases and treatments in the medical domain . A similar approach is used by [ 2 ] .
Syntactic parse trees provide detailed information on syntactic structure and can , for instance , represent the rela tion between subject , verb and object in a sentence . For feature based methods only a limited number of structural properties may be compared . Using parse trees [ 13 ] employ logic based frequent structural patterns mining for relation extraction with promising results . Based on concepts of [ 28 ] and string kernels , [ 7 ] proposed the Dependency Tree Kernel to transform parse trees for the use in support vector [ 6 ] investigated the Shortest Path Kermachine classifiers . nel that computes similarities between nodes on the shortest path of a dependency tree that connects the entities . [ 25 ] suggested the Path Dependency Tree Kernel and All pairs Dependency Tree Kernel for untyped dependency parse trees yielding improved extraction accuracy .
All those kernels utilize sophisticated node similarity functions . [ 22 ] altered a node labeled tree to include new artificial nodes representing semantical dependencies and employed the partial tree kernel [ 20 ] , which counts the common partial trees without considering a fine grained node simililarity . Kernels for other types of parse tree paradigms eg phrase grammar parse trees , have also been proposed [ 29 ] . All these kernels can be used inside a kernel classifier eg a Support Vector Machine ( SVM ) [ 27 ] .
Besides work on tree kernels for relation extraction there have been tree kernels proposed for other tasks like question classification [ 20 ] which have shown improvements over bagof words approaches . Considering features associable with words , [ 10 ] proposed to use semantic background knowledge from various sources like WordNet , FrameNet and PropBank to enhance the Dependency Tree Kernel of [ 7 ] yielding good results .
774 dep dependent aux auxiliary auxpass passive auxiliary cop copula conj conjunct cc coordination arg argument subj subject nsubj nominal subject nsubjpass passive nominal subject csubj clausal subject comp complement obj object dobj direct object iobj indirect object pobj object of preposition attr attributive agent agent ref referent expl expletive ( expletive there ) mod modifier advcl adverbial clause modifier purpcl purpose clause modifier tmod temporal modifier rcmod relative clause modifier amod adjectival modifier infmod infinitival modifier partmod participial modifier num numeric modifier number element of compound number appos appositional modifier nn noun compound modifier abbrev abbreviation modifier advmod adverbial modifier sdep semantic dependent xsubj controlling subject
Figure 2 : Abbreviated list of the 48 different link labels for the Stanford Parser . The hierarchy is indicated by indentation .
3 . TYPED DEPENDENCY TREES
The grammatical dependency between the words of a sentence can be represented by a typed dependency tree which is a labeled directed tree where each node corresponds to a word [ 17 ] . The structure is determined by the relation between a word ( a head ) and its dependents . Dependent words in turn can be the head for other words yielding a tree structure . The edges of typed dependency trees are labeled by a number of attributes [ 8 ] , eg as “ attributive ” , “ marker ” or “ relative ” . An overview of the edge types is given in figure 2 . Dependency trees are well suited to represent the sentence structure for languages with free word order eg German in a structured way . In this paper we consider typed and untyped dependency trees [ 9 ] as generated automatically by the Stanford Parser [ 16 ] . As an example consider the two sentences a = “ Recently Obama became the president of the USA ” and b = “ Ballmer is still the CEO of Microsoft ” which have the tree representations as shown in figure 1 .
4 . NOTATION
∗
We denote the typed dependency parse tree of a sentence w as T ( w ) = ( V , E ) which is a directed ordered tree with nodes V and edges E . Each node vi is labeled with a word and each edge is labeled with a type as provided by a parser . For each node v ∈ V the ordered sequence of its m children ch(v ) = ( u1 , . . . , um ) is given by the positions of the corresponding words in the sentence . We denote the set of all possible subsequence index sets of sequence s of length k as Ik . As in [ 26 ] the subsequence defined by the ordered sequence o(i ) of an index subset i is called s ( i ) . We can write the subsequence of children referenced by i from a node v of an ordered tree as ch ( v , i ) . In figure 1 , for example , ch(a1 ) = ( a2 , a3 , a4 , a5 , a6 ) has the index set {1 , 2 , 3 , 4 , 5} as it has 5 children . The subset {2 , 3} of this set in its representation as ordered sequence ( 2 , 3 ) defines the subsequence ch(a1 , ( 2 , 3 ) ) = ( a3 = “ Obama ” , a4 = “ became ” ) of child nodes of a1 .
In this paper we consider relations between two entities . Those entities can be automatically tagged and categorized by named entity recognition tools . We treat multiword entities as a single word w , eg the two words “ Steve Ballmer ” will be merged to a new word “ Steve Ballmer ” . The lowest common ancestor lca(v , w ) of two nodes v , w ∈ V in a directed tree T = ( V , E ) is the lowest node in T which has both v , w as descendants . We define the set of nodes sub(v , w ) of two nodes v , w as the set containing all nodes of the complete subtree of a tree T rooted in lca(v , w ) . In a directed tree T we define the implied path p(u , v ) between two nodes u , v as the sequence ( u , . . . , lca(u , v ) , . . . , v ) of nodes where u , . . . are the nodes on the path from the lca(u , v ) to u , analogous for v . As in a tree a node has always just one incoming edge we denote the label of this edge of a node u as in(u ) .
5 . TYPED DEPENDENCY TREE KERNELS Our development is motivated by the Dependency Tree Kernel [ 7 ] which is defined over untyped dependency trees , and thus ignores types of dependencies ( edge labels ) between the tree nodes . We adapt the Dependency Tree Kernel by introducing the similarity of two edge labels as a decay factor for the whole underlying subtree . To weaken this strong influence of the edge labels we introduce a new parameter α , which defines the weight of the edge similarity in the computation of the subtree similarity . The resulting kernel over typed dependency trees is called Typed Dependency Kernel ( TDK ) .
5.1 Typed Dependency Kernel
We propose the TDK which compares two dependency parse trees starting from the lowest common ancestors , ie the root nodes of the smallest subtrees enclosing both target entities of the respective instances . The tree nodes are characterized by different features like the word itself ( “ Obama ” , “ became ” , . . . ) , the part of speech tag ( “ NNP ” , “ VBD ” , . . . ) or the general part of speech ( “ Noun ” , “ Verb ” , . . . ) , etc . An overview of the features is given in table 1 .
To compare the relations in two instances X = ( x , ( x1 , x2) ) , Y = ( y , ( y1 , y2 ) ) with trees x , y and relation argument nodes ( x1 , x2 ) , ( y1 , y2 ) the TDK applies the node kernel Δ to the lowest common ancestors ( LCA ) of the relation argument nodes :
KTDK(X , Y ) = Δ(lca(x1 , x2 ) , lca(y1 , y2 ) )
( 1 )
775 Name
Values word POS general POS chunk tag entity type mention type hypernym relation argument
Balmer , CEO , . . . NN , NNP , . . . noun , verb , adj , . . . NP , VP , ADJP , . . . person , location , . . . name , nominal , . . . executive , city , . . . 1 , 2 or n/a
Table 1 : List of features assigned to every node which are considered in the similarity function .
⎧⎪⎨ γ · ⎪⎩0
The typed node kernel Δ is defined as
Δ(u , v ) = sim(u , v)+ C(ch(u ) , ch(v ) ) if mat(u , v ) = 0 otherwise
The matching function mat(u , v ) → 0 , 1 checks whether the nodes u and v are compatible , which is the case if generalPOS , entity type and relation argument match . The similarity function sim(u , v ) → N counts the number of matching features of two nodes u , v . To include the edge labels ( the dependency types ) into the kernel we define an additional decay factor
γ = 1 − α + α · edgeSim(in(u ) , in(v ) )
( 2 ) where α ∈ [ 0 , 1 ] is the amount of influence of the edge label similarity . The edge similarity of the incoming edges of two nodes u and v is given by edgeSim(in(u ) , in(v ) ) → [ 0 , 1 ] . It is a symmetric function , whose concrete characteristics are described in section 6 . For α = 0 the edge label similarity is not considered , and thus the TDK is equal to the Dependency Tree Kernel [ 7 ] . The edge similarity therefore weights the similarity of the subtrees below two nodes .
The recursive computation of the underlying substruc tures is performed by C(ch(u ) , ch(v) ) , which is defined as
C(s , t ) = ( i∈I|s| , j∈I|t| ,
|i|=|j|
λd(i)+d(j ) · Δ fi
( s(i ) , t(j ) ) · M ( s(i ) , t(j ) ) where s , t are sequences of nodes and s(i ) and t(j ) are the subsequences of nodes implied by the index sets i , j . Furthermore the parameter 0 < λ ≤ 1 is a penalty factor for lengths and gaps of the subsequences , and d(i ) is the covered fi distance of the index sequence i . With Δ the node kernel Δ can be called on node sequences : n( fi
Δ
( (s1 , . . . , sn ) , ( t1 , . . . , tn ) ) =
Δ ( sk , tk ) k=1
The function M checks if a sequence of nodes matches according to the matching function mat : n )
M ( (s1 , . . . , sn ) , ( t1 , . . . , tn ) ) = mat(sk , tk ) k=1
For the runtime critical kernel computations we employ the index caching strategy proposed by [ 25 ] for the Dependency
Tree Kernel which allows for a general efficient computation of subsequence functions like C(s , t ) .
Based on the TDK we can adapt strategies proposed by [ 25 ] for the Dependency Tree Kernel to the typed case which will allow for an efficient consideration of the structure of the subtrees in a more sophisticated way . 5.2 All Pairs TDK
Let Vx and Vy be the sets of nodes contained in the respective subtrees provided by the lowest common ancestors of the relation argument nodes of the two instances x and y . The All Pairs TDK applies the node kernel to every combination of nodes contained in these sets :
(
(
KAll Pairs TDK(X , Y ) =
Δ(u , v ) u∈Vx v∈Vy where Δ(u , v ) is the Typed Dependency Kernel . This kernel considers the similarity of every possible pair of nodes below and on the shortest path based on the similarities of the subtrees computed by the node kernel Δ of the TDK . 5.3 Path TDK
The All Pairs TDK considers every node pair including substructures possibly irrelevant for the current relation and therefore introducing noise . This kernel follows the shortest path hypothesis [ 6 ] which states that the most valuable information is contained on the shortest path between the two relation argument nodes . However [ 25 ] showed that the substructure below the nodes on the path also holds valuable clues and proposed the Path Dependency Tree Kernel . We adapt this strategy to the typed case :
KPath TDK(X , Y ) =
( i∈I|x| , j∈I|y| , |i|=|j| , d(i),d(j)≤q
μd(i)+d(j ) · Δ fi
( x(i ) , y(j ) ) · M ( x(i ) , y(j ) ) fi ( x(i ) , y(j ) ) is the Typed Dependency Tree Kernel where Δ applied to all possible subsequences of nodes on the shortest path between the relation argument nodes . Ik is the set of all index sequences and d(i ) = max(i ) − min(i ) + 1 is the covered distance . The parameter 0 < μ ≤ 1 is a regularization factor for gaps in the subsequence and q ∈ N is the limitation on the length of the subsequences of nodes on the shortest path . The function M is the sequence matching function defined in section 51 Therefore Path TDK considers the similarity of the substructures from node pairs on the shortest path between two relation argument nodes .
Because it has been proven that the sum and product of valid kernels are also valid kernels [ 26 ] and that the set of all valid kernels is closed under scalar multiplication [ 11 ] , it is clear that KTDK is a kernel because it is based on the tree child node subsequence similarity function which has been shown to be a kernel [ 28 ] . The same holds for KAll Pairs TDK and KPath TDK which extend KTDK only by scalar multiplication and sums and products of kernels . Thus , all kernels proposed in this work satisfy Mercer ’s condition . 5.4 Efficient Computation
For the efficient computation of the TDK , which underlies the All Pairs TDK and the Path TDK , we looked into different computation approaches for C(s , t ) , the child subsequence similarity , which counts the matching subsequences
776 Kernel
α = 0.0 α = 0.1 α = 0.2 α = 0.3 α = 0.4
α = 0.5
α = 0.6 α = 0.7 α = 0.8 α = 0.9 α = 1.0
TDK 64.4 ( .2 ) 66.2 ( .2 ) 66.2 ( .2 ) 66.3 ( .2 ) 66.4 ( .2 ) 66.4 ( .3 ) 66.5 ( .3 ) 66.5 ( .3 ) 66.5 ( .2 ) 66.4 ( .2 ) 66.5 ( .2 ) All Pairs TDK 66.4 ( .1 ) 68.3 ( .2 ) 68.5 ( .2 ) 68.7 ( .2 ) 68.8 ( .2 ) 68.9 ( .2 ) 68.9 ( .2 ) 68.9 ( .2 ) 69.0 ( .2 ) 69.0 ( .2 ) 69.1 ( .2 ) 70.4 ( .2 ) 72.2 ( .2 ) 72.4 ( .1 ) 72.5 ( .2 ) 72.7 ( .1 ) 72.7 ( .1 ) 72.6 ( .1 ) 72.5 ( .1 ) 72.6 ( .2 ) 72.5 ( .2 ) 72.3 ( .2 ) Path TDK
Table 2 : The Fmicro scores for different values of the edge similarity weight parameter α and different typed dependency tree kernels with 5 times repeated 5 fold cross validation . The values in parenthesis denote the standard error . in an ordered labeled tree . For the computation of this function different approaches were suggested . [ 28 ] proposed an O(mn3 ) algorithm ( Zelenko ) for sequence lengths n ≤ m . Another approach was later proposed by [ 25 ] yielding an O(mn2 ) solution based on dynamic programming ( DP ) . However as those two approaches were motivated by string subsequence enumeration approaches [ 18 , 26 ] they are most efficient for very large sequences . Especially their actual runtime for small values is dominated by the initialization of helper variables . For smaller sequence lengths the index cache strategy has been shown to be more efficient [ 25 ] .
As the average outdegree of the nodes in the typed dependency trees is just 2.02 the application of the index caching strategy for the computation of C(s , t ) is promising . Let SIp,q = {o(i)|i ∈ Ip ∧ |i| ≤ q} be the set of all ordered index subsequences of length q with highest index p . We can write the child subsequence kernel C(s , t ) as
C(s , t ) = min(m,n)(
(
( q=1 i∈SIm,q j∈SIn,q
λd(i)+d(j ) · Δ fi
( s(i ) , t(j ) ) · M ( s(i ) , t(j ) ) with m = |s| and n = |t| . This reformulation of the child subsequence formula allows an efficient computation by utilizing a cache for the Δ function . The computation procedure called index caching is described in the following pseudo code . It is easy to see that the worst case runtime ( if all nodes match ) of this algorithm for n ≤ m is ff ffff n(
O q=1 q ·
· n q m q x ← 0 for r = 1 : q do for i ∈ SIm,q do for j ∈ SIn,q do
Algorithm Index Cache : For sequences of nodes s , t compute C(s , t ) with prepared index sequences SI . 1 : k ← 0 , m ← |s| , n ← |t| , p ← min(m , n ) 2 : for q = 1 : p do 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : end for 16 : return k goto 4 // jump to next index sequence j end if x ← x + Δ(s(i(r) ) , t(j(r) ) ) end for k ← k + x · λd(i)+d(j ) if mat(s(i(r) ) , t(j(r) ) ) = 0 then end for end for
We looked at the distribution of the different node degrees m = |s| , n = |t| in typed dependency trees which governs the computation of C(s , t ) . The distribution of the different combination during the computation of C(s , t ) is shown in figure 3 . In addition the exact theoretical runtime ( including cost for initialization of helper variables ) of the three possible approaches for the computation of C(s , t ) is depicted . As can be seen the index cache strategy beats the other two computation approaches in exact runtime in 94.3 % of the cases or is only slightly slower . Only in the infrequent cases where m , n ≥ 5 the other solutions are theoretically faster . In order to show that this is useful in practice , we performed an empirical runtime analysis over all possible combinations of typed dependency trees of the whole corpus . We performed five consecutive runs of these experiments on the same machine . The results are depicted in table 3 . The average absolute runtime for the index cache was 76.9 minutes . This provides strong evidences that the utilization of the index cache strategy is indeed reasonable for the computation of the TDK , AP TDK , and Path TDK .
Algorithm
Index Cache [ 25 ] fi
Theoretical runtime q=1 q · n m q
·
DP [ 25 ]
Zelenko [ 28 ] mn2 mn3 n q
Rel . emp . runtime
Std . dev .
1
2.2013
2.1175
–
0.1057
0.1055
Table 3 : Empirical runtimes of the TDK with three different subsequence algorithms . The runtimes are averaged over five measurements and relative to the fastest algorithm .
6 . EDGE SIMILARITY
The label of an edge in a typed dependency tree expresses a syntactic relation between the corresponding words . The label nsubj ( nominal subject ) for the edge a1 → a3 in figure 1 , for example , implies that a3 = ” Obama ” is the syntactic subject of a1 = ” president ” . Some edge labels express very similar syntactic relations being quite dissimilar to other relations . For example the label type nsubj is syntactically far more similar to csubj ( clausal subject ) than it is to the tmod temporal modifier label .
The hierarchical edge label set of the Stanford Parser [ 8 , 9 ] is shown in figure 2 . We used it to define the similarity of edge labels for our typed dependency tree kernel . The hierarchy is a tree H in which each node is labeled with a type and has edges to its subtypes . The root dep of H is the most general type , whose children are the more specific subtypes like aux ( auxiliary ) and arg ( argument ) . With
777 y c n e u q e r f e c n e r r u c c o s h t g n e l e c n e u q e s f o
%
30
20
10 m n
94.3 %
Index Cache [ 25 ]
DP [ 25 ]
Zelenko [ 28 ] e m i t n u r l a c i t e r o e h t
2×
1× ×
1 2
1 1
2 1
3 1
4 1
3 2
2 2
5 1
4 2
4 3
3 3
5 2
6 1
5 3
6 2
7 1
. . .
5 5
. . .
10 10
Figure 3 : The algorithm runtimes and the occurrence frequencies . The vertical bars represent the occurrence frequency of sequence length combinations in the whole corpus . The lines denote the exact theoretical runtimes ( including initializations ) of the different algorithms relative to the Index Cache algorithm runtimes . In 94.3 % of the cases the Index Cache algorithm is the fastest or only slightly slower than the other algorithms . p(u , v ) denoting the path between two nodes u , v in H , there are various ways of extracting a symmetric similarity score out of the dependency type hierarchy . Our preliminary experiments revealed that the difference between alternatives are minor . Therefore we only report the best performing choice which is the reciprocal of the path length and omitted the other ones for brevity . We define the similarity between edge labels as edgeSim(u , v ) =
1
|p(u , v)| which is higher if the path between the labels in figure 2 is short . Please note that p(u , u ) = ( u ) . For example we get edgeSim(nsubj , csubj ) = 1/3 as p(nsubj , csubj ) = ( nsubj , subj , csubj ) edgeSim(tmod , csubj ) = 1/6 as p(tmod , csubj ) = ( tmod , mod , dep , arg , subj , csubj )
This example outlines that two semantically more related types are assigned a higher similarity then two less related . The TDK has the same computational complexity as the Dependency Tree Kernel [ 7 ] as for all possible pairs of edge labels the edgeSim function can be calculated in advance and be retrieved in O(1 ) .
7 . EXPERIMENTS
In this section we compare the performance of our new kernels for relation extraction with the performance of other dependency tree kernels . As benchmark data set we use the public ACE 2003 corpus [ 19 ] . 7.1 Data Set
The Automatic Content Extraction ( ACE ) evaluation organized by the National Institute of Standards and Technology ( NIST ) covers the task of relation extraction . During this evaluation an annotated corpus was created suitable for supervised learning approaches . The ACE 2003 data set [ 19 ] consists of 519 documents which are all news related , eg newspaper articles or newswire texts . This data set comes with a pre specified training and test set . The training set contains texts from the time period between January
1998 and July 1998 whereas the test contains document from October 1998 to December 1998 . In all those documents entities and relations between the entites were annotated by human annotators . The annotation of entity mentions included their type , eg person or organization as well as to their mention type , ie named , nominal or pronominal . In total 24 different fine grained relations like Located and Affiliate partner were annotated , which were grouped into the following 5 top level relations : At ( 587 Instances ) , Near ( 58 Instances ) , Part ( 329 Instances ) , Role ( 887 Instances ) and Social ( 59 Instances ) .
7.2 Data Preparation
As our approach operates on parse trees we used the Stanford Parser [ 16 ] , a freely available state of the art parser producing parse trees with a high quality . We have run the parser on each document of the data set and generated a typed and untyped parse tree for each sentence . Using the manually annotated entities each dependency tree was subsequently processed by our system , which first merged the multi word entities and then associated each node with information like part of speech tags , entity type or mention type1 . As currently neither nominal nor pronominal co reference resolution can be done with sufficient quality , we restricted our experiments to named –named relations , where named entity recognition approaches may be used to extract the arguments in real world scenarios without a labeled corpus . Nevertheless our kernels without any modification can also be applied to the all mention types setting as well . For each sentence we generated instances , where n is the number of named entity mentions in the sentence . Those instances together with the label of the relation type between the corresponding entity mention pairs are used as the input for the kernel based classifier . For this multi class classification problem we used the onevs all classification approach and selected the class with the highest confidence .
· 2 = n · ( n − 1 ) n 2
1Also known as entity mention level
778 Kernel
5 times repeated 5 fold cross validation pre specified test set
Prec
Rec
Fmicro
Fmacro
Prec
Rec
Fmicro
Fmacro
Averaged over the five top level relations
SPK [ Bunescu05 ] DK [ Culotta04 ] TDK All Pairs DK [ Reichartz09 ] All Pairs TDK Path DK [ Reichartz09 ] Path TDK
81.7 72.7 75.3 73.1 76.5 79.2 81.1
Averaged over the 24 subrelations
SPK [ Bunescu05 ] DK [ Culotta04 ] TDK All Pairs DK [ Reichartz09 ] All Pairs TDK Path DK [ Reichartz09 ] Path TDK
76.8 66.1 69.8 70.1 73.5 76.3 77.5
47.1 56.4 59.5 60.2 62.6 63.1 65.9
33.3 46.8 50.0 51.8 53.8 53.7 56.3
59.8 ( 0.1 ) 63.5 ( 0.3 ) 66.5 ( 0.3 ) 66.0 ( 0.2 ) 68.9 ( 0.2 ) 70.3 ( 0.1 ) 72.7 ( 0.1 )
46.5 ( 0.2 ) 54.8 ( 0.2 ) 58.3 ( 0.2 ) 59.6 ( 0.2 ) 62.1 ( 0.1 ) 63.1 ( 0.3 ) 65.2 ( 0.2 )
35.1 ( 0.2 ) 46.1 ( 0.9 ) 50.8 ( 1.0 ) 59.7 ( 0.6 ) 62.6 ( 0.7 ) 59.0 ( 0.4 ) 62.1 ( 0.9 )
19.6 ( 0.2 ) 25.8 ( 0.2 ) 27.6 ( 0.1 ) 31.5 ( 0.1 ) 32.8 ( 0.2 ) 32.2 ( 0.2 ) 33.7 ( 0.1 )
72.7 77.5 78.5 77.8 78.8 76.1 79.1
64.2 73.6 75.1 74.6 77.3 69.6 74.5
34.9 46.3 46.9 51.3 51.0 54.0 55.2
20.9 38.2 39.7 40.3 39.7 39.7 42.7
47.2 57.9 58.7 61.9 62.0 63.2 65.0
31.5 50.3 52.0 52.3 52.5 50.6 54.3
25.4 39.3 40.1 41.9 42.2 42.3 43.9
10.7 15.3 15.0 18.6 15.1 17.3 18.4
Table 4 : Precision , recall , and F scores in % for 5 times repeated 5 fold cross validation and on the prespecified test set . The values in parenthesis denote the standard error . The upper and lower table gives results averaged over the five top level relations and the 24 subrelations , respectively . Here SPK denotes the Shortest Path Kernel [ 6 ] , DK denotes the Dependency Tree Kernel [ 7 ] and All Pairs , Path DK denote the All Pairs , Path Dependency Tree Kernel [ 25 ] .
7.3 Experimental setup
As the kernel classifier we use the Support Vector Machine implementation SV M light[14 ] and implemented all reported tree kernels . Because the preprocessing and the experimental setup is crucial , this was the only way which allows a fair comparison of the different approaches .
For the state of the art kernels we used the same kernel and classifier parameters as in the cited papers . To avoid a fitting of free parameters to the test data and reporting biased results they were determined on data different from the test set . To estimate the variability and significance of differences between methods we have conducted a 5 times repeated 5 fold cross validation [ 4 ] . In addition we compared the performances on the pre specified ACE test set .
We report the usual evaluation measure Precision and Recall , the F score averaged over the classes ( Fmacro ) and averaged over the instances ( Fmicro ) . For the cross validation runs we report the standard error of F score as estimated according to [ 4 ] .
We investigated the sensitivity of the F score to the value of the parameter α which determines the influence of the edge labels on the similarity of parse trees . The results are shown in table 2 . For all weights except 0 we get similar F scores . For the Path TDK , for example , we get scores ranging from 72.2 % to 72.7 % whereas ignoring the edge labels ( α = 0 ) leads to a significant decrease of the F score to 704 % This indicates a low sensitivity of the results to the exact value of α . 7.4 Results
The upper part of table 4 describes the performance for top level relations . It shows an overview of the results of different kernels on the typed dependency trees on the cross validation and the pre specified test set . For the cross validation the Path TDK outperforms all other kernels by at least 2.4 % Fmicro and 3.1 % Fmacro . In addition the typed kernels TDK , All Pairs TDK and Path TDK all beat their untyped counterparts DK , All Pairs DK and Path DK with an Fmicro difference of at least 2.4 % and an Fmacro difference of at least 29 % The difference in F values between the two best kernels Path DK and Path TDK is significant at a level of 99.9 % according to the corrected resampled ttest [ 4 ] . Hence the edge labels utilized in typed dependency tree kernels provide essential information to improve relation extraction performance .
On the separate test set the Path TDK achieved an Fmicro of 65.0 % which is an improvement of 2.8 % over the second best kernel . The overall reduction in classification accuracy compared to the cross validation can be explained by the fact that the training documents are from a different time period then the test documents . Again the typed tree kernels show a better performance than their untyped variants , except for All Pairs DK and All Pairs TDK , which have nearly equal F scores .
In the lower part of table 4 results for the more specific 24 subrelations are reported . This table gives a similar picture as for the top level relations . Again the Path TDK exhibits the best performance , yielding a 2.1 % improved Fmicro which is highly significant at a level of 99.9 % according to the corrected resampled t test . Again all typed dependency tree kernels do better than their untyped counterparts .
In table 5 results for the three top level relations At , Part and Role with the largest number of training examples are shown . Again the typed dependency kernels have the best results for the cross validation as well as for the prespeci
779 Relation
Kernel
5 times rep . 5 fold cross validation Prec
F score .
Rec
At
Part
Role
SPK [ Bunescu05 ] DK [ Culotta04 ] TDK All Pairs DK [ Reichartz09 ] All Pairs TDK Path DK [ Reichartz09 ] Path TDK
SPK [ Bunescu05 ] DK [ Culotta04 ] TDK All Pairs DK Reichartz09 ] All Pairs TDK Path DK [ Reichartz09 ] Path TDK
SPK [ Bunescu05 ] DK [ Culotta04 ] TDK All Pairs DK [ Reichartz09 ] All Pairs TDK Path DK [ Reichartz09 ] Path TDK
79.7 64.7 65.7 64.3 68.7 72.4 75.6
77.1 69.5 80.6 68.7 79.6 80.2 86.5
83.8 80.5 80.8 79.9 80.6 83.3 83.3
44.1 47.1 49.6 53.9 57.4 58.8 62.3
26.4 55.9 67.4 54.0 61.7 59.1 68.4
62.4 68.5 68.9 69.1 69.2 71.9 71.9
56.8 ( 0.4 ) 54.5 ( 0.3 ) 56.5 ( 0.4 ) 58.6 ( 0.3 ) 62.6 ( 0.4 ) 64.9 ( 0.3 ) 68.3 ( 0.2 )
39.4 ( 0.5 ) 62.0 ( 0.3 ) 73.4 ( 0.3 ) 60.4 ( 0.6 ) 69.5 ( 0.4 ) 68.1 ( 0.3 ) 76.4 ( 0.2 )
71.5 ( 0.0 ) 74.0 ( 0.3 ) 74.4 ( 0.3 ) 74.1 ( 0.3 ) 74.5 ( 0.3 ) 77.1 ( 0.1 ) 77.1 ( 0.2 ) pre specified test set
Prec
58.6 65.6 60.9 63.2 67.1 62.4 65.0
60.0 87.1 90.6 87.5 82.9 87.9 83.3
84.1 83.2 83.2 84.7 86.7 83.1 86.0
Rec
33.0 40.8 40.8 46.6 47.6 51.5 50.5
14.3 42.9 46.0 44.4 46.0 46.0 47.6
49.0 55.6 55.6 62.3 60.3 64.9 64.9
F score
42.2 50.3 48.8 53.6 55.7 56.4 56.8
23.1 57.4 61.1 58.9 59.2 60.4 60.6
61.9 66.7 66.7 71.8 71.1 72.9 74.0
Table 5 : Precision , recall , and F scores in % for 5 times repeated 5 fold cross validation and on the prespecified test set for the three top level relations with most training examples . The values in parenthesis denote the standard error . Here SPK denotes the Shortest Path Kernel [ 6 ] , DK denotes the Dependency Tree Kernel [ 7 ] and All Pairs , Path DK denote the All Pairs , Path Dependency Tree Kernel [ 25 ] . fied test set . In most cases the Path DTK has optimal performance except for the Part relation , where the TDK is slightly better for the prespecified test set .
Table 6 lists the performance figures for six fine grained relations with the largest number of examples . The results exhibit a much higher fluctuation than the summary figures given before2 . A possible explanation is the smaller number of observations for each relation . For the Part.Part of relation and the At.Based in relation the Path TDK yields much better results than all other kernels . The Path TDK shows an improvement of at least 9.4 % F scores over the state of the art kernels on the At.Based in relation . On the At.Based in relation the Path TDK achieves an improvement of at least 8.4 % F score . The two differences in F values between the two best kernels Path DK and PathTDK is significant at a level of 999 % For the other relations , the difference is much smaller , and for two relations untyped approaches are better . Although the average figures indicate that typed dependency tree kernels generally achieve better results , there is a need to detect and utilize relation specific features in order to improve learning .
8 . CONCLUSION AND FUTURE WORK
In this paper we developed novel typed dependency tree kernels to detect semantic relations in natural language sentences . We were able to show that the additional information contained in the edge labels of the parse trees can be exploited by our typed tree kernels which on average do better than their untyped counterparts . This indicates that 2We omitted results for the test set , as 37 % of the different subrelations have no observation in the test set . the type information in dependency trees contains essential clues for relation detection .
In summary the Path TDK achieved significantly better F values than the other approaches . This holds for coarse top level relations as well as for more finegrained relations . The results for fine grained relations often exhibit relatively large fluctuations . This suggests that a certain number of training examples is necessary to achieve a good performance . Further research should be going into the investigation of active learning strategies for a reduction of needed annotated data . The application of this techniques to other texts like product descriptions , where the goal is the extractions of product details is promising . As typed dependency trees may be computed with high accuracy for natural language text from various sources like newspaper or social media . This opens a new way to reach better performance in practical relation extraction from text . As combining kernels for untyped dependency trees and phrase grammar trees is rewarding [ 22 , 24 ] , we plan to evaluate composite kernels for typed , untyped and phrase grammar tree kernels . Another promising research direction is the inclusion of other types of semantic information into the kernels . To this end word sense disambiguation approaches [ 23 ] might be employed or topic modeling algorithms may be used to assign broader semantic information to the words in a sentence .
9 . ACKNOWLEDGEMENT
The work presented here was funded by the German Federal Ministry of Economy and Technology ( BMWi ) under the THESEUS project .
780 Relation
Kernel
At.Located
Part.Part of
Role.General staff
Role.Management
Role.Affiliate partner
At.Based in
SPK [ Bunescu05 ] DK [ Culotta04 ] TDK All Pairs DK [ Reichartz09 ] All Pairs TDK Path DK [ Reichartz09 ] Path TDK
SPK [ Bunescu05 ] DK [ Culotta04 ] TDK All Pairs DK [ Reichartz09 ] All Pairs TDK Path DK [ Reichartz09 ] Path TDK
SPK [ Bunescu05 ] DK [ Culotta04 ] TDK All Pairs DK [ Reichartz09 ] All Pairs TDK Path DK [ Reichartz09 ] Path TDK
SPK [ Bunescu05 ] DK [ Culotta04 ] TDK All Pairs DK [ Reichartz09 ] All Pairs TDK Path DK [ Reichartz09 ] Path TDK
SPK [ Bunescu05 ] DK [ Culotta04 ] TDK All Pairs DK [ Reichartz09 ] All Pairs TDK Path DK [ Reichartz09 ] Path TDK
SPK [ Bunescu05 ] DK [ Culotta04 ] TDK All Pairs DK [ Reichartz09 ] All Pairs TDK Path DK [ Reichartz09 ] Path TDK
5 times rep . 5 fold CV
Rec
38.4 46.3 47.9 54.8 55.3 59.9 59.9
27.7 58.6 70.9 55.9 65.2 62.5 72.3
62.7 65.1 65.1 68.3 67.7 68.1 67.8
40.9 62.0 61.1 58.9 58.5 63.0 63.6
13.3 48.5 55.0 64.4 65.4 55.4 58.0
19.7 24.4 31.0 23.8 30.5 28.2 37.2
F score
51.4 ( 0.4 ) 52.7 ( 0.5 ) 53.7 ( 0.5 ) 59.2 ( 0.8 ) 60.5 ( 0.5 ) 65.4 ( 0.6 ) 65.0 ( 0.3 )
41.0 ( 1.4 ) 64.5 ( 0.1 ) 76.4 ( 0.2 ) 62.7 ( 0.4 ) 72.7 ( 0.2 ) 70.3 ( 0.4 ) 79.2 ( 0.2 )
72.2 ( 0.5 ) 71.3 ( 0.3 ) 71.7 ( 0.1 ) 74.9 ( 0.5 ) 74.8 ( 0.4 ) 74.9 ( 0.4 ) 74.5 ( 0.2 )
52.2 ( 0.4 ) 66.3 ( 0.4 ) 66.9 ( 0.4 ) 64.5 ( 0.4 ) 64.7 ( 0.5 ) 68.6 ( 0.3 ) 69.2 ( 0.2 )
22.6 ( 1.5 ) 57.7 ( 1.4 ) 64.6 ( 1.7 ) 73.7 ( 1.5 ) 75.3 ( 1.1 ) 65.9 ( 1.9 ) 69.2 ( 1.7 )
28.2 ( 2.5 ) 31.1 ( 1.7 ) 38.0 ( 1.6 ) 30.2 ( 1.5 ) 37.8 ( 1.0 ) 36.2 ( 1.6 ) 45.6 ( 0.7 )
Prec
77.6 61.2 61.3 64.3 66.9 71.9 71.0
79.2 71.8 82.7 71.3 82.1 80.5 87.6
85.3 78.8 79.8 83.0 83.6 83.4 82.8
72.2 71.3 73.8 71.4 72.5 75.2 75.8
75.5 71.4 78.5 86.2 88.8 81.7 86.0
50.1 43.3 49.5 41.2 50.1 51.0 59.5
Table 6 : Precision , recall , and F scores all in percent for experiments on the ACE 2003 data set with 5 times repeated 5 fold cross validation for the largest six subrelations . The values in parenthesis denote the standard error . SPK denotes the Shortest Path Kernel [ 6 ] , DK denotes the Dependency Tree Kernel [ 7 ] and All Pairs , Path DK denote the All Pairs , Path Dependency Tree Kernel [ 25 ] .
781 10 . REFERENCES
[ 1 ] M . Banko , M . Cafarella , S . Soderland , M . Broadhead , and O . Etzioni . Open information extraction from the web . In Proceedings IJCAI ’07 , pages 2670–2676 , 2007 . [ 2 ] M . Banko and O . Etzioni . The tradeoffs between open and traditional relation extraction . In Proceedings of ACL 08 : HLT , pages 28–36 , 2008 .
[ 3 ] S . Blohm and P . Cimiano . Scaling up pattern induction for web relation extraction through frequent itemset mining . In Proceedings KI 2008 WS on Ontology Based IE Systems , 2008 .
[ 4 ] R . R . Bouckaert and E . Frank . Evaluating the replicability of significance tests for comparing learning algorithms . In Proc . PAKDD 2004 , Sydney , Australia , pages 3–12 , 2004 .
[ 5 ] M . Bundschus , M . Dejori , M . Stetter , V . Tresp , and
H P Kriegel . Extraction of semantic biomedical relations from text using conditional random fields . BMC Bioinformatics , 9 , 2008 .
[ 6 ] R . C . Bunescu and R . J . Mooney . A shortest path dependency kernel for relation extraction . In Proceedings EMNLP ’05 , 2005 .
[ 7 ] A . Culotta and J . Sorensen . Dependency tree kernels for relation extraction . In Proceedings ACL ’04 , 2004 .
[ 8 ] M C de Marneffe , B . MacCartney , and C . D .
Manning . Generating typed dependency parses from phrase structure parses . In 5th International Conference on Language Resources and Evaluation ( LREC 2006 ) , pages 449–454 . , 2006 .
[ 9 ] M . C . de Marneffe and C . D . Manning . The Stanford typed dependencies representation . In Coling 2008 : Proceedings of the workshop on Cross Framework and Cross Domain Parser Evaluation , 2008 .
[ 10 ] S . Harabagiu , C . A . Bejan , and P . Morarescu . Shallow semantics for relation extraction . In Proceedings IJCAI ’05 , 2005 .
[ 11 ] D . Haussler . Convolution kernels on discrete structures . Technical report , University of California at Santa Cruz , 1999 .
[ 12 ] M . Hearst . Automatic acquisition of hyponyms from large text corpora . In Proceedings COLING 92 , 1992 . [ 13 ] T . Horvath , G . Paass , F . Reichartz , and S . Wrobel . A logic based approach to relation extraction from texts . In ILP ’09 , 2009 .
[ 14 ] T . Joachims . Text categorization with support vector machines : learning with many relevant features . In Proceedings ECML ’98 , 1998 .
[ 15 ] D . Jurafsky and J . H . Martin . Speech and Language
Processing . Pearson Education Inc . , 2nd edition , 2009 .
[ 16 ] D . Klein and C . D . Manning . Accurate unlexicalized parsing . In Proceedings ACL ’03 , 2003 .
[ 17 ] D . Klein and C . D . Manning . Corpus based induction of syntactic structure : Models of dependency and constituency . In Proceedings ACL ’04 , 2004 .
[ 18 ] H . Lodhi , C . Saunders , J . Shawe Taylor ,
N . Cristianini , and C . Watkins . Text classification using string kernels . JMLR , ( 2):419–444 , 2002 .
[ 19 ] A . Mitchell . ACE 2 Version 1.0 ; corpus LDC2003T11 .
Linguistic Data Consortium , Philadelphia . http://wwwldcupennedu , 2003 .
[ 20 ] A . Moschitti . Efficient convolution kernels for dependency and constituent syntactic trees . In Proceedings ECML ’06 , 2006 .
[ 21 ] D . Nadeau and S . Sekine . A survey of named entity recognition and classification . Lingvisticae Investigationes , 30 , 2007 .
[ 22 ] T V T . Nguyen , A . Moschitti , and G . Riccardi .
Convolution kernels on constituent , dependency and sequential structures for relation extraction . In Proceedings EMNLP ’09 , pages 1378–1387 , 2009 .
[ 23 ] G . Paaß and F . Reichartz . Exploiting semantic constraints for estimating supersenses with crfs . In Proceedings SDM ’09 , 2009 .
[ 24 ] F . Reichartz , H . Korte , and G . Paass . Composite kernels for relation extraction . In Proceedings ACL ’09 , 2009 .
[ 25 ] F . Reichartz , H . Korte , and G . Paass . Dependency tree kernels for relation extraction from natural language text . In Proceedings ECML PKDD ’09 , 2009 .
[ 26 ] J . Shawe Taylor and N . Cristianini . Kernel Methods for Pattern Analysis . Cambridge University Press , 2004 .
[ 27 ] V . Vapnik . The Nature of Statistical Learning Theory .
Springer , New York , 1995 .
[ 28 ] D . Zelenko , C . Aone , and A . Richardella . Kernel methods for relation extraction . J . Mach . Learn . Res . , 3:1083–1106 , 2003 .
[ 29 ] M . Zhang , G . Zhou , and A . Aw . Exploring syntactic structured features over parse trees for relation extraction using kernel methods . Inf . Process . Manage . , 44(2):687–701 , 2008 .
782
