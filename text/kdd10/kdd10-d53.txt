Document Clustering via Dirichlet Process Mixture Model with Feature Selection
Dept . of Industrial and Systems
Dept . of Industrial and Systems
The Hong Kong Polytechnic
The Hong Kong Polytechnic
Ruizhang Huang
Engineering
Guan Yu
Engineering
University
Kowloon , Hong Kong
University
Kowloon , Hong Kong yuguan@polyueduhk mfrzh@polyueduhk
Zhaojun Wang Dept . of Statistics
School of Mathematical Sciences
Nankai University
Tianjin , China zjwang@nankaieducn
ABSTRACT One essential issue of document clustering is to estimate the appropriate number of clusters for a document collection to which documents should be partitioned . In this paper , we propose a novel approach , namely DPMFS , to address this issue . The proposed approach is designed 1 ) to group documents into a set of clusters while the number of document clusters is determined by the Dirichlet process mixture model automatically ; 2 ) to identify the discriminative words and separate them from irrelevant noise words via stochastic search variable selection technique . We explore the performance of our proposed approach on both a synthetic dataset and several realistic document datasets . The comparison between our proposed approach and stage of the art document clustering approaches indicates that our approach is robust and effective for document clustering . Categories and Subject Descriptors H28 [ Database Management ] : Database Applications Data Mining ; I53 [ Pattern Recognition ] : Clustering
General Terms Algorithms
Keywords Document Clustering , Dirichlet Process Mixture Model , Feature Selection .
1 . INTRODUCTION With the rapid growth of Internet and the wide availability of news documents , document clustering , as one of the most useful tasks in text mining , has received more and more interest recently . A common challenge in document clustering is to determine the number of document clusters K . This issue is not considered by most of the existing document clustering approaches [ 9 , 18 , 22 ] .
They all take the assumption that K is a pre defined parameter determined by users and provided before the document clustering process . However , given a set of documents , users need to browse the whole document collection in order to estimate K . This is obviously time consuming and unrealistic especially when the size of document collection is extremely large . Moreover , an improper estimation of K might easily mislead the clustering process and result in bad clustering outcome . Therefore , it is useful if a document clustering approach could be designed relaxing the assumption of the pre defined K . Determine the number of clusters is a difficult problem . We attempt to group documents into an optimal number of document clusters based on the Dirichlet process mixture ( DPM ) model . The DPM model has been studied in nonparametric Bayesian for a long time [ 1 , 14 , 21 ] . As an infinite mixture model in which each component corresponds to a different cluster , the DPM model determines the number of clusters automatically . When a new data point arrives , it either rises from existing clusters or starts a new cluster . The clustering process based on the DPM model jointly considers both the data likelihood and the clustering property of the DP prior that data points are more likely to be related to popular and large clusters [ 2 , 10 ] . This flexibility of the DPM model makes it particularly useful for document clustering . However , there is no work investigating DPM model for document clustering . One reason is that the high dimensional representation of text documents is composed of all distinct words including discriminative words and a large number of irrelevant noise words . In [ 17 ] , it is mentioned that the optimal number of clusters is greatly affected by the quality of the feature subset . The involvement of irrelevant words confuses the process of estimating the optimal number of clusters K which causes poor clustering solution in return . Therefore , it is necessary to separate discriminative words from irrelevant noise words and only use them to group document collection especially when K is unknown . In this paper , we propose an approach , namely Dirichlet process mixture model with feature selection ( DPMFS ) , which 1 ) groups documents into a set of document clusters while K is determined automatically ; 2 ) identifies discriminative words and separates them from irrelevant noise words . In our proposed approach , a DPM model is designed and investigated to group documents as well as discover the optimal number of document clusters . The DPM model is not without problems . One problem for DPM is that DPM parameters cannot be estimated quickly . In our proposed approach , a Dirichlet Multinomial Allocation ( DMA )
763 model is used to approximate the DPM model . To identify discriminative words , a stochastic search variable selection technique [ 5 , 12 , 16 ] is applied . In our inference procedure , the Gibbs sampling algorithm [ 14 , 21 ] is used to infer both the cluster structure and the discriminative words . We have conducted extensive experiments on our proposed DPMFS approach by using both synthetic and realistic datasets . We also compared our approach with a stage of the art model based document clustering approach proposed in [ 9 ] and a standard model based clustering approach [ 24 ] . Experimental results show that our proposed DPMFS approach is effective . The remainder of this paper is organized as follows : First , related work on the identification of the number of clusters and document clustering is discussed in section 2 . In section 3 , we introduce background knowledge of the DPM model and the DMA model . Next , in section 4 , we describe the DPMFS model and DMAFS model . Our proposed algorithm is given in section 5 . Section 6 presents the design of experiments and discusses results of experiments . Finally , in section 7 , we draw conclusions and make suggestions for future work . 2 . RELATED WORK Many methods have been introduced to find an optical number of clusters K . The most straightforward method is the likelihood cross validation technique [ 27 ] which trains the model with different values of K and then picks the one with the highest likelihood on some held out data . Another solution is to assign a prior to K and then calculate the posteriori distribution of K to determine this number [ 6 ] . In the literature , there are also many information criteria proposed to choose K , eg , Minimum Description Length ( MDL ) [ 23 ] , Minimum Message Length ( MML ) [ 30 ] , Akaike Information Criterion ( AIC ) [ 4 ] and Bayesian Information Criterion ( BIC ) [ 25 ] . The basic idea of all these criteria is to penalize complicated models ( ie , models with large K ) in order to come up with an appropriate K to trade off data likelihood and model complexity [ 11 ] . Compared to all these methods , the method based on the DPM model to choose K is very different and flexible . In the DPM model , the number of clusters is determined after the clustering process rather than preestimated . Furthermore , this method is easy to use and does not require very expensive computation . In the previous work , [ 29 ] applies DPM model to the lexical semantic verb clustering and [ 3 ] uses this model in the image analysis . They all pointed that DPM model could determine appropriate number of clusters automatically . If the number of clusters is pre defined , many algorithms based on the probabilistic finite mixture model have been successfully applied to the document clustering . For example , [ 22 ] proposed a multinomial mixture model . It applies the EM algorithm for document clustering assuming that document topics follow multinomial distribution and each document is a mixture of these multinomial distributions . This method has been shown to perform well for the document dataset though it does not take into account the phenomenon that words in a document tend to appear in bursts . [ 19 ] used the DCM model to capture burstiness well . Their experiments showed that the performance of DCM was comparable to that obtained with multiple heuristic changes to the multinomial model . However , DCM model lacks intuitiveness and the parameters in that model cannot be estimated quickly . [ 9 ] derived the EDCM distribution which belongs to the exponential family and it is a good approximation to the DCM distribution . The EM algorithm with the EDCM distribution is faster than the corresponding algorithm with DCM distribution proposed in [ 19 ] . EM algorithm with EDCM distribution is the most competitive in the literature for document clustering in recent years . 3 . BACKGROUD 3.1 Dirichlet Process Mixture Model The DPM model is a mixture model with an infinite number of mixture components [ 28 ] . We introduce this infinite mixture model by firstly describing the simple finite mixture model . In the finite mixture model , each data point is drawn from one of K fixed unknown distributions . For example , the multinomial mixture model for document clustering assumes that each document xn is drawn from one of K multinomial distributions parameterized by K different multinomial parameters , θ1,…,θK . Since the number of clusters is always unknown , to allow it to grow with data , we assume that the data point xn follows a general mixture model in which the parameter θ is generated from a distribution G . The conditional hierarchical relationships are as follows :
θn | G ~ G , n =1,2,… , D , xn | θn ~ F( xn| θn ) , n =1,2,… , D , ( 1 ) where D is the number of data points and F(xn|θn ) is the distribution of xn given the parameter θn . In the general mixture model , probability distribution G is always unknown . If the unknown G is a discrete distribution on a finite set of values , this general mixture model reduces to the finite mixture model . Bayesian nonparametric methods view G as a ( infinite dimensional ) parameter and assign a prior to it . One class of Bayesian nonparametric techniques is called the Dirichlet process ( DP ) [ 10 ] .
G0
α
N
α
G0
G
P
θn
D xn zn
D xn
θz N
Figure 1 : Graphical representation of DPM model ( Left ) and
DMA ( Right ) .
Dirichlet process , as a distribution on distributions , is parameterized by a positive scaling parameter α and a base distribution G0 . Assigning a DP prior to G in the general mixture model leads to the Dirichlet process mixture ( DPM ) [ 1 ] model . The hierarchical Bayesian specification of DPM model is as follows : G | α , G0 ~ DP ( α , G0 ) ,
764 θn | G ~ G , n =1,2,… , D , ( 2 ) xn | θn ~ F( xn | θn ) , n =1,2,… , D . The DPM model can be best understood by the hierarchical graphical representation shown in Figure 1 . As shown in [ 1 ] , integrating out G , the joint distribution of the collection of variables {θ1,…,θD} exhibits a clustering effect . Let θ n denotes the set of all θj for j≠n . The conditional distribution of θn given θ n has the following form :
, αθθ n n
−
,
|
G 0
~
D
1 +− ∑ 1
α nj ≠
δ θ j
+
1 1 +−
.
G 0 α
D
( 3 )
Let Φ1 ,…,ΦC be the distinct values taken by θ n where C is the number of clusters estimated . Let mi be the number of times that the value of θj equals to Φi for j≠n . Equation ( 3 ) is transformed to :
, αθθ
,
|
− n n
G 0
~
C
∑ i
1 =
D m i 1 +−
δ Φ i
α
+
D
α 1 +−
α
G 0
.
( 4 )
Equation ( 4 ) means that parameters θ1 , …,θD are randomly partitioned into clusters , in which all θ take on the same value . It also indicates that DP prior allows a new data point either to share the same cluster with the previous data points or to start a new cluster . The number of clusters is determined automatically . We can best understand this clustering property by a famous metaphor known as the Chinese restaurant process [ 28 ] . Given data points x1,… , xD and the DP parameter ( α , G0 ) , DPM model yields a posterior distribution on θ1 , …,θD which also exhibits clustering effect [ 21 ] . Based on the posterior estimation of θ1 , …,θD , the data points x1,… xD can be partitioned into clusters . Data points in cluster i share the same parameter value Φi . Since this clustering process based on the DPM model not only considers the data likelihood as the finite mixture model but also combines the clustering property of the DP prior shown in Equation ( 4 ) , the DPM model is very suitable for document clustering . 3.2 Dirichlet Multinomial Allocation It has been proved that the DPM model can be derived as the limit of a sequence of finite mixture models when the number of mixture components is taken to infinity [ 13 , 15 , 20 ] . The Dirichlet Multinomial Allocation ( DMA ) [ 13 ] is one of the most famous approximations to the DPM model . The generative model for DMA is as follows : x n
| z n
~ , θ
F
( θ nz
) , n
=
,,1
D
, symmetric parameters α/N . The graphical representation of DMA is shown in Figure 1 . Let z n denote the set of all zj for j≠n . Integrating out the mixing proportions p , we can write the conditional distribution of zn given z n as the following form : zp ( n
= zz |
)
=
− n n
N / α + 1 α +− zn , n
,
( 6 ) where z ranges from 1 to N and nn,z is the number of zj for j≠n that are equal to z . Compare the Equation ( 4 ) and the Equation ( 6 ) , the clustering property of the DMA is the same as DPM model if we let N go to infinity . It has been shown in [ 14 ] that the L1 distance between the Bayesian marginal density of the data under DMA and the DPM model is O(4D exp( (N 1)/α) ) . This property provides good hints on how to choose the value of N . For example , if D=300 , N=30 , and α=1.0 , we get an L1 bound of 305E 10 Therefore , for D=300 and α=1.0 , a DMA model with N=30 is virtually indistinguishable from the DPM model .
4 . DPMFS AND DMAFS APPROXIMATION Suppose there are D documents in a dataset x with the vocabulary size W . The set of vocabulary is composed of all words appeared in x represented as {w1 , w2 , … , wW} . Given a document xi in x , let xij be the number of appearances of the word wj . Each document is represented as a W dimensional vector xi= ( xi1 , xi2,… , xiW ) .
4.1 Stochastic Search Variable Selection We introduce a latent binary vector γ=(γ1,… , γW ) to identify words that discriminate between the different clusters . jγ
=
,1 ⎧ ⎨ ,0 ⎩ is w j discrimina if j otherwise .
, tive ,,1 =
( 7 )
W
.
This latent vector partitions the dataset x into two parts : one part is the discriminative words , xγ= {(xi1γ1,… , xiWγW ) : i=1,2,… , D} which defines the latent cluster structure . Another part is the irrelevant noise words , x(1 γ ) ={(xi1(1 γ1),… , xiW(1 γW) ) : i=1,2,… , D} that confuses document clustering process . We assign a prior to γ and assume that its elements are independent Bernoulli random variables with common probability distribution . The distribution of γ is as follows : zn | p ~ Discrete ( p1,… , pN ) , n=1,… , D , ( 5 ) θz ~ G0 , p ~ Dirichlet ( α /N,… , α /N ) , where zn indicates the latent cluster allocation of the n th sample and N is the number of mixture components . For each cluster z , the parameter θz determines the distribution of the data points from that cluster . The N dimensional vector p , which is the mixing proportions for the clusters , is given a Dirichlet prior with p )( γ
=
1 γ ωω γ − ) j
1(
− j
W
∏ j
1 =
,
( 8 ) where ω is the prior probability of each word expected to be discriminative . This stochastic search variable selection technique has been used successfully in various applications to identify informative variables [ 12 , 16 ] . As [ 16 ] , we will combine this technique with DPM model and DMA in Section 4.2 43
765 λ
G0
α
G
D
ηi xi γ
ω
γ
η0
D xi(1 γ )
λ
N
ηi
ω
γ
η0
D xi ( 1 γ )
α
N
P
D zi xi γ
Figure 2 : Graphical representation of DPMFS model .
Figure 3 : Graphical representation of DMAFS model .
4.2 DPM Model with Feature Selection We assume the following generative process for the D documents in a dataset : 1 . Choose γ | ω ~ p(γ ) . 2 . Choose Nij ~ Poisson ( ξj ) , i =1 , 2,… , D , j = 1,2 . 3 . Choose G | γ , λ ~ DP ( α , G0 ) , where λ= ( λ1,… , λW ) and G0 is a
Dirichlet distribution with parameter λ1γ1,… , λW γW .
4 . Choose ηi | G ~ G , i = 1 , 2,… , D . 5 . Choose η0 | γ , λ ~ Dirichlet ( λ1 ( 1 γ1),… , λW ( 1 γW) ) . 6 . Choose xi γ | ηi ~ Multinomial ( ηi ; Ni1 ) , i =1,… , D . 7 . Choose xi ( 1 γ ) |η0 ~ Multinomial ( η0 ; Ni2 ) , i =1,… , D . where p(γ ) is shown in Equation ( 8 ) , Ni1 is the total appearances of the discriminative words in document xi and Ni2 is the total appearance of the irrelevant noise words in xi . Ni1 and Ni2 are both unobservable and considered as latent variable . xiγ and xi(1 γ ) represent ( xi1γ1,…,xiWγW ) and ( xi1(1 γ1),… , xiW(1 γW ) ) respectively . ηi denotes the multinomial parameter for the discriminative words in xi and η0 , as the multinomial parameter for the irrelevant noise words , is shared by all the documents in the dataset . The graphical representation of DPMFS model is shown in Figure 2 . From the generative process , it is not difficult to find that DPM model is only used to model the data with discriminative words , in particular , xiγ , i = 1,2,… , D . Parameters in the Dirichlet distribution and Multinomial distribution used in the our model may be zero . We only consider those non zero parameters . For example , the probability density functions for xi γ is as follows :
4.3 Approximating the DPMFS Model In this section , we design a DMA model with feature selection , named DMAFS . Since the DPM model can be approximated by the DMA , it is obvious that the DMAFS model is also a good approximation to the DPMFS model . The DMAFS assumes the following generative process for each document xi in a dataset : 1 . Choose γ | ω ~ p(γ ) . 2 . Choose Nij ~ Poisson ( ξj ) , i =1 , 2,… , D , j = 1,2 . 3 . Choose ηi | γ , λ ~ Dirichlet ( λ1γ1,… , λWγW ) , i =1 , , N . 4 . Choose η0 | γ , λ ~ Dirichlet ( λ1 ( 1 γ1),… , λW ( 1 γW) ) . 5 . Choose p | α ~ Dirichlet ( α /N,… , α /N ) . 6 . Choose zi | p ~ Discrete ( p1,… , pN ) , i = 1 , 2 , , D . 7 . Choose , i =1,… , D . 8 . Choose xi ( 1 γ ) |η0 ~ Multinomial ( η0 ; Ni2 ) , i =1,… , D . A graphical representation of DMAFS model we proposed is shown in Figure 3 . The DMAFS approximation provides a close connection between finite mixture model and infinite mixture model . It allows us to have a better understanding of the data generative process from DPMFS model by comparing the finite mixture model . Furthermore , the DMAFS model is very useful to derive simple and effective Gibbs sampling algorithm for DPMFS model . The Gibbs sampling algorithm is shown in Section 5 . Since Dirichlet distribution is the conjugate prior for the parameter of multinomial distribution , integrating over η0 , η1,… , in Equation ( 10 ) , the likelihood of the D documents ηN conditioned on the latent variables γ and z becomes : x ηηγ i
Multinomia l z ~ , iN
( iη z
, ,
N i 1
)
1
;
| xf ( ) ηγγ i i
,
|
=
W
!1 ∏ x ij η ij
! j 1 = 1 γ = j
N i W ∏ j 1 = 1 γ = j x ij
.
( 9 ) xf (
|
, γ z
)
=
(
∏
,1 =
D i
T i
) ( γ
)
⋅
S
( 1
) γ
⋅
S
( 2
) γ
⋅
Q
M ) ( γ
∏ k
,1 =
N
( 11 )
R k
) ( γ
, in which M is the number of distinct values taken by z and
In our model , words in each document are divided into two parts according to whether they define the underlying cluster structure . We assume that there is no correlation between the set of discriminative words and the set of irrelevant noise words . So the probability density function for xi is given by : f ( xi | γ , ηi , η0 ) = f ( xi γ | ηi ) f ( xi ( 1 γ ) |η0 ) . ( 10 )
( j
T i
(
γ
)
=
∑
=
,1
W x
γ j ij x ij
1(
−
γ
)! ) j
,
S
( 1
) γ
=
Γ
( i
∑ ∑
=
,1
D
Wj =
,1 x ij
) ) j
γ ∑
Wj =
,1
λ j
,
1(
−
γ
) ) j
∑ ( )! W ,1 ∏ x
= j
=
,1
W ij j (
Γ
!
1(
−
λ j
∑ Wj ,1 = 1( −
γ j
)
+
766 )
,
1(
− ∑
γ l
)( i
=
,1
D x il
+
λ l
) , l
=
,1
W
.
( 16 )
S
( 2 ∏
=
) γ
Wj ,1 = 0 = γ j
R k
(
γ
)
=
Γ
(
Γ
( ijx
∑ Di ,1 = ( Γ λ j
+
) j
) λ ,
Γ
Q
) ( γ
=
γλ j j
( λ j
)
∑ ( Wj ,1 = ∏ Γ
Wj ,1 = 1 = γ j
∏ ∑
Γ
( zi :{ i
=
Wj ,1 = 1 γ = j
+
λ j
) x } k ij
∑ ∑
= k
}
Wj =
,1 zi :{ i x
γ j ij
+
∑
Wj =
,1
γλ j
.
) j
5 . ALGORITHM We use the Gibbs sampling method to infer both the latent cluster structure and discriminative words in the context of DMAFS model . The inference procedure is effective for the DPMFS model if we choose the parameter N large enough following the advice of [ 14 ] . Let the state of Markov chain consist of γ= {γ1,…,γW} , η={η0 , *}denote the set of η1,…,ηN} and z={z1,…,zD} . Let {z1 distinct values of z . Our inference procedure is as follows : 1 .
Initialize the latent variables γ and z , set the parameter α , ω , λ and N .
*,…,zM
2 . Update the latent discriminative words indicator γ by repeating the following Metropolis step R1 times : A new candidate γnew which adds or deletes a discriminative word is generated by randomly picking one of the W indices in γold and changing its value . The new candidate is accepted with the probability
,1{min f
( γ zx where ) , Equation ( 11 ) .
|
∝ xf (
| pz ) , γ
} , f zx ( | ) , γ new f zx ) , ( | γ old xf γ is given by and )( ( γ
( 12 ) z ) ,
|
3 . Conditioned on the other latent variables , for k =1,… , N , if k is not in {z1,…,zD} , update ηk by sampling a value from a Dirichlet distribution with parameter λ1γ1,… , λWγW . For i =1,… , M , update izη by sampling a value from a Dirichlet distribution with the following parameters :
( 13 )
*
1
, ,
W
=
γλγ l
+ l l jl l
.
,
∑ zj :{
= z j x * } i
4 .
For i =1,2,… , D , update the latent data label zi by repeating the following Metropolis step R2 times : A new candidate new is drawn from the following distribution : zi zp ( new i
= zz |
)
= i − n iz D
α + 1 +−
N α
. ( 14 ) where z i denotes all the zj for j≠i and niz is the number of zj for j≠i that are equal to z . This new candidate is accepted with the probability :
,1 min{ xf ( | ηγ i new z i xf ( ηγ i z i
) } . )
|
( 15 )
5 . Update λ if necessary by the following sampling :
5a . update η0 by sampling a value from a Dirichlet distribution with the following parameters :
5b . Assign a prior p(λ ) to λ and draw λ from p
( ηηηγλ N
,
,
|
1
0
)
∝ p
( ) γληλ p
(
)
,
|
0 p
. ) ( γλη
,
| i
( 17 )
∏ i
,1 =
N
6 . After sampling γ , η , z and λ by step 2 5 for many times ( known as “ burn in ” period ) , we use the last H samples of z and γ to infer the latent data label and discriminative words as follows : 6a . The estimated label of document xi is the most frequent value of zi in the last H samples .
6b . The jth word is discriminative if the average value of the last H samples of γj is bigger than a threshold such as 0.7 which is used in our experiments .
Note that our inference procedure only focuses on the latent variables γ , η and z which are closely related with the cluster structure or the discriminative word subset . The other latent variables such as p are integrated out . We use a simple initialization method to initialize γ and z . The initial label of each document is selected randomly from 1 , 2,… , N . We randomly choose one discriminative word from those words appearing in the dataset . Because η is sampled in step 3 , we don’t have to initialize it . The advice for choosing the parameters is discussed in Section 613 6 . EXPERIMENTS We describe two sets of experiments to evaluate the performance of the DPMFS approach . For the first set of experiments , a synthetic dataset is used . For the second set of experiments , the DPMFS approach is evaluated using a set of real document datasets . 6.1 Evaluation Metric We used the normalized mutual information ( NMI ) [ 8 ] to evaluate the quality of a clustering solution . NMI is an external clustering validation metric that effectively measures the amount of statistical random variables representing the cluster assignments and the user labeled class assignments of the data points . In practice , NMI is estimated as follows [ 26 ] : shared by information the d
∑ lh , log( h
NMI
= d
(
∑ h d h d
))(
∑ l c l log( c l d
) ) log( dd ⋅ lh , cd lh
)
( 18 ) lh , where d is the number of documents , dh is the number of documents in class h , cl is the number of documents in cluster l and dh,l is the number of documents in class h as well as in cluster l . The NMI value is 1 when a clustering solution perfectly matches the user labeled class assignments and close to 0 for a random document partitioning . 6.2 Synthetic Dataset 621 Dataset and Experimental Setup We have generated a synthetic dataset for conducting experiments . The synthetic data consisted of 300 data points with 1000 features .
767 that long before is faster to stabilize than the feature selection process . Moreover , the document clustering our experiment indicated assignments also stabilized the number of discriminative features reach to a stable value . One possible reason is that the documents could be grouped with a subset of discriminative words . Since the purpose for document clustering is to group the dataset into an optimal partition , the sampling process could be terminated when the cluster assignment is not changed .
623 Discussion We investigated the sensitivity of the choices of parameters in our algorithm by large amounts of experiments .
Data points were generated by two different processes with four multinomial distributions . The first process was used to generate discriminative features . Specially , the first 50 features were regarded as discriminative features generated from a multinomial mixture distribution with three components . Each component represents one cluster and each cluster contains 100 data points . The second process was used to generate the irrelevant noise features . In particular , the remaining 950 features were regarded as irrelevant noise features generated from a multinomial distribution . The data was generated as follows : ( xi1,…,xi50 ) ~ Multinomial ( πj ; 100 ) , i=1+100(j 1),…,100j , j=1,2,3 . ( xi51,…,xi1000 ) ~ Multinomial ( π* ; 100 ) , i=1,… , 300 . where ( π1 ; 100 ) , ( π2 ; 100 ) , ( π3 ; 100 ) and ( π* ; 100 ) are the multinomial parameters . π1 , π2 , π3 and π* are chosen randomly in our experiment . In our proposed algorithm for this synthetic data , we set N=30 , R1 =R2 =5 , α =1.0 , ω=001 The components of parameter λ were all chosen to be 01 We ran our proposed algorithm 30 times and each time we ran 2500 iterations in which the first 2000 as burnin .
Figure 6 : Trace plot for the number of clusters when α is chosen to be different values ( Only show the first 400 iterations ) .
Figure 4 : Trace plot for the number of clusters .
Figure 5 : Trace plot for the number of discriminating features .
622 Experimental Performance Our algorithm identified the perfect cluster structure for all the 30 runs of experiments . The number of features identified as discriminative stabilized around 40 to 45 . On average , there were 41 true discriminating features identified successfully . Figure 4 and Figure 5 depict the number of clusters and the number of discriminative features estimated in one typical run by varying the number of iterations . The result shows that the number of clusters
Figure 7 : Trace plot for the number of discriminating features when ω is chosen to be different values .
Choice of N , R1 and R2 : In principle , we can choose N to be the number of data points . However , in order to save computing time , we could choose a relatively small N follow the advice of [ 14 ] as mentioned in Section 2 . Normally , in order to keep the DMA approximate the DPM model well , we should set a large N if we have chosen a large α . The number of Metropolis step R1 and R2 were both chosen to be 5 in our algorithm because we found that a bigger value had little improvement in the clustering quality though it would make the feature selection process more stable . Choice of α and ω : We investigated the sensitivity of the choice of parameters α and ω which influenced the estimated number of clusters and the estimated number of discriminative features
768 respectively . We simulated with different values of α where α was set to be 0.1 , 1.0 and 10.0 which corresponds to a small , moderate , large prior number of clusters in the data under the DPM model . We also experimented with different values of ω where ω was set to be a small value 1.0/W , a moderate value 50.0/W , and a very high value 1000/W For the three different values of α , ω was fixed as 0.01 and N=200 . For the three different values of ω , α was fixed as 1.0 and N=30 . The other parameters were chosen to be : R1 =R2 =5 and the component of λ = 05 Our proposed approach achieved perfect clustering structure in all these experiments . This indicates that our algorithm is robust to the choice of α and ω . Figure 6 and Figure 7 show the trace plot of the estimated number of clusters and the estimated number of discriminative features respectively . Figure 6 indicates that a large α requires relatively long time for the estimated number of clusters to be stable . This is because a large value of α will make the model generate a new cluster easily as shown in Equation ( 4 ) . From Figure 7 , we found that a large value of ω made the sampler visit models with more discriminative features . However , the final estimated discriminative features by the last 500 samples were almost the same for all these different ω . Choice of λ : The parameter λ not only affects the estimated number of clusters but also the estimated number of discriminating features . Some care is needed to choose this parameter in a reasonable range since a much larger value for it will result in a model with fewer mixture components than the true one . Our experiments indicate that a small value for λ performs well though it will require relatively long time for the sampling process to be stable . In order to acquire good clustering quality and save computing the characteristic of the dataset for setting the value of λ . For the document datasets used in our following experiments , we found that a good choice of λj is 1.0/σj , where σj is the sample standard variance of {x1i , x2i,… , xDi} . A simple interpretation for this choice is that if the standard variance of each column of the data x is large , there may be more clusters in the corpus and we should choose a small λ . Another effective method to handle the parameter λ is to place a prior to it and update this parameter in the inference procedure . This will require additional slow Metropolis Hastings updates in our algorithm because we don’t know the conjugate prior for this Dirichlet parameter λ . time , we must consider
Table 1 : Datasets Description
( D : Number of documents , K : Number of clusters , W :
Vocabulary size . )
Datasets
News Different 3 News Similar 3
News Moderated 6
Classic400
D 300 300 600 400
K 3 3 6 3
W 2121 1767 4036 6025
6.3 Real Document Datasets 631 Experimental Datasets Four standard text datasets were used in our experiments : NewsDifferent 3 , News Similar 3 , News Moderated 6 and Classic400 . The summary of these four real world text document datasets is shown in Table 1 . The first three datasets were derived from the topics similar
20 Newsgroups collection . This collection has messages collected from 20 different Usenet news groups , 1000 messages from each newsgroup . From the original corpus , a subset was first created by randomly selecting 100 messages from each of the 20 newsgroups . The first three datasets were then derived from the subset . NewsDifferent 3 consists of 300 messages from 3 newsgroups on different topics ( alt.atheism , recsportbaseball , sci.space ) with well separated clusters . News Similar 3 consists of 300 messages from 3 newsgroups on ( comp.graphics , composms windows , compwindowsx ) where cross posting often occurs . News Moderated 6 consists of 600 messages from 6 newsgroups on topics ( recsportbaseball , sci.space , alt.atheism , talkpoliticsguns , compwindowsx , socreligionchristian ) In the News Moderated 6 dataset , some topics are similar ( alt.atheism and socreligionchristian ) where others are different from each other . The Classic 400 dataset , which is a typical unbalanced dataset , is the same dataset used by the EDCM model proposed in [ 9 ] . We pre processed all the datasets by stop word removal . Low frequency words were removed following the methodology presented in [ 7 ] . The purpose of such processing is to eliminate those words which obviously do not define the latent cluster structure . The threshold for removing low frequency words for all datasets was set to 1 .
632 Experimental Setup For all the real world datasets experiments , we used the same setting of the parameters . The parameters were set as N=D/10 , α=1.0 , ω=50.0/W , R1 =R2 =5 and λj =1.0/σj , where j =1 , 2,… , W . The initialization method for γ and z was the same as previous discussion in Section 4 . Each time we ran 3000 iterations and the first 2500 as burn in .
Table 2 : Clustering results on News Similar 3 and News
Moderated 6
( C : Estimated number of clusters . EDCM and EM MN use the true number of clusters ) .
Datasets
News Similar 3
News Moderated 6
DPMFS C 8.1 7.9
NMI 0.231 0.663
EDCM NMI 0.163 0.531
EM MN
NMI 0.081 0.562 labeled as EM MN , was
For comparative investigation , a standard model based clustering approach [ 22 ] , investigated as benchmark . We also ran experiments for a stage of the art modelbased clustering approach [ 9 ] , labeled as EDCM . Since the deterministic annealing procedure [ 24 ] allows EM to find better local optima of the likelihood function and therefore improve the clustering quality , we added it to the EM MN and EDCM . The temperature parameter for the three phases was chosen to be 25 , 5 and 1 . EM MN and EDCM require the number of clusters as input . We studied the performance of them when we gave right or wrong number of clusters . Each algorithm was run 30 times and we used the average NMI to compare their performance for clustering .
769 Table 3 : Clustering results on News Different 3 ( the third row ) and Classic400 ( the fourth row )
( C : Estimated number of clusters , K : Pre defined number of clusters ) .
DPMFS NMI C 0.688 5.9 8.0 0.641
EDCM K=3 0.734 0.684
K=2 0.386 0.243
EM MN
K=10 0.561 0.403
K=2 0.464 0.36
K=3 0.867 0.496
K=10 0.634 0.506
Figure 8 : Estimated labels of data points in News Different 3 . that DPMFS approach could separate
633 Experimental Results Table 2 shows the experimental performances of DPMFS , EDCM and EM MN on the News Similar 3 and News Moderated 6 datasets . The number of clusters estimated is also depicted . The experimental results show that our proposed approach achieves the best clustering results for these two datasets . The reason is that the New Similar 3 and the New Moderated 6 datasets contain similar clusters . A large number of irrelevant noise words in these two datasets may mislead the clustering process . This result demonstrates the discriminative words from the irrelevant ones and therefore improve the clustering quality to some extent . In Table 3 , the experimental results on News Different 3 and Classic400 datasets are depicted . The documents in these two datasets are relatively well separated and there are many discriminative words aiding the clustering process . It is shown in the experimental results that relatively better clustering quality were achieved by the EDCM and the EM MN approaches when the number of clusters was correctly assigned . However , when the number of clusters was given imprecise , both of the EDCM and EM MN approaches performed far worse than the DPMFS approach . Therefore , our proposed approach is more robust to group documents into a set of clusters when the number of clusters is unknown in advance . In respect to the estimation of the number of clusters , the estimated values for these four datasets were all bigger than the true one as shown in Table 2 and Table 3 . In fact , it is very difficult to acquire exact estimation of the number of document clusters in these datasets since a couple of outliers could make the estimated value bigger than the true one . Figure 8 shows one estimated labels of documents in News Different 3 . The estimated labels of the documents provide strong support for the true cluster structure . The result indicates that DPMFS could acquire meaningful clustering outcome . 7 . CONCLUSIONS AND FUTURE WORK In this paper , our proposed DPMFS approach handles document clustering and feature selection simultaneously . We constrain the DPM model only to define the cluster structure of the data with discriminative features which are identified by a latent binary vector . The Gibbs Sampling technique is used to infer both the cluster structure and the latent discriminative word subset . Our experiment shows that DPMFS approach groups document dataset into meaningful clusters without requiring the number of clusters known in advance . The comparison of our algorithm with some existing stage of the art algorithms indicates that our approach is more robust and effective for document clustering when no information other than the observed values is available . Our analysis of the experiment result also shows that feature selection inserted in the DPM model could alleviate the negative impact of the irrelevant noise words and therefore improve the clustering quality . An interesting direction for future research is to study how to use the DPMFS approach in the semi supervised document clustering since more and more labeled documents or constraints are available in real life . We think that the additional information could improve the clustering quality from at least two aspects . The first one is that reasonable model parameters and initial value can be chosen from this additional information . The second one is that we can use this information guide our sampling process . 8 . ACKNOWLEDGMENTS The work described in this paper was fully supported by grants from the Hong Kong Polytechnic University Internal Competitive Research Grants ( Project Code : G YG39 and A PJ72 ) .
9 . REFERENCES [ 1 ] C . Antoniak . ( 1974 ) . Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems . The Annals of Statistics , 2(6):1152 1174 .
[ 2 ] D . Blackwell and J . MacQueen . ( 1973 ) . Ferguson distribution via Polya urn schemes . The Annals of Statistics , 1(2):353 355 .
[ 3 ] D . Blei and M . Jordan . ( 2006 ) . Variational inference for
Dirichlet process mixtures . Bayesian Analysis , 1(1):121 144 . [ 4 ] H . Bozdogan . ( 1983 ) . Determining the number of component clusters in the standard multivariate normal mixture model using model selection criteria . TR UIC/DQM/A83 1 , Quantitative Methods Department , University of Illinois , Chicago , IL .
[ 5 ] P . J . Brown , M . Vannucci and T . Fearn . ( 1998 ) . Multivariate
Bayesian variable selection and prediction . Journal of the Royal Statistical Society , Series B , 60:627 641 .
[ 6 ] P . Cheeseman , J . Kelly , M . Self , J . Stutz , W . Taylor , and D .
Freedman . ( 1988 ) . Autoclass : A Bayesian classification system . In Proceedings of the Fifth International Conference on Machine Learning , pages 54 64 .
770 [ 7 ] I . S . Dhillon and D . S . Modha . ( 2001 ) . Concept decompositions for large sparse text data using clustering . Journal of Machine Learning , 42(1):143 175 .
[ 8 ] B . E . Dom . ( 2001 ) . An information theoretic external cluster validity measure . Research Report RJ 10219 , IBM .
[ 9 ] C . Elkan . ( 2006 ) . Clustering Documents with an
Exponential Family Approximation of the Dirichlet Compound Multinomial Distribution . In Proceedings of the 23th International Conference on Machine Learning , 289296 .
[ 10 ] T . Ferguson . ( 1973 ) . A Bayesian analysis of some nonparametric problems . The Annals of Statistics , 1:209 230 .
[ 11 ] C . Fraley and A . E . Raftery . ( 1998 ) . How many clusters ?
Which clustering method ? Answers via model based cluster analysis . The Computer Journal , 41(8):578 588 .
[ 12 ] E . I . George and R . E . McCulloch . ( 1992 ) . Variable selection via Gibbs sampling . Journal of the American Statistical Association , 88:881 889 .
[ 13 ] P . J . Green and S . M . Richardson . ( 2001 ) . Modelling Heterogeneity with and without the Dirichlet Process . Scandinavian Journal of Statistics , 28:355 377 .
[ 14 ] J . Ishwaran and L . James . ( 2001 ) . Gibbs sampling methods for stick breaking priors . Journal of the American Statistical Association , 96:161 174 .
[ 15 ] H . Ishwaran and M . Zarepour . ( 2002 ) . Exact and
Approximate Sum Representations for the Dirichlet process . Canadian Journal of Statistics , 30:269 283 .
[ 16 ] S . Kim . ( 2006 ) . Variable selection in clustering via Dirichlet process mixture models . Biometrika , 93(4):877 893 .
[ 17 ] M . H . C . Law , M . A . T . Figueiredo , and A . K . Jain . ( 2004 ) . Simultaneous feature selection and clustering using mixture models . IEEE Trans . Pattern Analysis and Machine Intelligence , 26(9):1154 1166 .
[ 18 ] J . MacQueen . ( 1967 ) . Some methods for classification and analysis of multivariate observations . In Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability , 281 297 .
[ 19 ] R . Madsen , D . Kauchak , and C . Elkan . ( 2005 ) . Modeling word burstness using the Dirichlet distribution . In Proceedings of the 22th International Conference on Machine Learning , 545 552 .
[ 20 ] R . Neal . ( 1992 ) . Bayesian mixture modeling . In Proceedings of the Workshop on Maximum Entropy and Bayesian Methods of Statistical Analysis , 11:197 211 .
[ 21 ] R . Neal . ( 2000 ) . Markov chain sampling methods for
Dirichlet process mixture models . Journal of Computational and Graphical Statistics , 9(2):249 265 .
[ 22 ] K . Nigam , A . K . McCallum , S . Thrun , and T . M . Mitchel .
( 2000 ) . Text classification from labeled and unlabeled documents using EM . Journal of Machine Learning , 39(2/3):103 134 .
[ 23 ] J . Rissanen . ( 1978 ) . Modeling by shortest data description .
Automatica , 14:465 471 .
[ 24 ] K . Rose . ( 1998 ) . Deterministic annealing for clustering , compression , classification , regression , and related optimization problems . In Proceedings of the IEEE , 86(11):2210 2239 .
[ 25 ] G . Schwarz . ( 1978 ) . Estimating the dimension of a model .
The Annals of Statistics , 6:461 464 .
[ 26 ] Z . Shi . ( 2006 ) . Semi supervised model based document clustering : A comparative study . Journal of Machine Learning , 65(1):3 29 .
[ 27 ] P . Smyth . ( 1998 ) . Model selection for probabilistic clustering using cross validated likelihood . ICS Tech Report 98 09 , Statistics and Computing .
[ 28 ] Y . W . Teh , M . I . Jordan , MJ Beal , and DM Blei . ( 2007 ) . Hierarchical Dirichlet Processes . Journal of the American Statistical Association , 101(476):1566 1581 .
[ 29 ] A . Vlachos , Z . Ghahramani , and A . Korhonen . ( 2008 ) .
Dirichlet process mixture models for verb clustering . ICML Workshop on Prior Knowledge for Text and Language Processing , Helsinki , Finland .
[ 30 ] C . Wallace and P . Freedman . ( 1987 ) . Estimation and inference by compact coding . Journal of the Royal Statistical Society , Series B , 49(3):240 265 .
771
