Compressed Fisher Linear Discriminant Analysis :
Classification of Randomly Projected Data
Robert J . Durrant
School of Computer Science
University of Birmingham Edgbaston , UK , B15 2TT rjdurrant@csbhamacuk
Ata Kabán
School of Computer Science
University of Birmingham Edgbaston , UK , B15 2TT akaban@csbhamacuk
ABSTRACT We consider random projections in conjunction with classification , specifically the analysis of Fisher ’s Linear Discriminant ( FLD ) classifier in randomly projected data spaces .
Unlike previous analyses of other classifiers in this setting , we avoid the unnatural effects that arise when one insists that all pairwise distances are approximately preserved under projection . We impose no sparsity or underlying lowdimensional structure constraints on the data ; we instead take advantage of the class structure inherent in the problem . We obtain a reasonably tight upper bound on the estimated misclassification error on average over the random choice of the projection , which , in contrast to early distance preserving approaches , tightens in a natural way as the number of training examples increases . It follows that , for good generalisation of FLD , the required projection dimension grows logarithmically with the number of classes . We also show that the error contribution of a covariance misspecification is always no worse in the low dimensional space than in the initial high dimensional space . We contrast our findings to previous related work , and discuss our insights .
Categories and Subject Descriptors H28 [ Database Management ] : Database applications— Data mining ; I51 [ Pattern Recognition ] : Models—Statistical
General Terms Theory
Keywords Dimensionality Reduction , Random Projection , Compressed Learning , Linear Discriminant Analysis , Classification
1 .
INTRODUCTION
Dimensionality reduction via random projections has attracted considerable interest for its theoretical guarantees to approximately preserve pairwise distances globally , and for the computational advantages that it offers . While this theory is quite well developed , much less is known about exact guarantees on the performance and behaviour of subsequent data analysis methods that work with the randomly projected data . Obtaining such results is not straightforward , and is subject to ongoing recent research efforts .
In this paper we consider the supervised learning problem of classifying a query point xq ∈ Rd as belonging to one of several Gaussian classes using Fisher ’s Linear Discriminant ( FLD ) and the classification error arising if , instead of learning the classifier in the data space Rd , we instead learn it in some low dimensional random projection of the data space R(Rd ) = Rk , where R ∈ Mk×d is a random projection matrix with entries drawn iid from the Gaussian N ( 0 , 1/d ) . FLD is one of the most enduring methods for data classification , yet it is simple enough to be tractable to detailed formal analysis in the projected domain .
The main practical motivations behind this research are the perspective of mitigating the issues associated with the curse of dimensionality by working in a lower dimensional space , and the possibility of not having to collect or store the data in its original high dimensional form .
A number of recent studies consider efficient learning in low dimensional spaces . For example , in [ 5 ] Calderbank et al demonstrate that if the high dimensional data points have a sparse representation in some linear basis , then it is possible to train a soft margin SVM classifier on a low dimensional projection of that data whilst retaining a classification performance that is comparable to that achieved by working in the original data space . However , the data points must be capable of a sparse representation which for a general class learning problem may not be the case .
In [ 9 ] Davenport et al prove high probability bounds ( over the choice of random projection ) on a series of signal processing techniques , among which are bounds on signal classification performance for a single test point using NeymanPearson detector and on the estimation of linear functions of signals from few measurements when the set of potential signals is known but with no sparsity requirement on the signal . Similarly , in [ 12 ] Haupt et al demonstrate that ( m + 1) ary hypothesis testing can be used to specify , from few measurements , to which of a known collection of prototypes a signal belongs . More recently bounds on least squares regression in projected space , with no sparsity requirement on the data , have been presented by Maillard and Munos in [ 14 ] . learned classifier . Our approach is to bound :
We also do not require our data to have a sparse representation . We ask whether we can still learn a classifier using a low dimensional projection of the data ( the answer is ‘yes’ ) , and to what dimensionality k can the data be projected so that the classifier performance is still maintained .
We approach these questions by bounding the probability that the classifier assigns the wrong class to a query point if the classifier is learned in the projected space . Such bounds on the classification error for FLD in the data space are already known , for example they are given in [ 4 , 15 ] , but in neither of these papers is classification error in the projected domain considered ; indeed in [ 9 ] it is stated that establishing the probability of error for a classifier in the projected domain is , in general , a difficult problem .
As we shall see in the later sections , our bounds are reasonably tight and as a direct consequence we find that the projection dimension required for good generalisation of FLD in the projection space depends logarithmically on the number of classes . This is , of course , typically very low compared to the number of training examples . Unlike the bounds in [ 3 ] , where the authors’ use of the Johnson Lindenstrauss Lemma1 has the unfortunate side effect that their bound loosens as the number of training examples increases , because of the increased accuracy of mean estimates our bound will tighten with more training data .
The structure of the remainder of the paper is as follows : We briefly describe the supervised learning problem and describe the FLD classifier . We then bound the probability that FLD will misclassify an unseen query point xq having learned the classifier in a low dimensional projection of the data space and applied it to the projected query point Rxq . This is equivalent to bounding the expected ( 0 , 1) loss of the projected FLD classifier . Finally we discuss our findings and indicate some possible future directions for this work .
Prxq [ ˆh(xq ) 6= y : xq ∼ Dx ] = Exq [ L(0,1)(ˆh(xq ) , y ) ] and then to bound the corresponding probability in a random projection of the data : PrR,xq [ ˆh(Rxq ) 6= y : xq ∼ Dx ] = ER,xq [ L(0,1)(ˆh(Rxq ) , y ) ]
For concreteness and tractability , we will do this for the Fisher Linear Discriminant ( FLD ) classifier , which is briefly reviewed below .
3 . FISHER’S LINEAR DISCRIMINANT
FLD is a generative classifier that seeks to model , given training data TN , the optimal decision boundary between classes . It is a successful and widely used classification method . The classical version is formulated for 2 class problems , as follows . If π0 , Σ = Σ0 = Σ1 and µ0 and µ1 are known then the optimal classifier is given by Bayes’ rule [ 4 ] :
( 1 − π0)f1(xq )
π0f0(xq )
> 0 h(xq ) = 1fllog = 1fllog
π0
1 − π0
+ ( µ1 − µ0)T Σ−1xq −
( µ0 + µ1 )
2
> 0 where 1(A ) is the indicator function that returns one if A is true and zero otherwise , and fy is the probability density function of the y th data class , in its simplest form the multivariate Gaussian N ( µy , Σ ) , namely :
( 2π)d/2 det ( Σ)1/2−1 exp−
1 2
( x − µy)T Σ−1(x − µy )
In the sequel , we shall assume that the observations x are drawn from one of m+1 multivariate Gaussian classes2 Dx = y=0 πyN ( µy , Σ ) with unknown parameters µy and Σ that we need to estimate from training data . As usual , µy is a vector of means and Σ is a full rank covariance matrix .
Pm
2 . THE SUPERVISED LEARNING PROBLEM
In a supervised learning problem we observe N examples i=1 where ( xi , yi ) iid∼ D of training data TN = {(xi , yi)}N some ( usually unknown ) distribution with xi ∼ Dx ⊆ Rd and yi ∼ C , where C is a finite collection of class labels partitioning D . For a given class of functions H , our goal is to learn from TN the function ˆh ∈ H with the lowest possible generalisation error in terms of some loss function L . That is , find ˆh such that L(ˆh ) = arg min Exq [ L(h) ] , where ( xq , y ) ∼ D is a query h∈H point with unknown label y . Here we use the ( 0 , 1) loss L(0,1 ) as a measure of performance defined by :
L(0,1 ) =fl 0
1 if ˆh(xq ) = y otherwise .
In the case we consider here , the class of functions H consists of instantiations of FLD learned on randomly projected data , TN = {(Rxi , yi ) : Rx ∈ Rk , x ∼ Dx} and we seek to bound the probability that an arbitrarily drawn and previously unseen query point xq ∼ Dx is misclassified by the
1For a proof of the JLL and its application to random projection matrices see eg [ 8 , 1 ]
4 . RESULTS
4.1 Roadmap
Our main result is theorem 4.8 , which bounds the estimated misclassification probability of the two class Fisher ’s Linear Discriminant classifier ( FLD ) when the classifier is learnt from a random projection of the original training data and the class label is assigned to a query point under the same random projection of the data . Our bound holds on average over the random projection matrices R , in contrast to other bounds in the literature [ 8 , 3 ] where the techniques depend upon the fact that under suitable conditions randomly projected data satisfies the Johnson Lindenstrauss Lemma with high probability .
411
Structure of proof
We commence by an analysis of the error probability of FLD in the data space , which provides us with an upper bound on the error probability in a suitable form to make our subsequent average error analysis tractable in the projection space . Here we also show how we can deal with multiclass cases , and we highlight the contribution of the 2The Gaussianity of the true class distributions can be relaxed to sub Gaussian class densities without any effort — see comment in the Appendix section 704
Table 1 : Notation used throughout this paper
Random vector Observation/class label pair Query point ( unlabelled observation ) Set of m + 1 class labels partitioning the data Random projection matrix ‘Data space’ real vector space of d dimensions ‘Projected space’ real vector space of k 6 d dimensions Mean of class y ∈ C Sample mean of class y ∈ C Covariance matrix of the Gaussian distribution Dxy Estimated ( model ) covariance matrix of the Gaussian distribution Dxy Prior probability of membership of class y ∈ C Estimated prior probability of membership of class y ∈ C x
( xi , yi ) xq
C = {0 , 1 , . . . , m}
R Rd Rk µy ˆµy Σ ˆΣ πy ˆπy estimated error and the estimation error terms in the overall generalisation error . Our data space analysis also has the advantage of being slightly more general than existing ones in eg [ 4 , 15 ] , in that it holds also for non Gaussian but sub Gaussian classes .
We then review some tools from matrix analysis in preparation for the main section . The proof of our main result , namely the bound on the estimated error probability of FLD in a random projection of the data space , then follows , along with the required dimensionality of the projected space as its direct consequence . 4.2 Preliminaries : Data space analysis of FLD
421 Bound on two class FLD in the data space
Our starting point is the following lemma which , for com pleteness , is proved in the appendix :
Lemma 41 Let xq ∼ Dx . Let H be the class of FLD functions and let ˆh be the instance learned from the training data TN . Assume that we have sufficient data so that κy = ( ˆµ¬y + ˆµy − 2µy)T ˆΣ−1(ˆµ¬y − ˆµy ) − 2 log 1−ˆπy > 0 ( ie µy ˆπy and ˆµy lie on the same side of the estimated hyperplane ) ∀y , ¬y ∈ C = {0 , 1} , y 6= ¬y . Then the probability that xq is misclassified is bounded above by Prxq [ ˆh(xq ) 6= y ] 6
π0 exp
 
−
1 8
( 1−π0 ) exp
 
−
1 8 h(ˆµ1 + ˆµ0 − 2µ0)T ˆΣ−1(ˆµ1 − ˆµ0 ) − 2 log 1−ˆπ0
( ˆµ1 − ˆµ0)T ˆΣ−1Σ ˆΣ−1(ˆµ1 − ˆµ0 ) h(ˆµ0 + ˆµ1 − 2µ1)T ˆΣ−1(ˆµ0 − ˆµ1 ) − 2 log ˆπ0
( ˆµ0 − ˆµ1)T ˆΣ−1Σ ˆΣ−1(ˆµ0 − ˆµ1 )
ˆπ0 i2
  1−ˆπ0 i2
+
  with µy the mean of the class from which xq was drawn , estimated class means ˆµ0 and ˆµ1 , model covariance ˆΣ , true class priors π0 and 1 − π0 , and estimated class priors ˆπ0 and 1 − ˆπ0 .
422 Multi class case
The multi class version of FLD may be analysed in exten sion to the above analysis as follows :
Lemma 42 Let C = {0 , 1 , . . . , m} be a collection of m + 1 classes partitioning the data . Let xq ∼ Dx . Let H be the class of FLD functions and let ˆh be the instance learned from the training data TN . Then , the probability that an unseen query point xq is misclassified by FLD is given by Prxq [ ˆh(xq ) 6= y ] 6 : m
X y=0
πy m
X i6=y exp
 
−
1 8 h(ˆµy − ˆµi − 2µy)T ˆΣ−1(ˆµy − ˆµi ) − 2 log ˆπi
ˆπy i2
( ˆµy − ˆµi)T ˆΣ−1Σ ˆΣ−1(ˆµy − ˆµi )
 
( 4.1 )
Proof . The decision rule for FLD in the multi class case is given by :
ˆh(xq ) = j ⇐⇒ j = arg max
{Pry(i|xq)} y , j , i ∈ C i
Without loss of generality , we again take the correct class to be class 0 and we assume uniform estimated priors , the nonuniform case being a straightforward extension of lemma 41 Hence :
ˆh(xq ) = 0 ⇐⇒ ^i6=0
{Pry(0|xq ) > Pry(i|xq)}
⇐⇒ ^i6=0fl Pry(0|xq )
Pry(i|xq )
> 1
( 4.2 )
( 4.3 ) and so misclassification occurs when :
ˆh(xq ) 6= 0 ⇐⇒ _i6=0fl Pry(i|xq )
Pry(0|xq )
> 1
Then since if A ⇐⇒ B , Pr(A ) = Pr(B ) , we have :
Prxq [ ˆh(xq ) 6= 0 ] = Prxq 
Pry(0|xq )
_i6=0fl Pry(i|xq ) Prxq fl Pry(i|xq ) Prxq fllog
Pry(0|xq )
Pry(i|xq ) Pry(0|xq )
> 1  > 1 > 0 ( 4.5 )
( 4.4 )
6
= m m
Xi=1 Xi=1 where ( 4.4 ) follows by the union bound . Writing out ( 4.5 ) via Bayes’ rule , we find a sum of 2 class error probabilities of the form that we have dealt with earlier , so ( 4.5 ) equals : m
Xi=1
Prxq fllog
1 − π0
π0
+ ( ˆµi − ˆµ0)T ˆΣ−1xq −
ˆµ0 + ˆµi
2
> 0
( 4.6 ) The result for Prxq [ ˆh(xq ) 6= 0 ] given y = 0 now follows by applying the bounding technique used for the two class case m times to each of the m possible incorrect classes . The line of thought is then the same for y = 1 , . . . , y = m in turn .
Owing to the straightforward way in which the multiclass error bound boils down to sums of 2 class errors , as shown in lemma 4.2 above , it is therefore sufficient for the remainder of the analysis to be performed for the 2 class case , and for m + 1 classes the error will always be upper bounded by m times the greatest of the 2 class errors . This will be used later in Section 45
Next , we shall decompose the FLD bound of lemma 4.1 into two terms , one of which will go to zero as the number of training examples increases . This gives us the opportunity to assess the contribution of these two sources of error separately .
423 Decomposition of data space bound as sum of estimated error and estimation error
Lemma 43 Let xq ∼ Dx and let H be the class of FLD functions and ˆh be the instance learned from the training data TN . Write for the estimated error ˆB(ˆµ0 , ˆµ1 , ˆΣ , Σ , ˆπ0 ) =
1
Xy=0
πy exp 
−
1
ˆπy i2  8h(ˆµ1 − ˆµ0)T ˆΣ−1(ˆµ1 − ˆµ0 ) − 2 log 1−ˆπy (4.7 )
( ˆµ1 − ˆµ0)T ˆΣ−1Σ ˆΣ−1(ˆµ1 − ˆµ0 ) and B(ˆµ0,1 , ˆµ0,1 , µy , ˆΣ , Σ , ˆπ0 ) for the right hand side of lemma 41 Then Prxq [ ˆh(xq ) 6= y ] 6
ˆB(ˆµ0 , ˆµ1 , ˆΣ , Σ , ˆπy ) + C ·Xy,i
|ˆµyi − µyi|
( 4.8 )
∂By∈C with C = maxy,i supnfififi the class from which xq was drawn , estimated class means ˆµy with ˆµyi the i th component , model covariance ˆΣ , and estimated class priors ˆπy and 1 − ˆπy .
∂µyi fififio a constant , µy the mean of
Proof . We will use the mean value theorem3 , so we start by differentiating By∈C with respect to µ0 . Writing ˆB0 and ˆB1 for the two exp terms in ( 4.7 ) , we have :
∇µ0 B = π0 ˆB0 ×
1 2
κ0 ˆΣ−1(ˆµ1 − ˆµ0 )
( 4.9 )
Since the exponential term is bounded between zero and one , the supremum of the i th component of this gradient exists provided that |ˆµ1 + ˆµ0 − 2µ0| < ∞ and |ˆµ1 − ˆµ0| < ∞ . So we have that
B 6 π0 ˆB0 + max i
|ˆµ0i − µ0i| . . . supflfifififi
∂By∈C
∂µ0i fifififi
Xi
. . . + ( 1 − π0)Prxq [ ˆh(xq ) 6= 1|y = 1 ]
( 4.10 )
Now applying the mean value theorem again wrt µ1 decomposes the latter term similarly , then taking the maximum over both classes yields the desired result . We call the two terms obtained in ( 4.8 ) the ‘estimated error’ and ‘estimation error’ respectively . The estimation error can be bounded using Chernoff bounding techniques , and converges to zero with increasing number of training examples . 3Mean value theorem in several variables : Let f be differentiable on S , an open subset of Rd , let x and y be points in S such that the line between x and y also lies in S . Then : f ( y ) − f ( x ) = ( ∇f ( (1 − t)x + ty))T ( y − x ) , t ∈ ( 0 , 1 )
In the remainder of the paper , we will take uniform model priors for convenience . Then the exponential terms of the estimated error ( 4.7 ) are all equal and independent of y , so using thatPy∈C πy = 1 the expression ( 4.7 ) of the estimated error simplifies to : ˆB(ˆµ0 , ˆµ1 , ˆΣ , Σ ) =
−
1 8 exp 
· h(ˆµ1 − ˆµ0)T ˆΣ−1(ˆµ1 − ˆµ0)i2 ( ˆµ1 − ˆµ0)T ˆΣ−1Σ ˆΣ−1(ˆµ1 − ˆµ0) 
( 4.11 )
We now have the groundwork necessary to prove our main result , namely a bound on this estimated misclassification probability if we choose to work with a k dimensional random projection of the original data . From the results of lemma 4.3 and lemma 4.2 , in order to study the behaviour of our bound , we may restrict our attention to the two class case and we focus on bounding the estimated error term — which , provided sufficient training data , is the main source of error . Before proceeding , the next section gives some technical tools that will be needed . 4.3 Tools from matrix analysis
First , we note that due to linearity of both the expectation operator E[· ] , and the random projection matrix R , the projection of the true mean µ and sample mean ˆµ from the data space to the projected space coincides with the true mean Rµ and the sample mean Rˆµ in the projected space . Furthermore , the projected counterparts of the true covariance matrix Σ and the model covariance ˆΣ are given by RΣRT and R ˆΣRT respectively . Hence , we may talk about projected means and covariances unambiguously .
We will , in the proof that follows , make frequent use of several results . Apart from lemma 4.6 these are more or less well known , but for convenience we state them here for later reference .
Lemma 4.4 ( Rayleigh quotient . ( [13 ] , Theorem 422 Pg 176) ) . If Q is a real symmetric matrix then its eigenvalues λ satisfy :
λmin(Q ) 6 vT Qv vT v
6 λmax(Q ) and , in particular :
λmin(Q ) = min v6=0
λmax(Q ) = max v6=0 vT Qv vT v vT Qv vT v
= min vT v=1 vT Qv and
( 4.12 )
= max vT v=1 vT Qv
( 4.13 )
Lemma 4.5 ( Poincar´e Separation Theorem . ( [13 ] , Corollary 4316 Pg 190) ) . Let S be a symmetric matrix S ∈ Md , let k be an integer , 1 6 k 6 d , and let r1 , . . . , rk ∈ Rd be k orthonormal vectors . Let T = rT i Srj ∈ Mk ( that is , in our setting , the rT i are the rows and the rj the columns of the random projection matrix R ∈ Mk×d and so T = RSRT ) . Arrange the eigenvalues λi of S and T in increasing magnitude , then :
λi(S ) 6 λi(T ) 6 λi+n−k(S ) , i ∈ {1 , . . . , k} and , in particular :
λmin(S ) 6 λmin(T ) and λmax(T ) 6 λmax(S )
Lemma 4.6 ( Corollary to lemmata 4.4 and 45 ) Let Q be symmetric positive definite , such that λmin(Q ) > 0 and so Q is invertible . Let u = Rv , v ∈ Rd , u 6= 0 ∈ Rk . Then : uT hRQRTi−1 u > λmin(Q−1)uT u > 0
Proof : We use the eigenvalue identity λmin(Q−1 ) = 1/λmax(Q ) . Combining this identity with lemma 4.4 and lemma 4.5 we have :
λmin([RQRT ]−1 ) = 1/λmax(RQRT )
Since RQRT is symmetric positive definite . Then by positive definiteness and lemma 4.5 it follows that : 0 < λmax(RQRT ) 6 λmax(Q )
( 4.14 )
1/λmax(RQRT ) > 1/λmax(Q ) > 0 ⇐⇒ ⇐⇒ λmin([RQRT ]−1 ) > λmin(Q−1 ) > 0
( 4.15 )
( 4.16 )
And so by lemma 4.4 : uT hRQRTi−1 u > λmin([RQRT ]−1)uT u ( 4.17 )
> λmin(Q−1)uT u = uT u/λmax(Q ) > 0
( 4.18 )
( 4.19 )
Lemma 4.7 ( Kantorovich Inequality . ( [13 ] , Theorem 7441 Pg 444) ) . Let Q be a symmetric positive definite matrix Q ∈ Md with eigenvalues 0 < λmin 6 . . . 6 λmax . Then , for all v ∈ Rd :
( vT v)2
( vT Qv)(vT Q−1v )
>
4 · λminλmax
( λmin + λmax)2
With equality holding for some unit vector v . This can be rewritten :
( vT v)2 ( vT Qv )
> ( vT Q−1v ) ·
λmin 4 · λmax λmin2 1 + λmax
We now proceed to the promised bound .
4.4 Main result : Bound on FLD in the pro jected space
Theorem 48 Let xq ∼ Dx = N ( µy , Σ ) . Let H be the class of FLD functions and let ˆh be the instance learned from the training data TN . Let R ∈ Mk×d be a random projection matrix with entries drawn iid from the univariate Gaussian N ( 0 , 1/d ) . Then the estimated misclassification error ˆPrR,xq [ ˆh(Rxq ) 6= y ] is bounded above by :
1 +
1 4 g( ˆΣ−1Σ ) ·
1 d kˆµ1 − ˆµ0k2
λmax(Σ ) −k/2
( 4.20 ) with µy the mean of the class from which xq was drawn , estimated class means ˆµ0 and ˆµ1 , model covariance ˆΣ , and g(Q ) = 4 · λmax(Q )
.
λmin(Q ) ·1 + λmax(Q )
λmin(Q)−2
Proof . We will start our proof in the dataspace , highlighting the contribution of covariance misspecification in the estimated error , and then make a move to the projected space with the use of a result ( lemma 4.9 ) that shows that this component is always non increasing under the random projection .
Without loss of generality we take xq ∼ N ( µ0 , Σ ) , and for convenience take the estimated class priors to be equal ie 1 − ˆπ0 = ˆπ0 . By lemma 4.1 , the estimated misclassification error in this case is upper bounded by :
−
1 8 exp 
· h(ˆµ1 − ˆµ0)T ˆΣ−1(ˆµ1 − ˆµ0)i2 ( ˆµ1 − ˆµ0)T ˆΣ−1Σ ˆΣ−1(ˆµ1 − ˆµ0) 
( 4.21 )
Now , in the Kantorovich inequality ( lemma 4.7 ) we can take : v = ˆΣ−1/2(ˆµ1 − ˆµ0 ) where we use the fact ( [13 ] , Theorem 726 , pg . 406 ) that since ˆΣ−1 is symmetric positive definite it has a unique symmetric positive semi definite square root :
ˆΣ−1/2 = ˆΣ−11/2
= ˆΣ1/2−1
= ˆΣ−1/2T and we will take our positive definite Q to be Q = ˆΣ−1/2Σ ˆΣ−1/2 ( ibid . pg . 406 ) . Then , by lemma 4.7 we have the expression ( 4.21 ) is less than or equal to : exp
 
−
1 8
· ( ˆµ1 − ˆµ0)T ˆΣ−1/2 h ˆΣ−1/2Σ ˆΣ−1/2i−1
ˆΣ−1/2(ˆµ1 − ˆµ0 ) . . .
. . . × 4 ·
λmax ˆΣ−1Σ λmin ˆΣ−1Σ
·  1 +
λmax ˆΣ−1Σ λmin ˆΣ−1Σ
 
−2 
( 4.22 ) where the change in argument for the eigenvalues comes from the use of the identity eigenvalues(AB ) = eigenvalues(BA ) ( [16 ] , pg . 29 ) . After simplification we can write this as : exp−
1 8
· ( ˆµ1 − ˆµ0)T Σ−1(ˆµ1 − ˆµ0 ) · g( ˆΣ−1Σ ) ( 4.23 )
The term g( ˆΣ−1Σ ) is a function of the model covariance misspecification , eg due to the imposition of diagonal or spherical constraints on ˆΣ . The following lemma shows that this term of the error can only decrease or stay the same after a random projection .
Lemma 4.9 ( Non increase of covariance misspecification error in the projected space ) . Let Q be a symmetric positive definite matrix . Let K(Q ) = λmax(Q ) λmin(Q ) ∈ [ 1 , ∞ ) be the reciprocal of the condition number of Q . Let g(Q ) be as given in the theorem 48 Then , for any fixed k × d matrix R with orthonormal rows : g((R ˆΣRT )−1RΣRT ) > g( ˆΣ−1Σ )
( 4.24 )
Proof : We will show that g(· ) is monotonic decreasing with K on [ 1 , ∞ ) , then show that K((R ˆΣRT )−1RΣRT ) 6 K( ˆΣ−1Σ ) , and hence g((R ˆΣRT )−1RΣRT ) > g( ˆΣ−1Σ ) .
Step 1 We show that g is monotonic decreasing :
First note that for positive definite matrices 0 < λmin 6 λmax , and so K is indeed in [ 1 , ∞ ) . Differentiating g(· ) with respect to K we get : dg dK
=
4(1 + K ) − 8K
( 1 + K)3
=
4(1 − K ) ( 1 + K)3
Here the denominator is always positive on the range of K while the numerator is always non positive with maximum 0 at K = 1 . Hence g(· ) is monotonic decreasing on [ 1 , ∞ ) . orthonormalised rows ) . By lemma 4.9 , we can upper bound the projected space counterpart of ( 4.23 ) by the following :
Step 2 We show that K((R ˆΣRT )−1RΣRT ) 6 K( ˆΣ−1Σ ) :
We will show that if ˆΣ and Σ are symmetric positive definite and R is a random projection matrix then :
λmin([R ˆΣRT ]−1/2RΣRT [ R ˆΣRT ]−1/2 ) > λmin( ˆΣ−1Σ ) = λmin( ˆΣ−1/2Σ ˆΣ−1/2 )
( 4.25 )
( 4.26 ) and
λmax([R ˆΣRT ]−1/2RΣRT [ R ˆΣRT ]−1/2 ) 6 λmax( ˆΣ−1Σ ) = λmax( ˆΣ−1/2Σ ˆΣ−1/2 )
( 4.27 )
( 4.28 )
Combining these inequalities then gives :
K((R ˆΣRT )−1RΣRT ) 6 K( ˆΣ−1Σ )
We give a proof of the first inequality , the second being proved similarly .
First , by lemma 4.4 :
λmin([R ˆΣRT ]−1/2RΣRT [ R ˆΣRT ]−1/2 )
( 4.29 )
= min u∈Rk( uT [ R ˆΣRT ]−1/2RΣRT [ R ˆΣRT ]−1/2u uT u
)
( 4.30 )
Writing v = [ R ˆΣRT ]−1/2u so that u = [ R ˆΣRT ]1/2v then we may rewrite the expression ( 4.30 ) , as the following :
= min v∈Rkfl vT RΣRT v vT R ˆΣRT v
( 4.31 )
Writing w = RT v , and noting that the span of all possible vectors w is a k dimensional subspace of Rd , we can bound the expression 4.31 by allowing the minimal vector w ∈ Rd not to lie in this subspace :
> min w∈Rdfl wT Σw wT ˆΣw
( 4.32 )
Now put y = ˆΣ1/2w , with y ∈ Rd . This y exists uniquely since ˆΣ1/2 is invertible , and we may rewrite ( 4.32 ) as the following :
= min y∈Rd( yT ˆΣ−1/2Σ ˆΣ−1/2y yT y
)
( 4.33 )
= λmin( ˆΣ−1/2Σ ˆΣ−1/2 ) = λmin( ˆΣ−1Σ )
( 4.34 )
This completes the proof of the first inequality , and a similar approach proves the second . Taken together the two inequalities imply K( ˆΣ−1Σ ) > K([R ˆΣRT ]−1RΣRT ) as required .
Finally putting the results of steps 1 and 2 together gives the lemma 49
Back to the proof of theorem 4.8 , we now move into the low dimensional space defined by any fixed random projection matrix R ( ie with entries drawn from N ( 0 , 1/d ) and exp−
1 8
· ( ˆµ1 − ˆµ0)T RT hRΣRTi−1
R(ˆµ1 − ˆµ0 ) · g( ˆΣ−1Σ )
( 4.35 ) This holds for any fixed orthonormal matrix R , so it also holds for a fixed random projection matrix R .
Note , in the dataspace we bounded Prxq [ ˆh(xq ) 6= y ] but in the projected space we want to bound :
PrR,xq [ ˆh(Rxq ) 6= y ] = ER,xq [ L(0,1)(ˆh(Rxq ) , y ) ] = ER[Exq [ L(0,1)(ˆh(Rxq ) , y)]|R ]
( 4.36 )
( 4.37 )
This is the expectation of ( 4.35 ) wrt the random choices of R . So we have : PrR,xq [ ˆh(Rxq ) 6= y ]
6 ERexp−
1 8
6 ERexp−
1 8
·
· ( ˆµ1 − ˆµ0)T RT hRΣRTi−1
. . . × g( ˆΣ−1Σ)i
( ˆµ1 − ˆµ0)T RT R(ˆµ1 − ˆµ0 )
λmax(Σ )
R(ˆµ1 − ˆµ0 ) . . .
( 4.38 )
· g( ˆΣ−1Σ )
( 4.39 ) where the last step is justified by lemma 46 Now , since the entries of R where drawn iid from N ( 0 , 1/d ) , the term ( ˆµ1 − ˆµ0)T RT R(ˆµ1 − ˆµ0 ) = kR(ˆµ1 − ˆµ0)k2 is χ2 k distributed and ( 4.39 ) is therefore the moment generating function of a χ2 distribution . Hence we can rewrite ( 4.39 ) as :
=1 +
1 4
· g( ˆΣ−1Σ ) · kˆµ1 − ˆµ0k2 d · λmax(Σ)−k/2
( 4.40 )
A similar sequence of steps proves the other side , when xq ∼ N ( µ1 , Σ ) , and gives the same expression . Then putting the two terms together , applying the law of total probability with Py∈C πy = 1 finally gives theorem 48
441 Comment : Other projections R
Although we have taken the entries of R be drawn from N ( 0 , 1/d ) this was used only in the final step , in the form of the moment generating function of the χ2 distribution . In consequence , other distributions that produce inequality in the step from equation ( 4.39 ) to equation ( 4.40 ) suffice . Such distributions include sub Gaussians and some examples of suitable distributions may be found in [ 1 ] . Whether any deterministic projection R can be found that is both nonadaptive ( ie makes no use of the training labels ) and still yields a non trivial guarantee for FLD in terms of only the data statistics seems a difficult open problem . 4.5 Bound on the projected dimensionality k and discussion
For both practical and theoretical reasons , we would like to know to which dimensionality k we can project our original high dimensional data and still expect to recover good classification performance from FLD . This may be thought of as a measure of the difficulty of the classification task .
By setting our bound to be no more than δ ∈ ( 0 , 1 ) and solving for k we can obtain such a bound on k for FLD that guarantees that the expected misclassification probability ( wrt R ) in the projected space remains below δ :
Corollary 410 [ to Theorem 4.8 ] Let k , d , g(· ) , ˆµy , Σ , ˆΣ , C be as given in theorem 48 Then , in order that the probability of misclassification in the projected space remains below δ it is sufficient to take : k > 8 · dλmax(Σ ) kˆµi − ˆµjk2 · min i,j∈C,i6=j
1 g( ˆΣ−1Σ )
· log(m/δ )
( 4.41 )
Proof : In the 2 class case we have :
δ >1 +
1 4 log(1/δ ) 6
· g( ˆΣ−1Σ ) · k 2 log1 +
1 4 kˆµ1 − ˆµ0k2 dλmax(Σ ) −k/2
· g( ˆΣ−1Σ ) ·
⇐⇒
( 4.42 ) kˆµ1 − ˆµ0k2 dλmax(Σ ) ( 4.43 ) then using the inequality ( 1 + x ) 6 ex , ∀ x ∈ R we obtain : k > 8 · dλmax(Σ ) kˆµ1 − ˆµ0k2 ·
1 g( ˆΣ−1Σ )
· log(1/δ )
( 4.44 )
Using ( 4.44 ) and lemma 4.2 , it is then easy to see that to expect no more than δ error from FLD in an m + 1 class problem , the required dimension of the projected space need only be : k > 8 · dλmax(Σ ) mini,j∈C,i6=j kˆµi − ˆµjk2 ·
1 g( ˆΣ−1Σ )
· log(m/δ ) ( 4.45 ) as required .
We find it interesting to compare our k bound with that given in the seminal paper of Arriaga and Vempala [ 3 ] . The analysis in [ 18 ] shows that the bound in [ 3 ] for randomly projected 2 class perceptron classifiers is equivalent to requiring that the projected dimensionality k = O72 ·
L l2 · log(6N/δ )
( 4.46 ) where δ is the user specified tolerance of misclassification probability , N is the number of training examples , and L/l2 is the diameter of the data ( L = maxn=1,,N kxnk2 ) divided by the margin ( or ‘robustness’ , as they term it ) . In our bound , g(· ) is a function that encodes the quality of the model covariance specification , δ and k are the same as in [ 3 ] and the factor dλmax(Σ ) · kˆµ1 − ˆµ0k−2 — which , should be noted , is exactly the reciprocal of the squared class separation as defined by Dasgupta in [ 6 ] — may be thought of as the ‘generative’ analogue of the data diameter divided by the margin in ( 446 )
Observe , however , that ( 4.46 ) grows with the log of the training set size , whereas ours ( 4.44 ) grows with the log of the number of classes . This is not to say , by any means , that FLD is superior to perceptrons in the projected space . Instead , the root and significance of this difference lies in the assumptions ( and hence the methodology ) used in obtaining the bounds . The result in ( 4.46 ) was derived from the precondition that all pairwise distances between the training points must be approximately preserved uniformly cf . the Johnson Lindenstrauss lemma [ 8 ] . It is well understood [ 2 ] that examples of data sets exist for which the k = O(log N ) dimensions are indeed required for this . However , we conjecture that , for learning , this starting point is too strong a requirement . Learning should not become harder with more training points — assuming of course that additional examples add ‘information’ to the training set .
Our derivation is so far specific to FLD , but it is able to take advantage of the class structure inherent in the classification setting in that the misclassification error probability is down to very few key distances only — the ones between the class centers .
Despite this difference from [ 3 ] and approaches based on uniform distance preservation , in fact our conclusion should not be too surprising . Earlier work in theoretical computer science [ 6 ] proves performance guarantees with high probability ( over the choice of R ) for the unsupervised learning of a mixture of Gaussians which also requires k to grow logarithmically with the number of classes only . Moreover , our finding that the error from covariance misspecification is always non increasing in the projection space is also somewhat expected , in the light of the finding in [ 6 ] that projected covariances tend to become more spherical .
In closing , it is also worth noting that the extensive empirical results in eg [ 7 ] and [ 11 ] also suggest that classification ( including non sparse data ) requires a much lower projection dimension than that which is needed for global preservation of all pairwise distances cf . the JLL . We therefore conjecture that , all other things being equal , the difficulty of a classification task should be a function only of selected distances , and preserving those may be easier that preserving every pairwise distance uniformly . Investigating this more generally remains for further research . 4.6 Numerical validation
We present three numerical tests that illustrate and con firm our main results .
Lemma 4.9 showed that the error contribution of a covariance misspecification is always no worse in the low dimensional space than in the high dimensional space . Figure 1 shows the quality of fit between a full covariance Σ and its diagonal approximation ˆΣ when projected from a d = 100 dimensional data space into successively lower dimensions k . We see the fit is poor in the high dimensional space , and it keeps improving as k gets smaller . The error bars span the minimum and maximum of g([R ˆΣRT ]−1RΣRT ) observed over 40 repeated trials for each k . e c a p s d e t c e o r p e h t n i
) j
T R S R 1 −
]
’
T R S R
[ ( g t i f e c n a i r a v o c f o y t i l a u Q
Projecting d=100−dimensional full ( S ) & diagonal ( S’ ) covariance
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 0
20
40
60
Reduced dimension ( k )
80
100
Figure 1 : Experiment confirming Lemma 4.9 ; the error contribution of a covariance misspecification is always no worse in the projected space than in the data space . The best possible value on the vertical axis is 1 , the worst is 0 . We see the quality of fit is poor in high dimensions and improves dramatically in the projected space , approaching the best value as k decreases .
The second set of experiments demonstrates Corollary 4.10 of our Theorem 4.8 , namely that for good generalisation of FLD in the projected space , the required projection dimension k is logarithmic in the number of classes .
We randomly projected m equally distanced spherical unit variance 7 separated Gaussian classes from d = 100 dimensions and chose the target dimension of the projected space as k = 12 log(m ) . The boxplots in figure 2 show , for each m tested , the distribution of the empirical error rates over 100 random realisations of R , where for each R the empirical error was estimated from 500 independent query points . Other parameters being unchanged , we see the classification performance is indeed maintained with this choice of k .
Misclassification error when reduced dimension k=12 log(m )
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1 t e a r r o r r e l a c i r i p m E expected , the classification error in figure 3 decreases nearly exponentially as the projected dimensionality k tends to the data dimensionality d . We note also , from these empirical results , that the variability in the classification performance also decreases with increasing k . Finally , we observe that the worst performance in the worst case is still a weak learner that performs better than chance .
4.7 Two straightforward extensions
We proved our theorem 4.8 for classes with identical covariance matrices in order to ensure that our exposition was reasonably sequential , as well as to keep our notation as uncluttered as possible . However , it can be seen from the proof that this is not essential to our argument and , in particular , the following two simple extensions can be easily proved :
471 Different unimodal classes
By replacing Σ in equation ( 7.1 ) with Σy , the following analogue of theorem 4.8 can be derived for the 2 class case when the true class structure is Gaussian ( or sub Gaussian ) but with different class covariance matrices Σ0 and Σ1 :
1 4
π01 + . . . π11 +
1 4
+ . . . kˆµ1 − ˆµ0k2 d · λmax(Σ0)−k/2 d · λmax(Σ1)−k/2 kˆµ0 − ˆµ1k2
· g( ˆΣ−1Σ0 ) ·
· g( ˆΣ−1Σ1 ) ·
0
5
10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
Number of classes ( m )
Figure 2 : Experiment illustrating Theorem 4.8 & its Corollary 410 With the choice k = 12 log(m ) and kµi − µjk = 7 , ∀i 6= j , the classification performance is kept at similar rates while the number of classes m varies .
472 Different multimodal classes sians PMy
In a similar way if the classes have a multimodal structure then , by representing each class as a finite mixture of Gausi=1 wyiN ( µyi , Σyi ) , it is not too hard to see that provided the conditions of theorem 4.8 hold for the Gaussian in the mixture with the greatest contribution to the misclassification error , we can upper bound the corresponding form of equation ( 7.1 ) , for the case y = 0 as follows :
Misclassification errors in reduced dimension ; m+1 = 10 e t a r r o r r e l a c i r i p m E
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
10
15
20
25
30
35
45
40 70 Projected dimension ( k )
60
50
55
65
75
80
85
90
95 100
Figure 3 : Experiment illustrating Theorem 48 We fix the number of classes , m + 1 = 10 , and the data dimensionality , d = 100 , and vary the projection dimensionality k . The classification error decreases nearly exponentially as k → d .
The third experiment shows the effect of reducing k for a 10 class problem in the same setting as experiment two . As
M0
Exq hexp(ˆµ1 − ˆµ0)T κ0 ˆΣ−1xqi = Xi=1 w0i expµT
0iκ0 ˆΣ−1(ˆµ1 − ˆµ0 ) . . .
. . . +
1 2
( ˆµ1 − ˆµ0)T κ2 0
ˆΣ−1Σ0i ˆΣ−1(ˆµ1 − ˆµ0)!
6 max i∈{1,,M0}flexpµT
0iκ0 ˆΣ−1(ˆµ1 − ˆµ0 ) . . .
. . . +
1 2
( ˆµ1 − ˆµ0)T κ2 0
ˆΣ−1Σ0i ˆΣ−1(ˆµ1 − ˆµ0)! )
Where My is the number of Gaussians in the mixture for class y , wyi is the weight of the i th Gaussian in the mixture , and true covariance .
Pi wyi = 1 , and µyi , Σyi are the corresponding true mean
The proof then proceeds as before and the resultant bound , which of course is nowhere near tight , still gives k of the same order as the bound in theorem 48 In this setting , the condition κy > 0 implies that the centres of the Gaussian mixture components are at least nearly linearly separable . In the high dimensional data space this can still be a reasonable assumption ( unless the number of mixture components is large ) , but it is clear that in practice it is much less likely to hold in the projected space .
5 . CONCLUSIONS
We considered the problem of classification in non adaptive dimensionality reduction using FLD in randomly projected data spaces .
Previous results considering other classifiers in this setting gave guarantees on classification performance only when all pairwise distances were approximately preserved under projection . We conjectured that , if one were only interested in preserving classification performance , that it would be sufficient to preserve only certain key distances . We showed that , in the case of FLD , this is sufficient ( namely preserving the separation of the class means ) . We employed a simple generative classifier in our working , but one might imagine that eg for projected SVM it would be sufficient to preserve only the separation of the support vectors . Our only assumption on the data was that the distribution of data points in each class be dominated by a Gaussian and , importantly , we did not require our data to have a sparse or implicity low dimensional structure in order to preserve the classification performance . We also showed that misspecification of the covariance of the Gaussian in the projected space has a relatively benign effect , when compared to a similar misspecification in the original high dimensional data space , and we proved that if k = O log(m ) then it is possible to give guarantees on the expected classification performance ( wrt R ) of projected FLD .
One practical consequence of our results , and the other similar results in the literature , is to open the door to the possibility of collecting and storing data in low dimensional form whilst still retaining guarantees on classification performance .
Moreover , answering these questions means that we are able to foresee and predict the behaviour of a randomly projected FLD classifier , and the various factors that govern it , before actually applying it to a particular data set .
Future research includes an analysis of the behaviour of the projected classifier when there is only a small amount of training data , and the extension of our results to general multimodal classes .
We note that the finite sample effects are characterised by ( i ) the estimation error and ( ii ) the fact that when the condition κy > 0 holds in the data space it is possible that random projection causes it to no longer hold in the projected space . The estimation error ( i ) depends on the quality of the estimates of the class means and class covariances , and these can be analysed using techniques that are not specific to working in the randomly projected domain , eg [ 14 , 17 ] . Observe , however , that because there are fewer parameters to estimate , the estimation error in the projected space must be less than the estimation error in the data space . The second effect ( ii ) involves the probability that two vectors in the data space with angular separation θ ∈ ( 0 , π/2 ) , have angular separation θR > π/2 in the projected space following random projection . We can show that this probability is typically small , and our results regarding this effect are currently in preparation for publication [ 10 ] .
6 . REFERENCES
[ 1 ] D . Achlioptas . Database friendly random projections : Johnson Lindenstrauss with binary coins . Journal of Computer and System Sciences , 66(4):671–687 , 2003 .
[ 2 ] N . Alon . Problems and results in extremal combinatorics , Part I . Discrete Math , 273:31–53 , 2003 . [ 3 ] R . Arriaga and S . Vempala . An algorithmic theory of learning . Machine Learning , 63(2):161–182 , 2006 . [ 4 ] P . Bickel and E . Levina . Some theory for Fisher ’s linear discriminant function , ‘na¨ıve Bayes’ , and some alternatives when there are many more variables than observations . Bernoulli , 10(6):989–1010 , 2004 . [ 5 ] R . Calderbank , S . Jafarpour , and R . Schapire .
Compressed learning : Universal sparse dimensionality reduction and learning in the measurement domain . Technical report , Rice University , 2009 .
[ 6 ] S . Dasgupta . Learning Mixtures of Gaussians . In Annual Symposium on Foundations of Computer Science , volume 40 , pages 634–644 , 1999 .
[ 7 ] S . Dasgupta . Experiments with random projection . In
Uncertainty in Artificial Intelligence : Proceedings of the Sixteenth Conference ( UAI 2000 ) , pages 143–151 , 2000 .
[ 8 ] S . Dasgupta and A . Gupta . An Elementary Proof of the Johnson Lindenstrauss Lemma . Random Struct . Alg . , 22:60–65 , 2002 .
[ 9 ] MA Davenport , MB Wakin and RG Baraniuk .
Detection and estimation with compressive measurements . Technical Report TREE 0610 , Rice University , January 2007 .
[ 10 ] RJ Durrant and A . Kab´an . Finite Sample Effects in
Compressed Fisher ’s LDA . Unpublished ‘breaking news’ poster presented at AIStats 2010 , wwwcsbhamacuk/∼durranrj
[ 11 ] D . Fradkin and D . Madigan . Experiments with random projections for machine learning . In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining , page 522 . ACM , 2003 .
[ 12 ] J . Haupt , R . Castro , R . Nowak , G . Fudge and A . Yeh . Compressive sampling for signal classification . In Proc . 40th Asilomar Conf . on Signals , Systems , and Computers , pages 1430–1434 , 2006 .
[ 13 ] RA Horn and CR Johnson . Matrix Analysis . CUP ,
1985 .
[ 14 ] O A Maillard and R . Munos . Compressed
Least Squares Regression . In NIPS , 2009 .
[ 15 ] T . Pattison and D . Gossink . Misclassification
Probability Bounds for Multivariate Gaussian Classes . Digital Signal Processing , 9:280–296 , 1999 .
[ 16 ] KB Petersen and MS Pedersen . The Matrix
Cookbook . Technical University of Denmark , November 2008 .
[ 17 ] J . Shawe Taylor , C . Williams , N . Cristianini , and
J . Kandola . On the eigenspectrum of the Gram matrix and the generalization error of kernel PCA . IEEE Transactions on Information Theory , 51(7):2510–2522 , 2005 .
[ 18 ] T . Watanabe , E . Takimoto , K . Amano and
A . Maruoka . Random projection and its application to learning . In Randomness and Computation Joint Workshop ‘New Horizons in Computing’ and ‘Statistical Mechanical Approach to Probabilistic Information Processing’ , July 2005 .
 
Substituting κ0 back into the bound then yields , after some algebra , the following Prxq [ ˆh(xq ) 6= 0 ] 6
1
ˆπ0 i2 8h(ˆµ1 + ˆµ0 − 2µ0)T ˆΣ−1(ˆµ1 − ˆµ0 ) − 2 log 1−ˆπ0
( ˆµ1 − ˆµ0)T ˆΣ−1Σ ˆΣ−1(ˆµ1 − ˆµ0 )
− exp 
The second term , for when xq ∼ N ( µ1 , Σ ) , can be derived similarly and gives Prxq [ ˆh(xq ) 6= 1 ] 6
1
− exp 
( ˆµ0 − ˆµ1)T ˆΣ−1Σ ˆΣ−1(ˆµ0 − ˆµ1 )
1−ˆπ0i2 8h(ˆµ0 + ˆµ1 − 2µ1)T ˆΣ−1(ˆµ0 − ˆµ1 ) − 2 log ˆπ0
  the data ) : Prxq [ ˆh(xq ) 6= y ] = Py∈C Pr[xq ∼ N ( µy , Σ ) ] ·
Finally , putting these two terms together and applying the law of total probability ( since the classes in C partition
Pr[ˆh(xq ) 6= y|xq ∼ N ( µy , Σ) ] , we arrive at lemma 4.1 , ie that Prxq [ ˆh(xq ) 6= y ] 6
703 Comment 1
We should confirm , of course , that the requirement that κy > 0 is a reasonable one . Because the denominator in ( 7.2 ) is always positive the condition κy > 0 holds when :
( ˆµ¬y + ˆµy − 2µy)T ˆΣ−1(ˆµ¬y − ˆµy ) − 2 log 1 − ˆπy
ˆπy > 0
It can be seen that κy > 0 holds provided that for each class the true and estimated means are both on the same side of the decision hyperplane . Furthermore , provided that κy > 0 holds in the data space we can show that whp ( and independently of the original data dimensionality d ) it also holds in the projected space [ 10 ] , and so the requirement κy > 0 does not seem particularly restrictive in this setting .
704 Comment 2
We note that , in ( 7.1 ) it is in fact sufficient to have inequality . Therefore our bound also holds when the true distributions Dx of the data classes are such that they have a moment generating function no greater than that of the Gaussian . This includes sub Gaussian distributions , ie distributions whose tail decays faster than that of the Gaussian .
1
( ˆµ1 − ˆµ0)T ˆΣ−1Σ ˆΣ−1(ˆµ1 − ˆµ0 )
ˆπ0 i2  8h(ˆµ1 + ˆµ0 − 2µ0)T ˆΣ−1(ˆµ1 − ˆµ0 ) − 2 log 1−ˆπ0  1−ˆπ0i2 8h(ˆµ0 + ˆµ1 − 2µ1)T ˆΣ−1(ˆµ0 − ˆµ1 ) − 2 log ˆπ0
( ˆµ0 − ˆµ1)T ˆΣ−1Σ ˆΣ−1(ˆµ0 − ˆµ1 )
−
1
−
π0 exp  ( 1−π0 ) exp 
+
 
7 . APPENDIX
Proof . ( of lemma 4.1 ) We prove one term of the bound using standard techniques , the other term being proved similarly . Without loss of generality let xq have label y = 0 . Then the probability that xq is misclassified is given by Prxq [ ˆh(xq ) 6= y|y = 0 ] = Prxq [ ˆh(xq ) 6= 0 ] :
1 − ˆπ0
=Prxq log =Prxq κ0 log
+ ( ˆµ1 − ˆµ0)T ˆΣ−1xq − + ( ˆµ1 − ˆµ0)T κ0 ˆΣ−1xq −
2
ˆπ0 1 − ˆπ0
ˆπ0 for all κ0 > 0 . Exponentiating both sides gives :
ˆµ0 + ˆµ1
> 0
ˆµ0 + ˆµ1
2
> 0
=Prxqexp(ˆµ1 − ˆµ0)T κ0 ˆΣ−1xq − 6Exq exp(ˆµ1 − ˆµ0)T κ0 ˆΣ−1xq −
ˆµ0 + ˆµ1
1 − ˆπ0
2 +κ0 log + κ0 log
2
ˆπ0 > 1 ˆπ0
1 − ˆπ0
ˆµ0 + ˆµ1 by Markov inequality . Then , isolating terms in xq we have Prxq [ ˆh(xq ) 6= 0 ] 6
Exq hexp(ˆµ1 − ˆµ0)T κ0 ˆΣ−1xq . . .
. . . −
( ˆµ1 − ˆµ0)T κ0 ˆΣ−1(ˆµ0 + ˆµ1 ) +κ0 log
1 2
( ˆµ1 − ˆµ0)T κ0 ˆΣ−1(ˆµ0 + ˆµ1 ) + κ0 log
1 − ˆπ0
ˆπ0 ˆπ0 . . . . . . × Exq hexp(ˆµ1 − ˆµ0)T κ0 ˆΣ−1xqi
1 − ˆπ0
= exp−
1 2
This expectation is of the form of the moment generating function of a multivariate Gaussian and so :
Exq hexp(ˆµ1 − ˆµ0)T κ0 ˆΣ−1xqi =
0 κ0 ˆΣ−1(ˆµ1 − ˆµ0)+
( ˆµ1 − ˆµ0)T κ2 0
1 2 expµT
ˆΣ−1Σ ˆΣ−1(ˆµ1 − ˆµ0 )
( 7.1 ) where µ0 is the true mean , and Σ is the true covariance matrix , of Dxq .
Thus , we have the probability of misclassification is bounded above by the following : exp−
1 2
( ˆµ1 − ˆµ0)T κ0 ˆΣ−1(ˆµ0 + ˆµ1 ) + κ0 log
1 − ˆπ0
ˆπ0
. . .
. . .+ µT
0 κ0 ˆΣ−1(ˆµ1 − ˆµ0 ) +
1 2
( ˆµ1 − ˆµ0)T κ2 0
ˆΣ−1Σ ˆΣ−1(ˆµ1 − ˆµ0 )
Now , since this holds for every κ0 > 0 we may optimise the bound by choosing the best one . Since exponentiation is a monotonic increasing function , in order to minimise the bound it is sufficient to minimise its argument . Differentiating the argument wrt κ0 and setting the derivative equal to zero then yields :
κ0 =
( ˆµ1 + ˆµ0 − 2µ0)T ˆΣ−1(ˆµ1 − ˆµ0 ) − 2 log 1−ˆπ0 ˆπ0
2(ˆµ1 − ˆµ0)T ˆΣ−1Σ ˆΣ−1(ˆµ1 − ˆµ0 )
( 7.2 )
This is strictly positive as required , since the denominator is always positive ( Σ is positive definite , then so is ˆΣ−1Σ ˆΣ−1 ) , and the numerator is assumed to be positive as a precondition in the theorem .
