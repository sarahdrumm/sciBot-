Evolutionary Hierarchical Dirichlet Processes for Multiple
Correlated Time varying Corpora
Jianwen Zhang† ,
Yangqiu Song‡ ,
Changshui Zhang† ,
Shixia Liu‡
†State Key Laboratory of Intelligent Technology and Systems
†Tsinghua National Laboratory for Information Science and Technology †Department of Automation , Tsinghua University , Beijing 100084 , China
‡IBM Research – China , Beijing , China
†{jw zhang06@mails,zcs@mail}tsinghuaeducn ; ‡{yqsong,liusx}@cnibmcom
ABSTRACT Mining cluster evolution from multiple correlated time varying text corpora is important in exploratory text analytics . In this paper , we propose an approach called evolutionary hierarchical Dirichlet processes ( EvoHDP ) to discover interesting cluster evolution patterns from such text data . We formulate the EvoHDP as a series of hierarchical Dirichlet processes ( HDP ) by adding time dependencies to the adjacent epochs , and propose a cascaded Gibbs sampling scheme to infer the model . This approach can discover different evolving patterns of clusters , including emergence , disappearance , evolution within a corpus and across different corpora . Experiments over synthetic and real world multiple correlated timevarying data sets illustrate the effectiveness of EvoHDP on discovering cluster evolution patterns .
Categories and Subject Descriptors I26 [ Artificial Intelligence ] : Learning ; I53 [ Pattern Recognition ] : Clustering ; G.3 [ Probability and Statistics ] : Nonparametric statistics ; H28 [ Database Management ] : Database applications—Data mining
General Terms Algorithms ; Experimentation
Keywords Multiple correlated time varying corpora , clustering , mixture models , Bayesian nonparametric methods , Dirichlet processes
1 .
INTRODUCTION
Nowadays , we are surrounded by overwhelming quantities of textual materials from various heterogenous corpora ( eg , news , blogs ) everyday . The themes in these corpora are usually similar , however , diversity also exists . For example , news typically has more discussions on society , politics , and economics than blogs which might focus more on personal life . Even within a corpus , the
Figure 1 : The cluster “ financial crisis ” discovered by EvoHDP in 103,986 articles crawled from three types of web corpora : blogs , news and message boards . Each corpus is represented by a colored stripe . The height of a stripe is the proportion of the cluster in corresponding corpus . For each corpus , the top keywords of this cluster in each month are placed on the stripe , where the size of a keyword encodes its frequency in the cluster . The popularity of this cluster in each corpus varied over time . In addition , the cluster was first active in blogs and then became popular in news and message boards . popularity of themes may also vary over time , and some of them may first appear in blogs , and then spread to news and message boards . Fig 1 shows a real example of an evolving document cluster about “ financial crisis ” in three types of web corpora , including blogs , news and message boards .
To better understand the complex data , users not only want to examine the document clusters , but also want to discern the cluster evolution patterns over time and across corpora . Specifically , from multiple correlated time varying corpora , it is desirable to discover the following patterns : ( 1 ) clusters within each corpus at each time epoch , ( 2 ) shared clusters among different corpora at each epoch , ( 3 ) evolving patterns of clusters within a corpus , and ( 4 ) evolving patterns of clusters across corpora over time .
However , it is challenging to learn cluster evolution patterns from such complex data . The first challenge is how to model the clusters both across different corpora and over time . On the one hand , we need a single integrated model for all corpora to set up a global bookkeeper of clusters , otherwise we can not easily discern the evolution of a cluster across corpora over time . On the other hand , different corpora may share some clusters while also having their distinctive clusters . Hence the commonality and diversity should be both reflected in the single integrated model . The second challenge is how to model the time dependencies in the multiple corpora setting . It is very common the themes of a corpus evolves slowly along time , and thus the clustering patterns of adjacent time epochs usually exhibit strong correlations . Incorporating these correlations
1079 into the model in the multiple corpora setting is nontrivial . The last challenge is how to determine the cluster numbers . In time varying text data , a cluster may emerge and disappear . Consequently , the cluster numbers may change through time . It is awkward to require users to specify a cluster number at each time epoch for each corpus . Therefore a mechanism is preferred to automatically determine all the numbers of clusters .
The traditional clustering approaches deal with a single static data corpus . Hence the direct use of a general global clustering model on all data may fail to represent the diversity both over time and across different corpora . Beyond the classical clustering approaches , the most recent efforts only focus on tackling two sub problems . One is learning from multiple correlated text corpora , which aims to discover the related content across different text corpora as well as the distinctive information in each corpus [ 22 , 26 , 17 ] . Another is learning from a time varying data corpus , which aims to discover the evolving patterns in the corpus as well as the snapshot clusters at each time epoch [ 3 , 8 , 9 , 1 , 16 ] . Both types of approaches are not sufficient to tackle the above challenges .
To deal with above challenges , in this paper , we propose an evolutionary hierarchical Dirichlet process ( EvoHDP ) model , which extends the hierarchical Dirichlet process ( HDP ) [ 22 ] to a time evolving scenario . In EvoHDP , each HDP is built for multiple corpora at each time epoch , and the time dependencies are incorporated into adjacent epochs under the Markovian assumption . Specifically , the dependency is formulated by mixing two distinct Dirichlet processes ( DPs ) . One is the DP model for the previous epoch , and the other is an updating DP model . To infer the EvoHDP model , we also propose a cascaded Gibbs sampling scheme . The proposed EvoHDP model can effectively discover cluster evolution patterns over time and across corpora . Moreover , the cluster numbers can automatically be determined due to the infinity property of DP .
2 . RELATED WORK
In this section , we briefly introduce three categories of work related to this paper , including learning from multiple correlated data corpora , learning evolution patterns from a time varying corpus , and some initial efforts involving multiple dynamic data .
In the research of learning from multiple correlated data corpora , HDP is a milestone [ 22 ] . It extended DP [ 2 , 23 ] to model multiple correlated data corpora . In HDP , each data corpus is modeled by an infinite DP mixture model , and the infinite set of mixing clusters is shared among all corpora . Later works [ 16 , 6 ] relax the assumption of HDP and incorporate more correlations between different corpora . Besides HDP , other efforts [ 4 , 19 , 26 , 17 ] on this research topic are devoted to the extensions of the Latent Dirichlet Allocation ( LDA ) topic model [ 5 ] . However , none of them consider the problem of automatically determining the cluster/topic numbers . In fact , as pointed out in [ 22 ] , HDP can be used as an LDA based topic model , where the number of clusters can be automatically inferred from data . Therefore , HDP is more practical when users have little knowledge about the content to be analyzed .
In the research of learning evolutionary clusters from a timevarying corpus , evolutionary clustering [ 8 , 9 , 32 , 1 , 30 , 31 ] is a new research topic . Evolutionary clustering aims to preserve the smoothness of clustering results over time , while fitting the data of each epoch . Among the above works , the approaches in [ 1 , 30 , 31 ] utilized DP to automatically determine the cluster numbers . In fact , incorporating time dependencies into DP mixture models is a hot topic in the research of Bayesian nonparametric [ 20 , 10 , 7 , 16 , 15 , 14 ] . Moreover , some works have focused on extending LDA to dynamic topic models [ 3 , 27 , 25 ] . We noted that even though the title of [ 16 ] is similar to this paper , it actually presents an evolutionary DP mixture model for a single dynamic corpus .
To the best of our knowledge , there are four works that seemingly involve multiple time varying data but actually handle different problems [ 28 , 29 , 11 , 21 ] . Wang et al . [ 28 ] focused on detecting the simultaneous busting of some topics in multiple text streams . They did not concentrate on the evolving patterns but only the busting behavior of topics . Wang et al . [ 29 ] extended [ 28 ] to extract common topics from multiple text streams . They regarded that the underlying models of all streams are the same except that time delays exist between different streams . Hence they adjusted the timestamps of of all documents to synchronize multiple streams and then learned a common topic model . Their assumption pays more attention to aligning topics of different streams and thus degenerates the topic diversity among different streams . Leskovec et al . [ 11 ] focused on tracking the spreading behaviors of short phrases ( or “ memes ” ) across the web to represent news cycles . Although memes act as signatures of clusters/topics , they are not enough to represent clusters/topics as the context information is lost . Tang et al . [ 21 ] worked on the dynamic multi mode network with several types of actors . They aimed to provide a partition for each type of actors at each time epoch . There are two typical features in their problem : ( 1 ) the data to be handled is relational data , ie , linkages between actors are required ; ( 2 ) the partitions over time should be on a same set of actors while relationships among them vary over time . None of the above four works attempted to discover the cluster evolution over time and across corpora . On the contrary , this is exactly the focus of this paper . In addition , none of them handled the problem of automatically determining the cluster/topic numbers , which is one of the major considerations of our work . 3 . PRELIMINARIES
.
)
(
In this section , we briefly introduce DP and HDP . A DP can be considered as a distribution of a random probability measure1 G , and we write G ∼ DP(α0 , G0 ) , where α0 is a positive concentration parameter , and G0 is a base measure . Sethuraman [ 18 ] showed that a measure G drawn from a DP is discrete , by the following stick breaking construction : iid∼ G0 , ∼ GEM ( α0 ) , G =
{ϕk}∞ ∏k−1 i=1 ( 1 − ˜πi ) . We use boldface = ( πk)∞
( 1 ) The discrete set of atoms {ϕk}∞ k=1 are drawn from the base measure , and GEM ( α0 ) refers to such a process : ˜πk ∼ Beta(1 , α0 ) , πk = ˜πk k=1 to represent the vector , which will be followed in the rest of the paper . δϕk is a probability measure concentrated at ϕk . After observing draws θ1 , . . . , θn−1 from G , the posterior of G is still a DP
∑∞
πkδϕk k=1 k=1
,
( 2 )
G|θ1 , . . . , θn−1 ∼ DP
α0 + n − 1 , where mk is the number of draws in {θi}n−1 i=1 taking the same value ϕk . This posterior preserves the possibility of drawing a new distinct value from G0 but puts more concentration on observed values . mkδϕk + α0G0 α0 + n − 1
HDP uses multiple DPs to model multiple correlated corpora . In HDP , a global measure G0 is drawn from a DP ( γ , H ) , with concentration parameter γ and base measure H . Then , a set of measures {G j} j is drawn from a DP with base measure G0 . Each G j models the corpus j . Such a process is summarized as
G0 ∼ DP ( γ , H ) , G j|G0 , α0
( 3 ) Given the global measure G0 , G j ’s are conditionally dependent . Having G j , n j data samples {x ji}n j i=1 in each corpus j are drawn from 1In general , we can regard the measure as a distribution . iid∼ DP ( α0 , G0 ) .
1080 ( a )
( b )
Figure 2 : Graphical representation for HDP : circles denote random variables , oval nodes denote parameters , shaded nodes denote observed variables , and plates indicate replication . ( a ) HDP . ( b ) The stick breaking construction of HDP . the following mixture model iid∼ G j ,
{θ ji}i x ji ∼ F(x|θ ji ) ,
( 4 ) where F(x|θ ji ) is a distribution parameterized by θ ji to generate x ji , eg , that of an exponential family . Eqs . ( 3 ) and ( 4 ) together define the hierarchical Dirichlet process mixture model2 . The graphical representation for an HDP is described in Fig 2(a ) .
Moreover , according to Eq ( 1 ) , G0 has the form G0 = k=1 βkδϕk , iid∼ H , fi ∼ GEM(γ ) . Then it is shown in [ 22 ] that G j
∑∞ where ϕk can be constructed as
∑∞ j | fi , α0 ∼ DP ( α0 , fi ) ,
,
G j =
π jkδϕk
( 5 ) which means that different corpora share the same set of distinct atoms [ 22 ] . This is the stick breaking construction of HDP , and the corresponding graphical model is shown in Fig 2(b ) . k=1
4 . EVOLUTIONARY HDP
In this section , we begin with the introduction of the EvoHDP model , and then show how to infer the model using a proposed Gibbs sampling technique .
We first introduce the data settings and some notations which are useful for subsequent discussions . There are J corpora varying over T time epochs . Considering the possibility that no data observed for some corpora at an epoch , we denote the number of corpora at epoch t is Jt . At each epoch t , there are nt j data samples in corpus j , and we denote a data sample ( eg , a document3 ) as xt ji . We assume the underlying model to generate xt ji for corpus j at epoch t is an infinite mixture model j ) = j(x|Gt pt δϕk and f is the density of a distribution F(x|θ ) . where Gt j We call the density parameterized by a distinct atom ϕk as a mixing component , which describes a cluster.4 4.1 Model j(θ ) f ( x|θ)d θ , Gt
∑∞ k=1 πt jk
∫
=
We model the multiple correlated time varying corpora as a series of HDPs with time dependencies , as shown in the graphical presentation of Fig 3(a ) . Specifically , at each time epoch t , we use an HDP to model the multiple correlated corpora at that epoch and then put time dependencies between adjacent epochs based on the Markovian assumption . To build an overall bookkeeping of components for all epochs , we let these HDPs share an identical discrete base measure G , and G is drawn from DP(ξ , H ) with H as the base measure . We call G the overall measure . Moreover , for an HDP to 2We just call “ hierarchical Dirichlet process mixture model ” as HDP for short in this paper . 3Generally a data sample can be quite different . For example , when using HDP to formulate LDA topic model , a data sample is a word . 4When x represents a word , and F(x|θ ) is a multinomial distribution over a finite word vocabulary , the distribution F is often regarded as a “ topic ” [ 5 ] .
( a )
( b )
Figure 3 : The graphical representation for the EvoHDP model . ( a ) The original representation . ( b ) The stick breaking construction . model the corpora at epoch t , we use Gt 0 to denote the global measure at that epoch , and call it the snapshot global measure . Then , the local measure Gt j for the j th corpus at time epoch t is called the snapshot local measure . In this way , an EvoHDP has one more layer than the original HDP [ 22 ] .
The key issue of EvoHDP is how to incorporate time dependencies between adjacent epochs . We introduce two types of dependencies into different layers in EvoHDP to model the different evolving manner . In the following we will interpret this model stepby step to explain why we use this scheme and how it would be useful . 0 on Gt−1 0 is the measure for the components in all corpora at time t , the difference between Gt reflects the evolving of the global components in all corpora . We call this global time dependency .
The first is the dependency of snapshot global measure Gt 0
0 and Gt−1
. Since Gt
The second is the time dependency within a corpus , ie , the dependency of the snapshot local measure Gt j is the measure for the components within corpus j at time epoch t , the difference between Gt reflects the evolving of components within the corpus j . Then we call this intra corpus time dependency . j and Gt−1 j on Gt−1
. Since Gt
In Fig 3(a ) , we use dashed lines to represent the second type of dependencies , since in some cases ( eg in HDP based LDA ) , there are no intra corpus dependencies . The generation process of EvoHDP is as follows . 1 . Draw an overall measure G ∼ DP(γ , H ) . G plays a role of the overall component bookkeeping for all corpora at all epochs .
0 j j
2 . For each epoch t :
(
)
0
Gt 0
γt , wtGt−1
0 according to the overall measure G and the previous snapshot global measure Gt−1 : 0 ( 6 ) j for corpus j at epoch t is drawn according to the snapshot global measure Gt
21 Draw the snapshot global measure Gt + ( 1 − wt)G . 22 Draw the snapshot local measures {Gt }Jt j=1 . Each Gt ) 0 and the previous snapshot local measure Gt−1
∼ DP ( j + ( 1 − vt jGt−1 , vt }Jt 23 For data samples {{xt }nt j=1 , draw the parameters of the ji ∼ F(x|θt iid∼ Gt xt ji ) , j , component densities and generate the data samples : j ∼ DP Gt j)Gt 0
( 7 ) j i=1
αt 0
θt ji
: ji
. j j ji ) is a distribution parameterized by θt ji , eg , that from where F(x|θt an exponential family .
Compared to the original HDP model , two levels of time dependencies are incorporated in by Eq ( 6 ) and Eq ( 7 ) . When we set all wt and vt j to zero , the EvoHDP is a three layer HDP .
GjnjG0α0θjixji H γ Jnjα0xji γ JβπjzjiφkK(∞)H GtjntjGt0HθtjiJtxtjiGt+1jnt+1jGt+10θt+1jiJt+1xt+1jiGt−1jnt−1jGt−10αt−10γt−1θt−1jiJt−1xt−1ji
αt0γt
αt+10γt+1
G ξ wtvtjwt+1vt+1jπtjntjβtztjiJtxtjiπt+1jnt+1jβt+1zt+1jiJt+1xt+1jiπt−1jnt−1jβt−1αt−10γt−1zt−1jiJt−1xt−1ji
αt0γt
αt+10γt+1
νξ wtvtjwt+1vt+1jH φkK1081 ( a )
( b )
( a )
( b )
Figure 4 : The generation mechanism of Gt+1 . ( a ) A part insight into the generation process of restaurant j at epoch t + 1 . ( b ) The generation of tables of restaurant j at epoch t + 1 . j
Figure 5 : The generation mechanism of Gt+1 . ( a ) A part insight 0 into the generation process of the snapshot global measure . ( b ) The generation of meta tables of the franchise at epoch t + 1 .
Following the convention of DP related models [ 22 , 24 , 16 ] , we will also provide other two perspectives and a restaurant metaphor for the EvoHDP . They help us better understand the model and lay the foundations of the inference scheme introduced in Sec 44 4.2 The Stick Breaking Construction
According to the stick breaking construction ( Eq ( 1 ) ) of DP , we can write the explicit form of G : νkδϕk
G =
, k=1
∑∞ ∑∞ ∑∞ πt δϕk jk + ( 1 − vt j)fit .
βt k k=1 k=1
∼ GEM(ξ ) . fit ∼ DP
γt , ˆfit
( (
) )
Consequently , according to Eqs . ( 6 ) and ( 5 ) , G0 has the form
Gt 0
,
=
δϕk where ˆfit = wtfit−1 + ( 1 − wt ) . Similarly , we can also write the form of Gt j ∼ DP t
, j as αt 0
, ˆt j
,
,
( 8 )
( 9 )
Gt j = t−1 j where ˆt j
= vt j
In this way , we obtain the stick breaking construction for Evoji is the index of the
HDP , which is shown in Fig 3(b ) , where zt component emitting xt ji . According to this perspective , the EvoHDP provides a prior in }t , j of all corpora at all epochs which the snapshot models {Gt share the same infinite set of mixing components {ϕk}∞ k=1 . The differences among these snapshot models lie in the mixing weights . 4.3 Hierarchical Infinite Mixture Model and
}t , {Gt
A Restaurant Franchise Metaphor
0 j
Based on the stick breaking construction for a DP , if we continue to use the stick breaking constructions to represent t j and fit drawn from the two DPs in Eqs . ( 8 ) and ( 9 ) , we obtain the hierarchical infinite mixture model of EvoHDP . This perspective clearly interprets the generation mechanism of EvoHDP . ji , the left generation process of xt
We begin with the metaphor following the Chinese restaurant franchise ( CRF ) for HDP [ 22 ] . A corpus j is called a restaurant , and a global atom k is called a dish . We use a day to refer to a time epoch . We focus on the generation of component indicator zt ji . Having zt ji is straightforward , which is drawn from F(x|ϕzt ∑∞ 431 Generation of Snapshot Local Measure Gt j We first show the generation mechanism of each snapshot local measure Gt+1 δϕk , ie , the behavior of restaurant j in day t + 1 . Since t+1 ) as shown in Eq ( 9 ) , we can represent this DP using the stick breaking construction as t+1 j =
) , which is illustrated in Fig 4(a ) . We call τ a table , then ut+1 j ∼ GEM(αt+1 is drawn from DP(αt+1
∑∞ jτ }τ , {kt+1 iid∼ ˆt+1 ut+1 jτ δkt+1 k=1 πt+1 jk
, ut+1
, ˆt+1
( 10 ) is a
τ=1
) .
= jτ
0
0 ji j j j j j j j j j j j
( with probability ut+1 jτt+1 ji distribution on tables . We can explain Eq ( 10 ) as a peculiar way of dish serving in a restaurant . First , waiters place infinite number of tables . On each table τ , a dish kt+1 is selected from the local jτ table dish menu ˆt+1 by waiters . Then , when a customer i enters in restaurant j , he selects a table τt+1 from the local customer table ji menu ut+1 ) and enjoys the dish kt+1 on the jτt+1 ji = kt+1 table . Consequently , the component indicator zt+1 . Moreji jτt+1 ji over , t+1 plays the role of the local customer dish menu , which indicates the customers’ preference on dishes in day t + 1 . We then explain how a dish is placed on a table by the waiter . From Eq ( 10 ) , we see that {kt+1 jτ }τ , ie , a dish is drawn +(1− from the local table dish menu ˆt+1 vt+1 j is the local customerj dish menu in this restaurant yesterday and fit+1 is the global franchise menu of current day recommended by the franchise manager . Then for a table τ , a waiter select a dish kt+1 from yesterday ’s lojτ cal customer dish menu t j while from current day ’s global franchise menu fit+1 with probability 1 − vt+1 . This means in the franchise , a restaurant designs its localized menu by considering both yesterday ’s local taste and current day ’s franchise menu . iid∼ ˆt+1 . Notice that ˆt+1
)fit+1 is a mixture of two menus , where t j with probability vt+1
= vt+1 t j
∑
Obviously , it is possible that multiple tables have a same dish . In restaurant j in day t + 1 , we denote the number of tables with dish k as T t+1 jk . Then the total number of tables in restaurant j in day k T t+1 t + 1 is denoted as T t+1 jk . Then for a dish k , among the j• = tables , there are T t→t+1 T t+1 tables whose dishes are selected from jk yesterday ’s local customer dish menu t tables whose dishes are selected from current day ’s global franchise menu fit+1 . Then , ∀ k , we have T t+1 . This mechanism is shown in Fig 4(b ) . j , and T 0→t+1
= T t→t+1
+ T 0→t+1 jk jk jk jk jk j j
∑∞
0
= k=1 βt k
∑∞
432 Generation of Snapshot Global Measure Gt 0 Then we show the generation mechanism of the snapshot global measure Gt+1 δϕk , ie , how the franchise manager recommends the global franchise menu fit+1 to all restaurants . The procedure is explained in Fig 5(a ) . Since fit+1 is drawn from DP(γt+1 , ˆfit+1 ) as shown in Eq ( 8 ) , fit+1 can also be represented using the stickbreaking construction as ut+1 m δkt+1
( 11 ) We call each m a metatable , then ut+1 is a distribution on metatables . The manager has infinite number of empty metatables beforehand and he selects a dish kt+1 m for each metatable m from the metatable dish menu ˆfit+1 of day t + 1 . Remind that when a waiter in restaurant j place a table τ , with probability 1−vt+1 , he selects the jτ according to the global franchise menu fit+1 . Now he just dish kt+1 iid∼ ˆfit+1 , ut+1 ∼ GEM(γt+1 ) . m }m , {kt+1 fit+1 = m=1 m j zt+1jint+1jβt+1kt+1jτTt+1j•ut+1jαt+10τt+1jiπtjntjˆπtjztjiαt0 vt+1jkt+1jτTt→t+1j•πtjntjˆπtjztjiαt0 vt+1jβt+1kt+1jτT0→t+1j•Tt+1jk=Tt→t+1jk+T0→t+1jkβtβt−1γt γt+1 νwtwt+1ktjτT0→tj•kt+1mMt+1•mt+1jτT0→t+1j•Jt+1ut+1kt+1jτJtβtβt−1γt νwtwt+1ktjτT0→tj•kt+1mMt→t+1•Jtkt+1mM0→t+1•Mt+1k=Mt→t+1k+M0→t+1k1−wt+11082 need to select a metatable mt+1 jτ according to the waiter metatable menu ut+1 , and then the dish kt+1 on the metatable is the one he mt+1 jτ should select . Hence the global franchise menu fit+1 also plays the role of the waiter dish menu , which indicates how a waiter selects dishes for tables .
The dishes on the metatables of the franchise in day t + 1 also come from two menus , ie , the yesterday ’s global franchise menu fit and an overall menu . The overall menu reflects common taste of the franchise . It is also possible that different metatables have a same dish and we denote the number of metatables with dish k as Mt+1 metatables whose dishes are selected from yesterdays’ franchise menu , and M0→t+1 metatables whose dishes are selected from the overall menu . Then ∀ k , we have Mt→t+1 k metatables , there are Mt→t+1
. Among the Mt+1
+ M0→t+1
= Mt→t+1
. k k k k k k
4.4 A Cascaded Gibbs Sampler
The hierarchical infinite mixture model and the restaurant franchise metaphor actually define a Gibbs sampling scheme for EvoHDP . We can derive a cascaded Gibbs sampling procedure by sequentially sampling following variables .
∑∞ k
441 Sampling According to the generation of metatables introduced in Fig 5(b ) , what are drawn from the overall measure G = k=1 νkδϕk are the ∑ dishes on the metatables of all days designed by the franchise man∑ ager . We denote the number of all these metatables with dish k as Mk = t M0→t , and the total number of metatables drawn from k Mk . As G ∼ DP(ξ , H ) , assume we have known G as M• = the count variables {Mk}k , according to the property Eq ( 2 ) , the fififiξ , H,{Mk}K posterior of G is also a DP : ∼ DP ∑K G = = ( ν1 , . . . , νK , νu ) ∼ Dirichlet ( M1 , . . . ,MK , ξ ) . where K is the number of distinct dishes on all metatables . According to Sec 5.2 of [ 22 ] , G can be represented as
( 13 ) This augmented representation reformulates original infinite vector to an equivalent finite one with length K + 1 . Then is sampled from the Dirichlet distribution of Eq ( 13 ) .
∑K k=1 Mkδϕk ξ + M•
+ νuGu , Gu ∼ DP(ξ , H )
ξ + M• ,
 ,
νkδϕk
( 12 )
H +
G k=1 k=1
∑K
0 and Gt
∑K j are also represented using juGu , Gu ∼ DP(ξ , H ) .
+πt
Notice that in the following , Gt
= above augmented representation as Gt βt 0 k Then fit and t fit =
δϕk j are both represented as finite vectors βt jK , πt ju 1 uGu , Gt
, . . . , πt
, . . . , βt
K , βt u
πt j1
πt jk j =
δϕk j =
+βt
(
(
) k=1 k=1 t
,
)
.
∑ jk
•• =
442 Sampling fit and t j According to Fig 5(b ) , what are drawn from fit include two parts . j,k T 0→t dishes on the T 0→t One part is the T 0→t tables in all •• restaurants of day t . The second part is the Mt→t+1 dishes on the Mt→t+1 metatables of next day t + 1 . We call all these tables and • ) ( metatables drawn from fit as pseudo tables . We denote the number of pseudo tables with the same dish k as T t = T 0→t + •k . As fit ∼ DP Mt→t+1 , and assuming we have obtained the ( }k , the posterior of fit is also a DP . Similar to count variables {T t ) ∼ Dirichlet ( Eq ( 13 ) , fit can also be sampled from a Dirichlet distribution k , then T t ) )
γt , ˆfit
˜γt ·
( 14 )
(
•
,
, . . . , ˜βt K
, . . . , βt K
˜βt u , ˜βt 1
βt u , βt 1 k k k
( where ˜γt = γt + T t• , and (
1 ˜γt 1 ˜γt The sampling of t
˜βt u =
˜βt k
=
( where ˜αt 0 ju , πt πt j1 = αt 0
( (
, . . . , πt jK + N t j• , and 1 0vtπt−1 αt ˜αt 0 1 0vtπt−1 αt ˜αt 0
= jk jk
˜πt jk
˜πt ju = k k
) + γt(1 − wt)νk + T t (
γtwtβt−1 u + γt(1 − wt)νu γtwtβt−1 j is similar to that of fit : ˜πt ju , ˜πt j1
∼ Dir
˜αt 0
)
(
·
.
, . . . , ˜πt jK
+ N t jk
,
+ αt
+ αt k
0(1 − vt)βt 0(1 − vt)βt k
)
.
)
,
)
) )
,
( 15 )
( 16 )
( 17 )
( 18 )
( 19 ) p
(
)
(
)
(
) zt ji = k fififizt fififixt
443 Sampling zt ji j , sampling zt Given t ji is straightforward : ∝ p ji,··· zt xt ji = k ji ( ∫
( 20 ) where k ∈ {1 , . . . , K , u} . u refers to the index for the new component as introduced in Eq ( 12 ) . When k = u is sampled , we add a new component into the component bookkeeping . In Eq ( 20 ) , the first item is a prior p jk and the second item is a likelihood xt ji fififit ) fififit fififiϕk fififiXk¬t , ji , H ji = k,··· ji = k , . . . fififizt dϕk ,
( 21 )
= πt
= k xt ji
)
(
)
(
(
) zt ji
ϕk p p p
= f
, j j where Xk¬t , ji is the set of all the samples having been assigned to component k , other than xt ji . j k jk jk j•
∑
Sampling T t
444 As described in Sec 441 and Sec 442 , the posterior of , fit jk and Mt
(
T t jk and t fififi βt
+ T t→t+1
= nt j + T t→t+1 pseudo customers cluster into T t j depend on the count variables T t j• = jk and Mt k . k T t Remind that the count variable T t jk is the number of all tables in restaurant j at epoch t . These tables are occupied by the N t pseudo customers . A pseudo customer with dish k must have sat at a table with dish k . Hence ∀k , the N t = nt jk tables . As shown ) in [ 22 ] , given N t jk jk , the table number T t jk can be sampled from a Chinese Restaurant Process ( CRP ) ∼ CRP ,N t , πt−1 jπt−1 0vt αt + αt jk jk Notice that N t +T t→t+1 . As nt = nt jk is the number of customers jk with dish k , it can be counted from the component assignments . To know N t jk , we also need to know T t→t+1 in ( ( turn . Thus the variable T t jk can be sampled from following recursive procedure : T t→t+1 N t = nt jk , πt−1 T t jk
) , [ p , 1 − p ]
, which depends on T t+1 jk
∼ Multinomial
0(1 − vt jπt−1 αt 0vt
0(1 − vt
, T 0→t+1 fififi βt
( 22 ) ( 23 )
,N t
,N t
T t+1 jk j)βt k j)βt k
( 24 )
+ αt
)
)
( jk jk jk jk jk jk jk jk jk jk jk
.
, k k where p =
( 1−vt+1 j
( k
Similarly , we obtain the recursive sampling procedure for Mt k : ( 25 ) ( 26 ) ( 27 )
Mt→t+1 , M0→t+1 T t = T 0→t •k k Mt k
∼ Multinomial ( fififi νk , βt−1
+ γt(1 − wt)νk,T t
, [ q , 1 − q ]
γtwtβt−1
Mt+1
, k k k k k k
) )
( jk
+ T t→t+1 ,N t jk πt vt+1 j jk )βt+1 +vt+1
∼ CRP )
πt jk
. j k
+ Mt→t+1 ,Tk ∼ CRP . wt+1βt k where q =
( 1−wt+1)νk +wt+1βt k
1083 0
0
445 Sampling Hyper parameters The concentration parameters of DPs , ie , ξ , {γt} and {αt also be sampled by putting a vague gamma prior on them ∼ Ga(aα , bα ) .
ξ ∼ Ga(aξ , bξ ) , γt ∼ Ga(aγ , bγ ) , αt
( 28 ) The sampling method is the same as that in [ 22 ] . Moreover , the time dependency parameters wt and vt j can be taken as controlling parameters or also sampled using the method in [ 16 ] by putting a Beta prior for them and sampling from the posterior .
} , can
According to the sampling method for groups of variables described above , there are recursive dependencies along hierarchies and time epochs . We follow the dependencies of different sets of variables and design a cascaded Gibbs sample scheme . The procedure is summarized in Algorithm 1 .
∑
D = 2 and we set d xd = 200 . All the ϕk ’s are listed in Tab . 1 . Each corpus j at epoch t is a uniform mixture of 3 tables , and each table τ is associated with a dish indexed by kt jτ . We denote the dish associated with table τ as kt jτ . Hence this local model can be represented by pt j(x ) =
Multinomial
∑
(
)
3
. x ; ϕkt jτ
τ=1
Then nt j samples are drawn from this mixture model , which compose the corpus j at epoch t . More details of the data set are shown in Tab . 1 . In the conjunction area of “ Tables ( kt j3 ) ” , the triple j1 at row t and column j are the three dish indices to compose the local mixture model . In such a data set , different corpora overlap on some components , and the underlying models evolve over time . 5.2 Evaluation Criteria
, kt j2
, kt
1 3 for j = 1 to Jt do
Algorithm 1 A cascaded Gibbs sampling scheme ( one iteration ) 1 : for t = T to 1 do 2 : 3 : 4 :
Sampling Zt ∀k = 1 , . . . , K , sampling count variables T t→t+1 according to Eq ( 22 24 ) . j according to Sec 443
, T 0→t+1 jk jk
, and T t jk
5 : 6 : end for ∀k = 1 , . . . , K , sampling count variables Mt→t+1 according to Eq ( 25 27 ) . k
, M0→t+1 k
, and Mt k
7 : end for 8 : Sampling concentration parameters ξ , γt and αt 0 . 9 : Sampling according to Sec 441 10 : for t = 1 to T do 11 : 12 : 13 : 14 : 15 : end for
Sampling fit according to Sec 442 for j = 1 to Jt do j according to Sec 442
Sampling t end for
)
( p
ϕk ji|zt
4.5 Global and Local Components We not only need the component assignments Z , but also the component parameters {ϕk} . As introduced in Eq ( 21 ) , {ϕk} have been integrated out in the sampling process . Having assignments Z , we can obtain the posterior of a ϕk : ji = k,∀ t , j , i} , H )
∝ p ( ϕk|H ) p (
(
This distribution is a “ global ” one conditioned on data of all corpora from all epochs . In textual data , it denotes a global component k in the entire textual collection . If we limite the data in a corpus j at epoch t , we also obtain the posterior of ϕk as a local component ji = k,∀ i}|ϕk This is the component k as a local one in corpus j at epoch t . ji = k,∀ i} , H
∝ p ( ϕk|H ) p
{xt ji|zt ji|zt
)
ϕk p
. fififi{xt fififi{xt
( {xt ji|zt ji = k}|ϕk
.
)
5 . EXPERIMENTS ON SYNTHETIC DATA In this section , we use a synthetic data set from mixtures of multi nomial distributions to test our approach . 5.1 Data The data set consists of three time evolving corpora covering 5 time epochs . Hence ∀t , Jt , the number of corpora at epoch t , is 3 . There are totally K = 8 components ( dishes ) involved in all D∏ corpora . Each dish k is a 2 dimensional multinomial distribution F(x|ϕk ) = Multinomial(x ; ϕk ) , with density
∑D ∏D f ( x|ϕk ) = d=1 xd! d=1 xd! d=1
ϕxd k,d
,
∑ where x is a D dimensional nonnegative integer vector , and ϕk is a D dimensional nonnegative real vector with d ϕk,d = 1 . Here
We introduce several numerical criteria to evaluate two types of performances . The first type is the static performance on fitting training data and predicting held out data . The second type is the temporal performance on preserving correlation between epochs , including the correlation within a corpus and that across different corpora . 521 Two criteria are used to evaluate the static performance , the nor
Static Criteria malized mutual information ( NMI ) and perplexity .
NMI measures coherence between the clustering assignments and the true category labels . A higher value on NMI indicates a better clustering result . For each corpus j at each epoch t , having component assignments for all data samples , we can compute the value NMIt t , j 1 as the final result on the criterion of NMI . j for the corpus . Then we use average value t , j NMIt j
∑
∑
Perplexity is a standard metric in information retrieval . We denote the training set as Xtrain and the held out test set as Xtest , then the per sample perplexity of a model is defined based on the likelihood of “ generating the test set given the training set ” :
/
( fififiModel , Xtrain
) )
,
Perplexity = exp log p t , j,i xt ji,test
∑
(
− 1 ntest where xt ji,test the i th data sample in corpus j at time epoch t and ntest is the size of test data set . In text modeling , to eliminate the fluctuation caused by the different lengths of documents , the perword perplexity is often used instead , ie , ntest is the number of all the tokens int the test document collection . In this paper , we use log(perword perplexity ) and call it LogPerp . A lower value on LogPerp indicates better prediction performance . 522 Temporal Criteria We define three types of temporal criteria to evaluate the time dependency and model smoothness between time epochs .
Intra corpus temporal correlation ( IntraCorr ) is defined to describe the average correlation between adjacent epochs within a corpus :
∑
[
IntraCorr
⊤ ( t j )
( t+1 t , j E
1 . t , j
Inter corpora temporal correlation ( InterCorr ) is defined to describe the average correlation between different corpora of adjacent epochs :
InterCorr
△ =
⊤
( t j )
( t+1 l
)
1 , l , j .
△ =
∑
Global temporal correlation ( GCorr ) is defined to describe the correlation between global distributions Gt
0 of adjacent epochs : j
)
]/∑ ]/∑ ]/∑ t , j,l
[ t , j,l E
∑
[
△ =
GCorr
⊤ ( fit )
( fit+1 ) t E
1 t
Above three criteria measure the time dependencies from the as
1084 Table 1 : Synthetic data set .
3 0.3
4 0.4
Global components ( dishes ) 5 2 0.2 0.5
1 6 0.1 0.6 Local components ( tables ) and corpora sizes Tables ( kt , kt j1 j2 j = 2 2 , 3 , 4 3 , 4 , 5 4 , 5 , 6 5 , 6 , 7 j3 ) j = 3 3 , 4 , 5 4 , 5 , 6 5 , 6 , 7 6 , 7 , 8 j = 1 500 510 520 530 j = 1 1 , 2 , 3 2 , 3 , 4 3 , 4 , 5 4 , 5 , 6
, kt j = 2 300 320 320 340
Corpora sizes nt j k ϕk,1 t = 1 t = 2 t = 3 t = 4
7 0.7
8 0.8 j = 3 400 430 430 450
( a ) NMI
( b ) LogPerp
( c ) K
Figure 6 : Results on the synthetic data set : static performances , averaged on 10 fold cross validation .
Table 2 : Results on the synthetic data : local components and sizes . j = 1
HDP : k(size ) j = 2 j = 3 t = 1 t = 2 t = 3 t = 4
1 ( 111 ) , 2 ( 118 ) , 6 ( 121 )
3 ( 110 ) , 5 ( 120 ) , 14 ( 1 ) , 16 ( 126 )
9 ( 119 ) , 12 ( 131 ) , 16 ( 114 )
12 ( 117 ) , 13 ( 104 ) , 14 ( 149 ) , 16 ( 1 )
2(70 ) , 14(5 ) , 16(66 ) , 17(69 )
5(66 ) , 14(73 ) , 16(85 )
8(2 ) , 10(64 ) , 14(84 ) , 16(1 ) , 17(79 )
11(56),14(82),15(99 )
4(98 ) , 6(85 ) , 14(97 )
5(111 ) , 10(84 ) , 14(106 ) 8(90 ) , 10(103 ) , 14(115 )
7(112 ) , 8(91 ) , 10(110 ) , 14(2 ) j = 1 t = 1 t = 2 t = 3 t = 4
1(111 ) , 2(121 ) , 3(118 )
2(130 ) , 3(110 ) , 7(116 ) , 8(1 ) 2(112 ) , 7(135 ) , 8(115 ) , 3(2 ) 5(93 ) , 7(129 ) , 8(146),2(3 )
EvoHDP : k(size ) j = 2
2(66 ) , 3(70 ) , 7(72 ) , 8(2 ) 2(82),7(79),8(60),3(3 )
5(62 ) , 7(82 ) , 8(82),2(2 ) , 4(2 )
4(83 ) , 5(72 ) , 8(77),7(5 ) j = 3
2(82 ) , 7(106 ) , 8(92 )
5(78 ) , 7(113 ) , 8(107 ) , 2(3 ) 4(88 ) , 5(103 ) , 8(108 ) , 7(9 ) 4(95 ) , 5(109 ) , 6(103 ) , 8(8 ) pect of correlation . Higher values on them are favored when static performances are similar .
We also define another three types of criteria from the aspect of divergence between adjacent epochs’ distributions . On the contrary , lower values on them are favored when static performances are similar . They are defined as follows .
Intra corpus temporal KL divergence ( IntraKL )
IntraKL
1 . Inter corpora temporal KL divergence ( InterKL ) t , j E
KL t , j
∑
[
(
[
△ =
∑ j t j
)]/∑ ( fifififififit+1 )]/∑ fifififififit+1 )]/∑ fififififififit+1 t , j,l l
1 . t
1 , l , j .
Global temporal KL divergence ( GKL )
InterKL
△ = t , j,l E
∑
KL
[ t j
( t E
KL fit
△ =
GKL where KL(·||· ) is the KL divergence between two distributions . In all the temporal criteria defined above , the expectations are calculated by MCMC samples obtained during the cascaded Gibbs sampling process . 5.3 Settings and Results
Except for EvoHDP , we also ran a three layer HDP , which is just a special case of EvoHDP when all the time dependencies are removed . All the settings for EvoHDP and HDP are the same . The component model F(x|ϕk ) was set to a multinomial distribution , and the base measure H was set to the conjugate prior for F , a symmetric Dirichlet distribution with parameter 0.5 . The vague gamma priors in Eq ( 28 ) for the concentration parameters of EvoHDP and HDP were set to be Ga(10.0 , 10 ) An identical set of randomly generated component assignments was used to initialize both EvoHDP and HDP . In addition , to study the impact of time dependency parameters wt and vt = ω as a controlling parameter and swept ω in {0.1 , 0.3 , 0.5 , 0.7 , 09} We call the EvoHDP model with wt = vt = ω as “ EvoHDP ” , and call j the EvoHDP model with wt and vt j also sampled during the inference as “ EvoHDP spl ” . j , we also set wt = vt j
For the Gibbs sampling procedure of both models , we ran 20 chains , and set the burn in time as 1000 for each chain . After the burn in , from each chain another 50 MCMC samples were preserved , then we obtained 1000 MCMC samples to calculate the evaluation criteria . The models were evaluated via 10 fold cross validation , and all criteria results were averaged on the 10 rounds .
( a ) IntraCorr
( b ) InterCorr
( c ) GCorr
Figure 7 : Results on the synthetic data set : temporal correlations , averaged on 10 fold cross validation .
( a ) IntraKL
( b ) InterKL
( c ) GKL
Figure 8 : Results on the synthetic data set : temporal divergences , averaged on 10 fold cross validation .
For the legends in Figs . 6–9 , the the red dashed lines with errorbars are the results of HDP , and the purple lines with circles are the results of EvoHDP spl , and the black lines with down triangles are the results of HDP . In addition , the blue dotted lines with uptriangles are true values .
The static performances with different ω ’s are illustrated in Fig 6 . In a wide range of ω , EvoHDP achieves better results than HDP on NMI and LogPerp . The estimated component numbers are shown in Fig 6(c ) . We see that HDP tends to fit data by splitting into more components , and EvoHDP seems to eliminate this phenomenon .
The temporal performances are illustrated in Figs . 7 and 8 . The “ true ” values on the correlation and divergence criteria are calculated via the ground truth labels . EvoHDP achieves higher correlations and lower divergences than HDP , where larger ω gives stronger dependency between adjacent epochs . However , when ω becomes larger than a threshold ( eg larger than 0.7 ) , it seems it hurts the correlation criteria between epochs .
Tab . 3 and Tab . 2 further illustrate details of the clustering results . The two tables list a typical clustering result in one trial with ω = 0.8 . HDP produces much more components , and lots of them are actually similar and should be merged together .
6 . EXPERIMENTS ON REAL DATA
In this section , we report the experiments on a real online document collection . This data set consists of 103,986 text articles queried from a search engine , Boardreader5 , in which the time stamps of the articlles are ranged from July 2008 to December 2008 , using 20 financial companies’ names , eg , “ AIG insurance ” , “ Bank of America ” , “ State Farm ” , etc . All these articles come from three types of public websites , ie , news , blogs and message boards . We used the Mallet [ 13 ] package to pre process the data set . We removed the stop words and rare words ( appearing less than 10 times in the whole collection ) . After that , the vocabulary size of this text data set was W = 77 , 999 . Term frequencies were extracted to represent each article . We organized the data set into 5http://boardreader.com/
1085 k ϕk,1 Sizes
1 .10 111
2 .19 188
3 .19 110
4 .41 98
5 .40 297
6 .30 206
7 .80 112
HDP
8 .70 183
9 .50 119
10 .60 361
11 60 56
12 .40 248
13 .60 104
14 .50 714
15 .70 99
16 .30 393
17 .40 148 k ϕk,1 Sizes
1 .10 111
2 .30 601
3 .19 303
4 .71 268
5 .61 517
6 .81 103
7 .40 846
8 .51 798
EvoHDP
Table 3 : Results on the synthetic data : global components .
( a ) LogPerp
( b ) InterKL
( c ) GKL
( d ) K
Figure 9 : Results on real bank data set , averaged on 10 rounds .
Figure 10 : Overview of the overall clusters in all text corpora of all epochs . Each colored stripe represents a cluster , whose height is the number of articles assigned to the cluster . For each cluster , the top keywords of this cluster in each month are placed on the stripe , where the size of a keyword is proportional to its frequency in the cluster .
6 epochs by months , and each epoch was set as a month Hence we obtained a data set including 3 corpora along the 6 epochs . We regarded each article ( represented by a vector of term frequencies ) as a draw from a multinomial distribution with dimension W , ie , F(x|ϕk ) is a multinomial distribution . The base measure H was set to the conjugate Dirichlet prior with symmetric parameter 05
To compare EvoHDP with HDP using numerical criteria , we randomly sampled a smaller subset , which consisted of 5,353 articles with 19,420 distinct words as the vocabulary . Experimental settings and parameters were the same as those in Sec 53 All the results were also averaged on 10 fold cross validation . The numerical results are shown in Fig 9 . Since there is no ground truth labels for the data set , NMI can not be calculated and only the results of LogPerp are given . Limited to space , only two temporal criteria are provided here , ie , InterKL , and GKL . Consistent with the results on toy data , EvoHDP achieves lower LogPerp values and higher correlations ( low divergence values ) . The phenomenon is also observed here that HDP tends to split up into more components ( Fig 9(d ) ) to fit data and overlook the correlations .
To have a clear insight into the evolution patterns in the multiple correlated corpora discovered by EvoHDP , we further leveraged the time based topic visualization proposed by Liu et al . [ 12 ] to visually illustrate the analysis results on all the 103,986 text articles in several different views .
First , we present the overview of all text corpora including news , blogs and message boards , which is shown in Fig 10 . We can see five active clusters about “ financial crisis ” , “ election ” , “ trade market information ” , “ Barclays premier league ” and “ bank related ” . These five clusters are all finance related . For example , the “ elec tion ” cluster tells about Obama and Maccain ’s debate on bailout agreement , and the “ financial crisis ” cluster tells about the bankrupt events of largest companies such as Lehman and AIG .
Then , we present the clusters of different corpora , ie news , blogs and message boards respectively . In Figs . 11 ( a),(b ) , and ( c ) , we present the absolute values of document numbers in each cluster at each epoch . In Figs . 11 ( d),(e ) , and ( f ) , we present the mixing proportions of clusters in a corpus at each epoch .
From these figures we find several interesting patterns . ( 1 ) The three corpora are similar but diversity exists . All of the three corpora are interested in “ financial crisis ” . However , blogs and message boards focus more on “ election ” than news ; news and message boards focus more on “ Barclays premier league ” ; and news focuses more on general “ bank related information ” and “ trade market information ” . This may be because news tends to report more time sensitive events , while blogs and message boards like to have a deep discussion on a particular event or the progress of an affair . ( 2 ) Correlations over time and across different corpora . For each corpus , the clusters change smoothly along time . Since we added the time dependencies between adjacent epochs , the cluster content does not change too much . Moreover , there are some clus(3 ) Cluster emerging and ters shared across different corpora . disappearing . The cluster “ financial crisis ” emerged in news and message boards in September due to the bankrupt of two financial companies , Lahman and AIG . The cluster “ election ” emerged in blogs and message boards in September 2008 due to the televised presidential debate between Obama and McCain . We also note that there emerged a strange cluster ( “ ads./noise ” ) in blogs from October , with keywords like “ movies ” , “ videos ” , “ sex ” etc . . When we digged into the content , we found that the articles were full of noisy advertisement information . This may be because the blogs became very hot after September 2008 , and then more and more automatic robots came to the site and presented nonsense information . In addition , we also observe that the cluster “ Barclays premier league ” disappeared in December , after Barclays determined to renew its sponsorship of the premier league and a new league season started . To better compare the evolving behaviors of different clusters , we select three clusters from different corpora in the same view . The three clusters “ Barclays premier league ” , “ election ” , and “ financial crisis ” are shown in Fig 12(b ) , Fig 12(a ) and Fig 1 , respectively . From the three figures , besides clearer witness of the previous three patterns , another two patterns are also discovered . ( 4 ) Cluster evolution within a corpus . First , the strength of a cluster in corresponding corpus varies overtime , which can be clearly observed in all the three figures . Second , the most frequent keywords of a cluster also vary over time , reflecting the evolving of the content of a cluster . Taking the cluster “ election ” ( Fig 12(a ) ) as an example , in blogs , in August , the keywords “ Obama ” , “ Bush ” , “ presidential ” , “ democratic ” indicated the normal features before an election . However , in September , “ McCain ” appeared as the hottest keywords as Republicans nominated John McCain for president in September 4th . Besides , crisis related keywords such as “ crisis ” , “ aig ” , “ ( wall ) street ” became hot due to the break out of financial crisis in this month . More such features can also be observed in later months and other clusters . ( 5 ) Cluster evolution across different corpora . In Fig 1 and Fig 12(a ) , it is clear that both “ crisis ” and “ election ” clusters were first active in blogs , and then became popular in news and message boards .
1086 ( a ) Blogs
( b ) Mixture Components in Blogs
( c ) News
( d ) Mixture Components in News
( e ) Message Boards
( f ) Mixture Components in Message Boards
Figure 11 : Clusters in each corpus . For the two rows of figures , the meaning of the height of a colored stripe is different . In the first row , the width is the number of articles assigned to the cluster , ie , nt jk , while in the second row , it is the proportion of the cluster at that epoch , ie , πt jk .
1087 [ 4 ] D . Blei and J . Lafferty . A correlated topic model of Science . Annals of Applied Statistics , 1(1):17–35 , 2007 .
[ 5 ] D . Blei , A . Ng , M . Jordan , and J . Lafferty . Latent Dirichlet allocation .
Journal of Machine Learning Research , 3(4 5):993–1022 , 2003 .
[ 6 ] D . M . Blei and P . I . Frazier . Distance dependent Chinese restaurant processes . arXiv , October 2009 .
[ 7 ] F . Caron , M . Davy , and A . Doucet . Generalized Polya urn for time varying Dirichlet process mixtures . In UAI , 2007 .
[ 8 ] D . Chakrabarti , R . Kumar , and A . Tomkins . Evolutionary clustering .
In KDD , 2006 .
[ 9 ] Y . Chi , X . Song , D . Zhou , K . Hino , and B . L . Tseng . Evolutionary spectral clustering by incorporating temporal smoothness . In KDD , 2007 .
[ 10 ] J . Griffin and M . Steel . Order based dependent Dirichlet processes . Journal of the American Statistical Association , 101(473):179–194 , 2006 .
[ 11 ] J . Leskovec , L . Backstrom , and J . Kleinberg . Meme tracking and the dynamics of the news cycle . In KDD , 2009 .
[ 12 ] S . Liu , M . X . Zhou , S . Pan , W . Qian , W . Cai , and X . Lian . Interactive , topic based visual text summarization and analysis . In CIKM , 2009 . [ 13 ] A . K . McCallum . Mallet : A machine learning for language toolkit . http://malletcsumassedu , 2002 .
[ 14 ] I . Pruteanu Malinici , L . Ren , J . Paisley , E . Wang , and L . Carin .
Hierarchical Bayesian modeling of topics in time stamped documents . IEEE Transactions on Pattern Analysis and Machine Intelligence , to appear .
[ 15 ] L . Ren , D . Dunson , S . Lindroth , and L . Carin . Dynamic nonparametric Bayesian models for analysis of music . Journal of the American Statistical Association , to appear .
[ 16 ] L . Ren , D . B . Dunson , and L . Carin . The dynamic hierarchical
Dirichlet process . ICML , 2008 .
[ 17 ] K . Salomatin , Y . Yang , and A . Lad . Multi field correlated topic modeling . In SDM , 2009 .
[ 18 ] J . Sethuraman . A constructive definition of Dirichlet priors . Statistica
Sinica , 4(2):639–650 , 1994 .
[ 19 ] Z . Shen , J . Sun , and Y . Shen . Collective latent Dirichlet allocation . In
ICDM , 2008 .
[ 20 ] N . Srebro and S . Roweis . Time varying topic models using dependent Dirichlet processes . Technical report , CS , Univ . of Toronto , 2005 .
[ 21 ] L . Tang , H . Liu , J . Zhang , and Z . Nazeri . Community evolution in dynamic multi mode networks . KDD , 2008 .
[ 22 ] Y . Teh , M . Jordan , M . Beal , and D . Blei . Hierarchical Dirichlet processes . Journal of the American Statistical Association , 101(476):1566–1581 , 2006 .
[ 23 ] Y . W . Teh . Dirichlet processes . In Encyclopedia of Machine
Learning . Springer , 2010 .
[ 24 ] Y . W . Teh and M . I . Jordan . Hierarchical Bayesian nonparametric models with applications . In N . Hjort , C . Holmes , P . Müller , and S . Walker , editors , To appear in Bayesian Nonparametrics : Principles and Practice . Cambridge University Press , 2009 .
[ 25 ] C . Wang , D . Blei , and D . Heckerman . Continuous time dynamic topic models . In UAI , 2008 .
[ 26 ] C . Wang , B . Thiesson , C . Meek , and D . Blei . Markov topic models .
In AISTATS , 2009 .
[ 27 ] X . Wang and A . McCallum . Topics over time : A non markov continuous time model of topical trends . In KDD , 2006 .
[ 28 ] X . Wang , C . Zhai , X . Hu , and R . Sproat . Mining correlated bursty topic patterns from coordinated text streams . In KDD , 2007 .
[ 29 ] X . Wang , K . Zhang , X . Jin , and D . Shen . Mining common topics from multiple asynchronous text streams . In WSDM , 2009 .
[ 30 ] T . Xu , Z . M . Zhang , P . S . Yu , and B . Long . Dirichlet process based evolutionary clustering . In ICDM , 2008 .
[ 31 ] T . Xu , Z . M . Zhang , P . S . Yu , and B . Long . Evolutionary clustering by hierarchical Dirichlet process with hidden markov state . In ICDM , 2008 .
[ 32 ] J . Zhang , Y . Song , G . Chen , and C . Zhang . On line evolutionary exponential family mixture . In IJCAI , 2009 .
( a ) “ Election ”
( b ) “ Barclays premier league ”
Figure 12 : Comparison of a cluster in different corpora .
7 . CONCLUSIONS
We propose an evolutionary hierarchical Dirichlet process ( EvoHDP ) model to mine cluster evolution from multiple correlated time varying corpora . EvoHDP extends original HDP by incorporating time dependencies into a series of HDPs . A cascaded Gibbs sampling scheme is proposed to infer the model . Our approach can discover cluster emergence , disappearance , and evolution within a corpus and across different corpora . In addition , the cluster numbers for all corpora at all epochs are inferred from data rather than specified .
Experiments on a synthetic data set and a real world financial related web data set validated the effectiveness of our approach . Compared to the original HDP , EvoHDP exhibits better predicting ability and stronger correlations across corpora over time on both data sets . In addition , on the real financial related web data , we observed that the cluster evolution patterns , emergence , disappearance , evolution within a corpus and across corpora , can be effectively discovered by EvoHDP .
8 . ACKNOWLEDGEMENTS
The authors Jianwen Zhang and Changshui Zhang were supported by National Natural Science Foundation of China ( NSFC , Grant No . 60835002 ) . We would like to thank Weihong Qian and Furu Wei for their help on preparing the visualization results . We also thank all the reviewers for the suggestions to improve the paper .
9 . REFERENCES [ 1 ] A . Ahmed and E . Xing . Dynamic non parametric mixture models and the recurrent Chinese restaurant process : with applications to evolutionary clustering . In SDM , 2008 .
[ 2 ] C . Antoniak . Mixtures of Dirichlet processes with applications to
Bayesian nonparametric problems . Annals of Statistics , pages 1152–1174 , 1974 .
[ 3 ] D . Blei and J . Lafferty . Dynamic topic models . In ICML , 2006 .
1088
