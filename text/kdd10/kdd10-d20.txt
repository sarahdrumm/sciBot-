An Integrated Machine Learning Approach to Stroke Prediction
Aditya Khosla
Yu Cao
Cliff Chiung Yu Lin
Dept . of Computer Science
Dept . of Electrical Engineering
Dept . of Electrical Engineering
Stanford University Stanford , CA 94305 aditya86@stanford.edu
Stanford University Stanford , CA 94305 yufcao@stanford.edu
Stanford University Stanford , CA 94305 chiungyu@stanford.edu
Hsu Kuang Chiu
Dept . of Electrical Engineering
Stanford University Stanford , CA 94305 hkchiu@stanford.edu
Junling Hu∗ eBay , Inc
2145 Hamilton Avenue San Jose , CA 95125 juhu@ebay.com
Honglak Lee†
Dept . of Computer Science
Stanford University Stanford , CA 94305 hllee@csstanfordedu
ABSTRACT Stroke is the third leading cause of death and the principal cause of serious long term disability in the United States . Accurate prediction of stroke is highly valuable for early intervention and treatment . In this study , we compare the Cox proportional hazards model with a machine learning approach for stroke prediction on the Cardiovascular Health Study ( CHS ) dataset . Specifically , we consider the common problems of data imputation , feature selection , and prediction in medical datasets . We propose a novel automatic feature selection algorithm that selects robust features based on our proposed heuristic : conservative mean . Combined with Support Vector Machines ( SVMs ) , our proposed feature selection algorithm achieves a greater area under the ROC curve ( AUC ) as compared to the Cox proportional hazards model and L1 regularized Cox model . Furthermore , we present a margin based censored regression algorithm that combines the concept of margin based classifiers with censored regression to achieve a better concordance index than the Cox model . Overall , our approach outperforms the current state of the art in both metrics of AUC and concordance index . In addition , our work has also identified potential risk factors that have not been discovered by traditional approaches . Our method can be applied to clinical prediction of other diseases , where missing data are common and risk factors are not well understood . Categories and Subject Descriptors J.3 [ Computer Application ] : Life and medical sciences ; I26 [ Artificial Intelligence ] : Learning ; I52 [ Pattern recognition ] : Design methodology General Terms Experimentation , Algorithms , Performance ∗This work was done while the author was at Robert Bosch LLC , Research and Technology Center †Corresponding author . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee . KDD’10 , July 25–28 , 2010 , Washington , DC , USA . Copyright 2010 ACM 978 1 4503 0055 1/10/07 $1000
1 .
INTRODUCTION
Stroke is the third leading cause of death and the principal cause of serious long term disability in the United States [ 2 ] . Stroke risk prediction can contribute significantly to its prevention and early treatment . Numerous medical studies and data analyses have been conducted to identify effective predictors of stroke . The Framingham Study [ 6 , 34 ] reported a list of stroke risk factors including age , systolic blood pressure , the use of anti hypertensive therapy , diabetes mellitus , cigarette smoking , prior cardiovascular disease , atrial fibrillation , and left ventricular hypertrophy by electrocardiogram . Furthermore , in the past decade , a number of other studies [ 25 , 23 , 24 , 26 ] have led to the discovery of more risk factors such as creatinine level , time to walk 15 feet , and others .
Most previous prediction models have adopted features ( risk factors ) that are verified by clinical trials or selected manually by medical experts . For example , Lumley et al . [ 24 ] built a 5 year stroke prediction model based on the Cardiovascular Health Study [ 8 ] dataset using a set of 16 manually selected features ( given in [ 25 ] ) from a total of roughly one thousand features . With a large number of features in current medical datasets , it is a cumbersome task to identify and verify each risk factor manually . On the other hand , machine learning algorithms are capable of identifying features highly related to stroke occurrence efficiently from the huge set of features ; therefore , we believe machine learning can be used to : ( i ) improve the prediction accuracy of stroke risk and ( ii ) discover new risk factors .
Lumley et al . ’s [ 24 ] 5 year stroke prediction model adopted the Cox proportional hazards model , one of the most commonly used statistical methods in medical research [ 3 ] . It has been extensively studied [ 1 , 3 ] and applied to the prediction of various diseases including stroke [ 16 , 24 , 21 ] . However , the performance of the original Cox model depends heavily on the quality of the pre selected features . To address this problem , several approaches have been proposed recently [ 9 , 28 ] .
Thus far , there have been very few studies on comparing the Cox regression with machine learning methods in making predictions on censored data . Kattan [ 18 ] compared Cox proportional hazards regression with several machine learning methods ( neural networks and tree based methods ) based on three urological datasets . However , Kattan ’s study focused on datasets with only five features , while machine learning algorithms are expected to effectively handle many more features . In addition , the paper considered only some relatively simple machine learning algorithms and high performance machine learning algorithms such as SVM and logistic regression were not explored .
This paper presents an integrated machine learning approach for stroke risk prediction . We investigated machine learning algorithms to improve the prediction accuracy and conducted extensive comparisons between our results and those with the Cox proportional hazards model . Using the CHS dataset as a benchmark , we first duplicated the results of Lumley et al . [ 24 ] as a baseline . We then compared our machine learning approach with the baseline results and an extended version of the Cox model with feature selection . According to our experiments , our approach consistently outperformed the Cox model .
Our approach considers the problems of data imputation , feature selection , and prediction in medical datasets . We propose a novel automatic feature selection algorithm that selects robust features based on our proposed heuristic : conservative mean . We combine this feature selection algorithm with the popular SVM algorithm .
Furthermore , we present a margin based censored regression algorithm that combines the concept of margin based classifiers with censored regression to achieve a better concordance index than the Cox model . In addition , our work has also identified potential risk factors that have not been discovered by traditional approaches . Last , we note that this method can be applied to clinical prediction of other diseases , where missing data are common and risk factors are not well understood .
In summary , our main contributions are :
1 . An extensive evaluation of the problems of data imputation , feature selection and prediction in medical data , with comparisons against the Cox proportional hazards model .
2 . A novel feature selection algorithm , Conservative Mean feature selection , that outperforms both L1 regularized Cox model and L1 regularized logistic regression on the CHS dataset .
3 . A novel risk prediction algorithm , Margin based Censored Regression , that outperforms the Cox model given the same set of features .
4 . Discovery of new ( previously unknown ) potential risk factors of stroke .
5 . An integrated machine learning approach that significantly outperforms the current state of the art algorithm in stroke prediction .
This paper is organized as follows . Section 2 describes Cox proportional hazards regression and the L1 regularized Cox models that we use as the baselines in this study . Section 3 describes the various machine learning based methods we compare against the Cox models , and Section 4 provides the experimental results of our approach . Finally , Section 5 presents our conclusions .
2 . RELATED WORK
Cox proportional hazards model is widely adopted in clinical studies and used heavily in stroke prediction . We briefly compare the Cox model to some of our other approaches . 2.1 Cox proportional hazards model
The Cox proportional hazards model is given by h(t|x ) = h0(t ) exp(βT x ) ,
( 1 ) where h(t|x ) is the hazard value at time t given the feature vector x ∈ Rd for an individual , h0(t ) is an arbitrary baseline hazard function , β ∈ Rd are the parameters that we are trying to estimate for the model , and d is the number of features for each individual .
This model is known as a semi parametric model because the baseline hazard function is treated non parametrically . Thus , we can see that the parameters have a multiplicative effect on the hazard value which makes it different from the linear regression models [ 20 ] .
The Cox model is part of the Generalized Linear Model ( GLM ) family . Another member of this family is the logistic regression model , where the output takes the following form : h(x ) = ( 1 + exp(−βT x ) )
−1
( 2 )
In this study , we investigated both the Cox model and logistic regression model for stroke prediction . In addition , we broadened our approach to other non regression models such as SVM , taking an agnostic view on what the best model is for stroke prediction . We found that while the Cox model performs reasonably well for stroke prediction , it is inferior to more general machine learning models , such as SVM or margin based censored regression ( proposed in this paper ) . 2.2 Feature selection and L1 regularization
Finding the best estimate for β in equation ( 1 ) and ( 2 ) is typically computationally difficult , particularly given a large number of features . By introducing a complexity based penalty term , we can identify irrelevant features and remove them from our model . The L1 regularized sparse learning problem has the following general form : g(β ) + λ||β||1 ,
( 3 ) where g(· ) is a convex function , β is a vector of length d , and λ > 0 is a regularization parameter . min
β
In this study , we evaluated both L1 regularized Cox model and L1 regularized logistic regression . We found that L1 regularized feature selection gives better performance over the baselines ( ie , selecting features manually ) by reducing the feature set to the most relevant ones .
3 . OUR APPROACH
We present an integrated machine learning approach to stroke prediction . Our approach takes the following steps :
1 . Apply a systematic method for imputing the missing entries in the dataset .
2 . Select the relevant feature subset based on an auto matic procedure .
3 . Apply learning algorithms to evaluate the prediction performance .
3.1 Performance Metrics
For evaluating the performance of our methods , we used the following metrics : area under the ROC curve and concordance index . To define these precisely , we first outline the notation . 311 Notation Consider a dataset {(x(1 ) , y(1 ) , t(1) ) , , ( x(m ) , y(m ) , t(m))} , where x(i ) ∈ Rd is the feature vector1 for individual i ( ie , d is the number of features ) , m is the number of individuals in the dataset , y(i ) is the occurrence of stroke within a predefined time frame ( y = 1 if stroke occurs and 0 otherwise ) , and t(i ) is the time of stroke . If the stroke does not occur within the pre defined time frame for individual i , we set t(i ) = tmax , where tmax is the duration of the time frame ( eg , 5 years ) . Now , we define the set of indexes of all positive and negative examples as Mp = {i|y(i ) = 1} and Mn = {i|y(i ) = 0} respectively . Given a prediction function f : Rd → R , we can compute the prediction estimate for individual i as f ( x(i)).2 312 Area under the ROC curve The area under the ROC curve ( or AUC ) is one of the most important metrics for evaluating the performance of classifiers in the medical diagnosis domain ( where positive samples are usually small in number ) as it considers both sensitivity and specificity , providing a balanced measure for classifier performance . Specifically , the AUC ( associated with the function f ) is defined as follows [ 5 , 14 ] :
AU C =
1
|Mp| · |Mn|
1f ( x(i))>f ( x(j) ) ,
( 4 )
X
X i∈Mp j∈Mn where 1(· ) is an indicator function . The AUC is used to evaluate the performance of the binary stroke classification task . Essentially , this metric gives an estimate of how accurately the model can answer the question , “ is individual A likely to have a stroke within the next 5 years ? ” . 313 Concordance Index We would also like to measure how accurately the predictions reflect relative risk of stroke of two randomly selected individuals . A commonly used metric in survival models for this evaluation is the concordance index [ 15 , 29 ] . The concordance index is a generalization of the concept of AUC designed to handle ( i ) continuous values for prediction and ( ii ) censored data . Similar to the AUC , it takes values from 0.5 ( completely random ) to 1.0 ( perfect prediction ) . The concordance index gives an estimate of how well the output of a prediction model matches the relative time of the event for all pairs of individuals that can be ordered . In essence , it allows us to measure the ability of the model to answer the question , “ is individual A or individual B more likely to have a stroke ? ”
Formally , the concordance index is defined as :
Concordance Index =
1 |ε|
1f ( x(i))>f ( x(j) ) , ( 5 )
X
X i∈Mp t(j)>t(i )
1We extend the feature vector with a constant 1 as the intercept term . 2Throughout this paper , we interpret the binary classification output ˆy ∈ {0 , 1} of the prediction function value f ( x ) as follows : ˆy = 1 ⇐⇒ f ( x ) > 0 . where 1( . ) is the indicator function as before , and |ε| denotes the number of edges in the ordered graph of t.3 We assume that a larger value of f corresponds to a higher risk of stroke . In the following sections , we describe the details of data imputation , feature selection , and prediction models . 3.2 Missing Data Imputation
Clinical data often has significant omissions due to individuals dropping out of the survey , errors in data collection and so on . Missing data often leads to an inaccurate predictive model . Data imputation can be used to remedy missing data . We filled in missing entries using the following methods :
• Column mean : replace each missing value with the mean of the feature ’s observed values
• Column median : replace each missing value with the median of the feature ’s observed values • Imputation through linear regression [ 19 ] • Regularized Expectation Maximization ( EM ) [ 31 ]
As a post processing step to impute discrete valued features , we rounded the imputed values to the nearest discrete value . The imputation algorithms were evaluated using the following metrics :
1 . Imputation accuracy ( adopted from [ 7] ) :
( a ) Root Mean Square Deviation ( RMSD )
( b ) Mean Absolute Deviation ( MAD )
( c ) Bias ( mean of imputed values mean of ground truth data )
2 . Overall stroke prediction performance ( measured by the area under the ROC curve ) .
3.3 Feature Selection
Selecting relevant features [ 13 ] is crucial for building an accurate model of clinical data . For example , the CHS dataset has a large number of attributes , ranging from demographic information and clinical history to biomedical and physical measurements . However , only a small subset of attributes is highly relevant to stroke prediction . The traditional approach to stroke prediction has been to use manually selected features based on risk factors analyzed by medical and clinical studies [ 4 , 24 , 33 , 36 ] . Instead of manually selecting features , we evaluate three machine learning based algorithms for selecting features automatically : forward feature selection , L1 regularized logistic regression , and “ conservative mean ” feature selection .
331 Forward feature selection Forward feature selection [ 12 ] greedily adds one feature at a time . The best subset of features was selected based on cross validation . Note that adding more features does not necessarily improve the test performance since overfitting may occur . 3An ordered graph G(N , E ) of t = {t(1 ) , , t(m)} is defined on a set of m nodes , N = {n1 , , nm} , and a set of edges , E , where ( ni , nj ) ∈ E ⇐⇒ t(i ) < t(j ) .
Table 1 : Notation for a dataset D with d features and m examples , ie , D = {(x(1 ) , y(1) ) , , ( x(m ) , y(m))} ( x(i ) ∈ Rd , y(i ) ∈ {0 , 1},∀i )
Symbol D.x(i ) x(i ) D.y(i ) y(i ) D.xj ( x(1 ) , , x(m ) D.y ( y(1 ) , , y(m ) ) j j
Description i th example in the dataset i th label in the dataset j th features in the dataset set of all labels in the dataset
)
To describe the conservative mean heuristic formally , we first introduce the notation in Table 1 . The key observation here is that when we consider monotonic prediction functions over a single feature , then we only need to compute the AUC over the feature values and the labels ( without considering the prediction functions ) . This is because AUC is invariant under mapping from monotonic functions . For example , this eliminates the need to compute the weight vector or intercept term for a linear SVM when using a single feature as input . The AUC is only affected by the sign of the weight vector which can be easily determined to be the one that ensures the AUC is greater than or equal to 05 Specifically , the following two Lemmas provide the formal basis for the efficient computation .
Lemma 1 . Given any monotonically increasing function f : R → R and a dataset D = {(x(1 ) , y(1) ) , , ( x(m ) , y(m))} , where x(i ) ∈ Rd , y(i ) ∈ {0 , 1},∀i , the AUC for predicted function values of j th feature , f ( D.xj ) , and the labels , D.y , is equal to AU C(Dxj,Dy )
Proof . For notational convenience , we define f ( D.xj ) ( f ( x(1 ) j ) , , f ( x(m ) j
) ) , ie , function values for j th features . Then , the following equalities hold ( for all j ’s ) : AU C(f ( Dxj),Dy ) AU C((f ( x(1 ) j ) , , f ( x(m ) , , x(m ) = AU C((x(1 ) AU C(Dxj,Dy ) )
) ) , ( y(1 ) , , y(m) ) )
) , ( y(1 ) , , y(m) ) )
( 9 )
( 6 )
( 8 )
( 7 ) j j j
The second step holds because a monotonically increasing function does not affect the relative ordering of D.xj , causing the AUC to remain unchanged .
Lemma 2 . Given a hypothesis space H of monotonic ( either strictly increasing or strictly decreasing ) prediction functions f : R → R and a training set , Dtr , and a validation set , Dval , we define f∗ as follows : f
∗ arg max f∈H AU C(f ( Dtrxj),Dtry )
( 10 )
AU C(f
∗
(
Then , the following holds : ( Dvalxj),Dvaly ) AU C(Dvalxj,Dvaly ) AU C(−Dvalxj,Dvaly )
=
( 11 ) if AU C(Dtrxj,Dtry ) ≥ 0.5 otherwise tic could be applied to other feature selection algorithms that use cross validation for selecting features ( eg , forward feature selection ) .
Figure 1 : An illustration of Algorithm 2 for K = 3 . We use the sets T1,T2 and T3 shown in the shaded boxes as validation sets , and the corresponding sets T−1,T−2 and T−3 for training to find cross validation estimates to optimize the value of the threshold , t . CM ( T , K ) refers to the ConservativeM ean(D , K ) function defined in Algorithm 1 .
332 L1 regularized logistic regression L1 regularized logistic regression [ 30 ] is a popular algorithm for feature selection . L1 regularization has the beneficial effect of regularizing model coefficients ( as in L2 regularization ) , but yields sparse models that are more easily interpretable [ 27 , 32 , 35 ] . This model has a regularization parameter that controls the “ sparseness ” of the weights . Consequently , the features with nonzero weights are selected for prediction .
333 Conservative mean feature selection Here we present a novel and efficient feature selection algorithm , Conservative Mean ( CM ) feature selection . Consider a setting where positive examples are small in number and non homogeneous . Then , the prediction performance may vary significantly depending on how the training and testing examples are sampled . We want to select features that are relevant , yet robust to variations due to sampling over a small number of non homogeneous examples . In order to incorporate the above intuition , we introduce the heuristic of conservative mean ( µ − σ ) , where µ and σ refer to the mean and standard deviation of the AUC of a feature4 respectively . The setting is similar to K fold crossvalidation , but we also want to consider the variance across different folds along with the average of the prediction performance . In addition , we want to evaluate the performance of each feature individually . Therefore , subtracting the standard deviation from the mean provides a more ‘conservative’ estimate of the performance of each feature as compared to using the mean alone , which is the typical approach.5
4µ ( or σ ) refers to mean ( or standard deviation ) of the AUC of a given feature over K folds of the dataset . See Algorithm 1 for precise definition . 5The same heuristic can be used to optimize any other metric such as classification accuracy . Furthermore , this heuris
TT 2T 3CM ( T 1 , K )CM ( T 2 , K )CM ( T 3 , K )v 2 ℝdv2v1v3I := fj j vj ¸ tgT 1T 2T 2T 2T 1T 1T 1T 3T 3T 3T 3T 2T 1 Algorithm 1 Computing the conservative mean vector function ConservativeMean(D , K ) : Input : D : dataset with d features K : number of folds Output : v : Conservative mean vector ( of length d ) begin Divide D evenly into K disjoint sets D1 , ,DK such that D = D1 ∪ ∪ DK and Dk ∩ Dl = ∅,∀k , l . Set D−k D − Dk,∀k . for j := 1 to d do s := 0 ∈ RK for k := 1 to K do if AUC(D−kxj,D−ky ) ≥ 0.5 then sk :=AUC(Dkxj,Dky ) sk :=AUC(−Dkxj,Dky ) else end if end for vj := µ(s ) − σ(s ) end for where AUC(predictions , labels ) returns the area under the ROC curve given the predictions and labels . µ(s ) 1
PK σ(s ) q 1 PK k=1 sk k=1(sk − µ(s))2
K
K end
Proof . By definition of f∗ , AU C(f∗(Dtrxj),Dtry ) ≥ 05 Since we consider only monotonic functions , f∗ is either monotonically increasing or monotonically decreasing . If f∗ is monotonically increasing , we have ( from the Lemma 1 ) :
∗
AU C(f ⇒ AU C(f
( Dxj),Dy ) = AU C(Dxj,Dy)),∀D
( 12 ) ( Dvalxj),Dvaly ) = AU C(Dvalxj,Dvaly ) )
∗
Similarly , if f∗ is monotonically decreasing , we have
∗
( Dxj),Dy ) = AU C(−Dxj,Dy)),∀D
( 13 ) ( Dtrxj),Dtry ) = AU C(−Dtrxj,Dtry ) )
∗
AU C(f ⇒ 0.5 ≤ AU C(f ⇒ AU C(Dtrxj,Dtry ) ) ≤ 0.5 , and AU C(f
∗
( Dvalxj),Dvaly ) = AU C(−Dvalxj,Dvaly ) )
Therefore , we can efficiently compute a robust estimate of prediction performance for each feature ( summarized as conservative mean vector v ) as described in Algorithm 1 .
Based on the conservative mean heuristic for ranking the features , we can now describe an algorithm to select the appropriate number of features . The overall procedure is described in Algorithm 2 ( see Figure 1 for illustration ) . The training data is initially split into K folds , and we compute the conservative mean vector by holding out the k th fold each time , resulting in a total of K vectors . We then compute the vector ¯v by taking the average of the conservative mean vectors to select a robust set of features that generalize well across all folds . Finally , given a threshold value t ∈ [ 0 , 1 ] , we select all features I {j|¯vj ≥ t}.6 This 6Note that selecting a threshold is equivalent to selecting a number of features ranked according to their value of the ¯v vector .
Algorithm 2 Conservative mean feature selection
Input : T : dataset with d features K : number of folds t : threshold ∈ [ 0 , 1 ] Output : I : Set of selected feature indexes ⊂ {1 , , d} . begin Divide T evenly into K disjoint sets T1 , ,TK such that T = T1 ∪ ∪ TK and Tk ∩ Tl = ∅,∀k , l . Set T−k T − Tk,∀k . ¯v := 0 ∈ Rd for k := 1 to K do D := T−k ¯v := ¯v + 1 end for I := {j|¯vj ≥ t} end
K ConservativeMean(D , K ) threshold value can be determined using cross validation , as described in the caption of Figure 1 . 3.4 Learning Algorithms for Prediction
Support Vector Machines
In this section , we describe the learning algorithms that we used for stroke prediction : Support Vector Machines and Margin based Censored Regression ( MCR ) . 341 SVM is a popular machine learning algorithm that is widely used for classification . Conceptually , SVM optimizes the “ margin ” between positive and negative examples . We can formulate the stroke prediction problem as predicting the occurrence of stroke over a pre defined time frame , which makes it a binary classification problem that fits into the framework of SVM . Furthermore , SVM solvers can be used to optimize the area under the ROC curve directly , so they are well suited for the task of stroke prediction . We used linear SVMs implemented using SVM perf [ 17 ] in this study . 342 Margin based Censored Regression Since the SVM is in principle developed for classification , we use it to predict whether or not a stroke would occur within a given time frame while ignoring the information about when the stroke occurred . However , the time of stroke is indicative of the relative risk level of an individual . Incorporating this information would enable us to answer questions such as “ is individual A or individual B more likely to have a stroke ? ” and “ when is a stroke likely to occur ? , ” whereas SVM predictions are generally unable to address these concerns .
To address these concerns , we propose the Margin based Censored Regression algorithm that unifies linear regression with an SVM like classifier on censored data . More specifically , we propose a convex optimization problem as described below . Consider a dataset {(x(1 ) , y(1 ) , t(1) ) , , ( x(m ) , y(m ) , t(m))} , where x(i ) ∈ Rd , y(i ) ∈ {0 , 1} and t(i ) ∈ R , as described in Section 31 The time of stroke is then normalized as ˜t(i ) = t(i)/tmax . We then use a monotonically decreasing function to transform the normalized time of stroke to a “ hazard value , ” as in the Cox model . We use the function z(˜t(i ) ) = − log(˜t(i ) ) in our experiments . With this trans formation , we have z(˜t(i ) ) = 0 for i ∈ {i|y(i ) = 0} and z(˜t(i ) ) > 0 for i ∈ {i|y(i ) = 1}.7
Given the above transformation , our goal is to find a weight vector w such that wT x(i ) is “ close to ” z(˜t(i) ) . In addition , we want to be able to distinguish between positive and negative examples ; in other words , we want to find w such that the individuals who experienced a stroke are well separated from the individuals who did not . This is achieved by imposing wT x(i ) ≤ − for the individuals who did not have a stroke , where is the desired margin between positive and negative examples ( set to 1 in our experiments ) . Finally , i ξ(i ) to allow for non separable datasets , and to reduce the sensitivity to outliers . similar to SVM , we introduce a penalty termP X
X
To sum up , we formulate the problem as φ(wT x(i ) − z(˜t(i) ) ) + C minimize
ξ(i ) + γ||w||2
2 subj . to wT x(i ) ≤ − + ξ(i ) , ∀i ∈ {i|y(i ) = 0} ,
( 14 ) w,ξ i:y(i)=1 i:y(i)=0
ξ(i ) ≥ 0 , ∀i , where C and γ are the hyperparameters for the misclassification loss penalty and for regularization respectively , and φ : R → R is the regression loss penalty , which we fixed as the Huber function [ 11]8 in our study . To solve problem ( 14 ) , we used CVX , a package for specifying and solving convex programs [ 11 , 10 ] .
Note that we can easily apply the kernel trick to this model , as in SVM . Furthermore , the objective function can be modified to optimize the AUC directly in the same way as SVM perf [ 17 ] .
4 . EXPERIMENTAL RESULTS
4.1 Dataset and Preprocessing
The Cardiovascular Heart Study [ 8 ] is a study of risk factors for cardiovascular diseases in people over the age of 65 . According to the Centers for Disease Control and Prevention , nearly three quarters of all strokes occur in people over the age of 65.9 This makes CHS an invaluable resource for the investigation of risk factors and the prediction of stroke . In the original cohort recruited in the first phase of the CHS , 5,201 individuals were examined yearly from 1989 to 1999 , with a total of about one thousand attributes collected annually through medical examinations , questionnaires , and phone contacts . Events such as stroke and hospitalization were verified by specialists and recorded for each individual . The comprehensiveness of the CHS dataset makes it one of the most widely used benchmarks for studying risk factors for cardiovascular diseases , including stroke . However , it is also very challenging to use the CHS dataset effectively due to a significant fraction of missing values and a large number of features in the dataset . For example , about 25 % of the baseline measurements in the CHS dataset are missing , and some entries are recorded as “ unknown ” or “ refuse to answer . ”
7For the CHS dataset , z(˜t(i ) ) roughly ranges from 0 to 5 . 8The Huber function φ(x ) is defined as 2|x| − 1 for |x| ≥ 1 , and |x|2 for |x| ≤ 1 . 9http://wwwcdcgov/Stroke/factshtm
In our experiments , we considered the 5 year stroke prediction10 problem on the original cohort . To begin with , we removed the individuals with pre baseline stroke ( as done in [ 24 ] ) and the features with more than 60 % missing entries.11 This criterion was chosen because features with too many missing entries often turn out to be irrelevant . After preprocessing , the final dataset consisted of 796 features and 4 , 988 examples with 299 occurrences of stroke . Then the data was divided randomly with a ratio of 9 : 1 for training and testing respectively , while keeping the ratio of the positive and negative examples constant . This process was repeated to obtain a fixed set of 5 randomly sampled train and test sets.12 In the remaining sections , “ average test AUC ” refers to the average of AUC obtained by evaluating the prediction algorithm on the test set over these 5 random trials . We also define “ average test concordance index ” in a similar way . 4.2 Data Imputation
The data imputation quality was evaluated using 10 fold cross validation . For each feature j , we first removed all the examples that contained missing values for the particular feature . Then , we divided the examples in the resulting data into training and validation sets with the ratio of 9:1 respectively . Treating feature j of the validation set as being unobserved , we used the training data to impute the values of the particular feature of the validation data.13 The imputed values were then compared with the actual values in the validation dataset to obtain the performance metrics described in Section 32 This process was repeated for every feature and the results were averaged . The summary results are shown in Table 2.14 For the computation of the area under the ROC curve , we used conservative mean feature selection and SVM for stroke prediction.15
Among the imputation methods , linear regression gave the smallest RMSD and MAD values , which suggested that it achieved the highest imputation accuracy . However , the overall stroke prediction quality with column median was the best with an area of 0.774 under the ROC curve . In the following sections , we report the results using column median as the default imputation method . 4.3 Feature Selection
431 Forward feature selection As this method is computationally very expensive , the number of features was initially reduced from 796 to about 200 using L1 regularized logistic regression . Then we ran for
10Only the cases of stroke that occurred within 5 years after the baseline measurements were considered positive examples . 11Some features represented “ refuse to answer ” values as 9 or 99 . We replaced these entries with “ missing ” before data imputation . 12These random trials were used for all the remaining experiments to ensure all the results are directly comparable . 13For the linear regression imputation , any missing values in the other features were filled in using the column mean . 14Imputation methods without rounding have been left out as the results were very similar . 15Imputation with regularized EM was computationally expensive , so we used L1 logistic regression on data imputed with column median to reduce the number of features to about 200 before applying EM .
Imputation Method Column Median Linear Regression ( with rounding ) Regularized EM Column Mean ( with rounding )
Table 2 : Data Imputation Results RMSD MAD Bias 0.0125 0.0755 0.0039 0.0114 < 10−4 0.0526 0.0002 0.5537 0.9563 0.0747 0.0129 0.0032
Avg . Test AUC
0.774 0.768 0.765 0.765 ward feature selection on this reduced set to obtain the final set of features . Using SVM for prediction , we selected the optimal number of features through 10 fold cross validation . The final prediction performance was an average test AUC of 0.751 , which is slightly worse than that of using SVM and the 16 features used in [ 24 ] . In our experiments , this method selected a much larger number of features than other feature selection algorithms , which indicates that it may be susceptible to overfitting .
432 L1 regularized logistic regression The L1 regularized logistic regression ( L1LR ) was used for feature selection , followed by SVM for prediction . The implementation of L1LR was done using the SLEP package [ 22 ] . The optimal regularization parameter λ∗ was assigned to be the value that maximized the area under the ROC curve for 10 fold cross validation . The value of λ∗ was then used to run L1LR on the entire training set to select the final set of features for testing . The average test AUC was 0.764 , which is better than that of the L1 regularized Cox feature selection algorithm .
433 Conservative mean selection Conservative mean selection was run using 10 fold crossvalidation for both the generation of the conservative mean vectors as described in Algorithm 1 , and to obtain the subset of optimal features as described in Algorithm 2 . As observed for the forward search feature selection , using the maximum cross validation AUC may result in overfitting , and thus we propose a simple method to reduce this effect .
Throughout our experiments , we observed that overfitting may occur when the performance on the training set increases with increasing number of features while the crossvalidation performance decreases or remains fairly constant . An example of this can be seen in Figure 2 . Hence , we can say that the ‘extent of overfitting’ increases as the gap between the cross validation AUC ( CV AUC ) and train AUC increases . Therefore , we estimated this extent of overfitting by subtracting the cross validation AUC from the training AUC . Then , we computed a more conservative estimate of the CV AUC as ‘CV AUC’ − ( ‘train AUC’ − ‘CV AUC’).16 In Figure 2 , we can see that the CV AUC remains fairly constant after about 30 features . If the CV AUC was maximized without accounting for the overfitting , we would select 120 features instead of 30 . Furthermore , we compare the effect of using conservative mean ( “ vj = µ(s ) − σ(s ) ” ) against using mean ( “ vj = µ(s ) ” ) in Algorithm 1 while keeping the remaining algorithms unchanged . The average test AUC decreases from 0.774 ( conservative mean ) to 0.759 ( mean ) when using SVM for prediction . The difference in performance clearly shows that conservative mean selects more robust features as compared
Figure 2 : Plot showing the cross validation AUC and train AUC as features are added . We used CM feature selection with SVM for prediction on a random trial .
Table 3 : Average test AUC combining various feature selection algorithms with our prediction algorithms
Feature selection algorithm SVM 0.774 Conservative Mean 0.764 L1 logistic regression 16 features ( used in [ 24 ] ) 0.753
MCR 0.777 0.771 0.765 to mean alone . Furthermore , we note that the best performance for both SVM and MCR are obtained using the conservative mean feature selection algorithm .
Also note that the conservative mean selection algorithm is significantly more computationally efficient than the forward feature selection algorithm . On the same machine , forward feature selection took about 60 hours to select features from a set of about 200 features , while conservative mean took less than 10 minutes to select from a set of 796 features . 4.4 Stroke Prediction
First , we evaluated the performance of our prediction algorithms based on the area under the ROC curve . The prediction performance when using the feature selection algorithms ( described in the previous sections ) is compared against the set of 16 manually selected features used by Lumley et al . , as shown in Table 3 . When using SVM or MCR17 for prediction , we found that all the feature selection algorithms except forward feature selection performed better than using manually selected features . Overall , CM feature selection performed the best for both prediction methods . We also found that MCR performed better than SVM for all the feature selection methods .
In Table 4 , we compare the performance of our algorithms against the current state of the art Cox proportional haz
16This method improved average test AUC by about 0.5 % for all the algorithms .
17In our experiments , we added a small L1 regularization penalty to the MCR objective function .
20406080100120060650707508Number of features selectedArea under the ROC curve CV AUCTrain AUC2*CV AUC − Train AUC Table 4 : Average test AUC using different algorithms with comparison to the Cox models
Algorithm MCR + CM feature selection SVM + CM feature selection SVM + 16 features ( used in [ 24 ] ) SVM + forward selection Cox + L1 feature selection Cox + 16 features ( used in [ 24 ] )
Avg . Test AUC
0.777 0.774 0.753 0.751 0.747 0.734
Table 5 : Average test concordance index of Cox model and MCR using different sets of features
Method MCR + CM feature selection MCR + 16 features ( used in [ 24 ] ) SVM + CM feature selection SVM + 16 features ( used in [ 24 ] ) Cox + CM feature selection ( for MCR ) Cox + 16 features ( used in [ 24 ] )
Concordance Index
0.770 0.757 0.760 0.747 0.737 0.730 ards model and L1 regularized Cox model . All our methods outperformed these baseline models . The best method was combining CM feature selection with MCR for prediction , which achieved a 16 % error reduction in the average test AUC as compared to the Cox model ( as used in [ 24] ) .
Second , we evaluated the concordance index to compare the ability of MCR , SVM and Cox model to predict the relative risk of stroke . From Table 5 , we observe that the MCR algorithm outperforms the other models when using the same set of features . Also , the features selected using CM significantly increased the concordance index for all the models . It is interesting to note that the SVM performs better than the Cox model even though it does not use information about the relative risk of stroke . The combination of CM feature selection and MCR for prediction gave the best performance with a concordance index of 0770 4.5
Identifying risk factors
In addition to achieving better results , our method can automatically identify potential risk factors without carrying out extensive medical studies to understand each one in detail . This would allow for a quick method of characterizing a new disease and identifying its predictors before other studies confirm them . Furthermore , this procedure could also be used to suggest risk factors that might have been previously unexplored .
In our experiments , we found the top features by ranking the average of the conservative mean vectors over multiple random trials in descending order . Table 6 shows a representative set of features found among the top 40 features . Note that there is a large overlap between the top features selected by our feature selection algorithm and those identified by medical studies . This verifies that our algorithm is reliable in identifying risk factors . Thus , the features that are highly ranked but have not been clinically tested might be probable risk factors .
For example , “ any ECG abnormality ” is a highly ranked factor , whereas “ atrial fibrilliation by ECG ” is a commonly accepted risk factor of stroke . It may be possible that all ECG abnormalities are more indicative of stroke than just atrial fibrilliation . Also , “ minimental score ” could be an important risk factor because it gives an indication of the cere
Table 6 : A representative set of features obtained from the top 40 features selected by CM feature selection
Average ( µ − σ )
Feature description Age† Number of symbols correctly coded* Maximal inflation level* Systolic blood pressure† Calculated 100 point score* Total medications* Isolated systolic hypertension† General health* Calculated hypertension status† Time ( in sec ) to walk 15 feet† Any ECG abnormality* Right/left % Stenosis† Cardiac Injury Score† Min . ankle arm ratio* Diabetic status defined by ADA† Minimental score 35 point* Left ventricular mass† Creatinine† FVC percent predicted* * Potential risk factors that are not found in previous work ( to the best of our knowledge ) † Clinically established risk factors of stroke
0.6064 0.5828 0.5820 0.5738 0.5681 0.5634 0.5588 0.5519 0.5500 0.5485 0.5461 0.5444 0.5426 0.5374 0.5342 0.5337 0.5337 0.5273 0.5239 brovascular activity of an individual , which could be correlated to stroke risk . In addition , our results suggest a few other potential risk factors for stroke , such as total number of medications and FVC percent predicted . Further investigation of these features could lead to improved stroke prediction . 5 . DISCUSSIONS AND CONCLUSION
As we have shown in this study , the conservative mean feature selection performs very well for the CHS dataset . However , we realize that this feature selection algorithm may not work well in other datasets with highly correlated features as it evaluates the performance of each feature individually . To address this problem , we could use an L1 regularized feature selection algorithm ( eg , L1 regularized logistic regression ) to prune the features before applying conservative mean feature selection for fine tuning .
In this paper , we have presented an integrated machine learning approach combining the elements of data imputation , feature selection and prediction . We provide an extensive comparison of machine learning methods with the Cox proportional hazards model and show that the machine learning methods significantly outperform the Cox model in terms of both binary stroke prediction and stroke risk estimation . Specifically , we propose the conservative mean heuristic for feature selection , which gives us the best performance as compared to other methods . In addition , we present a novel prediction algorithm , Margin based Censored Regression , that achieves a better concordance index than the Cox model . Further , our method can be used for identifying potential risk factors for diseases without performing clinical trials . We hope that this paper will motivate the application of machine learning methods in healthcare data analysis .
Acknowledgments We thank Andrew Ng and Chaitanya Rastogi for helpful advice and the National Heart , Lung , and Blood Institute ( NHLBI ) for providing the CHS dataset . Support for this work from the Research and Technology Center of Robert Bosch LLC is gratefully acknowledged . 6 . REFERENCES [ 1 ] K . Akazawa , T . Nakamura , S . Moriguchi , M . Shimada , and
Y . Nose . Simulation program for estimating statistical power of Cox ’s proportional hazards model assuming no specific distribution for the survival time . Computer Methods and Programs in Biomedicine , 35(3):203–12 , 1991 .
[ 2 ] American Heart Association . Heart Disease and Stroke
Statistics 2009 Update . American Heart Association , Dallas , Texas , 2009 .
[ 3 ] R . Bender , T . Augustin , and M . Blettner . Generating survival times to simulate Cox proportional hazards models . Statistics in Medicine , 24(11):1713–1723 , 2005 . [ 4 ] L . E . Chambless , G . Heiss , E . Shahar , M . J . Earp , and
J . Toole . Prediction of ischemic stroke risk in the atherosclerosis risk in communities study . American Journal of Epidemiology , 160(3):259–269 , 2004 .
[ 5 ] C . Cortes and M . Mohri . AUC optimization vs . error rate minimization . In Advances in Neural Information Processing Systems 16 . MIT Press , 2004 .
[ 6 ] T . R . Dawber , G . F . Meadors , and F . E . Moore .
Epidemiological approaches to heart disease : The Framingham study . American Journal of Public Health and the Nation ’s Health , 41:279–286 , March 1951 .
[ 7 ] J . M . Engels and P . Diehr . Imputation of missing longitudinal data : a comparison of methods . Journal of Clinical Epidemiology , 56(10):968–976 , 2003 .
[ 8 ] L . P . Fried , N . O . Borhani , P . Enright , C . D . Furberg , J . M . Gardin , R . A . Kronmal , L . H . Kuller , T . A . Manolio , M . B . Mittelmark , A . Newman , D . H . O’Leary , B . Psaty , P . Rautaharju , R . P . Tracy , and P . G . Weiler . The Cardiovascular Health Study : design and rationale . Annals of Epidemiology , 1(3):263–276 , February 1991 .
[ 9 ] J . Goeman . l1 penalized estimation in the Cox proportional hazards model . Biometrical Journal , 52(1):70–84 , 2009 .
[ 10 ] M . Grant and S . Boyd . Graph implementations for nonsmooth convex programs . In V . Blondel , S . Boyd , and H . Kimura , editors , Recent Advances in Learning and Control , Lecture Notes in Control and Information Sciences , pages 95–110 . Springer Verlag Limited , 2008 .
[ 11 ] M . Grant and S . Boyd . CVX : Matlab software for disciplined convex programming , version 121 http://cvxr.com/cvx , May 2010 .
[ 12 ] E . I . Guyon , S . Gunn , M . Nikravesh , and L . A . Zadeh .
Feature Extraction : Foundations and Applications . Springer , 2006 .
[ 13 ] I . Guyon and A . Elisseeff . An introduction to variable and feature selection . Journal of Machine Learning Research , 3:1157–1182 , 2003 .
[ 14 ] J . A . Hanley and B . J . McNeil . The meaning and use of the area under a receiver operating characteristic ( ROC ) curve . Radiology , 143(1):29–36 , 1982 .
[ 15 ] F . E . Harrell . Regression Modeling Strategies , With
Applications to Linear Models , Logistic Regression , and Survival Analysis . Springer , 2001 .
[ 16 ] K . Ikeda , H . Kumada , S . Saitoh , Y . Arase , and
K . Chayama . Effect of repeated transcatheter arterial embolization on the survival time in patients with hepatocellular carcinoma . Cancer , 68(10):2150–4 , 2001 . [ 17 ] T . Joachims . A support vector method for multivariate performance measures . In Proceedings of the International Conference on Machine Learning , pages 377–384 , 2005 .
[ 18 ] M . W . Kattan . Comparison of Cox regression with other methods for determining prediction models and nomograms . The Journal of Urology , 170:S6–S10 , December 2003 .
[ 19 ] H . Kim , G . H . Golub , and H . Park . Imputation of missing values in DNA microarray gene expression data . In Proceedings of the IEEE Computational Systems Bioinformatics Conference , pages 572–573 , 2004 . [ 20 ] J . Klein and M . Moeschberger . Survival Analysis :
Techniques for Censored and Truncated Data . Springer , 2003 .
[ 21 ] K Y Liang , S . G . Self , and X . Liu . The Cox proportional hazards model with change point : An epidemiologic application . Biometrics , 46:783–793 , 1990 .
[ 22 ] J . Liu , S . Ji , and J . Ye . SLEP : Sparse Learning with Efficient Projections . Arizona State University , 2009 .
[ 23 ] W . T . Longstreth , Jr . , C . Bernick , A . Fitzpatrick ,
M . Cushman , L . Knepper , J . Lima , and C . Furberg . Frequency and predictors of stroke death in 5,888 participants in the Cardiovascular Health Study . Neurology , 56:368–375 , February 2001 .
[ 24 ] T . Lumley , R . A . Kronmal , M . Cushman , T . A . Manolio , and S . Goldstein . A stroke prediction score in the elderly : Validation and web based application . Journal of Clinical Epidemiology , 55(2):129–136 , February 2002 .
[ 25 ] T . A . Manolio , R . A . Kronmal , G . L . Burke , D . H . O’Leary , and T . R . Price . Short term predictors of incident stroke in older adults : The Cardiovascular Health Study . Stroke , 27:1479–1486 , September 1996 .
[ 26 ] A . P . McGinn , R . C . Kaplan , J . Verghese , D . M .
Rosenbaum , B . M . Psaty , A . E . Baird , J . K . Lynch , P . A . Wolf , C . Kooperberg , J . C . Larson , and S . Wassertheil Smoller . Walking speed and risk of incident ischemic stroke among postmenopausal women . Stroke , 39:1233–1239 , April 2008 .
[ 27 ] A . Y . Ng . Feature selection , l1 vs . l2 regularization , and rotational invariance . In Proceedings of the International Conference on Machine Learning , 2004 .
[ 28 ] M Y Park and T . Hastie . An l1 regularization path algorithm for generalized linear models . Journal of the Royal Statistical Society : Series B , 69(4):659–677 , 2007 .
[ 29 ] V . Raykar , H . Steck , B . Krishnapuram , C . Dehing Oberije , and P . Lambin . On ranking in survival analysis : Bounds on the concordance index . In Advances in Neural Information Processing Systems 20 . MIT Press , 2008 .
[ 30 ] M . Schmidt , G . Fung , and R . Rosales . Fast optimization methods for l1 regularization : A comparative study and two new approaches . In Proceedings of the European Conference on Machine Learning , 2007 .
[ 31 ] T . Schneider . Analysis of incomplete climate data :
Estimation of mean values and covariance matrices and imputation of missing values . Journal of Climate , 14:853–871 , 2001 .
[ 32 ] R . Tibshirani . Regression shrinkage and selection via the lasso . Journal of the Royal Statistical Society , Series B , 58(1):267–288 , 1996 .
[ 33 ] Z . Vok´o , M . Hollander , P . J . Koudstaal , A . Hofman , and
M . M . Breteler . How do American stroke risk functions perform in a western European population ? Neuroepidemiology , 23(5):247–253 , September October 2004 .
[ 34 ] P . A . Wolf , R . B . D’Agostino , A . J . Belanger , and W . B .
Kannel . Probability of stroke : a risk profile from the Framingham study . Stroke , 22:312–318 , March 1991 .
[ 35 ] E . P . Xing , M . I . Jordan , and R . M . Karp . Feature selection for high dimensional genomic microarray data . In Proceedings of the International Conference on Machine Learning , pages 601–608 , 2001 .
[ 36 ] X F Zhang , J . Attia , C . D’este , X H Yu , and X G Wu . A risk score predicted coronary heart disease and stroke in a Chinese cohort . Journal of Clinical Epidemiology , 58(9):951–958 , 2005 .
