Direct Mining of Discriminative Patterns for Classifying
Uncertain Data
Chuancong Gao , Jianyong Wang
Department of Computer Science and Technology
Tsinghua National Laboratory for Information Science and Technology
Tsinghua University , Beijing 100084 , China gaocc07@mailstsinghuaeducn , jianyong@tsinghuaeducn
ABSTRACT Classification is one of the most essential tasks in data mining . Unlike other methods , associative classification tries to find all the frequent patterns existing in the input categorical data satisfying a user specified minimum support and/or other discrimination measures like minimum confidence or information gain . Those patterns are used later either as rules for rule based classifier or training features for support vector machine ( SVM ) classifier , after a feature selection procedure which usually tries to cover as many as the input instances with the most discriminative patterns in various manners . Several algorithms have also been proposed to mine the most discriminative patterns directly without costly feature selection . Previous empirical results show that associative classification could provide better classification accuracy over many datasets .
Recently , many studies have been conducted on uncertain data , where fields of uncertain attributes no longer have certain values . Instead probability distribution functions are adopted to represent the possible values and their corresponding probabilities . The uncertainty is usually caused by noise , measurement limits , or other possible factors . Several algorithms have been proposed to solve the classification problem on uncertain data recently , for example by extending traditional rule based classifier and decision tree to work on uncertain data . In this paper , we propose a novel algorithm uHARMONY which mines discriminative patterns directly and effectively from uncertain data as classification features/rules , to help train either SVM or rule based classifier . Since patterns are discovered directly from the input database , feature selection usually taking a great amount of time could be avoided completely . Effective method for computation of expected confidence of the mined patterns used as the measurement of discrimination is also proposed . Empirical results show that using SVM classifier our algorithm uHARMONY outperforms the state of the art uncertain data classification algorithms significantly with 4 % to 10 % improvements on average in accuracy on 30 categorical datasets under varying uncertain degree and uncertain attribute number .
Categories and Subject Descriptors H28 [ Database Management ] : Database applications—Data Mining
General Terms Algorithm , Experimentation
Keywords Associative Classification , Uncertain Data , Frequent Pattern Mining , Expected Confidence
Code and Datasets All the code and datasets are available at http://dbgroup . cstsinghuaeducn/chuancong/uharmony/
1 .
INTRODUCTION
As one of the most essential tasks in data mining and machine learning area , classification has been studied for many years . Many effective models and algorithms have been proposed to solve the problem in different aspects , including decision tree , rule based classifier , support vector machine , etc .
Unlike some traditional rule based algorithms like Ripper [ 7 ] or FOIL [ 19 ] , associative classification tries to mine the complete set of frequent patterns from the input dataset , given the user specified minimum support threshold and/or discriminative measurements like minimum confidence threshold . Sequential covering technology is further employed to select the most discriminative patterns while covering most input training instances . A test instance is classified later using classifier trained based on the mined patterns . CBA [ 15 ] is one of the most classical associative classification algorithms . Empirical results show that associative classification algorithm could provide better classification accuracy than other algorithms on categorical datasets . However , this approach takes a great amount of running time in both pattern mining and feature selection , since most of the mined frequent patterns are not the most discriminative ones and will be dropped later .
To improve the efficiency of associative classification , several algorithms have been proposed in recent years to try to mine the most discriminative patterns directly during the pattern mining step . Different discriminative measures and different instance covering technologies have also been devised . One of the most typical algorithms is HARMONY [ 21 ] which uses confidence to evaluate the discrimination of patterns . It employs a so called instance centric framework to find one most discriminative pattern for each instance . Effective pruning methods have also been proposed to enhance the algorithm efficiency . To classify a test instance , a rule based classifier is built based on the mined patterns . From then on , several other algorithms [ 4 , 9 ] mining pattern directly have also been proposed . The main differences of these algorithms in comparison with HARMONY include the replacement of confidence with information gain or information gain ratio as the pattern quality measure , the adoption of SVM model , and more advanced covering technologies such as the one using decision tree to partition data . Among these changes , the adoption of SVM model contributes the most in improving the classification accuracy .
Recently , more and more research has been conducted on uncertain data mining to solve the uncertainty usually caused by noise , measurement precisions , etc . Several algorithms have already been proposed to solve the frequent itemset mining problem where each item has a probability to appear , using either expected support [ 6 ] or frequentness probability [ 2 ] to measure the pattern frequentness . Classification for uncertain data has also been studied recently . For uncertain data classification , the values of the uncertain attributes are now represented using a probabilistic distribution function , for uncertain numeric attribute or uncertain categorical attribute . Several classical algorithms like C4.5 and Ripper have been extended to process uncertain attributes [ 17 , 20 , 16 ] . Some of them try to convert the uncertain attributes into certain ones by discretization using sample points , while others adopt probabilistic cardinalities of entropy , information gain and information gain ratio .
In this paper , we propose a new algorithm called uHARMONY to solve the problem of classifying uncertain categorical data . The algorithm adopts the same framework as algorithm HARMONY . Expected support is adopted to represent pattern frequentness , while expected confidence is employed to represent the discrimination of the mined patterns . We also devise a novel method to calculate the expected confidence efficiently . A new instance covering strategy has been devised to try to ensure that the instances are covered with a probability higher than a user specified cover threshold . Evaluation on 30 public datasets with different number of uncertain attributes and different uncertain degrees ( which is defined as the probability the attribute takes values other than the original single value ) shows that our algorithm outperforms two state of the art algorithms significantly with 4 % to 10 % improvements in accuracy on average while using SVM as the classification model .
Our contributions could be summarized as follows . • We devise a new associative classification algorithm mining the most discriminative patterns directly on uncertain data . To our best knowledge , this is the first associative classification algorithm for classifying uncertain data . • We adopt the expected confidence as the measurement of discriminative degree , instead of other probabilistic cardinalities without reasonable theoretical explanations . A novel upper bound based approach is also proposed to speedup the calculation of expected confidence . • Unlike covering instance with only one pattern having the corresponding maximum confidence in HARMONY , we devise a novel instance covering strategy to assure the probability of each training instance covered by at least one pattern is higher than a user specified threshold . Evaluation shows that this technology could improve the accuracy significantly . • We conducted a comprehensive experiment using 30 public datasets under varying uncertain parameters . The empirical results validate that our algorithm outperforms two state ofthe art algorithms significantly with 4 % to 10 % improvements in accuracy on average .
For the rest of this paper , we first introduce the related work in Section 2 . Preliminaries are described in Section 3 . The compu tation of expected confidence is provided in Section 4 . Algorithm details are discussed in Section 5 . The evaluation part is presented in Section 6 . Our paper concludes in Section 7 .
2 . RELATED WORK
Various algorithms have been proposed for categorical data classification . Most of them could be classified into two types – the traditional rule induction ( or decision tree ) based methods and the association based methods . The rule induction based classifiers such as Ripper [ 7 ] , C4.5 [ 18 ] , FOIL [ 19 ] , and CPAR [ 22 ] use heuristics like information gain or gini index to grow the current rule . Sequential covering paradigm may also be adopted for speedup . While for association based classifiers , efficient associative rule mining algorithms are first applied to find the complete set of candidate rules . A set of rules are selected later based on several covering paradigms and discrimination heuristics . Some typical examples include CBA [ 15 ] and CMAR [ 14 ] . [ 3 ] proposes a method recently using the discovered rules as SVM training features and achieves higher accuracy . [ 11 ] includes an application on associative classification using frequent itemset generators mined on stream data .
In recent years , several studies have been conducted on how to mine associative rules directly and effectively from the input database without costly feature selection step . HARMONY [ 21 ] is an instance centric algorithm which mines directly for each instance a covering rule with the highest confidence . [ 8 ] proposes a method to discover top k covering rules for the input gene expression data . Extended from their previous study in [ 3 ] , the authors of [ 4 ] further introduced a feature centered mining approach to generate discriminative patterns sequentially by incrementally eliminating training instances on a progressively shrinking FP Tree . [ 9 ] also proposes a method to solve the same problem . Unlike DDPMine [ 4 ] , it builds a decision tree to partition the data onto different nodes . Then at each node , one discriminative pattern is discovered directly to further divide its covering examples into purer subsets . Uncertain data mining attracts much attention recently . Several research efforts focus on frequent itemset mining . [ 6 ] proposes the U Apriori algorithm using expected support to find all the frequent itemsets on uncertain data . Later a probabilistic filter for earlier candidate pruning was further devised in [ 5 ] . The UF Growth algorithm was proposed in [ 13 ] . Besides using expected support like U Apriori , UF Growth uses the FP Tree [ 12 ] approach to avoid expensive candidate generation . [ 1 ] discusses the frequent pattern mining for uncertain datasets and shows how to extend a broad classes of algorithms to uncertain data . Trying to solve the inaccuracy in measuring frequentness using expected support , [ 2 ] proposes to use frequentness probability under possible worlds semantics and devises an efficient computing technology .
In recent years , several classification algorithms have been proposed for uncertain data too . [ 20 ] proposes to use decision tree for classifying uncertain numeric data where the value uncertainty is represented by multiple values forming a probability distribution function . Simultaneously uRule [ 17 ] tries to solve the problem using rule based classifier extended from classical algorithm Ripper [ 7 ] . The authors also extended the entropy and information gain measure for uncertain data . Extending from classical decision tree algorithm C4.5 [ 18 ] and adopting the same discrimination measure of uRule , DTU [ 16 ] achieves close accuracy to uRule while runs much faster in most cases . Both uRule and DTU support uncertain numeric data and uncertain categorical data . However , the new measurements of probabilistic entropy and probabilistic information gain are only probabilistic cardinalities , which means that they do not have reasonable theoretical explanations .
3 . PRELIMINARIES 3.1 The Uncertain Data Model
We adopt the same uncertain model of categorical data used in both [ 17 ] and [ 16 ] . For each input dataset , it is composed of a set of attributes A . For each attribute Ai ∈ A(1 ≤ i ≤ |A| ) , if it contains values which are uncertain it is called an uncertain attribute and is denoted by Au i , or else a certain attribute which is i . The set of all uncertain and the set of all certain denoted by Ac attributes are denoted by Au and Ac , respectively . The value of attribute Ai in the jth instance is denoted by ai,j . For an uncertain attribute Au i , its value in each instance is represented as a probability distribution function pdfi,j which records the possibility for each possible value in the categorical domain domAi for Ai , instead of a single value for a certain attribute . Given the domain of Ai , domAi = {v1,··· , vk,··· , vn} , pdfi,j could be represented using a probability vector Pi,j = {p1,··· , pk,··· , pn} such that k=1 pk = 1 . There is also a class label attribute C containing class label for each instance . cj is used to denote the class label of the jth instance . In this paper , the class label attribute C is not included in the set of attributes A .
P ( ai,j = vk ) = pk andn
Table 1 provides a toy example of an uncertain database about computer buying evaluation with one uncertain attribute on quality .
Price Looking Tech . Spec .
Evaluation Unacceptable Acceptable
Good
Very Good
+ / /
+ +
/ / / +
Quality
{ : 0.8 , / : 0.1 , + : 0.1} { : 0.1 , / : 0.8 , + : 0.1} { : 0.1 , / : 0.8 , + : 0.1} { : 0.1 , / : 0.1 , + : 0.8}
Table 1 : Example of an Uncertain Database ( + : Good , / : Medium , : Bad )
3.2 Frequent Itemset Mining
Since the foundation of associative classification is frequent pattern mining , we also introduce the definitions and notations related to frequent pattern mining . Specifically , we discuss frequent itemset mining on uncertain categorical data . Given a set of items I 1 in input database input_db , an itemset x is defined as a subset of I . A transaction tj is defined as the set of values on each attribute Ai in the jth instance Ai,j and a class label cj . The complete set of transactions in any database db ⊆ input_db is denoted by T db or simply T when db is clear in context . action ti if x ⊆ ti . lute support of x with respect to db , denoted by supdb
Traditionally an itemset x is said to be supported by a trans|{ti|ti ⊆ T ∧ x ⊆ ti}| is called the absox or supx in clear context , while supx/fifiT input_dbfifi is called the relative sup c or supx port . When it is clear , absolute support and relative support could be used interchangeably . We also use supdb c to denote |{ti|ci = c ∧ ti ⊆ T ∧ x ⊆ ti}| , the support value of x under class x c . x is said to be frequent iff supx ≥ supmin , where supmin is a user specified minimum ( absolute/relative ) support threshold . While on uncertain database , there exists a probability of x ⊆ ti when x contains at least one item of uncertain attribute , and the support of x is no longer a single value but a probability distribution function instead . In this paper we use expected support to represent the support value on uncertain data . The expected support E(supx ) t∈T P ( x ⊆ t ) . For example , in Table 1 we have an itemset {/@P rice , +@Quality} with 1Note that items appeared in different attributes are different even if they are identical literally . of itemset x is defined as E(supx ) = expected support of 0.1 + 0.8 = 09 E(supx c ) could be defined similarly . The concept of frequent itemset is the same as on certain database . When the context is clear E(supx ) could also be denoted by supx for unified representation .
Finally , we summarize the notations used mostly through this paper in Table 2 .
Notation A ( Ac / Au ) Ai ( Ac i / Au i ) ai,j C cj I T tj domAi c ) supx ( supx supmin conf c x
Ei(confx c / supx c )
Description Set of ( certain / uncertain ) attributes ith attribute in A ( which is certain / uncertain ) Value of Ai in the jth instance Class attribute Class label in the jth instance Set of possible values on Ai Set of Items on the whole input database Set of transactions in current database jth transaction t in T ( Expected ) Support of itemset x ( on class c ) Minimum support threshold ( Expected ) Confidence of itemset x on class c Part of Expect on confx c when supx = i c / supx
Ei,n(confx c / supx boundi(confx c )
IS
U x@y c ) Part of Expect on confx c / supx c on the first c n transactions when supx = i ith upper bound on confx Set of discovered candidate itemsets through the algorithm With y uncertain attribute(s ) under uncertain degree of x %
Table 2 : Summary of Notations
4 . EXPECTED CONFIDENCE
In our algorithm uHARMONY we use expected confidence of a discovered itemset to measure its discrimination . Unlike probabilistic cardinalities like probabilistic entropy and probabilistic information gain used in [ 17 ] and [ 16 ] which may be not precise and are lack of theoretical explanations and statistical meanings , expected confidence is guarantied to be statistical meaningful in theory while providing relatively accurate measure of discrimination . However , the calculation of expected confidence is non trivial and requires careful consideration . On uncertain database expected c/supx ) of itemset x on class confidence E(confx c is not simply equal to E(supx c)/E(supx ) , although we have c/supx on certain database . For example in Taconfx ble 1 , for itemset x = {−@Looking,−@Quality} and class c ) = 10×(08×09)+05× c = U nacceptable we have E(confx c)/E(supx ) = 08/(08+01 ) ≈ ( 08×01 ) = 0.76 while E(supx 089 Obviously , E(confx c)/E(supx ) . 4.1 Definition of Expected Confidence c ) is not equal to E(supx c ) = E(supx c = supx
DEFINITION 1 . Given a set of transactions T and the set of possible worlds W with respect to T , the expected confidence of an itemset x on class c is
E(confx c ) = confx,wi c×P ( wi ) = c supx,wi supx,wi
×P ( wi ) wi∈W wi∈W where P ( wi ) is the probability of world wi . confx,wi c is the respected confidence of x on class c in world wi , while supx,wi c ) is the respected support of x ( on class c ) in world wi . ( supx,wi However , this formula could not be used directly to calculate the expected confidence , due to the extremely large number of possiAk∈Au |domAk|)|T| ) ble world |W| . Actually , there are O((1 + possible worlds , where 1 stands for not taking the transaction while Ak∈Au |domAk| stands for combinations of values in uncertain attributes when taking the transaction . Hence , more efficient computation technology is needed . 4.2 Efficient Computation of Expected Confi dence
In order to devise an efficient method for computing expected confidence , we first introduce a lemma . when cn = c , since we have :
Ei−1,n−1(supx c + 1 ) = Ei−1,n−1(supx c ) + Pi−1,n−1
For computing Pi,n , we also introduce the following theorem . THEOREM 2 . Denoting P ( x ⊆ ti ) as pi for each transaction ti ∈ T , we have
Pi,n = pn × Pi−1,n−1 + ( 1 − pn ) × Pi,n−1
LEMMA 1 . Since 0 ≤ supx c ≤ supx ≤ |T| , we have :
, where 1 ≤ i ≤ n ≤ |T| . wi∈W i |T| |T| j=0 i=0
E(confx c ) =
=
= confx,wi c × P ( wi )
× P ( supx = i ∧ supx j i
Ei(supx c )
= i=0 i
|T| i=0
Pi,n = c = j ) where i = 0 .
1 Pi,n−1 × ( 1 − pn ) f or n = 0 f or 1 ≤ n ≤ |T|
Ei(confx c ) where n < i .
Pi,n = 0
, where Ei(supx support and confidence of itemset x on class c when supx = i . c ) denote the part of expected c ) and Ei(confx
Given 0 ≤ n ≤ |T| , we define En(supx c ) i=0 Ei,n(supx as the expected support of x on class c on the first n transactions of T , and Ei,n(supx c ) as the part of expected support of x on class c with support of i on the first n transactions of T . We have the following theorem . c ) =|T|
PROOF . The proof is similar to that of Theorem 1 .
Now we could compute the expected confidence of itemset x on c ) . class c , since E(confx i=0 Ei,|T|(confx The whole computation is divided into |T|+1 steps with Ei,|T|(confx c)/i ( 0 ≤ i ≤ |T| ) computed in ith step . Figure 1 = Ei,|T|(supx shows the computation process . c ) = E|T|(confx c ) c ) =|T|
THEOREM 1 . Denoting P ( x ⊆ ti ) as pi for each transaction ti ∈ T , we have
Ei,n(supx c ) = pn × Ei−1,n−1(supx c ) + ( 1 − pn ) × Ei,n−1(supx c ) when cn = c , and
Ei,n(supx c ) = pn × Ei−1,n−1(supx
+ ( 1 − pn ) × Ei,n−1(supx c + 1 ) c ) when cn = c , where 1 ≤ i ≤ n ≤ |T| .
Ei,n(supx for ∀n where i = 0 , or where n < i . c ) = 0
Figure 1 : Computation Process of Expected Confidence
PROOF . If x ⊆ tn , we have Ei,n(supx pn ) × Ei,n−1(supx world remains the same . If x ⊆ tn , there exist two situations : c ) since both supx and supx c ) = Ei,n(supx c)+(1− c in each possible c in each possible world remains the same c)+ c ) = Ei,n(supx
When cn = c , supx while supx = supx+1 and we have Ei,n(supx pn × Ei−1,n−1(supx When cn = c , supx possible world . Hence we have Ei,n(supx pn × Ei−1,n−1(supx c ) . c = supx c + 1 ) similarly .
Thus , the theorem is proved . c +1 and supx = supx +1 in each c ) + c ) = Ei,n(supx
Defining Pi,n as the probability of x having support of i on the first n transactions of T , we have
Ei,n(supx c ) = pn × ( Ei−1,n−1(supx c + 1 ) )
+ ( 1 − pn ) × Ei,n−1(supx = pn × ( Ei−1,n−1(supx + ( 1 − pn ) × Ei,n−1(supx c ) c ) c ) + Pi−1,n−1 )
Finally , we prove the computation complexity of the expected confidence E(confx c ) in terms of time and space in Theorem 3 . c )
THEOREM 3 . The computation of the expected confidence E(confx requires at most O(|T|2 ) time and at most O(|T| ) space . the size of matrix depicted in Figure 1 containing|T|
PROOF . The number of computation iterations is bounded by i=0 |T| + 1− i cells . Each cell represents a computation iteration performed in O(1 ) time . Hence , the total computation requires O(|T|2 ) time . Since only two rows in Figure 1 need to be reserved to complete the computation , only O(|T| ) space is required . Together with the O(|T| ) space to store pi = P ( x ⊆ ti ) for each ti ∈ T , totally O(|T| ) space is required to finish the computation .
Comparing with the complexity using the definition of expected confidence , we could see that our computation strategy is very efficient and reduces the time complexity significantly .
#Transaction / nSupport / i01|T|01|T|,||()ciTxconfE1,||1()ciTxconfE 22Computation in One StepStart of Next StepExplaination1,||()ciTxconfE ||,||0()()TccxiTxiconfcEEonf==∑ Ek,|T|(supx c ) k
Ek,|T|(supx c ) k
Ek,|T|(supx c ) k
+
+
+ k=i
|T| |T| |T| k=i
Ek,|T|(supx c ) k
Ek,|T|(supx c ) i
Ek,|T|(supx c ) k=0 i
Figure 2 : Computation Process of Expected Confidence using Upc : Maximum ( expected ) confidence in curper Bound ( conf cur_db rent database on current class ) max
− i−1 k=0
Ek,|T|(supx c ) i
= k=0 i−1 ≤ i−1 i−1 i−1 k=0 k=0
=
= k=0
Figure 3 : Example on Upper Bound Computation ( car , U10@1 , supmin = 0.01 , x = {vhigh@buying , 3@doors , med@saf ety} , c = acc ) c ) .
4.3 Upper Bound of Expected Confidence
We further develop a theorem to compute the upper bound of expected confidence . Given a class c , if the upper bound of a pattern x is smaller than the maximum ( expected ) confidence of another pattern y we have mined previously having x ⊂ y , the computation could be stopped since x would never be provided as a more discriminative pattern with higher confidence value .
THEOREM 4 . Given 1 ≤ i ≤ |T| , boundi(confx c ) =
Ek,|T|(supx c ) × (
1 k
− 1 i
) +
E(supx c ) i i−1 k=0 is an upper bound of E(confx c ) .
PROOF . For ∀i(1 ≤ i ≤ |T| ) , we have E(confx c ) = E|T|(confx c )
Ek,|T|(supx c ) × (
1 k
− 1 i
) +
E(supx c ) i
=boundi(confx Hence boundi(confx c ) c ) is an upper bound of E(confx
COROLLARY 1 . For 1 ≤ i ≤ |T| , we have :
E(supx c ) = bound1(confx c )
≥ ··· ≥ boundi(confx ≥ bound|T|(confx c ) ≥ ··· c ) = E(confx c ) c ) − i−1 k=0
PROOF . It is easy to get that bound1(confx c ) = E(supx c ) using the definition . Since c ) and bound|T|(confx c ) = E(confx boundi−1(confx c ) − boundi(confx c )
=(
1 i − 1
− 1 i
) × ( E(supx
Ek,|T|(supx c ) ) ≥ 0 for 1 < i ≤ |T| , the corollary is proved .
For ith step in computing Ei,|T|(confx c ) , we could compute the respective upper bound boundi(confx c ) using Ek,|T|(supx c ) ( 0 ≤ k ≤ i − 1 < i ≤ |T| ) which all have been computed previously . Actually since boundi(confx c ) = boundi−1(confx c )
− (
1 i − 1
− 1 i
) × ( E(supx c ) − i−1 k=0
Ek,|T|(supx c ) )
, we could compute boundi(confx ous upper bound boundi−1(confx c ) more efficiently with previc ) .
With the corollary , we know that the upper bound in the current step would be smaller than the one in the previous step and is more close to the value of expected confidence . Figure 2 illustrates the computation process of expected confidence using upper bound .
We also provide an example on computation of upper bound shown in Figure 3 , conducted on one of the evaluation datasets . ( Details of the evaluation parameters could be found in Section 6 . )
5 . ALGORITHM DETAILS
Now we give the details of the whole algorithm of uHARMONY . First we present the frequent itemset mining algorithm on uncertain categorical data . Then we will discuss the instance covering technology using minimum cover probability . Finally , details of using either SVM classifier or rule based classifier are presented . 5.1 Mining Algorithm
The framework of our frequent itemset mining algorithm is similar to that of HARMONY [ 21 ] . However , there also exist significant differences . First , the infrequent pattern pruning technology used in HARMONY is no longer applicable on uHARMONY . Since on uncertain data even if the expected support is equal to or higher than the minimum support , there still remain situations where the itemset support is less than the minimum support . Second , items on uncertain attributes need carefully consideration since on uncertain attributes the pattern searching space could not shrink when the current prefix pattern gets extended .
Algorithm 1 gives the details of our frequent itemset mining algorithm in uHARMONY . Note that before running the algorithm , we need to first sort the attributes to place all the certain attributes before uncertain attributes . Hence when we traverse the attributes for extending items , uncertain attributes which would not help shrink the pattern searching space will be encountered at last . This helps to speedup the algorithm . calcExpConf is the function of expected confidence computation of current itemset x with upper bound computation used . Function coverInstances is used to cover instances with the current itemset , whose details will be provided later in Section 52 The variable IS is used throughout the algorithm to maintain all the discovered candidate itemsets as the output .
#Transaction / nSupport / i01|T|01|T|,||()ciTxconfE1,||()ciTxconfE 1,||1()ciTxconfE 22Stop Condition:SkippedComputation in One StepStart of Next StepExplaination_()cccurdbixmaxboundconfconf£ 0.1 1 10 10 20 30 40 50 60 70 80 90 100boundi(confxc)Support / i boundi(confxc ) confmaxcur_dbc confxcboundi(confxc ) ( Skipped ) Stop when boundi(confxc ) <= confmaxcur_dbc Algorithm 1 : Itemset Mining Algorithm of uHARMONY Function : mine(T , index , x , IS,{confmax Input : Current set of transactions , Current attribute index , Current itemset pattern , Set of discovered candidate itemsets through the algorithm , Set of maximum confidences for each class 2 c|c ∈ domC} ) confx confx c/supx ; c ← calcExpConf ( T , x , confmax foreach c ∈ domC do c ← supx foreach c ∈ domC do
1 if Aindex ∈ Au then 2 3 4 else 5 6 7 coveredN um ← 0 ; 8 foreach c ∈ domC do 9 10 11 12 13 if coveredN um > 0 then 14 15 for Aindex∗ ∈ {Aindex∗|Aindex∗ ∈ A ∧ index < index∗} coveredN umc ← coverInstances(T , x , IS ) ; confmax coveredN um ← coveredN um + coveredN umc ; IS ← IS ∪ {x} ; c ← confx if confx c > confmax c then c ) ; c ; do
16 17 18 19 20 21 22 23 for i ∈ domAindex∗ do x∗ ← x ∪ {i@Aindex∗} ; if supx∗ ≥ supmin then if Aindex∗ ∈ Au then
T ∗ ← {t|t ∈ T ∧ x∗ ⊆ t} ; T ∗ ← T ; else mine(T ∗ , index∗ , x∗ , IS,{confmax domC} ) ; c|c ∈
5.2
Instance Covering Strategy
HARMONY adopts a simple strategy for instance covering . It tries to find one most discriminative covering pattern with the highest confidence for each instance . However , this strategy is not practical for uncertain data , since each itemset has a probability being contained in the instance . If we just find the itemset with the highest confidence for each instance , the probability of the instance being covered could be very low .
In uHARMONY , we propose an instance covering strategy by applying a threshold of minimum cover probability coverP robmin . We try to assure that the probability of each instance not covered by anyone itemset is less than 1 − coverP robmin . For each instance t with class label c , we sort the covering itemsets in the descending order with respect to confidence on class c . When a new itemset is discovered we insert it into that list ISt = {x|x ∈ IS ∧ x ⊆ t} ⊆ IS . Then only the first k itemsets ISt[1 , k ] ⊆ ISt with x∈ISt[1,k](1 − P ( x ⊆ t ) ) < 1 − coverP robmin ( 1 ≤ k ≤ |ISt| ) are selected to remain in the list . For each removed itemsets in ISt − ISt[1 , k ] , we decrease its total covered number on all instances , and remove it from the candidate itemset set IS when its total covered number reaches 0 . 5.3 Classification Algorithm
The mined frequent itemsets could be used either as training features of SVM classifier or as classification rules of rule based clas
2A new set is created each time to avoid overwriting previous values . sifier . In this section , we will discuss the details of algorithms classifying instances using SVM classifier and rule based classifier .
SVM Classifier
531 It is very simple to convert the mined patterns to training features of SVM classifier . Each pattern is a feature with the feature weight for an instance as the probability of the instance containing the itemset . According to the accuracy evaluation , this approach could provide 4 % to 10 % improvement on average in terms of classification accuracy comparing with two state of the art algorithms .
532 Rule based Classifier To use the mined itemsets as classification rules , we adopt the similar classifier construction algorithm of HARMONY . For each test instance we just sum up the product of the confidence of each itemset on each class and the probability of the instance containing the itemset . The class with the largest value is the predicted class of the instance . Although this algorithm is simple , it is effective in classification . Accuracy evaluation shows that this algorithm outperforms two state of the art baselines too .
6 . EVALUATION RESULTS
In this section , we will present the evaluation results of our algorithm uHARMONY . Our algorithm is implemented in C# . All the experiments were conducted on a computer with Intel Core Duo 2 E6550 CPU ( 2.33GHz ) and 2GB memory installed . 6.1 Datasets
Due to the unavailability of public uncertain categorical datasets , we conducted our evaluation on 30 public certain datasets from UCI Machine Learning Repository 3 , by converting them into uncertain ones with varying uncertain degree ( defined as the probability of each instance on each uncertain attribute taking values other than the original value in the certain dataset ) over different attribute numbers . Details of the converting procedure on categorical datasets could be found in [ 17 ] . Missing values appearing in the uncertain attributes are converted to uncertain ones with the same probability for each possible value . For some datasets containing not only categorical but also continuous attributes , discretization was applied using the entropy method proposed in [ 10 ] and adopted in [ 15 ] using weka 4 . Attributes containing unique identifier for each instance and instances with missing class label have also been removed . Detailed characteristics of datasets are listed in Table 3 . We could see that those datasets cover most of the common areas . 6.2 Classification Accuracy Evaluation
Now we give the evaluation results of our algorithm on classification accuracy , comparing with two state of the art classification algorithms uRule [ 17 ] and DTU [ 16 ] , which are extended from the famous rule based classifier Ripper [ 7 ] and decision tree classifier C4.5 [ 18 ] . To our best knowledge , the two algorithms are the only available algorithms supporting uncertain categorical data . For training and classifying of SVM , svmlight and svmmulticlass are used for 2 class and multi class situations , respectively . 5
3All the datasets and their detailed descriptions could be found at http : //archiveicsuciedu/ml/indexhtml 4The software is available at http://wwwcswaikatoacnz/ ml/weka/ . 5They are available at http://wwwcscornelledu/people/ tj/svm%5Flight/ . For svmmulticlass , the parameter of trade off ( “ c ” ) is set to 1000 .
Dataset australian balance bands breast bridges v1 bridges v2 car contraceptive credit echocardiogram flag german heart hepatitis horse monks 1 monks 2 monks 3 mushroom pima spect survival ta_eval tic tac toe vehicle voting wine zoo postoperative promoters
#Instance
690 635 539 699 106 106 1728 1473 690 131 194 1000 920 155 368 556 601 554 8124 768 90 106 267 306 151 958 846 435 178 101
#Attribute
14 4 38 9 11 10 6 9 15 12 28 19 13 19 27 6 6 6 22 8 8 57 22 3 5 9 18 16 13 16
#Class
2 3 2 2 6 6 4 3 2 2 8 2 5 2 2 2 2 2 2 2 3 2 2 2 3 2 4 2 3 7
Table 3 : Dataset Characteristics
Area
Financial
Social Physical
Life N/A N/A N/A Life
Life N/A
Financial
Financial
Life Life Life N/A N/A N/A Life Life Life Life Life Life N/A Game N/A Social Physical
Life uRule , which validates the effectiveness of our algorithm . Experimental results with other uncertain attributes and under other uncertain degrees also show similar results .
Dataset australian balance bands breast bridges v1 bridges v2 car contraceptive credit echocardiogram flag german heart hepatitis horse colic monks 1 monks 2 monks 3 mushroom pima postoperative promoters spect survival ta_eval tic tac toe vehicle voting wine zoo
Average uHARMONYrule
85.37 89.3 58.63 65.52 62 62.2 77.72 47.59 85.95 93.29 52.42 69.6 56.64 82.52 82.88 91.36 65.72 96.4 97.45 65.11 69.75
69
80.19 73.53 45.04 76.2 63.44 92.86 51.11 88.76 73.2517
DTU 83.6232 56.32 uRule 84.3478 62.88
N/Amem N/Amem 94.5637 91.2732 55.6604 59.434 64.1509 57.5472 70.0231 70.0231 50.1018 44.2634 84.3478 74.3478 87.0229 92.3664 59.2784 44.8454
72.3
70.1
80
53.0435
52.3913 79.3548 85.3261 N/Atime 70.6835 74.6403 65.7238 65.7238 68.0505 79.9639 99.9877 67.3177
65.1042
100
70
70
71.6981 61.3208 81.6479 79.0262 73.5294 72.549 48.3444 33.7748 81.524 72.6514 64.7754 N/Amem 94.9425 94.4828 41.573 42.1348 92.0792 89.1089 72.2670 69.4649 supmin
0.05 0.1 0.25 0.05 0.1 0.1 0.01 0.01 0.1 0.1 0.1 0.1 0.25 0.1 0.1 0.1 0.1 0.1 0.1 0.25 0.25 0.25 0.1 0.1 0.01 0.05 0.01 0.25 0.01 0.1
621 Accuracy Comparison Table 4 shows the evaluation results in terms of classification accuracy , with 1 , 2 , or 4 uncertain attribute(s ) using SVM classifier . The upper part of the table is for uncertain degree of 10 % , while the lower part is for uncertain degree of 20 % . Ux@y denotes the datasets with y uncertain attribute(s ) under uncertain degree x % ; N/Atime denotes that the algorithm did not finish in an acceptable time ; N/Amem denotes that the algorithm ran out of memory . All the uncertain attributes are selected from all the non class attributes with the highest information gain values . All the experiments were conducted using 10 fold cross validation . For the same dataset , the same parameter values like supmin were used . Values in bold stand for the highest accuracies for the corresponding datasets . We could easily find that on most situations , our algorithm uHARMONY outperforms the state of the art algorithms DTU and uRule significantly , with up to 28 % improvement on dataset balance , and on average 4 % to 10 % improvements on all the 30 datasets under varying uncertain parameters . Besides , uHARMONY is also more memory efficient than both DTU and uRule , especially on datasets whose uncertain attributes have many possible values . For example , on dataset bands , DTU and uRule ran out of the memory and could not finish properly in situations with more uncertain attributes . Actually , the experiments with 8 uncertain attributes and under uncertain degree of 40 % show the same results .
Since uHARMONY supports the rule based classifier besides SVM classifier . We also conducted experiments on all the datasets with four uncertain attributes under uncertain degree of 10 % . From Table 5 we could see that although uHARMONYrule ( stands for the rule based uHARMONY classifier ) could not outperform uHARMONY ( stands for the SVM based uHARMONY classifier ) , it provides near 1 % to 4 % higher accuracy on average than DTU and
Table 5 : Accuracy ( in % ) Comparison of U10@4 for uHarmony using Rule based Classifier
Sensitivity Test
622 We also evaluated uHARMONY on two popular datasets breast and wine , under varying minimum supports and varying minimum cover probabilities . Figure 4 shows the results with varying minimum supports . We could find that the supmin is crucial to the accuracy . If a too high supmin is specified , few patterns could be found . But a too low supmin could also hurt the accuracy . In our experiments , supmin ranging from 0.01 to 0.25 were chosen . On many datasets , a minimum support of 0.1 could provide the best results . However , the accuracy is insensitive to supmin under varying uncertain degree and uncertain attribute number . This means we just need to choose one proper supmin which will work well for all the uncertain datasets derived from the same certain dataset . We also tested the algorithm sensitivity against minimum cover probability . The results are shown in Figure 5 . We see on average a minimum cover probability of 90 % could provide the best results ( Minimum cover probability of 90 % is used during all this paper if not specified explicitly . ) Note that a minimum cover probability of 0 % works as selecting at most one pattern for each instance like HARMONY . 6.3 Runtime Efficiency Evaluation 631 Efficiency Test Figure 6 provides the evaluation results of algorithm efficiency on six of the datasets in terms of both running time and memory us
Dataset uHARMONY uRule uHARMONY uRule uHARMONY uRule supmin
DTU U10@1 contraceptive credit echocardiogram australian balance bands breast bridges v1 bridges v2 car flag german heart hepatitis horse monks 1 monks 2 monks 3 mushroom pima spect survival ta_eval tic tac toe vehicle voting wine zoo
Average australian balance bands breast bridges v1 bridges v2 car postoperative promoters contraceptive credit echocardiogram flag german heart hepatitis horse monks 1 monks 2 monks 3 mushroom pima spect survival ta_eval tic tac toe vehicle voting wine zoo
Average postoperative promoters
86.542 90.577 69.939 95.998 65.068 64.143 88.66 51.797 85.517 93.289 65.667 72.8 57.943 83.772 86.108 100 69.554 96.402 99.618 68.106 69.614 87.166 85.846 73.529 54.298 100 65.323 96.099 53.086 93.954 79.0138
85.384 91.212 69.939 94.71 65.068 64.143 88.953 51.525 85.361 93.289 65.667 72.5 56.965 83.772 86.108 100 75.028 96.402 99.79 68.106 69.864 85.667 85.489 73.529 52.965 100 65.323 94.258 53.641 93.954 78.9537
84.2029 69.92
85.942 65.12 65.1206 N/Amem 93.7053 91.1302 51.8868 59.434 65.0943 60.3774 91.1458 85.5324 44.2634 50.1018 87.3913 84.2029 92.3664 92.3664 67.0103 62.3711
69
54.1304 79.3548 85.3261 97.8417 65.7238 98.917 100
70.6 50
78.0645
87.5
95.6835 64.2263 98.1949
100 68.099 65.1042 68.8889 68.8889 71.6981 76.4151 83.8951 79.7753 73.5294 70.2614 40.3974 44.3709 85.6994 97.7035 64.0662 N/Amem 91.954 93.7931 43.2584 39.8876 89.1089 92.0792 74.8738 75.2111
78.8406 69.92
84.4928 66.08 65.1206 N/Amem 93.8484 91.2732 59.434 51.8868 65.0943 60.3774 90.7986 81.25 44.1276 50.9165 80.7246 84.3478 92.3664 92.3664 67.0103 63.4021
69.3
54.1304 79.3548 85.3261 95.5036 65.7238 98.917 100
69.4 48.913 78.0645
87.5
95.3237 64.7255 97.8339
100 68.099 65.8854 68.8889 68.8889 71.6981 76.4151 83.1461 80.8989 73.5294 70.5882 41.0596 44.3709 84.3424 98.1211 64.0662 N/Amem 91.7241 90.5747 43.2584 39.8876 88.1188 92.0792 74.6577 74.6287
U20@1
DTU U10@2
86.087 66.08
85.2174
71.2
N/Amem N/Amem 93.5622 91.2732 53.7736 60.3774 67.9245 55.6604 69.0394 72.9745 51.5954 43.9919 86.9565 86.5217 92.3664 90.8397 65.9794 59.7938
70.7
54.2391 79.3548 85.3261 74.6403 65.7238 79.9639
100
65.1042
70
70.5
50.9783 77.4194 87.2283 97.1223 63.0616 79.9639
100
67.4479
70
69.8113 73.5849 83.5206 79.7753 73.5294 70.915 34.4371 48.3444 78.81 96.6597 64.0662 N/Amem 90.8046 93.5632 39.3258 39.8876 90.099 90.099 73.4107 73.1629
U20@2
84.058 66.24
80.5797
71.2
N/Amem N/Amem 93.7053 91.2732 60.3774 53.7736 67.9245 55.6604 70.0231 70.0231 50.9165 43.5166 81.7391 84.058 79.3893 86.2595 65.9794 59.7938
69.7
54.2391 79.3548 85.3261 74.6403 65.7238 79.9639
100
65.1042
70
68.3
50.8696 77.4194
87.5
90.8273 63.0616 79.9639
100
67.4479
70
70.7547 75.4717 83.8951 80.8989 73.5294 70.915 37.0861 43.7086 78.81 96.2422 64.0662 N/Amem 93.3333 91.0345 39.8876 39.8876 90.099 90.099 72.5460 72.5642
86.109 90.736 69.939 94.569 65.068 63.032 89.82 49.895 85.517 92.52 62.544 72.7 58.166 83.772 86.108 100 72.187 96.402 99.766 68.106 69.614
86
86.956 73.529 52.263 100 65.441 94.475 53.601 92.074 78.6970
85.535 91.212 69.939 94.853 65.068 63.033 88.26 50.506 85.217 92.52 62.544 72.8 57.627 83.772 86.108 100 74.852 96.402 99.74 68.106 69.614
87
85.859 73.529 48.298 100 65.441 94.708 53.602 92.074 78.6073
DTU U10@4
83.6232 56.32
84.3478 62.88
N/Amem N/Amem 94.5637 91.2732 59.434 55.6604 64.1509 57.5472 70.0231 70.0231 50.1018 44.2634 74.3478 84.3478 92.3664 87.0229 44.8454 59.2784
72.3
53.0435
80
85.3261 74.6403 65.7238 79.9639
100
65.1042
70
70.1
52.3913 79.3548 N/Atime 70.6835 65.7238 68.0505 99.9877 67.3177
70
61.3208 71.6981 81.6479 79.0262 73.5294 72.549 48.3444 33.7748 72.6514 81.524 64.7754 N/Amem 94.9425 94.4828 41.573 42.1348 89.1089 92.0792 72.2670 69.4649
U20@4
77.971 56.32
79.1304 63.04
N/Amem N/Amem 94.5637 90.9871 55.6604 60.3774 57.5472 55.6604 70.0231 70.0231 44.1276 47.0468 73.913 83.3333 87.0229 77.8626 49.4845 40.2062
71.2
53.3696 79.3548 85.3261 74.6403 65.7238 79.9639
100
65.1042
70
59.434 75.6554 73.5294 39.7351 72.8601 64.539 91.954 42.1348 92.0792 69.9157
68.3
50.4348 79.3548 N/Atime 69.2446 65.7238 70.5776
100
67.3177
70
54.717 78.6517 72.8758 32.4503 78.7056 N/Amem 89.1954 41.573 89.1089 68.2066
86.821 90.736 68.609 93.999 67.012 64.144 87.907 49.426 86.382 92.52 59.868 72.9 57.854 79.264 86.1 100 76.7 96.402 99.717 68.106 69.198 77.667 85.474 73.529 45.746 99.895 64.02 95.406 50.525 93.045 77.9657
88.416 91.527 69.73 94.287 65.532 64.087 82.413 49.422 86.959 92.52 57.689 72.8 58.397 81.3 86.633 100 73.205 96.402 99.975 68.106 69.614 81.333 84.762 73.529 43.798 98.747 64.02 95.174 51.635 93.045 77.8352
0.05 0.1 0.25 0.05 0.1 0.1 0.01 0.01 0.1 0.1 0.1 0.1 0.25 0.1 0.1 0.1 0.1 0.1 0.1 0.25 0.25 0.25 0.1 0.1 0.01 0.05 0.01 0.25 0.01 0.1
0.05 0.1 0.25 0.05 0.1 0.1 0.01 0.01 0.1 0.1 0.1 0.1 0.25 0.1 0.1 0.1 0.1 0.1 0.1 0.25 0.25 0.25 0.1 0.1 0.01 0.05 0.01 0.25 0.01 0.1
Table 4 : Accuracy ( in % ) Comparison of U{10,20}@{1,2,4}
632 Effectiveness of the Expected Confidence Up per Bound
We evaluated the effectiveness of the expected confidence upper bound . Figure 7 shows the results on two of the most popular datasets , car and heart , either with the expected confidence upper bound or without the upper bound . The effectiveness of adopting the expected confidence upper bound could be seen easily .
( a ) breast
( b ) wine
Figure 4 : Accuracy Evaluation of U10@1 wrt Minimum Support
( a ) breast
( b ) wine
Figure 5 : Accuracy Evaluation of U10@1 wrt Minimum Cover Prob . age . supmin values are the same as those in Table 4 . Note that the running time includes both the classifier construction time and classifier classification time . For example for uHARMONY , it includes the time of mining patterns , converting to SVM input , SVM training and SVM classifying . All values are measured on all 10 fold cross validations . We could see that DTU is the fastest algorithm in all cases . Note that for uHARMONY , SVM training and classifying take a great amount of running time for more than half . Well for memory usage , uHARMONY consumes almost the fewest and the stablest in most cases , while uRule always consumes the most . uRule even ran out of the available memory on several datasets .
( a ) car
( b ) heart
Figure 7 : Running Time Evaluation of U10@4
Scalability Test
633 Finally , we evaluated the scalability of our algorithm . Results in terms of running time are listed in Figure 8 . It is obvious that using the expected confidence bound offers better efficiency in running time . Figure 9 also shows the results in terms of memory usage . We could see that the increase of memory usage is smaller than 10 MB even when the size of dataset increases 16 times . Hence , our algorithm is also efficient in terms of memory usage . Note that since using the upper bound on computation of expected confidence and not using the upper bound consume nearly the same amount of memory , only the results of using the upper bound are shown .
( a ) car ( supmin = 0.01 )
( b ) heart ( supmin = 0.01 )
Figure 8 : Scalability Evaluation ( U10@1 , Running Time )
( a ) Running Time ( in sec )
( b ) Memory Use ( in MB )
( a ) car ( supmin = 0.01 )
( b ) heart ( supmin = 0.01 )
Figure 6 : Classification Efficiency Evaluation of U10@1
Figure 9 : Scalability Evaluation ( U10@1 , Memory Usage )
94 94.5 95 95.5 96 96.5 971252551020Accuracy ( in %)Minimum Support ( in % ) 49 50 51 52 53 5402505124Accuracy ( in %)Minimum Support ( in % ) 94 94.5 95 95.5 96 96.50105090100Accuracy ( in %)Minimum Cover Prob . ( in % ) 51 51.5 52 52.5 53 53.50105090100Accuracy ( in %)Minimum Cover Prob . ( in %)uHarmonyDTUuRule 0.1 1 10 100 1000breastcarcontraceptiveheartpimawineRunning Time ( in sec)Dataset 10 100 1000breastcarcontraceptiveheartpimawineMemory Use ( in MB)DatasetWith Expected Conf BoundWithout Expected Conf Bound 10 15 20 25 30 35 40 45 50 0.2 0.4 0.6 0.8 1 1.2Running Time ( in sec)Minimum Support ( in % ) 10 15 20 25 30 35 40 0.2 0.4 0.6 0.8 1 1.2Running Time ( in sec)Minimum Support ( in %)With Expected Conf BoundWithout Expected Conf Bound 0 20 40 60 80 100 120 140 160 180 1 2 4 8 16Running Time ( in sec)Dataset Duplication Ratio 0 200 400 600 800 1000 1 2 4 8 16Running Time ( in sec)Dataset Duplication Ratio 10 15 20 25 30 35 40 1 2 4 8 16Memory Use ( in MB)Dataset Duplication Ratio 10 15 20 25 30 35 40 1 2 4 8 16Memory Use ( in MB)Dataset Duplication Ratio 7 . CONCLUSIONS
In this paper we propose a novel algorithm to solve the classification problem on uncertain categorical data . To achieve both high classification accuracy and efficiency , we try to mine frequent patterns directly from uncertain data using expected confidence as discrimination measure . The costly feature selection is avoided , and effective method for calculation of expected confidence is also devised . Empirical results show that our algorithm outperforms the state of the art algorithms significantly with 4 % to 10 % improvements on average in terms of accuracy on 30 datasets under varying uncertain degrees and uncertain attribute numbers . 8 . ACKNOWLEDGMENTS
This work was supported in part by National Natural Science Foundation of China under grant No . 60873171 , National Basic Research Program of China under Grant No . 2006CB303103 , and the Program for New Century Excellent Talents in University under Grant No . NCET 07 0491 , State Education Ministry of China . 9 . REFERENCES [ 1 ] C . C . Aggarwal , Y . Li , J . Wang , and J . Wang . Frequent pattern mining with uncertain data . In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 29–38 , Paris , France , 2009 . ACM .
[ 2 ] T . Bernecker , H P Kriegel , M . Renz , F . Verhein , and
A . Züfle . Probabilistic frequent itemset mining in uncertain databases . In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 119–128 , Paris , France , 2009 . ACM .
[ 3 ] H . Cheng , X . Yan , J . Han , and C W Hsu . Discriminative frequent pattern analysis for effective classification . In Proceedings of the 23rd International Conference on Data Engineering , pages 716–725 , Istanbul , Turkey , 2007 . IEEE . [ 4 ] H . Cheng , X . Yan , J . Han , and P . S . Yu . Direct discriminative pattern mining for effective classification . In Proceedings of the 24th International Conference on Data Engineering , pages 169–178 , Cancún , México , 2008 . IEEE .
[ 5 ] C . K . Chui and B . Kao . A decremental approach for mining frequent itemsets from uncertain data . In Proceedings of the 12th Pacific Asia Conference on Knowledge Discovery and Data Mining , PAKDD 2008 , pages 64–75 , Osaka , Japan , 2008 . Springer .
[ 6 ] C . K . Chui , B . Kao , and E . Hung . Mining frequent itemsets from uncertain data . In Proceedings of the 11th Pacific Asia Conference on Knowledge Discovery and Data Mining , PAKDD 2007 , pages 47–58 , Nanjing , China , 2007 . Springer . [ 7 ] W . W . Cohen . Fast effective rule induction . In Proceedings of the Twelfth International Conference on Machine Learning ( ICML 1995 ) , pages 115–123 , Tahoe City , California , USA , 1995 .
[ 8 ] G . Cong , K L Tan , A . K . H . Tung , and X . Xu . Mining top k covering rule groups for gene expression data . In Proceedings of the ACM SIGMOD International Conference on Management of Data , pages 670–681 , Baltimore , Maryland , USA , 2005 . ACM .
[ 9 ] W . Fan , K . Zhang , H . Cheng , J . Gao , X . Yan , J . Han , P . S .
Yu , and O . Verscheure . Direct mining of discriminative and essential frequent patterns via model based search tree . In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 230–238 , Las Vegas , Nevada , USA , 2008 . ACM .
[ 10 ] U . M . Fayyad and K . B . Irani . Multi interval discretization of continuous valued attributes for classification learning . In IJCAI 93 , Proceedings of the 8th International Joint Conference on Artificial Intelligence , pages 1022–1029 , Chambery , France , 1993 .
[ 11 ] C . Gao and J . Wang . Efficient itemset generator discovery over a stream sliding window . In Proceedings of the 18th ACM Conference on Information and Knowledge Management , CIKM 2009 , pages 355–364 , Hong Kong , China , 2009 . ACM .
[ 12 ] J . Han , J . Pei , Y . Yin , and R . Mao . Mining frequent patterns without candidate generation : A frequent pattern tree approach . Data Min . Knowl . Discov . , 8(1):53–87 , 2004 . [ 13 ] C . K S Leung , C . L . Carmichael , and B . Hao . Efficient mining of frequent patterns from uncertain data . In Workshops Proceedings of the 7th IEEE International Conference on Data Mining ( ICDM 2007 ) , pages 489–494 , Omaha , Nebraska , USA , 2007 . IEEE Computer Society .
[ 14 ] W . Li , J . Han , and J . Pei . Cmar : Accurate and efficient classification based on multiple class association rules . In Proceedings of the 2001 IEEE International Conference on Data Mining , pages 369–376 , San Jose , California , USA , 2001 . IEEE Computer Society .
[ 15 ] B . Liu , W . Hsu , and Y . Ma . Integrating classification and association rule mining . In Proceedings of the Fourteen ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 80–86 , New York City , New York , USA , 1998 .
[ 16 ] B . Qin , Y . Xia , and F . Li . Dtu : A decision tree for uncertain data . In Proceedings of the 13th Pacific Asia Conference on Knowledge Discovery and Data Mining , PAKDD 2009 , pages 4–15 , Bangkok , Thailand , 2009 . Springer .
[ 17 ] B . Qin , Y . Xia , S . Prabhakar , and Y C Tu . A rule based classification algorithm for uncertain data . In Proceedings of the 25th International Conference on Data Engineering , ICDE 2009 , pages 1633–1640 , Shanghai , China , 2009 . IEEE .
[ 18 ] J . R . Quinlan . C4.5 : Programs for Machine Learning ,
Morgan Kaufmann : 1 ed . 1993 .
[ 19 ] J . R . Quinlan and R . M . Cameron Jones . Foil : A midterm report . In Machine Learning : ECML 93 , European Conference on Machine Learning , pages 3–20 , Vienna , Austria , 1993 . Springer .
[ 20 ] S . Tsang , B . Kao , K . Y . Yip , W S Ho , and S . D . Lee .
Decision trees for uncertain data . In Proceedings of the 25th International Conference on Data Engineering , ICDE 2009 , pages 441–444 , Shanghai , China , 2009 . IEEE .
[ 21 ] J . Wang and G . Karypis . On mining instance centric classification rules . IEEE Trans . Knowl . Data Eng . , 18(11):1497–1511 , 2006 .
[ 22 ] X . Yin and J . Han . Cpar : Classification based on predictive association rules . In Proceedings of the Third SIAM International Conference on Data Mining , San Francisco , CA , USA , 2003 . SIAM .
