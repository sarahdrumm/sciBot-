Semi Supervised Feature Selection for Graph Classification
Xiangnan Kong
Department of Computer Science
University of Illinois at Chicago
851 S . Morgan Street Chicago , IL 60607 0753 xkong4@uic.edu
Philip S . Yu
Department of Computer Science
University of Illinois at Chicago
851 S . Morgan Street Chicago , IL 60607 0753 psyu@csuicedu
ABSTRACT The problem of graph classification has attracted great interest in the last decade . Current research on graph classification assumes the existence of large amounts of labeled training graphs . However , in many applications , the labels of graph data are very expensive or difficult to obtain , while there are often copious amounts of unlabeled graph data available . In this paper , we study the problem of semi supervised feature selection for graph classification and propose a novel solution , called gSSC , to efficiently search for optimal subgraph features with labeled and unlabeled graphs . Different from existing feature selection methods in vector spaces which assume the feature set is given , we perform semi supervised feature selection for graph data in a progressive way together with the subgraph feature mining process . We derive a feature evaluation criterion , named gSemi , to estimate the usefulness of subgraph features based upon both labeled and unlabeled graphs . Then we propose a branch and bound algorithm to efficiently search for optimal subgraph features by judiciously pruning the subgraph search space . Empirical studies on several real world tasks demonstrate that our semi supervised feature selection approach can effectively boost graph classification performances with semi supervised feature selection and is very efficient by pruning the subgraph search space using both labeled and unlabeled graphs .
Categories and Subject Descriptors H28 [ Database Management ] : Database ApplicationsData Mining
General Terms Algorithm , Performance , Experimentation
Keywords Semi Supervised Learning , Feature Selection , Graph Classification , Data Mining
1 .
INTRODUCTION
Graphs are ubiquitous and have become increasingly important in modeling diverse kinds of objects . In many realworld applications , instances are not represented as feature vectors , but as graphs with complex structures , eg , chemical compounds , program flows and XML web documents . One central issue in graph mining research is graph classification , which has a wide variety of real world applications , eg drug activity predictions , toxicology tests and kinase inhibitions . A major difficulty in graph classification lies in the complex structure of graphs and lack of vector representations . Selecting a proper set of features for graph data is an essential and important procedure for graph classification .
The general problem of feature selection is well studied in the literature . Semi supervised feature selection problem for graph data , however , has not been studied in this context so far . Conventional feature selection approaches on graph data assume , explicitly or implicitly , that there exists a large amount of labeled training data . However , in many real world applications , the labels of graph data are very expensive or difficult to obtain . Creating a large training dataset can be too expensive , time consuming or even infeasible . For example , in molecular medicine , it requires time , efforts and excessive resources to test drugs’ anti cancer efficacies by pre clinical studies and clinical trials , while there are often copious amounts of unlabeled drugs or molecules available from various sources .
Thus it is much desired that the large amounts of unlabeled graphs can be effectively utilized to select better features for graphs , and improve the graph classification performances . For example , in Figure 1 , we show a dataset with two labeled graphs and four unlabeled graphs . Based only on the two labeled graphs , subgraph feature “ a b ” and “ a c ” are both discriminative features . Clearly , when we consider the distribution of the four unlabeled graphs , “ a b ” is more likely to be useful than “ a c ” . This is because the unlabeled graphs are not separable based on the subgraph feature “ ac ” .
Despite its value and significance , the semi supervised feature selection for graph classification is a much more challenging task due to the specific characteristics of the task . The reasons are listed as follows . the labeled graphs are too few , the usefulness of the selected subgraph features can be weak , and thus detriment to the classification performances . ( 2 ) During the subgraph feature mining procedure , most supervised graph classification approaches require a branch and bound search to avoid exhaustive enumeration of all subgraphs in a dataset . However , when there are not enough labeled graphs , the pruning ability of the upper bound based on labeled graphs can be poor , thus making it infeasible to find discriminative subgraph features within a reasonable amount of time .
In this paper , we introduce a novel framework to the above problems by mining subgraph features using both labeled and unlabeled graphs . Our framework is illustrated in Figure 3 . Different from existing supervised feature selection methods for graph classification , our approach , called gSSC , can utilize both labeled and unlabeled graphs to find optimal subgraph features for graph classification . We first derive a feature evaluation criterion , named gSemi , based upon a given graph dataset with both labeled and unlabeled graphs . Then we propose a branch and bound algorithm to efficiently search for optimal subgraph features by deriving an upper bound of gSemi and pruning the subgraph search space using labeled and unlabeled graphs . In order to evaluate our model , we perform comprehensive experiments on real world graph classification tasks . The experiments demonstrate that the proposed semi supervised feature selection method for graph classification outperforms supervised approaches and is very efficient by pruning the subgraph search space using both labeled and unlabeled graphs . The rest of the paper is organized as follows . We start by a brief review on related works of graph feature selection and semi supervised feature selection in Section 2 . We then introduce the preliminary concepts , give the problem analysis and present the gSemi criterion in Section 3 . In Section 4 , we derive an upper bound of gSemi and propose the gSSC method . Then Section 5 reports the experiment results on real world graph classification tasks . In Section 6 , we conclude the paper .
Figure 2 : Supervised Feature Selection Process for Graph Classification
Figure 3 : gSSC Semi Supervised Feature Selection Process for Graph Classification
Figure 1 : An example of semi supervised feature selection on graph data . The subgraph feature “ ab ” is more useful than “ a c ” based on both labeled and unlabeled graphs .
1 . Lack of labels . Conventional feature selection in graph classification approaches focuses on supervised settings [ 7 , 14 , 13 ] . The mining strategy of discriminative subgraph patterns strictly follows the assumption that there exists a large amount of labeled graphs . However , many real world graph classifications usually suffer from a lack of training graphs . It is usually laborious , or even infeasible to create a large training set of graph instances .
2 . Lack of features . Another fundamental problem in semi supervised feature selection on graph data lies in the complex structures and lack of feature representations of graphs . Conventional feature selection approaches in vector spaces , which assume a candidate feature set is available , cannot be directly applied to graph data , because it is usually infeasible to generate all the subgraph features of a graph dataset before feature selection . The number of subgraphs is usually too large to be fully generated , since it grows exponentially with the graph size . Furthermore checking subgraph isomorphism is NP complete .
In order to efficiently find discriminative subgraph features , conventional supervised subgraph feature mining approaches rely on the label information from a large training set to prune the subgraph search space and select useful features [ 14 ] . However , when the number of labeled graphs is not large enough , the usefulness of the mined subgraph features can be weak , and the pruning of the subgraph mining process can be ineffective .
Figure 2 illustrates the feature selection process in conventional graph classification approaches . Obviously , when there is only a small number of labeled graphs available , supervised approaches cannot work well due to two reasons : ( 1 ) During the subgraph features mining procedure , supervised feature selection approaches for graph classification need to employ evaluation criteria to select discriminative subgraph features based on labeled graphs . However , when
2 . RELATED WORK
To the best of our knowledge , this paper is the first work on semi supervised feature selection problem for graph classification . Some research works have been done in related areas .
Extracting subgraph features from graph data have been investigated by many researchers . The goal of such approaches is to extract informative subgraph features from a set of graphs . Typically some filtering criteria are used . Upon whether considering the label information , there are two types of approaches : unsupervised and supervised . A typical evaluation criterion is frequency , which aims at collecting frequently appearing subgraph features . Most of the frequent subgraph feature extraction approaches are unsupervised . For example , Yan and Han develop a depth first search algorithm : gSpan [ 15 ] . This algorithm builds a lexicographic order among graphs , and maps each graph to an unique minimum DFS code as its canonical label . Based on this lexicographic order , gSpan adopts the depth first search strategy to mine frequent connected subgraphs efficiently . Many other approaches for frequent subgraph feature extraction have also been developed , eg AGM [ 5 ] , FSG [ 8 ] , MoFa [ 2 ] , FFSM [ 4 ] , and Gaston [ 10 ] . Moreover , supervised subgraph feature extraction problem has also been studied in literature , such as LEAP [ 14 ] and CORK [ 13 ] , which look for discriminative subgraph patterns for graph classifications .
Dimensionality reduction and feature selection in vector spaces have also been studied . Several recent works use pairwise constraints as weak supervision for dimensionality reduction , ie must link constraints [ 1 ] ( pairs of instances with the same class ) and cannot link constraints [ 12 ] ( pairs of intstances with different classes ) . Feature selection methods in vector spaces using both labeled and unlabeled instances have also been proposed [ 16 , 11 ] , which select useful features within a pre defined feature set . These methods assume that a set of candidate features is given before the feature selection . However , conventional semi supervised feature selection approaches cannot be directly applied to graph data , because it is usually infeasible to generate all the subgraph features of a graph dataset before feature selection . The number of subgraphs is usually too large to be fully generated , since it grows exponentially with the graph size . Instead , our proposed semi supervised feature selection for graph data works in a progressive way : the semi supervised feature selection is integrated to the subgraph feature generation , which can skip most of the bad subgraph features without even generating them . 3 . PROBLEM FORMULATION
In this section , we formulate the semi supervised feature selection problem for graph classification based on subgraph features . 3.1 Semi Supervised Feature Selection
Before presenting the semi supervised feature selection model for graph classification , we first introduce the notations that will be used throughout this paper . Let D = {G1 , ··· , Gn} denote the entire graph dataset , which consists of n graph objects , represented as connected graphs . The data set includes both labeled and unlabeled graphs . We assume that the first l graphs within D are labeled by {y1,· ·· , yl} , where yi ∈ {−1 , +1} denotes the binary class label assigned to Gi . For convenience , we also denote the labeled graph dataset by Dl = {G1,· ·· , Gl} , and the unlabeled graph dataset as Du = {Gl+1 , ··· , Gn} , D = Dl ∪ Du .
Definition 1
( Connected Graph ) . A graph is represented as G = ( V , E,L ) , where V is a set of vertices V = {v1,··· , vnv} , E ⊆ V × V is a set of edges , L is the set of symbols for the vertices and the edges . A connected graph is a graph such that there is a path between any pair of vertices .
= ( V . . ( Subgraph ) . Let G ) and G = ( V , E,L ) be connected graphs . G . is a subgraph of . ⊆ E , ( 3 ) L . ⊆ L . If G(G . G
Definition 2 . ⊆ G ) iff : ( 1 ) V . ⊆ V , ( 2 ) E . is a subgraph of G , then G is a supergraph of G
, E
.
,L .
.
In this paper , we adopt the idea of subgraph based graph classification approaches , which assume that each graph obi ,··· , xm fi ject Gi is represented as a feature vector xi = [ x1 i ] corresponding to a set of subgraph patterns {g1,·· · , gm} . Denote xk i as the binary feature associated with the subi = 1 iff gk is a subgraph of Gi ( gk ⊆ Gi ) , graph pattern gk . xk otherwise xk i = 0 .
The key issue of semi supervised feature selection for graph classification is how to find the most informative subgraph patterns from a limited number of labeled graphs and a large number of unlabeled graphs . So , in this paper , the studied research problem can be described as follow : in order to train an effective graph classifier , how to efficiently find a set of optimal subgraph features from both labeled and unlabeled graphs ?
Mining the optimal subgraph features from both labeled and unlabeled graphs is a non trivial task due to the following problems :
( P1 ) How to properly evaluate the usefulness of a set of subgraph features based upon both labeled and unlabeled graphs ?
( P2 ) How to find the optimal subgraph features within a reasonable amount of time by avoiding the exhaustive enumeration ? The subgraph feature space of graph objects is usually too large , because the number of subgraphs grows exponentially with the size of the graphs . It is infeasible to completely enumerate all the subgraph features for a given graph dataset .
In the following sections , we will first introduce the optimization framework for selecting informative subgraph features from labeled and unlabeled graphs . Next we will describe our subgraph mining strategy using the evaluation criteria derived from the optimization solution . 3.2 Optimization Framework
We first address the problem ( P1 ) discussed in Section 3.1 by defining the subgraph feature selection as an optimization problem . Our target is to find an optimal set of subgraph features from both labeled and unlabeled graphs . Formally , let us introduce the following notations :
• S = {g1 , g2,· ·· , gm} : the given set of all the subgraph features , which are used to predict class membership of graph instances . Usually there is only a subset of the subgraph features T ⊆ S relevant to the graph classification task .
• T ∗
: the optimal set of subgraph features T ∗ ⊆ S .
( 3 )
( 4 )
( 5 )
By defining a matrix W = [ Wij]n×n as if yiyj = −1 if yiyj = 1 if Gi , Gj ∈ Du otherwise
⎧⎪⎪⎪⎨ α|C| − β|M| ⎪⎪⎪⎩ 1|Du|2 0
Wij =
.
J(T ) = we can rewrite the J(T ) in Eq 2 as follow : ( DT xi − DT xj ) X ( D − W ) X fi DT ) XLX fi
= tr(DT fi fi = tr(DT .
1 2 fi i,j
2
=
( fk
Lfk )
Wij
DT ) gk∈T where tr(· ) is the trace of a matrix , D is a diagonal matrix whose entries are column sums of W , ie Dii = j Wij . L = D − W is a Laplacian matrix .
By denoting function h(gk , L ) = fk
Lfk , the optimiza fi tion in Eq 1 can be written as . maxT h(gk , L ) gk∈T T ⊆ S,|T | ≤ t
Definition 3 st ( gSemi ) . Let D = {G1,· ·· , Gn} denote a graph dataset , with first l graphs labeled as y1,··· , yl . Suppose W is a matrix defined as Eq 3 . L is a Laplacian matrix defined as L = D − W , where D is a diagonal matrix , Dii = j Wij . We define a quality criterion q called gSemi , for a subgraph feature g as fi
Lfg q(g ) = h(g , L ) = fg ( 6 ) fi ∈ {0 , 1}n is the indicator vector ,··· , f ( 1 ) ( n ) where fg = [ f ] g g g = 1 iff g ⊆ Gi ( i = 1 , 2,··· , n ) . ( i ) for subgraph feature g , f Since the Laplacian matrix L is positive semi definite , for any subgraph pattern g , q(g ) ≥ 0 .
The optimal solution to the problem in Eq 5 can be found by using gSemi to make feature selection on a set of subgraphs S . Suppose the gSemi values for all subgraphs are denoted as q(g1 ) ≥ q(g2 ) ≥ ··· ≥ q(gm ) in sorted order . Then the optimal solution to the optimization problem in Eq 5 is :
T ∗
= {gi|i ≤ t} .
( 7 )
• J(T ) : an evaluation criterion to estimate the useful ness of subgraph feature subset T .
• X : the matrix consisting binary feature vectors using S to represent the graph dataset {G1 , G2,··· , Gn} . fi ∈ {0 , 1}m×n , X = [ x1 , x2,··· , xn ] = [ f1 , f2 , ··· , fm ] where X = [ Xij ]m×n , Xij = 1 iff gi ⊆ Gj . The first l graphs are labeled as y1 , ··· , yl .
• C and M : C = {(i , j)|yiyj = −1} denotes the cannotlink pairwise constraint sets among labeled graphs . M = {(i , j)|yiyj = 1} denotes the must link pairwise constraint sets among labeled graphs .
We propose the following general optimization framework to select optimal subgraph feature set :
T ∗
= argmax
T ⊆S J(T )
|T | ≤ t , st
( 1 ) where | · | denotes the size of the feature set and t is the maximum number of feature selected . The objective function in Eq 1 has two components : the evaluation criterion J(T ) and the subgraph features of graphs S .
We assume that the optimal subgraph features set should have the following properties : ( a ) cannot link : labeled graphs in different classes should be far away from each other ; ( b ) must link : labeled graphs in the same class should be close to each other ; ( c ) separability : unlabeled graphs should be able to be separated from each other . Intuitively , ( a ) and ( b ) only consider the constraints from labeled graphs , and tend to select the most discriminative subgraph features based on the graph labels . They are similar to the LDA [ 9 ] criterion . Note ( c ) incorporates the distribution of unlabeled graphs , and tends to select the subgraph features that can separate graphs far from each other . It is similar to the PCA ’s assumption , which is expressed as the average squared distance between unlabeled samples . An opposite example for property ( c ) is : The subgraph features that are too rare or too frequent in the dataset are not useful at all , because unlabeled graphs cannot be separated from each other using these subgraph features . Similar assumptions have also been used by previous works on dimensionality reduction in vector spaces [ 16 ] . criterion J(T ) as follow :
Based upon the above properties , we derive an evaluation
J(T ) =
( DT xi − DT xj )
2
.
α 2|C| yiyj =−1 . − β 2|M| 1
+
2|Du|2 yiyj =1
( DT xi − DT xj ) .
( DT xi − DT xj )
2
Gi,Gj∈Du where DT = diag(d(T ) ) is a diagonal matrix indicating which features are selected into feature set T from S , d(T )i = I(gi ∈ T ) . α , β are two parameters , which control the weights of the three types of constraints . Different settings of α and β can refer to different scenarios , and reflect different beliefs we have for the problem . A discussion on the parameter setting will be presented analytically in Section 4.4 and empirically in Section 54
2
( 2 )
4 . gSSC
In this section , we address the problem ( P2 ) discussed in Section 3.1 by proposing an efficient method to find the optimal set of subgraphs features from a dataset with both labeled and unlabeled graphs .
The straightforward method is the exhaustive enumeration : We first enumerate all subgraph patterns in the graph dataset , and then calculate the gSemi values for all subgraph patterns . This method is usually impractical , because the number of subgraphs grows exponentially with the size of the graphs . Inspired by recent graph classification approaches , eg [ 14 ] , which put their evaluation criteria into the subgraph pattern mining process and develop constraints to prune search spaces , we take a similar approach by deriving a different constraint from both labeled and unlabeled graphs . In order to avoid the exhaustive search , we proposed a branch and bound algorithm , named gSSC , which is summarized as follow : a ) Adopt a canonical search space where all the subgraph patterns can be enumerated . b ) Search through the space , and find the optimal subgraph features by gSemi . c ) Propose an upper bound of gSemi and prune the search space . Details with these three steps will be described in the next subsections . 4.1 Subgraph Mining
In this paper , we adopted a depth first search algorithm , gSpan proposed by Yan et al[15 ] , to enumerate all subgraphs from a graph dataset . The key idea of gSpan[15 ] is that , instead of enumerating subgraphs and testing for isomorphism , they first build a lexicographic order of all the edges of a graph , and then map each graph to an unique minimum DFS code as its canonical label . The minimum DFS codes of two graphs are equivalent iff they are isomorphic . Details can be found in [ 15 ] . Based on this lexicographic order , a depth first search ( DFS ) strategy is used to efficiently search through all the subgraphs in a DFS code tree . By a depth first search through the DFS code tree ’s nodes , we can enumerate all the subgraphs of a graph in their DFS codes’ order . And the nodes with non minimum DFS codes can be directly pruned in the tree , which saves us from performing an explicit isomorphic test among the subgraphs . 4.2 Upper Bound of gSemi
By adopting gSpan ’s DFS Code Tree , we can efficiently enumerate all the subgraph patterns of a graph dataset in a canonical search space . We now derive an upper bound for the gSemi value which can be used to prune the subgraph search space . A convenient method to compute a upperbound on gSemi value is given as follow :
.
( Upper bound of gSemi ) . Given any two . ⊇ g ) . The ) ≤ . is a supergraph of g ( g ) ) is bounded by ˆq(g ) ( ie , q(g
.
. ∈ S , g ( q(g
Theorem 1 subgraphs g , g gSemi value of g ˆq(g) ) . ˆq(g ) is defined as follow : ˆq(g ) . fg
. fi ˆLfg
( 8 ) where the matrix ˆL is defined as ˆLij . max(0 , Lij ) . fg = {I(g ⊆ Gi)}n i=1 ∈ {0 , 1}n is a vector indicating which graphs in a graph dataset {G1,··· , Gn} contain the subgraph g , I(· ) is the indicator function . Suppose the gSemi value of g is q(g ) = fg Proof . fi
Lfg . ( . ) q g
= fg.fi
Lfg . =
. i,j:Gi,Gj∈G(g . )
Lij
. where G(g
) . {Gi|g the supergraph of g ( g property , we have G(g . have ˆLij ≥ Lij and ˆLij ≥ 0 . So ,
. ⊆ Gi , 1 ≤ i ≤ n} . Since g is . ⊇ g ) , according to anti monotonic ) ⊆ G(g ) . Also ˆLij . max(0 , Lij ) , we .
.
.
Lij ≤
ˆLij i,j:Gi,Gj∈G(g . ) i,j:Gi,Gj∈G(g . )
.
ˆLij = ˆq ( g ) i,j:Gi,Gj∈G(g ) . ⊇ g , q(g
.
) ≤ ˆq(g ) .
. )
( g q
=
≤
Thus , for any g
T = gSSC(D , yl , min sup , t ) Input : D : Graph data set {G1 , ··· , Gn} yl : The first l graphs’ labels , where yl = [ y1,· ·· , yl ] fi min sup : Minimum support threshold t : number of subgraph feature selected
Process :
T = ∅ , θ = 0 ;
1 2 Recursively visit the DFS Code Tree in gSpan : g = currently visited subgraph in DFS Code 3 Tree if |T | < t , then T = T ∪ {g} ;
. else if q(g ) > ming.∈T q(g ) , then ) and T = T /gmin ; . gmin = argming.∈T q(g T = T ∪ {g} and θ = q(gmin ) ; if ˆq(g ) ≥ θ and f req(g ) ≥ min sup , then
Depth first search the subtree rooted from node g ;
4 5 6 7 8 9 10 return T ;
11 Output : T :
Set of optimal subgraph features
Figure 4 : The gSSC algorithm
4.3 Pruning Search Space
.
. of g ( g
We can now utilize the upper bound to efficiently prune the DFS Code Tree with a branch and bound method . During the depth first search through the DFS Code Tree , we always maintain the temporally suboptimal gSemi value ( denoted by θ ) among all the gSemi values calculated before . If . ⊇ g ) ˆq(g ) < θ , the gSemi value of any supergraph g is no greater than θ . Thus , we can safely prune the subtree from g in the search space . If ˆq(g ) ≥ θ , we cannot prune . ⊇ g that this space since there might exist a supergraph g ) ≥ θ . q(g The algorithm gSSC is summarized in Figure 4 . We initialize a set of selected subgraphs T as an empty set . In order to speed up the mining process , we can prune the search space from gSpan by always maintaining the currently top t best subgraphs according to q . During the course of mining , whenever we reach a subgraph g with ˆq(g ) ≤ mingi∈T q(gi ) , we can prune the branches originating from g . This is be ) ≤ ˆq(g ) , cause for any supergraph g according to the bound defined in Eq 8 . As long as the resulting subgraph g can improve the gSemi value of any subgraphs gi ∈ T , it is accepted into T and the least best subgraph is dropped off from T . And then we start searching for the next subgraph in the DFS Code Tree .
. ⊇ g we have q(g
.
We further note that in our experiments among almost all datasets gSemi provides such a bound that we can even omit the support threshold min sup and still find a set of optimal subgraphs within a reasonable time cost . 4.4 Discussion
In this section we show the connection between our framework and various application scenarios of graph classification .
Parameter Setting : There are two parameters in the objective function : α and β , which represent the weights of different constraints based on both labeled and unlabeled graphs . Different settings of these parameters fit the optimization to different scenarios of graph classification :
• α )= 0 , β = 0 .
In this case , we only consider the cannot link constraints and unlabeled graph ’s separability in subgraph feature selection . No must link constraint is considered , ie labeled graphs within the same classes are not necessarily close together . α controls how much we assume labeled graphs within different classes should be far from each other . This setting of parameters is useful when there is a large diversity within graphs from the same class . For example , drug molecules that have the same toxicology activities on one animal can have very different structures . Furthermore , if α = +∞ , we only trust the cannot link constraints . This reduce the problem into a supervised feature selection task .
• α = 0 , β )= 0 . In this setting of parameter , we only consider the must link constraints and unlabeled graph ’s separability in subgraph feature selection . The larger β is , the more we trust the must link constraints in feature selection . No cannot link constraint is considered , ie labeled graphs in different classes are not necessarily far from each other .
• α = 0 , β = 0 . In this case , we don’t trust label constraints . Only unlabeled graph ’s separability is considered in subgraph feature selection . This reduce the problem into an unsupervised feature selection task for the unlabeled graph data .
• α )= 0 , β )= 0 .
In this case , we consider all constraints ( must link , cannot link , unlabeled separability ) with different weights . This setting is a typical setting for semi supervised feature selection , where we need to consider both labeled and unlabeled graphs . The smaller the values of α and β , the more we trust the separability constraints from unlabeled graphs .
5 . EXPERIMENTS
In this section , we conduct extensive experiments to examine the effectiveness and efficiency of gSSC in semi supervised feature selection for graph classification . 5.1 Experimental Setup
Data Collections : In order to evaluate the performances of our semi supervised feature selection approach for graph classification , we tested our algorithm on five real world graph classification datasets including the following tasks : ( Summarized in Table 1 )
Table 1 : Summary of experimental datasets . “ Pos % ” denotes the average percentage of postive graphs in each dataset .
Name MCF 7 NCI H23 OVCAR 8 PTC MM PTC FM
#Graph Pos % Details
27784 40460 40626 336 349
Breast Cancer Lung Cancer
8.19 5.06 5.08 Ovarian Cancer 41.0 Male Mice Toxicology 38.4
Female Mice Toxicology
1 ) Anti cancer activity prediction : The first three benchmark datasets are collect from PubChem Website1 . The task is to classify chemical compounds’ anti cancer activities on three types of cancers , ie breast , lung and ovarian . The datasets consist information on the biological activities of small molecules , containing anticancer activity records of more then 10,000 chemical compounds against the three types of cancers . Each chemical compound is represented as a graph . We collected 3 graph datasets with active and inactive labels from PubChem Website . The original datasets are unbalanced , where the active class is around 5 % . We randomly sample 500 inactive compounds and 500 active compounds from each dataset for performance evaluation .
2 ) Toxicology prediction ( PTC ) : The last two benchmark datasets are collected from PTC datasets2 [ 3 ] . The task is to classify chemical compounds’ carcinogenicity on two animal models , ie MM ( Male Mouse ) and FM ( Female Mouse ) . The datasets consist carcinogenicity records of more than 300 chemical compounds . Each chemical compound is assigned with carcinogenicity labels for these animal models . On each animal model the carcinogenicity label is one of {CE , SE , P , E , EE , IS , NE , N} . We assume {CE , SE , P} as ‘positive’ labels , and {NE , N} as ‘negative’ , which is the same setting as [ 6 , 7 ] . Each chemical compound is represented as a graph with an average of 25.7 vertices .
Comparing Methods : In order to demonstrate the effectiveness of our semi supervised features selection approach for graph classification , we compare our methods with two baseline methods , including a supervised feature selection approach and an unsupervised approach .
The compared methods are summarized as follows : • Semi Supervised ( gSSC ) : The proposed semi supervised feature selection method for graph classification . We first use gSSC to find a set of subgraph features . The parameters in gSSC are set to α = β = 1 unless otherwise specified .
• Supervised ( IG ) : We compare with a supervised feature selection method for graph classification . In this approach , a set of frequent subgraphs within labeled graphs are first mined . Then a supervised feature selection based upon Information Gain ( IG ) , an entropy based measure , is used to select a subset of discriminative features from frequent subgraphs .
• Unsupervised ( Top k ) : We also compare with an unsupervised feature selection method . In this approach , the evaluation criterion for subgraph feature selection is based upon frequency . The top k frequent subgraph features in labeled graphs are selected .
All experiments are conducted on machines with 4 GB RAM and Intel XeonTMQuad Core CPUs of 2.40 GHz . 5.2 Performances on Graph Classification
In our experiments , the labeled training graphs are randomly sampled from each datasets . All the remaining graphs 1http://pubchemncbinlmnihgov 2http://wwwpredictive toxicologyorg/ptc/ y c a r u c c A
0.55
0.54
0.53
0.52
0.51
0.50
10 y c a r u c c A
0.55
0.54
0.53
0.52
0.51
0.50
10 y c a r u c c A
0.55
0.54
0.53
0.52
0.51
0.50
10
IG Top−k gSSC
20
30
40
Number of selected features
50 a ) MCF 3 y c a r u c c A
0.56
0.55
0.54
0.53
0.52
0.51
0.50
10
IG Top−k gSSC
20
30
40
Number of selected features b)NCI H23
50 y c a r u c c A
0.54
0.53
0.52
0.51
0.50
0.49
10
IG Top−k gSSC
20
30
40
50
Number of selected features c ) OVCAR 8
0.58
0.56
0.54
0.52 y c a r u c c A
0.50
10
IG Top−k gSSC
20
30
40
Number of selected features d ) PTC MM
50
0.58
0.56
0.54
0.52 y c a r u c c A
0.50
10
20
30
40
Number of selected features e)PTC FM
Figure 5 : Classification accuracy with different number of features . ( #label=30 )
IG Top−k gSSC
50
50
IG Top−k gSSC
20
30
40
Number of selected features
50 a ) MCF 3 y c a r u c c A
0.56
0.55
0.54
0.53
0.52
0.51
0.50
10 y c a r u c c A
IG Top−k gSSC
20
30
40
Number of selected features b)NCI H23
50
0.54
0.53
0.52
0.51
0.50
0.49
10
IG Top−k gSSC
20
30
40
Number of selected features c ) OVCAR 8
50
0.58
0.56
0.54
0.52 y c a r u c c A
0.50
10
IG Top−k gSSC
20
30
40
Number of selected features d ) PTC MM
50
0.58
0.56
0.54
0.52 y c a r u c c A
0.50
10
IG Top−k gSSC
20
30
40
Number of selected features e)PTC FM
Figure 6 : Classification accuracy with different number of features . ( #label=50 )
IG Top−k gSSC
20
30
40
Number of selected features
50 a ) MCF 3 y c a r u c c A
0.56
0.55
0.54
0.53
0.52
0.51
0.50
10
IG Top−k gSSC
20
30
40
Number of selected features b)NCI H23
50 y c a r u c c A
0.54
0.53
0.52
0.51
0.50
0.49
10
IG Top−k gSSC
20
30
40
50
Number of selected features c ) OVCAR 8
0.58
0.56
0.54
0.52 y c a r u c c A
0.50
10
IG Top−k gSSC
20
30
40
Number of selected features d ) PTC MM
50
0.58
0.56
0.54
0.52 y c a r u c c A
0.50
10
IG Top−k gSSC
20
30
40
50
Number of selected features e)PTC FM
Figure 7 : Classification accuracy with different number of features . ( #label=70 ) are used as unlabeled testing graphs . The results are average of over 30 runs of randomly sampled graph dataset . After the subgraph feature sets are selected by each method , the nearest neighbor ( 1 NN ) classifier is used for classification . The result of the feature selection methods with different number of labeled training graphs are displayed in Figure 5 ( # labeled graphs =30 ) , Figure 6 ( # labeled graphs =50 ) and Figure 7 ( # labeled graphs =70 ) . We show the number of selected subgraphs t among frequent subgraphs ( min sup = 10% ) , together with classification accuracy as the evaluation metric .
In all these datasets , our semi supervised feature selection algorithm ( gSSC ) outperform the supervised approach ( IG ) . gSSC can achieve a good performances with a few labeled training graphs together with a large amount of unlabeled graphs . Although the performance of IG improves with a larger number of features , the IG cannot reach the best performance achievable by gSSC . These results support our first intuition that semi supervised feature selection methods based on gSemi can boost the performance of graph classification with large amount of unlabeled graphs .
We further observe that gSSC ’s performances are better than our second baseline Top k , ie unsupervised feature selection approaches without label information . These results support our second intuition that the gSemi evaluation criterion in gSSC can find better subgraph patterns for graph classification than unsupervised top k frequent subgraph selection approaches .
5.3 Pruning Search Space
In our second experiment , we evaluated the effectiveness of the upper bound for gSemi proposed in Section 42 In this section we compare the runtime performance of two versions of implementation for gSSC : ‘nested gSSC’ versus ‘un nested gSSC’ . The ‘nested gSSC’ denotes the proposed method using the upper bound proposed in Section 4.2 to prune the search space of subgraph enumerations ; the ‘un nested gSSC’ denotes the method without the gSemi ’s upper bound pruning , which first uses gSpan to find a set of frequent subgraphs , and then selects the optimal set of subgraphs via gSemi . We run both approaches and record the average CPU time used on feature mining and selection . The result is shown in Figure 8 .
In all these datasets , the un nested gSSC needs to explore increasingly larger subgraph search spaces as we decrease the min sup in the frequent subgraph mining . The size increases exponentially when decreasing min sup . In the MCF 7 dataset , when the min sup get too low ( min sup < 8% ) , the subgraph feature enumeration step in un nested gSSC can run out of the computer memory . However , the nested gSSC ’s running time does not increase as much , because the gSemi can help pruning the subgraph search space using both labeled and unlabeled graphs . As we can see , the min sup can go to very low value in all datasets for the “ nested gSSC ” .
Figure 9 shows the number of subgraph feature explored in the process of subgraph pattern enumeration . In all datasets ,
100
100
14
17
1
2
5
14
17
1
2
5 nested unnested gSSC gSSC 8 11 min_sup % nested unnested gSSC gSSC 8 11 min_sup %
14
17 a ) MCF 3 b ) NCI H23 c ) OVCAR 8
Figure 8 : Average CPU time for nested gSSC versus un nested gSSC with varying min sup .
100000
100000
) e s ( t s o C e m T U P C i
10 d e r o l p x E s h p a r g b u S #
10000
1000
100
10
) e s ( t s o C e m T U P C i nested unnested gSSC
1
2
5 gSSC 8
11 min_sup %
100000
10000
1000 d e r o l p x E s h p a r g b u S #
) e s ( t s o C e m T U P C i
10 d e r o l p x E s h p a r g b u S #
10000
1000 nested unnested gSSC gSSC 11
8 min_sup % nested unnested gSSC gSSC 11
8 min_sup % nested unnested gSSC gSSC 11
8 min_sup %
100
2
5
14
17
100
2
5
14
17
100
2
5
14
17 a ) MCF 3 b ) NCI H23 c ) OVCAR 8
Figure 9 : Average number subgraph patterns explored during mining for nested gSSC versus un nested gSSC with varying min sup .
0.54
0.53
0.52
0.51 y c a r u c c A
0.5 10000
0.58
0.56
0.54
0.52 y c a r u c c A
10000
1000
1000
100
10
1
0.1
α
0.01
0.001
0.001
0.1
0.01
1
β
10000
1000
100
10 a ) MCF 3 y c a r u c c A
0.55
0.54
0.53
0.52
0.51 10000
10
100
α
1
0.1
1
0.1
100
10
β
0.01
0.001
0.001
0.01 b ) NCI H23
1000
100
10
10000
1000
10000
1000
100
10
1
0.1
0.01
α
0.001
0.001
0.1
0.01
1
β c ) OVCAR 8
Figure 10 : Classification accuracy of gSSC with different α and β . ( #label=50 ) we observe that the number of searched subgraph patterns in nested gSSC is much smaller than that of un nested gSSC . In our experiments , we further noticed that on most datasets , nested gSSC provides such a strong bound that we may even allow nested gSSC to omit the minimum support threshold min sup and still receive an optimal set of subgraph features within a reasonable time . 5.4 Parameter Settings
In our model we can take different weights on constraints from labeled graphs and unlabeled graphs . If we use different setting for the two parameters α and β , we can take the feature selection with different weights for the three types of constraints : must link , cannot link and unlabeled sep arability . α represents how much we weight the cannotlink constraints , and β denotes how much we weight the must link constraints . The larger α is , the further away the graphs with different classes are separated from each other . The larger β is , the closer the graphs with the same classes are from each other . We test α and β with values among {0.001 , 0.01 , ··· 10000} separately . The result in Figure 10 shows that the performance of our model using α with large values and β with small values is often better than other settings . The reason is that in these real world graph classification tasks , graphs in the same class are not always similar with each other , actually graphs can be very different within a same class .
In Figure 10 , we find the best parameter setting for MCF
[ 7 ] T . Kudo , E . Maeda , and Y . Matsumoto . An application of boosting to graph classification . In L . K . Saul , Y . Weiss , and L . Bottou , editors , Advances in Neural Information Processing Systems 17 , pages 729–736 . Cambridge , MA : MIT Press , 2005 .
[ 8 ] M . Kuramochi and G . Karypis . Frequent subgraph discovery . In Proceedings of the 1st International Conference on Data Mining , pages 313–320 , San Jose , CA , 2001 .
[ 9 ] K . V . Mardia , J . T . Kent , and J . M . Bibby .
Multivariate Analysis . Academic Press , San Diego , CA , 1980 .
[ 10 ] S . Nijssen and J . Kok . A quickstart in frequent structure mining can make a difference . In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 647–652 , Seattle , WA , 2004 .
[ 11 ] J . Ren , Z . Qiu , W . Fan , H . Cheng , and P . S . Yu .
Forword semi supervised feature selection . In Proceedings of the 12th Pacific Asia Conference on Knowledge Discovery and Data Mining , pages 970–976 , Osaka , Japan , 2008 .
[ 12 ] W . Tang and S . Zhong . Pairwise constraints guided dimensionality reduction . In SIAM International Conference on Data Mining Workshop on Feature Selection for Data Mining , Bethesda , MD , 2006 .
[ 13 ] M . Thoma , H . Cheng , A . Gretton , J . Han , H . Kriegel , A . Smola , L . Song , P . Yu , X . Yan , and K . Borgwardt . Near optimal supervised feature selection among frequent subgraphs . In Proceedings of the SIAM International Conference on Data Mining , pages 1075–1086 , Sparks , Nevada , 2009 .
[ 14 ] X . Yan , H . Cheng , J . Han , and P . Yu . Mining significant graph patterns by leap search . In Proceedings of the ACM SIGMOD International Conference on Management of Data , pages 433–444 , Vancouver , BC , 2008 .
[ 15 ] X . Yan and J . Han . gSpan : Graph based substructure pattern mining . In Proceedings of the 2nd International Conference on Data Mining , pages 721–724 , Maebashi City , Japan , 2002 .
[ 16 ] Z . Zhao and H . Liu . Semi supervised feature selection via spectral analysis . In Proceedings of the SIAM International Conference on Data Mining , pages 641–646 , Minneapolis , MN , 2007 .
3 dataset is α = 1 , β = 0.1 ( accuracy = 0.526 ) , and with our default parameter setting ( α = β = 1 ) the accuracy is 0523 For NCI H23 dataset , the best parameter setting is α = 1 , β = 0.1 ( accuracy= 0.556 ) , and the accuracy with default setting is 0553 For OVCAR 8 dataset , the best parameter setting is α = 1 , β = 0.1 ( accuracy= 0.539 ) , and the accuracy with default setting is 0530 Generally , we can see that the performance of gSSC with default setting ( α = β = 1 ) is pretty good . If we try to optimize the selection of α and β value , the accuracy improvement relative the two base line schemes will be even bigger .
6 . CONCLUSION
In this paper , we study the problem of semi supervised feature selection for graph classification . It is significantly more challenging than the conventional setting of supervised feature selection in graph data because of the lack of labeled training graphs . To address this challenge , we propose a feature evaluation criterion , named gSemi , to evaluate subgraph features with both labeled and unlabeled graphs , and derive an upper bound for gSemi to prune the subgraph search space . Then we propose a branch and bound algorithm to efficiently find a set of optimal subgraph feature which is useful for graph classification . Empirical studies on real world tasks show that our semi supervised feature selection approach for graph classification outperforms supervised and unsupervised approaches and is very efficient by pruning the subgraph search space using both labeled and unlabeled graphs .
7 . ACKNOWLEDGMENTS
This work is supported in part by NSF through grant IIS
0905215 .
8 . REFERENCES [ 1 ] A . Bar Hillel , T . Hertz , N . Shental , and D . Weinshall .
Learning a mahalanobis metric from equivalence constraints . Journal of Machine Learning Research , 6:937–965 , 2005 .
[ 2 ] C . Borgelt and M . Berthold . Mining molecular fragments : Finding relevant substructures of molecules . In Proceedings of the 2nd International Conference on Data Mining , pages 211–218 , Maebashi City , Japan , 2002 .
[ 3 ] C . Helma , R . King , S . Kramer , and A . Srinivasan . The predictive toxicology challenge 2000 2001 . Bioinformatics , 17(1):107–108 , 2001 .
[ 4 ] J . Huan , W . Wang , and J . Prins . Efficient mining of frequent subgraph in the presence of isomorphism . In Proceedings of the 3rd International Conference on Data Mining , pages 549–552 , Melbourne , FL , 2003 .
[ 5 ] A . Inokuchi , T . Washio , and H . Motoda . An apriori based algorithm for mining frequent substructures from graph data . In Proceedings of the 4th European Conference on Principles of Data Mining and Knowledge Discovery , pages 13–23 , Lyon , France , 2000 .
[ 6 ] H . Kashima , K . Tsuda , and A . Inokuchi . Marginalized kernels between labeled graphs . In Proceedings of the 20th International Conference on Machine Learning , pages 321–328 , Washington , DC , 2003 .
