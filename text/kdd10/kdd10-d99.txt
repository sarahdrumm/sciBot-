Mass Estimation and Its Applications
Kai Ming Ting , Guang Tong Zhou⁄ , Fei Tony Liu , James Tan Swee Chuany
Gippsland School of Information Technology
Monash University
Australia
{kaimingting,tonyliu}@monashedu , zhouguangtong@gmail.com , jamestansc@unisimedusg
ABSTRACT This paper introduces mass estimation|a base modelling mechanism in data mining . It provides the theoretical basis of mass and an e–cient method to estimate mass . We show that it solves problems very efiectively in tasks such as information retrieval , regression and anomaly detection . The models , which use mass in these three tasks , perform at least as good as and often better than a total of eight state of theart methods in terms of task speciflc performance measures . In addition , mass estimation has constant time and space complexities .
Categories and Subject Descriptors I.2 [ Artiflcial Intelligence ] : Miscellaneous ; I.5 [ Pattern Recognition ] : General
General Terms Algorithms , Theory
1 .
INTRODUCTION
‘Estimation of densities is a universal problem of statistics ( knowing the densities one can solve various problems.)’ | VN Vapnik [ 16 ] .
Density estimation has been the base modelling mechanism used in many techniques designed for tasks such as classiflcation , clustering , anomaly detection and information retrieval . For example in classiflcation , density estimation is employed to estimate class conditional density function ( or likelihood function ) p(xjj ) or posterior probability ⁄Guang Tong is a student at School of Computer Science and Technology , Shandong University , China . He was visiting Monash University , supported by a scholarship from Shandong University , when this research was conducted . yJames is now at SIM University , Singapore . p(jjx)|the principal function underlying many classiflcation methods eg , mixture models , Bayesian networks , and Naive Bayes . Examples of density estimation include kernel density estimation , k nearest neighbours density estimation , maximum likelihood procedures or Bayesian methods .
We shows in this paper that a new base modelling mechanism called mass estimation possesses difierent properties from those ofiered by density estimation :
† A mass distribution stipulates an ordering from core points to fringe points in a data cloud . In addition , this ordering accentuates the fringe points with a concave function|fringe points have markedly smaller mass than points close to the core points . These are the fundamental properties required for many tasks , including anomaly detection and information retrieval . In contrast , density estimation is not designed to provide an ordering .
† Mass estimation is more e–cient than density estimation because mass is computed by simple counting and it requires only a small sample through an ensemble approach . Density estimation ( often used to estimate p(xjj ) and p(jjx ) ) require a large sample size in order to have a good estimation and is computationally expensive in terms of time and space complexities [ 7 ] .
† Mass can be interpreted as a measure of relevance with respect to the concept underlying the data , ie , core points indicate that they are highly relevant and fringe points indicates that they are less relevance . We demonstrate in this paper that a relevance feature space consists of a vector of masses estimated from data is very efiective for three data mining tasks : information retrieval , regression and anomaly detection .
Mass estimation has two advantages in relation to e–cacy and e–ciency . First , the concavity property mentioned above ensures that fringe points are ‘stretched’ to be farther from the core points in a mass space|making it easier to separate fringe points from those points close to core points . This property , otherwise hidden , can then be exploited by a data mining algorithm to achieve a better result for the intended task than the one without it . We show the e–cacy of mass in improving the task speciflc performance of four existing state of the art algorithms in information retrieval and regression tasks in this paper . The signiflcant improvements are achieved through a simple mapping from the original space to a mass space using the mass estimation mechanism introduced here .
Table 1 : Symbols and notations . A real domain of u dimensions An one dimensional instance in R An instance in Ru A data set of x , where jDj = n A subset of D , where jDj = ˆ An instance in Rt A data set of z Level of mass distribution Number of mass distributions in ^mass( : )
Ru x x D D z D0 h t mi( : ) Mass base function deflned using binary split si mass( : ) Mass function which returns a real value ^mass( : ) Mass function which returns a vector of t values
Second , mass estimation ofiers to solve a problem more e–ciently using the ordering derived from data directly| without distance or related expensive calculation|when the problem demands ranking . An example of ine–cient application is in anomaly detection tasks where many methods have employed distance or density|a computationally expensive process|to provide the required ranking . An existing state of the art density based anomaly detector LOF [ 4 ] ( which has quadratic time complexity ) cannot complete a job involving half a million data points in less than two weeks ; yet the mass based anomaly detector we have introduced here completes it in less than 40 seconds! Section 4.3 provides the detail of this example .
Section 2 introduces mass and mass estimation , together with their theoretical properties . We also describe an e–cient method to estimate mass in practice . Section 3 describes a mass based formalism which serves as a basis of applying mass to difierent data mining tasks . We present a realisation of tasks : information retrieval , regression and anomaly detection , and report the empirical evaluation results in Section 4 . The relation to kernel density estimation is given in Section 5 . We provide related work , the conclusions and future work in the last two sections . formalism in three difierent the
2 . MASS AND MASS ESTIMATION
Data mass or mass is deflned as the number of points in a region ; and two groups of data can have the same mass regardless of the characteristics of the regions ( eg , density , shape or volume ) . Mass in a given region is deflned by a rectangular function which has the same value for the entire region in which the mass is measured .
Identifying a region occupied by a group of data in itself is a clustering problem , but mass can nonetheless be estimated without clustering . We show in this section that mass can be estimated in a way similar to kernel density estimation without involving clustering at all by using a function similar to a kernel function .
Note that mass is not a probability mass function , and it does not provide probability , as probability density function does through integration .
The detail of mass estimation is provided in the following two subsections . In Section 2.1 , we show how to estimate a mass distribution for a given data set , and the theoretical properties of mass estimation . Section 2.2 describes an approximation to the theoretical mass estimation which works more e–ciently in practice . This paper focuses on one dimensional mass distribution only . The symbols and notations used are provided in Table 1 . 2.1 Mass distribution estimation
We flrst show level 1 mass distribution estimation in Section 211 We then generalise the treatment for high level mass estimation in Section 212
211 Level 1 mass distribution estimation
Here , we employ a binary split to divide the data set into two separate regions and compute the mass in each region . The mass distribution at point x is estimated to be the sum of all ‘weighted’ masses from regions occupied by x , as a result of n ¡ 1 binary splits for a set of data of size n .
Let x1 < x2 < ¢ ¢ ¢ < xn¡1 < xn on the real line1 , xi 2 R and n > 1 . Let si be the binary split between xi and xi+1 , yielding two non empty regions having two masses mL i and mR i .
Deflnition 1 . Mass base function : mi(x ) as a result of si , is deflned as mi(x ) = ‰ mL i = n ¡ mR i mR i
Note that mL i = i . if x is on the left of si if x is on the right of si
Deflnition 2 . Mass distribution : mass(xa ) for a point xa 2 fx1 ; x2 ; ¢ ¢ ¢ ; xn¡1 ; xng is deflned as a summation of a series of mass base function mi(x ) weighted by p(si ) over n ¡ 1 splits as follows . mass(xa ) =
=
= n¡1
Xi=1 Xi=a n¡1 n¡1
Xi=a mi(xa)p(si ) mL i p(si ) + a¡1
Xj=1 mR j p(sj ) ip(si ) + a¡1
Xj=1
( n ¡ j)p(sj )
( 1 ) p(si ) is the probability of selecting si . Note that we have deflned Pr i=q f ( i ) = 0 , when r < q for any function f .
Example . For an example of flve points x1 < x2 < x3 < x4 < x5 , Figure 1 shows the resultant mi(x ) due to each of the four binary splits s1 ; s2 ; s3 ; s4 ; and their associated masses over four splits are given below : mass(x1 ) = 1p(s1 ) + 2p(s2 ) + 3p(s3 ) + 4p(s4 ) mass(x2 ) = 4p(s1 ) + 2p(s2 ) + 3p(s3 ) + 4p(s4 ) mass(x3 ) = 4p(s1 ) + 3p(s2 ) + 3p(s3 ) + 4p(s4 ) mass(x4 ) = 4p(s1 ) + 3p(s2 ) + 2p(s3 ) + 4p(s4 ) mass(x5 ) = 4p(s1 ) + 3p(s2 ) + 2p(s3 ) + 1p(s4 )
For a given data set , p(si ) can be estimated on the real line as p(si ) = ( xi+1 ¡ xi)=(xn ¡ x1 ) > 0 , as a result of random selection of splits based on a uniform distribution2 . For a point x =2 fx1 ; x2 ; ¢ ¢ ¢ ; xn¡1 ; xng , mass(x ) is deflned as an interpolation between two masses of adjacent points xi and xi+1 , where xi < x < xi+1 . 1In data having a pocket of points of the same value , an arbitrary order can be ‘forced’ by adding multiples of an insigniflcant small value † to each point of the pocket , without changing the general distribution . 2The estimated mass(x ) values can be calibrated to a flnite data range ¢ by multiplying a factor ( xn ¡ x1)=¢ .
L=1 m 1
R=4 m 1
L=4 m 4
R=1 m 4
4
3
2
1 s 1 x 2 x 1
4
3
2
1
L=2 m 2
R=3 m 2 s 2
4
3
2
1
L=3 m 3
R=2 m 3 s 3
4
3
2
1 x 3 x x 4 4 ( a ) mi(x ) due to s1 ( d ) mi(x ) due to s4 Figure 1 : Examples of mass base function mi(x ) due to each of the four binary splits : s1 ; s2 ; s3 ; s4 . x 4 ( b ) mi(x ) due to s2 x 4 ( c ) mi(x ) due to s3 x 1 x 1 x 2 x 1 x 2 x 2 x 3 x 5 x 3 x 5 x 5 s 4 x 5 x 3
Theorem 1 . mass(xa ) is the maximum at a = n=2 for any density distribution of fx1 ; ¢ ¢ ¢ ; xng ; and the points xa , where x1 < x2 < ¢ ¢ ¢ < xn¡1 < xn on the real line , can be ordered based on mass as follows . mass(xa ) < mass(xa+1 ) ; a < n=2 mass(xa ) > mass(xa+1 ) ; a > n=2
Proof . The difierence in mass between two subsequent points xa and xa+1 difiers in only one term , ie , the mass for p(sa ) only ; and 8i 6= a , the terms for p(si ) have the same mass . mass(xa ) ¡ mass(xa+1 )
=
=
= j=1 ( n ¡ j)p(sj ) i=a ip(si ) +Pa¡1 Pn¡1 i=(a+1 ) ip(si ) ¡Pa ¡Pn¡1 ap(sa ) ¡ ( n ¡ a)p(sa ) j=1(n ¡ j)p(sj )
( 2a ¡ n)p(sa )
( 2 )
Thus , sign(mass(xa ) ¡ mass(xa+1 ) ) = 8< :
⁄ negative 0 positive if a < n=2 if a = n=2 if a > n=2
The point xn=2 can be regarded as the median . Note that the number of points with the maximum mass depends on whether n is odd or even : When n is an odd integer , only one point has the maximum mass at xmedian , where median = dn=2e ; when n is an even integer , two points have the maximum mass at a = n=2 and a = 1 + n=2 .
Theorem 2 . mass(xa ) is a concave function deflned wrt fx1 ; x2 ; : : : ; xng , when p(si ) = ( xi+1 ¡ xi)=(xn ¡ x1 ) .
Proof . We only need to show that the gradient of mass(xa ) is non increasing , ie , g(xa ) > g(xa+1 ) for each a .
Let g(xa ) the gradient between xa and xa+1 , and from
( 2 ) : g(xa ) = mass(xa+1 ) ¡ mass(xa ) xa+1 ¡ xa
= n ¡ 2a xn ¡ x1
The result follows : g(xa ) > g(xa+1 ) for a 2 f1 ; 2 ; : : : ; n ¡ 1g .
⁄
Corollary 1 . A mass distribution estimated using binary splits stipulates an ordering , based on mass , of the points in a data cloud from xn=2 ( with the maximum mass ) to the fringe points ( with the minimum mass at either side of xn=2 ) , irrespective of the density distribution including uniform density distribution .
Corollary 2 . The concavity of mass distribution stipulates that fringe points have markedly smaller mass than points close to xn=2 .
The implication from Corollary 2 is that fringe points are ‘stretched’ to be farther away from the median in a mass space than in the original space|making it easier to separate fringe points from those points close to the median . ( The mass space is mapped from the original space through mass(x) . ) This property , otherwise hidden , can then be exploited by a data mining algorithm to achieve a better result for the intended task than the one without it . We will show that this simple mapping signiflcantly improves the performance of four existing algorithms in information retrieval and regression tasks in Sections 4.1 and 42
Equation ( 1 ) is su–cient to provide a mass distribution corresponds to a unimodal density function or a uniform density function . To better estimate multi modal distributions , a high level mass distribution is required . This is provided in the following .
212 Level h mass distribution estimation
Deflnition 3 . Level h mass distribution for a point xa 2 fx1 ; : : : ; xng , where h < n , is expressed as n¡1 mass(xa ; h ) =
= massi(xa ; h 1)p(si ) massL i ( xa ; h 1)p(si ) + massR j ( xa ; h 1)p(sj )
( 3 ) n¡1
Xi=1 Xi=a Xj=1 a¡1
Here a high level mass distribution is computed recursively by using the mass distributions obtained at lower levels . A binary split si in a level h(>1 ) mass distribution produces two level (h 1 ) mass distributions : ( a ) massL i ( x ; h 1)|the mass distribution on the left of split si which is deflned using fx1 ; : : : ; xig ; and ( b ) massR i ( x ; h 1)|the mass distribution on the right which is deflned using fxi+1 ; : : : ; xng . Equation ( 1 ) is the mass distribution at level 1 .
Figure 2 shows part of the intermediate process in calculating massL i ( x ; h = 1 ) for two example splits si=7 and si=11 in order to obtain mass(x ; h = 2 ) . i ( x ; h = 1 ) and massR
Using the same analysis in the proof for Theorem 1 , the above equation can be re expressed as : s s a m
16
12
8
4
0 mass h=1 mass h=2 mass h=3 density
0
0.2
0.4
0.6
0.8
1
( a ) unif orm
8
6 f d p
4 s s a m
2
0
12
10
8
6
4
2
0
12
10
8
6
4
2
0 f d p s s a m
15
10
5
0 mass h=1 mass h=2 mass h=3 density
0
0.5
1
( b ) trimodal
7.5 mass h=1 mass h=2 mass h=3
5 f d p
2.5 density
0
0.2
0
0.8
1
0.4 0.6 ( c ) skew
Figure 3 : Examples of level h mass distribution for h = 1 ; 2 ; 3 and density distribution from kernel density estimation : Gaussian kernel with bandwidth= 0:1 . All three data sets have 20 points each .
9
8
7
6
5
4
L(x,h=1 ) mass i
R(x,h=1 ) mass i s i=7
9
8
7
6
5
4 x
L(x,h=1 ) mass i
R(x,h=1 ) mass i s i=11 x
Figure 2 : Two examples of massL i ( x ; h = 1 ) and massR i ( x ; h = 1 ) due to si=7 and si=11 in the process to get mass(x ; h = 2 ) from a data set of 20 points with uniform density distribution . The resultant mass(x ; h = 2 ) is shown in Figure 3(a ) . mass(xa+1 ; h ) = mass(xa ; h)+
‰ [ massR
( n ¡ 2a)p(sa ) ; a ( xa ; h 1 ) ¡ massL a ( xa ; h 1)]p(sa ) ; h > 1 h = 1
( 4 )
As a result , only the mass for the flrst point x1 needs to be computed using Equation ( 3 ) . Note that it is more e–cient to compute the mass distribution from the above equation which has time complexity O(nh+1 ) ; the computation using Equation ( 3 ) has O(nh+2 ) .
Deflnition 4 . A level h mass distribution stipulates an ordering of the points in a data cloud from fi core points to the fringe points . The fi core point(s ) of a data cloud have the highest mass value within fi distance from the core point(s ) . A small fi deflnes local core point(s ) ; and a large fi , which covers the entire value range for x , deflnes global core point(s ) .
Examples of level h mass estimation in comparison with kernel density estimation are provided in Figure 3 . Note that h = 1 mass estimation treats the entire data as a group , and it produces a concave function . As a result , an h = 1 mass estimation always has its global core point(s ) at the median , regardless of the underlying density distribution|see three examples of h = 1 mass estimation in Figure 3 .
For h > 1 mass distribution , though there is no guarantee for a single concave function for the entire data set , each cluster within the data cloud still exhibits a concave function and it becomes more distinct ( as a concave function ) as h increases . This is shown in Figure 3(b ) which has a trimodal density distribution . Notice that the h > 1 mass distributions have three fi core points for some fi , eg , 02
Traditionally , one can determine the core ness or the fringeness of a non uniformly distributed data to some degree by s s a m
4
3
2
1
0 x 1 x 2 x 3 x 4 x 5
( a ) k n a r s n a m r a e p S t i n e c i f f e o c n o i t l a e r r o c
1
0.8
0.6
1
500
Gaussian COREL
100
200
300
400 c , number of mass distributions
( b )
Figure 4 : ( a ) An example of practical mass distribution mass(x ; hjD ) for 5 points , assuming a rectangular function for each point . ( b ) Correlation between the orderings provided by mass(x ; 1 ) and mass(x ; 1 ) for two data sets : one dimensional Gaussian density distribution and the COREL data set used in Section 4.1 ( whose result is averaged over 67 dimensions ) . using density or distance ( but not in uniform density distribution . ) Mass allows one to do that in any distributions without density or distance calculation|the key computational expense in all methods that employ them . For example in Figure 3(c ) which has a skew density distribution , the distinction between near fringe points and far fringe points are less obvious using density , unless distances are computed to reveal the difierence . In contrast , mass distribution depicts the relative distance from xmedian using the fringe points’ mass values , without further calculation .
This section has described properties of mass distribution from a theoretical perspective . Though it is possible to estimate mass distribution using Equations ( 1 ) and ( 3 ) , they are limited by its high computational cost . We suggest a practical mass estimation method in the next section . We use the term ‘mass estimation’ and ‘mass distribution estimation’ interchangeably hereafter . 2.2 Practical mass estimation
Here we devise an approximation to Equation ( 3 ) using random subsamples from the given data set .
Deflnition 5 . mass(x ; hjD ) is the approximate mass distribution for a point x 2 R , deflned wrt D = fx1 ; : : : ; xˆg , where D is a random subset of the given data set D , and ˆ ¿ jDj , h < ˆ .
Assume a rectangular function for each point x 2 D ( as shown in Figure 4(a) ) , mass(x ; hjD ) is implemented using a lookup table with each rectangle function covers a range ( xi¡1 + xi)=2 • x < ( xi+1 + xi)=2 for each point xi 2 D having the same mass(xi ; hjD ) value . The range for each of the two end points is set to have equal length on either side of the point . In addition , a number of mass distributions
¢ needs to be constructed from difierent samples in order to have a good approximation , that is , mass(x ; h ) =
1 c c
Xk=1 mass(x ; hjDk )
( 5 )
The computation of mass(x ; h ) using the given data set D costs O(jDjh+1 ) ; whereas mass(x ; h ) costs O(cˆh+1 ) .
Only relative , not absolute , mass is required to provide an ordering between instances . Because the relative mass is wrt median and median is a robust estimator [ 1]|that is why small subsamples produce a good order estimator .
Figure 4(b ) shows the correlation ( in terms of Spearman ’s rank correlation coe–cient ) between the orderings provided by mass(x ; 1 ) using the entire data set and mass(x ; 1 ) using ˆ = 8 in two data sets , each having 10000 data points . They achieve very high correlations when c ‚ 100 .
The ability to use a small sample , rather than a large sample , is a key characteristic of mass estimation . We show in this paper that mass(x ; hjD ) can be employed very efiectively for three difierent tasks : information retrieval , regression and anomaly detection , through a mass based formalism to be described in the next section . Although the mass estimation is designed for one dimension only , we show that it can be employed to solve multi dimensional problems .
3 . MASS BASED FORMALISM
Let xi = [ x1 i ; : : : ; xu i ] ; zi 2 D0 . The proposed formalism consists of three components : i ] ; xi 2 D ; and zi = [ z1 i ; : : : ; zt
C1 The flrst component constructs a number of mass distributions . A mass distribution mass(xd ; hjD ) for dimension d is obtained using our proposed mass estimation , as given in Deflnition 5 . A total number of t mass distributions is generated which forms ^mass(x ) ! Rt , where t ( cid:192 ) u . This procedure is given in Algorithm 1 .
C2 The second component maps the data set D in the original space of u dimensions into a new data set D 0 of t dimensions using ^mass(x ) = z . This procedure is described in Algorithm 2 .
C3 The third component employs a decision rule to determine the flnal outcome for the task at hand . It is a task speciflc decision function applied to z in the new feature space .
Algorithm 1 : Mass Estimation(D ; ˆ ; h ; t ) Inputs : D input data ; ˆ data size for Dk ; h level of mass distribution ; t number of mass distributions . Output : ^mass(x ) ! Rt a function consists of t mass distributions , mass(xd ; hjDk ) . 1 : for k = 1 to t do 2 : Dk ˆ a random subset of size ˆ from D ; 3 : 4 : 5 : end for d ˆ a randomly selected dimension from f 1 , . . . ,u g ; Build mass(xd ; hjDk ) ;
The formalism becomes a blueprint for difierent tasks . Components C1 and C3 are mandatory in the formalism , but component C2 is optional , depending on the task .
For information retrieval and regression , the task speciflc C3 procedure is simply using an existing algorithm for the task except that the process is carried out in the new mapped mass space , instead of the original space . This procedure is given in Algorithm 3 . The task speciflc C3 procedure for anomaly detection is shown in steps 2 5 in Algorithm 4 . Note that anomaly detection requires C1 and C3 only ; whereas the other two tasks require all three components .
Algorithm 2 : Mass Mapping(D;^mass ) Inputs : D input data ; ^mass a function consists of t mass distributions , mass(xd ; hjD ) . Output : D0 a set of mapped instances zi in t dimensions . 1 : for i = 1 to jDj do 2 : 3 : end for zi ˆ ^mass(xi ) ;
Algorithm 3 : Perform task in MassSpace(D ; ˆ ; h ; t ) Inputs : D input data ; ˆ data size for D ; h level of mass distribution ; t number of mass distributions . Output : Task speciflc model . 1 : ^mass( : ) ˆ Mass Estimation(D ; ˆ ; h ; t ) ; 2 : D0 ˆ Mass Mapping(D;^mass ) ; 3 : Perform task ( information retrieval or regression ) in the mapped mass space using D 0 ;
Algorithm 4 for Anomaly Detection : MassAD(D ; ˆ ; h ; t ) Inputs : D input data ; ˆ data size for D ; h level of mass distribution ; t number of mass distributions . Output : Ranked instances in D . 1 : ^mass( : ) ˆ Mass Estimation(D ; ˆ ; h ; t ) ; 2 : for i = 1 to jDj do 3 : mi ˆ Average of t masses from ^mass(xi ) ; 4 : end for 5 : Rank instances in D based on mi with low mass denotes anomalies and high mass denotes normal points ;
4 . EXPERIMENTS
We evaluate the performance of MassSpace and MassAD for three tasks in the following three subsections . In information retrieval and regression tasks , the mass estimation uses ˆ = 8 and t = 1000 . These settings are obtained by examining the rank correlation as shown in Figure 4(b)|having a high rank correlation between mass(x ; 1 ) and mass(x ; 1 ) . Note that this is done before any method is applied and no further flne tuning . In anomaly detection tasks , ˆ = 256 and t = 100 are used so that they are comparable to those used in a benchmark method for a fair comparison . h = 1 is used in all tasks , unless stated otherwise . All the experiments are run in Matlab and conducted on a Pentium 4 machine with an AMD Opteron machine with a 1:8 GHz processor and 4 GB memory . The performance of each method is measured in terms of task speciflc performance measure and runtime . Paired t tests at 5 % signiflcance level are conducted to examine whether the difierence in performance is signiflcant between two algorithms under comparison .
Note that we treat information retrieval and anomaly detection as unsupervised learning tasks . Classes/labels in the original data are used as ground truth for evaluation of performance only ; they are not used in building mass distributions . In regression , only the training set is used to build mass distributions in step 1 of Algorithm 3 ; the mapping in step 2 is conducted for both the training and testing sets .
Table 2 : CBIR results ( the higher the better for BEP . )
One query Round 1 Round 2 Round 3 Round 4 Round 5
MRBIR0 11.52 15.14 16.81 17.94 18.74 19.39
MRBIR 9.69 12.72 13.90 14.75 15.33 15.71
BEP ( £10¡2 ) Qsim0 Qsim 7.78 10.31 10.59 15.39 11.81 17.46 18.46 12.59 13.16 19.18 19.62 13.55
Processing time ( second )
InstR0 10.31 13.45 15.07 16.15 16.96 17.62
InstR 7.78 9.40 9.99 10.36 10.78 11.05
MRBIR0 1.980 2.499 2.501 2.499 2.501 2.499
MRBIR 1.111 2.155 2.155 2.155 2.155 2.155
Qsim0 0.410 0.588 0.646 0.737 0.862 1.016
Qsim 0.034 0.078 0.139 0.227 0.355 0.516
InstR0 0.410 0.558 0.559 0.560 0.561 0.562
InstR 0.034 0.046 0.047 0.048 0.049 0.050
Table 3 : Regression results ( the smaller the better for MSE ; the larger the better for SCC . ) data
MSE ( £10¡2 )
SCC ( £10¡2 ) size 9822 4898 2178 1599 1030 u 85 11 3 11 8
SVR0 5.58 1.21 2.86 1.62 0.33
SVR W/D/L 17/0/3 5.62 20/0/0 1.36 2.92 18/0/2 11/0/9 1.62 0.57 20/0/0
SVR0 2.12 45.18 0.84 38.20 92.62
SVR W/D/L 18/0/2 1.07 20/0/0 38.60 0.31 14/0/6 13/0/7 37.76 87.17 20/0/0 tic wine white quake wine red concrete
SVR
Processing time SVR0 63.61 17.30 3.18 2.00 1.08
29.85 7.24 1.09 0.76 0.44
Factor increase dimension time 12 2.1 91 2.4 2.9 333 91 2.6 2.5 125
4.1 Content Based Image Retrieval
We use a Content Based Image Retrieval ( CBIR ) task as an example of information retrieval . The MassSpace approach is compared with three state of the art CBIR methods that deal with relevance feedbacks : a manifold based method MRBIR [ 9 ] , and two recent techniques for improving similarity calculation , ie , Qsim [ 19 ] and InstRank [ 8 ] ; and we employ the Euclidean distance to measure the similarity between instances in these two methods . The default parameter settings are used for all these methods . Because the same CBIR method is employed in the mapped space in the MassSpace approach , we denote them as MRBIR0 , Qsim0 and InstRank0 for those employ MRBIR , Qsim and InstRank , respectively .
Our experiments are conducted using the COREL image database [ 18 ] of 10000 images , which contains 100 categories and each category has 100 images . Each image is represented by a 67 dimensional feature vector , which consists of 11 shape , 24 texture and 32 color features . To test the performance , we randomly select 5 images from each category to serve as the initial queries . For a query , the images within the same category are regarded as relevant and the rest are irrelevant . For each query , we continue to perform 5 rounds of relevance feedback . In each round , 2 positive and 2 negative feedbacks are provided . This relevance feedback process is also repeated 5 times , each up to 5 feedback rounds . Finally , the average results with one query and in difierent feedback rounds are recorded . The retrieval performance is measured in terms of Break Even Point ( BEP ) [ 19 , 18 ] of the precision recall curve . The online processing time reported is the time required in each method for a query plus the stated feedback rounds . The reported result is an average over 5£100 runs for query only ; and an average over 5 £ 100 £ 5 runs for query plus feedbacks . The o†ine costs of constructing the mass distributions and the mapping of 10000 images are 2.87 and 1.25 seconds , respectively .
The results are presented in Table 2 where the best performance at each round has been boldfaced . The results are grouped in pairs for ease of comparison .
The BEP results clearly show that the MassSpace approach achieves a better retrieval performance than that using the original space in all three methods MRBIR , Qsim and InstR , regardless it is with one query only or in relevance feedbacks . Paired t tests at 5 % signiflcance level also indicate that the MassSpace approach signiflcantly outperforms each of the three methods in all experiments , without exception . These results show that the mass space provides useful additional information that is hidden in the original space .
The processing time for each of the three methods in the mass space is expected to be longer than that in the original space because the number of dimensions in the mass space is signiflcantly higher than those in the original space , where t = 1000 and u = 67 .
Figure 5(a ) shows an example of performance for InstR0| BEP increases as t increases until it reaches a plateau at some t value ; and the processing time for InstR0 is linear wrt the number of dimensions of the mass space , t .
4.2 Regression
In this experiment , we compare SVR0 with SVR|support vector regression [ 16 ] that employs the mapped mass space versus that employs the original space . SVR is the † SVR algorithm with RBF kernel , implemented by LIBSVM [ 6 ] . SVR is chosen here because it is one of the top performing regression models .
We utilize flve benchmark data sets including four selected from UCI repository [ 2 ] and one earthquake data [ 14 ] from wwwcswaikatoacnz/ml/weka/ distribution . The data characteristics are summarized in the flrst three columns of Table 3 . We select only those data sets which are more than 1000 data points with all real valued attributes and without missing values|in order to get a result with a higher confldence than those obtained from small data sets .
On each data set , we randomly sample two thirds of the instances for training and the remaining one third for testing . This is repeated 20 times and we report the average result of these 20 runs . The data set , whether in the original space or the mass space , is min max normalized before an † SVR model is trained . To select optimal parameters for the † SVR algorithm , we conduct a 5 fold cross validation based on mean squared error using the training set only . The kernel parameter ( cid:176 ) is searched in the range f2¡15 ; 2¡13 ; 2¡11 ; ¢ ¢ ¢ ; 23 ; 25g ; the regularization parameter
P E B
0.18
0.17
0.16
0.15
0.14
0.13
0.12
BEP
1.4
1.2 e m i t i g n s s e c o r P
1
0.8
0.6
0.4
0.2
0.9
0.8
C U A
0.7
Processing time
0.92
0.9
C U A
0.88
0.86 h=1 h=2 h=3 h=1 h=2 h=3
500
1500 t , number of mass distributions
1000
2000
10 100 t , number of mass distributions
500
1000
10 100
500 t , number of mass distributions
1000
( a ) An example of CBIR round 5 result .
( b ) Efiect of h in the Forest data set .
( c ) Efiect of h in the Smtp data set .
Figure 5 : ( a ) The retrieval performance and the processing time as t increases for InstR0 . ( b ) High h produces a poorer detection performance in this case . ( c ) High h produces a better detection performance in this case .
Table 4 : Data characteristics of the data sets in anomaly detection tasks . The percentage in brackets indicates the percentage of anomalies .
Http Forest
Mulcross
Smtp Shuttle data size 567497 286048 262144 95156 49097 u 3 10 4 3 9 anomaly class attack ( 0:4 % ) class 4 ( 0:9 % ) vs class 2
2 clusters ( 10 % ) attack ( 0:03 % ) classes 2,3,5,6,7 ( 7 % ) vs class 1
C in the range f0:1 ; 1 ; 10g , and † in the range f0:01 ; 0:05 ; 0:1g . We measure regression performance in terms of mean squared error ( MSE ) and squared correlation coe–cient ( SCC ) , and runtime in seconds . The runtime reported is the runtime for SVR only . The total cost of mass estimation ( from the training set ) and mapping ( of training and testing sets ) is 3.95 seconds in the largest data set , tic . The cost of normalisation and the parameter search using 5 fold cross validation is not included in the reported result for both SVR0 and SVR . The result is presented in Table 3 . SVR0 performs significantly better than SVR in all data sets in both MSE and SCC measures ; the only exception is in the wine red data set . Although SVR0 takes more time to run as it runs on the data with a signiflcantly higher dimension , yet the factor of increase in time ranges from 2 to 3 only when the factor of increase in the number of dimensions ranges from 12 to over 300 ( shown in the last two columns of Table 3 ) . This is because the time complexity in the key optimisation process in SVR is not dependent on the number of dimensions . 4.3 Anomaly Detection
This experiment compares MassAD with four state of theart anomaly detectors : isolation forest ( or iForest ) [ 10 ] , a distance based method ORCA [ 3 ] , a density based method LOF [ 4 ] , and one class support vector machine ( or 1 SVM ) [ 13 ] . MassAD is built with t = 100 and ˆ = 256 , the same default settings as used in iForest [ 10 ] , which also employs a multi model approach . The parameter settings employed for ORCA and LOF are as stated in [ 10 ] . 1 SVM uses Radial Basis Function kernel and an inverse width parameter estimated by the method suggested in [ 5 ] .
All the methods are tested on the flve largest data sets used in [ 10 ] . The data characteristics are summarized in Table 4 , which include one anomaly data generator Mulcross [ 12 ] and the other four are from UCI repository [ 2 ] . The performance is evaluated in terms of averaged AUC ( area under ROC curve ) and processing time ( a total of training
Table 5 : AUC values for anomaly detection . 1 SVM iForest ORCA
MassAD
LOF
Http Forest
Mulcross
Smtp Shuttle
ˆ=8 ˆ=256 0.98 1.00 0.88 0.91 0.97 0.99 0.86 0.86 0.99 0.99
1.00 0.87 0.96 0.88 1.00
0.36 N/A 0.57 0.83 0.33 0.59 0.32 0.87 0.60 0.55
0.90 0.90 0.59 0.78 0.79
Table 6 : Runtime ( second ) for anomaly detection . 1 SVM iForest ORCA
MassAD
LOF
Http Forest
Mulcross
Smtp Shuttle
ˆ=8 ˆ=256 27 14 13 4 2
34 18 17 7 4
147 79 75 26 15
9487 > 2weeks 6995 2512 267 157
224380 156044 24281 7490
35872 9738 7343 987 333 time and testing time ) over ten runs ( following [ 10] ) . MassAD and iForest are implemented in Matlab and tested on an AMD Opteron machine with a 1:8 GHz processor and 4 GB memory . The results for ORCA , LOF and 1 SVM are conducted using the same experimental setting but on a faster 2:3 GHz machine , the same machine used in [ 10 ] .
The AUC values of all methods are presented in Table 5 where the flgures boldfaced are the best performance for each data set . The results show that MassAD with ˆ = 256 achieves the best performance on the three largest data sets ; and even on the other two data sets , MassAD is also competitive since the AUC gap is small between MassAD and the best method , ie , iForest . It is noteworthy that MassAD significantly outperforms the traditional density based , distancebased and SVM anomaly detectors in all data sets , except two : one in Smtp when compared with ORCA and another in Forest when compared with 1 SVM . The above observations validate the efiectiveness of our proposed mass estimation on anomaly detection tasks .
Table 6 shows the runtime result . Although MassAD is run on a slower machine , it still has a signiflcant advantage in term of processing time over ORCA , LOF and 1 SVM . The comparison with iForest is presented in Table 7 with a breakdown of training time and testing time . Note that MassAD takes the same time as iForest in training , but it only takes about one tenth of the time required by iForest in testing . These results show that MassAD is an e–cient anomaly detector .
Figures 5(b ) and 5(c ) show the efiect of h on the detection
Table 7 : Training time and testing time ( second ) for MassAD and iForest , using t = 100 and ˆ = 256 . Testing time
Training time
Http Forest
Mulcross
Smtp Shuttle
MassAD 20.96 11.26 10.54 4.97 3.43 iForest
19.72 11.47 10.69
4.1 3.23
MassAD 12.93 6.97 6.82 2.22 1.01 iForest 127.47 67.45 64.34 22.39 11.79 performance of MassAD with ˆ = 8|higher h degrades the detection performance in Forest ; but it improves in Smtp . This shows that for best performance in individual data set , some parameter tuning is required , like most other algorithms . Note that there is no attempt to tune this parameter ( or any other parameters ) in the result reported in Tables 5 , 6 and 7 where h = 1 is used throughout .
The time and space complexities for four methods are given in Table 8 . MassAD and iForest have the best time and space complexities due to their ability to use small ˆ ¿ n and h = 1 . Note that MassAD ( h = 1 ) is faster by a factor of log(ˆ = 256 ) = 8 which shows up in the testing time| ten times faster than iForest given in Table 7 . The training time disadvantage , compared to iForest , did not show up because of small ˆ . MassAD also has an advantage over iForest in space complexity by a factor of log(ˆ ) .
Table 8 : A comparison of time and space complexities . The time complexity includes both training and testing . n is the given data set size and u is the number of dimensions . For MassAD and iForest , the flrst part of the summation is the training time and the second the testing time .
Time complexity O(t(ˆh+1 + n ) )
Space complexity O(tˆ )
MassAD iForest O(t(ˆ + n ) ¢ log(ˆ ) ) O(tˆ ¢ log(ˆ ) ) ORCA LOF
O(un ¢ log(n ) ) O(un2 )
O(un ) O(un )
4.4 Constant time and space complexities
In this section , we show that mass(x ; hjD ) ( in step 4 of Algorithm 1 ) takes only constant time , regardless of the given data size n , when the algorithmic parameters are flxed . Table 9 reports the runtime time for sampling ( to get a random sample of size ˆ from the given data set|steps 2 and 3 of Algorithm 1 ) and the runtime for mass estimation|to construct mass(x ; hjD ) t times , for flve data sets which include the largest and smallest data sets in regression and anomaly detection tasks .
The results show that the sampling time increases linearly with the size of the given data set , and it takes a signiflcantly longer ( in the largest data set ) than the time to construct the mass distribution|which is constant , regardless of the given data size . Note that the training time provided in Table 7 includes both the sampling time and mass estimation time , and it is dominated by the sampling time .
The memory required for each construction of mass(x ; hjD ) is to store one lookup table of size ˆ which is constant , again independent of the given data size .
Table 9 : Runtime ( second ) for sampling , mass(x ; 1jD ) and mass(x ; 3jD ) , where t = 1000 and ˆ = 8 .
Http
Shuttle COREL tic concrete data size sampling mass(x ; 1jD ) mass(x ; 3jD ) 567497 49097 10000 9822 1030
185.21 12.47 2.34 2.28 0.36
17.15 17.37 17.28 17.23 17.28
0.57 0.59 0.53 0.56 0.48
Summary The above results in all three tasks show that the orderings provided by mass distributions deliver additional information about the data that would otherwise hidden in the original features . The additional information improves the task speciflc performance signiflcantly , especially in the information retrieval and regression tasks .
Using Algorithm 3 , the runtime is expected to be higher because the new space has much higher dimensions than the original space ( t ( cid:192 ) u ) . It shall be noted that the runtime increase ( linearly or worse ) is solely a characteristic of the existing algorithms used , not due to the mass space mapping which has constant time and space complexities .
We believe that a more tailored approach that better integrates the information provided by mass ( into the C3 component in the formalism ) for the speciflc task can potentially further improve the current level of performance in terms of either task speciflc performance measure or runtime . We have demonstrated this ‘direct’ application using Algorithm 4 for the anomaly detection task , in which MassAD performs equally well or signiflcantly better than four state of the art methods in terms of task speciflc performance measure , and it executes faster than all other methods in terms of runtime . Why does one dimensional mapping work when tackling multi dimensional problems ? The mapping transforms each original feature to approximately t u features in the mass space|unearth hidden information for each original feature . It is more of a question whether an algorithm can make full use of this information in the new space ; as both the original and new spaces are multi dimensional . A multi dimensional mapping may better enhance information in some domains . It is thus worthwhile to explore this extension .
5 . RELATION TO KERNEL DENSITY
ESTIMATION
A comparison of mass estimation and kernel density esti mation is provided in Table 10 .
Table 10 : A comparison of kernel density estimation and mass estimation . Kernel density estimation requires two parameter settings : kernel function K( : ) and bandwidth hw ; mass estimation has one : h . nhw Pn i=1 K( x¡xi hw
)
Kernel density(x ) = 1 mass(x ; h ) = ‰ Pn¡1 Pn¡1 i=1 massi(x ; h 1)p(si ) ; h > 1 i=1 mi(x)p(si ) ; h = 1
Like kernel estimation , mass estimation at each point is computed through a summation of a series of values from a mass base function mi(: ) , equivalent to a kernel function K(: ) . The two methods difier in the following ways :
Table 11 : CBIR results : Compare with Qsim00 and InstR00 which use Gaussian kernel density estimation .
One Query
Round 1 Round 2 Round 3 Round 4 Round 5
Qsim0 10.31 15.39 17.46 18.46 19.18 19.62
Qsim00 2.51 2.72 2.67 2.56 2.53 2.46
BEP ( £10¡2 ) InstR0 Qsim 10.31 7.78 13.45 10.59 15.07 11.81 12.59 16.15 16.96 13.16 13.55 17.62
Processing time ( second )
InstR00
2.51 2.66 2.51 2.31 2.20 2.07
InstR 7.78 9.40 9.99 10.36 10.78 11.05
Qsim0 0.410 0.588 0.646 0.737 0.862 1.016
Qsim00 0.409 0.633 0.780 0.989 1.275 1.629
Qsim 0.034 0.078 0.139 0.227 0.355 0.516
InstR0 0.410 0.558 0.559 0.560 0.561 0.562
InstR00 0.409 0.571 0.574 0.577 0.580 0.582
InstR 0.034 0.046 0.047 0.048 0.049 0.050
Table 12 : Anomaly detection : MassAD vs DensityAD .
AUC
Time ( second )
MassAD
DensityAD
MassAD
DensityAD
Http Forest
Mulcross
Smtp Shuttle
1.00 0.91 0.99 0.86 0.99
0.99 0.69 1.00 0.60 0.92
34 18 17 7 4
33 18 17 7 4
† Aim : Kernel estimation is aimed to do probability density estimation ; whereas mass estimation is to estimate an order from the core points to the fringe points .
† Kernel function : While kernel estimation can use difierent kernel functions for probability density estimation ; we doubt that mass estimation requires a difierent base function for two reasons . First , a more sophisticated function is unlikely to provide a better ordering than a simple rectangular function . Second , the rectangular function keeps the computation simple and fast . In addition , a kernel function must be flxed ( ie , having user deflned values for its parameters ) ; eg , the rectangular kernel function has flxed width or flxed per unit size . But the rectangular function used in mass has no parameter and no flxed width .
† Sample size : Kernel estimation or other density estimation methods require a large sample size in order to estimate the probability accurately [ 7 ] . Mass estimation using mass(x ; hjD ) needs only a small sample size in an ensemble to accurately estimate the ordering .
† Deflnition : Probability density can be deflned independent of data , whereas mass ( in its current form ) must be deflned wrt a set of data .
Because of a lack of concavity , density will not perform as successfully as mass . Here we present the results using a Gaussian kernel density estimation , replacing mass(x ; hjDk ) , using the same subsample size in an ensemble approach . The bandwidth parameter is set to be the standard deviation of the subsample ; and all the other parameters are the same . The results for information retrieval and anomaly detection are provided in Tables 11 and 12 . Compare to mass , density performs signiflcantly worse in information retrieval task in all experiments using Qsim and InstR , denoted as Qsim00 and InstR00 , respectively . They are even worse than those run in the original space . In anomaly detection , DensityAD , which uses a Gaussian kernel density estimation , performs signiflcantly worse than MassAD in three out of flve data sets in the anomaly detection tasks , and equally well in the other two data sets .
6 . RELATED WORK
There is a close relationship between the proposed mass and data depth [ 11 ] : they both delineate the centrality of a data cloud ( as opposed to compactness in the case of density . ) The properties common to both measures are : ( a ) the centre of a data cloud has the maximum value of the measure ; ( b ) an ordering from the centre ( having the maximum value ) to the fringe points ( having the minimum values ) .
However , there are three fundamental difierences . First , data depth can deal with unimodal data only ; whereas mass can deal with both unimodal and multi modal data by setting h = 1 or h > 1 .
Second , mass is a simple and straightforward measure , and has an e–cient estimation method ; whereas data depth has many difierent deflnitions , depending on the construct used to deflne depth . The constructs could be Mahalanobis , Convex Hull , simplicial and so on [ 11 ] , all of which are expensive to compute [ 1]|this has been the main obstacle in applying data depth for real applications in multi dimensional problems . In addition , the centre of a data cloud varies depending on the construct used to deflne data depth ; whereas mass ( h = 1 ) always has the centre located at the mid point in the series of data points .
Third , the h = 1 mass estimation guarantees concavity| the reason why a simple mass space mapping improves the task speciflc performance of four existing algorithms in information retrieval and regression tasks . In contrast , there is no such guarantee in data depth . Because of a lack of concavity , like density , data depth is unlikely to be as successful as mass in the three tasks we have reported here , even if we ignore the runtime issue .
Mass estimation can be implemented in difierent ways . For example , we have reported an implementation using a tree structure ( instead of a lookup table ) in [ 15 ] using HalfSpace Trees . It reduces the time complexity to O(th(ˆ + n ) ) from O(t(ˆh+1 + n) ) , making it feasible for very high levelh mass estimation . We have repeated the experiments reported in this paper using Half Space Trees , and it produces almost identical results .
Half Space Trees extends naturally from one dimensional mass estimation to multi dimensional mass estimation . This has been tested in anomaly detection task [ 15 ] . iForest [ 10 ] and MassAD shares some common features : Both are ensemble methods which build t models , each from a random sample of size ˆ , and they both combine the outputs of the models through averaging during testing . Although iForest [ 10 ] is designed speciflcally for anomaly detection which employs path length|an instance traverses from the root of a tree to its leaf|as the anomaly score , we have shown in [ 15 ] that the path length used in iForest is in fact a proxy to mass . In other words , iForest is a kind of mass based method|that is why MassAD and iForest have similar detection accuracy .
We have already established a direct application of mass in content based image retrieval [ 17 ] . In addition to the mass space mapping we have shown here , [ 17 ] presents a framework that assigns a weight ( based on iForest , thus , mass ) to each feature wrt a query image ; and then it ranks images in the database according to their weighted average feature values . The framework also incorporates relevance feedback which modifles the ranking based on the feedbacks through reweighted features . This framework makes use of all three components of the formalism stated in Section 3 . This direct application of mass performs signiflcantly better than the indirect approach we have shown in Section 4.1 , in terms of both retrieval performance and processing time . Like MassAD , no distance calculations are used at all|the key reason for its superior time complexity .
7 . CONCLUSIONS AND FUTURE WORK
This paper makes two key contributions . First , we introduce a base measure , mass , and delineate its three properties : ( i ) a mass distribution stipulates an ordering from core points to fringe points in a data cloud ; ( ii ) this ordering accentuates the fringe points with a concave function| the essential property that is easily exploited by existing algorithms to improve their task speciflc performance ; and ( iii ) it is a constant time and space complexities estimation method . Density estimation has been the base modelling mechanism employed in many techniques thus far . Mass estimation introduced here provides an alternative choice , and it is better suited for many tasks which require an ordering rather than probability density estimation .
Second , we present a mass based formalism which forms a basis to apply mass for difierent tasks . The three tasks ( ie , information retrieval , regression and anomaly detection ) in which we have successfully applied are just examples of its application . Mass estimation has potentials in applications as diverse as density estimation has applied now .
There are potential extensions to the current work . First , one shall consider a new way to best utilise mass when solving a problem . In other words , we advocate a direct application of mass , rather than an indirect application . Second , the algorithms provided here for the three tasks are by no means deflnitive , and even the formalism can be improved or extended to include more tasks . Third , because the purposes and their properties difier , mass estimation is not intended to replace density estimation|it is thus important to identify areas in which each is best suited for . This will ascertain areas in which density has been a mismatch , unbeknown thus far .
Acknowledgements This work is partially supported by the Air Force Research Laboratory , under agreement# FA2386 10 1 4052 . The US Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon . Phil Rayment , David Albrecht , Zhouyu Fu and Geofi Webb have provided many helpful comments in the early draft . Suggestions from the anonymous reviewers have helped to improve the clarity of this paper .
The Matlab source code of mass estimation is available at http://sourceforgenet/projects/mass estimation/
8 . REFERENCES [ 1 ] G . Aloupis . Geometric measures of data depth .
DIMACS Series in Discrete Math and Theoretical Computer Science , 72:147{158 , 2006 .
[ 2 ] A . Asuncion and D . Newman . UCI machine learning repository , 2007 .
[ 3 ] S . D . Bay and M . Schwabacher . Mining distance based outliers in near linear time with randomization and a simple pruning rule . In Proceedings of SIGKDD , pages 29{38 , 2003 .
[ 4 ] M . M . Breunig , H P Kriegel , R . T . Ng , and J . Sander .
LOF : Identifying density based local outliers . In Proceedings of SIGKDD , pages 93{104 , 2000 . [ 5 ] B . Caputo , K . Sim , F . Furesjo , and A . Smola .
Appearance based object recognition using svms : which kernel should i use ? In NIPS workshop on Statitsical methods for computational experiments in visual processing and computer vision , 2002 .
[ 6 ] C C Chang and C J Lin . LIBSVM : a library for support vector machines , 2001 .
[ 7 ] R . Duda , P . Hart , and D . Stork . Pattern
Classiflcation . Second Edition . John Wiley , 2001 . [ 8 ] G . Giacinto and F . Roli . Instance based relevance feedback for image retrieval . In Advances in NIPS , pages 489{496 , 2005 .
[ 9 ] J . He , M . Li , H . Zhang , H . Tong , and C . Zhang .
Manifold ranking based image retrieval . In Proceedings of ACM Multimedia , pages 9{16 , 2004 .
[ 10 ] F . T . Liu , K . M . Ting , and Z H Zhou . Isolation forest . In Proceedings of ICDM , pages 413{422 , 2008 .
[ 11 ] R . Liu , J . M . Parelius , and K . Singh . Multivariate analysis by data depth . The Annals of Statistics , 27(3):783{840 , 1999 .
[ 12 ] D . M . Rocke and D . L . Woodrufi . Identiflcation of outliers in multivariate data . Journal of the American Statistical Association , 91(435):1047{1061 , 1996 .
[ 13 ] B . Sch˜olkopf , R . C . Williamson , A . J . Smola ,
J . Shawe Taylor , and J . C . Platt . Support vector method for novelty detection . In Advances in NIPS , pages 582{588 , 2000 .
[ 14 ] J . S . Simonofi . Smoothing Methods in Statistics .
Springer Verlag , 1996 .
[ 15 ] K . M . Ting , S . C . Tan , and F . T . Liu . Mass : A new ranking measure for anomaly detection . Gippsland School of Information Technology , Monash University , Technical Report TR2009/1 , 2009 .
[ 16 ] V . N . Vapnik . The Nature of Statistical Learning
Theory . Second Edition . Springer , 2000 .
[ 17 ] G T Zhou , K . M . Ting , F . T . Liu , and Y . Yin .
Relevance feature mapping for content based image retrieval . In Proceedings of Multimedia Data Mining Workshop at KDD , 2010 .
[ 18 ] Z H Zhou , K J Chen , and H B Dai . Enhancing relevance feedback in image retrieval using unlabeled data . ACM Transactions on Information Systems , 24(2):219{244 , 2006 .
[ 19 ] Z H Zhou and H B Dai . Query sensitive similarity measure for content based image retrieval . In Proceedings of ICDM , pages 1211{1215 , 2006 .
