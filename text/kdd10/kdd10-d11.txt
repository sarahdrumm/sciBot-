A Scalable Two Stage Approach for a Class of
Dimensionality Reduction Techniques
Liang Sun
Arizona State University Tempe , AZ 85287 , USA sunliang@asuedu
Betul Ceran
Arizona State University Tempe , AZ 85287 , USA betul@asu.edu
Jieping Ye
Arizona State University Tempe , AZ 85287 , USA jiepingye@asuedu
ABSTRACT Dimensionality reduction plays an important role in many data mining applications involving high dimensional data . Many existing dimensionality reduction techniques can be formulated as a generalized eigenvalue problem , which does not scale to large size problems . Prior work transforms the generalized eigenvalue problem into an equivalent least squares formulation , which can then be solved efficiently . However , the equivalence relationship only holds under certain assumptions without regularization , which severely limits their applicability in practice . In this paper , an efficient two stage approach is proposed to solve a class of dimensionality reduction techniques , including Canonical Correlation Analysis , Orthonormal Partial Least Squares , Linear Discriminant Analysis , and Hypergraph Spectral Learning . The proposed two stage approach scales linearly in terms of both the sample size and data dimensionality . The main contributions of this paper include ( 1 ) we rigorously establish the equivalence relationship between the proposed twostage approach and the original formulation without any assumption ; and ( 2 ) we show that the equivalence relationship still holds in the regularization setting . We have conducted extensive experiments using both synthetic and real world data sets . Our experimental results confirm the equivalence relationship established in this paper . Results also demonstrate the scalability of the proposed two stage approach .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications Data Mining
General Terms Algorithm
Keywords Dimensionality reduction , generalized eigenvalue problem , least square , regularization , scalability
1 .
INTRODUCTION
Recent technological innovations have allowed us to collect massive amounts of data with a large number of features . One of the key issues in such data analysis is the curse of dimensionality [ 5 ] , ie , an enormous number of samples is required to perform accurate prediction on problems with large numbers of features . Dimensionality reduction , which extracts a small number of features by removing the irrelevant , redundant , and noisy information , is an effective way to overcome the curse of dimensionality . Many dimensionality reduction algorithms have been proposed in the past , including Canonical Correlation Analysis ( CCA ) [ 15 ] , Partial Least Squares ( PLS ) [ 21 ] , Linear Discriminant Analysis ( LDA ) [ 6 ] and Hypergraph Spectral Learning ( HSL ) [ 24 ] . A common characteristic of these algorithms is that they can be formulated as a generalized eigenvalue problem . Although well established algorithms in numerical linear algebra exist to solve generalized eigenvalue problems [ 22 , 12 ] , they are in general computationally expensive to solve and hence may not scale to large size problems .
There have been several recent attempts to improve the scalability of dimensionality reduction techniques [ 8 , 9 , 16 , 24 , 25 , 26 , 32 , 33 ] . The key idea is to transform the generalized eigenvalue problem into an equivalent least squares formulation , which can be solved efficiently using existing algorithms such as the iterative conjugate gradient algorithm [ 12 , 19 , 20 ] . In particular , an equivalent least squares formulation has been developed for a class of dimensionality reduction techniques in [ 26 ] . However , it suffers from several drawbacks which limits its applicability in practice . First , the equivalent transformation relies on a key assumption that all the data points are linear independent . This assumption tends to hold for high dimensional data , but it is likely to fail for ( relatively ) low dimensional data . Secondly , the equivalence relationship between the least squares formulation and the original formulation does not hold when the regularization is employed . However , regularization has been shown to be critical in many data mining and machine learning algorithms including support vector machines [ 23 ] . In this paper , we propose an efficient two stage approach to solve a class of dimensionality reduction techniques , including CCA , PLS , LDA and HSL . In the first stage we solve a least squares problem using the iterative conjugate gradient algorithm [ 12 , 19 , 20 ] . The distinct property of this stage is its low time complexity . In the second stage , the original data is projected onto a low dimensional space , and then we solve a generalized eigenvalue problem with a significantly reduced size . The proposed two stage approach scales linearly in terms of both the sample size and data dimensionality , thus applicable for large size problems . The main contributions of this paper include :
• We rigorously prove the equivalence relationship between the two stage approach and the direct approach which solves the generalized eigenvalue problem directly . Compared with previous work , the two stage approach does not require any assumption and can be applied in all cases .
• We show that the two stage approach can be further extended to the regularization setting . The equivalence relationship is also rigorously proved in this case .
We have conducted extensive experiments to evaluate the proposed two stage approach using both synthetic and realworld benchmark data sets . Our experimental results confirm the equivalence relationship established in this paper . Results also demonstrate the scalability of the proposed twostage approach . Organization The rest of the paper is organized as follows . Section 2 briefly reviews the class of dimensionality reduction techniques discussed in the paper . In Section 3 , we present the two stage approach , and establish the equivalence relationship . We further extend the two stage approach to the regularization setting in Section 4 . A comprehensive empirical study is reported in Section 5 . Finally , we conclude in Section 6 . Notations Throughout the paper , all matrices are boldface uppercase , and vectors are boldface lowercase . n is the number of samples , d is the data dimensionality , and k is the number of classes ( or labels ) . The ith sample is denoted as d , and its corresponding label is denoted as yi ∈ R xi ∈ R k . X = [ x1 , x2 , ··· , xn ] ∈ R d×n represents the data matrix , and Y = [ y1 , y2 , ··· , yn ] ∈ R k×n is the matrix representation for label information . S ∈ R n×n is a symmetric and positive semi definite matrix . Ip is the p by p identity matrix , and 1p is a vector of all ones with length p . Note that the subscript p may be omitted when the size is clear from the context .
2 . A CLASS OF DIMENSIONALITY REDUC
TION TECHNIQUES
The class of generalized eigenvalue problems considered in this paper exhibit the following form : T w = λXX
XSX
T
( 1 ) where S ∈ R n×n is a symmetric and positive semi definite matrix . In general , we are interested in the principal eigenvectors corresponding to nonzero eigenvalues . The generalized eigenvalue problem in Eq ( 1 ) is often reformulated as the following eigenvalue problem w ,
†
T
)
( XX
XSX
( 2 ) where ( XXT ) is the Moore Penrose pseudoinverse of XXT . In addition , the generalized eigenvalue problem in Eq ( 1 ) can also be formulated as an optimization problem :
†
T w = λw ,
Tr(WT XSXT W ) max W s . t . WT XXT W = I .
( 3 )
In the following derivation , we generally use the formulation in Eq ( 2 ) .
Many existing dimensionality reduction techniques exhibit the form in Eq ( 1 ) or ( 2 ) . In particular , the matrix S can be represented in the following form : where H ∈ R mation in supervised learning .
S = HH
( 4 ) n×k is often constructed from the label infor
T ,
To control the model complexity and avoid the singularity of XXT , a regularization term γId with γ > 0 is added to XXT in Eq ( 1 ) , leading to the following generalized eigenvalue problem :
XSX
T w = λ(XX
T
+ γId)w .
( 5 )
We briefly review several algorithms involving the generalized eigenvalue problem in the form of Eq ( 1 ) . Specifically , they include Canonical Correlation Analysis , Orthonormalized Partial Least Squares , Hypergraph Spectral Learning , and Linear Discriminant Analysis . For supervised learning methods , the label information is encoded in the matrix Y = [ y1 , y2,··· , yn ] ∈ R k×n , where yi(j ) = 1 if xi belongs to class j and yi(j ) = 0 otherwise . 2.1 Canonical Correlation Analysis
In Canonical Correlation Analysis ( CCA ) [ 4 , 13 , 15 ] , two different representations , X and Y , of the same set of objects are given , and a projection is computed for each representation such that the correlation coefficient
.
ρ = wT x XYT wy x XXT wx)(wT
( wT y YYT wy )
( 6 ) is maximized in the dimensionality reduced space , where d and wy ∈ Rk are projection vectors for X and wx ∈ R Y , respectively . Assume that YYT is nonsingular . It can be verified that wx is the first principal eigenvector of the following generalized eigenvalue problem :
XY
T
( YY
T
)
YX
T wx = λXX
T wx .
( 7 )
−1
Multiple projection vectors can be obtained simultaneously by computing the first ff principal eigenvectors of the generalized eigenvalue problem in Eq ( 7 ) . It can be observed that CCA is in the form of the generalized eigenvalue problem in −1/2 . Eq ( 1 ) with S = YT ( YYT ) 2.2 Orthonormalized Partial Least Squares
−1Y and H = YT ( YYT )
Partial least squares ( PLS ) [ 29 ] is a family of methods for modeling relations between two sets of variables . In this paper , the Orthonormalized Partial Least Squares ( OPLS ) [ 30 , 2 ] , a popular variant of PLS , is studied . In contrast to CCA , OPLS computes orthogonal score vectors by maximizing the covariance between X and Y . It solves the following generalized eigenvalue problem :
XY
T
YX
T w = λXX
T w .
( 8 )
It follows from Eq ( 8 ) that orthonormalized PLS involves a generalized eigenvalue problem in Eq ( 1 ) with S = YT Y and H = YT . 2.3 Hypergraph Spectral Learning
Hypergraph Spectral Learning ( HSL ) [ 24 ] is a dimensionality reduction technique for multi label classification . A hypergraph [ 1 ] is a generalization of the traditional graph in which the edges ( aka hyperedges ) are arbitrary nonempty subsets of the vertex set . HSL employs a hypergraph
Algorithm 1 The Two Stage Approach without Regularization
Input : X , H Output : W Stage 1 : Solve the following least squares problem : fiW
T
1 X − H
Tfi2 F . min W1
( 10 )
Stage 2 : Compute ˜X = WT optimization problem :
1 X , and solve the following
Tr(W
T 2 ˜XHH
T ˜X
T
W2 )
( 11 ) max W2 s . t . W
T 2 ˜X ˜X
T
W2 = I
Compute W = W1W2 as the final solution . to capture the correlation information among different labels for improved classification performance in multi label learning . Specifically , HSL constructs a hyperedge for each label , and includes all instances annotated with a common label into one hyperedge , thus capturing their joint similarity . Three different Laplacians have been proposed to capture the spectral properties of hypergraph , including clique expansion [ 1 ] , star expansion [ 1 ] and Zhou ’s Laplacian [ 34 ] . It has been shown that given the normalized Laplacian L for the constructed hypergraph , HSL involves the following generalized eigenvalue problem :
XSX
T w = λ(XX
T
)w , where S = I − L .
( 9 )
It has been shown that for all three Laplacian matrices , the resulting matrix S is symmetric and positive semi definite , and it can be represented as S = HHT , where H ∈ R n×k . Note that H can be constructed from the label information in Y explicitly without the matrix decomposition . 2.4 Linear Discriminant Analysis
As a supervised dimensionality reduction technique , LDA attempts to minimize the within class variance while maximizing the between class variance after the linear projection . It has been shown that the optimal linear projection consists † of the top eigenvectors of S t Sb corresponding to nonzero eigenvalues [ 11 , 31 ] , where St is the total covariance matrix and Sb is the between class covariance matrix . The matrices St and Sb are defined as follows :
St =
1 n XX
T , Sb =
1 n
( j ) nj c
( j ) T c
,
( 12 )
'n where c(j ) is the centroid of the jth class and we assume i=1 xi = 0 . We also assume that the data matrix that X is partitioned into k classes as X = [ X1,··· , Xk ] , where Xj ∈ R d×nj corresponds to the data points from the jth class , nj is the size of the jth class , and j=1 nj = n . Note that c(j ) = 1 Xj 1j , where 1j is a vector of all ones with nj length nj . Thus , we have kfi
'k kfi ff nSb =
T Xj1j 1 j X
T j =
1 nj
Xj j=1
1 nj
T 1j1 j
T j
X kfi j=1 j=1 kfi j=1
=
XjSj X
T j = XSX
T , where Sj = 1 nj j , and S is defined as diag ( S1 , S2,·· · , Sk ) † for LDA . Therefore , S t Sb can also be formulated in the following form :
1j 1T
†
† t Sb = ( XX S
T
)
( XSX It can be verified that S = HHT , where 1√ nk
11 , 1√ n2
12,··· ,
1√ n1
H = diag ff
1k
T
) .
( 13 )
∈ R n×k .
( 14 )
3 . THE TWO STAGE APPROACH WITHOUT REGULARIZATION
In this section , we present our two stage approach and show that the two stage approach is equivalent to the direct approach which solves the generalized eigenvalue problem in Eq ( 1 ) directly .
3.1 The Algorithm
In the two stage approach , we first solve a least squares problem by regressing X on HT . In other words , HT can be considered as the “ latent target ” encoded by the label information Y . After projecting the data matrix X onto the subspace , we solve the resulting generalized eigenvalue problem by replacing the data matrix in Eq ( 1 ) with the projected data matrix . Note that the data dimensionality is reduced dramatically after the projection , thus the resulting generalized eigenvalue problem in the second step can be solved efficiently . The two stage approach is summarized in Algorithm 1 . 3.2 Time Complexity Analysis
In our implementation , we apply the LSQR algorithm [ 19 , 20 ] , a conjugate gradient method for solving the least squares problem in the first stage . Previous studies have shown that LSQR is reliable even for ill conditioned problems [ 20 ] . In addition , when the data matrix X is sparse , the least squares problem can be solved very efficiently using LSQR . Note that the computational cost of each iteration of LSQR is O(3n + 5d + 2dn ) when X is dense or O(3n + 5d + 2z ) when X is sparse , where z is the number of nonzero entries in X [ 20 ] . Since HT ∈ R n×k , k least squares problems are solved simultaneously in the first stage of Algorithm 1 , which implies that the total computational cost of the first stage is O(N k(3n+5d+2dn ) ) using LSQR when X is dense , where N is the total number of iterations . When the data matrix X is sparse , the cost of LSQR is reduced to O(N k(3n + 5d + 2z) ) . In the second stage , the cost of computing ˜X is O(ndk ) when X is dense or O(kz ) when X is sparse . Since the size of ˜X is significantly reduced , the cost of solving the optimization problem is O(nk2 ) in the second stage . The cost of combining W1 and W2 is O(nkff ) , where ff(ff ≤ k ) is the number of final projection vectors . Therefore , the total computational cost is O(N k(3n + 5d + 2z ) + kz ) when X is sparse .
3.3 Equivalence
Next we rigorously prove the equivalence relationship between the two stage approach and the direct approach which solves the original eigenvalue problem in Eq ( 2 ) directly .
Using the standard technique in linear algebra , the solu tion to the least squares problem in Eq ( 10 ) is
W1 = ( XX
T
†
XH ∈ R
) d×k .
( 15 )
Tr(W
T T 2 H
V1V
T T 1 HH
V1V
T 1 HW2 ) max W2 s . t . W
T T 2 H
V1V
T 1 HW2 = I
Combing Eqs . ( 17 ) and ( 22 ) , we have
W = W1W2 = U1Σ
−1 1 A
T
( UAΣ
−1 A ) . = U1Σ
−1 1 VA −1 1 VA .
AAT AAT as follows :
( †
AA
T
( †
T A
T
AA
T
AA
=
2 AU
UAΣ −2 A U 2 AUA
= UAΣ = UAΣ ff
T AUAΣ
4 AU
T A fi
2 UAΣ AU
T AUAΣ
2 AU
T A
= [ UA , U A ] ∈ R ⊥
Σ2 A 0 0 0
⊥ A ] k×k is orthogonal . Thus , the eigenvec
[ UA , U
⊥ A ]
T , where [ UA , U tors corresponding to the top ff eigenvalues are given by
W2 = UA . , where UA . consists of the first ff columns of UA . To ensure that the constraint in Eq ( 20 ) is satisfied , we normalize the columns of W2 without affecting the range space of WT 2 :
W2 = ( UAΣ
−1 A )
When ff = rank(A ) , we have VA . = VA and W = U1Σ This completes the proof of this theorem .
†
†
Note that the solution to the generalized eigenvalue problem in Eq ( 1 ) consists of the principal eigenvectors of matrix ( XHHT XT ) . We follow [ 25 ] to derive the eigende(XXT ) composition of ( XXT ) ( XHHT XT ) and show the equivalence relationship between the two stage approach and the direct approach . The results are summarized in the following theorem : ff ( ff ≤ rank(A ) ) eigenvalues of ( XXT ) −1 1 VA . ,
Theorem 2 . The eigenvectors corresponding to the top
( 26 ) where VA . consists of the first ff columns of VA . Thus , the two stage approach produces the same solution as the direct approach which solves the original generalized eigenvalue problem directly . r×t with orthonormal columns , r×r is ⊥ A ] =
A ] ∈ R ⊥ there exists V an orthogonal matrix [ 12 ] . Hereafter , we denote [ VA , V Vs A . We can decompose ( XXT )
Proof . Given VA ∈ R r×(r−t ) such that [ VA , V
( XHHT XT ) are
A ∈ R ⊥
W0 = U1Σ
†
†
( XHHT XT ) as follows : T
( XX
= U1Σ = U1Σ = U1Σ
= U
= U
= U fi
) T
T ( XHH
† T X ) −2 T T 1 U 1 XHH X −2 T T T 1 U 1 U1Σ1V 1 HH −1 T T AΣ1U 1 A 1 fl fi Σ −1 1 AT AΣ1 0 fiff 0 Σ2 A 0 0 0
0 −1 1 Vs 0
A 0 I
−1 1 A
AΣ1
U
T
T ff ff ff
Ir 0
Σ
Σ
V1Σ1U
T 1 ffi
Ir 0
T
U fiff fi T Σ1 0 0 I
Vs A
T ,
U
Let the singular value decomposition ( SVD ) [ 12 ] of X be
)
AAT
†
T
= U1Σ1V
X = UΣV d×d and V ∈ R n×r have orthonormal columns , Σ ∈ R
( 16 ) d×r d×n and r×r are diagonal , and r = rank(X ) . Then W1 can n×n are orthogonal , U1 ∈ R
T 1 , where U ∈ R and V1 ∈ R Σ1 ∈ R be represented as
W1 = U1Σ
−1 1 V
T 1 H .
( 17 )
Then ˜X can be represented as
˜X = W
T T 1 X = H
V1Σ
−1 1 U
T 1 U1Σ1V
T T 1 = H
V1V
T 1 , ( 18 ) and
T
˜X ˜X
T = H
V1V
T 1 V1V
T T 1 H = H
V1V
T 1 H .
Thus , the optimization problem in Eq ( 11 ) can be simplified into the following form :
Denote
T A = H
V1 ∈ R k×r .
( 19 )
Then the optimization problem in the second stage can be reformulated as follows :
Tr(W
T 2 AA
T
T
AA
W2 )
( 20 ) max W2 s . t . W
T 2 AA
T
W2 = I
Let the compact SVD of A be
A = UAΣAV
T A , k×t , ΣA ∈ R t×t , VA ∈ R
( 21 ) r×t , and t = where UA ∈ R rank(A ) .
Based on the above definitions , the solution to the two stage approach is summarized in the following theorem .
Theorem 1 . The top ff ( ff ≤ rank(A ) ) projection vectors computed by Eq ( 11 ) are given by
−1 A ) . ,
W2 = ( UAΣ
( 22 ) −1 where ( UAΣ A ) . Thus , the projection vectors computed by the two stage approach are
−1 A ) . consists of the first ff columns of ( UAΣ
W = W1W2 = U1Σ
−1 1 VA
When ff = rank(A ) , W can be simplified as
W = U1Σ
−1 1 VA .
( 23 )
( 24 )
Proof . Using the Lagrange dual function in optimization theory , the optimization problem in Eq ( 20 ) can be reformulated as the following eigenvalue problem :
( †
T
AA
T
AA
AA
T w2 = λw2 .
( 25 )
It is clear that the eigenvectors corresponding to the top ff eigenvalues of ( XXT )
( XHHT XT ) are given by
†
Next we derive the eigendecomposition of the matrix
W0 = U1Σ
−1 1 VA
The equivalence relationship follows from Eqs . ( 23 ) and ( 26 ) . This completes the proof of the theorem .
A consequence of Theorem 1 is that solving the optimization problem in Eq ( 11 ) in the second stage amounts to computing the SVD of the matrix A . Note that A ∈ R k×r , where r = rank(X ) ≤ min{d , n} , thus the computational cost of the SVD of A is quite low . In practice we can perform SVD on A directly instead of solving the optimization problem in Eq ( 11 ) .
Remark 1 . A least squares formulation is proposed in [ 26 ] for a class of dimensionality reduction techniques with the same computational cost as the proposed two stage approach . However , the analysis in [ 26 ] assumes that the data matrix X is of full rank ( before centering ) . This tends to fail for ( relatively ) low dimensional data . In particular , when the number of data points is larger than the number of dimensions , this assumption is likely to be violated . The twostage algorithm proposed in this paper significantly improves previous work by relaxing this assumption .
4 . THE TWO STAGE APPROACH
WITH REGULARIZATION
Regularization is commonly employed to control the model complexity and avoid overfitting . In this section we present the two stage approach with regularization . We rigorously prove the equivalence relationship between the two stage approach and the direct approach which solves the regularized generalized eigenvalue problem in Eq ( 5 ) . 4.1 The Algorithm
To handle the regularization in the generalized eigenvalue problem in Eq ( 5 ) , we solve a penalized least squares problem , or ridge regression [ 14 ] in the first step . Note that the “ latent target ” is the same as the one used in Algorithm 1 ; the difference is the regularization term included in the least squares problem . After projecting the data matrix X onto the subspace , we compute an auxiliary matrix D ∈ R k×k and its SVD . Intuitively , the SVD computation of D amounts to solving the original optimization problem by replacing X with ˜X . Note that the size of D is very small , thus the cost of computing the SVD of D is relatively low . The algorithm outline is summarized in Algorithm 2 . 4.2 Time Complexity Analysis
Similar to Algorithm 1 , the least squares problem in the first stage is solved using the LSQR algorithm [ 20 ] with the In the second stage , since k d , the most same cost . expensive part is the computation of ˜X with a cost of O(kdn ) if X is dense or O(kz ) if X is sparse where z is the number of nonzero entries in the data matrix X . Therefore , the total computational cost is O(N k(3n+5d+2z)+kz ) if X is sparse . 4.3 Equivalence
Next we show that the two stage approach with regularization produces the same solution as the direct approach which solves the regularized generalized eigenvalue problem in Eq ( 5 ) directly .
Following the standard techniques in linear algebra , the solution to the least squares problem with regularization in Eq ( 27 ) is
W1 = ( XX
T
+ γI )
†
XH = U1
Σ
2 1 + γI
Σ1V
T 1 H . ( 28 )
−1
)
Algorithm 2 The Two Stage Approach with Regularization
Input : X , H , γ . Output : W Stage 1 : Solve the following least squares problem : fiW
T
1 X − H
Tfi2
F + γfiW1fi2 F . min W1
( 27 )
Compute ˜X = WT
Stage 2 : Compute the compact SVD of D = UDΣDUT W2 = UDΣ Compute W = W1W2 as the final solution .
−1/2 D .
1 X and D = ˜XH . D and set
) Then ˜X can be represented as
−1
Σ
2 1 + γI
˜X = W
T T T 1 . 1 X = H V1Σ1 Thus the matrix D ∈ R k×k can be represented as : ) T D = ˜XH = H V1Σ1 where the matrix B ∈ R
Σ1V Σ k×r is defined as
−1
Σ1V
T 1 H = BB
2 1 + γI )
T B = H
V1Σ1
Σ
2 1 + γI
−1/2 .
( 29 )
T , ( 30 )
( 31 )
The solution to the two stage approach in Algorithm 2 is summarized in the following theorem :
Theorem 3 . Let the compact SVD of B be
B = UBΣBV
( 32 ) where UB ∈ R r×q , and q = rank(B ) . The top ff ( ff ≤ rank(B ) ) projection vectors computed by Algorithm 2 are given by q×q , VB ∈ R k×q , ΣB ∈ R
T B ,
2 1 + γI ) W = W1W2 = U1(Σ
−1/2
VB . ,
( 33 ) where
W2 = ( UBΣ
−1 B ) . , and VB . consists of the first ff columns of VB , ( UBΣ consists of the first ff columns of UBΣ W can be simplified as
( 34 ) −1 B ) . −1 B . When ff = rank(B ) ,
2 1 + γI ) W = U1(Σ
( 35 ) Proof . Note that D = BBT . The SVD of D can be
VB .
−1/2 obtained from the SVD of B as follows :
D = UBΣ
2 BU
T B = UDΣDU
T D .
( 36 )
It follows from Algorithm 2 that
W2 = UBΣ
−1 B .
Recall that W1 can also be represented using B as :
)
−1
)
−1/2
W1 = U1
2 1 + γI Σ
Σ1V
T 1 H = U1
2 1 + γI
Σ
T .
B
We can thus derive W as follows :
Σ
= U1
W = W1W2 2 1 + γI 2 1 + γI 2 1 + γI
= U1
= U1
Σ
Σ
) ) )
−1/2 −1/2 −1/2
T
B
UBΣ
−1 B
VBΣBU
T BUBΣ
−1 B
VB .
If only the first ff projection vectors are required , then the resulting W is given by
W = U1(Σ
2 1 + γI )
−1/2
VB
This completes the proof .
)
−1
( XHHT XT ) . The eigendecomposition is sum
We follow [ 27 ] for the eigendecomposition of the matrix XXT + γI marized in the following theorem , based on which we also obtain the equivalence relationship between the two stage approach and the direct approach .
Theorem 4 . The eigenvectors corresponding to the top ff −1(XHHT XT ) are given eigenvalues of matrix ( XXT + γI ) by
−1/2
2 1 + γI ) W0 = U1(Σ
( 37 ) where VB . consists of the first ff ( ff ≤ rank(B ) ) columns of VB . Thus , the two stage approach in Algorithm 2 is equivalent to the direct approach which solves the generalized eigenvalue problem with regularization directly .
VB . ,
Proof . Given VB ∈ R there exists V orthogonal matrix . Hereafter , we denote [ VB , V r×(r−q ) such that [ VB , V
B ∈ R ⊥ r×q with orthonormal columns , B ] ∈ R ⊥ r×r is an ⊥ B ] = Vs B . −1(XHHT XT ) as follows :
We can diagonalize ( XXT + γI ) T
T
( XX
= U1(Σ
= U1(Σ
+ γI ) 2 1 + γI ) 2 1 + γI )
X
−1 −1 −1/2
T ( XHH ) T T 1 HH Σ1V 2 1 + γI )
2 1 + γI ) V1Σ1(Σ −1/2 fi 2 1 + γI )
= U1(Σ
1/2
2 1 + γI ) ( Σ T
2 1 + γI ) B(Σ
T 1
V1Σ1U −1/2
Σ1V T 1
U 1/2
T T 1 HH
( Σ −1/2
B −1/2
U
T 1
1/2fl fi ffi
T
U
Ir 0
T
U ff ff ff
= U
= U
= U ff
Ir 0 ( Σ2
1 + γI )
( Σ
2 1 + γI )
T
2 1 + λI ) B(Σ
B
−1/2BT B(Σ2 1 + γI)1/2 0 fiff 0 fi
0 −1/2Vs
Σ2 B 0 0 0
( Σ2
B 0 I
1 + λI ) 0 fi T ( Σ2 1 + γI)1/2 0 0 I
Vs B
T .
U
−1(XHHT XT ) are given by U1(Σ2
Thus , the eigenvectors corresponding to the top ff eigenvalues of ( XXT + γI ) 1 + −1/2VB The equivalence between the two stage approach γI ) and the direct approach follows from Eqs . ( 33 ) and ( 37 ) . This completes the proof of the theorem .
Remark 2 . The least squares algorithm proposed in [ 26 ] only works for dimensionality reduction techniques without regularization . This drawback limits its applicability in practice since the regularized algorithms are expected to be more effective in practice due to its better generalization performance . The two stage algorithm proposed in this paper significantly improves previous work by extending the equivalence to the regularization setting .
Remark 3 . The two stage approach can be extended to the kernel induced feature space . The equivalence relationship still holds for all dimensionality reduction techniques discussed above with and without regularization . Due to the space constraint , we skip the detailed proof in this paper .
Table 1 : Statistics of the data sets : n is the number of samples , d is the data dimensionality , and k is the number of labels ( classes ) . Type n
Data Set Syn1 Syn2 Syn3 Syn4 Ionosphere Optical digits Satimage USPS Wine Scene Yeast news20 rcv1v2
Multi class Multi class Multi label Multi label Multi class Multi class Multi class Multi class Multi class Multi label Multi label Multi class Multi label
1000 1000 1000 1000 351 5620 6435 9298 178 2407 2417 15935 3000 d 100 5000 100 5000
34 64 36 256 13 294 103
62061 47236 k 5 5 5 5 2 10 6 10 3 6 14 20 101
5 . EXPERIMENTS
We have performed extensive experiments to verify the established equivalence relationship and demonstrate the scalability of the proposed two stage approach . All the experiments are performed on a PC with Intel Core 2 Duo T9500 2.6G CPU and 4GB RAM . We implement all algorithms in Matlab . All Matlab codes and synthetic data sets are available at wwwpublicasuedu/∼lsun27/Code/TwoStagehtml 5.1 Experiment Setup
The dimensionality reduction techniques discussed in this paper can be divided into two categories : 1 ) LDA for multiclass classification ; 2 ) HSL , CCA , and OPLS for multi label classification . We utilize both multi class and multi label data sets , including synthetic and real world data sets , in the experiments . Two synthetic data sets for multi class classification as well as two synthetic data sets for multilabel classification are generated . In the synthetic data sets , each entry of the data matrix X is generated independently from the standard Gaussian distribution N ( 0 , 1 ) . The number of classes is k = 5 , and the labels are generated uniformly with random . Five real world data sets from the UCI machine learning repository [ 3 ] and two benchmark data sets in multi label classification [ 7 , 10 ] are used in our experiments . To investigate the scalability of the two stage approach , two large scale data sets news20 [ 17 ] and rcv1v2 [ 18 ] are used . The statistics of all data sets are summarized in Table 1 .
To distinguish different techniques tested in the experiments , we name the regularized techniques using a prefix “ r ” before the corresponding technique , eg , “ rLDA ” . The twostage versions are named using a prefix “ 2S ” ( “ 2S ” means two stage ) such as “ 2SLDA ” and “ 2SrLDA ” , which are the twostage versions of LDA and regularized LDA , respectively .
5.2 Performance Comparison
In this experiment , we compare the performance of different approaches for all techniques using both synthetic and real world data sets . Denote W0 as the solution of the generalized eigenvalue problem in Eq ( 5 ) by solving it directly , and W as the solution of Eq ( 5 ) by solving it using the two stage approach .
C U A
C U A
0.7
0.68
0.66
0.64
0.62
0.6
0.58
0.56
0.54
0.52
0.5
0.7
0.68
0.66
0.64
0.62
0.6
0.58
0.56
0.54
0.52
0.5 rCCA 2SrCCA
1
2
3
4
5
6
−6 −5 −4 −3 −2 −1
0 logγ ( A ) CCA rHSL Clique 2SrHSL Clique
−6 −5 −4 −3 −2 −1
1
2
3
4
5
6
0 logγ
C U A
C U A
0.7
0.68
0.66
0.64
0.62
0.6
0.58
0.56
0.54
0.52
0.5
0.7
0.68
0.66
0.64
0.62
0.6
0.58
0.56
0.54
0.52
0.5 rOPSL 2SrOPLS
1
2
3
4
5
6
−6 −5 −4 −3 −2 −1
0 logγ ( B ) OPLS rHSL Star 2SrHSL Star
−6 −5 −4 −3 −2 −1
1
2
3
4
5
6
0 logγ
( C ) HSL Clique
( D ) HSL Star
Figure 1 : Comparison of different approaches in terms of average AUC for different techniques on the Yeast data set as the regularization parameter γ varies .
To verify whether both approaches produce equivalent 0 − WWTfi2 under difprojections , we compute fiW0WT ferent values of the regularization parameter γ . We vary the value of γ from 0 to 1e6 . It follows from [ 27 ] that fiW0WT 0 −WWT fi2 = 0 if and only if W0 = WR , where R is an orthogonal matrix . Thus , both W and W0 project the original data onto the same low dimensional space . Note that a direct comparison between W and W0 is possible only when the generalized eigenvalue problem in Eq ( 5 ) admits a unique solution . This is not always the case , eg , when two eigenvalues coincide . 0 − WWT fi2 for all data sets are summarized in Table 2 . Note that two different variants of HSL , HSL Clique and HSL Star , which compute the Laplacian using different expansion schemes , are tested . From Table 2 it can be observed that for all values of the regularization parameter γ , fiW0WT 0 − WWTfi2 is always very small , which confirms the equivalence relationship between W and W0 for projection .
The values of fiW0WT
Next , we investigate the classification performance of different techniques . We compare the performance of differ ent approaches on the multi label data set Yeast [ 10 ] . The data set is randomly partitioned into a training set and a test data set with equal size . After the projection matrix is learned from the training set , the test data set is projected onto the low dimensional space . In our experiments , the linear support vector machines ( SVM ) is applied for classification . The average Area Under ROC Curve ( AUC ) over all labels are summarized in Figure 1 for all techniques . The regularization parameter γ varies from 1e 6 to 1e6 . From Figure 1 we conclude that the proposed two stage approach always produces the same classification results as the direct approach in all cases .
A similar experiment is performed on the multi class data set wine [ 3 ] for LDA . The classification accuracies of LDA under different values of γ are summarized in Figure 2 , and similar observations can be made .
5.3 Scalability Comparison
In this experiment , we study the scalability of the twostage approach in comparison with the direct approach . Since regularization is commonly used in practice , we compare
0 fi2 under different values of the regularization parameter γ on the Table 2 : The value of fiWWT − W0WT synthetic and real world data sets . W0 is the solution of the generalized eigenvalue problem in Eq ( 5 ) by solving it directly , and W is the solution of Eq ( 5 ) by solving it using the two stage approach . Each row corresponds to a specific technique and each column corresponds to a specific value of the regularization parameter γ .
Data
Syn1
Syn2
Syn3
Syn4
Scene
Yeast
Technique
0
1.0e 006
1.0e 004
1.0e 002
1.0e+000
1.0e+002
1.0e+004
1.0e+006
LDA
LDA
CCA
OPLS
2.9e 018
3.6e 018
3.4e 018
3.1e 018
2.6e 018
2.5e 018
3.1e 019
3.0e 021
5.8e 019
1.4e 018
1.2e 018
8.9e 019
1.2e 018
9.9e 019
2.3e 019
2.9e 021
4.9e 018
8.4e 018
7.0e 018
6.5e 018
9.5e 018
6.0e 018
5.1e 019
7.2e 021
4.6e 018
5.0e 018
8.7e 018
5.0e 018
6.6e 018
6.1e 018
5.4e 019
5.0e 021
HSL Clique
1.0e 017
1.8e 017
1.2e 017
1.2e 017
1.5e 017
1.4e 017
2.9e 018
2.5e 020
HSL Star
1.4e 017
2.4e 017
9.3e 018
2.6e 017
2.1e 017
5.0e 017
9.8e 019
1.3e 020
CCA
OPLS
1.3e 018
5.2e 018
3.2e 018
1.8e 018
1.3e 018
1.8e 018
4.2e 019
5.9e 021
1.0e 018
1.1e 018
1.3e 018
1.5e 018
1.3e 018
1.3e 018
2.9e 019
5.9e 021
HSL Clique
2.7e 018
2.9e 018
2.7e 018
5.0e 018
3.2e 018
2.7e 018
8.9e 019
1.4e 020
HSL Star
2.5e 018
3.7e 018
2.9e 018
5.7e 018
4.1e 018
2.9e 018
1.1e 018
3.1e 020
CCA
OPLS
2.4e 015
2.1e 015
6.1e 015
3.7e 015
1.2e 015
1.8e 016
6.0e 018
9.0e 020
2.0e 015
3.4e 015
3.8e 015
2.5e 015
1.1e 015
2.3e 016
1.1e 017
1.4e 019
HSL Clique
4.5e 015
9.1e 015
2.6e 014
1.2e 014
3.6e 015
1.3e 015
5.9e 017
1.0e 018
HSL Star
4.6e 015
3.3e 014
2.1e 014
7.7e 015
1.1e 014
2.5e 016
1.0e 016
6.5e 019
CCA
OPLS
1.6e 012
1.5e 011
1.2e 012
1.4e 015
6.9e 016
5.9e 017
1.7e 018
1.4e 020
4.1e 012
1.6e 011
3.7e 012
1.2e 014
1.5e 015
3.7e 016
3.2e 018
2.9e 020
HSL Clique
1.5e 012
1.4e 011
3.7e 012
3.9e 015
1.6e 015
2.7e 016
5.1e 018
2.5e 020
HSL Star
2.1e 012
1.0e 011
2.4e 012
1.1e 014
9.4e 015
1.1e 015
1.5e 017
4.4e 019
Wine
Satimage
LDA
LDA
Ionosphere
LDA Optical digits LDA USPS
LDA
5.9e 017
2.1e 016
2.3e 016
2.1e 016
3.2e 017
2.2e 018
1.3e 020
2.0e 020
4.6e 016
2.2e 015
8.4e 016
7.3e 016
7.7e 016
8.1e 017
3.9e 017
6.2e 019
8.5e 018
1.0e 017
4.3e 018
2.1e 017
6.8e 018
6.6e 018
6.6e 020
1.1e 021
6.2e 018
7.2e 018
6.7e 018
5.7e 018
1.9e 018
1.5e 019
5.9e 020
5.6e 021
7.0e 015
3.0e 014
2.6e 014
6.6e 015
1.1e 016
3.0e 018
4.1e 019
6.6e 021 the scalability of different algorithms with regularization . In terms of the implementation , the least squares problem in the first stage of the two stage approach is solved using the LSQR algorithm [ 19 , 20 ] . For the direct approach which solves the generalized eigenvalue problem directly , the Lanczos algorithm [ 12 ] is applied . It is well known that solving large scale generalized eigenvalue problems is much more difficult than the regular eigenvalue problems [ 22 , 28 ] . In order to transform the generalized eigenvalue problem into the regular eigenvalue problem , we can factor XXT or apply the −1XSXT standard Lanczos algorithm for the matrix ( XXT ) using the XXT inner product [ 22 ] . Due to the issue of singularity for high dimensional data set with small regularization for the second method , in this paper we follow the procedure in [ 26 ] , which factors XXT and solves a symmetric eigenvalue problem using the Lanczos algorithm .
The computation time ( in log scale ) of different techniques on the large scale data set rcv1v2 is shown in Figure 4 and 5 . In Figure 4 , we increase the sample size from 500 to 3000 with a step size 500 , and the dimensionality is fixed at 5000 . The computation time of both approaches increases as the sample size increases . However , it can be observed that the computation time of the two stage approach is significantly less than that of the direct approach . In Figure 5 we fix the sample size at 3000 and increase the dimensionality from 500 to 5000 with a step size 500 . Similar observations can be made from Figure 5 .
We perform a similar experiment on the news20 data set for LDA ( multi class classification ) . The experimental results are summarized in Figure 3 . In Figure 3 ( left ) , we fix the dimensionality at 5000 and increase the sample size from 500 to 3000 with a step size 500 . In Figure 3 ( right ) , the dimensionality is increased from 500 to 3000 with a step size 500 while the sample size is fixed at 5000 . It can be observed from these figures that the two stage approach is much more efficient than the direct approach .
6 . CONCLUSIONS
In this paper we propose an efficient two stage approach for a class of dimensionality reduction techniques , including Canonical Correlation Analysis , Orthonormalized Partial Least Squares , Hypergraph Spectral Learning and Linear Discriminant Analysis . We rigorously prove the equivalence relationship between the two stage approach and the rLDA 2SrLDA
7 . REFERENCES [ 1 ] S . Agarwal , K . Branson , and S . Belongie . Higher order y c a r u c c A
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
[ 7 ] M . R . Boutell , J . Luo , X . Shen , and C . M . Brown . learning with graphs . In Proceedings of the 23rd International Conference on Machine Learning ( ICML ) , pages 17–24 , 2006 .
[ 2 ] J . Arenas Garcia and G . Camps Valls . Efficient kernel orthonormalized PLS for remote sensing applications . IEEE Transactions on Geoscience and Remote Sensing , 46(10):2872–2881 , 2008 .
[ 3 ] A . Asuncion and D . J . Newman . UCI machine learning repository , 2007 .
[ 4 ] F . R . Bach and M . I . Jordan . Kernel independent component analysis . Journal of Machine Learning Research , 3:1–48 , 2003 .
[ 5 ] R . E . Bellman . Adaptive Control Processes : A Guided
Tour . Princeton Universsity Press , Princeton , NJ , 1961 .
[ 6 ] C . M . Bishop . Pattern Recognition and Machine
Learning . Springer , New York , NY , 2006 .
Learning multi label scene classification . Pattern Recognition , 37(9):1757–1771 , 2004 .
[ 8 ] D . Cai . Spectral Regression : A Regression Framework for Efficient Regularized Subspace Learning . PhD thesis , Department of Computer Science , University of Illinois at Urbana Champaign , 2009 .
[ 9 ] D . Cai , X . He , and J . Han . SRDA : An efficient algorithm for large scale discriminant analysis . IEEE Transactions on Knowledge and Data Engineering , 20(1):1–12 , 2008 .
[ 10 ] A . Elisseeff and J . Weston . A kernel method for multi labelled classification . In Advances in Neural Information Processing Systems 13 ( NIPS ) , pages 681–687 , 2001 .
[ 11 ] K . Fukunaga . Introduction to Statistical Pattern
Recognition . Academic Press , New York , NY , 1990 .
[ 12 ] G . H . Golub and C . F . Van Loan . Matrix
Computations . Johns Hopkins Press , Baltimore , MD , 3rd edition , 1996 .
[ 13 ] D . R . Hardoon , S . R . Szedmak , and J . R .
Shawe taylor . Canonical correlation analysis : An overview with application to learning methods . Neural Computation , 16(12):2639–2664 , 2004 .
[ 14 ] T . Hastie , R . Tibshirani , and J . Friedman . The Elements of Statistical Learning : Data Mining , Inference , and Prediction . Springer , New York , NY , 2nd edition , 2009 .
[ 15 ] H . Hotelling . Relations between two sets of variables .
Biometrika , 28:312–377 , 1936 .
[ 16 ] P . Howland and H . Park . Two stage methods for linear discriminant analysis : Equivalent results at a lower cost . Technical report , Georgia Institute of Technology , 2009 .
[ 17 ] K . Lang . Newsweeder : Learning to filter netnews . In Proceedings of the 12th International Conference on Machine Learning ( ICML ) , pages 331–339 , 1995 .
[ 18 ] D . Lewis , Y . Yang , T . Rose , and F . Li . Rcv1 : A new benchmark collection for text categorization research . Journal of Machine Learning Research , 5:361–397 , 2004 .
−6
−5
−4
−3
−2 logγ
−1
0
1
2
3
Figure 2 : Comparison of different approaches in terms of classification accuracy for LDA on the wine data set as the regularization parameter γ varies .
2SrLDA rLDA
6
5
4
3
2
1
) t ( g o l
2SrLDA rLDA
6
5
4
3
2
1
0
) t ( g o l
0 500
1000
1500 2000 Sample Size
2500
3000
−1 500
1000
1500 2000 Dimensionality
2500
3000
Figure 3 : Scalability comparison on the news20 data set as the sample size ( left ) and dimensionality ( right ) increase . The horizontal axis is the sample size ( left ) or the dimensionality ( right ) , and the vertical axis is log(t ) , where t is the computation time ( in seconds ) . direct approach which solves the generalized eigenvalue problem directly . Compared with previous work , one appealing feature of the two stage approach is that no assumption is required for the equivalence relationship . In addition , the two stage approach can be further extended to the regularization setting . We show that the proposed two stage approach scales linearly in terms of both the sample size and data dimensionality . We have performed extensive experiments on both synthetic and real world data sets . Our experimental results confirm the established equivalence relationship . Results also demonstrate the scalability of the proposed two stage approach .
The current two stage approach assumes that the complete data set for training is given in advance , and learning is carried out in one batch . However , in many real applications the data come as a stream . We plan to explore the online algorithm for the class of dimensionality reduction techniques .
Acknowledgements This work was supported by NSF IIS 0612069 , IIS 0812551 , IIS 0953662 , NGA HM1582 08 1 0016 , the Office of the Director of National Intelligence ( ODNI ) , Intelligence Advanced Research Projects Activity ( IARPA ) , through the US Army .
2SrCCA rCCA
6
5
4
3
2
1
0
) t ( g o l
2SrOPLS rOPLS
6
5
4
3
2
1
0
) t ( g o l
2SrHSL Clique rHSL Clique
6
5
4
3
2
1
0
) t ( g o l
6
5
4
3
2
1
0
) t ( g o l
2SrHSL Star rHSL Star
−1 500
1000
1500 2000 Sample Size
( A ) CCA
2500
3000
−1 500
1000
1500 2000 Sample Size
2500
3000
−1 500
1000
1500 2000 Sample Size
2500
3000
−1 500
1000
1500 2000 Sample Size
2500
3000
( B ) OPLS
( C ) HSL Clique
( D ) HSL Star
Figure 4 : Scalability comparison on the rcv1v2 data set as the sample size increases . The horizontal axis is the sample size , and the vertical axis is log(t ) , where t is the computation time ( in seconds ) .
2SrCCA rCCA
6
4
2
0
) t ( g o l
2SrOPLS rOPLS
6
4
2
0
) t ( g o l
6
4
2
0
) t ( g o l
2SrHSL Clique rHSL Clique
) t ( g o l
6
5
4
3
2
1
0
2SrHSL Star rHSL Star
−2
1000
2000
3000
Dimensionality ( A ) CCA
4000
5000
−2
1000
2000
3000
Dimensionality ( B ) OPLS
4000
5000
−2
1000
2000
3000
Dimensionality
4000
5000
−1
1000
2000
3000
4000
5000
Dimensionality
( C ) HSL Clique
( D ) HSL Star
Figure 5 : Scalability comparison on the rcv1v2 data set as the dimensionality increases . The horizontal axis is the dimensionality , and the vertical axis is log(t ) , where t is the computation time ( in seconds ) .
[ 19 ] C . C . Paige and M . A . Saunders . Algorithm 583 : LSQR : Sparse linear equations and least squares problems . ACM Transactions on Mathematical Software , 8(2):195–209 , 1982 . between canonical correlation analysis and orthonormalized partial least squares . In Proceedings of the 21st International Jont Conference on Artifical Intelligence ( IJCAI ) , pages 1230–1235 , 2009 .
[ 20 ] C . C . Paige and M . A . Saunders . LSQR : An algorithm
[ 28 ] DS Watkins . Fundamentals of matrix computations . for sparse linear equations and sparse least squares . ACM Transactions on Mathematical Software , 8(1):43–71 , 1982 .
[ 21 ] R . Rosipal and N . Kr¨amer . Overview and recent advances in partial least squares . In Subspace , Latent Structure and Feature Selection Techniques , Lecture Notes in Computer Science , pages 34–51 , 2006 .
[ 22 ] Y . Saad . Numerical Methods for Large Eigenvalue
Problems . Halsted Press , New York , NY , 1992 .
[ 23 ] B . Sch¨olkopf and A . J . Smola . Learning with kernels : support vector machines , regularization , optimization , and beyond . MIT Press , Cambridge , MA , 2002 .
[ 24 ] L . Sun , S . Ji , and J . Ye . Hypergraph spectral learning for multi label classification . In Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining ( KDD ) , pages 668–676 , 2008 .
[ 25 ] L . Sun , S . Ji , and J . Ye . A least squares formulation for canonical correlation analysis . In Proceedings of the 25th International Conference on Machine Learning ( ICML ) , pages 1024–1031 , 2008 .
[ 26 ] L . Sun , S . Ji , and J . Ye . A least squares formulation for a class of generalized eigenvalue problems in machine learning . In Proceedings of the 26th International Conference on Machine Learning ( ICML ) , pages 977–984 , 2009 .
[ 27 ] L . Sun , S . Ji , S . Yu , and J . Ye . On the equivalence
John Wiley & Sons , Inc . , New York , NY , 1991 .
[ 29 ] S . Wold and et al . Chemometrics , mathematics and statistics in chemistry . Reidel Publishing Company , Dordrecht , Holland , 1984 .
[ 30 ] K . Worsley , J B Poline , K . J . Friston , and AC
Evans . Characterizing the response of PET and fMRI data using multivariate linear models . Neuroimage , 6(4):305–319 , 1997 .
[ 31 ] J . Ye . Characterization of a family of algorithms for generalized discriminant analysis on undersampled problems . Journal of Machine Learning Research , 6:483–502 , 2005 .
[ 32 ] J . Ye and Q . Li . A two stage linear discriminant analysis via QR decomposition . IEEE Transactions on Pattern Analysis and Machine Intelligence , 27:929–941 , 2005 .
[ 33 ] Z . Zhang , G . Dai , and M . Jordan . A flexible and efficient algorithm for regularized fisher discriminant analysis . In Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases ( ECML PKDD ) , pages 632–647 . Springer , 2009 .
[ 34 ] D . Zhou , J . Huang , and B . Sch¨olkopf . Learning with hypergraphs : Clustering , classification , and embedding . In Advances in Neural Information Processing Systems 18 ( NIPS ) , pages 1601–1608 , 2006 .
