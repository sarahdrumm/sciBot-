Online Multiscale Dynamic Topic Models
Tomoharu Iwata
Takeshi Yamada
Yasushi Sakurai
Naonori Ueda
NTT Communication Science Laboratories
2 4 Hikaridai , Seika cho , Soraku gun , Kyoto , Japan
{iwata,yamada,yasushi,ueda}@cslabkeclnttcojp
ABSTRACT We propose an online topic model for sequentially analyzing the time evolution of topics in document collections . Topics naturally evolve with multiple timescales . For example , some words may be used consistently over one hundred years , while other words emerge and disappear over periods of a few days . Thus , in the proposed model , current topicspeciflc distributions over words are assumed to be generated based on the multiscale word distributions of the previous epoch . Considering both the long timescale dependency as well as the short timescale dependency yields a more robust model . We derive e–cient online inference procedures based on a stochastic EM algorithm , in which the model is sequentially updated using newly obtained data ; this means that past data are not required to make the inference . We demonstrate the efiectiveness of the proposed method in terms of predictive performance and computational e–ciency by examining collections of real documents with timestamps .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications| Data Mining ; I26 [ Artiflcial Intelligence ] : Learning ; I51 [ Pattern Recognition ] : Model|Statistical
General Terms Algorithms
Keywords Topic model , Time series analysis , Online learning
1 .
INTRODUCTION
Great interest is being shown in developing topic models that can analyze and summarize the dynamics of document collections , such as scientiflc papers , news articles , and blogs [ 1 , 5 , 7 , 11 , 14 , 20 , 21 , 22 ] . A topic model is a hierarchical probabilistic model , in which a document is modeled as a mixture of topics , and a topic is modeled as a probability distribution over words . Topic models are successfully used in a wide variety of applications including information retrieval [ 6 ] , collaborative flltering [ 10 ] , and visualization [ 12 ] as well as the analysis of dynamics .
In this paper , we propose a topic model that permits the sequential analysis of the dynamics of topics with multiple timescales , we call it the Multiscale Dynamic Topic Model ( MDTM ) , and its e–cient online inference procedures . Topics naturally evolve with multiple timescales . Let us consider the topic ‘politics’ in a news article collection as an example . There are some words that appear frequently over many years , such as ‘constitution’ , ‘congress’ , and ‘president’ . On the other hand , some words , such as the names of members in Congress , may appear frequently over periods of tens of years , and other words , such as the names of bills under discussion , may appear for only a few days . Thus , in MDTM , current topic speciflc distributions over words are assumed to be generated based on the estimates of multiple timescale word distributions at the previous epoch . Using these multiscale priors improves the predictive performance of the model because the information loss is reduced by considering the long timescale dependency as well as short timescale dependency .
The online inference and parameter estimation processes can be achieved e–ciently based on a stochastic EM algorithm , in which the model is sequentially updated using newly obtained data ; past data does not need to be stored and processed to make new inferences . Some topics may exhibit strong long timescale dependence , and others may exhibit strong short timescale dependence . Furthermore , the dependence may difier over time . Therefore , we infer these dependencies for each timescale , for each topic , and for each epoch . By inferring the dependencies from the observed data , MDTM can ( cid:176)exibly adapt to topic dynamics . A disadvantage of online inference is that it can be more unstable than batch inference . With MDTM , the stability can be improved by smoothing using multiple estimates with difierent timescales .
The remainder of this paper is organized as follows . In Section 2 , we formulate a topic model for multiscale dynamics , and describe its online inference procedures . In Section 3 , we brie(cid:176)y review related work . In Section 4 , we demonstrate the efiectiveness of the proposed method by analyzing the dynamics of real document collections . Finally , we present concluding remarks and a discussion of future work in Section 5 .
663 Table 1 : Notation w
Symbol Description Dt Nt;d x
( 4 ) t 1,z w x
( 3 ) t 1,z w x
( 2 ) t 1,z x
( 1 ) t 1,z w
,4
,3 t zl , t zl , t zl ,
,2 t zl ,
,1 f t,z
,0 t zl , t 1,zx
( 0 ) t 8 t 4 t 2 t 1 w
Figure 2 : Illustration of multiscale word distributions at epoch t with S = 4 . Each histogram shows »(s ) t¡1;z , which is a multinomial distribution over words with timescale s . multiscale word distributions at the previous epoch ,
`t;z » Dirichlet(
S
Xs=0
‚t;z;s»(s ) t¡1;z ) ;
( 1 ) t¡1;zgS where ‚t;z;s is a weight for scale s in topic z at epoch t , and ‚t;z;s > 0 . By estimating weights f‚t;z;sgS s=0 for each epoch , for each topic , and for each timescale using the current data as described in Section 2.3 , MDTM can ( cid:176)exibly respond to the in(cid:176)uence on the current distribution of the previous short and long timescale distributions . The estimated multiscale word distributions f»(s ) s=1 at the previous epoch are considered as hyperparameters in the current epoch . Their estimation will be explained in Section 24 There are many difierent ways of setting the scales , but for the simple explanation , we set them so that »(s ) t;z indicates the word distribution from t ¡ 2s¡1 + 1 to t , where larger s represents longer timescale , and »(s=1 ) is equivalent to the estimate of unit time word distribution `t;z . We use uniform word distribution »(s=0 ) t;z;w = W ¡1 for scale s = 0 . This uniform distribution is used to avoid the zero probability problem . Figure 2 illustrates multiscale word distributions with this setting . Word distributions are likely to be smoothed as the timescale becomes long , and be peaked as the timescale becomes short . By using the information presented in these various timescales as the prior for the current distribution with weights , we can infer the current distribution more robustly . In stead of using 2s¡1 epochs for scale s , we can use any number of epochs . For example , if we know that the given data exhibit periodicity eg of one week and one month , we can use the scale of one week for s = 1 and one month for s = 2 . In such case , we can still estimate parameters in the similar way with the algorithm described in Section 24 Typically , we do not know the periodicity of the given data in advance , we therefore consider the simple scale setting in the paper . t;z
In LDA , topic proportions ( cid:181)t;d are sampled from a Dirichlet distribution . In order to capture the dynamics of topic proportions with MDTM , we assume that the Dirichlet parameters fit = ffit;zgZ z=1 depend on the previous parameters . In particular , we use the following Gamma prior for a Dirichlet parameter of topic z at epoch t , fit;z » Gamma((cid:176)fit¡1;z ; ( cid:176) ) ;
( 2 ) where the mean is fit¡1;z , and the variance is fit¡1;z=(cid:176 ) . By using this prior , the mean is the same as that at the previous epoch unless otherwise indicated by the new data . Parame
W wt;d;n
Z zt;d;n
S ( cid:181)t;d
`t;z
»(s ) t;z number of documents at epoch t number of words in the dth document at epoch t number of unique words nth word in the dth document at epoch t , wt;d;n 2 f1 ; ¢ ¢ ¢ ; W g number of topics topic of the nth word in the dth document at epoch t , zt;d;n 2 f1 ; ¢ ¢ ¢ ; Zg number of scales multinomial distribution over topics for the dth document at epoch t , ( cid:181)t;d = f(cid:181)t;d;zgZ multinomial distribution over words for the zth topic at epoch t , `t;z = f`t;z;wgW multinomial distribution over words for the zth topic with scale s at epoch t , »(s ) t;z = f»(s ) z=1 , ( cid:181)t;d;z ‚ 0 , Pz ( cid:181)t;d;z = 1 w=1 , `t;z;w ‚ 0 , Pw `t;z;w = 1 w=1 , »(s ) t;z;w = 1 t;z;wgW t;z;w ‚ 0 , Pw »(s )
2 . PROPOSED METHOD
2.1 Preliminaries
In the proposed model , documents are assumed to be generated sequentially at each epoch . Suppose we have a set of Dt documents at the current epoch , t , and each document Nt;d is represented by wt;d = fwt;d;ng n=1 , ie the set of words in the document . Our notation is summarized in Table 1 . We assume that epoch t is a discrete variable , and we can set the time period for an epoch arbitrarily at , for example , one day or one year .
Before introducing the proposed model , we review latent Dirichlet allocation ( LDA ) [ 6 , 8 ] , which forms the basis of the proposed model . In LDA , each document has topic proportions ( cid:181)t;d . For each of the Nt;d words in the document , topic zt;d;n is chosen from the topic proportions , and then word wt;d;n is generated from a topic speciflc multinomial distribution over words `zt;d;n . Topic proportions ( cid:181)t;d and word distributions `z are assumed to be generated according to symmetric Dirichlet distributions . Figure 1 ( a ) shows a graphical model representation of LDA , where shaded and unshaded nodes indicate observed and latent variables , respectively .
2.2 Model
We consider a set of multiple timescale distributions over words for each topic to incorporate multiple timescale properties . In order to account for the in(cid:176)uence of the past at difierent timescales to the current epoch , we assume that current topic speciflc word distributions `t;z are generated according to the multiscale word distributions at the previous epoch f»(s ) w=1 represents a distribution over words of topic z with scale s at epoch t ¡ 1 . In particular , we use the following asymmetric Dirichlet distribution for the prior of current word distribution `t;z , in which the Dirichlet parameter is deflned so that its mean becomes proportional to the weighted sum of s=1 . Here , »(s ) t¡1;z = f»(s ) t¡1;z;wgW t¡1;zgS
664 a q z w f b
N D
Z
( a ) LDA t a q z w f x
N D l S+1
Z ( b ) MDTM t 1 a q z w f
N D t a q z w f
N D
N^ l S+1
Z
N^ l S+1
Z
( c ) online MDTM
Figure 1 : Graphical models of ( a ) latent Dirichlet allocation , ( b ) the multiscale dynamic topic model , and ( c ) its online inference version . ter ( cid:176 ) controls temporal consistency of the topic proportion prior .
Assuming that we have already calculated the multiscale parameters at epoch t ¡ 1 , ¥t¡1 = ff»(s ) z=1 and fit¡1 = ffit¡1;zgZ z=1 , MDTM is characterized by the following generative process for the set of documents Wt = fwt;dgDt t¡1;zgS s=0gZ d=1 at epoch t ,
1 . For each topic z = 1 ; ¢ ¢ ¢ ; Z :
( a ) Draw topic proportion prior fit;z » Gamma((cid:176)fit¡1;z ; ( cid:176) ) ;
( b ) Draw word distribution
`t;z » Dirichlet(Ps ‚t;z;s»(s ) t¡1;z ) ;
2 . For each document d = 1 ; ¢ ¢ ¢ ; Dt :
( a ) Draw topic proportions
( cid:181)t;d » Dirichlet(fit ) ;
( b ) For each word n = 1 ; ¢ ¢ ¢ ; Nt;d : i . Draw topic zt;d;n » Multinomial((cid:181)t;d ) ; ii . Draw word wt;d;n » Multinomial(`t;zt;d;n ) :
Figure 1 ( b ) shows a graphical model representation of MDTM .
2.3 Online inference
We present an online inference algorithm for MDTM , that sequentially updates the model at each epoch using the newly obtained document set and the multiscale model of the previous epoch . The information in the data up to and including the previous epoch is aggregated into the previous multiscale model . The online inference and parameter estimation can be e–ciently achieved by a stochastic EM algorithm [ 2 , 3 ] , in which the collapsed Gibbs sampling of latent topics [ 8 ] and the maximum likelihood estimation of hyperparameters are alternately performed [ 19 ] .
We assume the set of documents Wt at current epoch t , and estimates of parameters from the previous epoch fit¡1 and ¥t¡1 are given . The joint distribution on the set of documents , the set of topics , and the topic proportion priors given the parameters are deflned as follows ,
P ( Wt ; Zt ; fitjfit¡1 ; ( cid:176 ) ; ¥t¡1 ; ⁄t )
= P ( fitjfit¡1 ; ( cid:176))P ( Ztjfit)P ( WtjZt ; ¥t¡1 ; ⁄t ) ;
( 3 ) where Zt = ffzt;d;ng ⁄t = ff‚t;z;sgS s=0gZ term on the right hand side of ( 3 ) is as follows using ( 2 ) , d=1 represents a set of topics , and z=1 represents a set of weights . The flrst
Nt;d n=1 gDt
P ( fitjfit¡1 ; ( cid:176 ) ) = Yz
( cid:176)(cid:176)fit¡1;z fi
( cid:176)fit¡1;z ¡1 t;z ¡((cid:176)fit¡1;z ) exp(¡(cid:176)fit;z )
; ( 4 ) where ¡(¢ ) is the gamma function . We can integrate out the multinomial distribution parameters in MDTM , f(cid:181)t;dgDt d=1 and f`t;zgZ z=1 , by taking advantage of Dirichlet multinomial conjugacy . The second term is calculated by P ( Ztjfit ) = ing equation by integrating out f(cid:181)t;dgDt
QDt d=1R P ( zt;dj(cid:181)t;d)P ( (cid:181)t;djfit)d(cid:181)t;d , and we have the followQz ¡(fit;z)¶D P ( Ztjfit ) = ( cid:181 ) ¡(Pz fit;z )
Qz ¡(Nt;d;z + fit;z ) ¡(Nt;d +Pz fit;z ) where Nt;d;z is the number of words in the dth document
Yd
; ( 5 ) d=1 , assigned to topic z at epoch t , and Nt;d = Pz Nt;d;z . Sim z=1 , the third term is given ilarly , by integrating out f`t;zgZ as follows ,
P ( WtjZt ; ¥t¡1 ; ⁄t ) = Yz
¡(Ps ‚t;z;s ) Qw ¡(Ps ‚t;z;s»(s ) £Qw ¡(Nt;z;w +Ps ‚t;z;s»(s ) ¡(Nt;z +Ps ‚t;z;s ) t¡1;z;w ) t¡1;z;w )
;
( 6 ) where Nt;z;w is the number of times word w was assigned to topic z at epoch t , and Nt;z = Pw Nt;z;w .
The inference of the latent topics Zt can be e–ciently computed by using collapsed Gibbs sampling [ 8 ] . Let j = ( t ; d ; n ) for notational convenience , and zj be the assignment of a latent topic to the nth word in the dth document
665 at epoch t . Then , given the current state of all but one variable zj , a new value for zj is sampled from the following probability ,
P ( zj = kjWt ; Ztnj ; fit ; ¥t¡1 ; ⁄t )
/
Nt;d;knj + fit;k
Nt;dnj +Pz fit;z
Nt;k;wj nj +Ps ‚t;s;k»(s ) Nt;knj +Ps ‚t;s;k t¡1;k;wj
;(7 ) where nj represents the count yielded by excluding the nth word in the dth document .
The parameters fit and ⁄t are estimated by maximizing the joint distribution ( 3 ) . The flxed point iteration method described in [ 13 ] can be used for maximizing the joint distribution as follows , fit;z ˆ
( cid:176)fit¡1;z ¡ 1 + fit;z Pd( “ ( Nt;d;z + fit;z ) ¡ “ ( fit;z ) ) ( cid:176 ) +Pd( “ ( Nt;d +Pz0 fit;z0 ) ¡ “ ( Pz0 fit;z0 ) )
( 8 ) where “ ( ¢ ) is a digamma function deflned by “ ( x ) = @ log ¡(x ) and ,
@x
;
‚t;z;s ˆ ‚t;z;sPw »(s )
Bt;z t¡1;z;wAt;z;w
;
( 9 ) where
At;z;w = “ ( Nt;z;w+Xs0
‚t;z;s0 »(s0 ) t¡1;z;w)¡ “ ( Xs0
‚t;z;s0 »(s0 ) t¡1;z;w ) ;
Bt;z = “ ( Nt;z +Xs0
‚t;z;s0 ) ¡ “ ( Xs0
‚t;z;s0 ) :
( 10 )
( 11 )
By iterating Gibbs sampling with ( 7 ) and maximum likelihood estimation with ( 8 ) and ( 9 ) , we can infer latent topics while optimizing the parameters . Since MDTM uses the past distributions as the current prior , the label switching problem [ 17 ] is not likely to occur when estimated ‚t;z;s is high , which implies current topics strongly depend on the previous distributions . Label switching can occur when estimated ‚t;z;s is low . By allowing low ‚t;z;s , which is estimated from the given data at each epoch and each topic , MDTM can adapt ( cid:176)exibly to changes even if existing topics disappear and new topics appear in midstream .
2.4 Ef.cient estimation of multiscale word dis tributions
By using the topic assignments obtained after iterating the stochastic EM algorithm , we can estimate multiscale word distributions . Since »(s ) t;z;w represents the probability of word w in topic z from t ¡ 2s¡1 + 1 to t , the estimation is as follows ,
;
( 12 )
»(s ) t;z;w = t;z;w
^N ( s ) Pw
^N ( s ) t;z;w
= Pt Pw Pt t0=t¡2s¡1+1 t0=t¡2s¡1+1
^Nt0;z;w
^Nt0;z;w where ^N ( s ) t;z;w is the expected number of times word w was assigned to topic z from t ¡ 2s + 1 to t , and ^Nt;z;w is the expected number of times at t . The expected number is calculated by ^Nt;z;w = Nt;z ^`t;z;w , where ^`t;z;w is a point estimate of the probability of word w in topic z at epoch t . Although we integrate out `t;z;w , we can recover its point estimate as follows ,
^`t;z;w =
Nt;z;w +Ps ‚t;z;s»(s ) Nt;z +Ps ‚t;z;s t¡1;z;w
:
( 13 ) if t mod 2s¡1 = 0 then
^N ( s ) t;z;w ˆ ^Nt;z;w
1 : ^N ( 1 ) 2 : for s = 2 ; ¢ ¢ ¢ ; S do 3 : 4 : 5 : ^N ( s ) 6 : 7 : end if 8 : end for t;z;w ˆ ^N ( s ) t;z;w ˆ ^N ( s¡1 ) else t¡1;z;w t;z;w + ^N ( s¡1 ) t¡1;z;w
Figure 3 : Algorithm for the approximate update of ^N ( s ) t;z;w .
While it is simpler to use the actual number of times , Nt;z;w , instead of the expected number of times , ^Nt;z;w , in ( 12 ) , we use the latter in order to constrain the estimate of »(s=1 ) t;z;w to be the estimate of `t;z;w as follows ,
,
»(s=1 ) t;z;w =
^Nt;z;w Pw Note that the value ^N ( s ) from the previous value ^N ( s )
^Nt;z;w t;z;w can be updated sequentially t¡1;z;w as follows ,
= ^`t;z;w :
( 14 )
^N ( s ) t;z;w ˆ ^N ( s ) t¡1;z;w + ^Nt;z;w ¡ ^Nt¡2s¡1;z;w :
( 15 )
Therefore , ^N ( s ) instead of 2s¡1 additions . t;z;w can be updated through just two additions
However , to update ^N ( s ) t;z;w , we still need to store values ^Nt;z;w from t ¡ 2S¡1 to t ¡ 1 , which means that O(2S¡1ZW ) memory is required in total for updating multiscale word distributions . Since the memory requirement increases exponentially with the number of scales , this requirement prevents us from modeling long timescale dynamics . Thus , we consider approximating the update by decreasing the update frequency for long timescale distributions as in Algorithm 3 ; this reduces the memory requirement to O(SZW ) , which is linear against the number of scales . Figure 4 illustrates approximate updating ^N ( s ) t;z;w with S = 3 from t = 4 to t = 8 . Each rectangle represents ^Nt0;z;w , where the number represents t0 . Each row at each epoch represents ^N ( s ) t;z;w , and shaded rectangles represent that the values that differ from the previous values . t;z;w is updated at every 2s¡1nd epoch . Since the dynamics of a word distribution for a long timescale is considered to be slower than that for a short timescale , this approximation , decreasing the update frequency for long timescale distributions , is reasonable . Updating ^N ( s ) t;z;w with this approximation requires us to store only the previous ^N ( s¡1 ) t;z;w values , and so the memory requirement is O(SZW ) . Figure 1 ( c ) shows a graphical model representation of online inference in MDTM .
^N ( s )
For the Dirichlet prior parameter of the word distribution , we use the weighted sum of the multiscale word distributions as in ( 1 ) . The parameter can be rewritten as the weighted sum of the word distributions for each epoch as follows ,
S
Xs=1
‚t;z;s»(s ) t¡1;z;w = t¡1
Xt0=t¡2S¡1 t;z;t0 ^`t0;z;w ; ‚0
( 16 )
666 t=4 t=4 t=5 t=5 t=6 t=6 t=7 t=7 t=8 t=8
1 2 3 4 1 2 3 4 1 2 3 4
1 2 3 4 1 2 3 4 1 2 3 4
1 2 3 4 1 2 3 4 1 2 3 4
1 2 3 4 1 2 3 4 1 2 3 4
5 6 7 8 5 6 7 8 5 6 7 8
3 4 3 43 4
4 4
3 4 3 43 4
5 5
5 6 5 65 6
6 6
5 6 5 65 6
7 7
7 8 7 87 8
8 8 s=3 s=3 s=2 s=2 s=1 s=1 is inappropriate for discrete data such as document collections [ 9 ] .
4 . EXPERIMENTS
5 5
6 6
7 7
8 8
4.1 Setting
Figure 4 : Illustration of approximate updating ^N ( s ) from t = 4 to t = 8 with S = 3 . t;z;w
^Nt0;z;w t00=t¡2s¡1 ^Nt00;z;w
;
( 17 ) where
S
‚0 t;z;t0 =
Xs=dlog2(t¡t0+1)+1e
‚t;z;sPw
Pw Pt¡1 is its weight . See Appendix for the derivation . Therefore , the multiscale dynamic topic model can be seen as an approximation of a model that depends on the word distributions for each of the previous epochs . By considering multiscale word distributions , the number of weight parameters ⁄t can be decreased from O(2S¡1Z ) to O(SZ ) , and this leads to more robust inference . Furthermore , the use of multiscaling also decreases the memory requirement from O(2S¡1ZW ) to O(SZW ) as described above .
3 . RELATED WORK
A number of methods for analyzing the evolution of topics in document collections have been proposed , such as the dynamic topic model [ 5 ] , topic over time [ 21 ] , online latent Dirichlet allocation [ 1 ] , and topic tracking model [ 11 ] . However , none of the above methods take account of multiscale dynamics . For example , the dynamic topic model ( DTM ) [ 5 ] depends only on the previous epoch distribution . On the other hand , MDTM depends on multiple distributions with difierent timescales . Therefore , with MDTM , we can model the multiple timescale dependency , and so infer the current model more robustly . Moreover , while DTM uses a Gaussian distribution to account for the dynamics , the proposed model uses conjugate priors . Therefore , inference in MDTM is relatively simple compared to that in DTM .
The multiscale topic tomography model ( MTTM ) [ 14 ] can analyze the evolution of topics at various resolutions of timescales by assuming non homogeneous Poisson processes . In contrast , MDTM models the topic evolution within the Dirichletmultinomial framework as the same with most topic models including latent Dirichlet allocation [ 6 ] . Another advantage of MDTM over MTTM is that it can make inferences in an online fashion . Therefore , MDTM can greatly reduce the computational cost as well as the memory requirements because past data need not be stored . Online inference is essential for modeling the dynamics of document collections , in which large numbers of documents continue to accumulate at any given moment , such as news articles and blogs , because it is necessary to adapt to the new data immediately for topic tracking , and it is impractical to prepare su–cient memory capacity to store all past data . Online inference algorithms for topic models have been proposed [ 1 , 4 , 7 , 11 ] . Singular value decomposition ( SVD ) is used for analyzing multiscale patterns in streaming data [ 15 ] as well as topic models . However , since SVD assumes Gaussian noise , it
We evaluated the multiscale dynamic topic model with online inference ( MDTM ) using four real document collections with timestamps : NIPS , PNAS , Digg , and Addresses .
The NIPS data consists of papers from the NIPS ( Neural Information Processing Systems ) conference from 1987 to 1999 . There were 1,740 documents , and the vocabulary size was 14,036 . The unit epoch was set to one year , so there were 13 epochs . The PNAS data consists of the titles of papers that appeared in the Proceedings of the National Academy of Sciences from 1915 to 2005 . There were 79,477 documents , and the vocabulary size was 20,534 . The unit epoch was set at one year , so there were 91 epochs . The Digg data consists of blog posts that appeared in the social news website Digg ( http://digg.com ) from January 29th to February 20th in 2009 . There were 108,356 documents , and the vocabulary size was 23,494 . The unit epoch was set at one day , so there were 23 epochs . The Addresses data consists of the State of the Union addresses from 1790 to 2002 . We increased the number of documents by splitting each transcript into 3 paragraph \documents" as done in [ 21 ] . We omitted words that occurred in fewer than 10 documents . There were 6,413 documents , and the vocabulary size was 6,759 . The unit epoch was set at one year , and excluding the years for which data was missing there were 205 epochs . We omitted stop words from all data sets .
We compared MDTM to DTM , LDAall , LDAone , and LDAonline . DTM is a dynamic topic model with online inference that does not take multiscale distributions into consideration ; it corresponds to MDTM with S = 1 . Note that DTM used here models dynamics with Dirichlet priors while the original DTM with Gaussian priors . LDAall , LDAone , and LDAonline are based on LDA , and so do not model the dynamics . LDAall is an LDA that uses all past data for inference . LDAone is an LDA that uses just the current data for inference . LDAonline is an online learning extension of LDA , in which the parameters are estimated using those of the previous epoch and the new data [ 4 ] . For a fair comparison , the hyperparameters in these LDAs were optimized using stochastic EM as described by Wallach [ 19 ] . We set the number of latent topics at Z = 50 for all models . In MDTM , we used ( cid:176 ) = 1 , and we estimated the Dirichlet prior for topic proportions subject to fit;z ‚ 10¡2 in order to avoid overfltting . We set the number of scales so that one of the multiscale distributions covered the entire period , or S = dlog2 T + 1e , where T is the number of epochs . We did not compare with the multiscale topic tomography model ( MTTM ) because the perplexity of MTTM was worse than that of LDA in [ 14 ] and MDTM has a clear advantage over MTTM in that MDTM can make inferences in an online fashion .
We evaluated the predictive performance of each model using the perplexity of held out words ,
Perplexity = exp0
@¡PdP t;d;njt ; d ; Dt )
N test n=1 log P ( wtest t;d Pd N test t;d
1 A ;
( 18 )
667 where N test t;d is the number of held out words in the dth document at epoch t , wtest t;d;n is the nth held out words in the document , and Dt represents training samples until epoch t . A lower perplexity represents higher predictive performance . We used half of the words in 10 % of the documents as held out words for each epoch , and used the other words as training samples . We created ten sets of training and test data by random sampling , and evaluated the average perplexity over the ten data sets .
4.2 Results
The average perplexities over the epochs are shown in Table 2 , and the perplexities for each epoch are shown in Figure 5 . For all data sets , MDTM achieved the lowest perplexity , which implies that MDTM can appropriately model the dynamics of various types of data sets through its use of multiscale properties . DTM had higher perplexity than MDTM because it could not model the long timescale dependencies . The reason for the high perplexities of LDAall and LDAonline is that they do not consider the dynamics . The perplexity achieved by LDAone is high because it uses only current data and ignores the past information .
The average perplexities over epochs with difierent numbers of topics are shown in Figure 6 . Under the same number of topics , MDTM achieved the lowest perplexities in all of the cases except when Z = 150 and 200 in the NIPS data . Even if the number of topics of the other models increases , the perplexities of the other models did not become better than that of our model with fewer topics in PNAS , Digg , and Addresses data . This result indicates that the larger number of parameters of our model is not a major reason for the lower perplexity .
The average perplexities over epochs with difierent numbers of scales in MDTM are shown in Figure 7 . Note that s = 0 uses the uniform distribution only , while s = 1 uses the uniform distribution and the previous epoch ’s distribution . The perplexities decreased as the number of scales increased . This result indicates the importance of considering multiscale distributions .
Figure 8 shows the average computational time per epoch when using a computer with a Xeon5355 2.66GHz CPU . The computational time for MDTM is roughly linear against the number of scales . Even though MDTM considers multiple timescale distributions , its computational time is much smaller than that of LDAall which considers a single timescale distribution . This is because that MDTM uses only current samples for inference , in contrast , LDAall uses all samples for inference .
Figure 9 shows the estimated ‚t;z;s with difierent numbers of scales s in MDTM . The sum of the values for each epoch and for each topic are normalized to one . The parameters decrease as the timescale lengthens . This result implies that recent distributions are more informative as regards estimating current distributions , which is intuitively reasonable .
Figure 10 shows two topic examples of the multiscale topic evolution in NIPS data analyzed by MDTM . Note that we omit words appeared in the longer timescales from the table . In the longest timescale , basic words for the research fleld are appropriately extracted , such as ‘speech’ , ‘recognition’ , and ‘speaker’ in the speech recognition topic , ‘control’ , ‘action’ , ‘policy’ , and ‘reinforcement’ in the reinforcement learning topic . In the shorter timescale , we can see the evolution of trends in the research . For example , in the speech recognition research , phoneme classiflcation is a popular task until 1995 , and probabilistic approaches such as hidden Markov models ( HMM ) from 1996 are frequently used .
5 . CONCLUSION
In this paper , we have proposed a topic model with multiscale dynamics and e–cient online inference procedures . We have conflrmed experimentally that the proposed method can appropriately model the dynamics in document data by considering multiscale properties , and that it is computationally e–cient .
In future work , we could determine the unit time interval and the length of scale automatically from the given data . We assumed that the number of topics was known and flxed over time . We can automatically infer the number of topics by extending the model to a nonparametric Bayesian model such as the Dirichlet process mixture model [ 16 , 18 ] . Since the proposed method is applicable to various kinds of discrete data with timestamps , such as web access log , blog , and e mail , we will evaluate the proposed method further by applying it to other data sets .
6 . REFERENCES [ 1 ] L . AlSumait , D . Barbara , and C . Domeniconi . On line
LDA : Adaptive topic models for mining text streams with applications to topic detection and tracking . In ICDM ’08 , pages 3{12 , 2008 .
[ 2 ] C . Andrieu , N . de Freitas , A . Doucet , and M . I . Jordan . An introduction to MCMC for machine learning . Machine Learning , 50(1):5{43 , 2003 .
[ 3 ] A . Asuncion , M . Welling , P . Smyth , and Y . W . Teh .
On smoothing and inference for topic models . In UAI ’09 , pages 27{34 , 2009 .
[ 4 ] A . Banerjee and S . Basu . Topic models over text streams : A study of batch and online unsupervised learning . In SDM ’07 , 2007 .
[ 5 ] D . M . Blei and J . D . Lafierty . Dynamic topic models .
In ICML ’06 , pages 113{120 , 2006 .
[ 6 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent
Dirichlet allocation . Journal of Machine Learning Research , 3:993{1022 , 2003 .
[ 7 ] K . R . Canini , L . Shi , and T . L . Gri–ths . Online inference of topics with latent Dirichlet allocation . In AISTATS ’09 , volume 5 , pages 65{72 , 2009 .
[ 8 ] T . L . Gri–ths and M . Steyvers . Finding scientiflc topics . Proceedings of the National Academy of Sciences , 101 Suppl 1:5228{5235 , 2004 .
[ 9 ] T . Hofmann . Probabilistic latent semantic analysis . In
UAI ’99 , pages 289{296 , 1999 .
[ 10 ] T . Hofmann . Collaborative flltering via Gaussian probabilistic latent semantic analysis . In SIGIR ’03 , pages 259{266 , 2003 .
[ 11 ] T . Iwata , S . Watanabe , T . Yamada , and N . Ueda .
Topic tracking model for analyzing consumer purchase behavior . In IJCAI ’09 , pages 1427{1432 , 2009 .
[ 12 ] T . Iwata , T . Yamada , and N . Ueda . Probabilistic latent semantic visualization : topic model for visualizing documents . In KDD ’08 , pages 363{371 , 2008 .
[ 13 ] T . Minka . Estimating a Dirichlet distribution .
Technical report , MIT , 2000 .
668 Table 2 : Average perplexities over epochs . The value in the parenthesis represents the standard deviation over data sets .
MDTM 1754.9 ( 41.3 ) 2964.3 ( 122.0 ) 3388.9 ( 37.7 ) Addresses 1968.8 ( 56.5 )
NIPS PNAS Digg
DTM 1771.6 ( 37.2 ) 3105.7 ( 146.8 ) 3594.2 ( 46.4 ) 2105.2 ( 49.7 )
LDAall 1802.4 ( 36.4 ) 3262.9 ( 159.7 ) 3652.6 ( 27.1 ) 2217.2 ( 75.3 )
LDAone 1822.0 ( 44.0 ) 5221.5 ( 268.7 ) 5162.9 ( 43.4 ) 3033.5 ( 70.9 )
LDAonline 1769.8 ( 41.5 ) 3401.7 ( 149.1 ) 3500.0 ( 43.6 ) 2251.6 ( 62.0 ) y t i x e p r e p l y t i x e p r e p l
2000
1900
1800
1700
1600
2
6000
5000
4000
3000
2000
4
6
8 epoch
( a ) NIPS
10
12
5
10
15
20 epoch
( c ) Digg y t i x e p r e p l y t i x e p r e p l
12000
10000
8000
6000
4000
2000
0
6000
5000
4000
3000
2000
1000
MDTM DTM LDAall LDAone LDAonline
10
20
30
40
50 epoch
( b ) PNAS
60
70
80
90
MDTM DTM LDAall LDAone LDAonline
50
100 epoch
( d ) Addresses
150
200
Figure 5 : Perplexities for each epoch .
1800
1750
1700
1650
1600
1550
1500
1450 y t i x e p r e p l
MDTM DTM LDAall LDAone LDAonline
MDTM DTM LDAall LDAone LDAonline
9000
8000
7000
6000
5000
4000 y t i x e p r e p l
5000
4500
4000
3500 y t i x e p r e p l
MDTM DTM LDAall LDAone LDAonline
3200
3000
2800
2600
2400
2200
2000 y t i x e p r e p l
1400
50
100
150 200 number of topics ( a ) NIPS
250
300
3000
50
100
150 200 number of topics ( b ) PNAS
250
300
3000
50
100
150 200 number of topics ( c ) Digg
250
300
1800
50
100
150 200 number of topics
250
300
( d ) Addresses
Figure 6 : Average perplexities with difierent numbers of topics . y t i x e p r e p l
1880 1860 1840 1820 1800 1780 1760 1740 1720 1700
4
5
0
1
2 3 #scales ( a ) NIPS y t i x e p r e p l
5200
4800
4400
4000
3600
3200
2800
0 1 2 3 4 5 6 7 8
#scales ( b ) PNAS y t i x e p r e p l
4600 4400 4200 4000 3800 3600 3400 3200
0
1
2
3
4
5
6
#scales
( c ) Digg y t i x e p r e p l
2800
2600
2400
2200
2000
1800
0 1 2 3 4 5 6 7 8 9
#scales
( d ) Addresses
Figure 7 : Average perplexity of MDTM with difierent numbers of scales .
669 4000
3500
3000
2500
2000
1500
1000
500
0
0
1
4
3
2 5 ( a ) NIPS
800
700
600
500
400
300
200
100
0
0
1
2 all one online
6000
5000
4000
3000
2000
1000
0
8 all one online
0
1
2
3
4
5
6 all one online
1000
900
800
700
600
500
400
300
200
100
0
0
1
2
3
4
5
6
7
8
9 all one online
( c ) Digg
( d ) Addresses
5
6
4
3 7 ( b ) PNAS
Figure 8 : Average computational time ( sec ) of MDTM per epoch with difierent numbers of scales , LDAall , LDAone , and LDAonline . a d b m a l
0.16
0.14
0.12
0.1
0.08
0.06
0.04
1
2
3
4
5 scale ( a ) NIPS a d b m a l
0.24
0.2
0.16
0.12
0.08
0.04
1 a d b m a l
0.18 0.16 0.14 0.12 0.1 0.08 0.06 0.04
6
7
8 a d b m a l
0.16
0.14
0.12
0.1
0.08
0.06
1 2 3 4 5 6 7 8 9 scale
( d ) Addresses
1
2
3 4 scale ( c ) Digg
5
6
2
3
4 5 scale ( b ) PNAS
Figure 9 : Average normalized weight ‚ with difierent scales estimated in MDTM .
[ 14 ] R . Nallapati , W . Cohen , S . Ditmore , J . Lafierty , and
K . Ung . Multiscale topic tomography . In KDD ’07 , pages 520{529 , 2007 .
[ 15 ] S . Papadimitriou , J . Sun , and C . Faloutsos . Streaming pattern discovery in multiple time series . In VLDB ’05 , pages 697{708 , 2005 .
[ 16 ] L . Ren , D . B . Dunson , and L . Carin . The dynamic hierarchical Dirichlet process . In ICML ’08 , pages 824{831 , 2008 .
[ 17 ] M . Stephens . Dealing with label switching in mixture models . Journal of the Royal Statistical Society B , 62:795{809 , 2000 .
[ 18 ] Y . W . Teh , M . I . Jordan , M . J . Beal , and D . M . Blei .
Hierarchical Dirichlet processes . Journal of the American Statistical Association , 101(476):1566{1581 , 2006 .
[ 19 ] H . M . Wallach . Topic modeling : Beyond bag of words .
In ICML ’06 , pages 997{984 , 2006 .
[ 20 ] C . Wang , D . M . Blei , and D . Heckerman . Continuous time dynamic topic models . In UAI ’08 , pages 579{586 , 2008 .
[ 21 ] X . Wang and A . McCallum . Topics over time : a non Markov continuous time model of topical trends . In KDD ’06 , pages 424{433 , 2006 .
[ 22 ] X . Wei , J . Sun , and X . Wang . Dynamic mixture models for multiple time series . In IJCAI ’07 , pages 2909{2914 , 2007 .
APPENDIX In this appendix , we give the derivation of ( 16 ) . Let ^N t¡1 t¡2s¡1;z =
Pw Pt¡1 t0=t¡2s¡1 ^Nt0;z;w , and ^Nt;z = Pw
^Nt;z;w . The Dirichlet prior parameter of the word distribution can be rewritten as the weighted sum of the word distributions for each epoch using ( 12 ) as follows ,
S
Xs=1
‚t;z;s»(s ) t¡1;z;w
=
=
=
=
=
‚t;z;sPt¡1 t0=t¡2s¡1 ^Nt0;z;w
^N t¡1 t¡2s¡1;z
S
Xs=1
S t¡1
Xs=1
Xt0=t¡2s¡1
‚t;z;s
^N t¡1 t¡2s¡1;z
^Nt0;z;w t¡1
S
Xt0=t¡2S¡1
Xs=dlog2(t¡t0+1)+1e
‚t;z;s
^N t¡1 t¡2s¡1;z
^Nt0;z;w t¡1
S
Xt0=t¡2S¡1
Xs=dlog2(t¡t0+1)+1e
‚t;z;s ^Nt0;z ^N t¡1 t¡2s¡1;z
^Nt0;z;w ^Nt0;z t¡1
Xt0=t¡2S¡1 t;z;t0 ^`t0;z;w : ‚0
( 19 )
670 speech recognition word speaker training set tdnn time test speakers
1992{1999 system data letter state letters neural utterances words state hmm system probabilities model words context hmms phoneme classiflcation
1992 { 1995 markov probability
1996 { 1999 level phonetic segmentation language segment accuracy duration continuous units spectral feature false acoustic independent models normalization rate trained male
1992 { 1993 gradient
1994 { 1995 log likelihood models hidden states models feature sequence sequences hidden continuous modeling hybrid states frame transition
1996 { 1997 features adaptation human acoustic
1998 { 1999 sentence score dtw vocabulary processing waibel acoustics error delay architecture
1992 hit target score scores threshold detection veriflcation putative card alarms 1993 dependent performance talkers writer vocabulary writing trans formation table mapping waibel 1994 recurrent estimation dependent posterior forward mlp backward targets class frames 1995 parameters clustering update entropic mixture updates flgure decoder distance welch 1996 feedback subject segmented reading factor dictionary degradation character generaliza tion experiment
1997 discrete emission behaviors length detection parameters term eq pdfs real 1998 space missing systems ergodic user weakly reconstruction mapping variables constrained
1999
( a ) Speech recognition learning state control action time policy reinforcement optimal actions recognition
1992{1999 dynamic space model exploration states programming barto function states algorithm model agent decision step reward sutton goal task
1992 { 1995 markov space 1996 { 1999 robot based controller system forward level skills policies singh adaptive grid based memory controller rl machine policies iteration stochastic continuous cost system environment iteration mdp memory real jordan world 1992 { 1993 transition values expected based 1994 { 1995 temporal iteration interpolation 1996 { 1997 singh flnite update search 1998 { 1999 watkins manager sweeping tasks prioritized moore lqr learn cases dyna 1992 game moore asynchronous trajectory atkeson learned point trials position methods
1993 probability critic actor skill support bellman convergence learner probabilities problems
1994 functions learn problem car tra–c algorithms problems performance speed discrete 1995 trial actor process pole steps local processes problem problems demonstra tion 1996 ham bellman convergence equation processes vector repre sentation mdps choice problem 1997 local learned belief pomdp problems probability method current options call learn problem 1998 algorithms critic observable approximate pomdps actor partially problems
1999
( b ) Reinforcement learning
Figure 10 : Two topic examples of the multiscale topic evolution in NIPS data analyzed by MDTM : ( a ) speech recognition , and ( b ) reinforcement learning topics . The ten most probable words for each epoch , timescale , and topic are shown .
671
