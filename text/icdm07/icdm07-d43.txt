Seventh IEEE International Conference on Data Mining Seventh IEEE International Conference on Data Mining Seventh IEEE International Conference on Data Mining Seventh IEEE International Conference on Data Mining Seventh IEEE International Conference on Data Mining
Active Learning from Data Streams*
Xingquan Zhu1 , Peng Zhang2 , Xiaodong Lin3 , Yong Shi2
1Dept . of Computer Science & Engineering , Florida Atlantic University , Boca Raton , FL 33431 , USA
2Graduate University , Chinese Academy of Sciences , Beijing 10080 , China
3Dept . of Mathematical Sciences , University of Cincinnati , Cincinnati , OH 45221 , USA xqzhu@csefauedu ; {pzhang , yshi}@gucasaccn ; linxd@mathucedu
Abstract
In this paper , we address a new research problem on active learning from data streams where data volumes grow continuously and labeling all data is considered expensive and impractical . The objective is to label a small portion of stream data from which a model is derived to predict newly arrived instances as accurate as possible . In order to tackle the challenges raised by data streams’ dynamic nature , we propose a classifier ensembling based active learning framework which selectively labels instances from data streams to build an accurate classifier . A Minimal Variance principle is introduced to guide instance labeling from data streams . In addition , a weight updating rule is derived to ensure that our instance labeling process can adaptively adjust to dynamic drifting concepts in the data . Experimental results on the performances of the proposed efforts in comparison with other simple approaches . *
1 . Introduction Recent developments in storage technology and networking architectures have made it possible for broad areas of applications to rely on data streams for quick response and accurate decision making [ 1 10 ] . Depending on data characteristics and data mining objectives , existing solutions in this area roughly fall into the following three categories : clustering and querying high speed data streams [ 3 4 ] , association rule mining from stream data [ 5 ] , and generating predictive models for continuous data streams [ 1 2 , 6 10 ] . real world data demonstrate synthetic and incorrectly instances , and
In the domain of classification , in order to generate a predictive model , it is essential to label a set of training examples from which a classifier can be trained . It is well accepted labeling training examples is a costly procedure [ 11 ] , which requires comprehensive and intensive investigations on the labeled examples will significantly deteriorate the model built from the data [ 20 ] . A common practice in addressing this problem is to use active learning techniques to selectively label a small number of instances from which an accurate predictive model can be formed . The main challenge of active learning is to identify those instances that should be labeled to achieve the highest prediction accuracy under the fact that one could not afford to label all instances . The Uncertainty Sampling ( US ) [ 12 13 ] principle has been employed extensively to achieve this goal . The intuition behind this concept is to label instances on which the current learner(s ) has the highest uncertainty . A body of work has recently been proposed to address this
*This research has been supported by the National Science Foundation of China ( NSFC ) under Grant No60674109 that problem for static datasets , with an objective of building one single optimal model from the labeled data [ 14 ] . None of them fit in the setting of data streams . labeling all investigating and
For data streams with massive data volumes , the needs of employing active learning are compelling , simply because instances are manually infeasible for many applications . We believe the challenges of active learning from data streams is threefold : ( 1 ) in data stream environments , the candidate pool is dynamically changing , whereas existing active learning algorithms are dealing with static datasets only ; ( 2 ) the concepts , such as the genuine decision logics and class distributions , of the data streams are continuously evolving , whereas existing active learning algorithms only deal with constant concepts ; and ( 3 ) because of its increasing data volumes , building one single model from all labeled data is computationally expensive for data streams , even if memory is not an issue , whereas most existing active learning algorithms rely on a model built from the whole collection of labeled data for labeling . In data stream environments , it is impractical to build one single learner from all previously labeled examples . On the other hand , as the concepts of the data streams evolve , aggregating all labeled instances may reduce the learner performances instead of adding help [ 2 ] .
We present here our recent research efforts in addressing these challenges . In short , we propose a classifier ensembling based active learning framework , with an objective of maximizing the prediction accuracy of the classifier ensemble built from labeled stream data . This is achieved through the minimization of the classifier ensemble variance , which is used to guide our instance labeling . 2 . Problem Statement and Simple Solutions 2.1 Problem Statement A common solution in handing stream data is to partition the data into chunks , as shown in Figure 1 . We can label a small portion of data in each chunk and build one classifier from labeled data . As a result , a set of classifiers are built and used to form a classifier ensemble to identify newly arrived data . Assume that stream data are partitioned chunk by chunk according to the user specified chunk size , assume further that once the algorithm moves to chunk Sn , all instances in previous data chunks , … , Sn 3 , Sn 2 , Sn 1 , become inaccessible , except classifiers built from them ( ie , … ,Cn 3 , Cn 2 , Cn 1 ) . Our objective here is labeling instances in data chunk Sn , such that a classifier Cn built from Sn , along with the most recent k1 classifiers Cn k+1 , Cn k,… , Cn 1 can form an accurate classifier ensemble ( in terms of its prediction accuracy on unlabeled instances in Sn ) .
1550 4786/07 $25.00 © 2007 IEEE 1550 4786/07 $25.00 © 2007 IEEE 1550 4786/07 $25.00 © 2007 IEEE 1550 4786/07 $25.00 © 2007 IEEE 1550 4786/07 $25.00 © 2007 IEEE DOI 101109/ICDM2007101 DOI 101109/ICDM2007101 DOI 101109/ICDM2007101 DOI 101109/ICDM2007101 DOI 101109/ICDM2007101
749 749 749 757 757
Classifier Ensemble E prediction wn k+1
Cn k+1 wn k
Cn k
… wn
Cn
… …
Ln k+1 Un k+1
Ln k Un k
… …
Ln Un Sn = Ln ∪ Un
Sn k+1
Data stream Figure 1 : A general classifier ensemble framework for active
Sn k learning from data streams labeled instances no
2.2 Simple Solutions 221 Random Sampling ( RS ) Arguably , the simplest approach to solve our problem is random sampling , where instances in Sn are randomly sampled and labeled . Although simple , it turns out that RS performs surprisingly well in practice . The niche of random sampling stems from the fact that in data stream environments the class distributions may vary significantly across data chunks . While general active learning algorithms seek to label “ important ” instances , they may significantly change class distributions by favoring one class of instances , consequently , the longer reveal genuine class distributions in the data chunk . This problem is less severe for a static dataset where candidate pool is fixed and active learning algorithms are able to survey all candidates . Random sampling avoids this problem by randomly label instances . As a result , it can produce a training set with the most similar class distributions to the current data chunk . 222 Local Uncertainty Sampling ( LU ) Another way of solving the problem is to disregard the dynamic nature of data streams and treat each data chunk Sn as a static dataset . Existing active learning algorithm can then be applied to Sn without considering any other data chunks . Because instance labeling is carried out independently in each data chunk , the weakness of LU is obvious : contributions of the labeled instances with respect to the global classifier ensemble are not clear , although each data chunk might indeed be able to label the most important instances locally . 223 Global Uncertainty Sampling ( GU ) Global uncertainty sampling based active learning will use historical classifiers , along with the one from Sn , to form a classifier committee for instance labeling . Upon the receiving of a data chunk Sn , GU will randomly label a tiny set of instances from Sn and build a classifier Cn . This classifier along with k 1 historical classifiers Cn k+1 , Cn k , … , Cn 1 , will form a classifier committee , which is used to assess instances in Sn and label the ones with the largest uncertainty . The labeling process repeats until a certain number of instances in Sn are labeled . At any stage , the user may choose to rebuild Cn by using labeled examples in Sn to improve classifier committee ’s capability in assessing remaining instances in Sn . 3 . Classifier Ensemble Variance Reduction for Error Minimization
A Bayes optimal decision rule assigns input x to a class ci if xcp i ( | is the largest among the a posteriori probability learner candidate classes . We assume
) that a given
) xcp i ( | by an associated error
( classifier ) ’s probability of classifying x to a class ci deviates )(xiε . The error can from icb represents the bias of the be decomposed into two terms : )(xicη is a random variable current learning algorithm and accounts for the variance . This gives Eq ( 1 ) [ 2 , 20 22 ] .
| x
)
+ x )(
= i c i
( 1 ) Since the same learning algorithm is used in our analysis , without loss of generality , we ignore the bias term [ 2 , 20 ] . Consequently , the learner ’s probability in classifying x into class ci becomes
.
η+ c i cp ( x )( b c i f
η+ c x )(
= xcp (
|
) i i i c
.
( 2 ) When k base classifiers work together to form a classifier ensemble E , the probability of E in classifying an instance x is given by a linear combination of the probabilities produced by all its base classifiers . Here we employ a weighted classifier ensembling scheme , where each classifier Cn has a set of iw , i=1,,l , that denotes Cn ’s weight with n weight values respect to class ci . The probability of E in classifying x into class ci is denoted by Eq ( 3 ) , where denotes the probability of classifier Cm in classifying x into class ci .
)(x f m ci x )( f f
E i c
( x
)
= n
∑ knm
+−= fw m i 1
( x
) m c i n
∑ w 1 +−= knm m i
( 3 ) m i
∑ w 1 +−= This probability can be expressed as cp ( knm
+
= x
)
| i n m m η c i i n
∑ w 1 +−= knm f
E c i x )(
= xcp (
|
)
( 4 ) ciη is a random variable accounts for the variance of )(xE
Where the classifier ensemble E with respect to class ci , and i
E η+ c i x )( m m η c i i n
∑ w 1 +−= knm m i
( 5 ) n
=
E η c i
∑ w knm 1 +−= Therefore , the variance of n ∑ ∑
2 ση
∑ knm
+−=
=
E ic g n
1 m i
)(xE ciη is given by Eq ( 6 ) ww kn 1 +−= n w 1 +−= cov( ηη g c i w 1 +−=
∑ kn m c i m i g i g i
)
,
⋅ g n knm
Which can be rewritten as Eq ( 7 )
2 σ E η ic
=
( w m i
2
)
2 σ m η ic
1 n
∑ knm
+−=
1
+ n
∑ knm +−= ⎛ ⎜ ⎝ n
∑
2
⎞ ⎟ ⎠ g i ww m i mg ≠ cov( g ηη c i m c i
, n
∑
;1 kng +−= ⎛ ⎜ ⎝ n
∑
2
⎞ ⎟ ⎠ g m i m i knm knm w 1 +−= ciη and w 1 +−= Assuming that m ciη are independent for any classifier pairs Cg and Cm ( g ≠ m ) , the second term in Eq ( 7 ) equals ciη becomes Eq ( 8 ) , where zero , thus the variance of icησ denotes the variance of the random variable m ciη . In our icησ is calculated by Eq ( 9 ) , where Λx is an system , evaluation set , |Λx| denotes the number of instances in Λx ,
)(xE
2
2 m m
( 6 )
)
( 7 )
750750750758758 ciy is the genuine class probability of instance x . If x is and x labeled as class ci , x n
( w ciy equals to 1 , otherwise , it equals to 0 . ∑ knm +−= 1 Λ w knm 1 +−= 2 x ( )
2 σ m η ic (
∑
∑ m c i m i m i
) x c i
− y
(
)
) f n
|
|
1
2
2
( cx ,
)
Λ∈
= x x
( 8 )
( 9 )
2 σ E η ic
=
2 ησ m ic
The total variance of the classifier ensemble E is then given by Eq ( 10 ) . l ∑
( 10 )
2 σ E η
∑
∑
∑ w m i m i
=
=
)
(
)
( n n
2
2 l
2 σ m η ic w 1 +−= knm
2 σ E η ic i
1 = i
1 = knm
1 +−=
⎛ ⎜ ⎝
⎞ ⎟ ⎠
According to Tumer et al ’s conclusion [ 15 ] , a classifier ’s expected added error its variance . Consequently , a classifier ensemble ’s expected error can be denoted by Eq ( 11 ) is proportional to
Err
E add
=
2 ησ S
E
( 11 )
Eq ( 11 ) states that in order to minimize a classifier ensemble error rate , we can minimize its variance instead , this can be iw , n achieved through the adjustment of the weight values i=1,,l , associated with each of E ’s base classifier Cn .
In order to minimize Eq ( 10 ) , we can compute its first iw and set it to zero , which m partial derivative wrt weight will help us find weight values producing the smallest
2
Eησ .
∂
⎡ ∑ ⎢ ⎢ ⎣ l j
(
⎛ ⎜⎜ ⎝
1 =
2 ∂ σ E η w m ∂ i
= n
∑ kng 1 +−=
( w g j
2
)
2 ⋅ σ g η jc
( ) ⋅ w m ∂ i n
∑ w kng 1 +−=
−
2
) g j
⎤ ⎞ ⎟⎟ ⎥ ⎥ ⎠ ⎦
( 12 ) n
∑ kng 1 +−=
=
2
⎡ ww m g 2 σ ⎢⎣ i i m η ic
− w g 2 σ i g η ic
⎤ ⎥⎦
( n
∑ w g i kng 1 +−=
3
)
In order to solve Eq ( 12 ) , we adopt gradient descent rule [ 16 ] , iw in inverse m which minimizes Eησ ’s first derivative . Therefore , the weight direction of
Eησ by iteratively updating
2
2 updating rule for Kw ( iw is given by Eq ( 13 ) . m w + ∆+
Kw
)1
=
)
(
( 13 ) where K is the iteration of the weight updating process and iw∆ m is given by Eq ( 14 ) m i m i m i
,
∆ w m i
ξ ⋅−=
2 ησ ∂ E w m ∂ i
( 14 )
In Eq ( 14 ) , ξ is a parameter controlling the step size for Gradient Descent [ 16 ] , in our experiments , we set ξ=0.5 which is a common default value in practice . Consequently , our weight updating rule becomes m i
KwKw g i
Kw g i
−
)
(
)
(
)
(
⋅ n
2 ⋅ σ g η ic
⎤ ⎥⎦
( 15 )
2 ⋅ σ m η ic
⎡ ⎢⎣
∑ kng 1 +−=
Kw m i
(
)1 =+
Kw m i
(
)
−
( n
∑
Kw g i kng 1 +−=
(
3
) )
4 . MV : Minimal Variance Based Active
Learning from Data Streams
The main concern of active learning for data streams is to determine which instances should be labeled for each data chunk Sn , and our intuition is to label the ones which cause the current classifier ensemble to have the largest ensemble variance . We believe that those are the main instances responsible for classifier ensemble errors , and labeling them can reduce the overall classifier ensemble variance ( hence error rates ) on data chunk Sn . This intuition is the base for our Minimal Variance active learning algorithm for data streams ( algorithm details are reported in Figure 2 ) .
∈∀
Ie , we
, which is Cm ’s accuracy on Ln .
Assume that the algorithm has just finished data chunk Sn 1 with the most recent k 1 classifiers denoted by Cn k+1 , Cnk , , Cn 1 . In order to initiate active learning process on the new data chunk Sn , we first randomly label a tiny portion of instances from Sn and put them into Ln , followed by a learning process to build a classifier Cn from Ln . The k classifiers thus form a classifier ensemble E . After that , we initialize the iw , i=1,,l , for each classifier Cm , m=nweight values , m k+1,,n , by using Cm ’s prediction accuracy on Ln , as indicated on Step 6 of Figure 2 . set w m i
)0( After the above initialization process , for each unlabeled instance in Un , we need to calculate its ensemble variance from current ensemble E , such that the one with the largest value will be selected for labeling . According to Eqs . ( 8 ) and ( 9 ) , the variance of a classifier ensemble is based on its base icησ on a specific evaluation set Λx . For classifiers’ variance each unlabeled instance Ix in Un , its evaluation set consists of all labeled instances in Ln as well as Ix itself , eg , Λx=Ln ∪ Ix . icησ requires that each instance in Because the calculation of Λx should have a class label , and Ix ’s label is yet to be found , we will use E to assign a class label for Ix ( which might be incorrect ) . We then use Eqs . ( 8 ) and ( 10 ) to calculate ensemble variance on Λx , this value is taken as the ensemble variance of Ix , as shown by Steps 8 ( a ) to 8(c ) . l ],1[
),0( w
= m j j i
,
2
2 m m
2 m
After the calculation of the ensemble variance for all unlabeled instances in Un , we label the one with the largest variance and add it to Ln . After that , we will update the weight associated with each base classifier to ensure that classifier ensemble E evolves towards a minimal variance on Ln , such that it can be beneficial for instance labeling in the next round . For this purpose , we recalculate each base classifier Cm ’s icησ on Ln ( the one calculated on Step 8(c ) is not variance accurate in the sense that the class of Ix is not labeled but predicted from E ) . Eq ( 15 ) is used to calculate the new weight values for each base classifier Cm , m=n k+1,,n
Following the weight updating process , the algorithm checks the following three conditions consecutively to ensure an iterative labeling process : ( 1 ) whether the user specified number of instances have been labeled ( Step 12 ) ; ( 2 ) whether a new classifier Cn should be rebuilt after a certain number of instances are labeled ( Step 13 ) ; ( 3 ) whether the algorithm should repeat and label next instance ( Step 14 ) .
751751751759759
Procedure : Active Learning from Data Streams Given : ( 1 ) current data chunk Sn ; and ( 2 ) k 1 classifiers Cn k+1 , , Cm , , Cn 1 built from the most recent data chunks ; Parameters : ( 1 ) α , the percentage of instances should be labeled from Sn ; ( 2 ) e , # of epochs in labeling α percentage of instances from Sn ; Objective : Label α of instances in Sn , such that the classifier built from Ln , denoted by Cn , along with the previous k 1 classifier can form a classifier ensemble as accurate as possible ( in terms of its accuracy on unlabeled instances in Sn ) .
// recording the total number of labeled instances
1 . Ln ← ∅ ; Un ← Sn 2 . Ln ← Randomly label a tiny portion , eg 1∼2.5 % , of instances from Sn . 3 . κ ← 0 4 . Build a classifier Cn from Ln 5 . K ← 0 6 . 7 . Use Cn k+1,,Cm , … , Cn to form a classifier ensemble E . 8 . For each instance Ix in Un
Initialize weight values for each classifier Cm , m=n k+1 , , n , where
// recording the number of instances labeled in the current epoch w w m i
)0(
= m j
),0(
∈∀ ji , l ],1[
, which is Cm ’s prediction accuracy on Ln . a . Use ensemble E to predict a class label for Ix . xIˆ , where b . Build an evaluation set Λx = Ln ∪ c . Calculate each classifier Cm ’s variance on Λx ( Eq ( 11) ) , and feed the value into Eq ( 8 ) to calculate ensemble variance . Calculate total xIˆ means Ix with a predicted class label . variance over all classes by Eq ( 10 ) and use this value as Ix ’s expected ensemble variance .
9 . Choose instance Ix in Un with the largest ensemble variance , label Ix , and put it into Ln , ie , Ln = Ln ∪ Ix ; Un=Un \ Ix 10 . Recalculate the variance of each base classifier Cm on Ln , and use Eq ( 15 ) to update weight values for each classifier cm .
Kw m i
(
) w m ∆+ i
; i=1 , , l
Exit Repeat step 4 Repeat step 8
// exist if α percentage of instance are labeled // rebuild model Cn if one epoch ends //continue instance labeling without rebuilding Cn
Figure 2 : Active learning for data streams
EndFor
( a .
Kw m i
)1 =+ 11 . κ ←κ +1 ; K ← K +1 12 . If κ ≥ |Sn|⋅α
13 . Else if K ≥ ( |Sn|⋅α /e ) 14 . Else
5 . Experimental Comparisons 5.1 Experimental Setting 511 General Setting In order to compare different active learning methods and justify the quality of instances labeled by them , we build a classifier ensemble E for each of them by using the same framework in Figure 1 . Therefore , if one classifier ensemble outperforms others , we can conclude that this is due to the fact that the instances used to train the classifier ensemble are of better quality . Notice that different active learning methods will select different portions of instances from Sn , this leads to different test sets for classifier ensembles . For comparison purposes , we have to make sure that all methods are compared based on the same test sets . Therefore , our comparisons are made by comparing ensemble classifiers on the average prediction accuracies on all instances in data chunk Sn over the whole stream .
All methods are implemented in Java with an integration of WEKA data mining tool [ 17 ] . All tests are conduced on a PC machine with a 2.0G CPU and 512MB memory . 512 Data Streams Synthetic data : We employ a hyper plane based synthetic data stream generator , due to its convenience in controlling the magnitude of the concept drifting and its popularity in stream data mining research [ 2 , 6 7 ] . The hyper plane of the data generation is controlled by the function in Eq ( 16 ) . xf (
)
=
∑ = d i
1 x i a i ( + x i
2 )
( 16 )
In Eq ( 16 ) , d is the total dimensions of the input data x . Each dimension xi , i=1,,d , is a value randomly generated in the range of [ 0 , 1 ] . A weight value ai , i=1 , , d , is associated with
1
+ each input dimension , and the value of ai is initialized randomly in the range of [ 0 , 1 ] at the beginning . In data generation process , we will gradually change the value of ai to simulate the concept drifting . To generate a class label for each threshold value , then we set a region [ µ⋅a0 , ν⋅a0 ] for each = d 0 ∑= a i class . If f(x ) falls into the region of one specific class , we will use this class to label x . instance x , we first generate a ia
2 )5050(
The general concept drifting is simulated and controlled through the following three parameters [ 2 , 7 8 ] : ( 1 ) t , controlling the magnitude of the concept drifting ( in every N instances ) : ( 2 ) p , controlling the number of attributes whose weights are involved in the change ; ( 3 ) h and ni∈{ 1 , 1} , controlling the weight adjustment direction for attributes involved in the change . In summary , we use c2 I100k d10 p5N1000 t01 h02 to denote a two class data stream with 100k instances , each containing 10 dimensions . The concept drifting involves 5 attributes , and attribute weights change with a magnitude of 0.1 in every 1000 instances and weight adjustment inverts the direction with 20 % of chance . Real world Data : Due to the unavailability of public benchmark data streams ( from classification perspectives ) , we select three relatively large datasets from UCI data repository [ 18 ] and treat them as data streams for active learning . The datasets we selected are Adult ( two classes and 48,842 instances ) , Covtype ( 7 classes and 581,012 instances ) , and Letter ( 26 classes and 20,000 instances ) . 513 Benchmark Methods For comparison purposes , we implemented three simple methods introduced in Section 2 : random sampling ( RS ) , local uncertainty ( LU ) , and global uncertainty ( GU ) sampling based active learning . Our minimal variance based active learning method is denoted by MV .
752752752760760
5.2 Experimental Results 521 Active Learning with a Fixed α Value We use C4.5 [ 19 ] as our base learner and apply active learning to three types of synthetic data streams : two class ( Table 1.1 ) three class ( Table 1.2 ) and four class ( Table 13 ) For each data stream , we report its results of using different chunk sizes ( varying from 250 to 2000 ) . We fix α value to 0.1 and set value k to 10 , which means that only the most recent 10 classifiers are used to form a classifier ensemble .
Comparing all four methods , we can easily conclude that MV receives the best performances across all data streams . The local uncertainty based method ( LU ) is not an option for active learning from data streams , and its performances are constantly worse than RS regardless of whether the data streams are binary or multi class . Although GU outperforms random sampling ( RS ) quite often , for multi class data streams ( eg four classes ) its performances are unsatisfactory and are almost always inferior to RS . The results from RS are surprisingly good , and it is generally quite difficult to beat RS with a substantial amount of improvement . As we have analyzed in Section 2 , random sampling naturally address the challenges of varying class distributions in data streams by randomly label instances in the data chunk . As a result , it produces a subset with the most similar class distributions to the genuine distributions in the data chunk .
In order to compare different methods at individual data chunk level , we record each method ’s accuracy on each single data chunk and report the results ( 10 times average ) in Figures 3 ( a ) to 3(c ) , where the x axis represents data chunk ID in its temporal order and the y axis shows the classifier ensemble accuracy . The significant accuracy increase from data chunks 1 to 11 in Figures 3(a ) to 3(c ) is caused by the increasing number of base classifiers at the beginning of the data streams . The results in Figures 3(a ) to 3(c ) indicate that the accuracies of the data chunks fluctuate frequently and the fluctuations are quite significant for some data streams ( eg , Figure 3(b) ) . MV consistently outperforms other methods across all data chunks . This observation asserts that for concept drifting data streams with constant changing class distributions and continuous evolving decision surfaces , MV can adaptively label instances from data chunks to build a superior classifier ensemble . The advantage of MV can be observed across different types of data streams ( binary class and multi class ) , as well as data chunks inside each data stream . 522 Active Learning with Different α Values In Figure 4 , we compare all four methods for different α values . Overall , MV and GU achieve the best performances , and LU performs inferior to RS in the majority of cases . This , again , asserts that applying traditional uncertainty sampling locally to each single data chunk is inappropriate for data streams , where dynamically changing class distributions and evolving concepts require an active learning algorithm to take these issues into consideration for effective instance labeling . As we discussed in Section 2 , GU extends Query by Committee to data streams by taking classifiers learnt from different data chunks as committee members . In the original QBC , the committee members are learned from the data with the same distributions ( randomly sampled from the labeled data ) , therefore committee members are considered similar to each other with relatively small variances in their predictions . In data stream environments , in addition to the fact that the committee classifiers are learned from different portion of stream data , the concept drifting in the data chunks also render classifiers significantly different from each others . As a result , weighted average uncertainty over all committee members no longer reveals the genuine uncertainty of the classifier ensemble formed by them . The results in Figure 4 support our hypothesis very well . As we can see , MV constantly outperforms GU across all α values . For multiclass data streams , the performances of GU are unsatisfactory and are inferior to random sampling sometime . Table 1.1 : Accuracy on c2 I50k d10 p5 N1000 t01 h02 ( α=0.1 ) Chunk Size
MV
GU
LU
RS
250 500 750 1000 2000
7454±289 8307±242 8610±275 8643±352 8647±501
8108±285 8679±251 8867±274 8909±339 8888±427 Table 1.2 : Accuracy on c3 I50k d10 p5 N1000 t01 h02 ( α=0.1 ) Chunk Size
7370±275 8256±236 8569±313 8611±412 8594±482
7551±335 8516±295 8814±304 8879±347 8869±427
250 500 750 1000 2000
5636±271 6631±430 7179±286 7529±288 7392±611
6441±387 7159±423 7439±399 7822±438 7598±544 Table 1.3 : Accuracy on c4 I50k d10 p5 N1000 t01 h02 ( α=0.1 ) Chunk Size
5657±271 6699±498 7259±377 7645±372 7637±464
5546±264 6611±423 7049±361 7423±327 7297±601
MV
MV
GU
GU
LU
LU
RS
250 500 750 1000 2000
RS
4227±201 5047±234 5811±365 6308±367 7187±447
4138±180 4981±247 5675±326 6272±407 7067±498
4134±209 4991±245 5648±309 6192±365 7098±509
4645±204 5395±304 5986±388 6404±411 7213±533
RS
LU
GU
MV
RS
LU
GU
MV
RS
LU
GU
MV y c a r u c c A
95 90 85 80 75 70
1
11
21
31
51
41
61 Data Chunk ID
71
81
91
82 77 72 67 62 57 52 y c a r u c c A
1
11
21
31
51
41 Data Chunk ID
61
71
81
91
60
56
52
48
44
40 y c a r u c c A
1
11
21
31
( a ) c2 I50k d10 p5 N1000 t01 h02 ( b ) c3 I50k d10 p5 N1000 t01 h02 Figure 3 : Classifier ensemble accuracy across different data chunks : ( a ) two class ; ( b ) three class ; ( c ) four class data streams ( α=0.1 , e=3 )
( c ) c4 I50k d10 p5 N1000 t01 h02
51
41 61 Data Chunk ID
71
81
91
753753753761761
RS
LU
GU
MV
RS
LU
GU
MV
RS
LU
GU
MV n S n o y c a r u c c A
95
90
85
80
75
70
0.05
0.075
0.1
0.15 0.2 Alpha values
0.3
0.4
0.5 n S n o y c a r u c c A
85 80 75 70 65 60 55
0.05
0.075
0.1
0.15
0.2
0.3
0.4
0.5
Alpha values n S n o y c a r u c c A
85
75
65
55
45
35
0.05
0.075
0.1
( a ) c2 I50k d10 p5 N1000 t01 h02 ( b ) c3 I50k d10 p5 N1000 t01 h02
Figure 4 : Classifier ensemble accuracy with respect to different α values ( chunk size 500 , e=3 )
RS
LU
GU
MV
RS
LU
GU
MV
RS
LU
GU
MV
( c ) c4 I50k d10 p5 N1000 t01 h02
0.15 0.2 Alpha values
0.3
0.4
0.5 n S n o y c a r u c c A
80
70
60
50
40
30
20
0.075
0.1
0.15
0.2
Alpha values
0.3
0.4
0.5 n S n o y c a r u c c A
84
82
80
78
76
0.05 0.075
0.1
0.15 0.2 Alpha values ( a ) Adult n S n o y c a r u c c A
74 72 70 68 66 64 62 60
0.3
0.4
0.5
0.05 0.075
0.1
0.15 0.2 Alpha values
0.3
0.4
0.5
Figure 5 : Classifier ensemble accuracy on data chunk Sn ( chunk size 500 , e=3 )
( b ) Covtype
( c ) Letter
523 Active Learning from Real world Data In Figure 5 we report the algorithm performances on three real world data . Different from synthetic data streams where data chunks have strong correlations , the data chunks of the real word data do not share such property , and the concept drifting among data chunks are not clear to us ( in fact , we do not even know the genuine concepts of the data ) . The results in Figure 5 assert that MV consistently outperforms other methods . Although GU is able to perform well for both Adult and Covtype , its performance on Letter is significantly worse than all the other methods , where the accuracy of GU can be as much as 20 % lower than MV . Considering Letter is a sparse dataset with 26 evenly distributed classes , it suggests that for data streams with a large number of classes , traditional uncertainty sampling would perform poorly . This asserts our analysis in Section 2 and Section 5.2 , that for sparse data streams with a large number of classes , the classifiers built from different data chunk vary significantly . Simply calculating the averaged uncertainty over all committee classifiers ( like QBC does ) is not a good solution , and can produce much worse results than random sampling . 6 . Conclusions In this paper , we proposed a new research topic on active learning from data streams , where data volumes continuously increase and data concepts dynamically evolve , and our objective is to label a small portion of stream data to form a classifier ensemble to accurately predict newly arrival instances . In order to address the problem , we proposed a Minimal Variance ( MV ) principle , where the key is to label instances which cause the classifier ensemble to generate the largest variance . We derived a weight updating rule to ensure that labeled instances can help classifier ensemble to adapt to dynamic data streams and evolve towards the global minimal variance , and consequently minimal error rates . We have also provided a framework which accommodates the above minimal variance principle for active learning from data streams . Our experimental results on synthetic and real world data demonstrated the effectiveness of the proposed design .
References 1 . P . Domingos & G . Hulten , Mining high speed data streams , Proc . of
KDD , 2000 .
2 . H . Wang , W . Fan , P . Yu , & J . Han , Mining concept drifting data streams using ensemble classifiers , Proc . of KDD , 2003 .
3 . C . Koch , S . Scherzinger , N . Schweikardt , & B . Stegmaier , Schemabased scheduling of event processors and buffer minimization for queries on structured data streams . Proceedings of VLDB 2004 .
4 . S . Guha , N . Milshra , R . Motwani , & L . O’Callaghan , Clustering data streams , Proc . of FOCS , 2000 .
5 . Y . Chi , H . Wang , P . Yu , & R . Muntz , Moment : Maintaining closed frequent itemsets over a stream sliding window data streams , Proc . of ICDM 2004 .
6 . H . Wang , J . Yin , J . Pei , P . Yu , & J . Yu , Suppressing model overfitting in mining concept drifting data streams , Proc . of KDD , 2006 .
7 . Y . Yang , X . Wu , & X . Zhu , Combining proactive and reactive predictions of data streams , Proc . of KDD , 2005 .
8 . W . Street & Y . Kim , A streaming ensemble algorithm ( SEA ) for large scale classification , Proc . of KDD , 2001 .
9 . W . Fan , Y . Huang , P . Yu , Decision tree evolution using limited number of labeled data items from drifting data streams , Proc . of ICDM 2004 .
10 . W . Fan , Y . Huang , H . Wang , & P . Yu , Active mining of data streams ,
Prof . of SDM 2004 .
11 . D . Cohn , L . Atlas , R . Ladner , Improving Generalization with Active
Learning , Machine Learning 15(2 ) , 1994 .
12 . H . Seung , M . Opper , & H . Sompolinsky , Query by committee , Proc . of COLT , 1992 .
13 . Y . Freund , H . Seung , & E . Tishby , Selective sampling using the query by committee algorithm , Machine Learning , 1997 .
14 . S . Hoi , R . Jin , J . Zhu , & M . Lyu , Batch mode active learning and its application to medical image classification , Proc . of ICML , 2006 .
15 . K . Tumer & J . Ghosh , Error correlation and error reduction in ensemble classifier , Connection Science , 8(3 4 ) , 1996 .
16 . S . Haykin , Neural Networks : A comprehensive foundation , Prentice
Hall , 1998 .
17 . I . Witten & E . Frank , Data mining : practical machine learning tools and techniques , Morgan Kaufmann , 2005 .
18 . D . Newman , S . Hettich , C . Blake , C Merz . UCI Repository of machine learning , 1998 .
19 . J . Quinlan , C4.5 : Programs for Machine learning , M . Kaufmann , 1993 . 20 . Zhu X . , & Wu X . , Class Noise vs . Attribute Noise : A Quantitative study of their impacts , Artificial Intelligence Review , 22 , 2004 .
754754754762762
