Cocktail Ensemble for Regression
Yang Yu1
Zhi Hua Zhou1
Kai Ming Ting2
1National Key Laboratory for Novel Software Technology , Nanjing University , China emails : {yuy,zhouzh}@lamdanjueducn
2Gippsland School of Information Technology , Monash University , Australia email : kaimingting@infotechmonasheduau
Abstract
This paper is motivated to improve the performance of individual ensembles using a hybrid mechanism in the regression setting . Based on an error ambiguity decomposition , we formally analyze the optimal linear combination of two base ensembles , which is then extended to multiple individual ensembles via pairwise combinations . The Cocktail ensemble approach is proposed based on this analysis . Experiments over a broad range of data sets show that the proposed approach outperforms the individual ensembles , two other methods of ensemble combination , and two stateof the art regression approaches .
1 . Introduction of an error ambiguity decomposition on linear combination of individual ensembles , we derive the optimal combination for two individual ensembles . Moreover , we find this optimal combination can be estimated effectively and efficiently from data , by estimating the performance of individual ensembles it combines . This finding leads to a simple yet effective solution for combining multiple ensembles based on the optimal pairwise combination , while acknowledging that obtaining the global optimal is intractable . Based on the analysis , we propose the Cocktail ensemble approach to combine different ensembles . Empirical evaluation over a broad range of data sets shows that the proposed approach is superior to simple averaging , selecting the best ensemble , its individual ensembles , Stochastic Gradient Boosting [ 9 ] and Iterated Bagging [ 6 ] .
2 . Related Work
Ensemble learning uses multiple individual learners to solve a problem . Since it can significantly improve the generalization ability of a single learner , it has attracted a lot of attention , and many ensemble approaches have been developed . According to the styles of training individual learners , current ensemble approaches can be roughly categorized into two classes : ( i ) approaches where individual learners are trained in parallel , and ( ii ) approaches where individual learners must be trained sequentially . Representative parallel ensemble approaches include Bagging [ 4 ] and Random Forests [ 5 ] ; and examples of sequential ensemble approaches include AdaBoost [ 8 ] and Stochastic Gradient Boosting [ 9 ] . However , despite the emergence of many ensemble approaches , it has been shown that none is significantly superior to another over a range of data sets [ 10 ] .
We are motivated to further improve the performance of a single ensemble by combining more than one individual ensemble . The simplest approach in the regression setting is to average the outputs of its base ensembles . However , is there a better combination approach ? Does the hybrid approach perform better than its base ensembles ?
This paper investigates these issues . Through analysis
MultiBoosting [ 18 , 19 ] combines Bagging/Wagging [ 1 ] with Boosting , by taking Boosting as individual learner for Bagging/Wagging , in order to reduce both variance and bias at the same time . Stochastic Gradient Boosting [ 9 ] uses a bootstrap sample of the training set to train a regressor in each iteration , which can also be viewed as a hybrid of Boosting and Bagging [ 9 ] . Iterated Bagging [ 6 ] is motivated by iteratively reducing the bias of Bagging , which can be regarded as a sequential ensemble approach taking Bagging as the base learner .
In contrast to the above mentioned approaches , which combine a parallel ensemble approach and a sequential ensemble approach , our approach combines any kinds of individual ensembles , and it has a strong theoretical basis .
E GASEN [ 22 ] also combines ensembles . It trains multiple ensembles using the same ensemble approach GASEN [ 23 ] and the same base learning algorithm , and then uses simple averaging to combine these ensembles . In contrast , our work attempts to combine individual ensembles generated by different ensemble approaches and different base learning algorithms .
Our work uses a weighted averaging approach to combine individual ensembles . Various weighted averaging combination approaches have been employed in the literatures , eg , [ 14 , 12 , 16 , 23 ] . All of these methods set weights solely based on empirical estimation on training set , or by 10 fold cross validation or some other estimation methods that are computationally expensive , while we show in this paper that the solution of the optimal weights for combining two individual ensembles can be effectively estimated from data with a very small computational cost .
In [ 13 ] , error ambiguity decomposition is proposed to explain and exploit performance of ensembles . In this paper , the error ambiguity decomposition is our tool to find the optimal combination of two individual ensembles .
3 . Cocktail Ensemble
Given a training set D consisted of N examples , ie , D = {(x1 , y1 ) , . . . , ( xN , yN )} , where xi is an instance and yi is a single variant regression . The task of regression is to learn a function f : X → R from D that approximates an underlying function f 0 by optimizing certain criterion , eg minimization of the mean squared error . Combining Two Ensembles . Denote two ensembles f1 and f2 , which aggregate their individual learners in sets {f1,i}S1 j=1 with size S1 and S2 , respectively . The functions of the ensembles are : i=1 and {f2,j}S2 f1(x ) = Ei[f1,i(x ) ] , f2(x ) = Ej[f2,j(x ) ] where the expectation can be implemented , taking averaging as an example , as Ei[f1,i(x ) ] = i f1,i(x)/S1 and Ej[f2,j(x ) ] = j f2,j(x)/S2 . We explores the linear combination , ie the weighted averaging of ensembles f1 and f2 , since linear combination is a more generic combination than simple averaging . Through a linear combination of ensembles f1 and f2 , a new ensemble is formed : f c(x ) = pf1(x ) + ( 1 − p)f2(x ) wrt p ∈ [ 0 , 1 ] where p is the weight for f1 , and 1 − p is the weight for f2 . Here the restriction of p is to ensure that the generalization error of f c is upper bounded by the worse of f1 and f2 . By this restriction , f c can be viewed as an interpolation between f1 and f2 , which may have lower risk of making errors than an extrapolation .
Following the error ambiguity decomposition [ 13 ] , given E1 and E2 as generalization errors of f1 and f2 , respectively , we derive1 that the optimal weight of f1 is : p∗ = E2 − E1 2∆
+ 0.5
( 1 )
1Derivation details will be presented in a longer version of the paper . where , we define
∆ = Ex[(f1(x ) − f2(x))2 ] as the squared output difference of the two ensembles . The optimal weight leads to the minimum generalization error of f c :
Ec∗ = E1 + E2
2
− ∆ 4
− ( E1 − E2)2
4∆
( 2 ) where E1 , E2 and ∆ can be estimated from data efficiently and effectively ( to be shown in Section 4 ) . Combining Multiple Ensembles Optimal combination of multiple ensembles is intractable in general [ 23 ] . Since we have proved that the optimal combination of two ensembles is easy to solve , we propose a simple approach to combine multiple ensembles based on the optimal pair wise combination , as follows . the first base ensemble as the one with lowest error , i=1 be the set of N base ensembles . Select
Let F = {fi}N
1 = arg min f c f∈F
E(f ) , then each subsequent selected base ensemble is the one which reduces the combined estimated error the most , 1 + ( 1 − p)f ) ,
2 = p2f c f c
E(pf c
1 + ( 1 − p2 ) arg min f∈F p∈[0,1 ]
···
N = pN f c f c
N−1 + ( 1 − pN ) arg min f∈F p∈[0,1 ]
E(pf c
N−1 + ( 1 − p)f ) . where E(f ) is the estimated error of f , and weights p2 , . . . , pN are determined by Eq 1 . f c N is the final combination of the given base ensembles . Note that it is possible that a base ensemble is selected more than once . Cocktail Ensemble The result of the above analysis forms the basis of the Cocktail ensemble approach . The pseudo code is provided in Table 1 . The accuracy of the error estimation method used
Table 1 . Cocktail Ensemble Cocktail(F ) Input : F = a set of N trained base ensembles Process :
1 = the ensemble in F with the smallest estimated error f c emin = +∞ for i = 2 , . . . , N fi = null for each f ∈ F e = estimated error of combining f and f c if e < emin then let fi = f and emin = e i−1 by Eq 2 end for if fi is null then f c i = pif c f c
N = f c i−1 and break i−1 + ( 1 − pi)fi , where pi is obtained by Eq 1 end for return f c N
Table 2 . Normalized mean squared errors of Cocktail , PickBest , Avg , RF , LR and NN .
Dataset abalone analcat.g analcat.w autoHorse autoMpg autoPrice bank8FM bodyfat breastTumor chatfield.4 cholesterol chscase.c6 chscase.d chscase.w cleveland cloud cpu csb.ch15 csb.ch21a delta.a diggleta2 fruitfly housing kin8nm lowbwt meta mu284 no2 pbc pharynx plasma.r pollen puma8NH pwLinear quake rmftsa.l sensory servo socmob space.ga stock strike tecator veteran visual.g water.treat wisconsin witmer.1987
Cocktail3 610±00 943±02 707±03 164±01 394±01 418±02 043±00 044±00 925±01 436±01 942±01 977±00 128±00 000±00 858±01 217±01 136±05 873±01 317±01 636±00 012±00 928±01 244±00 363±00 475±01 963±02 009±00 841±01 855±01 766±02 931±02 975±00 364±00 348±00 962±00 522±01 743±01 223±01 492±02 447±00 024±00 855±01 054±00 850±02 055±00 250±12 886±02 008±00
Avg3 673±00 927±02 786±02 333±01 486±01 553±01 162±00 195±00 930±01 517±01 931±01 979±00 573±01 123±01 872±01 438±02 443±05 897±00 429±00 710±00 131±00 958±01 366±00 564±00 519±01 963±02 125±00 873±00 865±01 869±02 901±01 977±00 549±00 460±00 965±00 595±01 804±01 345±01 590±01 593±00 188±00 864±00 264±01 877±02 186±00 666±06 881±01 164±00
PickBest3 619±00 100±04 701±03 196±02 413±02 414±03 046±00 044±00 100±03 431±01 100±02 977±01 128±00 000±00 870±01 212±01 136±05 872±01 319±01 638±00 014±00 927±01 263±01 363±00 470±01 970±02 011±00 840±01 895±02 766±02 983±05 974±00 364±00 427±02 976±01 562±01 744±01 235±02 594±07 447±00 024±00 868±01 055±00 838±02 055±00 244±11 100±03 008±00
Cocktail2 610±00 945±02 707±03 164±01 394±01 418±02 043±00 044±00 932±01 434±01 943±01 976±00 128±00 000±00 860±01 217±02 136±05 873±01 317±01 636±00 012±00 928±01 244±00 363±00 475±01 963±02 009±00 841±01 854±01 766±02 926±02 974±00 364±00 348±00 963±00 522±01 743±01 223±01 492±02 447±00 024±00 855±01 054±00 851±02 055±00 250±12 893±01 008±00
Avg2 613±00 931±02 745±02 203±01 396±01 452±02 047±00 067±00 951±01 447±01 935±01 985±00 970±01 003±00 883±01 303±02 277±06 886±00 341±00 654±00 013±00 979±02 249±00 470±00 474±01 961±02 009±00 850±01 846±01 830±02 916±01 981±00 455±00 345±00 967±00 522±01 763±01 223±01 484±01 511±00 064±00 850±01 103±00 861±02 072±00 534±08 887±02 025±00
PickBest2 619±00 965±04 701±03 196±02 413±02 414±03 046±00 044±00 967±02 431±01 982±02 100±01 942±01 000±00 870±01 212±01 136±05 872±01 319±01 638±00 014±00 975±02 263±01 363±00 470±01 961±02 011±00 840±01 895±02 766±02 980±05 976±00 364±00 427±02 967±00 562±01 744±01 235±02 594±07 447±00 024±00 868±01 055±00 838±02 055±00 244±11 968±03 008±00
RF
619±00 951±03 701±03 503±02 477±01 414±03 046±00 133±00 982±01 548±01 954±01 989±01 100±01 011±00 100±02 553±04 580±08 872±01 319±01 638±00 023±00 100±02 263±01 363±00 470±01 961±02 013±00 840±01 909±01 766±02 894±01 100±00 364±00 410±01 967±00 562±01 744±01 225±01 500±01 447±00 024±00 858±01 282±01 926±03 055±00 977±02 938±02 089±01
LR
662±00 950±02 856±02 196±02 409±01 604±03 074±00 044±00 962±02 431±01 958±01 992±00 942±01 000±00 870±01 212±01 136±05 951±00 448±00 730±00 013±00 971±02 342±01 643±00 500±01 987±02 011±00 920±01 878±01 972±04 100±02 976±00 683±00 451±01 100±00 611±00 873±01 264±01 651±02 668±00 176±00 860±00 055±00 838±02 134±00 244±11 930±02 008±00
NN
100±00 951±01 100±01 100±02 100±01 100±01 100±00 100±01 980±00 100±02 965±00 973±00 128±00 100±07 937±01 100±04 100±03 100±00 100±01 100±01 100±01 927±01 100±01 100±01 100±01 100±02 100±03 100±00 100±01 100±01 920±00 974±00 100±01 100±02 974±00 100±02 100±00 100±01 100±01 100±00 100±00 100±00 100±00 100±01 100±02 100±03 941±01 100±02 is important , which directly affects the performance of the Cocktail ensemble . We have employed the out of bag estimation method [ 21 ] in our evaluation for convenience ( the reason will be clear in the next section ) . There is no reason that other equally effective methods cannot be used .
4 . Empirical Study
Experimental Settings To empirically evaluate the performance of the Cocktail ensemble , we use three types of base ensemble learning ap proaches , denoted by RF , LR and NN . RF is Random Forests [ 5 ] , whose base learners are REPTrees [ 20 ] without postpruning which are C4.5 like regression trees ; LR is Bagging [ 4 ] of logistic regressors [ 11 ] ; NN is Bagging of RBF neural networks [ 2 ] . Since RF , LR and NN all use bootstrap sampling , it is convenient to use the out of bag estimation [ 21 ] method for error estimation . Algorithm implementations are based on WEKA [ 20 ] with its default settings .
In addition to each of the three base ensembles , Cocktail is compared with Avg and PickBest . Avg is simple averaging , which uses the same base ensembles of Cocktail but aggregates their outputs with equal weights . PickBest chooses to use the best among the base ensembles according to their estimated generalization errors . The ensemble sizes of RF , LR and NN are set to 100 , which makes the ensemble sizes of Cocktail , Avg and PickBest 200 or 300 , depending on whether two or three base ensembles are used in the combination .
The empirical study uses 48 regression data sets from UCI Repository [ 3 ] and StatLib [ 17 ] . For each data set , we conduct a 10 times 10 fold cross validation to estimate the performance of each approach . Then several statistical significance tests are employed . The first one is Friedman test [ 7 ] , which is a non parametric test based on rank [ 7 ] applied to comparing multiple learners on multiple data sets . The Friedman test is performed in conjunction with the Bonferroni Dunn test [ 7 ] with significance level 005 We also conduct a t test with significance level 0.05 to compare two approaches on each data set . The win/tie/loss counts for the t test are recorded , where a win ( or loss ) is counted when Cocktail is significantly better ( or worse ) than the compared approach on a data set in the t test ; otherwise a tie is recorded . A sign test with significance level 0.05 is then conducted on the win/tie/loss counts of two approaches to determine whether the null hypothesis , ie the two approaches have no difference , should be accepted or rejected . Results Table 2 presents the results , where A3 denotes that the approach A takes RF , LR , and NN as base ensembles , while A2 denotes that the approach A takes RF and LR as base ensembles . Normalized mean squared errors are reported . That is , we divide the mean squared error of a specific algorithm on a specific data set by the maximum mean squared error at the same row , and then reports the ratio . There are three groups of algorithms , separated by vertical lines in the table . Algorithms in different groups employ different number of base ensembles . For every group , the best performance on each data set is bolded .
Figure 1 shows the result of the Friedman test of these approaches . Cocktail is shown to be significantly better than all other approaches . Note that , when combining two ensembles , neither Avg2 nor PickBest2 is significantly better than both of their base ensembles .
Figure 1 . The result of the Freidman test for Cocktail , PickBest and Avg with base ensembles RF and LR over 48 data sets . The circles indicate the average rankings , and the bars indicate the critical values of the Bonferroni Dunn test for a two tailed test at significance level 005 Approaches having non overlapped bars are significantly different .
Table 3 . Win/Tie/Loss counts of t test for Cocktail against PickBest , Avg , RF , LR and NN .
PickBest2 25/17/6 w/t/l
Cocktail2 against RF
Avg2 34/4/10
31/14/3
LR 36/7/5
Cocktail3 against
PickBest3 24/18/6
Avg3 42/3/3
RF
31/14/3
LR 40/5/3
NN 39/7/2 w/t/l
Table 3 summarizes the results of t tests of Cocktail against PickBest , Avg , RF , LR and NN . It reveals that Cocktail is the best , since it is significantly better than all other methods in the sign test on the win/tie/loss counts , where Cocktail has more than three times the number of wins than the number of losses , compared to any of the other methods . It can be observed that NN is worse than RF and LR on many data sets , because we make no attempt to tune its many parameters but only use the default configuration in WEKA [ 20 ] . Interestingly , we find that adding a bad base ensemble has little negative impact to Cocktail .
To investigate how well Cocktail estimates weight p according to Eq 1 , we plot the performance of Cocktail2 with p ∈ {0 , 0.01 , 0.02 , . . . , 1} for each data set from the first fold of the cross validation experiment . Figure 2 shows 12 of the data sets.2 The abscissa of each plot indicates weight p ; p = 0 in the error curve is the error for RF , and p = 1 is the error for LR . To provide a clear visualization , the axis has been normalized into the range [ 0 , 1 ] . The estimated weight and the optimal weight are marked by two lines parallel to the axis , while the weight for PickBest is indicated
2Details of this experiment and the following experiments will be pre sented in a longer version of the paper .
15225335445LRRFaverage rankingCocktail2PickBest2Avg2123average ranking1234NNLRRFaverage rankingCocktail3PickBest3Avg3Cocktail3 legend abalone autoHorse autoMpg bank8FM bodyfat chscase.djsp delta.ailerons fruitfly housing kin8nm lowbwt pbc
Figure 2 . The optimal weight , the estimated weights by Cocktail and PickBest , and the performance of Cocktail in a single fold of 10 fold cross validation .
Figure 3 . The influence of ensemble size on Friedman ranking by a box at either 0 or 1 of the abscissa .
It can be observed that for the 12 data sets shown in Figure 2 , the estimated weights are close to the optimal weights . Even in those cases in which the estimated weights might appear to have a big difference to the optimal weights , they are usually in the relatively flat region of the error curve . Thus , there is only a small real difference in terms of performance . It also discloses that the optimal weight on different data sets could appear at different p values .
To investigate the influence of the ensemble size on the performance of Cocktail2 , the predictive performance of ensembles with different sizes ( 1 , 2 , . . . , 100 ) are obtained . The Friedman rankings are depicted in Figure 3 , where the Friedman rankings are obtained for each ensemble size . It can be observed that Cocktail2 is consistently the best approach in term of the Friedman ranking .
Finally , in terms of efficiency , averaged over all data sets in the 10 times 10 fold cross validation , Cocktail3 only costs 1.2 % more time than using Avg3 which does not involve the error estimation . Compare to Sequential Ensemble Approaches We compare Cocktail2 to two state of the art sequential approaches for regression , ie , Stochastic Gradient Boosting [ 9 ] and Iterated Bagging [ 6 ] .
Figure 4 . The result of the Freidman test for Cocktail , Stochastic Gradient Boosting ( SGB ) and Iterated Bagging ( IB ) with base regressors REPTree ( Tree ) and logistic regression ( LR ) over 48 data sets . Table 4 . Win/Tie/Loss counts of t test for Cocktail against Stochastic Gradient Boosting ( SGB ) and Iterated Bagging ( IB ) with base regressors REPTree ( Tree ) and logistic regression ( LR ) .
SGBTree 37/4/7 w/t/l
Cocktail2 against SGBLR IBTree 30/7/11 41/1/6
IBLR 33/13/2
Denote SGBTree and SGBLR as the Stochastic Gradient Boosting approachs with base learners REPTree [ 20 ] and logistic regression [ 11 ] , respectively . Denote IBTree and IBLR as the Iterated Bagging approachs with the two base learners , respectively . The ensemble size is set to 200 for all these approaches , which is the same as that of Cocktail2 . For Stochastic Gradient Boosting , the shrinkage parameter is set to 0.005 for small data sets ( having less than 500 instances ) and 0.05 for large ones ( having more than 500 instances ) , which is the same as in [ 9 ] ; the fraction parameter is set to 0.5 , which leads to the best performance according to [ 9 ] . For Iterated Bagging , the threshold multiplier is set to 1.1 and 50 base learners per stage , as did in [ 6 ] .
Figure 4 shows the result of the Friedman test of these approaches . Cocktail is shown to be significantly better than the two sequential approaches .
Table 4 summarizes the results of t tests of Cocktail2 against Stochastic Gradient Boosting and Iterated Bagging . It also reveals that Cocktail is significantly better than both error curveoptimal weightestimated weightweight of PickBest00510051005100510051005100510051005100510051005100510051005100510051005100510051005100510051005102040608010012345ensemble sizeFriedman ranking CocktailPickBestMergeRandomForestsBagging123average rankingCocktail2SGBTreeIBTree123average rankingCocktail2SGBLRIBLR the sequential approaches . Cocktail2 has more than two times the number of wins than the number of losses , compared to any of the other methods .
Note that our experiment shows that Iterated Bagging is better , but not significantly better , than Stochastic Gradient Boosting , while the empirical study in [ 15 ] concludes that Iterated Bagging is significantly better than Stochastic Gradient Boosting . We think that the difference in results is because the shrinkage of Stochastic Gradient Boosting was set to 1 in [ 15 ] , which causes overfitting , while we set the shrinkage as recommended in [ 9 ] .
5 . Conclusions
This paper shows that it is possible to form a hybrid ensemble that often outperforms its base ensembles and two other forms of combination : Selecting the best and simple averaging . We also show that the proposed approach outperforms two state of the art sequential ensemble approaches , Stochastic Gradient Boosting and Iterated Bagging with either REPTree or logistic regression as base learners .
We have provided a theoretical basis through an analysis of error ambiguity decomposition . It shows that there is a simple closed form solution to the optimal weight for combining two base ensembles . When combining more than two ensembles , our proposed approach provides a simple yet effective solution based on the optimal pairwise combination . We also show that the out of bag estimation method works reasonably well for Cocktail .
Though we have only conducted experiments using three kinds of base ensembles , we believe that the same result can also be expected when other base ensemble approaches are used . The analysis reveals that the only condition to gain better performance is to use sufficiently different individual ensembles with a good error estimation method .
Acknowledgments : Supported by NSFC ( 60325207 , 60635030 ) and 863 Program ( 2007AA01Z169 ) . Part of the research was conducted when K . M . Ting was visiting the LAMDA group at Nanjing University . We thank the anonymous reviewers for their helpful suggestions .
References
[ 1 ] E . Bauer and R . Kohavi . An empirical comparison of voting classification algorithms : Bagging , boosting , and variants . Machine Learning , 36(1 2):105–139 , 1999 .
[ 2 ] C . M . Bishop . Neural Networks for Pattern Recognition .
Oxford University Press , New York , 1995 . [ 3 ] C . Blake , E . Keogh , and C . J . Merz . UCI repository of machine learning databases . [ http://wwwicsuciedu/∼mlearn /MLRepository.html ] , Department of Information and Computer Science , University of California , Irvine , CA , 1998 .
[ 4 ] L . Breiman .
Bagging predictors . Machine Learning ,
24(2):123–140 , 1996 .
[ 5 ] L . Breiman . Random forests . Machine Learning , 45(1):5–
32 , 2001 .
[ 6 ] L . Breiman . Using iterated bagging to debias regressions .
Machine Learning , 45(3):261–277 , 2001 .
[ 7 ] J . Demˇsar . Statistical comparisons of classifiers over multiple data sets . Journal of Machine Learning Research , 7:1– 30 , 2006 .
[ 8 ] Y . Freund and R . Schapire . A decision theoretic generalization of on line learning and an application to boosting . Journal of Computer and System Sciences , 55(1):119–139 , 1997 .
[ 9 ] J . H . Friedman . Stochastic gradient boosting . Computa tional Statistics & Data Analysis , 38(4):367–378 , 2002 .
[ 10 ] L . O . Hall , K . W . Bowyer , R . E . Banfield , D . Bhadoria , W . P . Kegelmeyer , and S . Eschrich . Comparing pure parallel ensemble creation techniques against bagging . In ICDM’03 , pages 533–536 , Melbourne , FL .
[ 11 ] F . E . Harrell . Regression Modeling Strategies with Applications to Linear Models , Logistic Regression , and Survival Analysis . Springer , New York , NY , 2001 .
[ 12 ] R . Jacob . Methods for combining experts’ probability as sessments . Neural Computation , 7(5):867–888 , 1995 .
[ 13 ] A . Krogh and J . Vedelsby . Neural network ensembles , cross validation , and active learning . In NIPS 7 , pages 231–238 . MIT Press , Cambridge , MA , 1995 .
[ 14 ] M . Perrone and L . Cooper . When networks disagree : EnIn R . Mamsemble methods for hybrid neural networks . mone , editor , Artificial Neural Networks for Speech and Vision , pages 126–142 . Chapman & Hall , London , 1993 .
[ 15 ] Y . L . Suen , P . Melville , and R . J . Mooney . Combining bias In and variance reduction techniques for regression trees . ECML’05 , pages 741–749 , Porto , Portugal .
[ 16 ] K . M . Ting and I . H . Witten . Issues in stacked generalization . Journal of Artificial Intelligence Research , 10:271– 289 , 1999 .
[ 17 ] P . Vlachos and M . Meyer . Statlib data set archive . [ http:// libstatcmuedu ] , Department of Statistics , Carnegie Mellon University , Pittsburgh , PA , 1998 .
[ 18 ] G . I . Webb . Multiboosting : A technique for combining boosting and wagging . Machine Learning , 40(2):159–196 , 2000 .
[ 19 ] G . I . Webb and Z . Zheng . Multistrategy ensemble learning : Reducing error by combining ensemble learning techniques . IEEE Trans . Knowledge and Data Engineering , 16(8):980– 991 , 2004 .
[ 20 ] I . H . Witten and E . Frank . Data Mining : Practical Machine Learning Tools and Techniques , 2nd ed . Morgan Kaufmann , San Francisco , 2005 .
[ 21 ] D . H . Wolpert and W . G . Macready . An efficient method to estimate bagging ’s generalization error . Machine Learning , 35(1):41–55 , 1999 .
[ 22 ] J X Wu , Z H Zhou , and Z Q Chen . Ensemble of GA based selective neural network ensembles . In ICONIP’01 , pages 1477–1482 , Shanghai , China .
[ 23 ] Z H Zhou , J . Wu , and W . Tang . Ensembling neural networks : Many could be better than all . Artificial Intelligence , 137(1 2):239–263 , 2002 .
