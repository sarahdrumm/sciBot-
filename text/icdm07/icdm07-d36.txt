Training Conditional Random Fields by Periodic Step Size Adaptation for
Large Scale Text Mining
Han Shen Huang
Yu Ming Chang
Chun Nan Hsu
Institute of Information Science
Academia Sinica Taipei , Taiwan
{hanshen,porter,chunnan}@iissinicaedutw
Abstract
For applications with consecutive incoming training examples , on line learning has the potential to achieve a likelihood as high as off line learning without scanning all available training examples and usually has a much smaller memory footprint . To train CRFs on line , this paper presents the Periodic Step size Adaptation ( PSA ) method to dynamically adjust the learning rates in stochastic gradient descent . We applied our method to three large scale text mining tasks . Experimental results show that PSA outperforms the best off line algorithm , L BFGS , by many hundred times , and outperforms the best on line algorithm , SMD , by an order of magnitude in terms of the number of passes required to scan the training data set .
1 . Introduction
Many algorithms have been proposed to train Conditional Random Fields ( CRF ) . Among them , the limitedmemory BFGS ( L BFGS ) algorithm [ 12 ] has become virtually the standard algorithm for training CRF . Though LBFGS can significantly reduce the number of iterations required , it still needs to scan the training set many times , which is impractical for some large scale applications . OnLine learning has the potential to obtain a parameter vector that achieves a likelihood as high as that achieved by off line learning even before scanning the entire training set . Another advantage of on line learning is that it has a much smaller memory footprint than its off line learning counterparts . Vishwanathan et al . [ 17 ] proposed a stochastic meta descent ( SMD ) method for CRF training , demonstrating that on line learning is feasible for large scale CRF training . However , experimental results show that though SMD can reduce the number of passes required to scan the training set , it requires careful tunings and may sometimes converge to a parameter vector that is worse than that which off line algorithms can reach .
In this paper , we present the periodic step size adaptation ( PSA ) method derived by aggregating on line update mappings . Though there are already many step size adaptation methods available , our method is different in that we adjust the learning rate periodically . Our adjustment is very simple but is based on accurate approximation of optimal adjustment . Periodic adjustment is proved to improve the accuracy of approximation . We applied our method to three large scale text mining tasks . Experimental results show that PSA outperforms the best off line algorithm , L BFGS , by many hundred times , and outperforms the best on line algorithm , SMD , by an order of magnitude .
2 . Conditional Random Fields
The CRF [ 9 ] is one of the most prevailing solutions to sequential data classification . In a CRF , sequences and their labels are transformed into features . The probability of a labeling result is a function of weighted sum of features . Training of CRFs is to assign proper weights for all features , trying to minimize the negative log likelihood or penalized negative log likelihood with the training data . Let D = {(x1 , y1 ) , . . . , ( xT , yT )} denote a set of T data sequences xk and the corresponding labels yk . A CRF defines l features to be transformed from a given instance ( x , y ) : F ( x , y ) = ( f1(x , y ) , . . . , fl(x , y))T , where fi(x , y ) is the number of times that feature i occurs in ( x , y ) . A CRF is parameterized by the weights for all features : θ = ( θ1 , . . . , θl)T . Then , the conditional probability of y given x is : pθ(y|x ) = exp(θT F ( x,y ) ) , where Zθ(x ) is a normalizay exp(θT F ( x , y) ) . Training of CRFs tion term : Zθ(x ) = is to search for the weight vector that minimizes the negative denoted by LD(θ ) , is : LD(θ ) = − log likelihood function as the objective function , which , k log pθ(yk|xk ) = −
. fi θT F ( xk , yk ) − log Zθ(xk )
Zθ ( x ) k
.
3 . Training CRF Off line
42 Triple Jump Extrapolation
The generalized iterative scaling ( GIS ) algorithm [ 2 ] , originally designed to train maximum entropy models , became well known because of its use as the training algorithm for CRF [ 9 ] . Since GIS is extremely slow to converge , other approaches to CRF training that applies gradientbased numerical optimization algorithms have been proposed [ 10 ] . Among them , the limited memory variable metrics ( L BFGS ) method has become the de facto standard now . L BFGS is a second order method that is derived from the second order Taylor expansion : LD(θ + ∆ ) ≈ LD(θ)+ 2∆T H(θ)∆ , where g(θ ) is ∇LD(θ ) , and H(θ ) is ∆T g(θ)+ 1 the Hessian matrix of LD(θ ) given by H(θ ) = I σ2 +Cov(θ ) , where Cov(θ ) is the covariance of the feature vectors under model distribution pθ(x , y ) . By applying Newton ’s method , we obtain an update rule as follows :
θ(t+1 ) = θ(t ) − H
−1(θ(t))g(θ(t) ) .
( 1 )
The Hessian matrix of the log likelihood involves the covariance matrix , which is usually difficult to compute given even a small number of features . L BFGS overcomes the problem by approximating the l × l Hessian matrix with a limited memory c × l variable metric , where c is a small constant , in the range of 3 to 10 , while l can be millions . A huge saving is therefore achieved . Malouf [ 10 ] reported that L BFGS significantly outperforms GIS and other gradientbased algorithms in terms of the rate of convergence .
4 . Accelerating Generalized Iterative Scaling
One of the drawbacks of Aitken ’s acceleration is that it must evaluate the Jacobian , which involves the covariance matrix that may not have a closed form and can be intractable even for a very simple model with a low dimensional parameter space . Other drawbacks include that it may not always converge and that it may be numerically unstable [ 4 ] . One may approximate the Jacobian to overcome these drawbacks . For example , replace ( I − J)−1 in Equation ( 3 ) with a dynamically adjusted scalar [ 13 ] .
∗
−1θ
∗ := Q
∗ . Then in the eigenspace of J , we have ψ Mi−ψ ( t )
Here we review another method where the adjustment can be guided as follows . Let ψ be the eigen i ≈ ∗ transformed θ ( t ) ( t ) i + 1 i ) , where λi is the i th eigenvalue of 1−λi ( ψ ψ −1 is not close to I , we can simplify Aitken ’s acJ . When Q celeration by replacing λi with γ(t ) such that γ(t ) ≈ λmax at the t th iteration : θ(t+1 ) = θ(t)+(1−γ(t))−1(θ M −θ(t) ) . M − θ(t ) , we Since J(θ(t ) − θ(t−1 ) ) ≈ θ(t+1 ) − θ(t ) = θ define
( t )
( t )
γ(t ) :=
( t )
'θ M − θ(t)' 'θ(t ) − θ(t−1)' .
( 4 )
Because to extrapolate to θ(t+1 ) , we need to apply M con(t ) secutively to obtain θ(t ) and θ M , Huang et al . [ 3 ] named this method as the Triple Jump Acceleration .
It is also possible to extrapolate with a different rate for each component or sub vector of the parameter vector . Let p be the index of a component or sub vector in θ . Then with
41 Aitken ’s Acceleration
γ(t ) p
:=
( t )
'θ Mp − θ p − θ 'θ p ' ( t ) ( t−1 ) p
( t )
' ,
( 5 )
When applied to large training sets for CRF , GIS is extremely slow to converge . One approach to accelerating GIS is to consider GIS as a fixed point iteration mapping and apply Aitken ’s acceleration . Suppose that we apply the fixed point iteration method from θ(t ) in the neighborhood . Assuming that the of θ mapping M is differentiable , we can apply a linear Taylor expansion of M around θ and the iteration converges at θ
∗
∗
∗ so that θ(t+1 ) = M(θ(t ) )
+ J(θ(t ) − θ
∗ ( 2 ) ∗ ) , the Jacobian of the mapping . Assuming that eig(J ) ∈ ( −1 , 1 ) , the multivariate where J abbreviates M M at θ Aitken ’s acceleration is given by [ 11 ] :
≈ θ .(θ
) ,
∗
∗
∗ ≈ θ(t ) +
θ
Jh(θ(t+1 ) − θ(t ) )
∞' h=0
= θ(t ) + ( I − J )
−1(θ(t+1 ) − θ(t) ) ,
( 3 )
. we can perform extrapolation for each component or subp ) , ∀p . vector by θ p + ( 1 − γ p )−1(θ
Mp − θ
( t+1 ) p
= θ
( t )
( t )
( t )
( t )
5 . Training CRF On Line
51 Stochastic Gradient Descent Methods
Stochastic gradient descent ( SGD ) methods iteratively update the parameters of a model with gradients computed by small batches of b examples . Unlike gradient descent , in SGD , the global objective function LD(θ ) is not accessible during the stochastic search . Instead , only a local objective function L(t)(θ ) based on the batch of examples at iteration t is used . Let examples be drawn from D with equal weights . Then , the following relation exists between L(t)(θ ) and LD(θ ) : E(L(t)(θ ) ) , where E(L(t)(θ ) ) is the expectation with respect to the distribution of examples in D , and |D| is the number of examples in D . The above equation describes that the normalized objective
LD(θ ) |D|
= b function given D is the expectation of the normalized online objective function given a random batch .
Let g(t ) denote ∂L(t)(θ(t ) )
∂θ
. The parameter vector is up dated with the following equation by SGD methods :
θ(t+1 ) = θ(t ) − η(t ) · g(t ) .
( 6 )
It has been proved that SGD with a proper fixed small learn[8 ] , but ing rate η can converge to the neighborhood of θ the convergence rate will be extremely slow . SGD can converge faster with a relatively large learning rate , which will be annealed or discounted gradually to ensure convergence . For further acceleration , second order methods are usually adopted in which estimation of Hessians is required .
∗
52 Stochastic Meta Descent
Stochastic meta descent ( SMD ) [ 17 ] is currently the fastest SGD method for training CRF reported in the literature . In CRF , most of the elements of the parameter vector are mutually independent so that the Hessian is close to a diagonal matrix . Therefore , they approximate the Hessian by means of a vector storing the diagonal elements . SMD uses a vector of local learning rates in Equation ( 6 ) .
Other on line algorithms that may potentially be applied to the training of CRF include the Margin Infused Relaxed Algorithm ( MIRA ) [ 1 ] . On each on line update , MIRA attempts to keep current and new parameter vectors as close as possible , subject to correctly classifying the given data with margins to incorrect classifications larger than some loss function . MIRA has been included in the latest release of CRF++ [ 6 ] .
6 . Our Method
61 Optimal Step Size Adaptation t=0 b −1 ff LB(θ)+ b'θ'2 2|D|σ2
In stochastic gradient descent , approximated objective function is used to compute the gradient . L(θ;D ) =
|D| , where B(t ) is a batch of b examples ⊆ D . In off line gradient descent , we apply this update rule : θ(t+1 ) = θ(t)− η∇L(θ(t);D ) , but in on line situation , the update rule becomes θ(t+1 ) = θ(t ) − η∇L(θ(t ) ; B(t) ) . b as the step size in SGD , we have θ(t+1 ) = θ(t)− Using η ff∇LB = θ(t ) − ηg(t ) := Mb(θ(t) ) . We can b + θ(t ) |D|σ2 η consider that the on line mapping Mb is a random variable with its mean equal to the off line mapping , where the entire set of data is used to evaluate the gradient . θ(t+1 ) = θ(t)+(I−J)−1(M(θ(t))−θ(t ) ) ≈ θ(t)− η Therefore , we can update ηi componentwise by
From Aitken ’s acceleration ,
, we need 1−γ g(t ) . to reach θ
∗
( t+1 ) η i
( t ) = η i
1 1 − γ
( t ) i
,
( 7 )
( t ) can be estimated by Equation ( 5 ) . However , due where γ i to stochastic nature of the mapping , it is unlikely that we can obtain an accurate eigenvalue estimation at each iteration . Moreover , since Aitken ’s acceleration does not guarantee convergence , when the extrapolation does not improve the log likelihood , we can throw away that extrapolation and apply the original mapping to ensure convergence in offline applications , but in on line situations , this is impossible . Therefore , we need to derive a new method to use the estimates of the eigenvalues in on line situations .
62 Periodic Step Size Adaptation n → R
We follow the SGD framework in Equation ( 6 ) to adjust the learning rates for on line text mining with huge models . Our method uses a vector of local learning rates corresponding to each dimension of the parameter vector . The strategy is to discount learning rates based on the estimated eigenvalues of each dimension : higher/lower discounts for learning rates of fast/slow dimensions . This strategy is consistent with the optimal step size adaptation given in Equation ( 7 ) . In our method , we consider Equation ( 6 ) as an on line n : Mb(θ(t ) ) := θ(t)−η·g(t ) . We mapping Mb(θ ) : R also assume that H(t ) = H(θ(t ) ) is a diagonal matrix . Taking the derivative of Mb(θ(t ) ) with respect to θ , we can obtain J(t ) , the Jacobian of Mb at θ(t ) , which is also a diagonal ∂θ Mb(θ(t ) ) := J(t ) = I− ηH(t ) . In on line gradimatrix : ∂ ent descent methods , however , the estimation is usually inaccurate because only small batches of data are used to update parameter vectors . To reduce the deviation to the true eigenvalues , we aggregate consecutive mappings to average n estimations . We use four parameter vectors , θ(t ) , θ(t+1 ) , θ(t+n ) , and θ(t+n+1 ) , to estimate eigenvalues . In an offline situation , we apply Taylor expansion at θ and derive the following relation : θt+n+1− θt+n ≈ ( J∗)n(θt+1− θt ) . From the result , we can estimate the eigenvalues to the n th power by γn i After running 2n iterations , we aggregate these estimates by computing their average , weighted by the ratio of componentwise one step difference and the entire search path ( t ) along dimension i after n iterations , that is , θ . i This is equivalent to averaging the numerators and denominators separately , resulting in efficient estimation of the average of γn
:= θ(t+n+1 ) θ(t+1 ) i
−θ(t+n ) −θ(t )
( t+n ) i
− θ
∗
. i i i i , denoted by ¯γn i : n−1
¯γn i
:=
= θ k=0
n−1 k=0
( t+2n ) i
( t+n ) i
θ i
−θ(t+n+k ) θ(t+n+k+1 ) i n −θ(t+k ) θ(t+k+1 ) i n ( t+n ) i i
− θ − θ
( t ) i
.
( 8 )
For computational stability , we introduce a constant κ as the upper bound of |¯γn i | . Let ai denote the constrained ¯γn i : ( 9 ) i ) min(|¯γn i | , κ ) . ai := sgn(¯γn
Instead of updating the learning rate at every iteration , we update the learning rates every 2n iterations based on ai ’s : where
( t+2n+1 ) η i
( t+2n ) = biη i
, bi = M + ai
M + κ + m
.
( 10 )
( 11 )
In Equation ( 11 ) , M and m control the scale of discount factors bi ’s . We can define the maximal value α and minimal value β of the discount and then obtain M and m by solving β ≤ bi ≤ α . Since −κ ≤ ai ≤ κ , we have bi = α when ai = κ and bi = β when ai = −κ . By solving the equations , M and m are : M = α + β α − β
2(1 − α ) α − β
κ and m =
( 12 )
κ .
At last , we summarize our SGD method as follows :
Algorithm 1 : PSA Given : η(0 ) , α , β , n ; Compute M and m by Equation ( 12 ) ; repeat
Compute θ(t+1 ) by Equation ( 6 ) ; if ( t + 1 ) mod 2n = 0 then
Compute ai for all i by Equation ( 8 ) and ( 9 ) ; Compute η(t+1 ) by Equation ( 10 ) and ( 11 ) ; else
η(t+1 ) = η(t ) ; end t = t + 1 ; until Convergence ;
7 . Experiments
We evaluate our proposed method by comparing the performance with SMD . We ran experiments on three data sets : CoNLL 2000 chunking task [ 14 ] , BioNLP/NLPBA2004 bio entity recognition task [ 5 ] , and BioCreative II gene mention task [ 18 ] . We randomly reorder the training data as the input sequences many times to simulate the online scenario . The performance is measured by the average F scores for the hold out set during the training time . The F score is defined as F = 2P R P +R , where P is the precision and R the recall .
We applied PSA , SMD , and L BFGS training methods in our experiments . PSA was implemented by modifying CRF++ 1 . We downloaded the SMD program 2 developed
1Available from http://crfppsourceforgenet/ from
2Available
LGPL under
LGPL under the the following
URL : following
URL : http://smlnictacomau/code/crfsmd/
)
%
( e r o c s − F
95
94
93
92
91
90
89
88
87
86
85 0
CoNLL 2000
PSA SMD L−BFGS
10
20
Passes
30
40
50
Figure 1 . Learning curves of our method , SMD , and L BFGS on CoNLL 2000 data set . by Vishwanathan et al . [ 17 ] , which was also implemented by revising CRF++ . At last , we used CRF++ again for LBFGS . Though L BFGS is an off line algorithm , we ran it to obtain the F scores every time after processing the whole data set to examine the advantage of on line methods and to estimate the final F score that on line methods are supposed to achieve after convergence .
Here are the parameters for SMD and our method : • SMD : We used the typical setting described in Vishwanathan et al . [ 17 ] for SMD , that is , µ = 0.1 , λ = 1.0 , and η(0 ) = 01 The batch size is fixed to 8 for all data sets .
• PSA : We used ( α , β ) = ( 0.9999 , 0.99 ) , n = 10 , and
η(0 ) = 01 The batch size is 1 for all data sets .
71 CoNLL 2000 Chunking Task
Our first experiment used the data set of the CoNLL2000 chunking task [ 14 ] . The task was to construct a shallow parser for base NP chunking , which labels each word as one of B ( beginning of chunk ) , I ( in a chunk ) , and O ( outside a chunk ) . The training data set contains 8,936 labeled sentences and the test data set contains 2,012 sentences .
We used a CRF model with about 1,000,000 features , which covers the features used by Sha and Pereira [ 16 ] . They achieved an F score of 94.19 % with their own extended features .
Figure 1 shows the learning curves of our method , SMD , and L BFGS for the CoNLL 2000 data set . The learning curves are defined as the function of the progress of F scores given the number of processed examples , measured by the number of passes through the entire training data sets . We plotted the learning curves in the first 50 passes because both on line methods converged earlier .
NLPBA04
BioCreative 2 GM Task
)
%
( e r o c s − F
80
70
60
50
40
30
20 0
PSA SMD L−BFGS
10
20
Passes
30
40
50
)
%
( e r o c s − F
85
80
75
70
65
60 0
PSA SMD L−BFGS
10
20
Passes
30
40
50
Figure 2 . Learning curves of our method , SMD , and L BFGS on BioNLP/NLPBA 2004 data set .
Figure 3 . Learning curves of our method , SMD , and L BFGS on BioCreative II data set .
The F score of L BFGS can converge to 94 % in about 200 passes , while it arrived at 93.5 % in 50 passes . Our method reached an F score of 93.6 % very quickly in about 1.12 passes , and stayed above 93.6 % steadily . After 8 passes , the F score of our method exceeded 94 % and finally converged at 9405 % SMD , reported as the fastest on line method for CRF , arrived at 93.6 % asymptotically in about 7.7 passes in our experiment . SMD once reached 93.8 % but finally stayed around 936 % The result of SMD is similar to that described in Vishwanathan et al . [ 17 ] .
From the experiment , we found that our method is about 6.87 times as fast as SMD in terms of passes in achieving the F score that SMD converged at and our method ended up with a better model than SMD did . The final F score of our method is almost the same as that of L BFGS .
72 BioNLP/NLPBA 2004 Bio Entity
Recognition Task the data
Our second experiment used set of BioNLP/NLPBA 2004 bio entity recognition task [ 5 ] . The goal of the task is to identify technical terms in the domain of molecular biology and classify them into five categories corresponding to concepts of interest to biologists [ 5 ] .
We used a CRF model with about 6,000,000 features , most of which follow the features used by Settles [ 15 ] . The overall F score of the system of Settles is 698 %
Figure 2 shows the learning curves of our method , SMD , and L BFGS on the BioNLP/NLPBA 2004 data set . LBFGS arrived at an F score of 67 % in 40 passes , 70 % in 70 passes , and stayed above 70 % until convergence . Our method reached an F score of 67 % very quickly in about 0.54 passes , then reached and kept above 70 % after processing 1.01 passes . After 1.68 passes , the F score of our method exceeded 71 % and finally converged at 714 %
SMD arrived at 67 % in about 13 passes in our experiment . Then , the F score of SMD oscillated between 66 % and 68 % until 50 passes . In Vishwanathan et al . [ 17 ] , they used different feature sets and achieved a remarkable F score of more than 85 % . However , SMD , with a slightly different setting ( µ = 0.02 and b = 6 ) , still took about 10 passes to converge .
In this experiment , our method is about 24 times as fast as SMD in terms of passes in achieving the F score that SMD converged at , and again our method ended up with a better model than SMD did . The quality of our learned model is the same as that of L BFGS in terms of the F score .
73 BioCreative II Gene Mention Task
Our third experiment used the data set of BioCreative II gene mention task [ 18 ] . The goal of the task is to extract all the genes and gene products mentioned in given MEDLINE sentences . In the competition , 15,000 sentences were released as the training data set and 5,000 were left as the test data set for final evaluation .
We employed a CRF model with about 7,300,000 features to label B , I , and O for gene mention extraction . We used most of the features of Kuo et al . [ 7 ] , who achieved an F score of 86.83 % in the competition by training with LBFGS , the best result achieved by CRF in that competition . Figure 3 shows the learning curves of our method , SMD , and L BFGS for the BioCreative II data set . L BFGS reached an F score at 84 % in 110 passes , 85 % in 156 passes , and stayed above 86 % after 200 passes . Our method jumped beyond an F score of 85 % in about 1.66 passes , then reached and kept above 86 % after processing 3 passes . After processing the data set for 4 passes , the F score of our method exceeded 86 % and finally converged at 8646 % SMD arrived at 84 % in about 16.67 passes in our experi ment . Then , SMD kept the F score between 84 % and 85 % until 50 passes .
Our method is about an order of magnitude faster than SMD in terms of passes in achieving the highest F score of SMD , and finally our method produced a better model than SMD did . The models trained by PSA and L BFGS achieved competing F scores .
8 . Conclusion
We have proposed PSA , a new step size adaptation method for stochastic gradient descent for on line largescale text mining with huge CRF models . We compared the asymptotic behavior of our method with that of SMD , which is currently reported as the fastest method for training CRF on line . Our method adopts an efficient way to estimate the Jacobian of mappings , and adjust learning rates according to the estimated Jacobian . The adjustment is more efficient than that of SMD . Moreover , our experiments show that training by PSA is 6.87 to 24 times faster than that of SMD in terms of the passes through the training data set , and at the same time , achieves competing F scores with LBFGS in all the experiments .
We plan to conduct more large scale experiments on a wide variety of application domains to further evaluate the performance of our method and compare with MIRA and other CRF training algorithms . We also plan to extend our method to other learning problems , such as the kernel based methods , to on line learning .
Acknowledgements
This work was supported in part by the National Research Program in Genomic Medicine ( NRPGM ) , National Science Council , Taiwan , under Grant No . NSC95 3112 B001 017 ( Advanced Bioinformatics Core ) , and in part under Grant No . NSC95 2221 E 001 038 .
References
[ 1 ] K . Crammer and Y . Singer . Ultraconservative online algorithms for multiclass problems . Journal of Machine Learning Research , 3:951–991 , January 2003 .
[ 2 ] J . N . Darroch and D . Ratcliff . Generalized iterative scaling for log linear models . The Annals of Mathematical Statistics , 43(5):1470–1480 , 1972 .
[ 3 ] H S Huang , B H Yang , and C N Hsu . Triple jump acceleration for the EM algorithm . In Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 ) , pages 649–652 , 2005 .
[ 4 ] M . Jamshidian and R . I . Jennrich . Acceleration of the EM algorithm by using quasi newton methods . Journal of the Royal Statistical Society , , Series B , 59(3):569–587 , 1997 .
[ 5 ] J D Kim , T . Ohta , Y . Tsuruoka , Y . Tateisi , and N . Collier . Introduction to the bio entity recognition task at JNLPBA . In Proceedings of the Joint Workshop on Natural Language Processing in Biomedicine and its Applications ( JNLPBA2004 ) , pages 70–75 , 2004 . CRF++ : another CRF toolkit
[ 6 ] T . Kudo .
Yet
( http://crfppsourceforgenet/ ) , 2006 .
[ 7 ] C J Kuo , Y M Chang , H S Huang , K T Lin , B H Yang , Y S Lin , C N Hsu , and I F Chung . Rich feature set , unification of bidirectional parsing and dictionary filtering for high f score gene mention tagging . In Proceedings of the Second BioCreative Challenge Evaluation Workshop , pages 105–107 , 2007 .
[ 8 ] H . J . Kushner and H . Huang . Asymptotic properties of stochastic approximations with constant coefficients . SIAM Journal on Control and Optimization , 19(1):87–105 , 1981 . [ 9 ] J . Lafferty , A . McCallum , and F . Pereira . Conditional random fields : Probabilistic models for segmenting and labeling sequence data . In Proceedings of 18th International Conference on Machine Learning ( ICML’01 ) , pages 282– 289 , 2001 .
[ 10 ] R . Malouf . A comparison of algorithms for maximum entropy parameter estimation . In Proceedings of the Sixth Conference on Natural Language Learning ( CoNLL 2002 ) , pages 49–55 , 2002 .
[ 11 ] G . J . McLachlan and T . Krishnan .
The EM Algorithm and Extensions . Wiley Series in Probability and Statistics . Wiley Interscience , 1997 .
[ 12 ] J . Nocedal and S . J . Wright . Numerical Optimization .
Springer , 1999 .
[ 13 ] R . Salakhutdinov and S . Roweis . Adaptive overrelaxed bound optimization methods . the Twentieth International Conference on Machine Learning ( ICML’03 ) , pages 664–671 , 2003 .
In Proceedings of
[ 14 ] E . F . T . K . Sang and S . Buchholz . Introduction to the conll2000 shared task : Chunking . In Proceedings of Conference on Computational Natural Language Learning ( CoNLL2000 ) , pages 127–132 , 2000 .
[ 15 ] B . Settles . Biomedical named entity recognition using conditional random fields and novel feature sets . In Proceedings of the Joint Workshop on Natural Language Processing in Biomedicine and its Applications ( JNLPBA 2004 ) , pages 104–107 , 2004 .
[ 16 ] F . Sha and F . Pereira . Shallow parsing with conditional random fields . In Proceedings of Human Language Technology , the North American Chapter of the Association for Computational Linguistics ( NAACL’03 ) , pages 213–220 , 2003 .
[ 17 ] S . Vishwanathan , N . N . Schraudolph , M . W . Schmidt , and K . P . Murphy . Accelerated training of conditional random fields with stochastic gradient methods . In Proceedings of the 23rd International Conference on Machine Learning , Pittsburgh , PA , USA , June 2006 .
[ 18 ] J . Wilbur , L . Smith , and L . Tanabe . Biocreative 2 . gene mention task . In Proceedings of the Second BioCreative Challenge Evaluation Workshop , pages 7–16 , 2007 .
