Seventh IEEE International Conference on Data Mining Seventh IEEE International Conference on Data Mining Seventh IEEE International Conference on Data Mining Seventh IEEE International Conference on Data Mining Seventh IEEE International Conference on Data Mining
Incremental Subspace Clustering over Multiple Data Streams
Qi Zhang
Jinze Liu
Wei Wang
University of North Carolina , Chapel Hill , NC 27599
{zhangq , liuj , weiwang}@csuncedu
Abstract
Data streams are often locally correlated , with a subset of streams exhibiting coherent patterns over a subset of time points . Subspace clustering can discover clusters of objects in different subspaces . However , traditional subspace clustering algorithms for static data sets are not readily used for incremental clustering , and is very expensive for frequent re clustering over dynamically changing stream data . In this paper , we present an efficient incremental subspace clustering algorithm for multiple streams over sliding windows . Our algorithm detects all the δ CC Clusters , which capture the coherent changing patterns among a set of streams over a set of time points . δ CC Clusters are incrementally generated by traversing a directed acyclic graph pDAG . We propose efficient insertion and deletion operations to update the pDAG dynamically . In addition , effective pruning techniques are applied to reduce the search space . Experiments on real data sets demonstrate the performance of our algorithm .
1
Introduction
Stream mining problem has attracted a lot of research interest due to emerging stream applications such as network monitoring , sensor networks , financial data analysis , etc . Different from static data set , stream data is dynamically changing , potentially infinite , and possibly with fast speed , which poses new challenges to mining algorithm design .
To find interesting patterns for multiple data streams evolving over time , clustering stream into groups which exhibit coherent patterns has attracted significant interest . Streams are often inherently correlated . For example , the traffic volumes of connections in the same network , the stock prices of related businesses , the environmental readings from nearby sensors often exhibit coherent or correlated patterns . Traditional distance based clustering methods group together streams similar to each other . The distance between streams are evaluated over the entire time dimension . However , for real world data , the stream corre lations are typically local and only to a subset of time points for a subgroup of streams . Therefore , subspace clustering is more effective for discovering patterns over any subgroup of streams and subset of time points . Stream data is dynamic . Subspace clustering algorithms for static data sets cannot readily be used for incremental computation and maintenance in stream setting . Re clustering whenever stream data is updated can be quite expensive especially for large window size or for the entire stream .
In this paper , we propose an efficient incremental subspace clustering algorithm . Our subspace clustering model δ CC Cluster considers the coherent changing pattern of a subgroup of streams over a subset of time points . Our model exerts two constraints for the subspace cluster . 1 ) Direction Constraint : the offsets of all the streams between any two time points inside the cluster are of the same direction ( increasing or decreasing ) , and 2 ) Difference Constraint : the maximum difference between the offsets of any two streams over two time points inside the cluster is bounded by δ . Based on the clustering model , we propose an efficient incremental algorithm for computing δ CC Cluster . The algorithm organizes the current patterns or potential future patterns into a directed acyclic graph pDAG according to the Anti monotone Property . Whenever a new time point is considered , the patterns in pDAG are accessed and extended to obtain new patterns in a certain order so that search space can be greatly reduced . Other pruning techniques are also proposed to further improve the algorithm performance .
2 Related Work
Subspace clustering has been extensively studied [ 3 , 4 , 2 , 1 ] in recent years . It finds clusters that exist in multiple , possibly overlapping subspaces within high dimensional data sets . In this section , we mainly review related work in δClusters . δ Clusters is a unique kind of subspace clusters that use coherence as the similarity measure for clustering objects over a subset of dimensions . It was first proposed in [ 5 ] to identify clusters of genes which exhibit coherent expression levels over subset of conditions . In [ 10 ] , pScore is proposed as the metric to evaluate the coherence within a
1550 4786/07 $25.00 © 2007 IEEE 1550 4786/07 $25.00 © 2007 IEEE 1550 4786/07 $25.00 © 2007 IEEE 1550 4786/07 $25.00 © 2007 IEEE 1550 4786/07 $25.00 © 2007 IEEE DOI 101109/ICDM2007100 DOI 101109/ICDM2007100 DOI 101109/ICDM2007100 DOI 101109/ICDM2007100 DOI 101109/ICDM2007100
719 719 719 727 727 subspace δ cluster described by similar shifting and scaling patterns . Liu and Wang [ 8 ] proposed OP Cluster , which defines the coherent ordering of values of a cluster of objects over a subset of attributes .
There are many algorithms for clustering data streams . We will only review the work in subspace clustering for data streams . Kontaki et al . [ 7 ] presented an algorithm for incremental subspace clustering over a set of data streams . They proposed α clusters , and provided efficient algorithms for computing the α clusters incrementally using a set of pruning techniques . However , α clusters only consider the subset of consecutive points over the time point dimension . In this paper , we consider a subspace clustering model which considers any subset of time points inside the sliding window . Narahashi and Suzuki [ 9 ] proposed an incremental subspace clustering algorithm to analyze web access log data for detecting the hostile accesses . Their algorithm uses a CF tree to compress data and effectively detect the subspace clusters .
3 Model
Consider a set of streams S = {s1 , . . . , si , . . . , sm} over a sequence of time points ht1 , t2 , . . . , tni , where tn is the current time point . We are interested in the patterns existing in the latest w time points T = htn−w+1 , . . . , tni . The stream data over this sliding window of size w can be represented as matrix W = ( di,j)m×w , where the ith row corresponds to stream si , the jth column corresponds to the jth time point inside the sliding window which is tn−w+j , also denoted as tW j .
Given the sliding window data matrix W = ( di,j)m×w over the stream set S and time point set T , we first define the Offset direction Constraint which describes the coherent offset direction exhibited by a pair of streams si1 , si2∈ S over a pair of time points tW j2 ∈ T . j1 , tW
Definition 3.1 Offset direction Constraint ( Dir C ) . Given sliding window data matrix W over stream set S and time point set T , an Offset direction Constraint is defined on a
2 by 2 sub matrix fifififi di1,j1 di2,j1 pair si1 , si2∈ S , and any time point pair tW requires : di1,j2 di2,j2 fifififi of W on any stream j2 ∈ T which j1 , tW pair si1 , si2∈ S , and any time point pair tW requires : j1 , tW j2 ∈ T which
|∆i1 j1,j2 − ∆i2 j1,j2| ≤ δ
( 2 )
To allow minor noises , we define j,j ′ =fl di,j − di,j ′
0
∆i if |di,j − di,j′ | > γ otherwise where δ , γ are user defined thresholds .
Based on Dir C and Dif C , we define our clustering model as follows :
Definition 3.3 δ CC Cluster . Given sliding window data matrix W over stream set S and time point set T . For any subset of streams Sp , and any subset of time points Tp , ( Sp , Tp ) is a δ CC Cluster iff . any 2 by 2 sub matrix in WSp,Tp satisfies Dir C and Dif C .
In fact , Dif C is the same requirement as in δ pCluster [ 10 ] and restricts p Score to be smaller than δ . δ pCluster controls the maximum difference between the offsets of any two streams for any pair of time points inside the cluster . However , it does not require the offsets to be in the same direction ( increasing/decreasing ) . In fact , the offset direction becomes critical if we have a considerably large δ ( in which case we want to relax δ to get more patterns ) . δ CCCluster controls both change in direction and maximum offset difference using Dir C and Dif C , and therefore qualifies as both a δ pCluster and an OP Cluster . As a result , the δ parameter can be safely and flexibly adjusted to get the patterns with desired quality of compactness .
In this paper , we consider δ CC Cluster ( Sp , Tp ) as a significant pattern if it satisfies |Sp| ≥ nr and |Tp| ≥ nc , where nr , nc are user defined thresholds for minimum number of streams ( rows ) and minimum number of time points ( columns ) required for a cluster .
4 Algorithms
Subspace clustering over data streams requires performing the clustering process in a sliding window over the streaming data . To avoid computationally intensive reclustering , an incremental clustering algorithm is required . sign(di1,j2 − di1,j1 ) = sign(di2,j2 − di2,j1 ) .
( 1 )
4.1 pDAG and pTable
With Offset direction Constraint , we define the Offset difference Constraint as follows :
Definition 3.2 Offset difference Constraint ( Dif C ) . Given sliding window data matrix W over stream set S and time point set T , an Offset difference Constraint is defined on a 2 by 2 sub matrixfifififi di1,j1 di2,j1 di1,j2 di2,j2 fifififi of W on any stream
We maintain a directed acyclic graph pDAG and a pattern table pT able which record all the δ CC Clusters for the current sliding window W as well as all the potential δ CC Clusters ( #rows ≥ nr but #columns < nc ) for future sliding windows . Note that all clusters in the current window are not potential δ CC Clusters ( even if #rows ≥ nr ) . Trivially storing all the δ CC Clusters
720720720728728 t1 20 11 40 10 50 s1 s2 s3 s4 s5 t2 30 12 51 19 20 t3 45 95 64 36 75
( a ) t4 3 0 26 4 38 t5 100 82 45 89 89 t6 101 82 44 15 25 t3 t5 t2 t4 t1 t6
( b ) t1t2 t2t3 t3t4 t1t2t3t4:{s1s3s4 } t2t3t4:{s1s3s4 } t3t4 : {s1s3s4 } , {s3s4s5 } t5t6 t5t6:{s1s2s3 } t1 t2 t3 t4 t5 t6
( c )
Figure 1 . Initial sliding window and corresponding pDAG and pTable . ( a ) is the initial sliding window data matrix . ( b ) is the corresponding pDAG of ( a ) . ( c ) is the corresponding pTable of ( a ) . Here , w = 6 , δ = 4 , γ = 2 , nr = 3 , nc = 4 . The detailed initialization process is illustrated in Fig 2 .
Algorithm 1 Initialization( ˆti ) Input ˆti : stream data column at current time point ti
1 : Insert( ˆti , pDAG , pT able )
2 : P rune(pDAG , pT able )
Algorithm 2 IncConstruct( ˆti ) Input ˆti : stream data column at current time point ti
1 : Delete( ˆti−m , pDAG , pT able ) 2 : Insert(ti , pDAG , pT able )
3 : P rune(pDAG , pT able ) t1 t1 t1 t1 t3 t3 t2 t2 t2 t4 t1
( a ) Insert t1 t1 t1t2 t1t2:{s1s3s4}
( b ) Insert t2 t1t2 t1t3 t2t3 t1 t2 t3 t1t2:{s1s3s4} , t1t2t3 :{s1s3s4} t1t3 : {s1s3s4s5} t2t3 : {s1s3s4}
( c ) Insert t3
, t1t2t4 :{s1s3s4} t1t2:{s1s3s4} , t1t2t3 :{s1s3s4} t1t2t3t4:{s1s3s4} t1t3:{s1s3s4s5} t1t3t4 :{s1s3s4} , {s3s4s5} t1t4 : {s1s3s4 } , {s2s3s4s5 } t2t3:{s1s3s4 } t2t4:{s1s3s4 } t3t4 : {s1s3s4 } , {s3s4s5 }
, t2t3t4:{s1s3s4 } t1t2 t1t3 t1t4 t2t3 t2t4 t3t4 t1 t2 t3 t4
( d ) Insert t4 t2 t4 t2 t4 t1 t1 t6 t3 t5 t3 t5
, t1t2t4 :{s1s3s4} t1t2:{s1s3s4} , t1t2t3 :{s1s3s4} t1t2t3t4:{s1s3s4} t1t3:{s1s3s4s5} t1t3t4 :{s1s3s4} , {s3s4s5} t1t4 : {s1s3s4 } , {s2s3s4s5 } t2t3:{s1s3s4 } t2t4:{s1s3s4 } t2t5:{s1s2s4s5 } t3t4 : {s1s3s4 } , {s3s4s5 }
, t2t3t4:{s1s3s4 } t1t2 t1t3 t1t4 t2t3 t2t4 t2t5 t3t4 t1 t2 t3 t4 t5
( e ) Insert t5 t1 t1t2 t1t2t3 :{s1s3s4} , t1t2t4 :{s1s3s4}
, t1t2t3t4:{s1s3s4} t1t3 t2t3 t2t4 t2t5 t3t4 t1t3t4 :{s1s3s4} , {s3s4s5} t2t3:{s1s3s4 } t2t4:{s1s3s4 } t2t5:{s1s2s4s5 }
, t2t3t4:{s1s3s4 } t3t4 : {s1s3s4 } , {s3s4s5 } t5t6 t5t6:{s1s2s3 }
( f ) Insert t6 t2 t3 t4 t5 t6
Figure 2 . Initialization process for pDAG and pTable for the first sliding window . Here , w = 6 , δ = 4 , γ = 2 , nr = 3 , nc = 4 . The sliding window data matrix is in Fig 1(a ) . The final pDAG and pTable after initialization are in Fig 1(b ) , Fig 1(c ) . is extremely inefficient for both memory usage and pattern growth . Therefore , carefully designed pruning techniques and pattern growth strategies are used to greatly reduce the search space and improve the performance .
In pT able ,
δ CC Clusters and potential δ CCClusters ( Sp , Tp ) are organized according to the corresponding time sequence Tp = hti1 . . . tik i ( ti1 < . . . < tik ) ( We will use “ pattern ” to refer to any such ( Sp , Tp ) recorded inside pT able in the following discussion ) . For example , ( Sp1 , Tp ) , . . . , ( Spi , Tp ) share entry ti1 . . . tik in pT able ( see Fig 1(c ) , entry t3t4 : {s1s3s4} , {s3s4s5} ) . Each Sp satisfies |Sp| ≥ nr . However , not every Tp has length k ≥ nc . Note that even if the length of Tp is smaller than nc , the corresponding pattern could be potential δ CC Clusters in the future . The pattern entry ti1 . . . tik : Sp1 , . . . , Spi is indexed by the first time point ti1 and first time pair ti1 ti2 for easy manipulation in pattern growing process ( see Fig 1(c) ) .
: Sp1 , . . . , Spi
In pDAG , each node uniquely represents a time point ti , ti ≤ tn , where tn is the current time point ( Fig 1(b) ) . For any edge tj → ti , we have tj > ti . Each path in pDAG ti1 ← . . . ← tik corresponds to an entry in pT able : ti1 ti2 tik : Sp1 , . . . , Spi . For example in Fig 1(b ) , path t2 ← t3 ← t4 corresponds to entry t2t3t4 : {s1s3s4} in Fig 1(c ) . Note that not every pattern ( Sp , Tp ) with |Sp| > nr is stored in pT able , therefore , not every pattern can find the corre
721721721729729 sponding path . For example , pattern ( s1s3s4 , ht2t4i ) has 3 streams ( nc = 3 ) , but it is not in pT able and does not have corresponding path in pDAG since it is not a δ CCCluster and it is not possible to be one in the future . In addition , for time sequence T1 , T2 corresponding to two entries in pT able , if T1 ⊂ T2 , then the path in pDAG for T1 is a sub path of the path for T2 . If we traverse pDAG from terminal nodes which have no incoming edges , pDAG guarantees we will meet all the patterns with growing time sequence patterns .
In the following section , we will describe in detail the algorithm of constructing and updating pDAG and pT able to incrementally generate the patterns . The algorithm has two phases . The initialization phase constructs pDAG and pT able for the initial sliding window W0 . After the sliding window is filled up , the incremental growing phase updates pDAG and pT able of the current sliding window W to obtain pDAG and pT able for the next sliding window W ′ . The brief routines of both phases are illustrated in Algo 1 and Algo 2 .
4.2 Initialization
We begin with an example of how pDAG and pT able are constructed during the initialization phase ( see Fig 1 ) . Before proceeding , we first introduce time pair MDS ( maximal dimension set)[6 ] . A δ CC Cluster C1 = ( S1 , hti , tji ) is a time pair MDS if there exists no δCC Cluster C2 = ( S2 , hti , tji ) such that S1 ⊂ S2 . Particularly , we denote StreamSets(titj ) as the set {Si,j|(Si,j , hti , tji ) is a time pair MDS} . In general , for any Tp ⊆ T , we define StreamSets(Tp ) as {Sp|(Sp , Tp ) is a δ CC Cluster} . We also define
StreamSets(T1)f\StreamSets(T2 ) = {Sk|∃S1 , S2 : ( S1 ∈ StreamSets(T1 ) ) ∧ ( S2 ∈ StreamSets(T2 ) ) ∧ ( Sk = S1 \ S2)} .
In the following , we describe the process of building pDAG and pT able for the initial sliding window matrix W0 ( Fig 1(a) ) , with parameters w = 6 , δ = 4 , γ = 2 , nr = 3 , nc = 4 , pDAG and pT able .
1 . Initially , pDAG and pT able are empty . 2 . When t1 comes , node t1 is inserted into pDAG . We then create an empty entry indexed by t1 inside pT able ( Fig 2(a) ) .
3 . When t2 comes , we insert node t2 into pDAG , and create an empty entry indexed by t2 inside pT able ( Fig 2(b) ) . We compute the time pair MDSs between t2 with every other node in pDAG , ie , obtain StreamSets(t1t2 ) . We prune off any S ′ ∈ StreamSets(t1t2 ) where |S ′| < nr , then StreamSets(t1t2 ) = {s1s3s4} . We add edge t2 → t1 into pDAG , and add corresponding entry t1t2 : {s1s3s4} into pT able , which is indexed by t1 and t1t2 .
4 . When t3 comes , we insert node t3 into pDAG , we create an empty entry indexed by t3 inside pT able ( Fig 2(c) ) . We compute the time pair MDS between t3 with every other node in pDAG , ie , t1 and t2 . The MDSs are not empty , thus we add edges t3 → t1 and t3 → t2 into pDAG , and insert t1t3 : {s1s3s4s5} , t2t3 : {s1s3s4} into pT able . Then we start growing the old patterns using t3 by traversing through all the paths in pDAG starting from t3 . For example , consider path t3 → t2 → t1 . Starting from t3 , we set curStreamSets = {S} , where S is the whole stream set , and then move to node At t2 , we get StreamSets(t2t3 ) from pT able . t2 .
We update curStreamSets = curStreamSets eT
StreamSets(t2t3 ) , if curStreamSets is not empty , then move to node t1 . At node t1 , we get StreamSets(t1t3 ) . Also , the path we have traversed is t2 → t1 . We get the corresponding StreamsSets(t1t2 ) from pT able , curStreamSets and StreamsSets(t1t2 ) . If empty , insert t1t2t3 : currentM DS into pT able , with index t1 : t1t2 . Since t1 is already the end of the path , we stop . The traversal of the other path t3 → t1 is similar . update StreamSets(t1t3 ) is curStreamSets = curStreamSets eT eT not we
5 . When t4 comes , we update pT able and pDAG in a similar way ( Fig 2(d) ) .
6 . When t5 comes , we update pT able and pDAG in a similar way(Fig 2(e) ) . After the insertion is done , we prune out all the entries Tp : StreamSets(Tp ) indexed by t1 where |Tp| < 3 . Those patterns can at most be extended by 1 after t6 comes . After pruning , there is no pattern which is indexed by t1t4 . We also delete edge t4 → t1 from pDAG .
7 . When t6 comes , similar insertion and pruning operations are performed ( Fig 2(f) ) .
The Insert and Prune operations are formally defined in
Algo 3 and Algo 4 .
421
Insert Operation
Given sliding window W , the corresponding pDAG and pT able , and the stream data ˆti of the new coming time point ti , the Insertion operation grows the patterns in pT able by traversing pDAG to generate new patterns containing ti , and update pDAG and pT able . To scan through all the old patterns ( Sj , Tj ) in pT able for pattern growth , we consider the following Anti Monotone property :
Property 4.1 Anti Monotone Property : If there exists no pattern ( Sj , Tj ) which can grow with ti , ie , StreamSets( Tjti ) = Φ , then no pattern ( Sk , Tk ) , Tk ⊃ Tj can grow with ti .
Algorithm 3 Insert( ˆti , pDAG , pT able ) Input ˆti : stream data column at current time point ti
Insert node ti into pDAG . for every node tj ∈ pDAG do
1 : 2 : Create empty entry indexed with ti in pT able . 3 : 4 : 5 : 6 : 7 : 8 :
Compute MDSs forhtj , tii , get StreamSets(tj ti ) Prune out all Sps where Sp ∈ StreamsSets(tj ti ) and |Sp| < nr if StreamSets(tj ti ) is not empty then
Add edge ti → tj into pDAG Insert entry tj ti : StreamSets(tj ti ) into pT able indexed by tj : tj ti end if 10 : end for 11 : Start from ti , traverse all the nodes in pDAG which is reachable from ti . Set
9 : t∗ = ti , curStreamSets = {S} , curP ath = hi . for every node tj where there exists tj ← t∗ in pDAG do curP ath = htj , curP athi Look up pT able for StreamSets(titj ) Look up pT able for StreamSets(curP ath ) , if |curP ath| = 1 , set StreamSets(curP ath ) = {S} Compute curStreamSets curStreamSets
= eT
Streamsets(titj ) eT StreamSets(curP ath ) if curStreamSets is not empty then
Insert hcurP ath , tii : curStreamSets into pT able if If tj does not have outgoing edges in pDAG then break else t∗ = tj , go to 11 end if end if
12 : 13 : 14 : 15 :
16 :
17 : 18 : 19 : 20 : 21 : 22 : 23 : 24 :
25 : end for
Algorithm 4 Prune(ti , pDAG , pT able ) Input ti : current time point
1 : 2 : 3 : 4 : 5 : 6 : 7 : 8 : 9 : for every index time point tj ∈ pDAG do for every entry Tp : StreamSets(Tp ) indexed by tj do if |Tp| + ( tj − t1 ) + ( tw − ti ) < nc then
Delete Tp : StreamSets(Tp ) from pT able if there is no entry indexed by the same secondary index then
Delete the corresponding edge of the secondary index from pDAG . end if end if end for
10 : end for
This Anti Monotone property determines our search strategy during pattern growth . The Insertion operation scans all the patterns in pT able by traversing through all the paths in pDAG , where we always visit patterns ( Sp , Tp ) with shorter Tp first . The Anti Monotone property guarantees that the search space is reduced during Insertion and all the necessary patterns in pT able are examined and all the possible patterns are generated . In addition to completeness , the correctness is ensured by the following Lemma .
Lemma 4.1 Consider sliding window W over stream set S and time point set T , the new incoming time point ti , if Insert operation inserts an entry hTp , tii : SS , then for any pattern ( Sp , hTp , tii ) , Sp ∈ SS , we have ( Sp , hTp , tii ) is a δ−CC−Cluster , and Sp ≥ nr .
Suppose that entry hTp , tii : SS is generated at node tj . According to Algo 3 , SS = curStreamSets is computed at line 16 .
If |Tp| = 1 , SS = S eT StreamSets(tjti ) eT S = StreamSets(tjti ) . Therefore , for any Sp ∈
722722722730730
If |Tp| > 1 , let Tp = hT ′
SS , ( Sp , hTp , tii ) is the MDS for ( tj , ti ) , thus it is a δ−CC−Cluster . According to the pruning at line 5 , we p , tji , also have Sp ≥ nr . p ) . Here , SS′ is generated at the previous node , and entry p , tii : SS′ has been inserted to pT able . We only need hT ′ then SS = SS′ eT StreamSets(tjti ) eT StreamSets(T ′ to prove that ∀Sp ∈ SS , t′ , t′′ ∈ TpS{ti} , ( Sp , ht′ , t′′i ) is a δ CC Cluster . First , if t′ , t′′ ∈ Tp , it holds . Second , if t′ , t′′ are ti and tj , it holds . Finally , if one of t′ , t′′ is ti , the other one is in T ′ p , it also holds .
422 Prune Operation
The Prune operation prunes out all the patterns that are not or unable to be significant patterns , which reduces the search space for future pattern growth .
Some pruning criteria are applied inside the Insert op eration .
Pruning Criterion 1 During pattern growth in Algo 3 , any generated MDS ( Sp , htj , tii ) for the new time point ti and a previous time point tj with |Sp| ≤ nr is pruned out . Pruning Criterion 2 During the traversal of the pDAG along a path , if at a node tj , the patterns at tj cannot grow with the new time point ti , then stop moving forward along that path . This pruning criterion is according to the AntiMonotone Property .
In Prune operation , we perform pruning according to the following critera .
Pruning Criterion 3 For any pattern ( Sp , Tp ) in pT able where Tp starts with tj , suppose that the first time point of the current sliding window is t1 , the sliding window size is w , the current time point is ti , then ( Sp , Tp ) is not a potentially significant pattern if |Tp| + ( tj − t1 ) + ( tw − ti ) < nc ( see Algo 4 ) . If the sliding window is full , then tw = ti .
The current size of the pattern ( Sp , Tp ) along the time dimension is Tp . If the pattern starts at tj with t1 as the starting time point of the sliding window , then the pattern ( Sp , Tp ) can at most be extended by ( tj − t1 ) + ( tw − ti ) along the time dimension , where ti is the current time point . Therefore , for pattern ( Sp , Tp ) to be able to be a future significant pattern , it has to satisfy |Tp|+(tj −t1)+(tw −ti ) ≥ nc .
Pruning Criterion 4 For any edge e = ti → tj in pDAG , if there are no entries indexed by e in pT able , then delete e .
There is no entry indexed by e since htj , tii is not in any significant or potentially significant pattern . Therefore , e can be deleted to reduce the search space .
4.3
Incremental Construction t1 20 11 40 10 50 s1 s2 s3 s4 s5 t2 30 12 51 19 20 t3 45 95 64 36 75 t4 3 0 26 4 38 t5 100 82 45 89 89 t6 101 82 44 15 25 t7 33 30 55 25 37 t2 t4
W
( a ) t3 t5 t2t3 t3t4 t4t7 t5t6 t6t7 t2t3t4t7:{s1s3s4 } t3t4t7 : {s1s3s4 } t4t7:{s1s2s3s4 } t5t6:{s1s2s3 } t6t7:{s3s4s5 } t2 t3 t4 t5 t6 t7
( c ) t6 t7
( b )
Figure 3 . Example of incremental construction : sliding window t2 t7 and corresponding pDAG and pT able operation deletes all the related patterns containing the oldest time point and all its incident edges from pDAG , which will be obselete . Then the Insert and Prune operations are performed as Initialization phase .
Fig 3 illustrates an example of incremental construction from sliding window over ht1 , . . . , t6i ( Fig 1(a ) ) to the next sliding window over ht2 , . . . , t7i ( Fig 3 ) . The pDAG and pT able are after Delete , Insert and Prune operations .
5 Experiments
We performed our experiments on two real data sets 1 ) STOCK Data Set : 30 stocks are chosen from historical data of S&P 500 stocks1 , each has 253 values which represent the daily open prices of that stock throughout the year from May , 2006 till May , 2007 . 2 ) Climate Data Set2 : temperature data for 25 cities , each contains the daily temperature of the corresponding city throughout a year .
The performance of our algorithm is mainly dependent on the pruning efficiency obtained in a sliding window . Higher pruning efficiency results in faster incremental computations as the sliding window advances in time . The pruning efficiency is dependent on several factors . In order to measure the performance of our experiments , we used a sliding window of size 15 for STOCK data set ( 3 weeks ) and 14 for climate data set ( 2 weeks ) . We used default parameters of δ = 4.0 , γ = 0.2 , nr = 5 and nc = 7 for the stock data set , and δ = 0.3 , γ = 0.002 , nr = 4 , nc = 7 for the climate data set . In our experiments , we tested the performance of our algorithm by using default parameters for the datasets and varying one parameter at a time . Minimum number of time point(nc ) : We expect higher pruning efficiency as nc increases . Fig 4(a ) and 4(f ) compare the performance of our algorithm against the algorithm without pruning as nc varies . We observe that for larger nc , our algorithm achieves higher speedup as expected . Our algorithm also performs better in capturing larger patterns . Overall , for both data sets , our algorithm exhibits 2 18x speedup over the algorithm without pruning . Stream count threshold nr : As nr increases , the number of patterns common to at least nr streams decreases , therefore increasing the pruning efficiency . Figs . 4(b ) , and 4(g ) show the performance of our algorithm by varying nr . We
After sliding window W is filled up , pDAG and pT able are incrementally updated according to Algo 2 . The Delete
1http://bizswcpcom/stocks/ 2http://hprccunledu/data/indexphp
723723723731731
) 8 0 ms 7 0 fiperfiWDfi( 6 0 5 0 4 0 me i fiT 3 0 esponse 2 0 0 1 R 0 3 ) ms fiperfiWDfi( 2 2 1 me i fiT 1 esponse R
5
0 0 0 0 0 0
0
0 5 0 5 0 5
5
0 0 0 0 0 0
0
4 3 3 2 2 1 1
0 0 0 0 0 0
) 6 ms fiperfiWDfi( 5 4 3 me i fiT 2 esponse 1 R ) ms fiperfiWDfi( me i fiT esponse R
9 r i u n u fiP n n g i u n r i n g n g
9 i n g
N P o r u fiP n
N P o r
8
0 5 0 5 0 5 0
5
0 0 0 0 0 0 0
0
0
3
6
( a )
7 n c
8
6
( f )
7 n c
3
4
( b )
5 r n
4
( g )
5 r n
N P o r fiP n o r u
6
N P
7 r i u u n fiP n i n g n g i n g r i u n n g
6
2 0 8 6 4 2
0 0
0 0 0 0
0
4 3 3 2 2 1 1
) 1 ms fiperfiWDfi( 1 me fiTi esponse R ) ms fiperfiWDfi( me i fiT esponse R
7
2
.
5
0 5 0 5 0 5 0 5
0 0 0 0 0 0 0 0
0
0
.
1
N P o r u fiP n
N P o r r
3 i u u n fi
P n n g n g r i
0 i u n
2
. i n g n g
( c )
3
5
. e l t a
4
4
.
5 d
( h )
0 d
3
. e l t a
0
.
4
1
0
) ms 9 fiperfiWDfi( 8 7 6 5 me fiTi 4 esponse 3 2 R ) ms fiperfiWDfi( me i fiT esponse R
0 0 0 0 0 0 0 0 0 0
1
1 1 1 1
6 4 2 0
8 6 4 2
0
0 0 0 0 0 0 0 0 0 0
0 0 0 0
0 0 0 0
1
1
1
1
0 0 0 0
0 0 0 0 0
0
.
5 r i r
N P
N P o r u o r u fiP n fi
P n u u n n
1 i n i n n n g g
3
3 i 1 g
W g
W i n i n d
( d ) d
1
5 w o i z e fi
S
1
7
1
( i )
1
5 w o i z e fi
S
1
7
1
9
) ms fiperfiWDfi( me i fiT esponse R 4 0 0 3 5 0 3 0 0 2 5 0 0 2 0 5 1 0 0 1 0 0
5
0
9 ) ms fiperfiWDfi( me i fiT esponse R
2 2 1 1
5 0 5 0 5
0 0 0 0 0
0
1
0
1
0 g n g
N i n u
N P
N P o r u fi
P n o u r n r i
1
5 g n
N fiP n u r i u n
2 n i u
0 g m m e
( e ) b e
0
3 r fio
4
0 s m a r e t
5
0
( j ) b
0
2 r fi o fi
S f t
2
5 s m a
3
0 f fiS e r
Figure 4 . Performance on Stock ( the first row ) and Climate data ( the second row ) observe that our algorithm is up to 4x faster on the stock data and 5x faster on climate data . δ threshold : The δ threshold determines the maximum difference between two points in a pattern . As δ increases , we expect to identify more patterns satisfying the threshold . Therefore , the pruning efficiency decreases as δ increases and the overall performance decreases . Fig 4(c ) , and Fig 4(h ) highlight the performance of our algorithm with varying δ , with a speedup of 4x in Fig 4(c ) and a speedup of 5x faster in Fig 4(h ) . Sliding window size : As the sliding window size increases , we expect higher number of patterns . Therefore , we expect the overall performance to decrease as the sliding window size increases and vice versa . Fig 4(d ) , and Fig 4(i ) indicate that our algorithm achieves 5x and 6x higher performance than without pruning . Besides , the response time for our algorithm increases slowly and scales better than the algorithm without pruning . Number of streams : Similar to the sliding window size parameter , as the number of streams increases we expect higher number of patterns . Therefore the response time is expected to decrease as the number of streams increase . We tested the algorithms on Stock data sets of sizes 10 , 20 , 30 , 40 , 50 ( Fig 4(e) ) , and climate data sets of sizes 10 , 15 , 20 , 25 , 30 ( Fig 4(j) ) . Our algorithm with pruning techniques is up to 4x faster in Fig 4(e ) and up to 5x faster in Fig 4(j ) .
6 Conclusion
We proposed an efficient incremental algorithm for subspace clustering of multiple streams over sliding windows . Our subspace clustering model δ CC Cluster captures the coherent changing pattern of a subgroup of streams over a subset of time points . We maintain all the significant patterns and potential future significant patterns of the current sliding window and organize them into an acyclic graph pDAG and incrementally update the pDAG efficiently .
724724724732732
The experiments over two real data sets demonstrate the efficiency and effectiveness of our algorithm .
References
[ 1 ] C . Aggarwal , J . Wolf , P . Yu , C . Procopiuc , and J . Park . Fast algorithms for projected clustering . ACM SIGMOD International Conference on Management of Data , pages 61–72 , 1999 .
[ 2 ] C . Aggarwal and P . Yu . Finding generalized projected clusters in high dimensional spaces . ACM SIGMOD International Conference on Management of Data , pages 70–81 , 2000 .
[ 3 ] R . Agrawal , J . Gehrke , D . Gunopulos , and P . Raghavan . Automatic subspace clustering of high dimensional data for data mining application . The ACM International Conference on Management of Data , pages 94–105 , 1998 .
[ 4 ] C H Cheng , A . Fu , and Y . Zhang . Entropy based subspace clustering for mining numerical data . The Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 84–93 , 1999 .
[ 5 ] Y . Cheng and G . Church . Biclustering of expression datas . International Conference on Intelligent Systems for Molecular Biology , pages 93–103 , 2000 .
[ 6 ] S . Goil , H . Nagesh , and A . Choudhary . Mafia : Efficient and scalable subspace clustering for very large data sets . Technical Report CPDC TR 9906 010 , Northwestern University , June 1999 .
[ 7 ] M . Kontaki , A . Papadopoulos , and Y . Manolopouulos . Efficient incremental subspace clustering in data streams . 10th International Database Engineering and Applications Symposium ( IDEAS’06 ) , 2006 .
[ 8 ] J . Liu and W . Wang . Op cluster : Clustering by tendency in high dimensional space . Third IEEE International Conference on Data Mining ( ICDM’03 ) , 2003 .
[ 9 ] M . Narahashi and E . Suzuki . Detecting hostile accesses IEEE/WIC Inter through incremental subspace clustering . national Conference on Web Intelligence ( WI’03 ) , 2003 .
[ 10 ] H . Wang , W . Wang , J . Yang , and P . Yu . Clustering by pattern similarity in large data sets . ACM International Conference on Management of Data , pages 394–405 , 2002 .
