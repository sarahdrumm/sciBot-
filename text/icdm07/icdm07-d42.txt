Statistical Learning Algorithm for Tree Similarity
Atsuhiro Takasu
Daiji Fukagawa
National Institute of Informatics ftakasu,daijig@niiacjp
Tatsuya Akutsu Kyoto University takutsu@kuicrkyoto uacjp
Abstract
Tree edit distance is one of the most frequently used distance measures for comparing trees . When using the tree edit distance , we need to determine the cost of each operation , but this is a labor intensive and highly skilled task . This paper proposes an algorithm for learning the costs of tree edit operations from training data consisting of pairs of similar trees . To formalize the cost learning problem , we define a probabilistic model for tree alignment that is a variant of tree edit distance . Then , the parameters of the model are estimated using the expectation maximization ( EM ) technique . In this paper , we develop an algorithm for parameter learning that is polynomial in time ( O(mn2d6 ) ) and space ( O(n2d4 ) ) where n , d , and m represent the size of the trees , the maximum degree of trees , and the number of training pairs of trees , respectively .
1 . Introduction
Similarity measurement is one of the most basic problems in tree data mining . Tree edit distance is one of the most frequently used distances . It is a well studied distance metric and many algorithms for calculating the tree edit distance have been developed [ 5 , 7 , 3 ] . Due to the recent emergence of tree structured data , it is still attracting many researchers . The tree edit distance and its variants are applied in various kinds of applications , such as XML document classification , information extraction from the Web , and tree pattern detection from XML streams .
Because the similarity calculation between trees is computationally expensive , many studies focus on approximate similarity algorithms for efficient processing [ 6 , 2 ] . Yang et al . [ 6 ] proposed a binary branch distance that enables the efficient calculation of an approximate edit distance . Garofalakis and Kumar [ 2 ] proposed an algorithm to embed the tree edit distance into an L1 vector space and calculate the tree similarity efficiently in a small space .
These efficient algorithms are very useful in the filtering phase of the “ filter and refinement ” approach [ 6 ] . However , we need an elaborate similarity in the refinement phase . In this paper , we propose a new method to obtain the cost of edit operations systematically . There have been some studies on string edit cost learning [ 4 ] and learned string similarity being effectively used for an information integration task [ 1 ] . This paper extends this work to trees .
The main contributions of this paper are as follows . We first introduce a probabilistic model of tree alignment that enables us to acquire the cost of edit operations in a statistical framework , which is a variant of the tree edit distance . Then , we develop a polynomial time algorithm for parameter learning . We omit experimental results , which show the efficiency of the proposed algorithm using synthetic data .
2 . Probabilistic Tree Alignment
Tree Alignment In this paper , we handle rooted , ordered , and labeled trees . First , we define the notations used in this paper . A tree T consists of a set of vertices V ( T ) and a set of edges E(T ) . For a vertex v , vi denotes the i th child of v , deg(v ) denotes the number of children of v , and S[v ] denotes the subtree rooted at v . For a set fu , v,¢¢¢ , wg of vertices in the tree T , T [ u , v,¢¢¢ , w ] denotes the forest fS[u],S[v],¢¢¢ ,S[w]g .
0 and S
An alignment between two trees is defined as follows[3 ] . An alignment A of T and S is obtained by first inserting vertices labeled with φ into T and S such that the two resultant 0 have the same structure ; ie , they are identrees T tical if the labels are ignored , and then overlaying T and S . Fig 1 depicts an alignment of trees T and S . Both trees are 0 by inserting vertices labeled φ to transformed into T make trees have identical structure , then the corresponding vertices are mapped . An alignment is represented with an alignment tree as depicted in Fig 1 .
0 and S
Assuming that every vertex except the root in T and S has a label from L , each vertex in an alignment tree A(T , S ) obtained by the above operations has a label from Λ £ Λ , where Λ · L [ fφg .
Position Index For an insert ( resp . delete ) operation , we insert a vertex labeled with φ into a tree T ( resp . S ) in tree
Figure 1 . Example of tree alignment .
0 and 2 and 6 of S alignment . We call the inserted vertex a null vertex . Vertices 0 in Fig 1 are examples . 3 , 7 and 8 of T For a vertex u in a tree S , suppose u has children u1 , u2,¢¢¢ , un and consider inserting a null vertex under u . We can insert the null vertex as a parent of children ui , ui+1,¢¢¢ , uj ( eg , vertex 2 in S 0 in Fig 1 ) or as a sibling of the children located between ui and ui+1 ( eg , ver0 in Fig 1 ) . For each vertex u , let us call the gap tex 6 in S between consecutive children a position . Positions 0 , i , and deg(u ) are those to left of u1 , between ui and ui+1 , and to the right of udeg(u ) , respectively . Then , the position of an inserted null vertex is represented by a pair hi , ji of start and end indices where 0 • i • j • deg(u ) . We denote the position as ui:j and call it a position index . As a special case , we abbreviate u0:deg(u ) to [ u ] . When i 6= j , the null vertex is inserted as a parent of the children ui+1,¢¢¢ , uj . Otherwise , when i = j , the null vertex is inserted between the node ui and ui+1 . For example , 0 are 10:2 in Fig 1 , the position indices of vertex 2 and 6 of S and 12:2 of a tree S ; whereas the position index of vertex 3 0 is 20:1 . The position index of both vertices 7 and 8 of of T 0 is 12:2 of T . T
Probabilistic Model We use a hidden Markov model ( HMM ) for generating the children of each vertex of an alignment tree . We call the HMM a vertex expansion HMM . Since a vertex in an alignment tree has a pair of labels , the HMM produces an element of Λ £ Λ as an output symbol . We can use various kinds of state transition models . However , in this paper , we use an HMM with two states : e that generates a child vertex and f for termination .
Definition 1 . The vertex expansion HMM is defined as four tuple ( Q , I , T , O ) where Q = fe , fg is a set of states , and I = fπe , πfg , T = fτee , τefg , and O = foab j a , b 2 Λg are the initial probabilities , transition probabilities , and output probabilities , respectively .
Figure 2 . HMM for vertex expansion .
Note that only the state e emits a label pair in our model , so we omit the state specification in the output probabilities . The parameters of the HMM are the initial , transition , and output probabilities , and are denoted θ .
Fig 2 ( a ) depicts the state transition model of the vertex expansion HMM . In this model , the initial and transition probabilities define the probability of the tree structure , whereas the output probabilities define the cost of edit operations of the labels . For an internal vertex of an alignment tree , the state transition sequence of the HMM is e¢¢¢ ef where the HMM produces a child vertex for each state e and terminates the transition at state f . The middle rectangle in Fig 2 depicts child production of the HMM for the root vertex in Fig 1 . For any leaf of the alignment tree , the HMM starts with state f and terminates without any transition , nor production of children . The right rectangle in Fig 2 depicts the state transition sequence for a leaf . For a vertex v in an alignment tree , let q1q2 ¢¢¢ qn be the state transition sequence of the vertex expansion HMM . Then , we call the probability of the state transition an expansion probability at vertex v , which we denote as Pr(v ; θ ) where θ is the parameter set of the HMM M .
Generally , there are multiple state transition sequences that generate a specific output string . However , any state transition sequence of the HMM of Definition 1 is in the form of edeg(v)f for a vertex v . Therefore , the expansion probability at a vertex v is given by :
{
∏
Pr(v ; θ ) =
πf πe deg(v ) i=1
τi oaibi v is a leaf otherwise where τi = τee for i < deg(v ) otherwise τi = τef . Then , the alignment probability of an alignment tree A is :
Pr(A ; θ ) =
Pr(v ; θ ) .
∏ v2V ( A )
∑
Let A(T , S ) be the set of alignment trees of trees T and S . Then the marginal probability of T and S is defined as :
Pr(T , S ; θ ) =
Pr(A ; θ ) .
∑
A2A(T,S ) A2A(T,S ) as
∑
A .
Hereafter , we abbreviate
2 r(cid:146)a(cid:146)b(cid:146)b(cid:146)a(cid:146)e(cid:146)S(cid:146)r/r/a(cid:146)/e(cid:146)b/b(cid:146)Traabbr(cid:146)a(cid:146)b(cid:146)b(cid:146)a(cid:146)e(cid:146)T(cid:146)raabbinsertalignment treeAS112a/ /a(cid:146)a/ b/b(cid:146)111222333324444435555566677788860123i:position01100011322000000/e(cid:146)/e(cid:146)),(feteeef),(eet)(ep)/(foa)’/(efoInternal VertexExpansionf)(fpLeaf Expansionef),(eet),(fetHMM),(eeta/ a/ a/ a/ )/(foa 3 . Parameter Estimation
Pr(A ; θt)C(A , q ) trees ,
EM Algorithm Given training data T D consisting of ∑ a set of similar pairs of the task is to derive the parameters θ that maximize the log likelihood hT,Si2T D log Pr(T , S ; θ ) . We omit the 2 T D from the summation index when it is clear from the context . This problem can be solved by the expectation maximization ( EM ) technique in the same way as ordinary HMMs . Let θt denote the parameters of the t th iteration and ˆπq , ˆτqr and ˆoab denote the parameters of t + 1 th iteration in the EM algorithm . Then , the following formulas are derived : Pr(A ; θt)C(A,ha , bi )
ˆoab /
1
∑ ∑ ∑ hT,Si hT,Si
∑ ∑ ∑
A
A
Pr(T , S ; θt )
1
Pr(T , S ; θt )
1
ˆπq /
ˆτqr /
A hT,Si
Pr(T , S ; θt )
Pr(A ; θt)C(A , q , r ) ( 1 ) where C(A,ha , bi ) denotes the number of vertices that are labeled with ha , bi in A . For example , in the alignment tree in Fig 1 , C(A,hb , b 0i ) is 1 . Similarly , C(A , q ) denotes the number of vertices where the state q is the initial state of the HMM transition sequence in A , and C(A , q , r ) denotes the number of vertices where the HMM moves from the state q to r in A . We omit the derivation of these formulas due to space limitations .
0i ) is 2 , whereas C(A,hφ , e
Definition of Expected Frequencies To calculate formulas ( 1 ) , we need to enumerate all possible alignment trees for each pair hT , Si in the training data . However , this enumeration is combinatorially difficult . For efficient calculation , let us introduce three expected frequencies describing how often a pair of vertices in trees T and S is mapped in the alignment , each of which corresponds to substitution , insertion , and deletion . Hereafter , we omit the parameter θt from the probability expressions . For a vertex u 2 T and v 2 S , the expected substitution frequency is defined as :
Es(u , v ) ·
Pr(A )
( 2 )
∑
A2A(u,v ) where A(u , v ) denotes the set of alignment trees in which vertices u and v are mapped . Similarly , for a position index ui:j ( resp . vi:j ) of a tree T ( resp . S ) and a vertex v 2 S ( resp . u 2 T ) , the expected insertion and deletion frequencies are defined as
Ed(ui:j , v ) · Ei(u , vi:j ) ·
∑ ∑
Pr(A )
Pr(A )
A2A(ui:j ,v )
A2A(u,vi:j )
( 3 )
The expected frequencies of cases 1 , 2 , and 3 , respectively , correspond to Es(u , v ) of eq . ( 2 ) , Ei(ui:j , v ) of ( 3 ) , and
3 where A(ui:j , v ) ( resp . A(u , vi:j ) ) denotes the set of alignment trees in which vertex v ( resp . u ) is mapped to a null vertex inserted at ui:j ( resp . vi:j ) .
Output Probabilities From the definition of expected substitution frequency ,
Pr(T , S ) = Es(rT , rS )
( 4 ) where rT ( resp . rS ) is the root vertex of T ( resp . S ) . Let us first consider substitution , ie , a 6= φ and b 6= φ . It is clear that the label pair ha , bi appears in an alignment A iff a vertex u labeled with a and a vertex v labeled with b are mapped in the alignment A . Suppose Va ‰ V ( T ) ( resp . Vb ‰ V ( S ) ) is the set of vertices labeled with a ( resp . b ) . Then , the following equation holds :
∑
A v2Vb u2Va u2Va
ˆoab /
∑
∑
∑
∑ Pr(A)C(A,ha , bi ) = ∑ v2Vb Es(rT , rS ) ∑ ∑ v2Vb Es(rT , rS ) Es(rT , rS )
∑ ∑
∑ ∑ u2Va hT,Si hT,Si vk:l ui:j hT,Si
ˆoφb /
ˆoaφ /
Es(u , v ) .
( 5 )
Es(u , v )
.
( 6 )
Ei(ui:j , v )
Ed(u , vk:l )
( 7 )
Similarly , the following formulas for insert and delete operations are derived . From eqs . ( 1 ) and ( 5 ) :
From eqs . ( 1 ) , ( 4 ) , and ( 5 ) : where the summation of ui:j ( resp . vk:l ) is taken over u 2 V ( T ) and 0 • i • j • deg(u ) ( resp . v 2 V ( S ) and 0 • k • l • deg(v) ) .
∑
Initial and Transition Probabilities Let us first introduce the expected number of vertices in an alignment tree for a pair hT , Si . The term q2fe,fg C(A , q ) is equivalent to the number of vertices in A . It is obvious that each vertex in A corresponds to one of 1 . a pair of u 2 V ( T ) and v 2 V ( S ) ( ie , substitution ) , 2 . a pair of a vertex labeled with φ and v 2 V ( S ) ( ie , insertion ) , or
3 . a pair of u 2 V ( T ) and a vertex labeled with φ ( ie , deletion ) .
Ed(u , vk:l ) of eq . ( 3 ) . Therefore , the expected number of vertices of alignment trees is :
∑
∑ N(T , S ) · ∑ ∑ u2V ( T ) +
=
A
Pr(A )
∑
{ q2fe,fg Es(u , v ) ∑ v2V ( S ) Ed(u , vk:l ) + k,l i,j
C(A , q )
} Ei(ui:j , v )
.
( 8 )
Figure 3 . Alignment for inside probability .
Next , let us consider the expected number of leaves in an alignment tree . The counter C(A , f ) is equal to the number of leaves in A . Let LT ( resp . LS ) be the set of leaves in the tree T ( resp . S ) , then a leaf in A corresponds to one of 1 . a pair of u 2 LT and v 2 LS ( ie , substitution of leaves ) ,
2 . a pair of a vertex labeled with φ and v 2 LS ( ie , insertion of leaves in S ) , or deletion of leaves in T ) .
3 . a pair of u 2 LT and a vertex labeled with φ ( ie , Hence , for each pair of trees hT , Si , the expected number of leaves in an alignment tree generated by the HMM is :
A
Pr(A)C(A , f )
∑ F ( T , S ) · deg(v)∑ ∑ ∑ Ed(u , vk:k ) deg(u)∑ ∑ ∑ ∑ ∑ u2V ( T ) v2V ( S ) u2LT v2LS k=0
=
+ i=0
Ei(ui:i , v )
Es(u , v )
+ u2LS v2LS
∑
From eqs . ( 1 ) , ( 8 ) , and ( 9 ) , we obtain the reestimation of the initial probability . ˆπe /
N(T , S ) ¡ F ( T , S )
, ˆπf /
∑ hT,Si
F ( T , S ) Es(rT , rS ) ( 10 )
Similarly , the transition probabilities are derived as : hT,Si
Es(rT , rS ) ∑ ∑ hT,Si hT,Si
ˆτee /
ˆτef /
F ( T , S ) ¡ Es(rT , rS )
Es(rT , rS )
N(T , S ) ¡ F ( T , S )
Es(rT , rS )
.
( 11 )
The reestimated output , initial , and transition probabilities are represented with expected frequencies . The remaining task is to derive an efficient algorithm for computing the expected frequencies .
4
4 . Inside Outside Algorithm
Inside and Outside Probabilities Expected frequencies are decomposed into a product of two probabilities , which we call the inside probability and the outside probability . Definition 2 ( Inside Probability ) . For a pair of trees T and S , let us consider the forests F · T [ ui+1,¢¢¢ , uj ] and G · T [ vk+1,¢¢¢ , vl ] , which correspond to the position indices ui:j of T and vk:l of S . Let us consider imaginary vertices , each of which becomes a root of two forests and 0 be such trees . Then , the constitutes a tree . Let T inside probability , denoted as IP(ui:j , vk:l ) , is defined as 0 ) . the marginal probability Pr(T
0 and S 0
, S
0 ( resp . S culating the inside probability .
0 ) for calFig 3 shows an example of a tree T For a vertex u of a tree T and positions i , j ( 0 • i • j • deg(u) ) , let us consider a position index ui:j . The position index specifies three forests composed of the children of u : E = T [ u1 , . . . , ui ] , I = T [ ui+1 , . . . , uj ] , and F = T [ uj+1 , . . . , udeg(u) ] . Note that any of these three forests can be empty , as T [ ui+1 . . . ui ] denotes an empty forest for a position i ( 0 • i • deg(u) ) .
Let us introduce the following two ways to trim the forest
† Type 1 . Replace the forest I with the root of the tree in I . This removal is valid only when I consists of a single tree . We denote the obtained tree as T1(T , ui:j ) . † Type 2 . Replace the forest I with a vertex labeled with φ . We denote the obtained tree as T2(T , ui:j ) .
Definition 3 ( Outside Probability ) . For a pair of trimmed trees Tα(T , ui:j ) and Tβ(S , vk:l ) , suppose that w ( resp . x ) is the substituted vertex of the removed forest . Let us consider a set A(w , x ) of alignments of Tα(T , ui:j ) and Tβ(S , vk:l ) , where w and x are mapped . Then the outside probability , denoted as OP α,β(ui:j , vk:l ) , is defined as the sum of alignA2A(w,x ) Pr(A ) , where α and β stand ment probabilities for the type of trimming ; ie , α , β 2 f1 , 2g . Fig 4 shows an example of trimmed trees used for alignment for outside probability . As the alignment does not al
∑
( 9 )
I from the tree T . forest FT(cid:146)forest GS(cid:146)ui+1vk+1vlvk+2ui+2ujforest FT(cid:146)forest GS(cid:146)ui+1vk+1vlvk+2ui+2uj For i , j , k , and l ( 0 • i • j • deg(u ) and 0 • k •
Base l • deg(v) ) ,
PIP(ui:j , vk:l )

1 j∏ l∏ r=i+1 r=k+1 if i = j , k = l oaφ τee IP([ur ] , vk:k ) if k = l oφb τee IP(ui:i , [ vr ] ) if i = j
Figure 4 . Outside probability .
= low substitution of vertices , both of which are labeled with φ , at least one of α or β must be 1 .
We can independently consider the alignments between † the descendants of w and x ( ie , the inside probabil ity ) , and
† the outside parts of u and v ( ie , the outside probabil ity ) in Fig 4 . For a vertex u 2 V ( T ) and v 2 V ( S ) , suppose u ( resp . v ) is a g th ( resp . h th ) child of a vertex t ( resp . s ) . Then , the expected substitution frequency is
Es(u , v ) = OP 1,1(tg¡1:g , sh¡1:h)IP([u ] , [ v ] ) .
( 12 )
Note that [ u ] is an abbreviation of u0:deg(u ) .
For a vertex v 2 V ( S ) , suppose v is the h th child of a vertex s . Then ,
Ei(ui:j , v ) = OP 2,1(ui:j , sh¡1:h)IP(ui:j , [ v] ) . ( 13 ) For a vertex u 2 V ( T ) , suppose u is the g th child of a vertex t . Then ,
Ed(u , vk:l ) = OP 1,2(tg¡1:g , vk:l)IP([u ] , vk:l ) . By eqs . ( 12 ) , ( 13 ) , and ( 14 ) , it is sufficient to calculate
( 14 ) the inside/outside probabilities for parameter estimation .
Algorithm for Inside Probability The inside probability and the outside probability require alignment probabilities between trees consisting of some of the children of a vertex . For deriving recurrent formulas , we first introduce partial inside probability where the initial probability and the terminal transitions are excluded from the inside probability . Precisely , it is defined as :
PIP(ui:j , vk:l ) · if i = j , k = l otherwise
.
( 15 ) Let us derive recurrent formulas for the partial inside probabilities .
{IP(ui:j ,vk:l )
πf
IP(ui:j ,vk:l ) πeτef /τee l∑ r=k j∑ r=i where a ( resp . b ) is the label of ur ( resp . vr ) .
Next , let us consider the inductive part of the partial inside probability . There are three cases for the rightmost vertex of the alignment tree . Case 1 : Substitution the rightmost vertices uj are substituted with vl . Then ,
For position indices ui:j and vk:l ,
PIP 1(ui:j , vk:l ) = PIP(ui:j¡1 , vk:l¡1 ) τee oab IP([uj ] , [ vl]),(16 ) where a ( resp . b ) is the label of a vertex uj ( resp . vl ) . Case 2 : Deletion rightmost vertex uj is deleted . Then ,
For position indices ui:j and vk:l , the
PIP 2(ui:j , vk:l )
=
PIP(ui:j¡1 , vk:r)τeeoaφ IP([uj ] , vr:l),(17 ) where a is a label of a vertex uj . Case 3 : Insertion rightmost vertex vl is deleted . Then ,
For position indices ui:j and vk:l , the
PIP 3(ui:j , vk:l )
=
PIP(ui:r , vk:l¡1)τeeoφb IP(ur:j , [ vl]),(18 ) where b is a label of a vertex vl .
Then the inductive step is given as :
3∑
PIP(ui:j , vk:l ) =
PIP c(ui:j , vk:l ) .
( 19 ) c=1
Algorithm for Outside Probabilities For a tree T , consider the position index [ rT ] consisting of the leftmost and rightmost positions of the children of the root of T . For each position index ui:j ( u 2 V ( T ) , 0 • i • Base j • deg(u ) ) and vk:l ( v 2 V ( S ) , 0 • k • l • deg(v) ) ,
OP 1,1([rT ] , [ rS ] ) = 1 , and OP α,β([rT ] , vk:l ) = OP α,β(ui:j , [ rS ] ) = 0 .
5 lkjixS(cid:146)vrforest Gforest HwT(cid:146)urforest Eforest FmappingalignmentpositionlkjixS(cid:146)vrforest Gforest HwT(cid:146)urforest Eforest FmappingalignmentxS(cid:146)vrforest Gforest Gforest Hforest HwT(cid:146)urforest Eforest Eforest Fforest Fmappingalignmentposition For an outside probability OP α,β(ui:j , vk:l ) , the label ha , bi of the vertex in the alignment tree that maps ui:j to { vk:l is given as :
{ a = if α = 1 if α = 2 label of uj φ if β = 1 if β = 2 . ( 20 ) In this subsection , we use oab to represent the output probability of the vertex for ui:j and vk:l . label of vl φ b =
Suppose u ( resp . v ) is the x th ( resp . y th ) child of a vertex t ( resp . s ) . There are three cases for mapping between the parents of ui:j and vk:l . Case 1 : Substitution When u and v are mapped in tree alignment :
OP α,β,1(ui:j , vk:l ) · OP 1,1(tx¡1:x , sy¡1:y ) PIP(u0:i , v0:k ) PIP(uj:deg(u ) , vl:deg(v ) ) πe τef oab .
( 21 ) Case 2 : Insertion When for some positions n , m ( 0 • n • i • j • m • deg(u) ) , a null vertex is inserted under u :
OP α,β,2(ui:j , vk:l ) · i∑
{ deg(u)∑ n=0
} OP 2,1(un:m , sy¡1:y ) PIP(un:i , v0:k ) PIP(uj:m , vl:deg(v ) ) πe τef oab m=j
.
( 22 ) Case 3 : Deletion When for some positions n , m ( 0 • n • k • l • m • deg(v) ) , a null vertex is inserted under v , then :
OP α,β,3(ui:j , vk:l ) · k∑ deg(v)∑
{
} OP 1,2(tx¡1:x , vn:m ) PIP(u0:i , vn:k ) PIP(uj:deg(u ) , vl:m ) πe τef oab m=l n=0
.
( 23 )
3∑
For each outside probability , we need O(d2 ) steps for eqs . ( 22 ) and ( 23 ) . Therefore , the total complexity is O(n2d6 ) . We can obtain expected frequencies Es(u , v ) Ei(ui:j , v ) and Ed(u , vk:l ) in O(1 ) by eqs . ( 12 ) , ( 13 ) , and ( 14 ) , using outside and inside probabilities . For each pair of trees in the training data , the calculation of outside probability is the dominant part of the parameter estimation in eqs . ( 6 ) , ( 7 ) , ( 10 ) , and ( 11 ) . For each EM step , we need O(mn2d6 ) calculations , where m is the number of pairs of trees .
The proposed algorithm stores O(n2d4 ) inside and out side probabilities at each EM step .
5 . Conclusion
In this paper , we propose a polynomial time algorithm for learning edit costs for the tree alignment algorithm . Due to its learning ability , we can construct a similarity model of tree structured data obtained from various sources , effectively and efficiently . The proposed algorithm is useful as a basic tool for tree data mining . We have conducted some experiments using synthetic data , though it is omitted due to space limitation .
Current tree alignment model uses a simple model for generating alignment trees . We plan to introduce more complex models in future . The proposed algorithm is designed for ordered trees . However , we often need to measure the similarity of unordered trees . It is another future work to extend the algorithm to unordered trees .
References
[ 1 ] M . Bilenko and R . J . Mooney . Adaptive Duplicate Detection Using Learnable String Similarity Measures . KDD03 , pp . 39–48 , 2003 .
[ 2 ] M . Garofalakis and A . Kumar . XML Stream processing using tree edit distance embeddings . ACM TODS , 30(1):279–332 , 2005 .
[ 3 ] T . Jiang , et al . Alignment of trees — an alternative to tree edit . Theoretical Computer Science , 14(3):137– 148 , 1995 .
Finally , the inductive formula for outside probability is given by :
OP α,β(ui:j , vk:l ) =
OP α,β,c(ui:j , vk:l ) .
( 24 )
[ 4 ] E . S . Ristad and P . N . Yianilos . Learning string edit distance . IEEE Trans . on PAMI , 20(5):522–532 , 1998 . c=1
Let n and d be the maximum size and degree of trees in the training data , respectively . For each pair in the training data , we need to calculate and store the inside and outside probabilities for all pairs of position indices . The number of position indices of a tree is O(nd2 ) . Therefore , we need to calculate O(n2d4 ) inside and outside probabilities .
For each inside probability , we need O(d ) steps for eqs . ( 17 ) and ( 18 ) . Therefore , the total complexity is O(n2d5 ) .
6
[ 5 ] K C Tai . The tree to tree correction problem . Jour nal of ACM , 26(3):422–433 , 1979 .
[ 6 ] R . Yang , et al .
Similarity Evaluation on Tree structured Data . SIGMOD , pp . 754–765 , 2005 .
[ 7 ] K . Zhang and D . Shasha . Simple fast algorithms for the editing distance between trees and related problems . SIAM J . Comput . , 18(6):1245–1262 , 1989 .
