PRISM : A Prime Encoding Approach for Frequent Sequence Mining
∗
† Karam Gouda , Mohammed J . Zaki ∗ Mathematics Dept . , Faculty of Science , Benha , Egypt † Department of Computer Science , RPI , Troy , NY , USA
, Mosab Hassaan
∗ karam g@hotmail.com , mosab ha@yahoo.com , zaki@csrpiedu
Abstract
Sequence mining is one of the fundamental data mining tasks . In this paper we present a novel approach called PRISM , for mining frequent sequences . PRISM utilizes a vertical approach for enumeration and support counting , based on the novel notion of prime block encoding , which in turn is based on prime factorization theory . Via an extensive evaluation on both synthetic and real datasets , we show that PRISM outperforms popular sequence mining methods like SPADE [ 10 ] , PrefixSpan [ 6 ] and SPAM [ 2 ] , by an order of magnitude or more .
1
Introduction Many real world applications , such as in bioinformatics , web mining , text mining and so on , have to deal with sequential/temporal data . Sequence mining helps to discover frequent sequential patterns across time or positions in a given data set . Mining frequent sequences is one of the basic exploratory mining tasks , and has attracted a lot of attention [ 1 , 2 , 4–6 , 9 , 10 ] . Problem Definition : The problem of mining sequential patterns can be stated as follows : Let I = {i1 , i2,··· , im} be a set of m distinct attributes , also called items . An itemset is a non empty unordered collection of items ( without loss of generality , we assume that items of an itemset are sorted in increasing order ) . A sequence is an ordered list of itemsets . An itemset i is denoted as ( i1i2 ··· ik ) , where ij is an item . An itemset with k items is called a k itemset . A sequence S is denoted as ( s1 → s2 → ··· → sq ) , where each element sj is an itemset . The number of itemsets in the sequence gives its size ( q ) , and the total number j |sj| ) . A of items in the sequence gives its length ( k = sequence of length k is also called a k sequence . For example , ( b → ac ) is a 3 sequence of size 2 . A sequence S = ( s1 → ··· → sn ) is a subsequence of ( or is contained in ) another sequence R = ( r1 → ··· → rm ) , denoted as S ⊆ R , if there exist integers i1 < i2 < ··· < in such that sj ⊆ rij for all sj . For example the sequence ( b → ac ) is a subsequence of ( ab → e → acd ) , but ( ab → e ) is not a subsequence of ( abe ) , and vice versa . Given a database D of sequences , each having a unique sequence identifier , and given some sequence S = ( s1 → ··· → sn ) , the absolute support of S in D is defined as the total number of sequences in D that contain S , given as sup(S,D ) = |{Si ∈ D|S ⊆ Si}| . The relative support of S is given as the fraction of database sequences that contain S . We use absolute and relative supports interchangeably . Given a user specified threshold called the minimum support ( denoted minsup ) , we say that a sequence is frequent if occurs more than minsup times . A frequent sequence is maximal if it is not a subsequence of any other frequent sequence . A frequent sequence is closed if it is not a subsequence of any other frequent sequence with the same support . Given a database D of sequences and minsup , the problem of mining sequential patterns is to find all frequent sequences in the database . Related Work : The problem of mining sequential patterns was introduced in [ 1 ] . Many other approaches have followed since then [ 2 , 4–10 ] . Sequence mining is essentially an enumeration problem over the sub sequence partial order looking for those sequences that are frequent . The search can be performed in a breadth first or depth first manner , starting with more general ( shorter ) sequences and extending them towards more specific ( longer ) ones . The existing methods essentially differ in the data structures used to “ index ” the database to facilitate fast enumeration . The existing methods utilize three main approaches to sequence mining : horizontal [ 1 , 5 , 7 ] , vertical [ 2 , 4 , 10 ] and projectionbased [ 6 , 9 ] . Our Contributions : In this paper we present a novel approach called PRISM ( which stands for the bold letters in : PRIme Encoding Based Sequence Mining ) for mining frequent sequences . PRISM utilizes a vertical approach for enumeration and support counting , based on the novel notion of prime block encoding , which in turn is based on prime factorization theory . Via an extensive evaluation on both synthetic and real datasets , we show that PRISM outperforms popular sequence mining methods like SPADE [ 10 ] , PrefixSpan [ 6 ] and SPAM [ 2 ] , by an order of magnitude or more .
2 Preliminary Concepts Prime Factors & Generators : An integer p is a prime integer if p > 1 and the only positive divisors of p are 1 and p . Every positive integer n is either 1 or can be expressed as a product of prime integers , and this factorization is unique except for the order of the factors [ 3 ] . Let p1 , p2,··· , pr be the distinct prime factors of n , arranged in order , so that p1 < p2 < ··· < pr . All repeated factors can be collected together and expressed using exponents , so that n = p , where each mi is a positive integer , called the multiplicity of pi , and this factor
··· pmr m1 1 p m2 2 r i=1 p mi i i p i=1 p mia ia
.rb
.ra
. and b = ization of n is called the standard form of n . For example , n = 31752 = 23 · 34 · 72 . mib Given two integers a = ib in their standard forms , the greatest common divisor of the two numbers is given as gcd(a , b ) = , where pi = pja = pkb is a factor common to both a and b , and mi = min(mja , mkb ) , with 1 ≤ j ≤ ra , 1 ≤ k ≤ rb . For example , if a = 7056 = 24 · 32 · 72 and b = 18900 = 22 · 33 · 52 · 7 , then gcd(a , b ) = 22 · 32 · 7 = 252 . For our purposes , we are particularly interested in square free integers n , defined as integers , whose prime factors pi all have multiplicity mi = 1 ( note : the name squarefree suggests that no multiplicity is 2 or more , ie , the number does not contain a square of any factor ) . Given a set G , let P ( G ) denote the set of all subsets of G . If we assume that G is ordered and indexed by the set {1 , 2,··· ,|G|} , then any subset S ∈ P ( G ) can be represented as a |G|length bit vector ( or binary vector ) , denoted SB , whose ith bit ( from left ) is 1 if the i th element of G is in S , or else the i th bit is 0 . For example , if G = {2 , 3 , 5 , 7} , and S = {2 , 5} , then SB = 1010 . Given a set S ∈ P ( G ) , we denote by ⊗S , the value obtained by applying the multiplication operator ⊗ to all members of S , ie , ⊗S = s1 · s2 · . . . · s|S| , with si ∈ S . If S = ∅ , define ⊗S = 1 . Let ⊗P ( G ) = {⊗S : S ∈ P ( G)} be the set obtained by applying the multiplication operator on all sets in P ( G ) . In this case we also say that G is a generator of ⊗P ( G ) under the multiplication operator . We say that a set G is a square free generator if each X ∈ ⊗P ( G ) is square free . In case a generator G consists of only prime integers , we call it a prime generator . Recall that a semi group is a set that is closed under an associative binary operator ⊗ . We say that a set P is a square free semigroup iff for all X , Y ∈ P , if Z = X ⊗ Y is square free , then Z ∈ P . Theorem 2.1 A set P is a square free semi group with operator ⊗ iff it has a square free prime generator G . In other words , P is a square free semi group iff P = ⊗P ( G ) .
As an example , let G = {2 , 3 , 5 , 7} be the set Then ⊗P ( G ) = of the first four prime numbers . {1 , 2 , 3 , 5 , 7 , 6 , 10 , 14 , 15 , 21 , 35 , 30 , 42 , 70 , 105 , 210} . It is easy to see that G is a square free generator of ⊗P ( G ) , which in turn is a square free semi group , since the product of any two of its elements that is square free is already in the set .
1111 ( 210 )
1110 ( 30 )
1101 ( 42 )
1011 ( 70 )
0111 ( 105 )
1100 ( 6 )
1010 ( 10 )
1001 ( 14 )
0110 ( 15 )
0101 ( 21 )
0011 ( 35 )
1000 ( 2 )
0100 ( 3 )
0010 ( 5 )
0001 ( 7 )
0000 ( 1 )
Figure 1 . Lattice over ⊗P ( G ) . Each node shows a set S ∈ P ( G ) using its bit vector SB and the value obtained by multiplying its elements ⊗S .
The set P ( G ) induces a lattice over the semi group ⊗P ( G ) as shown in Figure 1 . In this lattice , the meet operation ( ∧ ) is set intersection over elements of P ( G ) , which corresponds to the gcd of the corresponding elements of ⊗P ( G ) . The join operation ( ∨ ) is set union ( over P ( G) ) , which corresponds to the least common multiple ( lcm ) over ⊗P ( G ) . For example , 1010(10 ) ∧ 1001(14 ) = 1000(2 ) , confirming that gcd(10 , 14 ) = 2 , and 1010(10 ) ∨ 1001(14 ) = 1011(70 ) , indicating that lcm(10 , 14 ) = 70 . More formally , we have : Theorem 2.2 Let ⊗P ( G ) be a square free semi group with prime generator G , and let X , Y ∈ ⊗P ( G ) be two disthen gcd(X , Y ) = ⊗(SX ∩ SY ) , and tinct elements , lcm(X , Y ) = ⊗(SX ∪ SY ) , where X = ⊗SX and Y = ⊗SY , and SX , SY ∈ P ( G ) are the prime factors of X and Y , respectively . Define the factor cardinality , denoted ffXffG , for any X ∈ ⊗P ( G ) , as the number of prime factors from G in the factorization of X . Let X = ⊗SX , with SX ⊆ G . Then ffXffG = |SX| . For example , ff21ffG = {3 , 7} = 2 . Note that ff{1}ffG = 0 , since 1 has no prime factors in G . Corollary 2.3 Let ⊗P ( G ) be a square free semi group with prime generator G , and let X , Y ∈ ⊗P ( G ) be two distinct elements , then gcd(X , Y ) ∈ ⊗P ( G ) . Prime Block Encoding : Let T = [ 1 : N ] = {1 , 2 , . . . , N} be the set of the first N positive integers , let G be a base set of prime numbers sorted in increasing order . Without loss of generality assume that N is a multiple of |G| , i,e , N = m · |G| . Let B ∈ {0 , 1}N be a bit vector of length N . Then B can partitioned into m = N|G| consecutive blocks , where each block Bi = B [ (i − 1 ) · |G| + 1 : i · |G| ] , with 1 ≤ i ≤ m . In fact , each Bi ∈ {0 , 1}|G| , is the indicator bit vector SB representing some subset S ⊆ G . Let Bi[j ] denote the j th bit in Bi , and let G[j ] denote the j th prime in G . Define the value of Bi with respect to G as follows , ν(Bi , G ) = ⊗{G[j]Bi[j]} . For example if Bi = 1001 , and G = {2 , 3 , 5 , 7} , then ν(Bi , G ) = 21·30·50·71 = 2·7 = 14 . Note also that if Bi = 0000 then ν(Bi , G ) = 1 . Define ν(B , G ) = {ν(Bi , G ) : 1 ≤ i ≤ m} , as the prime block encoding of B with respect to the base It should be clear that each ν(Bi , G ) ∈ prime set G . ⊗P ( G ) . Note that when there is no ambiguity , we write ν(Bi , G ) as ν(Bi ) , and ν(B , G ) as ν(B ) . As an example , let T = {1 , 2 , , 12} , G = {2 , 3 , 5 , 7} , and B = 100111100100 . Then there are m = 12/4 = 3 blocks , B1 = 1001 , B2 = 1110 and B3 = 0100 . We have ν(B1 ) = ⊗SG(B1 ) = ⊗{2 , 7} = 2 · 7 = 14 , and the prime block encoding of B is given as ν(B ) = {14 , 30 , 3} . −1({14 , 30 , 3} ) = We also define the inverse operation ν −1(14)ν −1(3 ) = 100111100100 = B . Also a ν bit vector of all zeros ( of any length ) is denoted as 0 , and its corresponding value/encoding is denoted as 1 . For example , if C = 00000000 , then we also write C = 0 , and ν(C ) = {1 , 1} = 1 . Let G be the base prime set , and let A = A1A2 ··· Am , and B = B1B2 ··· Bm be any two bit vectors in {0 , 1}N , with N = m · |G| , and Ai , Bi ∈ {0 , 1}|G| . Define gcd(ν(A ) , ν(B ) ) = {gcd(ν(Ai ) , ν(Bi ) ) : 1 ≤
−1(30)ν
For example , i ≤ m} . for ν(B ) = {14 , 30 , 5} and ν(A ) = {2 , 210 , 2} , we have gcd(ν(B ) , ν(A ) ) = {gcd(14 , 2 ) , gcd(30 , 210 ) , gcd(5 , 2)} = {2 , 30 , 1} . Let A = A1A2 ··· Am be a bit vector of length N , is a |G| length bit vector . Let fA = where each Ai arg minj{A[j ] = 1} be the position of the first ‘1’ in A , across all blocks Ai . Define a masking operator ( A) as follows : fi
( A) [j ] = j ≤ fA j > fA
0 , 1 ,
In other words , ( A) is the bit vector obtained by setting A[fA ] = 0 and setting A[j ] = 1 for all j > fA . For example , if A = 001001100100 , then fA = 3 , and ( A) = 000111111111 . Likewise , we can define the masking operator for a prime block encoding as follows : For example , ( ν(A)) = ν((001001100100) ) = ν(000111111111 ) = ν(0001 ) ν(1111 ) ν(1111 ) = {7 , 210 , 210} . In other words , ( {5 , 15 , 3}) = {7 , 210 , 210} , since ν(A ) = ν(001001100100 ) = ν(0010 ) ν(0110 ) ν(0100 ) = {5 , 15 , 3} .
( ν(A)) = ν((A) ) .
3 The PRISM Algorithm
Sequence mining involves a combinatorial enumeration or search for frequent sequences over the sequence partial order . There are three key aspects of PRISM that need elucidation : i ) the search space traversal strategy , ii ) the data structures used to represent the database and intermediate candidate information , and iii ) how support counting is done for candidates . PRISM uses the prime block encoding approach to represent candidates sequences , and uses join operations over the prime blocks to determine the frequency for each candidate . Search Space : The partial order induced by the subsequence relation is typically represented as a search tree , defined recursively as follows : The root of the tree is at level zero and is labeled with the null sequence ∅ . A node labeled with sequence S at level k , ie , a k sequence , is repeatedly extended by adding one item from I to generate a child node at the next level ( k + 1 ) , ie , a ( k + 1) sequence . There are two ways to extend a sequence by an item : sequence extension and itemset extension . In a sequence extension , the item is appended to the sequential pattern as a new itemset . In an itemset extension , the item is added to the last itemset in the pattern , provided that the item is lexicographically greater than all items in the last itemset . Thus , a sequence extension always increases the size of the sequence , whereas , an itemset extension does not . For example , if we have a node S = ab → a and an item b for extending S , then ab → a → b is a sequence extension , and ab → ab is an itemset extension . Prime Block Encoding : Consider the example database in Figure 2(a ) , consisting of 5 sequences over the items I = {a , b , c} . Let G = {2 , 3 , 5 , 7} be the base squarefree prime generator set . Let ’s see how PRISM constructs the prime block encoding for a single item a . In the first step , PRISM constructs the prime encoding of the positions within each sequence . For example , since a occurs in positions 1,4 , and 6 ( assuming positions/indexes starting at 1 ) in sequence 1 , we obtain the bit encoding of a ’s occur sid 1 2 3 4 5 database sequence ab → b → b → ab → b → a ab → b → b b → ab b → b → b ab → ab → ab → a → bc
( a )
1001,0100 sid Bit encoded pos 1 2 3 4 5
1000 0100 0000
1111,0000
Prime encoded pos
{14 , 3} {2} {3} {1} {210,1}
( b )
Bit encoded sid
1110,1000
Prime encoded sid
{30,2}
Item Sequence Blocks a b c
{30,2} {210,2} {1,2}
Position Blocks
( c ) {14 , 3} , {2} , {3} , {1} , {210,1} {210 , 2} , {30} , {6} , {30} , {30,2} {1 , 1} , {1} , {1} , {1} , {1,2} ( d ) a sequence blocks position offsets
1
30
3
4
2
5 b
210
1
3
4
5
2
6 position blocks
14
3
2
3
210
210
2
30
6
30
30
2 c
1
2
1
2
( e )
Figure 2 . Example of Prime Block Encoding : ( a ) Example Database . ( b ) Position Encoding for a . ( c ) Sequence Encoding for a . ( d ) Full Prime Blocks for a , b and c . ( e ) Prime Block Encoding for a , b and c . rences : 100101 . PRISM next pads this bit vector so that it is a multiple of |G| = 4 , to obtain A = 10010100 ( note : bold bits denote padding ) . Next we compute ν(A ) = ν(1001)ν(0100 ) = {14 , 3} . The position encoding for a over all the sequences is shown in Figure 2 ( b ) .
PRISM next computes the prime encoding for the sequence ids . Since a occurs in all sequences , except for 4 , we can represent a ’s sequence occurrences as a bit vector A = 11101000 after padding . This yields the prime encoding shown in Figure 2(c ) , since ν(A ) = ν(1110)ν(1000 ) = {30 , 2} . The full prime encoding for item a consists of all the sequence and position blocks , as shown in Figure 2(d ) . A block Ai = 0000 = 0 , with ν(Ai ) = {1} = 1 , is also called an empty block . Note that the full encoding retains all the empty position blocks , for example , a does not occur in the second position block in sequence 5 , and thus its bitvector is 0000 , and the prime code is {1} . In general , since items are expected to be sparse , there may be many blocks within a sequence where an item does not appear .
To eliminate those empty blocks , PRISM retains only the non empty blocks in the prime encoding . To do this it needs to keep an index with each sequence block to indicate which non empty position blocks correspond to a given sequence block . Figure 2 ( e ) shows the actual ( compact ) prime block encoding for item a . The first sequence block is 30 , with factor cardinality ff30ffG = 3 , which means that there are 3 valid ( ie , with non empty position blocks ) sequences in this block , and for each of these , we store the offsets into the position blocks . For example , the offset of sequence 1 is 1 , with the first two position blocks corresponding to this sequence . Thus the offset for sequence 2 is 3 , with only one position block , and finally , the offset of sequence 3 is 4 . Note that the sequences which represent the sequence block 30 , can be found directly from the corresponding bit−1(30 ) = 1110 , which indicates that sequence 4 vector ν is not valid . The second sequence block for a is 2 ( corre−1(2 ) = 1000 ) , indicating that only sequence sponding to ν 5 is valid , and its position blocks begin as position 5 . The benefit of this sparse representation becomes clear when we consider the prime encoding for c . Its full encoding ( see Figure 2(d ) ) contains a lot of redundant information , which has been eliminated in the compact prime block encoding ( see Figure 2(e) ) . vi∈SS a = {14 , 3} . In the full encoding P 5
It is worth noting that the support of a sequence S can be directly determined from its sequence blocks in the prime block encoding . Let E(S ) = ( SS,PS ) denote the prime block encoding for sequence S , where SS is the set of all encoded sequence blocks , and PS is the set of all encoded position blocks for S . The support of a sequence S with prime block encoding E(S ) = ( SS,PS ) is given as ffviffG . For example , for S = a , since sup(S ) = Sa = {30 , 2} , we have sup(a ) = ff30ffG +ff2ffG = 3+1 = 4 . Given a list of full or compact position blocks PS for a sequence S , we use the notation P i S to denote those positions blocks , which come from sequence id i . For example , in P 1 a = {210 , 1} , but in the compact encoding P 5 a = {210} ( see Figure 2(d) (e) ) . Support Counting via Prime Block Joins : The frequent sequence enumeration process starts with the root of the search tree as the prefix node P = ∅ , and PRISM assumes that initially we know the prime block encodings for all single items . PRISM then recursively extends each node in the search tree , computes the support via the prime block joins , and retains new candidates ( or extensions ) only if they are frequent . The search is essentially depth first , the main difference being that for any node S , all of its extensions are evaluated before the depth first recursive call . When there are no new frequent extensions found , the search stops . To complete the description , we now detail the prime block join operations . We will illustrate the prime block itemset and sequence joins using the prime encodings E(a ) and E(b ) , for items a and b , respectively , as shown in Figure 2(e ) . Itemset Extensions : Let ’s first consider how to obtain the prime block encoding for the itemset extension E(ab ) , which is illustrated in Figure 3(a ) . Note that the sequence blocks Sa = {30 , 2} and Sb = {210 , 2} contain all information about the relevant sequence ids where a and b occur , respectively . To find the sequence block for itemset extension ab , we simply have to compute the gcd for the corresponding elements from the two sequence blocks , namely gcd(30 , 210 ) = 30 ( which corresponds to the bit gcd(30 , 210 ) = 30
1
1
1
0 gcd(2 , 2 ) = 2
1
0
0
0 gcd(30 , 210 ) = 30
1
1
1
0 gcd(2 , 2 ) = 2
1
0
0
0 a b gcd(a,b )
14
210
14
3
2
1
2
30
2
3
6
3
210
30
30 mask(a ) b gcd
105
210
105
210
2
2
105
30
15
35
6
1
105
210
30
15
2
2
1001
1000
0100
1110
0111
1000
0110
0110
1000 ab
30
2
3
1
2
4 a >b
6
1
3
2
4
14
2
3
30
105
2
15
15
2
( a ) Itemset Extension ( b ) Sequence Extension Figure 3 . Extensions via Prime Block Joins vector 1110 ) , and gcd(2 , 2 ) = 2 ( which corresponds to bitvector 1000 ) . We say that a sequence id i ∈ gcd(Sa,Sb ) −1(gcd(Sa,Sb) ) . if the i th bit is set in the bit vector ν −1({30 , 2},{210 , 2} ) = Since ν −1(30 , 2 ) = 11101000 , we find that sids 1 , 2 , 3 and 5 ν are the ones that contain occurrences of both a and b .
−1(gcd(Sa,Sb ) ) = ν
All that remains to be done is to determine , by looking at the position blocks , if a and b , in fact , occur simultaneously at some position in those sequences . Let ’s consider each sequence separately . Looking at sequence 1 , we find in Figure 2(e ) that its positions blocks are P 1 a = {14 , 3} b = {210 , 2} in E(b ) . To find where a in E(a ) and P 1 and b co occur in sequence 1 , all we have to do is compute the gcd of these position blocks to obtain gcb(a , b ) = {gcd(14 , 210 ) , gcd(3 , 2)} = {14 , 1} , which indicates that ab only occur at positions 1 and 4 in sequence 1 ( since −1(14 ) = 1001 ) . A quick look at Figure 2(a ) confirms ν that this is indeed correct . If we continue in like manner for the remaining sequences ( 2 , 3 and 5 ) , we obtain the results shown in Figure 3(a ) , which also shows the final prime block encoding E(ab ) . Note that there is at least one nonempty block for each of the sequences , even though for sequence 1 , the second position block is discarded in the final prime encoding . Thus sup(ab ) = ff30ffG +ff2ffG = 3+1 = 4 . Sequence Extensions : Let ’s consider how to obtain the prime block encoding for the sequence extension E(a → b ) , which is illustrated in Figure 3(b ) . The first step involves computing the gcd for the sequence blocks as before , which yields sequences 1 , 2 , 3 and 5 , as those which may potentially contain the sequence a → b . For sequence 1 , we have the positions blocks P 1 a = {14 , 3} for a and P 1 b = {210 , 2} for b . The key difference with the itemset extension is the way in which we process each sequence . Instead of computing gcd({14 , 3},{210 , 2} ) , we compute gcd(({14 , 3}) ,{210 , 2} ) = gcd({105 , 210},{210 , 2} ) = {gcd(105 , 210 ) , gcd(210 , 2)} = {105 , 2} . Note that −1({105 , 2} ) = 01111000 , which precisely indicate those ν positions in sequence 1 , where b occurs after an a . Thus , sequence joins always keep track of the positions of the last item in the sequence . Proceeding in like manner for sequences 2,3 , and 5 , we obtain the results shown in Figure 3(b ) . Note that for sequence 3 , even though it contains both items a and b , b never occurs after an a , and thus sequence 3 does not contribute to the support of a → b . This is also confirmed by computing gcd((3) , 6 ) = gcd(35 , 6 ) =
In a naive implementation ,
−1(1 ) = 0000 ) . Thus 1 , which leads to an empty block ( ν in the compact prime encoding of E(a → b ) , sequence 3 drops out . The remaining sequences 1 , 2 , and 5 , contribute at least one non empty block , which yields Sa→b = ν(11001000 ) = {6 , 2} , as shown in Figure 3(b ) , with support sup(a → b ) = ff6ffG + ff2ffG = 2 + 1 = 3 . Optimizations : Since computing the gcd is one of main operations in PRISM , we use a pre computed table called GCD to facilitate rapid gcd computations . Note that in our examples above , we used only the first four primes as the base generator set G . However , in our actual implementation , we used |G| = 8 primes as the generator set , ie , G = {2 , 3 , 5 , 7 , 11 , 13 , 17 , 19} . Thus each block size is now 8 instead of 4 . Note that with the new G , the largest element in ⊗P ( G ) is ⊗G = 9699690 . In total there are | ⊗ P ( G)| = 256 possible elements in semi group ⊗P ( G ) . the GCD lookup table can be stored as a two dimensional array with cardinality 9699690 × 9699690 , where GCD(i , j ) = gcd(i , j ) for any two integers i , j ∈ [ 1 : 9699690 ] . This is clearly grossly inefficient , since there are in fact only 256 distinct ( squarefree ) products in ⊗P ( G ) , and we thus really need a table of size 256 × 256 to store all the gcd values . We achieve this by representing each element in ⊗P ( G ) by its rank , as opposed to its value . Let S ∈ P ( G ) , and let SB its |G| length indicator bit vector , whose i th bit is ‘1’ iff the i element of G is in S . Then the rank of ⊗S is equal to the decimal value of SB ( with the left most bit being the least signifIn other words rank(⊗S ) = decimal(SB ) . icant bit ) . For example , the rank(1 ) = decimal(00000000 ) = 0 , rank(13 ) = decimal(00000100 ) = 32 , rank(35 ) = decimal(00110000 ) = 12 , and rank(9699690 ) = decimal(11111111 ) = 255 . Let S , T ∈ P ( G ) , and let SB , T B be their indicator bit vectors with respect to generator set G . Then rank(gcd(⊗S,⊗T ) ) = decimal(SB ∧ T B ) . Consider for example , gcd(35 , 6 ) = 1 . We have rank(gcd(35 , 6 ) ) = decimal(00110000 ∧ 11000000 ) = decimal(00000000 ) = 0 , which matches the computation rank(gcd(35 , 6 ) ) = rank(1 ) = 0 . Instead of using direct values , all gcd computations are performed in terms of the ranks of the corresponding elements . Thus each cell in the GCD table stores : GCD(rank(i ) , rank(j ) ) = rank(gcd(i , j) ) , where i , j ∈ ⊗P ( G ) . This brings down the storage requirements of the GCD table to just 256 × 256 = 65536 bytes , since each rank requires only one byte of memory ( since rank ∈ [ 0 : 255] ) .
Once the final sequence blocks are computed for after a join operation , we need to determine the actual support , by adding the factor cardinalities for each sequence block . To speed up this support determination , PRISM maintains a one dimensional look up array called CARD to store the factor cardinality for each element in the set ⊗P ( G ) . That is we store CARD(rank(x ) ) = ffxffG for all x ∈ ⊗P ( G ) . For example , since ff35ffG = 2 , we have CARD(rank(35 ) ) = CARD(12 ) = 2 .
Furthermore , in sequence block joins , we need to compute the masking operation for each position block . For this PRISM maintains another one dimensional array called M ASK , where M ASK(rank(x ) ) = rank((x) ) for each x ∈ ⊗P ( G ) . For example M ASK(rank(2 ) ) = rank((2) ) = rank(4849845 ) = 254 . Finally , as an optimization for fast joins , once we determine gcdXY or gcdX→Y in the prime itemset/sequence block joins , if the number of supporting sequences is less than minsup , we can stop further processing of position blocks , since the resulting extensions cannot be frequent in this case .
4 Experiments
In this section we study the performance of PRISM by varying different database parameters and by comparing it with other state of the art sequence mining algorithms like SPADE [ 10 ] , PrefixSpan [ 6 ] and SPAM [ 2 ] . The codes/executables for these methods were obtained from their authors . All experiments were performed on a laptop with 2.4GHz Intel Celeron processor , and with 512MB memory , running Linux . Synthetic and Real Datasets : We used several synthetic datasets , generated using the approach outlined in [ 1 ] . The datasets are generated using the following process . First NI maximal itemsets of average size I are generated by choosing from N items . Then NS maximal sequences of average size S are created by assigning itemsets from NI to each sequence . Next a customer ( or input sequence ) of average C transactions ( or itemsets ) is created , and sequences in NS are assigned to different customer elements , respecting the average transaction size of T . The generation stops when D input sequences have been generated . For example , the dataset C20T50S20I10N1kD100k , means that it has D=100k sequences , with C=20 average transactions , T=50 average transaction size , chosen from a pool with average sequence size S=20 and average transaction size I=10 , with N=1k different items . The default itemset and sequence pool sizes are always set to NS = 5000 and NI = 25000 , respectively .
We also compared the methods on two real datasets taken from [ 9 ] . Gazelle was part of the KDD Cup 2000 challenge dataset . It contains log data from a ( defunct ) web retailer . It has 59602 sequences , with an average length of 2.5 , length range of [ 1 , 267 ] , and 497 distinct items . The Protein dataset contains 116142 proteins sequences downloaded from the Entrez database at NCBI/NIH . The average sequence length is 482 , with length range of [ 400,600 ] , and 24 distinct items ( the different amino acids ) . Performance Comparison : Figure 4 shows the performance comparison of the four algorithms , namely , SPAM [ 2 ] , PrefixSpan [ 6 ] , SPADE [ 10 ] and PRISM , on different synthetic and real datasets , with varying minimum support . As noted earlier , for PRISM we used the first 8 primes as the base prime generator set G . Figure 4 ( a) (b ) show ( small ) datasets where all four methods can run for at least some support values . For these datasets , we find that PRISM has the best overall performance . For the support values where SPAM can run , it is generally in the second spot ( in fact , it is the fastest on C10T20S4I4N01kD10k ) However , SPAM fails to run for lower support values . PRISM outperforms SPADE by about 4 times , and PrefixSpan by over an order of magnitude .
Figure 4 ( c) (d ) show larger datasets ( with D=100k sequences ) . On these SPAM could not run on our laptop , and
C10T60S4I4N1kD10k
C35T40S20I20N1kD1k
Spam PrefixSpan Spade PRISM
10000
1000
100
Spam PrefixSpan Spade PRISM
10000
1000
100
10
) c e s ( e m T i l t a o T
10
60
55
50
45
40
35
Minimum Support ( % )
1
90
88
86
84
82
80
78
76
74
Minimum Support ( % )
( a )
( b )
C10T20S20I10N1kD100k
C20T20S20I10N0.1kD100k
PrefixSpan Spade PRISM
) c e s ( e m T i l t a o T
PrefixSpan Spade PRISM
10000
1000
100
) c e s ( e m T i l t a o T
) c e s ( e m T i l t a o T
600 550 500 450 400 350 300 250 200 150 100
5
4.5
4
3.5
3
Minimum Support ( % )
( c )
Gazelle
PrefixSpan Spade PRISM
10000
1000
100
10
) c e s ( e m T i l a t o T
10
98
97.5
96.5
96 97 Minimum Support ( % )
95.5
95
( d )
Protein
10000
Spade PRISM
) c e s ( e m T i l a t o T
1000
1 0.059
0.057
0.055
Minimum Support ( % )
100
99.99
99.98
99.97
Minimum Support ( % )
( e )
( f )
Figure 4 . Performance Comparison
C20T20S20I10N1kD?k
C20T50S4I4N0.5kD5k
PrefixSpan Spade PRISM
1000
100
) c e s ( e m T i l a t o T
10
30
20
10 Number of Sequences D ( in 1000s ; minsup = 30 % )
90 100
60
70
40
50
80
)
B M
( e g a s U y r o m e M k a e P
Spam PrefixSpan Spade PRISM
10000
1000
100
10
99
98
97
96
95
94
Minimum Support ( % )
( a ) Scalability
( b ) Memory Usage
Figure 5 . Scalability & Memory Consumption thus is not shown . PRISM again outperforms PrefixSpan and SPADE , by up to an order of magnitude . Finally , Figure 4 ( e) (f ) show the performance comparison on the real datasets , Gazelle and Protein . SPAM failed to run on both these datasets on our laptop , and PrefixSpan did not run on Protein . On Gazelle PRISM is an order of magnitude faster than PrefixSpan , but is comparable to SPADE . On Protein PRISM outperforms SPADE by an order of magnitude ( for lower support ) .
Based on these results on diverse datasets , we can observe some general trends . Across the board , our new approach , PRISM , is the fastest ( with a few exceptions ) , and runs for lower support values than competing methods . SPAM generally works only for smaller datasets due to its very high memory consumption ( see below ) ; when it runs ,
SPAM is generally the second best . SPADE and PrefixSpan do not suffer from the same problems as SPAM , but they are much slower than PRISM , or they fail to run for lower support values , when the database parameters are large . Scalability : Figure 5(a ) shows the scalability of the different methods when we vary the number of sequences 10k to 100k ( using as base values : C=20 , T=20 , S=20 , I=10 , and N=1k ) . Since SPAM failed to run on these larger datasets , we could not report on its scalability . We find that the effect of increasing the number of sequences is approximately linear . Memory Usage : Figure 5(b ) shows the memory consumption of the four methods on a sample of the datasets . The figures plot the peak memory consumption during execution ( measured using the memusage command in Linux ) . Figure 5(b ) quickly demonstrates why SPAM is not able to run on all except very small datasets . We find that its memory consumption is well beyond the physical memory available ( 512MB ) , and thus the program aborts when the operating system runs out of memory . We can also note that SPADE generally has a 3 5 times higher memory consumption than PrefixSpan and PRISM . The latter two have comparable and very low memory requirements . Conclusion : Based on the extensive experimental comparison with popular sequence mining methods , we conclude that , across the board , PRISM is one of the most efficient methods for frequent sequence mining . It outperforms existing methods by an order of magnitude or more , and has a very low memory footprint . It also has good scalability with respect to a number of database parameters . Future work will consider the tasks of mining all the closed and maximal frequent sequences , as well as the task of pushing constraints within the mining process to make the method suitable for domain specific sequence mining tasks . For example , allowing approximate matches , allowing substitution costs , and so on .
References [ 1 ] R . Agrawal and R . Srikant . Mining sequential patterns . In
11th ICDE Conf . , 1995 .
[ 2 ] J . Ayres , J . E . Gehrke , T . Yiu , and J . Flannick . Sequential pattern mining using bitmaps . In SIGKDD Conf . , 2002 .
[ 3 ] J . Gilbert and L . Gilbert . Elements of Modern Algebra . PWS
Publishing Co . , 1995 .
[ 4 ] H . Mannila , H . Toivonen , and I . Verkamo . Discovering fre quent episodes in sequences . In SIGKDD Conf . , 1995 .
[ 5 ] F . Masseglia , F . Cathala , and P . Poncelet . The PSP approach for mining sequential patterns . In European PKDD Conf . , 1998 .
[ 6 ] J . Pei , J . Han , B . Mortazavi Asl , H . Pinto , Q . Chen , U . Dayal , and M C Hsu . Prefixspan : Mining sequential patterns efficiently by prefixprojected pattern growth . In ICDE Conf . , 2001 .
[ 7 ] R . Srikant and R . Agrawal . Mining sequential patterns : Generalizations and performance improvements . In Intl . Conf . Extending Database Technology , 1996 .
[ 8 ] J . Wang and J . Han . Bide : Efficient mining of frequent closed sequences . In ICDE Conf . , 2004 .
[ 9 ] Z . Yang , Y . Wang , and M . Kitsuregawa . Effective sequential pattern mining algorithms for dense database . In Japanese Data Engineering Workshop ( DEWS ) , 2006 .
[ 10 ] M . J . Zaki . SPADE : An efficient algorithm for mining frequent sequences . Machine Learning Journal , 42(1/2):31– 60 , Jan/Feb 2001 .
