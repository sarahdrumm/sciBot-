Finding predictive runs with LAPS
Suhrid Balakrishnan
Department of Computer Science ,
David Madigan
Department of Statistics ,
Rutgers University , Piscataway , NJ 08854 USA
Rutgers University , Piscataway , NJ 08854 USA suhrid@csrutgersedu dmadigan@rutgers.edu
Abstract
We present an extension to the Lasso [ 9 ] for binary classification problems with ordered attributes . Inspired by the Fused Lasso [ 8 ] and the Group Lasso [ 10 , 4 ] models , we aim to both discover and model runs ( contiguous subgroups of the variables ) that are highly predictive . We call the extended model LAPS ( the Lasso with Attribute Partition Search ) . Such problems commonly arise in financial and medical domains , where predictors are time series variables , for example . This paper outlines the formulation of the problem , an algorithm to obtain the model coefficients and experiments showing applicability to practical problems of this type .
1 Predictive Runs
We consider regression and classification problems where the predictor variables are ordered and naturally form groups . For example , in predicting whether or not a vaccinated animal survives an anthrax challenge , relevant attributes might include a toxin neutralization assay ( TNA ) measured at ten different time points ( ie , a group of ten predictor variables ) , a protective antigen assay , measured at 20 time points ( ie , a group of twenty predictor variables ) , and vaccine dilution ( ie , a group containing a single predictor variable ) . We believe that many regression and classification applications exhibit such structure and we describe several in what follows .
Standard modelling approaches that ignore the group structure or ordering can lead to models that provide good predictive performance but make little sense . For example , applying feature selection to the problem above might result in selecting TNA predictor variables corresponding to measurements at weeks 2 , 8 , and 48 and dropping the measurements at weeks , 4 , 6 , 12 , 20 , 32 , 40 , and 52 . Similarly , feature selection might result in selecting values of other assays at seemingly arbitrary timepoints . Since the assay measurements are generally serially correlated , small per
Figure 1 . Typical classification problem setup . Plotted are 4 examples , two each drawn from the two different classes ( shown in red dotted and blue solid lines ) . Also shown in the figure are the 3 different groups ( the differently shaded and delineated bands x.1 , x.2 , x.3 ) and potential locations for predictive runs . turbations to the training data often lead to the selection of a drastically different set of predictor variables . We argue that in many applications , it makes more sense to select ( or omit ) contiguous “ runs ” of predictor variables for example , TNA measurements from week 2 through week 8 . ig , x(2 ) ig
In this paper we focus on binary classification problems . For each example , given an input vector xi = [ xi1 , . . . , xid ] , we seek to predict the corresponding label yi ∈ {−1 , 1} . Each xij corresponds to a “ group , ” xig = [ x(1 ) ] , Tg ≥ 1(the group length ) . Figure 1 shows a problem with three equal size groups , x.1 , x.2 and x.3 , where the second half of the first group ( variable indices ∼ 30—60 , denoted by I∗ ) has some discriminatory power with respect to the two classes ( indicated by bluesolid and red dotted lines ) , the second group has no dis ig , . . . , x(Tg ) criminatory power , and the entire third group ( indices I∗∗ ) , is useful , with the possible exception of the first few values . We restrict our attention to linear logistic regression models for interpretability , and we seek to develop a modelling approach that automatically identifies the sub group indices , or runs , I∗ and I∗∗ and the corresponding regression parameters .
2 Modelling Predictive Runs
Our work builds on two recent extensions to the original “ Lasso ” L1 regularized regression models of Tibshirani [ 9 ] 1 . The “ fused Lasso ” [ 8 ] addresses problems like ours where the variables are ordered ( their development and experiments are described for inputs with one group , ie , x = x1 ) The fused Lasso encourages contiguous subgroups of identical coefficients ( the corresponding variables being highly correlated ) to be non zero together . They accomplish this through an additional L1 penalty ( besides the regular Lasso regularization term ) on the differences of successive coefficient values ( βk − βk+1 terms ) . In a sense , what we propose below is a “ soft ” version of the fused lasso . Our work is more closely related to another Lasso extension , the “ group Lasso ” [ 10 , 4 ] . Here the emphasis is on adapting the Lasso sparsity to sets of predictors . In particular , the group lasso either selects or omits entire groups of variables , where the data analyst pre specifies what attributes form the groups . An elegant result of this formulation is that it reduces to the Lasso when all the variable sets are of size one .
Here , we propose a data driven approach to identify runs ( or contiguous subgroups ) of model coefficients , that are similar ( like a soft fused Lasso ) and that will be selected together ( have non zero model coefficients en block , like the group Lasso ) . The challenge is that we neither know the within group run structure of the attributes , nor the amount of similarity within runs beforehand . The following sections outline our approach to these problems , which essentially consists of modifying the group Lasso penalty to potentially include similarity between coefficients and searching over group partitions into runs . We call this approach LAPS ( the Lasso with Attribute Partition Search ) .
2.1 The LAPS model
Given hyper parameters λ ( a regularization parameter ) and k ( a parameter governing serial correlation of run coefficients ) , LAPS finds logistic regression coefficients β and
1The Lasso uses L1 regularization to achieve simultaneous sparsity and complexity control . Genkin et al . [ 2 ] and others report excellent predictive performance in high dimensional applications within this modelling framework . the run structure I such that : argmin β,I nll(β ) + λ
J
Xj=1
,
( 1 )
βIjflflflKj sjflflfl lengths of the d groups = Pd where nll(β ) = −Pt i=1 log Φ(−yiβT xi ) , is the negative log likelihood involving the logistic link function Φ(z ) = ez i=1 comprises t 1+ez . The training data D = {(xi , yi)}t labeled examples where the input examples , as before , are xi = [ xi1 , . . . , xid ] and can be thought of compactly as a single p dimensional vector ( as presented in the introduction . Thus p = the total number of attributes = sum of the g=1 Tg ) . The run structure ( or partition structure ) I comprises the set of run indices Ij , I = {I1 , I2 , . . . , IJ } . These run indices , in turn , form a disjoint partition that covers the entire attribute index range ( all groups ) , thus ∪J j=1Ij = 1 . . . p , and Iu ∩ Iv = 0 , ∀u 6= v . Additionally we impose the requirement that run indices respect all group boundaries ( that is , runs never cross the boundaries between the various x.j for different js ) . The Kj matrices are positive definite matrices parameterized by a single scalar k ( subsection 2.2 has details ) . The regularization term involves the Kj matrix norms2 of the run coefficients ( for a vector z and a matrix A , kzkA = ( zT Az)05 ) Finally , the sj = p0.5(|Ij| + 1 ) , are scalars factors that
“ normalize ” the prior β variance ( more about this also in subsection 22 )
Figure 2 shows the resulting LAPS model applied to a small simulated example with two groups , where one entire group is discriminative ( the shaded/right group of indices ) . Whereas the Lasso results are unsatisfactory , as it finds only two non zero coefficients among the discriminative group of correlated variables3 , LAPS does exactly what we’d like , finding runs in both regions , and giving ( lower , but ) almost equal predictive weight to the whole group ( which is found to be a single run ) .
2.2 K— “ Soft ” Fusion
LAPS models use non identity K matrices which provide them a very flexible set of modelling choices—all the way from strongly dependent run coefficients ( via strong correlation structure of K ) to models where the entire set of coefficients in the run is exchangeable ( K = I ) .
We parametrize these Kj matrices by a single scalar k , based on the following assumptions . First , we assume that
2This matrix norm penalty is the Mahalanobis distance of the run coefficients to the ( appropriate size ) zero vector . In a Bayesian interpretation , zero is the location of the prior mode and the prior covariance matrix involves , K−1 . Details can be found in Appendix A . Such matrix norms were suggested by Yuan and Lin [ 10 ] ( and in their references ) , but all their subsequent modelling and experiments used only the identity matrix . j
3Since the Lasso models all coefficients as exchangeable and strongly favors parsimony , this behavior is expected .
Figure 2 . Simple example showing proof of concept . In the top portion we plot the data set used ( created by collating different 10dimensional highly correlated Gaussians as shown . The correlation boundaries define groups , which are shaded . 20 samples total , 10 from each class ) . The two classes are shown in different colors . The bottom portion of the plot shows the Lasso model coefficients ( found using BBR [ 2 ] with hyperparameter selected by 10 fold CV ) and the estimated LAPS model coefficients and inferred run structure , I ( denoted by the bands below the coefficients ) . The thin shaded region on the left is for the intercept term , and its coefficient is cropped out of the y axis of the bottom portion . a priori all the components of β have equal variance , regardless of the size of the run they will be in ( a Bayesian interpretation is involved , see Appendix A ) . Second , since we seek runs on ordered variables , we try to impose the requirement that consecutive model coefficients in the same run should be similar . We accomplish this via tri diagonal Kj ’s . The corresponding K −1 is a symmetric positive definite matrix with ones on the diagonal ( this ensures the equal variance of components ) and terms in decreasing geometric progression ( multiplicative factor k ) proportional to the distance from the diagonal ( a Green ’s matrix ) . j
See Figure 3 for a graphical view of how the Bayesian prior varies with respect to the fusion parameter k on a twovariable size run . The k value rather intuitively controls how much soft fusion we enforce ( for k=0 , we obtain the group Lasso ) . For a slightly bigger example , consider the
Figure 3 . The effect of k on the prior for 2D—illustrating soft fusion . Notice how as k increases , prior mass shifts favoring both parameters to be more like each other . matrices obtained for a run of size 4 , with k=05 Here :
K −1 =
1 0.5 0.25 0.125
 
0.5 1 0.5 0.25
0.25 0.5 1 0.5
0.125 0.25 0.5 1
 
, and
K =
 
1.333 −0.667 −0.667
0
0 0
1.667 −0.667 −0.667
1.667 −0.667 −0.667 1.333
0
0 0
.
 
3 Learning LAPS models
We describe the algorithm for fitting full LAPS models ( with parameters β , I , k , and λ ) in stages . First , consider the situation where values for k , λ and the run structure I are known . In this case , given the labelled dataset D , as well as λ , I and k , we want to find β such that : argmin
β gλ,I,k(β ) = nll(β ) + λ
J
Xj=1 sjkβIj k
.
Kj
( 2 )
We will refer to this as the core LAPS optimization problem . It is a convex optimization problem and we use a standard block coordinate descent algorithm to solve it ( see Algorithm 1 , [ 4 ] . We use simple off the shelf line search and Newton solvers)4 . Convexity is crucial , and the algorithm results from repeated application of the optimality criteria
4Although this is probably not the most efficient algorithm for this problem , in our experiments it proved to be quite reasonable .
Algorithm 1 Core LAPS optimization problem
Input : Training data D , initial β . Result : β that satisfies Equation 2 . repeat
β0 ← argminβ0 gλ,I,k(β ) ( Line search for intercept ) . for j = 1 to J do
≤ λsj then j else
βIj ← 0 if flfl∇nll(β)IjflflK−1 βIj ← argminβIj method , say ) . gλ,I,k(βIj ) ( by Newton ’s end if end for until Some convergence criteria is met .
( Appendix B provides a sketch of the derivation ) . Note that throughout we do not penalize the intercept term β0 . Next , consider the search for I ( with λ and k still fixed ) .
3.1
I ∗—Run Structure Search
Motivated by the work of Consonni and Veronese [ 1 ] , we use a heuristic greedy procedure for the run structure search . The search starts at an initial I derived using the core LAPS optimization problem ( see Appendix D for details ) . The search then proceeds by locally perturbing an existing partition structure to generate a candidate run structure ( see Figure 4 ) . We then obtain the optimal β∗ corresponding to this candidate . We use the optimal objective function value , gλ,I,k(β∗ ) ( Equation 2 ) to score the candidates . The search stops when all changes to the existing run structure result in worse scoring models . Note that our greedy search just involves repeated solutions of the ( efficient ) core LAPS optimization problem . Further , since the amount of regularization is fixed at this stage ( λ , k constant ) , the optimal objective function value , g , makes a sensible scoring criterion for the models .
3.2 Selecting the Hyperparameters
Finally , we propose to search over a discrete grid for the remaining hyperparameters , λ and k . For each ( λ,k ) pair , we greedily search over the run structures for locally best I and β pairs ( as outlined in subsection 31 )
Having found β∗ and I ∗ for every ( λ,k ) pair , we now score these locally “ optimal ” models . We cannot use the same g function value for scoring across the different grid points as they have different amounts of regularization . A cross validation accuracy based score for instance , makes sense , but may prove computationally infeasible for even moderate size problems ( this would require repeated solutions of the I ∗ search problem for each fold ) . We instead
Illustrating the I ∗ search with a Figure 4 . graphical representation of the model coefficients . Edges ( parts of a run ) can only occur between adjacent nodes ( coefficients ) within the same group ( shaded regions , as before ) . The run structure I , is the set of all connected components defining the runs , Ijs . Our search strategy works by sequentially examining all potential or existing edges . Two accepted perturbations and the corresponding run partitions are shown for the middle group ( from the top to bottom rows ) . use an approximate marginal data likelihood5 based score ( Appendix C ) .
S(β∗ , I ∗ , λ , k ) = nll(β∗ ) + 0.5 log |Ψ(β∗ , I ∗)|
+λ
J
Xj=1 sjkβ∗ I∗ j k
−
Kj
J ∗
Xj=1
−0.5(nJ ∗ + 1 ) log(2π ) , log c(|I ∗
|Σj|0.5 j | )
Kj j=1 sj(kβIj k−1 where Ψ(β , I ) = X T AX + λPJ ∗ Kj − kβIj k−3 B ) , with A being the t × t diagonal matrix formed Kj by the aii = Φ(β∗T xi)(1 − Φ(β∗T xi ) ) terms , B = bbT ( an outer product of b = Kj β∗ vectors . The summation I∗ j till J ∗ is only over the non zero β∗ run indices ( and nJ ∗ is the number of non zero β∗ indices ) . Appendix A has expressions for c , Σj .
We compare the model scores S(β∗ , I ∗ , λ , k ) over the entire ( λ,k ) grid and pick the best model amongst those ( as defined , smaller S is better . This then results in values for
5Recall that the marginal data likelihood or evidence is proportional to the probability of the hypothesis ( the particular hyperparameter settings in our case ) given the data , and is a standard Bayesian model selection criterion .
λ∗ and k∗ ) . In our experiments we have found this procedure quite robust to variations in the dataset and parameter settings .
4 Experiments
We next apply LAPS models to real and simulated data . We are interested in evaluating both structural ( run partitioning ) performance and predictive accuracy . Predictive performance comparisons are made to Lasso logistic regression ( using BBR6 , [ 2 ] , publicly available software ) .
4.1 SIM Data
In our first set of experiments we simulate datasets from three different models , with regression coefficients designed to favor one of regular Lasso , the group Lasso , and LAPS . The data x ∼ N ( 0 , 1)15 , are simulated to be uncorrelated 15 dimensional Gaussian with mean zero and unit variance . The βtrue are shown in Table 1 . A large test dataset ( 104 examples ) was also simulated for each set of regression coefficients . As can be seen , each set of coefficients favors one of the three models—the first set has no intentionally long runs ( thus favors the Lasso model , SIM1 ) , the second set has runs , but with no internal similarity ( thus favors the group Lasso , SIM2 ) , and the third has runs with extremely high similarity ( SIM3 ) . In all three cases , the coefficients are scaled such that the Bayes risk , truexi)} , on the large corresponding test dataset is 02 In order to assess sample size effects , we also simulate training datasets of two sizes , small ( 50 examples , denoted in the results by SM ) and large ( LG , 500 examples ) . There is only one group here , which corresponds to the entire set of predictors , [ 1—16 ] ( Index 1 is for the intercept ) . truexi ) , 1 − Φ(βT i=1 min{Φ(βT r = Pt
The results show that LAPS does a fairly reasonable job of finding non zero coefficient runs7 even with the small datasets , and performs much better on the large datasets ( see the I bands in the plots in Figure 5 ) . The inferred k∗ parameter also provides insight , being 0 and 0.99 for SIM2 LG and SIM3 LG , indicating run structure that is not at all and highly similar respectively . Also , predictively , LAPS performs competitively with the Lasso ( see Table 2 ) , having small gains and losses in the expected cases .
6http://wwwstatrutgersedu/∼madigan/BBR 7We point out here that runs consisting only of zero coefficients aren’t necessarily grouped correctly because the the xs are uncorrelated and the score we use g , is insensitive to zero coefficients being grouped in a run together ( MAP zero coefficients in any number of runs result in the same g value ) .
Table 1 . SIM data regression coefficients ( βtrue , in columns ) . The first index corresponds is for the intercept term . The desired runs are shown as blocks in the columns ( same as the blue bands at the bottom of the relevant plots in Figure 5 ) .
β Index
SIM1
SIM2
SIM3
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
0
1.1500
0
0.5750 0.2875
0 0
0.2875 0.5750
0
1.1500
0
1.1500
0 0
0.8625
0 0
0 0
1.1609 0.5804 0.8706 0.5804
0.9540 0.9540 0.9540 0.9540
0 0 0
0 0 0
0.5804 0.2902 1.1609
0.4770 0.4770 0.4770
0 0
0 0
0.8706 0.2902
0.7155 0.7155
4.2 BF Data
The cylinder , bell , funnel dataset proposed by Saito [ 6 ] is a three class problem , with one input group variable per example . The equations describing the x for each class have both random noise as well as random start and end points for the generating events of each class , making for quite a lot of variability in the instances . We focus on just two classes , bell vs . funnel . The class labels roughly describe the shape of the examples—bells ramp up and then drop at some point sharply , and funnels spike sharply and then ramp down . As in past studies , we simulated 266 instances of each class to construct the dataset . The top two rows of Figure 6 show the data and a particular instance of each class in bold ( bells in red and funnels in blue ) . Once again , there is only one big group here , consisting the entire set of variable indices ( 128 predictors ) .
Both LAPS and Lasso make just one error on one fold in 10 fold cross validation predictive accuracy experiments ( Table 2 ) . Indeed the best LAPS model is very similar to the best Lasso model , with k∗=0 , see Figure 6 . However , some salient properties of the dataset become apparent when examining successive LAPS models with increasing run cohesiveness ( ie , fixing at λ∗ , increasing k ) . As can be seen in the figure , the k = 0.99 model seems to imply three specific
Figure 5 . Results on the SIM datasets . The SIM plots show the true , Lasso and best LAPS βs and LAPS I ( the bands at the bottom of each plot ) . The intended/true run structure for the SIM data is also shown in blue bands . runs which are discriminative—the first run is not so strong , occurs early ( indices around 15—30 ) and primarily focuses on “ early ” rising bells from the funnels . The next run is the most significant , occurs right afterwards ( from about 35— 40 ) and is the region of the data where the two classes are most segregated8 . Finally , a short positive run around 50 works in combination with the previous two runs—by this index location , a bell should be on the ascendency compared to a funnel .
4.3 NHP study
Our final application concerns a vaccine efficacy study . 136 non human primates ( NHPs ) were vaccinated , monitored for a year and then “ challenged ” with anthrax . Of the 136 NHPs , 93 survived the challenge and 43 died . Re
8While the region around 80—100 also appears to be similarly segregated , a careful look reveals it is actually much more mixed , because some examples for both classes have “ fallen ” trajectories by this index location . peated measurements during the year ( and some up to a year after ) assessed over a dozen aspects of the putative immune response . These measurements include an include an immunoglobulin G enzyme linked immunosorbent assay ( IgG ) , various interleukin measures ( IL2 , IL4 , IL6 ) , and a so called “ stimulation index ” ( SI ) , to name a few , with the number of measurements varying somewhat from animal to animal . The goal of the study is to understand the predictive value of the various assays with respect to survival . The assays thus define the groups , and we search for runs in their time series measurements .
The best LAPS model found with k∗ = 0 , looks a lot like the best Lasso model found . Again , it is instructive to look at the LAPS models obtained by varying k ( Figure 7 , holding λ∗ fixed ) . The models are biologically reasonable— high amounts of antibodies that neutralize the toxin predict survival ( IgG , ED50/TNA ) . High amounts of interleukins ( immune system response/signaling molecules ) , particularly as time increases , are predictive of death ( il4eli , il2m ,
Figure 6 . Results on the bell funnel data . The plots in the top most row show all bell and funnel instances in yellow along with one instance each highlighted . The plot in the second row also shows the whole dataset ( and one highlighted bell and funnel instance ) but with no lines connecting the data points for clarity . The bottom portion plots the Lasso and best LAPS β and corresponding Is . For the other LAPS models in this plot , λ is held fixed and k is varied . il4m etc ) Finally , the model also allows one to find runs that are likely not predictive—see for example the first half of the il6m assay , which is identified as a single run across different the k values , and consistently set to zero .
5 Discussion
In this paper we present a model based on the Lasso for finding predictive runs in particular types of structured classification problems . We provide the details of the model , an algorithm for inferring its parameters , and results of application on different types of data . Many extensions of the current work are possible , of which we mention a few . Viewed in Bayesian framework , in this work we have assumed a flat prior over partition space—it would be interesting to see the effect of various priors on run partition
Table 2 . Predictive performance—estimated error rates10
Data
SM1 SM2 SM3 LG1 LG2 LG3 BF NHP
Lasso
LAPS
% Err 25.43 30.83 35.98 22.31 21.14 21.86
0.1887 ± 0.6 30.81 ± 11.97
V ∗ 0.45 0.15 0.15 0.15 0.5 0.35 200 0.2
% Err 27.52 34.38 30.62 22.09 21.09 21.68
0.1887 ± 0.6 28.02 ± 10.27
V ∗ 0.28 0.54 0.37 0.54 0.63 0.19 0.45 0.46 k∗ 0
0.99 0.99 0.74
0
0.99
0 0
Figure 7 . Lasso vs . LAPS on the NHP dataset . space . In particular , a non parametric prior like the Chinese restaurant process prior ( adapted for the ordering of variables ) , might provide for an alternative way for the data to “ decide ” on the number and type of runs . One could also look at alternatives to the MAP style Bayesian analysis carried out here—numerical simulation may be used to generate a posterior distribution on LAPS model outputs . Finally , extending this model to larger problems ( in the number of attributes , the number of training examples and also dimension of structured data—2 D images , for example ) raises interesting computational and methodological issues .
A : The LAPS prior
Viewed as a Bayesian prior , the LAPS regularization term corresponds to a product of multivariate power expo
10The error estimates are obtained from the test set for the SIM data ( SM/LG 1 . . .3 ) and by 10 fold CV on BF and NHP( at k∗ , V ∗ with standard error shown ) . V = 2/λ2 , is an equivalent paramterization of λ . V ∗ ( and k∗ , for LAPS ) are found through grid search . The grid for k is consistently set to five values uniform over the range [ 0—0.99 ] ( including both end points ) . The search ranges for V for Lasso and LAPS are [ 0.01—1 ] and [ 01—09 ] respectively for the SIM datasets . For the BF dataset , the ranges were [ 0.01—103 ] and [ 0.01—1 ] , and [ 0.05—1 ] , [ 01—09 ] for the NHP data ( Lasso and then LAPS respectively ) . The Lasso V grid was always chosen at least thrice as fine as the uniform 10 grid points from the interval for LAPS . nential ( MVPE ) distributions with power 0.5 [ 3 ] . The particular MVPE distribution we use has the following density function ( for a particular run and with mean zero ) : f ( βIj |µ = z , Σj ) = c(|Ij| ) |Σj|1/2 exp −
1
2 hβT
Ij Σ−1 j βIji0.5
,
|Ij |Γ(|Ij |/2 ) where the normalizing constant contains c(|Ij| ) = π|Ij |/2Γ(1+|Ij |)21+|Ij | . The covariance matrix of this distribution is given by : cov(βIj ) = 4Γ(nj+2 ) njΓ(nj ) Σj = 4(n + 1)Σj , ∀|Ij| ∈ Z ( |Ij| denotes the size/cardinality of Ij ) . For LAPS , we set Σ−1 j = 2(nj + 1)λ2Kj . This results in cov(βIj ) = 2K −1 j /λ2 . Since the Kj matrices have unit diagonals , the marginal prior variance of every parameter is identical ( and equal to 2/λ2 , which is exactly the Lasso model prior variance ) . This Σj setting then results in sj = p0.5(|Ij| + 1 ) . Also , the Kj being tri diagonal results in approximate structural conditional independence11 . This can be seen by examining an approximating Gaussian graphical model , which would be found by matching the first and second moments ( exactly as above ) . The structural zeros in the
11True conditional independence is not possible for any non diagonal inverse covariance due to the 0.5 power in the exponent . approximating Gaussian ’s inverse covariance result in runs being a linear chain graphical models .
B : Core LAPS problem optimality criteria
There are two distinct optimality cases to check run wise for a purported solution β∗ of Equation 2 ( I is given ) : One , = 0 , and two , if all the elements if the run is set to zero β∗ Ij 6= 0 for the run , in the run are non zero β∗ Ij the optimality conditions are derived by simply setting the gradient of the objective function to zero .
6= 0 . If β∗ Ij
If β∗ Ij
= 0 , we need convex non smooth analysis results [ 5 ] because the regularization term is non differentiable at zero . We will use both the notions of the subgradient ( a tangent plane supporting a convex function , f . Precisely , the subgradient ξ ∈ R|z| , of f at z0 is defined to be any vector satisfying : f ( z ) ≥ f ( z0 ) + ξT ( z − z0 ) ) and the subdifferential ( ∂f which is the set of all subgradients , ξ , at a point . This collapses to the ordinary derivative when the function is differentiable ) We will also use the following theorem : ˆβ is a global minimizer of a convex function f ( β ) if and only if 0 ∈ ∂f ( ˆβ ) .
Now for β∗
R|Ij | satisfies flflβIjflflKj
Ij = 0 , use the theorem above . For the matrix norm part of the objective function , the subgradient ξ ∈ ≥ ξT βIj ( by the definition ) . Now , consider the ( unique ) Cholesky decomposition of the posij Rj ( also define αj = RjβIj ) . tive definite matrix Kj = RT Rewriting the previous condition , we can express the subdifferential as the set ξj : ( βT j αj)0.5 = Ij j βIj = ξT R−1 j αj . This inequality in turn , kαjk2 ≥ ξT ≤ 1 , which can also be j = j )−1 ) . The theorem then requires ≤ 1} be satisfied . ≤ λsj ∀βIj = 0 . seen to be equivalent to kξjkK−1 ( RT that 0 ∈ ∇nll(β)Ij + λsj{ξj : kξjkK−1 holds whenever flfl(R−1 j )T ξjflfl2 j Rj)−1 = R−1
≤ 1 ( because K −1
KjβIj )0.5 = ( αT
This finally yields : flfl∇nll(β)IjflflK−1
Thus the optimality criteria result : j j j ( RT j
∇nll(β)Ij + λsj
Kj βIj kβIj k
Kj
= 0 ∀βIj 6= 0 , flfl∇nll(β)IjflflK−1 j
≤ λsj ∀βIj = 0 .
( 3 )
C : The Approximate Marginal Data Likelihood Score by only considering the non zero coefficients/variables— we denote these by the summation till J ∗ , instead of J for all the attributes ) . This rather drastic appearing assumption and a straightforward second order Taylor expansion of the negative log posterior results in the score we use . We note here that the assumption is really not as bad as it appears on first glance . Indeed , the posterior is clearly less curved along zero coefficient axes—this can be seen from the optimality conditions , Equation 3 , by looking at the magnitude of the subdifferential in both cases . Further , simulation studies also show this approximation to be quite reasonable in practice .
D : Heuristic for the initial I
Given k and λ the procedure ( for a single group ) is : 1 . For each attribute , fit a group Lasso model to the outputs with the current neighbors as the only predictors ( a list initially containing just the attribute itself ) . If the minimum gλ,I,k value ( restricted to only the attributes in the list ) of an expanded neighborhood is better than that for the old neighborhood , update the neighborhood . Otherwise if all expansions result in poorer g value , stop and record the final neighborhood for that variable . 2 . Once the neighborhoods for all the variables ( in the group ) have been obtained , the initial run partition is given by the set of maximal cliques ( the largest subgroups where each variable in the clique votes to have the other variable in it ’s neighborhood , and vice versa ) . In our experiments , this heuristic performs quite well at very reasonable computational cost .
Acknowledgements
We thank the US National Science Foundation for financial support through grants DMS 0505599 and EIA 0087022 .
References
[ 1 ] G . Consonni and P . Veronese . A bayesian method for combining results from several binomial experiments . Journal of the American Statistical Association , 90 : 935 – 944 , 1995 .
[ 2 ] A . Genkin , D . D . Lewis , gan . sion for http://wwwstatrutgersedu/ madigan/PAPERS/ .
Large scale bayesian logisitic text categorization . , and D . MadiregresURL
2004 .
The S score we use , is based on a Laplace approximation to the posterior distribution . As anticipated , the nondifferentiability of the regularization term at zero complicates evaluating it . We use an approximation suggested in Shimamura et al . [ 7 ] , which essentially ignores the contribution to the curvature of the posterior by the β∗ = 0 comI∗ j ponents ( in other words , performs a Laplace approximation
[ 3 ] J . K . Lindsey . Multivariate elliptically contoured distributions for repeated measurements . Biometrics , 55 ( 4):1277 – 1280 , 1999 .
[ 4 ] L . Meier , van de Geer , S , B¨uhlmann , and P . The group lasso for logistic regression . Technical report , ETH , Zurich , Switzerland , 2006 .
[ 5 ] R . T . Rockafellar . Convex Analysis . Princeton Univer sity Press , Princeton , N.J , 1970 .
[ 6 ] N . Saito . Local feature extraction and its application using a library of bases . PhD thesis , Yale University , 1994 .
[ 7 ] T . Shimamura , H . Minami , and M . Mizuta . ReguIn larization parameter selection in the group lasso . COMPSTAT , Capri , Italy , 2006 .
[ 8 ] R . Tibshirani , M . Saunders , S . Rosset , J . Zhu , and K . Knight . Sparsity and smoothness via the fused lasso . Journal of the Royal Statistical Society , 67(1 ) : 91 – 108 , 2005 .
[ 9 ] R . J . Tibshirani . Regression shrinkage and selection via the lasso . Journal of the Royal Statistical Society , Series B , 58(1):267–288 , 1996 .
[ 10 ] M . Yuan and Y . Lin . Model selection and estimation in regression with grouped variables . Journal of the Royal Statistical Society , 68(Series B):49 – 67 , 2006 .
