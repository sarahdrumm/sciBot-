Learning to Trade Off Between Exploration and
Exploitation in Multiclass Bandit Prediction
Hamed Valizadegan Department of Computer
Science
University of Pittsburgh
Pittsburgh , USA hamed@cspittedu
Rong Jin
Department of Computer Science and Engineering Michigan State University East Lansing , PA , USA rongjin@csmsuedu
Shijun Wang
Radiology and Imaging
Sciences
National Institute of Health
USA wangshi@ccnihgov
ABSTRACT We study multi class bandit prediction , an online learning problem where the learner only receives a partial feedback in each trial indicating whether the predicted class label is correct . The exploration vs . exploitation tradeoff strategy is a well known technique for online learning with incomplete feedback ( ie , bandit setup ) . Banditron [ 8 ] , a multiclass online learning algorithm for bandit setting , maximizes the run time gain by balancing between exploration and exploitation with a fixed tradeoff parameter . The performance of Banditron can be quite sensitive to the choice of the tradeoff parameter and therefore effective algorithms to automatically tune this parameter is desirable . In this paper , we propose three learning strategies to automatically adjust the tradeoff parameter for Banditron . Our extensive empirical study with multiple real world data sets verifies the efficacy of the proposed approach in learning the exploration vs . exploitation tradeoff parameter .
Categories and Subject Descriptors I.2 [ Artificial Intelligence ] : Learning
General Terms Theory , Algorithms
Keywords Online Learning , Bandit Feedback , Multi class Classification , Exploration vs . Exploitation
1 .
INTRODUCTION
The exploitation vs . exploration tradeoff strategy has been widely used to develop online learning techniques when the feedback provided to the learner is bandit , the learner only receives the cost of its action but not the cost of other possible actions . Exploration refers to taking an ie action that is not recommended by the current model ( classifier ) . It allows the learner to explore the environment ( opponent ) , receive the feedback for different actions , and consequentially gain new knowledge from the environment . Exploitation refers to taking the best action according to the current statistical model in order to maximize the gain . These two objectives are complementary , but opposite : exploration leads to the maximization of the gain in the long run at the risk of losing short term reward , while exploitation maximizes the short term gain at the price of losing the gain over the long run . A careful tradeoff between these two objectives is important to the success of any online learner utilizing the combined strategy [ 17 ] .
The exploitation vs . exploration tradeoff strategy was originally designed for the multi armed bandit problem [ 15 ] where a slot machine has multiple arms and the player needs to choose the arm with minimum cost ( or maximum gain ) . In each trial , after choosing one arm , the learner receives the cost ( award ) for the selected arm , but does not know the cost ( or award ) of the other arms . Under this bandit setting , the learner is required to find the best ( possibly mixed ) arm in a repeated game . The learner can also be equipped with side information in order to identify the best arm more efficiently . Recently , this technique has been successfully applied to online multi class classification with partial feedback . Unlike the typical setup of online learning where the learner receives the correct class assignment for an input pattern after making prediction , in online multi class leaning with partial feedback , the learner only receives an incomplete feedback that indicates if the predicted class is correct . This problem is also referred to as Multiclass Bandit Prediction [ 8 ] and has found applications in several domains , including online advertisement and recommender systems [ 12 ] .
Banditron [ 8 ] , an algorithm that was recently developed for multi class bandit prediction , utilizes the exploration vs . exploitation tradeoff technique to handle the challenge of the partial feedback for online multi class learning . This tradeoff is explicitly captured by a single parameter γ ∈ ( 0 , 0.5 ) in Banditron : with probability 1− γ , the learner will predict the most likely class label based on the current classification model ( exploitation ) , and with probability γ , the learner will randomly choose one of the remaining class labels for prediction ( exploration ) . Figure 1 shows the performance of Banditron for three different data sets by varying the value of γ . It is clear that the performance of Banditron strongly depends on the value of γ and it is therefore important to develop strategies to automatically tune this parameter . In
204 tuitively , at the beginning of the learning stage , due to the fact that the classification model is trained by a small number of examples , we would expect it to perform poorly . As a result , it may be more desirable to have more exploration and consequentially set γ to a large value . As the learning procedure proceeds , after the classification model is updated with sufficiently large number of examples , we expect it to make accurate prediction . As a result , it is desirable to decrease the value of γ and consequentially reduce the amount of exploration with increasing number of training examples . Theoretically , as suggested by [ 8]1 , the choice of t )1/3 minimizes the mistake bound given ( i ) the γ = ρ( DK complexity measure D of optimal W in the Frobenius norm of W , ( ii ) the number of classes K and ( iii ) the noise level ρ2 . One major problem with this choice of γ is that it does not depend on the performance of the online classifier , as we would expect a small amount of exploration ( ie , small γ ) is needed when the online learner makes accurate prediction . This is because the optimal γ is obtained by minimizing the upper bound for the number of mistakes . Since most mistake bounds are concerned with the asymptotical behavior of an online learning algorithm , it tends to be very loose . As a consequence , the resulting γ is only optimal in the asymptotical sense , not necessarily for finite horizons . To further support our argument , in Figure 1 , we also include the error rates of online multi class bandit prediction using t )1/3 highlighted by horizonal lines . Here , we set γ = ρ( DK D = 2|Wp|2 F and ρ = '(WP )/T where Wp is the classifier learned by the Perceptron3 . It is clear that the error rates of using this γ are far from the true optimals .
This paper is organized as follows . Section 2 reviews the related work to this study . After a short description of Banditron algorithm in Section 3 , we propose three different methods to automatically learn the tradeoff parameter γ in Sections 3.2 , 3.3 , and 34 Section 4 shows the efficacy of the proposed approaches through an extensive experimental study . Section 5 concludes this work with future directions .
2 . RELATED WORK
Online multi class learning in bandit setting has been recently received increasing interests [ 9 , 8 , 11 , 2 ] . It is closely related to the multi armed bandit problem [ 14 , 1 , 6 ] , where the exploitation vs . exploration tradeoff is used [ 19 , 5 , 13 ] . The multi armed bandit cannot be applied directly to online multi class bandit prediction because they do not exploit any side information , such as the feature vector of training examples . Langford et al . extended the multi armed bandit setting to the case where side information is used to make a better prediction [ 9 ] . They refer to the new setting as “ contextual bandit problem ” and introduced the epoch greedy algorithm and derived its regret bound . Kakade et al . introduced an efficient algorithm for multiclass bandit prediction that is efficient and guarantees certain regret bound [ 8 ] , which utilizes the exploitation vs . exploration technique . Wang et al . developed a general potential based framework from which Banditron is a special case [ 18 ] . The major problem with Banditron is that its performance could be sensitive .T 1Corollary 3 in [ 8 ] . 2In [ 8 ] , it is assumed that there exists a ρ ∈ ( 0 , 1 ) such that i=1 't(W ) ≤ ρT , where 't(W ) is a convex loss function . 3Assuming that we already know the classifier learned by Perceptron to the parameter that trades off between exploration and exploitation , as shown in Fig 1 . To the best of our knowledge , this is the first study that aims to learn the tradeoff parameter for multi class bandit problem .
3 . OPTIMIZING THE TRADEOFF BETWEEN
EXPLORATION AND EXPLOITATION accordingly .
3.1 Preliminaries
Algorithm 1 shows Banditron [ 8 ] . At round t , Banditron
Banditron can be considered as the multi class perceptron [ 16 ] adapted for the bandit setting using the exploitation vs . exploration tradeoff strategy . Similar to the Per receives sample xt and predicts a class labelfiyt for xt . It then receives an incomplete ( bandit ) feedback [ yt = fiyt ] , where [ yt = fiyt ] is an indicator function that outputs 1 if yt = fiyt and zero otherwise , and updates its classification model ceptron algorithm , it computes label 'yt after receiving an to predictfiyt based on the exploitation vs . exploration tradeclass label'yt and with probability γt it chooses one random back [ yt '= fiyt ] , the algorithm computes the update matrix example xt using a linear model W . However , due to the bandit feedback , extra ( randomization ) steps are performed off strategy ( Steps 7 8 ) : with probability 1 − γt it predicts class label for prediction . After receiving the partial feed
.T t=1[fiyt '= yt ] . xtδt which , on average , is equivalent to the update matrix used in Perceptron for the full feedback setting . We denote by M the number of mistakes made by algorithm for all the trials , ie M = In our study , we assume a linear model for each class , denoted by W = ( w1 , . . . ,w K ) ∈ R d×K , although the extension to nonlinear classifiers using kernel trick is straightforward . We denote by W1 , . . . , WT the sequence of linear classifiers generated by an online learning algorithm over the trials . Our analysis is based on bounding the number of mistakes made by the online learning algorithm when compared to U , an arbitrary linear classifier . Since the proposed framework is a stochastic algorithm , we will focus on the expectation of the mistake bound . We denote by 't ( W ) the multi class hinge loss of classifier W on the training example ( xt , yt ) , defined as ff
't(W ) = ' max k.=y w k xt − w fi fi y xt where '(z ) = max(0 , z + 1 ) . Theorem 1 gives the mistake bound of the Banditron algorithm that is slightly different from the original work . It provides the foundation for deriving the strategies to learn the tradeoff parameter γ .
Theorem 1 . Let K be the number of classes . After running over a sequence of examples x1 , . . . ,x T , with ffxtff2 ≤ 1 for all t , the expected number of mistakes made by Banditron , denoted by E[M ] , is bounded as follows
+ E
( 1 ) where U is any arbitrary weight matrix ( classifier ) and {γt}T are sampling probabilities of trials . t=1 t=1 t=1
E[M ] ≤ |U|2 T
F +
T γt['yt = yt ] +
't(U ) + E t=1
( T T t=1
) [ 'yt '= yt ]
γt
K γt ff
205 0.8
0.7
0.6
0.5
0.4 e t a r r o r r
E
0
0.1
MNIST
Banditron Banditron_Adaptive
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6 e t a r r o r r
E
PENDIGITS
SEGMENTATION
Banditron Banditron_Adaptive
Banditron Banditron_Adaptive
0.8
0.78
0.76
0.74
0.72
0.7 e t a r r o r r
E
0.2
0.3 gamma
0.4
0.5
0.55
0
0.1
0.2
0.3 gamma
0.4
0.5
0.68
0
0.1
0.2
0.3 gamma
0.4
0.5
Figure 1 : The error rates of Banditron with different choice of γ for three different data sets t wt−1 fi Choose sampling probability γt
Algorithm 1 The Banditron Algorithm 1 : Set W 0 = 0 2 : for t = 1 , . . . , T do Receive xt ∈ R d 3 : 4 : 5 : 6 : 7 : 8 : 9 :
Compute 'yt = arg max1≤k≤K x Set pk = ( 1 − γt)[k ='yt ] +γ t/K , k = 1 , . . . , K Sample fiyt by distribution p = ( p1 , . . . , pK ) . Predict fiyt and receive feedback [ yt =fiyt ] Compute δt = 1 .yt − 1 fiyt where 1k stands for the vector with all its elements being zero except its kth element is 1 . Compute W t = W t−1 − xtδfi
[ yt= fiyt ] fiyt k p t
10 : 11 : end for line learning [ 3 ] , we have the following bound when'y is used
Proof . Following the gradient descent framework for on as the predictor : t=1
[ 'yt '= y ] ( 't(Wt−1 ) − 't(U ) ) [ 'yt '= y]U − Wt−1,∇'t(W )(
T ≤ T T U − Wt−1 , Et[Wt − Wt−1]( fi T t=1 t=1
=
|U − Wt−1|2
=
=
1 2
1 2
E t=1
|U − W0|2
≤ 1 2
≤ 1 2
|U|2
F +
1 2
|U|2
F + E
F + E
T
T t=1
E
1 2
( F − |U − Wt|2 T ffl ffi|δt|2 γt['yt = yt ] +
T t=1
F t=1 t=1 fl
F + |Wt − Wt−1|2
F
)
|Wt−1 − Wt|2
F ff
[ 'yt '= yt ]
K γt
In the above , the first inequality follows directly the convexity of the loss function , the first equality follows the relationt ] =∇' t(W ) [ 18 ] , the second inequality follows ship Et[xtδfi |xt|2 ≤ 1 , and the last inequality follows Lemma 5 of [ 8 ] .
We complete the proof by using the following relationships
E['yt '= yt ] ≤ T T E[M ] ≤ T E['yt '= yt ] + t=1 t=1
[ 'yt '= yt]'t(W )
T
γt t=1 t=1
Remark I : .
By scaling U and '(z ) as follows we generalize Theorem 1 as
U → ηU , '(z ) → 'η(z ) = max(0 , η + z )
) t=1
γt
( T T γt['yt = yt ] + ) ( T γt['yt = yt ] +
γt t=1
E[M ] ≤ t't(U ) + E t=1
|U|2
F +
+
η
2
1 η E
E[M ] ≤ T
+ |U|F t=1
't(U ) + E
T
2E t=1 By minimizing over scaling factor η , we have t=1 ff
T
[ 'yt '= yt ]
K γt
T ff
[ 'yt '= yt ]
K γt
( 2 ) t=1 t=1
K γt
L =
[ 'yt '= yt ] +
γt(1 + [ 'yt = yt ] )
Given the bound provided in Theorem 1 , the optimal set t=1 will be obtained by mini which the same inequality used by [ 8 ] to obtain the mistake bound . of sampling probabilities {γt}T T mizing the mistake bound stated in Theorem 1 , ie
T t=1 t=1
However [ 'yt '= yt ] and [ 'yt = yt ] are not provided explicitly previous studies , this difficulty is resolved by bound [ 'yt = the difficulty , we approximate [ 'yt '= yt ] and [ 'yt = yt ] by two in the partial feedback , making it impossible to obtain γt by directly minimizing the upper bound . Note that in the yt ] ≤ 1 , leading to the choice of γt independent from the prediction performance of the online classifier . To address biased estimators μt and τt , respectively :
206 • Approximate [ 'yt = yt ] by μt = 2['yt = fiyt][fiyt = yt ] .
This is because
[ 'yt = yt ] = Et
≤ Et ≤ Et [ μt ]
[ 'yt =fiyt][fiyt = yt ] [ 'yt =fiyt][fiyt = yt ] fiyt p
1 − γt
Note that in the last step , we use the fact γt ≤ 1/2 .
• Approximate [ 'yt '= yt ] by τt = 1 − [ 'yt =fiyt][fiyt = yt ]
[ 'yt '= yt ] = 1 − [ 'yt = yt ]
≤ 1 − Et [ ['yt =fiyt][fiyt = yt ] ]
= Et[τt ]
( 3 )
Using μt and τt allows us to compute the two quantities without have to know [ 'yt = yt ] or [ 'yt '= yt ] . However , it is infeasible to use μt and τt for approximation because both of them require knowing yt . We address this difficulty by .t−1 .t−1 approximating μt and τt with the average of μ and τ over with the trivial bounds that [ 'yt = yt ] ≤ 1 and [ 'yt '= yt ] ≤ 1 , all the trials up to the time t , ie , replacing μt and τt with i=1 μi/(t−1 ) and i=1 τi/(t−1 ) , respectively . Combining approximating [ 'yt = yt ] and [ 'yt '= yt ] • [ 'yt '= yt ] ≤ τt and [ 'yt = yt ] ≤ μt • [ 'yt '= yt ] ≤ 1 and [ 'yt = yt ] ≤ μt • [ 'yt = yt ] ≤ 1 and [ 'yt '= yt ] ≤ τt in the following subsections , we consider three strategies for
In order to generate a general family of functions for γt , we introduce the concept of good support function , which is the key gradient for defining the tradeoff parameter .
Definition 2 . A function ω(z ) defined in the domain of z ≥ 0 is called a good support function if it satisfies the following conditions : ( a ) ω(z ) is concave for z ≥ 0 and ω(0 ) ≥ 0 , ( b ) ω(z ) is monotonically increasing , ie , ( z ) > 0 , for z ≥ 0 , ( c ) ω(z ) is Lipschitz continuous with ωff ( z ) ≤ L , for z ≥ 0 , and ( d ) Lipschitz constant L , ie , ωff there exists a constant ρ ≥ 1 such that for any t ≥ 0 and z ≥ 0 , we have ωff ( z ) ≤ ρtωff
( z + t ) .
The discussion below allows us to connect the definition of good support function to the desired properties of γt .
• Given Property ( a ) , it is easy to conclude that ωff
( z ) ( z ) or ≤ ω.(z+t ) for t ≥ 0 . This non increasing property ( z ) also allows us to define non increasing γt that is a non decreasing function of z , ωff ω.(z ) of ωff is bounded between 0 and 1/2 .
( z + t ) ≤ ωff
1
1
• Property ( b ) allows us to have ω(z ) ≤ ω(z + t ) or ω(z ) for t ≥ 0 , another important quantity
≤ 1
1
ω(z+t ) for deriving the mistake bound .
• Property ( c ) and ( d ) allow us to upper bound ωff a constant L and by ωff
( z + t ) for t ≥ 0 .
( z ) by t=1
μt ≤ 2Kρ1ω1
ωff
2
The following proposition gives examples of good support function . where ρ1 and ρ2 are the constants defined respectively for the good support functions ω1 and ω2 .
T
'L =
K γt
T
Proposition 1 . ( 1 ) ω(z ) = ( a + z)λ , with λ ∈ ( 0 , 1 ] and a >0 , is a good support function , with Lipschitz constant L = λaλ−1 , and ρ = e(1−λ)/a , and ( 2 ) ω(z ) = ln(a + z ) with a >0 is a good support function , with L = 1/a , and ρ = e1/a .
Proof . We only show the result for ω(z ) = ( a + z)λ . A similar derivation can be applied to ω(z ) = ln(a + z ) . For Lipschitz continuous constant L , we have
L = max z≥0
λ
( a + z)1−λ = λaλ−1
For ρ , we have
( a + z + t)1−λ ( a + z)1−λ
≤ ( 1 + t/a )
1−λ ≤ et(1−λ)/a leading to ρ = e(1−λ)/a .
Using [ 'yt = yt ] ≤ μt and [ 'yt '= yt ] ≤ τt , we approximate
3.2 Finding Optimal γ using μt and τt L by
2
1 t=1 t=1
( 4 )
τt +
γt =
γt(μ + 1 ) . i=1 1 +μ i
ωff 2ωff
1(z ) ≥ ωff
To bound 'L , we introduce two good support functions ω1(z )
2(z ) for any z ≥ 0 , and define γt and ω2(z ) , with ωff as follows
.t−1 .t−1
.t−1 .t−1
.t−1 .t−1 i=1 τi
We have γt ≤ 1/2 for any t because ≤ ωff 2ωff 1(z ) ≥ ωff The first inequality follows ωff 2(z ) , and the last step follows the property τi ≤ 1 and the fact that ωff 1(z ) is nonincreasing in z . In addition , since ωff 1(z ) and ωff 2(z ) are nonincreasing functions of z , γt is decreasing in μt and increasing in τt , implying that the algorithm gives a large amount of exploration when the classifier makes a number of mistakes , and a small amount of exploration when the classifier encounters only a few prediction errors . The following proposition shows key properties for γt in Equation 4 .
=
γt ≤ ωff i=1 τi i=1 τi i=1 1 +μ i i=1 τi
2ωff
1 2
1
1
1
1
Proposition 2 . Given γt defined in ( 4 ) , we have the fol lowing inequalities : t=1
T T T t=1
K γt
γt ≤
1
2ωff γtμt ≤ ρ2 2ω2 2ωff
ρ2ω2 ( T ) t=1 τt
.T .T .T .T .T
1 t=1 τt t=1 μt t=1 1 +μ t t=1 τt
207 Proof . Define At =
T t=1
K γt
τt =
.t T T t=1
= t=1
≤ 2K
1(At−1)(At − At−1 )
2
2 i=1 1 +μ i i=1 τi . We have 2Kωff 1(At−1)τt ωff i=1 1 + μi
1(At−1)(At − At−1 ) 2Kωff ωff t=1 ωff ωff
.t−1 .t−1 .T .T T T i=1 1 + μi t=1
2
ω1(At ) − ω1(At−1 )
≤ ρ1 ≤ ρ1ω1(AT ) , t=1 where the inequality follows that ωff tion . Since
2 is a non increasing func
1(At−1)(At − At−1 ) ≤ ρ1 ωff
1(At)(At − At−1 ) ωff
T t=1 we have the first inequality . In the above , the first step fol1(At−1 ) ≤ lows the definition of good support functions ( ie ωff ρ1ωff 1(At) ) , the second step is due to the concavity of ω1 and the last step is due to ω1(0 ) ≥ 0 . Combining the above results produces the first inequality in the proposition . Similar to the first inequality , for the second inequality , we define Bt = i=1 μi and have
.t
T t=1
γtμt =
≤
≤
T .T .T t=1 t=1 ωff
μt
2(Bt−1 + t − 1 ) ωff 2ωff 1(At−1 ) 2(Bt−1)(Bt − Bt−1 ) 2ωff 2(Bt)(Bt − Bt−1 ) 2ωff
1(AT )
1(AT )
2ωff t=1 ρ2
≤ ρ2 2ω2(BT ) 2ωff 1(AT )
2(Bt ) and μt ≤ The second equality follows ωff 2 . The same analysis is applied to show the last inequality .
2(Bt−1 ) ≤ ρμt
2 ωff
Theorem 3 . Let ω1(z ) and ω2(z ) be two good support 2(z ) for any z ≥ 0 . By running functions with ωff Algorithm 1 with γt set as in Eq ( 4 ) , we have the following bound for the expected number of misclassified examples :
1(z ) ≥ ωff
E[M ] ≤ T
't(U ) + fl
ρ2ω2(T ) 2ωff 1(T ) fl
2ρ1Kω1(3T )
ωff 2(3T )
+ ρ2
ω2(2T ) 2ωff 1(2T ) t=1
+ |U|F where ρ1 and ρ2 are the constants of two good support functions .
√ Proof . The proof is straightforward by using the result b , a + b ≤ √ in Remark I , Proposition 2 , inequality and the properties of good support functions . a +
√
Corollary 4 . Suppose γt is in Eq ( 4 ) with ω1(z ) = ( 1+ z)λ1 and ω2(z ) = ( 1 + z)λ2 , where λ1 , λ2 ∈ ( 0 , 1 ] and λ1 =
λ2 + 1/3 . By running Algorithm 1 , we have the following bound for the expected number of misclassified examples :
E[M ] ≤ T ⎛⎝fl
+|U|F t=1
't(U ) +
ρ2
λ2 + 1 3
( 1 + T )
2 3
3 K
ρ2e 1 λ2
( 1 + 3T )
2 3 +
⎞⎠
ρ2(cid:30 )
λ2 + 1 3
( 1 + 2T )
1 3 where ρ2 = e1−λ2 . This bound is of O(T 2/3 ) and similar to the bound of the original Banditron .
Remark II : .
Although the mistake bounds given in Theorem 3 and Corollary 4 are not better than the result derived in the previous study , we emphasize that the analysis of mistake bound is often loose because its focus is to obtain the asymptotical behavior of the learner . The key advantage of the tradeoff parameter γt defined in Eq 4 is its capability of adapting to the classification performance ( ie , μt and τt ) of the learner , making it intuitively more attractive . 3.3 Finding Optimal γ using μt
Using [ 'yt '= yt ] ≤ 1 and [ 'yt = yt ] ≤ μt , we upper bound L by
T
'L =
T
K γt
+ t−1 t=1 t=1
γt(1 + μt )
Given a good support function ω(z ) , we define γt as
ωff
1 2L
γt =
1 +μ i i=1
( 5 ) It is straightforward to see γt ∈ [ 0 , 1/2 ] . Since ω(z ) is a concave function , ωff ( z ) is a non increasing function of z , leading to a decreasing value for γt as more and more training examples have been classified correctly . The following proposition shows key properties for γt in ( 5 ) .
Proposition 3 . Given γt defined in ( 5 ) , we have the fol lowing inequalities :
T
ω
μt t=1
γtμt ≤ ρ2 2L γt ≤ ρ 2L
ω ( T )
.T
≤
K γt
ωff
2KLT t=1 1 + μt
T T T t=1 t=1 t=1 fl
The proof is almost identical to that for Proposition 2 .
Theorem 5 . Let ω(z ) be a good support function . By running Algorithm 1 with γt set as in Eq ( 5 ) , we have the following bound for the expected number of mistakes made by the algorithm :
+
ρω(T )
2L + |U|F
ω(2T ) 2L +
2KLT ωff(3T )
E[M ] ≤ T t=1
!
ρ
208 The following corollary directly follows from Proposition 1
4 . EXPERIMENTS
Corollary 6 . By running Algorithm 1 with γt as in Eq ( 5 ) and Theorem 5 . and ω(z ) = ( 1 + z)λ , where λ ∈ ( 0 , 1 ] , we have the following bound for the expected number of classification mistakes :
E[M ] ≤ T e1−λ 2λ ( 1 + T ) ff t=1
λ
+ |U|F
( 1 + 2T )
λ 2 +
2K(1 + T )
1−λ
2 T
1 2
√
't(U ) + e1−λ√ 2λ
When λ = 2/3 , we have E[M ] = O(T 2/3 ) which is the same convergence rate as Banditron . 3.4 Finding Optimal γ using τt
Similar to the approach presented in the previous sections , we set γt as
In this section , we conduct experiments on several realword data sets to validate the proposed strategies for balancing the tradeoff between exploration and exploitation . 4.1 Data Set
Several real world data sets from UCI data repository [ 7 ] and LIBSVM web page [ 4 ] are used in our study as described in the following . Notice that for some of these data sets , there were two separate sets , one for training and one for testing . We only used the training set in our experiments . • RCV1 . RCV1 consists of 15 , 564 of training documents for text categorization . Each document is a news report from Reuters [ 10 ] , represented by a vector of 47 , 236 dimension , and belongs to one of the 53 classes .
( 6 )
γt =
2ωff
1 i=1 τi
ωff 2(t )
.t−1 .T where ω1(z ) and ω2(z ) are two good support functions and 1(z ) ≥ ωff 2(z ) . It is easy to verify that γt ∈ ( 0 , 1/2 ] due to ωff the properties of a good support function . The proposition below allows us to bound t=1 γt and t=1 K/γt .
Proposition 4 . Given the construction of γt in ( 6 ) , we have the following inequalities :
.T .T .T
ωff 2(T )
ω2(T )
ω1 t=1 τt
T T
K γt t=1
τt ≤ 2Kρ
γt ≤
2ωff
• MNIST . MNIST contains grey scale images of size 28×28 for hand written digits . It contains 60000 training samples , each represented by 780 features .
• Protein . Protein has 17766 samples , represented by
357 features and three classes .
• Letter . Letter contains 15000 instances of 26 charac ters , represented by 16 features .
• optdigits . This is a normalized bitmaps of handwritten digits from 30 people that contains 3823 instances , each represented by 8 × 8 features .
• pendigits . This is another digits collection containing
7495 samples , each with 16 features .
• Nursery . Originally developed to rank applications for nursery school , it has 12960 records , each represented by 8 features belonging to one of 4 classes ( we removed one class that only had two samples ) .
• Isolet . Isolet contains 7797 spoken alphabet with 617 attributes and 26 classes .
• Abalone . Developed to predict the age of abalone from physical measurement , it contains 4177 records , each represented by 8 features and one of the 29 classes . • Car . This data set contains 1728 samples , each repre sented by 6 features and one of the four classes .
• Waveform . Waveform data set contains 5000 samples of three classes , each represented by 21 features .
• RedWineQuality . This data set contains 1599 red wine records , each represented by 11 features and one of the 6 classes .
• RedWineQuality . WhiteWineQuality contains 4898 white wine records , each represented by 11 features and one of the 7 classes .
• Segmentation . The image Segmentation data set contains 2310 samples . Each sample is represented by 19 features and belongs to one of the 7 classes .
1 t=1 t=1 τt Proof . Similar to Proposition 2 and 3 . Theorem 7 . Let ω1(z ) and ω2(z ) be two good support functions . By running Algorithm 1 with γt set as in Eq ( 6 ) , we have the following bound for the number of misclassified examples M =
.T t=1['yt '= yt ] : E[M ] ≤ T fl
't(U ) + t=1
ω2(T )
.T fl t=1 τt
2ωff
1
+ |U|F
ω2(T ) 2ωff 1 ( T )
+
2Kρ1
ω1 ( T ) ωff 2(T )
Proof . The proof directly follows Theorem 1 and Propo sition 4 .
The following corollary directly follows from the result of
Proposition 1 and Theorem 7 . Corollary 8 . By running Algorithm 1 with γt in Eq ( 6 ) and ω1 = ( 1 + z)λ1 and ω2 = ( 1 + z)λ2 with λ1 , λ2 ∈ ( 0 , 1 ] , we have the following bound for the expected number of misclassified examples
E[M ] ≤ T ! t=1
+|U|F
't(U ) +
1 2λ1
( 1 + T )
λ2+1−λ1
( 1 + T )
λ2+1−λ1
2
+
1 2λ1
!
2ke1−λ1
λ2
λ1+1−λ2
2
( 1 + T ) with λ1 = λ2 + 1 same rate as Banditron .
3 we have E[M ] =O ( T 2/3 ) which is of the
209 CAR
OPTDIGITS
OPTDIGITS a m m a g
0.5
0.4
0.3
0.2
0.1
0
0
Banditron_ag0 Banditron_ag1 Banditron_ag2 Banditron_Ag3
500
1000
Training rounds
1500
2000 a m m a g
0.5
0.4
0.3
0.2
0.1
0
0
Banditron_ag0 Banditron_ag1 Banditron_ag2 Banditron_Ag3
1000
2000
Training rounds
3000
4000 a m m a g
0.5
0.4
0.3
0.2
0.1
0
0
Banditron_ag0 Banditron_ag1 Banditron_ag2 Banditron_Ag3
1000
2000
Training rounds
3000
4000
Figure 2 : Behavior of γ in different methods for data sets , Car and Optdigits . There are two figures for Optdigits to stress that the proposed methods adapt γ based on the sequence of the samples over trials .
4.2 Experimental Settings
We refer to the algorithms developed in Sections 3.2 , 3.3 , and 3.4 as Banditron ag3 , Banditron ag1 and Banditron ag2 . To evaluate the classification performance of the three proposed learning strategies for exploitation vs . exploration tradeoff parameter γ , we compare them with three different version of Banditron , namely , Banditron Worst , Banditron Best , and Banditron ag0 . Banditron Worst and Banditron Best are Banditron algorithm when γ is set to the worst and best value for a given data . The worst and best γ are found from the predefined set S = {0.001 , 0.01 , 0.1 , 0.15 , 0.2 , 0.25 , 0.3 , 0.35 , 0.4 , 0.45 , 05} Banditron ag0 is the t )1/3 ) as Banditron with the adaptive γt = min(0.5 , ρ( DK suggested in [ 8 ] for the general agnostic case . We used D = 2|Wp|2 F and ρ = '(WP )/T where Wp is the classifier learned by the Perceptron 4 . We repeat each experiment 50 times by generating random sequences of instances and report the average accumulate error rates , which are computed as the ratio of the number of misclassified samples to the number of samples received so far . For all three proposed methods in all the experiments , we use similar good support functions ω(z ) = ω1(z ) =ω 2(z ) = ( 1 + z)λ with λ = 0.1 for a fair comparison . Also notice that the result is stable for most of these data sets with different values of λ .
4.3 Experimental results
To study the behavior of different learning algorithms over trials , we show the average error rates of all the methods over the entire online process in Figure 3 . First notice that there is big gap between Banditron worst and Banditron Best in all data sets that emphasizes that the Banditron algorithm can perform very poorly if γ is not set appropriately .
We observe that overall the proposed algorithms exhibit similar or better learning rates as the Banditron algorithm with the optimal γ . In particular , banditron ag2 and banditron ag3 yields the best performance among the algorithms in comparison . In most of the data sets , banditron ag2 and banditron ag3 perform significantly better than bant )1/3 ) is ditron ag0 which suggests that γt = min(0.5 , ρ( DK not a good adaptive choice ( even with the assumption that Perceptron setting for variables D and ρ is known ) . As a few examples , notice that the final error rate of banditron ag0 is 56 % versus 37 % error rate of banditron ag2 , banditron ag3 4We also tried γt = 05t−1/3 The proposed approach consistently produces better result than γt = 0.5t−1/3 as well . and banditron best for MNIST data set . For Optdigits data set , the final error rate of banditron ag2 and banditron ag3 is 56 % which is significantly low compared to 59 % error rate of banditron best and banditron ag0 . The latter example also suggests that our adaptive strategy is better than the Banditron with a single best γ .
Although better than banditron worst , the performance of banditron ag1 is not comparable to that of the other methods for most of the cases . This can be explained by the inherited difference between banditron ag1 and the other two proposed approaches . Unlike banditron ag2 and banditron ag3 where two good support functions are introduced to determine γt , the γt defined in banditron ag1 is determined by a single good support function . As a result , we have a better control of the value for γ over time in banditron ag2 and banditron ag3 by a trade off between two functions : one which is the decreasing function of time and the other which is the increasing function of the number of misclassified examples .
Figure 2 shows how γ changes during the learning process for different algorithms for two data sets , car and optdigits . We also plot two different runs of optdigits to see how the proposed methods is sensitive to the order of presented samples . Notice that unlike banditron ag0 that constantly decreases the value of γ , the proposed methods are more adaptive . More particularly , banditron ag2 and banditron ag3 update γ based on the performance of Banditron over trials , and if needed , they increase the value of γ . Moreover , the proposed methods are sensitive to the sequence of the samples , and chooses the best updating rule based on how samples presented over trials and how much the learning process in Banditron succeeded so far .
5 . CONCLUSION
In this work , we study the problem of optimizing the exploration exploitation tradeoff in the context of multi class bandit prediction . We proposed three different strategies to automatically tune the tradeoff parameter used by the Banditron algorithm . We showed through extensive experimental study that the proposed approaches are effective in adjusting the exploration exploitation tradeoff . In particular , we found that banditron ag2 and banditron ag3 achieve similar or better performance compared to Banditron with the best value for γ . In the future , we plan to exploit other approaches for balancing the exploration exploitation tradeoff other than the greedy strategy applied in Banditron .
210 e t a r r o r r
E
0.9
0.8
0.7
0.6
0.5
0.4 e t a r r o r r
E e t a r r o r r
E
0.98
0.96
0.94
0.92
0.9
0.88
0.86
0.84
0
0.96
0.94
0.92
0.9
0.88
0.86
0.84
0.82
0.8
Banditron_Best Banditron_worst Banditron_ag0 Banditron_ag1 Banditron_ag2 Banditron_Ag3
PROTEIN
5000
10000
Training rounds
15000
OPTDIGITS
1000
2000
Training rounds
3000
4000
0.64
0.62
0.6
0.58
0.56
0.54
0.52
0.5
0.85
0.8
0.75
0.7
0.65
0.6
0.55 e t a r r o r r
E e t a r r o r r
E
MNIST
0.65
NURSERY
1
2 4 Training rounds
3
LETTER
5
6 x 104 e t a r r o r r
E e t a r r o r r
E
0.6
0.55
0.5
0.45
0.4
0
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
5000
10000
Training rounds
PENDIGITS
15000
5000
10000
Training rounds
15000
0.55
0
2000
4000
Training rounds
6000
8000
ISOLET
RCV1TRAIN t e a r r o r r
E
0.95
0.9
0.85
0.8
0.75
0.7 e t a r r o r r
E
0.55
0.5
0.45
0.4
0.35
2000 4000 6000 8000 10000 12000 14000
Training rounds
WAVES
1000
4000
5000
2000 3000 Training rounds
SEGMENTATION
2000
4000
Training rounds
6000
8000
ABALONE
CAR e t a r r o r r
E
0.665
0.66
0.655
0.65
0.645
0.64
0.635
0.63 e t a r r o r r
E
0.76
0.74
0.72
0.7
0.68
0.66
4000
1000
2000
3000
Training rounds
REDWINEQUALITY
500
1000 Training rounds
1500
0.6
0.55
0.5 e t a r r o r r
E
0.45
0.82
0.8
0.78
0.76
0.74
0.72
0.7
0.68 e t a r r o r r
E
500
1000
Training rounds
1500
WHITEWINEQUALITY
1000
2000 3000 Training rounds
4000
5000 e t a r r o r r
E
0.84
0.82
0.8
0.78
0.76
0.74
0.72
0.7
Figure 3 : The error rates of different methods over trials . Each point on a curve is the average results of 50 randomly generated sequences of data .
500
1000 1500 Training rounds
2000
211 6 . ACKNOWLEDGEMENT
This work is supported in part by National Science Foun dation ( IIS 0643494 ) , and the Office of Navy Research ( N0001409 1 0663 ) .
7 . REFERENCES [ 1 ] Peter Auer , Nicol`o Cesa Bianchi , and Paul Fischer .
Finite time analysis of the multiarmed bandit problem . Machine Learning , 47(2 3):235–256 , 2002 . [ 2 ] Alina Beygelzimer , John Langford , Lihong Li , Lev Reyzin , and Robert E . Schapire . An optimal high probability algorithm for the contextual bandit problem . Computational Research Repository , abs/1002.4058 , 2010 .
[ 3 ] N . Cesa Bianchi and G . Lugosi . Prediction , learning , and games . Cambridge Univ Pr , 2006 .
[ 4 ] Chih Chung Chang and Chih Jen Lin . Libsvm : a library for support vector machines , 2001 .
[ 5 ] Eyal Even Dar , Shie Mannor , and Yishay Mansour .
PAC bounds for multi armed bandit and markov decision processes . In COLT ’02 : Proceedings of the 15th Annual Conference on Computational Learning Theory , pages 255–270 , 2002 .
[ 6 ] Eyal Even Dar , Shie Mannor , and Yishay Mansour . Action elimination and stopping conditions for the multi armed bandit and reinforcement learning problems . Journal of Machine Learning Research , 7:1079–1105 , 2006 .
[ 7 ] A . Frank and A . Asuncion . UCI machine learning repository , 2010 .
[ 8 ] Sham M . Kakade , Shai Shalev Shwartz , and Ambuj
Tewari . Efficient bandit algorithms for online multiclass prediction . In ICML 2008 : Proceedings of the 25th international conference on Machine learning , pages 440–447 , 2008 .
[ 9 ] John Langford and Zhang Tong . The epoch greedy algorithm for contextual multi armed bandits . In NIPS 2007 : Proceeding of the 20th Annual Conference on Neural Information Processing System , 2007 .
[ 10 ] D . D . Lewis , Y . Yang , T . Rose , and F . Li . Rcv1 : A new benchmark collection for text categorization research . Journal of Machine Learning Research , 5:361–397 , 2004 .
[ 11 ] Lihong Li , Wei Chu , John Langford , and Robert E .
Schapire . A contextual bandit approach to personalized news article recommendation . In WWW ’10 : Proceedings of the 19th international conference on World wide web , pages 661–670 , New York , NY , USA , 2010 . ACM .
[ 12 ] Wei Li , Xuerui Wang , Ruofei Zhang , Ying Cui , Jianchang Mao , and Rong Jin . Exploitation and exploration in a performance based contextual advertising system . In KDD 2010 : Knoledge Discovery and Data Mining , pages 27–36 , 2010 .
[ 13 ] Shie Mannor and John N . Tsitsiklis . The sample complexity of exploration in the multi armed bandit problem . Journal of Machine Learning Research , 5:623–648 , 2004 .
[ 14 ] Herbert Robbins . some aspects of the sequential design of experiments . Bulletin of the American Mathematical Society , 58:527–535 , 1952 .
[ 15 ] Herbert Robins . Some aspects of the sequential design of experiments . Bull . Amer . Math . Soc . , 58(5):527–535 , 2010 .
[ 16 ] F . Rosenblatt . The perceptron : a probabilistic model for information storage and organization in the brain . Psychological review , 65:386–408 , 1958 .
[ 17 ] Joann`es Vermorel and Mehryar Mohri . Multi armed bandit algorithms and empirical evaluation . In In European Conference on Machine Learning , pages 437–448 . Springer , 2005 .
[ 18 ] Shijun Wang , Rong Jin , and Hamed Valizadegan . A potential based framework for online multi class learning with partial feedback . In ISTATS 2010 : Artificial Intelligence and Statistics , 2010 .
[ 19 ] C . Watkins . Learning from delayed Rewards . PhD thesis , Cambridge , 1989 .
212
