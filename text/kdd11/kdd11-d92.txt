Mining Frequent Closed Graphs on Evolving Data Streams
Albert Bifet , Geoff Holmes , and
Bernhard Pfahringer
University of Waikato Hamilton , New Zealand
{abifet,geoff,bernhard}@cswaikatoacnz
Ricard Gavaldà
LARCA Research Group
UPC Barcelona Tech Barcelona , Catalonia gavalda@lsiupcedu
ABSTRACT Graph mining is a challenging task by itself , and even more so when processing data streams which evolve in real time . Data stream mining faces hard constraints regarding time and space for processing , and also needs to provide for concept drift detection . In this paper we present a framework for studying graph pattern mining on time varying streams . Three new methods for mining frequent closed subgraphs are presented . All methods work on coresets of closed subgraphs , compressed representations of graph sets , and maintain these sets in a batch incremental manner , but use different approaches to address potential concept drift . An evaluation study on datasets comprising up to four million graphs explores the strength and limitations of the proposed methods . To the best of our knowledge this is the first work on mining frequent closed subgraphs in non stationary data streams .
Categories and Subject Descriptors H28 [ Database applications ] : Database Applications— Data Mining
General Terms Algorithms
Keywords Data streams , closed mining , graphs , concept drift
1 .
INTRODUCTION
Graph mining is a challenging task that extracts novel and useful knowledge from graph data [ 19 , 4 ] . Due to novel applications in social networks , chemical informatics , bioinformatics , communication networks , computer vision , video indexing and the Web [ 21 ] , more and more large scale graphs and sets of graphs are becoming available for analysis . Frequent pattern mining on graphs is one of the ways to obtain useful patterns , eg for discriminating graphs in classification and clustering tasks .
Conventional graph mining methods assume that the amount of data is limited and that it is therefore possible to store all data in memory or local secondary storage . There is no limitation on processing time , either . In the Data Stream model , we have space and time restrictions . Fundamentally , these restrictions imply the use of incremental techniques . Furthermore , as the data source is not necessarily stationary , methods must be able to adapt to changes over time in the data as well .
As the number of possible subgraphs is exponential , we propose to use coresets or compressed representations of the graphs based on closed patterns . Closed patterns are powerful representatives of frequent patterns , since they eliminate redundant information . A frequent pattern is closed for a dataset if none of its proper superpatterns has the same support as itself . Other possible definitions of a frequent closed pattern are the following :
• a frequent pattern is closed if it is one of the intersec tions of all transactions that contain it .
• a frequent pattern is closed if no superpattern is con tained in exactly the same transactions as itself .
Generally , there are many fewer frequent closed graphs than frequent ones . In fact , we can obtain all frequent subgraphs with their support from the set of frequent closed subgraphs with their support . So , the set of frequent closed subgraphs maintains the same information as the set of all frequent subgraphs .
There are many methods for computing frequent closed itemsets ( see [ 21] ) , frequent closed sequences [ 31 , 28 ] , and trees [ 18 , 6 , 26 , 5 ] , but only two for frequent closed graphs [ 30 , 13 ] , and none for frequent closed graphs on data streams .
We propose the first general methodology to identify closed graphs in a data stream . We develop three closed graph algorithms : IncGraphMiner , an incremental closed graph mining algorithm ; WinGraphMiner , a sliding window based closed graph mining algorithm ; and finally AdaGraphMiner , an adaptive closed graph mining algorithm . “ Adaptive ” means that the model maintains at all times the closed graphs that are frequent in the current state of the data stream , that is , since the latest change in distribution was detected .
The rest of the paper is organised as follows . Sections 2 and 3 give background and present the novel weighted frequent closed pattern setting . Section 4 introduces a new coreset structure and its properties . Section 5 details the handling of concept drift and Section 6 introduces the gen eral mining framework . Experimental results are given in Section 7 , and conclusions are drawn in Section 8 . 1.1 Related Work
There is a large body of work on itemset mining from data streams ; see the survey [ 23 ] and the references therein . We can divide these data stream methods into two different classes depending on whether they use a landmark window , containing all the examples seen so far , or a sliding window . Only a small fraction of these methods deal with frequent closed mining . Moment [ 17 ] , CFI Stream [ 24 ] and IncMine [ 22 ] are state of the art algorithms for mining frequent closed itemsets over a sliding window . CFI Stream stores only closed itemsets in memory , but maintains all closed itemsets as it does not apply a minimum support threshold , with the corresponding memory penalty . Moment stores much more information besides the current frequent closed itemsets , but it has a minimum support threshold to reduce the number of patterns found . IncMine proposes a notion of semi FCIs that increases the minimum support threshold for an itemset as it is retained longer in the window .
For trees , the work in [ 8 ] shows a general methodology to identify closed patterns in a data stream , using Galois Lattice Theory . This approach is based on an efficient representation of trees and a low complexity notion of relaxed closed trees , and presents an online strategy and an adaptive sliding window technique for dealing with changes over time . The approach is different to the one presented in this paper , as it does not use coresets , or weighted frequent mining techniques .
In terms of graphs , two main algorithms exist for mining frequent closed graphs :
• CloseGraph [ 30 ] : based on gSpan [ 29 ] , a miner for finding frequent subgraphs , based on depth first search ( DFS )
• MoSS [ 13 ] : an extension to MoFa [ 11 ] based on breadth first search ( BFS ) .
Aggarwal et al . [ 3 ] present a mining methodology to find frequent and dense patterns in graph streams . Their notion of density is based both on node occurrence and edge density , and they present an approach based on finding an approximation of the exact method with the use of a minhash approach .
To the best of our knowledge the work presented here is the first work dealing with mining frequent closed graphs in streaming data that evolve with time .
2 . PRELIMINARIES We are interested in ( possibly infinite ) sets of graphs , endowed with a partial order relation among these graphs . The set of all graphs will be denoted by G , but actually all our developments will proceed in some finite subset of G which will act as our universe of discourse . Given two graphs g and g , we say that g is a subgraph of g , or g is a super graph of g , if g g . Two graphs g , g are said to be comparable if g g or g g . Otherwise , they are incomparable . Also we write g ≺ g if g is a proper subgraph of g ( that is , g g and g = g ) . The input to our data mining process is a dataset D of weighted transactions , where each transaction s ∈ D consists
Transaction Id
Graph O
Weight
C C S N
O
O
C C S N
C
O
C S N
C
N
C C S N
N
C S N
N
C S O
1
1
1
1
1
1
1
2
3
4
5
6
Table 1 : Example of graph weighted transaction dataset . of a transaction identifier tid , and a graph . The dataset is a finite set in the standard setting , and a potentially infinite sequence in the data stream setting . Tids are supposed to run sequentially from 1 to the size of D .
Following standard usage , we say that a transaction s supports a graph g if g is a subgraph of the graph in transaction s . The number of transactions in the dataset D that support g is called the support of the graph g . A subgraph g is called frequent if its support is greater than or equal to a given threshold min sup . The frequent subgraph mining problem is to find all frequent subgraphs in a given dataset . Any subgraph of a frequent graph is also frequent and , therefore , any supergraph of a infrequent graph is also infrequent ( the antimonotonicity property ) . We define a graph g to be closed ( implicitly , wrt to D ) if none of its proper supergraphs has the same support as g has in D .
In this paper we focus our examples and experiments on molecular graphs , but our approach is general enough to be applied to any type of graph .
3 . FREQUENT CLOSED WEIGHTED
GRAPH MINING
In this section we introduce an extension to the frequent closed graph mining problem which enables the use of compressed graph representations in our adaptive methods : the frequent weighted closed graph mining problem . This problem differs from standard frequent closed graph mining in that each input graph has a weight , and this weight is used to compute its weighted support . The input into the data mining process is a dataset D of weighted transactions , where each transaction s ∈ D consists of a transaction identifier , tid , a graph , and a weight value .
Relative Support
Support
Graph
Relative Support
Support
Graph
N
C S
C C S N
O
C S N
N
C S
C S O
C S N
2
3
3
3
3
1
1
6
6
3
3
3
4
5
C C S N
O
C S N
N
C S
3
3
3
3
3
3
Table 3 : Example of a coreset with minimum support 50 % and δ = 1 for the graph dataset of Table 1 .
The main advantage of coresets is that we can apply any fast approximation algorithm on the usually much smaller coreset to compute an approximate solution for the original set more efficiently .
Here we use the set of frequent closed subgraphs as a smaller subset of the frequent subgraphs maintaining the same information but using less space . We define the relative support of a closed graph as the support of a graph minus the relative support of its closed supergraphs . We define the relative support in this way to exploit the fact that the sum of the closed supergraphs’ relative supports of a graph is equal to its own support .
We define a ( s , δ) coreset for the problem of computing closed graphs as a weighted multiset of frequent δ tolerance closed graphs with minimum support s using their relative support as a weight . Table 2 shows a ( 50%,0) coreset for the dataset in Table 1 and Table 3 shows a ( 50%,1) coreset for the same dataset .
Note that given a graph dataset D , its ( s , 0) coreset is the set of frequent closed graphs , and its ( s , 1) coreset contains the frequent maximal graphs . As the set of maximal graphs is contained in the set of closed graphs for a minimum support s , the set of graphs in the ( s , 1) coreset is also a subset of the graphs in the ( s , 0) coreset . For example , the set of graphs in the ( 50%,1) coreset of Table 3 is a subset of the graphs in the ( 50%,0) coreset of Table 2 .
For two minimum supports sL and sH where sL < sH , the set of graphs in its ( sH , δ) coreset is a subset of the graphs in the ( sL , δ) coreset .
A coreset is a “ compressed representation ” of the original dataset . This compression is two dimensional :
• Minimum support excludes infrequent graphs • δ tolerance excludes supergraphs with very similar sup port
In this paper we focus only on ( s , 0) coresets , ie , frequent closed graphs , as they are the only coresets that can guarantee the recovery of all frequent and frequent closed graphs with minimum support s . To show that this ( s , 0) coreset outputs all the frequent subgraphs with minimum support s , we need the following proposition .
Proposition 3 . Let D1 and D2 be two datasets of graphs . Let C1 and C2 be the ( s , 0) coresets of closed graphs for D1 and D2 . The set of closed graphs of C1 ∪ C2 is exactly the set of closed graphs of D1 ∪ D2 .
First , we note that the support of a graph in C1 and C2 is the same support of a graph in D1 and D2 , due to the
Table 2 : Example of a coreset with minimum support 50 % and δ = 0 for the transaction dataset of Table 1 .
Table 1 shows an example of a weighted dataset with six molecule graphs [ 11 ] . The sum of the weights of transactions in the dataset D that supports g is called the weighted support of the graph g . For brevity , weighted support will be abbreviated to just support . A subgraph g is called frequent if its weighted support is greater than or equal to a given threshold min sup . We define a graph g to be closed if none of its proper supergraphs has the same weighted support as it has . A graph g is maximal if none of its proper supergraphs is frequent . All maximal graphs are closed but not necessarily otherwise . We define a graph g to be δ tolerance closed [ 16 , 25 ] if none of its proper frequent supergraphs has a weighted support larger than or equal to ( 1 − δ ) · support(g ) . Note that a maximal graph is a 1 tolerance closed graph , and a closed graph is a 0 tolerance closed graph . Note that , from [ 8 ] we have the following propositions :
Proposition 1 . Adding a transaction with pattern g to a dataset of patterns D where g is closed does not modify the number of closed patterns for D .
Proposition 2 . Deleting a pattern transaction that is repeated in a dataset of patterns D does not modify the number of closed patterns for D .
Using these propositions we can observe that when adding to or removing from the weights of transactions , we are not modifying the number of closed patterns provided they remain frequent .
4 . CORESETS OF CLOSED GRAPHS
A coreset [ 2 ] of a set P with respect to some problem is a small subset that approximates the original set P , in the sense that solving the problem for the coreset provides an approximate solution for the problem on the original set P . This notion was introduced in computational geometry to denote a small subset of points that can be used to obtain approximate solutions with theoretical guarantees .
For example , for clustering [ 1 ] , a coreset for a set is a small set , such that for any set of cluster centers the clustering cost of the coreset is an approximation for the clustering cost of the original set with small relative error . fact that the support of a graph in C1 and C2 is the sum of all relative supports . As the support of a closed graph in D1 and D2 is the same support as in C1 and C2 , we can obtain from them the same closed subgraphs , using relative support instead of support .
Proposition 4 . Let D1 and D2 be two datasets of graphs . Let C1 and C2 be the ( s , 0) coresets of closed graphs for D1 and D2 . The set of frequent graphs of C1∪ C2 is exactly the set of frequent graphs of D1 ∪ D2 .
To show that we can also obtain the frequent ones , we see that the frequent non closed graphs have the support of their closure , so we obtain the same frequent graphs .
Proposition 5 . Let D1 , D2 and D be three datasets of graphs with D = D1 ∪ D2 . Let C1 , C2 and C be the ( s , 0)coresets of closed graphs for D1,D2 and D . The set of frequent graphs for C1 = C\C2 = C ∪ C2 obtained using relative support for C1 and negative relative support for C2 , is exactly the set of frequent graphs for D1 = D\D2 .
As the support of a graph in C1 is the support of the graph in C minus its support in C2 , we can obtain the closed graphs in C1 by adding the closed graphs in C2 , with negated relative support , to C .
In general terms , using ( s , 0) coresets we can obtain all frequent , closed and maximal graphs with minimum support s without losing any information . Using ( s , δ) coresets we get more compact representations of the original dataset , and we can still obtain the maximal graphs , but we cannot completely recover all frequent closed graphs of minimum support s .
5 . ESTIMATING FREQUENCIES
ADAPTIVELY
To deal with concept drift , our methods have to be able to adapt to changes in the input distribution . Keeping a sliding window of recent elements is an option , however , it has the cost of maintaining it in memory . In this paper , we propose to use ADWIN to estimate frequencies of graphs with theoretical guarantees .
ADWIN ( ADaptive sliding WINdow ) [ 7 ] is a change detector and estimation algorithm . It solves , in a well specified way , the problem of tracking the average of a stream of bits or real valued numbers . ADWIN keeps a variable length window of recently seen items , with the property that the window has the maximal length statistically consistent with the hypothesis “ there has been no change in the average value inside the window ” .
More precisely , an older fragment of the window is dropped if and only if there is enough evidence that its average value differs from that of the rest of the window . This has two consequences : one , change is reliably detected whenever the window shrinks ; and two , at any time the average over the existing window can be used as a reliable estimate of the current average in the stream ( barring a very small or recent change that is not yet statistically significant ) . The inputs to ADWIN are a confidence value δ ∈ ( 0 , 1 ) and a ( possibly infinite ) sequence of real values x1 , x2 , x3 , . . . , xt , . . . The value of xt is available only at time t . Each xt is generated according to some distribution Dt , independently for every t . We denote with µt the expected value of xt when it is drawn according to Dt . We assume that xt is always in [ 0 , 1 ] ; rescaling deals with cases where a ≤ xt ≤ b . No further assumption is being made about the distribution Dt ; in particular , µt is unknown for all t .
ADWIN is parameter and assumption free in the sense that it automatically detects and adapts to the current rate of change . Its only parameter is a confidence bound δ , indicating how confident we want to be in the algorithm ’s output , inherent to all algorithms dealing with random processes .
It is important to note that ADWIN does not maintain the window explicitly , but compresses it using a variant of the exponential histogram technique [ 20 ] . This means that it keeps a window of length W using only O(log W ) memory and O(log W ) processing time per item , rather than the O(W ) one expects from a na¨ıve implementation .
The main technical result in [ 7 ] about the performance of ADWIN is the following theorem , that provides bounds on the rate of false positives and false negatives :
Theorem 1 . With cut defined as cut =
· ln
1 2m
4|W| δ where for some partition of W in two parts W0W1 ( where W1 contains the most recent items ) , and m is the harmonic mean of |W0| and |W1| , at every time step we have :
1 . ( False positive rate bound ) . If µt has remained constant within W , the probability that ADWIN shrinks the window at this step is at most δ .
2 . ( False negative rate bound ) . Suppose that for some partition of W in two parts W0W1 we have |µW0 − µW1| > 2 cut . Then with probability 1−δ ADWIN shrinks W to W1 , or shorter .
6 . FREQUENT GRAPH MINING ON DATA
STREAMS
In this section we present new graph mining methods for data streams using the results discussed in previous sections . We start by reviewing non incremental basic methods , and then present the incremental miner IncGraphMiner , the sliding window based miner WinGraphMiner , and the adaptive miner AdaGraphMiner . 6.1 Non Incremental Graph Mining
[ 29 , 30 ] presents two algorithms for computing frequent and closed graphs from a dataset of graphs , in a non incremental way . They represent the potential subgraphs to be checked for , being frequent and closed on the dataset , in such a way that extending them by one single node , in all possible ways , corresponds to a clear and simple operation on the representation . The completeness of the procedure is assured , that is , all graphs can be obtained in this way . This allows them to avoid extending graphs that have already been found to be infrequent .
The pseudocode of CloseGraph is presented in Figure 1 . Note that the first line of the algorithm is a canonical representative check . Such checks are used frequently in tree and graph mining . The use of the right most extension approach in CloseGraph , based on depth first search , guarantees that all possible graphs are reached , but reduces the generation of duplicate graphs . CloseGraph selects the minimum DFS code based on a DFS lexicographical order as the canonical representative .
In MoSS [ 12 ] , Borgelt et al . present a different method to perform frequent closed graph mining using breadth first search instead of using the right most extension approach .
Note that using this methodology we can transform nonincremental methods into incremental ones by extending them to use weights and coresets of graphs . Figure 2 shows the pseudocode of IncGraphMiner .
CloseGraph(g , D , min sup , S )
Input : A graph g , a graph dataset D , min sup . Output : The frequent graph set S .
1 if g = Canonical Representative(g ) 2 then return S 3 isClosed ← true 4 C ← ∅ 5 for each g that is a one step right most extension of g do if support(g ) ≥ min sup then insert g into C if support(g ) = support(g ) then isClosed ← false
6 7 8 9 10 if isClosed = true 11 12 for each g in C 13 14 return S then insert g into S do S ← CloseGraph(g , D , min sup , S )
Figure 1 : The CloseGraph algorithm
IncGraphMiner(D , min sup )
Input : A graph dataset D , and min sup . Output : The frequent graph set G .
1 G ← ∅ 2 for every batch bt of graphs in D 3 4 5 return G do C ← Coreset(bt , min sup )
G ← Coreset(G ∪ C , min sup )
Coreset(bt , min sup )
Input : A graph dataset bt , and min sup . Output : The coreset C .
1 C ← CloseGraph(bt , min sup ) 2 C ← Compute Relative Support(C ) 3 return C
Figure 2 : The IncGraphMiner algorithm
6.2
Incremental Mining
The incremental setting is different . Suppose that the data arrives in batches of graphs . We consider every batch of graphs bt as a small finite dataset Db of transactions , where each transaction s ∈ Db consists of a transaction identifier , tid , a graph and a weight value . We maintain a set of graphs G and we update this set every time a new batch of graphs Db arrives . We compute the coreset of the batch bt , and then we use it to update the graph set G .
WinGraphMiner(D , W , min sup )
Input : A graph dataset D , a size window W and min sup . Output : The frequent graph set G .
1 G ← ∅ 2 for every batch bt of graphs in D 3 4 5 6 do C ← Coreset(bt , min sup ) Store C in sliding window if sliding window is full then R ← Oldest C stored in sliding window , else R ← ∅ negate all support values G ← Coreset(G ∪ C ∪ R , min sup )
7 8 9 return G
Figure 3 : The WinGraphMiner algorithm
6.3 Erroneous omission or inclusion bounds
When working on batches of graphs at a time , there is a possibility that the current batch does not contain enough occurrences of a frequent pattern , and it might also contain more than min sup occurrences of an otherwise infrequent pattern . The probabilities for these two types of errors , erroneous omission or inclusion , can be bounded in the following way . Given a fixed batch size n , every min sup value is equivalent to a probability p of finding a specific pattern in any given graph . Then , if the global true probability for some pattern is p + ∆ , and if we approximate the binomial distribution with the normal distribution N ( n ∗ ( p + ∆ ) , n ∗ ( p + ∆ ) ∗ ( 1 − p − ∆ ) , the following bound can be derived :
−2∆
√
−∞ pdiscard(∆ , n ) ≤
·
1√ 2π n
− t2 e
2 dt
( 1 )
If ∆ = 0 , then half of the patterns will be missed , but reasonably small values for ∆ will already result in acceptable probabilities : if n = 10000 , then ∆ = 0.01 results in less than 5 % erroneous omissions or inclusions , and for ∆ = 0.02 this percentage is already very close to zero .
This type of argument is a heuristic that can be made formal ( at the expense of worse figures ) by the use of rigorous Hoeffding bounds , which underlie eg the ADWIN algorithm . 6.4 Mining with a Sliding Window
We present WinGraphMiner as a learner that maintains a sliding window . Its main important parameter is the size of the window W . The difference to IncGraphMiner lies in the management of the items in the sliding window . We update the recent frequent closed graphs , using the coresets of the new batches that arrive . When the window is full , we delete the oldest batch on the sliding window using negative relative support ( Proposition 5 ) .
Figure 3 shows the pseudocode of WinGraphMiner . When a new batch bt arrives , the method performs as IncGraphMiner , while the sliding window is not full . Once the win dow is full , the oldest coreset C is removed . The new coreset for G is computed in one step from the coresets of G and bt , and C with negated relative support .
AdaGraphMiner(D , M ode , min sup )
Input : A graph dataset D , mode M ode and min sup . Output : The frequent graph set G . do C ← Coreset(bt , min sup )
1 G ← ∅ 2 Init ADWIN 3 for every batch bt of graphs in D 4 5 6 7 8 9
R ← ∅ if Mode is Sliding Window then Store C in sliding window if ADWIN detected change then R ← Batches to remove in sliding window with negative support
G ← Coreset(G ∪ C ∪ R , min sup ) if Mode is Sliding Window then Insert # closed graphs into ADWIN else for every g in G update g ’s ADWIN
10 11 12 13 14 return G
Figure 4 : The AdaGraphMiner algorithm
6.5 Adaptive Mining
Finally , we present AdaGraphMiner , an extension to the previous methods that is able to adapt to changes on the stream , maintaining only the currently frequent closed graphs .
We present two versions of this method . One uses an adaptive sliding window to maintain the batches of graphs . The other uses a separate ADWIN instance for managing the support of every single frequent closed subgraph .
Figure 4 shows the pseudocode of AdaGraphMiner , where the Sliding Window test distinguishes between the two methods .
Assume a scenario where at each time t the t th element of the stream is a graph taken independently from a distribution Dt over a fixed universe G of graphs . Then , changes in the Dt ’s over time t are modelling the evolution in the stream . In particular , we say that “ the stream is stationary between times t1 and t2 ” ( for t1 < t2 ) if
Dt1 = Dt1+1 = ··· = Dt2−1 = ··· = Dt2 .
For a graph g , let ft(g ) be the probability of g under distribution Dt , and ˆft(g ) the observed probability of g at time t ( the number of times g has appeared in the window at time t divided by the length of the window ) . Since ˆft(g ) is our estimate of the current support of t , we would like it to be close enough to ft(g ) to reliably decide whether g is frequent .
We can prove the following theorem :
Theorem 2 . Suppose the ADWIN ’s in Algorithm AdaGraph
Miner are initialized with parameter δ . For every time step t1 , t2 such that t1 < t2 , if the stream is stationary between t1 and t2 , then for every g ∈ G we have with probability at least 1 − 2δ
1
|ft2(g ) − ˆft2(g)| ≤ t2 − t1 ln
4W δ
√
That is , if the stream remains stationary for T steps , the estimates of graph frequencies tend to the their value at T ) , which tends to 0 as T → ∞ . a rate of roughly O(1/ Observe that this is the best convergence rate obtainable by sampling even on a totally stationary stream . That is , the algorithm is able to effectively forget previous history , and converge at an optimal rate on stationary periods . Note that , if instead of ADWIN , we keep a simple sliding window of fixed size W the approximation will not tend to zero , W . The dependence on ln(W ) in but remain at about 1/ the theorem is to a large extent an artifact of the proof of ADWIN ’s guarantees , and in practice , slightly increasing the leading constant in the bound suffices for accurate results . Proof . ( Note : this proof uses both the definition of the ADWIN algorithm as well as Theorem 1 stating false positive and false negative ratios ; see [ 7] ) . Fix t1 < t2 . We will show the following statement : For every fixed graph g ∈ G , with probability at least 1 − 2δ we have
√
4W δ
|ft2(g ) − ˆft2(g)| ≤
. ln t2 − t1
( 2 ) Now also fix a graph g ∈ G . If g is not present in the set G kept by the algorithm , then ˆft2(g ) = 0 and an application of the Hoeffding bound shows that ( 2 ) holds for g . Otherwise , g is in the set G , so it has an associated instance of ADWIN , A . Say that , at time t2 , A keeps a window of size W . Because the stream is stationary between t1 and t2 , by Theorem 1 , part ( 1 ) , we know that W ≥ t2−t1 ( ie , the element gathered at t1 has not been cut out from A ’s window ) with probability at least 1 − δ . Let t0 be the oldest element present in the window , so that t0 = t2 − W ≤ t1 . Now let ˆµ0 ( resp . , ˆµ1 ) be the observed frequence of g in the window [ t0t1−1 ] ( resp . , in the window [ t1t2] ) ; observe that E[µ1 ] = ft2(g ) . Observe also that by the definition of ADWIN , if the interval [ t0t1−1 ] has not been cut out from the window , we must have |ˆµ0 − ˆµ1| ≤ cut , where
1
1 cut =
1 2 t2 − t1
+
1 t1 − t0 ln
4W δ
.
The estimate produced by A at time t2 is the average of the elements in the window of length W , which is
ˆft2(g ) = ˆµ1 · t2 − t1 = ˆµ1 · t2 − t1 = ˆµ1 ± cut · t1 − t0
W
W
+ ˆµ0 · t1 − t0 + ( ˆµ1 ± cut ) · t1 − t0
W
W
.
W
Now using W = t2− t0 and some simple algebra one can see cut · t1 − t0
W
≤
≤ t1 − t0
2(t2 − t1)(t2 − t0 ) ln
4W δ
1
2(t2 − t1 ) ln
4W δ
.
Figure 5 : Memory and time used on Open NCI database with minimal support of 40 % .
Figure 6 : Memory and time used on ChemDB dataset with minimal support of 40 % . and by the Hoeffding bound and E[µ1 ] = ft2(g ) we have
|ˆµ1 − ft2(g)| ≤
Pr
1
2(t2 − t1 ) ln
2 δ
≤ δ .
Putting all of this together we conclude that , with probability at least 1 − 2δ ,
|ft2(g ) − ˆft2(g)| ≤
≤
1
1
1
2(t2 − t1 )
2(t2 − t1 ) t2 − t1 ln
4W δ
. ln
4W δ
+ ln
2 δ as was to be proved .
2
7 . EXPERIMENTAL EVALUATION benefits in memory and time . Second , we perform experiments on data streams with synthetic concept drift to show the performance of our adaptive strategy .
We run our experiments extending MOA [ 10 ] using MoSS [ 12 ] .
All experiments were performed on a 2.66 GHz Core 2 Duo machine with 64 GB of memory main memory , running CentOS 55
Massive Online Analysis ( MOA ) [ 10 ] is a framework for online learning from continuous supplies of examples , such as data streams . It comprises online classification and clustering methods as well as tools for both offline and online evaluation .
MoSS is a batch framework for finding frequent molecular substructures and discriminative fragments in a database of molecule descriptions . MoSS is not restricted to molecular data sets , it can mine arbitrary data sets of attributed graphs . Apart from the default MoSS/MoFa algorithm , this framework also contains the gSpan [ 29 ] and CloseGraph [ 30 ] algorithms as special processing modes .
For our experiments we use the following real datasets :
We tested our algorithms on synthetic and real datasets . First , we compare our new incremental strategy with closeGraph and MoSS as non incremental methods to show the
ChemDB dataset ChemDB [ 14 , 15 ] is a public dataset of approximately 4 million molecules built using the digital catalogs of over a hundred vendors and other public
Memory NCI Dataset01000200030004000500060007000800090001000030000500007000090000110000130000150000170000190000210000230000250000InstancesMegabytesIncGraphMinerIncGraphMiner CMoSScloseGraphTime NCI Dataset0204060801001201000030000500007000090000110000130000150000170000190000210000230000250000InstancesSecondsIncGraphMinerIncGraphMiner CMoSScloseGraphMemory ChemDB Dataset05000100001500020000250003000035000400004500050000100002400004700007000009300001160000139000016200001850000208000023100002540000277000030000003230000346000036900003920000InstancesMegabytesIncGraphMinerIncGraphMiner CMoSScloseGraphTime ChemDB Dataset0500100015002000250030003500400045005000100002400004700007000009300001160000139000016200001850000208000023100002540000277000030000003230000346000036900003920000InstancesSecondsIncGraphMinerIncGraphMiner CMoSScloseGraph Figure 7 : Number of graphs in ( 40 % , δ ) for NCI . sources . It is annotated with information derived from these sources as well as from computational methods , such as predicted solubility and 3D structure . It supports multiple molecular formats and is periodically updated , automatically whenever possible . It is maintained by the Institute for Genomics and Bioinformatics at the University of California , Irvine .
Open NCI Database The open National Cancer Institute ( NCI ) dataset [ 27 ] consists of approximately 250,000 structures . It is based on a large NCI database , built using samples from organic synthesis submitted to NCI for testing . While about half of the NCI database is not accessible , the other half of the structural data is free of any disclosure and usage restrictions and therefore termed “ open ” . This data is public domain and often referred to as the “ Open NCI Database ” .
First , we compare the runtime and memory usage of the incremental method IncGraphMiner to the non incremental ones . As all methods are implemented in Java , the memory shown is the memory allocated by the Java Virtual Machine . The results of our experiments using batches of 10 , 000 instances , minimum support of 40 % , δ = 0 , and 50 Gigabytes of maximum memory are shown in Figures 5 and 6 . We test two incremental methods , IncGraphMiner based on MoSS , and IncGraphMiner C based on closeGraph as batch learners . For the Open NCI database , which is the smaller dataset , we see that our new incremental methods need more time but less memory . However , for the larger ChemDB dataset , the non incremental methods can only process about 110,000 instances before running out of memory ( 50 Gigabytes ) . Surprisingly , the incremental methods also outperform the non incremental ones both in terms of runtime and memory usage for smaller subsets of ChemDB . Note that in Figure 6 there is a sudden bump in time used , since the graphs in this part of the stream are much larger . Figure 7 shows the size of the coreset ( 40 % , δ ) on the Open NCI database . For δ = 0 there are 649 frequent closed graphs , and for δ = 1 there are 140 frequent maximal graphs . The next experiment shows how the new methods are able to adapt to changes in the distribution of graphs in an evolving stream scenario . Figure 8 shows the number of closed
Figure 8 : Number of frequent closed graphs on Open NCI database with minimal support of 40 % with artificial concept drift . graphs detected using the incremental method IncGraphMiner , and the two adaptive methods . AdaGraphMinerWindow uses a sliding window monitored only by one ADWIN instance , and AdaGraphMiner uses one ADWIN instance for each graph to monitor its support . In this experiment , the data stream exhibits artificial concept drift , as it is a concatenation of parts of two different streams . One is the Open NCI database and the other is an artificial data stream with exactly 15 frequent closed graphs generated artificially . Concept drift occurs three times , after 250,000 , 500,000 , and 750,000 examples , respectively . We follow the methodology in [ 9 ] to combine two data streams into one in order to create artificial drift . We observe that the incremental method is the slowest to adapt , as it does not have any forgetting mechanism . Comparing the adaptive ones , we see that AdaGraphMiner Window , which only maintains one global ADWIN instance for change detection , shows a slower response to concept change . AdaGraphMiner , on the hand , can detect and respond to change faster , as it maintains a separate ADWIN instance for each graph .
8 . CONCLUSIONS
We have presented the first efficient algorithms for mining frequent closed graphs on evolving data streams .
If the distribution of the graph dataset is stationary , the best method to use is IncGraphMiner , as no past transactions need to be forgotten . If the distribution evolves , then a sliding window method is more appropriate . If the right size of the sliding window is known in advance , then WinGraphMineris the method of choice , otherwise AdaGraphMiner is the preferred option .
Future work will analyze the impact of the size of batches and apply these new adaptive frequent mining techniques to discriminative graph feature generation for classification and clustering . Another ambitious direction for future work would be going beyond batch incremental methods of graph mining , to develop methods that are fully instance incremental . It is currently unclear how this could be done efficiently .
01020304050601000060000110000160000210000260000310000360000410000460000510000560000610000660000710000760000810000860000910000960000InstancesNumber of Closed GraphsADAGRAPHMINERADAGRAPHMINER WindowIncGraphMiner 9 . ACKNOWLEDGMENTS
UPC research is partially supported by the Spanish Ministry of Science and Technology TIN 2008 06582 C03 01 ( SESAAME ) , by the Generalitat de Catalunya 2009 SGR1428 ( LARCA ) , and by the EU PASCAL2 Network of Excellence ( FP7 ICT 216886 ) .
The authors would like to thank Christian Borgelt for making his MoSS graph software available and for helpful discussions . 10 . REFERENCES [ 1 ] M . R . Ackermann , C . Lammersen , M . M¨artens ,
C . Raupach , C . Sohler , and K . Swierkot . StreamKM++ : A Clustering Algorithm for Data Streams . In Proceedings of the Workshop on Algorithm Engineering and Experiments ( ALENEX ’10 ) , 2010 .
[ 15 ] J . H . Chen , E . Linstead , S . J . Swamidass , D . Wang , and P . Baldi . ChemDB update full text search and virtual chemical space . Bioinformatics , 23(17):2348–2351 , September 2007 .
[ 16 ] J . Cheng , Y . Ke , W . Ng , and A . Lu . Fg index : towards verification free query processing on graph databases . In SIGMOD Conference , pages 857–872 , 2007 . [ 17 ] Y . Chi , H . Wang , P . S . Yu , and R . R . Muntz .
Moment : Maintaining closed frequent itemsets over a stream sliding window . In Proceedings of the 2004 IEEE International Conference on Data Mining ( ICDM’04 ) , November 2004 .
[ 18 ] Y . Chi , Y . Xia , Y . Yang , and R . Muntz . Mining closed and maximal frequent subtrees from databases of labeled rooted trees . Fundamenta Informaticae , XXI:1001–1038 , 2001 .
[ 2 ] P . K . Agarwal , S . Har Peled , and K . Varadarajan .
[ 19 ] D . J . Cook and L . B . Holder . Mining Graph Data .
Geometric approximation via coresets . In J . E . Goodman , J . Pach , and E . Welzl , editors , Combinatorial and computational geometry , pages 1–30 . Cambridge University Press . , 2005 .
[ 3 ] C . C . Aggarwal , Y . Li , P . S . Yu , and R . Jin . On dense pattern mining in graph streams . PVLDB , 3(1):975–984 , 2010 .
[ 4 ] C . C . Aggarwal and H . Wang . Managing and Mining
Graph Data . Springer Publishing Company , Incorporated , 1st edition , 2010 .
[ 5 ] H . Arimura and T . Uno . An output polynomial time algorithm for mining frequent closed attribute trees . In ILP , pages 1–19 , 2005 .
[ 6 ] J . L . Balc´azar , A . Bifet , and A . Lozano . Mining frequent closed rooted trees . Machine Learning , 78(1 2):1–33 , 2010 .
[ 7 ] A . Bifet and R . Gavald`a . Learning from time changing data with adaptive windowing . In SIAM International Conference on Data Mining , 2007 .
[ 8 ] A . Bifet and R . Gavald`a . Mining adaptively frequent closed unlabeled rooted trees in data streams . In 14th ACM SIGKDD , pages 34–42 , 2008 .
[ 9 ] A . Bifet , G . Holmes , B . Pfahringer , R . Kirkby , and
R . Gavald`a . New ensemble methods for evolving data streams . In 15th ACM SIGKDD , pages 139–148 , 2009 .
[ 10 ] A . Bifet , G . Holmes , B . Pfahringer , P . Kranen ,
H . Kremer , T . Jansen , and T . Seidl . MOA : Massive Online Analysis , a Framework for Stream Classification and Clustering . Journal of Machine Learning Research Proceedings Track , 11:44–50 , 2010 .
[ 11 ] C . Borgelt and M . R . Berthold . Mining molecular fragments : Finding relevant substructures of molecules . In ICDM ’02 , pages 51– , Washington , DC , USA , 2002 . IEEE Computer Society .
[ 12 ] C . Borgelt , T . Meinl , and M . Berthold . Moss : a program for molecular substructure mining . In OSDM ’05 , pages 6–15 , New York , NY , USA , 2005 . ACM .
[ 13 ] C . Borgelt , T . Meinl , and M . R . Berthold . Advanced pruning strategies to speed up mining closed molecular fragments . In IEEE Conf . on Systems , Man and Cybernetics ( SMC 2004 ) . IEEE Press , 2004 .
[ 14 ] J . Chen , S . J . J . Swamidass , Y . Dou , J . Bruand , and
P . Baldi . ChemDB : a public database of small molecules and related chemoinformatics resources . Bioinformatics , September 2005 .
Wiley Interscience , 2006 .
[ 20 ] M . Datar , A . Gionis , P . Indyk , and R . Motwani .
Maintaining stream statistics over sliding windows . SIAM Journal on Computing , 14(1):27–45 , 2002 . [ 21 ] J . Han , H . Cheng , D . Xin , and X . Yan . Frequent pattern mining : current status and future directions . Data Mining and Knowledge Discovery , 15:55–86 , 2007 .
[ 22 ] Y . K . James Cheng and W . Ng . Maintaining frequent closed itemsets over a sliding window . Journal of Intelligent Information Systems , 2007 .
[ 23 ] Y . K . James Cheng and W . Ng . A survey on algorithms for mining frequent itemsets over data streams . Knowledge and Information Systems , 2007 .
[ 24 ] N . Jiang and L . Gruenwald . Cfi stream : mining closed frequent itemsets in data streams . In KDD ’06 , pages 592–597 , New York , NY , USA , 2006 . ACM .
[ 25 ] I . Takigawa and H . Mamitsuka . Efficiently mining
δ tolerance closed frequent subgraphs . Machine Learning , 82(2):95–121 , 2011 .
[ 26 ] A . Termier , M C Rousset , M . Sebag , K . Ohara ,
T . Washio , and H . Motoda . DryadeParent , an efficient and robust closed attribute tree mining algorithm . IEEE Trans . Knowl . Data Eng . , 20(3):300–320 , 2008 . [ 27 ] J . H . Voigt , B . Bienfait , S . Wang , and M . C . Nicklaus . Comparison of the nci open database with seven large chemical structural databases . Journal of Chemical Information and Computer Sciences , 41(3):702–712 , 2001 .
[ 28 ] J . Wang and J . Han . Bide : Efficient mining of frequent closed sequences . In ICDE , pages 79–90 . IEEE Computer Society , 2004 .
[ 29 ] X . Yan and J . Han . gSpan : Graph based substructure pattern mining . In ICDM ’02 , page 721 , Washington , DC , USA , 2002 . IEEE Computer Society .
[ 30 ] X . Yan and J . Han . CloseGraph : mining closed frequent graph patterns . In KDD ’03 , pages 286–295 , 2003 .
[ 31 ] X . Yan , J . Han , and R . Afshar . Clospan : Mining closed sequential patterns in large databases . In SDM , 2003 .
