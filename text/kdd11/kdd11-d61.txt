Fast Coordinate Descent Methods with Variable Selection for Non negative Matrix Factorization
Cho Jui Hsieh
Dept . of Computer Science University of Texas at Austin Austin , TX 78712 1188 , USA cjhsieh@csutexasedu
Inderjit S . Dhillon
Dept . of Computer Science University of Texas at Austin Austin , TX 78712 1188 , USA inderjit@csutexasedu
ABSTRACT Nonnegative Matrix Factorization ( NMF ) is an effective dimension reduction method for non negative dyadic data , and has proven to be useful in many areas , such as text mining , bioinformatics and image processing . NMF is usually formulated as a constrained non convex optimization problem , and many algorithms have been developed for solving it . Recently , a coordinate descent method , called FastHals [ 3 ] , has been proposed to solve least squares NMF and is regarded as one of the state of the art techniques for the problem . In this paper , we first show that FastHals has an inefficiency in that it uses a cyclic coordinate descent scheme and thus , performs unneeded descent steps on unimportant variables . We then present a variable selection scheme that uses the gradient of the objective function to arrive at a new coordinate descent method . Our new method is considerably faster in practice and we show that it has theoretical convergence guarantees . Moreover when the solution is sparse , as is often the case in real applications , our new method benefits by selecting important variables to update more often , thus resulting in higher speed . As an example , on a text dataset RCV1 , our method is 7 times faster than FastHals , and more than 15 times faster when the sparsity is increased by adding an L1 penalty . We also develop new coordinate descent methods when error in NMF is measured by KLdivergence by applying the Newton method to solve the one variable sub problems . Experiments indicate that our algorithm for minimizing the KL divergence is faster than the Lee & Seung multiplicative rule by a factor of 10 on the CBCL image dataset .
Categories and Subject Descriptors I26 [ Artificial Intelligence ] : Learning ; G16 [ Numerical Analysis ] : Optimization
General Terms Algorithms , Performance , Experimentation
1 .
INTRODUCTION
Non negative matrix factorization ( NMF ) ( [20 , 14 ] ) is a popular matrix decomposition method for finding non negative representations of data . NMF has proved useful in dimension reduction of image , text , and signal data . Given a matrix V ∈ Rm×n , V ≥ 0 , and a specified positive integer k , NMF seeks to approximate V by the product of two nonnegative matrices W ∈ Rm×k and H ∈ Rk×n . Suppose each column of V is an input data vector , the main idea behind NMF is to approximate these input vectors by nonnegative linear combinations of nonnegative “ basis ” vectors ( columns of W ) with the coefficients stored in columns of H .
The distance between V and W H can be measured by various distortion functions — the most commonly used is the Frobenius norm , which leads to the following minimization problem : min
W,H≥0 f ( W , H ) ≡
1 2 kV −W Hk2
F =
( Vij −(W H)ij)2 ( 1 )
1
2 Xi,j
To achieve better sparsity , researchers ( [7 , 22 ] ) have proposed adding regularization terms , on W and H , to ( 1 ) . For example , an L1 norm penalty on W and H can achieve a more sparse solution : min
W,H≥0
1 2 kV − W Hk2
F + ρ1Xi,r
Wir + ρ2Xr,j
Hrj .
( 2 )
One can also replace Pi,r Wir and Pr,j Hrj by Frobenius norm of W and H . Note that usually m ≫ k and n ≫ k . Since f is non convex , we can only hope to find a stationary point of f . Many algorithms ( [15 , 5 , 1 , 23 ] ) have been proposed for this purpose . Among them , Lee & Seung ’s multiplicative update algorithm [ 15 ] has been the most common method . Recent methods follow the Alternating Nonnegative Least Squares ( ANLS ) framework suggested by [ 20 ] . This framework has been proved to converge to stationary points by [ 6 ] as long as one can solve each nonnegative least square sub problem exactly . Among existing methods , [ 18 ] proposes a projected gradient method , [ 11 ] uses an active set method , [ 10 ] applies a projected Newton method , and [ 12 ] suggests a modified active set method called BlockPivot for solving the non negative least squares sub problem . Very recently , [ 19 ] developed a parallel NMF solver for largescale web data . This shows a practical need for further scaling up NMF solvers , especially when the matrix V is sparse , as is the case for text data or web recommendation data .
Coordinate descent methods , which update one variable at a time , are efficient at finding a reasonable solution efficiently , a property that has made these methods successful in large scale classification/regression problems . Recently , a coordinate descent method , called FastHals [ 3 ] , has been proposed to solve the least squares NMF problem ( 1 ) . Compared to methods using the ANLS framework which spend significant time in finding an exact solution for each subproblem , a coordinate descent method can efficiently give a reasonably good solution for each sub problem and switch to the next round . Despite being a state of the art method , FastHals has an inefficiency in that it uses a cyclic coordinate descent scheme and thus , may perform unneeded descent steps on unimportant variables . In this paper , we present a variable selection scheme that uses the gradient of the objective function to arrive at a new coordinate descent method . Our method is considerably faster in practice and we show that it has theoretical convergence guarantees , which was not addressed in [ 3 ] . We conduct experiments on large sparse datasets , and show that our method generally performs 2 3 times faster than FastHals . Moreover when the solution is sparse , as is often the case in real applications , our new method benefits by selecting important variables to update more often , thus resulting in higher speed . As an example , on a text dataset RCV1 , our method is 7 times faster than FastHals , and more than 15 times faster when the sparsity is increased by adding an L1 penalty .
Another popular distortion measure to minimize is the
KL divergence between V and W H : Vij min
W,H≥0
L(W , H ) =Xi,j
Vij log(
)−Vij +(W H)ij ( 3 )
( W H)ij
The above problem has been shown to be connected to Probabilistic Latent Semantic Analysis ( PLSA ) [ 4 ] . As we will point out in Section 3 , problem ( 3 ) is more complicated than ( 1 ) and algorithms in the ANLS framework cannot be easily extended to solve ( 3 ) . For this problem , [ 3 ] proposes to solve by a sequence of local loss functions — however , a drawback of the method in [ 3 ] is that it will not converge to a stationary point of ( 3 ) . In this paper we propose a cyclic coordinate descent method for solving ( 3 ) . Our new algorithm applies the Newton method to get a better solution for each variable , which is better compared to working on the auxiliary function as used in the multiplicative algorithm of [ 15 ] . As a result , our cyclic coordinate descent method converges much faster than the multiplicative algorithm . We further provide theoretical convergence guarantees for cyclic coordinate descent methods under certain conditions .
The paper is organized as follows . In Section 2 , we describe our coordinate descent method for least squares NMF ( 1 ) . The modification for the coordinate descent method for solving KL NMF ( 3 ) is proposed in Section 3 . Section 4 discusses the relationship between our methods and other optimization techniques for NMF , and Section 5 points out some important implementation issues . The experiment results are given in Section 6 . The results show that our methods are more efficient than other state of art methods . 2 . COORDINATE DESCENT METHOD FOR
LEAST SQUARES NMF
2.1 Basic update rule for one element
The coordinate descent method updates one variable at a time until convergence . We first show that a closed form update can be performed in O(1 ) time for least squares NMF if certain quantities have been pre computed . Note that in this section we focus on the update rule for variables in W — the update rule for variables in H can be derived similarly .
Coordinate descent methods aim to conduct the following one variable updates :
( W , H ) ← ( W + sEir , H ) where Eir is a m×k matrix with all elements zero except the ( i , r ) element which equals one . Coordinate descent solves the following one variable subproblem of ( 1 ) to get s : min s:Wir +s≥0 gW ir ( s ) ≡ f ( W + sEir , H ) .
( 4 )
We can rewrite gW ir ( s ) as 1 gW ir ( s ) =
2 Xj
( Vij − ( W H)ij − sHrj)2
= gW ir ( 0 ) + ( gW ir )′(0)s +
1 2
( gW ir )′′(0)s2 .
( 5 )
For convenience we define
GW ≡ ∇W f ( W , H ) = W HH T − V H T . and GH ≡ ∇H f ( W , H ) = W T W H − W T V .
( 6 )
( 7 )
Then we have
( gW ir )′(0 ) = ( GW )ir = ( W HH T − V H T )ir and ( gW ir )′′(0 ) = ( HH T )rr .
Since ( 5 ) is a one variable quadratic function with the constraint Wir + s ≥ 0 , we can solve it in closed form : s∗= max “ 0 , Wir −(W HH T −V H T )ir/(HH T )rr ” −Wir . ( 8 )
Coordinate descent algorithms then update Wir to
Wir ← Wir + s∗ .
( 9 )
The above one variable update rule can be easily extended to NMF problems with regularization as long as the gradient and second order derivative of sub problems can be easily computed . For instance , with the L1 penalty on both W and H as in ( 2 ) , the one variable sub problem is
1 2
¯gW and thus the one variable update becomes ir ( s ) =„(W HH T −V H T )ir+ρ1«s+ Wir ← max “ 0 , Wir − ( (W HH T − V H T )ir + ρ1)/(HH T )rr ” .
( HH T )rrs2+¯gW
Compared to ( 8 ) , the new Wir is shifted left to zero by , so the resulting Wir generally has more the factor zeros .
( HH T )rr ir ( 0 ) ,
ρ1
A state of the art cyclic coordinate descent method called FastHals is proposed in [ 3 ] , which uses the above one variable update rules . 2.2 Variable selection strategy
With the one variable update rule , FastHals [ 3 ] conducts a cyclic coordinate descent . It first updates all variables in W in cyclic order , and then updates variables in H , and so on . Thus the number of updates performed for each variable is exactly the same . However , an efficient coordinate descent method should update variables with frequency proportional to their “ importance ” . In this paper , we exhibit a carefully chosen variable selection scheme ( with not too much overhead ) to obtain a new coordinate descent algorithm that has the ability to select variables according to their “ importance ” , thus leading to significant speedups . Similar strategies has been employed in Lasso or feature selection problems [ 17 , 21 ] . To illustrate the idea , we take a text dataset x 104
7.8
7.6
7.4
7.2
7
6.8
6.6
6.4 l e u a V e v i t c e b O j
6.2
0
2
4
Number of Coordinate Updates
10 x 107 ( a ) Coordinate updates versus objective value
6
8
CD FastHals x 10−3 5 s e t a d p u f o r e b m u n
5
4
3
2
1
0 0 0 n o i t l u o s e h t n i s e u a v l
4
3
2
1
0
0.5 0.5
1 1 variables in H
1.5 1.5
2 2 x 106 x 106
( b ) The behavior of FastHals
( c ) The behavior of GCD
Figure 1 : Illustration of our variable selection scheme . Figure 1(a ) shows that our method GCD reduces the objective value more quickly than FastHals . With the same number of coordinate updates ( as specified by the vertical dotted line in Figure 1(a) ) , we further compare the distribution of their coordinate updates . In Figure 1(b ) and 1(c ) , the X axis is the variables of H listed by descending order of their final values . The solid line gives their final values , and the light blue bars indicate the number of times they are chosen . The figures indicate that FastHals updates all variables uniformly , while the number of updates for GCD is proportional to their final values , which helps GCD to converge faster .
RCV1 with L1 penalty as an example . In Figures 1(b ) and 1(c ) the variables of the final solution H are listed on the X axis — note that the solution is sparse as most of the variables are 0 . Figure 1(b ) shows the behavior of FastHals , which clearly shows that each variable is chosen uniformly . In contrast , as shown in Figure 1(c ) , by applying our new coordinate descent method , the number of updates for the variable is roughly proportional to their final values . For most variables with final value 0 , our algorithm will never pick them to update . Therefore our new method focuses on nonzero variables and reduces the objective value more efficiently . Figure 1(a ) shows that we can attain a 10 fold speedup by applying our variable selection scheme .
We now show that with an appropriate data structure , we can select variables that lead to maximum decrease of objective function in O(k +log m ) time with a pre computed gradient . However , O(log m ) can be expensive in practice , so we further provide an alternate efficient row based variable selection scheme that needs O(k ) time per update .
Before presenting our variable selection method , we first introduce our framework . Similar to FastHals , our algorithm switches between W and H in the outer iterations : ( W 0 , H 0 ) → ( W 1 , H 0 ) → ( W 1 , H 1 ) → · · ·
( 10 ) Between each outer iteration are the following inner updates :
( W i , H i ) → ( W i,1 , H i ) → ( W i,2 , H i ) · · · .
( 11 ) Later we will discuss the reason why we have to focus on W or H for a sequence of updates .
Suppose we want to choose variables to update in W . If Wir is selected , as discussed in Section 2.1 , the optimal update will be ( 8 ) , and the function value will be decreased by
DW ir ≡ gW ir ( 0 ) − gW ir ( s∗ ) = −GW ir s∗ −
( HH T )rr(s∗)2 . ( 12 )
1 2
DW ir measures how much we can reduce the objective value by choosing the coordinate Wir . Therefore , if DW can be maintained , we can greedily choose variables according to it . If we have GW , we can compute s∗ by ( 8 ) and compute DW by ( 12 ) , and so an element of DW can be computed in O(1 ) time . At the beginning of a sequence of W ’s updates , we can precompute GW . The details will be provided in Section 23 Now assume we already have GW , and Wir is updated to Wir + s∗ . Then GW will remain the same except for the ith row , which will be replaced by
GW ij ← GW ij + s∗(HH T )rj ∀j = 1 , . . . , k .
( 13 ) Using ( 13 ) , we can maintain GW in O(k ) time after each variable update of W . Thus DW can also be maintained in O(k ) time .
However , maintaining GH at each inner update of W is more expensive . From ( 7 ) , when each element of W is changed , the whole matrix GH will be changed , thus every element of DH may also change . So the time cost for maintaining DH is at least O(kn ) , which is too much compared to O(k ) for maintaining DW . This is the reason that we follow the alternative minimization scheme and restrict ourselves to either W or H for a sequence of inner updates . After the i th row of GW is updated , we can immediately maintain the i th row of DW by ( 12 ) . To select the next variable to update , we want to select the index ( i∗ , r∗ ) that satisfies ( i∗ , r∗ ) = arg maxi,r DW ir . However , a brute force search through the whole matrix DW will require O(mk ) time . To overcome this , we can store the largest value vi and index qi for each row of DW , ie , qi = arg max j
DW ij , vi = DW i,qi .
( 14 )
As in ( 13 ) , when Wir is updated , only the ith row of GW and DW will change . Therefore the vector q will remain the same except for one component qi . Since the ith row of DW contains k elements and each element can be computed in constant time , we can recalculate qi in O(k ) time . Each time we only change the largest value in {qi | i = 1 , . . . , m} , therefore we can store these values using a heap data structure so that each retrieval and re calculation of the largest value can be done in O(log m ) time . This way the total cost for one update will be O(k + log m ) .
A stopping condition is needed for a sequence of updates .
At the beginning of updates to W , we can store pinit = max i,j
DW ij .
( 15 )
Our algorithm then iteratively chooses variables to update until the following stopping condition is met : max i,j
DW ij < ǫpinit ,
( 16 ) where ǫ is a small positive constant . Note that ( 16 ) will be satisfied in a finite number of iterations as f ( W , H ) is lower bounded , and so the minimum for f ( W , H ) with fixed H is achievable . A small ǫ value indicates each sub problem is solved to high accuracy , while a larger ǫ value means our algorithm switches more often between W and H . We choose ǫ = 0.001 in our experiments .
In practice , this method can work when k is large . However , when k ≪ m , the log m term in the update cost will dominate . Moreover , each heap operation costs more time than a simple floating point operation . Thus we further design a more efficient row based variable selection strategy .
First , we have an observation that when one Wir is updated , only the ith row of DW will be changed . Therefore if we update variables in the ith row and there is a variable in the jth row with larger potential decrease DW jr , we will jr . Choosing the largest DW value in one row not change DW costs O(k ) , which is cheaper than O(log m ) when k small . Therefore we can iteratively update variables in the ith row ( that lead to maximum decrease in objective function ) until the inner stopping condition is met : max j
DW ij < ǫpinit .
( 17 )
Our algorithm then update variables in the ( i+1) st row , and so on . Since changes in other rows will not affect the DW values in the ith row , after our algorithm sweeps through row 1 to row m , ( 17 ) will be met for each row , thus the stopping condition ( 16 ) will also be met for the whole W . 2.3 Time complexity analysis
Algorithm 1 GCD for least squares NMF
• Given : V , k , ǫ ( typically , ǫ = 0.001 ) • Output : W , H • Compute P V H = V H T , P HH = HH T , P W V =
W T V , P W W = W T W
• Initialize H new ← 0 • While ( not converged )
1 . Compute P V H ← P V H + V ( H new)T according to qi , : ← P W W qi , : +s∗Wqi , : ( Also do a sym metric update for P W W :,qi )
73 W new 74 GW 75 SW i,qi ← W new i,qi + s∗ i , : ← GW i , : + s∗P HH qi , : ir ← max(Wir − GW ir P HH rr r = 1 , . . . , k .
, 0 ) − Wir for all
76 DW ir ← −GW ir SW ir − 1
2 P HH rr
( SW ir )2 for all r = 1 , . . . , k .
77 qi ← arg maxj DW ij .
8 . W ← W + W new 9 . For updates to H , repeats analogues steps to Step
1 through Step 8 .
Our coordinate descent method with variable selection strategy can be summarized in Algorithm 1 . We call our new coordinate descent algorithm GCD– Greedy Coordinate the sparsity of H new
2 . W new ← 0 3 . GW ← W P HH − P V H ir ← max(Wir − GW 4 . SW 5 . DW ir ← −GW ir − 1 6 . qi ← arg maxj DW ij ir SW rr pinit ← maxi DW i,qi 7 . For i = 1 , 2 , . . . , m
– While DW i,qi < ǫpinit
71 s∗ ← SW i,qi 72 P W W ir P HH 2 P HH
, 0 ) − Wir for all i , r . ir )2 for all i , r .
( SW rr for all i = 1 , . . . , m , and
Table 1 : Time complexity analysis for Algorithm 1 . We focus on time cost for one complete update of W . Here t is average number of inner updates , and s is number of nonzeros in V . dense V
Compute matrix V H T ( Step 1 ) min(O(mt ) , O(nmk ) ) O(mk2 ) gradient/decreasing
Initialize matrix ( Steps 3 to 6 ) Update inner product ( Steps 7.2 ) O(tk ) Update gradient and decreasing O(tk ) matrix ( Step 7.4 to 7.7 ) sparse V min(O( st n ) , O(sk ) ) O(mk2 )
O(tk ) O(tk )
Descent since we take a greedy step of maximum decrease in objective function . The one variable update and variable selection strategy are in the inner loop ( step 7 ) of Algorithm 1 . Before the inner loop , we have to initialize the gradient GW and objective function decrease matrix DW . GW in ( 6 ) has two terms W HH T and V H T ( in Algorithm 1 , we use P V H , P HH , P W V and P W W to store matrices V H T , HH T , W T V and W T W ) . For the first term , since HH T is maintained in an analogous way to step 7.2 ) , computing W ( HH T ) costs O(mk2 ) time . For the second term V H T , we can also store the updates in H new so that H new = H i+1 − H i , and then updates V H T by step 1 . Assume that , on average , step 7 has t coordinate updates , then the number of nonzeros in H new is at most t . With sparse matrix multiplication , step 1 costs O(mt ) . However , when H new is dense , we should use a dense matrix multiplication for step 1 , which costs O(nmk ) and is independent of t .
Table 1 summarizes the cost of one outer iteration ( step 1 to 9 ) . To get the amortized cost per coordinate update , we can divide the numbers by t . We first consider when V is dense . When t < k2 , O(mk2 ) will dominate the complexity . When k2 ≤ t ≤ nk , computational time for computing V H T dominates the complexity and the time cost per coordinate update is O(m ) . In this case , time cost per update is the same as FastHals . When t is larger than nk , the time cost per update will decrease . We summarize the amortized cost per update as below : if k2 > t if nk > t ≥ k2 if nm > t ≥ nk if t > nm
O( mk2 t ) O(m ) O( nmk t O(k )
)
8>>>< >>> :
When V is sparse , assume there are s nonzero elements in V , the complexity for the computing V H T is modified while others remain the same . 3 . COORDINATE DESCENT METHOD FOR
NMF WITH KL DIVERGENCE
To apply coordinate descent for solving NMF with KLdivergence ( 3 ) , we consider the one variable sub problem : hir(s ) = L(W + sEir , H )
( 18 )
= n
Xj=1
−Vij log„(W H)ij + sHrj« + sHrj + constant .
Here we discuss the updates to W , the update rules for H can be derived similarly . Unlike the one variable subproblem of least squares NMF ( 5 ) , minimizing hir(s ) has no closed form solution . Thus FastHals fails to derive a coordi nate descent method for KL divergence . Instead of solving the one variable sub problem hir(s ) , FastHals in [ 3 ] minimizes the following one variable problem for each update :
¯hir(s ) =Xj
−(Vij −Xt6=r
WitHtj ) log(sHrj ) + sHrj .
The above problem has a closed form solution , but the solution is different from the one variable sub problem ( 18 ) , thus FastHals solves a different problem and may converge to a different final solution . Therefore we can say that application of coordinate descent to solve NMF with KL divergence has not been studied before .
In this section , we propose the use of Newton ’s method to solve the sub problems . Since hir is twice differentiable , we can iteratively update s by Newton direction : s ← max(−Wir , s − h′ ir(s)/h′′ ir(s) ) ,
( 19 ) where the first term comes from the non negativity constraint . The first derivative h′ ir(s ) and the second derivative h′′ ir(s ) can be written as h′ ir(s ) = h′′ ir(s ) = n n
Xj=1 Xj=1
Vij
( W H)ij + sHrj« .
Hrj„1 − ( (W H)ij + sHrj)2 .
VijH 2 rj
( 20 )
( 21 )
For KL divergence , we need to take care of the behavior when Vij = 0 or ( W H)ij = 0 . For Vij = 0 , by analysing the asymptotic behavior , Vij log((W H)ij ) = 0 for all positive values of ( W H)ij , thus we can ignore those entries . Also , we do not consider the case that one row of V is entirely zero , which can be removed by preprocessing . When ( W H)ij + sHrj = 0 for some j , the Newton direction −h′ ir(s ) should be infinity . In this situation we reset s so that Wir +s is a small positive value and restart the Newton method . When Hrj = 0 for all j = 1 , . . . , n , the second derivative ( 21 ) is zero . In this case hir(s ) is constant , thus we do not need to change Wir . ir(s)/h′′
To execute Newton updates , each evaluation of h′ ir(s ) and h′′ ir(s ) takes O(n ) time for the summation . For general functions , we often need a line search procedure to check sufficient decrease from each Newton iteration . However , as we now deal with the special function ( 18 ) , we prove the following theorem to show that Newton method without line search converges to the optimum of each sub problem : Theorem 1 If a function f ( x ) with domain x ≥ 0 can be written in the following form l f ( x ) = −ci
Xi=1 log(ai + bix ) +Xj bix , where ai > 0 , bi , ci ≥ 0 ∀i , then the Newton method without line search converges to the global minimum of f ( x ) .
The proof based on the strict convexity of log function is in the Appendix of our technical report [ 9 ] . By the above Theorem , we can iteratively apply ( 19 ) until convergence . In practice , we design the following stopping condition for Newton method :
|st+1 − st| < ǫ|Wir + st| for some ǫ > 0 , where st is the current solution and st+1 is obtained by ( 19 ) . As discussed earlier , variable selection is an important issue for coordinate descent methods . After each inner update
Algorithm 2 CCD for NMF with KL divergence
1 . Given : V , k , W , H , ǫ ( typically , ǫ = 0.5 ) 2 . Output : W , H 3 . P W H ← W H . 4 . While ( not converged )
401 For i = 1 , . . . , m ( updates in W )
• For r = 1 , . . . , k
– While 1
∗ Compute s by ( 19 ) . ∗ wold = Wir . ∗ Wir ← Wir + s . ∗ Maintain ( W H)i , : by ( 22 ) . ∗ If |s| < ǫwold , Break
402 For updates to H , repeats steps analogous to Step 401
Wir ← Wir + s∗ , the gradient for hit(s ) will change for all t = 1 , . . . , k . As mentioned in Section 2 , for NMF with square loss we can update the gradient for one row of W in O(k ) time . However , there is no easy way to update the values together for KL divergence . To maintain the gradient , we need to update ( W H)ij for all j = 1 , . . . , n by
( W H)ij = ( W H)ij + s∗Hrj ∀j ,
( 22 ) and then recompute h′ it(0 ) for all t = 1 , . . . , k by ( 20 ) . Therefore maintaining h′ it(0 ) will take O(kn ) time . This is expensive compared to the time cost O(n ) for updating one variable , so we just update variables in a cyclic order . Thus our method for KL divergence is a Cyclic Coordinate Descent ( CCD ) . Notice that to distinguish our method for KL divergence with FastHals ( cyclic coordinate descent for least squares NMF ) , through this paper CCD indicates our method for KL divergence .
In summary , our algorithm chooses each variable in W once in a cyclic order , minimizing the corresponding onevariable sub problem , and then switches to H . Each Newton update takes O(n ) time , so each coordinate update costs O(n ¯d ) time where ¯d is the average number of Newton iterations . Algorithm 2 summarizes the details . 4 . CONVERGENCE PROPERTIES AND RE
LATIONS WITH OTHER METHODS
4.1 Methods for Least Squares NMF
For NMF , f ( W , H ) is convex in W or H but not simultaneously convex in both W and H , so it is natural to apply alternating minimization , which iteratively minimizes the following two problems min W ≥0 f ( W , H ) and min H≥0 f ( W , H )
( 23 ) until convergence . As mentioned in [ 10 ] , we can categorize NMF solvers into two groups : exact methods and inexact methods . Exact methods are guaranteed to achieve optimum for each sub problem ( 23 ) , while inexact methods only guarantee decrease in function value . Each sub problem for least squares NMF can be decomposed into a sequence of non negative least square ( NNLS ) problems . For example , minimization with respect to H can be decomposed into minhi≥0 kV −W hik2 , where hi is the ith column of H . Since the convergence property for exact methods has been proved in [ 6 ] , any NNLS solver can be applied to solve NMF in an alternating fashion . However , as mentioned before , very re cently an inexact method FastHals has been proposed proposed [ 3 ] . The success of FastHals shows that exactly solving sub problems may slow down convergence . This makes sense because when ( W , H ) is still far from optimum , there is no reason in paying too much effort for solving sub problems exactly . Since GCD does not solve sub problems exactly , it can avoid paying too much effort for each sub problem . On the other hand , unlike most inexact methods , GCD guarantees the quality of each updates by setting a stopping condition and , thus converges faster than inexact methods .
Moreover , most inexact methods do not have a theoretically convergence proof , thus the performance may not be stable . In contrast , we prove that GCD converges to a stationary point by the following theorem :
Theorem 2 For least squares NMF , if a sequence {(Wi , Hi)} is generated by GCD , then every limit point of this sequence is a stationary point .
The proof can be found in the appendix of our technical report [ 9 ] . This convergence result holds for any inner stopping condition ǫ < 1 , thus it is different from the proof for exact methods , which assumes that each sub problem is solved exactly . It is easy to extend the convergence result for GCD to regularized least squares NMF . 4.2 Methods for NMF with KL divergence
NMF with KL divergence is harder to solve compared to square loss . Assume an algorithm applies an iterative method to solve minW ≥0 f ( W , H ) , and needs to compute the gradient at each iteration . After computing the gradient ( 6 ) at the beginning , least squares NMF solvers can maintain the gradient in O(mk2 ) time when W is updated . So it can do many inner updates for W with comparatively less effort O(mk2 ) ≪ O(nmk ) . Almost all least squares NMF solvers take advantage of this fact . However , for KL divergence , the gradient ( 20 ) cannot be maintained within O(nmk ) time after each update of W , so the cost for each sub problem is O(nmkt ) where t is the number of inner iterations , which is large compared to O(nmk ) for square loss .
Our algorithm ( CCD ) spends O(nmk ¯d ) time where ¯d is the average number of Newton iterations , while the multiplicative algorithm spends O(nmk ) time . However , CCD has a better solution for each variable because we use a second order approximation of the actual function , which is better compared to working on the auxiliary function as used in multiplicative algorithm proposed by [ 15 ] . Experiments in Section 6 show CCD is much faster .
In addition to the practical comparison , the following theorem proves CCD converges to a stationary point under certain condition . FastHals for least squares NMF can also be covered by this theorem because it is also a cyclic coordinate descent method .
Theorem 3 For any limit points ( W ∗ , H ∗ ) of CCD ( or FastHals ) , assume w∗ r is the rth row of H ∗ , if ( 24 ) r is the rth column of W ∗ and h∗ rk > 0 ∀r = 1 , . . . , k , kw∗ r k > 0 , kh∗ then ( W ∗ , H ∗ ) is a stationary point of ( 3 ) ( or ( 1) ) .
With the condition ( 24 ) , the one variable sub problems for the convergence subsequence are strictly convex . Then the proof follows the proof of Proposition 339 in [ 2 ] . For KLNMF , ( 24 ) is violated only when corresponding row/columns of V are all zero ( otherwise the objective value will be infinity ) . Therefore , zero row/columns of V can be removed by preprocessing to ensure the convergence of CCD . For least squares NMF , usually ( 24 ) holds in practice so that FastHals also converges to a stationary point .
For CCD with regularization on both W and H , each onevariable sub problem becomes strictly quasiconvex , thus we can apply Proposition 5 in [ 6 ] to prove the following theorem :
Theorem 4 Any limit point ( W ∗ , H ∗ ) of CCD ( or FastHals ) is a stationary point of ( 3 ) ( or ( 1 ) ) with L1 or L2 regularization on both W and H . 5 . 5.1
IMPLEMENTATION ISSUES Implementation with MATLAB and C
It is well known that MATLAB is very slow in loop operations and thus , to implement GCD , the “ for loop ” in Step 7 of Algorithm 1 is slow in MATLAB . To have an efficient implementation , we transfer three matrices W , GW , HH T to C by MATLAB C interface in Step 7 . At the end of the loop , our C code returns W new back to the main MATLAB program . Although this implementation gives an overhead to transfer O(nk + mk ) size matrices , our algorithm still outperforms other methods in experiments . In the future , it is possible to directly use BLAS3 library to have a faster implementation in C . 5.2 Stopping Condition
The stopping condition is important for NMF solvers . Here , we adopt projected gradient as stopping condition as in [ 18 ] . The projected gradient for f ( W , H ) , ie , ∇P f ( W , H ) has two parts including ∇P H f ( W , H ) , where
W f ( W , H ) and ∇P
∇P
W f ( W , H)ir ≡( ∂
∂Wir min(0 , f ( W , H )
∂
∂Wir f ( W , H ) ) if Wir > 0 , if Wir = 0 .
( 25 )
∇P W f ( W , H ) can be defined in a similar way . According to the KKT condition , ( W ∗ , H ∗ ) is a stationary point if and only if ∇P f ( W ∗ , H ∗ ) = 0 , thus we can use ∇P f ( W ∗ , H ∗ ) to measure how close we are to a stationary point . We stop the algorithm after the norm of projected gradient satisfies the following stopping condition : k∇P f ( W , H)k2
F ≤ ǫk∇P f ( W 0 , H 0)k2 F , where W 0 and H 0 are initial points . 6 . EXPERIMENTS
In this section , we compare the performance of our algorithms with other NMF solvers . All sources used for our comparisons are available at http://wwwcsutexasedu/ ~cjhsieh/nmf . All the experiments were executed on 2.83 GHz Xeon X5440 machines with 32G RAM and Linux OS . 6.1 Comparison on dense datasets
For least squares NMF , we compare GCD with three other state of the art solvers :
1 . ProjGrad : the projected gradient method in [ 18 ] . We use the MATLAB source code at http://wwwcsie ntuedutw/~cjlin/nmf/
2 . BlockPivot : the block pivot method in [ 12 ] . We use the MATLAB source code at http://wwwccgatech edu/~hpark/nmfsoftwarephp
Table 2 : The comparisons for least squares NMF solvers on dense datasets . For each method we present time/FLOPs ( number of floating point operations ) cost to achieve the specified relative error . The method with the shortest running time is boldfaced . The results indicate that GCD is most efficient both in time and FLOPs . dataset m n k relative error
Synth03
500
1,000
Synth08
500
1,000
10 30 10 30
CBCL
361
2,429
49
ORL 10,304
400
25
10−4 10−4 10−4 10−4 0.0410 0.0376 0.0373 0.0365 0.0335 0.0332
Time ( in seconds)/FLOPs
GCD 06/07G 40/50G
BlockPivot ProjGrad FastHals 17/11G 21/14G 23/29G 266/235G 124/87G 93/161G 053/041G 056/035G 021/011G 043/038G 254/270G 286/143G 043/046G 077/171G 135/144G 106/81G 23/23G 40/102G 456/494G 309/298G 89/88G 180/468G 846/912G 515/538G 146/145G 290/757G 74/54G 65/145G 141/201G 303/669G 986/677G 339/382G 330/515G 633/1390G 2568/1935G 765/824G
18/27G
90/91G e c n e r e f f i d e u a v l n o i t c n u f e v i t l a e R
100
10−2
10−4
10−6
10−8
10−10
0
GCD FastHals BlockPivot t i n e d a r g d e j t c e o r P
5
10
15
20 time(sec )
25
30
35
40
105
104
103
102
101
100
10−1
0
GCD FastHals BlockPivot
100 e c n e r e f f i d l e u a v n o i t c n u f e v i t l a e R
5
10
15
20 time(sec )
25
30
35
40
10−1
0
0.2
0.4
0.6 time(sec )
GCD FastHals
0.8
1
( a ) Objective value for YahooNews dataset
( b ) Project gradient for YahooNews dataset
( c ) L1 regularized objective value for Yahoo News dataset , with ρ1 = 10 , ρ2 = 20
100
10−1
10−2
10−3
10−4 e c n e r e f f i d l e u a v n o i t c n u f e v i t l a e R
10−5
0
GCD FastHals BlockPivot
50
100
150 200 time(sec )
250
300
350 t i n e d a r g d e j t c e o r P
109
108
107
106
105
104
0
GCD FastHals BlockPivot e c n e r e f f i d l e u a v n o i t c n u f e v i t l a e R
50
100
150 200 time(sec )
250
300
350
1011
1010
109
108
107
0
GCD FastHals
50
100
150 time(sec )
200
250
300
( d ) Objective value for MNIST dataset
( e ) Projected gradient for MNIST dataset
( f ) L1 regularized objective value for NMIST dataset with ρ1 = 50 , 000 , ρ2 = 100 , 000 e c n e r e f f i d l e u a v n o i t c n u f e v i t l a e R
GCD FastHals BlockPivot t i n e d a r g d e j t c e o r P
20
40
60
80 100 time(sec )
120
140
160
180
106
105
104
103
102
101
100
0
GCD FastHals BlockPivot
103 e c n e r e f f i
GCD FastHals d e u a v l
102 n o i t c n u f e v i t l a e R
20
40
60
80 100 time(sec )
120
140
160
180
0
50
100
150
200 time(sec )
250
300
350
400
100
10−1
10−2
10−3
10−4
0
( g ) Objective value for RCV1 dataset
( h ) Projected gradient for RCV1 dataset
( i ) L1 regularized objective value for RCV1 dataset with ρ1 = 0.005 , ρ2 = 0.05
Figure 2 : Time comparison for large sparse datasets . The result indicate that GCD is both faster and converges to better solutions .
3 . FastHals : Cyclic coordinate descent method in [ 3 ] . We implemented the algorithm in MATLAB .
For GCD , we set the inner stopping condition ǫ to be 0001 We test the performance on the following dense datasets :
1 . Synthetic dataset : Following the process in [ 12 ] , we generate the data by first randomly creating W and H , and then compute V = W H . We generate two datasets Synth03 and Synth08 , the suffix numbers indicate 30 % or 80 % variables in solutions are zeros .
2 . CBCL image dataset : http://cbclmitedu/cbcl/ software datasets/FaceData2.html
3 . ORL image dataset : http://wwwclcamacuk/ research/dtg/attarchive/facedatabase.html
We follow the same setting as in [ 8 ] for CBCL and ORL datasets . The size of the datasets are summarized in Table 2 . To ensure a fair comparison , all experimental results in this paper are the average of 10 random initial points .
Table 2 compares the CPU time for each solver to achieve F /kV k2 the specified relative error defined by kV − W Hk2 F . For synthetic dataset , since the exact factorization exists , all the methods can achieve very low objective function value . From Table 2 , we can conclude that GCD is two to three times faster than BlockPivot and FastHals on dense datasets . As mentioned in Section 5 , we implement part of GCD in C . To have a fair comparison , the FLOPs ( number of Floating Point Operations ) is listed in Table 2 . FastHals is competitive in time but slow in FLOPs . This is because it fully utilizes dense matrix multiplication operations , which is efficient in MATLAB .
For NMF with KL divergence , we compare the performance of our cyclic coordinate descent method ( CCD ) with the multiplicative update algorithm ( Multiplicative ) proposed in [ 15 ] . As mentioned in Section 3 , the method proposed in [ 3 ] solves a different formulation , so we do not include it in our comparisons . Table 3 shows the runtime for CCD and Multiplicative to achieve the specified relative error . For KL divergence , we define the relative error to be the objec(Pj Vij )/n ) , which is the distance between Vij and the uniform distribution for each row . We implement CCD in C and Multiplicative in MATLAB , so we also list the FLOPs in Table 3 . Table 3 shows that CCD is 2 to 3 times faster than Multiplicative at the beginning , and can be 10 times faster to get a more accurate solution . If we consider FLOPs , CCD is even better . 6.2 Comparison on sparse datasets tive value L(W , H ) in ( 3 ) divided byPi,j Vij log(
Vij
In Section 6.1 , BlockPivot , FastHals , and GCD are the three most competitive methods . To test their scalability , we further compare their performances on large sparse datasets . We use the following sparse datasets :
1 . Yahoo News ( K Series ) : A news articles dataset . 2 . RCV1 [ 16 ] : An archive of newswire stories from Reuters Ltd . The original dataset has 781,265 documents and 47,236 features . Following the preprocessing in [ 18 ] , we choose data from 15 random categories and eliminate unused features . However , our data is much larger than the one used in [ 18 ] .
3 . MNIST [ 13 ] : A collection of hand written digits .
The statistics of the datasets are summarized in Table 4 . We set k according to the number of categories for each datasets . We run GCD and FastHals with our C implementations with sparse matrix operations , and for BlockPivot we use the author ’s code in MATLAB with sparse V as input . In Figure 2(a ) , 2(d ) , 2(g ) , we show the CPU time for the 3
Table 3 : Time comparison results for KL divergence . ∗ indicates the specified objective value is not achievable . The results indicate CCD outperforms Multiplicative dataset k
Synth03
Synth08
CBCL
ORL
10
30
10
30
49
25 relative error 10−3 10−5 10−3 10−5 10−2 10−5 10−2 10−5 0.1202 0.1103 0.1093 0.3370 0.3095 0.3067
Time ( in seconds)/FLOPs
Multiplicative 340/681G 1442/2406G
CCD 114/52G 148/68G 1211/587G 7495/20574G 18432/893G 70923/187878G 25/17G 130/88G 226/112G 568/277G 382/182G 1232/584G 5626/7813G 1660/787G 32669/27054G 737/350G 2536/1170G 9022/13230G 3702/1775G 16319/32802G
303/716G * 460/939G * 212/641G
1652/3363G
Table 4 : Statistics of data sets . k is the value of reduced dimension we use in the experiments . n 2,340 60,000 152,120
Data set Yahoo News MNIST RCV1(subset )
#nz 349,792 8,994,156 7,817,031 m 21,839 7,80 31,025 k 20 10 15 methods to reduce the objective value of least squares NMF , versus with logarithmically decreasing values of ( f ( W , H ) − f ∗)/f ∗ , where f ∗ denote the lowest average objective value , for 10 initial points , of the 3 methods .
Compared to FastHals and BlockPivot , GCD converges to a local optimum with lower objective value . This is important for nonconvex optimization problems because we can find a better local optimum . To further compare the speed of convergence to local optimums , we check the projected gradient ∇P f ( W , H ) , which measures the distance between current solution and stationary points . The results are in Figure 2(b ) , 2(e ) and 2(h ) . The figures indicate that GCD converges to the stationary point in lesser CPU time .
We further add L1 penalty terms as in ( 2 ) . We only include GCD and FastHals in the comparison because BlockPivot does not provide the same regularized form in their package . Figures 2(c ) , 2(f ) and 2(i ) compare the methods for reducing the objective value of ( 2 ) . For this comparison we choose the parameters λ1 and λ2 so that on average more than half the variables in the solution of W and H are zero . The figures indicate that GCD achieves lower objective function value than FastHals in MNIST , and for Yahoo News and RCV1 , GCD is more than 10 times faster . This is because GCD can focus on nonzero variables while FastHals updates all the variables at each iteration . 7 . DISCUSSION AND CONCLUSIONS
In summary , we propose coordinate descent methods that do variable selection for solving least squares NMF and KLNMF . Our methods have theoretical guarantees and are efficient on real life data . The significant speedups on sparse data show a potential to apply NMF to larger problems . In the future , our method can be extended to solve NMF with missing values or other matrix completion problems . 8 . ACKNOWLEDGEMENTS
This research was supported by NSF grant CCF 0728879 .
9 . REFERENCES [ 1 ] M . Berry , M . Browne , A . Langville , P . Pauca , and
R . Plemmon . Algorithms and applications for approximate nonnegative matrix factorization . Computational Statistics and Data Analysis , 2007 . Submitted .
[ 2 ] D . Bersekas and J . Tsitsiklis . Parallel and distributed computation . Prentice Hall , 1989 .
[ 3 ] A . Cichocki and A H Phan . Fast local algorithms for large scale nonnegative matrix and tensor factorizations . IEICE Transaction on Fundamentals , E92 A(3):708–721 , 2009 .
[ 4 ] E . Gaussier and C . Goutte . Relation between PLSA and NMF and implications . 28th Annual International ACM SIGIR Conference , 2005 .
[ 5 ] E . F . Gonzales and Y . Zhang . Accelerating the
Lee Seung algorithm for non negative matrix factorization . Technical report , Department of Computational and Applied Mathematics , Rice University , 2005 .
[ 6 ] L . Grippo and M . Sciandrone . On the convergence of the block nonlinear Gauss Seidel method under convex constraints . Operations Research Letters , 26:127–136 , 2000 .
[ 7 ] P . O . Hoyer . Non negative sparse coding . In
Proceedings of IEEE Workshop on Neural Networks for Signal Processing , pages 557–565 , 2002 .
[ 8 ] P . O . Hoyer . Non negative matrix factorization with sparseness constraints . Journal of Machine Learning Research , 5:1457–1469 , 2004 .
[ 9 ] C J Hsieh and I . S . Dhillon . Fast coordinate descent methods with variable selection for non negative matrix factorization . Department of Computer Science TR 11 06 , University of Texas at Austin , 2011 .
[ 10 ] D . Kim , S . Sra , and I . S . Dhillon . Fast Newton type methods for the least squares nonnegative matrix appoximation problem . Proceedings of the Sixth SIAM International Conference on Data Mining , pages 343–354 , 2007 .
[ 11 ] J . Kim and H . Park . Non negative matrix factorization based on alternating non negativity constrained least squares and active set method . SIAM Journal on Matrix Analysis and Applications , 30(2):713–730 , 2008 .
[ 12 ] J . Kim and H . Park . Toward faster nonnegative matrix factorization : A new algorithm and comparisons . Proceedings of the IEEE International Conference on Data Mining , pages 353–362 , 2008 .
[ 13 ] Y . LeCun , L . Bottou , Y . Bengio , and P . Haffner .
Gradient based learning applied to document recognition . Proceedings of the IEEE , 86(11):2278–2324 , November 1998 . MNIST database available at http://yannlecuncom/exdb/mnist/
[ 14 ] D . D . Lee and H . S . Seung . Learning the parts of objects by non negative matrix factorization . Nature , 401:788–791 , 1999 .
[ 15 ] D . D . Lee and H . S . Seung . Algorithms for non negative matrix factorization . In T . K . Leen , T . G . Dietterich , and V . Tresp , editors , Advances in Neural Information Processing Systems 13 , pages 556–562 . MIT Press , 2001 .
[ 16 ] D . D . Lewis , Y . Yang , T . G . Rose , and F . Li . RCV1 :
A new benchmark collection for text categorization research . Journal of Machine Learning Research , 5:361–397 , 2004 .
[ 17 ] Y . Li and S . Osher . Coordinate descent optimization for l1 minimization with application to compressed sensing ; a greedy algorithm . Inverse Probl . Imaging , 3(3):487–503 , 2009 .
[ 18 ] C J Lin . Projected gradient methods for non negative matrix factorization . Neural Computation , 19:2756–2779 , 2007 .
[ 19 ] C . Liu , H . chih Yang , J . Fan , L W He , and Y M
Wang . Distributed Non negative Matrix Factorization for Web Scale Dyadic Data Analysis on MapReduce . 2010 .
[ 20 ] P . Paatero and U . Tapper . Positive matrix factorization : A non negative factor model with optimal utilization of error . Environmetrics , 5:111–126 , 1994 .
[ 21 ] S . Perkins , K . Lacker , and J . Theiler . Grafting : Fast , incremental feature selection by gradient descent in function space . Journal of Machine Learning Research , 3:1333–1356 , 2003 .
[ 22 ] J . Piper , P . Pauca , R . Plemmons , and M . Giffin . Object characterization from spectral data using nonnegative factorization and information theory . In Proceedings of AMOS Technical Conference , 2004 . [ 23 ] R . Zdunek and A . Cichocki . Non negative matrix factorization with quasi newton optimization . Eighth International Conference on Artificial Intelligence and Soft Computing , ICAISC , pages 870–879 , 2006 .
