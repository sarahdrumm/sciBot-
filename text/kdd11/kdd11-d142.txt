Temporal Multi Hierarchy Smoothing for Estimating Rates of Rare Events
Nagaraj Kota Yahoo! Labs
Bengaluru,Karnataka,India nagarajk@yahoo inc.com
ABSTRACT We consider the problem of estimating rates of rare events obtained through interactions among several categorical variables that are heavy tailed and hierarchical . In our previous work , we proposed a scalable log linear model called LMMH ( Log Linear Models for Multiple Hierarchies ) that combats data sparsity at granular levels through small sample size corrections that borrow strength from rate estimates at coarser resolutions . This paper extends our previous work in two directions . First , we model excess heterogeneity by fitting local LMMH models to relatively homogeneous subsets of the data . To ensure scalable computation , these subsets are induced through a decision tree , we call this Treed LMMH . Second , the Treed LMMH method is coupled with temporal smoothing procedure based on a fast Kalman filter style algorithm . We show that simultaneously performing hierarchical and temporal smoothing leads to significant improvement in predictive accuracy . Our methods are illustrated on a large scale computational advertising dataset consisting of billions of observations and hundreds of millions of attribute combinations(cells ) .
Categories and Subject Descriptors H11 [ Information Systems ] : Models and Principles
General Terms Algorithms , Theory , Experimentation
Keywords Computational Advertising , Display Advertising , Decision Trees , Multi Hierarchy smoother , Kalman Filtering , Count data
1 .
INTRODUCTION
Large scale recommender problems involve matching items to users in some context . For instance , in computational advertising ads(items ) are displayed to users in the context of web search or a visit to a webpage to maximize revenue [ 8 , 7 ] . In content optimization , articles(items ) are displayed to users visiting a web page
Deepak Agarwal Yahoo! Research
Santa Clara , CA , USA dagarwal@yahoo inc.com to maximize engagement [ 11 , 3 ] . Depending on the context and objectives , such user item matches are based on utilities that are functions of rates of rare events like click on an ad , post click conversion , and so on . Thus estimating rare rates is a crucial component of many recommender problems . Data sparsity is a formidable challenge , this happens because the occurence distribution of both user context pairs ( referred to as opportunities ) and items are heavy tailed , ie , a small fraction of values are frequent but the rest are more sporadic . Thus response counts ( success ) and sample size ( tries ) generated through interactions among such heavy tailed random variables is high dimensional with skewed sample size distribution . The typically low response rates further exacerbates sparsity . According to [ 20 ] , display advertising applications had an average click rate of 1/100 in 2008 . Churn in both opportunities and items means new opportunity item pairs appear and old ones disappear routinely over time . This makes the estimation more challenging .
Maximum likelihood estimates obtained independently for attribute combinations ( cell ) are noisy and unreliable due to data sparsity . It is not atypical to observe a large fraction of cells with zero successes but large variation in sample size . Prior work focus on small sample size correction techniques that pools data across different data aggregates . Pooling leads to reduction in variance but incurs bias , this bias variance tradeoff determines the quality of the procedure . When opportunities and items are organized hierarchically , pooling aggregates across multiple dimensions and multiple resolutions often improves performance since hierarchies that are obtained from domain knowledge typically provide good priors without “ dipping ” into the training data .
Our previous work [ 1 ] called Log Linear Models for Multiple Hierarchies ( LMMH ) smooth rare response rates by pooling data along a digraph obtained through a cross product of multiple hierarchies . It works well when rates for cells that are close siblings are correlated . LMMH was shown to improve prediction by exploiting such correlations . Besides accuracy , LMMH also addressed the issue of parsimony in a model based way by pruning non informative cells through a “ spike and slab ” prior . The method scales gracefully to high dimensional categorical problems in a map reduce framework [ 12 ] — billions of records aggregated into hundreds of millions of cells .
This paper presents a novel variant of LMMH that fits several local LMMH models to capture heterogeneity and higher order interactions in a better way . The main idea is as follows — a baseline model based on features of both opportunities and items is induced through a decision tree . Given a relatively homogeneous partitioning of the feature space , several local LMMH models are fitted to data subsets on different nodes of the decision tree . This helps in capturing higher order interactions more effectively , a global
1361 LMMH may miss some of these and incur more bias . For instance , click rates on different ( publisher,ad ) pairs may be different for users interested in finance . The existing LMMH model assumes a multiplicative relationship in such scenarios whereby click rates for finance users are proportional to a ( pub,ad ) specific parameter φpub,ad . This may not be true in practice , finance users may have stronger affinities to an ad on a finance page relative to a news page . A global multiplicative model fails to capture such strong three way interactions even with large sample sizes . Bias reduction through locality though attractive , may incur increased variance for many attribute combinations and hence effective smoothing and backoff procedures are imperative . We provide a modeling procedure to fit local LMMHs to data through smoothing techniques that prevents over fitting . One can interpret our approach as a mixture of LMMHs where the prior mixing probabilities are functions of features . Although fitting such a mixture model through an EM algorithm is possible , it presents considerable computational difficulties for large problems .
Another aspect not accounted for in LMMH is to simultaneously perform both hierarchical and temporal smoothing . Advertising applications are dynamic in nature with high churn in both opportunities and ads , hence it is important for models to adapt over time . The approach in this paper provides a solution to this problem .
Our contributions are as follows . We propose TTreed LMMH , a combination of several local LMMH models equipped to perform both hierarchical and temporal smoothing simultaneously . The added flexibility of TTreed LMMH presents formidable computational challenges , we provide a model fitting procedure that is scalable to very large scale applications in a map reduce framework . Time smoothing is achieved through a fast Kalman filter algorithm . The additional flexibility and ability to adapt over time provide significant improvements over existing methods on a large scale display advertising application .
The rest of the paper is organized as follows . Section 2 describes TTreed LMMH followed by details on scalable model fitting in a map reduce framework in section 3 . We discuss related work in section 4 , illustrate our method through experiments on vary large scale click dataset ( billions of records ) in section 5 , and end with discussion in section 6 .
2 . MODELING DETAILS
Although TTreed LMMH is generally applicable in recommender problems that involve rate estimation of some rare event , we primarily focus on computational advertising in this paper . Response data is obtained when users interact with ads in a given context like web search , visiting a publisher site , and so on . Each interaction produces either a negative or positive . Our goal then is to estimate response rates when ads are served to opportunities . For ease of expostion , context in our case is a webpage and response is click rate . Hence , click on an ad is a positive , no click is negative .
Denoting an opportunity by i = ( u , p ) where u and p denote the user and publisher page ids respectively , and an ad by j , we can formulate this as a problem of estimating click rates qij in cells of the opportunity ad matrix . Modeling opportunity at ( user,webpage ) resolution leads to extreme sparsity with most combinations having a marginal count of 0 clicks . This is because user visit distribution to a page p is heavy tailed with low arrival rates for a large user population . For the purposes of estimation in this paper , a user is completed characterized by a feature vector xu that may consist of user demographics , inferred browse behavior [ 9 ] , and so on . By abusing notations a bit , we denote by xp and xj feature vectors associated with webpage p and ad j respectively . Our problem now is to estimate click rates q(xu,p),j . In LMMH , this was estimated by factoring the probabilities as q(xu,p),j = b(xu , xp , xj)λp,j
( 1 ) where b is a baseline probability estimated through some regression model ( eg logistic regression , decision trees , etc ) using features of users,webpages and ads and the corrections λp,j compensate for “ lack of fit ” of the baseline model in the publisher ad matrix . To estimate the corrections , LMMH aggregates data in terms of observed and expected clicks cp,j and Ep,j , respectively , for observed cells in the ( p , j ) matrix . Expected success Ep,j is obtained by summing the baseline probabilities of all events that are assigned to cell ( p , j ) in the training data . Estimating these corrections is a daunting task due to data sparsity , a large fraction of observed ( p , j ) cells tend to have zero successes ( clicks ) and widely varying expected clicks . For instance , how should one smooth 2/.2 vs 20/2 ? An effective small sample size correction technique is necessary . Further , predictions for ( p , j ) cells not observed in the data is also desirable for better performance in cold start scenarios .
LMMH exploits hierarchies that are often available for attributes like webpages and ads from domain knowledge . For instance , webpages are nested within sites which may in turn be nested within publisher type ; ads are organized by campaigns that are nested within advertisers . The hierarchical structure is used by LMMH to provide smooth estimates of corrections at fine resolutions by using pooled estimates at coarser resolutions as priors . It uses the strategy of “ backoff ” whereby cells with small or no samples will fallback on estimates at coarser resolutions . This works in computational advertising applications because small cells nested in larger ones tend to have corrections that are correlated . 2.1 Modeling corrections in LMMH
We briefly describe the hierarchical modeling of corrections λp,j in LMMH and refer the reader to [ 1 ] for complete details . Each event in cell ( p , j ) comes with an associated hierarchical path on both p and j . Let the paths on p and j be denoted by p = p1 → p2 → ··· → pm and j = j1 → j2 → ··· → jn respectively . Then , LMMH assumes λp,j for each ( p , j ) is log linear with mn terms that are associated with all cross products of nodes in the paths of p and j . In words , this is log linear in parameters of all possible parents of ( p , j ) in the digraph obtained by taking the cross product of webpage and ad hierarchies . Mathematically , mY nY v=1 w=1
λp,j =
φpv ,jw
( 2 ) where φpv ,jw is the state parameter associated with node pair ( pv , jw ) . Denoting by φ the state parameter vector associated with all nodepairs , we note that the dimension of φ in our application is extremely large and hence constraints on the parameters is imperative . ( The dimension of φ in our example dataset is close to 100M . ) We estimate φ in LMMH as described below . Conditional on knowing λ or equivalently φ , cp,j|Ep,j , φ ∼ Poisson(Ep,jλp,j ) and λp,j is expressed in terms of φ by equation 2 . Since the probabilities are small and our goal is to estimate rates of rare events , Poisson assumption is reasonable and have been widely used in applications involving rare event estimation(see [ 17],[13 ] for examples ) . Thus , the log likelihood function of φ is given by ( −Ep,jλp,j + log(λp,j)cp,j ) + constant l(φ ) =
λp,j =
φpv ,jw
( 3 )
X mY p,j nY v=1 w=1
1362 To smooth(regularize ) parameters , we assume independent spikeand slab priors on each component of φ [ 21 ] . In other words , a component φ in φ has a prior π(φ ; a , P ) , a 2 component mixture of a Dirac and a Gamma distribution given by
π(φ ; a , P ) = P 1(φ = 1 ) + ( 1 − P )Gamma(φ ; 1 , 1/a ) ie , with probability P the parameter φ is exactly 1 ( ie , state not important ) and with probability ( 1− P ) it is drawn from a Gamma distribution with mean 1 and variance 1/a . This prior encourages automatic variable selection and smoothing simultaneously , posterior mode of φ under this model sets several components to be exactly unity , hence those node pairs could be pruned from the model . We used P = .5 in all our experiments . For a fixed value of a , combining the prior on φ with the likelihood gives the log posterior of φ as
X pj l(φ ) + log(π(φpj ; a , P ) )
( 4 )
The state estimates ˜φ for a fixed a are now obtained by maximizing the log posterior in equation 4 . For the purpose of scalability in a map reduce framework to extremely large applications , the posterior mode is estimated by using a sequential “ one at a time ” update procedure . In other words , for each sweep , the state components are updated one at a time conditional on other components being set to their latest values . The conditional mode computation is available in closed form and given in [ 1 ] . This is also referred to as iterated conditional modes ( ICM ) algorithm [ 5 ] . 2.2 Treed LMMH
The separability assumption in equation 1 may exhibit lack of fit in some regions of our high dimensional space with strong higher order interactions and large sample size . For instance , consider an example of estimating click rates for pairs ( p1 , j1 ) and ( p2 , j2 ) for finance users . Suppose the baseline probabilities are b1 and b2 , sample sizes are N1 and N2 , clicks are c1 and c2 , respectively , in the two publisher ad cells for finance users . Under LMMH if λp1,j1 and λp2,j2 are global estimated corrections for the two cells , the probabilities are b1λp1,j1 and b2λp2,j2 respectively , which may even for very large sambe quite different from MLEs c1 N1 ple sizes! Thus , having local corrections λp,j that depend on the features ( xu , xp , xj ) may lead to better fit through bias reduction in regions with large sample sizes . and c2 N2
Some important considerations for estimating such local interactions are — a)The method should assign every new event ( even those that involve new ( p , j ) pairs ) to the appropriate local model , hence it is better to make such an assignment a function of features alone . b ) Estimating local interactions on subsets of data reduces sample size and incurs high variance , appropriate smoothing across local LMMHs can help mitigate some of these issues . c)The fitting method has to scale to large amounts of data ( large here means data cannot all fit into memory of a few commodity PCs ) .
We address these issues by taking recourse to a decision tree based local correction models . We build a feature based baseline model based on decision trees and local LMMHs are then fitted on each leaf node of the tree . However with deep trees , the sample size available on individual leaf nodes may get too sparse to fit a local LMMH . To mitigate this issue , we build LMMH at two different resolutions — i ) at the root of the decision tree which gives the global LMMH with a decision tree baseline and ii ) local LMMH on frontier nodes of the decision tree as corrections to global LMMH in i ) . A node is defined to be on the frontier if the number of samples on the node exceeds a pre specified threshold and there is at least one child node with sample size below the threshold . Note that frontier nodes are not flat and could have a nesting . Also , the dimension of local LMMH on the frontier may be different and smaller than global LMMH . For instance , if a node only consists of sports publishers , the ps that occur in that local LMMH would be a subset of all sporty ps . This also reduces computational complexity and helps in scalable model fitting .
To summarize , Treed LMMH parametrizes the probability q(xu,p),j as p,j q(xu,p),j = DT(xu , xp , xj)λp,jλFk
( 5 ) Here , DT(xu , xp , xj ) is the baseline decision tree model based on features , λp,j is the global LMMH correction and finally the crucial local correction λFk p,j corresponds to local LMMH on frontier node Fk . Ideally both global and local frontier corrections should be learnt simultaneously but this is computationally prohibitive for our applications . Hence we fit the corrections sequentially — global followed by local on the residuals from global . To mitigate the impact of global corrections “ washing ” away important local signals due to the greedy forward fitting strategy , we perform a stronger smoothing(large a ) on φ that correspond to global corrections λp,j , this improved performance in our experiments . Such a strategy is often used in other procedures like boosting where one either uses a weak learner to successively fit residuals and/or shrink the fitted function values before fitting the next member in the ensemble [ 19 ] . All corrections ( global and local ) are performed by expressing λs in terms of φs and smoothing is done through a spike and slab prior . There are several advantages of constructing local LMMH on frontier instead of leaves . The baseline model based on features alone can be a deep tree to reduce bias . By rolling up expected clicks to coarser frontier nodes one avoids high variance associated with building local LMMH with smaller sample size . 2.3 Temporal smoothing with Kalman filter
Data obtained in computational advertising is dynamic , new webpages and ads are routinely introduced into the system ( and old ones retire over time ) . Also , fast online updates to the state parameters make the system adaptive . We provide a Kalman filter style algorithm [ 22 ] that performs temporal smoothing over time in addition to the hierarchical smoothing provided by Treed LMMH . The suffix t will denote data obtained at epoch t , temporal smoothing of parameters will be done over epochs . The length of an epoch depends on the engineering infrastructure available and time taken to receive user feedback from interactions . For instance , in estimating click rates , it is customary to obtain click feedback within 5 minutes with high probability after an ad is displayed . For estimating conversion rate per click , the delay in obtaining conversion data from advertisers may be significant ( eg few minutes to few days ) . For fast updates , infrastructure that can collect data from the webservers and transmit it to data clusters where model updates take place is required .
Let q(xu,p),j,t denote the click rate when ad j is shown to user u who visits page p during epoch t . We model temporal evolution in click rates as described in the state equation below . q(xu,p),j,t = DTt(xu , xp , xj)η(xu,p),j,t η(xu,p),j,t = η(xu,p),j,t−1λ(xu,p),j,t
( 6 )
Here , DTt is the decision tree model that provides the baseline probability in epoch t and η(xu,p),j,t is a random variable that provides the event level correction . The state equation assumes this event level correction is centered around the correction at epoch t − 1 with “ innovation ” λxu,p,j,t that has mean 1 and follows a Treed LMMH distribution ( and is independent of prior history ) . As
1363 in classical Kalman filtering , we plug in η(xu,p),j,t−1 , the correction estimate for the event at epoch t − 1 , this provides the prior mean of q(xu,p),j,t at epoch t that is given by
ˆ
∗ ( xu,p),j,t = DTt(xu , xp , xj ) b
η(xu,p),j,t−1
ˆ
( 7 )
( xu,p),j,t = DTt(xu , xp , xj )
Thus , the conditional distribution of q(xu,p),j,t given all data before epoch t is captured by the random variable b∗ ( xu,p),j,tλ(xu,p),j,t . Note that b∗ η(xu,p),j,t−1 is a modified ˆ baseline probability that incorporates model estimates at time t−1 . We now estimate λxu,p,j,ts with Treed LMMH as before but using E(xu,p),j,ts from b∗s . This provides us with a methodology that does both hierarchical and temporal smoothing of data at the event level , estimates from previous epoch provides a better baseline and smaller residuals to estimate at epoch t through our local and global LMMH procedures . 2.4 Decision tree Induction
Decision tree induction is an important component of our approach for obtaining a feature based baseline model and determining homogeneous subsets of data to perform local corrections . While there is a rich literature on decision tree induction , a proper review of which is not possible here , we describe the procedure followed in this paper for the sake of comprehensiveness .
Compactly denote the attribute value ( xu , xp , xj ) associated with each record as x , this is composed of values from N variables given by {x1 , x2 , , xN} where each xi is a categorical variable taking values in the finite set DXi . Examples include user location , webpage category , ad category , and so on . We shall sometimes abuse notations and use xi to denote both the variable and the value taken by the variable but the meaning would be clear from the context . To each record x we attach a binary label C that takes value 1 if the response is positive else it is 0 . We denote by b(x ) the true response rate associated with attribute value x and our goal is to estimate this function from training data . Since all our attributes are assumed to be categorical ( we discretize any continuous variable into bins using standard methods [ 18] ) , it is enough to work with sufficient statistics {cx , nx} for all attribute combinations x that occur in our training data . Here , nx is the number of times x occurs in our training data and cx denotes the total number of positive responses out of nx associated with x . One can also think of this as estimating a high dimensional density b(x ) through sparse sufficient statistics {cx , nx} . In our applications , the total number of attributes N is large and/or the size of |DXi| is large , both cases gives rise to a large number of unique x values in the data . Due to high dimensionality and rareness of response , a large fraction of combinations have small nx and cx . In such scenarios , the naive maximum likelihood estimator ˆbx = cx is unreliable . Also , we consider decision trees where we only evaluate splits on an entire set of values DXi for nx each i and choose the set that provides maximum improvement according to a splitting criteria .
Splitting Criteria : Our approach is based on Hunt ’s [ 15 ] recursive greedy top down splitting method , where we choose the attribute predicate that best locally decreases the data space impurity . Decreasing the impurity at a split means that the values of the attribute that was chosen segregates the points based on their response rather than randomly . This impurity criteria can be based on information theoretic measures like entropy , or Gini [ 15 ] or any other closed form functional measure . In this paper we consider a splitting criteria based on Gain Ratio , other criteria are easy to incorporate in our framework .
Gain Ratio Split Criteria : At any stage of the greedy procedure , this criteria chooses attribute x that reduces the conditional entropy of the response rate . The unconditional entropy of response at the current node is given by H(C ) = −P r(1 ) log2 P r(1 ) − ( 1 − P r(1 ) ) log2(1 − P r(1 ) ) where P r(1 ) is the empirical response rate ( c/n ) at the current node . The conditional entropy of any attribute x with domain DX is given as
H(C|x ) = −P l∈DX
P r(l)(P r(1|l)log2P r(1|l ) + +(1 − P r(1|l))log2(1 − P r(1|l) ) ) where P r(l ) is the fraction of samples at the current node that gets assigned to the set ( x = l ) and P r(1|l ) is the empirical response rate in the set ( x = l ) . Similarly , the unconditional entropy is
H(x ) = − X l∈DX
P r(l)log2(P r(l) ) .
In all our entropy calculations , 0log(0 ) = 0 . The Gain ratio when splitting by values of variable x is then defined as
GR(x ) =
H(C ) − H(C|x )
H(x )
( 8 )
GR(x ) can be interpreted as reduction in total variability in response rate due to split by x and H(| ) is the entropy function . We normalize by H(x ) to adjust for the intrinsic variation in how samples are distributed for different values of x . For instance , attributes with large number of values will tend to split more and would tend to have a larger reduction in absolute variation in response rates but they use up more degrees of freedom ; this gives more weight to attributes with large number of values and hence the normalization is important [ 23 ] .
Estimating leaf node rates : We grow our tree until a node has some minimum number of successes ( eg 3 clicks ) , we do not split the node any further . However , several nodes may also have smaller clicks than the minimum threshold . For instance , splitting a node with 100 clicks by its unique values may lead to a few leaf nodes with zero clicks . Thus , estimating click rates at the leaf nodes using MLE may overfit . We use two different estimation procedures that are described below .
Gamma Poisson smoothing : Prior work [ 27 ] estimates the leaf node probabilities by fallback on the emprical click rate at the root . More specifically , for a record with attribute x assigned to leaf lf ( x ) , estimated probability ˆb(x ) is given as
ˆblf ( x ) = proot clf ( x ) + a prootnlf ( x ) + a
( 9 ) where proot is the empirical click rate at root node , a is a constant which we choose as 1 . This however ignores important information that is available along the paths . For instance , it is more intuitive to fallback on the rate of 1(M ale ) when there is paucity of data to estimate rates for 1(M ale , CA ) instead of falling back on 1(root ) . We perform a top down smoothing in an autoregressive fashion until we reach the leaf node . More precisely , smoothed probability rate estimate bk on node k of a decision tree path is given by bk = bk−1(ck + a ) nkbk−1 + a
The predicted probability is obtained by simply traversing the tree and observing the smoothened rate estimate at the leaf node . We shall refer to this as DT baseline model .
1364 The DT smoothing described above does not prune noisy nodes , an effective pruning strategy may reduce model size and help in quick model predictions at runtime without much loss in accuracy . A compact model also reduces memory footprint and helps in quick online evaluation during serving . To do this , we use the same idea as in LMMH , the probability b at each leaf node is expressed as product of node corrections along the paths , the node corrections are estimated again through a Poisson likelihood and spike and slab prior . We shall refer to this as LLDT baseline model . 2.5 Putting it all together : TTreed LMMH
We summarize the components of TTreed LMMH . During each epoch , we build a baseline model based only on features using a decision tree . This baseline model could be either based on autoregressive top down smoothing ( DT ) or LMMH based hierarchical smoothing ( LLDT ) . We also note that the decision tree need not be updated every epoch , it could be built on large amounts of data and updated less frequently compared to the correction updates that happen in smaller epochs . Given a baseline model , corrections per event from the latest epoch models are used to compute a refined baseline b∗ per event as in equation 7 . A Treed LMMH using global and local LMMHs is fitted on the frontier nodes , it is used to obtain final event level probabilities . At epoch 0 , prior epoch corrections are all assumed to be 1 . The entire Treed LMMH model is trained offline after some period using large amounts of historic data ( eg , at the end of each day we build Treed LMMH using last 30 days data ) .
3 . MODEL FITTING
We describe our scalable map reduce procedures for estimating DT(xu , xp , xj ) , and λp,jλFk p,j in Equation 5 for TTreed LMMH . We note that fitting a model like TTreed LMMH to data that is massive ( does not fit in memory ) is challenging , map reduce offers an attractive distributed computing option but requires careful design considerations . We are able to scale up our procedure primarily by exploiting the hierarchical structure in the data . We also note that some of our modeling decisions were also driven by computational considerations .
Scalable Decision Tree computation : For the baseline estimate DT(xu , xp , xj ) , we induce Decision Tree in a map reduce framework . For every level of the tree building process , we compute the split criterion of the previously unconsidered attributes and choose the one with the maximum Gain ratio as our split . We scale this step by partitioning the dataset over multiple workers called mappers , and collecting statistics such as success and trials across values of each attribute . In the next stage reducer , the collected statistics are aggregated from different mappers and the best split is determined . We then move to the next level and partition the data in the child regions for further induction . For DT baseline model , the probabilities are estimated using the smoothing procedure described in Section 24 In case of LLDT baseline model , we estimate the node correction at each level of the tree by joining the correction from corresponding parent node to get corrected expected success . In the reducer , we aggregate over succ , and corrected expected succ at each node , while discounting previous iteration ’s correction to estimate the new correction φ . Note that the conditional posteriors of φ ’s at the ith level of the tree are independent of each other given the other coefficients and could be updated in parallel .
Estimating the global correction , λp,j:– This is based on the Iterated Conditional Modes ( ICM ) algorithm described in our previous paper [ 1 ] . While we refer the readers to that paper for complete implementation details , crucial point for scalable fitting is that the conditional posteriors of states φpv ,jw for nodes at the ( v , w)th level ( v = 1,··· , m , w = 1,··· , n ) are independent of each other and can be updated together in parallel . These updates for LLDT smoothing are also similar .
Estimating local LMMH on frontier nodes , λFk p,j : These are learnt in an intuitive data cube fashion . First , we note that since a node is on the frontier if at least one child is below the sample size threshold , there can be nesting within frontier nodes . Given a list of frontier nodes Fi ( i = 1,··· , K ) , where some of the Fi ’s are related by parent child relationship , computing the high dimensional state estimates φpv ,jw in Fi becomes a challenging problem in map reduce setting . For example , assume that there are three frontier nodes F0 , F1 , F2 , where F0 is a parent node of F1 , F2 and a given key ( pv , jw ) is appearing in all of these frontier nodes . To compute the state estimate φpv ,jw at F0 frontier , one needs to rollup the success and expected success from frontier nodes F1 and F2 . Note that there can be multiple levels of nesting of frontier nodes and thousands of such cases occur in our application . Optimizing map reduce jobs for these tasks is very challenging in terms of algorithm complexity . We provide a scheme in algorithm 1 which estimates the state parameters simultaneously for all the frontier nodes . With the above fitting procedures for global
Fi:0 = 1 .
Algorithm 1 Map reduce fitting pseudocode for Treed LMMH Initialize the global constant a , the state variables φ0 Iterate until convergence , Iterate t over the conjunction of paths z = ( p , j ) in the data , Iterate over all node pairs ( pv , jw ) in frontier Fi , indexed by k = 1 , . . . , M . Note that ( k − 1 ) is M from ( t − 1)’th iteration , when k = 1 and t > 1 . For 1 ’s t iteration with k=1 , ( k − 1 ) would be treated as record id and the corresponding parent node state variable as 1 . We estimate the state parameter for key k in frontier Fi as shown below .
//Rolling up the stats and computing the state parameter at frontier : ff
( Fi : k − 1 , data , Sz , E∗ z )
 ( Fip : k , Sz , E∗
→ z ) //ancestor stats
( Fi : k , Sz , E∗ z ) z ) //predecessor stats ff  ( Fi : k , Sz , E∗ Fi:k is computed for key Fi : k usingP Sz,P E∗
( Fi : k , Sz , E∗ z )
→ ( Fi : k , φt
Fi:k ) z /φt−1 Fi:k .
1 ( Fi : k , φt−1 Fi:k )
M ap :
Reduce : where , φt
//Joining the latest state parameter ( and discounting the older ) with data to update expected succ , E∗ z :
M ap :
Reduce :
( Fi : k , data , Sz , E∗
( Fi : k − 1 , data , Sz , E∗ z ) → ( Fi : k , data , Sz , E∗ z ) Fi:k ) 1 ( Fi : k , φt−1 z ) 1 ( Fi : k , φt Fi:k ) → ( Fi : k , data , Sz , E∗ Fi:k/φt−1 z φt Fi:k ) and local corrections , we can estimate λp,j and λFk manner as mentioned in Section 22 p,j in a sequential
Temporal smoothing with Kalman filter– This needs to be scalable and faster due to massive amount of events ( of the order hundreds of millions ) at each epoch , and to update the statistics in large number of cold start cells in the data . We provide a scheme to do temporal smoothing combined with hierarchical smoothing at each epoch interval in a map reduce framework . We shall update the baseline DTt(xu , xp , xj ) , with stats from epoch t−1 data , and
1365 compute the residuals λ(xu,p),j,t through global , and local fitting procedures described above on successes and expected successes from DTt(xu , xp , xj)η(xu,p),j,t−1 . Its suffice to run a very few iterations ( typically 1 to 2 ) to estimate the residuals and these are quickly learnt over just the epoch data and not the offline training data .
4 . RELATED WORK
There is a rich literature in statistics on multi level hierarchical model that is directly related to our work[4 ] . These methods have been applied to single hierarchies and for small problems and are generally referred to as nested random effects model . Reliable rate estimation by exploiting hierarchical correlations for large scale computational advertising applications was considered in our earlier [ 2 ] paper but only for single hierarchies . It works only for Gaussian response and fails to work on digraphs , which is the case in our data ( campaigns and ads form a digraph ) . Hence , no comparison to this method is possible for our data . Of course our previous method called LMMH in [ 2 ] is the most closely related . As mentioned earlier , we generalize it in two directions — localized corrections and time smoothing through a Kalman filter . In machine learning,[16 ] also considered such a problem for a single class problem when predicting species distribution by geographic location but also for single hierarchy . They considered model parsimony by assuming L1 prior through a centered parameterization . However , their application was not large scale compared to the datasets we illustrate in this paper and like LMMH , it was a global hierarchical model with no time smoothing .
Decision tree induction of course is an old and well established data mining technique [ 6 ] . However , we are not aware of prior work that use LMMH style model based smoothing and pruning . The idea of capturing non linearity through local models via decision tree is also not new and have been pursued extensively to build models like treed linear regressions(see [ 14 , 25 ] and references therein ) . However , application of such an idea for recommender problems by localizing a relatively complex model like LMMH is new . Building non linear models at terminal nodes of a decision tree have also been pursued in statistics[10 ] but only for small problems ( data fits into memory of commodity PC ) , these methods do not scale to our data and they do not consider high dimensional , hierarchical categorical variables . Building decision trees for large applications in a map reduce framework have been also pursued in recent work [ 24 ] but these models did not consider hierarchical corrections as we do in this paper .
5 . EXPERIMENTS
While TTreed LMMH is applicable to many recommender problems , we will illustrate on display advertising . The main goal in display advertising is to increase reach and brand awareness by targeting user segments that advertisers are interested in buying . In recent times , ad exchanges have bought new levels of efficiencies to the online display ad market , making is easier for marketers to find the opportunities they need at the right price . Like a stock exchange , an ad exchange provides a unified marketplace to connect buyers ( advertisers ) and sellers ( publishers ) in an automated fashion , eliminating the need for marketers to book inventory through a tedious and often manual process that involves interacting with several non compatible systems . The exchange enables this transparency by selecting a winner for every single opportunity through a real time auction .
In this paper , we analyze data obtained from Right Media Ad exchange . In Right Media , ad campaigns can participate using differ ent pricing types like CPM(Cost per millie , ie , cost for a bundle of 1000 pageviews ) , CPC ( cost per click ) , CPA ( cost per conversion ) and dCPM ( dynamic CPM ) . Dynamic CPM is an innovative pricing model that allows advertisers to buy clicks/conversions in bulk without taking as much risk as in CPM . An advertiser specifies a CPM bid but in addition also specifies a conversion/click goal the campaign should meet . The fundamental question is — given different pricing types that specify advertiser willingness to pay in different currencies , how should one conduct an auction in a unified way ? . The solution currently pursued in Right Media is to convert all pricing type specification into a single currency called expected CPM ( eCPM ) . For instance , if the click rate is .01 and the bid per click is $1 , the eCPM is 1000 × .01 × 1 = $10 . Thus , the statistical modeling problem here is to construct methods that can estimate click/conversion rates to maximize revenue in the eCPM based auctions . One important thing in this application is the need to obtain accurate estimate of absolute probabilities , this makes the estimation problem more challenging . We only present results on click data , conclusions for conversion data modeling were qualitatively similar and hence omitted . 5.1 Dataset
We created a CLICK dataset from RightMedia consisting of events for a period of 9 days , where each event consists of a binary response which is a success if a user clicks on the ad shown . The dataset has approximately 33.38B events in training and roughly 7.77B in test . Features on user include geo , demographics , on ad it includes ad type , ad size_id , binned recency and frequency information on when and how many times ads were displayed to each user . It also consists of two hierarchical attributes , namely , publisher hierarchy and an advertiser hierarchy . The former has four levels ( publisher_manager_id → publisher_id → site_id → section_id ) while the latter has three levels ( advertiser → campaign → ad_id ) . We note that the advertiser hierarchy is not a strict tree but a DAG , a single ad can participate in multiple campaigns for instance , whereas the publisher hierarchy is a strict tree . We added a root node to the advertiser hierarchy to capture the publisher popularity variations in the data . Popularity on the ad elements are already captured by publisher_manager_id which is a coarse categorical variable and consists of few values .
For temporal smoothing , we created 12 chunks of test data sorted by event arrival times . Each chunk had more than 300M events , a few go as high as 1B events during peak hours of ad serving . 5.2 Methods in Comparison
We describe variants of our LMMH models that were fitted on CLICK dataset . Decision Tree was induced on the non hierarchical attributes with a stopping criteria of 3 minimum success at the node . The tree had roughly 6.2M leaves , we chose 1.4K frontier nodes where at least one child is below the sample size threshold of 10M tries . With sparse click rates , it is important to have a higher threshold for building reliable local LMMH at the frontier . Note that for each train record , we can either compute 1 ) gammapoisson smoothing to compute click rates for the baseline decision tree model or 2 ) smoothen click rates using LLDT variant that is based on spike and slab prior , the latter is effective in pruning the tree and retains about 2M nodes in the tree , a significant reduction from the original 6.2M leaves . We refer to these two feature based baseline models as Base.DT and Base.LLDT respectively . With smoothed click rate estimates from the baseline , we compute clicks , expected clicks and aggregate over the hierarchical features to estimate the state parameters . The different methods of estimating corrections include : 1 ) Global LMMH ( global.LMMH ) as
1366 in [ 1 ] , which aggregates the stats to the root level of the tree using global fitting procedure , 2 ) local LMMH ( local.LMMH ) , does the aggregation at frontier nodes as described in Algorithm 1 , and 3 ) Treed LMMH ( treed.LMMH ) where we do sequential estimation of both global and local corrections . Note that we append the prefix DT or LLDT to the above three LMMH variations depending on the baseline model used .
For the time smoothed models , we do not rebuild the decision tree at each epoch . The decision tree structure is updated at larger lags(daily ) . However , during each epoch we do use the additional data to update the probabilities at the leaf nodes of the decision tree giving us a better baseline model . The corrections at each epoch are updated according to our Kalman filter algorithm . We attach the prefix "k" to each of the Kalman variations depending on the LMMH model used to update the parameter φ during epochs . Thus , our naming convention is k.[model during epochs.][model during training]DT For instance , kglobaltreedDT indicates we perform a global LMMH smoothing using the Kalman filter on a treed LMMH model trained offline with DT baseline model . It is reasonable to use a simpler LMMH during Kalman smoothing since residual deviations may not be too localized , a more localized model in the training phase can still help during early epochs where it provides a good initialization .
For all the LMMH models , we ran the 2 component Gamma ( ie we choose P = .5 ) for which some of the φ are estimated as 1 and hence could be removed when storing the model in ad servers for online scoring . Tuning parameters for smoothing ( eg a ) are selected through cross validation . For LMMH , this parameter has an intuitive interpretation as pseudo number of successes , and we have found that it is enough to consider values in the range of 1510 Specifically for treed.LMMH we tried several combination of a values for local and global correction . We report the results on a=10 and a=1.5 in global and local corrections respectively , this curtailed the impact of global corrections "washing away" local corrections and was found to be the best in several of our offline experiments . We also point out that the 2 component prior always does effective smoothing for each value of a ( even if small ) through an inbuilt hypothesis testing procedure , hence our LMMH models are less sensitive to mis specification in values of a .
Logistic Regression– For this variation denoted by Log III(for consistency with terminology in [ 1] ) , we included the main effects of all features in the dataset . There were about 528K such features . We augment these features by adding paths of lengths > 1 on both the publisher and advertiser hierarchies . Approximately 2.144M features got added . We employed the hashing trick proposed for massively large scale regression problems [ 26 ] , and included roughly 500K hashbins of conjunctions of publisher and advertiser hierarchy features . Note that the actual number of conjunctions were of the order 385M , increasing the number of hashbins beyond half a million provided small gains and prolonged computing time . The logistic regression was fitted in a map reduce framework using conjugate gradient method ( CG ) along with L2 regularization on the coefficients to ensure stable model fitting . The regularization parameter was selected through cross validation and the maximum number of CG iterations was set to 50 . Other simpler versions of logistic regression were shown to be inferior to global LMMH in [ 1 ] , hence we include the best logistic regression competitor in our experiments .
5.3 Metrics
Since obtaining the absolute estimates are important in our display advertising application , we report on average test loglikelihood under Bernoulli model as our prediction accuracy measure . Specif
Figure 1 : Prediction accuracy based on lift in log likelihood relative to Base.DT ically , for a model the average test log likelihood avgLL is
P k(Succk ∗ log(ˆpk ) + ( T riesk − Succk ) ∗ log(1 − ˆpk ) )
P k T riesk
We report percent improvement in log likelihood of a method relative to our Base.DT baseline . ie , avgLL(Model ) − avgLL(Base.DT )
|avgLL(Base.DT)|
Further , we split test data into 40 equal parts and report the distribution of log likelihood lifts for each method to provide a measure of statistical variation in test set metrics . We note that in a large scale system like ours with billions of events , a small percentage improvement that is statistically significant ( we almost always find statistical significance with such large sample size ) , the improvements are practically significant as well . The impact of small percentage accuracy improvements when applied to billions of auctions on a daily basis has a significant impact on both revenue and accuracy of the system . We also note that prediction accuracy of models although a crucial component is not the only factor that affects performance . Other factors like bids , budgets,explore/exploit scheme , pricing models , campaign management , intermediaries involved in the exchange , etc all play a crucial role in determining the final performance . In this paper we isolate ourselves to studying just the prediction model , final measure of how improvements in accuracy translate to revenue gains is only possible after the methods get deployed . We are in the process of working towards such a deployment .
For the time smoothed models , we measure the model performance at t using the updated model ( baseline and corrections evolved ) from t − 1 epoch data . We report the LL lifts at each epoch on our models . We compare the time smoothen models with offline models ( no time smoothing ) , by joining the predictions from timesmoothen models with 40 chunks of test data created previously , and report the LL lifts over baseline BaseDT
1367 localized Treed model provides an effective strategy to capture heterogeneity and higher order interactions , this is coupled with fast online updates using a lightweight Kalman filtering algorithm that significantly improves performance . Moreover , despite the flexibility we are able to control the memory footprint of our models by using a model based pruning strategy based on spike and slab prior . Together , this provides an effective strategy for large scale applications like display advertising studied in this paper . We periodically refresh our offline models ( eg everyday using last 30 days data ) , between the expensive offline update cycles we take recourse to lightweight online updates to keep our models adaptive and account for the dynamic behavior of our system . The small memory footprint enables us to store the models in adservers distributed across different geographic co locations and provides efficiency .
The improvements through localization in general depends on the homogeneity of partitions induced and data sparsity . This depends critically on the availability of good features . In recommender applications , higher order interactions are expected . It is also typical to find several hotspots with large sample size to estimate such higher order interactions . In applications with weakly predictive features and extreme sparsity , the localization may not provide significant benefits . The improvement through Kalman filtering depends on the dynamic nature of the system . For relatively static applications , fast online updates may not provide significant improvements .
The selection of frontier nodes based on sample size threshold although attractive from a practical perspective is not principled . How to perform such a selection in a model based way maintaining computational efficiency is an interesting research problem . For Kalman filtering , we are currently investigating procedures to automatically select the best model for online update per epoch . It is also interesting to investigate computationally scalable methods that induce local partitions even when features are absent/weakly predictive . In particular , constructing latent factors based on matrix factorization to obtain features that can then be used to partition our space in a better way is a promising direction .
7 . REFERENCES [ 1 ] D . Agarwal , R . Agrawal , R . Khanna , and N . Kota .
Estimating rates of rare events with multiple hierarchies through scalable log linear models . In KDD ’10 , pages 213–222 , 2010 .
[ 2 ] D . Agarwal , A . Z . Broder , D . Chakrabarti , D . Diklic ,
V . Josifovski , and M . Sayyadian . Estimating rates of rare events at multiple resolutions . In KDD ’07 , pages 16–25 , 2007 .
[ 3 ] D . Agarwal , B . C . Chen , P . Elango , N . Motgi , S . T . Park ,
R . Ramakrishnan , S . Roy , and J . Zachariah . Online models for content optimization . In NIPS , pages 17–24 , 2008 .
[ 4 ] A.Gelman and JHill Data Analysis using
Regression/Multi level Hierarchical Models . Cambridge University Press , 2007 .
[ 5 ] J . Besag . On the statistical analysis of dirty pictures . Journal of the Royal Statistical Society , Series B , 48:259–302 , 1986 .
[ 6 ] L . Brieman , J . Friedman , R . Olshen , and C . Stone .
Classification and Regression Trees . Wadsworth , 1984 .
[ 7 ] A . Broder . Computational advertising . In SODA ’08 :
Proceedings of the nineteenth annual ACM SIAM symposium on Discrete algorithms , pages 992–992 , 2008 .
[ 8 ] A . Broder . Computational advertising and recommender systems . In Proceedings of the 2008 ACM conference on Recommender systems , pages 1–2 . ACM , 2008 .
Figure 2 : Prediction accuracy of time smoothen models on lift in log likelihood relative to Base.DT over epochs
5.4 Results
Model size : Number of state parameters to be estimated were 385M , 1004M and 1389M in case of global.LMMH , local.LMMH , and treed.LMMH methods respectively . Note that number of state parameters were same across DT and LLDT variations of LMMH models . However we note that our 2 component models with spike and slab prior retained only 3.4M , 7.39M ,and 6.35M estimates respectively and pruned the rest . This makes it easier to store models in memory .
Offline models : For all models , we report the LL lifts over Base.DT in Figure 1 . First , we note that baseline prediction accuracy is similar in case of Base.DT and Base.LLDT , however the latter had much fewer parameters and hence is preferred in our applications . For offline only models , logistic regression was significantly worse than all our LMMH variants with only 7.6 % LL lift over BaseDT Both local and Treed LMMH had better performance than global LMMH , and the differences are statistically significant .
Kalman models : Performing online Kalman filter corrections significantly improves performance of all LMMH variants . In fact , a global LMMH correction in the Kalman filtering stage is the best . Global corrections applied to Treed LMMH provides a good initialization and ends up with the best performance . This shows the effectiveness of good offline initialization through Treed LMMH coupled with fast online Kalman filter updates . Inspecting the improvements over epochs from Figure 2 reveal interesting patterns . While kglobaltreedDT dominates kglobalglobalDT in the first 6 epochs , the performance of the former deteriorates in the later epochs . This is because the local corrections get stale after large number of epochs and offline Treed LMMH needs to be re learnt for conducting fast online updates for the next few epochs .
6 . DISCUSSION
We provided an effective and computationally scalable modeling strategy to estimate rare rates in very high dimensional problems for data that are categorical but organized hierarchically . Our epoch interval % LL lifts over BaseDT005010015020!!!!!!!!!!!!01234567891011Model!kglobalglobalDTkglobaltreedDT1368 [ 9 ] Y . Chen , D . Pavlov , and J . F . Canny . Large scale behavioral targeting . In KDD ’09 , pages 209–218 , 2009 .
[ 10 ] H . A . Chipman , E . I . George , and R . E . McCulloch . Bayesian treed models . Machine Learning , 48:299–320 , 2002 .
[ 11 ] A . Das , M . Datar , A . Garg , and S . Rajaram . Google news personalization : scalable online collaborative filtering . In Proceedings of the 16th international conference on World Wide Web , pages 271–280 . ACM , 2007 .
[ 12 ] J . Dean and S . Ghemawat . Mapreduce:simplified data processing on large clusters . In Sixth Symposium on Operating System Design and Implementation , pages 137–150 , 2004 .
[ 13 ] DGClayton and JKaldor Empirical bayes estimates of age standardized relative risks for use in disease mapping . Biometrics , 43:671–681 , 1987 .
[ 14 ] A . Dobra and J . Gehrke . Secret : a scalable linear regression tree algorithm . In KDD ’02 , pages 481–487 , 2002 .
[ 15 ] R . O . Duda , P . E . Hart , and D . G . Stork . Pattern
Classification ( 2nd Edition ) . Wiley Interscience , 2001 . [ 16 ] M . Dudik , D . M . Blei , and R . E . Schapire . Hierarchical maximum entropy density estimation . In ICML ’07 : Proceedings of the 24th international conference on Machine learning , pages 249–256 , 2007 .
[ 17 ] W . DuMouchel and D . Pregibon . Empirical bayes screening for multi item associations . In KDD ’01 , pages 67–76 , 2001 . [ 18 ] U . M . Fayyad and K . B . Irani . Multi interval discretization of continuous valued attributes for classification learning . In Proceedings of the 13th International Joint Conference on Artificial Intelligence , pages 1022–1027 , 1993 .
[ 19 ] J . H . Friedman , T . Hastie , and R . Tibshirani . Response to mease and wyner,evidence contrary to the statistical view of boosting . Journal of Machine Learning Research , 9:1–26 , 2008 .
[ 20 ] GMFulgoni and MPMorn How online advertising works:wither the click ? Empirical Generalizations in Advertising Conference for Industry and Academia , 2008 . [ 21 ] H . Ishwaran and J . Rao . Spike and Slab Variable Selection :
Frequentist and Bayesian Strategies . Annals of Statistics , 33(2):730–773 , 2005 .
[ 22 ] R . Kalman . A new approach to linear filtering and prediction problems . Journal of Basic Engineering , 82(1):35–45 , 1960 .
[ 23 ] R . Kohavi and R . Quinlan . Decision Tree Discovery .
Handbook of Data Mining and Knowledge Discovery,Oxford University Press , 2002 .
[ 24 ] B . Panda , J . S . Herbach , S . Basu , and R . J . Bayardo .
PLANET : Massively parallel learning of tree ensembles with MapReduce . In VLDB’09 , pages 1426–1437 , 2009 .
[ 25 ] D . S . Vogel , O . Asparouhov , and T . Scheffer . Scalable
Look Ahead Linear Regression Trees . In KDD’07 , pages 757–764 , 2007 .
[ 26 ] K . Weinberger , A . Dasgupta , J . Langford , A . Smola , and
J . Attenberg . Feature hashing for large scale multitask learning . In ICML ’09 : Proceedings of the 26th Annual International Conference on Machine Learning , pages 1113–1120 , 2009 .
[ 27 ] B . Zadrozny and C . Elkan . Learning and making decisions when costs and probabilities are both unknown . In KDD ’01 , pages 204–213 , 2001 .
1369
