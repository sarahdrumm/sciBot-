Multiple Domain User Personalization
Yucheng Low
Carnegie Mellon University Pittsburgh , PA 15213 , USA ylow@cscmuedu
Deepak Agarwal Yahoo! Research
Santa Clara , CA 95051 , USA dagarwal@yahoo inc.com
Alexander J . Smola
Santa Clara , CA 95051 , USA
Yahoo! Research alex@smola.org
ABSTRACT Content personalization is a key tool in creating attractive websites . Synergies can be obtained by integrating personalization between several internet properties . In this paper we propose a hierarchical Bayesian model to address these issues . Our model allows the integration of multiple properties without changing the overall structure , which makes it easily extensible across large internet portals . It relies at its lowest level on Latent Dirichlet Allocation and on latent side features for cross property integration . We demonstrate the efficiency of our approach by analyzing data from several properties of a major internet portal .
Keywords Latent Dirichlet Allocation , User Profiling , Domain Integration , Parallel Statistical Inference
1 .
INTRODUCTION
The rapid growth of the World Wide Web results in a tremendous amount of information available to people . To prevent information overload , many collaborative filtering and recommendation systems [ 1 , 19 , 8 , 3 , 2 ] have been developed to filter the information stream and display only information the user may be interested in . Such methods have been successfully deployed in various settings ranging from movies ( Netflix ) to music ( iTunes , last.fm ) and books ( Amazon ) .
A problem with existing recommendation systems is that they generally require some amount of user interaction to be recorded before personalization is possible.1 For instance , a movie recommendation system will need a user to provide preference information for some movies , before it is possible to provide personalized recommendation of other movies ; likewise , a news portal will require a number of page views before an interest profile of the user can be constructed . In the meantime , the user will experience poor recommendation results . This is known as the cold start problem . Many methods have been developed to tackle this problem typically by making use of additional information about the user ( such as age , gender and occupation ) [ 2 ] . A variation of the cold start problem is where a new item is added to the system . In this case there is typically a decent amount of prior information available about the new item and other
1User could be either a browser cookie or registered Yahoo! id . No personally identifiable user information was available in the data analyzed . feature based methods have been developed that performs well [ 13 , 17 ] .
In this paper , we propose a solution to the cold start problem by combining user information across multiple domains(properties ) . We construct a hierarchical Bayesian model which integrates recommendation systems over multiple different domains ( such as the frontpage of an internet portal and its news site ) and makes use of the user ’s interactions in one set of domain to immediately provide a personalized experience in other domains the user may have never interacted with . Note that the domains may operate over different “ vocabularies ” . For instance , this model could be used to infer that a person interested in sports news ( at a news webpage ) is also interested in buying sports apparel ( at a store ) even if no metadata matching is provided for the store items . Our contributions include :
• A novel graphical model formulation of the user personalization problem which is easily extensible and allows integration over an unlimited number of domains . • A scalable inference procedure which allows the model • Demonstrating excellent predictive performance on a to scale to millions of users . large real world dataset .
Outline : We begin with a background overview of related hierarchical graphical models used for text analysis . Next , we provide a detailed description of the estimation and the model associated with it in Sections 3 and 4 . This is followed in Section 5 by details on how to implement the estimator efficiently . Experimental results are reported in Section 6 and we conclude with a discussion and an outlook towards more advanced user modeling techniques .
2 . BACKGROUND
Latent variable graphical models have found great success in text analysis applications ; of which Latent Dirichlet Allocation ( LDA ) [ 4 ] is probably the most well known model . we provide a brief overview of LDA here since our proposed model has LDA as a sub model . 2.1 Latent Dirichlet Allocation
Latent Dirichlet Allocation [ 4 ] aims to extract “ semantically valid ” topics from document collections . Each topic is simply a distribution over words in a fixed vocabulary . For instance , Table 2.1 gives a few plausible examples of LDA topics . The size of the word represents the probability of the word in the topic .
Topic 1 Topic 2 Topic 3 basketball NBA hoop train 3 point golf hole on one Tiger Woods club learning network latent machine neural train
Table 1 : Example of LDA topics . Size of word represents the probability of the word in the topic .
Figure 1 : LDA Bayes Net
In LDA , each document is represented as “ bag of words ” where the sequence of words , as well as the number of occurences of each word , is ignored . The aim of LDA is then to infer for each document , what is its distribution over topics ( is this document about both basketball and golf ? Or is this document about machine learning? ) , as well as the contents of each topic .
Letting ϕt be the word distribution of topic t , the gener ative procedure is as follows :
1 . For each topic t :
( a ) Generate the topic >word distribution ϕt ∼ D(β )
2 . For each document i :
( a ) generate its distribution over topics θi ∼ D(α ) ( b ) For each word wi,j in document i : i . Pick the topic of the word : zi,j ∼ MN ( θi ) ii . Pick the word : wi,j ∼ MN ( ϕzi,j ) The equivalent Bayes Net is provided in Fig 1 . Where α and β are fixed constant priors .
Intuitively α is the “ smoothness ” of document ’s topics ( low α ’s allow for more variability of topics across documents ) . While β is the “ smoothness ” of the topic >word distributions ( low β ’s allow more variability of words across topics ) .
The typical inference procedure used is the collapsed Gibbs Sampler described in [ 7 ] where θ is integrated out . [ 20 ] describes an equivalent sampler which is significantly faster through careful choice of data structures and creative rearrangement of the conditional probabilities . 2.2 Dirichlet Multinomial Regression
[ 11 ] provides an extension to the LDA model called the Dirichlet Multinomial Regression ( DMR ) model which allows conditioning on arbitrary document features . For instance , we could learn a topic model which includes the authors of the document as features . This allows us to make
Figure 2 : Dirichlet Multinomial Regression predictions of the form “ Author X prefers to write documents about topics Y ” .
The Bayes Net of the model is provided in Fig 2 . The DMR model extends the LDA model by letting the topic prior α be a parameter which is computed from the document feature vector x . Where λ is a matrix of size #features by #topics ,
α = exp(λ × x ) .
Essentially , xi is a ( possibly sparse ) document feature vector for document i , and λ is a matrix which projects the feature vector into the “ topic space ” . The exp is used to ensure positivity of α .
The inference procedure is straight forward : Fixing λ , the remaining model is identical to LDA , allowing the standard LDA Gibbs sampler to be used on z and ϕ . λ however is difficult to sample , and [ 11 ] optimizes it using L BFGS .
3 . MODEL 3.1 Data Integration
Data integration between different sites is a key problem and challenge for large internet portals . For instance , for a content provider such as Yahoo , Google or Microsoft , serving specialized content sites such as search , news , advertising , a portal , instant messaging , photos , video , cars , dating , etc . it is highly desirable to be able to leverage information from one site on the other . Some of the benefits from such an approach are :
• We can use information gained about a user on one site to provide personalized content right from the beginning on a second site , too . This addresses issues with the cold start problem . • We can use such information for improving the overall recommendation accuracy even for users which visit several sites frequently . • Cross property information helps with abuse detection . • Personalization can be used for recommending other properties .
Naively we could simply attempt to address the problem by having one joint set of user specific features that are shared by all properties and which are modified based on the user behavior . However , this approach is problematic for a number of reasons :
1 . Users may exhibit different site specific behavior which may only partially relate to the big picture about them . tokens Wdu ⊆ Wd . Note that the space Wd can be different from domain to domain . For instance , in the News domain , this could be the text of the documents viewed , while in the Movies domain , this could be the list of movies rated . Additionally , let Du denote the set of domains the user interacted with . Note that the number of domains a user interacts with is variable . So is the number of interactions per site .
The task is to predict for each user his preferences in all domains — for those he visited we would like to improve the estimates , for those he never saw before we would like to obtain a good estimate of his future behavior . The latter is clearly the more difficult task as we study the case where we have no information regarding a user ’s behavior on , say , a news site at all . Hence , we will focus on the cold start problem while noting that the multiple site personalization problem is contained in it . Formally , for each user u ∈ U , predict Wdu where d /∈ Du .
As measures for the accuracy of the estimator we use a ) the increase in log likelihood by combining several domains , ie the increase in statistical prediction accuracy by combining several domains ; b ) the increase in the cosine similarity score when using several domains for improved personalization . Before we discuss the design of the graphical model associated with Figure 3 let us evaluate some alternatives . 3.3 Alternatives
Joint parameter vector : domain be S
Inferring two or more domains jointly is difficult due to the different domain spaces Wd . The simplistic solution of combining all the domains by letting the d∈D Wd is undesirable as it restricts the model excessively . Such a model will not deal well with often rather different amounts of data per domain . For instance , if we naively combine the Movie and News domains by using article headlines for the News domain , and movie titles for the Movies domain , the model would infer that a person interested in Sports news , must also be interested in Sports movies , and a person interested in Political news must also be interested political documentaries . The model will have difficulties modelling users who are interested in Political news and Action movies since it ignores the fact that users may exhibit different personalities on different properties . Secondly , if we combine search queries and page views we might end up with several orders of magnitude more tokens in the case of page views . Simply combining both datasets would give queries only a minuscule weight and consequently underestimate their importance as relevant user features .
Joint action space : Another possibility is to combine the domains in a way such that tokens from one domain do not interfere with tokens from the other domain . For instance , this could be done in the Movie and News example by append the numeral “ 1 ” to every word in the movie title , and appending the numeral “ 2 ” to every word in the news title . Forcing the “ dictionary ” of both domains to be different , but inferring a joint model naively will resolve the case described in the previous paragraph , but still will not lead to a desirable solution . There are several issues .
• The “ model complexity ” of the domains could be
Figure 3 : The highlevel structure of the proposed hierarchical statistical model . All properties share ( and update ) a joint profile vector . In addition to that , each property may keep a local model of the user ’s behavior in order to adapt to the possibly idiosyncratic activities of a user on a given site .
For instance , a user may well fancy himself as an expert in medieval numismatics on an answering site , yet at the same time he may exhibit some more mainstream preferences on the other sites . This means that we need to allow for the possibility of rather diverse and incoherent behavior between sites , and thus additional site specific personalization , in our model .
2 . It must be easy to add sites to the profiling system without the need to re engineer the model of the already integrated sites . This is a practical imperative since in reality internet properties are launched , bought or sold and our model must accommodate these requirements efficiently .
3 . The framework needs to allow for different levels of sophistication in the model across different sites . That is , while some properties might only be willing to use a simple demographic targeting approach , others may be open to using an advanced nonparametric Bayesian approach such as Latent Dirichlet Allocation [ 4 ] .
In this paper we propose a flexible model that addresses all of these concerns in a principled fashion , namely by means of directed graphical models [ 14 ] . Its main structure is given in the diagram of Figure 3 .
There are two basic use cases for such a model . In the first use case , each domain already has an existing personalization system . This model could then be used to provide for each user an “ initialization prior ” allowing personalization on the first visit to a new domain . In the second use case , the existing personalization system can be directly integrated into the hierarchical model , allowing personalization in different domains to be inferred jointly , potentially improving performance . The second use case is of course preferred . 3.2 Formal Definition In the following we denote by U the set of users in the system and let |U| be the number of users . Furthermore let D be the set of personalization domains , such as News , Answers , Frontpage or Movies . Each user u ∈ U interacts with one or more domains d ∈ D producing a sparse list of globaluser profilelocaluser profilelocalactivityNewslocaluser profilelocalactivityAnswerslocaluser profilelocalactivityFrontpageupdateupdate very different . For instance , the number of tokens in the News domain could be significantly greater than the number of tokens in the Movie domain . Inferring them jointly will cause the latent factors will be largely dominated by the News . Moreover , the problem of recommending movies might be considerably harder than that of recommending news , thus requiring a richer latent representation . • The modelling complexity could grow rapidly with the number of domains . For instance , we could observe that the News domain alone could be modelled with 10 latent factors , while the Movie domain could be modelled with 5 latent factors . However , when inferred jointly , the number of latent factors required could be much larger than 10+5 = 15 since there could be one group of users who are interested in Political news and Action movies , as well as another group of users who are interested in Political news and Horror movies . These would have to be modelled as different latent factors , resulting in potentially a multiplicative increase in the number of factors necessary to build an accurate model . This model therefore does not provide the scalability necessary to extend to a large number of domains .
Independent Parameter Vector : This model trivially solves both problems mentioned above , simply by modeling each domain on its own . However , such an approach entire defeats the purpose of integration . In the following we propose a modification of this model to allow for cross domain personalization .
3.4 Hierarchical Latent Model
Our model resolves these problems by adding an additional layer of latent parameters . We begin by assuming that each user has a hidden latent feature vector xu which completely describes the user ’s global preferences and interests . Each domain also has a hidden latent domain matrix λd . Furthermore we assume that for each property d the user u also exhibits local traits , say θdu . We assume that the local traits θdu characterize the actions Wdu that we observe . This yields the following generative model :
1 . For all properties d do
( a ) Sample trait vector λd
2 . For all users u do
( a ) Sample user trait xu ( b ) For all properties d do i . Sample θdu|xu , λd ii . Sample user actions Wdu|θdu
Y
Y
Y d u d,u
In other words , the joint distribution is given by p(λ , x , θ , W ) = p(λd ) p(xu ) p(θdu|xu , λd)p(Wdu|θdu )
This has the advantage that we can express the distribution of the global user vector in terms of its local parameter estimates only . Moreover , the distribution factorizes , that is we have p(xu|rest ) ∝ p(xu ) p(θdu|xu , λd ) .
( 1 )
Y d
4 . STATISTICAL FORMULATION
To make further progress we need to specify the distributions involved in p(λ , x , θ , W ) . Specifically we use Latent Dirichlet Allocation [ 4 ] ( LDA ) to describe p(Wdu|θdu ) and we subsequently employ a latent variant of “ upstream conditioning ” [ 11 ] to couple properties into a joint graphical model . We begin with an explanation of plain LDA . 4.1 Latent Dirichlet Allocation
One model for estimating user behavior is to assume that users have a number of interests and that their actions are given by a combination of said interests . For instance , if we had a model describing which websites are visited by users interested in politics and which websites are visited by users interested in automobiles then we could easily infer the activity pattern of a user interested in both topics , simply by mixing the topical interest patterns of both .
Such a model is considerably more flexible than , say , an attempt to cluster users : in the latter case we would need to generate clusters for each and every combination of interests . Even worse , interests can be expressed to various degrees , so a simple model clustering users as being interested in cars may not be sensitive enough to infer whether users are die hard car fanatics or whether they just exhibit a fleeting interest in automobiles .
LDA addresses this issue as follows : each user is endowed with a topic mixture θu that is drawn from some Dirichlet distribution Dir(α ) . Subsequently , for each user action , we draw a topic zui from the user specific topic distribution θu and finally , we draw an action wui from the action distribution associated with topic zui . The action distributions ϕk themselves are drawn from a Dirichlet distribution . The parameters α and β are priors that adjust how nonuniform the topic distributions and the word distributions are . We obtain [ 4 ]
Y p(w , z , θ , ϕ|α , β )
= p(ϕk|β ) p(θu|α )
Y k u
Y i∈Wu p(zui|θu)p(wui|zui , ϕ )
( 2 )
Large scale inference for ( 2 ) presents a significant challenge and there are only few published results on how to apply LDA to millions of instances ( ie users ) . We use the approach of [ 18 ] for distributed collapsed inference and build on this framework . Before going into details regarding large scale estimation let us extend ( 2 ) in the spirit of Figure 3 . 4.2 Upstream Coupling
The key to combining different properties is to make the prior α depend on both the property and the global features of the user . That is , instead of fixing α we assume that αdu|xu , λd where xu is the global user feature and λd is the site specific trait .
More specifically , to interact with a domain , a nonnegative2 user domain feature vector αdu is produced by computing the logistic transfer function lgt(λdxu ) where lgt(y ) = log [ 1 + ey ] .
( 3 )
In other words , αdu is computed by multiplying the user ’s latent feature vector with the domain ’s latent domain
2Nonnegativity of the coefficients α is needed to encode a valid Dirichlet prior on the topic distribution .
Figure 4 : Left : Latent Dirichlet Allocation . This model covers the behavior of users u , as expressed in the actions wui on a single property . Right : Hierarchical Model with LDA applied to all properties . Each property has a characterization vector λd and each user is endowed with a global personalization vector xu . σd and ψ are priors for these personalization terms . matrix and passed through a non linear transform to enforce positivity . Note that the choice of lgt is somewhat arbitrary as there are many ways to project from ( −∞,∞ ) to [ 0,∞ ) . In [ 11 ] this is done by exponentiation . However , we found exponentiation to be difficult to work with as its steep gradients cause numeric instabilities in inference . Hence we chose lgt for its simplicity . If sparsity in α is desirable , other possibilities such as Huber or L1 loss can be considered .
The user domain feature vector is a description of how the user may interact with a given domain and can be interpreted freely by the domain specific model . As already outlined above we use LDA [ 4 ] as the domain specific model . However note that the design of Figure 3 does not require this assumption .
Essentially , the user ’s latent feature vector can be interpreted as a “ super topic ” in the same spirit as [ 12 ] . Each column in the latent domain matrix can then be interpreted as the distribution of topics inside the super topic . Multiplying these two latent parameters together will therefore produce a distribution over topics within the domain , which can then be used as a prior ( αdu ) for the actual topic distribution of the user ’s interaction within the domain .
Note that this model allows us to have different degrees of complexity in different domains : while the dimensionality of the global latent space is fixed via xu ∈ Rk we may have different numbers of topics on different properties via λdxu , simply by choosing different dimensions for different λd . This makes sense from a modeling perspective — some properties may only see small amounts of a user ’s activity ( eg it is unlikely that on a cars site we may care much about the interest of a user in medieval numismatics ) . In summary the generative process is as follows :
1 . For all properties d do
( a ) Generate latent domain matrix λd ∼ N ( 0 , σd )
2 . For all users u do
( a ) Generate latent feature vector xu ∼ N ( 0 , ψ ) ( b ) For all visited properties d ∈ Du do i . Let user domain feature vector be
αdu = lgt(λdxu ) ii . Draw user domain topic distribution
θdu ∼ D(αdu ) iii . for each observed token i ∈ Wdu A . Draw topic zdui ∼ MN ( θdu ) B . Draw word w ∼ MN ( φzdui )
The graphical model is in Fig 4 . We observe that if all user and domain latent parameters are known , ie if λ and x are observed , each domain essentially reduces to an instance of LDA . This therefore provides a relatively straightforward inference scheme .
Inference
4.3 Making the observation that once all λ and x are fixed , the model is essentially equivalent to having |D| independent LDA models , where users are documents , and where the topic prior is different for each document . We can therefore perform collapsed sampling in a similar manner as in [ 7 ] by integrating out θ and ϕ , and sampling z . In addition we sample x using Langevin diffusion . The λ parameter is optimized using L BFGS [ 9 ] . We describe ( the rather technical ) details below . They are not essential to understanding the experiments but they matter for the purpose of reproducibility of the results . After integrating over θ the complete likelihood P ( z , λ , x ) of the model is given by
Y Y d,u:d∈Du
Γ(P Γ(P −λ2 td td lgt(xT
1 lgt
2πσ2 k d,td,k
Y td lgt(xT u λd,td ) ) u λd,td ) + ndu )
!Y d,td,k 2σ2 k
1 lgt
2πψ2 k u,k
Γ(lgt(xT u λd,td ) + nt|du ) u λd,td ) ) ( 4 )
Γ(lgt(xT
!
− x2 u,k 2ψ2 k
Note that this is very similar to [ 11 , Eq ( 1) ] , but with userproperties instead of documents , and different choices for λ and x . Let dlgt(y ) := ∂y lgt(y ) be the derivative of the logistic transfer function . In this case the derivative of the log likelihood with respect to λp,tp,k , ie ∂λp,tp ,k log P ( z , λ , x ) is
αθuzuiwuiϕkβαduθduzuidwuidϕkdβdforalli∈Wuforallu∈Uforallkxuλdforallu∈Uforalld∈Dforalli∈Wudforallk∈Kdψσd dlgt xT u λd,td dlgt xT u λd,td
+ ndu
X “ u:d∈Du
"
Ψ given by − λd,td,k σ2 k
0@X “ td
+
“
X “
“
"
Ψ
0@X “ td xu,k dlgt xT u λd,td td
“ ” 1A − Ψ 0@X “ ” − Ψ “ 0@X ” 1A − Ψ ” − Ψ “ td
”
”
” × dlgt dlgt
“ “
” × “ “
+ Ψ dlgt xT u λd,td
+ nt|du xT u λd,td
+ Ψ dlgt xT u λd,td
+ nt|du dlgt xT u λd,td
We use this equation to optimize λ using L BFGS . The derivative ∂xu,k log P ( z , λ , x ) with respect to xu,k is similar : − xu,k ψ2 k
λp,td,k dlgt
X xT u λp,td
( 6 )
+ u:d∈Du td dlgt xT u λp,td xT u λd,td
+ ndu
( 5 )
1A
1A
” ” ” #
” ” ” #
This equation allows us to sample each xu,k using a Langevin Diffusion sampler . 4.4 Langevin Sampler
The Langevin Diffusion sampler [ 15 ] is essentially a Metropolis
Hastings sampler with a Gaussian proposal shifted in the direction of the gradient : Where the current value of xu,k is xt u,k ( sample value at time t ) , we propose the new value : yu,k = xt u,k +
σ2 2
∂ log P ( z , λ , x )
+ N ( 0 , σ2 )
˛˛˛˛xt u,k
∂xu,k |xt
Where the notation ∂ log P ( z,λ,x ) tial derivative of log P ( z , λ , x ) against the variable xu,k ( Eq ( 6 ) ) and evaluate it at the value xt means to take the par
∂xu,k u,k u,k .
We then compute the acceptance ratio :
‚‚‚‚‚yu,k − xt (  −‚‚‚xt
− r = exp exp u,k − σ2
2
∂ log P ( z,λ,x )
∂xu,k u,k − yu,k − σ2
2
∂ log P ( z,λ,x )
∂xu,k
˛˛˛˛xt u,k
|yu,k
‚‚‚‚‚2ffi ‚‚‚2ffi
2σ2
2σ2
) ff
Then update xt u,k to the proposed value yu,k with proba
( bility min(r , 1 ) : xt+1 u,k = yu,k with probability min(r , 1 ) xt u,k with probability 1 min(r , 1 )
5 .
IMPLEMENTATION
To run experiments on large datasets , it is necessary to make use of large scale distributed processing . We therefore implemented the model by extending the distributed LDA implementation from [ 18 ] . The extension is relatively straightforward due to the similarities to regular LDA . The main contributions we made to the implementation is a distributed L BFGS solver , as well as a novel way to initialize the distributed sampler which accelerates convergence significantly .
Algorithm 1 Sampling executed on each machine . for Each user u in this machine ’s partition do
Initialize u ’s latent feature vector for Each domain d ∈ Du the user interacted with do for Each observed token w ∈ Wdu do
Sample w ’s topic using the procedure in Sec 5.2 end for end for end for if I am Machine 0 then
Initialize latent domain matrices Store latent domain matrices into Memcached else
Initialize latent domain matrices from Memcached end if for i = 1 to #sampling iterations do \\ Optimize every 15 iterations Optimize latent domain matrices using if i mod 15 = 0 then distributed L BFGS end if for Each user u in this machine ’s partition do
Sample u ’s latent feature vector using
Langevin Diffusion
Sample the topic in each observed token in u end for end for
An overview of our approach is provided in Algorithm 1 . Firstly , the set of users are equally partitioned among the P machines in the cluster . Each machine then initializes each user with a random latent feature vector , and uses the Distributed Initialization procedure described in Sec 5.2 to provide the initial topic assignments to each observed token , producing local word topic counts . A set of memcached servers3 are used to synchronize the local word topic counts with the global word topic counts . See [ 18 ] for details on the communication between nodes . The latent domain matrix is initialized by having one machine generate the random matrix , writing it into memcached , and all remaining machines reading from it .
The sampler then proceeds as described in [ 18 ] , with the exception that the user ’s latent feature vector must be sampled , and the latent domain matrices are optimized periodically ( every 15 iterations in the pseudo code in Alg . 1 ) .
5.1 Distributed L BFGS solver
The latent domain matrices are optimized using a distributed L BFGS solver . We first observe that the gradient of the log likelihood of the model with respect to the domain matrices λ can be written as a sum over all the users in the dataset . This naturally means that the gradient can be computed in a fully distributed fashion , collected and summed on one machine , which then performs the actual L BFGS update . This procedure is however further complicated by the Armijo line search stage of the L BFGS solver , which requires the log likelihood and and the gradient to be recomputed repeatedly for different values of λ . This requires a close level of coordination among all the machines which
3memcached is a fast distributed ( key , value ) storage system is not immediately provided by the memcached framework.4 To implement this algorithm , we must first re implement three standard distributed programming primitives within the memcached environment . The three primitives are the barrier , sum and broadcast .
The barrier is a function which provides “ sequentialization ” of execution across a distributed program . When a process call the barrier function , the process will stop execution and pause . The barrier function only returns when all machines in the distributed system enter the barrier . A common use case for the barrier is to “ wait ” for all processors to complete a particular task , before going on to the next task . We implement a version of the sense reversing barrier algorithm described in [ 10 ] .
The distributed sum function ( or more generally also known as reduce or fold ) simply collects and aggregates a value posted by each machine . Each machine p calls the sum function with a particular value vp . The function then returns with the sum of all the values ( v1 +v2 +···+vP−1 +vP ) . We implement this operation by having each machine write its value vp to a memcached . A barrier is then called to guarantee that all machines have completed the write . Machine 1 then reads all the values from memcached , sums them and writes back to memcached . Another barrier is used here to ensure that the write is completed . Then all machines reads the resultant value from from memcached .
The broadcast operation allows one machine to send a value to all the other machines in the network . This operation is similar to the sum operation .
Combining these operations , we can write the distributed
L BFGS solver in pseudo code in Alg . 2 . 5.2 Distributed Initialization
The standard way to initialize an LDA sampler is to sample the topic of each new word from a uniform distribution . However , this results in slow convergence . A more effective way of initializing the sampler is to perform online sampling [ 5 ] . That is , the topic of each new word is sampled by conditioning on all the words seen so far . This procedure is easy when performed on a single machine , but is extremely complex to implement in a distributed setting as the global topic counts will need to be synchronized very frequently .
A plausible alternative is to have each machine perform the online sampling process independently using only the local topic counts . This however , produces poor initializations as each machine could assign very different topics to the same word resulting in a uniform topic distribution when the topic counts are aggregated across all the machines .
Instead , we propose a simple solution : We perform online sampling independently on each machine as described above . However , when we sample a topic for a new word w , instead of using a random/pseudo random generator , we use hash ( w + #occurrences of w seen so far ) as the random number .
This procedure is equivalent to constructing a different pseudo random generator for each unique word , and that 4One might suspect that the entire algorithm could be efficiently implemented in Hadoop MapReduce . However , one of the issues is that by deferring all inter process communication to the Reduce phase convergence of the sampling algorithm is severely hampered . Moreover , it creates artificial barriers to the sampling iterations which can lead to considerable performance penalties . For a detailed discussion see [ 18 ] .
Algorithm 2 Pseudo code of Distributed L BFGS Solver ran on each machine in the cluster
\\ compute the initial gradient and log likelihoods Compute the local gradient g Using Eq ( 5 ) Compute the local log likelihood l Using Eq ( 4 ) Perform distributed sum over g and l if I am machine 1 then
Compute L BFGS step ∆ using g and l Update λ broadcast λ else receive broadcast λ end if \\ Armijo Line search while True do
Compute the local gradient g Using Eq ( 5 ) Compute the local log likelihood l Using Eq ( 4 ) Perform distributed sum over g and l if I am machine 1 then
Test Armijo and Wolfe conditions using g and l \\ quit if armijo conditions are satisfied Let quit = 1 if Armijo conditions are satisfied Let quit = 0 otherwise . broadcast quit if quit then break else\\ conditions not satisfied . Perform line search
Decrease step size and update λ broadcast λ end if else receive broadcast quit if quit then
\\ If quit is true then Armijo conditions are satisfied break else\\ If not , line search proceeds \\ and we need to receive the new λ receive broadcast λ end if end if end while this generator is seeded identically across all the machines . This algorithm does not correspond to a true online LDA initalization , but it simply “ correlates ” the random number generator across all machines , encouraging similar topic distributions for each word . An advantage of this procedure is that it does not require any communication between the machines , allowing for a simple implementation .
6 . TWO DOMAIN EXPERIMENTS
We tested the model on a large real world dataset comprising a random subsample of 5.6 million users who interacted with Yahoo! News and Yahoo! Frontpage for a week . We record “ click through ” information for each user . That is to say , when the user clicks on a Frontpage link , we record the words in the link . Likewise , when the user clicks on a News article , we record the words in the article .
The goal is to predict how a new user might interact with
Frontpage , given the user ’s past interactions with News ( and vice versa ) . The problem is made particularly difficult since only 1 % of all the users in our dataset interacted with both Yahoo! News and Yahoo! Frontpage .
To perform holdout testing , we selected 20 % of the users who interacted with both domains and hide the user ’s interaction with one of the properties when training the model . The model can then be used to predict the user ’s interaction with the hidden property . Where αdu is the user domain feature vector , we note that the predicted word distribution P ( wdu|α ) is simply proportional5 to ϕ × αdu . We evaluate accuracy using the cosine similarity score between the predicted word distribution P ( wdu|α ) and the true observed word distribution wdu . wdu , ϕ × αdu |wdu| × |ϕ × αdu|
We tested against a simple baseline , which is the meanprediction formed by averaging the word distribution across all the user interactions within the domain . This intuitively represents the “ average ” user . We do not test against nearestneighbor methods as they require large amounts of computation to evaluate each new user and do not scale well to large dataset sizes . We also do not consider the method of combining all the domains into a single LDA model ( joint parameter vector and joint action space in Sec 3.3 ) to be viable models due to the significant difference in the domain types for the three domain experiment of Section 7 .
Using 100 topics to model FrontPage and 400 topics to model News , we plot in Fig 5 the model log likelihood P ( w , z ) across sampling iterations . We compare the likelihood curves obtained when varying the dimensionality of the latent feature vector . As expected , running independent LDA on both domains ( equivalent to using a latent feature vector of length 0 ) produces the lowest log likelihood value . Increasing the dimensionality of the latent feature vector from 0 to 25 allows the model to fit the data better , resulting in a higher log likelihood . Further increasing from 25 to 50 still produces a small increase in log likelihood , though we observe diminishing returns .
Next in Fig 6 we plot the % improvement in hold out accuracy over baseline prediction . We observe significant gains in holdout accuracy for both Front Page and News . Increased model complexity ( by increasing the number of latent features ) appears to overfit slightly , resulting in decreased hold out accuracy .
Fig 6 also suggests that it is easier to predict FrontPage preferences from News preferences than vice versa . This is expected since the size of News vocabulary is about 4 times larger than the size of the FrontPage vocabulary . Each interaction with a News article also produces significantly more tokens . Moreover , the range of stories on the FrontPage is considerably more constrained than the set of news events in general . Hence there is simply less information to be gained from knowing which FrontPage story a given user viewed . user i in the Frontpage , we can use P
Next , since λnewsxi is the topic “ preference ” for user i in the News domain , and λfpxi is the topic “ preference ” for x(λnewsx)(λfpx)T to estimate the correlation between the topics across News and
5This simple identity holds for Dirichlet priors in the absence of any additional data since it forms a conjugate prior to the multinomial mixture .
Figure 5 : Two Property FrontPage + News model log likelihood over iterations using 100 topics for FrontPage and 400 topics for News . Each curve corresponds to using a user latent feature vector of different lengths . The “ Independent LDA ” is the loglikelihood of running LDA independently on both domains and is equivalent to using a latent feature vector of length 0 . As expected , increased dimensionality of the latent feature vector results in increased log likelihood .
Frontpage . Using this , we can try to infer that for a person who is interested in a particular topic on Frontpage , what other topics might he be interested in when he goes to the News domain .
A minor problem with this procedure is that it can be hard to factor out strong “ background preferences ” . That is , a Frontpage topic which everyone is interested in , might appear to be strongly correlated with almost all other News topics . However , we can still obtain very interpretable results .
In Table 2 , we demonstrate this for two random selected topics in Frontpage , and their top correlated topic in News . For instance , the first row suggests that people interested in reading about science articles on Frontpage might also be interested in reading about Avatar ( a science fiction movie ) on the News website . And people interested in the Oscars might also be interested in reality TV shows .
In Table 3 , we demonstrate the reverse . For two randomly selected topics in News , what are their top correlated topics in Frontpage . The first row suggests that people interested in technology , might also be interested in college graduate earnings . In the second row , we encounter the strong “ background preference ” problem as stated earlier . Here we try to infer the Frontpage preferences of a user who is interested reading about the healthcare debate on News . If we simply look at the top correated topic , we might infer that the user is interested in reading about sports . However , including the second and third most correlated topics , we also see that the user might also be interested in banking and terrorism related articles .
7 . THREE DOMAIN EXPERIMENT
0100200300400500−45−4−35−3−25−2x 109IterationsLog LikelihoodIntegration Model 50 FeaturesIndependent LDAIntegration Model 25 Features FrontPage
News bacteria , fight , super , struggling , developed , doctors , resistant , lethal , virtually , drugs , antibiotic , competitors , chad , andrews , ochocinco , erin , whos , aas , batteries , stronger sandra , oscar , oscars , red , carpet , bullock , golden , gown , bullocks , nominee , bestactress , sparkles , stunning , retirement , saving , cost , taxes , expenses , fully , major , film , movie , movies , films , director , story , avatar , james , time , hollywood , big , make , hes , star , good , remake , horror , great , award , man vienna , bachelor , jake , pavelka , giraldi , finale , show , stars , dancing , love , season , time , abc , episode , tonights , question , popped , relationship , maintains , man ,
Table 2 : Given a particular frontpage topic ( left column ) , the top correlated news topic ( right column ) . For instance , the first row suggests that people interested in science might also be interested in Avatar ( a science fiction movie ) .
News
Frontpage iphone , apple , app , apps , ipod , google , store , apples , android , mac , mobile , touch , ipad , device , phone , screen , jobs , developers , iphones , time , health , care , bill , obama , president , rep , house , republican , senate , news , sen , democrats , fox , congress , reform , federal , majority , obamas , roberts , john college , year , earn , years , 000 , bestpaid , average , 129 , colleges , graduates , ten , alums , schools , actor , likes , prompt , spirit , wrench , time , ghost , drafts , player , nfl , bryant , pick , 23 , riskiest , peril , team , nba , james , news , cleveland , decision , scouts , talented , lebron , familiar , number , dez , home , tion , client , bank , rosalina , janitor , facing , bought , offices , ceo , gomez , cleaning , surprising , evicforeclosed , video
” inside , mountain , terrorist , observers , imcaptured , presses , alqaidas , complexity , base , features , hideout , size , special , secret , struck , sell , products , month , ways , arctic
Table 3 : Given a news topic ( left column ) , the top correlated frontpage topic ( right column ) . For the second row , the top three topics are provided .
We next extend the model further by including a third domain : MyYahoo . Here , we demonstrate the scalability and extensibility of the model . To our knowledge this is first attempt at user personalization across three different domains .
For the myYahoo domain , we only record the id ’s of the articles the user clicked on . This makes the problem significantly harder as the article id ’s have no semantic significance . We selected 5.6 million users from the same date range as the two domain experiment . About 5.5 % of the users interact with two or more domains . We selected 10 % of the users who interacted with two or more domains for holdout testing .
We trained the model on the dataset , using 50 latent features and plot the % improvement in holdout accuracy over baseline prediction in Fig 7 . We demonstrated improvement for both Frontpage and News , as well as an insignificant improvement in MyYahoo . This is to be expected due to the lack of semantic information in the MyYahoo domain and due to the comparatively small number of applications provided on MyYahoo .
Next , in Fig 7 , we compare the holdout accuracy for FrontPage and News between the 3 domain model and the 2 domain model . Each model was tested with 25 , 50 and 75 latent features and the best setting for each model was used . The 2 domain model required only 25 latent features and increasing the feature count further results in overfitting , while the 3 domain model continues to improve in performance up to 50 latent features . We observe that even with the limitations of the MyYahoo data , including the third domain still improves holdout performance significantly on Frontpage prediction ( while incurring a small drop for News ) .
8 . EXTENSIONS
The model is designed to be a highly generic model for user personalization , supporting a variety of possible extensions .
Firstly , the design of the model does not require all the user features x to be latent . Instead , we may combine our model with the upstream conditioning of [ 11 ] which immediately allows us to add observed variables . For instance , one could use geographical location ( via the IP address or via HTML5 ) as a fully observed user feature . Alternatively , the model also permits the use of features which are not observed for all users ( such as gender ) . The latter case is particularly interesting as the model would then try to infer the feature value for the remaining users . For instance , the model would try to infer the gender of user if the user ’s gender is not observed .
Next , the model does not require the use of LDA within a domain . The inference process is highly modular with regards to each domain , and permits any domain model to be used as long as a log likelihood derivative with respect to x can be computed .
Figure 6 : Two Property FrontPage + News Hold out accuracy improvement over baseline for different number of latent features .
Figure 8 : Comparing Frontpage and News holdout accuracy for the two domain model and the three domain model strated its efficacy on a large real world dataset comprising of data from Yahoo News , Yahoo FrontPage and MyYahoo , obtaining significant gains in prediction accuracy . The model we developed is highly extensible and observed user features such as geographical location and gender can be integrated . Furthermore , the model treats each domain in a modular fashion , allowing other generative user personalization schemes to be connected easily . Finally , we provide a highly scalable inference procedure with a novel initialization schemem , allowing the model to scale to millions of users easily .
10 . REFERENCES [ 1 ] J . Abernethy , F . Bach , T . Evgeniou , and J P Vert . A new approach to collaborative filtering : Operator estimation with spectral regularization . Journal of Machine Learning Research , 10:803–826 , March 2009 . [ 2 ] D . Agarwal and B C Chen . Regression based latent factor models . In JF Elder , F . Fogelman Souli´e , PA Flach , and MJ Zaki , editors , Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 19–28 . ACM , 2009 .
[ 3 ] J . Basilico and T . Hofmann . Unifying collaborative and content based filtering . In Proc . Intl . Conf . Machine Learning , pages 65–72 , New York , NY , 2004 . ACM Press .
[ 4 ] D . Blei , A . Ng , and M . Jordan . Latent Dirichlet allocation . Journal of Machine Learning Research , 3:993–1022 , January 2003 .
[ 5 ] K . R . Canini , L . Shi , and T . L . Griffiths . Online inference of topics with latent dirichlet allocation . In Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics ( AISTATS ) , 2009 . [ 6 ] T . Griffiths and Z . Ghahramani . Infinite latent feature models and the indian buffet process . In NIPS , 2005 .
[ 7 ] TL Griffiths and M . Steyvers . Finding scientific topics . Proceedings of the National Academy of Sciences , 101:5228–5235 , 2004 .
[ 8 ] Y . Koren , RM Bell , and C . Volinsky . Matrix
Figure 7 : Three Property FrontPage + News + myYahoo hold out accuracy .
• This could permit the model to provide cross domain personalization into unusual domains such as personalized spam filtering . • We could use a Gaussian latent matrix factorization model along the lines of [ 16 ] which replaces the discrete set of topics by a continuous ( low dimensional ) factorization . • We could employ a factorization akin to the Indian Buffet Process [ 6 ] which uses a binary latent variable representation instead of a finite ( sparse ) set of topics .
Also , since the model places no restriction on the number of domains it can personalize over , the model could , with little effort , be used as a back end to connect other generative user personalization schemes .
9 . CONCLUSION
In this work , we designed a new model for providing user personalization across two or more domains and demon
Front PageNews0005010150202503035 % Gain in Cosine Similarity 25 Features50 FeaturesFront PageNewsMyYahoo000501015020250303504 % Gain in Cosine SimilarityFront PageNews000501015020250303504 % Gain in Cosine Similarity Using Front Page and NewsUsing Front Page , News and MyYahoo factorization techniques for recommender systems . IEEE Computer , 42(8):30–37 , 2009 .
[ 9 ] Dong C . Liu and Jorge Nocedal . On the limited memory BFGS method for large scale optimization . Mathematical Programming , 45(3):503–528 , 1989 . [ 10 ] John M . Mellor Crummey and Michael L . Scott .
Algorithms for scalable synchronization on shared memory multiprocessors . ACM Transactions on Computer Systems , 9(1):21–65 , February 1991 .
[ 11 ] D . M . Mimno and A . McCallum . Topic models conditioned on arbitrary features with dirichlet multinomial regression . In D . A . McAllester and P . Myllym¨aki , editors , UAI , Proceedings of the 24th Conference in Uncertainty in Artificial Intelligence , pages 411–418 . AUAI Press , 2008 .
[ 12 ] DM Mimno , W . Li , and A . McCallum . Mixtures of hierarchical topics with pachinko allocation . In Z . Ghahramani , editor , International Conference on Machine Learning , volume 227 , pages 633–640 . ACM , 2007 .
[ 13 ] Seung Taek Park , David Pennock , Omid Madani ,
Nathan Good , and Dennis DeCoste . Na¨ıve filterbots for robust cold start recommendations . In Tina Eliassi Rad , Lyle H . Ungar , Mark Craven , and Dimitrios Gunopulos , editors , KDD , pages 699–705 . ACM , 2006 .
[ 14 ] J . Pearl . Causality : Models , Reasoning and Inference .
Cambridge University Press , 2001 .
[ 15 ] C . Robert and G . Casella . Monte Carlo Statistical
Methods . Springer , second edition , 2004 .
[ 16 ] R . Salakhutdinov and A . Mnih . Bayesian probabilistic matrix factorization using markov chain monte carlo . In WW Cohen , A . McCallum , and ST Roweis , editors , Machine Learning , Proceedings of the Twenty Fifth International Conference ( ICML 2008 ) , Helsinki , volume 307 , pages 880–887 . ACM , 2008 .
[ 17 ] Andrew I . Schein , Alexandrin Popescul , Lyle H .
Ungar , and David M . Pennock . Methods and metrics for cold start recommendations . In Micheline Beaulieu , Ricardo Baeza Yates , Sung Hyon Myaeng , and Kalervo J¨arvelin , editors , Proceedings of the 25th annual international ACM SIGIR Conference on Research and Development in Information Retrieval , pages 253–260 , New York , August 11–15 2002 . ACM Press .
[ 18 ] AJ Smola and S . Narayanamurthy . An architecture for parallel topic models . In Very Large Databases ( VLDB ) , 2010 .
[ 19 ] M . Weimer , A . Karatzoglou , Q . Le , and A . Smola .
Cofi rank maximum margin matrix factorization for collaborative ranking . In JC Platt , D . Koller , Y . Singer , and S . Roweis , editors , Advances in Neural Information Processing Systems 20 . MIT Press , Cambridge , MA , 2008 .
[ 20 ] L . Yao , D . Mimno , and A . McCallum . Efficient methods for topic model inference on streaming document collections . In KDD’09 , 2009 .
