Integrating Low Rank and Group Sparse Structures for
Robust Multi Task Learning
Computer Science and Engineering , Center for Evolutionary Medicine and Informatics , The Biodesign
Jianhui Chen , Jiayu Zhou , Jieping Ye
Institute , Arizona State University , Tempe , AZ 85287
{Jianhui.Chen , Jiayu.Zhou , JiepingYe}@asuedu
ABSTRACT Multi task learning ( MTL ) aims at improving the generalization performance by utilizing the intrinsic relationships among multiple related tasks . A key assumption in most MTL algorithms is that all tasks are related , which , however , may not be the case in many realworld applications . In this paper , we propose a robust multi task learning ( RMTL ) algorithm which learns multiple tasks simultaneously as well as identifies the irrelevant ( outlier ) tasks . Specifically , the proposed RMTL algorithm captures the task relationships using a low rank structure , and simultaneously identifies the outlier tasks using a group sparse structure . The proposed RMTL algorithm is formulated as a non smooth convex ( unconstrained ) optimization problem . We propose to adopt the accelerated proximal method ( APM ) for solving such an optimization problem . The key component in APM is the computation of the proximal operator , which can be shown to admit an analytic solution . We also theoretically analyze the effectiveness of the RMTL algorithm . In particular , we derive a key property of the optimal solution to RMTL ; moreover , based on this key property , we establish a theoretical bound for characterizing the learning performance of RMTL . Our experimental results on benchmark data sets demonstrate the effectiveness and efficiency of the proposed algorithm .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications Data Mining
General Terms Algorithms
Keywords Multi task learning , robust , low rank patterns , group sparsity
1 .
INTRODUCTION
Multi task learning ( MTL ) aims at improving generalization performance by utilizing the intrinsic relationships among multiple tasks . One key assumption in most of the existing MTL algorithms is that all tasks are correlated via a certain structure , which , for example , includes hidden units in neural networks [ 7 ] , a common prior in a hierarchical Bayesian model [ 3 , 29 , 35 , 37 ] , parameters in Gaussian process covariance [ 17 ] , kernels and regularizations [ 10 ] , and common feature representation [ 1 , 9 , 2 , 27 , 16 , 34 ] . Under such an assumption , the knowledge learned from one task is transferable to the other tasks and learning multiple tasks simultaneously generally leads to improved performance .
In many real world applications involving multiple tasks , it is usually the case that a group of tasks are related while some other tasks are irrelevant to such a group . Simply pooling all tasks together and learning them simultaneously under a presumed structure may degrade the overall learning performance . It is thus desirable to identify irrelevant ( outlier ) tasks in the development of the multi task learning algorithms . Learning multiple tasks under this setting is usually referred to as robust multi task learning [ 36 ] .
Recently robust multi task learning has received increasing attention in the areas of data mining and machine learning . In [ 30 , 33 , 13 ] , the task clustering ( TC ) approach is proposed for discovering the common structures in multiple learning tasks . The main idea behind the TC algorithms is to cluster similar tasks into different groups and constrain the tasks from the same group to share the same model representation or parameters . In [ 36 , 38 ] , multivariate student t processes and their generalization are proposed for distinguishing good tasks from noisy or outlier tasks . The t processesbased MTL algorithms model the relationship of multiple tasks using a task covariance matrix and they are robust by nature as tprocess is implicitly an infinite Gaussian mixture . In [ 26 , 14 ] , the block sparse structures ( .1,∞ norm or .2,1 nrom ) are employed to extract essential features shared across the tasks and hence improve the robustness of the learning algorithms .
In this paper , we propose a robust multi task learning ( RMTL ) algorithm which learns multiple tasks simultaneously as well as identifies the irrelevant ( outlier ) tasks . Specifically , our proposed RMTL algorithm captures the relationship of multiple related tasks using a low rank structure and meanwhile identifies the outlier tasks using a group sparse structure . The proposed RMTL algorithm is formulated as a non smooth convex ( unconstrained ) optimization problem in which the least squares loss is regularized by a nonnegative linear combination of the trace norm and the 1,2norm The optimization problems involving the trace norm and the .1,2 norm can be routinely reformulated as semi definite programs or second order cone programs , both of which , however , are not scalable to large scale data . We propose to adopt the accelerated proximal method ( APM ) for solving the proposed RMTL formulation efficiently . One key component in applying APM for solving RMTL is the computation of the associated proximal op erator , which is a non smooth optimization problem involving two optimization variables . The associated proximal operator can be shown to admit an analytic solution . We also conduct theoretical analysis on the performance bound of the composite regularization in RMTL . We first present key properties of the optimal solution to RMTL ( Lemma 43 ) We then present an assumption associated with the prescribed training samples and the geometric structures of the matrices of interest ; based on this assumption , we derive a performance bound for the combined regularization for multi task regression ( Theorem 41 ) We conduct simulations on benchmark data sets to demonstrate the effectiveness and efficiency of the proposed algorithm . Notation Denote Nm = {1,··· , m} . For any A = [ a1,··· , am ] ∈ .m d be the i th column of A ; denote by fiaifi2 the d×m , let ai ∈ R .2 norm of ai ; let fiAfi∞,2 = fiajfi2 , where j = arg maxi fiaifi2 ; .r i=1 fiaifi2 ; let {σi(A)}r let fiAfi1,2 = i=1 be the set of non zero singular values in non increasing order , where r = rank(A ) ; denote by fiAfi2 = σ1(A ) and by fiAfi∗ = i=1 σi(A ) the operator norm and the trace norm of A , respectively .
R
2 . ROBUST MTL FRAMEWORK
Assume that we are given m ( regression ) learning tasks . Each task is associated with a set of training data ni )} ⊂ R
1),··· , ( x
{(x i ni , y i 1 , y i i d × R , i ∈ Nm , and a linear predictive function fi as fi(x i j ) = w
T i x i j ≈ y i j , x i j ∈ R d i j ∈ R ,
, y
( 1 )
( 2 ) where i and j index the task and the training sample respectively , wi is the weight vector , ni and d denote the training sample size and the feature dimensionality respectively .
We consider the multi task learning setting where multiple tasks are divided into two groups , ie , the related tasks group and the irrelevant ( outlier ) tasks group . We consider a composite structure which couples the related tasks using a low rank structure and identifies the outlier tasks using a group sparse structure . Denote the transformation matrix of the m tasks by W = [ w1,··· , wm ] ∈ d×m . Specifically , W is given by the direct summation of a low rank matrix L = [ l1,··· , lm ] ∈ R d×m ( of a smaller set of basis factors ) , and a group sparse ( column sparse ) matrix S = [ s1,··· , sm ] ∈ R d×m ( of zero vectors in the columns ) . The weight vector of the i th task can be expressed as , si ∈ R wi = li + si , li ∈ R
, i ∈ Nm ,
( 3 )
R d d where li and si are from the aforementioned low rank structure and the group sparse structure , respectively .
We propose a robust multi task learning formulation ( RMTL ) to learn multiple tasks simultaneously as well as identify the irrelevant outlier tasks . Mathematically , RMTL is formulated as
L min L,S
( li + si )
T i i j , y j x
+ αfiLfi∗ + βfiSfi1,2 ,
( 4 ) fi
' where the trace norm regularization term encourages the desirable low rank structure in the matrix L ( for coupling the related tasks ) , and the .1,2 norm regularization term induces the desirable groupsparse structure in the matrix S ( for identifying the outlier tasks ) , α and β are non negative trade off parameters , and L(·,· ) represents the commonly used least squares loss function . Note that the empirical evaluation of the ( averaged ) least square loss of the m tasks fi over the prescribed training data can be expressed as L
1
T
T
( li + si )
( li + si ) i i j , y j x
= i j − y i j x
' mff niff fi i=1 j=1 mni
'2
.
( 5 )
Our motivation behind the proposed RMTL formulation in Eq ( 4 ) is as follows : if the i th task is from the related tasks group , si is expected to be a zero vector and hence wi obeys the specified low rank structure constraint ; on the other hand , if the i th task is from the outlier tasks group , si is expected to be non zero and wi is equal to a direct sum of li and the non zero si .
The RMTL formulation in Eq ( 4 ) is an unconstrained convex optimization problem with a non smooth objective function . Such a problem is difficult to solve directly due to the non smoothness in the trace norm and the .1,2 norm regularization terms . The proposed RMTL formulation in Eq ( 4 ) subsumes several representative algorithms as special cases . As β → +∞ , RMTL is degenerated into mff niff fi min
L i=1 j=1
1 mni
'2 i j − y i j
T i x l
+ αfiLfi∗ .
( 6 ) min fi
'2
The formulation in Eq ( 6 ) is essentially the least squares regression with trace norm regularization , in which multiple learning tasks are coupled via a low rank structure . On the other hand , as α → ∞ , RMTL is degenerated into mff niff .m .m The formulation in Eq ( 7 ) is essentially a variant of the ridge rei=1 fisifi2 replaced by the nongression with the smooth term i=1 fisifi . In such a formulation , the multiple tasks '2 smooth term are decoupled and each task can be learned ( optimized ) via
+ βfiSfi1,2 . niff j − y fi mni
T i x
( 7 ) j=1 i=1
1 s i j
S i i j − y i j
T i x s
+ βfisifi2 . min si
1 mni j=1
Note that similar low rank and group sparse structures are studied from a different perspective in [ 32 , 12 ] , which focus on decomposing a given data matrix into a unique sum of a low rank structure and a column sparse structure and providing a theoretical guarantee for existence and uniqueness of the decomposition .
3 . ACCELERATED PROXIMAL METHOD In this section , we consider to solve the RMTL formulation in Eq ( 4 ) using the accelerated proximal method ( APM ) [ 4 , 24 , 25 ] . APM has attracted extensive attentions in the machine learning and data mining communities [ 20 , 19 , 15 , 8 , 18 , 21 ] due to its optimal convergence rate among all first order techniques and its ability of dealing with large scale non smooth optimization problems . Note that in this paper , we focus on discussing the key ingredient of APM , i.e , the proximal operator and its efficient computation ; the detailed description of APM can be found in [ 4 , 24 , 25 ] . 3.1 Proximal Operator
For the optimization problem in Eq ( 4 ) , we symbolically denote its variables by
Z =
L S d×m
, S ∈ R d×m
,
, L ∈ R ' fi and denote the smooth and non smooth components of its objective function respectively by f ( Z ) = L
, g(Z ) = αfiLfi∗ +βfiSfi1,2 . ( 8 )
( li + si ) i i j , y j x
T
To solve Eq ( 4 ) , APM maintains two sequences of variables : a feasible solution sequence {Zk} and a searching point sequence
{(Zk} . The general scheme of APM can be described as below : at the k th iteration of APM , the solution point Zk+1 can be computed via
))))Z −
( Zk − 1
∇f ( (Zk ) ff))))2
γk 2
F
Z
γk
+g(Z ) , ( 9 )
Zk+1 = arg min where ( Zk denotes a searching point constructed from a linear combination of Zk and Zk−1 from previous iterations , and ∇f ( (Zk ) denotes the derivative of the smooth component f ( · ) in Eq ( 8 ) at ( Zk , fiZk+1−(Zkfi2 f ( Zk+1)≤ f ( (Zk ) +∇f ( (Zk ) , Zk+1−(Zkff +
γk specifies the step size which can be appropriately determined by iteratively increasing its value until the inequality γk 2
F , ( 10 ) is satisfied . The procedure in Eq ( 9 ) is commonly referred to as the proximal operator [ 23 ] . The efficient computation of the proximal operator is critical for the practical convergence of APM , as it is involved in each iteration of the APM algorithm . 3.2 Proximal Operator Computation
For the optimization problem in Eq ( 4 ) , its proximal operator can be expressed as an optimization problem of the general form fiLz − Lˆzfi2
F + fiSz − Sˆzfi2
F + ˆαfiLzfi∗ + ˆβfiSzfi1,2 , ( 11 ) min Lz ,Sz and ˆβ = 2β γk where ˆα = 2α It can be easily verified that the γk optimization of Lz and Sz in Eq ( 11 ) are decoupled . Moreover , the optimal solution to Eq ( 11 ) admits an analytic form as presented below .
.
Computation of Lz The optimal Lz to Eq ( 11 ) can be obtained by solving the following optimization problem : F + ˆαfiLzfi∗ . fiLz − Lˆzfi2
( 12 ) min Lz
The computation procedure above is equal to the matrix shrinkage operator discussed in [ 6 , 11 ] . In essence it applies soft thresholding to the non zero singular values [ 28 ] of Lˆz as summarized in the following theorem .
THEOREM 31 Given an arbitrary Lˆz in Eq ( 12 ) , let rank(Lˆz ) =
T r and denote the singular value decomposition ( SVD ) of Lˆz in the reduced form as i=1 ) fifl
Lˆz = UˆzΣˆzV d×r and Vˆz ∈ R
ˆz , Σˆz = diag ( {σi}r ffl where , Uˆz ∈ R m×r consist of orthonormal columns , Σˆz ∈ R r×r is diagonal , and {σi}r i=1 represent the non zero sin∗ z to Eq ( 12 ) is given by gular values . Then the optimal L σi − 1 2
∗ z = Uˆz diag where {e}+ = max(e , 0 ) . The dominating cost in solving Eq ( 12 ) lies in the compact SVD operation on the matrix Lˆz ∈ R d×m ( m fi d in general MTL settings ) . ffi
T ˆz ,
L
ˆα
V
+
Computation of Sz The optimal Sz to Eq ( 11 ) can be obtained by solving the following optimization problem : fiSz − Sˆzfi2
F + ˆβfiSzfi1,2 . min Sz
( 13 )
It can be easily verified that in Eq ( 13 ) the column vectors of Sz can be optimized separately . Specifically , each vector of the optimal Sz to Eq ( 13 ) can be obtained via solving a subproblem in the form fis − ˆsfi2
2 + ˆβfisfi2 . min s
( 14 )
It can be verified that the optimization problem above admits an analytic solution [ 19 ] as summarized in the following lemma .
LEMMA 31 Let s
∗ problem in Eq ( 14 ) . Then s is given by be the optimal solution to the optimization
∗ fi
∗ s
=
ˆs
'
1 − ˆβ 2'ˆs'2 0 fiˆsfi2 > 0 ≤ fiˆsfi2 ≤ ˆβ
ˆβ 2
2
.
The computation cost of solving Eq ( 13 ) is relatively small compared to the cost of solving Eq ( 12 ) .
4 . THEORETICAL ANALYSIS
In this section , we derive a performance bound for the proposed RMTL formulation in Eq ( 4 ) . This performance bound can be used to theoretically evaluate how well the integration of the low rank structure and the group sparse structure can estimate the multiple tasks ( the ground truth of the linear predictive functions ) . Note that in the following analysis , for simplicity we assume that the training sample sizes for all tasks are the same ; the derivation below can be easily extended to the setting where the training sample size for each task is different .
Assume that the linear predictive function associated with the i y
T i x i j = fi(x j + δij , i ∈ Nm , j ∈ Nn , i th task satisfies i ( 15 ) j ) + δij = w j)} are the training data pairs of the i th task , and where {(xi j , yi δij ∼ N ( 0 , σ2 δ ) is a stochastic noise variable . For the i th task , denote its training data matrix Xi and its label vector yi respectively by d×n
Xi = [ x
, yi = [ y n ] ∈ R
1,··· , x
1,··· , y i n ]
, i ∈ Nm . ( 16 ) Denote the empirical evaluation of the i th task fi over the training data {xi ˆfi = [ fi(x j} and the associated noise vector δi respectively by
, δi = [ δi1,··· , δin ]
1),··· , fi(x
T ∈ R
T ∈ R
T ∈ R i n ) ] n n n i i i i
. ( 17 )
( 18 )
It follows that Eq ( 15 ) can be expressed in a compact form as yi = ˆfi + δi , i ∈ Nm . mff
Moreover , the optimization problem in Eq ( 4 ) can be rewritten as
( (Lz,(Sz ) where ( Lz = [ ˆl1,··· , ˆlm ] and ( Sz = [ ˆs1,··· , ˆsm ] are the optimal
2 +αfiLfi∗ +βfiSfi1,2 , ( 19 ) i ( li +si)−yifi2
= arg min L,S fiX mn i=1
1
T solution pair obtained via solving Eq ( 19 ) . 4.1 Basic Properties of the Optimal Solution We present some basic properties of the optimal solution pair defined in Eq ( 19 ) ; these properties are important building blocks for our following theoretical analysis . We first define two operators , namely Q and its complement Q⊥ , on an arbitrary matrix pair ( of the same size ) , based on Lemma 3.4 in [ 28 ] .
Σ 0 0 0
T
,
V
LEMMA 41 Given any L and ( L of the same size d × m , let rank(L ) = r ≤ min(d , m ) and denote the SVD of L as
L = U d×d and V ∈ R where U ∈ R is diagonal consisting of the non zero singular values on its main diagonal . Let m×m are orthogonal , and Σ ∈ R r×r
M11 M12 M21 M22
U where M11 ∈ R
M22 ∈ R
, ( d−r)×(m−r ) . Define Q and Q⊥ on(L − L as r×(m−r ) , M21 ∈ R
,Q⊥((L−L ) = U
Q((L−L ) = U Then rank(Q((L−L ) ) ≤ 2r , LQT⊥((L−L ) = LTQ⊥((L−L ) = 0 .
( d−r)×r , and
M11 M12 M21 0
0 0 0 M22
V
V
T
T
T
( (L − L)V = r×r , M12 ∈ R
The results in Lemma 4.1 imply a condition under which the trace norm on a matrix pair is additive . From Lemma 4.1 we can verify fiL + Q⊥((L − L)fi∗ = fiLfi∗ + fiQ⊥((L − L)fi∗ ,
Lemma 4.1 , we derive a bound on the trace norm of the matrices of interest as summarized below ( the detailed proof is provided in the Appendix ) . for arbitrary L and(L of the same size . As a direct consequence of COROLLARY 41 For an arbitrary matrix pair ( L and L , the fi(L − Lfi∗ + fiLfi∗ − fi(Lfi∗ ≤ 2fiQ((L − L)fi∗ . following inequality holds
( 20 )
Analogous to the bound on the trace norm derived in Corollary 4.1 , we derive a bound on the .1,2 norm of the matrices of interest . Denote by C(S ) the set of indices corresponding to the non zero columns of the matrix S as
C(S ) = {i : si ffi= 0 , i ∈ Nm} ,
( 21 ) and by C⊥(S ) the associated complement ( the set of indices corre sponding to the zero columns ) . Denote by ( SC(S ) the matrix of the same columns as ( S on the index set C(S ) and of zero columns on the index set C⊥(S ) , ie , ( SC(S ) = [ ˜s1,··· , ˜sm ] , where ˜si = ˆsi LEMMA 42 Given a matrix pair S and ( S of the same size , the fi(S − Sfi1,2 + fiSfi1,2 − fi(Sfi1,2 ≤ 2fi((S − S)C(S)fi1,2 . if i ∈ C(S ) and ˜si = 0 if i ∈ C⊥(S ) . The bound on the .1,2norm is summarized below ( the detailed proof is provided in the Appendix ) . following inequality holds
( 22 )
We now present some important properties of the optimal solution in Eq ( 19 ) as summarized in the following lemma .
LEMMA 43 Consider the optimization problem in Eq ( 19 ) for m ≥ 2 and n , d ≥ 1 . Let Xi and yi be defined in Eq ( 16 ) , and ˆfi and δi be defined in Eq ( 17 ) . Assume that all diagonal elements of the matrix XiX T i are equal to 1 ( features are normalized ) . Take the regularization parameters α and β as √
, β ≥ λ , λ =
α√ m
2σδ nm d + t ,
( 23 )
2
T
T
1 i=1 nm fiX fiX
1 + t d t − d log
− 1
2 ≤ 1 nm i ( li + si)− ˆfifi2 i ( ˆli + ˆsi)− ˆfifi2
( Lz,(Sz in Eq ( 19 ) and any L , S ∈ R where t > 0 is a universal constant . Then with probability of at least 1− m exp , for a global minimizer mff mff d×m , we have and S ) , respectively .
+ αfiQ((Lz − L)fi + βfi((Sz − S)C(S)fi1,2 , where ˆli and ˆsi ( li and si ) are the i th columns of ( Lz and ( Sz ( L PROOF . From the definition of ( (Lz,(Sz ) in Eq ( 19 ) , we have mff αfiLfi∗ + βfiSfi1,2 − αfi(Lzfi∗ − βfi(Szfi1,2 . i ( ˆli + ˆsi)− yifi2 i ( li + si)− yifi2
2 ≤ 1 nm mff fiX fiX
( 24 ) nm i=1 i=1 i=1
1
T
T
2
2
.
By substituting Eq ( 18 ) into the inequality above and rearranging all terms , we have mff i=1
1 nm mff mff i=1 ffi
T
2 ≤ 1 nm
+ α(fiLfi∗ − fi(Lzfi∗ ) + β(fiSfi1,2 − fi(Szfi1,2 ) i ( li + si)− ˆfifi2 fiX i ( ˆli + ˆsi)− ˆfifi2 mff fiX
T
2
2
2
ˆli − li , Xiδiff +
+ i=1 nm
.m .m i=1ˆli−li , Xiδiff i=1ˆsi − si , Xiδiff in Eq ( 25 ) , respectively . Define a set
Next we compute upper bounds for the terms 2 nm and 2 of random events {Ai} as nm
( 25 ) nm i=1
ˆsi − si , Xiδiff . fl
2
Ai =
, ∀i ∈ Nm . For each Ai , define a set of random variables {vij} as nm fiXiδifi2 ≤ λ nff vij =
1 σδ x k=1 i jkδik , j ∈ Nd ,
Pr
2 nm ffl2 where xi jk denotes the ( j , k) th entry of the data matrix Xi . Since all diagonal elements of the matrix XiX T i are equal to 1 , it can be .d shown that {vi1 , vi2,··· , vid} are iid Gaussian variables obeying N ( 0 , 1 ) ( Lemma 1 in the Appendix ) . We can also verify that ij is a chi squared random variable with d degrees of freeffl dom . Moreover taking λ as in Eq ( 23 ) , we have j=1 v2 fi ff fi fiXiδifi2 > λ dff
= Pr dff j=1 fi nff ffl k=1 i jkδik x
≥ λ2n2m2
4 ff
2
,
μ j=1
= Pr
2 d(t )
− 1 2
≤ exp ij ≥ d + t v m t − d log ( t > 0 ) , and the last inequalwhere μd(t ) = ity above follows from a concentration inequality ( Lemma 2 in the i=1 Ai . Denote by Ac Appendix ) . Let A = fi i the complement of m each event Ai . It follows that Ac
Pr ( A ) ≥ 1 − Pr
≥ 1 − m exp
1 + t d ffl ff
2 d(t )
μ
. i
− 1 2 i=1
.m i=1ˆli −
2 nm i=1 i=1
≤ λ nm i=1
Under the event A , we derive a bound on the term 2 mff li , Xiδiff as fiˆli − lifi2fiXiδifi2 mff ˆli − li , Xiδiff ≤ 2 mff fiˆli − lifi2 ≤ αfi(Lz − Lfi∗ , ( 26 ) nm m mff mfi(Lz − LfiF ≤ √ mff mfi(Lz − Lfi∗ . .m i=1ˆsi−
Similarly under A , we also derive a bound on the term 2 mff si , Xififf as where the first inequality above follows from Cauchy Schwarz inequality and the second inequality follows from fiˆli − lifi2 ≤ fiˆli − lifi2 mff
√ i=1 i=1 nm
=
2
ˆsi − si , Xiδiff ≤
2 nm i=1 fiˆsi − sifi2fiXiδifi2
2 nm
≤ βfi(Sz − Sfi1,2 . i=1
( 27 )
Moreover we bound the right side of Eq ( 25 ) using the results from Eqs . ( 26 ) and ( 27 ) . It follows that
T
1 fiX i ( ˆli + ˆsi)− ˆfifi2
α(fi(Lz−Lfi∗+fiLfi∗−fi(Lzfi∗)+β(fi(Sz−Sfi1,2+fiSfi1,2−fi(Szfi1,2 ) . i ( li +si)− ˆfifi2 2+
2 ≤ 1 nm fiX nm i=1 i=1
T mff mff
Finally by applying Corollary 4.1 and Lemma 4.2 together with the inequality above , we complete the proof . 4.2 Performance Bound
R
We present a performance bound of the proposed RMTL formulation in Eq ( 19 ) . This bound measures how well the multi task learning scheme ( via the integration of the low rank structure and the .1,2 norm structure ) can estimate the linear predictive functions in Eq ( 15 ) . We begin with some notations . Let X ∈ R md×mn be a blockdiagonal matrix with its i th block formed by the matrix Xi ∈ d×n ( i ∈ Nm ) . Define a diagonalization operator D on an arbitrary Ω = [ ω1 , ω2,··· , ωm ] ∈ R md×m is a block diagonal matrix with its i th block formed by the column vector ωi ∈ R d . Let F = [ ˆf1,··· , ˆfm ] , where ˆfi is defined in Eq ( 17 ) . Therefore we can rewrite Eq ( 24 ) in a compact form as TD(L+S)−D(F )fi2
TD((Lz +(Sz)−D(F )fi2
+ αfiQ((Lz − L)fi∗ + βfi((Sz − S)C(S)fi1,2 , d×m : D(Ω ) ∈ R
F ≤ 1 T fiX fiX
( 28 )
1 T
F where T = nm . We next introduce our assumption over a restricted set . The assumption is associated with training data X and the geometric structure of the matrices of interest .
ASSUMPTION 41 For a matrix pair ΓL and ΓS of size d by m , let s ≤ min(d , m ) and q ≤ m . We assume that there exist constants κ1(s ) and κ2(q ) such that √
κ1(s ) .
> 0 , min
( 29 )
ΓL,ΓS∈R(s,q ) fiXD(ΓL + ΓS)fiF TfiQ(ΓL)fi∗ fiXD(ΓL + ΓS)fiF √ Tfi(ΓS)C(S)fi1,2
κ2(q ) . min
ΓL,ΓS∈R(s,q ) where the restricted set R(s , q ) is defined as R(s , q ) =
ΓL , ΓS ∈ R d×m | ΓL ffi= 0 , ΓS ffi= 0 , rank(Q(ΓL ) ) ≤ s , |C(ΓS)| ≤ q} , and C(· ) is defined in Eq ( 21 ) , and |(C| denotes the number of elements in the set ( C .
The assumption in Eqs . ( 29 ) and ( 30 ) can be implied by several sufficient conditions as in [ 5 ] . Due to the space constraint , the details are omitted . Note that similar assumptions are used in [ 22 ] for deriving a certain performance bound for a different multi task learning formulation .
We present the performance bound of the RMTL formulation in the following theorem . t − d log
− 1 fiXD((Lz+(Sz)−D(F )fi2
THEOREM 41 Consider the optimization problem in Eq ( 19 ) for m ≥ 2 and n , d ≥ 1 . Take the regularization parameters α and β as in Eq ( 23 ) . Then with probability of at least 1 − m exp in Eq ( 19 ) , we have
, for a global minimizer ( Lz,(Sz
F ≤ ( 1+ ) inf fiXD(L+S)−D(F )fi2
1 + t d ff
2
1 T
F
+ E( )
L,S
1 T α2 κ2 1(2r ) d×m with rank(L ) ≤ r and
β2 κ2 2(c )
( 31 )
+
, where inf is taken over all L , S ∈ R |C(S)| ≤ c , and E( ) > 0 is a constant depending only on .
PROOF . Denote ΓL = ( Lz − L and ΓS = ( Sz − S . It follows from Eq ( 28 ) that
TD((Lz +(Sz)−D(F )fi2
1 T fiX fiX
F ≤ 1 T
+ αfiQ(ΓL)fi∗ + βfi(ΓS)C(S)fi1,2 .
TD(L+S)−D(F )fi2 ( 32 ) Given Q(ΓL ) ≤ 2r ( from Lemma 4.1 ) and |C(S)| ≤ c , we derive upper bounds on αfiQ(ΓL)fi∗ and βfi(ΓS)C(S))fi1,2 over the restrict set R(2r , c ) based on Assumptions 4.1 , respectively . It follows from Eq ( 29 ) in Assumption 4.1 that
F fi 2αfiQ(ΓL)fi∗ ≤ fiXD((Lz + ( Sz ) − D(F )fiF + fiXD(L + S ) − D(F )fiF fiXD(ΓL + ΓS)fiF ≤
2α κ1(2r )
2α κ1(2r )
√
'
T
√
T ≤ fiXD((Lz + ( Sz ) − D(F )fi2
α2τ κ2 1(2r ) fiXD(L + S ) − D(F )fi2 F ,
F +
+
( 33 )
1 τ T
α2τ κ2 1(2r )
+
1 τ T where the last inequality above follows from 2ab ≤ a2τ + b2 1 τ > 0 . Similarly , we have
τ for
2βfi(ΓS)C(S)fi1,2 ≤ β2τ κ2 2(c ) β2τ κ2 2(c )
+
+
1 τ T
1 τ T fiXD((Lz + ( Sz ) − D(F )fi2
F + fiXD(L + S ) − D(F )fi2
F . ( 34 )
> 0 ,
( 30 )
Substituting Eqs . ( 33 ) and ( 34 ) into Eq ( 32 ) and setting τ = 2 + 4 ,
F we obtain
1 T ≤ τ + 2 τ − 2 fiXD((Lz + ( Sz ) − D(F )fi2
2τ 2 fiXD(L + S ) − D(F )fi2 τ − 2 F + F + E( ) = ( 1 + )fiXD(L + S ) − D(F )fi2 where E( ) = ( 1
2 + 1
)2 . This completes the proof .
α2 κ2 1(2r ) α2 κ2 1(2r ) ff ff
,
+
+
β2 κ2 2(c ) β2 κ2 2(c )
The performance bound described in Eq ( 31 ) can be refined by choosing specific values for the regularization parameters α and β : 2(c ) is minimized it can be verified that the component κ2 if α and β are chosen to be proportional to κ2 2(c ) , respectively .
1(2r ) + β2
1(2r ) and κ2
α2
κ2
5 . EXPERIMENTS
In this section , we evaluate the proposed RMTL formulation in Eq ( 4 ) in comparison with other representative algorithms for multi task learning ; we also conduct numerical studies on the APM algorithm in comparison with the commonly used proximal method ( PM ) [ 25 , 24 ] for solving RMTL . All algorithms are implemented in Matlab . Note that for numerical accuracy consideration , we solve the RMLT formulation with its objective function multiplied by nm , where m and n correspond to the task number and the sum of the sample sizes for all tasks , respectively . l e u a v r a u g n S i l
14
12
10
8
6
4
2
0
0 s r o t c e v t i h g e W
10
20
30
40
50
60
30
5
5
10
15
20
25
Index of the singular values in L
10
15
20
25
30
Index of the tasks in S
Figure 1 : Demonstration of the extracted low rank and group structures : the left plot shows the singular values of the lowrank component L ( the last 18 singular values are zero ) ; the right plot demonstrates the structure of the group sparse component S ( the first 20 columns are zero vectors ) . In the right plot the grey area corresponds to the pixels of zero value .
5.1 Demonstration of Extracted Structures
We apply the RMTL algorithm on a synthetic data set and then demonstrate the extracted low rank and group sparse structures . The synthetic data is constructed as follows : set the task number m = 30 , the size of the training samples for each task ni = 50 , and the feature dimensionality of the training samples d = 60 ; generate the entries of the training data Xi ∈ R d×ni ( for the i th task ) randomly from the distribution N ( 0 , 25 ) ; generate the entries in the low rank component L ( of size d × m ) randomly from N ( 0 , 16 ) and then set its smallest 20 singular values at 0 ; generate the entries in the group sparse component S ( of size d × m ) randomly from N ( 0 , 20 ) and then set its first 20 columns as zerovectors ; construct the response ( target ) vector of each task as yi = i ( L + S ) + δi ∈ R ni ( i ∈ Nm ) , where each entry in the vector X T δi is randomly generated from N ( 0 , 1 ) . Under this experimental setting , we construct 20 related tasks as well as 10 outlier tasks , where each task is associated with 50 training samples of feature dimensionality 60 .
In Figure 1 , we present the low rank component L and the groupsparse component S obtained by solving RMTL with α = 50 and β = 10 . From the left plot of Figure 1 , we can observe that the matrix L ( of size 60 × 50 ) has 12 non zero singular values ; this result is consistent with our problem setting of using a low rank structure to capture the tasks relationship . From the right plot of Figure 1 , we can observe that the first 20 columns ( corresponding to the related tasks ) in S are zero vectors , while the last 10 columns ( corresponding to the outlier tasks ) are non zero vectors . The results in Figure 1 empirically demonstrate the effectiveness of RMTL .
5.2 Performance Evaluation of RMTL
We evaluate the RMTL algorithm on multi task regression problems in comparison with other representative algorithms including ridge regression ( Ridge ) , least squares with .1 norm regularization ( Lasso ) , least squares with trace norm regularization ( TraceNorm ) , least squares with low rank and sparse structures regularization ( Sparse LowRank ) [ 8 ] , and convex multi task feature learning ( CMTL ) [ 2 ] . The normalized mean squared error ( nMSE ) and the averaged mean squared error ( aMSE ) are employed as the regression performance measures as used in previous studies [ 2 , 38 ] . Note that nMSE is defined as the mean squared error ( MSE ) divided by the variance of the target vector ; aMSE is defined as MSE divided by the squared norm of the target vector . We adopt APM to solve RMTL and terminate APM when the relative change of the −5 . objective values in two successive iterations is smaller than 10 We use the School data1 and the SARCOS data2 for the experiments .
The School data consists of the exam scores of 15362 students from 139 secondary schools ; each student is described by 27 attributes such as gender and ethnic group . The exam score prediction of the students can be cast into a multi task regression ( learning ) problem : we are given 139 tasks ( schools ) , where each task has a different number of samples ( students ) and each sample has 27 features ( attributes ) . We randomly select 10 % , 20 % , and 30 % of the samples ( from each task ) to form the training set and use the rest of the samples as the test set . The experimental results averaged over 15 random repetitions are presented in Table 1 . From the presented results , we have the following observations : ( 1 ) RMTL outperforms all other competing algorithms in terms of nMSE and aMSE ; ( 2 ) the multi task learning algorithms ( TraceNorm , SparseLowRank , CMTL , and RMTL ) outperform the single task learning algorithms ( Ridge and Lasso ) in terms of both nMSE and aMSE ; ( 3 ) the performance of CMTL is similar to that of TraceNorm ; this result may be due to the use of similar penalty terms in CMTL and TraceNorm .
The SARCOS data is collected for an inverse dynamics prediction problem for a seven degrees of freedom anthropomorphic robot arm . This data consists of 48933 observations corresponding to 7 joint torques ; each of the observations is described by 21 features including 7 joint positions , 7 joint velocities , and 7 joint accelerations . Our goal is to construct mappings from each observation to 7 joint torques . We randomly select 50 , 100 , 150 observations to form 3 training sets and accordingly randomly select 5000 observations to form 3 test sets . The experimental results averaged over 15 random repetitions are presented in Table 2 . From the experimental results , we have the following observations : ( 1 ) RMTL performs better than or compares competitively to all other competing algorithms in terms of both nMSE and aMSE ; ( 2 ) the multitask learning algorithms ( TraceNorm , Sparse LowRank , CMTL ,
1http://wwwcsuclacuk/staff/AArgyriou/code/ 2http://wwwgaussianprocessorg/gpml/data/
Table 1 : Performance comparison of the six competing algorithms in terms of the normalized MSE ( nMSE ) and the averaged MSE ( aMSE ) with standard deviation using the School data . All parameters of the six methods are determined via cross validation and the reported regression performance is averaged over 15 random repetitions . Note that a smaller value of nMSE and aMSE represents better regression performance .
Measure training ratio nMSE aMSE
10 % 20 % 30 % 10 % 20 % 30 %
Ridge
1.0398 ± 0.0038 0.8773 ± 0.0043 0.8171 ± 0.0090 0.2713 ± 0.0023 0.2303 ± 0.0003 0.2165 ± 0.0021
Lasso
1.0261 ± 0.0132 0.8754 ± 0.0194 0.8144 ± 0.0091 0.2682 ± 0.0036 0.2289 ± 0.0051 0.2137 ± 0.0012
TraceNorm
0.9359 ± 0.0370 0.8211 ± 0.0032 0.7870 ± 0.0012 0.2504 ± 0.0102 0.2156 ± 0.0015 0.2089 ± 0.0012
Sparse LowRank 0.9175 ± 0.0261 0.8126 ± 0.0132 0.7657 ± 0.0091 0.2419 ± 0.0081 0.2114 ± 0.0041 0.2011 ± 0.0022
CMTL
0.9413 ± 0.0021 0.8327 ± 0.0039 0.7922 ± 0.0052 0.2552 ± 0.0032 0.2131 ± 0.0071 0.1922 ± 0.0102
Robust MTL
0.9130 ± 0.0039 0.8055 ± 0.0103 0.7600 ± 0.0032 0.2330 ± 0.0018 0.2018 ± 0.0025 0.1822 ± 0.0014
Table 2 : Performance comparison of the six competing algorithms in terms of nMSE and aMSE with standard deviation using the SARCOS data.The experimental setting is similar to the one described in Table 1 .
Measure training size nMSE aMSE
50 100 150 50 100 150
Ridge
0.2454 ± 0.0260 0.1821 ± 0.0142 0.1501 ± 0.0054 0.1330 ± 0.0143 0.1053 ± 0.0096 0.0846 ± 0.0045
Lasso
0.2337 ± 0.0180 0.1616 ± 0.0027 0.1469 ± 0.0028 0.1228 ± 0.0083 0.0907 ± 0.0023 0.0822 ± 0.0014
TraceNorm
0.2257 ± 0.0065 0.1531 ± 0.0017 0.1318 ± 0.0053 0.1122 ± 0.0064 0.0805 ± 0.0026 0.0772 ± 0.0023
Sparse LowRank 0.2127 ± 0.0033 0.1495 ± 0.0023 0.1236 ± 0.0004 0.1073 ± 0.0026 0.0793 ± 0.0047 0.0661 ± 0.0062
CMTL
0.2192 ± 0.0016 0.1568 ± 0.0037 0.1301 ± 0.0034 0.1156 ± 0.0011 0.0852 ± 0.0013 0.0755 ± 0.0025
Robust MTL
0.2123 ± 0.0038 0.1456 ± 0.0138 0.1245 ± 0.0015 0.0982 ± 0.0026 0.0737 ± 0.0083 0.0674 ± 0.0014 and RMTL ) outperform the single task learning algorithms ( Ridge and Lasso ) in terms of both nMSE and aMSE . We also observe that Sparse LowRank has a similar performance to RMTL . In SparseLowRank , incoherent low rank and ( .1 norm based ) sparse structures [ 8 ] are used to capture the task relatedness as well as identify discriminative features for each task . These results imply that allowing each task to independently select discriminative features may improve the robustness of the algorithm .
5.3 Sensitivity Studies on RMTL
We conduct a sensitivity study on the proposed RMTL formulation . In particular , we study how the regularization parameters and the training sample size affect the regression performance of RMTL in terms of nMSE and aMSE , respectively .
Effect of the Regularization Parameters For this experiment , we randomly select 10 % of the School data as the training set and use the rest of the data as the test set . By fixing β = 100 as well as varying the value of α in α value set , ie , [ 50 : 50 : 500 ] , we study how the parameter α affects the regression performance of RMTL . Similarly , by fixing α = 150 as well as varying the value of β in β value set of [ 20 : 5 : 115 ] , we study how the parameter β affects the regression performance of RMTL . In Figure 2 , we present the regression performance ( averaged over 15 random repetitions ) of RMTL in terms of nMSE ( 1st and 3rd plots ) and aMSE ( 2nd and 4th plots ) for each pair of ( α , β ) . From Figure 2 , we can observe that both nMSE and aMSE change with different settings of ( α , β ) ; we can also observe that the best performance of RMTL for a fixed α ( or a fixed β ) is obtained by setting β in the middle of β value set ( or setting the value of α in the middle of α value set ) .
Effect of the Training Ratio For this experiment , we randomly select {10 % , 20%,··· , 80%} of the School data as the training set and use the rest of the data as the test set . We study how the the training sample size ( in terms of the training ratio ) affects the regression performance of RMTL . Note that the regularization parameters α and β are determined via double cross validation . The experimental results are presented in Figure 3 . We can observe that by increasing the training ratio , both the nMSE and aMSE de crease ; this result is consist with our expectation that more training data will lead to more accurate predictive model and hence better generalization performance . 5.4 Numerical Studies on APM
We conduct numerical studies on APM in comparison with PM for solving RMTL in terms of the computation time ( in seconds ) and the iteration number . We randomly select 10 % of the School data for the following experiments . The experimental setting is described as follows : we stop PM when the change of the objective −i and record value in two successive iterations is smaller than 10 the attained objective value ; such a value is then used as the stopping criterion in APM , that is , we stop APM when the attained objective value in APM is equal to or smaller than the one previously obtained from PM ; we vary the stopping criterion of PM in the set {10 i=1 and record the required computation time and iteration number for both PM and APM . From the experimental results presented in Figure 4 , we have the following observations : ( 1 ) APM requires less computation time and iteration number than PM for attaining the same objective value ; ( 2 ) both APM and PM require more computation time and a larger iteration number if the stopping criterion is set as a smaller value ( higher accuracy ) .
−i}6
6 . CONCLUSION
In this paper , we propose a robust multi task learning ( RMTL ) algorithm which learns multiple tasks simultaneously as well as identifies the outlier tasks . The proposed RMTL algorithm captures the task relationships using a low rank structure , and simultaneously identifies the outlier tasks using a group sparse structure . RMTL is formulated as a non smooth convex ( unconstrained ) optimization problem in which the least square loss is regularized by a combination of the trace norm regularization and the .1,2norm regularization . We propose to adopt the accelerated proximal method ( APM ) for solving this optimization problem and develop efficient algorithms for computing the associated proximal operator . We also conduct a theoretical analysis on the proposed RMTL formulation . In particular , we derive a key property of the optimal solution to RMTL ; based on the key property , we establish
1.15
1.1
E S M d e z
1.05
1 i l a m r o N
0.95
0.9
0
2
Index of the α−value set
4
6
8
0.31
0.3
0.29
0.28
0.27
0.26
0.25
0.24
E S M d e g a r e v A
10
0.23
0
1.08
1.06
1.04
1.02
1
0.98
E S M d e z i l a m r o N
2 8 Index of the α−value set
4
6
0.96
0.94
0
10
5
Index of the β−value set
10
15
0.285
0.28
0.275
0.27
0.265
0.26
0.255
0.25
E S M d e g a r e v A
20
0.245
0
5
15 Index of the β−value set
10
20
Figure 2 : Sensitivity study on RMTL : study the effect of the parameters α and β in terms of nMSE ( 1st and 3rd plots ) and aMSE ( 2nd and 4th plots ) , respectively . For the first two plots , we set β = 100 and vary α in the α value set [ 50 : 50 : 500 ] ; for the last two plots , we set β = 150 and vary β in the β value set [ 20 : 5 : 115 ] . a theoretical performance bound to characterize the learning performance of RMTL . Our experimental results on benchmark data sets demonstrate the effectiveness and efficiency of the proposed algorithms . In the future , we plan to apply the proposed RMTL to other real world applications such as Alzheimer ’s disease study and Drosophila gene expression images analysis . for any matrix pair L and(L . It follows that fi(Lfi∗ = fiL + Q((L − L ) + Q⊥((L − L)fi∗ ≥ fiL + Q⊥((L − L)fi∗ − fiQ((L − L)fi∗ = fiLfi∗ + fiQ⊥((L − L)fi∗ − fiQ((L − L)fi∗ ,
0.95
0.9
E S M d e z
0.85
0.8 i l a m r o N
0.75
0.7 1
0.24
0.23
0.22
0.21
0.2
0.19
E S M d e g a r e v A
2 5 Index of the training ratio
3
4
0.18
0.17
6
0.16
1 where the inequality above follows from the triangle inequality and the last equality above follows from Eq ( 20 ) . Moreover , fiLfi∗ +fiQ⊥((L − L)fi∗−fiQ((L − L)fi∗
' fi(L − Lfi∗ + fiLfi∗−fi(Lfi∗ fi ≤ fi(L − Lfi∗ + fiLfi∗− ≤ 2fiQ((L − L)fi∗ .
2 5 Index of the training ratio
3
4
6
We complete the proof of this corollary .
Figure 3 : Sensitivity study on RMTL : study the effect of the training ratio in terms of nMSE ( left plot ) and aMSE ( right plot ) , respectively . The regularization parameters are determined via double cross validation . The i th coordinate on the x axis corresponds to the training ratio i × 10 % . l e a c s − g o l n i r e b m u n n o i t a r e t I
12
10
8
6
4
2
0
1
APM PM
2
3
4
5
Index of the stopping criterion
6 l e a c s − g o l n i e m i t n o i t a t u p m o C
7
6
5
4
3
2
1
0
−1
−2
1
APM PM
2
3
4
5
Index of the stopping criterion
6
Figure 4 : Computation cost comparison of APM and PM in terms of the iteration number ( left plot ) and the computation time in seconds ( right plot ) for solving RMTL . The i th coordi−i . nate on the x axis corresponds to the stopping criterion 10
Acknowledgment This research is sponsored in part by NSF IIS 0953662 and NSF CCF 1025177 .
APPENDIX Proof of Corollary 4.1
PROOF . From Lemma 4.1 , we have
( L − L = Q((L − L ) + Q⊥((L − L )
Proof of Lemma 4.2
It follows that
PROOF . From the definition of C(S ) in Eq ( 21 ) , we have
SC⊥(S ) = 0 , fi((S − S)C⊥(S)fi1,2 = fi(SC⊥(S)fi1,2 . fi((S − S)C⊥(S)fi1,2 + fiSfi1,2 − fi(Sfi1,2 = fi(SC⊥(S)fi1,2 + fiSfi1,2 − fi(Sfi1,2 = fiSC(S)fi1,2 − fi(SC(S)fi1,2 ≤ fi(S − ( S)C(S)fi1,2 = fi((S − S)C(S)fi1,2 .
By substituting the equation above into the left side of Eq ( 22 ) , we complete the proof of this lemma .
LEMMA 1 . Let δ1 , δ2,··· , δn be a random sample of size n from the Gaussian distribution N ( 0 , σ ) . Let x1 , x2,··· , xn satisfy x2 1 + x2 n = 1 . Denote a random variable v as
2 + ··· + x2 nff i=1 v =
1 σ xiδi .
Then v obeys the Gaussian distribution N ( 0 , 1 ) .
PROOF . Since {δi} are mutually independent , the mean of the random variable v can be computed as ffl fi nff fi i=1
1 σ
1 σ2 nff i=1 nff i=1
1 σ ffl
E(v ) = E xiδi
= xiE ( δi ) = 0 .
Similarly , the variance of v can be computed E ( v − E(v ) )
= E
2 i δ
=
2 i x
2 nff i=1
1 σ2
2 i
δ
2 i E x
= 1 , where the first equality follows from E ( δiδj ) = 0 ( i ffi= j ) . Using the fact that the sum of Gaussian random variables is Gaussian distributed , we complete the proof of this lemma .
LEMMA 2 . Let X 2 degrees of freedom . Then p ≥ p + π
Pr
X 2 p be a chi squared random variable with p
≤ exp X 2
π − p log ffffff ≤ Pr ( N0,1 ≥ zp(q ) ) , q > p , p ≥ q
− 1 2
1 +
π p
Pr
PROOF . From Theorem 4.1 in [ 31 ] , we approximate the chi square distribution using a normal distribution as
, π > 0 . fi
' fl
− 1 2 where N0,1 ∼ N ( 0 , 1 ) and zp(q ) = . It is known that for x ∼ N ( 0 , 1 ) , the inequality Pr ( x ≥ t ) ≤ exp(− t2 2 ) holds . Therefore we have p ≥ q
≤ exp q − p − p log
X 2
2 p(q ) z ff
Pr q p
.
By substituting q = p + π ( π > 0 ) into the inequality above , we complete the proof of this lemma .
A . REFERENCES [ 1 ] R . K . Ando and T . Zhang . A framework for learning predictive structures from multiple tasks and unlabeled data . Journal of Machine Learning Research , 6:1817–1853 , 2005 . [ 2 ] A . Argyriou , T . Evgeniou , and M . Pontil . Convex multi task feature learning . Machine Learning , 73(3):243–272 , 2008 .
[ 3 ] B . Bakker and T . Heskes . Task clustering and gating for bayesian multitask learning . Journal of Machine Learning Research , 4:83–99 , 2003 .
[ 4 ] A . Beck and M . Teboulle . A fast iterative shrinkage thresholding algorithm for linear inverse problems . SIAM Journal of Imaging Science , 2:183–202 , 2009 .
[ 5 ] P . J . Bickel , Y . Ritov , and A . B . Tsybakov . Simultaneous analysis of lasso and dantzig selector . Annals of Statistics , 37:1705–1732 , 2009 .
[ 6 ] J F Cai , E . J . Candes , and Z . Shen . A singular value thresholding algorithm for matrix completion . SIAM Journal on Optimization , 20(4):1956–1982 , 2010 .
[ 7 ] R . Caruana . Multitask learning . Machine Learning ,
28(1):41–75 , 1997 .
[ 8 ] J . Chen , J . Liu , and J . Ye . Learning incoherent sparse and low rank patterns from multiple tasks . In KDD , 2010 .
[ 9 ] J . Chen , L . Tang , J . Liu , and J . Ye . A convex formulation for learning shared structures from multiple tasks . In ICML , 2009 .
[ 10 ] T . Evgeniou , C . A . Micchelli , and M . Pontil . Learning multiple tasks with kernel methods . Journal of Machine Learning Research , 6:615–637 , 2005 .
[ 11 ] D . Goldfarb and S . Ma . Convergence of fixed point continuation algorithms for matrix rank minimization . Submitted to Foundations of Computational Mathematics .
[ 12 ] D . Hsu , S . Kakade , and T . Zhang . Robust matrix decomposition with outliers . arXiv:1011.1518 , 2010 . [ 13 ] L . Jacob , F . Bach , and J P Vert . Clustered multi task learning : A convex formulation . In NIPS , 2008 .
[ 14 ] A . Jalali , P . Ravikumar , S . Sanghavi , and C . Ruan . A dirty model for multi task learning . In NIPS , 2010 .
[ 15 ] S . Ji and J . Ye . An accelerated gradient method for trace norm minimization . In ICML , pages 457–464 , 2009 . [ 16 ] S . Kim and E . P . Xing . Tree guided group lasso for multi task regression with structured sparsity . In ICML , 2010 .
[ 17 ] N . D . Lawrence and J . C . Platt . Learning to learn with the informative vector machine . In ICML , 2004 .
[ 18 ] J . Liu , J . Chen , and J . Ye . Large scale sparse logistic regression . In KDD , 2009 .
[ 19 ] J . Liu , S . Ji , and J . Ye . Multi task feature learning via efficient l2,1 norm minimization . In UAI , pages 339–348 , 2009 .
[ 20 ] J . Liu , S . Ji , and J . Ye . SLEP : Sparse Learning with Efficient
Projections . Arizona State University , 2009 .
[ 21 ] J . Liu , L . Yuan , and J . Ye . An efficient algorithm for a class of fused lasso problems . In KDD , 2010 .
[ 22 ] K . Lounici , M . Pontil , A . B . Tsybakov , and S . van de Geer .
Taking advantage of sparsity in multi task learning . In COLT , 2008 .
[ 23 ] J J Moreau . Proximité et dualité dans un espace hilbertien .
Bull . Soc . Math . France , 93:273–299 , 1965 .
[ 24 ] A . Nemirovski . Efficient Methods in Convex Programming .
Lecture Notes , 1995 .
[ 25 ] Y . Nesterov . Introductory Lectures on Convex Programming .
Lecture Notes , 1998 .
[ 26 ] F . Nie , H . Huang , X . Cai , and C . Ding . Efficient and robust feature selection via joint l21 norms minimization . In NIPS , 2010 .
[ 27 ] G . Obozinski , B . Taskar , and M . I . Jordan . Multi task feature selection . In Technical report , Dept . of Statistics , UC Berkeley , 2006 .
[ 28 ] B . Recht , M . Fazel , and P . A . Parrilo . Guaranteed minimum rank solutions to linear matrix equations via nuclear norm minimization . SIAM Review , ( 3):471–501 , 2010 .
[ 29 ] A . Schwaighofer , V . Tresp , and K . Yu . Learning gaussian process kernels via hierarchical bayes . In NIPS , 2004 .
[ 30 ] S . Thrun and J . O’Sullivan . Discovering structure in multiple learning tasks : The TC algorithm . In ICML , 1996 .
[ 31 ] D . L . Wallace . Bounds on normal approximations to student ’s and the chi square distributions . Annals of Mathematical Statistics , 30(4):1121–1130 , 1959 .
[ 32 ] H . Xu , C . Caramanis , and S . Sanghavi . Robust pca via outlier pursuit . In NIPS , 2010 .
[ 33 ] Y . Xue , X . Liao , L . Carin , and B . Krishnapuram . Multi task learning for classification with dirichlet process priors . Journal of Machine Learning Research , 8:35–63 , 2007 .
[ 34 ] X . Yang , S . Kim , and E . P . Xing . Heterogeneous multitask learning with joint sparsity constraints . In NIPS , 2009 .
[ 35 ] K . Yu , V . Tresp , and A . Schwaighofer . Learning gaussian processes from multiple tasks . In ICML , 2005 .
[ 36 ] S . Yu , V . Tresp , and K . Yu . Robust multi task learning with t processes . In ICML , 2007 .
[ 37 ] J . Zhang , Z . Ghahramani , and Y . Yang . Learning multiple related tasks using latent independent component analysis . In NIPS , 2005 .
[ 38 ] Y . Zhang and D Y Yeung . Multi task learning using generalized t process . In AISTATS , 2010 .
