Diversified Ranking on Large Graphs :
An Optimization Viewpoint
Hanghang Tong
Jingrui He
Zhen Wen
Ravi Konuru
Ching Yung Lin
IBM TJ Watson Research Center
Hawthorne , NY , USA
{htong , jingruhe , zhenwen , rkonuru , chingyung}@usibmcom
ABSTRACT Diversified ranking on graphs is a fundamental mining task and has a variety of high impact applications . There are two important open questions here . The first challenge is the measure how to quantify the goodness of a given top k ranking list that captures both the relevance and the diversity ? The second challenge lies in the algorithmic aspect how to find an optimal , or near optimal , top k ranking list that maximizes the measure we defined in a scalable way ?
In this paper , we address these challenges from an optimization point of view . Firstly , we propose a goodness measure for a given top k ranking list . The proposed goodness measure intuitively captures both ( a ) the relevance between each individual node in the ranking list and the query ; and ( b ) the diversity among different nodes in the ranking list . Moreover , we propose a scalable algorithm ( linear wrt the size of the graph ) that generates a provably near optimal solution . The experimental evaluations on real graphs demonstrate its effectiveness and efficiency .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications – Data Mining
General Terms Algorithm , experimentation
Keywords Diversity , ranking , scalability , graph mining 1 .
INTRODUCTION
Given an author paper network , how to find the top k most related conferences for a given author ? How to diversify the ranking list so that it captures the whole spectrum of the given author ’s research interest ? It is now widely realized that diversity is a key factor to address the uncertainty and ambiguity in an information need ; and to cover the different aspects of the information need [ 32 ] .
Diversity is also positively associated with personnel performance and job retention rate in a large organization [ 38 ] .
Despite their own success of the previous works ( See Section 6 for a review ) , two important questions remain open in diversified ranking on large graphs . The first challenge is the measure for a given top k ranking list , how can we quantify its goodness ? Intuitively , a good top k ranking list should capture both the relevance and the diversity . For example , given a task which typically requires a set of different skills , if we want to form a team of experts , not only should the people in the team have relevant skills , but also they should somehow be ‘different’ from each other so that the whole team can benefit from the diversified , complementary knowledge and social capital . However , there does not exist such a goodness measure for the graph data in the literature . Most of the existing works for diversified ranking on graphs are based on some heuristics . The only exception is [ 27 ] , where the authors made an important step towards this goal by providing some optimization explanations , which is achieved by defining a time varying objective function at each iteration . But still , it is not clear what overall objective function the algorithm tries to optimize .
The second challenge lies in the algorithmic aspect how can we find an optimal , or near optimal , top k ranking list that maximizes the goodness measure ? Bringing diversity into the design objective implies that we need to optimize on the set level . In other words , the objective function for a subset of nodes is usually not equal to the sum of objective functions of each individual nodes . It is usually very hard to perform such set level optimization . For instance , a straight forward method would need exponential enumerations to find the exact optimal solution , which is infeasible even for medium size graphs . This , together with the fact that real graphs are often of large size , reaching billions of nodes and edges , poses the challenge for the optimization algorithm how can we find a near optimal solution in a scalable way ?
In this paper , we address these challenges from an optimization point of view . We propose a goodness measure which intuitively captures both ( a ) the relevance between each individual nodes in the ranking list and the query node ; and ( b ) the diversity among different nodes in the ranking list . We further propose a scalable algorithm ( linear wrt the size of the graph ) that generates a provably near optimal top k ranking list . To the best of our knowledge , this is the first work for diversified ranking on large graphs that ( 1 ) has a clear optimization formulation ; ( 2 ) finds a provably nearoptimal solution ; and ( 3 ) enjoys the linearly scalability . The main contributions of the paper are summarized as follows :
• A measure to quantify goodness for a top k ranking list that • An algorithm to find a diversified top k ranking list from captures both relevance and diversity ; large graphs ;
1028 Table 1 : Symbols
Definition and Description Symbol A , B , . . . matrices ( bold upper case ) A(i , j ) A(i , : ) A( : , j ) A . a , b , . . . I,J , . . . ⊗ r p I 1 0 n , m k c the element at the ith row and jth column of A the ith row of matrix A the jth column of matrix A transpose of matrix A vectors sets ( calligraphic ) element wise Hadamard product an n × 1 ranking vector an n × 1 query vector ( p(i ) ≥ 0 , an identity matrix a vector/matrix with all elements set to 1s a vector/matrix with all elements set to 0s the number of the nodes and edges in the graph the budget ( ie , the length of the ranking list ) the damping factor 0 < c <1
.n i=1 p(i ) = 1 )
• Proofs and complexity analysis , showing that our method is provably near optimal in terms of optimization quality with linear scalability ; • Extensive experimental evaluations , demonstrating the effec tiveness and efficiency of our method .
The rest of the paper is organized as follows . We introduce notation and formally define the problems in Section 2 . We present and analyze the proposed measure and algorithm in Section 3 and Section 4 , respectively . We provide experimental evaluation in Section 5 . We review the related work in Section 6 and conclude in Section 7 .
2 . PROBLEM DEFINITIONS
'
Table 1 lists the main symbols we use throughout the paper . In this paper , we consider the most general case of directed , weighted , irreducible unipartite graphs . We represent a general graph by its adjacency matrix1 . Following the standard notation , we use bold upper case for matrices ( eg , A ) , bold lower case for vectors ( eg , a ) , and calligraphic fonts for sets ( eg , I ) . We denote the transpose with a prime ( ie , A . is the transpose of A ) . For a bipartite graph fi with adjacency matrix W , we can convert it to the equivalent unipartite graph : A = . We use subscripts to denote the size of matrices/vectors ( eg , An×n means a matrix of size n× n ) . When the size of matrices/vectors are clear from the context , we omit such subscripts for brevity . Also , we represent the elements in a matrix using a convention similar to Matlab , eg , A(i , j ) is the element at the ith row and jth column of the matrix A , and A( : , j ) is the jth column of A , etc . With this notation , we can represent a sub matrix of A as A(I,I ) , which is a block of matrix A that corresponds to the rows/columns of A indexed by the set I .
0 W W 0
In this paper , we focus on personalized PageRank [ 30 , 11 ] since it is one of the most fundamental ranking methods on graphs , and has shown its success in many different application domains in the past decade . Formally , it can be defined as follows : where p is an n × 1 personalized vector ( p(i ) ≥ 0 , i=1 p(i ) = 1 ) . Sometimes , we also refer to p as the query vector . c ( 0 < c <
1In practice , we store these matrices using an adjacency list representation , since real graphs are often very sparse . r = cA.r + ( 1 − c)p
.n
( 1 )
.n 1 ) is a damping factor ; A is the row normalized adjacency matrix j=1 A(i , j ) = 1 ( i = 1 , , n ) ; and r is the n× of the graph ( ie , 1 resulting ranking vector . Note that if p(i ) = 1/n(i = 1 , , n ) , it is reduced to the standard PageRank [ 30 ] ; if p(i ) = 1 and p(j ) = 0(j '= i ) , the resulting ranking vector r gives the proximity scores from node i to all the other nodes in the graph [ 37 ] .
In order to simplify the description of our upcoming method , we also introduce the so called ‘Google matrix’ B :
B = cA . + ( 1 − c)p11×n
( 2 ) where 11×n is a 1 × n row vector with all elements set to 1s . Intuitively , the ‘Google matrix’ B can be viewed as the personalized adjacency matrix that is biased towards the query vector p . It turns out that the ranking vector r defined in eq . ( 1 ) satisfies r = Br . In other words , the ranking vector r is the right eigenvector of the B matrix with the eigenvalue 1 . It can be verified that B is a columnwise stochastic matrix ( ie , each column of B sums up to 1 ) . By Perron Frobenius theorem [ 10 ] , it can be shown that 1 is the largest ( in module ) simple eigenvalue of the matrix B ; and the ranking vector r is unique with all non negative elements since the graph is irreducible .
Our goal is two fold : ( 1 ) we want a goodness measure to quantify the quality of a given top k ranking list that captures both the relevance and the diversity ; and ( 2 ) given the goodness measure , we want an optimal or near optimal algorithm to find a top k ranking list that maximizes such goodness measure in a scalable way . With the above notations and assumptions , our problems can be formally defined as follows :
PROBLEM 1 . ( Goodness Measure . )
Given : A large graph An×n , the query vector p , the damping factor c , and a subset of k nodes S ; Output : A goodness score f(S ) of the subset of nodes S , which measures ( a ) the relevance of each node in S wrt the query vector p , and ( b ) the diversity among all the nodes in the subset S .
PROBLEM 2 . ( Diversified Top k Ranking Algorithm . )
Given : A large graph An×n , the query vector p , the damping facFind : A subset of k nodes S that maximizes the goodness measure tor c , and the budget k ; f(S ) .
In the next two sections , we present our solutions for these two problems respectively . 3 . THE PROPOSED GOODNESS MEASURE In this section , we address Problem 1 . Our goal is to define a goodness measure to quantify the quality of a given top k ranking list that captures both the relevance and the diversity . We first discuss some design objectives of such a goodness measure ; and then present our solution followed by some theoretical analysis . 3.1 Design Objectives
As said before , a good diversified top k ranking list should balance between the relevance and the diversity . The notion of relevance is clear for personalized PageRank , larger value in the ranking vector r means more relevant wrt the query vector p . On the other hand , the notion of diversity is more challenging . Intuitively , a diversified subset of nodes should be dis similar with each other . Take the query ‘Find the top k conferences for Philip Yu from the author conference network’ as an example . Dr . Philip
1029 still output an ordered subset based on the diminishing returns need when the user is seeking for a diverse top k ranking list . 3.3 Proofs and Analysis
Let us analyze how the proposed goodness measure meets our design objectives in subsection 31
There are two terms in eq ( 3 ) , the first term is twice the sum of the ranking scores in the ranking list . For the second term , recall that B can be viewed as the personalized adjacency matrix wrt the query vector p , where B(i , j ) indicates the similarity ( ie , the strength of the connection ) between nodes i and j . In other words , the second term in eq ( 3 ) is the sum of all the similarity scores between any two nodes i , j(i , j ∈ S ) in the ranking list ( weighted by r(j) ) . Therefore , the proposed goodness measure captures both the relevance and the diversity . The more relevant ( higher r(i ) ) each individual node is , the higher the goodness measure f(S ) . At the same time , it encourages the diversity within the ranking list by penalizing the ( weighted ) similarity between any two nodes in S . The proposed measure f(S ) also exhibits the diminishing returns property , which is summarized in Theorem 1 . The intuitions of Theorem 1 are as follows : ( 1 ) by P1 , it means that the utility of an empty ranking list is always zero ; ( 2 ) by P2 , if we add more nodes into the ranking list , the overall utility of the ranking list does not decrease ; and ( 3 ) by P3 , the marginal utility of adding new nodes is relatively small if we already have a large ranking list .
THEOREM 1 . Diminishing Returns Property of f(S ) . Let Φ be an empty set ; I , J , R be three sets st , I ⊆ J , and R∩J = Φ . The following facts hold for f(S ) : P1 : f(Φ ) = 0 ; P2 : f(S ) is monotonically non decreasing , ie , f(I ) ≤ f(J ) ; P3 : f(S ) is submodular , ie , f(I ∪R)−f(I ) ≥ f(J ∪R)−f(J ) . PROOF of P1 . It is obviously held by the definition of f(S ) . 2 PROOF of P2 . Let T = J \ I . Substituting eq ( 3 ) into f(J ) − f(I ) and canceling the common terms , we have ff
B(i , j)r(j ) − ff
B(i , j)r(j ) i∈T j∈J
= 2
= ( f(J ) − f(I ) ff r(i ) − ff r(j ) − ff j∈T i∈T i∈I ff ff ff ff ff ff j∈T i∈I j∈T
+( r(i ) − i∈T i∈T j∈J
B(i , j)r(j ) )
B(i , j)r(j ) )
( 4 )
Recall that the matrix B is a column wise stochastic matrix ( ie , each column of B sums up to 1 ) . The first half of eq ( 4 ) satisfies
Yu is a professor at University of Illinois at Chicago . His recent major research interest lies in databases and data mining . He also has broad interests in several related domains , including systems , parallel and distributed processing , web applications , and performance modeling , etc . A top k ranking list for this query would have high relevance if it consists of all the conferences from databases and data mining community ( eg , SIGMOD , VLDB , KDD , etc ) since all these conferences are closely related to his major research interest . However , such a list has low diversity since these conferences are too similar with each other ( eg , having a large overlap of contributing authors , etc ) . Therefore , if we replace a few databases and data mining conferences by some representative conferences in his other research domains ( eg , ICDCS for distributed computing systems , WWW for web applications , etc ) , it would make the whole ranking list more diverse ( eg , the conferences in the list are more dis similar with each other ) .
Furthermore , if we go through the ranking list from top down , we would like to see the most relevant conferences to appear first in the ranking list . For example , a ranking list in the order of ‘SIGMOD’,‘ICDCS’,‘WWW’ is better than ‘ICDCS’,‘WWW’,‘SIGMOD’ since databases ( SIGMOD ) is a more relevant research interest for Dr . Philip Yu , compared with distributed computing systems ( ICDCS ) , or web applications ( WWW ) . In this way , the user can capture Dr . Philip Yu ’s main research interest by just inspecting a few topranked conferences/nodes . This suggests the so called diminishing returns property of the goodness measure it would help the user to know better about Dr . Philip Yu ’s whole research interest if we return more conferences/nodes in the ranking list ; but the marginal benefit becomes smaller and smaller as we go down the ranking list . Another implicit design objective lies in the algorithmic aspect . The proposed goodness measure should also allow us to develop an effective and scalable algorithm to find an optimal ( or at least near optimal ) top k ranking list from large graphs . We will discuss and address this issue in the next section .
To summarize , for a given top k ranking list , we aim to provide a single goodness score that ( 1 ) measures the relevance between each individual node in the list and the query vector p ; ( 2 ) measures the similarity ( or dis similarity ) among all the nodes in the ranking list ; ( 3 ) exhibits some diminishing returns property wrt the size of the ranking list ; and ( 4 ) enables some effective and scalable algorithm to find an optimal ( or near optimal ) top k ranking list . 3.2 The Proposed Measure
Let A be the row normalized adjacency matrix of the graph , B be the ‘Google matrix’ defined in eq ( 2 ) , p be the personalized vector and r be the ranking vector . For a given ranking list S ( ie , S gives the indices of the nodes in the ranking list ; and |S| = k. ) , the proposed goodness measure is formally defined as follows :
Goodness Measure : f(S ) = 2 r(i ) − ff i∈S ff i,j∈S
B(i , j)r(j )
( 3 )
We can also represent f(S ) by using the matrix A instead : ff r(j )
A(j , i)r(j ) − ( 1 − c ) r(i ) − c ff ff ff f(S ) = 2 i∈S i,j∈S j∈S i∈S p(i )
=
=
B(i , j)r(j ) )
( j∈T ff ff ff j∈T j∈T
( i∈T ff ff ff i∈T r(j ) − r(j)(1 − ff r(j ) i /∈I r(i ) − ff i∈I ff ff j∈T
B(i , j ) ) i∈I B(i , j ) ≥ 0 ff ff ff j∈J i∈T
B(i , j)r(j ) )
( r(i ) − ff j∈J
B(i , j)r(j ) ≥ 0
B(i , j)r(j ) )
For the second half of eq ( 4 ) , we have that
( 5 )
( 6 ) where c is the damping factor in personalized PageRank , and 11×|S| is a row vector of length |S| with all the elements set to 1s . It can be shown that it is equivalent to eq . ( 3 ) . Notice that the goodness measure in eq ( 3 ) is independent on the ordering of the different nodes in the subset S . If we simply change the ordering of the nodes for the same subset S , it does not affect the goodness score . However , as we will show in Section 4 , we can
=
= i∈T j /∈J
1030 The last equality in eq ( 6 ) is due to the fact that r = Br , and each element in r is non negative . Putting eq ( 4) (6 ) together , we have that f(J ) ≥ f(I ) , which completes the proof of P2 . PROOF of P3 . Again , let T = J \ I . Substituting eq ( 4 ) into ( f(I ∪R)−f(I))− ( f(J ∪R)−f(J ) ) and canceling the common terms , we have
2
B(i , j)r(j ) )
= (
+ ( i∈J ff
( f(I ∪ R ) − f(I ) ) − ( f(J ∪ R ) − f(J ) ) ff ff ff ff B(i , j)r(j ) − ff ff i∈I B(i , j)r(j ) − ff ff j∈R ff ff ff i∈R j∈J ∪R j∈R i∈R
=
B(i , j)r(j ) +
B(i , j)r(j ) ) j∈I∪R B(i , j)r(j ) ≥ 0 j∈R i∈T i∈R j∈T
Therefore , we have that f(I ∪ R ) − f(I ) ≥ f(J ∪ R ) − f(J ) , which completes the proof of P3 .
2
4 . THE PROPOSED ALGORITHM
In this section , we address Problem 2 . Here , given the initial query vector p and the budget k , we want to find a subset of k nodes that maximizes the goodness measure defined in eq ( 3 ) . We first analyze the main challenges in optimizing eq ( 3 ) ; and then present the proposed algorithm DRAGON , followed by some theoretical analysis and discussion . 4.1 Challenges
Problem 2 is essentially a subset selection problem to find the optimal k nodes that maximize eq ( 3 ) . Theorem 1 indicates that it is not easy to find the exact optimal solution of Problem 2 it is NPhard to maximize a monotonic submodular function if the function value is 0 for an empty set [ 18 ] . For instance , a straight forward method would take exponential enumerations to find the exact optimal k nodes , which is not feasible in computation even for a medium size graph ( eg , with a few hundred nodes ) . n k
We can also formulate Problem 2 as a binary quadratic programming problem . Let xn×1 be a binary indicator vector ( x(i ) = 1 means node i is selected in the subset S , and 0 means it is not selected ) . Problem 2 can be expressed as the following binary quadratic programming problem : x.Dx x(i ) ∈ {0 , 1}(i = 1 , , n ) nff min Subject to : x(i ) = k
( 7 ) where D = ( B−2In×n)diag(r ) , In×n is an identity matrix of size n × n , and diag(r ) is a diagonal matrix with r(i , i)(i = 1 , , n ) being the diagonal elements . i=1
Eq ( 7 ) is still not easy to solve due to ( 1 ) the binary constrains on the variable x and ( 2 ) the quadratic term in the objective function . If we relax the binary constrain on x as 0 ≤ x(i ) ≤ 1(i = 1 , , n ) , we can solve the relaxed problem by standard quadratic programming packages . We refer to this strategy as ‘Lin QP’ . However , there are two major limitations of this method . First of all , we do not know what the gap is between eq . ( 7 ) and its relaxed version . Therefore , it is not clear how good the final solution is in terms of maximizing the original goodness measure ( eq ( 3 ) ) even if we can solve the relaxed problem optimally2 . Second , most , if not all , of the existing quadratic programming packages require polynomial
2It is worth pointing out that it is not even easy to find an optimal solution for the relaxed problem by quadratic programming complexity in computation . This makes this strategy very slow , or even infeasible , for a graph with more than a few thousand nodes . Another possible solution for eq . ( 7 ) is to remove the quadratic term in the objective function as follows . Starting from some initial indicator vector ˆx , we iterate between the following two steps : ( 1 ) approximate the objective function in eq . ( 7 ) by its first order Taylor expansion around ˆx ; and ( 2 ) update ˆx by solving a binary integer programming problem for the approximated objective function , which is linear wrt x . We refer to this strategy as ‘Ite BIP’ . However , the two main issues still exist : ( 1 ) it is not clear how such approximation will downgrade the overall optimization performance ; ( 2 ) the binary integer programming itself , again , requires polynomial time , which does not scale to large graphs . 4.2 The Proposed DRAGON Algorithm
Our proposed DRAGON algorithm is presented in Alg . 1 . In step 1 , we compute the ranking vector r ( eg , by the power method , etc ) . Then after some initializations ( steps 2 5 ) , we select k nodes one by one as follows . At each time , we compute the score vector s in step 7 . Then , we select one node with the highest score in the vector s and add it to the subset S ( steps 8 9 ) . After that , we use the selected node to update the two reference vectors u and v ( steps 1011 ) . Note that ‘⊗’ denote the element wise product between two matrices/vectors . Intuitively , the score vector s keeps the marginal contribution of each node for the goodness measure given the current selected subset S . From step 7 , it can be seen that at each iteration , the values of such marginal contribution either keeps unchanged or decreases . This is consistent with P3 of Theorem 1 as there are more and more nodes in the subset S , the marginal contribution of each node is monotonically non increasing . It is worth pointing out that we use the original normalized adjacency matrix A , instead of the ‘Google matrix’ B in Alg . 1 . This is because for many real graphs , the matrix A is often very sparse , whereas the matrix B might not be3 . In the case B is dense , it is not efficient in either time or space to use B in Alg . 1 .
In Alg . 1 , although we try to optimize a goodness measure that is not affected by the ordering of different nodes in the subset , we can still output an ordered list to the user based on in which iteration these nodes are selected earlier selected nodes in Alg . 1 are placed at the top of the resulting top k ranking list . This ordering naturally meets the diminishing returns need when the user is seeking for a diverse top k ranking list as we analyzed in subsection 31 4.3 Proofs and Analysis
Here , we analyze the optimality as well as the complexity of the proposed algorithm . We show that our DRAGON leads to a nearoptimal solution , and at the same time it enjoys linear scalability in both time and space .
Optimality . The optimality of the proposed DRAGON is given in Lemma 1 . According to Lemma 1 , our DRAGON is near optimal its solution is within a fixed fraction ( 1 − 1/e ≈ 0.63 ) from the global optimal one . Given the hardness of Problem 2 , such nearoptimality is acceptable in terms of optimization quality .
LEMMA 1 . Near Optimality of DRAGON . Let S be the subset found by DRAGON ; |S| = k ; and S∗ = argmax|S|=kf(S ) . We have that f(S ) ≥ ( 1− 1/e)f(S∗ ) , where e is the base of the natural logarithm .
PROOF . Omitted for Brevity
2 because the matrix D ( 1 ) might be asymmetric and ( 2 ) is not always semi positive definite . 3To see this , notice that B is a full matrix if p is uniform .
1031 Initialize ˆs(i ) = ( 2 − cA(i , i ) − ( 1 − c)p(i))r(i ) ;
Algorithm 1 DRAGON for Problem 2 Input : The row normalized adjacency matrix A of the graph , the damping factor c , the query vector p , and the budget k ; Output : A subset of k nodes S . 1 : Compute the ranking vector r : r = cA.r + ( 1 − c)p ; 2 : Initialize S as the empty set ; set u = v = 0n×1 ; 3 : for i = 1 : n do 4 : 5 : end for 6 : for iter = 1 : k do 7 : 8 : 9 : 10 : 11 : 12 : end for 13 : Return the subset S
Compute the score vector s = ˆs − u ⊗ r − v ; Find i = argmaxj s(j)(j = 1 , , n ; j /∈ S ) ; Add node i into S ; Update u ← u + cA( : , i ) + ( 1 − c)p(i)1n×1 ; Update v ← v + cA.( : , i)r(i ) + ( 1 − c)r(i)p ;
Time Complexity . The time complexity of the proposed DRAGON is given in Lemma 2 . According to Lemma 2 , our DRAGON has linear time complexity wrt the size of the graph . Therefore it is scalable to large graphs in terms of computational time .
LEMMA 2 . Time Complexity of DRAGON . The time complex ity of Alg . 1 is O(m + nk ) .
PROOF . Omitted for brevity . 2 We would like to point out that the proposed DRAGON can be further sped up . Firstly , notice that the O(m ) term in Lemma 2 comes from computing the ranking vector r ( step 1 ) by the most commonly used power method . There are a lot of fast methods for computing r , either by effective approximation ( eg , [ 37] ) , or by parallelism ( eg [ 13] ) . These methods can be naturally plugged in our DRAGON , which might lead to further computational savings . Secondly , the O(nk ) term in Lemma 2 comes from the greedy selection step in steps 6 12 . Thanks to the monotonicity of f(S ) as we show in Theorem 1 , we can use the similar lazy evaluation strategy as [ 20 ] to speed up this process , without sacrificing the optimization quality .
Space Complexity . The space complexity of the proposed DRAGON is given in Lemma 3 . According to Lemma 3 , our DRAGON has linear space complexity wrt the size of the graph . Therefore it is also scalable to large graphs in terms of space cost .
LEMMA 3 . Space Complexity of DRAGON . The space com plexity of Alg . 1 is O(m + n + k ) .
PROOF . Omitted for brevity .
2
4.4 Discussion Comparisons
In literature , there exist two other methods to encourage diversity in the top k ranking list for personalized PageRank . Here , we make a comparison in terms of optimality , convergence , and scalability of different methods . ARW [ 42 ] is based on an intuitive heuristic by greedily selecting the highest ranked node and setting it as the absorbing state . From theoretical point of view , it is not clear what ARW [ 42 ] tries to optimize . And also , it requires a matrix inverse of the same size of the graph , which is not scalable to large graphs . RRW [ 27 ] is based on vertex reinforced random walk [ 31 ] . Compared with ARW [ 42 ] , it makes an important step forward by providing some optimization explanations via defining a time varying objective function that changes at each iteration step . However , it is still not clear what overall metric it tries to measure ; and how good
Table 2 : Comparison of different methods . Our proposed DRAGON is the only method that leads to a near optimal solution with linear scalability .
Optimality
Scalability Convergence
Method Measure ARW [ 42 ] RRW [ 27 ] DRAGON
Partial Yes
NA
NA NA
Near optimal
No Yes Yes
Yes NA Yes its optimization solution is . Moreover , RRW [ 27 ] introduced some modifications and approximation techniques to the original vertex reinforced random walk , and it is not clear how the modified vertex reinforcement random walk converges4 .
5 . EXPERIMENTAL EVALUATIONS
In this section , we provide empirical evaluations for the proposed DRAGON . Our evaluations mainly focus on ( 1 ) the effectiveness and ( 2 ) efficiency of the proposed DRAGON . 5.1 Experimental Setup
Data sets . We use the DBLP publication data5 to construct a co authorship network , where each node is an author and the edge weight is the number of the co authored papers between the two corresponding persons . Overall , we have n = 418 , 236 nodes and m = 2 , 753 , 798 edges . We also construct much smaller co authorship networks , using the authors from only one conference ( eg , KDD , SIGIR , SIGMOD , etc ) For example , KDD is the co authorship network for the authors in the ‘KDD’ conference . These smaller co authorship networks typically have a few thousand nodes and up to a few tens of thousands edges . We also construct the co authorship networks , using the authors from multiple conferences ( eg , KDD+SIGIR ) . For these graphs , we denote them as Sub(n,m ) , where n and m are the numbers of nodes and edges in the graph , respectively .
Machine configurations . For the computational cost and scalability , we report the wall clock time . All the experiments ran on the same machine with four 2.4GHz AMD CPUs and 48GB memory , running Linux ( 2.6 kernel ) . For all the quantitative results , we randomly generate a query vector p and feed it into different methods for a top k ranking list with the same length . We repeat it 100 times and report the average .
Evaluation criteria . To the best of our knowledge , there is no universally accepted measure for diversity . In [ 27 ] , the authors suggested an intuitive notion based on the density of the induced subgraph from the original graph A by the subset S . The intuition is as follows : the lower the density ( ie , the less 1 step neighbors ) of the induced subgraph , the more diverse the subset S . Here , we generalize this notion to the t step graph in order to also take into account the effect of those in direct neighbors . Let Sign( . ) be a binary function operated element wise on a matrix , ie , Y = Sign(X ) , where Y is a matrix of the same size as X , Y(i , j ) = 1 if X(i , j ) > 0 , .t Y(i , j ) = 0 otherwise . We define the t step connectivity matrix Ct as Ct = Sign( i=1 Ai ) . That is , Ct(i , j ) = 1 ( 0 ) means that node i can ( cannot ) reach node j on the graph A within tsteps/hops . With this Ct matrix , we define the diversity of a given subset S as eq ( 8 ) . Here , the value of Div(t ) is always between 0.5 and 1 higher means more diverse . If all the nodes in S are reachable from each other within t steps , we say that the subset S 4Even if it converges , its stationary state might not be unique according to [ 31 ] . 5http://wwwinformatikuni trierde/˜ley/db/
1032 is the least diverse ( Div(t ) = 05 ) On the other extreme , If all the nodes in S cannot reach each other within t steps , the subset S is the most diverse ( Div(t ) = 1 ) .
Div(t ) =
1 +
1 i,j∈S,i=j Ct(i , j)/(|S| · ( |S| − 1 ) )
( 8 )
.
For the task of top k ranking , the notion of diversity alone , though important , might not be enough for the information need . For example , if we simply randomly select k nodes as the top k ranking list , these k nodes might not connected with each other at all given that the length of the ranking list k is usually much smaller than the number of nodes n in the graph . Therefore , it has a high diversity . However , it is unlikely that such a ranking list can well fit the user ’s information need since each of them might have very low relevance score . In other words , a diversified top k ranking list should also have high relevance . That said , we will mainly focus on evaluating how different methods balance between the diversity and the relevance .
Notice that the relevance score for each individual node is often very small on large graphs ( since the L1 norm of the ranking vector is 1 ) . To make the two quantities ( diversity vs . relevance ) comparable with each other , we need to normalize the relevance scores . Let ˆS be the top k ranking list by the original personalized PageRank , we define the normalized relevance score for a given subset S(|S| = k ) as eq ( 9 ) . Since the personalized PageRank always gives the k most relevant nodes , the Rel defined in eq ( 9 ) is always between 0 and 1 higher means more relevant .
. . i∈S r(i ) i∈ ˆS r(i )
Rel =
( 9 )
5.2 Effectiveness of DRAGON : Case Studies
Let us start with an illustrative example to gain some visual intuitions . In Fig 1 , we show a fictitious co authorship network , where each node corresponds to an author ( eg , John , Smith , etc ) , and the edge weight is the number of the co authored papers . There are three communities in this network ( eg , DM , DB and IR ) . From Fig 1 , we can see that node 1 has very strong connections to the DM community . In other words , DM might be his/her major research interest . In addition , s/he also has some connections to the IR and DB communities . Given the budge k = 3 , personalized PageRank returns all the three nodes ( nodes 2 , 3 and 5 ) from DM community which is consistent with the intuition since personalized PageRank solely focuses on the relevance . In contrast , the proposed DRAGON returns nodes 2 , 6 , and 10 , each of which is still relevant enough to the query node 1 . At the same time , they are diversified from each other , covering the whole spectrum of his/her research interest ( DM , DB , and IR ) .
We also conduct cast studies on real graphs . We construct a co authorship networks from SIGIR ( the major conference on information retrieval ) and ICML ( the major conference on machine learning ) . We issue a query to find the top 10 co authors for Prof . Yiming Yang . The results are shown in Fig 2 . We compare it with the original personalized PageRank . Yiming Yang is a professor from Carnegie Mellon University ; and she has broad interest in information retrieval and machine learning . From Fig 2 , we have the following observations . Firstly , both DRAGON and personalized PageRank share the same authors for the top 3 returned authors , indicating that DRAGON also captures those highly relevant authors wrt the querying author . Secondly , our DRAGON returns a more diverse list of authors . For example , although ChengXiang Zhai is not a co author of Yiming Yang , they shares a lot of research interest in information retrieval , and has a lot of indirect connections through other IR people . In contrast , the existence of some
Figure 1 : An illustrative example of a co authorship network with three communities . Given the query node 1 and the budget k = 3 , the proposed DRAGON returns three relevant and diversified nodes ( 2 , 6 , and 10 , in black ) . In contrast , personalized PageRank returns nodes 2 , 3 and 5 ( all from the DM community ) . authors in the ranking list by personalized PageRank is somehow redundant , in terms of helping the user to understand Prof . Yiming Yang ’s whole collaboration network . For example , Prof . Alex G . Hauptmann is also from Carnegie Mellon University . Although , he has a lot of co authored papers with Yiming Yang , they are also coauthored with Jian Zhang and Rong Jin . Therefore , given that Jian Zhang and Rong Jin are already in the ranking list , his existence does not provide much marginal information about Yiming Yang ’s collaboration network . As a quantitative indicator , the average degree of the induced subgraph by DRAGON is only 2.8 , which is much lower ( ie , more diverse ) than that by personalized PageRank ( 43 ) Finally , notice that for some authors , although they show up in both lists , their positions in the ranking list are different . For example , Jian Yun Nie shows at the 4th and the 8th positions in the two ranking lists , respectively . This is because Jian Yun Nie makes the top 4 authors more diverse compared with Thomas Pierce , although its individual relevance score is lower than the latter . 5.3 Comparison with Alternative Methods for
Diversified Ranking on Graphs
We compare the proposed DRAGON with ARW [ 42 ] and RRW [ 27 ] , both of which also aim to improve the diversity of personalized PageRank . We skip the comparison with MMR [ 6 ] for brevity since [ 27 ] shows that its performance is not as good as RRW for the graph type data . For RRW [ 27 ] , it has two variants based on different approximation methods it actually uses : the one based on the cumulative estimation ( referred to as ‘RRW a’ ) and the other one based on the pointwise estimation ( referred to as ‘RRW b’ ) .
First , let us compare how different methods balance between the relevance and the diversity . Fig 3 shows the results on the NIPS co authorship network . We test with different budgets ( k = 10 , 20 , 30 , 40 , 50 , 100 ) . In Fig 3 , Div(1 ) means that we only consider 1 step neighbors to measure the diversity ( ie , setting t = 1 in eq ( 8) ) . Div(2 ) means that we consider both 1 step and 2 step neighbors ( ie , setting t = 2 in eq ( 8) ) . We only present the results by RRW a since RRW b gives similar results . From Fig 3 , we can see that all the three methods are effective to improve the diversity . The proposed DRAGON achieves a better balance between the relevance and the diversity . For ARW , although it gives the highest diversity score , its ( normalized ) relevance score is too low only about half of the other two methods . This is because in ARW , only the first node is selected according to the relevance ;
1033 ( a ) Diversity by 1 step neighbors
( b ) Diversity by 1 and 2 step neighbors
Figure 2 : Top 10 authors for Prof . Yiming Yang . Our DRAGON return a relevant , but more diverse list of authors . The difference between the two lists is highlighted in black . and all the remaining ( k 1 ) are selected by diversity . As for RRWa , both its relevance and diversity scores are lower than the proposed DRAGON . It is interesting to notice from Fig 3(b ) that the diversity of RRW a drops a lot when it is measured by within 2step neighbors ( ie , Div(2) ) . This is consistent with the intuition of RRW . In RRW ( both RRW a and RRW b ) , it achieves the diversity by encouraging 1 step neighboring nodes to compete with each other . Consequently , the density of its within 1 step induced subgraph might low ( ie , high diversity ) , it is not necessary the case for the within t step ( t ≥ 2 ) induced subgraph .
Figure 3 : Diversity and Relevance trade off of different methods . highest optimization quality ( ie , highest f(S ) ) with least amount of wall clock time . The normalized f(S ) for ‘Lin QP’ is missing for Sub(24K,114K ) because it fails to finish within 100,000 seconds . This indicates that it is not feasible for large graphs . For the smaller graphs , ‘Lin QP’ leads to slightly lower f(S ) than the proposed DRAGON ; but it requires 3 5 orders of magnitude wall clock time . For all the other comparative methods , they lead to worse optimization quality with longer wall clock time . 5.5 Scalability
In order to test how the overall performance of different methods vary across different data sets , we take the average between relevance and diversity scores . The results are presented in Fig 4 , using four different co authorship networks ( SIGMOD , NIPS , SIGIR , SIGGRAPH ) . For the space limitation , we omit the results when the diversity is measured by within 1 steps neighbors , which is similar as the results by within 2 steps neighbors . It can be seen that the proposed DRAGON consistently performs the best .
5.4 Comparison with Alternative Optimiza tion Methods
Here , we evaluate the effectiveness and the efficiency of the proposed DRAGON in terms of maximizing the goodness measure f(S ) . We compared it with the two methods we introduced in subsection 41 We also compare it with two other heuristics . The first method ( referred to as ‘Heuristic1’ ) starts with generating a candidate pool ( eg , the top 10 × k most relevant nodes ) , picks one seed node , and then repeatedly adds the most dis similar ( measured by A ) node into the ranking list from the candidate pool . The second method ( referred to as ‘Heuristic2’ ) also starts with generating a candidate pool , puts all the nodes from candidate pool in the list , and then repeatedly drops a most similar ( measured by A ) node from the list . First , let us evaluate how the different methods balance between the optimization quality ( measured by f(S ) ) and the speed ( measured by wall clock time ) . Fig 5 shows the results from the coauthorship network of NIPS and KDD conferences with the budget k = 20 , where f(S ) is normalized by the highest one among different methods . It can be seen that the proposed DRAGON is the best it leads to the highest optimization quality ( ie , highest f(S ) ) with the least amount of wall clock time . Notice that the y axis is in logarithm scale .
We also conduct experiments on the co authorship network constructed from multiple conferences . Fig 6 shows the results on these data sets with the budget k = 20 . Here Sub(n,m ) means a co authorship network with n nodes and m edges . We stop the program if it takes more than 100,000 seconds ( ie , more than 10 days ) . It can be seen from Fig 6 that the proposed DRAGON is consistently best across all the different data sets it leads to
We also evaluate the scalability of DRAGON . When we evaluate the scalability wrt the number of the nodes in the graph , we fix the number of edges and vice versa . The results in Fig 7 are consistent with the complexity analysis in subsection 4.3 the proposed DRAGON scales linearly wrt both n and m , which means that it is suitable for large graphs .
K=5 K=20 K=50 K=100 K=200
100
90
80
70
60
50
40
30
20
) . c e s ( e m i t k c o c − l l l a w e g a r e v a
K=5 K=20 K=50 K=100 K=200
100
) c e s ( e m i t k c o c − l l l a w e g a r e v a
90
80
70
60
50
40
30
20
10
1
1.5
2
2.5 3 # of nodes
3.5
4 x 105
10
0
0.5
1 1.5 # of edges
2
2.5 x 106
( a ) Time vs . # of nodes
( b ) Time vs . # of edges
Figure 7 : Scalability of DRAGON . The proposed DRAGON scales linearly wrt the size of the graph .
6 . RELATED WORK
In this section , we review the related work , which can be categorized into four parts : ranking on graphs , diversity , set level optimization for data mining and general graph mining .
Ranking on Graphs . Personalized PageRank is one of the most fundamental and most widely used ranking methods on graphs . It has been successfully applied to many high impact applications [ 30 , 11].Many other ranking methods on graphs are built upon , and/or share the similar ideas as personalized PageRank , such as RalationalRank [ 9 , 3 ] , random walk with restart [ 37 ] , SimRank [ 22 ] , etc . Because of its generality and wide applicability , we choose Personalized PageRank as the starting point of our method . Other ranking methods on graphs include HITS [ 15 ] , electricity based methods [ 17 ] , etc . There also exist a lot of work to speed up the computation of personalized PageRank , such as [ 37 , 33 , 13 ] . It is worth pointing out that all these fast algorithms can be naturally plugged into the proposed DRAGON to gain further savings in computational time .
1034 1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
DRAGON ARW RRW_a RRW_b
. l e R d n a
) 2 ( v D i f o
. e v A
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
DRAGON ARW RRW_a RRW_b
. l e R d n a
) 2 ( v D i f o
. e v A
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
DRAGON ARW RRW_a RRW_b
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
DRAGON ARW RRW_a RRW_b
. l e R d n a
) 2 ( v D i f o
. e v A
. l e R d n a
) 2 ( v D i f o
. e v A
0.5
10
20
30
40
50 k
60
70
80
90
100
0.5
10
20
30
40
50 k
60
70
80
90
100
0.6
10
20
30
40
50 k
60
70
80
90
100
0.5
10
20
30
40
50
60
70
80
90
100 k
( a ) SIGMOD
( b ) NIPS
( c ) SIGIR
( d ) SIGGRAPH
Figure 4 : Comparison with alternative methods for diversified ranking on graphs . The average of the relevance and the diversity vs . the budget k ( larger is better ) . The proposed DRAGON(red star ) consistently performs the best .
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
)
S
( f d e z i l a m r o N
DRAGON Heuristic1 Heuristic2 Lin−QP Ite−BIP
>=100,000
10,000
1,000
) . c e S
(
DRAGON Heuristic1 Heuristic2 Lin−QP Ite−BIP i e m T k c o c − l l l a W g o L
100
10
1
0.1
0.01
Sub(2K,7K )
Sub(5K,19K )
Sub(9K,37K )
Sub(11K,45K ) Sub(24K,114K )
Comparison on wall clock time
( lower is better )
Figure 5 : Wall clock time vs . quality on the NIPS+KDD co authorship Network . The yaxis is in logarithm scale . The proposed DRAGON is the best . It has the highest f(S ) with the least amount of time .
0
Sub(2K,7K ) Sub(5K,37K )
Sub(9K,37K )
Sub(11K,45K ) Sub(24K,114K )
( a ) Comparison on the normalized f(S )
( higher is better )
Figure 6 : Comparison of different optimization methods . Our DRAGON(the left most one ) always leads to the highest f(S ) , with the least amount of time . Best viewed in color .
Diversity . It is now widely recognized that diversity is a highly desired property in many data mining tasks , such as expertise and legal search [ 32 ] , recommendation system [ 43 ] , blog filtering [ 7 ] , document summarization [ 6 ] , etc . It is a powerful tool to address the uncertainty and ambiguity ; and/or to cover the different aspects of an information need [ 32 ] . For the graph type data , [ 42 ] was among the first to address the diversified ranking on graphs . [ 27 ] proposed to balance between the relevance and diversity based on the vertex reinforcement random walk on graphs . However , they suffer from some important subtle issues as we show in subsection 44 There are also a lot of algorithms to improve the diversity for other types of data ( eg , document , etc ) , including [ 6 , 41 , 21 ] , etc .
Set level Optimization for Data Mining . In the recent years , set level optimizationhas been playing a very important role in many data mining tasks . Many set level optimization problems are NPhard . Therefore , it is difficult , if not impossible , to find the global optimal solutions . However , if the function is monotonic sub modular with 0 function value for the empty set , a greedy strategy can lead to a provably near optimal solution [ 18 ] . This powerful strategy has been recurring in many different settings , eg , immunization , outbreak detection , blog filtering , sensor placement , influence maximization , structure learning , etc . ( See [ 18 ] for a comprehensive review ) . In this paper , we introduce a new type of submodular function tailored for diversified ranking on large graphs .
General Graph Mining . There is a lot of work on graph mining . Representative works include pattern and law mining [ 5 ] , frequent substructure discovery [ 39 ] , compression [ 26 ] , fraud and anomaly detection [ 29 ] , community mining and graph partition [ 14 , 34 , 25 , 40 ] , social action tracking [ 36 ] , user click through modeling [ 1 , 2 ] , collaborative filtering [ 16 , 8 , 35 , 12 ] , term formation [ 19 ] , network classification [ 28 ] , link prediction [ 23 , 24 , 4 ] , etc .
7 . CONCLUSION
In this paper , we address the diversified ranking on large graphs from an optimization point of view . To the best of our knowledge , this is the first work for diversified ranking on large graphs that ( 1 ) has a clear optimization formulation ( see eq . ( 3) ) ; ( 2 ) finds provably near optimal solutions ( see Theorem 1 and Lemma 1 ) ; and ( 3 ) enjoys the linear scalability ( see Lemma 2 and Lemma 3 ) . Our experimental evaluations on real graphs validate that our method is ( 1 ) indeed effective to balance the relevance and the diversity in top k ranking ; and ( 2 ) scalable to large graphs .
8 . ACKNOWLEDGEMENT
Research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF09 2 0053 . The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies , either expressed or implied , of the Army Research Laboratory or the US Government . The US Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on .
9 . REFERENCES [ 1 ] C . L . 0001 , F . Guo , and C . Faloutsos . Bbm : bayesian browsing model from petabyte scale data . In KDD , pages 537–546 , 2009 .
[ 2 ] D . Agarwal , A . Z . Broder , D . Chakrabarti , D . Diklic ,
V . Josifovski , and M . Sayyadian . Estimating rates of rare events at multiple resolutions . In KDD , pages 16–25 , 2007 .
[ 3 ] A . Angel , S . Chaudhuri , G . Das , and N . Koudas . Ranking objects based on relationships and fixed associations . In EDBT’09 , pages 910–921 , 2009 .
1035 [ 4 ] L . Backstrom and J . Leskovec . Supervised random walks : predicting and recommending links in social networks . In WSDM , pages 635–644 , 2011 .
[ 5 ] A . Broder , R . Kumar , F . Maghoul1 , P . Raghavan ,
S . Rajagopalan , R . Stata , A . Tomkins , and J . Wiener . Graph structure in the web : experiments and models . In WWW Conf . , 2000 .
[ 6 ] J . G . Carbonell and J . Goldstein . The use of mmr , diversity based reranking for reordering documents and producing summaries . In SIGIR , pages 335–336 , 1998 .
[ 7 ] K . El Arini , G . Veda , D . Shahaf , and C . Guestrin . Turning down the noise in the blogosphere . In KDD , pages 289–298 , 2009 .
[ 8 ] Y . Ge , H . Xiong , A . Tuzhilin , K . Xiao , M . Gruteser , and M . J . Pazzani . An energy efficient mobile recommender system . In KDD , pages 899–908 , 2010 .
[ 9 ] F . Geerts , H . Mannila , and E . Terzi . Relational link based ranking . In VLDB , pages 552–563 , 2004 .
[ 10 ] G . H . Golub and C . F . V . Loan . Matrix Perturbation Theory .
The Johns Hopkins University Press , 1996 .
[ 24 ] R . Lichtenwalter , J . T . Lussier , and N . V . Chawla . New perspectives and methods in link prediction . In KDD , pages 243–252 , 2010 .
[ 25 ] A . S . Maiya and T . Y . Berger Wolf . Sampling community structure . In WWW , pages 701–710 , 2010 .
[ 26 ] H . Maserrat and J . Pei . Neighbor query friendly compression of social networks . In KDD , pages 533–542 , 2010 .
[ 27 ] Q . Mei , J . Guo , and D . R . Radev . Divrank : the interplay of prestige and diversity in information networks . In KDD , pages 1009–1018 , 2010 .
[ 28 ] J . Neville , B . Gallagher , and T . Eliassi Rad . Evaluating statistical tests for within network classifiers of relational data . In ICDM , pages 397–406 , 2009 .
[ 29 ] C . C . Noble and D . J . Cook . Graph based anomaly detection .
In KDD , pages 631–636 , 2003 .
[ 30 ] L . Page , S . Brin , R . Motwani , and T . Winograd . The
PageRank citation ranking : Bringing order to the web . Technical report , Stanford Digital Library Technologies Project , 1998 . Paper SIDL WP 1999 0120 ( version of 11/11/1999 ) .
[ 11 ] T . H . Haveliwala . Topic sensitive pagerank . WWW , pages
[ 31 ] R . Pemantle . Vertex reinforced random walk . Prob . Th . and
517–526 , 2002 .
Rel . Fields , pages 117–136 , 1992 .
[ 12 ] D . Heckerman , D . M . Chickering , C . Meek , R . Rounthwaite ,
[ 32 ] F . Radlinski , P . N . Bennett , B . Carterette , and T . Joachims . and C . M . Kadie . Dependency networks for collaborative filtering and data visualization . In UAI , pages 264–273 , 2000 .
Redundancy , diversity and interdependent document relevance . SIGIR Forum , 43(2):46–52 , 2009 .
[ 13 ] U . Kang , C . E . Tsourakakis , and C . Faloutsos . Pegasus : A peta scale graph mining system . In ICDM , pages 229–238 , 2009 .
[ 14 ] G . Karypis and V . Kumar . Multilevel way hypergraph partitioning . In DAC , pages 343–348 , 1999 .
[ 33 ] P . Sarkar and A . W . Moore . Fast nearest neighbor search in disk resident graphs . In KDD , pages 513–522 , 2010 .
[ 34 ] V . Satuluri and S . Parthasarathy . Scalable graph clustering using stochastic flows : applications to community discovery . In KDD , pages 737–746 , 2009 .
[ 15 ] J . M . Kleinberg . Authoritative sources in a hyperlinked
[ 35 ] H . Shan and A . Banerjee . Generalized probabilistic matrix environment . J . ACM , 46(5):604–632 , 1999 .
[ 16 ] Y . Koren . Collaborative filtering with temporal dynamics . In
KDD , pages 447–456 , 2009 .
[ 17 ] Y . Koren , S . C . North , and C . Volinsky . Measuring and extracting proximity in networks . In KDD , pages 245–255 , 2006 .
[ 18 ] A . Krause and C . Guestrin . Beyond convexity submodularity in machine learning . In ICML , 2008 .
[ 19 ] T . Lappas , K . Liu , and E . Terzi . Finding a team of experts in social networks . In KDD , pages 467–476 , 2009 .
[ 20 ] J . Leskovec , A . Krause , C . Guestrin , C . Faloutsos , J . M .
VanBriesen , and N . S . Glance . Cost effective outbreak detection in networks . In KDD , pages 420–429 , 2007 .
[ 21 ] L . Li , K . Zhou , G R Xue , H . Zha , and Y . Yu . Enhancing diversity , coverage and balance for summarization through structure learning . In WWW , pages 71–80 , 2009 .
[ 22 ] P . Li , H . Liu , J . X . Yu , J . He , and X . Du . Fast single pair simrank computation . In SDM , pages 571–582 , 2010 . [ 23 ] D . Liben Nowell and J . Kleinberg . The link prediction problem for social networks . In Proc . CIKM , 2003 . factorizations for collaborative filtering . In ICDM , pages 1025–1030 , 2010 .
[ 36 ] C . Tan , J . Tang , J . Sun , Q . Lin , and F . Wang . Social action tracking via noise tolerant time varying factor graphs . In KDD , pages 1049–1058 , 2010 .
[ 37 ] H . Tong , C . Faloutsos , and J Y Pan . Fast random walk with restart and its applications . In ICDM , pages 613–622 , 2006 . [ 38 ] L . Wu . Social network effects on performance and layoffs :
Evidence from the adoption of a social networking tool . Job Market Paper , 2011 .
[ 39 ] D . Xin , J . Han , X . Yan , and H . Cheng . Mining compressed frequent pattern sets . In VLDB , pages 709–720 , 2005 .
[ 40 ] X . Yin , J . Han , and P . S . Yu . Cross relational clustering with user ’s guidance . In KDD , pages 344–353 , 2005 .
[ 41 ] Y . Yue and T . Joachims . Predicting diverse subsets using structural svms . In ICML , pages 1224–1231 , 2008 .
[ 42 ] X . Zhu , A . B . Goldberg , J . V . Gael , and D . Andrzejewski .
Improving diversity in ranking using absorbing random walks . In HLT NAACL , pages 97–104 , 2007 .
[ 43 ] C N Ziegler , S . M . McNee , J . A . Konstan , and G . Lausen .
Improving recommendation lists through topic diversification . In WWW , pages 22–32 , 2005 .
1036
