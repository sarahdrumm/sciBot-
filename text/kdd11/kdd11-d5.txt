A Simple Statistical Model and Association Rule Filtering for Classification
György J . Simon
Dept . of Health Sciences
Research
Mayo Clinic , Rochester , MN simongyorgy@mayoedu
Vipin Kumar
Dept . of Computer Science
University of Minnesota ,
Minneapolis , MN kumar@csumnedu
Peter W . Li
Dept . of Health Sciences
Research
Mayo Clinic , Rochester , MN lipeter@mayoedu
ABSTRACT Associative classification is a predictive modeling technique that constructs a classifier based on class association rules ( also known as predictive association rules ; PARs ) . PARs are association rules where the consequence of the rule is a class label . Associative classification has gained substantial research attention because it successfully joins the benefits of association rule mining with classification . These benefits include the inherent ability of association rule mining to extract high order interactions among the predictors–an ability that many modern classifiers lack–and also the natural interpretability of the individual PARs .
Associative classification is not without its caveats . Association rule mining often discovers a combinatorially large number of association rules , eroding the interpretability of the rule set . Extensive effort has been directed towards developing interestingness measures , which filter ( predictive ) association rules after they have been generated . These interestingness measures , albeit very successful at selecting interesting rules , lack two features that are highly valuable in the context of classification . First , only few of the interestingness measures are rooted in a statistical model . Given the distinction between a training and a test data set in the classification setting , the ability to make statistical inferences about the performance of the predictive classification rules on the test set is highly desirable . Second , the unfiltered set of predictive assocation rules ( PARs ) are often redundant , we can prove that certain PARs will not be used to construct a classification model given the presence of other PARs .
In this paper , we propose a simple statistical model towards making inferences on the test set about the various performance metrics of predictive association rules . We also derive three filtering criteria based on hypothesis testing , which are very selective ( reduce the number of PARs to be considered by the classifier by several orders of magnitude ) , yet do not effect the performance of the classification ad versely . In the case , where the classification model is constructed as a logistic model on top of the PARs , we can mathematically prove , that the filtering criteria do not significantly effect the classifier ’s performance . We also demonstrate empirically on three publicly available data sets that the vast reduction in the number of PARs indeed did not come at the cost of reducing the predictive performance .
Categories and Subject Descriptors H28 [ Information Systems ] : Data Mining ; General Terms : Algorithms .
1 .
INTRODUCTION
Association rule mining [ 3 ] is the problem of discovering associations in a large database efficiently . Association rules are rules representing logical implications . Consider , for example , the rule glucose>125 → diabetes=Yes . This rule states that people with fasting glucose level exceeding 125 have diabetes with a specific probability , which is called the precision of the rule . The left hand side of the rule is its antecedent and the right hand side is its consequent .
Assume we have a database of patients with some descriptive statistics ( which we call variables ) and an outcome , presence or absence of diabetes . Further assume that the variables are binarized ( eg glucose>125 ) . Association rules that contain the outcome ( diabetes=Yes or diabetes=No ) as their consequent are predictive association rules ( PARs ) . Associative classification [ 21 ] is the problem of enumerating all PARs in a data set and constructing the classification model with these PARs as predictors of the outcome .
Associative classification has received much research attential over the years ( eg [ 21 , 20 , 29 , 30] ) , which is partly due to the ease of interpretation of the individual rules and partly due its good performance . Associative classification is less greedy than most other rule based classifiers , because associative classification enumerates all PARs , while rulebased classifiers [ 7 ] or trees [ 4 ] consider only a subset of the PARs . Linear models ( eg linear discriminant analysis [ 17 ] and regression models [ 15 , Chapter 3 ] ) tend to produce interpretable models , but they–with the notable exception of regression trees [ 5]–lack an inherent ability to discover interactions among the variables . Since PARs can be viewed as ( potentially ) high order interactions , the use of PARs is especially beneficial in domains such as genomics [ 14 ] and biomedical informatics [ 28 ] , where discovering interactions among the variables ( genes , disease ) is the focus of the study .
823 While the individual PARs are easily interpretable , association rule mining often discover a combinatorial set of rules . To help human users interpret these rule sets , a large number of interestingness measures have been proposed . After discovering association rules , these measures can be applied and the rules can be ranked . Section 6.7 of [ 25 ] provides an extensive survey of these measures .
The interestingness measures previously proposed , although successful at selecting interesting rules , fall short for the purpose of classification in two important ways .
First , most interestingness measure ( perhaps with exception of the lift [ 6 ] ) are not rooted in a statistical model . In the typical classification setting , the PARs are extracted from a training data set , a model is constructed on a training data set and the model is applied to a previously unseen test data set . Without a statistical model underlying the filtering criterion ( or interestingness measure ) , statistical inference cannot be made about the performance of the criterion on the test set .
Second , filtering criteria for classification should take the classifier ’s characteristics into account and only discard rules that cannot improve the classification performance .
In this paper , we develop a simple statistical model based on sampling theory that allows us to make inferences about the various aspects of PARs on a test set given a training set . Based on this model , we develop three filtering criteria that take the characteristics of the classifier into account . We show mathematically and also demonstrate empirically that our filtering criteria , albeit very selective , only discard rules that would not be utilized by the classifier .
Our paper assumes familiarity with formal hypothesis test ing ; [ 8 ] presents a clear deposition of the topic . Specifically , our contributions are as follows . • We develop a simple statistical framework for making inferences about measures related to association rules . • Based on the statistical model , we derive three filtering criteria with the explicit purpose of only discarding rules that are uninteresting to the classifier . Two of these criteria , closed itemsets [ 31 ] and predictive significance [ 6 , known as lift ] already exist , our contribution is to derive them from the perspective of the classification under the assumptions of our statistical model . The third criterion is proposed here for the first time . • We formally demonstrate via hypothesis testing that the above criteria indeed only discard PARs that do not significantly ( with respect to a user specified significance level ) impact the classifier ’s performance . We also provide empirical evidence on three publicly available large data sets .
2 . BACKGROUND AND RELATED WORKS Let an item be a predicate of a single term ( a logical statement that evaluates to true or false and does not contain logical OR or AND ) . For example , glucose>125 is such a predicate . A k−itemset is a set of k items . An itemset is also a predicate evaluating to the conjunction ( logical AND ) of the predicates corresponding to all k constituent items . For example { glucose>125 , diabetes=Yes } is a 2 itemset . Each item is also an itemset , a 1 itemset . We denote itemsets as a concatenation of their items : eg the 3 itemset {a , b , c} , will be denoted as abc .
A transaction is a set of items representing an observation in an experiment . The transaction database T is a collection of transactions . A transaction T is said to support an itemset I , if T ∧I = I : the predicates corresponding to all items in I evaluate to true in the transaction . We define the count of an itemset as the number of transactions in T that support it ; and the support of an itemset as the fraction of transactions that support it . An itemset is frequent if its support exceeds a user specified minimum support threshold . The minimum support threshold is related to the significance of the itemset . An association rule is of from A → B , where A and B are disjoint itemsets . An association rule can be thought of as a logical implication , with A being the antecedent , and B being the consequent of the rule ( implication ) . We define an association rule to evaluate to true only if both A and B are true . Association rules differ from the logical implications in this respect : if A is false , a logical implication evaluates to true while an association rule does not . The support of an association rule is the support of the itemset AB and the confidence of the association rule is confidence(A → B ) = s(AB ) s(A )
, where s(A ) denotes the support of A . In the context of predictive association rules , the confidence coincides with the predictive precision of the rule , hence we will refer to it as precision , too . An association rule is frequent , if its support exceeds the minimum support threshold .
An associative classification rule , also known as a predictive association rule ( PAR ) , is an association rule , whose consequent is an outcome ( or class label ) . The goal of associative classification is to build a classifier from frequent PARs .
Associative classification was first presented by Liu et al . [ 21 ] . They invented a technique called Classification Based on Association ( CBA ) , where a classifier is constructed from a collection of PARs by classifying each observation by the most precise PAR supported by the observation . The rule is added to the model and the observation is removed from the database . The model construction concludes when no more observations or applicable rules are left .
Motivated by CBA , a number of techniques have been proposed for constructing better classification models on top of PARs . For example , Li et al [ 20 ] proposed using multiple PARs for making a prediction for each observation leading to the Multiple Class Association Rules ( CMAR ) technique . Another improvement , Classification based on Predictive Association Rules ( CPAR ) , assigns weights to the observations and upon correct classification , the weight of the observation is reduced [ 29 ] . CPAR can be viewed as a precursor to boosting PARs into a classifier model , which was proposed by Yoon et al . [ 30 ] . Ozgur et al [ 22 ] proposed a procedure similar to growing decision trees on association rules for regression problems and Wang et al . [ 27 ] grew decision trees on PARs with minimum support of 0 . Emerging patterns – patterns whose support differs substantially between classes – have also been applied [ 10 , 19 ] .
The underlying theme for the above methods is to transform the classification problem . They transform the data set such that the predictors are the PARs instead of the original data variables . The PARs are presumed more predictive than the original data variables .
824 The use of PARs as predictors is advantageous for the following reasons . First , it allows for building a less greedy model . For example , decision trees heuristically prune the search space of conjunctions of the original predictors , while modeling on top of PARs readily offers the complete set of predictive conjunctions of the original variables . Second , the PARs are rules and hence they are individually easy to interpret . In contrast , SVM models are inherently difficult to interpret and even trees–as close as they are to rules–can get cumbersome to interpret due to the subtree replication problem ( see Chapter 437 in [ 25] ) . Third , additive models ( including regression models ) often have no ability to automatically discover interaction terms . PARs are in essence high order interaction terms–readily available to modeling . These advantages make PARs complementary to logistic regression , making logistic regression a natural choice for combining PARs into a classification model . Logistic regression analysis is a mature technique for classification problems that has enjoyed significant research effort to make model fitting possible and efficient under a wide variety of unfavorable conditions .
One such adverse condition arises when the number of predictors exceeds the number of observations . Given that association rule mining generally returns a very large number of predictors , such conditions can occur frequently . There are two specific techniques to address this issue . The first one is penalized logistic regression , more specifically Lasso regression [ 26 ] , which has built in predictor selection capability and the second one is stage wise regression [ 15 ] or boosting [ 13 ] . It has been shown that under some conditions , boosting and Lasso generate the same model [ 11 ] .
Logistic regression on top of PARs is closely related to logic regression [ 24 ] . A logic regression model is a logistic regression model , whose predictors are logical expressions of the predictors in the original data set . Logic regression aims to discover these logical expressions via simulated annealing . Logistic regression on top of PARs differs from logic regression in that ( i ) logic regression enumerates only a ( notnecessarily optimal ) subset of PARs and ( ii ) logic regression allows for disjunctions as well as conjunctions .
3 . METHOD
We begin by describing the computational framework in which we envision our proposed filtering techniques will be applied . In this framework , first , PARs are discovered . A wide range of algorithms can be used towards this end : the original apriori algorithm [ 3 ] , algorithms that discover closed itemsets [ 31 ] , discriminating or emerging patterns [ 10 ] . Second , the discovered patterns are filtered through one or more of the proposed criteria . For maximal efficiency we suggest the following order : Predictive Significance , Closed Itemsets and Component Independence . Third , a logistic regression model is built whose predictors are the filtered PARs .
In the following subsections , we first briefly describe the logistic regression analysis framework . Next , we present our statistical model for association rule mining and we conclude this section by describing our filtering criteria . 3.1 Logistic Regression
Let X be an N × p binary observation matrix , whose columns are the p PARs and rows are the N observations . Let Y be a vector denoting the outcomes of each observation and C be the outcome of interest ( target outcome ) . Further let yi be a binary response defined as yi = 1 if Yi = C ; yi = 0 , otherwise .
The logistic regression model is then logit(Y = C ) = log
= Xb + c ,
( 1 )
Pr(Y = C )
Pr(Y = C ) where b is the vector of p regression weights , one for each PAR ; and c is the weight of the constant term . Once the regression weights have been computed , prediction can be made for an instance xi
( cid:92)Pr(Yi = C ) = exp(xib + c )
1 + exp(xib + c )
.
( 2 )
Models can be fit via minimizing the binomial log likelihood ( in b and c )
−L = − yi log Pr(Yi = C ) + ( 1 − yi ) log Pr(Yi = C )
.
( 3 ) A Lasso penalized logistic regression model is fit using regularized negative log likelihood : a Lasso penalty term is added to L ( Equation 3 )
N i=1
N
− yi log Pr(Yi = C ) + ( 1 − yi ) log Pr(yi = C )
+λ
|bj| , i=1 j
( 4 ) where bj is the regression weight of the jth PAR and λ is the regularization parameter . The above maximization problem can be solved efficiently using coordinate wise optimization as described in [ 12 ] . 3.2 Statistical Model
Our statistical model is based on sampling theory . We assume that both the training and test data sets are random samples from a population of practically infinite transactions . Our goal is to estimate measures ( eg support or precision ) related to the PARs in the population from the training sample . The population measures allow us to make inferences about these measures in a test sample . Therefore , for each measure , we distinguish between the population and sample version .
To make this idea more concrete , let us consider the measure of support : the sample support of an itemset I is the fraction of items in the sample that support I ; the population support of an item is the fraction of transactions in the population that support I . Only the sample support can be observed , the population support needs to be estimated . We consider the population support a random variable . We can compute a point estimate for the population support as the plug in estimate
ˆs(I ) = cnt(I )
N
, where ˆs is the estimate of the population support , cnt(· ) is the count of I in the sample and N is the sample size ( number of transactions in the training set ) .
We can also compute a confidence interval using a binomial sampling distribution . Sampling from an infinite population can be viewed as a Bernoulli process , where transactions are selected at random , each supporting I with s(I )
825 ( population support ) probability . The count of I in the sample is then a binomial random variable cnt(I ) ∼ Binom(N , s(I) ) .
The ( 1 − α/2 ) confidence interval of the population support is thus
ˆs(I ) ( 1 − ˆs(I ) ) N−1 ,
( 5 ) where zα is the standard normal quantile of 1 − α/2 . Note , that the population support is a probability , hence we will use s(I ) and Pr(I ) interchangeably . Inference about the precision is more complex . Consider the predictive association rule ( PAR ) A → B , A and B are itemsets . The population precision of the PAR is
ˆs(I ) ± zα prec(A → B ) = s(AB ) s(A )
.
( 6 )
The limiting distributions of s(AB ) and s(A ) are both normal , so the distribution of precision can be considered a correlated quotient normal distribution . The formulae in [ 16 ] can be used to compute the quantiles of this distribution . Alternatively , if the covariance of s(A ) and s(AB ) is known , Fieller ’s theorem [ 1 ] can be applied to obtain a confidence interval for prec(A → B ) .
In some cases , it is useful to instantiate s(A ) by fixing its value to the plug in estimate ( the sample support ) . By doing so , we reduce precision to a binomial random variable whose point estimate is(cid:100)prec(A → B ) = cnt(AB ) cnt(A ) and its confidence interval can be computed as
1 s(A )
ˆs(AB ) ± zα
ˆs(AB ) ( 1 − ˆs(AB ) ) N−1
.
( 7 )
( Note that s(A ) is instantiated , so it does not need the hat . ) We can use the above confidence intervals to test hypotheses about the supports of itemsets and about the precisions of PARs ( given the population support of their antecedents ) . In the following sections , we derive three filtering criteria , two of which make use of hypothesis testing . 3.3 Filtering
In this section , we present three filtering methods that discard association rules that would not be meaningfully utilized by the logistic regression modeling . The Closed Itemsets criterion discards PARs that are synonyms of others ; the Predictive Significance criterion discards PARs that only appear predictive in the sample but are not predictive in the population and the Component independence criterion discards PARs that are expected to receive 0 regression weight . 331 Closed Itemsets Consider an itemset A and all its possible extensions X .
Definition 1
( Closed Itemset ) . Itemset A is closed if for all X , s(AX ) < s(A ) .
Intuitively , we only need closed itemsets for classification . Consider the predictive association rule A → C , where C is a class . If the itemset AC is not closed , a super itemset AXC of AC exists with the same population support as AC . Because every transaction that supports AC must also support AXC , the association rules A → C and AX → C In the evaluate to true for the exact same transactions . context of logistic regression , AX → C and A → C are perfectly correlated , so they are essentially synonyms .
Population vs . sample closed itemsets . For illustrations below , we will use itemset A , its super itemset AX and itemset A ¯X , which contains all items from A but no item from X . While A ¯X and A look identical , their supports are not the same . In fact , s(A ) = s(AX ) + s(A ¯X ) .
Lemma 1 . If an itemset is closed in the sample , it is also closed in the population .
If itemset A is closed in the sample , then for all X , ˆs(A ¯X ) > 0 and hence s(A ¯X ) > 0 . The converse is not true : an itemset that is closed in the population may appear not closed in the sample .
Convention 1 . If an itemset is closed in the sample , we will consider it closed in the population .
This convention errs on the exclusive side : if the significance is set too high , we may discard some itemsets that are not closed in the sample but are closed in the population . 332 Predictive Significance It is very intuitive ( and has been proven in the context of boosting ) that a PAR is only meaningful in a model if it has non random predictive capability . If a PAR with target class C classifies observations at random , we expect its precision to be Pr(C ) . To ensure that the predictive capability of a PAR is non random , we need to show that the confidence interval of its population precision does not contain Pr(C ) . ( Predictive ) . An association rule X → C is predictive of a target C , if the population precision prec(X → C ) > Pr(C ) .
Definition 2
The precision of rule X → C is Pr(C|X ) . For a random rule , C and X are independent , and hence Pr(C|X ) = Pr(C ) , which is the prevalence of the class in the population .
We wish to test whether prec(X → C ) = Pr(C|X ) = s(XC ) s(X )
> Pr(C ) .
By instantiating s(X ) ( fixing its value to the sample support ) , substituting Pr(C ) = s(C ) and preserving s(XC ) as a random variable , we obtain the hypothesis testing problem of
H0 : HA : s(XC ) = s(C)s(X ) s(XC ) > s(C)s(X ) .
One way to perform this test is through the χ2 test of independence , which was used by Brin et al[6 ] The drawback of the χ2 test is that it considers C and X associated if ¬C 1 and ¬X are strongly associated even if C and X themselves are not associated . This statistical notion of association is inconsistent with the asymmetric notion of association used in association rule mining .
Alternatively , we can use the confidence interval of precision ( Equation 7 ) , which strictly examines the association 1For itemset A , ¬A denotes that A evaluates to false .
826 between C and X ( without explicitly examining ¬C and ¬X ) . The rejection region is cnt(X ) is s(X)−1 . Here we assumed that the popula
ˆs(XC ) ( 1 − ˆs(XC ) ) N−1
ˆs(XC ) + zα where tion support of X is cnt(X)/N . cnt(X ) s(C ) <
N
N
,
Illustrative Example . Let us illustrate Predictive Significance through an example . On an 80 % training set of the prostate cancer data set that we will describe in detail in the Evaluation section , we selected 3 ” signal genes ” ( genes with 65 % precision and maximal support ) as well as 27 ” noise genes ” ( genes with 50 % precision and minimal support to ensure that these genes indeed represent noise ) .
At the minimum support threshold of 4 % ( support count of 3 ) , we discovered 4,167 rules . The predictive significance criterion ( at significance level .1 ) discarded all but 4 rules . These 4 rules had precisions between 68 % and 72 % on the training set and did not contain any ” noise genes ” .
On the other hand , 3,454 of the 4,167 rules had precision 1 on the training set . A CBA model was constructed using all rules–regardless of significance . The model consisted of 8 rules , naturally all having precision 1 . Only two of these 8 rules contained any ” signal genes ” and all 8 rules contained ” noise genes ” .
Of the 8 rules , only 1 rule classified any instances as positive on the test set , mostly incorrectly ( with precision of 33% ) . The overall test accuracy of the CBA model was 43 % . The significant rules , however , had precisions between 62.5 % and 66.7 % on the test set and upon building a CBA model using only these rules , we attained a test accuracy of 62 % .
Figure 1 : The precision and supports of 30 genes on the prostate cancer . The genes selected by CBA ( marked by ‘+’ ) fail to classify test instances reasonably , while the Significant genes ( marked by ‘×’ ) did .
Figure 1 depicts the 4,167 rules in terms of their supports and precisions on the training set . The rules marked ‘+’ ( red ) are the rules selected by CBA . The significant rules are marked by green ‘×’ . The figure underscores that precision– even when sorted on support–only tells half of the story ; Predictive Significance completes the story .
333 Component Independence Let us study a two class problem with classes Cp and Cn . Let a predictive association rule be AB → C , where C is one of Cp or Cn . The antecedent of the association rule is composed of two component itemsets A and B . We consider the independence of these component itemsets conditioned on Cp and Cn .
Definition 3
( Component Independence ) . An association rule X → C is component independent in A and B , if itemsets A and B exist such that
X = A ∪ B , s(ABCp ) = s(ACp)s(BCp ) s(ABCn ) = s(ACn)s(BCn ) . and
Component independence checking is the following hy pothesis testing problem .
H0 :
HA : s(ABCp ) = s(ACp)s(BCp ) s(ABCn ) = s(ACn)s(BCn ) s(ABCp ) > s(ACp)s(BCp ) s(ABCn ) > s(ACn)s(BCn ) and or
The hypothesis testing can be carried out using the binomial confidence interval as it was used to test the Predictive Significance . Random variables s(ACp ) , s(ACn ) , s(BCp ) and s(BCn ) can be instantiated ( by fixing their values to the sample estimates ) and the confidence interval of the population support for s(ABCp ) and s(ABCn ) can be computed using Equation 5 . The rejection region for the first part of the hypothesis is
ˆs(ACp)ˆs(BCp ) > cnt(ABCp )
ˆs(ABCp ) ( 1 − ˆs(ABCp ) ) N−1 ,
1 N +zα where ˆs(I ) is the sample estimate of the support of itemset I , computed as cnt(I ) N with N being the sample size . The rejection region of the second part ( component independence with respect to Cn ) can be derived analogously .
Theorem 1 . Logistic regression would assign 0 weight to predictive association rules that are component independent .
Assume , we have built a logistic regression model
Pr(Cp|x )
Pr(Cn|x ) log
= aIA⊂x + bIB⊂x + cIAB⊂x , where x is a transaction ; a , b and c are the regression weights ; and Ip is the index function that returns 1 if its predicate p evaluates to true and returns 0 otherwise . To simplify the expressions , we omit the constant term . If x supports A but not B , that is A ⊂ x but B ⊂ x and AB ⊂ x , then log
= a .
( 8 )
The quantity on the right is the log odds of Cp given that x supports only A , which we denote as log OCp|A .
Similarly , if x supports B but not A , then log,OCp|B
= b ,
( 9 )
Pr(Cp|x )
Pr(Cn|x )
827 and if x supports both A and B , we obtain log OCp|AB = a + b + c .
Inserting Equations 8 and 9 into Equation 10 leads to log OCp|AB = log OCp|A+log OCp|B+c = log,OCp|A OCp|B ec .
( 10 )
( 11 ) The regression weight c is 0 if the odds of Cp given A and given B are independent
OCp|AB = OCp|AOCp|B , or after expanding the definition of odds
Pr(Cp|AB ) Pr(Cn|AB )
=
Pr(Cp|A ) Pr(Cn|A )
Pr(Cp|B ) Pr(Cn|B ) leading to s(ABCp ) s(ABCn )
= s(ACp ) s(ACn ) s(BCp ) s(BCn )
.
( 12 )
( 13 )
Accepting the null hypothesis is a sufficient condition for
Equation 13 to hold . QED
In practice , we apply Conditional Independence as the last filter . Its computation cost is O(n2 ) , with n being the number of PARs . Consider itemsets I and its subitemsets A and ( I − A ) , all three of which have passed all previous filters . For all such itemsets , we check whether A and ( I−A ) are independent .
4 . RESULTS
The filtering methods were evaluated on three publicly available data sets . The first one is a genotype data set representing a difficult classification problem ( state of the art classifiers can achieve only approximately 65 % accuracy ) ; the second one is a gene expression data with 6,000 genes , where modern classifiers ( like SVM ) can achieve approximately 85 % accuracy ; and the third data set is very wide gene expression data set–it has 200 times as many predictors as observations,–but very clean : predictive accuracy is in the high 90 % range . Table 1 contains a summary description of these data sets .
Name
# of Samples
Alzheimer Prostate Lung
Cases Controls Original 600,000 1237 6,033 52 58 22,283
1254 50 49
# of Predictors Items 100 12,066 44,566
Available
[ 2 ] [ 9 ] [ 18 ]
Table 1 : Description of the data sets
Evaluation Method . We consider two classifiers : logistic regression ( Lasso penalized ) and CBA . Each classifier was evaluated using various filtering criteria . The filtering criteria are denoted with a string of three letters : C for Closed Itemsets , P for Predictive Significance and I for Component Independece . A dash signifies the lack of a particular filtering . For example , ’C I’ denotes filtering based on Closed Itemsets and Component Independence ( and no filtering based on Predictive Significance ) . Since the data sets have balanced class distribution , we use predictive accuracy as the evaluation metric . Predictive accuracy is the fraction of test instances that were classified correctly .
The classifiers were evaluated using 10 fold cross validation ( CV ) . In case of 10 fold CV , the data set is divided evenly into 10 partitions . The evaluation is performed in
10 iterations . In iteration i , partition i is left out as test set and the remaining 9 partitions are used for constructing the model . After completing the 10 iterations , we obtain 10 accuracy estimates . We report the mean and standard deviation of these 10 accuracies and on occasion , we report the individual measurements .
Lasso has the regularization parameter λ we need to tune . Towards this end , in each iteration , we further divide the 9 partitions 80/20 into a training and validation set . The training set was used for building the classifiers with various settings of the parameter , the validation set was used for selecting the optimal model ( optimal parameter setting ) and the test set was used for evaluating the model . 2 4.1 Alzheimer ’s
Figure 2 : Accuracies ( across the 10 folds ) for the Logistic model using various filtering criteria
The Alzheimer ’s data set is the National Institute of Drug Abuse ( NIDA ) Joint Addiction , Aging and Mental Health ( JAAMH ) Alzheimer ’s Disease ( AD ) genotype data set available from dbGap [ 2 ] . The data set is a genotype data set with 600,000 Single Nucleotide Polymorphisms ( SNP ) as predictors . The data set was preprocessed using the default settings of the genotype data analysis tool plink [ 23 ] . SNPs can take three different values based on the number of minor alleles the genotype may contain : 0 , 1 or 2 . SNPs were discretized as ‘having at least 1 minor allele’ . Our goal was to discover high order interactions among SNPs , so we selected the 100 most predictive SNPs . Using only 100 SNPs allowed us to lower the minimum support threshold to 180/2491 . At this threshold , we discovered approximately 270,000 PARs , some of the PARs having antecedent with over 10 SNPs .
Table 2 depicts the number of PARs remaining after filtering based on various combinations of the proposed criteria . With the exception of the Closed Itemset criterion , the proposed criteria resulted in 1 to 3 orders of magnitude reduction .
2In the case , when no filters are applied , we were unable to run the R implementation of Lasso due to insufficient memory . We are using a home grown boosted logistic model ( with efficient memory management ) instead . Boosted logistic models and Lasso are known to produce the same results [ 11 ] but runtimes in our case are not comparable .
828 Figure 3 : Accuracies ( across the 10 folds ) for the CBA algorithm
Figure 4 : Model sizes ( across the 10 folds ) for the logistic model using various filtering criteria
Filter mean — 271,822.40 P11,035.50 271.60 –I PI 4.50 202,592.50 C– 9,270.00 CPC I 217.60 4.50 CPI sd 7,033.02 1,070.18 1.17 .97 5,142.45 850.20 1.71 .97
Table 2 : Average number of Predictive Association Rules on the Alzheimer ’s data set for various filtering criteria . ( α = .1 ; Bonferroni corrected )
411 Effect of filtering on the accuracy In this section , we test the main hypothesis of the paper : while the proposed filtering methods aggressively reduce the number of PARs to be considered , they do not effect the classification performance adversely .
We first consider the logistic model .
In Figure 2 , we present the accuracies obtained under the various filtering criteria . To gain insight into the variability of the accuracy across the 10 folds , the 10 measurements for each filtering criterion are presented as a boxplot . The accuracies across the filtering criteria are only insignificantly different .
412 Model size One advantage of the proposed filtering is that the number of PARs to consider is reduced which leads to improved interpretability . In many cases , users interpret the actual models rather than the PARs that were discovered . In this section , we consider the model sizes under the various filtering criteria .
We define the model size in the context of logistic regression as the number of PARs with non zero regression weights . Figure 4 depicts the model sizes under various filtering criteria .
With the exception of the most restrictive filtering criteria , the median model sizes do not change significantly ; the 75th percentiles , however , mirror the patterns seen in the total number of PARs after filtering .
The models built under the two most restrictive criteria contain 2 SNPs , one on the TOMM40 gene and one on the
APOE4 gene . These SNPs are known to be significantly involved in Alzheimer ’s disease , and no other SNP in our data set showed significant involvement in the disease ( using the plink tool [ 23 ] ; detailed results not shown ) . Given that only these two SNPs ( and other SNPs in linkage disequilibrium with these SNPs ) are desired to be present in PARs , the PI and CPI filtering criteria appear clearly beneficial . 413 Comparison Using CBA For the sake of completeness , we also compare the effect of filtering on CBA . While we do not expect that filtering based on Closed Itemsets and Predictive Significance would adversely effect CBA ’s performance , filtering based on Component Independence may have an effect on it . Essentially , Component Independence removes spurious interaction terms ; however these interaction terms may have real predictive capability and can have higher precision than the component itemsets ( or more precisely , the corresponding PARs ) .
Figure 3 , depicts the accuracies for the CBA algorithm . The figure is analogous to Figure 2 and the conclusion is identical : the accuracies across the filtering criteria are only insignificantly different despite the aggressive filtering that some of the criteria represent . 414 Type I / Type II error tradeoff The Closed Itemsets criterion removes “ synonyms ” , Predictive Significance removes PARs that are not predictive and Component Independence removes PARs that would receive 0 weight . We claim that discarding these PARs does not effect the performance of the classifier , yet differences in performance exist .
First , our experimental evaluation has a random variability , since the data set is randomly partitioned and also the training partition of each fold is randomly partitioned further . Therefore , the models are not built on the exact same training set and random ( statistically insignificant ) differences between the performances of these models are expected .
Second , during hypothesis testing that determines which PARs are discarded , there are type I and type II errors . Type I error occurs when the null hypothesis is incorrectly rejected , that is a spurious PAR is included . Including un
829 necessary PARs rarely leads to decrease in performance , it only make interpretation more difficult and model construction more costly . Type II error occurs when the null hypothesis is erroneously accepted , that is when a potentially important PAR is discarded . Discarding important PARs can decrease the classification performance .
Through the confidence level α , we can only control the type I error . We would prefer to directly control the typeII error , but in the absence of a more concrete alternative hypothesis , it is not possible .
It is known , however , that a tradeoff exists between the type I and the type II error rates . If we allow the type I error to increase , we know that the type II error decreases . This is the reason , why we use relatively high type I error rates ( eg in our experiments , we used α = .1 instead of the more common choice of .05 or 01 ) In all practical hypothesis testing situations type II errors exist and they have the potential to introduce differences in the classification performance . 4.2 Gene Expression Data Sets 421 Prostate Cancer The prostate cancer data set is available in a pre processed form from http://statethzch/~ dettling/ . The data contains the gene expression values for 6,033 genes . Each gene was transformed into two new items signifying whether the gene was over or under expressed . The threshold for over and under expression was set at 0.5 standard deviations from the mean . All resulting 12,066 items were used . Henceforth , for brevity , we only focus on the main hypothesis , namely , we report the filtering capability of the various criteria and their impact on the classification accuracy . We also limit our discussion to logistic regression .
Table 3 contains the number of PARs remaining after applying the various combinations of the filtering criteria . A PAR was discarded if the null hypothesis of the filtering criteria was rejected at 0.1 significance level . The variance in the number of PARs across the 10 folds is large . With only 100 instances , the differences between the sample and population distributions can be substantial . This large variance did not translate into equally high variance in the classification accuracy .
Figure 5 depicts the predictive accuracy of the logistic model under the various filtering criteria . To illustrate the variability of accuracy across the 10 folds , the 10 measurements for each filtering criterion are summarized into a boxplot . The classification performance across the various filtering criteria are only insignificantly different .
Filter mean — 20,056.10 6,650.10 P401.80 –I 29.80 PI C– 1,852.50 548.20 CP328.40 C I CPI 28.00 sd 21,828.25 7,130.56 95.66 11.89 1,197.17 315.34 65.49 11.36
Table 3 : Prostate Data Set . Number of PARs remaining after applying various filtering criteria . Mean and standard deviation across the 10 folds .
Figure 5 : Average accuracy ( across the 10 folds ) for the logistic model on Prostate using various filtering criteria
422 EAGLE Lung Cancer The EAGLE Lung Cancer data set features the expressions of 20,283 genes measures across 107 patients . Analogously to the Prostate data set , each gene were converted into two items , indicating whether the gene is over or underexpressed , resulting in 44,566 items . All items were used .
Table 4 presents the number of PARs remaining after filtering based on the various criteria . Given that the data set contains only 107 patients , the variability of the number of PARs is high across the 10 folds . Once again , the selectivity of the criteria are substantial3 .
Figure 6 depicts the accuracy of the logistic models under the various filtering criteria . The 10 accuracy measurements across the 10 folds are summarized into a boxplot for each filtering criterion . The conclusion is consistent with the other data sets : there is only insignificant differences in performance .
Filter mean — 13,451.00 13,451.00 P–I 127.30 127.30 PI 922.60 C– 922.60 CP65.50 C I CPI 65.50 sd 8,824.40 8,824.40 24.96 24.96 593.00 593.00 21.17 21.17
Table 4 : Lung Cancer Data Set . Number of PARs remaining after applying various filtering criteria . Mean and standard deviation across the 10 folds .
5 . SUMMARY AND CONCLUSION
In this paper , we introduced a simple statistical model based on sampling theory , which allowed us to make inferences about the population versions of performance metrics of predictive association rules ( PARs ) . The population estimates of these metrics , in turn , lead to the ability to test hypotheses on the test set . Based on the hypothesis testing framework , we developed or re derived three filtering criteria
3Due to the pronounced differences between the classes , our setting of a high minimum support threshold resulted in the discovery of only predictive significant PARs .
830 [ 13 ] Jerome H . Friedman . Greedy function approximation : A gradient boosting machine . Annals of Statistics , 29:1180 , 2001 .
[ 14 ] Rohit Gupta , Smita Agrawal , Navneet Rao , Ze Tian , Rui
Kuang , and Vipin Kumar . Integrative biomarker discovery for breast cancer metastasis from gene expression and protein interaction data using error tolerant pattern mining . In International Conference on Bioinformatics and Computational Biology ( BICoB ) , 2010 .
[ 15 ] Trevor Hastie , Robert Tibshirani , and Jerome Friedman .
The Elements of Statistical Learning . Springer , 2nd edition , 2009 .
[ 16 ] D . Hinkley . On the ratio of two correlated normal random variables . Biometrika , 56(3 ) , 1969 .
[ 17 ] Richard A . Johnson and Dean W . Wichern . Applied
Multivariate Statistical Analysis . Prentice Hall , 2002 .
[ 18 ] Maria Teresa Landi , Tatiana Dracheva , Melissa Rotunno , et al . Gene expression signature of cigaratte smoking and its role in lung adenocarcinoma development and survival . PLoS ONE , 3(2 ) , 2008 .
[ 19 ] Jinyan Li , Guozhu Dong , and Kotagiri Ramamohanarao .
Instance based classification by emerging patterns . In Practice of Knowledge Discovery in Databases PKDD , 2000 .
[ 20 ] Wenmin Li , Jiawei Han , and Jian Pei . CMAR : Accurate and efficient classification based on multiple class association rules . In IEEE International Conference on Data Mining , 2001 .
[ 21 ] Bing Liu , Wynne Hsu , and Yiming Ma . Integrating classification and association rule mining . In ACM International Conference on Knowledge Discovery and Data Mining ( KDD ) , 1998 .
[ 22 ] Aysel Ozgur , Pang Ning Tan , and Vipin Kumar . RBA : An integrated framework for regression based on association rules . In SIAM International Conference on Data Mining ( SDM ) , 2004 .
[ 23 ] S . Purcell , B . Neale , K . Todd Brown , L . Thomas , MAR
Ferreira , D . Bender , J . Maller , P . Sklar , PC de Bakker , M . Daly , and P . Sham . PLINK : a toolset for whole genome association and population based linkage analysis . American Journal of Human Genetics , 81 , 2007 .
[ 24 ] Ingo Ruczinski , Charles Kooperberg , and Michael LeBlanc . Logic regression . Journal of Computational and Graphical Statistics , 12(3 ) , 2003 .
[ 25 ] Pang Ning Tan , Michael Steinbach , and Vipin Kumar .
Introduction to Data Mining . Addison Wesley , 2005 .
[ 26 ] Robert Tibshirani . Regression shrinkage and selection via the lasso . Journal of the Royal Statistical Society B , 58(1 ) , 1996 .
[ 27 ] Ke Wang , Senqiang Zhou , and Yu He . Growing decision trees on support less association rules . In ACM International Conference on Knowledge Discovery and Data Mining ( KDD ) , 2000 .
[ 28 ] Adam Wright , Elizabeth S Chen , and Francine L Maloney .
An automated technique for identifying associations between medications , laboratory results and problems . J Biomed Inform , 43(6):891–901 , Dec 2010 .
[ 29 ] Xiaoxin Yin and Jiawei Han . CPAR : Classification based on predictive association rules . In SIAM International Conference on Data Mining ( SDM ) , 2003 .
[ 30 ] Yongwook Yoon and Gary G . Lee . Text categorization based on boosting association rules . In IEEE International Conference on Semantic Computing , 2008 .
[ 31 ] Mohammed Zaki and Ching Jui Hsiao . Efficient algorithms for mining closed itemsets and their lattice structure . IEEE Transactions on Knowledge and Data Engineering , 17(4 ) , 2005 .
Figure 6 : Average accuracy ( across the 10 folds ) for the logistic model on Lung using various filtering criteria that vastly reduced the number of PARs to be considered by the subsequent classifier without significantly effecting its performance . We gave a mathematical proof for the case when the classification model was logistic regression . We also demonstrated on three publicly available data sets that the vast reduction in the number of PARs indeed did not come at the cost of reducing the classification performance . We believe that our statistical model generalizes to other association rule mining problems in a straightforward manner . We foresee that the same methodology can be applied to many flavors of association rule mining , including sequence mining , error tolerant association rules , spatiotemporal associations , just to name a few .
6 . REFERENCES [ 1 ] Fieller ’s theorem . http://enwikipediaorg/wiki/Fieller%27s_theorem
[ 2 ] National institute on drug abuse ( NIDA ) joint aging , addiction and mental health ( JAAMH ) late onset alzheimer ’s disease data set . Avaiable from DbGap http://wwwncbinlmnihgov/gap
[ 3 ] Rakesh Agrawal and Ramakrishnan Srikant . Fast algorithms for mining association rules . In VLDB Conference , 1994 .
[ 4 ] Leo Breiman . Random forests . Machine Learning , 45(1 ) ,
2001 .
[ 5 ] Leo Breiman , Jerome Friedman , Charles J . Stone , and RA
Olshen . Classification and Regression Trees . CRC Press Reprint , 1998 .
[ 6 ] Sergey Brin , Rajeev Motvani , and Craig Silverstein .
Beyond market baskets : Generalizing association rules to correlations . In ACM International Conference on Managment of Data ( SIGMOD ) , 1997 .
[ 7 ] William Cohen and Yoram Singer . A simple , fast and efficient rule learner . In American Association for Artificial Intelligence ( AAAI ) , 1999 .
[ 8 ] Morris H . DeGroon and Mark J . Schervish . Probability and
Statistics . Addison Wesley , 2010 .
[ 9 ] Marcel Dettling and Peter B¨uhlmann . Supervised clustering of genes . Genome Biology , 3(12 ) , 2002 .
[ 10 ] Guozhu Dong , Xiuzhen Zhang , Limsoon Wong , and Jinyan Li . CAEP : Classification by aggregating emerging patterns . Discovery Science , 99 , 1999 .
[ 11 ] Jerome Friedman , Trevor Hastie , Sharon Rosset , Robert
Tibshirani , and Ji Zhu . Discussion of boosting papers . 2003 .
[ 12 ] Jerome Friedman , Trevor Hastie , and Robert Tibshirani .
Regularization paths for generalized linear models via coordinate descent . Journal of Statistical Software , 33(1 ) , 2010 .
831
