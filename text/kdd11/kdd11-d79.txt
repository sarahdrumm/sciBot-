Latent Graphical Models for Quantifying and Predicting
Patent Quality
Yan Liu1 , Pei yun Hseuh2 , Rick Lawrence2 , Steve Meliksetian2 , Claudia Perlich2 , Alejandro Veen2
1 Computer Science Department , University of Southern California , Los Angeles , CA 90089
2 IBM T . J . Watson Research , Yorktown Heights , NY 10598
ABSTRACT The number of patents filed each year has increased dramatically in recent years , raising concerns that patents of questionable validity are restricting the issuance of truly innovative patents . For this reason , there is a strong demand to develop an objective model to quantify patent quality and characterize the attributes that lead to higher quality patents . In this paper , we develop a latent graphical model to infer patent quality from related measurements . In addition , we extract advanced lexical features via natural language processing techniques to capture the quality measures such as clarity of claims , originality , and importance of cited prior art . We demonstrate the effectiveness of our approach by validating its predictions with previous court decisions of litigated patents .
Categories and Subject Descriptors H28 [ Database Applications ] : Data Mining
General Terms Algorithms , Design , Experimentation
Keywords Graphical models , Text Mining , Measurement Models
1 .
INTRODUCTION
Technological advances and broadening innovation have led to a dramatic increase in the number of patents filed each year . The resulting patent backlog is staggering : at the end of 2008 , there were 1.2M patent applications pending at the US patent office , an increase by a factor of 4.3 over the number outstanding at the end of 1997 [ 3 ] . This backlog creates huge challenges to current patenting systems because more and more patent applications of dubious value are entering the pipeline , diverting the attention of critical resources ( eg patent examiners ) needed to ensure that high quality patents are issued in a timely manner .
From a legal perspective , patent quality is a measure of whether a patent is written in a manner such that it could withstand rigorous examination were it to be challenged in a court of law . In other words , a high quality patent should be clearly written and its prior art ( in patents as well as other publications ) is well researched and referenced . Note that the patent quality is different from the economic value of the patent in that the latter is largely determined by the demand in the market for the underlying technology covered in the claims of the application . Patent experts agree that higher quality patents are more likely to cite relevant ( and higher quality ) prior art and include claims that are clearly differentiated with respect to the prior art . The objective of this work is to build a machine learning model to understand and characterize patent quality based on the input from legal experts . If successful , such a model could eventually provide actionable insight to inventors , examiners , and attorneys on the quality of a patent at any point in the patent lifecycle . An immediate challenge in building a supervised model for patent quality is obtaining a sufficient number of labeled training examples . A landmark study [ 10 ] in the intellectual property ( IP ) law community addresses the issue by associating patent quality with validity as ruled by a court of law . Specifically , a set of labeled examples has been collected [ 10 ] by examining all Federal Circuit cases between 2003 and 2009 in which the court ruled on the validity of an issued patent . This labor intensive process yielded 250 cases adjudicating the validity of 351 patents , 104 of which were ruled valid ( some court cases deal with more than one patent at once ) . While court decisions are an accepted indicator of patent quality , the number of patents that reach the Federal Circuit is limited to a number far smaller than what is needed to train a robust high dimensional model for prediction . Hiring legal experts to examine and label a large number of patents is neither sufficiently scalable nor likely to result in a consistent dataset .
In this paper , we introduce a latent graphical framework in which patent quality is a latent ( hidden ) variable and its value can be estimated from other correlated measurements which can be easily obtained for a large number of patents . This idea of measurement modeling was first introduced in [ 9 ] . The measurement model is a Bayesian framework in exponential families to estimate the unobserved target from one or more functions ( called measurements ) of the target . The motivation behind the model is simple : available measurements , such as partial labels , general constraints , and structured label constraints on the model , provide a glimpse of the hidden target variable , thus providing partial infor
1145 mation about the underlying model . Although the measurement model in [ 9 ] provides a general and powerful framework , there remain at least two challenges in applying this methodology to the problem of estimating patent quality :
1 . What are the most effective measurements for estimating patent quality and how are they correlated with quality ?
2 . What are relevant lexical features of a patent that can help predict its quality ?
To answer the first question , we rely on domain experts and existing literature in the IP law community . In patent law , the number of forward references ( ie the number of times a patent is cited by other patents after it is issued ) has been identified as an indicator of the economic value of a patent [ 5 ] and could carry information on the quality of the patent . In addition , court ruling is considered as the most authoritative measurement of quality because patents upheld by the court are likely to be of higher quality than those rejected . Addressing the second question , we explore advanced natural language processing techniques to extract quality related lexical features . These include straightforward structural characteristics of the patent ( eg the number of independent claims ) as well as text based features to capture key attributes , such as novelty of the claims , alignment of the claims with the body of application , and other metrics suggested by patent experts .
In recent years , many studies have been conducted to model citation and influence in scientific publications via statistical models in the machine learning community [ ? , ? , ? ] . However , our work is significantly different from existing work from several perspectives : ( 1 ) our task focuses on quality analysis , which is related to individual parameters , such as citation or influence , but is more general and difficult to quantify ; ( 2 ) in our task , the value of our target variable is unobserved . As a result , specific latent models have to be developed and the evaluation is more challenging ; ( 3 ) our proposed approach not only utilizes machine learning models to estimate patent quality , but also extracts domainspecific features via natural language processing techniques . The rest of the paper is organized as follows . Section 2 describes the patent data used in our analysis . Section 3 discusses our modeling approaches , and Section 4 provides an overview of the lexical features used in the models . Section 5 summarizes the experiment results , illustrating both the performance of our broader methodology and its application to quantifying patent quality . Section 6 compares our approach with existing work in the area of modeling patent quality .
2 . BACKGROUND
An internal patent database is hosted at IBM to store and organize the USPTO ’s ( United States Patent and Trademark Office ) weekly data feed with bibliographic information for patent grants and published patent applications [ 6 ] . Each week , the data feed provides approximately 3000 newly granted US utility patents and an even larger number of published US patent applications .
For the work described in this paper , we only deal with granted patents . This results in a collection of approximately 4 millions patents from 1975 to 2010 . The “ raw ” data is an electronic ( XML ) version of the patent certificate , including patent number , title , filing and issue date , inventor and assignee information , the classes the patent is categorized under both the US and the IPC ( International Patent Classification ) systems , lists of the referenced US and foreign patents , a list of referenced non patent publications ( often academic articles ) , and the names of the primary and assistant examiners . It also includes unstructured texts such as the patent abstract , description , claims , and even some ( limited ) information on the drawings . This raw data feed is processed and organized into several tables , which make it easily accessible and analyzable . For instance , it provides fields that list the number of claims as well as the number of domestic , foreign , and non patent references . In addition , the patent database is supplemented by data from additional sources , for instance , assignee information ( assignee code and address ) . We also generate the citation network , ie , a graph linking each patent with the patents it cites ( namely , backward references in IP law terms ) as well as all subsequent patents that cite this patent ( namely , forward references ) .
3 . LATENT GRAPHICAL MODELS
The main challenge we face in the task of quantifying patent quality is the lack of ground truth . Since some indirect measure of quality is abundant and relatively easier to obtain , we might be able to develop machine learning models to estimate the value of target variables . In this section , we first review the measurement model and then describe our proposed latent graphical models to quantify patent quality . 3.1 Review of Measurement Models
In [ 9 ] , a Bayesian approach is developed for measurement modeling . Here we describe the model with a frequentist point of view . More specifically , suppose we are given observations x1 , . . . , xn and our goal is to estimate the response variables Y1 , . . . , Yn . Unlike classical supervised learning , we do not observe the labels of Y directly , but are provided a list of measurements τσ that reveals some information about Y , ie W = τσ(Y , x ) . We can define the joint probability of response variable Y and measurements W as follows :
P ( Y , W|σ , θ , x ) = P ( Y |x , θ)P ( W|Y , x , σ ) .
( 1 )
For computational convenience , we can define the probability distributions P ( Y |x , θ ) and P ( W|Y , x , σ ) to be in exponential families . For example , we can have
P ( Y |θ , x ) = exp(−h(ϕθ(x , Y ) ) + const ) , P ( W|Y , x , σ ) = exp(−g(W − τσ(x , Y ) ) + const ) , where h and g are log concave functions , and ϕθ(x , Y ) is the feature defined over x and Y . By estimating the values of θ , we can estimate the posterior probability of Y given the observations X . One major advantage of the measurement model is its capability to incorporate any type of measurement to help predict Y .
Given the model defined in eq(1 ) , a natural question is whether we are able to achieve globally optimal solutions . The question was not answered in [ 9 ] , but with some simple derivations , we can show that if the conditional distributions P ( Y |x , θ ) and P ( W|Y , x , σ ) correspond to generalized linear models with matching link functions , the incomplete loglikelihood L(θ , σ ) is a concave function of parameter θ and
1146 σ in a convex domain , thus resulting in globally optimal solutions . 3.2 Measurement Model for Quantifying Patent
Quality
The measurement model provides a flexible framework of estimating the response variable by integrating different measurements . However , in real applications , it is more important to identify relevant measurements , as well as the generating functions from the target variable to the identified measurements ( such as h and g ) , so that accurate estimates of the target variable can be obtained .
In our specific application to patent quality , several mea surements have been identified by domain experts :
1 . The number of forward citations : it is believed to be determined at least in part by quality ;
2 . Court decisions : patents ruled as “ valid ” by courts are expected to be of higher quality than those ruled as “ invalid ” ;
3 . Reexamination records : reexamination is a process in which a third party or an inventor can request patent examiners to verify that the subject matter in the claim is patentable . A patent that passes the reexamination tends to have a higher likelihood of being high quality .
In this paper , we demonstrate our solution using the first two measurements , namely forward citations and court decisions . We choose these two measurements not only because they are most related to patent quality , but also because they represent two common types of measurements in many applications . For example , in evaluating the quality of scientific publications , the number of citations and publishing venues are usually considered as important measures . 321 Prerequisites We begin by introducing the notation . Given patent observations , x1 , . . . , xn , we are interested in estimating their quality Q1 , . . . , Qn , which are unobserved . We are provided with two measurements of Q , ie forward citation count C and court decision D . Notice that the court decision is available only for a limited number of patents . We set the value of D to 0 if its value is not available .
Given observations x , we need to extract meaningful features relevant to Q , which we later refer to as Fq . This task itself is very challenging in natural language processing and text mining . Our solution to this problem is discussed in Section 4 . Furthermore , we need to define the linking functions between forward citation C and quality Q . We resort to the Poisson distribution since the citation count of a patent or a paper within one time period ( eg 10 years ) is usually modeled via Poisson [ 14 ] . In addition to quality , the number of citations can be also determined by many other factors , such as filing year , filing classes , venue/journal , inventors , and so on . These features do not necessarily directly contribute to the quality of a patent ( given the content of the patent ) , so we conceptually separate them from quality related features and denote them as Fc . Notice that our model allows any types of overlap between quality related features Fq and citation related features Fc . Here we only separate them for the convenience of illustrating ideas .
322 Latent Graphical Models for Quantifying Patent
Quality
Next we formally define the proposed latent graphical models ( LGM ) .
Measurement on Forward Citation : The forward citations include three components : self citations , examineradded citations , and citations provided by the patent authors which are not self citations . To remove the noise and potential bias added by authors citing their own work and by examiner intervention , we use only the final category of citations . Moreover , we limit the count to the one within 10 years after a patent is filed since it takes at least four to five years for a patent to be well accepted and cited . We model the number of citations C of each patent as a Poisson distribution whose mean is determined by a linear combination of control features Fc and its quality Q . In other words , we have
C ∼ Poisson(λ ) , log λ = βT
C Fc + log Q ,
( 2 ) where βC is the coefficient of feature Fc .
Measurement on Court Ruling : Court decisions can provide the constraint that valid patents have higher quality than invalid patents . To model these constraints , we can maximize the difference of quality value between the valid patents Qi and invalid patents Qj . Suppose di and dj are the labels of court decisions , which are equal to 1 if valid and 1 if invalid . Then we can model the posterior probability of P(d|Q ) as follows : P ( d|Q ) = exp(Qi − Qj ) = exp
( diQi + djQj ) ,
1 Z
( i,j )
1 Z
( i,j ) where i , j are any pair of valid and invalid patents . We can further derive the model as follows : i
P ( d|Q ) =
1 Z exp(w(di)diQi ) , w(di ) is equal to the number of negative examples if di is 1 and equal to the number of positive examples if di is 1 .
Distribution of Target Variable : Given quality related features Fq , we model the quality Q as a Gamma distribution :
Q ∼ Gamma(a , b ) , log b = βqFq .
The choice of the Gamma distribution has two advantages : it is conjugate to the Poisson distribution , which leads to mathematical convenience , and its shape is highly flexible .
Proposed Model : Putting everything together ( see Figure 1 ) , we can define the joint distribution of measurement C and D as well as hidden variable Q as follows : P ( c , d , Q ) = P ( c|Q)P ( d|Q)P ( Q ) = − exp(βC Fc)Q ] × [ ec(βC Fc)Qce exp(w(di)diQi ) ]
[
1 c! ×[ exp(βqFq)a
Γ(a )
Qa−1e
1 Z − exp(βq Fq )Q ] . i
To estimate the value of D , we can resort to the EM algorithm [ ? ] . Since there is no closed form for P ( Q|c , d ) , variational methods have to be applied [ ? ] . That is , we assume
1147 Figure 1 : Latent Graphical Models for Patent Quality Estimation that P ( Qi|ci , di ) has the following simple form : q(Qi|ci , di ) =
P ( Qi|ci , di ) , Qi|ci , di ∼ Gamma(a0 i , b0 i ) , i i and b0 where a0 i are variational parameters to be estimated . By minimizing the KL divergence between q and p , we have KL(q||p ) =
−
= i q log q 0 log βi q p 0 − log Γ(ai ( − log ci + ciβC F i
( ai
0 ) + ( ai c + ciE[log Q q ) − log Γ(a ) + ( a − 1)E[log Q
0E[Qi ] )
0 − 1)E[log Qi ] − bi i ] − exp(βC F i i ] − exp(βqF i c )E[Q i ] q )E[Q i ] i
+a(βqF i − log Z + i
E step : Taking the derivative of eq(3 ) and setting them to zero , we have the following solution to ai
0 and bi 0 : ai 0 = a + ci , bi 0 = exp(βC F i c ) + exp(βqF i q ) − w(di)di .
M step : Taking the derivative of Q with respect to βC , βq and a , we have the following updating equations : w(di)diE[Qi] ) .
( 3 ) st
We can easily derive the EM algorithm and therefore omit the details . This approach can be seen as the simplest version of the measurement model .
Regression Residual Analysis : An alternative to latent models is regression analysis . That is , we assume that the citation count C within a time period is generated from a Poisson distribution whose parameter λ is a linear combination of Fc :
C ∼ P oisson(λ ) , log(λ ) = βT Fc .
It can be seen easily that the control features Fc cannot fully explain the citation count . Borrowing the idea of residual analysis in statistics , we can think of the residuals of the Poisson regression , ie R = C − exp(βT Fc ) , as features strongly correlated with patent quality Q . Constrained Regression : Given additional measurement on court ruling , we can extend the model of residual analysis and improve the estimation of patent quality via constrained regression . In other words , we can use the court decisions as additional constraints in the objective function of Poisson regression so that valid patents have a higher residual than invalid patents . In the end , we have the following constrained optimization problem : maxβ cn − exp(βT F n i ci(βT F i c ) − exp(βT F i c ) c ) <= cp − exp(βT F p c ) + ξk , ξk >= 0 , where p and n are the indexes of any pair of positive and negative examples , and ξk is the slack variable to allow noise in court ruling . The solution to the optimization function above can be obtained by similar technique used in ranking logistic regression [ 7 ] . We omit further discussion .
These simple solutions provide reasonable alternatives to measurement models . However , we can see easily that for each specific measurement , we have to develop a model accordingly . In contrast , measurement models provide a unified framework that can easily incorporate different types of measurements . 4 . EXTRACTING QUALITY RELATED FEA
TURES
Similar to many other machine learning applications , the success of modeling patent quality depends heavily on whether we can extract the most appropriate features . In general , the features can be extracted directly from the patent itself , as well as from the examination history after the patent is filed with the patent office . Since our objective is to characterize quality at any point in the patent process , we focus on features that are available at the time of filing . They only only include simple numerical features , such as the number of inventors and the number of independent claims , but also could be much more sophisticated lexical features that capture specific concepts in patent law .
Our approach to lexical feature extraction is motivated by the following five questions : ( 1 ) How well does the language used in the claims and the abstract align with that used in the body ( specification ) of the patent ? ( 2 ) What is the importance of the invention inferred from the cited prior art ? ( 3 ) How complex is the patent and how broad is the patent coverage ? ( 4 ) How relevant is the invention to the technology covered within the patent field or class ? ( 5 ) How original is the invention when compared with prior art ? i i i
∂Q ∂βC
∂Q ∂βq
∂Q ∂a
=
=
=
( ciF i c − F i c ˆai
0 exp(βC F i c )/ˆbi
0 ) ,
( aF i q − F i q ˆai
0 exp(βqF i q )/ˆbi
0 ) ,
( βqF i q + Ψ(ˆai
0 ) − log ˆbi
0 ) − N Ψ(a ) , where Ψ is the digamma function . 3.3 Discussion
Measurement modeling is a powerful framework which integrates different measurements into one model . However , there are some alternatives to utilizing individual measurement and estimating the value of target variables . In this section , we examine some simpler solutions , including the EM algorithm , regression residual analysis , and constrained regression .
Expectation maximization : If we only consider the measurement of forward citations , a simple approach could be modeling the quality of a patent Q by the following generation process :
C ∼ Poisson(exp(βC Fc)Q ) ; Q ∼ Gamma(a , b ) .
Since the Gamma distribution is conjugate to the Poisson distribution , the posterior probability of Q also follows a Gamma distribution , ie
Q|c , βC , Fc , a , b ∼ Gamma(c + a , exp(βC Fc ) + b ) .
1148 While the first three groups of features characterize the applicants’ effort in application writing and prior art search , the latter two , ie , relevance and originality , can measure the technology impact . Note that we did not make any assumptions on the orthogonality of the five groups . In fact , we expect some groups to be complimentary or contradictory to each other . For example , relevance and originality seem to be contradictory at first sight , but the interplay between them is unclear and our learning models could potentially provide some interesting insight . 4.1 Written Description Alignment
Patent experts point regularly to the degree of alignment of the claims and the patent body as an important quality component [ 10 ] . Therefore , we analyze the texts of a patent to capture the degree of alignment between various parts of the patent . For example , we can calculate content similarity eg , Euclidean distance ( Euc ) , between pairs of word vectors representing different parts of the patents , such as abstract claims ( A C ) , abstract description ( A D ) , and claims description ( C D ) . Because this measure is a distance , it increases if the claims and description are poorly aligned . Accordingly , we would expect this measure to correlate inversely with validity . 4.2 Citation Quality
The number of backward citations to the prior art has been studied as both an indicator of economic value [ 12 ] and patent quality [ 13 ] . An obvious reason for this observation is that better patents ( as measured either by ultimate value or intrinsic quality ) are more likely to reference the relevant prior art [ 13 ] . Since we focus on features that are available at the time of filing , we can extract features that capture the average age of cited patents , ie , the time since the referenced patent was issued . It is likely that some measure of the quality ( or “ authority ” ) of the cited prior art is more meaningful than a simple count of backward references . Hence , we compute the mean PageRank score [ ? ] of all the citations by a patent , which indicates the authority level of the cited prior art . This is done by constructing a citation network from all domestic patents by the time of filing and then applying the standard PageRank algorithm to this time sensitive network . 4.3 Complexity and Reported Coverage
Patent complexity and breadth have been reported as an important indicator of patent value in previous research . For example , Reitzig [ 12 ] has studied the number of independent and dependent claims and the number of words describing the state of the art . In addition to these simple measures , we also include the number of classes the applicants have tagged as an initial step to characterizing patent breadth . We count both the number of reported classes under the US classification and International Patent Classification ( IPC ) scheme . 4.4 Technology Relevance
To measure how relevant an invention could be to other patents in the same field or class , we apply information theoretic measures to calculate the similarity between the target patent and the background ( ie , all patents filed in the same year or from the same class ) . In short , we represent each patent as a word vector , and calculate the average similar ity score ( eg cosine similarity or KL divergence ) , between the query and all patents in the background . Some examples include : 1 . avgCosine pyear : average cosine similarity score between the target patent and all patents filed in the same year in the same class ; 2 . avgConsine all : average cosine similarity score between the target patent and all patents filed till current year in the same class ; 3 . avgPKL : average point wise KL divergence between the target patent and all patents patents filed till current year in the same class . 4.5 Claim Originality
It is known that a patent cannot be filed if the subject matter would have been obvious to a person with ordinary skills in the area at the time of filing . The hypothesis is that higher quality patents describe technologies that are more recently developed ( more likely to be novel ) and are not commonplace ( less likely to be obvious ) . To measure these concepts , we construct n gram vectors to represent the technology space of each patent . Two types of features are then calculated to reflect claim originality and commonality . Claim originality features are calculated based on the age of n grams , ie how long ( in years ) a technology ( represented by an n gram ) has been in the field since its first introduction . Claim commonality is computed based on the n gram usage , ie how many times a technology has been mentioned up to the current filing year .
We address claim originality first . Given the earliest occurring year of words and the number of word occurrences at each time point , it is straightforward to compute the age of n grams ( words ) 1 . If a word has first appeared in this patent , the word will have its age as 0 , tagged as a “ nascent word ” . If a word appears for the first time within the last year , the word will have an age smaller than one year , tagged as a “ recent word ” . Similar ideas have also been explored recently in [ 6 ] . Consistent with the hypothesis above , we expect a higher quality patent will have a higher proportion of nascent and recent words . For example , consider the two patents listed in Figure 2 . While the patent on the left ( US6306383 ) uses many recent words to describe its invention , the one on the right ( US6605646 ) has a very equal distribution over word age . In fact , when brought to court , the Federal Circuit court has ruled US6306383 as valid and US6605646 as invalid . Notice that the range of the originality scores as defined here could vary significantly in different fields . For example , there might be more jargon terms in biology than in others , which translates to very different statistics for new words across domains . This problem could be solved by normalization so that the scores of patents in different fields could be comparable .
For commonality , it is straightforward to compute the usage of words by counting the number of times a word has been mentioned prior to the filing date . If a word has appeared more than a pre determined threshold , the word will be tagged as a heavily used word . As older words are expected to be mentioned more , we also compute word contribution , ie , the accumulated rate of word occurrences ( average word usage per day ) . If a word has accumulated faster than a pre determined threshold , the word will be tagged as a high contribution word . A higher quality patent is expected to use less common language , and thus have lower proportions of heavily used and high contribution words .
1For simplicity , the features here are computed on unigrams ( words ) .
1149 with two measurements : forward citations ( positive discrete value ) from a Poisson distribution and court decisions D ( binary value ) from Bernoulli distributions . To fully understand the performance of our model , we investigate two learning scenarios : ( 1 ) we are given the observations of all relevant features , ie Fc , Fq ; ( 2 ) we are given the observations of partial features only .
Complete Feature Observations : We conduct experiments on three versions of the latent graphical models ( LGM ) , by varying the inputs as follows : ( 1 ) C + Fc : measurement C and its associated features Fc ; ( 2 ) C + Fc + Fq : the input of C + Fc plus the quality associated features Fq ; ( 3 ) C + Fc + Fq + D : the input of C + Fc + Fq plus court ruling D . To examine the learning behavior of different algorithms , we vary the number of input examples from 20 to 100 by a step size of 20 , run each experiments 30 times , and report the averaged correlation coefficients between the predicted value and the ground truth ( see Figure 3(a) ) . We can see that when the number of training examples is limited ( eg 20 ) , the algorithm with the most input and measurements C + Fc + Fq + D performs the best . However , when we increase the number of examples , the three algorithms perform comparably . Note that the algorithm C + Fc + Fq performs even better than Fc + Fq + D when the number of examples is large ( eg 100 ) . This seems to indicate that the use of unreliable measurements may deteriorate the performance , even with a sufficient number of examples .
Partial Feature Observations : In many applications , we can work on smart feature engineering and extract as many useful features as possible . But it is still unlikely that we are able to identify all of the relevant features . Therefore we are interested in answering the following question : how much will partial features affect the performance of our model ? In Figure 3(b ) , we show the performance of different algorithms by fixing the number of examples to 100 , but varying the percentage of observable features ( including both Fc and Fq ) . We can see that when there are only 20 % feature observations , the algorithm C + Fc + Fq + D still performs comparably to those with complete set of features . However , as expected , this advantage diminishes when the percentage of features is increased to around 80 % .
Comparison With Other Baselines : Finally , we compare our approach with other baselines . Figure 3(c ) shows the results on 100 training examples by different methods . The first two bars show simple dummy predictors in which we simply use the value of measurements C or D directly as the prediction . The next two bars show results by residual analysis discussed in Section 33 More specifically , we use logistic regression for measurement D , and Poisson regression for measurement C . The final three bars show the results of our LGM model , which clearly outperforms other baseline methods .
5.2 Application on Analyzing Patent Quality In our application to patent quality , we use forward citations and court decisions as the measurements to build our model . We define forward citations as the number of cites within ten years after the patent is filed – this is available for all patents before 2000 ( since the data for this study were collected in 2010 ) . In contrast , there are a limited number of patents , which were litigated by Federal Circuit . Therefore we create two datasets : one is a set of randomly selected
Figure 2 : The distributions of word age features of a valid patent ( US6306383 : Method for topical treatment of scars with protein kinase C inhibitors ) and an invalid patent ( US6605646 : Vitamin supplement composition ) .
Here is a list of lexical features we have extracted : 1 . NascentWordProp : the proportion of nascent words in the claims ; 2 . RecentWordProp : the proportion of recent words in the claims ; 3 . HeavyUseWordProp : the proportion of heavily used words in the claims ; 4 . HighContrWordProp : the proportion of fast accumulated words in the claims ; 5 . AvgWordAge : the average age of words in the claims ; 6 . AvgWordUsage : the average usage of words in the claims ; 7 . AvgWordContr : the average accumulation rate of words in the claims .
5 . EXPERIMENT RESULTS
To fairly evaluate the performance of different methods , one needs to have the ground truth of target variables . Unfortunately , these are not directly available in our application . Therefore we generate a simulation dataset to mimic the properties of our application data while holding the ground truth for evaluation . The experiments on simulation data are essential since it is the only way to directly verify the effectiveness of our algorithm . In the actual application , we have no choice but to validate the results indirectly , ie by comparing the agreement of predictions with the outcomes of court decisions , which are believed to be strongly correlated with patent quality . The desired output in our application is a ranked list of patents based on their patent quality scores – therefore only the ranking order matters . We use Kendall ’s τ rank correlation coefficient [ 8 ] and AUC ( area under curve ) as the evaluation measures . In this section , we describe our results on both simulation data and application data respectively . 5.1 Simulation Data
Our objective here is to evaluate different methods on a simulation dataset with properties similar as the one in our application . We sample data points from the distribution with underlying model shown in Figure 1 . Specifically , we assume that the input features for forward citation Fc and and features for quality Fq are generated from multivariate Gaussian distributions with 5 features and 10 features respectively . The parameters βC and βq are generated uniformly from [ 0 , 1 ] , and the ground truth of hidden variable Q ( ie our desired labels ) comes from a Gamma distribution . Unlike traditional supervised learning , we are also provided
1150 ( a ) Complete Feature Observation ( b ) Partial Feature Observation ( c ) Comparison of Different Methods
Figure 3 : Experiment results on simulation data : ( a ) Correlation scores with complete observations of features ; ( b ) Correlation scores with partial observation of features ; ( c ) Comparison with alternative solutions : measure D and measure C refers to the dummy predictor which simply uses court decision D or citation C to estimate quality ; LR and PR refer to residual analysis using logistic regression and Poisson regression ; LGM refers to our latent graphical models .
( a ) Alignment Features
( b ) Linear Regression
( c ) Poisson Regression
( d ) LGM
Figure 4 : ( a ) Demonstration of alignment features for differentiating valid and invalid patents . ( b d ) Q Q Plot of Predicted Value by Different Methods ( Input : C + Fc ) of Valid and Invalid Patents . ( b ) Linear regression ; ( c ) Poisson regression ; ( d ) Latent graphical models ( LGM )
12K examples from all patents filed before 2000 ( referred to as dataset I ) , and the other is a subset of the data used in [ 10 ] , which consists of 351 patents ( filed before 2000 ) with decisions by the Federal Circuit Court ( referred to as dataset II ) .
Evaluation of Feature Extraction . Before discussing the modeling results , we first examine the effectiveness of feature extraction . To avoid topic drift , we will not discuss the detailed results on all lexical features ( the content of which might justify another paper ) , but highlight some interesting observations . First , we demonstrate in Figure 4(a ) how the alignment scores can help differentiate valid and invalid patents ruled by the court . Table 1 shows the list of lexical features used as input ( ie feature Fq ) in our model , as well as their p value scores of statistical significant tests ( the null assumption is that the feature value of valid patents has the same distribution as invalid patents ) . Note that the alignment features are the most significant ones among all the lexical features .
Another challenge in feature extraction is to identify informative features associated with forward citations , ie Fc . Recall that we extract these features because they contribute to forward references , but are not directly related to patent quality . Based on domain knowledge , we use filing year , patent classification , and the number of claims for Fc .
Evaluation of Patent Quality . Unlike simulation data , the ground truth of patent quality is not available . Therefore we have no choice but to indirectly evaluate the results . Here we use the agreement between estimated quality and court ruling decisions ( which are believed to be the closest observations of patent quality ) . In some models , we also need to include the court decision as measurements to estimate patent quality . Therefore , it is not legitimate to include this type of information during training and then validate its performance on the same information ( since the accuracy of such an in sample analysis would be overly optimistic ) . Therefore we use 5 fold cross validation , ie we split dataset II into 5 subsets , and withhold one subset for testing while training our model on a combined set of dataset I and the other 4 subsets ( ie excluding the court ruling information of the patents in the test set ) . To estimate the quality of a patent in the test set , we simply compute its expectation , ie E[Q ] = a/(exp(Fcβc ) + exp(Fqβq) ) , which does not require the knowledge of court decisions .
We first examine the effectiveness of forward citations as a measurement . Since this measure is available for both
1151 Table 2 : Performance of patent quality predictions by different methods . ( a ) . The results are reported based on 5 fold cross validation by evaluating the agreement between the predicted patent quality and the court ruling decision . Evaluation measures include P value ( the result of significance test with the null assumption that estimated quality of valid patents has the same distribution with invalid patents ) , Kendall ’s τ rank correlation ( Corr ) between predicted quality and court ruling , and AUC ( area under curve of using predicted quality to rank probability of being valid in court ruling ) . ( b ) . represents results from a classifier to predict the court ruling decision directly , instead of patent quality estimation models as previous rows . The input features in the classifier ( logistic regression ) include citation features Fc , quality features Fq and estimated patent quality ˆQ from latent graphical models ( LGM ) .
( a ) . Experiment Results of Patent Quality Prediction
Input C + Fc
C + Fc + Fq
C + D− + Fc + Fq
Method LinReg PosReg
LGM LGM LGM
P value 3.73e 1 2.42e 1 1.15e 2 5.75e 3 1.32e 5
Corr 0.048 0.053 0.109 0.198 0.245
AUC 0.504 0.508 0.543 0.574 0.625
( b ) . Experiment Results of Court Ruling Classifier C + Fc + Fq
0.618 0.734
C + Fc + Fq + ˆQ
LogReg LogReg the court ruling decisions using the output of LGM . As we can see in Table 2 , our LGM model outperforms the logistic regression model specifically built to predict court ruling ( with an AUC score of 0.625 compared to 0618 ) In addition , by including the predicted value of patent quality ˆQ as an additional input feature to the logistic regression model , we can achieve an AUC score as high as 0734 These results are important , because it indicates that our LGM approach is an effective means to estimate patent quality , and it could extract significant information complementary to known features related to patent quality and citation .
6 . DISCUSSION AND CONCLUSION
In this paper , we develop advanced machine learning and text mining techniques to address a key problem in patent law , namely providing objective measures of the quality of a patent at any stage in the patent lifecycle . A major challenge is the limited availability of reliable labels for quality . We address this limitation by introducing a latent graphical model , an extension of the measurement model , to infer quality from available measurements related to patent quality . These choice of measurements are guided by domain knoweldge , and we use the number of forward references and court decisions . In addition , we extract a number of lexical features to capture quality related features via natural language processing techniques . The output from latent graphical models , when combined with other text based features , achieves an AUC score close to 0.74 for predicting court decisions .
Our work is the first attempt to apply rigorous machine learning methods with lexical based features to the problem
( a ) LGM : C + Fc + Fq
( b ) LGM : C + Fc + Fq + D
Figure 5 : Q Q Plot of Predicted Value by ( a ) LGM ( Input : C + Fc + Fq ) of Valid and Invalid Patents and ( b ) LGM ( Input : C +Fc +Fq +D ) of Valid and Invalid Patents
Table 1 : Validation of extracted lexical features for differentiating court ruling ( P value ) . * means the statistically significant . + ( or − ) means positive ( or negative ) coefficient in a single variable logistic regression .
Group Alignment
Feature A D ( Euc ) A C ( Euc ) D C ( Euc ) Citation Quality Page rank
Complexity
Relevance Originality
Commonality
Num . of Reference Claim length Count of unique content words AvgCosineall Avg Age Recent Word Prop Avg Usage Avg Contribution
P value +/− + 0.027* + 0.043* + 0.022* + 0.650 0.002* + − 0.001* + 0.017*
0.304 0.107 0.207 0.040* 0.233
+ + + − − datasets , we run experiments on a combined set of datasets I and II . We compare the performance of our method with alternative approaches , including regression residual analysis using linear regression and Poisson regression ( discussed in Section 33 ) Figure 4 shows a Q Q plot of the predicted quality for valid and invalid patents in dataset II . We can see that our LGM approach can yield a comparably better distinction ( see Table 2(a ) for p values ) . Next , we examine whether including quality related features Fq helps to improve the performance . Figure 5(a ) shows a Q Q plot of predicted quality for valid and invalid patents on the 351 examples . We can clearly see a better distinction compared to using the forward citation measurement alone ( see Figure 4(d) ) . Finally , we examine whether additional constraint measurement , ie court decisions , can help improve our prediction . The Q Q plot in Figure 5(b ) suggests that our LGM approach with more measurements achieves the best performance of all methods .
Prediction of Court Ruling Decisions . Another approach to indirectly evaluate the effectiveness of our approach is to build a binary classifier ( eg logistic regression ) to predict
1152 of characterizing patent quality . A number of papers [ 1 , 5 , 13 , 2 ] have modeled patent citation behavior in an effort to understand patent quality . Our approach is fundamentally different in several respects :
1 . We model patent quality as a hidden variable that partially determines forward references since the number of forward references depends on both economic value and the quality .
2 . We represent a patent using both structured features and text based features in order to capture a broader set of expert specified characteristics ;
3 . We validate our model against a “ gold standard ” , namely court decisions on the validity of 351 patents .
We note that other recent work , eg [ 6 ] , have used sophisticated text based features to represent the novelty of patent claims , but they have not used such features in a supervised model for quality . Finally , our broad methodology could be applied to many other applications , such as analyzing the quality of scientific publications and microblogging data .
7 . REFERENCES [ 1 ] J . Allison and M . Lemley . How federal circuit judges vote in patent validity cases . Florida State University Law Review , 27:745–766 , 2000 .
[ 2 ] G . Csardi , J . Tobochnik , P . Erdi , L . Zalanyi , and K . J .
Strandburg . Patent citation networks revisited : Signs of a twenty first century change ? North Carolina Law Review , 87 , 2009 .
[ 3 ] M . Ehrlich . Taking the pulse of patents . Scientific
American , 2009 .
[ 4 ] A . V . et . al . A model for court decisions on patent validity . To be submitted , 2010 .
[ 5 ] B . H . Hall , A . Jaffe , and M . Trajtenberg . Market value and patent citations . RAND Journal of Economics , 36(1):16–38 , 2005 .
[ 6 ] M . A . Hasan , W . S . Spangler , T . D . Griffin , and
A . Alba . Coa : Finding novel patents through text analysis . In Proceedings of KDD , 2009 .
[ 7 ] E . H¨ullermeier , J . F¨urnkranz , W . Cheng , and
K . Brinker . Label ranking by learning pairwise preferences . Artif . Intell . , 172:1897–1916 , 2008 . [ 8 ] M . Kendall . Rank Correlation Methods . Charles
Griffin & Company Limited , 1948 .
[ 9 ] P . Liang , M . I . Jordan , and D . Klein . Learning from measurements in exponential families . In ICML , page 81 , 2009 .
[ 10 ] R . J . Mann and M . Underweiser . A new look at patent quality . To be submitted , 2010 .
[ 11 ] S . Merugu , S . Rosset , and C . Perlich . A new multi view regression approach with an application to customer wallet estimation . In KDD , pages 656–661 , 2006 .
[ 12 ] M . Reitzig and J . T . Paper . Improving patent valuation methods for management validating new indicators by understanding patenting strategies . Research Policy , 33 ( 6 7):939–=957 , 2004 .
[ 13 ] B . Sampat . Determinants of patent quality : An empirical analysis . working paper , 2005 . Mailman School of Public Health and School of International and Public Affairs , Columbia University .
[ 14 ] M . L . Wallace , V . Larivi`erea , and Y . Gingrasa .
Modeling a century of citation distributions . Journal of Informetrics , 3:296–303 , 2009 .
1153
