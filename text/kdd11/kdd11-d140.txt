Supervised Learning for Provenance Similarity of Binaries
Sagar Chaki , Cory Cohen , and Arie Gurfinkel
Carnegie Mellon Software Engineering Institute
4500 Fifth Avenue , Pittsburgh , PA , USA
{chaki,cfc,arie}@seicmuedu
ABSTRACT Understanding , measuring , and leveraging the similarity of binaries ( executable code ) is a foundational challenge in software engineering . We present a notion of similarity based on provenance – two binaries are similar if they are compiled from the same ( or very similar ) source code with the same ( or similar ) compilers . Empirical evidence suggests that provenance similarity accounts for a significant portion of variation in existing binaries , particularly in malware . We propose and evaluate the applicability of classification to detect provenance similarity . We evaluate a variety of classifiers , and different types of attributes and similarity labeling schemes , on two benchmarks derived from open source software and malware respectively . We present encouraging results indicating that classification is a viable approach for automated provenance similarity detection , and as an aid for malware analysts in particular . Categories and Subject Descriptors D27 [ Software Engineering ] : Distribution , Maintenance , and Enhancement—Restructuring , reverse engineering , and reengineering General Terms Algorithms , Measurement , Security Keywords Binary Similarity , Classification , Software Provenance
1 .
INTRODUCTION
Binary similarity is an important area of software engineering research . Techniques for checking binary similarity find applications in areas ranging from code clone detection [ 18 ] and software birthmarking [ 7 ] , to malware analysis [ 20 ] and software virology [ 12 ] . Several notions of similarity have been proposed and studied in the literature . Our focus is the similarity , which we call provenance similarity , that arises from the fact that many binaries are the result of compiling “ similar ” source code with “ similar ” compilers . The term provenance indicates that the similarity originates not only from the source of the binaries , but also from the ( compilation ) process by which they were derived .
The major application area for provenance similarity is malware analysis and virology . Empirical evidence [ 20 ] suggests that the vast majority of existing malware belong to a relatively small number of “ families ” of related instances . For example , a 2006 Microsoft report [ 3 ] states that over 97,000 malware variants were detected in the first six months of 2006 . At the same time , according to Symantec [ 19 ] and Microsoft [ 3 ] , the typical number of malware families encountered in any six month period is a few hundred . In fact , the Microsoft report ( see Figure 1 of [ 20 ] ) further indicates that the 7 and 25 most common families account for over 50 % and 75 % , respectively , of detected malware instances . There are several reasons to believe that many malware families consist of provenance similar instances . Similarity in source code stems from two factors . First , malware authors reuse old programs – it is very unlikely that the 1,131,335 ( presumed ) malicious executables with unique MD5 checksums collected by CERT in the final quarter of 2010 were all written from scratch in 92 days . Second , many malware families – such as Aliser [ 1 ] – generate new instances by changing a data element ( eg , an encryption key , etc. ) , but not other parts of the source code . In addition , similarity in generative compilers arises because a small number of related compilers are used to produce the vast majority of malware . For example , we scanned a collection of malware available within CMU CERT and found that of all the malware whose generative compilers are known with reasonable certainty , about 60 % were compiled with Microsoft Visual C++ , about 25 % with Delphi , and most of the rest with Microsoft Visual Basic . Such limited variability in compiler and source code suggests that provenance similarity is a pervasive feature in malware .
Current approaches to checking provenance similarity of binaries are largely manual ( eg , forensic analysis done by a malware analyst ) . While good tools are available for binary disassembly and visualization ( eg , IDAPro [ 14] ) , most of the technical analysis is still manual , and , therefore , nonscalable . Automated analysis tools , when available , are either not robust ( ie , they have high false positive and false negative rates ) or inefficient . Therefore , developing an automated , robust , and efficient approach for checking provenance similarity is an important and open problem . Addressing this problem is the subject of our paper .
We begin with a definition of provenance similarity .
15 Provenance similarity between two binaries implies similarity in not only the source code from which they were derived , but also in the compilers used to generate them . We define two binaries B1 and B2 to be provenance similar if : ( i ) B1 and B2 have been compiled from the same ( or very similar ) source code ; and ( ii ) B1 and B2 have been generated by closely related compilers ( or the same compilers with different compilation flags ) .
Note that our definition excludes the situations where the source code of B1 and B2 are very different , such as when B1 is derived from B2 via a major patch , or when B1 and B2 are radically different implementations of the same functionality , eg , merge sort and bubble sort . We also exclude cases where the compilers involved are very different ( eg , Visual C++ versus GCC ) . These problems are beyond the scope of this paper .
To understand the challenges in detecting provenancesimilarity , consider Microsoft Visual C++ . As a security feature to guard against buffer overflow attacks , the compiler is able to generate extra code that checks for stack integrity at runtime , and aborts if an integrity violation is detected . However , the exact code generated depends on the compiler context , which includes factors such as the compilation flags used ( eg , /GS to enable the security feature ) , the version of the compiler , calling conventions , etc . Consider two binaries , B1 and B2 , generated from the same source code by Microsoft Visual C++ under different contexts , and therefore containing different code for runtime buffer overflow checks . A typical problem is to detect the provenance similarity of B1 and B2 in a robust and efficient manner .
Developing a Robust and Efficient Provenance Similarity In the general context of binary similarity , the
Checker . following two main approaches have been proposed :
( a ) Syntactic approaches compare signatures [ 8 ] or feature vectors [ 18 , 21 ] extracted from the syntactic binary structure . They resemble techniques used by anti virus tools , and thus are efficient . However , they are not robust , and are foiled by even minor syntactic differences in binaries .
( b ) Semantic approaches compute and compare “ mathematical ” descriptions of the semantic meaning of binaries [ 11 , 15 ] . They are precise , but require considerable manual interaction , and computationally expensive technology for symbolic and mathematical reasoning ( eg , termrewriting , symbolic simulation , and theorem proving ) , and thus have limited practicability and scalability .
In this paper , we investigate the use of classification to develop a robust and efficient binary similarity checker ( BSC ) for provenance similarity . Classification is particularly suitable for problems where : ( i ) closed form solutions are hard to develop , and ( ii ) a solver can be “ learned ” using a training set composed of positive and negative samples . Both conditions apply to provenance similarity .
Provenance Similarity as a Classification Problem . At a high level , a BSC is a “ black box ” that accepts a pair of binaries B1 and B2 as input , and outputs “ yes ” if B1 and B2 are similar , and “ no ” otherwise . This is clearly a binary classification problem . However , to be effective , a classification based BSC must be trained properly and use the right classification technology . Moreover , proper training requires good features that are efficiently computable from binaries . We address these challenges , and make the following specific contributions .
Features . First , we develop a suite of “ semantic ” and “ syntactic ” attributes ( aka features ) ( cf . Sec 4 ) . Semantic features capture the effect of a binary ’s execution on specific components of the hardware state , viz . , registers and memory locations . Syntactic features represent groups of instruction opcodes occurring contiguously in the binary . They are derived from n grams and n perms [ 20 ] . Second , we extend these features two pairs of binaries to capture the difference between two binaries , and not a single binary in isolation .
Benchmark . While malware is a major target domain for provenance similarity , it is extremely challenging to develop good sample ( ie , training and testing ) sets from malware due to the lack of information on source code and generative compilers . Our second contribution is a benchmark ( cf . Sec 5 ) for provenance similarity using a benchmark derived from 14 commonly downloaded SourceForge projects , containing more than 21 million LOC .
Empirical Evaluation . Finally , we do a systematic evaluation of classification for provenance similarity over both open source and malware benchmarks . Our main results are : ( i ) we compare a suite of classifiers implemented in WEKA [ 22 ] on the open source benchmark , and find RandomForest [ 4 ] to be the most effective ; ( ii ) for the malware benchmark , we develop a suite of labeling schemes that do not require source level information ; we show very good classification using these labels and RandomForest ; we argue that this implies applicability of classification for provenance similarity in domains , such as malware , without reliable source level information ; ( iii ) we compare semantic and syntactic attributes , and show that 1 grams are almost as effective as semantic features , while being much less expensive to compute . Our experiments and results are presented in Sec 6 .
Function Similarity . We focus on similarity between binary “ functions ” instead of complete binaries . Intuitively , a function is a binary fragment that results from the compilation of a procedure or method at the source code level . We choose functions because : ( i ) they are the smallest externally identifiable units of behavior generated by compilers ; ( ii ) similarity at the function level is desirable for further analysis ; indeed , malware of similar provenance , will rarely be identical everywhere , but instead share important functions ; thus , malware analysts look for common functions to judge if two binaries are related ; also , information about function similarity are useful for clustering and as building blocks for measuring similarity at the binary level ; and ( iii ) we were able to use extensive function based malware datasets from CERT for our experimentation .
The rest of this paper is structured as follows . In Sec 2 , we survey related work . Sec 3 , we formalize our notion of functions . In Sec 4 , we present our semantic and syntactic features , and the procedures for extracting them from function descriptions . In Sec 5 , we present a benchmark derived from open source software , and our methodology for deriving testing and training sets from the benchmark . In Sec 6 , we present our experiments and results . In Sec 7 , we conclude .
2 . RELATED WORK
A number of different approaches have been proposed for binary similarity . For example , several researchers have used feature vector comparison to detect code clones [ 18 ] , and to protect against malware [ 21 ] . Gao et al . [ 11 ] use
16 a control flow based analysis that combines graph isomorphism , symbolic simulation , and theorem proving to detect semantic differences in binaries . Apel et al . [ 2 ] propose metrics for measuring similarity of malware . Dullien et al . [ 9 ] and Flake [ 10 ] investigate approaches to compare binaries at the level of graphs derived from their syntax . None of these approaches use supervised learning . Also , they require ultimately the selection of a threshold to decide similarity . In contrast , we use supervised learning to automatically compute a threshold that is appropriate for the target binaries . Rosenblum et al . [ 17 ] propose the use of machine learning to extract compiler provenance from binaries . The goal of compiler provenance is to correctly identify the compiler that was used to generate a target binary . Rosenblum et al . use a specific type of machine learning technology , called the linear chain conditional random field . They also rely on syntactic features , known as idiom features . The research presented in this paper is different in terms of both the problem – we consider provenance similarity between two binaries – and the solution – we explore a wide variety of classification techniques , and semantic features , to discover the most effective solution .
Hu et al . [ 13 ] present an approach to index malware based on the syntactic call graph structure of programs . Our approach is complementary since it focuses on features extracted from semantic behavior of functions . Ye et al . [ 23 ] use clustering to categorize malware . In contrast , we focus on using classification .
Cohen et al . [ 8 ] explore the use of cryptographic hashes to study program similarity . A possible basis for semantic function signatures are “ mathematical functions ” extracted [ 15 ] from binaries to describe their behavior . In contrast to these signature based approaches , our approach is based on supervised learning .
A number of researchers have developed techniques , such as alias analysis [ 5 ] and interface identification [ 6 ] , for binaries . These techniques are crucial for identifying functions and their boundaries , a problem that is orthogonal to our goal of developing effective techniques for binary similarity .
3 . FUNCTIONS
In this section , we present the notion of functions we use for this research . We writeD om(X ) to denote the domain of a mapping X . We write X .→ Y to denote a ( possibly partial ) mapping from X to Y with a finite domain . Intuitively , a function is a partial mapping from addresses to instructions , together with a starting address . We assume a totally ordered set Addr of addresses , viz . the set of 32bit unsigned integers , and a set I nst of instructions , each comprising of an opcode , and one or more operands .
Definition 1 . A function is a pair ( B ody , S tart ) where : ( i ) the partial mapping B ody : Addr .→ I nst is its body , and ( ii ) S tart ∈ Dom(B ody ) is its starting address .
In general , a function body is non contiguous , and comprises of a set of instruction blocks that are dispersed throughout the binary . This is true of code generated by state of the art compilers and obfuscators which relocate and reorganize code fragments in non trivial ways . In practice , however , many functions are contiguous , ie , their bodies consist of a single sequence of instructions spanning a contiguous range of addresses . For such functions , we now present a more concrete definition .
Address a
100026E3 100026E6 100026EC 100026ED 100026F3 100026F8 100026F9 100026FF 10002702 10002707 10002708 1000270E 10002714 10002716
Instruction i = Body(a ) push dword [ ebp+0x8 ] lea eax,[ebp 0x208 ] push eax lea eax,[ebp 0x308 ] push dword 0x10006470 push eax call [ 0x1000509c ] add esp,0x10 mov esi,0x100078d2 push esi call [ 0x10005064 ] mov edi,[0x1000501c ] test eax,eax jz 0xa
( i )
π(i )
FF7508 8D85F8FDFFFF 50 8D85F8FCFFFF 6870640010 50 FF159C500010 83C410 BED2780010 56 FF1564500010 8B3D1C500010 85C0 7408
FF7508 8D85F8FDFFFF 50 8D85F8FCFFFF 6800000000 50 FF1500000000 83C410 BE00000000 56 FF1500000000 8B3D00000000 85C0 7408
Figure 1 : A basic block of an example function , with and π mapping .
Representing Contiguous Functions as Byte Sequences . Let S tr be the set of finite length byte sequences ( or strings ) , and • be concatenation . Let : I nst '→ S tr and π : I nst '→ S tr be mappings such that : ( INJ ) is injective , and ( PIC ) π(i1 ) =π ( i2 ) iffi 1 and i2 differ only in Intuitively , ( i ) and π(i ) are the their address operands . exact and position independent representations of i , respectively . For a contiguous function f = ( S tart , B ody ) , let A = ffa1 , . . . , an be the sequence of addresses obtained by sorting Dom(B ody ) in increasing order , and I = ffi1 , . . . , in be the sequence of instructions such that : ∀1 ≤ k ≤ n · ik = B ody(ak ) . Then , the exact , f.E B , and position independent , f.P B , representations of f are : f.E B = ( i1 ) • ··· • ( in ) f.P B = π(i1 ) • ··· • π(in ) .
Example 1 . Figure 1 shows a basic block of a function f with starting address 100025E2 , B ody , and π . Note that while the function starts at 100025E2 , the basic block displayed in the figure starts at 100026E3 . There are additional addresses in the function both before and after the displayed basic block . The mapping ( i ) follows the Intel IA 32 instruction definitions , and , thus , satisfies ( INJ ) . The mapping π(i ) is obtained by replacing the bytes in ( i ) which represent address operands with zeros , and , thus , satisfies ( PIC ) . For instructions i at addresses 100026F3 , 100026F9 , 10002702 , 10002708 , and 1000270E , ( i ) )= π(i ) because the operand bytes corresponding to an address have been replaced with zeros in π(i ) . Instructions at addresses 100026F9 , 10002708 , and 1000270E have operands which are easily identified as memory dereferences by the type of the operand , while the operands for the instructions at addresses at 100026F3 and 10002702 , can only be identified as addresses by the observation that the values of these operands fall inside the defined range of addresses for the program . This heuristic approach is occasionally flawed ( eg , in invalid truncated programs ) , but we don’t believe this detail is relevant to our results . For all remaining instructions , ( i ) = π(i ) since i does not have any address operands . Finally , f.E B and f.P B are , respectively , the concatenation of the elements of the third and fourth columns .
Our approach assumes a reliable way to extract functions from executables . Identifying function boundaries in binaries is a complex and open problem . This is especially true for obfuscated ( ie , packed and/or encrypted ) malware . These issues are beyond the scope of this paper .
17 4 . ATTRIBUTES
Recall that our BSC classifies function pairs into two groups – similar ( the “ yes ” group ) , and dissimilar ( the “ no ” group ) . We now present our approach to extract attribute vectors from a function pair ( f1 , f2 ) . Since we are interested in the degree of similarity between f1 and f2 , each attribute captures the difference between f1 and f2 along some dimension . More specifically , the attribute vector is constructed as follows :
1 . We apply a function level attribute extraction scheme A to extract two attribute vectors : ffv1 = A(f1 ) and ffv2 = A(f2 ) . We assume that all attributes extracted by A are numeric .
2 . We construct the attribute vector for ( f1 , f2 ) , denoted by AΔ(f1 , f2 ) , by taking the pointwise difference of ffv1 and ffv2 . That is :
∀1 ≤ i ≤ | ffv1| · AΔ
( f1 , f2)[i ] = ffv1[i ] − ffv2[i ] , .
Note that AΔ is asymmetric , in general , AΔ(f1 , f2 ) )= AΔ(f2 , f1 ) . It is possible to make AΔ symmetric by ordering each function pair such that f1.E B < f2.E B . ie ,
We now present two function level attribute extraction schemes , which we use for our experiments :
1 . A semantic approach based on the effect of executing the function on a hardware state .
2 . A syntactic approach based on n grams and nperms [ 21 ] – this approach represents existing work on attribute vector extraction from binaries , and serves as a point of comparison . In Sec 6 , we show empirically that classification with semantic attributes outperforms classification with syntactic attributes .
4.1 Semantic Attributes
The semantic attribute vector of a function f , denoted by S emAtt(f ) , is a vector of values of semantic attributes . S emAtt(f ) represents the effect of executing f on a state of the hardware . Moreover , semantic attributes are defined so that if S emAtt(f1 ) and S emAtt(f2 ) are similar , then the executions of f1 and f2 have similar effects . We assume that every function f has a control flow graph f.C F G , and that each execution of f follows a specific path in f.C F G . The process of extracting a semantic attribute vector from a path in f.C F G involves simulation , sign abstraction , and counting abstraction . We now present this process in detail .
411 Extracting Attribute Vectors from Paths Let SymConst = {v0 , v1 , . . .} be a denumerable set of symbolic constants . Let ± denote the set of signs {+,−} and N = {1 , 2 , . . .} be the set of numerals representing natural numbers . Symbolic expressions , denoted by SymExp , are linear combinations of symbolic constants ( with unit or zero coefficient ) and integers . Formally , their syntax has the following BNF form :
SymExp := ±SymConst | ± N | ± SymConst ± N
Symbolic State . Let Reg = {r0 , r1 , . . .} be a denumerable set of virtual registers , and Mem = {m0 , m1 , . . .} be a denumerable set of memory locations . A symbolic state
( i ) R : Reg .→ SymExp maps is a pair ( R , M ) where : registers to the symbolic values of their contents ; and ( ii ) M : Mem .→ SymExp × SymExp maps memory locations to the symbolic values of their addresses and contents .
Sign Abstraction . The sign abstraction of a symbolic expression e , denoted by Sign(e ) , is an element of Σ , where Σ = ( ± ∪ {0} ) × ( ± ∪ {0} ) \ {(0 , 0)} . Intuitively , Sign(e ) is obtained by removing all symbolic constants and numerals from e , and replacing any missing component with 0 . Formally , it is defined as follows : Sign(q v ) = ( q , 0 ) Sign(q n ) = ( 0 , q ) Sign(q v q . n ) = ( q , q . ) where q , q . ∈ ± , v ∈ SymConst , and n ∈ N . Note that {(0 , 0)} is not a valid sign abstraction , and |Σ| = 8 . Sign abstraction extends to pairs of symbolic expressions in the natural manner : Sign(x , y ) = ( Sign(x ) , Sign(y) ) .
Sign Abstract State . Applying sign abstraction to a symbolic state s = ( R , M ) results in the sign abstract state Sign(s ) . Formally , Sign(s ) , is a pair of functions ( R# , M # ) such that for f = R , M , the following holds :
.∀x ∈ Dom(f ) · f #
)
Dom(f ) = Dom(f # ( x ) = Sign(f ( x ) ) Counting Abstraction . Let f : X .→ Σ be any mapping . The counting abstraction of f , denoted by Count(f ) is the vector of numbers defined as follows . First we order the elements of ± ∪ {0} strictly as : − < 0 < + . This induces the following strict ordering on Σ : ) ⇐⇒ ( x . < x .
) ∨ ( x = x . ∧ y < y .
( x , y ) < ( x . , y .
)
Let ffΣ be the vector of elements of Σ induced by the above strict ordering on Σ . Thus , ffΣ[1 ] = ( −,− ) and ffΣ[8 ] = ( + , + ) . Then , Count(f ) is the vector such that :
Count(f )[i ] = |{x ∈ X | f ( x ) = ffΣ[i]}|
In other words , the i th element of Count(f ) is the number of elements of Dom(f ) that are mapped byf to the i th element of ffΣ . Note that Count(f ) always has 8 elements . For any pair x = ( x1 , x2 ) , let us write Fst(x ) and Snd(x ) to mean x1 and x2 respectively . Let f : X '→ Σ × Σ be any mapping . Then the two functions Fst(f ) , Snd(f ) are defined as follows : ∀x ∈ X · Fst(f )(x ) = Fst(f ( x ) ) ∧ Snd(f )(x ) = Snd(f ( x ) ) We write ffx • ffy to denote the concatenation of vectors ffx and ffy . The counting abstraction of a sign abstract state s# = ( R# , M # ) , denoted by Count(s# ) , is the vector of numbers defined as follows : Count(s#
))•Count(Snd(M # In other words , we apply counting abstraction to the registers , memory addresses , and memory data , and concatenate the resulting vectors . Note that Count(f # ) always has 24 elements . Intuitively , each index of Count(f # ) corresponds to a different a semantic attribute .
)•Count(Fst(M #
) =Count ( R#
Post Condition . A path p in f.C F G is a sequence of assembly instructions . The post condition of p , denoted by Post(p ) , is a symbolic state representing the hardware configuration after the execution of p . The exact value of Post(p ) depends on the semantics of the instructions in p . In our experiments , we rely on the Rose [ 16 ] infrastructure to compute Post(p ) .
) )
18 Example 2 . Consider the path p with three instructions inc ax , inc ax , and mov [ bx ] ax . Thus , p increments register ax twice , and then copies the contents of ax into the memory location whose addresses is stored in bx . Then , Post(p ) = ( R , M ) where : R(r0 ) = +v0 + 2
R(r1 ) = +v1
M ( m0 ) = ( +v1 , +v0 + 2 )
Note that r0 and r1 represent registers ax and bx , respectively . Also , v0 and v1 are the initial values of ax and bx , respectively . Finally , m0 is the memory location whose address is stored in bx . Also , Sign(Post(p ) ) = ( R# , M # ) where :
R#(r0 ) = ( + , + )
R#(r1 ) = ( + , 0 )
M #(m0 ) = ( (+ , 0 ) , ( + , + ) )
Recall that ffΣ[7 ] = ( + , 0 ) and ffΣ[8 ] = ( + , + ) . Therefore , Count(Sign(Post(p) ) ) = ffx • ffy • ffz , where : ffx[7 ] = ffx[8 ] = ffy[7 ] = ffz[8 ] = 1 , and all other elements of ffx , ffy , and ffz are zero .
Extracting Attribute Vectors from Paths . The semantic attribute vector of a path p , denoted by S emAtt(p ) , is defined as follows :
S emAtt(p ) =Count ( Sign(Post(p) ) )
412 Extracting Attribute Vectors from Functions
Let f be a function represented by the byte sequence f.E B . The attribute vector S emAtt(f ) is constructed from f.E B as follows :
1 . f.E B is parsed and the control flow graph f.C F G In our implementation , we use the is constructed . Rose [ 16 ] infrastructure to perform this step .
2 . In general , f.C F G has cycles , and therefore infinitely many paths . We use a bounded depth first search traversal to extract #p paths of depth at most #d from f.C F G . The search is randomized , ie , successors for traversal are picked randomly . For our experiments , we use #p = 50 and #d = 50 . Note that since the number of possible paths is exponential in #d , we limit #p as well . Let P be the set of paths extracted . 3 . From each p ∈ P , we construct S emAtt(p ) as defined in the previous section . Note , |S emAtt(p)| = 24 .
4 . Let ffl be the element wise addition of two vectors , ie ,
∀i · ( ffx ffl ffy)[i ] = ffx[i ] + ffy[i ] Then , we compute S emAtt(f ) as follows :
S emAtt(f ) =
S emAtt(p ) fi p∈P
Increasing #p or #d also increases the time to extract semantic attributes . Empirically , we found #p = 50 and #d = 50 to be a good tradeoff . For example , to extract semantic attributes from a random sample of 541 functions in our benchmark using #p = 50 and #d = 50 , the time required is 95 mins . When #d is increased to 100 , the time is 165 mins , a 74 % jump . However , the number of attributes for which values differ between the two cases is only about 1.2 % of the total number of attributes .
Example 3 . Let f be a function such that f.C F G has two paths p1 and p2 . Let p1 be the same as p from Example 2 . Therefore , S emAtt(p1 ) =Count ( Sign(Post(p1) ) ) = ffx • ffy • ffz , where : ffx[7 ] = ffx[8 ] = ffy[7 ] = ffz[8 ] = 1 , and all other elements of ffx , ffy , and ffz are zero .
Let p2 have 4 instructions mov [ ax ] bx , dec ax , mov bx 10 , mov [ ax ] bx . Then , Post(p2 ) = ( R , M ) where :
R(r0 ) = +v0 − 1 M ( m0 ) = ( +v0 , +v1 )
R(r1 ) = +10 M ( m1 ) = ( +v0 − 1 , +10 )
Note that r0 and r1 represent registers ax and bx , respectively . Also , v0 and v1 are the initial values of ax and bx , respectively . The addresses of memory locations m0 and m1 are stored in ax initially and finally , respectively . Also , Sign(Post(p2 ) ) = ( R# , M # ) where :
R#
( r0 ) = ( +,− ) R# ( m0 ) = ( (+ , 0 ) , ( + , 0 ) ) M #
( r1 ) = ( 0 , + ) ( m1 ) = ( (+,− ) , ( 0 , + ) ) M # Recall that ffΣ[5 ] = ( 0 , + ) and ffΣ[6 ] = ( +,− ) . Therefore , S emAtt(p2 ) = Count(Sign(Post(p2) ) ) = ffx • ffy • ffz , where : ffx[5 ] = ffx[6 ] = ffy[6 ] = ffy[7 ] = ffz[5 ] = ffz[7 ] = 1 , and all other elements of ffx , ffy , and ffz are zero . Finally , S emAtt(f ) = S emAtt(p1 ) ffl S emAtt(p2 ) = ffx • ffy • ffz , where : ffx[5 ] = ffx[6 ] = ffx[7 ] = ffx[8 ] = 1 ffz[5 ] = ffz[7 ] = ffz[8 ] = 1 ffy[6 ] = 1 ffy[7 ] = 2 and all other elements of ffx , ffy , and ffz are zero .
Implementation Details . For our empirical evaluation , we generalize semantic attributes as follows . First , we split registers into three sub categories : general purpose , segment , and flag – based on their use by the compiler . We apply sign and counting abstractions separately to each category . Since now there are 3 types of registers in addition to memory addresses and data , |S emAtt(p)| = |Σ|×(3 + 2 ) = 40 for any path p . Second , we consider three ways of combining the elements of S emAtt(p ) to obtain an element of S emAtt(f ) – sum ( as described above ) , minimum , and maximum . We also consider splitting up a path at call sites , and replacing a counting abstraction with a 0 1 abstraction ( ie , replacing all counts greater than 0 with 1 ) . In all , we have 36 possible combinations , leading to 40 × 36 = 1440 attributes . 4.2 Syntactic Attributes
We present two flavors of syntactic attributes , based on n grams and n perms [ 20 ] , respectively . Let Mnem be the sequence of x86 instruction mnemonics , and Mnemn be the set ffMnemn of sequences of n elements drawn from Mnem . Let be the vector of elements of Mnemn ordered lexicographically . For any positive value of n , the n gram vector of f , denoted by N Gramn(f ) , is computed as follows :
1 . Parse f.E B to construct of sequence of assembly instructions I = ffi1 , . . . , ik . Extract the mnemonic from each instruction to obtain a sequence of mnemonics M = ffm1 , . . . , mk . In our implementation , this step is performed using Rose .
19 2 . Then , N Gramn(f ) is the vector of | such that for any 1 ≤ i ≤ | equals the number of times sequence of M . ffMnemn| numbers , ffMnemn| , N Gramn(f )[i ] ffMnemn[i ] occurs as a sub
Example 4 . Let Mnem = {inc , dec , mov} .
Then , Mnem2 = {inc· inc , . . . ,mov · mov} . Let f be a function such that f.E B leads to the following sequence of mnemonics : M = ffmov , inc , inc , mov , dec , mov . For any x ∈ Mnem2 , let us write N Gram2(f )[x ] to mean N Gram2(f )[i ] such that Mnem2[i ] = x . Then , ff N Gram2(f )[mov · inc ] =N Gram2(f )[inc · inc ] = 1 N Gram2(f )[inc · mov ] =N Gram2(f )[mov · dec ] = 1 N Gram2(f )[dec · mov ] = 1 and all other elements of N Gram2(f ) are 0 .
For any positive value n , the n perm vector of f is denoted by N P ermn(f ) . n perm vectors are similar to n gram vectors , except that ordering is ignored .
Example 5 . For the Mnem and f in Example 4 , we have , N P erm2(f )[inc · mov ] =N P erm2(f )[dec · mov ] = 2 N P erm2(f )[inc · inc ] = 1 and all other elements of N P erm2(f ) are 0 .
Detailed evaluation of our attributes is presented in Sec 62
5 . TRAINING AND TESTING SETS
Since our training and testing sets are constructed using the same procedure , we refer to them simply as sample sets . A ( N + , N− ) sample set – consisting of N + “ yes ” samples and N−
“ no ” samples – is constructed as follows :
1 . First , we construct a benchmark B comprising of sets of provenance similar functions .
2 . Next , using B , we create N + and N− random samples belonging to “ yes ” and “ no ” classes respectively . The final result is the union of all the samples obtained .
We now describe these two steps in more detail .
5.1 Benchmark Construction
Our benchmark B consists of a set of clusters , where each cluster is a set of provenance similar and contiguous functions . B was constructed as follows :
1 . We manually selected a set of 14 open source software packages ( containing collectively over 21 million LOC ) available at SourceForge ( http://wwwsfnet ) that are among the most downloaded , written in C/C++ , and executable on Windows . Each software yielded a set of clusters by the following steps .
2 . The software was compiled with Microsoft Visual Studio 2003 .NET , 2005 , and 2008 on Windows XP SP3 .
3 . The binaries were processed with IDAPro 5.6 [ 14 ] together with custom Python extensions to extract a set of functions . We write f.N ame to mean the compilergenerated name of a function f . We use this name to relate functions across the different compilers , as described in the next step .
Table 1 : Benchmark summary : KLoC = Kilo Lines of Code ; BSz = size of binaries in MB ; #Cl = no . of equivalence classes ; Avg , StD , Med = mean , standard deviation and median of equivalence class sizes .
7zip cppunit flac net snmp notecase NppExec
153 36 78 395 48 35 285 268 154 251 138
Software KLoC BSz #Cl Avg 4.4 15.2 4.4 4.6 4.7 4.1 2.6 5.1 2.9 2.9 5.4 4.2 7.4 9.8
2992 616 269 1507 228 888 1205 3745 376 872 1455 639 1414 1646
2.4 26.4 5.1 5.9 0.8 3.8 3.0 4.8 0.3 8.9 3.7 1.4 6.0 27.3 tightvnc tinyxml ultravnc ogl poco speed tcl
7
183 154 wincvs
StD Med 9.4
128.4
8.4 62.4 7.8 6.6 17.7 61.5 3.9 1.0 48.9 18.9 33.9 63.8
3 3 3 3 3 3 2 3 3 3 3 3 2 3
4 . Recall that f.P B is the position independent byterepresentation of a function f . The functions were clustered into equivalence classes induced by the reflexive transitive closure of the following relation R : f1Rf2 ⇐⇒ f1.N ame = f2.N ame ∨ f1.P B = f2.P B Thus , we assume that if f1Rf2 , then f1 and f2 are provenance similar . Alternatively , we assume that two provenance dissimilar functions have different names and position independent byte representations .
5 . An equivalence class c was discarded if it satisfied the following condition : |c| = 1
'
Maxf∈c|f.E B| < 50
This eliminates equivalence classes that are : ( i ) singleton – since , as we see later , singletons are not useful in generating “ yes ” samples ; and ( ii ) have no functions with bodies larger than 50 bytes – since we believe that classification is not applicable to detect similarity between very small functions . The cutoff of 50 was chosen empirically .
6 . Each remaining equivalence class yields a cluster . Table 1 summarizes our benchmark B . A median of ( mostly ) 3 indicates that the most common case is , as expected , one function for each of the 3 compilers . The number of equivalence classes , averages and standard deviations vary widely , indicating that we have a good mix of clusters in terms of their sizes . 5.2 Sample Set Construction We now present our approach to construct a ( N + , N− )sample set from B . When constructing ( either “ yes ” or “ no ” ) samples , we ignore function pairs ( f1 , f2 ) such that f1.P B = f2.P B1 , since , by assumption , such function pairs
1Without this restriction , the overall trend in our results is similar , but with even higher F measures .
20 are already provenance similar , and hence need not be classified . In the following , Ψ is a scheme to extract feature vectors from function pairs , as described in Sec 4 .
First , to construct a “ yes ” sample , we use procedure PickYes , consisting of the following steps : ( i ) Randomly pick a cluster c ∈ B . Recall that c contains at least two functions . ( ii ) Randomly pick two distinct functions f1 , f2 ∈ c st f1.P B )= f2.P B . ( iii ) Output Ψ(f1 , f2 ) labeled with “ yes ” . Next , to construct a “ no ” sample , we use procedure PickNo , consisting of the following steps : ( i ) Randomly pick two distinct clusters c1 , c2 ∈ B . ( ii ) Randomly pick two functions f1 ∈ c1 and f2 ∈ c2 . ( iii ) Output Ψ(f1 , f2 ) labeled with “ no ” . Finally , to construct the desired ( N + , N− ) sample set , we repeat PickYes N + times , PickNo N− times , and collect together the resulting samples . The benchmark B consists of over six million functions , divided into 17,852 clusters . Therefore , we have over 36 × 1012 possible samples available for training and testing . In our experiments , we use sample sets consisting of an equal proportion of “ yes ” and “ no ” samples . Thus , we say “ ( n1 , n2) classification ” to mean a classification with a ( n1 2 Since our testing and training sets are randomly created , we repeat our experiments several times , and use the averages of the measurements for our conclusions . Specifically , for any metric M , we say “ M of ( n1k1 , n2k2) classification ” to mean the average of the values of M for k1 × k2 classifications , done by : ( i ) selecting k1 random training sets of size n1 , and for each training set , ( ii ) learning a classifier and testing it with k2 random testing sets of size n2 . We use three main metrics for our evaluation – the F measure , training time , and testing time .
2 ) training set , and a ( n2
2 ) testing set .
, n2
, n1
2
6 . EXPERIMENTAL RESULTS
We performed three types of experiments : learner selection , feature selection , and to judge the applicability of our approach to detect similarity in malware . All experiments were performed on a quad core 2.8 GHz machine . Each session ( training or testing ) was run with a time limit of 1800s and a memory limit of 700MB . Our benchmark and tools are available at http://wwwcontribandrew cmuedu/user/schaki/binsim 6.1 Learner Selection
These experiments were done with the semantic attributes – ie , with Ψ = S emAttΔ – on our open source benchmark B . We first evaluated 24 classifiers implemented in the WEKA [ 22 ] tool ( version 362 ) For each classifier , we used the default WEKA configuration , and measured F for a ( 10000 5 , 20000 5) classification . We observed a clear separation between the classifiers : seven are effective – having F measures ≥ 0.875 ; the rest are ineffective – having F measures around ≈ 0.5 ( like random guessing ) . The results for the 7 effective schemes , in order of decreasing F measure , are summarized in Table 2 .
Comparing Effective Schemes . Next , we compared the performance of the 7 effective schemes . The IBk and J48graft schemes are eliminated immediately . They have worse performance – lower F measure , and higher training and testing times – compared to RandomForest . We evaluated each of the remaining 5 schemes using a series of ( N 5 , 2N 5) classifications , where the value of N was varied from 10000 to 30000 in increments of 2000 . The results
Table 2 : Comparison of various classifiers on a ( 10000 5 , 20000 5) classification . F measure = avg/stdev ; Train = training time ( avg/stdev in secs ) ; Test = testing time ( avg/stdev in secs ) .
Classifier RandomForest J48graft J48 Ridor REPTree RandomTree IBk
F measure 0928/0002 0909/0002 0900/0003 0895/0005 0887/0003 0876/0003 0861/0003
Train
1941/023 9796/628 9374/495 2667/271 2215/064 572/013 1646/297
Test
456/0075 652/0301 334/0056 279/0045 308/0044 389/0123 4590/266 are summarized in Figure 2 . All classifiers show increasing F measures with increasing N . However , RandomForest is the clear winner . All classifiers also require more testing and training time with increasing N . In particular , Ridor ’s training is most expensive , and it times out for training sets of size 24,000 and up . Overall , we conclude that RandomForest is the most effective classifier .
RandomForest . Next , we evaluated RandomForest by varying its three parameters – ( P1 ) number of trees , ( P2 ) number of attributes , and ( P3 ) tree depth . First , we performed ( N 5 , 2N 5) classifications with increasing P1 , and with N ranging from 5 to 150 . Figure 3(top ) summarizes the F measures we observed . The F measure improves rapidly with increasing P1 up to N = 20 , then improves very slowly till N = 40 . For N > 40 , the improvement tapers off , and is difficult to ascertain . Next , we performed ( N 5 , 2N 5) classifications with increasing P2 , and with different N . Figure 3 (bottom ) summarizes the F measures we observed . As expected , the F measure improves with increasing P2 , but the gains taper off after about 12 . Finally , we performed ( N 5 , 2N 5) classifications with increasing P3 – starting at 0 ( which indicates unlimited depth ) , and moving up to 80 in increments of 10 – and with different N . We saw that P3 ≥ 20 is required for an F measure within 0.1 of the F measure with unlimited P3 . However , at P3 = 20 , the training and testing times are similar to those with unlimited P3 . Therefore , unlimited P3 is the optimal choice .
In all cases , training and testing times increase monoton
Δ
Next , we compared S emAttΔ , N Gramn ically with the values of P1 , P2 , P3 , and N . 6.2 Feature Selection Δ and N P ermn for n = {1 , 2} using our open source benchmark B . We evaluated their performances using a series of ( N 5 , 2N 5)classifications , where the value of N was varied from 10000 to 30000 in increments of 2000 . Table 3 shows our reΔ ) is superior to sults . We see that N Gram1 S emAttΔ in terms of training and testing times , and almost as good in terms of F measure . Feature extraction Δ is also less expensive than S emAttΔ ( eg , with N Gram1 about one day vs . a week for our open source benchmark ) . A combination of S emAttΔ and N Gram1 Δ improves negligibly over S emAttΔ alone .
Δ(= N P erm1 expected ,
Δ and Δ due to a N P erm2 greater number of attributes . However , surprisingly , it is classification with N Gram2 than with N Gram1
Δ is slower
As
21 e e r r u u s s a a e e M M F F
0.95 0.95
0.93 0.93
0.91 0.91
0.89 0.89
0.87 0.87
10 10 i i
) ) c c e e s s ( ( e e m m T T g g n n i i t t s s e e T T
9 9
8 8
7 7
6 6
5 5
4 4
3 3 i
) ) c c e s ( e m T g n n a r T i i
2 2 1800 1800 1600 1600 1400 1400 1200 1000 800 600 400 200 0
10
12
14
16
18
20
22
24
Training Set Size ( 1000s ) . Testing Set Size = 2 x Training Set Size .
26
28
30
Figure 2 : Comparison of effective classifiers ; legend at the bottom applies to all charts .
Δ and N P erm2 instructions , upon reordering , will
Δ , but the same N Gram1 also worse in terms of F measure . This indicates that counts of pairs of adjacent instruction mnemonics characterize a function less well than the counts of single mnemonics . We believe that one reason for this is instruction reordering due to compiler optimizations and changes in source code , the two sources of provenance similarity . The same set lead to different of Δ . N Gram2 Δ is the best choice if time is Overall , we believe N Gram1 limited , and S emAttΔ is best otherwise . Recall that AΔ computes the element wise difference of function level attribute vectors . We also evaluated AΔ against another approach – denoted by A• – that simply concatenates the two function level vectors . Specifically , A• [ • denotes concatenation ] Note that A• ( f1 , f2 ) has more attributes than AΔ(f1 , f2 ) , and captures information about the difference between f1 and f2 less directly . Empirically , we found S emAttΔ to be superior to S emAtt• 6.3 Applicability to Malware To judge the applicability of classification to detect similarity in malware , we experimented with a benchmark M derived from 12 malware families from the CERT database . To ensure that we only consider true families , we picked a dozen candidates from cases on which a suite of anti virus tools all agree in their family identification . The details of the families are presented in Table 4 .
( f1 , f2 ) = A(f1 ) • A(f2 )
, as shown in Table 3 .
The main problem with malware is designing a good label e e r r u u s s a a e e M M F F
0.955 0.955
0.95 0.95
0.945 0.945
0.94 0.94
0.935 0.935
0.93 0.93
0.925 0.925
0.92
0.915 e e r r u u s s a a e e M M F F
0.945 0.945
0.94 0.94
0.935 0.935
0.93 0.93
0.925 0.925
0.92
RandomForest RandomForest
J48 J48
Ridor
RandomTree
REPTree
F measure F measure
TrainSet=10000 TrainSet=10000 TrainSet=12000 TrainSet=12000 TrainSet=14000 TrainSet=14000 TrainSet=16000 TrainSet=16000 TrainSet=18000 TrainSet=20000
5
10
20
30
40
50
60
90
120
150
Number of Trees
3
6
9
12
15
20
Number of Attributes
25
30
40
50
Figure 3 : Performance of RandomForest with different tree numbers ( top ) and attribute numbers ( bottom ) ; legend at the top applies to both charts .
Table 4 : Malware families in our benchmark ; #Ehashes and #Phashes = no . of unique exact and position independent function byte representations .
9
265 308
Name #Files #Funcs #Ehashes #Phashes F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12
6631 80831 9360 2410 1090 5159 10887 10815 1259 792
1889 15512 2363 1053 112 3053 4384 431 1055 180
487 1766 1070 704 112 346 1790 395 581 62
7 10 38 22 27 18 9 24 6
49318 2400
1251 102
19241
416 ing function for training provenance similarity . Since sourcelevel information is rarely available , it is difficult to automatically decide if two functions are similar ( or not ) with high confidence . While the labeling problem for malware is beyond the scope of our paper , as a starting point , we experimented with a labeling function – denoted LPIC – based on position independent ( PIC ) byte representations . Specifically , LPIC labels an attribute vector derived from a function pair ( f1 , f2 ) “ yes ” iff f1 and f2 have the same PIC byte representation . We don’t claim that LPIC is the best possible labeling function for malware similarity . However , we believe that it is a good approximation to the extent that if classification with LPIC is not accurate , it will likely be inaccurate with a better labeling function as well . We created sample sets using our benchmark M and LPIC , and performed a series of ( N5 , 2N5) classifications , where the value of N was varied from 10000 to 30000 in increments of 2000 . We repeated our experiments with
22 Table 3 : Comparison of S emAttΔ , N Gramn set ; Tr = Avg . training time ( s ) ; Te = Avg . testing time ( s ) .
Δ , and S emAtt•
; SA + N G1 = S emAtt + N Gram1 ; N = size of training
Δ
N Gram1 F
( SA + N G1 ) F
Δ
N Gram2
Δ
N P erm2
Δ
S emAtt•
N
S emAttΔ
F
10K 0.929 12K 0.930 14K 0.933 16K 0.936 18K 0.939 20K 0.939 22K 0.942 24K 0.945 26K 0.946 28K 0.947 30K 0.949
Tr Te 4.6 19 25 5.1 5.5 28 6.0 34 6.4 40 6.8 45 50 7.3 7.8 55 8.3 60 8.7 63 73 9.2
Tr Te 3.3 17 22 3.5 3.8 25 4.1 28 4.4 33 4.6 37 40 4.8 5.0 46 5.2 49 5.4 51 58 5.7
0.919 0.924 0.928 0.931 0.935 0.937 0.938 0.942 0.944 0.945 0.948
Tr Te 4.8 22 29 5.3 5.8 33 6.3 40 6.8 46 7.3 56 60 7.8 8.3 67 8.8 73 9.4 84 89 9.8
0.930 0.933 0.935 0.939 0.943 0.944 0.947 0.947 0.949 0.950 0.952
F
0.884 0.890 0.894 0.900 0.904 0.908 0.913 0.915 0.918 0.922 0.924
Tr 90.9 118 145 155 185 211 233 261 292 310 362
Te 5.5 6.1 6.8 7.2 7.8 8.4 9.4 9.8 10.0 10.6 11.3
F
0.880 0.889 0.894 0.899 0.904 0.907 0.912 0.914 0.918 0.920 0.924
Tr 83 98 120 125 158 171 190 224 236 257 285
Te 5.1 5.6 6.2 6.7 7.2 7.8 8.3 9.2 9.4 9.6 10.1
F
0.905 0.909 0.914 0.918 0.922 0.926 0.926 0.928 0.932 0.935 0.936
Tr 39 52 58 70 82 94 99 112 125 138 146
Te 9.1 10.5 11.9 13.2 14.6 16.0 17.5 18.7 20.0 21.6 23.0
[ 9 ] T . Dullien and R . Rolles . Graph based comparison of
Executable Objects . In Proc . of SSTIC , 2005 .
[ 10 ] H . Flake . Structural Comparison of Executable
Objects . In Proc . of DMIVA , 2004 .
[ 11 ] D . Gao , M . K . Reiter , and D . X . Song . BinHunt :
Automatically Finding Semantic Differences in Binary Programs . In Proc . of ICICS , 2008 .
[ 12 ] M . Hayes , A . Walenstein , and A . Lakhotia . Evaluation of malware phylogeny modelling systems using automated variant generation . Journal in Computer Virology ( JCV ) , 5(4):335–343 , November 2009 . [ 13 ] X . Hu , T . Chiueh , and K . G . Shin . Large scale malware indexing using function call graphs . In Proc . of CCS , 2009 .
[ 14 ] IDA Pro . http://wwwhex rayscom/idapro [ 15 ] R . Linger , S . Prowell , and K . Sayre . Computing the behavior of malicious code with function extraction technology . In Proc . of CSIIRW , 2009 .
[ 16 ] ROSE . http://rosecompilerorg [ 17 ] N . E . Rosenblum , B . P . Miller , and X . Zhu . Extracting compiler provenance from program binaries . In Proc . of PASTE , 2010 .
S emAttΔ and N Gram1 In all cases , we got a high F measure ( 0.998 ) , indicating that classification is a promising approach for provenance similarity in malware .
Δ .
7 . CONCLUSION
In this paper , we explore the use of classification to detect provenance similarity of binaries . Specifically , we use classification to predict if a pair of functions is “ similar ” or “ dissimilar ” . Our classification uses features that capture the difference between the target function pair . We evaluate our approach on a benchmark derived from open source software , with encouraging results . Preliminary experiments also indicate that classification is a promising approach to detect malware similarity . A major challenge with malware is the difficulty of obtaining sufficiently many samples with reliable similarity labeling . We believe that semi supervised learning is a promising avenue in addressing this issue .
8 . REFERENCES [ 1 ] Aliser worm . http://wwwsophoscom/security/ analyses/viruses and spyware/w32aliserdamhtml
[ 2 ] M . Apel , C . Bockermann , and M . Meier . Measuring similarity of malware behavior . In Proc . of LCN , 2009 .
[ 18 ] A . Sæbjørnsen , J . Willcock , T . Panas , D . J . Quinlan ,
[ 3 ] M . Braverman , J . Williams , and Z . Mador . Microsoft security intelligence report : January–June 2006 , 2006 . http://microsoftcom/downloads/detailsaspx ? FamilyId=1C443104 5B3F 4C3A 868E 36A553FE2A02 .
[ 4 ] L . Breiman . Random Forests . Machine Learning ,
45(1):5–32 , 2001 .
[ 5 ] D . Brumley and J . Newsome . Alias analysis for assembly . Technical report CMU CS 06 180[R ] , Carnegie Mellon University , Pittsburgh , 2006 .
[ 6 ] J . Caballero , N . M . Johnson , S . McCamant , and
D . Song . Binary code extraction and interface identification for security applications . Technical report UCB/EECS 2009 133 , University of California , Berkeley , Berkeley , CA , October 2009 . and Z . Su . Detecting code clones in binary executables . In Proc . of ISSTA , 2009 .
[ 19 ] Symantec . Symantec internet security threat report : Trends for January 06–June 06 , 2006 . http://www . symanteccom/enterprise/threatreport/indexjsp
[ 20 ] A . Walenstein and A . Lakhotia . The Software
Similarity Problem in Malware Analysis . In Duplication , Redundancy , and Similarity in Software , volume 06301 of Dagstuhl Seminar Proceedings , 2007 . [ 21 ] A . Walenstein , M . Venable , M . Hayes , C . Thompson , and A . Lakhotia . Exploiting Similarity Between Variants to Defeat Malware : “ Vilo ” Method for Comparing and Searching Binary Programs . In Proc . of BLACKHAT DC , 2007 .
[ 7 ] S . Choi , H . Park , H . il Lim , and T . Han . A Static
[ 22 ] WEKA website .
Birthmark of Binary Executables Based on API Call Structure . In Proc . of ASIAN , 2007 .
[ 8 ] C . Cohen and J . Havrilla . Function Hashing for
Malicious Code Analysis , 2009 . wwwcertorg/research/2009research reportpdf http://wwwcswaikatoacnz/ml/weka
[ 23 ] Y . Ye , T . Li , Y . Chen , and Q . Jiang . Automatic malware categorization using cluster ensemble . In Proc . of KDD , 2010 .
23
