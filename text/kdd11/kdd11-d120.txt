Rank Aggregation via Nuclear Norm Minimization
David F . Gleich∗
Sandia National Laboratories†
Livermore , CA dfgleic@sandia.gov
Lek Heng Lim‡ University of Chicago
Chicago , IL lekheng@uchicago.edu
ABSTRACT The process of rank aggregation is intimately intertwined with the structure of skew symmetric matrices . We apply recent advances in the theory and algorithms of matrix completion to skew symmetric matrices . This combination of ideas produces a new method for ranking a set of items . The essence of our idea is that a rank aggregation describes a partially filled skew symmetric matrix . We extend an algorithm for matrix completion to handle skew symmetric data and use that to extract ranks for each item . Our algorithm applies to both pairwise comparison and rating data . Because it is based on matrix completion , it is robust to both noise and incomplete data . We show a formal recovery result for the noiseless case and present a detailed study of the algorithm on synthetic data and Netflix ratings .
Categories and Subject Descriptors H35 [ Information Storage and Retrieval ] : On line information Services—Web based services ; G13 [ Numerical analysis ] : Numerical Linear Algebra—Singular value decomposition
General Terms Algorithms
Keywords nuclear norm , skew symmetric , rank aggregation
1 .
INTRODUCTION
One of the classic data mining problems is to identify the important items in a data set ; see Tan and Jin [ 2004 ] for ∗Supported by the Natural Sciences and Engineering Research Council of Canada and the Dept . of Energy ’s John von Neumann fellowship . †Sandia National Laboratories is a multi program laboratory managed and operated by Sandia Corporation , a wholly owned subsidiary of Lockheed Martin Corporation , for the US Department of Energy ’s National Nuclear Security Administration under contract DE AC04 94AL85000 . ‡Supported by NSF CAREER Award DMS 1057064
Copyright 2011 Association for Computing Machinery . ACM acknowledges that this contribution was authored or co authored by an employee , contractor or affiliate of the US Government . As such , the Government retains a nonexclusive , royalty free right to publish or reproduce this article , or to allow others to do so , for Government purposes only . KDD’11 , August 21–24 , 2011 , San Diego , California , USA . Copyright 2011 ACM 978 1 4503 0813 7/11/08 $1000 an interesting example of how these might be used . For this task , we are concerned with rank aggregation . Given a series of votes on a set of items by a group of voters , rank aggregation is the process of permuting the set of items so that the first element is the best choice in the set , the second element is the next best choice , and so on . In fact , rank aggregation is an old problem and has a history stretching back centuries [ Condorcet , 1785 ] ; one famous result is that any rank aggregation requires some degree of compromise [ Arrow , 1950 ] . Our point in this introduction is not to detail a history of all the possible methods of rank aggregation , but to give some perspective on our approach to the problem .
Direct approaches involve finding a permutation explicitly – for example , computing the Kemeny optimal ranking [ Kemeny , 1959 ] or the minimum feedback arc set problem . These problems are NP hard [ Dwork et al . , 2001 , Ailon et al . , 2005 , Alon , 2006 ] . An alternate approach is to assign a score to each item , and then compute a permutation based on ordering these items by their score , eg Saaty [ 1987 ] . In this manuscript , we focus on the second approach . A key advantage of the computations we propose is that they are convex problems and efficiently solvable .
While the problem of rank aggregation is old , modern applications – such as those found in web applications like Netflix and Amazon – pose new challenges . First , the data collected are usually cardinal measurements on the quality of each item , such as 1–5 stars , received from voters . Second , the voters are neither experts in the rating domain nor experts at producing useful ratings . These properties manifest themselves in a few ways , including skewed and indiscriminate voting behaviors [ Ho and Quinn , 2008 ] . We focus on using aggregate pairwise data about items to develop a score for each item that predicts the pairwise data itself . This approach eliminates some of the issues with directly utilizing voters ratings , and we argue this point more precisely in Section 2 .
To explain our method , consider a set of n items , labeled from 1 to n . Suppose that each of these items has an unknown intrinsic quality si : 1 ≤ i ≤ n , where si > sj implies that item i is better than item j . While the si ’s are unknown , suppose we are given a matrix Y where Yij = si − sj . By finding a rank 2 factorization of Y ( there is no rank 1 skewsymmetric factorization ) , for example Y = seT − esT ,
( 1 ) we can extract unknown scores . The matrix Y is skewsymmetric and describes any score based global pairwise ranking . ( There are other possible rank 2 factorizations
60 Table 1 : Notation for the paper . a linear map from a matrix to a vector a vector of all ones a vector with 1 in the ith entry , 0 elsewhere the nuclear norm
Sym . Interpretation A(· ) e ei ·∗ R a rating matrix ( voters by items ) Y ˆY Ω a fitted or model pairwise comparison matrix a measured pairwise comparison matrix an index set for the known entries of a matrix of a skew symmetric matrix , a point we return to later in Section 31 )
Thus , given a measured ˆY , the goal is to find a minimum rank approximation of ˆY that models the elements , and ideally one that is rank 2 . Phrased in this way , it is a natural candidate for recent developments in the theory of matrix completion [ Cand`es and Tao , 2010 , Recht et al . , 2010 ] . In the matrix completion problem , certain elements of the matrix are presumed to be known . The goal is to produce a low rank matrix that respects these elements – or at least minimizes the deviation from the known elements . One catch , however , is that we require matrix completion over skew symmetric matrices for pairwise ranking matrices . Thus , we must solve the matrix completion problem inside a structured class of matrices . This task is a novel contribution of our work . Recently , Gross [ 2010 ] also developed a technique for matrix completion with Hermitian matrices . With a “ completed ” matrix Y , the norm of the residual ˆY − Y gives us a certificate for the validity of our fit – an additional piece of information available in this model .
To continue , we briefly summarize our main contributions and our notational conventions .
Our contributions . the space of skew symmetric matrices .
• We propose a new method for computing a rank aggregation based on matrix completion , which is tolerant to noise and incomplete data . • We solve a structured matrix completion problem over • We prove a recovery theorem detailing when our ap• We perform a detailed evaluation of our approach with synthetic data and an anecdotal study with Netflix ratings . proach will work .
Notation .
We try to follow standard notation conventions . Matrices are bold , upright roman letters , vectors are bold , lowercase roman letters , and scalars are unbolded roman or Greek letters . The vector e consists of all ones , and the vector ei has a 1 in the ith position and 0 ’s elsewhere . Linear maps on matrices are written as script letters . An index set Ω is a group of index pairs . Each ω ∈ Ω is a pair ( r , s ) and we assume that the ω ’s are numbered arbitrarily , ie Ω = {ω1 , . . . , ωk} . Please refer to Table 1 for reference .
Before proceeding further , let us outline the rest of the paper . First , Section 2 describes a few methods to take voter item ratings and produce an aggregate pairwise comparison matrix . Additionally , we argue why pairwise aggregation is a superior technique when the goal is to produce an ordered list of the alternatives . Next , in Section 3 , we describe formulations of the noisy matrix completion problem using the nuclear norm . In our setting , the lasso formulation is the best choice , and we use it throughout the remainder . We briefly describe algorithms for matrix completion and focus on the svp algorithm [ Jain et al . , 2010 ] in Section 31 We then show that the svp algorithm preserves skew symmetric structure . This process involves studying the singular value decomposition of skew symmetric matrices . Thus , by the end of the section , we’ve shown how to formulate and solve for a scoring vector based on the nuclear norm . The following sections describe alternative approaches and show our recovery results . At the end , we show our experimental results . In summary , our overall methodology is
Pairwise comparisons ( = Y )
Ratings ( = R )
⇓ ( §2 ) ⇓ ( §3 ) ⇓ ( sorting ) Rank aggregations .
Ranking scores ( = s )
An example of our rank aggregations is given in Table 2 . We comment further on these in Section 63
Finally , we provide our computational and experimental codes so that others may reproduce our results : https://dgleich.com/projects/skew nuclear
2 . PAIRWISE AGGREGATION METHODS To begin , we describe methods to aggregate the votes of many voters , given by the matrix R , into a measured pairwise comparison matrix ˆY . These methods have been well studied in statistics [ David , 1988 ] . In the next section , we show how to extract a score for each item from the matrix ˆY .
Let R be a voter by item matrix . This matrix has m rows corresponding to each of the m voters and n columns corresponding to the n items of the dataset . In all of the applications we explore , the matrix R is highly incomplete . That is , only a few items are rated by each voter . Usually all the items have a few votes , but there is no consistency in the number of ratings per item .
Instead of using R directly , we compute a pairwise aggregation . Pairwise comparisons have a lengthy history , dating back to the first half of the previous century [ Kendall and Smith , 1940 ] . They also have many nice properties . First , Miller [ 1956 ] observes that most people can evaluate only 5 to 9 alternatives at a time . This fact may relate to the common choice of a 5 star rating ( eg the ones used by Amazon , eBay , Netflix , YouTube ) . Thus , comparing pairs of movies is easier than ranking a set of 20 movies . Furthermore , only pairwise comparisons are possible in certain settings such as tennis tournaments . Pairwise comparison methods are thus natural for analyzing ranking data . Second , pairwise comparisons are a relative measure and help reduce bias from the rating scale . For these reasons , pairwise comparison methods have been popular in psychology , statistics , and social choice theory [ David , 1988 , Arrow , 1950 ] . Such methods have also been adopted by the learning to rank community ; see the contents of Li et al . [ 2008 ] . A final advantage of pairwise methods is
61 Table 2 : The top 15 movies from Netflix generated by our ranking method ( middle and right ) . The left list is the ranking using the mean rating of each movie and is emblematic of the problems global ranking methods face when infrequently compared items rocket to the top . We prefer the middle and right lists . See Section 6 and Figure 4 for information about the conditions and additional discussion . LOTR III appears twice because of the two DVDs editions , theatrical and extended .
Mean
LOTR III : Return . . . LOTR I : The Fellowship . . . LOTR II : The Two . . . Lost : Season 1 Battlestar Galactica : S1 Fullmetal Alchemist Trailer Park Boys : S4 Trailer Park Boys : S3 Tenchi Muyo! . . . Shawshank Redemption Veronica Mars : S1 Ghost in the Shell : S2 Arrested Development : S2 Simpsons : S6 Inu Yasha
Log odds ( all )
LOTR III : Return . . . LOTR I : The Fellowship . . . LOTR II : The Two . . . Star Wars V : Empire . . . Raiders of the Lost Ark Star Wars IV : A New Hope Shawshank Redemption Star Wars VI : Return LOTR III : Return . . . The Godfather Toy Story Lost : S1 Schindler ’s List Finding Nemo CSI : S4
Arithmetic Mean ( 30 )
LOTR III : Return . . . LOTR I : The Fellowship . . . LOTR II : The Two . . . Lost : S1 Star Wars V : Empire . . . Battlestar Galactica : S1 Star Wars IV : A New Hope LOTR III : Return . . . Raiders of the Lost Ark The Godfather Shawshank Redemption Star Wars VI : Return Gladiator Simpsons : S5 Schindler ’s List that they are much more complete than the ratings matrix . For Netflix , R is 99 % incomplete , whereas Y is only 0.22 % incomplete and most entries are supported by many comparisons . See Figure 1 for information about the number of pairwise comparisons in Netflix and MovieLens .
More critically , an incomplete array of user by product ratings is a strange matrix – not every 2 dimensional array of numbers is best viewed as a matrix – and using the rank of this matrix ( or its convex relaxation ) as a key feature in the modeling needs to be done with care . Consider , if instead of rating values 1 to 5 , 0 to 4 are used to represent the exact same information , the rank of this new rating matrix will change . Furthermore , whether we use a rating scale where 1 is the best rating and 5 is worst , or one where 5 is the best and 1 is the worst , a low rank model would give the exact same fit with the same input values , even though the connotations of the numbers is reversed .
On the other hand , the pairwise ranking matrix that we construct below is invariant under monotone transformation of the rating values and depends only on the degree of relative preference of one alternative over another . It circumvents the previously mentioned pitfalls and is a more principled way to employ a rank/nuclear norm model .
We now describe five techniques to build an aggregate pairwise matrix ˆY from the rating matrix R . Let α denote the index of a voter , and i and j the indices of two items . The entries of R are Rαi . To each voter , we associate a pairwise comparison matrix ˆY . The aggregation is usually computed by something like a mean over ˆY
α
α
.
1 . Arithmetic mean of score differences The score ij = Rαj − Rαi . The arithmetic mean of difference is Y α all voters who have rated both i and j is
α(Rαi − Rαj )
ˆYij =
#{α | Rαi , Rαj exist} .
These comparisons are translation invariant .
2 . Geometric mean of score ratios Assuming R > 0 , the score ratio refers to Y α ij = Rαj/Rαi . The ( log ) geometric mean over all voters who have rated both i and j is
ˆYij =
α(log Rαi − log Rαj ) #{α | Rαi , Rαj exist} .
These are scale invariant .
3 . Binary comparison Here Y α ij = sign(Rαj − Rαi ) . Its average is the probability difference that the alternative j is preferred to i than vice versa
ˆYij = Pr{α | Rαi > Rαk} − Pr{α | Rαi < Rαj} . These are invariant to a monotone transformation .
4 . Strict binary comparison This method is almost the same as the last method , except that we eliminate cases where users rated movies equally . That is ,
1
ˆY α ij =
Rαi > Rαj − Rαi = Rαj −1 Rαi < Rαj .
Again , the average Yij has a similar interpretation to binary comparison , but only among people who expressed a strict preference for one item over the other . Equal ratings are ignored .
5 . Logarithmic odds ratio This idea translates binary comparison to a logarithmic scale :
ˆYij = log
Pr{α | Rαi ≥ Rαj} Pr{α | Rαi ≤ Rαj} .
3 . RANK AGGREGATION WITH THE
NUCLEAR NORM
Thus far , we have seen how to compute an aggregate pairwise matrix ˆY from ratings data . While ˆY has fewer missing entries than R – roughly 1 80 % missing instead of almost 99 % missing – it is still not nearly complete . In this section , we discuss how to use the theory of matrix completion to estimate the scoring vector underlying the comparison matrix ˆY . These same techniques apply even when ˆY is
62 ( a ) MovieLens 85.49 % of total pairwise comparisons
( b ) Netflix 99.77 % of total pairwise comparisons
Figure 1 : A histogram of the number of pairwise comparisons between movies in MovieLens ( left ) and Netflix ( right ) . The number of pairwise comparisons is the number of users with ratings on both movies . These histograms show that most items have more than a small number of comparisons between them . For example , 18.5 % and 34.67 % of all possible pairwise entries have more than 30 comparisons between them . Largely speaking , this figure justifies dropping infrequent ratings from the comparison . This step allows us to take advantage of the ability of the matrix completion methods to deal with incomplete data . not computed from ratings and is measured through direct pairwise comparisons .
Let us now state the matrix completion problem formally [ Cand`es and Recht , 2009 , Recht et al . , 2010 ] . Given a matrix A where only a subset of the entries are known , the goal is to find the lowest rank matrix X that agrees with A in all the non zeros . Let Ω be the index set corresponding to the known entries of A . Now define A(X ) as a linear map corresponding to the elements of Ω , ie A(X ) is a vector where the ith element is defined to be [ A(X)]i = Xωi ,
( 2 ) and where we interpret Xωi as the entry of the matrix X for the index pair ( r , s ) = ωi . Finally , let b = A(Y ) be the values of the specified entries of the matrix Y . This idea of matrix completion corresponds with the solution of minimize subject to A(X ) = b . rank(X )
( 3 )
Unfortunately , like the direct methods at permutation minimization , this approach is NP hard [ Vandenberghe and Boyd , 1996 ] .
To make the problem tractable , an increasingly well known technique is to replace the rank function with the nuclear norm [ Fazel , 2002 ] . For a matrix A , the nuclear norm is defined rank(A ) i=1
A∗ =
σi(A ) property . Because the nuclear norm is convex ,
X∗ minimize subject to A(X ) = b
( 5 ) is a convex relaxation of ( 3 ) analogous to how the 1 norm is a convex relaxation of the 0 norm . In ( 5 ) we have A(X ) = b , which is called a noiseless completion problem . Noisy completion problems only require A(X ) ≈ b . We present four possibilities inspired by similar approaches in compressed sensing . For the compressed sensing problem with noise : minimizex1 subject to Ax ≈ b there are four well known formulations : lasso [ Tibshirani , 1996 ] , qp [ Chen et al . , 1998 ] , ds [ Cand`es and Tao , 2007 ] and bpdn [ Fuchs , 2004 ] . For the noisy matrix completion problem , the same variations apply , but with the nuclear norm taking the place of the 1 norm : lasso
A(X ) − b2 minimize subject to X∗ ≤ τ ds
X∗ minimize subject to σmax(A∗(A(X ) − b ) ) ≤ µ qp
Mazumder et al . [ 2009 ]
( 4 ) minimize
A(X ) − b2
2 + λX∗ where σi(A ) is the ith singular value of A . The nuclear norm has a few other names : the Ky Fan n norm , the Schatten 1 norm , and the trace norm ( when applied to symmetric matrices ) , but we will just use the term nuclear norm here . It is a convex underestimator of the rank function on the unit spectral norm ball {A : σmax(A ) ≤ 1} , ie A∗ ≤ rank(A)σmax(A ) and is the largest convex function with this bpdn
Mazumder et al . [ 2009 ]
X∗ minimize subject to A(X ) − b2 ≤ σ Returning to rank aggregation , recall the perfect case for the matrix Y : there is an unknown quality si associated with each item i and Y = seT − esT . We now assume that
100101102103104105100101102103104105106107Number of Pairwise ComparisonsOccurences100101102103104105106100101102103104105106107Number of Pairwise ComparisonsOccurences63 the pairwise comparison matrix computed in the previous section approximates the true Y . Given such a ˆY , our goal is to complete it with a rank 2 matrix . Thus , our objective : norm ) is A = ( B − BT )/2 . These results have elementary proofs . We continue by characterizing the singular value decomposition of a skew symmetric matrix .
A(X ) − b2 minimize subject to X∗ ≤ 2 and X = −X T
( 6 ) where A(· ) corresponds to the filled entries of ˆY . We adopt the lasso formulation because we want rank(X ) = 2 , and X∗ underestimates rank as previously mentioned . This problem only differs from the standard matrix completion problem in one regard : the skew symmetric constraint . With a careful choice of solver , this additional constraint comes “ for free ” ( with a few technical caveats ) . It should also be possible to use the skew Lanczos process to exploit the skewsymmetry in the SVD computation . The problem remains convex because these new equality constraints are linear . 3.1 Algorithms
Algorithms for matrix completion seem to sprout like wildflowers in spring : Lee and Bresler [ 2009 ] , Cai et al . [ 2008 ] , Toh and Yun [ 2009 ] , Dai and Milenkovic [ 2009 ] , Keshavan and Oh [ 2009 ] , Mazumder et al . [ 2009 ] , Jain et al . [ 2010 ] . Each algorithm fills a slightly different niche , or improves a performance measure compared to its predecessors .
We first explored crafting our own solver by adapting projection and thresholding ideas used in these algorithms to the skew symmetrically constrained variant . However , we realized that many algorithms do not require any modification to solve the problem with the skew symmetric constraint . This result follows from properties of skew symmetric matrices we show below .
Thus , we use the svp algorithm by Jain et al . [ 2010 ] . For the matrix completion problem , they found their implementation outperformed many competitors . It is scalable and handles a lasso like objective for a fixed rank approximation . For completeness , we restate the svp procedure in Algorithm 1 .
Algorithm 1 Singular Value Projection [ Jain et al . , 2010 ] : Solve a matrix completion problem . We use the notation Ω(X ) to denote output of A(X ) when A(· ) is an index set . input index set Ω , target values b , target rank k , maximum rank k , step length η , tolerance ε to be the rank k SVD of a matrix
1 : Initialize X ( 0 ) = 0 , t = 0 2 : repeat 3 :
Set U ( t)Σ(t)V ( t)T with non zeros Ω and values Ω(X ( t ) ) − η(Ω(X ( t ) ) − b ) 4 : X ( t+1 ) ← U ( t)Σ(t)V ( t)T 5 : 6 : until Ω(X ( k ) ) − b2 > ε t ← t + 1
If the constraint A(X ) = b comes from a skew symmetric matrix , then this algorithm produces a skew symmetric matrix as well . Showing this involves a few properties of skewsymmetric matrices and two lemmas . We begin by stating a few well known properties of skewsymmetric matrices . Let A = −AT be skew symmetric . Then all the eigenvalues of A are pure imaginary and come in complex conjugate pairs . Thus , a skew symmetric matrix must always have even rank . Let B be a square real valued matrix , then the closest skew symmetric matrix to B ( in any
Lemma 1 . Let A = −AT be an n × n skew symmetric matrix with eigenvalues iλ1,−iλ1 , iλ2,−iλ2 , . . . , iλj,−iλj , where λi > 0 and j = n/2 . Then the SVD of A is given by
λ1

 V T
λ1
λ2
λ2
λj
λj
A = U
( 7 ) for U and V given in the proof .
Proof . Using the Murnaghan Wintner form of a real matrix [ Murnaghan and Wintner , 1931 ] , we can write
A = XT X T for a real valued orthogonal matrix X and real valued blockupper triangular matrix T , with 2 by 2 blocks along the diagonal . Due to this form , T must also be skew symmetric . Thus , it is a block diagonal matrix that we can permute to the form :
T =
0 λ2 −λ2 0
−λ1 0
 0 λ1 λ1 0 1
λ1 
1 0
=
λ1
λ2
λ2
0
λ1 0
 . −1 0 −1 0
0 1 −1 0 0 1
0
1
0 λ1
=V T

.
−λ1
0 0 1
1 0
=U
0 1 1 0
 X T
.
A = X
Note that the SVD of the matrix
We can use this expression to complete the theorem :
Both the matrices U and V are real and orthogonal . Thus , this form yields the SVD of A .
We now use this lemma to show that – under fairly general conditions – the best rank k approximation to a skewsymmetric matrix is also skew symmetric .
Lemma 2 . Let A be an n by n skew symmetric matrix , and let k = 2j be even . Let λ1 ≥ λ2 ≥ . . . ≥ λj > λj+1 be the magnitude of the singular value pairs . ( Recall that the previous lemma showed that the singular values come in pairs . ) Then the best rank k approximation of A in an orthogonally invariant norm is also skew symmetric .
Proof . This lemma follows fairly directly from Lemma 1 . Recall that the best rank k approximation of A in an orthogonally invariant norm is given by the k largest singular values and vectors . By assumption of the theorem , there is a gap in the spectrum between the kth and k+1 st singular value . Thus , taking the SVD form from Lemma 1 and truncating to the k largest singular values produces a skew symmetric matrix .
Finally , we can use this second result to show that the svp algorithm for the lasso problem preserves skew symmetry in all the iterates X ( k ) .
64 Algorithm 2 Nuclear Norm Rank Aggregation . The svp subroutine is given by Algorithm 1 . input ranking matrix R , minimum comparisons c 1 : Compute Y from R by a procedure in Section 2 . 2 : Discard entries in Y with fewer than c comparisons 3 : Let Ω be the index set for all retained entries in Y and b be the values for these entries
4 : U , S , V = svp(index set Ω , values b , rank 2 ) 5 : Compute s = ( 1/n)U SV T e
Theorem 3 . Given a set of skew symmetric constraints A(· ) = b , the solution of the lasso problem from the svp solver is a skew symmetric matrix X if the target rank is even and the dominant singular values stay separated as in the previous lemma .
Proof . In this proof , we revert to the notation A(X ) and use A∗(z ) to denote the matrix with non zeros in Ω and values from z . We proceed by induction on the iterates generated by the svp algorithm . Clearly X ( 0 ) is skew symmetric . In step 3 , we compute the SVD of a skew symmetric matrix : A∗(A(X ( k ) ) − b ) . The result , which is the next iterate , is skew symmetric based on the previous lemma and conditions of this theorem .
The svp solver thus solves ( 6 ) for a fixed rank problem . A final step is to extract the scoring vector s from a rank2 singular value decomposition . If we had the exact matrix Y , then ( 1/n)Y e = s − ( sT e)/ne , which yields the score vector centered around 0 . The outcome that a rank2 U ΣV T from svp is not of the form seT − esT is quite possible because there are many rank 2 skew symmetric matrices that do not have e as a factor . If we had a fullbut inconsistent pairwise comparison matrix Y , then using a Borda count s = ( 1/n)Y e provides the best leastsquares approximation to s [ Jiang et al . , 2010 ] . Formally , ( 1/n)Y e = argmins justifies using the scoring vector s = ( 1/n)U ΣV T e derived from this completed matrix . flflY T − ( seT − esT )flfl . This discussion
Our complete ranking procedure is given by Algorithm 2 .
4 . OTHER APPROACHES
Now , we briefly compare our approach with other tech niques to compute ranking vectors from pairwise comparison data . An obvious approach is to find the least squares solu(i,j)∈Ω(Yi,j − ( si − sj))2 . This is a linear least tion mins squares method , and is exactly what Massey [ 1997 ] proposed for ranking sports teams . The related Colley method introduces a bit of regularization into the least squares problem [ Colley , 2002 ] . By way of comparison , the matrix completion approach has the same ideal objective , however , we compute solutions using a two stage process : first complete the matrix , and then extract scores .
A related methodology with skew symmetric matrices underlies recent developments in the application of Hodge theory to rank aggregation [ Jiang et al . , 2010 ] . By analogy with the Hodge decomposition of a vector space , they propose a decomposition of pairwise rankings into consistent , globally inconsistent , and locally inconsistent pieces . Our approach differs because our algorithm applies without restriction on the comparisons . Freeman [ 1997 ] also uses an SVD of a skew symmetric matrix to discover a hierarchical structure in a social network .
We know of two algorithms to directly estimate the item value from ratings [ de Kerchov and van Dooren , 2007 , Ho and Quinn , 2008 ] . Both of these methods include a technique to model voter behavior . They find that skewed behaviors and inconsistencies in the ratings require these adjustments . In contrast , we eliminate these problems by using the pairwise comparison matrix . Approaches using a matrix or tensor factorization of the rating matrix directly often have to determine a rank empirically [ Rendle et al . , 2009 ] .
The problem with the mean rating from Netflix in Table 2 is often corrected by requiring a minimum number of rating on an item . For example , IMDB builds its top 250 movie list based on a Bayesian estimate of the mean with at least 3000 ratings ( imdbcom/chart/top ) Choosing this parameter is problematic as it directly excludes items . In contrast , choosing the minimum number of comparisons to support an entry in Y may be easier to justify .
5 . RECOVERABILITY
A hallmark of the recent developments on matrix completion is the existence of theoretical recoverability guarantees ( see Cand`es and Recht [ 2009 ] , for example ) . These guarantees give conditions under which the solution to the optimization problems posed in Section 3 is or is nearby the low rank matrix from whence the samples originated . In this section , we apply a recent theoretical insight into matrix completion based on operator bases to our problem of recovering a scoring vector from a skew symmetric matrix [ Gross , 2010 ] . We only treat the noiseless problem to present a simplified analysis . Also , the notation in this section differs slightly from the rest of the manuscript , in order to match the statements in Gross [ 2010 ] better . In particular , Ω is not necessarily the index set , ı represents the complex field .
√−1 , and most of the results are for
The goal is this section is to apply Theorem 3 from Gross [ 2010 ] to skew symmetric matrices arising from score difference vectors . We restate that theorem for reference .
Theorem 4
( Theorem 3 , Gross [ 2010] ) . Let A be a rank r Hermitian matrix with coherence ν with respect to an operator basis {W i}n2 i=1 . Let Ω ⊂ [ 1 , n2 ] be a random set of size |Ω| > O(nrν(1 + β)(log n)2 ) . Then the solution of
X∗ minimize subject to trace(X∗W i ) = trace(A∗W i ) i ∈ Ω is unique and is equal to A with probability at least 1 − n−3 .
The definition of coherence follows shortly . On the surface , this theorem is useless for our application . The matrix we wish to complete is not Hermitian , it ’s skew symmetric . However , given a real valued skew symmetric matrix Y , the matrix ıY is Hermitian ; and hence , we will work to apply this theorem to this particular Hermitian matrix .
The following theorem gives us a condition for recovering the score vector using matrix completion . As stated , this theorem is not particularly useful because s may be recovered from noiseless measurements by exploiting the special structure of the rank 2 matrix Y . For example , if we know Yi,j = si − sj then given si we can find sj . This argument may be repeated with an arbitrary starting point as long as the known index set corresponds to a connected set over the
65 indices . Instead we view the following theorem as providing intuition for the noisy problem .
Consider the operator basis for Hermitian matrices :
H = S ∪ K ∪ D where √ S = {1/ j + ejeT 2(eieT √ j − ejeT K = {ı/ 2(eieT : 1 ≤ i ≤ n} . D = {eieT i i ) : 1 ≤ i < j ≤ n} ; i ) : 1 ≤ i < j ≤ n} ;
Theorem 5 . Let s be centered , ie , sT e = 0 . Let Y = seT − esT where θ = maxi s2 i /(sT s ) and ρ = ( (maxi si ) − ( mini si))/s . Also , let Ω ⊂ H be a random set of elements with size |Ω| ≥ O(2nν(1 + β)(log n)2 ) where ν = max((nθ + 1)/4 , nρ2 ) . Then the solution of
X∗ minimize subject to trace(X∗W i ) = trace((ıY )∗W i ) , W i ∈ Ω is equal to ıY with probability at least 1 − n−β .
The proof of this theorem follows directly by Theorem 4 if ıY has coherence ν with respect to the basis H . We now show this result .
Definition 6
( Coherence , Gross [ 2010] ) . Let A be n × n , rank r , and Hermitian . Let U U∗ be an orthogonal projector onto range(A ) . Then A has coherence ν with respect to an operator basis {W i}n2 i=1 if both maxi trace(W iU U maxi trace(sign(A)W i)2 ≤ νr/n2 .
∗
W i ) ≤ 2νr/n , and
For A = ıY with sT e = 0 :
∗
=
U U ssT sT s
− 1 n eeT and sign(A ) =
1 s √ n
A .
Let Sp ∈ S , K p ∈ K , and Dp ∈ D . Note that because sign(A ) is Hermitian with no real valued entries , both quantities trace(sign(A)Di)2 and trace(sign(A)Si)2 are 0 . Also , because U U∗ is symmetric , trace(K iU U∗K p ) = 0 . The remaining basis elements satisfy : trace(SpU U
∗
Sp ) = trace(DpU U
∗
Dp ) = trace(sign(A)K p)2 =
+ s2 i + s2 j 2sT s s2 i sT s
1 n 1 n 2(si − sj)2
+ nsT s
≤ ( 1/n ) + θ
≤ ( 1/n ) + θ
Figure 2 : An experimental study of the recoverability of a ranking vector . These show that we need about 6n log n entries of Y to get good recovery in both the noiseless ( left ) and noisy ( right ) case . See §6.1 for more information . 6.1 Recovery
The first experiment is an empirical study of the recoverability of the score vector in the noiseless and noisy case . In the noiseless case , Figure 2 ( left ) , we generate a score vector with uniformly distributed random scores between 0 and 1 . These are used to construct a pairwise comparison matrix Y = seT − esT . We then sample elements of this matrix uniformly at random and compute the difference between the true score vector s and the output of steps 4 and 5 of Algorithm 2 . If the relative 2 norm difference between these vectors is less than 10−3 , we declare the trial recovered . For n = 100 , the figure shows that , once the number of samples is about 6n log n , the correct s is recovered in nearly all the 50 trials . Next , for the noisy case , we generate a uniformly spaced score vector between 0 and 1 . Then Y = seT − esT + εE , where E is a matrix of random normals . Again , we sample elements of this matrix randomly , and declare a trial successful if the order of the recovered score vector is identical to the true order . In Figure 2 ( right ) , we indicate the fractional of successful trials as a gray value between black ( all failure ) and white ( all successful ) . Again , the algorithm is successful for a moderate noise level , ie , the value of ε , when the number of samples is larger than 6n log n . 6.2 Synthetic
Inspired by Ho and Quinn [ 2008 ] , we investigate recovering item scores in an item response scenario . Let ai be the center of user i ’s rating scale , and bi be the rating sensitivity of user i . Let ti be the intrinsic score of item j . Then we generate ratings from users on items as :
Ri,j = L[ai + bitj + Ei,j ]
≤ ( 2/n)ρ2 . where L[α ] is the discrete levels function :
Thus , A has coherence ν with ν from Theorem 5 and with respect to H . And we have our recovery result . Although , this theorem provides little practical benefit unless both θ and ρ are O(1/n ) , which occurs when s is nearly uniform .
6 . RESULTS
We implemented and tested this procedure in two synthetic scenarios , along with Netflix , movielens , and Jester joke set ratings data . In the interest of space , we only present a subset of these results for Netflix .
L[α ] = max(min(round(α ) , 5 ) , 1 ) and Ei,j is a noise parameter . In our experiment , we draw ai ∼ N ( 3 , 1 ) , bi ∼ N ( 0.5 , 0.5 ) , ti ∼ N ( 0.1 , 1 ) , and Ei,j ∼ εN ( 0 , 1 ) . Here , N ( µ , σ ) is a standard normal , and ε is a noise parameter . As input to our algorithm , we sample ratings uniformly at random by specifying a desired number of average ratings per user . We then look at the Kendall τ correlation coefficient between the true scores ti and the output of our algorithm using the arithmetic mean pairwise aggregation . A τ value of 1 indicates a perfect ordering correlation between the two sets of scores .
1021031040020406081Fraction of trials recoveredSamples5n2n log(n)6n log(n)1021031040020406081Fraction of trials recoveredSamples200100050000001002003004005Noise levelSamples5n2n log(n)6n log(n)66 Figure 3 : The performance of our algorithm ( left ) and the mean rating ( right ) to recovery the ordering given by item scores in an item response theory model with 100 items and 1000 users . The various thick lines correspond to average number of ratings each user performed ( see the in place legend ) . See §6.2 for more information
Figure 3 shows the results for 1000 users and 100 items with 1.1 , 1.5 , 2 , 5 , and 10 ratings per user on average . We also vary the parameter ε between 0 and 1 . Each thick line with markers plots the median value of τ in 50 trials . The thin adjacency lines show the 25th and 75th percentiles of the 50 trials . At all error levels , our algorithm outperforms the mean rating . Also , when there are few ratings per user and moderate noise , our approach is considerably more correlated with the true score . This evidence supports the anecdotal results from Netflix in Table 2 .
6.3 Netflix
See Table 2 for the top movies produced by our technique in a few circumstances using all users . The arithmetic mean results in that table use only elements of Y with at least 30 pairwise comparisons ( it is a am all 30 model in the code below ) . And see Figure 4 for an analysis of the residuals generated by the fit for different constructions of the matrix ˆY . Each residual evaluation of Netflix is described by a code . For example , sb all 0 is a strict binary pairwise matrix ˆY from all Netflix users and c = 0 in Algorithm 2 ( ie accept all pairwise comparisons ) . Alternatively , am 6 30 denotes an arithmetic mean pairwise matrix ˆY from Netflix users with at least 6 ratings , where each entry in ˆY had 30 users supporting it . The other abbreviations are gm : geometric mean ; bc : binary comparison ; and lo : log odds ratio .
These residuals show that we get better rating fits by only using frequently compared movies , but that there are only minor changes in the fits when excluding users that rate few movies . The difference between the score based residu alsflflΩ(seT − esT ) − bflfl ( red points ) and the svp residuals flflΩ(U SV T ) − bflfl ( blue points ) show that excluding compar isons leads to “ overfitting ” in the svp residual . This suggests that increasing the parameter c should be done with care and good checks on the residual norms .
To check that a rank 2 approximation is reasonable , we increased the target rank in the svp solver to 4 to investigate . For the arithmetic mean ( 6,30 ) model , the relative residual at rank 2 is 0.2838 and at rank 4 is 02514 Meanwhile , the nuclear norm increases from around 14000 to around 17000 . These results show that the change in the fit is minimal and our rank 2 approximation and its scores should represent a reasonable ranking .
Figure 4 : The labels on each residual show how we generated the pairwise scores and truncated the Netflix data . Red points are the residuals from the scores , and blue points are the final residuals from the SVP algorithm . Please see the discussion in §63 7 . CONCLUSION
Existing principled techniques such as computing a Kemeny optimal ranking or finding a minimize feedback arc set are NP hard . These approaches are inappropriate in large scale rank aggregation settings . Our proposal is ( i ) measure pairwise scores ˆY and ( ii ) solve a matrix completion problem to determine the quality of items . This idea is both principled and functional with significant missing data . The results of our rank aggregation on the Netflix problem ( Table 2 ) reveal popular and high quality movies . These are interesting results and could easily have a home on a “ best movies in Netflix ” web page . Such a page exists , but is regarded as having strange results . Computing a rank aggregation with this technique is not NP hard . It only requires solving a convex optimization problem with a unique global minima . Although we did not record computation times , the most time consuming piece of work is computing the pairwise comparison matrix Y . In a practical setting , this could easily be done with a MapReduce computation .
To compute these solutions , we adapted the svp solver for matrix completion [ Jain et al . , 2010 ] . This process involved ( i ) studying the singular value decomposition of a skewsymmetric matrix ( Lemmas 1 and 2 ) and ( ii ) showing that the svp solver preserves a skew symmetric approximation through its computation ( Theorem 3 ) . Because the svp solver computes with an explicitly chosen rank , these techniques work well for large scale rank aggregation problems .
We believe the combination of pairwise aggregation and matrix completion is a fruitful direction for future research . We plan to explore optimizing the svp algorithm to exploit the skew symmetric constraint , extending our recovery result to the noisy case , and investigating additional data .
Acknowledgements . The authors would like to thank Amy Langville ,
Carl Meyer , and Yuan Yao for helpful discussions .
002040608105060708091ErrorMedian Kendall ’s Tau 20 10 5 215002040608105060708091ErrorMedian Kendall ’s Tau 002040608105060708091ErrorMedian Kendall ’s Tau002040608105060708091ErrorMedian Kendall ’s Tau020304050607am all 30am 6 30gm 6 30gm all 30am all 100am 6 100sb 6 30sb all 30gm all 100gm 6 100bc 6 30bc all 30bc 6 100bc all 100lo all 30lo 6 30lo 6 100lo all 100sb all 100sb 6 100am 6 0am all 0bc 6 0bc all 0lo 6 0lo all 0gm 6 0gm all 0sb 6 0sb all 0Relative Residual67 References N . Ailon , M . Charikar , and A . Newman . Aggregating inconsistent information : ranking and clustering . In STOC ’05 , pages 684–693 , 2005 . doi : 101145/10605901060692
N . Alon . Ranking tournaments . SIAM J . Discret . Math . , 20 ( 1):137–142 , 2006 . doi : 101137/050623905
K . J . Arrow . A difficulty in the concept of social welfare . J . Polit . Econ . , 58(4):328–346 , 1950 . http://jstor.org/ stable/1828886 .
J F Cai , E . J . Cand`es , and Z . Shen . A singular value thresholding algorithm for matrix completion . arXiv , mathOC:08103286v1 , 2008 . http://arxiv.org/abs/ 08103286
E . Cand`es and T . Tao . The Dantzig selector : Statistical estimation when p is much larger than n . Ann . Stat . , 35(6 ) : 2313–2351 , 2007 . doi : 101214/009053606000001523
E . J . Cand`es and B . Recht . Exact matrix completion via convex optimization . Found . Comput . Math . , 9(6):717–772 , 2009 . doi : 101007/s10208 009 9045 5
E . J . Cand`es and T . Tao . The power of convex relaxation : Near optimal matrix completion . IEEE Trans . Inform . Theory , 56(5):2053–2080 , 2010 . doi : 101109/TIT2010 2044061 .
S . S . Chen , D . L . Donoho , and M . A . Saunders . Atomic decomposition by basis pursuit . SIAM J . Sci . Comp . , 20(1 ) : 33–61 , 1998 . doi : 101137/S1064827596304010
W . N . Colley . Colley ’s bias free college football ranking method : The Colley matrix explained . Technical report , Princeton University , 2002 .
J A N d . C . Condorcet . Essai sur l’application de l’analyse `a la probabilit´e des d´ecisions de L’imprimerie Royale , Paris , 1785 . http://gallica2bnffr/ark : /12148/bpt6k417181 .
W . Dai and O . Milenkovic . Set : an algorithm for consistent matrix completion . arXiv , 2009 . http://arxiv.org/abs/ 09092705
H . A . David . The method of paired comparisons . Number 41 in Griffin ’s Statistical Monographs and Courses . Charles Griffin , 1988 . ISBN 0195206169 .
C . de Kerchov and P . van Dooren . Iterative filtering for a dynamical reputation system . arXiv , csIR:07113964 , 2007 . http://arXivorg/abs/07113964
C . Dwork , R . Kumar , M . Naor , and D . Sivakumar . Rank aggregation methods for the web . In WWW ’01 , pages 613– 622 , New York , NY , USA , 2001 . ACM . ISBN 1 58113 348 0 . doi : 101145/371920372165
M . Fazel . Matrix rank minimization with applications . PhD thesis , Stanford University , 2002 . http://faculty . washingtonedu/mfazel/thesis finalpdf
D . E . Ho and K . M . Quinn . Improving the presentation and interpretation of online ratings data with model based figures . Amer . Statist . , 62(4):279–288 , 2008 . doi : 10.1198/ 000313008X366145 .
P . Jain , R . Meka , and I . Dhillon . Guaranteed rank minimization via singular value projection . In NIPS 23 , pages 937–945 , 2010 . http://booksnipscc/papers/files/ nips23/NIPS2010_0682pdf
X . Jiang , L H Lim , Y . Yao , and Y . Ye . Statistical ranking and combinatorial hodge theory . Mathematical Programming , 127(1):1–42 , 2010 . doi : 101007/s10107 010 0419 x
J . G . Kemeny . Mathematics without numbers . Daedalus , 88 ( 4):577–591 , 1959 . http://jstororg/stable/20026529
M . G . Kendall and B . B . Smith . On the method of paired comparison . Biometrika , 31(3 4):324–345 , 1940 . doi : 10 . 1093/biomet/313 4324
R . H . Keshavan and S . Oh . A gradient descent algorithm on the grassman manifold for matrix completion . arXiv , 2009 . http://arxivorg/abs/09105260
K . Lee and Y . Bresler . Admira : Atomic decomposition for minimum rank approximation . arXiv , 2009 . http : //arxivorg/abs/09050044
H . Li , T Y Liu , and C . Zhai , editors . SIGIR ’08 Workshop : Learning to Rank for Information Retrieval . 2008 . http://researchmicrosoftcom/en us/um/beijing/ events/lr4ir 2008/PROCEEDINGS LR4IR%202008PDF
K . Massey . Statistical models applied to the rating of sports teams . Master ’s thesis , Bluefield College , 1997 .
R . Mazumder , T . Hastie , and R . Tibshirani . Regularization methods for learning incomplete matrices . arXiv , 2009 . http://arxivorg/abs/09062034v1
G . A . Miller . The magical number seven , plus or minus two : Some limits on our capacity for processing information . Psychol . Rev . , 101(2):343–352 , 1956 . doi : 101037/h0043158
F . D . Murnaghan and A . Wintner . A canonical form for real matrices under orthogonal transformations . PNAS , 17 ( 7):417–420 , 1931 . http://wwwpnasorg/content/17/7/ 417fullpdf+html
B . Recht , M . Fazel , and P . A . Parrilo . Guaranteed minimum rank solutions of linear matrix equations via nuclear norm minimization . SIAM Review , 52(3):471–501 , 2010 . doi : 101137/070697835
S . Rendle , L . Balby Marinho , A . Nanopoulos , and L . Schmidt Thieme . Learning optimal ranking with tensor factorization for tag recommendation . In KDD ’09 , pages 727–736 . ACM , 2009 . doi : 101145/15570191557100
T . L . Saaty . Rank according to Perron : A new insight . Math . Mag , 60(4):211–213 , 1987 . http://jstor.org/ stable/2689340 .
P N Tan and R . Jin . Ordering patterns by combining opinions from multiple sources . In KDD ’04 , pages 695–700 . ACM , 2004 . doi : 101145/10140521014142
L . C . Freeman . Uncovering organizational hierarchies . Computational and Mathematical Organization Theory , 3 ( 1):5–18 , 1997 . doi : 101023/A:1009690520577
R . Tibshirani . Regression shrinkage and selection via the lasso . J . R . Stat . Soc . Ser . B Stat . Methodol . , 58(1):267– 288 , 1996 . http://wwwjstororg/stable/2346178
J J Fuchs . Recovery of exact sparse representations in the presence of noise . In ICASSP ’04 , volume 2 , pages ii–533–6 vol.2 , 2004 . doi : 101109/ICASSP20041326312
D . Gross . Recovering low rank matrices from few coefficients in any basis . arXiv , csNA:09101879v5 , 2010 . http://arxivorg/abs/09101879
K C Toh and S . Yun . An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems . Opt . Online , 2009 . http://www . optimization onlineorg/DB_FILE/2009/03/2268pdf
L . Vandenberghe and S . Boyd . Semidefinite programming . SIAM Rev . , 38(1):49–95 , 1996 . doi : 101137/1038003
68
