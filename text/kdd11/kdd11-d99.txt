Multi View Transfer Learning with a Large Margin Approach
Dan Zhang
Computer Science
Department
Purdue University West Lafayette , IN zhang168@cspurdueedu
Jingrui He
Machine Learning Group IBM TJ Watson Research
Center
Yorktown Heights , NY jingruhe@usibmcom
Yan Liu
Computer Science
Department
University of Southern
California
Los Angeles , CA yanliucs@uscedu
Luo Si
Computer Science
Department
Purdue University West Lafayette , IN lsi@cspurdueedu
ABSTRACT Transfer learning has been proposed to address the problem of scarcity of labeled data in the target domain by leveraging the data from the source domain . In many real world applications , data is often represented from different perspectives , which correspond to multiple views . For example , a web page can be described by its contents and its associated links . However , most existing transfer learning methods fail to capture the multi view nature , and might not be best suited for such applications .
To better leverage both the labeled data from the source domain and the features from different views , this paper proposes a general framework : Multi View Transfer Learning with a Large Margin Approach ( MVTL LM ) . On one hand , labeled data from the source domain is effectively utilized to construct a large margin classifier ; on the other hand , the data from both domains is employed to impose consistencies among multiple views . As an instantiation of this framework , we propose an efficient optimization method , which is guaranteed to converge to precision in O(1/ ) steps . Furthermore , we analyze its error bound , which improves over existing results of related methods . An extensive set of experiments are conducted to demonstrate the advantages of our proposed method over state of the art techniques .
Categories and Subject Descriptors I26 [ Artificial Intelligence ] : Learning—Knowledge acquisition
General Terms Algorithms , Performance , Experimentation
Richard D . Lawrence Machine Learning Group IBM TJ Watson Research
Center
Yorktown Heights , NY ricklawr@usibmcom
Keywords Multi View Learning , Transfer Learning , Large Margin Approach
1 .
INTRODUCTION
Transfer learning has been proposed and commonly used to address the problem of scarcity of labeled data in a particular target domain . It builds a model for the target domain by leveraging the label information from another related domain ( source domain ) , thus avoids the costly process of generating labels for target domain examples . In many real world applications , the examples are often described from different perspectives , which correspond to multiple views . For example , in web mining , a web page can be represented by its contents and in bound/out bound links ; in image analysis , an image can be described by different types of features , such as , color , texture , and shape . It has been shown that leveraging the consistencies between different views can help improve the learning performance . However , most existing transfer learning methods are designed only for single view problems . In other words , if applied to problems with multiple views , these methods would fail to utilize the redundancy incurred by the multi view property . Therefore , they are not ideal for such applications .
Despite its importance , the problem of Multi View Transfer Learn ing ( MVTL ) has received limited attention . Most existing methods integrate the multi view and transfer learning nature by some heuristics without theoretical analysis , such as the convergence rate of their algorithms , and how the learned model for the target domain can be improved by integrating the characteristics of multiview features into transfer learning . And researchers tend to put more emphasis on the multi view side . For example , in [ 40 ] , the proposed algorithm uses the classifier trained on the source domain to generate the initial seed set , and then applies the co training [ 4 ] algorithm to construct the classifier for the target domain ; in [ 13 ] , the authors explicitly model the view consistency in their objective function without considering the data distribution difference between the source domain and the target domain . Another straightforward method for MVTL is to concatenate the features from multiple views , and apply the transfer learning methods for single view problems to build the model for the target domain . However , this kind of methods disregard the redundancy incurred by different
1208 views . As shown in a lot of previous works [ 37 , 44 , 47 ] , without considering the consistencies between different views , the performance of multi view learning cannot be guaranteed . On the contrary , in MVTL , we utilize the nature of both the transfer learning and the multi view learning in an unified way .
To achieve this goal , this paper proposes a general framework : Multi View Transfer Learning with a Large Margin Approach ( MVTL LM ) . In particular , from the transfer learning perspective , we integrate the nature of the multi view setting into the transfer learning framework and impose the consistencies among multiple views , which implicitly limits the capacity of the hypothesis class . This is significantly different from previous large margin transfer learning methods [ 29 ] , which do not consider the problem of multi view setting . From multi view learning perspective , the new formulation explicitly models the data distribution difference between the source domain and the target domain , applies the technique of importance sampling [ 1 ] , and uses weighted labeled data from the source domain to construct a large margin classifier for the target domain . In particular , we propose a novel optimization method based on the bundle method [ 38 , 39 ] , which can solve an instantiation of the MVTL LM framework in a very efficient way . We further prove that the optimization method can converge to pre ) steps ( See Theorem 3.1 ) and each step takes time cision in O( 1 O(sn ) , where s is the average sparsity for features and n is the total number of source domain examples ( See Theorem 32 ) Moreover , we analyze the generalized error bound of MVTL LM in the target domain , which depends on its performance in the source domain and the empirical Rademacher complexity of a related hypothesis class ( See Theorem 33 ) The empirical Rademacher complexity is reduced by making use of the multi view nature . Experimental results demonstrate the advantages of our proposed method over state of the art techniques .
The rest of this paper is organized as follows . The related works are discussed in Section 2 . Section 3 introduces the research problem , presents the proposed algorithm and analyzes some properties of the proposed method . Section 4 presents the experimental results . Section 5 concludes the whole paper .
2 . RELATED WORKS
This section briefly introduces related works on transfer learning , multi view learning , MVTL and the bundle method . 2.1 Transfer Learning
As an important technique to address the problem of scarcity of labeled data in the target domain , transfer learning has received much attention recently . A key problem in transfer learning is what kind of knowledge can be transferred from the source domain to the target domain . Roughly speaking , the assumptions introduced in previous transfer learning work can be grouped into four categories : 1 . Feature Representation Transfer . In [ 2 , 7 , 9 , 11 , 28 , 30 ] , the authors assume that there exists a common feature space shared by both the source domain and the target domain , which can be used as a bridge to transfer knowledge .
2 . Parameter Transfer .
In [ 5 , 19 ] , the authors make use of Gaussian Process ( GP ) models , and assume that the source domain and the target domain have shared parameters / hyperparameters .
3 . Instance Transfer . Due to the data distribution difference between the source domain and the target domain , in [ 8 , 42 ] , the authors select or re weight the examples from the source domain for use in the target domain such that the expectation of the adjusted loss function on the source domain examples can be as close to the expectation of the loss function on the target domain examples as possible . In [ 23 , 24 ] , the authors utilize the data distribution differences between several source domains and the target domain , and propose a method to combine the classifiers from these source domains optimally .
4 . Relation Transfer .
In [ 10 , 25 , 26 ] , the authors build the relational map between the source and the target domains , and relax the independently and identically distributed ( ie , iid ) assumption in these two domains .
Despite their success on single view problems , existing transfer learning methods may not work well on MVTL problems since the nature of multiple views are not considered in these works . In contrast , besides transferring knowledge from the source domain to the target domain via instance transfer , our proposed MVTL LM approach imposes the consistencies between multiple views by explicitly modeling the differences between the outputs from different views , which promises to improve the performance . 2.2 Multi View Learning
In many real world applications , examples are represented by multiple views . It has been shown extensively in prior research that leveraging the redundancy among the multiple views can improve the learning performance [ 12 , 17 , 20 , 31 , 36 , 44 , 45 ] . For example , the authors of [ 12 ] construct a classifier on each view and regulate the consistencies between different views . Furthermore , they show that the Rademacher complexity of the function class can also be greatly reduced by regulating the consistencies .
This idea is further exploited in [ 20 ] , where the authors incorporate the consistency term into multi view semi supervised learning problems , and show a substantial improvement on the classification performance . Similarly , in [ 44 ] , the authors incorporate this idea into local learning [ 41 ] and propose a novel way to define the graph Laplacian . Most existing multi view learning methods are for the single domain settings . However , in MVTL problems , the source domain and the target domain do not have the same data distribution . As we will show in the experiments , disregarding the data distribution difference may adversely affect the classification performance . In contrast , besides leveraging the consistency between different views , our proposed MVTL LM approach also considers the domain difference by an effective re weighting scheme , so it can achieve better performance in MVTL problems . 2.3 Multi View Transfer Learning
As mentioned in introduction , existing methods for multi view transfer learning combine the multi view learning and transfer learning by some heuristics , and tend to put more emphasis on the multiview side . For example , the co adaptation algorithm proposed in [ 40 ] uses the labeled examples from the source domain to construct classifiers , which will be used to generate the initial seed set . It then applies the co training algorithm [ 4 ] to construct the classifier for the target domain . Note that in the latter stage , labeled examples from the source domain are not utilized , which may otherwise provide useful insights about consistency between multiple views and the optimal classifier . In [ 13 ] , the co regularized loss function consists of the standard regularized log likelihood on multiple views based on the labeled data , as well as the expected Bhattacharyya distance based on the unlabeled data . When applied in MVTL problems , it may fail to capture the data distribution difference between the source domain and the target domain . In contrast , our proposed MVTL LM approach integrates the multi view
1209 and transfer learning nature in a principled way , and is tailored for MVTL problems . Furthermore , we address some important theoretical problems , such as the convergence rate , time complexity and the generalization error bound , which have not been discussed in previous works for MVTL . 2.4 Bundle Method
The proposed formulation is a convex optimization problem . In this paper , we propose an optimization algorithm based on the bundle method [ 38 , 39 ] , which has shown its superior performances in both efficiency and effectiveness over state of the art methods , to solve this proposed formulation . The basic motivation of the bundle method is to approximate the objective function J(w ) through a set of linear functions , where w is the model parameter . In particular , this objective function is lower bounded as follows : {J(wi−1 ) + w − wi−1 , ai} ,
J(w ) ≥ max 1≤i≤t where wi is a set of points picked by the bundle method , and ai is the gradient/sub gradient at point wi . The bundle method monotonically decreases the gap between J(w ) and max1≤i≤t{J(wi−1)+ w − wi−1 , ai} such that the minimal point of J(w ) can be approximated by that of the line segments max1≤i≤t {J(wi−1 ) + w − wi−1 , ai} .
Some recent developments in bundle method [ 39 ] show that if J(w ) contains some regularizers by itself , the bundle method is guaranteed to converge to the precision in O(1/ ) steps . In MVTLLM , we adapt the bundle method to solve the proposed problem , which can also be proven to have an efficient convergence rate . 3 . MULTI VIEW TRANSFER LEARNING WITH A LARGE MARGIN APPROACH In this section , we first introduce the problem statement and some notations for MVTL . Then , a general framework named MVTLLM is proposed , which integrates the multi view and transfer learning nature in a principled way . Based on an instantiation of the framework , we propose an optimization method , which is adapted from the bundle method [ 38 , 39 ] . Towards the end of this section , we analyze some important properties of the proposed method . 3.1 Problem Statement and Notations i n , yS
1 , yS
1 , z(p ) m } , z(p ) n )} , x(p )
2 , . . . , z(p )
1 ) , . . . , ( x(p ) is the class label of x(p )
Suppose we are given a set of labeled source domain examples from M independent views : {(x(p ) i ∈ i ∈ {−1 , 1} , p ∈ {1 , 2 , . . . , M} , where n is the total Rdp×1 , yS number of source domain examples and dp is the dimensionality of the p th view . yS . Besides the source i domain examples , a set of unlabeled target domain examples are i ∈ also available , and are denoted as : {z(p ) Rdp×1 , p ∈ {1 , 2 , . . . , M} . The goal of MVTL is to construct an accurate classifier for the target domain by making use of the labeled examples from the source domain as well as the redundancy incurred by multiple views . In this paper , similar to [ 15 ] , we assume that P rT ( y|x ) = P rS(y|x ) . In other words , the conditional probability of the class label given the features is the same for both the source domain and the target domain . This particular case can also be referred to as covariate shift [ 35 ] . As claimed in [ 15 ] , even in some cases when this assumption does not hold , the algorithm , which is based on this assumption , can still perform well . 3.2 MVTL LM Framework
In this subsection , we propose a general large margin framework for MVTL , which fully exploits the multi view and transfer learn ing nature . In this framework , we construct linear classifiers for all of the views , whose weight vectors are obtained via the following optimization problem :
M
M w(1),,w(M ) min
M
M
+ p=1 q=1
γpΩ(w(p ) ) +
CpR(P rT , l(x(p ) , y , w(p) ) ) p=1 p=1
Cp,qRc(P r(p,q )
T
, lc(x(p ) , x(q ) , w(p ) , w(q)) ) ,
( 1 ) where Ω(w(p ) ) is the regularization term defined on the p th view weight vector w(p ) ; R(P rT , l(x(p ) , y , w(p) ) ) is the expected classification loss with respect to the data distribution of the target domain examples ( P rT ) , which measures the deviations between the true labels and the predicted labels based on the p th view ; l(x(p ) , y , w(p ) ) is the classification loss ( such as the hinge loss [ 32] ) ; Rc(P r(p,q ) consistency between the p th view and the q th view with respect to their joint distribution in the target domain ; lc(x(p ) , x(q ) , w(p ) , w(q ) ) is the consistency between the p th view and the q th view ( such as the squared loss between the predictions on different views ) ; γp , Cp , and Cp,q are non negative parameters that balance the relative importance of the three terms in the objective function in Eq ( 1 ) .
, lc(x(p ) , x(q ) , w(p ) , w(q) ) ) measures the expected
T
Notice that when Cp,q = 0 , Eq ( 1 ) is equivalent to training a large margin classifier on each view independently . When Cp,q > 0 , by minimizing Eq ( 1 ) , we can obtain large margin classifiers which are also consistent across different views . The final classifier is the average of large margin classifiers on all the views , ie ,
M p=1 f ( x ) =
1 M
( w(p))T x(p ) , where x = [ (x(1))T , . . . , ( x(M ))T ]T . Next , we will discuss the loss term and the consistency term respectively . 321 Classification Loss R(P rT , l(x(p ) , y , w(p) ) ) measures the expected classification loss with respect to the data distribution on the target domain . To be specific , R(P rT , l(x(p ) , y , w(p) ) ) = E(x,y)∼P rT [ l(x(p ) , y , w(p)) ] . However , in our problem setting , we do not have any labeled examples from the target domain . Therefore , we estimate this term using labeled examples from the source domain as follows :
E(x,y)∼P rT [ l(x(p ) , y , w(p) ) ]
=E(x,y)∼P rS [
=E(x,y)∼P rS [ l(x(p ) , y , w(p) ) ]
P rT ( x , y ) P rS(x , y ) P rT ( y|x)P rT ( x ) P rS(y|x)P rS(x ) P rT ( x ) P rS(x ) l(x(p ) , y , w(p) ) ]
=E(x,y)∼P rS [ l(x(p ) , y , w(p)) ] .
=E(x,y)∼P rS [ β(x)l(x(p ) , y , w(p)) ] . ≈ 1 n
, yi , w(p) ) ,
βil(x(p ) i n i=1
( 2 ) where P rS is the distribution of the source domain , weight function β(x ) := P rT ( x ) P rS ( x ) ; and βi = β(xi ) . Notice that the value of β(x ) reflects the distribution difference between the source domain and the target domain . If the two distributions are similar , β(x ) will be close to 1 ; if the two distributions are dissimilar , underrepresented examples in P rT will receive a higher weight , whereas over represented examples will receive a lower weight . In this way ,
1210 we are able to estimate the classification error for the target domain using labeled examples from the source domain .
There are various ways to estimate βi , such as Gaussian Mixture Model ( GMM ) [ 22 ] , Kernel Density Estimation [ 34 ] , Kernel Mean Matching [ 15 ] , etc . In our approach , we concatenate the features on different views together , and measure the probability ratios between source and target domains by using GMM . To be specific , we first estimate the marginal distribution of all the features in the source domain and the target domain using two GMMs respectively . Then , βi is estimated as the ratio between the two generative probabilities on xi given by these two GMMs . Notice that in GMM , we need to specify the number of components . As will be shown in Section 4 , the proposed approach is very robust to small perturbations in this number . 322 Consistency As a consistency term , Rc(P r(p,q )
, lc(x(p ) , x(q ) , w(p ) , w(q) ) ) regulates that the outputs on individual views should be consistent , and not deviate too much . lc(x(p ) , x(q ) , w(p ) , w(q) ) ) is the consistency loss function , which penalizes the deviations between the output of x(p ) and x(q ) , under the classifiers w(p ) and w(q ) . Similar to Eq ( 2 ) , To estimate this term , we use both the labeled examples from the source domain and the unlabeled examples from the target domain as follows :
T
Rc(P r(p,q )
, lc(x(p ) , x(q ) , w(p ) , w(q) ) )
=E
( x(p),x(q))∼P r
( p,q ) T
[ lc(x(p ) , x(q ) , w(p ) , w(q)) ) ]
T
( cid:195 ) n i=1 m + n
≈ 1 m
+
βilc(x(p ) i
, x(q ) i
, w(p ) , w(q ) ) lc(z(p ) i
, z(q ) i
, w(p ) , w(q ) )
. i=1
This term regulates the consistency on both the source domain and target domain examples . Combining this constraint with the standard objective functions for each view yields a multi view learning algorithm , which was shown to perform better than single view approach on many classification applications . 3.3 Method
In this section , a concrete form of the above framework will be studied . Without loss of generality , we focus on the two view formulation , ie , M equals 2 , which can be easily extended to the case when M is larger than 2 . The hinge loss is used to define l(x(p ) , y , w(p) ) , and the squared loss is used for lc(x(p ) , x(q ) , w(p ) , w(q)) ) . 2 norm is used as the regularization term Ω(· ) . Then , the concrete form of the Eq ( 1 ) turns to : min w(1),w(2 ) w(p)2 +
Cp
βiξ(p ) i
( 4 )
2 n p=1 i=1
γp 2
2 n m p=1 i=1
+C(
βiw(1)T x(1 ) i − w(2)T x(2 ) i 2
+ w(1)T z(1 ) i − w(2)T z(2 ) i 2 ) i=1 st ∀i ∈ {1 , 2 , . . . , n} , yiw(1)T x(1 ) i ≥ 1 − ξ(1 ) i
, yiw(2)T x(2 ) i ≥ 1 − ξ(2 ) i
.
Here in this proposed method , we have absorbed the scaling comm+n into the trade off parameters Cp and ponents . C respectively , for simplicity . The final classification function f : n and ie , 1
1
Algorithm : Multi View Transfer Learning with a Large Margin Approach ( MVTL LM ) Input : 1 . Reweighting Ratios for Source Domain Examples : βi , i = {1 , 2 , . . . , n} 2 . Optimization Parameters : γ1 and γ2 for regularizers , and the trade off parameters C , C1 , C2 in Eq ( 5 ) , = 0001 3 . Source Domain Examples : {(x(i ) 4 . Target Domain Examples : {z(i ) 1 , z(i ) Output : The label assignment l = [ l1 , l2 , . . . , lm ] for the target domain examples .
1 . Initialization t = 0 , randomly initialize w0 . 2 . Construct X− , X(1 ) , X(2 ) according to Eq ( 5 ) . n , yn)} , i ∈ {1 , 2 , . . . , M} . m } , i ∈ {1 , 2 , . . . , M}
2 , y2 ) , . . . , ( x(i )
1 , y1 ) , ( x(i )
2 , . . . , z(i )
3 . Construct the matrix H(β , C ) . 4 . repeat 5 . 6 .
7 . Derive the optimization problem : RCP t = t + 1 Compute the gradient for the empirical loss : at =
∂wRemp(wt−1 ) , and bt = Remp(wt−1)− < wt−1 , at > . w , ai > +bi 8 . wt = arg minw wT H(β , C)w + RCP if 0.5 × wT ( z(1 )
9 . 10 . until t ≤ 11.Classification Assignment : for target domain example zi , ) > 0 , li equals 1 , and otherwise it equals −1 . t = min0≤i≤tJ(wi ) − Jt(wt ) i +z(2 ) t = max1≤i≤t < t i
( 3 )
Table 1 : Algorithm Description : Multi View Transfer Learning with a Large Margin Approach ( MVTL LM )
( X ( 1),X ( 2 ) ) −→ R can be specified as : f ( x ) = w(1)T x(1)+w(2)T x(2 ) To simplify this formulation , several concatenate vectors are further
2
. introduced:w = [ wT x(1 ) where inx(p ) i
2 ]T ,x
1 , wT i = [ x(1)T i
−T i = [ x(1)T
, 0]T ,x(2 )
,−x(2)T i = [ 0 , x(2)T i i i
]T ]T ,
( 5 )
, only the d(p−1 ) + 1 to dp th elements ( d0 = 0 ) are . After introducing these notations , Eq ( 4 ) nonzero and equals x(p ) can be simplified to the following form : i minw st ∀i ∈ {1 , 2 , . . . , n} wT H(β , C)w + yiwTx(1 ) i ≥ 1 − ξ(1 ) i
Cpβiξ(p ) i p=1
2 i=1 n , yiwTx(2 ) n i=1 βix i x i ≥ 1 − ξ(2 ) −T − i + m i=1z i z
, i
−T ) , where H(β , C ) = I(γ1 , γ2)+C( i and I(γ1 , γ2 ) is a diagonal matrix , with the first d1 elements being 2 and the remaining ones being γ2 2 . It is clear that this problem is γ1 convex , since βi is given .
−
There are several alternatives to solve this problem efficiently . Here , an efficient way , which is an adaption of the bundle method , is proposed to solve this optimization problem of the MVTL LM approach . The concrete procedure is described in Table 1 . Here , n 2 Remp(w ) = 2 n wT H(β , C)w+ i=1 Cpβi max{0 , 1−yS i } , J(w ) = i Jt(w ) = wT H(β , C)w+ i wTx(p ) i=1 Cpβiξ(p ) p=1 p=1
1211 max1≤i≤t < w , ai > +bi . Since Remp(w ) is non smooth , so , when calculating its gradient , we use the sub gradient instead , which can be calculated as
∂wRemp(w ) = − 2 n i wTx(p ) p=1 i=1 i is set to be 1 , if yS where I ( p ) otherwise . 3.4 Theoretical Analysis ix(p ) i
,
CpβiI ( p ) i yS i < 1 , and I ( p ) i is set to be 0 ,
In this section , we deduct the convergence rate , time complexity as well as the generalized error bound of the proposed method . 341 Convergence i x scribed in Table 1 , Suppose Rmax = maxp,i(Cpβi)x(p ) n i=1 βix THEOREM 31 For the convergence rate of the algorithm dei . A = ) , the corresponding eigenvalC( ues of A are specified as : σ1(A ) ≥ σ2(A ) ≥ . . . ≥ σ(d1+d2)(A ) ≥ 0 . Assume that βi ≤ B . The proposed method converges in O(1/ ) . In particular , m i=1z i z
−T i +
−T i
−
−
• If > 16R2 max/(min{γ1 , γ2} + 2σ(d1+d2)(A) ) , the pro posed method converges to precision after at most log2 nB(C1+C2)(min{γ1,γ2}+2σ(d1+d2)(A ) ) steps .
4R2 max max/( ( min
• If ≤ 16R2 max/(min{γ1 , γ2} + 2σ(d1+d2)(A) ) , the pro max p=1
4R2
ξ(p ) i n nB(C1+C2)(min{γ1,γ2}+2σ(d1+d2)(A ) ) posed method converges to precision after at most +32R2 2 log2 PROOF . We have J(w ) = wT H(β , C)w + {γ1 , γ2} + 2σ(d1+d2)(A) ) ) − 1 steps . , Ω(w ) = wT H(β , C)w . Ω∗(µ ) = µT H−1(β , C)µ is the 2 n i=1 Cpβi Fenchel dual of Ω(w ) . J(0 ) = C2 ) . ∂wRemp(w ) ≤ Rmax . i=1 Cpβi ≤ nB(C1 + µΩ∗(µ ) ≤ 4/(min {γ1 , γ2} + 2σ(d1+d2)(A) ) . By integrating these inequalities into Theorem 4 of [ 39 ] , we can get 1 , t(min{γ1 , γ2} + 2σ(d1+d2)(A))/16R2 t − t+1 ≥ t where t = min0≤i≤tJ(wi ) − Jt(wt ) . The algorithm will termi2 max/(min{γ1 , γ2}+2σ(d1+d2)(A) ) ,
It is clear that ∂2 nate if t < . So , if > 16R2 t − t+1 ≥ t
( cid:161 ) min p=1
2 , and the algorithm will terminate after at most : J(0)(min{γ1 , γ2} + 2σ(d1+d2)(A ) ) nB(C1 + C2)(min{γ1 , γ2} + 2σ(d1+d2)(A ) )
4R2 max log2
≤log2
( 6 )
4R2 max
( cid:162 )
, max steps .
If ≤ 16R2 max/(min{γ1 , γ2}+2σ(d1+d2)(A) ) , then , this method needs the above indicated steps to converge to the precision 16R2 /(min{γ1 , γ2}+2σ(d1+d2)(A) ) , then , we should have t− t+1 ≥ 2 ( min{γ1 , γ2}+2σ(d1+d2)(A))/16R2 2 max . It is clear that it needs t max/( ( min{γ1 , γ2} + 2σ(d1+d2)(A) ) ) − 1 steps to another 32R2 converge to the precision . So , in total , this algorithm converges in max nB(C1 + C2)(min{γ1 , γ2} + 2σ(d1+d2)(A ) ) log2
+32R2 max/( ( min{γ1 , γ2} + 2σ(d1+d2)(A) ) ) − 1
4R2 max steps .
In summary , the algorithm converges in O(1/ ) steps . It is clear that the number of iterations also highly depends on R2 max , which can be viewed as the maximum reweighted norm for the source domain examples . Furthermore , it is clear that increasing the parameter values of C1 and C2 will increase the number of iterations . 342 Time Complexity
THEOREM 32 For each iteration of the proposed method , it takes time O(sn ) .
PROOF . The gradient computation in step 6 takes time O(sn ) , where s is the average sparsity on both views . Instead of solving the primal quadratic program , one can instead solve the optimization problem in step 8 in the dual form . Setting up the dual for each iteration is dominated by computing the O(t2 ) elements of the Hessian , which can be done in O(t2s ) steps . Since t2 is normally much smaller than n , it leads to an overall time complexity of O(sn ) per iteration . This result is similar to the time complexity result per iteration in [ 16 ] . However , the total number of iterations in [ 16 ] may be as worse as O(1/ 2 ) , as given by the Lemma 2 of [ 16 ] . On the contrary , the number of iterations required in this paper is guaranteed to be in O(1/ ) . So , solving the proposed problem by the proposed method is much faster than using the Cutting Plane method [ 18 ] . 343 Generalized Error Bound In this subsection , we assume that ∀x , β(x ) ≤ B if the marginal probabilities in the source domain or in the target domain are greater than 0 . Let w(1)∗ denote the solution of Eq ( 4 ) . Similar to [ 12 ] , we consider the class of functions FE,D = {f|f : 2 ( (w(1))T x(1 ) + ( w(2))T x(2))} such that w(1)2 ≤ E2 , x → 1 n w(2)2 ≤ E2 , and with probability at least 1 − δ , m n m n i − ( w(2)∗ )T x(2 ) i 2 i − ( w(2)∗ )T z(2 ) i 2 m i − ( w(2))T x(2 ) i − ( w(2))T z(2 )
βi(w(1)∗ )T x(1 )
βi(w(1))T x(1 )
( w(1)∗ )T z(1 )
( w(1))T z(1 )
≤ 1 and w(2)∗ i 2 i 2 m + n m + n m + n m + n
EB i=1 i=1 i=1 i=1
+
+
1
1
1 zi2 + 3B xi2 +
= : D .
+ ln(2/δ ) 2(m + n ) m + n i=1 i=1
Furthermore , define Fβ,E,D to be the following class of functions , Fβ,E,D = {h|h : ( x , y ) → β(x ) · A(−f ( x ) · y ) , f ∈ FE,D} where A(a ) = if a > 0
1 + a if − 1 ≤ a ≤ 0 otherwise
1
0
. Based on these func tion classes , we have the following theorem with respect to the generalization error of the classifier obtained via the MVTL algorithm . THEOREM 33 Fix δ ∈ ( 0 , 1 ) . Then , with probability at least
1 − δ , every f ∈ Fβ,E,D satisfies : n 2 EP rT ( sign(f ( x ) ) = y ) ≤
βiξ(p )
Cp
1 n(C1 + C2 ) p=1 i=1 i + ˆRS,n(Fβ,E,D ) + 3B ln(2/δ )
2n
,
1212 where ˆRS,n(Fβ,E,D ) is the empirical Rademacher complexity of Fβ,E,D in the source domain .
PROOF . Since the conditional probability of y given x is the same for the source domain and the target domain ,
EP rT ( sign(f ( x ) ) = y ) = EP rS ( β(x ) · sign(f ( x ) ) = y ) .
Notice that ∀h ∈ Fβ,E,D , h(x , y ) ∈ [ 0 , B ] . By similar proof as Theorem 4.17 in [ 33 ] , we obtain that with probability greater than 1 − δ ,
EP rS ( β(x ) · sign(f ( x ) ) = y ) ≤ EP rS ( β(x ) · A(−f ( x ) · y ) ) ≤ i + ˆRS,n(Fβ,E,D ) + 3B
2 n
βiξ(p )
Cp
1 n(C1 + C2 ) p=1 i=1 ln(2/δ )
2n
According to Theorem 3.3 , the generalization error bound of any function in Fβ,E,D depends on the weighted sum of all the slack variables associated with labeled examples in the source domain and the empirical Rademacher complexity of Fβ,E,D in the source domain ( up to a constant ) . Therefore , from transfer learning perspective , if the data distributions of the source domain and the target domain are similar ( B is relatively small ) , constructing the large margin classifier in the source domain helps improve the generalization error of the classifier in the target domain . More importantly , from multi view learning perspective , in Fβ,E,D , by leveraging the consistencies among different views , we effectively limit the hypothesis class , thus reduce the empirical Rademacher complexity . As empirically shown in [ 12 ] , after imposing the consistency on different views , the Rademacher complexity is significantly reduced compared with the single view correspondents .
This bound improves some existing theoretical results in transfer learning . For example , compared with the bound in Theorem 1 of [ 3 ] , we make use of the data dependent convergence measures , which can yield more accurate bounds . In Theorem 5 of [ 6 ] , the authors also proved a bound for classifiers trained on multiple sources based on empirical Rademacher complexity . However , their bound can not be estimated from the data . For example , it depends on the expected difference between the labeling functions of different domains , which can not be estimated in our case where the target domain does not have any labeled examples .
Our bound also improves the results in [ 12 ] , which is a multiview learning algorithm . First , our bound is proposed for the transfer learning setting , whereas SVM 2K [ 12 ] is for the single domain setting . Furthermore , Theorem 3 of [ 12 ] can be seen as a special case of our bound when the source domain and the target domain have the same distribution and we do not use the unlabeled data from the target domain . Second , compared with SVM 2K , we make additional use of the unlabeled data from the target domain . If the number of unlabeled examples from the target domain is much larger than the number of labeled examples from the source domain , we tend to decrease the value of D , which is the upper bound on the view consistency . In this way , we reduce the function class , the corresponding Rademacher complexity , and thus improve the bound .
4 . EXPERIMENTS
In this section , we present and analyze an extensive set of experimental results , which clearly demonstrate the advantages of the proposed method .
Dataset comp vs rec comp vs sci comp vs talk rec vs sci
. rec vs talk sci vs talk composms windowsmisc
Source Domain comp.graphics rec.autos rec.motorcycles comp.graphics sci.electronics sci.space composms windowsmisc compsysibmpchardware compsysmachardware talkpoliticsguns talkpoliticsmideast rec.autos rec.motorcycles sci.electronics sci.space recsportbaseball recsporthockey talkpoliticsguns talkpoliticsmideast sci.electronics sci.space talkpoliticsguns talkpoliticsmideast
Target Domain compsysibmpchardware compsysmachardware recsportbaseball recsporthockey compsysibmpchardware compsysmachardware sci.crypt sci.med comp.graphics composms windowsmisc talkpoliticsmisc recsportbaseball recsporthockey sci.crypt sci.med rec.autos rec.motorcycles talkpoliticsmisc sci.crypt sci.med talkpoliticsmisc
Table 2 : Descriptions of Six Sub datasets from 20Newsgroup
4.1 Datasets
411 20 Newsgroups 20 Newsgroups dataset1 contains 4 main categories , ie , ‘comp’ , ‘rec’ , ‘sci’ , ‘talk’ , as well as some small categories , such as ‘alt.athe ism’ , ‘misc.forsale’ , etc . The number of examples for each of the four main categories ranges from 3253 to 4881 . Each of the four main categories contains some subcategories , which are assigned to different domains . Using the 4 main categories , we create 6 sub datasets . The detailed descriptions of these sub datasets are summarized in Table 2 . For each sub dataset , we extract features from 2 views . One view ( View 1 ) corresponds to the original tfidf content information processed by Principle Component Analysis ( PCA ) , and the other view ( View 2 ) corresponds to the hidden topic information obtained by Probabilistic Latent Semantic Analysis ( PLSA)2 of the binary word features . 412 Spam Detection This dataset is from ECML/PKDD Discovery Challenge 20063 , which focuses on personalized spam filtering and generalization across related learning tasks . In particular , in Task A , we aim to construct spam filters for 3 different users , each of which have 2500 emails . In our experiments , we take the labeled emails from one user as the source domain , and the unlabeled emails from another user as the target domain . The two views are generated using the same way as 20 Newsgroups . 413 WebKB This dataset contains web pages from computer science departments of several different universities4 . They are divided into 7
1http://peoplecsailmitedu/jrennie/20Newsgroups/ 2Actually , PLSA [ 14 ] can be considered as a dimensionality reduction method , which maps the documents to some fixed number of hidden topics . The topic distribution for each document can be used as low dimensional representation . 3http://wwwecmlpkdd2006org/challengehtml 4http://wwwcscmuedu/∼webkb/
1213 SVM View1 SVM View2
SVM
SVM 2K CDSC LLGC LMTTL
Co Training MVTL LM comp vs rec
85.4 84.0 86.2 88.5 87.6 77.5 85.8 86.9 92.9 comp vs sci
73.3 70.6 70.9 79.3 72.3 71.1 69.8 72.5 80.1
95.4 96.4 96.7 97.2 81.3 92.6 96.4 97.1 98.3
20 Newsgroups comp vs talk rec vs sci rec vs talk
56.8 65.8 53.6 81.6 80.1 73.9 53.7 61.2 83.4 sci vs talk
81.8 78.3 80.1 83.1 80.4 81.2 78.6 81.4 83.0 student 0.595 0.171 0.544 0.725 0.361 0.167 0.546 0.495 0.742
WebKB course 0.541 0.116 0.562 0.512 0.202 0.277 0.545 0.554 0.565 faculty 0.441 0.236 0.464 0.563 0.291 0.310 0.475 0.444 0.671
67.2 63.5 64.6 71.0 71.2 72.6 64.5 66.7 75.3
Table 3 : Classification Results on 20 Newsgroups and WebKB . For 20 Newsgroups , we report the classification accuracy ; for WebKB , we report the F measure due to the extremely imbalanced nature of this data set . It can be seen that MVTL LM performs the best in most cases . categories ( ie , student , faculty , staff , course , project , department and other ) . We generate three sub datasets out of them , ie , student , course and faculty . For each sub dataset , we pick the corresponding webpages from the four main universities , ie , Cornell , Washington , Wisconsin , and Texas as the source domain positive examples , and the webpages in ’other’ category from these four universities as the source domain negative examples . In the target domain , a similar way is used to extract examples from the other universities . We use the content ( View 1 ) and the link information ( View 2 ) as the two views of this dataset .
4.2 Methods
We compare the proposed method with the following competitors : Support Vector Machines ( SVM ) , which is a supervised classification method ; LLGC [ 46 ] , which is a semi supervised learning method ; SVM 2K [ 12 ] , which is a multi view learning method ; CDSC [ 21 ] , LMTTL [ 29 ] , which are transfer learning methods and have shown state of the art performances . We also adapt the cotraining [ 27 ] algorithm to work for MVTL problems as follows : we disregard the domain difference , put labeled examples from the source domain and unlabeled examples from the target domain together , and apply the co training algorithm to construct a classifier for each view of the target domain via SVM . This is similar to the co adaptation algorithm proposed in [ 40 ] except that besides generating the initial seed set , labeled examples from the source domain are also used to construct classifiers for the target domain . Notice that besides SVM 2K and the co training algorithm , the other baseline methods only work in the single view settings . Therefore , for the sake of comparison , we first represent each example using a single set of features by concatenating the features from different views , and then apply these methods on this single view . Furthermore , to better understand the benefits brought by the multi view methods , we also apply SVM on each view and report the performance .
For our proposed method , the re weighting factors βi , i = 1 , . . . , n are learned by Gaussian Mixture Model , and the numbers of Gaussian components are both set to be 4 . We will show later that the performance of MVTL LM is very robust against small perturbations in the number of Gaussian components . We set γ1 = γ2 = 1 , and tune the remaining three parameters ( C1 , C2 and C ) through five fold cross validation on both the source domain and the target domain . The parameters of SVM 2K are also set in the same way , except that in SVM 2K we do not need to consider the number of Gaussian components . For LLGC , the RBF kernel is used , with its Gaussian variance being determined automatically by local scaling [ 43 ] . The parameters of CDSC , SVM , LMTTL and CoTraining are all set through five fold cross validation similarly .
( a )
Figure 1 : The impact of changing the number of Gaussian components on the performance of our proposed method . In this evaluation , we are assuming that the numbers of Gaussian components for both the source domain and the target domain are the same . The experiments are conducted by fixing the number of Gaussian components , while tuning the other parameters .
4.3 Results and Analysis
To study the impact of changing the number of Gaussian components on the performance of our proposed method , we vary this number when estimating βi , and report the classification accuracy on 20 Newsgroups dataset in Fig 1 . From these results , we can see that the proposed method is very stable with different numbers of Gaussian components . Similar results have also been observed on WebKB and Spam datasets .
Next we report the comparison results in Tables 3 and 4 respectively . For 20 Newsgroups and Spam datasets , the classification accuracy is reported ; whereas for WebKB dataset , the F measure is reported instead due to the extremely imbalanced nature of this dataset 5 . From these results , we have the following observations .
1 . Our proposed method MVTL LM performs the best in most cases . This is because our method models both the consis5The number of negative examples is around 6 times more than that of the positive examples .
2345678556065707580859095100Numer of Gaussian Mixture ModelsAccracy20Newsgroup comp vs reccomp vs scicomp vs talkrec vs scirec vs talksci vs talk1214 SVM View1 SVM View2
Co Training MVTL LM
SVM
SVM 2K CDSC LLGC LMTTL user1 vs user2
79.7 94.8 94.6 94.1 84.1 97.1 94.9 92.5 95.2 user1 vs user3
65.7 97.3 96.9 97.6 97.2 96.3 96.2 96.6 97.9 user2 vs user3
83.4 97.4 97.6 97.2 96.6 93.2 97.5 96.6 98.1 user2 vs user1
76.7 91.4 92.1 91.8 90.3 91.7 92.0 92.2 93.7 user3 vs user1
76.0 89.8 88.7 90.8 89.1 91.3 88.9 88.6 92.9 user3 vs user2
80.9 94.2 92.9 94.1 91.7 93.2 93.1 93.9 96.3
Table 4 : Classification Results on Spam dataset . The classification accuracy is reported . It can be seen that MVTL LM performs the best in most cases . tency between different views and the domain difference simultaneously , whereas the other methods ignore some useful information ( ie , the data distribution difference between different domains and the redundancy incurred by multiple views ) .
2 . Comparing with multi view learning methods ( SVM 2K and Co Training ) , MVTL LM performs better because it explicitly models the data distribution difference between the source domain and the target domain ; whereas the multi view learning methods simply treat the two domains as a single one .
3 . Comparing with transfer learning methods ( CDSC and LMTTL ) , in MVTL LM , we are able to transfer additional information about view consistency from the source domain to the target domain . We also suspect that these traditional transfer learning methods may not work well in the cases when different kinds of features are merged together . Therefore , MVTL LM could achieve better performance in most cases .
4 . The performance of SVM is worse than SVM 2K in most cases . This is because in SVM , simply concatenating the features from different views together fails to capture the consistency between different views ; whereas SVM 2K explicitly models this consistency , which is able to improve the overall performance .
5 . Comparing with SVM , SVM View1 , and SVM View2 , we can see that concatenating the features from different views may not necessarily result in an increase in the classification performances although SVM uses more information than SVMView1 and SVM View2 .
6 . As a graph based semi supervised method , the performance of LLGC is not promising because the basic mainfold assumption in semi supervised learning does not hold in transfer learning , and concatenating multi view features together is not well suited in the multi view learning scenario .
7 . The traditional transfer learning methods show very poor performances in WebKB . This is because in the subdatasets of WebKB , the link view contains too much noise as can be seen from the performance of SVM View2 . Concatenating these features together may bring more noise for classification , and therefore could cause a decrease in the classification performance , especially for traditional transfer learning methods .
These observations clearly demonstrate the advantages of the proposed method over state of the art ones . It validates our claims and theoretical analysis that by integrating the multi view learning and transfer learning together , the classification performance can be greatly improved .
5 . CONCLUSIONS
Transfer learning is an important technique for utilizing data in a related source domain for building predictive models in a target domain . Much valuable prior research has been conducted for traditional transfer learning with data from a single view . However , many real world applications often contain data from multiple views , and there is limited work for transfer learning with data from different views . This paper proposes a formal learning framework for Multi View Transfer Learning with a Large Margin ( MVTLLM ) approach . In particular , the weighted labeled data from the source domain is used to construct a large margin classifier for target domain and both the unlabeled data from the target domain and data from source domain are used to ensure the classification consistency between different views . A novel optimization method based on bundle method is proposed to learn model parameters in an efficient manner , which has a theoretical guarantee to generate accurate results in O(1/ ) steps . Furthermore , theoretical analysis is provided for the generalization error bound of the proposed method and shows the improved results of the Rademacher complexity . An extensive set of results on three different datasets have been provided to demonstrate the advantages of the proposed method against several other alternatives .
6 . ACKNOWLEDGEMENT
We would like to express our sincere thanks to Dr . Zheng Wang ( Tsinghua University ) , Prof . SVN Vishwanathan ( Purdue University ) , and the anonymous reviewers for their valuable comments and suggestions . This research was partially supported by the NSF research grants IIS 0746830 , CNS 1012208 , IIS 1017837 , CCF0939370 .
7 . REFERENCES [ 1 ] E . C . Anderson . Monte Carlo Methods and Importance
Sampling , 1999 .
[ 2 ] A . Argyriou , T . Evgeniou , and M . Pontil . Multi task feature learning . In NIPS , page 41 . MIT Press , 2007 .
[ 3 ] S . Ben David , J . Blitzer , K . Crammer , and F . Pereira .
Analysis of representations for domain adaptation . In NIPS , pages 137–144 , 2006 .
[ 4 ] A . Blum and T . M . Mitchell . Combining labeled and unlabeled sata with co training . In COLT , pages 92–100 , 1998 .
[ 5 ] E . Bonilla , K . Chai , and C . Williams . Multi task Gaussian process prediction . NIPS , 20:153–160 , 2008 .
[ 6 ] K . Crammer , M . Kearns , and J . Wortman . Learning from multiple sources . Journal of Machine Learning Research , 9:1757–1774 , 2008 .
1215 [ 7 ] W . Dai , G R Xue , Q . Yang , and Y . Yu . Co clustering based classification for out of domain documents . In KDD , pages 210–219 , 2007 .
[ 8 ] W . Dai , Q . Yang , G . Xue , and Y . Yu . Boosting for transfer learning . In ICML , pages 200–207 , 2007 .
[ 9 ] W . Dai , Q . Yang , G . Xue , and Y . Yu . Self taught clustering .
In ICML , pages 200–207 , 2008 .
[ 10 ] J . Davis and P . Domingos . Deep transfer via second order
Markov logic . In ICML , pages 217–224 , 2009 .
[ 11 ] L . Duan , I . Tsang , D . Xu , and S . Maybank . Domain transfer svm for video concept detection . CVPR , pages 1375–1381 , 2009 .
[ 12 ] J . Farquhar , D . Hardoon , H . Meng , J . Shawe Taylor , and
S . Szedmak . Two view learning : SVM 2K , theory and practice . NIPS , 18:355 , 2006 .
[ 13 ] K . Ganchev , J . Graça , J . Blitzer , and B . Taskar . Multi view learning over structured and non identical outputs . In UAI , pages 204–211 , 2008 .
[ 14 ] T . Hofmann . Probabilistic latent semantic indexing . In
SIGIR , pages 50–57 , 1999 .
[ 15 ] J . Huang , A . Smola , A . Gretton , K . Borgwardt , and
B . Scholkopf . Correcting sample selection bias by unlabeled data . NIPS , 19:601 , 2007 .
[ 16 ] T . Joachims . Training linear SVMs in linear time . In KDD , pages 217–226 , 2006 .
[ 17 ] T . Joachims and N . Cristianini . Composite kernels for hypertext categorisation . In ICML , pages 250–257 , 2001 .
[ 18 ] J . Kelley . The cutting plane method for solving convex programs . Journal of the SIAM , 8(4):703–712 , 1960 . [ 19 ] N . Lawrence and J . Platt . Learning to learn with the informative vector machine . In ICML , page 65 , 2004 .
[ 20 ] G . Li , S . C . H . Hoi , and K . Chang . Two view transductive support vector machines . In SDM , pages 235–244 , 2010 . [ 21 ] X . Ling , W . Dai , G R Xue , Q . Yang , and Y . Yu . Spectral domain transfer learning . In KDD , pages 488–496 , 2008 . [ 22 ] X . Liu , Y . Gong , W . Xu , and S . Zhu . Document clustering with cluster refinement and model selection capabilities . In SIGIR , pages 191–198 , 2002 .
[ 23 ] Y . Mansour , M . Mohri , and A . Rostamizadeh . Domain adaptation with multiple sources . In NIPS , pages 1041–1048 , 2008 .
[ 24 ] Y . Mansour , M . Mohri , and A . Rostamizadeh . Multiple source adaptation and the rényi divergence . In UAI , pages 367–374 . AUAI Press , 2009 .
[ 25 ] L . Mihalkova , T . Huynh , and R . Mooney . Mapping and revising Markov logic networks for transfer learning . In AAAI , volume 22 , pages 608–613 , 2007 .
[ 26 ] L . Mihalkova and R . Mooney . Transfer learning by mapping with minimal target data . In Proceedings of the AAAI 08 Workshop on Transfer Learning for Complex Tasks , 2008 . [ 27 ] K . Nigam and R . Ghani . Analyzing the effectiveness and applicability of co training . In CIKM , pages 86–93 , 2000 .
[ 28 ] S . Pan , J . Kwok , and Q . Yang . Transfer learning via dimensionality reduction . In Proceedings of AAAI , pages 677–682 .
[ 29 ] B . Quanz and J . Huan . Large margin transductive transfer learning . In CIKM , pages 1327–1336 , 2009 .
[ 30 ] R . Raina , A . Battle , H . Lee , B . Packer , and A . Ng .
Self taught learning : transfer learning from unlabeled data . In ICML , pages 766–763 , 2007 .
[ 31 ] D . Rosenberg , V . Sindhwani , P . Bartlett , and P . Niyogi . A
Kernel for Semi Supervised Learning With Multi View Point Cloud Regularization . IEEE Signal Processing Magazine , 2009 .
[ 32 ] B . Scholkopf and A . Smola . Learning with kernels . MIT press Cambridge , Mass , 2002 .
[ 33 ] J . Shawe Taylor and N . Cristianini . Kernel Methods for
Pattern Analysis . Cambridge University Press , New York , NY , USA , 2004 .
[ 34 ] S . Sheather and M . Jones . A reliable data based bandwidth selection method for kernel density estimation . Journal of the Royal Statistical Society . Series B ( Methodological ) , 53(3):683–690 , 1991 .
[ 35 ] H . Shimodaira . Improving predictive inference under covariate shift by weighting the log likelihood function . Journal of Statistical Planning and Inference , 90(2):227–244 , 2000 .
[ 36 ] V . Sindhwani and P . Niyogi . A co regularized approach to semi supervised learning with multiple views . In ICML Workshop on Learning with Multiple Views , 2005 .
[ 37 ] V . Sindhwani and D . Rosenberg . An RKHS for multi view learning and manifold co regularization . In ICML , pages 976–983 , 2008 .
[ 38 ] A . Smola , S . Vishwanathan , and Q . Le . Bundle methods for machine learning . NIPS , 20 , 2008 .
[ 39 ] C . Teo , S . Vishwanthan , A . Smola , and Q . Le . Bundle methods for regularized risk minimization . The Journal of Machine Learning Research , 11:311–365 , 2010 . [ 40 ] G . Tur . Co adaptation : Adaptive co training for semi supervised learning . In ICASSP , 2009 .
[ 41 ] M . Wu and B . Schölkopf . A local learning approach for clustering . In NIPS , pages 1529–1536 , 2006 .
[ 42 ] B . Zadrozny . Learning and evaluating classifiers under sample selection bias . In ICML , 2004 .
[ 43 ] L . Zelnik Manor and P . Perona . Self tuning spectral clustering . NIPS , 17:1601–1608 , 2004 .
[ 44 ] D . Zhang , F . Wang , C . Zhang , and T . Li . Multi view local learning . In AAAI , pages 752–757 , 2008 .
[ 45 ] T . Zhang , A . Popescul , and B . Dom . Linear prediction models with graph regularization for web page categorization . In SIGKDD , pages 821–826 , 2006 .
[ 46 ] D . Zhou , O . Bousquet , T . N . Lal , J . Weston , and
B . Schölkopf . Learning with local and global consistency . In NIPS , 2003 .
[ 47 ] S . Zhu , K . Yu , Y . Chi , and Y . Gong . Combining content and link for classification using matrix factorization . In SIGIR , pages 487–494 , 2007 .
1216
