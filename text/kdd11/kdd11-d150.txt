Incorporating Term Volume into Temporal Topic Models
Tracking Trends :
Liangjie Hong† Dawei Yin†
Jian Guo§ Brian D . Davison†
† Dept . of Computer Science and Engineering , Lehigh University , Bethlehem , PA , USA
§ Dept . of Statistics , University of Michigan , Ann Arbor , MI , USA
† {lih307,day207,davison}@cselehighedu , § guojian@umich.edu
ABSTRACT
Text corpora with documents from a range of time epochs are natural and ubiquitous in many fields , such as research papers , newspaper articles and a variety of types of recently emerged social media . People not only would like to know what kind of topics can be found from these data sources but also wish to understand the temporal dynamics of these topics and predict certain properties of terms or documents in the future . Topic models are usually utilized to find latent topics from text collections , and recently have been applied to temporal text corpora . However , most proposed models are general purpose models to which no real tasks are explicitly associated . Therefore , current models may be difficult to apply in real world applications , such as the problems of tracking trends and predicting popularity of keywords . In this paper , we introduce a real world task , tracking trends of terms , to which temporal topic models can be applied . Rather than building a general purpose model , we propose a new type of topic model that incorporates the volume of terms into the temporal dynamics of topics and optimizes estimates of term volumes . In existing models , trends are either latent variables or not considered at all which limits the potential for practical use of trend information . In contrast , we combine state space models with term volumes with a supervised learning model , enabling us to effectively predict the volume in the future , even without new documents . In addition , it is straightforward to obtain the volume of latent topics as a by product of our model , demonstrating the superiority of utilizing temporal topic models over traditional time series tools ( eg , autoregressive models ) to tackle this kind of problem . The proposed model can be further extended with arbitrary word level features which are evolving over time . We present the results of applying the model to two datasets with long time periods and show its effectiveness over non trivial baselines .
Categories and Subject Descriptors
H.4 [ Information Systems Applications ] : Miscellaneous ; H33 [ Information Storage and Retrieval ] : Information Search and Retrieval—clustering
General Terms
Algorithms , Experimentation , Theory
Keywords
Topic models , Text mining , Temporal dynamics
1 .
INTRODUCTION
Text corpora with documents covering a long time span are natural and ubiquitous in many application fields , and include such data as research papers and newspaper articles . Mining from these collections , discovering and understanding underlying topics and ideas , continues to be an important task . In addition to traditional text collections , many types of content in social media make applying machine learning techniques to these new data sources more challenging , such as forums , question answering communities and blog entries . People not only would like to know what kind of topics can be found from these data sources but also wish to understand the temporal dynamics of these topics , and hopefully predict certain properties of terms or documents in the future .
Topic models ( eg , [ 5] ) , as a class of newly developed machine learning tools , have been studied extensively in recent years . From the seminal work done by Blei et al . [ 5 ] , a large body of literature about topic models has been established . Multiple disciplines of computer science , ranging from information retrieval ( eg , [ 24] ) , computer vision ( eg , [ 19 ] ) to collaborative filtering ( eg , [ 1 ] ) have applied topic models to their problems . For text modeling , topic models are applied to find latent topics from text collections , which is particularly useful for temporal text corpora where discovered latent topics can help researchers visualize and understand the thematic evolution of the corpora over time . This has led to the recent development of incorporating temporal dynamics into topic models ( eg , [ 14 , 3 , 21 , 13 , 15 , 22 , 20 , 12 , 23 , 25 , 2 , 9 , 10] ) . These models enable us to browse and explore datasets with temporal changes in a convenient way and open future directions for utilizing these models in a more comprehensive fashion . One drawback of these existing models is that most of them are general purpose models with which no real tasks are explicitly associated . Therefore , it might be difficult to employ these models in real world applications , such as the problems of tracking trends and predicting popularity of keywords . As a result of the lack of a particular task , there is also no consensus on how these models should be evaluated and compared . Although perplexity is widely used in these papers , as pointed out in [ 6 ] , this measure may not have correlations with the quality ( eg , coherence ) of topics discovered . Furthermore , no empirical or theoretical work has been done as far as we know to show the the correlations between the low perplexity values and high performance in third party tasks such classification , regression and clustering . In this paper , we argue that temporal topic models should be evaluated on specific real world tasks and propose such a task to compare how they can contribute to applications . Some recent extensions of topic models ( eg , [ 4 , 11 , 26 , 18 ] ) have tried to incorporate side information , such as document level labels and word level features ( eg , [ 17 ] ) into models in order to perform classification and regression tasks . A basic conclusion made from these attempts is that these special purposed models , aiming to optimize particular tasks , perform better than general purpose models , on the tasks they evaluated . We share a similar spirit in this paper , showing that temporal topic models for special tasks perform better than general purpose models .
In this paper , we introduce a real world task — tracking trends of terms — to which temporal topic models can be applied . Rather than building a general purpose model , we propose a new type of topic model incorporating the volume of terms into the temporal dynamics of topics and directly optimize for the task . Unlike existing models in which trends are either latent variables or not considered at all and thus are difficult to apply in practice , we combine state space models with term volumes in a supervised learning fashion which enables us to effectively predict volumes in the future , even without new documents . In addition , it is straightforward to obtain the volumes of latent topics as a by product of our model , demonstrating the superiority of utilizing temporal topic models over traditional time series tools ( eg , autoregressive models ) to tackle this kind of problem . The proposed model can be further extended with arbitrary word level features which are evolving over time . We present the results of applying the model to two datasets with long time periods and show its effectiveness over non trivial baselines . Our contributions are threefold :
• Introduce a task — volume tracking — that can be used as a standard evaluation method for temporal topic models
• Propose a temporal topic model that directly optimizes the task introduced
• Demonstrate the effectiveness of the model as compared to state of the art algorithms by experimenting on two realworld datasets
We organize the paper as follows . In Section 2 , we review some related developments of topic models and existing evaluation methods for temporal topic models . In Section 3 , we introduce the task of volume tracking , as a case of trend monitoring , and propose our model . In Section 4 , we show how to utilize variational inference with Kalman Filter to estimate hidden parameters of the model . In Section 5 , we discuss some other models that can be used in the volume tracking task . In Section 6 , we demonstrate the experimental results on two datasets and conclude the paper in Section 7 .
2 . RELATED WORK
In this section , we review three directions of related work . First , we summarize all up to date topic models which try to incorporate temporal dynamics into the model . Then , we discuss the evaluation of these models and the potential to apply them in real world applications . In the end , we present the attempts to embed sideinformation , or features into topic models .
To incorporate temporal dynamics into topic models , many models have been proposed . Note , as we mentioned , these attempts are general purpose models , meaning that no real world tasks are explicitly addressed . In general , all these models fall into two categories . The models in the first category do not impose a global distribution assumption about how topics evolve over time . In
Table 1 : Evaluation on Temporal Topic Models
( Temporal ) Perplexity Timestamp Prediction Classification/Clustering Ad Hoc
[ 3 , 15 , 20 , 23 , 25 , 2 , 9 , 10 ]
[ 21 , 20 , 10 ]
[ 25 ]
[ 21 , 23 , 25 ] other words , these models assume that topics change over time depending on their previous conditions , effectively making “ Markovian assumptions ” . The examples in this category are Dynamic Topic Model ( DTM ) , proposed by Blei and Lafferty [ 3 ] and Continuous Time Dynamic Topic Models ( cDTM ) , proposed by Wang et al . [ 20 ] , embedding state space models into topic models . Our work is inspired by this type of model . The second category of models usually imposes a global distribution of temporal dynamics . For instance , Wang et al . [ 21 ] introduce a beta distribution over timestamps and incorporate it into the standard topic model . Masada et al . [ 12 ] assume a Gaussian distribution over the whole time line of topics . Although these models are proposed under different contexts , the drawback of this category is that the distributional assumption is hard to justify . Based on the two basic categories , other extensions are proposed . For example , Nallapati et al . [ 15 ] and Iwata et al . [ 9 ] focus on the problem of modeling topic spreading on timelines with multiple resolutions , namely how topics can be organized in a hierarchical way over time .
As in traditional topic models , the effectiveness of temporal topic models is difficult to evaluate in general . This is partly because these models are introduced without considering any tasks , making the process of evaluating them on third party tasks ad hoc . Due to a lack of evaluation tasks , comprehensive comparisons between models are seldom conducted . In order to better illustrate how temporal topic models have been evaluated , we show them in Table 1 , according to the evaluation methods mentioned in papers . It is clear that temporal perplexity is a popular evaluation method . However , as pointed out in [ 6 ] , perplexity may not have correlations with the quality ( eg , coherence ) of latent topics . In addition , little is known , both theoretically and empirically , that a model achieving lower perplexity will perform better on real world applications which we care about . Besides perplexity , several papers proposed some ad hoc evaluation methods ( named under “ Ad hoc ” in the table ) to demonstrate the potential capabilities of their models , such as the coherence of topics measured by K L divergence , where these methods are not shared by other papers and are also not really task driven . Nearly all papers show “ anecdotal examples ” of what kind of topics are found over time .
Since our model can be considered as an extension to incorporate side information , or features into topic models , we also review other similar attempts . Basically , two kinds of side information might be considered : document level features and word level features . For document level features , models are proposed ( eg , [ 4 , 11 , 26 , 18 ] ) to incorporate them either conditioned on latent topic assignments or conditioned on per document hyper parameters . Either maximum conditional learning or max margin learning is employed for inference . For word level features , a recently proposed model [ 17 ] introduce a method to embed arbitrary word level features . Unlike the ones for document level features , this model is not a fully generative model and therefore we cannot easily infer these feature values .
αt
θ z w
βt
W
Dt
αt+1
θ z w
W
Dt+1
βt+1
K
K
Yt
π
Yt+1
V
V
V
Figure 1 : A graphical representation of the model with only two time epochs
3 . TRACKING TRENDS BY INCORPO
RATING VOLUMES
In this section , we will introduce the task of volume prediction as a case of trend tracking . One reason that temporal topic models are favored is perhaps that these models can be potentially used as a tool to analyze trends and changes of keywords over time . However , these tasks are never evaluated directly or seriously in current literature .
The task of predicting the volume of terms is to predict the numeric volume of one or a set of keywords , given the historical data of these keywords in the past . This is a natural extension of tracking and monitoring keywords over time . Indeed , some commercial products provide such tools to allow users to browse and understand the rise and fall of keywords , such as Google Trends . One drawback of existing tools is that people usually only have a limited view of certain topics in which they are interested before they fully understand these topics . For instance , for the event of “ World Cup ” , the phrase “ World Cup ” is certainly of interest . However , there are many more related terms to be explored , such as “ FIFA ” , “ South Africa ” and “ Ronaldo ” . Sometimes , users have these related terms in mind but usually they are unable to prepare them in advance . It would be great if users could track the trends ( volume ) of a topic as a whole and discover all those related terms at the same time . Moreover , the volume of terms in the same topic are correlated , which may help the model to find better topics . Overall , we would like to achieve three goals in tracking trends :
• Track and predict the volume of individual terms
• Obtain latent topics so that related terms can be grouped to gether
• Model the evolution of latent topics
The second goal will happen automatically through the modeling of topic models . The last goal can be achieved by temporal topic models , through either one of the assumptions mentioned in Section 2 . The first goal is the center of this work . We believe that our work would help to track the volume of topics as a whole if the first goal can be achieved . Note , in terms of “ prediction ” , we indicate the ability to estimate the volume of individual terms in the future where no documents are realized .
Two design issues need to be tackled when introducing term volumes into the model . First , they are word level variables ( if we treat features as random variables ) . Second , we need to predict values of these variables without documents . These two issues prevent these variables from being placed in the document plates , in terms of graphical modeling . This decision distinguishes our model from previous models ( eg , [ 4 , 11 , 26 , 18 ] ) where response variables are placed in document plates . Recently , Petterson et al . [ 17 ] demonstrate a technique to embed word level features into topic models . Although our work shares similar ideas to theirs , their model is not a generative model for word features but only for words in the documents . In addition , their work is not to predict these word level features . Since their work is for a static text corpus , it cannot be easily utilized to model temporal data . Therefore , we do not include this model in our experiments for comparison . Our model is a fully generative model for both word instantiations in documents and word level features .
Before we further go to the formal description of our model , we discuss some intuitions behind the model . In standard topic models , each word v is associated with many latent topics β1:K . Each topic βk is a distribution over all terms in the vocabuary V . Intuitively , the more a term appears in many topics , the more likely the term will have a high volume , such as some stop words and functional words . On the other hand , many terms only appear in a handful of topics and therefore these topics determine the volume of the term . If we think of β as another representation of terms , we would like to associate these latent variables with the term volumes . Following this intuition , we treat the volume of term v at time stamp t , denoted as Y ( t ) v , as a function of latent topics β . The simplest form of such functions is a linear function :
Y ( t ) v =
KXk=0
π(v,k)β(t )
( k,v ) + ǫv
( 1 ) where πv is a vector of coefficients , β(t ) ( k,v ) is the probability that the term is “ generated ” from topic k at time stamp t , and ǫv is a perterm “ error ” . In other words , the volume of a term v depends on its prevalence in all topics at that time point . If ǫv follows a normal distribution , namely ǫv ∼ N ( 0 , σ2 v ) , we can express the generation process of Y ( t ) in terms of a Normal distribution as follows :
V
Here , Y ( t ) we use the raw counts of term v at time epoch t as Y ( t ) v . is treated as a real valued variable . In our experiments , v
In order to obtain Yv at different time epochs , we need to have β for different time points . We mention two basic categories of approaches in Section 2 and here we adapt the first category , having a “ Markovian assumption ” on the evolution of topics over time . More specifically , topics β evolve according to a state space model and the documents with their words are “ generated ” by the corresponding topics in the same time epoch . Embedding these intuitions into the model , the generative process of the model is as follows :
Y ( t ) v
| π(v ) , β(t ) v β(t )
( ∗,v ) , σ2
( 2 )
( ∗,v ) ∼ NπT v
1 . For each topic k in K :
Draw topics β(t ) k | β(t−1 ) k
2 . For each term v in V :
Draw term volume Y ( t )
∼ Nβ(t−1 ) k v ∼ NπT v β(t )
, δ2I . ( ∗,v ) , σ2 .
3 . For each document d in time epoch t :
( a ) Draw θd ∼ Dir(α )
( b ) For each word n : i . Draw z(d,n ) ∼ Multi(θ ) . z ) ii . Draw w(d,n ) ∼ Multif ( β(t ) where function f maps the multinomial natural parameters to mean parameters . The graphical representation of the model is shown in Figure 1 . Note , the model can be easily extended in multiple ways . For instance , we can also allow the hyper parameters of topic proportions α to evolve over time , according to a different statespace model , as already mentioned in [ 3 ] . In addition , the simple state space model can be replaced by a Brownian motion model [ 20 ] , allowing arbitrary granularity of time series . We will explore these extensions in future work .
4 . VARIATIONAL INFERENCE WITH
KALMAN FILTERING
The central problem in topic modeling is posterior inference , ie , determining the distribution of the latent topic structure conditioned on the observed documents . In our case , the latent structures comprise the per document topic proportions θd , per word topic assignments z(d,n ) , the K sequences of topic distributions β(t ) and perterm coefficient vector πv for characterizing term volumes . Similar to many topic models , the true posterior is intractable [ 3 , 20 ] , meaning that we must appeal to an approximation . k
Several approximate inference approaches have been developed for topic models . The most widely used are variational inference ( eg , [ 5 , 3 , 20 ] ) and collapsed Gibbs sampling ( eg , [ 7 , 21] ) . As noted previously by others [ 3 , 20 ] , collapsed Gibbs sampling is not an option in the sequential setting because the distribution of words for each topic is not conjugate to the word probabilities . Therefore , we employ variational inference for the model .
The main idea behind variational inference is to posit a simple family of distributions over the latent variables , namely variational distributions , and to find the member of that family which is closest in Kullback Leibler divergence to the true posterior . Variational inference has been successfully adopted in temporal topic models ( eg , [ 3 , 15 , 20] ) .
For the model descried above , we adapt variational Kalman filtering [ 3 ] to the sequential modeling setting . We employ the following variational distribution : q(β1:T , θ , Z| ˆβ1:T , λ , Φ ) = k , · · · , βT k | ˆβ1 k , · · · , ˆβT k ) × q(β1
KYk=1 TYt=1 DtYd=1 q(θd|λd )
NdYn=1 q(z(d,n)|φ(d,n ) )
( 3 )
The variational parameters are a Dirichlet λd for the per document topic proportions , multinomials φ for each word ’s topic assignment , and ˆβ variables , which are “ observations ” to a Variational Kalman Filter . The central idea of the variational Kalman filter is that variational parameters are treated as “ observations ” in a common Kalman filter setting , while true parameters , here β(t ) , are treated as latent states of the model . By utilizing a Kalman filter , we can effectively estimate these “ latent states ” through “ observations ” .
More specifically , our state space model is :
β(t ) k | β(t−1 ) k
ˆβ(t ) k | β(t ) k k
∼ Nβ(t−1 ) , δ2I ∼ Nβt t I k , ˆδ2
( 4 ) k
The variational parameters are ˆβ(t ) and ˆδt . The key problem of Kalman filter is to derive the mean and variance for forward and backward equations , which can be used to calculate the lower bound in variational inference . Using the standard Kalman filter calculation , the forward mean and variance of the variational posterior are given by : mt k = E[βt
1:t k ] k| ˆβ ˆδ2
V t−1
= k + 1 − k + δ2 + ˆδ2!mt−1 k = Eh(βt k i k + δ2 + ˆδ2!(V t−1 = k + δ2 ) k − mt k ] ) | ˆβ
V t−1
V t
ˆδ2
1:t
ˆδ2 k + δ2 + ˆδ2! ˆβt
V t−1 k
( 5 ) with initial conditions specified by fixed m0 and V 0 . The backward recursion then calculates the marginal mean and variance of k given ˆβ βt as :
1:T k
1:T k ] k
| ˆβ
δ2 V t−1 k = E[βt−1 emt−1 k + δ2!mt−1 k + 1 − = k = Eh(βt−1 k − emt−1 eV t−1 k + V t−1 k + δ2!2eV t k V t−1
= V t−1
] ) | ˆβ k
V t−1 k
1:T k i k
δ2
] + δ2!emt k + δ2 ) k − ( V t−1
( 6 ) with initial conditions emT = mT and eV T = V T .
With these forward and backward equations in hand , we turn to calculate the following lower bound ( assuming Ω = {α , β , π , σ2} ) with the help of variational distributions introduced in Equation 5 : log P ( W , Y|Ω ) ≥ Eq[log p(β ) ] + Eq[log p(W , Z , θ|β , α ) ] +Eq[log p(Y|π , β , σ2 ) ] + H(q ) = Eq[log p(β ) ] + Eq[log p(W|Z , β ) ] + Eq[log p(Z|θ ) ] +Eq[log p(θ|α ) ] + Eq[log p(Y|π , β , σ2 ) ] + H(q )
( 7 ) where term H(q ) is the entropy . To tighten the above bound on the likelihood of the observations given by Jensen ’s inequality is equivalent to minimize KL divergence . In the above bound , the term Eq[log p(W , Z , θ|β , α ) ] is standard for topic models , when logistic normal distribution is applied to represent topics ( eg , [ 3 , 20 ] . The term Eq[log p(β ) ] is standard for temporal topic models , which utilize the Kalman filter as a sequantial modeling tool . The term Eq[log p(Y|π , β , σ2 ) ] can be calculated similarly to the document level response variables , introduced in [ 4 ] . We will discuss these expectations in detail .
For the first term of the last line in Equation 7 , we utilize the forward and backward equations introduced in Equation 6 and follow the similar steps in [ 3 ] :
Eq[log p(β ) ] = −
( log δ2 + log 2π )
V KT
2
−
1 2δ2
+
1 2δ2
TXt=1 KXk=1
1 δ2
KXk=1"emt k 2#− k − emt−1 TreV T TreV 0 k − k KXk=1
1 2δ2
TXt=1
KXk=1
TreV t k
For the second term in the same line , we have :
Eq[log p(W|Z , β ) ] =
NdXn=1 KXk=1 DtXd=1 TXt=1 exp(β(k,w′))i! φ(n,k)Eqh logXw′
−
KXk=1
( k,w )
φ(n,k)emt where the second line demonstrates the essential problem of nonconjugacy of using the logistic normal distribution for topics . In lower bound by introducing another variational parameter ζt and upper bound the negative log normalizer with a Taylor expansion as follows : order to calculate Eqh logPw′ exp(β(k,w′))i , we further obtain a Eq[exp(β(k,w′) ) ] exp(β(k,w′))i≤ ζ −1
Eqh logXw′ t Xw′
−1 + log(ζt )
Algorithm 1 : Variational inference with Kalman filtering . Initialize ˆβ randomly . while relative improvement in L > 0.00001 do
E step : for t = 1 to T do for i = 1 to D do
Update λd according to Equation 8 Update φd according to Equation 9
Update ζt according to Equation 10
M step : for v = 1 to V do
Update πv according to Equation 12 Update σ2 v according to Equation 13
Update ˆβ by using conjugate gradient descent where the expectation Eq[exp(β(k,w′) ) ] is the mean of a log normal distribution with the mean and variance obtained from the variational parameters , essentially Kalman Filters , in our case . For the third term of the last line in Equation 7 , we have :
By using the expectations with respect to variational distributions , we can optimize the variational parameters as follows . For perdocument parameters λ(d,k ) , per word parameters φn and per time epoch parameters ζt , we have similar update equations as standard topic models :
DtXd=1
NdXn=1
KXk=1
φ(n,k)hΨ(λ(d,k ) )
Eq[log p(Z|θ ) ] =
TXt=1 λ(d,k′)i
−Ψ KXk′=1 and for the fourth term , we have :
Eq[log p(θ|α ) ] =
TXt=1
DtXd=1( KXk=1 λ(d,j)i!+ log Γ KXk=1
( αk − 1)hΨ(λ(d,k ) ) log Γ(αk ) ) αk−
KXk=1
−Ψ KXj=1
For the last term in the same line , we have :
Eq[log p(Y ( t ) v |πv , β(t )
( ,v ) , σ2 ) ] = −
1 2 log 2π −
1 2 log σ2
2
−Y ( t ) v 2 KXj=1 KXi=1
1 2
−
+ v
1
σ2"Y ( t ) π(v,i)emt
KXk=1 ( i,v)emt
( k,v )
π(v,k)emt ( j,v)π(v,j)#
For the entropy term H(q ) , we have :
−H(q ) = Eq[log q(β| ˆβ ) ] + Eq[log q(θ|λ ) ] + Eq[log q(Z|Φ ) ]
DtXd=1
2
=
1 2
( k,v ) +
TXt=1
KXk=1 log 2π+
KXk=1 T TXt=1 VXv=1 logeV t λ(d,j)i! ( λ(d,k ) − 1)hΨ(λ(d,k ) ) − Ψ KXj=1 log Γ(λ(d,k) ) )
TXt=1 ( KXk=1 λ(d,k)− + log Γ KXk=1 NdXn=1 DtXd=1 TXt=1 KXk=1
φ(n,k ) log φ(n,k )
KXk=1
+
φ(n,k )
λ(d,k ) = αk +
NdXn=1 φ(n,k ) ∝ exp Ψ(λ(d,k ) ) − Ψ KXk′=1 exp emt ( k,w ) − Eqh logXw′ NdXn=1 KXk=1 DtXd=1 φ(n,k)Xw ( k,w)/2! +eV t
1 Nt
ζt =
( 8 )
λ(d,k′)!× exp(β(k,w′))i! ( 9 ) expemt
( k,w )
( 10 )
Since πv is a vector of coefficients across all time epochs T , we ( ∗,v ) from all time epochs and form a T × K matrix X gather the β ∗ where each row is a vector of β values discussed before . We can obtain the following equation by using the notation of X :
Eq[XT X]πv = Eq[X]T Yv
( 11 ) and therefore , we have
πv =Eq[XT X]−1
Eq[X]T Yv
( 12 ) where the tth row of Eq[X ] is just Eq[βt ( ,v) ] . Similar to linear regression but in the expected version , we can obtain the update equation for σ2 v as :
σ2 v =
1
T YT v
Yv − 2YT v
Eq[X]πv + πT v
Eq[XT X]πv! ( 13 ) where πv is the new estimate value .
The real computational hurdle is to calculate the updates of ˆβ . Gathering all terms in the lower bound involving β and differenti
2009
2007
2008
Table 2 : AR model on NIPS dataset Avg . p 96.17 92.00 90.39 94.81 106.03 108.66 111.75 111.79 111.54 122.76
99.42 91.06 97.00 95.98 108.33 108.34 117.50 116.72 115.85 124.40
90.51 83.20 77.31 75.62 91.64 99.00 98.99 95.93 96.23 100.71
98.57 101.72 97.66 112.83 118.10 118.65 118.76 122.73 122.55 143.17
1 2 3 4 5 6 7 8 9 10 ating them with respect to ˆβt
( k,v ) , we have :
+
−
−
( k,v )
( k,v )
( k,v )
∂ ˆβt
∂ ˆβt
1 δ2
N(t,v)φ(v,k)ζ −1
( k,v ) ∂emt TXt=1emt ( k,v ) − emt−1 TXt=1 N(t,v)φ(v,k ) − VXv=1 ( k,v)/2! ∂emt −" 1 KXi=1 KXj=1
( k,v)! ∂emt−1 expmt TXt=1 ∂emt ( j,v)π(v,j)# ∂emt ( i,v)emt
π(v,i)emt v π(v,k )
1 σ2
+V t
∂ ˆβt
∂ ˆβt
∂ ˆβt
2σ2
Y t
( k,v )
( k,v )
( k,v )
( k,v )
( k,v )
+ t
( k,v )
( k,v )
Unfortunately , no closed form solution for ˆβ can be found . We adapt optimization techniques to obtain a local optimum of the ˆβ values . In our experiments , we utilize the conjugate gradient algorithm implemented in GSL library1 , which requires us to provide the gradients . The forward backward equations for Eq can be used to derive a recurrence for the gradients . The forward recurrence is :
( k,v )
∂mt ∂ ˆβ(s )
( k,v )
ˆδ2
V t−1
= + 1 − k + δ2 + ˆδ2! ∂mt−1 k + δ2 + ˆδ2!I[s == t ]
V t−1
∂ ˆβs
( k,v )
ˆδ2 k k/∂ ˆβk s
= 0 . The backward recur k with the initial condition ∂m0 rence is then :
∂ ˆβk s = ∂emt + 1 − with the initial condition ∂emT k
δ2 V t−1 k + δ2! ∂mt−1 k + δ2! ∂mt
δ2 V t−1
∂ ˆβk s k ( s )
∂ ˆβk k /∂ ˆβk s k /∂ ˆβk
= ∂mT s
. We outline the overall inference algorithm in Algorithm ( 1 ) .
For prediction , since no documents are observed at test time , we initialize β values with their expected values , according to Equation 4 and then obtain the mean of the posterior distribution by the Kalman filter algorithm , as a standard problem . By using the learned π values , we could easily predict the volume of terms through Equation 1 .
5 . BASELINE MODELS
Time series analysis has been long studied in many fields . Here , we discuss the possibility to employ one traditional time series tool ,
Table 3 : AR model on ACL dataset 2009
2006
2007
2008
2005
131.85 210.74 247.73 258.74 244.41 250.49 169.25 168.54 155.96 156.59
524.04 316.38 248.17 246.58 223.99 297.98 328.75 332.20 326.73 355.13
39.57 106.31 104.72 114.23 53.12 42.74 51.14 51.58 47.11 49.15
592.91 434.15 381.84 447.71 428.17 385.26 345.98 396.08 400.96 399.28
126.29 181.98 140.87 166.09 185.00 209.24 262.54 291.13 291.60 310.65 p
1 2 3 4 5 6 7 8 9 10
Avg . 282.93 249.91 224.65 246.67 226.94 237.14 231.53 247.90 244.47 254.16 autoregressive model , to track the volume of terms . In univariate autoregressive model AR(p ) , a response Xt can depend on its previous values , ranging from Xt−1 to Xt−p :
Xt = w +
πkXt−k
( 14 ) pXk=1 pXk=1 where w is a constant and π is a vector of coefficients . Similar to linear regression , the aim of AR(p ) is to learn w and π , as well as the optimal choice of p , sometimes . If we treat the volume of each term as X , it is obvious that the volume of terms are independent with each other . A slightly more complicated model , Multivariate AutoRegressive model MAR(p ) , captures the correlations between M variables and preserves the simplicity of the model :
Xt = w +
AkXt−k
( 15 ) where X and w are both M dimensional vectors and each A is a M × M matrix , encoding the correlations . Although it first seems appealing , some limitations of the model prevent it from being applied in text mining scenarios . One of the drawbacks is that the model usually requires the number of variables to be smaller than the time stamps , which is not a problem in many traditional fields ( eg , temperature and humidity over time ) . However , in many text corpora , we wish to track thousands , or even millions of terms ( eg , in Twitter ) while the total number of time epochs to be measured is significantly smaller ( eg , in year , months , days ) . In that case , it is impossible to solve the Equation 15 , according to Neumaier and Schneider [ 16 ] . Therefore , we do not use MAR in our experiments . The second baseline used in experiments is Latent Dirichlet Allocation ( LDA ) [ 5 ] . We run LDA for the whole dataset . For each time epoch t , we obtain empirical topic distributions on t , βt . For each term v , we treat β(,v ) as features and Y ( t ) as the response , building a regression model on them . Note , this model is unrealistic because in reality , we cannot obtain empirical topic distributions from the test set due to the fact that no documents should be observed from the test set . However , we include this model in the experiments for the purpose to show that topic representations can help volume prediction . A more realistic state of the art model , DTM , is also used in the experiments . Like our model , β values on the test time epoch are estimated by the Kalman filter algorithm . Similar to LDA , the topic distributions obtained by DTM are treated as features and we build a regression model based upon these features . The regression model used in experiments is Support Vector Regression ( SVR ) , implemented in libSVM2 . v
1http://wwwgnuorg/software/gsl/
2http://wwwcsientuedutw/˜cjlin/libsvm/
E S M R
110
100
90
80
70
60
50
40
30
20
AR LDA DTM Our Model
2007
2008 Year
2009
E S M R
400
350
300
250
200
150
100
50 20
AR LDA DTM Our Model
2005
2006
2007 Year
2008
2009
Figure 2 : Performance comparison on the NIPS dataset . The best RMSE values achieved by each model are shown for the last three years .
Figure 4 : Performance comparison on the ACL dataset . The best RMSE values achieved by each model are shown for the last five years .
E S M R
75
70
65
60
55
50 0
LDA DTM Our Model
60
50
E S M R
40 0
10
50
100
150
Topics
200
250
300
350
E S M R
120 100 80 60 40 0
10
DTM Our Model
90
100
DTM Our Model
90
100
20 80 Percentage of Test Documents on NIPS dataset
30
40
50
60
70
20
80 Percentage of Test Documents on ACL dataset
30
40
50
60
70
Figure 3 : Performance comparison by varying the number of topics K on the NIPS dataset .
Figure 5 : Performance when a fraction of the test documents is provided to the model .
6 . EXPERIMENTS
Two datasets of scientific papers are used in our experiments . One is from the NIPS conference series . We downloaded all electronic copies of papers from online proceedings3 and converted into text format using pdftotext . We tokenize the converted files and keep the terms with frequency larger than 10 , resulting in to 38,029 distinct terms and 4,360 papers in total , spanning 24 years . The second dataset is from the 2009 release of The ACL Anthology4 , consisting of text format of papers published in the community of computational linguistics . This dataset has 14,590 papers with 74,189 distinct terms ( frequency more than 10 ) , ranging over 37 years . Both datasets have timelines that are long enough such that some topics have changed over time .
The major evaluation measure is of course the accuracy of the predicted volume of terms . In this work , we denote the estimated volume of term v at time stamp t as ˆY ( t ) v . Therefore , we measure the estimation error by calculating the Root Mean Square Error ( RMSE ) between estimated values and real values :
RMSEt =s 1
V Xv ˆY ( t ) v − Y ( t ) v 2
For both datasets , we adapt an “ incremental ” evaluation process , mimicking real application scenarios . In order to predict the volume at time t , we use the documents in all possible previous years for training . We sequentially train and test the model in multiple years and average the RMSE over these time periods . We conduct experiments on the last three years for the NIPS dataset and the
3http://booksnipscc/ 4http://clairsiumichedu/clair/anthology/ last five years for the ACL dataset . For hyper parameters , α is set to 50/K , δ2 is set to 0.1 and ˆδ2 is set to 1.0 , similar as [ 3 ] , for all experiments .
6.1 Volume Prediction
As discussed in Section 5 , the first baseline we consider is the AR model for terms . In our case , we essentially build an AR model for each term . Rather than choosing the optimal p by some criteria , such as Bayesian information criterion ( BIC)5 or Akaike information criterion ( AIC)6 , we simply show the predictive performance by varying p values . Therefore , it is possible that the optimal p value is out of the ranges demonstrated here . The results for the AR model on the NIPS dataset are shown in Table 2 and the results on the ACL dataset are shown in Table 3 , where the optimal performance is in bold . Several conclusions can be made regarding these results . First , for both datasets , the optimal performance is not always obtained on p = 1 , when the volume of terms only depends on the previous year . On average , p = 3 gives optimal performance on both datasets , meaning that the volume of terms in the year t depends on the previous three years . For the NIPS dataset , after the optimal point , the performance decreases as p increases , which indicates that for the AR model , no additional advantages can be obtained if we consider higher order dependencies on this particular dataset . This observation might also indicate that the latent relationships among terms , essentially topics , may change over time . Some new terms are introduced and some old concepts are outdated . For the ACL dataset , this is more complicated since the performance fluctuates significantly as p varies . Unlike the the
5http://enwikipediaorg/wiki/Bayesian_information_criterion 6http://enwikipediaorg/wiki/Akaike_information_criterion
LDA DTM Our Model
2400
2200
2000
1800
1600
1400
1200 l y t i x e p r e P
1000
20
50
100
150
200
Topics
250
300
350 l y t i x e p r e P
3200
3000
2800
2600
2400
2200
2000
LDA DTM Our Model
50
100
150
200
Topics
250
300
350
Figure 6 : Perplexity comparison on NIPS dataset .
Figure 7 : Perplexity comparison on ACL dataset .
NIPS dataset in which performance is relatively consistent over the recent three years , predictive performance on the ACL dataset differs significantly from year to year .
We run LDA , DTM and our model on both datasets while varying the number of topics , K . The results for the NIPS and the ACL datasets are shown in Figures 2 and 4 , respectively . For each model , we only report its best performance . In addition , for both datasets , we also compare these models to the best performance achieved by the AR model . Note , as we mentioned before , LDA is unrealistic since β values for the test years are from test documents while in reality these values should be estimated from the past , assuming no documents observed in these test years . However , the purpose of showing the results from plain LDA is to demonstrate that the volume predictive performance can be greatly improved by treating topic probabilities as features if we can obtain them “ correctly ” . For DTM and our model , these β values are estimated by the Kalman filter algorithm , mentioned in Section 3 , which do not depend on the test documents at all . The first observation is that the overall performance is significantly improved over the AR model , in general . LDA is usually , but not always , better than AR in terms of average performance . For DTM and our model , which both consider temporal smoothing on topics , the performance is consistently better than both LDA and AR . Our model is also better than DTM on both datasets not only in terms of average performance but also in terms of performance on individual years .
In order to better understand the performance of topic models , we plot the performance on different K values averaged over the test years for the NIPS dataset in Figure 3 . It is clear that performance is relatively stable compared to the AR model , where it is sensitive to the p value , shown in Table 2 . However , for all models , as K increases , the performance slightly decreases , indicating that a higher value of K may lead models to over fit . In any case , optimal performance is obtained from 50 70 topics for DTM and our model , which seems reasonable since NIPS is a relatively small research community and the topics are consistent over consecutive years . Similar conclusions can also be made for the ACL dataset .
Since DTM and our model prediction are performed on the year in which no documents are observed , it may be interesting to see whether performance would be improved if we partially observe the test documents . We pick the best K from the above experiments and feed a given fraction of test documents in a particular year to both models . The results are shown in Figure 5 . As expected , performance improves on both datasets for both models if we observe partial data . However , when around 30 % to 50 % of test documents are observed , performance stabilizes .
6.2 Temporal Perplexity
Although we argue in Section 2 that perplexity may not be an appropriate evaluation method for temporal topic models , or for topic models in general , we still provide a comparison of perplexity between LDA , DTM and our model . Note , the performance on perplexity might be misleading because this measure is to evaluate how words in the documents can be assessed . Therefore , we perform the standard steps to calculate perplexity on documents in test years . As mentioned earlier , the real performance of these models should be considered when test documents are not available and how reliably the models can predict the response variables , not words . We show perplexity on the NIPS and ACL datasets in Figures 6 and 7 , respectively . Overall , the perplexity values of DTM and our model are lower than LDA , for different K values , which confirms the observations in [ 3 , 20 ] . In addition , perplexity decreases as K increases in general , indicating that a larger K may explain words better . However , the difference of perplexity between DTM and our model is relatively small , compared to the volume predictive performance . This is not unexpected because our model shares the same “ generative ” process for words in documents as DTM . Therefore , this observation also confirms that perplexity may not be appropriate to truly reflect the performance of different models , in terms of the tasks we care about . However , we do believe that a thorough study of the relationships of perplexity and the performance of third party tasks for topic models is needed .
7 . CONCLUSION
In this paper , we introduced a real world task—tracking the volume of terms—to which temporal topic models can be applied . We proposed a new type of topic model incorporating the volumes of terms into the temporal dynamics of topics and directly optimize for the task . We combined state space models and the volume of terms in a supervised learning fashion which enables us to effectively predict the volume in the future . The volumes of latent topics are by products of our model , demonstrating the superiority of utilizing temporal topic models over traditional time series tools ( eg , autoregressive models ) to tackle this kind of problem . The proposed model can be further extended with arbitrary word level features which are evolving over time . We presented the results of applying the model to two datasets with long time periods and showed its effectiveness over non trivial baselines . Future work might include the adoption of recently developed online variational inference algorithms [ 8 ] to our model , enabling the processing of large scale datasets .
Acknowledgements
This material is based upon work supported in part by the National Science Foundation under Grant Numbers IIS 0545875 and IIS0803605 .
Transactions on Mathematical Software , 27:27–57 , March 2001 .
[ 17 ] J . Petterson , A . Smola , T . Caetano , W . Buntine , and
S . Narayanamurthy . Word features for latent dirichlet allocation . In J . Lafferty , C . K . I . Williams , J . Shawe Taylor , R . Zemel , and A . Culotta , editors , Advances in Neural Information Processing Systems 23 , pages 1921–1929 . 2010 .
[ 18 ] D . Ramage , D . Hall , R . Nallapati , and C . D . Manning .
Labeled LDA : A supervised topic model for credit attribution in multi labeled corpora . In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing , pages 248–256 , Singapore , August 2009 . Association for Computational Linguistics .
[ 19 ] C . Wang , D . Blei , and F F Li . Simultaneous image classification and annotation . IEEE Conference on Computer Vision and Pattern Recognition , 0:1903–1910 , 2009 .
[ 20 ] C . Wang , D . M . Blei , and D . Heckerman . Continuous time dynamic topic models . In Proceedings of the 24th Conference in Uncertainty in Artificial Intelligence ( UAI ) , 2008 .
[ 21 ] X . Wang and A . McCallum . Topics over time : a non Markov continuous time model of topical trends . In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2006 .
[ 22 ] X . Wang , C . Zhai , X . Hu , and R . Sproat . Mining correlated bursty topic patterns from coordinated text streams . In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2007 .
[ 23 ] X . Wang , K . Zhang , X . Jin , and D . Shen . Mining common topics from multiple asynchronous text streams . In Proceedings of the Second ACM International Conference on Web Search and Data Mining ( WSDM ) , 2009 .
[ 24 ] X . Wei and W . B . Croft . LDA based document models for ad hoc retrieval . In Proceedings of the 29th annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 178–185 , New York , NY , USA , 2006 . ACM .
[ 25 ] J . Zhang , Y . Song , C . Zhang , and S . Liu . Evolutionary hierarchical Dirichlet processes for multiple correlated time varying corpora . In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2010 .
[ 26 ] J . Zhu , A . Ahmed , and E . P . Xing . MedLDA : maximum margin supervised topic models for regression and classification . In Proceedings of the 26th Annual International Conference on Machine Learning ( ICML ) , pages 1257–1264 , New York , NY , USA , 2009 . ACM .
8 . REFERENCES [ 1 ] D . Agarwal and B C Chen . fLDA : matrix factorization through Latent Dirichlet Allocation . In Proceedings of the third ACM International Conference on Web Search and Data Mining ( WSDM ) , pages 91–100 , New York , NY , USA , 2010 . ACM .
[ 2 ] A . Ahmed and E . P . Xing . Timeline : A dynamic hierarchical
Dirichlet process model for recovering birth/death and evolution of topics in text stream . In Proceedings of the 26th International Conference on Conference on Uncertainty in Artificial Intelligence ( UAI ) , 2010 .
[ 3 ] D . M . Blei and J . D . Lafferty . Dynamic topic models . In
Proceedings of the 23rd International Conference on Machine Learning ( ICML ) , 2006 .
[ 4 ] D . M . Blei and J . D . Mcauliffe . Supervised topic models . In
Advances in Neural Information Processing Systems 21 , 2007 .
[ 5 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent Dirichlet
Allocation . Journal of Machine Learning Research , 3:993–1022 , 2003 .
[ 6 ] J . Boyd Graber , J . Chang , S . Gerrish , C . Wang , and D . Blei . Reading Tea Leaves : How Humans Interpret Topic Models . In Neural Information Processing Systems ( NIPS ) , 2009 . [ 7 ] T . L . Griffiths and M . Steyvers . Finding scientific topics . Proceedings of the National Academy of Sciences of the United States of America , 2004 .
[ 8 ] M . Hoffman , D . Blei , and F . Bach . Online learning for latent dirichlet allocation . In J . Lafferty , C . K . I . Williams , J . Shawe Taylor , R . Zemel , and A . Culotta , editors , Advances in Neural Information Processing Systems 23 , pages 856–864 , 2010 .
[ 9 ] T . Iwata , T . Yamada , Y . Sakurai , and N . Ueda . Online multiscale dynamic topic models . In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2010 .
[ 10 ] N . Kawamae and R . Higashinaka . Trend detection model . In Proceedings of the 19th International Conference on World Wide Web ( WWW ) , pages 1129–1130 , New York , NY , USA , 2010 . ACM .
[ 11 ] S . Lacoste Julien , F . Sha , and M . I . Jordan . DiscLDA :
Discriminative Learning for Dimensionality Reduction and Classification . In Advances in Neural Information Processing Systems ( NIPS ) , 2008 .
[ 12 ] T . Masada , D . Fukagawa , A . Takasu , T . Hamada , Y . Shibata , and K . Oguri . Dynamic hyperparameter optimization for Bayesian topical trend analysis . In Proceeding of the 18th ACM Conference on Information and Knowledge Management ( CIKM ) , 2009 .
[ 13 ] Q . Mei , C . Liu , H . Su , and C . Zhai . A probabilistic approach to spatiotemporal theme pattern mining on weblogs . In Proceedings of the 15th International Conference on World Wide Web ( WWW ) , 2006 .
[ 14 ] Q . Mei and C . Zhai . Discovering evolutionary theme patterns from text : an exploration of temporal text mining . In Proceedings of the eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining , 2005 .
[ 15 ] R . M . Nallapati , S . Ditmore , J . D . Lafferty , and K . Ung . Multiscale topic tomography . In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2007 .
[ 16 ] A . Neumaier and T . Schneider . Estimation of parameters and eigenmodes of multivariate autoregressive models . ACM
