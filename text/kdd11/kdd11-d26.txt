Brain Effective Connectivity Modeling for Alzheimer ’s
Disease by Sparse Gaussian Bayesian Network
Shuai Huang1 , Jing Li1 , Jieping Ye1 , Adam Fleisher2 , Kewei Chen2 , Teresa Wu1 , Eric Reiman2 1School of Computing , Informatics , and Decisions Systems Engineering , Arizona State University , Tempe , AZ ,
2Banner Alzheimer ’s Institute , Banner Good Samaritan Medical Center , Phoenix , AZ , 85006
85287
ABSTRACT Recent studies have shown that Alzheimer's disease ( AD ) is related to alteration in brain connectivity networks . One type of connectivity , called effective connectivity , defined as the directional relationship between brain regions , is essential to brain function . However , there have been few studies on modeling the effective connectivity of AD and characterizing its difference from normal controls ( NC ) . In this paper , we investigate the sparse Bayesian Network ( BN ) for effective connectivity modeling . Specifically , we propose a novel formulation for the structure learning of BNs , which involves one L1 norm penalty term to impose sparsity and another penalty to ensure the learned BN to be a directed acyclic graph – a required property of BNs . We show , through both theoretical analysis and extensive experiments on eleven moderate and large benchmark networks with various sample sizes , that the proposed method has much improved learning accuracy and scalability compared with ten competing algorithms . We apply the proposed method to FDG PET images of 42 AD and 67 NC subjects , and identify the effective connectivity models for AD and NC , respectively . Our study reveals that the effective connectivity of AD is different from that of NC in many ways , including the global scale effective connectivity , intra lobe , interlobe , and inter hemispheric effective connectivity distributions , as well as the effective connectivity associated with specific brain regions . These findings are consistent with known pathology and clinical progression of AD , and will contribute to AD knowledge discovery . Categories and Subject Descriptors H28 [ Database Management ] : Database Applications – Data Mining ; J.3 [ Life and Medical Sciences ] : Health , Medical information systems General Terms Algorithms Keywords Brain network , Alzheimer ’s disease , neuroimaging , FDG PET , Bayesian network , sparse learning ie , two interact in clinical practice . There are
1 . INTRODUCTION Alzheimer ’s disease ( AD ) is the most common cause of dementia and the fifth leading cause of death in people over 65 in the US . The current annual cost of AD care in the US is more than $100 billion , which will continue to grow fast . The existing knowledge about the cause of AD is very limited . Clinical diagnosis is imprecise with a definite diagnosis only possible by autopsy . Also , there is currently no cure for AD , while most drugs only modestly alleviate symptoms . To tackle these challenging issues in AD studies , fast advancing neuroimaging techniques hold great promise . Recent studies have shown that neuroimaging can provide sensitive and reliable measures of AD onset and progression , which can complement the conventional clinical based assessments and cognitive measures . In neuroimaging based AD research , one important area is brain connectivity modeling , identification of how different brain regions to produce a cognitive function in AD , compared with normal aging . Research in this area can substantially promote AD knowledge discovery and identification of novel connectivity based AD biomarkers to be used types of connectivity being studied : functional connectivity refers to the covarying pattern of different brain regions ; effective connectivity refers to the directional relationship between regions [ 1 ] . A vast majority of the existing research focuses on functional connectivity modeling . Various methods have been adopted such as correlation analysis [ 2 ] , Principal Component Analysis ( PCA ) [ 3 ] , PCA based Scaled Subprofile Model [ 4 ] , Independent Component Analysis [ 5 ] , and Partial Least Squares [ 6 ] . Recently , sparse models have also been introduced , vector autoregressions [ 7 ] and sparse inverse covariance estimation [ 8 ] . Sparse models have shown great effectiveness because neuroimaging datasets are featured by "small n large p" , ie , the number of AD patients ( n ) can be close to or less than the number of brain regions modeled ( p ) . Also , many past studies based on anatomical brain databases have shown that the true brain network is indeed sparse [ 9 ] . Compared with functional connectivity modeling , effective connectivity modeling has the advantage of helping identify the pathway/mechanism whereby distinct brain regions communicate with each other . However , the existing research in effective connectivity modeling is much less extensive . Models that have been adopted include structural equation models [ 10 ] and dynamic causal models [ 11 ] . The limitations of these models include ( i ) they are confirmative , rather than such as sparse multivariate or
931 them inappropriate limitations make explanatory , ie , they require a prior model of connectivity to start with ; ( ii ) they require a substantially larger sample size than the number of regions modeled ; a typical number of regions included is less than 10 given the sample size limit . These for AD connectivity modeling , because there is little prior knowledge of which regions should be included and how they are connected . We propose sparse Bayesian Networks ( BN ) for effective connectivity modeling . A BN is an explanatory model and the sparse estimation makes it possible to include a large number of brain regions . In a BN representation of effective connectivity , the nodes are brain regions . A directed arc from node ( cid:1850 ) to ( cid:1850 ) ( (cid:1850 ) is called a parent of ( cid:1850 ) ) indicates a direct influence from ( cid:1850 ) to ( cid:1850 ) . Sparsity consideration has been these existing algorithms employ a common in the BN learning literature . For example , some early work used score functions , such as BIC and MDL , to measure the goodness of fit of a BN , in which a penalty term on the model complexity is usually included in the score [ 12 ] . Recently , driven by modern applications such as genetics , learning of large scale BNs has been very popular , in which sparsity consideration is indispensible . The Sparse Candidate ( SC ) algorithm [ 13 ] , one of the first BN structure learning algorithms to be applied to a large number of variables , assumes that the maximum number of parents for each node is limited to a small constant . The L1MB DAG algorithm developed in [ 14 ] uses LASSO [ 15 ] to identify a small set of potential parents for each variable . Some other algorithms , such as the Max Min Hill Climbing ( MMHC ) [ 16 ] , GrowShrink [ 17 ] , TC and TC_bw [ 18 ] , all follow the same line . Most of two stage approach : Stage 1 is to identify the potential parents of each variable ; Stage 2 usually applies some heuristic search algorithms ( eg , hill climbing ) or orientation methods ( eg , Meek's rules [ 19 ] ) to identify the parents out of the potential parent set . An apparent weakness of the two stage approach is that if a true parent is missed in Stage 1 , it will never be recovered in Stage 2 . Another weakness of the existing algorithms is computational efficiency , ie , it may take hours or days to learn a large scale BN such as one with 500 nodes . In this paper , we propose a new sparse BN learning algorithm , which is called SBN . It is a one stage approach that identifies the parents of all variables directly . The main contributions of this paper include :  We propose a novel sparse BN learning algorithm , ie , SBN , using one L1 norm penalty term to impose sparsity and another penalty term to ensure the learned BN is a directed acyclic graph ( DAG ) .
 We present theoretical guidance on how to select the regularization parameter associated with the second penalty .
 We perform theoretical analysis to reason why the two stage approach popularly adopted in the existing literature has a high risk of failing to identify the true parents . Also , we conduct extensive experiments on synthetic data to compare SBN and the existing algorithms in terms of the learning accuracy and scalability .
 We apply SBN to FDG PET data of 42 AD patients and 67 normal controls ( NC ) subjects enrolled in the ADNI ( Alzheimer's Disease Neuroimaging Initiative ) project , and identify the effective connectivity models for AD and NC .
Our study reveals that the effective connectivity of AD is different from NC in many ways , including the global scale effective connectivity , interhemisphere effective connectivity distributions , as well as the effective connectivity associated with specific brain regions . The findings are consistent with known pathology and clinical progression of AD . inter lobe , and intra lobe ,
 
X1
X3
X4
X2
5X there
Figure 1 . A BN structure called spouses if they share a common child . If is a independent of all other variables . The MB includes the parents , also a directed path and a parent is also an ancestor according to this definition . The Markov Blanket and directed arcs between some nodes ; no cycle is allowed in a DAG . Each node represents a random variable . If there is a
2 . BAYESIAN NETWORK : KEY DEFINITIONS AND CONCEPTS This section introduces the key definitions and concepts of BNs that are relevant to this paper : A BN is composed of a structure and a set of parameters . The structure ( Fig 1 ) is a DAG that consists of ( cid:1868 ) nodes ( cid:3427)(cid:1850),…,(cid:1850)(cid:3043)(cid:3431 ) directed arc from ( cid:1850 ) to ( cid:1850 ) , ( cid:1850 ) is called a parent of ( cid:1850 ) and ( cid:1850 ) is called a child of ( cid:1850 ) . Two nodes are directed path from ( cid:1850 ) to ( cid:1850 ) , ie , ( cid:1850)(cid:1372)(cid:1710)(cid:1372)(cid:1850 ) , ( cid:1850 ) is called an ancestor of ( cid:1850 ) . A directed arc is ( MB ) of ( cid:1850 ) is a set of variables given which ( cid:1850 ) will be children , and spouses of ( cid:1850 ) . structure : Denote the structure by a ( cid:1868)(cid:3400)(cid:1868 ) matrix ( cid:1781 ) , with entry ( cid:1781)1 representing a directed arc from ( cid:1850 ) to ( cid:1850 ) . The set of parents of a node ( cid:1850 ) is denoted by ( cid:1790)(cid:1775)(cid:4666)(cid:1850)(cid:4667)(cid:3427)(cid:1850)(cid:3117),…,(cid:1850)(cid:3286)(cid:3431 ) addition , we define a ( cid:1868)(cid:3400)(cid:1868 ) matrix , ( cid:1790 ) , which records all the ( cid:1850 ) to ( cid:1850 ) , entry ( cid:1790)1 ; otherwise , ( cid:1790)0 . type parameterization can be adopted , ie , ( cid:1850)(cid:2746)(cid:1790)(cid:1775)(cid:4666)(cid:1850)(cid:4667)(cid:3397 ) with ~(cid:1840)(cid:4666)0,(cid:2026)(cid:2870)(cid:4667 ) and ( cid:2746)(cid:3427)(cid:2010)(cid:3117),(cid:1710),(cid:2010)(cid:3286)(cid:3431 ) of a BN are ( cid:1776)(cid:3427)(cid:2746),…,(cid:2746)(cid:2926)(cid:3431 ) . Without loss of generality , we mean and unit variance . This means , if using ( cid:2206)(cid:4670)(cid:1876),…,(cid:1876)(cid:4671 ) to denote the sample vector for ( cid:1850 ) , and to denote the sample size , we have ∑ 0 and ( cid:2206)(cid:2206)1 . ( cid:2880 )
In addition to the structure , other important components of a BN are the parameters . The parameters are the conditional probability distribution of each node given its parents . Specifically , when the nodes follow a multivariate Gaussian distribution , a regression
In this paper , we use the following notations with respect to a BN assume that the nodes are standardized , ie , each with a zero directed paths in the structure , ie , if there is a directed path from
. Then , the parameters
( cid:1876 )
. In
3 . THE PROPOSED SPARSE BN STRUCTURE LEARNING ALGORITHM One of the challenging issues in BN structure learning is to ensure that the learned structure must be a DAG , ie , no cycle is present . To achieve this , we first identify a sufficient and necessary condition for a DAG , which is given as Lemma 1 below . Lemma 1 . A sufficient and necessary condition for a DAG is
( cid:2010)(cid:3400)(cid:1790)0 for every pair of nodes ( cid:1850 ) and ( cid:1850 ) .
Proof . To prove the necessary condition , suppose that a BN structure ,
932 ( cid:3397)(cid:2019)(cid:1313)(cid:2746)(cid:1313 )
Based on Lemma 1 , we further present our formulation of the sparse BN structure learning . It is an optimization problem with the objective function and constraints given by :
( cid:1781 ) , is a DAG . Let ’s assume that ( cid:2010)(cid:3400)(cid:1790)(cid:3405)0 for a pair of nodes ( cid:1850 ) and ( cid:1850 ) . Then , there exists a directed path from ( cid:1850 ) to ( cid:1850 ) and a directed path from ( cid:1850 ) to ( cid:1850 ) , ie , there is a cycle in ( cid:1781 ) , which is a contradiction to our presumption that ( cid:1781 ) is a DAG . To prove the sufficient condition , suppose that ( cid:2010)(cid:3400)(cid:1790)0 for every pair of nodes ( cid:1850 ) and ( cid:1850 ) . If ( cid:1781 ) is not a DAG , ie , there is a cycle , it means that there exist two variables , ( cid:1850 ) and ( cid:1850 ) , with a directed arc from ( cid:1850 ) to ( cid:1850 ) ( (cid:2010)(cid:3405)0 ) and a directed path from ( cid:1850 ) to ( cid:1850 ) ( (cid:1790)1 ) . This contradicts with our assumption that ( cid:2010)(cid:3400)(cid:1790)0 for every pair of nodes ( cid:1850 ) and ( cid:1850 ) . □ ( cid:1776)(cid:3553)(cid:1865)(cid:1861)(cid:1776)∑ ( cid:4682)(cid:3435)(cid:2206)(cid:2746)(cid:2206)/(cid:3439)(cid:3435)(cid:2206)(cid:2746)(cid:2206)/(cid:3439 ) ( cid:4683 ) ( cid:3043)(cid:2880 ) ( cid:2010)(cid:3400)(cid:1790)0,(cid:1861),1,…,(cid:1868),(cid:1861)(cid:3405 ) . The notations are explained as follows : ( cid:2206)(cid:4670)(cid:1876),…,(cid:1876)(cid:4671 ) denotes the sample vector for ( cid:1850 ) , where n is the sample size . ( cid:2206)/ denotes the sample matrix for all the variables except ( cid:1850 ) .The first term in the objective function , ∑ ( cid:4676)(cid:3435)(cid:2206)(cid:2746)(cid:2206)/(cid:3439)(cid:3435)(cid:2206)(cid:2746)(cid:2206)/(cid:3439)(cid:4677 ) ( cid:3043)(cid:2880 ) likelihood to measure the model fit . The second term , ( cid:1313)(cid:2746)(cid:1313 ) , is the sum of the absolute values of the elements in ( cid:2746)(cid:2191 ) and thus is the socalled L1 norm penalty [ 15 ] . The regularization parameter , ( cid:2019 ) , controls the number of nonzero elements in the solution to ( cid:2746 ) , ( cid:2746)(cid:3553 ) ; larger ( cid:2019 ) , less nonzero elements . Because less nonzero elements in ( cid:2746)(cid:3553 ) correspond to fewer arcs in the learned BN structure , a larger ( cid:2019 ) We remark that these constraints are functions of ( cid:1776 ) only , since ( cid:1790)expm(cid:4666)(cid:1781)(cid:4667 ) [ 20 ] . Here , expm(cid:4666)(cid:1781)(cid:4667 ) is the matrix exponential of ( cid:1781 ) . ( cid:1776)(cid:3553)(cid:3028)(cid:3043)min(cid:1776)∑ ( cid:1858)(cid:4666)(cid:2236)(cid:4667 )
( cid:3043)(cid:2880 ) ( cid:3435)(cid:2206)(cid:2746)(cid:2206)/(cid:3439)(cid:3435)(cid:2206)(cid:2746)(cid:2206)/(cid:3439 ) ( cid:1865)(cid:1861)(cid:1776)∑ ( cid:3421 ) ( cid:3425 ) ( cid:3397)(cid:2019)(cid:1313)(cid:2746)(cid:1313)(cid:3397 ) ( cid:2019)(cid:2870)∑ ( cid:3627)(cid:2010)(cid:3400)(cid:1790)(cid:3627 ) ( cid:3043)(cid:2880 ) ( cid:1679)(cid:3025)/(cid:3284 ) where ( cid:1679)(cid:1850)/ denotes that the variable indexed by , ie , ( cid:1850 ) is a variable different from ( cid:1850 ) . Here , ( cid:2019)(cid:2870)∑ ( cid:3627)(cid:2010)(cid:3400)(cid:1790)(cid:3627 ) ( cid:2010)(cid:3400)(cid:1790 ) to become zero . Under some mild conditions , there exists a ( cid:1679)(cid:3025)/(cid:3284 ) ( cid:2019)(cid:2870)(cid:1499 ) such that for all ( cid:2019)(cid:2870)(cid:3410)(cid:2019)(cid:2870)(cid:1499 ) , ( cid:1776)(cid:3553)(cid:3028)(cid:3043 ) is also a minimizer for ( 1 ) [ 21 ] . Theorem 1 gives a practical estimation for ( cid:2019)(cid:2870)(cid:1499 ) . Theorem 1 . Any ( cid:2019)(cid:2870)(cid:3408)(cid:4666)1(cid:4667)(cid:2870)(cid:1868 ) ( cid:2019)⁄ ( cid:2019 ) will guarantee ( cid:1776)(cid:3553)(cid:3028)(cid:3043 ) to ( cid:2019 ) and any value of ( cid:2019)(cid:2870 ) , ( cid:1776)(cid:3553)(cid:2911)(cid:2926 ) is bounded . This can be seen ( cid:2019)(cid:2746)(cid:3553)(cid:4672)(cid:2206)(cid:2746)(cid:3553)(cid:2206)/(cid:4673)(cid:4672)(cid:2206)(cid:2746)(cid:3553)(cid:2206)/(cid:4673)(cid:3397)(cid:2019)(cid:2746)(cid:3553)(cid:3397 ) ( cid:2206)(cid:2206)1 , for each ( cid:2746)(cid:3553 ) . The ( cid:3627)(cid:2010)(cid:4632)(cid:3400)(cid:1790)(cid:3627 ) ( cid:2019)(cid:2870)∑ inequality holds because ( cid:2206)(cid:2206 ) is the value of the function on the ( cid:1679)(cid:3025)/(cid:3284 ) left hand side with ( cid:2746)0 , which is obviously larger than the function value with ( cid:2746)(cid:2746)(cid:3553 ) . The last equality holds since we have max(cid:3038)(cid:1679)(cid:1790)(cid:1775)(cid:4666)(cid:3025)(cid:3284)(cid:4667)(cid:3627)(cid:2010)(cid:4632)(cid:3038)(cid:3627)(cid:4666)1(cid:4667)(cid:2019)⁄ .
Solving the constrained optimization in ( 1 ) is difficult . Therefore , the penalty method [ 21 ] is employed to transform it into an unconstrained optimization problem , through adding an extra L1 norm penalty into the original objective function , ie , results in a sparser structure . In addition , the constraints are to assure that the learned BN is a DAG ( see Lemma 1 and Theorem 1 below ) .
Proof . To prove this we first need to show that with a certain value of variables . Thus , we that proof by
, ( 2 )
Now , we standardized is to push be a DAG . know from last use all the optimality slowly . This is because that the unconstrained problem in ( 2 ) may be we will get a minimizer of ( 1 ) through solving ( 2 ) . However , in contradiction to show that , with any ( cid:2019)(cid:2870)(cid:3408)(cid:4666)1(cid:4667)(cid:2870)(cid:1868 ) ( cid:2019)⁄ ( cid:2019 ) , we will get a DAG . Suppose that such a ( cid:2019)(cid:2870 ) doesn’t guarantee a DAG . Then , there must be at least a pair of variables ( cid:1850 ) and ( cid:1850 ) with ( cid:2010)(cid:3400)(cid:1790)(cid:3405)0 , ie , ( cid:2010)(cid:3405)0 and ( cid:1790)1 . Based on the first order ( cid:3627)(cid:3435)(cid:2206)(cid:2746)(cid:3553)/ ( cid:2206)/(cid:4666),(cid:4667)(cid:3439)(cid:2206)(cid:3627 ) condition , ( cid:2010)(cid:3405)0 iff ( cid:3435)(cid:2019)(cid:3397)(cid:2019)(cid:2870)(cid:3627)(cid:1790)(cid:3627)(cid:3439)(cid:3408)0 . Here , ( cid:2746)(cid:3553)/ denotes the elements in ( cid:2746)(cid:3553)without ( cid:2010)(cid:4632 ) , and ( cid:2206)/(cid:4666),(cid:4667)denotes the sample matrix for all the variables except ( cid:1850 ) and ( cid:1850 ) . However , we have ( cid:3627)(cid:3435)(cid:2206)(cid:2746)(cid:3553)/ ( cid:2206)/(cid:4666)(cid:1861),(cid:4667)(cid:3439)(cid:2206)(cid:3627)(cid:3627)(cid:2206)(cid:2206)(cid:3627)(cid:3397)∑ ( cid:3627)(cid:2010)(cid:4632)(cid:3038)(cid:2206)(cid:3038)(cid:2206)(cid:3627 ) ( cid:4666)1(cid:4667)(cid:1868)max(cid:3038)(cid:1679)(cid:1850)/(cid:1861)(cid:2010)(cid:4632)(cid:3038)(cid:4666)1(cid:4667)(cid:2870)(cid:1868 ) ( cid:2019)⁄ , ( cid:3038)(cid:1679)X/(cid:4666)(cid:1861),(cid:4667 ) which results in ( cid:3627)(cid:3435)(cid:2206)(cid:2746)(cid:3553)/ ( cid:2206)/(cid:4666)(cid:1861),(cid:4667)(cid:3439)(cid:2206)(cid:3627)(cid:3435)(cid:2019)(cid:3397)(cid:2019)(cid:2870)(cid:3627)(cid:1790)(cid:3627)(cid:3439)0.□ Theorem 1 implies that if we specify any ( cid:2019)(cid:2870)(cid:3408)(cid:4666)1(cid:4667)(cid:2870)(cid:1868 ) ( cid:2019)⁄ ( cid:2019 ) , ( 1 ) practice , directly solving ( 2 ) by specifying a large ( cid:2019)(cid:2870 ) may converge ill conditioned with a too large value for ( cid:2019)(cid:2870 ) [ 40 ] . To avoid this , the way : first , it specifies a series of values for ( cid:2019)(cid:2870 ) , ie , ( cid:2019)(cid:2870)(cid:2019)(cid:2870)(cid:2019)(cid:2870)(cid:2870 ) ( cid:1710)(cid:2019)(cid:2870)(cid:3014 ) , where ( cid:2019)(cid:2870 ) is small and ( cid:2019)(cid:2870)(cid:3014)(cid:3408)(cid:4666)1(cid:4667)(cid:2870)(cid:1868 ) ( cid:2019)⁄ ( cid:2019 ) ; next , it optimizes ( 2 ) with ( cid:2019)(cid:2870)(cid:2019)(cid:2870 ) to get a minimizer ( cid:1776)(cid:3553)(cid:3028)(cid:3043 ) , using an arbitrary initial value ; then , it optimizes ( 2 ) with ( cid:2019)(cid:2870)(cid:2019)(cid:2870 ) , using ( cid:1776)(cid:3553)(cid:3028)(cid:3043 ) as an initial value ; this process iterates , until it optimizes ( 2 ) with ( cid:2019)(cid:2870)(cid:2019)(cid:2870)(cid:3014 ) . Given ( cid:2019 ) and ( cid:2019)(cid:2870 ) , the BCD algorithm [ 22 ] can be employed to solve ( 2 ) . The BCD algorithm updates each ( cid:2746)(cid:2191 ) iteratively , assuming that all optimizing ( cid:1858)(cid:4666)(cid:2746)(cid:2191)(cid:4667 ) iteratively and the algorithm will terminate when some convergence conditions are satisfied . We remark that ( cid:1858)(cid:4666)(cid:2746)(cid:2191)(cid:4667 ) , ( cid:1858)(cid:4666)(cid:2746)(cid:2191)(cid:4667)(cid:3435)(cid:2206)(cid:2746)(cid:2191)(cid:2206)/(cid:3439)(cid:3435)(cid:2206)(cid:2746)(cid:2191)(cid:2206)/(cid:3439 ) ( cid:3397)∑ ( cid:1679)(cid:3025)/(cid:3284 ) optimize ( cid:1858)(cid:4666)(cid:2746)(cid:2191)(cid:4667 ) in each iteration . Note that at each iteration for optimizing ( cid:1858)(cid:4666)(cid:2746)(cid:2191)(cid:4667 ) , we also need to calculate ( cid:1790 ) for ( cid:1679)(cid:1850)/ . This can be done by a Breadth first search on ( cid:1781 ) with ( cid:1850 ) being the root node . A
With the last minimizer as the initial value for the next optimization problem , this method can be quite efficient .
( cid:3435)(cid:2019)(cid:3397)(cid:2019)(cid:2870)(cid:3627)(cid:1790)(cid:3627)(cid:3439)(cid:3627)(cid:2010)(cid:3627 )
“ warm start ” method [ 21 ] can be used , which works in the following
As a result , the shooting algorithm [ 23 ] for LASSO may be used to other parameters are fixed . In our situation , this is equivalent to after some transformation , is similar to LASSO [ 15 ] , ie ,
. ( 3 )
, is a profile
Repeat more detailed description of the BCD algorithm used to solve ( 2 ) is given in Figure 2 .
Input : sample matrix , ( cid:1798 ) ; ( cid:4668)(cid:2019)(cid:4669)(cid:2880),(cid:2870 ) ; initial ( cid:1776 ) ; Initialize : Let 0 ; For ( cid:1861)1,2,…,(cid:1868 ) A Breadth first search on ( cid:1781 ) with ( cid:1850 ) being the root node to calculate ( cid:1790 ) for 1,…,(cid:1868 ) . ( cid:1858)(cid:4666)(cid:2746)(cid:4667 ) and get ( cid:2746)(cid:3047 ) ; ( cid:3435)(cid:2010)(cid:3400)(cid:1790)(cid:3439)(cid:2870 )
( cid:2019)(cid:2870)∑ that it is a differentiable function of ( cid:2010 ) . Also , as shown in [ 21 ] , ( cid:1679)(cid:3025)/(cid:3284 )
End for Until converge Figure 2 . The BCD algorithm used for solving ( 2 ) the L2 norm penalty , to mention , might also be used in ( 2 ) . The advantage is
Use the shooting algorithm to optimize
Finally , we want that
933 norm penalty , compared with the L1 norm penalty , is that there is no
( cid:2010)(cid:3400)(cid:1790)(cid:1372)0 when ( cid:2019)(cid:2870)(cid:1372)∞ . However , the limitation of the L2guarantee that a finite ( cid:2019)(cid:2870 ) exists to assure ( cid:2010)(cid:3400)(cid:1790)0 for all pairs of ( cid:1850 ) and ( cid:1850 ) . shooting algorithm and a Breadth first search on ( cid:1781 ) . These two operations cost ( cid:1841)(cid:4666)(cid:1868)(cid:4667 ) [ 24 ] and ( cid:1841)(cid:4666)(cid:1868)(cid:3397)|(cid:1781)|(cid:4667 ) , respectively . Here |(cid:1781)| is the number of nonzero elements in ( cid:1781 ) . If ( cid:1781 ) is sparse , ie , |(cid:1781)|(cid:1829)(cid:1868 ) with a small constant ( cid:1829 ) , ( cid:1841)(cid:4666)(cid:1868)(cid:3397)|(cid:1781)|(cid:4667)(cid:1841)(cid:4666)(cid:1868)(cid:4667 ) . Thus , the computational cost at each iteration is only ( cid:1841)(cid:4666)(cid:1868)(cid:4667 ) . Furthermore , each sweep through all columns of ( cid:1776 ) costs ( cid:1841)(cid:4666)(cid:1868)(cid:2870)(cid:4667 ) . Our
Time complexity analysis of the proposed algorithm : Each iteration of the BCD algorithm consists of two operations : a that employ a simulation study shows that it usually takes no more than 5 sweeps to converge . 4 . THEORETICAL ANALYSIS OF THE COMPETITIVE ADVANTAGE OF THE PROPOSED SBN ALGORITHM Simulation studies in Sec 5 will show that SBN is more accurate than various existing algorithms two stage approach . This section aims to provide some theoretical insights about why it is so . Recall that Stage 1 of the two stage approach is to identify potential parents of each variable ( cid:1850 ) . The existing algorithms achieve this by identifying the MB of ( cid:1850 ) . A typical regression of ( cid:1850 ) on all other variables and consider the variables In the regression of ( cid:1850 ) , the coefficients for the variables that are selected to be the MB . The key differences between various algorithms are the type of regression used and the method in variable selection . For example , the TC algorithm [ 18 ] uses ordinary regression and a t test for variable selection ; the L1MBDAG algorithm [ 14 ] uses LASSO . way is variable selection based on regression , ie , to build a not in the MB will be small ( theoretically zero due to the definition of MB ) . However , the coefficients for the parents may also be very small due to the correlation between the parents and children . As a result , some parents may not be selected in the variable selection , ie , they will be missed in Stage 1 of the twostage approach , leading to greater BN learning errors . In contrast , the SBN may not suffer from this , because it is a one stage approach that identifies the parents directly . To further illustrate this point , we analyze one two stage algorithm , the TC algorithm . TC does variable selection using a ttest . To determine whether a variable should be selected , a t test uses the statistic ( cid:2010)(cid:4632 ) ( cid:1857)(cid:4666)(cid:2010)(cid:4632)(cid:4667 ) , where ( cid:2010)(cid:4632 ) is the estimate for the ⁄ regression coefficient of this variable and ( cid:1857)(cid:4666)(cid:2010)(cid:4632)(cid:4667 ) is the standard error . The larger the ( cid:2010)(cid:4632 ) ( cid:1857)(cid:4666)(cid:2010)(cid:4632)(cid:4667 ) ⁄ value of ( cid:2010)(cid:4632 ) ( cid:1857)(cid:4666)(cid:2010)(cid:4632)(cid:4667 ) ⁄ corresponding to a parent of ( cid:1850 ) is large in the true BN , its value may decrease drastically in the regression of ( cid:1850 )
, the higher chance the variable will be selected . Theorems 2 and 3 below show that even though the on all other variables . Theorem 2 focuses on a specific type of BNs , a general tree , in which all variables have one common ancestor and there is at most one directed path between two variables . Theorem 3 focuses on a general inverse tree , which becomes a general tree if all the arcs are reversed . The proof of Theorem 2 can be found in the Appendix . The proof of Theorem 3 is not shown here due to space limitations .
Theorem 2 . Consider a general tree with ( cid:1865)(cid:3397)2 variables , whose structure and parameters are given by ( cid:1850)(cid:1857 ) , ( cid:1850)(cid:2870)(cid:2010)(cid:2870)(cid:1850)(cid:3397)(cid:1857)(cid:2870 ) , ( cid:1850)(cid:2010)(cid:2870)(cid:1850)(cid:2870)(cid:3397)(cid:1857 ) , ( cid:1861)3,4,…,(cid:1865 ) . All the variables have unit
, variance . If ( cid:1850)(cid:2870 ) is regressed on all other variables in Stage 1 of TC , ie , ( cid:1850)(cid:2870)(cid:2010)(cid:2870)(cid:3014)(cid:3003)(cid:1850)(cid:3397)(cid:2010)(cid:2870)(cid:2871)(cid:3014)(cid:3003)(cid:1850)(cid:2871)(cid:3397)(cid:1710)(cid:3397)(cid:2010)(cid:2870)(cid:3040)(cid:3014)(cid:3003)(cid:1850)(cid:3397)(cid:1857)(cid:2870)(cid:3014)(cid:3003 ) , the coefficient for ( cid:1850)(cid:2870)'s parent , ( cid:1850 ) , is : ( cid:2010)(cid:2870 ) , and ( cid:2010)(cid:2870)(cid:3014)(cid:3003)(cid:2010)(cid:2870 ) ∏ ( cid:3435)(cid:3081)(cid:3118)(cid:3284)(cid:3118)(cid:3439 ) ( cid:3288)(cid:3284)(cid:3128)(cid:3119 ) ∏ ( cid:3435)(cid:3081)(cid:3118)(cid:3284)(cid:3118)(cid:3439)∑ ( cid:4674)(cid:3081)(cid:3118)(cid:3284)(cid:3118)∏ ( cid:3435)(cid:3081)(cid:3118)(cid:3284)(cid:3118)(cid:3439 ) ( cid:4675 ) ( cid:3288)(cid:3285)(cid:3128)(cid:3119),(cid:3285)(cid:3247)(cid:3284 ) ( cid:3288)(cid:3284)(cid:3128)(cid:3119 ) ( cid:3288)(cid:3284)(cid:3128)(cid:3119 ) ( cid:3046)(cid:3032)(cid:4666)(cid:3081)(cid:3117)(cid:3118)(cid:3262)(cid:3251)(cid:4667 ) ( cid:3081)(cid:3117)(cid:3118 ) ( cid:3081)(cid:3117)(cid:3118 ) ( cid:3046)(cid:3032)(cid:4666)(cid:3081)(cid:3117)(cid:3118)(cid:4667)(cid:3496 ) ∏ ( cid:3435)(cid:3081)(cid:3118)(cid:3284)(cid:3118)(cid:3439 ) ( cid:3081)(cid:3117)(cid:3118)(cid:3262)(cid:3251 ) ( cid:3288)(cid:3284)(cid:3128)(cid:3119 ) ( cid:3046)(cid:3032)(cid:4666)(cid:3081)(cid:3117)(cid:3118)(cid:4667 ) . ∏ ( cid:3435)(cid:3081)(cid:3118)(cid:3284)(cid:3118)(cid:3439)∑ ( cid:4674)(cid:3081)(cid:3118)(cid:3284)(cid:3118)∏ ( cid:4672)(cid:3081)(cid:3118)(cid:3285)(cid:3118)(cid:4673 ) ( cid:4675 ) ( cid:3288)(cid:3284)(cid:3128)(cid:3120 ) ( cid:3288)(cid:3285)(cid:3128)(cid:3119),(cid:3285)(cid:3247)(cid:3284 ) ( cid:3288)(cid:3284)(cid:3128)(cid:3120 ) Theorem 3 . Consider a general inverse tree with ( cid:1865)(cid:3397)(cid:1864)(cid:3397)2 variables , whose structure and parameters are given by ( cid:1850)(cid:1857 ) , ( cid:1861)1,2,…,(cid:1864),(cid:1864)(cid:3397)3,…,(cid:1864)(cid:3397)(cid:1865),(cid:1850)(cid:3039)∑ ( cid:2010),(cid:3039)(cid:1850 ) ( cid:3397)(cid:1857)(cid:3039 ) ( cid:3039)(cid:2880 ) ( cid:1850)(cid:3039)(cid:2870)(cid:2010)(cid:3039),(cid:3039)(cid:2870)(cid:1850)(cid:3039)(cid:3397)∑ ( cid:2010)(cid:3039),(cid:3039)(cid:2870)(cid:1850)(cid:3039 ) ( cid:3397)(cid:1857)(cid:3039)(cid:2870 ) . All the variables ( cid:3040)(cid:2880)(cid:2871 ) have unit variance . If ( cid:1850)(cid:3039 ) is regressed on all other variables , ie , ( cid:1850)(cid:3039)∑ ( cid:2010),(cid:3039)(cid:3014)(cid:3003)(cid:1850 ) ( cid:1850)(cid:3039)(cid:2870)(cid:3397)∑ ( cid:2010)(cid:3039),(cid:3039)(cid:2870 ) ( cid:3397)(cid:1857)(cid:3039)(cid:3014)(cid:3003 ) , ( cid:3014)(cid:3003 ) ( cid:1850)(cid:3039 ) ( cid:3397)(cid:2010)(cid:3039),(cid:3039)(cid:2870 ) ( cid:3040)(cid:2880)(cid:2871 ) ( cid:3039)(cid:2880 ) ( cid:3014)(cid:3003 ) the coefficients for ( cid:1850)(cid:3039)'s parent , ( cid:1850)(cid:3038 ) ( (cid:1863)1,2,…,(cid:1864) ) , are : ( cid:2010)(cid:3038),(cid:3039 ) , and ( cid:2010)(cid:3038),(cid:3039)(cid:3014)(cid:3003 ) ( cid:2010)(cid:3038),(cid:3039 ) ∑ ( cid:3081)(cid:3287)(cid:3126)(cid:3117),(cid:3287)(cid:3126)(cid:3118 ) ( cid:3081)(cid:3287)(cid:3126)(cid:3118)(cid:3126)(cid:3284),(cid:3287)(cid:3126)(cid:3118 ) ( cid:3118)(cid:3288)(cid:3284)(cid:3128)(cid:3117 ) ( cid:3118 ) ∑ ∑ ( cid:3081)(cid:3284),(cid:3287)(cid:3126)(cid:3117)(cid:3118)(cid:3288)(cid:3284)(cid:3128)(cid:3117 ) ( cid:3081)(cid:3287)(cid:3126)(cid:3117),(cid:3287)(cid:3126)(cid:3118 ) ( cid:3081)(cid:3287)(cid:3126)(cid:3118)(cid:3126)(cid:3284),(cid:3287)(cid:3126)(cid:3118 ) ( cid:3118)(cid:3288)(cid:3284)(cid:3128)(cid:3117 ) ( cid:3118 ) ( cid:3046)(cid:3032)(cid:3435)(cid:3081)(cid:3286),(cid:3287)(cid:3126)(cid:3117)(cid:3262)(cid:3251 ) ( cid:3439 ) ( cid:3081)(cid:3286),(cid:3287)(cid:3126)(cid:3117 ) ∑ ( cid:3081)(cid:3286),(cid:3287)(cid:3126)(cid:3117)(cid:3262)(cid:3251 ) ( cid:3081)(cid:3287)(cid:3126)(cid:3118)(cid:3126)(cid:3284),(cid:3287)(cid:3126)(cid:3118 ) ( cid:3081)(cid:3287)(cid:3126)(cid:3117),(cid:3287)(cid:3126)(cid:3118 ) ( cid:3118)(cid:3288)(cid:3284)(cid:3128)(cid:3117 ) ( cid:3118 ) ∑ ∑ ( cid:3046)(cid:3032)(cid:3435)(cid:3081)(cid:3286),(cid:3287)(cid:3126)(cid:3117)(cid:3439 ) ( cid:3081)(cid:3284),(cid:3287)(cid:3126)(cid:3117)(cid:3118)(cid:3288)(cid:3284)(cid:3128)(cid:3117 ) ( cid:3081)(cid:3287)(cid:3126)(cid:3118)(cid:3126)(cid:3284),(cid:3287)(cid:3126)(cid:3118 ) ( cid:3081)(cid:3287)(cid:3126)(cid:3117),(cid:3287)(cid:3126)(cid:3118 ) ( cid:3118)(cid:3288)(cid:3284)(cid:3128)(cid:3117 ) ( cid:3118 ) ( cid:3400)(cid:3496)(cid:3435)∑ ∑ ( cid:3439)(cid:3435)∑ ( cid:3439 ) ( cid:3081)(cid:3284),(cid:3287)(cid:3126)(cid:3117)(cid:3118)(cid:3288)(cid:3284)(cid:3128)(cid:3117 ) ( cid:3081)(cid:3287)(cid:3126)(cid:3118)(cid:3126)(cid:3284),(cid:3287)(cid:3126)(cid:3118 ) ( cid:3081)(cid:3284),(cid:3287)(cid:3126)(cid:3117)(cid:3118)(cid:3288)(cid:3284)(cid:3128)(cid:3117 ) ( cid:3081)(cid:3287)(cid:3126)(cid:3117),(cid:3287)(cid:3126)(cid:3118 ) ( cid:3118)(cid:3288)(cid:3284)(cid:3128)(cid:3117 ) ( cid:3118 ) ∑ ( cid:3435)∑ ( cid:3439 ) ( cid:3081)(cid:3284),(cid:3287)(cid:3126)(cid:3117)(cid:3118 ) ( cid:3081)(cid:3287)(cid:3126)(cid:3118)(cid:3126)(cid:3284),(cid:3287)(cid:3126)(cid:3118 ) ( cid:3081)(cid:3287)(cid:3126)(cid:3117),(cid:3287)(cid:3126)(cid:3118 ) ( cid:3118)(cid:3288)(cid:3284)(cid:3128)(cid:3117 ) ( cid:3118 ) ( cid:3288)(cid:3284)(cid:3128)(cid:3117),(cid:3284)(cid:3247)(cid:3286 ) ( cid:3081)(cid:3117)(cid:3118 ) ( cid:3400)(cid:3496 ) ∑ ( cid:3435)∑ ( cid:3439 ) ( cid:3081)(cid:3287)(cid:3126)(cid:3118)(cid:3126)(cid:3284),(cid:3287)(cid:3126)(cid:3118 ) ( cid:3081)(cid:3284),(cid:3287)(cid:3126)(cid:3117)(cid:3118 ) ( cid:3081)(cid:3287)(cid:3126)(cid:3117),(cid:3287)(cid:3126)(cid:3118 ) ( cid:3118)(cid:3288)(cid:3284)(cid:3128)(cid:3117 ) ( cid:3118 ) ( cid:3288)(cid:3284)(cid:3128)(cid:3117 ) ( cid:3435)∑ ∑ ( cid:3439 ) ( cid:3081)(cid:3287)(cid:3126)(cid:3118)(cid:3126)(cid:3284),(cid:3287)(cid:3126)(cid:3118 ) ( cid:3081)(cid:3284),(cid:3287)(cid:3126)(cid:3117)(cid:3118 ) ( cid:3081)(cid:3287)(cid:3126)(cid:3117),(cid:3287)(cid:3126)(cid:3118 ) ( cid:3046)(cid:3032)(cid:4666)(cid:3081)(cid:3117)(cid:3118)(cid:4667 ) . ( cid:3118)(cid:3288)(cid:3284)(cid:3128)(cid:3117 ) ( cid:3118 ) ( cid:3288)(cid:3284)(cid:3128)(cid:3117),(cid:3284)(cid:3247)(cid:3286 ) For example , consider a general tree with ( cid:2010)(cid:2870)0.3 , ( cid:2010)(cid:2870)0.8 , ( cid:1865)8 . Based on Theorem 2 , ( cid:2010)(cid:2870)(cid:3014)(cid:3003)0.028(cid:1575)(cid:2010)(cid:2870 ) and ⁄ ( cid:2010)(cid:2870)(cid:3014)(cid:3003 ) ( cid:1857)(cid:4666)(cid:2010)(cid:2870)(cid:3014)(cid:3003)(cid:4667 ) 0.092√1(cid:1575 ) ( cid:2010)(cid:2870 ) ( cid:1857)(cid:4666)(cid:2010)(cid:2870)(cid:4667)0.3144√1 ⁄ Consider a general inverse tree with seven variables : ( cid:1850),(cid:1861)1,(cid:1710),5 , are parents of ( cid:1850)(cid:2874 ) which is a parent of ( cid:1850)(cid:2875 ) ; ( cid:4670)(cid:2010)(cid:2874),(cid:1710),(cid:2010)(cid:2873)(cid:2874)(cid:4671)(cid:4670)024,0325,0256,0304,0216(cid:4671 ) ; ( cid:2010)(cid:2874)(cid:2875)03 ( cid:4670)(cid:2010)(cid:2874)(cid:3014)(cid:3003),(cid:1710),(cid:2010)(cid:2873)(cid:2874)(cid:3014)(cid:3003)(cid:4671)(cid:4670)0036,0053,0038,0045,0032(cid:4671)(cid:1575 ) ( cid:4671 ) is equal to ( cid:4670)(cid:2010)(cid:2874),(cid:1710),(cid:2010)(cid:2873)(cid:2874)(cid:4671 ) and ( cid:4670)(cid:2010)(cid:2874)(cid:3014)(cid:3003 ) ( cid:1857)(cid:4666)(cid:2010)(cid:2874)(cid:3014)(cid:3003)(cid:4667 ) ⁄ ⁄ ,(cid:1710),(cid:2010)(cid:2873)(cid:2874)(cid:3014)(cid:3003 ) ( cid:1857)(cid:4666)(cid:2010)(cid:2873)(cid:2874)(cid:3014)(cid:3003)(cid:4667 ) ( cid:4670)01142,01672,01216,01444,01026(cid:4671)(cid:3400)(cid:3493)(cid:4666)1(cid:4667 ) which is less than ( cid:3427)(cid:2010)(cid:2874 ) ( cid:1857)(cid:4666)(cid:2010)(cid:2874)(cid:4667 ) ( cid:3431 ) since the later ( cid:3415 ) ( cid:3415 ) ,(cid:1710),(cid:2010)(cid:2873)(cid:2874 ) ( cid:1857)(cid:4666)(cid:2010)(cid:2873)(cid:2874)(cid:4667 ) ( cid:4670)0306,04491,03266,03878,02785(cid:4671)(cid:3400)(cid:3493)(cid:4666)1(cid:4667 ) Note that these theorems derive the relationship between ( cid:2010)(cid:3014)(cid:3003 ) ) and ( cid:2010 ) ( (cid:2010 ) ( cid:1857)(cid:4666)(cid:2010)(cid:4667 ) ⁄ ( (cid:2010)(cid:3014)(cid:3003 ) ( cid:1857)(cid:4666)(cid:2010)(cid:3014)(cid:3003)(cid:4667 ) ( cid:3415 ) sampling error is not considered ) , so they use the notation “ ( cid:2010 ) ” not “ ( cid:2010)(cid:4632 ) ” .
) at the population level ( ie ,
Based on Theorem 3 , one is equal to
.
,
,
5 . SIMULATION STUDIES ON SYNTHETIC DATA We show four sets of simulations . The first one is to show that , on a general tree , the existing algorithms based on the two stage approach may miss some true parents with high probabilities , while the proposed SBN performs well . The second simulation is to compare the structure learning accuracy of SBN with other competing algorithms , on benchmark networks . The third and fourth simulations are to investigate the scalability of SBN and compare it with other competing algorithms .
934 Figure 3 . ( a ) : The general tree in Sec 5.1 ( regression coefficients are 0.3 for arcs between ( cid:2180)(cid:2778 ) and ( cid:2180)(cid:2191 ) , ( cid:2191)(cid:2779),(cid:1710),(cid:2784 ) ,
( a ) ( b ) ( c ) and 0.8 for others ) ; ( b ) Simulation results for the general tree ; ( c ) Simulation results for 11 benchmark networks .
5.1 Learning accuracy for general tree Ten existing algorithms are selected : HITON PC [ 25 ] , IAMB and three of its variants [ 26 ] , GS [ 17 ] , SC [ 13 ] , TC and its advanced version TC bw [ 18 ] , and L1MB [ 14 ] . We simulate data from the general tree in Figure 3(a ) with a sample size of 200 . We apply the selected algorithms on the simulated data ; the parameters of each algorithm are selected in the way that the authors have suggested in their papers , respectively . In applying the proposed SBN , ( cid:2019 ) is selected by BIC ; ( cid:2019)(cid:2870 ) is set to be 10(cid:4670)(cid:4666 ) 1(cid:4667)(cid:2870)(cid:1868 ) ( cid:2019)⁄ ( cid:2019)(cid:4671 ) which empirically guarantees a DAG to be learned . The initial value of SBN is the output of L1MB which uses LASSO in Stage 1 to identify the MB for each variable . We treat the identified MB by L1MB as parents and use the resulting “ BN ” ( not necessarily a DAG ) as the initial value for SBN . The results over 100 repetitions are shown in Figure 3(b ) . The X axis records the 10 selected algorithms and the proposed SBN ( the last one ) . The Y axis records the frequency of a true arc being identified . Each color bar corresponds to a true arc indicated in the legend . Six true arcs are shown , ie , the arcs between ( cid:1850 ) and ( cid:1850 ) , ( cid:1861)2,(cid:1710),7 . Because ( cid:1850 ) is the parent of ( cid:1850 ) , the Y axis actually shows how well the parent of ( cid:1850 ) can be identified by each of the the MB of ( cid:1850 ) includes ( cid:1850 ) and six children . Although the coefficient linking ( cid:1850 ) to ( cid:1850 ) is as high as 0.3 , this coefficient in the regression that regresses ( cid:1850 ) on its MB reduces to 0.028 , due to the inclusion of the children in the regression . As a result , ( cid:1850 ) will algorithms . Figure 3 ( b ) shows that , SBN performs much better than all others . This can be explained by Theorem 1 . Specifically , have a high probability of being excluded from the MB identified in Stage 1 of the existing algorithms . 5.2 Learning accuracy for Benchmark networks We select seven moderately large networks from BNR [ 27 ] . We also use the tiling technique to produce two large BNs , alarm2 and hailfinder2 . Two other networks with specific structures , factor and chain , are also considered . The 11 networks are : ( number of nodes/edges ) : 1 . factors ( 27/68 ) , 2 . alarm ( 37/46 ) , 3 . barley ( 48/84 ) , 4 . carpo ( 61/74 ) , 5 . chain(7/6 ) , 6 . hailfinder ( 56/66 ) , 7 . insurance ( 27/52 ) , 8 . mildew ( 35/46 ) , 9 . water ( 32/66 ) , 10 . alarm2 ( 296/410 ) , 11 . hailfinder2 ( 280/390 ) . To specify the parameters of a network , ie , to specify the regression coefficients of each variable on its parents , we randomly sample from ( cid:3399)1(cid:3397 ) ( cid:1840)(cid:4666)0,1/16(cid:4667 ) . Then , we simulate data from the networks with sample size 100 , and apply the 10 algorithms to learn the BN structures . The results over 100 repetitions are shown in Figure 3(c ) . The X axis records the 11 networks . The Y axis records the ratio of the total learning error ( false positives plus false negatives ) to the number of arcs in the true BN . Each curve corresponds to one of the 11 algorithms under comparison . We can observe that the lowest curve ( ie , best performance ) is SBN .
( a ) ( b ) Figure 4 . Scalability of SBN with respect to ( a ) the number of variables in a BN , ( cid:1816 ) , and ( b ) the sample size , ( cid:1814 ) , on a computer with Intel Core 2 , 2.2 G Hz , 4G
Figure 5 . Comparison of SBN with competing algorithms on the CPU time in structure learning of the other nine benchmark networks
5.3 Scalability We study two aspects of scalability for SBN : the scalability with respect to the number of variables in a BN , ( cid:1868 ) , and the scalability with respect to the sample size , . We use the CPU time for each sweep through all the columns of ( cid:1776 ) as the parameter for measurement . Specifically , we fix =1000 , and vary ( cid:1868 ) by using the 11 benchmark networks . Also , we fix ( cid:1868 ) =37 ( the Alarm is linear in and quadratic in ( cid:1868 ) , which confirms our theoretical network ) . The results over 100 repetitions are shown in Figure 4 ( a ) and ( b ) , respectively . It can be seen that the computational cost time complexity analysis in Section 3 .
935
Table 2 . Names of the AVOIs for effective connectivity modeling ( L = left hemisphere , R = right hemisphere ) Frontal lobe Temporal lobe
Parietal lobe 13 Parietal_Sup_L 14 Parietal_Sup_R 15 Parietal_Inf_L 16 Parietal_Inf_R
1 Frontal_Sup_L 2 Frontal_Sup_R 3 Frontal_Mid_L 4 Frontal_Mid_R 5 Frontal_Sup_Medial_L 17 Precuneus_L 6 Frontal_Sup_Medial_R 18 Precuneus_R 7 Frontal_Mid_Orb_L 8 Frontal_Mid_Orb_R 9 Rectus_L 10 Rectus_R 11 Cingulum_Ant_L 12 Cingulum_Ant_R
19 Cingulum_Post_L 20 Cingulum_Post_R
Occipital lobe 21 Occipital_Sup_L 27 T emporal_Sup_L 22 Occipital_Sup_R 28 T emporal_Sup_R 23 Occipital_Mid_L 29 T emporal_Pole_Sup_L 24 Occipital_Mid_R 30 T emporal_Pole_Sup_R 25 Occipital_Inf_L 31 T emporal_Mid_L 26 Occipital_Inf_R 32 T emporal_Mid_R
33 T emporal_Pole_Mid_L 34 T emporal_Pole_Mid_R 35 T emporal_Inf_L 8301 36 T emporal_Inf_R 8302 37 Fusiform_L 38 Fusiform_R 39 Hippocampus_L 40 Hippocampus_R 41 ParaHippocampal_L 42 ParaHippocampal_R
6 . BRAIN EFFECTIVE CONNECTIVITY MODELING OF AD BY SBN FDG PET baseline images of 49 AD and 67 normal control ( NC ) subjects from the ADNI project ( wwwloniuclaedu/ADNI ) were used in this study . After spatially normalizing the images to the Montreal Neurological Institute ( MNI ) template coordinate space , average PET counts were extracted from the 116 brain anatomical regions of interest ( AVOIs ) defined by the Automated Anatomical Labeling [ 28 ] technique . We then selected 42 AVOIs that are considered to be potentially relevant to AD , as reported in the literature . Each of the 42 AVOIs became a node in the SBN . Please see Table 2 for the name of each AVOI brain region . These regions are distributed in the four major neocortical lobes of the brain , ie , the frontal , parietal , occipital , and temporal lobes . We apply SBN to learn a BN for AD and another one for NC , to represent their respective effective connectivity models . In the learning of an AD ( or NC ) effective connectivity model , the value for ( cid:2019 ) needs to be selected . In this paper , we adopt two criteria in selecting ( cid:2019 ) : one is to minimize the prediction error of the model represented by a "matrix" . Each row/column is one AVOI , ( cid:1850 ) . A black cell at the i th row and j th column of the matrix represents that ( cid:1850 ) is a parent of ( cid:1850 ) . On each matrix , four red squares are used and the other is to minimize the BIC . Both criteria have been popularly adopted in sparse learning [ 12,14,15 ] . The two criteria lead to similar findings discovered from the effective connectivity models , so only the results based on the minimum prediction error are shown in this section . Specifically , Figure 6 shows the effective connectivity models for AD and NC . Each model is to highlight the four lobes , ie , the frontal , parietal , occipital , and temporal lobes , from top left to bottom right . The black cells inside each red square reflect intra lobe effective connectivity , whereas the black cells outside the squares reflect inter lobe effective connectivity . The following interesting observations can be drawn from the effective connectivity models : Global scale effective connectivity : The total number of arcs in a BN connectivity model , equal to the number of black cells in a matrix plot in Figure 6 , represents the amount of effective connectivity ( ie , the amount of directional information flow ) in the whole brain . This number is 285 and 329 for AD and NC , respectively . In other words , AD has 13.4 % less effective connectivity than NC . Loss of connectivity in AD has been widely reported in the literature [ 34 , 38 40 ] .
( a ) ( b ) Figure 6 . Brain effective connectivity models by SBN . ( a )
AD . ( b ) NC .
Intra /inter lobe effective connectivity distribution : Aside from having different amounts of effective connectivity at the global scale , AD may also have a different distribution pattern of connectivity across the brain from NC . Therefore , we count the number of arcs in each of the four lobes and between each pair of lobes in the AD and NC effective connectivity models . The results are summarized in Table 3 . It can be seen that the temporal lobe of AD has 22.9 % less effective connectivity than NC . The decrease in connectivity in the temporal lobe of AD has been extensively reported in the literature [ 29 , 2 , 30 ] . The interpretation may be that AD is featured by a dramatic cognitive decline and the temporal lobe is responsible for delivering memory and other cognitive functions . As a result , the temporal lobe is affected early and severely by AD and the connectivity network in this lobe is severely disrupted . On the other hand , the frontal lobe of AD has 27.6 % more connectivity than NC . This has been interpreted as compensatory reallocation or recruitment of cognitive resources [ 29 32 ] . Because the regions in the frontal lobe are typically affected later in the course of AD ( our data are mild to moderate AD ) , the increased connectivity in the frontal lobe may help preserve some cognitive functions in AD patients . In addition , AD shows a decrease in the amount of effective connectivity in the parietal lobe which has also been reported to be affected by AD . There is no significant difference between AD and NC in the occipital lobe . This is reasonable because the occipital lobe is primarily involved in the brain ’s visual function which is not affected by AD .
Table 3 . Intra – and inter – lobe effective connectivity amounts
( a ) AD ( b ) NC in reduction the most significant
Furthermore , inter lobe connectivity in AD is found between the frontal and temporal lobes , ie , AD has 29.5 % less effective connectivity than NC . This may be attributed to the temporal lobe disruption of the default mode network in AD [ 34 ] . Direction of local effective connectivity : One advantage of BNs over undirected graphical models in brain connectivity modeling is that the directed arcs in a BN reflect directional effect of one region over another , ie , the effective connectivity . Specifically , if there is a directed arc from brain regions ( cid:1850 ) to ( cid:1850 ) , it indicates that ( cid:1850 ) takes a dominant role in the
936 to brain communication with ( cid:1850 ) . The connectivity modes in Figure 6 two regions take a significantly dominant role reveal a number of interesting findings in this regard : ( i ) There are substantially fewer black cells in the area defined by rows 27 42 and columns 1 26 in AD than NC . Recall that rows 27 42 correspond to regions in the temporal lobe . Thus , this pattern indicates a substantial reduction in arcs pointing from temporal regions to the other regions in the AD brain , ie , temporal regions lose their dominating roles in communicating information with the other regions as a result of AD . The loss is most severe in the communication from temporal to frontal regions . ( ii ) Rows 31 and 35 , corresponding regions “ Temporal_Mid_L ” and “ Temporal_Inf_L ” , respectively , are among the rows with the largest number of black cells in NC , ie , these in communicating with other regions in normal brains . However , the dominancy of the two regions is substantially reduced by 34.8 % and 36.8 % , respectively , in AD . A possible interpretation is that these are neocortical regions associated with amyloid deposition and early FDG hypometabolism in AD [ 34 37 , 41 42 ] . ( iii ) Columns 39 and 40 correspond to regions “ Hippocampus_L ” and “ Hippocampus_R ” , respectively . There are a total of 33 black cells in these two columns in NC , ie , 33 other regions dominantly communicate information with the hippocampus . However , this number reduces to 22 ( 33.3 % reduction ) in AD . The reduction is more severe in Hippocampus_L ( 50% ) . The hippocampus is well known to play a prominent role in making new memories and in recall . It has been widely reported that the hippocampus is affected early in the course of AD , leading to memory loss – the most common symptom of AD . ( iv ) There are a total of 93 arcs pointing from the left hemisphere to the right hemisphere of the brain in NC ; this number reduces to 71 ( 23.7 % reduction ) in AD . The number of arcs from the right hemisphere to the left hemisphere in AD is close to that in NC . This provides evidence that AD may be associated with interhemispheric disconnection and the disconnection is mostly unilateral , which has also been reported by some other papers [ 4344 ] . 7 . CONCLUSION In this paper , we proposed a BN structure learning algorithm , called SBN , for identifying effective connectivity of AD from FDG PET data . SBN adopted a novel formulation that involves one L1 norm penalty term to impose sparsity on the learning and another penalty to ensure the learned BN to be a DAG . We performed theoretical analysis on the competitive advantage of SBN over the existing algorithms in terms of learning accuracy . Our analysis showed that the existing algorithms employ a twostage approach in BN structure identification , which makes them have a high risk of failing to identify the parents of each variable . Also , we performed theoretical analysis on the time complexity of SBN , which showed that it is linear in the sample size and quadratic in the number of variables . Furthermore , we conducted experiments to compare SBN with 10 competing algorithms and found that SBN has significantly better performance in learning accuracy . We applied SBN to identify the effective connectivity models of AD and NC from PDG PET data , and found that the effective connectivity of AD is different from NC in many ways . Clinically , our findings may provide an additional tool for monitoring disease progress , evaluating treatment effects , and
( cid:2880 ) the
( A 1 )
, ( A 2 )
( cid:1854)1(cid:1854)(cid:1854)(cid:1854)(cid:1854)(cid:2870)(cid:1710)(cid:1854)(cid:1854 )
( cid:1854)(cid:1854)(cid:1854)1(cid:1854)(cid:1854)(cid:2870)(cid:1710)(cid:1854)(cid:1854 )
( cid:1743)(cid:1742)(cid:1742)(cid:1742)(cid:1742)(cid:1741)1(cid:1854)(cid:1854)(cid:1854)(cid:2870)(cid:1710)(cid:1854 )
( cid:1854)(cid:1854)(cid:1854)(cid:1854)(cid:1854)(cid:1854)(cid:2870)(cid:1854)(cid:1710)1
( cid:1854)(cid:2870)(cid:1854)(cid:1854)(cid:2870)(cid:1854)(cid:1854)(cid:2870)1(cid:1710)(cid:1854)(cid:2870)(cid:1854 )
Least Square criterion , the regression coefficients ,
Based on Wright ’s second decomposition rule [ 33 ] , enabling early detection of network disconnection in prodromal AD . Future studies may be conducted to verify the statistical significance of the identified effective connectivity difference between AD and NC , and estimate the strength of the directed arcs identified by SBN to help improve the sensitivity and specificity of the effective connectivity models . Finally , although this paper focuses on the structure learning of Gaussian BNs , the same formulation can be adopted to discrete BNs , which will be explored in our future research . Appendix A There are some notation changes as follows for the convenience of the derivation : we use ( cid:1854)(cid:2010)(cid:2870 ) , ( cid:1854)(cid:2010)(cid:2870)(cid:2871 ) , ( cid:1854)(cid:2870)(cid:2010)(cid:2870)(cid:2872 ) , … , ( cid:1854)(cid:2010)(cid:2870)(cid:3040 ) , ( cid:1851)(cid:1850)(cid:2871 ) , ( cid:1851)(cid:2870)(cid:1850)(cid:2872 ) , … , ( cid:1851)(cid:1850)(cid:3040 ) . covariance matrix of all the variables , ( cid:1846 ) , can be represented as a function of the parameters of the BN , ( cid:1854),(cid:1710),(cid:1854 ) , ie , T(cid:2924)(cid:1855)(cid:1867)(cid:4666)(cid:1850)(cid:2870),(cid:1850),(cid:1851),(cid:1710),(cid:1851)(cid:4667 ) ( cid:1710)(cid:1710)(cid:1710)(cid:1710)(cid:1710)(cid:1710 ) ( cid:1746)(cid:1745)(cid:1745)(cid:1745)(cid:1745)(cid:1744 )
Now , consider the regression of ( cid:1850)(cid:2870 ) on all other variables , ie , ( cid:1850)(cid:2870)(cid:1854)(cid:3014)(cid:3003)(cid:1850)(cid:3397)(cid:1854)(cid:3014)(cid:3003)(cid:1851)(cid:3397)(cid:1710)(cid:3397)(cid:1854)(cid:3014)(cid:3003)(cid:1851)(cid:3397)(cid:1857)(cid:2870)(cid:3014)(cid:3003 ) . According to the ( cid:4670)(cid:1854)(cid:3014)(cid:3003),(cid:1710),(cid:1854)(cid:3014)(cid:3003)(cid:4671)(cid:1829)(cid:3427)(cid:1854),(cid:1710),(cid:1854)(cid:3431 ) where ( cid:1829 ) is a sub matrix of T(cid:2924 ) by deleting the 1st column and 1st row from ( cid:1846 ) . Denote C(cid:2924 ) by A(cid:2924)(cid:3435)(cid:1853)(cid:3439 ) . Then , ( cid:1854)(cid:3014)(cid:3003)(cid:1853)(cid:1854)(cid:3397)(cid:1853)(cid:2870)(cid:1854)(cid:3397)(cid:1853)(cid:2871)(cid:1854)(cid:2870)(cid:3397)(cid:1710)(cid:3397)(cid:1853)(cid:1854 ) . ( A 3 ) Our final objective is to express ( cid:1854)(cid:3014)(cid:3003 ) by the parameters of the BN . This can be achieved if we can express ( cid:1853),(cid:1853)(cid:2870),(cid:1853)(cid:2871),(cid:1710),(cid:1853 ) by ( cid:1853)(cid:4666)1(cid:4667)(cid:2914)(cid:2915)(cid:2930)(cid:3435)(cid:3004)(cid:3284)(cid:3285),(cid:3289)(cid:3439 ) ( cid:2914)(cid:2915)(cid:2930)(cid:4666)(cid:3004)(cid:3289)(cid:4667 ) , ( A 4 ) where ( cid:1829 ) , is a matrix by deleting the 1st row and the jth column from ( cid:1829 ) . So , the problem becomes calculation of det(cid:4666)(cid:1829)(cid:4667 ) and det(cid:3435)(cid:1829),(cid:3439 ) . ( i ) Calculation of det(cid:4666)(cid:1829)(cid:4667 ) : We first show the result : det(cid:4666)(cid:1829)(cid:4667)∏ ( cid:3435)1(cid:1854)(cid:2870)(cid:3439)(cid:3397)∑ ( cid:3435)(cid:1854)(cid:2870)∏ ( cid:3435)1(cid:1854)(cid:2870)(cid:3439 ) ( cid:2880 ) ( cid:2880),(cid:2999 ) 1 ) When 1 , it is easy to see that ( A 5 ) holds . 2 ) Assume that ( A 5 ) holds for 1 , ie , det(cid:4666)(cid:1829)(cid:4667)∏ ( cid:3435)1(cid:1854)(cid:2870)(cid:3439)(cid:3397)∑ ( cid:3435)(cid:1854)(cid:2870)∏ ( cid:3435)1(cid:1854)(cid:2870)(cid:3439 ) ( cid:3439 ) ( cid:2880 ) ( cid:2880),(cid:2999 ) ( cid:2880 ) Then we need prove that ( A 5 ) holds for . Based on the det(cid:4666)(cid:1829)(cid:4667)∑ ( cid:4666)1(cid:4667)(cid:1855 ) det(cid:3435)(cid:1829),(cid:3439 ) , ( A 6 ) ( cid:2880 )
( A 5 ) Next , we will use the induction method in 1) 2 ) below to prove ( A 5 ) : the parameters of the BN , which is the goal of the following derivation . It is known that
( cid:3439 )
. definition of a determinant ,
937 due to page limits ) : where ( cid:1855 ) is the entry at the 1st row and jth column of ( cid:1829 ) . Now we need to derive det(cid:3435)(cid:1829),(cid:3439 ) ( only results are shown below When 1 , det(cid:3435)(cid:1829),(cid:3439)∏ ( cid:3435)1(cid:1854)(cid:2870)(cid:3439)(cid:3397)∑ ( cid:3435)(cid:1854)(cid:2870)∏ ( cid:3439 ) ( cid:3435)1(cid:1854)(cid:2870)(cid:3439 ) ( cid:2880)(cid:2870 ) ( cid:2880),(cid:2999 ) ( cid:2880)(cid:2870 ) When ( cid:3405)1 , det(cid:3435)(cid:1829),(cid:3439)(cid:4666)1(cid:4667)(cid:1854)(cid:1854)∏ ( cid:3435)1(cid:1854)(cid:3038)(cid:2870)(cid:3439 ) ( cid:3038)(cid:2880),(cid:3038)(cid:2999 ) Then , insert ( A 7 ) , ( A 8 ) , and ( cid:1855)1 , ( cid:1855)(cid:2870)(cid:1854)(cid:1854 ) , … , ( cid:1855 ) ( cid:1854)(cid:1854 ) into ( A 6 ) :
, ( A 7 )
. ( A 8 )
C det(
)
 n n
 i

2
( 1
 b 2 i
)
 n
 i

2 b ( 2 i n
 j j i  1 
( 1
 b
2 j
) )
 n
 j
1  b b 2 2  j 0 n
 k k
1  j 
( 1
 b 2 k
) n


( 1
 b
2 j
)
 n
 b
2 j n

( 1
 b 2 k
) j j k k
0 j
 
1 
1 
( ii ) Calculation of det(cid:3435)(cid:1829),(cid:3439 ) : det(cid:3435)(cid:1829),(cid:3439)has been obtained by ( A
This completes the proof for ( A 5 ) .
7 ) and ( A 8 ) . Inserting ( A 5 ) , ( A 7 ) , and ( A 8 ) into ( A 4 ) , we get : n
 j

2
( 1
 b
2 j
)
 n
 j

2 b (
2 j
( 1
 b 2 k
) ) n
 k k j  1  n a 11
 n n i
1 
)



( 1
( 1 b 2 i b ( 2 i



Furthermore , ( cid:1853)(cid:3397)(cid:1853)(cid:2870)(cid:1854)(cid:1854)(cid:3397)(cid:1853)(cid:2871)(cid:1854)(cid:1854)(cid:2870)(cid:3397)(cid:1710)(cid:3397)(cid:1853)(cid:1854)(cid:1854)1 . Inserting this into ( A 3 ) , we get ( cid:1854)(cid:3014)(cid:3003)(cid:1853)(cid:4672)1(cid:4666)1(cid:1854)(cid:2870)(cid:4667)(cid:4673)/(cid:1854 ) . Plugging in the ( cid:1853 ) above , we can get : k i  k 0  b 2 k
) )
1  i n
 j
1 
( 1
 b
2 j
)
( A 9 ) b MB 0
 b 0 n

( 1
 b 2 i
)
 n
 b ( 2 i n

( 1
 b 2 k
) ) i i
1 
1  k i  k 0 
Obviously , the fraction at the right hand side is between 0 and 1 .
Therefore , |(cid:1854)(cid:3014)(cid:3003)||(cid:1854)| . Next we derive the formula for ( cid:1854)(cid:3014)(cid:3003)/(cid:1857)(cid:4666)(cid:1854)(cid:3014)(cid:3003)(cid:4667 ) . It is known that ( cid:1857)(cid:2870)(cid:4666)(cid:1854)(cid:3014)(cid:3003)(cid:4667)(cid:1853)/(cid:4670)(cid:4666)1(cid:4667)(cid:4666)(cid:1846)(cid:4667)(cid:4671 ) ( cid:4666)(cid:1846)(cid:4667 ) det(cid:4666)(cid:1829)(cid:4667)/det(cid:4666)(cid:1846)(cid:4667)det(cid:4666)(cid:1829)(cid:4667)/∏ ( cid:3435)1(cid:1854)(cid:2870)(cid:3439 ) and det(cid:4666)(cid:1829)(cid:4667 ) is ( cid:2880 )
Since
. given in ( A 5 ) , we can get :
2 s e
 b MB 0


1 
1 n n
 i
1 
( 1
 b 2 i
( 1
 b 2 i
) n
 i )
0   n
 i
1  b ( 2 i n
 k i  k 0 
( 1
 b 2 k
) )
( A 10 ) n
 j

2 n
 i
1 
( 1
 b
2 j
)

( 1
 b 2 i
)
 n
 j

2 n
 i
1  b (
2 j b ( 2 i
 n
 k k j  1  n
 k i  k 0 
( 1
 b 2 k
) )
( 1
 b 2 k
) )
Also , se(cid:2870)(cid:4666)(cid:1854)(cid:4667)(cid:4666)1(cid:1854)(cid:2870)(cid:4667)/(cid:4666)1(cid:4667 ) . Putting this together with ( A
9 ) and ( A 10 ) , we can get : n
 i 1  
( 1
 b 2 i
) n
 j

2
( 1
 b
2 j
) n
 j

2 b (
2 j n
 k k j  1 
( 1
 b 2 k
) ) b MB 0  se b MB 0

 e s b 0  b 0


( cid:1854)(cid:3014)(cid:3003)/(cid:1857)(cid:4666)(cid:1854)(cid:3014)(cid:3003)(cid:4667)(cid:1854)/(cid:1857)(cid:4666)(cid:1854)(cid:4667 ) .
It is obvious that the part in the “ { } ” is less than one . Therefore ,

Acknowledgments This research is sponsored in part by NSF IIS 0953662 and NSF MCB 1026710 .
8 . REFERENCES [ 1 ] Horwitz , B . 2003 . The Elusive Concept of Brain
Connectivity , Neuroimage 19 , 466 470 .
[ 2 ] Azari , N . , Rapoport , S . 1992 . Patterns of Interregional
Correlations of Cerebral Glucose Metabolic Rates in Patients with Dementia of the Alzheimer Type . Neurodegeneration 1 : 101–111 .
[ 3 ] Friston , KJ 1994 . Functional and Effective Connectivity in
Neuroimaging : A Synthesis . Human Brain Mapping 2 , 5678 .
[ 4 ] Alexander , G . , Moeller , J . 1994 . Application of the Scaled
Subprofile Model to Functional Imaging in Neuropsychiatric Disorders : A Principal Component Approach to Modeling Brain Function in Disease . Human Brain Mapping 2 , 79 94 .
[ 5 ] Calhoun , V . , Adali , T . , Pearison , G . and Pekar , J . 2001 .
Spatial and Temporal Independent Component Analysis of Functional MRI Data Containing a Pair of Task Related Waveforms . HumanBrain Mapping 13 , 43 53 .
[ 6 ] McIntosh , A . , Bookstein , F . , Haxby , J . and Grady , C . 1996 . Spatial Pattern Analysis of Functional Brain Images Using Partial Least Squares . Neuroimage 3 , 143 157 .
[ 7 ] Chiang , J . , Wang , Z . , and McKeown , MJ 2009 . Sparse
Multivariate Autoregressive ( mAR) based Partial Directed Coherence ( PDC ) for EEG Analysis . Proceedings of the 2009 IEEE International Conference on Acoustics , Speech and Signal Processing : 457 460 .
[ 8 ] Huang , S . , Li , J . , Sun , L . , Ye , J . , Fleisher , A . , Wu , T . , Chen ,
K . and Reiman , E . 2010 . Learning Brain Connectivity of Alzheimer ’s Disease by Sparse Inverse Covariance Estimation , NeuroImage , 50 , 935 949 .
[ 9 ] Hilgetag , C . , Kotter , R . , Stephan , KE and Sporns , O . 2002 .
Computational Methods for the Analysis of Brain Connectivity . In : Ascoli , GA ( Ed. ) , Computational Neuroanatomy . Humana Press , Totowa , NJ .
[ 10 ] Bullmore , E . , Horwitz , B . , Honey , G . , Brammer , M . ,
Williams , S . and Sharma , T . 2000 . How Good is Good Enough in Path Analysis of fMRI ? Neuroimage 11 , 289–301 .
[ 11 ] Friston , KJ , Harrison , L . and Penny , W . 2003 . Dynamic
Causal Modeling . Neuroimage 19 , 1273 1302 .
938 [ 12 ] Lam , W . and Bacchus , F . 1994 . Learning Bayesian Belief
Networks : an Approach based on the MDL Principle , Computational Intelligence 10 , p269–293
[ 13 ] Friedman , N . , Nachman , I . and Pe’er , D . 1999 . Learning Bayesian Network Structure from Massive Datasets : The “ Sparse Candidate ” Algorithm , UAI 1999 .
[ 14 ] Schmidt , M . , Niculescu Mizil , A . and Murphy , K . 2007 . A .
Learning Graphical Model Structures using L1Regularization Paths , AAAI 2007 .
[ 15 ] Tibshirani , R . 1996 . Regression Shrinkage and Selection via the Lasso , Journal of Royal Statistical Society , Series B , 58(1):267–288 .
[ 16 ] Tsamardinos , I . , Brown , L . and Aliferis , C . 2006 . The Max
Min Hill Climbing Bayesian Network Structure Learning Algorithm , Machine Learning , 65(1 ) , 31 78 .
[ 17 ] Margaritis , D . and Thrun , S . 1999 . Bayesian Network induction via local neighborhoods . NIPS 1999 .
[ 18 ] Pellet , J . and Elisseeff , A . 2008 . Using Markov Blankets for
Causal Structure Learning , JMLR 9 , 1295 1342 .
[ 19 ] Meek , C . 1995 . Causal inference and causal explanation with background knowledge . UAI 1995 .
[ 20 ] Estrada , E . and Naomichi , H . 2008 . Communicability in
Complex Networks . Phys . Rev . E 77 036111 .
[ 21 ] Gill , PE Class Notes for Math 271ABC : Numerical Optimization . Department of Mathematics , UCSD .
[ 22 ] BERTSEKAS , D . 1999 . Nonlinear Programming , 2nd
Edition , Athena Scientific , Belmont .
[ 23 ] Fu , W . 1998 . Penalized Regressions : the Bridge versus the Lasso , Computational and Graphical Statistics , 7 ( 3 ) , 397416 .
[ 24 ] Friedman , J . ; Hastie , T . ; Hofling , H . and Tibshirani , R . 2007 .
Pathwise Coordinate Optimization , The Annals of Applied Statistics 2 , 302 332 .
[ 25 ] Aliferis , C . , Tsamardinos , I . and Statnikov , A . 2003 . HITON , a Novel Markov Blanket Algorithm for Optimal Variable Selection . AMIA 2003 , 21 25 .
[ 26 ] Tsamardinos , I . and Aliferis , C . 2003 . Towards Principled
Feature Selection : Relevancy , Filters and Wrappers . AISTAT 2003 .
[ 27 ] http://wwwcshujiacil/labs/compbio/Repository [ 28 ] Tzourio Mazoyer , N . , Landeau , B . , Papathanassiou , D . ,
Crivello , F . , Etard , O . , Delcroix , N . , Mazoyer , B . and Joliot , M . 2002 . Automated Anatomical Labeling of Activations in SPM using a Macroscopic Anatomical Parcellation of the MNI MRI Single Subject Brain . Neuroimage,15:273 289 . [ 29 ] Supekar , K . , Menon , V . , Rubin , D . , Musen , M and Grecius ,
M . 2008 . Network Analysis of Intrinsic Functional Brain Connectivity in AD . PLoS Comput Biol 4(6 ) 1 11 .
[ 30 ] Wang , K . , Liang , M . , Wang , L . , Tian , L . , Zhang , X . , Li , K . and Jiang , T . 2007 . Altered Functional Connectivity in Early
AD : A Resting State fMRI Study . Human Brain Mapping 28 , 967 978 .
[ 31 ] Gould , R . , Arroyo , B . Brown , R . , Owen , A . , Bullmore , E and
Howard , R . 2006 . Brain Mechanisms of Successful Compensation during Learning in AD , Neurology 67 ( 6 ) , 1011 1017 .
[ 32 ] Stern , Y . 2006 . Cognitive Reserve and Alzheimer Disease ,
Alzheimer Disease Associated Disorder 20 , 69 74 .
[ 33 ] Korb , KB and Nicholson , AE 2003 . Bayesian Artificial
Intelligence . Chapman & Hall/CRC , London , UK .
[ 34 ] Greicius , M . , Srivastava , G . , Reiss , A . and Menon , V . 2004 .
Default mode Network Activity Distinguishes AD from Healthy Aging : Evidence from Functional MRI , PNAS . 101 , 4637–4642 .
[ 35 ] Alexander , GE , Chen , K . , Pietrini , P . , Rapoport S . and
Reiman , E . 2002 . Longitudinal PET Evaluation of Cerebral Metabolic Decline in Dementia : A Potential Outcome Measure in AD Treatment Studies . AmJPsychiatry 159 , 738 745 .
[ 36 ] Braak , H . , Braak , E . , 1996 . Evolution of the Neuropathology of Alzheimer's Disease . Acta Neurol Scand Suppl 165 , 3 12 .
[ 37 ] Braak , H . , Braak , E . , Bohl , J . , 1993 . Staging of Alzheimer
Related Cortical Destruction . Eur Neurol 33 , 403 408 .
[ 38 ] Hedden , T . , Van Dijk , K . Becker , J . , Mehta , A . , Sperling , R . , Johnson , K . and Buckner , R . 2009 . Disruption of Functional Connectivity in Clinically Normal Older Adults Harboring Amyloid Burden . J . Neurosci . 29 , 12686–12694 .
[ 39 ] Andrews Hanna , J . , Snyder , A . , Vincent , J . , Lustig , C . , Head ,
D . , Raichle , M and Buckner , R . 2007 . Disruption of LargeScale Brain Systems in advanced Aging , Neuron 56 , 924– 935 .
[ 40 ] Wu , X . , Li , R . Fleisher , AS , Reiman , E . , Guan , X . , Zhang ,
Y . , Chen , K . and Yao , L . 2011 . Altered Default Mode Network Connectivity in Alzheimer ’s Disease A Resting Functional MRI and BN Study , Human Brain Mapping , in press .
[ 41 ] Ikonomovic , M . , Klunk , W . Abrahamson , E . Mathis , C .
Price , J . , Tsopelas , N . , Lopresti , B . et al . 2008 . Post mortem Correlates of in vivo PiB PET Amyloid Imaging in a Typical Case of Alzheimer's Disease . Brain 131 , 1630 1645 .
[ 42 ] Klunk , W . , Engler , H . , Nordberg , A . et al . 2004 . Imaging brain amyloid in Alzheimer's disease with Pittsburgh Compound B . Ann Neurol . 55 , 306 319 .
[ 43 ] Reuter Lorenz , P . and Mikels , J . 2005 . A Split Brain Model of Alzheimer ’s Disease ? Behavioral Evidence for Comparable intra and interhemispheric Decline . Neuropscyhologia 43 , 1307 1317 .
[ 44 ] Lipton , A . , Benavides , R . , Hynan , LS , Bonte , FJ , Harris ,
TS , White , CL 3rd , Bigio , EG 2004 . Lateralization on Neuroimaging does not Differentiate Frontotemporal Lobar Degeneration from Alzheimer ’s Disease . Dement Geriatr Cogn Disord 17(4 ) , 324 327 .
939
