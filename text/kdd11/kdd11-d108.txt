Online Heterogeneous Mixture Modeling with Marginal and Copula Selection
Ryohei Fujimaki
NEC Laboratories America Media Analytics Department NEC Laboratories America rfujimaki@sv.nec labs.com
∗ Yasuhiro Sogawa
The Institute of Scientific and
Industrial Research
Osaka University
Satoshi Morinaga Information and Media
Processing Research Labs .
NEC Corporation sogawa@arsankenosaka morinaga@cwjpneccom uacjp
ABSTRACT This paper proposes an online mixture modeling methodology in which individual components can have different marginal distributions and dependency structures . Mixture models have been widely studied and applied to various application areas , including density estimation , fraud/failure detection , image segmentation , etc . Previous research has been almost exclusively focused on mixture models having components of a single type ( eg , a Gaussian mixture model . ) However , recent growing needs for complicated data modeling necessitate the use of more flexible mixture models ( eg , a mixture of a lognormal distribution for medical costs and a Gaussian distribution for blood pressure , for medical analytics . ) Our key ideas include : 1 ) separating marginal distributions and their dependencies using copulas and 2 ) online extension of a recently developed “ expectation minimization of description length , ” which enable us to efficiently learn types of both marginal distributions and copulas as well as their parameters . The proposed method provides not only good performance in applications , but also scalable , automatic model selection , which greatly reduces the intensive modeling costs in data mining processes . We show that the proposed method outperforms state of the art methods in application to density estimation and to anomaly detection .
Categories and Subject Descriptors H28 [ Database Management ] : Database applications— Data Mining ; I26 [ Artificial Intelligence ] : Learning
General Terms Algorithms
∗Under internship program of NEC Corporation .
1 .
INTRODUCTION
Mixture models have been widely studied and applied in a variety of areas , including topic analysis [ 15 ] , fraud detection [ 27 ] , image segmentation [ 25 ] , and speech recognition [ 20 ] . Previous research has been almost exclusively focused on mixture models having single types of distributions ( eg , a Gaussian mixture model , a Bernoulli mixture model , etc . ) However , in many real applications , the assumption that all variables have the same distribution type will not be satisfied , and the estimation/prediction performance of such models will consequently degrade . In medical analytics [ 6 ] , for examples medical costs and blood pressure usually have , respectively , a log normal distribution ( or a mixture of lognormal and normal distributions ) and a ( skewed ) normal distribution .
Fig 1 presents the concept of the issue dealt with in this paper . “ Component 1 ” has ( lognormal , Gaussian ) marginal distributions , and they have negative dependency . “ Component 2 ” has ( Gaussian , Gaussian ) marginal distributions , and they have positive dependency . The learning problem with such models includes selection of the number of components and types of marginal distributions and their dependency structures as well as parameter estimation . A brute force search with such classical statistical information criteria as Akaike ’s information criterion [ 1 ] or the minimum description length principle ( MDL ) [ 21 ] might be expected to suffer from a combinatorial scalability problem with respect to types of components .
Recently , Fujimaki et al . [ 9 ] have proposed the concept of “ heterogeneous mixture models , ” in which individual components can have different types . The algorithm , “ expectation minimization of description length ( EMDL ) , ” addresses the model selection issue for heterogeneous mixture models in which types of components , as well as the number of components and their parameters , are optimized . Their model and EMDL have two limitations wrt our problem : Limitation 1 ) it is difficult to define joint distributions for individual components having different types of marginal distributions1 , and Limitation 2 ) the number of component candidates will become prohibitively large with increasing data dimensionality D if we allow different variables to have different types of marginal distributions2 .
1Let us consider the case where random variables X1 and X2 have different types of marginal distributions . The pdf p(X1|X2 ) or P ( X2|X1 ) will not generally be obvious . 2Let us assume that we have two marginal candidates ,
645 Figure 1 : Concept of our problem . The goal is to specify all of the number of components , types of marginal distributions , dependency structures , and their parameters .
The goal of this study is to construct a mixture modeling methodology for the simultaneous model selection and parameter estimation described above . In addition to achievement of good estimation/prediction performance using a flexible model , we address computational scalability and modeling automation issues , which are essential for reduction of the total cost of data mining processes .
Our key ideas and contributions may be summarized as :
Heterogeneous Marginals and Copulas .
We separate , for individual components , marginal distributions and their dependency structures using copulas [ 17 ] to address Limitation 1 . Our model here , which we refer to as a “ mixture model with heterogeneous marginals and copulas ” ( MHMC ) , allows us to use both heterogeneous types of marginal distributions and heterogeneous types of dependencies among variables ( ie , copulas ) , and it provides a powerful tool of modeling complicated data . From the viewpoint of computational and modeling costs , this direct incorporation of copulas into the heterogeneous mixture model actually contributes importantly to the efficiency of our learning algorithm ( we can separately optimize individual marginal distributions and copulas . )
Online Model Selection Algorithm .
We propose an online EMDL algorithm for MHMC which automatically optimizes the number of components , types of marginal distributions , types of copulas , and their parameters . By combining MHMC and ( online ) EMDL , we can avoid the combinatorial scalability issue involved in selecting marginal and copula types ( Limitation 2 ) . Further , MHMC with online EMDL , which we abbreviate as OMHMC , addresses the scalability issue for large datasets , and it is also expected to have good convergence properties , on the basis
Gaussian and lognormal . Then we have 2D candidates for marginal combinations for each component : ie , ( Gaussian , Gaussian ) , ( Gaussian , lognormal ) , ( lognormal , Gaussian ) , ( lognormal , lognormal ) for D = 2 .
Figure 2 : Related work of an analogy with the EM algorithm [ 5 ] and the incremental EM algorithm3 [ 16 ] .
To the best of our knowledge , OMHMC is the first online model selection algorithm which is capable of optimizing types of components ( further , OMHMC is capable of optimizing marginal distributions and copulas for individual components . ) Fig 2 lists a number of studies for parameter estimation and model selection of the number of component algorithms [ 1 , 4 , 5 , 12 , 18 , 21 ] and their online/nonstationary learning extensions [ 16 , 15 , 22 , 27 , 28 ] , including OMHMC .
We summarize the notation in Section 6 . It is rather complicated because we must distinguish a large number of variables and parameters ( marginal distributions of individual variables , copulas for individual components , and their parameters . )
2 . MIXTURE MODEL WITH HETEROGE
NEOUS MARGINALS AND COPULAS
2.1 Copulas
Copulas [ 17 ] separate a dependency structure among variables from their marginal distributions . Intuitively speaking , their joint distribution can be represented as the product of the marginals of individual variables and a copula . Historically , copulas have been actively studied in financial engineering [ 13 , 17 ] in order to model dependencies among different types of risks . Recently , data mining and machine learning communities have also noticed their powerful and useful properties , and a number of studies have been conducted [ 7 , 26 ] .
Let X = ( X1 , . . . , XD ) be a random variable . Let F ( x ) = P ( X1 ≤ x1 , . . . , XD ≤ xD ) and Fd(xd ) = Pd(Xd ≤ xd ) be cumulative distribution functions ( cdfs ) , respectively .
A copula function is formally defined as follows :
Definition 1
( Copula ) . Let U = ( U1 , . . . , UD ) and u = ( u1 , . . . , uD ) be , respectively , random variables uniformly distributed on [ 0 , 1]D and its realization . Copula function Q : [ 0 , 1]D → [ 0 , 1 ] is a joint distribution function represented as follows :
Q(u ) = P ( U1 ≤ u1 , . . . , UD ≤ uD ) .
( 1 )
Sklar ’s theorem [ 24 ] provides theoretical support of copu las : 3We were unable to confirm that our online EMDL had better convergence than the batch EMDL because a numerical error resulted in non convergence of the batch algorithm . Briefly , x 6= ζ −1(ζ(x) ) , where ζ and ζ −1 are , respectively , a cumulative distribution function and its inverse .
646 Theorem 1
( Sklar ’s theorem ) . There exists a cop
3 . ONLINE MIXTURE MODELING ula function such that
F ( x ) = Q(F1(x1 ) , . . . , FD(xD) ) .
( 2 )
If F1 , . . . , FD are all continuous , then Q is unique .
Let us denote joint and marginal probabilistic density and fd(Xd ) = ∂Fd(Xd ) , functions ( pdfs ) as f ( X ) = ∂D F ( X ) respectively . Then , using the chain rule , we have ,
∂X1 ···∂XD
∂Xd f ( X ) =
∂DQ(F1(X1 ) , . . . , FD(XD ) )
∂F1(X1 ) · · · ∂FD(XD )
D
Y d=1 fd(Xd )
= q(F1(X1 ) , . . . , FD(XD ) )
D
Y d=1 fd(Xd ) ,
( 3 ) where q(F1(X1 ) , . . . , FD(XD ) ) is referred to as the “ copula density function . ”
2.2 Heterogeneous Mixture Modeling
Let us define “ component candidates ” as S = {Sj|j = 1 , . . . , |S|} , where | ◦ | represents the number of elements in a set ◦ . If Sj is Gaussian , p(X ; φ(Sj ) , Sj ) is the Gaussian distribution having parameter φ(Sj ) . For notational simplicity , we denote φ(Sj ) as φ alone , so long as there is no fear of confusion .
For given component candidates S , a heterogeneous mix ture model is defined as follows [ 9 ] :
3.1 Expectation Minimization of Description
Length
For MHMC , we must specify the number of components C , the types of marginal distributions Mcd , and the types of copulas Qc as well as their parameters θ . This is a issue of “ model selection for heterogeneous models , ” and an efficient batch learning framework , “ expectation minimization of description length ” ( EMDL ) , has been proposed [ 9 ] .
Let xN = x1 , . . . , xN be N observed data of X . Let us denote latent variables as zN = z1 , . . . , zN , where zn = ( zn1 , . . . , znC ) . zn indicates the component assignment of xn and znc = 1 if xn is generated from the c th component and znc = 0 otherwise . A pair ( xN , zN ) is called “ complete data ” while xN by itself is called “ incomplete data . ”
Like the EM algorithm [ 5 ] , for the fixed number of components C , EMDL iterates the EM steps by optimizing the types of components Sc and their parameter θc = ( αc , φc1 , . . . , φcD , ξC ) for ∀c . This procedure for MHMC is described below . In the t th E step , we calculate the posterior of latent variables as follows :
D p(znc|xN ,θ(t−1 ) c
, S(t−1 ) c
) ∝ α(t−1 ) c pcd(xnd ; φ(t−1 ) cd
, M ( t−1 ) cd
)
× qc(ψ(t−1 ) c
; ξ(t−1 ) c
( 6 )
Y d=1 , Q(t−1 ) c
) , p(X ; θ , S , C ) =
C
X c=1
αcpc(X ; φc , Sc ) .
( 4 ) where superscription ( t ) represents the t th EM step . Then , in the t th M step , for individual components , we optimize Sc and φc as follows :
We denote the number of components , types of components , and parameters of components as C , S = ( S1 , . . . , SC ) ( Sc ∈ S ) and θ = ( α , φ1 , . . . , φC ) , respectively . α = ( α1 , . . . , αC ) is the mixture coefficient .
This paper proposes a mixture model with heterogeneous marginals and copulas ( MHMC ) in order to address Limitations 1 ) and 2 ) for ( 4 ) . Let us define “ marginal candidates ” and “ copula candidates ” as M = {Mj|j = 1 , . . . , |M|} and Q = {Qj|j = 1 , . . . , |Q|} , respectively . MHMC is defined by replacing the pdf of each component in ( 4 ) with the product of marginal distributions and a copula as follows : pc(X ; φc , Sc ) = qc(ψc ; ξc , Qc )
D
Y d=1 pcd(Xd ; φcd , Mcd ) .
( 5 ) pcd(Xd ; φcd , Mcd ) and φcd are the marginal distribution and its parameter4 specified by Mcd ∈ M . ψcd = Fcd(Xd ) is the cdf . of pcd(Xd ; φcd , Mcd ) where ψc = ( ψc1 , . . . , ψcD ) . qc(ψc ; ξc , Qc ) and ξc are , respectively , the copula density function and its parameter5 specified by Qc ∈ Q . We redefine the component types and the parameter in ( 4 ) as Sc = ( Mc1 , . . . , McD , Qc ) and θ = ( α , φ11 , . . . , φCD , ξ1 , . . . , ξC ) , respectively .
A copula plugged into each component naturally addresses Limitation 1 ) . In addition , by separating the marginal distributions and dependencies among them using copulas , we can avoid the combinatorial issue ( Limitation 2 ) ) and can efficiently learn the model . 4For example , if Mcd is Gaussian , pcd is a one dimensional Gaussian distribution and φcd is mean and variance . 5For example , if Qc is Gaussian , q is a Gaussian copula and ξc is a correlation matrix .
S(t ) c , θ(t ) c = arg min Sc,θc
Z hJ(xN , zN ; αc , φc , Sc)i , E(t )
( 7 )
J(xN , zN ; θc , Sc ) =
N
X n=1
−znc
D
X d=1 log pcd(xnd ; φcd , Mcd )
+ log qc(ψnc ; ξc , Qc ) + log αc + PD d=1 β(Mcd ) + β(Qc ) log
2
N
X n=1 znc , where ψnc = ( ψnc1 , . . . , ψncD ) , ψncd = Fcd(xnd ) and φc = ( φc1 , . . . , φcD , ξc ) . The objective J(xN , zN ; θc , Sc ) is the expected description length of complete data wrt the cth component . E(t ) the posterior ( 6 ) . β(• ) is the dimensionality of • , ie , β(Mcd ) = 2 if Mcd is Gaussian .
Z [ • ] is the expectation of • wrt
Fujimaki et al . [ 9 ] show that it is possible to efficiently cal culate the second order approximation6 of E(t ) for the last term in ( 7 ) as follows :
Z [ log(PN n=1 znc ) ]
E(t )
Z [ log(
N
X n=1 znc ) ] ≈ log
Z [ znc ] E(t )
N
X n=1 E(t ) n=1
− PN
( 8 )
Z [ znc ] − PN n=1 E(t ) 2(PN
Z [ znc])2 n=1(E(t )
Z [ znc])2
.
The number of components is determined on the basis of
6We can calculate the higher order approximations with O(N ) computational cost .
647 MDL [ 21 ] as follows :
N
X
C n−
ˆC = arg min log p(xn ; ˆθ , ˆS , C)+ log No , ( 9 ) where ˆθ and ˆS are the estimated parameters and the component types for C through the EM procedure . n=1
2
β(S ) + C − 1
The key to MHMC in EMDL is that the M step can be separated into optimizations of individual marginals and copulas for individual components , and therefore we can avoid the combinatorial issue for component types . Otherwise , a naive modeling of the joint distribution of each component would suffer from a drastic increase in the number of component candidates ( ie , the combination of marginal distributions . ) even if we made use of EMDL for Limitation 2 ) . Further , in order to address the scalability issue for large scale data , we next propose our online learning algorithm .
3.2 Online EMDL for MHMC
We design the online EMDL with reference to two existing algorithms : an incremental EM algorithm [ 16 ] and an inference functions for marginals ( IFM ) [ 10 ] . The incremental EM algorithm estimates a mixture model by incrementally updating sufficient statistics with respect to complete data . Neal and Hinton [ 16 ] show that incremental learning provides faster convergence than the standard batch EM algorithm . For a joint distribution having a single component , IFM estimates the marginal parameters first and then estimates the copula parameter among them because the simultaneous estimation of marginals and a copula is generally prohibitively difficult .
321 Online Update Procedure
Let us assume that the number of components C is fixed ( C is optimized in ( 21) . ) The online EMDL sequentially updates the model and sufficient statistics on the basis of input dataset Xt = xK0+K(t−1)+1 , . . . , xK0+Kt , where K0 and K are the numbers of initial data X0 and sequential dataset Xt ( X0 = x1 , . . . , xK0 . ) Though the cases K = 1 and K > 1 are usually called “ online ” and “ mini batch ” , respectively , we do not distinguish them .
The update procedure for each component consists of the four steps summarized below . Let us redefine the superscription ( t ) as the notation of the t th online update .
Latent Variable Update .
The latent variable update corresponds to the E step in EMDL . First , we calculate the posterior expectation of the latent variable E(t ) Z [ znc ] ( n = K0 + K(t − 1 ) + 1 , . . . , K0 + Kt ) , which calculation is identical to that of the posterior distribution of znc , as follows :
E(t )
Z [ znc ] =
PC c
α(t−1 ) c=1 α(t−1 ) c pc(xn ; φ(t−1 ) c c pc(xn ; φ(t−1 ) c
, S(t−1 )
)
, S(t−1 ) c
.
)
( 10 )
For sufficient statistics , we update the r th order moment of latent variables as follows :
U ( t ) rc = U ( t−1 ) rc
K0+Kt
( E(t ) + n=K0+K(t−1)+1
X
Z [ znc])r
( r = 1 , . . . , Rz ) , ( 11 ) where Rz is set7 to Rz = 2 . The first order moment is used 7To obtain more accurate approximation than ( 8 ) , we could in ( 19 ) and ( 12 ) , and moments of higher order than one are used in ( 12 ) . We can calculate the complexity term of the c th component in the t th update as follows : c = log(U ( t ) E ( t )
1c ) − ( U ( t )
1c − U ( t )
2c )/2(U ( t )
1c )2 ,
( 12 ) where E ( t ) c is used in ( 15 ) and ( 18 ) .
Marginal Update .
This update step , corresponding to the M step of EMDL , updates moments of individual marginal distributions and optimizes types of marginals and their parameters . As for sufficient statistics , we update the r th order moment of observed variables as follows :
K0+Kt
V ( t ) rcd = V ( t−1 ) rcd +
E(t ) n=K0+K(t−1)+1
X
Z [ znc](xnd)r
( r = 1 , . . . , Rx ) .
( 13 ) For a given marginal candidate ¯M ∈ M , we obtain the moment matching estimator8 ¯φcd( ¯M , V ( t ) cd ) of φcd , where V ( t ) ( V ( t ) 1cd , . . . , VRcd ) . Using moment matching estimation , we need to memorize only moments . This is an advantage over other estimation methods , such as maximum likelihood estimation , which memorizes parameters for all marginal candidates . We set Rx to a value sufficient for moment matching estimation ( Here , Rx = 2 . See the next section for more detail . ) cd =
We then update the ( negative ) expected complete likeli hood of the marginal distribution ¯M as follows :
I ( t ) cd ( ¯M , V ( t ) cd ) = I ( t−1 ) cd
( ¯M , V ( t−1 ) cd
)−
( 14 )
K0+Kt
Z [ znc ] log pcdxcd ; ¯φcd( ¯M , V ( t ) E(t ) n=K0+K(t−1)+1
X cd ) , ¯M , using the moment matching estimator ¯φcd( ¯M , V ( t ) cd ) . The optimal types of marginals and their parameters are obtained by solving the following optimization problem :
M ( t ) cd , φ(t ) cd = arg min
¯M∈M , ¯φcdnI ( t ) cd ( ¯M , V ( t ) cd ) +
β( ¯M )
2 c o . E ( t )
( 15 )
Copula Update .
This update step , updating moments of copulas and optimizing types and parameters of copulas , also corresponds to the M step in EMDL . As with IFM , the types and parameters of marginals are fixed in this step . In order to avoid notational confusion , we here explain the update procedure for the Gaussian type of copulas which this paper implements ( Section 4 . ) Other types of copulas , such as tcopulas and Archimedean copulas [ 17 ] , can be implemented in a similar manner . set a higher value of Rz but this would scarcely have a practical impact . 8For a Gaussian distribution , the first and second order moment matching estimator is consistent with the maximum likelihood estimator . The same holds for the first order moment matching estimator in an exponential distribution .
648 For sufficient statistics , we update the moment for copulas as follows :
K0+Kt
W ( t ) c = W ( t−1 ) c
+
E(t ) n=K0+K(t−1)+1
X
Z [ znc]ζ −1(ψ(t ) nc )ζ −1(ψ(t ) nc )† , ( 16 ) where ζ −1(ψnc ) = ( ζ −1(ψnc1 ) , . . . , ζ −1(ψncD) ) . ζ −1 is the inverse cdf of the standard Gaussian distribution . The superscription † represents the vector/matrix transpose operation .
We then update the ( negative ) expected complete likeli hood of the copula ¯Q as follows :
J ( t ) c c ) = J ( t−1 ) c
( ¯Q , W ( t ) K0+Kt
( ¯Q , W ( t−1 ) c
)−
( 17 )
E(t ) n=K0+K(t−1)+1
X
Z [ znc ] log qc(ψ(t ) nc ; ¯ξc(W ( t ) c , ¯Q ) , ¯Q ) , c where ¯Q ∈ Q . We use the moment matching estimator ¯ξc(W ( t ) of ξc as in the case of the marginal update . The optimal types of copulas and their parameters are obtained by solving the following optimization problem : β( ¯Q )
Q(t ) c , ξ(t ) c = arg min
¯Q∈Q,¯ξc nJ ( t ) c
( ¯Q , W ( t ) c ) + c o . E ( t )
( 18 )
2
In the marginal update and the copula update , we can separately optimize the individual marginal distributions and copulas for individual components , and therefore can address the combinatorial scalability issue ( Limitation 2 ) . This is an important advantage in our modeling , in which the joint distributions of individual components are divided into marginal distributions and copulas ( 5 ) .
Other Updates .
At the end of the t th update procedure , we update the number of data , the mixture ratio , and the negative incomplete likelihood as follows :
N ( t ) = N ( t−1 ) + K , α(t ) c = U ( t )
1c /N ( t ) ,
( 19 )
K0+Kt
L(t)(C ) = L(t−1)(C ) − log p(xn ; θ(t ) , S(t ) , C ) , ( 20 )
X n=K0+K(t−1)+1 where both θ(t ) and S(t ) are functions of C . We can then select the number of components which will minimize the value of MDL as follows :
β(S(t ) ) + C − 1
C ( t ) = arg min
1≤C≤CmaxnL(t)(C ) + log N ( t)o , ( 21 ) where Cmax is the predetermined maximum component size . Through these online update procedures , we optimize the number of components ( 21 ) , types and parameters of marginal distributions ( 15 ) , and types and parameters of copulas ( 18 ) .
2
322 Initialization Procedure
For the initialization ( t = 0 ) , we randomly set E(0 )
Z [ znc ] ( n =
1 , . . . , K0 ) . Then we execute the same procedure with ( 11)(21 ) , where the terms having superscription ( t − 1 ) are set to zero .
323 Whole Algorithm Description
( T )
, A(T )
Algorithm 1 Online EMDL for MHMC input : {X0 , X1 , . . . , XT } , Cmax , M , Q output : C ( T ) , S(T ) , ˆθ 1 : for C = 1 , . . . , Cmax do 2 : 3 : end for 4 : for t = 1 , . . . , T do 5 : 6 : for C = 1 , . . . , Cmax do
( S(t ) , θ(t ) , A(t ) ) =
( S(0 ) , θ(0 ) , A(0 ) ) = Initialization(X0 , C , M , Q ) updateM odel(Xt , C , M , Q , S(t−1 ) , θ(t−1 ) , A(t−1 ) ) end for Select C ( t ) , S(t ) , θ(t ) by solving ( 21 )
7 : 8 : 9 : end for rc and E ( t ) c
( (10) (12 ) ) cd ) using the moment
Algorithm 2 updateModel input : Xt , C , M , Q , S(t−1 ) , θ(t−1 ) , A(t−1 ) output : S(t ) , θ(t ) , A(t ) 1 : Calculate E(t ) 2 : for c = 1 . . . C do 3 : 4 : 5 : 6 :
Update V ( t ) for ¯M ∈ M do
Z [ znc ] and update U ( t ) rcd based on ( 13 ) for d = 1 . . . D do
¯φ(t ) cd ( ¯M , V ( t ) Calculate matching method for ¯M Update I ( t ) cd ( ¯M , V ( t ) cd ) ( 14 ) cd and φ(t ) cd based on ( 15 ) c based on ( 16 ) end for Select M ( t ) end for Update W ( t ) for ¯Q ∈ W do
7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : 17 : end for
Calculate ¯ξc(W ( t ) Update J ( t ) c , ¯Q ) ( ¯Q , W ( t ) c c ) based on ( 17 ) end for Select Q(t ) c and ξ(t ) c based on ( 18 ) all moments A = Urc , Vqcd , Wc , Icd , Jc , L , N ( for all c = 1 , . . . , C , d = 1 , . . . , D , r = 1 , . . . , Rz , q = 1 , . . . , Rx . )
First , the initialization ( steps 1 3 in Algorithm 1 ) is executed for each component on the basis of the initialization procedure described in Section 322 For the t th update with the fixed number of components C ( steps 4 9 in Algorithm 1 ) , we update the corresponding model on the basis of Algorithm 2 . In Algorithm 2 , we first execute the latent variable update ( step 1 ) . Then , for each component , marginal updates ( step 3 10 ) and a copula update ( steps 11 16 ) are executed , and we obtain the updated model and parameters S(t ) , θ(t ) , A(t ) . In step 8 of Algorithm 1 , we select the number of components C ( t ) .
3.3 Implemented Model
In this paper , we implement MHMCs in which M is a set of ( Gaussian , exponential , lognormal ) and Q is a set of sparse Gaussian copulas .
Gaussian Marginal .
The Gaussian marginal distribution described as
The whole process of Online EMDL for MHMC ( OMHMC ) is described in Algorithms 1 and 2 . Let us define A as pcd(Xd ; φcd , Mcd ) =
1 p2π(σcd)2 exp −(Xd − µcd)2
2(σcd)2
( 22 )
649 has two parameters , mean µcd and variance ( σcd)2 , and the moment matching estimator is obtained as follows :
¯φ(t ) cd = ( ¯µ(t ) cd , ( σ(t ) cd )2 ) = ( V ( t )
1cd/U ( t )
1c , ( V ( t )
2cd − ( V ( t )
1cd)2)/U ( t )
1c ) ,
( 23 )
The parameter dimensionality β(Mcd ) is 2 .
Exponential Marginal .
The exponential marginal distribution described as pcd(Xd ; φcd , Mcd ) =
1 λcd exp(−Xd/λcd )
( 24 ) has one scale parameter , λcd , and the moment matching estimator is obtained as follows :
For OMHMC , the hyperparameters are set to Cmax = 10 , M = {gaussian , lognormal , exponetial} , Q = {τ = 0 , τ = 0.05 , τ = 0.1 , τ = 0.2} , K0 = 10 , T = 100 ( therefore , K takes different values for different data ) . All results represent the averages of ten runs ( or ten fold cross validation . ) In Tables 1 and 2 , the best value for each dataset is highlighted in boldface .
4.1 Model Selection
In this evaluation , we investigated the basic model selection performance of OMHMC , especially focusing on selection of marginal and copulas types . Hereafter , superscription ∗ represents the true model .
1cd/U ( t ) The parameter dimensionality β(Mcd ) is 1 .
¯φ(t ) cd = ( ¯λ(t ) cd ) = ( V ( t )
1c ) ,
( 25 )
Experimental Settings .
We first evaluated marginal selection error and copula se lection error , which are calculated9 as :
Lognormal Marginal .
The lognormal marginal distribution described as
1 pcd(Xd ; φcd , Mcd ) =
( 26 ) has two parameters , log mean νcd and log variance ( ρcd)2 , and the moment matching estimator is obtained as follows : exp −(log(Xd ) − νcd)2 p2π(ρcd)2Xd
2(ρcd)2
¯φ(t ) cd = ( ¯ν ( t ) cd , ( ρ(t ) = ( log(V ( t ) cd )2 ) 1cd ) − ( σ(t ) cd )2/2 , 1 + log(V ( t )
2cd/(V ( t )
1cd)2 ) − 1 ) ,
( 27 )
The parameter dimensionality β(Mcd ) is 2 .
Sparse Gaussian Copula .
The sparse Gaussian copula , having sparse correlation pa rameter ξc = Σc(τ ) , is described as :
ERRM =
ERRC =
1
C ∗D
C∗
D
X c=1
X d=1
1
C ∗D(D − 1 )
( M ∗ cd 6= M ( T ) cd ) ,
C∗
X c=1
||B(ξ∗ c ) − B(ξ(T ) c
( 31 )
)||F ,
( 32 ) where [ B(•)]ij = 0 if [ •]ij = 0 , and [ B(•)]ij = 1 otherwise . | • |F is the Frobenius norm of • . ERRC is the ratio of non diagonal elements of [ B(ξ∗ )]ij . Note that both ERRM and ERRC are zero if M ∗ cd and ξ∗ c = ξ(T ) for all c and d . Since there is no previously reported c algorithm which can optimize marginal and copula types , we only evaluated them for OMHMC . c )]ij 6= [ B(ξ(T ) cd = M ( T ) c
We also evaluated the Kullback Leibler ( KL ) divergence between the true pdf and the estimated pdf , which was approximated as10 : qc(ψc ; ξc , Qc ) =
N ( ψc ; 0 , Σc(τc ) ) QD d=1 N ( ψcd ; 0 , 1 )
,
( 28 )
KL =
NKL
X n=1 p(xKL n ; θ∗ , S∗ , C ∗ ) log n ; θ∗ , S∗ , C ∗ ) p(xKL n ; θ(T ) , S(T ) , C ( T ) ) p(xKL
, where 0 is a zero vector and N ( • ; ∗ , ⋆ ) is the multivariate Gaussian distribution of • with mean ∗ and correlation ⋆ ( diagonal elements of ⋆ are one ) . The moment matching estimator is obtained as follows :
[ ¯ξ
( t ) c ]ij = [ ¯Σc(τc)]ij =
 
1 if i = j c(t ) if i 6= j ij if i 6= j 0 c ]ij /q[W ( t ) cij = [ W ( t ) c ]ii[W ( t ) c ]jj ,
|c(t ) |c(t ) ij | > τc ij | ≤ τc
( 29 )
( 30 ) where [ •]ij represents the ( i , j) th element of a matrix • . If τc is 0 , the sparse Gaussian copula will be consistent with the regular Gaussian copula . Candidates for sparse Gaussian copulas can be defined as those having different values of τ . The parameter dimensionality β(Qc ) is the number of non diagonal , non zero elements divided by two .
4 . EXPERIMENTS AND DISCUSSION
We conducted experiments with respect to two basic investigations and two applications . The former consisted of evaluations of 1 ) model selection performance and 2 ) scalability and update scheduling . The applications were 3 ) density estimation and 4 ) anomaly detection , both of which are important applications for mixture modeling . n where xKL are samples for the KL calculation . We compared OMHMC with a state of the art variational Dirichlet process Gaussian mixture model11 [ 12 ] ( VDPGMM ) .
We artificially generated the true model on the basis of Algorithm 3 . Here we only explain the results for C ∗ = 5 because of space limitations ( in fact , results show that the value of C ∗ scarcely affected basic behaviors . ) In step 1 , parameters for marginal distributions were sampled from cd ∼ [ −5 , 5 ] , ( σ∗ µ∗ cd ∼ ( 0 , 1.5 ] and ( ρ∗ c was sampled from [ 0 , 0.5 ] and ξ∗ c were generated using the “ gallery ” function in MATLAB . In step 3 , α∗ cd)2 ∼ [ 0.5 , 1.5 ] , λ∗ c was sampled from ( 0 , 1 ] . cd)2 ∼ [ 0.5 , 10 ] cd ∼ ( 0 , 1 ] , ν ∗
In step 2 , τ ∗
Results and Discussion .
Fig 3 presents ERRM and ERRC over the number of data and the data dimensionality . The marginal selection
9We had to identify the correspondence between the estimated components and the true components . For this purpose , we calculated Kullback Leibler divergences between all component pairs and determined the correspondence which minimizes the sum of KL divergence . 10We set NKL = 10000 in our experiments . 11We used the implementation of Kenichi Kurihara at http : //sitesgooglecom/site/kenichikurihara/ , and also used the default hyperparameter values in the software .
650 Algorithm 3 Data Generation Recipe input : C ∗ and D 1 : Randomly select M ∗ nential ) , and set their parameters φ∗ cd . cd from ( Gaussian , lognormal , expo c . Normalize α∗ c so that PC∗ c=1 α∗ c = 1 . c . The non diagonal elements of ξ∗ c and generate correlation matric smaller than τ ∗ c for Q∗ c
2 : Randomly set τ ∗ ces ξ∗ are set to be zero . 3 : Randomly set α∗
Figure 4 : KL divergences between the true model and the estimated model ( left : OMHMC , right : VDPGMM ) .
Figure 3 : Marginal selection error ( left ) and copula selection error ( right ) for OMHMC . errors are distributed around ERRM = 0.5 and are better than the marginal error of the usual Gaussian mixture model12 ( GMM ) . Because there are parameter ranges for which it is difficult ( or unnecessary ) to distinguish ( Gaussian , lognormal , exponential ) distributions , ERRM did not decrease over N ( in the other settings , we observed that ERRM decreased over N . ) The copula selection errors EM MC converged over N around EM MC = 0.2 , and OMHMC succeeded in its sparse dependency discovery . Since the correlation between different types of marginal distributions is no longer “ linear , ” this results indicate that OMHMC can discover nonlinear dependency structures among variables ( further , different components can have different nonlinearities13 . ) With respect to both ERRM and ERRC , the data dimensionality ( D = 5 ∼ 100 ) scarcely affected the model selection performance , while it will degrade in higher dimensional spaces due to the curse of dimensionality .
Fig 4 presents a comparison of the KL between OMHMC and VDPGMM . The KL of OMHMC ( left ) decreased over N , while the KL of VDPGMM did not markedly decrease . At high dimensionality ( D = 50 , 75 , 100 ) , OMHMC greatly outperformed VDPGMM .
4.2 Scalability and Update Scheduling
OMHMC has two important predetermined parameters : initial data size K0 and the number of updates T ( or size of mini batch data Xt , ie , K = ( N ( T ) − K0)/T . ) These update scheduling parameters determine the scalability and stability of OMHMC .
To investigate the behavior of OMHMC over K0 and T , we evaluated : 1 ) CPU time ( scalability ) , and 2 ) MDL value per data item , L(T )/N ( T ) ( convergence and stability ) . Here , we employed the KDDCUP99 network intrusion dataset , which has 976158 data in total ( see Section 4.4 for more detail . )
12Since we randomly selected M ∗ tial , lognormal ) , ERRM of GMM is 067 13For example , the correlation between Gaussian and lognormal distributions has different nonlinearity from that of the correlation between two lognormal distributions . cd from ( Gaussian , exponen
Figure 5 : CPU time and L(T )/N ( T ) . The left figure ( K0 = 10 ) shows CPU time over N ( T ) ( double log plot ) . The right figure ( N ( T ) = 5000 ) shows CPU time ( left axis ) and L(T )/N ( T ) ( right axis ) over T .
First , we observed that the impact of N ( T ) on CPU time was only linear ( Fig 5 , left . ) In fact , the difference between the cases of N ( T ) = 5000 and N ( T ) = 100000 is only about 20 sec , with little relation to T . Second , with T = 100 , L(T )/N ( T ) nearly converged , with little relation to K0 ( Fig 5 , right . ) On the other hand , CPU time grew linearly with T . We have obtained the significant insight that the choice of a relatively small value of T has a great advantage in scalability , with little performance degradation . We further observed that the smaller value of K0 showed better performance wrt L(T )/N ( T ) . All components were initialized to closely similar values of moments and parameters when K0 was large , and they had little difference through the updates in OMHMC . On the other hand , a small value of K0 generated differences among components . In summary , both T and K0 will preferrably be set to small values to obtain both good scalability and performance . The CPU time for all data ( N ( T ) = 976158 , K0 = 10 , T = 100 ) was 242.6 sec in total .
4.3 Density Estimation
Density estimation is the most basic and significant application of mixture modeling , and a number of unsupervised methods , such as topic modeling [ 15 ] , image segmentation [ 25 ] , and speech recognition [ 20 ] , are based on it .
We compared OMHMC with VDPGMM on the basis of the predictive likelihood ( likelihood of test data ) per data item . We used the seven datasets from the UCI data repository [ 8 ] that are shown in Table 1 .
As Table 1 shows , for the well known iris data , VDPGMM outperformed OMHMC , partly because iris data can be well modeled by a Gaussian mixture ( therefore OMHMC is likely
651 Table 1 : Predictive likelihood per data .
Data14
N 150 466 1255 5848 61236 68040 YPMSD 463811 iris FF yeast YQ CT CM
D 4 11 8 11 16 9 12
OMHMC 460±341
21.51 ± 2.90 31.77 ± 3.49 3.76 ± 0.66 16.16 ± 0.90 7.51 ± 0.62 13.59 ± 0.09
VDPGMM 2.00 ± 0.99 24.78 ± 0.23 2.081 ± 2.91 5.68 ± 1.73 15.50 ± 0.86 7.45 ± 0.64 13.59 ± 0.12 to be overfitted ) , and partly because N = 150 is too small for the online learning algorithm . With increasing N , OMHMC outperformed VDPGMM , except for the Color Moment data . This result indicates that the “ Gaussian assumption ” is not satisfied with real data , which our modeling with heterogeneous marginal distributions and copulas fit well . Since VDPGMM also has strong expressiveness , the density estimation performance of the two methods was competitive for large scale data ( YPMSD ) .
4.4 Anomaly Detection
Anomaly detection is also a significant application . On the basis of the assumption that the number of anomaly data is much smaller than that of normal data , mixture modelbased anomaly detectors [ 27 ] are used to detect anomalous data having low log likelihood ( or high negative loglikelihood . ) We employed datasets from the R¨atsch ’s benchmark repository [ 19 ] ( Dataset 1 ) and the KDDCUP99 network intrusion dataset15 . The sample size and data dimensionality of each dataset are shown in Table 2 .
For each dataset in Dataset 1 , to satisfy the small sample size assumption for anomaly data , we randomly selected 10 % of the anomaly data in each run . Further , for KDDCUP99 , we generated normal ( normal access ) and anomaly ( intrusion success ) data on the basis of generation recipe in [ 27 ] . Briefly , we chose ( duration , src bytes , dst bytes ) attributes and picked up all 972781 normal samples and 3377 intrusions whose logged in attributes were positive . Then , for each run , we randomly selected 50 % of the samples .
We compared OMHMC based anomaly detection with state of the art anomaly detectors : VDPGMM [ 12 ] , i forest [ 14 ] , and one class support vector machine ( OSVM ) [ 23 ] with a Gaussian kernel16 and local outlier factor ( LOF ) [ 2 ] . For a comparison with another mixture modeling method , we applied VDPGMM on the basis of “ log likelihood ” scoring , as with OMHMC , while the original VDPGMM has not been applied to anomaly detection . The performance was measured using Area Under the ROC Curve ( AUC ) , which is widely used in the domain of anomaly detection .
Table 2 shows a comparison of AUCs . OMHMC , VDPGMM , and i forest worked in large scale datasets where OSVM and LOF were not available because of computational time/memory issues . OMHMC and i forest outperformed the others , and their performance was similar ( both of them took first place for four datasets . ) Since estimation performance increases with an increased number of data , OMHMC has tended to perform well with comparatively large datasets . OSVM and
14FF:forest fire , WQ : wine quality , CT : Cooc Texture , CM : Color Moment 15http://wwwsigkddorg/kddcup/indexphp 16We use the libsvm [ 3 ] for implementation of OSVM .
LOF were sensitive to parameter changes , and we do not have systematic ways to choose their parameters ( cross validation is not available in anomaly detection . )
5 . SUMMARY
This paper has addressed the data mining issue for mixture modeling with complicated data having heterogeneous marginal distributions and dependencies among variables . The proposed OMHMC provides a powerful , scalable model selection tool that can execute fully automatic model selection of types of marginal distributions and copulas as well as parameter estimations . Our experimental results confirm the scalability and performance of OMHMC in application to density estimation and anomaly detection .
We have two challenges for future study . First , as seen in Fig 2 , a nonstationary extension of OMHMC remains as an open problem . It should be addressed by incorporating a time discounting concept into OMHMC . Second , mixture models have been employed for classification/regression problems ( eg , a mixture of experts [ 11] ) , and OMHMC might be usable for help in their model selections .
6 . NOTATION
We summarize here the notations used in this paper . c : component number n : data number d : dimension number r : the order of the indexed moment t : update number ∗ : true model N : data size D : data dimensionality X and xn : random variable in RD and its observation zn : latent variable of xn p : pdf of whole model pc : pdf of component pcd : pdf of marginal qc : pdf of copula M : possible candidates of marginal distribionts Q : possible candidates of copulas C : # of component Mcd : type of marginal distribution Qc : type of copula α : mixture ratio φcd : parameter of Mcd ψcd : cdf . corresponding to pcd(• ; φcd , Mcd ) ξc : parameter of Qc Sc : ( Mc1 , . . . , McD , Qc ) θ : ( α , φ11 , . . . , φCD , ξ1 , . . . , ξC ) ) θc : ( αc , φc , ξC ) and φc = ( φc1 , . . . , φcD ) Urc : the r th order momnent of latent variables Vrcd : the r th order moment of observed variables Vcd : ( V1cd , . . . , VRxcd ) Wc : moment for copula T : the number of update K0 : initial data size K : mini batch data size β(• ) : dimensionality of • EZ[• ] : expectation of • wrt latent variables’ posterior [ •]ij : the ( i , j) th element of •
652 Table 2 : AUC values ( N/A represents “ not available ” because of computational time/memory issues . ) Dataset
VDPGMM
OMHMC
LOF i forest
OSVM
186/77 150/120 500/268 dim # of normal / total anomalies D 9 13 8 20 20 9 3
3697/3703 3736/3664 45586/3511 972781/3377 bcancer heart diabetis twonorm ringnorm shuttle
KDDCUP
0.61 ± 0.09 0.78 ± 0.07 0.73 ± 0.03 0.87 ± 0.01 1.00 ± 0.00 0.99 ±0.00 0.89 ± 0.02
0.70 ± 0.10 0.73 ± 0.05 0.68 ± 0.04 0.73 ± 0.01 1.00 ± 0.00 0.99 ± 0.00 0.87 ± 0.04
0.70 ± 0.07 0.83 ± 0.05 0.72 ± 0.04 0.84 ± 0.01 1.00 ± 0.00 1.00 ± 0.01 0.85 ± 0.00
0.61 ± 0.05 0.56 ± 0.03 0.67 ± 0.05 0.57 ± 0.01 0.80 ± 0.00 0.78 ± 0.01
N/A k = 5
0.53 ± 0.08 0.47 ± 0.10 0.60 ± 0.05 0.56 ± 0.02 1.00 ± 0.00 k = 30
0.67 ± 0.01 0.76 ± 0.06 0.67 ± 0.05 0.62 ± 0.02 1.00 ± 0.00
N/A N/A
N/A N/A
7 . REFERENCES [ 1 ] H . Akaike . Information theory and an extension of the maximum likelihood principle . In B . N . Petrov and F . Caski , editors , Proceedings of the 2nd International Symposium on Information Theory , pages 267–281 , 1973 .
[ 2 ] M . M . Breunig , H . P . Kriegel , R . T . Ng , and
J . Sander . LOF : identifying density based local outliers . Sigmod Record , 29(2):93–104 , 2000 .
[ 3 ] C C Chang and C J Lin . LIBSVM : a library for support vector machines . http://wwwcsientuedutw/~cjlin/libsvm , 2001 .
[ 4 ] A . Corduneanu and C . Bishop . Variational bayesian model selection for mixture distributions . In Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics , 2001 .
[ 5 ] A . P . Dempster , N . M . Laird , and D . B . Rubin .
Maximum likelihood from imcomplete data via the em algorithm . Journal of the Royal Statistical Society , B39(1):1–38 , 1977 .
International Conference on Data Mining , pages 413–422 , 2008 .
[ 15 ] S . Morinaga and K . Yamanishi . Tracking dynamics of topic trends using a finite mixture model . In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining , pages 811–816 . ACM Press , 2004 .
[ 16 ] R . Neal and G . E . Hinton . A view of the EM algorithm that justifies incremental , sparse , and other variants . In Learning in Graphical Models , pages 355–368 , 1998 .
[ 17 ] R . B . Nelsen , editor . An Introduction to Copulas .
Springer Series in Statistics , 2006 .
[ 18 ] C . E . Rasmussen . The infinite gaussian mixture model . In In Advances in Neural Information Processing Systems 12 , pages 554–560 , 2000 .
[ 19 ] G . R¨atsch , T . Onoda , and K . M¨uller . Soft margins for
AdaBoost . Machine Learning , 42(3):287–320 , 2001 .
[ 20 ] D . A . Reynolds , T . F . Quatieri , and R . B . Dunn .
Speaker verification using adapted gaussian mixture models . Digital Signal Processing , 10(1):19–41 , 2000 .
[ 6 ] P . Diehr , D . Yanez , A . S . Ash , M . Hornbrook , , and
[ 21 ] J . Rissanen . Modeling by shortest data description .
D . Y . Lin . Methods for analyzing health care utilization and costs . Annual Review of Public Health , 20:125–144 , 1999 .
[ 7 ] G . Elidan . Copula bayesian networks . In Advances in
Neural Information Processing Systems 23 , pages 559–567 , 2010 .
[ 8 ] A . Frank and A . Asuncion . UCI machine learning repository . http://archiveicsuciedu/ml , 2010 . University of California , Irvine , School of Information and Computer Sciences .
[ 9 ] R . Fujimaki , S . Morinaga , M . Momma , K . Aoki , and T . Nakata . Linear time model selection for mixture of heterogeneous components . In Proceedings of the 1st Asian Conference on Machine Learning , pages 82–97 , 2009 .
[ 10 ] H . Joe , editor . Multivariate Models and Dependence
Concepts . Chapman & Hall , 1997 .
[ 11 ] M . I . Jordan and R . A . Jacobs . Hierarchical mixtures of experts and the EM algorithm . Neural Computation , 6:181–214 , 1994 .
[ 12 ] K . Kurihara , M . Welling , and N . Vlassis . Accelerated variational dirichlet mixture models . In Advances in Neural Information Processing Systems 19 , pages 761–768 . MIT Press , 2006 .
[ 13 ] D . X . Li . On default correlation : A copula function approach . Journal of Fixed Income , 9(4):43–54 , 1999 .
[ 14 ] F . T . Liu , K . M . Ting , and Z . H . Zhou . Isolation
Forest . In Proceedings of the 2008 Eighth IEEE
Automatica , 14:465–471 , 1978 .
[ 22 ] M . Sato . On line model selection based on the variational bayes . Neural Computation , 13:1649–1681 , 2001 .
[ 23 ] B . Scholkopf , J . C . Platt , J . Shawe Taylor , A . J .
Smola , and R . C . Williamson . Estimating the support of a high dimensional distribution . Neural computation , 13(7):1443–1471 , 2001 .
[ 24 ] A . Sklar . Fonctions de repartition a n dimensions et leurs marges . Technical report , Publications de l’Institut de Statistique de L’Universite de Paris , 1959 .
[ 25 ] C . Stauffer and W . E . L . Grimson . Adaptive background mixture models for real time tracking . In IEEE Computer Society Conference on Computer Vision and Pattern Recognition , pages 246–252 , 1999 .
[ 26 ] A . Wilson and Z . Ghahramani . Copula processes . In Advances in Neural Information Processing Systems 23 , pages 2460–2468 , 2010 .
[ 27 ] K . Yamanishi , J . ichi Takeuchi , G . Williams , and
P . Milne . On line unsupervised outlier detection using finite mixtures with discounting learning algorithms . Data Mining and Knowledge Discovery , 8(3):275–300 , 2004 .
[ 28 ] K . Yamanishi and Y . Maruyama . Dynamic model selection with its applications to novelty detection . IEEE Transactions on Information Theory , 53:2180–2189 , 2007 .
653
