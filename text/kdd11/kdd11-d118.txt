Prominent Streak Discovery in Sequence Data
∗ Xiao Jiang
Chengkai Li
† ‡
Shanghai Jiao Tong University showbufire@gmail.com
University of Texas at Arlington cli@uta.edu
Ping Luo
HP Labs China pingluo@hpcom
Min Wang
HP Labs China minwang6@hpcom
Yong Yu
Shanghai Jiao Tong University yyu@cssjtueducn
ABSTRACT
1 .
INTRODUCTION
This paper studies the problem of prominent streak discovery in sequence data . Given a sequence of values , a prominent streak is a long consecutive subsequence consisting of only large ( small ) values . For finding prominent streaks , we make the observation that prominent streaks are skyline points in two dimensions– streak interval length and minimum value in the interval . Our solution thus hinges upon the idea to separate the two steps in prominent streak discovery– candidate streak generation and skyline operation over candidate streaks . For candidate generation , we propose the concept of local prominent streak ( LPS ) . We prove that prominent streaks are a subset of LPSs and the number of LPSs is less than the length of a data sequence , in comparison with the quadratic number of candidates produced by a brute force baseline method . We develop efficient algorithms based on the concept of LPS . The non linear LPS based method ( NLPS ) considers a superset of LPSs as candidates , and the linear LPS based method ( LLPS ) further guarantees to consider only LPSs . The results of experiments using multiple real datasets verified the effectiveness of the proposed methods and showed orders of magnitude performance improvement against the baseline method .
Categories and Subject Descriptors
H28 [ Database Management ] : Database Applications—data mining General Terms
Algorithms , Performance Keywords sequence database , time series database , skyline query ∗Work performed while the author was visiting HP Labs China . †Work partially performed while the author was visiting HP Labs China . ‡This material is based upon work partially supported by NSF Grant IIS 1018865 . Any opinions , findings , and conclusions or recommendations expressed in this publication are those of the author(s ) and do not necessarily reflect the views of NSF .
This paper defines the problem of prominent streak discovery in sequence data and presents efficient algorithms for finding prominent streaks . A piece of sequence data is a series of values or events . This includes time series data , in which the data values or events are often measured at equal time intervals . Sequence and time series data is produced and accumulated in a rich variety of applications . Examples include stock quotes , sports statistics , temperature measurement , Web usage logs , network traffic logs , Web clickstream , and customer transaction sequence .
Given a sequence of values , a prominent streak is a long consecutive subsequence consisting of only large ( small ) values . Examples of such prominent streaks include consecutive days of high temperature , consecutive trading days of large stock price oscillation , consecutive games of outstanding performance in professional sports , consecutive hours of high volume of TCP traffic , consecutive weeks of high flu activity , and so on . It is insightful to investigate prominent streaks since they intuitively and succinctly capture extraordinary subsequences of data .
Prominent streak discovery can be particularly useful in helping journalists to identify newsworthy stories when data sequences evolve , investigators to find suspicious phenomena , and news anchors and sports commentators to bring out attention seizing factual statements . Therefore it will be a key enabling technique for computational journalism [ 7 ] . In fact , we witness the mentioning of prominent steaks in many real world news articles :
∙ “ This month the Chinese capital has experienced 10 days with a maximum temperature in around 35 degrees Celsius – the most for the month of July in a decade . ” ( http://wwwchinadailycomcn/china/2010 07/27/ content_11055675.htm )
∙ “ The Nikkei 225 closed below 10000 for the 12th consecutive week , the longest such streak since June 2009 . ” ( http://wwwbloombergcom/news/ 2010 08 06/japanese stocks fall for second day this week on u s joblessclaims yen.html )
∙ “ He ( LeBron James ) scored 35 or more points in nine consecutive games and joined Michael Jordan and Kobe Bryant as the only players since 1970 to accomplish the feat . ” ( http://wwwnbacom/cavaliers/news/lbj_ mvp_candidate_060419.html )
∙ “ Only player in NBA history to average at least 20 points , 10 rebounds ( Kevin Garnett ) ” and 5 assists per game for 6 consecutive seasons . ( http://enwikipediaorg/wiki/Kevin_Garnett )
The examples indicate that general prominent streaks can have a variety of constraints . A streak can be on multiple dimensions ( eg , ⟨point , rebound , assist⟩ ) , its significance can be with regard to a certain period ( eg , “ since June 2009 ” ) or a certain comparison group ( eg , “ the month of July ” ) , and we may be interested in not only the most prominent streaks but also the top k most prominent ones ( eg , “ X joined Y and Z as the only players ” ) . literature on this topic , eg , [ 5 , 21 , 11 , 14 ] . The effectiveness of pruning in the first step is critical to overall performance , because execution time of skyline algorithms increases superlinearly by the number of candidate points [ 5 ] .
2
Candidate streak generation : We considered three methods with increasing pruning power in candidate generation– a baseline method , a non linear LPS ( local prominent streak) based method , and a linear LPS based method . The baseline method exhaustively enumerates , all possible streaks in a sequence , by a nested loop over the values in . The sketch of this method is in Algorithm 1 . It produces quadratic ( ( +1 ) ) candidate streaks . We then propose the concept of local prominent streak ( LPS ) for substantially reducing the number of candidate streaks ( Section 3 ) . The intuition is , given a prominent streak , there cannot be a super sequence of with greater or equal minimal value . In other words , must be locally prominent as well . Hence we only need to consider LPSs as candidates . The algorithm sequentially scans the data sequence and maintains possible LPSs . We further make the observation that an LPS cannot have a preceding or succeeding data entry that is greater than or equal to its minimal value . Therefore we find the left end ( right end ) of an LPS when we find a data entry that is greater than its preceding ( succeeding ) entry . The non linear LPSbased method finds a superset of LPSs as candidates , while the linear LPS based method guarantees to find only LPSs .
Note that to couple candidate streak generation with skyline operation , Algorithm 1 maintains a dynamic skyline and updates it whenever a new candidate streak is produced . The updating procedure _ is in Algorithm 2 and is introduced below .
Algorithm 1 : Baseline Method
Input : Data sequence =( 1 , , ) Output : Prominent streaks
← for = 1 to do
_ ← ∞ for = downto 1 do
_ ← min( , _ ) ← ⟨[ , ] , _ ⟩ // candidate streak ← _ ( , )
Algorithm 2 : Update Dynamic Skyline ( _ )
Input : Dynamic skyline , new candidate streak = ⟨[ , ] , ⟩ Output : Updated dynamic skyline
Find the largest in st ≤ if ≺ or ≺ +1 then return while ≻ and > 0 do Delete from ← − 1
Insert into return
1 2 3 4 5 6 7
1 2 3 4 5 6
7
8
Skyline operation : Our focus is not to compare various skyline algorithms . Many existing algorithms can be leveraged . What matters is the number of candidate streaks produced by the candidate generation step . This is also verified by our experiments which show that , under various skyline algorithms , the candidate streak generation methods in Section 3 perform and compare consistently . We can use a sorting based method for finding the skyline points in a two dimensional space [ 5 ] . If the candidate streak generation step does not prune streaks effectively , we cannot hold all candidate streaks in memory . The memory overflow can be addressed by external memory sorting .
Another approach is to progressively update a dynamic skyline
Figure 1 : A Data Sequence and its Prominent Streaks .
Given its real world usefulness and variety , the research on prominent streaks in sequence data opens a spectrum of challenging problems . In this initial effort , we undertake the problem of discovering the simplest kind of prominent streaks , ie , those without the above constraints . We formally define the concept of prominent streak and our problem as follows .
1.1 Problem Definition Definition 1 ( Streak and Prominent Streak ) : Given an element sequence =( 1 , ⋅ ⋅ ⋅ , ) , a streak is an interval value pair ⟨[ , ] , ⟩ , where 1≤ ≤ ≤ and =min ≤ ≤ .
Consider two streaks 1=⟨[ 1 , 1 ] , 1⟩ and 2=⟨[ 2 , 2 ] , 2⟩ . We say 1 dominates 2 , denoted by 1≻ 2 or 2≺ 1 , if 1− 1≥ 2− 2 and 1> 2 , or 1− 1> 2− 2 and 1≥ 2 .
With regard to =( 1 , ) , the set of all possible streaks is denoted by . A streak ∈ is a prominent streak if it is not dominated by any streak in , ie , ∄ ′ st ′∈ and ′≻ . The set of all prominent streaks in is denoted by .
Problem Statement : The prominent streak discovery problem is to , given a sequence , produce .
Figure 1 shows our running example of a 10 value sequence =(3 , 1 , 7 , 7 , 2 , 5 , 4 , 6 , 7 , 3 ) . There are 5 prominent streaks in – ⟨[1 , 10 ] , 1⟩ , ⟨[3 , 10 ] , 2⟩ , ⟨[6 , 10 ] , 3⟩ , ⟨[6 , 9 ] , 4⟩ , ⟨[3 , 4 ] , 7⟩ . For instance , ⟨[6 , 9 ] , 4⟩ is a prominent streak of minimal value 4 , whose interval is from 6 to 9 . ⟨[1 , 10 ] , 1⟩ , the whole data sequence , is also a trivial prominent streak because no other streak can possibly dominate the sequence itself . The streak ⟨[8 , 9 ] , 6⟩ is an instance of non prominent streaks because it is dominated by ⟨[3 , 4 ] , 7⟩ .
1.2 Overview of the Solution
A brute force method for discovering prominent streaks is not appealing . One can enumerate all possible streaks and decide if each streak is prominent by comparing it with every other streak . Given a sequence with length , there are ∣ ∣= +1 2 streaks in total . Thus the number of pair wise streak comparison would be ∣ ∣ . Given a sequence of length 10000 , the brute force approach enumerates 108 streaks and performs 1016 comparisons . Many real world sequences can be quite long . The sequence of daily closing prices for a stock with 40 year history has about 10000 values . A one year usage log for a Web site has 8760 values at hourly interval .
2 = 4+2 3− 2−2
8
Prominent streaks are in fact skyline points [ 5 ] in two dimensions– streak interval length ( − ) and minimum value in the interval ( ) . A streak is a prominent streak ( skyline point ) if it is not dominated by any point , ie , there exists no streak that has both longer interval and greater minimum value .
Based on this observation , our solution hinges upon the idea to separate the two steps of prominent streak discovery– candidate streak generation and skyline operation over candidate streaks . In candidate generation , we prune a large portion of non prominent streaks without exhaustively considering all possible streaks . For skyline operation , we leverage efficient algorithms from the rich with candidate streaks , based on the nested loop method in [ 5 ] . The outline of this approach is shown in Algorithm 2 . We use to denote the dynamic skyline . When a new candidate streak is generated , is inserted into if it is not dominated by any point in . The algorithm also checks if some points in are dominated by and eliminates them from .
The dominance relationship can be efficiently checked , given that the streaks have only two dimensions– interval length ( − ) and minimum value ( ) . The key idea is that the lengths of streaks monotonically decrease as their minimal values increase ( except that there can be identical points , ie , streaks with equal lengths and equal minimal values . ) Hence the streaks in are ordered by ( or by − ) . Suppose the candidate streak is = ⟨[ , ] , ⟩ . We only have to find in a pivoting streak = ⟨[ , ] , ⟩ such that is the largest index with ≤ , ie , ≤ < +1 , then check its neighbors to determine the dominance relationship . For quickly finding , we use a balanced binary search tree ( BST ) on to store . ( Thus we call it BST based skyline method . ) We can prove that is dominated by some points in if and only if is dominated by or +1 . Furthermore , if dominates totally streaks in , then the streaks are , −1 , . . . , − +1 . ( We omit the discussion of boundary cases , ie , =0 or =∣ ∣ . )
In comparison with the sorting based method , the above BSTbased skyline method saves both memory space and execution time . It avoids memory overflow because the number of streaks in the dynamic skyline in most cases remains small enough to fit in memory . Hence no streak needs to be read from/written to secondary memory . The small size of dynamic skyline in real data is verified by our experiments in Section 5 . After all , prominent streaks ( and skyline points in general ) are supposed to be minority , otherwise they cannot stand out to warrant further investigation . Furthermore , even if the dynamic skyline grows large , a method such as the block nested loop based method in [ 5 ] can be applied to fall back on secondary memory . The small size of dynamic skyline also means small number of streak comparisons . Intuitively , given candidate streaks , a fast comparison based sorting algorithm ( say quicksort ) requires ( log ) comparisons , while the BST based method only requires ( log ) comparisons , where is the maximal size of the dynamic skyline during computation . Experiments in Section 5 show that is typically much smaller than .
1.3 Summary of Contributions and Outline To summarize , this paper makes the following contributions :
∙ We defined the problem of prominent streak discovery . The simple concept is useful in many real world applications . To the best of our knowledge , there has not been study along this line .
∙ We proposed the solution framework to separate candidate streak generation and skyline operation during prominent streak discovery . Under this framework , we designed efficient algorithms for candidate streak generation , based on the concept of local prominent streak . Both the non linear LPS based method ( NLPS ) and the linear LPS based method ( LLPS ) produce substantially less candidate streaks than the quadratic number of candidates produced by a baseline method . LLPS further guarantees a linear number of candidate streaks .
∙ We conducted experiments over multiple real datasets . The results verified the effectiveness of our methods and showed orders of magnitude performance improvement over the baseline method . We also showed some insightful prominent streaks discovered from real data , to highlight the practicality of this work . The rest of the paper is organized as follows . In Section 2 we review related work . Section 3 presents the NLPS and LLPS methods for candidate streak generation . Section 4 discusses how to adapt the algorithms to monitor prominent streaks when data sequence continuously grows . Experiment setup and results are reported in Section 5 . Section 6 concludes the paper .
2 . RELATED WORK
Data mining on sequence and time series data has been an active area of research , where many techniques are developed for similarity search and subsequence matching in sequence and timeseries databases [ 1 , 8 , 2 , 26 ] , finding sequential patterns [ 3 , 20 , 27 , 15 , 25 ] , classification and clustering of sequence and time series data [ 19 , 13 , 12 , 18 ] , biological sequence analysis [ 4 , 17 ] , etc . However , we are not aware of prior work on the prominent streak discovery problem proposed in this paper .
The skyline of a set of tuples is the subset of tuples that are not dominated by any tuple . A tuple dominates another tuple if it is equally good or better on every attribute and better on at least one attribute . Skyline query has been intensively studied over the last decade . Kung et al . [ 9 ] first proposed in memory algorithms to tackle the skyline problem . Börzsönyi et al . [ 5 ] considered the problem in database context and integrated skyline operator into database system . They also invented a block nested loop algorithm ( BNL ) and extended the divide and conquer algorithm ( DC ) from [ 9 ] . An improvement of BNL is discussed in [ 6 ] .
Progressive skyline algorithms optimize the efficiency in returning initial skyline points while producing more results progressively . Various algorithms developed along this line include the bitmap based algorithm and the index based algorithm [ 21 ] , the nearest neighbor search algorithm [ 11 ] , and the branch and bound skyline algorithm ( BBS ) [ 14 ] . Other variants of skyline queries have also been studied , including skyline cube which aims to answer skyline queries over any combination of dimensions [ 16 , 24 ] . Jiang et al . [ 10 ] studied the problem of interval skyline queries on time series . Given a set of time series and a time interval , they find the time series that are not dominated by others in the interval . A time series dominates another one if its value at every position is at least equal to the corresponding value in the other time series and it is at least larger at one position . The point by point equi length interval comparison is clearly different from our problem .
The plateau of a time series is the time interval in which the vlaues are close to each other ( within a given threshold ) and are no smaller than the values outside the interval [ 22 ] . The plateau problem is not concerned about comparing different intervals .
Our techniques can be useful in disease outbreak detection , by identifying prominent streaks in time series of aggregated disease case counts . Previous works on outbreak detection focus on conventional data mining tasks such as clustering and regression [ 23 ] . The concept of prominent streaks has not been studied before .
3 . DISCOVERING PROMINENT STREAKS FROM LOCAL PROMINENT STREAKS For an element sequence , the baseline method ( Algorithm 1 ) produces ( +1 ) candidate streaks . In this section , based on the concept of local prominent streak ( LPS ) we propose the non linear LPS based ( NLPS ) and linear LPS based ( LLPS ) methods . Both dramatically reduce the number of candidate streaks in practice . LLPS further guarantees only a linear number of candidate streaks .
2
3.1 Local Prominent Streak ( LPS )
Definition 2 ( Local Prominent Streak ) : Given a sequence of data values = ( 1 , ⋅ ⋅ ⋅ , ) , we say a streak = ⟨[ , ] , ⟩ ∈ is a local prominent streak ( LPS ) or locally prominent if there does not we only consider local prominent streaks . Property 2 further shows how small ℒ is and thus how good it is as a candidate set . Specifically , the size of ℒ is at most ∣ ∣ , the length of the sequence , in contrast to the all ∣ ∣(∣ ∣+1 ) possible streaks considered by the baseline method ( Algorithm 1 ) . Thus , ℒ helps to prune most streaks from further consideration . In the following sections we present efficient algorithms for computing a superset of ℒ and ℒ itself exactly .
2
Figure 2 : Local Prominent Streaks .
3.2 ℒ and ℒ exist any other streak ′ = ⟨[ ′ , ′ ] , ′⟩ ∈ such that [ ′ , ′ ] ⊃ [ , ] and ′ ≻ . ( Ie , there does not exist such ′ that [ ′ , ′ ] ⊃ [ , ] and ′ ≥ . ) The symbol ⊃ denotes the subsumption check between two intervals , ie , [ ′ , ′ ] ⊃ [ , ] if and only if ′ ≤ ∧ ′ > or ′ < ∧ ′ ≥ . We denote the set of local prominent streaks in sequence as ℒ .
Figure 2 shows all the local prominent streaks found in our running example . All other streaks are not locally prominent . For example , ⟨[6 , 8 ] , 4⟩ is not locally prominent since it is dominated by ⟨[6 , 9 ] , 4⟩ and [ 6 , 9 ] ⊃ [ 6 , 8 ] . In the following we give several important properties of local prominent streaks .
Property 1 : Every prominent streak is also a local prominent streak , ie , ⊆ ℒ .
Proof : Suppose there is a prominent streak that is not locally prominent , ie , ∃ ∈ st /∈ ℒ . By Definition 2 , there exists some streak ′ such that [ ′ , ′ ] ⊃ [ , ] and ′ ≻ . That is contradictory to Definition 1 which says cannot be dominated by any other streak . Therefore a streak cannot be prominent if it is not even locally prominent .
The above Property 1 is illustrated by Figure 2 , as all the prominent streaks in Figure 1 also appear in Figure 2 . However , the reverse of Property 1 does not hold– local prominent streaks are not necessarily prominent streaks . For example , ⟨[8 , 9 ] , 6⟩ is an LPS but is dominated by ⟨[3 , 4 ] , 7⟩ and therefore is not in Figure 1 . Lemma 1 : Suppose = ⟨[ , ] , ⟩ and ′ = ⟨[ ′ , ′ ] , ′⟩ are two different local prominent streaks in , ie , , ′ ∈ ℒ , ∕= ′ or ∕= ′ . For any ∈ ∈[ , ] and ′ ∈ ∈[ ′ , ′ ] , we have ∕= ′ . Ie , ∈[ , ] ∩ ∈[ ′ , ′ ] = . Proof : If [ , ]∩[ ′ , ′ ] = , ie , the two intervals do not overlap , it is obvious that ∕= ′ . Now consider the case when [ , ]∩[ ′ , ′ ] ∕= , ie , ≤ ′ ≤ or ′ ≤ ≤ ′ . By definition of , = = ∈[ , ] and ′ = ′ = ∈[ ′ , ′ ] . Suppose there exist such and ′ that = ′ . Thus = ′= . By Definition 1 , we have ≥ for every ∈ [ , ] and every ∈ [ ′ , ′ ] . Since the two intervals [ , ] and [ ′ , ′ ] overlap , their combined interval corresponds to a new streak ′′=⟨[ , ] ∪ [ ′ , ′ ] , ⟩ . 1 It is clear ′′≻ and ′′≻ ′ . That is a contradiction to the precondition that both and ′ are LPSs . Thus , this lemma holds .
Lemma 1 indicates that two different LPSs cannot reach their minimal values at the same position . Therefore each value position in sequence can correspond to the minimal value of at most one LPS . What immediately follows is that there are at most LPSs in an element sequence . Formally we have the following property .
Property 2 : ∣ℒ ∣ ≤ ∣ ∣ .
From Property 1 we know that ℒ is a sufficient candidate set for , ie , we can guarantee to find all prominent streaks if
1The two intervals can overlap in four different ways . Thus [ , ] ∪ [ ′ , ′ ] = [ , ] or [ , ′ ] or [ ′ , ] or [ ′ , ′ ] .
( a ) ℒ 9 9
( b ) ℒ 10 10
( c ) plot of ℒ 9 9
Figure 3 : From ℒ 9
( d ) plot of ℒ 10 10 9 to ℒ 10
10 .
To facilitate our discussion , we first define a new notation , ℒ . is the set of local prominent streaks in that
Definition 3 : ℒ end at position , ie , ℒ
={ ∣ ∈ ℒ and =⟨[ , ] , ⟩} .
, ℒ 2
, . . . , ℒ ∣ ∣
There are two key components in the definition of ℒ
. The first is the upper script , which fixes the right end of every interval in the set . It is clear that ℒ 1 is a natural partition of ℒ . We use this partition scheme in the design of our algorithms . Specifically , we show how each ℒ in this partition is calculated in a sequential and progressive way . The second key component in the definition of ℒ is the lower script , which provides the scope for local prominent streaks . By generalizing this component we define ℒ . We denote the sequence with the first entries of as . Then ℒ is the set of local prominent streaks with regard to sequence ( instead of ) and ℒ are those LPSs in ℒ that end at . Due to the change of scope , ℒ . Formally , we have the following property . Property 3 : ℒ Proof : Consider any streak ∈ ℒ . By Definition 3 , = ⟨[ , ] , ⟩ and ∈ ℒ . Therefore by Definition 2 , there does not exist any ′ = ⟨[ ′ , ′ ] , ′⟩ in such that ′ ≻ and [ ′ , ′ ] ⊃ [ , ] . Since is a prefix of , ie , the first values in , it follows that there does not exist any such ′ in either . Therefore ∈ ℒ is a superset of ℒ
⊆ ℒ
.
.
Consider the running example again . Figure 3(a ) shows ℒ 9
9 , including ⟨[1 , 9 ] , 1⟩ , ⟨[3 , 9 ] , 2⟩ , ⟨[6 , 9 ] , 4⟩ , ⟨[8 , 9 ] , 6⟩ , ⟨[9 , 9 ] , 7⟩ . As shown in Figure 2 , ℒ 9 contains ⟨[6 , 9 ] , 4⟩ , ⟨[8 , 9 ] , 6⟩ , ⟨[9 , 9 ] , 7⟩ . Streaks ⟨[1 , 9 ] , 1⟩ and ⟨[3 , 9 ] , 2⟩ do not belong to ℒ , thus do not belong to ℒ 9 , since they are locally dominated by ⟨[1 , 10 ] , 1⟩ and ⟨[3 , 10 ] , 2⟩ , respectively . By contrast , ⟨[1 , 9 ] , 1⟩ and ⟨[3 , 9 ] , 2⟩ are part of ℒ 9 9 because they are not locally dominated by any streak of 9 , which only contains the first 9 values of . of 3 is 4=⟨[ 3 , ] , 4⟩ , where 4=min{ 3 , }≥min{ 2 , }= 1 . Therefore 4 ≻ 1 , which contradicts with the pre condition that 1∈ℒ
. The property holds .
3.3 Non linear LPS Method By Property 3 and the fact that ℒ 1
, ⋅ ⋅ ⋅ , ℒ ∣ ∣ is a parti tion of ℒ , we have
ℒ = Þ
ℒ
⊆ Þ
ℒ
( 1 )
1≤ ≤∣ ∣
1≤ ≤∣ ∣
Thus , we can use Þ1≤ ≤∣ ∣ ℒ as our candidate set for prominent streaks . Although its size can be greater than that of ℒ , in practice it does substantially reduce the size of candidate streaks , verified by the experimental results in Section 5 .
Algorithm 3 : Non linear LPS Method ( NLPS )
Input : Data sequence = ( 1 , ⋅ ⋅ ⋅ , ) Output : Prominent streaks
← for = 1 to do
Compute ℒ for each streak in ℒ by Algorithm 4 do
← _ ( , )
Algorithm 4 : Progressive Computation of ℒ and
Input : ℒ −1 −1 Output : ℒ
// When it starts , stack consists of streaks in ℒ −1 −1 ← while ! . ( ) do
. if . ( ) . < then break else
← . ( ) if == then
. ℎ(⟨[ , ] , ⟩ ) else
. ← . ℎ( )
// Now , contains all the streaks in ℒ
.
1 2
3
4 5
1 2 3 4 5 6
7 8 9 10 11
Along this line , Algorithm 3 presents the method to compute candidate streaks . Since the number of candidates may be superlinear to the length of data sequence , we call it the non linear LPS method ( NLPS ) . The algorithm iterates from 1 to ∣ ∣ , progressively computes ℒ when the th element is visited , and includes them into candidate streaks . The details of updating from ℒ −1 are in Algorithm 4 , which −1 is based on the following Lemma 2 . For convenience of discussion , we first define the right end extension of a streak and a streak set . from ℒ −1 to ℒ
−1
Definition 4 : If = ⟨[ , ] , ⟩ is a streak in an element data sequence and < , the right end extension of is streak ⟨[ , + 1 ] , ′⟩ , where ′ = min{ , +1} . The extension of a streak set is the set which consists of extensions of all the streaks in . Lemma 2 : If 1=⟨[ , ] , 1⟩ ∈ ℒ 2=⟨[ , − 1 ] , 2⟩ ∈ ℒ −1 −1 and ∕= , then the streak
.
Proof : First , note that 2=min 1≤ ≤ −1 and 1=min{ 2 , } . We prove by contradiction . Suppose 2=⟨[ 1 , −1 ] , 2⟩ /∈ℒ −1 −1 By Definition 3 , 2 /∈ℒ −1 . Further by Definition 2 , there exists 3=⟨[ 3 , 3 ] , 3⟩∈ −1 such that [ 3 , 3 ] ⊃ [ 1 , − 1 ] and 3 ≻ 2 . Given any =⟨[ , ] , ⟩∈ −1 , we have ≤ − 1 . Therefore 3= − 1 , 3 < 1 and 3 ≥ 2 . The right end extension
.
, its prefix streak is in ℒ −1
Lemma 2 indicates that , except ⟨[ , ] , ⟩ , for each streak in . Hence , to produce ℒ ℒ we only need to consider the right end extension of ℒ −1 . Be−1 yond that , we only need to consider one extra streak ⟨[ , ] , ⟩ since it may belong to ℒ as well .
−1
,
In order to articulate how to derive ℒ from ℒ −1
−1
, we partition ℒ −1 into two disjoint sets , namely , −1 ℒ −1 = { ∣ = ⟨[ , − 1 ] , ⟩ ∈ ℒ −1 −1
<
−1 , < } , ( 2 )
ℒ −1 −1
≥
= { ∣ = ⟨[ , − 1 ] , ⟩ ∈ ℒ −1
−1 , ≥ } . ( 3 )
< is the disjoint union of these two sets ,
= ℒ −1 −1
It is clear that ℒ −1 −1 ie , ℒ −1 −1 ≥ ℒ −1 −1 Figure 3(a ) , since 10 = 3 , the two sets are ℒ 9 9 ⟨[3 , 9 ] , 2⟩} , ℒ 9 9
, and ℒ −1 −1 = . Use the running example again . For ℒ 9
= {⟨[6 , 9 ] , 4⟩ , ⟨[8 , 9 ] , 6⟩ , ⟨[9 , 9 ] , 7⟩} .
∪ ℒ −1 −1
9 in < = {⟨[1 , 9 ] , 1⟩ ,
<
∩
≥
≥
≥
,
<
<
We consider how to extend streaks in ℒ −1 −1 and ℒ −1 −1 respectively . For simplicity of presentation , we omit the formal proofs when we make various statements below . ∙ ℒ −1 −1 ℒ −1 has a minimal −1 value less than , the corresponding extended new streak has the same minimal value . Hence all the new streaks belong to < ℒ , we have 1 = {⟨[1 , 10 ] , 1⟩ , ⟨[3 , 10 ] , 2⟩} .
: We use 1 to denote the right end extension of . Since every streak in ℒ −1 −1
. For the running example , corresponding to ℒ 9
9
<
<
≥
≥
≥
: We use 2 to denote the right end extension of . Since every streak in ℒ −1 −1
∙ ℒ −1 −1 ℒ −1 has a minimal −1 value greater than or equal to , the minimal value of every streak in 2 equals . Hence , the longest streak in 2 , denoted as 2∗ , dominates all other streaks in 2 and it is the only streak in 2 that belongs to ℒ . Furthermore , since every streak in 2 has the same value ( the right end of the interval ) , ie , , 2∗ is the streak with the minimal value ( the left end of the interval ) in 2 . Clearly there cannot be another streak in 2 with the same length . For the running example , corresponding to ℒ 9 , we have 2 = {⟨[6 , 10 ] , 3⟩ , ⟨[8 , 10 ] , 3⟩ , ⟨[9 , 10 ] , 3⟩} . 9 The longest streak in 2 is ⟨[6 , 10 ] , 3⟩ .
≥
∙ ℒ −1 −1
≥
= : If ℒ −1 −1
≥ is empty , a new streak ⟨[ , ] , ⟩ belongs to ℒ
. ( Otherwise , it is dominated by 2∗ . )
The above discussion is captured by the following Property 4 .
Property 4 : ℒ 1 ∪ {⟨[ , ] , ⟩} if 2 = .
= 1 ∪ { 2∗} if 2 ∕= and ℒ
=
−1
. Figure 3(a ) and 3(b ) show ℒ 9
We use Figure 3 to explain the above procedure of producing from ℒ −1 ℒ 9 and 10 , respectively . Figure 3(c ) and 3(d ) also show ℒ 9 ℒ 10 9 and ℒ 10 10 , by using a different presentation– plot . All the streaks ⟨[ , ] , ⟩ in ℒ −1 share the same value of , which −1 is − 1 . Therefore we plot the streaks by ( axis ) and ( axis ) . In Figure 3(c ) , the 5 points represent the 5 streaks in ℒ 9 9 : ⟨[1 , 9 ] , 1⟩ , ⟨[3 , 9 ] , 2⟩ , ⟨[6 , 9 ] , 4⟩ , ⟨[8 , 9 ] , 6⟩ , ⟨[9 , 9 ] , 7⟩ . The dotted line represents the 10 th data entry 10 = 3 . It bisects ℒ 9 9 into ℒ 9 ( 2 9
( 3 hollow points above the line ) and ℒ 9 9
≥
<
10 by extending the right ends of streaks in ℒ 9 all belong to ℒ 10 filled points below the line ) . We produce new candidate streaks ℒ 10 9 to 10 . The streaks extended from ℒ 9 10 . They 9 are the 2 filled points in Figure 3(d ) , corresponding to ⟨[1 , 10 ] , 1⟩ and ⟨[3 , 10 ] , 2⟩ . Among the streaks extended from ℒ 9 , only 9 the one with the smallest ( the longest one ) belongs to ℒ 10 10 . It is the hollow point in Figure 3(d ) , corresponding to ⟨[6 , 10 ] , 3⟩ . Hence ℒ 10
10 ={⟨[1 , 10 ] , 1⟩ , ⟨[3 , 10 ] , 2⟩ , ⟨[6 , 10 ] , 3⟩} .
≥
<
The details of the algorithm are shown in Algorithm 4 . We use a stack to maintain ℒ . Since the streaks ⟨[ , ] , ⟩ in ℒ have the same value which equals , we do not need to store in . Hence each item in has two data attributes , and . The items in the stack are ordered by ( and , since their and values both strictly monotonically increase).2 In fact , Figure 3(c ) and 3(d ) visualize all items in , before and after 10 is encountered , respectively . In each figure , the leftmost point denotes the bottom of the stack ( with the smallest ) , while the rightmost point denotes the top of the stack ( with the largest ) . After data entries 1 , , −1 are encountered , contains ℒ −1 . Given −1 data entry , we popped from the stack all the streaks whose values are greater than or equal to . Among the popped streaks , the leftmost one ( with the smallest and ) is pushed back into the stack , with value replaced by and extended from − 1 to . ( Again , the value is not explicitly stored in the stack . ) If no streak was popped , then ⟨[ , ] , ⟩ is pushed into the stack . The remaining streaks in the original stack are kept , with their and values unchanged and extended from − 1 to .
Algorithm 3 computes candidate streaks for an element sequence . It invokes Algorithm 4 times.3 In each invocation , exactly 1 item is pushed into the stack . Therefore in total there are insertions and thus at most deletions . Hence , the amortized time complexity of Algorithm 4 is ( 1 ) .
In each iteration of Algorithm 3 , we compute ℒ and include them into candidate streaks . Thus , for an element sequence , the total number of candidate streaks considered is ࢣ ∣ . In the worst case , we may have a strictly increasing sequence and the candidate streaks include all possible streaks . This is as bad as the exhaustive baseline method in Algorithm 1 . For example , given sequence ( 10 , 20 , 30 ) , we have ℒ 1 1 = {⟨[1 , 1 ] , 10⟩} , ℒ 2 2 = {⟨[1 , 2 ] , 10⟩ , ⟨[2 , 2 ] , 20⟩} and ℒ 3 3 = {⟨[1 , 3 ] , 10⟩ , ⟨[2 , 3 ] , 20⟩ , ⟨[3 , 3 ] , 30⟩} .
=1 ∣ℒ
3.4 Linear LPS Method
. Computation of both ℒ
Now we present the linear LPS ( LLPS ) method ( Algorithm 5 ) , which guarantees to produce a linear number of candidate streaks even in the worst case . Similar to Algorithm 3 , this method iterates from ℒ −1 through the data sequence and computes ℒ −1 when the th data entry is encountered , for from 1 to . However , different from Algorithm 3 , it also computes ℒ −1 from ℒ −1 is done −1 in Algorithm 6 , which is a simple extension of Algorithm 4 . It is worth noting that , since = , ℒ are identical . from ℒ −1 given the th entry , −1 Algorithm 6 is based on the following Property 5 . Due to space limitations , we omit the formal proof , but explain its intuition as ≥ follows . Recall that the minimal value of any streak in ℒ −1 −1 ( Equation ( 3 ) ) is not smaller than . It follows that if the minimal
To produce ℒ −1 and ℒ −1 and ℒ
2We omit the proof of monotonicity . 3With regard to the first data element 1 , ⟨[1 , 1 ] , 1⟩ is pushed into the stack . It is the only prominent streak and local prominent streak for 1 .
≥ value of a streak in ℒ −1 is greater than , the streak can−1 not grow into a longer local prominent streak without changing the minimal value . Hence , the streak itself is a local prominent streak . To summarize , ℒ −1 ception is the longest streak in ℒ −1 , ie , the streak with the −1 smallest and thus the smallest minimal value . If its minimal value is equal to , then it does not belong ℒ −1 , because it can be right extended and included in ℒ ′ is the same as ℒ −1 −1 for some ′ ≥ .
. The only ex
≥
≥
Property 5 : Given an entry sequence , for any position 1< ≤ , ℒ −1
= { ∣ = ⟨[ , − 1 ] , ⟩ ∈ ℒ −1 and > } .
≥
−1
Continue the running example . We have ℒ 9 {⟨[6 , 9 ] , 4⟩ , ⟨[8 , 9 ] , 6⟩ , ⟨[9 , 9 ] , 7⟩} . Note that ℒ 9 9 are identical because the minimal values for the streaks in ℒ 9 9 are all greater than 10 .
= and ℒ 9
≥
= ℒ 9
9
≥
≥
Similar to Algorithm 4 , Algorithm 6 has an amortized time complexity of ( 1 ) . However , in LLPS we only need to consider the streaks in ℒ −1 as candidates . Consequently , LLPS reduces the total number of candidate streaks to ࢣ ∣ , ie , ∣ℒ ∣ ( Equation ( 1) ) . By Property 2 , ∣ℒ ∣ is at most , thus LLPS guarantees to produces only a linear number of candidate streaks even in worst case .
=1 ∣ℒ
Algorithm 5 : Linear LPS Method(LLPS )
Input : Data sequence = ( 1 , . . . ) Output : Prominent streaks
← for = 1 to do
Compute ℒ −1 for each streak in ℒ −1 and ℒ do by Algorithm 6
← _ ( , )
← ℒ
ℒ for each streak in ℒ do
← _ ( , )
1 2
3
4 5
6
7 8
Algorithm 6 : Computing ℒ −1 and ℒ
Input : ℒ −1 −1 Output : ℒ −1 and and ℒ
// Insert the following line before Line 1 in Algorithm 4 . ℒ −1
←
// Insert the following two lines after Line 6 in Algorithm 4 , in the same else branch as Line 6 . if . > then
ℒ −1
← ℒ −1
∪{ }
4 . MONITORING PROMINENT STREAKS One desirable property of a prominent streak discovery algorithm is the capability of monitoring new data entries as the sequence grows continuously and always keeping the prominent streaks up to date . For example , a network administrator may check the prominent streaks in the network traffic of a Web server till any particular moment . Formally , given a continuously growing data sequence ( such as a data stream ) , the th data entry that has just come is denoted by and the sequence so far is denoted by . At this moment , if the user requests , the prominent streaks of , our method should efficiently discover them .
The BST based method progressively updates the dynamic skyline with new candidate streaks , thus can be applied for monitoring prominent streaks without modification . name Gold River Melb1 Melb2 Wiki1 Wiki2 Wiki3 SP500 HPQ IBM AOL WC98 length 1074 1400 3650 3650 4896 4896 4896 10136 12109 12109 132480 7603201
# prominent streaks
137 93 55 58 58 51 118 497 232 198 127 286 description Daily morning gold price in US dollars , 01/1985 03/1989 . Mean daily flow of Saugeen River near Port Elgin , 01/1988 12/1991 . The daily minimum temperature of Melbourne , Australia , 1981 1990 . The daily maximum temperature of Melbourne , Australia , 1981 1990 . Hourly traffic to enwikipediaorg/wiki/Main_page , 04/2010 10/2010 . Hourly traffic to enwikipediaorg/wiki/Lady_gaga , 04/2010 10/2010 . Hourly traffic to enwikipediaorg/wiki/Inception_(film ) , 04/2010 10/2010 . S&P 500 index , 06/1960 06/2000 . Closing price of HPQ in NYSE for every trading day , 01/1962 02/2010 . Closing price of IBM in NYSE for every trading day , 01/1962 02/2010 . Number of queries sent to AOL search engine in every minute over three months . Number of requests to World Cup 98 web site in every second , 04/1998 07/1998 .
Table 1 : Data Sequences Used in Experiments .
With regard to candidate streak generation , all three methods ( baseline , NLPS , LLPS ) use one pass sequential scan of the data sequence , therefore they all naturally fit into the monitoring scenario . Specifically , the new data point corresponds to the next iteration of the outer loop in Algorithm 1 , 3 , and 5 . The baseline method exhaustively lists all streaks ending at and updates the skyline with these streaks . The NLPS method updates ℒ −1 −1 to ℒ .
, and updates the skyline with the streaks in ℒ
The adaptation of LLPS is a bit more complex , as shown in Algorithm 7 . This algorithm records the last position when the user requested the prominent streaks . When arrives , ℒ −1 and ℒ are dynamically computed by Algorithms 6 . The skyline is updated with the candidate streaks in ℒ −1 , only if −1 was not requested by the user when −1 was visited . Note that if −1 was requested , the skyline has already been updated with the streaks in ℒ −1 , we do −1 not need to update the skyline with ℒ −1 user requests , the skyline has to be updated with ℒ since all the local prominent streaks ( with regard to ) ending at must be considered . In Section 5 we will show the significant superiority of this adaptation of LLPS over other methods .
⊆ ℒ −1
. Since ℒ −1 again . Finally , if the
−1
Note that this algorithm degrades to NLPS ( Algorithm 3 ) if the user requests the prominent streaks at every data entry . On the other hand , if the prominent streaks are only requested at , ie , the last entry in the sequence , it becomes the same as LLPS ( Algorithm 5 ) .
Algorithm 7 : Continuous Monitoring of Prominent Streaks
Input : The new data entry Compute ℒ −1 if _ _ < − 1 then and ℒ by Algorithms 6 for each streak in ℒ −1 do
← _ ( , ) if is requested then for each streak in ℒ do
← _ ( , )
_ _ ← // Now , contains all prominent streaks in
1
2
3 4
5
6 7 8 9
5 . EXPERIMENTS
The algorithms were implemented in Java . The experiments were conducted on a computer with 2.26GHz Intel Core 2 Duo CPU under Windows 7 . The limit on the heap size of Java Virtual Machine ( JVM ) was set at 512MB .
We used multiple real world datasets , including time series data library4 , Wikipedia traffic statistics dataset5 , NYSE exchange data6 ,
4http://robjhyndman.com/TSDL/ 5http://dammit.lt/wikistats/ 6http://infochimps.com/datasets/daily 1970 current open closehi low and volume nyse exchange up
Baseline
5.77 × 105 9.81 × 105 6.66 × 106 6.66 × 106 1.20 × 107 1.20 × 107 7
1.20 × 10 5.14 × 107 7.33 × 107 7.33 × 107 8.78 × 109 2.89 × 1013
NLPS
6.04 × 104 2.18 × 104 4.47 × 104 4.28 × 104 7.16 × 104 5.77 × 104 4
7.31 × 10 1.69 × 106 5.24 × 105 6.97 × 105 3.53 × 106 1.78 × 108
LLPS
1.05 × 103 1.33 × 103 3.50 × 103 3.49 × 103 4.79 × 103 4.75 × 103 3
4.70 × 10 9.98 × 103 1.08 × 104 1.13 × 104 1.20 × 105 6.69 × 106
Gold River Melb1 Melb2 Wiki1 Wiki2 Wiki3 SP500 HPQ IBM AOL WC98
Table 2 : Number of Candidate Streaks .
AOL search engine log7 , and FIFA World Cup 98 web site access log8 . These datasets cover a variety of application scenarios , including meteorology , hydrology , finance , web log , and network traffic . Table 1 shows the information of 12 data sequences from these data sets that we used in experiments . For each data sequence , we list its name , length , and the number of prominent streaks in the sequence . Each data sequence was stored in a data file .
Examples of Interesting Prominent Streaks Discovered :
From 1985 to 1989 , there had been more than one thousand consecutive trading days with morning gold price greater than $300 . During this period , there had been a streak of four hundred days with price more than $400 , though the $500 price only lasted two days at most .
In Melbourne , Australia , during the years between 1981 and 1990 , the weather had been pleasant . There had been more than two thousand days with minimal temperature above zero , and the streak was not ending . ( We do not have data beyond 1990 . ) The longest streak during which the temperature hit above 35 degrees Celsius is six days . It was in the summer of the year 1981 .
More than half of the prominent streaks we found in the traffic data of the Lady Gaga Wikipedia page were around September 12th , when she became a big winner in the MTV Video Music Awards ( VMA ) 2010 . During that time , the page had been visited by at least 2000 people in every hour for almost four days .
Number of Candidate Streaks :
The three algorithms for candidate streak generation , namely Baseline ( Algorithm 1 ) , NLPS ( Algorithm 3 ) , and LLPS ( Algorithm 5 ) , differ by the ways they produce candidates and thus the numbers of produced candidates . Table 2 shows the total number of candidate streaks considered by each algorithm on each data sequence . The baseline algorithm produces an extremely large number of candidates since it enumerates all possible streaks , eg , 7603202 =2.89×1013 for WC98 . By contrast , NLPS only needs to 7http://gregsadetsky.com/aol data/ 8http://itaeelblgov/html/contrib/WorldCuphtml
2
SP500
SP500
2000
1500
1000
500 e u a v l
0 0
2000
4000
6000 position
8000
10000 e u a v l l i a m n m i
2000
1500
1000
500
0 0
2000
4000
8000 streak length ( days )
6000 s e t a d d n a c i f o #
5 10
0 10
0
10000
SP500
Baseline
NLPS
LLPS
600
400
200 e z i s
SP500
Baseline , NLPS LLPS
2000
4000
6000 position
8000
10000
0
0
2000
4000
6000 position
8000
10000
( a ) Data Sequence
( b ) Prominent Streaks
( c ) Number of Candidate Streaks
( d ) Size of Dynamic Skyline
Figure 4 : Detailed Results on SP500 . data sequence
Baseline
NLPS
LLPS
Gold River Melb1 Melb2 Wiki1 Wiki2 Wiki3 SP500 HPQ IBM AOL WC98
78 78 390 387 711 711 689 4717 6099 5079
446622 >1 hour
40 15 22 21 28 18 40 599 165 209 546
9 3 12 15 15 15 16 21 18 22 78
27477
3404
Table 3 : Execution Time ( in Milliseconds ) . consider Þ1≤ ≤∣ ∣ ℒ , which is a superset of the real prominent streaks but a much smaller subset of all possible streaks . For instance , the number of candidate streaks by NLPS is 1.78×108 for WC98 , which is 5 orders of magnitude smaller than what Baseline considers . LLPS further significantly educes the number of candidates by only considering LPSs . For example , there are 6.69×106 LPSs in WC98 , which is about 30 times smaller than 178×108 Note that the number of LPSs for LLPS is bounded by sequence length ( Property 2 ) , which is verified by Table 2 . Execution Time :
The number of candidate streaks directly determines the efficiency of our algorithms . In Table 3 we report the execution time of our algorithms using the three candidate streak generation methods ( Baseline , NLPS , LLPS ) , for all 12 data sequences . For skyline operation , we implemented the sorting based , external memory sortingbased , and BST based skyline methods mentioned in Section 1 . Under these different skyline methods , Baseline , NLPS , and LLPS perform and compare consistently . Therefore in Table 3 we only report the results for implementations based on the BST based skyline method , due to space limitations . The reported execution time is in milliseconds and is the average of five runs .
We excluded data loading time , ie , the time spent on just reading each data file . This is because data loading time is dominated by processing time of the algorithms once the data file gets large . In our experiments , WC98 cost 1 second to load while the loading time of all other datasets was below 30ms .
In Table 3 we use ‘>1 hour’ to denote the execution time when an algorithm could not finish within one hour ( ie , 3600000ms ) . This lower bound is sufficient in showing the performance difference of the various algorithms .
With regard to the comparison of Baseline , NLPS , and LLPS , it is clear from Table 3 that LLPS outperforms NLPS , and both NLPS and LLPS are far more efficient than Baseline . This is exactly due to the large gap in the number of candidate streaks ( shown in Table 2 ) , which in turn determines the number of comparisons performed during skyline operations . A Closer Look :
To have a better understanding of the experimental results , we take a close look at the SP500 data sequence . Similar observations are made on other data sequences , hence we omit the analysis of
600
400
200
) s m ( e m i t
0
0
AOL
LLPS−1 , NLPS LLPS−2 LLPS−4 LLPS−8 LLPS−16
4 x 10
WC98
LLPS−1 , NLPS LLPS−2 LLPS−4 LLPS−8 LLPS−16
3
2
1
) s m ( e m i t
5
10 position
( a ) AOL
0
0
15 4 x 10
2
4 position
6
8 6 x 10
( b ) WC98
Figure 5 : Cumulative Execution Time at Various Positions , for Different Reporting Frequencies .
600
400
200
) s m ( e m i t l a t o t
0 0 10
AOL
4 x 10
WC98
3
2
1
) s m ( e m i t l a t o t
1 10
2 10 length of the interval
( a ) AOL
3 10
0 0 10
1 10
2 10
3 10 length of the interval
( b ) WC98
Figure 6 : Total Execution Time by Reporting Frequencies . other sequences . Figure 4(a ) shows the data sequence itself . We see that the sequence is almost monotonically increasing at the coarse grain level . Due to that , the number of prominent streaks found in SP500 ( 497 , as shown in Table 1 ) is the most among all the data sequences . We also visualize the prominent streaks in SP500 in Figure 4(b ) , where the axis is for interval length and the axis is for minimal value in the interval .
In Table 2 we have seen the huge difference among Baseline , NLPS and LLPS in total number of candidate streaks . These three algorithms all generate candidates progressively . Therefore in Figure 4(c ) we show , for each algorithm , the number of new candidate streaks produced at every value position of the data sequence . The figure clearly shows the superiority of LLPS since it always generates orders of magnitude less candidates at each position .
The BST based skyline method maintains a dynamic skyline , as a binary search tree , in memory . The size of this tree affects the efficiency of tree operations , such as inserting and deleting a streak . Figure 4(d ) shows the size of the dynamic skyline along the sequence of SP500 by each algorithm . The curves for Baseline and NLPS overlap since they both store , at every position , in the dynamic skyline . In contrary , LLPS does not need to store some streaks in , hence the tree size is much smaller than that for Baseline/NLPS when the sequence is almost constantly growing in the second half of SP500 .
Monitoring Prominent Streaks :
In Section 4 we discussed how to monitor the prominent streaks as a data sequence evolves and new data values come . The adaptation of LLPS for monitoring purpose was shown in Algorithm 7 . This algorithm can control at which positions the prominent streaks ( so far ) need to be reported .
Take AOL and WC98 as examples . Figure 5 shows the execution time of Algorithm 7 ) . The axis represents the sequence po sition , and the axis is for the total execution time by that position . There are five curves in each figure , corresponding to five different frequencies of reporting prominent streaks . For instance , LLPS 1 means that , whenever a new data entry comes , all the prominent streaks so far are reported ; LLPS 16 means the prominent streaks are requested at every 16 data entries . As discussed in Section 4 , LLPS 1 is identical to NLPS ( Algorithm 3 ) , and LLPS is identical to LLPS ( Algorithm 5 ) , where is the sequence length when it does not evolve anymore . Figures 5(a ) and 5(b ) clearly show that the total execution time of LLPS increases as the reporting frequency increases ( ie , reporting interval decreases ) . Figures 6(a ) and 6(b ) further show how the total execution time changes along different reporting intervals . We can see that the execution time drops rapidly at the beginning and quickly reaches near optimal value even when the frequency is still pretty high ( eg , reporting the prominent streaks at every 16 entries . )
6 . CONCLUSION
In this paper , we study the problem of discovering prominent streaks in sequence data . A prominent streak is a long consecutive subsequence consisting of only large ( small ) values . We propose efficient methods based on the concept of local prominent streak ( LPS ) . We prove that prominent streaks are a subset of LPSs and the number of LPSs is less than the length of a data sequence . Our linear LPS based method guarantees to consider only local prominent streaks , thus achieving significant reduction in candidate streaks . The results of experiments over multiple real datasets verified the effectiveness of the proposed methods .
Acknowledgements : We thank Jun Yang for discussion of the initial ideas of this paper when Chengkai Li and Jun Yang were both visiting HP Labs in Beijing , China in the summer of 2010 .
7 . REFERENCES [ 1 ] R . Agrawal , C . Faloutsos , and A . Swami . Efficient similarity search in sequence databases . Foundations of Data Organization and Algorithms , pages 69–84 , 1993 .
[ 2 ] R . Agrawal , K . ip Lin , H . S . Sawhney , and K . Shim . Fast similarity search in the presence of noise , scaling , and translation in time series databases . In VLDB , pages 490–501 , 1995 .
[ 3 ] R . Agrawal and R . Srikant . Mining sequential patterns . In Proceedings of the Eleventh International Conference on Data Engineering , pages 3–14 , 1995 .
[ 4 ] S . Altschul , W . Gish , W . Miller , E . Myers , and D . Lipman .
Basic local alignment search tool . Journal of molecular biology , 215(3):403–410 , 1990 .
[ 5 ] S . Börzsönyi , D . Kossmann , and K . Stocker . The skyline operator . In Proceedings of the 17th International Conference on Data Engineering , pages 421–430 , 2001 .
[ 6 ] J . Chomicki , P . Godfrey , J . Gryz , and D . Liang . Skyline with presorting . In Proceedings of the International Conference on Data Engineering , pages 717–719 , 2003 .
[ 7 ] S . Cohen , C . Li , J . Yang , and C . Yu . Computational journalism : A call to arms to database researchers . In Proceedings of the 5th Biennial Conference on Innovative Data Systems Research ( CIDR ) , pages 148–151 , 2011 .
[ 8 ] C . Faloutsos , M . Ranganathan , and Y . Manolopoulos . Fast subsequence matching in time series databases . In SIGMOD , pages 419–429 , 1994 .
[ 9 ] HTKung , F.Luccio , and FPPreparata On finding the maxima of a set of vectors . Journal of the ACM , 22(4):469 – 476 , 1975 .
[ 10 ] B . Jiang and J . Pei . Online interval skyline queries on time series . In Proceedings of the 25th International Conference on Data Engineering , pages 1036–1047 , 2009 .
[ 11 ] D . Kossmann , F . Ramsak , and S . Rost . Shooting stars in the sky : an online algorithm for skyline queries . In Proceedings of the 28th international conference on Very Large Data Bases , pages 275–286 , 2002 .
[ 12 ] T . W . Liao . Clustering of time series data–a survey . Pattern
Recognition , 38(11):1857 – 1874 , 2005 .
[ 13 ] T . Oates , L . Firoiu , and P . Cohen . Clustering time series with hidden markov models and dynamic time warping . In Proceedings of the IJCAI 99 Workshop on Neural , Symbolic and Reinforcement Learning Methods for Sequence Learning , pages 17–21 , 1999 .
[ 14 ] D . Papadias , Y . Tao , G . Fu , and B . Seeger . Progressive skyline computation in database systems . ACM Transactions on Database Systems ( TODS ) , 30(1):41–82 , 2005 .
[ 15 ] J . Pei , J . Han , B . Mortazavi Asl , J . Wang , H . Pinto , Q . Chen ,
U . Dayal , and M C Hsu . Mining sequential patterns by pattern growth : the prefixspan approach . IEEE Transactions on Knowledge and Data Engineering , 16(11):1424–1440 , 2004 .
[ 16 ] J . Pei , Y . Yuan , X . Lin , W . Jin , M . Ester , Q . Liu , W . Wang , Y . Tao , J . X . Yu , and Q . Zhang . Towards multidimensional subspace skyline analysis . ACM Trans . Database Syst . , 31(4):1335–1381 , 2006 .
[ 17 ] L . Rabiner . A tutorial on hidden Markov models and selected applications in speech recognition . Proceedings of the IEEE , 77(2):257–286 , 1989 .
[ 18 ] Y . Shin and D . Fussell . Parametric kernels for sequence data analysis . In Proceedings of the 20th international joint conference on Artifical intelligence , pages 1047–1052 , 2007 . [ 19 ] P . Smyth . Clustering sequences with hidden Markov models . In Advances in neural information processing systems , 1997 .
[ 20 ] R . Srikant and R . Agrawal . Mining sequential patterns :
Generalizations and performance improvements . Advances in Database Technology ( EDBT ) , pages 1–17 , 1996 .
[ 21 ] K L Tan , P K Eng , and B . C . Ooi . Efficient progressive skyline computation . In Proceedings of the 27th International Conference on Very Large Data Bases , pages 301–310 , 2001 .
[ 22 ] M . Wang and X . Wang . Finding the plateau in an aggregated time series . In Advances in Web Age Information Management ( WAIM ) , pages 325–336 , 2006 .
[ 23 ] W K Wong . Data mining for early disease outbreak detection . PhD thesis , Pittsburgh , PA , USA , 2004 .
[ 24 ] T . Xia and D . Zhang . Refreshing the sky : the compressed skycube with efficient support for frequent updates . In Proceedings of the 2006 ACM SIGMOD international conference on Management of data , pages 491–502 , 2006 .
[ 25 ] X . Yan , J . Han , and R . Afshar . CloSpan : Mining closed sequential patterns in large datasets . In Proceedings of SIAM International Conference on Data Mining , pages 166–177 , 2003 .
[ 26 ] B . Yi , H . Jagadish , and C . Faloutsos . Efficient retrieval of similar time sequences under time warping . In Proceedings of the 14th International Conference on Data Engineering , pages 201–208 , 1998 .
[ 27 ] M . Zaki . SPADE : An efficient algorithm for mining frequent sequences . Machine Learning , 42(1):31–60 , 2001 .
