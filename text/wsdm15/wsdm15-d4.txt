Listwise Approach for Rank Aggregation in Crowdsourcing
Shuzi Niu† , Yanyan Lan‡ , Jiafeng Guo‡ , Xueqi Cheng‡ , Lei Yu† and Guoping Long†
†Institute of Software , Chinese Academy of Sciences , Beijing , P . R . China
‡Institute of Computing Technology , Chinese Academy of Sciences , Beijing , P . R . China {shuzi,yulei,guoping}@iscasaccn , {lanyanyan,guojiafeng,cxq}@ictaccn
ABSTRACT Inferring a gold standard ranking over a set of objects , such as documents or images , is a key task to build test collections for various applications like Web search and recommender systems . Crowdsourcing services provide an efficient and inexpensive way to collect judgments via labeling by sets of annotators . We thus study the problem of finding a consensus ranking from crowdsourced judgments . In contrast to conventional rank aggregation methods which minimize the distance between predicted ranking and input judgments from either pointwise or pairwise perspective , we argue that it is critical to consider the distance in a listwise way to emphasize the position importance in ranking . Therefore , we introduce a new listwise approach in this paper , where ranking measure based objective functions are utilized for optimization . In addition , we also incorporate the annotator quality into our model since the reliability of annotators can vary significantly in crowdsourcing . For optimization , we transform the optimization problem to the Linear Sum Assignment Problem , and then solve it by a very efficient algorithm named CrowdAgg guaranteeing the optimal solution . Experimental results on two benchmark data sets from different crowdsourcing tasks show that our algorithm is much more effective , efficient and robust than traditional methods .
Categories and Subject Descriptors H33 [ Information Search and Retrieval ] : Relevance Feedback
General Terms Algorithms
Keywords Crowdsourced Labeling ; Rank Aggregation ; Evaluation Measures
1 .
INTRODUCTION
Inferring ranking over a set of objects is a critical task for building gold standard test collections in many rankingbased real applications , such as information retrieval and recommender systems . Recently , crowdsourcing services have attracted much attention since it provides an inexpensive and efficient means to obtain judgments over the objects . Typically , there are two kinds of judgments widely adopted in crowdsourcing , ie ratings and preferences . For example in a query document relevance labeling task , annotators are asked to present the rating for each document such as binary or graded relevance judgments with absolute labeling strategy [ 13 ] , while they are asked to present preferences for each document pair with relative labeling strategy [ 3 ] .
In literature , many rank aggregation methods have been proposed to find a consensus ranking over these judgments . They all fall into the framework of minimizing a distance between predicted ranking and input judgments . According to different distances used for optimization , they can be mainly divided into three categories : pointwise , pairwise and listwise approaches . Pointwise rank aggregation methods like MedianRank [ 7 ] utilize Footrule Distance as the objective function , which aims to well predict the rank of each object from annotators’ judgments on that object . Pairwise methods such as Bradley Terry [ 23 ] , MPM [ 26 ] and CrowdBT[4 ] measure the distances in a pairwise way , and pairs are viewed as independent . We can see that both pointwise and pairwise approach define the distance from a local perspective . Therefore , they all ignore the position importance in ranking , which is nevertheless critical for rank aggregation . Traditional listwise rank aggregation methods such as Cranking [ 16 ] and Plackett Luce [ 10 ] are infeasible for most crowdsourced labeling tasks , such as crowdsourced pairwise labeling task .
Therefore we propose a novel listwise approach to tackle this problem . The key idea is that in order to take position importance into account , it is better to define the distance in a listwise way . Inspired by the fact that IR measures such as NDCG and RBP are designed for this purpose , and usually used for evaluation in rank aggregation , we propose to directly utilize these measures for distance definition . However , the characteristics of crowdsourcing poses two challenges to this approach : ( 1 ) Annotators usually provide ratings or preferences over a subset of objects in crowdsourcing , which make the computation of ranking measures impossible since the ranking information cannot be induced from the incomplete input information ; ( 2 ) Annotator quality should be considered in the objective function since the reliability of annotators can vary significantly in crowdsourcing .
To address these challenges , we propose to map the judgments ( ratings or preferences ) to input ranking , and incorporate annotator quality in this process . Specifically , the rank of an object is viewed as a random variable and defined in the form of pairwise contests . The distribution of the random variable is then derived iteratively based on the pairwise probability , which can be estimated through pairwise preference relationships conveyed in judgments . Meanwhile , annotator quality is modeled as the probability that one agrees with the ( unknown ) true pairwise preference relationships , which is estimated in an iterative way based on the intermediate aggregation results . By incorporating annotator quality into the definition of the pairwise probability , we obtain the final form of rank distribution of each object . Based on this listwise mapping , we define the new objective functions for rank aggregation as the expectation of IR measures ( ie NDCG and RBP ) over the rank distribution , called expected measures . Note that the ground truth label of an object in the expected measure is derived from its rank using a mapping function as in [ 19 ] .
Due to the property of ranking measures [ 15 ] , we find that expected ranking measures have a general formulation as a sum of utility functions on ranks in both the induced ranking inputs and the output permutations . Therefore , the goal of the optimization approach is to find a ground truth permutation which maximize the sum of utility functions . As a sequence , it is natural to transform the rank aggregation problem to the Linear Sum Assignment Problem ( LSAP for short ) , where the profit for each assignment is defined by taking the sum of utility functions over all the induced ranking inputs . As the utility functions are represented as a product of two functions , the optimal solution of the optimization problem can be directly obtained through sorting as shown by Rearrangement Inequality . We refer this optimization algorithm as CrowdAgg .
Finally we conduct extensive experiments on two benchmark data sets from different crowdsourcing tasks , ie querydocument relevance labeling and music similarity labeling . One is collected based on graded judgments , and the other is based on pairwise preferences . Experimental results show that CrowdAgg is much more effective , efficient and robust than traditional rank aggregation methods in both data sets . In summary , the main contributions of our proposed ap proach are listed as follows :
• we introduce expected measures as the new objective functions for rank aggregation to consider position importance , by utilizing rating and preference judgments in a listwise way ;
• We incorporate annotator quality into the expected measures to well cope with the reliability of annotators in crowdsourcing ;
• We propose an efficient algorithm by formalizing the aggregation problem as LSAP and obtaining an optimal solution with Rearrangement Inequality .
2 . BACKGROUNDS
In this section , we first give a problem formalization of rank aggregation in crowdsourcing . We then review some related works of traditional pointwise and pairwise rank aggregation methods . Finally , we give a brief introduction to some major evaluation measures used in rank aggregation . 2.1 Rank Aggregation in CrowdSourcing Assume that we are given a set of n objects denoted as D = {x1 , x2 , . . . , xn} and a set of m annotators in a crowdsourcing task . Each annotator i provides a set of labels τi over these objects , which are often unreliable and incomplete . Typically , each τi can be represented in two forms : ratings or preferences .
( 1 ) With absolute labeling strategy , annotators are asked to present the relevance rating of each object . Usually , the ratings are incomplete , which means that annotators may only rate a subset of all the objects . We denote this subIn this case , τi can be set labeled by annotator i as Di . represented as ( τi(x1),··· , τi(xn) ) , where j ∈ {0 , 1,· ·· , C − 1} , z(i ) N ull , if xj ∈ Di ; if xj /∈ Di .
τi(xj ) =
.
( 2 ) With relative labeling strategy , annotator are asked to present the relative relevance between any two objects . Usually , the preferences are also incomplete , which means that annotators may only provide preferences over a subset of all the possible object pairs . We denote this subset labeled by annotator i as Pi . In this case , τi can be represented as ( τi(x1 , x2),·· · , τi(xn−1 , xn) ) , where τi(xs , xt ) is defined as follows , and z(i ) j = 1 means xs is more relevant than xt in terms of annotator i ; otherwise z(i ) j ∈ {0 , 1} , z(i ) N ull , if ( xs , xt ) ∈ Pi ; if ( xs , xt ) /∈ Pi .
τi(xs , xt ) = j = 0 .
.
The goal of rank aggregation in crowdsourcing is then to find a consensus ranking π over these n objects which best represents the ranking relations conveyed in multiple sets of labels {τ1 , . . . , τm} in the form of ratings or preferences , where π is a permutation and π(xi ) stands for the position of object xi in the final consensus ranking π . To this end , most aggregation algorithms try to optimize a distance measure M between the inputs τ1,··· , τm and the final ranking π , which can be formulated as follows . mfi i=1 max π∈Π
M ( π , τi ) .
( 1 )
2.2 Rank Aggregation Methods
Here we briefly review some of the traditional rank aggregation methods in unsupervised scenario . According to different kinds of distance measures used for optimization , they can be mainly divided into three categories : pointwise , pairwise and listwise methods .
221 Pointwise Rank Aggregation Methods
Pointwise rank aggregation methods utilize the label information per object from all the inputs to define the ranking function or objective function . The distance measure between the consensus ranking and the input ranking for Borda Count [ 1 ] and Median Rank [ 7 ] is decomposed into position difference per object . Borda Count [ 1 ] minimizes the average Spearman Rank Coefficient and obtains the optimal ranking by the mean position of each object . Median
Rank [ 7 ] optimizes the average Spearman Footrule Distance between the consensus ranking and each ranking input , and obtains the optimal solution by sorting objects according to their median rank in ascending order .
222 Pairwise Rank Aggregation Methods
Pairwise rank aggregation methods organize their ranking inputs in a pairwise way whether for ranking functions or optimization objective functions .
Graph based Method . Condorcet Fuse [ 18 ] constructs the Condorcet Graph with n items and its arc representing the pairwise comparison results between two items by majority voting , and a Hamiltonian path is obtained from this graph by QuickSort . With all the aggregated pairwise preferences summarized in a tournament , GreedyOrder [ 5 ] minimizes the pairwise disagreement cost in this tournament to obtain the consensus ranking .
Pairwise Preference Matrix Based Methods . With all the inputs summarized in a pairwise preference matrix , SVP ( Singular Vector Projection ) [ 9 ] minimized nuclear norm of this pairwise preference matrix by rank 2 factorization .
Pairwise Probabilistic Approach . Probabilistic approaches define the generative probability of pairwise preferences and optimize the likelihood function by a gradient based approach [ 23 , 26 , 4 ] . Bradley Terry [ 23 ] defines the pairwise probability based on the Bradley Terry model [ 22 ] . In crowdsourcing , annotator accuracy should be incorporated . MPM ( Multinomial Preference Model ) [ 26 ] models both the power of rank difference and the deviation from the consensus ranking per annotator in the pairwise generative probability . Crowd BT [ 4 ] extends the Bradley Terry model by explicitly incorporating the labeling qualities of different annotators . The key preprocess in EloRating [ 2 , 27 ] is to estimate pairwise probabilities and annotator qualities through EM algorithm , then obtain the relevance score from the remaining preferences with the estimated information through Elo Rating system .
223 Listwise Rank Aggregation Methods
Listwise rank aggregation methods take the position importance into consideration and treat ranking inputs in a listwise way whether for ranking functions or optimization objective functions . Plackett Luce [ 10 ] and Cranking [ 16 ] define the similarity measure to be the generative probability of each ranking list with Plackett Luce model [ 22 ] and Mallow ’s model [ 22 ] respectively , and optimize this similarity function by a maximum likelihood procedure . Although the annotator quality can be incorporated into the optimization objective function , these listwise methods do not work when these inputs are in the form of pairwise preferences . St.Agg [ 20 ] , one of state of the art methods in traditional rank aggregation tasks like metasearch , can solve this problem by deriving one item ’s rank position from its pairwise preferences with other items , but it is not fit for the crowdsourced setting . So these listwise approaches are not used as baselines in our experiments .
2.3 Evaluation Measure Given a set of objects D = {x1 , ··· , xn} , and the groundtruth labels Y = ( y1,··· , yn ) , which are usually based on multi grade ratings . Let π be the consensus ranking list over D by some rank aggregation method . Evaluation measures such as NDCG [ 12 ] , Precision [ 6 ] and RBP [ 17 ] defined as follows are often employed to evaluate the performance of this aggregation method .
'n
NDCG@k(π , Y ) =
,
( 2 ) j=1 g(yj )D(rπ j )I(rπ j ≤ k )
DCGmax(n ) j stands for the rank of xj in the ranking list π , j ) is the where rπ g(yj ) is the gain function with g(yj ) = 2yj −1 , D(rπ discount function with D(rπ function with I(A ) = 1 if A is true and I(A ) = 0 otherwise . j ) , I(· ) is an indicator log(1+rπ j ) =
1 nfi nfi j=1
Precision@k(π , Y ) =
RBP(π , Y ) = ( 1 − p ) j ≤ k ) ,
I(rπ yj k j −1 rπ yjp
( 3 )
( 4 ) j=1 where p ∈ [ 0 , 1 ] is a constant value . yj in Eq ( 3 ) and Eq ( 4 ) takes a binary value from 0 and 1 , which can be transformed from multi level ratings like LETOR1 .
3 . MOTIVATION
For the aggregation task in crowdsourcing , pointwise and pairwise rank aggregation methods will be unsuitable , because both the position importance and annotator quality , which are two main characteristics of crowdsourced data , are not incorporated in these aggregation methods .
.n j=1(π(xj ) − τ ( xj))2 , where
First , both the traditional pointwise and pairwise methods define the distance from a local perspective . For example , the Spearman Rank Correlation used in Borda Count [ 1 ] treats each object equal , and measures the distance between two ranking lists π and τ as π(xj ) and τ ( xj ) means the position of xj in the ranking list π and τ respectively . The distances used in pairwise methods treat each object pair as equal , and do not distinguish the different impact of pairs from different positions . Therefore , both pointwise and pairwise methods ignore the position importance in ranking , which is nevertheless critical for rank aggregation . Specifically , one usually cares more about objects ranked high in the output ranking list , and thus objects with different positions as well as pairs constructed from different positions should have different impact on the distance measure .
Second , the reliability of judgments obtained at low cost from crowdsourcing services varies significantly , which is the major difference from traditional aggregation task . Experimental results in [ 26 ] shows that the aggregation methods taking annotator quality into consideration , such as MPM [ 26 ] always achieve better performances than those without annotator quality factor , such as Borda Count [ 1 ] minimizing .m i=1
.n j=1(π(xj ) − τi(xj))2 . So it is reasonable to satisfy the majority for some cases while put emphasis on a small subset for the other cases , which can be adjusted by weighting judgments from various sources differently .
To tackle these challenges , we propose to define the distance in a listwise way which taking both position importance and annotator quality into consideration , namely listwise rank aggregation approach .
4 . LISTWISE RANK AGGREGATION
Inspired by the fact that position importance is considered in IR evaluation measure such as NDCG and RBP , we
1 http://researchmicrosoftcom/en us/um/beijing/projects/letor/ propose to directly utilize these measures as the distance to optimize . Specifically , the output ranking list is viewed as ( unknown ) ground truth , and the input judgments from annotator i can be viewed as some observations of the groundtruth waiting for evaluation . When defining the specific distance based on these evaluation measures , however , we find the characteristics of crowdsourcing pose great challenges to this approach .
Firstly , the incompleteness and labeling form ( ratings or preferences ) make the computation of these measures impossible . As we described above , the annotators usually only select a subset of data for labeling , thus the labeled data are incomplete . Furthermore , labels from annotators either in ratings or preferences , are quite different from the required full order ranking input in the computation of evaluation measures . Therefore , the direct computation of evaluation measures based on these kinds of data is not available .
Secondly , the annotator quality is not included in these evaluation measures . As we know , the reliability of annotators can vary significantly in crowdsourcing , which make annotator quality an important factor to consider in rank aggregation . However , it is not easy to include this factor in the computation of these evaluation measures .
To address these challenges , we propose to map the judgments ( ratings or preferences ) to input ranking , and incorporate annotator quality during this process . In the following subsection , we will introduce how we conduct the mapping and incorporation process . 4.1 Listwise Mapping
We cannot induce a ranking list directly by sorting based on ratings or pairwise comparisons mainly due to their incompleteness . In order to make the computation of evaluation measures possible with the input judgments ( ratings or preferences ) , we propose a mapping function from the judgments to input rankings . By analyzing these evaluation measures , we find that the rank of an object is actually needed in the computation . Therefore , we turn to the problem how to map the judgments to obtain the rank of each object . However , since the provided judgments are often incomplete , we incorporate uncertainty into the mapping process . Specifically , we view the pairwise contest as a random variable , then the rank of each object can be defined based on all the results of pairwise contests . Furthermore , the annotator quality can be incorporated in the derivation of the rank distribution .
411 Randomized Pairwise Comparisons
τi
A pairwise contest between xs and xt refers to deciding which one is ranked higher in terms of annotator i . We view each pairwise contest as a Bernoulli trial , so that the result that xs wins the contest denoted as Xst follows the Bernoulli st ∼ Binomial(1 , P ( xs ≺τi xt) ) , where pairdistribution , X st = 1 ) = P ( xs ≺τi xt ) means the wise probability P ( X τi probability that xs is ranked higher than xt , denoted as p st . τi st when the input judgments are ratings , we first transform the rating data to preference data so that we can treat both types of judgements in a unified way .
To estimate the pairwise probability p
τi
⎧⎨ ⎩ 1 ,
0 , N ull ,
τi(xs , xt ) = if τi(xs ) > τi(xt ) , if τi(xs ) < τi(xt ) , otherwise .
( 5 )
According to previous studies [ 8 , 20 ] , the pairwise probability that one document xs is more relevant than xt is dependent on the relative rank position difference between two documents . The basic idea is to estimate this probability by leveraging all pairwise preference relationships conveyed in judgments from an annotator . On the basis of the preference data ( either directly provided in preference judgments or generated from ratings as above ) , we give the estimation of the pairwise probability p
τi st as shown in Eq ( 6 ) . n
Ni(xs)−Ni(xt ) 1− Ni(xt)−Ni(xs ) 0.5 , n
,
Ni(xs ) > Ni(xt ) , , Ni(xs ) < Ni(xt ) ,
Ni(xs ) = Ni(xt ) or τi(xs , xt ) = N ull , ( 6 ) where Ni(xj ) denotes the number of pairwise contests that xj wins in τi , which is determined by the known pairwise preferences in Eq ( 7 ) ,
⎧⎪⎨ ⎪⎩
τi st= p nfi
Ni(xj ) =
τi(xj , xl ) .
( 7 ) l=1,l.=j,τi(xj ,xl).=Null
412 Incorporating Annotator Quality
'
To incorporate annotator quality into the distance , we propose to introduce a parameter ηi to stand for the quality of annotator i . It is natural that we define the parameter as the degree the judgments by annotator i agrees with the ( unknown ) ground truth . Since we have turned both types of input judgements into preference data above , we define the degree based on preferences in a unified way . That is the ratio of the number of preference pairs appear in both the judgments of annotator i and ground truth against the number of all the preference pairs in ground truth . The formal definition of the quality of annotator i is represented as follows . s,t I(τ
ηi =
' ∗(xs , xt ) = 1&&τi(xs , xt ) = 1 ) s,t I(τi(xs , xt ) = 1 )
,
( 8 )
∗ denotes the ground truth and I(· ) is the indiwhere τ cator function . We can see that when annotator i is perfect , we have ηi ≈ 1 ; if he/she is a random spammer , we have ηi ≈ 0.5 ; if he/she is a malicious ( ie poorly informed ) spammer , we have ηi ≈ 0 . Note that in estimation of the annotator quality , the ground truth τ is actually unknown . Instead , we estimate the annotator quality in an iterative way by taking the aggregated ranking in previous step as the approximation of the ground truth . Please refer to section 5.3 for detailed machinery to parameter estimation .
∗
With the annotator quality defined above , we modify the τi st by incorporating this quality factor pairwise probability p as follows
τi st + ( 1 − ηi)(1 − p
τi st ) .
ηip
( 9 )
413 From Pairwise Contests to Rank Distribution
We now derive the rank of an object . When the results of pairwise contests are deterministic and the data is complete , the rank of an object xj provided by annotator i is determined by the number of objects being beaten in Eq ( 11 ) .
I(xj ≺τi xl )
( 11 ) nfi
τi j = r l=1,l.=j
Algorithm 1 Iterative Procedure for Rank Distribution of xj in τi . Input : ( 1)An item set D with n items ; ( 2)Pairwise preferences from input τi with Eq ( 5 ) ; ( 3)Annotator quality ηi ;
τi jl }n
Output : A rank distribution of item xj on n positions . 1 : Compute {p 2 : initialize the distribution:[P ( 0)(r 3 : t = 1 ; 4 : for each xl ∈ D − xj do l=1,lfi=j using Eq ( 9 ) ;
τi j = 0 ) , P ( 0)(r
τi j = 1 ) ] = [ 1 , 0 ] ;
τi j = 0 ) , . . . , P ( t)(r
τi j = t + 1 ) ]
τi j = 0 ) , . . . , P ( t−1)(r
τi j = t ) ] ∗ [ p
τi jl , 1 − p
τi jl ] ;
( 10 )
[ P ( t)(r = [ P ( t−1)(r t + + ;
5 : end for 6 : return [ P ( n−1)(rτ j = 0 ) , . . . , P ( n−1)(rτ j = n − 1) ] .
Algorithm 2 Divide and Conquer Algorithm for Rank Distribution of xj in τi . Input : ( 1)An item set D with n items ; ( 2)Pairwise preferences from input τi with Eq ( 5 ) ; ( 3)Annotator quality ηi ;
Output : A rank distribution of item xj on n positions [ P ( r
τi j = l=1,lfi=j using Eq ( 9 ) and pjj = 1 ; l=1,1,n ) .
τi
τi
τi j = n − 1) ] . jl }n jl }n
0 ) , . . . , P ( r 1 : Compute {p 2 : return DC RankDistribution({p 3 : 4 : DC RankDistribution({psl}n 5 : if a + 1 == b then , 6 : 7 : else if a == b then , 8 : 9 : end if 10 : mid=(a+b)/2 ; 11 : return return [ psa , 1 − psa ] ∗ [ psb , 1 − psb ] ; return [ psa , 1 − psa ] ; l=1,a,b ) :
RankDistribution({psl}n l=1,mid + 1,b ) ;
DC RankDistribution({psl}n l=1,a,mid)*DC
When the pairwise contests are random experiments like Bernoulli trials , the rank of object xj provided by annotator i is a random variable , which means the number of successes of the n − 1 independent Bernoulli trials defined in Eq ( 12 ) .
τi j = r
τi jl
X
( 12 ) nfi l=1,l.=j
τi j
τi
τi j
The distribution of r is the convolution of the individual probability density distributions [ 21 ] . This yields an iterative computation of the rank distribution with complexity O(n2 ) , as shown in Alg . 1 . The operator ∗ in Alg . 1 stands for the convolution operation . Specifically , the distribution is initialized ( t = 0 ) by the indicator function with of r the probability at the top position is 1 and 0 otherwise . When a new object xl comes , the rank distribution of xj is updated by the convolution between its current rank distribution and the pairwise contest distribution denoted as τi jl ] estimated by annotator τi . For the iteration t , [ p the running time of this convolution computation between the rank distribution ( a sequence with length t + 2 ) and pairwise contest distribution ( a sequence with length 2 ) is O(2t ) . The final distribution ( t = n− 1 ) of r involves such n − 1 convolutions , so the time complexity for computing .n−1 t=0 2t = O(n2 ) . rank distribution of xj is jl , 1 − p
The time complexity of the Alg . 1 can be improved with divide and conquer strategy . The main idea comes from the fact that the required rank distribution can be divided into convolutions of two parts : the first part is convolutions of the first n 2 objects , and the second part is convolution of the
τi j rest n 2 objects . For each convolution , the divide and conquer process continues . Therefore , we can change the original sequential computation process to the following divide andconquer process as shown in Alg . 2 . Suppose the distribution of pairwise contest between xj and xj is denoted as [ 1 , 0 ] , then the rank distribution of xj by annotator τi can be computed as the convolution of n sequences with each sequence τi represented as a pairwise contest distribution [ p jl ] . For the first iteration ( T = 1 ) , there are n 2 convolution computations between any two sequences with length 2 , thus n 2 sequences with length 3 will be obtained ; for the T th iteration , there are n 2T convolutions between any two sequences with length 2T−1 + 1 , and thus n 2T sequences with length 2T +1 will be obtained by FFT ( Fast Fourier Transform ) . Finally , the rank distribution of xj will be derived after log2 n iterations , and the time complexity for this efficient algorithm is
2T ( 2T + 1 ) log2(2T + 1 ) = O(n log2 jl , 1 − p log2 T =1
2 n ) .
.
τi n n
4.2 Expected Ranking Measures
Through listwise mapping described above , all the judgments ( ratings or preferences ) provided by each annotator can be transformed to the listwise information , described as ranks of all the objects in D with estimated rank distributions . Therefore , the input data has become the form of ranking lists .
Recall that we view the output consensus ranking as the ground truth , and the input ranking lists from each annotator as observations waiting for evaluation . In this sense , the computation of traditional evaluation measures is still infeasible in application , since the ground truth is in the form of a ranking list . Inspired by using extended evaluation measures such as κ NDCG and κ RBP for ranking based ground truth as in [ 19 ] , where a mapping function κ : κ(rπ is utilized to map the rank rπ j to ground truth label , we propose to define the listwise distance on the basis of these measures as follows . n−rπ j ) = n j
κ NDCG(π , τi ) =
κ RBP(π , τi ) = nfi nfi j=1
τi j )
, g(κ(rπ j ))D(r DCGmax(n ) j −1 .
π j )p
τi r
κ(r j=1
Obviously they can be represented as the following general τi j ) , where Ev stands for any form Ev(π , τi ) = evaluation measures . j=1 v(rπ
.n j , r
The above measures represent a distance between the output consensus ranking π and the input ranking with respect to τi . Since the input ranking with respect to τi is stochastic according to section 413 , we propose to use expectation as the distance . As a consequence , the expectation over the rank distribution is conducted , and we obtain the expected measures defined as follows as the final listwise distance . nfi n−1fi
Evs(π , τi ) = v(r
π j , r
τi j )P ( r
τi j = r )
( 13 ) j=1 r=0
5 . OPTIMIZATION ALGORITHMS
In this section , we investigate optimization algorithms to solve the problem maximizing these expected measures , such as κ NDCGs and κ RBPs . Firstly , we find that these expected measures can be represented as a summation of the utility function for each item at a certain position , and obtain the final objective function L(η , π ) parameterized by annotator quality η = ( η1,· ·· , ηm ) and the aggregated ranking π . Then given the annotator quality η , the optimization problem can be easily interpreted as the assignment problem with a cost function in the form of linear sum , called the Linear Sum Assignment Problem ( LSAP for short ) . Therefore we can transform the optimization problem to the LSAP . With each utility function expressed as a special product form in expected measures , the optimal ranking can be directly obtained by Rearrangement Inequality . After the optimal ranking is obtained , we update the annotator quality η with its definition formula , and repeat the above optimization process until the obtained ranking list keeps unchanged . In this way , we obtain an alternating approach to optimize L(η , π ) , referred to as CrowdAgg . 5.1 General Form of Expected Measures
As shown in Eq ( 13 ) , these expected ranking measures can be formulated into a general form as the sum of utility functions . Each utility function measures the utility that an object with ranking information encoded in its rank distributions , is placed at some position in the output permutation , denoted as us in this paper . As a consequence , the original expected measures can be rewritten as follows : nfi n−1fi
Evs(π , τi ) = us(rπ j ) , where us(rπ j ) = v(rπ j , r)P ( rτ j = r ) . j=1 r=0
The optimization objective is to find a permutation which maximizes the sum of utility functions in Eq ( 14 ) . mfi nfi mfi max
π∈Π,η∈[0,1]m
L(η , π ) =
Evs(π , τi ) = us(rπ j )
( 14 ) i=1 j=1 i=1
5.2 Optimization with fixed Parameter η
Through the representation as the sum of utility functions in the above section , we can easily interpret this optimization problem with fixed annotator quality η as to find the optimal assignment of positions for each object according to inputs ( τ1,··· , τm ) . Moreover , each utility function is a function on an individual object and its output position . Therefore , the optimization problem in our listwise rank aggregation is intrinsically a Linear Sum Assignment Problem ( LSAP ) , which is formalized as follows . The object set is denoted as D = {x1 , . . . , xn} and the possible position set is represented as R = {1 , . . . , n} . For any object xj , the permutation π can be viewed as the assignment of object xj , j = 1 , . . . , n to the position rπ j , which brings about the utility denoted by w(rπ j ) = .m i=1 To find the optimal permutation , we need to optimize the i=1 us(rπ r=0 v(rπ
τi j = r ) .
.n−1 j , r)P ( r j , j ) =
.m following function in Eq ( 15 ) . nfi j=1 max π∈Π w(r
π j , j )
( 15 )
Many classical algorithms such as Hungarian method [ 14 ] have been proposed to solve this LSAP . For a LSAP with size n , the running time of Hungarian algorithm is O(n4 ) . The high time complexity motivates us to further probe the optimization problem . In light of the factorization in v(·,· ) [ 15 ] ,
Table 1 : Factorizations of Expected Ranking Measures f ( · ) h(· ) measures
κ NDCGs
κ RBPs
1
DCGmax ( n ) .n−1
( 1 − p ) r=0 p
.n−1 r=0
τi P ( r j =r ) log(1+r ) g(κ(rπ j ) ) r−1P ( r
τi j = r )
κ(r
π j ) we find that the utility function us(·,· ) for most expected measures can be factorized as a product of two functions . One is on positions in output permutation denoted as h(· ) , and the other is the function of position per object in the estimated ranking input denoted as f ( · ) . The mathematical form is shown as below . us(rπ j , r
τi j ) = h(rπ j )f ( r
τi j )
( 16 )
For example , κ NDCGs and κ RBPs can both be represented in this way . Specifically , two different kinds of definition of f ( · ) and h(· ) with regard to these expected measures are listed in Table 1 . ization of utility function us(·,· ) in Eq ( 16 ) , w(rπ .m
According to the definition of w(rπ j , j ) and the factorj , j ) =
τi i=1 h(rπ j ) = h(rπ Let uj = h(rπ j ) and vj =
τi j ) ) . τi j ) , the infimum and supremum of the objective function in Eq ( 14 ) will be directly obtained by the Rearrangement Inequality [ 11 ] as shown in Lemma 1 . i=1 f ( r .m i=1 f ( r j )f ( r
.m j )(
Lemma 1 . Let 0 ≤ u1 ≤ . . . ≤ un and 0 ≤ v1 ≤ . . . ≤ vn .
Then for any permutation φ nfi uivn+1−i ≤ nfi uivφ(i ) ≤ nfi uivi . i=1 i=1 i=1
.m
τi j ) j ) and vj =
Specifically , we can see that uj = h(rπ i=1 f ( r satisfies the non negative conditions since f ( · ) and h(· ) are both positive . Sorting x1 , . . . , xn according to vj in descending order , we obtain the optimal permutation πG with the supremum of the objective function based on Lemma 1 . As a consequence , we obtain a simple algorithm which can efficiently find the optimal solution for the optimization problem with fixed η by sorting based on vj . The details of algorithm are shown in Alg . 3 . Algorithm 3 Maximize L(η , π ) with fixed η Input : An item set D and a collection of ranking inputs {τ1 , . . . , τm} Output : An aggregated ranking π . 1 : Compute F ( xj ) =
τi j ) , where f is defined for different over D ;
.m i=1f ( r measures in Table 1 ;
2 : Obtain the permutation π by sorting F ( xj ) in descending order ; 3 : return π .
5.3 Estimating Parameter η with Fixed π
Recall that η is defined as the degree the judgments by annotator i agrees with the unknown ground truth . We propose an iterative way to estimate η by taking the currently aggregated ranking list π as the approximation of the ground truth . Therefore , η can be estimated , where each ηi is calculated as the pairwise agreement between π and the input τi .
' s,t I(π(xs , xt ) = 1&&τi(xs , xt ) = 1 )
'
( 17 )
ηi = s,t I(τi(xs , xt ) = 1 )
5.4 CrowdAgg As described above , a natural optimization strategy to maximize the objective L(η , π ) is the alternating approach : ( 1 ) initialize η = 1 ; ( 2 ) fix η and optimize over π by LSAP as described in Alg . 3 ; ( 3 ) fix π and estimate the η as described in Eq ( 17 ) ; ( 4 ) repeat step ( 2 ) and ( 3 ) until the obtained permutations keep stable . This alternating approach for optimizing L(η , π ) is referred to as CrowdAgg .
6 . EXPERIMENTS
In this section we present our experimental results . Firstly , we introduce the experimental setting , including data sets , baseline methods and evaluation measures . Then we present ranking performance comparison between our proposed CrowdAgg and the baseline methods . In addition , we empirically study the annotator accuracy and the robustness of these rank aggregation methods with respect to two kinds of noisy annotators , such as spammers and malicious annotators [ 4 ] . Finally , we compare the running time of the proposed CrowdAgg with the state of the art rank aggregation algorithms . 6.1 Experimental Setting
Here we implement our listwise rank aggregation methods CrowdAgg where Alg . 2 is used to compute rank distributions . We denote the algorithm optimizing expected measures κ NDCG and κ RBP as CrowdAggNDCG and CrowdAggRBP , respectively .
Data Sets . To evaluate the performance of our proposed method , we use two data sets from two different kinds of crowdsourcing tasks as described below . Conventional rank aggregation data sets like Million Query data sets from meta search application may not simulate the crowdsourced setting well , so we utilize such two benchmark data sets directly from crowdsourced labeling tasks .
( 1 ) Graded Judgment Data from Crowdsourced Relevance Labeling Task . CS TREC2011 data set is a collection of topic document pairs labeled as relevant or non relevant by Mechanical Turks . Therefore , the judgments are provided in the form of ratings . There are 100 topics with 190 documents per topic , and the number of annotators is varied with topics , about 43 on average . Each annotator rates less than 10 documents on average .
( 2 ) Preference Judgment Data from Crowdsourced Similarity Labeling Task . MIREX2005 data set is a collection of music form Symbolic Melodic Similarity task in Music Information Retrieval Evaluation eXchange ( MIREX ) in 2005 . This task aims to find a ranking list of musical pieces according to the similarity to certain piece of music , which acts as a query in information retrieval . There are 11 pieces of music as queries . Pairwise preference labels are collected from Mechanical Turks in [ 25 ] . The ground truth is in the form of 5 grade judgements [ 24 ] .
Baseline Methods . Since pointwise methods and listwise methods can only be applied to graded judgment data2 , we use pairwise rank aggregation methods as our baselines to obtain a comprehensive comparison on both data sets . According to whether annotator quality is considered in the model , traditional pairwise rank aggregation methods fall into two categories : ( 1 ) Methods without annotator quality , such as CondorcetFuse [ 18 ] , GreedyOrder [ 5 ] , BradleyTerry [ 23 ] ; ( 2 ) Methods with annotator quality , such as MPM [ 26 ] ,
2
St.Agg [ 20 ] does not incorporate the annotator quality .
Crowd BT [ 4 ] and EMEloRating [ 2 , 27 ] . Here we describe some parameter settings in these methods : the learning rates used in gradient methods for BradleyTerry ( 0.1 for both data sets ) , MPM ( 0.1 for MusicCrowd and 0.0001 for TREC2011 ) and Crowd BT ( 0.01 for both data sets ) were chosen according to the best performance in terms of NDCG@10 , and the parameter setting in EMEloRating is the same as in [ 2 ] .
Evaluation Methods . There are several routines to evaluate these unsupervised rank aggregation methods . One of the leading methods is to use traditional IR evaluation measures with ground truth labels given for evaluation . In this paper we use NDCG@{3 , 5 , 7 , 9 , 10} , RBP , Precision@{3 , 5 , 7 , 9 , 10} and MAP for evaluation . Routinely the constant value p in RBP is 095 In MIREX2005 , the transformation of five grade relevance labels to binary values is mentioned in the background for RBP .
6.2 Ranking Accuracy Analysis
The ranking performance comparison results on both CSTREC2011 and MIREX2005 are shown in Table 2 . It is clear that our proposed methods CrowdAgg outperform the baseline methods on both data sets in most cases .
In terms of NDCG@10 , CrowdAggNDCG achieves 1.5 % improvement with respect to the best pairwise rank aggregation method , ie GreedyOrder on CS TREC2011 , and 6.2 % improvement with respect to the best pairwise method , ie MPM on MIREX2005 . In terms of RBP , the improvement of CrowdAggRBP is 5.6 % with respect to the best pairwise method , ie GreedyOrder on CS TREC2011 , and 5.4 % with respect to the best pairwise method .
The improvements can be explained as follows : ( 1 ) Compared with the first kind of pairwise rank aggregation methods ( without annotator quality ) , CrowdAgg are superior by distinguishing the reliability of preference judgments by the annotator quality ; ( 2 ) Compared with the second kind of pairwise rank aggregation methods , CrowdAgg obtain better ranking performance by treating the judgments per annotator in a listwise way and taking into consideration of position importance in optimization . Besides , more precise estimation of annotator quality η can be obtained from better aggregated ranking and enhance each other in an alternating way in CrowdAgg .
6.3 Annotator Quality Analysis
In this section , we empirically study the influence of annotator quality . Firstly , we validate the hypothesis that annotator quality will impact much to the results and thus should be taken into consideration in the rank aggregation method . Secondly , we compare the quality distribution among different methods as conducted in [ 4 ] .
631 Impact of Annotator Quality
To validate the hypothesis that annotator quality is important for rank aggregation , we propose to verify whether the aggregation methods with annotator quality will perform better than that without annotator quality . Therefore , we consider the methods with annotator quality , such as MPM , Crowd BT , EMEloRating and our proposed CrowdAggNDCG . These methods can be easily reduced to the version without annotator quality by treating each annotator equally . Specifically , we set these annotator quality parameters η = 1 as in MPM , Crowd BT and CrowdAggNDCG , and denote the new corresponding version as MPM eq , Crowd BT eq ,
Table 2 : Ranking Performance Comparison of Preference Aggregation Methods on two data sets ( CS TREC2011 and MIREX2005 ) . For each specific metric , the result with bold type is significantly better than the other corresponding results through one tailed and paired t tests ( p value< 005 )
( a ) CS TREC2011
Methods CondorcetFuse GreedyOrder BradleyTerry MPM Crowd BT EMEloRating CrowdAggNDCG CrowdAggRBP
P@3 P@5 P@7 P@9 P@10 MAP N@3 N@5 N@7 N@9 N@10 RBP 0.597 0.604 0.621 0.626 0.621 0.560 0.591 0.597 0.609 0.613 0.611 0.564 0.710 0.698 0.674 0.661 0.657 0.604 0.713 0.704 0.688 0.679 0.676 0.621 0.373 0.370 0.386 0.378 0.377 0.326 0.377 0.375 0.384 0.379 0.379 0.345 0.690 0.690 0.671 0.663 0.657 0.570 0.687 0.687 0.676 0.671 0.667 0.580 0.403 0.416 0.416 0.422 0.417 0.534 0.404 0.412 0.413 0.417 0.414 0.473 0.690 0.666 0.673 0.654 0.649 0.588 0.691 0.676 0.679 0.667 0.663 0.602 0.713 0.694 0.680 0.673 0.673 0.635 0.715 0.702 0.692 0.687 0.686 0.657 0.680 0.684 0.686 0.679 0.674 0.636 0.683 0.686 0.686 0.682 0.679 0.658
( b ) MIREX2005
Methods CondorcetFuse GreedyOrder BradleyTerry MPM Crowd BT EMEloRating CrowdAggNDCG CrowdAggRBP
P@3 P@5 P@7 P@9 P@10 MAP N@3 N@5 N@7 N@9 N@10 RBP 0.364 0.455 0.468 0.434 0.418 0.472 0.523 0.596 0.514 0.478 0.414 0.170 0.273 0.418 0.455 0.424 0.409 0.459 0.444 0.525 0.490 0.462 0.399 0.171 0.242 0.400 0.429 0.424 0.499 0.441 0.412 0.496 0.460 0.447 0.385 0.170 0.485 0.509 0.494 0.475 0.446 0.516 0.695 0.693 0.612 0.576 0.501 0.179 0.576 0.564 0.507 0.465 0.446 0.608 0.814 0.801 0.640 0.578 0.496 0.200 0.364 0.436 0.455 0.424 0.409 0.495 0.606 0.629 0.568 0.511 0.436 0.203 0.697 0.600 0.546 0.485 0.455 0.712 0.955 0.880 0.703 0.621 0.532 0.214 0.697 0.600 0.546 0.485 0.455 0.713 0.955 0.879 0.702 0.620 0.531 0.215
CrowdAggNDCG eq . While for EMEloRating , we simply take all the pairwise preferences as output to the Elo rating system without filtering by annotator quality , denoted as EMEloRating eq .
MPM−eq MPM
0.7 0.65
Crowd−BT−eq Crowd−BT
P@10 MAP N@10
RBP
( b )
CrowdAggNDCG−eq CrowdAggNDCG
0.6 0.55 0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15
0.55 0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15
0.55
0.45
0.35
0.25
0.15
0.8 0.75 0.65 0.55 0.45 0.35 0.25 0.15
P@10 MAP
N@10
RBP
( a )
EMEloRating−eq EMEloRating
P@10 MAP
N@10
RBP which also takes the annotator quality into consideration in essence . In other words , EMEloRating eq is not a true reduced version without annotator quality .
632 Accuracy for Different Distributions of Anno tator Quality
Here we study the difference between the estimated annotator quality distribution and the ground truth quality distribution . We say the estimated annotator quality is more accurate if the difference is small . There are various definition of annotator quality for different aggregation methods , such as adherence parameter in MPM [ 26 ] , pairwise preference accuracy ACC in Crowd BT [ 4 ] ( known as the accuracy based on Wilcoxon Mann Whitney statistics employed in [ 26] ) , and confusion matrix C in EMEloRating . In this paper , we conduct a similar empirical study as in [ 4 ] , and thus employ pairwise preference accuracy ACC to compute the annotator quality . s,t I(ys > yt)I(π(xs , xt ) ) s,t I(ys > yt )
,
( 18 ) where ys is the ground truth label for each item xs ∈ D . Specifically , we conduct experiments on MIREX2005 with 11 topics to evaluate the quality distribution of annotators for each topic . We calculate the KL divergence between the estimated quality distribution and the ground truth distribution . The average divergence score is for CrowdAggNDCG , Crowd BT , EMEloRating and MPM is 1.43 , 2.86 , 214,193 , respectively . Obviously , CrowdAggNDCG achieves the smallest divergence . It indicates that our proposed CrowdAgg can make better estimation on the annotator quality than the baseline methods . We randomly selected 9 topics from MIREX2005 and plot the quality distributions of different methods and groud truth in Fig 2 for illustration . Each sub figure is corresponding to one topic . From Fig 2 we can also find that the distribution estimated from CrowdAggNDCG ( ie blue curve ) in each sub figure is more similar to the real distribution ( ie black histogram ) than other methods .
'
'
6.4 Robustness Analysis
In this section we investigate the robustness of aggregation algorithms to spammers . Similar to [ 4 ] , we mainly care
P@10 MAP N@10
RBP
ACC =
( c )
( d )
Figure 1 : Ranking Performance Comparison between models without and with annotator quality on MIREX2005 ( The comparison results are significant through paired t tests with p value< 005 )
We conduct experiments on MIREX2005 with the two groups of aggregation methods , as mentioned above . The ranking performances are compared between the original one and the corresponding reduced version in terms of P@10 , MAP , NDCG@10 and RBP . The comparison results are shown in Fig 1 . Obviously the method with annotator quality can significantly outperform the corresponding method without annotator quality . For example , the improvement of CrowdBT is 10.3 % with the consideration of annotator quality in terms of NDCG@10 . The largest performance difference between CrowdAggNDCG and CrowdAggNDCG eq is 56 % with respect to MAP , while the smallest performance difference is 11.1 % in terms of P@10 . Note that the performance difference between EMEloRating and EMEloRating eq is relatively small . Through our analysis , we find the major reason lies in the EM process of the EMEloRating eq method ,
0 1 @ P
0.5
0.48
0.46
0.44
0.42
0.4 0
GreedyOrder CondorcetFuse BradleyTerry MPM CrowdBT EMEloRating CrowdAgg
NDCG
0.2 0.8 Ratio of Added Random Spammers
0.4
0.6
0.75
P A M
0.7
0.65
0.6
0.55
0.5
0.45
1
0.4 0
0.2 0.8 Ratio of Added Random Spammers
0.4
0.6
0.55
0.5
0.45
0.4
0 1 @ G C D N
0.35 0
1
0.2 0.8 Ratio of Added Random Spammers
0.4
0.6
0.23
0.22
0.21
0.2
0.19
0.18
0.17
P B R
0.16 0
1
0.2 0.8 Ratio of Added Random Spammers
0.4
0.6
1
( a )
( b )
( c )
( d )
Figure 3 : Ranking Performance Variation along with Ratio of Added Random Spammers on MIREX2005 ( Performance variation results for various aggregation methods are significantly different through paired t tests with p value< 005 )
1
0.5
0
0
1 s r o t
0.5 a t o n n A f o %
0
0
1
0.5
0
0
Real Distribution
CrowdAgg 1
CrowdBT
0.5
0
0
1
0.5
0
0
1
0.5
0
0
1
1
1
0.5
0.5
0.5
0.5
0.5
0.5
1
1
1
EMEloRating 1
MPM
0.5
0
0
1
0.5
0
0
1
0.5
0
0
0.5
0.5
1
1
0.5
1 Annotator Quality
Figure 2 : Quality Distribution of Annotators computed from the ground truth labels ( Real Distribution ) and Estimated in aggregation methods on randomly sampled 9 topics of MIREX2005 about two kinds of spammers , random spammers and malicious ( ie poorly informed ) spammers . The random spammers assign the label randomly , while the malicious spammers assign the wrong label most of time . We investigate the robustness of all these aggregation methods to two kinds of spammers respectively , and the experimental results are shown only on MIREX2005 for space limitation . Similar results can be also obtained on CS TREC2011 .
641 Random Spammers
To simulate the labeling behavior of spammers , we define the decision function on the labels of the pairwise objects between xi and xj . For random spammer , the decision function τRandSpam as follows is proposed , where y(xi , xj ) is the preference label inferred from the ground truth relevance labels in MIREX2005 .
)
τRandSpam(xi , xj ) =
1 − y(xi , xj ) with probability 0.5 y(xi , xj ) with probability 0.5
Here we consider the robustness of aggregation methods with the addition of random spammers . Since the number of annotators m is different for different topics , we consider the number of random spammers to be added is in proportion to m , and refer the proportion as Ratio of Added Random Spammers . We vary the ratio from 0 to 1 with a step of 0.05 , and obtain 20 different data sets for each given ratio . The performance is then averaged over the 20 data sets for each ratio . The ranking performance comparisons over different algorithms in terms of P@10 , MAP , NDCG@10 and RBP are shown in Fig 3 .
To quantitatively compare the robustness of different methods , we use the coefficient of variation of performance3 , which is a widely used measure for variance comparison . From the results , we find that our proposed CrowdAggNDCG achieve the best coefficient of variation in terms of RBP among methods from top to bottom in the legend ( 0.0227 , 0.0245 , 0.0436 , 0.0330 , 0.0312 , 0.0191 , 0.0185 ) , and is the third best in terms of NDCG@10 among methods from top to bottom in the legend ( 0.0147 , 0.0174 , 0.3347 , 0.0228 , 0.0406 , 0.3238 , 00216 ) Furthermore , we can see that with the increase of the ratio of added random spammers , CrowdAggNDCG can almost always outperform all the other methods . Therefore , we conclude that our proposed CrowdAgg is robust to random spammers .
642 Malicious Spammers To simulate the labeling behavior of malicious spammers , we define the decision function τMaliSpam = 1 − y(xi , xj ) , where y(xi , xj ) is the preference label inferred from the groundtruth relevance labels in MIREX2005 .
Here we consider the robustness of aggregation algorithms with the addition of malicious spammers . We introduce Ratio of Added Malicious Spammers and conduct experiments with different ratios as in the above section . The performance comparison results are depicted in Fig 4 .
Similarly , we also compare the robustness of different methods in terms of coefficient of variation of performance . From the results , we find that our proposed CrowdAggNDCG achieve the best coefficient of variation in terms of both NDCG@10 and RBP among methods from top to bottom in the legend ( NDCG@10:0.0100 , 0.0122 , 0.0112 , 0.0186 , 0.0498 , 0.0175 , 0.0099 ; RBP:0.0118 , 0.0174 , 0.0218 , 0.0407 , 0.0382 , 0.0275 , 00116 ) Meanwhile , we can also see that with the increase of the ratio of added malicious spammers , CrowdAggNDCG can almost always outperform all the other methods . Therefore , CrowdAgg is also robust to malicious spammers on MIREX2005 .
In summary , our proposed CrowdAgg is robust to both random spammers and malicious spammers . Each random spammer always provides half useful information to derive the ground truth ranking , so there is no reason for the performance decline in Fig 3 . Each malicious spammer always provides harmful information to derive the ground truth
3
Coefficient of variation is defined as the the ratio of the standard deviation to the mean
0 1 @ P
0.5
0.48
0.46
0.44
0.42
0.4 0
GreedyOrder CondorcetFuse BradleyTerry MPM CrowdBT EMEloRating CrowdAgg
NDCG
0.2
0.4
0.6
0.8
Ratio of Added Malicious Spammers
0.75
P A M
0.7
0.65
0.6
0.55
0.5
0.45
1
0.4 0
0.2
0.4
0.6
0.8
Ratio of Added Malicious Spammers
0.55
0.5
0.45
0.4
0 1 @ G C D N
0.35 0
1
0.2
0.4
0.6
0.8
Ratio of Added Malicious Spammers
0.23
0.22
0.21
0.2
0.19
0.18
0.17
P B R
0.16 0
1
0.2
0.4
0.6
0.8
Ratio of Added Malicious Spammers
1
( a )
( b )
( c )
( d )
Figure 4 : Ranking Performance Variation along with Ratio of Added Malicious Spammers on MIREX2005 ( Performance variation results for various aggregation methods are significantly different through paired t tests with p value< 005 ) ranking , so the performance decline in Fig 4 is deemed to appear when the ratio achieves some threshold . In terms of different spammers , CrowdAgg is more robust to the malicious spammers than the random spammer , which is promising in real application .
7 . CONCLUSION
In this paper , we propose a listwise rank aggregation method in crowdsourcing . The main idea is to adopt IR measures as objective function to take position importance into consideration , as compared with traditional pointwise or pairwise rank aggregation methods . To solve the challenges introduced by the characteristics of crowdsourcing , we propose to map the judgments ( ratings or preferences ) to input ranking and incorporate annotator quality in this process . So we define the new expected measures , and use them as objective functions . For optimization , we propose a novel alternative optimization algorithm named CrowdAgg based on LSAP and the iterative estimation of annotator quality . Finally , our experimental results on benchmark data sets shows the effectiveness and robustness of our proposed CrowdAgg .
For future work , we will investigate how to adapt our listwise rank aggregation method to an active learning setting , which is suitable to be blended to crowdsourcing .
Acknowledgments This research work was funded by the 973 Program of China under Grants No . 2012CB316303 , No . 2014CB340401 , the 863 Program of China under Grants No . 2012AA011003 , the National Natural Science of China under Grant No . 61472401 , No . 61203298 , No . 61100072 , and the National Key Technology R&D Program of China under Grants No . 2012BAH39B02 , No . 2012BAH46B04 .
8 . REFERENCES [ 1 ] J . A . Aslam and M . Montague . Models for metasearch .
SIGIR2001 , pages 276–284 .
[ 2 ] M . Bashir , J . Anderton , J . Wu , P . B . Golbus , V . Pavlu , and
J . A . Aslam . A document rating system for preference judgements . SIGIR ’13 , pages 909–912 .
[ 3 ] B . Carterette , P . N . Bennett , D . M . Chickering , and S . T .
Dumais . Here or there : Preference judgments for relevance . ECIR’08 , pages 16–27 , 2008 .
[ 4 ] X . Chen , P . N . Bennett , K . Collins Thompson , and E . Horvitz . Pairwise ranking aggregation in a crowdsourced setting . WSDM ’13 , pages 193–202 .
[ 5 ] W . W . Cohen , R . E . Schapire , and Y . Singer . Learning to order things . JAIR1999 , 10(1):243–270 , May .
[ 6 ] C . D . Manning , P . Raghavan , and H . Sch´l´ztze . Introduction to
Information Retrieval . Cambridge University Press , 2008 .
[ 7 ] R . Fagin , R . Kumar , and D . Sivakumar . Efficient similarity search and classification via rank aggregation . pages 301–312 . [ 8 ] M . Farah and D . Vanderpooten . An outranking approach for rank aggregation in information retrieval . SIGIR2007 , pages 591–598 .
[ 9 ] D . F . Gleich and L h Lim . Rank aggregation via nuclear norm minimization . KDD2011 , pages 60–68 .
[ 10 ] J . Guiver and E . Snelson . Bayesian inference for plackett luce ranking models . ICML2009 , pages 377–384 .
[ 11 ] G . H . Hardy , J . E . Littlewood , and G . P´l˝olya . Inequalities .
Cambridge University Press , 1952 .
[ 12 ] K . J¨arvelin and J . Kek¨al¨ainen . Cumulated gain based evaluation of ir techniques . ACM TOIS2002 , 20(4):422–446 .
[ 13 ] J . Kek¨al¨ainen . Binary and graded relevance in ir evaluations comparison of the effects on ranking of ir systems . IPM , 41(5):1019–1033 , Sept . 2005 .
[ 14 ] H . W . Kuhn . The hungarian method for the assignment problem . Naval Research Logistics , 2(1 2):83–97 , 1955 .
[ 15 ] Q . Le and A . Smola . Direct optimization of ranking measures . arXiv Preprint arXiv : 0704.3359 , 2007 .
[ 16 ] G . Lebanon and J . D . Lafferty . Cranking : Combining rankings using conditional probability models on permutations . ICML2002 , pages 363–370 .
[ 17 ] A . Moffat and J . Zobel . Rank biased precision for measurement of retrieval effectiveness . ACM TOIS2008 , 27(1):2:1–2:27 , Dec . [ 18 ] M . Montague and J . A . Aslam . Condorcet fusion for improved retrieval . pages 538–548 .
[ 19 ] S . Niu , J . Guo , Y . Lan , and X . Cheng . Top k learning to rank : labeling , ranking and evaluation . SIGIR ’12 , pages 751–760 .
[ 20 ] S . Niu , Y . Lan , J . Guo , and X . Cheng . Stochastic rank aggregation . UAI2013 , pages 478–487 .
[ 21 ] A . Papoulis . Random Variables and Stochastic Processes .
McGraw Hill , 1991 .
[ 22 ] P . RL The analysis of permutations . Applied Statistics ,
24(2):193–202 , 1974 .
[ 23 ] L . L . Thurstone . The method of paired comparisons for social values . The Journal of Abnormal and Social Psychology , 21(4):384 , 1927 .
[ 24 ] R . Typke , M . den Hoed , J . de Nooijer , F . Wiering , and R . C . Veltkamp . A ground truth for half a million musical incipits . JDIM , 3(1):34–38 , 2005 .
[ 25 ] J . Urbano , J . Morato , M . Marrero , and D . Mart´ın .
Crowdsourcing preference judgments for evaluation of music similarity tasks . In SIGIR CSE , pages 9–16 .
[ 26 ] M . N . Volkovs and R . S . Zemel . A flexible generative model for preference aggregation . WWW2012 , pages 479–488 .
[ 27 ] J . Wu . Applying em to compute document relevance from crowdsourced pair preferences . Master ’s thesis , Northeastern University , 2013 .
