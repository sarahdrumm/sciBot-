Ef.cient mining of high branching factor attribute trees
Alexandre Termier1 , Marie Christine Rousset2 , Mich(cid:30)ele Sebag2 ,
Kouzou Ohara1 , Takashi Washio1 & Hiroshi Motoda1
1 : ISIR , Osaka University
8 1 , Mihogaoka , Ibarakishi , Osaka , 567 0047 , Japan ftermier , ohara , washio , motodag@arsankenosaka uacjp 2 : CNRS & Universit·e Paris Sud ( LRI ) & INRIA ( Futurs )
Building 490 , Universit·e Paris Sud , 91405 Orsay Cedex , France . fmcr , sebagg@lri.fr
Abstract
In this paper , we present a new tree mining algorithm , DRYADEPARENT , based on the hooking principle .rst introduced in DRYADE [ 9 ] . In the experiments , we demonstrate that the branching factor and depth of the frequent patterns to .nd are key factor of complexity for tree mining algorithms . We show that DRYADEPARENT outperforms the current fastest algorithm , CMTreeMiner , by orders of magnitude on datasets where the frequent patterns have a high branching factor .
1 . Introduction
In the recent tree mining research , most algorithms use the same generate and test principle that made the success of frequent itemset mining algorithms . They usually deal with .nding all the frequent subtrees from a collection of trees . Pioneering algorithms by Asai & al . [ 3 ] and Zaki [ 12 ] extend the Apriori algorithm [ 1 ] principle to trees , using tree inclusion de.nitions imposing the preservation of the order of the siblings . The second generation of tree mining algorithms used canonical forms to get rid of the order preservation constraint [ 4 , 7 , 13 ] . Newest algorithms tend to search only closed frequent subtrees , for the computation gains that can be achieved without quality loss : DRYADE [ 9 ] , relying on a very general tree inclusion definition , CMTreeMiner [ 6 ] , mining closed frequent induced subtrees and the recent CLOTT [ 2 ] for mining closed frequent attribute trees ( not yet implemented ) .
We present DRYADEPARENT , a new closed tree mining algorithm that searches the same search space as state of the art algorithms but with a different hooking principle .
Experiments show that our approach consistently has faster computation times , and is nearly unaffected by the structures of the frequent patterns to .nd , whereas the state of the art algorithm exhibits a severe dependency on branching factor .
The outline of the paper is as follows . Section 2 introduces notations and denitions Section 3 gives an overview of the DRYADEPARENT algorithm . Section 4 reports detailed comparative experiments . In section 5 , we conclude and give some directions for future work .
2 . Formal Background
Let L = fl1 ; :: : ; lng be a set of labels . A labeled tree T = ( N ; A ; root(T ) ; ’ ) is an acyclic connected graph , where N is the set of nodes , A fl N . N is a binary relation over N de.ning the set of edges , root(T ) is a distinguished node called the root , and ’ is a labeling function ’ : N 7! L assigning a label to each node of the tree . We assume without loss of generality that edges are unlabeled . We assume that the reader is familiar with the notions of child , parent , ancestor and descendant for the nodes of a tree .
A tree is an attribute tree if ’ is such that two sibling nodes cannot have the same label ( more details on attribute trees can be found in [ 2] ) .
Tree inclusion : Let AT = ( N1 ; A1 ; root(AT ) ; ’1 ) be an attribute tree and T = ( N2 ; A2 ; root(T ) ; ’2 ) be a tree . AT is an induced subtree of T if there exists an injective mapping : N1 7! N2 such that : 1 ) preserves the labels : 8u 2 N1 ’1(u ) = ’2((u ) ) and 2 ) preserves the parent relationship : 8u ; v 2 N1 ( u ; v ) 2 A1 , ( (u ) ; ( v ) ) 2 A2 . This relation will be written AT v T , and we will sometimes say that AT is included into T .
T1
A1
T2
A11
Datatree
T3
A22
T4
A27
T5
J34
B2
C7
A12
B13
C19
B23
C25
B28
C31
D33
C35
D3
E4
F8
K10
D14
E16
F20
G24
H26
K30
G24
H32
H5 G6
I9
I15
G17
H18
I21
F36
I37
C
F
I
Locc = f7 ; 19 ; 35g
Patterns :
P1
B
D
E
G
H
Locc = f1 ; 11g
A
P2
A
P3
A
P4
C
F
I
B
G
C
H
Locc = f22 ; 27g
B
C
Locc = f1 ; 11 ; 22 ; 27g
Figure 1 : Datatree example ( node identi.ers are subscripts of node labels ) , and patterns for " = 2
If AT v T , the set of mappings supporting the inclusion is denoted EM(AT ; T ) . The set of occurrences of AT in T , denoted Locc(AT ; T ) , is the set of nodes of T onto which the root of AT is mapped by a mapping of EM(AT ; T ) .
Frequent attribute tree : Let T D = fT1 ; :: : ; Tmg be a tree database . The datatree DT D is the tree whose root is an unlabeled node , having the trees fT1 ; :: : ; Tmg as its direct subtrees .
Let " be an absolute frequency threshold . AT is a frequent attribute tree of DT D if supportd(AT ) " , where i=1 d(AT ; Ti ) with d(AT ; Ti ) = 1 supportd(AT ) = Pm if AT v Ti , 0 otherwise . The set of all frequent attribute trees is denoted by F(DT D ; " ) , abbreviated as F in this paper .
Closed trees : A frequent attribute tree is closed if it is maximal , according to inclusion , for its set of occurrences , ie : a frequent attribute tree AT 2 F is closed either if it is not included into any other frequent attribute trees , or if it is included into a frequent attribute tree AT 0 2 F , there exists a mapping in EM(AT ; DT D ) which is not in the mappings of EM(AT 0 ; DT D ) .
We will denote the set of all closed frequent attribute trees as C .
In the example of Figure 1 , the frequent attribute trees P1 and P2 are closed because they are not included into any other frequent attribute tree , P3 is closed because even if it is included into P1 and P2 , neither the occurrences of P1 nor the occurrences of P2 can cover all the occurrences of P3 , and in the same way P4 is also closed .
The tree mining problem we are interested in is to .nd all the closed frequent attribute trees for a given datatree and support threshold . From now on , we will refer to the closed frequent attribute trees as patterns .
3 . The DRYADEPARENT algorithm
The goal of DRYADEPARENT is to .nd all the patterns in C , depth level by depth level , starting with the root and .nishing with the deepest leaves in a breadth .rst fashion .
T i1 A
T i2 B
T i3 E
B
C
D
E
G
H
Locc : f1 ; 11 ; 22 ; 27g f2 ; 13g f4 ; 16g
T i4 C
T i5 F
T i6 B
T i7 C
F f7 ; 19 ; 35g
I f8 ; 20 ; 36g
G f23 ; 28g
H f25 ; 31g
Locc :
Figure 2 : Tiles of our example
Iteration 1
P3
A
B
C
Locc = f1 ; 11 ; 22 ; 27g
Iteration 2
A
B
D
E
Locc = f1 ; 11g Iteration 3
A
B
D
E
G
H
Locc = f1 ; 11g
C
F
C
F
P2
A
C
F
Locc = f7 ; 19 ; 35g
B
G
C
H
Locc = f22 ; 27g
P1
P4
C
F
I
Locc = f7 ; 19 ; 35g
Figure 3 : DRYADEPARENT discovery process
We only give in the following section the intuition behind the DRYADEPARENT algorithm , the interested reader is referred to [ 10 ] for more details . 3.1 Tiles
The essence of DRYADEPARENT is to build the patterns depth level by level through proper hookings ( de.ned later ) of the closed frequent attribute trees of depth 1 , which we call tiles .
Finding such tiles can be reformulated as a propositional frequent itemset mining problem as follows : for each label , use a closed frequent itemset discovery algorithm like LCM2 [ 11 ] to compute all the closed frequent sets of children labels for this label . The Figure 2 shows the tiles for our example .
2
3.2 Hooking the tiles
The previously computed tiles can then be hooked together , ie a tile whose root has label l becomes a subtree of another tile having a leaf of label l , to build more complex trees . A proper strategy is needed to avoid as much as possible to construct attributes trees that would be found unclosed in a later iteration . Our strategy consists in constructing attributes trees which are isomorphic to the k .rst depth levels of the patterns , each iteration adding one depth level to the isomorphism . For this purpose , the .rst task of DRYADEPARENT is to discover in the tiles those corresponding to the depth levels 0 and 1 of the patterns , the root tiles . Some of these tiles can be found immediately for they cannot be hooked on any other tile , they will be the starting point for the .rst iteration of DRYADEPARENT . This is the case for T i1 in our example , as show in Figure 3 . For the rest of the root tiles , they can also be used as building blocks for other patterns : they will be used as root of a pattern only when it will become clear that they are not only a building block , to avoid generating unclosed attribute trees . This is the case for T i4 in our example , which can be hooked on T i1 . Only in iteration 2 will this tile be used as a root tile to construct pattern P4 . The computation of the set of tiles that must be hooked to the root tiles to make closed frequent attribute trees with one more depth level is delegated to a closed frequent item set mining algorithm . The attribute trees created by hooking become starting points for the next iteration .
The whole process is shown for our example in Figure 3 . On the root tile T i1 , one can either hook the tiles fT i2 ; T i4g or the tiles fT i6 ; T i7g , the latter leading to the pattern P2 . Note the different occurrences of the two constructed attribute trees . From the hooking of fT i2 ; T i4g on T i1 , one can then hook the tile T i3 , leading to the pattern P1 . The tile T i4 is not only a building block of P1 , it also have an occurrence which do not appear in P1 ( 35 ) : it is used as a root tile , and the only possible hooking on it is T i5 , leading to the pattern P4 .
The soundness and completeness of this hooking mech anism has been proved in [ 8 ] .
4 . Experiments
This section reports on the experimental validation of DRYADEPARENT on real world and arti.cial datasets . All runtimes are measured on 2.8 GHz Intel Xeon processor with 2GB memory ( Rocks 330 Linux ) . DRYADEPARENT is written in C++ , involving the closed frequent itemset algorithm LCM2 [ 11 ] , kindly provided by Takeaki Uno . Reported results are wall clock runtimes , including data loading and preprocessing .
3
Real datasets :
The runtimes obtained for various frequency thresholds for both DRYADEPARENT and CMTreeMiner are displayed on Figure 4 , for the widelyknown CSLOGS [ 12 ] and NASA [ 5 , 6 ] tree datasets .
Nasa/Multicast
DryadeParent CMTreeMiner
90 80 70 60 50 40 30 20 10 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
) s ( e m T i
Support
CSLOGS
DryadeParent CMTreeMiner
12
10
8
) s ( 6 e m T i
4
2
1
0 0 0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04
Support
Figure 4 : Running time wrt support for the NASA and CSLOGS datasets .
DRYADEPARENT is more than twice faster than CMTreeMiner on the CSLOGS dataset . For the NASA dataset the performances are similar , DRYADEPARENT having an advantage for the lowest support values . We discovered that the patterns found in both datasets were very different : NASA contains deep patterns with low branching factor , whereas CSLOGS contains shallow patterns with an higher branching factor . Arti.cial datasets will be used to get a deeper understanding of the infiuence of the structure of the patterns on compute time performance of the two algorithms .
Arti.cial datasets : In the usual tree mining algorithms studies , at most the length ( ie the number of nodes ) of the found patterns is reported , without any information about the structure of these patterns . However , branching factor and depth of the patterns intervene directly in the candidate generation process , so they are likely to play a major role wrt the computation time . To ascertain this hypothesis , we wrote a random tree generator that can generate trees with a given node number N and a given average branching factor b . Nodes are labeled with their pre order identi.er , so there are no couples of nodes with the same label in a tree . We generated trees with N = 100 nodes and b 2 [ 1:0 ; 5:0 ] , b increasing by increment of 01 For each value of b we generated 10,000 random trees and regrouped them by their depth d , and got a point ( b ; d ) by averaging the processing times for all the trees of average branching factor b and depth d . Figure 5a shows the logarithms of these averaged time values wrt the average branching factor b , and Figure 5b shows the logarithms of these averaged time values wrt the depth d .
The Figure 5a shows that DRYADEPARENT is orders of magnitudes faster than CMTreeMiner as long as the branching factor exceeds 1.3 , that is the case in most of the experiments space . For lower branching factor values , CMTreeMiner has a small advantage . Patterns with such a low branching factor necessarily have a high depth , this is con.rmed by Figure 5b . This .gure shows that DRYADEPARENT exhibits a linear dependency on the depth of the patterns . This is
) e m i t ( g o L
100 nodes trees
1
2
4
3
DryadeParent CMTreeMiner
   
0
−1
−2
−3
4
4.5
1
1.5
2
2.5
3
3.5
Average branching factor
4
3
2
1
0
−1
−2
−3
) e m i t ( g o L
5
( a ) Log(time)/average branching factor
100 nodes trees
DryadeParent CMTreeMiner
Acknowledgments : We wish to thank especially Takeaki Uno for the LCM2 implementation , and Yun Chi for making available the CMTreeMiner implementation and giving us the Nasa dataset . This work is partly supported by the grant in aid of scienti.c research No . 16 04734 .
0
10
20
30
40
50
Depth
60
70
80
90
100
( b ) Log(time)/depth
References
Figure 5 : Random trees with 100 nodes not surprising : each iteration of DRYADEPARENT computes one more depth level of the patterns , so very deep patterns will need more iterations . CMTreeMiner , on the other hand , shows a dependency on the average branching factor , but for a given value of b the computation time varies greatly , being especially high for low depth values . Because of the constraints on the random tree generator , a tree that have a low depth with a high average branching factor will necessarily have some nodes with a very large branching factor . We plotted in Figure 6 a new curve , showing the computation time with respect to the maximal branching factor .
4
100 nodes trees
DryadeParent CMTreeMiner
) e m i t ( g o L
3
1
2
0
−1
−2
−3
0
10
20
30 50 Maximal branching factor
40
60
70
80
Figure 6 : Random trees with 100 nodes , log(time ) wrt maximal branching factor
DRYADEPARENT is nearly unaffected by the maximal branching factor , but the computation time of CMTreeMiner depends strongly on this parameter .
5 . Conclusion and perspectives
In this paper , we have presented the DRYADEPARENT algorithm , based on the computation of tiles in the data , and on an ef.cient hooking strategy that reconstructs the patterns from these tiles . Thorough experiments have shown that DRYADEPARENT is faster than CMTreeMiner in most settings , and that its performances are robust wrt the structure of the patterns to nd We have proposed new benchmarks taking into account the structure of the patterns to test the behavior of tree mining algorithms . As far as we know , such kind of tests are new in the tree mining community . Improving these benchmarks and making more detailed analyzes is one of our future research directions . We also plan to extend DRYADEPARENT to structures more general than attribute trees .
4
[ 1 ] R . Agrawal and R . Srikant . Fast algorithms for mining association rules . In Proceedings of the 20th VLDB Conference , Santiago , Chile , 1994 .
[ 2 ] H . Arimura and T . Uno . An output polynomial time algoIn 15th rithm for mining frequent closed attribute trees . International Conference on Inductive Logic Programming ( ILP’05 ) , 2005 .
[ 3 ] T . Asai , K . Abe , S . Kawasoe , H . Arimura , H . Sakamoto , and S . Arikawa . Ef.cient substructure discovery from large semi structured data . In In Proc . of the Second SIAM International Conference on Data Mining ( SDM2002 ) , Arlington , VA , pages 158(cid:150)174 , Avril 2002 .
[ 4 ] T . Asai , H . Arimura , T . Uno , and S . ichi Nakano . Discovering frequent substructures in large unordered trees . In the Proc . of the 6th International Conference on Discovery Science ( DS’03 ) , pages 47(cid:150)61 , 2003 .
[ 5 ] R . Chalmers and K . Almeroth . Modeling the branching characteristics and ef.ciency gains of global multicast trees . In Proceedings of the IEEE INFOCOM’2001 , April 2001 .
[ 6 ] Y . Chi , Y . Yang , Y . Xia , and R . R . Muntz . Cmtreeminer : Mining both closed and maximal frequent subtrees . In The Eighth Paci.c Asia Conference on Knowledge Discovery and Data Mining ( PAKDD’04 ) , 2004 .
[ 7 ] S . Nijssen and J . N . Kok . Ef.cient discovery of frequent unordered trees . In First International Workshop on Mining Graphs , Trees and Sequences , 2003 , 2003 .
[ 8 ] A . Termier . Extraction of frequent trees in an heterogeneous corpus of semi structured data : application to xml documents mining . Technical Report 1388 , LRI , May 2004 . http://wwwlrifr/termier/publis/phdTermierENpsgz
[ 9 ] A . Termier , M . Rousset , and M . Sebag . Dryade : a new approach for discovering closed frequent trees in heterogeneous tree databases . In International Conference on Data Mining ICDM’04 , Brighton , England , pages 543(cid:150)546 , 2004 .
[ 10 ] A . Termier , M . Rousset , M . Sebag , K . Ohara , T . Washio , and H . Motoda . Computation time ef.cient and robust attribute In Third International tree mining with DRYADEPARENT . Workshop on Mining Graphs , Trees and Sequences ( MGTS ) , 2005 .
[ 11 ] T . Uno , M . Kiyomi , and H . Arimura . Lcm v.2 : Ef.cient mining algorithms for frequent/closed/maximal itemsets . In 2nd Workshop on Frequent Itemset Mining Implementations ( FIMI’04 ) , 2004 .
[ 12 ] M . J . Zaki . Ef.ciently mining frequent trees in a forest . In In Proc . 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , Juillet 2002 .
[ 13 ] M . J . Zaki . Ef.ciently mining frequent embedded unordered trees . Fundamenta Informaticae , special issue on Advances in Mining Graphs , Trees and Sequences , 65(1 2):33(cid:150)52 , March/April 2005 .
