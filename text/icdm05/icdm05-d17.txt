A Preference Model for Structured Supervised Learning Tasks
Fabio Aiolli
Dip . di Matematica Pura e Applicata , Universit`a di Padova ,
Via G . Belzoni 7 , 35131 Padova , Italy , aiolli@mathunipdit
Abstract
The preference learning model introduced in this paper gives a natural framework and a large margin principled solution for a broad class of supervised learning problems with structured predictions , including ranking based predictions ( label and instance ranking ) , hierarchical classification , and ordinal regression . We show how all these problems can be cast as linear problems in an augmented space , and we propose a stochastic gradient method to efficiently solve them . Experiments performed on an ordinal regression task confirm the generality and the effectiveness of the approach .
Keywords : Supervised Learning , Ranking , Ordinal Re gression , Preferences .
1 Introduction
Supervised learning deals with algorithms that give machines the ability to learn from experience . Many real world learning problems are characterized by heterogeneous tasks which currently cannot be solved by general purpose algorithms . These include ranking based problems ( either label or instance ranking ) and ordinal regression . The typical approach followed to cope with these complex problems is to map them into a series of simpler , well known settings and then to combine the resulting predictions . Often , these solutions lack a principled theory and/or require too much computational resources to be practical for data mining applications .
Although some efforts have been recently made to generalize label ranking tasks [ 7 , 5 , 2 ] , a general framework and a theory encompassing all these supervised learning settings is missing . In this paper we propose a quite detailed taxonomy of supervised learning problems , based on the different type of predictions and the supervision involved . Then , we show how supervision can be seen as a set of order preferences over the predictions of the learner . Finally , we show how all these problems can be seen as linear binary problems defined on an augmented space , thus suggesting very simple optimization procedures available for the binary case .
Another contribution of this paper is to define a preference model which is very flexible and allows a user to optimize the parameters on the basis of a proper evaluation function . Often , while the goal of a problem in terms of its evaluation function is clear , a crucial thing in the design of learning algorithms is how to define them in such a way to have some theoretical guarantee that a learning procedure leads to the effective minimization of that particular cost function . The model introduced in this paper gives a natural and uniform way to encode the cost function of a supervised learning problem and plug it into a learning algorithm .
In Section 2 , starting from a definition of a detailed taxonomy of supervised learning tasks , the proposed learning model is presented . Examples of instantiations of the model to supervised learning problems is presented in Section 3 . In Section 4 , we propose principled batch and efficient online optimization procedures for training the model . Finally , in Section 5 the experimental results are presented .
2 A Model for Supervised Learning
In supervised learning , we assume supervision is provided according to an unknown probability distribution D . Generally , it consists of pairs , example and corresponding correct prediction . For reasons that will be clearer in the following , we prefer to consider supervision as ( soft ) constraints over the learner predictions , that is constraints whose violation entails a cost for the solution . Specifically , assuming a learner makes its predictions on the basis of a set of parameters Θ , characterizing its hypothesis space , supervision S makes the learner suffering a cost c(S|Θ ) . This subsumes the case of supervision as pairs previously pointed out . In fact , this is obtained when a unitary cost is given to hypotheses generating an incorrect labeling .
1
Two main settings of learning can be identified . In the on line paradigm , learning takes place in rounds . At each step the learner receives supervision and updates its parameters with the aim to minimize future costs . In batch learning a training set S = {S1 , . . . , Sn} is available where the Si are supposed to be drawn iid from D and a single training session is made with the explicit goal to minimize the expected cost on the true distribution D .
Different learning problems are often characterized by different types of prediction and supervision . Nevertheless , we will show that a broad set of them can be studied in a common framework , whose general setting is as follows . We consider a space X of instances and a space Y of labels ( both sets possibly infinite ) . Moreover , we assume the hypothesis space , based on which the learner makes its predictions , to consist of relevance functions u : X × Y → R , depending on some set of parameters Θ . The goal of the learner is then to select a function ˆu from its hypothesis space , which is ” consistent ” with the supervision in a sense that will depend on the particular setting .
2.1 Prediction and Supervision
In this section , we present a detailed taxonomy of the main supervised learning tasks organized on the basis of the required predictions and supervision . To this end , we first need to define order relations .
A partial order is a pair ( P,(cid:186 ) ) where P is a set and ( cid:186 ) is a reflexive , antisymmetric and transitive binary relation . A partial ranking of length r is a partial order where the set P can be partitioned in r sets P1 , . . . ,Pr such that z ∈ Pi , z ∈ Pj , i < j , implies z ( cid:186 ) z and no further information is conveyed about the ordering within subsets Pk . A full order on P is defined as a partial ranking of length |P| . We denote by P O(P ) , P R(P ) , and F O(P ) the set of partial orders , partial rankings and full orders over the set P , respectively .
211 Label Rankings
A first important family of supervised learning tasks is related to the ordering of the classes on the basis of their relevance for an instance . This family of problems is referred to as label rankings . Problems in this family take supervision in the form of general partial orders over the classes . In our notation , given x ∈ X , Y ⊆ Y , we have S ∈ P O(Y ) and predictions are in F O(Y ) . A few well known instances are listed in the following :
Category Ranking ( CR ) In this setting , the goal is to order categories on the basis of their relevance for an instance . As an example , in a collaborative filtering setting , users could correspond to our instances and the different movies to our classes . Then , one could be interested into the ordering ( by relevance ) of the set of movies based on user preferences . This is trivially a particular case of label ranking where supervision is given as full orders over Y .
Bipartite Category Ranking ( BCR ) In this task , supervision is given as two groups of classes and it is required to predict full orders in which the first group of classes is ranked over the second . As a leading example , in information retrieval , given a document , one might have to rank the available topics with the aim to return the most relevant topics on the top of the list . This is again a specific case of label ranking where supervision is given as partial rankings of length two . This task has been also referred to as category ranking in literature [ 4 ] . Here a different terminology is adopted to avoid confusion between these two different tasks.1
Sometimes , we are also interested in predictions consisting of the most relevant classes , that is , of a prefix of the full order induced by the relevance function u(x , y ) . This family of tasks is commonly referred to as classification problems . They can however be considered as subcases of the BCR ranking task . A few examples of this kind of problems , listed by increasing specificity , is given here : q label classification ( QC ) In this task , the goal is to select the q most appropriated classes for a given instance , with q fixed . The supervision here is a partial ranking of length two where a set of exactly q labels are preferred over the rest .
Single label classification ( SC ) In this well known classification task , the goal is to select exactly one class ( the most relevant ) for an instance . This is a trivial subcase of QC with q = 1 .
212 Instance Rankings
Another interesting family of tasks is instance rankings where the goal is to order instances on the basis of the relevance of a given class . In our notation , given y ∈ Y , X ⊆ X , prediction is in F O(X ) and supervision is given in the form S ∈ P O(X ) .
The duality with respect to label rankings is self evident . In principle , a corresponding problem setting could be defined for each of the label ranking settings . We can easily
1Note that this task and the two that follow , are conceptually different from the task to decide about the membership of an instance . Here , supervision only gives qualitative information about the fact that some classes are more relevant than others .
2 see that the well known task , commonly known as ( Bipartite ) Instance Ranking ( IR ) , corresponds to BCR and is the one to induce an order such that a given set of instances is top ranked . A natural application of this kind of prediction is in information retrieval , eg when listing the results returned by a search engine . Similarly to BCR , here supervision consists of partial rankings ( this time over the set X ) of length two . Another interesting task which can be considered in this family is the one to learn preference relations from a given set of ranked instances . For example , an information retrieval task is that to learn the preference relations on the basis of basic preferences given as pairs of documents [ 8 ] .
The two families of tasks above can be considered qualitative tasks since they are concerned with order relations between instance class pairs . On the other side , quantitative tasks are the ones which are more concerned with the absolute values of the relevance of instance class pairs .
213 Quantitative Predictions
Sometimes there is the necessity to do quantitative predictions about data at hand . For example , in binary classification , one has to decide about the membership of an instance to a class as opposed to rank instances by relevance . These settings are not directly subsumed by the settings presented above . As we will see this can be overcome by adding a set of thresholds and doing predictions based on these thresholds .
Multivariate Ordinal Regression ( MOR ) There are many settings where it is natural to rank instances according to an ordinal scale , including collaborative filtering , where there is the need to predict people ratings on unseen items . Borrowing the movie related application introduced above , suitable ranks for movies could be given as ’bad’ , ’fair’ , ’good’ , and ’recommended’ . With no loss in generality , we can consider the target space as the integer set Z = {0 , . . . , R − 1} of R available ranks . Following an approach similar to the one in [ 10 ] , ranks are made corresponding to intervals of the real line . Specifically , a set of thresholds T = {τ0 = −∞ , τ1 , . . . , τR−1 , τR = +∞} is defined and the prediction is based on the rule
ˆz = {i : u(x , y ) ∈ ( τi−1 , τi)} .
Given the target label z , a correct prediction will be consistent with the conditions : u(x , y ) > τi when i < z and u(x , y ) < τi when i ≥ z .
The well known ( Univariate ) Ordinal Regression(OR ) [ 9 , 12 ] task is a trivial subcase of MOR when a single class is available .
Multi label Classification ( MLC ) In this task , it is required to classify instances with a subset ( the cardinality of which is not specified ) of the available classes . For us , it is convenient to consider this task as a MOR problem where only two ranks are available , relevant and irrelevant , and Z = {0 , 1} . The well known Binary Classification ( BC ) can be considered a subcase of OR with two ranks Z = {0 , 1} . Note that this task is considered here conceptually different from SC with two classes .
Clearly , the taxonomy presented above is not exhaustive but highlights how many different kinds of structured supervision can be seen as simple constraints over the predictions of a learner . Specifically , they consist of constraints in conjunctive form ( here referred to as preference sets , or p sets ) where each basic preference is defined over the scoring values and/or some threshold value .
In particular , we can differentiate between two types of order preferences : qualitative preferences in the form
( u(xi , yr ) , u(xj , ys ) ) telling that the value of u(xi , yr ) should be higher than the value of u(xj , ys ) , and quantitative preferences in the form
( u(x , y ) , τ ) or ( τ , u(x , y) ) , τ ∈ R relating the value of u(x , y ) to a given threshold τ .
In Table 1 , a summary of supervision obtained for the most general settings are presented . Particular instantiations to more specific problems are immediate anyway .
Setting
LR IR
MOR
Supervision P sets
{(u(x , yr ) , u(x , ys))}(x,yr)(cid:186)S ( x,ys ) {(u(xi , y ) , u(xj , y))}(xi,y)(cid:186)S ( xj ,y ) {(u(x , y ) , τi)}i<z ∪ {(τi , u(x , y))}i≥z
Table 1 . Supervision of problems in Section 21 Label and instance rankings ( LR and IR respectively ) , have a preference for each order relation induced by the supervision S . In ordinal regression ( MOR ) , a preference is associated to each threshold and z ∈ Z is the rank given by the supervision .
2.2 A Model for the Learner
In the following , we will focus on a particular form of the relevance function , that is u(x , y ) = w · φ(x , y )
3 where φ(x , y ) ∈ Rd is a joint representation of instanceclass pairs and w ∈ Rd is a weight vector [ 11 ] . Note that this form encompasses the more standard form u(x , y ) = wy · φ(x ) which has a weight vector for each different label . In fact , if |Y| = m , we can write : and w = ( w1 , . . . , wm ) y−1
φ(x , y ) = ( 0 , . . . , 0
, φ(x ) , 0 , . . . , 0 ) .
With this assumption , it is possible to conveniently reformulate an order constraint as a linear constraint . Let T = {τ1 , . . . , τR−1} be the available thresholds , in the qualitative case , given a ≡ ( u(xi , yr ) , u(xj , ys) ) , we obtain u(xi , yr ) > u(xj , ys ) ⇔
( w , τ1 , . . . , τR−1 ) · ( φ(xi , yr ) − φ(xj , ys ) , 0 , . . . , 0 ) R−1
> 0
Viceversa , in the quantitative case , given δ ∈ {−1 , +1} ,
ψ(a ) we have
δ(u(x , y ) − τr ) > 0 ⇔ ( w , τ1 , . . . , τR−1 ) · ( δφ(x , y ) , 0 , . . . , 0 r−1
,−δ , 0 , . . . , 0 R−r−1
)
> 0 .
ψ(a )
In general , we can see that supervision constraints of all the problems discussed above , can be reduced into sets of particular linear preferences of the form w·ψ(a ) > 0 where w = ( w , τ1 , . . . , τR−1 ) is the vector of weights augmented with the set of available thresholds and ψ(a ) is an opportune representation of the preference under consideration .
The quantity
ρA(a|w ) = w · ψ(a ) will be also referred to as the margin of the hypothesis wrt the preference . Note that this value is greater than zero when the preference is satisfied and less than zero otherwise . We will say that a preference a is consistent with an hypothesis when ρA(a|w ) > 0 ( and we write a ( cid:64 ) w ) . The margin of an hypothesis wrt the whole supervision S , can be consequently defined as the minimum of the margins of preferences in g[S ] , ie
ρ(g[S ] ) = min a∈g[S ]
ρA(a ) .
This definition turns out to be consistent with definitions of the margin commonly used in different problems . In particular , the margin is positive if and only if the prediction is consistent with the supervision .
Summarizing , all the problems defined in the taxonomy in Section 2.1 can be seen as an homogeneous linear binary problem in a opportune augmented space . Specifically , any algorithm for linear classification ( eg perceptron or linear programming ) can be used to solve it , provided the problem has a solution .
2.3 Evaluation and GPLM
The mere consistency of supervision constraints is not necessarily the ultimate goal of a supervised learning setting . Rather , cost functions are often preferred measuring the disagreement between the current hypothesis and the supervision . These functions may either depend on the particular structure of the prediction or other factors .
In [ 2 ] a general model for label rankings has been proposed . Here , we extend the same idea to general supervised settings by mapping supervision into sets of preferences with costs . We will refer to this method as Generalized Preference Learning Model ( or simply GPLM ) .
Definition 2.1 Preference Sets w/ Costs A ( conjunctive ) preference set with costs , or simply ” cp set ” , is a p set where preferences have costs associated . Preferences of a cp set will be denoted by aγ(a ) . When the cost is not indicated γ(a ) = 1 will be considered .
With this definition in mind , given a cp set g , an hypothesis suffers a cost which is defined as the maximum among the costs of its unfulfilled preferences , ie c(g|w ) = max a∈g,aw
γ(a ) .
( 1 )
In GPLM , we consider supervision S as a p set g[S ] and we consider a cost mapping
G : g[S ] → {g1(S ) , . . . , gqS ( S)} where each cp set gi(S ) is a subset of g[S ] with some costs assigned to the preferences . Once the cost mapping G is fixed , the total cost suffered by an hypothesis w for the supervision S is defined as the cumulative cost of cp sets , ie qS c(g[S]|w ) = c(gj(S)|w ) .
( 2 ) j=1
Let gp be a p set , natural mappings already proposed in [ 5 ] for preference graphs can be easily adapted to our setting . This is made by considering classes of equivalence among preferences and by defining mappings in which a different cp set is built for each partition . Specifically , let a ≡ ( as , ae ) and a ≡ ( a e ) denote a pair of preferences , we have the following : s , a
4
( i ) the identity mapping , denoted by GI , where gp is mapped on a single cp set gc . This corresponds to define the trivial equivalence relation ( as , ae ) ≡ ( a s , a e ) ;
( ii ) the domination mapping , denoted by GD , where gp is split into a set of cp sets on the basis of the equivalence relation ( as , ae ) ≡ ( a e ) ⇔ as = a s ; s , a
( ii ) the dominated mapping , denoted by Gdom , where gp is split into a set of cp sets on the basis of the equivalence relation ( as , ae ) ≡ ( a e ) ⇔ ae = a e ; s , a
( iv ) the disagreement mapping , denoted by Gd , where gp is split into a set of cp sets on the basis of equivalence relations ( as , ae ) ≡ ( a s ∧ ae = a e . e ) ⇔ as = a s , a
3 Examples of GPLM Cost Mappings
In this section , a set of examples of supervised learning problems and suitable GPLM cost mappings are discussed . In particular , we show how general the models is and how many common cost functions can be defined with the tools offered by our model . the same frequencies in the training and the test set . Another interesting case is when there is some structure between the available classes and a different metric for misclassification costs is introduced . For example , in hierarchical classification , it makes sense to pay costs proportional to the path length in the tree between the true class and the predicted one . In all these cases , a cost matrix ∆ is used to have a better control over the learning algorithm , where the element ∆(yr , ys ) represents the cost of classifying a pattern as yr when it is actually in ys . In our model , the same can be easily obtained by associating costs to cp sets of the GPLM mapping .
3.2 Cost functions for Instance Rankings
A common loss function used in IR is the so called AUC ( Area under ROC curve ) measure . It can be shown that it directly derives using the cost mapping Gd . Interestingly , our model suggests new possible settings and loss definitions one might use for the tasks in the family of instance rankings .
3.1 Cost functions for Label Rankings .
3.3 Cost functions for Ratings
Basic mappings for rankings and classification can be found in [ 2 ] and can be reproduced with the model proposed in this paper . In fact , it can be shown quite easily that , for label rankings , PLM preference graphs and GPLM cp sets with unitary costs are equivalent .
Applying these simple mappings we are able to reproduce many of the different losses used for ranking problems . For example , the cost mapping GD seems particularly suitable for q label classification since it gives a ’s oft’ indication of how many relevant labels are wrongly classified as irrelevant . On the other side , the GI mappings gives a cost function which returns a binary value indicating if any of the relevant labels are wrongly classified as irrelevant . The multiclass loss commonly used for single label classification can be obtained using the GI mapping . Note that , using the Gd mapping would have lead to a cost function which returns the number of incorrect classes which have a relevance higher than the relevance of the correct class . Another example is the so called ranking loss that has been proposed in [ 4 ] for the binary category ranking problem . This cost function corresponds to the number of pairs which are not correctly ordered and corresponds to the cost mapping Gd .
However , the extension presented here introduces far more flexibility on the choice of the cost function for label rankings because of the use of cp sets in place of preference graphs .
A typical example is classification where misclassifications can have different costs . This can be the case in singlelabel classification when categories are not represented with
A brief review of standard loss functions used for rating tasks and the implementation in our model is now presented .
Ordinal Regression Recalling the natural definition of cost for ordinal regression problems , ie c = |ˆz(x)− z(x)| , where ˆz(x ) is the rank given as output by the hypothesis and z(x ) the correct rank , we would like to define a cost mapping for GPLM consistent with the same cost function . At least two different cost mappings have this property . The easiest one is the mapping Gd . In this case , the resulting cost will be the number of thresholds which are not correctly ordered wrt u(x , y ) . This is exactly the cost as given before . A second possibility is to define a mapping GI followed by an assignment of costs where the r th preference is set to ( u(x , y ) , τr)z−i+r whenever r ≤ z , and ( τr , u(x , y))r−z otherwise .
As an example of this second situation , consider a R = 4 univariate ordinal regression problem . Then , we have three thresholds T = {τ1 , τ2 , τ3} and cost mappings defined as in the following :
G(g[0 ] ) = {(τ1 , u(x , y))1 , ( τ2 , u(x , y))2 , ( τ3 , u(x , y))3} G(g[1 ] ) = {(u(x , y ) , τ1)1 , ( τ2 , u(x , y))1 , ( τ3 , u(x , y))2} G(g[2 ] ) = {(u(x , y ) , τ1)2 , ( u(x , y ) , τ2)1 , ( τ3 , u(x , y))1} G(g[3 ] ) = {(u(x , y ) , τ1)3 , ( u(x , y ) , τ2)2 , ( u(x , y ) , τ3)1}
It is easy to verify that this mapping respects the costs as they could be obtained by the natural cost definition given
5 above . For example , considering the instance x with target rank 1 being ranked 3 . Then , it means that the scoring function is such that u(x , y|Θ ) ∈ ( τ3 , +∞ ) , ie
−∞ ≤ τ1 ≤ τ2 ≤ τ3 ≤ u(x , y|Θ ) ≤ +∞ , and hence the cost suffered by the hypothesis is correctly computed by c(g[1]|Θ ) = max{0 , +1 , +2} = +2 .
Binary Classification The natural cost function for BC problems is trivially obtained by using GI , ie by setting ( u(x , y ) , τ ) when x is a positive example for the class , and ( τ , u(x , y ) ) otherwise .
Very similar examples , omitted for space reasons , can be given for the multi variate versions of ratings problems .
4 Learning in GPLM
In earlier sections we have discussed the structure behind the supervision and how it can be modelled using cp sets . Now , we see how to give learning algorithms for the batch and the on line settings . In GPLM we propose to minimize costs c(S|w ) . Since these are not continuous wrt w , we approximate them by introducing a continuous non increasing function l : R → R+ approximating the indicator function . Then , we define the approximate cost ˜c(S|w ) =
γ(a)l(ρA(a|w) ) . max a∈g g∈G(g[S ] )
Examples of losses one can use are presented in Table 2 .
Methods Perceptron β margin Exponential Sigmoidal l(ρ ) max(0,−ρ ) max(0 , β − ρ ) e−ρ ( 1 + eλ(ρ−θ))−1
Table 2 . Approximation losses as a function of the margin . β > 0 , λ > 0 , θ ∈ R are external parameters .
Although D is unknown , we can still try to minimize this function by exploiting the same structure of supervision and as much of the information we can gather from the training set . The general problem can be given as in the following :
• Given a set V(S ) = • Find a set of parameters w in such a way to minimize
S∈S g[S ] of cp sets the functional
Q(w ) = L(V(S)|w ) + µR(w )
( 3 ) where L(V(S)|w ) = S∈S ˜c(S|w ) is related to the empirical cost and R(w ) is a regularization term over the set of parameters . Note that , for the solution to be admissible when multiple thresholds are used and there are constraints defined over their values ( as in the ordinal regression settings ) , these constraints should be explicitly enforced .
The use of a regularization term on a problem of this type has many different motivations , including the theory on regularization networks ( see eg [ 6] ) . However , given the huge amount of data available in many data mining applications , this term can usually be disregarded without affecting the performance .
Moreover , we can see that by choosing a convex loss function and a convex regularization term ( let say the quadratic term R(w ) = 1 2||w||2 ) it warranties the convexity of the functional Q(w ) in Eq 3 and then the uniqueness of the solution . Indeed , current kernel based approaches defined for basic supervised learning tasks can be seen in this form when using the β margin with β = 1 . This suggests a new universal kernel method which is able to solve many complex learning tasks [ 1 ] .
Given the large amount of examples in data mining applications , a drawback of this learning setting is the onerous computational requirements . In the following section , a principled stochastic approximation is presented aiming at minimizing the same functional efficiently .
4.2 Stochastic On line Learning for GPLM
As already pointed out , in on line learning , supervision becomes available one by one and each time the learner updates the hypothesis to minimize future costs . A suitable measure of performance after m rounds is the cumulative cost function m
4.1 Batch Learning for GPLM
Rm t [ w ] = c(g[Si]|wi )
The goal in batch learning is to find the parameters w such to minimize the expected cost over D , the actual distribution ruling the supervision feed , which is defined by
Rt[w ] = ES∼D[c(g[S]|w) ] . i=1 where wi is the hypothesis obtained after seeing supervision S1 , . . . , Si−1 .
Following a typical approach for on line learning , we propose to perform a stochastic gradient descent [ 13 ] with
6 respect to the instantaneous cost Q(wt ) = ˜c(St|wt)+µR(wt ) . Then , assuming R(w ) = 1 form wt = wt − λQ w and
2||w||2 , the update will be in the g∈G(g[St ] ) γ(ˆa[g])l g∈G(g[St ] ) γ(ˆa[g])l
Q w = w + = w +
ρ(ρ(ˆa[g]))ρ ρ(ρ(ˆa[g]))ψ(ˆa[g ] ) w(ˆa(g ) ) where ˆa[g ] = arg maxa∈g γ(a)l(ρ(a ) ) and f x(v ) stands for the gradient of f wrt the parameters x evaluated in v . It can be easily shown that this update rule makes the weight vector w taking the ( sparse ) form : i,r w = i φ(xi , yr ) αr i ∈ R , thus obtaining an ( sparse ) implicit represen where αr tation of the relevance function as : u(x , y ) = i φ(xi , yr)φ(x , y ) . αr i,r
As in the batch setting , here we have the problem to enforce the hard constraints defined over the thresholds . However , this is a far less stringent issue in a stochastic method . In our implementation , this is made by projecting the values of the thresholds back into the constraint after each iteration .
5 Experiments
To demonstrate the flexibility and validate the general model proposed in this paper , we performed a set of experiments on a synthetic dataset . The explicit purpose was the one to try different cost mappings and loss functions in a relatively self contained task in such a way to have a better control and to do fair comparisons between different configurations . In particular , we have considered an ordinal regression problem ( |Y| = 1 ) in the online paradigm . Since |Y| = 1 , in this case we have that the relevant function is u(x ) = w·φ(x ) . Moreover , since we dealt with kernels , the implicit t K(xt , x ) is actually used . representation u(x ) = Finally , no regularization has been performed , ie µ = 0 . t,r αr
5.1 Experimental Setting and Results
The experimental setting is the same used in [ 3 ] . The dataset is synthetic . Points x = ( x1 , x2 ) are uniformly distributed in the unit square [ 0 , 1]2 . The ranks are then assigned basing on the following rule : r ∈ {0 , . . . , 4} : 10(x1 − 0.5)(x2 − 0.5 ) + ∈ ( br , br+1 ) where b = {b0 , . . . , b5} = {−∞,−1,−0.1 , 0.25 , 1 , +∞} and is a normally distributed noise ∼ N(0 , σ ) . We
7 generated 100 sequences of 100,000 examples each . Moreover , a non homogeneous second order polynomial kernel K(x1 , x2 ) = φ(x1)· φ(x2 ) = ( x1 · x2 + 1)2 has been used . m The performance on a sequence is obtained by feeding all the instances of the sequence and computing the cumulative t=1 |ˆrt − rt| . Finally , cost at each iteration m as cm = the obtained costs are averaged over the 100 sequences to obtain higher statistical significance .
Experiments have been performed using configurations produced according to three dimensions :
• Cost Mapping : Three cost mappings have been used . Two of them are the ones presented in Section 3.3 , ie the mapping GI with costs ( denoted Gc I ) and the mapping Gd . The last mapping is basically the mapping GI where the cost assignment is not performed . Note that , this mapping represents the cost function which gives a unitary cost for uncorrect predicted ranks .
• Complexity of the task : Different values of the standard deviation σ ∈ {0 , 0.125 , 0.5 , 1.0} have been used . A greater σ leads to a more difficult task .
• Preference Loss : Two losses from the ones in Table the Perceptron loss , and the
2 have been used , ie sigmoidal loss with parameter λ = 1 , θ = −1 .
One may notice that the configuration ( Gd,· , PLoss ) is equivalent to the PRank algorithm proposed in [ 3 ] .
In Fig 1 , the curves of cost obtained for the three mappings and σ = 0.5 are shown . Different plots refer to the two preference losses . In Table 3 a detail of results after 10000 presentations is shown . Results show that the baseline cost mapping GI is consistently worse than the other two , while the performance of Gc I and Gd ar quite similar . Interestingly , a far larger improvement is obtained for the sigmoidal loss and this can be due to the better approximation of the true cost .
—– Perc . Loss —– Gd GI 0.317 0.369 0.452 0.502 1.148 1.057 1.620 1.661
Gc I 0.339 0.470 1.062 1.575
—– Sigm . Loss —– Gd GI 0.236 0.326 0.364 0.454 1.104 0.910 1.447 1.626
Gc I 0.259 0.384 0.944 1.474
σ
0.000 0.125 0.500 1.000
Table 3 . Costs for different methods and task complexities .
[ 3 ] K . Crammer and Y . Singer . Pranking with ranking . In Advances in Neural Information Processing Systems , 2001 .
[ 4 ] K . Crammer and Y . Singer . A new family of online algorithms for category ranking . Journal of Machine Learning Research , 2003 .
[ 5 ] O . Dekel , CD Manning , and Y . Singer . Log linear models for label ranking . In Advances in Neural Information Processing Systems , 2003 .
[ 6 ] T . Evgeniou , M . Pontil , and T . Poggio . Regularization networks and support vector machines . Advances in Computational Mathematics , 13:1–50 , 2000 .
[ 7 ] S . Har Peled , D . Roth , and D . Zimak . Constraint classification for multiclass classification and ranking . In Advances in Neural Information Processing Systems , 2002 .
[ 8 ] R . Herbrich , T . Graepel , P . Bollmann Sdorra , and K . Ober mayer . Learning a preference relation for information retrieval . In Proceedings of the AAAI Workshop Text Categorization and Machine Learning , 1998 .
[ 9 ] R . Herbrich , T . Graepel , and K . Obermayer . Large margin rank boundaries for ordinal regression . In Advances in Large Margin Classifiers , pages 115–132 , . MIT Press , 2000 .
[ 10 ] P . McCullagh and JA Nelder . Generalized Linear
Models . Chapman & Hall , 1983 .
[ 11 ] I . Tsochantaridis , T . Hofmann , T . Joachims , and Y . Altun . Support vector machine learning for interdependent and structured output spaces . In Proceedings of the Twenty first international conference on Machine learning , 2004 .
[ 12 ] Hong Wu , Hanqing Lu , and Songde Ma . A practical svm based algorithm for ordinal regression in image retrieval . In Proceedings of the eleventh ACM international conference on Multimedia , 2003 .
[ 13 ] T . Zhang . Solving large scale linear prediction problems using stochastic gradient descent algorithms . In Proceedings of the International Conference on Machine learning , 2004 .
( a )
( b )
Figure 1 . Curves of the cost obtained for σ = 0.5 with different cost mappings . ( a ) Perceptron loss , ( b ) Sigmoidal loss .
6 Conclusion
We have proposed a general preference model for supervised learning and its application to on line and batch algorithms . The model allows to codify cost functions as preferences and naturally plug them into the same training algorithm . In this view , the role of the cost functions here resembles the role of kernels in kernel machines . Furthermore , the proposed method gives a tool for comparing different methods and cost functions on a same learning problem . Experiments performed on an ordinal regression problem have confirmed the validity of the approach and highlighted the important role of the loss functions used for training .
References
[ 1 ] F . Aiolli .
Large Margin Multiclass Learning : Models and Algorithms . PhD thesis , Dept . of Computer Science , University of Pisa , 2004 . http://wwwdiunipiit/˜ aiolli/thesisps
[ 2 ] F . Aiolli and A . Sperduti . Preference learning for multiclass problems . In Advances in Neural Information Processing Systems . MIT Press , 2004 .
8
0.8 1 1.2 1.4 1.6 1.8 2 1000 5000 10000costsroundssd = 0.5 , Perc . LossIde w/o costsIde w/ costsDis 0.8 1 1.2 1.4 1.6 1.8 2 1000 5000 10000costsroundssd = 0.5 , Sigm . LossIde w/o costsIde w/ costsDis
