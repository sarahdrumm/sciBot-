Example Based Robust Outlier Detection in High Dimensional Datasets
Cui Zhu† ,
Hiroyuki Kitagawa† ††
† Graduate School of Systems and Information Engineering
†† Center for Computational Sciences
University of Tsukuba
Tennohdai , Tsukuba , Ibaraki 305 8573 , Japan zhucui , kitagawa @kdecstsukubaacjp
Christos Faloutsos
School of Computer Science Carnegie Mellon University
Wean Hall , 5000 Forbes Avenue
Pittsburgh , PA 15213 3891 christos@cscmuedu
Abstract
Detecting outliers is an important problem . Most of its applications typically possess high dimensional datasets . In high dimensional space , the data becomes sparse which implies that every object can be regarded as an outlier from the point of view of similarity . Furthermore , a fundamental issue is that the notion of which objects are outliers typically varies between users , problem domains or , even , datasets . In this paper , we present a novel robust solution which detects high dimensional outliers based on user examples and tolerates incorrect inputs . It studies the behavior of projections of such a few examples , to discover further objects that are outstanding in the projection where many examples are outlying . Our experiments on both real and synthetic datasets demonstrate the ability of the proposed method to detect outliers corresponding to the user examples .
1 Introduction
Detecting outliers is a critical task in applications such as fraud detection , financial analysis , health monitoring and so on . It is also a difficult problem especially when the data is in high dimensional spaces , which is often the case with its most applications .
It has been shown that by examining the behavior of the data in low dimensional projections , meaningful outliers are likely to be defined [ 1 ] . It is also observed in [ 1 ] that different objects may be detected as outliers with respect to different subsets of dimensions .
To detect outliers , a fundamental issue is that , the notion of what is an outlier varies among users , problem domains and even datasets ( problem instances ) . The issue intuitively leads to a result that different objects may be regarded as outliers , depending on different views from which we examine the datasets .
Being experts in their problem domain , not in outlier detection , users often have a few example outliers in hand , which may “ describe ” their intentions and they want to find more objects that exhibit “ outlier ness ” characteristics similar to those examples . Existing systems do not provide a direct way to incorporate such examples in the discovery process to find out the “ hidden ” outlier concept that users may have in mind .
A main problem is the lack of knowledge of the outliers . Users may provide example outliers which are NOT “ true ” outliers . Or else , all of the provided examples can NOT be treated as outliers from the SAME views of projections . Therefore , an solution of outlier detection based on examples should be robust to allow incorrect inputs .
In this paper , we propose a robust outlier detection method specially designed for high dimensional datasets that can discover the desired view of “ outlier ness ” , based on a small number of examples .
The remainder of the paper is organized as follows : In the next section , we discuss related work on outlier detection . In section 3 , we discuss the definition of “ outlier ness ” in a high dimensional dataset . Section 4 presents the novel method to detect outliers based on examples and its design decisions in detail . The experiment evaluation on both synthetic and real datasets are reported in Section 5 . Finally , section 6 concludes the paper .
2 Related work
In essence , outlier detection techniques traditionally employ unsupervised learning processes . Many clustering algorithms detect outliers as by products [ 4 ] . However , they are not optimized for outlier detection . Distribution based approaches in [ 11 ] and depth based approach [ 10 ] are well known to suffer from the curse of dimensionality . Recently more approaches , including distance based [ 5 ] , densitybased [ 8 ] and MDEF [ 9 ] algorithms , have been proposed
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 )
1550 4786/05 $20.00 © 2005 IEEE low dimensional subspace , where as many examples as possible are outstanding significantly than any other subspaces . Objects that are outlying in the same subspace are most likely to meet users on the point of what is an outlier .
Let the total dimensions of the data is d . In order to find a k dimensional subspace where examples are isolated from d the great majority , there are totally ( cid:166)d k possible combinations to be examined . It is exhausting or even untenable to discover a desired subspace by brute force method , even for middle d . k 2
We propose to solve the problem by employing an evolutionary algorithm [ 3 ] which is able to quickly find the hidden subspace in which examples stand out greatly .
4.1 The algorithm of outlier detection based on examples
In our context , a solution , ie a subspace , is represented by a bit string of length d . The fitness of a solution Pi is the quality of the associated subspace . Suppose , there are m m user examples E j j 1 . Let Si E j be the sparsity coefficient of the cube that contain the example E j in the subspace Pi . The fitness function , denoted as f Pi , is designed as follows : f Pi
0 Si E j
( cid:166)Si E j ( cid:166)m j 1 Si E j m
Si E j Si E j
0
0 j j
( 2 )
Thus , the fitness function is the negative average of all examples’ sparsity coefficient , if no one is a “ true ” outlier in the associated subspace . Here , a “ true ” example outlier is an object in a cube whose sparsity coefficient is negative in the associated subspace . As long as “ true ” outlier examples are discovered in a subspace , the fitness value becomes positive . More isolated “ true ” examples contribute to its fitness value . The evolutionary search method starts with a population of p random solutions . Then it seeks to optimize the overall fitness function in the population by an iterative processes of selection , crossover and mutation , until the population converge to a global optimum . The convergence goal of a population is achieved when all features in solutions have converged . Convergence of a feature is defined as the stage where 95 % of the population has the same feature value [ 2 ] . Finally , the best solution discovered by the evolutionary algorithm is exactly the subspace where most user examples stand out significantly . Then , a post processing phase scans the subspace to discover more outliers . The incorrect or “ false ” examples are reported as well . Figure 1 illustrates the overall procedure of the algorithm .
The evolutionary algorithm employs the following processes in order to create a new better population from the current generation . to detect outliers . The multi granularity deviation factor ( MDEF ) measures the “ outlier ness ” of an object in a neighborhood which is defined by distance . [ 12 ] proceeded to use MDEF to detect outliers based on user examples . Basically , these notions of an outlier , including distancebased , density based and MDEF , all employ full dimensional distances . When dimensionality is high and the data is sparse , the full dimensional distance measurement becomes no more meaningful [ 6 ] . Therefore , these measures of ” outlier ness ” are not appropriate to copy with high dimensionality .
A quite different technique is proposed in [ 1 ] to deal with the curse of high dimensionality . They find the most sparse low dimensional cubes in the data by a GA algorithm , and report all objects in these cubes as outliers . However , no notion of example was used in their approach .
In summary , none of existing methods detect outliers by incorporating directly user examples and are specifically designed for the high dimensional datasets as well .
3 Definition of outliers for high dimensional datasets
In the contexts like clustering and similarity search , many researches show that it is possible and reasonable to design more effective algorithms by studying the behavior of data in subspaces . This insight is suggested to be similar for outlier detection [ 1 ] . In [ 1 ] , “ outlier ness ” is measured by the sparsity coefficient shown below . Our approach employs their sparsity coefficient .
Each attribute of the data is divided into ( cid:77 ) equi depth 1 ( cid:77 ) of the ranges . In each range , there are a fraction f data . Then , a k dimensional cube is made by ranges from k different dimensions . Let N be the dataset size and n D denote the number of objects in a k dimensional cube D . Under the condition that attributes were independent statistically , the sparsity coefficient S D of the cube D is defined in [ 1 ] as :
S D n D N f k 1
N f k f k
( 1 )
The less objects reside in a k dimensional cube , the smaller the sparsity coefficient . For the same number of objects in a cube , lower dimensionality of the cube leads to smaller sparsity coefficient . 1
4 Proposed method
In this section , we discuss the robust algorithm to detect outliers based on examples in high dimensional datasets . Here , the problem is whether we can discover an extremely
1We have proved that S D is a monotonously increasing function of k .
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 )
1550 4786/05 $20.00 © 2005 IEEE
Input :
Set of outlier examples : E Number of equi depth ranges for each attribute : ( cid:77 )
Output :
Outliers : O “ False ” examples : FE
Algorithm : begin
P := Initial population of p solutions ; while not convergence criterion do begin
P := Selection(P ) ; P := Crossover(P ) ; P := Mutation(P ) ; end ;
S :=Solution in P with the best fitness value ; O :=Sets of objects contained in cubes of S which are sparser than or as sparse as cubes containing examples ;
FE :=Set of examples whose sparsity coefficient are positive in S ; return O , FE ; end
Figure 1 . Overall procedure of the examplebased outlier detection algorithm
1 . Selection . The idea of selection is that better solutions should have a larger number of copies . We use rank selection mechanism because it is often more stable .
2 . Crossover . We explain firstly the common scattered crossover mechanism and then show the optimized crossover method specially designed for our outlier detection problem .
Scattered crossover . The scattered mechanism starts by creating a random binary vector . Then it selects the genes where the vector is a 1 from the first parent , and the genes where the vector is a 0 from the second parent . The child is formed by combining the selected genes .
Optimized crossover . We call positions where parent strings have different values type II . It is obvious that the child string can only change within possible combinations of positions of type II . To be effective , the optimized crossover looks for a child string that has better fitness value .
The sparsity coefficient S D is a monotonously increasing function of k , which implies that a better solution is more likely to be found in lower dimensional subspaces . Therefore , the low dimensional recombinations are examined first .
The recombination with the lowest dimensionality is a solution whose values are 0 in all positions of type II . We examine such a solution first and keep increasing the di mensionality of the solution by adding one position of type II , which results in the solution with maximal fitness value , until a solution that is better than either parent solution is found . Finally , the better solution is returned as the child recombination . Such a crossover procedure creates a new solution which combines the good aspects of both parent strings .
3 . Mutation . A simple uniform mutation algorithm is used . It is a two step process . First , the algorithm picks a fraction of the positions of a solution for mutation , where each position has a probability prob of being mutated . In the second step , the algorithm inverses each selected position to form a child solution .
4.2 Post processing phase
After the evolutionary algorithm terminates , the best subspace in which most user examples are outlying greatly is found . In the post processing phase , we find all the objects contained in the cubes which are sparser than or as sparse as cubes containing “ true ” examples in the subspace . These objects are expected to be example liked outliers and are reported to users . Furthermore , users are advised of “ false ” examples which are not isolated in the identical subspace .
5 Experimental evaluation
In this section , we describe our experimental methodology and the performance of the proposed method over synthetic and real data .
1 . Test data . The synthetic data I has 20000 50dimensional data . Two sets of target outliers scatter in its 2D 1 and 2 and 3 dimensional subspaces , named as S 3D 2 respectively . The real abalone data is obtained S from the UCI machine learning repository [ 7 ] . It has 4177 examinations of abalones with 8 attributes . We test two kinds of abalone outliers . One is called A DW which is extracted in diameter whole weight subspace and the other , named as A SS , is in shucked weight shell weight subspace .
2 . Experimental procedure . Among the target outliers , we randomly sample x % 2 to serve as “ true ” examples and “ hide ” the rest . In addition , “ false ” examples are sampled from normals . The number of “ false ” examples is y % 3 of “ true ” examples . Then , the “ true ” and “ false ” examples are mixed together and are supposed to be provided by a user . We detect outliers using the proposed method , by learning only from such a few and noise holding examples . We mainly test two aspects of our method : one is how often
2In our experiments , x 3Also , y
10 .
10 .
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 )
1550 4786/05 $20.00 © 2005 IEEE
6 Conclusion
We proposed a robust algorithm that detects outliers based on user examples . The method is especially designed to deal with the curse of dimensionality and for tolerance of incorrect examples . Experiments demonstrated the ability of the method to find the best relevant subspaces indicated by user examples . Acknowledgements . This research has been supported in part by the Grant in Aid for Scientific Research from JSPS and MEXT ( #15300027 , #16016205 ) .
References
[ 1 ] Charu C . Aggarwal and Philip S . Yu . Outlier Detection for High Dimensional Data . In Proc . SIGMOD Conf . , 2001 .
[ 2 ] K . A . De Jong . Analysis of the Behavior of a Class of Genetic Adaptive Systems . Ph . D . Dissertation , University of Michigan , Ann Arbor , MI , 1975 .
[ 3 ] D . E . Goldberg . Genetic Algorithms in Search , Optimization and Machine Learning . Addison Wesley , Reading , MA , 1989 .
[ 4 ] A . K . Jain , M . N . Murty , and P . J . Flynn . Data Clusa Review . ACM Comp . Surveys , 31(3):264 tering : 323,1999 .
[ 5 ] E . M . Knorr and R . T . Ng . Algorithms for Mining Distance Based Outliers in Large Datasets . In Proc . VLDB , pp . 392 403 , 1998 .
[ 6 ] K . Beyer , J . Goldstein , R . Ramakrishnan , and U . Shaft . When is Nearest Neighbors Meaningful ? . In Proc . of the Int . Conf . Database Theories , pp . 217 235 , 1999 .
[ 7 ] http://wwwicsuciedu/˜ mlearn/MLRepository.html
[ 8 ] M . M . Breunig , H . P . Kriegel , R . T . Ng , and J . Sander . LOF : Identifying Density Based Local Outliers . In Proc . SIGMOD Conf . , pp . 93 104 , 2000 .
[ 9 ] S . Papadimitriou , H . Kitagawa , P . B . Gibbons , and C . Faloutsos . LOCI : Fast Outlier Detection Using the Local Correlation Integral . In Proc . ICDE , pp . 315 326 , 2003 .
[ 10 ] T . Johnson , I . Kwok , and R . T . Ng . Fast Computation of 2 Dimensional Depth Contours . In Proc . KDD , pp . 224 228 , 1998 .
[ 11 ] V . Barnett and T . Lewis . Outliers in Statistical Data .
John Wiley and Sons , 1994 .
[ 12 ] C . Zhu , H . Kitagawa , S . Papadimitriou , and C . Faloutsos . OBE : Outlier by Example . In Proc . PAKDD . pp . 222 234 , 2004 .
Table 1 . Performances of the scattered and optimized crossovers .
Test data
S 2D 1 S 3D 2 A DW A SS
Scattered
Optimized
Accuracy Time Accuracy Time 292 311 3 3
96 % 99 % 100 % 100 %
29 % 47 % 57 % 69 %
210 253 2 2
Table 2 . Time cost and accuracy versus number of dimensions .
Dimensionality Accuracy Total time ( second )
50 75 100
96 % 90 % 85 %
292 509 770 the evolutionary algorithm discovers the right relevant subspaces , and the other is the execution time .
3 . Results . The evolutionary algorithm is run on each test dataset 100 times with different sets of examples and initial populations . Statistical measurements of accuracy are obtained on success rate of the methods in discovering the right subspaces among 100 trials . Table 1 shows the results . It is obvious that the optimized crossover outperforms substantially in terms of the accuracy of finding the best solution . This is because the optimized crossover process always identifies better child solutions by combining the good aspects of both parent solutions . The time extension of the optimized crossover than that of the scattered crossover is within a reasonable and tolerant range .
4 . Effect of examples . By incorporating user examples directly in detecting process , our method discovers outliers which are more likely to meet user ’s interests . Based on a few A SS examples ( only 11 , including 2 “ false ” examples ) , for example , only A SS outliers are discovered in abalone data . Other methods which leave users outside the loop of detection do not have this merit .
5 .
Impact of dimensionality . We further examine the performance of the optimized crossover algorithm on various dimensionality ( as aforementioned , the optimized crossover performs better in our experiments ) . Results are average of 20 trials and are shown in Table 2 . Total time in Table 2 includes CPU and I/O taken to detect the best subspace . In Table 2 , the time is still within the bounds of tolerance .
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 )
1550 4786/05 $20.00 © 2005 IEEE
