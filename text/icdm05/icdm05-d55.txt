A Bernoulli Relational Model for Nonlinear Embedding
Gang Wang1 , Hui Zhao2 , Zhihua Zhang1 , Frederick H . Lochovsky1
1Department of Computer Science ,
Hong Kong University of Science and Technology , Hong Kong
2Automation Department , Xi’an Jiaotong University , China
Abstract
The notion of relations is extremely important in mathematics . In this paper , we use relations to describe the embedding problem and propose a novel stochastic relational model for nonlinear embedding . Given some relation among points in a high dimensional space , we start from preserving the same relation in a low embedded space and model the relation as probabilistic distributions over these two spaces , respectively . We illustrate that the stochastic neighbor embedding and the Gaussian process latent variable model can be derived from our relational model . Moreover we devise a new stochastic embedding model and refer to it as Bernoulli relational embedding ( BRE ) . BRE ’s ability in nonlinear dimensionality reduction is illustrated on a set of synthetic data and collections of bitmaps of handwritten digits and face images .
1 Introduction
Dimensionality reduction is an important issue in machine learning and pattern recognition . Principal component analysis ( PCA ) and multidimensional scaling ( MDS ) are two important linear techniques for it . Recently , nonlinear dimensionality reduction techniques have become increasingly popular for visualization and other applications . Most of the nonlinear dimensionality reduction techniques typically begin with an affinity matrix between pairwise instances or variates and are followed by an eigendecomposition process . Thus , they are also referred to as spectral embedding methods [ 3 , 4 ] . Along this line , Tenenbaum et al . [ 15 ] presented ISOMAP by using the geodesic distance on manifolds . Roweis and Saul [ 13 ] proposed locally linear embedding ( LLE ) by preserving local metric information in the original instances . Other representative work includes kernel based techniques , such as the kernel PCA [ 14 ] , Laplacian eigenmaps [ 1 ] and kernel eigenmaps [ 2 ] .
Recently , Hinton and Roweis [ 7 ] proposed a probabilistic model called stochastic neighbor embedding ( SNE ) , in which a probabilistic distribution over all the potential neighbors of points in a high dimensional space is defined under Gaussian assumptions and this distribution is then approximated in a low dimensional space . A generalization of SNE , called multiple relational embedding ( MRE ) [ 10 ] , has been developed by incorporating multiple different types of similarity relations . Alternatively , a generative model , called Gaussian process latent variable model ( GPLVM ) , has been proposed by Lawrence [ 9 ] . GPLVM uses a Gaussian process prior as a mapping from a latent space to a feature space induced by a kernel . Thus , it is a kernelized probabilistic model .
In this paper , we propose a new stochastic framework for nonlinear embedding . We use the “ relation ” , an extremely important notion in mathematics , to describe the embedding problem and then model the relation as a distribution . Given some relation among points in a high dimensional space , we seek to obtain corresponding coordinates in a low dimensional space by preserving relational identities before and after embedding . Further , we introduce a set of latent variables to characterize the relation . The latent variables are defined as two sets of distributions over the high and low dimensional spaces , respectively . Minimizing the Kullback Leibler ( KL ) divergence between the two sets of distributions is used as an embedding criterion . Consequently , the low dimensional embeddings can be obtained via minimizing the KL divergence with a numerical optimization method , such as the scaled conjugate gradient algorithm . More interestingly , if multinomial distributions and Gaussian distributions are used as the priors of the latent variables , we obtain respectively SNE of Hinton and Roweis [ 7 ] and GPLVM , also called “ twin kernel PCA ” , of Lawrence [ 9 ] . In this paper , we employ Bernoulli distribution priors and induce a new stochastic nonlinear embedding model , which is referred to as Bernoulli relational embedding ( BRE ) .
It is worthy to note that “ relation ” [ 8 ] represents an abstract mathematical concept . For practical applications in machine learning and pattern recognition , however , we are usually concerned with several concrete “ relations ” , such
1 as adjacency in a graph , neighbor of a position , similarity within a class , etc . Indeed , the neighborhood relation and the adjacency relation have widely been used in the existing unsupervised embedding methods [ 13 , 15 , 1 , 2 , 7 ] , while the class relation has also been used in supervised methods [ 5 , 6 ] . Interestingly , if we concretely consider the neighbor relation in BRE and resort to a first order numerical optimization method for embedding , BRE can be regarded as a probabilistic version of LLE .
The rest of this paper is organized as follows . Section 2 outlines a generic stochastic model for nonlinear embedding and accordingly re derives SNE and the twin kernel PCA . Section 3 presents our BRE model and its implementations . The experiments on the synthetic and real dataset are then presented in Section 4 . Finally , some concluding remarks are given in Section 5 .
2 Stochastic Relational Models
Given a set of k high dimensional points X = {x1 , x2 , . . . , xk} ⊂ Rn , our objective is to embed them into a low dimensional space Rm where m < n . Let Y = {y1 , y2 , . . . , yk} be the set of embedded coordinates corresponding to X . Consider a binary relation R [ 8 ] between elements of X . If R ⊆ X × X and ( xi , xj ) ∈ R , we say xi is in relation R with xj , and denoted xi ∼ xj for convenience . If xi is not in relation R with xj , we denote this as xi xj . Although , relations are an abstract concept in mathematics , for our purpose we are especially concerned with some concrete types such as adjacency relation , neighborhood relation or class relation . That is , Ra = {(xi , xj)|xi is adjacent with xj in a graph} , Rn = {(xi , xj)|xi is a neighbor of xj in position} , Rc = {(xi , xj)|xi and xj belong to the same class} . All pairwise relations between elements over X are represented by the affinity matrix Rk×k = [ rij ] , and over Y are represented by the affinity matrix Sk×k = [ sij ] , where rij(sij ) encodes the degree of such a relation from xi(yi ) to xj(yj ) . Our first step is to define the same relation on X and Y . Moreover , we assume that xi ∼ xj if and only if yi ∼ yj . In other words , we seek to preserve the relation R before and after embedding . From now on , we denote i ∼ j when either xi ∼ xj or yi ∼ yj . Our second step is to introduce a set of latent variables l to represent the relation R for each pair of points i and j . l can be either continuous or discrete . Each possible value of the latent variables encodes some relation among the elements . We model the latent variables l as distributions over X and Y , respectively . Based on different assumptions , the value ranges and the distributions of the latent variables will be different . The distribution of l over X depends on R , and the distribution of l over Y depends on S . Since the data in high dimensional space is fixed , the distribution p(l|R ) is already known . By tuning the coordinates of elements in Y , the affinity matrix S is changed correspondingly . We use the KL divergence minimization between two distributions over X and Y as an embedding criterion to reduce the difference of the affinity relations between R and S . When the affinity matrices R and S are the same , all pairwise relations over X are the same as those over Y , and therefore the divergence is zero . Consequently , the low dimensional embedding can be obtained via minimizing the KL divergence with a numerical optimization method . The structure of our stochastic model is clarified in Figure 1 . pX qYpq
We now illustrate that both SNE [ 7 ] and the twin kernel PCA [ 9 ] can be derived from our stochastic relational model in terms of the different prior distributions of l . First , we assume that the latent variable li = ( li1 , . . . , lik ) is a ( k − 1 ) vector ( other than lii ) for the point i , and li has values all zero except a 1 in one entry . We model li as multinomial distributions on X and Y , respectively . Then we have p(li|ri ) = lil , where ril and sil are , respectively , defined over X and Y subject to l=i sil = 1 . It is easy to obtain the KL divergence between p(li|ri ) and q(li|si ) as lil and q(li|si ) = l=i ril = 1 and l=i sil l=i ril
KL p(li|ri)q(li|si )
= ril log ril sil
. l=i
( cid:161 )
( cid:162 )
( cid:161 )
Since li is defined only for point i , the overall latent variable is a matrix L = ( l1 , . . . , lk ) . Thus , if the li ’s are mutually independent , we derive SNE , which obtains yi through minimizing k i=1 i=j
L =
=
( cid:162 )
KL p(li|ri)q(li|si ) rij log rij sij
The latent variable li has an explicit physical meaning . Namely , if lij = 1 , the point i will pick j as its neighbor .
2
XYlKL(p,q)p(l|X)q(l|Y ) Figure 2 shows the structure of the SNE model for the point i . ilik − 1risii j=i rij = 1 j=i sij = 1k
On the other hand , we can assume that the latent variable l is a k dimensional continuous vector and follows Gaussian distributions on both X or Y . Specifically , p(l|R ) = N ( 0 , R ) and q(l|S ) = N ( 0 , S ) , where R and S are k× k positive definite matrices ( also called kernel matrices ) over X and Y , respectively . In this case , |S| |R| + tr(R−1S ) − k 2 . q(l|S)p(l|R )
( cid:161 )
( cid:162 )
KL
= log
1 2
1 2
This leads us to the twin kernel PCA [ 9 ] . The structure of the GPLVM model is illustrated in Figure 3 . lkRSijji
.
When gradient descent methods , such as the scaled conjugate gradient ( SCG ) algorithm , [ 11 ] are used , the twin kernel PCA has higher computational costs than SNE because the former requires inverting a kernel matrix during each iteration , although it can be sped up through sparsification techniques ( see [ 9 ] for more details ) . Recall that the multinomial latent variable li has an explicit physical meaning . However , since li has only one element with value 1 , this implies that point i is related to one and only one point for one latent value . From this perspective , the constraint of SNE is a little strong . For example , consider the adjacent relation in a graph , where each vertex may have several adjacent vertices . In the next section , we shall relax this constraint and develop a new embedding model .
3 Bernoulli Relational Embedding
As we stated , in the SNE model , lij = 1 implies i ∼ j , while lij = 0 implies i j . This motivates us to directly model lij ’s ( i = j ) as independent Bernoulli distributions . Accordingly , we form a set of independent Bernoulli distributions , {lij | i , j = 1 , 2 , . . . , k and i = j} . Specifically , we define p(lij|rij ) = rlij ij ( 1 − rij)1−lij with rij ∈ [ 0 , 1 ] is a relational coefficient between xi and xj on X , while q(lij|sij ) = slij ij ( 1 − sij)1−lij with sij ∈ [ 0 , 1 ] is a relational coefficient between yi and yj on Y . We refer to this embedding model as Bernoulli relational embedding ( BRE ) , where rij is pre specified as a function of xi and xj , and sij is a link function between unknown yi and yj . It is easy to obtain the KL divergence between p(lij|rij ) and q(lij|sij ) as
KL(p(lij|rij)q(lij|sij ) ) = rij log rij sij
1 − rij 1 − sij ( 1 ) Figure 4 represents the Bernoulli relational model , which encodes the relation from point i to point j .
+(1−rij ) log
. ijlijrij = rjisij = sjiijrij ∈ [ 0 , 1]sij ∈ [ 0 , 1]k ∗ ( k − 1)k ∗ ( k − 1 )
Furthermore , we assume the rij and sij share some com mon attributes . That is ,
3
xix1xkliyiy1ykrisix1x2xkly1y2ykRSxixjlijyiyjrijsij ( cid:184 )
.
( cid:183 ) i=j
Assumption ( 1 ) rii = 1 and sii = 1 ; Assumption ( 2 ) rij = 0( or 1 ) ⇐⇒ sij = 0( or 1 ) . Assumption ( 1 ) shows that the self relation exist in BRE . The goal is to make BRE consistent with other models . In fact , we do not need to use the self relation . The goal of Assumption ( 1 ) makes BRE consistent . Assumption ( 2 ) is necessary because it ensures the KL is well defined due to 0 = 0 . Note that it is permissible that rij = rji . Given 0 log 0 independent observations {lij | i , j = 1 , 2 , . . . , k and i = j} from the Bernoulli distributions with the parameters rij ’s , we seek to find the coordinates of the yi ’s through minimizing the following loss function
L({yi} ) =
1 4 rij log rij sij
+ ( 1 − rij ) log
1 − rij 1 − sij
( 2 ) Let R = [ rij ] and S = [ sij ] be affinity matrices . The latent variable in BRE and SNE is a matrix , where each element lij represents the relation R from point i to point j . In BRE , lij ( i , j = 1 , . . . , k and i = j ) is a binary variable , and the lijs are independent of each other . BRE minimizes the similarity between corresponding elements of R and S in the case that their elements are in [ 0 , 1 ] . In SNE , the ( k − 1 ) dimensional latent vector li encodes the relations from point i to the rest of the ( k − 1 ) points . Since there are constraints that the row summation of the affinity matrices R and S are 1 , the ( k − 1 ) relations from point i are coupled together . SNE minimizes the similarity between corresponding rows of R and S in the case that their rows are normalized . The latent variable in GPLVM is a continuous k dimensional vector . GPLVM directly minimizes the similarity between R and S in the case that they are positive definite .
The most direct approach is to compute each entry of the affinity matrix by each pairwise distance . We employ the Gaussian kernel to define rij ( i = j ) , eg , rij = r(xi , xj , β ) = exp
,
( 3 ) where β > 0 is a width parameter . Similarly , we can define sij as
( cid:181 ) −xi − xj2
( cid:182 )
( cid:181 ) −yi − yj2
( cid:182 )
β
γ sij = s(yi , yj , γ ) = exp
,
( 4 ) where γ > 0 is also a width parameter . It is clear that rij ( or sij ) is translation invariant in x ( or y ) . Moreover , the definitions of rij and sij are consistent for the above assumptions . This affinity definition tries to map nearby points in
4 the original space to nearby points in the embedding and faraway points to faraway points . Therefore , it tends to give a more faithful representation of the data ’s global and local structure in the embedding . With the rij and sij , our current problem becomes finding the coordinates of the yi ’s via minimizing L({yi} ) wrt {yi} . In this case , the gradient of L({yi} ) wrt {yi} is i=j
∂L ∂yi
=
1 γ rij − sij 1 − sij
( yi − yj ) .
To update yi , we only use relevant relations rij(sij ) ( j = 1 , , k ) from point i . Hence , compared with the gradient in SNE [ 7 ] where one update needs to compute all pairwise relations , BRE costs less time . We can resort to the SCG algorithm to obtain the yi ’s . Specifically , we treat the minimization of L({yi} ) as a multi parameter optimization problem and employ a conditional SCG algorithm with parallel update scheme . That is , one finds the ( t+1)st estimate yi(t+1 ) of yi via minimizing L(y1(t ) , . . . , yi−1(t ) , yi , yi+1(t ) , . . . , yk(t) ) . Although γ can be adaptively estimated by using SCG , we let γ = 1 in the following experiments for computational simplicity .
In many cases , the high dimensional data may have its own intrinsic structure on a low dimensional manifold . The two points , which are closed in the original space , may be faraway on the intrinsic manifold . As such , embedding needs to be optimized only to preserve the local configurations of nearest neighbors like LLE . Similarly , other relation information could be used to guide the distance measurement such as class label , side information [ 16 ] , etc . Since BRE seeks to propagate some relation among xi ’s in Rn to yi ’s in Rm , it is natural to utilize this relation to define rij . Here we present several effective methods to choose rij . One candidate is rij =
2 + 1 2 − 1
2 exp 2 exp
1
−xi−xj2 −xi−xj2
β
β i ∼ j i j .
( 5 )
 1
( cid:179 ) ( cid:179 )
( cid:180 ) ( cid:180 )
The affinity matrix R = [ rij ] defined by ( 5 ) possesses some nice discriminant properties . Firstly , its induced distance is metric1 . In addition , the affinity between any two points with the same relation ( eg , in the same neighborhood ) will never be larger than that between any two points with different relations ( eg , in different neighborhoods ) . The larger the Euclidean distance between points , the smaller the within relation affinity while the larger the betweenrelation affinity .
1Here the proof is omitted to save the space .
Another candidate comes from a truncated version of the
Gaussian kernel
( cid:179 )
( cid:179 ) exp 0 exp 0 rij = sij =
( cid:180 )
( cid:180 )
−xi−xj2
β
−yi−yj2
γ i ∼ j i j . i ∼ j i j .
To match Assumption ( 3 ) , we define sij as
( 6 )
( 7 ) the different area of the manifold onto the same region . It is not a good dimensionality reduction mapping method as illustrated in the figure . Although points are arranged as in PCA at the beginning of the iteration , the BRE model can coordinate the points to keep the original neighborhood relations as shown in Figure 5 .
We also apply BRE to a collection of bitmaps of handwritten digits ( USPS ) and a set of face images from a video sequence , all of which have been used in [ 12 , 7 , 9 ] . These two high dimensional datasets are likely to have intrinsic discriminant structures in lower dimensional spaces . Here we explore the neighborhood relation between pairwise points . The low dimensional embeddings are obtained via SCG with a maximum number of 200 iterations and initializations based on the standard linear PCA . We follow the settings in [ 7 ] to randomly choose a subset of 3000 of the digits 0 4 ( 600 for each digit ) from a 16× 16 grayscale version of the USPS database . We adopt ( 5 ) with β = 500 for rij and ( 4 ) with γ = 1 for sij . Moreover , we define i ∼ j if xi − xj2 ≤ 2β , and i j otherwise . Figure 6 illustrates a 2 D visualization on this dataset after having implemented BRE . We also apply BRE to Brendan ’s face dataset , which consists of 1965 grayscale images at 28 × 20 resolution . Each image contains one face with a certain pose , expression and lightness . For this dataset , we adopt ( 6 ) with β = 200 for rij and ( 7 ) with γ = 1 for sij . In addition , we determine i ∼ j if and only if xi belongs to K nearest neighborhoods of xj . In the experiment , we set K = 200 . Figure 7 shows our results where the faces are embedded into a two dimensional space . As we can see , some attributes of the images change smoothly across this embedded space , eg , the images in the lower part are brighter than those in the upper part . Moreover , BRE seems to cluster these faces in terms of the features of pose , expression and lightness . We enlarge four small regions in Figure 7 , each of which depicts the images in a local area of the embedded space for better visualization . As can be seen , the
The affinity matrix R = [ rij ] defined by ( 6 ) is a positive semi definite matrix , so its induced distance is Euclidean . If we consider the neighborhood relation , this definition shows that the affinity between any two points xi and xj belonging to different neighborhoods is zero . Furthermore , the affinity between yi and yj corresponding to xi and xj is also zero . Thus , BRE can preserve the local neighborhood relation before and after embedding like LLE [ 13 , 12 ] . In addition , the conditional SCG algorithm that we use works in a first order gradient iterative setting . The updating equation in the gradient method is yi(t+1 ) = yi(t ) − α = yi(t ) − α γ
( yi(t ) − yj(t ) ) yi=yi(t ) rij − sij(t ) 1 − sij(t ) αjyj(t ) .
= αiyi(t ) +
∂L ∂yi
( cid:175)(cid:175)(cid:175 ) j∼i j∼i
This equation illustrates that a low dimensional embedding , eg , yi , is a linear combination of other low dimensional embeddings , yj ’s , within the same neighborhood with yi . Thus , BRE can be regarded as a probabilistic version of LLE .
4 Experiments
In this section , we illustrate BRE ’s ability in modeling local neighborhood relations by producing two dimensional visualization of data . We firstly generate 2000 points on each of surfaces of the S curve and the twin peaks in three dimensional space . In these two manifolds , two nearby points in the original space may be faraway on the manifold surface . Hence , the obtained embedding should not preserve a faithful representation for every pairwise distance . Instead , each point in the embedding tries to keep the same neighbors as those in the original space . Therefore , we handle the relation by the neighborhood concept and use equations ( 6 ) and ( 7 ) to define the affinity matrices R and S . The embedding results are shown in Figure 5 . PCA is used to initialize the coordinates of the iteration . Since PCA is a kind of linear mapping , it generally projects the points of
5
RawPCABRERawPCABRE [ 8 ] B . Kolman , R . C . Busby , and S . C . Ross . Discrete Mathematical Structures . Prentice Hall , NJ , fourth edition , 2000 . [ 9 ] N . D . Lawrence . Gaussian process latent variable models for visualisation of high dimensional data . In Advances in Neural Information Processing Systems 16 , 2004 .
[ 10 ] R . Memisevic and G . Hinton . Multiple relational embedIn Advances in Neural Information Processing Sys
[ 11 ] I . T . Nabney . NETLAB : Algorithms for Pattern Recognition . ding . tems 17 , 2005 .
Springer , 2001 .
[ 12 ] S . Roweis , L . K . Saul , and G . Hinton . Global coordination In Advances in Neural Information of local linear model . Processing Systems 14 , pages 889–896 , 2002 .
[ 13 ] S . T . Roweis and L . K . Saul .
Nonlinear dimensionScience , ality reduction by locally linear embedding . 290(22):2323–2326 , 2000 .
[ 14 ] B . Sch¨olkopf , A . Smola , and K R M¨uller . Nonlinear component analysis as a kernel eigenvalue problem . Neural Computation , 10:1299–1319 , 1998 .
[ 15 ] J . B . Tenenbaum , V . de Silva , and J . C . Langford . A global geometric framework for nonlinear dimensionality reduction . Science , 290(22):2319–2323 , 2000 .
[ 16 ] E . P . Xing , A . Y . Ng , M . I . Jordan , and S . Ressell . Distance metric learning with application to clustering with side information . In Advances in Neural Information Processing Systems 15 , volume 15 , 2003 . images in the same local region possess similar attributes ( see the illustrations in Figure 7 ) .
5 Concluding Remarks
In this paper , we have proposed a new method for nonlinear embedding problems . This method is motivated by the need to maintain some relational identities between a highdimensional space and a low dimensional embedded space and introduces stochastic latent variables to describe the relation over these two spaces , respectively . With different prior distributions of the latent variables , we can construct different embedding methods . For example , the multinomial , Gaussian and Bernoulli distributions lead us to SNE , the twin kernel PCA and BRE , respectively . Naturally , this motivates us to develop new embedding methods in the future along this line .
BRE not only has an explicit physical meaning , but has a low computational cost as well , especially when the number of points , k , is very large . Since like SNE , BRE uses an iterative optimization process , it avoids either spectral decomposition on a k × k matrix required by ISOMAP and KPCA , or inverting a k × k matrix required by GPLVM . We have applied BRE to the embedding of the USPS digit dataset and Brendan ’s face dataset , in which we explored the “ neighborhood relation ” between pairwise points . Experimental results show that BRE is efficient for nonlinear embedding . It is readily applicable to the multiclass learning problem [ 5 , 6 ] by considering the “ class relation ” . Inspired by MRE of Memisevic and Hinton [ 10 ] , we can also use multiple types of relations in BRE according to the nature of the problem at hand .
References
[ 1 ] M . Belkin and P . Niyogi . Laplacian eigenmaps and spectral In Advances in techniques for embedding and clustering . Neural Information Processing Systems 14 , 2002 .
[ 2 ] M . Brand . Nonlinear dimensionality reduction by kernel eigenmaps . In Proceedings , International Joint Conference on Artificial Intelligence(IJCAI 2003 ) , 2003 .
[ 3 ] M . Brand . A unifying theorem for spectral embedding and clustering . In The 9th International Conference on Artificial Intelligence and Statistics , Key West , Florida , 2003 .
[ 4 ] F . R . Chung . Spectral Graph Theory . American Mathemat ical Society , 1997 .
[ 5 ] O . Dekel and Y . Singer . Multiclass learning by probabilistic embeddings . In Advances in Neural Information Processing Systems 15 , 2003 .
[ 6 ] J . Goldberger , S . Roweis , G . Hinton , and R . Salakhutdinov . Neighbourhood components analysis . In Advances in Neural Information Processing Systems 17 , 2005 .
[ 7 ] G . Hinton and S . Roweis . Stochastic neighbor embedding . In Advances in Neural Information Processing Systems 15 , 2003 .
6
( a ) The distribution for the images in a 2 D embedded space . Red plus signs , blue circles , magenta crosses , green stars and yellow squares represent the digits 0 , 1 , 2 , 3 , 4 , respectively .
( b ) The digit images are visualized in the 2 D embedded space . We plotted images in a random order , but did not plot the images which would overlap the existing images . Consequently , 523 of the 3000 digits are plotted . Note that within each class , properties like line thickness and orientation tend to vary smoothly across the space .
7
−1−08−06−04−020020406081−1−08−06−04−020020406081 ( a ) The face images are visualized in a 2 D embedded space . We plotted images according to the same procedure used in Figure 6 . There are 251 faces shown in this figure . Note that the images in the lower part are much brighter than those in the upper part . The faces in the left images are likely to turn to the left , and turn to the right in the right images . Four small blocks are enlarged and shown in the following figures .
( b ) Images in the upper sub block . Brendan sticks out his tongue .
( c ) Images in the center sub block . Brendan smiles .
( d ) Images in the left sub block . Brendan turns his face to the left .
( e ) Images in the right sub block . Brendan turns his face to the right .
8
