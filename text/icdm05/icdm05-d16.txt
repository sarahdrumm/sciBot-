Supervised Tensor Learning
Dacheng Tao1 , Xuelong Li1 , Weiming Hu2 , Stephen Maybank1 , and Xindong Wu3
1 . School of Computer Science and Information Systems , Birkbeck College , University of London , UK
2 . National Lab of Pattern Recognition , Institute of Automation , Chinese Academy , P . R . China
3 . Department of Computer Science , University of Vermont , USA .
{dacheng , xuelong , sjmaybank}@dcsbbkacuk , wmhu@nlpriaaccn , xwu@csuvmedu
Data structures in the real world : in many computer vision applications objects are represented as 2nd–order , 3rd–order , or higher–order tensors . For example , grey level face images [ 5 ] are usually represented as 2nd–order tensors ; an attention region [ 6 ] , which is used in natural image understanding [ 4 ] , is represented as a 3rd–order tensor ; another 3rd–order tensor example is the bi–level down sampled silhouette images used to represent human gait [ 7 ] ; and moreover , video sequences [ 8 ] are 4th–order tensors .
Data structures for conventional learning methods are restricted to one–dimensional vectors , ie the 1st– order tensors as inputs . Learning machines such as MPM and SVM obtain top–level performances in machine learning and data mining ; however , they have to convert many data items naturally represented by high –order tensors to one–dimensional vectors in order to comply with their input requirements . As a result of this conversion , much useful information in the original data is destroyed , leading to an increased number of classification errors . In addition , the conversion to vector input usually leads to the so called curse of dimension problem since the dimension of the feature space becomes much larger than the number of training samples . If the data are represented in their natural , high–order tensors , then this curse of dimension problem is usually reduced .
The main contributions of this paper are as follows . Tensor–plane ( a set of projection orientations ) : by an iterative scheme , the STL–based classifier ’s optimal tensor–plane can be obtained . Our method aims to approach an optimization criterion iteratively ; and this criterion can be any convex functions , such as , the margin maximization ( SVM ) and the probability of correct classification of future data maximization ( MPM ) .
Kernelization : a kernel space representation of general tensor inputs is developed .
Tensor MPM ( TMPM ) : as an example , MPM is generalized to its STL version , named the tensor MPM ( TMPM ) . TMPM is then compared with MPM on a sample image classification problem . In addition , TMPM is kernelized .
Abstract the STL
This paper aims to take general tensors as inputs for supervised learning . A supervised tensor learning ( STL ) framework is established for convex optimization based learning techniques such as support vector machines ( SVM ) and minimax probability machines ( MPM ) . Within framework , many conventional learning machines can be generalized to take nth–order tensors as inputs . We also study the applications of tensors to learning machine design and feature extraction by linear discriminant analysis ( LDA ) . Our method for tensor based feature extraction is named the tenor rank–one discriminant analysis ( TR1DA ) . These generalized algorithms have several advantages : 1 ) reduce the curse of dimension problem in machine learning and data mining ; 2 ) avoid the failure to converge ; and 3 ) achieve better separation between the different categories of samples . As an example , we generalize MPM to its STL version , which is named the tensor MPM ( TMPM ) . TMPM learns a series of tensor projections iteratively . It is then evaluated against the original MPM . Our experiments on a binary classification problem show that TMPM significantly outperforms the original MPM .
1 . Introduction learning and data mining and
Supervised learning [ 1–3 ] is an important topic in machine their applications . Generally , only one–dimensional vectors are accepted as inputs by existing supervised learning machines , such as support vector machines ( SVM ) [ 2 ] , and minimax probability machines [ 3 ] . However , in the real world many data items such as images are represented by 2nd–order or high–order tensors rather than the one dimensional vectors . We develop a supervised tensor learning ( STL ) framework in order to apply convex optimization based supervised learning techniques to tensor data .
( MPM )
The motivation for the STL framework comes from the following observations .
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 )
1550 4786/05 $20.00 © 2005 IEEE
B . Minimax probability machines ( MPM ) for the probability of correct MPM maximizes classification future data , or alternatively , minimizes the maximum of the Mahalanobis distances of the positive and negative samples . It is a second order cone programming :
( cid:173 ) ( cid:176)(cid:176 ) ( cid:174 ) ( cid:176 ) ( cid:176)(cid:175 )
. . s t
T w x b b w y max
, 0 , b w ff fi fi ff ( cid:78 ) ff fi ( cid:78 ) 1 xn ( cid:166 ) , x i n 1 ix
T
( 2 )
T w
T w w w x y y
1 yn ( cid:166 ) , n y j
1 j y x and where fi ( cid:78 ) ff
1
, x y are the covariance matrices of x and y , and w is the optimal projection orientation for classification . MPM has a solid theoretical foundation based on the powerful Marshall and Olkin ’s theorem [ 15 ] . MPM can outperform SVM consistently [ 3 ] .
22 Supervised tensor learning framework
Motivated by the successes of convex optimization based learning algorithms and the effective image representation by general tensors [ 13 ] [ 14 ] , we propose a tensor framework for this type of learning algorithm . Generally , convex optimization based learning can be written as : ( cid:173 ) ( cid:176)(cid:176 ) ( cid:174 ) ( cid:176 ) ( cid:176)(cid:175 ) ff ff where jC are linear or quadratic constrained functions . It is not difficult to show that ( 1 ) and ( 2 ) can be unified into ( 3 ) . The optimal hyper–plane for the classification can thus be written as : fi ff ( cid:84 ) , f w b fi ( cid:84 ) ( cid:75 ) , ,1 C w x b i fi ( cid:84 ) , j
( 3 ) max 0 w ,
C w y b iC and
( cid:100 ) ( cid:100 )
( cid:100 ) ( cid:100 )
. . s t
,1 n n x i y j i j
,
,
T
T x y x y i j
T sign ff fi g z ff fi g z ffi , then z belongs to the positive
. ( 4 )
Here , if class ; otherwise , it belongs to the negative class . w z b
1 fi ff
Based on ( 3 ) and ( 4 ) , we can use the tensor idea to re–represent convex optimization learning with nth– order tensors as inputs by : ff , f w w w
( 5 ) w b w b
, ,
,1 n
,
1
2 n
2
2 n n
, fi ( cid:84 ) , fi ( cid:84 ) ( cid:75 )
, i fi ( cid:84 )
, j i
( cid:100 ) ( cid:100 ) ( cid:100 ) ( cid:100 ) j x n y
( cid:173 ) ( cid:176)(cid:176 ) ( cid:174 ) ( cid:176 ) ( cid:176)(cid:175 )
. . s t i x i ff C x ff y C y j
1
1 max
0 w w 1
j w w 1
,1 and the optimal classification function is :
sign w b z w 1 1 ff fi g z w 2 ff
,
2 n n
2
2 n n w b fi
( 6 )
Stability analysis : in a series of experiments we observe that TMPM converges quickly within a very few iterations ( around 5 ) . Furthermore , unlike many other learning models , it is stable under different choices of initial values [ 11 ] , ie it does not become trapped in a local minimum .
Feature extraction : we study tensor–based feature extraction based on linear discriminant analysis ( LDA ) . In this effort [ 18 ] , we inherit the merits from both the defined differential scatter based discriminant criterion ( DSDC ) and the rank–one ( or rank–n ) tensor decomposition ; as a result DSDC can extract features from tensors . The new established feature extraction algorithm is named the tenor rank–one discriminant analysis ( TR1DA ) .
2 . Supervised tensor learning ( STL )
This section introduces some basic concepts in convex optimization based learning ; establishes a STL framework for general tensors with an iterative procedure ; and also deduces a kernel extension of the STL framework for high–order tensor pattern classification using convex optimization . supervised
21 Convex optimization based learning
The intrinsic connections between machine learning methods and convex optimization techniques have been well studied . Examples include quadratic programming [ 1 ] in SVM and second order cone programming [ 3 ] in MPM . More recently , some convex optimization tools , such as semi–definite programming [ 12 ] , have become popular because they are simple and yet powerful . Below , ) ) are tensors belonging to the and positive class and the negative class respectively . iy ( 1 ix ( 1
( cid:100 ) ( cid:100 )
( cid:100 ) ( cid:100 ) n n i i y x
A . Support Vector Machines ( SVM )
SVM [ 2 ] is an effective bi–category classification algorithm with sound theoretical foundations and a good generalization ability . It aims to maximize the margin between the positive and negative samples as :
( cid:173 ) ( cid:176)(cid:176 ) ( cid:174 ) ( cid:176 ) ( cid:176)(cid:175 )
. . s t
T w x i T w y j min
T w w ff fi 1,1 1,1 ffi ffi b ffi ( cid:100 ) b
( cid:100 ) ( cid:100 ) ( cid:100 ) ( cid:100 ) i j
( 1 ) n x n y
. where w is the optimal projection orientation for bi category classification .
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 )
1550 4786/05 $20.00 © 2005 IEEE k kw where [ 14 ] . It is defined as : of projection orientations for classification . is the mode–k product in tensor analysis . Here , we use a set kw ( optimal tensor plane ) k |n
B A w k
1 k solutions to ( 5 ) are known . In the next section , an alternating approach is developed to solve ( 5 ) .
23 Alternating approach i
1|n
In our tensor based supervised learning framework , we define and study an optimal tensor plane ( iw , a set of projection orientations ) to approach the objective function with some constrained functions , which are related to the positive and negative samples . A close– form solution does not exist , so we develop an alternating procedure ( AP ) for the framework , listed in Table 1 . In this table , Steps 2 and 3 finds the optimal iw and Step 4 finds the optimal bias b . tensor plane
1|n i
24 Kernelization
As for many other machine learning and data the STL framework can be mining algorithms , kernelized directly . The kernelized model is :
( cid:173 ) ( cid:176)(cid:176 ) ( cid:174 ) ( cid:176 ) ( cid:176)(cid:175 )
. . s t x C i
C y j x i ff ff ( cid:77 ) ff ff ( cid:77 ) y j max f
0 w ff fi
( cid:77 ) 1 fi ff ( cid:77 )
1
, , w 1 ff ff fi ( cid:77 ) fi
w 1 fi
n w 1 n n
, fi ff fi ( cid:77 ) ( cid:84 ) , w b fi ff fi
( cid:77 ) ( cid:84 ) ( cid:75 ) , w b i fi ff fi ( cid:84 ) ( cid:77 ) , w b j
,
, n n
,1
,1
( 7 )
( cid:100 ) ( cid:100 ) i n x
( cid:100 ) ( cid:100 ) j n y where and and the optimal classification function is : ff ( cid:77 ) w n ff fi g z ( 8 ) k is the mode–k product in tensor analysis [ 14 ]
ff ( cid:77 ) ff fi z ff ( cid:77 ) sign w 1
1 fi b fi fi n fi w 1
fi ff
( cid:77 ) 1 tensor is a nonlinear mapping function . ff ( cid:77 ) rank–one decomposition , ff fi(cid:77 ) ( cid:152 ) The key issue in kernelization is how to calculate . The answer is to perform x i ie x can be ( cid:150 ) . In ( cid:79 ) the kernel space the decomposition can be written as : decomposed into
ff ( cid:77 )
( cid:79 ) i i w n i w j w n i w 1
1 x i fi
1 n n n i j j ff ( cid:77 ) x i fi ff ( cid:79 ) ( cid:77 )
1 i i w 1 fi
n ff ( cid:77 ) i w n fi n ff ( cid:150 ) ( cid:79 ) ( cid:77 ) i j w i j fi
. j
1
With this representation , the kernel trick can be utilized directly : fi w 1 x i n n i k k k k
, l
( cid:152 )
( cid:152 ) i k k
1 ff fi fi fi fi ff w l w l
1 w n i w k i w k ff ( cid:77 ) ff ( cid:77 )
1 w k i w k fi
( 9 )
n fi ff fi ff ( cid:150 ) ( cid:79 ) ( cid:77 ) i ( cid:150 ) fi ( cid:150 ) fi ff ff ( cid:77 ) ( cid:77 ) l ( cid:150)(cid:150 ) ff l ( cid:71)(cid:77 ) ( cid:77 ) ( cid:79 ) ( cid:77 ) ( cid:77 ) ( cid:79 ) k i l ff ( cid:150 ) ( cid:79 ) K w w k i ff fi kK ( cid:152 ) where is a typical kernel function , such as the Gaussian function and the polynomial function . By this kind of representation , we can have n kernel functions for the tensor learning framework in the kernel space with the nth–order tensors as inputs . k
Table 1 . Alternating procedure ( AP ) for STL . ix , 1
The positive nth–order tensor points i and the negative nth–order tensor points 1
: t u p n I
( cid:100 ) ( cid:100 ) n i
. y
( cid:100 ) ( cid:100 ) x n iy , t u p t u O
A supervised tensor classifier based on the optimal tensor plane and the following bias : ff fi sign g z w b i denotes the mode–i product [ 14 ] . where z w 1 1
w 2 ff fi
,
2 n n
. Initialization : Set m as the nth–order tensor defined
1 p e t S by m
1 2 n x n x ffi(cid:166 ) x i i
1
1 2 n x n y
( cid:166 ) . y j j
1
Estimate the initial values for according to ( cid:79 )
1
0 w 2
0 w 1 m w
2 n kw ( 1 k
( cid:100 ) ( cid:100 ) n
) , min ( cid:100 ) ( cid:100 ) ,1 w k n k where For For
,
0 n F .
( cid:79 ) 1
0 m w 1
2
0 w 2
n
0 w n
1p , 2 , … , N or ( until converged ) , do :
T x i
C
1l ,…,n , do : ( cid:173 ) max ( cid:176 ) p
0 w l ( cid:176 ) ( cid:176)(cid:176 ) ( cid:174 ) ( cid:176 ) ( cid:176 ) ( cid:176 ) ( cid:176)(cid:175 ) x fi ff fi ff ( cid:100 ) ( cid:100 ) 1 i ff ff ff ff
. . s t C p w l p w l y y j y
T x i j ff f w b l p l
, p w l
1
, fi ( cid:84 ) , fi fi ( cid:100 ) ( cid:100 ) j
,
1 p w l b l b l n y l l n x
,1 fi fi
( cid:84 ) ( cid:75 ) , i
( cid:84 ) , j
.
2 p e t
S
.
3 p e t
S
. 4 p e t S iC and w 1p l l
1 p w 1 where jC are constrained functions . Here ,
1 is defined by :
l x i x i Find the optimal bias b for all the projected positive and negative training samples by :
1 p w
1 l
1 p w ffi 1 l p w n l
ffi 1
1
1
. n x i ff C x i ff . . s t C y y j j
1
1
( cid:173 ) ( cid:176 ) ( cid:176)(cid:176 ) ( cid:174 ) ( cid:176 ) ( cid:176 ) ( cid:176)(cid:175 ) max N w 1
2 i
N w 1 2 ( cid:100 ) ( cid:100 ) 1 ff fi ( cid:84 ) , f b N
w 2 n
n ( cid:100 ) ( cid:100 ) ,1 j
N w 2 n x
N n n y w b
,
N n w b
, fi ( cid:84 ) ( cid:75 ) , i fi ( cid:84 ) , j
According to the definition of this novel tensor learning framework , we can oriented supervised directly represent objects in their original format for machine learning and data mining applications in computer vision etc . However , so far , no closed–form
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 )
1550 4786/05 $20.00 © 2005 IEEE d d
1
M u u
M | d
1 d d
1 r
1 r , j i ru given
. The projection orientations
( cid:150)X ( cid:79 ) 1 r , i j d are obtained by the alternating least square u 1 1 r ( ALS ) method . In ALS , we obtain the optimal base vector . We can conduct the procedure iteratively to obtain . The flowchart u 1 1 of the algorithm is given in Figure 1 and the detailed procedure for TR1DA is given in [ 18 ] due to length constraints . More extensions , such as a graph embedding extension and a kernel extension for TR1DA , are also given in [ 18 ] . i d 1 1|
( cid:100 ) ( cid:100 ) i M
M | d d r i r
X 1 r , i j
1 1ru
M ru 1
2 1ru
M ru 1
2 1ru
1 j(cid:79 ) r , i
1 1ru
X 1 r , i j
( cid:79 ) 1 r , i j fi u
1 r
1 fi u
2
1 r fi fi
u
M
1 r r jX , i
Figure 1 . Greedy approach in TR1DA . d r
M | d
1 k , i u 1
The coordinate value
With TR1DA , we can obtain iteratively . 1 j(cid:79 ) can represent the original pX for each tensor X . For recognition , the prototype tX individual class in the database and the test tensor to be classified are projected onto the bases to get the p k(cid:79 ) and test weight vector prototype weight vector k(cid:79 ) . The test tensor class is found by minimizing the distance
|k R t
|k R
.
1
1 k R ( cid:72 ) ( cid:79 ) | t k
( cid:79 ) k R |
1 p k
1 r u
1|d
( cid:100 ) ( cid:100 ) 1 r R ( cid:100 ) ( cid:100 ) d M
Unlike existing tensor extensions of discriminant do not analysis , TR1DA can converge ( all change any more during training stage ) . We can check fi 1 the convergence through ff ( cid:100 ) where ( cid:72 ) 1 ffi is a small number . If ff fi , the calculated 1 projection orientation in the tth iteration is equivalent to the ( t+1)th iteration . ff ( cid:152 ) fi 1 u ff fi u
( cid:72 ) u u l k l k
( cid:152 ) l k l k ffi t t t t
3 . Tensor minimax probability machines ( TMPM ) : an instance of STL framework
The alternating procedure can also be kernelized because the lth kernel tensor projection orientation is , which is similar to vector based ffi n n x y ff ( cid:77 ) w l fi
( cid:166 ) i
1 ff ( cid:77 ) i l i w l fi i kernel algorithms , such as kernel discriminant analysis . l is the linear combination coefficient for the Here , lth direction of the ith training sample . Based on these observations , it is straightforward to outline the kernel AP . In this paper , we do not focus on the kernel method , because the kernel parameter , defined in kernel functions , tuning is still a problem and the number of the kernel parameters is much more than that in the 1st–order form .
25 LDA–based feature extraction : TR1DA
, j j i k , i
1 , i
X . Moreover ,
TR1DA extends the STL framework for feature extraction based on linear discriminant analysis ( LDA ) . Because TR1DA includes many variables , we first jX is the ith object in the jth class define the variables . in the kth training iteration . For k=1 , we have jX is an Mth–order tensor . X ff is the jth class mean tensor in the kth ( cid:166)M training iteration and is the total mean tensor of all objects in the kth training iteration . iu is the jth direction base vector for decomposition in the ith these definitions , TR1DA is defined by the following equations : iteration . With ff ( cid:166)M training fi , jn
1 i
M fi
X k , i c n
1 k j k i k j c k j j j j k X , i j
1 k X , i j
1 k ( cid:79 )
, i j
M
( cid:150 ) d
1 d u
1 k
1 k ( cid:79 ) ; , i j
1 k X , i j d
M
( cid:150 ) d
1 d u
1 k d
( 10 ) arg max l M | u
1 k l
( cid:167 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:169 ) c
( cid:166 ) i
1
( cid:167 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:169 ) n i ff
( cid:167 ) ( cid:168 ) ( cid:169 )
M M k i k fi ff
( cid:167 ) ( cid:168 ) ( cid:169 )
M
( cid:150 ) l
1 ff l u k
T fi l
( cid:183 ) ( cid:184 ) ( cid:185 )
M M k i k fi
M
( cid:150 ) l
1 ff l u k
T fi l
( cid:183 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:185 ) fi
T ( cid:183 ) ( cid:184 ) ( cid:185 ) ff
( 11 )
( cid:183 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:185 )
( cid:183 ) ( cid:184 ) ( cid:185 ) T ( cid:183 ) ( cid:184 ) ( cid:185 )
( cid:183 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:185 ) c n i
( cid:166)(cid:166 ) i
1 j
1 k X M i k , j i k X M i k , j i fi fi ff ff
( cid:167 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:169 )
( cid:167 ) ( cid:168 ) ( cid:169 ) ( cid:167 ) ( cid:168 ) ( cid:169 )
M
( cid:150 ) l
1
M
( cid:150 ) l
1
T l u k l ff l u k
T fi l where
X
1 , i j
X . j i
,
From the definition of the problem , which is given by ( 10 ) and ( 11 ) , we know that TR1DA can be calculated by a greedy approach , because of the lack of the closed form solution for the problem . The greedy approach is illustrated in Figure 1 . The calculation of X and jX is based on the given . With the 1 given , we calculate via and u 1
M | d
1 j r , i r , i d r
1 u 1 d r
M |
1 d
X 1 r , j i j(cid:79 ) r , i
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 )
1550 4786/05 $20.00 © 2005 IEEE optimization . Based on the procedure developed in Section 2.3 , to solve ( 12 ) is straightforward . In Table 2 , we have only listed Step 3 and Step 4 , which require more detail , while Step 1 and Step 2 are the same as in the procedure shown in Table 1 . to analyze the computational complexity of this generalized TMPM . If the second order cone programming is solved by the primal–dual interior–point method , the computational complexity
We now turn
O N of TMPM is
( cid:167 ) ( cid:168 ) ( cid:169 ) 1 L R n n belong to convergence .
L
( cid:166 ) i
1 ff i n fi3
( cid:183 ) ( cid:184 ) ( cid:185 )
, when the input tensors and N iterations are required for
Regarding kernelization , the key issue in a kernel tensor minimax probability machine ( KTMPM ) is the adaptation of ( 12 ) to the kernel trick . Here , we present our results for the positive class only . For the negative class , the deduction process is similar . We define
( cid:84 ) ( cid:79 ) i i k n n x ffi n y
( cid:166)(cid:150 ) q
1
1 l l k l ( cid:74 ) q ff q K w w l i l
, fi
( 13 )
( cid:77 ) then we have : ff fi x ( cid:173 ) ( cid:176 ) 1 ( cid:174 ) n ( cid:176 ) ( cid:175 ) x k ff ( cid:77 ) ffi n ( cid:166 ) ( cid:166 ) n n x x i
1 j
1 y w fi k k i ( cid:74)(cid:84 ) k j k fi w k ff ( cid:77 ) ff j K w w k i k
,
( 14 ) fi
( cid:189 ) ( cid:176 ) ( cid:190 ) ( cid:176 ) ( cid:191 ) and ff ff ( cid:77 ) w k fi
T fi
1 n x n x n x
( cid:166)(cid:166 ) i
1 j
1 ff ( cid:167 ) ( cid:77 ) ( cid:168 ) ( cid:168 )
( cid:77 ) ( cid:169 ) ff ( cid:167 ) ( cid:77 ) ( cid:168 ) ( cid:168 )
( cid:77 ) ( cid:169 ) fi x i ff fi x fi x ff fi x j k k k k k w ff ( cid:77 ) w ff ( cid:77 ) ff ( cid:77 ) w ff ( cid:77 ) w k fi fi fi fi k k
( cid:183 ) ( cid:184 ) ( cid:184 ) ( cid:185 ) ( cid:183 ) ( cid:184 ) ( cid:184 ) ( cid:185 )
T
( cid:189 ) ( cid:176 ) ( cid:176 ) ( cid:176 ) ( cid:190 ) ( cid:176 ) ( cid:176 ) ( cid:176 ) ( cid:191 ) ff ff ( cid:77 ) w k fi fi
( cid:173 ) ( cid:176 ) ( cid:176 ) ( cid:176 ) ( cid:174 ) ( cid:176 ) ( cid:176 ) ( cid:176 ) ( cid:175 ) ( cid:167 ) ( cid:167 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:169 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:169 ) ( cid:167 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:168 ) ( cid:169 )
( cid:167 ) ( cid:168 ) ( cid:168 ) ( cid:169 ) x x x x y x y y x j i x x k n n p n p q n n n p n n n ffi ffi ffi
,
,
, k p k p k p
1
1
1
1
1
1 p k p k fi fi fi
( cid:183 ) ( cid:184 ) ( cid:184 ) ( cid:185 )
( cid:183 ) ( cid:184 ) ( cid:184 ) ( cid:185 )
( cid:183 ) ( cid:184 ) ( cid:184 ) ( cid:185 )
( cid:167 ) ( cid:168 ) ( cid:168 ) ( cid:169 ) j w k
1 n
1 n
( 15 )
( cid:166 )
( cid:166 ) i ( cid:74)(cid:84 ) q ( cid:74)(cid:84 ) k
( cid:166)(cid:166 )
( cid:166 ) ( cid:166 ) ff i K w w k ff q K w w k
( cid:173 ) ( cid:176 ) ( cid:176 ) ( cid:176 ) ( cid:176 ) ( cid:176 ) ( cid:176 ) ( cid:174 ) ( cid:176 ) ( cid:176 ) ( cid:176 ) ( cid:176 ) ( cid:176 ) ( cid:176 ) ( cid:175 ) l where defined in Section 24 With ( 13 ) , ( 14 ) , and ( 15 ) , the KTMPM can be implemented according to AP , which is similar to the procedure listed in Table 2 . q(cid:74 ) is linear combination coefficients , like ff q K w w k
( cid:189 ) ( cid:176 ) ( cid:176 ) ( cid:176 ) ( cid:176 ) ( cid:176 ) ( cid:176 ) ( cid:190 ) ( cid:176 ) ( cid:176 ) ( cid:176 ) ( cid:176 ) ( cid:176 ) ( cid:176 ) ( cid:191 )
( cid:183 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:185 ) ( cid:183 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:184 ) ( cid:185 )
( cid:166 ) ( cid:166 ) ff p K w k q ( cid:74)(cid:84 ) k j ( cid:74)(cid:84 ) k
Figure 2 illustrates TMPM for learning with nth– order tensors as inputs . All the data in the figure come from the experiment section below . First , we have a set
1 n
( cid:183 ) ( cid:184 ) ( cid:184 ) ( cid:185 )
( cid:167 ) ( cid:168 ) ( cid:168 ) ( cid:169 ) fi p k
1
1 k p
,
. ffi n n n q p x i l x y x
We can easily generalize existing STL based convex optimization based machine learning and data mining algorithms . In this section , we generalize MPM to tensor MPM ( TMPM ) as an example . MPM is recently built and reported to achieve a top–level performance [ 3 ] . Moreover , MPM has a very strong probability theory foundation based on the powerful Marshall and Olkin ’s theorem [ 15 ] . TMPM aims at maximizing the probability of the correct classification for future data points according to : ff fi st
( cid:100 ) ( cid:100 ) k n
,
, b w k max
0,1
( cid:173 ) ( cid:176 ) ( cid:176 ) ff x ( cid:174 ) ( cid:176 ) ff ( cid:176 ) b y ( cid:175 ) k w fi k k w k k k w b fi w k k fi ff ( cid:78 ) fi ff ( cid:78 )
T w k ff
T w k ff x k w k y k w k w k fi w k fi
( 12 ) where fi ( cid:78 ) ff ff y k w k fi
1 1 yn n
( cid:166 ) y
1 j j y
, ff x k w k fi
1 xn ( cid:166 ) n x i
1 ix w k
, k w k k
, and x w k and k are the covariance matrices of x w k k and y y k w k w k respectively . k
Table 2 . Alternating procedure ( AP ) for tensor minimax probability machines ( TMPM ) . Input
: t A tensor minimax probability machine using the
Same as in Table 1 . u p t u O optimal tensor plane and the bias : ff fi g z z w 1 1
sign w 2 ff
2 n n w b fi
.
Step1 . Same as Step 1 in Table 1 . Step2 . Same as Step 2 in Table 1 .
1p , 2 , … , N or ( until converged ) , do :
For For
.
3 p e t S
.
4 p e t S
1l ,…,n , do : max
0,1 ff fi
( cid:100 ) ( cid:100 ) k n
, p , b w k
( cid:173 ) ( cid:176 ) ( cid:176)(cid:176 ) ff x ( cid:174 ) ( cid:176 ) ( cid:176 ) p b y k ( cid:176)(cid:175 ) ff k
1 p w k fi k
1 p w k k st p k p w b k fi p w k k fi ff ( cid:78 ) pT w k fi ff ( cid:78 ) pT w k ff ff x k
1 p w k y k
1 p w k p w k p w k fi fi
Find the optimal bias b for all the projected positive/negative training samples according to : max b x 1 i y
( cid:173 ) ( cid:176)(cid:176 ) ( cid:174 ) ( cid:176 ) b ( cid:176)(cid:175 ) ff fi
N w 1
1 j st fi ff ( cid:78 ) fi ff ( cid:78 )
N w n n
N w 2
2 N w 1
N w n
N w 2 b
2 n
The optimization problem for learning in ( 12 ) is a sequential second–order cone programming in convex
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 )
1550 4786/05 $20.00 © 2005 IEEE
In this STL based classification environment , two groups of images are separated from each other by a trained TMPM . Inputs ( representing features ) of TMPM are the regions of interest ( ROIs ) within the pictures , which are extracted by the attention model [ 6 ] and represented as 3rd–order tensors . of samples to train TMPM . We then get three 1st–order tensors as the projection orientations for classification . To represent the outer product of the three 1st–order tensors conveniently , we only do the outer product for the first and second projection 1st–order tensors , the first and third projection 1st–order tensors , and the tensors , second and respectively . The results of each production are shown as a 2–dimensional tensor plane . With these tensor planes , we can project the training or testing data onto . a real axis easily through After projections , we can calculate the optimal bias according to the criterion of the 0th–order TMPM . third projection 1st–order
( cid:79 ) i w n w 2 x i
1 w 1
2 n
Figure 3 . Attention model for classification .
The attention model [ 6 ] is capable of reproducing human–level performances for a number of pop–out tasks [ 16 ] . A target “ pops–out ” from its surroundings when it has it ’s a unique orientation , color , intensity , or size . Pop–out targets are always easily noticed by an observer . Therefore , utilizing the attention model to describe an image ’s semantic information is reasonable . As shown in Figure 3 , to represent an attention region from an image consists of several steps : 1 ) extract the salient map as introduced by Itti et al . in [ 6 ] ; 2 ) find the most attentive region , whose center has the largest value in the salient map ; 3 ) extract the attention region by a square ( called ROI ) in size of 64 64 ; and 4 ) finally , represent this ROI in the hue , saturation , and value ( HSV ) perceptual color space . Consequently , we have a 3rd–order tensor for the image representation .
Note that although we only select a small region from the image , the size of the extracted 3rd–order ; if we vectorize tensor is already as large as 64 64 3 it , the dimensionality of the vector will be 12288 . From the next subsection , we will be aware that the numbers of elements in the training and test sets are only of hundreds , much smaller than12288 .Therefore , the small samples size ( SSS ) problem is always met when a 3rd–order tensor is converted to a vector for input to a conventional learning machine . In contrast , our tensor oriented supervised learning scheme can reduce the SSS problem and at the same time represent the ROIs much more naturally .
42 Training/test data sets
Figure 2 . The proposed learning framework .
4 . Experimental results
With the established STL framework , TMPM is developed as an example of a generalized learning machine . In this section , its performance is examined with a binary image classification problem . The experimental results show that the STL version outperforms the original vector based version in terms of the ability for generalization and the stability for as referred to in different initial parameters ABSTRACT
1|n iw i
41 Sample binary classification problem
To categorize images into groups based on their semantic contents is a very important and challenging issue . The fundamental task is binary classification . A hierarchical structure can be built using a series of binary classifiers . As a result , this semantic image classification [ 19 ] can make the growing image repositories easy to search and browse [ 17 ] ; moreover , the semantic image classification is of great help for many other applications .
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 )
1550 4786/05 $20.00 © 2005 IEEE wrongly extracted ROIs were not excluded from each training set .
43 Binary classification performance
We carried out the binary classification ( “ Tiger ” and “ Leopard ” ) experiments on the above training/ test sets . The proposed tensor based TMPM algorithm is compared with the original MPM . The experimental results are shown in Table 3 . Error rates for both training and testing are reported , as the size of the training set ( STS ) increases from 5 to 30 .
From the training error rates in Table 3 , it can be seen that the traditional method ( MPM ) cannot learn a satisfactory model for classification when the size of the sample set is small . However , the learning algorithm TMPM under the proposed STL framework has a good characteristic on the volume control according to the computational learning theory and its real performances .
Also from Table 3 , based on the testing error rates of the comparative experiments , the proposed TMPM algorithm more effectively represents the intrinsic discriminant information ( in forms of 3rd–order ROIs ) . TMPM learns a better classification model for unseen data classification than MPM and thus has a better performance on the testing set . It is observed that the TMPM error rate is a decreasing function of the size of the training set . This is consistent with statistical learning theory .
5 . Experimental–based convergence and stability analysis
In this section we study two important issues in machine the convergence property and the insensitiveness to the initial values . learning and data mining , namely ,
We carry out a series of experiments based on the same database employed in Section 4 . Experimental results prove that TMPM converges well . In addition , it is insensitive to the initial values and thus has a good stability . x i
1 n
2 w 1 w n of
increasing number of
Figure 6 shows tensor projected position values ( cid:79 ) the original general w 2 i tensors with an learning iterations using 10 training samples for each class . We find that the projected values converge to stable values . As shown in Figure 6 , five to six iterations are usually enough to achieve convergence .
Figure 7 shows TMPM is insensitive to the initial values . Many learning algorithms converge to different local minima with different initial parameter values for
The training set and the test set for the following experiments are built upon the Corel photo gallery [ 17 ] , from which 100 images are selected for each of the two sets . These 200 images are processed to extract the 3rd–tensor attention features for TMPM .
Figure 4 . Some successful attention ROIs .
Figure 5 . Some unsuccessful attention ROIs .
We choose the “ Tiger ” category and the “ Leopard ” category for binary classification experiments since it is a very difficult task for a machine to distinguish between “ Leopard ” classification is carried out in the next subsection . We choose the top N images as a training set according to the image IDs , while all the images are used to form the corresponding test set . them . The
“ Tiger ” and
Table 3 . TMPM vs . MPM .
Training Error Rate MPM TMPM 0.0000 0.4000 0.5000 0.0000 0.4667 0.0667 0.5000 0.0500 0.4800 0.0600 0.1167 0.5000
STS
5 10 15 20 25 30
Testing Error Rate MPM TMPM 0.4600 0.5050 0.4900 0.4250 0.4150 0.3250 0.4800 0.2350 0.4650 0.2400 0.2550 0.4600
We introduced 3rd–order tensor attention ROIs , which can mostly be found correctly from the images . Some successful results , respectively extracted from the “ Tiger ” category and the “ Leopard ” category , are shown in Figure 4 . By this means , the underlying data structures are well kept for the next step : classification . However , we should note that the attention model sometimes cannot depict the semantic information in an image . This is because the attention model always locates its surroundings and thus might be “ cheated ” when some complex or bright background exists . Some unsuccessful ROIs in the “ tiger ” ( top ) and “ leopard ” ( bottom ) categories are shown in Figure 5 . It should be emphasized that in order to keep the following comparative experiments fair and automatic , these is different region from that the
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 )
1550 4786/05 $20.00 © 2005 IEEE iw . This is the so–called local minimum problem . However , our new TMPM does not slump into this local minimum problem , which is proved by a set of experiments , with different initial parameters , 10 learning samples . Theoretically , this is also true , because each sub– optimization problem to optimize iterations , and 20 training iw is convex .
Tensor MPM is convergence on the training data
References reduces meaningfully and directly by using the tensor based representation , which the number of parameters in the learning procedure ; 2 ) it makes a better use of the information on the input than vector based discriminant analysis , efficiently ; 3 ) it converges within a few training iterations ; and 4 ) it is insensitive to the initial parameter values . The stability of the proposed scheme has also been analyzed .
[ 1 ] S . Boyd and L . Vandenberghe , Convex Optimization .
Cambridge University Press , 2003 .
[ 2 ] V . Vapnik , The Nature of Statistical Learning Theory .
Springer–Verlag , New York , 1995 .
[ 3 ] G . Lanckriet et al . , “ A Robust Minimax Approach to
Classification , ” JMLR , 3 , 555–582 , 2002 .
[ 4 ] J . Li and J . Wang , “ Automatic Linguistic Indexing of Pictures by a Statistical Modeling Approach , ” IEEE Trans . PAMI , 25(9 ) , 1075–1088 , 2003 .
[ 5 ] M . Turk and A . Pentland , “ Face recognition using eigenfaces , ” in Proc . of IEEE CVPR , 586–591 , 1991 .
[ 6 ] L . Itti et al . , “ A Model of Saliency–Based Visual Attention for Rapid Scene Analysis , ” IEEE Trans . PAMI , 20(11 ) , 1254–1259 , 1998 .
[ 7 ] S . Sarkar et al . , “ The Human ID Gait Challenge Problem : Data Sets , Performance , and Analysis , ” IEEE Trans . PAMI , 27(2 ) , 162–177 , 2005 .
[ 8 ] S . Chang , “ The Holy Grail of Content–Based Media Analysis , ” IEEE Multimedia Magazine , 9(2 ) , 6–10 , 2002 . [ 9 ] R . Bellman , Adaptive Control Processes : A Guided Tour .
Princeton University Press , 1960 .
[ 10 ] M . Anthony et al . , Computational Learning Theory .
Cambridge Univ . Press , 1997 .
[ 11 ] R . O . Duda et al . , Pattern Classification . 2/e , Wiley . [ 12 ] G . Lanckriet et al . , “ Learning the Kernel Matrix with
Semidefinite Programming , ” JMLR , 5 , 27–72 , 2004 .
[ 13 ] A . Shashua and A . Levin , “ Linear Image Coding for Regression and Classification Using the Tensor–rank Principle , ” in Proc . of IEEE CVPR , 2001 .
[ 14 ] L . Lathauwer , Signal Processing based on Multilinear
Algebra . PhD Thesis , Universiteit Leuven , 1997 .
[ 15 ] A . Marshall , and I . Olkin , “ Multivariate Chebyshev Inequalities , ” Annals of Mathematical Statistics , 31(4 ) , 1001–1014 , 1960 .
[ 16 ] A . Treisman and G . Gelade , “ A Feature–Integration Theory of Attention , ” Cognitive Psychology , 12(1 ) , 97– 136 , 1980 .
[ 17 ] J . Wang et al . , “ SIMPLIcity : Semantics–Sensitive Integrated Matching for Picture Libraries , ” IEEE Trans . PAMI , 23(9 ) , 947–963 , 2001 .
[ 18 ] D . Tao et al . , Tensor Rank One Discriminant Analysis . Tech . Rep . , Birkbeck College , University of London , Nov . 2004 .
[ 19 ] A . Vailaya et al . , “ On Image Classification : City Images vs . Landscapes , ” Pattern Recognition , 31(12 ) , 1921– 1935 , 1998 .
1|n i
1
1.5
2 n o i t i s o P
2.5
3
3.5
4
0
10
20
30
40 60 Number of Iterations
50
70
80
90
100
Figure 6 . TMPM converges effectively .
Tensor Minimax Probability Machine is stable with different initial values
0.25
0.2
0.15 t e a R r o r r
E
0.1
0.05
0
0
10
20
30
40 60 Experimental Rounds
50
70
80
90
100
Testing Error Rate Training Error Rate
Figure 7 . TMPM is stable with different initial values in 10 learning iterations ( 20 training samples in each class ) .
6 . Conclusion
In this paper , we have developed a supervised tensor learning ( STL ) framework to generalize convex optimization based schemes so that they accept nth– order tensors as inputs . We have also studied the ( TR1DA ) tensor rank–one discriminant analysis method for extracting features from tensors . Under this STL framework , a novel approach called tensor minimax probability machines ( TMPM ) has been developed learn a series of projections for classification . TMPM has the following properties : 1 ) it can reduce the curse of dimension problem to
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 )
1550 4786/05 $20.00 © 2005 IEEE
