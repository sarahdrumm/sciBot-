Discriminant Analysis : A Unified Approach
Peng Zhang & Jing Peng
Tulane University
Electrical Engineering & Computer Science Department
New Orleans , LA 70118
{zhangp,jp}@eecstulaneedu
Norbert Riedel
Tulane University
Mathematics Department New Orleans , LA 70118 riedel@mathtulaneedu
Abstract size . This results in maximizing
Linear discriminant analysis ( LDA ) as a dimension reduction method is widely used in data mining and machine learning . It however suffers from the small sample size ( SSS ) problem when data dimensionality is greater than the sample size . Many modified methods have been proposed to address some aspect of this difficulty from a particular viewpoint . A comprehensive framework that provides a complete solution to the SSS problem is still missing . In this paper , we provide a unified approach to LDA , and investigate the SSS problem in the framework of statistical learning theory . In such a unified approach , our analysis results in a deeper understanding of LDA . We demonstrate that LDA ( and its nonlinear extension ) belongs to the same framework where powerful classifiers such as support vector machines ( SVMs ) are formulated . In addition , this approach allows us to establish an error bound for LDA . Finally our experiments validate our theoretical analysis results .
1 Introduction
The purpose of this paper is to present a unified theoretical framework for discriminant analysis . Discriminant analysis [ 6 , 10 ] has been successfully used as a dimensionality reduction technique for many classification problems . According to Fisher ’s criterion , one has to find a projection matrix W that maximizes :
J(W ) =
|W T SbW| |W T SwW|
( 1 ) where Sb and Sw are so called between class and withinclass matrices , respectively . In practice , the “ small sample size ” ( SSS ) problem is often encountered , where Sw is singular . Therefore , the maximization problem can be difficult to solve .
To address this issue , the term εI is added , where ε is a small positive number and I the identity matrix of proper
J(W ) =
|W T SbW|
|W T ( Sw + εI)W| .
( 2 )
It can then be solved without any numerical problems .
In [ 9 ] , Friedman discusses regularized discriminant analysis with regard to the small sample size problem . Equation ( 2 ) is a special case of his regularized discriminant analysis in practice . However , Friedman leaves open the question of whether an optimal choice for the parameter ε exists , and if so , whether this optimal choice is a unique one ? Further , Friedman ’s analysis is based on traditional parametric methods in statistics , and Gaussian distributions for underlying random variables are often assumed . That is , his regularization technique is one of improving the estimation of the covariance matrices that are possibly biased due to the small data size . The validity of this technique can be very limited . For example , the nonlinear case of discriminant analysis , called generalized discriminant analysis ( GDA ) , becomes very popular in practice and shows promising potential in many applications . The idea is to map the data into a kernel induced feature space and then perform discriminant analysis there . However , in a high and possible infinite dimensional feature space , GDA is not well justified from Friedman ’s point of view . Thus , a rigorous justification for the regularization term εI is still missing , especially when GDA is considered .
In [ 7 ] , Evgeniou , Pontil and Poggio present a theoretical framework for the regularization functional ( the so called regularization network ) l i=1
H[f ] = min f
1 l
( yi − f(xi))2 + λ||f||2 K ,
( 3 ) where ||f||2 K is a norm in a Reproducing Kernel Hilbert Space H defined by the positive definite function K , l is the number of data points or examples and λ is a regularization parameter . In their paper they justify the above functional using statistical learning theory that is mostly developed by Vapnik [ 22 ] .
H[f ] = 1 l
In this paper , we will show that discriminant analysis can be cast in the framework of statistical learning theory . l More specifically , Fisher ’s criterion ( 1 ) is equivalent to the regularization network ( 3 ) without regularization , which is , i=1(yi − f(xi))2 . And Equation ( 2 ) is min f equivalent to the regularization network ( 3 ) with the parameter ε corresponding to the regularization term λl . Thus the role of εI is well justified by statistical learning theory . Further we show that the optimal choice of λ exists , and that it is unique . Additionally we establish an error bound for the function that discriminant analysis tries to find .
1.1 Related Work
The relationship between least squares regression and Fisher ’s linear discriminant has been well known for a long time . There is a good review in [ 6 ] . Actually in the seminal paper [ 8 ] , Fisher already pointed out its connection to the regression solution . In [ 17 ] , Mika also provides an extensive review of LDA . Another widely used technique in statistics , canonical correlation analysis , is also closely related to least squares regression [ 1 ] .
The regularized least squares ( RLS ) methods have been studied for a long time , under different names . In statistics , ridge regression [ 12 ] has been very popular for solving badly conditioned linear regression problems . After Tikhonov published his book [ 21 ] , it was realized that ridge regression uses the regularization term in Tikhonov ’s sense . In the 1980 ’s , weight decay was proposed to help prune unimportant neural network connections , and was soon recognized [ 13 ] that weight decay is equivalent to ridge regression . Recently this old method was found essential in the framework of statistical learning theory , labeled by different names ( RLS [ 18 ] , Regularized Network [ 7 ] , least squares SVMs [ 19 ] , and proximal SVMs [ 11] ) . It is demonstrated that this regression method is fully comparable to SVMs when used as a classifier [ 18 , 24 ] . Most recently , an error bound for RLS given a finite sample data set was developed in [ 5 ] .
Many elements discussed in this paper are scattered throughout the literature and may look familiar to some audience but not to others . The fact is that most are not comprehensive enough for this topic and unclear issues pop up for many . For example , many researchers such as Mika [ 17 ] apply the regularization technique to address the SSS problem but they still follow Friedman ’s principle that is quite limited . It is our goal to adopt a simple yet unified approach to making this topic more comprehensive .
2 Discriminant Learning Analysis
In this section , we first review LDA using Fisher ’s criterion , and then go on to study it from a learning theory point of view , which we call discriminant learning analysis ( DLA ) .
2.1 Linear Discriminant Analysis j=1 i=1(xj
J lj i − mj)(xj
In LDA , within class , between class , and mixture scatter matrices are used to formulate the criteria of class separability . Consider a J class problem , where m0 is the mean vector of all data , and mj is the mean vector of j th class data . A within class scatter matrix characterizes the scatter of samples around their respective class mean vectors , and i − mj)T , it is expressed by Sw = where lj is the size of the data in the j−th class . A betweenJ class scatter matrix characterizes the scatter of the class means around the mixture mean m0 . It is expressed by j=1 lj(mj − m0)(mj − m0)T . The mixture scatSb = l ter matrix is the covariance matrix of all samples , regardless of their class assignments , and it is given by Sm = i=1(xi − m0)(xi − m0)T = Sw + Sb . Fisher ’s criterion is used to find the projection matrix that maximizes ( 1 ) . In order to determine the matrix W that maximizes J(W ) , one can solve the generalized eigenvalue problem : Sbwi = λiSwwi . The eigenvectors corresponding to the largest eigenvalues form the columns of W . For a two class problem , it can be written in a simpler form :
Sww = m = m1 − m2 ,
( 4 ) where m1 and m2 are the means of the two classes .
2.2 LDA as Least Squares Function Estimation
LDA is a supervised subspace learning problem where we are given a set z = {(xi , yi)}l i=1 of l training examples drawn iid from the probability space Z = X × Y . Here probability measure ρ is defined , and xi are the ndimensional inputs . We first consider two class problems where we have yi ∈ {−1 , 1} as class labels . Without loss of generality , we assume x has zero mean , ie , E(x ) = 0 . Let us consider : fM SE = arg min f
1 l
( yi − f(xi))2 .
( 5 ) l i=1 l i=1
This is a mean squared function estimation problem . Here we first consider the hypothesis space HL , the function class of linear projections . That is , HL = {f|f(x ) = wT x , x ∈ X} . Thus we will solve wopt = arg min w
1 l
( yi − wT xi)2 .
( 6 )
It turns out that the solution of ( 6 ) is the same as the solution obtained by Fisher ’s criterion .
Lemma 1 The linear system derived by the least squares criterion ( 6 ) is equivalent to the one derived by Fisher ’s criterion ( 1 ) , up to a constant , in two class problems .
Proof : We rewrite ( 6 ) as : l i=1
( yi − wT xi)2 = ( y − X T w)T ( y − X T w ) = ( yT − wT X)(y − X T w ) = l − 2(l1m1 − l2m2)T w + wT Smw . We use the fact that Xy = l1m1 − l2m2 . Taking the derivative with respect to w and setting the result to 0 we obtain
Smw = ( l1m1 − l2m2 ) .
( 7 )
This is equivalent to Equation ( 4 ) . When Sw is full rank , three equations Smw = ( l1m1 − l2m2 ) , Smw = m1 − m2 , and Sww = m1 − m2 have the same solution w up to a constant factor , given that the overall mean is 0 . For details see Appendix A .
2.3 Interpretation of LDA within Learning The ory
When both classes are gaussian distributed , the LDA can be shown optimal . However more often in reality they are not , then it is not clear how good the LDA is . The conventional view of LDA is limited here . The learning framework , on the other hand , is more general . There is no need to assume the distribution of the data to discuss the ‘goodness’ . Instead , the learning approach directly aims to find a good function mapping and the goodness is well defined ( see Appendix C and ?? ) .
In a general sense , LDA tries to find a transformation f such that the transformed data have a large class mean difference and small within class scatters . The equivalence between ( 6 ) and ( 1 ) can be understood in another way : one minimizes the mean squared error with respect to each class mean while keeping the mean difference constant . We define fρ as the best function that minimizes the mean squared error . That is fρ = arg min f
Z
( y − f(x))2dρ .
( 8 )
Given a set of data z , one tries to learn the function fz that is as close as possible to the optimal mapping fρ . That is , Z(fz − fρ)2dρ . However , the probabilfopt = arg min fz ity measure ρ is unknown and so is fρ . Instead , we may consider ( 5 ) that is called the empirical error ( risk ) minimization . Because ( 6 ) is equivalent to Fisher ’s criterion , it is clear that Fisher ’s criterion leads to an empirical error minimization problem . From the statistical learning theory point of view , however , without controlling the function norm , solving equation ( 5 ) often leads to overfitting data and the solution is not stable . And when the sample size is smaller than the dimensionality , it is an ill posed problem and the solution is not unique . That is where diverse LDA techniques have been developed to solve the SSS problem .
2.4 Discriminant Learning Analysis as Regular ized Least Squares Regression
Following [ 7 ] , we instead minimize over a hypothesis space H the following regularized functional for a fixed positive parameter λ : fDLA = arg min f∈H
1 l
( yi − f(xi))2 + λ||f||2 H .
( 9 ) l i=1 l i=1
Again , we consider the linear projection function space HL . In this case , ||f||2 = wT w .
1 l
( yi − wT xi)2 + λwT w . wopt = arg min w l i=1(yi − wT xi)2 + λlwT w = l − 2(l1m1 − Thus , l2m2)T w + wT ( Sm + λlI)w . Taking the derivative with respect to w and setting the result to 0 , we have ( Sm + λlI)w = ( l1m1 − l2m2 ) .
( 10 )
( 11 )
In Appendix A , it is shown that ( 11 ) is equivalent to
( Sw + λlI)w = m ,
( 12 ) and therefore solving it is equivalent to maximizing ( 2 ) . We call this approach discriminant learning analysis . Since for any given positive regularization term λ , ( 9 ) always has a unique solution , we have
Proposition 2 Discriminant learning analysis is a wellposed problem .
2.5 Nonlinear DLA
In the new framework of discriminant learning analysis , the nonlinear case is not an “ extension ” of the linear case by the “ kernel ” trick . In the above analysis we have adopted the linear hypothesis space HL , whereby we have been looking for an optimum solution in that space . If we choose a general Reproducing Kernel Hilbert Space ( RKHS ) as our hypothesis space , we will obtain nonlinear function mappings as the subspace . The derivation of DLA in a general function space is simply carried out in terms of function analysis , in a similar way as in linear space . Here we skip the details of the derivations . Briefly speaking , for a two class problem , the nonlinear DLA solution is found by following a simple algorithm [ 18 ]
( K + λlI)c = y ,
( 13 ) where K is the Gram matrix of the training data , and K = [ k(xi , xj ) ] where k is the kernel function . y is the vector of class labels . After solving c = [ c1 , , cl ] , any given new data x will be projected onto the subspace by s = l i=1 cik(xi , x ) .
2.6 Error Bound for DLA
By setting the discriminant learning analysis in the framework of statistical learning theory , we can provide an error bound for fDLA obtained by ( 9 ) in the following theorem : Theorem 3 Let fρ be defined by ( 8 ) , with confidence 1− δ , the error bound for the solution fDLA of ( 9 ) is given by :
( fDLA − fρ)2dρX ≤ S(λ ) + A(λ ) ,
( 14 ) where A(λ ) and S(λ ) are given by λ1/2L 32M 2(λ+Ck)2 minimizes S(λ ) + A(λ ) .
− 1 k fρ2 and 4 v∗(l , δ ) . And there exists an unique λ that
λ2
− 1 4 k
This decomposition is often called bias variance tradeoff . A(λ ) is called the approximation error . Given a hypothesis space , it measures the ‘error’ between the optimal in A(λ ) is simply an function in it and the true target . L operator . S(λ ) bounds the sample error and is essentially derived by the big number law . M and Ck are constants and v∗(l , δ ) is the unique solution of an equation whose coefficients contain sample size l and confidence parameter δ . Due to its complex nature , we do not provide the details of the proof , which can be found in [ 5 ] . To the best of our knowledge , this is the first known error bound for LDA .
3 Multi class DLA mapping from vector x to vector y ∈ Rd . This direct approach has some difficulties . First , statistical learning theory is concerned with only the first kind of function mapping f : Rn → R , and cannot be directly extended to f : Rn → Rd . Second , subspace dimensionality d is not fixed ( rather it is often provided by the user ) . These difficulties make the attempt at directly casting DLA in the multiple class case as a least squares estimation problem a very difficult task .
Notice that LDA in a multiple class problem can be decomposed into l two class problems . In the i th two class problem , it treats the i th class as one class and all remaining classes as the second class . Each binary class problem is solved first , and after finding all subspaces , PCA is applied to find eigenvectors having the largest eigenvalues . These new eigenvectors are the solution of the original multi class LDA problem ( See Appendix B ) .
Thus , DLA can be naturally extended to the multi class case . Simply , in the decomposition step , we replace two class LDA by two class DLA . In the linear case , it turns out that the DLA algorithm in the multi class case simply solves
SbW = ( Sw + λlI)W Λ .
( 15 )
This is equivalent to solving ( 2 ) .
4 Discussions
In this section , we will analyze the role of the regularization term λ . We are especially interested in comparing the DLA algorithm ( 15 ) to various LDA algorithms , such as PCA+LDA [ 2 , 20 ] , scatter LDA [ 15 , 10 ] , newLDA [ 3 ] and DLDA [ 23 ] . These techniques are mostly proposed for solving facial recognition problems where the SSS problem always occurs . We summarize them briefly :
PCA+LDA : Apply PCA to remove the null space of Sw first , then maximize J(W ) = |W T SbW| |W T SwW| .1
Scatter LDA : Same as PCA+LDA but maximizing
J(W ) = |W T SbW|
|W T SmW| instead .
We have presented discriminant learning analysis as an alternative approach to LDA in two class problems . The data is projected onto only one dimension that is adequate for the two class problem . However , LDA is generally used to find a subspace with d dimensions for a multiple class problem . In this section we extend DLA to the multi class case . For a two class problem , the function f is a mapping from a vector x to a scaler y ∈ R . If we want to find a subspace with d > 1 dimensions , we actually consider a newLDA : If Sw is full rank then solve regular LDA ; else in the null space of Sw , find the eigenvectors of Sb with largest eigenvalues .
DLDA : Apply PCA to remove the null space of Sb first , then find the eigenvectors of Sw corresponding to the smallest eigenvalues .
1Some researchers also maximize J(W ) = lent to maximizing J(W ) =
|W T SbW| |W T SwW| .
|W T SmW| |W T SwW| , it is equiva
It should be noted that PCA+LDA and Scatter LDA can be equivalent when Sw and Sm span the same subspace . However , they are different when Sb totally or partially spans the null space of Sw , thus Sw and Sm span different subspaces . For facial images the latter case turns out to be more common . In [ 3 ] , Chen et al . prove that the null space of Sw contains discriminant information . They also show that Scatter LDA is not ‘optimal’ in that it fails to distinguish the most discriminant information in the null space of Sw . Thus they proposed the newLDA method . However , newLDA fell short of making use of any information outside of that null space . DLDA , on the other hand , discards the null space of Sb . This is very problematic . First , it fails to distinguish the most discriminant information in the null space of Sw , as in Scatter LDA . Second , the null space of Sb is not completely useless . This can be seen in a two class case . DLDA simply chooses m as the subspace . However this can be biased because it ignores Sw .
It should be noted that in [ 16 ] there is a misleading claim , where the authors mention that there is no significant difference between newLDA and DLDA . This incorrect claim is also pointed by others [ 14 ] . In a word , all these techniques make “ hard ” decisions , either discarding a null space , or only working in a null space . In the following , we can see that DLA always works in the whole space , not making any “ hard ” decisions . Instead , it ‘fuses’ information from both subspaces . DLA can work in the whole space because it is well posed . We first consider how LDA is solved . For the moment let us assume that Sw is full rank , and thus S−1 w exists . For symmetric matrix Sw there is a matrix U such that w = UΣ−1U T , where U T U = I , Sw = UΣU T , and ST Σ is a diagonal matrix . U T is a transform matrix . Consider the following in a two class LDA w = S−1 w m = UΣ−1U T m .
( 16 )
The between vector m is transformed to a new basis by U T , then multiplied by a constant 1 along each dimension , and Σii finally transformed again by U back to the origin basis .
( cid:183 )
( cid:184 )
( cid:183 )
1 and x
( cid:184 )
To illustrate this process . We consider a simple case where the data are in two dimensions . x1 and x2 are the original axis bases , as shown in Figure 1 . x 2 are the orthogonal axis bases in which Sw is diagonalized . We choose Sw such that U T SwU = Σ = . Thus Σ−1 11 = 1 and m multiplying Σ−1 , m∗ solution , S−1 “ LDA ” .
= U T m . Then m 1 2 , respectively . By 2 = 2m 2 . The LDA w m is shown in Figure 1 (a ) and denoted by
2 , and Σ−1 2 are the projection on x
22 = 2 . Let
2 0 0 0.5
1 m m 1 and x 1 and m∗
2
1 = 1
2 m
Now consider DLA : ( Sw + λlI)w = m . Notice that if matrix U can diagonalize Sw then it can diagonal
( a ) The solution of LDA and DLA
( b ) The solution of DLA when Sw ’s eigenvalue is zero in one dimension
Figure 1 . Illustration and comparison of solutions by LDA , DLA , DLDA and newLDA . ize ( Sw + λlI ) at the same time : U T ( Sw + λlI)U = U T SwU + λlU T U = Σ + λlI . Thus , ( Sw + λlI)−1 = U(Σ + λlI)−1U T . To illustrate the effect of the regularization term , we choose λl = 1 and in Fig 1 , Λi are the eigenvalues of Sw + λlI . Now Λ−1 2 = 2 3 . The vector ( Sw + λlI)−1m is shown in Fig 1 (a ) and denoted by “ DLA ” .
3 , and Λ−1
1 = 1
We now consider the case where the SSS problem occurs . We keep Λ2 = 1.5 the same but Λ1 = 1 ( that is , the original eigenvalue of Sw is 0 along this dimension ) . This case is plotted in Fig 1 (b ) . DLA takes advantage of information from both worlds . However , PCA+LDA will throw away the dimension x1 and only keep the direction x2 . Thus it ignores critical discriminant information . newLDA , on the other hand , will only keep the dimension x1 and throw away any discriminant information along x2 . DLDA simply chooses m .
Thus , when the SSS problem does not occur , DLA has a smoothing effect on the eigenvalues of Sw . If the SSS problem does occur , DLA fuses and keeps a balance between the information both in and out of the null space of Sw . This balance is achieved by choosing proper λ .
5 Experiments
5.1 Feret Facial Images
We tested DLA algorithm against PCA+LDA , newLDA , DLDA and Scatter LDA on FERET facial image data ( http://wwwitlnistgov/iad/humanid/feret/ ) We extracted 400 images for the experiment , where there are 50 individuals with 8 images from each . The images have been preprocessed and normalized using standard methods from the FERET database . The size of each image is 150 x 130 pixels , with 256 grey levels per pixel . x1 x2 x’2 x’1 m L2−1=2/3 L1−1=1/3 LDA DLAx1 x2 x’2 x’1 m , DLDA L2−1=2/3 L1−1=1 LDA DLAnewLDA We randomly choose five images per person for training , and the remaining three for testing . Thus the training set has 250 images while the testing set has 150 . Subspaces are calculated from the training data , and the 1 NN classifier ( 3 NN failed for all the methods due to five images per person in the training data ) is used to obtain the accuracy rates after projecting the data onto the subspace . To obtain average performance , each method is repeated 10 times . Thus there are total 1,500 testing images for each method . The regularization term λl is chosen by 10 fold cross validation . The average accuracy rates are shown in Figure 2 . The Xaxis represents the dimensionality of the subspace . For each technique , the higher the dimension , the less discriminant the dimension . For most techniques , the accuracy rates increase quickly in the first 15 dimensions , and then increase slowly with additional dimensions .
DLA is uniformly better than any other algorithms , demonstrating its efficacy . It achieves the highest accuracy rate of 0977 newLDA performs quite well in these experiments , again demonstrating that the most discriminant information is in the null space of Sw , for the facial recognition task . On the other hand , Scatter LDA does not perform well at lower dimensional subspaces . But it eventually performs better than PCA+LDA , when dimensions are more than 42 . All methods achieve their highest accuracy rate with a 49 dimensional subspace , which is not surprising , for this is a 50 class problem . It is noticed that the performance of newLDA and Scatter LDA ( its tail is not shown in the plot ) drops quickly with unnecessary dimensions .
Figure 2 . Comparison of DLA , LDA , newLDA , DLDA and Scatter LDA on the FERET image data . data sets breast cancer cancer Wisconsin credit heart Cleve heart Hungary ionosphere new thyroid pima glass sonar iris average
GDA NDLA Better ? 0.449 0.033 0.20 0.214 0.236 0.088 0.030 0.310 0.088 0.162 0.073 0.171
0.30 0.038 0.174 0.185 0.216 0.077 0.029 0.294 0.068 0.170 0.065 0.147
√ × √ √ √ √ √ √ √ × √ √
Table 1 . Classification error rates in subspaces computed by GDA and NDLA , using 3 nn classifier , on 11 UCI data sets .
5.2 UCI Data Sets
In these experiments , we compare nonlinear LDA ( GDA ) and nonlinear DLA ( NDLA ) in two class classification problems . We did not test other nonlinear techniques such as KDDA [ 16 ] due to their complex implementation . The implementation of kernel DLDA is very involved . We use 11 data sets from the UCI machine learning database . They are all two classification problems . For each data set , we randomly choose 60 % as training and the remaining 40 % as testing . We train GDA and NDLA on the training data and obtain projections . We then project both training and test data on the chosen subspace and use the 3 NN classifier to obtain error rates . Note that for the two class case , one dimensional subspace is sufficient . We use the Gaussian kernel k(x1 , x2 ) = exp −||x1−x2||2 to induce the Reproducing Kernel Hilbert Space for both GDA and NDLA . The kernel parameter σ and regularization term λ were determined through 10 fold cross validation . We repeat the experiments 10 times on each data set to obtain the average error rates .
σ2
The results are shown in Table 1 . On 9 data sets out of 11 , NDLA performs better than GDA . Especially on the breast cancer credit , heart cleve , heart Hungary and glass data , it is significantly better than GDA , with 95 % confidence .
6 Summary
This paper presents a learning approach , Discriminant Learning Analysis for effective dimension reduction . While discriminant analysis is formulated as a least square approx
051015202530354045505509091092093094095096097098Dimensionsaccuracy rateDLAnewLDADLDAPCA+LDAScatter−LDA imation problem , the DLA is a regularized one . Regardless of the data distribution , the approach allows us to always measure the goodness of mapping function that DLA computes : an error bound for it is established . The commonly adopted technique : Sw + εI is well justified in the framework of statistical learning theory . The existence and uniqueness of the term ε is obtained . The DLA is also compared to many other competing algorithms and it is demonstrated both theoretically and experimentally , that DLA is more robust than others .
Appendix A
In this appendix , we say Aw = c1v and Aw = c2m are equivalent in the sense that the solution ws are in the same direction , where A is a matrix , cis are scalers and m is a vector . First we show that Smw = m and Sww = m have the same solution ( same set of eigenvectors ) , where m = m1 − m2 is the mean difference of the two class problem . We know that solving Sww = m is equivalent to solving
[ 6 ]
S−1 w SbΦ = ΦΛ
( 17 ) where Φ and Λ are the eigenvector and eigenvalue matrices w Sb . Since we have Sw = Sm − Sb , following [ 10 ] of S−1 ( pp . 454 ) , ( 17 ) can be converted to ( Sm − Sb)ΦΛ = SbΦ
SmΦΛ = SbΦ(I + Λ ) S−1 m SbΦ = ΦΛ(I + Λ)−1 .
( 18 )
1+α1
≥ ≥ αn
This shows that Φ is also the eigenvector matrix of S−1 m Sb , and its eigenvalue matrix is Λ(I + Λ)−1 . When the components of Λ and αi are arranged from the largest to the smallest as α1 ≥ ≥ αn , the corresponding components of Λ(I + Λ)−1 are also arranged as α1 . 1+αn That is , the t eigenvectors of S−1 m Sb corresponding to the t largest eigenvalues are the same as the t eigenvectors of S−1 w Sb corresponding to the t largest eigenvalues . As a special case ( two class problems ) , Smw = m and Sww = m share the same “ eigenvector ” solution . Now we show that Smw = ( l1m1 − l2m2 ) and Smw = m1 − m2 share the same solution also . Consider that the overall mean m0 is 0 . lm0 = l1m1 + l2m2 = 0 , we have l m , and l1m1 − l2m2 = 2l1l2 m1 = l2 l m . Thus Smw = ( l1m1 − l2m2 ) becomes Smw = 2l1l2 l m . With a constant c , the solution of Smw = cm is still in the same direction along the mean difference m , and thus is equivalent to solving S−1 It is easy to show that ( Sm + λlI)w = ( l1m1 − l2m2 ) is equivalent to ( Sw+λlI)w = m1−m2 , simply replacing Sw and Sm by Sw + +λlI and Sm + λlI in the above analysis . l m , m2 = − l1 w SbΦ = ΦΛ .
Appendix B : Multi Class LDA
In Appendix A , we showed that for a two class problem , LDA and DLA correspond to solving ( 4 ) and ( 12 ) , respectively . Sm can be written as Sm = UmΛ1/2 m , ( 4 ) mm ≡ v . Note that can be written as Λ1/2 v can be treated as a “ data point . ” mw = Λ−1/2 m Λ1/2 m U T m U T m U T
For a multi class problem , Assuming Sm is full rank , LDA corresponds to the system SbW = SmW Λ . It can be write as m U T m U T
NbV = V Λ , ( 19 ) mW , Nb = Λ−1/2 mSbUmΛ−1/2 where V = Λ1/2 m . This is a simple eigenvalue problem . By solving V , we can compute W by W = UmΛ−1/2 m V . The eigenvectors in V with the largest eigenvalues have a one to one J correspondence to the eigenvectors in W with the corresponding largest eigenvalues in the original equation . j=1 lj(mj − m0)(mj − m0)T = Note that Sb = j=1( lj ˜mj)( Nb = Λ−1/2 lj ˜mj)T and Nb is
J m U T
J
=
( j=1 mSbUmΛ−1/2 ljΛ−1/2 m U T m m ˜mj)( ljΛ−1/2 m U T m ˜mj)T .
( 20 ) ljΛ−1/2 m U T m ˜mj as a point , then If we treat every vector Nb is the covariance of these J points . Equation ( 19 ) can also be viewed as a PCA problem on these J points . In the above analysis if we replace Sm by Sm + λlI , we obtain that DLA corresponds to solving SbW = ( Sm+λlI)W Λw . Further
SbW = ( Sw + λlI)W Λw .
( 21 )
The eigenvectors corresponding to the largest eigenvalues are chosen as the DLA subspace basis .
Appendix C : Overview of Statistical Learning Theory
The first step in the development of learning theory is the assumption of existence of a probability measure ρ on the product space Z = X × Y , from which the data are drawn . One way to define the expected error of f is the least
Z(f(x ) − y)2dρ for f : X → Y . squares error : ε(f ) = We try to minimize this error by “ learning . ” Define fρ(x ) = Y ydρ(y|x ) , where ρ(y|x ) is the conditional probability measure on Y wrt x . It has been shown [ 4 ] that the expected error ερ of fρ represents a lower bound on the error ε(f ) for any f . Hence at least in theory , the function fρ is the ideal one and so is often called the target function . However , since measure ρ is unknown , fρ is unknown as well .
Starting from the training data z = {(xi , yi)}l i , statistical learning theory as developed by Vapnik [ 22 ] builds on the so called empirical risk minimization ( ERM ) induction principle . Given a hypothesis space H , one attempts at minimizing
( fz(xi ) − yi)2 , where fz ∈ H ,
( 22 ) l i=1
1 l l i=1
1 l
However , in general , without controlling the norm of approximation functions , this problem is ill posed . Generally speaking , a problem is called well posed if its solution exists , and is unique and stable ( depends continuously on the data ) . A problem is ill posed if it is not well posed .
Regularization theory [ 21 ] is a device that was developed to turn an ill posed problem into a well posed one . The crucial step is to replace the functional in ( 22 ) by the following regularized functional
( f(xi ) − yi)2 + λ||f||2
K , f ∈ H ,
( 23 ) where λ is a regularization term , and ||f||2 K the norm of ρ(X ) . Now minimizing ( 23 ) makes it well posed f in L2 and can be solved by elementary linear algebra . Let fλ,z be the minimizer of ( 23 ) , the question then is : How good an approximation is fλ,z to fρ , or how small is ε(fλ,z ) = X(fλ,z(x)− fρ(x))2dρX ? Further , what is the best choice for λ to minimize this error ? In [ 5 ] , the answers to these questions are given . They state : For each m ∈ N and δ ∈ [ 0 , 1 ) , there is a function Em,δ = E : R+ → R , X(fλ,z(x)− fρ(x))2dρX ≤ E(λ ) such that , for all λ > 0 , with confidence 1 − δ . And there is a unique minimizer of E(λ ) that is found by a simple algorithm to yield the “ best ” regularization parameter λ = λ∗ .
References
[ 1 ] M . Bartlett . Further aspects of the theory of multiple reIn Proceedings of the Cambridge Philosophical gression . Society , number 34 , 1938 .
[ 2 ] V . Belhumeur , J . Hespanha , and D . Kriegman . Eigenfaces vs . fisherfaces : Recognition using class specific linear projection . IEEE Trans . Pattern Analysis and Machine Intelligence , 19(7):711–720 , 1997 .
[ 3 ] L . Chen , H . M . Liao , MTKo , J . Lin , and G . Yu . A new lda based face recognition system which can solve the small sample size problem . Pattern Recognition , 33:1713–1726 , 2001 .
[ 4 ] F . Cucker and S . Smale . On the mathematical foundations of learning . Bulletin of the American Mathematical Society , 39(1):1–49 , 2001 .
[ 5 ] F . Cucker and S . Smale . Best choices for regularization parameters in learning theory : On the bias variance problem . Foundations Comput . Math . , ( 4):413–428 , 2002 .
[ 6 ] R . O . Duda and P . E . Hart . Pattern Classification and Scene
Analysis . John Wiley Sons , New York , 1 edition , 1973 .
[ 7 ] T . Evgeniou , M . Pontil , and T . Poggio . Regularization networks and support vector machines . Advances in Computational Mathematics , 13(1):1–50 , 2000 .
[ 8 ] R . Fisher . The use of multiple measurements in taxonomic problems . Ann . Eugenics , 7:178–188 , 1936 .
[ 9 ] J . H . Friedman . Regularized discriminant analysis . Journal of the American Statistical Association , 84(405):165–175 , 1989 .
[ 10 ] K . Fukunaga . Introduction to statistical pattern recognition .
Academic Press , 1990 .
[ 11 ] G . Fung and O . L . Mangasarian . Proximal support vector machine classifiers . In F . Provost and R . Srikant , editors , Proceedings KDD 2001 : Knowledge Discovery and Data Mining , August 26 29 , 2001 , San Francisco , CA , pages 77– 86 , New York , 2001 .
[ 12 ] A . Hoerl and R . Kennard . Ridge regression : Biased estimation for nonorthogonal problems . Technometrics , 12(3):55– 67 , 1970 .
[ 13 ] A . K . J . Hertz and R . Palmer . Introduction to the Theory of
Neural Computation . Addison Wesley , 1991 .
[ 14 ] J.Yang , AFFrangi , J Y Yang , D . Zhang , and Z . Jin . Kpca plus lda : a complete kernel fisher discriminant framework for feature extraction and recognition . IEEE Trans . on Pattern Analysis and Machine Intelligence , 27:230–244 , 2005 . [ 15 ] K . Liu , Y . Cheng , and J . Yang . A generalized optimal set of discriminant vectors . Pattern Recognition , 25(7):731–739 , 1992 .
[ 16 ] J . Lu , K . N . Plataniotis , and A . N . Venetsanopoulos . Face recognition using kernel direct discriminant analysis algorithms . IEEE Transactions on Neural Networks , 14(1):117– 126 , 2003 .
[ 17 ] S . Mika . Kernel Fisher Discriminants . PhD thesis , Univer sity of Technology , Berlin , October 2002 .
[ 18 ] T . Poggio and S . Smale . The mathematics of learning : Dealing with data . Notices of the American Mathematical Society , 50(5):537–544 , 2003 .
[ 19 ] J . A . K . Suykens , T . V . Gestel , J . D . Brabanter , B . D . Moor , and J . Vandewalle . Least Squares Support Vector Machines . World Scientific Pub . Co . , Singapore , 2002 .
[ 20 ] D . Swets and J . Weng . Using discriminant eigenfeatures for image retrieval . IEEE Trans . Pattern Analysis and Machine Intelligence , 18(8):831–836 , 1996 .
[ 21 ] A . N . Tikhonov and V . Y . Arsenin . Solutions of Ill posed problems . John Wiley and Sons , Washington DC , 1977 .
[ 22 ] V . Vapnik . Statistical Learning Theory . Wiley , New York ,
1998 .
[ 23 ] H . Yu and J . Yang . A direct lda algorithm for highdimension data with application to face recognition . Pattern Recognition , 34:2067–2070 , 2001 .
[ 24 ] P . Zhang and J . Peng . Svm vs regularized least squares classification . In Proceedings of IEEE International Conference on Pattern Recognition , volume 1 , pages 176–179 , 2004 .
