Learning through Changes : An Empirical
Study of Dynamic Behaviors of Probability Estimation Trees
1Electrical Engineering and Computer Science Department , Tulane University , New Orleans
Kun Zhang1 , Zujia Xu2 , Jing Peng1 , Bill Buckles1
2Computer Science Department , Dillard University , New Orleans , zxu@dillard.edu zhangk , jp , buckles@eecstulaneedu
Abstract to evaluate
In practice , learning from data is often hampered by the limited training examples . In this paper , as the size of training data varies , we empirically investigate several probability estimation tree algorithms over eighteen binary classification problems . Nine metrics their performances . Our are used aggregated trees consistently outperform single trees . Confusion factor trees(CFT ) register poor calibration even as training size increases , which shows that CFTs are potentially biased if data sets have small noise . We also provide analysis on the observed performance of the tree algorithms . 1 . Introduction ensemble results show that
Given sufficient training examples , many classifiers perform quite well if test points are also drawn iid from the same distribution as the training data . However , in many real world applications , there may never be enough training examples due to practical factors . Known as a soft classifier , probability estimation trees ( PETs ) are trained decision trees that generate class membership probabilities for a test point . In cost sensitive learning , PETs are widely used and various algorithms have been proposed for better ranking . In this paper , we systematically compare two recent PETs , ie random decision tree ( RDT)[3 ] and confusion factor tree ( CFT)[7 ] with their conventional counterparts , C4.5 [ 1 ] and its variations , C4.4 [ 2 ] , baggedC4.4 [ 2 ] and baggedC4.5 on eighteen UCI binary data sets . For each set , we gradually vary the training size , build a tree , and then evaluate the obtained tree on the test examples . Nine evaluation metrics are used to gauge the aggregated behaviors of the above trees : error rate , mean squared error ( MSE ) , cross entropy , calibration , refinement , resolution , area under ROC curves ( AUC ) , area under lift charts ( AULC ) and precision . Our motivation for conducting such large scale experiments is as follows :
1 ) . Intuitively , learning curves of classifiers [ 6 ] will always improve as training size increases . But is this valid for different evaluation metrics ? Even if this holds , beyond the general trend , what are the relative ranks among these PETs with respect to these metrics ?
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 ) 1550 4786/05 $20.00 © 2005 IEEE
1
2 ) . K fold cross validation and random splits of training and test sets in terms of 60%:40 % ratio are the most common methods for empirical data set generation . A classifier ’s performance tends to be evaluated vertically at such a split against others . However , it is uninformative about how the classier itself behaves across different split ratio horizontally , especially with respect to multiple criteria .
3 ) . Some PETs are designed for certain criteria . CFT is designed for optimizing AUC value , while RDT[4 ] focuses on optimality of probability estimation . How do these trees perform on other standards ? i is
)x|y(pˆ i
4 ) . Different evaluation metrics have their own merits and weaknesses , and it is not unusual to expect a classifier which is optimal on one metric to be inferior on another . However , to what extent do these metrics agree with each other for a specific PET ? this the first comprehensive empirical investigation which takes into account above considerations , typically for PETs . 2 . Notation and Evaluation Metrics
To best of our knowledge ,
Assume all
For a random vector of features x , a classifier is a mapping from x to a class y∈{0,1} . Given a test point {xi , yi} , represents the predicted probability of the true a posteriori probability p(yi|xi ) . Class 1 is referred as positive class and class 0 as negative class . We consider here two types PETs , a single tree estimator and an ensemble of single trees . C4.5 , C4.4 and CFT are representatives of single PETs . Bagged trees and RDT are ensembles . the data sets are completely deterministic and noise free , p(yi|xi ) is 1 if the label of xi is yi and 0 otherwise . We calculate MSE as ( ∑ = N and cross entropy as )x|y(p N/1 i 1i − ∑ = N − N/1 ) ) ) 1i N is the size of test set . Error rate is obtained by thresholding the predicted probabilities at 05 Let ipˆ , suppose pˆ for j=1,…,k . Nj we estimate k distinct probabilities jpˆ , is the number of predictions having the same value and rj is the fraction of Nj points whose true labels are positive . Thus , MSE can be decomposed as follows :
. Of the N probabilities
)x|y(pˆ i )x|y(pˆ i
))x|y(p1( log)x|y(p(
− x|y(pˆ1
)x|1 i y(pˆ log( jpˆ
)
2 i i i i i i
= i
= i
− i i i
+
N1
∑
⎛ ⎜⎝ k = 1j r(N j j
−
)pˆ j
2
+
∑ k = 1j
)r1(rN j j j
−
⎞ ⎟⎠
( 1 ) for definition of
The first term is known as calibration and the second one is refinement . Please refer [ 12 ] for the details and [ 13 ] resolution . To calculate calibration , we use the binning method [ 4 ] to discretize the predicted probabilities . The number of bins changes as the test set size reduces gradually . We follow Shapiro et al [ 10 ] to compute AULC and Fawcett [ 11 ] to calculate AUC . Precision is defined as the fraction of positive predictions that are correctly classified . 3 . Experiment Designs
Eighteen UCI binary data sets [ 9 ] , including adult , mushroom , spam , chess , hypothyroid , sick euthyroid , tic , pima , breast cancer wisc , australian , breast cancerwdbc , housevote , ionosphere , spectf , liver , spect , sonar , and hepatitis , are used as test beds . For each set , we evaluate the following five ratios between training and test data sizes in terms of percentage of the total data points : 10:90 , 20:80 , 40:60 , 60:40 and 80:20 . Average results over twenty runs are reported for each PET at each ratio . The minority class is treated as the positive class and the natural class distribution is assumed .
For C4.5s , both the pruned ( C4.5P ) and unpruned ( C4.5UP ) trees are investigated . FullC4.5 is built without collapsing and pruning . Four variations of CFTs , pruned ( CFTLCP ) and unpruned ( CFTLCUP ) CFTs with laplace correction , as well as pruned ( CFTFEP ) and unpruned ( CFTFEUP ) CFTs with frequency estimation , are constructed . As in [ 7 ] , we set the confusion factor to be 03 In addition to RDT with half depth ( RDTH ) , we also implement RDT with full depth ( RDTF ) . Each ensemble includes thirty trees as base learners . 4 . Experimental Results and Discussion
For a specific metric , we aggregate experiment results over eighteen sets , not just a single domain , to access the overall general trend of each PET . The legends are consistent across all figures . 4.1 MSE , Cross Entropy and Error Rate
Figure 1 shows the learning curves for MSE , cross entropy and error rate . The overall relative ranks and changing behaviors of baggedC4.4 , RDTH , C4.4 and CFTs in the MSE plot are quite similar to that in the cross entropy plot . Cross entropy values are unavailable for baggedFullC4.5 , RDTF and C4.5s can be 0 . Ensembles are obviously better since than single trees . On average , MSE decreases from 14.9 % for the poorest tree , CFTLCUP , to 8.7 % for the best tree , baggedFullC45 Cross entropy decreases from 69.1 % for the poorest tree , CFTLCUP , to 41.5 % for the best tree , baggedC44 In ensembles , bagged trees outperform RDTs and RDTF is superior to RDTH .
)x|y(pˆ i i
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 ) 1550 4786/05 $20.00 © 2005 IEEE
2 is
Among single trees , C4.4 performs the best , followed by C4.5s and CFTs . Since the only difference between C4.4 and FullC4.5 the probability estimation methods , this indicates that laplace correction results in lower MSE than frequency counts [ 8 ] . However , once bagging is used , the influence of laplace correction is dramatically reduced . The average MSE difference between these two bagged trees is a mere 014 % For error rate , bagged trees once again dominate other methods . RDTF is superior to RDTH , which is consistent with [ 5 ] . C4.5P is the best among single trees . The changes in the ranks indicate that error rate , more or less , deviates from MSE and cross entropy . 4.2 Calibration , Refinement and Resolution
Figure 2 presents the learning curves of the PETs for calibration , refinement and resolution . In the calibration plot , CFTs have the largest calibration errors and their curves consistently go upward as the training size increases . The curves of bagged trees and RDTs exhibit a slightly upward trend . C4.4 and C4.5s’ curves first decrease , then keep steadily . In the refinement plot , each curve for individual PET decreases monotonically . This indicates that the MSE reductions of these PETs , especially CFTs , are achieved mainly through reduction in refinement . BaggedFullC4.5 has the smallest calibration and refinement error . Although the calibration errors of RDTs are close to C4.4 and C4.5s , their much lower refinement value makes their MSE still better than single trees . C4.4 has the smallest MSE in single trees . This is because its calibration and refinement error are the lowest . C4.5s have the poorest refinement values , but this deficiency is compensated by their much smaller calibration errors compared with CFTs . Therefore , their MSEs are still better than CFTs . If we ignore the absolute scales of refinement and resolution , the resolution plot actually mimics the refinement plot if we project refinement with respect to the x axis .
1
But why does refinement decrease as the training set grows ? And why do the curves in calibration plot assume such different appearances ? In the following , we answer the two questions with the notation and definitions specified in section 2 .
Recall from equation ( 1 ) , refinement achieves its minimum value 0 when all of the examples in bin j are either positive or negative . And it reaches its maximum value 0.25 if there is equal number of positive and jpˆ be the negative examples . Given total k bins , let binj mean of and . We have 0 0≤r1≤r2≤…≤(r(k+1)/2 ≈ 0.5 ) ≤…≤rk≤1.Generally , as the training size increases , a classifier can identify more rules with higher confidence . Its predicted probabilities probability < < value < k
<
<
<
< pˆ pˆ
1 pˆ j pˆ
2 jpˆ jpˆ for each test case will gradually shift to 0 or 1 , and the number of probabilities between 0 and 1 , especially around the random guess will decrease . During this shifting , rj will approach 0 for binj if is close to 0 . is close to 1 , rj will approach 1 . For binj whose jpˆ also changes a very small amount ) . In the ( Note that extreme case , all of the examples will fall into the bin1 with r1=0 and bink with rk =1 if the classifier can always issue probability which exactly matches the truth . At this point , refinement is 0 .
As defined in [ 7 ] , CFTs estimate probability for a test point by aggregating the contributions of all leaves , not just the target leaf where the test point falls . Since the tree size always increases with the growth of training set , the contributions from the non target leaves account for a larger proportion while the target leaf contributes less . If the inherent noise of a data set is small or even free , CFTs will affect the probability estimation by the contributions of more unrelated leaves into account , thereby causing the calibration to degrade . taking than single trees . With
Indicated by MSE plot , ensembles can issue more accurate predictions the expansion of training set , these better classifiers can produce quite good predictions for test cases . This increases the fraction of the number of cases in bin1 and bink while decreasing that of bins in between . Since the majority of the test cases are in bin1 and bink , we can show that , as the training size increases , the calibration error of bin1,( , will become larger . Here , we use ( ·)0 to denote the variables at step 0 when the training size is small , and ( ·)1 represents variables at step 1 at which training size is increased . Since the classifiers improve predictions by moving the positive examples out of bin1 or by moving negative cases into ) bin1 , we will have ( ≈ , ( r1)0 > ( r1)1 , pˆ 01 ≈ we As . and pˆ < have )r( )r( )r( , 11 01 11 thus ( )2 ( )2 < − it can be pˆ pˆ r )r( k 11 shown , also increases in the same way . The similar explanation can be applied to C4.4 and C45s 4.3 AUC , AULC and Precision
= pˆ )r( 01 − pˆ2 1 ( pˆ − k
Figure 3 shows the learning curves for AUC , AULC and precision . In the AUC plot , ensembles dominate the space , followed by CFTs , C4.4 and C45s The curves of bagged trees and RDTs are almost identical . In CFTs , unpruned trees perform better than pruned trees and laplace correction results in larger AUC than frequency estimation . This observation is in accord with [ 2 ] , but slightly differs with [ 7 ] . C4.5s ’s AUC values are quite poor . The average AUC increment
( ) r 11 pˆ > 1 pˆ − 1
( pˆ
) 11
< .
−
)r( 01
1
)2 r 1
1
,
)
2
1
1
, from 0.839 for C4.5UP to 0.912 for baggedC4.4 is 96%In the AULC plot , baggedC4.4 is still the best . For the single trees , CFTLCUP achieves the largest AULC value . In CFTs , unpruned trees beat pruned tree and laplace correction leads to a larger AULC . RDTF is superior to other PETs with overall precision value of 0831 The average precision increment from 0.711 for CFTFEP , the poorest one , to RDTF is 17 % . Among the single trees , C4.5P has the largest value . 5 . Conclusions
We have carried out an empirical comparison of several PET algorithms with respect to a varying training size using nine metrics . By responding to the hypotheses proposed in section 1 , we reach the following conclusions :
( 1).Ensembles consistently outperform single trees on all of the nine metrics . Bagged trees are better than RDT on seven metrics , the exception being AUC and precision . RDT with full depth exhibits better overall performance than half depth tree , which is in accord with [ 3 5 ] . ( 2).Among single trees , C4.4 performs best on MSE , cross entropy , calibration , refinement and resolution . Pruned C4.5 is superior for error rate and precision . Unpruned CFT with laplace correction has the advantage on AUC and AULC . ( 3).As training size increases , the learning curves of these trees improve for all metrics but calibration . ( 4).MSE and cross entropy are quite consistent with each other ; MSE can be better understood through decomposition ; AUC and AULC are nearly isomorphic rank metrics . In summary , this paper can be viewed as a supplement of what has been done in [ 2 8 ] . Future work will focus on correcting some PETs’ deficiencies through algorithm modification . 6 . References [ 1]JRQuinlan C4.5 : programs for empirical learning . Morgan Kaufmann,1993 [ 2]F.Provost and PDomingos “ Tree induction for probability based rankings ” .Machine Learning , 52(3 ) , 2003 , pp . 199 215 [ 3 ] W.Fan , HXWang , PSYu and S.Ma , “ Is random model better ? On its accuracy and efficiency ” ,ICDM2003 , pp.51 58 [ 4 ] W.Fan , “ On the optimality of probability estimation by random decision trees ” , AAAI 2004 , pp.336 341 [ 5 ] F . Liu , K . M . Ting , W . Fan , “ Maximizing tree diversity by building complete random decision trees . PAKDD2005 [ 6 ] G.Weiss and F.Provost , “ Learning when training data are costly : the effect of class distribution on tree induction ” , JAIR , 19 , 2003 , pp.315 354 [ 7 ] C.Ling and R.Yan , “ Decision Tree with better ranking ” , ICML,2003 [ 8]JBradford,CKunz,RKohavi,CBrunk,CBrodley , Pruning decision trees with misclassification costs ” , ECML98 [ 9]C.Blake and C.Merz , “ UCI repository of machine learning database ” , UCI , 1998 [ 10]G P.Shapiro and B . Masand . “ Estimating campaign benefits and modeling lift ” . KDD1999 .
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 ) 1550 4786/05 $20.00 © 2005 IEEE
3 graphs :
[ 11]TFawcett “ ROC considerations for data mining researchers ” . HPL 2003 4 . [ 12]M.DeGroot and S.Fienberg , “ Assessing probability assessors : calibration and refinement ” , Statistica Decision Theory and Related Topics III , Vol 1 , 1982 , pp . 291 314 practical notes and
[ 13]AHMurphy , “ A new vector partition of the probability score ” , J . Appl . Met.,12 , 1973 , pp.534 537
0.16
0.15
0.14
0.13
0.12
0.11
0.1
0.09
0.08
E
S
M
10
20
30
40
50
60
70
80
Training Data Size : Percentage( % ) of the Total Data Points
0.75
0.7
0.65
0.6 y p o r t n
E
0.55 s s o r
C
0.5
0.45
0.4
0.35 10
BagC4.4 RDTH C4.4 CFTFEUP CFTFEP CFTLCUP CFTLCP
0.19
0.17
0.15 e t a
0.13
R r o r r
E
0.11
0.09
BagC4.4 BagFullC4.5 RDTH RDTF C4.4 FullC4.5 C4.5UP C4.5P CFTFEUP CFTFEP CFTLCUP CFTLCP
20
30
40
50
60
70
80
Training Data Size : Percentage( % ) of the Total Data Points
0.07 10
20
30
40
Training Data Size : Percentage( % ) of the Total Data Points
50
60
70
80
Figure 1 : From left to right , learning curves for MSE , cross entropy and error rate as training set size increases n o i t a r b i l a
C
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0 10
0.12
0.11
0.1
0.09
0.08
0.07
0.06
0.05
0.04 t n e m e n i f e
R
BagC4.4 BagFullC4.5 RDTH RDTF C4.4 FullC4.5 C4.5UP C4.5P CFTFEUP CFTFEP CFTLCUP CFTLCP
20
30
40
50
60
70
80
10
20
30
40
50
60
70
80
Training Data Size : Percentage( % ) of the Total Data Points
Training Data Size : Percentage( % ) of the Total Data Points n o i t u l o s e
R
0.15
0.14
0.13
0.12
0.11
0.1
0.09
0.08
0.07 10
BagC4.4 BagFullC4.5 RDTH RDTF C4.4 FullC4.5 C4.5UP C4.5P CFTFEUP CFTFEP CFTLCUP CFTLCP
20
30
40
50
60
70
80
Training Data Size : Percentage( % ) of the Total Data Points
Figure 2 : From left to right , learning curves for calibration , refinement and resolution as training set size increases
C
U
A
0.94
0.92
0.9
0.88
0.86
0.84
0.82
0.8
0.78
0.76 10
BagC4.4 BagFullC4.5 RDTH RDTF C4.4 FullC4.5 C4.5UP C4.5P CFTFEUP CFTFEP CFTLCUP CFTLCP
20
30
40
50
60
70
80
Training Data Size : Percentage( % ) of the Total Data Points
2.02
2
1.96
1.92
1.88
C L
U
A
1.84
1.8
1.76
1.72
1.68 10
BagC4.4 BagFullC4.5 RDTH RDTF C4.4 FullC4.5 C4.5UP C4.5P CFTFEUP CFTFEP CFTLCUP CFTLCP
20
30
40
50
60
70
80
Training Data Size : Percentage( % ) of the Total Data Points n o i s i c e r
P
0.88
0.84
0.8
0.76
0.72
0.68
0.64
0.6 10
BagC4.4 BagFullC4.5 RDTH RDTF C4.4 FullC4.5 C4.5UP C4.5P CFTFEUP CFTFEP CFTLCUP CFTLCP
20
30
40
50
60
70
80
Training Data Size : Percentage( % ) of the Total Data Points
Figure 3 : From left to right , learning curves for AUC , AULC and precision as training set size increases
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 ) 1550 4786/05 $20.00 © 2005 IEEE
4
