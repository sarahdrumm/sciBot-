Semi supervised Mixture of Kernels via LPBoost Methods
Jinbo Bi Glenn Fung Murat Dundar Bharat Rao
Computer Aided Diagnosis and Therapy Solutions Siemens Medical Solutions , Malvern , PA 19355 jinbo.bi , glenn.fung , murat.dundar , bharatrao@siemenscom
Abstract
We propose an algorithm to construct classification models with a mixture of kernels from labeled and unlabeled data . Unlike traditional kernel methods which select a kernel according to cross validation performance , we derive classifiers that are a mixture of models , each based on one kernel choice from a library of kernels . The sparse favoring 1 norm regularization method is employed to restrict the complexity of mixture models and to achieve the sparsity of solutions . By modifying the column generation boosting algorithm LPBoost to a more general linear programming formulation , we are able to efficiently solve mixtureof kernel problems and automatically select kernel basis functions centered at labeled data as well as unlabeled data . The effectiveness of the proposed approach is proved by experimental results on benchmark datasets and a real world lung nodule detection system .
1 Introduction
Recent years have seen considerable interests in learning with labeled and unlabeled data since experiments for labeling data are often expensive while vast amount of unlabeled data are easily available . Semi supervised learning is commonly used to explore the information of input distribution provided by unlabeled data to improve the performance of supervised learning ( see a recent survey [ 22] ) . In this paper , we propose a boosting algorithm to construct classification models from labeled and unlabeled data based on kernel methods .
Boosting algorithms are well studied learning methodologies developed in the last decade . They construct classifiers in an incremental fashion by using a weighted “ vote ” of various simple models obtained from a weak learner . A general setting of boosting algorithms requires three components : a hypothesis set , a weak learner that produces simple models from the hypothesis set , a mechanism that combines the simple model into the final model by minimizing certain cost functional . Successful boosting methods , such as AdaBoost [ 11 ] and LPBoost [ 10 ] , have established applications in many practical domains .
Kernel based methods have proven to be very effective for solving inference problems in many applications . By introducing a positive semidefinite kernel K , nonlinear models can be created using linear learning algorithms such as in support vector machines ( SVM ) , kernel ridge regression , kernel logistic regression , etc . The model is often written in the form of kernel expansions . For example , the decision boundary f obtained by SVM classification is expressed as f ( x ) =Xj
αjK(x , xj ) ,
( 1 ) where α is the model coefficients and xj is the jth training input vector . as a type of basis expansion model with kernel terms K(x , xj ) as basis functions centered at xj . In such kernel methods , the choice of kernel mapping is of crucial importance . Usually , the choice of kernel is determined by predefining the type of kernel ( e.g , RBF , and polynomial kernels ) , and tuning the kernel parameters using cross validation performance . Cross validation is expensive and the resulting kernel is not guaranteed to be an excellent choice .
Recent work [ 14 , 8 , 17 , 9 ] has attempted to design kernels that adapt to the data of a particular task to be solved . For example , Lanckriet et al . [ 14 ] and Bousquet et al . [ 8 ] proposed the use of a linear combination of kernels
K = Pp µpKp from a family of various kernel functions
Kp . To ensure the positive semidefiniteness , the combination coefficients µp are either simply required to be nonnegative or determined in the way such that the composite kernel is positive semidefinite , for instance , by solving a semi definite program as in [ 14 ] or by boosting as in [ 9 ] . The decision boundary f thus becomes f ( x ) =Xj
αj Xp
µpKp(x , xj )! .
( 2 )
In this paper , we devise algorithms that boost on the columns of kernel matrices ( the basis functions ) constructed from labeled and unlabeled data via column generation ( CG ) techniques . Our goal is to develop effective methods to construct inference models that make use of various geometry of the feature spaces introduced by a family of kernels other than a less expressive feature space induced by a single kernel . The proposed approach is designed for tasks where only a limited amount of labeled data is available whereas a large set of unlabelled data can be easily accessed . The unlabeled data is used jointly with the labeled data as possible centers for the kernel basis functions . From the boosting point of view , this corresponds to incorporating the basis functions constructed by unlabeled data into the hypothesis space H , which tend to produce better predictive models , specially when very few labels can be obtained . The 1 norm regularization method is employed to restrict the capacity of mixture models and to achieve sparsity . We modify LPBoost , a CG based boosting algorithm [ 10 ] to a more general linear programming formulation . The LPBoost modification can thus be used to efficiently optimize the proposed semi supervised mixture ofkernel models and further enhance the sparsity . The proposed algorithm becomes more general and is suitable for constructing a much larger variety of inference models .
We outline this paper as follows . In Section 2 , we discuss the mixture of kernels model and describe its characteristics . Section 3 introduces the semi supervised learning problem for inference with mixture of kernels . Section 4 extends the existing LPBoost algorithm to a general linear program , and adjusts the modified LPBoost to learn mixture of kernel models from labeled and unlabeled data . Experimental results on benchmark problems and a real world nodule detection system are included in Section 5 to demonstrate the performance of the proposed approach . The last section 6 concludes this paper . Throughout this article , vectors are presumed to be column vectors unless otherwise stated , and denoted using bold face lower letters . Matrices are denoted by bold face upper letters .
2 Mixture of Kernels
Other than constructing new kernels or kernel matrices ( the Gram matrix induced by a kernel on the training data ) as in the model ( 2 ) , our approach constructs models that are a mixture of models , each based on one kernel choice from a library of kernels . Our algorithm automatically determines the kernel basis functions to be used in the mixture model . Given a library of kernels S = {Kp , p = 1 , · · · , P } , a Gram matrix Kp can be calculated for each kernel in S on sample data with the column Kp(· , xj ) corresponding to the jth example . The decision boundary represented by the mixture of kernel approach is f ( x ) =Xp Xj
αp j Kp(x , xj ) .
( 3 ) where αp j gives the weight for the j th column in the pth kernel matrix . Similar strategies have appeared in some works such as [ 3 , 19 ] to improve generalization and reduce training and prediction computational costs . MARK [ 3 ] optimized a heterogeneous kernel using a gradient descent algorithm in function space . GSVC and POKER [ 19 , 18 ] grew mixtures of kernel functions from a family of RBF kernels , and the mixture model was designed to be used only with weighted least squares VMS . The proposed approach can be used in conjunction with any linear generalized SVM formulations and algorithms . Moreover , our approach has been extended to solve many quadratic learning formulations with any suitable kernel families [ 5 ] .
A mixture model of form ( 3 ) can potentially give a larger hypothesis space than a model that uses either a single kernel or a composite kernel . Models ( 3 ) based on a mixture of kernels are not necessarily equivalent to models ( 2 ) based on a composite kernel . Rewriting a composite ker nel model ( 2 ) as f ( x ) = PpPi αiµpKp(xi , x ) , we can see it is equivalent to a mixture of kernel model ( 3 ) with αp i = αiµp . The opposite is not necessarily true , ie , a mixture model is not necessarily equivalent to a composite model since given any composite kernel model ( 2 ) , for any two kernels Kp and Kq , the ratio αp i is fixed to µp/µq , for all i ( assuming µq 6= 0 without loss of generality ) . Note that for a mixture of kernel model ( 3 ) , we do not have this restriction . i /αq
Models of form ( 3 ) have potential to achieve better approximation capability since they can generate sparse solutions if appropriate regularization conditions are used . In the composite model , a kernel matrix is formed with an optimized µ vector , and then used to construct a kernel expansion as the classification model . For any αj > 0 in the constructed model , the corresponding columns Kp j from all kernels specified by µ in the library have to be calculated although some columns or basis functions may not be necssary . In case the solution α is not very sparse ( which is true for most quadratic cost objectives ) , the execution time and computational cost in the testing phase will be drastic . For the mixture of kernel model , only necessary kernel basis functions will be selected and included in the classification model .
We discuss the relationship among different basis expansion models , such as models obtained by classic SVMs , mixture of kernel approaches and RBF nets [ 7 ] . In basis expansion methods [ 12 ] , such as RBF networks , the model takes a form of f ( x ) = P αjφj(x ) , where each function
φj is a basis function of a form exp(−||x − cj||2/σj ) . The centers c and the bandwidths σ of the basis functions in RBF nets are heuristically determined using unsupervised techniques . Therefore to construct the decision rule , one has to estimate : 1 . the number of RBF centers ; 2 . the estimates of the centers ; 3 . the linear model parameter α ,
2 the bandwidth parameter σ . Compared with RBF and 4 . networks , the benefits of using SVMs have been studied in [ 21 , 20 ] . The first three sets of parameters can be automatically determined by SVMs when learning support vectors . The last parameter is usually obtained by cross validation tuning . Classic SVMs , however , use only a single parameter σ , which means that all centers ( support vectors ) are associated with a single choice of σ . Contrary to SVMs , RBF networks estimate a different σ for every center c of the RBF basis . More generally , the bandwidth σ can be different on different directions . The mixture of kernel model has a flexibility in between SVMs and RBF networks . Construction of a mixture of kernel model still benefits from the SVM like algorithms , ie , parameters except σ can be learned by solving an optimization problem . In addition , the mixture of kernel model allows the RBF basis positioned at different centers ( support vectors ) to associate with different bandwidths .
The mixture of kernel approach investigated here has interesting properties concerning its learning and approximation behaviors . These theoretical issues will be addressed elsewhere and more complete discussions on algorithmic design for constructing mixture of kernel models are presented in [ 5 ] . This paper focuses on the development of an algorithm for learning a mixture of kernel model that benefits from unlabeled data .
3 Semi supervised Mixture of Kernels
We consider the problem of learning models with a mixture of kernels from labeled and unlabeled data . In reality , it is often comparatively easier and cheaper to obtain data with no labels . A learning algorithm may take advantage of unlabelled data to enhance the performance of the inference model to be derived . The problem that we deal with in this article is slightly different from traditional transductive learning [ 21 ] where the task is to predict the labels of the unlabeled examples given a training set of labeled examples and a working set of unlabeled examples . No inference model needs to be explicitly derived . In our problem , an inference model is required to predict labels of the test examples which are unseen in the training process . The validation of the algorithm performance is conducted only on an independent test set .
In a semi supervised setting , besides a set of labeled data , L = {(xi , yi ) , i = 1 , · · · , ℓ} , a set of unlabeled data U = {xℓ+j , j = 1 , · · · , ℓu} is also given . The kernel matrix calculated on both labeled and unlabeled data is
Kp = Kp
L,L KpT L,U
Kp Kp
L,U
U,U
( 4 ) where Kp i,j = Kp(xi , xj ) , i , j = 1 , · · · , ℓ , ℓ + 1 , · · · , ℓ + ℓu . The classification model based on a mixture of these kernel matrices takes form of f ( x ) =Xp
ℓ+ℓu
Xj=1
αp j Kp(x , xj ) ,
( 5 ) and the classifier is defined by sgn(f ( x) ) . Parallel to connection analysis with RBF nets in Section 2 , models of form ( 5 ) allows basis functions to locate at examples with no labels . The estimates of the centers or support vectors can still be automatically optimized by a SVM like algorithm . Furthermore , a lot more choices for basis centers come for free from unlabelled examples without any need of unsupervised techniques . ric Pℓ
Similar to what has been used in SVMs , we optimize models by minimizing the margin based 1 norm error meti=1 ξi where ξi satisfies yif ( xi ) ≥ 1 − ξi , ∀i = 1 , · · · , ℓ . Without loss of generality , we include an explicit offset b in the model f ( x ) . Note that the offset can be incorporated into the kernel matrix . We write out b explicitly since we do not require the regularization to be taken on b . To achieve good generalization performance , it is important to apply appropriate regularization conditions to the model class . A commonly used regularization condition by single kernel methods is the reproducing kernel Hilbert space ( RKHS ) regularization R(f ) .
= αT Kα .
( 6 )
R(f ) =flflflflfl Xi
αiK(x , xi)flflflflfl
2
H where H is the RKHS induced by the kernel K . The natural extension of the RKHS regularization condition to the semisupervised mixture of kernel model is the following R(f )
αpT Kpαp .
( 7 )
R(f ) =Xp ization is to minimize Pp αpT Kpαp + CPℓ
The objective of the learning problem with RKHS regulari=1 ξi where the error term is defined only using the top ℓ rows of the kernel matrix K as illustrated in ( 4 ) , and the regularization term uses the entire kernel matrix K including the part produced by unlabeled data .
The RKHS regularization condition requires positive semi definiteness ( PSD ) of each of the kernel matrices Kp , and solving the resulting optimization problem can be computationally expensive . To remove the SDP requirement and achieve computational efficiency , we can apply other regularization conditions , such as penalizing the 1 norm or 2 norm of α , that are equally suitable for capacity control [ 15 , 4 ] . These regularization methods are more generally applicable since they do not require the kernel matrix to be positive semi definite . In particular , we are in favor of the 1 norm regularization ||α||1 = P |αj | since it is well
3 known that the 1 norm regularization leads to sparse solutions , which in the mixture of kernels model , is very desirable . Furthermore , we prove that the RKHS norm can be
Hp i | . i Kp(x , xi)k2 bounded using the 1 normPi |αp kPi αp i Pj αp =Pi αp i | ) supi,jfififiPj αp j Kp(xi , xj )fififi ≤ ( Pi |αp i | ( supi Kp(xi , xi))1/2 kPi αp ≤Pi |αp i Kp(x , xi)flflflflflHp flflflflfl i |sup Xi
≤Xi
Thus ,
|αp
αp i j Kp(xi , xj ) i Kp(xi , x)kHp
.
Kp(xi , xi)1/2
.
Consequently , the 1 norm regularization is less restrictive than RKHS regularization , and hence gives a more expressive model .
For notational convenience , let us line up all kernel matrices from S together K = [ K1 K2 · · · KP ] , and re index the columns in K . Let index j run through the columns and index i run along the rows . Hence Ki· denotes the ith row of K , and K·j denotes the jth column . There are d = ( ℓ + ℓu ) × p columns in total . We thus formulate the learning problem as minα,ξ st
|αj| + C
ξi
ℓ
Pi=1
Kij αj + b! + ξi ≥ 1 , d
Pj=1 yi Pj
ξi ≥ 0 , i = 1 , · · · , ℓ , where the jth column now only consists of the first ℓ elements of that in K . There exist much more columns ( variables α ) in problem ( 8 ) than the number of constraints . In other words , compared with inductive learning , this formulation has significantly more basis functions to choose considering the size of labeled data available , it may require extra steps for capacity control . If an extra validation set is available , the optimization of ( 8 ) can be terminated when the performance on the validation set cannot be improved further . In other words , our algorithm will stop when adding a new kernel basis function does not improve performance on the validation set . This is similar to the early stopping technique often used in neural network training .
4 Generalized LPBoost
The CG techniques have been widely used for solving large scale linear programs ( LPs ) or difficult integer programs since 1950s [ 16 ] . In the primal space , the CG method solves LPs on a subset of variables α , which means not all columns of the kernel matrix are generated at once and used to construct the function f . Columns are generated iteratively and added to the problem to achieve optimality . In the dual space , a column in the primal problem corresponds to a constraint in the dual problem . When a column is not included in the primal , the corresponding constraint does not appear in the dual . If a constraint absent from the dual problem is violated by the solution to the restricted problem , this constraint ( a cutting plane ) needs to be included in the dual problem to further restrict its feasible region . Thus these techniques are also referred to as cutting plane methods [ 1 ] . We first briefly review the existing LPBoost with 1 norm regularization . Then we propose our modification that allows us to consider kernels that do not necessarily comply with the PSD requirement .
If the hypothesis K·jαj based on a single column of the matrix K is regarded as a weak model or base classifier , we can rewrite LPBoost using our notation and following the statement in [ 10 ] : minα,ξ st
αj + C
ξi
ℓ
Pi=1 d
Pj=1 yiPj
αj ≥ 0 , j = 1 , · · · , d ,
Kij αj + ξi ≥ 1 , ξi ≥ 0 , i = 1 , · · · , ℓ
( 9 ) where C > 0 is the regularization parameter . The dual of LP ( 9 ) is
( 8 ) maxβ st
βi
βiyiKij ≤ 1 , j = 1 , · · · , d ,
( 10 )
0 ≤ βi ≤ C , i = 1 , · · · , ℓ .
ℓ
ℓ
Pi=1 Pi=1
These problems are referred to as the master problems . The CG method solves LPs by incrementally selecting a subset of columns from the simplex tableau and optimizing the tableau restricted on the subset of variables ( each corresponding to a selected column ) . After a primal dual solution ( ˆα , ˆξ , ˆβ ) to the restricted problem is obtained , we solve
τ = max j Xi
ˆβiyiKij ,
( 11 ) where j runs over all columns of K . If τ ≤ 1 , the solution for the restricted problem is optimal to the master problems . If τ > 1 , then the solution to ( 11 ) provides a column to be included in the restricted problem .
As illustrated in problem ( 8 ) , kernel methods in general do not require the model coefficients αi = ˆαiyi to be non negative . Moreover , an explicit offset term b can be included in the function f . To form a LP from problem ( 8 ) , we rewrite αj = uj − vj where uj , vj ≥ 0 . Then |αj| = uj + vj if either uj or vj has to be 0 . The LP is then
4 formulated in variables u , v , b , ξ as minu,v,ξ st d
Pj=1 yi Pj
( uj + vj ) + C
ξi
ℓ
Pi=1
Kij ( uj − vj ) + b! + ξi ≥ 1 ,
ξi ≥ 0 , i = 1 , · · · , ℓ , uj , vj ≥ 0 , j = 1 , · · · , d . and dual objectives . Recall how we define ˆu = ( uW uN = 0 ) and ˆv = ( vW vN = 0 ) , so ( ˆu , ˆv , ˆξ ) is feasible for LP ( 12 ) . Since the solution is optimal to the restricted problems , the primal objective is equal to the dual objective . Now the key issue to evaluate is the dual feasibility . Since ˆβ is optimal for the restricted problem , it satisfies all constraints of the restricted dual . Hence the dual feasibility is
( 12 )
Any column that violates dual feasibility can be added . For LPs , a common heuristic is to choose the column K·j In other over all j ∈ N .
≤ 1 , j ∈ N . words , the column K·j that solves validated iffififiPi ˆβiyiKijfififi that maximizes fififiPi ˆβiyiKijfififi j∈N fififififi Xi
τ = max
ˆβiyiKijfififififi will be included in the restricted problem . Compared with original LPBoost , our method is in general equivalent to enclosing negations of weak models K·j in the hypothesis set . We describe the modified LPBoost for semi supervised learning with a mixture of kernels ( SSMK ) in Algorithm 1 where e is a vector of ones of appropriate dimension corresponding to the bias term b .
( 14 )
Algorithm 1 SSMK : CG Boosting for LP ( 12 ) 1 . Initialize the first column K0 = e , specify the tolerance tol
Solving the above LP yields solutions equivalent to those obtained by ( 8 ) because in the optimal solution , at least one of the two variables uj and vj will be zero for all i = 1 , · · · , d . Otherwise , assume uj > vj > 0 without loss of generality , and we can find a better solution by setting another feasible solution ˆuj = uj − vj and ˆvj = 0 . Then ˆuj + ˆvj = uj − vj < uj + vj contradicting the optimality of ( u , v ) .
There are two variables uj , vj corresponding to each column K·j of the kernel matrix in problem ( 12 ) . Correspondingly the Lagrangian dual problem has two coni=1 βiyiKij ≤ 1 and i=1 βiyiKij ≤ 1 . Combining both constraints , we i=1 βiyiKij ≤ 1 . Hence the dual problem straints for the column K·j , ie , Pℓ −Pℓ have −1 ≤ Pℓ Pi=1 becomes : maxβ
βi
ℓ
−1 ≤
βiyiKij ≤ 1 , j = 1 , · · · , d , st
ℓ
Pi=1
βiyi = 0 ,
ℓ
Pi=1
0 ≤ βi ≤ C , i = 1 , · · · , ℓ .
The CG method partitions the variables αj into two sets , the working set W that is used to build the model and the remaining set denoted as N that is eliminated from the model as the corresponding columns are not generated . Each CG step optimizes a subproblem over the working set W of variables and then selects a column from N based on the current solution to add to W . At each iteration , αj in N can be interpreted as αj = 0 , or accordingly , uj , vj = 0 . Hence once a solution αW = uW − vW to the restricted problem is obtained , ˆα = ( αW αN = 0 ) is feasible to the master LP ( 12 ) . The following statement examines when an optimal solution for the master problem is obtained in the CG procedure .
Proposition 1 ( Optimality of LP CG ) Let ( ˆu , ˆv , ˆξ , ˆβ ) be the primal dual solution to the current restricted problems with variable b included in W . The solution is optimal to
LP ( 12 ) if and only if for all j ∈ N ,fififiPi
To show the optimality is achieved , we need to confirm primal feasibility , dual feasibility and the equality of primal
ˆβiyiKijfififi
≤ 1 .
( 13 )
2 . For t = 1 to T , do 3 .
Solve problem ( 12 ) with Kt−1 , obtain solution ( ut , vt , ξt , βt ) Solve problem ( 14 ) to obtain τ , and let z be the solution If τ ≤ 1 + tol , optimal , break from loop , otherwise , Kt = [ Kt−1 z ] , continue
4 .
5 .
6 . End of loop 7 . ˆα = ut − vt .
After the column has been generated , we can either solve the updated primal problem or the updated dual problem . Any suitable algorithm can be used to solve the primal or dual restricted problem . The original LPBoost [ 10 ] solves the dual problem at each iteration . From the optimization point of view , it is not clear if solving the dual problem will be computationally cheaper than solving the primal . In Algorithm 1 , we solve the primal problem since the current primal dual solution is primal feasible to the updated problem , and there is no need to find the first feasible solution for the updated primal . Therefore solving each restricted primal can be cheap in CG algorithms . Another advantage of using the LPBoost based approach is that at each iteration , only a small subset of the basis functions has to be kept in memory which is a limitation of traditional SVM formulations .
5
5 Experimental Study
In this section , we validate the proposed approach by investigating its prediction accuracy , optimization efficiency as well as sparsity of solutions in several experiments . We compare the performance of our approach SSMK to the models obtained by inductive learning with a mixture of kernels ( MK ) on two benchmark datasets and a real world medical imaging dataset . Two types of kernels are explored : the linear kernel and RBF kernel . For the RBF kernel , we employ a fixed strategy to find the bandwidth . First we calculate the mean of ||xi − xj||2 where i , j run through the labeled examples , and then set bandwidth equal to square root of the mean value . More thorough comparison between inductive MK models and models based on single kernels can be found in [ 5 ] .
5.1 Benchmark data
Two large databases were used in experiments . The Forest data came from UCI KDD Archive with 54 variables , 495,141 examples . The NIST hand written digit database was downloaded from http://yannlecuncom/exdb/mnist/ Each digit is represented by a 28 × 28 image so there are 784 variables in total . The database contains 60,000 digits . We created binary classification problems by distinguishing between spruce fir and lodgepole pine cover types on Forest data , and discriminating between odd numbers and even numbers on NIST Digits data . The distribution of classes in both problems is balanced , so we mainly report misclassification rates in Figure 1 . Commercial optimization package ILOG CPLEX 8.0 [ 13 ] was used as the LP solver .
We generated datasets in the following way : preserve ℓt = 2000 examples randomly drawn from the databases as test sets ( T ) ; randomly take ℓu = 500 examples from the remaining data as unlabelled data ( U ) used in model construction ; then randomly take ℓ =10 , 20 , 50 , 100 , 200 , 300 , 400 , 500 examples as training data ( L ) with labels . We performed 10 trials , and present error rates averaged on these trials in Figure 1 . The data was preprocessed by normalizing each original feature to have mean 0 and standard deviation 1 . In the training phase , the computational complexity of the LPs can be roughly measured by the size of the kernel matrices . For example , if we choose 100 labeled examples , the size of the kernel matrix used in LP(12 ) is 100 × 1200 considering there are 500 unlabelled data and 2 types of kernels . All results were obtained by setting regularization parameter C to 10 for Digits data and to 1 for Forest data . Other choices of C produced similar comparison results between our approach SSMK and the inductive MK model with no use of unlabeled data .
Clearly , from Figure 1 , the use of unlabeled data in LP(12 ) helps improve the prediction performance both on
50
45
40
35
30
25
20
15
10
5
0
0
45
40
35
30
25
20 e t a R r o r r
E n o i i t c d e r P e t a R r o r r
E n o i i t c d e r P
100
15
0
100 test ( Inductive ) unlabel ( Inductive ) train ( Inductive ) test ( SSMK ) unlabel ( SSMK ) train ( SSMK )
200
300
Training Sample Size
400
500 test ( Inductive ) unlabel ( Inductive ) train ( Inductive ) test ( SSMK ) unlabel ( SSMK ) train ( SSMK )
200
300
Training Sample Size
400
500
Figure 1 . Plots of error percentage versus training size ℓ for Digits up and Forest bottom . the unlabeled data which is used in training with no labels and the test data which is completely blinded to the training process . With more and more labeled data available , the difference between inductive MK and SSMK is generally reduced . It is reasonable to see that performance on unlabeled data is slightly better than test prediction for SSMK . Table 1 provides more detailed comparison between inductive MK ( IMK ) and SSMK . By applying the generalized LPBoost , we dramatically reduced the size of optimization problems to be solved and thus gained computational efficiency . For example , when ℓ = 100 for Digits , we solved a problem of very large size 100×1200 in IMK , and 47 linear kernel basis and 2 RBF basis were used in the final model ( see Table 1 ) . In SSMK with the CG procedure , we need to solve problems of much smaller size ( the largest size = 100 × 34 during the 34 ( given by ( SSMK)CG Iter in Table 1 ) iterations . The resulting model used only 26 linear kernel columns . Hence SSMK had potentials to find more sparse solutions although both IMK and SSMK obtained sparse solutions due to the use of 1 norm regularization . We can see SSMK benefited from the use of unlabeled data by referencing the last two rows of Table 1 . These two terms give
6
500
100
500
Data ℓ Problem size ( IMK)linear ( IMK)RBF ( SSMK)CG Iter ( SSMK ) subproblem size ( SSMK)linear ( SSMK)RBF ( SSMK)K L ( SSMK)K U
Table 1 . Efficiency comparison between SSMK and inductive MK . Forest 200
Digits 200
100
100 × 1200
200 × 1400
500 × 2000
100 × 1200
200 × 1400
500 × 2000
47 2 34
107 5 38
127 4 37
15 0 23
18 0 30
23 3 48
100 × 34
200 × 38
500 × 37
100 × 23
200 × 30
500 × 48
26 0 9 17
30 0 5 25
35 0 7 28
16 0 5 11
18 1 5 14
23 2 9 16 the numbers of kernel columns generated from the labeled data ( S)K L and the unlabeled data ( S)K U at the end of CG optimization .
5.2 Medical imaging data for CAD
We also tested the proposed algorithm on a real world data set used in our Computer Aided Detection ( CAD ) system for identifying lung nodules . Typical CAD data sets for classification are large ( several thousand candidates ) and unbalanced ( significantly fewer than 1 % of the candidates are “ positive ” ) . To be accepted by physicians , CAD systems must generalize well with extremely high sensitivity and very few false positives . Researchers often generate a large amount of candidates for structures of interest in the image for example , cancerous nodules in a CT ( computed tomography ) volume of a patients lung , or polyps in a CT volume of colons . Only a small subset of candidates actually correspond to nodules or polyps . It is extremely expensive and tedious to ask physicians to label the obtained candidates with high confidence . The typical workflow for a CAD system when used to identify structures in a new patient image is : 1 . identify candidate structures in the image ; 2 . extract features for each candidate ; 3 . classify candidates as positive or negative ( using a classifier trained on the feature matrix generated by the training candidates ) ; 4 . display a positive candidate .
In our study , we used a dataset of 112 high resolution CT images that was obtained from one North American and three European sites . The CT data was acquired with the following scanners : Siemens VolumeZoom CT , a Sensation 16 , and a Sensation 64 scanner , all with 100kV , 120kV , or 140kV , and an exposure from 20mAs to 253mAs . All volumes were reconstructed with a B50f or B60f reconstruction kernel . The slice thickness ranged in 1.0mm and 1.25mm with an axial resolution between 0.5mm and 08mm The ground truth labels used in this study were established by 3 radiologists ( 1 expert and 2 experienced readers ) . Our in house candidate generation algorithm gen erated 22405 candidates and 42 features were calculated . We split the candidate set into the labeled and unlabeled training sets of 5000 candidates respectively for each , and all remaining candidates are in the test set . Instead of computing the linear kernel , we used the data matrix itself to form K = [ X Krbf ] where X is the input data and Krbf is the RBF kernel matrix . Figure 2 shows the ROC curves plotting sensitivity versus false positive value per volume . For the respective classifiers obtained by IMK and SSMK , figure 2 shows two curves , one for unlabeled data performance , the other for test performance . We can see that with more choices of basis functions constructed from unlabeled data , the prediction accuracy is improved , especially over the small false positive rates from 1 to 3 which is the desired range of prediction accuracy for lung nodule detection system . Due to the 1 norm regularization condition , SSMK actually performed as a feature selection algorithm . Both IMK and SSMK selected small subsets of features or basis functions ( around 25 features ) , and SSMK ran 100 times faster than solving problem ( 12 ) without the CG boosting scheme .
6 Conclusions
We have explored the mixture of kernels model in semisupervised learning settings . By using kernels with different geometric properties , allowing kernels to associate with different bandwidths or parameter values , and allowing the basis functions introduced by kernels to locate at unlabelled data points , we enhance the capability of kernel methods to adapt to tasks with various target functions . The optimization problems involved in model construction can be solved in a very efficient way through the exploitation of column generation techniques .
SVM like algorithms have been previously proposed for transductive learning as integer programs [ 2 ] and semidefinite programs [ 6 ] using overall risk minimization posed by Vapnik [ 21 ] . To achieve computational efficiency , our
7
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
)
%
( y t i v i t i s n e S
0
0
1
ROC Plot
IMK unlabel SSMK unlabel IMK test SSMK test
2
3
False Positive Rate Per Volume
4
5
6
7
8
Figure 2 . Plots of ROC curves for lungCAD . method does not minimize the overall risk . One of the future work is to extend the CG approach to integer programs formed for Vapnik ’s transduction problems . Another direction is to incorporate our approach in active learning tasks where a set of unlabelled examples need to be identified and given priority to obtain their observed labels in experimental investigation . The columns that are selected by the SSMK algorithm provide clues for which examples should be examined in the subsequent experiment .
References
[ 1 ] M . S . Bazaraa , H . D . Sherali , and C . M . Shetty . Nonlinear Programming : Theory and Algorithms . John Wiley & Sons , Inc . , New York , NY , 1993 .
[ 2 ] K . Bennett and A . Demiriz . Semi supervised support vector machines . In D . A . Cohn , M . S . Kearns , and S . A . Solla , editors , Advances in Neural Information Processing Systems 12 , pages 368–374 . MIT Press , Cambridge , MA , 1998 .
[ 3 ] K . Bennett , M . Momma , and M . Embrechts . MARK : A boosting algorithm for heterogeneous kernel models . In Proceedings of SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 24–31 , 2002 .
[ 4 ] K . P . Bennett . Machine Learning via Mathematical Programming . PhD thesis , University of Wisconsin , Madison , Wisconsin , 1993 .
[ 5 ] J . Bi , T . Zhang , and K . P . Bennett . Column generation boosting methods for mixture of kernels . In R . Kohavi , J . Gehrke , W . DuMouchel , and J . Ghosh , editors , Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 521–526 , 2004 .
[ 6 ] T . D . Bie and N . Cristianini . Convex methods for transduction . In S . Thrun , L . Saul , and B . Sch¨olkopf , editors , Ad
8 vances in Neural Information Processing Systems 16 . MIT Press , Cambridge , MA , 2004 .
[ 7 ] C . M . Bishop . Neural Networks for Pattern Recognition .
Oxford University Press , New York , 1995 .
[ 8 ] O . Bousquet and D . J . L . Herrmann . On the complexity of learning the kernel matrix . In S . T . S . Becker and K . Obermayer , editors , Advances in Neural Information Processing Systems 15 , pages 399–406 . MIT Press , Cambridge , MA , 2003 .
[ 9 ] K . Crammer , J . Keshet , and Y . Singer . Kernel design using boosting . In S . T . S . Becker and K . Obermayer , editors , Advances in Neural Information Processing Systems 15 , pages 537–544 . MIT Press , Cambridge , MA , 2003 .
[ 10 ] A . Demiriz , K . P . Bennett , and J . Shawe Taylor . Linear programming boosting via column generation . Machine Learning , 46(1–3):225–254 , 2002 .
[ 11 ] Y . Freund and R . E . Schapire . A decision theoretic generalization of on line learning and an application to boosting . Journal of Computer and System Sciences , 55:119–139 , 1997 .
[ 12 ] T . Hastie , R . Tibshirani , and J . Friedman . The Elements of Statistical Learning : data mining , inference and prediction . Springer Verlag , New York , 2001 .
[ 13 ] ILOG .
CPLEX Optimizer . vision , 889 Alder Avenue , http://wwwcplexcom/ , 2004 .
ILOG CPLEX DiIncline Village , Nevada ,
[ 14 ] G . Lanckriet , N . Cristianini , P . Bartlett , L . Ghaoui , and M . I . Jordan . Learning the kernel matrix with semidefinite programming . Journal of Machine Learning Research , 5:27– 72 , 2003 .
[ 15 ] O . L . Mangasarian . Generalized support vector machines . In P . Bartlett , B . Sch¨olkopf , D . Schuurmans , and A . Smola , editors , Advances in Large Margin Classifiers , pages 135– 146 . MIT Press , 2000 .
[ 16 ] S . G . Nash and A . Sofer . Linear and Nonlinear Program ming . McGraw Hill , New York , NY , 1996 .
[ 17 ] C . S . Ong , A . J . Smola , and R . C . Williamson . Hyperkernels . In S . T . S . Becker and K . Obermayer , editors , Advances in Neural Information Processing Systems 15 , pages 478–485 . MIT Press , Cambridge , MA , 2003 .
[ 18 ] E . Parrado Hernandez , J . Arenas Garca , I . Mora Jimenez , , and A . Navia Vazquez . On problem oriented kernel refining . Neurocomputing , 55:135–150 , 2003 .
[ 19 ] E . Parrado Hernandez , I . Mora Jimenez , J . Arenas Garcia , A . R . Figueiras Vidal , and A . Navia Vazquez . Growing support vector classifiers with controlled complexity . Pattern Recognition Letters , 36:1479–1484 , 2003 .
[ 20 ] B . Sch¨olkopf , K.Sung , C . Burges , F . Girosi , P . Niyogi , T . Poggio , and V . Vapnik . Comparing support vector machines with gaussian kernels to radial basis function classifiers . IEEE Transactions on Signal Processing , 45(11):2758–2765 , 1997 .
[ 21 ] V . N . Vapnik . Statistical Learning Theory . John Wiley &
Sons , Inc . , New York , 1998 .
[ 22 ] X . Zhu . Semi supervised learning with graphs .
In Doctoral Thesis , CMU LTI 05 192 . Carnegie Mellon University , 2005 .
