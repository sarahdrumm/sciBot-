Parameter Free Spatial Data Mining Using MDL
Spiros Papadimitriou§ ∗
Risto A . V¨ais¨anen‡
Aristides Gionis‡ Heikki Mannila‡
Panayiotis Tsaparas‡ Christos Faloutsos§ †
§ Carnegie Mellon University
Pittsburgh , PA , USA
‡ University of Helsinki
Helsinki , Finland
Abstract
Consider spatial data consisting of a set of binary features taking values over a collection of spatial extents ( grid cells ) . We propose a method that simultaneously finds spatial correlation and feature co occurrence patterns , without any parameters . In particular , we employ the Minimum Description Length ( MDL ) principle coupled with a natural way of compressing regions . This defines what “ good ” means : a feature co occurrence pattern is good , if it helps us better compress the set of locations for these features . Conversely , a spatial correlation is good , if it helps us better compress the set of features in the corresponding region . Our approach is scalable for large datasets ( both number of locations and of features ) . We evaluate our method on both real and synthetic datasets .
1 Introduction
In this paper we deal with the problem of finding spatial correlation patterns and feature co occurrence patterns , simultaneously and automatically . For example , consider environmental data where spatial locations correspond to patches ( cells in a rectangular grid ) and features correspond to species presence information . For each patch and species pair , the observed value is either one or zero , depending on whether the particular species was observed or not at that patch . In this case , feature co occurrence patterns would correspond to species co habitation and spatial correlation patterns would correspond to natural habitats for species groups . Combining the two will generate homogeneous regions characterised by a set of species that live in those regions . We wish to find “ good ” patterns of this form simultaneously and automatically . search Unit , HIIT , University of Helsinki , Finland .
∗Part of this work was done while the author was visiting Basic Re† This material is based upon work supported by the National Science Foundation under Grants No . IIS 0083148 , IIS 0209107 , IIS0205224 , INT 0318547 , SENSOR 0329549 , EF 0331657 , IIS 0326322 , NASA Grant AIST QRS 04 3031 , CNS 0433540 . This work is supported in part by the Pennsylvania Infrastructure Technology Alliance ( PITA ) . Additional funding was provided by donations from Intel , and by a gift from Northrop Grumman Corporation . Any opinions , findings , and conclusions or recommendations expressed in this material are those of the author(s ) and do not necessarily reflect the views of the National Science Foundation , or other funding parties .
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 )
1550 4786/05 $20.00 © 2005 IEEE
1
Spatial data in this form ( binary features over a set of locations ) occur naturally in several settings , eg : • Biodiversity data , such as the example above . • Geographical data , eg , presence of facilities ( shops , hospitals , houses , offices , etc ) over a set of city blocks . • Environmental data , eg , occurrence of different phenomena ( storms , hurricanes , snow , drought , etc . or ) over a set of locations in satellite images .
• Historical/linguistic data , eg , occurrence of different words in different counties , or occurrence of various types of historical events over a set of locations .
In all these settings , we would like to discover meaningful feature co occurrence and spatial correlation patterns . Existing methods either discover one of the two types of patterns in isolation , or require the user to specify certain parameters or thresholds .
We view the problem from the perspective of succinctly summarizing ( ie , compressing ) the data , and we employ the Minimum Description Length ( MDL ) principle to automate the process . We group locations and features simultaneously : feature co occurrence patterns help us compress spatial correlation patterns better , and vice versa . Furthermore , for location groups , we incorporate spatial affinity by compressing regions in a natural way .
Section 2 presents some of the background , in the context of our problem . Section 3 builds upon this background , leading to the proposed approach described in Section 4 . Section 5 presents experiments that illustrate the results of our approach . Section 6 surveys related work . Finally , in Section 7 we conclude . 2 Background
In this section we introduce some background , in the context of the problem we wish to solve . In subsequent sections we explain how we adapt these techniques for our purposes . 2.1 Minimum description length ( MDL )
In this section we give a brief overview of a practical formulation of the minimum description length ( MDL ) principle . For further information see , eg , [ 5 , 8 ] . Intuitively , the main idea behind MDL is the following : Let us assume that we have a family M of models with varying degrees of complexity . More complex models M ∈ M involve more parameters but , given these parameters ( ie , the model M ∈ M ) , we can describe the observed data more concisely . As a simple , concrete example , consider a binary sequence D := [ d(1),d(2 ) , . . . ,d(n ) ] of n coin tosses . A simple model M(1 ) might consist of specifying the number h of heads . Given this model M(1 ) ≡ {h/n} , we can encode the dataset D using L(D|M(1 ) ) := nH(h/n ) bits [ 26 ] , where H(· ) is the Shannon entropy function . However , in order to be fair , we should also include the number L(M(1 ) ) of bits to transmit the fraction h/n , which can be done using log . n bits for the denominator and 'log(n + 1)ff bits for the numerator h ∈ {0,1 , . . . ,n} , for a total of L(M(1 ) ) := log . n + 'log(n + 1)ff bits .
Definition 1 ( Code length and description complexity ) L(D|M(1 ) ) is code length for D , given the model M(1 ) . L(M(1 ) ) the model description complexity and L(D,M(1 ) ) := L(D|M(1 ) ) + L(M(1 ) ) is the total code length . is
A slightly more complex model might consist of segmenting the sequence in two pieces of length n1 ≥ 1 and n2 = n − n1 and describing each one independently . Let h1 and h2 be the number of heads in each segment . Then , to describe the model M(2 ) ≡ {h1/n1,h2/n2} , we need L(M(2 ) ) := log . n + 'lognff + 'log(n − n1)ff + 'log(n1 + 1)ff + 'log(n2 + 1)ff bits . Given this information , we can describe the sequence using L(D|M(2 ) ) := n1H(h1/n1 ) + n2H(h2/n2 ) bits . Now , assume that our family of models is M := {M(1),M(2)} and we wish to choose the “ best ” one for a particular sequence D . We will examine two sequences of length n = 16 , both with 8 zeros and 8 ones , to illustrate the intuition .
( 1 ) 1 ) = 16H(1/2 ) = 16 and L(M
( 2 ) 1 ) ≈ 19 . The total code lengths are L(D1,M
Let D1 := {0,1,0,1 , · · · ,0,1} , with alternating values . ( 1 ) We have L(D1|M 1 ) = ( 2 ) log . 16 + 'log(16 + 1)ff = 10 + 5 = 15 . However , for M 1 ( 2 ) 1 ) ≈ 15 and the best choice is n1 = 15 , with L(D1|M ( 1 ) 1 ) ≈ L(M ( 2 ) 1 ) ≈ 15 +19 = 34 . Thus , based 16 +15 = 31 and L(D1,M on total code length , the simpler model is better1 . The more complex model may give us a lower code length , but that benefit is not enough to overcome the increase in description complexity : D1 does not exhibit a pattern that can be exploited by a two segment model to describe the data . tiguous . We have again L(D2|M
Let D2 := {0 , · · · ,0,1 , · · · ,1} with all similar values con(1 ) 2 ) = 1The absolute codelengths are not important ; the bit overhead compared to the straight transmission of D tends to zero , as n grows to infinity .
( 1 ) 2 ) = 16 and L(M
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 )
1550 4786/05 $20.00 © 2005 IEEE
Image ( 4x4 )
Tree
Naive coding : 16 x 3 = 48 bits Entropy coding : 16 x H(7/16 , 5/16 , 4/16 ) = 24.74 bits
Structure : Colour : Total :
0 01111 1 1 1 ( depth−first order ) = 9 bits 7 x H(5/7 , 1/7 , 1/7 ) = 8.04 bits 17.04 bits
Figure 1 . Quadtree compression : The map on the left has 4 × 4 = 16 cells ( pixels ) , each having one of three possible values . The resulting quadtree has 10 leaf nodes , again each having one of three possible values .
( 2 ) 2 ) = 8H(0 ) + 8H(1 ) = 0 and L(M
( 2 ) 15 . But , for M the best choice is n1 = n2 = 8 so that 2 ( 2 ) 2 ) ≈ 24 . The L(D2|M ( 1 ) 2 ) ≈ 16 + 15 = 31 and total code lengths are L(D2,M ( 2 ) 2 ) ≈ 0 + 24 = 24 . Thus , based on total code L(D2,M length , the two segment model is better . Intuitively , it is clear that D2 exhibits a pattern that can help reduce the total code length . This intuitive fact is precisely captured by the total code length .
In fact , this simple example is prototypical of the groupings we will consider later . More generally , we could consider a family M := {M(k ) | 1 ≤ k ≤ n} of k segment models and apply the same principles . Furthermore , the datasets we will consider are two dimensional matrices D := [ d(i , j) ] , instead of one dimensional sequences . In Section 3.2 we address both of these issues . To complicate matters even further , one of the dimensions of D has a spatial location associated with it . Section 4 presents data description models that also incorporate this information .
In fact , choosing the appropriate family of models is nontrivial . Roughly speaking , at one extreme we have the singleton family of “ just the raw data , ” which cannot describe any patterns . At the other extreme , we have “ all Turing machine programs that produce the data as output , ” which can describe the most intricate patterns , but make model selection intractable . Striking the right balance is a challenge . In this paper , we address it for the case of spatial data .
2.2 Quadtree compression
A quadtree is a data structure that can be used to efficiently index contiguous regions of variable size in a grid . It has been used successfully in image coding and has the benefit of small overhead and very efficient construction [ 28 ] . Figure 1 shows a simple example . Each internal node in a quadtree corresponds to a partitioning of a rectangular region into four quadrants . The leaf nodes of a quadtree represent rectangular groups of cells and have a value p associated them , where p is the group ID . In the following we briefly describe quadtree codelengths .
2
Structure . The structure of a quadtree uniquely corresponds to a partitioning of the grid . For example , the partitioning into three regions in Figure 1 on the left corresponds to the structure on the right . This partitioning is chosen in a way that respects spatial correlations . The structure can be described easily by performing a traversal of the tree and transmitting a zero for non leaf nodes and a one for leaf nodes . The traversal order is not significant ; we choose depth first order ( see Figure 1 ) . Values . Quadtree structure conveys information about the partition boundaries ( thick grid lines in Figure 1 ) . These capture all correlations : in effect , we have reduced the original set of equal sized cells to a ( smaller ) set of variablesized , square cells ( each one corresponding to a leaf node in the quadtree ) . Since the correlations have already been taken into account , we may assume that the leaf node values are independent . Therefore , the cost to transmit the values is equal to the total number of leaf nodes , multiplied by the entropy of the leaf value distribution . Lemma 1 ( Quadtree codelength ) Let T be a quadtree with m . leaf nodes , of which m.p have value p , where 1 ≤ p ≤ k . Then , the number of internal nodes is 'm./3ff − 1 . Structure information can be transmitted using one bit per node ( leaf/non leaf ) and values can be transmitted using entropy coding . Therefore , the corresponding total codelength is
. fi
' ff
L(T ) = m.H mfi1 mfi , mfi2 mfi , . . . , mfi k mfi
+
4mfi 3
− 1
This has a straightforward but important consequence : Lemma 2 The codelength L(T ) for a quadtree T can be computed in constant time , if we know the distribution of leaf node values . In other words , for a full quadtree ( ie , one where each node has either zero or four descendants ) , if we know m . and m.p , for 1 ≤ p ≤ k , we can compute the cost in closed form , using Lemma 1 . Note that the quadtree does not have to be perfect ( ie , all leaves do not have to be at the same level ) . When a node is reassigned a different value , region consolidations may occur ( ie , pruning of leaves with same value ) . Updating m . and m.p will require time proportional to the number of consolidations , which are typically localized . In the worst case , the time will be O(log m ) if pruning cascades up to the root node .
3 Preliminaries
In this section we formalize the problem and prepare the ground for introducing our approach in Section 4 .
3.1 Problem definition
Assume we are given m cells on an evenly spaced grid ( eg , field patches in biological data ) and n features ( eg ,
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 )
1550 4786/05 $20.00 © 2005 IEEE
3
Symbol D m,n k , . k∗,.∗ QX ,QY Dp,q mp,nq |Dp,q| ρp,q H(· ) L(Dp,q|QX ,QY ,k, . ) L(D,QX ,QY ,k, . )
Definition Binary data matrix . Dimensions of D ( rows , columns ) ; rows correspond to cells . Number of row and column groups . Optimal number of groups . Row and column assignments to groups . Submatrix for intersection of p th row and q th column group . Dimensions of Dp,q . Number of elements |Dp,q| := mpnq . Density of 1s in Dp,q . Binary Shannon entropy function . Codelength for Dp,q . Total codelength for D .
Table 1 . Symbols and definitions . species ) . For each pair ( i , j ) , 1 ≤ i ≤ m and 1 ≤ j ≤ n , we are also given a binary observation ( eg , species presence/absence at each cell ) .
We want to group both cells and features , thus also implicitly forming groups of observations ( each such group corresponding to an intersection of cell and feature groups ) . The two main requirements are :
1 . Spatial affinity : Groups of cells should exhibit spatial coherence , ie , if two cells i1 and i2 are close together , then we wish to favour cell groupings that place them in the same group . Furthermore , spatial affinity should be balanced with feature affinity in a principled way .
2 . Homogeneity : The implicit groups of observations should be as homogeneous as possible , ie , be nearly all ones or all zeros .
The problem and our proposed solution can be easily extended to a collection of categorical features ( ie , taking more than two values , from a finite set of possible values ) per cell .
3.2 MDL and binary matrices
Let D = [ d(i , j ) ] denote a m × n ( m,n ≥ 1 ) binary data matrix . A bi grouping is a simultaneous grouping of the m rows and n columns into k and . disjoint row and column groups , respectively . Formally , let
QX : {1,2 , . . . ,m} → {1,2 , . . .,k} QY : {1,2 , . . . ,n} → {1,2 , . . . , .} denote the assignments of rows to row groups and columns to column groups , respectively . The pair {QX ,QY } is a bigrouping .
Based on the observation that a good compression of the matrix implies a good , concise grouping , both k , . as well as the assignments QX ,QY can be determined by optimizing the description cost of the matrix . Let Rp := Q−1 X
, Cq := Q−1 Y
1 ≤ p ≤ k,1 ≤ q ≤ . q
, p be the set of rows and columns assigned to row group p and column group q , with sizes mp := |Rp| and nq := |Cq| , respectively . Then , let
Dp,q := [ d(Rp,Cq) ] ,
1 ≤ p ≤ k,1 ≤ q ≤ be the sub matrix of D defined by the intersection of row group p and column group q . The total codelength L(D ) ≡ L(D,QX ,QY ,k , . ) for transmitting D is expressed as L(D ) = L(D|QX ,QY ,k , . ) + L(QX ,QY ,k , )
For the first part of Eq 3.2 , elements within each Dp,q are assumed to be drawn independently , so that L(Dp,q|QX ,QY ,k , . ) = 'log(|Dp,q| + 1)ff + |Dp,q|H where ρp,q is the density ( ie , probability ) of ones within Dp,q and |Dp,q| = mpnq is the number of its elements . This is analogous to the coin toss sequence models described in Section 21 Finally ,
ρp,q
L(D|QX ,QY ,k , . ) := ∑k For the second part of Eq 3.2 , row and column groupings p=1 ∑fiq=1 L(Dp,q|QX ,QY ,k , ) are assumed to be independent , hence
L(QX ,QY ,k , . ) = L(QX ,QY |k , . ) + L(k , . )
= L(QX |k ) + L(k ) + L(QY | . ) + L( )
Finally , a uniform prior is assigned to the number of groups , as well as to each possible grouping given the number of groups , ie ,
L(k ) = −logPr(k ) = logm
L(QX |k ) = −logPr(QX |k ) = log m m1 ··· mk and similarly for the column groups .
Using Stirling ’s approximation ln n! ≈ nln n − n and the fact that ∑i mi = m , we can easily derive the bound L(QX |k ) = log m m1 ··· mk m1!···mk! i=1 log mi! ≈ mlogm − ∑k = logm! − ∑k i=1 mi log mi m1 m , . . . , mk m log mi mi = −m∑k i=1 m ≤ mlogk . m = mH
= log m!
Therefore , we have the following : Lemma 3 The codelength for transmitting an arbitrary mto k mapping QX , where mp symbols from the range are mapped into each value p,1 ≤ p ≤ k , is approximately
L(QX |k ) = mH m1 m , . . . , mk m
3.3 Map boundaries
The set of all cells may form an arbitrary , complex shape , rather than a square with a side that is a power of two . However , we wish to penalize only the complexity of interior cell
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 )
1550 4786/05 $20.00 © 2005 IEEE
4
( a )
Map
( b )
Map negation
( c )
Unconditional quadtree
( d )
Conditional quadtree don’t care non−existent
5 bits ( structure )
1 bit ( structure ) Figure 2 . Quadtree compression to discount the complexity of the enclosing region ’s shape ; only the complexity of cell group shapes within the map ’s boundaries matters . group boundaries . The shape of boundaries on the edges ( eg , coastline ) of the map should not affect the cost .
For example , assume that our dataset consists of the three black cells in Figure 2(a ) . If all three cells belong to the same group and we encode this information na¨ıvely , then we get a quadtree with five nodes ( Figure 2(c) ) . However , the complexity of the resulting quadtree is only due to the fact that the bottom left is “ non existent . ”
If we know the shape of the entire map a priori , we can encode the same information using 1 bit , as shown in Figure 2(d ) . In essence , both transmitter and receiver agree upon a set of “ existing ” cell locations ( or , equivalently , a prior quadtree corresponding to the map description ) . This information should not be accounted for in the total codelength , as it is fixed for a given dataset . Given this information , all cells groups in the transmitted quadtree ( eg , group of both light and dark gray in Figure 2(d ) ) should be intersected with the set of existing cells ( eg , black in Figure 2(a ) ) to get the actual cells belonging to each group ( eg , only dark gray in Figure 2(d) ) .
Since the “ non existent ” locations are known to both parties , we do not need to take them into account for the leaf value codelength , which is still mH(m k/m . ) ( see Lemma 1 ) , where m.p is the number of quadtree leaves having value p ( 1 ≤ p ≤ k ) and m . = ∑k p=1 mp However , for the tree structure codelength we need to include the num0 of nodes corresponding to non existent locations ber m . ( eg , white in Figure 2(c) ) . Thus , the structure codelength is '4(m . + m .
1/m . , . . . ,m .
0)/3ff − 1 .
4 Spatial bi grouping
In the previous sections we have gradually introduced the necessary concepts that lead up to our final goal : coming up with a simple but powerful description for binary data , which also incorporates spatial information and which allows us to automatically group both cells as well as features , without any user specified parameters .
In order to exploit dependencies due to spatial affinity , we can pursue two alternatives :
1 . Relax the assumption that the values within each Dp,q are independent , thus modifying L(D|QX ,QY ,k , ) This amounts to saying that , given cells i1 and i2 belong to the same group , then it is more likely that feature j will be present in both cells if they are neighbouring .
2 . Assign a non uniform prior to the space of possible groupings , thus modifying L(QX ,QY ,k , ) This amounts to saying that two cells i1 and i2 are more likely to belong to the same group , if they are neighbouring .
We choose the latter , since our goal is to find cell groups that exhibit spatial coherence . In the former alternative , spatial affinity does not decide how we form the groups ; it only comes into play after the groupings have been decided . The second alternative fortunately leads to efficient algorithms . Each time we consider changing the group of a cell , we have to examine how this change affects the total cost . As we shall see , this test can be performed very quickly .
In particular , we choose to modify the term L(QX |k ) . Let us assume that the dataset has m = 16 cells , forming a 4 ×4 square ( see Figure 1 ) , and that cells are placed into k = 3 groups ( light gray , dark gray and black in the figure ) . Instead of transmitting QX as an arbitrary m to k mapping ( see Section 3.2 ) , we can transmit the image of m = 16 pixels ( cells ) , each one having one of k = 3 values . The length ( in bits ) of the quadtree for this image is precisely our choice of L(QX |k ) ( compare Lemmas 1 and 3 ) .
By using the quadtree codelength , we essentially penalize cell group region complexity . The number of groups is factored into the cost indirectly , since more groups typically imply higher region complexity . 4.1 Intuition
For concreteness , let us consider the case of patch locations and species presence features . The intuitive interpretation of cell and feature groups is the following :
• Row ( ie , cell ) groups correspond to “ neighbourhoods ” or “ habitats . ” Clearly , a habitat should exhibit a “ reasonable ” degree of spatial coherence .
• Column ( ie , species ) groups correspond to “ families . ” For example , a group consisting of “ gull and pelican ” may correspond to “ seabirds , ” while a group with “ eagle and falcon ” would correspond to “ mountain birds . ” The patterns we find essentially summarise species and cells into families and habitats . The summaries are chosen so that the original data are compressed in the best way . Given the simultaneous summaries , we wish to make the intersection of families and habitats as uniform as possible : a particular family should either be mostly present or mostly absent from a particular habitat . This criterion jointly decides the
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 )
1550 4786/05 $20.00 © 2005 IEEE
5
Quadtree length : Two clusters : Single cluster :
Block codelength :
21 ( struct . ) + 16 x H(1/2 , 1/2 ) = 37 bits [ above ] 1 bit [ root node only ]
Two clusters : 16 bits / Single cluster : 37 bits
Figure 3 . In this simple example ( 16 cells and 2 species , ie , 32 binary values total ) , if we require groupings to obey spatial affinity , we obtain the shortest description of the dataset ( locations and species ) if we place all cells in one group . Any further subdivision only adds to the total description complexity ( due to cell group region shapes ) . species of a family and the cells of a habitat . However , our quad tree model complexity favours habitats that are spatially contiguous without overly complicated boundaries .
The group search algorithms are presented in Intuitively , we alternatively re group subsection 42 cells and features , always reducing the total codelength . Example . A simple example is shown in Figure 3 . We choose this example as an extreme case , to clarify the tradeoffs between feature and spatial affinity . Experiments based on this boundary case are presented in section 5 . Assume we have two species , located on a square map in a checkerboard pattern ( ie , odd cells have only species A and even cells only species B ) . Consider the two alternatives ( we omit the number of bits to transmit species groups , which is the same in both cases ) :
• Two cell groups , in checkerboard pattern : One group contains only the even cells and the other only the odd cells . In this case , we need 37 bits for the quadtree ( see Figure 3 ) . For the submatrices , we need 'log(8 · 1 + 1)ff + 8H(1 ) = 4 bits for each of the four blocks ( two species groups and two cell groups ) , for a total of 16 bits . The total codelength is 37 + 16 = 53 bits .
• One cell group , containing all cells : In this case we need only 1 bit for the ( singleton node ) quadtree and 'log(32 · 1 + 1)ff + 32H(1/2 ) = 37 bits total for the submatrices . The total codelength is 37 +1 = 38 bits . Therefore , our approach prefers to place all cells in one group . The interpretation is that “ both species A and B occupy the same locations , with presence in ρ1,1 = 50 % of the cells . ” Indeed , if we chose to perfectly separate the species instead , the cell group boundaries become overly complex without any spatial affinity . Furthermore , if the number of species was different , the tipping point in the trade off between cell group complexity and species group “ impurity ” would also change . This is intuitively desirable , since describing exceptions in larger species groups is inherently more complex .
Algorithm INNER : Start with an arbitrary bi grouping ( Q0 Y ) of the matrix D into k row groups and . column groups . Subsequently , at each iteration t perform the following steps :
X ,Q0
1 . For this step , we will hold column assignments , ie , t+1 Qt X and , for each row Y , fixed . We start with Q X t+1 X ( i ) ← p , 1 ≤ p ≤ k so that i,1 ≤ i ≤ n , we update Q the choice maximizes the “ cost gain ”
:= Qt t t t L(D|Q − X |k ) Y ,k , . ) + L(Q X ,Q t t+1 X ,Q Y ,k , . ) + L(Q
L(D|Q t+1 . X |k )
We also update the corresponding probabilities ρt+1 p,q after each update to Q t+1 X .
2 . Similar to step 1 , but swapping group labels of columns instead and producing a new bi grouping ( Q t+2 t+1 X ,Q Y
) .
3 . If there is no decrease in total cost L(D ) , stop . Other wise , set t ← t + 2 , go to step 1 , and iterate .
Figure 4 . Row and column grouping , given the number of row and column groups .
4.2 Algorithms
Finding a global optimum of the total codelength is computationally very expensive . Therefore , we take the usual course of employing a greedy local search ( as in , eg , standard k means [ 13 ] or in [ 4] ) . At each step we make a local move that always reduces the objective function L(D ) . The search for cell and feature groups is done in two levels :
• INNER level ( Figure 4 ) : We assume that the number of groups ( for both cells and features ) is given and try to find the grouping that minimizes the total codelength . The possible local moves at this level are : ( i ) swapping feature vectors ( ie , group labels for rows of D ) , and ( ii ) swapping cell vectors ( ie , group labels for columns of D ) .
• OUTER level ( Figure 5 ) : Given a way to optimize for a specific number of groups ( ie , outer level ) , we progressively try the following local moves : ( i ) increase the number of cell groups , and ( ii ) increase the number of feature groups . Each of these moves employs the inner level search .
If k and . were known in advance , then one could use only INNER to find the best grouping . These moves guide the search towards a local minimum . In practice , this strategy is very effective . We can also perform a small number of restarts from different points in the search space ( eg , by randomly permuting rows and columns of D ) and keep the best result , in terms of total codelength L(D ) .
For each row ( ie , cell ) swap , we need to evaluate the change in quadtree codelength , which takes O(logm ) time
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 )
1550 4786/05 $20.00 © 2005 IEEE
6
Algorithm OUTER : Start with k0 = .0 = 1 and at each iteration T : 1 . Try to increase the number of row groups , holding the number of column groups fixed . We choose to split the row group p∗ with maximum per row entropy , ie , p∗ := argmax1≤p≤k ∑1≤q≤fi |Dp,q|H(ρp,q)/mp .
T +1fi X
Construct an grouping Q by moving each row i of the group p∗ that will be split ( QT X ( i ) = p∗ , 1 ≤ i ≤ m ) into the new row group kT +1 = kT + 1 , if and only if this decreases the per row entropy of group p∗ .
2 . Apply algorithm INNER with initial bi grouping
( Q
T +1fi X
,QT
Y ) to find new ones ( Q
T +1 X
T +1 ,Q Y
)
3 . If there is no decrease in total cost , stop and return ( k∗ , .∗ ) = ( kT , .T ) with corresponding bi grouping ( QT
Y ) . Otherwise , set T ← T + 1 and continue .
X ,QT
4–6 . Similar to steps 1–3 , but trying to increase column groups instead .
Figure 5 . Algorithm to find number of row and column groups . in the worst case ( where m is the number of cells ) . However , in practice , the effects of a single swap in quadtree structure tend to be local . Complexity . Algorithm INNER is linear in the number nnz of non zeros in D . More precisely , the complexity is = O(nnz · ( k + . +logm ) · T ) , ( nnz · ( k + . ) + nlogm ) · T O where T is the number of iterations ( in practice , about 10– 15 iterations suffice ) . We make the reasonable assumption that nnz > n + m . The nlogm term corresponds to the quad tree update for each row swap . In algorithm OUTER , we increase the total number k + . of groups by one at each iteration , so the overall complexity of the search is O((k∗ + .∗)2nnz + ( k∗ + .∗)nlogm ) , which is is linear with respect to the dominating term , nnz .
5 Experimental evaluation
In this section we discuss the results our method produces on a number of datasets , both synthetic ( to illustrate the intuition ) and real . We implemented our algorithms in Matlab 65 In order to evaluate the spatial coherence of the cell groups , we plot the spatial extents of each group ( eg , see also [ 29] ) . In each case we compare against non spatial bi grouping ( as presented in Section 32 ) This non spatial approach produces cell groups of quality similar to or better than , eg , straight k means ( with plain Euclidean distances on the feature bit vectors ) which we also tried . SaltPepper . This in Section 4.1 , with two features in a chessboard pattern . For the experiment , the map size is 32 ×32 cells , so the size essentially example the is
Non−spatial
Spatial
Non−spatial ( k=23 , l=18 )
Spatial ( k=14 , l=16 )
30
25
20
15
10
5
30
25
20
15
10
5
5
10
15
20
25
30
( a ) Non spatial grouping
5
10
15
20
25
30
( b ) Spatial grouping
Figure 6 . Noisy regions . of D is 1024 × 2 . The spatial approach places all cells in the same group , whereas the non spatial approach creates two row and two column groups . The total codelengths are ( for a detailed explanation , see Section 4.1 ) :
Groups 1 ×1 2 ×2
Codelength
Non spatial
Spatial
2048 + 22 = 2070
0 + 61 = 61
2048 + 14 = 2062 0 + 2431 = 2431
NoisyRegions . This dataset consists of three features ( say , species ) on a 32 ×32 grid , so the size of D is 1024 ×3 . The grid is divided into three rectangles . Intuitively , each rectangle is a habitat that contains mostly one of the three species . However , some of the cells contain “ stray species ” in the following way : at 3 % of the cells chosen at random , we placed a wrong , randomly chosen species . Figure 6 shows the groupings of each approach . The spatial approach favours more spatially coherent cell groups , even though they may contain some of the stray species , because that reduces the total codelength . Thus , it captures the “ true habitats ” almost perfectly ( except for a few cells , since the algorithms find a local minimum of the codelength ) . Birds . This dataset consists of presence information for 219 Finnish bird species over 3813 10Km×10Km patches which cover Finland . The 3813 × 219 binary matrix contains 33.8 % non zeros ( 281,953 entries out of 835,047 ) .
First , the cell groups in Figure 7(b ) clearly exhibit a higher degree of spatial affinity than those in Figure 7(a ) . In fact , the grouping in Figure 7(b ) captures the boreal vegetation zones in Finland : the light blue and green regions correspond to the south boreal , yellow to the mid boreal and red to the north boreal vegetation zone .
With resepct to the species groups , the method successfully captures statistical outliers and biases in the data . For example , osprey is placed in a singleton group . The data for this species was received from a special study , where a big effort was made to seek nests . Similarly , black throared diver is placed in a singleton group , most likely because of its good detectability from large distances . Rustic bunting has highly specialized habitat requirements ( mire forests ) and is also not grouped with any other species .
110
100
90
80
70
60
50
40
30
20
10
110
100
90
80
70
60
50
40
30
20
10
10
20
30
40
50
60
10
20
30
40
50
60
( a ) Non spatial grouping
( b ) Spatial grouping ( k = 14 cell groups , ( k = 23 cell groups , . = 16 species groups ) . = 18 species groups ) Figure 7 . Finnish bird habitats ; our approach produces much more spatially coherent cell groups ( see , eg , red , purple and light blue ) and captures the boreal vegetation zones .
6 Related work
In “ traditional ” clustering we seek to group only the rows of D , typically based on some notion of distance or similarity . The most popular approach is k means ( see , eg , [ 13] ) . There are several interesting variants , which aim at improving clustering quality ( eg , k harmonic means [ 30 ] and spherical k means [ 7 ] ) or determining k based on some criterion ( eg , X means [ 23 ] and G means [ 10] ) . Besides these , there are many other recent clustering algorithms that use an altogether different approach , eg , CURE [ 9 ] , BIRCH [ 31 ] , Chameleon [ 16 ] and DENCLUE [ 14 ] ( see also [ 11] ) . The LIMBO algorithm [ 2 ] uses a related , information theoretic approach for clustering categorical data .
The problem of finding spatially coherent groupings is related to image segmentation ; see , eg , [ 29 ] . Other more general models and techniques that could be adapted to this problem are , eg , [ 3 , 19 , 24 ] . However , all deal only with spatial correlations and cannot be directly used for simultaneously discovering feature co occurrences .
Prevailing graph partitioning methods are METIS [ 17 ] and spectral partitioning [ 22 ] . Related is also the work on conjunctive clustering [ 21 ] and community detection [ 25 ] . However , these techniques also require some user specified parameters and , more importantly , do not deal with spatial data . Information theoretic coclustering [ 6 ] is related , but focuses on lossy compression of contingency tables , with distortion implicitly specified by providing the number of row and column clusters . In contrast , we employ MDL and a lossless compression scheme for binary matrices which also incorporates spatial information . The more
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 )
1550 4786/05 $20.00 © 2005 IEEE
7 recent work on cross associations [ 4 ] is also parameter free , but it cannot handle spatial information . Finally , Keogh et al . [ 18 ] propose parameter free methods for classic data mining tasks ( ie , clustering , anomaly detection , classification ) based on standard compression tools .
Frequent itemset mining brought a revolution [ 1 ] with a lot of follow up work [ 11 , 12 ] . These techniques have also been extended for mining spatial collocation patterns [ 20 , 27 , 32 , 15 ] . However , all these approaches require the user to specify a support and/or other parameters ( eg , significance , confidence , etc ) .
7 Conclusion
We propose a method to automatically discover spatial correlation and feature co occurrence patterns . In particular :
• We group cells and features simultaneously : feature co occurrence patterns help us compress spatial correlation patterns better , and vice versa .
• For cell groups ( ie , spatial correlation patterns ) , we propose a practical method to incorporate and exploit spatial affinity , in a natural and principled way .
• We employ MDL to discover the groupings and the number of groups , directly from the data , without any user parameters .
Our method easily extends to other natural spatial hierarchies , when available ( eg , city block , neighbourhood , city , county , state , country ) , as well as to categorical feature values . Finally , we employ fast algorithms that are practically linear in the number of non zero entries .
References
[ 1 ] R . Agrawal and R . Srikant . Fast algorithms for mining asso ciation rules in large databases . In VLDB , 1994 .
[ 2 ] P . Andritsos , P . Tsaparas , R . Miller , and K . Sevcik . LIMBO :
Scalable clustering for categorical data . In EDBT , 2004 .
[ 3 ] S . Basu , M . Bilenko , and R . J . Mooney . A probabilistic framework for semi supervised clustering . In KDD , 2004 . [ 4 ] D . Chakrabarti , S . Papadimitriou , D . Modha , and C . Falout sos . Fully automatic cross associations . In KDD , 2004 .
[ 5 ] T . M . Cover and J . A . Thomas . Elements of Information
Theory . Wiley Interscience , 1991 .
[ 6 ] I . S . Dhillon , S . Mallela , and D . S . Modha .
Information theoretic co clustering . In KDD , 2003 .
[ 7 ] I . S . Dhillon and D . S . Modha . Concept decompositions for large sparse text data using clustering . Mach . Learning , 42 , 2001 .
[ 8 ] P . Gr¨unwald . A tutorial introduction to the minimum deIn Advances in Minimum Description length principle . scription Length : Theory and Applications . MIT Press , 2005 .
Proceedings of the Fifth IEEE International Conference on Data Mining ( ICDM’05 )
1550 4786/05 $20.00 © 2005 IEEE
8
[ 9 ] S . Guha , R . Rastogi , and K . Shim . CURE : an efficient clus tering algorithm for large databases . In SIGMOD , 1998 . [ 10 ] G . Hamerly and C . Elkan . Learning the k in k means .
In
NIPS , 2003 .
[ 11 ] J . Han and M . Kamber . Data Mining : Concepts and Tech niques . Morgan Kaufmann , 2000 .
[ 12 ] J . Han , J . Pei , Y . Yin , and R . Mao . Mining frequent patterns without candidate generation : A frequent pattern tree approach . Data Min . Knowl . Discov . , 8(1):53–87 , 2004 .
[ 13 ] T . Hastie , R . Tibshirani , and J . Friedman . The Elements of Statistical Learning : Data Mining , Inference , and Prediction . Springer , 2001 .
[ 14 ] A . Hinneburg and D . A . Keim . An efficient approach to clusIn KDD , tering in large multimedia databases with noise . 1998 .
[ 15 ] Y . Huang , H . Xiong , S . Shekhar , and J . Pei . Mining confident co location rules without a support threshold . In SAC , 2003 .
[ 16 ] G . Karypis , E H Han , and V . Kumar . Chameleon : Hierarchical clustering using dynamic modeling . IEEE Computer , 32(8 ) , 1999 .
[ 17 ] G . Karypis and V . Kumar . Multilevel algorithms for multi constraint graph partitioning . In SC98 , 1998 .
[ 18 ] E . Keogh , S . Lonardi , and C . A . Ratanamahatana . Towards parameter free data mining . In KDD , 2004 .
[ 19 ] J . Kleinberg and E . Tardos . Approximation algorithms for classification problems with pairwise relationships : Metric labeling and Markov random fields . JACM , 49(5):616–639 , 2002 .
[ 20 ] A . Leino , H . Mannila , and R . L . Pitk¨anen . Rule discovery In PKDD , and probabilistic modeling for onomastic data . 2003 .
[ 21 ] N . Mishra , D . Ron , and R . Swaminathan . On finding large conjunctive clusters . In COLT , 2003 .
[ 22 ] A . Y . Ng , M . I . Jordan , and Y . Weiss . On spectral clustering :
Analysis and an algorithm . In NIPS , 2001 .
[ 23 ] D . Pelleg and A . Moore . X means : Extending K means with efficient estimation of the number of clusters . In ICML , pages 727–734 , 2000 .
[ 24 ] R . B . Potts . Some generalized order disorder transforma tions . Proc . Camb . Phil . Soc . , 48:106 , 1952 .
[ 25 ] P . K . Reddy and M . Kitsuregawa . An approach to relate the web communities through bipartite graphs . In WISE , 2001 . [ 26 ] J . Rissanen and G . G . Langdon Jr . Arithmetic coding . IBM
J . Res . Dev . , 23:149–162 , 1979 .
[ 27 ] M . Salmenkivi . Evaluating attraction in spatial point patterns with an application in the field of cultural history . In ICDM , 2004 .
[ 28 ] D . J . Vaisey and A . Gersho . Variable block size image cod ing . In ICASSP , 1987 .
[ 29 ] R . Zabih and V . Kolmogorov . Spatially coherent clustering with graph cuts . In CVPR , 2004 .
[ 30 ] B . Zhang , M . Hsu , and U . Dayal . K harmonic means—a spatial clustering algorithm with boosting . In TSDM , 2000 . [ 31 ] T . Zhang , R . Ramakrishnan , and M . Livny . BIRCH : An efIn ficient data clustering method for very large databases . SIGMOD , 1996 .
[ 32 ] X . Zhang , N . Mamoulis , D . W . Cheung , and Y . Shou . Fast mining of spatial collocations . In KDD , 2004 .
