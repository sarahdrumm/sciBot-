A Rule Evaluation Support Method with Learning Models
Based on Objective Rule Evaluation Indexes
Hidenao Abe1 , Shusaku Tsumoto1 , Miho Ohsaki2 , and Takahira Yamguchi3 1Department of Medical Informatics , Shimane University , School of Medicine
89 1 Enya cho , Izumo , Shimane 693 8501 , Japan abe@medshimane uacjp , tsumoto@computer.org
2Faculty of Engineering , Doshisha University
3Faculty of Science and Technology , Keio University
Abstract
In this paper , we present a novel rule evaluation support method for post processing of mined results with rule evaluation models based on objective indexes . Post processing of mined results is one of the key issues to make a data mining process successfully . However , it is difficult for human experts to evaluate many thousands of rules from a large dataset with noises completely . To reduce the costs of rule evaluation procedures , we have developed the rule evaluation support method with rule evaluation models , which are obtained with objective rule evaluation indexes and evaluations of a human expert for each rule . Since the method is needed more accurate rule evaluation models , we have compared learning algorithms to construct rule evaluation models with the actual meningitis data mining result and actual rule sets from UCI datasets . Then we show the availability of our adaptive rule evaluation support method .
1 . Introduction
In recent years , data mining techniques have been widely known as a hopeful process to utilize huge data which are easily stored on information systems in various fields . Especially , IF THEN rule extraction methods are discussed as one of highly usable and readable output of data mining . However , to large dataset with hundreds attributes including noises , the process often obtains many thousands of rules . From such huge rule set , it is difficult for human experts to find out valuable knowledge which are rarely included in the rule set . To support such a crucial rule evaluation procedure , many efforts have done to select valuable ones using an objective rule evaluation index such as recall , precision , and other interestingness measurements[6 , 12 , 14 ] ( we call them ‘objective indexes’ later ) . However , it is also dif ficult to estimate a criterion of a human expert with single objective index , because subjective criterion of the expert is influenced by the amount of his/her knowledge and/or a passage of time .
To above issues , we have developed an adaptive rule evaluation support method for human experts with rule evaluation models , which predict criteria of a human expert based on multiple objective indexes . In Section 2 , we explain the rule evaluation model construction method concretely . Then we present a performance comparison of learning algorithms for constructing rule evaluation models in Section 3 . Finally , we discuss about the availability of our rule evaluation model construction approach .
2 . Rule Evaluation Support with Rule Evalu ation Model based on Objective Indexes
We considered the process of modeling rule evaluations of human experts as the process to clear up relationships between the human evaluations and features of input ifthen rules . With this consideration , we have decided to design such rule evaluation process as a learning task . Figure1 shows the process of rule evaluation model construction based on re use of human evaluations and objective indexes for each mined rule . At the training phase , attributes of a meta level training data set is obtained by objective indexes such as recall , precision and other rule evaluation values . The human evaluations for each rule are joined as class of each instance . To obtain this data set , a human expert has to evaluate the whole or part of input rules at least once . After obtaining the training data set , its rule evaluation model is constructed by a learning algorithm . At the prediction phase , a human expert receives predictions for new rules based on their values of the objective indexes . Then the evaluation for the new rules became new evaluation labels for another rule evaluation model training phase .
Training Phase :
Human Expert
Prediction Phase :
Mined Rules
New Mined Rules
Calculating Objective Rule Index Values
Evaluation labels for each rule
New Dataset for Mining
Calculating Objective Rule Index Values
Dataset for Mining
Training dataset for a rule evaluation model
Rule ID Accuracy 0.85 0.90
1 2
Recall 0.14 0.65
Human Expert Interesting
Not Interesting
Rule ID Accuracy 0.75 0.88 new1 new2
Recall 0.24 0.56 new n
0.95
0.32
n
0.59
0.01
Not Understandable
Learning Algorithm
The Rule Evaluation Model
Test dataset for the rule evaluation model
Human Expert Unknown Unknown
Unkown
Display with the predictions
A Rule Evaluation Model
Predicting rule evaluations with learned rule evaluation model
Human Expert
Figure 1 . Overview of method of rule evaluation models . the construction
3 . Performance Comparison of Learning Al gorithms for Rule Model Construction
To predict human evaluation labels of a new rule based on objective indexes more exactly , the method have to construct a better rule evaluation model . So we evaluate our method from the following three viewpoints : accuracies of rule evaluation models , learning curves of learning algorithms , and contents of learned rule evaluation models .
To evaluate our approach , we have taken the dataset from the mining result of a meningitis data mining[4 ] . Then to confirm the performance of our approach on the other datasets , we have also taken the rule sets from three UCI benchmark datasets[5 ] . To construct a dataset to learn a rule evaluation model , values of objective indexes have been calculated for each rule , taking 39 objective indexes investigated by Ohsaki et . al [ 9 ] .
To these dataset , we applied five learning algorithms to compare their performance as a rule evaluation model construction method . We used the following learning algorithms from Weka[13 ] : C4.5 decision tree learner[11 ] called J4.8 , neural network learner with back propagation ( BPNN)[7 ] , support vector machines ( SVM)1[10 ] , classification via linear regressions ( CLR)2[1 ] , and OneR[8 ] .
31 Constructing Rule Evaluation Models on an
Actual Datamining Result
In this case study , we have taken 244 rules , which are mined from a dataset consisted of appearances of meningitis patients and six kinds of diagnosis as shown in Table1 . For each rule , we labeled three evaluations ( I:Interesting , NI:Not Interesting , NU:Not Understandable ) , according to comments from a medical expert .
The kernel funcution was set up polynomial kernel .
1 2 We set up the elimination of collinear attributes and the model selec tion with greedy search based on Akaike Informatio Metric .
Dataset Diag C Cource Culture+diag Diag2 Course Cult find TOTAL
#Mined rules 53 22 57 35 53 24 244
#’I’ 15 3 7 8 12 3 48
#’NI’ 38 18 48 27 38 18 187
#’NU’ 0 1 2 0 3 3 9
Table 1 . Number of rules obtained by the meningitis datamining result and distribution of their class .
On the whole training dataset
Recall of
Precision of
NI 97.9 89.8 97.3 97.3 92.5
NU 66.7 55.6 0.0 0.0 0.0
I 80.0 65.0 68.0 71.4 57.4
NI 86.3 94.9 83.5 84.3 87.8
Leave One Out
Recall of
Precision of
NI 95.7 90.9 97.3 95.7 92.0
NU 0.0 0.0 0.0 0.0 0.0
I 63.6 50.0 68.0 60.7 37.1
NI 82.5 85.9 83.5 82.9 82.3
I 41.7 81.3 35.4 41.7 56.3
I 29.2 39.6 35.4 35.4 27.1
NU 85.7 71.4 0.0 0.0 0.0
NU 0.0 0.0 0.0 0.0 0.0
Acc . 85.7 86.9 81.6 82.8 82.0
Acc . 79.1 77.5 81.6 80.3 75.8
J4.8 BPNN SVM CLR OneR
J4.8 BPNN SVM CLR OneR
Table 2 . Accuracies(% ) , Recalls( % ) and Precisions( % ) of the five learning algorithms .
311 Comparison on Classification Performance The performances of the five learning algorithms to the whole training dataset and the results of Leave One Out(LOO ) are also shown in Table2 . LOO is a deterministic evaluation method to measure a robustness of learning algorithms to another unknown dataset .
As shown in Table2 , all of the accuracies , Recalls of I and NI , and Precisions of I and NI are higher than predicting default labels . In addition , the learning algorithms , which combine multiple objective indexes , achieve equal or higher performance than that of OneR . Looking at Recalls on class I , BPNN have achieved the highest performance . As for the other algorithms , they have tended to be learned classification patterns for the major class NI . On the accuracies of LOO , these learning algorithms have achieved from 75.8 % to 81.9 % , excepting that of minor class NU . Although OneR selects an adequate objective index to sort and classify 244 training datasets , it is not always work well to select just one objective index .
312 Learning Curves of the Learning Algorithms We have investigated learning curves of each learning algorithm to estimate how many evaluations are needed to construct an adequate rule evaluation model , because our method requires evaluations of mined rules by a human expert at least once . The upper table in Figure2 shows accuracies to the
% Accuracy to the whole training dataset
%training sample
J4.8 BPNN SMO CLR OneR
10
20
30
40
50
60
70
80
90
100
73.4 74.7 79.8 78.6 72.8 83.2 83.7 84.5 85.7 85.7 74.8 78.1 80.6 81.1 82.7 83.7 85.3 86.1 87.2 86.9 78.1 78.6 79.8 79.8 79.8 80.0 79.9 80.2 80.4 81.6 76.6 78.5 80.3 80.2 80.3 80.7 80.9 81.4 81.0 82.8 75.2 73.4 77.5 78.0 77.7 77.5 79.0 77.8 78.9 82.4 t e s a t i o i t a r y c a r u c c A i a D g n n a r T e o h W e h l t n o
100
95
90
85
80
75
70
SMO CLR J4.8 OneR BPNN
0
20
40
60
80
100
%training sample
Figure 2 . Learning Curves of accuracies( % ) on the learning algorithms with sub sampled training dataset . whole training dataset with each subset of training dataset . The percentages of achievements for each learning algorithm , comparing with the accuracy with the whole dataset , are shown in the lower chart of Figure2 . As shown in these results , SVM and CLR , which learn hype planes , achieves grater than 95 % with only less than 10 % of training subset . Besides , J4.8 and BPNN need more training data to learn accurate classifiers than SVM and CLR .
313 Rule Evaluation Models on the Actual Datamining Result Dataset Figure3 shows the readable rule evaluation models of the meningitis data mining result learned with OneR , J4.8 and CLR . The rule set of OneR is shown
( a )
( b )
IF YLI1 < 0.02 THEN "I" IF YLI1 >= 0.02 and YLI1 < 0.29 THEN "NI" IF YLI1 >= 0.29 and YLI1 < 0.43 THEN "I" IF YLI1 >= 0.43 and YLI1 < 0.44 THEN "NI" IF YLI1 >= 0.44 and YLI1 < 0.55 THEN "I" IF YLI1 >= 0.55 and YLI1 < 0.63 THEN "NI" IF YLI1 >= 0.63 and YLI1 < 0.83 THEN "I" IF YLI1 >= 0.83 THEN "NI"
<=0.11
NI(3.0 )
<=0.61
NU ( 3.0 )
Accuracy
<=0.20
Peculiarity
>0.61
I ( 6.0 )
Laplace Correction
<= 0.44
>0.44
>0.11 Recall
Prevalence
<=0.57
>0.20
<=0.25
GBI
I ( 170/50 )
<=0.05
Coverage >0.05
>0.57
NI ( 1380/90 ) >0.25 Precision
<=0.93
>0.93
NU(4.0 )
NI(2.0 )
I ( 2.0 ) NI ( 690/200 )
( c ) NU = 0.6202 * Specificity + 0.6224 * Accuracy + 1.1384 * Leverage + 0.6895 * RelativeRisk + 0.3704 * CertaintyFactor + 0.5722 * OddsRatio + 0.7656 * BI + 0.222 * Credibility + 0.3941 * LaplaceCorrection + 0.7986 * GiniGain + 0.0966 * GBI + 0.8895
NI = 1.7173 * Precision + 0.5063 * Accuracy + 0.5673 * RelativeRisk + 1.2718 * CertaintyFactor + 0.5955 * YulesQ + 0.4609 * K Measure + 0.4613 * PSI + 0.4181 * Peculiarity + 0.5302
I = 1.4417 * Precision + 0.7286 * Specificity + 0.4085 * Lift + 0.6297 * CertaintyFactor + 1.4477 * CollectiveStrength + 1.5449 * GiniGain + 0.5318 * PSI + 0.4981 * Peculiarity + 1.4872
Figure 3 . Learned models to the meningitis data mining result dataset . in Figure3(a ) . OneR has selected YLI1[14 ] to classify the evaluation labels . Although YLI1 was suggested to predict interestingness of a human expert , YLI1 estimates a correctness of each rule on a validation dataset . As shown in Figure3(b ) , J4.8 leaned the decision tree . At upper level includ ing the root node , this model takes Laplace Correction[12 ] , Accuracy , Precision and Recall , which represent correctness of rules . Coverage and Prevalence represent a generality of the antecedent and the consequent of a rule . At the lower level of the tree takes GOI[3 ] and Peculiarity[15 ] , which calculates specificity of each rule compared to other rules . Figure3(c ) shows linear models to classify each class . As for models to class NI and I , they have the same indexes such as Precision , Certainty Factor , PSI , and Peculiarity with opposite coefficients . The strongest factors on these models are Precision and Gini Gain , which increase their values with the correctness of a rule . To class NU , the strongest factor is Leverage based on Precision with a correction using a generality of a rule .
32 Constructing Rule Evaluation Models on Ar tificial Evaluation Labels
We have also evaluated our rule evaluation model construction method with rule sets from three datasets of UCI Machine Learning Repository to confirm the lower limit performances on probabilistic class distributions .
We selected the following three datasets : Mushroom , Heart , and Internet Advertisement Identification ( called InternetAd later ) . With these datasets , we obtained rule sets with bagged PART , which repeatedly executes PART[2 ] to bootstrapped training sub sample datasets . To these rule sets , we calculated the 39 objective indexes as attributes of each rule . As for the class of these datasets , we set up three class distributions with multinomial distribution . The class distribution for ‘Distribution I is P = ( 0.35 , 0.3 , 0.3 ) where pi is the probability for class i . Thus the number of class i in each instance Dj become piDj . As the same way , the probability vector of ‘Distribution II is P = ( 0.3 , 0.5 , 0.2 ) , and ‘Distribution III is P = ( 0.3 , 0.65 , 005 ) Table3(A ) shows us the datasets with three different class distributions .
321 Accuracy Comparison on Classification Performances To above datasets , we have attempted the five learning algorithms to estimate whether their classification results can go to or beyond the accuracies of just predicting each default class . Table3(B ) shows the accuracies of the five learning algorithms to each class distribution of the three datasets . J48 and BPNN always work better than just predicting a default class .
322 Evaluation on Learning Curves We also executed learning curve analysis on these artificial data sets as shown in Section 312 Table3(C ) shows sizes of minimum training subset , which needs to go beyond percentages of a default class by each learning algorithm . Although the performances SVM , CLR and OneR suffered from probabilistic class distributions of the datasets , the trend of that SVM and CLR can achieve as good performance as its of whole training dataset earlier is not suffered .
#Mined
#Class Labels
Rules
L1
L2
L3
%Def . Class
101
341
114
31
38
32
98 125 118
38
38
38
101
25
47
341 115 166
114
31
59
29
60
24
37.6
36.7
33.3
46.5
48.7
51.8
Distribution I
Mushroom
Heart
InternetAd
Distribution II
Mushroom
Heart
InternetAd
Distribution III
J48
BPNN SVM CLR OneR
J48
BPNN SVM
CLR OneR
Distribution I
Mushroom
Heart
InternetAd
Distribution II
Mushroom
Heart
InternetAd
Distribution III
51.5
47.5
62.3
58.4
51.9
74.6
58.4
56.9
73.7
59.4
62.5
77.2
38.6
34.9
36.8
42.6
48.6
49.1
50.5
38.4
44.7
52.5
50.1
54.4
49.5
53.1
57.9
53.5
58.4
58.8
Distribution I
Mushroom
Heart
InternetAd
Distribution II
Mushroom
Heart
InternetAd
Distribution III
14
30
4
50
92
41
11
85
2
36
75
36
18
3
102
28
27
3
67
341
59
15
64
9
67
146
62
Mushroom
101
5
63
33
62.4
Mushroom
62.4
66.3
62.4
61.4
64.4
Mushroom
101
101
101
101
Heart
341 105 215
Heart
65.1
Heart
21
7
63.0
65.8
32
75
114
InternetAd
( A ) Table 3 . ( A)Datasets of the rule sets learned from the UCI benchmark datasets , ( B)Accuracies( % ) on whole training datasets labeled with three different distributions , ( C)Number of minimum training sub samples to outperform %Def . class .
( C )
( B )
InternetAd
InternetAd
86.0
67.5
74
43
109
143
45
341
98
341
108
211
72.1
86.0
63.0
65.8
64.5
67.7
71.1
4 . Conclusion
In this paper , we have described rule evaluation support method with rule evaluation models to predict evaluations for an IF THEN rule based on objective indexes , re using evaluations of a human expert .
As the result of the performance comparison with the five learning algorithms , rule evaluation models have achieved higher accuracies than just predicting each default class on both of the actual and artificial dataset . We also investigated the amount of training sub samples to construct more accurate rule evaluation models with less samples . The result shows that we can construct enough models with only 10 % . These results indicate that our method can support human rule evaluations with adaptive rule evaluation models based on objective indexes .
As future works , we will introduce a selection method of learning algorithms to construct a proper rule evaluation model according to each situation . We also apply this rule evaluation support method to estimate other data mining result such as decision tree , rule set , and committee of them with objective indexes , which evaluate whole mining results .
Acknowledgement
This research is supported by the Grant in Aid for Young Scientists(B ) , 17700152 , by Ministry of Education , Science , and Culture for Japan .
References
[ 1 ] Frank , E . , Wang , Y . , Inglis , S . , Holmes , G . , and Witten , I . H . : Using model trees for classification , Machine Learning , Vol.32 , No.1 ( 1998 ) 63–76
[ 2 ] Frank , E , Witten , I . H . , Generating accurate rule sets without global optimization , in Proc . of the Fifteenth International Conference on Machine Learning , ( 1998 ) 144–151
[ 3 ] Gray , B . , Orlowska , M . E . : CCAIIA : Clustering Categorical Attributes into Interesting Association Rules . Proc . of Pacific Asia Conf . on Knowledge Discovery and Data Mining PAKDD 1998 ( 1998 ) 132–143
[ 4 ] Hatazawa , H . , Negishi , N . , Suyama , A , Tsumoto , S . , and Yamaguchi , T . : Knowledge Discovery Support from a Meningoencephalitis Database Using an Automatic Composition Tool for Inductive Applications , in Proc . of KDD Challenge 2000 in conjunction with PAKDD2000 ( 2000 ) 28–33
[ 5 ] Hettich ,
S . , Blake , C . L . , and Merz , C .
J . : databases of machine
UCI Repository [ http://wwwicsuciedu/˜mlearn/MLRepositoryhtml ] , Irvine , CA : University of California , Department of Information and Computer Science , ( 1998 ) . learning
[ 6 ] Hilderman , R . J . and Hamilton , H . J . : Knowledge Discovery and Measure of Interest , Kluwe Academic Publishers ( 2001 ) [ 7 ] Hinton , G . E . : “ Learning distributed representations of concepts ” , Proceedings of 8th Annual Conference of the Cognitive Science Society , Amherest , MA . REprinted in RGMMorris ( ed . ) ( 1986 )
[ 8 ] Holte , R . C . : Very simple classification rules perform well on most commonly used datasets , Machine Learning , Vol . 11 ( 1993 ) 63–91
[ 9 ] Ohsaki , M . , Kitaguchi , S . , Kume , S . , Yokoi , H . , and Yamaguchi , T . : Evaluation of Rule Interestingness Measures with a Clinical Dataset on Hepatitis , in Proc . of ECML/PKDD 2004 , LNAI3202 ( 2004 ) 362–373
[ 10 ] Platt , J . : Fast Training of Support Vector Machines using Sequential Minimal Optimization , Advances in Kernel Methods Support Vector Learning , B . Sch¨olkopf , C . Burges , and A . Smola , eds . , MIT Press ( 1999 ) 185–208
[ 11 ] Quinlan , R . : C4.5 : Programs for Machine Learning , Morgan
Kaufmann Publishers , ( 1993 )
[ 12 ] Tan , P . N . , Kumar V . , Srivastava , J . : Selecting the Right Interestingness Measure for Association Patterns . Proc . of Int . Conf . on Knowledge Discovery and Data Mining KDD 2002 ( 2002 ) 32–41
[ 13 ] Witten , I . H and Frank , E . : DataMining : Practical Machine Learning Tools and Techniques with Java Implementations , Morgan Kaufmann , ( 2000 )
[ 14 ] Yao , Y . Y . Zhong , N . : An Analysis of Quantitative Measures Associated with Rules . Proc . of Pacific Asia Conf . on Knowledge Discovery and Data Mining PAKDD 1999 ( 1999 ) 479–488
[ 15 ] Zhong , N . , Yao , Y . Y . , Ohshima , M . : Peculiarity Oriented Multi Database Mining . IEEE Trans . on Knowledge and Data Engineering , 15 , 4 , ( 2003 ) 952–960
