On Feature Selection through Clustering
Richard Butterworth
Univ . of Massachusetts Boston ,
Dept . of Computer Science ,
Boston , MA 02125 , rickb@csumbedu
Gregory Piatetsky Shapiro
KDnuggets , gregory@kdnuggets.com
Dan A . Simovici
Univ . of Massachusetts Boston ,
Dept . of Computer Science ,
Boston , MA 02125 , dsim@csumbedu
Abstract
We study an algorithm for feature selection that clusters attributes using a special metric and then makes use of the dendrogram of the resulting cluster hierarchy to choose the most relevant attributes . The main interest of our technique resides in the improved understanding of the structure of the analized data and of the relative importance of the attributes for the selection process .
1 Introduction
The performance , robustness , and usefulness of classification algorithms are improved when relatively few features are involved in the classification . Thus , selecting relevant features for the construction of classifiers has received a great deal of attention . A lucid taxonomy of algorithms for feature selection was discussed in [ 17 ] ; a more recent reference is [ 7 ] . Several approaches to feature selection have been explored , including wrapper techniques [ 12 ] , support vector machines [ 6 ] , neural networks [ 11 ] and prototypebased feature selection [ 9 ] that is close to our own approach . The central idea of this work is to introduce an algorithm for feature selection that clusters attributes using a special metric and , then uses a hierarchical clustering for feature selection .
Hierarchical algorithms generate clusters that are placed in a cluster tree , which is commonly known as a dendrogram . Clusterings are obtained by extracting those clusters that are situated at a given height in this tree .
Our intent is to show that good classifiers can be built by using a small number of attributes located at the centers of the clusters identified in the dendrogram . This type of data compression can be achieved with little or no penalty in terms of the accuracy of the classifier produced . The clustering of attributes helps the user to understand the structure of data , the relative importance of attributes . Alternative feature selection method mentioned above are excellent in reducing the data without having a severe impact on the accuracy of classifiers ; however , such methods cannot identify how attributes are related to each other . An object system is a pair S = ( S , H ) , where S is set called the set of objects of S , H = {A1 , . . . , Am} is a set of mappings defined on S . We assume that for each mapping Ai ( referred to as an attribute ( or a feature ) of S ) there exists a nonempty set Ei called the domain of Ai such that Ai : S −→ Ei for 1 ≤ i ≤ m . The value of an attribute Ai on an object t is denoted by t[Ai ] . This terminology is consistent with the terminology used in relational databases , where a table can be regarded as an object system ; however , the notion of object system is more general because objects have an identity as members of the set S , instead of being regarded as just m tuples of values . In this spirit , we shall refer to t[Ai ] as projection of t on Ai . Let S be a set . A partition on S is a non empty collection of subsets of S indexed by a set I , π = {Bi | i ∈ I} such i∈I Bi = S and i = j implies Bi ∩ Bj = ∅ . The sets that Bi are commonly referred to as the blocks of the partition π . The set of partitions on S is denoted by PART(S ) .
An attribute A of an object system S = ( S , H ) generates a partition πA of the set of objects S , where two objects belong to the same block of πA if they have the same projection on A . We denote by BA a the block of πA that consists of all tuples of S whose A component is a . Note that for relational databases , πA is the partition of the set of rows of a table that is obtained by using the group by A option of select in standard SQL . The set of partitions of a set can be naturally equipped with a partial order . For π , σ ∈ PART(S ) we write π ≤ σ if every block B of π is included in a block of σ , or equivalently , if every block of σ is an exact union of blocks of π . This partial order generates a lattice structure on PART(S ) ; this means that for every two partitions π , π ∈ PART(S ) there is a least partition π1 such that π ≤ π1 and π ≤ π1 and there is a largest partition π2 such that π2 ≤ π and π2 ≤ π . The first partition is denoted by π ∨ π , while the second is denoted by π ∧ π .
2 Distance between partitions and the Pear son index n To introduce a metric on the set of partitions of a finite set we define the mapping v : PART(S ) −→ R by v(π ) = i=1 |Bi|2 , where π = {B1 , . . . , Bn} . The mapping v is a lower valuation on PART(S ) , that is , v(π ∨ σ ) + v(π ∧ σ ) ≥ v(π ) + v(σ )
( 1 )
For every lower valuation v the mapping d for π , σ ∈ PART(S ) . : ( PART(S))2 −→ R defined by d(π , σ ) = v(π)+ v(σ)−2· v(π ∧ σ ) is a metric on PART(S ) ( see [ 3 , 2 , 14] ) . We will refer to d as the Barth´elemy Montjardet distance .
Using the cardinalities of the blocks of the partitions we can write d(π , σ ) =
|Bi|2 +
|Cj|2 − 2
|Bi ∩ Cj|2 , i j i j where π = {B1 , . . . , Bn} and σ = {C1 , . . . , Cp} . This metric was used for the development of an incremental clustering algorithm[15 ] . In this paper we use it to cluster attributes . For a partition π = {B1 , . . . , Bn} denote by Mπ and Let π = {B1 , . . . , Bn} , σ = {C1 , . . . , Cp} be two partitions . The contingency matrix of π , σ is the matrix Pπ,σ whose entries are given by pij = |Bi ∩ Cj| for 1 ≤ i ≤ n and 1 ≤ j ≤ p . The Pearson χ2 association index can be written in our framework as : mπ the largest and the smallest size of a block of π . i j
π,σ = χ2
( pij − |Bi||Cj|)2
|Bi| · |Cj|
.
It is well known ( See [ 1 ] ) that the asymptotic distribution of this index is a χ2 distribution with ( n − 1)(p − 1 ) degrees of freedom .
Theorem 2.1 Let S be a finite set and let π , σ ∈ PART(S ) , where π = {B1 , . . . , Bn} and σ = {C1 , . . . , Cp} . We have : v(π)+v(σ)−d(π,σ ) ≤ χ2 v(π)+v(σ)−d(π,σ )
2MπMσ
2mπmσ
− 2np + |S|2
π,σ ≤
− 2np + |S|2 . p2 ij
|Bi|·|Cj| − 2np + |S|2 .
Proof.Note that χ2
π,σ = i j
Since mπmσ ≤ |Bi||Cj| ≤ MπMσ , we have : |Bi| · |Cj| ≤ p2
MπMσ mπmσ p2 ij p2 ij
≤ ij
.
− 2np + |S|2 ≤ χ2
Thus , v(π ∧ σ ) MπMσ Since d(π , σ ) = v(π ) + v(σ ) − 2 equality follows immediately .
π,σ ≤ v(π ∧ σ ) j p2 mπmσ i
− 2np + |S|2 ij , the desired
The Pearson coefficient decreases with the distance and , thus , the probability that π and σ are independent increases with the distance . This suggest that partitions that are correlated are close in the sense of the Barth´elemy Montjardet distance ; therefore , if attributes are clustered using the corresponding distance between partitions we could replace clusters with their centroids and , thereby , drastically reduce the number of attributes involved in a classification without significant decreases in accuracy of the resulting classifiers .
3 Experimental Validation
We experimented with several data sets from the UCI dataset repository [ 4 ] and , due to space limitations we discuss only the results obtained with the votes and zoo datasets , which have a relative small number of categorical features . In each case , starting from the matrix ( d(πAi , πAj ) ) of Barth´elemy Montjardet distances between the partitions of the attributes A1 , . . . , An , we clustered the attributes using AGNES , an agglomerative hierarchical algorithm [ 10 ] implemented as a component of the cluster package of system R ( see [ 13] ) .
Clusterings were extracted from the tree produced by the algorithm by cutting the tree at various heights starting with the maximum height of the tree created above ( corresponding to a single cluster ) and working down to a height of 0 ( which consists of single attribute clusters ) . A ‘representative’ attribute was created for each cluster as the attribute that has the minimum total distance to the other members of the cluster , again using the Barth´elemy Montjardet distance . The J48 and the Na¨ıve Bayes algorithms of the WEKA package [ 16 ] were used for constructing classifiers on data sets obtained by projecting the initial data sets on the sets of representative attributes .
Attribute Set ( class attribute not listed ) 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 1,2,3,4,5,6,7,9,10,11,12,13,14,15 1,2,3,4,5,6,7,10,11,12,13,14,15 1,2,4,5,6,7,10,11,12,13,14,15 1,2,4,5,6,10,11,12,13,14,15 1,2,4,5,6,10,11,13,14,15 1,2,6,8,10,11,13,14,15 1,2,8,10,11,13,14,15 1,2,8,10,11,14,15 1,2,8,10,11,14 2,8,10,11,14 2,5,10,11 2,5,10 2,5 5
Classifiers
J48 % NB % 90.34 96.78 91.03 96.78 91.26 96.55 95.17 92.18 92.64 95.17 92.18 95.40 85.28 86.20 86.20 85.74 85.74 84.13 85.74 83.69 84.36 83.67 88.50 88.73 84.82 84.82 84.82 84.82 84.82 84.82
Table 1 . Accuracy of classifiers for the Votes dataset constructed on attribute sets obtained by clustering erated using ten fold cross validation by extracting centrally located attributes from cluster obtained by cutting the dendrogram at successive levels . The accuracy of these classifiers is shown in Table 1 . This experiment shows that our method identifies the most influential attribute 5 ( in this case ” el salvador aid ” ) . So , in addition to reducing number of attributes , the proposed methodology allows us to assess the relative importance of attributes .
A similar study was undertaken for the zoo database , after eliminating the attribute animal which determines uniquely the type of the animal . Starting from a dendrogram build by using the Ward method shown in Figure 2 we constructed J48 and Na¨ıve Bayes classifiers for several sets of attributes obtained as successive sections of the cluster tree . The results are shown in Table 2 . Note that attributes that are biologically correlated ( eg hair,milk , and eggs , or aquatic ( 6 ) , breathes ( 10 ) , and fins(12 ) ) belong to relatively early clusters ) .
We believe that the main interest of the proposed approach to attribute selection is the possibility of the supervision of the process allowing the user to opt between quasi equivalent attributes ( that is , attributes that are close relatively to the Barth´elemy Montjardet metric ) in order to produce more meaningful classifiers . We compared our approach with two existing attribute set selection techniques : the correlation based feature ( CSF ) selection ( developed in [ 8 ] and incorporated in the WEKA package and the wrapper technique , using the “ best first ” and the greedy method as search methods , and the J48 classifier for the classifier incorporated by the wrapper . For the zoo data set we obtained identical attribute sets with either “ best first ” or with handicapped infants budget resolution physician fee freeze el salvador aid religious groups in schools anti satellite test ban aid to nicaraguan contras
1 2 water project cost sharing 3 4 5 6 7 8 9 mx missile immigration 10 synfuels corporation cutback 11 education spending 12 13 superfund right to sue crime 14 15 duty free exports
Figure 1 . Dendrogram of votes Dataset using AGNES and the Ward method
The dataset votes records the votes of 435 US Congressmen on 15 key questions , where each attribute can have the value ” y ” , ” n ” , or ” ? ” ( for abstention ) , and each Congressman is classified as a democrat or republican . It is interesting to note that by applying the AGNES clustering algorithm with the Ward method of computing the inter cluster distance the voting issues group naturally into clusters that involve larger issues , as shown in Figure 1 . For ” el salvador aid ” , ” aid to nicaraguan contras ” , example , ” mx missile ” , and ” anti satellite test ban ” are grouped quite early into a cluster that can be described as dealing with defence policies . Similarly , social budgetary legislation issues such as ” budget resolution ” , ” physician fee freeze ” , and ” education spending ” , are grouped together .
Two types of classifiers ( J48 and Na¨ıve Bayes ) were gen
16141315341258972111020000400006000080000100000120000Dendrogram of agnes(x = dvotesx , diss = TRUE , method = "ward")Agglomerative Coefficient = 0.51dvotesxHeight Attribute Selection
CSF
Experimental Results
Attr . set : 1,2,4,8,9,10,12,13,14 Accuracy for J48 : 91.08 % Accuracy for NB : 95.04 %
Wrapper with J48
Attr . set : 1,2,4,8,9,12,13 Accuracy for J48 : 96.03 % Accuracy for NB : 92.07 %
Table 3 . Accuracy of classifiers obtained through attribute selection techniques the greedy method . The results are shown in Table 3 .
These results suggest that this method is not as good for accuracy as the the wrapper method or CSF . However , the tree of attributes helps to understand the relationships between attributes and their relative importance .
4 Conclusion and Future Work
Attribute clustering help to build classifiers in a semisupervised manner allowing analysts a certain degree of choice in the selection of the features that may be considered by classifiers , and illuminating relationships between attributes and their relative importance for classification .
As stated in [ 7 ] , in early studies of relevance published in the late 90s [ 5 , 12 ] , few applications explored data with more than 40 attributes . With the increased interest of data miners in bio computing in general , and in microarray data in particular , classification problems that involve thousands of features and relatively few examples came to the fore . We intend to apply our techniques to this type of data .
References
[ 1 ] A . Agresti . An Introduction to Categorical Data Analysis .
John Wiley , New York , 1997 .
[ 2 ] J . Barth´elemy . Remarques sur les propri´et´es metriques des ensembles ordonn´es . Math . Sci . hum . , 61:39–60 , 1978 .
[ 3 ] J . Barth´elemy and B . Leclerc . The median procedure for partitions . In Partitioning Data Sets , pages 3–34 , Providence , 1995 . American Mathematical Society .
[ 4 ] C . L . Blake and C . J . Merz .
UCI Repository of machine learning databases . University of California , Irvine , Dept . of Information and Computer Sciences , http://wwwicsuciedu/∼mlearn/MLRepositoryhtml , 1998 . [ 5 ] A . Blum and P . Langley . Selection of relevant features and examples in machine learning . Artificial Intelligence , pages 245–271 , 1997 .
[ 6 ] M . Brown , W . Grundy , D . Lin , N . Cristiani , C . W . Sugnet , T . Furey , M . Ares , and D . Haussler . Knowledge based analysis of microarray gene expression data by using support vector machines . PNAS , 97:262–267 , 2000 .
1 hair 2 feathers 3 eggs 4 milk 5 6 7 8 airborne aquatic predator toothed
9 10 11 12 13 14 15 16 17 backbone breathes venomous fins legs tail domestic catsize type
Figure 2 . Dendrogram of zoo dataset using AGNES and the Ward method
Attribute Set ( class attribute not listed ) 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16 1,2,4,5,6,7,8,9,10,11,12,13,14,15,16 2,4,5,6,7,8,9,10,11,12,13,14,15,16 2,4,5,6,7,8,9,10,11,12,13,15,16 2,4,6,7,8,9,10,11,12,13,15,16 2,4,6,7,8,9,10,11,13,15,16 2,4,6,7,8,9,10,11,13,16 2,4,7,8,9,10,11,13,16 2,4,7,9,10,11,13,16 2,4,7,9,10,11,13 4,5,7,9,10,11 4,5,7,9,10 4,5,9,10 4,5,10 4,10 4
Classifiers
J48 % NB % 92.07 93.06 92.07 92.07 88.11 87.12 88.11 87.12 87.12 88.11 91.08 91.08 90.09 89.10 90.09 86.13 90.09 84.15 87.12 89.10 88.11 88.11 90.09 88.11 91.09 89.10 73.26 73.26 73.26 73.26 60.39 60.39
Table 2 . Accuracy of classifiers for the zoo dataset constructed on attribute sets obtained by clustering
134161317258791411156101202000400060008000Dendrogram of agnes(x = dz , diss = TRUE , method = "ward")Agglomerative Coefficient = 0.73dzHeight [ 7 ] E . Guyon and A . Elisseeff . An introduction to variable and feature selection . J . of Machine Learning Research , pages 1157–1182 , 2003 .
[ 8 ] M . A . Hall . Correlation based feature selection for machine learning . PhD thesis , The University of Waikato , Hamilton , New Zealand , 1999 .
[ 9 ] B . Hanczar , M . Courtine , A . Benis , C . Hannegar , K . Clement , and J . Zucker . Improving classification of microarray data using prototype based feature selection . SIGKDD Explorations , pages 23–28 , 2003 .
[ 10 ] L . Kaufman and P . J . Rousseeuw . Finding Groups in Data – An Introduction to Cluster Analysis . Wiley Interscience , New York , 1990 .
[ 11 ] J . Khan , J . Wei , M . Ringner , L . H . Saal , M . Ladanyi , F . Westerman , F . Berthold , M . Schwab , C . R . Antonescu , C . Peterson , and P . Meltzer . Classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks . Nature Medicine , 7:673–679 , 2001 .
[ 12 ] R . Kohavi and G . John . Wrappers for feature selection . Ar tificial Intelligence , pages 273–324 , 1997 .
[ 13 ] J . Maindonald and J . Brown . Data Analysis and Graphics
Using R . Cambridge University Press , Cambridge , 2003 .
[ 14 ] B . Monjardet . Metrics on parially ordered sets – a survey .
Discrete Mathematics , 35:173–184 , 1981 .
[ 15 ] D . Simovici and N . Singla . Metric incremental clustering of categorical data . In Proceedings of ICDM , pages 523–527 , 2004 .
[ 16 ] I . H . Witten and E . Frank . Data Mining – Practical Machine Learning Tools and Techniques with Java Implementations . Morgan Kaufmann , San Francisco , 2000 .
[ 17 ] D . Zongker and A . Jain . Algorithms for feature selection : An evaluation . In Proceedings of the International Conference on Pattern Recognition , pages 18–22 , 1996 .
