Hierarchy Regularized Latent Semantic Indexing
Yi Huang1 , Kai Yu2 , Matthias Schubert1 , Shipeng Yu1 , Volker Tresp2 , Hans Peter Kriegel1
1Institute for Computer Science , University of Munich huang@cipifilmude {kaiyu,volkertresp}@siemenscom
{schubert,spyu,kriegel}@dbsifilmude
2Siemens Corporate Technology
Abstract
Organizing textual documents into a hierarchical taxonomy is a common practice in knowledge management . Beside textual features , the hierarchical structure of directories reflects additional and important knowledge annotated by experts . It is generally desired to incorporate this information into text mining processes . In this paper , we propose hierarchy regularized latent semantic indexing , which encodes the hierarchy into a similarity graph of documents and then formulates an optimization problem mapping each document into a low dimensional vector space . The new feature space preserves the intrinsic structure of the original taxonomy and thus provides a meaningful basis for various learning tasks like visualization and classification . Our approach employs the information about class proximity and class specificity , and can naturally cope with multi labeled documents . Our empirical studies show very encouraging results on two real world data sets , the new Reuters ( RCV1 ) benchmark and the Swissprot protein database .
1 . Introduction
A characteristic of textual documents is the high dimensionality ( typically tens of thousands ) . Thus , dimensionality reduction plays an important role in reducing computational costs and in improving the performance of text mining algorithms . Another typical characteristic of text applications is that often a document is allowed to belong to more than one class , ie the documents are multi labeled . For example , a news article about a football team could belong to both categories “ sports ” and “ business ” . Large text databases usually contain large amounts of classes . To allow easy navigation and express the inheritance relationships between these classes , the classes are often organized in a class hierarchy or taxonomy .
The taxonomy is an intrinsic structure of categories and documents . Each node in a class hierarchy represents a subclass of the father node . The leaf nodes describe basic classes that are not distinguishable any further , while the root corresponds to the most general class , comprising all documents . An example is the large topic hierarchy of a web directory service like Yahoo! that allows us to navigate to any category among several thousands topics by just a few clicks . Additional examples for large topic trees are the library of congress catalogue or biological class systems like Gene Ontology [ 5 ] .
In this paper , we describe a novel approach to exploit a given class hierarchy for text indexing . The idea is to directly integrate the information that is contained in the class hierarchy , into a new highly descriptive feature space . We interpret the classes in the hierarchy as “ bridges ” connecting the documents and introduce a hierarchy regularized latent semantic indexing . Our method naturally incorporates the similarity between the classes into feature transformation . Thus , large distances between the documents belonging to very similar classes are penalized , while large distances between the documents belonging to dissimilar classes are encouraged . Furthermore , the generated output space considers the specificity of classes , ie very general classes are considered to be less informative than very specific classes . Thus , classes that are close to the leaf nodes and classes being characterized by a small number of documents play a more important role . Our method employs both the similarity and the specificity of the classes , an aspect that has not been sufficiently addressed by previous methods . Finally , our method handles multi labeled documents in a natural way , while other approaches ( eg see [ 15 ] ) often need to involve constraints having combinatoric complexity .
Our proposed hierarchy regularized approach is used to develop a novel textual feature reduction technique , called hierarchy regularized latent semantic indexing ( HLSI ) . The resulting feature space offers the possibility to integrate a class hierarchy into a variety of text mining and retrieval tasks . Furthermore , it increases the efficiency of these techniques due to the smaller dimensionality of the output space . Our experimental evaluation demonstrates that the proposed approach is capable to derive low dimensional and highly descriptive feature spaces that allow fast and accurate classification on two real world data sets . The first is the Reuters Corpus Volume 1 benchmark ( RCV1 ) and the second is the Swissprot [ 2 ] protein database .
The rest of the paper is organized as follows : Section 2 describes our hierarchy regularized approach . In section 3 , we briefly survey related work in the area of feature reduction and hierarchical classification of multi labeled documents . Section 4 presents our experimental results on two real world data sets and section 5 conclude the paper .
2 . Hierarchy Regularized Approach
21 Training Data
Given a set of predefined classes C and a set of labeled documents {(xi , yi)} , where xi is an n dimensional feature vector and yi ∈ C the class label , we want to learn a function that can predict the labels for new test documents . In hierarchical classification , the training data does not simply consist of “ feature label ” pairs , but also a hierarchical structure of classes , which offers some additional information about the characteristics of data . Formally , we define the training data in the following way .
Definition 21 A hierarchy structured training set T ( X,C , g , s ) consists of ( 1 ) a set of N labeled documents X = [ x1 , . . . , xN ] , where xi ∈ Rn ; ( 2 ) a set of classes C = {c1 , . . . , cl} ; ( 3 ) a function g : X × C → {−1 , 1} with g(xi , ck ) = 1 if xi belongs to ck and 1 otherwise ; ( 4 ) a function s : C\{cr} → C , such that s(ck ) gives the direct father class of ck , where cr is the root class .
Because the classes are organized in a tree structure , the following condition should be fulfilled : ck = s(ck ) , and g(xi , ck ) = 1 ⇒ g(xi , s(ck ) ) = 1 . Note that the definition allows the multi label case where one document can be assigned to multiple leaf classes .
In addition , we define some operators on the tree : ( 1 ) h(xi ) returns the corresponding classes as well as all of their ancestors containing xi ; ( 2 ) H(xi , xj ) = h(xi ) ∩ h(xj ) returns the common classes of xi and xj ; ( 3 ) |ck| is the number of documents in class ck .
22 Hierarchy Induced Similarity Graph
Figure 1 . A hierarchy structured training set and the corresponding hierarchy induced similarity graph .
Definition 22 A hierarchy induced similarity graph G|T ( V , E ) consists of ( 1 ) a set V of vertices with a bijective function to X ; ( 2 ) a set E ⊆ V × C × V of edges , where [ i , k , j ] = [ j , k , i ] ∈ E is the edge between documents xi and xj via class ck ∈ H(xi , xj ) , i = j .
In this graph , each vertex corresponds to an document xi . For each class ck ∈ H(xi , xj ) two documents xi , xj have in common , the graph induces an edge [ i , k , j ] . Since every document is part of the root class , there is at least one edge connecting any pair of documents and thus the graph is fully connected ( cf . figure 1 ) .
The hierarchy can be used to derive implications about the connections between the documents . The similarity of two documents naturally depends on the number of edges connecting them . If two documents share a common leaf class , the number of edges tends to be rather big because each of the predecessor classes provides an additional edge as well . Thus , documents sharing specific classes are connected by more edges than documents that only share very general common classes . However , the specificity of a class is not exclusively dependent on its level in the hierarchy , but also on the number of documents belonging to the class . For example , if two documents are the only documents belonging to a particular class , then the class is very specific and the connection between the both documents is very strong . In order to express the strength of these connections , we define the edge weight w : E → R+ as follows : w([i , k , j ] ) =
1 |ck|
( 1 )
A class hierarchy is not just a notion of class proximity , but also a way to describe the similarity between the documents . A class is like a “ bridge ” connecting all the documents within this class . Therefore , we define a hierarchyinduced similarity graph as follows : where the weights of edges from xi via ck to xj evenly divided by the size of ck . This is consistent with the intuition that more popular classes are less informative for indicating documents’ similarities . Accordingly , by summing over all the shared classes , the induced connection strengths be
C1flC2flC3flC4flC5flAflAflBflBflA,CflA,CflAflBflCflC1:1/3flC1:1/3flC1:1/3flC2:1/2flC3:1/2flhierarchy structured training setflhierarchy weighted similarity graphfl tween two documents is computed as w([i , j ] ) = w([i , k , j ] )
( 2 ) ck∈H(xi,xj )
Then the vertex degree d : V → R+ is defined as : d(i ) = w([i , j ] )
( 3 ) j which is the total strength of all of edges connected with xi . Our definition of edge weights and vertex degrees can also be justified from a random walk point of view . Suppose a reader is browsing documents in a hierarchical directory . The transition probability from document xi to document xj via category ck should be p(j , k|i ) = p(j|i , k)p(k|i ) = w([i , k , j ] )
( 4 ) d(i )
Then the expected transition probability from xi to xj is p(j|i ) = ck∈H(xi,xj ) p(j , k|i ) = w([i , j ] ) d(i )
( 5 )
Eq ( 5 ) indicates that the transition probability depends not only on the number of classes shared by xi and xj , but it is also dependent on the size of these classes . Therefore , transitions across high level branches are considered to be rather unlikely while transitions within deep or small branches occur with a rather high probability .
In our approach , multi labeled documents are naturally handled . More importantly , these multi labeled documents are connected to the documents from different branches and somehow inform a closeness of these branches . Thus the similarity of documents and the similarity of classes are further informing to each other , which is similar to the hubauthority idea in web search [ 12 ] .
23 Regularization on the Similarity Graph
We seek for a mapping function Φ : X → Rm , m ( cid:191 ) n , that maps feature vector x into a new m dimensional space . It is desired to ensure the mapping functions Φ(x ) to be consistent with respect to the structure of G|T ( V , E ) . Let the mapping function Φ contain m elementary functions , and each of them φ : X → R map documents into a onedimensional space . Intuitively , a tight connection between two documents should induce similar outputs in the new space . Similar to the idea of spectral clustering [ 17 ] , the cost induced by an one dimensional mapping function is defined as :
Γ(φ ) = w([i , j ] )
φ(xi ) − φ(xj )
( 6 )
( cid:164)2
( cid:163 ) i j
The cost function emphasizes the variations of φ(x ) between tightly connected documents . In the following , we call Γ(φ ) the smoothness functional , since it measures the non smoothness of φ with respect to the hierarchy structure . Furthermore , Eq ( 6 ) can now be rewritten into the following form :
( 7 ) where φ = [ φ(x1 ) , . . . , φ(xN ) ] and is an N×N matrix :
Γ(φ ) = φ
φ
( )[i,j ] = d(i ) , −w([i , j] ) , if i = j otherwise
( 8 )
In this paper , we mainly consider linear functions φ(x ) = wx . Then we replace Γ(φ ) by Γ(w ) and write the smoothness functional as :
Γ(w ) = wXXw
( 9 )
The cost can be easily plugged into a formalism of latent semantic indexing ( LSI ) , to ensure the derived features consistent with the structure of hierarchies ( see Sec 24 )
24 Hierarchy Regularized Latent Semantic Index ing
The high dimensionality ( typically tens of thousands ) of text data always hampers the generalization of learning machines and seriously increases the computational costs . However , in general , the effective subspace responsible for the document labels has often a lower dimensionality . Latent semantic indexing ( LSI)[6 ] is a popular featurereduction technique for text data that identifies such a subspace . The method is however unsupervised and cannot incorporates additional information .
In this section , we employ the hierarchical structure to identify the effective subspace of text data . Various algorithms ( eg clustering , classification and retrieval ) can then be efficiently and effectively based on the new low dimensional feature space .
First , we derive a formalism of LSI such that the hierarchy induced cost Eq ( 7 ) can be easily plugged in . Let Φ : X → Rm be the feature mapping consisting of m linear functions φj(x ) = w j x , j = 1 , . . . , m . LSI finds the projections of data X = [ x1 , . . . , xN ] by applying singular value decomposition ( SVD ) :
X = UDV where U = [ u1 , . . . , uN ] is an N × N matrix , D is an N × n diagonal matrix with diagonal entries sorted nonincreasingly , and V = [ v1 , . . . , vn ] an n × n matrix . Then the results of mapping Φ on X are given by the first m columns of U .
In the following theorem , we interpret SVD from a dif ferent point of view .
Theorem 23 Let X = UDV be the singular value decomposition of X . Then uj = Xwj where wj are the solutions to w2 min w∈Rn st wXXw = 1 ,
Xw ⊥ Xw1 , . . . , Xwj−1
Proof . we give the sketch . U = XVD−1 apparently suggests uj = Xwj . Let K = XX , then it is known that uj are the directions maximizing uKu with constraints uu = 1 and u ⊥ u1 , . . . , uj−1 . The objective can be replace by minu uK−1u . Inserting uj = Xwj to the optimization problem completes the proof .
Theorem 2.3 gives the formalism of LSI that enables us to easily perform hierarchy regularized LSI ( HLSI ) . Since we wish to have the mapping functions to be consistent with the class hierarchy , the optimization problem for HLSI is denoted as follows :
Since w⊥ does not affect Γ(w ) but only increases w2 , w⊥ must be zero at the optimum . Therefore w ∈ S which completes the proof .
Then , the HLSI problem has the dual form , φj(x ) = i(αj)ixi , x , j = 1 , . . . , m , where αj are solved by
γαKα + αKKα min α∈RN st αKKα = 1 ,
Kα ⊥ Kα1 , . . . , Kαj−1 where K = XX . The problem is also equivalent to a generalized eigenvalue problem :
( γK + KK)α = λKKα
Finally , the learned mapping functions transform a highdimensional feature vector x to a m dimensional space . In the new space , data mining and retrieval tasks can be efficiently done .
( 10 )
25 Hierarchy Regularized Classifier
HLSI also suggests a direct optimization approach to handle the multi label hierarchical categorization problem . Clearly , the simplest solution is to train binary classifiers for each leaf class ck . Given the training documents X = [ xi , . . . , xN ] with labels ( yk)i ∈ {+1,−1} for class ck , a linear classifier1 φk(x ) = w kx can be learned by ( wxi , ( yk)i ) + βw2
( 11 ) wk = arg min w i where ( ·,· ) is the loss function , β ∈ R+ , and w2 is the regularizer preventing overfitting . The optimization in Eq ( 11 ) treats all negative documents identically . There is no bigger penalty if a document is miss classified into a leaf class which is faraway in the tree from the correct class . Therefore , we insert Γ(w ) = wXXw into the optimization problem Eq ( 11 ) and get the following objective function
Jk(w ) =
( wxi , ( yk)i ) + ξΓ(w ) + βw2
( 12 ) i where β , ξ ∈ R+ and I ∈ RN×N is an identity matrix . In Eq ( 12 ) there are two parts of loss based on empirical data : one is the conventional classification loss ( wxi , ( yk)i ) , the other is the hierarchy induced loss Γ(w ) . The square error loss ( wxi , ( yk)i ) = ( wxi − yk)2 was often reported to achieve superior performance in text
1Linear support vector machine is the state of the art method for text categorization ( see [ 22 , 19] ) .
γw2 + wXXw min w∈Rn st wXXw = 1 ,
Xw ⊥ Xw1 , . . . , Xwj−1 where γ ∈ R+ , wXXw is the cost induced by the hierarchy structure , and γ determines how much the projections should tend to follow the structure of input features . From the regularization point of view , γ prevents the mappings from being over fitted by the hierarchy structure . When γ → ∞ , HLSI becomes identical to LSI .
By setting the derivatives of its Lagrange formalism to be zero , it turns out that the linear weights are the solutions to a generalized eigenvalue problem :
( γI + XX)w = λXXw
The m generalized eigenvectors with the smallest eigenvalues are the linear weights wj , j = 1 , . . . , m , of the feature mapping functions .
Since text data is usually very high dimensional , it is very expensive to solve the large scale generalized eigenvalue problem . The following theorem enables the algorithm to work in the dual space where the dimensionality depends on the number of documents .
Theorem 24 The solutions wj , j = 1 , . . . , m , to the HLSI problem have the form wj =
( αj)ixi = Xαj i
Proof . Let S be the space span{x1 , . . . , xN} and P the projection onto it . Then w = P w + ( I − P )w = w + w⊥ . categorization [ 22 , 21 ] . For this case , the estimate of wk has closed form : wk = ( XX + ξXX + βI)−1yk .
( 13 ) which is derived by setting ∂Jk(w )
∂w = 0 .
Let us note that the methods suggested in this section have a close connection to HLSI as suggested in Sec 24 Here , the feature projection is implicitly done via the regularization . Despite its equivalence , explicit feature mappings enable learning methods to work on a low dimensional feature space and greatly improve the efficiency . This advantage is very important for real world applications .
3 . Related Work
Dimensionality reduction is a well established approach in data mining and information retrieval . One sort of the most well known techniques is feature selection , like mutual information , information gain and χ2 statistic [ 20 ] . In general , established feature selection methods ignore the dependency between features , which exists obviously in textual data sets . Furthermore it is difficult for feature selection to deal with multi label problems . Another sort of dimensionality reduction techniques is feature transformation/mapping . A representative approach is latent semantic indexing [ 6 ] , which uses singular value decomposition ( SVD ) to find the principal components of term document matrices . However , this method is unsupervised and thus the found dimensions are not necessarily relevant with class labels .
Our approach is a supervised feature mapping method . It considers the co occurrence between features and handles multi labeled problems in a natural way . Similar methods are the canonical correlation analysis ( CCA ) [ 10 ] , partial least square ( PLS ) and linear discriminant analysis ( LDA ) [ 18 ] . LDA aims to find transformation directions that maximize distances between class means and minimize variances within classes . However , LDA can only handle the single label problems . PLS and particularly CCA are classical statistical methods and measure the linear correlation between two multidimensional data sets ( eg inputs and outputs ) . The difference of both methods is that in CCA the correlation is normalized by variances within two data sets . Compared to CCA , our approach considers additionally hierarchical structure of output space , when optimizing directions with respect to normalized correlations .
The resulting feature representations with our approach are usable for any problem settings in data mining and information retrieval . In particular for hierarchical classification one can add the information of the class taxonomy to the loss function as suggested in Sec 25 However , none of the former hierarchical classification approaches
[ 13 , 14 , 8 , 1 , 15 , 4 , 7 , 3 ] does directly influence the employed feature space using a class hierarchy . Another conceptual difference is that we use the information about the specificity of the classes as well as the information about the similarity between classes .
4 . Empirical Study
41 Data Sets
In order to demonstrate the advantages of the introduced approach , we evaluated our methods on two real world data sets . The first is the Reuters Corpus Volume 1 ( RCV1 ) which consists of 806,791 English news stories . We randomly chose 10,000 documents from this data set having 31,613 class labels . These class labels refer to a class hierarchy of 81 classes , 64 leaf classes and 17 inner classes . The depth of the tree is 4 and each topic is represented by at least 20 documents . In all 10,000 documents occur 9,705 different words . The second data set is derived from the SwissProt[2 ] protein database that contains textual annotations of proteins . The entries in Swissprot provide links to the class system of Gene Ontology ( GO ) [ 5 ] which is used as a class hierarchy . We selected the subtree “ Oxido reductase ” from GO , which contains 125 categories . The corresponding entries in Swissprot comprise 8,335 proteins having 18,955 labels . The class hierarchy has also a maximum depth of 4 and provides 94 leaf classes and 31 inner classes . For each category , there are at least 10 entries available . This document collection contains 10,404 different words . For both data sets , we derived an original feature space of word vectors by dropping the words being contained in less than 5 documents and afterwards applied TFIDF . The output space of labels is a subset of {−1 , 1}N×|C| ( see Def . 2.1 ( 3) ) .
42 Data Visualization
One important application of feature reduction is to visualize data patterns in a 2 or 3 dimensional space , providing an impression about the quality of the underlying feature space . Therefore , we used LSI , CCA and HLSI to project the documents into a two dimensional space and visualized them . The results for RCV1 data are displayed in Fig 2 . Let us note that we observed a similar visualization for the Swissprot data , but we had to omit it due to the space limitation . The mapping functions are computed based on 2000 documents and were afterwards applied to project another set 1000 documents2 . We visualized 3 top level classes and one second level class C15 which is a subclass of the class CCAT . In Fig 2 different colors and marks are used to distinguish the classes . The results of LSI and CCA do
2More data points make the figures difficult to render without color .
Figure 2 . Visualization of the RCV1 on two dimensional space ( left ) LSI , ( mittle ) CCA and ( right ) HLSI not present a very meaningful data distribution because the classes are not separated very well . On the other hand , using HLSI provided a visualization which is quite relevant to the class memberships , in the sense that documents from the same class often display a close distance . This result indicates that the method employs the multi label information to represent the class similarities . Since class C15 is a subclass of CCAT , HLSI mapped the members of C15 into a concentrated subregion of area it mapped the members of CCAT . Thus , the projection preserved the inheritance relationship within the data . The good performance of HLSI on visualization demonstrates that the proposed algorithm effectively detects the meaningful subspaces within hierarchical data . Thus , the resulting feature spaces should allow fast and accurate solutions for various learning and retrieval tasks .
43 Classification Performance
Our second set of experiments studied the quality of HLSI in terms of dimensionality reduction for text classifications . The experimental results are evaluated using macro averaged F1 and micro averaged F1 , which are suitable to measure the classification accuracy when the classes are very unbalanced . In particular , micro averaged F1 reflects the quality on the classes with a large number of positive documents , while macro averaged F1 emphasizes on the minor classes , which correspond to the leaf classes in the hierarchical case . As a comparison , we investigate the quality of 4 different feature spaces : The first was generated by LSI , the second by HLSI , the third by CCA and the last was the original feature space . For classification , we used a linear support vector machines ( SVMs ) that was implemented in the SV M light package [ 11 ] . For each run , we randomly selected 2000 documents with the constraint that each leaf class had at least 5 positive documents . Treating the selected data as training set , we trained classifiers for all inner classes and leaf classes . Then , the trained models were used to predict the class of the remaining 8000 documents . For LSI , CCA and HLSI , the same set of 2000 documents were employed for learning the feature mapping . Because of large number of training documents were used kernelized CCA [ 9 ] and kernelized PCA [ 16 ] . We changed the dimensionality of projections and compared change of performance . The experiment was randomized for 10 times and the mean and error bar of the results were computed .
Finally the results are shown in Fig 3 . We can see that , the full feature case is always working very well . HLSI gives the performance significantly better than LSI . In the case of micro averaged F1 for RCV1 data , the performance of 50 dimensional HLSI features is almost as good as full features , while the cases of more than 80 dimensional HLSI features are even better than full features . In the meantime LSI needs 200 dimensions to reach almost the same performance . Similar observations can be made in the other 3 subplots . In general , 50 dimensional HLSI features for RCV1 data and 80 dimensional features for Swissprot data are sufficient to give comparable accuracy as the full feature case , however , the calculation for training the mapping only needs to be done once , summing up over the training of all classifiers , the total cost is much smaller than SVMs using full features . In our experiments , we observed on RCV1 data a 12 times improvement of efficiency with 50dimensional HLSI , and on Swissprot database a 9 times improvement for 80 dimensional HLSI .
We also performed top down classification on the class hierarchy , as the method was mentioned in many papers ( eg [ 13] ) . However , we did not observe any big differences from the setting we just described . All the methods including HLSI showed the same behaviors . Moreover , we test the performance of the proposed hierarchy regularized classifier , however the method shows close predictive accuracy as SVMs using the full feature . The reason seems to be that with 2000 training documents , the classifiers converges to almost the same hyperplane anyway . Compared to CCA , HLSI gives significant better visualization performance and
CCATGCATMCATC15CCATGCATMCATC15CCATGCATMCATC15 quite better classification performance on RCV1 with 20 50 dimensions . It shows exactly that HLSI improves quality of found directions by using hierarchy .
44 Sensibility of γ
There is a parameter γ in HLSI algorithm . Here we examine how sensitive the performance of HLSI on the setting of γ . Here we run the same setting of experiments described in the last section , with only γ changed . All the evaluations are averaged from 10 random repeats . We only report the mean in Fig 4 , while omit the error bar since some curves stay very close , like the cases of γ = 0.01 or 0.001 in subplots ( c ) and ( d ) , which are almost completely overlapped . From the figures , we can see for the RCV1 data , the optimal choice of γ is 0.01 or 0.1 , while for the Swissprot data , the optimal setting is 0.001 or 001 In general , the methods using the hierarchy information is always better than LSI without this information . However , the setting for RCV1 data seems to be a little bit sensitive— large values ( eg γ → ∞ to become LSI ) or small values ( eg γ = 0.001 ) both degrade the performance . In general , the setting of parameter γ depends on the nature of data . In practice , we need a valid set to guide the selection .
5 . Conclusions
In this paper , we introduced a new method for using class hierarchies in text mining . Our method is based on a hierarchy regularized approach that incorporates the proximity of classes within the hierarchy which implies a connection between the documents belonging the same class . Additionally , the approach uses the specificity of classes which can be measured by the number of documents belonging to each class . We use our approach to derive a new hierarchy regularized method for feature transformation called HLSI . HLSI enables us to integrate the information within a class hierarchy into a variety of learning and retrieval tasks . Additionally , our experiments on two realworld text data sets demonstrate that the proposed methods are capable to derive low dimensional and highly descriptive feature spaces that mirror the structure of the underlying class hierarchy . Thus , they are well suited for a variety of learning tasks employing hierarchical class systems . For future work , an interesting direction is to develop a global method for feature selection based on hierarchy induced graphs . This is especially interesting for applications demanding human understandable class models .
References
[ 1 ] H . Blockeel , M . Bruynooghe , S . Dzeroski , J . Ramon , and J . Struyf . Hierarchical mult classification . In MRDM Work shop on Multirelational data mining at SIGKDD’02 , Edmonton , Canada , 2002 .
[ 2 ] B . Boeckmann , A . Bairoch , R . Apweiler , M C Blatter , A . Estreicher , E . Gasteiger , M . Martin , K . Michoud , C . O’Donovan , I . Phan , S . Pilbout , and M . Schneider . ” The SWISS PROT Protein Knowledgebase and its Supplement TrEMBL in 2003 ” . Nucleic Acid Research , 31:365–370 , 2003 .
[ 3 ] L . Cai and T . Hofmann .
Hierarchical document categorization with support vector machines . In Proc . 13th Conf . on Information and Knowledge Management(CIKM’04),Washington DC , USA , pages 78 – 87 , 2004 .
[ 4 ] N . Cesa Bianchi , C . Gentile , A . Tironi , and L . Zaniboni . Incremental algorithms for hierarchical classification . In Proc . 8th ann . Conf . on Neural Information Processing Systems , Vancouver , BC , Canada , 2003 .
[ 5 ] T . G . O . Consortium . ” Gene Ontology : Tool for the Unifica tion of Biology ” . Nature Genetics , 25:25–29 , 2000 .
[ 6 ] S . C . Deerwester , S . T . Dumais , T . K . Landauer , G . W . Furnas , and R . A . Harshman . Indexing by latent semantic analysis . Journal of the American Society of Information Science , 41(6):391–407 , 1990 .
[ 7 ] O . Dekel , J . Keshnet , and Y . Singer . Large margin hierarIn Proc . 21th International Conf . on chical classification . Machine Learning ( ICML’04 ) , Banff , Canada , 2004 .
[ 8 ] S . Dumais and H . Chen . ” Hierarchical Classification of Web Content ” . In Proc . 23rd Int . Conf . on Research and Development in Information Retrieval ( SIGIR’00 ) , pages 256–263 , 2000 .
[ 9 ] D . R . Hardoon , S . Szedmak , and J . Shawe Taylor . Canonical correlation analysis ; an overview with application to learning methods . Technical Report CSD TR 03 02 , Royal Holloway University of London , 2003 .
[ 10 ] H . Hotelling . Relations between two sets of variables .
Biometrika , 28:321–377 , 1936 .
[ 11 ] T . Joachims . Text categorization with support vector machine : learning with many relevant features . In Proceeding of ( ECML) 98 , 10th European Conference on Machine Learning , pages 137–142 , Chemnitz , DE , 1998 . Springer Verlag , Heidelberg , DE .
[ 12 ] J . M . Kleinberg . Authoritative sources in a hyperlinked en vironment . J . ACM , 46(5):604–632 , 1999 .
[ 13 ] D . Koller and M . Sahami . Hierarchically classifying documents using very few words . In Proc . 14th Int . Conf . on Machine Learning ( ICML’97 ) , Nashville , TN , pages 170–178 , 1997 .
[ 14 ] A . McCallum , R . Rosenfeld , T . Mitchell , and A . Ng . ” Improving Text Classification by Shrinkage in a Hierarchy of Classes ” . In Proc . 15th Int . Conf . on Machine Learning ( ICML’98 ) , Madison , WI , pages 359–367 , 1998 .
[ 15 ] J . Rousu , C . Saunder , S . Szedmak , and J . Shawe Taylor . On maximum margin hierarchical mulitlabel classification . In Proc . of Workshop on Learning with Structured Outputs at NIPS 2004 , Whistler , Canada , 2004 .
[ 16 ] B . Sch¨olkopf , A . Smola , and K R M¨uller . Kernel principal component analysis . In Advances in Kernel Methods Support Vector Learning , pages 327–352 , 1999 .
[ 17 ] J . Shi and J . Malik . Normalized cuts and image segmentation . In IEEE Conf . Computer Vision and Pattern Recognition(CVPR ) , 1997 .
( a ) Micro F1 on RCV1
( b ) Macro F1 on RCV1
( c ) Micro F1 on Swissprot
( d ) Macro F1 on Swissprot
Figure 3 . Classification of the RCV1 data set ( a,b ) and the Swissprot data set(c,d ) with different features
( a )
( b )
( c )
( d )
Figure 4 . Comparison different γ of the RCV1 data set ( a,b ) and the Swissprot data set(c,d ) with micro averaged F1 ( a,c ) and macro averaged F1 ( b,d ) .
[ 18 ] H . Wold . Partial least squares . Encyclopedia of the Statistical
Sciences , pages 581–591 , 1985 .
[ 19 ] Y . Yang and X . Liu . A re examination of text categorization methods . In The 22th Annual International SIGIR Conference ( SIGIR’99 ) , pages 42–49 , 1999 .
[ 20 ] Y . Yang and J . O . Pedersen . A comparative study on feature selection in text categorization . In D . H . Fisher , editor , Proc . 14th International Conference on Machine Learning ( ICML’97 ) , pages 412–420 , Nashville , US , 1997 . Morgan
Kaufmann Publishers , San Francisco , US .
[ 21 ] J . Zhang and Y . Yang . Robustness of regularized linear classifcation methods in text categorization . In The 26th Annual International SIGIR Conference ( SIGIR’99 ) , 2003 .
[ 22 ] T . Zhang and F . J . Oles . Text categorization based on regularized linear classification methods . Information Retrieval , ( 4):5–31 , 2001 .
050100150200001020304050607Micro−averaged F1DimensionalityLSIHLSICCAALLFEA05010015020000102030405Macro−averaged F1DimensionalityLSIHLSICCAALLFEA05010015020000102030405060708091Micro−averaged F1DimensionalityLSIHLSICCAALLFEA05010015020000102030405060708091Macro−averaged F1DimensionalityLSIHLSICCAALLFEA050100150200001020304050607Micro−averaged F1DimensionalityHLSI with γ → ∞HLSI with γ = 0.1HLSI with γ = 0.01HLSI with γ = 000105010015020000102030405Macro−averaged F1DimensionalityHLSI with γ → ∞HLSI with γ = 0.1HLSI with γ = 0.01HLSI with γ = 000105010015020000102030405060708091Micro−averaged F1DimensionalityHLSI with γ → ∞HLSI with γ = 0.1HLSI with γ = 0.01HLSI with γ = 000105010015020000102030405060708091Macro−averaged F1DimensionalityHLSI with γ → ∞HLSI with γ = 0.1HLSI with γ = 0.01HLSI with γ = 0.001
