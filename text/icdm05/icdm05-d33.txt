In Proc . 5th IEEE International Conference on Data Mining ( ICDM ) , Houston , TX , 2005
Effective and Efficient Distributed Model based Clustering
Hans Peter Kriegel , Peer Kr¨oger , Alexey Pryakhin , Matthias Schubert
Institute for Computer Science , University of Munich , Germany
{kriegel,kroegerp,pryakhin,schubert}@dbsifilmude
Abstract
In many companies data is distributed among several sites , ie each site generates its own data and manages its own data repository . Analyzing and mining these distributed sources requires distributed data mining techniques to find global patterns representing the complete information . The transmission of the entire local data set is often unacceptable because of performance considerations , privacy and security aspects , and bandwidth constraints . Traditional data mining algorithms , demanding access to complete data , are not appropriate for distributed applications . Thus , there is a need for distributed data mining algorithms in order to analyze and discover new knowledge in distributed environments . One of the most important data mining tasks is clustering which aims at detecting groups of similar data objects . In this paper , we propose a distributed model based clustering algorithm that uses EM for detecting local models in terms of mixtures of Gaussian distributions . We propose an efficient and effective algorithm for deriving and merging these local Gaussian distributions to generate a meaningful global model . In a broad experimental evaluation we show that our framework is scalable in a highly distributed environment .
1 Introduction
Many companies gather terabytes of data containing significant , hidden information that needs to be uncovered using data mining techniques . Nowadays , more and more companies typically are decentrally organized and spatially distributed . Each site generates its own data and manages its own data repository . The challenge , however , is to access current knowledge from anywhere at any time with a delay of no more than a few minutes . Analyzing and mining these distributed sources requires distributed data mining techniques because the wanted patterns should be mined on a global view of these distributed sources . However , the traditional approach for a data mining environment is a centralized warehouse based architecture where the relevant data is regularly uploaded into the warehouse for centralized data mining . Obviously , this centralized approach causes high response times and may be limited by network and security constraints . It does usually not exploit the full capacity of all distributed resources since the data mining step resides on the central sever while the computational power of the clients is idle . In addition , in terms of privacy issues , it may be inappropriate to send the entire data over a public network to a global server . Thus , the traditional , centralized data mining model is not suitable for most distributed environments . Thus , a scalable and privacy preserving solution for distributed data mining applications requires the distributed processing of data [ 10 ] . A good distributed data mining framework performs data mining operations based on the type and the availability of the distributed resources . As suggested in [ 10 ] , a distributed data mining solution consists of the following steps . First , a data mining algorithm is locally applied to each of the k sites separately and independently . The results are k local sets of patterns called local models . Second , the local models are transferred to a central server . The central server combines the local models to generate a global model . Third , the global model may optionally be sent back to local sites .
The data mining technique we focus in this paper is clustering which aims at partitioning the data objects into distinct groups ( clusters ) while maximizing the intra cluster similarity and minimizing the inter cluster similarity . Many clustering algorithms for the centralized approach have been proposed so far using different clustering notions , eg distribution (or model )based , center based , or densitybased ( cf . [ 7 ] for an overview ) . In general , all those methods are applicable for a distributed solution as far as they produce a local model in Step 1 of the distributed data mining process that is as compact as possible but provides as much information as needed for building a global model in Step 2 . Unfortunately , many traditional clustering algorithms produce a clustering that cannot be easily described by a simple prototype . For example , density based clustering [ 3 ] detects clusters of arbitrary shape . However , describing a cluster having a complex shape might become quite expensive possibly causing large transfer rates . Thus ,
1 a local model should describe each cluster by a “ suitable ” prototype . Obviously , such prototyping should also meet privacy constraints . We argue , that the EM clustering algorithm provides exactly such prototypes . EM describes the data set by a set of Gaussian distributions consisting of the cluster center ( mean ) and covariance matrix . The latter describes the density of points around the center of the cluster . If certain constraints are met , privacy is preserved because the exact values of the data objects cannot be retrieved from the distribution .
We propose a novel distributed clustering algorithm called DMBC ( Distributed Model Based Clustering ) . The local models are acquired using EM clustering . Since the necessary number of clusters on each site might be strongly varying , DMBC automatically determines a suitable number of local clusters based on three privacy and performance constraints . The constraints control the maximum transfer volume that is allowed from an individual site and assure that each local data object is described as good as possible and prohibit the transfer of clusters that could leas to a violation of privacy aspects . To combine the local clusters at the central server , the aggregation step of DMBC can employ two variants of parametrization to either derive a global clustering offering k clusters or an arbitrary set of clusters that are considerably different from each other . In both cases , DMBC derives a meaningful global mixture model of Gaussian in efficient time . Our broad experimental evaluation shows that DMBC is a scalable solution for clustering in a distributed environment that achieves comparative results compared to a centralized EM based approach .
The rest of the paper is organized as follows . Section 2 discusses related work on distributed clustering and introduces the main concepts of EM clustering . In Section 3 we describe our novel DMBC algorithm in more detail . Section 4 provides an extensive experimental evaluation of the performance and the accuracy of DMBC . Section 5 concludes the paper .
2 Related Work
2.1 Distributed and Parallel Clustering
In the following , we will review recent work on parallel and distributed clustering . Parallel clustering is related to the problem of distributed clustering because the data objects are also distributed over several clients where a local clustering is performed . The local clusterings are merged to produce the final model . However , parallel clustering methods can control the assignment of data objects to each site . Thus , the merge step is usually less complex and implying different problems than the merge step of distributed data mining approaches . However , several recent approaches for
2 distributed data mining are adoptions of parallel clustering algorithms .
Parallel versions of k means , k Harmonic Means , EM [ 2 , 5 ] , and DBSCAN [ 12 ] are all not applicable within distributed environments because all methods rely on a centralized view of the data during or before the clustering is computed .
In [ 11 ] a parallel algorithm is proposed for clustering web documents distributed randomly over several sites . Any clustering algorithm can be used to generate local clusters . The entire local clusters are sent back to the server rather than compact prototypes . Clusters are merged if they share a given number of documents which is determined by deriving maximum sized itemsets from the documents . Obviously , since all local documents are transferred to the server , this approach does not consider any privacy issues . In [ 8 ] a distributed version of DBSCAN [ 3 ] is presented . The local clusters are represented by special objects that have the best representative power . This representative power is based on two quality measures that take the density based clustering concepts into account . For each representative , a covering radius and a covering number is aggregated for the global merge step . The performance of the proposed method is heavily dependent on the number of representatives . If it is chosen too small , the accuracy significantly decreases . Otherwise , the runtime increases due to high transfer cost . In addition , since real data objects are sent to the global server , this approach does also not consider any privacy issues .
In [ 9 ] a single link hierarchical clustering algorithm for vertically distributed data is proposed . However , our new approach DMBC is focused on horizontally distributed data .
2.2 Model based Clustering
Clustering algorithms can also be used to obtain a compact representation of a data set . An efficient and effective way to represent a local subset of a distributed data set is to use a mixture of different distribution functions . The most prominent algorithm that tries to describe the data by multiple distribution functions is the EM algorithm [ 1 ] . In the following , we describe a variant of this algorithm which is used by our DMBC algorithm .
Let D be a set of d dimensional points , ie D ⊆ R d . The general idea of the EM algorithm is to describe the data by a mixture model M of k Gaussian distributions , where k is the only input parameter . Instead of assigning each object to a cluster as it is the case for k means based clustering algorithms , EM assigns each object to a cluster according to a weight representing the probability of membership . Each cluster C ∈ M is a tuple C = ( µC , ΣC ) , where µC is the mean value of all points in C and ΣC is the d × d covariance matrix of all points in C . To compute the probability distributions , we need the following concepts . The probability density of a point x ∈ D within a Gaussian density distribution C = ( µC , ΣC ) is computed in the following way :
NµC ,ΣC ( x ) =
1(2π)d|ΣC| e− 1
2 ( x−µC )T(ΣC )−1( x−µC ) .
The combined density for k clusters can then be com puted by : k . i=1
P ( x ) = wCi · NµCi
,ΣCi ( x ) , where wCi is the fraction of points that belongs to cluster Ci = ( µCi , ΣCi ) , ie wCi is the weight of Ci . Then , the probability that a point x ∈ D belongs to a cluster C can be computed by the rule of Bayes :
P ( C| x ) = wC
NµC ,ΣC ( x )
P ( x )
.
The log likelihood of a mixture model M = ( C1 , . . . , Ck ) of k Gaussian distributions which describes how good the model approximates the actual data set can be computed by :
E(M ) = log ( P ( x) ) .
. x∈D
The higher the value of E(M ) , the more likely it is that the data set D corresponds to the mixture M . Thus , the aim of the EM algorithm is to optimize the parameters of M in a way that E(M ) is maximized . For that purpose , the algorithm proceeds in four steps :
1 . Initialization Since the clusters , ie Gaussian distributions C1 , . . . , Ck , are unknown at the beginning , a set of k initial clusters are generated randomly . For that purpose , each point x ∈ D is randomly assigned to one cluster Ci . An initial model is produced by computing µC and ΣC for each cluster C ∈ M .
2 . Expectation Based on the current model , the parameters µC and ΣC can be computed for each cluster C ∈ M and the log likelihood E(M ) of this mixture model M is obtained .
3 . Maximization In this step , E(M ) is improved via a recomputation of the parameters for each of the k clusters . Given a mixture model M , the parameters µC , ΣC , and wC of each cluster C ∈ M are recomputed . The resulting mixture M . has an equal or higher log likelihood than M , ie
3
E(M ) ≤ E(M ) For improving the mixture , the parameters are recomputed as follows :
. x∈D
P ( C| x ) ,
1 |D| wC = fi x∈D x · P ( C| x ) fi x∈D P ( C| x ) fi x∈D P ( C| x )
µC = fi x∈D P ( C| x)( x − µC)( x − µC)T
,
.
ΣC =
4 . Iteration Step 2 and 3 are iterated until the loglikelihood of the improved mixture model M . differs from the log likelihood of the previous mixture M by a smaller value than a user specified threshold ε , ie until |E(M ) − E(M.)| < ε .
The result of the EM algorithm is a set of k ddimensional Gaussian distributions , each represented by the mean value µ and the covariance matrix Σ and a weight w . The assignment of a point x ∈ D to a cluster C is given by the probability P ( C| x ) . We thus can compute how likely a point is assigned to each of the k clusters .
The log likelihood of the result of the EM algorithm is usually dependent on the initial mixture model , ie on the model assumed in step 1 , and on the number of clusters k . In [ 4 ] a method for producing a good initial mixture is presented which is based on multiple sampling . It is empirically shown that using this method , the EM algorithm achieves accurate clustering results .
3 Distributed Model based Clustering
In the following , we will refer to the clustering generated by the centralized approach as centralized clustering and call the clusters that are part of the global clustering centralized clusters .
3.1 Problem Analysis
As discussed above , the centralized solution has several drawbacks which led to the distributed approach where the data is clustered locally at each site . Afterwards only the information about the local clusters are transfered to a central server . The server can now reconstruct the global clustering which should be as similar to the centralized clustering as possible , by combining the local clusters . For this recombination , we can distinguish the following cases : Case 1 : All objects of a global cluster are found on a single local site . In this case , the global cluster can be spotted easily at the local site and should be added to the global clustering on the central site .
Case 2 : The objects of a global cluster are spread over several sites . In this case , we have to distinguish : Case 2(a ) : All objects are rather well described by some local cluster . In this subcase , a good clustering algorithm should discover which local clusters belong to the same global cluster and merge them at the central site . Case 2(b ) : Some of the objects of a global cluster are locally considered as noise or as members of local clusters that are built from object predominantly belonging to other global clusters . These objects contribute to the wrong global cluster or noise . Case 2(c ) : A cluster is distributed over several sites and none of them contains enough data objects for deriving a local cluster . In this case , the objects of the global cluster are considered as noise or parts of other clusters at each client site . Only by combining the local objects the global cluster would become visible . Because of the last two subcases , it is quite difficult , if not impossible , to develop an efficient distributed clustering algorithm that transmits local clusters and exactly rebuilds the centralized clustering for all cases . Since some of the centralized clusters may only be discovered if the noise objects of several sites are combined , the clustering algorithm would have to transmit all objects that are not well described by any local cluster as well as the clusters . However , transmitting all these objects has two major drawbacks . First , privacy preservation gets almost impossible by transmitting single data objects . Second , with large amounts of noise , it becomes necessary to transmit large amounts of data as well and thus , the advantages of distributed clustering might get lost . However , since there is no other solution for this dilemma , a good distributed clustering algorithm should at least offer the possibility to adjust to the users preferences on privacy , performance and the degree of how good the derived distributed clustering corresponds to the centralized clustering . In the following , we will refer to the degree of how good a distributed clustering corresponds to the centralized clustering as the agreement of both clusterings . Note , that a good distributed clustering algorithm cannot guarantee an agreement of 100 % in all scenarios as discussed above .
In the following , we describe a method , called Distributed Model based Clustering ( DMBC ) that is based on the EM clustering of local sites . Instead of transmitting the complete local data set , we only transmit a number of local Gaussians and their weights to the central site . Since a Gaussian distribution is represented only by a mean vector and a covariance matrix , the amount of transferred information is much smaller . Therefore , the needed bandwidth is much smaller . Furthermore , the Gaussians derived by EM are always built according to all underlying data objects and drawing detailed conclusions about individual data objects is not possible in almost all settings . Since we will addition ally control the remaining cases , the privacy of local data objects is preserved . Thus , our method avoids the problem of building a global clustering and still derives a mixture of Gaussian distributions achieving a high agreement with the centralized clustering .
The DMBC algorithm proceeds in 4 steps :
Step 1 : The local data on each client site is clustered using EM . Thus , the data at each client site is now represented by a small but descriptive set of Gaussian distributions and a distribution of weights over these Gaussians . The number of clusters for the EM algorithm is optimized automatically with respect to the parametrization controlling the privacy level and the transfer volume . Step 2 : The local Gaussians and weights are transmitted to the central site . Step 3 : Similar local Gaussians are joined to find a compact global distribution . Thus , each cluster in a global EM clustering is only represented by a single Gaussian . Step 4 : The calculated global clustering can optionally be transmitted back to the client sites .
3.2 Computation of Local Models
To cluster the data at each client site , we employ the EM algorithm as described in section 22 The most important aspect of this step is the question how to choose the parameter k , ie the number of Gaussians that is used to describe the local data distribution . Since the data distribution can strongly vary between each site , simply selecting a global value for k as the expected number of global clusters might be rather inappropriate . Therefore , our algorithm automatically determines a particular value ki for each site Si . Let us note that ki not only influences the transfer volume , but also the privacy and exactness of the clustering as well . The larger ki , the higher is the probability that a Gaussian is strongly influenced by only a few data sets . In this case , the privacy can be seriously jeopardized because it might be easy to approximate the instances that are represented by this Gaussian . On the other hand , a large number of ki usually increases the tendency that all local data objects are well represented by a local Gaussian . To conclude , a very high value for ki will increase the agreement between our derived clustering and the global clustering , but it will also increase the transferred data volume .
To find a clustering containing an appropriate number of clusters , we first of all introduce the parameter kmax describing the maximum number of Gaussians for each local site . kmax limits the maximum transfer from a local site to the central site in step 2 and thus can be derived from the available bandwidth . To measure the degree that all data objects are well represented by the given clustering , we introduce the function cover(Ci,j ) .
Definition 1 ( Cover ) Let M = C1 , . . . , Ck be a mixture
4 localEM(Database D , Integer kmax ) maxcover = 0 ; bestClustering = ∅ ; for k := 1 to kmax do M := EM(D , k ) ; if cover(M ) = |D| then return M ; end if if cover(M ) > maxcover then maxcover = cover ; bestClustering = M ; end if end for return bestClustering ;
Figure 1 . Algorithm for local clustering . of Gaussians describing the density distribution within D . Furthermore , let t ∈ [ 0 , . . . , 1 ] be a probability threshold . Then , the cover of the model M , denoted by Cov(M ) , is defined as follows : Cov(M ) = |{ x | x ∈ D ∩ ∃ Ci ∈ M : P ( Ci| x ) ≥ t}|
Intuitively , the cover is the number of data objects that provide at least a probability of t for some Gaussian in the clustering . Let us note that Cov(M ) is related but different to the log likelihood E(M ) that is optimized by the EM algorithm .
The pseudo code of the algorithm localEM to derive a local clustering is depicted in Figure 1 . The algorithm chooses the smallest clustering M that achieves a maximum cover by successively increasing k and testing the cover of the resulting clustering .
At last , we have to control the level of privacy , we need to ensure . Thus , we have to measure how far it is possible to draw conclusions about individual data sets from the found clustering C . Therefore , we define the so called privacy score ( PScore ) : Definition 2 ( Privacy Score ) Let Ci ∈ M be a cluster that is described by a d dimensional Gaussian determined by the variance vector µi and a covariance matrix Σi . Then , the privacy score , denoted by P Score(Ci ) , is defined as follows : d .
P Score(Ci ) =
Σj,j j=1
The idea of the P Score(Ci ) is quite simple . Only if the variance in each dimension is very small , it is possible to draw conclusions about the underlying feature vectors . Let us note that the local cluster description of a cluster determined by the EM algorithm is always built using the complete data set . As a result , it is impossible to derive detailed
5 information about single feature vectors even for clusters having small weights if the variance values are large enough for at least a single dimension . Thus , we define a privacy threshold τp that is the lower limit for the P Score(Ci ) of a cluster Ci that is allowed to be transferred . If a cluster Ci has a smaller privacy score , ie P Score(Ci ) < τp , we do not transmit the cluster because it would be possible to conclude that there is at least one feature vector stored on the local site that strongly resembles the transferred mean value .
To conclude , at each site we determine the smallest EM clustering providing a maximum cover and afterwards transfer all clusters that do not violate the predefined level of privacy .
3.3 Computation of the Global Model
The purpose of this step is to combine the locally derived clusters to a distributed clustering describing the complete data distribution in a best possible way . The difficulty in this step is to find out which of the clusters are likely to describe the same global cluster . To find out which of the given local clusters should be joined , first of all we need a measure that describes the likelihood of two local Gaussians C1 and C2 to model the same global cluster . Simply , using the distance between mean vectors is not applicable here because the significance of this distance strongly decreases with increasing variance values . Therefore , we define a new measure that considers the dependency between variance and mean value , called mutual support .
Definition 3 ( Mutual Support ) Let C1 , C2 be two Gaussian determined by a mean vector µi and a covariance matrix Σi . Then the mutual support of C1 , C2 is given by :
' +∞
M S(C1 , C2 ) =
−∞
Nµ1,Σ1( x ) · Nµ2,Σ2( x)d x
Let us note that x is a d dimensional feature vector and thus M S( x ) is defined using the integral over all d dimensions . The mutual support has several characteristics that makes it well suited for measuring the similarity between two Gaussians . The larger the variance values become , the less steep are the probability density functions of the Gaussians and the less important is the distance between the mean values . Comparing a low variance distribution with a high variance Gaussian , will display a small mutual support . In the comparably small range where a low variance distribution displays strong density the high variance distribution provides only moderate density and in the large area where the high variance distribution displays still moderate density , the density of the low variance distribution decreases the product very strongly . Thus , the mutual support of two globalMerge(SetOfLocalClusters C , Integer k ) for each pair ( Ci , Cj ) ∈ C do compute M S(Ci , Cj ) ; end for sort the pairs wrt descending mutual support ; mark the first |C| − k pairs of clusters ; build the transitive closure over the pairs having some common clusters and unite them into a common global cluster ;
Figure 2 . Algorithm for global clustering .
Gaussians specified by very similar mean values but quite different covariance matrices is also rather small .
After finding a method to compare two local Gaussians , we now start to determine which of the local clusters should be merged . To determine a distributed clustering from a set of local clusters C with a number of k global clusters , we can now proceed as described in Figure 2 .
Another alternative for deriving a joined distributed clustering is to specify a threshold parameter τ and join all clusters displaying a mutual support of at least τ . In this case , all pairs of clusters ( Ci , Cj ) are marked if M S(Ci , Cj ) ≥ τ . Again we first of all , find the marked pairs of clusters that are connected by common clusters and afterwards merge all clusters in this connected set . Therefore , both approaches are independent of the order the clusters are merged . After determining which clusters have to be joined , we still need to derive a common Gaussian from a connected set of local clusters . Therefore , we derive a new mean µC for a set of Gaussian clusters C = {C1 , . . . , Cm} that are specified by µi and Σi in the following way . fim fim k=1(wCk k=1(wCk
· λ(Ck ) · µk ) · λ(Ck ) )
.
µC =
Here , λ(Ck ) = Cov(Ml ) where cluster Ck has its origin on site l , ie λ(Ck ) denotes the cover of site l . The entries of the covariance matrix for the ith line and the jth column are calculated as following :
Σi,j
C = ff +∞ −∞ ( fim fim k=1 wCk k=1(
λ(Ck)Nµk,Σk( x ) · ( xi − µi ff +∞ −∞ wCk
λ(Ck)Nµk,Σk( x))d x )
C)( xj − µj
Let us note that we again need to employ a multiple integral to calculate the new covariance matrix because we do not have the actual data distribution at each site . Therefore , we assume that the local density given by each local clustering is a well enough description of this distribution . To consider the number of data objects that are stored at each site , we additionally weight the influence of each distribution with the cover we transmitted from this site .
C))d x
M S(C1 , C2 ) =
Proof . Let Nµi µ and σ by :
1,σi
· exp− ( µi 2 · ( σi
1 − µi 2)2 1 − σi 2 ) 1 + σi 2 ) 2 = ϑi · Nµ,σ . If we replace d i=1
√ 1 2π · ( σi 1( x ) · Nµi 2 · σi
1
2,σi fi
The weight of C can be determined as Ci∈C wCi · λ(Ci ) fi Ci∈C λ(Ci ) wC =
.
3.4 Scaling to High Dimensional Data Sets
If we apply DMBC as proposed in the previous subsection on higher dimensional data sets ( d > 2 ) , we face the problem that , in order to compute both the mutual support as well as the covariance matrix of a merged cluster , we have to evaluate multiple integrals .
Thus , in order to be scalable for higher dimensional data sets , we propose a variant of DMBC that uses variances instead of covariances for cluster representation . In particular , we assume the attributes to be indepent of each other and represent a cluster C by its mean vector µC and its ddimensional variance vector σC . The i th value of σC , denoted by σi C , indicates the variance of the Gaussian along attribute i .
As a consequence , the resulting Gaussians form ellipsoid shaped clusters that are constrainted to be axisparallel . We will see later in the experimental evaluation , that this simplification does not cause a significant loss of quality . However , the benefits of this modification are the following . First , we are able to solve the integral of the mutual support analytically . Second , to compute the variance vector of a merged cluster , we need to solve only one integral rather than multiple integrals . Third , the transfer cost for each local cluster are reduced from O(d2 ) for the covariance matrix to O(d ) for the variance vector . {C1 , C2} can be computed as ' +∞
In fact , the mutual support of a pair of clusters C = d
Nµi
1,σi
1( x ) · Nµi
2( x)d x
2,σi
M S(C1 , C2 ) =
−∞ i=1
The following lemma enables us to solve this integral over d dimensions analytically .
Lemma 1 Let C1 = ( µC1 , σC1 ) and C2 = ( µC2 , σC2 ) be local clusters . Then
µ =
1 · σi 2 + µi µi σi 2 + σi 1 and σ =
2 · σi σi 1 σi 2 + σi 1 and apply the logarithm to the equation , it follows that
ϑi =
√ 1 2π · ( σi
1 + σi 2 )
6
· exp− ( µi 2 · ( σi
1 − µi 2)2 1 − σi 2 )
Thus , we obtain
M S(C1 , C2 ) =
' +∞ d d i=1
ϑi · i=1
' +∞
−∞
Nµi
1,σi
−∞
2( x)d x =
2,σi
1( x ) · Nµi d
Nµi,σi( x)d x =
ϑi · 1 i=1
The j th component of the variance vector of the global cluster C which evolved from the merge of m clusters Ci is given as :
( (( ) ff +∞ −∞ (
σj C = fim fim i=1 wCi λ(Ci)Nµj i=1( ff +∞ −∞ wCi λ(Ci)Nµj i ,σj i
( xj ) ) · ( xj − µj ( xj))d xj i ,σj i
C)2d xj
Note that for component σj
C we compute only a 1 dimensional integral over dimension j .
4 Experimental Evaluation
We implemented our versions of DMBC in Java and run several tests on a workstation featuring two 1.8 GHz Opteron processors and 8 GByte main memory .
The test bed consists of one artificial 2 dimensional data set ( denoted as DS1 ) and two real word data sets ( denoted as DS2 and DS3 ) . The latter two are derived from 68,040 images of the corel image feature collection of the UCI KDD archive1 . DS2 contains 9 dimensional color moments of images in HSV color space ( mean , standard deviation and skewness ) . DS3 comprises a description of the corel images based on co occurrence textures with 16 dimensions .
The experimal results of DMBC on the synthetic DS1 ( Figure 3 ) demonstrates that our algorithm is capable to handle cases 1 , 2(a) (c ) described in Section 3.1 : DMBC finds the global cluster “ A ” the objects of which are existent on only one single client , ie client 2 ( Case 1 ) . DMBC finds the global clusters “ B ” , “ D ” , and “ E ” the objects of which are rather well described by local clusters on all sites ( Case 2(a) ) . DMBC finds the global cluster “ C ” the objects of which are distrubuted over all sites such that none of the sites exhibit a local cluster ( Case 2(c) ) . Several objects of clusters “ D ” and “ E ” are prototypes for Case 2(b ) because they are members of local clusters that are built from objects predominantly belonging to another global cluster .
In order to demonstrate the robustness of our clustering algorithm wrt the number of clusters on each client site , we performed distributed clustering with a predetermined number of clusters . We measured the agreement between
1http://kddicsuciedu/
7
Site 1
Site 2
Site 3
A
C
B
D
E
Server Site
Figure 3 . Results of DMBC on DS1 . the distributed clustering and the results of the centralized EM algorithm using the Rand Index [ 6 ] , also known as Rand Statistics .
On the 2 dimensional synthetic data set ( DS1 ) shown in Figure 3 DMBC achieved a Rand index of approximately 99.9 % , indicating a high agreement of our method with the centralized clustering . For data DS2 and DS3 we used the variant of DMBC based on variances ( cf . Section 34 ) As shown in Figure 4 , DMBC achieves high Rand Index values , ie our distributed approach produces a high level of agreement with the results of centralized clustering algorithms on all numbers of clusters . This also indicates that the variant proposed in Section 3.4 using variances instead of covariances does produce accurate results , too .
We evaluated the scalability of the proposed algorithm wrt the number of clusters on each client site . The results are depicted in Figure 4(a ) . As it can be observed , in all settings , the Rand Index is near the optimal value . Thus , the agreement between the centralized clustering and the global distributed clustering is very high .
In addition , we investigated the scalability of the proposed algorithm wrt the number of client sites . Figure 4(b ) presents the agreement using the Rand Index between results calculated by our approach and the centralized clustering algorithm . The number of client sites involved in the distributed clustering was varying from 2 to 10 . The high value of the Rand Index in all experiment evaluations shows that our algorithm is scalable wrt the number of client sites and delivers results that do not differ from that of the global
%
, x e d n
I d n a R
100 90 80 70 60 50 40 30 20 10
Data Set 2 Data Set 3
%
, x e d n
I d n a R
100 90 80 70 60 50 40 30 20 10
2
20
50
100
2
5
10
( a )
# Cluster
( b )
# Client Sites
Data Set 2
Data Set 3
0,6
0,4
0,2
0
%
, e t a R n o i i s s m s n a r T
0
20
40
60
80
100
( c )
# Cluster
%
, e t a R n o i i s s m s n a r T
0,06
0,04
0,02
0
( d )
0
2
4
6
8
10
# Client Sites
Figure 4 . Results of DMBC on DS2 and DS3 . acting algorithm .
We also investigated the transfer cost wrt the number of clusters on each client site and wrt the number of client sites . The results are depicted in Figure 4(c ) and 4(d ) . As transfer cost we measured the ratio of the number of bytes that are transfered using DMBC and of the number of bytes that are transfered using the centralized approach . As it can be seen , the transfer cost is in general very low . Even for a very large number of clusters , DMBC needs less than 1 % of the bytes transfered by the centrailized approach . In addition , we can observe , that the transfer cost increases only linearly wrt the number of local clusters and wrt the number of client sites . Compared to other existing distributed clustering approaches , eg the density based distributed approach in [ 8 ] , where the local transfer cost is at least 15 % of the local data in order to achieve a high agreement , our DMBC reduces the transfer cost dramatically .
Last , we investigated the robustness of DMBC wrt the probability threshold t which affects the cover of the local models . As the results ( not shown due to space limitations ) suggest , DMBC is rather robust wrt a broad range of values for t . In fact , we observed a Rand Index over 98 % when varying the values of t from 0.05 up to 045
To sum up , our experiments demonstrated the robustness , the efficiency , and the applicability of both of our proposed variants for distributed model based clustering .
5 Conclusion
In this paper , we proposed a distributed version for EM clustering called distributed model based clustering ( DMBC ) . Our method applies the EM algorithm at the local sites generating a model containing a set of Gaussian
8 distributions . Each Gaussian is represented by its mean and its covariance matrix or — for higher dimensions the variance vector . We also proposed a merge step of the local Gaussians that can handle covariances as well as variances . Compared to recent approaches for pure distributed clustering , DMBC enables respecting an arbitrary level of privacy and dramatically reduces the transfer costs . Our experimental evaluation demonstrates the robustness , the efficiency , and the applicability of both of our proposed variants for distributed clustering .
For future work we will examine the use of other distri bution functions instead of Gaussian for clustering .
References
[ 1 ] A . P . Dempster , N . M . Laird , and D . B . Rubin . ” Maximum Likelihood from Incomplete Data via the EM Algorithm ” . Journal of the Royal Statistical Society , Series B , 39(1):1– 31 , 1977 .
[ 2 ] I . S . Dhillon and D . S . Modha . ” A Data Clustering AlgoIn Proc . rithm On Distributed Memory Multiprocessors ” . KDD WS on High Performance Data Mining , 1999 .
[ 3 ] M . Ester , H P Kriegel , J . Sander , and X . Xu . ” A DensityBased Algorithm for Discovering Clusters in Large Spatial Databases with Noise ” . In Proc . Int . Conf . on Knowledge Discovery and Data Mining ( KDD ) , pages 291–316 , 1996 . [ 4 ] U . Fayyad , C . Reina , and P . Bradley . ” Initialization of Iterative Refinement Clustering Algorithms ” . In Proc . Int . Conf . on Knowledge Discovery in Databases ( KDD ) , 1998 .
[ 5 ] G . Forman and B . Zhang . ” Distributed Data Clustering can be Efficient and Exact ” . SIGKDD Explorations , 2 , 2000 .
[ 6 ] M . Halkidi , Y . Batistakis , and M . Vazirgiannis . ” On Clustering Validation Techniques ” . Journal of Intelligent Information Systems , 2/3(17):107–145 , 2001 .
[ 7 ] J . Han and M . Kamber . Data Mining : Concepts and Tech niques . Academic Press , 2001 .
[ 8 ] E . Januzaj , H P Kriegel , and M . Pfeifle . ” Scalable DensityIn Proc . Europ . Conf .
Based Distributed Clustering ” . PKDD , 2004 .
[ 9 ] E . Johnson and H . Kargupta . ” Hierarchical Clustering from Distributed , Heterogeneous Data ” . In M . Zaki and C . Ho , editors , Large Scale Parallel KDD Systems , volume 1759 of Lecture Notes in Computer Science ( LNCS ) . Springer Verlag , 1999 .
[ 10 ] B H Park and H . Kargupta . ” Distributed Data Mining : Algorithms , Systems , and Applications ” . In N . Ye , editor , The Handbook of Data Mining . Lawrence Erlbaum Associates Publishers , 2003 .
[ 11 ] M . Sayal and P . Scheuermann . ” A Distributed Algorithm for Web Based Access Patterns ” . In Proc . KDD WS on Distributed and Parallel Knowledge Discovery , 2000 .
[ 12 ] X . Xu , J . J¨ager , and H P Kriegel . ” A Fast Parallel Clustering Algorithm for Large Spatial Databases ” . Data Mining and Knowledge Discovery , an International Journal , 3(3):263–290 , 2003 .
