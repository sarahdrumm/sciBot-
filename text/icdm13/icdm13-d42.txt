4 1 0 2 r a
M 1 1
]
G L . s c [
1 v 4 8 4 2
.
3 0 4 1 : v i X r a
Transfer Learning across Networks for Collective
Classification
Meng Fang† , Jie Yin‡ , Xingquan Zhu§
† Centre for Quantum Computation & Intelligent Systems , FEIT , University of Technology , Sydney , Australia
§Dept . of Computer & Electrical Engineering and Computer Science , Florida Atlantic University , USA
MengFang@studentutseduau ; JieYin@csiroau ; xzhu3@fau.edu
‡Computational Informatics , CSIRO , Australia
Abstract—This paper addresses the problem of transferring useful knowledge from a source network to predict node labels in a newly formed target network . While existing transfer learning research has primarily focused on vector based data , in which the instances are assumed to be independent and identically distributed , how to effectively transfer knowledge across different information networks has not been well studied , mainly because networks may have their distinct node features and link relationships between nodes . In this paper , we propose a new transfer learning algorithm that attempts to transfer common latent structure features across the source and target networks . The proposed algorithm discovers these latent features by constructing label propagation matrices in the source and target networks , and mapping them into a shared latent feature space . The latent features capture common structure patterns shared by two networks , and serve as domain independent features to be transferred between networks . Together with domain dependent node features , we thereafter propose an iterative classification algorithm that leverages label correlations to predict node labels in the target network . Experiments on real world networks demonstrate that our proposed algorithm can successfully achieve knowledge transfer between networks to help improve the accuracy of classifying nodes in the target network .
Keywords—Network ; Transfer learning ;
With recent advance in Web 2.0 technology , information networks , such as social networks , communication networks and bibliographic networks , are becoming ubiquitous in our daily life . Examples include the friendship network in Facebook , co author networks in DBLP and citation networks in PubMed for biomedical articles . Such networks have common properties that they all contain different kinds of entities which interact with one another . Accordingly , an information network is represented as a large graph , in which nodes denote entities or instances ( eg , users or scientific publications ) and links denote relationships between nodes ( eg , friendship or citation relationships ) . To analyze such networks , an important task is to predict the labels of nodes in the networked data , which is commonly achieved by exploiting label correlations through collective classification [ 1 ] , [ 2 ] .
Despite the abundance of networked data , labels are usually very expensive and time consuming to obtain , particularly for newly formed information networks or any new/emerging disciplines in an existing network . In the meanwhile , it is not uncommon that plenty of labeled data exists in some different but related domains . To address this situation , transfer learning has emerged as a new machine learning framework that explores knowledge from auxiliary source domains to facilitate a new learning task in the domains of interest [ 3 ] . The basic idea behind transfer learning is that the involved domains share some common latent factors , which can be uncovered and exploited using different techniques as the bridge for knowledge transfer . Most of the existing works on transfer learning have mainly considered traditional vector based data [ 4]–[6 ] , in which each instance is represented by a multidimensional feature vector and the instances are assumed to be independent and identically distributed ( iid ) However , little research work has been done to address the problem of effective and reliable knowledge transfer across different networks .
Performing transfer learning across different networks poses a number of new challenges , due to the characteristics of networked data . First , the source and target networks can be heterogeneous in nature , because they are formed by different reasons and driven by different applications and user groups . The two networks can be distinct in that their nodes represent different entities , and the associated links indicate different relationships between nodes . For example , a Facebook network indicates friendship relationship between users and a PubMed network represents the citation relationships between scientific publications . A friendship is clearly different from a citation relationship in the sense that the former relies on social interactions between users , whereas the latter is more focused on the content sharing between scientific publications . Even in the case that links may share similar relationships across networks , each network may reveal different features for its own nodes . For two citation networks , CiteSeer and PubMed , the former mainly comprises academic papers in computer science while the latter focuses on biomedical articles . Clearly , the feature spaces of the nodes from the two networks , ie , keywords in paper titles , can be largely different with limited overlap . Thus , the knowledge on node features is not necessarily transferable across different networks , and in the presence of network heterogeneity , discovering common latent factors using the overlap of node features would render sub optimal results , as traditional transfer learning does . Second , in the context of networked data , independent , but are connected by links between each other to form a network . As a result , the labels of connected nodes are correlated in a local neighborhood . This indicates that , closely connected nodes tend to share the same label and nodes on the same substructure are likely to share the same label . Such correlations should be preserved when common latent factors are discovered as the bridge for knowledge transfer across networks . Based on these observations , one key research question is , what information can be transferred from the source network to build an accurate instances are not classifier in the target network ?
In this paper , we propose a novel approach to address the problem of transfer learning across information networks . Our key idea is to discover and transfer some common structure knowledge from the source network to the target network . Specifically , we construct a label propagation matrix that captures the influence of structure information on the labels of connected nodes in a network . Based on this , we design an optimization problem to uncover latent structure features which can capture common structure patterns shared by the source and target networks . These latent features are domainindependent , and can thus serve as generic features transferred from the source network to boost the classification task in the target network . With domain independent , latent structure features and domain dependent node features , we develop an iterative classification algorithm ( ICA ) that makes use of label correlations to predict the labels of nodes in the target network . We have conducted extensive experiments on four real world networks and demonstrated that our proposed transfer learning algorithm can significantly improve the accuracy of classifying nodes in the target network .
I . RELATED WORK
In this section , we briefly review related studies on collective classification over networked data and existing research works on transfer learning .
Collective Classification : Collective classification has recently attracted significant attention for classifying relational data in the data mining area [ 2 ] , [ 7 ] . Networked data is one typical type of relational data , in which instances are represented as nodes and the relationships between nodes are represented as edges . Collective classification exploits dependencies between instances , which makes it one of the most favorable classification methods for networked data .
Approaches to collective classification can be roughly grouped into global methods and iterative methods . Global methods aim to train a classifier that seeks to optimize a global objective function , often based on a Markov random field . These methods are usually computationally expensive , which limits their applicability to large scale , real world networked data . On the other hand , iterative methods employ an iterative process whereby a local classifier predicts labels for each node by using node features and relational features derived from the current label predictions . After that , a collective inference algorithm recomputes the class labels , which will be used in the next iteration .
Iterative classification algorithm ( ICA ) is an iterative method that is widely applied and extended in many studies [ 1 ] , [ 8 ] , [ 9 ] . The basic assumption of ICA is that , given the labels of a node ’s neighbors , the label of the node is independent of the features of its neighbors and non neighbors , and the labels of all non neighbors . In ICA , each node is expressed by combining the node features and relational features constructed by using the labels of all the neighbors of the node . The relational features can be computed by using an aggregation function over the neighbors , such as count , mode , proportion and so on . Based on the node features and relational features , ICA trains a classifier and iteratively updates the predictions of all nodes by using the predictions for node with unknown labels . This process continues until the algorithm converges . In this work , we adopt an ICAlike algorithm to perform collective classification with focuses on transferring structure knowledge from the source network to improve collective classification accuracy on the target network , under the assumption that the number of labeled nodes is very limited .
Transfer Learning : Transfer learning has emerged as a new machine learning paradigm that exploits labeled data in the source domain to build an accurate classifier in the target domain , where the labeled data in the target domain is very limited [ 3 ] . According to the type of information to be transferred , transfer learning approaches can be broadly summarized into three categories . The first category is based on instance transfer [ 4 ] , [ 10 ] , in which certain parts of the instances in the source domain can be reused for learning in the target domain via instance weighting . TrAdaBoost [ 4 ] is one typical example of such methods . TrAdaBoost adjusts the contributions of training instances by giving larger weights to the instances from the source domain that are more similar to the target instances . These methods usually require that different domains share the same feature space and label space , so that the same classifier can be trained on both domains to perform classification . The second category is the parameter transfer approach [ 5 ] , which assumes that the source and target learning tasks share similar parameters or prior distributions of the models , and thus transferring these parameters or priors can help improve the learning task in the target domain . The third family of methods aim to learn a good latent feature representation shared by two domains [ 6 ] , [ 11 ] , where the knowledge used to transfer across domains is encoded into the learned feature representation .
While a large amount of research has been proposed for transfer learning , existing studies have focused on conventional vector based data , in which each instance is represented by a multi dimensional features vector , and all instances are assumed to be independent and identically distributed ( iid ) Recently , some early attempts intend to deal with transfer learning in relational domains , where the instances are noniid and can be represented by multiple relations . Mihalkova et al . [ 12 ] proposed a TAMAR algorithm to transfer relational knowledge with Markov Logic Networks ( MLNs ) across relational domains . In MLNs , entities are represented by predicates and their relationships are represented in firstorder logic . TAMAR tries to map an MLN learned for a source domain to the target domain based on weighted pseudo log likelihood measure , and the mapped structure is further revised as a relational model for inference in the target domain . Another work [ 13 ] proposed an approach to leveraging the edge sign information across signed social networks for edge sign prediction .
To the best of our knowledge , our work is the first research endeavor focusing on transferring knowledge across information networks to predict node labels , where the feature space of the nodes and the node labels of two networks can be largely different . Our proposed method falls into the third category of transfer learning approaches , which attempts to discover common latent structure features shared by the source and target networks . Being domain independent , these latent features are considered as the bridge to transfer knowledge across different networks .
A . Label Propagation Matrix
II . PROBLEM DEFINITION
We focus on an inductive transfer learning setting , where the nodes in the source network are fully labeled , while the target network only has a small number of labeled nodes . We consider one source network Gs and one target network Gt for our classification task . The target network is represented as a t denotes the small set of graph Gt = ( V u t , Et ) , where V l labeled nodes in the network and V u t denotes the set of nodes whose class labels are unknown and need to be predicted . Et denotes the set of edges connecting the nodes . Each node vi t ∈ t , V u t ∈ V l t ∪V l it is also associated with a class label yi t ∈ Yt , where Yt denotes a set of class labels in the target domain . t is described by a feature vector xi t . For a node vi t , V l
In transfer learning setting , we also have a fully labeled source network which is represented as Gs = ( V l s , Es ) , where s denotes the set of labeled nodes and Es denotes the set V l of edges between the labeled nodes . Each node vi s is associated with a feature vector xi s ∈ Ys , where Ys denotes a set of class labels in the source domain . Note that , in our transfer learning problem , we do not require nodes in Gs and Gt to share the same feature space and label space . s and a class label yi s ∈ V l t , V l
Given the source network Gs = ( V l s , Es ) and the target t , Et ) , the goal of our transfer learning network Gt = ( V u task is to ( 1 ) uncover common latent factors shared by the source and target networks , and ( 2 ) leverage these latent factors in the target network to help predict unlabeled nodes vi t ∈ Yt . with one of class labels yi t ∈ V u t
III . THE PROPOSED ALGORITHM
The most important issue of our transfer learning problem is to identify knowledge/patterns which are transferable across different networks . Unlike traditional transfer learning problems , networks can contain nodes with different content features , and the label space of the networks can be totally different . For networked data , nodes are connected by links to form a network , closely connected nodes tend to have the same label , and nodes sharing the same structure patterns are likely to have the same label . Therefore , we propose to transfer structure information from the source network to the target network for predicting node labels in the target network .
Our proposed algorithm consists of two major parts : learning latent structure features and carrying out collective classification . In order to learn latent structure features , we first define a label propagation matrix which reveals the influence of structure information on the labels of the nodes that are connected with each other . Based on this , we formulate and solve an optimization problem for discovering common latent structure features . These latent features serve as domain independent features that capture the common structure patterns shared by networks . Together with domain dependent node features , we further develop a transfer learning algorithm for collective classification .
In the following , we first define the label propagation matrix and propose an objective solution to learn the latent structure features . Then we detail our proposed transfer learning algorithm for collective classification .
Our work is to find “ good ” feature representations shared by different networks to minimize domain divergence and classification errors . Although nodes in different networks can have different feature space and label space , they do share some common structure patterns , based on which nodes can have the same label . To capture such information , we propose to construct a label propagation matrix to model how network structures influence the labels of connected nodes in the network .
Specifically , we borrow the idea from semi supervised learning . Semi supervised learning [ 14 ] , [ 15 ] builds a graph in which nodes represent data points and edges represent similarities between points . We use the geometry of network to represent the similarities between nodes . Those similarities are given by a weighted matrix W , where Wij is non zero if xi and xj are neighbors in the network . Thus we have
Wij = fl 1
0 xi and xj are neighbors , otherwise .
( 1 )
An alternative weight matrix can be given by a Gaussian kernel with width σ :
Wij = expfl− kxi − xj k
2σ2
( 2 ) where Wij is symmetric positive matrix given by a symmetric positive function WX .
Given a graph G , we consider a process of propagating the labels on the graph , for both labeled nodes 1 , 2 , . . . , l , and unlabeled nodes l + 1 , . . . , n . Each node propagates its label to its neighbors , and the propagation process is repeated until reaching to convergence .
Based on this process , we introduce a new matrix , named label propagation matrix , for expressing the propagated correlations between connected nodes in a network , inspired by the idea of semi supervised learning [ 16 ] , [ 17 ] . We assume that a node i receives a contribution from its neighbors Ni , and also retains an additional contribution given by its initial value . The process is given in Algorithm 1 below .
Algorithm 1 Label Propagation Process
1 : Calculate the affinity matrix W by using Eq ( 2 ) if i ≤ j and Wii = 0
2 : Calculate the diagonal degree matrix of D : Dii = Pj Wij
3 : Calculate the matrix L = D−1/2W D−1/2 4 : Give a parameter α ∈ [ 0 , 1 ) 5 : while ˆY is not convergence do 6 : 7 : end while
ˆY ( t+1 ) = αL ˆY ( t ) + ( 1 − α ) ˆY ( 0 )
We now prove the convergence of Algorithm 1 .
Proof : From Algorithm 1 , the iteration equation is
ˆY ( t+1 ) = αL ˆY ( t ) + ( 1 − α ) ˆY ( 0 ) ,
( 3 ) then we have
ˆY ( t+1 ) = ( αL)t ˆY ( t ) + ( 1 − α ) t
Xi=0
( αL ) ˆY ( 0 ) .
( 4 )
The Laplacian matrix L is similar to S = D−1W = D−1/2LD1/2 and they have the same eigenvalues . Since S is a stochastic matrix , its eigenvalues are within the range of [ −1 , 1 ] . Given that 0 < α < 1 , we have ( αL)t = 0 ,
( 5 ) lim t→∞ and lim t→∞ t
Xi=0
( αL)i = ( I − αL)−1 .
So when t → ∞ we have
ˆY ( t ) = ˆY ∞ = ( 1 − α)(I − αL)−1 ˆY 0 .
( 6 )
( 7 )
Now we can see there exists the convergence when t → ∞ and the convergence rate depends on specific properties of the graph , that is , the eigenvalues of the Laplacian matrix .
The main part of Algorithm 1 is the iteration process ( as defined by Eq ( 3) ) . The first term of Eq ( 3 ) indicates that each data point receives the information from its neighbors . The second term of Eq ( 3 ) indicates that the data point is also influenced by its initial label information . Now we focus on how ˆY ( t ) is influenced and becomes stable when nodes receive information from the neighbors and their initial labeling information . The Proof above indicates that we can compute limt→∞ ˆY ( t ) directly without doing iterations using Eq ( 7 ) . Accordingly , we define the label propagation matrix as follows :
ˆY ( t ) = P ˆY ( 0 ) ,
( 8 ) where P = ( I − αL)−1 . Here , P is the label propagation matrix and it translates ˆY ( 0 ) to its convergence status ˆY ∞ . P is a nonnegative matrix .
We give a simple proof to show that P is a nonnegative matrix . We let Q = I − αL , and thus P = Q−1 . Because
0 < α < 1 , we have Qii = 1 and Pj6=i Qij < −1 . We can translate [ Q I ] to [ I Q−1 ] by using elementary row operations . Because only pivot elements are 1 and others are negative ( −1 , 0 ) , we only need to do row addition and the elements which are not pivot elements can be zero . As pivot elements are in ( 0 , 1 ) , we obtain row multiplication by multiplying a positive value for each pivot element . Therefore the left parts of elementary raw operations on [ Q I ] are always positive , ie P is a nonnegative matrix .
B . Learning latent structure features t , V l
Given the source network Gs = ( V l s , Es ) , and the target network Gt = ( V u t , Et ) , we can calculate their label propagation matrices , respectively . Note that Gs is fully labeled and we have Ys . We can compute the propagation matrix Ps . For partially labeled target graph Gt , we can compute the propagation matrix Pt .
Given Ps and Pt , we propose to use nonnegative matrix factorization [ 18 ] to construct latent propagation features through factorizing Ps and Pt under the same space . For Ps , we have min kPs − FsRT s k2 ,
( 9 ) and for Pt we have min kPt − FtRT t k2 .
( 10 )
However , the two factorizations below are very limited because Fs and Ft , Rs and Rt have different scales and dimensions . As a result , it is very difficult to find shared latent feature space directly . Instead , we define Rs with RsAT , and similarly , Rt with RtAT . Therefore , we can rewrite Eq ( 9 ) and Eq ( 10 ) as and min kPs − FsART s k2 , min kPt − FtART t k2 .
( 11 )
( 12 ) where the matrix A is common latent features for both networks and ensures the extracted latent structure features can be represented by the same space .
To discover common latent features shared by networks , we define our optimization objective function as min kPs − FsART st Xj Xj Fs , Rs ∈ RM ×k
Fs(.j ) = 1,Xj Ft(.j ) = 1,Xj
+ s k2 + kPt − FtART t k2 + βkAk2 ,
Rs(.j ) = 1 ,
Rt(.j ) = 1 ,
( 13 )
, Ft , Rt ∈ RN ×k
+ , A ∈ Rk×k + .
In the above objective function , the first two terms are two matrix factorizations where Ps ≈ FsART s and Pt ≈ FtART t . A is latent structure features for both networks . Fs and Ft are two new feature representations in the latent space . RT s and RT t are two additional factors that absorb different scales of P , F and A . kAk2 is a penalty when kAk is too large . β balances the trade off between the complexity of A and two factorization terms . Since all variables are nonnegative , a larger value of A would make other variables Fs , Rs , Ft and Rt smaller . Especially , extremely large values in A would make lots of elements in other variables be close to zeros . As a result , the new feature representation of nodes in the target network would have many missing values . Consequently , it would degrade the node classification accuracy . Therefore , it is necessary to control the values in A by adding a regularization term .
1 ) Solving optimization : Given the optimization function , we write Eq ( 13 ) as
J = kPs − FsART s k2 + kPt − FtART s FsART t FtART s + RsAT F T t + RtAT F T t k2 + βkAk2 , s FsART s ) t FtART t ) s Ps − 2P T t Pt − 2P T
= Tr(P T + Tr(P T + βTr(AT A ) .
We iteratively compute the variables for above function by updating one variable and letting others be fixed .
Update A : Fixing Ps , Fs , Rs , Pt , Ft , Rt and given the + , we introduce the Lagrangian multipliers constraint A ∈ Rk×k λA , λA ∈ Rk×k and minimize the Lagrangian function
L(A , λA ) = J − Tr(λAA ) .
( 14 )
The gradient of L(A , λA ) with respect to A is
∂L ∂A
= −2F T s PsRs + 2FsF T s ART s Rs
−2F T t PtRt + 2FtF T t ART t Rt + 2βA − λA .
( 15 )
Then from the KKT complementarity condition we have
∂L(A , λA )
∂A
= 0 ,
λAA = 0 , and we can rewrite above function as
( −F T s PsRs + FsF T s ART
+FtF T t ART s Rs − F T t PtRt t Rt + βA)A = 0 .
( 16 )
( 17 )
( 18 )
We solve the above coupled equations by using auxiliary function approach [ 18 ] . According to [ 18 ] , the auxiliary function is defined as
Definition 1 : Z(h , h′ ) is an auxiliary function for F ( h ) if the conditions Z(h′ , h ) ≥ F ( h ) and Z(h , h ) = F ( h ) are satisfied .
According to Eq ( 13 ) and ignoring the fixed variables , we can define objective function as
We verify that Z(A , A ) = J(A ) . We rewrite the third term in Eq ( 20 ) by setting A′ = A as follows
( F T s FsART
Xi,j
( ij ) s Rs)(ij)A2 A(ij ) s Rs)(ij)A2
( ij )
( AT F T s FsART
AT A(ij )
= Xi,j
( AT F T s FsART s Rs)(ij )
= Xi,j s FsART
= Tr(AT F T s FsART s Rs ) = Tr(FsF T
( 23 ) where Tr(AT F T s RsAT ) . In the same way we can show that the fourth and fifth terms in Eq ( 20 ) equal the fourth and fifth terms in Eq ( 19 ) respectively when setting A′ = A . Now we can show that Z(A , A ) = J(A ) . Thus the conditions of Definition 1 are satisfied . s Rs ) , s ART
Now we try to find the global minimum of Z(A , A ) . Fixing
A′ , we have
∂Z(A , A′ )
∂A
= −2FsPsRs + 2
( F T s FsA′RT s Rs)(ij)A(ij ) A′ ( βA′)(ij)A(ij )
( ij )
( 19 )
−2FtPtRt + 2
( F T t FtA′RT t Rt)(ij)A(ij ) A′
( ij )
+ 2
. ( 24 )
A′
( ij )
J(A ) = −2Tr(F T s PsRs ) − 2Tr(F T s ART t PtRt ) s RsAT ) +Tr(FsF T t RtAT ) + βTr(AT A ) .
+Tr(FtF T t ART
From Eq ( 19 ) we define the following function
Z(A , A′ ) = −2Tr(F T t PtRtA )
( F T s PsRsA ) − 2Tr(F T +Xi,j t Rt)(ij)A2 t FtA′RT A′ s Rs)(ij)A2 s FsA′RT A′
( βA′)(ij)A2
A′
( ij )
( ij )
( ij )
+Xi,j This function is an auxiliary function of J(Fs ) . We will give proof later . Firstly we give a Lemma from [ 19 ] .
+Xi,j
( 20 )
( ij )
( ij )
.
( ij )
( F T
Lemma 1 : For any matrices C ∈ Rn×n
+ ,H ∈ and C,D are symmetric , the following
+ , D ∈ Rk×k
+ ,H ′ ∈ Rn×k
Rn×k + inequality holds
( CH ′D)ij H 2 ij
H ′ ij
Xi,j
≥ Tr(H T CHD ) ,
( 21 ) and then we show the proof of auxiliary function .
Proof : According to Lemma 1 and the third term in s Rs , H ′ = A′ and s Fs , D = RT
Eq ( 20 ) , we let C = F T H = A . We have
Tr(H T CHD ) = Tr(AT F T s FsART s Rs ) ,
( 22 ) s FsART where Tr(AT F T s Rs ) = Tr(FsF T s RsAT ) . Then we can show that the third term in Z(A , A ) is always bigger than the third one in J(A ) . In the same way we can show that the fourth and fifth terms in Z(A , A ) are always bigger than the fourth and fifth terms in J(A ) respectively . And they have the same first term and second term . Thus Z(A , A′ ) ≥ J(A ) . s ART
′
′
We set ∂Z(A,A ∂A
′
)
= 0 then we have update rule as follows
A(ij ) = A
′
( ij )
( F T s FsA′RT s PsRs + F T s Rs + F T
( F T t PtRt)(ij ) t FtA′RT t Rt + βA′)(ij )
.
( 25 )
Further we have
′
)
= 2
( F T s FsA′RT s Rs)(ij )
∂Z(A , A ∂A∂A ( F T
+2 t FtA′RT t Rt)(ij )
A′
( ij )
A′
+ 2
( ij ) ( βA′)(ij )
A′
( ij )
.
( 26 )
′
We can show that the second partial derivative is positive . Thus , Z(A , A ) is a convex function and we can achieve its global minimum by using Eq ( 25 ) . In other words , we have A(t+1 ) = arg minA Z(A , A(t ) ) by using our update rule . The update rule satisfies Eq ( 18 ) .
By using the update rule we have
J(A(t ) ) = Z(A(t ) , A(t ) ) ≥ Z(A(t+1 ) , A(t ) ) ≥ J(A(t+1) ) ,
( 27 ) where it shows J(A ) is monotonically decreasing . Thus the value of J will monotonically decrease under the update rule . The update rule can minimize J .
So far we assume others are fixed except A . Similarly we can update other variables in the same way while fixing remaining variables and the update rules are as follows :
Fs(ij ) ← Fs(ij )
Rs(ij ) ← Rs(ij )
( PsRsAT )(ij )
( FsF T s PsRsAT )(ij ) ( P T ( RsRT s FsA)(ij ) s P T s FsA)(ij )
,
,
( 28 )
( 29 )
Ft(ij ) ← Ft(ij )
( PtRtAT )(ij )
( FtF T t PtRtAT )(ij )
Rt(ij ) ← Rt(ij )
( P T ( RtRT t FtA)(ij ) t P T t FtA)(ij )
,
.
( 30 )
( 31 )
We can alternatively update Fs , Rs , Ft , Rt and residue
J(Fs , Rs , Ft , Rt , A ) will monotonically decrease s
J(F ( 0 ) ≥ J(F ( 1 ) , R(1 ) , R(1 )
, R(0 ) , R(0 ) s , F ( 0 ) s , F ( 1 ) s , F ( 0 ) s , F ( 0 ) , R(0 ) , R(1 ) s s t t t t t
, R(0 ) , A(0 ) ) , R(0 ) , A(0 ) ) , A(0 ) ) ≥ , A(1 ) ) ≥ t t t
≥ J(F ( 1 ) ≥ J(F ( 1 ) s
( 32 )
Since the lower bound of Eq ( 13 ) is 0 . Our update rules can guarantee convergence .
2 ) Computing k :
In most existing works that involve nonnegative matrix factorization , there is a lack of discussions on how to determine the number of features k . In our work , we devise a heuristic strategy to optimize the value of k , when the objective function Eq ( 13 ) is optimized to find the common latent structure features .
The goal of learning new structure features is to benefit the classification performance on the target data . To estimate the number of features , an appropriate criterion is that we can measure its ability to represent different classes of the target data . In other words , we want the nodes in the same class to have similar features , yet the nodes belonging to different classes to be separated from each other . Given a specific number k of latent features , we can compute a latent feature space A , and accordingly , we have a new feature representation Ft for the nodes in the target network . Given the new feature representation Ft in the target network , we compute a correlation matrix as follows
Ck = FtF T t ,
( 33 ) where element ckij of the matrix Ck represents the similarity between two vectors vi and vj . The smaller the ckij is , the more similar two vectors vi and vj are in the new latent feature space . Therefore , based on the matrix Ck , we can compute a quality score Qs using the new feature representation of the labeled data as
Q =
C
Xc=1
1
Nc Xi,j∈Zc ckij ,
( 34 )
9 : 10 : 11 : where Zc is the set of nodes which belong to class c , and Nc is the number of nodes in Zc . This quality score would have a higher value if the nodes in each category are more similar . Therefore , the number of latent feature can be automatically determined by evaluating the local maximum value of this quality score . In summary , our proposed strategy works as follows : given a maximum number of latent features K , for k = 2 , , Kmax , we compute A by using our algorithm iteratively . We can find the optimal number of latent features such that the corresponding quality score Q is maximized .
IV . TRANSFER LEARNING FOR ICA
After discovering the common latent structure features , our next step is to perform collective classification on the target network . Given the target network Gt = ( V u t , Et ) , we need to train a classifier to predict the labels of the unlabeled nodes V u t . However , since there only exist a small number of t in the target domain , we resort to transferring labeled nodes V l structure features from the source network to facilitate the collective classification task in the target network . t , V l
For our classification problem , we adopt an iterative classification algorithm ( ICA ) that leverages label correlations to predict node labels in the target network . After identifying the common latent feature space A , we have new structure features Ft for the target network . These structure features capture the common structure patterns shared by two networks , and thus serve as domain independent features that are transferred between networks . To capture label correlations in the neighborhood , we also compute relational features by using an aggregation function , such as count , mode , and proportion , information from the neighbors Ni of each node vi t . By combining node features , structure features , and relational features , we train an ICA classifier that iteratively updates the predictions of all the nodes by using the previous predictions for unknown labels in the neighborhood , until the algorithm converges . to aggregate the label
The detailed description of our transfer learning algorithm for collective classification is summarized in Algorithm 2 .
Algorithm 2 Transfer Learning for ICA Input : The source network Gs = ( V l s , Es ) and the target network Gt = ( V u t , V l t , Et ) , a base learning algorithm f
Output : Labels of unlabeled nodes in V u t 1 : Calculate the label propagation matrix Ps for Gs and Pt for Gt using Algorithm 1 .
2 : Calculate the common structure feature space by solving the optimization problem Eq ( 13 ) .
3 : Reconstruct features of the target data by adding new features Ft .
4 : for each node vi 5 : t in Gt do
Compute relational features using only observed nodes in Ni Predict the label for an unlabeled node : yi t ← f ( vi t )
6 : 7 : end for 8 : while All yi t ’s are not stabilized or number of iterations does not equal a threshold do
Generate an ordering O over nodes in Gt for each node vi t ∈ O do
Compute relational features using the current labels of Ni Predict the label for an unlabeled node : yi t ← f ( vi t )
12 : end for 13 : 14 : end while 15 : Assign the last predicted labels to V u t
V . EXPERIMENTS
To evaluate the performance of our proposed algorithm , we perform extensive experiments on four real world networks .
Data Set # of Nodes # of Links # of Classes # in Largest Class # in Smallest Class
CiteSeer Cora WebKB Attack
3312 4732
6 701 249
2708 5429
7 818 180
265 479 5 122 22
645 3172
6 312 4
TABLE I .
SUMMARY OF THE FOUR DATA SETS
A . Data sets
The four real world data sets used in our experiments include : CiteSeer , Cora , WebKB and Terrorist Attacks1 . For the data sets , we ignore the node ’s self links and the direction of links , and thus two nodes are connected if either of them has a directed link to the other . In the four networks , the features of nodes are different in the domains and the label spaces are also different indicating different classification problems . The detailed description of the four data sets is discussed as follows .
CiteSeer : The CiteSeer data set consists of 3312 scientific publications and 4732 citation links . Each node is represented by a 0/1 valued word vector indicating absence/presence of the corresponding words from a dictionary of 3703 words , and is labeled as one of six classes : Databases ( DB ) , Machine Learning ( ML ) , Information Retrieval ( IR ) , Artificial Intelligence ( AI ) , Human Computer Interaction ( HCI ) , and Agents . We consider a binary classification problem which takes DB as the positive class and the rest as the negative class .
Cora : The Cora data set contains 2708 scientific publications classified into one of seven classes : Probabilistic Methods , Neural Networks , Case Based , Rule Learning , Reinforcement Learning , Genetic Algorithms and Theory . The citation network contains 5429 links . We consider a binary classification problem and use Neural Networks as the positive class and all others are treated as the negative class .
WebKB : The WebKB data set contains information about Web pages and their hyperlinks . We use Wisconsin data which contains 265 Web pages and 479 hyperlink relationships . Each Web page is classified into one of five classes : student , course , faculty , project and staff . We consider the majority class student as positive and the rest as negative .
Attack : This data set consists of 645 terrorist attacks each assigned one of six labels , indicating the type of the attack , including Bombing , Weapon Attack , Kidnapping , Arson , NBCR Attach , and Other Attack . Each node represents a terrorist attack and a link is created between two co located attacks . Each attack is described by a 0/1 valued vector of attributes whose entries indicate the absence/presence of a feature . There are a total of 106 distinct features . We also take the majority class Bombing as positive and the rest as negative .
B . Baselines
Our proposed algorithm is referred to as TrICA in the experiments . Since our work is the first to perform transfer learning across networks for predicting node labels , and no existing state of the art transfer learning method is available for
1http://wwwcsumdedu/projects/linqs/projects/lbc/indexhtml comparison , we compare TrICA with other two non transferlearning baseline methods , with the objective to demonstrate that carefully transferring knowledge from other networks can indeed help improve the node classification accuracy .
•
•
ICA : This method uses the content features of the labeled nodes in the target network to train an ICA classifier for predicting unlabeled nodes [ 2 ] . Propagation based ICA ( PICA ) : This method also relies on the target network to perform collective classification . In addition to the nodes’ content features , it also uses a propagation matrix constructed in the target network as structure features to train an ICA classifier .
It is worth noting that we have indeed considered to use TrAdaBoost , which is a popular transfer learning algorithm [ 4 ] , as a baseline . However , this algorithm assumes that the source and target domains share the same feature space and label space . In contrast , in our problem , the features of the nodes in different networks can be largely different . For example , the feature space of the nodes in CiteSeer contains word occurrences in scientific publications in computer science area , which differs radically from the feature space in Attack where node features represent attributes of attacks . Therefore , TrAdaBoost cannot be used as a baseline to compare with the proposed algorithm .
C . Experimental settings
In our experiments , we focus on binary classification problems in the target network , in which the largest class for each data set is considered as the positive class , and the rest belongs to the negative class . In the target network , we randomly select a fixed percentage p of nodes as labeled data , and our objective is to build a classifier to predict labels of unlabeled nodes in the network .
For this purpose , we use logistic regression as a base classifier to perform collective classification in the target network . Specifically , we train an ICA classifier that uses proportion as the aggregation function to compute relational features , which are the proportions of each class in the neighbors of a node to aggregate the label information from the neighbors of each node . Thereafter , the ICA is trained based on a combined set of aggregated features and other features , depending on the algorithm itself . We apply ICA iteratively to the whole target network until it converges . We then evaluate the classification accuracy only on the unlabeled nodes . For evaluation , we repeat each algorithm for three times and report the average results .
D . Classification performance
To provide comprehensive validations for transfer learning tasks , we take turns to consider each single data set as the target network and the other three as the source networks , respectively . We perform the first set of experiments to compare the classification accuracy of different methods with respect to different numbers of labeled nodes in the target network . We vary the percentage of labeled nodes p in the target network ( from 2 % to 60 % ) and run ICA algorithms on the respective data sets . A better classification algorithm is expected to
0.84
0.82
0.80
0.78
0.76
0.74 y c a r u c c A
0.72
0.0
0.80
0.78
0.76
0.74
0.72
0.70 y c a r u c c A
0.68
0.0
0.84
0.82
0.80
0.78
0.76
0.74
0.72
0.70
0.68
0.0
0.60
0.58
0.56
0.54
0.52
0.50
0.48
0.46
0.44
0.0 y c a r u c c A y c a r u c c A
0.84
0.82
0.80
0.78
0.76
0.74 y c a r u c c A
0.72
0.0
ICA PICA TrICA
0.6
0.84
0.82
0.80
0.78
0.76
0.74 y c a r u c c A
0.72
0.0
ICA PICA TrICA
0.6
0.1 0.5 Ratio of labeled data in target network
0.2
0.3
0.4
0.1 0.5 Ratio of labeled data in target network
0.2
0.3
0.4
0.1 0.5 Ratio of labeled data in target network
0.4
0.2
0.3
( a ) T:CiteSeer S:Cora
( b ) T:CiteSeer S:WebKB
( c ) T:CiteSeer S:Attack
0.80
0.78
0.76
0.74
0.72
0.70 y c a r u c c A
0.68
0.0
0.84
0.82
0.80
0.78
0.76
0.74
0.72
0.70
0.68
0.0
0.60
0.58
0.56
0.54
0.52
0.50
0.48
0.46
0.44
0.0 y c a r u c c A y c a r u c c A
ICA PICA TrICA
0.6
ICA PICA TrICA
0.6
CC PCC TrCC
0.6
0.80
0.78
0.76
0.74
0.72
0.70 y c a r u c c A
0.68
0.0
0.84
0.82
0.80
0.78
0.76
0.74
0.72
0.70
0.68
0.0
0.60
0.58
0.56
0.54
0.52
0.50
0.48
0.46
0.44
0.0 y c a r u c c A y c a r u c c A
ICA PICA TrICA
0.6
ICA PICA TrICA
0.6
ICA PICA TrICA
0.6
0.5 0.1 Ratio of labeled data in target network
0.2
0.3
0.4
( e ) T:Cora S:WebKB
0.1 0.5 Ratio of labeled data in target network
0.2
0.3
0.4
( h ) T:WebKB S:Cora
0.1 0.5 Ratio of labeled data in target network
0.2
0.3
0.4
0.5 0.1 Ratio of labeled data in target network
0.2
0.3
0.4
( f ) T:Cora S:Attack
0.1 0.5 Ratio of labeled data in target network
0.2
0.3
0.4
( i ) T:WebKB S:Attack
0.1 0.5 Ratio of labeled data in target network
0.2
0.3
0.4
0.5 0.1 Ratio of labeled data in target network
0.4
0.2
0.3
( d ) T:Cora S:CiteSeer
0.1 0.5 Ratio of labeled data in target network
0.4
0.2
0.3
( g ) T:WebKB S:CiteSeer
0.1 0.5 Ratio of labeled data in target network
0.4
0.2
0.3
ICA PICA TrICA
0.6
ICA PICA TrICA
0.6
ICA PICA TrICA
0.6
ICA PICA TrICA
0.6
( j ) T:Attack S:CiteSeer
( k ) T:Attack S:Cora
( l ) T:Attack S:WebKB
Fig 1 . Accuracy comparison of different algorithms on four data sets with respect to different percentages of the labeled nodes in target networks . T indicates target networks and S indicates source networks . achieve a higher classification accuracy given a same number of labeled data .
The classification results are reported in Figure 1 , which shows that the proposed algorithm TrICA consistently achieves higher accuracy than other baselines over all the transfer learning settings . This confirms that transferring latent structure features across networks can significantly improve the accuracy of classifying nodes in the target network . Noticeably , when CiteSeer is used as the source network and Cora is the target network , or vice versa , TrICA outperforms other baselines to a larger margin , especially when there exists only a small number of labeled nodes in the target network . This is because CiteSeer and Cora are in two similar domains , and they both represent citation relationships between scientific publications . Thus , the two networks share striking similarity in their latent features , which enables transfer learning to be more effective . Meanwhile , PICA is observed to have a better performance than ICA . This indicates that , structure features , discovered via constructing the label propagation matrix , can
) 0 1 g o l ( n o i t c n u f e v i t c e b o f o e u a V l j
4
3
2
1
0
 1
 2
 3
 4 1
2
3
4
5
6
# of Iterations
S:Cora S:WebKB S:Attack
7
8
9
10
) 0 1 g o l ( n o i t c n u f e v i t c e b o f o e u a V l j
4
3
2
1
0
1
2
3
4 1
2
3
4
5
6
# of Iterations
S:CiteSeer S:WebKB S:Attack
7
8
9
10
) 0 1 g o l ( n o i t c n u f e v i t c e b o f o e u a V l j
5
4
3
2
1
0
1
2
3
4 1
2
3
4
S:CiteSeer S:Cora S:Attack
7
8
9
10
) 0 1 g o l ( n o i t c n u f e v i t c e b o f o e u a V l j
5
4
3
2
1
0
1
2
3
4 1
2
3
4
S:CiteSeer S:Cora S:WebKB
7
8
9
10
5
6
# of Iterations
5
6
# of Iterations
( a ) T:CiteSeer
( b ) T:Cora
( c ) T:WebKB
( d ) T:Attack
Fig 2 . Convergence of the objective function for discovering the common latent structure features . y axis denotes the value of objective function in log scale and x axis denotes the number of iterations . T indicates target networks and S indicates source networks .
65
60
55
50
45
40
35
30
25 y t i l a u Q
50
45
40
35
30
25
20 y t i l a u Q
S:Cora S:WebKB S:Attack
20 0
50
100 k
150
200
15 0
50
( a ) T:CiteSeer
S:CiteSeer S:WebKB S:Attack
150
200
100 k
( b ) T:Cora
50
45
40
35 y t i l a u Q
30
25
20
15
10
5 0
40
35
30
25
20
15
10 y t i l a u Q
S:CiteSeer S:Cora S:Attack
150
200
5 0
50
50
100 k
( c ) T:WebKB
S:CiteSeer S:Cora S:WebKB
150
200
100 k
( d ) T:Attack
Fig 3 . Quality scores with respect to different values of k . T indicates target networks and S indicates source networks .
0.82
0.81
0.80
0.79 y c a r u c c A
S:Cora S:WebKB S:Attack
0.78 0
50
100 k
150
200
( a ) T:CiteSeer
0.80
0.79
0.78 y c a r u c c A
0.77
0.76
0.75
0.74
0.73 0
0.83
0.82
0.81
0.80
0.79
0.78
0.77 y c a r u c c A
S:CiteSeer S:WebKB S:Attack
0.59
0.58
0.57
0.56
0.55
0.54 y c a r u c c A
S:CiteSeer S:Cora S:Attack
50
100 k
( b ) T:Cora
150
200
0.76 0
50
100 k
150
200
0.53 0
50
( c ) T:WebKB
S:CiteSeer S:Cora S:WebKB
150
200
100 k
( d ) T:Attack
Fig 4 . Classification accuracy with respect to different values of k . T indicates target networks and S indicates source networks . help improve the collective classification accuracy .
E . Convergence of the objective function
As the core part of our proposed TrICA algorithm , the optimization function Eq ( 13 ) aims to find common latent structure features across the source and target networks . We have its derivatives to solve this optimization problem and prove that its solution can converge in Section III B . Here , we also empirically validate the convergence of the objective function at different settings , where the percentage of the labeled nodes in the target network is set to be 05 Figure 2 reports the values of the objective function as it converges . We can observe that the objective function can quickly converge to its optimal solution . For example , when CiteSeer is used as the target network and Cora is the source network , the value quickly decreases from 104 to 10−2 which asserts that the objective function only takes seven iterations to converge .
F . Determining the optimal value of k
One important parameter of our proposed TrICA algorithm is the number of latent features k , when the objective function
Eq ( 13 ) is optimized to find the common latent structure features . Different k values would lead to different feature representations used for transfer learning , and thus affect the classification accuracy on the target network . Therefore , we fix the percentage of the labeled nodes in the target network to be 0.5 , and carry out experiments to test the ability of our proposed strategy to determine the optimal value of k .
Figure 3 and Figure 4 report the quality score Q and classification accuracies , respectively , by varying the values of k . In the case that Cora is used as the target network and CiteSeer is the source network , we can see that , the maximum value of quality scores is achieved when k is equal to 110 , and the classification accuracy also becomes stable after k reaches the value of 110 . In the case that WebKB is used as the target network and CiteSeer is the source network , the maximum value of quality scores is achieved when k is 110 but classification accuracy becomes stable before k approaches to 110 .
The results in Figures 3 and 4 show that for most cases TrICA algorithm always achieves the highest accuracy when the quality score is at its local maximum value , although in some cases , the classification accuracy becomes saturated
0.84
0.82
0.80
0.78
0.76
0.74
0.72 y c a r u c c A
0.80
0.78
0.76
0.74
0.72
0.70
0.68 y c a r u c c A
S:Cora S:WebKB S:Attack
0.84
0.82
0.80
0.78
0.76
0.74 y c a r u c c A
S:CiteSeer S:WebKB S:Attack
0.60
0.58
0.56
0.54
0.52
0.50
0.48 y c a r u c c A
S:CiteSeer S:Cora S:Attack
S:CiteSeer S:Cora S:WebKB
0.70
0.0
0.5
1.0 beta
1.5
2.0
2.5
0.66
0.0
0.5
1.0 beta
1.5
2.0
2.5
0.72
0.0
0.5
1.0 beta
1.5
2.0
2.5
0.46
0.0
0.5
1.0
1.5
2.0
2.5 beta
( a ) T:CiteSeer
( b ) T:Cora
( c ) T:WebKB
( d ) T:Attack
Fig 5 . Accuracy comparison with different β . T indicates target networks and S indicates source networks . earlier before k reaches its optimal values . Therefore , it still works for our requirement , because our aim is to find an optimal value of k which leads to the best classification accuracy . This concludes that the local maximum value of the quality score designed in our algorithm can help decide the optimal number of latent features k for achieving the best classification performance .
G . Study on the impact of β
Now we study the impact of the parameter β on TrICA algorithm with respect to the classification accuracy . Parameter β is a trade off term that balances the matrix factorization and the complexity of the common feature space A , as defined in Eq ( 13 ) . For this set of experiments , we fix the percentage of the labeled nodes in the target network to be 05 Figure 5 shows the classification accuracy by varying the β values . We can observe that , at the beginning , as the β value increases , TrICA achieves higher accuracies . For all the settings , when β reaches the values between 0.5 and 1.0 , the classification accuracy becomes relatively saturated . A small value of β would relax the constraints on the values of A and allow the elements in A to have larger values . Consequently , this would make many values in the new features approach to become zeros in the target network , and due to the missing feature values , the node classification accuracy will deteriorate .
VI . CONCLUSION
In this paper , we proposed a new algorithm to address the problem of transfer learning across different networks for node classification . We argued that for different networks the nodes’ feature space and the label space can be largely ( or even completely ) different , and the valuable information that can be transferred is structure knowledge of the networks . Therefore , we proposed to construct a label propagation matrix to capture the influence of the structure information to the node labels in a network . Based on this idea , we formulated and solved an optimization problem to discover common latent structure features that are used for knowledge transfer . By doing so , we are able to reconstruct new structure features in the target network , which capture common structure patterns shared between networks . At the last step , an iterative classification algorithm called TrICA is proposed as the learning framework to perform collective transfer learning on the target network . Experiments and comparisons demonstrated that our proposed algorithm outperforms other baselines and the identified common latent structure features can indeed help improve the performance of collective classification for networked data .
REFERENCES
[ 1 ]
J . Neville and D . Jensen , “ Iterative classification in relational data , ” in Proc . of AAAI 2000 Workshop on Learning Statistical Models from Relational Data , 2000 , pp . 13–20 .
[ 2 ] P . Sen , G . Namata , M . Bilgic , L . Getoor , B . Galligher , and T . EliassiRad , “ Collective classification in network data , ” AI magazine , vol . 29 , no . 3 , p . 93 , 2008 .
[ 3 ] S . Pan and Q . Yang , “ A survey on transfer learning , ” IEEE Transactions on Knowledge and Data Engineering , vol . 22 , no . 10 , 2010 .
[ 4 ] W . Dai , Q . Yang , G . Xue , and Y . Yu , “ Boosting for transfer learning , ”
[ 5 ] in Proc . of ICML , 2007 , pp . 193–200 . J . Gao , W . Fan , J . Jiang , and J . Han , “ Knowledge transfer via multiple model local structure mapping , ” in Proc . of KDD , 2008 , pp . 283–291 . [ 6 ] S . Pan , X . Ni , J . Sun , Q . Yang , and Z . Chen , “ Cross domain sentiment classification via spectral feature alignment , ” in Proc . of WWW , 2010 , pp . 751–760 .
[ 7 ] L . K . McDowell , K . M . Gupta , and D . Aha , “ Cautious collective classification , ” Journal of Machine Learning Research , vol . 10 , pp . 2777–2836 , 2009 .
[ 8 ] M . Bilgic , G . M . Namata , and L . Getoor , “ Combining collective classification and link prediction , ” in Proc . of the Seventh IEEE International Conference on Data Mining Workshops , 2007 , pp . 381–386 .
[ 9 ] M . Bilgic , L . Mihalkova , and L . Getoor , “ Active learning for networked data , ” in Proc . of ICML , Haifa , Israel , 2010 .
[ 10 ] W . Dai , G . Xue , Q . Yang , and Y . Yu , “ Transferring naive bayes classifiers for text classification , ” in Proc . of AAAI , 2007 , pp . 540–545 . J . Blitzer , R . McDonald , and F . Perira , “ Domain adaptation with structural correspndence learning , ” in Proc . of EMNLP , 2006 , pp . 120– 128 .
[ 11 ]
[ 13 ]
[ 12 ] L . Mihalkova , T . Huynh , and R . Mooney , “ Mapping and revising markov logic netowrks for transfer learning , ” in Proc . of AAAI , 2007 , pp . 608–614 . J . Ye , H . Cheng , Z . Zhu , and M . Chen , “ Predicting positive and negative links in signed social networks by transfer learning , ” in Proceedings of the 22nd international conference on World Wide Web . International World Wide Web Conferences Steering Committee , 2013 , pp . 1477– 1488 .
[ 14 ] X . Zhu , Z . Ghahramani , and J . Lafferty , “ Semi supervised learning using gaussian fields and harmonic functions , ” in Proc . of ICML workshops , vol . 20 , no . 2 , 2003 , p . 912 .
[ 15 ] O . Chapelle , A . Zien et al . , “ Label propagation and quadratic criterion . ” [ 16 ] X . Zhu and Z . Ghahramani , “ Learning from labeled and unlabeled data with label propagation , ” Technical Report CMU CALD 02 107 , Carnegie Mellon University , Tech . Rep . , 2002 .
[ 17 ] D . Zhou , O . Bousquet , T . N . Lal , J . Weston , and B . Sch¨olkopf , “ Learning with local and global consistency , ” Advances in neural information processing systems , vol . 16 , no . 753760 , p . 284 , 2004 .
[ 18 ] D . Seung and L . Lee , “ Algorithms for non negative matrix factorization , ” Advances in neural information processing systems , vol . 13 , pp . 556–562 , 2001 .
[ 19 ] C . Ding , T . Li , W . Peng , and H . Park , “ Orthogonal nonnegative matrix tri factorizations for clustering , ” in Proc . of SIGKDD . ACM , 2006 , pp . 126–135 .
