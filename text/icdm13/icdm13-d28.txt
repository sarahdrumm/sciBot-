Mining Summaries of Propagations
Lucrezia Macchia
University “ Aldo Moro ”
Bari , Italy
Francesco Bonchi
Yahoo Labs
Barcelona , Spain
Francesco Gullo
Yahoo Labs
Barcelona , Spain
Luca Chiarandini
Universitat Pompeu Fabra
Barcelona , Spain lucreziamacchia@unibait bonchi@yahoo inc.com gullo@yahoo inc.com chiarluc@yahoo inc.com
Abstract—Analyzing the traces left by a meme of information propagating through a social network or by a user browsing a website can help to unveil the structure and dynamics of such complex networks . This may in turn open the door to concrete applications , such as finding influential users for a topic in a social network , or detecting the typical structure of a web browsing session that leads to a product purchase .
In this paper we define the problem of mining summaries of propagations as a constrained pattern mining problem . A propagation is a DAG where an entity ( eg , information exchanged in a social network , or a user browsing a website ) flows following the underlying hierarchical structure of the nodes . A summary is a set of propagations that ( i ) involve a similar population of nodes , and ( ii ) exhibit a coherent hierarchical structure when merged altogether to form a single graph . The first constraint is defined based on the Jaccard coefficient , while the definition of the second one relies on the graph theoretic concept of “ agony ” of a graph . It turns out that both constraints satisfy the downwardclosure property , thus enabling Apriori like algorithms . However , motivated by the fact that computing agony is much more expensive than computing Jaccard , we devise two algorithms that explore the search space differently . The first algorithm is an Apriori like , bottom up method that checks both the constraints level by level . The second algorithm consists of a first phase where the search space is pruned as much as possible by exploiting the Jaccard constraint only , while involving the second constraint only afterwards , in a subsequent phase .
We test our algorithms on four real world datasets . Quantitative results reveal that the choice of the most efficient algorithm depends on the selectivity of the two constraints . Qualitative results show the relevance of the extracted summaries in a number of real world scenarios .
I . INTRODUCTION
We study the novel data mining problem of extracting informative summaries from a database recording activity sequences on a graph . As better explained next , ours is a general framework that can be instantiated in different contexts , ranging from information propagation in social networks to user browsing activity over the Web .
In our problem , we are given a database D of propagations , where each propagation represents the trace left by a specific entity φ that “ flows ” over an underlying graph G = ( V , E ) . More precisely , a trace of an entity φ is a sequence of observations v , φ , t representing the fact that the entity φ is observed at node v at time t . The trace of φ can naturally be represented as a directed acyclic graph ( DAG ) Dφ , whose arcs ( u , v ) express the fact that the same arc exists in the underlying graph G and both u and v activate on φ , with u activating strictly before v . In this case , we can think that φ
Fig 1 : An example of summary extracted from a database of browsing sessions in Wikipedia ( application scenario #2 ) . The numbers inside the nodes identify one optimal ranking . Links that violate the ranking are depicted in red . flowed from u to v . An example of our input is provided later , in Section II ( and Figure 2 ) .
Our goal is to extract summaries of propagation traces from D . A summary consists of two parts : ( i ) a directed graph G(S ) ( subgraph of G ) , and ( ii ) a ranking r of the nodes in G(S ) . The graph G(S ) results from merging the DAGs corresponding to the propagations in the summary , while the ranking r is the expression of the hierarchical structure underlying G(S ) . A major requirement that propagations in the same summary should satisfy is to be structurally similar . More precisely , we want for the propagations within a summary to involve ( more or less ) the same population of nodes .
This is however not enough : we also require for the propagations to exhibit a well defined hierarchical structure when merged together . By hierarchical structure , we mean that one can identify a ranking , such that there exist nodes that usually participate early in the propagations inside the summary , and nodes that instead activate later . The ranking of nodes makes our summaries informative and useful in a wide and diverse range of applications , like the ones discussed next . Application scenario #1 . In a social network like Twitter the underlying graph G represents the social connections : the nodes of the graph are the users of the social network and an arc ( u , v ) exists if u and v are related in some way ( eg , v is a follower of u ) . Here the entity φ is a piece of information ( eg , the URL of an interesting blog , multimedia content , such as photos/videos , etc . ) propagating
Seven Wonders of Ancient World Statue of Zeus at Olympia Lighthouse of Alexandria Colossus of Rhodes Mausoleum at Halicarnassus Hanging Gardens of Babylon Parthenon Great Pyramid of Giza 1 Freemasonry 2 3 4 6 5 8 7 9 in the social networks by means of re tweets . Given a database of propagations D , finding summaries corresponds to finding groups of entities that propagate in the social network in a similar way . Nodes of these groups may represent different communities of users interested in different topics : politicsrelated entities flow trough a different set of nodes than entities related to electronic music . Moreover , a summary also comes with a ranking of nodes which reflects , to some extent , the directionality of the information flow in all the propagations in the summary . This is crucial to distinguish users that are “ early adopters ” ( or “ opinion leaders ” , or “ trend setters ” , or “ influencers ” ) from users that are instead simple followers . Application scenario #2 . In another context , the graph G could be a ( large ) website ( eg , Amazon or Wikipedia ) , whose nodes and arcs correspond to web pages and hyperlinks between pages , respectively . In this case , the entity φ moving in the directed graph corresponds to ( a browsing session of ) a specific user visiting the website , and multiple users clearly leave different traces . The browsing behavior of a user within the website can be represented as a DAG , where the user moves from the homepage down in the hierarchical structure of the pages of the website . Cycles might arise because of user backtracking or because they are allowed by the website structure . However , as what really matters is the sequence of the pages visited , one can safely ignore cycles and simply consider only the first visit to each page .
Finding summaries , ie , groups of users ( sessions ) that navigate the website in a similar fashion is essential for websiteusage analysis and re organization [ 1 ] . In fact , a summary indicates how a specific group of users access the website , which are the pages accessed first , and which access page is a good entry point to discover many other pages . The typical browsing activity of a group of users is thus described by its corresponding summary , and different groups of users can be detected and discerned based on their summaries . Moreover , the typical behavior of successful sessions ( eg , the ones ending in a product purchase ) can easily be detected and studied in order to improve the website organization .
An example of summary extracted from a database of browsing sessions in Wikipedia is provided in Figure 1 ( more details are given in Section IV ) . The numbers inside the nodes correspond to one optimal ranking , while red links denote ranking violations . In the next section we will formalize the concept of violating the ranking , and based on that , we will define the constraint to be satisfied in order to guarantee good hierarchical structures . Paper contributions . Our contributions are as follows :
• We the novel formulate problem of extracting structurally similar summaries from a set of propagations . We translate the requirement about structural similarity into two constraints that the extracted summaries are required to satisfy . One constraint aims to force all the propagations in a summary to involve more or less the same set of nodes , while the second constraint requires for the nodes in the union graph of a summary to have a well defined hierarchy .
• We show that both the constraints satisfy the downwardclosure property , thus allowing the definition of Apriorilike methods .
• We devise two algorithms to solve our problem , which differ from each other by the way how they visit the lattice of all possible sets of propagations . The first algorithm , called BOTTOM UP , relies on a bottom up lattice traversing strategy , which directly exploits the aforementioned closure properties . Motivated by the fact is more time consuming , we develop a second algorithm , called UP&DOWN , which consists of two separate phases : a bottom up phase where the hierarchy constraint is discarded , followed by a top down phase that partially re visits the lattice starting from those summaries that violate the hierarchy constraint . that checking the hierarchy constraint
• We extensively evaluate our algorithms on four real world datasets coming from the application scenarios discussed above , ie , information propagation on social networks and web browsing . Quantitative results show that the UP&DOWN algorithm generally achieves better efficiency , even though BOTTOM UP can be faster in some settings . Qualitative results provide evidence about the significance of the summaries extracted and how they can be exploited for practical purposes .
Roadmap . The rest of the paper is organized as follows . In Section II we define our problem , while in Section III we describe the proposed algorithms to solve it . Section IV shows experiments . In Section V we briefly review related research , and , finally , Section VI concludes the paper . II . PROBLEM DEFINITION
The input to our problem is ( i ) a directed graph G = ( V , A ) representing a network of interconnected objects , ( ii ) a set E of entities , and ( iii ) a set O of observations involving the objects of the network and the entities in E . As mentioned in the Introduction , objects can be , eg , users in a social network or pages of a website , while entities can be , eg , pieces of information ( such as multimedia content ) shared by users or web page visits . Each observation in O is a triple v , φ , t , where v ∈ V , φ ∈ E , and t ∈ N+ , denoting that the entity φ is observed at node v at time t . We assume that the same entity cannot be observed multiple times at the same node ; should this happen , we consider only the first one ( in order of time ) of such observations .
The set O of observations can alternatively be viewed as a database D of propagation traces ( or simply propagations ) , ie , traces left by entities that “ flow ” over G . Formally , a propagation trace of an entity φ corresponds to the subset of all observations in O involving that entity , ie , {v , φ , t ∈ O | φ = φ} . Considering the graph G , the database of propagation traces corresponds to a set of directed acyclic graphs ( DAGs ) D = {Dφ | φ ∈ E} , where , for each φ ∈ E , Dφ = ( Vφ , Aφ ) , Vφ = {v ∈ V | v , φ , t ∈ O} , Aφ = {(u , v ) ∈ A | u , φ , tu ∈ O,v , φ , tv ∈ O , tu < tv} . Note that each Dφ ∈ D is guaranteed to contain no cycles
D v φ Ω φ1 φ1 v2 φ1 v3 φ1 v4 v5 φ1 Ω φ2 φ2 v2 φ2 v1 φ2 v5 φ2 v7 v6 φ2 v3 φ2 Ω φ3 φ3 v1 φ3 v2 v6 φ3 φ3 v7 v4 φ3 t 0 2 4 5 7 0 1 3 6 7 8 9 0 1 3 5 7 8
G
Dφ1
Dφ2
Dφ3
Fig 2 : An example of the input of our problem : a graph G , and a database of propagation traces D defined over a set of entities E = {φ1 , φ2 , φ3} . The graph is here represented undirected : each edge corresponds to the two directed arcs . Each propagation is started at time 0 by a dummy node Ω /∈ V . Given the graph G , the propagation database D is equivalent to the set of dags {Dφ1 , Dφ2 , Dφ3} . due to time irreversibility . Without any loss of generality , we hereinafter refer to D as a set of DAGs . Moreover , we assume that each propagation is started at time 0 by a dummy node Ω /∈ V , representing a source of information external to the network that is implicitly connected to all nodes in V . Thus , each DAG in D is assumed to contain such a dummy node Ω connected to all its “ real ” nodes . An example of our input is provided in Figure 2 . A summary S ⊆ D is a set of DAGs . Given a summary S , we denote by G(S ) the union graph of all the DAGs in S . The union of two graphs G1 = ( V1 , A1 ) and G2 = ( V2 , A2 ) is defined as G1 ∪ G2 = ( V1 ∪ V2 , A1 ∪ A2 ) . An example of graph resulting by the union of two DAGs is given in Figure 3 . Note that , even though S is a set of DAGs , G(S ) is not necessarily a DAG itself , as the union of multiple DAGs can clearly correspond to a cyclic graph .
As informally anticipated in the previous section , our goal is to extract summaries that ( i ) are homogeneous in terms of the population of nodes involved , and ( ii ) exhibit a good hierarchical structure , ie , they are close as much as possible to a DAG structure when merged together to form a single graph . We next formalize these two concepts in two constraints that we require for our summaries to satisfy . Homogenous population of nodes . To force our summaries to involve an homogeneous population of nodes , a natural choice is to quantify the amount of nodes common to all DAGs in a summary and require that it is no less than a certain threshold . In this work , we measure the fraction of nodes shared by a set of DAGs by means of the popular Jaccard similarity coefficient , which is one of the most used measures of similarity among sets of objects . Moreover , it has
Fig 3 : The union graph Dφ1 ∪ Dφ2 of the DAGs Dφ1 and Dφ2 depicted in Figure 2 . the desirable property of having a fixed range codomain , [ 0 , 1 ] , which makes the threshold setting task easier . Formally , given a summary S , we define
|(cid:84 ) |
Dφ∈S Vφ| Dφ∈S Vφ| . j(S ) =
Hierarchical structure . Here we borrow the concept of “ agony ” introduced by Gupte et al . [ 2 ] to define a measure of the hierarchy existing in a directed graph . Given a directed graph G = ( V , A ) , consider a ranking function r : V → N for the nodes in G , such that r(u ) < r(v ) expresses the fact that u is “ higher ” in the hierarchy than v , ie , the smaller r(u ) is , the more u is an “ early adopter ” . If r(u ) < r(v ) , then the arc u → v is expected and does not cause any “ agony ” . Instead , if r(u ) ≥ r(v ) the arc u → v would cause agony because it would mean that u has a follower v ( in the social graph terminology ) that is higher ranked than u itself . Therefore , given a graph G and a ranking r , the agony of each arc ( u , v ) is defined as max{r(u)− r(v ) + 1 , 0} , and the agony a(G , r ) of the whole graph given the ranking r is just the sum over all arcs : a(G , r ) = max{r(u ) − r(v ) + 1 , 0} .
( u,v)∈A
In most cases ( as in our problem ) , the ranking r is not explicitly provided . The objective therefore becomes finding a ranking ( they might be multiple ) that minimizes the total agony of the graph . This way , one can compute the agony of any graph G as a(G ) = min r a(G , r ) .
As a DAG implicitly induces a partial order over its nodes , it has always zero agony : the nodes of a DAG form a perfect hierarchy . For instance , in the DAGs Dφ1 , Dφ2 , Dφ3 in Figure 2 , it is sufficient to take the temporal ordering as ranking , ie , r(u ) = tu where u , φi , tu ∈ Dφi , in order to obtain agony equal to zero .
However , as already mentioned above , merging several DAGs to form a summary S leads to a union graph G(S ) that is not necessarily a DAG , therefore agony can appear . Consider for instance the union graph Dφ1 ∪ Dφ2 reported in Figure 3 . It is easy to see that the graph Dφ1 ∪ Dφ2 is not a DAG as it contains the cycle v3 → v4 → v5 → v7 → v6 → v3 . Due v2 v1 v6 v5 v7 v3 v2 v4 v6 v7 v3 v4 v5 v2 v2 v1 v3 v4 v5 v6 v7 Ω Ω Ω v1 v2 v1 v6 v5 v7 v3 v3 v4 v5 v2 Ω Ω to this cycle , it is impossible to find a ranking r that provides zero agony . In fact , any directed cycle containing k arcs ( and not sharing arcs with any other cycle ) always incurs agony equal to k [ 2 ] . One ranking r providing the minimum agony for Dφ1 ∪ Dφ2 is : ( Ω : 0)(v2 : 1)(v1 : 2)(v4 : 2)(v5 : 3)(v7 : 4)(v6 : 5)(v3 : 6 ) . This ranking yields no agony on all the arcs , except on the arc v3 → v4 that incurs agony 6 2+1 = 5 , which is indeed the length of that directed cycle .
Although the number of possible rankings of a directed graph is exponential , Gupte et al . provide a polynomial time algorithm for finding a ranking of minimum agony . They provide a linear programming formulation and show that ( i ) the dual problem has an optimal integral solution , and ( ii ) the optimal value obtained by maximizing the dual problem coincides with the minimum value of the primal . This finding allows to define an algorithm that decomposes the input graph G into a DAG D and a graph H that corresponds to the maximum ( in terms of number of arcs ) Eulerian subgraph of G ( an Eulerian graph is a graph in which the indegree of each node is equal to its outdegree ) . Let m be the number of arcs and n the number of nodes of G , then the algorithm to compute such a decomposition takes O(m2n ) time : it requires at each iteration to find a negative weight cycle , which can be done by the Bellman Ford algorithm in O(mn ) , while , in the worst case , the number of iterations is m . Mining maximal summaries under constraints . We have now all the ingredients to define the problem we study in this paper . Informally , given a database of DAGs D , we want to extract sets S ⊆ D of DAGs that have high Jaccard of nodes ( no less than a threshold β ) , and small agony of the graph G(S ) obtained by merging the DAGs in S ( no more than a threshold α ) . Moreover , we want S to be maximal and have non trivial size ( ie , |S| ≥ 2 ) . The formal statement of the problem we tackle in this work is reported next . Problem 1 ( Mining summaries of propagations ) : Given a set of dags D , and two thresholds , α ∈ N and β ∈ [ 0 , 1 ] , we want to extract S = {S ⊆ D | |S| ≥ 2 , a(G(S ) ) ≤ α , j(S ) ≥ β , ( cid:64)T ∈ S : S ⊂ T} . For each summary S ∈ S we output the pair G(S ) , r∗ where r∗ is one ranking providing minimal agony for G(S ) , that is r∗ = arg minr a(G(S ) , r ) .
III . ALGORITHMS
The search space of Problem 1 corresponds to the whole D of all subsets of D . The two constraints that are part lattice 2 of our problem definition hold the downward closure property , which enables effective pruning of such a large search space , as explained next .
The Jaccard value is monotonically non increasing as the number of sets to be compared increases . More precisely , given any two sets ( of sets ) S and T , with T ⊃ S , it holds that j(S ) ≥ j(T ) . For our purposes , this result can easily be translated into the following pruning rule to be exploited during any ( bottom up ) traversal of the lattice 2
D .
Fact 1 : Given a set of DAGs D , a subset S ⊆ D , and a threshold β ∈ [ 0 , 1 ] , if j(S ) < β , then j(T ) < β for all T ∈ 2 D
: T ⊃ S .
The second property we show is that the agony is monotonically non decreasing with the increasing of the size of the graph . We formally state such a result in the following theorem .
Theorem 1 ( Agony is monotone ) : Given a directed graph G = ( V , A ) and an arc ( u , v ) /∈ A , let Guv = ( V ∪{u , v} , A∪ {(u , v)} ) denote the graph deriving from adding the arc ( u , v ) to G . It holds that a(G ) ≤ a(Guv ) .
Proof : As shown in [ 2 ] , the problem of minimizing the agony of a graph G can be formulated as a linear program whose dual problem corresponds to finding an Eulerian subgraph of G having the maximum number of arcs . Let E(G ) denote the maximum Eulerian subgraph of G and let |E(G)| denote the number of arcs in E(G ) . Another result reported in [ 2 ] is that |E(G)| = a(G ) . Now , it is easy to see that , although not necessarily optimal , the subgraph E(G ) represents at least an admissible solution ( thus a lower bound ) of the maximumEulerian subgraph problem when the graph in input is Guv . Thus , combining all such results : a(G ) = |E(G)| ≤ |E(Guv)| = a(Guv ) , which proves the theorem .
An immediate corollary of the above theorem we exploit in our context is the following : if for any subset of DAGs S ⊆ D it holds that the agony a(G(S ) ) of the corresponding union graph exceeds a certain threshold α , then the agony a(G(T ) ) of ( the union graph of ) every superset T ⊃ S is guaranteed to be greater than α as well . An opposite result clearly holds for the subsets of a summary S whose agony is no less than α . Corollary 1 : Given a set of DAGs D , a subset S ⊆ D , and a threshold α ∈ N , it holds that :
1 ) if a(G(S ) ) > α , then a(G(T ) ) > α for all T ∈ 2 D 2 ) if a(G(S ) ) ≤ α , then a(G(T ) ) ≤ α for all T ∈ 2 D
:
:
T ⊃ S ; T ⊂ S .
Based on these properties we devise two algorithms , namely D BOTTOM UP and UP&DOWN , which explore the lattice 2 of all subsets of D in two different ways : the BOTTOM UP algorithm performs a bottom up , breadth first visit , while the UP&DOWN algorithm consists of two phases : a first phase roughly similar to the BOTTOM UP algorithm where only the Jaccard constraint is considered , followed by a top down phase where the lattice is partially re visited to check the agony constraint .
In the remainder of the section , we provide the details of the two algorithms , while also discussing the advantages and disadvantages of both , especially in terms of time/space requirements . a(G(S ) ) ≤ α , and j(S ) ≥ β , for all S ∈ S
Algorithm 1 BOTTOM UP Input : set of DAGs D , thresholds α ∈ N , β ∈ [ 0 , 1 ] Output : set S of all maximal subsets of D such that |S| ≥ 2 , 1 : S ← ∅ , C ← {{D} | D ∈ D} 2 : while C = ∅ do 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : end if 11 : end for 12 : 13 : end while
C ← generateCandidates(C ) C ← ∅ for all C ∈ C do if j(C ) ≥ β then
C ← C ∪ {C} S ← S \ {S ∈ S | S ⊂ C} ∪ {C} if a(G(C ) ) ≤ α then end if
A . The BOTTOM UP algorithm
We describe here the first algorithm we devise to solve Problem 1 . The pseudocode of the proposed algorithm , called BOTTOM UP , is summarized in Algorithm 1 .
BOTTOM UP resembles the classic Apriori algorithm for frequent itemset mining [ 3 ] . Given a set of DAGs D and two thresholds α , β , the proposed algorithm visits the lattice of all possible subsets of D in a bottom up fashion ( this motivates the name of the algorithm ) . Particularly , the algorithm performs a breadth first visit , starting from the subsets of D of size 2 ( level 2 ) , and increasing the level one by one until no summaries satisfying the requirements can be extracted . This way , the pruning rules stated in Fact 1 and ( the first statement of ) Corollary 1 can easily be exploited at each level : whenever a candidate C ( ie , a subset of D ) does not satisfy the constraints about either the Jaccard threshold β or the agony threshold α , it is removed from the candidate set so to skip the visit of all its supersets . Particularly , as computing the agony of a candidate C is much more timeconsuming than computing Jaccard ( ie , O(m2n ) vs . O(n ) , where n and m are the number of nodes and arcs in the union graph G(C ) , respectively ) , the first constraint checked by the algorithm is the one concerning Jaccard ( Line 6 ) . Only if this constraint is not violated , then the algorithm proceeds with computing the agony and checking whether its value is within the corresponding threshold ( Line 7 ) .
The generateCandidates procedure invoked in Line 3 aims to derive the set of candidates to be processed in the next level i + 1 from the set of potential candidates C that have passed the tests about the threshold α and β at level i . The procedure essentially performs a classic Apriori like join step . Each pair of potential candidates C1 , C2 in C sharing a common prefix of length i−1 ( ie , for which |C1∩C2| = i−1 ) is merged to form the set C12 = C1 ∪ C2 of size i + 1 ; such a set C12 will be then included in the being formed candidate set C only if all its subsets of size i are present in C . execution , the the generateCandidates procedure following simple result about Jaccard : given any two sets S1 , S2 , the exploits speed up further order
In to the Jaccard value j({S1 , S2} ) between with |S1| ≤ |S2| , the smallersuch sets is upper bounded by the size of sized set divided by the size of the larger sized one , ie , j({S1 , S2} ) ≤ |S1| |S2| . Hence , a further test is performed on a candidate C12 = C1 ∪ C2 that has passed all the tests required by the Apriori like join phase : C12 is actually included in the final set C only if the above constant time computable upper bound on Jaccard is no less than the threshold β . B . The UP&DOWN algorithm
A key advantage of the BOTTOM UP algorithm is the capability of profitably exploiting the pruning rules stated in Fact 1 and Corollary 1 , which allow a smart yet efficient visit of the lattice . Moreover , due to its closeness to the classic frequent itemset mining Apriori algorithm , it is rather simple and easy to implement . Nevertheless , BOTTOM UP still suffers from a couple of major limitations :
1 ) The efficiency bottleneck of the algorithm is the comtakes putation of the agony , which , as said above , O(m2n ) time . Even though the BOTTOM UP algorithm tries to minimize the number of agony computations by first checking the less expensive Jaccard constraint , unneeded agony computations may still arise . Indeed , each maximal summary S satisfies the property that all its subsets do not exceed the agony threshold ( Corollary 1 , Statement 2) ) ; this means that , for each of such maximal summaries S , all subsets of S have necessarily been involved into an agony computation in previous iterations that resulted in no pruning . For a bottom up visit there is no way to avoid that . An idea could be to compute agony incrementally : however this is not even a viable solution , as even adding a single new arc might force the algorithm for agony computation to perform a number of operations comparable to re doing the whole computation from scratch [ 2 ] .
2 ) The space complexity of BOTTOM UP can be high , as the algorithm needs to keep in memory the union graph of all candidates of the current level to allow agony computations . On the other hand , the solution of loading at runtime the union graph of the various candidates would slow down the algorithm too much .
Within this view , we devise here a second algorithm to it UP&DOWN , as it overcome the above issues . We call D in which a performs a two step traversal of the lattice 2 bottom up phase is followed by a top down phase . The outline of UP&DOWN is reported as Algorithm 2 .
The first step of UP&DOWN ( Lines 2 11 ) is a bottom up visit similar to the BOTTOM UP algorithm , but here performed considering only the Jaccard constraint ( thus completely discarding agony ) . The aim of this step is to find the set Sj of all maximal summaries that satisfy Jaccard . Among these summaries , there will clearly be some that do not satisfy agony ; such summaries are collected into the set S¬a ( Line 12 ) . The second step of the algorithm ( Lines 15 24 ) restarts from these summaries that violate the agony constraint and performs a top down visit of the lattice aimed at discovering a(G(S ) ) ≤ α , and j(S ) ≥ β , for all S ∈ S
C ← generateCandidates(C ) C ← ∅ for all C ∈ C do if j(C ) ≥ β then C ← C ∪ {C} Sj ← Sj \ {S ∈ Sj | S ⊂ C} ∪ {C}
Algorithm 2 UP&DOWN Input : set of DAGs D , thresholds α ∈ N , β ∈ [ 0 , 1 ] Output : set S of all maximal subsets of D such that |S| ≥ 2 , 1 : Sj ← ∅ , C ← {{D} | D ∈ D} 2 : while C = ∅ do 3 : 4 : 5 : 6 : 7 : 8 : end if 9 : end for 10 : 11 : end while 12 : S¬a ← {S ∈ Sj | a(G(S ) ) > α} 13 : S ← Sj \ S¬a 14 : M ← maxS∈S¬a |S| 15 : for i = M , M − 1 , . . . , 2 do 16 : 17 : 18 : 19 : 20 : 21 :
S(i)¬a ← {C ∈ S¬a | |C| = i} for all C ∈ S(i)¬a do S ← S ∪ {C} S¬a ← S¬a ∪ {C ∈ Sj | C⊂ C,|C| = i − 1 , ( cid:64)S ∈ S : S ⊃ C} if a(G(C ) ) ≤ α then else end if end for
22 : 23 : 24 : end for the summaries whose agony is instead within the threshold α . The top down visit is performed in a breadth first fashion ( like the bottom up counterpart ) , starting from level i = M , where M denotes the maximum size of a summary in S¬a ( in general S¬a may in fact contain summaries of different size ) . For each level i , the algorithm computes the agony of all candidate summaries in S(i)¬a , which denotes the set of all summaries in S¬a of size i ( Line 18 ) . Each candidate C ∈ S(i)¬a satisfying the agony constraint is added to the solution set S ( Line 19 ) : indeed , according to ( the second statement of ) Corollary 1 , all subsets of C are guaranteed to satisfy agony as well , then no backtracking is further needed from C . If the agony constraint is violated , the candidate C in S(i)¬a is processed so to add to the candidate set to be considered in the next iteration all ( i−1) sized subsets of C that do not have any superset in the current solution set S ( Line 21 ) .
C . Comparing BOTTOM UP and UP&DOWN
The main advantage of the two step lattice traversal of the UP&DOWN algorithm is that , in most cases , it significantly reduces both the total number of agony computations and the space complexity , thus offering a valid solution to the issues of the BOTTOM UP algorithm discussed at the beginning of Section III B . Indeed , the bottom up phase of UP&DOWN completely ignores the agony constraint , whose computation , as said , constitutes a bottleneck in terms of both time and space . The agony constraint is considered only in the subsequent top down phase , where , however , the number of agony computations is expected to be less than the agony the bottom up phase of computations performed by a purely bottom up strategy . For a better explanation , let us consider the following example . Example 1 : Let D ={A , B , C , D , E , F , G , H , I , J , K , L} and assume that the UP&DOWN algorithm produces the set Sj = {ABCD , EF GH , IJKL} ( where ABCD is a shorthand for {A , B , C , D} ) . Assume also that , among the summaries in Sj , IJKL violates the agony constraint , while ABCD and EF GH do not , ie , assume that S¬a = {IJKL} . For the summaries in Sj \ S¬a ( ie , ABCD and EF GH ) , the speed up achieved by the UP&DOWN algorithm with respect to BOTTOM UP is guaranteed and quite evident : for each of such summaries , UP&DOWN computes agony only once , while BOTTOM UP would perform a number of agony computations proportional to the number of subsets of that summary ( ie , 11 agony computations for a 4 sized summary like the ones in our Sj \ S¬a ) . Also the space requirements of UP&DOWN are significantly less , as , for the bottom up phase , UP&DOWN needs to keep in memory only the set of nodes of each candidate summary ( because only the node set is needed to compute Jaccard ) , unlike the BOTTOM UP algorithm that requires in memory the entire graph G(C ) for each candidate summary C ( needed for computing agony ) .
Concerning the summaries in S¬a , the speedup and the memory saving achieved by UP&DOWN depend on how further the top down phase needs to go . For instance , assume that the actual summaries that satisfy agony are {JKL , IKL , IJK} . This way , the top down phase of UP&DOWN would last only one level , thus still guaranteeing a speed up and memory saving with respect to BOTTOM UP . But if , as another example , the actual set of summaries satisfying agony is instead the singleton {IJ} , the UP&DOWN algorithm would need to visit a large portion of the lattice under IJKL before encountering the set IJ , whereas BOTTOM UP would have processed IJ quite soon . instead ,
According to the above reasoning , the main conclusion that may be drawn here is that UP&DOWN guarantees better efficiency and smaller space complexity than BOTTOM UP in most of the cases . Nevertheless , the BOTTOM UP algorithm still remains preferable in cases where the time ( and space ) spent by UP&DOWN in the top down phase is predominant . This may happen , eg , when small agony thresholds α and/or large Jaccard thresholds β are involved . As we will show in Section IV , this conclusion is also confirmed by experimental evidence .
IV . EXPERIMENTAL EVALUATION
We provide here experimental evidence of the performance of our BOTTOM UP and UP&DOWN algorithms . We experiment with four real world datasets , whose main characteristics are summarized in Table I . Three datasets ( ie , Twitter , Last.fm , and Flixster ) come from the domain of information propagation in a social network ( application scenario #1 in the Introduction ) , while the remaining one ( ie , Wikipedia ) concerns the web browsing domain ( application scenario #2 in the Introduction ) .
TABLE I : Characteristics of the datasets used in the experiments : number of observations ( |O| ) ; number of propagations/ DAGs ( |D| ) ; nodes ( |V | ) and arcs ( |A| ) in the input graph G ; min , max , and avg number of nodes in a DAG in D ( nmin , nmax , navg ) ; min , max , and avg number of arcs in a DAG in D ( mmin , mmax , mavg ) . |V | 28 185 1 372 29 357 39 756 navg mmin mmax mavg 347 39 1 561 9
|O| 580 141 1 208 640 6 529 012 50 092
|A| 1 636 451 14 708 425 228 46 610
Twitter Last.fm Flixster Wikipedia
|D| 8 888 51 495 11 659 5 477 nmax 13 547 472 16 129 125
240 153 2 704 85 165 131 nmin 12 6 14 4
66 24 561 9
11 5 13 4
Twitter ( twittercom ) We obtained the dataset by crawling the public timeline of the popular online microblogging service . The nodes of the graph G are the Twitter users , while each arc ( u , v ) expresses the fact that v is a follower of u . The entities in E correspond to URLs , while an observation u , φ , t ∈ O means that the user u ( re )tweets ( for the first time ) the URL φ at time t .
Last.fm ( wwwlastfm ) Last.fm is a music website , where users listen to their favorite tracks and communicate with each other . The dataset was created starting from the HetRec 2011 Workshop dataset available at wwwgrouplensorg/node/462 , and enriching it by crawling . The graph G corresponds to the friendship network of the service . The entities in E are the songs listened by the users . An observation u , φ , t ∈ O means that the first time that the user u listens to the song φ happens at time t .
Flixster ( wwwflixstercom ) Flixster is a social movie site where people can meet each other based on tastes in movies . The graph G corresponds to the social network underlying the site . The entities in E are movies , and an observation u , φ , t is included in O when the user u rates for the first time the movie φ with the rating happening at time t .
Wikipedia ( wwwwikipediacom ) We created this dataset by looking at the browsing sessions of the popular online encyclopedia . Any node of the graph G corresponds to a Wikipedia page , while an arc ( u , v ) is present in G if there exists a browsing session where the page v has been reached from the page u , even making use of intermediate pages external to the website . An entity φ ∈ E corresponds to a browsing session . An observation u , φ , t ∈ O means that the page u is visited during the session φ at time t . For each page , we consider only the first visit among the multiple ones possibly performed within the same session .
In the following we discuss the results achieved by the proposed algorithms from both a quantitative ( Section IV A ) and a qualitative ( Section IV B ) viewpoint . We use Twitter and Last.fm mainly for quantitative evaluation , while we resort to Wikipedia and Flixster for qualitative evaluation .
All algorithms are implemented in JAVA , and all experiments are performed on a single machine with Intel Xeon CPU at 2.20GHz and 48GB RAM .
A . Quantitative evaluation
We report here quantitative results achieved by our BOTTOM UP and UP&DOWN algorithms on Twitter and Lastfm We test our algorithms with Jaccard thresholds
TABLE II : Number of maximal summaries extracted from Twitter ( left ) and Last.fm ( right ) .
α
50
40
30
20
10 60 515 646 537 416 410 411 121 128 128 120 116 116 44 45
51
51
47
45
β 0.7 0.8 0.9
α
7
3
5
10 12 208 12 292 12 306 12 293 5 132 5 126 1 895 1 902
5 136 1 903
5 128 1 900
TABLE III : Max and avg size ( ie , number of DAGs ) of the maximal summaries extracted from Twitter ( top ) and Last.fm ( bottom ) .
β 0.7 0.8 0.9
10 8 ( 3.3 ) 5 ( 2.4 ) 5 ( 2.4 )
20 9 ( 4.4 ) 9 ( 2.7 ) 8 ( 2.7 )
30 11 ( 4.6 ) 10 ( 2.8 ) 8 ( 2.8 )
α
40 12 ( 3.9 ) 12 ( 2.9 ) 10 ( 2.7 )
50 14 ( 4.1 ) 13 ( 2.7 ) 11 ( 2.7 )
60 14 ( 4.1 ) 13 ( 2.7 ) 11 ( 2.7 )
β 0.7 0.8 0.9
3 13 ( 3.3 ) 13 ( 2.9 ) 11 ( 2.5 )
5 13 ( 3.3 ) 13 ( 2.9 ) 11 ( 2.5 )
α
7 13 ( 3.3 ) 13 ( 3.4 ) 11 ( 2.5 )
10 13 ( 3.3 ) 13 ( 2.9 ) 11 ( 2.5 )
β ∈ [ 0.7 , 0.9 ] and agony thresholds α ∈ [ 10 , 60 ] ( Twitter ) or α ∈ [ 3 , 10 ] ( Lastfm ) General characterization . Tables II–IV show general statistics about the summaries extracted . In particular , Table II reports on the number of maximal summaries , while the remaining tables show statistics about the size of the summaries : maximum and average number of DAGs in a summary ( Table III ) and maximum and average number of nodes and arcs in the union graph of a summary ( Table IV ) .
As expected , the size of the summaries increases as the agony threshold α increases and/or the Jaccard threshold β decreases ( Tables III–IV ) , because this corresponds to less selective constraints . As far as the number of summaries ( Table II ) , this is not necessarily true , because it may happen that , for a less restrictive constraint , the number of maximal summaries is less but the summaries include a larger number of DAGs . Efficiency evaluation . The running times of our algorithms are shown in Figures 4 and 5 . Particularly , in Figure 4 we show the results with varying the Jaccard threshold β while keeping fixed the agony threshold α ; in Figure 5 , instead , we keep β fixed and show the times with varying α .
Figure 4 clearly shows that the running times of both BOTTOM UP and UP&DOWN are decreasing as the Jaccard threshold β increases : this is expected as the larger β , the smaller the number of candidates to be processed ( larger β denotes a more restrictive constraint ) . The two algorithms instead differ from each other when looking at the behavior with
TABLE IV : Max/avg number of nodes/arcs of the union graph of the maximal summaries extracted from Twitter ( top ) and Last.fm ( bottom ) . nodes arcs
β = 0.7 β = 0.8 β = 0.9
α = 30
α = 20
α = 10
α = 60 2 252(34 ) 2 252(33 ) 2 252(35 ) 2 252(37 ) 2 252(37 ) 2 252(37 ) 2 252(55 ) 2 252(52 ) 2 252(53 ) 2 252(55 ) 2 252(57 ) 2 252(57 ) 2 185(73 ) 2 185(65 ) 2 185(65 ) 2 185(70 ) 2 185(72 ) 2 185(72 )
α = 40
α = 50
α = 20
α = 30
α = 50
α = 10
α = 60 13 003(129 ) 13 003(137 ) 13 003(141 ) 13 003(142 ) 13 003(142 ) 13 003(142 ) 13 003(221 ) 13 003(220 ) 13 003(224 ) 13 003(233 ) 13 003(237 ) 13 003(237 ) 6 771(214 ) 6 771(212 )
6 771(199 )
6 771(208 )
6 771(212 )
6 771(201 )
α = 40 nodes arcs
β = 0.7 β = 0.8 β = 0.9
α = 3 483 ( 22 ) 376 ( 20 ) 333 ( 17 )
α = 5 493 ( 22 ) 376 ( 20 ) 333 ( 17 )
α = 7 493 ( 22 ) 394 ( 19 ) 333 ( 17 )
α = 10 496 ( 22 ) 407 ( 19 ) 333 ( 17 )
α = 3 2 904 ( 35 ) 1 998 ( 32 ) 1 621 ( 22 )
α = 5 2 960 ( 36 ) 1 998 ( 32 ) 1 621 ( 22 )
α = 7 2 990 ( 37 ) 2 126 ( 31 ) 1 621 ( 22 )
α = 10 3 019 ( 38 ) 2 221 ( 30 ) 1 621 ( 22 )
Fig 5 : Running times ( secs ) of the proposed BOTTOM UP ( BU ) and UP&DOWN ( U&D ) algorithms on Twitter ( left ) and Last.fm ( right ) with varying the agony threshold : α ∈ [ 10 , 60 ] ( Twitter ) and α ∈ [ 3 , 10 ] ( Lastfm )
Fig 4 : Running times ( secs ) of the proposed BOTTOM UP ( BU ) and UP&DOWN ( U&D ) algorithms on Twitter ( left ) and Last.fm ( right ) with varying the Jaccard threshold : β ∈ [ 0.7 , 09 ] varying the agony threshold α ( while keeping β fixed ) . Indeed , as Figure 5 shows , the BOTTOM UP times are increasing as α increases , while the opposite happens for UP&DOWN ( there are some fluctuations , but mainly due to bookkeeping ) . This is again expected , because larger values of α imply more candidates to be processed at each level of the BOTTOM UP algorithm . On the other hand , the larger α is , the more likely is that the summaries found at the end of the first phase of the UP&DOWN algorithm satisfy the agony constraint , thus leading to a more lightweight ( and faster ) top down phase .
As far as the comparison of the two algorithms with each other , the results confirm what discussed in Section III C . Indeed , we can observe here that the UP&DOWN algorithm is more efficient than BOTTOM UP in most settings , with gains up to 65 % ( Twitter ) and 35 % ( Lastfm ) Nevertheless , in some cases BOTTOM UP outperforms UP&DOWN . This happens , for instance , for small agony thresholds α and/or large Jaccard thresholds β ( indeed , the gain achieved by UP&DOWN over BOTTOM UP is overall decreasing as α decreases and/or β increases ) . The reason is that smaller α and/or larger β imply a smaller number of candidates to be processed in the bottom up lattice traversing phase , which is an advantage for BOTTOMUP ( it reduces the number of agony computations ) , but a disadvantage for UP&DOWN , as it increases the time spent in the top down phase .
In conclusion , hence , although in most cases the fastest algorithm is UP&DOWN , the efficiency of the two algorithms depends on the selectivity of the constraints used , thus leading to cases where BOTTOM UP is instead preferable .
B . Qualitative evaluation
We analyze here the summaries extracted by our algorithms from a qualitative viewpoint . Wikipedia . Figure 6 shows examples of summaries from Wikipedia . Summary ( a ) consists of two main parts : the first part is a loop going through Socialism and Proletariat ; the second is a branch that ends up in Liberalism . One could imagine that the first part is due to users who would like to explore
( a )
( b )
( c )
Fig 6 : Three example summaries extracted from Wikipedia . The numbers inside the nodes correspond to one optimal ranking . Links that violate the ranking are represented in red . the topic of socialism , while the second one is browsing to a different one . Summary ( b ) is a chronological sequence of the sultans from the Ottoman Empire . This summary has no agony . The nodes of Summary ( c ) are pharaohs of Ancient Egypt . The ranking in the summary is almost the same as the chronological order . The exceptions are Mentuhotep II , who reigned before Amenemhat I , and Khafra , who reigned between Khufu and Menkaure .
We can observe that each summary is related to a train of thought . The hierarchy of the summaries defines a temporal sequence of concepts that are presented one after another . For example , Summary ( a ) consists in the description of the socialism , passing through Karl Marx , and the connections to other political philosophies , while Summary ( b ) goes through the history of the Ottoman Empire . Applications might exploit this to organize knowledge and present it to people . A summary could also inspire a lecture since it condenses the way how many people move through concepts . Moreover , as we can see in the examples , nodes in a summary are topically similar to each other . Summaries could hence help categorizing the knowledge space . Finally , summaries can be used for online contextual recommendation of sequences of pages . Given the current user session , one can fit a summary and predict not only the next page the user will visit but the whole future browsing path . This recommendation is contextualized since it leverages the browsing history of the user .
Flixster . In Table V we report two examples of summaries extracted from Flixster . For each of such summaries , we show some information ( ie , title , genre , and director ) of the movies corresponding to the DAGs in that summary . We can observe that movies in the same summary exhibit some homogeneity of genre and type of audience . In this context , one might exploit summaries to discover early adopters for a topic ( group of movies ) , by looking at the ranking that is coupled with each summary . Applications may leverage this information to target recommendations of new movies sharing similar topics with the summary , so to guarantee the desired spread over the network .
TABLE V : Two summaries extracted from Flixster : title , genre , and director of the movies corresponding to the DAGs of each of the two summaries .
Title Man of the Year Conversations With Other Women Step Up 2 the Streets Brubaker
Genre comedy/drama/romance comedy/drama/romance drama/music/romance drama
Director B . Levinson H . Canosa J . M . Chu S . Rosenberg
Title Prick Up Your Ears Crooklyn Boy Culture One Eyed Jacks Cop and a Half Operation Pacific
Classification biography/drama comedy/drama drama/comedy western/drama/action adventure family/comedy drama/war/action adventure/comedy
Director S . Frears S . Lee Q . A . Brocka M . Brando H . Winkler G . Waggner
V . RELATED WORK
The problem addressed in this paper represents an original type of structured pattern mining problem , for which not much related prior research exists . We however briefly discuss the work in some neighborhood areas . Graph pattern mining . The problem of graph pattern mining is to extract graph patterns ( eg , trees or subgraphs ) that appear frequently in a graph database , ie , a database composed by a large set of graphs . This research area has been quite active in the last decade and a lot of algorithms have been defined , such as AGM [ 4 ] , FSG [ 5 ] , gSpan [ 6 ] , FFSM [ 7 ] , SPIN [ 8 ] , and Gaston [ 9 ] . Moreover , Yan et al . [ 10 ] propose a general framework to mine different graph patterns with possibly nonmonotonic objective functions . The problem we study in this paper departs from graph pattern mining , as we do not mine frequent sub structures from a set of graphs , rather we look for sets of graphs ( DAGs ) that satisfy certain requirements when merged together . Graph summarization . The problem of graph summarization is to create a coarser grained version of a graph such that the most important features of the graph are retained . The typical approach is based on identifying and aggregating sets of similar nodes so that the error deriving from the aggregation is minimized . Navlakha et al . [ 11 ] exploit the MDL principle
Marxism  Renaissance  Category:Socialism  The  Communist  Manifesto  Proletariat  Liberalism  Anarchism  Bourgeoisie  Communism  1  Socialism  2  3  2  3  4  4  5  5  6  Selim  II  Selim  I  Mustafa  I  Ahmed  I  Mehmed  III  Abdul  Hamid  I  Murad  III  Osman  II  1  Suleiman  the  Magnificent  2  7  2  3  4  5  6  Murad  IV  Mahmud  II  7  7  8  9  Mehmed  IV  Ibrahim  I  Abdul  Hamid  II  Mehmed  V  Murad  V  Al ­‐Nahda  11  11  11  10  10  Amenemhat  I  Mentuhotep  II  Khafra  Sobekneferu  Amenemhat  III  Seqenenre  Tao  Senusret  III  1  Middle    Kingdom    of  Egypt  2  3  4  5  5  5  5  1  Menkaure  1  Djoser  Khufu  2   to summarize a graph with accuracy guarantees . Tian et al . [ 12 ] define a graph summarization method which allows the user to specify the granularity level of the summarization in real time . Our problem is evidently different from graph summarization : the output of graph summarization is a reduced version of a single graph , whereas our summaries are sets of graphs ( DAGs ) structurally similar . Applications . The study of the spread of information and influence through a social network has a long history in the social sciences . The first investigations focused on the adoption of medical [ 13 ] and agricultural innovations [ 14 ] . Later marketing researchers have investigated the “ word ofmouth ” diffusion process for viral marketing applications [ 15 ] , [ 16 ] , which has then attracted most of the attention of the datamining community , fueled by the seminal work by Domingos and Richardson [ 17 ] and Kempe et al . [ 18 ] . The main computational problems in this area are : ( i ) distinguishing genuine social influence from “ homophily ” and other factors of correlation [ 19 ] , [ 20 ] , [ 21 ] ; ( ii ) measuring the strength of social influence over each social link [ 22 ] , [ 23 ] ; ( iii ) discovering a set of influential users [ 17 ] , [ 18 ] , [ 24 ] . Finally , a wide literature exists on the analysis of social influence in specific domains : for instance , studying person to person recommendation for purchasing books and videos [ 25 ] , telecommunication services [ 26 ] , or studying information cascades driven by social influence in Twitter [ 27 ] , [ 28 ] .
Our framework can also find applications in the field of website usage analysis and re organization [ 1 ] . Typical browsing patterns can be exploited for reorganizing a website , creating quicklinks [ 29 ] , and in general , making the navigation in the website more efficient for the users .
VI . CONCLUSIONS
In this paper we studied for the first time the problem of extracting informative summaries from a database of propagations . We defined the summaries of interest using two constraints : the first constraint is defined based on the Jaccard coefficient , while the definition of the second one relies on the graph theoretic concept of “ agony ” of a graph . We showed that both constraints satisfy the downward closure property , thus enabling Apriori like algorithms . We developed two algorithms that visit the search space in different ways and apply to various real world datasets .
Our work can be extended from various perspectives . First , taking inspiration from the wide literature on pattern mining , more efficient algorithms can be devised : for instance by depth first visit of the search space or by taking advantage of the fact that we only look for the maximal patterns . Second , we can devise methods that adaptively decide which constraint to check first as the computation progresses [ 30 ] . Finally , we can study how to avoid setting rigid thresholds and make the constraints soft [ 31 ] . Acknowledgments . This work was supported by MULTISENSOR project , partially funded by the European Commission , under the contract number FP7 610411 .
REFERENCES
[ 1 ] R . Srikant and Y . Yang , “ Mining web logs to improve website organi zation , ” in WWW , 2001 , pp . 430–437 .
[ 2 ] M . Gupte , P . Shankar , J . Li , S . Muthukrishnan , and L . Iftode , “ Finding hierarchy in directed online social networks , ” in WWW , 2011 , pp . 557– 566 .
[ 3 ] R . Agrawal and R . Srikant , “ Fast algorithms for mining association rules in large databases , ” in VLDB , 1994 , pp . 487–499 .
[ 4 ] A . Inokuchi , T . Washio , and H . Motoda , “ An apriori based algorithm for mining frequent substructures from graph data , ” in PKDD , 2000 , pp . 13–23 .
[ 5 ] M . Kuramochi and G . Karypis , “ Frequent subgraph discovery , ” in IEEE
ICDM , 2001 , pp . 313–320 .
[ 6 ] X . Yan and J . Han , “ gSpan : Graph based substructure pattern mining , ” in IEEE ICDM , 2002 , pp . 721–724 .
[ 7 ] J . Huan , W . Wang , and J . Prins , “ Efficient mining of frequent subgraphs in the presence of isomorphism , ” in IEEE ICDM , 2003 , pp . 549–552 . [ 8 ] J . Huan , W . Wang , J . Prins , and J . Yang , “ Spin : mining maximal frequent subgraphs from graph databases , ” in KDD , 2004 , pp . 581–586 .
[ 9 ] S . Nijssen and J . N . Kok , “ A quickstart in frequent structure mining can make a difference , ” in KDD , 2004 , pp . 647–652 .
[ 10 ] X . Yan , H . Cheng , J . Han , and P . S . Yu , “ Mining significant graph patterns by leap search , ” in SIGMOD , 2008 , pp . 433–444 .
[ 11 ] S . Navlakha , R . Rastogi , and N . Shrivastava , “ Graph summarization with bounded error , ” in SIGMOD , 2008 , pp . 419–432 .
[ 12 ] Y . Tian , R . A . Hankins , and J . M . Patel , “ Efficient aggregation for graph summarization , ” in SIGMOD , 2008 , pp . 567–580 .
[ 13 ] J . Coleman , H . Menzel , and E . Katz , Medical Innovations : A Diffusion
Study . Bobbs Merrill , 1966 .
[ 14 ] T . Valente , Network Models of the Diffusion of Innovations . Hampton
Press , 1955 .
[ 15 ] F . Bass , “ A new product growth model for consumer durables , ” Man agement Science , vol . 15 , pp . 215–227 , 1969 .
[ 16 ] J . Goldenberg , B . Libai , and E . Muller , “ Talk of the network : A complex systems look at the underlying process of word of mouth , ” Marketing Letters , vol . 12 , no . 3 , pp . 211–223 , 2001 .
[ 17 ] P . Domingos and M . Richardson , “ Mining the network value of cus tomers , ” in KDD , 2001 , pp . 57–66 .
[ 18 ] D . Kempe , J . M . Kleinberg , and ´E . Tardos , “ Maximizing the spread of influence through a social network , ” in KDD , 2003 .
[ 19 ] A . Anagnostopoulos , R . Kumar , and M . Mahdian , “ Influence and correlation in social networks , ” in KDD , 2008 , pp . 7–15 .
[ 20 ] D . J . Crandall , D . Cosley , D . P . Huttenlocher , J . M . Kleinberg , and S . Suri , “ Feedback effects between similarity and social influence in online communities , ” in KDD , 2008 , pp . 160–168 .
[ 21 ] T . L . Fond and J . Neville , “ Randomization tests for distinguishing social influence and homophily effects , ” in WWW , 2010 , pp . 601–610 .
[ 22 ] A . Goyal , F . Bonchi , and L . V . S . Lakshmanan , “ Learning influence probabilities in social networks , ” in WSDM , 2010 , pp . 241–250 .
[ 23 ] K . Saito , R . Nakano , and M . Kimura , “ Prediction of information diffusion probabilities for independent cascade model , ” in KES , 2008 , pp . 67–75 .
[ 24 ] A . Goyal , F . Bonchi , and L . V . S . Lakshmanan , “ A data based approach to social influence maximization , ” PVLDB , vol . 5 , no . 1 , pp . 73–84 , 2011 .
[ 25 ] J . Leskovec , A . Singh , and J . M . Kleinberg , “ Patterns of influence in a recommendation network , ” in PAKDD , 2006 , pp . 380–389 .
[ 26 ] S . Hill , F . Provost , and C . Volinsky , “ Network based marketing : Identifying likely adopters via consumer networks , ” Statistical Science , vol . 21 , no . 2 , pp . 256–276 , 2006 .
[ 27 ] E . Bakshy , J . M . Hofman , W . A . Mason , and D . J . Watts , “ Everyone ’s an influencer : quantifying influence on twitter , ” in WSDM , 2011 , pp . 65–74 .
[ 28 ] D . M . Romero , B . Meeder , and J . M . Kleinberg , “ Differences in the mechanics of information diffusion across topics : idioms , political hashtags , and complex contagion on Twitter , ” in WWW , 2011 , pp . 695– 704 .
[ 29 ] D . Chakrabarti , R . Kumar , and K . Punera , “ Quicklink selection for navigational query results , ” in WWW , 2009 , pp . 391–400 .
[ 30 ] F . Bonchi , F . Giannotti , A . Mazzanti , and D . Pedreschi , “ Adaptive constraint pushing in frequent pattern mining , ” in PKDD , 2003 .
[ 31 ] S . Bistarelli and F . Bonchi , “ Interestingness is not a dichotomy : Intro ducing softness in constrained pattern mining , ” in PKDD , 2005 .
