Itemsets for Real valued Datasets
HIIT , Department of Information and Computer Science Aalto University , Finland
Nikolaj Tatti nikolajtatti@aaltofi
Abstract—Pattern mining is one of the most well studied subfields in exploratory data analysis . While there is a significant amount of literature on how to discover and rank itemsets efficiently from binary data , there is surprisingly little research done in mining patterns from real valued data . In this paper we propose a family of quality scores for real valued itemsets . We approach the problem by considering casting the dataset into a binary data and computing the support from this data . This naive approach requires us to select thresholds . To remedy this , instead of selecting one set of thresholds , we treat thresholds as random variables and compute the average support . We show that we can compute this support efficiently , and we also introduce two normalisations , namely comparing the support against the independence assumption and , more generally , against the partition assumption . Our experimental evaluation demonstrates that we can discover statistically significant patterns efficiently .
Keywords pattern mining , itemsets , real valued itemsets
I . INTRODUCTION
Pattern mining is one of the most well studied subfields in exploratory data analysis . While there is a significant amount of literature on how to discover and rank itemsets efficiently from binary data , there is surprisingly little research done in mining patterns from real valued data . In this paper we propose a family of quality scores for real valued itemsets . In order to motivate our approach , assume that we are given a dataset D containing real numbers and a miner for mining itemsets from a binary data . The most straightforward way to use the miner to find patterns from D is to transform D into a binary data , and apply the miner . More formally , assume that we have selected a threshold ti for every item i in the dataset . Then we define a binary data B by setting bji = 1 , if dji ≥ ti , and 0 otherwise , where j ranges over all transactions of D .
This approach has two immediate setbacks . Firstly , we have to select the thresholds ti . In addition , such a measure is coarse , any intricate interaction between items is destroyed as data values are categorised into two coarse categories , 0s and 1s . Hence , instead of selecting just one set of thresholds , we will vary ti , and instead of computing support only for one dataset , we will compute an average support . More formally , we will attach a distribution p(Ri = ti ) to each threshold and compute the mean E[fr ( X ; B) ] , where fr ( X ; B ) is the frequency ( support ) of an itemset X in a binarized data B .
This approach has several benefits . First of all , the support is monotonically decreasing , which allows us to discover all frequent itemsets efficiently . On the other hand , we will show that we can compute the support efficiently , even though it involves taking an average over a complex function .
We still need to choose the threshold distribution p(Ri = ti ) . In this work we focus on a specific distribution involved with copulas [ 1 ] : roughly speaking , we will define p(Ri ≤ dji ) = k/(|D| − 1 ) , where k is the rank of the jth transaction after data is sorted wrt the ith column . We will see that this distribution induces a support in which the actual values of individual items do not matter , instead the support is based on the ranks of the values . Interestingly enough , several popular statistical tests , such as the MannWhitney U test or the Wilcoxon signed rank test , are also based on the ranks of values .
A standard technique in pattern mining is to compare the observed support against the expected value under some null hypothesis , where the hypothesis is typically an independence assumption . Here we consider two approaches , in the first approach we do a z normalisation by comparing the support against the independence assumption . In our second approach , we generalise the null hypothesis to a partition model , where we assume that items from different parts of the partition are independent . A particular difficulty with these approaches is that in order to compute them we need to compute the expected mean and the variance . While this is trivial when dealing with simple transactional data , it becomes intricate since the threshold distribution actually depends on the dataset . Nevertheless , we can compute the exact mean and variance for the independence assumption and exact mean and asymptotic variance for the partition assumption . Interestingly enough , the independence test is non parametric , that is the mean and the variance depend only on the number of datapoints , whereas in the partition assumption we need to estimate parameters from the dataset . The rest paper of the paper is organized as follows . We introduce preliminary notation in Section II . We define our general measure in Section III and introduce copula support in Section IV . We present an independence test in Section V and test based on partitions in Section VI . We discuss related work in Section VII and present our experiments in Section VIII . Finally , we conclude our paper with remarks in Section IX .
II . PRELIMINARIES AND NOTATION
In this section we introduce the preliminary notation . A dataset D is a multiset of N transactions d1 , . . . , dN , where dj ∈ RK is a vector of length K . We will often use N = |D| as the number of datapoints and K as the dimension of the dataset . We treat each vector di as a sample from an unknown distribution , p(a1 , . . . , aK ) . We refer to the random variables ai as items , or as features . Let A = {a1 , . . . , aK} be the set of all items . An itemset X is a set of items X ⊆ A . Assume that you are given an itemset X and a binary vector d ∈ {0 , 1}K . We say that d covers X if di = 1 , for every ai ∈ X . We will use standard notation , by writing x1 ··· xM to mean {x1 , . . . , xM} .
Assume now that we are given a collection of binary vectors D = d1 , . . . , dN . We define the support or the frequency of an itemset X as the proportion of transactions in D covering X , fr ( X ; D ) =
|{1 ≤ i ≤ N ; di covers X}|
N
.
An important property of the support is that it is monotonically decreasing , that is , fr ( X ; D ) ≤ fr ( Y ; D ) , if Y ⊆ X . This property allows us to use efficient techniques [ 2 ] to discover all itemsets whose frequency is higher than some given threshold .
III . ITEMSET SUPPORT FOR REAL VALUED DATA
In this section we define our measure for real valued data . In order to do so , let D be a dataset over K items , a1 , . . . , aK , and N transactions . Assume that we are given a threshold ti ∈ R for each item ai . Let us write T = ( t1 , . . . , tK ) . Given a vector x ∈ RK of length K , we define y = xT to be a binary vector with yi = 1 if xi ≥ ti , and 0 otherwise . We now define a binarized data DT to be
DT = {xT | x ∈ D}
.
Essentially , DT is a dataset where each value is binarized either to 0 or to 1 , depending on the threshold . We can now compute a support for a given itemset X by computing fr ( X ; DT ) .
The problem with this approach is that we need to select a threshold set T . Additionally , once we have made this choice , the treatment of values in D is coarse : a value slightly higher than the threshold contributes to the support as much as the values that are significantly higher .
To remedy this , we treat thresholds as random variables . That is , we have K random variables , R1 , . . . , RK . We will assume that each threshold is assigned independently , that is , Ri are independent variables . We will go over some of the natural choices for distributions of Ri later on . If we write p(Ri = ti ) to be the density function of the ith threshold , we can now define support as an average support , where the mean is taken over the possible thresholds , that is , fr ( X ; D , p ) = E[fr ( X ; DT ) ]
=
···
K fr ( X ; DT ) p(Ri = ti)dti
. t1 tK i=1
The important property of this support is that it is monotonically decreasing . This allows us to mine all frequent itemsets using the standard pattern mining search . Proposition 1 : Assume two itemsets X , Y such that X ⊆ Y . Then fr ( X ; D , p ) ≥ fr ( Y ; D , p ) . Proof : For any given threshold set T , we have fr ( X ; DT ) ≥ fr ( Y ; DT ) . It follows immediately , that E[fr ( X ; DT ) ] ≥ E[fr ( Y ; DT ) ] , which proves the proposition . Computing the support from the definition is awkward as it requires taking |X| integrals . Fortunately , we can rewrite the support in a much more accessible form .
Proposition 2 : Assume a dataset D with N transactions and a distribution p over the thresholds . Then the support of itemset X is equal to fr ( X ; D , p ) =
1 N p(Ri ≤ xi )
. x∈D i∈X
Proof : We can rewrite the support as fr ( X ; D , p ) = E[fr ( X ; DT ) ] x∈D
=
1 N p(xT covers X )
.
Transaction y = xT covers X if only if yi ≥ Ri for each i ∈ X . Since Ri are independent , it follows that . p(xT covers X ) = p(Ri ≤ xi )
This completes the proof . i∈X
IV . COPULA SUPPORT
Our measure depends on the threshold distribution . In this section we focus on a specific distribution related to copulas . Assume that we are given a dataset D = d1 , . . . , dN . Let us assume for simplicity that for each item , say aj , the data points dij are unique . Fix an item aj and for notational simplicity let us assume that the datapoints are ordered according to the jth item , dij < d(i+1)j for i = 1 , . . . , N−1 . Let us define the probability of a threshold Ri by requiring that the threshold will hit the interval [ dij , d(i+1)j ] with a probability of 1/(N − 1 ) , where i = 1 , . . . , N − 1 . In other words , the cumulative distribution is equal to p(Rj < dij ) = i − 1 N − 1
.
This gives us straightforward way of computing the support . Given a dataset D of N points , we compute rij = ( c − 1)/(N − 1 ) , where c is the rank of the ith transaction
N
1 N j∈X rij . i=1 according to the jth column . We can now define a copula1 support by cp(X ; D ) = where rnk ( i ; X , D ) = rnk ( i ; X , D ) ,
.
Example 1 : Consider that we are given a dataset with 4 items and 3 transactions {(1.2 , 4.5 , 3.8 , 8.9 ) , ( 4.4 , 4.7 , 1.9 , 8.8 ) , ( 8.2 , 8.5 , 3.0 , 6.5)} . The corresponding ranks {rij} are then
{(0 , 0 , 1 , 1 ) , ( 0.5 , 0.5 , 0 , 0.5 ) , ( 1 , 1 , 0.5 , 0)} . For example , the copula support for {a2a3} is then 1 6
( 0 × 1 + 0.5 × 0 + 1 × 0.5 ) = cp(a2a3 ) =
1 3
As we see in the experiments , using cp(X , D ) as a filtering condition is not enough . Consequently , we also define cp(X ; D , α ) by setting i − 1 − M
N − 1 − 2M p(Ri < di ) = max min
, 1
, 0
, where M = αN , that is , the top αN items will be always above threshold and the bottom αN will be always below threshold .
Copula support has some peculiar features . First of all , the support does not depend on the actual values of D , only on their ranks . This makes this support excellent for cases where computing the difference between the values of D does not make sense . In addition to that cp(ai ; D ) = 1/2 for any item , hence the support is not useful for selecting itemsets of size 1 . Even though , we assume that D has independent samples , the ranks rij are no longer independent . However , if we assume independence between the items , we can compute the mean and the variance as we will see in the next section .
V . COPULA SUPPORT AS A STATISTICAL TEST
A standard technique in pattern mining is to compare the observed support against the independence model . In this section we demonstrate how to do this comparison for copula support . More specifically , we are interested in the quantity zIND(X ; D ) =
N
√ cp(X ; D ) − µ
,
σ where µ and σ are the mean and the variance of the copula support under the null hypothesis . We will now show how to compute the mean and the variance of the copula support . In fact , if we set M = |X| , then we will show that µ = 1/2M and
σ2 =
( 2N − 1)M 6M ( N − 1)M
( N − 2)M ( 3N − 1)M 12M ( N − 1)2M−1
+
− N 4M
.
1Copula stands for a cumulative joint distribution of random variables that have gone through such a transformation [ 1 ] .
We will also show that zIND(X ; D ) approaches the Gaussian distribution N ( 0 , 1 ) as the number of data points goes to infinity .
To simplify the analysis we will make an assumption that the probability of a tie between two values of an item is 0 . This assumption is reasonable if the dataset is generated for example from sensor readings .
We will dedicate the remaining section to proving these results . Note that we cannot use Central Limit Theorem to prove the normality because the ranks of individual rows are not independent . Case in point , cp(x ) for a single item will always be 1/2 , hence the variance will be 0 for this case . In order to prove the result , we will first need to establish some notation . Assume that we have N samples , independent and identically distributed random variables , Y = Y1 , . . . , YN , each sample is a vector of size K . Define
Sij = rnk ( i ; j,Y ) =
1
N − 1
I [ Yij > Ykj ] , where I [ B ] returns 1 if the statement B is true , and 0 otherwise . Note that the term I [ Yij = Yij ] = 0 , however , we keep it in the sum for notational convenience . Similarly , we can now define
N k=1
N i=1 j∈X
U = cp(X;Y ) =
1 N
Sij
.
If we are given a dataset D , then cp(X ; D ) is an estimate of the random variable U . Our goal is to compute µ = E[U ] and σ2 = Var.√
N Ufi .
Note that since we assume that Yij and Ykl are independent for j = l , it follows also that Sij and Skl are also independent for j = l . However , unlike Yij and Ykj , Sij and Skj are not independent .
In order to continue we need the following lemma . Lemma 3 : Fix j and let i , k , and l be distinct integers .
Then p(Yij > Ykj ) = 1/2 , p(Yij > Ylj , Ykj > Ylj ) = 1/3 , p(Yij > Ykj , Ykj > Ylj ) = 1/6
.
Proof : Since the probability of a having a tie between variables is 0 , using the symmetry argument , the probability Yij will be larger than Ykj is 1/2 .
Similarly , if we sort the three variables based on their value , there are 6 possible permutations , each permutation has a probability of 1/6 . There are two permutations that satisfy the second event , namely Yij > Ykj > Ylj and Ykj > Yij > Ylj . This shows that the probability of the second event is equal to 1/3 . Finally , there is only one permutation that satisfies the third event , namely , Yij > Ykj > Ylj , which proves the lemma .
We will first compute the mean of U . Proposition 4 : The average of U is E[U ] = 1/2M .
Proof : According to Lemma 3 , E[Sij ] = 1/2 . Since Sij and Skl are independent for j = l , we can write
E[U ] =
1 N
E[Sij ] =
1 N
1 2
=
1 2M
.
N i=1 j∈X
N i=1 j∈X
This proves the result .
Our next step is to compute the variance of U . Since the variables Sij are not independent , we will have to compute them in two stages . Our first step is to compute the second moment of Sij .
Lemma 5 : The second moment of Sij is equal to
E.S2 ij fi = ij
E.S2 fi = E . ,
2N − 1 6(N − 1 )
.
I [ Yij > Ykj] 2fi k=i p(Yij > Ykj )
( N − 1)2
1
1
=
( N − 1)2 1 k=i ( N − 1)2
+ k=i l=k,i
Proof : Decompose the second moment into two sums , p(Yij > Ykj , Yij > Ylj ) .
According to Lemma 3 , the terms in the first sum are equal to 1/2 while the terms in the second sum are equal to 1/3 . This gives us
,(N − 1)/2 + ( N − 1)(N − 2)/3
E.S2 fi =
1 ij
( N − 1)2 6(N − 1 )
1
=
( 3 + 2(N − 2 ) ) =
2N − 1 6(N − 1 )
.
This completes the proof .
Our next step is to compute the cross moment of Sij . Lemma 6 : The cross moment is equal to ( N − 2)(3N − 1 )
E[SijSkj ] =
12(N − 1)2
.
Proof : Decompose the moment into four sums
E . ,
I [ Yij > Ymj] ,
I [ Ykj > Ynj] fi
E[SijSkj ] 1 n=k
=
=
( N − 1)2 A + B + C + D
( N − 1)2 m=i , where
A =
B =
C =
D = m=i,k m=i,k m=i,k m=i,k p(Yij > Ymj , Ykj > Ymj ) , p(Yij > Ymj , Ykj > Ynj ) , n=m,i,k p(Yij > Ykj , Ykj > Ynj ) , and p(Yij > Ymj , Ykj > Yij )
. independent , hence the probability is equal
The random variables in the term of the sum of B are all to 1/4 . According to Lemma 3 the term in the sum of A is equal to 1/3 and the term in the sum for C and D is equal to 1/6 . This gives us N − 2
( N − 2)(N − 3 )
N − 2
, C = D =
.
6
A =
, B =
3
4 Grouping the terms gives us
4(N − 2 ) + 3(N − 2)(N − 3 ) + 4(N − 2 )
E[SijSkj ] =
=
12(N − 1)2
( N − 2)(3N − 1 )
12(N − 1)2
.
This completes the proof .
σ2 = variance .
( 2N − 1)M 6M ( N − 1)M
We can now use both lemmas in order to compute the
N Ufi is equal to Proposition 7 : The variance Var.√ ( N − 2)M ( 3N − 1)M Proof : We begin by splitting E.( N U )2fi into two 12M ( N − 1)2M−1 fi + N U )2fi = E[SijSkj ]
E.S2 sums and applying Lemma 5 and Lemma 6 ,
, N
E.(
− N 4M
√
√
+
. ij i=1
1 N j∈X ( 2N − 1)M 6M ( N − 1)M
= i,k i=k j∈X
( N − 2)M ( 3N − 1)M 12M ( N − 1)2M−1
.
+
We can now use this to express the variance as
σ2 = E.(
N U )2fi − N E[U ]2
√
( 2N − 1)M 6M ( N − 1)M
=
( N − 2)M ( 3N − 1)M 12M ( N − 1)2M−1
+
− N 4M
.
This proves the result . Finally , we show that zIND(X;Y ) approaches a Gaussian distribution . Note that this result does not depend on the assumption that items are independent . Hence , we will be able to use the same result in the next section . N ( U − E[U ] ) approaches
Proposition 8 : The quantity
√ a Gaussian distribution as N approaches infinity . We postpone the proof of this proposition to Appendix .
VI . PRODUCTIVE ITEMSETS AND COPULA SUPPORT In the previous section we tested the support against the independence assumption . A natural extension of this is to assume a partition of the given itemset such that items are independent only when they belong to different blocks of the partition . In fact , an approach suggested in [ 3 ] mines itemsets from binary data whose support is substantially larger than the expectation given by the partition . In order to mimic this for real valued data , we define cp(X ; D ) − µ zPRT(X , P ; D ) =
,
σ where P is a partition of X and where µ and σ is the mean and the variance under the assumption that items belonging to different blocks in P are independent . Our final goal is to find a partition that produces the lowest score , that is , a partition that explains the support the best , zPRT(X ; D ) = minP zPRT(X , P ; D ) , where P goes over all partitions of at least size 2 . Note that we are only interested in one side test . However , we can easily adjust the formula for a symmetrical two side test . In addition , in [ 3 ] the authors were looking only at partitions of size 2 , whereas we go over all nontrivial partitions .
In this section we show how we can compute the needed mean and the variance in order to normalise the support . Unlike with the independence model , the test is no longer non parametric and we will have to estimate several parameters for each subitemset in the partition . Moreover , we will only provide the variance only when N approaches infinity as the interactions between variables are complex and hard to compute exactly for finite N .
We proceed as follows : We will first show what statistics we need from each subitemset and how to compute them . Then we will show how to use these statistics in order to compute the mean and the variance .
A . Statistics needed to compute the rank
Assume that we are given an itemset X = x1 ··· xM . This itemset will eventually be a block in the partition . Let Y = Y1 , . . . , YN be N data samples . Let us shorten Oijx = N−1 I [ Yix > Yjx ] . Let us define Ti = rnk ( i ; X,Y ) =
N
Oijx ,
1 x∈X j=1 which is essentially a product of normalised ranks of the ith datapoint . Similar to Section V , let U = 1 i=1 Ti , a N random variable corresponding to the copula support cp(X ) . Ultimately , we will need three statistics from X , namely
µ = E[U ] , α = E.T 2 fi , and β = Var.√
N N Ufi . We will discuss how to estimate these statistics in the next subsection . If Ti were distributed independently , then β = α− µ2 . However , Ti are dependent . Fortunately , we know enough about the dependency so that we can compute β .
1
N
In order to compute β we need to introduce several random variables . Let
Tix = rnk ( i , X \ {x} ,Y ) =
In addition , let us define Ckx = N be the rank of the ith transaction for an itemset X \ {x} . i=1 TixOikx . We can express the variance β with α , µ and Ckx . The benefit of this is that we can estimate these parameters , and by doing so estimate β , as we will demonstrate in the next subsection . y∈X,y=x
Oijy j=1
Proposition 9 : The variance β approaches
α − ( M + 1)2µ2 +
2 N
N
E . , k=1 x∈X
2fi
Ckx as N approaches infinity .
We postpone the proof of this proposition to Appendix .
B . Estimating statistics
Unlike with zIND(X ) , the mean and the variance of zIND(X ; P ) depend on the underlying distribution , and we are forced to estimate the statistics , namely α , β , µ described in the previous section . These estimates are given in Algorithm 1 . Estimating µ and α is trivial . However , estimating β is more intricate due to the last term given in Proposition 9 . Assume that we are given a dataset D and itemset X . Fix x ∈ X and assume that D is sorted based on xth column , largest first . Let Z = X \{x} . Note that rnk ( k ; Z , D ) is an estimate for Tkx . Hence , we can estimate Ckx as k−1 i=1 ckx =
1
N − 1 rnk ( i ; Z , D )
= c(k−1)x +
1
N − 1 rnk ( k − 1 ; Z , D )
.
We can use the right hand side to compute ckl for every k efficiently , and then use ckl to estimate β . We can assume that we have precomputed the order wrt each item xl before the actual mining . Hence , the cost of estimating the parameters is O(N|X| ) .
N
N i=1 rnk ( i ; X , D)2 ; i = 1 , . . . , N , x ∈ X ;
Algorithm 1 : ESTIMATE , estimates the statistics needed for zPRT . input : dataset D , itemset X output : estimates µ , α , and β 1 µ ← cp(X ; D ) ; 2 α ← 1 3 cix ← 0 , 4 foreach x ∈ X do sort D according to x , largest first ; 5 foreach k ∈ [ 2 , N ] do 6 ckx ← c(k−1)x + 1 7 8 β ← α − ( |X| + 1)2µ2 + 2 9 return µ , α , β ;
2 ; N N−1 rnk ( k − 1 , X \ {x} , D ) ;
, x∈X ckx k=1
N
We should stress that we use the same dataset to compute the estimates and to compute zPRT(X ; D ) . This means that zPRT(X , P ; D ) will be somewhat skewed and we cannot interpret zPRT(X , P ; D ) as a p value . However , our main goal is not to interpret the obtained values as a statistical test , rather our goal is to rank patterns .
C . Computing z score
Now that we have computed statistics for each itemset occurring in a partition , we can combine them in order to compute the mean and the variance needed for zPRT(X ) . Proposition 10 : Assume that we are given an itemset X and a partition P1 , . . . , PL of X . Let Y = Y1 , . . . , YN be N random data points . Let U = cp(X;Y ) , and let Ui = cp(Pi;Y ) . Let µi = E[Ui ] , αi = E.rnk ( 1 ; Pi,Y)2fi , βi = fi . limN→∞ Var.√ E[U ] =L N Ufi → L Var.√
Under the assumption that Pi are independent , we have
αi + ( L − 1)µ2 + µ2
N Ui i=1 µi and
L
βi − αi
µ2 i i=1 i=1 as N approaches infinity . We postpone the proof of this proposition to Appendix .
VII . RELATED WORK
While pattern mining has been well researched for binary data , the problem of discovering patterns from real valued data is open . The most straightforward approach to mine patterns is to discretize data using threshold , see for example [ 4 ] . Among methods that do not use thresholds , Calders et al . [ 5 ] proposed 3 quality measures for itemsets from numerical attributes . The first two measures were based on the extrema values of the items in an itemset . The most related measure to our work is the third measure , suppτ , which is a generalisation of Kendall ’s τ , essentially the number of pairs in which all items are concordant . Interestingly enough , similar to the copula support , suppτ also depends only the order of values not on the actual values . In this work we were able to define two normalisations zIND(X ) and zPRT(X ) for our approach , while the authors did not introduce any statistical normalisation for suppτ . We conjecture that a similar normalisation can be done also for suppτ .
Jaroszewicz and Korzen [ 6 ] suggested discovering polynomial itemsets , essentially cross moments from real valued data . We can show that for a certain threshold distribution , our support is equal to the support of polynomial itemsets . Steinbach et al . [ 7 ] considered several support functions for itemsets , such as , taking the smallest value in a transaction among the items in the itemset .
Ranking and filtering patterns based on a statistical test has been well studied . Brin et al . compared likelihoodratio against independence assumption [ 8 ] . Webb proposed , among many other criteria , to compare the observed support to an expected support of a partition of size 2 that fits best [ 3 ] . More complex null hypotheses such as Bayesian networks [ 9 ] or Maximum Entropy models [ 10 ] have been also suggested .
Our approach has similarities with mining itemsets from uncertain data [ 11 ] , where instead of binary data , we have real valued values between [ 0 , 1 ] expressing the likelihood
BASIC STATISTICS OF DATASETS AND EXPERIMENTS
Table I
Name Ind Plant Alon Thalia Yeast
Size 10 000 × 100 10 000 × 100 2000 × 62 734 × 69 2993 × 173
Threshold
0.1 0.1 0.26 0.12 0.2
Time 7m37s 7m14s 8m17s 2m10s 19m50s
|patterns| 166 750 171 303 393 683 148 334 529 872 of the entry being equal to 1 . In fact , if we interpret rij values computed in Section IV as probabilistic dataset , then cp(X ) will be the same as the expected support computed from probabilistic dataset . However , in probabilistic setting the entries are assumed to be independent , whereas in our case they have an intricate dependency . Consequently , the variance given by Propositions 7 and 9 do not hold for probabilistic datasets . In addition , we cannot compute frequentness measure suggested by Bernecker et al . [ 12 ] in our case , however we can estimate it by a normal distribution as suggested by Calders et al . [ 13 ] .
Defining and computing a quality score for two realvalued variables , essentially an itemset of length 2 , is a surprisingly open problem . The approach based on Information Theory was suggested in [ 14 ] . An interesting starting point is also a measure of concordance , see Definition 517 in [ 1 ] . These approaches are suitable only for itemsets of size 2 whereas we are interested in measuring the quality of itemset of any size . Finally , Sze´ekely and Rizzo [ 15 ] suggested a measure based on how pair wise distances correlate . This measure is symmetric while our measure was specifically designed to focus on large values .
VIII . EXPERIMENTS
In this secion we present our experiments . Datasets : We used 2 synthetic and 3 real world data sets as our benchmark data . The first dataset Ind consists of 10 000 data points , each of 100 items , generated independently uniformly from the interval [ 0 , 1 ] . The second dataset Plant has the same dimensions as the first dataset . In this dataset we planted 5 subspace clusters each having 4 items : We generated independently 5 × 10 000 boolean variables Bti indicating whether a transaction t belongs to the ith cluster , a transaction can belong to multiple clusters . We set p(Bti = 1 ) = 04 If Bti = 1 , then we set the corresponding items to 05 All other values were set to 0 . Finally , we added noise sampled uniformly from [ 0 , 1 ] . As real world benchmark datasets we used the following 3 gene expression data sets : Alon [ 16 ] , Arabidopsis thaliana or Thalia , and Saccharomyces cerevisiae or Yeast.2 The sizes of the datasets are given in Table I .
Setup : For each dataset we computed frequent itemsets using cp(X ; D , 0.25 ) as a support . We set the threshold such 2Thalia and Yeast are available at http://wwwtikeeethzch/∼sop/bimax/ that we get roughly several hundred thousand itemsets , see Table I . We then ranked itemsets using zIND(X ) and zPRT(X ) . The results are given in Figure 1 .
Support comparison : Let us first compare supports cp(X ) and cp(X ; 0.25 ) , given in the top row of Figure 1 . We see that for a fixed itemset length there is a strong linear correlation between the supports . The histograms reveal why we should consider cp(X ; 0.25 ) as a stopping criterion instead of cp(X ) . A significantly large number of itemsets of length , say M , will have larger support than any itemset of length M + 1 or higher , that is , in order to discover any itemset of length 3 , we will have to discover all itemsets of length 2 . This problem does not occur with cp(X ; D , 025 ) Normalisation comparison : Our next step is to compare ranks , given on the second row of Figure 1 . As expected zPRT(X ) is more conservative than zIND(X ) . For example , in Ind , zIND(X ) is distributed as N ( 0 , 1 ) , as predicted by Proposition 8 , whereas zPRT(X ) is skewed towards negative values . In general , zIND(X ) prefers large itemsets whereas zPRT(X ) prefers small ones . This can be beneficial as seen with Plant dataset . The first 5 itemsets A according to zIND(X ) are the itemsets related to subspace clusters . However , the next itemsets B are the clusters with some additional unrelated items , on the other hand , zPRT(X ) will assign a low score to B . In addition , zPRT(X ) favours sets C and D itemsets of size 2–3 that are subitemsets of A .
Computational complexity : While optimising for speed is not the focus in this work , our implementation3 is able to discover several hundred thousand patterns in minutes . The datasets we consider here are relatively small when compared to the size of the binary datasets used for mining normal patterns . However , the speed of traditional miners is based on the fact that binary datasets are typically very sparse . We do not have the same luxury and computing of each itemset requires a full scan . On the other hand , the cost for computing the support a single itemset depends only on the size of the itemset and the number of datapoints whereas the performance of traditional itemset miners depends heavily on how 1s are distributed in the dataset .
IX . CONCLUDING REMARKS
In this paper we proposed a measure of quality for itemsets mined from real valued dataset . Our approach was to compute the average support from binarized data with random thresholds . Despite the complex definition we can compute the measure efficiently . As a distribution for a threshold we considered a special distribution related to copulas . We normalised the support by comparing the observed support to the expected support according to a null hypothesis . We considered two hypotheses : the first items are independent , while the assumption is that all
3Python implementation available at http://usersicsaaltofi/ntatti/ second assumption is more general—we assume that items are independent wrt to a given partition .
This research opens up several directions for future work . Firstly , we considered one specific threshold distribution . This distribution is a good choice if you do not have any information about the distribution of individual items . However , there are other choices . For example , if we know that data is distributed between [ a , b ] , we can consider a uniform distribution over the interval , see [ 6 ] , or possibly a shorter interval that excludes the extreme values .
The speed up techniques used for mining sparse binary data no longer apply . This raises a question whether we can speed up significantly the mining procedure .
Lastly , the distribution of itemsets is different than of those that are obtained from binary data . Typically , in binary data , the margins of the items are distributed unevenly : there will be a lot of items that are rare and some items that are frequent . This means a lot of itemsets will be pruned in first steps . This is not the case with the copula support , where typically you will pass almost all items of size 2 . This emphasizes the need for ranking itemsets , in our case , we used zIND(X ) and zPRT(X ) . However , as future work it would be interesting to see what type of constraints one can impose on itemsets in order to reduce the output .
ACKNOWLEDGMENT
This work was supported by Academy of Finland grant
118653 ( ALGODAN )
REFERENCES
[ 1 ] R . B . Nelsen , An introduction to Copulas . Springer , 2006 .
[ 2 ] R . Agrawal , H . Mannila , R . Srikant , H . Toivonen , and A . I . Verkamo , “ Fast discovery of association rules , ” in Advances in Knowledge Discovery and Data Mining , 1996 , pp . 307– 328 .
[ 3 ] G .
I . Webb , “ Self sufficient itemsets : An approach to screening potentially interesting associations between items , ” TKDD , vol . 4 , no . 1 , pp . 3:1–3:20 , 2010 .
[ 4 ] R . Srikant and R . Agrawal , “ Mining quantitative association rules in large relational tables , ” in SIGMOD , 1996 , pp . 1–12 .
[ 5 ] T . Calders , B . Goethals , and S . Jaroszewicz , “ Mining rankcorrelated sets of numerical attributes , ” in KDD , 2006 , pp . 96–105 .
[ 6 ] S . Jaroszewicz and M . Korzen , “ Approximating representa tions for large numerical databases , ” in SDM , 2007 .
[ 7 ] M . Steinbach , P N Tan , H . Xiong , and V . Kumar , “ General izing the notion of support , ” in KDD , 2004 , pp . 689–694 .
[ 8 ] S . Brin , R . Motwani , and C . Silverstein , “ Beyond market baskets : Generalizing association rules to correlations , ” in SIGMOD , 1997 , pp . 265–276 .
Figure 1 . Scatter plots and histograms of supports and ranks . Each plot contains a scatter plot of two variables and the corresponding marginal histograms . The top row contains cp(X ; D , 0.25 ) plotted as a function of cp(X ; D ) . The bottom row contains contains zPRT(X ; D ) as a function of zIND(X ; D ) . Each column corresponds to a single dataset . Pattern sizes are encoded with different colours . Note that the axis’ ranges vary .
[ 9 ] S . Jaroszewicz and D . A . Simovici , “ Interestingness of frequent itemsets using bayesian networks as background knowledge , ” in KDD , 2004 , pp . 178–186 .
[ 10 ] N . Tatti , “ Maximum entropy based significance of itemsets , ”
KAIS , vol . 17 , no . 1 , pp . 57–77 , 2008 .
[ 11 ] C . K . Chui , B . Kao , and E . Hung , “ Mining frequent itemsets from uncertain data , ” in PAKDD , 2007 , pp . 47–58 .
[ 12 ] T . Bernecker , H P Kriegel , M . Renz , F . Verhein , and A . Z¨ufle , “ Probabilistic frequent itemset mining in uncertain databases , ” in KDD , 2009 , pp . 119–128 .
[ 13 ] T . Calders , C . Garboni , and B . Goethals , “ Approximation of frequentness probability of itemsets in uncertain data , ” in ICDM , 2010 , pp . 749–754 .
[ 14 ] D . N . Reshef , Y . A . Reshef , H . K . Finucane , S . R . Grossman , G . McVean , P . J . Turnbaugh , E . S . Lander , M . Mitzenmacher , and P . C . Sabeti , “ Detecting novel associations in large data sets , ” Science , vol . 334 , no . 6062 , pp . 518–1524 , 2011 .
[ 15 ] G . J . Sz´ekely and M . L . Rizzo , “ Brownian distance covariance , ” The Annals of Applied Statistics , vol . 3 , no . 4 , pp . 1236–1265 , 2009 .
[ 16 ] U . Alon , N . Barkai , D . A . Notterman , K . Gish , D . M . S . Ybarra , and A . J . Levine , “ Broad patterns of gene expression revealed by clustering of tumor and normal colon tissues probed by oligonucleotide arrays , ” PNAS , vol . 96 , no . 12 , pp . 6745–6750 , 1999 .
[ 17 ] A . W . van der Vaart , Asymptotic Statistics .
University Press , 1998 .
Cambridge
A . Proof of Proposition 8
APPENDIX
In order to prove the proposition we need the following proposition .
U =
M +1 i0,,iM h(Yi0 , . . . , YiM ) ,
1 , N
Proposition 11 ( Theorem 12.3 in [ 17] ) : Let h be a function ( called kernel ) of L parameters . Assume that h is symmetric wrt its parameters ( that is , any permutation of parameters will yield the same result ) . Let Y1 , . . . , YN be N iid variables such that E.h2(Y1 , . . . , YL)fi < ∞ . Then
√
N,N
L i1,,iL h(Yi1 , . . . , YiL ) − µ , where the sum goes over all subsets of size L and µ = E[h(Y1 , . . . , YL) ] , approaches a Gaussian distribution as N goes to infinity .
Proof of Proposition 8 : Note that since Sij and Skj are not independent , we cannot use Central Limit theorem to prove normality . Instead we will use U statistics to prove the result . In order to do that let us first define a function of M + 1 vectors of length M , g(y0 , y1 , . . . , yM ) =
I [ y0xi > yixi ] , where xi are the items of X = x1 ··· xM . Note that
U =
1
N ( N − 1)M g(Yi0 , . . . , YiM )
.
Proposition 11 requires a kernel to be symmetric wrt its parameters . In order to do that , let us define h(y0 , y1 , . . . , yM ) = g(yτ ( 0 ) , . . . , yτ ( M ) ) , where the sum goes over all permutations τ of size M + 1 . N U , where Then according to Proposition 11 a statistic
√ i=1 i0=1
N
M ··· N
τ iM =1 , iM =i0
0150202501502025cp(X;D)cp(X;D,025)(a)Ind0102010203cp(X;D)(b)Plant01502025030303504cp(X;D)(c)Alon010203010203cp(X;D)(d)Thalia010203020250303504cp(X;D)(e)Yeast−4−2024−4−2024zIND(X;D)zPRT(X;D)05010002040ABCDzIND(X;D)5010015020010203040zIND(X;D)0102030−5051015zIND(X;D)0200400−2002040zIND(X;D)patternsize2345678 where the sum goes over all M + 1 subsets of ( 1 , . . . , N ) , converges to a Gaussian distribution .
The statistics U and U have the same mean , say µ = E[U ] = E[U ] , but they are different . We will show next that this difference becomes minute as N approaches infinity . In order to do that , let us define a(N ) = N ( N−1)M and
N
( M +1)! b(N ) =
.
M + 1
The sum of U requires that all rows Yik for must be different where as U only requires that Yi0 is different from the remaining rows . Hence , there are a(N )−b(N ) less terms in U . Let Z be the sum of these terms . We have
U = b(N ) a(N )
U +
Z a(N )
.
Let us write r(N ) = ( a(N )− b(N ))/a(N ) . Both a(N ) and b(N ) are polynomials of degree M + 1 and the coefficient of the highest term is 1 for both polynomials . Consequently , a(N )−b(N ) is a polynomial of degree M . This implies that √ r(N ) and r(N ) N both go to 0 as N approaches infinity . √
We can express the difference as N ( U − U ) = r(N )
, a(N ) N ( U − µ ) converges to According to Proposition 11 , a Gaussian distribution and since r(N ) converges to 0 , it follows that the first term goes to 0 as N goes to infinity . Similarly , the second term goes to 0 since r(N ) N goes to 0 . Finally , to bound the last term note that
√ N ( U − µ ) + r(N )
N µ +
√
√
√
Z
0 ≤ Z a(N )
≤ a(N ) − b(N )
= r(N ) a(N ) √ which implies that Z/a(N ) goes to 0 as N approaches N ( U − θ ) infinity . We have shown that converge to each other in probability and that the latter approaches a Gaussian distribution .
N ( U − θ ) and
√
B . Proof of Proposition 9
First , we need the following technical proposition . Proposition 12 : Assume that we a given integers N and K let Ω = [ 1 , . . . , N ]K be the set of integer vectors of length K . Let fN : Ω → [ 0 , 1 ] be a function such that maxω∈Ω |fN ( ω)| ∈ O(N−K+1 ) . Let P ⊂ Ω be the subset containing only vectors with distinct entries . Assume that we are given K(K − 1)/2 subsets Ωij such that {ω ∈ Ω | ωi = ωj} ⊆ Ωij ⊆ Ω \ P , that is , Ωij contains vectors for which ith and jth entries are the same and has no vectors from P . Then f ( ω ) → K−1
K f ( ω )
ω∈Ω\P i=1 j=i+1
ω∈Ωij as N approaches infinity .
Proof : Partition Ω into K groups Ω0 , . . . , ΩK−1 such that
Ωi = {ω ∈ Ω | ω has K − i distinct entries }
.
A direct computation shows that fifiΩififi = S(K , K − i )
N !
( N − K + i)!
∈ O(N K−i ) ,
|Θ| ∈ O(N K−2 ) . Note that P = Ω0 and that Ω1 ⊆ where S(K , K − i ) is a Sterling number of the second kind . Let Θ = Ω2 ∪ ··· ∪ ΩK−1 . This immediately implies that i,j Ωij . Let ∆ij = Ωij \ Ω1 be the set of vectors that have i and j as common entries and have less than K − 1 unique entries . Note that ∆ij ⊂ Θ . Consequently |∆ij| ∈ O(N K−2 ) . We can now write the sum as f ( ω)− f ( ω ) = f ( ω)+ f ( ω ) . i=j i=j
ω∈Ωij
ω∈∆ij
ω∈Ω\P Since f is bounded by O(N−K+1 ) , the third and fourth terms vanish as N goes to infinity . Proof of Proposition 9 : For notational simplicity , let us assume that X = 1··· M and define f and g as functions of 2M + 2 variables ,
ω∈Θ
E . M k=1 k=1
Oi0ikk
M fi E . M
N Ufi2 = k=1
1 N
E . M and E.√ k=1
Oj0jkk fi fi
. g(ω )
.
Oi0ikk
Oj0jkk f ( i0 , . . . , iM , j0 , . . . , jM ) =
1 N and g(i0 , . . . , j0 , . . . , jM ) =
Let Ω = [ 1 , . . . , N ]2M +2 . Note that
E.N U 2fi = f ( ω )
ω∈Ω Let Ωi as defined in Proposition 12 . Let us define
ω∈Ω for 1 ≤ i ≤ M and M + 1 ≤ j ≤ 2M , and also
Ωij = {ω ∈ Ω | ωi = ωj}
Ωij =(ω ∈ Ω1 | ωi = ωj
) whenever 1 ≤ i , j ≤ M or M + 1 ≤ i , j ≤ 2M . Note that f ( ω ) = g(ω ) whenever ω does not share any entry between the first M entries and the last M entries . This holds when ω ∈ Ω0 or when ω ∈ Ωij for 1 ≤ i , j ≤ M or M + 1 ≤ i , j ≤ 2M .
We can now apply Proposition 12 to conclude that g(ω ) → M f ( ω ) −
( f ( ω ) − g(ω ) )
2M
ω∈Ω as N approaches infinity .
ω∈Ω i=1 j=M +1
ω∈Ωij
Our final step is to compute the sums in the above equation .
N
E.T 2 k fi−E.Tk fi2 = α−µ2 .
fi
TljOlmj
TkiOkmi
, N fi = γij , l=1
E[TkiOkmi ] E[TljOlmj ]
TkiOkmi fi E.Cmj fi
TljOlmj fi E . N fi = µ2 N l=1
.
The last term goes to 0 as N approaches infinity . Hence we have On the other hand ,
ω∈Ωi(M +1 ) f ( ω ) +
ω∈Ω1(M +i ) f ( ω ) → γ2 ii
. g(ω ) =
E[TkiOkli ] E[Tl ]
ω∈Ωi(M +1 )
=
E[Tk ] µ = µ2
.
N N k,l=1 k
1 N
1 N and a similar result holds for Ω1(M +i ) . Combining all these equations proves the proposition .
C . Proof of Proposition 10
Proof : Since the blocks Pi are independent , it follows i=1 µi . In order to prove the result for the variance , let Tk = rnk ( k ; X,Y ) and Tki = rnk ( i ; Pi,Y ) . Let us define immediately that µ = E[U ] = L α = E.T 2 fi , γ(N ) = E[T1T2 ] , γ(N ) We see that α =L i=1 αi and γ(N ) =L fi and β(N ) = Var.√ i = Var.√
Let us define β(N )
N Ufi i = E[T1iT2i ] i=1 γ(N )
.
.
N Ui
.
1 i
A straightforward calculation reveals that i = αi + ( N − 1)γ(N ) β(N ) i − N µ2 i and
β(N ) = α + ( N − 1)γ(N ) − N µ2
.
We can express the variance β(N ) as
β(N ) = α + ( N − 1 )
L i=1(β(N )
= α + i − αi + N µ2 β(N ) i
L N − 1 i − αi + N µ2 i=1
( N − 1)L−1
− N µ2 i ) − N ( N − 1)L−1µ2
.
Let us now consider the right hand side as a function of N . Both terms in the numerator contain µ2N L , consequently this term is annihilated and the highest term in the numerator has degree of L − 1 , its coefficient is equal to i − αi ) ( βN cN = ( L − 1)µ2 + µ2
L
.
1 µ2 i i=1 f ( ω ) =
E[TkiTljOkmiOlmj ]
First note that f ( ω)−g(ω ) =
1 N Let 1 < i , j ≤ M . Then
ω∈Ω1(M +1 )
ω∈Ωi(j+M )
= k=1 m=1 k,l,m=1 k=1
N N E . , N N E.CmiCmj N E . N N N E.Cmi k,l,m=1 m=1 k=1 m=1 m=1
1 N
1 N
=
1 N
N g(ω ) =
=
=
1 N
1 N
1 N
ω∈Ωi(j+M ) where γij = 1 N k=1 E[CkiCkj ] , and
The last equality follows from the fact that 1 k=1 Ckl = N U . Since E[Ckl ] does not depend on k , this immediately implies that E[Ckl ] = E[U ] = µ .
Let 1 < i ≤ M , Then f ( ω ) =
E[TkiOkliTl ] k,l,m=1
1 N
1 N k,l=1
N N N N k,l=1 k,l,m=1
E[TkiOkliTliOlmi ]
E[TliOlkiTk ]
E[TliOlkiTkiOkmi ]
.
ω∈Ωi(M +1 ) and similarly
ω∈Ω1(M +i )
=
1 N
1 N f ( ω ) =
=
Since OkliOlmi + OlkiOkmi = OkmiOlmi for k = l and 0 for k = l , summing two previous sums leads to
E[TkiOkliTliOlmi + TliOlkiTkiOkmi ]
N
1 N
N k,l,m=1
=
1 N k,l,m=1
E[TkiTliOlmiOkmi ] − 1 N
= γ2 ii −
1
N ( N − 1 )
E[TkiTk ]
.
N k=1
N
E.T 2 k,m=1 kiO2 kmi fi
Since the highest term in the demoninator is N L−1 , the fraction converges to limN→∞ cN .
