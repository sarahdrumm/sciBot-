Focal Test Based Spatial Decision Tree Learning : A
Summary of Results
Zhe Jiang∗ , Shashi Shekhar∗ , Xun Zhou∗ , Joseph Knight† and Jennifer Corcoran†
∗Department of Computer Science & Engineering , University of Minnesota , Twin Cities , Minneapolis , USA .
†Department of Forest Resources , University of Minnesota , Twin Cities , Saint Paul , USA .
Email : {zhe , shekhar , xun}@csumnedu Email : {jknight , murph636}@umn.edu
Abstract—Given a raster spatial framework , as well as training and test sets , the spatial decision tree learning ( SDTL ) problem aims to minimize classification errors as well as salt and pepper noise . The SDTL problem is important due to many societal applications such as land cover classification in remote sensing . However , the SDTL problem is challenging due to the spatial autocorrelation of class labels , and the potentially exponential number of candidate trees . Related work is limited due to the use of local test based decision nodes , which can not adequately model spatial autocorrelation during test phase , leading to high salt and pepper noise . In contrast , we propose a focal test based spatial decision tree ( FTSDT ) model , where the tree traversal direction for a location is based on not only local but also focal ( ie , neighborhood ) properties of the location . Experimental results on real world remote sensing datasets show that the proposed approach reduces salt and pepper noise and improves classification accuracy . Keywords—spatial decision tree , focal test , spatial autocorrelation , spatial data mining
I .
INTRODUCTION
Given a raster spatial framework , as well as training and test sets , the spatial decision tree learning ( SDTL ) problem aims to find a decision tree model that minimizes classification errors as well as salt and pepper noise . Figure 1 is an illustrative example . Some input features of aerial photo bands are in Figure 1(a ) as well as Figure 1(b ) and ground truth class labels ( red for dryland , green for wetland ) are in Figure 1(c ) . A decision tree model is learned from the dataset by C4.5 algorithm [ 1 ] , and its final classification result is shown in Figure 1(d ) . As can be seen , the generated decision tree has very poor classification performance with lots of salt andpepper noise . Societal Applications : The SDTL problem has many societal applications . In the field of remote sensing , a large amount of images of the earth surface are collected ( eg , NASA collects about 5TB data per day ) . SDTL can be used to classify remote sensing images into different land cover types [ 2 ] . For example , in wetland mapping [ 3 ] [ 4 ] , explanatory features , including R , G , B , NIR , NDVI bands of remote sensors , are used to mapping land surface into wetland areas and dryland areas . Land cover classification is of great societal importance in urbanization study [ 5 ] , natural resource management [ 6][7 ] , and disaster management [ 8 ] , etc . In medical image processing , SDTL can help in lesion classification and brain tissue segmentation [ 9][10 ] on MRI images . It can also be used for galaxy classification [ 11 ] in astronomy and semi conductor inspection
( a ) Aerial photo features in 2005 ( b ) Aerial photo features in 2008
( c ) ground truth class labels
( d ) DT classification with saltand pepper noise
Fig 1 . Real world problem example ( best viewed in color )
[ 12 ] in material science . Challenges : There are several challenges in the SDTL problem . First , learning samples show spatial autocorrelation in class labels and spatial heterogeneity in feature values . Thus , testing only local feature information in decision nodes results in salt and pepper noise ( locations or pixels whose class labels are different from those for their neighbors ) . For example , the class labels in Figure 1(c ) show strong spatial autocorrelation due to the phenomenon of “ patches ” ( ie , contiguous regions of the same class ) . The final prediction of the local test based decision tree generated by C4.5 algorithm exhibits salt and pepper noise , as shown in Figure 1(d ) . Second , the number of candidate trees is exponential to the number of features . Constructing an optimal decision tree ( eg , minimizing number of nodes , or maximizing average information gain , etc . ) is known to be NP hard [ 13 ] . Related work and limitations : Figure 2 presents a classification of related work . Existing work includes non spatial decision trees ( eg , ID3 [ 14 ] , C4.5 [ 1 ] and CART [ 15 ] ) and spatial entropy or information gain based decision trees [ 16][17][18][19 ] . Non spatial decision trees make the iid assumption of learning samples and ignore the spatial autocorrelation effect . Spatial entropy or information gain based decision trees use spatial autocorrelation level as well as information gain to select candidate tree node tests . However , when all the candidate tree node tests have poor spatial autocorrelation , the spatial entropy or information gain based decision tree will still select one of them , and produce salt andpepper noise . In summary , both of these existing approaches use local test based decision nodes ( ie , decision tree node testing each learning sample independently ) , and thus can not adequately model spatial autocorrelation in the test phase , leading to salt and pepper noise . In contrast to existing work ,
I(f ≤ δ ) =fl 1
−1 if f ≤ δ ; if f > δ .
II . BASIC CONCEPTS AND PROBLEM FORMULATION
This section first introduces basic concepts . Then , it formally defines the spatial decision tree learning problem and illustrates it .
A . Basic Concepts Indicator formula : An indicator formula represents if a test result is TRUE or not . It is defined as below , where f is the feature value of a location and δ is a threshold . For example , the indicator variables of Figure 4(a ) are shown in Figure 4(b ) for ( f ≤ 1 ) .
Neighborhood relationship : A neighborhood relationship characterizes the range of dependency between spatial locations . It is commonly represented as a W matrix , whose element Wi,j has a non zero value when locations i and j are neighbors , and has a zero value otherwise . One important characteristic of a neighborhood is its size ( eg , size s means a ( 2s+1 ) by ( 2s+1 ) window ) . For example , in the spatial raster framework of Figure 3(a ) , the centering pixel has two neighborhoods of different sizes : inner window of size 1 ( 3 by 3 ) , outer window of size 2 ( 5 by 5 ) . The W matrix of the nine pixels of the inner box under a 3 by 3 neighborhood is shown in Figure 3(c ) . For instance , the center pixel ’s neighbors are shown by 1s of the 5th row ( all the other pixels are its neighbors ) .
Fig 2 . Related work classification we propose a focal test based spatial decision model , whereby the tree traversal direction of a learning sample is based on not only local but also focal ( neighborhood ) properties of features . Contributions : There are three main contributions of the paper : ( 1 ) we define a focal test based spatial decision tree ( FTSDT ) model ; ( 2 ) we propose learning algorithms ( including a training algorithm and a prediction algorithm ) for the FTSDT model ; ( 3 ) we evaluate the proposed approach using experiments on real world remote sensing datasets . We also run the proposed FTSDT algorithm in a case study with domain interpretations . Scope : This paper focuses on incorporating focal tests inside a decision tree for raster data classification . Other classification algorithms such as Markov Random Field [ 20 ] , Spatial Autoregression ( SAR ) model[21 ] , logistic regression , neural network , etc . , are beyond the scope . In addition , for simplicity , this paper only considers continuous features . The case of discrete features is not addressed . Outline : The outline of the paper is as follows : Section II defines basic concepts and formalizes the SDTL problem ; Section III introduces the FTSDT learning algorithm with illustrative execution traces . The effectiveness the proposed approach is evaluated on real world remote sensing datasets in Section IV . Section V discusses some relevant techniques . Section 6 concludes the paper with future work .
( a ) neighborhoods
( b ) attribute values
( c ) W matrix
Fig 3 . Neighborhood , autocorrelation , and size
Salt and pepper noise : Salt and pepper noise [ 22 ] is often defined as a kind of fat tail impulse noise in which only a few pixels are noisy but their values are often extreme ( eg , minimum or maximum ) . In a gray scale image , these noisy pixels look like salt and pepper spread on the image . This paper mainly focuses on salt and pepper noise in classified images , where pixel values are nominal class labels . In this case , salt and pepper noise can be considered as a single pixel ( or a small group of contiguous pixels ) that is distinct from its ( or their ) spatial neighborhood . For example , in Figure 5(f ) , the green pixels inside red neighborhoods are salt and pepper noise . Spatial autocorrelation statistic : Given a neighborhood definition and a non spatial attribute , a spatial autocorrelation statistic measures the dependency between attribute values of neighboring locations . A positive high value of the statistic indicates that nearby locations have very similar attribute values . One common spatial autocorrelation statistic is Gamma index [ 23 ] Γ , as defined in Equation ( 1 ) , where W S i,j is a w matrix element with a neighborhood size s , and Si,j is similarity between attribute values ( eg , if the attribute is I , and Si,j = Ii ∗ Ij ) Its local indicator variable , Γ = ΓS
Local Test Based Decision Tree Traditional Non spatial Tree Decision Tree Spatial Entropy or Information Gain Proposed Approach Focal Test Based Spatial Decision Tree 1  1  1  1  1  1  3  3  3  1  1  3  3  3  1  1  3  3  3  1  1  1  1  1  1  010110000101111000010011000110010110111101111011010011000110010000111101000011010 version is defined in Equation ( 2 ) . ( Note : ΓS examples refers to ΓS
I ( i ) omitting i . )
I used in later i,jI(i)I(j ) i,j
I = i,j W S i,j W S I ( i ) = j W S j W S i,j i,jI(i)I(j )
( 1 )
( 2 ) i,j
Γ = i,j W S i,j W S Γ(i ) = j W S j W S i,j i,jSi,j
; ΓS i,jSi,j
; ΓS
Due to spatial heterogeneity , a local autocorrelation statistic may show different trends from a global autocorrelation statistic , especially for salt and pepper noise pixels , whose local gamma values are negative . For example , given the gamma index definition above and a neighborhood size 3 by 3 , the global autocorrelation of Figure 4(b ) is 0.45 , but local autocorrelation of the pixel at ( 3,2 ) is 1 shown in Figure 4(c ) . Moreover , a local autocorrelation level is sensitive to neighborhood size choices . For example , the local gamma of the centering pixel in Figure 3 ( b ) is 1 in a 3 by 3 neighborhood , but it is 0.33 in a 5 by 5 neighborhood .
( a ) values of feature f
( b ) indicator I(f ≤ 1 )
( c ) ΓS
I , neighborhood 3 by 3
( d ) ΓS
I < 0
( e ) f ≤ 1⊕ ΓS
I < 0 , focal test
( f ) f ≤ 1 , local test
Fig 4 . Example of indicator variable , local gamma , local and focal tests
Local test based tree node : A local test based tree node tests the feature value of the location itself , either against a threshold ( continuous features ) or against a set of possible values ( discrete features)[24 ] , and then determines the sample ’s tree traversal direction . Local test is based on the assumption that samples are identically independently distributed , which violates the spatial autocorrelation effect on spatial samples . Thus , a local test has limitation of not incorporating the local spatial autocorrelation or variation of a sample . For example , given feature f whose values are in Figure 4(a ) and a local test f ≤ 1 , the test results are shown in Figure 4(f ) , where two salt and pepper noise pixels appear due to ignorance of spatial autocorrelation effect . Focal function : A focal function is a function of a set of cells within the neighborhood of the current cell in a raster framework . For example , local gamma index ΓS I introduced previously is a focal function . Given feature f shown in Figure 4(a ) , its focal function ΓS
I is shown in Figure 4(c ) .
Focal ( function ) test : A focal ( function ) test is a test of focal function value against a threshold . For example , ΓS I < 0 is a focal test and its test result TRUE means that the current pixel has negative local spatial autocorrelation on values of indicator variable I(f ≤ δ ) , ie , the value is different from those in its neighborhood . The focal test result of Figure 4(a ) is shown in Figure 4(d ) . The two pixels that behaves differently from their neighborhoods are tested as TRUE ( T ) , while other pixels are tested as FALSE ( F ) . Focal test based spatial tree node : A focal test based spatial tree node is a decision node which checks not only local but also focal properties of a location . For instance , ( f ≤ δ ) ⊕ I < 0 ) ( ⊕ means “ XOR ” ) can define a focal test based ( ΓS decision node . The focal test part can help identify salt andpepper noise locations , whose final tree traversal directions are corrected via “ XOR ” operator . As a specific example , the final focal test result of the feature f in Figure 4(a ) is shown in Figure 4(e ) . Compared with local test result in Figure 4(f ) , the focal tree node test maintains spatial autocorrelation structure and has fewer salt and pepper noise pixels . Focal test based spatial decision tree ( FTSDT ) : An FTSDT is a decision tree model made up of focal test based tree nodes . Compared with traditional local test based decision tree , it not only consider a sample ’s local feature property but also its spatial autocorrelation information . Figure 5(g ) is an example , where the focal function is ΓS
I with s = 1 .
B . Problem Definition Formally , the spatial decision tree learning problem is defined as follows :
Given : • A spatial framework S • A spatial neighborhood definition N , and its maximum size Smax • Training and test samples ( with features and class labels ) drawn from S Find : • A decision tree model based on training samples . Objective : • Minimize classification errors as well as salt and pepper noise Constraints : • Training samples form contiguous patches of locations in S • Spatial autocorrelation exists in class labels • Spatial framework S is a 2 D regular grid Example : Consider a raster spatial framework S ( 8 by 8 ) shown in Figure 5(a ) , which consists of training pixels on the upper half and test pixels on the lower half . Neighborhood N is defined as a square window , whose maximum size is 1 ( 3 by 3 ) . The minimum tree node size is 4 . Figure 5(b)(c ) show two candidate features F1 and F2 . Figure 5(d ) shows class labels . The output local test based traditional decision tree learned from the training set is shown in Figure 5(e ) , whose classification has salt and pepper noise , as shown in Figure 5(f ) . In contrast , the output focal test based spatial decision tree is shown in Figure 5(g ) , and its classification is shown in Figure 5(h ) , which has no salt and pepper noise .
1  1  1  1  3  3  3  3  1  1  1  1  3  3  1  3  1  3  1  1  3  3  3  3  1  1  1  1  3  3  3  3  1  1  1  1   ­‐1   ­‐1   ­‐1   ­‐1  1  1  1  1   ­‐1   ­‐1  1   ­‐1  1   ­‐1  1  1   ­‐1   ­‐1   ­‐1   ­‐1  1  1  1  1   ­‐1   ­‐1   ­‐1   ­‐1  1  1  1  0.2  0.2  0.6  0.6  0.3  0.6  0.8  0.8  0.3  0.3  0.8   ­‐1  0.6  0.6   ­‐1  0.8  0.3  0.3  0.8  0.8  0.6  0.3  0.6  0.6  0.2  0.2  1  1    1  F  F  F  F  F  F  F  F  F  F  F  F  F  F  T  F  F  T  F  F  F  F  F  F  F  F  F  F  F  F  F  F  T  T  T  T  F  F  F  F  T  T  T  T  F  F  F  F  T  T  T  T  F  F  F  F  T  T  T  T  F  F  F  F  T  T  T  T  F  F  F  F  T  T  T  T  F  F  T  F  T  F  T  T  F  F  F  F  T  T  T  T  F  F  F  F   Algorithm 1 FTSDT Train(T , C , Smax , N0 ) Input :
Output :
• T : training samples where T [ i][f ] is f th feature value in ith sample • C : class labels where C[i ] is class label of ith sample • Smax : maximum neighborhood size • N0 : minimum decision tree node size • root of an FTSDT model if |T| < N0 or C unique class then IG0 = −∞ for each candidate neighborhood size s ∈ {0Smax} do
L=CreateLeaf( class(C ) ) ; Return L ; for each candidate feature f ∈ {1F} do spatial
( a ) framework raster
( b ) feature F1
( c ) feature F2
( d ) class labels
( e ) DT
( f ) DT classification
( g ) FTSDT
( h ) FTSDT classification
Fig 5 .
Illustrative problem example ( best viewed in color )
III . FTSDT LEARNING ALGORITHMS
This section introduces the learning algorithm of focal testbased spatial decision tree with an illustrative execution trace . The learning algorithm has two phases : a training phase , named FTSDT Train ; and a prediction phase , named FTSDTPredict .
A . Training Phase FTSDT Train : The FTSDT Train algorithm ( Algorithm 1 ) learns an FTSDT classifier from training samples . It is a divide and conquer method with a greedy strategy ( ie , maximizing information gain ) . Steps 1 to 2 check the stopping criteria . If the number of training samples is smaller than a minimum tree node size , or the class labels are identical , a leaf labeled with the majority class will be returned . Steps 3 to 14 select the best candidate feature , threshold , and neighborhood size through exhaustive enumeration . More specifically , for each candidate neighborhood size , feature , and threshold ( generated by picking up unique feature values after sorting ) , training samples are split based on focal test results ( Node Split subroutine ) and the information gain is evaluated . Steps 15 to 17 find the neighborhood size , feature , and threshold with the maximum information gain and create an internal node . The training samples are then split into two subsets based on the focal test results . Steps 18 to 20 recursively call the training algorithm on each subset to construct a subtree . Finally , the internal node is returned .
Sort feature f values T [ i][f ] ( i ∈ {1N} ) in ascending order for each i ∈ {1(N − 1)} do if T [ i][f ] < T [ i + 1][f ] then
δ = ( T [ i][f ] + T [ i + 1][f ])/2 {T1 , T2}=Node Split(T , f , δ , s ) ; Split C into {C1 , C2} according to {T1 , T2} IG=InformationGain(C,C1,C2 ) if IG > IG0 then
IG0 = IG ; s0 = s ; f0 = f ; δ0 = δ ;
I=CreateInternalNode(f0 , δ0 , s0 ) ; {T1 , T2}=Node Split(T , f0 , δ0 , s0 ) ; Split C into C1 and C2 based on {T1 , T2} I.LeftChild=FTSDT Train(T1 , C1 , Smax , N0 ) I.RightChild=FTSDT Train(T2 , C2 , Smax , N0 ) Return I
Node Split : The Node Split subroutine ( Algorithm 2 ) splits the training samples into two subsets based on their focal test results in the decision node . Step 1 initializes the two subsets as empty sets . One subset is to contain samples with node test results TRUE and the other is to contain samples with test results FALSE . Steps 2 to 10 compute the focal tree node test result of each training sample and add the sample to its proper subset accordingly . For each training sample , it computes the local test result ( shown as LocalV al[i] ) , the focal function value test result ( shown ( shown as F ocalV al[i] ) , and the focal as F ocalRes[i] ) . The final ( focal ) test result is a logical operation ( shown as(cid:78 ) ) of the local test result and the focal I < 0 , and the logical operator as ⊕ ( ie , “ XOR ” ) . function test result . For example , we may specify the focal function as ΓS I ( introduced earlier in 2.1 ) , focal function test as ΓS
An Execution Trace of FTSDT Train : Consider the problem example in Figure 5 , where the training set has two features F1 and F2 , the maximum neighborhood size is 1 ( 3 by 3 neighborhood ) , and the minimum tree node size is 4 . Assume ( local gamma index of indicator the focal function is ΓS I variable , introduced earlier ) , the focal test function is ΓS I < 0 and the final tree node test is f ≤ δ⊕ΓS I < 0 . We now execute the FTSDT Train algorithm ( Algorithm 1 ) on the example as follows . Steps 1 to 2 check the stop criteria . Since there are 8 samples ( above the minimum node size 4 ) from 2 different classes , the stop criterion is not met . Steps 4 to 9 enumerate parameter space < s , f , δ > . Since F1 and F2 both have only two distinct values , the only candidate threshold is δ = 1 . Thus , there are four candidates , with s =
Training Set Test Set 1 1 1 1 3 3 3 3 1 1 1 1 3 3 1 3 1 3 1 1 3 3 3 3 1 1 1 1 3 3 3 3 1 1 1 1 3 3 3 3 1 1 3 1 3 3 3 3 1 1 1 1 3 1 3 3 1 1 1 1 3 3 3 3 1 1 1 1 3 3 3 3 1 1 1 3 3 3 3 3 1 1 1 3 3 3 3 3 1 1 1 3 3 3 3 3 1 1 1 3 3 3 3 3 1 1 1 3 3 3 3 3 1 1 1 1 3 3 3 3 1 1 1 1 3 3 3 3 F1≤1 yes no red green yes no red green ( F1≤1)XOR(Γ1I<0 ) ( a ) training : F1
( b ) local test F1 ≤ 1
Fig 6 . Candidate 1 : F1 , δ = 1 , and s = 0 ( local test )
( a ) training : F2
( b ) local test F2 ≤ 1
Fig 7 . Candidate 2 : F2 , δ = 1 and s = 0 ( local test )
( a ) training F1
( b ) indicator I(F1≤1 )
( c ) focal function , ΓS I
( d ) focal test ΓS
I <0
( e ) F1 ≤1 ⊕ ΓS
I <0 , focal test
Fig 8 . Candidate 3 : F1 , δ = 1 and s = 1 ( focal test : 3 by 3 neighborhood )
( a ) training F2
( b ) indicator I(F2≤1 )
( c ) focal function , ΓS I
( d ) focal test ΓS
I <0
( e ) F2 ≤1 ⊕ ΓS
I <0 , focal test
Fig 9 . Candidate 4 : F2 , δ = 1 , and s = 1 ( focal test : 3 by 3 neighborhood )
Algorithm 2 Node Split(T , f , δ , S ) Input :
• T : training samples where T [ i][f ] is f th feature value in ith sample • f : a feature index • δ : threshold of feature test • S : neighborhood size • {T1 , T2} : sample subsets with test result TRUE and FALSE respectively T1 = T2 = ∅ for each i ∈ {1N} do
Output :
LocalRes[i ] = ( T [ i][f ] ≤ δ ) F ocalV al[i]=FocalFunction(T ,i,f,δ,S ) ; F ocalRes[i]=FocalTest(FocalVal[i] ) ;
F inalRes[i ] = LocalRes[i](cid:78 ) F ocalRes[i ] if F inalRes[i]==TRUE then
T1 = T1 ∪ {T [ i]} T2 = T2 ∪ {T [ i]} else return {T1 , T2}
I ) , focal test result ( ΓS
0 , 1 and f = F 1 , F 2 respectively . They are represented in Figure 6 , Figure 7 , Figure 8 , and Figure 9 respectively . Step 10 calls the subroutine Node Split . It computes the local test result ( f ≤ δ ) , focal function ( ΓS I < 0 ) , as well as the the final test result ( f ≤ δ ⊕ ΓS I < 0 ) for each training sample . For example , in Figure 8 for candidate 3 , values of a feature F1 are shown in Figure 8(a ) ; indicator variables are shown in Figure 8(b ) ; focal function values Γ1 I are shown in Figure 8(c ) , where two salt and pepper noise pixels have negative values ; the focal test result is shown in Figure 8(d ) ; final test result is shown in Figure 8(e ) , where the red class has final test result as TRUE , and the green class as FALSE . For candidate 1 ( shown in Figure 6 ) , s = 0 ( ie , I = 0 and focal test results are always FALSE . Thus , local ) , ΓS the final focal test result ( “ XOR ” operation ) is the same as the traditional local test . Steps 11 to 14 split the training samples according to the final tree node test results and compute the information gain . In the example , the information gain values of four candidates are 0.663 , 1 , 0.626 , and 0.626 respectively . Thus , feature F1 , threshold 1 , and neighborhood size s = 1 ( 3 by 3 ) are the greedy choices . Steps 15 to 20 create an internal node based on the selected parameters , as shown in Figure 10(a ) . Training pixels are split accordingly . Thus , four red pixels are in one subset , and four green pixels are in another subset . Since each subset has the same class , leaf nodes are returned . FTSDT Predict : The FTSDT Predict algorithm ( Algorithm 3 ) uses an FTSDT to predict class labels of test samples based on their feature values and a spatial neighborhood structure . It is a recursive algorithm . If the tree node is a leaf , then the class label of the leaf is assigned to all current samples . Otherwise , samples are split into two subsets according to the focal test results in the root node , and each subset is classified by its corresponding subtree . Execution Trace of FTSDT Predict : Given the previously constructed FTSDT model ( Figure 10(a) ) , and feature F1 from test set ( Figure 10(b) ) , the FTSDT Predict algorithm first finds that the current node is a non leaf . Then , the test samples
1  1  1  1  3  3  3  3  1  1  1  1  3  3  1  3  1  3  1  1  3  3  3  3  1  1  1  1  3  3  3  3  T  T  T  T  F  F  F  F  T  T  T  T  F  F  T  F  T  F  T  T  F  F  F  F  T  T  T  T  F  F  F  F  1  1  1  1  3  3  3  3  1  1  1  3  3  3  3  3  1  1  1  3  3  3  3  3  1  1  1  3  3  3  3  3  T  T  T  T  F  F  F  F  T  T  T  F  F  F  F  F  T  T  T  F  F  F  F  F  T  T  T  F  F  F  F  F  1  1  1  1  3  3  3  3  1  1  1  1  3  3  1  3  1  3  1  1  3  3  3  3  1  1  1  1  3  3  3  3  1  1  1  1   ­‐1   ­‐1   ­‐1   ­‐1  1  1  1  1   ­‐1   ­‐1  1   ­‐1  1   ­‐1  1  1   ­‐1   ­‐1   ­‐1   ­‐1  1  1  1  1   ­‐1   ­‐1   ­‐1   ­‐1  1  1  1  0.2  0.2  0.6  0.6  0.3  0.6  0.8  0.8  0.3  0.3  0.8   ­‐1  0.6  0.6   ­‐1  0.8  0.3  0.3  0.8  0.8  0.6  0.3  0.6  0.6  0.2  0.2  1  1    1  F  F  F  F  F  F  F  F  F  F  F  F  F  F  T  F  F  T  F  F  F  F  F  F  F  F  F  F  F  F  F  F  T  T  T  T  F  F  F  F  T  T  T  T  F  F  F  F  T  T  T  T  F  F  F  F  T  T  T  T  F  F  F  F  1  1  1  1  3  3  3  3  1  1  1  3  3  3  3  3  1  1  1  3  3  3  3  3  1  1  1  3  3  3  3  3  1  1  1  1   ­‐1   ­‐1   ­‐1   ­‐1  1  1  1   ­‐1   ­‐1   ­‐1   ­‐1   ­‐1  1    1  1   ­‐1   ­‐1   ­‐1   ­‐1   ­‐1  1  1  1   ­‐1   ­‐1   ­‐1   ­‐1   ­‐1  1  1  1  0.2  0.6  1  1  1  1  1  0  0  0.8  1  1  1  1  1  0.3  0.3  1  1  1  1  1  1  0.2  0.2  1  1  1    1  F  F  F  F  F  F  F  F  F  F  F  F  F  F  F  F  F  F  F  F  F  F  F  F  F  F  F  F  F  F  F  F  T  T  T  T  F  F  F  F  T  T  T  F  F  F  F  F  T  T  T  F  F  F  F  F  T  T  T  F  F  F  F  F   Algorithm 3 FTSDT Predict(R , T ) Input :
• R : Root of an FTSDT model • T : training samples where T [ i][f ] is f th feature value in ith sample Output : • C : class labels where C[i ] is class label of ith sample if R.type == Leaf then C = R.class ; Return C f0 = R.f , δ0 = R.δ , s0 = R.s {T1 , T2}=Node Split ( T , f0 , δ0 , s0 , 0 ) C1=FTSDT Predict ( R.Lef t , T1 ) ; C2=FTSDT Predict ( R.Lef t , T2 ) ; return C=Merge(C1,C2 ) are split into two subsets based on results of the focal test I < 0 ) , as shown in Figure 10(c)(d)(e)(f ) . The ( F1 ≤ 1 ⊕ Γ1 16 pixels on the left have final test results TRUE , and thus traverse the left branch . In the next recursion , they are labeled as the “ red ” class by a leaf node . Similarly , the 16 pixels on the right are labeled as the “ green ” class .
( a ) output FTSDT
( b ) test set F1
( c ) I(F1 ≤ 1 )
( d ) focal function , ΓS I
( e ) focal test , ΓS
I < 0 ( f ) F1 ≤ 1⊕ΓS
I < 0 , focal test
( g ) FTSDT prediction
Fig 10 . Output FTSDT and its prediction
IV . EVALUATION
We evaluated the proposed focal test based spatial decision tree learning algorithm on a real world remote sensing dataset . The goal was to investigate the following questions : • Does the focal test based spatial decision tree reduce saltand pepper noise compared with a local test based decision tree ? • Does the focal test based spatial decision tree improve classification accuracy compared with a local test based decision tree ? • Does the focal test based spatial decision tree have smaller tree size compared with a local test based decision tree ? • How sensitive is the focal test based spatial decision tree learning algorithm to its paramter Smax ?
A . Experiment Setup Experiment design : The experiment design is shown in Figure 11 . For effectiveness evaluation , there are two candidate learning algorithms : the FTSDT learner ( focal test based spatial decision tree ) and LTDT ( local test based decision tree , we used traditional decision tree ( DT ) C4.5 as a representative ) learner . Output LTDT and FTSDT models learned from the training set were used to classify the test set . Their classification accuracy , salt and pepper noise , and tree sizes were compared . All experiments were conducted in C language on a Dell workstation with Quad core Intel Xeon CPU E5630 @ 2.53GHz , and 12 GB RAM .
Fig 11 . Experiment Design
Dataset description : We used high resolution ( 3m*3m ) remote sensing imagery collected from the city of Chanhassen , MN , by the National Agricultural Imagery Program and Markhurd Incorporation . There were 12 continuous explanatory features including multi temporal spectral information ( R,G,B,NIR bands ) , and Normalized Difference Vegetation Index ( NDVI ) for the years 2003 , 2005 , and 2008 . Class labels ( ie , wet land , dry land ) are created by a field crew and photo interpreters . We further selected three different scenes from the city . On each scene , we use stratified clustered sampling . More specifically , we randomly selected a number of wetland and dryland circular contiguous clusters of pixels ( whose radius is around 7 pixel ) to create training set . The remaining pixels in the scenes were used as test sets . Contiguous clusters instead of isolated pixels were used in order to learn focal test rules in decision nodes . More details are given in Table I .
TABLE I .
DESCRIPTION OF DATASETS
Scene
1 2 3
Size 476*396 858*759 482*341
Training samples 2319 ( dryland class ) ; 1883 ( wetland class ) 2963 ( dryland class ) ; 3140 ( wetland class ) 4804 ( dryland class ) ; 3829 ( wetland class )
Choice of focal test functions : For the focal test based spatial decision tree , we used the specific focal test ( f ≤ δ ) ⊕ ( ΓS I < 0 ) described earlier in Section II .
B . Does the focal test based spatial decision tree reduce saltand pepper noise ? Evaluation metric : We compared the amount of salt andpepper noise in prediction through a quantitative spatial autocorrelation measure , ie , the gamma index ( introduced earlier in Section II ) with a queen neighborhood . Parameter settings : The maximum neighborhood size was set to 11 pixels by 11 pixels . Minimum tree node size was 50 pixels . Result : The last columns of Table II , Table III , and Table IV show the gamma index autocorrelation levels of DT and FTSDT predictions on three datasets . Gamma index value ranges between negative one and positive one . A larger gamma index means less salt and pepper noise . As can be seen , FTSDT improves the spatial autocorrelation by mostly over yes no red green ( F1≤1)XOR(Γ1I<0)1  1  1  1  3  3  3  3  1  3  1  1  3  3  3  3  1  1  1  1  3  1  3  3  1  1  1  1  3  3  3  3  1  1  1  1   ­‐1   ­‐1   ­‐1   ­‐1  1   ­‐1  1  1   ­‐1   ­‐1   ­‐1   ­‐1  1  1  1  1   ­‐1  1   ­‐1   ­‐1  1  1  1  1   ­‐1   ­‐1   ­‐1   ­‐1  1  1  1  0.2  0.2  0.6  0.6  0.3  0.6   ­‐1  0.8  0.3  0.3  0.8  0.8  0.6  0.6  0.8  0.8  0.3  0.3   ­‐1  0.8  0.6  0.3  0.6  0.6  0.2  0.2  1  1    1  F  F  F  F  F  F  F  F  F  T  F  F  F  F  F  F  F  F  F  F  F  T  F  F  F  F  F  F  F  F  F  F  T  T  T  T  F  F  F  F  T  T  T  T  F  F  F  F  T  T  T  T  F  F  F  F  T  T  T  T  F  F  F  F  LTDT remote sensing images training patches FTSDT patch size number of patches max neigh size FTSDT Learner LTDT Learner test set , map analysis : accuracy , salt and pepper noise TABLE II .
COMPARISON OF DT AND FTSDT ON SCENE 1
Models
DT
FTSDT
Confusion Matrix 99060 20172 49606 15456 10128 109104 18406 46656
Precision Recall Overall Accuracy Node Number Autocorrelation Level
0.711
0.822
0.762
0.717
0.807
0.845
33
31
0.838
0.957
TABLE III .
COMPARISON OF DT AND FTSDT ON SCENE 2
Models
DT
FTSDT
Confusion Matrix 29749 383044 161192 71134 384631 28162 185653 46673
Precision Recall Overall Accuracy Node Number Autocorrelation Level
0.844
0.868
0.694
0.799
0.844
0.884
21
21
0.917
0.976
TABLE IV .
COMPARISON OF DT AND FTSDT ON SCENE 3
Models
DT
FTSDT
Confusion Matrix 99723 18234 28935 8837 14044 103913 8008 29764
Precision Recall Overall Accuracy Node Number Autocorrelation Level
0.613
0.679
0.766
0.788
0.826
0.858
55
35
0.86
0.964
TABLE V .
SIGNIFICANCE TEST ON DIFFERENCE OF CONFUSION MATRICES
Scene Scene 1 Scene 2 Scene 3
DT
FTSDT
KHAT statistic KHAT variance KHAT statistic KHAT variance 3.52 ∗ 10−6 7.67 ∗ 10−7 34.98 ∗ 10−6
3.85 ∗ 10−6 1.01 ∗ 10−6 5.44 ∗ 10−6
0.584 0.647 0.564
0.651 0.744 0.634
Z score
Result
24.8 72.8 21.9
Significant Significant Significant
10 % . This confirms that FTSDT reduces salt and pepper noise compared to DT . matrices of DT ’s and FTSDT ’s , a Z test is conducted on a pair of KHAT statistics between DT and FTSDT , as shown below :
C . Does the focal test based spatial decision tree improve classification accuracy ? Parameter setting : The maximum neighborhood size was 11 by 11 , minimum tree node size was 50 pixels . Classification accuracy : We compared the classification performance of the proposed FTSDT classifier with DT in terms of confusion matrices , precision & recall , and overall accuracy on test set . The results are in Table II , Table III , and Table IV . In the confusion matrix , the columns are test samples classified as dryland and wetland respectively , and the two rows are test samples whose true class labels are dryland and wetland respectively . Precision and recall were computed on the wetland class . As can be seen , the FTSDT improves the overall classification accuracy ( percentage of correctly classified pixels ) of DT by around 4 % . Improvements are also shown in the precision and recall . The reason of accuracy improvement is that the focal test in tree node helps correct salt n pepper noise errors during classification process . We further conducted significance tests on the difference of confusion matrices . The statistic used is KHAT ( estimate of Kappa coefficient ) [ 25][26][27 ] . More specifically , it is defined as :
ˆK = nk i=1 nii −k n2 −k i=1 ni+n+i i=1 ni+n+i
, where n is the sum of all elements , and nii , ni+ and n+i are diagonal , row sum and column sum respectively . KHAT reflects how a confusion matrix is different from a random guess . After KHAT and its variance is computed for confusion
Z =
| ˆK1 − ˆK2|
V ar( ˆK1 ) + V ar( ˆK2 )
Table V shows the test results . All datasets from three scenes show increase of KHAT statistics and have Z scores over 20 . Thus , the improvement of classification performance of FTSDT over DT is statistically significant .
D . Does the focal test based spatial decision tree have smaller tree size ? Parameter setting : The maximum neighborhood size was 11 by 11 , minimum tree node size was 50 pixels . Size of trees : We compared the number of nodes in learned FTSDT and DT models . A smaller tree is often desirable since it is more compact and easier to interpret . The results are shown in the “ node number ” columns of Table II , Table III , and Table IV . As can be seen , the FTSDT model has relatively a smaller number of nodes . This is due to the fact that the traditional decision tree has the iid assumption and can only discriminate different classes according to local feature values in each node . In contrast , focal test based decision tree can exploit the spatial context of a sample to help discriminate classes , especially for samples with potential salt n pepper noise errors . Thus , focal test based spatial decision tree requires fewer nodes to fit the training samples .
E . Parameter Sensitivity Analysis : maximum neighborhood size Smax Parameter setting : The minimum tree node size was 50 pixels . Dataset scene 3 was used .
( a ) Features RGB 2008
( b ) Feature NDVI 2008
( c ) Features RGB 2003
( d ) Features RGB,NIR 2005
( e ) Features NDVI 2005
( f ) Prediction of DT
( g ) Prediction of FTSDT
( h ) Legend of prediction map
Fig 13 . Case study dataset and prediction results of DT and FTSDT ( best viewed in color )
We observed the effectiveness ( ie accuracy , spatial autocorrelation ) of proposed approach under different values of parameter Smax ( 0 represents local test based decision tree ) . As can be seen in Figure 12(a ) , as Smax value increases , the autocorrelation levels of prediction first increase quickly and then increase slowly or remain relatively stable . This is due to the fact that salt n pepper error patterns are in relatively small spatial scales . When Smax is large enough to capture the saltn pepper noise patterns , increasing Smax does not improve autocorrelation a lot . Figure 12 ( b ) shows the sensitivity of test accuracy on different Smax . As Smax increases , the test accuracy first increases and then decreases . A very large Smax may lead to overfitting . In practice , Smax can be set according to the resolution of image and the scale of autocorrelation on class labels .
F . Case Study Settings : To visually illustrate the difference of predictions between FTSDT learning algorithm and DT algorithm , we run a case study . The dataset is still from the city of Chanhassen . The input multi temporal optical features are shown from
Figure 13(a ) to Figure 13(e ) . Target classes are wetland and dryland . We use the right part of the image as training set , and the left part as test set . The maximum neighborhood size is set to 6 ( 13 by 13 ) and minimum tree node size is 500 ( due to large training set ) . Results : The predictions of DT and FTSDT are shown in Figure 13(f ) and Figure 13(g ) respectively . The green and red colors represent correctly classified wetland and correctly classified dryland . The black and blue colors represent false wetland and false dryland . The prediction of DT has lots of salt and pepper noise due to high local variation of features within patches of the same class . For example , the area in the yellow circle in Figure 13(f ) is a dryland area consisting of trees ( referring to the same area of Figure 13(a) ) . The black salt and pepper noise pixels inside the yellow circle corresponds to locations without tree coverage . These pixels are misclassified as wetland here due to the iid assumption and local tree node tests of DT . In contrast , FTSDT can capture these local variations by focal tree node tests in a large neighborhood . Thus , FTSDT has much less salt and pepper noise in the same area .
True wetland True dryland False wetland False dryland and pepper noise and improves classification accuracy , compared with local test based decision tree . In future work , we plan to work on the computational performance tuning of FTSDT learning algorithms , including identifying computational bottleneck , characterizing the computational structure , designing computationally efficient learning algorithm and conducting cost complexity analysis . We will work on ensemble of decision trees . We may also work on spatial heterogeneity effect , eg , focal tree node test and neighborhood sizes vary across different locations to better model the real world spatial data .
VII . ACKNOWLEDGEMENTS
This material is based upon work supported by the National Science Foundation under Grant No . 1029711 , IIS 1320580 , 0940818 and IIS 1218168 as well as USDOD under Grant No . HM1582 08 1 0017 , and HM0210 13 1 0005 . We would like to give special thanks to Kim Koffolt for improving the readability of the paper .
REFERENCES
[ 1 ]
J . R . Quinlan , C4.5 : programs for machine learning . Morgan kaufmann , 1993 .
[ 2 ] M . A . Friedl and C . E . Brodley , “ Decision tree classification of land cover from remotely sensed data , ” Remote sensing of environment , vol . 61 , no . 3 , pp . 399–409 , Elsevier , 1997 .
[ 3 ]
[ 4 ]
J . M . Corcoran , J . F . Knight , and A . L . Gallant , “ Influence of multisource and multi temporal remotely sensed and ancillary data on the accuracy of random forest classification of wetlands in northern minnesota , ” Remote Sensing , vol . 5 , no . 7 , pp . 3212–3238 , 2013 .
J . F . Knight , B . P . TOLCSER , J . M . CORCORAN , and L . P . RAMPI , “ The effects of data selection and thematic detail on the accuracy of high spatial resolution wetland classifications , ” Photogrammetric engineering and remote sensing , vol . 79 , no . 7 , pp . 613–623 , 2013 .
[ 5 ] F . Yuan , K . E . Sawaya , B . C . Loeffelholz , and M . E . Bauer , “ Land cover classification and change analysis of the twin cities ( minnesota ) metropolitan area by multitemporal landsat remote sensing , ” Remote sensing of Environment , vol . 98 , no . 2 , pp . 317–328 , 2005 .
[ 6 ] A . Deschamps , D . Greenlee , T . Pultz , and R . Saper , “ Geospatial data integration for applications in flood prediction and management in the red river basin , ” in International Geoscience and Remote Sensing Symposium , Toronto , Canada . Symposium , Geomatics in the Era of RADARSAT ( GER’97 ) , Ottawa , Canada , 2002 .
[ 7 ] R . Hearne , “ Evolving water management institutions in the red river basin , ” Environmental Management , vol . 40 , no . 6 , pp . 842–852 , Springer , 2007 .
[ 8 ] C . Van Westen , “ Remote sensing for natural disaster management , ” International Archives of Photogrammetry and Remote Sensing , vol . 33 , no . B7/4 ; PART 7 , pp . 1609–1617 , 2000 .
[ 9 ] A . Akselrod Ballin , M . Galun , R . Basri , A . Brandt , M . Gomori , M . Filippi , and P . Valsasina , “ An integrated segmentation and classification approach applied to multiple sclerosis analysis , ” in Computer Vision and Pattern Recognition , 2006 IEEE Computer Society Conference on , vol . 1 .
IEEE , 2006 , pp . 1122–1129 .
[ 10 ] M . Celebi , H . Kingravi , Y . Aslandogan , and W . Stoecker , “ Detection of blue white veil areas in dermoscopy images using machine learning techniques , ” in Proc . of SPIE Vol , vol . 6144 . Citeseer , 2006 , pp . 61 445T–1 .
[ 11 ] D . Bazell and D . W . Aha , “ Ensembles of classifiers for morphological galaxy classification , ” The Astrophysical Journal , vol . 548 , no . 1 , p . 219 , 2001 .
Fig 12 . Single parameter sensitivity analysis
V . DISCUSSIONS
We propose a focal test based spatial decision tree to automatically address the salt n pepper noise issue of decision tree for raster image classification . There are some other relevant techniques to reduce salt n pepper noise , including pre processing ( median filter [ 22 ] , weighted median filter [ 28 ] , adaptive median filter [ 29 ] , decision based filter [ 30 ] [ 31] ) , post processing ( per parcel classification [ 32 ] , spectral and spatial classifier [ 33] ) , and adding additional contextual variables into the input features [ 34 ] . These techniques can help reduce salt n pepper noise , but they are mostly non automatic , and require extra efforts by users beyond model learning and prediction . Another increasingly popular technique is image segmentation , especially Geographic Object Based Image Analysis ( GEOBIA ) [ 35 ] . In this technique , the image is first segmented into different objects . Then each object is a minimum classification unit . The object based classification method shows promising results in reducing salt and pepper noise , but it requires great efforts of domain experts by manual tuning and thus is not automatic either .
VI . CONCLUSIONS AND FUTURE WORK
This paper proposed a novel approach for the spatial decision tree learning ( SDTL ) problem . The SDTL problem is important due to many societal applications but is challenging due to spatial autocorrelation , spatial heterogeneity , and large computational cost . We proposed an FTSDT model , which tests not only local but also focal properties of a location in decision nodes . Experimental results on a real world remote sensing dataset show that the proposed FTSDT reduces salt
02468085090095100Sensitivity of Spatial Autocorrelation to S_maxmax neighborhood sizegamma index02468080082084086088090Sensitivity of Classification Accuracy to S_maxmax neighborhood sizeaccuracy ( % ) techniques , ” Geoscience and Remote Sensing , IEEE Transactions on , vol . 47 , no . 8 , pp . 2973–2987 , 2009 .
[ 34 ] A . Puissant , J . Hirsch , and C . Weber , “ The utility of texture analysis to improve per pixel classification for high to very high spatial resolution imagery , ” International Journal of Remote Sensing , vol . 26 , no . 4 , pp . 733–745 , 2005 .
[ 35 ] G . Hay and G . Castilla , “ Geographic object based image analysis ( geobia ) : A new name for a new discipline , ” in Object based image analysis . Springer , 2008 , pp . 75–89 .
[ 12 ] T . Yuan and W . Kuo , “ Spatial defect pattern recognition on semiconductor wafers using model based clustering and bayesian inference , ” European Journal of Operational Research , vol . 190 , no . 1 , pp . 228– 240 , 2008 .
[ 13 ] S . R . Safavian and D . Landgrebe , “ A survey of decision tree classifier methodology , ” Systems , Man and Cybernetics , IEEE Transactions on , vol . 21 , no . 3 , pp . 660–674 , 1991 . J . Quinlan , “ Induction of decision trees , ” Machine learning , vol . 1 , no . 1 , pp . 81–106 , Springer , 1986 .
[ 14 ]
[ 15 ] L . Breiman , J . Friedman , C . J . Stone , and R . A . Olshen , Classification and regression trees . Chapman & Hall/CRC , 1984 .
[ 16 ] X . Li and C . Claramunt , “ A spatial Entropy Based decision tree for classification of geographical information , ” Transactions in GIS , vol . 10 , no . 3 , pp . 451–467 , Blackwell Publishing Ltd , 2006 .
[ 17 ] D . Stojanova , M . Ceci , A . Appice , D . Malerba , and S . Dˇzeroski , “ Global and local spatial autocorrelation in predictive clustering trees , ” in Discovery Science . Springer , 2011 , pp . 307–322 .
[ 18 ] D . Stojanova , M . Ceci , A . Appice , D . Malerba , and S . Dzeroski , “ Dealing with spatial autocorrelation when learning predictive clustering trees , ” Ecological Informatics , Elsevier , 2012 .
[ 19 ] Z . Jiang , S . Shekhar , P . Mohan , J . Knight , and J . Corcoran , “ Learning spatial decision tree for geographical classification : a summary of results , ” in Proceedings of the 20th International Conference on Advances in Geographic Information Systems . ACM , 2012 , pp . 390–393 .
[ 20 ] A . H . Solberg , T . Taxt , and A . K . Jain , “ A markov random field model for classification of multisource satellite imagery , ” Geoscience and Remote Sensing , IEEE Transactions on , vol . 34 , no . 1 , pp . 100–113 , 1996 .
[ 21 ] M . Celik , B . M . Kazar , S . Shekhar , D . Boley , and D . J . Lilja , “ Spatial dependency modeling using spatial auto regression , ” in Workshop on Geospatial Analysis and Modeling with Geoinformation Connecting Societies ( GICON ) , International Cartography Association ( ICA ) , 2006 . [ 22 ] C . Boncelet , “ Image noise models , ” in Handbook of Image and Video Processing , 2nd ed . , A . C . Bovik , Ed . Academic Press , 2005 , ch . 45 [ 23 ] L . Anselin , “ Local indicators of spatial association lisa , ” Geographical analysis , vol . 27 , no . 2 , pp . 93–115 , 1995 . J . R . Quinlan , “ Learning decision tree classifiers , ” ACM Computing Surveys ( CSUR ) , vol . 28 , no . 1 , pp . 71–72 , 1996 .
[ 24 ]
[ 25 ] R . G . Congalton , “ A review of assessing the accuracy of classifications of remotely sensed data , ” Remote sensing of Environment , vol . 37 , no . 1 , pp . 35–46 , 1991 . J . D . Bossler , J . R . Jensen , R . B . McMaster , and C . Rizos , Manual of geospatial science and technology . CRC Press , 2004 .
[ 26 ]
[ 27 ] R . G . Congalton and K . Green , Assessing the accuracy of remotely sensed data : principles and practices . CRC press , 2008 .
[ 28 ] D . Brownrigg , “ The weighted median filter , ” Communications of the
ACM , vol . 27 , no . 8 , pp . 807–818 , 1984 .
[ 29 ] H . Hwang and R . A . Haddad , “ Adaptive median filters : new algorithms and results , ” Image Processing , IEEE Transactions on , vol . 4 , no . 4 , pp . 499–502 , 1995 .
[ 30 ] R . H . Chan , C W Ho , and M . Nikolova , “ Salt and pepper noise removal by median type noise detectors and detail preserving regularization , ” Image Processing , IEEE Transactions on , vol . 14 , no . 10 , pp . 1479–1485 , 2005 .
[ 31 ] S . Esakkirajan , T . Veerakumar , A . N . Subramanyam , and C . PremChand , “ Removal of high density salt and pepper noise through modified decision based unsymmetric trimmed median filter , ” Signal Processing Letters , IEEE , vol . 18 , no . 5 , pp . 287–290 , 2011 . J . Wijnant and T . Steenberghen , “ Per parcel classification of urban ikonos imagery , ” in Proceedings of 7th AGILE Conference on Geographic Information Science , 2004 , pp . 447–455 .
[ 32 ]
[ 33 ] Y . Tarabalka , J . A . Benediktsson , and J . Chanussot , “ Spectral–spatial classification of hyperspectral imagery based on partitional clustering
