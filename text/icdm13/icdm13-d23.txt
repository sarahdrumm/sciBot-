Beyond Boolean Matrix Decompositions : Toward Factor Analysis and
Dimensionality Reduction of Ordinal Data
Radim Belohlavek
Palacky University , Olomouc , Czech Republic
Email : radimbelohlavek@acmorg
Marketa Krmelova
Palacky University , Olomouc , Czech Republic marketakrmelova@gmailcom
Abstract—Boolean matrix factorization ( BMF ) , or decomposition , received a considerable attention in data mining research . In this paper , we argue that research should extend beyond the Boolean case toward more general type of data such as ordinal data . Technically , such extension amounts to replacement of the two element Boolean algebra utilized in BMF by more general structures , which brings non trivial challenges . We first present the problem formulation , survey the existing literature , and provide an illustrative example . Second , we present new theorems regarding decompositions of matrices with ordinal data . Third , we propose a new algorithm based on these results along with an experimental evaluation . Keywords ordinal data ; matrix decomposition ; Galois con nection ; concept lattice ; aggregation ; factor analysis
I . INTRODUCTION
A . Motivation , Problem Description , Contributions
In this paper , we are concerned with extending the problems and methods of BMF toward a more general case . Instead of Boolean matrices , we consider matrices with entries in a partially ordered set L bounded by 0 and 1 , such as the five element scale L = {0 , 1 4 , 1} . In a Boolean matrix I , the entries represent presence/absence of attributes , ie Iij = 1/0 if the object i has/does not have the attribute j . In the more general case , the entries represent degrees to which attributes are present , with 0 and 1 representing full absence and full presence .
4 , 1
2 , 3
Data represented by matrices over scales L appears in many areas . Examples are questionnaires in which the respondents ( objects , matrix rows ) provide answers to questions ( attributes , matrix columns ) in the form of grades ( degrees , levels ) , such as the degree of satisfaction of the customer ( respondent ) with a particular product or service ( question ) , or various kinds of performance data in which objects are evaluated in terms of attributes/criteria . In such data , the degrees in L are often small in number and have linguistic labels such as “ bad ” , “ weak ” , “ medium ” , “ good ” , “ excellent ” , in congruence with the well known Miller ’s 7 ± 2 phenomenon [ 11 ] . In addition , real valued data may often conveniently be transformed to data with degrees by an established or ad hoc scaling . Clearly , Boolean matrices are a particular case for L = {0 , 1} . Supported by grant GACR No . P103/11/1456 .
Central to this paper is the decomposition problem and its variants : Given an n× m ( object attribute ) matrix I with entries in a scale L , find a decomposition of I into a product
I = A ◦ B
( 1 ) of two matrices with entries in L , namely an n × k ( objectfactor ) matrix A and a k × m ( factor attribute ) matrix B with k reasonably small ( more in Section II ) . The Boolean matrix product ◦ , defined by
( A ◦ B)ij = k max l=1 min(Ail , Blj ) ,
( 2 )
( A ◦ B)ij =(cid:87)k may naturally be generalized in the setting with grades . Namely , one may put where ( cid:87 ) denotes supremum in L and ⊗ is an appropri l=1 Ail ⊗ Blj ,
( 3 ) ate conjunctive aggregation function generalizing the truth function of classical conjunction . Such functions are studied in many valued logics ( more in Section II ) . Importantly , the model ( 1 ) retains its meaning from the Boolean case . Namely , according to the principles of many valued logic [ 8 ] , ( 1 ) says that the degree to which object i has attribute j equals to the truth degree of the proposition “ there exists factor l such that l applies to i and j is one of the particular manifestations of l ” . Apart from this logical interpretation , ( 3 ) may simply be understood as a new type of matrix product .
We formulate two decomposition problems and present an illustrative example . Next , we present new results regarding the geometry of decompositions , which help answer the question of where to focus in computing decompositions . In view of these results , we propose a new decomposition algorithm for matrices over scales , inspired by [ 2 ] , [ 3 ] , [ 4 ] , and provide an experimental evaluation .
B . Related Work
As the literature on matrix decompositions is too numerous to be included here , we survey only directly related work and refer the reader to [ 5 ] and [ 9 ] for further references . Regarding BMF , [ 12 ] , in which NP completeness of the basic problem is observed , is one of the first papers in this area . The interest in BMF in data mining is due to Miettinen : [ 9 ] with the ASSO algorithm whose extension for scales we use in experiments ; further papers include [ 10 ] ( selecting the number of factors using MDL ) . [ 7 ] is the first paper on “ tiling ” Boolean data , which is closely related to BMF . The utilization of formal concepts in BMF , their universality , and a a fast BMF algorithm and other issues are the content of [ 5 ] . Matrices over scales and other structures are examined in many papers , including those on matrices over semiring like algebras and binary fuzzy relations [ 1 ] , [ 8 ] . Direct predecessor to this paper are [ 2 ] , [ 4 ] .
II . DECOMPOSITION PROBLEM IN DETAIL ie a ⊗ ( (cid:87 ) j∈J bj ) =(cid:87 )
A . Scales and Aggregation lattice [ 1 ] , [ 8 ] ( arbitrary infima ( cid:86 ) and suprema ( cid:87 ) exist ) .
We assume that the partial order ≤ makes L a complete This is automatically satisfied if L is a chain ( ie a ≤ b or b ≤ a for every a , b ∈ L ) , in which case a ∧ b = min(a , b ) and a ∨ b = max(a , b ) . We also assume that ⊗ is commutative , associative , has 1 as its neutral element ( a⊗ 1 = a = 1⊗ a ) , and distributes over arbitrary suprema , j∈J ( a ⊗ bj ) . Such ⊗ is considered a truth function of ( many valued ) conjunction [ 8 ] and the classical conjunction is a particular case of ⊗ for L = {0 , 1} . That is , if a and b are truth degrees of propositions p1 and p2 , then a ⊗ b is the truth degree of p1&p2 . Such L conforms to the structure of a complete residuated lattice and satisfies several properties , utilized in Section IV , for which we refer to [ 8 ] . Among the properties are natural properties of conjunction such as a ⊗ 0 = 0 and monotony wrt ≤ . Importantly , ⊗ induces another operation , → , called the residuum of ⊗ , which plays the role of the truth function of implication and is defined by a → b = max{c ∈ L| a ⊗ c ≤ b} . n , . . . , n−1
Many examples of such scales are known in many valued logic [ 8 ] , among them those where L is the real unit interval [ 0 , 1 ] or its finite equidistant subinterval , ie L = n , 1} . In particular , we use scales with the {0 , 1 Łukasiewicz operations , ie a ⊗ b = max(0 , a + b − 1 ) and a → b = min(1 , 1 − a + b ) , which have convenient properties . ( Many other ⊗ are available , such as a ⊗ b = min(a , b ) , and all of them coincide with classical conjunction for a , b ∈ {0 , 1} [ 8] . ) In what follows , we assume that the scale L is equipped with the above operation ⊗ and , for simplicity , that L is a finite chain in the rest of this paper ( the general results appear in an extended version of this paper ) . B . Decomposition Problem and Its Two Variants
We now present two concrete variants , the discrete basis problem over L , DBP(L ) , and the approximate factorization problem over L , AFP(L ) , see [ 9 ] , [ 3 ] , [ 5 ] for the Boolean case . Let sL : L × L → [ 0 , 1 ] be an appropriate function measuring closeness of degrees in L . For matrices I , J ∈ Ln×m , put n,m i,j=1 sL(Iij , Jij ) n · m s(I , J ) =
.
( 4 )
In general , we require sL(a , b ) = 1 if and only if a = b , and sL(0 , 1 ) = sL(1 , 0 ) = 0 , in which case s(I , J ) = 1 if and only if I = J . sL may be defined by sL(a , b ) = a ↔ b , where a ↔ b = min(a → b , b → a ) is the so called biresiduum ( degree of equivalence from a logical point of view ) of a and b ( → is the residuum of ⊗ ) . For the Łukasiewicz operations we obtain sL(a , b ) = 1 − |a − b| this way . One easily observes that the normalized Hamming distance of Boolean matrices I and J , often used in BMF , is just 1 − s(I , J ) provided sL(a , b ) = a ↔ b . Problems : – DBP(L ) : Given I ∈ Ln×m and a positive integer k , find A ∈ Ln×k and B ∈ Lk×m that maximize s(I , A ◦ B ) . – AFP(L ) : Given I and prescribed error ε ∈ [ 0 , 1 ] , find A ∈ Ln×k and B ∈ Lk×m with k as small as possible such that s(I , A ◦ B ) ≥ ε .
In view of [ 9 ] , [ 5 ] and the remarks above , DBP(L ) and AFP(L ) are clearly NP hard optimization problems . Hence , we need to resort to algorithms providing approximate solutions .
III . ILLUSTRATIVE EXAMPLE
The data in Table I describes 5 most popular dog breeds and their 11 attributes1 ( we analyze the full set of 151 breeds in Section VI ) . We transform the original scale {1 , . . . , 6} to the six element chain L = {0 , 0.2 , 0.4 , 0.6 , 0.8 , 1} and use the Łukasiewicz ⊗ ( Section II A ) . We reprethe grades in L by shades of gray as follows : sent
.
The 5 × 11 object attribute matrix I and its decomposition I = AF ◦ BF into the object factor and factor attribute matrices AF and BF are shown in Fig 1 . The decomposition is obtained using the algorithm described in Section V and utilizes a set F of formal concepts as factors ( these notions are fully described below ; we proceed on intuitive ground ) .
Every factor Fl is represented by the lth column in AF and the lth row in BF . The entries ( AF )il indicate the degrees to which factor l applies to breed i , while ( BF )lj represents the degree to which attribute j is a particular manifestation ( is typical ) of factor l . For example , F1 is manifested by the three kinds of friendliness and affection ( attributes with high degrees in the first row of BF ) and applies in particular to Labradors , Golden Retrievers and Beagels ( breeds with high degrees in the first column of
1http://wwwpetfindercom/
000204060810 Table I : Five most popular dog breeds s g o d s d r a w o t
. d n e i r F
5 6 3 2 6 s r e g n a r t s
. w o t
. d n e i r F
6 6 4 3 6 s t e p r e h t o
. w o t
. d n e i r F
6 6 3 4 6 y t i l i b a n o i t c e t o r P
3 3 2 6 2 s s e n l u f y a l P
6 6 5 3 4 y g r e n E 5 4 5 4 4 g n i n i a r t f o e s a E 6 6 3 6 2 y t i l i b a g o d h c t a
W 5 4 6 6 5 g n i m o o r G 3 4 5 3 2 e s i c r e x E 4 4 2 5 4 n o i t c e f f
A 6 6 4 4 6
Labrador Retrievers Golden Retrievers Yorkshire terriers German shepherds Beagles
Figure 1 : Decomposition I = AF ◦ BF . I , AF , and BF are the bottom right , bottom left , and top matrix , respectively .
AF ) , and to some extent to Yorkshires . The factor may hence be termed friendliness . In a similar manner , F2 and F3 may be interpreted as guardian dog and dogs suitable for kids . Interestingly , F1 , F2 , and F3 explain , by and large , the whole data and hence , the other factors may be neglected . Namely , denoting by AF3 and BF3 the 5×3 and 3×11 matrices ( parts of AF and BF ) , the degree s(I , AF3 ◦ BF3 ) of similarity of I to AF3 ◦ BF3 , ie reconstructability of the original data I from the first three factors , equals 092
IV . GEOMETRY OF DECOMPOSITION AND ESSENTIAL
PARTS OF MATRICES OVER SCALES
A . Basic Observations , Coverage , Formal Concepts
For two matrices J1 , J2 ∈ Ln×m we put
J1 ≤ J2 iff ( J1)ij ≤ ( J1)ij for every i , j
( 5 ) in which case we say that J1 is contained in J2 . J ∈ Ln×m is called a rectangle if J = C ◦ D for some C ∈ Ln×1 and D ∈ L1×m . ( In the Boolean case , rectangles are just tiles in terms of [ 7] . ) Unlike the Boolean case , the C and D for which J = C ◦ D are not unique . We say that a rectangle J covers i , j in I if Jij = Iij . Lemma 1 : The following conditions are equivalent : ( a ) I = A ◦ B for some A ∈ Ln×k and B ∈ Lk×m . ( b ) There exist rectangles J1 , . . . , Jk ∈ Ln×m such that
I = J1 ∨ ··· ∨ Jk , ie Iij = maxk l=1(Jl)ij .
( c ) There exist rectangles J1 , . . . , Jk ∈ Ln×m contained in
I such that every i , j in I is covered by some Jl .
( C↑)j =(cid:86)n
Importantly , Lemma 1 allows us to consider the problem of decomposition of I as the problem of covering the entries in I by rectangles contained in I . Next we show that optimal in such coverage are rectangles that correspond to so called formal concepts of I . A formal concept in I ∈ Ln×m [ 1 ] may be looked at as a pair C , D of vectors C ∈ L1×n and D ∈ L1×m satisfying C↑ = D and D↓ = C , where i=1(Ci → Iij ) and ( D↓)i =(cid:86)m j=1(Dj → Iij ) . The set B(I ) = {C , D | C↑ = D , D↓ = C} of all formal concepts equipped with a partial order ≤ , defined by C1 , D1 ≤ C2 , D2 iff C1 ≤ C2 ( iff D2 ≤ D1 ) , forms a complete lattice , called the concept lattice of I . For an ( indexed ) set F = {C1 , D1 , . . . ,Ck , Dk} ⊆ B(I ) of formal concepts of I , define the n × k and k × m matrices AF and BF by
( AF )il = ( Cl)i and ( BF )lj = ( Dl)j .
By Lemma 1 , AF ◦ BF is the ∨ superposition of the l ◦ Dl . The following theorem shows that rectangles C T formal concepts of I are optimal factors for the approximate decompositions of I providing a from below approximation of I , ie A ◦ B ≤ I ( generalizing [ 2 ] which concerns exact decompositions I = A ◦ B ) . Theorem 1 : Let for I ∈ Ln×m there exist A ∈ Ln×k and B ∈ Lk×m such that A ◦ B ≤ I . Then there exists a set F ⊆ B(I ) of formal concepts of I with |F| ≤ k such that for the n × |F| and |F| × m matrices AF and BF over L we have s(I , AF ◦ BF ) ≥ s(I , A ◦ B ) . B . Essential Parts of Matrices over Scales
Interestingly , an inspection of the concept lattice B(I ) reveals an important fact—the possibility to differentiate the role of matrix entries for decompositions . In , particular , we identify so called essential part of I , the part to focus on when computing decompositions . Definition 1 : J ≤ I is called an essential part of I if J is minimal wrt ≤ having the property that for every F ⊆ B(I ) we have : if J ≤ AF ◦ BF then I = AF ◦ BF . In other words , the coverage of J by formal concepts of I guarantees the coverage of all entries in I . It turns out that intervals in B(I ) play a crucial role . For C ∈ L1×n , D ∈ L1×m , put γ(C ) = C↑↓ , C↑ and µ(D ) = D↓ , D↓↑ , and denote by IC,D the interval
IC,D = [ γ(C ) , µ(D) ] .
Let {a/i} denote the “ singleton ” vector C with zero components except Ci = a . Let
Iij = {I{a/i},{b/j} | a ⊗ b = Iij}
F1F1F2F2F3F3F4F4F5F5F6F6F7F7LabradorRetrieversGoldenRetrieversYorkshireterriersGermanshepherdsBeaglesEnergyPlayfulnessFriendtowardsdogsFriendtowardsstrangersFriendtowardsotherpetsProtectionabilityExerciseAffectionEaseoftrainingWatchdogabilityGrooming and put
Iij .
Iij =
Correctness of GREESSL easily follows from Theorems 3 and 4 and the above considerations .
0 flIij
We now present an important theorem that identifies which formal concepts of I cover a given entry of I . Theorem 2 : A rectangle corresponding to E , F ∈ B(X , Y , I ) covers i , j in I iff E , F ∈ Iij . Denote by E(I ) ∈ Ln×m the matrix over L defined by if Iij is non empty and minimal wrt ⊆ , otherwise ,
( E(I))ij = where ⊆ denotes inclusion of intervals in B(I ) . The following two theorems provide are the main result in this section and are utilized in the new algorithm .
Theorem 3 : E(I ) is the unique essential part of I . Theorem 4 : Let G ⊆ B(E(I ) ) be a set of factor concepts of E(I ) , ie E(I ) = AG ◦ BG . Then every set F ⊆ B(I ) containing for each C , D ∈ G at least one concept from IC,D is a set of factor concepts of I , ie I = AF ◦ BF .
V . GREESSLALGORITHM
↑I j .
GREESSL , inspired by the algorithm in [ 3 ] , is based on the results in Section IV and some other facts mentioned below , is primarily designed for AFP(L ) , but can also be used for DBP(L ) . The pseudocode describes the algorithm computing an exact decomposition of I but an easy modification makes it an algorithm computing ε approximate decompositions ( in l . 3 , stop when precision ε is reached ) . In the pseudocodes , ∅ denotes the empty set or the vector full of zeroes , depending on the context , F ∨{a/j} denotes F with the component j updated to Fj ∨ a , and C ⊗ D denotes the crossproduct of C and D , ie the rectangle for which ( C ⊗ D)ij = Ci ⊗ Dj . Moreover , cov ( U , F , J ) and cov I ( U , D,E ) denote the number of i , j ∈ U covered in I by the rectangle F ↓J ⊗F ↓J↑J and ( D↓E )↑I↓I ⊗(D↓E↑E )↓I↑I , respectively . The fuzzy set like notation {a/j} ∈ C↑I \ F means Fj < a ≤ C COMPUTEINTERVALS first computes E(I ) and then a set G of factors of E(I ) , each C , D ∈ G representing the interval IC,D in B(I ) from which it is possible to obtain a decomposition of I according to Theorem 4 . In fact , we use an improvement of Theorem 4 : for G , it suffices that the crossproducts C↑I↓I ⊗ D↓I↑I corresponding to C , D ∈ G cover all entries in I ( l . 12 ) . The formal concepts in G are computed in a greedy manner from E(I ) in l . 5–10 . The entries covered by C↑I↓I ⊗ D↓I↑I are removed from U . The selection is repeated until U is empty . With G obtained this way , GREESSL performs a greedy search for factors , in the intervals IC,D , C , D ∈ G , in l . 3–21 . For every IC,D , we select the formal concept in IC,D with best coverage in l . 6–11 in a manner similar to the one used in COMPUTEINTERVALS . The best found concept E , F over all the intervals is then added to F in l . 18 ; IC,D in which E , F was found is removed from G in l . 19 .
Algorithm 1 : GREESSL Input : matrix I with entries in scale L Output : set F of factors for which I = AF ◦ BF 1 G ← COMPUTEINTERVALS(I ) 2 U ← {i , j|Iij > 0} ; F ← ∅ 3 while U is non empty do 4 5 6 7
J ← D↓I ⊗ C↑I ; F ← ∅ ; sC,D ← 0 while exists {a/j} ∈ C↑I \ F st cov ( U , F ∨ {a/j} , J ) > sC,D do s ← 0 foreach C , D ∈ G do select{a/j} maximizing cov ( U , F ∨ {a/j} , J ) F ← ( F ∨ {a/j})↓J↑J ; E ← ( F ∨ {a/j})↓J sC,D ← cov ( U , F , J )
8 9 10 11 12 13 14 15 16 17 18 19 20 21 end 22 return F end if sC,D > s then
E , F ← E , F C , D ← C , D s ← sC,D end end add E , F to F remove C , D from G remove from U entries i , j covered by E ⊗ F in I
Algorithm 2 : COMPUTEINTERVALS Input : matrix I with entries in scale L Output : set G ⊆ B(E(I ) ) 1 E ← E(I ) 2 U ← {i , j|Eij > 0} 3 while U is non empty do 4 5
D ← ∅ ; s ← 0 while exists {a/j} ∈ D st cov I ( U , D ∨ {a/j},E ) > s do select {a/j} maximizing cov I ( U , D ∨ {a/j},E ) D ← ( D ∨ {a/j})↓E↑E ; C ← ( D ∨ {a/j})↓E s ← cov I ( U , D,E ) end add C , D to G remove from U all i , j covered by C↑I↓I ⊗ D↓I↑I in I
6 7 8 9 10 11 12 end 13 return G
VI . EXPERIMENTAL EVALUATION
In the evaluation on real and synthetic data , we use GREESSL and another algorithm , ASSOL , which is inspired by the well known ASSO algorithm [ 9 ] and presented in detail in an extended version of this paper . We observe the ability of the extracted factors to explain ( reconstruct ) the input data by means of s(I , AF ◦ BF ) , where F is the examined set of factors ( usually the first k factors obtained ) as well as the interpretation of factors . dataset Breeds Decathlon IPAQ Music size
151 × 11 28 × 10 4510 × 16 900 × 26
|L| 6 5 3 7
||I|| 1963 266 41624 20377
||E(I)|| 362 59 1281 5952
||E(I)||/||I||
0.184 0.221 0.031 0.292
Table II : Real data
A . Real Data
The basic characteristics of data are shown in Table II . ( ||A|| denotes the number of non zero entries in matrix A ) . Notice that the reduction in the number of non zero entries in E(I ) compared to I is significant .
Dog Breeds2 extends the dataset from Section III to 151 breeds . GREESSL found 20 factors providing an exact decomposition of the 151 × 11 matrix I . Already the set F3 consisting of the first three factors explains a large portion of the data , namely s(I , AF3 ◦ BF3 ) = 0795 Among these is a factors which represents dogs that excel in sports ( such as agility , flyball , frisbee ) or guide , service , and therapy dogs , such as Golden Retriever , Labrador Retriever , or Papillon . Another is naturally interpreted as guardian dog . Interestingly , these two factors are similar to factors F3 and F2 described in Section III . In fact , the factors F1 , F2 , and F3 from Section III , when extended to the 151 × 11 matrix , cover 0.85 of the matrix according to s , illustrating an interesting natural property that we observed in several examples . ASSOL , on the other hand , computed 7 factors , which are not so easy to interpret compared to those from GREESSL—a feature that we observed in most examples we examined . Decathlon3 We analyzed the results of 2004 Olympic decathlon and represented them by a 28 × 10 matrix I ( 28 athletes , 10 disciplines of decathlon ) using a five element scale L . Using GREESSL , we obtained 10 factors that we consulted with an experienced decathlon coach . Among the most important factors are the factor speed , containing to high degrees the attributes 100m , long jump , and hurdles ; explosiveness , containing long jump , shot put , high jump , and javelin ; and a factor containing high jump and 1500m , typical of light weight athletes . All these factors were found natural by the decathlon coach .
IPAQ Data4 consists of international questionnaire data involving 4510 respondents answering 16 questions using a three element scale , regarding physical activity . The questions include those regarding their age , sex , body mass index ( BMI ) , health , to what extent the person bicycles , walks , etc . GREESSL produced 17 factor concepts . As with the other examples , the first three factors may be considered sufficient to explain the data . These may be interpreted as as healthy people with good education who cycle on a regular basis ; people with normal BMI who walk on a daily basis ; and people who are employed , own a car , and cycle on a regular basis . All these groups are considered important according to an expert kinanthropologist .
Music Data5 consists of results of a study inquiring how people perceive speed of song depending of various song characteristics . The data consists of a 900 × 26 matrix over a six element scale L , representing a questionnaire involving 30 participants who were presented 30 music samples ( 29 complex complex ones and one simple tone ) , and 26 attributes regarding emotional experience of participants , using a 6 element scale L . Using GREESSL , we obtained 29 factors . The authors of this study examined the factors and concluded that the groups of music samples corresponding to the factors are meaningful and that the factors can be interpreted in terms of emotional experience . For example , an interesting factor with a good coverage contained songs No . 5 , 7 , 16 , and 26 , all of which are melancholic . Among other interesting factors are the one manifested to a high degree by attributes Ugly and Violent ; the one manifested by Restful , Safe , Stable , and Inert ; and the factor manifested by Successful , Valuable , Meaningful , and Significant .
Coverage of Data by Factors Table III displays the numbers of factors produced by the two algorithms that are needed to achieve a prescribed coverage represented by s , see ( 4 ) . For example , the first row corresponding to Breeds says that we need the set F2 consisting of the first two factors produced by ASSOL in order for the coverage to be at least 0.75 , ie s(I , AF2 ◦ AF2 ) ≥ 0.75 , while in case of GREESSL , we need the first three factors . “ NA ” indicates that the prescribed coverage is not achievable by the factors produced by ASSOL .
B . Synthetic Data
We used Set 1–4 , each consisting of 500 n×m matrices I obtained as products of n×k and k×m randomly generated matrices A and B with prescribed probability distributions over L , see Table IV . Table VI displays selected results of coverage s by the first k factors for the datasets and the two algorithms . We also include the percentage s= of entries i , j for which Iij = ( AFk ◦ BFk )ij , where Fk is the set of the first k factors , ie s= is stronger than s . “ – ” for ASSOL means that no new factors were produced .
C . Discussion
The first couple of factors produced by ASSOL tend to have a better coverage compared to GREESSL . On the other hand , beyond certain coverage , ASSOL stops producing factors and is not able to compute an ( exact ) decomposition
2http://wwwpetfindercom/ 3http://wwwsports referencecom/ 4http://wwwipaqkise/ , Belohlavek et al . ,
181(2011 ) , 1774–1786 .
Inf . Sciences
5Flaska K . , Cakirpaloglu P . , Identification of the multidimensional model of subjective time experience . Int . Conf . Time Perspective , Converging Paths in Psychology , Portugal , September 4–8 , 2012 ( extended version submitted ) . number of factors needed ASSOL
GREESSL coverage s/s= by the first k factors dataset Breeds
Decathlon
IPAQ
Music s
0.75 0.85 0.95
1
0.75 0.85 0.95
1
0.75 0.85 0.95
1
0.75 0.85 0.95
1
2 3 NA NA 2 4 NA NA 1 1 NA NA 2 NA NA NA
3 7 11 15 3 5 8 10 10 12 15 17 7 14 25 29 dataset Set 1
Set 2
Set 3
Set 4 k 1 4 8 14 1 4 8 12 16 1 5 10 20 47 1 5 10 20 38
ASSOL
0849/0699 0884/0759
0841/0416 0878/0555
0900/0604 0947/0794
0872/0585 0930/0725
– –
– – –
– – –
– – –
GREESSL 0638/0398 0874/0760 0961/0927
1/1
0688/0287 0900/0679 0974/0905 0999/0994
1/1
0729/0345 0899/0614 0948/0885 0983/0959
1/1
0674/0416 0872/0606 0937/0752 0986/0945
1/1
Table III : Quality of decompositions ( real data ) . dataset Set 1 Set 2 Set 3 Set 4 size 50×50 50×50 100×50 100×100
|L| 3 5 5 5 k 10 10 25 20 distribution on L in A and B
[ 1 3 1 8 1 8 1 8
[ 1 8 [ 1 8 [ 1 8
1 3 1 4 1 4 1 4
1 3 ] 1 4 1 4 1 4
1 4 ] 1 4 ] 1 4 ]
Table IV : Synthetic data . of I , while GREESSL always is , with a reasonably small number of factor needed for coverage very close to 1 . This is congruent with the fact that ASSOL and GREESSL are primarily designed for DBP(L ) and AFP(L ) , respectively , as well as with the available evidence from the Boolean case . We found that GREESSL produces easier interpretable factors compared to ASSOL . This is partly because if |L| > 2 ( non Boolean case ) , rectangles with values “ around the middle ” in L , such as 0.5 , which may be produced as factors by ASSOL but not in general by GREESSL , have good coverage and are thus sometimes selected by ASSOL . This issue needs to be carefully examined in future research . We also performed experiments comparing GREESSL with the algorithm in [ 4 ] . On average , GREESSL requires 30 % less factors to achieve a prescribed coverage .
VII . CONCLUSIONS
We presented theoretical results regarding the geometry of decompositions of data over scales . We presented a new decomposition algorithm and provided its experimental evaluation . We demonstrated that decompositions , generalizing avg ||E(I)||/||I|| avg ||E(I)|| avg ||I|| 2452 2499 4998 10000 dataset Set 1 Set 2 Set 3 Set 4 Table V : Characteristics of synthetic data .
0.079 0.143 0.123 0.213
193 358 614 2130
Table VI : Coverage s and s= by the first k factors .
BMF , help reveal interesting factors in data over scales . Future research shall include : ( a ) further theoretical advancement matrices over scales ; ( b ) further design of algorithms including those inspired by the existing BMF algorithms ; ( c ) development of case studies in factor analysis of ordinal data and applications of the decomposition methods in machine learning in presence of ordinal data ; ( d ) comparison to other relevant decomposition algorithms such as the non negative matrix factorization .
REFERENCES
[ 1 ] R . Belohlavek , Fuzzy Relational Systems : Foundations and
Principles . , Kluwer , New York , 2002 .
[ 2 ] R . Belohlavek , Optimal decompositions of matrices over residuated lattices , J . Logic Comput . 22(2012 ) , 1405–1425 . [ 3 ] R . Belohlavek , M . Trnecka , From below approximations in Boolean matrix factorization : Geometry and new algorithm . ( submitted , available at arXiv ) .
[ 4 ] R . Belohlavek , V . Vychodil , Factor analysis of incidence data via novel decomposition of matrices . LNAI 5548(2009 ) , 83–97 . [ 5 ] R . Belohlavek , V . Vychodil , Discovery of optimal factors in binary data via a novel method of matrix decomposition . J . Computer and System Sciences 76(1)(2010 ) , 3–20 .
[ 6 ] B . Ganter , R . Wille , Formal Concept Analysis . Mathematical
Foundations . Springer , Berlin , 1999 .
[ 7 ] F . Geerts , B . Goethals , T . Mielik¨ainen , Tiling databases , Proc .
Discovery Science 2004 , pp . 278–289 .
[ 8 ] S . Gottwald , A Treatise on Many Valued Logics . Research
Studies Press , Baldock , Hertfordshire , England , 2001 .
[ 9 ] P . Miettinen , T . Mielik¨ainen , A . Gionis , G . Das , H . Mannila , The discrete basis problem , IEEE TKDE 20(2008 ) , 1348–62 . [ 10 ] Miettinen P . , Vreeken J . , Model order selection for Boolean matrix factorization , ACM SIGKDD 2011 , pp . 51–59 .
[ 11 ] G . T . Miller , The magical number seven , plus or minus two ,
Psychol . Rev . 63(1956 ) , 81–97 .
[ 12 ] D . S . Nau et al . , A mathematical analysis of human leukocyte antigen serology , Math . Biosci . 40(1978 ) , 243–270 .
