External Evaluation of Topic Models : A Graph Mining Approach
Hau Chan
Department of Computer Science
Stony Brook University hauchan@csstonybrookedu
Leman Akoglu
Department of Computer Science
Stony Brook University leman@csstonybrookedu
Abstract—Given a topic and its top k most relevant words generated by a topic model , how can we tell whether it is a low quality or a high quality topic ? Topic models provide a low dimensional representation of large document corpora , and drive many important applications such as summarization , document segmentation , word sense disambiguation , etc . Evaluation of topic models is an important issue ; since lowquality topics potentially degrade the performance of these applications . In this paper , we develop a graph mining and machine learning approach for the external evaluation of topic models . Based on the graph centric features we extract from the projection of topic words on the Wikipedia pagelinks graph , we learn models that can predict the humanperceived quality of topics ( based on human judgments ) , and classify them as high or low quality . Experiments on four real world corpora show that our approach boosts the prediction performance up to 30 % over three baselines of various complexities , and demonstrate the generality of our method to diverse domains . Further , we provide interpretation of our models and outline the discriminating characteristics of topic quality .
Keywords topic models ; human evaluation ; graph mining
I . INTRODUCTION
Topic modeling is an area that focuses on the extraction of topics from document corpora . Given a large collection of documents D and the number of desired topics T , a topic modeling method M , such as LDA [ 1 ] , models each document d ∈ D as a multinomial distribution over T topics , where each topic is in turn a multinomial distribution over W words . Typically , only a small number of words are important ( ie have high likelihood ) in each topic ( also only a small number of topics are relevant for each document ) . Topic models have been studied widely [ 1 ] , [ 2 ] , [ 3 ] and have important applications in database summarization [ 4 ] , word sense discrimination [ 5 ] , information discovery [ 6 ] , and many others . Naturally these applications rely on the quality of topics that the topic models generate . An issue of concern , however , is that it is often likely for topic models to output low quality topics in addition to the high quality ones . For example , consider the two topics with their top 10 most likely words in Table I . From humans’ perspective , the first topic ( T1 ) consists of more semantically coherent words , while the second topic ( T2 ) contains patchy groups of mostly incoherent words .
Low quality topics can potentially degrade the performance of the applications ; eg they could mislead topicbased document similarity , introduce noise in clustering , and cause poor semantic interpretation . This makes the evaluation of topic models a crucial task .
Previous research focused on the statistical ( or quantitative ) evaluation of topic models [ 7 ] . This type of evaluation measures either the generalization performance of a topic model based on likelihood on held out test datasets , or its performance on external tasks . However these do not measure the interpretability of individual topics . In fact , the seminal paper [ 8 ] showed that there is a negative correlation between human evaluation and statistical evaluation of topic models . This finding started a new episode in topic model evaluation , by shifting focus to semantic coherence of topics . It prompted researchers to come up with conceptual ( or qualitative ) evaluation techniques , that can identify the human perceived quality of topics .
Several works on conceptual evaluation of topic models have been proposed within the last 4 5 years [ 8 ] , [ 9 ] , [ 10 ] , [ 11 ] , which consider the coherence of individual topics . [ 8 ] elicits human input , while others try to develop a single statistical measure that mimics real human judgements on topic evaluation tasks . ( See §V for details on related work ) . None of these proposals ( i ) exploits a collection of evidential measures , or ( ii ) builds a learning model to predict conceptual topic quality ; which is the basis of our work . We summarize our contributions below .
• Problem formulation : We formulate the evaluation problem as a supervised classification task and derive a predictive model that “ learns ” from human judgments to classify topics as good or poor as perceived by humans .
• Novel graph centric features using Wikipedia : To construct a set of evidential features for our learner , we develop a novel graph mining approach which revolves around the creation and extraction of graph centric properties of the topic words’ projection subgraphs on the Wikipedia page links graph ( referred to as WikiLinks throughout text ) . WikiLinks consists of nodes that represent “ things ” that have a Wikipedia page where edges capture the hyperlinks among these pages . Intuitively , we think of semantically coherent topics
Table I
EXAMPLE TOPICS T1 ( HIGH QUALITY ) AND T2 ( LOW QUALITY ) OF A TOPIC MODEL . steam , engine , valve , piston , cylinder , pressure , boiler , air , pump , pipe cut , system , capital , pointed , opening , building , character , round , france , paris
T1 : T2 : to consist of words that are “ close by ” in this graph , and construct features based on graph topology and closeness accordingly .
• Experiments : Using our predictive model we perform experiments on topics extracted from four real world document corpora : two from news , one from books , and one from medicine . Our results show the effectiveness and generality of our approach in predicting and interpreting the human perceived quality of topic models , where we achieve up to 30 % better classification performance compared to three baseline predictors .
In the rest of the paper , we give an overview of our proposed method ( §II ) , explain it in detail ( §III ) present experiment results ( §IV ) , survey related work ( §V ) , and conclude with summary and future directions ( §VI ) .
II . OVERVIEW
Problem Statement . The main research question we consider can be stated as follows : Given a set of topics output by a topic model , how can we learn to classify them into low versus high quality topics
( as perceived by humans ) ?
We give a more detailed definition of our problem in ( §III A ) and provide the highlights of our proposed method next . Proposed Topic Evaluation Framework . We describe our framework in five parts ; ( §III B ) Wikipedia page links graph , ( §III C ) graph projections , ( §III D ) graph centric features , ( §III E ) labeled case libraries , and ( §III F ) prediction models . A flow diagram showing the operation of our method is given in Figure 1 . ( §III B ) WikiLinks graph : Wikipedia is a large knowledge base being used and edited by millions of people around the world . To evaluate the human perceived quality of topics , we use this knowledge base generated by humans themselves . Particularly we use the WikiLinks graph , in which nodes represent Wikipedia pages and edges denote their hyperlink relations . ( §III C ) Graph projections : Given a set of k topic words , we project the words onto the WikiLinks graph , that is , we map each topic word to the page that is associated with it in the graph . For example , the word steam in T1 above would map to page ( or WikiLinks node ) http : //enwikipediaorg/wiki/Steam We then consider the induced subgraph of these nodes ( projection graph ) , which may not be a connected subgraph . We choose several connector nodes in the original graph to obtain a second subgraph ( spanning graph ) .
( §III D ) Graph centric features : Wikipedia links graph is constructed by humans where editors introduce edges between pages ( ie nodes in WikiLinks ) by their relevance . Therefore , we expect the words of a semantically coherent topic to lie “ close by ” in this graph , and extract features based on graph closeness . ( §III E ) Generating labeled case libraries : For our supervised classification task , we obtain binary training labels for topics ( good versus poor quality ) . We consider two learning settings ; one of predicting the relative quality of topics ( based on rank order of words ) where the labels are obtained implicitly , and another of predicting the absolute ( ie human perceived ) quality of topics ( based on human judgments ) where labels are obtained explicitly . ( §III F ) Learning to predict topic quality : Finally , we use the graph centric properties from ( §III D ) as evidential features and the case libraries from ( §III E ) as labels to learn statistical models that provide predictors of topic quality . Given many possible graph centric features , we perform feature selection to identify a subset of discriminative ones , which we use to interpret our models .
III . PROPOSED FRAMEWORK
A . Problem Definitions
We consider the topic quality prediction problem . We study it under two settings : ( 1 ) absolute and ( 2 ) relative quality prediction .
Our main problem aims to build models to predict the absolute , or the human perceived , quality of the topics . Here , scores provided by several human judges determine the positive and negative class training labels .
• ( P1 ) Absolute ( Human Perceived ) Quality Prediction : Given a topic ( ie a set of k words ) , predict its quality ( good/poor ) as judged by humans .
We also study a related classification task of predicting relative quality of topic words . Each topic output by a topic model consists of a sequence of top K words sorted by their relevance to the topic . In other words , the words ranked higher are more strongly related to a given topic than the words that come later in the sequence . We treat the top k words of the topics as the positive ( ie good ) class examples , and bottom k words in top K as the negative ( ie poor ) class examples . For example for k = 10 , bottom k consists of words in rank order [ 11 20 ] when K = 20 , and [ 91 100 ] when K = 100 . As such , the prediction task becomes easier when K gets larger , since the separation between the training examples increases . Obtaining good prediction accuracy on
Figure 1 . Proposed topic evaluation framework . Given output topics by a topic model , projection and spanning graphs of topic words are created based on the WikiLinks structure and graph centric features are extracted for learning predictive models . this task would prove WikiLinks a suitable external resource to rely on .
• ( P2 ) Relative Quality Prediction : Discriminate good versus poor quality topics defined by top k versus bottom k words , respectively .
Having described our classification problems , we next need to represent each topic with a set of features . Our key idea for feature extraction is to exploit Wikipedia , and use this human generated resource to construct topic subgraphs , from which we derive evidential topic features . B . Wikipedia Links Graph
Wikipedia page links dataset contains internal links between Wikipedia articles ( ie entities)1 . As such , the pagelinks data lends itself for a graph representation ( which we call the WikiLinks graph ) in which nodes denote Wikipedia entities , and edges capture the internal link relations among the Wikipedia articles .
For example , let us consider the entity steam . The corresponding Wiki page can be found at http://enwikipedia org/wiki/Steam . Other Wiki pages can be reached from this page by following hyperlinks on this page , eg , the page on piston ( http://enwikipediaorg/wiki/Piston ) and mist ( http://enwikipediaorg/wiki/Mist ) are among those other , related entities . As such , the nodes piston and mist are 1 hop away from steam , thus are its neighbors .
WikiLinks is an excellent resource to guide for humanperceived evaluation of topic qualities , exactly because it is created by humans themselves—the entities are linked by their relatedness , as perceived by human editors .
Our key insight is to exploit the “ graph closeness ” of related entities in WikiLinks to quantify the semantic quality of topics . Intuitively , the words of a semantically coherent topic , such as {steam engine valve piston } , would have high proximity in the WikiLinks . In fact , the wiki page for engine directly links to steam , and steam links to engine through steam engine . putting these two words 1 2 hops away . In the sample visualization2 of WikiLinks in Figure 1(b ) , related entities are observed to cluster in the graph topology .
As for coverage , Wikipedia provides a comprehensive resource with a massive collection of entities . In our version of WikiLinks1 , the graph statistics are :
|N|
WikiLinks
17 , 170 , 893
Directed |E| Undirected |E| 117 , 434 , 138 158 , 373 , 970
C . Projection and Spanning Graphs
We next provide definitions for topic subgraphs . Consider the WikiLinks graph G(N , E ) with node set N , edge set E ( we experimented with both directed and undirected WikiLinks ) . Let W denote the set of k topic words , ie |W| = k . We project the topic words onto WikiLinks by mapping each word to a node ( or entity ) in the graph . In general , not all words will exist in WikiLinks , that is , |N ∩ W| ≤ k . We denote the mapped word set as M = N ∩ W ⊆ W .
• Topic projection graph is a subgraph gM ( M , EM ) induced on G with node set M and edge set EM : {(u , v ) ∈ E , u ∈ M ∧ v ∈ M} .
This graph may potentially consist of multiple disconnected components . In order to obtain a connected graph , we use a set of additional , connector nodes C ⊆ N to build a graph that spans the topic words .
1http://wikidbpediaorg/Downloads38#wikipedia pagelinks
2Resource : http://wwwflickrcom/photos/mbiddulph/6070900906/
( a ) Topic Model Topics M T1 . steam engine valve piston … T2 . cut system capital building … T3 . pain disease cases fever … T4 . furniture chair table wood … T5 . modern view study face … T6 . porcelain pottery ceramic … … . ( c ) Projection Graph ( d ) Spanning Graph ( e ) Evidential Graph based Features ( g ) Learning / Prediction Model Predictions ( b ) Wikipedia Links Graph ( f ) Labels ( Good / Poor ) • Topic spanning graph is a subgraph gS(M ∪ C , ES ) with node set U = M ∪ C and edge set ES : {(u , v ) ∈ E , u ∈ U ∧ v ∈ U} .
Ideally , the spanning graph contains the minimal set C to make the projection graph connected . However , it is NP hard to find the minimal set , by reduction from the Steiner tree problem [ 12] ) : given a set X of nodes , interconnect them by a subgraph of shortest cost , where cost is defined as the sum of the ( weights ) of edges in the resulting subgraph . Therefore , we use the Minimum Spanning Tree ( MST ) approximation of the Steiner tree problem .
To construct the spanning graph , we first compute the pairwise shortest paths among the mapped M nodes to build a graph gSP with edge weights w(u , v ) , where w(u , v ) denotes the shortest path length in G between nodes u , v ∈ M . For undirected WikiLinks , gSP is a complete graph as all nodes have paths from one to another ( ie WikiLinks is a weakly connected graph ) . For directed WikiLinks , gSP may contain missing edges as not all nodes have a directed path to others ( ie WikiLinks contains multiple strongly connected components ) . Next we find the MST of gSP , and expand it to obtain the spanning graph . Expansion involves introducing the connector nodes along the shortest paths of the MST , where we denote the union of connector nodes by C . Note that the spanning graph may no longer be a tree but may contain loops due to the intersection of the connector node sets of the paths . Following on our running example , we show the projection and spanning graphs for the topics T1 and T2 of §I in Figure 2 .
D . From Topic Subgraphs to Graph Centric Features
Given the projection and spanning subgraphs , we generate a set of evidential graph centric features . There are many features one could extract from a given graph . We want features that could potentially help differentiate good topics from poor ones . Good quality topic words are conjectured to lie “ close by ” in WikiLinks , reachable with many short paths from one another . On the other hand , the words of a poor topic would be separated in the graph topology . We can observe that these insights hold for T1 ( good ) and T2 ( poor ) of §I in Figure 2 . Specifically , T1 contains more words that exist in WikiLinks ( ie words that map to WikiLinks nodes ) , consists of fewer connected components in its projection subgraph ( ie more nodes with direct connection ) , requires fewer connector nodes to build its spanning graph , and so on . Using these observations , we construct features based on graph topology and closeness .
Table II gives the list of features we constructed . In total , we constructed 19 features capturing the key topological properties of the projection and spanning subgraphs , as well as the closeness measures of the topic words in the original
WikiLinks graph.3 We group our features into three :
• PROJ contains features of the projection graph gM , the number of such as the maximum node degree , connected components in gM , etc . ;
• D SPAN consists of topological features of the directed spanning graph gS including its density , ratio of connector nodes to mapped nodes , etc . ;
• D SP consists of features capturing the pairwise reachability between the topic words ( excluding the selfpairs ) , based on the directed shortest paths .
Next we describe how we construct labeled case libraries for training and how we learn classification models for topic quality prediction .
E . Generating Case Libraries
We used news articles , books , and medical documents as our corpora . Descriptions of the datasets are in Table III .
For the prediction of the human perceived ( absolute ) quality of topics ( P1 ) , we used the BOOKS and NEWS corpora , as previously used in [ 10 ] , [ 11].4 They consist of T = 120 and T = 117 topics , respectively . We considered the topics to consist of their top 10 words . All 237 topics were presented to 9 human judges . The judges were given guidelines on how to judge the goodness of the topics , and decide to what extent the topics were coherent , interpretable , meaningful , and easy to label with a short subject heading . They were also shown examples of good and bad topics . Notice that the BOOKS and NEWS corpora come from domains that are quite general , and thus we do not require the judges to have expertise in a specific domain ( eg , medicine ) .
These nine judges evaluated the topics and provided annotations for each topic in 3 point scale : 1 : ‘good’ , 2 : ‘mediocre’ , 3 : ‘poor’ . We used these human ratings to generate labels for our classification models . Specifically , topics with average rating below 1.5 are assigned to the positive ( good ) class , and negative otherwise . Examples of training topics from BOOKS ( top few words ) are given below ( average rating in parentheses ) :
+ silk lace embroidery tapestry gold embroidered ( 1 ) + garden plant soil planting seed bloom spring ( 1.11 ) + seed trees soil root planting plant tree ( 1.33 ) − world people soul mind read reading live ( 2.56 ) − white munich phil room student people head ( 2.67 ) − person occasion purpose respect answer short ( 3 ) We assume that there exists a global ground truth of labels and each human expert is a noisy version of it . To validate this , we performed two measurements to quantify the inter annotator agreement among the nine judges . The average pairwise Spearman ’s rank correlation coefficient is
3Our experiments with directed and undirected versions of the WikiLinks graph revealed that the directed features provide more predictive power than the undirected ones . Therefore we focus our discussion on the directed features .
4We thank David Newman and his group for sharing the NEWS and
BOOKS datasets as well as their human topic annotations .
( a1 ) gM of T1 Figure 2 . word , dotted white square : missing word , gray oval : connector node .
( b1 ) gS of T1
Projection and spanning graphs , gM and gS respectively , for the two example topics T1 and T2 as given in §I . Blue square : mapped topic
( a2 ) gM of T2
( b2 ) gS of T2
EVIDENTIAL FEATURES PROJ AND D SPAN EXTRACTED RESPECTIVELY FROM PROJECTION AND SPANNING GRAPHS OF TOPIC WORDS ON WikiLinks ,
AS WELL AS PAIRWISE SHORTEST PATH FEATURES D SP , ALL USED IN MODEL LEARNING . ( MST : MINIMUM SPANNING TREE )
Table II
Description number of missing words in WikiLinks ( ie k − |M| ) number of connected components in gM number of nodes in largest component of gM maximum node degree in gM average weight of MST ( ie WM ST /|M| ) ratio of connector to original nodes in gS ( ie |C|/|M| ) maximum original node degree in gS maximum connector node degree in gS average degree of nodes in gS density of gS ( ie |ES|/(|M ∪ C|(|M ∪ C| − 1) ) )
WikiLinks Feature PROJ : Topic projection graph ( gM ) features ( 4 ) gM N umM iss gM N umConnComp gM SizeM axComp gM M axDeg D SPAN : ( Directed ) Topic spanning graph ( gS ) features ( 6 ) gSAvgM ST W eight gSRatioC gSM axDegreeM gSM axDegreeC gSAvgDegree gSDensity D SP : ( Directed ) Shortest path features among topic word pairs ( 9 ) N umN oP ath AvgSP Len M axSP Len N umSP 1 N umSP 2 N umSP 3 N umSP 4 N umSP 5 N umSP 6+ number of pairs with no directed path inbetween average pairwise directed shortest path length maximum pairwise directed shortest path length number of pairwise directed paths of length 1 number of pairwise directed paths of length 2 number of pairwise directed paths of length 3 number of pairwise directed paths of length 4 number of pairwise directed paths of length 5 number of pairwise directed paths of length ≥ 6 found as ρ = .73 for NEWS and ρ = .78 for BOOKS . We also used Cohen ’s kappa , which provides a measure of the degree to which two judges concur in their respective sortings of items into mutually exclusive categories . As our categories are ordinal , where a human rating of 1 is better than 2 which in turn is better than 3 , we used a weighted version of the statistic . The average pairwise Cohen ’s kappa is found as κ = .64 for NEWS ( max κ = .79 ) , and κ = .69 for BOOKS ( max κ = 85 ) Randomization tests yielded κ = 0 as expected . While there is no precise rule for interpreting kappa scores , [ 13 ] suggests that scores in the range ( .60 , .80 ] correspond to “ substantial agreement ” between the annotators .
For the prediction of the relative quality of topics ( P2 ) , we used the publicly available PRESS5 and BRAIN6 corpora and learned topic models with T = 100 and T = 200 topics , respectively . We considered the top 10 words for each topic to be in the positive ( good ) class . For the negative class , we built three case libraries with words of ranks [ 11 20 ] , [ 3140 ] , and [ 91 100 ] . This way we constructed three different learning tasks each with 200 and 400 training examples for PRESS and BRAIN , respectively . Examples of training topics from PRESS ( top few words ) are given below ( [top 1 10 ] vs . [ 91 100] ) :
5http://wwwcsprincetonedu/∼blei/lda c/ 6https://codegooglecom/p/topic modeling tool/downloads/list
SteamPistonEnginePumpPressureValveBoilerCylinderAirPipePointed Pointed DATASETS USED IN OUR EXPERIMENTS . D : NUMBER OF DOCUMENTS IN THE CORPUS , T : NUMBER OF TOPICS , Labels : WHETHER HUMAN
Table III
ANNOTATIONS EXIST OR NOT .
Dataset BOOKS NEWS PRESS BRAIN
D 12 , 000 55 , 000 2 , 246 10 , 000
T 120 117 100 200
Labels
Yes Yes No No
Description Books downloaded from the Internet Archive NYTimes news articles from LDC Gigaword Documents from the Associated Press Pubmed abstracts for the query “ brain injury ”
+ space soviet shuttle nasa launch mission earth venus − jupiter day help report released days data laboratory + research scientists researchers animals project state − defense usda caused two temperatures side agricultural + power cars heat oil fuel energy electricity day − account total carbon year just united lower i plan
F . Learning to Predict
After constructing our topic libraries with labeled examples ( §III E ) and extracting their graph centric features ( §III D ) , we train logistic regression classifiers with L1 norm regularization . More specifically , we are given n training examples ( n topics ) {(x(i ) , y(i ) , i = 1 , . . . , n} , where each x(i ) ∈ Rm is an m dimensional feature vector , and y(i ) ∈ {1 , 0} denotes the class label ( 1 : positive ( good ) vs . 0 : negative ( poor) ) . Logistic regression classifier models the probability distribution of the class label y given a feature vector x as p(y = 0|x ; w ) = σ(wT x ) = 1+exp(−wT x ) , where w ∈ Rm are the parameters of the model ( feature weights ) , and σ( . ) is the sigmoid function .
1 the Laplace prior of
We regularize the logistic regression model using L1 norm , which corresponds to Bayesian learning under the parameters ; p(w ) = ( λ/2)mexp(−λw1 ) , with λ > 0 . The maximum n a posteriori estimate of the parameters can be obtained by solving the ( convex ) optimization problem : i=1 − log p(y(i)|x(i ) ; w ) + λw1 . As such , the arg minw Laplace prior “ pushes ” the weights towards zero , and biases the solution to be sparse . This helps us with feature selection . In order to solve for the model parameters w , and hence the feature “ weights ” , we employ efficient algorithms [ 14 ] where we choose the hyperparameter λ using crossvalidation . We report the leave one out cross validation accuracies , which provide a good approximation to the true accuracy of our models . The results of our experiments are discussed in the next section .
IV . EXPERIMENT RESULTS
We start with discussing the performance results on the relative quality prediction problem . Later , we introduce two baseline techniques and proceed with human perceived topic quality prediction results .
A . Relative Quality Prediction
As given in §III A , the relative quality prediction problem ( P2 ) deals with differentiating the top ranked words of a given topic from its non top ranked words . The goal of this set of experiments is to understand the value of using the graph centric evaluation framework we developed . Achieving promising performance on this pilot study would show us the feasibility of our approach .
In Table IV , we present the prediction accuracy of our model on the PRESS and BRAIN topics . The results are listed for our three different relative classification tasks ( §III E ) and for our various groups of features ( §III D ) . From the tables , we observe that using our graph centric features we achieve improved classification performance in all cases , and when features are used collectively we obtain 15 % to 30 % boost over the random baseline . As expected , the boost is gradually higher for the easier tasks ( from left to right ) where the negative class words are chosen further down in the rank order of topic words . These preliminary results show that WikiLinks is useful as an external resource and that our method is suitable for topic quality prediction tasks .
B . Building baselines
Before we move on to the results , we introduce two nontrivial baselines that we developed and compared to our approach , which are much smarter than the simple majorityclass baseline .
1 ) Google baseline : : Given a set of k topic words , we used several Google operators7 to query for results containing these words . In particular , from different types of Google queries , we built four what we call “ Google features ” per topic . Each Google feature is the logarithm of the number of webpages returned for its corresponding query , or in other words log(hitcount(query) ) . the topic words in their
Table V gives a summary of the Google features we constructed . First , we queried for all the webpages that contain all text ; by using the allintext:word1 , word2 , . . . , wordk opthe pages that contain erator . Second , we queried for at title ; using intitle:word1 OR . . . OR intitle:wordk . Similar to the latter , we also queried for pages by their anchor the query words in their least one of
7http://wwwgoogleguidecom/advanced operators.html
PRESS|BRAIN RELATIVE QUALITY PREDICTION RESULTS . CLASSIFICATION ACCURACIES FOR PREDICTING RELATIVE ( TOP k VERSUS NON TOP k )
Table IV
TOPIC QUALITY , FOR VARIOUS GROUPS OF FEATURES . top [31 40 ] top [11 20 ] top [91 100 ]
PRESS BRAIN 0.500 0.500 0.725 0.765 0.762 0.805 0.790 0.750 0.790 0.777 0.800 0.815 0.810 0.807
Feature set top 10 vs .
BASELINE MAJORITY PROJ D SPAN D SP PROJ+D SPAN PROJ+D SP PROJ+D SPAN+D SP
PRESS BRAIN 0.500 0.500 0.622 0.505 0.687 0.650 0.665 0.605 0.650 0.687 0.672 0.650 0.660 0.687
PRESS BRAIN 0.500 0.500 0.705 0.715 0.740 0.760 0.760 0.710 0.745 0.722 0.752 0.710 0.735 0.752 or URL containment using inanchor:word1 OR . . . OR inanchor:wordk as well as using inurl:word1 OR . . . OR inurl:wordk to obtain the third and fourth features . As such , we represent each topic with four numerical features , and learn classification models based on those features .
Google features also rely on an external resource ; the Google search engine . Unlike WikiLinks features , however , they do not exploit graph centric properties of any projection or spanning graphs . We compare to this Google baseline to understand the amount of benefits gained by using the WikiLinks graph .
2 ) PPR baseline : : A second baseline classifier we built uses features based on the graph proximities among the topic words . To measure the proximity of a given pair of words on the WikiLinks graph , we used the personalized PageRank ( PPR ) scores [ 15 ] . Intuitively , the PPR score of a node v with respect to a given node u is high if there exist many , short paths between these two nodes . We constructed four PPR features capturing the pairwise graph proximity between the topic words ( excluding the self pairs ) as given in Table VI .
Table VI
PPR FEATURES GENERATED TO BUILD A BASELINE CLASSIFIER . PPR Feature AvgP P Rscore M edP P Rscore AvgP P Rorder M edP P Rorder
Description average pairwise PPR score median pairwise PPR score average pairwise PPR order median pairwise PPR order
PPR based features also exploit the underlying WikiLinks graph structure , and they are known as being more robust than shortest paths in capturing graph centric proximities . As such , they build a strong baseline classifier . However , PPR computations are expensive as they rely on the mixing of random walks with restarts on the input graph ( in tens of millions of nodes/edges ) . On the other hand , computing our graph features is fast since projected graphs are fairly small , and finding the shortest paths takes only a few seconds as often times the mapped nodes are close by and thus most of the graph need not be traversed . Therefore we compare to PPR as a strong but expensive baseline , to understand its relative benefits compared to our method .
C . Absolute ( Human Perceived ) Quality Prediction
As we motivated throughout the paper , the absolute quality prediction problem ( P1 in §III A ) deals with differentiating the good quality topics from poor ones as perceived by human judges . We present our main results in Table VII .
We observe that all subsets of our feature groups outperform all three baselines . In particular , the Google baseline introduces 3 10 % improvement in accuracy over the majority class baseline , and the PPRbaseline based on the WikiLinks graph structure yields up to 23 % increase . While these demonstrate the value of WikiLinks for this task , PPRbaseline is costly as we discussed earlier . On the other hand , all our graph centric features introduce at least 25 % and up to 30 % boost over the majority baseline . In fact even the simplest group of our features PROJ , based on the immediate induced subgraph of topic words on the WikiLinks , outperforms the baselines alone .
We note that combined features do not always yield the best accuracy . We attribute this to the fact that learning with more features increases the size and complexity of our model space . With the same amount of data to learn from and a larger search space , our learning algorithm is less likely to find a good model , where having sufficiently large training data would mitigate this issue . Cross domain classification . In order to understand the generalization power of our framework , we also studied its cross domain classification performance . Specifically , we learned a classification model using the BOOKS dataset and tested it on the NEWS dataset , similarly we also trained on NEWS and treated BOOKS as our test data . We show our results in Table VIII . The diagonal entries give the leave out out cross validation accuracies within the same domain as before ( last row in Table VII ) . The cross domain accuracies are given on the off diagonal entries . We observe that the cross domain accuracies are fairly comparable to those of within domain . This generalization power is partic
GOOGLE FEATURES GENERATED TO BUILD A BASELINE CLASSIFICATION MODEL .
Table V
Google Feature Operator : allintext:word1 , word2 , . . . , wordk intitle:word1 OR . . . OR intitle:wordk inanchor:word1 OR . . . OR inanchor:wordk inurl:word1 OR . . . OR inurl:wordk
Description Log count of webpages that contain : all the topic words in their text at least one topic word in their title at least one topic word in their anchor at least one topic word in their URL
Table VII
BOOKS AND NEWS ABSOLUTE QUALITY PREDICTION RESULTS .
ACCURACIES FOR PREDICTING ABSOLUTE ( HUMAN PERCEIVED ) TOPIC
QUALITY , FOR VARIOUS GROUPS OF FEATURES .
Feature set BASELINE MAJORITY BASELINE GOOGLE BASELINE PPR PROJ D SPAN D SP PROJ+D SPAN PROJ+D SP PROJ+D SPAN+D SP
BOOKS NEWS
0.610 0.642 0.842 0.875 0.892 0.883 0.883 0.892 0.900
0.521 0.624 0.735 0.812 0.769 0.786 0.795 0.795 0.821
BOOKS +NEWS 0.549 0.629 0.785 0.848 0.844 0.852 0.844 0.848 0.831
CROSS DOMAIN ABSOLUTE QUALITY PREDICTION RESULTS .
Table VIII
PPPPPPPP
Test
Train BOOKS NEWS
BOOKS
0.900 0.867
NEWS
0.769 0.821 ularly driven by our graph centric features that are domainindependent . Analysis of the prediction models . Finally , we study the characteristics of our learned models . As we use Lassoregularization in our model training which lends itself to feature selection , we analyze the selected features ( ie those with non zero coefficients ) for BOOKS and NEWS , as given in Table IX . We notice that the two models coincide in the majority of their selected features which hints towards the consistent evidential power of those features . One traditional way of interpreting the coefficients is to think in terms of the log odds ratio , log P ( y=0 ) P ( y=1 ) = wT x . Here , an increase of one unit in a particular feature i ( under the same conditions for the others ) contributes to the log odds by wi . Therefore features with positive coefficients contribute to the odds that a given topic is poor ( ie y = 0 ) , whereas features with negative coefficients advocate for the topic being good . More specifically , we deduce that good topics are those with fewer missing mapped words onto WikiLinks ( or larger M ) , fewer connector nodes C in their spanning graphs gS , and higher degree nodes in their projection graphs .
V . RELATED WORK
Topic modeling has been a widely studied topic of interest especially for the machine learning ( ML ) [ 1 ] , [ 16 ] ( LDA , random projections ) , information retrieval ( IR ) [ 2 ] , [ 3 ] ( LSI , pLSA ) , as well as cognitive science [ 17 ] communities . Simply put , topic models describe the documents of a corpus as a mixture of topics , which in turn consist of a mixture of topic words . Typically , only a small number of words are important in each topic , and only a small number of topics are present in each document . As such , topics provide lowdimensional representation for document collections [ 16 ] and drive many applications including document database summarization [ 4 ] , segmentation [ 18 ] , ontology learning [ 19 ] , word sense disambiguation [ 5 ] , information discovery [ 6 ] , to name a few .
Evaluation of topic models is an important issue , as unsupervised nature of the learning process makes model selection hard . Within the last 4 5 years8 , the natural language processing ( NLP ) community has shown increasing interest into the semantic coherence or in other words evaluation of topic models in capturing the human perceived quality of topics . Accurately identifying and getting rid of lowquality topics not only would improve the understanding and interpretation of the semantic nature of topics , but it would also help boost the performance of many applications as listed above , eg better topic based document similarity .
In this section we give a survey of topic model evaluation , and present related works in chronological order . Most works in quantitative evaluation of topic models [ 7 ] employ a variety of measures of model fit , such as estimating the likelihood of held out documents or measuring the performance of an external task that is independent of the topic space such as information retrieval .
Drawbacks of model fit measures : While useful , these methods ignore the evaluation of the interpretability and semantic meaning of the topics for users . In fact , quite surprisingly , [ 8 ] showed that “ traditional measures negatively correlated with the measures of topic quality ” and that “ models are often trading improved likelihood for lower interpretability ” .
Later , [ 10 ] proposed a new measure called pairwise information ( PMI ) of topic words based on cotext mutual occurrence statistics of word pairs in large external
8The first works on topic quality evaluation dates back to 2009 [ 8 ] , [ 10 ] .
SELECTED FEATURES AND LEARNED COEFFICIENTS OF OUR L1 REGULARIZED LOGISTIC REGRESSION MODEL FOR BOOKS AND NEWS . NEGATIVE
( POSITIVE ) COEFFICIENTS CONTRIBUTE TO THE ODDS OF A GIVEN TOPIC TO BE GOOD ( POOR ) QUALITY .
Table IX
BOOKS
Selected Feature gM N umM iss gSRatioC gM M axDeg gM SizeM axComp N umSP 2
Coefficient
0.0626 0.2940 0.2921 0.8667 0.9685
NEWS
Selected Feature gM N umM iss gSRatioC gM M axDeg gSAvgM ST W eight
Coefficient
0.0918 0.5909 0.4541 0.2598 corpora , and showed that PMI scores of topics are highly correlated ( according to Pearson ’s correlation statistic ) with human scores . [ 11 ] showed that PMI outperforms a range of other topic scoring measures such as those based on lexical similarity and similarity in a given ontology . In [ 20 ] , the PMI model is extensively evaluated on various different genres and domains of corpora ( news , books , National Institutes of Health ( NIH ) abstracts ) and various external corpora ( Wikipedia articles , Google 5 grams , pubmed.gov abstracts ) . Drawbacks of PMI based evaluation : First , it requires the entire scan of external documents to compute the cooccurrence count for every pair of topic words which is quite costly as the external corpora may be quite large ( eg , 2 million Wikipedia articles , 1 trillion Google 5 grams ) . Second , the best correlation to human perceived quality depends on the type of external corpora used ( according to [ 20 ] , Google for books , Wikipedia for news , and pubmed.gov for NIH abstracts yield the best correlation ) . This makes it challenging to identify relevant , and burdensome to keep numerous corpora .
Rather than using external corpora , [ 9 ] proposed to use the original ( ie training ) corpus itself , which has been used for topic extraction , to compute a PMI like score based on cooccurrence statistics of topic words in the original document collection . Experiments on NIH document collection proved to be effective in separating low and high quality topics judged by domain experts . This is interesting , as the reasons behind not using the training corpus was stated in [ 10 ] as “ instead of using the collection itself to measure word association , we use a large external text data source to provide regularization ” . This , of course , comes with the same challenges as for PMI . Recently [ 21 ] used cohesion and specificity of the topics to define a conceptual topic relevance score based on a concept hierarchy ( ie an ontology ) . Main drawback of existing methods : Relevance based [ 21 ] and PMI like measures [ 10 ] , [ 9 ] as well as others compared to in [ 20 ] are all based on a single statistic . None of the methods exploit a collection of evidential measures to build a ( learning ) model that could potentially perform better than its parts . This is exactly the approach we take in this work .
Finally , while not directly applicable to evaluation , related work include automatic topic labeling [ 22 ] , [ 23 ] , [ 24 ] where the goal is to find a single most representative phrase ( ie topic label or name ) for each topic . Most related work in data mining that has inspired our work is [ 25 ] , which used graph mining for evaluating the quality of search engine results to user queries . Other ( although not directly ) related graph based techniques include connection subgraphs , with a goal of summarizing a subset of nodes [ 26 ] , [ 27 ] , [ 28 ] .
VI . CONCLUSION AND RESEARCH DIRECTIONS
In this paper we introduced a novel graph mining approach for the external evaluation of topic models . We proposed to use Wikipedia as an external resource , constructed graph centric features based on its page links graph structure , and built classification models that can predict human perceived quality of topics based on those evidential features . We summarize our contributions as follows .
• Novel evaluation framework : We develop a new topic quality evaluation framework that classifies a given topic as good or poor . It creates subgraphs of the topic words based on Wikipedia , and use their graph centric properties to learn classification models .
• Wikipedia as a knowledge base : Wikipedia page links graph consists of articles about entities , which are linked by their relatedness , as perceived by human editors . Thus we hypothesized that good , ie semantically coherent , topics’ words would lie in close proximity in this graph . We validated this hypothesis with experiments that show significant improvements in prediction performance when WikiLinks is exploited .
• Evidential graph centric features : We constructed novel features based on graph topology and closeness based on WikiLinks , in particular we introduced the projection and spanning subgraphs and their related features .
• Prediction models and experiments : Based on a carefully built list of features , we learned statistical classification models . Experiments highlighted the potential value of employing contextual subgraphs for understanding the quality of topics . One key aspect of our framework is its generality ; it can be used with real world corpora from diverse domains ( eg , news , books , and medicine ) , thanks to the immense coverage of Wikipedia as a knowledge base and domainindependent nature of our features .
Future work will look at graph centric features describing the position of each mapped node in regard to other mapped nodes as well as to the rest of the graph , to identify specific topic words that are potentially out of context and outlying9 . Another research direction is to exploit the WikiLinks graph structure to quantify the similarity of two or more topics by their positioning of words in the graph .
We believe that the presented work reveals an example where graph and data mining has potential impact to problems in related fields . We find that the proposed methods achieve desirable performance and provoke interesting directions for future research .
ACKNOWLEDGMENTS
We thank the anonymous reviewers for their feedback . This material is based upon work supported by the Stony Brook University Office of the Vice President for Research and the National Science Foundation Graduate Research Fellowship . Any findings and conclusions expressed in this material are those of the author(s ) and do not necessarily reflect the views of the funding parties .
REFERENCES
[ 1 ] D . M . Blei , A . Y . Ng , and M . I . Jordan , “ Latent dirichlet allocation , ” Journal of Machine Learning Research , vol . 3 , pp . 993–1022 , 2003 .
[ 2 ] S . Deerwester , S . Dumais , G . Furnas , T . Landauer , and R . Harshman , “ Indexing by latent semantic analysis , ” J . of American Soc . for Info . Sci . , vol . 41 , no . 6 , pp . 391–407 , 1990 .
[ 3 ] T . Hofmann , “ Probabilistic latent semantic indexing , ” in SIGIR , 1999 , pp . 50–57 .
[ 4 ] A . Haghighi and L . Vanderwende , “ Exploring content models for multi document summarization . ” in NAACL , 2009 , pp . 362–370 .
[ 5 ] S . Brody and M . Lapata , “ Bayesian word sense induc tion . ” in EACL , 2009 .
[ 6 ] M . Steyvers , P . Smyth , M . Rosen Zvi , and T . L . Griffiths , “ Probabilistic author topic models for information discovery . ” in KDD , 2004 , pp . 306–315 .
[ 7 ] H . M . Wallach , I . Murray , R . Salakhutdinov , and D . M . Mimno , “ Evaluation methods for topic models . ” in ICML , vol . 382 . ACM , 2009 , p . 139 .
[ 8 ] J . Chang , J . L . Boyd Graber , S . Gerrish , C . Wang , and D . M . Blei , “ Reading tea leaves : How humans interpret topic models . ” in NIPS , 2009 , pp . 288–296 .
[ 9 ] D . M . Mimno , H . M . Wallach , E . M . Talley , M . Leenders , and A . McCallum , “ Optimizing semantic coherence in topic models . ” in EMNLP , 2011 , pp . 262–272 . [ 10 ] D . Newman , S . Karimi , and L . Cavedon , “ External evaluation of topic models , ” in Australasian Document Computing Symposium , 2009 , pp . 11–18 .
9Topic outlier words would be similar to the intrusive words in [ 8 ] .
[ 11 ] D . Newman , J . H . Lau , K . Grieser , and T . Baldwin , “ Automatic evaluation of topic coherence , ” in ACL , 2010 , pp . 100–108 .
[ 12 ] R . M . Karp , “ Reducibility among combinatorial problems . ” in Complexity of Computer Computations , 1972 , pp . 85–103 .
[ 13 ] J . R . Landis and G . G . Koch , “ The Measurement of Observer Agreement for Categorical Data , ” Biometrics , vol . 33 , no . 1 , pp . 159–174 , 1977 .
[ 14 ] S I Lee , H . Lee , P . Abbeel , and A . Y . Ng , “ Efficient L1 regularized logistic regression . ” in AAAI , 2006 , pp . 401–408 .
[ 15 ] T . H . Haveliwala , “ Topic sensitive pagerank , ” in WWW ,
2002 , pp . 517–526 .
[ 16 ] E . Bingham and H . Mannila , “ Random projection in dimensionality reduction : applications to image and text data . ” in KDD , 2001 , pp . 245–250 .
[ 17 ] T . L . Griffiths , M . Steyvers , and J . Tenenbaum , “ Topics in semantic representation , ” Psychological Review , 2007 .
[ 18 ] T . Brants , F . Chen , and I . Tsochantaridis , “ Topicbased document segmentation with probabilistic latent semantic analysis , ” in CIKM , 2002 , pp . 211–218 .
[ 19 ] W . Wang , P . M . Barnaghi , and A . Bargiela , “ Probabilistic topic models for learning terminological ontologies . ” IEEE Trans . Knowl . Data Eng . , vol . 22 , no . 7 , pp . 1028–1040 , 2010 .
[ 20 ] D . Newman , Y . Noh , E . M . Talley , S . Karimi , and T . Baldwin , “ Evaluating topic models for digital libraries . ” in JCDL , 2010 , pp . 215–224 .
[ 21 ] C . C . Musat , J . Velcin , S . Trausan Matu , and M A Rizoiu , “ Improving topic evaluation using conceptual knowledge . ” in IJCAI , 2011 , pp . 1866–1871 .
[ 22 ] Q . Mei , X . Shen , and C . Zhai , “ Automatic labeling of multinomial topic models . ” in KDD , 2007 , pp . 490– 499 .
[ 23 ] J . H . Lau , K . Grieser , D . Newman , and T . Baldwin , “ Automatic labelling of topic models . ” in ACL , 2011 , pp . 1536–1545 .
[ 24 ] I . Hulpus , C . Hayes , M . Karnstedt , and D . Greene , “ Unsupervised graph based topic labelling using dbpedia . ” in WSDM , 2013 , pp . 465–474 .
[ 25 ] J . Leskovec , S . T . Dumais , and E . Horvitz , “ Web projections : learning from contextual subgraphs of the web . ” in WWW . ACM , 2007 , pp . 471–480 .
[ 26 ] L . Akoglu , J . Vreeken , H . Tong , D . H . Chau , N . Tatti , and C . Faloutsos , “ Mining connection pathways for marked nodes in large graphs , ” in SIAM SDM , 2013 . [ 27 ] C . Faloutsos , K . S . McCurley , and A . Tomkins , “ Fast discovery of connection subgraphs , ” in KDD , 2004 , pp . 118–127 .
[ 28 ] H . Tong and C . Faloutsos , “ Center piece subgraphs : problem definition and fast solutions , ” in KDD , 2006 , pp . 404–413 .
