Markov Blanket Feature Selection with Non Faithful Data Distributions
Kui Yu1 , Xindong Wu1,2 , Zan Zhang1 , Yang Mu3 , Hao Wang1 , and Wei Ding3
2Department of Computer Science
University of Vermont
Burlington , VT 05405 , USA
3Department of Computer Science
University of Massachusetts Boston , MA 02125 , USA
1Department of Computer Science Hefei University of Technology
Hefei , 23009 , China ykui713@gmail.com ; xwu@csuvmedu ; zhangzan99@163.com ; jsjxwangh@hfuteducn ; {ding,yangmu}@csumbedu faithful Bayesian networks ,
Abstract—In the Markov blanket of the class attribute is a unique and minimal feature subset for optimal feature selection . However , little attention has been paid to Markov blanket feature selection in a non faithful environment which widely exists in the real world . To tackle this issue , in this paper , we deal with nonfaithful data distributions and propose the concept of representative sets instead of Markov blankets . With a standard sparse group lasso for selection of features from the representative sets , we design an effective algorithm , SRS , via Representative Sets with non faithful data distributions . Empirical studies demonstrate that SRS outperforms the state of the art Markov blanket feature selectors and other well established feature selection methods . for Markov blanket feature Selection
Keywords Feature selection ; Markov blankets ; Faithful
Bayesian networks ; Representative sets ; Sparse group lasso
I .
INTRODUCTION introduced by Pearl [ 13 ] , and
Markov blankets in Bayesian networks were first in faithful Bayesian networks , for every node X , its Markov blanket is the set X ) as shown in Figure 1 [ 6 ] . Koller and Sahami [ 7 ] first of parents , children and spouses ( parents of the children of introduced Markov blankets for feature selection defined by feature relevance . In feature relevance with respect to the class attribute , an input feature can be classified into a strongly relevant , irrelevant , redundant , or non redundant feature , and a Markov blanket should include strongly relevant and non redundant features [ 22 ] . However , using feature relevance to exactly determine Markov blankets is very difficult because of a limited sample size and noise in the data [ 7 ] .
To tackle this issue , Tsamardinos and Aliferis [ 19 ] provided theoretical results that link the concepts of feature relevance in feature selection and Markov blankets in Bayesian networks . Their theoretical results proved that if a probability distribution can be faithfully represented by a Bayesian network , then the Markov blanket of the class attribute in the Bayesian network is not only unique but also the solution to feature selection . With those theoretical results , Markov blanket feature selection has attracted much attention in recent years [ 1 , 14 , 16 , 24 ] . Tsamardinos and Aliferis [ 19 ] proposed IAMB that returns the Markov blanket of any target node in a faithful Bayesian network without learning a complete Bayesian network , even with hundreds of thousands of features . However , it requires a sample size exponential in the size of a Markov blanket . More recent variations of IAMB include PCMB ( Parent Children Markov Blanket ) [ 14 ] , MMMB ( Max Min Markov Blanket ) [ 1 , 20 ] , and HITONMB [ 1 ] , proposed to conquer the data inefficiency problem of IAMB . Moreover , Aliferis et al . [ 1 ] demonstrated that Markov blanket feature selection outperforms most of the state of the art feature selection algorithms .
Figure 1 . The Markov blanket ( in blue ) of Lung Cancer
Meanwhile , the previous studies mentioned above typically assume that a data distribution and an underlying Bayesian network which models that domain are faithful to each other . This assumption relies on an important theoretical result that if a joint probability distribution ( cid:1334 ) distribution ( cid:1334 ) that is faithful to an underlying Bayesian satisfies the intersection property ( see Section III ) , then it is guaranteed to have a unique Markov blanket of the target feature/variable [ 13 ] . Moreover , the probability network also satisfies the intersection property [ 13 14 ] . Thus , in a faithful Bayesian network , it is guaranteed to have a unique Markov blanket of any target node [ 12 , 19 ] . However , in some real life distributions , Markov blankets of a target feature are not unique and may vary in size due to various factors , such as ( but not limited to ) small sample size , noise in data , hidden variables , and data pre processing [ 14 , 17 ] . This makes real data in many cases violate the intersection property or faithfulness condition . Thus , it is strict yet difficult to handle data in faithfulness conditions , and dealing with real data with non faithful distributions is evidently more meaningful .
For example , Figure 2 gives an example to illustrate the multiple Markov blanket problem in real data of the arcene ( cancer benchmark ) dataset ( with 100 instances and 10000 features ) from the NIPS 2003 feature selection challenge . In Figure 2 , IAMB , HITON MB , PCMB and MMMB can only discover a single Markov blanket . Different from those four algorithms , KIAMB [ 14 ] can find multiple Markov blankets , by employing a stochastic search heuristic that repeatedly disrupts the order in which features are selected for inclusion into a Markov blanket with the probability p , thereby introducing a chance of identifying alternative Markov blankets of a target feature . We run KIAMB 100 times to attain 100 Markov blankets ( the parameter p is set 0.6 ) , respectively . By using Decision Tree J48 and Knn classifiers , Figure 2 gives a summary of the prediction accuracies of those 100 Markov blankets and those of the Markov blankets selected by IAMB , HITON MB , PCMB and MMMB .
Figure 2 . Prediction accuracy on the arcene dataset
From Figure 2 , we can see that the Markov blankets identified by the existing Markov blanket algorithms are not the optimal feature subsets for feature selection , compared to the Markov blankets discovered by KIAMB . Therefore , Markov blanket feature selection in nonfaithful data distributions needs further attention . However , when we deal with non faithful data distributions , the challenges are three fold as follows .
Firstly , if the real data distributions contain multiple Markov blankets , we don’t know the exact number of Markov blankets of any target feature in real data .
Secondly , even if we know all Markov blankets of a target feature , it is very expensive , or even impossible to discover all of them since the number of Markov blankets can grow exponentially in the number of features in the underlying Bayesian network .
And thirdly , with multiple Markov blankets , the Markov blankets discovered by the existing Markov blanket feature selection algorithms might not be the optimal solutions to feature selection , and thus , how can we efficiently find a best Markov blanket as the solution to feature selection from a large and even exponential number of Markov blankets in a non faithful distribution ? To address those problems , the main contributions of the paper are as follows .
( 1 ) We propose the concept of a representative set instead of a Markov blanket to focus on a feature space of all possible Markov blankets , instead of an exhaustive search over an unknown and even exponential number of Markov blankets in real data .
( 2 ) To define representative sets , we extend the theoretical results provided by Tsamardinos and Aliferis [ 19 ] about Markov blankets and feature relevance . Our theoretical analysis and empirical study both show that in Markov blankets only parents and children correspond to strongly relevant features in feature relevance . With this theoretical result , a representative set is defined as parents or children in a Markov blanket and their corresponding correlated features .
( 3 ) With representative sets , we employ the standard sparse group lasso approach , and design an effective algorithm , SRS , for Markov blanket feature Selection via Representative Sets in non faithful distributions . Empirical results on high dimensional datasets show that the SRS algorithm outperforms state ofthe art Markov blanket feature selectors and other wellestablished feature selection methods . to process data
II . RELATED WORK
Feature selection aims to reduce the computational complexity without performance degradation by removing irrelevant and redundant features . The major effort is to maximize relevance and minimize redundancy among the selected features for classification . For instance , the mRMR ( Minimum Redundancy Maximum Relevance ) algorithm proposed by [ 15 ] while the FCBF ( Fast Correlation Based Filter ) algorithm proposed by [ 22 ] . Recently , Cheng et al . [ 3 ] presented a Fisher Markov filter method to identify a maximally separable feature subset using the Fisher ’s discriminant analysis and the Markov random fields ( MRFs ) . Brown et al . [ 2 ] proposed a unifying framework to bring almost two decades of research on heuristic scoring criteria through a novel interpretation of information theoretic feature selection as an optimization of the conditional likelihood . Zhao et al . [ 26 ] proposed a framework to unify different criteria for handling feature redundancies .
Markov blanket feature selection as an emerging successful class of filter methods presents a solution of the feature selection problem by discovery of a Markov blanket of the class attribute [ 1 ] , and thus it has attracted much attention [ 14 , 23 ] .
Margaritis and Thrun invented the first yet sound Markov blanket discovery algorithm , the GS algorithm with the intent to discover the Markov blanket for the purpose of speeding up global Bayesian network learning [ 11 ] . But the GS algorithm requires the number of instances exponentially to the size of the Markov blanket , and this makes it impractical for many real datasets .
To conquer this drawback of the GS algorithm and apply the concept of Markov blanket to the feature selection task , Tsamardinos and Aliferis [ 19 ] provided theoretical results that link feature relevance as defined by Kohavi and John [ 8 ] and the Markov blanket in faithful Bayesian networks . And then , they proposed a modified version of the GS algorithm , called the IAMB algorithm for feature selection , which guarantees to find the actual Markov blanket given enough training data and is more sample efficient than GS [ 19 ] . However , the IAMB algorithm still requires a sample size exponential in the size of a Markov blanket . Thus , HITON MB and MMMB were introduced without requiring samples exponential in the size of the Markov blanket . Different from GS and IAMB , HITON MB and MMMB take two steps to find the Markov blanket of a target node : ( 1 ) discovering the parents and children of the target node ; and then ( 2 ) identifying its spouses based on Step 1 . As an efficient implementation of Step 1 , two major algorithms HITONPC and MMPC were introduced [ 1 , 20 ] . Following the idea of MMMB , PCMB was also proposed to conquer the data inefficiency problem of IAMB [ 14 ] .
However , the algorithms mentioned above are wellestablished only for selection of a single Markov blanket problem by handling data in faithful data distributions , and little research has been done in the development of algorithms for dealing with Markov blanket feature selection problem with non faithful data distributions .
A naïve approach for handling Markov blanket feature selection in non faithful data distributions involves first clustering all features into multiple clusters , and then randomly sampling a representative from each cluster . But this strategy is intractable since the computation is intensive for high feature dimensions , and features in each cluster don’t indicate they are correlated in terms of feature relevance [ 21 ] . Peña et al . [ 14 ] proposed a stochastic Markov blanket algorithm , called KIAMB , that involves running multiple times initialized with a random seed . Recently , among the most notable advances in the field is that Statnikov et al . [ 17 ] proposed the TIE* ( Target Information Equivalence ) algorithm that can discover all Markov blankets in a non faithful data distribution . But TIE* is very expensive or prohibitive when the number of Markov blankets grows exponentially in the number of features in the network . let F(cid:4668)F,F(cid:2870),…,F(cid:2924)(cid:4669 ) and F(cid:4668)F(cid:2919)(cid:4669 ) represent the feature subset excluding F(cid:2919 ) . distinct features F(cid:2919)(cid:1488)F and F(cid:2921)(cid:1488)F are conditionally independent on a subset S(cid:1603)F(cid:4668)F(cid:2919)(cid:1515)F(cid:2921)(cid:4669)(Ind(cid:4666)F(cid:2919),F(cid:2920)|(cid:4667 ) iff P(cid:4666)F(cid:2919)|F(cid:2921),S(cid:4667)P(cid:4666)F(cid:2919)|S(cid:4667 ) or P(cid:4666)F(cid:2921)|F(cid:2919),S(cid:4667 ) P(cid:4666)F(cid:2921)|S(cid:4667 ) . Definition 2 ( Bayesian Networks ) [ 13 ] Let ( cid:1334 ) be a represent a full set of features , C denote the class attribute ,
( Conditional Independence ) Two
III . NOTATIONS AND DEFINITIONS the following sections ,
A . Bayesian networks
Definition 1 for short ) , discrete joint probability distribution of a set of random
In of X . over the feature set F satisfies the intersection property ,
Theorem 1 [ 13 ] Let X , Y , Z , and W be any four subsets of features from F and a joint probability independent of any subset of its non descendant nodes conditioned on its parents . ( A simple Bayesian network of Lung Cancer has been shown in Figure 1 . ) nodes ( features ) F via a directed acyclic graph ( cid:2419 ) . We call the triplet ( cid:1832),(cid:2419),(cid:1334)(cid:3408 ) a ( discrete ) Bayesian network if ( cid:1832),(cid:2419),(cid:1334)(cid:3408 ) satisfies the Markov condition : every node is distribution ( cid:1334 ) is strictly positive . Then the following intersection property holds in ( cid:1334 ) over the feature set F : Ind(cid:4666)X,Y|Z(cid:1666)W(cid:4667 ) and Ind(cid:4666)X,W|Z(cid:1666)Y(cid:4667)(cid:1436 ) Ind(cid:4666)X,(cid:4666)Y(cid:1666)W(cid:4667)|Z(cid:4667 ) . Theorem 2 [ 13 ] If a joint probability distribution ( cid:1334 ) then for each X(cid:1488)F , there exists a unique Markov blanket graph ( cid:2419 ) is also present in the joint probability ( cid:1334 ) . Theorem 3 [ 13 ] If ( cid:1334 ) is faithful to ( cid:2419 ) , then ( cid:1334 ) satisfies Bayesian networks , for every node F(cid:2919 ) , its Markov blanket F(cid:2919 ) . Definition 5 ( Collider ) [ 13 ] A node F(cid:2919)(cid:1488)F of a path p is a collider if p contains two incoming edges into F(cid:2919 ) . Proposition 1 [ 13 ] A path p from node F(cid:2919)(cid:1488)F to node F(cid:2921)(cid:1488)F is blocked by a set of nodes S(cid:1599)F , if there is a node F(cid:2923)(cid:1488)F on p for which one of the following two conditions hold : ( a ) F(cid:2923 ) is not a collider and F(cid:2923)(cid:1488)S , or ( b ) F(cid:2923 ) is a collider and neither F(cid:2923 ) or its descendants are in S . Definition 6 ( D separation ) [ 13 ] Two nodes F(cid:2919)(cid:1488)F and F(cid:2921)(cid:1488)F are d separated by S(cid:1599)F in graph G if and only if every path from F(cid:2919 ) to F(cid:2921 ) is blocked by S .
Definition 3 ( Faithfulness ) [ 13 ] A Bayesian network satisfies the faithfulness condition if and only if every conditional independence entailed by the directed acyclic
With Theorems 2 and 3 , the concept of Markov blankets in faithful Bayesian networks is defined as follows . is unique with the set of parents , children and spouses of
Definition 4 ( MB : Markov Blanket ) [ 19 ] In faithful the intersection property .
In this section , we introduce the concepts of feature
Theorem 4 [ 19 ] In faithful Bayesian networks , dseparation captures all conditional dependence and independence relations that are encoded in the graph which implies that two nodes are d separated with each other given a subset S , they are conditionally independent conditioned on S . B . Feature relevance in Feature Selection
Definition 7 ( Strong Relevance ) A feature F(cid:2919 ) is strongly relevant to C iff ( cid:1482)S(cid:1603)F(cid:4668)F(cid:2919)(cid:4669 ) stP(cid:4666)C|S(cid:4667)(cid:3405)P(cid:4666)C|S ,F(cid:2919)(cid:4667 ) . Definition 8 ( Weak Relevance ) A feature F(cid:2919 ) is weakly relevant to C iff it is not strongly relevant , and ( cid:1484)S(cid:1599)F(cid:4668)F(cid:2919)(cid:4669 ) stP(cid:4666)C|S(cid:4667)(cid:3405)P(cid:4666)C|S,F(cid:2919)(cid:4667 ) Definition 9 ( Irrelevance ) A feature F(cid:2919 ) is irrelevant to C iff it is neither strongly nor weakly relevant , and relevance proposed by Kohavi and John [ 8 ] . iff
To
( cid:1482)S(cid:1603)F(cid:4668)F(cid:2919)(cid:4669 ) st P(cid:4666)C|S ,F(cid:2919)(cid:4667)P(cid:4666)C|S(cid:4667 ) Definition 10 ( Redundant Features ) Assuming S(cid:1603 ) F(cid:4668)F(cid:2919)(cid:4669 ) as the current feature set , a feature F(cid:2919 ) is redundant
Yu and Liu [ 22 ] divided weakly relevant features into redundant features and non redundant features . and hence should be discarded from S , iff it is weakly relevant and has a MB within S .
Accordingly , an optimal feature subset should consist of strongly relevant features and non redundant features .
Tsamardinos and Aliferis [ 19 ] proved the following theorem to link feature relevance in feature selection and the Markov blanket in faithful Bayesian networks .
Theorem 5 A feature X(cid:1488)F is strongly relevant , iff it belongs to the Markov blanket of the class attribute in a faithful Bayesian network .
Theorem 5 confirms that the Markov blanket of the class attribute in faithful Bayesian networks is not only unique but also the solution to feature selection .
IV . SELECTION OF FEATURES VIA REPRESENTATIVE
SETS
A . Representative Sets
As stated above , when a data set doesn’t satisfy the intersection property or faithful distribution , it may have multiple Markov blankets of a target feature . Since the number of Markov blankets can grow exponentially in the number of features in the underlying Bayesian network , the discovery of all Markov blankets in non faithful data distribution and picking up a best Markov blanket for feature selection are very expensive , and sometimes are infeasible with high feature dimensions . This motivated us to propose a novel algorithm to solve the problem of multiple Markov blanket selection in real data .
In feature selection , redundant features can replace others in a feature subset , and this is why a MB need not be unique in real data . Figure 2 illustrates that redundant features discarded by a Markov blanket feature selection algorithm actually carries a stronger predictive ability than the selected in Markov blankets . Feature redundancy usually is defined by means of feature correlation [ 9 ] , thus we call those redundant yet discarded features as correlated features with respect to the selected features in Markov blankets .
With correlated features , the feature space of all possible Markov blankets may consist of features in a Markov blanket and their corresponding correlated features . It is an efficient way to discover the feature space of all possible Markov blankets instead of an exhaustive search over an unknown yet even exponential number of Markov blankets in real data . With those observations , by dealing with data in non faithful distributions , we extend the concept of Markov blankets , and propose the concept of representative sets for defining the feature space of all possible Markov blankets .
Definition 11 ( Representative Sets ) A representative set consists of a feature in a Markov blanket and its corresponding correlated features . features this that tackle
From Theorem 4 , this problem , we further extend the class attribute C in a faithful Bayesian network .
Lemma 2 [ 19 ] In Bayesian networks , with two nodes
Lemma 1 [ 19 ] In Bayesian networks , with two nodes
Different from Markov blankets , each member in representative sets isn’t a single feature any longer , but a feature subset . With Definition 11 , now the problem is how can we obtain representative sets in an efficient way ? the theoretical result in Theorem 5 which illustrates that the features in a MB are all strongly relevant features .
F(cid:2919)(cid:1488)F and F(cid:2921)(cid:1488)F , if F(cid:2919 ) and F(cid:2921 ) are never d separated given any subset of the nodes within S(cid:1603)F(cid:4668)F(cid:2919)(cid:1515)F(cid:2921)(cid:4669 ) , iff there must exist a direct edge between F(cid:2919 ) and F(cid:2921 ) . F(cid:2921)(cid:1488)F and F(cid:2919)(cid:1488)F , and their common child F(cid:2923)(cid:1488)F , if F(cid:2919 ) has no direct edge to F(cid:2921 ) , F(cid:2919 ) and F(cid:2921 ) cannot be d separated given any subset of the nodes within S(cid:1603)F(cid:4668)F(cid:2921)(cid:1515)F(cid:2919)(cid:4669 ) that contains F(cid:2923 ) . Proposition 2 A feature F(cid:2919)(cid:1488)F is a strongly relevant feature , iff F(cid:2919 ) belongs to the set of parents and children of Proof : Assume F(cid:2919 ) is a strongly relevant feature . By Definition 7 , F(cid:2919 ) and C are conditionally dependent given any subset S within F(cid:4668)F(cid:2919)(cid:4669 ) , is , ( cid:1482)S(cid:1603)F ( cid:4668)F(cid:2919)(cid:4669 ) stP(cid:4666)C|S(cid:4667)(cid:3405)P(cid:4666)C|S ,F(cid:2919)(cid:4667 ) . implies that F(cid:2919 ) and C are never d separated by any subset within F excluding F(cid:2919 ) . By Lemma 1 , we conclude that in Bayesian networks , feature F(cid:2919 ) belongs to the set of parents Conversely , in faithful Bayesian networks , if node F(cid:2919 ) belongs to the set of parents and children of C , then F(cid:2919 ) and F(cid:2919 ) by Lemma 1 . Accordingly , by Theorem 4 , we come to a conclusion that node F(cid:2919 ) coincides with the definition of a F(cid:2919)(cid:1488)F belongs to spouses of the class attribute C and F(cid:2919 ) does not have a direct edge to C , then F(cid:2919 ) is a nonProof : Since F(cid:2919 ) is a spouse of C , and F(cid:2919 ) has not a direct edge to C , in the path p from F(cid:2919 ) to C , the common child of both C and F(cid:2919 ) , named F(cid:2923 ) , is a collider . Thus , this implies that F(cid:2919 ) and C are d separated by a subset that doesn’t contain F(cid:2923 ) by Lemma 2 . Then we can obtain the term : ( cid:1484)S(cid:1599)F(cid:4668)F(cid:2923)(cid:1515)F(cid:2919)(cid:4669),P(cid:4666)C| F(cid:2919),S(cid:4667)P(cid:4666)C|S(cid:4667 ) by Theorem 4 . Thus , F(cid:2919 ) is not a strongly relevant feature . On the other hand , according to Lemma 2 , we can conclude that F(cid:2919 ) and C cannot be d separated by any subset that contains F(cid:2923 ) , ZF(cid:4668)F(cid:2923)(cid:1515)F(cid:2919)(cid:4669),(cid:1482)S(cid:1599)Z(cid:1515)F(cid:2923),P(cid:4666)C,|S(cid:4667)(cid:3405)P(cid:4666)C|S , F(cid:2919)(cid:4667 ) . Thus , F(cid:2919 ) is weakly relevant to C , and we cannot find a Markov blanket in Z that contains F(cid:2923 ) to make F(cid:2919 ) redundant to C , hence F(cid:2919 ) should be a non redundant strongly relevant feature in Section IIIB □ Proposition 3 In faithful Bayesian networks , if node
C are never d separated by any subset within F excluding that is , the following term holds by Theorem 4 : redundant feature . and children of C . feature with respect to C . □
With Propositions 2 and 3 , we extend Theorem 5 and obtain Theorem 6 .
Theorem 6 In faithful Bayesian networks , the Markov blanket of the class attribute includes ( 1 ) parents and children corresponding to strongly relevant features ; and ( 2 ) spouses corresponding to non redundant features .
With Theorem 6 , the next step is to correlated features related to the features in a It is clear that node X and its parents ( o conditionally dependent given any subs remaining nodes in a Bayesian network . Thu its parents ( or children ) are correlated to addition , by the Markov condition , X is i any subset of its non descendant nodes con parents , but not its children nodes , so the fe correlated to X can be defined in Definition Definition 12 The features directly corre defined as the set of parents and children o Bayesian network . determine the an MB . or children ) are set within the us , node X and each other . In independent of nditioned on its eatures directly 12 . elated to X are of node X in a
Figure 3 . Representative sets related to node “ T ”
Proposition 4 A representative set ca from Bayesian networks , including a paren the class attribute ( a strongly relevant featur of parents and children of this parent or ch features ) .
But why don’t we treat a spouse in MBs and children as a representative set ? The r spouse as a non redundant feature has selected in a representative set . For insta shows four representative sets related to no color , considering node “ T ” as the class attri B . Selection of Features from Representativ With representative sets , the problem be select a best subset from representative se this problem , we need to solve how to optimize selections within each representat as between those sets to achieve a featu maximizes the predictive power to the c Suppose the predictive power of a feature class attribute is measured by a loss f and G(cid:4668)G,G(cid:2870),…,GK(cid:4669 ) represents K repr element G(cid:2919)(cid:4666)i1…K(cid:4667 ) in G incorporates th to K strongly relevant f with respect relevant feature and its corresponding corre Then our solution has two following steps . The first step is to identify which repr have the most predictive power to the class we formulate this step as follows . an be obtained nt or a child of re ) , and the set hild ( correlated and its parents reason is that a already been ance , Figure 3 ode “ T ” in red ibute . ve Sets ecomes how to ets ? To handle simultaneously tive set as well ure subset that class attribute . e subset to the function ( cid:2278)(cid:4666)·(cid:4667 ) , resentative sets features . Each he ith strongly elated features . resentative sets s attribute , and
( 1 ) is the regularization term to contr coefficient vector for all the coefficient vector
∑ Ω(cid:4666)β(cid:2919)(cid:4667 ) β(cid:1499)argmin(cid:2962)(cid:2278)(cid:4666)β,G,C(cid:4667)(cid:3397)(cid:1674 ) K(cid:2919)(cid:2880 ) where β={β,β(cid:2870),…,βK(cid:4669 ) is the c representative sets , and β(cid:2919 ) is s attribute vector , Ω(cid:4666)β(cid:2919)(cid:4667 ) corresponding to G(cid:2919 ) , C is the clas rol the complexity of β(cid:2919 ) , the parameter ( cid:1674 ) controls the sele ection of set , and if β(cid:2919)=0 , β(cid:1499)argmin(cid:2962)(cid:2278)(cid:4666)β,G,C(cid:4667)(cid:3397)(cid:1674)∑K(cid:2919 ) Ω(cid:4666)β(cid:2919)(cid:4667 ) ( cid:3397)(cid:1674)(cid:2870)Ω(cid:2870)(cid:4666)β(cid:4667 ) ( 2 ) where Ω(cid:2870)(cid:4666)β(cid:4667 ) penalizes the compl K(cid:2880 ) lexity of β.The parameter ( cid:1674)(cid:2870 ) adjusts the individual featur re coefficient β to select ere is a coefficient in β up then the ith set will be excluded en The second step is to calculat to be chosen from each selected is expected to contribute the mos class attribute . The objective fun further formulated as , ntirely . te one or a few feature(s ) representative set , which st predictive power to the nction in Eq ( 1 ) is then features within each set and if the to 0 , then the corresponding featur To solve Eq ( 2 ) , we adopt approach which is an extension o [ 18 , 25 ] . The sparse group lasso subset within each group simultaneously [ 4 ] , and in our ca groups . The sparse group lasso p
β(cid:2919 ) and β , by adding two constra Ω(cid:4666)β(cid:2919)(cid:4667 ) to constrain the coefficient ℓ norm for Ω(cid:2870)(cid:4666)β(cid:4667 ) to penalize fea function ( cid:2278)(cid:4666)·(cid:4667 ) , Eq ( 2 ) can be rew ( cid:2870)(cid:2870 ) ( cid:3397)(cid:1674)∑K(cid:2919)(cid:2880 ) min(cid:2962)C∑ G(cid:2919)β(cid:2919 ) K(cid:2919)(cid:2880 ) set . Finally , if we employ the lea objective function : re is discarded . the sparse group lasso of Lasso and group Lasso can yield a best feature and between groups ase representative sets are penalizes the coefficients , aints : 1 ) an ℓ(cid:2870 ) norm for ||β(cid:2919)||(cid:2870)(cid:3397 ) ( cid:1674)(cid:2870 ) ||β|| ( 3 ) ts between sets , and 2 ) an ature coefficients within a ast square loss as the loss written as the following
Thus we cast our problem as t lasso model . Note that Eq ( 3 ) c standard sparse group lasso algori C . The Proposed Algorithm
We design SRS ( Selection via implement the ideas discussed in SF denotes a set of strongly rele representative sets , and PC(T ) children of a target feature T in Fi
The SRS Algorithm
Input : data with the class attrib Step 1 : Identify representative ( 1 ) SF=Get PC(C)// Get the S ( 2 ) K=|SF| //Number of featur ( 3 ) For i=1 to K //Find G by D
G(cid:2919)GetPC(cid:4666)SF(cid:2919)(cid:4667)(cid:1515)SF G(cid:2919)(cid:1514)G(cid:2920)(cid:1486),i(cid:3405)j
End Step 2 : Select features from re ( 4 ) Divide G into K non over
( 5 ) Solve Eq ( 3 ) ( 6 ) Return a best feature subs
Figure 4 . The SRS algorithm set . the standard sparse group can be solved using any ithm . a Representative Sets ) to Sections IV.A and IVB evant features , G denotes denotes the parents and igure 4 . bute C , ( cid:1674 ) and ( cid:1674)(cid:2870 ) F(cid:2919),SF(cid:2919)(cid:1488)SF e sets G SF set by Proposition 2 res in SF Definition 12 epresentative sets rlapping sets : to discover learning strategy
In Step 1 , we can get the representative sets by learning Bayesian networks . Instead of learning a complete Bayesian network among all features , we adopt a local Bayesian network the strongly relevant features of the class attribute ( Proposition 2 ) . Once we get the strongly relevant features , we use the same local learning technique to select the correlated features for each strongly relevant feature ( Definition 12 ) . Thus , as for the GET PC function , we can use MMPC or HITON PC which are both the state of the art local Bayesian ( detailed descriptions in [ 1 ] ) 1 . Since both algorithms have very similar results , we employ HITON PC as the GET PC function in Step 1 . algorithms
In Step 2 , to solve Eq ( 3 ) , we employ a standard sparse group lasso using a least square loss function 2 . Instead of all possible sets ( which could involve all features ) , in Step 2 , the sparse group lasso method only needs to optimize over a small number of representative sets including the most informative features . network learning
V . EXPERIMENTAL RESULTS
A . Experimental Setup
We have chosen 16 benchmark datasets as described in Table 1 . There are 5 datasets from the UCI machine learning repository ( the first 5 datasets ) , 3 biomedical datasets ( hiva , ovarian cancer , and breast cancer ) , 4 NIPS 2003 feature selection challenge datasets ( arcene , dexter , dorothea , and madelon ) , and 4 public microarray datasets ( the last 4 datasets ) [ 21 ] . In our experiments , we treat those datasets in non faithful distributions . in Table I as data
For the 4 NIPS 2003 challenge datasets and the spect dataset , we use the originally provided training and validation sets ; for the 4 gene datasets we adopt the first 2/3 instances for training and the last 1/3 instances for testing ; and for the rest datasets we use 10 fold crossvalidation .
TABLE I SUMMARY OF THE BENCHMARK DATASETS .
( #F : NUMBER OF FEATURES , #I : NUMBER OF INSTANCES )
Dataset spect wdbc spectf promoter infant arcene dexter dorothea
#F
22 30 44 57 86 10000 20000 100000
#I 267 569 267 106 5337 100 300 800
Dataset madelon colon prostate leukemia lung cancer breast cancer ovarian cancer hiva
#F 500 2000 6033 7129 12533 17816 2190 1617
#I 2000 62 102 72 181 286 216 4229
We use two classifiers , Knn and J48 provided by the Spider machine learning package3 . Our comparative study
1 The codes of HITON_PC are available at http://wwwdsl laborg/causal_explorer 2 The codes of sparse group lasso are available at http://wwwpublicasuedu/~jye02/Software/SLEP/indexhtm uses five state of the art Markov blanket filters , including IAMB [ 19 ] , MMMB [ 20 ] , PCMB [ 14 ] , HITON MB [ 1 ] , and HITON PC ( only discovering parents and children of a target feature ) [ 1 ] , the state of the art multiple Markov blanket discovery algorithm TIE* [ 17 ] , and four wellestablished feature selection algorithms , FCBF [ 22 ] , mRMR [ 15 ] , SPSF LAR [ 26 ] , and MRF [ 3 ] . For parameter settings , ( cid:1674 ) and ( cid:1674)(cid:2870 ) are both varied from [ 0.001 ,
0.1 ] with step 0.005 for SRS ; and the significant level is set 0.01 for IAMB , MMMB , PCMB , HITON PC and HITON MB . All experiments were conducted on a computer with Inter(R ) i7 2600 3.4GHz CPU and 12GB memory . B . Comparison with HITON PC , HITON MB , and RES
Figures 5 and 6 summarize the classification errors of SRS against HITON PC , HITON MB and RES , using the Knn and J48 classifiers . RES ( REpresentative Sets ) means that we use the union of representative sets as a feature subset and calculate its classification errors . In both figures , points above the y = x diagonal are datasets for which SRS achieved lower classification errors than the competing algorithm . From Figures 5 to 11 , we have three findings . Firstly , from Figures 5 to 6 , when we process data in non faithful distributions , the Markov blanket selected by HITON MB might not be an optimal solution to feature selection while SRS outperforms HITON MB , HITON PC and RES , especially on datasets with a small sample tofeature ratio . Moreover , from Figure 11 , on the number of selected features , SRS is also very competitive with HITON MB and HITON PC .
Secondly , in Figures 9 and 11 , we can see that the selection of both parents and children of the class attribute may be enough instead of MBs since HITON PC is superior to HITON MB on both classification errors and the number of selected features on most datasets . Furthermore , HITON PC is faster than HITON MB and SRS since it only needs to discover parents and children . This validates Theorem 6 that in Markov blankets only parents and children are strongly relevant features .
Thirdly , Figure 10 illustrates that RES also gets excellent results on classification errors , even better than HITON MB . Thus , we can conclude that representative sets contain sufficiently predictive features and we only need to focus on representative sets without an exhaustive search over all candidate MBs . C . Comparison with Other Markov Blanket Filters
Figures 7 and 8 summarize the classification errors of SRS against IAMB , MMMB , and PCMB ( points above the y = x diagonal are datasets for which SRS achieved lower classification errors than the competing algorithm ) . MMMB fails on the dorothea and leukemia datasets due to long running time ( exceeding three days ) . Figures 7 and 8 show that SRS significantly outperforms those MB filters on the classification errors , especially using the Knn
3 The Spider machine learning package in Matlab is available at http://peoplekybtuebingenmpgde/spider/ classifier . As for the number of selected features , in Figure 12 , SRS is also very competitive with its rivals since it considers not only the strongly relevant features but also their corresponding correlated features .
Figure 5 . ( Knn ) : Classification errors of SRS vs . HITON PC , HITON MB , and RES
Figure 6 . ( J48 ) : Classification errors of SRS vs . HITON PC , HITON MB , and RES
Figure 7 . ( Knn ) : Classification errors of SRS vs . IAMB , PCMB , and MMMB
Figure 8 . ( J48 ) : Classification errors of SRS vs . IAMB , PCMB , and MMMB
From Figure 13 , on running time ( in seconds ) , SRS is also very competitive with the other MB filters , even though it needs to consider not only the strongly relevant features but also their corresponding correlated features . We don’t present the dorothea and leukemia datasets as
MMMB fails on them while on colon , the running time is as follows : SRS : 63 ; HITON MB : 2013 ; IAMB : 1 , and PCMB : 1 .
In summary , from Figures 5 to 13 , our empirical study has indicated that when we process data in non faithful distributions , MBs selected by the existing MB feature selection methods may not be an optimal feature subset , especially on datasets with a small sample to feature ratio while SRS can effectively and efficiently handle MB feature selection in real data . More importantly , with representative sets , SRS can efficiently find a best feature subset without an exhaustive search over an unknown space of the all MBs in each dataset .
Figure 9 . ( Knn ) : Classification errors of HITON PC vs . HITON MB
Figure 10 . ( Knn ) : Classification errors of HITON MB vs . RES
Figure 11 . Number of selected features of SRS vs . HITON PC , HITON
MB and RES
Figure 12 . Number of selected features of SRS vs . IAMB , PCMB , and
MMMB
Figure 13 . Running time ( in seconds ) of SRS against the other rivals ( the labels of the x axis from 1 to 13 denote the datasets in the left figure : 1 . wdbc , 2 . spectf , 3 . infant , 4 . promoter , 5 . lung cancer , 6 . prostate , 7 . arcene , 8 . dexter , 9 . madelon , 10 . breast cancer , 11 . ovariancancer , 12 . hiva , and 13 . spect )
D . Comparison with the TIE* algorithm
In this section , we compare our SRS algorithm with the state of the art multiple MB discovery algorithm , the TIE* algorithm which attempts to find all MBs in real data with non faithful distributions . In our experiments , with the same parameter setting of the TIE* algorithm in [ 17 ] , TIE* is parameterized with Semi Interleaved HITON PC as the base Markov blanket induction algorithm and a classification error as a criterion that verifies whether a new feature subset is a Markov blanket of the class attribute . The parameter alpha of Semi Interleaved HITON PC is set 005 We selected the Markov blanket with the lowest classification error from all of the MBs discovered by TIE* . In the following figures , we don’t plot the errors of the ovarian cancer dataset for SRS and TIE* , since TIE* failed on this dataset due to long running time ( exceeding three days ) .
From Figures 14 to 15 , we can see that SRS outperforms TIE* on most of the datasets . Why is SRS superior to TIE* ? The possible explanation is that TIE* simply selects one feature from a set of strongly correlated features while SRS might pick out more features from a group of strongly correlated features , and this might be beneficial to reduce classification error . This also explains why SRS selects more features than TIE* as shown in Table II . For example , on the spectf dataset , SRS gets four group features {5,6,32} , {14,16,24,25,26} , {30,39,40} , and{15,41,42,43} , and features in each group are strongly correlated . SRS selects seven features {5 , 32 , 24 , 26 , 30 , 40 , 15} from those groups while the MB selected by TIE* only contains two features 30 and 42 which attain the lowest errors among all Markov blankets . On Knn and J48 , SRS gets the classification errors 16.42 % and 11.94 % respectively , while TIE* attains the errors 20.5 % and 16.04 % respectively .
From Table II , we can see that SRS is much faster than TIE* , especially on the arcene , dorothea and breast cancer datasets . On the ovarian cancer datasets , TIE* failed due to long running time ( exceeding three days ) . But why on the colon and leukemia datasets , is TIE* faster than SRS ? The main reason is that Semi HITON PC employed in TIE* is faster than HITON PC used in SRS .
In summary , instead of an exhaustive search for all Markov blankets , the discovery of a best Markov blanket from representative sets is not only more effective but also more efficient than TIE* . E . Comparison with Other Feature Selection Methods
Figures 16 and 17 present the classification errors of feature selection SRS against algorithms , FCBF and mRMR , and two state of the art algorithms , SPSF LAR , and MRF . two well established
Figure 14 . ( Knn ) : Classification errors of SRS vs . TIE*
Figure 15 . ( J48 ) : Classification errors of SRS vs . TIE*
TABLE II NUMBERS OF SELECTED FEATURES AND RUNNING TIME Running time Dataset SRS TIE* 1 1 57 1 2 1 1 6 2 128 1292 20 190 22 10173 848 121 29 63 4 26 42 31 468 361 1 198080 3 254 12 128 /
# Selected features SRS 2 21 7 3 5 9 13 64 11 101 2 4 23 36 21 9 spect wdbc spectf promoter infant arcene dexter dorothea madelon colon prostate leukemia lung cancer breast cancer hiva ovarian cancer
TIE* 1 8 2 3 2 3 4 5 5 2 2 2 3 6 4 /
Figure 16 . ( Knn ) : Classification errors of SRS vs . FCBF , mRMR ,
SPSF LAR and MRF
Since SRS selects no more than 65 features to get the lowest classification error on all 16 datasets , we set the parameter k for the SPSF LAR , MRF , and mRMR methods from 1 to a maximum number of 60 , respectively . For 5 UCI datasets , we use the feature subset whose size ranges from 1 to 15 and choose the lowest classification error rate achieved by Knn and J48 while for the remaining 11 high dimensional datasets , we use the top 5 , 10 , 15 , , 60 features selected by each algorithm .
From Figures 16 to 17 , we can see that SRS often outperforms the other rivals by using Knn while it produces significantly better results the other algorithms by using J48 . than
VI . CONCLUSION
In this paper , we explored Markov blanket feature selection by dealing with data in non faithful distributions .
To tackle this issue , we extended the concept of Markov blankets and proposed the concept of representative sets . With representative sets , we presented the SRS algorithm for Markov blanket feature selection by employing a standard sparse group lasso . The experimental results have shown that the SRS selector outperforms both state of theart Markov blanket feature selectors and other wellestablished feature selection methods on real datasets .
Figure 17 . ( J48 ) : Classification errors of SRS vs . FCBF , mRMR , SPSF
LAR and MRF
ACKNOWLEDGMENT
This work was supported by the National 863 Program of China ( 2012AA011005 ) , the National 973 Program of China under Grant 2013CB329604 , the National Natural Science Foundation of China ( 61305064 , 61070131 , 61175033 , and 61229301 ) .
REFERENCES
[ 1 ]
C . F . Aliferis , A . Statnikov , I . Tsamardinos , S . Mani , and X . Koutsoukos . ( 2010 ) Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I : Algorithms and Empirical Evaluation . Journal of Machine Learning Research , 11 , 171 234 .
[ 2 ] G . Brown , A . Pocock , M . Zhao , and M . Luján . ( 2012 ) Conditional Likelihood Maximisation : A Unifying Framework for Information Theoretic Feature Selection . Journal of Machine Learning Research , 12 , 27 66 .
[ 3 ] Q . Cheng , H . Zhou , and J . Cheng . ( 2011 ) The Fisher Markov Selector : Fast Selecting Maximally Separable Feature Subset for Multiclass Classification with Applications to High Dimensional Data . IEEE Transactions on Pattern Analysis and Machine Intelligence , 33(6 ) , 1217 1233 . J . Friedman , T . Hastie , and R . Tibshirani . ( 2010 ) A Note on the Group Lasso and a Sparse Group Lasso . Arxiv preprint arXiv:10010736
[ 4 ]
[ 5 ]
[ 6 ]
I . Guyon and A . Elisseeff . ( 2003 ) An Introduction to Variable and Feature Selection . Journal of Machine Learning Research , 3 , 11571182 I . Guyon , C . F . Aliferis and A . Elisseeff . ( 2007 ) Causal Feature Selection in chapter of computational methods of feature selection , 63–86 . Chapman and Hall .
[ 7 ] D . Koller and M . Sahami . ( 1996 ) Toward Optimal Feature
Selection . ICML’96 , 284 292 .
[ 8 ] R . Kohavi and G . H . John . ( 1997 ) Wrappers for Feature Subset
Selection . Artificial Intelligence , 97 , 273 324 .
[ 9 ] M . A . Hall.(2000 ) Correlation based Feature Selection for Discrete and Numeric Class Machine Learning . ICML’00 , 359–366 .
[ 10 ] H . Liu and L . Yu . ( 2005 ) Toward integrating feature selection algorithms for classification and clustering . IEEE Transactions onKnowledge and Data Engineering,17(4 ) , 491 502 .
[ 11 ] D . Margaritis and S . Thrun . ( 2000 ) Bayesian Network Induction via Local Neighborhoods . In Advances in Neural Information ProcessingSystems 1999 . Denver , Colorado , USA : The MIT Press . [ 12 ] R . E . Neapolitan ( 2004 ) Learning Bayesian Networks . Prentice Hall series in artificial intelligence . Pearson Prentice Hall , Upper Saddle River , NJ .
[ 13 ] J . Pearl ( 1988 ) Probabilistic Reasoning in Intelligent Systems : Networks of Plausible Inference . Morgan Kaufmann , San Mateo , CA .
[ 14 ] J . M . Peña , R . Nilsson , J . Björkegren , and J . Tegnér . ( 2007 ) Towards Scalable and Data Efficient Learning of Markov Boundaries . International Journal of Approximate Reasoning , 45(2 ) , 211 232 .
[ 15 ] H . Peng , F . Long , and C . Ding . ( 2005 ) Feature Selection Based on Mutual Information : Criteria of Max Dependency , MaxRelevance , and Min Redundancy . IEEE Transactions on Pattern Analysis and Machine Intelligence , 27(8 ) , 1226 1238 .
[ 16 ] J . Shen , L . Li , and W K . Wong . ( 2008 ) Markov Blanket Feature
Selection for Support Vector Machines . AAAI’08 , 696 701 .
[ 17 ] A . Statnikov , N . Lytkin , J . Lemeire and F . C . Aliferis . ( 2013 ) Algorithms for Discovery of Multiple Markov Boundaries . Journal of Machine Learning Research 14 , 499 566 .
[ 18 ] R . Tibshirani . ( 1996 ) Regression Shrinkage and Selection via the
Lasso . J . Roy . Stat . Soc . B , 58(1):267 288 .
[ 19 ] I . Tsamardinos and C . F . Aliferis . ( 2003 ) Towards Principled Feature Selection : Relevancy , Filters and Wrappers . AI & Statistics’03 .
[ 20 ] I . Tsamardinos , L . E . Brown , and C . F . Aliferis . ( 2006 ) . The Maxmin Hill climbing Bayesian Network Structure Learning Algorithm . Machine Learning , 65 , 31–78 .
[ 21 ] L . Yu , C . Ding , and S . Loscalzo . ( 2008 ) Stable Feature Selection via Dense Feature Groups . KDD’08 , 803 811 .
[ 22 ] L . Yu and H . Liu . ( 2004 ) Efficient Feature Selection via Analysis of Relevance and Redundancy . Journal of Machine Learning Research , 5 : 1205 1224 .
[ 23 ] K . Yu , X . Wu , W . Ding , H . Wang , and H . Yao . ( 2011 ) Causal
Associative Classification . ICDM’11,914 – 923 .
[ 24 ] K . Yu . , W . Ding , H . Wang , and X . Wu . ( 2013 ) Bridging Causal Relevance and Pattern Discriminability:Mining Emerging Patterns from High Dimensional Data . IEEE Transactions on Knowledge and Data Engineering . In press .
[ 25 ] M . Yuan and Y . Lin . ( 2006 ) Model Selection and Estimation in
Regression with Grouped Variables . J . Roy . StatSoc B , 49 67 .
[ 26 ] Z . Zhao , L . Wang , H . Liu , and J . Ye . ( 2013 ) On Similarity Preserving Feature Selection . IEEE Transactions on Knowledge and Data Engineering , 25(3 ) , 619 632 .
