Active Matrix Completion
Shayok Chakraborty1 , Jiayu Zhou1,2 , Vineeth Balasubramanian1 , Sethuraman Panchanathan1 ,
Ian Davidson3 and Jieping Ye1,2
1School of Computing , Informatics and Decision Systems Engineering , Arizona State University
3Department of Computer Science , University of California , Davis
2Center for Evolutionary Medicine and Informatics , The Biodesign Institute , Arizona State University Email : {shayok.chakraborty , jiayu.zhou , vineeth.nb , panch , jiepingye}@asuedu , davidson@csucdavisedu Abstract—Recovering a matrix from a sampling of its entries is a problem of rapidly growing interest and has been studied under the name of matrix completion . It occurs in many areas of engineering and applied science . In most machine learning and data mining applications , it is possible to leverage the expertise of human oracles to improve the performance of the system . It is therefore natural to extend this idea of “ human in theloop ” to the matrix completion problem . However , considering the enormity of data in the modern era , manually completing all the entries in a matrix will be an expensive process in terms of time , labor and human expertise ; human oracles can only provide selective supervision to guide the solution process . Thus , appropriately identifying a subset of missing entries ( for manual annotation ) in an incomplete matrix is of paramount practical importance ; this can potentially lead to better reconstructions of the incomplete matrix with minimal human effort . In this paper , we propose novel algorithms to address this issue . Since the query locations are actively selected by the algorithms , we refer to these methods as active matrix completion algorithms . The proposed techniques are generic and the same frameworks can be used in a wide variety of applications including recommendation systems , transductive / multi label active learning , active learning in regression and active feature acquisition among others . Our extensive empirical analysis on several challenging real world datasets certify the merit and versatility of the proposed frameworks in efficiently exploiting human intelligence in data mining / machine learning applications . the learner is exposed to a small amount of labeled data and a huge quantity of unlabeled data . Annotation of such a large scale unlabeled data involves significant manual labor . Active learning algorithms identify a subset of promising and exemplar unlabeled samples , which are given to a human user for label annotation . This tremendously reduces the human annotation effort in inducing a reliable model as only a few representative instances , which are selected by the algorithm need to be labeled manually . Further , it endows the model with greater generalization capability , as it gets trained on the promising and representative samples from the underlying data population . Thus , intelligent usage of human supervision leads to better models for classification and regression applications . Interactive image segmentation and relevance feedback based systems are other examples which integrate human intelligence to improve the performance of machine learning systems . It is therefore logical to conceive of the development of matrix completion algorithms with selective human supervision . This can potentially lead to better reconstruction of the partially observed data matrix and can be immensely useful in a wide variety of applications ( eg recommendation systems ) .
I .
INTRODUCTION
The data collected in most modern applications are mostly structured in the form of matrices ( for instance , in a recommendation system , the data is represented in the form of a matrix , where each row is a user , each column is an object and the corresponding entry represents the rating given by the particular user to that object ) . The problem of matrix completion , or reconstructing a matrix from a set of partially observed entries , is of immense practical importance . Missing values is a recurrent problem in applications like machine learning computer vision and graphics and recommendation systems among others . Missing data in any of these applications can bias results , reduce generalizability and lead to erroneous conclusions . Several matrix completion algorithms have been proposed over the last couple of decades to address this practical and useful challenge [ 1 ] , [ 2 ] , [ 3 ] .
In this paper , we propose novel algorithms to integrate the intelligence of human oracles in the matrix completion problem . We develop frameworks to quantify the uncertainty of prediction of each missing entry in the matrix , which can be used to identify the indices which need to be labeled manually . The frameworks are generic and can be used in conjunction with any popular matrix completion algorithm ( EM/SVD and others ) . Since our algorithms actively identify the matrix indices for manual annotation , we address them as Active Matrix Completion . Further , the proposed frameworks can be adapted to different variants of active learning ( multiclass , multi label , transductive active learning , active feature acquisition ) to select a set of exemplar unlabeled data instances for manual annotation . If the data matrix is represented as the feature vectors ( of the training and unlabeled samples ) together with the corresponding class labels , then the missing entries correspond to the labels of the unlabeled samples . The proposed algorithms can be applied to quantify the uncertainty of prediction of each of the missing entries ( labels of the unlabeled instances ) , which can then be used to decide the subset of samples to be queried for manual annotation .
Of late , there has been a growing interest in the design and development of machine learning algorithms with “ humansin the loop ” . The basic purpose is to leverage human intelligence to improve the performance of a learning system . A classical example is active learning [ 4 ] , [ 5 ] , [ 6 ] , where
The rest of the paper is organized as follows : we present a survey of related approaches in Section 2 , Section 3 details the mathematical formulations of our approaches , the results of our experiments are presented in Section 4 , we present extensions of the framework to active learning in Section 5 and conclude with discussions in Section 6 .
II . RELATED WORK
In this section , we first present a brief review of active learning , followed by a survey of techniques that apply active learning in recommender systems .
A . Related Work on Active Learning
Active learning algorithms automatically identify the salient and exemplar samples from large quantities of unlabeled data and tremendously reduce human annotation effort in training a classifier . Researchers have approached the problem of active learning in several different ways [ 7 ] . Most of these approaches have focused on selecting a single instance at a time from the unlabeled pool ( called pool based active learning ) and retraining the model with the selected samples sequentially . Common techniques of pool based active learning include SVM based methods [ 4 ] , [ 8 ] , statistical methods [ 9 ] and ensemble based methods [ 5 ] , [ 10 ] and informationtheoretic methods [ 11 ] . To avoid frequent retraining of classifiers and to utilize the possible presence of multiple labeling oracles , batch mode active learning ( BMAL ) schemes have been proposed in recent years which attempt to select a batch of unlabeled samples simultaneously for manual annotation . Hoi et al . [ 12 ] used the Fisher information matrix as a measure of model uncertainty and proposed to query the set of points that maximally reduced the Fisher information . The same authors [ 13 ] proposed a BMAL scheme based on SVMs , where a kernel function was first learned from a mixture of labeled and unlabeled samples which was then used to identify the informative and diverse examples through a min max framework . Guo and Schuurmans [ 6 ] proposed a discriminative strategy that selected a batch of points which maximized the log likelihoods of the selected points with respect to their optimistically assigned class labels and minimized the entropy of the unselected points in the unlabeled pool . Very recently , Guo [ 14 ] proposed a batch mode active learning scheme which maximized the mutual information between the labeled and unlabeled sets and was independent of the classification model used . The methods proposed in [ 6 ] and [ 14 ] have been shown to be the state of the art BMAL techniques [ 14 ] .
B . Related Work on Active Learning for Collaborative Filtering
Collaborative filtering is a traditional technique for recommender systems , which makes automatic predictions about the interests of a user by reusing taste information from other users . Collaborative filtering methods fall into two categories memory based methods and model based methods . In memory based techniques , the value of the unknown rating is computed as an aggregate of the ratings of some other users for the same item . Model based collaborative techniques provide recommendations by estimating parameters of statistical models for user ratings . Matrix Factorization ( MF ) and Aspect Model ( AM ) are two popular model based methods . When a new user enters a recommendation system , the system will not have any knowledge about him/her as the he/she has not provided any ratings . To alleviate this problem , active learning can be used to acquire only those ratings from the users which are useful in determining their tastes .
There have been some recent research efforts in applying active learning for collaborative filtering [ 15 ] . Boutilier et al . [ 16 ] applied the metric of expected value of utility to find the most informative item for query in a collaborative filtering application . Rish and Tesauro [ 17 ] used a heuristic maximum margin based approach for active sample selection in collaborative prediction . Jin and Si [ 18 ] developed a new active learning algorithm based on AM which is similar to applying active learning for parameter estimation in Bayesian networks . Along similar lines , Silva and Carin [ 19 ] presented an online variational Bayesian approach to the problem of collaborative filtering , which selected near optimal subset of users and items through active learning . Harpale and Yang [ 20 ] proposed a framework that personalized active learning to the preferences of each new user as it queried only those items for which the users are expected to provide a rating . Karimi et al . [ 21 ] compared AM with MF and showed that MF is more suitable for applying active learning in recommender systems because it of its speed and accuracy . The same authors exploited the characteristics of matrix factorization for active learning by identifying users who are similar to the new user in the latent space [ 22 ] . They also developed a non myopic active learning framework which capitalized explicitly on the update procedure of MF model [ 23 ] . Recently , Karimi et al . [ 24 ] proposed an active learning strategy which selected the items by directly minimizing the resulting expected error .
Although the aforementioned techniques are based on actively identifying a set of unlabeled entries in an incomplete matrix , they are mostly tailored towards collaborative filtering applications ie most of them select a subset of entries for new incoming users whose labels will be most informative in infering their tastes , so as to make better recommendations . It is non trivial to use such algorithms to solve other variants of the active learning problem , like transductive active learning , multi label active learning , active learning in regression and active feature acquisition . In this paper , we propose generic active matrix completion algorithms to select a subset of exemplar entries in an incomplete data matrix for manual annotation . Our techniques are versatile and the same frameworks can be used in recommendation systems and also to solve different variants of the conventional active learning problem . We also propose techniques to ensure scalability of our algorithms . We now describe the mathematical formulations of the frameworks .
III . PROPOSED FRAMEWORK
The fundamental idea behind the proposed algorithms is to compute a measure of uncertainty of prediction of every missing entry in the incomplete data matrix . The top uncertain entries can then be queried for manual annotation . We present three strategies to quantify the uncertainty of prediction and consequently , three active matrix completion algorithms . We first present the mathematical details of these algorithms ; we then present methodologies to improve the computational efficiency of our frameworks , so as to make them scalable to large datasets .
A . Active Matrix Completion using the Conditional Gaussian Distribution
This method treats each row ( or column , as the case may be ) of the data matrix as a particular case ( or sample).For each case , it is assumed that the set of missing entries conditioned on the set of observed entries follows a multivariate normal distribution . A well known result from statistical learning theory enables us to compute the mean and the covariance matrix of this conditional distribution . The overarching idea is to impute the missing entries of each case with the conditional mean vector while the diagonal elements of the covariance matrix of the conditional distribution quantifies the variance ( uncertainty ) associated with each imputation . The top k uncertain entries across the entire matrix ( where k is the batch size or the allowable number of entries that can be queried ) are then selected for manual annotation . Our algorithm is based on the GLasso and MissGlasso frameworks that have been proposed for sparse inverse covariance estimation of the multivariate normal distribution in the presence of missing entries . We now present the details of these methods .
1 ) GLasso : Consider a variable X ( of dimensionality p ) which follows the multi variate normal distribution with mean vector µ and covariance matrix Σ that is , X ∼ N ( µ , Σ ) . The problem of Graphical Lasso ( or GLasso ) [ 25 ] is to estimate the parameters µ and Σ from a complete random sample x = ( x1 , x2 , . . . , xn)T . Let K denote the concentration matrix , K = Σ−1 . A typical approach is to minimize the negative 1penalized log likelihood [ 25 ] : n
−(µ , K ; x ) + λ||K||1 = − n 2 1 2 log |K| ( xi − µ)T K(xi − µ ) + λ||K||1
+
( 1 ) i=1 parameter and ||K||1 =p where K is a positive semidefinite matrix , λ > 0 is a tuning j,j=1 |Kjj| . The minimizer ˆK can be obtained by solving the following optimization problem :
ˆK = arg min
( − log |K| + tr(KS ) + ρ||K||1 ) n K0 i=1(xi − ¯x)(xi − ¯x)T and ρ = 2λ n . Friedman where S = 1 n et al . [ 26 ] proposed an efficient algorithm called GLasso to solve the optimization problem ( 2 ) .
( 2 )
2 ) MissGlasso : This algorithm was proposed by Stadler and Buhlmann [ 27 ] to estimate the mean and covariance matrix of the multi variate normal model in the presence of missing entries . Let x = ( xobs , xmis ) denote the set of observed values and missing values respectively of a random sample x of size n . Further , let xobs = ( xobs,1 , xobs,2 , . . . , xobs,n ) where xobs,i represents the set of observed variables for case i , i = 1 , . . . n . The likelihood function is now based on the observed indices for each case and is summed over all the cases : n
( µ , Σ ; xobs ) = − 1 2
( log |Σobs,i| +(xobs,i − µobs,i)T ( Σobs,i)−1(xobs,i − µobs,i ) ) i=1 where µobs,i and Σobs,i are the mean and covariance matrix of the observed components of X for case i . Equivalently , in terms of K :
( µ , K ; xobs ) = − 1 2 +(xobs,i − µobs,i)T ( K−1 n ( log |(K−1)obs,i| obs,i)−1(xobs,i − µobs,i ) ) i=1
( 3 )
( 4 )
Similar to Equation ( 1 ) , the inference for µ and K are now based on the sum of the observed log likelihood over all the cases , together with an 1 penalty on the concentration matrix K :
− ( µ , K ; xobs ) + λ||K||1
( 5 )
ˆµ , ˆK = arg min ( µ,K):K0
This problem can be solved using a well known theorem on partitioned Gaussians , which can be stated as follows [ 28 ] . Consider a joint Gaussian distribution N ( x|µ , Σ ) with K = Σ−1 and consider a partition xa
Σaa Σab x = xb
Σba Σbb
, µ =
µa
Kaa Kab
µb
Kba Kbb
Σ =
, K =
Then , xa conditioned on xb will also follow a normal distribution and its mean and covariance can be expressed in terms of the known parameters [ 28 ] , that is Xa|b ∼ N ( µa|b , Σa|b ) where ( 6 )
µa|b = µa − K−1 aa Kab(xb − µb )
Σa|b = K−1 aa
( 7 )
Assuming that for each case in the data , the set of missing entries conditioned on the set of observed entries follows a multivariate normal distribution , the problem in Equation ( 5 ) can be solved using the results stated above together with the Expectation Maximization ( EM ) algorithm . The complete data x follows a multivariate normal distribution which belongs to the exponential family with sufficient statistics n n n
T1 = xT .1 = and xi1 , xi2 , . . . , xip i=1 i=1 i=1
T2 = xT x
The log likelihood for the complete data , given in Equation ( 1 ) can be expressed in terms of the sufficient statistics T1 and T2 as follows : −(µ , K ; x ) + λ||K||1 = − n 2
µT Kµ − µT KT1 n 2 tr(KT2 ) + λ||K||1 ( 8 ) which is linear in T1 and T2 . The expected complete penalized log likelihood is denoted by : log |K| + 1 2
+
Q(µ , K|µ
, K
) = − E[(µ , K ; x)|xobs , µ
] + λ||K||1
, K
The EM algorithm works by iterating between the E step and the M step . Let ( µm , K m ) denote the parameter values in iteration m .
E step : In the expectation step , the expected value of the complete penalized log likelihood is computed . As the complete penalized log likelihood is linear in terms of the sufficient statistics T1 and T2 , the E step essentially consists of calculating
T m+1 1 T m+1 2
= E[T1|xobs , µm , K m ] = E[T2|xobs , µm , K m ]
To evaluate these , we need to compute the conditional expectation of xij and xijxij for i = 1 , . . . , n and j , j = 1 , . . . , p . Assuming for each case , the set of missing entries conditioned on the set of observed entries is normally distributed , Equation ( 6 ) gives flxij cj
E[xij|xobs,i , µm , K m ] = if xij is observed if xij is missing where the vector c is defined as ( from Equation ( 6 ) ) mis,obs(xobs,i − µm obs ) mis,mis)−1K m mis − ( K m c = µm
Here , Kmis,mis is the sub matrix of K with rows and columns corresponding to the missing entries for case i . Kmis,obs is defined analogously . Similarly ,
xijxij xijcj ( K m mis,mis)−1 jj + cjcj
E[xijxij|xobs,i , µm , K m ] = if xij and xij are observed if xij observed xij missing if xij , xij missing
M step : In the maximization step , the expected loglikelihood for the complete dataset is maximized to obtain the update equations for the mean vector and the covariance matrix . Differentiating Equation ( 8 ) with respect to µ and equating it to 0 , we get the update equation for µ as :
µm+1 =
1 n
T m+1 1
Also , from a re organization of the terms in Equation ( 8 ) , it is evident that the covariance matrix can be updated by solving the following optimization problem :
K m+1 = arg min
K0
− log |K| + tr(KS(m+1 ) ) +
||K||1
2λ n
2 n T m+1
− µm+1(µm+1)T . The M step where Sm+1 = 1 therefore reduces to a standard GLasso problem , as discussed in Section III A1 . The E step and the M step are iterated until convergence . Once the final matrix K is obtained , the covariance matrix corresponding to the missing entries for a particular case ( or data sample ) i can be derived as the sub matrix of K with rows and columns corresponding to the indices of the missing variables for case i . The diagonal elements of the covariance matrix quantify the variance of prediction of each of the missing entries for that case . Thus , it is possible to estimate the variance ( uncertainty ) of prediction of each of the missing entries ( on a case by case basis ) in the partially observed matrix . The missing entries are ranked in descending order of their uncertainties and the top k entries ( k being the required batch size ) are queried for manual annotation .
B . Active Matrix Completion using Query by Committee ( QBC )
The QBC framework quantifies the prediction uncertainty based on the level of disagreement among an ensemble of matrix completion algorithms . Specifically , a committee of matrix completion algorithms are applied on the partially observed data matrix to impute the missing values . The variance of prediction ( among the committee members ) of each missing entry is taken as a measure of uncertainty of that entry . The top k uncertain entries are then queried for manual annotation . We used the following three commonly used matrix completion algorithms as members of our committee : k NN : The k nearest neighbors ( k NN ) method identifies the k most similar features to the current one with a missing value and uses the average of these k nearest neighbors as a guess for the missing one [ 29 ] .
EM : This method imputes the missing values using the Expectation Maximization ( EM ) algorithm [ 29 ] . An iteration of the EM algorithm involves two steps . In the E step , the mean and covariance matrix are estimated from the data matrix ( with the missing entries filled with zeros or estimates from the previous M step ) ; in the M step , the missing value of each data column is filled in with their conditional expectation values based on the available entries and the estimated mean and covariance . The mean and the covariance are re estimated based on the newly filled matrix and the process is iterated until convergence .
SVD : Singular value decomposition ( SVD ) is a standard method for matrix completion based on low rank approximation [ 29 ] . In this method , some initial guesses are first provided to the missing data values . SVD is then applied to obtain a low rank approximation of the filled in data matrix . The missing values are then updated based on their corresponding values in the low rank estimation . SVD is applied to the updated matrix again and the process is iterated until convergence .
C . Active Matrix Completion using Committee Stability
This approach is similar to the QBC strategy . However , instead of different matrix completion techniques , we used the same SVD based imputation algorithm with different values of the rank parameter to form the committee . The uncertainty of prediction of each missing entry was computed as the variance of the values from the committee members for that entry , as before . We refer to this method as the stability based active matrix completion algorithm since it essentially measures the regularity of prediction of a particular entry from an ensemble of predictors ( similar to the QBC framework ) .
A general pseudo code of the three active matrix comple tion algorithms is presented in Algorithm 1 .
D . Computational Considerations
In Section III A , we noted that the active matrix completion algorithm based on Conditional Gaussian distribution involves estimation of the inverse covariance matrix K ( in the M step of the EM algorithm ) . Also , both the QBC and the Stability based algorithms involve low rank matrix completion using the singular value decomposition technique . These can adversely affect the computation time for large scale matrices .
Algorithm 1 Active Matrix Completion Input : A partially observed matrix M ∈ p×q , set Ω of observed indices , batch size k and number of iterations n graph G(λ ) 2 given by : admits a decomposition into connected components
2 = ∪m2(λ ) G(λ ) l=1 G(λ )
2l
( 11 )
1 : for rounds = 1 → n do 2 :
3 :
4 :
5 : 6 :
Complete the partially observed matrix and compute the variance of prediction of every missing entry ( as detailed in Sections III A , III B and III C ) Sort the missing entries in descending order of their uncertainty ( variance ) values Query the ground truth values of the top k uncertain entries from human oracles Update the matrix with the newly acquired entries Complete the matrix using any standard matrix completion algorithm Compute the reconstruction error
7 : 8 : end for 9 : return The completed matrix after n iterations
K0
In this section , we present two efficient algorithms one for inverse covariance estimation and the other for low rank matrix completion , to speed up the computations in our algorithms . 1 ) Efficient Inverse Covariance Estimation : Sparse inverse covariance estimation is typically achieved using the GLasso algorithm [ 26 ] , which solves the following problem : ( − log |K| + tr(KS ) + λ||K||1 )
K = arg min
( 9 ) where S is the sample covariance matrix of dimension p × p . To address the scalability issue , we used the thresholding strategy , proposed by Mazumder and Hastie [ 30 ] , for large scale graphical lasso . The authors presented a novel property characterizing the family of solutions to the graphical lasso problem in Equation ( 9 ) as a function of the regularization parameter λ , which states that the vertex partition induced by the connected components of the non zero pattern of the estimated concentration matrix ( at λ ) and the thresholded sample covariance matrix S ( at λ ) are exactly equal . Specifically , the symmetric edge matrix/skeleton ∈ {0 , 1}p×p defined by : sparsity pattern of the solution K ( λ ) to ( 9 ) gives rise to the if K ( λ ) ij otherwise
= 0 , i = j
E1(λ ) ij =
1 0 fl1
0
This defines a symmetric graph G(λ )
1 = ( V , E1(λ) ) , and suppose that it admits a decomposition into m1(λ ) connected components :
1 = ∪m1(λ ) G(λ ) , E1(λ )
1l = ( V ( λ ) l=1 G(λ ) where G(λ ) ) . Now , a thresholding on the entries of the sample covariance matrix S ( for a given λ ) is 2 ∈ {0 , 1}p×p performed to obtain a graph edge skeleton E(λ ) defined by :
( 10 )
1l l l
E2(λ ) ij = if |Sij| > λ , i = j otherwise
The symmetric matrix E2(λ ) defines a symmetric graph on 2 = ( V , E2(λ) ) . The the nodes V = {1 , . . . , p} given by G(λ ) l l
, E2(λ ) where G(λ ) 2l = ( V ( λ ) ) are the components of the graph G(λ ) 2 . Mazumder and Hastie [ 30 ] proved that the vertex partition of the connected components of ( 11 ) is exactly equal to that of ( 10 ) . This implies that the optimization problem in ( 9 ) completely separates into m2(λ ) separate sub problems of the same form as ( 9 ) . The sub problems have size equal to the number of nodes in each component pi = |Vi| , i = 1 , . . . , m2(λ ) . Also , the cost of computing the connected components of the thresholded sample covariance graph ( 11 ) is much smaller than the cost of fitting graphical models ( 9 ) . Further , the computations pertaining to the covariance graph can be done off line and are amenable to parallel processing . This property of thresholding a graph into connected components facilitates efficient computation of the inverse covariance matrix for large scale data . For more details , please refer to [ 30 ] .
2 ) Efficient Low Rank Matrix Completion : A standard approach to solve the low rank matrix completion problem is to relax the rank function to its convex surrogate trace norm , which is then solved using singular value decomposition ( SVD ) operations . To avoid the computational overhead of SVD , some researchers proposed to sacrifice the convexity by local searching . These methods only require light weighted matrix computations and are scalable . In this work , we used the low rank factorization LMaFit proposed in [ 31 ] . The main idea of the method is presented as follows . For a low rank matrix X ∈ Rm,n of rank r , where r < min(m , n ) , we can represent the matrix by X = UV where U ∈ Rm,r and V ∈ Rr,n , we plugin this product form of X into the objective and solve the following problem : stPΩ(Z ) = PΩ(X0 ) ( U∗ , V∗ , Z∗ ) = arg min where Z ∈ Rm,n is an auxiliary matrix and the required low rank matrix is given by X∗ = U∗V∗ . The formulation can be directly solved by the block coordinate descent algorithm . In each step of the algorithm , two least squares problems need to be solved , which can be effectively reduced to a single least squares problem [ 31 ] . The improved algorithm is depicted in Algorithm 2 .
UV − Z2
U,V,Z
F the major computational cost
Since QR decomposition is very cheap for the matrices in Algorithm 2 is Z−VT− , the computation of U+V+ for computing Z+ = U+V+ + PΩ(X0 − U+V+ ) . Note that Z+ is a sparse + low rank structure , thus there is no need to compute the multiplication operation , but directly work on the sparse + low rank structure whenever Z+ is used . This reduces the computational complexity and makes the method scalable .
IV . EXPERIMENTS AND RESULTS
In this section , we study the performance of the proposed active matrix completion algorithms . We started with a given data matrix and manually deleted a certain percentage ( ranging from 40 % to 98 % ) of the entries at random . The active matrix completion algorithms ( refered to as Conditional Gaussian ,
Algorithm 2 Block coordinate descent for efficient low rank matrix completion Output : U∗ , V∗ , Z∗ , Ω , X0 Input : V0 , Z0
V− = V0 , X− = X0 while true do perform QR decomposition on Z−VT− and let Q be its orthogonal basis . U+ = Q V+ = QZ− Z+ = U+V+ + PΩ(X0 − U+V+ ) if convergence break return U− = U+ , V− = V+ , Z− = Z+ end while return U∗ = U+ , V∗ = V+ , Z∗ = Z+
QBC and Stability for the methods proposed in Sections III A , III B and III C respectively ) were then applied to query a fixed number of entries in each iteration . After the batch query , the selected locations in the matrix were annotated using a human oracle ( we simulated this by supplying the ground truth value of the batch of selected indices ) . The matrix was then completed using any standard matrix completion algorithm ( we used the SVD based completion algorithm [ 29 ] in our work ) . The reconstruction error was then computed as the Frobenius norm of the error matrix ( the difference between the original data matrix and the predicted matrix , normalized by the number of indices predicted ) . The process was then repeated and the reduction in the reconstruction error with increasing number of iterations ( equivalently , with increasing number of observed entries ) was noted . We compared our results against the case where there was no human intervention and the matrix was merely completed using the SVD algorithm ( referred to as passive completion in our experiments ) . We also compared our approaches against the case where the query locations for manual annotation were selected at random .
A . Image Datasets
Fig 1 . GrayScale images used in our experiments
We first conducted experiments on images ( represented as grayscale matrices of size 256 × 256 ) . We used four commonly used images in computer vision research the Lena , Cameraman , Vegetables and Building images for our study . These images are shown in Figure 1 . The degree of sparsity ( percentage of missing entries to begin with ) was set to 60 % . The batch size ( number of entries to be queried in each iteration ) was set to 50 and the process was repeated for 50 iterations . The results were averaged over 5 runs ( where the specific positions of the missing entries in the starting matrix were randomly permuted ) to rule out the effects of randomness . Since these datasets are relatively small in size , we used the standard SVD method here ( and not the algorithm detailed
Dataset
Movie 100k Movie 1M
Netflix Dating TABLE I .
Rows 943 1000 442 152
Cols 1682 2000 8307 17906
Matrix Size
1.58 M
2 M
3.67 M 2.7 M
SL 93.7 % 96.2 % 97.6 % 98.2 %
RECOMMENDATION DATASETS DETAILS in Section III D2 ) . For the inverse covariance estimation , the efficient approach proposed by Mazumder and Hastie [ 30 ] was used . The results are presented in Figure 2 .
The x axis denotes the number of queried entries and the y axis denotes the reconstruction error . The objective was to study the rate of decrease of the error as more and more entries in the matrix were labeled . The dotted horizontal line depicts the scenario when no human being is involved in the matrix completion process the matrix is merely completed using SVD imputation ( note that since there is no human labelers for this method , the number of observed entries remain the same , which leads to the same error value from iteration to iteration ; this line is plotted for comparison between active and passive completion ) . It is evident that active matrix completion results in enormous reduction in the reconstruction error . This corroborates the advantage of leveraging human intelligence in the matrix completion process . We further note that all the proposed algorithms outperform random sampling as the errors drop at a faster rate ( with increasing number of observed entries ) using the proposed methodologies . The method based on Conditional Gaussians depicts the best performance on these datasets . Further empirical studies ( not presented here for brevity ) revealed that the pattern of the graphs remained the same for different degrees of sparsity ( 20 % , 40 % and 80 % ) the only difference being the absolute values of the reconstruction errors .
B . Recommendation Systems
Recommendation systems is an important application of the proposed active matrix completion algorithms where requesting the rating of a particular object ( movie , music , book ) from a particular user makes intuitive sense . The proposed algorithms can be used to judiciously exploit human intelligence to facilitate a more accurate reconstruction , which in turn , can help making better recommendations to appropriate users .
We used the following recommendation datasets in our experiments : ( 1 ) MovieLens 100k , ( 2 ) MovieLens 1M , ( 3 ) Netflix , all of which contain ratings given by a group of users on a set of movies and ( 4 ) Dating Recommendation , containing anonymous ratings of profiles made by users . In each dataset , rows represent users , columns represent items and the matrix entry denotes the rating given by a particular user for a particular item . The details of the datasets together with their sparsity levels ( SL ) or the percentage of missing entries , are reported in Table I .
These datasets inherently contain a lot of missing entries as it was not possible to get the ratings of each of the items from all the users . However , to test the performance of our algorithms from iteration to iteration , we need the ground truth values of all the entries in the matrix . To alleviate this issue , we focused only on the set of observed entries in the matrix and manually deleted 50 % of the observed entries in each dataset . The active matrix completion algorithms were then run
( a ) Lena
( b ) Cameraman
( c ) Vegetables
( d ) Buildings
Fig 2 . Active Matrix Completion on Image Datasets ( Figures best viewed in color ) . Degree of Sparsity = 60 % on the entire matrix and the prediction uncertainty values were ranked only on the set of entries which were manually deleted . After supplying the ground truth values of these indices , the matrix was completed and the error was measured only on the observed subset of the matrix . The batch size was set at 50 and the process was repeated for 100 iterations . As before , the results were averaged over 5 random runs . The efficient matrix completion algorithm ( Section III D2 ) was used in place of the standard SVD ( for the QBC and the Stability based active matrix completion algorithms ) for these datasets .
Figure 3 depicts the performance on these datasets . We once again note that the incorporation of human supervision significantly reduces the reconstruction error ( as evident from the dashed line represeting passive completion and the solid lines representing active matrix completion ) . The QBC and the Conditional Gaussian based algorithms consistently depict good performance across all in fact , marginally outweighs the Conditional Gaussian based framework and achieves the lowest reconstruction error after 100 iterations . From the results , we also note that random sampling can sometimes depict good performance ( as in the Movie 100k dataset ) . However , is not consistent across datasets in its performance . the datasets ; QBC , it
C . Computation Time Analysis
In this section we study the computation time of each of the active matrix completion algorithms to select a batch of entries from a matrix for manual annotation . The results are presented in Table II . Other than Random Sampling , the QBC approach is the most efficient in terms of computation time and can handle a matrix with about 3.67 million entries in less than 2 minutes . The Stability and the Conditional Gaussian based frameworks also depict promising runtime values and can scale to a matrix with 3.67 million entries in approximately 5 minutes ( for the dating recommendation dataset however , the Conditional Gaussian based approach took almost 15 minutes for each batch query ; the reason behind this needs to be investigated ) . From this table , it is evident that the QR decomposition based algorithm for efficient matrix completion and the connected components based graph theoretic method for scalable inverse covariance estimation are effective in significantly reducing the computational overhead of the proposed frameworks . Thus , besides outweiging passive matrix completion and random sampling in terms of error reduction , the proposed active matrix completion algorithms are also efficient computationally and thus have the potential to scale to large datasets .
( a ) Movie 100k
( b ) Movie 1M
( c ) Netflix
( d ) Dating
Fig 3 . Active Matrix Completion on Recommendation Datasets ( Figures best viewed in color )
Datasets
Lena
Cameraman Vegetables Building
Movie 100k Movie 1M
0.91 0.94 0.42 0.89 1.07 1.76 1.78 1.59
Random
Stability
QBC 7.40 7.28 8.72 17.50 12.59 17.18 93.87 31.92
17.14 12.56 14.18 17.33 49.81 56.77 245.69 235.86
CG 14.89 12.34 15.28 17.09 32.54 138.62 328.37 883.67
Netflix Dating AVERAGE TIME TAKEN ( SECONDS ) TO QUERY A BATCH OF
TABLE II .
INDICES FROM THE MATRIX . THE CONDITIONAL GAUSSIAN BASED
METHOD IS DENOTED AS CG
V . EXTENSION TO ACTIVE LEARNING
As mentioned previously , the proposed frameworks are versatile and the same algorithms can be used to address different variants of the active learning problem . To this end , the labeled and the unlabeled instances are concatenated into a matrix where the missing entries correspond to the missing features / class labels . The algorithms can then be used to query the feature values / class labels of a batch of salient instances for manual annotation . To apply the active matrix completion algorithms for active learning , the class labels are represented as {−1 , 1} ; during prediction , the matrix is completed and if the value of a label entry is positive , it is discretized as 1 and if it negative , it is discretized as −1 ( similar to the approach in [ 32] ) . We consider four different problem settings , which are detailed below ( due to space constraints , we present results with one dataset from each setting ) :
Transductive Active Learning : The concept of transductive inference as introduced by Vapnik [ 33 ] . A transductive learning problem is different from an inductive problem in at least two respects [ 34 ] . First , the learning algorithm does not necessarily have to learn a general rule , it only needs to predict accurately for a finite number of test examples . Thus , it has the obvious advantage of not having to specify a model apriori . Secondly , the test examples are known apriori and can be observed by the learning algorithm during training . This allows the learning algorithm to exploit any information that might be contained in the test samples . The Breast Cancer dataset from the UCI Machine Learning Repository was used here .
Multi label Active Learning : Multi label classification is a generalization of conventional classification problem , where each data sample can have multiple labels . In a multi label learning problem , the labels share an inherent correlation and for each selected sample , only some effective labels need to be annotated while others can be infered by exploring the label correlations . An effective way to exploit the label correlations is through low rank matrix completion . Minimizing the rank of the data matrix provides a natural way to exploit the dependencies among the labels of a multi label sample . Thus ,
( a ) Transductive Active Learning : Breast Cancer
( b ) Multi label Active Learning : Scene
( c ) Active Learning in Regression : FacePix
( d ) Active Feature Acquisition : Spect
Fig 4 . Extension of Active Matrix Completion to Active Learning ( Figures best viewed in color ) the proposed active matrix completion methodologies can be judiciously used to query the informative sample label pairs and efficiently model the label correlations . The multi label Scene dataset ( with 6 classes ) was used in this experiment .
Active Learning in Regression : Contrary to classification problems , where the labels are discreet , the labels in a regression problem are continuous . For the proposed algorithms , the matrix entries being discreet or continuous is unimportant , so the exact same methods can be applied to regression settings . We used the FacePix pose dataset [ 35 ] ( which contains the images of subjects under natural conditions , with head poses ranging from −90o to 90o ) in this experiment .
Active Feature Acquisition : The incomplete data problem , in which certain features are missing for particular data points , exist in a wide variety of fields , including social sciences , computer vision , biological systems , medical diagnosis and remote sensing . In a medical diagnosis application , for instance , the data for a particular patient may have missing features corresponding to the medical tests that could not be performed due to the associated time / costs . In such is possible to acquire the missing data an application , at a cost deciding which medical tests to administer is equivalent to deciding which missing data values to acquire . it
In contrast to conventional active learning , which selects the most informative samples to label , this process would select the most informative features to acquire . The Spect dataset from the UCI Machine Learning Repository , represents a natural setting to develop predicitve models to classify a subject as normal / abnormal , where the acquisition of features has an associated cost . This dataset was therefore used in our work . The results are depicted in Figure 4 and corroborate the efficacy of the proposed active matrix completion algorithms over random sampling in achieving low generalization error . This also emphasizes the generalizibility of the proposed frameworks as the same algorithms can be used to address several variants of the active learning problem .
VI . CONCLUSIONS AND FUTURE WORK
In this paper , we presented novel algorithms to leverage the intelligence of human oracles to actively complete a partially observed data matrix . We presented two ensemblebased methods Query by Committee and a Stability based method and a strategy based on Conditional Gaussian distributions to compute the uncertainty of prediction of every missing entry in the matrix . The top k entries were then selected for manual annotation , where k is the desired batch size . Our results corroborated the advantage of active matrix completion over passive completion and also the efficacy of the algorithms in appropriately identifying a set of missing entries over random sampling in reducing the reconstruction error . Previous research efforts in this domain are mostly tailored towards collaborative filtering applications and focus on actively selecting a set of promising missing entries to infer the interests of a new user . It is non trivial to extend such techiques to related active learning problem domains . On the other hand , the proposed frameworks are versatile and flexible and can be easily extended to solve different variants of active learning like transductive and multi label active learning , active learning in regression and active feature acquisition .
An important direction of future research is to prove performance guarantees of the proposed active matrix completion frameworks . There is a rich body of literature on the theoretical guarantees of the Query by Committee ( QBC ) algorithm in active learning [ 5 ] , [ 36 ] , which quantify the expected number of queries required to achieve a certain level of generalization error . It will be interesting to derive analogous bounds for our QBC based algorithms . Further , for the method based on Conditional Gaussians , the variance of each unobserved entry for a particular observation was computed from the diagonal elements of the covariance matrix . However , apart from the variance , the covariance matrix also provides the correlation among the different entries ( given by the offdiagonal elements ) . It will be interesting to factor the redundancy ( correlation ) among the different entries in the batch selection criterion for this framework and observe the impact on the results . We also plan to extend the proposed frameworks to tensor completion , which has been used to estimate missing values in visual data [ 37 ] .
REFERENCES
[ 1 ] E . Candes and T . Tao , “ The power of convex relaxation : Near optimal matrix completion , ” in IEEE Transactions on Information Theory , 2010 . [ 2 ] B . Recht , “ A simpler approach to matrix completion , ” in Journal of
Machine Learning Research ( JMLR ) , 2011 . J . Cai , E . Candes , and Z . Shen , “ A singular value thresholding algorithm for matrix completion , ” in SIAM Journal on Optimization , 2010 .
[ 3 ]
[ 4 ] S . Tong and D . Koller , “ Support vector machine active learning with applications to text classification , ” in Journal of Machine Learning Research ( JMLR ) , 2000 .
[ 5 ] Y . Freund , S . Seung , E . Shamir , and N . Tishby , “ Selective sampling using the query by committee algorithm , ” in Machine Learning , 1997 . [ 6 ] Y . Guo and D . Schuurmans , “ Discriminative batch mode active learning , ” in Adavances in Neural Information Processing Systems ( NIPS ) , 2007 .
[ 7 ] B . Settles , “ Active learning literature survey , ” in Technical Report 1648 ,
University of Wisconsin Madison , 2010 .
[ 8 ] C . Campbell , N . Cristianini , and A . Smola , “ Query learning with large margin classifiers , ” in International Conference on Machine Learning ( ICML ) , 2000 .
[ 9 ] D . Cohn , Z . Ghahramani , and M . Jordan , “ Active learning with statistical models , ” Journal of Artificial Intelligence Research ( JAIR ) , 1996 . [ 10 ] R . Liere and P . Tadepalli , “ Active learning with committees for text categorization , ” National Conference on Artificial Intelligence , 1997 .
[ 11 ] Y . Guo and R . Greiner , “ Optimistic active learning using mutual information , ” in International Joint Conference on Artificial Intelligence ( IJCAI ) , 2007 .
[ 12 ] S . Hoi , R . Jin , and M . Lyu , “ Large scale text categorization by batch mode active learning , ” in ACM World Wide Web Conference , 2006 .
[ 13 ] S . Hoi , R . Jin , J . Zhu , and M . Lyu , “ Semi supervised SVM batch mode active learning for image retrieval , ” in IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , 2008 .
[ 14 ] Y . Guo , “ Active instance sampling via matrix partition , ” in Adavances in Neural Information Processing Systems ( NIPS ) , 2010 .
[ 15 ] N . Rubens , D . Kaplan , and M . Sugiyama , “ Active learning in recom mender systems , ” in Recommender Systems Handbook , 2011 .
[ 16 ] C . Boutilier , R . Zemel , and B . Marlin , “ Active collaborative filtering , ”
[ 17 ] in Uncertainty in Artificial Intelligence ( UAI ) , 2002 . I . Rish and G . Tesauro , “ Active collaborative prediction with maximum margin matrix factorization , ” in Information Theory and Applications Workshop , 2007 .
[ 18 ] R . Jin and L . Si , “ A bayesian approach toward active learning for collaborative filtering , ” in Uncertainty in Artificial Intelligence ( UAI ) , 2004 . J . Silva and L . Carin , “ Active learning for online bayesian matrix factorization , ” in ACM Conference on Knowledge Discovery from Data ( KDD ) , 2012 .
[ 19 ]
[ 20 ] A . Harpale and Y . Yang , “ Personalized active learning for collaborative filtering , , ” in ACM SIGIR conference on Research and development in information retrieval , 2008 .
[ 21 ] R . Karimi , C . Freudenthaler , A . Nanopoulos , and L . Schmidt Thieme , “ Comparing prediction models for active learning in recommender systems , ” in Annual Conference of the German Classification Society , 2010 .
[ 22 ] —— , “ Exploiting the characteristics of matrix factorization for active learning in recommender systems , ” in ACM Conference on Recommender Systems , 2012 .
[ 23 ] —— , “ Non myopic active learning for recommender systems based on matrix factorization , ” in IEEE Information Reuse and Integration ( IRI ) , 2011 .
[ 24 ] —— , “ Towards optimal active learning for matrix factorization in recommender systems , ” in IEEE International Conference on Tools with Artificial Intelligence , 2011 .
[ 25 ] M . Yuan and Y . Lin , “ Model selection and estimation in the gaussian graphical model , ” in Biometrika Journal , 2007 . J . Friedman , T . Hastie , and R . Tibshirani , “ Sparse inverse covariance estimation with the graphical lasso , ” in Biostatistics Journal , 2007 .
[ 26 ]
[ 27 ] N . Stadler and P . Buhlmann , “ Missing values : sparse inverse covariance estimation and an extension to sparse regression , ” in Journal of Statistics and Computing , 2012 .
[ 28 ] C . M . Bishop , “ Pattern recognition and machine learning . ” Springer ,
Oct . 2007 .
[ 29 ] T . Hastie , R . Tibshirani , G . Sherlock , M . Eisen , P . Brown , and D . Botstein , “ Imputing missing data for gene expression arrays , ” in Technical Report , Stanford University , 1999 .
[ 30 ] R . Mazumder and T . Hastie , “ Exact covariance thresholding into connected components for large scale graphical lasso , ” in Journal of Machine Learning Research ( JMLR ) , 2012 .
[ 31 ] Z . Wen , W . Yin , and Y . Zhang , “ Solving a low rank factorization model for matrix completion by a nonlinear successive over relaxation algorithm , ” in Mathematical Programming Computation , 2010 .
[ 32 ] A . Goldberg , X . Zhu , B . Recht , J . Xu , and R . Nowak , “ Transduction with matrix completion : Three birds with one stone , ” in Advances in Neural Information Processing Systems ( NIPS ) , 2010 .
[ 33 ] V . Vapnik , “ Statistical learning theory , ” in Wiley Interscience , 1998 . [ 34 ] O . Chapelle , B . Scholkopf , and A . Zien , “ Semi supervised learning , ” in
MIT Press , 2006 .
[ 35 ] G . Little , S . Krishna , J . Black , and S . Panchanathan , “ A methodology for evaluating robustness of face recognition algorithms with respect to changes in pose and illumination angle , ” in International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) , 2005 .
[ 36 ] P . Sollich and D . Saad , “ Learning from queries for maximum information gain in imperfectly learnable problems , ” in Advances in Neural Information Processing Systems ( NIPS ) , 1995 . J . Liu , P . Musialski , P . Wonka , and J . Ye , “ Tensor completion for estimating missing values in visual data , ” in IEEE International Conference on Computer Vision ( ICCV ) , 2009 .
[ 37 ]
