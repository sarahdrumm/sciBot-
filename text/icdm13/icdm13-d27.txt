3 1 0 2 p e S 6 2
]
G L . s c [
1 v 2 8 9 7
.
9 0 3 1 : v i X r a
On the Feature Discovery for App Usage Prediction in Smartphones
Zhung Xun Liao∗ , Shou Chun Li∗ , Wen Chih Peng∗ and Philip S Yu†
∗Department of Computer Science ,
National Chiao Tung University , HsinChu , Taiwan
Email : {zxliao.cs96g , scli.cs02g , wcpeng}@nctuedutw
†Department of Computer Science ,
University of Illinois at Chicago , Chicago , IL , USA
Email : psyu@csuicedu
Abstract—With the increasing number of mobile Apps developed , they are now closely integrated into daily life . In this paper , we develop a framework to predict mobile Apps that are most likely to be used regarding the current device status of a smartphone . Such an Apps usage prediction framework is a crucial prerequisite for fast App launching , intelligent user experience , and power management of smartphones . By analyzing real App usage log data , we discover two kinds of features : The Explicit Feature ( EF ) from sensing readings of built in sensors , and the Implicit Feature ( IF ) from App usage relations . The IF feature is derived by constructing the proposed App Usage Graph ( abbreviated as AUG ) that models App usage transitions . In light of AUG , we are able to discover usage relations among Apps . Since users may have different usage behaviors on their smartphones , we further propose one personalized feature selection algorithm . We explore minimum description length ( MDL ) from the training data and select those features which need less length to describe the training data . The personalized feature selection can successfully reduce the log size and the prediction time . Finally , we adopt the kNN classification model to predict Apps usage . Note that through the features selected by the proposed personalized feature selection algorithm , we only need to keep these features , which in turn reduces the prediction time and avoids the curse of dimensionality when using the kNN classifier . We conduct a comprehensive experimental study based on a real mobile App usage dataset . The results demonstrate the effectiveness of the proposed framework and show the predictive capability for App usage prediction .
Keywords Mobile Application ; Usage Prediction ; Classifica tion ; Apps ;
I . INTRODUCTION
With the increasing number of smartphones , mobile applications ( Apps ) have been developed rapidly to satisfy users’ needs [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] . Users can easily download and install Apps on their smartphones to facilitate their daily lives . For example , users use their smartphones for Web browsing , shopping and socializing [ 5 ] , [ 6 ] . By analyzing the collected real Apps usage log data , the average number of Apps in a user ’s smartphone is around 56 . For some users , the number of Apps is up to 150 . As many Apps are installed on a smartphone , users need to spend more time swiping screens and finding the Apps they want to use . From our observation , each user has on average 40 launches per day . In addition , the launch delay of Apps becomes longer as their functionality becomes more complicated . In [ 7 ] , the authors investigated the launch delay of Apps . Even simple Apps ( eg , weather report ) need 10 seconds , while complicated Apps ( eg , games ) need more than 20 seconds to reach a playable state . Although some Apps could load stale content first and fetch new data simultaneously , they still need several seconds to complete loading .
To ease the inconvenience of searching for Apps [ 8 ] , [ 9 ] and to reduce the delay in launching Apps [ 7 ] , one possible way is to predict which Apps will be used before the user actually needs them . Although both the iOS and Android systems list the most recently used ( MRU ) Apps to help users relaunch Apps , this method only works for those Apps which would be immediately relaunched within a short period . Another common method is to predict the most frequently used ( MFU ) Apps . However , when a user has a lot of frequently used Apps , the MFU method has very poor accuracy . In our experiments , these two methods are the baseline methods for comparison .
Recently , some research works have addressed the Apps usage prediction problems [ 7 ] , [ 8 ] , [ 9 ] . In [ 8 ] , a temporal profile is built to represent the usage history of an App . The temporal profile records the usage time and usage period of the App . Then , when a query time is given , the usage probability of each App could be calculated through comparing the difference between the temporal profile and the query time . However , since they only consider the periodicity feature of Apps , some Apps with no significant periods cannot be predicted by their temporal profiles . In [ 7 ] , the authors adopted three features to predict Apps usage : time , location , and used Apps . Based on those three features , they designed and built a system to remedy slow App launches . However , they always use these three features to predict different users’ usage , which is impractical as users could have different usage behavior . For example , the location information could be less useful for those users who have lower mobility . We claim that the features which are able to accurately predict Apps usage are different for different users and different Apps . The authors in [ 9 ] collected 37 features from accelerometer , Wi Fi signal strength , battery
Hardware Sensors
Apps
Usage Log
Explicit Feature
Implicit Feature
Personalized
Feature Selection kNN
Classifier
AUG Graph
Figure 1 . Overview of kNN based App Prediction framework . level , etc . , and proposed a Naive Bayes classification method to predict Apps usage . However , a Naive Bayes classification method needs sufficient training data to calculate the conditional probability , which does not always hold . Therefore , the system would fail to predict Apps if there are not exactly the same instances existing in the training dataset . In addition , they still apply all the same features to each user , instead of selecting personalized features for different users with different usage behaviors .
In this paper , we adopt the concept of minimum description length ( MDL ) to select personalized features for different users and propose a kNN based App Prediction framework , called KAP , to predict Apps usage . Once we distinguish the useful and useless features , only the useful features need to be collected . Therefore , the size of the log data could be reduced . The overall framework is shown in Figure 1 . KAP investigates features from both explicit and implicit aspects . The explicit feature is a set of sensor readings from built in hardware sensors , such as GPS , time , accelerometers , etc . On the other hand , the implicit feature is referred to as the correlations of Apps usage . To capture these correlations , the implicit feature is represented as the transition probability among Apps .
For the explicit feature , we focus on three types of hardware sensors : 1 ) device sensors , such as free space , free ram , and battery level , 2 ) environmental sensors , such as time , GSM signal , and Wi Fi signal , and 3 ) personal sensors : acceleration , speed , heading , and location . We claim that the usage of different Apps is related to different types of sensors . Obviously , the advantages of selecting sensors for the explicit feature is that it reduces the effect of noisy data and also saves power and storage consumption for logging data and performing the prediction .
For the implicit feature , we calculate the transition probability for each App . However , the previous works [ 7 ] , [ 9 ] only take the usage order into account , and not the time duration between Apps . We claim that the length between Apps usage means different things . For example , users may take pictures via a camera App and upload those pictures to Facebook . However , some users may upload pictures immediately , while others would upload them when they have a Wi Fi connection . Therefore , the time duration be tween camera and Facebook use depends on different users and different usage behaviors . To model the usage relation among Apps , an Apps Usage Graph ( AUG ) , which is a weighted directed graph , is proposed . The weight on each edge is formulated as an exponential distribution to describe the historical usage durations . Based on AUG , the implicit feature of each training instance is derived by traversing the AUG . Consequently , the implicit feature of each testing case is derived by an iterative refinement process .
With both explicit and implicit features , KAP adopts a kNN classification model to predict Apps usage which is represented as class labels . In the experimental study , the proposed KAP framework outperforms both baseline methods and achieves accuracy of 95 % . We also show that the personalized sensor selection for the explicit feature is efficient and effective . In addition , the implicit feature is useful for improving the prediction accuracy of KAP .
The major contributions of this research work are sum marized as follows .
• We address the problem of Apps usage prediction by discovering different feature sets to fulfill different users’ Apps usage behavior , and propose the concept of explicit and implicit features for Apps usage prediction . • We estimate the distribution of the transition probability among Apps and design an Apps Usage Graph ( AUG ) to model both Apps usage order and transition intervals . Two algorithms are proposed to extract the implicit features from the AUG graph for training and testing purposes respectively .
• We propose a personalized feature selection algorithm in which one could explore MDL to determine a personalized set of features while still guaranteeing the accuracy of the predictions .
• A comprehensive performance evaluation is conducted on real datasets , and our proposed framework outperforms the state of the art methods [ 9 ] .
The rest of this paper is organized as follows . Section II investigates the related works which discuss the conventional prediction problem and Apps usage prediction . Section III introduces the explicit and implicit features . Section IV presents the mechanism of personalized feature selection . Section V conducts extensive and comprehensive experiments . Finally , this paper is concluded with Section VI .
II . RELATED WORKS
To the best of our knowledge , the prediction problem of Apps usage in this paper is quite different from the conventional works . We focus on not only analysing usage history to model users’ behavior , but on personalizing varied types of features including hardware and software sensors attached to smartphones . The proposed algorithm selects different features for different users to satisfy their usage behavior . Although there have been many research works solving the prediction problem in different domains , such as music items or playlist prediction [ 10 ] , dynamic preference prediction [ 11 ] , [ 12 ] , location prediction [ 13 ] , [ 14 ] , [ 15 ] , social links prediction [ 16 ] , [ 17 ] , and so on , the prediction methods are only based on analysing the usage history . In [ 18 ] , the author selected features from multiple data streams , but the goal is to solve the communication problem in a distributed system .
Currently , only a few studies discuss mobile Apps usage prediction . Although the authors in [ 19 ] adopted location and time information to improve the accuracy of Apps usage prediction , the total number of Apps is only 15 . Concurrently , in [ 20 ] , the authors stated that the prediction accuracy could achieve 98.9 % , but they still only focus on predicting 9 Apps from a set of 15 . In [ 7 ] , the authors solved the prediction problem through multiple features from 1 ) location , 2 ) temporal burst , and 3 ) trigger/follower relation . However , they did not analyze the importance of each feature . Therefore , for different users , they always use the same three features to predict their Apps usage . In [ 9 ] , the authors investigated all possible sensors attached to a smartphone and adopted a Naive Bayes classification to predict the Apps usage . However , collecting all possible sensors is inefficient and impractical . Moreover , the useful sensors for different users could vary according to users’ usage behavior . We claim that for different users , we need to use different sets of features to predict their usage . In this paper , we collect only the subset of all features which are personalized for different users .
This paper is the first research work which discusses how to select suitable sensors and features for different users to predict their Apps usage . Through the personalized feature selection , we could perform more accurate predictions for varied types of usage bahavior , reduce the dimensionality of the feature space , and further save energy and storage consumption . In addition , the proposed KAP framework derives the implicit feature by modelling the usage transition among Apps .
III . EXPLICIT AND IMPLICIT FEATURES
In this paper , we separate the features into two main categories : the explicit feature and the implicit feature . The explicit feature represents the sensor readings which are explicitly readable and observable . The implicit feature is the Apps usage relations .
A . Explicit Feature Collection
Table I shows the hardware sensors we use for the explicit feature . As different models of smartphones could have different sets of hardware sensors , we only list the most common ones whose readings are easy to record . It is totally free to add or remove any hardware sensors here .
To show the prediction ability of different types of mobile sensors , we randomly select two users from the collected dataset and perform kNN classification via the three types
HARDWARE SENSORS FOR THE EXPLICIT FEATURE .
Table I
Sensors
Location
Time
Battery
Accelerometer
Wi Fi Signal GSM Signal
System
Contextual Information Longitude Latitude Altitude Location Cluster Hour of day Day of week Battery Level Charging status Avg . and std . dev . of {x , y , z} Acceleration changes speed Heading Received signal Signal Strength Free space of each drive Free RAM l l a c e R l l a c e R
1
0.8
0.6
0.4
0.2
0
Messenger Contacts Browser
Application
Device Environmental
Personal
( a ) U ser1
1
0.8
0.6
0.4
0.2
0
Messenger Contacts Browser
Apllication
Device Environmental
Personal
( b ) U ser2
Figure 2 . Varied recalls of predicting Apps usage via different types of sensors for different users . of sensors respectively to predict their Apps usage . Figure 2 shows the prediction recall of ” Messenger ” , ” Contacts ” , and ” Browser ” for the two users . As can be seen in Figure 2 , personal sensors would be a good explicit feature for predicting user1 ’s Apps usage , while environmental sensors are good for user2 . The reason is that user2 probably needs a Wi Fi signal to access the Internet .
B . Implicit Feature Extraction
The implicit feature formulates the usage transitions among Apps in a usage session . As mentioned in [ 7 ] , users use a series of Apps , called a usage session , to complete a specific task . For example , one user could use ” Maps ” when travelling to a sightseeing spot , then use camera to take pho y t i l i b a b o r P
0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0
Transition Interval
^ p11
^ p12
1
^ p31
2
^ p23
3
1.1x
^ p11(x)=0.4 e ^ p12(x)=0.45 e ^ p23(x)=0.43 e ^ p31(x)=0.1 e
0.8x
1.2x
0.5x
0
5
10
15
20
25
30
Time(Minute )
Figure 3 . The PDF of the duration of two consecutive App launches . tos , and upload those photos to Facebook . Thus , the series of using ” Maps ” , ” Camera ” and ” Facebook ” is called a usage session , denoted as ” Map ” δ1−→ ” Camera ” δ2−→ ” Facebook ” , where δ1 and δ2 represent the transition intervals .
The implicit feature of ” Facebook ” in this usage session is thus < pM F ( δ1 ) , pCF ( δ1 + δ2 ) , pF F ( ∞ ) > , where pM F ( · ) , pCF ( · ) , and pF F ( · ) are probability models which represent the probability of using ” Maps ” , ” Camera ” and ” Facebook ” respectively before using ” Facebook ” with the transition interval as the random variable . Note that because there is no ” Facebook ” to ” Facebook ” in this usage session , the transition interval is thus set to ∞ and then the probability would be 0 .
The probability model could be estimated from a user ’s historical usage trace . In this section , we introduce an Apps Usage Graph ( AUG ) which models the transition probability among Apps for a single user . For training purposes , the implicit features for the training usage sessions are derived by traversing the AUG . However , for testing purposes , since we do not know which is the App to be invoked , the derivation of the implicit feature for the training usage session cannot be utilized directly . Therefore , an iterative refinement algorithm is proposed to estimate both the next App and its implicit feature simultaneously . The following paragraphs will illustrate the details of the AUG construction and the implicit feature derivation for both the training and testing usage sessions .
1 ) Apps Usage Graph ( AUG ) : For each user , we construct an Apps Usage Graph ( AUG ) to describe the transition probability among Apps . An AUG is a directed graph where each node is an App , the direction of an edge between two nodes represents the usage order , and the weight on each edge is a probability distribution of the interval between two Apps . Since two consecutive launches could be viewed as a Poisson arrival process , we can formulate the intervals between two launches as an exponential distribution . For example , Figure 3 shows the probability density function ( PDF ) of two consecutive launches which exactly fulfils the exponential distribution where most transitions ( eg , 0.45 % ) are within 1 minute .
Here , Equation 1 formulates the exponential density function of the launch interval being in [ x , x+ 1 ) . The parameter
Figure 4 . An example of the Apps Usage Graph ( AUG ) .
α = ˆp(0 ) is derived by assigning x = 0 in Equation 1 , and could be calculated by p(0 ) , the real probability derived from the training data . Then , β is solved by minimizing ˆp(i ) and the difference between the estimated probability the real probability p(i ) as shown in Equation 2 for every interval i .
Empirically , we do not need to fit every interval when obtaining the exponential model . For example , in Figure 3 , only the first 5 intervals already cover more than 75 % of the training data . Therefore , we can iteratively add one interval until the data coverage reaches a given threshold . We will discuss the impact of the data coverage threshold in the experiments section .
ˆp(x ) = α exp−βx
( 1 )
β = argmin
β X β X i i
= argmin
| ˆp(i ) − p(i)|
|p(0 ) exp−βi −p(i)|
( 2 )
For example , Figure 4 shows an AUG with three Apps . From Figure 4 , the probability of two consecutive usages of 0.3−−→ App1 ) App1 with an interval of 0.3 minutes ( ie , App1 1.5−−→ App2 is 02 Although AUG only takes is 0.4 , and App1 two consecutive Apps into account , such as p12 and p23 , the probability of p13 , could be calculated by p12 × p23 .
1t , p′
2t , , p′
2 ) Implicit Features for Training : For each training case , the implicit features are derived by looking up the AUG . Suppose the currently used App ( ie , class label ) is Appt , the implicit feature is thus , < p′ nt > , where p′ it represents the probability of transiting from Appi to any random Apps and then to Appt . The probability of p′(s ) is defined as in Equation 3 which is the summation of every probability from Appi to Appt . Note that we use a superscript , s , to indicate how many Apps are between Appi and Appt , and Appmk is the k th App after Appi . Once we derive the implicit feature in a reverse time order , the sub problem of estimating p′(s−k ) is already solved . The calculation of the implicit feature for Appi stops when the transition probability falls below a given threshold , mintp . In our collected dataset , the transition probability falls to mk,t it
App1
0.5 →App3
App1
'(0 ) p13=p13=0
^
App2
App3
App2
0.5 →App1
0.5 →App3
'(1 ) '(0 ) p23 =p23+p21p13
^
^
=0.13
App1
1 →App2
0.5 →App1
0.5 →App3
'(2 ) '(0 ) p13=p13+p12p23+p11p13
'(1 )
^
^
^
=0+009x013+0 =0.01
0.01
0.13
0
Figure 5 . Steps of obtaining the implicit feature of App3 in the training case , · · · → App1
0.5 −−→ App3 .
0.5 −−→ App1
1 −→ App2
0.1 % when we look backward to more than 5 Apps , which is the default parameter for mintp . Algorithm 1 depicts the derivation of the implicit feature for a training case with Appt as its class label . p′(s ) it = ˆpit + X k
ˆpi,mk × p′(s−k ) mk,t
( 3 )
Algorithm 1 : Deriving the implicit feature of Appt for training . Input : Appt : a training App Output : IFt : the implicit feature of Appt
1 foreach Appi prior than Appt do 2 3
IFt[i ] ← IFt[i ] + ˆpit(δit ) ; foreach Appm between Appi and Appt do pim(δjm ) × IFm[t ] ;
IFt[i ] ← IFt[i ] +
ˆ end
4 5 6 end 7 return IFt
1−→ App2
For example , suppose we have an AUG as shown in 0.5−−→ Figure 4 and a usage trace as · · · → App1 0.5−−→ App3 → . . . . Figure 5 shows the process of App1 obtaining the implicit feature of App3 . We first estimate p′(0 ) 13 0.5−−→ 0.5−−→ App3 , then p′(1 ) from App1 23 0.5−−→ App3 , and finally update p′(2 ) 13 is reused for calculating App1 p′(1 ) 23 , and p′(1 ) 13 . The implicit feature of App3 is < 0.01 , 0.13 , 0 > .
0.5−−→ App1 from App2 from App1 1−→ App2
13 are reused for updating p′(2 )
0.5−−→ App3 . Note that p′(0 )
23 and p′(0 )
13 invocation , Appt ,
3 ) Implicit Features for Testing : Since the App to be is unknown for predicted for current testing , the derivation process of implicit features for training does not work . We propose an iterative refinement algorithm to estimate both Appt and its implicit feature , IFt , for testing . Suppose θi is the probability of Appt = Appi , the implicit feature IFt is calculated as in Equation 4 which is a linear combination of the IF of each Appi . In addition , M = [ IF T 2 , . . . ] represents the transition matrix
1 , IF T
1 , IF T among Apps , where IF T 2 , . . . are column vectors . Then , the value of θi could be updated by Equation 5 , which is the probability of staying in Appi after one step walking along the transition matrix M . We keep updating θi and IFt iteratively , until Appt is fixed to one specific App . In our experiments , the iterative refinement process converges in about 3 iterations . Algorithm 2 depicts the derivation of the implicit feature for testing .
IFt = X
Appi
θi × IFi
θi = X
Appm
IFt[m ] × M [ m][i ]
( 4 )
( 5 )
Algorithm 2 : Deriving the implicit feature for testing . Input : t : a testing case Output : IFt : the implicit feature at t foreach θj do
IFt ← IFt + θi × IFi ; end foreach Appi prior than time t do θi ← θi + IFt[m ] × M [ m][i ] ; Normalize θi ;
1 while iter < threshold do 2 3 4 5 6 7 8 9 10 end 11 return IFt end iter ← iter + 1 ;
 
0.5−−→ App1
0.49 0.6 0.01 0.13
App , and the transition matrix M =
For example , suppose the testing case is · · · → App1
1−→ 0.5−−→ Appt . First , we initialize θi as App2 < 1/3 , 1/3 , 1/3 > , which gives equal probability to each  ,  which is derived by calculating the IF of each App shown in Equation 3 . Note that the last row is all zero because there is no App3 transiting to any other Apps . Then , the implicit feature is < 0.37 , 0.04 , 0 > in the first iteration . Next , θi is updated to < 0.18 , 0.22 , 0.01 > , and normalized as < 0.44 , 0.54 , 0.02 > according to one step walk in M with the calculated implicit feature as the prior probability . Then , we can obtain the implicit feature as < 0.53 , 0.01 , 0 > in the second iteration .
0 0
0 0
0
IV . PERSONALIZED FEATURE SELECTION
The goal of the personalized feature selection is to use as fewer features as possible to guarantee an acceptable accuracy . Due to the energy and storage consumption of collecting sensors readings and Apps transition relations , we should select useful features for different users in advance .
Furthermore , through the personalized feature selection , we could avoid the curse of dimensionality on performing the kNN . We first apply the personalized feature selection on the training data , and then only the selected features are required to be collected in the future .
Here , we propose a greedy algorithm to select the best feature iteratively . We adopt the concept of Minimum Description Length ( MDL ) [ 21 ] , [ 22 ] to evaluate the goodness of the features . For different features , we can have varied projections of the training data . We claim that if a feature needs fewer bits to describe its data distribution , it is good for predicting the data . Therefore , in each iteration , the feature with the minimum description length is selected . Then , those data points which are correctly predicted are logically eliminated from the training data , and the next feature is selected by the same process repeatedly . We define the description length of the hypothesis , which is shown in Equation 6 , as the length of representing the training data . N G(Appi ) is the number of groups of Appi . The description length of Data given the hypothesis is the total number of miss classified data which is formulated as in Equation 7 .
L(H ) = X i log2 N G(Appi )
( 6 ) i
( 7 ) log2(missClassif ied(Appi ) + 1 )
L(D|H ) = X For example , given 8 data points in the training data and three features as shown in Figure 6 . In the first round , Time is the feature with minimum description length . Those data points marked as red are correctly predicted and will be removed . Therefore , in the second round , only two data points are left , and the feature of Wi Fi signal is selected due to its minimum description length .
The selection process stops when a percentage of ρ of the training data is covered . We also discuss the impact of ρ in the experimental section . Note that the number of features affects the energy and storage consumption and is set according to the capability of the smartphones . Algorithm 3 depicts the process of personalized feature selection . After the selection , only the readings of the sensors which are selected will be collected as the explicit feature in the future . In addition , only the selected Apps will be used to construct AUG .
V . EXPERIMENTAL STUDY
In this section , we conduct a comprehensive set of experiments to compare the performance of the proposed KAP framework with other existing methods including 1 ) most frequently used ( MFU ) method , 2 ) most recently used ( MRU ) method which is the built in prediction method in most mobile OS , such as Android and iOS , 3 ) SVM , 4 ) App Naive Bayes [ 9 ] , 5 ) Decision Tree , and 6 ) AdaBoost . In the following , we first discuss the collected dataset , then fi fi selection
1 1 2 2
1 1
0
Battery
2 3
1
2 1 1
2 2
1 1 3
0
1
Time
•
•
L(H)=log2(2+2+1 ) L(D|H)=log2(0+1+1+1 )
• MDL=log2 5fi+filog2 3
•
•
L(H)=log2(2+2+1 ) L(D|H)=log2(0+2+1+1 )
• MDL=log2 5fi+filog2 4
Nextfiround
2 3
1
0
Battery
•
•
L(H)=log2(1+1 ) L(D|H)=log2(0+1+1 ) • MDL=log2 2fi+filog2 2
0
•
0
• fi fi
11
2
1
2
3
1
2
Wi(cid:882)FifiSignal
1
L(H)=log2(4+3+1 ) L(D|H)=log2(0+0+0+1 )
• • MDL=log2 8
2
3
Wi(cid:882)FifiSignal
1
L(H)=log2(1+1 ) L(D|H)=log2(0+0+1 )
• • MDL=log2 2
Figure 6 . An example of feature selection where the red data points are correctly predicted .
Algorithm 3 : Personalized feature selection . Input : Dz : the training data Output : P F : the personalized features
1 Let Nz ← |Dz| ; 2 while |Dz| < ρNz do 3 4 foreach feature f do
Calculate DLf : description length for feature f ; end P F ← P F ∪ {argmax
DLf } ; f
Let Da be the set of accurately predicted data points ; Dz ← Dz − Da ;
5 6
7
8 9 end 10 return P F introduce the metrics employed to evaluate the performance , and finally deliver the experimental results .
A . Dataset Description
In this paper , we use a real world dataset collected by a mobile phone company which installed a monitoring program on every volunteer ’s smartphone . In this dataset , we have totally 50 volunteers including college students and faculty from June 2010 to January 2011 . For each user , we separate the dataset into three parts , where each part consists of three months , and we use the first two months as training data , and the last one month as testing data . Totally , there are more than 300 different Apps installed on their smartphones , and the average number of Apps on one smartphone is 56 .
B . Performance Metrics
In this paper , we use two performance metrics : 1 ) average recall and 2 ) nDCG [ 23 ] score .
Average Recall : Since there is only one App being launched in each testing case , recall score is thus adopted as
|C|
I(Appci ,Lci ) one performance metric which evaluates whether the used App is in the prediction list . The recall score of one user is defined as Pci∈C , where C is the set of testing cases , Appci is the ground truth , and Lci is the prediction list at the i th testing case . I(· ) is an indicator function which equals 1 , when Appci ∈ Lci , and equals 0 , otherwise . Finally , the average recall is the average of the recall values of all users . nDCG Score : To evaluate the accuracy of the order of the prediction list , we also test the nDCG score of the prediction results . The IDCG score is fixed to 1 because there is only one used App in the ground truth . The DCG log2(i+1 ) when the used App is predicted at position score is i of the prediction list . Then , nDCG is the average of DCG IDCG for all testing cases .
1
C . Experimental Results
To evaluate the performance of predicting Apps usage by the proposed KAP framework , we first evaluate the overall performance when predicting different numbers of Apps . Then , we test the performance of the personalized feature selection algorithm . The impact of different parameters for the KAP framework and kNN classification is also included . Note that we use top k = 4 , kNN=40 % , and the minimum data coverage of personalized feature selection as 70 % to be the default parameter settings throughout the experiment .
1 ) Overall Performance : First , we evaluate the performance KAP and other different methods under various numbers of prediction , k . As can be seen in Figure 7 , when the number of prediction k increases , both the recall and nDCG values also increase . However , KAP , MRU , MFU , and SVM perform better than others . In Figure 7(a ) , when k = 9 ( the number of predictions shown in the latest Android system ) , the recall of KAP could be more than 95 % , while it is only about 90 % for MFU , MRU , and SVM . On the other hand , the nDCG value of KAP shown in Figure 7(b ) is always higher than that of the other methods , which means the prediction order of KAP is better .
5+3
Second , we test the accuracy of varied top k frequency . The top k frequency is defined as the ratio of the usage of the most frequent k Apps . For example , if a user has 5 Apps and the usage counts are 3 , 1 , 2 , 5 , and 2 , the top 2 frequency is thus 13 . Figure 8 shows the results when top k = 4 . Intuitively , when the top k frequency increases , the accuracy of the MFU method could be better . However , in Figure 8(a ) , even when the ratio is 0.9 , the MFU method performs just slightly better than the MRU method , but worse than both KAP and SVM . In Figure 8(b ) , the prediction order of KAP is also better than the results of the other methods .
3+1+2+5+2 = 8
2 ) Impact of Personalized Feature Selection : For the proposed KAP method , we evaluate the performance of the personalized feature selection to see if the proposed MDL l l a c e R
G C D n
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
1
2
3
4
5
6
7
8
9
C4.5 AdaBoost App NB
Top k
MFU MRU SVM
( a ) Recall
KAP
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
2
3
4
5
6
7
8
9
C4.5 AdaBoost App NB
Top k
MFU MRU SVM
( b ) nDCG
KAP
Figure 7 .
Impact of the number of prediction , k . l l a c R
G C D n
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
0.5
0.6
0.7
0.8
0.9
Top k Frequency
C4.5 AdaBoost App NB
MFU MRU SVM
KAP
( a ) Recall
0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
0.5
0.6
0.7
0.8
0.9
Top k Frequency
C4.5 AdaBoost App NB
MFU MRU SVM
KAP
( b ) nDCG
Figure 8 .
Impact of top k frequency . based selection algorithm could reduce the used storage when maintaining a good prediction accuracy . For one user , the average used storage and prediction accuracy is shown in Table II under different data coverage ρ . As can be seen in Table II the personalized feature selection could reduce 55 % of training data size and only lose 1 % of recall and 3 % of nDCG when the data coverage is 70 % . In addition , Table III compares the execution time of KAP with and without the personalized feature selection , where the training time is reduced dramatically under ρ = 70 % .
THE STORAGE CONSUMPTION AND ACCURACY UNDER VARIED DATA
COVERAGE ρ .
Table II
Coverage( % ) Storage(KB ) Recall nDCG
30 28 0.78 0.50
40 31 0.78 0.51
50 34 0.80 0.52
60 37 0.80 0.53
70 43 0.82 0.55
80 52 0.82 0.57
90 82 0.82 0.57
100 94 0.83 0.58
THE EXECUTION TIME OF KAP WITH AND WITHOUT PERSONALIZED
Table III
FEATURE SELECTION .
Execution time ( ms ) KAP KAP without selection
Training
Testing
86 185
160 160
Total 246 345
D . Comparison of Different Usage Behavior
Since different users have different usage behaivor , which could extremely affect the prediction accuracy . In this section , we separate users into different groups according to 1 ) number of installed Apps , 2 ) usage frequency , and 3 ) usage entropy . Then , we test the performance of applying different methods on different groups .
1 ) Impact of the Number of Installed Apps : When users launch more Apps , it becomes more difficult to accurately predict Apps usage . Figure 9 shows the recall and ndcg results for a varying number of used Apps . As can be seen in Figure 9 , both the recall and ndcg values decrease when the number of used Apps increases for all methods . However , the decreasing rate of the proposed KAP method is much smoother than that of the others . The recall of KAP is around 85 % while that of the others is below 40 % when the number of used Apps is 30 .
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 10.0 15.0 20.0 25.0 30.0 35.0 40.0 45.0 50.0
Number Of App .
C4.5 AdaBoost App NB
MFU MRU SVM
KAP
( a ) Recall
0.7
0.6
0.5
0.4
0.3
0.2
0.1 l l a c e R
G C D n
0 10.0 15.0 20.0 25.0 30.0 35.0 40.0 45.0 50.0
Number Of App .
C4.5 AdaBoost App NB
MFU MRU SVM
KAP
( b ) nDCG
Figure 9 .
Impact of the number of Apps . l l a c e R
G C D n
0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 300.0
600.0
900.0 1200.0 1500.0 1800.0 2100.0
Number Of Usage
C4.5 AdaBoost App NB
MFU MRU SVM
KAP
( a ) Recall
0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 300.0
600.0
900.0 1200.0 1500.0 1800.0 2100.0
Number Of Usage
C4.5 AdaBoost App NB
MFU MRU SVM
KAP
( b ) nDCG
Figure 10 .
Impact of the usage count .
2 ) Impact of the Usage Count : Now , we test the impact of the usage count . A higher usage count means we could have more training data to learn the classification model for App prediction . Concurrently , it provides more complicated information of users’ usage behavior , and could make noisy data . Figure 10 shows the recall and ndcg values . The performance of KAP , Naive Bayes , and SVM goes up when the usage count increases . However , AdaBoost and Decision Tree have worse performance as the usage count goes up . The result shows that the KAP algorithm can handle more complicated and noisy data .
3 ) Impact of the Entropy of the Apps Usage : We evaluate the impact of the entropy of the Apps usage . Intuitively , as the entropy of the Apps usage becomes larger , the Apps usage is almost random , and the performance of Apps usage prediction would become worse . Figure 11 depicts that the proposed KAP could have around 50 % accuracy when the entropy goes to 3 where the other methods only have accuracy of less than 40 % .
E . Impact of Different Parameters
1 ) Number of Iterations for Implicit Feature Extraction : First , we test the number of iterations of deriving the implicit feature for each testing case . As shown in Table IV , the accuracy stays almost the same after the second iteration . This indicates that the iterative refinement algorithm could converge within 2 iteration which is sufficient to estimate the implicit feature .
2 ) Minimum Probability for Identifying Usage Sessions : As users usage sessions could be varied according to different tasks , we only need the useful length of the usage sessions to perform accurate Apps usage prediction , instead l l a c e R
G C D n
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
1.5
2.0
2.5
3.0
C4.5 AdaBoost App NB
Entropy
MFU MRU SVM
( a ) Recall
KAP
0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
1.5
2.0
2.5
3.0
C4.5 AdaBoost App NB
Entropy
MFU MRU SVM
( b ) nDCG
KAP
Figure 11 .
Impact of the entropy of Apps .
THE RECALL AND NDCG VALUES UNDER VARIED NUMBERS OF
Table IV
ITERATIONS .
#Iterations Recall nDCG
1
0.67 0.43
2
0.79 0.59
3
0.79 0.59
4
0.80 0.60
5
0.81 0.61 of calculate the full usage sessions . Therefore , we conduct this experiment to evaluate the impact of the length of usage sessions . Ac can be seen in Table V , the results are not affected by the minimum transition probability , mintp , too much . From our collected data , the session length is around 2 when mintp is 0.5 , and the best case is under mintp = 0.1 , which has the session length as around 5 .
3 ) Parameters for kNN Classification : Finally , we evaluate the impact of selecting different numbers of neighbors to perform kNN classification . Here , we fix the number of predictions to 4 Apps and compare the recall and nDCG values of KAP and the other methods . Because the training data of different users could vary from several hundreds to thousands . we use a relative value for the number of neighbors . Table VI shows the results of the recall and nDCG values for different number of neighbors . As can be seen in Table VI , even only select 40 % of training data as the neighbors , the recall value is almost 80 % . Therefore , we
THE RECALL AND NDCG VALUES UNDER VARIED MINIMUM
PROBABILITY FOR SESSION IDENTIFICATION .
Table V mintp Recall nDCG
0.5 0.73 0.53
0.25 0.77 0.57
0.1 0.83 0.61
0.075 0.81 0.58
0.05 0.80 0.55
0.025 0.75 0.53
0.001 0.74 0.52
THE RECALL AND NDCG VALUES UNDER VARIED NUMBER OF
NEIGHBORS FOR KNN .
Table VI kNN( % ) Recall nDCG
20 0.74 0.55
40 0.79 0.61
60 0.80 0.63
80 0.80 0.63
100 0.81 0.64 set the default number of neighbor as 40 % throughout the experiments .
VI . CONCLUSION
In this paper , we propose an Apps usage prediction framework , KAP , which predicts Apps usage regarding both the explicit readings of mobile sensors and the implicit transition relation among Apps . For the explicit feature , we consider three different types of mobile sensors : 1 ) device sensors , 2 ) environmental sensors , and 3 ) personal sensors . For the implicit features , we construct an Apps Usage Graph ( AUG ) to model the transition probability among Apps . Then , for each training datum , we could represent the next used App as the implicit feature which describes the probability of transition from other Apps . Note that , since the next App in the testing data is unknown , we propose an iterative refinement algorithm to estimate both the probability of the App to be invoked next and its implicit feature . We claim that different usage behaviors are correlated to different types of features . Therefore , a personalized feature selection algorithm is proposed , where for each user , only the most relative features are selected . Through the feature selection , we can reduce the dimensionality of the feature space and the energy/storage consumption .
We integrate the explicit and implicit features as the feature space and the next used App as the class label to perform kNN classification . In the experimental results , our method outperforms the state or the art methods and the currently used methods in most mobile devices . In addition , the proposed personalized feature selection algorithm could maintain better performance than using all features . We also evaluate the performance of KAP for different types of users , and the results show that KAP is both adaptive and flexible .
REFERENCES
[ 1 ] P . Yin , P . Luo , W C Lee , and M . Wang , “ App recommendation : a contest between satisfaction and temptation , ” in Sixth ACM International Conference on Web Search and Data Mining , WSDM 2013 , Rome , Italy , February 4 8 , 2013 , 2013 , pp . 395–404 .
[ 2 ] T . M . T . Do , J . Blom , and D . Gatica Perez , “ Smartphone usage in the wild : a large scale analysis of applications and context , ” in Proceedings of the 13th International Conference on Multimodal Interfaces , ICMI 2011 , Alicante , Spain , November 14 18 , 2011 , 2011 , pp . 353–360 .
[ 3 ] K . Shi and K . Ali , “ Getjar mobile application recommendations with very sparse datasets , ” in The 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’12 , Beijing , China , August 12 16 , 2012 , 2012 , pp . 204–212 .
[ 14 ] S . Scellato , M . Musolesi , C . Mascolo , V . Latora , and A . T . Campbell , “ Nextplace : A spatio temporal prediction framework for pervasive systems , ” in Pervasive Computing 9th International Conference , Pervasive 2011 , San Francisco , CA , USA , June 12 15 , 2011 . Proceedings , 2011 , pp . 152–169 .
[ 4 ] B . Yan and G . Chen , “ Appjoy : personalized mobile application discovery , ” in Proceedings of the 9th International Conference on Mobile Systems , Applications , and Services ( MobiSys 2011 ) , Bethesda , MD , USA , June 28 July 01 , 2011 , 2011 , pp . 113–126 .
[ 15 ] A . Monreale , F . Pinelli , R . Trasarti , and F . Giannotti , “ Wherenext : a location predictor on trajectory pattern mining , ” in Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , Paris , France , June 28 July 1 , 2009 , 2009 , pp . 637–646 .
[ 16 ] Y . Dong , J . Tang , S . Wu , J . Tian , N . V . Chawla , J . Rao , and H . Cao , “ Link prediction and recommendation across heterogeneous social networks , ” in 12th IEEE International Conference on Data Mining , ICDM 2012 , Brussels , Belgium , December 10 13 , 2012 , 2012 , pp . 181–190 .
[ 17 ] D . Liben Nowell and J . M . Kleinberg , “ The link prediction problem for social networks , ” in Proceedings of the 2003 ACM CIKM International Conference on Information and Knowledge Management , New Orleans , Louisiana , USA , November 2 8 , 2003 , 2003 , pp . 556–559 .
[ 18 ] J . Kogan , “ Feature selection over distributed data streams through convex optimization , ” in Proceedings of the Twelfth SIAM International Conference on Data Mining , Anaheim , California , USA , April 26 28 , 2012 , 2012 , pp . 475–484 .
[ 19 ] M . Matsumoto , R . Kiyohara , H . Fukui , and M . Numao , “ Proposition of the context aware interface for cellular phone operations , ” in 5th International Conference on Networked Sensing Systems , June 17 19 , 2008 , 2008 , pp . 233–233 .
[ 20 ] D . Kamisaka , S . Muramatsu , H . Yokoyama , and T . Iwamoto , “ Operation prediction for context aware user interfaces of mobile phones , ” in 2009 Ninth Annual International Symposium on Applications and the Internet , 2009 , pp . 16–22 .
[ 21 ] J . Rissanen , “ Modeling by shortest data description , ” Auto matica , vol . 14 , pp . 465–471 , 1978 .
[ 22 ] —— , “ Hypothesis selection and testing by the mdl principle , ”
The Computer Journal , vol . 42 , pp . 260–269 , 1999 .
[ 23 ] K . J¨arvelin and J . Kek¨al¨ainen , “ Cumulated gain based evaluation of ir techniques , ” ACM Trans . Inf . Syst . , vol . 20 , no . 4 , pp . 422–446 , 2002 .
[ 5 ] E . H C Lu , W C Lee , and V . S M Tseng , “ A framework for personal mobile commerce pattern mining and prediction , ” IEEE Trans . Knowl . Data Eng . , vol . 24 , no . 5 , pp . 769–782 , 2012 .
[ 6 ] D . Choujaa and N . Dulay , “ Predicting human behaviour from selected mobile phone data points , ” in UbiComp 2010 : Ubiquitous Computing , 12th International Conference , UbiComp 2010 , Copenhagen , Denmark , September 26 29 , 2010 , Proceedings , 2010 , pp . 105–108 .
[ 7 ] T . Yan , D . Chu , D . Ganesan , A . Kansal , and J . Liu , “ Fast app launching for mobile devices using predictive user context , ” in The 10th International Conference on Mobile Systems , Applications , and Services , MobiSys’12 , Ambleside , United Kingdom June 25 29 , 2012 , 2012 , pp . 113–126 .
[ 8 ] Z X Liao , P R Lei , T J Shen , S C Li , and W C Peng , “ Mining temporal profiles of mobile applications for usage prediction , ” in 12th IEEE International Conference on Data Mining Workshops , ICDM Workshops , Brussels , Belgium , December 10 , 2012 , 2012 , pp . 890–893 .
[ 9 ] C . Shin , J H Hong , and A . K . Dey , “ Understanding and prediction of mobile application usage for smart phones , ” in The 2012 ACM Conference on Ubiquitous Computing , Ubicomp ’12 , Pittsburgh , PA , USA , September 5 8 , 2012 , 2012 , pp . 173–182 .
[ 10 ] S . Chen , J . L . Moore , D . Turnbull , and T . Joachims , “ Playlist prediction via metric embedding , ” in The 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’12 , Beijing , China , August 12 16 , 2012 , 2012 , pp . 714–722 .
[ 11 ] Z X Liao , W C Peng , and P . S . Yu , “ Mining usage traces of mobile applications for dynamic preference prediction , ” in 17th Pacific Asia Conference on Knowledge Discovery and Data Mining , PAKDD 2013 , Gold Coast , Australia , April 1317 , 2013 , 2013 .
[ 12 ] N . Lathia , S . Hailes , L . Capra , and X . Amatriain , “ Temporal diversity in recommender systems , ” in Proceeding of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval , SIGIR 2010 , Geneva , Switzerland , July 19 23 , 2010 , 2010 , pp . 210–217 .
[ 13 ] P R Lei , T J Shen , W C Peng , and I J Su , “ Exploring spatial temporal trajectory model for location prediction , ” in 12th IEEE International Conference on Mobile Data Management , MDM 2011 , Lule˚a , Sweden , June 6 9 , 2011 , Volume 1 , 2011 , pp . 58–67 .
