2013 IEEE 13th International Conference on Data Mining
Noise Resistant Bicluster Recognition
Huan Sun
CS Department
University of California
Santa Barbara , USA huansun@csucsbedu
Gengxin Miao ECE Department
University of California
Santa Barbara , USA miao@umailucsbedu
Xifeng Yan
CS Department
University of California
Santa Barbara , USA xyan@csucsbedu
Abstractâ€”Biclustering is crucial in finding co expressed genes and their associated conditions in gene expression data . While various biclustering algorithms ( eg , combinatorial , probabilistic modelling , and matrix factorization ) have been proposed and constantly improved in the past decade , data noise and bicluster overlaps make biclustering a still challenging task . It becomes difficult to further improve biclustering performance , without resorting to a new approach . Inspired by the recent progress in unsupervised feature learning using deep neural networks [ 1 ] , in this work , we propose a novel model for biclustering , named AutoDecoder ( AD ) , by relating biclusters to features and leveraging a neural network that is able to automatically learn features from the input data . To suppress severe noise present in gene expression data , we introduce a non uniform signal recovery mechanism : Instead of reconstructing the whole input data to capture the bicluster patterns , AD weighs the zero and non zero parts of the input data differently and is more flexible in dealing with different types of noise . AD is also properly regularized to deal with bicluster overlaps . To the best of our knowledge , this is the first biclustering algorithm that leverages neural network techniques to recover overlapped biclusters hidden in noisy gene expression data . We compared our approach with four state of the art biclustering algorithms on both synthetic and real datasets . On three out of the four real datasets , AD significantly outperforms the other approaches . On controlled synthetic datasets , AD performs the best when noise level is beyond 15 % .
Source Code : http://grafiacsucsbedu/autodecoder/ Keywords Gene Expression , Biclustering , Neural Network
I . INTRODUCTION
High throughput gene expression profiling is readily accessible as the development of new technologies such as the Affymetrix array plates and next generation sequencing , which necessiates advanced analysis tools to deal with massive amounts of data . To analyze expression data , numerous computational tools have been developed and steadily improved , among which , biclustering becomes a dominant unsupervised technique for gene expression analysis .
Biclustering refers to a process of grouping genes and conditions simultaneously , producing a set of biclusters each including a gene set and a condition set . Fig 1 shows two overlapped biclusters in a sample dataset . The gene expression values â€œ 1 â€ , â€œ 1 â€ and â€œ 0 â€ indicate upregulating , downregulating , and unchanged , respectively . As shown in the figure , several important characteristics exist in bicluster recognition : ( 1 ) There are various kinds of biclusters : Genes ( conditions ) can be positively and negatively correlated ;
1550 4786/13 $31.00 Â© 2013 IEEE DOI 101109/ICDM201334
707
0 1
0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0
0 0 0
0
0
Fig 1 . Biclusters in a Sample Dataset .
( 2 ) Biclusters can overlap with each other in both gene dimension and condition dimension since multiple pathways containing the same gene could be active under different conditions ;
( 3 ) It is not necessary that each gene ( or condition ) has to participate in at least one bicluster ( not necessarily full coverage ) ;
( 4 ) Bicluster detection shall be robust against heavy noise in the input data .
Due to these unique characteristics , many non overlap , fullcoverage clustering methods are ineffective to recognize embedded biclusters .
To tackle the aforementioned challenges , many biclustering algorithms have been developed . We can roughly classify them into three categories : combinatorial methods , eg , CTWC [ 2 ] , OPSM [ 3 ] , ISA [ 4 ] , BIMAX [ 5 ] , association analysis based RAP [ 6 ] , COALESCE [ 7 ] , and QUBIC [ 8 ] ; probabilistic and generative approaches such as SAMBA [ 9 ] , FABIA [ 10 ] and many others [ 11 ] , [ 12 ] , [ 13 ] ; matrix factorization methods like spectral clustering [ 14 ] , SSVD [ 15 ] , and S4VD [ 16 ] . While biclustering performance has been significantly improved in the past decade , data noise and bicluster overlaps make the problem still quite challenging . Our controlled experiments will show that when noise level or bicluster overlap degree is high , most of existing algorithms can only discover a small percentage of biclusters . In this work , we are going to reexamine the biclustering problem from a new perspective and demonstrate the superior performance of our proposed model . Inspired by the recent progress in unsupervised feature learning through deep neural networks ( ie , deep learning ) [ 1 ] , we relate the biclustering problem to feature learning where the expression pattern of the gene set under the condition set in a bicluster can be regarded as a feature . Different variants of deep neural networks , such as [ 17 ] , [ 18 ] , have recently been tested to have tremendous power in extracting high level features in images . A deep neural network is composed of multiple hidden layers , where a higher ( lower ) hidden layer extracts features at a higher ( lower ) level . In our biclustering setting , we relate the expression of one gene under all conditions to the pixel values of one image . Our interested expression pattern corresponds to the correlations among pixels , which are essentially low level features . Therefore , we save ourselves the trouble of building a deep neural network ; instead , we resort to one neural network with a single hidden layer . Nevertheless , we expect the unsupervised feature learning power of neural networks will still boost the biclustering performance .
Particularly , we generalize sparse autoencoder ( SAE ) , a recently proposed neural network for unsupervised feature learning [ 19 ] , to mine biclusters . While SAE focuses on reconstructing the whole data , we are less interested in data reconstruction , than in extracting bicluster patterns . This motivates us to develop a model that can selectively reconstruct the data by putting different penalties on reconstruction errors in bicluster and non bicluster areas in the input . This design significantly improves the performance over SAE and makes it possible to outperform the existing highly optimized biclustering algorithms . We name this model AutoDecoder ( AD ) .
Our method is also related to matrix factorization , but significantly differs in the following aspects . First , matrix factorization is a linear method while the neural network in our model incorporates a non linear activation layer . PCA or SVD can be considered as a special case when the hidden layer picks a linear activation function . Second , we do not search for a low dimensional approximation to the whole expression data ; instead , we allow partial false reconstructions based on the location of noise , inside or outside biclusters . Third , a sparse activation constraint is imposed on the hidden neurons , which allows us to artificially control the number of genes to be discovered in each bicluster . With these advantages , AD is demonstrated to be more accuate than the leading matrix factorization approaches such as SSVD [ 15 ] and S4VD [ 16 ] . To the best of our knowledge , this is the first work to relate the biclustering problem to feature learning and apply neural network approaches to biclustering gene expression data . Our motivation is based on the conjecture that since biclustering has been a long standing topic , we might need to tackle the problem from a new perspective that has never been explored by previous studies in detail . Our model parameters are learnt using back propagation and a second order optimization algorithm L BFGS [ 21 ] , which have been popularly exploited in the deep learning literature . According to the optimized model parameters , we then design a bicluster embedding strategy to determine the genes and conditions in a bicluster . Empirically , our biclustering method runs very fast and outperforms the most up to date leading methods such as QUBIC , COALESCE , FABIA , and S4VD .
The rest of the paper is organized as follows . We first
708
( )ix
W 1
W
T
W
2
W ia ( ) 1 ia ( ) 2 x x x
( )i
Ka
2b ix ( ) 1 ix ( ) 2 ix ( ) 3 x x x
( )i
Mx
1b
( )Ë† ix
( )
( )
1Ë† ix 2Ë† ix 3Ë† ix
( ) x x x
( )Ë† i Mx
Fig 2 . Graphical Representation of an Autoencoder . briefly introduce sparse autoencoder , the predecessor of our to be proposed model , and make a connection between neural networks and biclustering in Section II . Our method , AutoDecoder , is presented in Section III , with discussions on noise resistance analysis and bicluster patterns . Experimental results on both real and synthetic datasets are given in Section V . Related works are reviewed in Section VI . Finally , we discuss future work and conclude in Section VII .
II . PRELIMINARIES
In this section , we build a connection between biclustering and the sparse autoencoder ( SAE ) [ 19 ] . Then we discuss the necessity to develop a generalized model AutoDecoder .
The graphical representation of an autoencoder with a single hidden layer is shown in Fig 2 . We begin with clarifying the notations in the context of biclusering . ( 1 ) ğ‘‹ : an ğ‘€ Ã— ğ‘ gene expression matrix , where ğ‘€ is the number of conditions and ğ‘ is the number of genes . ğ‘¥(ğ‘– ) ğ‘š denotes the expression value of the ğ‘–ğ‘¡â„ gene under the ğ‘šğ‘¡â„ condition . ( 2 ) ğ‘Š : a ğ¾ Ã— ğ‘€ matrix , the combination weights between layer and the hidden layer . Each element ğ‘¤ğ‘˜,ğ‘š the input represents the contribution of the expression under the ğ‘šğ‘¡â„ condition to bicluster ğ‘˜ , and determines whether the ğ‘šğ‘¡â„ condition is a member in bicluster ğ‘˜ or not . The weights in the first layer and those in the second layer are optionally tied 2 =ğ‘Š [ 1 ] . ğ‘1 : a ğ¾Ã—1 vector ; ğ‘2 : an ğ‘€ Ã—1 together by ğ‘Š1=ğ‘Š ğ‘‡ vector . ğ‘1 and ğ‘2 work as the bias terms at the input layer and the hidden layer respectively . ( 3 ) ğ‘(ğ‘– ) : a ğ¾ Ã— 1 vector representing activation values of the ğ‘–ğ‘¡â„ gene in ğ¾ hidden neurons , where each of the ğ¾ hidden neurons denotes a potential bicluster . Unless otherwise stated , we employ the popular element wise sigmoid activation function , ie , ğ‘(ğ‘– ) = sigmoid(ğ‘Š ğ‘¥(ğ‘– ) + ğ‘1 ) . Each component ğ‘˜ âˆˆ ( 0 , 1 ) encodes how likely a gene belongs to bicluster ğ‘(ğ‘– ) ğ‘˜ . Let ğ´ = [ ğ‘(1 ) , ğ‘(2 ) , . . . , ğ‘(ğ‘– ) , . . . , ğ‘(ğ‘ ) ] be the activation matrix of all the genes .
Ë†ğ‘‹ = [ Ë†ğ‘¥(1 ) , Ë†ğ‘¥(2 ) , . . . , Ë†ğ‘¥(ğ‘– ) , . . . , Ë†ğ‘¥(ğ‘ ) ] , where Ë†ğ‘¥(ğ‘– ) = ğ‘Š ğ‘‡ ğ‘(ğ‘– ) + ğ‘2 . reconstructed gene expression data .
Ë†ğ‘‹ :
( 4 )
Parameters in an autoencoder are learnt by minimizing the reconstruction error between ğ‘‹ and Ë†ğ‘‹ . For each hidden neuron , its activation rate is defined as Ë†ğœŒğ‘˜ = âˆ‘ğ‘ ğ‘˜ /ğ‘ . Let a ğ¾ Ã— 1 vector ğœŒ be our expected activation ğ‘–=1 ğ‘(ğ‘– ) rates of the ğ¾ hidden neurons . The Kullback Leibler ( KL ) divergence between a Bernoulli random variable with mean ğœŒğ‘˜ and a Bernoulli random variable with mean Ë†ğœŒğ‘˜ is added to the objective function as an additional penalty term . Autoencoder with this sparsity penalty term is called sparse autoencoder ( SAE ) [ 19 ] . This term can be entitled with more meanings in our setting . In the biclustering context , Ë†ğœŒğ‘ can be regarded as the number of genes each bicluster actually contains after learning , which is regularized to be close to ğœŒğ‘ we expect each bicluster to contain . Usually , it is a desideratum to obtain a bicluster which is neither too small nor too large . By using this term , the number of genes in one bicluster can be controlled artificially , which cannot be achieved by most of the existing biclustering algorithms . 2ğ‘ âˆ¥ğ‘‹ âˆ’ Ë†ğ‘‹âˆ¥2
SAE learns the optimal model parameters by minimizing 2âˆ¥ğ‘Šâˆ¥2 ğ¹ , the cost where âˆ¥ â‹… âˆ¥ğ¹ is the Frobenius norm . SAE can work well when the expression data are uniformly corrupted by the same Gaussian noise ( the same mean and variance ) . However , in practice , Gaussian random noise inside and outside biclusters could be different . In such situation , putting equal weights on reconstructing the bicluster part and non bicluster part of the expression data will hurt bicluster finding . Therefore , we develop a non uniform weighting strategy to discriminatively treat the background and the patterns . We call the new model , AutoDecoder ( AD ) , to emphasize that it decodes the biclusters rather than encodes the entire expression data .
ğ‘˜=1 KL(ğœŒğ‘˜âˆ¥Ë†ğœŒğ‘˜ ) + ğœ†
ğ¹ + ğ›½
âˆ‘ğ¾
1
III . FRAMEWORK
In this section , we introduce our model AutoDecoder ( AD ) , followed by discussions on model solution and bicluster recognition processes .
A . Optimization Formulation
AutoDecoder shares the same neural network structure as shown in Fig 2 . However , it enhances sparse autoencoder ( SAE ) to be more robust against noise and bicluster overlaps .
â„‹ = arg min ğ‘Š,ğ‘1,ğ‘2
1 2ğ‘ fi
ğ‘âˆ‘
ğ‘€âˆ‘
ğ‘–=1
ğ‘š=1
[ ğ¼ğ‘š,ğ‘– + ğ›¼(1 âˆ’ ğ¼ğ‘š,ğ‘–)](Ë†ğ‘¥(ğ‘– )
ğ‘š âˆ’ ğ‘¥(ğ‘– ) ğ‘š )2
'ff
( I )
ğ¾âˆ‘
ğ‘˜=1
ğ¾âˆ‘
KL(ğœŒğ‘˜âˆ¥Ë†ğœŒğ‘˜ ) 'ff
âˆ‘
( II )
+ ğ›½ fi
+
ğ›¾ 2 fi
ğ‘˜=1
ğ‘–âˆ•=ğ‘—
ğ‘†ğ‘–,ğ‘—(âˆ£ğ‘Šğ‘˜,ğ‘–âˆ£ âˆ’ âˆ£ğ‘Šğ‘˜,ğ‘—âˆ£)2
'ff
( III )
+ ğœ†âˆ¥ğ‘Šâˆ¥1 fi 'ff
( IV )
The above objective function is a sum of four terms , where ğ›½ , ğ›¾ , and ğœ† take non negative values and control the trade off
709 among different regularization terms . We now explain each of the terms in details :
ğ‘š = 0 and ğ¼ğ‘š,ğ‘– = 1 denoting ğ‘¥(ğ‘– )
âˆ™ Term ( I ) quantifies the average reconstruction error with non uniform weights . ğ¼ is an indicator matrix with ğ¼ğ‘š,ğ‘– = ğ‘š âˆ•= 0 . 0 denoting ğ‘¥(ğ‘– ) ğ›¼ controls the relative weight of reconstructing the zero values and non zero values in the input data . One can check that when ğ›¼ = 1 , AD will put the same emphasis on reconstructing the zero values and non zero values ( uniform weighting ) . In this case , this term degenerates to the reconstruction function in SAE . Conceptually , our model is suitable for any data with background as zeros and patterns as non zeros . In this work , we focus on the discretized matrix with elements { 1 , 0 , 1} , representing down regulating , unchanged , and up regulating . âˆ™ Term ( II ) is the sparsity penalty term as in SAE . To make the activation of each hidden neuron as sparse as we desire , we constrain the KL divergence between the sparsity parameter ğœŒğ‘˜ and the activation rate Ë†ğœŒğ‘˜ : + ( 1 âˆ’ ğœŒğ‘˜ ) log 1âˆ’ğœŒğ‘˜ KL(ğœŒğ‘˜âˆ¥Ë†ğœŒğ‘˜ ) = 1âˆ’ Ë†ğœŒğ‘˜ .
âˆ™ In Term ( III ) , ğ‘† is an ğ‘€ Ã— ğ‘€ symmetric matrix , where ğ‘†ğ‘–,ğ‘— is the absolute value of cosine similarity between the ğ‘–ğ‘¡â„ condition ( row ) and ğ‘—ğ‘¡â„ condition in matrix ğ‘‹ . If ğ‘†ğ‘–,ğ‘— is large enough , two conditions ğ‘– and ğ‘— should have similar membership after optimization ( âˆ£ğ‘Šğ‘˜,ğ‘–âˆ£ âˆ¼ âˆ£ğ‘Šğ‘˜,ğ‘—âˆ£ , for any ğ‘˜ ) . This term prevents dramatic membership change of two similar conditions , which will enhance the robustness against bicluster overlaps . âˆ™ Term ( IV ) is a regularizer named as weight decay ( Chapter 5 in [ 20] ) , where âˆ¥ğ‘Šâˆ¥1 is the ğ¿1 norm of ğ‘Š , ie , âˆ¥ğ‘Šâˆ¥1 = ğ‘š=1 âˆ£ğ‘Šğ‘˜,ğ‘šâˆ£ . ğ¿1 norm regularization tends to recover ğ‘Š with more sparsity by forcing the insignificant values to zeros , which will make easier the bicluster membership interpretation . We neglect the bias terms ğ‘1 and ğ‘2 because their inclusion will make the results of ğ‘Š depend on the choice of origin ( Section 1.1 and 551 in [ 20] ) .
ğ‘˜=1 ğœŒğ‘˜ log ğœŒğ‘˜ Ë†ğœŒğ‘˜
âˆ‘ğ‘€
âˆ‘ğ¾
âˆ‘ğ¾
ğ‘˜=1
B . Model Solution The classic L BFGS algorithm [ 21 ] , [ 22 ] is applied to minimize the objective function â„‹ . L BFGS needs the derivatives of parameters . They are calculated by the classic backpropagation algorithm [ 23 ] , where error messages are split at each neuron and then propagated to the neurons at previous layers . Given the current parameters ( ğ‘Š ğ¾Ã—ğ‘€ , ğ‘ğ¾Ã—1 , ğ‘ğ‘€Ã—1 ) , 2 the backpropagation procedure is performed as follows :
1
1 . Perform a forward pass , i.e , compute the activation matrix ğ´ of the hidden layer and the reconstructed matrix Ë†ğ‘‹ of the output layer . 2 . Let ğ›¿(1 ) âˆˆ ğ‘…ğ¾Ã—ğ‘ and ğ›¿(2 ) âˆˆ ğ‘…ğ‘€Ã—ğ‘ denote the error terms of the hidden layer and the output layer , respectively . Compute the error terms ğ›¿(2 ) and ğ›¿(1 ) as follows :
ğ‘š,ğ‘› = [ ğ¼ğ‘š,ğ‘› + ğ›¼(1 âˆ’ ğ¼ğ‘š,ğ‘›)](Ë†ğ‘¥(ğ‘› ) ğ›¿(2 ) ğ‘š,ğ‘› + ğ›½(âˆ’ ğœŒğ‘˜ Ë†ğœŒğ‘˜
ğ‘š âˆ’ ğ‘¥(ğ‘› ) ğ‘š ) 1 âˆ’ ğœŒğ‘˜ 1 âˆ’ Ë†ğœŒğ‘˜
ğ‘Šğ‘˜,ğ‘šğ›¿(2 )
ğ›¿(1 ) ğ‘˜,ğ‘› = [
ğ‘€âˆ‘
+
ğ‘š=1
) ] ğ‘(ğ‘› )
ğ‘˜ ( 1 âˆ’ ğ‘(ğ‘› ) ğ‘˜ )
3 . Compute the derivatives of ğ‘Š ğ¾Ã—ğ‘€ , ğ‘ğ¾Ã—1
1
, ğ‘ğ‘€Ã—1
2
:
ğ‘âˆ‘
ğ‘›=1
ğ‘âˆ‘
ğ‘›=1
ğ‘âˆ‘
ğ‘›=1
âˆ‚â„‹ âˆ‚ğ‘1ğ‘˜ âˆ‚â„‹ âˆ‚ğ‘2ğ‘š âˆ‚â„‹ âˆ‚ğ‘Šğ‘˜,ğ‘š
=
=
=
1 ğ‘
1 ğ‘
1 ğ‘
ğ›¿(1 ) ğ‘˜,ğ‘›
ğ›¿(2 ) ğ‘š,ğ‘›
( ğ›¿(1 )
ğ‘˜,ğ‘›ğ‘¥(ğ‘› )
ğ‘š + ğ›¿(2 )
ğ‘š,ğ‘›ğ‘(ğ‘› ) ğ‘˜ )
+ ( ğ›¾ ğ¿ğ‘š,(:)ğ‘Š ğ‘‡
ğ‘˜,( : ) + ğœ† ) sgn(ğ‘Šğ‘˜,ğ‘š )
âˆ‘ where ğ¿ is an ğ‘€ Ã— ğ‘€ symmetric matrix with ğ¿ğ‘–,ğ‘— = âˆ’ğ‘†ğ‘–,ğ‘— when ğ‘– âˆ•= ğ‘— , and ğ¿ğ‘–,ğ‘– = 1â‰¤ğ‘—â‰¤ğ‘€,ğ‘—âˆ•=ğ‘– ğ‘†ğ‘–,ğ‘— . ğ¿ğ‘š,(: ) , ğ‘Šğ‘˜,( : ) are the ğ‘šğ‘¡â„ and ğ‘˜ğ‘¡â„ row of ğ¿ and ğ‘Š respectively . sgn(â‹… ) is the sign function of a real number and takes 0 as the subgradient of the absolute value function ğ‘¦ = âˆ£ğ‘¥âˆ£ at ğ‘¥ = 0 . Once the objective function â„‹ and its derivatives are computed , the L BFGS algorithm is applied to minimize â„‹ . instead of storing the dense Hessian During optimization , approximation matrix , the L BFGS algorithm saves only a few vectors to represent the approximation implicitly , which significantly decreases the memory requirement . Since LBFGS is a classic and well established algorithm [ 21 ] , [ 22 ] , we omit the detailed analysis on its complexity and convergence rate . In practice , we observe that the L BFGS algorithm converges very fast ( within around 1000 iterations on all the real datasets ) .
C . Bicluster Recognition
In order to detect the embedded biclusters , we need to select both genes and conditions for each bicluster . The weight âˆ£ğ‘Šğ‘˜,ğ‘šâˆ£ determines the contribution of the ğ‘šğ‘¡â„ condition to the ğ‘˜ğ‘¡â„ bicluster . If âˆ£ğ‘Šğ‘˜,ğ‘šâˆ£ is large enough , the bicluster should contain the ğ‘šğ‘¡â„ condition . Similarly for selecting genes into one bicluster , if the activation value ğ‘(ğ‘– ) is high , the ğ‘–ğ‘¡â„ gene should belong to bicluster ğ‘˜ .
ğ‘˜
The bicluster embedding process is summarized as follows : For each hidden neuron ğ‘˜ , ie , each potential bicluster , âˆ™ Gene selection ğ‘˜ âˆ£>ğ›¿ ( ğ›¿ âˆˆ ( 0 , 1) ) ; Pick any gene ğ‘– corresponding to âˆ£ğ‘(ğ‘– ) Pick any condition ğ‘š corresponding to âˆ£ğ‘Šğ‘˜,ğ‘šâˆ£>ğœ– ( ğœ– âˆˆ ( 0 , 1) ) .
âˆ™ Condition selection
The thresholds ğ›¿ and ğœ– can be tuned by users ; their values are fixed at 0.7 and 0.3 respectively for all of our experiments unless otherwise stated . Intuitively , the same threshold should work for both the gene and condition selection ; however , elements in weight matrix ğ‘Š are usually much smaller due to the weight decay regularizer ; we pick different thresholds respectively for the gene and condition selection .
D . Robustness Against Noise
One of the major contributions we made is the non uniform weighting of the zero values and non zero values in the input data . As shown in Fig 1 , biclusters contain expression values representing upregulating ( +1 ) or downregulating ( âˆ’1 ) .
710 e r o c S
1
0.8
0.6
0.4
0.2
0
0
Fig 3 . Types .
Î±=1.5 Î±=1 Î±=0.5
0.2
0.4
0.6
0.8
Noise Level ( Type A )
1 e r o c S
1
0.8
0.6
0.4
0.2
0
0
Î±=1.5 Î±=1 Î±=0.5
0.2
0.4
0.6
0.8
Noise Level ( Type B )
1
Performance of AD with ğ›¼ = {0.5 , 1 , 1.5} wrt Different Noise
Therefore , there are two kinds of noise : Type A noise corresponding to the cases where nonzeros appear outside the biclusters , and Type B noise corresponding to the cases where zeros appear inside a bicluster . Accordingly , there are two kinds of reconstruction errors : ( 1 ) false negative error when +1 or âˆ’1 is reconstructed as 0 ; ( 2 ) false positive error when 0 is reconstructed as +1 or âˆ’1 . According to Term ( I ) in â„‹ , penalty weight on the false positive error is controlled by ğ›¼ while penalty weight on the false negative error is set to 1 . When ğ›¼ is greater than 1 , AD allows more false negative errors in the reconstruction process . In this situation , AD tends to exclude Â±1 â€™s out from the final patterns than to include 0 â€™s into the patterns , which is robust against Type A noise . In real gene expression datasets , we first run existing algorithms to detect biclusters and estimate the dominant noise type . In the case where Type A noise occurs more often than Type B noise , ğ›¼ > 1 is recommended . Here we verify our analysis on AD â€™s robustness against noise . We generate a set of matrices of size 100 Ã— 500 . In each matrix , 100 of the 500 columns are 1 â€™s while the others are 0 â€™s . The area in the matrix containing 1 â€™s is called pattern area while the area containing 0 â€™s is called background area . We consider two simple , yet illustrative , scenarios . In the first scenario , the elements in background area are flipped to 1 â€™s with probability ğ‘ ranging from 0 to 1 , whereas the pattern area is kept clean ( Type A noise ) . The flipping probability ğ‘ is named noise level . Oppositely , in the second scenario , the elements in pattern area are flipped to 0 â€™s with probability ğ‘ ( noise level ) ranging from 0 to 1 whereas the background area is kept clean ( Type B noise ) . At each noise level , we generate 50 matrices and compute the average F score ( See Section V A ) . We show the result of AD with ğ›¼ = 0.5 , 1 , 1.5 in Fig 3 . ğ›¼ = 1 corresponds to the uniform weighting as in SAE . Obviously , with an appropriate ğ›¼ our proposed model is more robust than the classic SAE approach .
E . Robustness Against Bicluster Overlaps
Many of existing biclustering algorithms cannot detect biclusters accurately when biclusters overlap at both gene and condition dimensions . Term ( III ) in â„‹ is designed to deal with such two dimension overlaps . Our intuition is that if two conditions ( genes ) have similar expression values , they should have similar bicluster membership . That is , for a condition
1 1 1 1
1 1 1 1
1 1 1 1
1 1 1 1
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
1
1
1 1 1 1 1 1 1 1 1 1 1 1
1
1
1 1 1 1
1 1 1 1 1 1 1 1
1 1 1 1
1 1 1 1 1 1 1 1
1 1 1 1 1 1 1 1
1.5 r e
1
0.5 l i t s u c B e h t n i s t i h g e W
1 1 1 1 # 1 1
" "
1 1 0 0 1 1 0 0
# % #
#
#
"
1 1 0 0
"" ""
0 0 # % # 0
"
1
0.5
0
âˆ’0.5 l r e t s u c B e h i t n i n o i t a v i t c A
Fig 4 . Bicluster Patterns that Can Be Discovered by AD .
0 0
2
4 6 Row Index
8
10
âˆ’1 0
10
20
30
Column Index
ğ‘–,ğ‘— pair ( ğ‘– , ğ‘— ) and any bicluster ğ‘˜ , ( âˆ£ğ‘Šğ‘˜,ğ‘–âˆ£ âˆ’ âˆ£ğ‘Šğ‘˜,ğ‘—âˆ£)2 should be as small as possible if the gene expression values in condition ğ‘– is similar to that in condition ğ‘— . Furthermore , if one condition is similar to conditions in multiple biclusters , it should belong to these biclusters simultaneously , thus âˆ‘ğ¾ resulting in biclusters overlapped in conditions . Formally , ( âˆ£ğ‘Šğ‘˜,ğ‘–âˆ£ âˆ’ âˆ£ğ‘Šğ‘˜,ğ‘—âˆ£)2 should be minimized ğ‘˜=1 ğ‘†(ğ‘ğ‘œğ‘›ğ‘‘ğ‘–ğ‘¡ğ‘–ğ‘œğ‘› ) where ğ‘†(ğ‘ğ‘œğ‘›ğ‘‘ğ‘–ğ‘¡ğ‘–ğ‘œğ‘› ) is the similarity between the expression values under condition ğ‘– and condition ğ‘— . Similarly for a gene âˆ‘ ğ‘–âˆ•=ğ‘— ğ‘†(ğ‘”ğ‘’ğ‘›ğ‘’ ) ğ‘˜ )2 should be minipair ( ğ‘– , ğ‘— ) , mized where ğ‘†(ğ‘”ğ‘’ğ‘›ğ‘’ ) is the similarity between the expression values of gene ğ‘– and gene ğ‘— across all the conditions . Due to our neural network structure ğ‘(ğ‘– ) = sigmoid(ğ‘Š ğ‘¥(ğ‘– ) + ğ‘1 ) , if two genes have similar expression values ( ğ‘¥(ğ‘– ) âˆ¼ ğ‘¥(ğ‘—) ) , they will naturally have similar activation values ( ğ‘(ğ‘– ) âˆ¼ ğ‘(ğ‘—) ) . Therefore , in AD , only the condition term is included , and ğ‘†(ğ‘ğ‘œğ‘›ğ‘‘ğ‘–ğ‘¡ğ‘–ğ‘œğ‘› ) is replaced with ğ‘† for simplicity .
ğ‘˜ âˆ’ ğ‘(ğ‘— ) ( ğ‘(ğ‘– )
âˆ‘ğ¾
ğ‘˜=1
ğ‘–,ğ‘—
ğ‘–,ğ‘—
ğ‘–,ğ‘— l r e t s u c B e h i t n i s t h g e W i
1.5
1
0.5
0
âˆ’0.5
âˆ’1
âˆ’1.5 0
Fig 5 . Bicluster Recognition with Pattern IV .
" "
1 1 1 1 # % # # 1 1
"
1 1 0 0 1 1 0 0
#
#
1 1 0 0
"" ""
0 0 # "" # 0
""
1
0.5
0
âˆ’0.5 l r e t s u c B e h i t n i t a v i t c A i n o
2
4 6 Row Index
8
10
âˆ’1 0
10
20
30
Column Index
40
50
40
50
IV . BICLUSTER PATTERNS
Fig 6 . Bicluster Recognition with Pattern V .
AutoDecoder can identify a broad range of bicluster patterns in one trial . Fig 4 shows five types of bicluster patterns that can be found by AD .
( I III ) In these three patterns , conditions can be either positively or negatively correlated under the gene set whereas genes are positively correlated under the condition set . Our AD model can directly discover such patterns . For some ğ‘˜ , the weights ğ‘Šğ‘˜,ğ‘š corresponding to the condition set will be large in magnitude . The sign of the ğ‘Šğ‘˜,ğ‘š will be the same as that of the expressions under the ğ‘šğ‘¡â„ condition . The gene set will have large activation values in the ğ‘˜ğ‘¡â„ neuron .
( IV V ) In the fourth and fifth patterns , conditions can be either positively or negatively correlated under the gene set whereas genes can also be either positively or negatively correlated under the condition set . For such patterns , our AD model with a sigmoid hidden layer splits the bicluster into two subclusters in which the two subsets of genes behave oppositely . This is because column 1 ( 3 ) and column 2 ( 4 ) cannot simultaneously have large activation values under the same hidden neuron . Since both subclusters are associated with
711 the same condition set in our embedding process , they can be combined through postprocessing . Alternatively , if the sigmoid activation function in the hidden layer is replaced by a tanh activation function , these patterns can be directly discovered . We are going to show that AD with a tanh activation layer is able to discover Patterns IV and V . Recall that in Section II , ğ‘(ğ‘– ) = sigmoid(ğ‘Š ğ‘¥(ğ‘– ) + ğ‘1 ) . Now we replace sigmoid with a tanh function , ie , ğ‘(ğ‘– ) = tanh(ğ‘Š ğ‘¥(ğ‘– ) + ğ‘1 ) âˆˆ ( âˆ’1 , 1 ) . As long as âˆ£ğ‘(ğ‘–)âˆ£ is large , we regard the ğ‘–ğ‘¡â„ gene to be active . In this case , the original definition of the sparsity parameter Ë†ğœŒğ‘˜ = ğ‘˜ /ğ‘ is problematic ; therefore , we re define ğ‘˜ âˆ£/ğ‘ , 2 Ë†ğœŒğ‘˜ = however , we favor the former due to its smoothness . All other components in our original AD model are kept unchanged . We generate two toy matrices of size 10Ã— 50 , in which the 10 rows and the first 10 columns form a bicluster with Pattern IV and Pattern V respectively . The number of biclusters ğ¾ is set at 1 . Fig 5 and 6 show the membership of rows and
âˆ‘ğ‘ ğ‘–=1 ğ‘(ğ‘– ) âˆ‘ğ‘ ğ‘–=1 ğ‘(ğ‘– )
/ğ‘ . One can also use Ë†ğœŒğ‘˜ =
ğ‘–=1 âˆ£ğ‘(ğ‘– )
âˆ‘ğ‘
ğ‘˜ columns for a bicluster with Pattern IV and V respectively . The upper subfigure shows a simulated matrix with Pattern IV or V . The lower left subfigure provides the learnt weights of the 10 rows ( ğ‘Š1,ğ‘š : ğ‘š = 1 , . . . , 10 ) while the lower right subfigure illustrates the activation values of the 50 columns ( ğ´1,ğ‘› : ğ‘› = 1 , . . . , 50 ) . According to the bicluster recognition process discussed in Section III C , both the columns and rows belonging to the bicluster can be successfully identified due to a high magnitude in either weights or activation values .
V . EXPERIMENTS
In this section , we present empirical studies of AutoDecoder ( AD ) on both real gene expression datasets and controlled synthetic datasets , and compare AD with the state of the art biclustering algorithms in different categories : ( 1 ) Combinatorial search , Qubic [ 8 ] and COALESCE[7 ] , ( 2 ) Probabilistic model , FABIA [ 10 ] , and ( 3 ) Matrix factorization , S4VD [ 16 ] . These four algorithms represent the best results in their own category after a decade of efforts in searching high quality biclusters in gene expression data . A detailed comparison of many other existing algorithms with FABIA is available in [ 10 ] .
AD was implemented in MATLAB . All the experiments were conducted on a 3.4GHZ , 16GB , Intel PC running Windows 7 .
A . Evaluation Measures
Suppose ğ‘€âˆ—
To assess various biclustering approaches , we use the average Relevance and Recovery scores , which were widely adopted in biclustering literature , such as S4VD [ 16 ] , QUBIC [ 8 ] , and BIMAX [ 5 ] . Overall , these two scores quantify the similarity between the set of computed biclusters and the set of true biclusters in the data . is the set of true biclusters , and ğ‘€ is the set of discovered biclusters each of which contains a gene set G and a condition set C . We denote ğ‘€âˆ— and ğ‘€ respectively as ğ‘€âˆ— = {ğ‘€âˆ— ğ‘šâˆ—} and ğ‘€ = {ğ‘€1 , ğ‘€2 , , ğ‘€ğ‘š} . 2 , , ğ‘€âˆ— ğ‘– and ğ‘€ğ‘– = ğºğ‘– Ã— ğ¶ğ‘– , where Ã— is the Let ğ‘€âˆ— ğ‘– = ğºâˆ— Cartesian product . The average Relevance and Recovery scores are defined as follows :
1 , ğ‘€âˆ— ğ‘– Ã— ğ¶âˆ—
Relevance =
Recovery =
1 ğ‘š
1 ğ‘šâˆ—
ğ‘šâˆ‘
ğ‘–=1
ğ‘šâˆ—âˆ‘
ğ‘–=1 max
ğ‘—âˆˆ{1,2,,ğ‘šâˆ—} max
ğ‘—âˆˆ{1,2,,ğ‘š}
ğ‘—
ğ‘€ğ‘– âˆ© ğ‘€âˆ— ğ‘€ğ‘– âˆª ğ‘€âˆ— ğ‘– âˆ© ğ‘€ğ‘— ğ‘€âˆ— ğ‘– âˆª ğ‘€ğ‘— ğ‘€âˆ—
ğ‘—
ğ‘€ğ‘–âˆ©ğ‘€âˆ— ğ‘€ğ‘–âˆªğ‘€âˆ—
ğ‘—
ğ‘— where is the well known Jaccard coefficient [ 24 ] . The average Relevance evaluates to what degree the discovered biclusters represent the true biclusters , whereas the average Recovery measures how well the true biclusters are recovered by the current biclustering algorithm . We calculate the harmonic mean of the Relevance and Recovery scores , ğ¹ = 2ğ‘…ğ‘’ğ‘™âˆ—ğ‘…ğ‘’ğ‘ in order to jointly consider them . Note ğ‘…ğ‘’ğ‘™+ğ‘…ğ‘’ğ‘ , that although there are various kinds of evaluation measures available for general clustering problems , for biclustering ,
712 we select the most widely adopted Relevance and Recovery measures in the literature .
The performance on all the simulated datasets is evaluated using ğ¹ score computed as above . However , for real gene expression datasets , ground truth on the bicluster membership of genes is usually not available , therefore we can only compute the scores on the condition clusters each of which corresponds to the condition set within a bicluster , ie , ğ‘€âˆ— ğ‘– and ğ‘€ğ‘– = ğ¶ğ‘– . Meanwhile , the biological significance of the gene set within a detected bicluster is evaluated . This strategy was also adopted in our competing algorithms [ 10 ] , [ 16 ] .
ğ‘– = ğ¶âˆ—
B . Gene Expression Datasets
We experimented four real gene expression datasets widely used in biclustering studies : breast cancer , multiple tissue types , diffuse large B cell lymphoma , and lung cancer . For the first three datasets , Hoshida et al . [ 25 ] clustered the conditions using additional datasets and verified the clusters by gene set enrichment analysis . FABIA [ 10 ] studied how well biclustering algorithms can re identify these clusters without any additional information . Similarly , for the lung cancer dataset , S4VD [ 16 ] and SSVD [ 15 ] first compared the obtained condition clusters with ground truth and then evaluated gene sets within biclusters by doing enrichment analysis . We adopted a similar approach to evaluate the detected biclusters . We briefly describe the datasets as follows :
( a ) Breast cancer dataset [ 26 ] . As stated in [ 10 ] , the sample ( ie , condition ) S54 is an outlier and was removed from the original data . The dataset has 1,213 genes and 97 samples .
( b ) Multiple tissue types dataset [ 27 ] . It contains 5,565 genes and 102 samples from diverse tissues and cell lines . Hoshida et al . [ 25 ] discovered four sample clusters from this dataset .
( c ) Diffuse large B cell lymphoma ( DLBCL ) dataset [ 28 ] . This dataset was to predict the survival after chemotherapy , and contains 3,795 genes and 58 samples . Four sample clusters were discovered in [ 25 ] .
( d ) Lung cancer dataset [ 29 ] . It was previously analyzed in [ 16 ] , [ 15 ] . 12,625 genes under 56 samples were measured using the Affymetrix 95av2 GeneChip . are not valued among in the procedure discretization increasing order for each gene , sort
1 ) Preprocessing : For
{âˆ’1 , 0 , 1} , we similar those gene expression datasets adopted to QUBIC [ 8 ] . its original expression as that a Specifically , values on ğ‘€ conditions follows:(ğ‘¥ğœ‹(1 ) , ğ‘¥ğœ‹(2 ) , . . . , ğ‘¥ğœ‹(ğ‘ âˆ’1 ) , ğ‘¥ğœ‹(ğ‘  ) , . . . , ğ‘¥ğœ‹(ğ‘âˆ’1 ) , ğ‘¥ğœ‹(ğ‘ ) , ğ‘¥ğœ‹(ğ‘+1 ) , . . . , ğ‘¥ğœ‹(ğ‘€âˆ’ğ‘ +1 ) , ğ‘¥ğœ‹(ğ‘€âˆ’ğ‘ +2 ) , . . . , ğ‘¥ğœ‹(ğ‘€ ))ğ‘‡ , where ğœ‹(ğ‘— ) is the permutated ğ‘—ğ‘¡â„ index , ğ‘ = âŒˆğ‘€/2âŒ‰ and ğ‘  âˆ’ 1 = âŒŠğ‘€ ğ‘âŒ‹ . Same as [ 8 ] , we set ğ‘ at 006 A condition is considered as downregulating ( âˆ’1 ) for a gene if its expression value is less than ğ‘¥ğœ‹(ğ‘ ) âˆ’ ğ‘‘ and as is greater than ğ‘¥ğœ‹(ğ‘ ) + ğ‘‘ , where upregulating ( +1 ) if it ğ‘‘ = min{ğ‘¥ğœ‹(ğ‘ ) âˆ’ ğ‘¥ğœ‹(ğ‘  ) , ğ‘¥ğœ‹(ğ‘€âˆ’ğ‘ +1 ) âˆ’ ğ‘¥ğœ‹(ğ‘)} ; otherwise , the gene is not activated ( ie , 0 ) . The rationale to choose this procedure is given in the Supplementary Data of [ 8 ] . The choice is not exclusive ; other discretization methods can also be applied here . Since we find that FABIA , S4VD , and COALESCE do not work better on preprocessed data , the raw data are used as their input .
2 ) Parameter Setting : Biclustering algorithms generally involve many parameters . We explored the number of biclusters in all the methods except COALESCE from 5 to 8 , slightly larger than the true number in each dataset , as suggested in FABIA . COALESCE can find any number of biclusters that satisfy their criteria . In these popularly used real datasets , only a small number of biclusters are present ; nevertheless , we will later synthesize datasets with more biclusters embedded for comprehensive comparison . For each competing algorithm , we tuned 2 to 4 important parameters according to their manual to show their best performance on each dataset .
ğ‘ , ğ›¾ = 0.2
ğ‘ , and ğœ† = 0.1âˆš
For AD , we observe that a fixed setting of parameters generalize very well across different datasets . Particularly , we set ğ›½ = 2ğ‘€âˆš ğ‘ . Two important parameters in AD are ğ›¼ and ğœŒ . There are three typical values for ğ›¼ : 1.5 , 0.5 , and 1 , which can deal with Type A noise , Type B noise , and balanced Type A and B noise as discussed in Section III D . For ğœŒ , it will be easier to set if one has any prior knowledge or preference over the number of genes contained in biclusters . In our experiments , we first run other algorithms to get a rough idea about the noise type and gene cluster size . In the real gene expression datasets we tested , Type A noise occurs more often than Type B noise , and therefore , we set ğ›¼ at 15 We randomly draw each component of ğœŒ from the interval [ 0.01 , 0.03 ] , as we expect a bicluster to contain 1 % âˆ¼ 3 % the total number of genes .
3 ) Performance Comparisons : On all of these datasets , AD completes biclustering within 2 minutes . The results are summarized in Fig 7 . Columns â€œ ğ‘…ğ‘’ğ‘™ . â€ , â€œ ğ‘…ğ‘’ğ‘ . â€ , and â€œ ğ¹ â€™ respectively provide the average Relevance score , average Recovery score and their harmonic mean . The columns â€œ #g â€ , â€œ #s â€ , and â€œ #bc â€ give the average number of genes , conditions in each bicluster , and the number of discovered biclusters respectively . â€œ TN â€ gives the true number of condition clusters in each dataset . For the breast cancer dataset , FABIA performs the best . For the remaining three datasets , AD significantly outperforms other tuned biclustering algorithms . Understanding the differences between the breast cancer dataset and other datasets will be helpful to further improve the performance , which we leave for future study . The superiority of AD over SAE verifies the effectiveness of our proposed strategy in dealing with real data . Moreover , AD can achieve better performance once the parameters are tuned as in our competing algorithms .
To evaluate the biological significance of the gene set in a discovered bicluster , as in [ 8 ] , [ 10 ] , [ 16 ] , we conducted gene ontology ( GO ) enrichment analysis by computing the ğ‘ƒ value [ 8 ] of each biological category wrt the gene set . Particularly , we caclulate the probability of having ğ‘Ÿ ( ğ‘Ÿ > 0 ) genes of the same biological category in a bicluster with ğ‘› genes as :
Pr(ğ‘Ÿâˆ£ğ‘ , ğ‘š , ğ‘› ) =
(
ğ‘š ğ‘Ÿ
)( (
) ğ‘âˆ’ğ‘š ğ‘›âˆ’ğ‘Ÿ ) ğ‘ ğ‘›
( 1 )
713
Methods
AD SAE QUBIC COALESCE FABIA S4VD
Methods
AD SAE QUBIC COALESCE FABIA S4VD
Methods
AD SAE QUBIC COALESCE FABIA S4VD
Methods
AD SAE QUBIC COALESCE FABIA S4VD
#g 15 20 33 79 98 124
ğ¹ 0.43 0.48 0.21 0.41 0.57 0.49
Breast Cancer ( TN : 3 ) ğ‘…ğ‘’ğ‘™ . ğ‘…ğ‘’ğ‘ . #s 23 0.42 0.44 30 0.48 0.47 15 0.14 0.42 36 0.34 0.52 31 0.49 0.67 0.41 27 0.60 Multiple Tissue ( TN : 4 ) #s 23 34 18 77 31 8
#g 290 270 103 546 98 245 DLBCL ( TN : 4 )
ğ‘…ğ‘’ğ‘™ . ğ‘…ğ‘’ğ‘ . 0.89 0.75 0.70 0.63 0.67 0.58 0.52 0.44 0.77 0.77 0.19 0.07
ğ¹ 0.82 0.66 0.63 0.47 0.77 0.10
ğ‘…ğ‘’ğ‘™ . ğ‘…ğ‘’ğ‘ . 0.48 0.53 0.48 0.48 0.35 0.34 0.39 0.36 0.55 0.18 0.24 0.34 Lung Cancer ( TN : 4 )
ğ¹ 0.50 0.48 0.34 0.38 0.27 0.28
#g 103 118 174 289 99 270
#s 17 24 7 40 17 18
ğ‘…ğ‘’ğ‘™ . ğ‘…ğ‘’ğ‘ . 0.89 0.96 0.76 0.60 0.59 0.65 0.48 0.29 0.85 0.85 0.78 0.68
ğ¹ 0.92 0.67 0.62 0.36 0.85 0.72
#g 442 476 130 167 607 445
#s 14 15 12 14 13 16
#bc 3 6 1 11 4 2
#bc 5 5 5 3 4 1
#bc 5 5 6 6 1 2
#bc 5 6 4 89 4 3
Fig 7 . Results on Real Datasets . where ğ‘ is the total number of genes in the input , and ğ‘š is the number of genes from that biological category and encoded in the input . For each biological category with ğ‘Ÿ > 0 in one bicluster , we calculate its ğ‘ƒ value wrt the gene set in the bicluster using the probability defined by ( 1 ) . We then use the smallest ğ‘ƒ value among all possible biological categories as the ğ‘ƒ value of the current bicluster . The smaller the ğ‘ƒ value , the more biologically significant the bicluster [ 8 ] , [ 16 ] , [ 10 ] . Our model can generally discover biclusters with ğ‘ƒ value around or less than 10âˆ’4 , much often less than 10âˆ’10 on all the datasets , which is comparable to the gene set analysis result shown by FABIA [ 10 ] and S4VD [ 16 ] . Due to space limit , we only show the summarized gene enrichment analysis results on the last three datasets .
For Multiple Tissue dataset , Bicluster 1 is enriched with genes related to the acyl CoA metabolic process and the thioester metabolic process ( ğ‘ƒ value 2.8 Ã— 10âˆ’6 ) . Genes related to collagen fibril organization enrich Bicluster 2 ( ğ‘ƒ value 4.0Ã— 10âˆ’6 ) . The most significant GO terms in Bicluster 3 are related to immune response ( ğ‘ƒ value 12Ã—10âˆ’20 ) Genes in Bicluster 4 are related to positive regulation of superoxide anion generation ( ğ‘ƒ value 6.9 Ã— 10âˆ’5 ) . The most significant GO terms in Bicluster 5 are related to G protein coupled receptor protein signaling pathway ( ğ‘ƒ value 6.9 Ã— 10âˆ’7 ) .
For DLBCL dataset , genes in Bicluster 1 are most related to the regulation of transcription involved in G1/S phase of mitotic cell cycle ( ğ‘ƒ value 1.1 Ã— 10âˆ’4 ) . Bicluster 2 is highly related to defense response ( ğ‘ƒ value 6.3Ã— 10âˆ’10 ) . Bicluster 3 is enriched by genes from the regulation of peptide hormone secretion . The most significant GO terms in Bicluster 5 are highly related to the regulation of phospholipase activity ( ğ‘ƒ value 5.3 Ã— 10âˆ’4 ) . Bicluster 4 is too small to allow for a reliable biological interpretation .
In the discovered biclusters from Lung Cancer dataset , Bicluster 1 is enriched with genes related to positive regulation of kidney development ( ğ‘ƒ value 22Ã—10âˆ’6 ) The most significant GO terms in Bicluster 2 are highly related to cell division ( ğ‘ƒ value 9.2 Ã— 10âˆ’26 ) . Bicluster 3 is related to interferongamma mediated signaling pathway ( ğ‘ƒ value 1.2 Ã— 10âˆ’12 ) . Genes related to mitochondrial electron transport enrich Bicluster 4 ( ğ‘ƒ value 39Ã—10âˆ’8 ) The most significant biological category in Bicluster 5 is related to cell adhesion ( ğ‘ƒ value 2.2 Ã— 10âˆ’10 ) .
The evaluation of biclustering results illustrates that our approach not only improves the performance on condition clusters of these datasets , but also guarantees the biological significance of the gene sets discovered in the biclusters .
ğ‘ , and ğœ† = 0.1âˆš
4 ) Sensitivity Analysis : For most of the learning problems , model selection is a critical problem . The learning performance might vary significantly under different parameter settings . ğ›½ , ğ›¾ , and ğœ† are our model parameters that control the trade off among the regularization terms . We fix ğ›½ = 2ğ‘€âˆš ğ‘ , ğ›¾ = 0.2 ğ‘ on the previous experiments . In this subsection , we study the impact of the parameters on the performance of AD . Specifically , we fix other parameters as before and let one of {ğ›½ , ğ›¾ , ğœ†} vary . As shown in Fig 8 , AD is generally not very sensitive with respect to these parameters , since high quality scores can be achieved under a large range of the studied parameters .
C . Synthetic Datasets
Our algorithm is also tested on a set of controlled synthetic datasets , which are generated as follows : Given the matrix size 100 Ã— 500 and the number of biclusters ğ¾ , the number of rows ğ‘Ÿ in a bicluster is randomly selected from the interval [ 10 , 30 ] , and the number of columns ğ‘ is randomly selected from [ 50 , 100 ] ; then we randomly choose ğ‘Ÿ rows from the total 100 rows and ğ‘ columns from the total 500 columns as the members of the bicluster . The matrix is initially filled with â€œ 0 â€ . Each bicluster is filled with â€œ 1 â€ . In total , ğ¾ biclusters are generated . We then inject noise to each matrix by flipping the value of elements . Specifically , we flip the 1 â€™s inside biclusters to be zeros with probability ğ‘ and flip the 0 â€™s outside biclusters to be 1 or âˆ’1 respectively with probability ğ‘ 2 . The flipping probability ğ‘ is named noise level .
1
0.8
0.6
0.4
0.2
0 0.5
0.9
K=6 TPR K=6 FPR K=10 TPR K=10 FPR K=20 TPR K=20 FPR
0.6
Threshold Î´ on Gene Selection
0.7
0.8
1
0.8
0.6
0.4
0.2
K=6 TPR K=6 FPR K=10 TPR K=10 FPR K=20 TPR K=20 FPR
0 0.1
0.2
Threshold Îµ on Condition Selection
0.3
0.4
Fig 10 . True Positive Rate ( TPR ) and False Positive Rate ( FPR ) of AD .
For each parameter setting ğ¾ and ğ‘ , 100 matrices are generated and the average performance of each algorithm on these 100 matrices is measured . We tune parameters in our competing algorithms and show their best performance . For AD , we still use a fixed parameter setting , where ğ›¼ = 1 and ğœŒğ‘˜ = 01,âˆ€ğ‘˜
Fig 9 illustrates the performance of five algorithms . It is observed that AD significantly outperforms all the other algorithms when noise level is beyond 15 % . When we increase the number of biclusters , recognition becomes more and more difficult because overlapping among biclusters also increases significantly . For example , around 12 % of the nonzeros belong to two biclusters when ğ¾ = 10 , whereas the ratio increases to 22 % when ğ¾ = 20 . In such situation , all the competitors are only able to detect a much smaller percentage of true biclusters , while AD still maintains a much higher F score . Fig 9 shows that AD covers the situation ( high noise and high overlaps ) where the existing algorithms cannot work well .
We further show the change of the true positive rate ( TPR ) and false positive rate ( FPR ) of AD , when varying the recognition thresholds ğ›¿ and ğœ– in Section III C . We detail how to caculate the TPR and FPR in our biclustering setting : We first map each detected bicluster ğ‘€ğ‘– to a true bicluster ğ‘€âˆ— ğ‘— ğ‘€ğ‘–âˆ©ğ‘€âˆ— by maximizing the Jaccard coefficient , i.e , by max . ğ‘€ğ‘–âˆªğ‘€âˆ— We compute the TPR and FPR for each true bicluster , and then obtain their average as the overall TPR and FPR of AD . The above synthetic datasets with noise level at 0.2 are used for study . Fig 10 shows at 0.2 noise level , AD generally still achieves a high TPR while keeping the FPR at a low level . On the other hand , AD turns out to be not very sensitive to the thresholds , since similar results can be achieved under a large range of the thresholds .
ğ‘—
ğ‘—
ğ‘—
VI . RELATED WORK
The first biclustering algorithm for gene expression analysis was proposed in the year 2000 by Cheng and Church [ 30 ] . Since then , various combinatorial biclustering algorithms have been developed , including the Coupled Two Way Clustering [ 2 ] , the Order Preserving SubMatrix [ 3 ] , the Iterative Signature Algorithm [ 4 ] , BIMAX developed by [ 5 ] , QUBIC [ 8 ] , and COALESCE [ 7 ] . An association rule based method RAP [ 6 ] also falls into this category . We include the recent QUBIC and COALESCE for comparative studies in our experiments . Computational complexity and performance have
714 e r o c s
F e r o c s
F
1
0.8
0.6
0.4
0.2
0
0
1
0.8
0.6
0.4
0.2
0
0
Breast Lung Multi . DLBCL
4
1
0.8
0.6
0.4
0.2 e r o c s
F
0
0.1
0.2
Breast Lung Multi . DLBCL
0.5
0.6 e r o c s
F
1 3 Î² ( unit : M/sqrt(N ) )
2
0.3
0.4 Î³ ( unit : 1/N )
Fig 8 . Sensitivity Analysis of {ğ›½ , ğ›¾ , ğœ†} on Real Datasets .
K=6
K=10
AD QUBIC COALESCE FABIA S4VD
1
0.8
0.6
0.4
0.2 e r o c s
F
AD QUBIC COALESCE FABIA S4VD e r o c s
F
0.1
0.2
0.3
0.4
Noise level ( flipping probability p )
0.5
0.1
0.2
0.3
0.4
0.5
0
0
Noise level ( flipping probability p ) Fig 9 . Results on Synthetic Datasets
1
0.8
0.6
0.4
0.2
0
1
0.8
0.6
0.4
0.2
0
0
Breast Lung Multi . DLBCL
0.1
0.2
0.3 Î» ( unit : 1/sqrt(N ) )
0.4
K=20
AD QUBIC COALESCE FABIA S4VD
0.1
0.2
0.3
0.4
Noise level ( flipping probability p )
0.5 been steadily improved as the development of new algorithms . Probabilistic and generative methods are also available for biclustering gene expression data . The statistical algorithmic method for bicluster analysis ( SAMBA [ 9 ] ) is one of the earliest probabilistic approaches to biclustering . Other approaches include those proposed in [ 11 ] , [ 12 ] , [ 13 ] . FABIA [ 10 ] is one of the recently developed probabilistic approaches , and has demonstrated good performance on various real datasets . Another category of biclustering algorithms is based on matrix factorization . Spectral biclustering [ 14 ] applies singular vector decomposition ( SVD ) to extracting expression patterns in a matrix . It is indicated in [ 31 ] that SVD is capable of finding biclusters . Lee et al . [ 15 ] developed a sparse SVD ( SSVD ) algorithm . S4VD [ 16 ] improved SSVD [ 15 ] by incorporating a stability selection technique . Recently , query based biclustering algorithms [ 32 ] , [ 33 ] are developed , which utilize a set of seed genes provided by the user to prune the search space and guide the biclustering algorithm . Hanczar et al . [ 34 ] focus on the corrected measurement of the biclustering methods . Despite of many existing studies on biclustering , we are motivated to further improve the performance by approaching the problem from a novel perspective .
Cross association [ 35 ] , co clustering [ 36 ] , [ 37 ] , [ 38 ] , [ 39 ] , exclusive row biclustering [ 40 ] , monochromatic biclustering proposed in [ 41 ] , block models [ 42 ] , Boolean Matrix Factorization [ 43 ] , [ 44 ] , and techniques on noisy information theoretic tiles detection [ 45 ] have been applied to group rows and columns simultaneously in a matrix . Each of these methods also falls into one of the three previously mentioned categories , ie , combinatorial approach , probabilistic modelling , or matrix factorization . These existing models usually have one or more of the following characteristics : ( 1 ) Only 0 1 binary input can be handled , which is impossible to detect negative correlations ; ( 2 ) Disjoint/non overlapping or full coverage biclusters are generated , which is different from our emphasis introduced in Section I ; ( 3 ) They rely on stochastic approximation to do inference due to lack of computationally efficient algorithms . Our work is inspired by deep learning , ie , unsupervised feature learning using deep neural networks . Since the breakthrough on training multi layer neural networks was made by Hinton and Salakhutdinov[1 ] , numerous studies on learning deep feature hierarchies have been conducted . Among them , the most related one is sparse autoencoder proposed in [ 19 ] . Different from sparse autoencoder , our model is more resistant against noise by differently reconstructing the bicluster and background part of the input data . Besides , we also enhance the ability of sparse autoencoder to deal with bicluster overlaps . Through extensive experiments , we clearly demonstrate that our neural network based approach can outperform the existing biclustering algorithms , as well as the original sparse autoencoder .
VII . CONCLUSION AND FUTURE WORK
We developed a novel biclustering approach , AutoDecoder , to effectively discover biclusters in highly noisy expression data . To the best of our knowledge , this is the first attempt to relate the biclustering problem to unsupervised feature learning methods and apply neural network approaches to biclustering gene expression data . AutoDecoder associates the activation of hidden neurons in a two layer neural network with the membership of genes and conditions in biclusters . Compared with four state of the art algorithms , AutoDecoder
715 performs better on both real and synthetic datasets , especially when there are more overlapped biclusters and higher noise . Our experimental results show that neural network is a promising approach to biclustering , a long standing problem in gene expression data analysis .
Apart from evaluation in terms of Relevance and Recovery scores , and ğ‘ƒ value , it will be interesting to verify whether the biclusters recognized by AutoDecoder are more useful to biologists in the future . Biclustering methods usually involve many parameters . Another very useful future work will be to test different parameter settings to fully explore their potential , especially for AutoDecoder . Since biclustering has been a long standing topic , to further improve the performance , one might need to explore new ideas different from existing biclustering methodologies .
ACKNOWLEDGMENT
This work was partially supported by the US National Science Foundation under grant IIS 0954125 and the Institute for Collaborative Biotechnologies through grant W911NF 090001 from the US Army Research Office . The content of the information does not necessarily reflect the position or the policy of the Government , and no official endorsement should be inferred .
REFERENCES
[ 1 ] G . Hinton and R . Salakhutdinov , â€œ Reducing the dimensionality of data with neural networks , â€ Science , vol . 313 , no . 5786 , p . 504 , 2006 .
[ 2 ] G . Getz , E . Levine , and E . Domany , â€œ Coupled two way clustering analysis of gene microarray data , â€ PNAS , vol . 97 , no . 22 , p . 12079 , 2000 .
[ 3 ] A . Ben Dor , B . Chor , R . Karp , and Z . Yakhini , â€œ Discovering local structure in gene expression data : the order preserving submatrix problem , â€ J . of Computational Biology , vol . 10 , no . 3 4 , pp . 373â€“384 , 2003 .
[ 4 ] J . Ihmels , S . Bergmann , and N . Barkai , â€œ Defining transcription modules using large scale gene expression data , â€ Bioinformatics , vol . 20 , no . 13 , pp . 1993â€“2003 , 2004 .
[ 5 ] A . PreliÂ´c , S . Bleuler , P . Zimmermann et al . , â€œ A systematic comparison and evaluation of biclustering methods for gene expression data , â€ Bioinformatics , vol . 22 , no . 9 , pp . 1122â€“1129 , 2006 .
[ 6 ] G . Pandey , G . Atluri , M . Steinbach , C . L . Myers , and V . Kumar , â€œ An association analysis approach to biclustering , â€ in SIGKDD . ACM , 2009 , pp . 677â€“686 .
[ 7 ] C . Huttenhower , K . Mutungu , N . Indik , W . Yang , M . Schroeder , J . Forman , O . Troyanskaya , and H . Coller , â€œ Detailing regulatory networks through large scale data integration , â€ Bioinformatics , vol . 25 , no . 24 , pp . 3267â€“3274 , 2009 .
[ 8 ] G . Li , Q . Ma , H . Tang , A . Paterson , and Y . Xu , â€œ Qubic : a qualitative biclustering algorithm for analyses of gene expression data , â€ Nucleic acids res , vol . 37 , no . 15 , pp . e101â€“e101 , 2009 .
[ 9 ] A . Tanay , R . Sharan , and R . Shamir , â€œ Discovering statistically significant biclusters in gene expression data , â€ Bioinformatics , vol . 18 , no . suppl 1 , pp . S136â€“S144 , 2002 .
[ 10 ] S . Hochreiter , U . Bodenhofer , M . Heusel et al . , â€œ Fabia : factor analysis for bicluster acquisition , â€ Bioinformatics , vol . 26 , no . 12 , pp . 1520â€“1527 , 2010 .
[ 11 ] Q . Sheng , Y . Moreau , and B . De Moor , â€œ Biclustering microarray data by gibbs sampling , â€ Bioinformatics , vol . 19 , no . suppl 2 , p . ii196 , 2003 . [ 12 ] L . Lazzeroni and A . Owen , â€œ Plaid models for gene expression data , â€
Statistica Sinica , vol . 12 , no . 1 , pp . 61â€“86 , 2002 .
[ 13 ] J . Gu and J . Liu , â€œ Bayesian biclustering of gene expression data , â€ BMC genomics , vol . 9 , no . Suppl 1 , p . S4 , 2008 .
[ 14 ] Y . Kluger , R . Basri , J . Chang , and M . Gerstein , â€œ Spectral biclustering of microarray data : coclustering genes and conditions , â€ Genome research , vol . 13 , no . 4 , pp . 703â€“716 , 2003 .
[ 15 ] M . Lee , H . Shen , J . Huang , and J . S . Marron , â€œ Biclustering via sparse singular value decomposition , â€ in Biometrics , vol . 66 , 2010 , pp . 1087â€“ 1095 .
716
[ 16 ] M . Sill , S . Kaiser , A . Benner , and A . Kopp Schneider , â€œ Robust biclustering by sparse singular value decomposition incorporating stability selection , â€ Bioinformatics , vol . 27 , no . 15 , pp . 2089â€“2097 , 2011 .
[ 17 ] H . Lee , R . Grosse , R . Ranganath , and A . Y . Ng , â€œ Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations , â€ in ICML . ACM , 2009 , pp . 609â€“616 .
[ 18 ] Q . V . Le , R . M . MarcAurelio Ranzato , K . C . Matthieu Devin , and J . D . Greg Corrado , â€œ Building high level features using large scale unsupervised learning , â€ in ICML . ACM , 2012 , pp . 131â€“138 .
[ 19 ] H . Lee , C . Ekanadham , and A . Ng , â€œ Sparse deep belief net model for visual area v2 , â€ NIPS , vol . 20 , pp . 873â€“880 , 2008 .
[ 20 ] C . Bishop , Pattern recognition and machine learning , 2006 . [ 21 ] D . Liu and J . Nocedal , â€œ On the limited memory bfgs method for large scale optimization , â€ Mathematical programming , vol . 45 , no . 1 , pp . 503â€“ 528 , 1989 .
[ 22 ] J . Nocedal and S . J . Wright , Numerical optimization . Springer Science+
Business Media , 2006 .
[ 23 ] C . Goller and A . Kuchler , â€œ Learning task dependent distributed representations by backpropagation through structure , â€ in IEEE Int . Conf . on Neural Networks , vol . 1 .
IEEE , 1996 , pp . 347â€“352 .
[ 24 ] M . Halkidi , Y . Batistakis , and M . Vazirgiannis , â€œ On clustering validation techniques , â€ Journal of Intelligent Information Systems , vol . 17 , no . 2 , pp . 107â€“145 , 2001 .
[ 25 ] Y . Hoshida , J . Brunet , P . Tamayo , T . Golub , and J . Mesirov , â€œ Subclass mapping : identifying common subtypes in independent disease data sets , â€ PLoS ONE , vol . 2 , 2007 .
[ 26 ] L . vanâ€™t Veer , H . Dai , V . D . Vijver et al . , â€œ Gene expression profiling predicts clinical outcome of breast cancer , â€ Nature , vol . 415 , no . 6871 , pp . 530â€“536 , 2002 .
[ 27 ] A . Su , M . Cooke , K . Ching et al . , â€œ Large scale analysis of the human and mouse transcriptomes , â€ PNAS , vol . 99 , no . 7 , p . 4465 , 2002 .
[ 28 ] A . Rosenwald , G . R . Wright , W . Chan et al . , â€œ The use of molecular profiling to predict survival after chemotherapy for diffuse large b cell lymphoma , â€ New England Journal of Medicine , vol . 346 , no . 25 , pp . 1937â€“1947 , 2002 .
[ 29 ] A . Bhattacharjee , W . Richards , J . Staunton et al . , â€œ Classification of human lung carcinomas by mrna expression profiling reveals distinct adenocarcinoma subclasses , â€ PNAS , vol . 98 , no . 24 , p . 13790 , 2001 .
[ 30 ] Y . Cheng and G . Church , â€œ Biclustering of expression data , â€ in ISMB , vol . 8 , 2000 , pp . 93â€“103 .
[ 31 ] S . Busygin , O . Prokopyev , and P . Pardalos , â€œ Biclustering in data mining , â€ Computers & Operations Research , vol . 35 , no . 9 , pp . 2964â€“2987 , 2008 . [ 32 ] H . Zhao , L . Cloots et al . , â€œ Query based biclustering of gene expression data using probabilistic relational models , â€ BMC Bioinformatics , vol . 12 , 2011 .
[ 33 ] F . Alqadah , J . S . Bader , R . Anand , and C . K . Reddy , â€œ Query based biclustering using formal concept analysis . â€
[ 34 ] B . Hanczar and M . Nadif , â€œ Precision recall space to correct external indices for biclustering , â€ in ICML , 2013 , pp . 136â€“144 .
[ 35 ] D . Chakrabarti , S . Papadimitriou , D . Modha , and C . Faloutsos , â€œ Fully automatic cross associations , â€ in SIGKDD , 2004 , pp . 79â€“88 .
[ 36 ] I . Dhillon , S . Mallela , and D . Modha , â€œ Information theoretic co clustering , â€ in SIGKDD . ACM , 2003 , pp . 89â€“98 .
[ 37 ] H . Shan and A . Banerjee , â€œ Bayesian co clustering , â€ in ICDM .
IEEE ,
2008 , pp . 530â€“539 .
[ 38 ] M . Shafiei and E . Milios , â€œ Latent dirichlet co clustering , â€ in ICDM .
IEEE , 2006 , pp . 542â€“551 .
[ 39 ] G . Bisson and C . Grimal , â€œ Co clustering of multi view datasets : a parallelizable approach , â€ in ICDM .
IEEE , 2012 , pp . 828â€“833 .
[ 40 ] A . Painsky and S . Rosset , â€œ Exclusive row biclustering for gene expresIEEE , 2012 , sion using a combinatorial auction approach , â€ in ICDM . pp . 1056â€“1061 .
[ 41 ] S . Wullf , R . Urner , and S . Ben David , â€œ Monochromatic bi clustering , â€ in ICML , 2013 .
[ 42 ] E . Airoldi , D . Blei , S . Fienberg , and E . Xing , â€œ Mixed membership stochastic blockmodels , â€ JMLR , vol . 9 , pp . 1981â€“2014 , 2008 .
[ 43 ] Z . Zhang , T . Li , C . Ding , X . Ren , and X . Zhang , â€œ Binary matrix factorization for analyzing gene expression data , â€ Data Min Knowl Disc , vol . 20 , pp . 28â€“52 , 2010 .
[ 44 ] P . Miettinen and J . Vreeken , â€œ Model order selection for boolean matrix factorization , â€ in SIGKDD . ACM , 2011 , pp . 51â€“59 .
[ 45 ] K . Kontonasios and T . De Bie , â€œ An information theoretic approach to finding informative noisy tiles in binary databases , â€ 2010 .
