Efficient Online Sequence Prediction with Side
Information
Han Xiao
Institute of Informatics
Claudia Eckert
Institute of Informatics
Technische Universit¨at M¨unchen , Germany
Technische Universit¨at M¨unchen , Germany
Email : xiaoh@intumde
Email : claudiaeckert@intumde
Abstract—Sequence prediction is a key task in machine learning and data mining . It involves predicting the next symbol in a sequence given its previous symbols . Our motivating application is predicting the execution path of a process on an operating system in real time . In this case , each symbol in the sequence represents a system call accompanied with arguments and a return value . We propose a novel online algorithm for predicting the next system call by leveraging both context and side information . The online update of our algorithm is efficient in terms of time cost and memory consumption . Experiments on real world data sets showed that our method outperforms state of the art online sequence prediction methods in both accuracy and efficiency , and incorporation of side information does significantly improve the predictive accuracy .
I .
INTRODUCTION
Online sequence prediction is the problem of observing a sequence of symbols one at a time and predicting the next symbol before it is revealed . This technique has been successfully applied in a large variety of disciplines , such as stock market analysis , natural language processing and DNA sequencing . The problem of sequence prediction has received considerable attention throughout the years in information theory , machine learning and data mining . Typically , the Markov property is assumed when modeling a sequence . That is , a finite history of the past , ie the context , can be useful in predicting the future . The length of the context is called the order of Markov models . Previous work shows ample evidence of the fact that making such an assumption is often reasonable in a practical sense [ 1 ] , [ 2 ] . For instance in natural language processing , it is often well enough to describe text by a fixed order Markov models ( eg bigram , trigram ) , though the next word is not necessarily related to its previous words .
Our motivating application is modeling the execution path of a process on a desktop/mobile system in real time . Each process produces an ordered sequence of system calls which request different services from the operating system . An illustrative example is depicted in Fig 1 .
Three remarks are in order . First , some system calls have a long range dependency . For instance , after creating a file the process may produce hundreds of system calls before it finally closes the file . In this case , the dependency between creat and close can not be observed from a short context of close . Although one can increase the order of Markov models to capture information from a long distant context , it is often difficult in practice due to the requirement of vast amounts of training data and more sophisticated smoothing algorithms [ 3 ] . In general , the length of context needed to make an accurate prediction is not constant , but rather depends on the recently executed system calls . Second , the information from the arguments and return values ( eg file descriptor , memory address and signal ) may be also indicative in predicting the next system call . Considering a process repetitively reads data from the file 1 and writes data to the file 2 . A resulting system call sequence may look like open(1 ) , read(1 ) , close(1 ) , open(2 ) , write(2 ) , close(2 ) , open(1 ) ,
Assume that we have observed the above sequence with seven system calls ; the goal is to predict the next system call . Without using the knowledge of the arguments , a bigram model based merely on the name of adjacent system call will predict read and write with even chance . However , as the file 2 has been closed , the correct prediction should be read . Although one can solve this problem by extending Markov models with more sophisticated graphical models , incorporating side information is in general not straightforward for probabilistic Markov models . Third , a process may exhibit different behaviors at various points during its lifetime , depending on user ’s input and the status of the system . In other word , the sequence is usually not stationary and no prior assumption on its distribution should be made . This suggests the necessity of an online model that can be continuously updated , preserving information from a long distant context while giving more emphasis to recent data , so that the stationarity is not required .
We focus on the problem of predicting the next system call given an observed sequence . The solution of this problem can be extremely useful in a wide range of applications , such as anomaly detection [ 4 ] , [ 5 ] , buffer cache management in operating system [ 6 ] , power management in smartphones [ 7 ] and sandbox systems [ 8 ] . We leverage both context and side information of each system call and model a sequence in an online fashion . The proposed algorithm performs prediction in real time and can quickly update the model when a prediction error is made .
The rest of the paper is organized as follows . Section II briefly reviews previous work on sequence prediction . Subsequently , our novel contribution is highlighted . Section III describes the problem formulation . We next cast sequence prediction as a linear separation problem in Section IV . The proposed method is presented in Section V . Experimental results are demonstrated in Section VI . Section VII concludes the paper and points out some future directions . p u o r g _ t i x e
: s 9 6 2 2 1 0 . 0 > e v c e x e
: s 0 0 0 0 0 0 . 0 < p a m n u m
: s 1 5 1 2 1 0 . 0 e s o l c
: s 3 0 2 2 1 0 . 0
.
0
1
0 k r b
: s 2 1 5 0 0 0 . 0 p a m m
: s 7 5 7 0 0 0 . 0 t a t s
: s 7 0 7 1 0 0 . 0 t a t s
: s 5 7 2 2 0 0 . 0 t a t s
: s 9 7 6 2 0 0 . 0 t a t s
: s 7 7 0 3 0 0 . 0 t a t s
: s 8 9 4 3 0 0 . 0 t a t s
: s 1 1 8 3 0 0 . 0 t a n t e s p o : s : 9 s 0 9 3 9 0 3 4 0 0 0 0 . . 0 0 e e t e t i e t i r e t i r w t i r w e e t t i i r e s o l r w w
:
: c
: s
2
1 e t i r w
: s e i r w
: s t r w
: s
3 s
4
9
8
4 e i
: s
7 w
3 t r
: s
1
7 e i w t r
: s
5
2
8 s
0
6
8
1 e i w
: t i r w
: e t i r s w
:
7 e t r s
3 w
:
9
5 s
4
1
7
1
1
5
7
1
1
5
0
9
6
1
1
0
.
5
0
1
1
0
.
1
1
0
.
0
0
0
.
1
2
0
2
9
1
9
1
1
0
1
0
1
0
.
.
.
0
0
0 e t i r w i r p a w s
7
:
3
4
1
1 s
2
1
: s
6
4
6
1
1
0
0
.
0
1
0
.
0
.
0
8
0
3
1
1
0
3
1
.
0
.
0 t a t s m m
: s
:
1
4
1
0
1
.
0
0
0
1
0
.
0
.
0
0
.
0
0
2
1 s
9
7
1
1
1 f
: s
4
1
1
1
1
0
.
0.010871s : close 0.010822s : getdents 0.010450s : getdents 0.010388s : openat 0.010301s : ioctl 0.010222s : ioctl 0.010134s : close 0.010091s : mmap 0.010039s : fstat 0.009993s : open 0.009903s : munmap 0.009847s : close 0.009775s : read 0.009669s : read 0.009586s : mmap 0.009463s : fstat 0.009396s : open 0.009342s : brk
0
.
0
0 0 9 9
0 0 8
0 9 0 7
9 1 2
3
3 7 3 s
. 0 0
8
0
.
0
.
0
0
0 9 s
1 s : s :
: g
0 .
0
0 .
0 s b t r a k t f s c t n m a s k o r
: o r p g i s
_ t i m i l r t e i t c a g i s
_ t r
: s
0
2
9 o i t c a g s
_ t r
: s
7
5
8
8
0 i
0.008760s : set_robust_list 0.008719s : set_tid_address 0.008665s : munmap 0.008622s : mprotect 0.008573s : mprotect 0.008522s : mprotect 0.008467s : mprotect 0.008420s : mprotect n
0
0
.
0
0
.
0
.
0
0
0
0
0
.
0
0
6
0
.
0
.
0
0
0
6
0
1
5
2 s
:
0
0
4 s
: m m a p c l o o p s e
0
.
0
.
0
0
0
.
0
0
0
6
0
1
2 s
:
6
5 s
6
0
2
3
:
0 s
2
3
: f
4 s s r e n e a d
0
0
.
0
.
0
0
6
0
2
.
0
0
6
.
0
0
6
3
9
0
0
6
7
4
2
0
0
6
8
5
4 s
4
: m s m t a
: m m a p t
6
8
0
6 s
9
4
1 s
:
: m a p p m r
1
7 s
: m o
6
9 s
: c l a t
8 s
: o p e s
: r p o s
: f e e e m s a n c t
0 0 . . 0 0 0 0 8 8 3 7 3 1 1 s 9 : s
: m p m r p o r t o e t c e t c t
0 . 0 0 8 2 6 2 s : m p r o t e c t
0 . 0 0 8 2 0 9 s : m p r o t e c t
0 . 0 0 8 0 6 3 s : a r c h _ p r c t l
0 . 0 0 8 0 1 3 s : m m a p
0 . 0 0 7 9 4 0 s : m m a p
0 . 0 0 7 8 9 0 s : c l o s e
0 . 0 0 7 8 3 4 s : m m a p
0 . 0 0 7 7 8 6 s : m p r o t e c t
0
0
0
.
.
.
0
0
0
0
7
0
7
0
1
7
0
6
9
8 s
3 s
:
: m m m m a p
0
7
1
2
5
0
0 . 0 0 7 7 4 3 s : m m a p
0
.
0 . 0 0 7 6 3 5 s : r e a d
0 . 0 0 7 5 8 4 s : o p e n
0 . 0 0 7 6 8 6 s : f s t a t s
: c l o s e m m t d p a a r p t a o p t e c t
0.004053s : fstat 0.004077s : mmap 0.004100s : close 0.004147s : open 0.004170s : read 0.004198s : fstat 0 . 0 0 4 2 4 5 s : m p r o t e c t 0 . 0 0 4 2 2 4 s : m m a p
0
4
2
6
8 s
: m m a p
2
9
8 s
: m m a p
: c l o
0
. 0
0
. 0
0
.
0
0
.
4
0 0 . s
1 5 5
2
3 5 4
2 5 4 0
4 0 0
3 4 0 0 o
:
: s
: s 9 s 5 7 e n s e p e f r a s d t
0
0
.
0
0 . 0 0 4 6 0 5 s : m m a p 0 . 0 0 4 6 2 8 s : m m a p 0 . 0 0 4 6 4 8 s : m p r o t e c t 0 . 0 0 4 6 7 0 s : m m a p 0 . 0 0 4 6 9 6 s : c l o s e 0.004908s : open 0.004932s : read 0.004956s : fstat 0.004982s : mmap 0.005002s : mprotect 0.005024s : mmap 0.005050s : close 0.005258s : open 0.005282s : read 0.005305s : fstat 0.005330s : mmap 0.005351s : mprotect 0.005372s : mmap
5
.
0
0
3
9
7 s
:
0
5
4
2
1
6
Call open read t a
Argument
( "/lib/librt.so" , O_RDONLY )
( 3 , "\177ELF\2\1\1" )
Return
= 3
= 832 fstat
( 3 , {st_mode=S_IFREG , st_size=317} ) = 0 mmap mmap
( NULL , 4096 , PROT_READ|PROT_WRITE ) = 0x7f2f7
( NULL , 2129016 , PROT_READ )
= 0x7f2fc mprotect ( 0x7f2f7 , 2093056 , PROT_NONE )
= 0 mmap
( 0x7f2fa , 8192 , PROT_READ )
= 0x7f2fb close
( 3 )
= 0
Fig 1 . Left panel shows a circular plot of a system call trace when running ls on Linux , which was collected using strace . System calls are plotted clockwise , starting with execve and ending with exit on top . A time stamp is labeled in front of each system call . A curve connects two system calls if the return value of the former was used as an argument of the latter . On the right panel , a sample segment of this sequence is detailed , with argument defined within the parentheses . For the sake of clarity , some long arguments ( eg string ) are omitted . The dependencies between the return value and argument are highlighted with arrow lines .
II . RELATED WORK
The problem of sequence prediction has a fairly long history and has received much attention from the field of game theory [ 9 ] , [ 10 ] , [ 11 ] , information theory [ 12 ] , [ 13 ] , [ 14 ] , [ 15 ] , and machine learning [ 16 ] , [ 17 ] , [ 18 ] , [ 19 ] , [ 20 ] , [ 21 ] . One of the most useful tools is context trees , which store informative histories and the probability of the next symbol given these [ 15 ] , [ 22 ] . Context trees use only a few recently observed symbols for prediction . The number of symbols that are used depends on the specific context in which the prediction is made . The motivation for exploring context tree strategies stems from their simplicity and their success in lossless data compression applications [ 23 ] . Another family of approach based on Bayesian nonparametric models has generated considerable recent research interest [ 24 ] , [ 19 ] , [ 20 ] . It is assumed that the distribution of the current symbol is determined by some random process ( eg Dirichlet process , Pitman Yor process ) governed by its context . The hierarchy is defined recursively to the first symbol in the sequence , on which a global base distribution is defined . These models give state of the art performance in language modeling , however , inference in such models is not straightforward . It often relies on repeated random sampling ( eg Markov chain Monte Carlo ) , which can be time consuming in practice .
Another related line of work is online learning , which takes place in a sequence of consecutive rounds . On each round , the learner is given a question and is required to provide an answer to this question . The performance of an online learning algorithm is measured by the cumulative loss suffered by the prediction along the run on a sequence of question answer pairs . The Perceptron algorithm [ 25 ] , [ 26 ] , [ 27 ] is perhaps the first and simplest online learning algorithm designed for answering yes/no questions . Adaptations of the Perceptron for multiclass categorization tasks include [ 28 ] , [ 29 ] . As the Perceptron algorithm is essentially a gradient descent ( firstorder ) method , recent years have seen a surge of studies on the second order online learning [ 30 ] , [ 31 ] , [ 32 ] . For example , the confidence weighted algorithm [ 32 ] maintains a Gaussian distribtuion over some linear classifier hypotheses and employs it to control the online update of parameters . Several work has followed this idea and showed that parameters’ confidence information can significantly improve online learning performance [ 32 ] , [ 33 ] , [ 34 ] , [ 35 ] .
The system call sequence was mainly studied by computer security researchers in the early days [ 36 ] , [ 37 ] , [ 4 ] . They used patterns in the sequence to identify misuses and intrusions in systems . To contain the attack preemptively , plan recognition was developed , aiming at recognizing and predicting goals based on observed system call sequences [ 38 ] , [ 39 ] . Recently , the problem of system call prediction attracted much attention due to its importance in many applications . For example , in a sandbox the amount of time that a process must suspend for a security check can be eliminated when the current system call is correctly predicted , yielding a more efficient sandbox implementation [ 8 ] . On mobile devices it has been demonstrated that system calls prediction can be used to design user oriented prefetching techniques [ 6 ] and reduce power consumption [ 7 ] . However , most of these studies are over simplistic in the sense that they focused only on the names of system calls and overlooked the arguments and return values . One possible reason is the difficulty in representing this side information , which requires a different modeling technique , such as rule learning [ 40 ] , [ 41 ] . Hence , it can not be incorporated into a sequence prediction model in a straightforward manner .
In this paper we introduce a novel online algorithm for predicting system calls in a sequence . Our algorithm combines the ideas from both context trees [ 18 ] , [ 42 ] and second order online learning algorithms [ 32 ] , [ 33 ] , [ 34 ] , [ 35 ] . Unlike previous work on system call prediction that only uses context information , we also consider side information such as arguments , return values and structures into learning and prediction . The side information can be straightforwardly incorporated into our model , giving a further boost to the accuracy of prediction . Furthermore , we propose several techniques to improve the efficiency ( in terms of both time and memory ) of our algorithm on long sequences , yielding a good scalabilty on big data .
III . PROBLEM FORMULATION
We denote the alphabet of the observed symbols as Σ := {1 , . . . , K} . Let Σ∗ be the set of all finite length sequences over the alphabet Σ . Specifically , the empty sequence ǫ is included in Σ∗ . We focus on the online learning framework , where learning is performed in rounds . Let x[t ] ∈ Σ be the tth symbol in a sequence . Denote x[1:t−1 ] ∈ Σ∗ be the context of x[t ] , ie x[1:t−1 ] := x[1 ] , . . . , x[t−1 ] . For completeness , let x[t:t−1 ] := ǫ . On round t , the algorithm first predicts ˆx[t ] ∈ Σ according to its current prediction rule and the context x[t:t−1 ] . After that , the true symbol x[t ] is revealed and the algorithm suffers a loss which reflects the degree to which its prediction was wrong . The algorithm then has the option to modify its prediction rule , with the explicit goal of improving the accuracy of its predictions for the rounds to come .
Assume that any symbol in the sequence is determined by its context , the problem of sequence prediction can be formulated as finding a function f : Σ∗ → Σ . To predict := f ( x[1:t−1] ) . We the tth symbol one can simply set ˆx[t ] generalize this definition and allow the algorithm to output predictions from a real valued set Y . Specifically , let Y := RK and f : Σ∗ → Y , where a prediction y ∈ Y is interpreted as a degree of confidence for each of the symbols in Σ . Consequently , the mapping from a score vector y to an actual symbol in Σ is via ˆx := arg maxk∈Σ yk . On round t , the loss of f is measured by a zero one loss function ℓ1,f ; ( x[1:t−1 ] , x[t]) .
That is , ℓ1 is zero if ˆx[t ] = x[t ] . Therefore , our ultimate goal is to incrementally learn a function f which minimizes
1 T
T
Xt=1
ℓ1f ; ( x[1:t−1 ] , x[t ] ) , where T is the length of the sequence .
IV . SEQUENCE PREDICTION AS LINEAR SEPARATION
Having described a general scheme for sequence prediction , we now focus on determining the form of f to obtain a concrete algorithm . In what follows we cast the sequence prediction problem as the problem of linear separation in a Hilbert space , which is a popular topic in machine learning . We shall see that by doing so one can harness powerful machine learning tools such as the Perceptron algorithm [ 26 ] , [ 27 ] and online convex programming [ 43 ] to our purpose .
As it was suggested in the Section I , the number of previous symbols needed to make an accurate prediction is usually not constant , but rather depends on the identity of those symbols . With this consideration in mind , we define a suffix closed set V ⊂ Σ∗ such that for every s ∈ V , every suffix of s ( including ǫ ) is also contained in V . To allow the algorithm to look as far back as needed , we can set V to be large enough . Specifically , let H be the Hilbert space of square integrable functions ψ : V → R endowed with the inner product hζ , ψi = Xs∈V
ζ(s)ψ(s ) , and the induced norm kζk = phζ , ζi . Note that if we can bound |V | by a constant , then the Hilbert space H is isomorphic to the |V | dimensional vector space , ie R|V | .
On round t the context x[1:t−1 ] is observed , we map this sequence to the function ψ ∈ H as follows
ψ(s[1:i ] ) :=  
1 e−ρi 0 if s[1:i ] = ǫ if s[1:i ] ∈ suf(x[1:t−1 ] ) otherwise
,
( 1 ) where suf(x[1:t−1 ] ) denotes the set of all suffixes of x[1:t−1 ] . The decay factor ρ > 0 is a predefined hyperparameter and mitigates the effect of long contexts on the function ψ . It is noticed from Eq ( 1 ) that all suffixes of x[1:t−1 ] are mapped to non zero values ; the value tends to decrease as the length of suffix increases . This idea expresses the assumption that symbols appearing earlier in a sequence have the least importance in modeling the current symbol . As we shall see in Section V C , this assumption can be infringed to some extent by incorporating side information into our model .
We have mapped sequences to functions in H . The next step is to create separating hyperplanes in H for prediction . We employ a multi class context tree . A multi class context tree is a K ary tree , each node of which represents one of the sequences in V . Specifically , the root of the tree represents the empty sequence ǫ . The node that represents the sequence x[i:j ] is the child of the node representing the sequence x[i+1:j ] . An observed sequence thus defines a path from the root of the tree to one of its nodes . Note that this path can either terminate at an inner node or at a leaf . We associate each node with a Kdimensional vector . In other words , a multiclass context tree can be represented as a function τ : V → RK . An illustrative example is given in Fig 2 . In particular , if we only look at the kth element of the vector on every node and denote the corresponding context tree as τk : V → R , then it is easy to verify that τk is embedded in H .
To construct the context tree on rounds , we initialize τ [ 1 ] to be a tree of a single ( the root ) node which assigns a weight of zero to the empty sequence , ie V [ 1 ] := {ǫ} . After receiving x[1:t−1 ] , a trivial solution is adding all sequences in the set suf(x[1:t−1 ] ) to V [ t ] and associate each of which with an undetermined vector in RK . The method for determining the value of these vectors will be presented in Section V A . For a long sequence adding all suffixes to the tree can impose serious computational problems , as the required memory for storing the tree grows quadratically with t . We shall resolve this issue in Section V B .
Returning to our sequence prediction problem , let x[1:t−1 ] be the sequence of observed symbols on round t , and let ψ[t ] be its corresponding function in H defined by Eq ( 1 ) . Denote τ [ t ] k as the current context tree subject to the class k . By following the description in Section III , the prediction problem can be formulated as
ˆx[t ] := arg max
.
( 2 ) k E k∈Σ Dψ[t ] , τ [ t ] {z } f ( x[1:t−1 ] )
|
Geometrically , we can consider H as a space partitioned by K hyperplanes , whose normal are given by τ1 , . . . , τk , respectively . The tth symbol is then predicted by picking the hyperplane that gives the maximum ( signed ) distance to the vector ψ[t ] .
( −1 , 0.4 , 1 )
1
“ b ”
( 0 , 0.2 , 2 )
2
“ a ”
ǫ
“ b ”
( 1 , 0.2 , 0 )
3
“ a ”
“ c ”
( 0.6 , 0.5 , −1 )
4
( −0.3 , 0.8 , 0.1 )
5
“ c ”
( 0.2 , 0.9 , 0.2 )
( −1 , −1 , 0.3 )
6
“ b ”
7
( 0.3 , 0.5 , −1 )
“ a ”
8
( 0.1 , 0.6 , −0.2 )
“ b ”
9
Fig 2 . An example of a multi class context tree , where K = 3 and V = {ǫ , a,ba,b,ab,cb,c,bc,abc,bbc} . The label on each node represents the index . Notice how the index matches the element in V . The context associated with each node is indicated on the edges of the tree along the path from the root ǫ to that node . The vector associated with each node is provided above each node . This context tree can be parameterized as a 10 × 3 matrix , with the first column ( 0 , 0 , 0)⊤ corresponds to the empty sequence ǫ . Considering the context tree as a function , given an input sequence “ aabbc ” , the output from this context tree is ( 0.1 , 0.6 , −0.2 ) , whose path is plotted with double lines .
V . ONLINE LEARNING ALGORITHM
It can be seen in Section IV , our predictor is fully specified by a multi class context tree τ , which can be represented by τ1 , . . . , τK . Given a fixed V , we can parameterize τk by a vector wk ∈ R|V | . Denote W := [ w1 , . . . , wK ] , in which rows correspond to vectors associated with each node as depicted in Fig 2 . The size of W is thus |V |× K . Note that a context tree is now fully specified by its weight vector W and structure V . That is , every {W , V } represents a unique τ and vice versa . Therefore , the problem of learning an accurate predictor can be reduced to the problem of determining W and V . Denote ψ[t ] ∈ R|V | a vector corresponding to the sequence x[1:t−1 ] . To construct ψ[t ] we simply follow Eq ( 1 ) and only assign values to the sequences in x[1:t−1 ] ∩ V . Elements of ψ[t ] are indexed in the same order as w[t ] k . Thus , the score vector y described in Section III amounts to ( W[t])⊤ψ[t ] .
This section describes our online learning algorithm in four parts . We first describe the method to learn W , subsequently , we present an approach for constructing V in a memoryefficient way . Extension for incorporating side information is described towards the end . Finally , several implementation issues are highlighted .
A . Learning Weight Vectors
We first show how the update of W can be performed in rounds . Our method is closely related to the family of confidence weighted linear classifiers [ 32 ] , [ 33 ] , [ 34 ] , [ 35 ] . Following the idea of previous work , we maintain a Gaussian distribution for every column of W with a mean vector µk ∈ R|V | and a diagonal covariance matrix Λk ∈ R|V |×|V | , ie wk ∼ N ( µk , Λk ) . Notice that by restricting Λk to a diagonal matrix , the weights become independent 1 . This is not true in real world , yet it is necessary due to the large value of |V | . For the sake of efficiency , the predicted symbol µk · ψ[t ] instead of using is simply approximated by arg max k∈Σ weight vectors sampled from N ( µk , Λk ) . In other words , the information captured by Λk does not influence the decision . This is analogous to Bayes point machines [ 44 ] .
On each round , we update the model by minimizing the Kullback Leibler divergence between new distribution and the old one while ensuring that the probability of correct prediction on tth symbol is not smaller than the confidence hyperparameter η ∈ [ 0 , 1 ] . After revealing the true symbol r := x[t ] , we need to update ( µk , Λk ) to the solution of the following optimization problem k
µ[t+1 ]
, Λ[t+1 ] k st
µ,Λ
= arg min DKLN ( µ , Λ ) flfl N µ[t ] Prw∼N ( µ,Λ)hwr · ψ[t ] ≥ w · ψ[t]i ≥ η . k , Λ[t ] k
( 3 )
Notice that the optimization problem in Eq ( 3 ) needs to be solved K − 1 times for every k ∈ Σ \ r on each round , which can be computationally expensive . We therefore provide a simplified algorithm , where only two updates is required on each round . The intuition was to ensure that the true symbol is more likely to be predicted than the symbol that is its closest competitor . Specifically , let s be the highest ranked wrong symbol on round t . That is , s := arg max k∈Σ\r
µ[t ] k · ψ[t ] .
( 4 )
In each round only ( µr , Λr ) and ( µs , Λs ) are updated as
1One may consider W as a random variable from a matrix normal distribution , ie W ∼ MN ( M , U , V ) , where U and V represents the correlation among row and among column , respectively . However , under the assumption of independent weights and independent symbols , U and V are simply diagonal matrices . This results an equivalent formulation to ours . follows where φ = Φ−1(η ) . Further , we introduce a loss function as r
µ[t+1 ] µ[t+1 ] s
, Λ[t+1 ] r st
, Λ[t+1 ] s st
µ,Λ
DKLN ( µ , Λ ) flfl N µ[t ] = arg min Prw∼N ( µ,Λ)hw · ψ[t ] ≥ ws · ψ[t]i ≥ η . = arg min DKLN ( µ , Λ ) flfl N µ[t ] Prw∼N ( µ,Λ)hwr · ψ[t ] ≥ w · ψ[t]i ≥ η .
µ,Λ r , Λ[t ] s , Λ[t ]
( 5 ) r s
( 6 )
Notice how the constraint of Eq ( 5 ) and Eq ( 6 ) differs from each other . We follow the derivation in [ 33 ] and obtain the closed form update as r
µ[t+1 ] µ[t+1 ] s
=µ[t ] =µ[t ] r + αΛ[t ] s − αΛ[t ] r ψ[t ] s ψ[t ]
Λ[t+1 ] r
Λ[t+1 ] s
=Λ[t ] =Λ[t ] r −1 s −1
+ 2αφ diag2ψ[t]−1 + 2αφ diag2ψ[t]−1
,
( 7 ) ( 8 )
( 9 )
( 10 ) where diag2ψ[t ] is a diagonal matrix made from the squares of the elements of ψ[t ] on the diagonal . The inverse of diagonal matrix can be computed element wise . The coefficient α is calculated as follows
α =
−(1 + 2φm ) +p(1 + 2φm)2 − 8φ(m − φv )
4φv
, where r − µ[t ] s · ψ[t ] m =µ[t ] r ⊤ v =µ[t ]
φ =Φ−1(η ) ,
Λrµ[t ] s ⊤ r −µ[t ]
Λsµ[t ] s
( 11 )
( 12 )
( 13 ) and Φ−1(· ) is the inverse of the normal cumulative distribution function . k := 0 and Λ[1 ]
For initialization we set µ[1 ] k := I for all k , where I is the identity matrix . It is noticed from Eq ( 7 ) and Eq ( 8 ) that during online learning the mean weight vector is updated in a similar fashion as in the Perceptron . The confidence of all observed suffixes is increased by shrinking the corresponding value on the diagonal of Λk ( see Eq ( 9 ) and Eq ( 10) ) , which leads to the update of weight vector in the next round more focusing on low confidence features .
B . Memory Efficient Update of Suffix Set
Having described the method for learning the weight vectors of the context tree , we now focus on determining its structure , ie V . Instead of adding all suffixes of the context to V on each round , we introduce three strategies for constructing V in a memory efficient way .
First of all , we only update V if the probability constraint
Prwr ∼N ( µr ,Λr ) ws∼N ( µs,Λs ) hwr · ψ[t ] ≥ ws · ψ[t]i ≥ η
( 14 ) is violated . Note that Eq ( 14 ) can be rewritten as
( µr − µs ) · ψ[t ] ≥ φrψ[t]⊤
( Λr + Λs)ψ[t ] , k=1 ; ( x[1:t−1 ] , x[t ] ) :=
ℓφ{(µk , Λk)}K max 0 , φrψ[t]⊤
( Λr + Λs)ψ[t ] − ( µr − µs ) · ψ[t]! .
( 15 )
It is easy to verify that satisfying the probability constraint Eq ( 14 ) is equivalent to satisfying ℓφ = 0 . In this case , we simply set V [ t+1 ] to be equal to V [ t ] . Otherwise we add all sequences in suf(x[1:t−1 ] ) to V [ t ] . Note that ρ and φ can be tuned as a trade off between the passiveness and aggressiveness of the update .
Second , when a sequence is extremely long , adding all suffixes of a long context can impose serious memory growth problem . Hence , it is not a practical solution . To limit the maximum depth of the context tree , we prune the context x[1:t−1 ] to a certain length κ[t ] before adding its suffixes to V , where κ[t ] is given by
κ[t ] = min 1
ρ log ℓ1(t ) , t − 1 , with ℓ1(t ) denoting the number of prediction mistakes made by the algorithm so far . The intuition behind is to limit the depth of the context tree by the number of prediction mistakes , which is inspired by [ 18 ] . As a consequence , one can straightforwardly translate the mistake bound of confidence weighted classifier ( Theorem 4 in [ 33 ] ) into a bound on the growth rate of the resulting context tree [ 18 ] , [ 42 ] . giving smallest Pk µ2
Finally , we limit the size of V by removing the elements k,i when |V | exceeds the maximum allowed size Q , where µk,i is the ith element of µk and i ∈ [ 1 , Q ] . This criterion has been shown effective in recursive feature elimination [ 45 ] and has a good theoretical sup port [ 46 ] , [ 47 ] . Alternatively , one can also use the Pk 1/λk,i or Pk |µk,i|/λk,i as the criterion , where λk,v is the vth element on the diagonal of Λk . By employing the above three strategies the context tree grows at a much slower pace and the algorithm can utilize memory more conservatively . Finally , the pseudo code of our algorithm is summarized in Algorithm 1 , which is called EOSP in the sequel for short .
C . Incorporation of Side Information
Thus far we augment only context information from the sequence . As we highlighted in Section I side information of system calls can support the prediction of the next symbol . Apart from that , in language modeling grammars ( eg part of speech tags ) , topics , styles are helpful to predict the next word [ 48 ] , [ 49 ] . Comparing to the n gram models and Bayesian nonparametrics models [ 24 ] , [ 19 ] , a key advantage of our approach is its simplicity of leveraging side information . Specifically , if side information on round t can be given in the form of a vector b[t ] ∈ RB , we can directly incorporate it into the prediction via a linear combination as follows
ˆx[t ] := arg max k∈Σ
µ[t ] k · ψ[t ] + γ [ t ] k · b[t ] .
This corresponds to replacing ψ[t ] in Algorithm 1 as a ( Q+B)dimensional vector [ ψ[t ] , b[t] ] . The dimension of the mean vector and confidence matrix associated with each symbol is
Algorithm 1 : Efficient online sequence prediction ( EOSP ) . Input
: Damping factor : ρ > 0 ; confidence parameter : η ∈ [ 0 , 1 ] ; maximum allowed size of V : Q > 0 . : Mean vectors and confidences matrices : {(µk , Λk)}K Initialize : ∀k ∈ Σ , ( µ[1 ] k ) = ( 0 , I ) , φ = Φ−1(η ) , k=1 ; set : V . k , Λ[1 ]
Output
V [ 1 ] = {ǫ} ;
1 for t = 1 , 2 , . . . do 2
3 4 5 6 7
8 9
10
11
12
13
Construct ψ[t ] from x[1:t−1 ] ; Rank all symbols by µ[t ] Receive the true symbol r ; Compute s ; Suffer loss ℓφ ; if ℓφ > 0 then k · ψ[t ] ; while |V [ t]| > Q − κ[t ] do
/* Eq ( 1 ) */
/* Eq ( 4 ) */ /* Eq ( 15 ) */ j=1,,QPk∈Σ µ2 i = arg min ∀k ∈ Σ , µk,i = 0 ; Remove the ith sequence from V [ t ] ; k,j ;
V [ t+1 ] = V [ t ] ∪ {x[t−i:t−1 ] | 1 ≤ i ≤ κ[t]} ; ) and ( µ[t+1 ] Set ( µ[t+1 ] ) ; /* Eqs . ( 7 ) to ( 10 ) */
, Λ[t+1 ]
, Λ[t+1 ] s s r r extended accordingly . Note that an ineffective representation of side information can adversely affect the prediction performance as well , hence there has to be some mechanism for selecting features that really contribute to prediction . In our algorithm , this can be done by constantly setting µk,i+b to zero if Pk |µk,i+b|/λk,i+b is too small . In addition , one can γIB×B with 0 < γ < 1 also initialize Λk := I|V |×|V | to balance the learning rate of the weights on the context and side information . Specifically , when γ = 1 the side information shares the same learning rate with context information ; when γ = 0 the side information does not contribute the learning and prediction at all .
The side information used in this work is summarized in Table I . The idea of using these attributes is mainly based on our experiences and observations . For instance , we observed that system calls with similar functionality tend to occur together , which could be due to some sub task of the process . Thus , if a particular group of system calls is frequently observed in the recent context , then the next system call is very likely from the same group . In present work , system calls are grouped manually by their documentation , which is partially based on [ 50 ] . It is also possible to automatically group system calls by using topic models [ 51 ] . Another observation is that a block of system calls repeats themselves especially when some of them return an error . This was probably attributed to the exception handling ( eg restart mechanism ) of a process . Thus , a simple statistic of the error codes is maintained and considered as one of the evidences for predicting the next system call . In practice , the side information listed in Table I can be easily extracted from the context with negligible computational cost .
D . Efficient Implementation
It can be observed from Eqs . ( 7 ) to ( 10 ) that most of the entries of ψ , µk and Λk are zero , which implies a possibility to improve the efficiency by storing them in a compact way . In the implementation , we store µk , Λk and ψ in sparse vectors . The algorithms of addition and dot product for sparse vectors can be found in [ 52 ] . Moreover , as the updates of ( µr , Λr ) and ( µs , Λs ) are independent to each other , line 13 Algorithm 1 can be implemented in a parallel manner . Furthermore , the operations on V can be implemented efficiently using a data structure called suffix trie . Finally , removing one element at a time ( line 8 to 11 Algorithm 1 ) is time consuming and in practice we remove as much as half of Q when |V [ t]| > Q − κ[t ] .
VI . EXPERIMENTAL RESULTS
Two sets of experiments were carried out to validate our algorithm . First , we compared the accuracy and efficiency of EOSP with state of the art sequence prediction methods . Second , we investigated several factors that affect the performance of EOSP in order to gain more insights of it .
The experiments were conducted on three groups of data . The first set of data is from BSM ( Basic Security Module ) data portion of 1998 DARPA intrusion detection evaluation data set created by MIT Lincoln Labs2 . We used a subset of training data , which contained four hour BSM audit data of all processes running on a Solaris machine . Each system call was recorded with its corresponding arguments and return value . The second group of data was obtained from University of New Mexico [ 4 ] , in which system call traces of several process were generated in either “ synthetic ” or “ live ” manner3 . Our experiment was conducted on their “ live ” normal data , where traces of programs were collected during normal usage of real users . Unlike DARPA data set , a trace in UNM data set is just a list of system call names ; no arguments and return values are available . Therefore , for UNM data set only the functional group in Table I was available as side information . The third data set was collected by ourselves4 . By using strace and a prepared script , we collected system call sequences with their corresponding arguments and return values from all executable programs on an Ubuntu system . The program options were chosen solely for the purpose of exercising the program , and not to meet any real user ’s requests . From these three sources we selected a total of 8 data sets , and their characteristics are summarized in Table II .
TABLE II .
CHARACTERISTICS OF DATA SETS USED IN THE
EXPERIMENT .
Data set darpa lpr1 lpr2 sendmail1 sendmail2 stide1 stide2 ubuntu
# calls
# seq . Min . len .
Max . len .
Avg . len .
243 182 182 190 190 164 164 458
200 2 , 766 1 , 232 8 , 000 8 , 000 8 , 000 8 , 000 1 , 218
2 82 74 8 8 225 108 2
3 , 074 59 , 565 39 , 306 173 , 664 149 , 616 146 , 695 174 , 401 53 , 247
57 1 , 080 449 669 648 1 , 055 1 , 255 952
2http://wwwllmitedu/mission/communications/cyber/CSTcorpora/ideval/ data/
3http://wwwcsunmedu/∼immsec/systemcalls 4URL is masked for blind review .
TABLE I .
SIDE INFORMATION USED IN OUR ALGORITHM FOR SYSTEM CALL PREDICTION .
Feature set
Size
Description
File descriptor
File type
Functional group
2
9
9
The number of opened files and the number of closed files , respectively .
Each element represents the number of opened files of a particular type , such as RDONLY , WRONLY , APPEND , etc .
Each element represents the number of occurrences of system calls associated with a group given a context . The groups were built in advance by categorizing similar system calls together , resulting 9 groups in total . For instance , the “ file ” group includes creat , open , close , read , etc . The “ process ” group includes fork , wait , exec , etc . The “ signal ” group includes signal , kill , alarm , etc .
Access location
12
Each element represents the number of accesses to a particular directory , such as /usr/bin , /usr/lib , /usr/tmp , etc .
Error code
POSIX signal
String character
124
28
256
Each element represents the number of caught errors of each code , such as ENOENT , EAGAIN , EBGDF , etc .
Each element represents the number of sent signals of each type , such as SIGSEGV , SIGABRT , SIGBUS etc .
Each element represents the frequency value of a string character . A char is considered as an 8 bit value , resulting 256 possible characters . We only count characters in the string that is not file path .
Four sequence prediction methods were employed in the experiment . They were interpolated Kneser Ney ( IKN ) [ 3 ] , online prediction suffix tree ( PST ) [ 18 ] , sequence memoizer ( SM ) [ 20 ] , and learning experts ( LEX ) [ 21 ] . We restricted the maximum length of context to 50 for all algorithms except for SM , which was designed for modeling context with infinite length . Specifically , we used a 50 gram IKN in our experiment . For LEX the number of experts was set to one and d := 50 , resulting an individual sequence predictor trained with the log loss . The maximum depth of the context tree for PST was set to 50 . These four methods were compared with the proposed EOSP , and the algorithm with side information denoting as EOSPs . The confidence parameter η was 0.8 ; the damping factor ρ was 0.1 ; the maximum length of the context was 50 and the maximum size of V was 20 , 000 .
A . Comparison of Predictive Performance
The comparison of predictive performance between different methods is summarized in Table III , where the online error rate and perplexity were used as evaluation metrics . The online error rate of an algorithm on a given input sequence is defined to be the number of prediction mistakes the algorithm makes on that sequence normalized by the length of the sequence . The perplexity reflects an algorithm ’s performance when taking its probabilistic output into account . For EOSP we just normalized the score vector to obtain the prediction probability Pr[ˆx[t ] | x1:t−1 ] . The reported results were averaged over all sequences in each data set respectively .
It is evident from the results that , EOSP and EOSPs gave a considerably better prediction than other baseline algorithms . In particular , EOSPs achieved the best performance on the majority data sets ( seven out of eight in terms of perplexity ) , which indicates the effectiveness of incorporating side information into the model . On five out of six UNM data sets , we observed an improvement by just incorporating the functional group information . On darpa and ubuntu data sets where side information are fully available , a striking improvement of EOSPs over EOSP was observed . In general , we found SM is a strong competitor in terms of online error rate . However , EOSP and EOSPs still outperformed SM with appreciable lower perplexity on all data sets . This suggests a potentially valuable property for our method , eg for combining it with other probabilistic model in a big system . Moreover , SM is much slower than EOSP on long sequence , as we shall see in the next experiment .
TABLE III .
EXPERIMENTAL RESULTS ON DIFFERENT DATA SETS .
SMALLER VALUE INDICATES BETTER PERFORMANCE .
( a ) Online error rate ( % ) of different algorithms . SM
EOSP
IKN
PST
Data set darpa lpr1 lpr2 sendmail1 sendmail2 stide1 stide2 ubuntu
EOSPs 48.17 41.53 47.03 34.26 33.91 8.29 7.75 36.13
50.11 41.63 47.44 33.47 33.11 8.34 7.75 40.90
52.14 41.09 47.61 35.62 33.52 8.54 8.09 38.90
49.25 46.24 48.52 33.65 34.17 8.59 7.95 39.23
49.75 40.88 47.24 33.06 32.19 8.41 7.78 75.26
LEX
51.11 42.27 51.15 36.81 38.96 9.06 8.51 52.72
( b ) Online perplexity of different algorithms .
Data set darpa lpr1 lpr2 sendmail1 sendmail2 stide1 stide2 ubuntu
EOSP
48.98 9.82 12.94 8.31 8.33 1.42 1.39 33.13
EOSPs 40.23 9.23 11.08 8.17 7.96 1.41 1.41 31.65
IKN
78.34 16.14 21.43 11.48 11.50 2.08 1.98 42.81
PST
98.97 17.05 16.34 30.34 30.38 3.42 3.23 35.35
SM
63.07 14.75 19.94 9.23 9.17 1.92 1.67 68.25
LEX
82.36 11.71 22.19 11.90 12.46 4.06 4.51 35.62
B . Comparison of Efficiency
The comparison of computation speed and memory consumption for all algorithms is shown in Fig 3 and Fig 4 , respectively . We concatenated all traces in sendmail1 to obtain a long sequence , and tested different methods on this sequence with increasing length . The setup of each method was same as in the last experiment . For the sake of fair comparison , all algorithms were implemented in C/C++ . We only plotted the curve for EOSP as EOSPs took almost same amount of time and memory in our experiment . As can be seen in Fig 3 , EOSP showed a substantial reduction of time comparing to other baseline algorithms . Moreover , the time cost of EOSP only increased at a very low pace with respect to the length of the sequence . As we expected , LEX and SM were extremely slow especially on long sequences , since on each round they require gradient descent and Gibbs sampling , respectively . On contrary , in EOSP one only need to compute dot products of sparse vectors on each round , which can be done efficiently . On the other hand , though the memory consumption of EOSP is higher than other baselines at the beginning , it remained almost constant with increasing length of the sequence . Methods such as IKN and LEX , however , consume more and more memory as the sequence becomes longer . This demonstrates the effectiveness of our update strategies described in Section V B .
4 10
3 10
2 10
1 10
] s [ t s o c e m T i
0 10
2
4 allows the algorithm to perform a more confident update on each round , which generally leads to higher predictive accuracy when the data is noise less .
TABLE IV .
PERFORMANCE OF EOSP WRT DIFFERENT SETTINGS OF
CONFIDENCE PARAMETER η . SMALLER VALUE INDICATES BETTER
PERFORMANCE .
( a ) Online error rate ( % ) of EOSP
Data set darpa lpr1 lpr2 sendmail1 sendmail2 stide1 stide2 ubuntu
0.6
50.41 41.61 47.21 34.25 33.32 8.21 7.79 44.77
0.7
50.52 41.55 47.02 34.12 33.10 8.25 7.79 44.76
0.8
50.11 41.63 47.44 33.47 33.11 8.34 7.75 40.90
0.9
50.39 41.43 46.56 33.12 33.74 8.13 7.64 44.60
( b ) Perplexity of EOSP
Data set darpa lpr1 lpr2 sendmail1 sendmail2 stide1 stide2 ubuntu
0.6 48.82 9.74 12.62 8.67 8.69 1.43 1.40 32.93
0.7 49.05 9.73 12.57 8.64 8.67 1.44 1.41 33.02
0.8 48.98 9.82 12.94 8.31 8.33 1.42 1.39 32.93
0.9 49.03 9.72 12.33 8.44 8.47 1.42 1.39 32.29
Table V summarizes the results of EOSP with respect to different maximum length of the context , where η := 0.8 and Q := 20 , 000 . Although one may expect an improvement of the predictive accuracy by allowing the algorithm to look back long distant context , we found that the optimal length of the context varies with data sets . On darpa , lpr1 and stide1 , for example , the context length of 40 was sufficient for a good prediction ; increasing this length did not improve the prediction . On ubuntu , the online error rate decreased with increasing context length up to 100 . In general , we found that the prediction of EOSP is not adversely affected by the overlength context , though its efficiency can be degraded due to more memory consumption .
TABLE V .
PERFORMANCE OF EOSP WRT DIFFERENT MAXIMUM
LENGTH OF CONTEXT .
( a ) Online error rate ( % ) of EOSP
Data set darpa lpr1 lpr2 sendmail1 sendmail2 stide1 stide2 ubuntu
Data set darpa lpr1 lpr2 sendmail1 sendmail2 stide1 stide2 ubuntu
20
40
60
50.21 41.66 47.45 35.70 33.67 8.45 8.02 41.84
50.10 41.63 47.45 33.48 33.60 8.27 7.75 41.23
50.10 41.63 47.45 33.47 33.11 8.27 7.75 40.90 ( b ) Perplexity of EOSP 60 20
40
48.97 9.78 12.94 8.36 8.38 1.43 1.40 33.15
48.97 9.82 12.93 8.31 8.32 1.42 1.39 33.13
48.98 9.82 12.94 8.31 8.32 1.42 1.40 33.12
80
50.10 41.65 47.45 33.47 33.11 8.27 7.75 40.75
100
50.10 41.65 47.45 33.47 33.11 8.27 7.75 40.69
80
48.98 9.82 12.94 8.31 8.33 1.42 1.40 33.11
100
48.98 9.82 12.94 8.31 8.33 1.42 1.40 33.10
Finally , to study the performance with respect to different sizes of V , we fixed η := 0.8 and the maximum length of context to 50 . Results are summarized in Table VI . It was
EOSP
IKN
PST
SM
LEX
40
8
16 20
Length of the sequence ( × 104 )
Fig 3 . Time cost in second ( averaged over 10 runs ) of different algorithms . Both axes are in logarithmic scale .
2 10
]
B M
[ d e s u y r o m e M
1 10
0 10
10
−1
2
4
8
16 20
Length of the sequence ( × 104 )
EOSP
IKN
PST
SM
LEX
40
Fig 4 . Memory consumption ( averaged over 10 runs ) of different algorithms . Both axes are in logarithmic scale .
C . Exploration of Model Parameters
In order for EOSP to be a practical tool in real world applications , it is necessary to make decisions about the details of its specification . Our exploration focused on three parameters that mainly govern the performance of EOSP . Namely , the confidence parameter η , the maximum length of the context , and the maximum size of V . We focused only on EOSP and ignored all side information in this set of experiments .
The performance of EOSP with respect to different settings of confidence parameter η is summarized in Table IV . We fixed the maximum length of the context to 50 and maximum size of V to 20 , 000 . On the majority of data sets , the online error rate hit the bottom when η is around 09 However , the lowest perplexity was often observed when η := 0.8 ; the perplexity slightly increased after that . In general , bigger value of η found that on the majority of data sets predictive performance can be improved by allowing V to contain more suffixes , which can be expected . However , on darpa data set having at most 4 , 000 suffixes in V was sufficient for obtaining a good result ; increasing the upper bound of |V | did not improve the performance but raised the memory consumption . This is probably due to that most sequences in darpa are short ( with average length of 57 ) and hence there are not many combinations for frequently occurred subsequences . In general , if the patterns in a sequence are simple ( especially with some periodicity ) , then one can set a small size for V .
TABLE VI .
PERFORMANCE OF EOSP WRT DIFFERENT MAXIMUM
SIZE ( ×103 ) OF V .
( a ) Online error rate ( % ) of EOSP .
Data set darpa lpr1 lpr2 sendmail1 sendmail2 stide1 stide2 ubuntu
1
2
4
50.39 42.45 48.21 38.01 34.98 9.23 8.74 42.73
50.21 42.18 48.00 36.57 33.55 8.99 8.57 42.20
50.13 41.83 47.56 35.92 32.88 8.62 8.28 41.97 ( b ) Perplexity of EOSP .
8
50.13 41.68 47.45 35.24 32.60 8.32 7.90 41.51
16
50.13 41.65 47.45 34.63 32.43 8.27 7.85 41.21
Data set darpa lpr1 lpr2 sendmail1 sendmail2 stide1 stide2 ubuntu
1
48.97 10.37 13.54 9.46 9.49 1.50 1.46 33.53
2
48.98 10.15 13.44 8.84 8.88 1.47 1.45 33.30
4
48.98 9.93 13.01 8.47 8.49 1.45 1.42 33.19
8
48.98 9.82 12.94 8.31 8.33 1.43 1.40 33.15
16
48.98 9.82 12.94 8.31 8.33 1.43 1.40 33.11
VII . CONCLUSIONS
Motivated by the problem of system call prediction , this work has proposed a novel method for predicting the next symbol in a sequence . Unlike previous methods our algorithm does not rely on a fixed length context during learning and can be easily incorporated with side information . The algorithm maintains a set of distributions over parameters . On each round , the distributions are updated to satisfy a probabilistic constraint . The update can be computed in closed form and implemented using sparse vectors . Moreover , we proposed several strategies to reduce the memory consumption , allowing a good scalability on long sequences . An improvement of accuracy and efficiency over existing methods has been demonstrated in the experiments .
Our method can serve as a backbone in a wide range of real time applications , such as intrusion detection and power consumption modeling on mobile devices . Comparing to previous methods in this area , our algorithm allows one to incorporate the domain knowledge as side information to improve prediction . Besides , our framework can be also adopted to perform other tasks , such as language modeling and structure prediction . An important question for future studies is to explore theoretical properties of our algorithm , such as the convergence rate under different noise settings . In particular , it would be interesting to develop a robust algorithm for predicting sequence with adversarial noise .
ACKNOWLEDGMENT
This work is funded by the HIVE project
( FKZ : 16BY1200D ) of the German Federal Ministry of Education and Research . We would like to thank Yurong Tao for help with experiments , and anonymous reviewers for helpful suggestions .
REFERENCES
[ 1 ]
J . Ziv and A . Lempel , “ Compression of individual sequences via variable rate coding , ” Information Theory , IEEE Transactions on , vol . 24 , no . 5 , pp . 530–536 , 1978 .
[ 2 ] P . F . Brown , P . V . Desouza , R . L . Mercer , V . J . D . Pietra , and J . C . Lai , “ Class based n gram models of natural language , ” Computational linguistics , vol . 18 , no . 4 , pp . 467–479 , 1992 .
[ 3 ] S . F . Chen and J . Goodman , “ An empirical study of smoothing techniques for language modeling , ” in Proceedings of the 34th annual meeting on Association for Computational Linguistics . Association for Computational Linguistics , 1996 , pp . 310–318 .
[ 4 ] C . Warrender , S . Forrest , and B . Pearlmutter , “ Detecting intrusions using system calls : Alternative data models , ” in Security and Privacy , 1999 . Proceedings of the 1999 IEEE Symposium on . IEEE , 1999 , pp . 133– 145 .
[ 5 ] E . Eskin , W . Lee , and S . J . Stolfo , “ Modeling system calls for intrusion detection with dynamic window sizes , ” in DARPA Information Survivability Conference & Exposition II , 2001 . DISCEX’01 . Proceedings , vol . 1 .
IEEE , 2001 , pp . 165–175 .
[ 6 ] P . Fricke , F . Jungermann , K . Morik , N . Piatkowski , O . Spinczyk , M . Stolpe , and J . Streicher , “ Towards adjusting mobile devices to users behaviour , ” in Analysis of Social Media and Ubiquitous Data . Springer , 2011 , pp . 99–118 .
[ 7 ] A . Pathak , Y . C . Hu , M . Zhang , P . Bahl , and Y M Wang , “ Finegrained power modeling for smartphones using system call tracing , ” in Proceedings of the sixth conference on Computer systems . ACM , 2011 , pp . 153–168 .
[ 8 ] Y . Oyama , K . Onoue , and A . Yonezawa , “ Speculative security checks in sandboxing systems , ” in Parallel and Distributed Processing Symposium , 2005 . Proceedings . 19th IEEE International . IEEE , 2005 , pp . 8–pp .
[ 9 ] H . Robbins , “ Asymptotically subminimax solutions of compound statistical decision problems , ” in Herbert Robbins Selected Papers . Springer , 1985 , pp . 7–24 .
[ 10 ] D . Blackwell , “ An analog of the minimax theorem for vector payoffs , ”
[ 11 ]
Pacific Journal of Mathematics , vol . 6 , no . 1 , pp . 1–8 , 1956 . J . Hannan , “ Approximation to bayes risk in repeated play , ” Contributions to the Theory of Games , vol . 3 , pp . 97–139 , 1957 .
[ 12 ] T . Cover and P . Hart , “ Nearest neighbor pattern classification , ” Information Theory , IEEE Transactions on , vol . 13 , no . 1 , pp . 21–27 , 1967 . [ 13 ] T . M . Cover and A . Shenhar , “ Compound bayes predictors for sequences with apparent markov structure , ” Systems , Man and Cybernetics , IEEE Transactions on , vol . 7 , no . 6 , pp . 421–424 , 1977 .
[ 14 ] M . Feder , N . Merhav , and M . Gutman , “ Universal prediction of individual sequences , ” Information Theory , IEEE Transactions on , vol . 38 , no . 4 , pp . 1258–1270 , 1992 .
[ 15 ] F . M . J . Willems , Y . M . Shtarkov , and T . J . Tjalkens , “ The contexttree weighting method : Basic properties , ” Information Theory , IEEE Transactions on , vol . 41 , no . 3 , pp . 653–664 , 1995 .
[ 16 ] D . P . Helmbold and R . E . Schapire , “ Predicting nearly as well as the best pruning of a decision tree , ” Machine Learning , vol . 27 , no . 1 , pp . 51–68 , 1997 .
[ 17 ] F . C . Pereira and Y . Singer , “ An efficient extension to mixture techniques for prediction and decision trees , ” Machine Learning , vol . 36 , no . 3 , pp . 183–199 , 1999 .
[ 18 ] O . Dekel , S . Shalev Shwartz , and Y . Singer , “ Individual sequence prediction using memory efficient context trees , ” Information Theory , IEEE Transactions on , vol . 55 , no . 11 , pp . 5251–5262 , 2009 .
[ 41 ] G . Tandon and P . Chan , “ Learning rules from system call arguments and sequences for anomaly detection , ” in ICDM Workshop on Data Mining for Computer Security ( DMSEC ) , 2003 , pp . 20–29 .
[ 42 ] N . Karampatziakis and D . Kozen , “ Learning prediction suffix trees with winnow , ” in Proceedings of the 26th Annual International Conference on Machine Learning . ACM , 2009 , pp . 489–496 .
[ 43 ] S . Shalev shwartz and Y . Singer , “ Convex repeated games and fenchel duality , ” in Advances in Neural Information Processing Systems 19 . MIT Press , 2006 .
[ 44 ] R . Herbrich , T . Graepel , and C . Campbell , “ Bayes point machines , ” The
[ 45 ]
Journal of Machine Learning Research , vol . 1 , pp . 245–279 , 2001 . I . Guyon , J . Weston , S . Barnhill , and V . Vapnik , “ Gene selection for cancer classification using support vector machines , ” Machine learning , vol . 46 , no . 1 3 , pp . 389–422 , 2002 .
[ 46 ] X w Chen , X . Zeng , and D . van Alphen , “ Multi class feature selection for texture classification , ” Pattern Recognition Letters , vol . 27 , no . 14 , pp . 1685–1691 , 2006 .
[ 47 ] O . Chapelle and S . S . Keerthi , “ Multi class feature selection with support vector machines , ” in Proceedings of the American statistical association , 2008 .
[ 48 ] T . L . Griffiths , M . Steyvers , D . M . Blei , and J . B . Tenenbaum , “ Integrating topics and syntax , ” Advances in neural information processing systems , vol . 17 , pp . 537–544 , 2005 .
[ 49 ] X . Wang , A . McCallum , and X . Wei , “ Topical n grams : Phrase and topic discovery , with an application to information retrieval , ” in Data Mining , 2007 . ICDM 2007 . Seventh IEEE International Conference on . IEEE , 2007 , pp . 697–702 .
[ 50 ] A . Silberschatz , P . B . Galvin , and G . Gagne , Operating system concepts .
J . Wiley & Sons , 2009 .
[ 51 ] H . Xiao and T . Stibor , “ A supervised topic transition model for detecting malicious system call sequences , ” in Proceedings of the 2011 workshop on Knowledge discovery , modeling and simulation . ACM , 2011 , pp . 23–30 .
[ 52 ] T . A . Davis , Direct methods for sparse linear systems .
Society for
Industrial and Applied Mathematics , 2006 , vol . 2 .
[ 19 ] F . Wood , C . Archambeau , J . Gasthaus , L . James , and Y . W . Teh , “ A stochastic memoizer for sequence data , ” in Proceedings of the 26th Annual International Conference on Machine Learning . ACM , 2009 , pp . 1129–1136 .
[ 20 ] F . Wood , J . Gasthaus , C . Archambeau , L . James , and Y . W . Teh , “ The sequence memoizer , ” Communications of the ACM , vol . 54 , no . 2 , pp . 91–98 , 2011 .
[ 21 ] E . Eban , A . Birnbaum , S . Shalev Shwartz , and A . Globerson , “ Learning the experts for online sequence prediction , ” in ICML , 2012 .
[ 22 ] R . Begleiter , R . El Yaniv , and G . Yona , “ On prediction using variable order markov models , ” J . Artif . Intell . Res . ( JAIR ) , vol . 22 , pp . 385–421 , 2004 .
[ 23 ] A . Martin , G . Seroussi , and M . J . Weinberger , “ Linear time universal coding and time reversal of tree sources via fsm closure , ” Information Theory , IEEE Transactions on , vol . 50 , no . 7 , pp . 1442–1468 , 2004 .
[ 24 ] Y . W . Teh , “ A hierarchical bayesian language model based on pitmanyor processes , ” in Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics . Association for Computational Linguistics , 2006 , pp . 985–992 .
[ 25 ] T . Motzkin and I . Schoenberg , “ The relaxation method for linear inequalities , ” Canadian Journal of Mathematics , vol . 6 , no . 3 , pp . 393– 404 , 1954 .
[ 26 ] F . Ronsenblatt , “ The perceptron : a probabilistic model for information storage and organization in the brain , ” Psychological review , vol . 65 , pp . 386–408 , 1958 .
[ 27 ] A . B . Novikoff , “ On convergence proofs for perceptrons , ” DTIC Doc ument , Tech . Rep . , 1963 .
[ 28 ] R . O . Duda , P . E . Hart et al . , Pattern classification and scene analysis .
Wiley New York , 1973 , vol . 3 .
[ 29 ] K . Crammer and Y . Singer , “ Ultraconservative online algorithms for multiclass problems , ” The Journal of Machine Learning Research , vol . 3 , pp . 951–991 , 2003 .
[ 30 ] N . Cesa Bianchi , A . Conconi , and C . Gentile , “ A second order perceptron algorithm , ” SIAM Journal on Computing , vol . 34 , no . 3 , pp . 640–668 , 2005 .
[ 31 ] C . Brotto , C . Gentile , and F . Vitale , “ On higher order perceptron algorithms , ” Advances in Neural Information Processing Systems , vol . 19 , 2007 .
[ 32 ] M . Dredze , K . Crammer , and F . Pereira , “ Confidence weighted linear classification , ” in Proceedings of the 25th international conference on Machine learning . ACM , 2008 , pp . 264–271 .
[ 33 ] K . Crammer , M . D . Fern , and O . Pereira , “ Exact convex confidenceweighted learning , ” in In Advances in Neural Information Processing Systems 22 . Citeseer , 2008 .
[ 34 ] K . Crammer , A . Kulesza , M . Dredze et al . , “ Adaptive regularization of weight vectors , ” Advances in Neural Information Processing Systems , vol . 22 , pp . 414–422 , 2009 . J . Wang , P . Zhao , and S . C . Hoi , “ Exact soft confidence weighted learning , ” arXiv preprint arXiv:1206.4612 , 2012 .
[ 35 ]
[ 36 ] S . Forrest , S . A . Hofmeyr , A . Somayaji , and T . A . Longstaff , “ A sense of self for unix processes , ” in In Proceedings of the 1996 IEEE Symposium on Security and Privacy . IEEE Computer Society Press , 1996 , pp . 120–128 .
[ 37 ] W . Lee , S . J . Stolfo , and P . K . Chan , “ Learning patterns from unix process execution traces for intrusion detection , ” in AAAI Workshop on AI Approaches to Fraud Detection and Risk Management , 1997 , pp . 50–56 .
[ 38 ] C . W . Geib and R . P . Goldman , “ Plan recognition in intrusion detection systems , ” in DARPA Information Survivability Conference & Exposition II , 2001 . DISCEX’01 . Proceedings , vol . 1 .
IEEE , 2001 , pp . 46–55 .
[ 39 ] L . Feng , X . Guan , S . Guo , Y . Gao , and P . Liu , “ Predicting the intrusion intentions by observing system call sequences , ” Computers & Security , vol . 23 , no . 3 , pp . 241–252 , 2004 .
[ 40 ] A . Chaturvedi , S . Bhatkar , and R . Sekar , “ Improving attack detection in host based ids by learning properties of system call arguments , ” in In Proceedings of the IEEE Symposium on Security and Privacy . Citeseer , 2005 .
