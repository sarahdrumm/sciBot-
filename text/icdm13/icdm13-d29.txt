Generative Maximum Entropy Learning for Multiclass Classification
Ambedkar Dukkipati , Gaurav Pandey , Debarghya Ghoshdastidar , Paramita Koley , D . M . V . Satya Sriram
Department of Computer Science and Automation
Email:{ad , gp88 , debarghya.g , paramita2000 , dmvsriram}@csaiiscernetin
Indian Institute of Science Bangalore , 560012 , India discriminative classification , a subclass of discriminative classification , models the posterior density directly , which is then used to classify a new instance . Examples of discriminative classifiers include logistic Regression and support vector machines ( SVM ) .
Abstract—Maximum entropy approach to classification is very well studied in applied statistics and machine learning and almost all the methods that exists in literature are discriminative in nature . In this paper , we introduce a maximum entropy classification method with feature selection for large dimensional data such as text datasets that is generative in nature . To tackle the curse of dimensionality of large data sets , we employ conditional independence assumption ( Naive Bayes ) and we perform feature selection simultaneously , by enforcing a ‘maximum discrimination’ between estimated class conditional densities . For two class problems , in the proposed method , we use Jeffreys ( J ) divergence to discriminate the class conditional densities . To extend our method to the multi class case , we propose a completely new approach by considering a multi distribution divergence : we replace Jeffreys divergence by Jensen Shannon ( JS ) divergence to discriminate conditional densities of multiple classes . In order to reduce computational complexity , we employ a modified Jensen Shannon divergence ( JSGM ) , based on AM GM inequality . We show that the resulting divergence is a natural generalization of Jeffreys divergence to a multiple distributions case . As far as the theoretical justifications are concerned we show that when one intends to select the best features in a generative maximum entropy approach , maximum discrimination using J−divergence emerges naturally in binary classification . Performance and comparative study of the proposed algorithms have been demonstrated on large dimensional text and gene expression datasets that show our methods scale up very well with large dimensional datasets .
Keywords Maximum Entropy ; Jefferys Divergence ; Jensen
Shannon Divergence ; Text categorization ;
I . INTRODUCTION
Broadly , supervised learning can be divided into discriminative learning and generative learning [ 1 ] . In the generative approach to classification the aim is to model the joint distribution of the data and the class labels from the training data . One can then compute the class conditional densities for each class and then assign the instance to the class with highest posterior probability . Examples of generative classifiers include linear discriminant analysis ( LDA ) and Bayes classifier .
On the other hand , one can directly model a discriminating function without actually constructing a model for the data . A discriminating function may be chosen so as to minimize some measure of error on the training data . Such an approach is termed as a discriminative classification . Statistical
The generative classifiers have a smaller variance than their discriminative counterparts , and hence require lesser data for training [ 1 ] to achieve their asymptotic error . This is in contrast to discriminative models , which tend to overfit the data , when the number of training instances is small . Furthermore , incomplete data and latent variables can be taken care of in generative models . Moreover , since generative models model the generation of the entire data ( hidden as well as observed ) , one can incorporate complex dependencies between data/features in the model , thereby allowing the construction of models that are closer to the true data generating mechanism . Any domain knowledge about the data generating mechanism can be incorporated in a generative model quite easily . Lastly , generative models are more intuitive to understand than their discriminative counterparts .
After the works of [ 2 ] , [ 3 ] , a variety of statistical methods and machine learning techniques have taken up ideas from information theory [ 4 ] , [ 5 ] . Maximum entropy or minimum divergence methods form a subclass of such techniques where the aim is to make minimum assumption about the data . These approaches proved to be more useful in the field of natural language processing and text classification [ 6 ] , [ 7 ] , where the curse of dimensionality becomes more significant . Variants of maximum entropy techniques have also been considered in literature . For instance , regularized maximum entropy models have been considered in [ 5 ] .
In spite of the vast literature surrounding maximum entropy models , almost all classification methods considered are discriminative in nature . The reason for this is that the partition function for most generative models cannot be obtained in closed form . However , as mentioned earlier , when the number of training instances is small , a discriminative model tends to overfit the data , and hence , a generative model must be preferred . Hence , we explore generative maximum entropy models in this paper and compare it with other discriminative methods . The use of a generative model also allows us to incorporate feature selection simultaneously .
Contributions
We propose a new method of classification using a generative model to estimate the class conditional densities . Furthermore , we perform feature selection using a discriminative criteria based on Jeffreys divergence . We call this method as Maximum Entropy with Maximum Discrimination ( MeMd ) . The basic approach is based on the idea presented in [ 8 ] that does not scale up for large datasets , and further , extension to multi class classification is not studied . We improve on the time complexity of the algorithm in [ 8 ] by assuming class conditional independence of all the features .
Most of the classification methods that are designed for binary case can be extended to multi class case by formulating the problem as several binary classification problems [ 9 ] . Among these ‘one vs all’ is well known and successfully applied to SVMs [ 10 ] .
One of our main contributions in this paper is the unique way we extend our binary classification method ( MeMd ) to multi class case . The main idea is to use Jensen Shannon divergence , which can be naturally defined for more than two distributions ( these divergences are known as multidistribution divergences [ 11] ) . To simplify the calculations , as well as the computational complexity , we replace arithmetic mean in JS−divergences with geometric mean and study performance of MeMd . We show that this leads to a multi distribution extension of Jeffreys divergence .
We perform experimental study of the proposed method on some large benchmark text datasets and show that the proposed method is able to do drastic dimensionality reduction and also give good accuracies comparable to SVMs and outperforms discriminative approaches .
II . PRELIMINARIES AND BACKGROUND
We dedicate this portion to set up the notations . The problem at hand is that of classification , ie , we are given a training data set with class labels {c1 , c2 , . . . , cM} . In binary classification , we have M = 2 . Consider each instance of the data to be of the form x = ( x1 , x2 , . . . , xd ) , where xi ∈ Xi is the ith feature of the data . So the input space is X = X1 × X2 × . . . × Xd . In this setting , our objective is to rank the features according to their ‘discriminative’ ability . A key step in the proposed algorithms is estimation of the class conditional densities , denoted as Pcj ( . ) := P ( .|cj ) , for each each class . This step requires some statistics of the data in the form of expected values of certain functions Γ = {φ1(x ) , φ2(x ) , . . . , φl(x)} , where φj , j = 1 , . . . , l , is defined over the input space X . These are often termed as feature functions [ 5 ] , and should not be confused with the features . In fact , the feature functions can be chosen so that they result in the moments of the individual features , for i , whose expected example it can be of the form φ(x ) = xk value gives the kth moment of the ith feature . Let X = ( X1 , . . . , Xd ) be a random vector that takes values from the set X . Suppose the only information ( observations ) available about the distribution of X is in the form of expected values of the real valued feature functions Γ = {φ1(x ) , . . . , φl(x)} . We therefore have ,
X
EP [ φr(X ) ] =
φr(x)P ( x ) dx , r = 1 , 2 , . . . , l ,
( 1 ) where these expected values are assumed to be known . In the maximum entropy approach to density estimation one choose the distribution of the form
−λ0 − l j=1
 ,
P ( x ) = exp
λjφj(x )
( 2 ) where λ0 , λ1 , . . . , λl are obtained by replacing ( 2 ) in ( 1 ) and X P ( x)dx = 1 . This is known as the maximum entropy ( ME ) distribution . In presence of an observed data {x(k ) , k = 1 , . . . , N} , in the maximum entropy modeling one assumes that the expected values of the moment constraint functions ie , EP [ φr(x ) ] can be approximated by observed statistics or sample means of φr(x ) , r = 1 , . . . , l [ 7 ] . Therefore we set
EP [ φr(x ) ] ≈ 1 N
φr(x(k ) ) = µemp r
, r = 1 , . . . , l . ( 3 )
For the estimation of Λ = ( λ0 , . . . , λl ) one can show that the maximum likelihood estimator Λ is given by ,
N k=1
N i=1
Λ
= argmax
Λ
1 N ln P ( x(i ) ; Λ , Γ ) ,
( 4 ) where , x(1 ) , . . . , x(N ) are assumed to be iid samples drawn from a unknown distribution . To solve ( 4 ) , one can use iterative methods like gradient descent method or iterative scaling methods [ 12 ] , [ 13 ] . Moving on to divergences , a symmetrized version of KL−divergence [ 3 ] is known as Jeffreys divergence [ 14 ] , or simply J−divergence . Given two pdfs P and Q , J−divergence between them is defined as
X
= dx .
J(P Q ) = KL(P Q ) + KL(Q P ) P ( x ) Q(x )
( P ( x ) − Q(x ) ) ln
( 5 ) Although J−divergence was proposed in the context of statistical estimation problems to provide measures of the discrepancy between two laws [ 14 ] , its connection to KL−divergence has made it popular in classification tasks [ 15 ] , [ 16 ] . Its relationship with other divergence measures have been studied in [ 17 ] . In fact , this J−divergence can also be obtained as a f−divergence [ 18 ] with a coupled convex function .
M
Jensen Shannon ( JS ) divergence appeared in the literature relatively recently [ 19 ] , and the unique characteristic of this divergence is that one can measure a divergence between more than two probability distributions . Hence , one can term this as a multi distribution divergence .
M πi ∈ [ 0 , 1 ] , i = 1 , . . . , M and M
Let P1 , . . . , PM be probability distributions and let P = i=1 πiPi be a convex combination of P1 , . . . , PM , where i=1 πi = 1 . Then JS−divergence ( or , information radius ) among P1 , . . . , PM is defined as
JS(P1 , . . . , PM ) =
πiKL(Pi P ) .
( 6 ) i=1
JS−divergence is non negative , symmetric and bounded . For k = 2 , it has been shown that it is the square of a metric [ 20 ] . Grosse et al . [ 21 ] studied several interpretations and connections of JS−divergence . However , the close association of this divergence to classification [ 19 ] makes it quite interesting in our work .
Related work Since the seminal work of [ 22 ] , a plethora of maximum entropy techniques for learning have been introduced in literature . For text classification , a simple discriminative maximum entropy technique was used in [ 7 ] to estimate the posterior distribution of class variable conditioned on training data . Each feature is assumed to be a function of the data and the class label . By maximizing the conditional entropy of the class labels , we get a distribution of the form
P ( c|x ) =
1
, i=1 exp
Z(x )
λifi(x , c )
( 7 ) where Z is the normalizing constant . The parameters λi , 1 ≤ i ≤ m are obtained by maximizing the conditional log likelihood function . It was shown that for some datasets , the discriminative maximum entropy model outperformed the more commonly used multinomial naive Bayes model significantly . An extension of this approach for sequential data is presented in [ 23 ] .
Now we proceed to the proposed generative maximum entropy classification and feature selection method .
III . MEMD FOR BINARY CLASSIFICATION
A . Why maximum discrimination ?
What is a ‘natural’ way to select features when the intention is to do maximum entropy Bayes classification ? In order to answer this question , we first explain what we mean by ‘natural’ with the help of an example . For notational convenience , we assume that the classes are labeled as +1 and −1 in this subsection . We will revert to our original notations at the end of this subsection . Let us assume that our decision surfaces ( or classification boundaries ) are hyperplanes of the form f ( x ) = {wT Φ(x)+ m b : ||w|| = 1} , where Φ = ( φ1 , . . . , φl ) is a vector of feature functions . Furthermore , assume that the weight vector w is known and the aim is to select a subset of feature functions . The obvious strategy to select features in such a scenario would be to select them so as to maximize either of the two quantities y(i ) y(i )
N i=1 N min i=1 wT Φ(x(i ) ) + b wT Φ(x(i ) ) + b
,
ˆΦsum = argmax
Φ
ˆΦmin = argmax
Φ where x(1 ) , x(2 ) . . . x(N ) are N iid samples that constitute the training data and y(i ) are their corresponding class labels taking values in {+1,−1} . Here , the quantity y(i)(wT Φ(x(i ) ) + b ) measures the classification margin for the point x(i ) . Ideally , we would like it to be as high as possible for all points .
The question that we wish to address is whether we can follow a similar approach to select subset of features for the Bayes classifier . In Bayes classification , a point x is assigned the label +1 if
π+P+(x ) > π−P−(x ) , which can be rewritten as log
π+P+(x ) π−P−(x )
> 0 .
Here π+ , π− are the prior probabilities and P+ and P− denote the class conditional probabilities of the two classes . Hence , the above quantity plays the same role as the equation of hyperplane in the former case . Therefore , we can define our Bayes’ classification margin for the point x as y log π+P+(x ) π−P−(x ) which must be positive for all correctly classified points . As in the case of hyperplanes , we can select features so as to maximize either of the two quantities .
Γ∗ sum = arg max
S∈2Γ y(i ) log
N i=1
Γ∗ min = arg max
, where Γ is the set of all feature functions and S ⊂ Γ .
S∈2Γ y(i ) log
N min i=1
( 8 )
π+P+(x(i ) ; S ) π−P−(x(i ) ; S ) π+P+(x(i ) ; S ) π−P−(x(i ) ; S )
If the class conditional distributions are obtained using maximum entropy by using the expected values of feature functions in S , we can further simplify ( 8 ) by plugging in the equation for maximum entropy distribution for the two classes . The corresponding feature selection problem then

φ∈S
( λ
φ − λφ ) y(i)φ(x(i ) ) becomes Γ∗ sum = arg max
S∈2Γ log
φ∈S
+ where
P ∗ +(x(i ) ; S ) =
P ∗ −(x(i ) ; S ) =
1 Z1
1 Z2 exp exp
+ log i=1
Z2 Z1
N − −
φ∈S
φ∈S y(i )
N i=1
π+ π−
,
  .
( 9 )
( 10 )
( 11 )
λφφ(x(i ) )
λ φφ(x(i ) )
We use the superscript ‘*’ to indicate that the class conditional distributions are the ME distributions .
If the number of training points in the two classes are equal , the first term in ( 9 ) can be discarded . Furthermore , by separating the terms in the two classes , we get
 × N ( 12 )
Γ∗ sum = arg max
S∈2Γ
( λ
φ − λφ)(µφ − µ φ )
It is easy to see that this is exactly N times the Jdivergence between the distributions in ( 10 ) and ( 11 ) . One can similarly show that when the number of points in the two classes are not equal , we still obtain the above equation if we assign proper weights to instances in the two classes ( based on the number of points in the class ) . Reverting back to our original notations , we can say that if one intends to use Bayes classifier where the class conditional distributions are obtained using maximum entropy , a natural way to do so would be to select features as below . ( x ; S)||P ∗
J(P ∗
( x ; S) ) ,
( 13 ) c1 c2
Γ∗ = arg max S∈2Γ cj where P ∗ ( x ; S ) indicates the ME distribution estimated for class cj using expected values of the feature functions that are in S ⊂ Γ . B . The MeMd Approach
The MeMd approach can be formulated as follows . Given a set of feature functions Γ = {φ1 , . . . , φl} , the problem is to find the subset Γ∗ ⊂ Γ such that
Γ∗ = arg max S∈2Γ
J(P ∗ c1
( x ; S)||P ∗ c2
( x ; S) ) ,
( 14 ) cj where P ∗ ( x ; S ) indicates the ME distribution estimated for class cj , j ∈ {1 , 2} , using expected values of the feature functions that are in S ⊂ Γ .
This problem is intractable particularly when large number of features are involved ( which is the case for high dimensional data ) since it involves estimation of J−divergence for 2l subsets to find the optimal subset from the given set of l feature functions . The problem was studied in Dukkipati et al . [ 8 ] where a greedy search was used to select the features thereby reducing the complexity from exponential to O(l2 ) . Even with this greedy approach , the method does not scale up well for large dimensional data such as text data . Moreover estimating the ME distributions , and hence finding the exact value of J−divergence between the estimated class conditional densities is computationally demanding especially for large dimensional data .
Our strategy is to use naive Bayes approach since for text data , naive Bayes classifiers have shown to outperform ( or given comparable performance ) compared with other classifiers [ 24 ] and its good performance is attributed to optimality under zero one loss function [ 25 ] . Therefore we have d i=1
Pcj ( x ) =
P ( i ) cj
( xi ) , where cj is the class label and P ( i ) cj for xi , the ith feature of the data x . is the marginal density
This leads to simplification of the greedy step of the algorithm proposed by Dukkipati et al . [ 8 ] to great extent as shown in following result . Theorem 1 . The feature chosen at the kth step of the greedy approach is the one with kth largest J−divergence between the marginals of class conditional densities .
Proof : Using the additivity of KL−divergence under independence , J−divergence between the two class conditional densities Pc1 and Pc2 can be written as d i=1 flflflflfl d i=1
P ( i ) c1
P ( i ) c2
=
J,P ( i ) c1 flflP ( i ) c2
. d i=1
J,Pc1 flflPc2
= J
( 15 ) Suppose a set S of ( k − 1 ) features are already chosen . The corresponding approximation of the class conditional density is i∈S and the optimal feature is
P ( i ) cj
Pcj ( x ) ≈  flflflP ( j ) i∈S∪{j}
P ( j ) c1
P ( i ) c1 i∈S
J
J
J c2
P ( i ) c1 flflflflflfl flflflflfl i∈S
. j∗ = arg max j /∈S
= arg max j /∈S
= arg max j /∈S
( xi ) , j = 1 , 2 , i∈S∪{j}
P ( i ) c2
P ( i ) c2
+ J

P ( j ) c1 flflflP ( j ) c2
The above maximization is equivalent to choosing the feature with kth largest J−divergence in the kth step . Thus J−divergence can be readily used to rank the features based on the discrimination between the two classes , and prune out those features which gives rise to a small value of J−divergence between the estimated marginal densities P ( i ) cj ( xi ) , which are of the ME form
−λ0ij − l
P ( i ) cj
( xi ) = exp
λkij φk(xi )
,
( 16 ) k=1 for each class cj , j = 1 , 2 , and each feature xi , i = 1 , 2 , . . . d .
For classification , any standard method can be used . However , since the class conditional densities are estimated during the above process , Bayes decision rule [ 26 ] turns out to be an obvious choice , ie , a test pattern is assigned class c1 if
Pc1 ( x)P ( c1 ) > Pc2(x)P ( c2 ) , otherwise to class c2 , where P ( c1 ) and P ( c2 ) are the priors for each class . Using only the top K features , the class conditional densities can be approximated as
P ( i ) cj
( xi ) , j = 1 , 2 .
( 17 )
Pcj ( x ) ≈ i∈S
So , the decision rule can be written as
P ( i ) P ( i ) c1 ( xi ) c2 ( xi )
>
P ( c2 ) P ( c1 )
, i∈S which , after taking logarithm , turns out to be i∈S l
( λ0i2 − λ0i1 +
( λki2 − λki1)φ(i ) > ln P ( c2 ) − ln P ( c1 ) . k=1 k ( xi ) )
( 18 )
We list the above method in Algorithm 1 . The corresponding experimental results are presented in Section VI .
An interesting fact to note here is that for distributions of the form in ( 16 ) , the J−divergence can be obtained in a simple form as given in the following result .
−λ0 −l l
−λ 0 −l
Remark 1 . Suppose there are two ME distributions and Q(x ) = P ( x ) = exp j=1 λjφj(x ) , obtained using same set of jφj(x ) exp feature functions {φ1 , . . . , φl} , but with different expected values {µ1 , . . . , µl} and {µ l} , respectively . Then
1 , . . . , µ j=1 λ
J(P Q ) =
( λ j − λj)(µj − µ j ) .
( 19 ) j=1
This result can be used to evaluate the J−divergence between the marginals of the class conditional densities for each feature , which can be used to rank all the features in O(d ) time ( in decreasing value of J−divergence ) . feature .
Algorithm 1 MEMD with Naive Bayes classification INPUT : for binary
• Two labeled datasets of class c1and c2 . • Data of the form x = ( x1 , x2 , . . . , xn ) , xi denoting ith
• A set of l constraints Γ(i ) =
φ(i ) 1 , . . . , φ(i ) l to be applied on each feature xi , i = 1 , . . . , d .
ALGORITHM :
1 ) ME densities for each P ( i ) cj , i = 1 , 2 , . . . , d and j = 2 ) J−divergence for each feature ( denote as Ji , i =
1 , 2 are estimated using ( 16 ) .
1 , . . . , d ) is calculated using ( 5 ) . 3 ) The features are ranked in descending order according to their J−divergence values , and the top K features are chosen ( to be considered for classification ) .
4 ) Bayes decision rule is used for classification us ing ( 18 ) .
IV . MULTI CLASS CLASSIFICATION
A . One vs . All Approach
The most common technique in practice for multi class classification is to use “ one vs . all ” approach , where the number of classifiers built is equal to the number of classes , ie , for each class we build a classifier for that class against all the other classes [ 9 ] . Incorporating such a technique does not affect the basic MeMd approach for ranking the features . Hence , Algorithm 1 can be easily extended to multi class case by using one vs . all approach as presented in Algorithm 2 .
Here , we consider a M class problem , with classes c1 , c2 , . . . , cM . The rest of the setting is same as Algorithm 2 . The modification can be described as follows . For j = ∪k=jck . So , the each class cj , consider the class c ‘discriminative capacity’ of each feature for a particular class can be measured by its J−divergence between the class conditional densities cj and c j ,
Jij := J
P ( i ) cj
,
( 20 ) flflflP ( i ) c j j and P ( i ) c where P ( i ) are the ME marginal densities for cj classes cj and c j , respectively , for the ith feature . Jij can be easily computed using ( 19 ) . However , this provides us a J−divergence for each feature for a particular class . A natural way to obtain a J−divergence for each feature is
Ji =
JijP ( cj ) ,
( 21 ) j=1 ie , the divergence is averaged over all the classes , weighted by their prior probabilities P ( cj ) . The algorithm is listed as below .
M j =
Algorithm 2 classification using one vs . all approach . INPUT :
( MeMd J )
: MEMD for multi class
• Labeled datasets of M classes . • Data of the form x = ( x1 , x2 , . . . , xn ) , xi denoting ith feature .
• A set of l constraints Γ(i ) =
φ(i ) 1 , . . . , φ(i ) l to be applied on each feature xi , i = 1 , . . . , d .
ALGORITHM :
1 ) Construct the classes c ck , j = 1 , 2 , . . . , M . k=j 2 ) ME densities P ( i ) , i = 1 , 2 , . . . , d and j = 3 ) The average J−divergence for each feature is calcu
1 , 2 , . . . , M are estimated using ( 16 ) . cj and P ( i ) c j lated using ( 21 ) . 4 ) The features are ranked in descending order according to their average J−divergence values , and the top K features are chosen ( to be considered for classification ) .
5 ) Bayes decision rule is used to assign a test pattern to cj∗ such that j∗ = arg max j=1,,M
Pcj ( x)P ( cj ) , where the class conditional densities are approximated as in ( 17 ) for j = 1 , 2 , . . . , M . j
The above approach is an obvious extension of the binary classification problem . Although the algorithm is O(d ) , it is relatively computationally inefficient in the sense that it requires estimation of the additional ME distributions P ( i ) c ( which is the most time consuming step of MeMd approach ) . Now , we present our main contribution in this paper . To deal with the multi class classification , we invoke a natural multidistribution divergence , the JS−divergence . B . Multi class Classification using JS−divergence
The JS−divergence , as mentioned in Section II can be defined over multiple distributions , and hence , would seem to be more useful for dealing with multi class problems . However , its most interesting feature is presented in the following result [ 21 ] . We state it in a form more suitable for our purpose , and interpret it for multi class classification similarity . Proposition 1 . Consider a M class problem with classconditional densities Pci ( x ) = P ( x|ci ) , i = 1 , 2 , . . . , M . If X is a random data , and Z is an indicator random variable , ie , Z = i when X ∈ ci , then
JS(Pc1 , . . . , PcM ) = M I(X ; Z ) .
Here , M I(X ; Z ) is the mutual information between X and Z , and the JS−divergence is computed using the priors as weights , ( ie , πj = P ( cj ) ) as
M
JS(Pc1 , . . . , PcM ) =
P ( cj)KL(PcjP ) ,
( 22 )
M j=1
P ( cj)Pcj ( X ) is the probability of where P ( X ) = j=1 sampling the data X . it
The problem of classification basically deals prediction of the value of Z , given the test data X . Hence , is natural to device a technique which would maximize the mutual information , or rather , maximize the JS−divergence among the class conditional densities . Thus JS−divergence turns out to be a more natural measure of discrimination compared to J−divergence . However , unlike J−divergence , JS−divergence does not satisfy the additivity property , there is no equivalent for ( 15 ) for JS−divergence . ie , Hence , the “ correct ” extension of the algorithm in case of JS−divergence becomes involved as we need to consider a quadratic time greedy approach , where at each iteration we select a new feature that maximizes JS−divergence among the classes and also has minimum mutual information with the previously chosen feature set .
The latter term leads to an O(d2 ) time complexity for the feature selection rule . A linear time algorithm similar to MeMd J can be obtained if we assume that all the features are independent , which is a quite strict assumption , particularly for text datasets . In spite of the above issues , use of JS−divergence is quite significant due to its theoretical justifications . Moreover , use of a multi distribution divergence significantly reduces the the number of models that needs to be estimated in a one vs all approach . Hence , we look for an approximation of the JS−divergence that some what retains its properties , and can also exploit the linearity of J−divergence .
V . JSGM− DISCRIMINATION
A . AM to GM
Although JS−divergence seemed to be a natural discriminative measure , its drawbacks from algorithmic point of view motivates us to look into some form of approximation in terms of J−divergence , which looked more promising ( computationally ) . The idea comes from [ 27 ] , which introduced the notion of average divergence by just taking average over KL−divergence between all possible pairs of distributions . weighted mean P =k
However , we approach the problem differently , but obtain the basic a similar result . The modification is made at definition of JS−divergence ( 6 ) , where we replace the i=1 πiPi by the weighted geometric does not have any . Though P mean P straightforward physical interpretation ( in fact , it is not a distribution ) , it leads to simpler expressions as discussed below .
= k i=1 P πi i
We call the corresponding divergence as JS−divergence with geometric mean , or simply JSGM−divergence . Given distributions P1 , P2 , . . . , PM with weights π1 , π2 , . . . , πM , the JSGM−divergence among them is given by
M M i=1 i=1 j=i
( MeMd JS )
Algorithm 3 classification using JSGM−divergence INPUT :
: MEMD for multi class
• Labeled datasets of M classes . • Data of the form x = ( x1 , x2 , . . . , xn ) , xi denoting ith
• A set of l constraints Γ(i ) =
φ(i ) 1 , . . . , φ(i ) l to be applied on each feature xi , i = 1 , . . . , d .
ALGORITHM :
1 , 2 , . . . , M are estimated using ( 16 ) .
1 ) ME densities P ( i ) cj , i = 1 , 2 , . . . , d and j = 2 ) The J−divergence for each feature and each pair of 3 ) The JSGM−divergence for each feature can be com classes is calculated using ( 22 ) . puted as
JS(i )
GM = JSGM ( P ( i ) c1
, P ( i ) c2
, . . . , P ( i ) cM
)
M j=1 k=j
= flflflP ( i ) ck
.
P ( cj)P ( ck)J
P ( i ) cj
4 ) The features are ranked in descending order according to their JSGM−divergence values , and the top K features are chosen ( to be considered for classification ) . 5 ) Bayes decision rule is used to assign a test pattern to cj∗ such that j∗ = arg max j=1,,M
Pcj ( x)P ( cj ) , where the class conditional densities are approximated as in ( 17 ) for j = 1 , 2 , . . . , M . discussed in the experimental results .
VI . EXPERIMENTAL RESULTS
In this section , we compare the proposed algorithms with some popular algorithms , commonly used in practice . Table I provides a summary of the proposed algorithms , as well as the existing algorithms . We also introduce some notations for each algorithm , which will be used in sequel to refer to the algorithm . We note that the binary classification algorithm ( Algorithm 1 ) is a special case of the multi class “ one vs . all ” approach ( Algorithm 2 ) , and so , in sequel , we always refer to both together as MeMd J algorithm .
A . Summary of computational complexity of algorithms
We first discuss about the computational complexity of the various algorithms . This is detailed in Table I . It shows the training and testing time complexities of the various algorithms in terms of number of features ( d ) , number of classes ( M ) and number of training samples ( N ) . For SVM , S denotes the number of support vectors , while for proposed algorithms K is the chosen number of features ( obtained
πiKL(PiP
) feature .
JSGM ( P1 , . . . , PM ) =
=
πiπjKL(PiPj ) .
( 23 )
M i=1 j=i
M ) , it is same as In the case of uniform weights ( πi = 1 the average divergence [ 27 ] upto a constant scaling factor . Observing the symmetric nature of ( 23 ) , the divergence can also be written in terms of J−divergence as
JSGM ( P1 , . . . , PM ) =
1 2
πiπjJ(PiPj ) .
( 24 )
The JSGM−divergence satisfies the following inequality , which has been shown in [ 27 ] for uniform weights . Proposition 2 . For any set of weights π1 , π2 , . . . , πM ,
JS(P1 , . . . , PM ) JSGM ( P1 , . . . , PM )
KL−divergence is convex in its second argument .
Proof : The claim follows from the observation that The usefulness of the JSGM−divergence stems from the fact that it is a multi distribution divergence that generalizes J−divergence , which helps us to overcome the difficulties of the one vs . all approach . So it does not requires estimation of extra ME distributions . At it can be expressed in terms of J−divergence ( 24 ) , which helps us to exploit the nice properties of J−divergence discussed in Theorem 1 and Remark 1 . So , the following algorithm using JSGM−divergence is analogous to the binary classification algorithm with J−divergence ( Algorithm 1 ) . The equivalence can be easily observed by considering the following equivalence . For each feature , we replace the same time , flflflP ( i ) c j
J
P ( i ) cj k=j in Algo 2 ←→ flflflP ( i ) ck
P ( ck)J
P ( i ) cj in Algo 3 ,
( 25 ) ie , J−divergence between class conditional densities for cj and all other classes taken together ( cj ) is replaced by the weighted average of the J−divergences between cj and other classes .
We can also extend the arguments in Section III A to show that for maximum entropy Bayes classification in the multiclass scenario , JSGM−divergence is a natural choice . The algorithm has linear time complexity O(d ) , and requires half the time as compared to Algorithm 2 ( one vs . all ) . However , the performance of both algorithms are quite similar as
Table I : Comparison of complexity of algorithms .
Algorithm
Notation
Classification
MeMd using one vs . all approach MeMd using JSGM−divergence MeMd using greedy approach [ 8 ]
Support Vector Machine [ 28 ]
Discriminative approach using ME [ 7 ]
MeMd J MeMd JS
MeMd SVM DME multiclass multiclass binary multiclass multiclass
Training time
Estimation O(M N d ) O(M N d )
Feature ranking O(M d + d log d ) O(M 2d + d log d )
O(N d2 )
#iterations*O(M d ) #iterations*O(M N d )
Testing time per sample O(M K ) O(M K ) O(M d ) O(M 2Sd ) O(M d )
Table II : Performance comparison on gene expression datasets .
Dataset
Colon cancer [ 29 ] Leukemia [ 29 ] , [ 30 ]
DLBCL [ 30 ]
Prostate cancer [ 30 ]
SRBCT [ 30 ]
Embryonal tumors of CNS [ 29 ]
Human lung carcinomas [ 30 ]
Global Cancer Map [ 29 ]
Data attributes
No . of samples
No . of classes
2 2 2 2 2 4 5 14
62 72 60 77 102 83 203 190
No . of features 2000 5147 7129 7070 12533 2308 12600 16063
MeMD J ( 2 moment )
10 fold cross validation accuracy MeMd JS SVM ( linear ) ( 2 moment ) 84.00 96.89 62.50 97.74 89.51 99.20 93.21 66.85
86.40 98.97 63.75 86.77 89.75
97.27 93.52 66.98
98.33 92.60 66.98 from the ranking ) . The training time complexity for SVM assumes that the rows have been cached , as in the case of LIBSVM . SVM was implemented using LIBSVM [ 28 ] , which uses a “ one vs . one ” approach . Furthermore , the number of iterations for SVM tend to be O(N 2 ) , whereas for DME , the algorithm gives best results when the number of iterations is small ( O(1 ) ) as mentioned in [ 7 ] .
The greedy MeMd technique [ 8 ] is severely affected by curse of dimensionality . In addition to its quadratic complexity , implementations indicated that the 2 moment ME joint distribution ( Gaussian ) appeared to be unstable due to the inverse of the large dimensional covariance matrix . Hence , results corresponding to this algorithm have not been presented in the comparisons . We also skip the MeMd algorithm using JS−divergence since determination of true value of JS−divergence involves considerable amount of computation as it requires numerical calculation of integrals . Hence , we work with its approximate version ( Algorithm 3 ) . It is worth noting here that though MeMd JS and MeMd J have same order of complexity for parameter estimation , for each class , MeMd J builds a model for all data not in the class . Hence , it estimates twice the number of parameters . Hence , using MeMd JS over MeMd J is computationally efficient in the following cases .
1 ) The number of points is large thereby making parameter estimation time for MeMd J twice as MeMd JS . Note that feature selection phase does not depend on the number of points , hence , having an M 2 term in MeMd JS is not a problem .
2 ) The number of iterations required for parameter estimation is not O(1 ) , ie , when we use iterative scaling algorithm . However , this issue is not there if only 1moment or 2 moment models are considered .
B . Experiments on biological datasets
We compare the performance of our algorithms ( MeMdJ and MeMd JS ) with that of SVM on a variety of gene datasets , collected from [ 29 ] , [ 30 ] . The details of the datasets are presented in Table II , where the number of features is same as the number of genes in these datasets . We again mention here that the greedy MeMd [ 8 ] and MeMd using JS−divergence have not been implemented due to their computational complexities addressed in the previous section .
Table II also lists the accuracies obtained from different algorithms using 10 fold cross validation , where the folds are chosen randomly . The class conditional distributions are 2 moment ME distributions . The optimal number of features is selected by cross validation within the training sample . We note here that for binary classification , the procedure in both MeMd J and MeMd JS are same , ie , they result in the same algorithm . Hence , in these cases , the results for both algorithms are combined together .
The best accuracy for each dataset is highlighted . From Table II , one can observe that the MeMd classifier is more successful in distinguishing among the classes for most of the cases . This is because , a very small number of genes are actually involved in these diseases . By using a feature pruning strategy in the classification algorithm , the MeMd classifier is able to prune away most of the genes that are not discriminatory .
C . Experiments on text datasets
We perform our experiments on the 20 Newsgroups dataset obtained from [ 31 ] . The dataset contains 20 classes , which have been grouped in different ways to construct different binary and multiclass problems . The data has been preprocessed prior to classification . We remove all stop words , and words with frequency less than some cut off value ( γ ) from the entire document corpus . This is done since such words do not have much discriminative power . The value of γ is chosen to be 2 in all cases . Each document Dj of the text corpus is represented as a vector of termweights Dj = W1j , W2j , . . . , WT j , where Wij represents T the normalized frequency of the word wi in the document Dj [ 32 ] , ie , Wij = k=1 N ( wk,Dj ) , where N ( wi , dj ) is the number of times the word wi occurs in the document Dj , and T is the total number of words in the corpus . Thus 0 ≤ Wkj ≤ 1 represents how much the word wk contributes to the semantics of the document Dj .
N ( wi,Dj )
Table III presents the classification accuracy of the proposed methods ( MeMd J and MeMd JS ) along with SVM and DME using 2 fold cross validation . The main reason behind using only two folds is that , in such a case , the training data size reduces considerably , and learning correctly from small data becomes an additional challenge . We demonstrate how the proposed generative approach overcomes this problem . Furthermore , MeMd J and MEMd JS algorithms use only the top K ranked features for classification . For choosing the the optimal value of K , we employ the following strategy . We divide the training data in two portions : a considerable fraction ( 80 % ) of the training data is used to rank the features , while the remaining 20 % is used as a “ test ” set on which classification is performed using varying number of features . Hence , we obtain a plot of the accuracy vs . the number of features . We choose K to be the minimum number of features , where the maximum accuracy is attained for that small portion of training data . We also note , as in previous section , that for binary classification MeMd J and MeMd JS are exactly same , and so we present their results together .
The best accuracies for each experiment is shown in bold . We also underline the cases , where results are very close to best case ( more than 0.2 % of maximum accuracy ) . Our observation can be summarized as follows : In all of the cases ( except one ) , MeMd outperforms linear SVM , which showed quite poor performance . We also implemented SVM with polynomial and RBF , which exhibited even poorer performance , and hence , those results are not presented . On the other hand , we observed that DME performs quite well , providing the best accuracies in a considerable number of cases . Further , in multiclass problems , MeMd J is always observed to perform marginally better than MeMd JS .
VII . CONCLUSION
As far as our knowledge , this is the first work that proposes and studies a generative maximum entropy approach to classification . In this paper , we proposed a method of classification using maximum entropy with maximum discrimination ( MeMd ) which is a generative approach with simultaneous feature selection . The proposed method is suitable for large dimensional text dataset as the classifier is built in linear time with respect to the number of features and it provides a way to eliminate redundant features . Also this is the first work that uses multi distribution divergence in multiclass classification . It will also be interesting to study the proposed methods as feature selection algorithms .
REFERENCES
[ 1 ] A . Y . Ng and M . Jordan , “ On discriminative vs . generative classifiers : A comparison of logistic regression and naive bayes , ” in Advances in Neural Information Processing Systems . MIT Press , 2002 , p . 14 .
[ 2 ] E . T . Jaynes , “ Information theory and statistical mechanics
I , ” Physical Review , vol . 106 , no . 4 , pp . 620–630 , 1957 .
[ 3 ] S . Kullback , Information Theory and Statistics . New York :
Wiley , 1959 .
[ 4 ] S . C . Zhu , Y . N . Wu , and D . Mumford , “ Minimax entropy principle and its application to texture modelling , ” Neural Computation , vol . 9 , pp . 1627–1660 , 1997 .
[ 5 ] M . Dudik , S . J . Phillips , and R . E . Schapire , “ Maximum entropy density estimation with generalized regularization and an application to species distribution modeling , ” Journal of Machine Learning Research , vol . 8 , pp . 1217–1260 , 2007 .
[ 6 ] D . Beeferman , A . Berger , and J . Lafferty , “ Statistical models for text segmentation , ” Machine Learning , vol . 34 , no . 1 , pp . 177–210 , 1999 .
[ 7 ] K . Nigam , J . Lafferty , and A . McCallum , “ Using maximum entropy for text classification , ” in IJCAI 99 workshop on machine learning for information filtering , 1999 , pp . 61–67 .
[ 8 ] A . Dukkipati , A . K . Yadav , and M . N . Murty , “ Maximum entropy model based classification with feature selection , ” in Proceedings of IEEE International Conference on Pattern Recognition ( ICPR ) .
IEEE Press , 2010 , pp . 565–568 .
[ 9 ] E . Allwein , R . E . Shapire , and Y . Singer , “ Reducing multiclass to binary : A unifying approach for margin classifiers , ” Journal of Machine Learning Research , vol . 1 , pp . 113–141 , 2000 .
[ 10 ] R . Rifkin and A . Klautau , “ In defense of one vs all classification , ” The Journal of Machine Learning Research , vol . 5 , pp . 101–141 , 2004 .
[ 11 ] D . Garc´ıa Garc´ıa and R . C . Williamson , “ Divergences and risks for multiclass experiments , ” in Proceedings of Annual Conference on Learning Theory , vol . 23 , 2012 , pp . 28.1– 2820
Table III : Performance comparison on 20 Newsgroup dataset ( ’/’ used to separate classes ; names abbreviated in some cases ) .
Experiment
No . of classes
No . of samples
MeMD J ( 1 moment )
MeMd JS ( 1 moment )
Data attributes
2 fold cross validation accuracy rec.autos / rec.motorcycles talk.religion / soc.religion composms windows / compwindowsx compsysibm / compsysmac rec.(autos,motorcycles ) / rec.sport sci.(space,crypt ) / sci.(med,elec ) comp.windows(os,x ) / compsys(ibm,mac ) all ‘talk’ categories all ‘sci’ categories all ‘rec’ categories all ‘comp’ categories
2 2 2 2 2 2 2 4 4 4 4
1588 1227 1973 1945 3581 3952 3918 3253 3952 3581 4891
No . of features 7777 8776 9939 6970 13824 17277 13306 17998 17275 13822 15929
SVM DME ( RBF ) 95.96 91.35 92.80 89.61 98.49 96.63 88.23 88.62 94.63 95.83 81.08
95.59 92.33 93.60 90.48 98.68 96.93 91.88 90.34 95.26 96.23 83.41
97.35 91.69 93.81 89.77 99.02 95.04 91.75
91.91 95.14 96.14 82.11
91.39 94.88 95.86 82.03
[ 12 ] J . N . Darroch and D . Ratcliff , “ Generalized iterative scaling for log linear models , ” The annals of mathematical statistics , vol . 43 , no . 5 , pp . 1470–1480 , 1972 .
[ 13 ] I . Csisz´ar , “ A geometric interpretation of Darroch and Ratcliff ’s generalized iterative scaling , ” The Annals of Statistics , pp . 1409–1413 , 1989 .
[ 14 ] H . Jeffreys , “ An invariant form for the prior probability in estimating problems , ” Proceedings of the Royal Society of London . Series A . Mathematical and Physical Sciences , vol . 186 , no . 1007 , pp . 453–461 , 1946 .
[ 15 ] R . Nishii and S . Eguchi , “ Image classification based on markov random field models with Jeffreys divergence , ” Journal of multivariate analysis , vol . 97 , pp . 1997–2008 , 2006 .
[ 16 ] T . Deselaers , D . Keysers , and H . Ney , “ Features for image retrieval : A quantitative comparison , ” Pattern Recognition , pp . 228–236 , 2004 .
[ 17 ] S . S . Dragomir , J . Sunde , and C . Buse , “ New inequalities for Jeffreys divergence measure , ” Tamsui Oxford Journal of Mathematical Sciences , vol . 16 , pp . 295–309 , 2000 .
[ 18 ] S . M . Ali and S . D . Silvey , “ A general class of coefficients of divergence of one distribution from another , ” J . Roy . Statist . Soc . Ser . B , vol . 28 , pp . 131–142 , 1966 .
[ 19 ] J . Lin , “ Divergence measures based on the Shannon entropy , ” IEEE Transactions on Information Theory , vol . 37 , pp . 145– 151 , 1991 .
[ 20 ] D . M . Endres and J . E . Schindelin , “ A new metric for probability distributions , ” IEEE Transactions on Information Theory , vol . 49 , pp . 1858–1860 , 2003 .
[ 21 ] I . Grosse , P . Bernaola Galv´an , P . Carpena , R . Rom´an Rold´an , J . Oliver , and H . E . Stanley , “ Analysis of symbolic sequences using the Jensen Shannon divergence , ” Physical Review E . , vol . 65 , 2002 .
[ 22 ] A . L . Berger , V . J . D . Pietra , and S . A . D . Pietra , “ A maximum entropy approach to natural language processing , ” Computational Linguistics , vol . 22 , no . 1 , pp . 39–71 , 1996 .
[ 23 ] A . McCallum , D . Freitag , and F . Pereira , “ Maximum entropy Markov models for information extraction and segmentation , ” in Proceedings of the Seventeenth International Conference on Machine Learning , vol . 951 , 2000 , pp . 591–598 .
[ 24 ] N . Friedman , D . Geiger , and M . Goldszmidt , “ Bayesian network classifiers , ” Machine Learning , vol . 29 , pp . 131–163 , 1997 .
[ 25 ] P . Domingos and M . Pazzani , “ On the optimality of the simple Bayesian classifier under zero one loss , ” Machine learning , vol . 29 , pp . 103–130 , 1997 .
[ 26 ] R . O . Duda , P . E . Hart , and D . G . Stork , Pattern
Classification . Wiley , 2001 .
[ 27 ] A . Sgarro , “ Informational divergence and the dissimilarity of probability distributions , ” GNIM CNR research activity , December 1979 .
[ 28 ] C . C . Chang and C . J . Lin , “ LIBSVM : A library for support vector machines , ” ACM Transactions on Intelligent Systems and Technology , vol . 2 , no . 3 , pp . 27:1–27:27 , 2011 , software available at http://wwwcsientuedutw/ cjlin/libsvm .
[ 29 ] BioInformatics Research Group of Seville , repository ( WEKA ) , http://wwwupoes/eps/bigs/datasetshtml ”
ARFF in
“ Dataset at :
Avaliable
[ 30 ] M . Mramor , G . Leban , J . Demˇsar , and B . Zupan , “ Supplementary material to : Visualization based cancer microarray data classification analysis , ” Bioinformatics , vol . 23 , no . 16 , pp . 2147–2154 , Available at : http://wwwbiolabsi/supp/bi– cancer/projections/ , 2007 .
[ 31 ] K . Bache and M . Lichman , “ UCI machine learning repository , ” University of California , Irvine , School of Information and Computer Sciences , vol . Available at : http://archiveicsuciedu/ml , 2013 .
[ 32 ] F . Sebastiani , “ Machine learning in automated text categorization , ” ACM Computing Surveys , vol . 34 , pp . 1–47 , 2002 .
