2013 IEEE 13th International Conference on Data Mining
Controlling Attribute Effect in Linear Regression
Toon Calders , and Xiangliang Zhang ∗ Computer and Decision Engineering Dept . , Universite Libre de Bruxelles ( ULB ) , Belgium
, Faisal Kamiran
, Asim Karim
‡
, Wasif Ali
†
∗
†
‡
†
Email : tooncalders@ulbacbe
Dept . of Computer Science , SBASSE , Lahore University of Management Sciences ( LUMS ) , Pakistan
‡
Email : akarim@lumsedupk ; xs2wasifali@gmail.com
CEMSE Division , King Abdullah University of Science and Technology ( KAUST ) , KSA
Email : faisalkamiran@gmailcom ; xiangliangzhang@kaustedusa
Abstract—In data mining we often have to learn from biased data , because , for instance , data comes from different batches or there was a gender or racial bias in the collection of social data . In some applications it may be necessary to explicitly control this bias in the models we learn from the data . This paper is the first to study learning linear regression models under constraints that control the biasing effect of a given attribute such as gender or batch number . We show how propensity modeling can be used for factoring out the part of the bias that can be justified by externally provided explanatory attributes . Then we analytically derive linear models that minimize squared error while controlling the bias by imposing constraints on the mean outcome or residuals of the models . Experiments with discrimination aware crime prediction and batch effect normalization tasks show that the proposed techniques are successful in controlling attribute effects in linear regression models .
Keywords—Linear Regression ; Fair Data Mining ; Batch Ef fects ; Propensity Score
I .
INTRODUCTION
In data mining we are often confronted with situations where we have to learn from data that is biased in one way or another . A first potential reason of data bias comes from data being collected from different sources ; nowadays , many companies and scientific communities are collecting huge data repositories , opening up unprecedented opportunities for data mining . For instance , by combining data from multiple medical studies one may be able to identify statistically relevant patterns that are not apparent in the individual datasets in isolation . More recently , another type of bias arising from dependence on a socially sensitive attribute was identified in the data mining community [ 1]–[3 ] . This type of bias , which in general can be categorized as a measurement cumselection bias , can lead to unfair and illegal decisions if not controlled in statistical models . For instance , in a demographic dataset it may be observed that for the same type of work , overall females receive a lower income than males . A classifier trained on such a dataset may pick up this dependency and predict lower wages either directly for females , or indirectly for persons with female characteristics . Depending on the application field , using such a biased classifier may be undesirable , unethical , or even illegal .
From now onward , we will abstract away from the reason of the data bias , and assume that ( 1 ) the data can be grouped in one way or another , by batch , gender , ethnicity , or any other grouping attribute , ( 2 ) there is a bias with respect to this grouping , and ( 3 ) we want to control and remove , at least to some extent , this bias in the models we learn from the data . Specifically , we study how we can learn linear regression models in such a situation by imposing additional constraints on the learning process . There already exist several approaches for removing the bias from the training data [ 4 ] . These approaches , however , may fail to identify some specific subgroups of the data where the bias occurs . Our approach is the first one to directly adjust regression models . As such , it is orthogonal to existing bias removal techniques and can be used in combination .
We start by proposing two measures , one based on the mean difference between predictions in one group versus the other , and another one based on the area under the ROC curve ( AUC ) . Then , we introduce a way to deal with the fact that often the bias can partially be justified by some explanatory attributes . For example , attributes “ number of working hours ” and “ education level ” may explain salary differences between males and females to some extent . We assume that these attributes are externally nominated by the domain experts . To remove this effect of the explanatory attributes and balance the groups , we use a technique from statistics , named propensity modeling . Based on a so called propensity score we divide the data into strata such that within the stratum none of the bias can be justified by the explanatory attributes .
Once the explanatory part of the bias has been removed by dividing into strata , we start the modeling process . To compensate for the remaining , unexplainable bias in the input data , we impose additional constraints in the learning process . We study two constraints : the first constraint imposes that the mean prediction needs to be the same for the different batches or groups . Loosely speaking , this constraint expresses that members of the different groups need to be treated similarly even if that is not the case in the input data . The second constraint expresses that the mean residuals needs to be the same ; that is , the errors for the different groups need to be balanced . For both constraints we show analytical solutions to derive an optimal linear regression model on the training data under different conditions : one model per stratum versus one general model valid for all strata , and whether or not the group identifier is part of the final model . Depending on the setting the result is hence either one unbiased model per stratum , or one global model that is unbiased conditioned on stratum .
All techniques have been implemented and were tested on two datasets . The experiments show that the techniques are able to satisfy the constraints on the training data and that these results carry over to the test data . They also demonstrate
1550 4786/13 $31.00 © 2013 IEEE DOI 101109/ICDM2013114
71 the applicability of the techniques for discrimination aware regression and source rating normalization .
In summary , our contributions in this paper are as follows :
1 )
2 )
3 )
4 ) time the problem of bias control in For the first two regression models is introduced . We present measures to quantify bias ; one based on the mean difference in predictions , and one on the AUC . For the first time the issue of attributes explaining part of the bias is treated in a principled way , based upon the propensity scoring technique from statistics . A new technique for controlling the influence of a categorical grouping attribute , such as batch number or ethnicity , in linear regression models is presented . This technique is based upon the introduction of constraints in the optimization problem at the basis of the ordinary least squares regression modelling . Optimal analytical solutions to the constrained optimization problem are derived . Experiments with two real datasets show the potential of the two step technique consisting of first removing the explanatory bias with the propensity based method , followed by the induction of a regression model under constraints .
II . RELATED WORK AND MOTIVATION
Bias in observations and statistical models has been studied for decades with applications in sociology [ 5 ] , econometrics [ 6 ] , and biomedicine [ 7]–[9 ] . Also attribute bias in regression models appears in many applications . The issue of gender income or wage gap , for instance , has been studied extensively in sociology and economics [ 10 ] , [ 11 ] . Legislation exists in many countries that disallow wage discrimination with respect to gender , with severe penalties stipulated for violating employers . Thus , a discrimination ignorant approach to wage prediction based on historical data ( which is often biased ) can lead to violation of laws . Similarly , racial discrimination in criminology and police arrests is a continuing concern [ 12]–[14 ] . In this setting , a discrimination ignorant regression model based on historical data can further exacerbate the racial discrimination . This has been demonstrated recently for crime suspect prediction using real world data [ 14 ] .
There already exist quite some works in discriminationaware classification that deal with social discrimination during learning a classifier . For instance , techniques exist for learning decision trees [ 15 ] , Bayesian models [ 16 ] , and logistic regression [ 2 ] from biased data . The regression problem we study in this paper , however , is more challenging in the following senses . Firstly , instead of assessing the correlation between two categorical attributes ( for instance race and label ) , we now have to assess the correlation between the categorical sensitive attribute and the continuous target . Secondly , the number of ways to change how a model predicts increases greatly ; in classification the only possible modification is the change of one class label into another . For regression tasks , the continuous character of the target allows for a continuous range of potential changes .
Besides the discrimination aware regression application , attribute bias arises in many other application domains as well . For example , two publishers or evaluators can generate
72 ratings for products/services where the ratings of one publisher are generally higher than that of the other [ 17 ] , [ 18 ] . It is therefore important that this bias is controlled in any regression model learned over data from both publishers . Another application of bias aware regression is that of observational and experimental studies of data from two or more different sources with different selection and measurement biases [ 19 ] . One notorious example of such data collections comes from the field of computational biology , where huge repositories of experimental micro array data from different studies are being collected such that they can be combined and reused in new studies . Regarding these collections , however , Chen et al . [ 20 ] state that “ data produced by the thousands of micro array studies published annually are confounded by “ batch effects , ” the systematic error introduced when samples are processed in multiple batches . Although batch effects can be reduced by careful experimental design , they cannot be eliminated unless the whole study is done in a single batch . ” This type of bias arises in other situations as well , like product ratings being influenced by their source/publisher [ 18 ] . It is crucial to either remove such bias before mining , or to explicitly take it into account during the learning process .
A situation in which the failure to remove bias in an application leads to undesirable outcomes comes from Sweeney [ 21 ] , who analyzed ads provided by Google , and came to the conclusion that for certain searches “ a black identifying name was 25 % more likely to get an ad suggestive of an arrest record ” and raised the question whether “ Google ’s advertising technology exposes racial bias in society and how ad and fairness . ” search technology can develop to assure racial Discrimination aware data mining studies the development and application of methods for discovering and preventing discrimination from models learned over discriminatory datasets .
III . PRELIMINARIES
T
In this section , we introduce the task of controlling the effect of an attribute in a regression model and we define different measures of model unbiasedness . We consider a dataset D = {xi , ti}N i=1 . The vector xi represents the input attributes and the scalar ti represents the target for the ith instance . We will focus our work on learning linear regression models of the form t(x ) = w0 + w x . For notational convenience , we take parameter w0 as the first element of weight vector w and x0 = 1 as the first element of all input vectors xi . We denote the length of the vectors w and xi by M . Furthermore we assume that D can be divided into different groups , based upon a factor , or grouping attribute xs . For ease of presentation we will restrict our derivations to a factor with two levels , dividing the dataset into two partitions D↑ . All results can be generalized to multiple groups . We will use N ↑ to respectively denote the number of instances in D↑ . In the case of social fairness , for example , xs could be gender dividing D into D↑ , the males , and D↓ , the females . Inspired by the applications in fairnessaware data mining we will often refer to the factor xs as the sensitive attribute . and N ↓ and D↓ and D↓
The specific goal in the paper is to learn the weights w of a linear regression model such that the sum of squared errors , SSE := , on D is minimized , while the xi − ti)2
.N i=1(w
T
TABLE I . EXAMPLE DATASET FOR A SALARY STUDY . WAGE IS THE TARGET VARIABLE . THE DATA IS BIASED TOWARD LOWER WAGES FOR
FEMALES .
The mean difference is a real number with a value of zero signifying no dependency or attribute effect .
#
1 2 3 4 5 6 7 8 9 10
Gender
Study ( Years )
Working
Hours
Sector Health ?
M M M M M F F F F F
5 5 3 2 2 4 3 3 2 2
40 40 40 30 40 40 40 30 30 30
0 0 1 0 1 1 0 1 0 1
Wage ( K ) 66 66 60 44 56 60 55 42 40 40 direct and indirect influence of the sensitive attribute xs in the predictions gets controlled .
While presenting our strategy for quantifying and controlling attribute bias in regression models , we will illustrate concepts and techniques by referring to a fictitious employee wage dataset , given in Table I , that is known to be biased or discriminatory with respect to attribute gender , with female employees generally having lower wages than comparable male employees . In this example , education level ( expressed as number of years of study ) , weekly working hours , and whether or not the person works in the health sector are the predictor attributes , wage is the target variable , and gender is the sensitive attribute describing the grouping .
Example 1 : When learning a linear regression model on the dataset of Table I , we find the following model ( female is encoded as G = 0 , male as G = 1 ) :
−4 × G + 3.3 × S + 1.3 × W + 0.16 × HC − 1.1
( 1 )
Hence , we can see a strong influence of gender in the predictions . If we remove Gender from the list of attributes , we obtain the following model :
3.1 × S + 1.5 × W − 0.7 × HC − 8.9
( 2 )
Here we can observe the influence of HC ( person working in the health care sector ) increases as this sector has a high relative number of female employees and is used as a proxy . The mean salary for males under this model is 58.4 , and for females is 514 This example already shows that simply removing the Gender attribute does not solve the problem .
A . Measuring Imbalance in Data and Models
Intuitively , an attribute effects a target variable if there is a statistical dependency between the attribute and the target . Since the sensitive attribute xs is binary valued while target variable t is continuous , is not possible to use typical same type measures of dependency like correlation coefficient and point wise mutual information to quantify the statistical dependency between xs and t . We will use the Mean Difference ( MD ) and the Area Under the ROC Curve ( AUC ) for quantifying the effect of xs on the target variable . it
Definition 1 : ( MD ) : The mean difference ( MD ) of the continuous target variable t in dataset D , partitioned into D↑ and D↓ by a sensitive attribute xs is given by :
.
.
MD(t , xs ; D ) =
( x,t)∈D↑ t
−
N ↑
( x,t)∈D↓ t
N ↓
( 3 )
73
Definition 2 : ( AUC ) : The area under the ROC curve ( AUC ) of the continuous target variable t in dataset D , partitioned into D↑ by a sensitive attribute xs , is given by : and D↓ .
.
AUC ( t , xs ; D ) =
( xu,tu)∈D↑
( xd,td)∈D↓ I(tu > td )
( 4 )
N ↑ × N ↓ where I(· ) is the indicator function that returns 1 when its argument is true and 0 otherwise .
The AUC varies from zero to one , and it is symmetric around 0.5 which represents random predictability or zero attribute effect . The AUC value is a statistically consistent measure of predictive strength [ 22 ] .
In the definitions above , the effect of the sensitive attribute on the target is measured ; however , similar definitions can be applied to regression models y(x ) = w x , by replacing the true target t with either the prediction y(x ) , or the residual y(x ) − t . In this way , we get the measures MD o and AUC o that measure the effect of xs on the outcome of the regression model , and MD r and AUC r that measure the effect of xs on the residuals of the regression model ’s predictions .
T the dataset example of Table
Example 2 : For
I , MD(wage , gender ) = 58.4 − 47.4 = 11 , and AUC ( wage , gender ) = 21/25 = 084 Thus , there is a strong ( predictive ) dependency between wage and gender . The regression model of Equation ( 2 ) makes the following ( rounded ) predictions for the dataset : 6 61
2 65 65
7 8 59 44
# 1
9 41
10 40
3 58
4 41
5 55 score
The residuals are :
# 1
6 score −1 −1 −2 −3 −1 1
2
3
4
5
7 8 4 2
9 1
10 0
Hence , the measures for the classifier are as follows : prediction residual
MD AUC
7.85 0.68
3.15
0
From this we can conclude that the classifier ’s predictions are still biased with respect to gender , although the bias is less strong . From the measures applied to the residuals we learn that the errors that the classifier makes ( if we consider the target t as the ground truth ) , are highly biased ; salaries of males are systematically underestimated , while those of females are overestimated . Such a strong bias in the residuals may indicate a so called omitted variable bias ; in this case omitting the gender attribute led to the overestimation of the effect of “ Sector Health ? ” attribute and overestimation ( in absolute terms , but in negative direction ) of the constant factor .
IV . ADJUSTING FOR EXPLANATORY ATTRIBUTES : A
PROPENSITY SCORE BASED APPROACH
In many cases we cannot directly apply our constraintbased method , because it is unreasonable to assume that the constraint holds on the raw data . In the example of the wages in section III , for instance , the reason for lower wages of females can partially be attributable to their shorter working hours . To filter out this explainable difference , we propose a propensity score based stratification approach for balancing the dataset .
A . Propensity Modeling
We illustrate the propensity score with an example . The treatment patients receive often depends on their general condition , which in turn has a direct influence on their survival ( the target variable ) . If we want to study the influence of treatment A versus B on survival , we will have to adjust for all parameters that influenced the treatment the patient received , including his or her general health at the start of the treatment . That is , we determine the propensity of a patient to receive a certain treatment based on his or her parameters and capture it as a score . This score is usually estimated from the dataset by a logistic regression model . Roughly put , this score expresses how likely the patient was , based on his or her characteristics , to receive treatment A ( regardless of the treatment he or she received in the end ) . An important property of the propensity score is [ 23 ] : given the propensity score , the target is independent of the explanatory attributes . Thus , two instances that have the same propensity score should have the same value for the target , as explained by these attributes ; if the target values differ , then this represents an unexplained effect ( unexplained by the specified explanatory attributes ) of the attribute on the target . After obtaining the propensity score , it is either included as part of the prediction model , or the analysis is designed as to ensure that only patients with similar propensity are compared . We will base ourselves on a common technique that partitions the input data into strata according to the propensity score .
B . Accounting for Explainable Bias
In our setting , we assume there are externally provided explanatory attributes xe , such as the number of working hours and years of study in the wages example , or a set of variables of which we know that they have been varied between different batches of data . The differences in treatment of the groups D↑ that is explainable by these variables xe can be considered explainable bias , and should not be controlled . To achieve this , we proceed as follows : and D↓
1 )
2 )
The propensity score is defined as the conditional probability that a randomly selected instance of D belongs to D↑ , given the explanatory attributes , ie , ps = P ( x ∈ D↑|xe ) . Using the propensity scores the dataset is split into five strata via the propensity score quintiles . That is , each split contains 20 % of the data lying between quintile i and quintile i + 1 of the propensity score ( i = 0 . . . 5 ) . Let Si ( i = 1 , 2 , . . . , 5 ) denote the five strata of dataset D . Recall the fact whether or not x ∈ D↑ is independent of the variables xe , conditioned on the ps , and hence within the strata , the dependency between the division into D↑ and D↓ and the explanatory attributes xe will be minimal , as all instances in a stratum have similar propensity .
The potential difference in treatment between D↑ and D↓ within a stratum can no longer be attributed to the explanatory attributes . We will use AUC i = AUC ( t , xs ; Si ) and MD i =
MD(t , xs ; Si ) to denote the AUC and MD for stratum i . AUC i or MD i represent the unexplained effect of attribute xs on t in stratum i . This is the effect that needs to be controlled or removed in the regression models we learn .
Example 3 : Suppose that for the data in Table I , we assume that differences in salary between the genders can partially be attributed to Study ( numbers of years spent to study ) and Working Hours ( number of working hours per week ) . Hence , we consider the attributes Study and Working Hours explanatory attributes . As a first step we will learn the propensity score for Gender given these two attributes . For the data the following logistic regression function for gender is learned : logit(p(Gender = f emale ) ) = −0.047×S −0.17×W +6.4
Based on this function , the following scores are computed for the different instances . For the males :
# 1
2
3
4
5 score
0.32 0.32
0.34
0.75 0.35
And for the females : # 6
7
8 score
0.33 0.34
0.74
9
10 0.75 0.75
Given the size of the dataset we will divide the dataset into two strata instead of 5 , and compute the MD and AUC for both strata :
Males Females MD AUC
Stratum 1
1(66 ) , 2(66 ) , 3(60 )
Stratum 2
4(44 ) , 5(56 )
6(60 ) , 7(55 )
8(42 ) , 9(40 ) , 10(40 )
6.5
5/6 = 83 %
9.33 100 %
So , in comparison to the measures on the complete dataset , we can observe that the mean difference decreases while the AUC increases . This reverse relation between the two measures indicates that after filtering out the effect of the explanatory variables , even though the differences in wages become smaller on average , they also become more persistent .
V . CONTROLLING BIAS WITH CONSTRAINTS
In this section we will concentrate on learning models in which the influence of a sensitive attribute xs on the target t is controlled by constraints . In the last section we introduced an approach based on propensity scoring to partition the dataset into strata to reduce the biasing effect of a sensitive attribute xs on the target t that can be justified by a set of explainable attributes xe . This stratification will be an important first step in our methodology to control the influence of xs , because obviously we do not want to remove the explainable part of the bias . Therefore , in the learning process , we will deal with the remaining bias in the strata by either :
1 )
2 ) with a constraint , learning different models for every stratum separately , and control the difference in treatment between D↑ and D↓ or , learn one global model for the whole dataset controlling the difference in treatment between D↑ and D↓ for all strata at the same time with multiple constraints .
74
We will consider the following two constraints on a model y(x ) for a stratum S :
Equal Means : The mean predictions for S ↑ = D↑ ∩ S and
S ↓ = D↓ ∩ S need to be equal . That is :
MD o(y(x ) , xs ; S ) = 0
This constraint expresses that future predictions across the two groups should be comparable , regardless if this was the case in the original data or not . Notice that this constraint does not depend on the target . As such , if the ( unlabeled ) test instances are already known at training time , we could enforce this constraint directly on the test instances .
Balanced Residuals : Sometimes we do have a true difference between two groups that can be reflected in the predictions , as long as there is no bias in the errors made by the predictor . This can be controlled by requiring that the mean residual for both groups is equal :
MD r(y(x ) , xs ; S ) = 0
It could hence be acceptable that predictions in one group are consistently higher than in the other group , as long as this was the case in the original data , and the effect is not exaggerated .
We start by showing two analytical solutions in which the constraints are strictly satisfied by the model . After showing the solution for building a model for one partition , we extend it to multiple partitions . We end the section with a relaxed version where the degree to which the constraints are satisfied is added as a penalty term .
A . Strict Equal Means Constraint
The goal is to learn the coefficients w of a linear function w · x such that the SSE on S is minimized conditioned to the strict constraint the mean difference between the predictions on S ↑ is 0 . That is : fi .
( w · xi − ti)2 that and S ↓ minimize
( xi,ti)∈S
.
( xi,ti)∈S ↑ w · xi
( xi,ti)∈S ↓ w · xi subject to
↑ S
N
=
.
↓ S
N
( xi ,ti)∈S↑ xi
.
.
( xi ,ti)∈S↓ xi
−
We will use d to denote
N ↓ S ie , d is the difference between the mean vector of S ↑ the mean vector of S ↓ then becomes : w · d = 0 . and . The condition for the mean difference
N ↑ S
;
We solve this minimization problem using Lagrange multipliers . We use the following Lagrangian for the constrained minimization problem : fi
Equating to 0 gives that for all j , ⎞ ⎠ · w =
⎛ ⎝ fi xixij fi tixij − λdj
( xi,ti)∈S
( xi,ti)∈S
T
Xw = X T
Hence , X and thus , w = ( X Together with the equality w · d = 0 , we can solve for λ : t − λ(X d .
X
T t − λd , T
T X)−1 ( ( X ( (XTX)−1d ) · d
X)−1
X)−1 )
· d
X
T
T t
λ = 2
Reinserting this in the formula for w , we get : w = ( X
T
X)−1
X
T t −
T
X)−1
T(X t d dT(XTX)−1 · d
X
T
( X
T
X)−1 d
Example 4 : Consider again the wages dataset of Table I . If we apply the Equal Means constraint , the optimal linear regression model becomes :
7 × G + 3.3 × S + 1.3 × W + 0.16 × HC − 1.1
( 5 )
In comparison to the model in Eq 1 , the gender attribute is used to give a “ bonus ” to the females in the dataset . The predictions ( rounded ) for this new model are :
# 1
2 61 61 score
3 54
4 38
5 51
6 65
7 8 61 49
9 45
10 45
The residuals are :
# 1
6 score −5 −5 −6 −6 −5 5
2
3
4
5
7 8 6 7
9 5
10 5
The measures for the classifier are as follows : prediction residual
MD AUC
0
0.48
11 0
Not surprisingly , the mean difference for the predictions is 0 , and for the residuals it is −11 . AUC follows the trend .
B . Strict Balanced Residual Constraint
We follow a similar approach as in last subsection . The constraint for balanced residuals , however , is slightly different :
.
. w · d =
( xi,ti)∈S ↑ ti
−
↑ S
N
( xi,ti)∈S ↓ ti
↓ S
N
.
We will denote the right hand side of this equality by b . The constraint thus becomes w · d − b = 0 . Using the same technique as for Equal Means , we obtain :
L :=
( w · xi − ti)2 + 2λw · d w = ( X
T
X)−1
X
T t − d
T
X)−1
T(X dT(XTX)−1 · d
X
T t − b
( X
T
X)−1 d
( xi,ti)∈S
We take partial derivatives wrt the coefficients wj : fi
∂L ∂wj
=
( xi,ti)∈S
⎛ ⎝ fi
2(w · xi − ti)xij + 2λdj fi
⎞ ⎠ · w − 2 xixij
= 2
( xi,ti)∈S
( xi,ti)∈S tixij + 2λdj
75
Example 5 : Consider again the wages dataset of Table I . This time , in contrast to the previous example , we do not need a predictor that assigns equal wages to males and females in our dataset , but we do need the errors between males and females to be balanced . This time a model such as in Eq ( 5 ) is completely unacceptable , as it makes all negative errors on the males and all positive errors on the females . Therefore , now we apply the Balanced Residuals constraint . Under this constraint , the optimal linear regression model becomes :
−4 × G + 3.3 × S + 1.3 × W + 0.16 × HC − 1.1
( 6 )
This model is exactly the same as the normal linear regression model in Eq ( 1 ) . This is not a coincidence as we will see later , but true in general when the sensitive attribute partitions D in two parts , and is used as one of the predictor variables . If we omit the sensitive attribute gender , we get the following model :
2.7 × S + 2.1 × W − 4 × HC − 31
( 7 ) This model has a mean difference of 11 , and a difference in residual of 0 , as expected . AUC follows this trend with 72 % for the predictions and 52 % for the residuals .
C . Extending to Multiple Partitions and Groups
Recall that in the previous subsections we have dealt with the case of only one constraint . Usually we will have multiple constraints :
1 ) xs may have more than 2 values , and thus there are more than 2 groups ( D↑ ) in the data ; that is : D is partitioned into D1 , . . . , Dk by xs . In that case the constraints for Strict Equal Means become : and D↓
Equating the gradient with respect to w to 0 , and solving for λ gives : ff
λ = ( Δ
T(X
T
X)−1
Δ)−1
Δ
T(X
T
X)−1
T t − δ
X
.
T w = δ and solving for w gives the
Substituting this into Δ following final formula for w : fi w = ( X
T
X)−1
T
X t− fl t − δ )
Δ(Δ
T(X
T
X)−1
Δ)−1(Δ
T(X
T
X)−1
T
X
D . Relaxing the Constraints
In the previous two subsections we have required in our analytical deductions that the constraints should be satisfied exactly . This , however , can lead to degenerate solutions when the number of constraints increases too much . Therefore , in this section we study a somewhat more standard approach in which we incorporate the constraints directly into the objective function as penalty terms . For the “ equal means ” constraint the objective to be minimized , using the notations introduced before in this section , becomes : fi obj
:=
( w · xi − ti)2 + α(w · d)2
,
( 8 ) w · d
12 = 0 , w · d 1i
13 = 0 , . . . , and w · d
1k = 0 ,
( xi,ti)∈S where α is a weighing factor controlling the influence of the penalty term . This formula can be optimized by taking partial derivatives for wj : fi ∂obj ∂wj
= 2
( w · xi − ti)xij + 2α(w · d)dj fi
( xi,ti)∈D
⎛ ⎝ fi
= 2w · xixij + αddj
⎞ ⎠ − 2 tixij
( xi,ti)∈S
( xi,ti)∈S
Equating all partial derivatives to 0 gives : w = ( X
T
X + αdd
T)−1(X
T t )
Due to space restrictions we do not show the formulas for the other cases , as they can be derived using the same techniques .
E . Relations Between the Approaches
The following theorem relates the different settings of learning linear models with and without constraints to each other . These relations , however , only hold when the sensitive attribute xs splits the data in no more than two parts , and is a predictive variable of the prediction model . The theorem can be seen as a sanity check showing that in cases with full control over the sensitive attribute in our models , the optimal solutions under constraints can be obtained with intuitive adaptations to unconstrained optimal models .
Theorem 1 : Given a dataset D and a sensitive attribute xs . We that divides the dataset into two partitions D↑ assume D has full column rank . and D↓
Let w , wEM and wBR , be the optimal linear models wrt SSE under , respectively , no constraint , the equal means constraint , and the balanced residuals constraint , and where d denotes the difference between the mean of group 1 and group i . Similarly , for the Strict Balanced Residual constraint , we get : w · d
12 = b
12
, w · d
13 = b
13
, . . . , and w · d
1k = b
1k .
2 ) Sometimes it may be useful to build one global model for the complete dataset , instead of different models for each stratum . In such a case we may opt to enforce the Strict Equal Means constraint on all strata at the same time , instead of building separate models for the different strata . Again this will result in a set of similar constraints . For instance , suppose we have two groups D↓ , and ' strata dividing D into . . S1 , . . . , S Let di , i = 1 . . . ' denote and D↑ i d
:=
( x,t)∈S ↑ i x
−
( x,t)∈S ↓ i x
.
|S ↓ i | The Strict Equal Means constraint becomes :
|S ↑ i | w · d
1 = 0 , w · d
2 = 0 , . . . , and w · d
. = 0 .
We will show how to optimize for multiple constraints in general . For all cases described above , the optimization problem can be casted as follows : fi minimize
( x
T i w − ti)2
( xi,ti)∈D subject to Δ
T w = δ .
Δ is the matrix containing the dij
’s or dj as columns , and
δ is a vector containing the constant factors bij
, or 0 .
Again we can use the same technique via Lagrange multi pliers . The Lagrangian for the multiple constraints is : fi
L :=
( xi,ti)∈D
( x
T i w − ti)2 + 2λ
T(Δ T w − δ )
76 approx,α EM approx,α BR and w let w be the optimal linear models wrt the approximate equal means and balanced residuals optimization problems of Subsection V D . Then the following relations hold :
( 1 ) w and wEM only differ in the constant term ( w0 ) and the coefficient for xs ;
( 2 ) w = wBR ; approx,0 EM
( 3 ) w
= w approx,0 BR
= w ;
( 4 )
( 5 ) limα→∞ w approx,α EM limα→∞ w approx,α BR
= wEM ; and = wBR .
∗
. Let w is m↓
Proof : We first prove ( 2 ) . This statement comes down to showing that the optimal linear model without constraints ( xs = 1 ) and D↓ already has a mean residual of 0 in both D↑ ( xs = 0 ) : Suppose the average residual on D↑ and on D↓ be the following weight vector : take the weight vector w and subtract m↓ from the intercept , and add m↓−m↑ to the coefficient for xs . This will shift the predictions for D↑ by −m↑ , making the mean residual in both groups 0 . In this way the SSE will decrease by ( N ↑m↑)2 + ( N ↓m↓)2 . Since w was optimal this sum must be 0 which is only possible if m↓ = m↑ = 0 . and those for D↓ by −m↓ is m↑
) by w for D↑ for D↓ and D↓
, and ↓
( resp . D↓
. Since the residual of w
, the mean outcome for D↑
For ( 1 ) , we will apply a similar construction with shifting the regression lines . Suppose that the mean residual for the weight vector wEM is ↑ . Construct ∗ w EM by shifting wEM to reduce the residuals in both groups ∗ to 0 as above . The mean difference in outcome for w EM will increase , due to this shift , to ↑ − ↓ , and the SSE will decrease by ( N ↑ ↑)2 +(N ↓ ↓)2 ∗ EM is 0 in both ∗ D↑ EM is equal to the mean target value for D↑ ) . Hence , we also have that ↑ − ↓ equals the difference between the mean target value in D↑ and the mean target value in D↓ . Suppose that the mean difference between the groups of the outcomes by the optimal unconstrained model with weights w is δ . Since w has a mean residual of 0 , δ must be equal to the difference between the mean target value for D↑ and the mean target value for D↓ . Construct the weight are shifted by shift 1 := vector w −δ ( N ↑)2+(N ↓)2 . The model based on vector w will have equal means in both groups , since the sum of these two shifts is −δ , compensating for the mean difference δ of w . The SSE of w
. Hence , δ = ↑ − ↓ such that the instances in D↑
( N ↑)2+(N ↓)2 , and those in D↓ by shift 2 = −δ
( resp . D↓
( N ↑)2
( N ↓)2
(
( is
(
SSE ( w ) + ( shift 1N
↑)2 + ( shift 2N
↓)2
= SSE ( w ) + ( shift 1N
↑)2 + ( (−δ − shift 1)N
↓)2
( 9 )
Since w is the optimal weight vector for the unconstrained optimization problem , and wEM the optimal vector for the equal means constraint , we get the following equations :
SSE ( w ) ≤ SSE ( w
∗ EM )
= SSE ( wEM ) − ( N
↑
↑)2 − ( N
↓
↓)2
SSE ( wEM ) ≤ SSE ( w
( )
= SSE ( w ) + ( shift 1N ↓)2
+((−δ − shift 1)N
↑)2
TABLE II .
KEY CHARACTERISTICS OF CRIME AND WINE DATASETS
N M t xs N ↑ MD O AUC O
, N ↓
Crime 1994
99
Wine 6497
11
Crime Rate
Race ∈ {black , non black}
Rating
Type ∈ {white , red}
970 , 1024
0.22 0.80
4898 , 1599
0.94 0.76
Combining the two equations leads to :
( shift 1N
↑)2 + ( (−δ − shift 1)N
≥ ( N
↑
↓)2 ↓ ↑)2 + ( N
= ( N
↓)2 ↑)2 + ( N
↑
↓(−δ −
↑))2
.
Since shift 1 is the unique minimum of the function ( xN ↑)2 + , we can conclude from this that shift 1 = ↑ ( (−δ − x)N ↓)2 and shift 2 = ↓ ∗ , and hence SSE ( w ) = SSE ( w EM ) . Since D has full column rank , the optimal weights minimizing the SSE ∗ is unique , and hence w must be equal to w EM . This concludes ∗ the proof , since w EM was obtained from wEM by changing the weights for the constant factor and the coefficient for xs only .
The proofs of ( 3 ) , ( 4 ) , and ( 5 ) are straightforward and have been omitted due to space restrictions .
VI . EXPERIMENTS AND RESULTS
We evaluate the proposed techniques for controlling attribute effects in linear regression models using two real datasets . These datasets reflect different application settings and problem domains . The first dataset , Communities and Crime ( Crime ) , is suitable for discrimination aware regression where a learned regression model is required to not discriminate among communities of different races . The second dataset , Wine Quality ( Wine ) , is appropriate for batch effect control where ratings for red and white wine are required to be normalized . We discuss our techniques for each dataset in the following sections .
A . Communities and Crime
The Communities and Crime ( Crime ) dataset1 contains socio economic information of communities and their crime rates . The goal with this dataset is to learn a model for the crime rate given communities’ socio economic information . Moreover , to prevent discrimination , it is required by law that predictions are not discriminatory between favored and deprived communities based on the majority race of the communities .
For our experiments , we preprocess the dataset by removing attributes with many missing values . We create two groups of the dataset : one group comprises of communities with a majority black population while the other group contains communities with a majority non black population . All the attributes are standardized to zero mean and unit variance . In the end , the Crime dataset contains 1994 instances ( 970 and 1024 instances , respectively , for black and non black
1http://archiveicsuciedu/ml/datasets/Communities+and+Crime
77
TABLE III .
CONSOLIDATED OVERALL DATASET RESULTS FOR CRIME AND WINE DATASETS
MD R
−
0.02 0.21 0.00 0.05 0.01 0.04 0.00
Data OLS S SEM S SBR S SEM MP OLS M SEM M SBR M
( a )
Crime AUC R MD O
AUC O
RMSE MD R
Wine AUC R MD O
AUC O
RMSE
−
0.49 0.16 0.52 0.41 0.49 0.43 0.51
0.22 0.20 0.01 0.22 0.17 0.21 0.18 0.22
0.80 0.81 0.50 0.82 0.76 0.80 0.75 0.81
−
0.14 0.20 0.14 0.14 0.15 0.16 0.15
( b )
−
0.05 0.94 0.00 0.46 0.04 0.14 0.00
− −
0.48 0.19 0.50 0.35 0.48 0.44 0.50
0.94 0.90 0.00 0.94 0.48 0.90 0.80 0.94
0.76 0.89 0.51 0.90 0.82 0.89 0.85 0.90
−
0.83 0.93 0.83 0.94 0.83 0.83 0.83
( c )
( d )
( e )
Fig 1 . Experimental results for the Crimes dataset : ( a ) Mean outcome difference in different strata , ( b ) Mean residual difference in different strata , ( c ) Mean residual difference in different strata , ( d ) RMSE in different strata , ( e ) Mean difference versus RMSE curve for regularized least squares communities ) described by 99 attributes . Table II shows key characteristics of the Crime dataset .
1 ) Discrimination Analysis : The Crime dataset shows a strong dependency between the target ( Crime Rate ) and the sensitive attribute ( Race ) . Communities with a majority black population have a mean crime rate of 0.35 in comparison to a mean crime rate of 0.13 for communities with a majority non black population ( MD o = 0.22 and AUC o = 080 ) However , part of this dependency can be explained by some of the attributes like female divorce percentage and number of illegal immigrants . Such explanatory or confounding attributes can be identified by dependency analysis whereby attributes that are highly correlated to both the target and the sensitive attribute represent potential confounders . Domain background knowledge may be required to filter out attributes that do not contain any objective information for the target .
For our experiments , we identify and utilize four explanatory attributes . Propensity score analysis with quintile stratification reveals that the Crime Rate Race dependency within each stratum is significantly lower than that in the entire dataset . Figures 1 ( a ) and 1 ( b ) show mean outcome difference ( M Do ) and AUC value for outcome sensitive attribute ( AU Co ) in each stratum of the dataset ( the dashed lines ) . The average MD o and AUC o values over all strata is 0.07 and 0.66 , respectively . Thus , part of the dependency or discrimination ( ie 0.80 − 0.66 = 0.14 in AUC value ) is explainable by the differences in distribution of the explanatory attributes in the two groups . A regression model that eliminates all dependency between Crime Rate and Race can be challenged as unfair and will lead to reverse discrimination in the predictions .
2 ) Discrimination Control : Here , we present and discuss techniques when the results of our attribute effect control applied to the Crime dataset . All results are obtained from 10fold cross validation of the dataset . Furthermore , as required by discrimination law in many regions , the sensitive attribute is not used in the prediction models .
Figure 1 ( a ) , ( b ) , ( c ) , and ( d ) shows respectively stratumwise mean outcome difference ( MD o ) , AUC value for outcome and sensitive attribute ( AUC o ) , mean residual difference
78
   
 
 
 
 
 
   
   
 
 
 
 
 
 
! " #
$
! % %
&
 '
   
 
 
 
 
 

( )
 
 
 
 
 
 
 
 
  *

( ) ( MD r ) , and root mean square error ( RMSE ) for techniques based on stratification . All techniques , but Strict Equal Means in Multiple Partitions ( SEM MP ) , involve a separate regression model for each stratum . Single and multiple strata versions of techniques are differentiated by a ’ S’ or a ’ M’ at the end of the technique ’s acronym ( SEM MP has a single model version only ) .
These results show the effectiveness of the equal outcome mean constraint in ensuring that prediction sensitive attribute dependency in each stratum is reduced to zero ( Figures 1 ( a ) and 1 ( b) ) . The SEM M and SEM MP techniques provide good control as compared to the ordinary least squares technique ( OLS M ) . It is worth emphasizing that SEM M and SEMMP remove the unexplained discrimination only rather than removing all dependency between Crime Rate predictions and Race .
It is interesting to note that the RMSE s of the different techniques ( Figure 1 ( d ) ) are not significantly different in each stratum . Normally , it is expected that RMSE is higher when enforcing constraints in addition to requiring squared error minimization . However , this behavior is not very obvious on this dataset . The SEM MP technique has a lower RMSE because it is a single model for the entire dataset rather than different models for each stratum .
The SBR M technique provides greater control over the residual in the two groups ( Figure 1 ( d ) ) but does not reduce discrimination significantly and often performs similarly to the OLS M technique . Figure 1 ( c ) also shows how the residuals are impacted when SEM M and SEM MP try to balance the predictions in the two groups . The discrimination effect is stronger in the higher strata where probability of black community instances is higher , indicating that greater correction is needed in these strata .
Table III gives the performance of the various techniques when calculated over the entire dataset . These techniques include both single models ( like OLS S , SEM S , and SEMMP ) and per stratum models ( like SEM M ) . The OLS S technique represents the standard discrimination ignorant linear model which is observed to magnify discrimination slightly . The SEM S removes all dependency in the entire dataset , irrespective of whether it is explainable or not , but pays in terms of higher RMSE . Notice that the AUC o values of SEM M and SEM MP are 0.76 and 0.75 , respectively , even though these techniques reduce the unexplainable dependency in each stratum to near zero . This is because of the explanatory attributes’ influence on the target in the overall dataset ( ie the overall dataset is not balanced wrt these attributes ) .
To obtain a better control over the discrimination in linear regression models , a regularized approach can be adopted . Figure 1 ( e ) shows the MD o and RMSE plot for different values of the regularization parameter α . The ordinary least squares solution is given by the right most point of the curve where α = 0 . The value of α can be selected based on the desired level of discrimination over a validation dataset .
( a )
( b )
( c )
( d )
Fig 2 . Experimental results for the Wine Quality dataset : ( a ) Mean outcome difference in different strata , ( b ) Mean residual difference in different strata , ( c ) Mean residual difference in different strata , ( d ) RMSE in different strata
B . Wine Quality
The Wine Quality ( Wine ) dataset2 contains descriptions of red and white wines and their ratings . The wines are described by physical characteristics ( such as alcohol content ) , while the ratings range from 1 to 10 . The goal with this dataset is to predict the rating of a wine given its characteristics irrespective of whether it belongs to the red or white wine type . In fact , since the models for the two types of wine are not known and normalized predictions are required without knowledge of type , a rating normalized linear regression model ( without the type attribute ) is desired .
The original dataset has a small mean rating difference between the two types of wines . For our experiments , we increase the rating of 70 % ( randomly selected ) white wines by one ( with constraint that ratings cannot be greater than 10 ) . The mean rating difference of this modified dataset is 0.94 with the corresponding AUC value of 076 The key characteristics of this dataset are given in Table II .
1 ) Normalized Rating Predictions : We apply our attribute effect control techniques to learn linear regression models that make rating predictions without bias for red or white wine . All results are based on 10 fold cross validation of the dataset , and wine type is not used in prediction models .
First , we identify the attributes that can potentially explain the dependency between the ratings and types of wine . We select two attributes ( volatile acidity and chlorides ) as the confounding attributes that need to be adjusted for in the prediction models . Both these attributes have strong correlation with rating and wine type . Propensity score analysis with stratification reveals that the dependency between rating and wine type varies somewhat across the different strata ( unlike in the Crime dataset ) . Thus , wine type effect control over local regions is more appropriate for this dataset .
2http://archiveicsuciedu/ml/datasets/Wine+Quality
79
+ + , . . , / / , 0 0 , 1 2 , . 2 2 , . 2 , 0 2 , 3 2 , 4 + + , . 5 6 7 8 6 8 9 : ; 8 6 8 < = 5 1 > 5 ? > 1 > 5 ? > 1 > @ 5 A B 1 > C C D E F F D E G G D E H H D E E I D H I D H E I D E I D E E I D J I D J E I D K I D K E I D L M N O P N P Q R S T P N P U V M W X M Y X W X M Y X W X Z M [ \ W X C C D E F F D E G G D E H H D E E W C W I D L W I D J W I D H W I D F I I D F I D H M N O P N P ] ^ _ ` a ^ b c d e _ f g c h h ^ i ^ ` j ^ U V M W X M Y X W X M Y X W X Z M [ \ W X C C D E F F D E G G D E H H D E E I D J E I D K I D K E I D L I D L E I D k I D k E C M N O P N P a ] l m U V M W X M Y X W X M Y X W X Z M [ \ W X Second , we apply our various attribute effect control techniques for normalized rating predictions . Figures 2 ( a ) , ( b ) , ( c ) , and ( d ) show the strata wise performance of the techniques on this dataset . These results follow the general trends observed in the Crime dataset . Both SEM M and SEM MP techniques provide good control over the predictions between the two groups as compared to OLS M and SBR M . Unlike in the Crime dataset , SEM M and SEM MP techniques produce higher RMSE over the biased dataset . An important observation from these results is that SBR M is more appropriate than OLSM when attribute effect exaggeration needs to be controlled without requiring effect reduction .
The results for the entire dataset are given in Table III . It is observed that the OLS S technique significantly exaggerates the ratings difference between the two types of wine . Thus , this technique is not recommended for normalized rating predictions . The SEM M technique performs the best producing low RMSE while maintaining rating normalization within each stratum . Again , notice that the overall effect in the overall dataset is not reduced to zero even though effects within strata are close to zero for the SEM M and SEM MP techniques .
VII . CONCLUSION
In this paper , we provided a systematic treatment of attribute effect in regression problems . We discussed the nature and motivated the appearance of attribute effects in regression problems , with specific focus on applications in discrimination aware regression . A key contribution of this work is the introduction of propensity score analysis from statistics for filtering out explainable effects . This strategy can handle multiple explanatory attributes in contrast to previously proposed strategies in the discrimination aware data mining community . We also defined two measures for quantifying attribute effects in regression problems ( mean difference and AUC ) . We then developed constrained linear regression models for controlling the effect of an attribute on the predictions . Analytical solutions were derived that satisfy equal outcome means and balanced residuals in addition to minimizing the squared error . These techniques allow easy and principled applicability of linear models to control attribute effects . We conducted experiments on two real world datasets : one dataset represents a discrimination aware regression problem and the other represents a rating normalization problem . The results show that our techniques are able to achieve perfect control over the training data and this control generalizes well to test data .
To the best of our knowledge , this is the first paper that presents solutions to the discrimination aware regression problem . Previous works in discrimination aware data mining were mostly restricted to classification problems .
REFERENCES
[ 1 ] D . Pedreshi , S . Ruggieri , and F . Turini , “ Discrimination aware data mining , ” in Proceeding of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . ACM , 2008 , pp . 560–568 .
[ 2 ] T . Kamishima , S . Akaho , H . Asoh , and J . Sakuma , “ Fairness aware classifier with prejudice remover regularizer , ” Machine Learning and Knowledge Discovery in Databases , LNCS , vol . 7524 , pp . 35–50 , 2012 .
[ 3 ] F . Kamiran , A . Karim , and X . Zhang , “ Decision theory for discrimination aware classification , ” in IEEE 12th International Conference on Data Mining ( ICDM ) .
IEEE , 2012 , pp . 924–929 .
[ 4 ] F . Kamiran and T . Calders , “ Data preprocessing techniques for classification without discrimination , ” Knowledge and Information Systems , vol . 33 , no . 1 , pp . 1–33 , 2012 .
[ 5 ] R . A . Berk , “ An introduction to sample selection bias in sociological data , ” American Sociological Review , pp . 386–398 , 1983 .
[ 6 ] L F Lee , “ Some approaches to the correction of selectivity bias , ” The
Review of Economic Studies , vol . 49 , no . 3 , pp . 355–372 , 1982 .
[ 7 ] W . Cochran and D . Rubin , “ Controlling bias in observational studies : A review , ” Sankhy¯a : The Indian Journal of Statistics , Series A , pp . 417–446 , 1973 .
[ 8 ] P . Rosenbaum and D . Rubin , “ The central role of the propensity score in observational studies for causal effects , ” Biometrika , vol . 70 , no . 1 , pp . 41–55 , 1983 .
[ 9 ]
J . Bailar and D . Hoaglin , Medical uses of statistics . Wiley , 2012 .
[ 10 ] T . Petersen and L . A . Morgan , “ Separate and unequal : Occupationestablishment sex segregation and the gender wage gap , ” American Journal of Sociology , pp . 329–365 , 1995 .
[ 11 ] D . Weichselbaumer and R . Winter Ebmer , “ A meta analysis of the international gender wage gap , ” Journal of Economic Surveys , vol . 19 , no . 3 , pp . 479–511 , 2005 .
[ 12 ] D . A . Smith , C . A . Visher , and L . A . Davidson , “ Equity and discretionary justice : The influence of race on police arrest decisions , ” J . of Criminal Law & Criminology , vol . 75 , p . 234 , 1984 .
[ 13 ] R . Weitzer and S . A . Tuch , “ Race , class , and perceptions of discrimination by the police , ” Crime & Delinquency , vol . 45 , no . 4 , pp . 494–507 , 1999 .
[ 14 ] F . Kamiran , A . Karim , S . Verwer , and H . Goudriaan , “ Classifying socially sensitive data without discrimination : An analysis of a crime suspect dataset , ” in IEEE 12th International Conference on Data Mining Workshops ( ICDMW ) .
IEEE , 2012 , pp . 370–377 .
[ 15 ] F . Kamiran , T . Calders , and M . Pechenizkiy , “ Discrimination aware decision tree learning , ” in 2010 IEEE 10th International Conference on Data Mining ( ICDM ) .
IEEE , 2010 , pp . 869–874 .
[ 16 ] T . Calders and S . Verwer , “ Three naive Bayes approaches for discrimination free classification , ” Data Mining and Knowledge Discovery , vol . 21 , no . 2 , pp . 277–292 , 2010 .
[ 17 ] C . D . Anderson , J . L . Warner , and C . C . Spencer , “ Inflation bias in selfassessment examinations : Implications for valid employee selection . ” Journal of Applied Psychology , vol . 69 , no . 4 , p . 574 , 1984 .
[ 18 ] V . Skreta and L . Veldkamp , “ Ratings shopping and asset complexity : A theory of ratings inflation , ” Journal of Monetary Economics , vol . 56 , no . 5 , pp . 678–695 , 2009 .
[ 19 ] P . Whiting , A . W . Rutjes , J . B . Reitsma , A . S . Glas , P . M . Bossuyt , J . Kleijnen et al . , “ Sources of variation and bias in studies of diagnostic accuracy . a systematic review , ” Annals of Internal Medicine , vol . 140 , no . 3 , pp . 189–202 , 2004 .
[ 20 ] C . Chen , K . Grennan , J . Badner , D . Zhang , E . Gershon , L . Jin , and C . Liu , “ Removing batch effects in analysis of expression microarray data : an evaluation of six batch adjustment methods , ” PloS One , vol . 6 , no . 2 , p . e17238 , 2011 .
[ 21 ] L . Sweeney , “ Discrimination in online ad delivery , ” Communications of the ACM , vol . 56 , no . 5 , pp . 44–54 , 2013 .
[ 22 ] C . Ling , J . Huang , and H . Zhang , “ AUC : a statistically consistent and more discriminating measure than accuracy , ” in International Joint Conference on Artificial Intelligence , vol . 18 , 2003 , pp . 519–526 .
[ 23 ] P . Rosenbaum and D . Rubin , “ Reducing bias in observational studies using subclassification on the propensity score , ” Journal of the American Statistical Association , vol . 79 , no . 387 , pp . 516–524 , 1984 .
80
