2013 IEEE 13th International Conference on Data Mining
Feature Transformation with Class Conditional Decorrelation
Xu Yao Zhang†
† NLPR , Institute of Automation , Chinese Academy of Sciences . {xyz , liucl}@nlpriaaccn ‡ Dept . of EEE , Xi’an Jiaotong Liverpool University , China . kaizhuhuang@xjtlueducn
Kaizhu Huang‡
Cheng Lin Liu†
Abstract—The well known feature transformation model of Fisher linear discriminant analysis ( FDA ) can be decomposed into an equivalent two step approach : whitening followed by principal component analysis ( PCA ) in the whitened space . By proving that whitening is the optimal linear transformation to the Euclidean space in the sense of minimum log determinant divergence , we propose a transformation model called class conditional decorrelation ( CCD ) . The objective of CCD is to diagonalize the covariance matrices of different classes simultaneously , which is efficiently optimized using a modified Jacobi method . CCD is effective to find the common principal components among multiple classes . After CCD , the variables become class conditionally uncorrelated , which will benefit the subsequent classification tasks . Combining CCD with the nearest class mean ( NCM ) classification model can significantly improve the classification accuracy . Experiments on 15 smallscale datasets and one large scale dataset ( with 3755 classes ) demonstrate the scalability of CCD for different applications . We also discuss the potential applications of CCD for other problems such as Gaussian mixture models and classifier ensemble learning .
Keywords class conditional decorrelation , simultaneous diag onalization , feature transformation .
I . INTRODUCTION
In pattern classification for high dimensional data , feature transformation is widely applied as a pre processing technique . Feature transformation can reduce the computational complexity by dimensionality reduction , and also obtain better generalization performance by reducing irrelevant and redundant information in data , overcoming the estimation problem in statistical classifier learning , and revealing the latent structure of data .
Feature transformation can be divided into linear and nonlinear methods . A large variety of linear methods , such as random projection ( RP ) [ 1 ] , principal component analysis ( PCA ) [ 20 ] , Fisher linear discriminant analysis ( FDA ) [ 15 ] , independent component analysis ( ICA ) [ 19 ] , non negative matrix factorization ( NMF ) [ 23 ] , and locality preserving projections ( LPP ) [ 17 ] , have been proposed from different statistical or geometrical viewpoints . The nonlinear methods include : ( i ) kernel extension of the linear methods , such as kernel PCA [ 33 ] and kernel FDA [ 38 ] ; ( ii ) manifold learning models such as ISOMAP [ 36 ] , LLE [ 32 ] and Laplacian eigenmaps [ 2 ] ; ( iii ) deep neural networks [ 18 ] , [ 34 ] which use a deep architecture to learn the nonlinear data mapping . Fisher linear discriminant analysis ( FDA ) [ 15 ] is one of the most famous linear supervised algorithms . The principle
1550 4786/13 $31.00 © 2013 IEEE DOI 101109/ICDM201343
887 of FDA is to minimize the within class variance as well as maximize the between class variance . Under the homoscedastic Gaussian assumption , FDA leads to the optimal projection axes when the reduced dimensionality is K−1 ( K is the number of classes ) . However , in many other occasions , FDA is only a suboptimal model , eg , many models have been proposed : ( i ) to solve the class separation problem when the reduced dimensionality is much smaller than the number of classes [ 27 ] , [ 35 ] , [ 3 ] , [ 40 ] ; ( ii ) to improve FDA under the heteroscedastic case [ 26 ] , [ 42 ] ; and ( iii ) to alleviate the small sample size problem [ 7 ] , [ 39 ] .
In this paper , we first decompose FDA into an equivalent two step approach : whitening and PCA in the whitened space . By proving that whitening is the optimal linear transformation to transform the covariance matrices of different classes to the identity matrices , we further propose a new model called class conditional decorrelation ( CCD ) . The objective of CCD is to learn a linear transformation attempting to diagonalize all the covariance matrices for each class simultaneously . CCD is more flexible than whitening and can find the common principal components among multiple classes [ 14 ] . Furthermore , the modified Jacobi method is used to solve the optimization problem of CCD efficiently . After CCD transformation , the variables become class conditionally uncorrelated . This will benefit the following classification tasks . In this paper , we combine CCD with the nearest class mean ( NCM ) classification model to learn an improved distance metric . The original NCM classifiers are inferior to other discriminative classifiers , however , with the help of CCD , the improved NCM classifiers can achieve comparable performance with other benchmark classifiers such as the NCM metric learning method [ 29 ] . Experiments on 15 small scale datasets and one large scale dataset ( with 3 , 755 classes ) demonstrate that : the structure of latent common principal components for multiple classes exists not only in small category problems but also in large category problems . CCD is effective in finding such structure and improving the classification performance . Besides the applications of CCD in this paper , we also extend CCD into a much more generalized formulation and show the potential advantages of CCD for other problems such as Gaussian mixture models and classifier ensemble learning .
The rest of this paper is organized as following : Section II introduces the decomposition of FDA ; Section III proves that whitening is the optimal linear transformation to the
Euclidean space ; Section IV presents the proposed model of class conditional decorrelation ( CCD ) ; Section V describes the combination of CCD for nearest class mean ( NCM ) classification ; Section VI reports the experimental results ; Section VII offers some potential extensions of CCD ; and Section VIII draws the concluding remarks . II . DECOMPOSITION OF FDA d be the mean vector and Σk ∈ R d×d be the covariance matrix for class k ( k = 1 , . . . , K ) . The withinclass and between class scatter matrices are defined as :
Let μk ∈ R
K . k=1
1 K
Σk ,
( μk − μ0)(μk − μ0 )
. ,
Sw =
K . k=1
Sb =
1 K fiK
( 1 )
( 2 )
( 4 )
( 5 ) where μ0 = 1 k=1 μk , and we assume the prior probaK bilities are equal for all the classes . The objective of FDA is to learn a transformation matrix W ∈ R d×d . to transform the feature into a low dimensional space xfi = W .x by minimizing the within class variance as well as maximizing the between class variance . It is easy to check that the scatter matrices in the transformed space become W .SwW and W .SbW . There are many formulations of FDA , and two typical criteria are given in the following [ 15 ] : W .SbW
,
( 3 ) tr max
W max
W ln
( ( .
'ff −1ff ( (W .SwW ( ( − ln ( (W .SbW W .SwW ff W .SbW , st W .SwW = I , tr max
W∈Rd×d .
The above two criteria are equivalent problem : to a constrained where I is the identity matrix . Usually , this model is solved by a two step approach . The first step is the whitening . Definition 1 . The whitening transformation matrix is
−1/2 ∈ R d×d ,
Wwhiten = P Λ
( 6 ) where P is the eigenvector matrix and Λ is the diagonal eigenvalue matrix of the within class scatter matrix : Sw = P ΛP The whitening transformation satisfies
W . whitenSwWwhiten = I .
( 7 ) The whitening transformation is to transform the withinclass scatter matrix into the identity matrix , after that the Euclidean distance becomes a suitable measurement between different classes .
Let WFDA = WwhitenW , we can rewrite FDA of ( 5 ) as : whitenSbWwhitenW ff W .W . st W .W = I . tr max
W∈Rd×d .
,
( 8 )
888
Hence the second step of FDA is to solve ( 8 ) . This is exactly the PCA among W . whitenμK . That means FDA is equivalent to whitening followed by PCA of the class means on the whitened space . whitenμ1 , . . . , W .
III . WHITENING
In this section , we show that , in an information theoretic viewpoint [ 10 ] , whitening is the optimal linear transformation to the Euclidean space , which implies the advantages of the two step approach of FDA . Theorem 1 . The whitening transformation of ( 6 ) minimizes the Log Determinant divergence between the transformed covariance W .
ΣkW and the identity matrix I .
Proof : The Log Determinant divergence [ 11 ] between two n × n matrices is defined as :
) − log det(XY −1
) − n .
( 9 )
Dld(X , Y ) = tr(XY −1 ff W .
K .
F =
Dld
ΣkW , I
In this paper , we define the objective of whitening ( 6 ) as : k=1 tr(W .
ΣkW ) − log det(W .
ΣkW ) − d
.
( 10 ) min
W∈Rd×d
K .
)
= k=1
By setting the derivative of the objective function wrt W to zero , we get ∂F ∂W = 2
ΣkW − 2KW − .
K .
= 0 ,
( 11 )
= 0 . Insert Sw = P ΛP . and k=1 which means SwW − W − . Wwhiten = P Λ
−1/2 into it , we get −1/2 − P Λ P ΛP .P Λ
1/2
= 0 .
( 12 )
Since P .P = I , this completes the proof .
From Theorem 1 , we can conclude that : even when the covariances Σk , k = 1 , . . . , K are not equal for all the classes ( heteroscedastic ) , whitening transformation is still a good model to find the optimal Euclidean space . In the whitened space , the Euclidean distance becomes a suitable measurement . Therefore each class can be described by W . whitenμk , and the PCA transformation among them ( 8 ) can find the optimal separated subspace for classification .
IV . CLASS CONDITIONAL DECORRELATION
A . Motivation and Definition
The whitening in ( 10 ) is to transform Σ1 , . . . , ΣK into the identity matrix . However , in most cases this is impossible . Although the within class scatter matrix Sw is transformed to the identity matrix in Eq ( 7 ) , the covariance matrices of each class Σ1 , . . . , ΣK are still far from the identity matrix due to the divergences among them . In light of this , we consider a relaxed formulation of the whitening . Instead of transforming Σ1 , . . . , ΣK into the identity matrix , we would like to diagonalize them as much as possible : min
W∈Rd×d ffff
ΣkW
K . ffffW . st W .W = I , . fiAfi2,off = k=1
A2 ij . iff=j
,
2,off
( 13 )
( 14 ) where
The constraint W .W = I is to avoid trivial solutions . We call this model class conditional decorrelation ( CCD ) , because we try to diagonalize ( or decorrelate ) simultaneously Σ1 , . . . , ΣK , which are the class conditional covariance matrices for each class . After CCD transformation , the variables become class conditionally uncorrelated , which can benefit the subsequent classification tasks . Lemma 1 . For arbitrary A ∈ R ffff2 orthogonal transformation W ∈ R F = fiAfi2 F , d×d , under any full rank d×d , W .W = I , we have ( 15 ) ffffW .AW fid where fiAfi2
F = i,j=1 A2 ij .
Because of Lemma 1 , by minimizing the sum of the squared non diagonal elements , the objective of CCD ( 13 ) will concentrate its energy on the diagonal elements , which leads to two main advantages . First , CCD is more flexible than whitening : CCD focuses on diagonalizing all the covariance matrices simultaneously , while the objective of whitening is not only diagonalization but further restricts the transformed matrix to be the identity matrix ( 10 ) . Second , CCD can find the common principal components among multiple classes ( known as common PCA [ 14 ] , [ 13] ) . The goal of PCA is to diagonalize the total scatter matrix of all the samples . Furthermore , the goal of CCD is to diagonalize all the class wise covariance matrices simultaneously . If there is only one class ( K = 1 ) , CCD is reduced to PCA . For multiple classes , the transformation axes learned by CCD are the common principal components for all the classes . In the following section , we will give an illustration of these two advantages . B . Illustration of CCD
We use the well known handwritten digit database MNIST [ 22 ] to give an intuitive understanding of CCD . First , the original covariance matrices of the digits are shown in Figure 11 . We can see that all the covariance matrices are very dense . For each matrix , the percentage of the sum of the squares of the diagonal elements is defined as :
P ( A ) =
.
( 16 ) fid fid i=1 A2 ii i,j=1 A2 ij
1For better visualization , the 3D bar images represent the absolute values of the elements in each covariance matrix .
889
Because the sum of the squares of all the elements will not change under any orthogonal transformation ( Lemma 1 ) , P ( · ) is a good metric to measure the concentration of the energy on the diagonal elements . We show the P ( · ) of each covariance matrix above the image . We can find that the percentages P ( · ) are all below 10 % , which indicates the non diagonal elements are very dense . After whitening , the covariance matrices are shown in Figure 2 . Although the covariance matrices after whitening are much like the identity matrices , the non diagonal elements of them are still very dense . Furthermore , with class conditional decorrelation ( CCD ) , the covariance matrices of the digits are shown in Figure 3 . We can find that : after CCD transformation , the covariance matrix of each class becomes very sparse , ie , most of the non diagonal elements become very small . The percentages P ( · ) are dramatically increased . Moreover , the diagonal elements are also sparse , for which the largest elements are concentrated in the first few diagonal elements , and the remaining diagonal elements are very small . Since the largest diagonal elements means the principal components of a particular class , this indicates the advantages of CCD on finding the common principal components among multiple classes .
C . Optimization
In this section , we dive into solving the optimization problem of CCD ( 13 ) . This problem is known as simultaneous diagonalization [ 43 ] which cannot be solved by the eigenvalue decomposition algorithm such as whitening ( 6 ) . Therefore , we adopt the modified Jacobi method to solve the CCD problem . The original Jacobi method is very effective to find the eigenvectors and eigenvalues of a single symmetric matrix [ 16 ] , and has been successfully used for sparse high dimensional covariance matrix estimation [ 5 ] , [ 4 ] . To deal with multiple symmetric matrices , the modified Jacobi method [ 6 ] , [ 28 ] can be used efficiently and effectively . R(i , j , θ ) ∈ R
1 ) Jacobi Rotation : A basic Jacobi plane rotation d×d ( i '= j ) is defined as
⎧⎪⎪⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎪⎪⎩ p = q , p '= i , p '= j 1 , cos θ , p = q = i cos θ , p = q = j sin θ , p = j , q = i − sin θ , p = i , q = j otherwise . 0 ,
[ R(i , j , θ)]pq =
( 17 )
The [ ·]pq denotes the pq th element in a matrix . R(i , j , θ ) is an orthogonal rotation in the plane of the two coordinates i and j with rotation angle θ . Moreover , R(i , j , θ ) is a very sparse matrix with only the diagonal , ij th and ji th elements being non zero .
The basic Jacobi plane rotation R(i , j , θ ) is used sequentially to transform the sum of squares of the off diagonal elements of all the covariance matrices to be as low as possible . By accumulating the Jacobi plane rotations , we
Figure 1 . The covariance matrices of 5 digits . The images of digit 5 to digit 9 are omitted due to the high similarity .
Figure 2 . The covariance matrices after whitening .
Figure 3 . The covariance matrices after class conditional decorrelation ( CCD ) .
θ
. k=1 the ffff such angle
2,off is minimized . rotation ΣkR(i , j , θ ) ffffR(i , j , θ ) can get the final orthogonal transformation matrix of CCD . The basic steps of the modified Jacobi method [ 28 ] involve : • choose an index pair ( i , j ) that satisfies 1 ≤ i < j ≤ d . fiK • calculate that • overwrite Σk with Σnew k R(i , j , θ ) . With the index pair ( i , j ) traveling through {1 , 2 , . . . , d− 1} × {i + 1 , i + 2 , . . . , d} , the updating is implemented repeatedly until convergence . In each iteration , the oblast , we get jective function of the CCD matrix as the accumulated Jacobi rotations W = R(i1 , j1 , θ1)R(i2 , j2 , θ2 ) . . . R(in , jn , θn ) . Therefore , the key problem is to learn the rotation angle when fixing the index pair ( i , j ) . is decreased . At k = R(i , j , θ )
Σold
( 13 )
.
2 ) Rotation Angle θ for Fixed i and j : When fixing i and j , the learning of the rotation angle θ can be formulated as : min
θ ffffR(i , j , θ )
K . ffffR(i , j , θ ) k=1
. ffff ffff
Lemma changed on the ij th elements wrt different θ .
2 .
.AR(i , j , θ )
2,off will only be
ΣkR(i , j , θ )
.
2,off
( 18 )
Hence , the problem in ( 18 ) is equivalent to
R(i , j , θ )
.
ΣkR(i , j , θ )
.
( 19 )
2 ij
K .
) min
θ k=1
)
Moreover
R(i , j , θ )
.AR(i , j , θ ) ij
F
2
( 20 )
( 21 )
= ( cos
2 θ − sin
2 θ)Aij + cos θ sin θ(Ajj − Aii ) . Therefore the rotation angle problem in ( 19 ) becomes :
2
2θD + ( 1/4 ) sin
2
2θE + sin 2θ cos 2θF cos 4θ + sin 4θ +
D + E/4
. min
θ cos D − E/4
=
Here
2
D =
F =
K . K . k=1 k=1
2 ff
Σ k jj − Σ k ii
K . k=1
.
2
,
( 22 )
( 23 )
( Σ k ij )
2 , E = k ij
Σ k jj − Σ k ii
Σ ff
We use Σk ij to denote the ij th elements in Σk . By some trigonometric analysis [ 28 ] , the optimal θ can be computed
890
⎧⎪⎪⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎪⎪⎩ as :
θ = where
D − E/4 > 0 , F = 0 π/4 , D − E/4 ≤ 0 , F = 0 0 , ( 3π/2 − φ)/4 , D − E/4 ≥ 0 , F > 0 ( π/2 − φ)/4 , D − E/4 ≥ 0 , F < 0 ( π/2 − φ)/4 , D − E/4 < 0 , F < 0 ( −π/2 − φ)/4 , D − E/4 < 0 , F > 0
D − E/4
F
∈ ( − π 2
,
π
2
φ = arctan
) .
( 25 )
( 24 )
)
%
( e g a t n e c r e P e g g a r e v A
65 60 55 55 50 45 40 35 30 25 20 15 10 5
3 ) The CCD Algorithm : The Jacobi plane rotation ( 17 ) is used to find the optimal rotation angle θ when fixing i and j ( 18 ) . To solve the CCD model in ( 13 ) , the basic Jacobi plane rotations are used sequentially to minimize the sum of squares of the off diagonal elements . The complete procedures are shown in Algorithm 1 . After we get the optimal Jacobi plane rotation , all the covariance matrices are rotated in Eq ( 26 ) . The transformations are accumulated in Eq ( 27 ) to get the final transformation of CCD .
Input : the covariance matrices {Σ1 , . . . , ΣK} ∈ R Initial : W = I Do the following steps repeatedly until convergence : for i = 1 : d − 1 , for j = i + 1 : d , d×d get R(i , j , θ ) by Eq ( 24 ) and Eq ( 17 ) k R(i , j , θ),∀k new k = R(i , j , θ )
. old
Σ
Σ
Return : W ∈ R
W new
= W oldR(i , j , θ ) d×d
Algorithm 1 : Class Conditional Decorrelation
( 26 )
( 27 )
Theorem 2 . The returned transformation matrix of Algorithm 1 satisfies the orthogonal constraint in ( 13 ) .
Proof : The output of Algorithm 1 is W = R(i1 , j1 , θ1)R(i2 , j2 , θ2 ) . . . R(in , jn , θn ) . ( 28 ) is easy to check that each Jacobi plane rotation is
.R(i , j , θ ) = I .
R(i , j , θ ) This leads to W .W = I . Theorem 3 . The objective function in ( 13 ) is non increasing under the updating rules in ( 26 ) and ( 27 ) .
( 29 )
Proof : When fixing i and j , the Jacobi plane rotation angle is selected to minimize ( 18 ) . Therefore the objective function of ( 13 ) at the optimal solution of current Jacobi plane rotation will not be larger than the former step , since the searching space of ( 18 ) covers θ = 0 which reduces the Jacobi plane rotation to the identity matrix . Therefore , with accumulated Jacobi plane rotations in ( 26 ) and ( 27 ) , the objective function in ( 13 ) will be non increasing .
It orthogonal :
0 1 2 3 4 5 6
7 8 9 10 11 12 13 14 15 16
Numb ber of Iterations
Figure 4 . Convergence analysis .
4 ) Computational Complexity : In each rotation , the sum of squares of off diagonal elements is decreased . When the calculated rotation angle θ goes to zero , there will be no more transformations ( the Jacobi rotation with θ = 0 is an identity matrix ) . This means that the objective function of CCD becomes invariant to any transformations . We stop the iterations in Algorithm 1 when the change of the objective function in ( 13 ) is lower than a pre defined threshold .
The main computational complexity of CCD ( as shown in Algorithm 1 ) is the outside iteration of the traveling of index pair . For each fixed index pair i and j , the main computations are : ( i ) calculate the Jacobi rotation ( 17 ) ; ( ii ) rotate the covariance matrices ( 26 ) ; and ( iii ) accumulate the transformation ( 27 ) . These steps are linear dependent with the number of classes K . By considering the outside index traveling steps of i = 1 : d− 1 , j = i + 1 : d in Algorithm 1 as one iteration , we show the average percentage P ( · ) of the 10 covariance matrices ( see Section IV B ) with respect to the number of iterations in Figure 4 . We can find that : ( i ) with one iteration , the percentage is dramatically increased ; and ( ii ) after three iterations , the algorithm is nearly converged . This indicates the effectiveness of the modified Jacobi method in solving the CCD problem .
V . NEAREST CLASS MEAN CLASSIFICATION WITH CCD In this section , we integrate CCD into the nearest class mean ( NCM ) classification models [ 37 ] , [ 29 ] . NCM represents each classes by their mean feature vector of its samples , and assigns a new pattern to the class k ∈ {1 , . . . , K} with the closest mean : d(x , μk ) , x ∈ class arg
μk =
1 Nk
K min k=1
. xi , i:yi=k
( 30 )
( 31 ) where μk is the mean vector for class k , and d(x , μk ) is a distance metric between a pattern x and class mean μk , and yi is the ground truth label of pattern xi , and Nk is the number of training samples in class k . Contrary to the
891 k NN classifier , NCM is much more efficient , because only the class wise mean vectors are needed to be estimated and saved for future prediction . Furthermore , the NCM classifier is much more efficient and effective in generalizing to new classes [ 29 ] by adding or adjusting the new class mean .
The success of the NCM classifier critically depends on the used distance metric d(x , μk ) . The simplest metric is the Euclidean distance ( ED ) :
NCMED : d(x , μk ) = fix − μkfi2 .
( 32 )
However , in many situations , the Euclidean distance is not the optimal measurement .
Given the mean vector μk and covariance matrix Σk for each class , the optimal Bayes classifier is a quadratic discriminant function : d(x , μk ) = log |Σk| + ( x − μk )
. k ( x − μk ) . −1
Σ
( 33 )
For many applications ( eg large category K or high dimensionality d ) , the computation and storage for the inverse −1 covariance matrix Σ are both very expensive due to the k singular problem and Kd(d + 1)/2 free parameters . Therefore , derived from the diagonal assumption of covariance matrices , the weighted distance ( WD ) is widely used as an approximation of the original distance :
NCMWD : d(x , μk ) = log |diag(Σk)| +
.
−1 diag(Σk )
( x − μk )
( x − μk ) ,
( 34 ) where diag(A ) is a d × d matrix with i , i th elements equal to Ai,i and i , j th ( ∀j '= i ) elements equal to zero . In this way , the inverse matrix can be efficiently calculated as ]i,j = 0,∀j '= i . [ diag(A ) In the singular situation , the zero diagonal elements Ai,i are replaced with a positive constant ( selected via crossvalidation ) .
,∀i and [ diag(A )
]i,i = 1 Ai,i
−1
−1
A . Using CCD to Improve NCMED and NCMWD
NCMED and NCMWD are only suboptimal models , however , we can use CCD to improve their performance by learning a more suitable distance metric d(x , μk ) . Suppose the CCD transformation matrix WCCD ∈ R d×d has already been learned from the training data , we can define the multiple dimensional scaling as
δi =
1 K
W .
CCDΣkWCCD
, i = 1 , . . . , d . ii
( 35 )
We use Λ ∈ R d×d to represent the matrix with Λi,i = √ δi,∀i and Λi,j = 0,∀j '= i . For NCMED , the improved metric is defined as :
CCD NCMED : d(x , μk ) = ffffΛ
−1W .
CCD(x − μk ) ffff2
K .
) k=1
( 36 )
,
892
For NCMWD , the improved metric is defined as
( ((diag(
( (( +
.WCCDdiag(
Σk )
Σk )
−1
CCD NCMWD : d(x , μk ) = log ( x − μk ) where Σk = W .
( 37 )
CCD(x − μk ) , W .
CCDΣkWCCD . After CCD transformation , the variables become class conditionally uncorrelated , therefore , the classification performance will be improved for both NCMED and NCMWD . In other words , CCD is used to learn a much better distance metric for NCM classification by considering the class conditional correlation information of different classes simultaneously .
( 39 )
B . Comparison with Other Models
There are also other models attempting to learn a distance metric for NCM classification .
1 ) NCMML : Nearest Class Mean Metric Learning : The recently proposed NCMML [ 29 ] model learns a low rank Mahalanobis distance metric for NCM : NCMML : d(x , μk ) = fiW ( x − μk)fi2
= ( x − μk )
.W .W ( x − μk ) ,
( 38 ) where W ∈ R d.×d ( dfi < d ) is a dimensionality reduction matrix and W .W is a low rank Mahalanobis distance metric . Using a multi class logistic regression formulation :
,
2 d(x , μk )
2 d(x , μk . ) ff− 1 ff− 1 exp fiK p(k|x ) = fiN i=1 log p(yi|xi ) . k.=1 exp the projection matrix W can be learned via maximizing the log likelihood of the correct predictions of the training samples : maxW
Compared with the original NCMED and NCMWD , NCMML is more flexible and accurate by learning a Mahalanobis distance from the data . However , the covariance information of different classes are not taken into consideration . Furthermore , the learned Mahalanobis distance matrix is shared for all classes , which cannot reflect the difference between the covariance matrices of different classes . By taking the second order covariance information into the learning process , CCD NCMED and CCD NCMWD can learn better distance metrics for NCM classification .
2 ) LDF : Linear Discriminant Function : The classical LDF model assumes all the classes sharing the same covariance matrix [ 15 ] : Σ0 = 1 k=1 Σk , and defines the K distance metric as :
LDF : d(x , μk ) = ( x − μk )
.
0 ( x − μk ) . −1
Σ
( 40 )
LDF is equivalent to whitening ( Section III ) followed by NCMED . For heteroscedastic problems ( different classes have different covariance matrices ) , LDF is only a suboptimal model and can not achieve good performance . fiK
Table I
INFORMATION OF 15 DATASETS . german.numer mushrooms australian breast cancer heart ionosphere liver disorders iris svmguide2 wine vehicle svmguide4 glass segment vowel
#class
2 2 2 2 2 2 2 3 3 3 4 6 6 7 11
#feature
24 112 14 10 13 34 6 4 20 13 18 10 9 19 10
#sample
1000 8124 690 683 270 351 345 150 391 178 846 612 214 2310 990 testing cv 10 cv 10 cv 10 cv 10 cv 10 cv 10 cv 10 cv 10 cv 10 cv 10 cv 10 cv 10 cv 10 cv 10 cv 10
3 ) SVM : Support Vector Machine : SVM is a state ofthe art classifier in many domains . Equipped with the large margin training ( hinge loss and regularization ) and kernel tricks , SVM is effective to find the optimal classification boundaries due to the convex formulation . The performance of NCMED and NCMWD should be inferior to SVM . However , with the help of CCD , we will show in the following sections that the performance of CCD NCMED and CCD NCMWD will become competitive with SVM .
VI . EXPERIMENTS
In this section , we first compare different classification models on 15 small scale databases . We also evaluate the possibility of CCD on simultaneously diagonalizing thousands of covariance matrices .
A . Classification on 15 Databases
We use the datasets collected on the LIBSVM website2 to evaluate different models . The complete information of different databases are shown in Table I . For the preprocessing of data , each attribute is linearly scaled to [ −1 , 1 ] or [ 0 , 1 ] . In the SVM training process [ 8 ] , the cost parameter C was set as 1 and the γ in RBF kernel was set as num features . For multi class problems , the one versus one strategy is adopted for SVM , while the NCM based classifiers are intrinsically multi class models . For each database , we randomly partition the data into two subsets : using 90 % of them for training and the remaining 10 % for testing . This “ partitionevaluation ” process is repeated 10 times , and we report the average accuracy and standard deviation for each model .
1
The comparisons of NCMED , NCMWD , LDF , NCMML , SVM Linear , SVM RBF , CCD NCMED , and CCDNCMWD are shown in Table II . We can find that : the performance of NCMED and NCMWD are very poor when compared with the SVM classifiers . This is because NCMED and NCMWD are only generative models which are not optimized to minimize the empirical loss on training data ( such as the discriminative model of SVM ) . Furthermore , only the Euclidean distance and weighted distance are equipped with NCMED and NCMWD , therefore , the rich discriminative information embedded in the training data are not fully explored by NCMED and NCMWD . However , with the help of class conditional decorrelation ( CCD ) , the improved distance metrics of CCD NCMED and CCDNCMWD can significantly boost the accuracies as shown in Table II . The performance of CCD NCMED and CCDNCMWD are comparable with SVM classifiers . This is because CCD can decorrelate the class conditional variables , and the covariance information of different classes are taken into consideration simultaneously , which can learn a much better distance metric for NCM classification .
Compared with other metric learning methods for NCM classification such as LDF and NCMML , CCD achieves superior performance . This is because LDF is based on the homoscedastic assumption which is equivalent to whitening followed by Euclidean distances , and CCD is more flexible than the whitening model ( see Section IV A ) . NCMML directly learns a low rank Mahalanobis distance metric for NCM classification by maximizing the likelihood of data under a multi class logistic regression formulation , therefore , its performance is comparable with SVM classifiers as shown in [ 29 ] . However , the second order covariance information is not taken into consideration for the learning process of NCMML .
Taking all the comparisons together , we can conclude : with class conditional decorrelation ( CCD ) , the performance of NCMED and NCMWD can be significantly improved , which become very competitive to other classifiers .
B . CCD for Large Scale Application
Previous evaluations are conducted on some small scale databases . In this section , we use a large scale database to evaluate the possibility of simultaneously diagonalizing thousands of covariance matrices , and also evaluate the performance of CCD on improving the classification accuracies for problems with thousands of classes
The used database is the 3 , 755 class handwritten online Chinese character database CASIA OLHWDB1.1 [ 24 ] . Handwritten Chinese character recognition is a challenging problem due to the large number of classes and handwriting style variation across individuals [ 41 ] . For representing a character sample , we use a benchmark feature extraction method [ 25 ] : 8 direction histogram feature extraction combined with pseudo 2D bi moment normalization . The feature dimensionality is 512 . The number of training and testing samples are 898 , 573 and 224 , 559 respectively , for which the statistical significance of evaluations should be sufficient . The extracted feature data can be downloaded from website3 .
2http://wwwcsientuedutw/%7Ecjlin/libsvmtools/datasets
3http://wwwnlpriaaccn/databases/handwriting/Downloadhtml
893
CLASSIFICATION ACCURACIES AND STANDARD DEVIATIONS ( % ) OF DIFFERENT MODELS ON 15 DATASETS .
Table II
NCMML 7360(386 ) 9996(006 ) 8793(266 ) 9652(183 ) 8444(488 ) 9206(394 ) 6314(791 ) 9933(211 ) 8195(682 ) 10000(000 ) 7930(337 ) 4937(537 ) 6000(815 ) 9221(144 ) 7181(508 )
SVM Linear 7790(264 ) 10000(000 ) 8671(243 ) 9536(150 ) 8222(672 ) 8639(661 ) 6371(556 ) 9600(466 ) 5610(000 ) 9737(372 ) 7884(379 ) 2937(376 ) 6391(650 ) 9433(125 ) 7172(539 )
SVM RBF 7510(300 ) 9985(010 ) 8429(381 ) 9667(237 ) 8519(720 ) 9500(255 ) 5771(443 ) 9667(351 ) 5610(000 ) 9895(222 ) 7151(443 ) 2111(318 ) 6087(767 ) 9152(175 ) 7768(379 )
CCD NCMED
7310(321 ) 9977(024 ) 8814(323 ) 9739(272 ) 8296(681 ) 8667(468 ) 6457(776 ) 9800(450 ) 8000(524 ) 9842(254 ) 7698(398 ) 5095(439 ) 6522(845 ) 9030(216 ) 5354(557 )
CCD NCMWD
7180(343 ) 9998(005 ) 8843(593 ) 9667(227 ) 8148(605 ) 9389(315 ) 6343(599 ) 9600(466 ) 8049(501 ) 10000(000 ) 8023(478 ) 5159(546 ) 5652(680 ) 9130(170 ) 7232(393 )
LDF
7336(216 ) 9966(012 ) 8600(403 ) 9650(156 ) 8412(355 ) 8933(371 ) 6310(531 ) 9882(116 ) 8030(302 ) 9856(116 ) 7920(215 ) 4861(521 ) 6132(532 ) 9211(123 ) 6541(566 )
90
88
)
% german.numer mushrooms australian breast cancer heart ionosphere liver disorders iris svmguide2 wine vehicle svmguide4 glass segment vowel
NCMED 6870(362 ) 8947(087 ) 8700(297 ) 9638(196 ) 8037(553 ) 6917(723 ) 5486(518 ) 8933(644 ) 7878(822 ) 9684(368 ) 4326(550 ) 2095(634 ) 4826(694 ) 8446(293 ) 4828(377 )
NCMWD 7050(414 ) 9905(032 ) 8743(335 ) 9536(191 ) 8000(949 ) 8583(462 ) 5343(934 ) 9200(820 ) 7561(630 ) 9737(447 ) 4663(631 ) 4476(539 ) 4174(716 ) 8095(190 ) 6869(431 )
)
%
( y c a r u c c A n o i t a c i f i s s a C l
87 84 81 78 75 72 69 66 63 60
NCMED
CCDͲNCMED
NCMWD
CCDͲNCMWD
( y c a r u c c A A n o i t a c i f i s s a C l
86
84
82
80
NCMED
CCDͲNCMED
NCMWD
CCDͲNCMWD
91
90
89
88 88
87
86
85
NCMED
CCDͲNCMED
NCMWD
CCDͲNCMWD
)
%
( y c a r u c c A A n o i t a c i f i s s a C l
60
80
100
120
140
160
180
200
60
80
Reduced Dimensionality Random Projection
100
160
120
40 14 nsionality Reduced Dimen
PCA
180
200
60
80
100
120
140
160
180
200
Reduced Dimensionality
FDA
Figure 5 . Effectiveness of CCD on the 3 , 755 class problem .
We use different dimensionality reduction methods ( random projection , PCA , and FDA ) to reduce the highdimensional data into different subspaces . In each subspace , the classification accuracies of NCMED and NCMWD are compared with CCD NCMED and CCD NCMWD respectively . Because our goal is to evaluate the effectiveness of CCD for thousands of classes problem , we did not compare other methods ( NCMML and SVM ) . The comparison results are shown in Figure 5 . We can find that : for both NCMED and NCMWD , the classification accuracies can be significantly improved when equipped with CCD transformation . Random projection ( RP ) [ 1 ] is a data independent model , while PCA [ 20 ] is an unsupervised model . The supervised class information is ignored by RP and PCA . Contrarily , the objective of CCD is to decorrelate the covariance matrices of each class simultaneously , which can learn a more suitable distance metric . Therefore CCD can improve the accuracies significantly for RP and PCA . FDA [ 15 ] is the most wellknown supervised dimensionality reduction model , which is equivalent to a two step approach : whitening followed by PCA in the whitened space ( Section II ) . However , CCD can still significantly improve the classification accuracies in the FDA transformed subspaces as shown in Figure 5 .
This is because CCD can decorrelate the variables in each class , while after whitening the non diagonal elements of the covariance matrices are still dense ( see Section IV B ) . This indicates the advantages of CCD against whitening .
Altogether , from the analyses above , we can conclude that : even when the number of classes is as large as 3 , 755 , CCD is still effective in improving the classification accuracies . This indicates that : ( i ) the structure of latent common principal components for multiple classes exists not only in small category problems but also in large category problems ; and ( ii ) the modified Jacobi algorithm used in CCD is efficient and effective to find such structure via simultaneous diagonalization of even thousands of covariance matrices . This makes CCD scalable for both small scale and largescale applications .
VII . FURTHER EXTENSIONS
Besides the previous applications of CCD in nearest class mean ( NCM ) classification , CCD can be also used for many other problems in pattern recognition and machine learning . The Gaussian mixture model ( GMM ) widely used to approximate arbitrarily complicated distributions , has been successfully applied in many applications such as speak
894 holds , hence the modified Jacobi method cannot be used to solve the p '= 2 problems . Finding efficient algorithms to solve GCCD with arbitrary p is an interesting topic .
VIII . CONCLUSION
In this paper , motivated from the whitening ( Theorem 1 ) used in the classical Fisher linear discriminant analysis ( two step decomposition ) , we proposed the class conditional decorrelation ( CCD ) model for simultaneous diagonalization of covariance matrices for all classes . The modified Jacobi method is adopted to solve the optimization problem efficiently . After CCD transformation , the variables become class conditionally uncorrelated which can benefit the following classification tasks . Combining CCD with the nearest class mean ( NCM ) classification model is show to be competitive with other classifiers . CCD is also shown to be effective for large scale problems which have thousands of classes . Besides the presented applications in this paper , CCD can also be hopefully extended to other applications such as Gaussian mixture models and classifier ensemble learning . The nonlinear extension of CCD such as kernelization is also an interesting direction .
ACKNOWLEDGEMENTS
This work was supported by National Basic Research Program of China ( 973 Program ) Grant 2012CB316300 and National Natural Science Foundation of China ( NSFC ) Grant 61075052 .
REFERENCES
[ 1 ] N . Ailon and B . Chazelle .
Faster dimension reduction .
Communications of the ACM , 53(2):97–104 , 2010 .
[ 2 ] M . Belkin and P . Niyogi . Laplacian eigenmaps and spectral techniques for embedding and clustering . Proc . Advances in Neural Information Processing Systems , 14:585–591 , 2001 . [ 3 ] W . Bian and D . Tao . Max Min distance analysis by using sequential SDP relaxation for dimension reduction . IEEE Trans . Pattern Analysis and Machine Intelligence , 33(5):1037–1050 , 2011 .
[ 4 ] G . Cao , LR Bachega , and CA Bouman .
The sparse matrix transform for covariance estimation and analysis of high dimensional signals . IEEE Trans . Image Processing , 20(3):625–640 , 2011 .
[ 5 ] G . Cao and CA Bouman . Covariance estimation for high dimensional data vectors using the sparse matrix transform . Advances in Neural Information Processing Systems , 2008 . Jacobi angles for simultaneous diagonalization . SIAM J . Matrix Analysis and Applications , 17(1):161–164 , 1996 .
[ 6 ] JF Cardoso and A . Souloumiac .
[ 7 ] H . Cevikalp , M . Neamtu , M . Wilkes , and A . Barkana . Discriminative common vectors for face recognition . IEEE Trans . Pattern Analysis and Machine Intelligence , 27(1):4–13 , 2005 . [ 8 ] C C Chang and C J Lin . LIBSVM : A library for support vector machines . ACM Trans . Intelligent Systems and Technology , 2:27:1–27:27 , 2011 .
( a )
( b )
Figure 6 . Two structure templates . Sij = 0 for the black area and Sij = 1 for the white area . er verification [ 31 ] , object representation [ 30 ] and image classification [ 9 ] . However , GMM has many parameters to estimate which is prone to overfit the training data . A widely used method to alleviate the overfitting and reduce the computational complexity is to use diagonal covariance matrices for each mixture component . Therefore the CCD algorithm can be incorporated into the expectation maximization ( EM ) learning process of GMM to make the diagonal assumption more reasonable .
We can also extend the CCD ( 13 ) into a more generalized formulation ( GCCD ) by integrating some structure information into the learning process
K . ffffW . min
W∈Rd×d k=1 where ffff d . p,S
ΣkW fiAfip,S =
, st W .W = I , ( 41 )
Sij|Aij|p .
( 42 ) i,j=1
The S ∈ {0 , 1}d×d is the pre defined structure template . For example , in this paper , we use a structure template as Figure 6(a ) to get uncorrelated dimensionalities ( see the definition of fiAfi2,off in Eq ( 14) ) . We can also use other templates such as Figure 6(b ) to transform the covariance matrices into block like forms to get uncorrelated subspaces . In this way , different classifiers trained in different subspaces will contain complementary information , and the combination of them can be used to further boost the classification accuracy . This is known as classifier ensemble learning [ 12 ] . Another benefit of this ensemble is that the number of parameters can significantly be reduced for the classifiers which have the number of parameters superlinear dependent on the dimensionality .
The modified Jacobi method ( Algorithm 1 ) can be used directly to solve the GCCD problem with p = 2 . Because the Jacobi rotation ( 17 ) is an elementary operator which is easy for incorporating structure information , we only need to change the traveling of the index in Algorithm 1 from i = 1 : d − 1 , j = i + 1 : d to Sij = 1,∀i , j .
We can also consider other values of p , eg , the L1 norm of p = 1 which is proven to be less sensitive and more robust to outliers [ 21 ] . However , for p '= 2 the Lemma 2 no longer
895
[ 9 ] K . Chatfield , V . Lempitsky , A . Vedaldi , and A . Zisserman . The devil is in the details : an evaluation of recent feature encoding methods . Proc . British Machine Vision Conference , 2011 .
[ 28 ] M . Maleko . A Jacobi algorithm for simultaneous diagonalization of several symmetric matrices . Department of Numerical Analysis and Computer Science , Royal Institute of Technology , Stockholm , 2003 .
[ 10 ] JV Davis , B . Kulis , P . Jain , S . Sra , and IS Dhillon . In Proc . Int’l Conf .
Information theoretic metric learning . Machine Learning , pages 209–216 , 2007 .
[ 11 ] IS Dhillon and JA Tropp . Matrix nearness problems with Bregman divergences . SIAM J . Matrix Analysis and Applications , 29(4):1120–1146 , 2007 .
[ 12 ] T . Dietterich . Ensemble methods in machine learning . Mul tiple Classifier Systems , pages 1–15 , 2000 .
[ 13 ] BK Flury . Two generalizations of the common principal component model . Biometrika , 74(1):59–69 , 1987 .
[ 14 ] BN Flury . Common principal components in k groups . J .
American Statistical Association , 79(388):892–898 , 1984 .
[ 15 ] K . Fukunaga . Introduction to Statistical Pattern Recognition .
Academic Press , 1990 .
[ 16 ] GH Golub and CF Van Loan . Matrix Computations , volume 3 . Johns Hopkins University Press , 1996 .
[ 17 ] X . He and P . Niyogi . Locality preserving projections . Proc . Advances in Neural Information Processing Systems , 2004 . [ 18 ] GE Hinton and RR Salakhutdinov . Reducing the diScience , mensionality of data with neural networks . 313(5786):504–507 , 2006 .
[ 19 ] A . Hyv¨arinen and E . Oja . Independent component analysis : algorithms and applications . Neural Networks , 13(4 5):411– 430 , 2000 .
[ 20 ] IT Jolliffe . Principal Component Analysis . Springer Verlag ,
1986 .
[ 21 ] N . Kwak . Principal component analysis based on L1 norm IEEE Trans . Pattern Analysis and Machine maximization . Intelligence , 30(9):1672–1680 , 2008 .
[ 22 ] Y . LeCun , L . Bottou , Y . Bengio , and P . Haffner . Gradientbased learning applied to document recognition . Proc . IEEE , 86(11):2278–2324 , 1998 .
[ 23 ] DD Lee and HS Seung . Learning the parts of objects by non negative matrix factorization . Nature , 401(6755):788– 791 , 1999 .
[ 24 ] C L Liu , F . Yin , D H Wang , and Q F Wang . CASIA online and offline Chinese handwriting databases . In Proc . Int’l Conf . Document Analysis and Recognition , pages 37–41 , 2011 .
[ 25 ] C L Liu and X D Zhou . Online Japanese character recognition using trajectory based normalization and direction feature extraction . Proc . Int’l Workshop Frontiers in Handwriting Recognition , 2006 .
[ 26 ] M . Loog and RPW Duin . Linear dimensionality reduction via a heteroscedastic extension of LDA : the Chernoff criterion . IEEE Trans . Pattern Analysis and Machine Intelligence , 26(6):732–739 , 2004 .
[ 27 ] M . Loog , RPW Duin , and R . Haeb Umbach . Multiclass linear dimension reduction by weighted pairwise Fisher criteria . IEEE Trans . Pattern Analysis and Machine Intelligence , 23(7):762–766 , 2001 .
[ 29 ] T . Mensink , J . Verbeek , F . Perronnin , and G . Csurka . Distance based image classification : Generalizing to new classes at near zero cost . IEEE Trans . Pattern Analysis and Machine Intelligence , 35(11):2624–2637 , 2013 .
[ 30 ] B . Moghaddam and A . Pentland . Probabilistic visual learning for object representation . IEEE Trans . Pattern Analysis and Machine Intelligence , 19(7):696–710 , 1997 .
[ 31 ] DA Reynolds , TF Quatieri , and RB Dunn .
Speaker verification using adapted gaussian mixture models . Digital Signal Processing , 10(1 3):19–41 , 2000 .
[ 32 ] ST Roweis and LK Saul . Nonlinear dimensionality reduction by locally linear embedding . Science , 290(5500):2323– 2326 , 2000 .
[ 33 ] B . Sch¨olkopf , A . Smola , and KR M¨uller . Nonlinear component analysis as a kernel eigenvalue problem . Neural Computation , 10(5):1299–1319 , 1998 .
[ 34 ] A . Stuhlsatz , J . Lippel , and T . Zielke . Feature extraction with deep neural networks by a generalized discriminant analysis . IEEE Trans . Neural Networks and Learning Systems , 23(4):596–608 , 2012 .
[ 35 ] D . Tao , X . Li , X . Wu , and SJ Maybank . Geometric mean IEEE Trans . Pattern Analysis and for subspace selection . Machine Intelligence , 31(2):260–274 , 2009 .
[ 36 ] JB Tenenbaum , V . De Silva , and JC Langford . A global geometric framework for nonlinear dimensionality reduction . Science , 290(5500):2319–2323 , 2000 .
[ 37 ] A . R . Webb . Statistical pattern recognition . New York , NY ,
USA : Wiley , 2002 .
[ 38 ] J . Yang , AF Frangi , J . Yang , D . Zhang , and Z . Jin . KPCA plus LDA : a complete kernel Fisher discriminant framework for feature extraction and recognition . IEEE Trans . Pattern Analysis and Machine Intelligence , 27(2):230–244 , 2005 .
[ 39 ] H . Yu and J . Yang . A direct LDA algorithm for highdimensional data with application to face recognition . Pattern Recognition , 34(10):2067–2070 , 2001 .
[ 40 ] X Y Zhang and C L Liu . Evaluation of weighted Fisher criteria for large category dimensionality reduction in application to Chinese handwriting recognition . Pattern Recognition , 46:2599–2611 , 2013 .
[ 41 ] X Y Zhang and C L Liu . Writer adaptation with style transfer mapping . IEEE Trans . Pattern Analysis and Machine Intelligence , 35(7):1773–1787 , 2013 .
[ 42 ] M . Zhu and AM Martinez . Subclass discriminant analysis . IEEE Trans . Pattern Analysis and Machine Intelligence , 28(8):1274–1286 , 2006 .
[ 43 ] A . Ziehe , P . Laskov , G . Nolte , and KR M¨uller . A fast algorithm for joint diagonalization with non orthogonal transformations and its application to blind source separation . J . Machine Learning Research , 5:777–800 , 2004 .
896
