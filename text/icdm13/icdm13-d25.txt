Non negative Multiple Tensor Factorization
Koh Takeuchi∗ , Ryota Tomioka† , Katsuhiko Ishiguro∗ , Akisato Kimura∗ , and Hiroshi Sawada∗
∗ NTT Communication Science Laboratories , Kyoto , Japan
{takeuchi.koh , ishiguro.katsuhiko , hiroshisawada}@labnttcojp , akisato@ieee.org
†Toyota Technological Institute at Chicago , Chicago , Illinois , USA tomioka@ttic.edu
Abstract—Non negative Tensor Factorization ( NTF ) is a widely used technique for decomposing a non negative value tensor into sparse and reasonably interpretable factors . However , NTF performs poorly when the tensor is extremely sparse , which is often the case with real world data and higher order tensors . In this paper , we propose Non negative Multiple Tensor Factorization ( NMTF ) , which factorizes the target tensor and auxiliary tensors simultaneously . Auxiliary data tensors compensate for the sparseness of the target data tensor . The factors of the auxiliary tensors also allow us to examine the target data from several different aspects . We experimentally confirm that NMTF performs better than NTF in terms of reconstructing the given data . Furthermore , we demonstrate that the proposed NMTF can successfully extract spatio temporal patterns of people ’s daily life such as leisure , drinking , and shopping activity by analyzing several tensors extracted from online review data sets .
I .
INTRODUCTION
As the amount and variety of available data has grown rapidly in recent years , there are many tensors consisting of only non negative data : eg ratings of users × shops × times , and social connections between users × friends × social networking systems . To analysing such tensor data , we can employ several techniques such as Non negative Tensor Factorization ( NTF ) [ 1 ] , which is a generalization of Non negative Matrix Factorization ( NMF ) [ 2 ] . NTF factorizes a target tensor consisting of non negative values into factor matrices under the non negative constraints . The non negativity constraints yield sparse and reasonably interpretable factorization results [ 3 ] . NTF has been applied and preforms well in various fields [ 4]– [ 7 ] . An advantage of tensor data over conventionally studied matrix data is its ability to represent observations with various attributes . In fact , tensor factorization techniques have been extended to collaborative filtering [ 8 ] and multi relational networks [ 9 ] and have been proven effective in these problems . In spite of these advantages , data sparseness and computational costs have significantly hampered the application of tensor factorization methods to real world problems . This problem becomes more serious as the order of the tensor becomes higher and degrades the predictive performance of NTF . The problem of sparseness has also been a serious problem in matrix factorization [ 10 ] . To deal with this problem , only a few solutions have been proposed in the context of multiple data analysis [ 11 ] , which have been developed based on probabilistic matrix factorization [ 12 ] . To compensate for the sparseness of the data matrix , these approaches augment the target matrix by incorporating auxiliary matrices in the analysis . Then the target matrix and the auxiliary matrices are simultaneously factorized . Factorizing multiple matrices simultaneously performs better than factorizing a single target matrix . Employing auxiliary data also seems a promising way to resolve the sparse issue for tensors . However , there have been almost no attempts to validate the idea of factorizing multiple tensors simultaneously . The only exceptions are described in [ 13 ] , [ 14 ] , but we cannot apply those methods to multimodal data sets including tensors of different sizes and scales : this is very typical in commercial purchasing records and social network data . tensor
In this paper , we propose a novel factorization method called Non negative Multiple Tensor Factorization ( NMTF ) , which naturally incorporates auxiliary data tensors into standard tensor factorization . The auxiliary tensor shares indices ( axes ) with the target tensors , thus NMTF is able to merge information from auxiliary tensors and mitigates the problem of sparseness . Furthermore , factorizing auxiliary tensors simultaneously allows us to examine given data from several different aspects , because we obtain factors from auxiliary tensors as well . We can control the influence of auxiliary tensors during the factorization by employing scaling parameters . We show that the simultaneous tensor decomposition approach can be reformulated into the decomposition of a larger ( partially observed ) tensor . NMTF is a generalization of NTF and is reduced to NTF in the special case where all scaling parameters are set at zero ( no influence ) . In this paper , we employ the generalized Kullback Leibler ( gKL ) divergence as the metric of NMTF , which is widely used in the context of NMF/NTF [ 15 ] . Minimizing gKL divergence is known to be equivalent to maximizing the Poisson distribution likelihood , which fits particularly well if the data are discrete values . We employ a three way tensor in this paper but the extension of NMTF to a N way tensor is straightforward .
Empirical results showed that NMTF achieved better performance than NTF in a quantitative way . The synthetic data experiments revealed that the performance improvement of NMTF compared with NTF become larger as the target tensor become sparser . We also found that NMTF factorization results are highly interpretable and suggestive for the analysis of real world complex review data . NMTF reveals the detailed user preferences of beers and spatio temporal patterns of people ’s daily life such as leisure , drinking , and shopping activity from online review data sets .
II . RELATED WORK
Non negative Tensor Factorization ( NTF ) was first proposed in [ 1 ] , as a generalization of Non negative Matrix factorization ( NMF ) [ 2 ] . NTF is based on a CANDECOMP/PARAFAC ( CP ) decomposition [ 16 ] and imposes nonnegative constraints on tensor and factor matrices . There have
Fig 1 . NTF factorizes a tensor X into factor matrices T , U , V been a lot of NTF researches concerning sparse constraints [ 6 ] , [ 17 ] and acceleration techniques [ 7 ] , [ 18 ] . As explained , data sparsity becomes a serious problem for large or high order tensors . However , there have been no study concerning the data sparsity in NTF .
As we explained in the introduction , combining multiple data matrices has proved effective in sparse matrix analysis [ 11 ] , [ 19]–[21 ] . Therefore combining multiple tensors is a reasonable idea for solving the sparsity problem . A concatenating method for a collection of tensors is proposed in [ 13 ] . A decomposing method for multiple tensors of exactly the same size simultaneously is proposed in [ 14 ] . To the best of our knowledge , there are currently no solutions to our sparse data tensor problem .
III . NON NEGATIVE TENSOR FACTORIZATION ( NTF ) Let us denote a three way tensor1 X = {xijk} ∈ RI×J×K with only non negative values xijk ≥ 0,∀ i , j , k . Figure 1 illustrates a concept of NTF . NTF factorizes a tensor X into three matrices each of which consists of R factors and only contains non negative values . We denote the three matrices as T = {tir} ∈ RI×R , U = {ujr} ∈ RJ×R and V = {vkr} ∈ RK×R . The r th column vectors of each matrix correspond to the r th factor of the tensor .
Let us denote the reconstructed tensors of X with T , U , V as ˆX = {ˆxijk} ∈ RI×J×K . We define the elements of ˆX as the sum of the linear products of the three matrices .
R r=1
ˆxijk = tirujrvkr .
( 1 )
I
J
K
Let us denote a divergence between two tensors as D . We employ an element wise divergence d to measure the divergence between X and ˆX . D(X| ˆX ) is written as :
D(X| ˆX ; Θ ) = d(xijk|ˆxijk ) ,
( 2 ) i=1 j=1 k=1 where Θ {U , T , V } . Using the above notations , NTF is formulated as follows :
D(X| ˆX ; Θ ) subject to T , U , V ≥ 0 .
( 3 ) min T,U,V
A . Generalized Kullback Leibler Divergence
There are a number of candidate divergences for NTF including Euclidean divergence , Itakura Saito divergence and generalized Kullback Leibler divergence . Although there is no
1In this paper , we denote a tensor by a bold face underlined capital letter , a matrix by a bold face capital letter , a vector by a bold letter and a scalar by a plain letter . definitive way of choosing the divergence . To this end , we are interested in discrete value observations such as stars in product reviews . For that purpose , we choose the generalized Kullback Leibler ( gKL ) divergence , which is formulated as follows : d(p|q ) = −p log q + q + p log p − p . It is known that minimizing the gKL divergence is equivalent to maximizing the log likelihood of the NTF model if we assume the observations are Poisson distributed , which is a natural choice for a discrete value observation [ 3 ] . Please note that our algorithm can be derived for other divergences in an analogous manner .
IV . PROPOSED METHOD
A . Non negative Multiple Tensor Factorization ( NMTF ) we propose Non negative Multiple Tensor Factorization ( NMTF ) , which effectively combines multiple data tensors under a non negative constraint . NMTF eases the problem of tensor sparsity and further allows us to examine given data from several different aspects . In NMTF , we have a target tensor and a few auxiliary tensors that share one or two indices ( axes ) with the target tensor . We want to factorize these tensors simultaneously in order to make use of available auxiliary information .
Let us denote the target tensor as Y ∈ RIy×Jy×Ky . We denote the n th auxiliary tensors as A(n ) ∈ RIn×Jn×Kn ( n = 1,··· , N ) . We set N = 3 in this paper for the convenience . We define the three factor matrices of Y as Θy = {Ty , Uy , Vy} where Ty ∈ RIy×R , Uy ∈ RJy×R , and Vy ∈ RKy×R . Let us define the factor matrices of the n th auxiliary tensor as Θ(n ) = {T ( n ) , U ( n ) , V ( n)} where T ( n ) ∈ RIn×R , U ( n ) ∈ RJn×R , and V ( n ) ∈ RKn×R . We define estimated ( reconstructed ) tensor values based on factor matrix elements as follows :
R
R
ˆyijk = iruy ty jrvy kr , ˆa(n ) ijk = t(n ) ir u(n ) jr v(n ) kr .
( 4 ) r=1 r=1
For simplicity , we assume that Y shares first indices with A(1 ) , second indices with A(2 ) , and third indices with A(3 ) , respectively . Note that it is straightforward to extend our results to cases where auxiliary tensors share two indices with the tensor . According to this assumption , we set I1 = target Iy , J2 = Jy and I3 = Ky . We set T ( 1 ) = Ty , U ( 2 ) = Uy and V ( 3 ) = Vy .
The goal of our NMTF is to factorize the target tensor Y with the auxiliary tensors A(n ) ( n = 1,··· , N ) simultaneously . To achieve this goal , we regard tensors Y and A(n ) ( n = 1,··· , N ) as parts of a larger tensor X ∈ R(Iy+I1+I2)×(J1+Jy+J3)×(Ky+K2+K1 ) , and perform NTF on this larger tensor ( see Fig 2 ) .
We augment the target tensor Y into X as follows . First , three axes are augmented by simply concatenating the indices of the target tensor and auxiliary tensors . For example , the size of the first axis is enlarged from I to Iy + I2 + I3 . Given a large empty tensor X , we place data tensors so as to match all indices as in Fig 2 . Please recall that Iy = I1 , Jy = J2 , and Ky = K3 . As evident from Fig 2 , there are several blocks with no observation , called undefined regions . To encompass
( 5 )
( 6 )
Fig 2 . Y , A(1 ) , A(2 ) and A(3 ) are included in X . Transparent elements of X are undefined regions . Factor matrices Uy , Ty , Vy are shared among the target tensor and additional tensors . these regions , we introduce the following binary variable ωijk : fl1 xijk is defined ,
0 xijk is undefined .
ωijk =
Let Ω ∈ RI×J×K be a tensor consisting of ωijk . We set undefined regions in X using Ω . Let us denote sets Ωy and Ω(n ) ( n = 1,··· , N ) consisting of indices of Y and A(n ) ( n = 1,··· , N ) .
1 if {i , j , k} ∈ Ωy
ωijk =
ˆη(n ) if {i , j , k} ∈ Ω(n ) ( n = 1,··· , N ) 0 otherwise . where η(n ) ≥ 0 ( n = 1,··· , N ) are scaling parameters of A(n ) ( n = 1,··· N ) . The elements ωijk are set at 0 if the indices do not indicate the observed values in Y or A(n ) ( n = 1,··· , N ) . Let us denote ˆD as the divergence of Non negative Tensor Factorization with an undefined region .
ˆD(X| ˆX ; Ω ) =
ωijkd(xijk|ˆxijk )
( 7 ) i=1
I  , U = j=1 k=1
K J  , V = U ( 1 )
Uy U ( 3 )
 Ty
T ( 2 ) T ( 3 )
T =
The reconstructed values of the elements ˆxijk are the same as Eq 1 . Then we set T , U , and V as ,
 .
 Vy
V ( 2 ) V ( 1 )
( 8 )
Therefore ˆD is rewritten as , ˆD(X| ˆX ; Ω )
Iy
Jy
Ky d(yijk|ˆyijk ) +
= i=1 j=1 k=1
= D(Y | ˆY ; Θy ) +
N
N
In
Jn
Kn n=1 i=1 j=1 k=1
η(n)d(a(n ) ijk|ˆa(n ) ijk )
D(η(n)A(n)|η(n ) ˆA
( n )
; Θ(n ) ) n=1
( n )
; ˆΘ , n = 1,··· , N ) ,
= D(Y , η(n)A(n)| ˆY , η(n ) ˆA ( 9 ) where we set factorized matrices as ˆΘ {Θy , Θ(n ) , n = 1 . . . , N} . NMTF minimizes the divergence between the given non negative tensors {Y , A(n ) ; n = 1,··· , N} and those estimated under the non negative constraints .
,
,
.
Note that if the scaling parameters η(1 ) = η(2 ) = η(3 ) = 0 , NMTF is reduced to the original NTF . And if the non negative constraints are removed , we can regard this model as a multiple tensor version of CP factorization .
B . Multiplicative Update Rules
In this section , we derive multiplicative update rules for ir = kr . We derive multiplicative kr , as below :
NMTF , similar to those of NTF . Remember that we set t(1 ) ir , u(2 ) ty update rules for ty kr = vy jr , and vy jr and v(3 ) jr = uy ir , uy kr jrvy uy yijk ˆyijk k=1 uy jrvy
( new ) =
Jy j=1
( new ) =
Iy i=1
( new ) =
Iy k=1 j=1
Ky Jy Ky Iy Jy Iy i=1
Ky
Ky
Jy k=1 i=1 i=1 j=1 ty ir ty ir uy jr uy jr vy kr vy kr kr irvy ty yijk ˆyijk k=1 ty irvy jr ty iruy yijk ˆyijk j=1 ty iruy j=1 j=1
+ η(1)J1 kr + η(1)J1
+ η(2)I2 kr + η(2)I2
+ η(3)J3 jr + η(3)I3 i=1 i=1 j=1 i=1
K1 K1 K2 K2 K3 J3 a(1 ) a(2 ) a(3 ) t(2 ) ir v(2 ) kr k=1 ijk ˆa(2 ) ijk ir v(2 ) k=1 t(2 ) kr t(3 ) ir u(3 ) jr k=1 ijk ˆa(3 ) ijk ir u(3 ) j=1 t(3 ) jr u(1 ) jr v(1 ) kr k=1 ijk ˆa(1 ) ijk k=1 u(1 ) jr v(1 ) kr
( 11 ) We can derive multiplicative update rules for other parameters t(n ) ir , u(n ) kr in a similar way . Note that this algorithm is stable . Let us focus on the update of ty ir is always non negative because all elements in the rhs of the update equations are non negative . ir . The updated ty jr , and v(n ) j=1
K1 ensure thatJy
Ky k=1 yi,j,k > 0 andJ1
The only concern regarding computational stability is the case where some u and v take the value 0 , then the rhs of the equation becomes 0 0 . But this will never happens if we handle the data and the model initialization appropriately . First , k=1 ai,j,k > 0 for all i . This is easily accomplished by excluding the empty index i that violates these conditions . Second , initialize the model so that ∀ ˆy > 0 and ∀ ˆa > 0 hold . If yi,j,k = 0 or ai,j,k = 0 , then we can simply skip the summation for the corresponding elements and speeding up computational times . Details of deriving update rules and a proof of convergence at local minima are to be published . j=1
C . NMTF as Probabilistic Generative Model
We mentioned that a generalized Kullback Leibler divergence is equal to a negative log likelihood of Poisson distribution . NMTF could be interpreted as a probabilistic generative model . Let us denote a Poisson distribution with a parameter ζ as p(ξ|ζ ) = Poisson(ξ|ζ ) . From the definition , a negative log likelihood of the Poisson distribution is approximately equal to a generalized Kullback Leibler divergence as below : − log p(ξ|ζ ) = dgKL(ξ|ζ ) + const . We denote a log likelihood of a tensor X as :
D(Y , η(n)A(n)| ˆY , η(n ) ˆA min subject to T , U , V ≥ 0 .
Θ
( n )
; ˆΘ , n = 1,··· , N )
( 10 ) log p(X| ˆX ) = log p(xijk|ˆxijk ) .
( 12 )
I
J
K i=1 j=1 k=1
Finally , the probabilistic generative model of NMTF can be written as : log p(X| ˆX ) = log p(Y | ˆY ) + log p(ˆη(n)A(n)|ˆη(n ) ˆA
( n )
)
N n=1 ( n )
= −D(Y , η(n)A(n)| ˆY , η(n ) ˆA
; ˆΘ , n = 1,··· , N ) + const . .
( 13 )
Eq ( 13 ) indicates that minimizing the divergence in NMTF to maximizing the
( Eq ( 10 ) ) is approximately equivalent Poisson log likelihood of the probabilistic model .
V . EXPERIMENTS
A . Evaluation Measure
In our experiments , almost all the tensor elements are equal to zero . Thus , we focus on the predictive log likelihood for the non zero elements to evaluate the performance of the factorization results . We split the elements of the target tensor for 5 fold cross validation . We employ the average log likelihood of nonM zero elements in the test sets . A higher average log likelihood results in the better modeling performance . We define the m=1 log p(xm|θ ) , where M average log likelihood as : is the number of non zero elements in the test sets and θ is the estimated parameter of a model .
1 M
B . Synthetic Data Experiment
In this experiment we evaluate the performance of NTF and NMTF in terms of the average test log likelihoods on a synthetic data set . Data sets are stochastically sampled from the probabilistic model . We set the sizes of the tensors at Iy = Jy = Ky = 100 and I ( n ) = J ( n ) = K ( n ) = 100 ( n = 1,··· , N ) , respectively . The number of factors is set at R = 5 , 10 , and 20 . We set the sparseness of the auxiliary tensors at 90 % ( ie , 90 % of all the tensor elements have zero values ) . We set the scaling parameters at η(1 ) = η(2 ) = η(3 ) = 1 .
In the first experiment , we evaluated the model perfortensor . We exammance on the sparseness of the target tensor was ined cases where the sparseness of the target 90 % , 99 % , 99.9 % and 9999 % In the second experiment , we also evaluated the effect of using auxiliary tensors . We examined NMTF with different numbers of auxiliary tensors , N . The scaling parameters were determined by cross validation . The sparseness of Y is set at 99 % . Note that NMTF reduces to NTF when no auxiliary tensors are available ( N = 0 ) .
Table I shows the results of the first experiment evaluating the effect of different data sparseness . The numbers of bases R was set at 5 , 10 , and 20 , respectively . The average log likelihoods for test sets worsens as the data sparseness increases . We confirmed that NMTF significantly outperforms the original NTF especially when the target tensor is extremely sparse . Table II also shows the results of the second experiment evaluating the effects of the number of available auxiliary tensors . It is evident that the average log likelihoods improve as the number of auxiliary tensors increase . This improvement indicates that NMTF successfully integrates the auxiliary tensor and target tensor information into factors .
TABLE I . SYNTHETIC DATA EXPERIMENT : THE AVERAGE TEST LOG LIKELIHOODS FOR DIFFERENT SPARSENESS OF THE TARGET TENSOR Y .
Sparseness 90 % 99 % 99.9 % 99.99 %
Sparseness 90 % 99 % 99.9 % 99.99 %
Sparseness 90 % 99 % 99.9 % 99.99 %
NTF
NMTF
−1.97 ± 0.02 −1.72 ± 0.01 −4.16 ± 0.28 −4.44 ± 0.19 −6.12 ± 0.59 −56.65 ± 49.61 −273.65 ± 237.86 −8.18 ± 3.80
( a ) R = 5
NTF
NMTF
−3.48 ± 0.06 −3.23 ± 0.05 −8.74 ± 0.38 −8.22 ± 0.22 −12.53 ± 0.75 −92.34 ± 23.32 −628.25 ± 514.51 −13.61 ± 5.24
( b ) R = 10
NTF
NMTF
−6.33 ± 0.06 −6.20 ± 0.05 −14.65 ± 0.12 −18.82 ± 0.48 −250.02 ± 43.43 −25.44 ± 1.25 −628.25 ± 514.51 −13.00 ± 4.92
( c ) R = 20
TABLE II . SYNTHETIC DATA EXPERIMENT : THE AVERAGE TEST LOG LIKELIHOODS OF THE TARGET TENSOR Y FOR DIFFERENT NUMBERS OF
AUXILIARY TENSORS .
Training set Y ( = NTF ) Y + A(1 )
Y +2 Y +3 n A(n ) n A(n )
NMTF ( R = 5 ) NMTF ( R = 20 ) −56.65 ± 49.61 −250.02 ± 43.43 −44.47 ± 2.91 −168.08 ± 7.16 −6.49 ± 0.52 −26.08 ± 1.79 −6.12 ± 0.59 −12.53 ± 0.75 −25.44 ± 1.25
NMTF ( R = 10 ) −92.34 ± 23.32 −77.10 ± 16.64 −13.17 ± 0.98
TABLE III .
REAL WORLD DATA SETS : THE AVERAGE TEST LOG
LIKELIHOODS ON THE TARGET TENSOR Y .
Yelp
MovieLens
NTF
R 10 −22.30 ± 0.39 −17.72 ± 0.32 −23.96 ± 0.09 −23.74 ± 0.12 20 −30.69 ± 0.69 −18.38 ± 0.30 −27.16 ± 0.30 −24.31 ± 0.12 50 −62.97 ± 1.94 −21.00 ± 0.37 −39.54 ± 0.44 −26.68 ± 0.37
NMTF
NMTF
NTF
C . Real Data Experiments
In this section , we evaluate NMTF and NTF with three public data sets provided by Yelp and MovieLens2 . These data sets include the user ’s reviews of , for example , places , and movies with various auxiliary data such as time stamp , geolocation , and check in counts .
The Yelp data set is a collection of real world reviews about numerous business places ( eg restaurants , department stores , etc ) in the greater Phoenix area , Arizona . We construct a target tensor Y containing 1 , 228 users , 1 , 860 business places , and 7 days of weeks . An element yijk represents the user ij who reviewed the business places jy on the day ky . We prepared two auxiliary tensors for the Yelp data set . The first auxiliary tensor A(1 ) consists of users 1 , 228 user , 235 business categories , and 92 , 052 words . An element x(1 ) ijk denotes the term frequency of the word k(1 ) about the business category j1 in reviews by user iy . The second auxiliary tensor A(2 ) contains 63 geolocation grids and 1 , 860 business places , and ( 24 ∗ 7 ) = 186 hours of weeks . We assume an approximately 10km × 10km geolocation grid . An element x(2 ) ijk represents the check ins count of at the business place jy within the geolocation grid i(2 ) on the hour time k(2 ) . We eliminate
2http://wwwyelpcom , http://wwwmovielensorg business places and users whose total numbers of reviews with fewer than 30 . Y is 99.9963 % sparse and A(1 ) is 99.9729 % sparse , and A(2 ) is 99.4265 % sparse . The numbers of bases are determined at R = 10 , 20 , and 50 in preliminary experiments . The MovieLens data set is a famous collection of commercial movie reviews . We construct a target tensor Y containing 101 , 970 movies , 2 , 113 users , and 590 weeks . The element yijk represents the rating user jy rating the movie iy posted in the week ky . An auxiliary tensor A(1 ) consists of 101 , 970 movies , 186 locations of movies , and 20 genres . An element x(1 ) ijk is set at 1 if a location j(1 ) is included in meta data about the shooting location of a movie iy , and also the genre k(1 ) is tagged to the movie . Y is 99.9933 % sparse and A(1 ) is 99.9320 % sparse . The numbers of bases are set at R = 10 , 20 , and 50 by preliminary experiments .
D . Results
1 ) Quantitative analysis : Before closely examining the learned bases from the real world data , we conduct numerical evaluations . The average test log likelihoods for the the Yelp data set and the MovieLens data set are shown in Table III . The scaling parameters are set at η1 = 0.1 , η2 = 0.001 on the Yelp data set and η1 = 0.01 on the MovieLens data set by 5fold cross validation . As with the synthetic data experiments , NMTF performed better than the original NTF with real world sparse tensor data .
3 the extracted
Fig
2 ) Qualitative analysis of Yelp data set : We constructed three tensors from the Yelp data set including an auxiliary tensor of geolocations and check in counts . The number of bases are set at R = 50 . show factors Uy , Vy , T ( 1 ) , V ( 1 ) , T ( 2 ) , and V ( 2 ) of bases learned from the Yelp data set . The colors in the figures corresponds to the colors of the factors in Fig 2 . The blue bars in the top left show the 10 highest values in Uy with corresponding the names of business places . The green and pink lines in the top right show the estimated check in counts per hour on each day of the week , namely V ( 2 ) . The green lines indicate weekday responses while the pink lines indicate those of weekends . The red bars placed in the middle left present the 10 highest values in T ( 1 ) with corresponding business categories . The red bars in the middle right show the 10 highest values in V ( 1 ) with corresponding words . The blue circles in the bottom left show the locations in Phoenix city , Arizona in Iy , and the circle size indicates the estimated values in Uy .
Fig 3a shows decomposed factors for a specific learned base ( r = 1 ) . Zoo , museum , and a few parks are selected as representative business places . The estimated check in counts hit peaks in the mornings on weekends , which is a reasonable pattern for these business places . Top categories and words are also easily interpretable in this base . Moreover , it is easy to see that the selected business places are located across the map . From this evidence , we can imagine that dining and drinking behavior on weekdays can be extracted in this basis .
Fig 3b shows another extracted basis ( r = 2 ) . Restaurants , bars and pubs are the top valued business places . The check in pattern is vary different from the previous basis : there are many check in counts on weekday evenings and weekend mornings . The selected categories and words match these business places . The business places are densely located in the central area of Phoenix . We see that the going out and drinking patterns of users on weekdays are mainly extracted in this factor .
Fig 3c shows yet another basis ( r = 3 ) . IKEA , Apple store , a toy store , a gardening store , and a pet shop achieve higher values for the place factor . Users checked into these business places most frequently at 5 pm on Sundays . The 10 highest categories and words include “ shopping ” , “ food ” , “ store ” , and “ IKEA ” . A place with a very large location weight is situated in the outer area of the city , but many other business places are located in the central downtown area . Our understanding of this basis is that it shows daily purchasing behavior downtown and occasional shopping on the periphery .
Finally , we present an interesting and completely different pattern in Fig 3d ( r = 4 ) . The listed business places serve typical Asian foods such as sushi and noodles . The check in counts do not very greatly between weekdays and weekends . The categories and words are also related to Asian foods such as “ Sushi Bar ” , “ Japanese ” , “ roll ” , and “ fish ” . The patterns of users enjoying these typical foods are extracted in this basis . NMTF revealed expressive patterns with geolocation and check in counts in this experiment .
VI . CONCLUSION
In this paper , we proposed a novel tensor factorization technique called Non negative Multiple Tensor Factorization ( NMTF ) . We formulated NMTF as a generalization of NTF and NMTF includes NTF as a special choice of scaling parameters . We adopted the generalized Kullback Leibler divergence as the distance metric between tensors and we derived a method for parameter estimation based on the multiplicative update rule . We evaluated NMTF and the original NTF on both synthetic and real world data sets . The performance of NMTF was quantitatively better than that of NTF . We also confirmed that NMTF successfully extracted informative and understandable factors from multiple tensors .
In this paper , we considered a case where each auxiliary tensor shares only one axis with the target tensor . One natural extension of this work is to allow the tensor to the share multiple axes . Decomposing different orders of tensors should allow us to model more complex and rich data set , for example , factorizing matrices and three way tensors or factorize more higher 4 or 5 way tensors simultaneously . Other choices of divergence such as Euclidean distance are also important topics to investigate . Finally , we remark on the choice of the number of factor bases R . Determining appropriate R is an unsolved issue in NTF including the proposed NMTF . Though there are no definitive ways , this is also an important task to investigate . Acknowledgment : This work was partially supported by
JSPS KAKENHI 25870192 .
REFERENCES
[ 1 ] A . Shashua and T . Hazan , “ Non negative tensor factorization with applications to statistics and computer vision , ” in Proc . ICML , 2005 .
[ 2 ] D . D . Lee and H . S . Seung , “ Learning the parts of objects by non negative matrix factorization , ” Nature , vol . 401 , pp . 788–791 , 1999 .
( a ) Amusement and park activity factor on weekend morning ( r=1 ) .
( b ) Bar and restaurant factor on weekday night and weekend morning ( r=2 ) .
( c ) Shopping at various stores factor on weekend evening ( r=3 ) . nonnegative tensor factorization algorithm for mining global climate data , ” in Proc . ICCS , 2009 .
[ 8 ] W . Chu and Z . Ghahramani , “ Probabilistic models for incomplete multi dimensional arrays , ” in Proc . AISTATS , 2009 .
[ 9 ] M . Nickel , V . Tresp , and H P Kriegel , “ A three way model for collective learning on multi relational data , ” in Proc . ICML , 2011 .
[ 10 ] Y . Koren , R . Bell , and C . Volinsky , “ Matrix factorization techniques for recommender systems , ” IEEE Computer Society , vol . 42 , no . 8 , pp . 30–37 , 2009 .
[ 11 ] H . Ma , H . Yang , M . R . Lyu , and I . King , “ SoRec : Social recommendation using probabilistic matrix factorization , ” in Proc . CIKM , 2008 . [ 12 ] R . Salakhutdinov and A . Mnih , “ Probabilistic matrix factorization , ” in
Proc . NIPS , 2008 .
[ 13 ] K . Y . Yılmaz , A . T . Cemgil , and U . Simsekli , “ Generalised coupled tensor factorisation , ” in Proc . NIPS , 2011 .
[ 14 ] T . Yokota , A . Cichocki , and Y . Yamashita , “ Linked PARAFAC/CP tensor decomposition and its fast implementation for multi block tensor analysis , ” in Proc . ICONIP , 2012 .
[ 15 ] S . Zafeiriou and M . Petrou , “ Nonnegative tensor factorization as an alternative Csiszar–Tusnady procedure : algorithms , convergence , probabilistic interpretations and novel probabilistic tensor latent variable analysis algorithms , ” Data Mining and Knowledge Discovery , vol . 22 , no . 3 , pp . 419–466 , 2011 .
[ 16 ] T . G . Kolda and B . W . Bader , “ Tensor decompositions and applications , ”
SIAM review , vol . 51 , no . 3 , pp . 455–500 , 2009 .
[ 17 ] A . Cichocki , R . Zdunek , S . Choi , R . Plemmons , and S . Amari , “ Novel multi layer non negative tensor factorization with sparsity constraints , ” in Proc . ICANNGA , 2007 . J . Antikainen , J . Havel , R . Josth , A . Herout , P . Zemcik , and M . HautaKasari , “ Nonnegative tensor factorization accelerated using GPGPU , ” IEEE Trans . , vol . 22 , no . 7 , pp . 1135–1141 , 2011 .
[ 18 ]
[ 19 ] S . K . Gupta , D . Phung , B . Adams , T . Tran , and S . Venkatesh , “ Nonnegative shared subspace learning and its application to social media retrieval , ” in Proc . SIGKDD , 2010 .
[ 20 ] G . Bouchard , S . Guo , and D . Yin , “ Convex collective matrix factoriza tion , ” in Proc . AISTATS , 2013 .
[ 21 ] K . Takeuchi , K . Ishiguro , A . Kimura , and H . Sawada , “ Non negative multiple matrix factorization , ” in Proc . IJCAI , 2013 .
( d ) Asian restaurant factor on both weekdays and weekends ( r=4 ) .
Fig 3 . Factors corresponds to four bases extracted from the Yelp data set .
[ 3 ] A . Cichocki , R . Zdunek , A . H . Phan , and S i Amari , Nonnegative matrix and tensor factorizations : applications to exploratory multi way data analysis and blind source separation . Wiley , 2009 .
[ 4 ] D . FitzGerald , M . Cranitch , and E . Coyle , “ Sound source separation using shifted non negative tensor factorisation , ” in Proc . ICASSP , 2006 . [ 5 ] T . Hazan , S . Polak , and A . Shashua , “ Sparse image coding using a 3D non negative tensor factorization , ” in Proc . ICCV , 2005 .
[ 6 ] E . C . Chi and T . G . Kolda , “ On tensors , sparsity , and nonnegative factorizations , ” SIAM Journal on Matrix Analysis and Applications , vol . 33 , no . 4 , pp . 1272–1299 , 2012 .
[ 7 ] Q . Zhang , M . W . Berry , B . T . Lamb , and T . Samuel , “ A parallel
0510152025Hour Value : K(2)weekdaysweekendsLocation Value : JyzooparkingtopwaterdogpeoplemountaintrailhikeparkWord Value : K(1)Four Peaks Brewing CoPapago ParkScottsdale Green BeltChildren's Museum Of PhoenixPinnacle PeakMcCormick StillmanRailroadParkPhoenix ZooPiestewa PeakSouth Mountain ParkCamelback MountainPlace Value : JyLocal FlavorDog ParksMuseumsArts & EntertainmentAmusement ParksZoosClimbingHikingParksActive LifeCaterogy Value : J(1)0510152025Hour Value : K(2)weekdaysweekendsLocation Value : JysandwichlunchpeoplehappyeatcheeseorderedsaladdrinksdeliciousWord Value : K(1)ScrambleDurant'sSideBarTuck ShopGreenThe TurfCiboSwitch Restaurant & Wine BarFEZHula's Modern TikiPlace Value : JyItalianLoungesBreakfast & BrunchWine BarsPizzaSandwichesAmerican ( New)NightlifeBarsRestaurantsCaterogy Value : J(1)0510152025Hour Value : K(2)weekdaysweekendsLocation Value : JypeoplebuydogitemsselectioncandystufffindikeastoreWord Value : K(1)Scottsdale Fashion SquareMojo YogurtFresh&EasyNeighborhoodMarketThe ParlorWag N' Wash Healthy Pet CenterPita JungleBaker NurserySmeeksApple StoreIKEAPlace Value : JyHome DecorPetsPetsFurniture StoresHome & GardenSpecialty FoodBeer , Wine & SpiritsGroceryFoodShoppingCaterogy Value : J(1)0510152025Hour Value : K(2)weekdaysweekendsLocation Value : JybarfreshorderedrestaurantfishhourhappyrollsrollsushiWord Value : K(1)Pure SushiPostino ArcadiaHiro SushiPearlSushiLounge&BomberBarGeisha A Go GoCherryBlossom Noodle CafeYasu Sushi BistroHana Japanese EaterySakana Sushi & GrillRoka AkorPlace Value : JySeafoodKaraokeItalianKoreanTea RoomsAsian FusionSteakhousesJapaneseRestaurantsSushi BarsCaterogy Value : J(1 )
