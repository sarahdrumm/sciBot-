Accelerating Active Learning with Transfer Learning
David Kale and Yan Liu
Department of Computer Science University of Southern California
Los Angeles , CA 90007
Email : {dkale,yanliucs}@uscedu
Abstract—Active learning , transfer learning , and related techniques are unified by a core theme : efficient and effective use of available data . Active learning offers scalable solutions for building effective supervised learning models while minimizing annotation effort . Transfer learning utilizes existing labeled data from one task to help learn related tasks for which limited labeled data are available . There has been limited research , however , on how to combine these two techniques . In this paper , we present a simple and principled transfer active learning framework that leverages pre existing labeled data from related tasks to improve the performance of an active learner . We derive an intuitive bound on the generalization error for the classifiers learned by this algorithm that provides insight into the algorithm ’s behavior and the problem in general . We provide experimental results using several well known transfer learning data sets that confirm our theoretical analysis . What is more , our results suggest that this approach represents a promising solution to a specific weakness of active learning algorithms : cold starts with zero labeled data .
I .
INTRODUCTION
In the age of the “ data tsunami , ” we are confronted with a central challenge : how do we efficiently and effectively learn from massive amounts of data ? Supervised learning remains the dominant learning paradigm for many practical problems , and many supervised learning problems can be formulated as classification . Learning a classifier requires class labels , which can be difficult or expensive to acquire in large quantities . In response to this dilemma , researchers have developed active learning . An active learner is given access to an ( often human ) oracle that can label data , a limited budget to spend on acquiring labels , and the freedom to choose which observations to label [ 1 ] . The goal of active learning is to build an effective classifier with as few label queries as possible .
Recent theoretical breakthroughs have produced active learning algorithms that are practical and have strong statistical consistency and unbiased sampling guarantees [ 2 ] . Nevertheless , there remain significant barriers to wider adoption of active learning . One challenge that has both practical and theoretical implications is the cold start phenonemon . Active learning requires a good classifier to generate useful label queries ; training a good classifier requires labeled data . If the active learner begins with zero labeled data , then it must query labels at random until it has enough to train a good classifier . Thus , early in the query process or when the labeling budget is small , active learning offers little or no advantage over passive learning [ 3 ] . What is more , classifier performance ( eg , test set error ) often improves slowly as a function of the number of label queries . The cold start problem has not been studied in earnest , although a number of approaches ( eg , cluster based active learning ) offer potential remedies [ 4 ] .
Another promising solution to the cold start problem is transfer learning . The intuition behind transfer learning is that learning a new task should be easier if we transfer knowledge from previously learned tasks [ 5 ] . Related ( or source ) tasks often take the form of labeled data sets that are “ similar ” to our target task data . Examples include product reviews from different categories [ 6 ] or clinical trials data from different hospitals [ 7 ] . In these settings , straightforward supervised learning ( train a model on source data , then apply it to target task ) often produces models that perform poorly . However , with a proper transfer learning framework , we can use source data to improve our ability to learn the new task , especially when little or no labeled target data is available .
This suggests a strategy for addressing the cold start problem in active learning : use transfer learning to initialize the active learner using data from a related task . In this way , the active learner begins with a classifier to guide early label queries , eliminating the need to query at random . If the transfer from the source task is effective , then the active learner should begin with a good classifier and require many fewer target label queries to improve it . This would mitigate the cold start problem . If transfer learning produces a poor classifier , the active learner may be forced to query many more target labels in order to recover . In this way , we can understand transfer learning as providing an initial bias to the active learner . A good framework for combining transfer and active learning should provide a way to measure the impact of the transferbased bias on the active learner ’s behavior and performance . In this paper , we describe a simple , principled approach to transfer initialized active learning , based on two relatively new frameworks for transfer learning [ 8 ] and active learning [ 2 ] . This approach is easy to implement and efficient , and it permits a theoretical analysis that provides insight into the interaction between these two learning paradigms . We derive a bound on the generalization error that relates target task performance to the similarity between source and target tasks . We identify a trade off between potential sources of error that can be exploited to produce effective transfer active learners . We present experimental results that confirm our theory and show that this approach accelerates active learning . We conclude by identifying the most fruitful directions for future research .
II . RELATED WORK
To our knowledge , there has been only a handful of papers , most of them quite recent , exploring the combination of transfer learning and active learning [ 9 ] , [ 10 ] . [ 11 ] combines uncertainty region sampling with several transfer learning concepts , including the use of a domain separation classifier trained to distinguish between unlabeled source and target samples . The authors provide convincing empirical results on a number of standard transfer learning tasks , as well as a simple analysis of label complexity and error rates . [ 12 ] describes a novel active transfer learning framework that combines sample reweighting with batch mode active learning , which chooses all of its label queries simultaneously . What makes this approach especially interesting is that it uses a different set of criteria to select queries : diversity among labeled samples and distributional similarity between labeled and unlabeled target data . Their empirical results indicate that this approach can be used to build effective classifiers with a small number of target label queries . Unfortunately , most of these approaches are heuristic in nature and lack guarantees for consistency and sampling bias . The most notable exception is [ 7 ] , which presents a theoretically rigorous Bayesian framework for active transfer learning , based on prior dependent learning . Assuming a prior distribution over target concepts ( ie , classifiers ) greatly accelerates active learning , and the authors show that the prior is identifiable from a finite number of labeled examples in sequential multitask settings . The empirical effectiveness of this approach remains an open question .
III . METHODOLOGY
Our approach to transfer active learning combines two principled learning frameworks . For transfer learning , we use a convex combination of source and target empirical risks [ 8 ] . For active learning , we use the importance weighted consistent active learner ( IWAL CAL ) algorithm [ 2 ] . We provide a brief overview of each and then describe how to combine them in order to address the cold start problem . i=1
A . Transfer learning framework Formally , we define a task or domain as a distribution D on a set of points X paired with a labeling function f : X → Y , where Y = {±1} . In transfer learning , we seek to transfer knowledge from a source domain DS , fS to a target domain DT , fT of interest . When learning we search over a hypothesis space H for a function h : X → Y that does a good job of predicting the true f ( x ) for any point x ∈ X . We measure the quality of a hypothesis h by its risk , relative to a domain ( eg , the target domain T ) : T(h , fT ) = Ex∼DT [ 1{h(x ) = fT(x)} ] where 1 is the indicator function . The empirical risk of a hypothesis , relative to a finite sample {x1 , . . . , xn} , is defined 1{h(xi ) = fT(xi)} where as ˆ T(h , fT , ( x)1:n ) = 1 n ( x)1:n is a notational convenience . When it is clear from context , we will use shorthand , such as T(h ) and ˆ T(h , n ) . Our goal is to choose a hypothesis to minimize the target risk ( h∗ = arg minh∈H T(h) ) , though this is impossible in practice . Instead we minimize a weighted sum of empirical risks ˆ α(h ) = αˆ T(h , f , ( x)1:n)+(1−α)ˆ S(h , f , ( x)1:m ) with scalar weight α ∈ [ 0 , 1 ] . We assume access to m > 0 labeled source examples and n ≥ 0 labeled target examples . n
This approach to transfer learning is attractive because of its simplicity and elegant theoretical properties . [ 8 ] derives an upper bound on the target generalization error of ˆh = arg minh∈H ˆ α(h ) , the classifier that minimizes the combined empirical risk . This bound includes two particularly interesting terms that quantify the similarity between domains . The first is a hypothesis dependent measure of the similarity between the source and target data distributions DS and DT . Even if the domains share the same labeling function ( ie , fS = fT ) , training examples with different distributions may produce different classifiers . We define the dH distance between two distributions : dH(DS,DT ) = 2 sup h∈H
|PDS{Ah} − PDT{Ah}| where Ah = {x : x ∈ X where h(x ) = +1} . This is the maximum possible difference between probability masses assigned by our domains to a set Ah of points classified as +1 by any hypothesis h ∈ H . Now let H∆H = {g : g(x ) = +1 if h(x ) = h(x ) for given h , h ∈ H} be the symmetric difference hypothesis space . Additionally , let S(h , h ) be the disagreement between two hypotheses h , h ∈ H about the labels of points drawn from DS ( likewise for T and DT ) . Then we can define a distance dH∆H for which the following inequality holds for all h , h ∈ H : | S(h , h ) − T(h , h)| ≤ 1 2 dH∆H(DS,DT )
This distance places an upper bound on the difference between source label and target label disagreement between any two hypotheses h , h ∈ H . dH∆H has two useful properties : first , for any H with finite VC dimension , it can be computed from finite unlabeled samples US ∼ DS and UT ∼ DT [ 13 ] . Second , it can be approximated using a domain separator hypothesis , ie , a classifier from H trained to separate US and UT [ 14 ] . The second term of interest is the combined source and target risk : ∗ ST = minh∈H S(h ) + T(h ) . This can be thought of as a general measure of the similarity between the source and target domains . A small ∗ ST implies the existence a hypothesis h ∈ H that simultaneously minimizes source and target risk , which in turn implies minimal differences between data distributions and labeling functions . This corresponds to the traditional transfer learning assumption that domains are “ sufficiently similar ” [ 8 ] . We assume that ∗ ST is negligible but acknowledge that this may not be true in real applications . B . Active learning framework t i=1 wi1{h(xi ) = f ( xi)} . [ 2 ] proves that
IWAL CAL is an importance weighted mellow active learner designed for online learning settings : rather than choosing from a pool , it waits as points “ arrive ” in streaming fashion and queries each label with some probability . When a point ’s label is queried , it is assigned an importance weight inversely proportional to its query probability . Importance weights correct for bias that accrues during selective sampling . After seeing t points , we choose a classifier ¯ht to minimize the importance weighted empirical risk ¯ ( h , t ) = ¯ ( h , f , ( x , w)1:t ) = this is an 1 t unbiased estimate of the true risk and provides a nice deviation bound for it . When compared with aggressive active learners ( eg , uncertainty region sampling ) , mellow active learners often exhibit a slower rate of improvement in performance as a function of the number of queries . However , they have sounder theoretical properties and are more conducive to analysis [ 3 ] . Formally , for the tth unlabeled point xt , IWAL CAL queries the label yt = f ( xt ) with probability pt computed using a rejection threshold function p((x , q , w)1:t−1 , xt ) . Here qi is a binary indicator of whether the ith label was queried , and wi = 1/qi is the importance weight ( bounded from above since pi > 0 ) . ( x , q , w)1:t is a notational convenience for {(x1 , q1 , w1 ) , . . . , ( xt , qt , wt)} . We can now redefine ¯ over all points seen : ¯ ( h , t ) = ¯ ( h , f , ( x , q , p)1:t ) = 1{h(xi ) = f ( xi)} . Unlabeled points have qi = 0 1 t and so are ignored in the error . This risk estimator is unbiased ; notice that E [ Qi/pi ] = E [ Qi ] /pi = pi/pi = 1 [ 2 ] . t qi pi i=1
After seeing t − 1 samples , IWAL CAL uses them to implicitly maintain a space Ht−1 of candidate hypotheses that with high probability contains h∗ , the optimal classifier in H . The probability pt of querying the label for xt is inversely proportional to the level of disagreement in Ht−1 . The difference between the importance weighted empirical risk ¯ ( h , t−1 ) and the true risk ( h ) is bounded , giving us a method to compute pt . Let ¯ht−1 = arg minh∈H ¯ ( h , t − 1 ) be the hypothesis that minimizes the importance weighted empirical error . Next let ¯h t−1 be the hypothesis that minimizes this error t−1(xt ) = ¯ht−1(xt ) . but disagrees with ¯ht−1 on xt ’s label : ¯h Then Gt = ¯ ( ¯ht−1 , t − 1 ) − ¯ ( ¯h t−1 , t − 1 ) is an estimate of the disagreement within Ht−t about the label of xt . If Gt exceeds the upper bound on disagreement , then ¯ht−1 likely agrees with h∗ on xt and so it is probably unnecessary to query xt ’s label . Thus , the label for xt is queried with where C0 = O(log(|H|/δ ) ) [ 2 ] . C . Transfer active learning probability pt ≈ min(1,,1/G2
C0 log(t)/(t − 1 ) ) t + 1/Gt
We now assume that an IWAL CAL active learner has access to m labeled points from the source domain and has seen t points from the target domain . We can define a new weighted empirical risk over these m + t points , where the weights depend on m and t , the α parameter , and the IWAL CAL importance weights qi/pi : Definition 1 . We define a combined weighted empirical risk for transfer accelerated active learning as
¯ α(h , m , t ) α¯ T(h ) + ( 1 − α)ˆ S(h )
¯ α(h , m , t ) 1 m + t wi1{h(xi ) = f ( xi)} m+t i=1 or equivalently where wi =
 ( 1−α)(m+t )
α(m+t ) m tpi 0 i ≤ m i > m , qi = 1 i > m , qi = 0
( source ) ( labeled target ) ( unlabeled target )
These can be shown to be equivalent with a simple derivation . The first form is easier to analyze , allowing us to leverage the results from [ 2 ] for IWAL CAL and from [ 8 ] for transfer learning . The second form is easier to implement as it permits us to use any supervised learning routine that accepts individually weighted training data . Algorithm 1 shows pseudocode for our Transfer IWAL CAL ( TIWAL CAL ) algorithm . It uses the combined weighted empirical risk in Steps 36 of the algorithm . Gboundt = α + ( 1 − α ) is the upper bound on the disagreement within Ht−1 . To obtain pt , we solve the quadratic equation − c1 + 1 Gt = α t−1 pt +(1−α ) 2m . The constants C0 , c1 , and c2 can be treated as tunable parameters but are defined for analysis as follows : C0 = O(log(|H|/δ ) , c1 = 5+2 2 , and c2 = 5 . This algorithm uses labeled source data to provide the active learner with a transfer based bias that can improved by labeling target data . In the next section , we show that TIWAL CAL ’s behavior and performance depend upon the similarity between source and target domains and the value of α .
C0 log t c2
C0 log 2 c1√ C0 log 2
C0 log t
C0 log t t−1 + C0 log t t−1
− c2 + 1 t−1 +α
√
2m pt
Algorithm 1 Transfer IWAL CAL 1 : for t = 1 , 2 , . . . until target samples exhausted do 2 : 3 : 4 :
Receive unlabeled xt Compute weights ( w)1:(t−1 ) as in Definition 1 . Choose ¯ht−1 = arg min Choose ¯h t−1 = Set Gt = ¯ α(¯h if Gt ≤ Gboundt then else h∈H:h(xt)=¯ht−1(xt ) t−1 , t − 1 ) − ¯ α(¯ht−1 , t − 1 )
¯ α(h , t − 1 )
¯ α(h , t − 1 ) arg min h∈H
5 :
Solve for pt ( see below ) end if Sample qt ∼ Bernoulli(pt ) if qt = 1 then
Set pt = 1
6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : 17 : end for 18 : return ¯ht = arg min
Query label yt Set wt = 1/pt end if h∈H
¯ α(h , t ) , ( x , y , q , 1/p)1:t
D . Deviation and generalization bounds
We provide two useful bounds that codify the above intuition and guide the application of our algorithm . Lemma 1 places an upper bound on the deviation of the combined weighted empirical risk ¯ α(h , m , t ) from the true combined risk α(h , m , t ) . This directly motivates Steps 6 10 of Algorithm 1 , in which we compute Gboundt and pt and decide whether to query xt ’s label . Lemma 1 . With probability at least 1− δ , the following holds for all t ≥ 1 and all h ∈ H :
|(¯ α(h , m , t ) −¯ α(h∗
εt
α))| α , m , t ) ) − ( α(h ) − α(h∗ √ + ( 1 − α )
εt
≤ α
+ pmin,t(h ) pmin,t(h )
εS where εt = O ( log(t|H|/δ)/t ) , εS = O ( log(2|H|/δ)/(2m) ) , and pmin,t(h ) is the minimum query probability assigned to a target point about whose label h and h∗ disagree . The proof of this lemma involves decomposition of the combined empirical risk , followed by an application of the triangle inequality and Hoeffding ’s inequality.1 Theorem 1 places an upper bound on the target risk of ¯ht . Theorem 1 . For ¯ht = arg minh∈H ¯ α(h , m , t ) , the following holds with probability at least 1 − δ :
T(¯ht ) ≤ T(h∗
T ) + α
2C0 log(t + 1 ) t
+
2C0 log(t + 1 ) t
+ 2(1 − α )
C0 log 2
2m
+
1 2 dH∆H(DS,DT ) + ∗
ST
The proof proceeds along lines similar to that of Theorem 2 from the appendix of [ 8].1
1 Complete proofs are available at http://www scfuscedu/˜dkale/ active transfer/ .
Implication : This is an interesting and intuitive bound that trades off two sources of error via the parameter α . The active learning error term ( with coefficient α ) decreases as t grows large but may be significant early on . The transfer learning error term ( with coefficient 1 − α ) depends primarily upon the number of source points m and upon the dissimilarity of our source and target domains , which is determined primarily by the dH∆H distance since we assume ∗ ST is small enough to be ignored . The transfer learning error can be viewed as constant ; it does not depend directly on t or change as we query labels . If the domains are substantially different and transfer learning error is high , an injudicious choice of α can introduce a large and constant negative bias , from which the active learner may never recover and which may increase the overall number of queries made . If the domains are sufficiently similar and transfer error is small , then a careful choice of α should significantly improve performance early on in the query process and reduce the overall number of queries . [ 8 ] gives a detailed analysis of choices of α that also applies here . A rule of thumb is that lower values of α should work well when the domains are similar and we have a lot of source data . Otherwise , we should use a higher value of α or even consider plain active learning .
IV . EXPERIMENTS
We compare IWAL CAL and TIWAL CAL using two publicly available transfer learning data sets.2 For each , we choose a target domain and divide it into a test set and two training sets . The first is treated as unlabeled to start and is used for active learning . The second is treated as a labeled source domain . We also choose two additional labeled source domains . We then compare the test set error and query rates of TIWAL CAL against IWAL CAL .
The details of our data sets are shown in the table in Figure 3 . These include approximate ˆdH∆H distances using a linear domain separator and hinge loss . Our base learner is a linear model with hinge loss and L2 regularization . For the free parameters in TIWAL CAL , we follow [ 2 ] by setting c1 = c2 = 1 and dropping the log(t ) terms when computing Gboundt and pt . We use a heuristic to learn the constrained hypothesis ¯h t : set the instance weight for xt to be equal to the sum of the weights for the rest of the training data . The above changes result in an approximation to our the abstract algorithm that works in practice . We use C0 = 0.25 for IWAL CAL and 25 ≤ C0 ≤ 100 for TIWAL CAL . A . Data classification
20 Newsgroups : Our first
20 Newsgroups data set . We create a handful of “ category versus target subset of recsportbaseball vs . domain is talkpoliticsmisc ( BvP ) . Our domains include a second subset of recsportbaseball vs . talkpoliticsmisc ( BvP2 ) , recsporthockey vs . talkreligionmisc ( HvR ) , and rec.autos vs . socreligionchristian ( AvC ) . The original 20 Newsgroups data has 61,188 word counts , which we convert to log term frequency . We then reduce the number of features by keeping only the 250 words with the top term frequency inverse document frequency ( TF IDF ) scores category ” source tasks . data set is the
Our across all categories . This is an efficient way to choose a small number of interesting features without using labels [ 15 ] . It also changed the ˆdH∆H distance between domains , making for interesting experiments .
Sentiment : Our second data set is the sentiment classification data set [ 6 ] . We use the preprocessed binary ( “ positive ” vs . “ negative ” review ) version , with a subset of kitchen as our target domain and a second subset of kitchen ( kitchen2 ) , dvd , and electronics as our source domains . The preprocessed version of the data includes 1,110,352 unigram and bigram count features . As with 20 Newsgroups , we convert these to log counts and then keep only the 1000 features with the top TF IDF scores across all domains . B . Results
Typical results are shown in Figures 1 and 2 . The first thing to observe is that basic IWAL CAL falls prey to the cold start phenomenon . For sentiment , IWAL CAL requires nearly 400 queries to reach error of 0.20 or less and thousands of queries before it reaches the same performance as fully supervised learning . On the easier 20 Newsgroups data set , IWAL CAL still needs 200 queries to achieve the supervisedlevel performance ( error of 010 ) The results for TIWAL CAL are consistent with our analysis : the transfer learning bias drastically improves test set error early in the query process ( with early error rates near optimal ) and reduces the overall number of queries by as much as 50 % . Further , TIWAL CAL often converges to nearly the same error rate as IWAL CAL , suggesting little or no negative bias . The exceptions to this pattern are the AvC and dvd source domains . Both yield less early improvement in test set error , and dvd actually increases the overall number of queries . These results are explained by our theory : each has a relatively high ˆdH∆H distance with its respective target domain . Nonetheless , even for these sources , there is still an early 30 40 % reduction in error , while the “ penalty ” on future test set error is relatively small ( TIWAL CAL converges to a 5 10 % higher error ) .
V . DISCUSSION
Researchers are increasingly interested in how to introduce useful biases into active learning without compromising consistency guarantees.3 This will allow active learners to produce good classifiers faster , mitigating the cold start problem . In this paper , we presented a principled framework that addresses cold starts by using transfer learning to leverage data from related tasks . Our framework is straightforward to analyze and apply . We proved a generalization bound that provides intuition into the problem and helps trade off different sources of error . We demonstrated empirical results that suggest this approach significantly improves classifier performance early in the query process and reduces the overall number of target label queries . In other words , we can accelerate active learning with transfer learning . Our work establishes a sound foundation that will facilitate future research on this topic and empower practitioners to apply these ideas to real world problems .
Our empirical results are modest ; their primary virtue is consistency with our theoretical analysis . Clearly further experimentation and evaluation are warranted . In particular , it sometimes appears as though transfer learning is doing most of the work and that active learning plays little role other than to a
2 Code and data to reproduce our experiments can be found at https://githubcom/uscmelady/active transfer
3See “ The End of the Beginning of Active Learning ” by Daniel Hsu and
John Langford at http://hunchnet/?p=1800
Fig 1 . 20 Newsgroups results . The lefthand plots show test set error versus number of points seen by the active learner ( the bottom is zoomed ) . The upper right shows test set error versus number of queries . The bottom right plot shows the query rate ( number of queries versus number of points seen ) . manage the label query budget . This is due to the conservative nature of IWAL CAL ; the results in [ 2 ] are similarly modest . The natural question is whether this framework can be used with more aggressive approaches to active learning , such as uncertainty region sampling , while avoiding the usual sampling bias problems . We are pursuing this line of work .
One unsatisfying property of our framework is the persistent nature of the transfer based bias . This introduces a constant source of error into our generalization bound and may prevent us from learning an optimal classifier , even with a large number of labeled target examples . Intuitively , with enough target data , we should de emphasize the source data when training classifiers . One simple strategy that we are investigating is to gradually increase α ( the weight on target risk ) as we query more target labels . Another promising approach would be to combine active learning with an adaptive transfer learning framework that re weights or transforms the source data to reduce the difference between domains [ 16 ] .
ACKNOWLEDGMENTS
We would like to thank Sanjoy Dasgupta , Byron Wallace , and Taha Bahadori for many illuminating discussions and for their insightful feedback . This work was partially supported by the US Defense Advanced Research Projects Agency ( DARPA ) under the Social Media in Strategic Com munication ( SMISC ) program , Agreement Number W911NF 121 0034 and a Yahoo! Faculty Award .
REFERENCES
[ 1 ] B . Settles , Active Learning , ser . Synthesis Lectures on Artificial Intelligence and Machine Learning Series . Morgan & Claypool Publishers , Jun . 2012 , vol . 6 , no . 1 .
[ 2 ] A . Beygelzimer , J . Langford , D . Hsu , and Z . Tong , “ Agnostic Active Learning Without Constraints , ” in Advances in Neural Information Processing Systems 23 , 2011 , pp . 199–207 .
[ 3 ] S . Dasgupta , “ Two Faces of Active Learning , ” Theoretical Computer
Science , vol . 412 , no . 19 , pp . 1767–1781 , 2011 .
[ 4 ] X . Zhu , J . Lafferty , and Z . Ghahramani , “ Combining Active Learning and Semi Supervised Learning Using Gaussian Fields and Harmonic Functions , ” in ICML 2003 Workshop on The Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining , 2003 , pp . 58–65 .
[ 5 ] S . Thrun , “ Is Learning the Nth Thing Any Easier Than Learning The First ? ” in Advances in Neural Information Processing Systems 8 , 1996 , pp . 640–646 .
0100200300400500600700# unlabeled data points005010015020025030035040045050Test set error20 Newsgroups ( BvP ) : Test set error vs . unlabeled dataIWAL CALT . IWAL CAL : AvC ( dH∆H=04368)T IWAL CAL : HvR ( dH∆H=03072)T IWAL CAL : BvP2 ( dH∆H=0.0981)Fully supervised050100150200250300# label queries005010015020025030035040045050Test set error20 Newsgroups ( BvP ) : Test set error vs . label queriesIWAL CALT . IWAL CAL : AvC ( dH∆H=04368)T IWAL CAL : HvR ( dH∆H=03072)T IWAL CAL : BvP2 ( dH∆H=0.0981)Fully supervised050100150200# unlabeled data points005010015020025030035040045050Test set error20 Newsgroups ( BvP ) : Test set error vs . unlabeled data ( zoomed)IWAL CALT . IWAL CAL : AvC ( dH∆H=04368)T IWAL CAL : HvR ( dH∆H=03072)T IWAL CAL : BvP2 ( dH∆H=0.0981)Fully supervised0100200300400500600700# unlabeled data points050100150200250300# label queries20 Newsgroups ( BvP ) : Label queries vs . unlabeled dataIWAL CALT . IWAL CAL : AvC ( dH∆H=04368)T IWAL CAL : HvR ( dH∆H=03072)T IWAL CAL : BvP2 ( dH∆H=0.0981 ) Fig 2 . sentiment results . The lefthand plots show test set error versus number of points seen by the active learner ( the bottom is zoomed ) . The upper right shows test set error versus number of queries . The bottom right plot shows the query rate ( number of queries versus number of points seen ) .
Source BvP2 HvR AvC kitchen2 electronics dvd
Target m 360 BvP 974 BvP 1191 BvP 1001 kitchen 5760 kitchen 4189 kitchen
ˆdH∆H 0.0981 0.3072 0.4368 0.1521 0.2573 0.6659
α 0.3 0.6 0.9 0.3 0.3 0.9
Fig 3 . examples and approximate dH∆H distances , and values of α .
Summary of experiments , including number of labeled source
[ 6 ]
J . Blitzer , M . Dredze , and F . Pereira , “ Biographies , Bollywood , Boomboxes and Blenders : Domain Adaptation for Sentiment Classification , ” in Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics , 2007 .
[ 7 ] L . Yang , S . Hanneke , and J . Carbonell , “ Two Faces of Active Learning , ”
Machine Learning , vol . 90 , no . 2 , pp . 1–28 , 2012 . J . Blitzer , K . Crammer , A . Kulesza , F . Pereira , and J . Wortman , “ Learning Bounds for Domain Adaptation , ” in Advances in Neural Information Processing Systems 21 , 2008 , pp . 1–12 .
[ 8 ]
[ 9 ] Y . S . Chan and H . T . Ng , “ Domain Adaptation with Active Learning for Word Sense Disambiguation , ” in Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics , 2007 .
[ 10 ] X . Shi , W . Fan , and J . Ren , “ Actively Transfer Domain Knowledge , ” in Proceedings of the 2008 European Conference on Machine Learning and Knowledge Discovery in Databases , 2008 , pp . 342–357 .
[ 11 ] A . Saha , P . Rai , H . Daum´e , S . Venkatasubramanian , and S . L . DuVall , “ Active Supervised Domain Adaptation , ” in Proceedings of the 2011 European Conference on Machine Learning and Knowledge Discovery in Databases , 2011 , pp . 97–112 .
[ 12 ] R . Chattopadhyay , S . M . Ave , W . Fan , and I . Davidson , “ Joint Transfer and Batch mode Active Learning , ” in Proceedings of the 30th Annual International Conference on Machine Learning , 2013 .
[ 13 ] D . Kifer , S . Ben David , and J . Gehrke , “ Detecting Change in Data Streams , ” in Proceedings of the 30th international Conference on Very Large Databases , 2004 , pp . 180–191 .
[ 14 ] S . Ben David , J . Blitzer , K . Crammer , and O . Pereira , “ Analysis of Representations for Domain Adaptation , ” in Advances in Neural Information Processing Systems 19 , 2007 , pp . 137–144 .
[ 15 ] Y . Yang and J . O . Pedersen , “ A comparative study on feature selection in text categorization , ” in Proceedings of the 14th International Conference on Machine Learning , 1997 , pp . 412–420 .
[ 16 ] M . T . Bahadori , Y . Liu , and D . Zhang , “ Learning with Minimum Supervision : A General Framework for Transductive Transfer Learning , ” in Proceedings of the 11th IEEE International Conference on Data Mining , 2011 , pp . 61–70 .
05001000150020002500# unlabeled data points010015020025030035040045050Test set errorSentiment ( kitchen ) : Test set error vs . unlabeled dataIWAL CALT . IWAL CAL : dvd ( dH∆H=06659)T IWAL CAL : electronic ( dH∆H=02573)T IWAL CAL : kitchen2 ( dH∆H=0.1521)Fully supervised0200400600800100012001400# label queries010015020025030035040045050Test set errorSentiment ( kitchen ) : Test set error vs . label queriesIWAL CALT . IWAL CAL : dvd ( dH∆H=06659)T IWAL CAL : electronic ( dH∆H=02573)T IWAL CAL : kitchen2 ( dH∆H=0.1521)Fully supervised0100200300400500# unlabeled data points010015020025030035040045050Test set errorSentiment ( kitchen ) : Test set error vs . unlabeled data ( zoomed)IWAL CALT . IWAL CAL : dvd ( dH∆H=06659)T IWAL CAL : electronic ( dH∆H=02573)T IWAL CAL : kitchen2 ( dH∆H=0.1521)Fully supervised010002000300040005000# unlabeled data points0200400600800100012001400# label queriesSentiment ( kitchen ) : Label queries vs . unlabeled dataIWAL CALT . IWAL CAL : dvd ( dH∆H=06659)T IWAL CAL : electronic ( dH∆H=02573)T IWAL CAL : kitchen2 ( dH∆H=0.1521 )
