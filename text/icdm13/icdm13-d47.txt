2013 IEEE 13th International Conference on Data Mining
Efficient and Scalable Information Geometry
Metric Learning
Wei Wang∗† , Bao Gang Hu† , Zengfu Wang∗
∗Department of Automation , University of Science and Technology of China , Hefei , China
†Institute of Automation , Chinese Academy of Sciences , Beijing , China ∗†weiyi889@mailustceducn , †hubg@nlpriaaccn , ∗zfwang@ustceducn into a full rank one since it
Abstract—Information Geometry Metric Learning ( IGML ) is shown to be an effective algorithm for distance metric learning . In this paper , we attempt to alleviate two limitations of IGML : ( A ) the time complexity of IGML increases rapidly for highdimensional data ; ( B ) IGML has to transform the input lowrank kernel is undefined for singular matrices . To this end , two novel algorithms , referred to as Efficient Information Geometry Metric Learning ( EIGML ) and Scalable Information Geometry Metric Learning ( SIGML ) , are proposed . EIGML scales linearly with the dimensionality , resulting in significantly reduced computational complexity . As for SIGML , it is proven to have a range space preserving property . Following this property , SIGML is found to be capable of handling both full rank and low rank kernels . Additionally , the geometric information from data is further exploited in SIGML . In contrast to most existing metric learning methods , both EIGML and SIGML have closed form solutions and can be efficiently optimized . Experimental results on various data sets demonstrate that the proposed methods outperform the state ofthe art metric learning algorithms .
Keywords—Mahalanobis distance learning , Information Geometry Metric Learning , closed form solution , range space preserving
INTRODUCTION Metric learning is of fundamental
I . interest in machine learning and data mining . The metric distances provide a measurement of dissimilarity between different points and have a critical impact on the success of many classical algorithms , eg , k nearest neighbor ( kNN ) classification , support vector machines ( SVM ) and radial basis function ( RBF ) networks . To this end , a number of excellent methods have been developed for exploiting distance metrics in different settings , eg , [ 1 ] , [ 2 ] , [ 3 ] , [ 6 ] . In this direction , much representative work tends to learn a global Mahalanobis distance from labeled data [ 3 ] , [ 4 ] , [ 5 ] , [ 11 ] , [ 13 ] . These global methods formulate a semidefinite programming ( SDP ) [ 24 ] to keep all the data points in the same class close together while ensuring those from different classes far apart . To further utilize both the label information and the geometric information from data , some local Mahalanobis distance learning algorithms are proposed recently [ 7 ] , [ 8 ] , [ 9 ] , [ 10 ] which can be divided into two categories : convex and nonconvex . The convex method [ 8 ] optimizes an SDP and perform a full eigen decomposition of the Mahalanobis distance at each iteration to maintain the positive semi definite property . Therefore , the computational cost rises rapidly with the increase of dimension . The nonconvex methods [ 7 ] , [ 9 ] , [ 10 ] are prone to being trapped in local solutions and may suffer from computationally expensive optimizations .
In contrast to the metric learning methods discussed above , Information Geometry Metric Learning ( IGML ) , introduced by Wang & Jin [ 12 ] , can find a closed form solution rather than solving an SDP . Despite the popularity of IGML in metric learning , it may show some limitations in the following aspects . ( 1 ) The computational complexity of IGML is O(d3 + nd2 ) , where n is the number of training points and d is the dimensionality . The running time increases rapidly for high dimensional data . ( 2 ) IGML employs a ( potential ) low rank ideal kernel [ 13 ] . However , IGML requires that the input kernel should be full rank , since it is infinite for singular matrices . Restricted by this requirement , IGML has to smooth the low rank kernel with an identity matrix and a smoother parameter . This smoother has great influences on metric learning performance . Moreover , the ideal kernel used in IGML only exploits the label information , thus the geometric information of data is lost .
In this paper , we attempt to alleviate the limitations discussed above respectively and propose two novel distance metric learning algorithms , ie , Efficient Information Geometry Metric Learning ( EIGML ) and Scalable Information Geometry Metric Learning ( SIGML ) . EIGML is advantageous for highdimensional data sets since it reduces the computational complexity of IGML to O(nd ) and is also fast for testing . In particular , EIGML learns an identity plus low rank Mahalanobis distance [ 16 ] and represents the Mahalanobis distance by O(d ) parameters instead of O(d2 ) memory . On the other hand , SIGML is proposed to generalize and formalize IGML to both full rank and low rank kernel cases . We prove that SIGML has a range space preserving property . That is , SIGML is finite if the range space of the kernel matrix contains the range space of the Mahalanobis distance metric and the Mahalanobis distance metric is positive definite . Based on this property , for lowrank kernels , SIGML is restricted to the range spaces of the matrices and alternatively solves a full rank problem in a lower dimensional space . In this term , SIGML can directly handle the low rank kernels without smoother parameter . Another key contribution of SIGML is that the learnt Mahalanobis distance simultaneously integrates two sources of information : 1 ) preserving the neighborhood structure according to data itself ; 2 ) maximally aligning with the labels of data . We emphasize that both EIGML and SIGML can find the closedform solutions , leading to efficient optimization .
The rest of paper is organized as follows . IGML is reviewed briefly in Section II . Section III presents EIGML for high dimensional data . In Section IV , SIGML is proposed to expand the domain of IGML . We also show how to effectively preserve
1550 4786/13 $31.00 © 2013 IEEE DOI 101109/ICDM201367
1217 the local information in SIGML . Section V reports the experimental results . Finally , conclusions are given in Section VI .
IGML in high dimensional data . With the purpose of improving computational efficiency , we propose Efficient Information Geometry Metric Learning ( EIGML ) .
II . RELATIVE FORMULAS
In this section , we outline Information Geometry Metric Learning ( IGML ) [ 12 ] . Associated with two covariance matrices P and Q , two Gaussian distributions with zero mean are defined as : P r(x|P ) and P r(x|Q ) . Theorem 1 . [ 12 ] The distance function between two positive definite matrices P and Q can be derived by the KullbackLeibler divergence between P r(x|P ) and P r(x|Q ) which is equal to the following Bregman distance function [ 15 ] : ( tr(Q−1P ) + log|Q| − log|P| − n ) . d(P.Q ) = ( 1 ) Given a set of variables X = {(x1 , y1 ) , . . . , ( xn , y2)} , where xi ∈ Rd and yi is the class label , the Mahalanobis distance between xi and xj can be calculated as follows :
1 2 dA(xi , xj ) = ( xi − xj)T A(xi − xj ) ,
( 2 ) where A ∈ Rd×d is positively semi definite . IGML constructs a linear kernel KX for A : KX = XT AX . In the ideal kernel function , two points should be considered similar if and only if they belong to the same class . Thus , the so called ideal kernel [ 13 ] is given by :
.
KD(i , j ) = KD(xi , xj ) =
1 0 if yi = yj if yi '= yj .
( 3 )
When the number of classes C < n , KD is singular . Therefore , KD has to be smoothed with an identify matrix In , as : ¯KD = KD + λIn , where λ > 0 is the smoother parameter . In IGML , the optimal distance metric A is searched by minimizing the matrix distance d(KX . ¯KD ) defined in Eq ( 1 ) :
A = arg min A.0 = arg min A.0 d(KX . ¯KD ) tr( ¯K−1
D XT AX ) − log|A| .
Proposition 1 . [ 12 ] The optimal solution to Eq ( 4 ) is
D XT )−1
.
A = ( X ¯K−1 Cfi
The optimal A in Eq ( 5 ) can also be expressed as follows :
A = λ( k=1 sk[Σk + λ¯xk¯xT k λ + sk
])−1
,
( 6 ) where sk is the number of the input points in the k th class , ¯xk and Σk are the mean and the covariance matrix for the points in the k th class respectively .
III . EFFICIENT INFORMATION GEOMETRY
METRIC LEARNING
Note that the computational complexity of IGML is O(d3+ nd2 ) , relying on the computation of covariance matrix and matrix inversion in Eq ( 6 ) . Moreover , the number of involved parameters is O(d2 ) . For example , a data set with 1,000 dimensions leads to a Mahalanobis distance matrix with 1 million parameters . These aspects limit the application of
Consider a low dimensional space in Rd and let the columns of U form an orthogonal basis of this subspace . The Mahalanobis distance A is constrained to take the form [ 16 ] : A = Id + Al = Id + ULUT , where Id ∈ Rd×d is the identity matrix , Al denotes the low rank part of A and L ∈ S k×k ( k ff d ) . The selection of U can follow some heuristics in [ 16 ] , + [ 17 ] . We propose the following algorithm EIGML to learn an identity plus low rank Mahalanobis distance : tr( ¯K−1
D XT AX ) − log|A| min A,L.0 st A = Id + ULUT
.
( 7 )
Following the Sylvester ’s determinant lemma [ 18 ] , the second term of Eq ( 7 ) can be rewritten as : log|A| = log|Id + ULUT| = log|Ik + L| = log|S| ,
( 8 ) where S = Ik + L . The first term of Eq ( 7 ) is rewritten as : tr( ¯K−1 = tr(X ¯K−1 = tr(X ¯K−1
D XT AX ) = tr((Id + ULUT )X ¯K−1 D XT U )
D XT ) + tr(LUT X ¯K−1 D XT ) − tr(X . ¯K−1
D X.T ) + tr(SX . ¯K−1
D XT )
D X.T ) ,
( 9 ) where Xfi = UT X can be considered as the reduceddimensional representation of X .
Substituting Eq ( 8 ) and Eq ( 9 ) into Eq ( 7 ) , we obtain the following objective function : tr( ¯K−1 min S.0
D XfiT SXfi ) − log|S| .
( 10 )
The solution of Eq ( 10 ) can be given as follows based on Proposition 1 :
S = λ( sk[Σfi k + λ¯xfi k¯xfiT k λ + sk
])−1
,
( 11 ) fi k and Σfi where ¯x k are the mean and the covariance matrix for the points of Xfi in the k th class respectively . Note that Eq ( 11 ) solves for a k × k matrix rather than a d × d matrix required in IGML . The computational complexity is reduced to O(k3 + nk2 + ndk ) . Based on Eq ( 7 ) , in EIGML , the optimal Mahalanobis distance A is obtained as : A = Id + U(S − Ik)UT . EIGML also shows its advantages in less testing time since it can store the optimal A implicitly using O(dk + k2 ) memory and the Mahalanobis distance between any two points can be computed in O(dk + k2 ) time .
Cfi k=1
IV . SCALABLE INFORMATION GEOMETRY
METRIC LEARNING that
Recall the ideal kernel KD defined in Eq ( 3 ) is ( potential ) low rank . IGML modifies KD to the full rank ¯KD using an identity matrix since IGML is undefined and infinite for low rank kernel matrices . However , this modification may change some intrinsic properties of KD and the involved smoother parameter has great influences on metric learning performance . Moreover , KD is derived only from the labels of data while neglects the local structure information . Generally , we extend the idea of IGML and propose the following
1218
( 4 )
( 5 ) optimization problem , named Scalable Information Geometry Metric Learning ( SIGML ) :
Substituting φ(x ) = −log|x| into F ( A ) , we obtain :
A = arg min A≥0 tr(K−1
I XT AX ) − log|A| ,
( 12 )
F ( A ) =
( mT i uj)2(− logλi |mi|2
+ λi θj
) − n . dfi nfi i=1 j=1 where KI is the ideal kernel constructed by exploiting more information from data . Meanwhile , KI can be either full rank or low rank , ie , rank(KI ) = r ≤ n .
A . Range Space Preserving Property
We relate the optimization defined in Eq ( 12 ) to the real valued strictly convex function φ over a convex set . Proposition 2 . The objective function in Eq ( 12 ) is equivalent to the following function : tr(K−1
I XT AX)− log|A| = φ(A)− tr(A− KI )T∇φ(KI ) , ( 13 ) where φ(X ) = −log|X| = −' i logλi = i φ(λi ) .
'
Based on the eigenvalues and eigenvectors of A and KI , an alternative expression for Eq ( 13 ) can be provided in the following theorem . Theorem 2 . Let the eigen decomposition of A and KI be VΛVT and UΘUT , respectively . Then XT AX = XT VΛVT X = MΛMT , where M = XT V . Eq ( 13 ) is equal to the following expression :
( mT i uj)2(− logλi |mi|2
+ λi θj
) − n .
( 14 ) dfi nfi i=1 j=1
Proof :
F ( A ) = φ(A ) − tr(A − KI )T∇φ(KI ) d . d .
φ(λi ) − tr((MΛMT − UΘUT )T∇φ(KI ) ) n . i=1
=
= i=1 j=1
( mT i uj)2 φ(λi ) |mi|2 where the third line uses the fact that ∇φ(KI ) can be rewritten as the following function [ 22 ] :
'n
− tr((MΛMT − UΘUT )T∇φ(KI ) ) , i uj)2 = |mi|2 . ⎞ ⎟⎠ UT
∇φ(θ2 ) j=1(mT
0
.
··· ···
⎛ ⎜⎝ ∇φ(θ1 )
0
∇φ(KI ) = U
In term of this notation , we have : tr(UΘUT∇φ(KI ) ) = nfi θj∇φ(θj ) , dfi nfi j=1 i=1 j=1 tr(MΛMT∇φ(KI ) ) =
( mT i uj)2
λi∇φ(θj ) .
Finally , F ( A ) is equal to the following expression : dfi nfi i=1 j=1
( mT i uj)2( φ(λi ) |mi|2
− λi∇φ(θj ) ) + nfi j=1
In the information geometry metric learning problems , the optimal Mahalanobis distance A is expected to be full rank . In Eq ( 14 ) , if KI is of low rank , some eigenvalues θi are equal to zero . However , F ( A ) is infinite when λi '= 0 but θi = 0 . [ 22 ] proves that the key to using φ(X ) = −log|X| in the lowrank setting comes from restricting φ(X ) to the range spaces of matrices . To make F ( A ) finite for rank deficient KI , we have the following Proposition 3 . Proposition 3 . The objective function in Eq ( 14 ) is finite if rank(A)=d and rank(A ) ≤ rank(KI ) .
Proof : For finiteness of F ( A ) , we have that mi and i uj = 0 ) when λi '= 0 but uj must be orthogonal ( ie , mT θi = 0 . mi is also the eigenvector of A since mi = XT vi . When θi = 0 , the corresponding eigenvector uj is in the null space of KI . When λi '= 0 , the corresponding eigenvector mi is in the range space of A . Therefore , every vector uj in the null space of KI is orthogonal to any vector mi in the range space of A . This explains that Null(KI ) ⊆ Null(A ) or , equivalently Range(A ) ⊆ Range(KI ) . The property that Range(A ) ⊆ Range(KI ) implies rank(A ) ≤ rank(KI ) .
Based on Theorem 2 and Proposition 3 , SIGML for low rank matrices can be expressed as :
F ( A ) =
( mT i uj)2(− logλi |mi|2
+ λi θj
) − n ,
( 15 ) where r = rank(KI ) and d ≤ r < n . The eigenvalues of A and KI are listed in non increasing order .
B . Low Rank SIGML via Restriction on the Range Space
The section above demonstrates that the range space of KI must contain the range space of A in the low rank cases . We now show that the optimization problem ( 15 ) for low rank KI can be cast as a full rank problem in a lower dimensional space ( ie , the range space of KI ) . Afterwards , the closedform solution to SIGML can be given . Theorem 3 . Let the positive semidefinite d × d matric A and n × n matric KI satisfy rank(A ) = d and rank(KI ) = r , Range(A ) ⊆ Range(KI ) . Let W be an n × r column orthogonal matrix with Range(KI ) ⊆ Range(W ) . If we define :
I XT AX ) − log|A| tr(K−1 = tr((WT KIW)−1WT XT AXW ) − log|A| .
( 16 )
Then this definition is consistent with the low rank IGML in Eq ( 15 ) and is not dependent on the choice of W . dfi rfi i=1 j=1
θj∇φ(θj ) .
Proof : Denote A = VΛVT and KI = UΘUT , respectively . Assume the eigenvalues of A and KI are sorted in non increasing order . The upper left r × r submatrix of Θ
1219 are Θr , and the corresponding reduced eigenvectors are Ur . Substituting W = Ur into Eq ( 16 ) , we obtain :
Ur)−1UT r XT AXUr ) − log|A| r XT VΛVT XUr ) − log|A| r M)Λ(MT Ur ) ) − log|A| , r UΘUT r UT r ( UT tr((UT = tr(Θ−1 = tr(Θ−1 rfi dfi where Θr and A are full rank . Therefore , following Theorem 2 , Eq ( 16 ) can be written as :
( mT i uj)2(− logλi |mi|2
+ λi θj
) − n . i=1 j=1
Next , we prove that this definition is independent of the choice of W . Note that all the n× r orthogonal matrices with the same range space of Ur can be expressed as WQ , where Q is an r × r orthogonal matrix . Substituting WQ into W in Eq ( 16 ) , we obtain : tr(((WQ)T KI ( WQ))−1(WQ)T XT AX(WQ ) ) = tr(Q−1(WT KIW)−1Q−T QT ( WT XT AXW)Q ) = tr((WT KIW)−1WT XT AXW ) , where the third line uses the fact Q and WT KIW are both square , non singular matrices .
Following Theorem 3 and Proposition 1 , the solution of
SIGML in Eq ( 12 ) can be given by : fi ( XK−1 ( XW(WT KIW)−1WT XT A)−1
I XT )−1
A =
( 17 ) where W is an n × r column orthogonal matrix with Range(KI ) ⊆ Range(W ) . if r = n if d ≤ r < n ,
C . Locality Preserving in SIGML
In IGML , the optimal Mahalanobis metric is learnt based on KD for classification . KD is formulated using only the class labels of data points and has the fixed rank C . It is expected to construct an ideal kernel KI which has an adjustable rank ( full rank or low rank ) . Moreover , the construction of KI should be based on two simple idealizations for KNN classification : 1 ) similarities between points with different labels will be zero ; 2 ) similarities between points in the same class will be encouraged according to their relative locations . In this term , KI in SIGML is defined on a weighted graph structure G with the following adjacency matrix M : if i '= j & yi = yj otherwise , exp(−ffxi−xjff2 0
M(i , j ) = ff
( 18 )
2σ2
) where σ is the width of Gaussian function .
'
1 ) Full Rank Ideal Kernel KI : Specific kernel functions can be constructed to capture a useful and more global sense of similarity on the graph G [ 19 ] , [ 20 ] . Let D be an n × n diagonal matrix with Dii = j Mij . The Laplacian of G can be defined as L = D − M , and the Normalized Laplacian is ˜L = D− 1 2 . The eigenvalues and eigenvectors of ˜L are denoted as λi and φi , so that ˜L = i . In this paper , we investigate the diffusion kernel [ 19 ] which is proven to be a generalization of Gaussian kernel to graphs . In this term , KI is defined as : KI = i , where σd is the width of the diffusion kernel .
'n i=1 exp(−σ2 d/2λi)φiφT
2 LD− 1 i λiφiφT
'
2 ) Low Rank Ideal Kernel KLI : The kernel matrix KI constructed above is of full rank since its eigenvalues f ( λi ) = exp(−σ2 d/2λi ) > 0 ( i = 1 , , n ) . Despite the effectiveness of n × n matrix KI , it has relatively high computational efficiency due to the full eigen decomposition of Laplacian ˜L and the matrix multiplication . Recently , many kernel based algorithms [ 21 ] , [ 22 ] employ low rank representations to improve computational efficiency . Recall that ˜L has an interesting property [ 20 ] : a large λi corresponds to a rather uneven φi on the graph G . From a regularization perspective , the uneven vectors should be penalized more strongly than the vectors with small eigenvalues , since the changes should vary slowly over the graph [ 20 ] . Based on this intention , we can construct a low rank KLI ( ie , rank(KLI ) = r ≤ n ) which just contains only the r smallest eigenvalues λi and the corresponding eigenvector φi : KLI = i , where the eigenvalues of ˜L are listed in non decreasing order . This lowrank representation relieves the burden of memory from O(n2 ) storage to O(nr ) and has the advantages of efficient eigendecomposition and fast matrix multiplication .
'r i=1 exp(−σ2 d/2λi)φiφT
D . Proposed Algorithms
For full rank KI in SIGML ( SIGML F ) , we can find the
A = ( XK−1 following closed form solution based on Eq ( 17 ) : I XT )−1 = ( XΦΛ−1ΦT XT )−1
( 19 ) where Φ = ( φ1 , , φn ) and Λ−1 is an n × n diagonal matrix with [ Λ−1]ii = [ exp(−σ2 d/2λi)]−1 .
,
For low rank KLI in SIGML ( SIGML L ) , we can find the following closed form solution based on Eq ( 17 ) : A = ( WT KLIW)−1WT XT AXW
= ( XWΛ−1 r WT XT )−1
( 20 ) where W is an orthogonal n× r matrix : W = ( φ1 , φ2 , , φr ) and Λ−1 r ]ii = [ exp(−σ2 is a r × r diagonal matrix with [ Λ−1 d/2λi)]−1 .
, r
V . EXPERIMENTS
In this section , we evaluate the proposed metric learning methods ( ie , EIGML and SIGML ) for classification tasks . To quantitatively evaluate the separability of points in different classes , a simple measurement is used : misclassification rate by 1 nearest neighbor classifier ( 1 NN ) . The experiments are presented on 16 data sets from different domains : UCI data1 , IDA data2 , text data 20newsgroups3 and handwritten digit data USPS3 . Their detailed information can be found in Table I . Each data set is randomly split into training and test subsets and all metrics are trained using only the training sets . Test instances are compared to points in the training sets using the learnt distance metric . Results are obtained by averaging 100 runs on random splits of these data sets . For all the data sets , the features are scaled into [ −1 , +1 ] . The experiments are carried out on a single machine with Intel Core 2 Quad @ 2.40 Ghz and 10 GB of RAM running 64 bit Windows 7 .
1Available at : urlhttp://wwwicsuciedu/˜mlearning/ 2Available at : http://idafirstfraunhoferde/projects/bench/benchmarkshtml 3Available at : http://wwwcsnyuedu/˜roweis/datahtml
1220
Table I .
LIST OF DATA SETS . DATA SETS WITH * CONTAIN INTRINSIC
WITHIN CLASS MULTIMODAL STRUCTURES .
Data name 20newsgroups USPS IDA : banana* IDA : breast cancer IDA : diabetes IDA : flare solar IDA : german IDA : heart IDA : image IDA : thyroid* IDA : titanic IDA : twonorm IDA : waveform* UCI : glass UCI : wine
# of attributes 100 784 2 9 8 9 20 13 18 5 3 20 21 9 13
Training Size 11370 7700 400 200 468 666 700 170 1300 140 150 400 400 107 89
Test Size 4872 3300 4900 77 300 400 300 100 1010 75 2015 7000 4600 107 89
Classes 4 10 2 2 2 2 2 2 2 2 2 2 2 6 3
A . High Dimensional Data Sets
First we evaluate the efficiency of EIGML on highdimensional data in comparison with IGML . The text data set , namely 20newsgroups , consists of posted articles from 20 newsgroups , with roughly 1000 articles per newsgroup . We use the tiny version3 of this data set with binary occurrence data for 100 words across 16242 postings . The postings have been tagged by the highest level domain in the array “ newsgroups ” . The USPS data set of handwritten digits includes images of “ 0 ” through “ 9 ” with 1100 examples of each class . The original 28 × 28 grayscale images are downsampled to 8 × 8 pixels resulting in 64 dimensions . On these two high dimensional data sets , the Mahalanobis distance is restricted to a set of k basis vectors constructed as follows [ 16 ] . We define R ∈ Rk to be the basis of U : U = R(RT R)−1/2 . If the number of classes C is greater than k , the class mean points are clustered into k clusters and each cluster ’s center ri is used to form the basis matrix R = [ r1 , , rk ] . If C ≤ k , we cluster points within each class into approximately k/C clusters and define R by the center of each cluster . EIGML is also compared with Latent Semantic Analysis ( LSA ) [ 26 ] . LSA projects the data by PCA [ 23 ] and computes distances in the projected space . IGML is used as the baseline . quite small bases ( eg , 3 or 4 ) , the error rate of EIGML is higher than the baseline IGML , while EIGML has significantly reduced computational complexity as analysed in Section III ; 3 ) with the bases increasing , EIGML rapidly gets the similar results as IGML . These advantages demonstrate that EIGML is reliable and effective by learning an identity plus low rank Mahalanobis distance .
B . IDA and UCI Data Sets
Next we evaluate the effectiveness of SIGML F and SIGML L on 13 data sets of varying size and difficulty . IDA data sets are standard for binary classification evaluation [ 25 ] . Among them , the ringnorm , twonorm and waveform data sets contain features with only noise . The thyroid and waveform data sets are converted from multi classes problems so that they contain intrinsic within class multimodal structures . The banana data set is multimodal as well . SIGMLF is systematically compared with IGML and the following three state of the art metric learning methods : InformationTheoretic Metric Learning ( ITML ) [ 5 ] , Large Margin Nearfi est Neighbor ( LMNN ) [ 8 ] , Metric Learning by Collapsing Class ( MCML ) [ 4 ] . The Euclidean distance d(xi , xj ) = ( xi − xj)T(xi − xj ) is used as the baseline . In SIGMLF , the diffusion kernel parameter σd and the Gaussian width σ are tuned in the range {0.1 , 1 , 10} by cross validation .
Figure 2 shows the average error rates on test data sets . Some observations can be drawn from the results . Firstly , SIGML F consistently outperforms Euclidean distance . In general , the learnt Mahalanobis metrics can result in significantly improved kNN classification accuracy . Secondly , the metric learning methods MCML , ITML and IGML show their limitations for the multimodal data sets with * , since they just exploit the global information . In contrast , despite the Gaussian assumption in SIGML F , the inclusion of locality notion enables it perform well in multimodal distributions . This observation indicates that the constructed ideal kernel KI can well capture the global and the local data structures . Thirdly , SIGML F shows better metric learning performance than the local algorithm LMNN on most of the data sets . These observations illustrate the generalized and effective performance of SIGML F for classification .
D QHZVJURXS
E 8636
Figure 1 . Classification error rate ( in percent ) across bases of varying sizes on ( a ) 20 newsgroups data set and ( b ) USPS data set . Compared algorithms are EIGML , LSA [ 26 ] and IGML [ 12 ] .
Figure 1 shows the classification error rate across bases of varying sizes on 20 newsgroups and USPS data sets respectively . Some advantages can be concluded from the results : 1 ) compared with LSA , EIGML can provide much better performance across all the basis sizes ; 2 ) in terms of
Figure 2 . Classification error rate ( in percent ) on 13 test data sets . Compared algorithms are ITML [ 5 ] , LMNN [ 8 ] , MCML [ 4 ] , IGML [ 12 ] and our proposed SIGML F . Euclidean distance is used as the baseline .
1221
Table II .
CLASSIFICATION ERROR RATE ( IN PERCENT ) AND AVERAGE TRAINING TIME ( IN SECONDS ) ON WAVEFORM AND TWONORM DATA SETS .
Waveform
Twonorm
Error Rate
Rank Time
Rank Time
Error Rate
30
31.45 27.95
30
66.76 6.41
50
32.80 16.85
50
71.40 5.68
100 35.43 14.60 100 91.23 5.65
SIGML L 200 44.13 14.55 200 94.26 5.65
500 87.94 14.55 500
122.55 5.59
800
201.16 14.55 800
231.87 5.56
1000 300.04 14.50 1000 339.98 5.55
SIGML F
3500 399.13 14.50 5180
1345.13
5.55
Furthermore , the performance of SIGML L with the lowrank kernel matrix KLI is investigated . In this case , waveform and twonorm data sets are employed across different ranks of KLI . Table II shows the error rate on test sets and the running time of SIGML L . SIGML F with KI is used as the baseline . Unlike previous experiments , the results are obtained by averaging over 10 runs with 70/30 splits of the entire data for training and test sets . The results clearly show that when the rank is quite low ( eg , 30 or 50 ) , SIGML L greatly decreases the computational time , while the error rate of SIGML L is close to that of SIGML F . As the rank becomes high , SIGML L achieves comparable performance with SIGML F and benefits from small increase in the running time , especially on twonorm data set . We have similar observations for other data sets , the details are not given due to the lack of space .
VI . CONCLUSION
In this paper , we have proposed two novel algorithms to alleviate the limitations of Information Geometry Metric Learning ( IGML ) respectively . The main contribution of this paper is three fold . ( 1 ) By restricting the distance metric to a small dimension basis , we present an algorithm which scales linearly with the dimensionality . Therefore , this proposed method can significantly reduce the time and the space complexity of IGML with competitive performance for high dimensional data . ( 2 ) It is proven that the optimization problem of IGML for low rank kernels can be cast as a full rank problem in a lower dimensional space . Based on this theorem , the domain of IGML can be extended to both full rank and low rank kernel matrices . ( 3 ) We focus on preserving both the local information and the global information in the construction of ( full rank or low rank ) ideal kernel matrices , result in effective and generalized performance . In contrast to most existing metric learning methods , the proposed methods can find closed form solutions without solving an SDP . Compared with several stateof the art metric learning algorithms , extensive experimental results demonstrate the advantages of our methods .
ACKNOWLEDGEMENT
This work was supported in part by the Natural Science of
Foundation of China under Grant 61075051 .
REFERENCES
[ 1 ]
J . Chen , Z . Zhao , J . Ye , and H . Liu , “ Nonlinear adaptive distance metric learning for clustering , ” in : SIGKDD , 2007 , pp . 123 132 .
[ 2 ] S . T . Roweis and L . K . Saul , “ Nonlinear dimensionality reduction by locally linear embedding , ” Science , vol . 290 , no . 5500 , pp . 2323 2326 , 2000 .
[ 3 ] E . P . Xing , A . Y . Ng , M . I . Jordan , and S . J . Russell , “ Distance metric learning , with application to clustering with side information , ” in : NIPS , 2002 , pp . 505 512 .
[ 4 ] A . Globerson and S . Roweis , “ Metric learning by collapsing classes , ” in : NIPS , 2006 , pp . 451 458 . J . V . Davis , B . Kulis , P . Jain , S . Sra , and I . S . Dhillon , “ Informationtheoretic metric learning , ” in : ICML , 2007 , pp . 209 216 .
[ 6 ] R . Jin , S . Wang , and Y . Zhou ,
“ Regularized distance metric learn ing:theory and algorithm , ” in : NIPS , 2009 , pp . 862 870 . J . Goldberger , S . Roweis , G . Hinton , and R . Salakhutdinov , “ Neighbourhood components analysis , ” in : NIPS , 2005 , pp . 513 520 .
[ 5 ]
[ 7 ]
[ 8 ] K . Q . Weinberger , J . Blitzer , and L . K . Saul , “ Distance metric learning for large margin nearest neighbor classification , ” in : NIPS , 2005 , pp . 1473 1480 .
[ 9 ] L . Yang , R . Jin , R . Sukthankar , and Y . Liu , “ An efficient algorithm for local distance metric learning , ” in : AAAI , 2006 , pp . 543 548 .
[ 10 ] G . Q . Zhong , K . Z . Huang , and C . L . Liu , “ Low rank metric learning with manifold regularization , ” in : ICDM , 2011 , pp . 1266 1271 .
[ 11 ] A . Bar Hillel , T . Hertz , N . Shental , and D . Weinshall , “ Learning a Mahalanobis metric from equivalence constraints , ” J . Mac . Learn . Res . , vol . 6 , pp . 937 965 , 2005 .
[ 12 ] S . Wang and R . Jin , “ An information geometry approach for distance metric learning , ” in : AISTATS , 2009 , pp . 591 598 .
[ 13 ] N . Cristianini , J . Kandola , A . Elisseeff , and J . Shawe Taylor , “ On kernel target alignment , ” in : NIPS , 2002 , pp . 367 373 .
[ 14 ] K . Tsuda , S . Akaho , K . Asai , and C . Williams , “ The EM algorithm for kernel matrix completion with auxiliary data , ” J . Mac . Learn . Res . , vol . 4 , pp . 67 81 , 2003 .
[ 15 ] L . Bregman , “ The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming , ” USSR Comp . Mathematics and Mathematical Physics , vol . 7 , pp . 200 217 , 1967 .
[ 16 ] P . Jain , B . Kulis , J . V . Davis , and I . S . Dhillon , “ Metric and kernel learning using a linear transformation , ” J . Mac . Learn . Res . , vol . 13 , pp . 519 547 , 2012 . J . V . Davis and I . S . Dhillon , “ Structured metric learning for high dimensional problems , ” in : KDD , 2008 , pp . 195 203 .
[ 17 ]
[ 18 ] D . A . Harville , Matrix Algebra From a Statistician ’s Perspective .
Springer Berlin , 2008 .
[ 19 ] R . S . Kondor and J . Lafferty , “ Diffusion kernels on graphs and other discrete input spaces , ” in : ICML , 2002 , pp . 315 322 .
[ 20 ] A . Smola and R . Kondor , “ Kernels and regularization on graphs , ” in :
COLT , 2003 , pp . 144 158 .
[ 21 ] B . Kulis , A . Surendran , and J . Platt , “ Fast low rank semidefinite programming for embedding and clustering , ” in : AISTATS , 2007 .
[ 22 ] B . Kulis , M . A . Sustik , and I . S . Dhillon , “ Low rank kernel learning with bregman matrix divergences , ” J . Mac . Learn . Res . , vol . 10 , pp . 341376 , 2009 . I . Jolliffe , Principal Component Analysis . Springer Verlag , 1986 .
[ 23 ] [ 24 ] S . Boyd and L . Vandenberghe , Convex Optimization . Cambridge
University Press , Cambridge , 2004 .
[ 25 ] G . R¨etsch , T . Onoda , and K R M¨uller , “ Soft margins for adaboost , ”
Mach . Learn . , vol . 42 , pp . 287 320 , 2001 .
[ 26 ] S . C . Deerwester , S . T . Dumais , T . K . Landauer , G . W . Furnas , and R . A . Harshman , “ Indexing by latent semantic analysis , ” J . Am . Soc . Inf . Sci . Tec . , vol . 41 , pp . 391 407 , 1990 .
1222
