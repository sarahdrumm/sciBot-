2013 IEEE 13th International Conference on Data Mining
A Novel Relational Learning to Rank Approach for Topic Focused Multi Document
Summarization
Institute of Computing Technology , Chinese Academy of Sciences , Beijing 100190 , China
Yadong Zhu Xueqi Cheng {zhuyadong , dupan}@softwareictaccn , {lanyanyan , guojiafeng , cxq}@ictaccn
Yanyan Lan
Jiafeng Guo
Pan Du
Abstract—Topic focused multi document summarization aims to produce a summary over a set of documents and conveys the most important aspects of a given topic . Most existing extractive methods view the task as a multi criteria ranking problem over sentences , where relevance , salience and diversity are three typical requirements . However , diversity is a challenging problem as it involves modeling the relationship between sentences during ranking , where traditional methods usually tackle it in a heuristic or implicit way . In this paper , learning to rank approach we propose a novel relational ( R LTR ) to solve this problem . Relational learning to rank is a new learning framework which further incorporates relationships into traditional learning to rank in an elegant way . Specifically , the ranking function is defined as the combination of content based score of individual sentence , and relation based score between the current sentence and those already selected . On this basis , we propose to learn the ranking function by minimizing the likelihood loss based on Plackett Luce model , which can naturally model the sequential ranking procedure of candidate sentences . Stochastic gradient descent is then employed to conduct the learning process , and the summary is predicted by the greedy selection procedure based on the learned ranking function . Finally , we conduct extensive experiments on benchmark data sets TAC2008 and TAC2009 . Experimental results show that our approach can significantly outperform the state of the art methods from both quantitative and qualitative aspects .
I . INTRODUCTION
Due to the explosive growth of information on the Web , there is a great need to provide improved techniques for information presentation and exploration . Automatic summarization of a single document , or a set of related documents , has become an important means for people to consume large scale online textual information . One specific task of document summarization is Topic focused MultiDocument Summarization ( TMDS ) , which aims to create a short summary over a set of documents , and conveys the most important aspects of the given topic . In this paper , we focus on the scenario of extractive summarization , where the summary is produced by extracting sentences from the original documents .
A good summarization of TMDS task is generally supposed to meet the following typical demands : relevance , salience and diversity [ 1 ] , [ 2 ] . Relevance requires the summary to provide related information with respect to the given topic . Salience means that the summary must neglect those trivial contents and retain the most important pieces of information . Diversity requires that the summary should be with less redundant information , and cover different aspects of the given topic . Among the three criteria , relevance and salience are usually captured based on the content of individual sentences with respect to the topic , which can be computed independent of the extraction procedure . While diversity involves modeling the relationship between the current sentence and those already selected [ 2 ] , thus is coupled with the extraction procedure and become a major challenging problem .
In the literature , most previous work takes the topicfocused multi document summarization as a multi criteria ranking problem and proposes different methods to solve this problem . For example , the authors of [ 3 ] , [ 4 ] , [ 5 ] take summarization as a metric based sentence ranking problem , where the diversity is described by some heuristically predefined metrics such as marginal relevance [ 4 ] , distortion measures [ 3 ] and the combined weights of the terms covered [ 5 ] . Some other methods such as DivRank [ 6 ] and MRSP [ 2 ] leverage the sentence graph for summarization . In these methods , pairwise relationships are directly defined between the sentence and its neighbors , and the diversity is actually implicitly captured through some random walk process in the graph . However , neither heuristic nor implicit ways can well reflect the demand of diversity . Therefore , topicfocused multi document summarization remains an challenging problem , especially from the criteria of diversity .
In this paper , we propose a novel relational learningto rank approach to solve this problem . Firstly , the topicfocused multi document summarization is also formalized as a sentence ranking problem . Secondly , the inter relationships among candidate sentences are considered in the ranking process , besides the relevance and salience information of sentences as used in traditional learning to rank [ 7 ] .
Specifically , the ranking function is defined as the combination of content based score and relation based score . The content based score ( for relevance and salience ) only depends on the features of an individual sentence , while the relation based score ( for diversity ) depends on the relationships between the current sentence and those already selected . In this paper , we describe three different ways to represent the relation based score . The loss function is then
1550 4786/13 $31.00 © 2013 IEEE DOI 101109/ICDM201338
927 defined as the likelihood of the ground truth summary based on Plackett Luce model [ 8 ] , which can naturally model the sequential ranking procedure of candidate sentences . On this basis , stochastic gradient descent is employed to conduct the learning process , and the summary is predicted by the greedy selection procedure based on the learned ranking function . For evaluation , we conduct extensive experiments based on the benchmark data sets of TAC2008 and TAC2009 . The experimental results show that : ( 1 ) our approach is more effective than all the state of the art summarization methods with ROUGE 1 , ROUGE 2 and ROUGE 4 as evaluation measures ; ( 2 ) our approach can cover more important facts with respect to reference summaries as compared with traditional methods . In summary , our approach can significantly outperform traditional methods in both quantitative and qualitative aspects .
The rest of the paper is organized as follows . We first review some related work in Section II . Then we describe our proposed relational learning to rank approach in section III , including the definitions of ranking function and loss function , learning and prediction procedures . Section IV presents the experimental results and Section V concludes the paper .
II . RELATED WORK
We will briefly review some related extractive summarization methods , especially those that considers relevance and diversity requirements . Most extractive methods treat topic focused multi document summarization as a ranking problem , and try to assign scores to candidate sentences and extract the sentence with high scores .
Some approaches take summarization as a metric based sentence ranking problem [ 4 ] , [ 3 ] , [ 9 ] , [ 5 ] , and predefine different metrics to conduct the ranking process . For example , Carbonell and Godlstein [ 4 ] employ a linear combination of relevance and diversity as the metric called “ marginal relevance ” , and iteratively select sentences with the largest “ marginal relevance ” . The method in [ 3 ] proposes to define a proper distortion measure , and tackles the multi document summarization via minimum distortion . Davis et al . use LSA [ 10 ] to produce term weights , and then try to maximize “ the combined weights of the terms covered ” [ 5 ] . These methods deal with the relevance and diversity issues heuristically by using a pre defined fixed criterion , and do not well capture the relationships among sentences .
Many other approaches are mainly graph based which leverage the sentence graph for summarization . For example , Erkan and Radev proposes LexRank [ 11 ] , which is a variant of PageRank [ 12 ] for generic text summarization . Topicsensitive LexRank [ 13 ] has been applied to the task of queryfocused summarization . Zha in [ 14 ] proposes a mutual reinforcement principle for sentence extraction using the idea of HITS [ 15 ] . Wan et al . apply a manifold ranking algorithm to topic focused summarization [ 16 ] . DivRank [ 6 ] leverages a vertex reinforced random walk to achieve relevance and diversity for summarization . Absorbing random work [ 17 ] is also applied to achieve a diverse ranking for summarization . Du et al . [ 2 ] introduce sink points into manifold ranking for summarization to capture relevance and diversity . The work in [ 1 ] proposes a supervised lazy random walk approach , which learns the strengths of edges and self loops and then conducts the lazy random walk on such a weighted sentence graph . Overall , in these graph based methods , the pairwise relationships are directly defined between the sentences and its neighbors , and the diversity is implicitly captured through some random walk process in the graph . This implicit ways also can not well reflect the demand of diversity . Additionally , these methods are usually with high time complexity and low computing efficiency .
Some previous work also attempts to view the summarization task as a machine learning problem , and trains accurate models for extractive summary . For example , the Support Vector Machine ( SVM ) [ 18 ] , [ 19 ] based methods focus on constructing a decision boundary between summary sentences and non summary sentences . Several traditional learning to rank methods such as ranking SVM , support vector regression and gradient boosted decision trees are also applied to the summary task [ 20 ] , [ 21 ] . These methods rely on the assumption that sentences are independent from each other and ignore the relationships among sentences . Some other methods such as Hidden Markov Models ( HMM ) based [ 22 ] and Conditional Random Field ( CRF ) based [ 23 ] , have been proposed by relaxing the independence assumption and try to modeling the relations between sentences , but the demand of diversity is still not well captured .
The method proposed in [ 24 ] is closely related to our work for it explicitly considers relevance and diversity requirements , by the formulation of a supervised sentence ranking problem , based on Structural SVM techniques . Whereas , it focuses on generic summarization rather than topic focused scenario , and is different from our research problem . In our work , we propose a novel relational learning to rank approach to formulate the TMDS task as a sentence ranking problem , which explicitly captures the diversity , besides the relevance and salience information of sentences .
III . OUR APPROACH
As described in Section 2 , many traditional methods formalize the topic focused multi document summarization as a ranking problem . In this paper , we follow this methodology and utilize learning to rank technique to solve this problem . In traditional learning to rank ( LTR for short ) [ 7 ] , a ranking function is defined on the content of each individual sentence and learned toward some loss function . However , in the task of TMDS , the overall ranking of sentences for a given topic , should depend not only on the individual ranked sentences , but also on how they related to each other . Therefore ,
928 a summary in a sequential manner , considering relevance , salience and diversity for each sentence based on what he/she has already extracted in the previous steps . Therefore , the TMDS task can be treated as a sequential ranking process , where each sentence is ranked according to its relevance/salience to the topic and the relation between all the sentences ranked before it .
The intuitive idea is illustrated in Figure 1 , all the balls represent candidate sentences of a given topic , and different colors represent different subtopics ( or aspects ) . The solid ball is relevant to the topic , and the hollow ball is irrelevant to the topic , and larger size means more relevant . 𝑋 denotes all the candidate sentences . 𝑆 denotes previously selected sentences , and 𝑋∖𝑆 denotes the remanent sentences . When ranking sentences in 𝑋∖𝑆 given the already ranked results 𝑆 , both content based relevance/salience , and diversity relations between this sentence and the previously selected sentences in 𝑆 should be considered . Noting that larger size of the ball means the sentence is more relevant to the topic , and different colors represent different subtopics ( or aspects ) of the given topic . Therefore , the sentence 8 may be more preferred than sentence 4 given 𝑆 , since it is relevant to the topic , and also provides different aspects additionally comparing with the selected set 𝑆 .
Based on this ranking process , we give the precise definition of ranking function as follows . Given a topic 𝑞 , we assume that a set of sentences have been selected , denoted as 𝑆 , the ranking function on the rest sentences 𝑋∖𝑆 is then defined as the combination of the content based score ( for relevance and salience ) and the relation based score ( for diversity ) between the current sentence and those previously selected , as shown below :
𝑟 𝑥𝑖 + 𝜔𝑇
𝑑 ℎ𝑆(𝑅𝑖),∀𝑥𝑖 ∈ 𝑋∖𝑆 ,
𝑓𝑆(𝑥𝑖 , 𝑅 ) = 𝜔𝑇
𝑟 and 𝜔𝑇
( 2 ) where 𝑅𝑖 stands for the matrix of relationships between sentence 𝑥𝑖 and other sentences , with each 𝑅𝑖𝑗 stands for the relationship between 𝑥𝑖 and 𝑥𝑗 , represented by the feature vector of ( 𝑅𝑖𝑗1,⋅⋅⋅ , 𝑅𝑖𝑗𝑙 ) , ℎ𝑆(𝑅𝑖 ) stands for the relational function on 𝑅𝑖𝑗 , 𝑥𝑗 ∈ 𝑆 , 𝜔𝑇 𝑑 stands for the corresponding content and relation weight vector . When 𝑆 = 𝜙 , 𝑓𝑆(𝑥𝑖 , 𝑅 ) is directly represented as 𝜔𝑇 𝑟 𝑥𝑖 . From the above definition , we can see that if we do not consider diversity relations , our ranking function reduces to 𝑓 ( 𝑋 ) , which is the traditional ranking function in learning to rank . In order to accomplish the definition of new ranking function , we first need to define three key components : relational function ℎ𝑆(𝑅𝑖 ) , relation based feature vector 𝑅𝑖𝑗 and the content based feature vector 𝑥𝑖 . We introduce their definitions one by one .
1 ) Relational Function ℎ𝑆(𝑅𝑖 ) : Please note that the relational function ℎ𝑆(𝑅𝑖 ) represents the diversity relationship between the current sentence 𝑥𝑖 and the previously selected sentences in 𝑆 , and each 𝑅𝑖𝑗 , 𝑥𝑗 ∈ 𝑆 stands for the diversity relationship between sentence 𝑥𝑖 and 𝑥𝑗 . If we treat diversity
Figure 1 . An illustration of the sequential ranking for candidate sentences . in this paper , we introduce a novel relational learning torank approach to solve the topic focused multi document summarization problem . The difference between LTR and R LTR is that the latter considers both content of individual sentences and relations between sentences when defining the ranking function First , we introduce the framework of relational learningto rank . Let 𝑋 = {𝑥1,⋅⋅⋅ , 𝑥𝑛}𝑇 be a 𝑛×𝑑 matrix representing 𝑑 dimensional feature vectors of 𝑛 candidate sentences ; each row corresponds to one sentence and each column corresponds to one feature . Let 𝑅 ∈ ℛ𝑛×𝑛×𝑙 denote a 3 way tensor representing relationships between the 𝑛 sentences , where 𝑅𝑖𝑗𝑘 stands for the 𝑘 th feature of relation between sentences 𝑥𝑖 and 𝑥𝑗 . Let 𝑦 be a ground truth summary of 𝑋 . Supposing that 𝑓 ( 𝑋 , 𝑅 ) is a ranking function , and the goal of relational learning to rank is to output the best ranking function from a function space ℱ . the labeled data about 𝑁 topics are given as ( 𝑋1 , 𝑅1 , 𝑦1 ) , ( 𝑋2 , 𝑅2 , 𝑦2),⋅⋅⋅ , ( 𝑋𝑁 , 𝑅𝑁 , 𝑦𝑁 ) , where 𝑋𝑖 denote feature vectors of 𝑛𝑖 sentences . A loss function 𝐿 is defined , and the learning process is conducted by minimizing the total loss with respect to the given training data .
In training ,
ˆ𝑓 = arg min 𝑓∈ℱ
𝐿(𝑓 ( 𝑋𝑖 , 𝑅𝑖 ) , 𝑦𝑖 ) .
( 1 )
𝑁∑
𝑖=1
In prediction , given features 𝑋𝑡 and relations 𝑅𝑡 for 𝑛𝑡 sentences , we output 𝑦𝑡 based on the learned ranking function ˆ𝑓 ( 𝑋𝑡 , 𝑅𝑡 ) . We will define specific ranking function and loss function for the TMDS task in the following subsections .
A . Definition of Ranking Function
Before we define the ranking function , we first consider how human beings extract a summary . People usually extract
929
CONTENT BASED FEATURE FOR LEARNING .
Table I
Category
𝑇 𝑆 𝑇 𝑆 𝑇 𝑆 𝑇 𝑆 𝑇 𝑇
Feature Description
Total
TF IDF BM25 QL.DIR
MRF Length
Pos
1 1 1 2 1 1 defined as follows .
𝑅𝑖𝑗2 = 1 − ∣𝑆𝑖 ∩ 𝑆𝑗∣ ∣𝑆𝑖 ∪ 𝑆𝑗∣ where 𝑆𝑖 , 𝑆𝑗 are the term vectors of sentences .
Figure 2 . Different ways to measure the distance of 𝑥𝑖 to 𝑆 as distance , ℎ𝑆(𝑅𝑖 ) can be viewed as the distance of 𝑥𝑖 to the set 𝑆 . According to different definitions of the distance between an item and a set of items , ℎ𝑆(𝑅𝑖 ) can be defined as the following three forms ( illustrated in Figure 2 ) .
Minimal Distance . The distance between a sentence 𝑥𝑖 and a set 𝑆 is defined as the minimal distance of all the sentence pairs ( 𝑥𝑖 , 𝑥𝑗 ) , 𝑥𝑗 ∈ 𝑆 . Therefore , ℎ𝑆(𝑅𝑖 ) is defined as follows :
ℎ𝑆(𝑅𝑖 ) = min 𝑥𝑗∈𝑆
𝑅𝑖𝑗 .
Average Distance . The distance between a sentence 𝑥𝑖 and a set 𝑆 is defined as the average distance of all the sentence pairs ( 𝑥𝑖 , 𝑥𝑗 ) , 𝑥𝑗 ∈ 𝑆 . Therefore , ℎ𝑆(𝑅𝑖 ) is defined as follows :
ℎ𝑆(𝑅𝑖 ) =
𝑅𝑖𝑗 .
1 ∣𝑆∣
∑
𝑥𝑗∈𝑆
Maximal Distance . The distance between a sentence 𝑥𝑖 and a set 𝑆 is defined as the maximal distance of all the sentence pairs ( 𝑥𝑖 , 𝑥𝑗 ) , 𝑥𝑗 ∈ 𝑆 . Therefore , ℎ𝑆(𝑅𝑖 ) is defined as follows :
ℎ𝑆(𝑅𝑖 ) = max 𝑥𝑗∈𝑆
𝑅𝑖𝑗 .
2 ) Relation based Feature Vector 𝑅𝑖𝑗 : How to define discriminative relation based features that can well capture diversity relation is critical for the success of R LTR in TMDS . In this work , we provides several representative features for the learning process .
Cosine Diversity . The cosine diversity between two sentences is calculated based on their weighted term vector representations , and define the feature as follows .
𝑅𝑖𝑗1 = 1 − s𝑖 ⋅ s𝑗 ∥s𝑖∥∥s𝑗∥ where s𝑖 , s𝑗 are the weighted term vectors of sentences based on 𝑡𝑓∗𝑖𝑠𝑓 , and 𝑡𝑓 denotes the term frequencies , 𝑖𝑠𝑓 denotes inverse sentence frequencies .
Jaccard Diversity . The Jaccard diversity between two sentences measures the ratio of overlapped terms , and is
930
Subtopic Diversity . Different sentences may associate with different aspects of the given topic . We use Probabilistic Latent Semantic Analysis ( PLSA ) [ 25 ] to model implicit subtopics distribution of candidate sentences . Then we can define a kind of subtopic diversity feature based on the KL distance , as follows .
∑
𝑅𝑖𝑗3 =
𝑧𝑖∈𝑍 𝑃 ( 𝑧𝑖∣𝑆𝑖 ) =
𝑃 ( 𝑧𝑖∣𝑆𝑖 ) 𝑃 ( 𝑧𝑖∣𝑆𝑗 ) 𝑃 ( 𝑧𝑖∣𝑆𝑖 , 𝑤𝑗 )
𝑃 ( 𝑧𝑖∣𝑆𝑖)𝑙𝑜𝑔
∑
𝑤𝑗∈𝑆𝑖
1 ∣𝑆𝑖∣ where 𝑃 ( 𝑧𝑖∣𝑆𝑖 , 𝑤𝑗 ) is calculated and saved in the E step of the EM procedure .
Document Level Co occurrence . The document level co occurrence is a binary feature , which indicates whether the two sentences co occur in a same document . It is set to be 0 if they are within one document , 1 otherwise , and denoted as 𝑅𝑖𝑗4 .
Based on these diversity features , we can obtain the relation based feature vector 𝑅𝑖𝑗 = ( 𝑅𝑖𝑗1 , 𝑅𝑖𝑗2 , 𝑅𝑖𝑗3 , 𝑅𝑖𝑗4 ) . Please note that here we only list some representative diversity features used in our work , and our model is flexible to incorporate other useful diversity features for capturing the diversity relation .
3 ) Content based Feature Vector 𝑥𝑖 : Content based features are used to capture the relevance and salience of each individual sentence . For content based features , we view each sentence as a “ short ” document , and then employ some standard relevance and salience features used in LTR research [ 26 ] , as summarized in Table I . They include four topic dependent and two topic independent features , and 𝑇 𝑆 means that the feature is dependent on both the topic and the sentence , and 𝑆 means that the feature only depends on the sentence .
Weighting features . The typical weighting models include TF IDF , BM25 and language models . For language model , we use query likelihood language model with Dirichlet prior .
Term Dependency features . We also employ the classic term dependency features such as MRF [ 27 ] , to enhance capturing of relevance . Additionally , the MRF has two types of values : ordered phrase and unordered phrase , so the total feature number is 2 .
Length . It is the number of words in each sentence after removing the stopping words . We prefer sentences with its length located in a certain range . Several length levels are introduced to denote the sentence length , and a feature vector can be generated with each component denoting a length level .
Pos . It is a binary feature which indicates whether the sentence is the first sentence of a document . If the sentence appears at the beginning of a document , it is set to be 1 , otherwise 0 .
B . Definition of Loss Function
In order to accommodate the sequential ranking process as described in the definition of ranking function , we propose to model the generation of the summary in a sequential way , and define the loss function as the likelihood loss of the generation probability .
Intuitively , the probability of a extracted summary can be viewed as a process to iteratively select the top ranked sentences from the remaining sentences . The precise definition is given as follows . 𝑃 ( 𝑦∣𝑋 ) = 𝑃 ( 𝑥𝑦(1 ) , 𝑥𝑦(2),⋅⋅⋅ , 𝑥𝑦(𝑛)∣𝑋 )
( 3 ) = 𝑃 ( 𝑥𝑦(1)∣𝑋)𝑃 ( 𝑥𝑦(2)∣𝑋∖𝑆1)⋅⋅⋅ 𝑃 ( 𝑥𝑦(𝑛−1)∣𝑋∖𝑆𝑛−2 ) where 𝑦(𝑖 ) stands for the index of sentence which is ranked in position 𝑖 in the summary 𝑦 , 𝑆𝑖 = {𝑥𝑦(1),⋅⋅⋅ , 𝑥𝑦(𝑖)} , 𝑖 = 1,⋅⋅⋅ , 𝑛 , 𝑃 ( 𝑥𝑦(1)∣𝑋 ) the probability that is ranked first among the sentences in 𝑋 , and 𝑥𝑦(1 ) 𝑃 ( 𝑥𝑦(𝑗)∣𝑋∖𝑆𝑗−1 ) stands for the probability that sentence 𝑥𝑦(𝑗 ) is ranked first among the sentences in 𝑋∖𝑆𝑗−1 . stands for
On the basis of the generation probability of a summary , we define the loss function as the likelihood loss :
𝐿(𝑓 ( 𝑋 , 𝑅 ) , 𝑦 ) = − log 𝑃 ( 𝑦∣𝑋 ) .
( 4 )
The above sequential definition approach can be well captured by the Plackett Luce Model [ 8 ] . Specifically , we propose to define 𝑃 ( 𝑥𝑦(1)∣𝑋 ) and 𝑃 ( 𝑥𝑦(𝑗)∣𝑋∖𝑆𝑗−1 ) as follows .
𝑃 ( 𝑥𝑦(1)∣𝑋 ) =
𝑃 ( 𝑥𝑦(𝑗)∣𝑋∖𝑆𝑗−1 ) = exp{𝑓 ( 𝑥𝑦(1))} ∑𝑛 𝑘=1 exp{𝑓 ( 𝑥𝑦(𝑘))} , ( 𝑥𝑦(𝑗 ) , 𝑅)} exp{𝑓𝑆𝑗−1 ∑𝑛 𝑘=𝑗 exp{𝑓𝑆𝑗−1
( 𝑥𝑦(𝑘 ) , 𝑅)} .
( 5 )
( 6 )
Incorporating Eq ( 5 ) and Eq ( 6 ) into Eq ( 3 ) , the probability of a ranking list is formulated as follows .
𝑃 ( 𝑦∣𝑋 ) =
𝑛∏
𝑗=1
∑𝑛 exp{𝑓𝑆𝑗−1 𝑘=𝑗 exp{𝑓𝑆𝑗−1
( 𝑥𝑦(𝑗 ) , 𝑅)}
( 𝑥𝑦(𝑘 ) , 𝑅)} , where 𝑆0 = 𝜙 , 𝑓𝜙(𝑥 , 𝑅 ) = 𝜔𝑇
𝑟 𝑥 .
( 7 )
931
Algorithm 1 Optimization Algorithm Input : training data {(𝑋𝑖 , 𝑅𝑖 , 𝑦𝑖)}𝑁 𝑖=1 , parameter : learning rate 𝜂 , tolerance rate 𝜖
Output : model vector : 𝜔𝑟 , 𝜔𝑑 1 : Initialize parameter value 𝜔𝑟 , 𝜔𝑑 2 : repeat 3 : 4 : 5 : 6 :
Shuffle the training data for 𝑖 = 1 , , 𝑁 do
( 𝑖 ) and Δ𝜔𝑑 ( 𝑖 ) Compute gradient Δ𝜔𝑟 Update model : 𝜔𝑟 = 𝜔𝑟 − 𝜂 × Δ𝜔𝑟 ( 𝑖 ) , 𝜔𝑑 = 𝜔𝑑 − 𝜂 × Δ𝜔𝑑 ( 𝑖 ) end for 7 : Calculate likelihood loss on the training set 8 : 9 : until the change of likelihood loss is below 𝜖
C . Learning and Prediction
Based on the definitions of ranking function and loss function , we present the learning and prediction process in this subsection . Specifically , we first introduce the optimization procedure , and then we show how to make predictions based on the learned ranking function . {(𝑋𝑖 , 𝑅𝑖 , 𝑦𝑖)}𝑁 ⎧⎨ − 𝑁∑ ⎩
𝑖=1 , the total loss is represented as follows . exp{𝜔𝑇 𝑟 𝑥(𝑖 ) ∑𝑛𝑖 𝑘=𝑗 exp{𝜔𝑇
𝑦(𝑗 ) + 𝜔𝑇 𝑟 𝑥(𝑖 )
𝑆(𝑖 ) 𝑗−1 𝑦(𝑘 ) + 𝜔𝑇 𝑑 ℎ
))} ( 𝑅𝑦(𝑘 )
1 ) Optimization :
⎫⎬ ⎭ training
( 𝑅𝑦(𝑗 )
Given
𝑛𝑖∑ data
𝑑 ℎ
)} the log
𝑗=1
𝑖=1
𝑖
𝑖
𝑆(𝑖 ) 𝑗−1
We employ Stochastic Gradient Descent ( SGD ) to conduct minimization as shown in Algorithm 1 , and the gradient at summary 𝑋𝑖 is computed as follows . 𝑟 𝑥(𝑖 )
𝑦(𝑘 ) + 𝜔𝑇
𝑛𝑖∑
)}
𝑑 ℎ
⎧⎨ ⎩
𝑦(𝑘 ) exp{𝜔𝑇 𝑟 𝑥(𝑖 )
∑𝑛𝑖 𝑘=𝑗 𝑥(𝑖 ) ∑𝑛𝑖 𝑘=𝑗 exp{𝜔𝑇 𝑦(𝑗 ) exp{𝜔𝑇 exp{𝜔𝑇 𝑟 𝑥(𝑖 )
− 𝑥(𝑖 )
𝑆(𝑖 ) 𝑗−1
𝑖
𝑆(𝑖 ) 𝑗−1 ( 𝑅𝑦(𝑘 )
( 𝑅𝑦(𝑘 ) )} )}
𝑖
𝑖
( 𝑅𝑦(𝑗 ) )}
𝑆(𝑖 ) 𝑗−1 ( 𝑅𝑦(𝑗 )
𝑖
⎫⎬ ⎭ ,
𝑦(𝑘 ) + 𝜔𝑇
𝑑 ℎ
𝑟 𝑥(𝑖 )
𝑦(𝑗 ) + 𝜔𝑇
𝑑 ℎ
𝑦(𝑗 ) + 𝜔𝑇
𝑑 ℎ
𝑆(𝑖 ) 𝑗−1
Δ𝜔(𝑖 )
𝑟 =
𝑗=1
⎧⎨ ∑𝑛𝑖 𝑛𝑖∑ 𝑘=𝑗 ℎ ⎩
𝑗=1
Δ𝜔(𝑖 )
𝑑 =
𝑖
𝑆(𝑖 ) 𝑗−1
( 𝑅𝑦(𝑘 ) ∑𝑛𝑖 𝑘=𝑗 exp{𝜔𝑇 ( 𝑅𝑦(𝑗 ) exp{𝜔𝑇
𝑖
𝑆(𝑖 ) 𝑗−1
− ℎ
) exp{𝜔𝑇
𝑟 𝑥(𝑖 )
𝑦(𝑘 ) + 𝜔𝑇
𝑟 𝑥(𝑖 ) ) exp{𝜔𝑇
𝑦(𝑘 ) + 𝜔𝑇
𝑑 ℎ
𝑆(𝑖 ) 𝑗−1 𝑦(𝑗 ) + 𝜔𝑇 𝑑 ℎ
𝑟 𝑥(𝑖 )
𝑟 𝑥(𝑖 )
𝑦(𝑗 ) + 𝜔𝑇
𝑑 ℎ
𝑆(𝑖 ) 𝑗−1
𝑑 ℎ
( 𝑅𝑦(𝑘 ) 𝑆(𝑖 ) 𝑗−1 )} ( 𝑅𝑦(𝑘 )
𝑖
𝑖
)}
( 𝑅𝑦(𝑗 ) 𝑆(𝑖 ) 𝑗−1 )} ( 𝑅𝑦(𝑗 )
𝑖
𝑖
)}
⎫⎬ ⎭ .
2 ) Prediction : In order to accommodate the sequential ranking process as described in the definition of ranking function , we propose a greedy selection procedure for prediction , as described in Algorithm 2 . Specifically , in the first step , the most relevant and salient sentence with maximal content based score will be selected and ranked first . If the top 𝑘 items have been selected , then the sentence in position 𝑘+1 should be with maximum 𝑓𝑆𝑘 . At last , a proper number of sentences are selected to generate a final summary .
Algorithm 2 Summary Generation via Greedy Selection Input : 𝑋𝑡 , 𝑅𝑡 , 𝜔𝑟 , 𝜔𝑑 Output : 𝑦𝑡 1 : Initialize 𝑆0 ← ∅ , 𝑦𝑡 = ( 1,⋅⋅⋅ , 𝑛𝑡 ) 2 : for 𝑘 = 1 , , 𝐾 do best sen ← argmax𝑥∈𝑋𝑡 𝑓𝑆𝑘−1 3 : 𝑆𝑘 ← 𝑆𝑘−1 ∪ best sen 4 : 𝑦𝑡(𝑘 ) ← the index of best sen 5 : 6 : end for 7 : return 𝑦𝑡 = ( 𝑦𝑡(1),⋅⋅⋅ , 𝑦𝑡(𝐾 ) )
( 𝑥 , 𝑅𝑡 )
IV . EXPERIMENTS
In this section , we conduct extensive experiments to evaluate the effectiveness of our proposed approach in topic focused multi document summarization . Firstly , we give some introductions on the data sets , evaluation measures , baseline methods and experimental setup . Secondly , we present our experimental results from both quantitative and qualitative aspects . version of ROUGE 2 ) recall metrics [ 28 ] as the evaluation measures , which were shown to have a high correlation with human judgements . They were also used as official automatic evaluation metrics for TAC2008 and TAC2009 . The evaluation results are obtained with version 155 of ROUGE with the default settings used for TAC2008 .
ROUGE evaluates summary quality by counting the number of overlapping units such as n gram , word sequences , and word pairs between the computer generated summary and the ideal summaries created by humans . The n gram recall metric , ROUGE N , is computed as follows .
∑
∑
𝐶𝑛𝑡𝑚𝑎𝑡𝑐ℎ(𝑔𝑟𝑎𝑚𝑛 )
𝑅𝑂𝑈 𝐺𝐸 𝑁 =
∑ 𝑆∈𝑅𝑒𝑓 𝑠
∑ 𝑔𝑟𝑎𝑚𝑛∈𝑆
𝑆∈𝑅𝑒𝑓 𝑠
𝑔𝑟𝑎𝑚𝑛∈𝑆
𝐶𝑛𝑡(𝑔𝑟𝑎𝑚𝑛 ) where 𝑛 is the length of the n gram , 𝐶𝑛𝑡𝑚𝑎𝑡𝑐ℎ(𝑔𝑟𝑎𝑚𝑛 ) is the maximum number of n grams co occurring in a candidate summary and a set of reference summaries 𝑅𝑒𝑓 𝑠 , and 𝐶𝑛𝑡(𝑔𝑟𝑎𝑚𝑛 ) is the number of n gram in the reference summaries .
A . Datasets and Evaluation Metric
B . Baseline Methods
THE STATISTICAL INFORMATION OF SET A IN TAC2008 AND TAC2009
Table II
#docs #topics #average sentences #average words data collection summary length
TAC2008
TAC2009
480 48 22.1 22.5
440 44 21.6 22.3
AQUAINT 2 100 words
AQUAINT 2 100 words
TMDS has been one of main task in Text Analysis Conference ( TAC ) , which is hold by NIST1 for several years . In our experiments , we use the benchmark datasets of TAC2008 and TAC2009 , which provide 48 topics and 44 topics respectively . Each topic was associated with 20 relevant documents from the AQUAINT 2 collection of news articles . The documents were further equally divided into two sets : set A and set B . All the documents in set A chronologically preceded the documents in set B . The detailed statistical information of set A is present in Table II . In TMDS task , a 100 word summary is required to be generated for each set of documents . The summary of set A should be a topic focused multi document summary , and that of B should be a update summary . Here we focus on the performance comparison of topic focused multi document summarization ( the summary of set A ) , and do not show the summarization results of set B .
In our work , we use the ROUGE 1 ( unigram based ) , ROUGE 2 ( bigram based ) and ROUGE SU4 ( an extended
1http://wwwnistgov
We compare our proposed R LTR method with the stateof the art approaches as described in Section 2 , including Leading Sentence Selection ( LEAD ) , Maximal Marginal Relevance ( MMR ) , Personalized PageRank ( PPR ) , Manifold Ranking ( MR ) , DivRank ( DR ) , GrassHopper ( GH ) , Supervised Lazy random walk ( SuperLazy ) and Support Vector Machine ( SVM ) .
LEAD . It is a simple baseline method that selects the leading sentences of documents to compose the summary . It can be viewed as a lower bound of extractive approach for TMDS task .
MMR . It employs a linear combination of relevance and diversity as the metric called “ marginal relevance ” [ 4 ] . MMR will iteratively select sentences with the largest “ marginal relevance ” .
PPR . PPR is a widely used random walk based ranking approach [ 29 ] , which is appropriate for identifying relevant and salient sentences for summary .
MR . MR is a graph based ranking approach using the graph regularization [ 30 ] . It is capable of giving higher ranks to the salient sentences which are close to the topic on the manifold .
DR . DR is also a graph based ranking approach which uses a vertex reinforced random walk [ 6 ] . It focuses on achieving diversity in sentence ranking for summary .
GH . GH conducts on absorbing random walk over the graph [ 17 ] . It aims to capture diversity in ranking by iteratively select top ranked nodes and set it as absorbing state .
SuperLazy . SuperLazy is a supervised approach based on sentence graph [ 1 ] . It learns the strength of edges and self
932
THE SUMMARIZATION PERFORMANCE BASED ON 4 FOLD CROSS VALIDATION ON TAC2009
Table III
Method LEAD MMR MR PPR DR GH SVM
SuperLazy R LTR𝑚𝑖𝑛 R LTR𝑎𝑣𝑔 R LTR𝑚𝑎𝑥
Method LEAD MMR MR PPR DR GH SVM
SuperLazy R LTR𝑚𝑖𝑛 R LTR𝑎𝑣𝑔 R LTR𝑚𝑎𝑥
ROUGE 1 0.30192
0.34291 ( +13.58 % ) 0.35499 ( +17.58 % ) 0.36163 ( +19.78 % ) 0.36571 ( +21.13 % ) 0.36802 ( +21.89 % ) 0.35889 ( +18.87 % ) 0.38210 ( +26.56 % ) 0.40874 ( +35.38 % ) 0.40135 ( +32.93 % ) 0.40627 ( +34.56 % )
ROUGE 2 0.06311
0.07915 ( +25.42 % ) 0.08612 ( +36.46 % ) 0.08490 ( +34.53 % ) 0.08283 ( +31.25 % ) 0.08927 ( +41.45 % ) 0.09103 ( +44.24 % ) 0.10857 ( +72.03 % ) 0.12964 ( +105.42 % ) 0.12761 ( +102.20 % ) 0.12548 ( +98.83 % )
THE SUMMARIZATION PERFORMANCE ON TAC2008
Table IV
ROUGE 1 0.28889
0.33666 ( +16.53 % ) 0.36312 ( +25.69 % ) 0.36045 ( +24.77 % ) 0.36548 ( +26.51 % ) 0.36549 ( +26.52 % ) 0.36092 ( +24.93 % ) 0.36739 ( +27.17 % ) 0.38927 ( +34.75 % ) 0.38654 ( +33.80 % ) 0.38576 ( +33.53 % )
ROUGE 2 0.05871
0.07533 ( +28.31 % ) 0.09402 ( +60.14 % ) 0.08889 ( +51.41 % ) 0.09077 ( +54.61 % ) 0.09039 ( +53.96 % ) 0.09432 ( +60.65 % ) 0.09645 ( +64.28 % ) 0.11576 ( +97.17 % ) 0.11043 ( +88.09 % ) 0.10924 ( +86.07 % )
ROUGE SU4 0.09869
0.11138 ( +12.86 % ) 0.12072 ( +22.32 % ) 0.12497 ( +26.63 % ) 0.12306 ( +24.69 % ) 0.12469 ( +26.34 % ) 0.12794 ( +29.64 % ) 0.14245 ( +44.34 % ) 0.15873 ( +60.83 % ) 0.15461 ( +56.66 % ) 0.15394 ( +55.98 % )
ROUGE SU4 0.09300
0.11383 ( +22.40 % ) 0.12910 ( +38.81 % ) 0.12708 ( +36.65 % ) 0.12897 ( +38.68 % ) 0.13017 ( +39.97 % ) 0.12814 ( +37.78 % ) 0.13070 ( +40.54 % ) 0.14765 ( +58.76 % ) 0.14386 ( +54.69 % ) 0.14205 ( +52.74 % ) loops , and then conducts the lazy random walk on such a weighted sentence graph .
SVM . SVM is widely used as a binary classifier [ 18 ] . It is a supervised approach that generally used to distinguish summary sentences from non summary sentences . Additional steps are taken to remove redundant sentences .
According to the different ways in defining the diversity function ℎ𝑆(𝑅𝑖 ) in section III , our R LTR methods will have three variants , denoted as R LTR𝑚𝑖𝑛 , R LTR𝑎𝑣𝑔 , and R LTR𝑚𝑎𝑥 , respectively .
C . Experimental Setup
In our experiments , we view each candidate sentence as a “ short ” document , and then use Indri toolkit ( version 5.2)2 as the retrieval platform for indexing and basic feature extraction . For data preprocessing , we apply Porter stemmer and stopwords removing for topics and sentences .
For training , we leverage the human extracted summaries as ground truth . NIST has provided a set of manual summarization results on TAC2009 , which is extracted by a team of five human “ sentence extractors ” from the University of Montreal . This kind of summary can be viewed as an approximate upper bound on what can be achieved with a purely extractive summarizer . Therefore , we used
2http://lemurproject.org/indri such human extracted summaries as ground truth for our approach and two other supervised baseline methods such as SuperLazy and SVM .
For the topic set on TAC2009 , we use a 4 fold cross validation for training and testing . The learning rate 𝜂 parameter in Algorithm 1 is chosen from 10−7 to 10−1 , and the best learning rate is chosen based on the performance of training . For testing , we permute the folds until all the folds have been chosen for the test set , and the final performance is reported as the average over all test data . For all the baseline methods , we set their parameters ( if exist ) to be the values as they can achieve their best performances on ROUGE 2 evaluation metric .
D . Quantitative Evaluation
1 ) Evaluation Results on TAC2009 : We first conduct performance comparison based on the dataset TAC2009 . The 4 fold cross validation results are demonstrated in Table III . The number in the parentheses are the relative improvements compared with the baseline method LEAD . Boldface indicates the highest scores among all runs .
From the results we can see that , our three methods all outperform metric based sentence ranking method , ie MMR , in terms of all the ROUGE metrics consistently . Specifically , the relative improvement over the ROUGE 1 ,
933
ROUGE 2 and ROUGE SU4 scores of our best method RLTR𝑚𝑖𝑛 are 19.20 % , 63.79 % and 42.51 % , respectively . It indicates that our approach can better capture the relevance , salience and diversity by using rich features in a supervised way , as compared with the heuristically predefined way in MMR . Meanwhile , our R LTR approaches also outperform the graph based methods . For example , when compared with the best performed graph based approach SuperLazy , the relative improvements over the ROUGE 1 , ROUGE 2 and ROUGE SU4 scores of our best method R LTR𝑚𝑖𝑛 are 6.97 % , 19.41 % and 11.43 % , respectively . It shows that better summary can be produced by explicitly model the content and relation based features , as compared with the implicit way in graph based methods .
Furthermore , our approach is also better than SVM . The relative improvement over the ROUGE 1 , ROUGE 2 and ROUGE SU4 scores of our best method R LTR𝑚𝑖𝑛 are 13.89 % , 42.41 % and 24.07 % , respectively . This demonstrates that by modeling the sequential generation procedure of extracted summary in a ranking perspective , we can learn a much better summarization model than that in a classification view . We further conduct statistical tests on the results , which indicates that all these improvements are statistically significant ( 𝑝 𝑣𝑎𝑙𝑢𝑒 < 001 )
It can also be seen that the R LTR𝑚𝑖𝑛 obtains better performance than the other two variants of our R LTR approach . It indicates that when defining the diversity function ℎ𝑆(𝑅𝑖 ) , the minimal distance would be a better choice , yet the performance differences among them are very small .
2 ) Evaluation Results on TAC2008 : We conduct experiments on dataset TAC2008 to further evaluate the effectiveness of our model . Since there is no human extracted summarization results as ground truth on dataset TAC2008 , we train our model based on dataset TAC2009 and use dataset TAC2008 as the test set . In this way , we can further evaluate the generalization ability of our approach . The evaluation results are shown in Table IV . The evaluation results again demonstrate that our methods can achieve much better performance than both the state of the art supervised and unsupervised approaches in terms of all the ROUGE metrics . This is consistent with the results we obtained on dataset TAC2009 . The results also indicate that our approach has strong generalization ability on different datasets .
E . Qualitative Evaluation
Besides the quantitative evaluation in above subsection , we also conduct qualitative comparison based on the content of summary generated by our approaches and baseline methods to get a more intuitive understanding . We randomly choose a topic on dataset TAC2009 as example . The topic and its description is , “ glendale–describe the glendale train crash , the cause of the crash , and the arrest and trial of the man accused of causing it . ” We show four manual summaries provided by NIST as reference summaries , and the sum maries generated by our approach and three representative baselines in Table V .
We manually annotated the important facts covered by the reference summaries with circled numbers and identified in total 11 topic related facts in the references . We then manually annotated these facts in the summaries generated by our approach and baseline methods . Due to the page limitation , here we list the statistics of the fact coverage of R LTR𝑚𝑖𝑛 and three baseline methods ( ie LEAD , GH , and SuperLazy ) in Table VI . The Facts column shows the major related words , the Weight column shows the times that the corresponding fact has appeared in the four reference summaries , and the rest of columns record the fact coverage status of different approaches .
The results in Table V show that our R LTR approach covers the most facts as compared to other baselines . Specifically , our R LTR approach captures all the four facts with the highest weight ( ie , weight = 4 ) , and 3 out of 4 important facts ( weight = 3 ) , which can strongly prove the effectiveness of our approach . Among baseline methods , SuperLazy and GH have shown comparable performance . GH method also captures all the facts with the highest weight , but its total number of facts covered is less than our approach . The LEAD method performs worst , and only covers one fact with the highest weight .
V . CONCLUSION
In this paper , we propose a novel relational learning torank approach for the task of topic focused multi document summarization . Our approach models the diversity relationship among sentences , besides the content information of sentences .
Firstly , we define the ranking function as the combination of content based score and relation based score between the current sentences and those previously selected . Secondly , the loss function is defined as the likelihood of the groundtruth summary based on Plackett Luce model , which can naturally model the sequential ranking procedure of candidate sentences . On this basis , stochastic gradient descent is employed to conduct the training process , and the summary is generated by a greedy selection process based on the learned ranking function . The experimental results on the benchmark data sets TAC2008 and TAC2009 demonstrate that our approach can significantly outperform the stateof the art methods from both quantitative and qualitative aspects .
For the future work , it would be interesting to adapt the learning method to the update summarization task , and also test our approach on other types of dataset beyond the traditional documents , eg , the prevalent short texts in social media .
ACKNOWLEDGMENT
This research work was funded by the 973 Program of China under Grants No . 2012CB316303 and No .
934
Topic
RefSum I
RefSum II
RefSum III
RefSum IV
LEAD
GH
SuperLazy
R LTR𝑚𝑖𝑛
QUALITATIVE COMPARISON ON A EXAMPLE TOPIC IN TAC2009
Table V
2⃝set off a collision that derailed two trains ,
Describe the Glendale train crash , the cause of the crash , and the arrest and trial of the man accused of causing it . 1⃝Juan Alvarez left his car on a railroad track in Glendale CA . It 3⃝killed at least 10 people and injured nearly 200 . Witness identified Alvarez and 4⃝he was detained . He was distraught and 5⃝put on suicide watch . Glendale police believed 6⃝he intended to commit suicide but changed his mind . 7⃝He was taken into custody and charged with at least 10 counts of homicide . Investigator felt Alvarez wanted to kill himself because his 8⃝estranged wife denied him visits with his children . He was held on 7⃝suspicion of murder in Los Angeles without bail . 5⃝A man intend on suicide 1⃝left his car on a railroad track in Clendale , California , where it 2⃝set off a collision that derailed two commuter trains and 3⃝killed ten people and injured nearly 200 . Police held Juan Manuel Alvarez on 7⃝ten counts of murder and indicated that he 6⃝had intended suicide but changed his mind and fled his vehicle before the train struck . Alvarez , who 9⃝had self inflicted superficial wounds , remained at the scene and was cooperating with police . 10⃝He has a criminal record involving drugs and his wife obtained a 8⃝restraining order against him . Juan Manuel Alvarez 1⃝drove his SUV onto Glendale train tracks in 5⃝a suicide attempt . 6⃝he got out and left . A southbound Metrolink commuter train When his SUV got stuck between the track , 2⃝crashed into the immovable SUV and climed over it . The front cab car rammed into an adjacent stationary Union Pacific work train and toppled it . The remaining cars jackknifed and derailed a northbound Metrolink commuter train . The accident and subsequent fire 3⃝killed 11 and injured 200 . Alvarez watched the crash then attempted suicide again by 9⃝slashing his wrists and stabbing himself . He 4⃝was arrested and 7⃝charged with homicide . 2⃝A train crash in Glendale , California , on 26 January 3⃝killed 11 people and injured close to 200 , many critically . A southbound commuter train , pushed from behind by a locomotive , hit an SUV lodged on the tracks , then hit a freight train and a northbound commuter train . 6⃝jumped from the car just before the impact . At first , 9⃝self inflicted wounds on his wrists and chest led investigators to think that Juan Miguel Alvarez , 9⃝slashed and stabbed himself after seeing the horrific train crash he caused , sources close to the investigation said Thursday . Alvarez , 25 , despondent over the 8⃝breakup of his marriage had planned to kill himself when he drove his green Jeep Grand Cherokee in the path of a Metrolink train , officials said , but As he watched , southbound Train No . A man 5⃝intend on committing suicide 1⃝left his car on a railroad track in Glendale near downtown Los Angeles Wednesday , where it authorities said . Several passengers said they saw Alvarez 6⃝jump from his SUV just before the southbound train plowed into the vehicle , Adams said . Metrolink train 100 , a four car train being pushed by a locomotive , was southbound from Moorpark to Los Angeles and approaching the Glendale station at 6:02 morning when its front car hit Alvarez ’s Jeep . A man 5⃝intend on committing suicide 1⃝left his car on a railroad track in Glendale near downtown Los Angeles Wednesday , where it authorities said . Alvarez is accused of leaving a Jeep Cherokee on the tracks , causing the derailment of one train , which then crashed into another . Juan Miguel Alvarez , 7⃝charged with murder with special circumstances in the deaths of 11 Metrolink passengers , 9⃝slashed and stabbed himself after seeing the horrific train crash he caused , sources close to the investigation said Thursday . A man 5⃝intend on committing suicide 1⃝left his car on a railroad track in Glendale near downtown Los Angeles Wednesday , where it authorities said . The driver of the SUV , identified as Juan Manuel Alvarez , 25 , of Compton , Calif . , 4⃝was taken into custody , and police said 7⃝he would be charged with homicide . He apparently 6⃝changed his mind about killing himself , 1⃝abandoned the vehicle and watched the crash , the Glendale police chief , Randy Adams , said .
5⃝it was an attempted suicide . It was determined later that he 11⃝got the wounds after he fled the scene .
7⃝charged with murder with special circumstances in the deaths of 11 Metrolink passengers ,
2⃝set off a collision that derailed two commuter trains ,
3⃝killing at least 10 people and injuring nearly 200 ,
2⃝set off a collision that derailed two commuter trains ,
3⃝killing at least 10 people and injuring nearly 200 ,
2⃝set off a collision that derailed two commuter trains ,
3⃝killing at least 10 people and injuring nearly 200 ,
4⃝Police arrested Juan Manuel Alvarez , the driver of SUV , who had
6⃝bolted from the vehicle at the last moment .
COMPARISON ON FACT COVERAGE OVER DIFFERENT APPROACHES
Table VI
2⃝ 3⃝ 5⃝ 6⃝ 1⃝ 4⃝ 7⃝ 9⃝ 8⃝ 10⃝ 11⃝
Facts It set off a collision that derailed two trains It killed at least 10 people and injured nearly 200 He put on suicide watch He intended to commit suicide , but change his mind He left his car on a railroad track He was arrested He was charged at least 10 counts of homicide He has self inflicted superficial wounds He has a estranged wife He has criminal records involving drugs He got the wounds after he fled the scene
Weight
4 4 4 4 3 3 3 3 2 1 1
R LTR𝑚𝑖𝑛
√ √ √ √ √ √ √ √
SuperLazy
√ √ √ √ √ √
√ GH √ √ √ √
LEAD
√
√ √ √
935
2013CB329602 , National Natural Science Foundation of China under Grant No . 61232010 , No . 61003166 , and No . 61203298 , and National Key Technology R&D Program under Grants No . 2012BAH46B04 , and No . 2012BAH39B02 .
REFERENCES
[ 1 ] P . Du , J . Guo , and X . Cheng , “ Supervised lazy random walk for topic focused multi document summarization , ” in Proceedings of the 11th ICDM , ser . ICDM ’11 , 2011 , pp . 1026–1031 .
[ 2 ] X Q Cheng , P . Du , J . Guo , X . Zhu , and Y . Chen , “ Ranking on data manifold with sink points , ” IEEE Trans . on Knowl . and Data Eng . , vol . 25 , no . 1 , pp . 177–191 , Jan . 2013 .
[ 3 ] T . Ma and X . Wan , “ Multi document summarization using minimum distortion , ” in Proceedings of the 10th ICDM , ser . ICDM ’10 , 2010 , pp . 354–363 .
[ 4 ] J . Carbonell and J . Goldstein , “ The use of mmr , diversitybased reranking for reordering documents and producing summaries , ” in Proceedings of the 21st ACM SIGIR , 1998 , pp . 335–336 .
[ 5 ] S . T . Davis , J . M . Conroy , and J . D . Schlesinger , “ Occams – an optimal combinatorial covering algorithm for multidocument summarization , ” 2012 IEEE 12th ICDM Workshops , vol . 0 , pp . 454–463 , 2012 .
[ 6 ] Q . Mei , J . Guo , and D . Radev , “ Divrank : the interplay of prestige and diversity in information networks , ” in Proceedings of the 16th ACM SIGKDD , ser . KDD ’10 , 2010 , pp . 1009–1018 .
[ 7 ] T Y Liu , Learning to Rank for Information Retrieval .
Springer , 2011 .
[ 8 ] J . I . Marden , Analyzing and Modeling Rank Data . Chapman and Hall , 1995 .
[ 9 ] X . Li , Y D Shen , L . Du , and C Y Xiong , “ Exploiting novelty , coverage and balance for topic focused multi document summarization , ” in Proceedings of the 19th ACM CIKM , ser . CIKM ’10 , 2010 , pp . 1765–1768 .
[ 10 ] S . Deerwester , S . T . Dumais , G . W . Furnas , T . K . Landauer , and R . Harshman , “ Indexing by latent semantic analysis , ” JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE , vol . 41 , no . 6 , pp . 391–407 , 1990 .
[ 11 ] G . Erkan and D . R . Radev , “ Lexrank : graph based lexical centrality as salience in text summarization , ” J . Artif . Int . Res . , vol . 22 , no . 1 , pp . 457–479 , Dec . 2004 .
[ 12 ] S . Brin and L . Page , “ The anatomy of a large scale hypertextual web search engine , ” in Proceedings of the 7th WWW , ser . WWW7 , 1998 , pp . 107–117 .
[ 13 ] J . Otterbacher , G . Erkan , and D . R . Radev , “ Using random walks for question focused sentence retrieval , ” in Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing , ser . HLT ’05 , 2005 , pp . 915–922 .
[ 14 ] H . Zha , “ Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering , ” in Proceedings of the 25th ACM SIGIR , ser . SIGIR ’02 , 2002 , pp . 113–120 .
[ 15 ] J . M . Kleinberg , “ Authoritative sources in a hyperlinked environment , ” J . ACM , vol . 46 , no . 5 , pp . 604–632 , Sep . 1999 .
[ 16 ] X . Wan , J . Yang , and J . Xiao , “ Manifold ranking based topicfocused multi document summarization , ” in Proceedings of the 20th IJCAI , ser . IJCAI’07 , 2007 , pp . 2903–2908 .
[ 17 ] X . Zhu , A . B . Goldberg , J . Van , and G . D . Andrzejewski , “ Improving diversity in ranking using absorbing random walks , ” in Physics Laboratory C University of Washington , 2007 , pp . 97–104 .
[ 18 ] V . N . Vapnik , The nature of statistical learning theory . New
York , NY , USA : Springer Verlag New York , Inc . , 1995 .
[ 19 ] C . J . V . Rijsbergen , Information Retrieval , 2nd ed . Newton ,
MA , USA : Butterworth Heinemann , 1979 .
[ 20 ] D . Metzler and T . Kanungo , “ Machine learned sentence selection strategies for query biased summarization , ” in Proceedings of SIGIR Learning to Rank Workshop , 2008 .
[ 21 ] Y . Ouyang , W . Li , S . Li , and Q . Lu , “ Applying regression models to query focused multi document summarization , ” Inf . Process . Manage . , vol . 47 , no . 2 , pp . 227–237 , Mar . 2011 .
[ 22 ] J . M . Conroy and D . P . O’leary , “ Text summarization via hidden markov models , ” in Proceedings of the 24th ACM SIGIR , ser . SIGIR ’01 , 2001 , pp . 406–407 .
[ 23 ] D . Shen , J T Sun , H . Li , Q . Yang , and Z . Chen , “ Document summarization using conditional random fields , ” in Proceedings of the 20th IJCAI , ser . IJCAI’07 , 2007 , pp . 2862–2867 .
[ 24 ] L . Li , K . Zhou , G R Xue , H . Zha , and Y . Yu , “ Enhancing diversity , coverage and balance for summarization through structure learning , ” in Proceedings of the 18th WWW , ser . WWW ’09 , 2009 , pp . 71–80 .
[ 25 ] T . Hofmann , “ Probabilistic latent semantic indexing , ” in Proceedings of the 22nd ACM SIGIR , ser . SIGIR ’99 , 1999 , pp . 50–57 .
[ 26 ] T . Qin , T Y Liu , J . Xu , and H . Li , “ Letor : A benchmark collection for research on learning to rank for information retrieval , ” Inf . Retr . , pp . 346–374 , 2010 .
[ 27 ] D . Metzler and W . B . Croft , “ A markov random field model for term dependencies , ” in Proc . of the 28th ACM SIGIR , 2005 , pp . 472–479 .
[ 28 ] C Y Lin , “ Rouge : A package for automatic evaluation of summaries , ” in Text Summarization Branches Out : Proceedings of the ACL 04 Workshop , July 2004 , pp . 74–81 .
[ 29 ] T . H . Haveliwala , “ Topic sensitive pagerank , ” in Proceedings of the 11th WWW , ser . WWW ’02 , 2002 , pp . 517–526 .
[ 30 ] D . Zhou ,
J . Weston , A . Gretton , O . Bousquet , and B . Scholkopf , “ Ranking on data manifolds , ” in Advances in Neural Information Processing Systems 16 . MIT Press , 2004 .
936
