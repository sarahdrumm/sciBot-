2013 IEEE 13th International Conference on Data Mining
Constrained Clustering : Effective Constraint Propagation with Imperfect Oracles
Xiatian Zhu1 , Chen Change Loy2 , Shaogang Gong1
1Queen Mary , University of London , London E1 4NS , UK
2The Chinese University of Hong Kong , Shatin , NT , Hong Kong xiatianzhu@eecsqmulacuk , ccloy@iecuhkeduhk , sgg@eecsqmulacuk
Abstract—While spectral clustering is usually an unsupervised operation , there are circumstances in which we have prior belief that pairs of samples should ( or should not ) be assigned with the same cluster . Constrained spectral clustering aims to exploit this prior belief as constraint ( or weak supervision ) to influence the cluster formation so as to obtain a structure more closely resembling human perception . Two important issues remain open : ( 1 ) how to propagate sparse constraints effectively , ( 2 ) how to handle ill conditioned/noisy constraints generated by imperfect oracles . In this paper we present a unified framework to address the above issues . Specifically , in contrast to existing constrained spectral clustering approaches that blindly rely on all features for constructing the spectral , our approach searches for neighbours driven by discriminative feature selection for more effective constraint diffusion . Crucially , we formulate a novel data driven filtering approach to handle the noisy constraint problem , which has been unrealistically ignored in constrained spectral clustering literature .
Keywords Constrained clustering , constraint propagation , feature selection , imperfect oracles , spectral clustering .
I . INTRODUCTION
Constrained clustering has been studied extensively [ 11 ] , [ 9 ] . The objective is to effectively exploit a small amount of supervision to help finding data partitions that capture consistent concepts as perceived by human . The supervision by oracles is typically expressed in the form of pairwise constraint , namely must link a pair of samples must be in the same cluster , and cannot link a pair of samples belong to different clusters . Though great strides have been made in this field , two important and non trivial questions remain open . ( I ) Effective sparse constraint propagation : Pairwise constraints are sparse in practice since exhaustive pairwise labelling are laborious and/or may not be available in data . Constraint propagation [ 9 ] is thus designed to propagate pairwise constraints from labelled samples to unlabelled samples for maximising the influence of constraints . Effective constraint propagation relies on robust identification of unlabelled nearest neighbours ( NN ) around the labelled samples in the feature space . Often , the NN search is susceptible to noisy or ambiguous features , especially so on image and video datasets . Trusting all the available features blindly for NN search ( as what most existing constrained clustering approaches [ 11 ] , [ 9 ] did ) is likely to result in suboptimal constraint diffusion . It is challenging to determine
1550 4786/13 $31.00 © 2013 IEEE DOI 101109/ICDM201345
1307
( a ) Ground Truth
( b ) Spectral Clustering
( c ) Our Approach
Figure 1 . ( a ) Ground truth cluster formation , with invalid pairwise constraints highlighted in red colour ; must and cannot links are represented by solid and dashed lines respectively ; ( b ) the clustering result obtained using unsupervised clustering ; ( c ) the result obtained using our method . how to propagate their influence effectively to neighbouring unlabelled points . In particular , it is non trivial to reliably identify the neighbouring unlabelled points for propagation . ( II ) Noisy constraints from imperfect oracles : Human annotators ( oracles ) may provide invalid/mistaken constraints . For instance , a portion of ‘must links’ are actually ‘cannotlinks’ and vice versa . For example , annotations or constraints obtained from online crowdsourcing services , eg Amazon Mechanical Turk , are very likely to contain errors or noises due to data ambiguity , unintentional human mistakes or even intentional errors by malicious workers . Learning such constraints blindly may result in sub optimal cluster formation . Most existing methods make an unrealistic assumption that constraints are acquired from perfect oracles thus they are noise free . It is non trivial to quantify and determine which constraints are noisy prior to clustering .
To address the above issues , we formulate a novel COnstraint Propagation Random Forest ( COP RF ) , not only capable of effectively propagating sparse pairwise constraints , but also able to identify and thus filter noisy constraints produced by imperfect oracles . The COP RF is flexible in that it generates an affinity matrix that encodes the constraint information for existing spectral clustering methods [ 10 ] for the subsequent constrained clustering .
More precisely , the proposed model allows for effective sparse constraint propagation through using the NN samples that are found in discriminative feature subspaces , rather than those that found considering the whole feature space , which can be suboptimal due to noisy and ambiguous features . This is made possible by introducing a new objective/split function into COP RF , which searches for discriminative features that induce the best data subspaces while simultaneously considering the model parameters that best satisfy the pairwise constraints imposed . To identify and filter noisy constraints generated from imperfect oracles , we introduce a filtering mechanism to discover consistent constraint subsets that incur less internal conflict with one another and more coherent with the underlying data distribution . This is achieved through quantifying the information gain induced by individual constraints during tree node splitting in COP RF . Figure 1 shows an example to illustrate how a COP RF is capable of discovering data partitions close to the ground truth clusters despite it is provided only with sparse and noisy constraints .
The sparse and noisy constraint issues are inextricably linked but no existing constrained clustering methods , to our knowledge , address them in a unified framework . This is the very first study that proposes a principled data driven approach to address them jointly . In particular , our work makes the following contributions : ( I ) We formulate a novel discriminative feature driven approach for effective sparse constraint propagation . Existing methods fundamentally ignore the role of feature selection in this problem . ( II ) We propose a data driven method to filter potentially noisy constraints , a problem that is largely unaddressed by existing constrained clustering algorithms . All these capabilities are achieved using a single unified COF RF model .
We evaluate the effectiveness of the proposed approach on UCI and video datasets . We demonstrate that the COP RF is superior when compared to the state of the art constrained clustering algorithms [ 11 ] , [ 5 ] , [ 9 ] in exploiting sparse constraints . In addition , we show that the proposed model , unlike existing methods , is capable of performing robust clustering even when noisy pairwise constraints are included in the learning process .
II . EFFECTIVE CONSTRAINT PROPAGATION
A . Problem Formulation
Given a set of samples denoted as X = {xi} , i = 1 , . . . , N , with N referring to the total number of samples , and xi = ( xi,1 , . . . , xi,d ) ∈ F , d the feature dimensionality of the feature space F ⊂ R d , the goal of unsupervised clustering is to assign each sample xi with a cluster label ci . In constrained clustering , additional pairwise constraints are available to influence the cluster formation . There are two typical types of pairwise constraints
Must link Cannot link
: M = {(xi , xj ) | ci = cj} , : C = {(xi , xj ) | ci '= cj} .
( 1 ) We denote the full constraint set as P = M ∪ C , and the cardinality of P as |P| . The pairwise constraints may arise from pairwise similarity as perceived by a human annotator ( oracle ) , temporal continuity , or prior knowledge on the sample class label . Acquiring pairwise constraints from a human annotator is expensive . In addition , owing to data ambiguity and human unintentional mistakes , the pairwise constraints are likely to be incorrect and inconsistent with the underlying data distribution .
( a )
Features of data
Pairwise constraints
Tree 1
Spectral clustering
( b )
( c )
( d )
( e )
Constraint filtering
CO on COnstraint Propagation
Random Forest
( COP RF )
…
Affinity matrix i
… Clusters
Tree
Figure 2 . Overview of our approach .
We propose a model that can flexibly generate a constraint aware affinity matrix , which is directly employed by existing spectral clustering methods as input for constrained clustering ( Figure 2 ) . Before detailing our model we briefly describe the conventional random forests .
B . Conventional Random Forests Classification forests A general form of random forests is the classification forests . A classification forest [ 2 ] is an ensemble of Tclass binary decision trees T ( x ) : F → R K , K = [ 0 , 1]K denoting the space of class probability with R distribution over the label space L = {1 , . . . , K} . Tree training : Decision trees are learned independently from each other , each with a random training set X t ⊂ X , iebagging [ 2 ] . Growing a decision tree involves a recursive node splitting procedure until some stopping criterion is satisfied , eg the number of training samples arriving at a node is equal to or smaller than a threshold φ , and leaf nodes are then formed , and their class probability distributions are estimated with the labels of the arrival samples as well .
The training of each internal ( or split ) node is a process of optimising a binary split function defined as
. h(x , Θ ) =
0 1 if xϑ1 < ϑ2 , otherwise .
( 2 )
This split function is parameterised by two parameters : ( i ) a feature dimension ϑ1 ∈ {1 , . . . , d} , and ( ii ) a feature threshold ϑ2 ∈ R . All arrival samples of a split node will be channelled to either the left or right child node , according to the output of Equation ( 2 ) . The optimal split parameter ∗ is chosen via Θ
∗
Θ
= argmax {Θi}mtry i=1
ΔIclass ,
( 3 ) where {Θi} represents the parameter space over mtry randomly selected features . That is , multiple candidate data splittings are performed on mtry random feature dimensions during the above optimisation process . Typically , a greedy search strategy is exploited to identify Θ
∗ .
1308
The information gain ΔIclass is formulated as
ΔIclass = Is − |L|
|S|Il − |R| |S|Ir ,
( 4 ) where s , l , r refer to a split node , the left and right child node , respectively . The sets of data routed into l and r are denoted as L and R , and S = L ∪ R as the sample set residing at s . The I can be computed as either the entropy or Gini impurity [ 2 ] . In this study we utilise the Gini impurity due to its simplicity and efficiency . Clustering forests In contrast to classification forests , clustering forests [ 8 ] , [ 7 ] require no ground truth information during the training phase . A clustering forest consists of Tclust binary decision trees . The leaf nodes in each tree define a spatial partitioning of the training data . Interestingly , the training of a clustering forest can be performed using the classification forest optimisation approach by adopting the pseudo two class algorithm [ 2 ] , [ 7 ] . Specifically , we add N uniformly distributed pseudo samples ¯x = {¯x1 , . . . , ¯xd} , with ¯xi ∼ U ( xi|min ( xi ) , max ( xi ) ) into the original data space X . With this strategy , the clustering problem becomes a canonical classification problem that can be solved by the classification forest training method as discussed above .
C . Our Model : Constraint Propagation Random Forest
To address the issues of sparse and noisy constraints , we formulate a novel COnstraint Propagation Random Forest ( COP RF ) ( see Figure 2 ) . We consider using a random forest , particularly a clustering forest [ 2 ] , [ 7 ] as the basis to derive our new model for two main reasons : ( I ) It has been shown that random forest has a close connection with adaptive k nearest neighbour methods , as a forest model adapts neighbourhood shape according to the local importance of different input variables [ 6 ] . This motivates us to exploit the adaptive neighbourhood shape1 for effective constraint propagation . ( II ) The forest model also offers a way to evaluate information gain of the underlying data distribution . We can build upon it to quantify the consistency between constraints and the data distribution effectively , which could be useful in identifying noisy constraints .
The proposed COP RF differs significantly from the conventional random forests in that the COP RF is formulated with a new split function , which considers not only the bottom up data information gain maximisation , but also the joint satisfaction of top down pairwise constraints . Next , we discuss the mechanism to achieve effective sparse constraint propagation though discriminative feature subspaces . Propagation via discriminative feature subspaces We construct a COP RF through learning a collection of Tc constraint aware COP trees . Similar to the training of an ordinary decision tree , to train a COP tree we optimise the ∗ with both the best split function ( Equation ( 2 ) ) by finding Θ 1The neighbours of a data x in forest interpretation are the points which fall into the same child node .
= Mt ∪ Ct . feature dimension and cut point to partition its node training samples S . The difference is that the term ‘best’ or ‘optimal’ is no longer defined only as to maximising the bottom up information gain , but also simultaneously satisfying the imposed top down pairwise constraints . More precisely , at the t th COP tree , its training set X t only encompasses a subset of the full constraint set P , ie P t = {Mt ∪ Ct} ⊂ P . Instead of using the information gain in Equation ( 4 ) , we optimise each internal node s in a COP tree via Equation ( 3 ) with the information gain ΔI defined as follow |S|Il − |R| |S|Ir , ∀(xi , xj ) ∈ Ct ⇒ ci '= cj , maximise ΔI = Is − |L| st ∀(xi , xj ) ∈ Mt ⇒ ci = cj ∈ {l , r} , where xi , xj ∈ S , P t Equation ( 5 ) differs significantly from the conventional information gain function [ 2 ] , [ 7 ] as the maximisation is bounded by the constraint set P t . More specifically , it automatically selects discriminative features and their optimal cut point via information based energy optimisation , whilst at the same time fulfilling the guiding conditions imposed by pairwise constraints . Algorithm 1 summarises the split function optimisation procedure in a COP tree . Effective constraint propagation occurs when we construct a constraintaware data affinity matrix for spectral clustering [ 10 ] , taking into account the discriminative neighbourhoods induced by individual COP trees . Combining with spectral clustering Conventionally , an affinity matrix is constructed by computing pairwise distance with some Euclidean based measure . It is however observed in some studies that the Euclidean distance is often not an accurate representation of the underlying shape of data [ 3 ] . In addition , defining data neighbourhoods via the whole feature space can be susceptible to noisy features .
( 5 )
The learned COP RF offers an effective way to derive a more meaningful affinity matrix , which not only defines data similarity through discriminative feature subspaces , but also encodes the pairwise constraint information . Note that the t th COP tree only considers a subset of constraints P t but not the full constraint P . Nevertheless , since a different tree considers a random set of P t ( due to the random set X t ) , a good coverage of all constraints can be achieved by averaging many trees’ statistics .
Formally , each individual tree within a COP RF partitions d → L ⊂ N , where the training samples at its leaves '(x ) : R ' represents a leaf index and L refers to the set of all leaves in a given tree . For each COP tree , we first compute a treelevel N × N affinity matrix At with elements defined as At i,j = exp
−distt(xi,xj ) with
0 +∞ if '(xi ) = '(xj ) , otherwise .
( 6 )
. t dist
( xi , xj ) =
1309
1 Optimisation : 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 end 20 21 22 end 23 24 { end end else end fiTc
We assign the maximum affinity ( affinity=1 , distance=0 ) to points xi and xj if they fall into the same leaf , and the minimum affinity ( affinity=0 , distance=+∞ ) otherwise . By averaging all the tree level affinity matrices we obtain a smooth matrix as A = 1 t=1 At . with Ai,i = 0 . We Tc then construct a sparse k NN graph , whose edge weights are defined by A ( Figure 2 c ) . Since the affinity matrix A is constraint encoded , using the k NN graph as input readily transforms the conventional spectral clustering methods [ 10 ] for constrained clustering . Filtering noisy constraints from imperfect oracles Constraint propagation should be reinforced by noisy constraint filtering to avoid error propagation to neighbouring unlabelled points . To this end , we formulate a novel method to identify noisy constraints through quantifying constraint inconsistency by the information gain criterion . Specifically , an inconsistent constraint is likely to
• Conflict with the majority of other constraints , assum ing that most constraints are valid .
• Disagree with the underlying data distribution .
During the tree node splitting , we observe that satisfying constraints that disagree with the underlying data distribution would incur sub optimal data partition , leading to low data information gain . Motivated by this observation , we exploit the information gain measure to filter possibly noisy constraints from the set P t . The filtering process does not physically remove the suspected noisy constraints . But it is a process that is conducted at the root node of each COP tree t , so that only selected constraints S t ⊂ P t will be used to perform the root data partition {L0 , R0} . This partition would ‘set a good starting point’ for the subsequent data splittings in tree branches . As compared to physically removing suspected constraints , this scheme is more conservative but empirically gives better results .
Next , we describe the steps to estimate the consistency of a pairwise constraint and subsequently the way to determine S t . A conflict will only occur when we consider multiple constraints together . Hence , to better quantify the degree to which a constraint conflicts with other randomly selected constraints and the underlying data distribution , we repeat the following steps for f repetitions . For a repetition i 1 ) Randomly sample a temporary subset of constraints Q from P t , Q ⊂ P t , where |Q| = α|P t| and 0 < α < 1 . 2 ) Compute the information gain δI following Algorithm 1 by using Q as the constraint set2 . For any j th constraint in the set P t , we assign its induced information gain δI i
.
δI 0
δI i j = j as the j th constraint ∈ Q , otherwise .
( 7 )
2δI is computed in a similar way of ΔI in Equation 5 . We use a different symbol for clarity .
1310
Algorithm 1 : Split function optimisation in a COP tree . Input : At a split node s of a COP tree t : Training samples available to s : S ; Pairwise constraints : P t = Mt ∪ Ct ; The best feature cut point Θ∗ The associated child node partition {L , R} ;
Output : and ;
Initialise L = R = ∅ and ΔI = 0 ; for var ← 1 to mtry do
Select a feature fvar ∈ {1 , . . . , d} ; for each possible cut point of the feature fvar do
Split S into a candidate partition { ˆL , ˆR} ; dec = respect all constraints({ ˆL , ˆR} , if dec is true then
.Mt , Ct ) ; Compute information gain Δ ˆI following Equation ( 5 ) ; if Δ ˆI > ΔI then ;
Update Θ∗ Update ΔI = Δ ˆI , L = ˆL , and R = ˆR . fi end
Ignore the current splitting . if No valid splitting found then
A leaf is formed . function respect all constraints({L , R} , {M , C} )
∀(xi , xj ) ∈ M , ∀(xi , xj ) ∈ C , if ( xi ∈ L and xj ∈ R , or vice versa ) , return false . if ( xi , xj ∈ L , or xi , xj ∈ R ) , return false .
Otherwise , return true .
25 26 27 28 29
30 }
Each repetition employs a different random subset sampled from P t . Now the consistency of the j th constraint in the set P t can be quantified by the corresponding information gain averaged across f repetitions , ie δI i j ,
'δI j = fff
( 8 )
1 r i=1 where r is the number of times the j th constraint is selected value in 'δI . Consequently , the optimal constraint subset S t within the f repetitions . A noisy constraint would have a low highest values in 'δI . In this study we set α = 0.5 and is selected as the top α × |P t| constraints that achieve the f = 500 so that each individual constraint has a fair chance to be paired with other constraints .
III . EXPERIMENTAL SETTINGS
Datasets To evaluate the effectiveness of our method in coping with data of varying numbers of dimensions and clusters , we select five diverse UCI benchmark datasets [ 1 ] . We also collect an intrinsically noisy video dataset from a publicly available web camera deployed in a university ’s Educational Resource Center ( ERCe ) . This dataset consists of 600 video clips with six possible clusters of events ( see Figure 3 for example images ) . The details of all datasets are summarised in Table I .
Table I
THE DETAILS OF DATASETS .
Dataset
Ionosphere ( Iono . )
Segmentation ( Seg . ) Parkinsons ( Park . )
Iris
Glass ERCe
# Clusters
2 3 7 2 6 6
# Features
34 4 19 22 10 2672
# Instances
351 150 210 195 214 600
( a )
( d )
( b )
( e )
( c )
( f )
Figure 3 . Example images from the ERCe video dataset . It contains six events including ( a ) Student Orientation , ( b ) Cleaning , ( c ) Career Fair , ( d ) Group Study , ( e ) Gun Forum , and ( f ) Scholarship Competition .
Features For the UCI datasets , we use the original features provided . As for the ERCe video data , we segment a long video into non overlapping clips , from which a number of visual features are then extracted , including colour , local texture , optical flow , holistic image features and object detections . We perform PCA on the resulting 2672 D feature vectors of video clips , and use the first 30 PCA components as the final representation . All raw features are scaled to the range of [ 1,1 ] . Baselines For comparison , we present the results of ( 1 ) Spectral Clustering ( SPClust ) [ 10 ] , which does not exploit any pairwise constraint ; ( 2 ) COP Kmeans [ 11 ] , a popular constrained clustering method based on k means ; ( 3 ) Spectral Learning ( SL ) [ 5 ] , a constrained spectral clustering method without constraint propagation . It extends SPClust by trivially adjusting the elements in a data affinity matrix with 1 and 0 to satisfy must link and cannot link constraints , respectively ; ( 4 ) a state of the art constrained spectral clustering approach E2CP [ 9 ] , in which constraint propagation is achieved by manifold diffusion [ 12 ] ; and ( 5 ) Forest + E2CP – we modify E2CP [ 9 ] , ie instead of generating the data affinity matrix with Euclidean based measure , we use a conventional clustering forest to generate the affinity matrix . This allows E2CP to enjoy a limited capability of feature selection using a random forest model . Evaluation metrics We use the widely adopted adjusted Rand Index ( ARI ) [ 4 ] as the evaluation metric . Throughout all the experiments , we report the ARI values averaged over 10 trials . In each trial we generate a random pairwise constraint set from the ground truth cluster labels . Implementation details The number of trees , Tc , in a COP RF is set to 1000 . Each X t is obtained by performing N times of random selection with replacement ( see Section II B ) . The depth of each COP tree is governed by either constraint satisfaction , ie a node will stop growing if during any attempted data partition some constraints are
1311
√ violated ( see Algorithm 1 ) , or the size of a node equals d to 1 ( ie φ = 1 ) . We set mtry ( see Equation ( 3 ) ) to with d the feature dimensionality of the input data and employ a linear data separation as the split function ( see Equation ( 2) ) . We set k ≈ N/10 for the k nearest neighbour graph construction .
IV . EVALUATIONS
We conduct comparative experiments to ( 1 ) evaluate the effectiveness of different clustering methods in exploiting sparse but perfect pairwise constraints ( Section IV A ) , and ( 2 ) compare their clustering performances in the case of having imperfect oracles to provide ill conditioned pairwise constraints ( Section IV B ) . A . Evaluation of Sparse Constraint Propagation
In this experiment , we assume perfect oracles thus all the pairwise constraints agree with the ground truth cluster labels . Figure 4 reports the ARI curves of different methods along with varying numbers of pairwise constraints from 20 to 100 . The overall performance of various methods can be quantified by the area under the ARI curve and the results are reported in Table II . It is evident from the results ( Figure 4 and Table II ) that on most datasets , the proposed COP RF outperforms other baselines , by as much as >300 % against COP Kmeans3 and >30 % against the state of the art E2CP in averaged area under the ARI curve .
COMPARING DIFFERENT METHODS BY THE AREA UNDER THE ARI CURVE . PERFECT ORACLES ARE ASSUMED . HIGHER IS BETTER .
Table II
Dataset
SPClust
Iono . Iris Seg . Park . Glass ERCe Average
0.43 3.47 1.96 0.78 1.14 2.76 1.76
COPKmeans 0.65 0.55 0.36 0.21 0.62 0.84 0.54
SL
0.23 3.53 1.96 0.83 1.21 2.74 1.75
E2CP
0.37 3.54 1.99 1.06 1.36 2.40 1.79
Forest + E2CP 2.48 3.49 2.20 1.35 1.67 3.01 2.37
COP RF
2.15 3.51 2.19 1.45 2.22 3.06 2.43
It is worth pointing out that although the state of theart E2CP performs generally better than other baselines , it is inferior to the proposed COP RF , since its manifold construction still considers the full feature space , which may be corrupted by noisy features . We observe in some cases , such as the challenging ERCe dataset , the performance of E2CP is worse than the naive SL method that comes without constraint propagation . This result suggests that propagation could be harmful when the feature space is noisy . The variant modified by us , ie Forest + E2CP , employs a conventional clustering forest ( [2 ] , [ 7 ] ) to generate the data affinity matrix . This allows E2CP to take advantage of a limited capability of forest based feature selection , and better results are obtained compared with the pure E2CP . Nevertheless ,
3COP Kmeans fails to converge ( early termination without a solution ) on datasets Iris , Segmentation , and Glass .
SPClust E2CP SL
COP−Kmeans Forest+E2CP COP−RF
40
60
80
100
Number of Pairwise Constraints
( a ) Ionosphere
SPClust E2CP SL
COP−Kmeans Forest+E2CP COP−RF
SPClust E2CP SL
COP−Kmeans Forest+E2CP COP−RF 80
40
60
Number of Pairwise Constraints
100
( b ) Iris
SPClust E2CP SL
COP−Kmeans Forest+E2CP COP−RF
I
R A
I
R A
1 0.8 0.6 0.4 0.2 0 20
1 0.8 0.6 0.4 0.2 0 20
1 0.8 0.6 0.4 0.2 0 20
I
R A
I
R A
I
R A
1 0.8 0.6 0.4 0.2 0 20
1 0.8 0.6 0.4 0.2 0 20
1 0.8 0.6 0.4 0.2 0 20
40
60
80
100
Number of Pairwise Constraints
40
60
80
100
Number of Pairwise Constraints
( c ) Segmentation
( d ) Parkinsons
SPClust E2CP SL
COP−Kmeans Forest+E2CP COP−RF
I
R A
40
60
80
100
Number of Pairwise Constraints
SPClust E2CP SL
COP−Kmeans Forest+E2CP COP−RF 80
40
60
Number of Pairwise Constraints
100
( e ) Glass
( f ) ERCe
Figure 4 . ARI comparison : comparison of clustering performance between different methods given a varying number of perfect pairwise constraints .
Forest + E2CP ’s performance is generally poorer than COPRF ’s ( see Table II ) . This is because the feature selection of the ordinary forest model is less effective than that of COPRF , which jointly considers information gain maximisation and constraint satisfaction . B . Evaluation on Filtering Noisy Constraints
In this experiment , we assume imperfect oracles thus pairwise constraints are noisy . We deliberately prepare constraint sets that are mixed with a fixed ratio ( 15 % ) of random invalid constraints that disagree with the ground truth . This is to simulate the annotation behaviour of imperfect oracles for the comparison of our approach with existing models . We repeat the same experimental protocol as discussed in Section IV A . It is observed from Table III that in spite of the imperfect oracle assumption , COP RF again achieves better results than other constrained clustering models on most datasets as well as the best average clustering performance across datasets , eg >300 % increase against COP Kmeans and >35 % increase against E2CP . Furthermore , Table III also shows that COP RF maintains encouraging performance given noisy constraints , in some cases such as the challenging ERCe video dataset even larger improvements are obtained over E2CP and other models , than that when perfect constraints are provided . The results suggest the effectiveness of the proposed constraint filtering algorithm in coping with noisy constraints .
1312
COMPARING DIFFERENT METHODS BY THE AREA UNDER THE ARI CURVE . IMPERFECT ORACLES ARE ASSUMED . HIGHER IS BETTER .
Table III
Datasets
SPClust
Iono . Iris Seg . Park . Glass ERCe Average
0.43 3.47 1.96 0.78 1.14 2.76 1.76
COPKmeans 0.59 0.52 0.64 0.14 0.21 0.79 0.48
SL
0.22 3.52 1.96 0.82 1.15 1.21 1.47
E2CP
0.23 3.51 1.97 0.94 1.31 1.19 1.52
Forest + E2CP 1.56 3.13 2.02 1.05 1.35 2.31 1.90
COP RF
1.38 3.39 2.11 1.11 1.68 2.81 2.08
V . CONCLUSION
We have presented a unified constrained spectral clustering framework to ( 1 ) propagate sparse constraints effectively , and ( 2 ) handle noisy constraints generated by imperfect oracles . The proposed COP RF model is novel in that it propagates constraints more effectively via discriminative feature subspaces . This is in contrast to existing methods that perform propagation considering the whole feature space , which may be corrupted by noisy features . Effective propagation regardless of the constraint quality could lead to poor clustering results . Our work addresses this crucial issue by formulating a way to quantify the inconsistency of constraints and effectively filter potentially noisy ones before propagation takes place . The model is flexible in that it generates a constraint aware affinity matrix that can be used by the popular spectral clustering methods for constrained clustering . Experimental results on various datasets have demonstrated the advantages of the proposed approach over the state of the art constrained clustering methods .
REFERENCES
[ 1 ] A . Asuncion and D . Newman . UCI machine learning repos itory , 2007 .
[ 2 ] L . Breiman . Random forests . ML , 45(1):5–32 , 2001 . [ 3 ] A . Frome , Y . Singer , and J . Malik .
Image retrieval and classification using local distance functions . In NIPS , 2007 . [ 4 ] L . Hubert and P . Arabie . Comparing partitions . Journal of
Classification , 2(1):193–218 , 1985 .
[ 5 ] S . D . Kamvar , D . Klein , and C . D . Manning .
Spectral learning . In IJCAI , 2003 .
[ 6 ] Y . Lin and Y . Jeon . Random forests and adaptive nearest neighbors . Journal of the American Statistical Association , 101(474):578–590 , 2006 .
[ 7 ] B . Liu , Y . Xia , and P . S . Yu . Clustering through decision tree construction . In CIKM , pages 20–29 , 2000 .
[ 8 ] C . Liu , S . Gong , C . C . Loy , and X . Lin .
Person reidentification : what features are important ? In International Workshop on Re Identification , ECCV , pages 391–401 , 2012 . [ 9 ] Z . Lu and H . H . Ip . Constrained spectral clustering via In ECCV , exhaustive and efficient constraint propagation . pages 1–14 , 2010 .
[ 10 ] A . Y . Ng , M . I . Jordan , Y . Weiss , et al . On spectral clustering :
Analysis and an algorithm . NIPS , 2:849–856 , 2002 .
[ 11 ] K . Wagstaff , C . Cardie , S . Rogers , and S . Schr¨odl . Constrained k means clustering with background knowledge . In ICML , pages 577–584 , 2001 .
[ 12 ] D . Zhou ,
J . Weston , A . Gretton , O . Bousquet , and B . Sch¨olkopf . Ranking on data manifolds . NIPS , 16:169– 176 , 2004 .
