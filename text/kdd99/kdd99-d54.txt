The Application of AdaBoost for Distributed , Scalable and On line Learning
Wei Fan
Columbia University wfan@cscolumbiaedu
Salvatore J . Stolfo Columbia University sal@cscolumbiaedu
Junxin Zhang
Columbia University jzhang@cscolumbiaedu
Abstract
We propose to use AdaBoost to efficiently learn classifiers over very large and possibly distributed data sets that cannot fit into main memory , as well as on line learning where new data become available periodically . Empirical studies on four real world and artifical data sets have shown results that are either comparable to or better than learning classifiers over the complete training set and , in some cases , are comparable to boosting on the complete data set . However , our algorithms use much smaller samples of the training set and require much less memory .
1
Introduction
Learning from very large and distributed databases imposes major performance challenges for data mining . Freund and Schapire ’s AdaBoost [ 2 ] learns a highly accurate weighted voting ensemble of many “ weak ” hypotheses whose accuracy is only moderate . Most prior work on AdaBoost focuses on improving the accuracy of a weak classifier on the same single chunk of data at a central site that is small enough to fit into main memory . There hasn’t been much research on using AdaBoost for scalable , distributed or online learning . The major difference of our work from previous research is that each weak classifier is not trained from the same data set at each round but only a small portion of the training set .
2
Scalable and Distributed Learning
In AdaBoost , the weak learner is treated as a “ blackbox ” . We don’t have much control over it except for observing its accuracy and providing it with a different training sample at each round according to its accuracy in previous rounds . Each weak learner may freely choose examples in the sample given to it . Conversely ,
• Given : S = {(x1 , y1 ) , . . . , ( xm , ym)} ; xi ∈ X , yi ∈
1 . Randomly choose n examples from S without replace ment .
2 . Re normalize their weights into a distribution , D′ t . 3 . Train weak learner using distribution D′ t for the chosen
• Initialize D1(i ) ( such as D1(i ) = 1 • For t = 1 , . . . , T : m )
{−1 , +1} and sample size n .
4 . Compute weak hypothesis ht : X →R .
!
TX for the complete training set S .
• Output the final hypothesis :
Dt+1(i ) = n examples .
5 . Update
Dt(i)exp
− αtyiht(xi )
Zt
H(x ) = sign
αtht(x ) t=1
Figure 1 : r sampling AdaBoost we can also give the weak learner a sample which is just a small “ portion ” of the complete training set . The weak classifier produced from the small portion of the training set is very likely “ weaker ” than one generated from the entire weighted training set , but the overall accuracy of the voted ensemble can still be boosted .
In order to increase accuracy , the small portion should not be a highly skewed sample of the complete training set . We propose two methods . The presentation of both algorithms follow that of AdaBoost in [ 4 ] . In r sampling ( Figure 1 , r is random ) , a fixed number ( n ) of examples are randomly picked from the weighted training set ( without replacement ) together with their assigned weights . All examples have equal chance of being selected , and Dt is not taken into account in this selection . At different rounds , a new r sample is picked . In d sampling ( Figure 2 , d is disjoint ) , the weighted training set is partitioned into p disjoint subsets . Each subset is a d sample . At each round , a different dsample is given to the weak learner . The weights of the chosen examples of both r and d sampling are renormalized to make them a distribution , D′ t . A weak classifier is generated from these examples with distribution D′ t . The update of weights and calculation of αt are performed on the entire data set with distribution Dt . Details on the choice and the calculation of α can be found in [ 4 ] . Both methods can be used for learning over very large data sets , but d sampling is more suitable for distributed learning where data at each site cannot be culled together to a single site . The data at each site are taken as a d sample . Weak learning at each round is carried out independently of all the local data of each site . On the other hand , r sampling would choose data from different sites and transfer them to a single site . This is not possible for some applications and inefficient in general .
When all data are available at a local site , both the distribution Dt and classifier weight αt are calculated exactly as in AdaBoost . We use all the locally available training data in the computation . However , for distributed learning over many sites , in order to calculate αt , it requires either the predictions by the weak classifiers at all sites be brought to a single site or possibly several distributed computations of Z ′ t ( for a candidate of αt ) among different sites . We use the second approach since the load on network traffic is much less .
The extra storage to run both r sampling and dsampling is a floating point vector Dt(i)(i : 1 , . . . , m ) to record the weight of each training example . The size of the vector is linear in the size of the training set , fixed before run time and can be pre allocated . The memory required to hold this vector is usually much smaller than that of the data set . If main memory is unavailable , this vector can be disk resident . This is still efficient since the access patterns to Dt are strictly sequential to both update weights and calculate αt .
The extra computing power is to label the entire data set at each round of boosting , update weights and calculate αt . All these can be done efficiently in main memory . Predicting the data set is sequential and linear in the size of the data set . In network implementations , the extra network traffic to send partial values of Z ′ t is nominal .
3
On line Learning
In on line learning , there is a steady flow of new instances generated periodically or in real time that ought to be incorporated to correct or update the model previously learned . We propose an on line learning scheme using AdaBoost as a means to re weight classifiers in an ensemble , and thus to reuse previously computed classifiers along with new classifier computed on a new increment of data .
The basic idea is to reuse previously learned weak classifiers {h1 , . . . , hT −1} and learn a new classifier from the weighted new data ( Figure 3 ) . When the increment ST arrives , we use the weight updating rule to both “ reweight ” previous classifiers based on their accuracy on m ) .
1 . Re normalize the weight of partition St mod p to make
• Given : partition number p . • Initialize D1(i ) ( such as D1(i ) = 1 • Divide S into p partitions {S0 , . . . , Sp−1} . • For t = 1 , . . . , T :
3 . Compute weak hypothesis ht : X →R .
!
TX for the complete training set S .
• Output the final hypothesis : it a distribution , D′ t .
2 . Train weak learner using distribution D′ t .
4 . Update
Dt(i)exp
− αtyiht(xi )
Dt+1(i ) =
Zt
H(x ) = sign
αtht(x ) t=1
Figure 2 : d sampling AdaBoost
ST and generate a weighted training set . Data items that are not accurately predicted by these hypotheses receive higher weights . A new classifier hT is trained from this weighted increment . At the end of this procedure , we have α1 , . . . , αT −1 for h1 , . . . , hT −1 , a new classifier hT and its weight αT calculated from the weighted training set ST during successive updates of the distribution . The interesting observation is that AdaBoost provides a means of assigning weights to classifiers based on a validation set that these classifiers are not necessarily trained from . It also identifies data items that these classifiers perform poorly on and generates a weighted increment accordingly .
One problem with the above on line learning method is that we are forced to retain all the classifiers previously learned . This naturally increases our memory requirements and slows the learning and classifying procedures . We may use Margineantu and Dietterich ’s pruning method [ 3 ] to prune the voted ensemble to speed up classification . We propose to use a “ window ” of a fixed number of classifiers to solve this problem . Instead of retaining all classifiers , we only use and store the k most recent classifiers that reflect the most recent online data . The value of k can either be fixed or change according to the amount of resource and accuracy requirement . The number of new data items accumulated on line before on line learning starts can either be fixed or change at runtime as well .
This on line learning scheme with a window size of k is efficient . The extra overhead involves k classifications of the increment with weight updating and k + 1 computations of α . The predicting and weight updating procedure is linear in the size of the data increment . The computation of α has a bounded number of linear computations ( to calculate Z ′ ) .
4
Experiment
We compare the error rate of r and d sampling to the error rate over learning and boosted learning from
1 . Choose αt ∈R .
• Compute weak hypothesis hT : X →R .
TX
• Output the final hypothesis : for the new increment ST .
Dt+1(i ) =
2 . Update
Dt(i)exp
• Given : {h1 , . . . , hT −1} and ST ( |ST | = mT ) . • Initialize D1(i ) ( such as D1(i ) = 1 mT • For t = 1 , . . . , T − 1 :
)
• Train weak learner using distribution DT .
!
− αtyiht(xi )
Zt
H(x ) = sign
αtht(x ) t=1
Figure 3 : On line AdaBoost
Data Set
ADULT
BOOLEAN
WHIRL CHASE
Continuous Discrete Feature
Feature
Class
6 0 14 14
8 15 6 6
2 2 2 2
Training
Testing
Size 32561 25000 50407 40841
Size 16281 7768 12602 41439
Table 1 : Data Set Summary the entire data set , and calculate the change in error rate over each boosting round and the amount of computation . It is also important to see if we can obtain the same level of error by simply AdaBoost on the same randomly chosen small sample with equal size and number of rounds as used by d and r sampling . For on line learning , it is interesting to know if the online algorithm actually performs better than a simple classifier learned over the new increment or the global classifier learned over all the training data including the new increment .
4.1
Experimental Set up
Four data sets ( summarized in Table 1 ) are used in this study . The sample sizes for both d and r sampling are chosen to range from 1 256 of the original training set and the rounds of boosting range over 4 , 8 to 512 .
4 to 1 s = 1
2 , 1 s = 1
We don’t have any real on line data set on hand . Instead , we have used a simulated on line flow of data similar to d sampling . Each d sample is taken as an increment . The size of the window k is arbitrarily selected to be 10 and the size of the increment is chosen 1 from 1 256 of the entire training set . At the start of the flow of on line data , we don’t have k classifiers . In this case , we reuse all available classifiers . We used Cohen ’s RIPPER [ 1 ] as the “ weak ” learner . We used the Laplace estimate to generate the confidence for each rule . We carefully engineered the bisection search algorithm to calculate αt .
32 to
Base Error 14.8 14.8 14.8 14.8 14.8 14.8 14.8 14.8
1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2
10.7 10.7 10.7 10.7 10.7 10.7 10.7 10.7
11.6 11.6 11.6 11.6 11.6 11.6 11.6 11.6
Boosted
Base 14.7 14.7 14.7 14.7 14.7 14.7 14.7 14.7
0.13 0.13 0.13 0.13 0.13 0.13 0.13 0.13
5.98 5.98 5.98 5.98 5.98 5.98 5.98 5.98
11.3 11.3 11.3 11.3 11.3 11.3 11.3 11.3
ADULT Boosted Sample
14.8 15.5 14.9 15.4 16.6 18.6 19.2 20.8
WHIRL 0.587
0.270
0.246
0.579
1.10 2.13 2.48 4.52
BOOLEAN
8.55
7.67
7.72
9.14
10.0
10.4 11.2 11.1
CHASE 11.2
11.1 11.6 11.7 13.5 12.4 15.1 14.1 d r
14.7 14.8 14.7
14.7 14.8 14.8 14.8 14.9
14.7
14.7
14.6
14.6 14.8 14.9 14.8 15.1
0.270
0.214
0.183
0.153
0.294
0.294
0.421
0.381
0.674
0.579
0.762
0.674
0.823
0.762
0.786
0.786
8.47
7.47
6.49
7.05
6.68
7.60
7.58
7.26
11.2
11.2
11.1 11.6 12.5 11.9 12.6 12.4
8.26
6.84
7.07
7.49
7.89
8.37
8.07
7.83
11.2
11.2
11.0 11.6 11.8 12.1 12.7 12.4
1 s
2 4 8 16 32 64 128 256
2 4 8 16 32 64 128 256
2 4 8 16 32 64 128 256
2 4 8 16 32 64 128 256
Table 2 : d and r Sampling AdaBoost Error Rate Summary ( % )
1 s
32 64 128 256
32 64 128 256
32 64 128 256
32 64 128 256
Baseline
Sample Avg
Perfect Good
Bad
14.8 14.8 14.8 14.8
1.2 1.2 1.2 1.2
10.7 10.7 10.7 10.7
11.6 11.6 11.6 11.6
16.8 19.3 22 22.5
2.45 4.1 4.6 6.5
13.5 13.6 14.4 17.5
13.2 14.6 15 18
ADULT
15.9 15.8 15.3 15.7 WHIRL 1.22 1.30 2.04 3.00 BOOLEAN 10.0 10.9 12.2 13.7
CHASE
11.4 11.4 11.0 11.2
18.8 28.1 42.2 38.7
53.1 56.3 11.7 6.25
81.3 40.6 26.6 22.3
56.3 57.8 59.4 59.0
53.1 68.8 56.3 58.6
43.8 43.8 86.7 92.2
18.8 56.2 56.3 60.5
40.6 39.1 36.7 39.1
28.1 3.1 1.56 2.73
3.13
0
1.56 1.56
0 3.1 17.2 17.2
3.13 3.13 3.91 1.95
4.2
Results for d and r Sampling
The results for d and r sampling are summarized in Table 2 . The detailed plots ( Figure 4 for BOOLEAN and
Table 3 : On line AdaBoost with window size k=10 Error Rate Summary ( % ) others are available from cscolumbiaedu/∼wfan ) show the change in error rate with respect to sample size and boosting rounds . For each data set and each chosen sample size , the last two columns of Table 2 ( under “ d ” and “ r ” ) lists the error rate for d and rsampling . As a comparison , the table also lists , from the first to third columns , the baseline error rate of a single RIPPER classifier learned from all the available data ( under “ Baseline Error ” ) , the boosted baseline error rate of 10 rounds AdaBoost RIPPER on all available data ( under “ Boosted Base ” ) and the boosted sample error rate of running AdaBoost RIPPER on the “ same ” small randomly chosen sample with equal size and number of rounds as used by d and r sampling ( under column “ Boosted Sample ” ) . In each detailed plot , the x axis is the rounds of boosting and the y axis is the error rate . For all data sets , we draw a plot for every chosen sample size . There are five curves in each detailed plot . Two horizontal lines are the baseline error rate and the boosted baseline error rate ( lower one ) . The other three lines display the results for d sampling ( drawn with lines points ) , r sampling and boosted sample ( both drawn with lines ) . In most cases , the curves of d and r sampling are twisted together and hard to separate . The boosted sample curve is mostly high above d and r sampling lines and becomes very flat with increasing boosting rounds .
From the curves and the summary table , we can see that d and r sampling AdaBoost reduce the error rate significantly . We observe that the error rates achieved by d and r sampling are either comparable to or even much lower than the baseline error rate of the global classifier . In many cases , their error rates are comparable to that of AdaBoost RIPPER on the complete training set . In Table 2 , the results lower than the baseline are highlighted in bold font . This result applies to all data sets with all sample sizes ( 1 256 ) under study . The error rates achieved by the different sample sizes for the same data set are in comparable levels , but bigger sample sizes exhibit slightly lower error rates . These observations suggest that both dand r sampling are quite robust to the change in sample size . In real world applications , we are mainly memoryconstrained . These results show that we can overcome these constraints and use AdaBoost d and r sampling to compute accurate classifiers .
2 to 1
We compare the performance of d and r sampling with simply applying AdaBoost RIPPER to the same sample . From the curves , we can see that when the sample size is “ big ” ( 1 16 ) , boosting the same sample can reduce the error rate for BOOLEAN and WHIRL . However the level of reduction is not as much as either dor r sampling . For ADULT and CHASE , simply boosting the same sample increases the error rate for bigger sample sizes . The possible reason for reducing the error
2 to 1 rate for “ big ” samples is that the sample is big enough for effective learning . When the sample size is small ( 1 32 to 1 256 ) , we observe a trend in error reduction early but it quickly flattens , yet the final resultant error rate is still significantly higher than the error rates attained by d and r sampling .
The speed of error reduction is very fast . By looking at the curves in Figure 4 and our web page , we find that the quickest error reduction usually happens in the first 5 % to 40 % of the total number of boosting rounds . This is especially true when the sample sizes are small . In practice , it means that we can stop the learning process quite early without losing much accuracy . We also see that the error rate is still slowly decreasing at the last rounds of boosting . If we had allowed more time to compute , we could have obtained even lower error rates . There isn’t much difference in performance between d sampling and r sampling for the data sets under study . The curves are mostly “ twisted ” with each other . In training sets with highly skewed distributions , we may see some difference between these two methods .
For WHIRL and CHASE with sampling sizes of 1 2 and 1 4 , the error rate of the classifier learned from the sample before boosting is even lower than that of the global classifier . This is probably because 1 4 are large enough for effective learning , and overfitting easily occurs with more data .
2 and 1
4.3
Results for On line Learning
The results are summarized in Table 3 . Detailed scatter plots are shown in Figure 5 and our webpage . In each scatter plot , we draw two lines for comparison : baseline error rate of the global classifier learned over all available data and average sample error rate . Sample error is the error of the single classifier learned on the increment itself . For each data set under study , Table 3 shows the baseline error rate ( under “ Baseline ” ) , average sample error rate ( under “ Sample ” ) , average error rate ( under “ Avg ” ) of online AdaBoost for the flow of increments , and three categories to distinguish on line AdaBoost performance . An on line AdaBoost error rate is perfect if it is lower or equal to the baseline error rate . It is bad if it is higher than the error rate of the monolithic classifier learned on the on line increment . Otherwise , it is in the good category .
In most of the cases ( 96 + % for WHIRL and CHASE , 80 + % for ADULT and BOOLEAN ) , the on line error rate are in either good or perfect categories , implying that in an overwhelming majority of the cases , on line learning has better performance than learning a single classifier from the on line increment . In many cases ( from 20 % to 60% ) , the on line error rate is even lower than the error rate of a global classifier learned over all available training data . For the CHASE data set , there
1/2 v sample 1/2 r sample Baseline Boosted Sample Boosted Baseline
0
0.5
1
1.5
Boosting Round
2
2.5
3
1/32 v sample 1/32 r sample Baseline Boosted Sample Boosted Baseline
0.11
0.1
0.09
0.08
0.07
0.06
0.05
0
1
2
0.18
0.16
0.14
0.12
0.1
0.08
0.06 r o r r
E r o r r
E
0.11
0.105
0.1
0.095
0.09
0.085
0.08
0.075
0.07
0.065
0.06
0.055
0.14
0.13
0.12
0.11
0.1
0.09
0.08
0.07
0.06
0.05
1/8 v sample 1/8 r sample Baseline Boosted Sample Boosted Baseline
1/4 v sample 1/4 r sample Baseline Boosted Sample Boosted Baseline
0.12
0.11
0.1
0.09
0.08
0.07
0.06 r o r r
E
3
4
Boosting Round
5
6
7
0.05
0
2
4
6 Boosting Round
8
10
12
14
1/64 v sample 1/64 r sample Baseline Boosted Sample Boosted Baseline
1/128 v sample 1/128 r sample Baseline Boosted Sample Boosted Baseline r o r r
E
0.16
0.15
0.14
0.13
0.12
0.11
0.1
0.09
0.08
0.07
0.06
0.05
1/16 v sample 1/16 r sample Baseline Boosted Sample Boosted Baseline
0
5
10
15
Boosting Round
20
25
30
1/256 v sample 1/256 r sample Baseline Boosted Sample Boosted Baseline
0.13
0.12
0.11
0.1
0.09
0.08
0.07
0.06
0.05
0.18
0.16
0.14
0.12
0.1
0.08
0.06 r o r r
E r o r r
E r o r r
E r o r r
E r o r r
E
0
10
20
30
40
Boosting Round
50
60
0.04
0
20
40
60
80
100
120
Boosting Round
0
50
100 150 Boosting Round
200
250
0.04
0
100
200 300 Boosting Round
400
500
Figure 4 : BOOLEAN d and r sampling Results
0.14
0.13
0.12
0.11
0.1
0.09
0.08
0
1/32 Complete Data Avg Sample
5
10
15
Boosting Round
20
25
30 r o r r
E
0.17
0.16
0.15
0.14
0.13
0.12
0.11
0.1
0.09
0.08
0.07
0.06
0
10
20
1/64 Complete Data Avg Sample
30
40
Boosting Round
50
60 r o r r
E
0.2
0.18
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0
20
40
1/256 Complete Data Avg Sample
1/128 Complete Data Avg Sample
0.25
0.2
0.15
0.1
0.05 r o r r
E
100
120
0
0
50
100 150 Boosting Round
200
250
60
80
Boosting Round
Figure 5 : BOOLEAN On line Results are many points where the error rate is much lower than the boosted base line error rate .
Comparing the change in performance for the same data set with different sample sizes ( 1 256 ) , we find that the results are quite insensitive to the size of the sample . Except for WHIRL , there is an insignificant increase in average error rate when the size decreases . The percentage of results in the bad category remains almost the same for all experiments .
32 to 1
5
Conclusion
We have given two new ways to apply AdaBoost . In the first case , we only choose samples from the complete weighted training set . This approach allows us to use AdaBoost for scalable and distributed learning . In the second case , we regard the AdaBoost weight updating formula as a way of assigning weights to classifiers in a weighted voting ensemble . We have experimented with d and r sampling as two alternatives for scalable and distributed learning . We tested them on four real world and artificial data sets . The results are in most cases comparable to or better than learning a global classifier from the complete training set and in many cases comparable to boosting the global classifier on the complete data set . However , the cost of learning and the requirements for memory are significantly lower . We also tested an on line AdaBoost with window size of 10 on the same data sets . The results suggest significant improvement from learning a single classifier on the new increment of data itself and in many cases even better than learning the global classifier where all data participates in learning . But its cost is similar to learning over the new increment data . The storage overhead for all these methods is bounded and can be pre allocated before runtime . The computation overhead is also limited .
Our full paper ( available from cscolumbiaedu/∼wfan ) compares our approach with other methods for scalable , distributed and on line learning and discusses future work directions .
References [ 1 ] W . Cohen . Fast Effective Rule Induction .
In Proc . Twelfthh Internatioanl Conference on Machine Learning , pp . 115 123 , Morgan Kaufman .
[ 2 ] Y . Freund and R . Schapire . A decision theoretic generalization of on line learning and an application to boosting . Journal of Computer and System Sciences , 55(1):119 139,1997 .
[ 3 ] D , Margineantu and T . Dietterich . Pruning Adaptive Boosting .
In Proc of ICML 97 .
[ 4 ] R . Schapire and Y . Singer . Improved boosting algorithms using confidence rated predictions . In Proceedings of the Eleventh Annual Conference on Computational Learning Theorey , 1998 .
