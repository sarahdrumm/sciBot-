Mining Features for Sequence Classification
Neal Lesh1 , Mohammed J . Zaki2 , Mitsunori Ogihara3 lesh@merl.com , zaki@csrpiedu , ogihara@csrochesteredu
1 MERL Mitsubishi Electric Research Laboratory , 201 Broadway , 8th Floor , Cambridge , MA 02139
2 Computer Science Dept . , Rensselaer Polytechnic Institute , Troy , NY 12180
3 Computer Science Dept . , U . of Rochester , Rochester , NY 14627
Abstract Classification algorithms are difficult to apply to sequential examples because there is a vast number of potentially useful features for describing each example . Past work on feature selection has focused on searching the space of all subsets of features , which is intractable for large feature sets . We adapt sequence mining techniques to act as a preprocessor to select features for standard classification algorithms such as Naive Bayes and Winnow . Our experiments on three different datasets show that the features produced by our algorithm improve classification accuracy by 10 50 % ,
Introduction
1 Some classification algorithms work well when there are thousands of features for describing each example ( e.g , [ Littlestone , 1988] ) . In some domains , however , the number of potentially useful features is exponential in the size of the examples . Data mining algorithms ( eg , [ Zaki , 1998 ] ) have been used to search through billions of rules , or patterns , and select the most interesting ones . In this paper , we adapt data mining techniques to act as a preprocessor to construct a set of features to use for classification .
In past work , the rules produced by data mining algorithms have been used to construct classifiers primarily by ordering the rules into decision lists ( eg [ Segal and Etzioni , 1994 , Liu et al . , 1998 ] ) or by merging them into more general rules that occur in the training data ( eg , In this paper , we convert the patterns discovered by the mining algorithm into a set of boolean features to feed into standard classification algorithms . The classification algorithms , in turn , assign weights to the features which allows evidence from different rules to be combined in order to classify a new example .
[ Lee et al . , 1998] ) .
While there has been a lot of work on feature selection , it has mainly concentrated on non sequential domains . In contrast , we focus on sequence data in which each example is represented as a sequence of “ events ” , where each event might be described by a set of predicates . Examples of sequence data include text , DNA sequences , web usage data , and execution traces . In this paper we combine two powerful mining paradigms : sequence mining , which can efficiently search for patterns that are correlated with the target classes , and classification , which learns to weigh evidence from different features to classify new examples . We present FeatureMine , a scalable disk based feature mining algorithm . We also specify criteria for selecting good features , and present pruning rules that allow for more efficient feature mining . FeatureMine integrates pruning constraints in the algorithm itself , instead of post processing , enabling it to efficiently search through large pattern spaces .
2
Data mining for features
We now formulate and present an algorithm for feature mining . Let F be a set of distinct features , each with some finite set of possible values . Let I be the set of all possible feature value pairs . A sequence is an ordered list of subsets of I . For example , if I = {A , B , C} , then an example sequence would be AB → A → BC . A sequence α is denoted as ( α1 → α2 → → αn ) where each sequence element αi is a subset of I . The length of sequence ( α1 → α2 → → αn ) is n and its width is the maximum size of any αi for 1 ≤ i ≤ n . We say that α is a subsequence of β , denoted as α ≺ β , if there exists integers i1 < i2 < < in such that αj ⊆ βij for all αj . For example , AB → C is a subsequence of AB → A → BC . Let C be a set of class labels . An example is a pair hα , ci where α = α1 → α2 → → αn is a sequence and c ∈ C is a label . Each example has a unique identifier eid , and each αi has a time stamp at which it occurred . An example hα , ci is said to contain sequence β if β ≺ α .
Our input database D consists of a set of examples . This means that the data we look at has multiple sequences , each of which is composed of sets of items . The frequency of sequence β in D , denoted fr(β , D ) , is the fraction of examples in D that contain β . Let β be a sequence and c be a class label . The confidence of the rule β ⇒ c , denoted conf(β , c , D ) , is the conditional probability that c is the label of an example in D given that it contains sequence β . That is , conf(β , c , D ) = fr ( β , Dc)/fr ( β , D ) where Dc is the subset of examples in D with class label c . A sequence is said to be frequent if its frequency is more than a user specified min freq threshold . A rule is said to be strong if its confidence is more than a user specified min conf threshold . Our goal is to mine for frequent and strong patterns . Figure 1 shows a database of examples . There are 7 examples , 4 belonging to class c1 , and 3 belonging to class c2 . In general there can be more than two classes . We are looking for different min freq in each class . For example , while C is frequent for class c2 , it ’s not frequent for class c1 . The rule C ⇒ c2 has confidence 3/4 = 0.75 , while the rule C ⇒ c1 has confidence 1/4 = 025
A sequence classifier is a function from sequences to C . A classifier can be evaluated using standard metrics
EID
Time
Items
Class
1
2
3
4
5
6
7
10
20
30
20
30
50
10
30
40
30
40
50
10
50
30
40
20
A B
B
A B
A C c1
A B C c1
B
A
B
A
A B
A
B
A B
A C
A
C
C c1 c1 c2 c2 c2
FREQUENT SEQUENCES
Class = c1 min_freq ( c1 ) = 75 %
A
B
A >A
100 %
100 %
100 %
AB
75 %
A >B
B >A
B >B
100 %
75 %
75 %
AB >B
75 %
Class = c2 min_freq ( c2 ) = 67 %
A
C
A >C
67 %
100 %
67 %
New Boolean Features
EID l s e p m a x E
1
2
3
4
5
6
7
A 1
1
1
1
1
1
0
A >A 1
B >A 1
1
0
1
1
0
0
0
1
1
1
0
0
B 1
1
1
1
1
0
0
AB 1
A >B 1
B >B 1
1
0
1
1
0
0
1
1
1
0
0
0
1
0
1
0
0
0
AB >B
1
1
0
1
0
0
0
C 0
1
0
0
1
1
1
A >C 0
Class c1
0
0
0
1
1
0 c1 c1 c1 c2 c2 c2
Figure 1 : A ) Original Database , B ) New Database with Boolean Features such as accuracy and coverage .
Finally , we describe how frequent sequences β1 , , βn can be used as features for classification . Recall that the input to most standard classifiers is an example represented as vector of feature value pairs . We represent a example sequence α as a vector of featurevalue pairs by treating each sequence βi as a boolean feature that is true iff βi α . For example , suppose the features are f1 = A → D , f2 = A → BC , and f3 = CD . The sequence AB → BD → BC would be represented as hf1 , truei , hf2 , truei , hf3 , f alsei . Note that features can “ skip ” steps : the feature A → BC holds in AB → BD → BC .
Selection criteria for mining
2.1 We now specify our selection criteria for selecting features to use for classification . Our objective is to find sequences such that representing examples with these sequences will yield a highly accurate sequence classifier . However , we do not want to search over features [ Caruana and the space of all subsets of Freitag , 1994] ) , but instead want to evaluate each feature in isolation or by pair wise comparison to other candidate features . Certainly , the criteria for selecting features might depend on the domain and the classifier being used . We believe , however , that the following domain and classifier independent heuristics are useful for selecting sequences to serve as features : 1 ) Features should be frequent . 2 ) Features should be distinctive of at least one class . 3 ) Feature sets should not contain redundant features . The intuition behind the first heuristic is simply that rare features can , by definition , only rarely be useful for classifying examples . In our problem formulation , this heuristic translates into a requirement that all features have some minimum frequency in the training set . Note that since we use a different min freq for each class , patterns that are rare in the entire database can still be frequent for a specific class . We only ignore those patterns which are rare for any class . The intuition for the second heuristic is that features that are equally likely in all classes do not help determine which class an example belongs to . Of course , a conjunction of multiple non distinctive features can be distinctive . In this case , our algorithm prefers to use the distinctive conjunction as a feature rather than the non distinctive conjuncts . We encode this heuristic by requiring that each selected feature be significantly correlated with at least one class that it is frequent in .
The motivation for our third heuristic is that if two features are closely correlated with each other , then either of them is as useful for classification as both are together . We show below that we can reduce the number of features and the time needed to mine for features by pruning redundant rules . In addition to wanting to prune features which provide the same information , we also want to prune a feature if there is another feature available that provides strictly more information . Let M ( f , D ) be the set of examples in D that contain feature f . We say that feature f1 subsumes feature f2 with respect to predicting class c in data set D iff M ( f2 , Dc ) ⊆ M ( f1 , Dc ) and M ( f1 , D¬c ) ⊆ M ( f2 , D¬c ) . Intuitively , if f1 subsumes f2 for class c then f1 is superior to f2 for predicting c because f1 covers every example of c in the training data that f2 covers and f1 covers only a subset of the non c examples that f2 covers . The third heuristic leads to two pruning rules , in our feature mining algorithm described below . The first pruning rule is that we do not extend ( i.e , specialize ) any feature with 100 % accuracy . Let f1 be a feature contained by examples of only one class . Specializations of f1 may pass the frequency and confidence tests in the definition of feature mining , but will be subsumed by f1 . The following lemma captures this pruning rule : Lemma 1 : If fi ≺ fj and conf(fi , c , D ) = 1.0 then fi subsumes fj with respect to class c .
Our next pruning rule concerns correlations between individual items . Recall that the examples in D are represented as a sequence of sets . We say that A ; B in examples D if B occurs in every set in every sequence in D in which A occurs . The following lemma states that if A ; B then any feature containing a set with both A and B will be subsumed by one of its generalizations , and thus we can prune it : Lemma 2 : Let α = α1 → α2 → → αn where A , B ∈ αi for some 1 ≤ i ≤ n . If A ; B , then α will be subsumed by α1 → αi−1 → ( αi − B ) → αi+1 → αn . Feature mining : We can now define the feature mining task . The inputs to the FeatureMine algorithm are a set of examples D and parameters min f req , maxw , and maxl . The output is a non redundant set of the frequent and distinctive features of width maxw and length maxl . Formally : Given examples D and parameters min f req , maxw , and maxl return feature set F such that for every feature fi and every class cj ∈ C , if length(fi ) ≤ maxl and width(fi ) ≤ maxw and fr ( β , Dcj ) ≥ min freq(cj ) and conf ( β , cj , D ) is significantly greater ( via chi squared test ) than |Dc|/|D| then F contains fi or contains a feature that subsumes fi with respect to class cj in data set D .
Efficient mining of features
2.2 We now present the FeatureMine algorithm which leverages existing data mining techniques to efficiently mine features from a set of training examples . FeatureMine is based on the recently proposed SPADE algorithm [ Zaki , 1998 ] for fast discovery of sequential patterns . SPADE is a scalable and disk based algorithm that can handle millions of example sequences and thousands of items . Consequently FeatureMine shares these properties as well . To construct FeatureMine , we adapted the SPADE algorithm to search databases of labeled examples . FeatureMine mines the patterns predictive of all the classes in the database , simultaneously . As opposed to previous approaches that first mine millions of patterns and then apply pruning as a post processing step , FeatureMine integrates pruning techniques in the mining algorithm itself . This enables it to search a large space , where previous methods would fail .
FeatureMine uses the observation that the subsequence relation defines a partial order on sequences . If α ≺ β , we say that α is more general than β , or β is more specific than α . The relation is a monotone specialization relation with respect to the frequency f r(α , D ) , ie , if β is a frequent sequence , then all subsequences α β are also frequent . The algorithm systematically searches the sequence lattice spanned by the subsequence relation , from general to specific sequences , in a depth first manner .
3
AB >B
4
A >A
3
B >A
3 AB
4
A >B
3
B >B
A >C
A
4
B
4
C
{ }
( Intersect A >B and B >B )
( Intersect A and B )
B Time 10 20 30 30 50 30 30 50 10
EID 1 1 1 2 2 3 4 4 5
A
Time 10 30 20 30 10 40 30 40 10 50 30
EID 1 1 2 2 3 3 4 4 5 5 6
AB >B
EID 1 2 4
Time 10 30 30
B >B
EID Time 1 10 1 20 30 2 4 30
A >B
EID 1 2 2 3 4 4
Time 10 20 30 10 30 40
FREQUENT SEQUENCE LATTICE
ORIGINAL ID LIST DATABASE
SUFFIX JOINS ON ID LISTS
EID
Class
1 c1
2 c1
3 c1
4 c1
5 c2
6 c2
7 c2 frequency(c1 ) frequency(c2 )
CLASS INDEX TABLE
A 4
2
B 4
1
A >B 4
0
B >B AB >B
3
0
3
0
FREQUENCY TABLE
Figure 2 : Sequence Lattice and Frequency Computation Frequency Computation : FeatureMine uses a vertical database layout , where we associate with each item X in the sequence lattice its idlist , denoted L(X ) , which is a list of all example IDs ( eid ) and event time ( time ) pairs containing the item . Given the sequence idlists , we can determine the support of any k sequence by simply intersecting the idlists of any two of its ( k−1 ) length subsequences . A check on the cardinality of the resulting idlist tells us whether the new sequence is frequent or not . Figure 2 shows that the idlist for A → B is obtained by intersecting the lists of A and B , ie , L(A → B ) = L(A ) ∩ L(B ) . Similarly , L(AB → B ) = L(A → B ) ∩ L(B → B ) . We also maintain the class index table indicating the classes for each example . Using this table we are able to determine the frequency of a sequence in all the classes at the same time . For example , A occurs in eids {1 , 2 , 3 , 4 , 5 , 6} . However eids {1 , 2 , 3 , 4} have label c1 and {5 , 6} have label c2 . Thus the frequency of A is 4 for c1 , and 2 for c2 . The class frequencies for each pattern are shown in the frequency table .
To use only a limited amount of main memory FeatureMine breaks up the sequence search space into small , independent , manageable chunks which can be processed in memory . This is accomplished via suffixbased partition . We say that two k length sequences are in the same equivalence class or partition if they share a common k − 1 length suffix . The partitions , such as {[A ] , [ B ] , [ C]} , based on length 1 suffixes are called parent partitions . Each parent partition is independent in the sense that it has complete information for generating all frequent sequences that share the same suffix . For example , if a class [ X ] has the elements Y → X , and Z → X . The possible frequent sequences at the next step are Y → Z → X , Z → Y → X , and ( Y Z ) → X . No other item Q can lead to a frequent sequence with the suffix X , unless ( QX ) or Q → X is also in [ X ] .
FeatureMine(D , min f req(ci) ) :
P = { parent partitions , Pi} for each parent partition Pi do EnumerateFeatures(Pi )
EnumerateFeatures(S ) : for all elements Ai ∈ S do for all elements Aj ∈ S , with j > i do
R = Ai ∪ Aj ; L(R ) = L(Ai ) ∩ L(Aj ) ; if RulePrune(R , maxw , maxl ) == FALSE and f requency(R , ci ) ≥ min f req(ci ) for any ci
T = T ∪ {R} ; F = F ∪ {R} ;
EnumerateFeatures(T ) ;
RulePrune(R , maxw , maxl ) : if width(R ) > maxw or length(R ) > maxl return TRUE ; if accuracy(R ) == 100 % return TRUE ; return FALSE ;
Figure 3 : The FeatureMine Algorithm
Feature Enumeration : FeatureMine processes each parent partition in a depth first manner , as shown in the pseudo code of Figure 3 . The input to the procedure is a partition , along with the idlist for each of its elements . Frequent sequences are generated by intersecting the idlists of all distinct pairs of sequences in each partition and checking the cardinality of the resulting idlist against min sup(ci ) . The sequences found to be frequent for some class ci at the current level form partitions for the next level . This process is repeated until we find all frequent sequences . Integrated Constraints : FeatureMine integrates all pruning constraints into the mining algorithm itself , instead of applying pruning as a post processing step . As we shall show , this allows FeatureMine to search very large spaces efficiently , which would have been infeasible otherwise . The Rule Prune procedure eliminates features based on our two pruning rules , and also based on length and width constraints . While the first pruning rule has to be tested each time we extend a sequence with a new item , there exists a very efficient one time method for applying the A ; B rule . The idea is to first compute the frequency of all 2 length sequences . Then if P ( B|A ) = f r(AB)/f r(A ) = 1.0 , then A ; B , and we can remove AB from the suffix partition [ B ] . This guarantees that AB will never appear together in any set of any sequence .
Empirical evaluation
3 We now describe experiments to test whether the features produced by our system improve the performance of the Winnow [ Littlestone , 1988 ] and Naive Bayes [ Duda and Hart , 1973 ] classification algorithms . We ran experiments on three datasets . In each case , we experimented with various settings for min f req , maxw , and maxl to generate reasonable results . We report the values used , below . Random parity problems : We first describe a nonsequential problem on which standard classification algorithms perform very poorly . The problem consists of N parity problems of size M with L distracting , or irrelevant , features . For every 0 ≤ i ≤ N and 0 ≤ j ≤ M , there is a boolean feature Fi,j . Additionally , for 0 ≤ k ≤ L , there is an irrelevant , boolean feature Ik . To generate an instance , we randomly assign each boolean feature true or false with 50/50 probability . An example instance for N = 3 , M = 2 , and L = 2 is ( F1,1=true , F1,2=false , F2,1=true , F2,2=true , F3,1=false , F3,2=false , I1=true,I2= false ) . There are N × M + L features , and 2N ×M+L distinct instances . We also choose N weights w1 , , wN which are used to assign each instance one of two class labels ( ON or OFF ) as follows . An instance is credited with weight wi iff the ith set of M features has an even parity . That is , the “ score ” of an instance is the sum of the weights wi for which the number of true features in fi,1 , fi,M is even . If an instance ’s score is greater than half the sum of all the weights , PN i=1 wi , then the instance is assigned class label ON , otherwise it is assigned OFF . Note that if M > 1 , then no feature by itself is at all indicative of the class label ON or OFF , which is why parity problems are so hard for most classifiers . The job of FeatureMine is essentially to figure out which features should be grouped together . Example features produced by FeatureMine are ( f1,1=true , f1,2=true ) , and ( f4,1=true , f4,2=false ) . We used a min f req of .02 to .05 , maxl = 1 and maxw = M . Forest fire plans : The FeatureMine algorithm was originally motivated by the task of plan monitoring in stochastic domains . As an example domain , we constructed a simple forest fire domain based loosely on the Phoenix fire simulator [ Hart and Cohen , 1992 ] . We use a grid representation of the terrain . Each grid cell can contain vegetation , water , or a base . At the beginning of each simulation , the fire is started at a random location . In each iteration of the simulation , the fire spreads stochastically . The probability of a cell igniting at time t is calculated based on the cell ’s vegetation , the wind direction , and how many of the cell ’s neighbors are burning at time t − 1 . Additionally , bulldozers are used to contain the fire before they reach the bases . For each example terrain , we hand designed a plan for bulldozers to dig a fire line to stop the fire . The bulldozer ’s speed varies from simulation to simulation . An example simulation looks like :
( time0 Ignite X3 Y7 ) , ( time0 MoveTo BD1 X3 Y4 ) , ( time0 MoveTo BD2 X7 Y4 ) , ( time0 DigAt BD2 X7 Y4 ) , , ( time6 Ignite X4 Y8 ) , ( time6 Ignite X3 Y8 ) , , ( time32 Ignite X6 Y1 ) , ( time32 Ignite X6 Y0 ) , We form a database of instances from a set of simulations as follows . Because the idea is to predict success or failure before the plan is finished , the instance itself is a list of all events that happen by some time k , which we vary in our experiments . We label each instance with SUCCESS if none of the locations with bases have been burned in the final state , or FAILURE otherwise . Thus , the job of the classifier is to predict if the bulldozers will prevent the bases from burning , given a partial execution trace of the plan . Example features produced by FeatureMine in this domain are ( MoveTo BD1 X2 ) → ( time6 ) , and ( Ignite X2 ) → ( time8 MoveTo Y3 ) The first sequence holds if bulldozer BD1 moves to the second column before time 6 . The second holds if a fire ignites anywhere in the second column and then any bulldozer moves to third row at time 8 . Many correlations used by our second pruning rule described
Experiment parity , N = 5 , M = 3 , L = 5 parity , N = 3 , M = 4 , L = 8 parity , N = 10 , M = 4 , L = 10 fire , time = 5 fire , time = 10 fire , time = 15 spelling , their vs . there spelling , I vs . me spelling , than vs . then spelling , you’re vs . your
W WFM B .51 .50 .50 .60 .56 .52 .70 .86 .83 .77
.96 .98 .88 .79 .85 .88 .94 .94 .92 .86
.50 .50 .50 .69 .68 .68 .75 .66 .79 .77
BFM .96 1.0 .84 .81 .75 .72 .78 .90 .81 .86
Table 1 : Classification results ( W=Winnow , B=Bayes , WFM , BFM = Winnow , Bayes with FeatureMine , resp . )
Experiment random , N = 10 , M = 4 , L = 10 fire world , time =10 spelling , there vs . their
Selected features 196 553 318 Table 2 : FeatureMine Mining results
Evaluated features 7,693,200 64,766 782,264 in section 2.2 arise in these data sets . For example , Y 8 ; Ignite arises in one of our test plans in which a bulldozer never moves in the eighth column .
For fire data , there are 38 boolean features to describe each event . Thus there are ( (38 × 2)maxw))maxl possible composite features for describing each sequence of events . In the experiments reported here , we used a min f req = .2 , maxw = 3 , and maxl = 3 . Context sensitive spelling correction : We also tested our algorithm on the task of correcting spelling errors that result in valid words , such as substituting there for their ( [Golding and Roth , 1996] ) . For each test , we chose two commonly confused words and searched for sentences in the 1 million word Brown corpus [ Kucera and Francis , 1967 ] containing either word . We removed the target word and then represented each word by the word itself , the part of speech tag in the Brown corpus , and the position relative to the target word . For example , the sentence “ And then there is politics ” is translated into ( word=and tag=cc pos= 2 ) → ( word=then tag=rb pos= 1 ) → ( word=is tag=bez pos=+1 ) → ( word=politics tag=nn pos=+2 ) .
Example features produced by FeatureMine include ( pos=+3 ) → ( word=the ) , indicating that the word the occurs at least 3 words after the target word , and ( pos= 4 ) → ( tag=nn ) → ( pos=+1 ) , indicating that a noun occurs within three words before the target word . These features ( for reasons not obvious to us ) were significantly correlated with either there or their in the training set . In the experiments reported here , we used a min f req = .05 , maxw = 3 , and maxl = 2 .
Results
3.1 For each test in the parity and fire domains , we mined features from 1,000 examples , pruned features that did not pass a chi squared significance test ( for correlation to a class the feature was frequent in ) in 2,000 examples , and trained the classifier on 5,000 examples . Thus , the entire training process required 7,000 examples . We then tested the resulting classifier on 1,000 fresh examples . The results in Tables 1 and 2 are averaged over 25 trials of the process ( ie , we retrained and then re tested the classifier on fresh examples in each trial ) . For the spelling correction , we trained on 80 percent of the examples in the Brown corpus and tested on the remaining 20 percent . During training , we mined features from 500 sentences and trained the classifier on all training examples .
Table 1 shows that the features produced by FeatureMine improved classification performance . We compared using the feature set produced by Fea
Experiment
CPU seconds with no pruning random fire world spelling
320 5.8 hours 490
CPU seconds with only A ; B pruning 337 560 407
CPU seconds with all pruning 337 559 410
Features examined with no pruning 1,547,122 25,336,097 1,126,114
Features examined with only A ; B pruning 1,547,122 511,215 999,327
Features examined with all pruning 1,547,122 511,215 971,085
Table 3 : Impact of pruning rules : results taken from one data set for each example . features of length 1 . tureMine with using only the primitive features themselves , ie Both Winnow and Naive Bayes performed much better with the features produced by FeatureMine . In the parity experiments , the mined features dramatically improved the performance of the classifiers and in the other experiments the mined features improved the accuracy of the classifiers by a significant amount , often more than 20 % .
Table 2 shows the number of features evaluated and the number returned , for several problems . For the largest parity problem , FeatureMine evaluated more than 7 million features and selected only about 200 . There were in fact 100 million possible features ( there are 50 booleans features , giving rise to 100 feature value pairs ; we searched to depth M = 4 . ) but most were rejected implicitly by the pruning rules .
Table 3 shows the impact of the A ; B pruning rule on mining time . The results are from one data set from each domain , with slightly higher values for maxl and maxw than in the above experiments . The pruning rule did not improve mining time in all cases , but made a tremendous difference in the fire world problems , where the same event descriptors often appear together . Without A ; B pruning , the fire world problems are essentially unsolvable because FeatureMine finds over 20 million frequent sequences .
Related work
4 A great deal of work has been done on feature subset selection , motivated by the observation that classifiers can perform worse with feature set F than with some F ′ ⊂ F ( eg , [ Caruana and Freitag , 1994] ) . The algorithms explore the exponentially large space of all subsets of a given feature set . In contrast , we explore exponentially large sets of potential features , but evaluate each feature independently . The featuresubset approach seems infeasible for the problems we consider , which contain hundreds of thousands to millions of potential features .
[ Golding and Roth , 1996 ] applied a Winnow based algorithm to context sensitive spelling correction . They use sets of 10,000 to 40,000 features and either use all of these features or prune some based on the classification accuracy of the individual features . They obtain higher accuracy than we did . Their approach , however , involves an ensemble of Winnows , combined by majority weighting , and they took more care in choosing good parameters for this specific task . Our goal , here , is to demonstrate that the features produced by FeatureMine improve classification performance . Data mining algorithms have often been applied to the task of classification . [ Liu et al . , 1998 ] build decision lists out of patterns found by association mining . [ Ali et al . , 1997 ] and [ Bayardo , 1997 ] both combine association rules to form classifiers . Our use of sequence mining is a generalization on association mining . Our pruning rules resemble ones used by [ Segal and Etzioni , 1994 ] , which also employs data mining techniques to construct decision lists . Previous work on using data mining for classification has focused on combining highly accurate rules together . By contrast , our algorithm can weigh evidence from many features which each have low accuracy in order to classify new examples .
[ Liu and Setiono , 1998 ] describes recent work on scaling up feature subset selection . They apply a probabilistic Las Vegas Algorithm to data sets with 16 to 22 features . One of the problems is a parity problem , much like the one described above , which contains 20 features ( N=2,M=5,L=10 ) . Their algorithms , thus , search the space of all 220 subsets of the available features . For comparison , we have applied our algorithms to parity problems with 50 features , which results in 100 featurevalue pairs . Our algorithm then searches over the set of all conjunctions of up to maxw feature value pairs . FeatureMine can handle millions of examples and thousands of items , which makes it extremely scalable . Our work is close in spirit to [ Kudenko and Hirsh , 1998 ] , which also constructs a set of sequential , boolean features for use by classification algorithms . They employ a heuristic search algorithm , called FGEN , which incrementally generalizes features to cover more and more of the training examples , based on its classification performance on a hold out set of training data , whereas we perform an exhaustive search ( to some depth ) and accept all features which meet our selection criteria . Additionally , we use a different feature language and have tested our approaches on different classifiers than they have .
References [ Ali et al . , 1997 ] K . Ali , S . Manganaris , and R . Srikant . Partial classification using association rules . In KDD97 .
[ Bayardo , 1997 ] RJ Jr . Bayardo . Brute force mining of high confidence classification rules . In KDD97 .
[ Caruana and Freitag , 1994 ] R . Caruana and D . Freitag . Greedy attribute selection . In ICML94 .
[ Duda and Hart , 1973 ] RO Duda and PE Hart .
Classification and Scene Analysis . Wiley .
Pattern
[ Golding and Roth , 1996 ] A . Golding and D . Roth . Applying winnow to context sensitive spelling correction . In ICML96 .
[ Hart and Cohen , 1992 ] D.M Hart and P.R Cohen . Predicting and explaining success and task duration in the phoenix planner . In 1st Intl . Conf . on AI Planning Systems .
[ Kucera and Francis , 1967 ] H . Kucera and WN Francis . Computational Analysis of Present Day American English . Brown University Press , Providence , RI .
[ Kudenko and Hirsh , 1998 ] D . Kudenko and H . Hirsh . Feature generation for sequence categorization . In AAAI98 .
[ Lee et al . , 1998 ] W . Lee , S . Stolfo , and K . Mok . Mining audit data to build intrusion detection models . In KDD98 .
[ Littlestone , 1988 ] N . Littlestone . Learning quickly when irrelevant attributes abound : A new linear threshold algorithm . Machine Learning , 2:285–318 .
[ Liu and Setiono , 1998 ] H . Liu and S . Setiono . Some issues on In 4th World Congress of Expert scalable feature selection . Systems : Application of Advanced Info . Technologies .
[ Liu et al . , 1998 ] B . Liu , W . Hsu , and Y . Ma .
Integrating classification and association rule mining . In KDD98 .
[ Segal and Etzioni , 1994 ] Richard Segal and Oren Etzioni . Learning decision lists using homogeneous rules . In AAAI94 . [ Zaki , 1998 ] M . J . Zaki . Efficient enumeration of frequent sequences . In 7th Intl . Conf . Info . and Knowledge Management .
