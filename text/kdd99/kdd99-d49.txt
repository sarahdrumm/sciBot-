Parallel Sequence Mining on Shared Memory Machines
Computer Science Dept . , Rensselaer Polytechnic Institute , Troy , NY 12180
Mohammed J . Zaki
Abstract We present pSPADE , a parallel algorithm for fast discovery of frequent sequences in large databases . pSPADE decomposes the original search space into smaller suffix based classes . Each class can be solved in main memory using efficient search techniques , and simple join operations . Further each class can be solved independently on each processor requiring no synchronization . However , dynamic inter class and intraclass load balancing must be exploited to ensure that each processor gets an equal amount of work . Experiments on a 12 processor SGI Origin 2000 shared memory system show good speedup and scaleup results .
1 The sequence mining task is to discover a sequence of attributes ( items ) , shared across time among a large number of objects ( transactions ) in a given database . The task of discovering all frequent sequences in large databases is quite challenging . The search space is extremely large . For example , with attributes there are k potentially frequent sequences of length at mostk . Clearly , sequential algorithms cannot provide
Introduction scalability , in terms of the data size and the performance , for large databases . We must therefore rely on parallel multiprocessor systems to fill this role .
Previous work on parallel sequence mining has only looked at distributed memory machines [ Shintani and Kitsuregawa , 1998 ] . In this paper we look at sharedmemory systems , which are capable of delivering high performance for low to medium degree of parallelism at an economically attractive price . SMP machines are the dominant types of parallel machines currently used in industry . Individual nodes of parallel distributedmemory machines are also increasingly being designed to be SMP nodes .
Our platform is a 12 processor SGI Origin 2000 system , which is a cache coherent non uniform memory access ( CC NUMA ) machine . For cache coherence the hardware ensures that locally cached data always reflects the latest modification by any processor . It is NUMA because reads/writes to local memory are cheaper than reads/writes to a remote processor ’s memory . The main challenge in obtaining high performance on these systems is to ensure good data locality , making sure that most read/writes are to local memory , and reducing/eliminating false sharing , which occurs when two different shared variables are ( coincidentally ) located in the same cache block , causing the block to be exchanged between the processors due to coherence maintenance operations , even though the processors are accessing different variables . Of course , the other factor influencing parallel performance for any system is to ensure good load balance , ie , making sure that each processor gets an equal amount of work .
In this paper we present pSPADE , a parallel algorithm for discovering the set of all frequent sequences , targeting shared memory systems , the first such study . pSPADE has been designed such that it has very good locality and has virtually no false sharing . Further , pSPADE is an asynchronous algorithm , in that it requires no synchronization among processors , except when a load imbalance is detected . We carefully consider several design alternatives in terms of data and task parallelism , and in terms of the load balancing strategy used , before adopting the best approach in pSPADE . An extensive set of experiments is performed on the SGI Origin 2000 machine . pSPADE delivers good speedup and scales linearly in the database size .
The rest of the paper is organized as follows : We describe the sequence discovery problem in Section 2 . Section 3 describes the serial algorithm , while the design and implementation issues for pSPADE are presented in Section 4 . An experimental study is presented in Section 5 , and we conclude in Section 6 . as follows : Let =fi1;i2 ; ;i g be a set of
2 The problem of mining sequential patterns can be stated
Sequence Mining
1 distinct attributes , also called items . An itemset is a non empty unordered collection of items ( without loss of generality , we assume that items of an itemset are sorted in increasing order ) . A sequence is an ordered list of itemsets . An itemseti is denoted as i1i2 ik , whereij is an item . An itemset withk items is called a k itemset . A sequenceff is denoted as ff17!ff27! 7!ff , where the sequence elementffj is an itemset . A sequence withk items ( k= jjffjj ) is called a ksequence . For example , B7!AC is a 3 sequence . A sequenceff= ff17!ff27! 7!ff is a subsequence of another sequencefi= fi17!fi27! 7!fi , denoted asfffi , if there exist integers i1<i2< <i such thatajbij for allaj . For example the sequence B7!AC is a subsequence of AB7!E7!ACD , since the sequence elements BAB , andACACD . On the other hand the sequence AB7!E is not a subsequence of ABE , and vice versa . We say thatff is a proper subsequence of fi , denotedff(cid:30)fi , iffffi andfi6ff . A sequence is
An item can occur only once in an itemset , but it can occur multiple times in different itemsets of a sequence . maximal if it is not a subsequence of any other sequence .
DATABASE
CID
TID
Items
1
2
10
20
30
20
30
50
A B
B
A B
A C
A B C
B
A
B
A
10
30
40
30
40
3
4
50
A B
A
B
FREQUENT SET ( 75 % Minimum Support )
{A , A >A , B >A , B , AB , A >B , B >B , AB >B}
Figure 1 : Original Database
A transactionT has a unique identifier and contains a set of items , ie,T . A customer,C , has a unique fT1;T2 ; ;T g . Without loss of generality , we assume customer is itself a sequenceT17!T27! 7!T , called the customer sequence . The database,D , consists sequence,C , is said to contain a sequenceff , ifffC , ie , ifff is a subsequence of the customer sequence that no customer has more than one transaction with the same time stamp , so that we can use the transactiontime as the transaction identifier . We also assume that the list of customer transactions is sorted by the transaction time . Thus the list of transactions of a identifier and has associated with it a list of transactions of a number of such customer sequences . A customer
C . The support or frequency of a sequence , denoted ff , is the the total number of customers that contain times . The set of frequentk sequences is denoted as Fk . Given a databaseD of customer sequences and database has three items ( A;B;C ) , four customers , and this sequence . Given a user specified threshold called the minimum support ( denoted min sup ) , we say that a sequence is frequent if occurs more than min sup min sup , the problem of mining sequential patterns is to find all frequent sequences in the database . For example , consider the customer database shown in figure 1 . The twelve transactions in all . The figure also shows all the frequent sequences with a minimum support of 75 % or 3 customers .
The Serial SPADE Algorithm
3 Several sequence mining algorithms have been proposed in recent years [ Srikant and Agrawal , 1996 , Mannila et al . , 1997 , Oates et al . , 1997 ] . GSP [ Srikant and Agrawal , 1996 ] is one of the best known serial algorithms . Recently , SPADE [ Zaki , 1998 ] was shown to outperform GSP by a factor of two in the general case , and by a factor of ten with a pre processing step . In this section we describe SPADE [ Zaki , 1998 ] , a serial algorithm for fast discovery of frequent sequences , which forms the basis for the parallel pSPADE algorithm . the subsequence relation defines a partial order on the fffi , we say thatff is more general thanfi , orfi is more specific thanff . The second observation used is that the relation is a monotone specialization relation with respect to the frequency ff;D , ie , iffi is a frequent sequence , then all subsequencesfffi are
Sequence Lattice SPADE uses the observation that set of sequences , also called a specialization relation . If also frequent . The algorithm systematically searches the sequence lattice spanned by the subsequence relation , from the most general to the maximally specific frequent sequences in a breadth/depth first manner . For example , Figure 2 A ) shows the lattice of frequent sequences for our example database . layout , where we associate with each itemX in the sequence lattice its idlist , denoted X , which is a list of
Support Counting Most of the current sequence mining algorithms [ Srikant and Agrawal , 1996 ] assume a horizontal database layout such as the one shown in Figure 1 . In the horizontal format the database consists of a set of customers ( cid ’s ) . Each customer has a set of transactions ( tid ’s ) , along with the items contained in the transaction . In contrast , we use a vertical database all customer ( cid ) and transaction identifiers ( tid ) pairs
2
Given the sequence idlists , we can determine the that share a common suffix ( the generating sequences ) containing the atom . The vertical idlist format , called binary association table , has also been used in [ Holsheimer et al . , 1996 ] for parallel data mining . Figure 2 B ) shows the idlists for all the items . simple check on the cardinality of the resulting idlist tells us whether the new sequence is frequent or not . Figure 2 shows this process pictorially . It shows the initial vertical database with the idlist for each item . There are two kinds of intersections : temporal and equality . For example , Figure 2 shows the idlist for support of anyk sequence by simply intersecting the idlists of any two of its k 1 length subsequences . In particular , we use the two k 1 length subsequences to compute the support of a newk length sequence . A A!B obtained by performing a temporal intersection on the idlists ofA andB , ie , A!B = A \ B . This is done by looking if , within the same cid , the other hand the idlist forAB!B is obtained by an equality intersection , ie , AB!B = A! B \e B!B . Here we check to see if the two based partition . We say that twok length sequences are a commonk 1 length suffix . The partitions , such as f[A℄;[B℄g , based on length 1 suffixes are called parent example , if a class[X℄ has the elementsY!X , andZ!X . The possible frequent sequences at the next step areY!Z!X,Z!Y!X , and YZ igh a wX . No other item can lead to a frequent sequence with the suffixX , unless X or !X is also in[X℄ . partitions . Each parent partition is independent in the sense that it has complete information for generating all frequent sequences that share the same suffix . For
A occurs before B , and listing all such occurrences . On subsequences occur within the same cid at the same time . Additional details can be found in [ Zaki , 1998 ] .
To use only a limited amount of main memory SPADE breaks up the sequence search space into small , independent , manageable chunks which can be processed in memory . This is accomplished via suffix in the same equivalence class or partition if they share
SPADE recursively decomposes the sequences at each new level into even smaller independent classes , which produces a computation tree of independent classes as shown in Figure 2B ) . This computation tree is processed in a breadth first manner within each parent class . Figure 3 shows the pseudo code for SPADE ; we refer the reader to [ Zaki , 1998 ] for more details .
The Parallel pSPADE Algorithm
4 While parallel association mining has attracted wide attention [ Agrawal and Shafer , 1996 , Zaki et al . , 1997 ] ,
SPADE ( i ;D ) : C=f parent classesCi=[Xi℄g ; for eachCi2C do Enumerate Frequent Seq(Ci ) ; // ev is list of frequent classes from previous level // ew is list of new frequent classes for current level Enumerate Frequent Seq( ev ) : for ( ; ev 6= ; ; ev = ev : ex ) ew = ew [ Get New Classes ( ev :i e ) ; if ( ew 6= ; ) then Enumerate Frequent Seq( ew ) ; Get New Classes(S ) : for all atomsAi2S do for all atomsAj2S , withj>i do R=Ai[Aj ; R = Ai \ Aj ; if ( R i ) thenCi=Ci[fRg ; C i =C i [ Ci ; returnC i ;
Figure 3 : SPADE pseudo code there has been relatively less work on parallel mining of sequential patterns . Three distributed memory parallel algorithms based on GSP were presented in [ Shintani and Kitsuregawa , 1998 ] . pSPADE is the first algorithm for shared memory systems . pSPADE is best understood by visualizing the computation as a dynamically expanding irregular tree of independent suffix based classes , as shown in Figure 4 . This tree represents the search space for the algorithm . There are three independent suffix based parent equivalence classes . These are the only classes visible at the beginning of computation . Since we have a sharedmemory machine , there is only one copy on disk of the database in the vertical idlist format . It can be accessed by any processor , via a local file descriptor . Given that each class in the tree can be solved independently the crucial issue is how to achieve a good load balance , so that each processor gets an equal amount of work . We would also like to maximize locality and minimize/eliminate cache contention . data parallelism processors work on distinct portions
There are two main paradigms that may be utilized in the implementation of parallel sequence mining : a data parallel approach and a task parallel approach . In of the database , but synchronously process the global computation tree . It essentially exploits intra class parallelism , ie , the parallelism available within a class . In task parallelism , the processors share the database , but work on different classes in parallel , asynchronously processing the computation tree . This scheme is thus
3
3
AB >B
( Intersect A >B and B >B )
( Intersect A and B )
4
A >A
3
B >A
3 AB
4
A >B
3 B >B
A
4
B
4
{ }
A .
B .
A
CID TID 10 1 30 1 20 2 2 30 10 3 40 3 30 4 4 40
B
CID TID 10 1 20 1 1 30 30 2 50 2 30 3 30 4 50 4
C .
FREQUENT SEQUENCE LATTICE
ORIGINAL ID LIST DATABASE
AB >B CID TID 1 10 2 30 4 30
B >B
CID TID 1 10 1 20 30 2 4 30
A >B
CID TID 10 1 2 20 2 30 3 10 4 30 4 40
[ AB >B ]
[ A >A ]
[ B >A ]
[ AB ]
[ A >B ]
[ B >B ]
[ A ]
[ B ]
Figure 2 : SPADE : Lattices and Joins
[ {} ]
SUFFIX JOINS ON ID LISTS
D .
EQUIVALENCE CLASS LATTICE tersection is performed in parallel among the procesid range , and increments support in a shared variable .
Idlist Parallelism There are two ways of implementing the idlist parallelism . In the first method each in sors . Each processor performs the intersection over its
LEVEL 5
P0
LEVEL 4
P0
P0
P1
P1
P0
P0
P1
LEVEL 3
P0
P0
P0
P0
P0
P0
P1
P1
P1
P1
P1
P1
P1
P0
P1
P0
Z1
Z2
Y1
Y2
Y3
LEVEL 2
P0
P0
P0
P1
P1
P1
P1
P1
P0
New Class X1
New Class X2
New Class X3
LEVEL 1
P0
Parent Class C1
P1
Parent Class C2
P1
Parent Class C3
401
Data Parallelism based on inter class parallelism .
Figure 4 : Dynamic , Irregular Computation Tree of Classes in which we physically partition each idlist into ranges over the customer sequence id ( for example , processor 0 is responsible for the id range0   , processor 1 for range  1 2  , and so on ) . Each processor is responsible for1= of the id . The
For sequence mining , data parallelism can come in two flavors . The first case corresponds to idlist parallelism , other case corresponds to join parallelism , where each processor picks a sequence and performs intersections with the other sequences in the same class , generating new classes for the next level .
A barrier synchronization must be performed to make sure that all processors have finished their intersection for the candidate . Finally , based on the support this candidate may be discarded or added to the new class . This scheme suffers from massive synchronization overheads . As we shall see in Section 5 for some values of minimum support we performed around 0.6 million intersections . This scheme will require as many barrier synchronizations . The other method uses a level wise approach . In other words , at each new level of the computation tree , each processor processes all the classes at that level , performing intersections for each candidate , but only over its local database portion . The local supports are stored in a local array to prevent false sharing among processors . After a barrier synchronization signals that all processors have finished processing the current level , a sum reduction is performed in parallel to determine the global support of each candidate . The frequent sequences are then retained for the next level , and the same process is repeated for other levels until no more frequent sequences are found .
We implemented the level wise idlist parallelism and found that it performed very poorly . In fact , we got a speed down as we increased the number of processors ( see Section 5 ) . Even though we tried to minimize the synchronization as much as possible , performance was still unacceptable . Since a candidate ’s memory cannot be freed until the end of a level , the memory consumption of this approach is extremely high . We
4 were unable to run this algorithm for low values of minimum support . Also , when the local memory is not sufficient the Origin allocates remote memory for the intermediate idlists , causing a performance hit due to the NUMA architecture .
Join Parallelism In join parallelism each processor performs intersections for different sequences within the same class . Once the current class has been processed , the processors must synchronize before moving on to the next class . While we have not implemented this approach , we believe that it will fare no better than idlist parallelism . The reason is that it requires one synchronization per class , which is better than the single candidate idlist parallelism , but still much worse than the level wise idlist parallelism , since there can be many classes .
Task Parallelism
402 In task parallelism all processors have access to the entire database , but they work on separate classes . We present a number of load balancing approaches starting with a static load balancing scheme and moving on to a more sophisticated dynamic load balancing strategy . represent the set of the parent classes at level 1 as shown in Figure 4 . We need to schedule the classes among the processors in a manner minimizing load imbalance . In our approach an entire class is scheduled on one processor . Load balancing is achieved by assigning a weight to each equivalence class based on the number of elements in the class . Since we have to consider all pairs of items for the next iteration , we assign the
Static Load Balancing ( SLB ) LetC=fC1;C2;C3g weightW1i= jCij2 to the classCi . Once the such asW2i= jj Aj j for all itemsAj in the classCi . This cost function gives each class a weight two , ie,W3i= jCij2 jj Aj j . We did not over the other , and decided to useW1 . weights are assigned we generate a schedule using a greedy heuristic . We sort the classes on the weights ( in decreasing order ) , and assign each class in turn to the least loaded processor , ie , one having the least total weight at that point . Ties are broken by selecting the processor with the smaller identifier . We also studied the effect of other heuristics for assigning class weights , proportional to the sum of the supports of all the items . We also tried a cost function that combines the above observe any significant benefit of one weight function
Figure 5 shows the pseudo code for the SLB algorithm . We schedule the classes on different processors based on the class weights . Once the classes have been scheduled , the computation proceeds in a purely asynchronous manner since there is never any need to synchronize or share information among the processors . If we applyW1 to the class tree shown in Figure 4 , we get W11=W12=W13=3 . Using the greedy scheduling scheme on two processors , 0 gets the classesC1 andC3 , and 1 gets the classC2 . We immediately see cessingC1 , 0 will be busy working onC3 , while after processingC2 , 1 has no more work . The main problem that SLB suffers from load imbalance , since after pro with SLB is that , given the irregular nature of the computation tree there is no way of accurately determining the amount of work per class statically .
Inter Class Dynamic Load Balancing ( CDLB ) To get better load balancing we utilize inter class dynamic load balancing . Instead of a static or fixed class assignment of SLB , we would like each processor to dynamically pick a new class to work on from the list of classes not yet processed . We also make use of the class weights in the CDLB approach . First , we sort the parent classes in decreasing order of their weight . This forms a logical central task queue of independent classes . Each processor atomically grabs one class from this logical queue . It processes the class completely and then grabs the next available class . Note that each class usually has a non trivial amount of work , so that we don’t have to worry too much about contention among processors to acquire new tasks . Since classes are sorted on their weights , processors first work on large classes before tackling smaller ones , which helps to achieve a greater degree of load balance . The pseudo code for CDLB algorithm appears in Figure 5 . The compare and swap ( CAS ) is an atomic primitive on the Origin . It compares
 a id withi . If they are equal it replaces  a id with i 1 , returning a 1 , else it returns a 0 . CAS ensures that the beginning 1 grabsC1 , and 0 acquiresC2 . Since C2 has less work , 0 will grab the next classC3 and work on it . Then 1 becomes free and finds that there is no more work , while 0 is still busy . For this example ,
If we apply CDLB to our example computation tree in Figure 4 , we might expect a scenario as follows : In processors acquire separate classes .
CDLB did not buy us anything over SLB . However , when we have a large number of parent classes CDLB has a clear advantage over SLB , since a processor grabs a new class only when it has processed its current class . This way only the free processors will acquire new classes , while others continue to process their current class , delivering good processor utilization . We shall see in Section 5 that CDLB can provide up to 40 % improvement over SLB . We should reiterate that the processing of classes is still asynchronous . For both
5
Figure 5 : The SLB ( Static Load Balancing ) and CDLB ( Dynamic Load Balancing ) Algorithms
SLB ( i ;D ) : C=f parent classesCi=[Xi℄g ; Sort on Weight(C ) ; for allCi2C do j = Proc with Min Weight( ) ; S j=S j[Ci ; parallel for allCi2S j do Enumerate Frequent Seq(Ci ) ;
SLB and CDLB , false sharing doesn’t arise , and all work is performed on local memory , resulting in good locality .
Recursive Dynamic Load Balancing ( RDLB ) While CDLB improves over SLB by exploiting dynamic load balancing , it does so only at the inter class level , which may be too coarse grained to achieve a good workload balance . RDLB addresses this by exploiting both interclass and intra class parallelism . To see where the intra class parallelism can be exploited , lets examine the behavior of CDLB . As long as there are more parent classes remaining , each processor acquires a new class and processes it completely . If there are no more parent classes left , the free processors are forced to idle . The worst case happens when  1 processors are free and only one is busy , especially if the last class has a deep computation tree ( although we try to prevent this case from happening by sorting the classes , so that the smaller ones are at the end , it can still happen ) . We can fix this problem if we can provide a mechanism for the free processors to join the busy ones . We accomplish this by recursively applying the CDLB strategy at each new level , but only if there is some free processor waiting for more work . Since each class is independent , we can treat each class at the new level in the same way we treated the parent classes , so that different processors can work on different classes that the new level . sert them in the global class list,G  ba  . A processor crementsF eeC , and waits for more work . When a it inserts the remaining classes at that level ( ev ) inG  ba  , emptying ev in the process , and sets G  ba F g . This processor then quits the loop on line 16 , and continues working on the new classes ( ew )
Figure 6 shows the pseudo code for the pSPADE algorithm , which uses a recursive dynamic load balancing ( RDLB ) scheme . We start with the parent classes and in atomically acquires classes from this list until all parent classes have been taken . At that point the processor in processor is processing the classes at some level , it periodically checks if there is any free processor . If so , shared int classid=0 ;
5 . 6 .
Process GlobalQ( ) ;
16 . 17 . 18 . 19 . 20 .
9 . 10 . Process GlobalQ( ) : 11 . 12 . 13 . 14 .
CDLB ( i ;D ) : C=f parent classesCi=[Xi℄g ; Sort on Weight(C ) ; parallel for ( i=0;i<jCj;i ) if ( compare and swap (  a id;i;i 1 ) ) Enumerate Frequent Seq(Ci ) ; 1 . shared intF eeC =0 ; //Number of free processors 2 . shared intG  ba F g=0 ; //Is there more work ? 3 . shared listG  ba  ; //Global list of classes pSPADE ( i ;D ) : 4.G  ba  =C=f parent classesCi=[Xi℄g ; Sort on Weight(C ) ; 7.F eeC ; 8 . while ( F eeC 6= ) if ( G  ba F g ) then F eeC ; Process GlobalQ();F eeC ; shared int  a id=0 ; parallel for ( i=0;i<G  ba  : ize ;i ) if ( compare and swap (  a id;i;i 1 ) ) RDLB Enumerate Frequent Seq(Ci ) ; 15.G  ba F g=0 ; RDLB Enumerate Frequent Seq( ev ) : for ( ; ev 6= ; ; ev = ev : ex ) if ( F eeC >0 ) then Add to GlobalQ( ev : ex );G  ba F g=1 ; ew = ew [ Get New Classes ( ev :i e ) ; if ( ew 6= ; ) then RDLB Enumerate Frequent Seq( ew ) ; working on the classes inG  ba  . When there is no more work,F eeC equals the number of processors , and the computation stops . parent class level , 0 acquiresC1 , and 1 acquires C2 . SinceC2 is smaller , 1 grabs classC3 , and starts level , ew =fX1;X2;X3g , which becomes ev when 1 start the next level . Let ’s assume that 1 finishes processingX1 , and inserts classesZ1;Z2 in the new ew . In the meantime , 0 becomes free . Before processingX2 , 1 notices in line 17 , that there is a free processor . At this point 1 insertsX3 in G  ba  , and empties ev . work onX2 , insertingY1;Y2;Y3 in ew . 0 sees the new insertion inG  ba  and start working onX3 in its entirety . 0 meanwhile starts processing the next
Let ’s illustrate the above algorithm by looking at the computation tree in Figure 4 . The nodes are marked by the processors that work on them . First , at the generated before a free processor was detected . When a waiting processor sees that there is more work , it starts processing it . It generates three new classes at the next
Figure 6 : The pSPADE Algorithm ( using RDLB )
It then continues to
6
C
T
S
1.25 2.5 1.25 2.5 1.25
2
1.25
4 4 4 4 8 8 4
5 5 2.5 2.5 5 5 2.5
Dataset 10 C10T5S4I1.25D1M C10T5S4I2.5D1M 10 C20T25S4I125D1M 20 C20T25S4I25D1M 20 20 C20T5S8I1.25D1M 20 C20T5S8I2D1M C5T25S4I125DxM 5
If at any stage it detects a free processor , it will repeat the procedure described above recursively . Figure 4 shows a possible level classes,fZ1;Z2;Y1;Y2;Y3g . execution sequence for the classC3 . The RDLB scheme for insertions and deletions fromG  ba  . In summary , of pSPADE preserves the good features of CDLB , ie , it dynamically schedules entire parent classes on separate processors , for which the work is purely local , requiring no synchronization . So far only inter class parallelism has been exploited . Intra class parallelism is required only for a few ( hopefully ) small classes towards the end of the computation . We simply treat these as new parent classes , and schedule each class on a separate processor . Again no synchronization is required except computation is kept local to the extent possible , and synchronization is done only if a load imbalance is detected .
Experimental Results
5 Experiments were performed on a 12 processor SGI Origin 2000 machine at RPI , with 195 MHz R10000 MIPS processors , 4MB of secondary cache per processor , 2GB of main memory , and running IRIX 65 The databases we stored on an attached 7GB disk , and all I/O is serial . Further , there were only 8 free processors available to us .
Synthetic Datasets We used the publicly available dataset generation code from the IBM Quest data mining project [ IBM , ] . These datasets mimic realworld transactions , where people buy a sequence of sets of items . Some customers may buy only some items from the sequences , or they may buy items from multiple sequences . The customer sequence size and transaction size are clustered around a mean and a few of them may have many elements . The datasets are generated using the following process . First maximal itemsets of average size are generated by choosing from items . Then S maximal sequences of average sizeS are created by assigning itemsets from
Size
1M 1M 1M 1M 1M 1M
D to each sequence . Next a customer of average C transactions is created , and sequences in S are average transaction size ofT . The generation stops whenD customers have been generated . We set S= 5000 , =25000 and =10000 . Table 1 shows the
MinSup 0.25 % 0.33 % 0.25 % 0.25 % 0.33 % 0.5 % assigned to different customer elements , respecting the
320MB 320MB 440MB 440MB 640MB 640MB
1M 10M 110MB 1.1GB 025% 001 %
Table 1 : Synthetic Datasets datasets with their parameter settings .
In [ Zaki , 1998 ] SPADE was Parallel Performance shown to outperform GSP , thus we chose not to parallelize GSP for comparison against pSPADE . As we mentioned in Section 401 , the level wise idlist data parallel algorithm performs very poorly , resulting in a speed down with more processors . Results on 1 , 2 , and 4 processors shown in Figure 7 confirm this .
We now look at the parallel performance of pSPADE . Figure 8 shows the total execution time and the speedup charts for each database on the minimum support values shown in Table 1 . We obtain near perfect speedup for 2 processors , ranging as high as 192 On 4 processors , we obtained a maximum of 3.5 , and on 8 processors the maximum was 54 As these charts indicate , pSPADE achieves good speedup performance . Finally , we study the effect of dynamic load balancing on the parallel performance . Figure 7 shows the performance of pSPADE using 8 processors on the different databases under static load balancing ( SLB ) , inter class dynamic load balancing ( CDLB ) , and the recursive dynamic load balancing ( RDLB ) . We find that CDLB delivers more than 22 % improvement over SLB in most cases , and ranges from 7.5 % to 38 % improvement . RDLB delivers an additional 10 % improvement over CDLB in most cases , ranging from 2 % to 12 % . The overall improvement of using RDLB over SLB ranges from 16 % to as high as 44 % . Thus our load balancing scheme is extremely effective .
Scaleup Results Figure 9 shows how pSPADE scales up as the number of customers is increased ten fold , from 1 million to 10 million ( the number of transactions is increased from 5 million to 50 million , respectively ) .
7 i
] c e s [ e m T n o i t u c e x E l a t o T
500
450
400
350
300
250
200
150
100
50
00
P=1
P=2
P=4
Static vs . Dynamic DLB
SLB
CDLB
RDLB
] c e s [ i e m T n o i t u c e x E l t a o T
C10T5S4I1.25 D1M
C20T25S4I12 5D1M
C20T25S4I25 D1M
C20T25S8I12 5D1M
Databases
600
500
400
300
200
100
00
C10T5S4I1 .25D1M
C10T5S4I2 .5D1M
C20T2.5S4 I1.25D1M
C20T2.5S4 I2.5D1M
C20T5S8I1 .25D1M
C20T5S8I2 D1M
Databases
Figure 7 : A ) Level Wise Idlist Data Parallelism , B ) Effect of Load Balancing
] c e s [ i e m T n o i t u c e x E l a t o T
C10T5S4I1.25D1M 400
C10T5S4I1.25D1M 55
350
300
250
200
150
100
50
00
22
44
11 88 Number of Processors p u d e e p S
4.5
44
3.5
33
2.5
22
1.5
11
0.5
00
44
22
11 88 Number of Processors
] c e s [ i e m T n o i t u c e x E l a t o T
C20T25S4I125D1M 275 250 225 200 175 150 125 100 75 50 25 00
44
22
11 88 Number of Processors p u d e e p S
C20T25S4I125D1M 5.5 55 4.5 44 3.5 33 2.5 22 1.5 11 0.5 00
44
22
11 88 Number of Processors
] c e s [ i e m T n o i t u c e x E l a t o T
C20T5S8I2D1M 1400
C20T5S8I2D1M 55
1200
1000
800
600
400
200
00
44
22
11 88 Number of Processors p u d e e p S
4.5
44
3.5
33
2.5
22
1.5
11
0.5
00
44
22
11 88 Number of Processors
Figure 8 : pSPADE Parallel Performance
C5T25S4I125
C5T25S4I125
] c e s [ i e m T n o i t u c e x E l t a o T
200
175
150
125
100
75
50
25
00 i e m T n o i t u c e x E e v i t
1M 2M 4M 8M 10M l a e R
4.5
44
3.5
33
2.5
22
1.5
11
0.5
00
1M 2M 4M 8M 10M
Number of Customers [ millions ]
Number of Customers [ millions ]
Figure 9 : Sizeup
C5T25S4I125D1M 90
C5T25S4I125D1M
#frequent
#joins i
] c e s [ e m T n o i t u c e x E l t a o T
80
70
60
50
40
30
20
10
00 t n u o C
700000
650000
600000
550000
500000
450000
400000
350000
300000
250000
200000
150000
100000
50000
00
0.25 %
0.1 %
0.075 %
0.05 %
0.025 %
0.01 %
0.25 %
0.1 %
0.075 %
0.05 %
0.025 %
0.01 %
Minimum Support
Minimum Support
Figure 10 : Effect of Minimum Support
8
The database size goes from 110MB to 11GB All the experiments were performed on the C5T25S4I125 dataset with a minimum support of 0025 % Both the total execution time and the normalized time ( with respect to 1M ) are shown . It can be seen that while the number of customers increases ten fold , the execution time goes up by a factor of less than 4.5 , displaying sublinear scaleup .
Finally , we study the effect of changing minimum support on the parallel performance , shown in Figure 10 . We used 8 processors on C5T25S4I125D1M dataset . The minimum support was varied from a high of 0.25 % to a low of 001 % Figure 10 also shows the number of frequent sequences discovered and the number of joins performed ( candidate sequences ) at the different minimum support levels . Running time goes from 6.8s at 0.1 % support to 88s at 0.01 % support , a time ratio of 1:13 vs . a support ratio of 1:10 . At the same time the number of frequent sequences goes from 15454 to 365132 ( 1:24 ) , and the number of joins from 22973 to 653596 ( 1:29 ) . The number of frequent/candidate sequences are not linear with respect to the minimum support .
Conclusions
6 In this paper we presented pSPADE , a new parallel algorithm for fast mining of sequential patterns in large databases . We carefully considered the various parallel design alternatives before choosing the best strategy for pSPADE . These included data parallel approaches like idlist parallelism ( single vs . level wise ) and join parallelism . In the task parallel approach we considered different load balancing schemes such as static , dynamic and recursive dynamic . We adopted the recursive dynamic load balancing scheme for pSPADE , which was designed to maximize data locality and minimize synchronization , by allowing each processor to work on disjoint classes . It also has no false sharing . Finally , the scheme minimizes load imbalance by exploiting both inter class and intra class parallelism . Experiments conducted on the SGI Origin CC NUMA shared memory system show that pSPADE has good speedup and scaleup properties .
References [ Agrawal and Shafer , 1996 ] R . Agrawal and J . Shafer . Parallel mining of association rules . IEEE Trans . on Knowledge and Data Engg . , 8(6):962–969 , December 1996 .
[ Holsheimer et al . , 1996 ] M . Holsheimer , M . L . Kersten , and A . Siebes . Data surveyor : Searching the
In U . Fayyad , G . Piatetskynuggets in parallel . Shapiro , P . Smyth , and R . Uthurusamy , editors , Advances in Knowledge Discovery and Data Mining . AAAI Press , Menlo Park , CA , 1996 .
[ IBM , ] IBM . http://wwwalmadenibmcom/cs/quest/syndatahtml Quest Data Mining Project , IBM Almaden Research Center , San Jose , CA 95120 .
[ Mannila et al . , 1997 ] H . Mannila , H . Toivonen , and I . Verkamo . Discovery of frequent episodes in event sequences . Data Mining and Knowledge Discovery : An International Journal , 1(3):259–289 , 1997 .
[ Oates et al . , 1997 ] T . Oates , M . D . Schmill , D . Jensen , and P . R . Cohen . A family of algorithms for finding temporal structure in data . In 6th Intl . Workshop on AI and Statistics , March 1997 .
[ Shintani and Kitsuregawa , 1998 ] T . and M . Kitsuregawa . Mining algorithms for sequential patterns in parallel : Hash based approach . In Pacific Asia Conf . on Knowledge Discovery and Data Mining , April 1998 .
Shintani
[ Srikant and Agrawal , 1996 ] R . and R . Agrawal . Mining sequential patterns : Generalizations and performance improvements . In 5th Intl . Conf . Extending Database Technology , March 1996 .
Srikant
[ Zaki et al . , 1997 ] M . J . Zaki , S . Parthasarathy , M . Ogihara , and W . Li . Parallel algorithms for fast discovery of association rules . Data Mining and Knowledge Discovery : An International Journal , 1(4):343 373 , December 1997 .
[ Zaki , 1998 ] M . J . Zaki .
Efficient enumeration of frequent sequences . In 7th Intl . Conf . on Information and Knowledge Management , November 1998 .
9
