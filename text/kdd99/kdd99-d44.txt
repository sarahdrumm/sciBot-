Appears in Proc . of the Fifth ACM SIGKDD Int’l Conf . on Knowledge Discovery and Data Mining , 145 154 , 1999 .
Mining the Most Interesting Rules
Roberto J . Bayardo Jr .
IBM Almaden Research Center
Rakesh Agrawal
IBM Almaden Research Center http://wwwalmadenibmcom/cs/people/bayardo/ http://wwwalmadenibmcom/u/ragrawal/ bayardo@alummitedu ragrawal@acm.org
Abstract Several algorithms have been proposed for finding the “ best , ” “ optimal , ” or “ most interesting ” rule(s ) in a database according to a variety of metrics including confidence , support , gain , chi squared value , gini , entropy gain , laplace , lift , and conviction . In this paper , we show that the best rule according to any of these metrics must reside along a support/confidence border . Further , in the case of conjunctive rule mining within categorical data , the number of rules along this border is conveniently small , and can be mined efficiently from a variety of real world data sets . We also show how this concept can be generalized to mine all rules that are best according to any of these criteria with respect to an arbitrary subset of the population of interest . We argue that by returning a broader set of rules than previous algorithms , our techniques allow for improved insight into the data and support more user interaction in the optimized rule mining process . 1 . Introduction There are numerous proposals for mining rules from data . Some are constraint based in that they mine every rule satisfying a set of hard constraints such as minimum support or confidence ( eg [ 1,2,6] ) . Others are heuristic in that they attempt to find rules that are predictive , but make no guarantees on the predictiveness or the completeness of the returned rule set ( eg decision tree and covering algorithms [ 9,15] ) . A third class of rule mining algorithms , which are the subject of this paper , identify only the most some interestingness metric [ 12,18,20,24 ] . Optimized rule miners are particularly useful in domains where a constraint based rule miner produces too many rules or requires too much time . It is difficult to come up with a single metric that quantifies the “ interestingness ” or “ goodness ” of a rule , and as a result , several different metrics have been proposed and used . Among them are confidence and support [ 1 ] , gain [ 12 ] , variance and chi squared value [ 17,18 ] , entropy gain [ 16,17 ] , gini [ 16 ] , laplace [ 9,24 ] , lift [ 14 ] ( aka interest [ 8 ] or strength [ 10] ) , and conviction [ 8 ] . Several algorithms are known to efficiently find the best rule ( or a close approximation to the best rule [ 16 ] ) according to a specific one of these metrics [ 12,18,20,24 ] . In this paper , we show that a single yet simple concept of rule goodness captures the best rules according to any of them . This concept involves a partial order on rules defined in terms of both rule support and confidence . We demonstrate that the set of rules that are optimal according to this partial order includes all rules that are best according to any of the above metrics , even given arbitrary minimums on support and/or confidence . interesting , or optimal , rules according to
In the context of mining conjunctive association rules , we present an algorithm that can efficiently mine an optimal set according to this partial order from a variety of real world data sets . For example , for each of the categorical data sets from the Irvine machine learning repository ( excepting only connect 4 ) , this algorithm requires less than 30 seconds on a 400Mhz Pentium II class machine . Specifying constraints such as minimum support or confidence reduces execution time even further . While optimizing according to only a single interestingness metric could sometimes require less overhead , the approach we propose is likely to be advantageous since it supports an interactive phase in which the user can browse the optimal rule according to any of several interestingness metrics . It also allows the user to interactively tweak minimums on support and confidence . Witnessing such effects with a typical optimized rule miner requires repeated mining runs , which may be impractical when the database is large . Another need for repeated invocations of an optimized rule miner arises when the user needs to gain insight into a broader population than what is already well characterized by previously discovered rules . We show how our algorithm can be generalized to produce every rule that is optimal according to any of the previously mentioned interestingness metrics , and additionally , with respect to an arbitrary subset of the population of interest . Because datamining is iterative and discovery driven , identifying several good rules up front in order to avoid repeatedly querying the database reduces total mining time when amortized over the entire process [ 13 ] . 2 . Preliminaries 2.1 Generic Problem Statement A data set is a finite set of records . For the purpose of this paper , a record is simply an element on which we apply boolean predicates called conditions . A rule consists of two conditions called the antecedent and consequent , and is denoted as is the consequent . A rule constraint is a boolean the antecedent and predicate on a rule . Given a set of constraints , we say that a rule evaluates to r true given . Some common examples of constraints are item constraints [ 22 ] and minimums on support and confidence [ 1 ] . The input to the problem of mining optimized rules is a tuple satisfies the constraints in
, U D is a finite set of conditions ; U is a data set ; D is a total order on rules ; is a condition specifying the rule consequent ; is a set of constraints on rules .
• • • • • When mining an optimal disjunction , we treat a set of conditions A U˝ as a condition itself that evaluates to true if and only if one evaluates to true on the given or more of the conditions within record . When mining an optimal conjunction , we treat as a condition that evaluates to true if and only if every condition within is A empty then it always evaluates to true . Algorithms for mining optimal conjunctions and disjunctions differ significantly in their evaluates to true on the given record . For both cases , if if every constraint in
, C N
, where
A
Cfi
A
,
C N
A
A
N
N where :
A
C
N r
5
£ Æ æ £ such that
A1 U˝ satisfies the input details , but the problem can be formally stated in an identical manner1 : PROBLEM ( OPTIMIZED RULE MINING ) : Find a set
A1
( 1 ) ( 2 ) there exists no set satisfies the input constraints , and such that
A2
A2 U˝ < .
A1 A2 constraints and
Cfi
I
I r
A
A whose antecedent is a solution to an instance of Any rule the optimized rule mining problem is said to be optimal ( or just optimal if the instance is clear from the context ) . For simplicity , we and possibly sometimes treat rule antecedents ( denoted with some subscript ) and rules ( denoted with and possibly some subscript ) interchangeably since the consequent is always fixed and clear from the context . We now define the support and confidence values of rules . These values are often used to define rule constraints by bounding them above a pre specified value known as minsup and minconf respectively [ 1 ] , and also to define total orders for optimization is equal to the number of [ 12,20 ] . The support of a condition evaluates to true , and this value records in the data set for which sup A( ) is denoted as , denoted . The support of a rule Cfi ( ) , is equal to the number of records in the similarly as evaluate to true.2 The antecedent data set for which both and A support of a rule is the support of its antecedent alone . The confidence of a rule is the probability with which the consequent evaluates to true given that the antecedent evaluates to true in the input data set , computed as follows :
Cfi sup A
C
A
A
A conf A
(
Cfi )
=
Cfi sup A ( ) sup A( )
2.2 Previous Algorithms for the Optimized Rule
Mining Problem contains an existence test for each attribute/value pair appear
Many previously proposed algorithms for optimized rule mining solve specific restrictions of the optimized rule mining problem . For example , Webb [ 24 ] provides an algorithm for mining an optimized conjunction under the following restrictions : •
U ing in a categorical data set outside a designated class column ; orders rules according to their laplace value ( defined later ) ; is empty .
• • Fukuda et al . [ 12 ] provide algorithms for mining an optimized disjunction where : • contains a membership test for each square of a grid formed U by discretizing two pre specified numerical attributes of a dataset ( a record is a member of a square if its attribute values fall within the respective ranges ) ;
N
•
• orders rules according to either confidence , antecedent sup port , or a notion they call gain ( also defined later ) ; includes minimums on support or confidence , and includes N one of several possible “ geometry constraints ” that restrict the allowed shape formed by the represented set of grid squares ;
Rastogi and Shim [ 20 ] look at the problem of mining an optimized disjunction where : • includes a membership test for every possible hypercube U defined by a pre specified set of record attributes with either
1 Algorithms for mining optimal disjunctions typically allow a single fixed conjunctive condition without complications , eg see [ 20 ] . We ignore this issue for simplicity of presentation . 2 This follows the original definition of support as defined in [ 1 ] . The reader is warned that in the work of Fukuda et . al . [ 14 ] and Rastogi and Shim [ 20 ] ( who are careful to note the same discrepancy ) , the definition of support corresponds to our notion of antecedent support . ordered or categorical domains ;
• • orders rules according to antecedent support or confidence ; includes minimums on antecedent support or confidence , a N maximum on the number of conditions allowed in the antecedent of a rule , and a requirement that the hypercubes corresponding to the conditions of a rule are non overlapping . k
In general , the optimized rule mining problem , whether conjunctive or disjunctive , is NP hard [ 17 ] . However , features of a specific instance of this problem can often be exploited to achieve tractability . For example , in [ 12 ] , the geometry constraints are used to develop low order polynomial time algorithms . Even in cases where tractability is not guaranteed , efficient mining in practice has been demonstrated [ 18,20,24 ] . The theoretical contributions in this paper are conjunction/disjunction neutral . However , we focus on the conjunctive case in validating the practicality of these results through empirical evaluation . 2.3 Mining Optimized Rules under Partial Orders We have carefully phrased the optimized rule mining problem so that it may accommodate a partial order in place of a total order . With a partial order , because some rules may be incomparable , there can be several equivalence classes containing optimal rules . The previous problem statement requires an algorithm to identify only a single rule from one of these equivalence classes . However , in our application , we wish to mine at least one representative from each equivalence class that contains an optimal rule . To do so , we could simply modify the previous problem statement to find all optimal rules instead of just one . However , in practice , the equivalence classes of rules can be large , so this would be unnecessarily inefficient . The next problem statement enforces our requirements specifically : PROBLEM ( PARTIAL ORDER OPTIMIZED RULE MINING ) : Find a set
O ( 1 ) every set of subsets of A
U in such that : O is optimal as defined by the optimized rule mining problem .
( 2 ) for every equivalence class of rules as defined by the partial order , if the equivalence class contains an optimal rule , then exactly one member of this equivalence class is within
O
.
I
I x
I
I
)
)
)
) if f x2( f x2( f x1( f x( ) < x2
optimal set . An of this problem an
We call a set of rules whose antecedents comprise a solution to an instance optimal rule is one that may appear in an optimal set . 2.4 Monotonicity Throughout this paper , we exploit ( anti )monotonicity properties of functions . A function is said to be monotone ( resp . antimonotone ) in implies that ( resp . x1 f x1( ) . For example , the confidence function , which is defined in terms of rule support and antecedent support , is antimonotone in antecedent support when rule support is held fixed . 3 . SC Optimality 3.1 Definition Consider the following partial order < r2 and sup r2( • < sup r2( • Additionally , conf r1( An contains this partial order is depicted in Figure 1 . Intuitively , such a set of rules defines a supportconfidence border above which no rule that satisfies the input constraints can fall . if and only if : < ) ) sc= . )
) conf r2( , or ) conf r2( . if and only if conf r1( conf r1( r2 on rules . Given rules
, sup r1( sup r1(
optimal set where r1 sc ) ) conf r2( sup r1( sup r2( and r1 r1 r2 sc
) )
)
=
)
=
)
I
I
£ £ £ £ ‡ £ ( cid:217 ) £ £ ( cid:217 ) sc optimal rule s c optimal rule
No rules fall outside these borders
Confidence non optimal rules fall within these borders
0 %
Support
>
<
) )
) )
) )
) ) if r2 r1 s c(cid:216 )
, or . such that sup r2( sup r2( sup r1( sup r1( conf r2( conf r2( conf r1( conf r1(
Figure 1 . Upper and lower support confidence borders . s c(cid:216)<
Consider also the similar partial order and only if : • • The equivalence condition is the same as before . An optimal set of rules according to this partial order forms a lower border . 3.2 Theoretical Implications In this section , we show that for many total orders rank in order of interestingness , we have < sc sc= r1 r1 r2 r1 is implied by such total order is useful due to the following fact : LEMMA 3.1 : Given the problem instance intended to that . We say that any . This property of a total order rules r2
, U D t= r2
, and r2 sc
C N t t r1
= t
I
,
,
, such that within any t is implied by Isc
optimal set where
, an I Isc that is not sc
optimal rule is contained = Isc
, U D
C N sc
,
,
.
Proof : Consider any rule
N r1 r1
) . Because
. This implies that any non
optimal ( for simplicity is we will ignore the presence of constraints in that is optimal such non optimal , there must exist some rule r2 < r2 r1 t r1 sc . But then we also have that is that r2 sc optimal rule is implied by Isc optimal , or it is equivalent to some optimal rule either nonI I which resides in an optimal equivalence class . At least one optimal equivalence class must therefore contain an optiIsc , every rule in mal rule . Further , because this equivalence class must be optimal . By definition , an Isc optimal set will contain one of these rules , and the claim follows . is implied by I since sc= t=
Isc t
I t ,
Put simply , mining the upper support/confidence border identifies optimal rules according to several different interestingness metrics . We will show that these metrics include support , confidence , conviction , lift , laplace , gain , and an unnamed interest measure proposed by Piatetsky Shapiro [ 19 ] . If we also mine the lower border , metrics such as entropy gain , gini , and chi squared value are also included . But first , consider the following additional property of total orders implied by OBSERVATION 3.2 : Given instance
, U D sc =
,
,
,
: is implied by t
, and
N sc and/or a minimum confidence constraint ns straint optimal rule is contained within any . Isc
,{ ns nc
, U D
C N
} } sc
=
–
,
,
,
Isc
I t
C N such that contains a minimum support conI optimal set where
, an nc
The implication of this fact is that we can mine without minimum support and confidence constraints , and the optimal rule given any setting of these constraints will remain in the mined rule set . This allows the user to witness the effects of modifying these constraints without further mining of the data a useful fact since the user often cannot determine accurate settings of minimum support or confidence apriori . Minimum support and confidence constraints are quite critical in some applications of optimized rule mining , particularly when the optimization metric is itself support or confidence as in [ 12 ] and [ 20 ] . To identify the interestingness metrics that are implied by use the following lemma . LEMMA 3.3 : The following conditions are sufficient for establishf r( ) defined over a rule value function
, we sc t f r( ) ing that a total order is implied by partial order ( 1 ) dence , and f r( ) ( 2 ) support .
: sc is monotone in support over rules with the same confi is monotone in confidence over rules with the same
Proof : Suppose
,
= sc r1 sup r( )
< sc r2 conf r2( and sc r then consider a rule sup r1( ) tion r1 tonicity properties from above , then total orders are transitive , we then have that establishes the claim . where . Note that by defini . Now , if a total order has the mono . Since r2 , which r2 r t r1 t conf r( ) r1 t and and r2
= r r r
)
These conditions trivially hold when the rule value function is sup r( ) . So consider next the Laplace function which is commonly used to rank rules for classification purposes [ 9,24 ] . conf r( ) or laplace A
(
Cfi )
= sup A 1+
Cfi ( sup A( )
) k+ k
The constant is an integer greater than 1 ( usually set to the number of classes when building a classification model ) . Note that if confidence is held fixed to some value , then we can rewrite the Laplace value as below . c laplace r( )
= sup r( ) 1+ sup r( ) c⁄ k+ c k
0‡
1> and
It is straightforward to show that this expression is monotone in rule support since . The Laplace function is also monotone in confidence among rules with equivalent support . To see why , note that if support is held constant , in order to raise the function value , we need to decrease the value of the denominator . This decrease can only be achieved by reducing antecedent support , which implies a larger confidence . Note that an optimal set contains the optimized Laplace rule for any valid setting of . k on the This means the user can witness the effect of varying optimal rule without additional mining of the database . k gain A
(
Cfi )
= sup A
(
Cfi )
– sup A( )
The gain function of Fukuda et al . [ 12 ] is given above , where is a fractional constant between 0 and 1 . If confidence is held fixed at , which is c
, then this function is equal to
–( sup r( ) 1 c⁄
)
£ ( cid:217 ) £ ‡ ( cid:217 ) £ £ ( cid:222 ) ( cid:222 ) £ £ £ Æ æ £ £ £ Æ æ £ £ £ £ £ Æ æ £ £ £ Æ æ £ £ £ £ £ £ £ £ q · q Q )
) r c c
Q<
Q< and conf r( ) conf r2( and r2 gain r1( if we assume there exists any rule
. We can ignore the trivially monotone in support as long as satisfying case where . This is because for any the input constraints such that conf r1( such that , pair of rules r1 ) ) gain r2( irrespective of their support . we know that Should this assumption be false , the gain criteria is optimized by any rule with zero support should one exist ( eg in the conjunctive case , one can simply add conditions to a rule until its support drops to zero ) . The gain function is monotone in confidence among rules with equivalent support for reasons similar to the case of the Laplace function . If support is held constant , then an increase in gain implies a decrease in the subtractive term . The subtractive term can be decreased only by reducing antecedent support , which implies a from the Laplace function , larger confidence . Note that , like after identifying the optimal set of rules , the user can vary and view its effect on the optimal rule without additional mining . Another interestingness metric that is identical to gain for a fixed value of was introduced by Piatetsky Shapiro [ 19 ] :
) D⁄ sup C(
= k p s A
(
Cfi )
= sup A
(
Cfi )
– sup A( )sup C( )
D
Consider next conviction [ 8 ] , which was framed in [ 6 ] as a function of confidence : conviction A
(
Cfi )
=
) D sup C( ) Cfi –( D 1 )
– conf A
(
Conviction is obviously monotone in confidence since confidence appears in a subtractive term within the denominator . It is also unaffected by variations in rule support if confidence is held constant , which implies monotonicity . Lift , a well known statistical measure that can be used to rank rules in IBM ’s Intelligent Miner [ 14 ] ( it is also known as interest [ 8 ] and strength [ 10] ) , can also be framed as a function of confidence [ 6 ] : lift A
(
Cfi )
=
Cfi D conf A ) ) sup C(
( sc sc
Like conviction , lift is obviously monotone in confidence and unaffected by rule support when confidence is held fixed . The remaining interestingness metrics , entropy gain , gini , and chisquared value , are not implied by . However , we show that the space of rules can be partitioned into two sets according to confidence such that when restricted to rules in one set , each , and when restricted to rules in the other metric is implied by set , each metric is implied by . As a consequence , the optimal rules with respect to entropy gain , gini , and chi squared value must reside on either the upper or lower support confidence border . This idea is formally stated by the observation below . , U D OBSERVATION 3.4 : Given instance sc over the set of rules whose confidence is greater over the set , then an I optimal set where optimal set where implies t than equal to some value of rules whose confidence is less than or equal to optimal rule appears in either ( a ) any = Isc Is c(cid:216 )
, , U D , U D
, or ( b ) any
, sc s c(cid:216 )
, C N , ,
Isc Is c(cid:216 ) implies
, and
C N
C N s c(cid:216 ) s c(cid:216 )
, if
, t
=
= t
I
,
,
,
.
To demonstrate that the entropy gain , gini , and chi squared values satisfy the requirements put forth by this observation , we need to know when the total order defined by a rule value function is implied by . We use an analog of the Lemma 3.3 for this purpose : s c(cid:216 )
LEMMA 3.5 : The following conditions are sufficient for establishf r( ) defined over a rule value function t f r( ) ing that a total order is implied by partial order ( 1 ) dence , and ( 2 ) same support . f r( ) s c(cid:216 )
: is monotone in support over rules with the same confi is anti monotone in confidence over rules with the
⁄
(
–
= a = in ( and sup A( ) where
= Cfi sup A C¨ ) c= sup C( ) D⁄ function sup A C¨ ) terms of y f x y,( ) conf x y,( )
In [ 16 ] , the chi squared , entropy gain , and gini values of a rule are f x y,( ) each defined where given the rule x to which it is applied3 ( definitions appear in Appendix A ) . A These functions are further proven to be convex . Another important property of each of these functions is that they reach their minimum at any rule whose confidence is equal to the “ expected ” is minimum when confidence [ 17 ] . More formally , conf x y,( = ) and . To prove our claims , we exploit two properties c of convex functions from [ 11 ] : ( 1 ) Convexity of a function dividing point , x1 y1 ( 2 ) A convex function over a given region must be continuous at every point of the region ’s relative interior . The next two lemmas show that a convex function conf x y,( ) minimum at Observation 3.4 , where expected confidence value . LEMMA 3.6 : For a convex function which is has the properties required by from the observation is set to the implies that for an arbitrary of the line segment between two points
, x3 y3 , we have max f x1 y1
, ) f x2 y2
( f x3 y3 y+( x f x y,( ) f x y,( ) f x y,( ) x2 y2 and c=
.4 y
)
(
(
)
(
)
)
,
,
,
,
.
) x x y
, c= c‡
A c‡ conf x Y,( ) is ( 1 ) monotone in is fixed to a constant is clearly anti monotone in conf x y,( ) , and ( 2 ) monotone in conf x y,( c= ) , so long as y conf x y,( A= ) conf x Y,( ) f x y,( ) , conf x y,( ) for any constant c‡ which is minimum at for fixed when y Y conf x Y,( ) represents a horizontal line segment that leftward . The value of is also in this region as a consequence of its con for extends from point conf x Y,( anti monotone in vexity , it follows that For case ( 2 ) , assume the claim is false . This implies there exists A c‡ conf x y,( ) A= for some constant a vector . That is , there exist two along which is not monotone in y , ( ) and points where x2 y2 < ( ) ( see Figure 2 ) . If were y1 f x1 y1 y2 0 0,( ) , then this would contradict the defined to be minimum at is convex . But since fact that as well as are undefined at f this point , another argument is required . Consider then some sufficiently small non zero value and ( ) is convex and continuous in . Because f x1 y1 f
. Because conf x Y,( )
( ) x1 y1 , > ( ) f x2 y2 is monotone in defined by such that f x Y,( ) along yet
( f x2
0‡ y2 x2 d– d–
5 . f ,
> v v v
)
,
,
, f f f
Proof : For case ( 1 ) , when x
A
C and of the rule . which correspond to
3 We are making some simplifications : these functions are actually defined in terms of a vector defining the class distribution after a binary split . We C(cid:216 ) are restricting attention to the case where there are only two classes ( respectively ) . The binary split in our and y case is the segmentation of data set made by testing the antecedent D condition
4 This well known property of convex functions is sometimes given as the definition of a convex function , eg [ 16 ] . While this property is necessary for convexity , it is not sufficient . The proofs of convexity for gini , entropy , and chi squared value in [ 16 ] are nevertheless valid for the actual definition of convexity since they show that the second derivatives of these functions are always non negative , which is necessary and sufficient [ 11 ] . 5 We are not being completely rigorous due to the bounded nature of the is defined . For example , the point may not be within this bounded region since can be no . Verifying that these boundary conditions do not affect convex region over which conf x Y,( greater than the validity of our claims is left as an exercise . c= D x
) f
Q ‡ Q ‡ Q ‡ ‡ q Q £ £ £ £ Æ æ £ £ g £ £ g £ Æ æ £ Æ æ £ £ £ ‡ g d y
( x2 d–
,
) y2
(
, x2 y2
) conf(x,y ) = c
(
, x1 y1
)
(
, x3 y3
) x
Figure 2 . Illustration of case ( 2 ) from Lemma 36
0= , x1 y1 is guaranteed to exist its interior region , such a value of , which is a trivial boundary case . Now , consider unless x2 ) ] [ ( ) ( , the line . This line must contain a point x2 , ) ( such that are non negative , and one or both x3 y3 x3 or is convex and minimum of y3 x3 f , ( ) ( , which is a at f x2 x3 y3 contradiction . is non zero . But because , we have that d– , and
( ) f x1 y1 y2 y3 y2 d–
)
,
, c=
LEMMA 3.7 : For a convex function c£ for any constant conf x y,( ) fixed conf x y,( ) conf x y,( )
, so long as f x y,( )
A=
, y is ( 1 ) anti monotone in f x y,( ) which is minimum at conf x y,( ) for when y
, and ( 2 ) monotone in A c£
.
Proof : Similar to the previous . The previous two lemmas and Observation 3.4 lead immediately to the following theorem , which formalizes the fact that mining the upper and lower support confidence borders identifies the optimal rules according to metrics such as entropy gain , gini , and chisquared value . Conveniently , an algorithm specifically optimized for mining the upper border can be used without modification to mine the lower border by simply negating the consequent of the given instance , as stated by the subsequent lemma . , t THEOREM 3.8 : Given instance
C N defined over the values given by a convex function Cfi rules A sup A C¨ ( = ) sup A( ) ( 1 ) , and x ) D⁄ f x y,( sup C( ) ( 2 ) , is minimum at then an optimal set optimal rule appears in either ( a ) any Isc I where optimal set Isc Is c(cid:216 ) where Is c(cid:216 ) and y conf x y,( ) sup A C¨ )
, , U D , U D
, if f x y,( ) is t over
, sc s c(cid:216 )
, U D where :
= =
= =
=
–
(
I
,
,
,
=
, or ( b ) any
, C N , , . C N , any = Is c(cid:216 ) , C(cid:216 ) , U D evaluates to false ) is also an C C(cid:216 ) ) A
– . Thus , minimizes the confi
, s c(cid:216 ) ( where N
1 conf A = C(cid:216 )
, , U D C(cid:216 ) , , sc
Cfi )
C N
(
,
LEMMA 3.9 : Given an instance
optimal set for
Isc evaluates to true only when Is c(cid:216 )
optimal set .
Isc
Proof Idea : Note that conf A
( maximizing the confidence of dence of
Cfi
A
.
Before ending this section we consider one practical issue that of result visualization . Note that the support confidence borders as displayed in Figure 1 provide an excellent means by which optimal sets of rules may be visualized . Each border clearly illustrates the trade off between the support and confidence . Additionally , one can imagine the result visualizer color coding points along these borders that are optimal according to the various interestingness metrics , eg blue for Laplace value , red for chi squared value , green for entropy gain , and so on . The result of modifying minimum support or confidence on the optimal rules could be displayed in real time as the user drags a marker along either axis in order to specify a minimum bound . 3.3 Practical Implications for Mining Optimal Con junctions r
R
U sc
2U
( and
In this section we present and evaluate an algorithm that efficiently s c(cid:216 ) mines an optimal set of conjunctions according to due to Lemma 3.9 ) from many real world categorical data sets , without requiring any constraints to be specified by the user . We also demonstrate that the number of rules produced by this algorithm for a given instance is typically quite manageable on the order of a few hundred at most . We address the specific problem of mining optimal conjunctions within categorically valued data , where each condition in is simply a test of whether the given input record contains a particular attribute/value pair , excluding values from a designated class column . Values from the designated class column are used as consequents . While our algorithm requires no minimums on support or confidence , if they are specified , they can be exploited for better performance . Space constraints prohibit a full explanation of the workings of this algorithm , so we highlight only the most important features here . A complete description appears in an extended draft [ 7 ] . The algorithm we use is a variant of Dense Miner from [ 6 ] , which is a constraint based rule miner suitable for use on large and dense data sets . In Dense Miner , the rule mining problem is framed as a set enumeration tree search problem [ 21 ] where each node of the tree enumerates a unique element of . Dense Miner returns every rule that satisfies the input constraints , which include minimum support and confidence . We modified Dense Miner to that are potentially optimal instead maintain only the set of rules at any given point during its execution . Whenever a rule is enumerated by a node and found to satisfy the input constraints , it is compared against every rule presently in is better than or according to the partial incomparable to every rule already in order , then rule that is worse than is removed . Given this policy , assuming the tree enumerates every subset of Because an algorithm which enumerates every subset would be unacceptably inefficient , we use pruning strategies that greatly reduce the search space without compromising completeness . These strategies use Dense Miner ’s pruning functions ( appearing in Appendix B ) , which bound the confidence and support of any rule that can be enumerated by a descendent of a given node . To see how these functions are applied in our variant of the algorithm , consider a node . c can be pruned , the algorithm determines if there exists To see if a rule is some imaginary rule such that R and confidence with support . Given such a rule , if any s descendent of enumerates an optimal rule , then it must be g equivalent to . This equivalence class is already represented in r can R be pruned . This algorithm differs from Dense Miner in only two additional ways . First , we allow the algorithm to perform a set oriented bestfirst search of the tree instead of a purely breadth first search . Dense miner uses a breadth first search since this limits the number of database passes required to the height of the search tree . In the context of optimized rule mining , a breadth first strategy can be inefficient because pruning improves as better rules are found , and good rules sometimes arise only at the deeper levels . A pure best first search requires a database pass for each node in the tree , which would be unacceptable for large data sets . Instead , we process several of the best nodes ( at most 5000 in our implementation ) with each database pass in order to reduce the
, so there is no need to enumerate these descendents , and and confidence bound with support bound
, upon termination ,
. Also , any rule in is an optimal set . is added to
, where g in
. If sc ri ri
U
R
R
R
R
R g g c r r r r r s d £ £ Æ æ £ £ Æ æ £ Æ æ £ Æ æ £ Æ æ fi fi £ £ £ k
N number of database passes while still substantially reducing the search space . For this purpose , a node is better than another if the rule it enumerates has a higher confidence value . The remaining modification is the incorporation of inclusive pruning as proposed by Webb [ 23 ] . This pruning strategy avoids enumerating a rule when it can be determined that its antecedent can be extended with an additional condition without affecting the support of the rule . In the absence of item constraints , this optimization prunes many rules that are either non optimal or equivalent to some other optimal rule to be enumerated . Unfortunately , when there are item constraints in ( eg “ rules must contain fewer than conditions ” ) , this pruning strategy cannot be trivially applied without compromising completeness , so it must be disabled . Full details of this pruning strategy are provided in Appendix B . We evaluated our algorithm on the larger of the categorical datasets from the Irvine machine learning database repository,6 including chess , mushroom , letter , connect 4 , and dna . We also used the pums data set from [ 6 ] which is compiled from census data ( a similar data set was used in [ 8] ) . For the Irvine data sets , we used each value of the designated class column as the consequents . For the pums data set , we used the values of the RELAT1 column ( 13 in all)7 . Each of these data sets is known to be difficult for constraint based rule mining algorithms such as Apriori , even when specifying a strong minimum support constraint [ 4,6,8 ] . Experiments were performed on an IBM IntelliStation with 400 MHZ Intel Pentium II processor and 128 MBytes of main memory . Execution time and the number of rules returned by the algorithm appear in Table 1 ; characteristics of each data set appear in Table 2 . For the Irvine data sets , with the exception of connect 4 , our algorithm identified an optimal set of rules within 30 seconds in every case , with many runs requiring less than 1 second . Connect 4 was the most difficult of the data sets for two reasons . First , it has substantially more records and more columns than many of the other data sets . But a stronger contributor to this discrepancy was the fact that rules with high confidence within the connect 4 dataset have very low support . For example , with the “ tie ” class as the consequent , rules with 100 % confidence have a support of at most 14 records . This property greatly reduces pruning effectiveness , resulting in almost one hour of execution time given this consequent . In cases like these , modest settings of the minimum support or confidence constraint can be used to improve runtime considerably . For example , a minimum support of 676 records ( 1 % of the data set ) reduces execution time to 6 minutes . The number of rules in each optimal set was on the order of a few hundred at most . Of the Irvine data sets , connect 4 contained the most optimal rules , with 216 for win , 171 for tie , and 465 for lose . We plot the upper support confidence border for each of these consequents in Figure 3 . Rule support is normalized according to consequent support so that each border ranges from 0 to 100 % along the 4 . PC Optimality 4.1 Definition While sc optimality is a useful concept , it tends to produce rules that primarily characterize only a specific subset of the population axis . x
6 http://wwwicsuciedu/~mlearn/MLRepositoryhtml 7 This data set is available in the form used in these experiments through : http://wwwalmadenibmcom/cs/quest The values 1 13 for the RELAT1 column correspond to items 1 13 in the apriori binary format of this data .
Consequent
Time ( sec )
# of Rules
Data set chess connect 4 letter dna mushroom pums edible poisonous win nowin win draw loss A Z EI IE N
60 41 216 171 465 322 9 15 9 12 7 324 702 267 152 91 81 183 210 383 424 165 11 102 Table 1 . Execution time and number of rules returned .
<1 <1 642 3066 1108 18 20 23 <1 <1 <1 740 204 509 174 46 19 50 270 843 572 88 12 22
1 2 3 4 5 6 7 8 9 10 11 12 13
) % ( e c n e d i f n o C
100
90
80
70
60
50
40
30
20
10
0
0
10
20 win tie lose
80
90 100
40
30 70 sup(r)/sup(C ) ( % )
50
60
Figure 3 . Upper support/confidence borders for Connect 4 .
C of interest ( by population of interest , we mean the set of records for which condition evaluates to true ) . In this section , we propose another partial order with the goal of remedying this deficiency . Cfi is simply the set of records First , the population of a rule evaluate to true . We from data set C A pop r( ) denote the population of a rule . Clearly then , pop r( ) within its population is said to characterize
. A rule which contains some record for which both and as r sup r( )
D
A
= t t
.
Data set chess connect 4 dna letter mushroom pums
# of Rows 3,196 67,557 3,124 20,000 8,124 49,046
# of Columns
37 43 61 17 23 74
<
<
>
) )
) )
) )
) )
) ) pc pc r1 p c(cid:216 ) if and
, or . if and only if : on rules where pop r2( pop r2( pop r2( pop r2( pop r1( pop r1( conf r2( conf r2( conf r1( conf r1( conf r1( conf r1( p c(cid:216)< r2 pop r1( ) pop r1( )
Table 2 . Number of rows and columns in each data set . r2
Consider now the partial order only if : • • Two rules are equivalent according to this partial order if their population sets are identical and their confidence values are equal . One can analogously define the partial order where r1 • • 4.2 Theoretical Implications It is easy to see that ) , so all the claims from Section 3 also hold with respect to pcoptimality . Note that results in more incomparable rule pairs than due to the population subsumption requirement . This implies there will be many more rules in a pc optimal set compared to an sc optimal set . The consequence of these additional rules , as formalized by the observation below , is that a pc optimal set always contains a rule that is optimal with respect to any of the previous interestingness metrics , and further with respect to any constraint that requires the rules to characterize a given subset of the population of interest . OBSERVATION 4.1 : Given an instance conf r2( conf r2( is implied by
, U D where or .
( and
C N p c(cid:216 ) s c(cid:216 ) by pc pc sc sc
) )
) )
=
I
,
,
, t
( 1 ) ( 2 ) an Ipc is implied by t has a constraint pc n
, and on rules
N a given subset optimal rule appears in any I =
, U D n{ }
C N
P
– r
,
,
.
, pc stating that
P pop r( ) for of the population of interest ,
Ipc
optimal set where t r
R
P such that ( 1 ) each rule in
We note that this generalization of sc optimality is quite broad . Given the large number of rules that can appear in a pc optimal set ( see next subsection ) , a more controlled generalization might be desirable . One idea is to have the rule set support any constraint that requires a rule to characterize a given member ( in place of a subset ) of the population of interest . This would still guarantee a broad characterization , but potentially with much fewer rules . This as uninteresting if there exists some set notion designates a rule of rules satisfies the input is characterized by at least constraints , ( 2 ) every member of one rule in r R . ( Note that pc optimality designates a rule as according to uninteresting only if there exists such a set containing exactly one rule . ) This notion of uninterestingness cannot , to our knowledge , be expressed using a partial order on rules . Instead , we are examining how the concept of pc optimality combined with constraints can successfully express ( and exploit ) this notion . 4.3 Practical Implications for Mining Optimal Con is equal to or better than and ( 3 ) each rule in pop r( ) sc
R
R
R junctions
Producing an algorithm that can efficiently mine an optimal set of conjunctions with respect to may seem an impossible proposition in large data sets due to the need to verify subsumption pc between rule populations over many rules . However , we will show that we can verify the relationships induced by the partial order “ syntactically ” in many cases ; that is , by checking the conditions of the rules along with support and confidence , instead of examining the set of database records that comprise their populations . Keep in mind that in this subsection we are restricting attention to mining optimal conjunctions . In typical formulations of mining optimized disjunctions ( eg [ 12,20] ) , syntactic checks for population subsumption simply require geometrically comparing the regions represented by the base conditions . DEFINITION ( A MAXIMAL ) : A rule
Cfi is a maximal if there and
A1 A2 that is no sup A1
( rule Cfi ) =
A2 ( sup A2
Cfi
Cfi )
A1 such .
)
)
=
=
) ) r r and amax( ) amax r( ) amax r2(
Proof : The
) pop r2( pop r2( )
) amax r1( amax r1( ) amax r2( ) ) amax r2(
Note that the definition of an a maximal rule is such that it need not satisfy the input constraints . The intuition behind a maximality ( “ antecedent ” maximality ) is that an a maximal rule cannot have its antecedent enlarged without strictly reducing its population . , we denote an a maximal rule that has the same Given a rule . Lemma 4.2a implies there is only one as population of is in fact a function . The such rule , which means that purpose of this function is to provide a concise , canonical description of a rule ’s population that allows for efficiently checking the subsumption condition ( Lemma 42 ) In turn , this pc provides a syntactic method for comparing rules with ( Theorem 43 ) LEMMA 4.2 A : B : pop r1( ) = and r1 amax r1( ) ) pop r2( r2 is different than ) amax r2( pop r1( pop r1( direction is immediate for both cases , so we prove the direction . Suppose the claims are false and consider the rule which is formed by taking the union of the antecedents r3 amax r1( from . We establish each claim by contradiction . has the For claim A , if . Since we assume the claim is same population as . Given false , we have that ) amax r1( this , either . But since all three rules have the same population , this leads to the contradiction that either is not a maximal . ) . For claim B , if by As a consequence , we must have that claim A from above . Since we assume the claim is false , we must or amax r1( . The case where they are equal is an immediate contradiction due to claim A . For the case where amax r1( , which contradicts the fact that pc r2 r1 ) conf r1( conf r2( ) ( 1 ) conf r1( ) ) conf r2( ( 2 ) if and only if B : pc= r2 r1 conf r1( ) = amax r1( r3 if and only if : amax r1( ) and ) amax r1( and in addition have )
THEOREM 4.3 A : <
, then amax r1(
) = amax r3(
) amax r2( amax r2( amax r2(
) amax r3( amax r2( .
, then clearly pop r3( = pop r1( )
, note that amax r2( amax r2( amax r1( amax r3( amax r2( amax r1( amax r1( amax r3( amax r2( amax r2( amax r2( conf r2( pop r1( pop r2(
, or . and that or or r3
) )
<
=
=
=
)
)
)
)
)
)
)
)
)
)
)
)
.
)
)
)
)
)
) amax r( )
These facts cannot be trivially applied in an efficient manner since computing requires scanning the data set ( an algorithm for this purpose appears in Figure 4 ) . Instead , we describe pruning optimizations which can be incorporated into a rule miner to avoid generating many rules that are not pc optimal . A post processing phase then computes for every rule identified in one final pass over the database , and this information is used to efficiently identify any remaining non optimal rules . amax r( )
£ ˝ ( cid:217 ) ( cid:204 ) ( cid:217 ) £ £ ( cid:217 ) ˝ ‡ ( cid:217 ) ( cid:204 ) £ £ £ £ £ £ £ Æ æ £ £ ˝ £ Æ æ £ £ ( cid:204 ) £ ( cid:219 ) ( cid:204 ) ( cid:219 ) ( cid:201 ) ( cid:220 ) ( cid:222 ) ( cid:204 ) ( cid:204 ) ( cid:204 ) ( cid:204 ) ( cid:204 ) ( cid:204 ) ¨ ˚ £ ( cid:201 ) r and a data set
INPUT : a rule OUTPUT : 1 . Find the first record in amax r( )
D population of conditions of U not already in r
D that is a member of the . Initialize a set to contain those r that evaluate to true on this record , but are .
A
2 . For each remaining record in
D , remove any condition in r of on this record . which is in the population which evaluates to false A
3 . Add the conditions in the result .
A to the antecedent of r and return
Figure 4 . Algorithm for computing amax r( )
.
,
Ipc
INPUT : = OUTPUT : An 1 . Find all rules with positive improvement among those
, U D optimal set .
C N
, pc
Ipc
, satisfying the input constraints . Call this set of rules
R
. r r with amax r( ) amax r( ) and put
2 . For each rule
3 . Remove any rule
, associate . in R into a set Ra from r1 pc such that such that r1 , if there is more than one rule ra Ra ra= amax r( ) . r2 R˛ if there exists some according to Theorem 43a in , then remove all but one of
4 . For each rule such that R
R r2
< r
R them from .
R
5 . Return
Figure 5 . Algorithm for mining a pc optimal set of conjunctions .
OBSERVATION 4.4 : Given an instance rule A2 and
Cfi where >
A1 Cfi conf A2( ) cannot be A2 A1 conf A1( ) .
, U D
,
=
Ipc
, a optimal if there exists some rule satisfies the input constraints , A2
C N
Ipc ,
, pc
,
.
N
Using the terminology from [ 6 ] , this observation simply states that a pc optimal rule must have a non negative improvement value . We can in fact require that improvement be positive since we only need one member of each equivalence class . The Dense Miner algorithm already provides pruning functions that bound the improvement value of any rule derivable from a given node . We thus modify Dense Miner to prune nodes whose improvement bound is 0 ( see Appendix B for the simplified pruning functions that can be used for this special case ) . We also again exploit Webb ’s inclusive pruning strategy for the case where there are no item constraints in We can now fully describe an algorithm for mining an optimal set of rules without performing any explicit subsumption checks between rule populations ( Figure 5 ) . We use the above described variant of Dense Miner to implement step 1 of this algorithm . Step 3 applies Theorem 4.3a to identify non optimal rules for removal . This step requires we first associate each rule with its a maximal rule ( step 2 ) . To find the a maximal rules , we apply the algorithm in Figure 4 for each rule in , using a single shared database scan . For a data set such as connect 4 , our implementation of this step requires less than 3 seconds for a set with up to 10,000 rules . Finally , step 4 applies Theorem 4.3b in order to identify equivalent rules so that there is only one representative from each equivalence class in the returned result . This algorithm could be simplified slightly when the input constraints N could be returned whenever r have the property that N does . In this case , the set satisfies
Ipc amax r( ) Ra
R
Data set chess connect 4 letter dna mushroom pumsb* edible poisonous
Time ( sec )
Consequent win nowin win draw loss A Z EI IE N
# of Rules 236,735 42,187 178,678 119,984 460,444 37,850 55,347 49,505 9,071 217 389 84,594 33,443 14,927 28,553 21,244 5,717 15,474 22,992 160,908 175,061 5,701 991 59,088 Table 3 . Execution time and number of rules returned .
2,821 504 19,992 18,123 34,377 65 64 46 53 1 3 1,058 829 305 770 308 59 412 428 3,079 3,857 118 12 482
1 2 3 4 5 6 7 8 9 10 11 12 13 k
Ra immediately following step 2 ( since is a set , we assume it contains no duplicates ) . However , some common rule constraints ( eg “ a rule must have fewer than conditions ” ) do not have this property . In practice , we find that the number of rules returned by this algorithm is considerably larger than that of the algorithm for mining sc optimal sets . On most data sets , rule constraints such as minimum support or confidence must be specified to control the size of the output as well as execution time . For the execution times reported in Table 3 , the minimum support constraint was set to ensure that each rule ’s population is at least 5 % of the population of interest . For the pums data set , this constraint was not sufficiently strong to control combinatorial explosion . We therefore simplified the data set by removing values which appear in 80 % or more of the records ( the resulting data set is known as pumsb* , and was used in [ 5] ) . We conclude that pc optimal sets of rules are useful primarily on data sets where the density is somewhat subdued , or when the user is capable of specifying strong rule constraints prior to mining . 5 . Conclusions We have defined a new optimized rule mining problem that allows a partial order in place of the typical total order on rules . We have also shown that solving this optimized rule mining problem with respect to a particular partial order ( and in some cases its analog ) is guaranteed to identify a most interesting rule according to several interestingness metrics including support , confidence , gain , laplace value , conviction , lift , entropy gain , gini , and chi squared value . In practice , identifying such an optimal set of conjunctive rules can be done efficiently , and the number of rules in such a set is typically small enough to be easily browsed by s c(cid:216 ) sc
£ Æ æ ( cid:204 ) £ Æ æ ˛ £ £ pc an end user . We lastly generalized this concept using another partial order in order to ensure that the entire population of interest is well characterized . This generalization defines a set of rules that contains the most interesting rule according to any of the above metrics , even if one requires this rule to characterize a specific subset of the population of interest . These techniques can be used to facilitate interactivity in the process of mining most interesting rules . After mining an optimal set of rules according to the first partial order , the user can examine the most interesting rule according to any of the supported interestingness metrics without additional querying or mining of the database . Minimum support and confidence can also be modified and the effects immediately witnessed . After mining an optimal set of rules according to the second partial order , in addition to the above , the user can quickly find the most interesting rule that characterizes any given subset of the population of interest . This extension overcomes the deficiency of most optimized rule miners where much of the population of interest may be poorly characterized or completely uncharacterized by the mined rule(s ) . Acknowledgments We are Gunopulos for their helpful suggestions and assistance . References [ 1 ] Agrawal , R . ; Imielinski , T . ; and Swami , A . 1993 . Mining to Ramakrishnan Srikant and Dimitrios indebted
Associations between Sets of Items in Massive Databases . In Proc . of the 1993 ACM SIGMOD Int’l Conf . on Management of Data , 207 216 .
[ 2 ] Agrawal , R . ; Mannila , H . ; Srikant , R . ; Toivonen , H . ; and
Verkamo , A . I . 1996 . Fast Discovery of Association Rules . In Advances in Knowledge Discovery and Data Mining , AAAI Press , 307 328 .
[ 3 ] Ali , K . ; Manganaris , S . ; and Srikant , R . 1997 . Partial Classification using Association Rules . In Proc . of the 3rd Int'l Conf . on Knowledge Discovery in Databases and Data Mining , 115 118 .
[ 4 ] Bayardo , R . J . 1997 . Brute Force Mining of High Confidence
Classification Rules . In Proc . of the Third Int’l Conf . on Knowledge Discovery and Data Mining , 123 126 .
[ 5 ] Bayardo , R . J . 1998 . Efficiently Mining Long Patterns from
Databases . In Proc . of the 1998 ACM SIGMOD Int’l Conf . on Management of Data , 85 93 .
[ 6 ] Bayardo , R . J . ; Agrawal , R . ; and Gunopulos , D . 1999 . Con straint Based Rule Mining in Large , Dense Databases . In Proc . of the 15th Int’l Conf . on Data Engineering , 188 197 . [ 7 ] Bayardo , R . J . and Agrawal , R . 1999 . Mining the Most Inter esting Rules . IBM Research Report . Available from : http://wwwalmadenibmcom/cs/quest [ 8 ] Brin , S . ; Motwani , R . ; Ullman , J . ; and Tsur , S . 1997 .
Dynamic Itemset Counting and Implication Rules for Market Basket Data . In Proc . of the 1997 ACM SIGMOD Int’l Conf . on the Management of Data , 255 264 .
[ 9 ] Clark , P . and Boswell , P . 1991 . Rule Induction with CN2 :
Some Recent Improvements . In Machine Learning : Proc . of the Fifth European Conference , 151 163 .
[ 10 ] Dhar , V . and Tuzhilin , A . 1993 . ABSTRACT driven pattern discovery in databases . IEEE Transactions on Knowledge and Data Engineering , 5(6 ) .
[ 11 ] Eggleston , H . G . 1963 . Convexity . Cambridge Tracts in Math ematics and Mathematical Physics , no . 47 . Smithies , F . and Todd , J . A . ( eds ) Cambridge University Press .
[ 12 ] Fukuda , T . ; Morimoto , Y . ; Morishita , S . ; and Tokuyama , T .
1996 . Data Mining using Two Dimensional Optimized Association Rules : Scheme , Algorithms , and Visualization . In Proc . of the 1996 ACM SIGMOD Int’l Conf . on the Management of Data , 13 23 .
[ 13 ] Goethals , B . and Van den Bussche , J . 1999 . A Priori Versus A Posteriori Filtering of Association Rules . In Proc . of the 1999 ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery , paper 3 .
[ 14 ] International Business Machines , 1996 . IBM Intelligent Miner
User ’s Guide , Version 1 , Release 1 .
[ 15 ] Mitchell , T . M . 1997 . Machine Learning . McGraw Hill , Inc . [ 16 ] Morimoto , Y . ; Fukuda , T . ; Matsuzawa , H . ; Tokuyama , T . ; and Yoda , K . 1998 . Algorithms for Mining Association Rules for Binary Segmentations of Huge Categorical Databases . In Proc . of the 24th Very Large Data Bases Conf . , 380 391 . [ 17 ] Morishita , S . 1998 . On Classification and Regression . In
Proc . of the First Int’l Conf . on Discovery Science Lecture Notes in Artificial Intelligence 1532:40 57 .
[ 18 ] Nakaya , A . and Morishita , S . 1999 . Fast Parallel Search for
Correlated Association Rules . Unpublished manuscript .
[ 19 ] Piatetsky Shapiro , G . 1991 . Discovery , Analysis , and Presentation of Strong Rules . Chapter 13 of Knowledge Discovery in Databases , AAAI/MIT Press , 1991 .
[ 20 ] Rastogi , R . and Shim , K . 1998 . Mining Optimized Associa tion Rules with Categorical and Numeric Attributes . In Proc . of the 14th Int’l Conf . on Data Engineering , 503 512 .
[ 21 ] Rymon , R . 1992 . Search through Systematic Set Enumera tion . In Proc . of Third Int’l Conf . on Principles of Knowledge Representation and Reasoning , 539 550 .
[ 22 ] Srikant , R . ; Vu , Q . ; and Agrawal , R . 1997 . Mining Association Rules with Item Constraints . In Proc . of the Third Int'l Conf . on Knowledge Discovery in Databases and Data Mining , 67 73 .
[ 23 ] Webb , G . I . 1996 . Inclusive Pruning : A New Class of Pruning
Axiom for Unordered Search and its Application to Classification Learning . In Proc . of the 1996 Australian Computer Science Conference , 1 10 .
[ 24 ] Webb , G . I . 1995 . OPUS : An Efficient Admissible Algorithm for Unordered Search . Journal of Artificial Intelligence Research , 3:431 465 .
Appendix A Here we provide definitions for the gini , entropy gain , and chisquared rule value functions . For a given condition , we denote the fraction of records that satisfy the consequent condition C , and the fraction of records among those that satisfy that do not satisfy the consequent condition among those that satisfy p A( ) as as
A
A
A p A( )
= p A( )
. Note then that : Cfi sup A ( ) sup A( ) and p A( )
=
Cfi sup A( ) )
– ( sup A sup A( )
£ gini A
(
Cfi
) = 1
[ p ˘(
–
)2
+ p ˘(
]
)2
– sup A( ) –( 1
D
[ p A( )2
+ p A( )2
]
)
– sup A(cid:216)( ) –( 1
D
[ p A(cid:216)(
)2 p A(cid:216)(
)2
]
)
+ ent A
(
Cfi
) =
[ p ˘(
–
)log p ˘(
(
) )
+ p ˘(
)log p ˘(
(
]
)
)
–
– sup A( ) p A( )log p A( )
[
(
D
)
+ p A( )log p A( )
(
)
] sup A(cid:216)( p A(cid:216)(
[
)
D
)log p A(cid:216)(
(
)
)
+ p A(cid:216)(
)log p A(cid:216)( )
(
]
) chi2 A
(
Cfi
) =
]2 sup A( ) p A( ) sup A(cid:216)(
) p A(cid:216)( [ p ˘( p ˘(
]2
–
–
[
)
)
)
]2 sup A( ) p A( ) p ˘(
) p A(cid:216)( [ ) sup A(cid:216)( p ˘(
]2
–
–
[
)
)
+
– p ˘(
– p ˘(
)
)
)
C
D p ˘( terms –
Since and are constant for a given instance of the problem , terms such as are constants . Any of the above as variable functions of Cfi ) , and hence so x = x+ sup A( ) can the functions themselves . For example , , y x+( ) sup A(cid:216)( ) ⁄ = , y x and so on . expressed sup A ( y x+( y
= can Cfi ) ( ) x+( y
) D⁄ sup C( be and sup A( ) sup A p A( ) p A( )
D
=
=
=
=
,
–
, y
)
⁄
Appendix B Set Enumeration Tree Search . The set enumeration tree search framework is a systematic and complete tree expansion procedure for searching through the power set of a given set . The idea is to first impose a total order . The root node of the tree will enumerate the on the elements of will enumerate those sets that empty set . The children of a node can be formed by appending a single element of , with the restriction that this single element must follow every element already in according to the total order . For example , a fullyexpanded set enumeration tree over a set of four elements ( where each element of the set is denoted by its position in the ordering ) appears in Figure 6 . to
U
U
U
N
N
N
{}
2
3
4
1,3
1,4
2,3
2,4
3,4
1
1,2
1,2,3
1,2,4
1,3,4
2,3,4
1,2,3,4
Figure 6 . A complete set enumeration tree over a 4 item set . g
U
U the
. The set of viable elements of h g( ) . Making
To facilitate pruning , we use a node representation called a group where the set enumerated by a node is called the head and h g( ) denoted which can be appended to in order to form a child is called the tail and t g( ) denoted tail elements explicit enables optimizations such as element reordering ( for dynamic tree rearrangement ) and tail pruning . Element reordering involves locally changing the total order on at each node in the tree to maximize pruning effectiveness . Tail pruning involves removing elements from the tail if they cannot possibly be part of any solution in the sub tree rooted at the node . This directly reduces the search space , and indirectly reduces the search space by improving the bounds we compute on values such as the confidence of any rule that can be enumerated by a descendent of the node . Pruning with Confidence , Support , and Improvement We say a rule since the consequent it fixed ) is derivable from a group h g( ) enumerated by a descendent of also derivable from • The function
( which we represent using only its antecedent if . By definition , any rule that can be in the set enumeration tree is
. From [ 6 ] we have that : ) sup h g( ) • The value of • If the confidence bound given by provides an upper bound on the confidence of any conjunctive rule derivable from a given are non negative integers such that and group y C(cid:216 ) ) and y g fc x y,( x t g( ) from above provides an upper bound on support . from above for some group enumerg ated thus far such that satisfies the input constraints , then the maximum improvement of any derivable rule is zero . is less than or equal to the confidence of any rule g y+( x is a subset of sup h g( )
, where
Cfi )
, and and h g( ) h g( ) t g( ) g ( fc
= g
. x x x
) r r r r r r
(
⁄
• If the following value is equal to zero , then the maximum improvement of any derivable rule is zero :
= min
( u"
, h g( )
( sup h g( )
(
– u{ }
) u(cid:216){
}
C(cid:216 ) )
) g g g
T
U
Cfi ) sup h g( ) our = partial u{ } is a superset of
We refer the reader to [ 6 ] for details on how to compute the above values economically given that the data set is large , and how to heuristically reorder the elements of in order to ensure these functions have plenty of pruning opportunities . Inclusive Pruning of the tail Webb ’s inclusive pruning strategy [ 23 ] moves a subset into its head whenever the following fact can be of a group , then at least one established : if some solution is derivable from of the solutions derivable from . For example , T in the case of mining optimized conjunctions according to the Laplace function ( and many of the other metrics we have examined including suppose ( sup h g( ) in u t g( ) . Ignoring the effects of rule constraints , if some rule r derivable from , which is also derivable from . If one or more such elements are found in the tail of some node , instead of expanding its children , these elements can all be moved into the head to form a new node that replaces it . Some constraints may unfortunately prohibit straightforward application of this particular inclusive pruning strategy . For from participating in a example , an item constraint may disallow solution when combined with some other items from and t g( ) . Another problematic constraint is one which bounds the size of a rule ’s antecedent . Luckily , work arounds are typically possible . For example , in the case where a bound is specified on the number of base conditions that may appear in an antecedent , the strategy can be applied safely whenever is optimal , then so is the rule for some element orders ) ,
Cfi ) u{ } k£ h g( ) h g( ) t g( ) g g u k r
(
.
( cid:204 ) ¨ ˝ fi ¨ £ ‡ b ˛ fi ¨ ¨ ¨ ¨
