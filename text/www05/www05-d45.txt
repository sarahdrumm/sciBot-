CubeSVD : A Novel Approach to Personalized Web Search∗
Jian Tao Sun
Dept . of Computer Science
TsingHua University Beijing 100084 , China sjt@mailstsinghuaeducn
Hua Jun Zeng
Microsoft Research Asia
5F , Sigma Center , 49 Zhichun Road , Beijing 100080 , China hjzeng@microsoft.com
Huan Liu
Dept . of Computer Science
Arizona State University
Tempe , AZ85287 8809 , USA hliu@asu.edu
Yuchang Lu
Dept . of Computer Science
TsingHua University Beijing 100084 , China lyc@tsinghuaeducn
Zheng Chen
Microsoft Research Asia
5F , Sigma Center , 49 Zhichun Road , Beijing 100080 , China zhengc@microsoft.com
ABSTRACT As the competition of Web search market increases , there is a high demand for personalized Web search to conduct retrieval incorporating Web users’ information needs . This paper focuses on utilizing clickthrough data to improve Web search . Since millions of searches are conducted everyday , a search engine accumulates a large volume of clickthrough data , which records who submits queries and which pages he/she clicks on . The clickthrough data is highly sparse and contains different types of objects ( user , query and Web page ) , and the relationships among these objects are also very complicated . By performing analysis on these data , we attempt to discover Web users’ interests and the patterns that users locate information . In this paper , a novel approach CubeSVD is proposed to improve Web search . The clickthrough data is represented by a 3 order tensor , on which we perform 3 mode analysis using the higher order singular value decomposition technique to automatically capture the latent factors that govern the relations among these multi type objects : users , queries and Web pages . A tensor reconstructed based on the CubeSVD analysis reflects both the observed interactions among these objects and the implicit associations among them . Therefore , Web search activities can be carried out based on CubeSVD analysis . Experimental evaluations using a real world data set collected from an MSN search engine show that CubeSVD achieves encouraging search results in comparison with some standard methods .
Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Information Search and Retrieval Search Process ; H35 [ Information Storage and Retrieval ] : Online Information Services Web based services
∗This work was conducted and completed while the first au thor was doing internship at Microsoft Research Asia , Beijing , China . Copyright is held by the International World Wide Web Conference Committee ( IW3C2 ) . Distribution of these papers is limited to classroom use , and personal use by others . WWW 2005 , May 10 14 , 2005 , Chiba , Japan . ACM 1 59593 046 9/05/0005 .
General Terms Algorithms , Experimentation , Performance
Keywords Clickthrough Data , CubeSVD , Higher Order Tensor , Singular Value Decomposition , Personalized Web Search , Searching and Ranking
1 .
INTRODUCTION
The increase of WWW resources has fueled the demand for effective and efficient information retrieval . Millions of searches are conducted every day on search engines such as Yahoo! , Google and MSN , etc . Despite the popularity , search engines have their deficiencies : given a query , they usually return a huge list of results and the pages ranked at top may not meet users’ needs . One reason for this problem is the keyword based query interface , which is difficult for users to describe exactly what they need . Besides , typical search engines often do not exploit user information . Even two users submit the same query , their information need may be different [ 4 , 16 ] . For example , if a query “ jaguar ” is issued to Google , 11,900,000 results are returned . Regardless of who submits the query , both the pages returned and the rank orders are identical . Since “ jaguar ” may refer to “ jaguar car ” or “ jaguar cats ” , two users with different interests may want the search results ranked differently : a car fan may expect car relevant pages ranked highly , however , these pages may be unnecessary to be displayed for a zoologist . Thus the search results should be adapted according to the person who submits the query and which query he/she submits .
Personalized Web search is to carry out retrieval for each user incorporating his/her own information need . As the competition in search market increases , some search engines have offered the personalized search service . For example , Google ’s Personalized Search allows users to specify the Web page categories of interest [ 1 ] . Some Web search systems use relevance feedback to refine user needs or ask users to register their demographic information beforehand in order to provide better service[2 , 8 ] . Since these systems require users to engage in additional activities beyond search to specify/modify their preferences manually , approaches that are able to implicitly capture users’ information needs should be developed .
This paper focuses on utilizing clickthrough data to improve Web search . Consider the typical search scenario : a user submits a query to a search engine , the search engine returns a list of ranked Web pages , then the user clicks on the pages of interest . After a period of usage , the server side will accumulate a collection of clickthrough data , which records the search history of Web users . The data objects contained in the clickthrough data are of different types : user , query and Web page . Furthermore , these objects are highly interrelated and relationships among them are complicated [ 25 ] . For example , users with similar information needs may visit pages of similar topic even they submit different queries ; users with dissimilar needs may visit different pages even they submit the same query , as the “ jaguar ” example indicates . It can be assumed that the clickthrough data may reflect Web users’ interests and may contain patterns that users found their information [ 13 , 14 ] . By performing analysis on the clickthrough data , we attempt to discover the latent factors that govern the associations among these multi type objects . Consequently , Web pages can be recommended according to the associations captured .
Here we clarify some characteristics specific to personalized Web search based on clickthrough data analysis . This task is related to recommender systems which have been extensively studied [ 3 , 6 , 11 , 21 ] . While most recommendation algorithms like Collaborative Filtering ( CF ) are applied to two way data containing user preferences over items , the clickthrough data analysis deals with three way co occurrence data . As far as we know , previous literature on recommendation contains few studies on data of this kind . The threeway clickthrough data imposes at least two challenges :
1 ) The relations among user , query and Web page are complicated . There exist intra relations among objects of the same type , as well as inter relations among objects of different type [ 25 ] . For personalized Web search tasks , what we are concerned about are the 3 order relations among them . That is , given a user and a query issued by the user , the purpose is to predict whether and how much the user is interested in a Web page . Therefore , a unified framework is needed to model the multi type objects and the multi type relations among them .
2 ) The three way data are highly sparse . As we know , most collaborative filtering algorithms are susceptible to data sparsity [ 21 , 3 ] . For clickthrough data , the sparseness problem becomes more serious because each user only submits a small number of queries , and only a very small set of Web pages are visited by each user . Latent Semantic Indexing ( LSI ) [ 7 ] has been proved useful to address the data sparseness problem in two way data recommender systems [ 20 , 21 ] , however , it is still an open problem for the three way data case .
In order to address the problems mentioned above , we need an approach dealing with the clickthrough data which is three way and highly sparse . In this paper , we develop a unified framework to model the three types of objects : user , query and Web page . The clickthrough data is represented by a 3 order tensor . Then we perform 3 mode analysis using the Higher Order Singular Value Decomposition ( HOSVD ) technique [ 15 ] . Because our tensor representation is 3 dimensional and our approach is a multilinear exten sion of the matrix Singular Value Decomposition ( SVD ) , we name it CubeSVD .
The remainder of this paper is organized as follows . Section 2 provides related work . Section 3 gives a brief introduction to SVD and HOSVD techniques . Section 4 describes our proposed CubeSVD algorithm . Section 5 presents the experimental results and Section 6 offers some concluding remarks and directions for future research .
2 . RELATED WORK
In this section we briefly present some of the research literature related to personalized Web search , recommender systems , SVD for recommendation , clickthrough data relevant mining technique and Higher Order Singular Value Decomposition ( HOSVD ) .
Some previous personalized search techniques , eg , [ 2 , 16 , 19 ] , are mostly based on user profiling . Generally , user profiles are created by asking users to fill out registration forms or to specify the Web page categories of their interests [ 1 ] . Users have to modify their preferences by themselves if their interests change . There are also some works on automatic creation of user preferences . In [ 23 ] , user profiles were updated by accumulating their preferences reflected in the past browsing history . In [ 16 ] , the user profile was represented by a hierarchical category tree and the corresponding keywords associated with each category . The user profile was automatically learned from the user ’s search history .
Many current Web search engines focus on hyperlink structures of the Web . For example , Google calculated a universal PageRank vector which reflects the relative importance of each page . Personalized PageRank , which is a modification of global PageRank , was first proposed for personalized Web search in [ 18 ] . In [ 10 ] , “ topic sensitive ” PageRank was proposed to improve personalized Web search . The authors proposed to compute a set of PageRank vectors which capture the page importance with respect to a particular topic . Since no user ’s context information is used in this approach , it is difficult to evaluate whether the results achieved satisfy a user ’s information need .
Besides search engines , many recommender systems have been developed which recommend movies , music , Web pages , etc . Most recommender systems analyze a matrix containing user preferences over items . Among the algorithms used , Collaborative Filtering ( CF ) is a group of popular approaches [ 6 , 11 ] . The philosophy behind CF is to recommend items based on preferences of similar users . That is , if a group of users share similar interests , the items preferred by one user can be recommended to others of the group . Since neighborhood formation requires sufficient amounts of training data , CF is sensitive to data sparsity [ 21 , 3 ] . In order to address this issue , Latent Semantic Indexing ( LSI ) was applied to recommender systems and promising results were achieved [ 20 , 21 ] . LSI was based on truncated singular valued decomposition and has also been successfully used in information retrieval ( IR ) community [ 7 ] . In [ 21 ] , the authors use LSI for two recommendation tasks : to predict the likeliness of a product preferred by a customer ; and to generate a list of top N recommendations . LSI was also studied in [ 22 ] for collaborative filtering applications .
Web usage mining techniques have achieved great success in various application areas [ 13 , 14 , 17 ] . As far as we know , there was seldom works on incorporating three way clickthrough data for personalized Web search . An exception is
Figure 1 : Visualization of matrix SVD
[ 3 ] , which extended Hofmann ’s aspect model to incorporate three way co occurrence data for recommendation problem . However , it was not used for Web search application . The technique introduced in [ 14 ] uses clickthrough data in order to improve the quality of Web search . The author uses the relative preferences between Web pages and learns the retrieval functions . In [ 25 ] , the authors also examine the interrelated data objects of clickthrough data and put forward a reinforcement clustering algorithm to cluster these multi type objects .
The higher order singular value decomposition technique was proposed in [ 15 ] . It is a generalization of singular value decomposition and has been successfully applied for computer vision problems in [ 24 ] . We propose to use the HOSVD technique for personalized Web search in this paper .
3 . SVD AND HOSVD
Since our CubeSVD approach is based on HOSVD technique , which is a generalization of matrix SVD , we first briefly review matrix SVD and then introduce tensor and the HOSVD technique . In this paper , tensors are denoted by calligraphic upper case letters ( A,B ··· ) , matrices by uppercase letters ( A , B ··· ) , scalars by lower case letters ( a , b··· ) , vectors by bold lower case letters ( a , b··· ) . 3.1 Matrix SVD I1 × I2 matrix F , it can be written as the product :
The SVD of a matrix is visualized in Figure 1 . For a i
I1
1 u(1 )
1 u(2 )
) and U ( 2 ) = ( u(2 )
F = U ( 1 ) · S · U ( 2 ) 2 ··· u(1 ) ,1 ≤ i ≤ I1 and u(2 )
( 1 ) 2 ··· u(2 ) where U ( 1 ) = ( u(1 ) I2 are the matrices of the left and right singular vectors . The ,1 ≤ j ≤ I2 are column vectors u(1 ) orthogonal . S = diag(σ1 , σ2,··· , σmin(I1,I2 ) ) is the diagonal matrix of singular values which satisfy σ1 ≥ σ2 ≥ ··· ≥ σmin(I1,I2 ) ≥ 0 . By setting the smallest ( min{I1 , I2} − k ) singular values in S to zero , the matrix F is approximated with a rank k matrix and this approximation is best measured in reconstruction error . Theoretical details on matrix SVD can be found in [ 9 ] . 3.2 Tensor and HOSVD j
A tensor is a higher order generalization of a vector ( first order tensor ) and a matrix ( second order tensor ) . Higherorder tensors are also called multidimensional matrices or multi way arrays . The order of a tensor A ∈ RI1×I2×···×IN is N . Elements of A are denoted as ai1···in···iN where 1 ≤ in ≤ In . In tensor terminology , matrix column vectors are referred to as mode 1 vectors and row vectors as mode 2 vectors . The mode n vectors of an N th order tensor A are the In dimensional vectors obtained from A by varying the index in and keeping the other indices fixed , that is the column vectors of n mode matrix unfolding A(n ) ∈
Figure 2 : Visualization of a 3 order Singular Value Decomposition
RIn×(I1I2···In−1In+1···IN ) of tensor A . See [ 15 ] for details on matrix unfoldings of a tensor . The n mode product of a tensor A ∈ RI1×I2×···×IN by a matrix M ∈ RJn×In is an I1 × I2 ×···× In−1 × Jn × In+1 × ··· × IN tensor of which the entries are given by ( A ×n M )i1i2···in−1jnin+1···iN
X ai1i2···in−1inin+1···iN mjnin
= in
( 2 ) Note that the n mode product of a tensor and a matrix is a generalization of the product of two matrices . It can be expressed in terms of matrix unfolding :
B(n ) = M A(n )
( 3 ) where B(n ) is the n mode unfolding of tensor B = A ×n M . In terms of n mode products , the matrix SVD can be rewritten as F = S ×1 V ( 1 ) ×2 V ( 2 ) . By extension , HOSVD is a generalization of matrix SVD : every I1 × I2 × ··· × IN tensor A can be written as the n mode product [ 15 ] :
A = S ×1 V1 ×2 V2 ··· ×N VN
( 4 ) as illustrated in Figure 2 for N = 3 . Vn contains the orthonormal vectors ( called n mode singular vectors ) spanning the column space of the matrix A(n ) ( n mode matrix unfolding of tensor A ) . S is called core tensor . Instead of being pseudodiagonal ( nonzero elements only occur when the indices satisfy i1 = i2 = ··· = iN ) , S has the property of all orthogonality . That is , two subtensors Sin=α and Sin=β are orthogonal for all possible values of n , α and β subject to α 6= β . At the same time , the Frobenius norms i = kSin=ik are n mode singular values of A and are in σn In ≥ 0.1 S is in general decreasing order : σn a full tensor and governs the interactions among Vn .
2 ≥ ··· ≥ σn
1 ≥ σn
4 . CUBESVD BASED WEB SEARCH
)
When using a search engine to find information : a user(u ) submits a query(q ) , the search engine returns a list of URLs and the corresponding descriptions of the target Web pages , then the user clicks on the pages(p ) of interest . After some time of usage , the search engine accumulates a collection of clickthrough data , which can be represented by a set of 1The Frobeninu norm of a tensor A is defined as kAk = is defined as hA,Bi= ai1i2···iN · bi1i2···iN . Sin=i is the subtensor of S obtained by fixing the nth index of S to i . More details are referred to [ 15 ] . phA,Ai . And the scalar product hA,Bi of two tensors A,B
···P
P
P iN i1 i2
fi fi fi 'ff( fi)fi(A=V1SV3V2 Table 1 : Details of the Web Pages Used in the Toy Problem Title
URL http://wwwbmwcom http://wwwaudiusacom
BMW International Website
Audiusa.com Home Page http://wwwjaguarusacom/us/en/homehtm
Jaguar Cars http://dspacedialpipexcom/agarman/bco/ver4htm
Big Cats Online Home
Page p1 p2 p3 p4
1 . Construct tensor A from the clickthrough data . Suppose the numbers of user , query and Web page are m , n , k respectively , then A ∈ Rm×n×k . Each tensor element measures the preference of a huser , queryi pair on a Web page . 2 . Calculate the matrix unfolding Au , Aq and Ap from tensor A . Au is calculated by varying user index of tensor A while keeping query and page index fixed . Aq and Ap are computed in a similar way . Thus Au , Aq , Ap is a matrix of m × nk , n × mk , k × mn respectively . 3 . Compute SVD on Au , Aq and Ap , set Vu , Vq and Vp to be the left matrix of the SVD respectively . 4 . Select m0 ∈ [ 1 , m ] , n0 ∈ [ 1 , n ] and k0 ∈ [ 1 , k ] . Remove the right most m − m0 , n − n0 and k − k0 columns from Vu , Vq and Vp , then denote the reduced left matrix by Wu , Wq and Wp respectively . Calculate the core tensor as follows :
S = A ×1 W T p u ×2 W T q ×3 W T
5 . Reconstruct the original tensor by :
ˆA = S ×1 Vu ×2 Vq ×3 Vp
( 5 )
( 6 )
Figure 3 : Outline of the CubeSVD algorithm . triplets hu , q , pi . From the clickthrough data , we can construct a 3 order tensor A ∈ RU×Q×P , where U ,Q,P are sets of users , queries and pages respectively . Each element of tensor A measures the preference of hu , qi pair on page p . In the simplest case , the co occurrence frequency of u , q and p can be used . In this paper , we also tried several other approaches to measure the preference . After tensor A is constructed , the CubeSVD algorithm can be applied on it . 4.1 CubeSVD Algorithm
Our CubeSVD approach is to apply the HOSVD technique on the 3 order tensor constructed from the clickthrough data . In accordance with the HOSVD technique introduced in Section 3.2 , the CubeSVD algorithm is given in Figure 3 : the input is the clickthrough data , the output is the reconstructed tensor ˆA . ˆA measures the associations among the users , queries and Web pages . The elements of ˆA can be represented by a quadruplet hu , q , p , wi , where w measures the likeliness that user u will visit page p when u submits query q . Therefore , Web pages can be recommended to u according to their weights associated with hu , qi pair . 4.2 A Toy Problem Example
In this subsection , in order to illustrate how our approach works , we apply the CubeSVD algorithm to a toy problem . As illustrated in Figure 4 , 4 users issued 4 different queries ( “ bmw ” , “ audi ” , “ jaguar ” , “ big cat ” ) and clicked on 4 Web pages . In Figure 4 , the arrow line between a user and a query
Figure 4 : Clickthrough data of the toy problem . represents the user issued the corresponding query . The line between a query and a page indicates the user clicked on the page after he/she issued the query . The numbers on the arrow line gives the correspondence between the three types of objects . For example , user u1 issued query “ bmw ” and then clicked on page p1 . The users performed seven clicks on the 4 pages in this toy problem . The URLs and titles of the pages visited are given in Table 1 . Query “ jaguar ” may refer to “ jaguar car ” or “ jaguar cats ” . From Table 1 , we can find that p1 , p2 and p3 are Web pages on “ cars ” , page p4 is related to “ cats ” . From Figure 4 , we can see that user u1 and u2 have common interests on cars , while user u3 and u4 are interested in big cat animals . A 3 order tensor A ( 4×4×4 ) can be constructed from the clickthrough data . For simplicity , we assume there are no duplicate page visits . That is , if a user issues a query and then clicks on a Web page , the user only clicks on the page once . We use the co occurence frequency of user , query and page as the elements of tensor A , which are given in Table 2 . After performing the CubeSVD analysis , we can get the reconstructed tensor ˆA . Table 3 gives the output of the CubeSVD algorithm , as illustrated in Figure 5 . In Table 3 , the rows in italic font represents that this link relation do not exist in the original clickthrough data .
As given in Table 3 and Figure 5 , the output of the CubeSVD algorithm for this toy problem is interesting : new associations among these objects come out . From the original clickthrough data ( Figure 4 ) , we can find that neither user u1 nor u4 issued query q3 . There is also no direct indication on which pages to recommend if either of the two users submits query q3 , because query q3 is ambiguous . According to the algorithm outputs given in Table 3 , the element of ˆA associated with hu1 , q3 , p3i is 0.354 and elements associated with other pages are zero . Thus if u1 issues query q3 , then
fi'ff ( ) fffi(fl (ffi fflfifl(fflff fifi''ffff(() )fi)')ff Table 2 : Tensor Constructed from the Clickthrough Data of the Toy Problem
Arrow Line User Query Page Weight
1 2 3 4 5 6 7 u1 u2 u2 u2 u3 u3 u4 q1 q1 q2 q3 q3 q4 q4 p1 p1 p2 p3 p4 p4 p4
1 1 1 1 1 1 1
Table 3 : Output of CubeSVD Algorithm on the Toy Problem
Arrow Line User Query Page Weight
1 2 3 4 5 6 7 8 9 10 u1 u2 u2 u2 u3 u3 u4 u1 u1 u4 q1 q1 q2 q3 q3 q4 q4 q2 q3 q3 p1 p1 p2 p3 p4 p4 p4 p2 p3 p4
0.5
1.207 0.853 0.853 0.723 1.171 0.723 0.354 0.354 0.447 u1 is likely to visit page p3 ( arrow line 9 ) . Similarly , if user u4 submits query q3 , then u4 is likely to visit p4 ( arrow line 10 ) . The results are reasonable since u1 is concerned about cars rather than big cat animals , while u4 is opposite . Even the two users have not issued query q3 , our algorithm can still recommend Web pages by analyzing the clickthrough data . That is , the CubeSVD approach is able to capture the latent associations among the multi type data objects : user , query and Web page . The associations can then be used to improve the Web search accordingly .
4.3 Dimension Selection
The latent associations among the three types of objects captured by CubeSVD are stored in the reconstructed tensor ˆA . From step 5 of the CubeSVD algorithm in Figure 3 , we know tensor ˆA is constructed by the product of the core tensor S and the left matrix Vu , Vq and Vp and the dimensions of S are selected in step 4 . Since the core tensor S governs the interactions among user , query and Web page objects , the determination of core tensor dimensionality may play an important role in the result of the algorithm . This is further verified by our experiments in Section 5 .
Recall in the two dimensional case [ 7 ] , LSI computes a low rank approximation of the original term by document matrix to capture the semantic concepts of a document set . The resulted matrix is calculated by truncated SVD as Figure 1 indicates . Previous experiments indicate that the number of singular values kept in the diagonal matrix S is crucial for LSI ’s performance [ 12 ] . And how to determine the dimension is still an ongoing research problem .
For the CubeSVD approach , determination of the core tensor ’s dimensions seems more difficult than LSI . Because for LSI , the term by document matrix is two dimensional , thus only one parameter ( the number of nonzero singular values ) needs to be decided . For CubeSVD , there are three dimensional parameters to be determined . According to the CubeSVD algorithm in Figure 3 , the core tensor S is cal0 0 culated from the product of tensor A by W p . q and W Therefore how many columns of Vu , Vq , and Vp are kept determines the dimensions of the core tensor ( m0 × n0 × k0 ) . Since the left matrix Vu , Vq and Vp are calculated by solving SVD problems on the matrix unfolding Au , Aq and Ap respectively , in this paper we use an eigenvalue based method to determine the core tensor dimensions empirically .
0 u , W
According to the tensor decomposition property [ 15 ] : kA − ˆAk ≤
( σu iu )2 +
( σq iq
)2 +
( σp ip
)2 iu=m0+1 iq =n0+1 ip=k0+1 m , σq n,σp m0 ,σq n0 and σp k0+1,··· ,σp n0+1 , ··· ,σq
( 7 ) By discarding the smallest n mode singular values σu m0+1 , ··· , σu k to zero , we obtain an approximation ˆA of the original tensor A . As discussed in [ 15 ] , if σu n0+1 , σk k0+1 respectively , the energy lost is not significant and is bounded as in Equation 7 . Based on this property , we use the eigenvalues in the three matrix unfolding SVD problems , i . e . , the smallest eigenvalues are discarded , thus reducing the dimensionality of the core tensor to λ · ( m × n × k ) . In this paper , λ is tuned empirically . k0 are much bigger than σu m0+1 , σq mX nX kX
4.4 Weighting Policy In our CubeSVD algorithm , the tensor value measures the preference of a huser , queryi pair on a Web page . If the page click frequency is used as tensor value , the algorithm is inclined to biasing towards tensor elements with high frequency . We also try three other weighting approaches : for each huser , queryi pair , if a page is clicked on , then the tensor value associated with the three objects is 1 , otherwise 0 .
1 ) The first is a Boolean model . That is ,
2 ) The second is by re weighting of click frequency . We use a method used in IR community . For each clickthrough data triple hu , q , pi , the weight of the corresponding tensor value is a re weighting of the page click frequency f :
0 f
= log2 ( 1 + f )
( 8 )
Figure 5 : Illustration of the CubeSVD algorithm output for the toy problem given in Figure 4 .
fi'ff ( ) fffi(fl (ffi fflfifl(fflff fifi''ffff(() )ff fffiff'fffffifi The log function is used for scaling the page click frequency in order to reduce the impact of highly frequent visits .
3 ) The third approach is similar with the second one . Here we take into account the Inverse Document Frequency ( IDF ) of a Web page ( that is , frequency of a page visited by different users ) . The intuition is that , if a Web page is visited by most users , then it is not representative for measuring users’ interests :
0 f
= log2 ( 1 + f /f0 )
( 9 )
In Equation 9 , f0 denotes IDF of a Web page .
The above three weighting schemes
( denoted by Weight Boolean , Weight Log Freq , Weight Log Freq IDF respectively ) , as well as the scheme without weighting ( denoted by Weight Freq ) , are all tested in our experiments in Section 5 . 4.5 Smoothing Scheme
In the 2 dimensional case , LSI uses the co occurrence of words and documents to capture the latent semantics of a document set : if two words co occur frequently , they may be semantically related . In the 3 dimensional case , our CubeSVD algorithm is applied on the clickthrough data , which contains the co occurrence of the three types of objects : user , query and Web page . If the link relations among them are scarce , the latent associations may be difficult to capture . Generally , when a user issues a query , she may only visit a very small set of pages of interest , which may lead to a highly sparse tensor . In this work , we employ two smoothing methods to address the sparseness problem and the corresponding results are compared with the one without smoothing .
451 Constant Based Smoothing For pages that a user query pair hu , qi does not visit , the corresponding tensor value is zero . An intuitive and straightforward smoothing method is to replace the zero tensor elements with a small constant c(0 ≤ c ≤ 1 ) . That is , even a page p is not visited by hu , qi according to the clickthrough data , it is assumed that page p is in general visited by u with a small probability if u issues query q .
452 Page Similarity Based Smoothing The second smoothing method is based on content similarities between Web pages . For each user query pair hu , qi , a set of pages S1 are visited . For each page p ∈ S2 ( S2 denotes pages not visited by hu , qi ) , an overall similarity between p and pages S1 can be calculated and used to replace the corresponding tensor elements : sim(p , S1 ) =
1 a∈S1 s(p , a )
, p ∈ S2
( 10 )
|S1|P
In Equation 10 , s(p , a ) measures the similarity between page p and a . Here , each page is represented by a vector of word weight and the similarity between two pages is measured by cosine of the angle between the corresponding vectors : s(p , a ) = j
( 11 )
ć
P
ą wpj · waj ||wp|| · ||wa|| where wpj denotes weight of term j in page p .
The two smoothing techniques , as well as no smoothing , are denoted by Smooth Constant , Smooth Content and Smooth None respectively .
4.6 Normalization
For the 2 dimensional case , when LSI is used for information retrieval , normalization scheme has a high impact on the retrieval precision [ 12 ] . Since the tensor A is of 3 dimensions , it can be normalized from any dimension and the experiment result may be different . In this work , we compared all the three normalization methods . For example , if the tensor is normalized from the user dimension , then for each user u , all the tensor values corresponding with u are devided by a constant and the tensor values sum to 1 after division , that is : X
X aiuiq ip = 1
( 12 )
1≤iq≤n
1≤ip≤k
Normalization from query or Web page dimension is similar . The three normalization methods are denoted by Normalize User , Normalize Query , Normalize Page respectively . More is discussed in Section 542
There is an ordering issue when the techniques discussed in Sections 43 46 are combined with the CubeSVD algorithm . As discussed in Section 4.1 , dimension selection is used in step 4 of the CubeSVD algorithm . Since the weighting , smoothing and normalization techniuqes are used to construct a tensor from the clickthrouth data , they are applied in the first step of CubeSVD . Similar with LSI applied in IR applications , the order of the three kinds of techniques used is : weighting , smoothing and normalization . The weighting technique is first used to assign a value to the tensor elements associated with the hu , q , pi triples which occurred in the clickthrough data . Next , the smoothing techniques are used to replace some empty elements of the tensor . After smoothing is used , normalization is applied in order to regard objects of the same type with equal importance in the tensor construction . For example , if the tensor is normalized from the user dimension , then each user is equally important for tensor construction , even though the number of queries each user issued or the number of pages each user visited may be different . After the weighting , smoothing and normalization techniques are applied , the tensor construction ( step 1 in Figure 3 ) is complete .
5 . EXPERIMENTS
In this section , we introduce the experimental data set , our evaluation metrics , and the experiment results . 5.1 Data Set
A set of MSN clickthrough was collected as our experimental data set . This data set contains about 44.7 million records of 29 days from Dec 6 of 2003 to Jan 3 of 2004 . As we collected the clickthrough data , we crawled all Web pages of the ODP ( http://dmoz.org/ ) directory ( about 1.3 million ) . The clickthrough data was split into two parts : a training and a test set . The former comprises of the first two weeks of data collection . The rest of the data is used for testing . For the training data , unique items with same user , query and Web page are grouped into one entry and the frequency is summed up . And we remove the Web pages which occurred in the clickthrough data but not crawled by our crawler . After this processing step , the training data contains 19,644,518 entries having 3,676,296 users , 248,149 pages and 996,090 queries . That is , among the 1.3 million ODP Web pages , 248,149 of them are clicked by Web users in
Figure 6 : Performance of CubeSVD as the dimensions of the core tensor vary . For the leftmost figure , the user dimension is fixed at 115 and the other two dimensions change . For the middle figure , the query dimension is fixed at 144 . For the rightmost figure , the page dimension is fixed at 112 . the first 2 weeks . Each user is identified by their IP address . This is not appropriate sometimes when multi users share one IP address or user accesses Web by dynamic IPs . In other words , the Web search may be conducted by a group of users . From the training dataset , we randomly select 500 users’ clickthrough data and apply our CubeSVD algorithm on it . The noise is reduced by removing the Web pages which was visited by no more than 3 times and users who visited no more than 3 pages . Then we use these users’ clickthrough data from the test set to evaluate the search performance . In this work , we do not handle the new queries and new Web pages contained in the test set . The SVDPACKC/las1 software package is used for SVD computation[5 ] . 5.2 Baseline Algorithms
For comparison purpose , we also investigate whether the 3 order associations can be captured by the 2 dimensional SVD approaches . We apply LSI on the huser , queryi bypage matrix and use the reduced rank approximation of the original matrix for Web page prediction [ 22 ] . Besides , we also use the Collaborative Filtering algorithm in the experiments . For CF , we apply the memory based algorithm with the vector similarity measure to form neighbors ( Refer to Equation ( 1 ) and ( 3 ) in [ 6] ) . 5.3 Evaluation Measurements
We evaluate the Web search accuracy of different algorithms using rank scoring metric[6 ] . The expected utility of a ranked list of items is defined as
Rs =
δ(s , j )
2(j−1)/(α−1 )
( 13 )
X j where j is the rank of a Web page in the list recommended , δ(s , j ) is 1 if a huser , queryi pair s accessed page j in the test set and 0 otherwise , and α is set to 5 as the author did . The final score reflects the utilities of all huser , queryi pairs in the test set :
R = 100
( 14 )
P s RsP s RM ax s s where RM ax is the maximum possible utility obtained when all pages that each huser , queryi pair has accessed appear at the top of the ranked list .
5.4 Experimental Results
We implemented all the 4 weighting methods , 3 smoothing schemes and 3 normalization methods discussed in Section 4 , which lead to 36 different settings . In this work , we evaluated CubeSVD with all the settings . We also compare CubeSVD with CF and LSI in our experiments .
541
Influence of the Core Tensor Dimensions
We first conduct experiments to study the influence of core tensor dimensions on the performance of our CubeSVD algorithm . When we apply CubeSVD to tensors constructed with different weighting , smoothing and normalization methods , all the results show the search accuracy has high dependency on dimensions of the core tensor . For example , when we use Boolean weighting , normalization from query dimension without smoothing , we get a 500× 168× 182(u× q × p ) tensor . Dimensions associated with the three matrix unfoldings are 235 , 157 and 182 respectively after SVD is performed . The CubeSVD algorithm achieves optimal accuracy ( utility is 69.62 ) when the core tensor dimension is 115 , 144 and 112 respectively . If one dimension of the core tensor is fixed , we can find the search accuracy varies as the other two dimensions change , as illustrated in Figure 6 : the vertical axis denotes the utility measure and the other two axes denote the corresponding dimensions . For each figure , one dimension is fixed and the other two dimensions are varied . Each dimension increases in step ( 0.1 × the corresponding highest dimension ) and is measured with fraction .
We also employed our eigenvalue based method to determine dimensions of the core tensor . The parameter λ is varied from 0.1 to 1 in step 01 For this experiment , when λ = 0.9 , we get a 211× 141× 163 dimension core tensor and the utility achieved is 68.6 , which is approximate with the optimal result ( utility 6962 )
542
Influence of Weighting , Smoothing and Normalization Methods
According to our experiment results , we find normalization from query dimension is slightly better than normalization from user or page dimension . Even when different weighting or smoothing techniques are used , this conclusion is consistent . We give a group of experiment results in Figure 7 , these results correspond with normalization from query dimension . Different weighting and smoothing methods are used in this experiment . We can find that the weight
0020406081002040608140455055606570PQUtility00204060810020406081455055606570PUUtility0020406081002040608140455055606570QUUtility4550556065 Figure 7 : Search Results of CubeSVD algorithm normalized from query dimension , associated with different weighting policies and smoothing schemes . ing policy may influence the search results , especially when the log frequency weighting method is used . The Boolean model performs worst compared with the other three weighting methods . Out of our expectation , the Weight Log Freq IDF weighting method is not so good as Weight Log Freq method , sometime even worse than without weighting scheme ( Weight Freq ) . From Figure 7 , we can also find that smoothing can improve the search accuracy . Even the constant based smoothing method ( c = 0.05 in this experiment ) outperforms the one without smoothing . The page similarity based smoothing approach is better than constant based smoothing .
543 Comparison with Other Approaches
We also conduct experiments to compare CubeSVD with LSI and CF . In all the settings , CubeSVD outperforms both LSI and CF . Figure 8 describes the results of the three algorithms with page similarity based smoothing and normalization from query dimension . Results associated with the 4 weighting methods are plotted . For LSI , the reduced dimension varies from 1 to the highest possible dimension ( the matrix rank ) and the best result is reported . For CF , we vary the number of neighbors and report the best result . According to the results , we can find CubeSVD outperforms either of the two baseline algorithms significantly .
544 Discussions
From the experiments , we observe that CubeSVD achieves better search accuracy than CF and LSI . The reason is CubeSVD can exploit the clickthrough data to capture the latent associations among the multi type objects . And this kind of high order associations can not be well captured by CF or LSI applied on the 2 dimensional matrix data .
We can also find that the core tensor dimensionality is crucial to the performance of CubeSVD . Different weighting , smoothing and normalization methods also have impacts on the search accuracy . According to the experimental results , the Weight Log Freq approach is the best weighting method . When Inverse Document Frequency is used , the search result does not improve . In our opinion , the reason is : there do not exist so many pages which are frequently visited by users with different interests . Therefore , when IDF is used for weighting , the search accuracy even decreases . Smoothing techniques can improve the search result . Since
Figure 8 : Search Results of CF , LSI and CubeSVD . the page content information is used , the page similarity based smoothing is better than constant based smoothing . The effect of similarity based smoothing for sparse data is also observed in [ 3 ] .
By analyzing the CubeSVD algorithm illustrated in Figure 3 , we can find that most time is consumed by steps 3 5 . In step 3 , SVD is performed on the three unfolded matrices . If the tensor scale is large , this step is quite time consuming . Especially if smoothing is used , the original sparse tensor becomes relatively dense and the scale of the SVD problem increases . If no smoothing is used , there are many zero columns in the unfolded matrices which decrease the scale of the SVD problem . Even though the large scale CubeSVD algorithm is quite time consuming , the computation can be performed offline beforehand . After the CubeSVD analysis , the results can be used to help search Web pages in real time . Because the preferences of each huser , queryi pair on Web pages have been computed in advance . Thus the search results can be adapted to users according to the associations among Web pages , users and queries submitted .
6 . CONCLUSION AND FUTURE WORK
Personalized Web search service will play an important role on the Web . This paper focuses on utilizing clickthrough data to improve Web search . A novel CubeSVD approach is proposed to deal with the clickthrough data which is three way and highly sparse . We used a real world data set to evaluate the CubeSVD algorithm combined with a variety of techniques , examining the impact of different weighing , smoothing and normalization methods . The experimental results indicate that CubeSVD approach can significantly improve Web search performance .
There are also many areas for future research : 1 ) In our current work , we are concerned about the users whose clickthrough data was recorded . And only queries issued and pages clicked on by these users are considered . Therefore , it would be interesting to adapt our framework to newly emerged objects ( new users , queries and Web pages ) . One possible approach is by combining the CubeSVD technique with traditional content based search model .
2 ) The offline computation of CubeSVD is quite time
fifi '' ff(()fffi(flffi(()ffffl(fl)fl)(()ffffl(fl)ffifl)ffi)ff((ffiflffi)ffffiffi)fffl(ffffiffi)fffl(ffffiff fi'ff()fffffiflffifflflffflffflflfl consuming , especially when the clickthrough data contains a large number of objects . With CubeSVD as a base approach , we will seek ways to improve its efficiency .
3 ) We also plan to conduct more research on how to automatically determine the optimal dimensionality of the core tensor .
4 ) The CubeSVD framework proposed in this paper is not limited to Web search but is general enough and can be applied to other applications where three way relations exist .
7 . ACKNOWLEDGMENTS
We thank Xue Mei Jiang and Ya Bin Kang for their help in preparing the data used in this work . We also express thanks to Xuan Hui Wang for his comments on this paper and helpful discussions .
8 . REFERENCES [ 1 ] Google personalized search . http://labsgooglecom/personalized
[ 2 ] My yahoo! http://myyahoocom/?myhome [ 3 ] P . Alexandrin , U . Lyle , P . David , and L . Steve .
Probabilistic models for unified collaborative and content based recommendation in sparse data environments . In Proceedings of the 17th Annual Conference on Uncertainty in Artificial Intelligence ( UAI 01 ) , pages 437–444 , San Francisco , CA , 2001 . Morgan Kaufmann Publishers .
[ 4 ] R . B . Almeida and V . A . F . Almeida . A community aware search engine . In Proceedings of the 13th international conference on World Wide Web , pages 413–421 . ACM Press , 2004 .
[ 5 ] M . Berry , T . Do , and S . Varadhan . Svdpackc ( version
1.0 ) user ’s guide . Technical Report CS 93 194 , University of Tennessee , 1993 .
[ 6 ] J . S . Breese , D . Heckerman , and C . Kadie . Empirical analysis of predictive algorithms for collaborative filtering . In Proceedings of the Fourteenth Annual Conference on Uncertainty in Artificial Intelligence , pages 43–52 . Morgan Kaufman , 1998 .
[ 7 ] S . C . Deerwester , S . T . Dumais , T . K . Landauer , G . W . Furnas , and R . A . Harshman . Indexing by latent semantic analysis . Journal of the American Society of Information Science , 41(6):391–407 , 1990 .
[ 8 ] L . Fitzpatrick and M . Dent . Automatic feedback using past queries : social searching ? In Proceedings of the 20th annual international ACM SIGIR conference on Research and development in information retrieval , pages 306–313 . ACM Press , 1997 .
[ 9 ] G . Golub and C . V . Loan . Matrix Computations , 2nd edition . The Johns Hopkins University Press , Baltimore , Ma , 1989 .
[ 10 ] T . H . Haveliwala . Topic sensitive pagerank . In
Proceedings of the eleventh international conference on World Wide Web , pages 517–526 . ACM Press , 2002 .
[ 11 ] J . L . Herlocker , J . A . Konstan , A . Borchers , and
J . Riedl . An algorithmic framework for performing collaborative filtering . In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval , pages 230–237 . ACM Press , 1999 .
[ 12 ] P . Husbands , H . Simon , and C . H . Q . Ding . On the use of the singular value decomposition for text retrieval . Computational information retrieval , pages 145–156 , 2001 .
[ 13 ] X . Jin , Y . Zhou , and B . Mobasher . Web usage mining based on probabilistic latent semantic analysis . In Proceedings of the 2004 ACM SIGKDD international conference on Knowledge discovery and data mining , pages 197–205 . ACM Press , 2004 .
[ 14 ] T . Joachims . Optimizing search engines using clickthrough data . In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , pages 133–142 . ACM Press , 2002 .
[ 15 ] L . D . Lathauwer , B . D . Moor , and J . Vandewalle . A multilinear singular value decomposition . SIAM Journal on Matrix Analysis and Applications , 21(4):1253–1278 , 2000 .
[ 16 ] F . Liu , C . Yu , and W . Meng . Personalized web search by mapping user queries to categories . In Proceedings of the eleventh international conference on Information and knowledge management , pages 558–565 . ACM Press , 2002 .
[ 17 ] B . Mobasher , H . Dai , M . Nakagawa , and T . Luo .
Discovery and evaluation of aggregate usage profiles for web personalization . Data Mining and Knowledge Discovery , 6(1):61–82 , 2002 .
[ 18 ] L . Page , S . Brin , R . Motwani , and T . Winograd . The pagerank citation ranking : Bringing order to the web . Technical report , Stanford Digital Library Technologies Project , 1998 .
[ 19 ] J . Pitkow , H . Schutze , T . Cass , R . Cooley ,
D . Turnbull , A . Edmonds , E . Adar , and T . Breuel . Personalized search . Communications of the ACM , 45(9):50–55 , 2002 .
[ 20 ] M . H . Pryor . The effects of singular value decomposition on collaborative filtering . Technical Report PCS TR98 338 , Dartmouth College , Computer Science , Hanover , NH , June 1998 .
[ 21 ] B . Sarwar , G . Karypis , J . Konstan , and J . Riedl .
Application of dimensionality reduction in recommender systems a case study , 2000 .
[ 22 ] N . Srebro and T . Jaakkola . Weighted low rank approximations . In Proceedings of the 12th International Conference on Machine Learning , pages 720–727 . AAAI Press , 2003 .
[ 23 ] K . Sugiyama , K . Hatano , and M . Yoshikawa . Adaptive web search based on user profile constructed without any effort from users . In Proceedings of the 13th international conference on World Wide Web , pages 675–684 . ACM Press , 2004 .
[ 24 ] M . A . O . Vasilescu and D . Terzopoulos . Multilinear image analysis for facial recognition . In ICPR , pages 511–514 , 2002 .
[ 25 ] J . Wang , H . Zeng , Z . Chen , H . Lu , L . Tao , and W Y
Ma . Recom : reinforcement clustering of multi type interrelated data objects . In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval , pages 274–281 . ACM Press , 2003 .
