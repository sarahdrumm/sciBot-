A Combinatorial Approach to Nearest Neighbor Search
Disorder Inequality :
Navin Goyal fi
College of Computing
Georgia Tech
Atlanta , GA , USA navingoyal@gatechedu
Yury Lifshits y
California Institute of
Technology
Pasadena , CA , USA yury@caltech.edu
Hinrich Sch(cid:252)tze University of Stuttgart Stuttgart , Germany hinrich@hotmail.com
ABSTRACT We say that an algorithm for nearest neighbor search is combinatorial if only direct comparisons between two pairwise similarity values are allowed . Combinatorial algorithms for nearest neighbor search have two important advantages : ( 1 ) they do not map similarity values to artificial distance values and do not use triangle inequality for the latter , and ( 2 ) they work for arbitrarily complicated data representations and similarity functions .
In this paper we introduce a special property of the similarity function on a set S that leads to efficient combinatorial algorithms for S . Disorder constant D(S ) of a set S is defined to ensure the following inequality : if x is the a’th most similar object to z and y is the b’th most similar object to z , then x is among the D(S ) ( a + b)’th most similar objects to y .
Assuming that disorder is small we present the first two known combinatorial algorithms for nearest neighbors whose query time has logarithmic dependence on the size of S . The first one , called Ranwalk , is a randomized zero error algorithm that always returns the exact nearest neighbor . It uses space quadratic in the input size in preprocessing , but is very efficient in query processing . The second algorithm , called Arwalk , uses near linear space . It uses random choices in preprocessing , but the query processing is essentially deterministic . For every fixed query , there is only a small probability that the chosen data structure does not support it .
Finally , we show that for Reuters corpus average disorder is , indeed , quite small and that Ranwalk efficiently computes the nearest neighbor in most cases . fiPart of this work was performed when author was at McGill University ySupported by the Lee Center of Advanced Networking and the Center for the Mathematics of Information . Part of this work was performed when author was at Steklov Institute of Mathematics at StPetersburg
Categories and Subject Descriptors E.1 [ Data Structures ] ; F22 [ Analysis of Algorithms and Problem Complexity ] : Nonnumerical Algorithms and Problems : computations on discrete structures , sorting and searching ; H24 [ Database Management ] : Systems : query processing ; H33 [ Information Storage and Retrieval ] : Information Search and Retrieval : retrieval models , search process ; H31 [ Information Storage and Retrieval ] : Content Analysis and Indexing : indexing methods ; I31 [ Pattern Recognition ] : Clustering : similarity measures
General Terms Algorithms , Theory , Experimentation
Keywords Nearest neighbor search , similarity search , proximity search , random walk , randomized algorithms , disorder inequality , disorder dimension , disorder constant
1 .
INTRODUCTION
Challenge .
Nearest neighbor search ( also called similarity search ) is the problem of preprocessing a set S of n objects ( which we henceforth refer to as points ) lying in some space U with a similarity function so that given a query q 2 U , one can efficiently locate the point that is most similar to q among the points in S . There are several surveys on nearest neighbor search ( NNS ) ; eg , [ 10 , 9 , 20 , 23 ] . This intensively studied problem has numerous web related applications besides playing a central role in many other important areas . These applications include : ffl Near duplicate detection [ 4 , 8 , 16 , 22 ] . ffl Classification/filtering tasks like detecting spam pages [ 41 ] , spelling correction [ 12 ] , sense disambiguation[40 ] . ffl Recommendation systems [ 38 ] . ffl Computing co occurrence and co citation similarity
[ 7 , 13 ] . ffl Discovering related tags in folksonomies [ 14 , 35 ] . ffl Social network analysis ( eg suggesting new friends )
[ 35 ] . ffl News aggregation ( searching for news articles that are most similar to the user ’s profile of interests ) [ 29 , 35 ] . ffl Advertisement targeting ( searching for the most relevant website for displaying a given advertisement ) [ 36 ] .
Applying classical similarity search solutions for these tasks is not straightforward for several reasons . First , data models are heterogeneous and far from simple abstractions like Euclidean space . Combination of text data and link structure [ 3 ] is now a norm . For instance , consider a description of a blog : language , geographic location ( dictionary parameters ) , age , number of posts ( numerical parameters ) , referring links and reader list ( graph parameters ) , text of profile and posts ( text parameters ) and posting timestamps ( time parameters ) . Such a heterogeneous description leads to quite complex similarity functions . Eg , it can include manually defined logical rules and threshold functions . Finally , definition of similarity usually contains various \customization" parameters which can be adjusted by domain experts .
Second , many applications are focused on similarity , not distance . For any given similarity values it is possible to define a metric that provide the same closeness ordering . However , this artificial metric will have distances close to maximal possible ( as is often the case in high dimensional spaces ) . In this setting triangle inequality becomes noninformative and metric algorithms will have no opportunity for search pruning based on it .
Third , the number of description parameters is overwhelming and collections we face in practice are very far from \random [ families ] of random sets" [ 21 ] . We cannot expect to find efficient NNS algorithms for general sets with arbitrary similarity functions . Hence , instead of addressing the problem in general we attempt to find additional properties that hold for real data and support efficient and provably correct NNS algorithms .
Combinatorial approach .
Complicated data models and the absence of the triangle inequality for similarity values leave very few tools for the NNS problem . To our knowledge , there are no NNSalgorithms that work directly with similarity values without first modelling the problem in a metric space . In the present paper , we propose a framework for designing algorithms that do not translate the problem to metric spaces .
Besides working with the similarity function directly , we introduce a restriction on the way the similarity function can be used . We call a search algorithm combinatorial if only direct comparisons between two pairwise similarity values are allowed . As we will see , even working with this restriction we can design good algorithms in certain situations . The restriction offers some advantages : Since we make minimal assumptions about the similarity function , the algorithms in our framework have wide applicability . Moreover , the algorithms are quite robust to certain changes in the similarity function . Eg adding a constant to all similarity values does not affect such an algorithm at all . Also , any transformation of the data representations that preserves the order of similarity values between the points , does not change anything in the output of the algorithm .
As mentioned before , we cannot expect efficient NNSalgorithms for general databases with arbitrary similarity functions . We introduce a natural property of databases which allows us to design efficient NNS algorithms and is essentially satisfied for a real database , as described in our experimental results .
Consider a set S of n points . For every point x we sort all other points by their similarity to x . Overall , we have n sorted lists of n , 1 points each . This is the only information actually to be used in the preprocessing stage . We denote the position of point x in y ’s list by ranky(x ) .
In this paper we consider a special class of datasets . Namely , we assume that for every triple of points x ; y ; z , if x has small rank with respect to z , and y has small rank with respect to z then x also has quite small rank with respect to y . This is our replacement for the triangle inequality . Actually , we relax it by a multiplicative factor D and call it disorder inequality : ranky(x ) D ( rankz(x ) + rankz(y) ) :
We note that disorder constant D is connected to the concept of \intrinsic dimension" for metric spaces . More precisely , we can use log2 D + 1 formula as the disorder dimension of any set in some similarity space . In the classic case of uniformly distributed sets in Euclidean space of some fixed dimension disorder dimension coincides with actual one up to some close to one multiplicative factor .
It is natural to conjecture that if the disorder constant ( and hence , the disorder dimension ) is small , then the nearest neighbors problem has provably more efficient solutions than brute force . Thus our plan is ( 1 ) to verify the \small disorder assumption" on some real data set and ( 2 ) to construct a search algorithm utilizing this assumption .
Results .
Our first contribution is the introduction of the concepts of combinatorial algorithm , disorder inequality and disorder dimension . To our knowledge this is the first time that a framework for comparison based similarity search has been explicitly introduced .
Next , we present two new randomized algorithms for exact nearest neighbor search : Ranwalk and Arwalk . They are the first ones known to be purely combinatorial in the sense defined above . They bear some resemblance to the greedy search algorithms for small world networks ( see , eg , [ 27] ) . Ranwalk performs a random walk in the search phase . It requires O(n2 ) preprocessing space , O(n2 log n ) preprocessing time and uses expected O(D log n log log n+D3 ) time for answering a query . It always produces the correct answer .
The Arwalk algorithm ( walk via navigation array ) requires O(nD log n log log n ) preprocessing space , O(n2 log n ) preprocessing time and uses O(D log n(log log n+log 1=ffi)+D3 ) time for answering a query . For every query it produces the correct answer with probability at least 1 , ffi . The underlying data structure , called navigation array , is a n.D0 .log n table of pointers to points in the database S . Informally , for every point x 2 S and every k log2 n we keep pointers to D0 = D log log n random points in the n=2k neighborhood of x .
The analysis of Arwalk shows that similarity search is tractable ( near linear preprocessing space , near logarithmic query time ) when the disorder dimension log D+1 is at most log log n . Thus , we have similar results to [ 10 , 11 , 30 , 31 , 26 ] where the tractability was shown under the log log n bound for other definitions of intrinsic dimension .
On the experimental side , we compute the disorder dimension of Reuters corpus [ 34 ] . The results show that ( 1 ) on average the disorder inequality requires a fairly small constant D , ( 2 ) even for triangles with large rank values on the sides , the average D is still small , but ( 3 ) in the worst case for some triangles , the disorder fraction is large .
Outline .
The next section is devoted to our concept of disorder . Then we present our algorithms : Ranwalk and Arwalk . The experiments on the Reuters corpus are presented in Section 4 . Finally , we explain the relation of our results to previous research and present six directions for further research .
2 . COMBINATORIAL FRAMEWORK AND
DISORDER INEQUALITY
As mentioned earlier , one of the main contribution of the present work is the combinatorial framework for the nearest neighbor problem . To describe this framework we need some notation .
Definition 1 . ( similarity space ) A similarity space is a tuple ( U ; ) , where U is a finite universe of points , and U is equipped with a similarity function : U . U ! R .
Intuitively , for p ; q 2 U , p and q are more similar to each other if ( p ; q ) is large , than when ( p ; q ) is small . Thus similarity has the opposite interpretation of distance in a metric space . Note that we do not require the similarity function to be symmetric .
The nearest neighbor problem is the following : given a database S U , we would like to preprocess it so that for any query q 2 U we are able to find a point in S that is closest to q ; that is to say , we would like to find p 2 S such that ( q ; p ) = minf(q ; r ) j r 2 Sg . Note that the similarity function need not be symmetric .
In our combinatorial framework , we propose to do away with the numerical value of the similarity function . Instead , we distill the information down to the relative values of the similarity function . To make this precise we introduce the notion of rank .
One advantage of our framework is that any transformation of the set , or change in similarity function , that does not affect the rank function also does not affect the output of the nearest neighbor search . For example , if one adds a fixed number to all similarity values , an algorithm in our framework will work precisely in the same way . Note that classical approximate algorithms for nearest neighbors do not have this property .
Definition 2 . ( rank function ) For points x ; y and set S define rankx;S(y ) to be the position of y when the elements of S [ fx ; yg are ordered according to decreasing similarity to x . When the set S is clear from the context we abbreviate rankx;S(y ) to rankx(y ) .
Let ’s restate the nearest neighbor problem . In a similarity space ( U ; ) , given a database S U , preprocess S so that given a query q 2 U , we can quickly find the nearest neighbor of q in S , that is p 2 S such that rankq;S(p ) = 1 .
In our combinatorial framework , the only operations an algorithm can do are to compute the similarity values and compare them with each other ; in the preprocessing step , the algorithm can store the outcomes of these operations . We cannot expect to find efficient nearest neighbor algorithms for general similarity spaces , and thus we must seek some properties that are likely to be satisfied in real databases , and which also admit efficient nearest neighbor algorithms . We identify one such property next .
But before we do that , let ’s consider an example to motivate the definition . Suppose that set S is an ( infinite ) set of equally distant points on a line . Let us define the similarity function between two points to be the reciprocal of the Euclidean distance between them . Then it is easy to verify that for any triple x ; y ; z 2 S we have ranky(x ) rankz(x ) + rankz(y ) + 3 :
Similarly we have three more inequalities , if we replace rankz(x ) by rankx(z ) and/or rankz(y ) by ranky(z ) in the above inequality .
Of course , for a general set of points the above inequalities will not be satisfied . Disorder constant D(S ) of a set of points is a measure of how far a given set of points S is from a total order :
Definition 3 . ( disorder constant D ) Let S be a set equipped with corresponding similarity function . Define D(S ) , the disorder constant of S , to be the smallest number D such that for all triples x ; y ; z 2 S we have the following disorder inequality : ranky(x ) D(rankz(x ) + rankz(y) ) :
( 1 )
We often abbreviate D(S ) to D , when S is clear from the context . Other variants of this definition may also be useful ; eg , we may also require three \brothers" of the above inequality to be satisfied : ranky(x ) D(rankx(z ) + rankz(y) ) ; ranky(x ) D(rankx(z ) + ranky(z) ) ; ranky(x ) D(rankz(x ) + ranky(z) ) :
It follows from inequality ( 1 ) that for all x ; y we have rankx(y ) D ranky(x ) :
( 2 )
( 3 ) ( 4 )
( 5 )
There is a natural notion of similarity attached to a metric space : we can think of the similarity between two points as reciprocal or negative of the distance between them . In either case , we get the same similarity ordering . As it turns out , sometimes a notion of dimension defined for the metric space is closely related to the disorder constant for its associated similarity order . We now show that for grids log2 D + 1 is essentially the dimension of the grid .
Lemma 1 . For the d dimensional integer grid Zd with Euclidean distance of sufficiently small step size , its disorder constant is 2d,1 up to a multiplicative constant close to one . Proof . Let us fix some step of Euclidean lattice and radius r0 . Let c be a constant such that the number of points in an inner space of ball of radius r > r0 centered at a lattice point is lower upper bounded by crd . Let c0 be a constant such that the number of points in a ball with surface of radius r is upper bounded by c0rd . Actually , for any " > 0 there is sufficiently small step of lattice and sufficiently large r0 such that c0=c 1 + " .
For points x ; y 2 Zd , denote by jx ; yj , the Euclidean dis tance between x and y . For x ; y 2 Zd , we have c(jx ; yj)d rankx(y ) c0(jx ; yj)d :
This gives c jz ; xj rankz(x ) jz ; yj rankz(y ) jy ; xj ranky(x ) c0 c
1=d 1=d 1=d
;
;
:
Ranwalk algorithm
Preprocessing : ffl For every point x in database we sort all other points by their similarity to x . Thus our data structure consists of n lists of n,1 points each .
Now , the triangle inequality jy ; xj jz ; xj + jz ; yj , along with the above inequality yields
Query processing : ranky(x ) c0
1=d rankz(x ) c
1=d
+ rankz(y ) c
1=d
:
Raising both sides to dth power gives ranky(x ) c0 c rankz(x)1=d + rankz(y)1=dd c0 c
2d,1(rankx(y ) + ranky(z) ) :
Similarly , we can show that other inequalities in Def . 3 are satisfied . This shows that for Zd we can take the disorder constant to be close to 2d,1 ( because c0 c 1 ) .
In Section 5 we mention further connections with metric spaces . The above lemma motivates the following definition .
Definition 4 . ( disorder dimension ) Let ( S ; ) be a set in similarity space and D(S ) be the disorder constant of S . Then we call 1 + log D disorder dimension of S .
3 . NEAREST NEIGHBOR SEARCH
IN SETS WITH SMALL DISORDER
In this section we present two related randomized algo rithms for nearest neighbor search in similarity spaces ( databases equipped with a similarity order ) with disorder constant D . Both algorithms , called Ranwalk and Arwalk , use a walk through the database . Ranwalk always returns a correct answer but uses quadratic preprocessing space . Arwalk substantially reduces preprocessing space at the cost of allowing a small probability of error . Throughout the paper we assume that computing similarity value and comparing any two of them have a unit cost . 3.1 Ranwalk algorithm
Now we describe our random walk algorithm . The high level idea of the algorithm is to start at an arbitrary point p 2 S , and then search its appropriate neighborhood for points that are more similar to the query point q , move to the best such point and repeat the process . The neighborhood in which we search can have a large number of points making it difficult to find points more similar to q . We resolve this difficulty by taking a small random sample of the neighborhood and choosing the point in the sample most similar to q .
We now formally present the algorithm , and then prove its correctness and analyze its performance . Let us set
D0 := 3D(log log n + 1 ) :
1 . Step 0 : choose a random point p0 in the database .
2 . From k = 1 to k = log n do Step k : Choose D0 random points from min(n ; 3Dn 2k ) neighborhood of pk,1 . Compute similarities of these points wrt q and set pk to be the most similar one .
3 . If rankplog n ( q ) > D go to step 0 , otherwise search the whole D3 + D neighborhood of plog n and return the point most similar to q as the final answer .
Theorem 1 . Assume that database points together with query point S [ fqg satisfy disorder inequality with constant D : rankx(y ) D(rankz(x ) + rankz(y) ) :
Then Ranwalk algorithm always answers nearest neighbor queries correctly . It uses the following resources :
Preprocessing space : O(n2 ) . Preprocessing time : O(n2 log n ) . Expected query time : O(D log n log log n + D3 ) .
Proof . Indeed , claimed time and space constraints for preprocessing are satisfied .
We call a single execution of steps 0 to log n a trip . For every k log n we say that the first k steps are successful if rankq(pk ) n 2k . Now we estimate the probability of success for the first k steps under the assumption that the first k , 1 steps are , indeed , successful .
During the query processing the following lemma holds .
Lemma 2 . Assume rankq(pk,1 ) n
2k,1 . Then with probability 1,1=2 log n the next inequality also holds : rankq(pk ) n 2k
Proof . There are exactly n
2k points satisfying inequality 2k : For any of such points x disorder inequality rankq(x ) n implies rankpk,1 ( x ) D( n 2k,1 + n 2k ) =
3Dn 2k :
Thus , in 3Dn range from pk,1 the fraction of good points 2k equals to 1=3D . Since we use D0 = 3D(log log n+1 ) random pointers of level k , there is at most
( 1 ,
1 3D
)3D(log log n+1 ) log log n+1
1 2
= 1=2 log n chance that we miss all good points .
The above lemma states an upper bound for probability of not reducing the rank with respect to q by a factor of half in a single step . Hence , summing up error probabilities for all log n steps of a single trip , we have probability at least 1=2 for trip success : rankq(plog n ) is at most 2log n = 1 . By disorder inequality this implies that rankplog n ( q ) D . Therefore , a single trip has probability at least 1=2 to achieve stopping condition . And hence , expected number of trips is at most 2 . n
Now let us consider the final \refinement procedure" . Let rankp(q ) D . We can reverse the rank having rankq(p ) D2 . Let p0 be the true nearest neighbor for q , that is rankq(p0 ) = 1 . Applying disorder inequality to triple p ; p0 ; q ; we get rankp(p0 ) D(D2 + 1 ) . Thus exhaustive search of D3 + D neighborhood guarantees the correct answer . This completes the analysis of Ranwalk . 2 3.2 Arwalk algorithm
We now present our second algorithm called Arwalk ( walk via navigation array ) . It achieves better preprocessing space complexity but sometimes makes mistakes ( ie it is not a zero error algorithm ) . However , error probability ffi can be arbitrarily decreased under reasonable increase of used resources .
The new algorithm is based on the first one , and is obtained by making the random choices of our basic algorithm in advance . Let us set
D0 = 3D(log log n + log 1=ffi ) :
Our data structure consists of D0 pointers for every of log n levels for every of n points . We give navigation array name to this n . log n . D0 table .
Arwalk algorithm
Preprocessing : ffl For every point x in database we sort all other points by their similarity to x . For every level number k from 1 to log n we store pointers to D0 random points within min(n ; 3Dn
2k ) most similar to x points .
Query processing :
1 . Step 0 : choose a random point p0 in the database .
2 . From k = 1 to k = log n do Step k : go by pk,1 pointers of level k . Compute similarities of these D0 points to q and set pk to be the most similar one .
3 . Return plog n .
Theorem 2 . Assume that database points together with query point S [ fqg satisfy disorder inequality with constant D : rankx(y ) D(rankz(x ) + rankz(y) ) :
Then for any probability of error ffi Arwalk algorithm answers nearest neighbor query within the following constraints : Preprocessing space : O(nD log n(log log n + log 1=ffi) ) . Preprocessing time : O(n2 log n ) . Query time : O(D log n(log log n + log 1=ffi) ) .
Proof . Computing pointers for every point require
O(n log n ) + D0 log n time . Doing it sequentially we can manage to use only space proportional to output , that is O(nD0 log n ) . Thus , claimed time and space constraints for preprocessing are satisfied .
During the query processing the following lemma is satis fied .
Lemma 3 . Assume rankq(pk,1 ) n
2k,1 . Then with probability 1,ffi= log n the next inequality also holds : rankq(pk ) n 2k
Proof . There are exactly n
2k points satisfying inequality 2k : For any of such points x disorder inequality rankq(x ) n implies rankpk,1 ( x ) D( n 2k,1 + n 2k ) =
3Dn 2k :
Thus , in 3Dn range from pk,1 the fraction of good points 2k equals to 1=3D . Since we use D0 = 3D(log log n + log 1=ffi ) random pointers of level k , there is at most ffi= log n chance that we miss all good points .
Summing up error probabilities for all levels , we have at 2log n = most ffi probability that rankq(plog n ) is larger than 1 . This completes the analysis of Arwalk . 2 n
Remark . By additional cost of D + D3 we can make Arwalk to be one side error algorithm . That is , we can do the same \stopping condition" and \refinement procedure" as in Ranwalk . If stopping condition is satisfied we guarantee the correctness of answer . If it failed we return \Sorry , it was a bad luck . Here is the answer , but it does not seems to be a true nearest neighbor" .
Discussion .
Let us summarize important properties of Ranwalk and
Arwalk algorithms : ffl Both are exact algorithm . That is , the objective of algorithm is to output the nearest neighbor , not the second or the third one . ffl Ranwalk has deterministic preprocessing and randomized query processing . Arwalk has randomized preprocessing and deterministic query processing . The latter is a version of the random walk algorithm where all random choices are done in advance . ffl For Arwalk , probability of error is taken entirely from random choices of navigation array . Unlike many works ( eg , [ 21] ) , we do not assume any particular distribution on query and/or database . ffl Decreasing probability of Arwalk ’s error has logarithmic cost . Eg to obtain probability error equal to 1=n we have to increase space and time by a factor of log n .
4 . EXPERIMENTAL EVALUATION
In this section , we evaluate the concept of disorder dimension and the Ranwalk algorithm experimentally . For evaluation , we use the problem of nearest neighbor search in the Reuters RCV1 corpus [ 34 ] . Text classification is an important application of nearest neighbor search and the RCV1 corpus is one of the most widely used data sets for evaluating text classification performance . RCV1 has roughly one gigabyte of text . It consists of about 800,000 documents that were sent over the Reuters newswire during a one year period between August 20 , 1996 , and August 19 , 1997 . The collection covers a wide range of topics , including politics , business , and sports . The labels assigned to documents by Reuters editors can be used as a gold standard to evaluate nearest neighbor algorithms . closest neighbors of x . If values of this ratio are within a small interval , then this indicates that the disorder constant approximately reflects the true distribution of points . This does seem to be the case for Reuters as shown in Fig 2 for R = 5 ( the graph shows a histogram for 10,000 trials ) . Almost all ratios r are 10,1 r 100:3 and none are < 10,1:2 . We obtained similar results for R = 2 ; 5 ; 10 ; 50 ; 100 ; 1000 .
Disorder constant D .
In the previous section we introduced two algorithms , whose query time and preprocessing space depend upon the disorder constant D . These theoretical estimates become practical only if D is sufficiently small . Hence , a natural question arises : How small is D for real data sets ?
The value of D as defined in Def . 3 is in a sense a worstcase definition : we want the inequality ( 1 ) to hold for all triples . This worst case definition leads disorder constant to being too large . However , as we will see presently , in the case of Reuters corpus D is small for most triples .
We run a large number of micro experiments with parameters x 2 S and R 2 Z on the first 1000 documents of Reuters . The similarity measure we use is the cosine similarity on tf idf weighted vectors ( see , eg , [ 39] ) . Choose a ; b in [ 1 ; : : : ; R ] uniformly at random . Consider a randomly chosen news article z . Let x and y be the articles with rankz(x ) = a and rankz(y ) = b . Let c := ranky(x ) . Finally , c compute a+b . This ratio corresponds to D for triple ( x ; y ; z ) using inequality ( 3 ) .
Our experiments indicate that for R = 2 ; 5 ; 10 ; 50 ; 100 ; 1000 , c the ratio a+b is no more than 200 , for an overwhelmingly large fraction of the cases examined . In fact , for a large fraction of cases the ratio was close to 1 . We illustrate this for R = 5 in Fig 1 . a=5 y c n e u q e r F
0 0 4
0 0 3
0 0 2
0 0 1
0
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
2.5 log10 c/(a+b ) y c n e u q e r F
0 0 0 5
0 0 0 4
0 0 0 3
0 0 0 2
0 0 0 1
0
−1.0
−0.5
0.0 log10 rank_y(x)/rank_x(y )
Figure 2 : Distribution of ranky(x)=rankx(y ) for R = 5
Ranwalk algorithm .
We also evaluated the performance of the Ranwalk algorithm on Reuters . We chose the parameter D = 10 and selected a set of 1000 documents as training set and a disjoint set of 1000 documents as test set . We then performed 10,000 trials of randomly selecting a test point q and executing Ranwalk to find the nearest neighbor p . For some points q , rankp(q ) > D for all training points p since the disorder relationship does not hold for all points in the database . We therefore limited the number of \restarts" ( ie , returning to step 0 after failing to find a p with rankp(q ) D ) to 100 . In those cases , we selected from the set of 100 plog n candidates that p in step 3 that was closest to q .
Fig 3 shows the results of the experiment . For almost all query points , the algorithm successfully identified the nearest neighbor ( note logarithmic scale ) . There was only 1 case with rankq(p ) 30 . These results indicate good performance of Ranwalk on practical nearest neighbor search problems .
5 . RELATED WORK AND
OPEN PROBLEMS
Figure 1 : Distribution of ranky(x)=(rankz(x ) + rankz(y ) ) for R = 5
Related work .
A second test of the disorder constant is to compute the ratio ranky(x)=rankx(y ) for pairs < x ; y > , where x is randomly chosen from S and y is randomly chosen from the R
Our work is inspired by several ideas presented in the previous publications . The first one is to use walks and , in particular random walks for search problems . For nearest neighbors a walk algorithm was suggest by Orchard [ 42 ] . A y c n e u q e r f
0 1 _ g o l
4
3
2
1
0
0
10
20
30
40
50
60 rank_q of document p returned by ranwalk
Figure 3 : Distribution of rankq(p ) of p returned by Ranwalk hierarchical version of Orchard algorithm was presented by Clarkson [ 10 ] . It is very similar to our algorithms in the sense that every stage of the algorithm presumably bring us twice closer to the query point . Also , the random walk idea was used in SAT algorithms [ 19 ] , for measuring index quality [ 18 ] and for ranking nodes in the Web graph [ 1 ] .
The second idea is the small disorder constant assumption . The informal idea , that small dimensional spaces ( for some appropriate notion of dimension ) are more nicely behaved and tractable , is now well known in many settings . In particular , for nearest neighbor search it has appeared in several recent papers , where various notions of the dimension of a metric space are introduced , and efficient algorithms are given for small dimensional spaces ; eg , KR dimension , doubling dimension [ 6 , 10 , 11 , 26 , 30 , 31 ] . Comparing to these algorithms we also have near logarithmic search time . Our preprocessing time is quadratic while the recent paper by Har Peled and Mendel [ 17 ] provides a near linear preprocessing time in doubling metrics . Stress here , that disorder dimension might be small while doubling constant and KRdimension are large , example is due to Piotr Indyk [ 24 ] :
Example . A set of n points p1 ; :: : ; pn , such that , if i 6= j ; d(pi ; pj ) = 10n + ji , jj . The rank for ties are broken arbitrarily . It is not hard to see that i ; j ; ji , jj rankpi ( pj ) 2ji , jj , therefore , the disorder constant is at most 2 . However , note that the points are almost equidistant , in which case the doubling constant and the expansion rate is close to n .
Thus , we find a new class of datasets , for which nearest neighbor problem is tractable .
Definition 5 . KR dimension of a metric space M is the minimum d such that jB2r(x)j 2djBr(x)j , for all x and r ; here Br(x ) denotes the ball of radius r with center x .
We are currently unable to prove any direct comparison result for disorder dimension vs KR dimension or doubling dimension . However , one modelling example shows an advantage of disorder dimension over KR one . Consider a set of points where almost all distances ( dissimilarity values ) are in the interval [ 1=2 ; 1 ] . Then take any point x , let y be it ’s nearest neighbor and say d(x ; y ) = 0:6 . Then by taking radius 0:6 we have just one point in the ball but by taking 1:2 we get the whole database . Hence , KR dimension is the maximal possible for n points in this example . On the other hand , there is still a chance for disorder to be small .
Finally , the principle \respective order but not absolute values are really important" was previously applied in the somewhat related setting of Web ranking problem [ 33 ] . They were concerned with how different Web ranking algorithms change the ranks of the webpages because of small changes in the input .
Next steps .
We feel that the idea of focusing on rank values instead of similarity values is quite promising . Hence , we suggest to continue the study of nearest neighbors in this direction . Here is our question list :
Average disorder . If disorder inequality does not hold for a small fraction of pairs , how should we modify our algorithm ? One possible approach is to use some other method ( eg inverted index [ 39 ] ) for determining a \relatively similar" point in the database and then start the random walk from this point .
Improving our algorithms . How can one decrease preprocessing complexity of our algorithms ? Is it possible to combine advantages of Ranwalk and Arwalk ? Does there exist a deterministic algorithm with sublinear search time utilizing small disorder assumption ? Eg , can we use expanders for derandomization ? Can we use Ranwalk/Arwalk under other than disorder based assumptions ?
Disorder of random sets . Compute disorder values for some modelling examples . For example , consider n random points on d dimensional sphere , or n random strings of some fixed length in size alphabet for Hamming/edit distance .
Further experiments . Compute disorder constant for other data sets . Implement and test Ranwalk and Arwalk . Study them in the context of MESSIF project [ 5 ] .
Lower bounds . Is it possible to prove lower bounds on preprocessing and query complexities in some \blackbox" model of computation ? Can we adapt techniques from recent papers [ 31 , 43 ] ?
Utilizing combinatorial framework . Consider well known techniques like search trees [ 9 , 20 ] , hashing [ 2 , 25 , 32 ] or random projections [ 15 , 28 ] under bounded disorder assumption . What are their \combinatorial" analogues ? Can they beat random walk ? Construct disorder inequality based clustering algorithms . Also , what is analogue of disorder inequality for bichromatic nearest neighbor search ?
Remark . New algorithmic results in combinatorial framework were reported [ 37 ] during the preparation of cameraready version of this paper . Namely , nearest neighbor problem can be solved deterministically with O(D7n log2 n ) preprocessing and O(D4 log n ) search time . It was also shown that disorder dimension is at most twice larger than KRdimension .
6 . REFERENCES [ 1 ] A . Agarwal and S . Chakrabarti . Learning random walks to rank nodes in graphs . In ICML’07 .
[ 2 ] A . Andoni and P . Indyk . Near optimal hashing algorithms for approximate nearest neighbor in high dimensions . In FOCS’06 , 2006 .
[ 3 ] R . Angelova and G . Weikum . Graph based text classification : Learn from your neighbors . In SIGIR’06 .
[ 4 ] Z . Bar Yossef , I . Keidar , and U . Schonfeld . Do not crawl in the dust : Different urls with similar text . In WWW’06 .
[ 5 ] M . Batko , D . Novak , and P . Zezula . MESSIF : Metric similarity search implementation framework . In DELOS’07 .
[ 6 ] A . Beygelzimer , S . Kakade , and J . Langford . Cover trees for nearest neighbor . In ICML’06 .
[ 7 ] A . Blessing and H . Sch(cid:127)utze . Inverted indexes for fast distributional similarity computations . To appear , 2007 .
[ 8 ] A . Z . Broder , M . Charikar , A . M . Frieze , and
M . Mitzenmacher . Min wise independent permutations . In STOC’98 .
[ 9 ] E . Chavez , G . Navarro , R . Baeza Yates , and J . L .
Marroqun . Searching in metric spaces . ACM Computing Surveys , 2001 .
[ 10 ] K . L . Clarkson . Nearest neighbor searching and metric space dimensions . In Nearest Neighbor Methods for Learning and Vision : Theory and Practice , MIT Press , 2006 .
[ 11 ] R . Cole and L A Gottlieb . Searching dynamic point sets in spaces with bounded doubling dimension . In STOC’06 .
[ 12 ] R . Cole , L A Gottlieb , and M . Lewenstein .
Dictionary matching and indexing with errors and don’t cares . In STOC ’04 , pages 91{100 , 2004 .
[ 13 ] J . Dean and M . R . Henzinger . Finding related web pages in the world wide web . In WWW’99 .
[ 14 ] M . Dubinko , R . Kumar , J . Magnani , J . Novak ,
P . Raghavan , and A . Tomkins . Visualizing tags over time . In WWW’06 .
[ 15 ] R . Fagin , R . Kumar , and D . Sivakumar . Efficient similarity search and classification via rank aggregation . In SIGMOD’03 .
[ 16 ] D . Fetterly , M . Manasse , and M . Najork . Detecting phrase level duplication on the world wide web . In SIGIR’05 .
[ 17 ] S . Har Peled and M . Mendel . Fast construction of nets in low dimensional metrics , and their applications . SoCG’05 , SIAM Journal on Computing 2006 .
[ 18 ] M . Henzinger , A . Heydon , M . Mitzenmacher , and M . Najork . Measuring index quality using random walks on the web . In WWW’99 .
[ 19 ] E . A . Hirsch and A . Kojevnikov . Unitwalk : A new
SAT solver that uses local search guided by unit clause elimination . Annals of Mathematics and Artificial Intelligence , 2005 .
[ 20 ] G . Hjaltason and H . Samet . Index driven similarity search in metric spaces . ACM Transactions on Database Systems , 2003 .
[ 21 ] B . Hoffmann , Y . Lifshits , and D . Nowotka . Maximal intersection queries in randomized graph models . In CSR’07 .
[ 22 ] S . Ilyinsky , M . Kuzmin , A . Melkov , and I . Segalovich .
An efficient method to detect duplicates of web documents with the use of inverted index . In WWW’02 .
[ 23 ] P . Indyk . Nearest neighbors in high dimensional spaces . Chapter 39 of Handbook of Discrete and Computational Geometry , CRC Press , Second Edition , 2004 .
[ 24 ] P . Indyk . Private communication , 2007 . [ 25 ] P . Indyk and R . Motwani . Approximate nearest neighbors : towards removing the curse of dimensionality . In STOC’98 .
[ 26 ] D . R . Karger and M . Ruhl . Finding nearest neighbors in growth restricted metrics . In STOC’02 .
[ 27 ] J . Kleinberg . The small world phenomenon : an algorithm perspective . In STOC ’00 : Proceedings of the thirty second annual ACM symposium on Theory of computing , pages 163{170 , New York , NY , USA , 2000 . ACM Press .
[ 28 ] J . M . Kleinberg . Two algorithms for nearest neighbor search in high dimensions . In STOC’97 , pages 599{608 , 1997 .
[ 29 ] J . A . Konstan , B . N . Miller , D . Maltz , J . L . Herlocker ,
L . R . Gordon , and J . Riedl . Grouplens : applying collaborative filtering to usenet news . Commun . ACM , 40(3):77{87 , 1997 .
[ 30 ] R . Krauthgamer and J . R . Lee . Navigating nets : simple algorithms for proximity search . In SODA’04 .
[ 31 ] R . Krauthgamer and J . R . Lee . The black box complexity of nearest neighbor search . Theoretical Computer Science , 2005 .
[ 32 ] E . Kushilevitz , R . Ostrovsky , and Y . Rabani . Efficient search for approximate nearest neighbor in high dimensional spaces . SIAM J . Comput . , 2000 . [ 33 ] R . Lempel and S . Moran . Rank stability and rank similarity of link basedweb ranking algorithms in authority connected graphs . Information Retrieval , 2005 .
[ 34 ] D . D . Lewis , Y . Yang , T . G . Rose , and F . Li . RCV1 :
A new benchmark collection for text categorization research . Journal of Machine Learning Research , 2004 .
[ 35 ] Y . Lifshits . A guide to web research . Materials of mini course at Stuttgart University . Available at http://logicpdmirasru/yura/webguidehtml , 2007 .
[ 36 ] Y . Lifshits and D . Nowotka . Estimation of the click volume by large scale regression analysis . In CSR’07 .
[ 37 ] Y . Lifshits and S . Zhang . Similarity search via combinatorial nets . Submitted .
[ 38 ] G . Linden , B . Smith , and J . York . Amazon.com recommendations : item to item collaborative filtering . Internet Computing , 2003 .
[ 39 ] C . D . Manning , P . Raghavan , and H . Sch(cid:127)utze .
Introduction to Information Retrieval . Cambridge University Press , to appear in 2008 .
[ 40 ] C . D . Manning and H . Sch(cid:127)utze . Foundations of
Statistical Natural Language Processing . MIT Press , 1999 .
[ 41 ] A . Ntoulas , M . Najork , M . Manasse , and D . Fetterly .
Detecting spam web pages through content analysis . In WWW’06 .
[ 42 ] M . T . Orchard . A fast nearest neighbor search algorithm . In ICASSP’91 .
[ 43 ] C . Sahinalp and A . Utis . Hardness of string similarity search and other indexing problems . In ICALP’04 .
