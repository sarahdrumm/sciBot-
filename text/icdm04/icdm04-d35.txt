SVD based Term Suggestion and Ranking System
David Gleich
Harvey Mudd College
Claremont,CA dgleich@cshmcedu
Leonid Zhukov
Yahoo! Research Labs
Pasadena , CA leonidzhukov@overturecom
Abstract similarity .
In this paper , we consider the application of the singular value decomposition ( SVD ) to a search term suggestion system in a pay for performance search market . We propose a novel positive and negative refinement method based on orthogonal subspace projections . We demonstrate that SVD subspace based methods : 1 ) expand coverage by reordering the results , and 2 ) enhance the clustered structure of the data . The numerical experiments reported in this paper were performed on Overture ’s pay per performance search market data .
1 . Introduction
In a pay for performance search market , advertisers compete in online auctions by bidding on search terms for sponsored listings in affiliated search engines . Because of the competitive nature of the market , each search term may have bids from many advertisers , and almost every advertiser bids on more than one search term . The practical motivation for our work was creating a “ term suggestion ” tool , which , for any given search term , provides a sorted list of relevant search term suggestions from the existing database . One of the desired features of this term suggestion tool is a smoothly controlled level of “ generality ” of suggested terms . To that end , we decided to use a vector space model and a singular value decomposition ( SVD ) [ 8 ] based approach to term ranking and suggestion .
It is well known that Latent Semantic Indexing ( LSI ) [ 6 , 7 ] , an SVD based method , can help to expose semantic information within a dataset . Most papers cite the use of LSI to enhance text information retrieval systems [ 2 , 3 , 5 ] . In this context , LSI is used to compute a document query similarity score for each document in the collection . However , as noticed in [ 6 , 10 ] , we can also compute the similarity between documents and other documents , between documents and terms , and between terms and other terms . In this paper , we focus on using LSI ( SVD ) for term term
According to the vector space model [ 12 ] , every search term in our dataset is represented as a vector in a space of all advertisers with nonzero entries corresponding to advertisers bidding on this term . Then , the proximity between search terms can be measured as the cosine of the angle between corresponding term vectors . We can calculate the proximity between any given search term and the rest of the terms in the collection and then sort the retrieved terms according to this proximity measure .
Using the vector space approach allows retrieval of all terms correlated with the search term ; we call this an exact match . Using SVD , we can also perform a conceptual match [ 6 , 2 ] , which might significantly expand the number of suggested terms and also change their ranking . In other words , SVD enables us to match terms globally , or conceptually , without the need for explicit connections .
The main goal of this paper is to investigate the use of SVD as a suggestion tool for relational data , establish a better understanding of its behavior , and provide a new method for interactive refinement of search results . To our knowledge , this is a first analysis of the effect of SVD ( LSI ) application to relational/textual data based on a direct comparison of the similarity curves and corresponding histograms . We also contribute a new refinement operation based on positive and negative subspaces .
Additionally , the dataset we use obeys a power law distribution ie , the number of terms of degree y is proportional to y−α for α > 0 . Thus , the results obtained in this study are applicable to other data that follow a power law distribution , such as textual data [ 1 ] . Although our dataset and application may seem overly specific , our algorithms are applicable to any set of relational data , such as that for collaborative filtering and recommender systems [ 13 ] .
The rest of the paper is organized as follows .
In the “ Methods ” section we provide mathematical formulation of the problem and describe details of exact and SVD subspace ranking and subspace based iterative refinement procedure . In the “ Implementation ” section we provide some details of the suggestion system we have developed . The
( a ) Bipartite Graph Representation .
( b ) Vector Space Representation .
Figure 1 . Bipartite graph and vector space representations for bidded search terms advertiser data
“ Results ” section demonstrates the application of the methods and tools to the Overture ’s US market data . Finally in “ Discussion ” , we describe the evaluation of our methods and provide discussion of the results and some additional insight into SVD based projection properties and behavior .
2 . Data
Aij = non negative and is also sparse , since the majority of the advertisers bid only on a small number of terms . Fig 1(b ) presents this matrix .
The matrix A is normalized using the binary frequency variant of term frequency , inverse document frequency normalization [ 3 ] ,
Pm χij log(n/ni ) k=1[χkj log(n/nk)]2 ,
( 1 )
A typical “ advertiser search term ” dataset from Overture ’s US market consists of over 150,000 advertisers , 130,000 bidded keywords and more than 3 million bids . In this study , we use a small , densely connected subset of that data with 10,000 bidded search terms , 8,850 advertisers , and more than 250,000 bids where each term has at least 29 next nearest neighbors through the advertiser associations , all advertisers bid on at least three terms , and each search term has at least 9 bids .
The “ advertiser search term ” relationship can be represented by a bipartite graph with edges connecting advertisers to keywords . The advertisers are on one side of the graph and the keywords are on the other side as depicted in Fig 1(a ) . The edges of the graph might also contain bid values . In this representation all correlated terms are connected through the same advertiser , ie are next nearest neighbors . An alternative representation for the data can be given by an “ advertiser search term ” matrix ,A , whose columns correspond to advertisers and rows to bidded search terms . The number of rows in this matrix , m , is equal to the number of unique bidded search terms , and the number of columns , n , is the number of unique advertisers active on the market . Thus , every column of this matrix represents an advertiser vector described in the bidded terms space and every row is a bidded term vector in the advertiser space . This arrangement follows the traditional vector space model [ 12 ] for a textual collection , where every column is a word distribution histogram for a document . This matrix is strictly where χij is 1 if advertiser j bids on term i , n is the total number of advertisers in the collection , and ni is the number of advertisers that have bids on term i .
3 . Method
In this section , we describe methods used in our system . First , we depict projection operators from linear algebra . Second , we discuss the cosine distance measure and its interactions with projection operators . Next , we discuss projections in the reduced dimensionality subspace and how we use these projections to rank term suggestions . Finally , we introduce a novel method of relevance feedback using subspace projections .
31 Projection Operators
An orthogonal projection on a unit vector v is defined as ˆP = vvT , where v is a column vector . An orthogonal projection on a subspace spanned by a collection of k vectors v1 , v2 , . . . , vk is given by a projection operator
ˆPk = vivT i = VkV T k .
( 2 ) i=1
Any projection operator has the property that ˆPk ˆPk = ˆPk and orthogonal projections have the additional property that ˆP T k = ˆPk . kX
TermsAdvertisersajtiA =TermsAdvertisersajtiA = 32 Cosine Distance Measure
For any two terms , ti and tj , we define the similarity metric as a cosine of the angle between corresponding vectors , sim(ti , tj ) = cos(ti , tj ) = tT i
( 3 )
· tj
||ti|| ||tj|| .
In the subspace , defined by its orthogonal projection ˆPk , the similarity ( cosine of the angle ) between vector projections is sim( ˆPkti , ˆPktj ) = cos( ˆPkti , ˆPktj ) =
( ˆPkti)T · ( ˆPkti ) || ˆPkti|| || ˆPktj|| . ( 4 ) Using Eq 2 , the scalar product between vectors can be expressed as
( ˆPkti)T · ( ˆPktj ) = tT i
ˆPktj = ( V T k ti ) · ( V T k tj ) , and the vector norm in the subspace is given by k t||2 .
|| ˆPkt||2 = ( ˆPkt)T ( ˆPkt ) = tT ˆPkt = ||V T
33 SVD Subspace Projections
( 5 )
( 6 )
We first decompose term advertiser matrix A using the singular value decomposition ,
A = U SV T .
( 7 )
The first k columns of the matrix V form the truncated orthogonal subspace , Vk . Note , that we are using Vk instead of Uk since we are interested in term space , not advertiser space , and thus it might be easier to think about AT = V SU T factorization instead . The number of columns in Vk is equal to the rank k of the subspace we use . Every column of Vk is a basis vector , and any search term in the dataset can be represented as a linear combination of Vk and SkU T k are the coefficients of the terms in the Vk subspace . The Eckart and Young theorem [ 8 ] guarantees that top k singular vectors provide the best ( closest in the L2 norm sense ) approximation of the data vectors Ak to A in any basis of the order k . k , where the columns of SkU T
34 Search Terms Ranking
A query q is a search term represented in the advertiser space , or in other words , a query is a column ai of the matrix AT ; alternatively , it is a row of the matrix A . Mathematically , it is convenient to express qi = ai = AT ei , where ei is a column vector of all zeros except for a position corresponding to the column of interest in the matrix AT , or row
Figure 2 . The cosine similarity score between search terms and the data : the solid line is the exact cosine similarity ; the dotted lines are cosine similarities in three SVD subspaces . in the matrix A . The angle between a query vector and any other term vector in the matrix is sim(ti , qj ) = cos(ti , qj ) = aT i aj
( AT ei)T ( AT ej )
||ai|| ||aj||
||ai|| ||aj|| = ( AAT )ij = ||ai|| ||aj|| ,
( 8 ) which is a normalized inner product of AT columns , or A matrix rows .
The similarity for the same vectors in the SVD orthogo nal subspace is given by sim(V T k ti , V T k AT ej ) k AT ej)|| = k qj ) = k tj ) = cos(V T k ti , V T k )j k )T i ( SU T ( SU T k AT ei)T ( V T ( V T ||(V T k AT ei)T|| ||(V T ||(SU T i || ||(SU T k )j|| . k )T The above result of SVD decomposition is equivalent to the use of eigen decomposition ( ie principal component analysis ) on a correlation ( affinity ) term term matrix AAT , where Λi = S2 i and Ui are eigenvectors , that is ,
( 9 )
( AAT )Ui = UiΛi .
( 10 )
35 Iterative Refinement
We can iteratively refine the results when the user chooses terms from the returned results to reinforce or reject the ranking , thus performing positive or negative refinements or relevance feedback .
1001011021031040020406081Term "jewelry silver"Cosine Similarity1001011021031040020406081Term "future trading"Cosine Similarity1001011021031040020406081Term "marketing strategy"Cosine Similarity1001011021031040020406081Term "instrument musical"Cosine Similarityk=50k=100k=150exact By positive refinement , we mean the user selecting positive , reinforcing examples to his query from the provided term list . Then , instead of using q0 as a query , we can construct a new extended query , that spans the space {q0 , aj1 , aj2 , ajp} , where aj is the j th column of AT corresponding to the term that the user chooses to reinforce the query . Notice , that we are not computing the centroid for a new query as in [ 3 , 2 ] , but rather are measuring the angle between the terms and an extended query subspace . If the space is formed by non orthogonal vectors , the projection operator on that subspace is given by [ 14 ] . ˆPQ = Q(QT Q)−1QT .
( 11 )
When vectors qi forming the subspace are orthogonal , ie QT Q = I , then the projection operator reduces to Eq ( 2 ) . The angle between a term and the positive term subspace is defined as the angle between a term and its orthogonal projection on that subspace . Formally , ˆPQti sim(ti , Q ) = tT i
||ti|| || ˆPQti|| .
( 12 )
This feedback mechanism works in both the entire space and SVD subspace . For the SVD subspace , instead of ti we use tik = VkV T k ai . Table 2 displays the change in search term ranking after positive refinement , and Fig 5 shows the change in similarity scores with positive refinement . k ti and Q is formed using aik = VkV T
Negative refinement allows users to choose irrelevant documents and force the search results to be orthogonal to them . Thus , we are looking for a vector term in the collection with the smallest angle with the query and , at the same time , orthogonal to the negative term vectors specified by the user . Again , we want to emphasize that our method will produce results orthogonal to the entire subspace spanned by negative examples and not to only those terms1 . In this case , we need to build a complementary projector to the negative examples space ,
ˆPQn = I − ˆPQ .
Then the new similarity score becomes i ( I − ˆP T tT ||ti|| ||(I − ˆP T sim(ti , Q ) =
Qn)ti Qn)ti|| .
( 13 )
( 14 ) and search results are ranked according to this similarity .
4 . Implementation
We developed two programs for this work . The first is a program to compute the truncated SVD of a sparse matrix using the Implicitly Restarted Lanczos Method implemented in ARPACK [ 11 ] , and ARPACK++ [ 9 ] libraries . 1The subspace spanned by examples means that the returned result will be orthogonal to any possible linear combination of negative documents
Figure 3 . A screen shot of the user interface to our term suggestion tool . The table shows the suggestions for the word “ flower , ” using a fairly specific level of generality ( k > 100 ) .
The second is a Java program to query a dataset and retrieve results between general and specific associations ( Fig 3 shows the user interface in the Java application ) .
While the formulas presented in the previous section provide a compact description of the operations , they are extremely inefficient as written . For example , while the original matrix A is around 3 MB in a sparse matrix representation , the matrix AAT is more than 300 MB . Thus , we needed to plan the application solely using sparse and dense matrix vector multiplications .
Instead of directly computing the final form of Equation 8 , we first row normalize the sparse matrix A to ˆA then compute the intermediate vector ai = ˆAT ei through a sparse matrix vector multiply . This operation extracts the query vector , row i from matrix A , which is already normalized . Then we compute the matrix vector product Aai , which simultaneously computes the cosines for each term in the collection . We perform a similar operation using the pre computed dense matrix UkS to compute the cosines in the SVD subspace as in Equation 9 .
Finally , before we perform any projection operations for refinement operations , we orthogonalize vectors using the Gram Schmidt process [ 8 ] . This operation is computationally more efficient and stable than inverting the matrix in the non orthogonal case .
5 . Results
Our main results are illustrated in Figs . 2 5 and Fig 6 . Figures 2 5 present our novel similarity curve analysis of the differences between exact and SVD based matches . Fig
( a ) Rank ordered similarity scores .
( b ) Histograms of similarity scores .
Figure 4 . The suggested results for the term “ flower ” from the exact cosine method and the SVD method with k = 100 . In ( a ) on the lower horizontal axis we include sorted suggestions at various points for the SVD results ; these terms correspond to the upper curve . The upper horizontal axis has suggestions from the exact approach , corresponding to the lower curve . In ( b ) the horizontal axis is the similarity score and the vertical axis shows the logarithm of the number of results with that similarity score , ie histograms based on the curves in ( a ) . Notice how a the cluster of terms with small angles is visibly grouped and separated from the main results in the lower histogram .
( a ) Rank ordered similarity scores .
( b ) Histograms of similarity scores .
Figure 5 . Cosine similarity result curves with user feedback . The bottom curve shows the exact similarity results for the term “ internet provider ; ” the middle curve is for k = 100 SVD similarity ; and the upper curve is for k = 100 similarity with positive refinement on the term ” cheap isp . ”
00102030405060708091flower,1bouquet,2florist,3flower fresh,10discount flower,20flower online order,30christmas wreath,100mall online,200gift shop,300toy wooden,1000biloba ginkgo,2000hotel washington,3000executive gift idea,10000flower,1delivery flower,2flower online,3arrangement flower,10anniversary flower,20birthday flower,30magazine,100joop,200basket gift golf,300silver sterling,1000snore,2000insurance online,3000zoloft,10000Cosine Similarity to "flower"0010203040506070809101234Cosine distanceLSI frequency0010203040506070809101234Exact frequency10010110210310400102030405060708091Term ’internet provider’Cosine similarity0010203040506070809101234full distance frequency.Cosine similarity to ’internet provider’0010203040506070809101234LSI distance frequency001020304050607080910123LSI refined distance frequency . ure 6 shows a precision and recall analysis of the suggestions for two terms .
51 Similarity Curves
Figure 2 demonstrates the cosine similarity scores for all terms in the dataset to four different search terms in exact match and how the similarity scores change when projecting into SVD subspaces with k = 50 , 100 , 150 .
A more detailed example is given in Fig 4 , where we present the suggested results for the term “ flower ” from the exact cosine method and the SVD method with k = 100 . In this example , we directly compare the two similarity curves based on their values and the ordering each method generates . The histograms of the curves present another view of the results showing some properties of the projection into the SVD subspace . We discuss these properties in the forthcoming section .
The corresponding term suggestions for the query “ flower ” are shown in Table 1 . There is little different between the first set of results ( 1 − 10 ) . In the second set of results ( 71−78 ) , the results from LSI are better . One important aspect of the results is that they are suggested terms to an advertiser which sells flowers . Hence , the suggestion of “ office product ” and “ cosmetic ” are not relevant . However , the suggestion of “ stuffed bear ” and “ gourmet basket ” are relevant in that many companies which sell flowers also sell these items . Many other terms suggested for LSI are holidays or events where flowers are frequently given as gifts , ie “ valentine ’s day ” and “ birthday ” – good terms for an advertiser selling flowers .
Next , Fig 5 demonstrates the results of positive user feedback obtained by the subspace projection method . The “ bulge ” in the results with relevant feedback shows a weak cluster of cosine values between 0.5 and 06
Finally , Table 2 provides the top 10 suggested terms before and after positive refinement on the term “ cheap isp . ” The results after refinement are more relevant to the concept behind the query , ie inexpensive internet access .
“ flower ” ( full ) flower deliver flower flower online flower send florist florist online flower shop rose flower fresh arrangement flower flower funeral order flower funeral arrangement office product sunflower birthday cooking baby cosmetic
1 2 3 4 5 6 7 8 9 10 71 72 73 74 75 76 77 78
1
0.591 0.548 0.520 0.516 0.505 0.466 0.463 0.446 0.440 0.108 0.108 0.107 0.106 0.106 0.106 0.100 0.098 flower online rose ftd
1 arrangement flower florist online flower bouquet florist floral
“ flower ” ( SVD k = 100 ) 1 2 3 4 5 6 7 8 9 10 71 72 73 74 75 76 77 78
0.983 0.983 0.977 0.977 0.977 0.974 0.974 0.974 0.971 0.646 0.636 0.588 0.579 0.565 0.552 0.519 0.501 flower fresh thanksgiving valentine stuffed bear fruit basket gift birthday gourmet basket valentine day wreath
Table 1 . The top ten and higher results from the query ” flower . ” The left column presents the suggested term and the cosine similarity score from the exact method ; the right column presents the SVD method with k = 100 . The terms from LSI are more relevant in the later results . whether terms were relevant to the query “ internet provider ” with positive relevance feedback on “ cheap isp . ” This judge was asked to mark a term relevant if an advertiser offering inexpensive internet access would like to see the term . The second judge is an author on this paper .
With the relevance judgments , we evaluated precision and recall for the top 10 , 20 , 30 , 40 50 , 75 , 100 , 150 , 200 , 500 , 1000 , 2000 , 5000 , and 10000 results from the various methods . Fig 6 presents the precision and recall curves from the exact method , the SVD method with k = 100 , and an example with relevance feedback . The results show when precision is high , the methods are all roughly equivalent . As precision decreases , the results from SVD are better . Finally , when the precision is low ( ≈< 0.3 ) , the methods all show bad results , although SVD appears moderately worse .
52 Precision and Recall Evaluation
6 . Discussion
We also evaluated the precision and recall of SVD and the new relevance feedback technique . Precision is the fraction of relevant results in a sample of the returned set . Recall is the fraction of relevant results in the returned set out of the entire set of relevant results [ 1 ] . The returned set is the top n items suggested by a method . We had two human judges score each of the 10,000 terms in our dataset . The first judge evaluated whether terms were relevant to the query “ flower . ” Specifically , that judge was asked to determine if he or she would like to see a term suggested to an advertiser that sells flowers . The second judge evaluated
The above results indicate two major tendencies of dimensionality reducing by SVD subspace projection : expanded coverage and clustering behavior .
61 Expanded Coverage
By expanded coverage , we refer to the increase in recall at moderate precision ( 03 07 ) In SVD subspace projection , the angles between term vectors change nonuniformly . This change results in differences in ranking ( ordering ) of search results for a given search term from the
“ internet provider ” internet provider internet access provider internet provider service access internet isp isp provider cheap isp internet service access web isp national
“ internet provider ” + “ cheap isp ” cheap isp internet provider low cost isp internet access provider national isp isp provider cheap internet service unlimited internet access cheap internet service provider cheap internet access
Table 2 . Changes in the top 10 suggested terms after positive refinement on the term “ cheap isp , ” that is , inexpensive internet service providers . “ Cheap isp ” was the 18th result in the first query . exact match method to the approximate match method ( LSI ) and , consequently , different suggestions . The difference in suggestions results in the increase in coverage through SVD subspace projections .
Vectors that are close in the original data get closer when projected into the SVD subspace , as seen in the “ flower ” example in Fig 4 . Likewise , vectors that were distant in the original data become more distant in the SVD subspace . There is still a reordering within projected vectors , but only a local reordering . The dotted line in the middle of Fig 4(a ) represents the distortion between the distances in SVD subspace and in complete space . That is , we plot the similarity values from the exact cosine sorting results using the ordering from the SVD results . Of note in this figure is that there is very little distortion of the results beyond the plateau . This fact indicates that SVD only performs a local reordering of the results .
62 Clustering Behavior
Figures 4 5 demonstrate the clustering behavior that occurs with SVD . The plateau at the top of the SVD curve represents a set of results whose similarity scores are high . If we take the histogram of the distances , as in Fig 4(b ) this behavior becomes even more apparent . The cluster at the right of the SVD histogram represents the “ flower ” cluster in the data . Interestingly , the steep decline of the SVD curve corresponds to the end of related terms .
This behavior , however , does not occur for all terms in the dataset . In Fig 2 , the terms “ marketing strategy ” and “ instrument musical ” do not display any clear clustering behavior . From Fig 5 , there is a cluster of terms associated with “ internet provider , ” which remains after positive relevance feedback .
We believe that this behavior explains the results in the precision recall graph . The “ clusters ” from SVD cause the items in the cluster associated with the search term to be returned first . Because SVD is a global dimensionality reduc tion technique , these clusters may contain terms not directly associated with the query term . The other related terms in the cluster cause the recall of SVD to increase as precision begins to decrease .
63 Polarization Theorem
Our results indicate that as the data representation is truncated , the angles between vectors become smaller causing the cosine values to increase as in Figs . 2 , 4 , and 5 . One explanation for these results comes from [ 4 ] . In that paper , the author suggests that as the dimensionality of the representation is reduced , the distribution of cosines ( similarity measures ) migrates away from zero to ±1 . This behavior is easily seen in Figs . 4 and 5 . Mathematically , this statement follows from a polarization theorem [ 4 ] that states when a positive matrix A is projected to successively lower dimensions through projection onto the largest magnitude eigenvectors AD−1 , . . . , Ak , . . . , A1 , the sum of squared angle cosines between projected column vectors , i6=j[cos(ti , tj)]2 , is strictly increasing . Our numerical results confirm this statement . For example , the sum of squared angle cosines between the query term “ flower ” and all other terms in the dataset strictly increases as k de
P creases : k=100,P = 190.5 ; k=75,P = 217.8 ; k = 50,P = 284.8 ; k=25,P = 9359
Thus , the singular value decomposition of A exaggerates the structure of A by reducing the angle between similar vectors and increasing the angle between dissimilar vectors . Returning to Fig 4(a ) and 4(b ) , the plateau in the results from SVD clearly shows how term vectors similar to “ flower ” are almost collapsed to the same point . The terms , “ jewelry silver ” and “ future trading ” from Fig 2 also demonstrate this effect . However , there is no such result for “ marketing strategy ” and “ instrument musical . ” In these cases , we believe that no set of similar vectors exists .
Since the steep decline in the SVD curve corresponds to the end of the related terms for the query “ flower , ” identifying this cutoff suggests a natural way to cluster a dataset using SVD . For the terms , “ marketing strategy ” and “ instrument musical , ” then , there is no good cluster in the data for these terms . We observed that a topic generally does not correspond to a single axis in the SVD subspace and accurate clustering requires working in subspaces .
The local distortion effect identified in the previous section reinforces our belief in SVD ’s clustering ability . The largest distortion is between vectors that are already similar ( the left side of the curve in Fig 4(a) ) . Thus , projecting into the SVD subspace moves clusters of vectors closer .
Because the change in similarity scores only induces a local reordering of the suggested terms , the SVD method extracts the appropriate clusters of terms by enhancing the accuracy of the cosine distance metric . Therefore , in the
( a ) Precision and Recall for ” flower . ”
( b ) Precision and Recall for ” internet provider ” with positive feedback on ” cheap isp . ”
( c ) Precision and Recall for ” cheap isp . ”
Figure 6 . The precision and recall curves for the term “ flower ” and the inexpensive internet access concept .
SVD subspace , large angles represent significant dissimilarity between terms .
7 . Summary and Conclusions
We investigated the effect of SVD subspace projections on data from Overture ’s advertising market . We developed a tool to suggest related terms at varying levels of generality . Additionally , we developed a novel relevance feedback system for positive and negative examples using subspaces . Our results demonstrate that the angle distorting properties of SVD have a number of benefits . The local reordering of results expands coverage to improve precision and recall , and the distorted angles emphasize clusters inherent in the data . The polarization theorem [ 4 ] for reduced dimensional projections suggests that projection into the SVD subspace clusters the dataset .
8 . Acknowledgments
We would like to thank Professor Al Barr for illuminating discussions on the properties of dimensionality reducing projections . Also , we wish to thank Rosie Jones for discussing our precision and recall experiments .
References
[ 1 ] R . Baeza Yates and B . Ribeiro Neto . Modern Information
Retrieval . Addison Wesley , 1999 .
[ 2 ] M . Berry , Z . Drmac , and E . Jessup . Matrices , vector spaces , and information retrieval . SIAM Review , 41(2):335–362 , 1999 .
[ 3 ] M . W . Berry and M . Browne . Understanding search engines : mathematical modeling and text retrieval . Society for Industrial and Applied Mathematics , 1999 .
[ 4 ] M . Brand and K . Huang . A unifying theorem for spectral embedding and clustering . In C . M . Bishop and B . J . Frey , editors , Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics , January 2003 .
[ 5 ] C M Chen , N . Stoffel , M . Post , C . Basu , D . Bassu , and C . Behrens . Telcordia lsi engine : Implementation and scalability issues . In Proceedings of the Eleventh International Workshop on Research Issues in Data Engineering , pages 51–58 , April 2001 .
[ 6 ] S . C . Deerwester , S . T . Dumais , T . K . Landauer , G . W . Furnas , and R . A . Harshman . Indexing by latent semantic analysis . Journal of the American Society of Information Science , 41(6):391–407 , 1990 .
[ 7 ] S . Dumais . Improving the retrieval of information from external sources . Behavior Research Methods , Instruments , and Computers , 23(2 ) , 1991 .
[ 8 ] G . H . Golub and C . F . V . Loan . Matrix Computations . John
Hopkins Univ . Press , 1989 . and D .
[ 9 ] F . R . Gomes
Sorensen .
Arpack++ . http://wwwimeunicampbr/ chico/arpack++/ , 1998 .
[ 10 ] J . Kleinberg and A . Tomkins . Applications of linear algebra in information retrieval and hypertext analysis . In Proceedings of the eighteenth ACM SIGMOD SIGACT SIGART symposium on Principles of database systems , pages 185– 193 . ACM Press , 1999 .
[ 11 ] R . Lehoucq , D . Sorensen , and C . Yang . ARPACK Users’ Guide : Solutions of Large Scale Eigenvalue Problems with Implicitly Restarted Arnoldi Methods , 1997 .
[ 12 ] G . Salton , A . Wong , and C . S . Yang . A vector space model for automatic indexing . Commun . ACM , 18(11):613–620 , 1975 .
[ 13 ] B . M . Sarwar , G . Karypis , J . A . Konstan , and J . Riedl . Application of dimensionality reduction in recommender sys
0010203040506070809100102030405060708091RecallPrecisionPrecision and Recall for term ’flower’LSI ( k=100)Full Space0010203040506070809100102030405060708091RecallPrecisionPrecision and Recall for relevance freedback.’internet provider’’internet provider’ + ’cheap isp’010203040506070809100102030405060708091RecallPrecisionPrecision and Recall for ’cheap isp.’Relevance FeedbackFull SpaceLSI ( k=100 ) tems – a case study . In ACM WebKDD Web Mining for ECommerce Workship , 2000 .
[ 14 ] L . N . Trefethen and I . D . Bau . Numerical Linear Algebra .
SIAM , 1997 .
