Scalable Construction of Topic Directory with
Nonparametric Closed Termset Mining
Hwanjo Yu , Duane Searsmith , Xiaolei Li , Jiawei Han
Department of Computer Science
University of Illinois at Urbana Champaign
Urbana , IL 61801 hwanjoyu@uiuc.edu , dsears@ncsauiucedu , xli10@uiuc.edu , hanj@csuiucedu
ABSTRACT A topic directory , eg , Yahoo directory , provides a view of a document set at difierent levels of abstraction and is ideal for the interactive exploration and visualization of the document set . We present a method that dynamically generates a topic directory from a document set using a frequent closed termset mining algorithm . Our method shows experimental results of equal quality to recent document clustering methods and has additional beneflts such as automatic generation of topic labels and determination of a clustering parameter .
Keywords topic directory , document clustering , hierarchical clustering
1 .
INTRODUCTION
A topic directory is a hierarchical document tree or graph structure in which each node has a topic label ( or a cluster description ) and corresponding documents . The topic of a higher node conceptually covers its children nodes . For a static topic directory , eg , Yahoo Web directory , the taxonomy is static and manually constructed by the domain experts , and documents are classifled into the taxonomy by ( non )automatic classiflers . Such static directories are usually used for organizing and searching targeted documents . On the other hand , dynamic topic directories are constructed automatically given a flxed document set or a temporal interest across document sets , eg , browsing the main news of year 2000 from the AP news data . The directory and topic labels are constructed dynamically ( ie , no preset taxonomy ) based on the contents of the document set .
Construction of a dynamic topic directory requires the techniques of hierarchical document soft clustering and cluster summarization for constructing topic labels . Recent studies in document clustering show that UPGMA [ 4 ] and bisecting k means [ 7 , 3 ] are the most accurate algorithms in
1 the categories of agglomerative and partitioning clustering algorithms respectively and outperform other recent hierarchical clustering methods in terms of the clustering quality [ 7 , 4 , 3 ] . However , such clustering methods ( 1 ) do not provide cluster descriptions , ( 2 ) are not scalable to large document sets ( for UPGMA ) , ( 3 ) require the user to decide the number of clusters a priori which is usually unknown in real applications , and ( 4 ) focus on hard clustering ( whereas in the real world a document could belong to multiple categories ) .
Another recent approach is to use frequent itemset mining to construct clusters with corresponding topic labels [ 2 , 5 ] . This approach flrst run a frequent itemset mining algorithm , eg , Apriori [ 1 ] , to mine frequent termsets from a document set . ( An itemset corresponds to a termset , ie , a set of terms , in this case . ) Then they cluster documents based on only the low dimensional frequent termsets . Each frequent termset serves as the topic label of a cluster . The corresponding cluster consists of the set of documents containing the termset . This method indeed turns out to be as accurate as the other leading document clustering algorithms ( eg , bisecting k means and UPGMA ) in terms of clustering quality [ 5 ] , and is more e–cient since it substantially reduces the dimensions when constructing clusters .
However , this approach introduces another critical issue { determination of the support threshold . Since clusters are constructed by frequent termsets and cluster size is the support of termset , the number of clusters ( ie , previously a user parameter ) is now determined by the support threshold ( ie , a new user parameter ) . The support threshold afiects the entire cluster processing in terms of the quality and scalability : An over set threshold could delay the mining time exponentially and also generate too many frequent terms . An under set threshold could generate a too abstract directory to cover every document in the set . The seemingly best way to adjust the support threshold is to run the mining algorithm multiple times with difierent support thresholds from small to large , and probe the information about the abstraction level of the directory or the cluster coverage , ie , what portion of documents is covered by the clusters . However , then we would end up losing the beneflt of using mining algorithm for document clustering . The entire clustering process becomes unscalable and need tedious manual optimization .
We propose a nonparametric closed termset mining method for e–cient topic directory construction , which ( 1 ) adjusts the support threshold before running the mining algorithm by introducing an FT tree ( See Section 2.1 ) , and ( 2 ) run the most e–cient frequent closed termset mining algorithm { CLOSET+ [ 8 ] . While the previous clustering methods [ 5 , 2 ] use all the frequent termsets to construct hierarchical clusters , only closed termsets are meaningful in hierarchical clustering ( discussed in Section 213 ) We flnally present an e–cient way to build the soft clusters from the initial clusters . Soft clustering is necessary for many applications because a document can belong to multiple clusters . Empirically , our method shows clustering quality as high as most recent document clustering methods but is more e–cient . It also naturally produces topic labels for the clusters using frequent closed termsets .
2 . SCALABLE CONSTRUCTION OF TOPIC DIRECTORY WITH NONPARAMETRIC CLOSED TERMSET MINING
211 FT tree Construction
The FP tree published in [ 6 ] is a preflx tree with sorted items in which each node contains an item and the support of the itemset from root to path . The FP tree has proven to be an e–cient structure for mining frequent ( closed ) itemsets [ 8 ] . The FT tree is similar to the FP tree except that the FT tree includes document IDs in addition . For instance , Figure 2(a ) shows the FT tree constructed from document set D of the table in Fig 2 . The FT tree can be deflned as the FP tree including document ID at the last node of the corresponding path . Constructing a FT tree is also similar to constructing a FP tree except that when we insert a termset representing a document ( ie , a pattern in the FPtree ) into the tree , we insert the document ID at the last node . Note that each document ID will show only once the FT tree because each document or termset is represented by only one path in the FT tree , so multiple paths cannot have the same document ID .
TFIDF Vectors
FT tree
212 Probing Support Threshold
How can we e–ciently identify the maximal sup thr without running a mining algorithm , such that the clusters ( ie , the mined termsets ) generated from the sup thr cover every document in the document set ( or cluster coverage = 1.0 ) ? To illustrate , consider the FT tree of Figure 2(a ) that is constructed from the table in Fig 2 . We start pruning the tree from the bottom . ( Since a FT tree is a preflx tree with sorted items , as is a FP tree , the lower nodes contain the items of lower supports . ) The item f of support = 2 , ie , the two nodes of thick lines in Figure 2(a ) , will be pruned flrst . If the pruned nodes contain any document IDs , we pass the IDs to their parents nodes . Thus , the FT tree after pruning f becomes the tree of Figure 2(b ) . As you see , the parent nodes a and b now in Figure 2(b ) contain the IDs d1 and d4 respectively . This means that after we prune a term f , documents d1 and d4 { previously covered by termsets fe ; b ; a ; f g and fb ; f g respectively { are now covered by termsets fe ; b ; ag and fbg . Next , we prune the term a of support = 3 , ie , the three nodes of thick lines in Figure 2(b ) . Then , the tree of Figure 2(b ) becomes the tree of Figure 2(c ) . In other words , documents d1 , d5 and d2 { previously covered by termsets fe ; b ; ag , fe ; c ; d ; ag and fc ; d ; ag respectively { are now covered by termsets fe ; bg , fe ; c ; dg and fc ; dg . When we start pruning the terms b and d of the next higher support = 4 , ie , the four nodes of thick lines in Figure 2(c ) , we flnd that d4 will not be covered by any node since its parent is N ull . Thus , we stop the pruning procedure here , and the maximal sup thr that covers every document is 4 .
Note that we can compute this maximal sup thr without actually pruning the tree but not by searching over the tree from the bottom to flnd the flrst node whose parent is N ull . However , showing the \(cid:176)ow" of document IDs in the tree as sup thr increases helps users understand the relations among the sup thr , the covered documents , and the length of the termset that covers the documents . In addition to the shown example , there are certain subtleties . For instance , suppose that a document set contains very few \outlier" documents that do not share any terms with other documents in the set , then the maximal sup thr becomes very low for mined termsets to cover such outlier documents . In such cases , the document coverage information of Table 212 becomes very useful in determining the proper sup thr . Col
Raw Documents
( 1 ) d1 ( t1:0.7 , t8:0.9 , … ) d2 ( t4:0.3 , t6:0.3 , … )
( 2 )
D = {d1,…,dn}
… dn ( t1:0.7 , t6:0.3 , … )
Topic Directory
{t6} : d2 , …
{t6 t4} : d9 , …
…
…
{t6 t4 t8} : d5 , …
( 4 )
( 3 )
Initial Clusters
{t1 t8} : d1 , … {t4 t6 t8} : d2 , d5 , … …
{t6 t8} : d3 , d7 , …
Figure 1 : Framework
Every text clustering method preprocesses documents in several steps , such as removing stopwords ( ie , \I" , \am" , \and" ) and word stemming ( ie , merging the same words of difierent forms like \term" and \terms" ) . After we preprocess the raw documents ( Step ( 1 ) in Figure 2 ) , each document can be represented as a vector of the weighted term frequencies , ie , term frequency £ inverse document frequency ( TFIDF ) , which the information retrieval community calls a vector space model . Our algorithm applies TFIDF . However , in our running examples , we will simply use TF for better understanding .
Starting from this vector space model , we construct FTtree to mine closed termsets and then construct the initial clusters ( Step ( 2) ) . Note that termsets are found based on word presence not on the TFIDF . After that , we construct the initial clusters from the FT tree ( Step ( 3) ) , which can be done without scanning the TFIDF vectors . The initial clusters are a list of a frequent closed termset with the documents that contain the termset . So , the documents are duplicated in multiple clusters within the initial clusters . When we construct the flnal topic directory with maximally max dup number of document duplications ( Step ( 4) ) , we use the original TFIDF vectors to trim the duplication from the initial clusters . 2.1 Nonparametric Closed Termset Mining for
Document Clustering
2
ID d1 d2 d3 d4 d5 d6 d7 d8 d9 d10 termset a , b , e , f a , c , d ordered e , b , a , f c , d , a e b , f e b , f a , c , d , e e , c , d , a c , d c , d c , e b , e b , e c , d c , d e , c e , b e , b
Term
Link e:6 c:5 b:4 d:4 a:3 f:2
Document set D b:3
{d9 d10} a:1 f:1 {d1} c:2 {d8} d:1 a:1 {d5}
( a )
Null e:6 {d3} c:3 b:1 {d4} c:2 {d8} d:1 a:1 {d5} d:3
{d6 d7} a:1 {d2} e:6 {d3} b:3
{d1 d9 d10}
Null c:2 {d8} d:1 {d5} c:3 b:1 {d4} d:3
{d2 d6 d7}
Term
Link e:6 c:5 b:4 d:4
( b )
( c )
Null e:6 {d3} c:3 b:1 f:1 {d4} d:3
{d6 d7} a:1 {d2}
Term
Link e:6 c:5 b:4 d:4 a:3 b:3
{d9 d10} a:1 {d1}
Figure 2 : Determining Support sup
4 5 6 7
Cov 1.0 0.9 0.6 0.0
Not Covered Doc . d4 d2 d6 d7 d1 d3 d5 d8 d9 d10
Cluster < e > < c > < b > < cd > fd2 d5 d6 d7 g
Doc . IDs fd1 d3 d5 d8 d9 d10g fd2 d5 d6 d7 d8g fd1 d4 d9 d10 g
† Input : frequent closed termsets
† Output : initial clusters ( pairs of termset and document
IDs )
Table 1 : Coverage table
Table 2 : Initial clusters
Method :
† for each closed termset T umn \Coverage" in the table denotes the portion of documents that is covered by the corresponding sup thr . Column \Not Covered Doc . IDs" denotes the actual document IDs that are not covered by the sup thr . This coverage table can be e–ciently generated from the FT tree before mining frequent terms .
213 Mining Closed Termsets from FT tree
As noted in [ 8 ] , mining frequent closed termsets can lead to orders of magnitude smaller result termsets than mining frequent termsets while retaining the completeness , ie , from the concise result set , it is straightforward to generate all the frequent termsets with accurate support counts . Closed termsets are meaningful for constructing a topic directory since non closed termsets are always covered by closed sets .
Since FT tree subsumes FP tree , we can simply apply the most recent closed itemset mining algorithm CLOSET+ [ 8 ] on an FT tree . Running CLOSET+ on the FT tree of Figure 2(c ) with sup thr = 4 generates closed termsets : < e > , < c > , < b > , < cd > . 2.2 Constructing Initial Clusters
For each frequent closed termset , we construct an initial cluster to contain all the documents that contain the itemset . Initial clusters are not disjoint because one document may contain several termsets . We will restrain the maximal number of duplications of each document in the clusters in Section 23 The termset of each cluster is the cluster label { identity of each cluster . Cluster labels also specify the set containment relationship of the hierarchical structure in topic directory .
Using FT tree , we do not need to scan the documents to construct the initial clusters while the previous methods [ 5 ] do . Document IDs are included in a FT tree . To retrieve all the documents containing a closed termset , we need to flnd all the paths containing the termset ; the document IDs below the paths are all the documents containing the termset . Figure 2.2 describes the method and rationale for retrieving the document IDs for each closed termset to construct initial clusters . Table 212 shows the initial clusters constructed from the FT tree of our running example ( Figure 2(c) ) . 2.3 Topic Directory Construction
{ for each node t in the sidelink of the last term of T from the header table
⁄ if the path from the root to t contains the termset T , assign to the termset with the document IDs in and below t
Figure 3 : Constructing initial clusters from FT tree
After initial clusters are constructed , Step ( 4 ) builds a topic directory from the initial clusters and the TFIDF vectors . Before building the topic directory , we prune the directory by ( 1 ) removing \inner termsets" ( Section 231 ) and ( 2 ) constraining the maximal number of document duplication ( Section 232 ) After that , a topic directory is constructed ( Section 233 ) and the flrst level nodes are flnally merged ( Section 234 )
231 Removing Inner Termsets
Doc d1 d2 d3 d4 d5 d6 d7 d8 d9 d10
Cluster Labels < e > ; < b > ( < c > ) ; < cd > < e > < b > < e > ; ( < c > ) ; < cd > ( < c > ) ; < cd > ( < c > ) ; < cd > < e > ; < c > < e > ; < b > < e > ; < b >
Table 3 : Clusters for termsets each document . within parentheses are inner termsets
Table 4 : Topic directory
If multiple nodes in the same path in a directory contain the same documents , to minimize the document redundancy , we only leave the one in the lowest node and remove the others . This is done by removing \inner termsets" { among frequent closed termsets , the termsets whose superset exists in the same document , eg , in Table 231 , termset < c > in document d2 is an inner termset as its superset < cd > also exists in d2 . Removing inner termsets will not cause an empty node in the directory and will not afiect the clustering quality .
3
232 Constraining Document Duplication
We allow the user to set the maximal number of duplication max dup of each document in the directory . By allowing the directory to be a graph and max dup ‚ 1 , our method naturally supports soft clustering , which is necessary for many applications ( eg , Yahoo directory ) because a document can belong to multiple clusters . We refer to the original TFIDF vectors to exclude inferior nodes for each document by applying a heuristic score function such as score(d ; T ) = Pt2T d £ t where d £ t denotes the vector of term t in document d . 233 Constructing Topic Directory
Constructing a topic directory from the document cluster list , eg , Table 231 with max dup = 2 , can be done in a top down way .
† Input : nodes ( termsets ) , document cluster list
† Output : topic directory
Main :
† for m = 1 to maximal length of nodes
{ for node of length = m
⁄ link(node , m )
† connect document IDs to corresponding nodes using the document cluster list link(node , m ) :
† if m = 0 , then link node to root , else :
{ if there exist inner nodes of length m ¡ 1 , then link the node to them as a child , else link(node , m 1 )
Figure 4 : Constructing topic directory
We start building a directory from the root : link the nodes of length one at the flrst level , and link the nodes of larger length to their inner nodes as children nodes . Figure 233 describes the method of constructing a topic directory . The topic directory from Table 231 , ie , max dup = 2 , is shown in Figure 231 234 Merging the First Level Nodes
Common mining algorithms usually generate a large number of frequent termsets of length one . Thus , a clustering method based on frequent termset mining tends to generate a lot of flrst level nodes , in which merging the flrst level nodes helps to provide users with more comprehensible interface . We merge the nodes of high similarity by creating a higher level node between the root and the similar nodes until the total number of the flrst level nodes becomes less than or equal to a user specifled number . We use a heuristic similarity function as follows : sim(n1 ; n2 ) =
# of common documents in n1 and n2
# of documents in n1 and n2
3 . EXPERIMENT
We compare our method with other recent document clustering methods { agglomerative UPGMA [ 4 ] , bisecting kmeans [ 7 , 3 ] , and those using frequent itemset mining {
4
FIHC [ 5 ] , HFTC [ 2 ] . We used the same evaluation method and the same datasets as used in [ 5 ] except that , for Reuters , we do not exclude the articles assigned to multiple categories . Due to space limitations , we report the main results and leave the details to a technical report .
305 Performance comparison
Dataset
# of clus
Hitech
Re0
Wap
Classic4
Reuters
3 15 30 60
Ave .
3 15 30 60
Ave .
3 15 30 60
Ave .
3 15 30 60
Ave .
3 15 30 60
Ave .
TDC 0.57
0.52
0.48
0.44
0.50
0.57
0.51
0.47
0.41
0.49
0.47 0.45 0.43 0.41 0.44 0.61 0.53 0.48 0.41 0.50 0.46
0.45
0.42
0.40
0.43
FIHC 0.45 0.42 0.41 0.41 0.42 0.53 0.45 0.43 0.38 0.45 0.40 0.56 0.57 0.55 0.52 0.62 0.52 0.52
0.51
0.54
0.37 0.40 0.40 0.39 0.39
Bi k means
UPGMA
0.54 0.44 0.29 0.21 0.37 0.34 0.38 0.38 0.28 0.34 0.40 0.57 0.44 0.37 0.45 0.59 0.46 0.43 0.27 0.44 0.40 0.34 0.31 0.26 0.33
0.33 0.33 0.47 0.40 0.38 0.36 0.47 0.42 0.34 0.40 0.39 0.49 0.58
0.59
0.51
£ £ £ £ £ £ £ £ £ £
Table 5 : F measure comparison . # of clus : # of clusters ; £ : not scalable to run
Table 305 shows the overall performance of the four methods on the flve data sets . TDC outperforms the other methods on data sets Hitech , Re0 , and Reuters , and shows similar performance to FIHC for others . Table 305 shows the sup thr of coverage = 1:0 determined for each data set . sup thr
363/2301
138/1504
333/1560
Hitech
Re0
Wap
Classic4 70/7094
Reuters
174/10802
Table 6 : sup thr of coverage = 1:0 # of total document in each data set is within the parentheses .
4 . REFERENCES [ 1 ] R . Agrawal and R . Srikant . Fast algorithms for mining association rules . In Proc . Int . Conf . Very Large Databases ( VLDB’94 ) , pages 487{499 , 1994 .
[ 2 ] F . Beil , M . Ester , and X . Xu . Frequent term based text clustering . In Proc . ACM SIGKDD Int . Conf . Knowledge Discovery and Data Mining ( KDD’02 ) , pages 436{442 , 2002 .
[ 3 ] D . R . Cutting , D . R . Karger , J . O . Pedersen , and J . W .
Tukey . Scatter/gather : A cluster based approach to browsing large document collections . In Proc . ACM SIGIR Int . Conf . Information Retrieval ( SIGIR’92 ) , pages 318{329 , 1992 .
[ 4 ] R . C . Dubes and A . K . Jain , editors . Algorithms for clustering data . Prentice Hall , 1998 .
[ 5 ] B . C . M . Fung , K . Wang , and M . Ester . Herarchical document clustering using frequent itemsets . In SIAM Int . Conf . Data Mining , 2003 .
[ 6 ] J . Han , J . Pei , and Y . Yin . Mining frequent patterns without candidate generations . In Proc . ACM SIGMOD Int . Conf . Management of Data ( SIGMOD’00 ) , 2000 .
[ 7 ] M . Steinbach , G . Karypis , and V . Kumar . A comparison of document clustering techiniques . In KDD Workshop on Text Mining , 2000 .
[ 8 ] J . Wang , J . Han , and J . Pei . CLOSET+ : Searching for the best strategies for mining frequent closed itemsets . In Proc . ACM SIGKDD Int . Conf . Knowledge Discovery and Data Mining ( KDD’03 ) , pages 236{245 , 2003 .
