Semi Supervised Mixture of Experts Classification
Grigoris Karakoulas
Department of Computer Science
University of Toronto
Toronto , Ontario , Canada M5S 1A4
Ruslan Salakhutdinov
Department of Computer Science
University of Toronto
Toronto , Ontario , Canada M5S 1A4 grigoris@cstorontoedu rsalakhu@cstorontoedu
Abstract decomposition
We introduce a mixture of experts technique that is a generalization of mixture modeling techniques previously suggested for semi supervised learning . We apply the bias variance semi supervised classification and use the decomposition to study the effects from adding unlabeled data when learning a mixture model . Our empirical results indicate that the biggest gain from adding unlabeled data comes from the reduction of the model variance , whereas the behavior of the bias error term heavily depends on the correctness of the underlying model assumptions . to
1 . Introduction
In many application domains , such as information filtering , credit scoring , customer marketing and drug design , the cost of assigning labels to the input data can be very expensive . The machine learning challenge in this scenario is to build a learning algorithm that can make effective use of unlabeled data by improving classification accuracy . For this reason , researchers have used the idea of applying mixture modeling and EMbased learning for combining labeled and unlabeled data [ 12 , 13 , 14 ] . However , the resulting techniques seem to be disparate as their formulation is intertwined with specific application contexts .
At the same time , due to some reports of degradation in classification performance from the addition of unlabeled data , researchers have tried to analyze the performance of semi supervised learning with mixture models . Zhang and Oles [ 15 ] and Cozman et al . [ 3 ] have studied this using asymptotic analysis that offers limited practical guidance in real world applications .
Thus , the purpose of this paper is two fold : ( i ) to develop a semi supervised mixture modeling technique that is more robust than existing mixture modeling techniques when adding unlabeled data ; and ( ii ) to study the effect of unlabeled data on learning mixture models . As part of our work for ( i ) we introduce a unifying mixture model framework for semi supervised learning . Under this framework we study several variations for modeling a probabilistic classifier and draw connections with existing work . For ( ii ) we apply the analysis of biasvariance decomposition learning . Using this analysis we aim to provide finite sample results for answering the question of when and why unlabeled data can help or hurt performance , particularly with respect to bias and variance . to semi supervised
In Section 3 we define
In Section 2 we provide the framework of mixture modeling . the different mechanisms that can explain labels being missing and show how unlabeled data can be incorporated into this modeling framework . In Section 4 we present the mixture model variants for semi supervised learning , including the mixture of experts ( MoE ) model that we propose . In Section 5 we describe EM based learning for MoE . In Section 6 we present a procedure for estimating the biasvariance decomposition in semi supervised classification . In Section 7 we analyze the empirical results from studying the effects of unlabeled data on the mixture model variants . The paper concludes with a discussion .
2 . Mixture modeling background
K
Consider a classification problem with K classes ky , ,,2,1 k . In our general framework of learning with labeled and unlabeled data , we assume that the training set consists of two subsets : , where X l X and
X is the labeled subset , is the unlabeled subset . x 2 x , l XX
) , ,
L }
, ,
( ) ,
)}
{
} y x y x
(
,
L
,
2
, u
{( yx , 1 1 x { We define u ffi L 1 ix , ffi L 2
,,2,1 ffi UL ffi UL i
, to be a d dimensional feature vector . We also note that in many application lX is very small and the domains , the labeled set unlabeled set uX is generally much larger . that
Suppose the data was generated by M components , and that these components can be well , approximated by the densities j(cid:84 ) being the corresponding parameter vector . The feature vectors are then generated according to the density :
,,2,1
( cid:84 ) , ) , jxp with
M
( j
| j xp
(
( cid:84 ) )
|
M ( cid:166 ) j 1 j jxp
(
|
( cid:84 ) , j
)
( 1 )
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE where j
)( jp are the mixing proportions of the components that have to sum to one . Thus , we can express the generation process of a pair that is x
)
( i y , i fully observed by obtaining the joint distribution of ix and iy : xp
(
, i y i
)
|
M ( cid:166 ) j 1 xjp
,(
, i y i
)
|
M
( cid:166 ) j j 1
( xp i
| j
( cid:84 ) , j
)
( yp i
| xj , i
, j
)
( 2 ) where denotes the whole set of model parameters .
To apply this mixture model to solving a supervised classification problem , we need to compute the posterior probability of the class label given a feature vector : yp
( i
| x i
,
)
M ( cid:166 ) j 1
( cid:170 ) ( cid:171 ) ( cid:171 ) ( cid:172 )
| xjp
(
M ( cid:166 ) j 1 xp ( i j
M ( cid:166 ) l l 1
| j
( xp i
)
, i yp
( i
| x i
, j
, j
)
( cid:84 ) ) , j ( cid:84 ) , l l
|
( yp i
| jx i
,
, j
)
)
( cid:186 ) ( cid:187 ) ( cid:187 ) ( cid:188 )
( 3 )
Equation ( 3 ) shows that the posterior probabilities of the model have a mixture of expert structure , originally introduced in [ 9 ] . In such structure , several local experts model the input dependent distribution of the output in small regions of the input space . The output of the mixture model is computed via a gating network . The latter probabilistically combines the estimates of each local expert . In our case , the units of the gating network ( cid:84 ) , and correspond to component probabilities , . ) , j j , . In general , a local expert could be trained
Each of the local experts is parameterized by vector the local experts are represented by ixjp (
,,2,1 yp
M x
(
) j j
,
|
| j i i with any parametric or non parametric probabilistic classification algorithm , ie logistic regression , Naive Bayes , etc . In this work we will focus on using Naive Bayes for each local expert .
The above mixture of experts model applies to the case of supervised classification . In Sections 3 and 4 we show how this modeling framework can be extended to handle unlabeled data for semi supervised learning .
3 . Incorporating unlabeled data into mixture models
The key idea is to estimate a mixture model from labeled and unlabeled data by modeling the missing labels through a hidden variable , in addition to the hidden variable that is used for modeling the components in the standard supervised setting . To pursue this idea , we first need label three different missing to define the mechanisms that can explain why data may occur as unlabeled .
However ,
Under the first mechanism , the class label is missing completely at random ( MCAR ) if the probability that y is labeled does not depend on the value of y or x , ie P(labeled=1|x,y)=P(labeled=1 ) . In this case one does not need to explicitly model the missing label mechanism within the mixture . Under the second mechanism , the class label is missing at random ( MAR ) if labeling depends on x but it does not depend on y given x , ie P(labeled=1|x,y)=P(labeled=1|x ) . the conditional distribution of x given y is not the same in the labeled and unlabeled data , ie there is a bias in the labeled set compared to the data distribution in the population . In generative modeling techniques that aim to learn such conditional distributions one needs to correct for this bias by incorporating unlabeled data . Under the third mechanism , the class label is missing not at random ( MNAR ) if labeling depends on y , even when conditioned ie P(labeled=1|x,y)(cid:143)P(labeled=0|x,y ) . This on introduces a sample selection bias [ 7 ] for any modeling technique . The bias has to be corrected by modeling the underlying missing mechanism . This is the most difficult case of unlabeled data amongst the three . Correcting for this bias remains an open research issue . In the following we assume that unlabeled data are of MAR type . x ,
To estimate a semi supervised mixture model for MAR data we define the joint data likelihood that incorporates both labeled and unlabeled data [ 15 ] , ie
DP
(
|
)
(
( cid:150 ) ( cid:143 ) yx ) i
, i
M
( cid:166 ) j l j 1
X
( xp i
| j
( cid:84 ) , j
)
( yp i
| x i
, j
, j
) ffi
M
( cid:150 ) ( cid:166 ) u ( cid:143 ) j 1 i Xx xp
( i j
| j
( cid:84 ) , j
)
( 4 )
Instead of maximizing the above quantity , we will function log likelihood work with
L
D
)
(
| the P ( joint data |
D
) log
. Thus , we obtain from ( 4 ) :
( cid:166 ) ( cid:143 ) yx )
, i i
L
(
|
D
)
( log
X l
M
( cid:166 ) j j 1
( xp i
| j
( cid:84 ) , j
)
( yp i
| jx i
,
, j
) ffi
( cid:166 ) ( cid:143 ) i Xx u log
M
( cid:166 ) j 1 xp
( i j
| j
( cid:84 ) , j
)
( 5 )
"supervised" term , which is derived from
In equation ( 5 ) the likelihood function contains a lX labeled uX data , and an "unsupervised" term , which is based on unlabeled data .
4 . Semi supervised mixture variations
In this section we present three different variations for modeling the probabilistic classifier in the mixture of experts framework . The variations differ in terms of how they model the conditional probability
. ) ,
|
( yp i jx i
, j
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE
41 Common components mixture the applying
Consider following simplistic assumption : the posterior probability of the class label is conditionally independent of the feature vector given the mixture words in equation ( 5 ) . This model x yp component , j ) ) other
, yp in
(
( j
,
|
| i i j i essentially makes the assumption that class conditional densities are modeled via a mixture having common components for the classes [ 6,12 ] .
42 Separate components mixture
Another approach to modeling
( yp i
| jx i
, is ,
) j through the separate component ( SC ) mixture model : each class conditional density is modeled by its own mixture components [ 14 ] . In our setting this amounts to constraining in advance the conditional probability table ( CPT ) , . This establishes a deterministic
( cid:143)j }1,0{ )
| yp i (
M mapping between mixture components and classes by restricting components to modeling only one prespecified class . As an example , consider modeling class k M k ( cid:143 ) number of components . Then we can with constrain the conditional probability table by setting yp i for ( kMj ( cid:144 ) . Clearly this restricts the mixture components M k ( cid:143 ) to focus on modeling input feature vectors of M class k only . kMj ( cid:143 )
, and
1 ) yp i ( for jk jk
0
)
|
| text classification using
It is worth pointing out that a well known EM based technique for labeled and unlabeled documents is actually using the SC model , although the technique was approached from a different perspective [ 13 ] . In that work , the complete set of model parameters is a set of mixture components for estimating ( cid:84 ) , parameterized by multinomials , and prior jxp , j over those multinomials . Furthermore , ( cid:143)j }1,0{ ) conditional probability probabilities table the
(
)
|
| j yp i ( represents a predetermined , deterministic mapping between mixture components and classes .
43 Mixture of experts with Naïve Bayes as a local expert
In the previous sections we made a simplistic experts : . In this section we abandon assumption with yp , respect local the yp to x
)
(
)
( j j
,
|
| i i j i
For the sake of simplicity and without restricting generality , let us assume that feature vectors are discrete . values . We Consider each attribute taking on make the standard Naive Bayes assumption that each feature of the input feature vector is conditionally independent of all the other features :
,,2,1
V v yxp
,(
)
| yp
(
)
|
D ( cid:150 ) d 1 d xp
(
) , y
|
( 6 )
We cdp define conditional probability table xp d ( yv
| c
)
, which is the probability that a dx takes on value v given value c for the class . feature The idea is that each local , Naïve Bayes expert will specialize in the region of the input space defined by the mixture components . The probability of a class label given the input feature vector and a mixture component takes the form : yp
( i
| x i
,
*
, j
) yp
( i
*
|
)
D ( cid:150 ) d 1 xp
( d i
| y i
,
*
, j
)
( 7 ) features
The conjugate prior of the multinomial distribution of distribution , the , where the hyper parameter , ( cid:74 ) , is ( cid:74 ) ) (
Dirichlet
~)(
1 v p
V ( cid:150 ) v 1 p affects the strength of the prior . For the experiments 2(cid:74 ) . This essentially reported in this paper , we set amounts to Laplace smoothing .
5 . Semi supervised training using EM
To perform maximum likelihood estimation of the joint data log likelihood ( equation ( 5) ) , we apply the EM algorithm . The latter alternates between estimating missing information ( hidden ) variables given the current model and refitting the model given the estimated , complete data .
In our problem there is missing information regarding the class labels as well as the mixture components . Thus , we consider an M for each feature vector
{
)
, x ( cid:143 ) dimensional binary vector l XX )(xz u that indicates which component generated x . Also , for each unlabeled feature we introduce a K dimensional binary vector
{ uX , a second hidden variable , which indicates x ( cid:143 ) )(xy vector
} the class label of x . The complete data log likelihood is :
( cid:166 ) ( cid:143 ) yx ) i
, i
M ( cid:166 ) z l j 1 log(
X j j x ( i
)
( 8 ) xp
( i
| j
( cid:84 ) , j
) yp
(
| x i
, i j
, j
) )
L
(
|
D
)
( ffi this assumption and consider modeling each local expert via a more powerful model , Naive Bayes , that models the class y as conditionally dependent on the input feature vector ix .
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE
M
K ( cid:166 ) ( cid:166 ) ( cid:166 ) z
( cid:143 ) c 1 1 j i Xx u
( j yx
) i
( x i
) c log( xp
( i j
| j
( cid:84 ) , j
) yp
( i xc
|
, i j
, j
) )
Since each introduced variable
)(xz and
)(xy is unobserved ( missing ) , we can only employ an approximation to these variables via their expected values , and then proceed by maximizing the resulting "expected" complete data likelihood . More specifically , the EM algorithm iterates through two steps , ie ( cid:120)fi E Step : ( cid:120)fi M Step : Set ffi ffi 1
DyzE yzD yz ,( max
,( , arg ffi 1
| )
[ (
P
]
)
(
)
)
1
,
,
| t t t t
In the E step the algorithm estimates probabilistic component labels for the entire datasets as well as probabilistic class labels for the unlabeled data vectors . In the M step it maximizes the expected complete loglikelihood defined in equation ( 8 ) . This iterative process is repeated until some stopping criterion is met . Under certain regularity conditions is guaranteed to find a local optimum . Due to space constraints , the specifics of the EM based training of the three model variants with real valued or discrete data are given in a longer version of this paper [ 10 ] . the EM algorithm
6 . Bias variance decomposition and the value of unlabeled data in that
[ 12,13 ] have empirically shown
The question of whether unlabeled data can help generalization performance has received growing interest in specific application contexts . For example , the authors in text classification unlabeled data can result to better model posterior class probabilities by improving estimation of the mixture density . At the same time , a few researchers [ 3,15 ] have studied this question by analyzing the asymptotic behavior of maximum likelihood estimators in semi supervised learning . In [ 15 ] , Zhang and Oles use the Cramér Rao bound estimated from the Fisher information matrix to show that under the assumption of unbiased estimators unlabeled data always help . In [ 3 ] , Cozman et al . , have also used analysis of asymptotic behavior in order the phenomenon of performance degradation from unlabeled data in terms of asymptotic bias in the estimators . However , this asymptotic analysis provides real world applications , as the labeled set Xl tends to be small . limited practical guidance to explain in
Therefore , to analyze the value of unlabeled data across different application contexts we introduce the bias plus variance decomposition in semi supervised learning . In general , the bias variance decomposition forms a powerful the performance of supervised learning algorithms [ 5 ] . There are a few approaches proposed in the literature for the bias variance decomposition in supervised classification . We use the decomposition in [ 11 ] for the zero one ( misclassification ) loss function , and apply this decomposition to semi for analyzing tool supervised classification as explained below . Since we use the mixture models for classification , we analyze the behavior of the models with respect to the zero one loss function . We leave for future work the bias variance decomposition of the log likelihood loss as in [ 8 ] .
Let YH be the random variable that represents the label of the observed feature vector in the hypothesis space , and YF be the random variable that represents the label in the target function . Then : ffi
Error
( 9 ) bias xP ffi
)
(
2(cid:86 ) )( x var x
2 x
( cid:166 ) x 1
2
1
2
1
2
2 ( cid:86 ) x bias
2 x var x
1(
( cid:166 ) [ ( cid:143)Yy
1(
YP
(
F
( cid:166 ) ( cid:143)Yy xy
|
2 ) )
YP
(
F xy
|
)
YP
(
H xy
|
2 ) ]
YP
(
H
( cid:166 ) ( cid:143)Yy xy
|
2 ) ) where ( cid:305)2 is the intrinsic "target noise" , which is the expected error of the Bayes optimal classifier .
We apply the two stage sampling procedure described in [ 11 ] to the semi supervised setting . This procedure can help us in studying the effect of the unlabeled data on the learning algorithm . Note that in practice , it is impossible to estimate the intrinsic noise . However , this procedure estimates the bias term that includes the intrinsic noise .
First , the data is split into the test and training sets . From the training set D , we sample a small labeled set L . This labeled set remains fixed throughout the procedure . The remaining set constitutes large unlabeled set U . To empirically generate the bias and variance terms : the relatively
( cid:120)fi Uniformly sample repeatedly without replacement m examples from the unlabeled set U , holding the labeled set L fixed . To get better error estimates , we repeat this process N times , forming N training sets ( L,Un ) , n=1,2,,N We set N=20 .
( cid:120)fi Apply the semi supervised learning algorithm on each of the training sets and estimate the terms in equation ( 9 ) , by applying each resulting classifier to the test set .
In many real world applications , one would typically have small , fixed amount of labeled data , and great abundance of unlabeled data . Thus , to estimate the effect of the unlabeled data on the bias variance decomposition , we increase the size of the sampled unlabeled set , while holding the labeled set fixed . If we increased the labeled and unlabeled data at the same time then we would not be able to distinguish the effects of the unlabeled data from that of the labeled data . This is different from the original method for supervised learning in [ 11 ] where the size of the labeled set is increased .
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE
7 . Empirical analysis
Our experiments aim to address the following two questions . Given a small labeled set : ( i )
Are unlabeled data useful for the MoE learner compared to the CC and SC learners ? Do they help in reducing the variance of each learner , or do they introduce additional bias ?
( ii )
To find answers to these questions we conducted a series of experiments across four well known datasets : Adult , Satimage and SpamBase from the UCI repository [ 1 ] , and the web categorization dataset , WebKB , [ 4 ] . We also used two synthetic Artificial datasets to study the effect of incorrect model assumptions on the bias and variance terms .
71 Dataset description
The details of the datasets are provided in Table 1 . For the Satimage dataset we transformed the original dataset from 6 classes to a binary classification problem by merging the first 5 classes into a single class . The WebKB dataset consists of web pages gathered from university computer science departments , where each web page represents one of four categories : course , student , faculty , and project . The original dataset contains more categories , however these four are the most populated ones . We then transformed the dataset into "course" and "non course" target values . For our experiments we used preprocessed WebKB dataset1 . We formed the feature set by selecting the 200 most informative features ( word counts ) as measured by information gain .
Table 1 . Description of the datasets
Name
Training
Test Set
Nom/Cont
Adult
SpamBase Satimage WebKB
Artificial I,II
Set
32561 3101 4435 3199 8000
16281 1500 2000 1000 4000
Attrs 8/6 0/57 0/36 0/200 0/30
We built two artificial datasets by controlling the degree of independence and relevance amongst features . These two characteristics can affect the bias of the mixture models and Naïve Bayes . More specifically , the Artificial I and II datasets consist of 30 continuous features , out of which in Artificial I 30 % ( =9 ) are independent with 80 % ( =7 ) of those being relevant , whereas in Artificial II 80 % ( =24 ) are independent with
1 The preprocessed dataset is available at http://wwwcstechnionacil/~ronb/thesishtml
80 % ( =19 ) of those being relevant . All independent features are drawn from N(0,1 ) . Random noise is added to all the features from N(0,.1 ) , and then the features are rescaled and shifted randomly . The relevant features are centered and rescaled to a standard deviation of 1 . The binary class labels are assigned according to a random weight vector using only the relevant features . The class ratio in these two datasets is 80/20 .
Depending on the value type of the features in a given dataset we used mixtures of gaussians or mixtures of multinomials . To apply Naïve Bayes in the MoE model we pre descretized the continuous features of each dataset into 20th percentiles . In general one could use crossvalidation to tune the number of components . However , given the number of experiments reported here it would not have been practical fine tuning . Furthermore , our goal in this paper is to provide insight into learning from labeled and unlabeled data with mixture models , rather than to find out which technique performs the best . Thus , for each mixture technique we trained models with 10 and 30 components in order to also analyze the effects of model complexity . We initialized each mixture model with k means then trained five mixture models with different random seeds and selected the model with the highest log likelihood in the training data . to do such
72 Results
Table 2 presents the summary of the results across the four real world datasets , as we increase the pool of unlabeled data and the complexity of the mixture models , ie number of mixture components , from 10 to 30 . The first column of the results shows the error from supervised learning , ie no unlabeled data , |Xu|=0 . For the latter type of learning we also report performance of the Naive Bayes model for benchmarking pursposes .
In three out of the four datasets , Adult being the exception , unlabeled data helped across all mixture models . For those three datasets more unlabeled data help , with the biggest improvement noticed for the semisupervised MoE model over its supervised counterpart . This is because the MoE model has higher complexity than the other two mixture models and the unlabeled data act as a regularizer to prevent MoE from overfitting by reducing the variance term of the error . In fact in those three datasets unlabeled data always help the MoE model . The SC model seems to be more sensitive to the amount of unlabeled data than MoE . In the Satimage dataset more unlabeled data hurt SC's performance . Even in the Adult dataset where all mixture models show deterioration with unlabeled data MoE shows the least deterioration . The CC model is not so sensitive as the SC model . It is , however , lagging performance compared to the other two models across all four datasets .
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE
Table 2 . Bias variance decomposition of the semi supervised mixture models as the size of the unlabeled data increases across various datasets . Best performances per dataset and number of components are shown in bold .
|Xu|=0 Err
|Xu|=50 Bias Var
|Xu|=200
|Xu|=800
|Xu|=2000
Err
Bias Var
Err
Bias Var
Err
Bias Var
Err
|Xl|=100 ADULT , M=10 , Mixture of Multinomials NB CC SC
0.21 0.22 0.20 0.20
0.22 0.20 0.20
0.12 0.13 0.13
0.10 0.08 0.07
0.22 0.21 0.20
0.21 0.21 0.20
MoE M=30 CC SC MoE SPAMBASE , M=10 , Mixture of Multinomials NB CC SC
0.10 0.08 0.07
0.12 0.13 0.13
0.21 0.21 0.20
M=30
MoE
CC SC
0.17 0.10 0.12
0.16 0.11 0.13
0.07 0.07 0.09
0.08 0.07 0.08
0.09 0.03 0.04
0.08 0.04 0.05
0.22 0.21 0.20
0.15 0.10 0.11
0.14 0.10 0.12
0.11 0.13 0.15
0.11 0.08 0.06
0.23 0.24 0.21
0.12 0.17 0.16
0.11 0.07 0.05
0.24 0.26 0.21
0.12 0.20 0.17
0.12 0.06 0.04
0.12 0.14 0.15
0.10 0.08 0.06
0.21 0.23 0.21
0.12 0.15 0.16
0.09 0.07 0.05
0.22 0.26 0.21
0.12 0.21 0.18
0.10 0.05 0.04
0.07 0.07 0.06
0.07 0.07 0.06
0.07 0.09
0.08
0.07 0.11
0.07
0.15 0.17 0.13
0.14 0.15 0.14
0.08 0.03 0.05
0.08 0.04 0.06
0.07 0.06
0.03
0.04 0.05
0.04
0.11 0.04 0.10
0.10 0.04 0.07
0.14 0.10 0.08
0.14 0.11 0.11
0.13 0.16
0.11
0.11 0.17
0.12
0.26 0.22 0.22
0.22 0.20 0.21
0.06 0.07 0.06
0.06 0.07 0.07
0.07 0.09
0.08
0.07 0.12
0.08
0.15 0.18 0.13
0.13 0.16 0.16
0.07 0.03 0.02
0.07 0.04 0.04
0.06 0.06
0.02
0.04 0.05
0.04
0.11 0.03 0.09
0.09 0.03 0.05
0.12 0.09 0.08
0.12 0.09 0.09
0.13 0.21
0.10
0.11 0.18
0.11
0.06 0.07 0.07
0.06 0.07 0.06
0.07 0.13
0.09
0.07 0.14
0.09
0.06 0.02 0.01
0.06 0.03 0.03
0.06 0.07
0.01
0.03 0.05
0.02
0.13 0.17 0.10 0.14
0.16 0.11 0.14
0.14 0.12 0.10
0.12
SC MoE M=30 0.12 CC 0.11
MoE SATIMAGE , M=10 , Mixture of Multinomials NB CC
0.13 0.10
0.07 0.06
0.06 0.04
0.13 0.15
0.12
0.08
0.04
0.11
0.10 0.10
0.07 0.07
0.04 0.04
0.07
0.12
0.12
SC MoE 0.05 WebKB , M=10 , Mixture of Multinomials 0.30 NB 0.31 CC 0.22 SC 0.27 MoE M=30 0.28 CC 0.23 SC 0.28 MoE
0.11 0.06 0.09
0.12 0.06 0.11
0.25 0.21 0.23
0.27 0.22 0.25
0.14 0.15 0.14
0.15 0.16 0.14
0.11 0.16
0.11
0.26 0.22 0.23
0.24 0.20 0.21
In the SpamBase and WebKB datasets unlabeled data significantly help across all variants of the mixture models , especially the MoE model . This is consistent with the literature on using unlabeled data in text classification .
To understand the reason for the overall positive impact of unlabeled data on MoE we examined the biasvariance decomposition of error in the WebKB and Adult datasets , two markedly different datasets in terms of MoE performance . In addition , we applied MoE to Artificial I and II . As mentioned above , Artificial I represents the type of dataset where the Naive Bayes model assumptions within each local expert are incorrect . Such dataset could help us explain the relatively poor performance of MoE with unlabeled data in Adult .
Figure 1 shows the results of this comparison . For each dataset there are three graphs . The first two are the bias variance decomposition bar charts for 10 and 30 components . The first bar in each chart shows the error from supervised learning of MoE . The third graph depicts the feature correlation matrix of the respective dataset ; the less the correlation the darker the pixel . In the case of Adult we computed the mutual information matrix . In WebKB and Artificial II unlabeled data reduce the error of MoE because they cause the variance term of error to decrease while keeping the bias term almost constant . The higher the complexity of MoE is , the bigger the variance reduction and , hence , the error reduction . In contrast , in Adult and Artificial I the bias term increases as more unlabeled data are added even though variance decreases .
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE
WebKB
Adult
Artificial I
Artificial II
Figure 1 . Bias variance decomposition of the semi supervised mixture of experts classifier for 10 mixture components ( left column ) and 30 mixture components ( middle column ) , as the size of the unlabeled set increases , for four datasets : WebKB , Adult , Artificial I and Artificial II . The plots on the right column show correlation matrices of the feature vectors conditioned on class 1 .
In the case of Adult the variance decrease is not enough to fully offset the increase in the bias term and this leads to a slight increase in error . This increase in the bias term can be attributed to incorrect model assumptions for the local experts as implied by the similar structure of the correlation matrix in these two datasets . Cozman et al . [ 3 ] have used asymptotic analysis to also argue that bias may be adversely affected by unlabeled data . In this regard our analysis has provided finite sample results . In WebKB and
Artificial II the model assumptions do not seem to be violated as much since the corresponding correlation matrices have far less structure .
9 . Conclusions
In this paper we have introduced a semi supervised mixture of experts technique , and provided EM based learning using both labeled and unlabeled data . We
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE showed that many popular EM based approaches to semi supervised learning can be seen as variants of one underlying model – mixture of experts .
Our empirical findings suggest that unlabeled data can help the MoE learner the most compared to the other two mixture model learners , CC and SC . At the same time in cases where addition of unlabeled data can degrade performance of learners , particularly that of SC , MoE seems to be the most robust of the three learners as its performance degrades the least . the mixture
We used the bias variance decomposition to explain the effects of unlabeled data on semi supervised learning . We showed that when unlabeled data help they do so by reducing the variance more than increasing the bias error term . This particularly applies to the MoE learner that exhibits the largest benefit from unlabeled data . It is worth pointing out that although increasing the complexity the supervised MoE learner causes the learner to overfit , the addition of unlabeled data counteracts this overfitting and improves performance of the more complex semisupervised MoE model , even relative to the less complex supervised MoE model . Thus , unlabeled data seem to be acting as a regularizer by preventing MoE to overfit .
( number of components ) of
We also showed that the behavior of the bias error term heavily depends on the correctness of model assumptions : due to incorrect model assumptions , unlabeled data can potentially introduce additional bias by inferring missing class labels and thus degrade the performance of the model .
There is work underway to extend the empirical evaluation of semi supervised MoE to more datasets , including multi class ones . There are also two streams of future work . The first one is to apply the biasvariance decomposition for analyzing the performance of other semi supervised learning techniques . The second one is to take advantage of the regularization effect of unlabeled data by deciding which area of the example space to select unlabeled data from and by how much depending on the variance of the learner in that area .
10 . References
[ 1 ] CL Blake and CJ Merz , UCI repository of machine learning URL http://wwwicsuciedu/ mlearn/MLRepository.html , 1998 . databases ,
[ 3 ] F . G . Cozman , I . Cohen , and M . Cirelo , “ Semi supervised learning of mixture models and bayesian networks ” , Proceedings of 20th International Conference on Machine Learning , Morgan Kaufmann , Washington DC , 2003 , pp . 99106 .
[ 4 ] M . Craven , D . DiPasquo , D . Freitag , AK McCallum , T . Mitchell , K . Nigam , and S . Slattery . “ Learning to extract symbolic knowledge the World Wide Web ” , Proceedings of AAAI 98 , 15th Conference of the American Association for Artificial Intelligence , AAAI Press , Madison , 1998 , pp . 509 516 . from
[ 5 ] S . Geman , E . Bienenstock and R . Doursat , “ Neural dilemma ” , Neural networks Computation , 1992 , pp . 1 48 . bias/variance and the
[ 6 ] Ghahramani , Z . and MI Jordan , Learning from incomplete data , Technical Report 108 , MIT Center for Biological and Computational Learning , December 1994 .
[ 7 ] J . Heckman , “ Sample selection bias as a specification error ” , Econometrica , 1979 , pp . 153 161 .
[ 8 ] T . Heskes , “ Bias/variance decompositions for likelihoodbased estimators ” , Neural Computation , 1998 , pp . 1425 1433 .
[ 9 ] R . A . Jacobs , M . I . Jordan , S . J . Nowlan , and G . E . Hinton , “ Adaptive mixtures of local experts ” , Neural Computation , 1991 , pp . 79 87 .
[ 10 ] Karakoulas , G and R . Salakhutdinov , Mixture models for classification using both labeled and unlabeled data , Technical Report , Customer Behaviour Analytics , CIBC , Toronto , Canada , 2003 .
[ 11 ] R . Kohavi and D . Wolpert , “ Bias plus variance decomposition for zero one loss functions ” , Proceedings of 13th International Conference on Machine Learning , 1996 , pp . 275 283 .
[ 12 ] D . Miller and S . Uyar . “ A mixture of experts classifier with learning based on both labelled and unlabelled data ” , Advances in Neural Information Processing Systems 9 , MIT Press , 1997 , pp . 571 578 .
[ 13 ] K . Nigam , AK McCallum , S . Thrun , and TM Mitchell , “ Text classification from labeled and unlabeled documents using EM ” , Machine Learning , 2000 , pp . 103 134 .
[ 14 ] B . Shahshahani and D . Landgrebe , “ The effect of unlabeled samples in reducing the small sample size problem and mitigating the Hughes phenomenon ” , IEEE Transactions on Geoscience and Remote Sensing , 1994 , pp . 1087 1095 .
[ 2 ] A . Blum and T . Mitchell , “ Combining labeled and unlabeled data with co training ” , Proceedings of the 11th Annual Conference on Computational Learning Theory ( COLT 98 ) , ACM Press , New York , 1998 , pp . 92 100 .
[ 15 ] T . Zhang and FJ Oles , “ A probability analysis on the value of unlabeled data for classi fication problems ” , Proceedings of 17th International Conference on Machine Learning , Morgan Kaufmann , 2000 , pp . 1191 1198 .
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE
