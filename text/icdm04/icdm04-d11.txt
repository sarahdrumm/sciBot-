Mining Associations by Linear Inequalities
Tsay Young ( ’T . Y.’ ) Lin
Department of Computer Science
San Jose State University San Jose , CA 95192 , USA tylin@cssjsuedu
Abstract
1.1 Basics Terms in Association Mining ( AM )
The main theorem is : Generalized associations of a relational table can be found by a finite set of linear inequalities within polynomial time . It is derived from the following three results , which were established in ICDM0’02 and are re developed here . They are ( 1 ) Isomorphic Theorem : Isomorphic relations have isomorphic patterns . Such an isomorphism classifies relational tables into isomorphic classes . ( 2 ) A variant of the classical bitmaps indexes uniquely exists in each isomorphic class . We take it as the canonical model of the class . ( 3 ) All possible attributes/features can be generated by a generalized procedure of the classical AOG ( attribute oriented generalization ) . Then , ( 4 ) the main theorem for canonical model is established . By isomorphism theorem , we had the final result ( 5 ) .
Keywords : association , deduction , bitmaps feature , granules ,
1 Introduction
Though there is no formal definition of data mining , but its informal version of [ ? ] is rather universal :
, Drawing useful high level information ( patterns , knowledge and etc . ) from data .
In this paper we will attempt to analyze such a informal notion critically on one of the core techniques , namely , association rule mining [ 4 ] . As a byproduct of the analysis , we have the results stated in the abstract .
In AM , two measures , support and confidence , are the criteria . It is well known among researchers , support is the essential one . In other words , high frequency are more important than the confidence of implications . We call them undirected association rules , associations , or high frequency patterns .
Association mining is originated from the market basket data [ 1 ] . However , in many software systems , the data mining tools are applied to relational tables . For definitive , we have the following translations of terms and will use theminterchangeably :
1 . an item is an attribute value ,
2 . a q itemset is a subtuple of length q ,
3 . A q subtuple is a high frequency q pattern or a qassociation , if its occurrences are greater than or equal to a given threshold .
2 Anatomy of Association Mining(AM )
In order to fully understand the mathematical mechanics of AM , we need to understand how the information are developed . First we set up a convention :
, A symbol is a string of ” bit and bytes ; ” it has no real world meaning . A symbol is termed a word , if the intended real world meaning participates in the formal reasoning .
Our methodology is very rigorous , but we take the informal style to explain the rigorous method . We use illustrations , if formal proofs do not give good insight . The main goal is to get the idea cross .
In summary a word is an interpreted symbol and its interpretation is part of reasoning processes . Notations of word and symbol appear to be the same , but their meanings are very different .
2.1 Information Flows of AM
2.3 Interpretations of Patterns
1 . Representation phase : real world a relational table of words . Each symbol ( column names and attribute values ) in the table represents some real world facts ; so we refer to them as words . The representations are incomplete . The semantics of words are not implemented and relied on human support ( traditional data processing professionals ) .
2 . Data Mining Phase : a table of symbols patterns of symbols . In this phase , table of words is used as a table of symbols , because data mining algorithms do not consult human for the semantics of symbols ; they are treated as ” bits and bytes ” in AM algorithm . Patterns , which are algebraic or logic expressions of symbols , are derived mathematically . Briefly , the table of symbols is the only ” axioms , ” and the patterns are the ” theorems . ”
3 . Interpretation Phase : patterns of symbols patterns of words . Patterns are discovered as expressions of symbols in the previous phase . In this phase , those individual symbols are regarded as words again(using the meaning acquired in representation Phase ) .
4 . Realizations Phase : patterns of words real world phenomena . Do patterns of words represent some real world phenomena ?
2.2 Axiomatic Approach to Data Mining
To examine the foundation of each data mining technique , we take the axiomatic approach . We require each technique explicitly specified the following items :
1 . Input data : Any information utilized by data mining algorithms are considered the input . In AM , the input is a table of symbols . However , in clustering techniques the input is the given set of points plus the background knowledge the geometry of the ambient space of the given points .
2 . The logic/reasoning system : AM uses mathematical deduction ( Counting is a very simple deduction ) .
3 . The output patterns : The model of output patterns needs to be specified . The model of the traditional AM is the set of associations . They are ” conjunctions ” of input symbols . We will generalize it to the set of algebraic or logic expressions of symbols .
Such an approach has been called Deductive Data Mining [ 8 ] .
In representation phase , each word , to human , does correspond to a real world phenomenon ; we express it by saying the meaning of the word is the real world phenomenon . Again such meaning is known only to human , not to the system . For systems words are symbols . The patterns are discovered in the form of expressions of symbols . To interpreting them , we convert the expression of symbols to expression of words based on the interpretations established in the representation phase . Any pattern ( expression of symbols ) is said to be an un interpreted pattern , if the transformation of the expression of symbols to the expression of words has not been done . A pattern is un interpretable , if such a transformation cannot be done .
2.4 Realization of Patterns
A pattern is un realizable , if the pattern ( = expression of symbol ) does not correspond to a real world phenomenon . Note that an expression of words is , of course , interpretable , but may not be realizable ; we refer to .
3 Tables of Symbols Understand the input
First , we will examine how the real world is represented :
1 Select a set of attributes , called relational schema .
2 Represent a set of real world entities by a table of words . These words , called attribute values , are meaningful words to human , but their meaning are not implemented in the system .
In traditional data processing ( TDP ) environment , for example , the attribute name , COLOR , means exactly what human thinks . Therefore its possible values are yellow , blue , and etc . More importantly ,
, DBMS processes these data under human commands , and carries out the human perceived semantics .
To stress such view , we call it
3 Computing with words
At the same time , it is equally important to stress that
4 the human views or interpretations of those words are not implemented in the system . They are merely symbols in the system .
In the system , COLOR , yellow , blue , and etc are ” bits and bytes ” without any meaning ; they are pure symbols . Using AI ’s terminology [ 3 ] , those attribute names and values
, fi , ,fi ,fi . .fifi fifi' .fiff .fi ,fi .
. '
) '
)
, , fi , , , fi , , fi . . , fifi fi , fi' . , fiff . , fi , , fi . ,
. , ' , , ) , ' , , , , ) ,
Table 1 . A Relational Table K and its Isomorphic Copy K’
Value , Value
,.fi
, , . , fi ,
. Value . Value ff ff )
, ff , ff ) ,
=Bit Vectors = 11000110 = 00101001 = 00010000 =Bit Vectors = 10010000 = 01001010 = 00100101
=Granules
= .fi fi fi( fi)fi
= .fi , fifi fifi
= fifi =Granules = .fi fi.fi
= .fi fifi fi)fi = .fi , fi( fifi
Table 2 . Words in K and K’ have the same bitmaps and granules
( column names , and elements in the tables ) are the semantic primitives . They are primitives , because they are undefined terms inside the system , yet the symbols do represent ( unimplemented ) human perceived semantics .
3.1 Isomorphic Tables and Patterns this section with obvious , but somewhat Let us start a surprise observation . Intuitively , data is a table of symbols(Section 2.2 ) , so if we change some or all of the symbols , the mathematical structure of the table will not be changed . So its patterns , eg , association rules , will be preserved . Formally , we have the following theorem [ 9 ] :
Theorem 31 terns .
Isomorphic relations have isomorphic pat
We will illustrate the idea by an example . The following example is adopted from ( [6 ] , pp 702 ) :
Example 32 Suppose a relation consists of two attributes , and . , of type integer and string respectively . The current instance has eight tuples . These tuples are knowledge representations of entities . Table 1 illustrates the representations and isomorphism . To illustrate Theorem 31 , let us assume the support is ” two tuples ” and count the items . It is easy to see we have :
1 . 1 assoication in K : , , . , , ) ,
2 . 1 assoication in K’ : , , , . , , , , ) , ,
3 . 2 assoication in K : , and . ) ,
4 . 2 assoication in K’ : , , , and . , ) , . q association rules ( q=1,2 ) are isomorphic in the sense that adding prime ’ to associations in K become associations in K’ ; this illustrate the theorem .
3.2 The Canonical Models
In this section , we will introduce tables of Bitmaps ( TOB ) , Granules(TOG ) , and Granular Data Model(GDM ) . We will illustrate the idea by examples . Let us consider the bitmap indexes for K ( see Table 1 ) the first attributes , F , would have three bit vectors . The first , for value 30 , is 11000110 , because the first , second , sixth , and seventh tuple have =30 ; see Table 2 .
Using Table 2 as the translation table , Table 1 is transformed into bitmap table , Table 3 . It should be obvious that we will have the exact same bitmap table for K’ .
Next , we note that a bit vector can be interpreted as a granule ( subset ) of , . For example , the bit vector , 11000110 , of = 30 represents the subset .fi , fi fiff fifi , similarly , 00101001 , of = 40 represents the subset .fi . fi' fifi . As in the bitmap case , Table 1 is transformed into granular table , Table 4 .
Note that granules forms a partition , and hence induces an equivalence relation , denoted by , ; similarly , we have . Pawlak called the the pair , . , fi knowledge base and note that it is equivalent to table of granule . Since knowledge base often means something else , we have called it granular data model(GDM ) . Now we can summarize these observations in :
Theorem 33
1 . Isomorphic tables have the same canonical model .
2 . The canonical model has three forms , table of granules ( TOG ) , bitmaps ( TOB ) , and granular data model(GDM ) . TOB , TOG , and GDM are isomorphic and regarded as synonyms .
Theorem 34 It is adequate to do AM in one of the canonical model .
, fi , fifi . fifi fi' fiff fi fi
,,fi , ,
. '
) '
)
)
bit
. bit
Table 3 . Tables of Words and Bitmaps ) fl ffi ffi ffi , ffi . ffifi ffi( fi fi
,,fi , ,
. '
) '
)
)
granule
.fi fi fi( fi)fi .fi fi fi( fi)fi
.fi , fifi fifi
fifi
.fi , fifi fifi
.fi fi fi( fi)fi .fi fi fi( fi)fi
.fi , fifi fifi
. granule .fi fi.fi
.fi fifi fi)fi .fi , fi( fifi
.fi fi.fi
.fi fifi fi)fi .fi , fi( fifi .fi fifi fi)fi .fi , fi( fifi
Table 4 . Tables of Words , and Granules : For each attribute , the collection of granules forms a partition on ,
This follows immediately from the isomorphism theorem ; see Theorem 31
, fi , ,fi ,fi . .fifi fifi' .fiff .fi ,fi .
. fiffifi fiffifi fiffifi fiffifi
.
'
) '
)
Table 5 . A Generalized Table GK with a new named attribute
3 . A level two concept is a named ( or interpreted ) equivalence class of first level concepts , in general , are the second innermost relation , and etc .
4 . A n level two concept is a named ( or interpreted ) equivalence class of   level concepts .
5 . These concept hierarchy groups the base concepts into a nested sequence of named partitions of based concepts . For each named partition ( that is , the partition and each equivalence class has a name ) , a new attribute is introduced into the given table .
In this example , attribute values are the base concepts . A named partition is defined : The equivalence class ., fi fi is named , another equivalence class fi fiffifi , and the partition . . This generalization introduces a new named attribute . ( column ) into the given table ( Table 5 . )
4 The Theory of Features in GDM
4.2 AOG on GDM
The notion of generalizations of features/attributes has not been formally defined ; we will examine the classical case and reach our definition ; see Definition41
In traditional concept hierarchy , all partitions are named . To be uniform , we will consider the unnamed case . We will take the following
Let us first examine the well accepted case . Then we will take an obvious extension .
4.1 Attribute Oriented Generalizations(AOG )
Let be the given relational table . In the traditional attribute oriented generalization ( AOG ) , concept hierarchy is introduced by recursively defining a sequence of named equivalence relations on a given single attribute of :
1 . A level zero concept is a base concept(distinct attribute values ) .
2 . A level one concept is a named ( or interpreted ) equiv alence class of base concept ;
, Convention : unnamed partition will be regarded as canonically named , that is , the partition and equivalence classes themselves are their own names .
To illustrate the idea . The newly named . in , will be ” unnamed ” in its GDM . Let us use GDM , . , fi of : the partition , fi fi fifi of attribute induces a new partition on , as follows : From Table 2 , , defines a granule .fi fi fi( fi)fi , fi defines fifi , and . defines .fi , fifi fifi . The new granule is .fi fi fi( fi)fi ff fifi =.fi fi fi . fi( fi)fi , and .fi , fifi fifi . These two new granules define a new partition Table 6 .
The GDM of new . =
, . , fi fi fi . fi( fi)fi .fi , fifi fifififi :
, fi , ,fi ,fi . .fifi fifi' .fiff .fi ,fi .
, fi fi fifi
., fi fi ., fi fi
fi
., fi fi
fi fi
., fi fi
fi
.
'
) '
)
Table 6 . A Generalized Table GK with a uninterpreted attribute
By the convention , a canonically named partition or equivalence class will be referred to as an un interpreted attribute or attribute value . Following the same spirit , a TOG , TOB or GDM is called a un interpreted table or data model .
4.3 The Feature Completion on GDM
Traditional AOG focuses on one attribute .
There are no reasons to stop at considering one attribute , here we will consider a concept hierarchy on a set ff ff .ff , ff ff fi of attributes . As we have observed in Theorem 34 , it is adequate to do AM in GDM is , . In this case , ff is a subset of ; we will denote it bu fi .
Definition 41 A generalization over fi in a GDM is a coarser partition of fi , fi fi , where fi ff . fi , fi fi fi is a non empty subset of .
If we let fi varies through all non empty subsets of , we have all possible generalizations of fi in GDM . We will denote the set of all generalizations by . fi . Observe that the intersection of generalizations is still a generalization . For any given finite set of generalizations , there is the smallest generalization . So . fi is closed under meet ( =the intersection ) and join ( =the smallest generalization ) . So . fi is a lattice . More importantly , the meet and join are the meet and join in the lattice , of partitions on , . Let be the smallest sublattice of , that contains , and be the smallest sublattice of , that contains all coarsening of . Now we have
Theorem 42 = . .
Based on this observation , we define
Definition 43 GDM , ) is called Universal Model of , in the sense it contains all its generalizations . is the feature completion of .
4.4 Intuitive Discussions on Features/attributes
We often hear such an informal statement ” a new feature ( attribute ) is selected , extracted , or constructed from a subset ff ff .ff , ff ff fi of attributes in the table . ” What does such a statement mean ?
First we observe that feature and attribute have been used interchangeably . In the classical data model , an attribute or a feature is a representation of property , characteristic , and etc . ; see eg , [ 15 ] . A feature represents a human perception about the data ; each perception is represented by a symbol , and has been called attribute and the set of attributes a schema . Based on our convention , they are words in TDP ( Section 3 ) , but are symbols in AM .
Let us assume a new feature has bee selected , extracted , or constructed . Let us insert it into the table . The new table is denoted by . The informal statement probably means in the new table , is an attribute . As it is derived from ff , it is functionally depended on ff ; as extraction and construction are informal words , we can use the functional dependency as formal definition of feature selection , extraction and constructions . Formally we define
Definition 441 is a feature derived ( selected , extracted and constructed feature ) from ff , if is functional dependent on ff in the new table .
In Theorem 4.2 , we have shown that is feature completion of . By the convention in Section 4.2 , is uninterpreted feature completeion of .
This theorem is rather anti intuitive . Taking human ’s view there are infinitely many features . But the theorem says there are only finitely many features ( as is a finite set ) . How one can reconcile the contradiction ? Where did the finite ness slip in ? Our analysis says it comes in at the representation phase . We represent the universe by finite words . However , in phase two , suddenly these words are reduced to symbols . Thus the infinite world now is encoded by a finite set of symbols . In particular , features can only be encoded in a finite distinct ways . A common confusing most likely comes from the confusing of data mining and ” facts ” mining .
5 Generalized Associations in GDM
As we have observed that it is adequate to conduct AM in the canonical model , such as GDM .
Main Theorem 51 Let , be the universal model , Let g be a granule in a partition such that ( ( ) . Then g is an un interpreted generalized associations .
Let us define an operation of binary number x and a set S . We write S*x to be defined by The two equations is a union of some granules from the partition ( by the expression ” a granule in ” we mean a granule belonging to one of its partitions . ff , if ff and ffff ff , if ff or fl ff .
Main Theorem 52 Let ff , be the smallest element in . Let , be the granules in . Then the union
, , ff ff is a granule that represents a un interpreted generalized association rule , if its cardinality
( ( ) where s is the threshold .
Remark : The cardinal number of is bounded by the Bell number [ 2 ] of ff ( ( , the cardinal number of ff , . The total number of derived attributes is bounded by Bell number ff . However the complexity of ( ** ) is not too high . Let s be the then the possible ” minimal solutions ” is bounded by the combination . We will report the calculation on real world data in future report soon .
5.1 Find Generalized Association Rule by Linear
Inequalities an example
We will illustrate the idea of the procedure of finding generalized association rules in Table 7 by linear inequality ( support : ) , ) . The association can be expressed as granules :
1 . Associations of length one :
( a ) TEN = .fi fi . fifi fi' fifffi ( b ) SJ = .fi fi . fifi fi' fifffi ( c ) LA =.fi fi fi(fi
2 . Associations of length two :
( a ) ( TEN,SJ )
=
TEN
SJ ff ( TEN,SJ ) .fi fi . fifi fi' fifffi ; ff .fi fi . fifi fi' fifffi .fi fi . fifi fi' fifffi is in table format , that is equivalent to GDM format : TEN SJ . where
In this example , the granules in are
, = TWENTY NY = .fi,fi ,
= TEN SJ = .fi fi . fifi fi' fifffi ,
. = TWENTY LA =.fifi , fi = THIRTY LA =.fi fi(fi
Let fl , fl be the cardinality of , . The following expression represents the cardinality of granules in , which is a union of some granules from the partition .
( TWENTY NY( , + ( TEN SJ( + ( TWENTY
LA ( . +(THIRTY LA ( fi ) , .
By taking the actual value of the cardinalities of the granules , we have ,
( ,( , + ( ( + ( .( . +(fi( fi ) , .
, + fi + . + fi ) , . express It
We will , . fi . dimensional space : The ” boundary solutions ” are : solutions the form , is an ” integral convex set ” in 4 vector in
1 ( 0 , 1 , 0 , 0 ) ; this solution means ’s cardinality by itself already meets the threshold ( ) , ) .
2 ( 0 , 0 , 1 , 1 ) ; this solution means we need the union of two granules ,
TWENTY LA and THIRTY LA , to meet the threshold . In other words , we need a generalized concept that covers both the sub tuple
( TWENTY , LA)= TWENTY LA and
( THIRTY , LA)= THIRTY LA .
For this particular case , since
LA = ( TWENTY , LA ) ff ( THIRTY , LA ) ,
3 . No associations of length fi , . hence LA is the desirable generalized concept .
Now let us examine the universal model in Table ? ? . The column in Table ? ? is the smallest element in the complete relation lattice . So every element of is a coarsening of . In other words , every granule in
3 ( 1 , 0 , 0 , 1 ) ; this solution means we need the union of two granules ,
TWENTY NY ff THIRTY LA ,
( ) , ( .ffi,fi ffi , ( .ffi fi ffi(ffifi ffi . ffifi   ( .ffififi ( .ffi'fi ffi' ( .ffifffi ffiff ( .ffifi ffi ( .ffifi ffi ( .ffi(fi ffi(
Table of Granules
.ffi , ffifi
. ) .ffi,fi )
.ffi ffi . ffifi ffi' ffifffi .ffi ffi . ffifi ffi' ffifffi .ffi ffi . ffifi ffi' ffifffi .ffi ffi . ffifi ffi' ffifffi .ffi ffi . ffifi ffi' ffifffi
.ffi ffi . ffifi ffi' ffifffi ) .ffi ffi . ffifi ffi' ffifffi ) .ffi ffi . ffifi ffi' ffifffi ) .ffi ffi . ffifi ffi' ffifffi ) .ffi ffi . ffifi ffi' ffifffi )
.ffi , ffifi .ffi ffi(fi .ffi ffi(fi
.ffi ffi ffi(fi ) .ffi ffi ffi(fi ) .ffi ffi ffi(fi )
Table of Symbols ( ffi fl ( , % $ ( ( .   ( fi ( ' ( ff ( ( ( (
% $   ’ $   ’ $
$ )
NY ) SJ ) SJ ) SJ ) SJ ) SJ ) LA ) LA ) LA )
Table 7 . Table of Granules at left hand side is isomorphic to at right hand side : By Theorem 31 one can find patterns in either table as a single generalized concept .
” Internal points ” are:[4](1 , 1 , 0 , 0 ) ; we skip the interpretations ; [ 5](0 , 1 , 1 , 0 ) ;[6](0 , 1 , 0 , 1 ) ; [ 7](0 , 1 , 1 , 1 ) ; [ 8](1 , 1 , 1 , 0);[9](1 , 1 , 0 , 1 ) ; [ 10](1 , 0 , 1 , 1 )
; [ 11](1 , 1 , 1 , 1 ) We re express these formulas in granular form and simplify them into disjoint normal forms .
1 TEN SJ = TEN = SJ
2 TWENTY LA ff THIRTY LA =LA
3 TWENTY NY ff THIRTY LA
4 TWENTY NY ff TEN SJ ff ffi LA
5 TEN SJ ff TWENTY LA = TEN ff TWENTY LA ff SJ ff TWENTY LA
6 TEN SJ ff THIRTY LA ff ffi TWENTY
7 TEN SJ ff TWENTY LA ff THIRTY LA = TEN ff
LA = S J ff LA
8 TWENTY NY ff TEN SJ ff TWENTY LA = TEN ff TWENTY ff ffi THIRTY
9 TWENTY NY ff TEN SJ ff THIRTY LA ff ffi
( TWENTY LA )
10 TWENTY NY ff TWENTY LA ff THIRTY LA ff ffi SJ
11 TWENTY NY ff TEN SJ ff TWENTY LA ff
THIRTY LA = all
If the simplified expression is a single clause ( in the original symbols ) , it is the ( non generalized ) associations . We have the following associations
1 . TEN ( = SJ = TEN SJ )
2 . SJ
3 . TEN SJ
4 . LA ( =TWENTY LA ff THIRTY LA ) )
6 Conclusions
Data , patterns , method of derivations , and useful ness are key ingredients in AM . In this paper , we formalize the current state of AM : Data are a table of symbols . The patterns are the formulas of input symbols that repeat . The method of derivations is the most conservative and reliable one , namely , mathematical deductions . The results are somewhat surprising :
1 . Patterns are properties of the isomorphic class , not an individual relation This implies that the notion of patterns may not mature yet and explains why there are so many extracted association rules .
2 . Un interpreted attributes ( features)are partitions ; they can be enumerated .
3 . Generalized associations can be found by solving integral linear inequalities . Unfortunately , the number is enormous . This signifies the current notion of data and patterns ( implied by the algorithms ) are too primitive .
4 . Real world modeling may be needed to create a much more meaningful notion of patterns . In the current state of AM , a pattern is simply a repeated data that may have no real world meaning . So we may need to introduce some semantics into the data model [ 12],[10],[11 ] .
References
[ 1 ] R . Agrawal , T . Imielinski , and A . Swami , ” Mining Association Rules Between Sets of Items in Large Databases , ” in Proceeding of ACM SIGMOD international Conference on Management of Data , pp . 207216 , Washington , DC , June , 1993
[ 2 ] Richard A . Brualdi , Introductory Combinatorics , Pren tice Hall , 1992 .
[ 3 ] A . Barr and EA Feigenbaum , The handbook of Artifi cial Intelligence , Willam Kaufmann 1981
[ 4 ] Margaret H . Dunham , Data Mining Introduction and Advanced Topics Prentice Hall , 2003 , ISBN 0 13088892 3
[ 5 ] Fayad U . M . , Piatetsky Sjapiro , G . Smyth , P . ( 1996 ) From Data Mining to Knowledge Discovery : An overview . In Fayard , Piatetsky Sjapiro , Smyth , and Uthurusamy eds . , Knowledge Discovery in Databases , AAAI/MIT Press , 1996 .
[ 6 ] H Gracia Molina , J . Ullman . & J . Windin , J , Database
Systems The Complete Book , Prentice Hall , 2002 .
[ 7 ] T . T . Lee , ” Algebraic Theory of Relational Databases , ” The Bell System Technical Journal Vol 62 , No 10 , December , 1983 , pp3159 3204
[ 8 ] T . Y . Lin , ” Deductive Data Mining : Mathematical Foundation of Database Mining , ” in : the Proceedings of 9th International Conference , RSFDGrC 2003 , Chongqing , China , May 2003 , Lecture Notes on Artificial Intelligence LNAI 2639 , Springer Verlag , 403 405
[ 9 ] T . Y . Lin , ” Attribute ( Feature ) Completion – The Theory of Attributes from Data Mining Prospect , ” in : Proceeding of IEEE international Conference on Data Mining , Maebashi , Japan , Dec 9 12 , 2002 , pp . pp282 289
[ 10 ] T . Y . Lin , “ Data Mining and Machine Oriented Modeling : A Granular Computing Approach , ” Journal of Applied Intelligence , Kluwer , Vol . 13 , No 2 , September/October,2000 , pp113 124
[ 11 ] T . Y . Lin , N . Zhong , J . Duong , S . Ohsuga , ” Frameworks for Mining Binary Relations in Data . ” In : Rough sets and Current Trends in Computing , Lecture Notes on Artificial Intelligence 1424 , A . Skoworn and L . Polkowski ( eds ) , Springer Verlag , 1998 , 387 393 .
[ 12 ] E . Louie,T . Y . Lin , ” Semantics Oriented Association Rules , ” In : 2002 World Congress of Computational Intelligence , Honolulu , Hawaii , May 12 17 , 2002 , 956961 ( paper # 5702 )
[ 13 ] ” The Power and Limit of Neural Networks , ” Proceedings of the 1996 Engineering Systems Design and Analysis Conference , Montpellier , France , July 1 4 , 1996 , Vol . 7 , 49 53 .
[ 14 ] Morel , Jean Michel and Sergio Solimini , Variational methods in image segmentation : with seven image processing experiments Boston : Birkhuser , 1995 .
[ 15 ] H . Liu and H . Motoda , “ Feature Transformation and Subset Selection , ” IEEE Intelligent Systems , Vol . 13 , No . 2 , March/April , pp.26 28 ( 1998 )
[ 16 ] Z . Pawlak , Rough sets . Theoretical Aspects of Reasoning about Data , Kluwer Academic Publishers , 1991
