Cluster Cores based Clustering for High Dimensional Data
Yi Dong Shen , Zhi Yong Shen and Shi Ming Zhang
Qiang Yang
Laboratory of Computer Science
Department of Computer Science
Institute of Software , Chinese Academy of Sciences
Hong Kong University of Science and Technology
Beijing 100080 , China fydshen , zyshen , zhangsmg@iosaccn
Clearwater Bay , Kowloon , Hong Kong qyang@csusthk
Abstract
In this paper we propose a new approach to clustering high dimensional data based on a novel notion of cluster cores , instead of nearest neighbors . A cluster core is a fairly dense group with a maximal number of pairwise similar/related objects . It represents the core/center of a cluster , as all objects in a cluster are with a great degree attracted to it . As a result , building clusters from cluster cores achieves high accuracy . Other characteristics of our cluster coresbased approach include : ( 1 ) It does not incur the curse of dimensionality and is scalable linearly with the dimensionality of data . ( 2 ) Outliers are effectively handled with cluster cores . ( 3 ) Clusters are efficiently computed by applying existing algorithms in graph theory . ( 4 ) It outperforms both in efficiency and in accuracy the well known clustering algorithm , ROCK .
1 Introduction
Clustering is a major technique for data mining , together with association mining and classification [ 4 , 11 , 21 ] . It divides a set of objects ( data points ) into groups ( clusters ) such that the objects in a cluster are more similar ( or related ) to one another than to the objects in other clusters . Although clustering has been extensively studied for many years in statistics , pattern recognition , and machine learning ( see [ 14 , 15 ] for a review ) , as Agrawal et al [ 3 ] point out , emerging data mining applications place many special requirements on clustering techniques , such as scalability with high dimensionality of data . A number of clustering algorithms have been developed over the last decade in the data base/data mining community ( eg , DBSCAN [ 5 ] , CURE [ 9 ] , CHAMELEON [ 17 ] , CLARANS [ 18 ] , STING [ 20 ] , and BIRCH [ 22] ) . Most of these algorithms rely on a distance function ( such as the Euclidean distance or the Jaccard distance that measures the similarity between two objects ) such that objects are in the same cluster if they are nearest neighbors . However , recent research shows that clustering by distance similarity is not scalable with the dimensionality of data because it suffers from the socalled curse of dimensionality [ 13 , 12 ] , which says that for moderate to high dimensional spaces ( tens or hundreds of dimensions ) , a full dimensional distance is often irrelevant since the farthest neighbor of a data point is expected to be almost as close as its nearest neighbor [ 12 , 19 ] . As a result , the effectiveness/accuracy of a distance based clustering method would decrease significantly with increase of dimensionality . This suggests that ” shortest distances/nearest neighbors ” are not a robust criterion in clustering high dimensional data .
1.1 Contributions of the Paper
To resolve the curse of dimensionality , we propose a new definition of clusters that is based on a novel concept of cluster cores , instead of nearest neighbors . A cluster core is a fairly dense group with a maximal number of pairwise similar objects ( neighbors ) . It represents the core/center of a cluster so that all objects in a cluster are with a great degree attracted to it . This allows us to define a cluster to be an expansion of a cluster core such that every object in the cluster is similar to most of the objects in the core .
Instead of using Euclidean or Jaccard distances , we define the similarity of objects by taking into account the meaning ( semantics ) of individual attributes . In particular , two objects are similar ( or neighbors ) if a certain number of their attributes take on similar values . Whether two values of an attribute are similar is semantically dependent on applications and is defined by the user . Note that since any two objects are either similar/neighbors or not , the concept of nearest neighbors does not apply in our approach .
Our definition of clusters shows several advantages . Firstly , since the similarity of objects is measured semantically wrt the user ’s application purposes , the resulting clusters would be more easily understandable by the user . Secondly , a cluster core represents the core/center of a clus
1 ter , with the property that a unique cluster is defined given a cluster core and that all objects in a cluster are with a great degree attracted to the core . Due to this , the cluster cores based method achieves high accuracy . Thirdly , since clusters are not defined in terms of nearest neighbors , our method does not incur the curse of dimensionality and is scalable linearly with the dimensionality of data . Finally , outliers are effectively eliminated by cluster cores , as an outlier would be similar to no or just a very few objects in a core . provide a threshold and define a function f ( ) . However , as the authors admit [ 10 ] , it may not be easy to decide on an appropriate function f ( ) for different applications .
Unlike the above mentioned shared nearest neighbor approaches , our cluster cores based method does not perform any merge operations . It requires the user to specify only one threshold : the minimum size of a cluster core . We will show that our method outperforms ROCK both in clustering efficiency ( see Corollary 4.3 ) and in accuracy ( see experimental results in Section 5 ) .
1.2 Related Work
2 Semantics based Similarity
In addition to dimensionality reduction ( eg , the principal component analysis ( PCA ) [ 14 , 8 ] ) and projective/subspace clustering ( eg , CLIQUE [ 3 ] , ORCLUS [ 2 ] and DOC [ 19] ) , more closely related work on resolving the dimensionality curse with high dimensional data includes shared nearest neighbor methods , such as the Jarvis Patrick method [ 16 ] , SNN [ 6 ] , and ROCK [ 10 ] . Like our cluster cores based method , they do not rely on the shortest distance/nearest neighbor criterion . Instead , objects are clustered in terms of how many neighbors they share . A major operation of these methods is to merge small clusters into bigger ones . The key idea behind the Jarvis Patrick method is that if any two objects share more than T ( specified by the user ) neighbors , then the two objects and any cluster they are part of can be merged . SNN extends the JarvisPatrick method by posing one stronger constraint : two clusters Ci and Cj could be merged if there are two representative objects , oi 2 Ci and oj 2 Cj , which share more than T neighbors . An object A is a representative object if there are more than T1 objects each of which shares more than T neighbors with A . To apply SNN , the user has to specify several thresholds including T and T1 . ROCK is a sophisticated agglomerative ( ie bottom up ) hierarchical clustering approach . It tries to maximize links ( common neighbors ) within a cluster while minimizing links between clusters by applying a criterion function such that any two clusters Ci and Cj could be merged if their goodness value g(Ci ; Cj ) defined below is the largest : link[Ci ; Cj ]
( jCij + jCjj)1+2f ( ) , jCij1+2f ( ) , jCj j1+2f ( ) where link[Ci ; Cj ] is the number of common neighbors between Ci and Cj , is a threshold for two objects to be similar under the Jaccard distance measure ( see Equation ( 2) ) , and f ( ) is a function with the property that for any cluster Ci , each object in it has approximately jCijf ( ) neighbors . The time complexity of ROCK is O(jDBj2 fi logjDBj + jDBj fi DG fi m ) , where DB is a dataset , and DG and m are respectively the maximum number and average number of neighbors for an object . To apply ROCK , the user needs to
Let A1 ; :: : ; Ad be d attributes ( dimensions ) and V1 ; :: : ; Vd be their respective domains ( ie Ai takes on values from Vi ) . Each Vi can be finite or infinite , and the values in Vi can be either numerical ( eg , 25:5 for price ) or categorical ( eg , blue for color ) . A dataset ( or database ) DB is a finite set of objects ( or data points ) , each o of which is of the form ( ido ; a1 ; :: : ; ad ) where ido is a natural number representing the unique identity of object o , and ai 2 Vi or ai = nil . nil is a special symbol not appearing in any Vi , and ai = nil represents that the value of Ai is not present in this object . For convenience of presentation , we assume the name o of an object is the same as its identity number ido .
To cluster objects in DB , we first define a measure of similarity . The Euclidean distance and the Jaccard distance are two similarity measures which are most widely used by existing clustering methods . Let o1 = ( o1 ; a1 ; :: : ; ad ) and o2 = ( o2 ; b1 ; :: : ; bd ) be two objects . When the ais and bjs are all numerical values , the Euclidean distance of o1 and o2 is computed using the formula distance(o1 ; o2 ) = ( d X k=1
( ak , bk)2)1=2
( 1 )
When the ais and bjs are categorical values , the Jaccard distance of o1 and o2 is given by distance(o1 ; o2 ) = 1 , Pd k=1 ak ffl bk
2d , Pd k=1 ak ffl bk
( 2 ) where ak ffl bk = 1 if ak = bk 6= nil ; otherwise ak ffl bk = 0 . The similarity of objects is then measured such that for any objects o1 , o2 and o3 , o1 is more similar ( or closer ) to o2 than to o3 if distance(o1 ; o2 ) < distance(o1 ; o3 ) . o2 is a nearest neighbor of o1 if distance(o1 ; o2 ) distance(o1 ; o3 ) for all o3 2 DB , fo1 ; o2g .
As we mentioned earlier , clustering by a distance similarity measure like ( 1 ) or ( 2 ) suffers from the curse of dimensionality . In this section , we introduce a non distance measure that relies on the semantics of individual attributes .
We begin by defining the similarity of two values of an attribute . For a numerical attribute , two values are considered similar wrt a specific application if their difference is within a scope specified by the user . For a categorical attribute , however , two values are viewed similar if they are in the same class/partition of the domain . The domain is partitioned by the user based on his/her application purposes . Formally , we have
Definition 2.1 ( Similarity of two numerical values ) Let A be an attribute and V be its domain with numerical values . Let ! 0 be a scope specified by the user . a1 2 V and a2 2 V are similar if ja1 , a2j ! .
Definition 2.2 ( Similarity of two categorical values ) Let A be an attribute and V = V1 [ V2 [ : : : [ Vm be its domain of categorical values , with each Vi being a partition wrt the user ’s application purposes . a1 2 V and a2 2 V are similar if both are in some Vi .
For instance , we may say that two people are similar in age if the difference of their ages is below 10 years , and similar in salary if the difference is not over $500 . We may also view two people similar in profession if both of their jobs belong to the group fsoldier , police , guardg or the group fteacher , researcher , doctorg . The similarity of attribute values may vary from application to application . As an example , let us consider an attribute , city , with a domain Vcity = fBeijing , Shanghai , Guangzhou , Chongqing , Lasa , Wulumuqig . For the user of government officials , Vcity would be partitioned into fBeijing , Shanghai , Chongqingg [ fLasa , Wulumuqig [ fGuangzhoug , as Beijing , Shanghai and Chongqing are all municipalities directly under the Chinese Central Government , while Lasa and Wulumuqi are in two autonomous regions of China . However , for the user who is studying the SARS epidemic , Vcity would be partitioned into fBeijing , Guangzhoug [ fShanghai , Chongqingg [ fLasa , Wulumuqig , as in 2003 , Beijing and Guangzhou were two most severe regions in China with the SARS epidemic , Shanghai and Chongqing had just a few cases , and Lasa and Wulumuqi were among the safest zones with no infections .
With the similarity measure of attribute values defined above , the user can then measure the similarity of two objects by counting their similar attribute values . If the number of similar attribute values is above a threshold ffi , the two objects can be considered similar wrt the user ’s application purposes . Here is the formal definition .
Definition 2.3 ( Similarity of two objects ) Let o1 and o2 be two d dimensional objects and K be a set of key attributes wrt the user ’s application purposes . Let ffi be a similarity threshold with 0 < ffi jKj d . o1 and o2 are similar ( or neighbors ) if they have at least ffi attributes in K that take on similar values .
The set of key attributes is specified by the user . Whether an attribute is selected as a key attribute depends on whether it is relevant to the user ’s application purposes . In case that the user has difficulty specifying key attributes , all attributes are taken as key attributes . The similarity threshold ffi is expected to be specified by the user . It can also be elicited from a dataset by trying several possible choices ( at most d alternatives ) in clustering/learning experiments . The best similarity threshold is one that leads to a satisfying clustering accuracy ( see Section 5 ) . Note that since any two objects are either similar/neighbors or not , the concept of nearest neighbors does not apply in our approach .
3 Cluster Cores and Disjoint Clusters
The intuition behind our clustering method is that every cluster is believed to have its own distinct characteristics , which are implicitly present in some objects in a dataset . This suggests that the principal characteristics of a cluster may be represented by a certain number of objects C r such that ( 1 ) any two objects in C r are similar and ( 2 ) no other objects in the dataset can be added to C r without affecting property ( 1 ) . Formally , we have
Definition 3.1 ( Cluster cores ) C r DB is a cluster core if it satisfies the following three conditions . ( 1 ) jC rj ff , where ff is a threshold specifying the minimum size of a cluster core . ( 2 ) Any two objects in C r are similar wrt the user ’s application purposes . ( 3 ) There exists no C 0 , with C r fl C 0 DB , that satisfies condition ( 2 ) . C r is a maximum cluster core if it is a cluster core with the maximum cardinality .
Note that a cluster core C r is a fairly dense group with a maximal number of pairwise similar/related objects . Due to this , it may well be treated as the core/center of a cluster , as other objects of the cluster not in C r must be attracted with a great degree to C r .
Definition 3.2 ( Clusters ) Let C r DB be a cluster core and let be a cluster threshold with 0 1 . C is a cluster if C = fv 2 DBjv 2 C r or C r contains at least fi jC rj objects that are similar to vg .
The threshold is the support ( degree ) of an object being attracted to a cluster core . Such a parameter can be learned from a dataset by experiments ( see Section 5 ) . Note that a unique cluster is defined given a cluster core . However , a cluster may be derived from different cluster cores , as the core/center of a cluster can be described with different sets of objects that satisfy the three conditions of Definition 31 The above definition of cluster cores/clusters allows us to apply existing graph theory to compute them . We first define a similarity graph over a dataset .
Definition 3.3 ( Similarity graphs ) A similarity graph , SGDB = ( V ; E ) , of DB is an undirected graph where V = DB is the set of nodes , and E is the set of edges such that fo1 ; o2g 2 E if objects o1 and o2 are similar .
Theorem 3.1 Let C r DB with jC rj ff . C r is a cluster core if and only if it is a maximal clique in SGDB . C r is a maximum cluster core if and only if it is a maximum clique in SGDB .
Example 3.1 Let us consider a sample dataset , DB1 , as shown in Table 1 , where nil is replaced by a blank . For simplicity , assume all attributes have a domain f1g with a single partition f1g . Thus , two values , v1 and v2 , of an attribute are similar if v1 = v2 = 1 . Let us assume all attributes are key attributes , and choose the similarity threshold ffi 2 . The similarity relationship between objects of DB1 is depicted by a similarity graph SGDB1 shown in Figure 1 .
Table 1 . A sample dataset DB1 .
1
3
,@
@ ,
, @
,
@
2
4
6
5
,@
@ ,
, @
,
@
7
8
Figure 1 . A similarity graph SGDB1 .
Let G = ( V ; E ) be an undirected graph and V 0 V . We denote G(V 0 ) for the subgraph of G induced by V 0 ; namely , G(V 0 ) = ( V 0 ; E0 ) such that for any v1 ; v2 2 V 0 , fv1 ; v2g 2 E0 if and only if fv1 ; v2g 2 E . G is complete if its nodes are pairwise adjacent , ie for any two different nodes v1 , v2 2 V , fv1 ; v2g 2 E . A clique C is a subset of V such that G(C ) is complete . A maximal clique is a clique that is not a proper subset of any other clique . A maximum clique is a maximal clique that has the maximum cardinality . It turns out that a cluster core in DB corresponds to a maximal clique in SGDB .
Similarly , we have the following result .
Theorem 3.2 Let C r DB with jC rj ff and let V = fv 62 C rjv is a node in SGDB and there are at least fijC rj nodes in C r that are adjacent to vg . C is a cluster with a cluster core C r if and only if C r is a maximal clique in SGDB and C = C r [ V .
Example 3.2 In Figure 1 , we have four maximal cliques : C1 = f1 ; 2 ; 3 ; 4g , C2 = f4 ; 5g , C3 = f5 ; 6 ; 7g and C4 = f5 ; 6 ; 8g . If we choose the least cluster size ff = 2 , all of the four maximal cliques are cluster cores . Let us choose ff = 3 and the cluster threshold = 0:6 . C1 , C3 and C4 are cluster cores . C1 is also a cluster and f5 ; 6 ; 7 ; 8g is a cluster which can be obtained by expanding either C3 or C4 .
From Example 3.2 , we see that there may be overlaps among cluster cores and/or clusters . In this paper we are devoted to building disjoint clusters .
Definition 3.4 ( Disjoint clusters ) DC(DB ) = fC1 ; :: : ; Cmg is a collection of disjoint clusters of DB if it satisfies the following condition : C1 is a cluster in DB , and for any i > 1 , Ci is a cluster in DB ,Sj=i,1 j=1 Cj .
Consider Figure 1 again .
Let ff = 2 and = 0:6 . We have several collections of disjoint clusters including : DC(DB1)1 = ff1 ; 2 ; 3 ; 4g ; f5 ; 6 ; 7 ; 8gg and DC(DB1)2 = ff4 ; 5g ; f1 ; 2 ; 3g ; f6 ; 7gg . Apparently , we would rather choose DC(DB1)1 than DC(DB1)2 . To obtain such optimal ones , we introduce a notion of maximum disjoint clusters .
Definition 3.5 ( Maximum disjoint clusters ) DC(DB ) = fC1 ; :: : ; Cmg is a collection of maximum disjoint clusters if it is a collection of disjoint clusters with the property that each Ci is a cluster with a maximum cluster core in DB , Sj=i,1 j=1 Cj .
Back to the above example , DC(DB1)1 is a collection of maximum disjoint clusters , but DC(DB1)2 is not , as f4 ; 5g is not a maximum cluster core in DB1 .
4 Approximating Maximum Clusters
Ideally , we would like to have maximum disjoint clusters . However , this appears to be infeasible for a massive dataset DB , as computing a maximum cluster core over DB amounts to computing a maximum clique over its corresponding similarity graph SGDB ( Theorem 31 ) The maximum clique problem is one of the first problems shown to be N P complete , which means that , unless P =N P , exact algorithms are guaranteed to return a maximum clique in a time which increases exponentially with the number of nodes in SGDB .
In this section , we describe our algorithm for approximating maximum disjoint clusters . We begin with an algorithm for computing a maximal clique . For a node v in a graph G , we use ADG(v ) to denote the set of nodes adjacent to v , and refer to jADG(v)j as the degree of v in G .
Algorithm 1 : Computing a maximal clique . Input : An undirected graph G = ( V ; E ) . Output : A maximal clique C in G . function maximal clique(V ; E ) returns C 1 ) C = ; ; 2 ) V 0 = V ; 3 ) while ( V 0 6= ; ) do begin 4 ) 5 ) 6 ) 7 ) end 8 ) return C end
Select v at random from V 0 ; C = C [ fvg ; V 0 = ADG(V 0)(v )
Since there may be numerous maximal cliques in a graph , Algorithm 1 builds one in a randomized way . Initially , C is an empty clique ( line 1 ) . In each cycle of lines 3 7 , a new node is added to C . After k 0 cycles , C grows into a clique fv1 ; :: : ; vkg . In the beginning of cycle k+1 , V 0 consists of all nodes adjacent to every node in C . Then , vk+1 is randomly selected from V 0 and added to C ( lines 4 5 ) . The iteration continues with V 0 = ADG(V 0)(vk+1 ) ( line 6 ) until V 0 becomes empty ( line 3 ) .
Let DG be the maximum degree of nodes in G and let S be the ( sorted ) set of nodes in G that are adjacent to a node v . The time complexity of line 6 is bounded by O(DG ) , as computing ADG(V 0)(v ) is to compute the intersection of the two sorted sets S and V 0 . Thus we have the following immediate result .
Theorem 4.1 Algorithm 1 constructs a maximal clique with the time complexity O(D
2 G ) .
Algorithm 1 randomly builds a maximal clique . This suggests that a maximum clique could be approximated if we apply Algorithm 1 iteratively for numerous times . Here is such an algorithm ( borrowed from Abello et al . [ 1 , 7 ] with slight modification ) .
Algorithm 2 : Approximating a maximum clique . Input : A graph G = ( V ; E ) and an integer maxitr . Output : A maximal clique C r . function maximum clique(V ; E ; maxitr ) returns C r i = 0 ;
1 ) C r = ; ; 2 ) 3 ) while ( i < maxitr ) do begin 4 ) 5 ) 6 ) 7 ) end 8 ) return C r end
C = maximal clique(V ; E ) ; if jCj > jC rj then C r = C ; i = i + 1
The parameter maxitr specifies the times of iteration . In general , the bigger maxitr is , the closer the final output C r would be to a maximum clique . In practical applications , it would be enough to do maxitr DG iterations to obtain a maximal clique quite close to a maximum one . For instance , in Figure 1 choosing maxitr = 2 guarantees to find a maximum clique , where DSGDB1 = 4 :
By Theorem 3.1 , Algorithm 2 approximates a maximum cluster core C r when G is a similarity graph SGDB . Therefore , by Theorem 3.2 an approximated maximum cluster can be built from C r by adding those nodes v such that C r contains at least fi jC rj nodes adjacent to v . So we are ready to introduce the algorithm for approximating maximum disjoint clusters over a similarity graph .
Algorithm 3 : Approximating maximum disjoint clusters . Input : SGDB = ( V ; E ) , maxitr , ff and . Output : Q = fC1 ; :: : ; Cmg , a collection of approximated maximum disjoint clusters . function disjoint clusters(V ; E ; ff ; ; maxitr ) returns Q 1 ) Q = ; ; 2 ) ( V 0 ; E0 ) = peel((V ; E ) ; ff ) ; 3 ) while ( jV 0j ff ) do begin 4 ) 5 ) 6 )
C r = maximum clique(V 0 ; E0 ; maxitr ) ; if jC rj < ff then break ; C = C r [ fv 2 V 0 , C rjC r has at least fi jC rj nodes adjacent to vg ;
Add C to the end of Q ; V1 = V 0 , C ; ( V 0 ; E0 ) = peel(SGDB(V1 ) ; ff )
7 ) 8 ) 9 ) 10 ) end 11 ) return Q end
Given a graph G , the function peel(G ; ff ) updates G into a graph G0 = ( V 0 ; E0 ) by recursively removing ( labeling with a flag ) all nodes whose degrees are below the least size of a cluster core ff ( lines 2 and 9 ) , as such nodes will not appear in any disjoint clusters . By calling the function maximum clique(V 0 ; E0 ; maxitr ) ( line 4 ) , Algorithm 3 builds a collection of disjoint clusters Q = fC1 ; :: : ; Cmg , where each Ci is an approximation of some maximum cluster in DB , Sj=i,1 j=1 Cj .
Theorem 4.2 When Algorithm 3 produces a collection of m disjoint clusters , its time complexity is O(m fi maxitr fi D
2 G ) .
Since it costs O(jDBj2 ) in time to construct a similarity graph SGDB from a dataset DB,1 the following result is immediate to Theorem 42
Corollary 4.3 The overall time complexity of the cluster 2 cores based clustering is O(jDBj2 + m fi maxitr fi D G ) .
In a massive high dimensional dataset DB , data points would be rather sparse so that DG jDBj . Our experiments show that most of the time is spent in constructing 2 SGDB because jDBj2 * m fi maxitr fi D G . Note that the complexity of our method is below that of ROCK .
5 Experimental Evaluation
We have implemented our cluster cores based clustering method ( Algorithms 1 3 together with a procedure for building a similarity graph SGDB ) and made extensive experiments over real life datasets . Due to the limit of paper pages , in this section we only report experimental results on one dataset : the widely used Mushroom data from the UCI Machine Learning Repository ( we downloaded it from http://wwwsgicom/tech/mlc/db/ ) The Mushroom dataset consists of 8124 objects with 22 attributes each with a categorical domain . Each object has a class of either edible or poisonous . One major reason for us to choose this dataset is that ROCK [ 10 ] has achieved very high accuracy over it , so we can make a comparison with ROCK in clustering accuracy . Our experiments go through the following two steps .
Step 1 : Learn a similarity threshold ffi . In most cases , the user cannot provide a precise similarity threshold used for measuring the similarity of objects . This requires us to be able to learn it directly from a dataset .
For a dataset with d dimensions , ffi has at most d possible choices . Then , with each ffii d we construct a similarity graph SGffii DB and apply Algorithm 3 with = 1 to derive a collection of disjoint clusters . Note that since = 1 , each cluster is itself a cluster core . Next , we compute the clustering precision for each selected ffii using the formula precision(ffii ; 1 ; jDBj ) = N jDBj , where N is the number of objects sitting in right clusters . Finally , we choose ffi = maxffii fprecision(ffii ; 1 ; jDBj)g as the best threshold since under it the accuracy of a collection of disjoint cluster cores would be maximized .
Step 2 : Learn a cluster threshold given ffi . With the similarity threshold ffi determined via Step 1 , we further elicit a
1It costs O(d ) to compute the similarity of two objects with d dimen sions . Since d DB , it is often ignored in most existing approaches . cluster threshold from the dataset , aiming to construct a collection of disjoint clusters with high accuracy .
With each i 2 f0:8 ; 0:81 ; :: : ; 0:99 ; 1g we apply Algorithm 3 to generate a collection of disjoint clusters and compute its precision using the formula precision(ffi ; i ; jDBj ) . The best comes from maxi fprecision(ffi ; i ; jDBj)g :
Note that i begins with 0:8 . This value was observed It may vary from application to in our experiments . application . In addition , to obtain clusters with largest possible sizes , one may apply different cluster thresholds : a relatively small one for large cluster cores and a relatively big one for small cluster cores . Again , such cluster thresholds can be observed during the learning process .
We performed our experiments on a PC computer with a Pentium 4 2.80GHz CPU and 1GB of RAM . To approximate maximum cluster cores ( Algorithm 2 ) , we set maxitr = 10 ( ie we do 10 times of iteration ) . In Step 1 , different similarity thresholds were tried , with different accuracy results produced as shown in Figure 2 . Clearly , ffi = 15 is the best for the Mushroom dataset . With this similarity threshold , in Step 2 we applied different cluster thresholds and obtained different accuracy results as shown in Figure 3 . We see that the highest accuracy is obtained at = 0:88 . The sizes of clusters that were produced with ffi = 15 and = 0:88 are shown in Table 2 ( C# stands for the cluster number and the results of ROCK were copied from [ 10] ) . Note that our method achieves higher accuracy than ROCK .
Figure 2 . Accuracy with varying similarity thresholds .
To show that the two thresholds learned via Steps 1 and 2 are the best , we made one more experiment by learning the two parameters together . That is , for each ffii d and j 2 f0:8 ; 0:81 ; :: : ; 0:99 ; 1g , we compute precision(ffii ; j ; jDBj ) . If ffi and selected in Steps 1 and 2 are the best , we expect precision(ffi ; ; jDBj ) maxffii;j fprecision(ffij ; j ; jDBj)g : Apparently , Figure 4 confirms our expectation .
In addition to having high clustering accuracy , our cluster cores based method can achieve high accuracy when being applied to make classifications . The basic idea is as
Table 2 . Clustering results ( Mushroom data ) .
Cluster Cores based Clustering
Figure 3 . Accuracy with varying cluster thresholds given ffi = 15 .
C# Edible 1 1704 2 3 4 5 6 7 8 9 10 11
0 0 762 0 288 509 192 0 0 192
0
1280 1556
Poisonous C# Edible 192 0 32 96 96 48 48 0 0 24
12 13 14 15 16 17 18 19 20 21
0 288 0 0 0 256 192 0
Poisonous
0 172 72 0 0 0 0 36 32 0
P recision = 8035=8124 = 98:9 %
96 0 704 96 768 0
C# Edible 1 2 3 4 5 6 7 8 9 10 11
0 0 0 48
1728
ROCK
Poisonous C# Edible
Poisonous
0 256 0 0 0 192 0 32 1296
8 0
12 13 14 15 16 17 18 19 20 21
48 0 192 32 0 288 0 192 16 0
0 288 0 72 1728
0 8 0 0 36
P recision = 7804=8124 = 96:1 % using cluster cores to derive clusters achieves high accuracy . Since clusters are not defined in terms of nearest neighbors , our method does not incur the curse of dimensionality and is scalable linearly with the dimensionality of data . Outliers are effectively eliminated by cluster cores , as an outlier would be similar to no or just a very few objects in a cluster core . Although our approach needs three thresholds , ff ( the minimum size of a cluster core ) and ffi ( the similarity threshold ) and ( the cluster threshold ) , the last two can well be learned from data . We have shown that our approach outperforms both in efficiency and in accuracy the well known ROCK algorithm .
Like all similarity matrix/graph based approaches , most of the time of our approach is spent in building a similarity graph . Therefore , as part of future work we are seeking more efficient ways to handle similarity graphs of massive datasets . Moreover , we are going to make detailed comparisons with other closely related approaches such as SNN [ 6 ] .
Figure 4 . Accuracy with varying similarity and cluster thresholds . follows . We first apply Algorithm 3 over a training dataset to learn a similarity threshold ffi and generate a collection of disjoint cluster cores with this threshold and = 1 . Then , we label each cluster core with a class . Next , for each new object o to be classified we count the number Ni of similar objects of o in each cluster core C r i . Finally , we compute i = maxifNi=jC r i jg and label o with the class of C r i .
We also made experiments for classification on the Mushroom data . The 8,124 objects of the Mushroom dataset have already been divided into a training set ( 5416 objects ) and test set ( 2,708 objects ) on the SGI website ( http://wwwsgicom/tech/mlc/db/ ) The experimental results are shown in Figure 5 , where we obtain over 98 % of classification accuracy at ffi = 15 .
6 Conclusions and Future Work
We have presented a new clustering approach based on the notion of cluster cores , instead of nearest neighbors . A cluster core represents the core/center of a cluster and thus
ACM SIGMOD Intl . Conf . Management of Data , pp . 73 84 , 1998 .
[ 10 ] S . Guha , R . Rastogi and K . Shim , ROCK : A robust clustering algorithm for categorical attributes , in : Proc . ICDM Intl . Conf . on Data Engineering , pp . 512521 , 1999 .
[ 11 ] J . Han and M . Kamber , Data Mining : Concepts and
Techniques , Morgan Kaufmann Publishers , 2000 .
[ 12 ] A . Hinneburg , C . C . Aggarwal and D . A . Keim , What is the nearest neighbor in high dimentional spaces ? VLDB , pp . 506 515 , 2000 .
[ 13 ] A . Hinneburg and D . A . Keim , Optimal gridclustering : towards breaking the curse of dimentionality in high dimentional clustering ? VLDB , pp . 506517 , 1999 .
[ 14 ] A . K . Jain and R . C . Dubes , Algorithms for Clustering
Data , Prentice Hall , 1988 .
[ 15 ] A . K . Jain , M . N . Murty and P . J . Flynn , Data clustering : a review , ACM Computing Surveys 31(3 ) : 264323 ( 1999 ) .
[ 16 ] R . A . Jarvis and E . A . Patrick , Clustering using a similarity measure based on shared nearest neighbors , IEEE Transactions on Computers C 22(11 ) : 264 323 ( 1973 ) .
[ 17 ] G . Karypis , E H Han and V . Kumar , Chameleon : Hierarchical clustering using dynamic modeling , IEEE Computer 32(8 ) : 68 75 ( 1999 ) .
[ 18 ] R . T . Ng and J . Han , Efficient and effective clustering methods for spatial data mining , in : Proc . 20th Intl . Conf . Very Large Data Bases , pp . 144 155 , 1994 .
[ 19 ] C . M . Procopiuc , M . Jones , P . K . Agarwal and T . M . Murali , A monte carlo algorithm for fast projective clustering , ACM SIGMOD Intl . Conf . Management of Data , pp.418 427 , 2002 .
[ 20 ] W . Wang , J . Yang and R . Muntz , STING : a statistical information grid approach to spatial aata mining , in : Proc . of the 23rd VLDB Conference , pp . 186 195 , 1997 .
Figure 5 . Accuracy of clustering based classification ( Mushroom data ) .
References
[ 1 ] J . Abello , PM Pardalos and MGC Resende , On maximum cliques problems in very large graphs , in : ( J . Abello and J . Vitter , Eds . ) DIMACS Series on Discrete Mathematics and Theoretical Computer Science , American Mathematical Society ( 50 ) : 119 130 ( 1999 ) .
[ 2 ] C . C . Aggarwal and P . S . Yu , Finding generalized projected clusters in high dimensional spaces , in : Proc . of ACM SIGMOD Intl . Conf . Management of Data , pp . 70 81 , 2000 .
[ 3 ] R . Agrawal , J . Gehrke , D . Gunopolos and P . Raghavan , Automatic subspace clustering of high dimentional data for data mining applications , ACM SIGMOD Intl . Conf . Management of Data , pp . 94 105 , 1998 .
[ 4 ] M . R . Anderberg , Cluster Analysis for Applications ,
Academic Press , New York and London , 1973 .
[ 5 ] M . Ester , H . P . Kriegel , J . Sander and X . Xu , A density based algorithm for discovering clusters in large spatial databases with noise , in : Proc . 2nd Intl . Conf . Knowledge Discovery and Data Mining , pp . 226 231 , 1996 .
[ 6 ] L . Ertoz , M . Steinbach and V . Kumar , Finding clusters of different sizes , shapes , and densities in noisy , high dimensional data , in : Proc . 3rd SIAM International Conference on Data Mining San Francisco , CA , USA , 2003
[ 7 ] TA Feo and MGC Resende , Greedy randomized adaptive search procedures , J . of Global Optimization ( 6 ) : 109 133 ( 1995 ) .
[ 21 ] I . H . Witten and E Frank , Data Mining : Practical Machine Learning Tools and Techniques with Java Implementations , Morgan Kaufmann Publishers , 1999 .
[ 8 ] K . Fukunaga ,
Introduction to Statistical Pattern
Recognition , Academic Press , 1990 .
[ 9 ] S . Guha , R . Rastogi and K . Shim , CURE : An efficient clustering algorithm for large databases , in : Proc .
[ 22 ] T . Zhang , R . Ramakrishnan and M . Livny , Birch : an efficient data clustering method for very large databases , in : Proc . ACM SIGMOD Intl . Conf . Management of Data , pp.103 114 , 1996 .
