IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL . 18 , NO . 7 ,
JULY 2006
1
Orthogonal Decision Trees
Hillol Kargupta , Byung Hoon Park , and Haimonti Dutta
Abstract—This paper introduces orthogonal decision trees that offer an effective way to construct a redundancy free , accurate , and meaningful representation of large decision tree ensembles often created by popular techniques such as Bagging , Boosting , Random Forests , and many distributed and data stream mining algorithms . Orthogonal decision trees are functionally orthogonal to each other and they correspond to the principal components of the underlying function space . This paper offers a technique to construct such trees based on the Fourier transformation of decision trees and eigen analysis of the ensemble in the Fourier representation . It offers experimental results to document the performance of orthogonal trees on the grounds of accuracy and model complexity .
Index Terms—Orthogonal decision trees , redundancy free trees , principle component analysis , Fourier transform .
æ
1 INTRODUCTION
DECISION tree [ 1 ] ensembles are frequently used in data mining and machine learning applications . Boosting [ 2 ] , [ 3 ] , Bagging [ 4 ] , Stacking [ 5 ] , and Random Forests [ 6 ] are some of the well known ensemble learning techniques . Many of these techniques often produce large ensembles that combine the outputs of a large number of trees for producing the overall output . Ensemble based classification and outlier detection techniques are also frequently used in mining continuous data streams [ 7 ] , [ 8 ] . Large ensembles pose several problems to a data miner . They are difficult to understand and the overall functional structure of the ensemble is not very “ actionable ” since it is difficult to manually combine the physical meaning of different to produce a simplified set of rules that can be used in practice . Moreover , in many time critical applications , such as monitoring data streams in resource constrained environments [ 9 ] , maintaining a large ensemble and using it for continuous monitoring are computationally challenging . So , it will be useful if we can develop a technique to construct a redundancy free meaningful compact representation of large ensembles . This paper offers a technique to do that and possibly more . trees in order
This paper presents a technique to construct redundancy free decision tree ensembles by using orthogonal decision trees . The technique first constructs an algebraic representation of trees using multivariate discrete Fourier basis set . The new representation is then used for eigenanalysis of the covariance matrix generated by the decision trees in Fourier representation . The proposed approach then converts the corresponding principal components to decision trees . These trees are defined in the original attributes space and they are functionally orthogonal to each other . These orthogonal trees are in turn used for accurate ( in many cases , with improved accuracy ) and
. H . Kargupta and H . Dutta are with the Department of Computer Science and Electrical Engineering , University of Maryland Baltimore County , 1000 Hilltop Circle , Baltimore , MD 21250 . E mail : {hillol , hdutta1}@cseeumbcedu
. B H Park is with the Computer Science and Mathematics Division , Oak Ridge National Laboratory , PO Box 2008 MS6164 , Oak Ridge , TN 378316164 . E mail:parkbh@ornlgov
Manuscript received 20 Dec . 2004 ; revised 29 Nov . 2005 ; accepted 23 Jan . 2006 ; published online 18 May 2006 . For information on obtaining reprints of this article , please send e mail to : redundancy free ( in the sense of an orthogonal basis set ) compact representation of large ensembles .
Section 2 presents the motivation of this work . Section 3 presents a brief overview of the Fourier spectrum of decision trees . Section 4 describes the algorithms for computing the Fourier transform of a decision tree . Section 5 offers the algorithm for computing the tree from its Fourier spectrum . Section 6 discusses orthogonal decision trees . Section 7 presents experimental results using many well known data sets . Finally , Section 8 concludes this paper .
2 MOTIVATION This paper extends our earlier work [ 10 ] , [ 9 ] , [ 11 ] on the Fourier spectrum of decision trees . The main motivation behind this approach is to create an algebraic framework for the metalevel analysis of models produced by many ensemble learning , data stream mining , distributed data mining , and other related techniques . Most of the existing techniques treat the discrete model structures such as decision trees in an ensemble primarily as a black box . Only the output of the models is considered and combined in order to produce the overall output . Fourier bases offer a compact representation of a discrete structure that allows algebraic manipulation of decision trees . For example , we can literally add two different trees , produce weighted average of the trees themselves , or perform eigen analysis of an ensemble of trees . Fourier representation of decision trees may offer something that is philosophically similar to what spectral representation of graphs [ 12 ] offers—an algebraic representation that allows deep analysis of discrete structures .
Fourier representation allows us to bring in the rich volume of well understood techniques from Linear Algebra and Linear Systems Theory . This opens up many exciting possibilities for future research , such as quantifying the stability of an ensemble classifier , mining , and monitoring mission critical data streams using properties of the eigenvalues of the ensemble . This paper takes some steps toward achieving these goals .
The main contributions of this paper are listed below :
1 .
2 .
It offers several new analytical results regarding the properties of the Fourier spectra of decision trees . It presents a detailed discussion on the Tree Construction from Fourier Spectrum ( TCFS ) algo
1041 4347/06/$20.00 ß 2006 IEEE
Published by the IEEE Computer Society
2
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL . 18 , NO . 7 , JULY 2006 rithm for computing a decision tree from the Fourier coefficients . This includes discussion and experimental evaluation of the TCFS algorithm . New experimental results compare the performance of the trees constructed using the TCFS technique with that of the trees constructed using standard techniques such as C45 It discusses Orthogonal Decision Trees ( ODTs ) in detail and offers extensive experimental results documenting the performance of ODTs on benchmarked data sets .
3 .
The following section reviews the Fourier representation of decision trees .
3 DECISION TREES AND THE FOURIER
REPRESENTATION
This section reviews the Fourier representation of decision tree ensembles , introduced elsewhere [ 13 ] , [ 14 ] . It also presents some new analytical results .
3.1 Decision Trees as Numeric Functions The approach developed in this paper makes use of linear algebraic representation of the trees . In order to do that , we first need to convert the tree into a numeric tree just in case the attributes are symbolic . A decision tree defined over a domain of categorical attributes can be treated as a numeric function . First note that a decision tree is a function that maps its domain members to a range of class labels . Sometimes , it is a symbolic function where attributes take symbolic ( nonnumeric ) values . However , a symbolic function can be easily converted to a numeric function by simply replacing the symbols with numeric values in a consistent manner . Since the proposed approach of constructing orthogonal trees uses this representation as an intermediate stage and eventually the physical tree is converted back to the exact scheme for replacing the symbols ( if any ) , it does not matter as long as it is consistent .
Once the tree is converted to a discrete numeric function , we can also apply any appropriate analytical transformation as necessary . Fourier transformation is one such interesting possibility . Fourier representation of a function is a linear combination of the Fourier basis functions . The weights , called Fourier coefficients , completely define the representation . Each coefficient is associated with a Fourier basis function that depends on a certain subset of features defining the domain . This section reviews the Fourier representation of decision tree ensembles , introduced elsewhere [ 9 ] .
3.2 A Brief Review of Multivariate Fourier Basis A Fourier basis set is comprised of orthogonal functions that can be used to represent any discrete function . In other words , it is a functionally complete representation . Consider the set of all ‘ dimensional feature vectors where the ith feature can take i different discrete values . The Fourier basis set that spans this space is comprised of ‘ i¼0 i basis functions . Each Fourier basis function is defined as j ðxÞ ¼ q
1ffiffiffiffiffiffiffiffiffiffiffiffiffiffi l i¼1 i l m¼1 exp
2 i m xmjm ;
P where j and x are vectors of length ‘ ; xm and jm are mth attribute value in x and j , respectively ; xm ; jm 2 f0 ; 1 ; ig and represents the feature cardinality vector , 0 ; ‘ ; j ðxÞ is called the jth basis function . The vector j is called a partition and the order of a partition j is the number of nonzero feature values it contains . A Fourier basis function depends on some xi only when the corresponding ji 6¼ 0 . If a partition j has exactly number of nonzero values , then we say the partition is of order since the corresponding Fourier basis function depends only on those number of variables that take nonzero values in the partition j .
A function f : X‘ ! < , x j wj j ðxÞ is the complex conjugate of that maps an ‘ dimensional discrete domain to a real valued range , can be represented j ðxÞ , where using the Fourier basis functions : fðxÞ ¼ wj is the Fourier Coefficient ( FC ) corresponding to the P j ðxÞ ; partition j and j ðxÞfðxÞ . The Fourier coefficient wj can be viewed wj ¼ as the relative contribution of the partition j to the function value of fðxÞ . Therefore , the absolute value of wj can be used as the “ significance ” of the corresponding partition j . If the magnitude of some wj is very small compared to other coefficients , we may consider the jth partition to be insignificant and neglect its contribution . The order of a Fourier coefficient is nothing but the order of the corresponding partition . We shall often use terms like high order or low order coefficients to refer to a set of Fourier coefficients whose orders are relatively large or small , respectively . The j . Let energy of a spectrum is defined by the summation us also define the inner product between two spectra wð1Þ and wð2Þ , where wðiÞ ¼ ½wðiÞ;1wðiÞ;2 ; wðiÞ;jJjT is the column matrix of all Fourier coefficients in an arbitrary but fixed order . Superscript T denotes the transpose operation and jJj denotes the total number of coefficients in the spectrum . The inner product , < wð1Þ ; wð2Þ >¼ j wð1Þ;jwð2Þ;j . We will also use the definition of the inner product between a pair of realP valued functions defined over some domain . This is defined as < f1ðxÞ ; f2ðxÞ >¼ x2 f1ðxÞf2ðxÞ .
P j w2
P
The following section considers the Fourier spectrum of decision trees and discusses some of its useful properties .
3.3 Properties of Decision Trees in the
Fourier Domain
For almost all practical purposes , decision trees have bounded depths . This section will therefore consider decision trees of finite depth bounded by some constant . The underlying functions in such decision trees are computable by a constant depth Boolean AND and OR circuit ( or , equivalently , AC0 circuit ) . Linial et al . [ 15 ] noted the Fourier spectrum of an AC0 circuit has very that interesting properties and proved the following lemma : Lemma 1 ( [15] ) . Let M and d be the size and depth of an AC0 circuit . Then , X j 2M2 t1=d=20 ; w2 fjjoðjÞ>tg where oðjÞ denotes the order ( the number of nonzero variable ) of partition j and t is a nonnegative integer . The term on the
KARGUPTA ET AL . : ORTHOGONAL DECISION TREES left hand side of the inequality represents the energy of the spectrum captured by the coefficients with order greater than a given constant t .
The lemma essentially states the following properties about decision trees :
1 . High order Fourier coefficients are small in magni tude .
2 . The energy preserved in all high order Fourier coefficients is also small .
The key aspect of these properties is that the energy of the Fourier coefficients of higher order decays exponentially . This observation suggests that the spectrum of a Boolean decision tree ( or , equivalently , bounded depth function ) can be approximated by computing only a small number of low order Fourier coefficients . So , Fourier basis offers an efficient numeric representation of a decision tree in terms of an algebraic function that can be easily stored and manipulated .
The exponential decay property of the Fourier spectrum also holds for non Boolean decision trees . The complete proof is given in the Appendix , which can be found on the Computer Society Digital Library at http://computer . orgtkde/archiveshtm
There are two additional important characteristics of the Fourier spectrum of a decision tree that we will use in this paper :
1 . The Fourier spectrum of a decision tree can be efficiently computed [ 9 ] .
2 . The Fourier spectrum can be directly used for constructing the tree .
In other words , we can go back and forth between the tree and its spectrum . This is philosophically similar to the switching between the time and frequency domains in the traditional application of Fourier analysis for signal processing . These two issues will be discussed in details later in this paper . However , before that , we would like to make a note of one additional property .
The Fourier transformation of decision trees preserves the inner product . The functional behavior of a decision tree is defined by the class labels it assigns . Therefore , if fx1 ; x2 ; xj jg are the members of the domain , then the functional behavior of a decision tree fðxÞ can be captured by the vector ½fx2 ¼ ½fðx1Þfðx2Þ fðxj jÞT , where the superscript T denotes the transpose operation . the inner product The following lemma proves that between two such vectors is identical to the same in P between their respective Fourier spectra : Lemma 2 . Given two functions f1ðxÞ ¼
ðxÞ and j wð1Þ;j j
P f2ðxÞ ¼ j wð2Þ;j j
ðxÞ in Fourier representation , then ,
< f1ðxÞ ; f2ðxÞ >¼< wð1Þ ; wð2Þ > .
3 tu
Fig 1 . A Boolean decision tree .
Proof .
< f1ðxÞ ; f2ðxÞ > ¼
¼
¼
¼
X
X x2
X x2
X j;i j f1ðxÞf2ðxÞ X wð1Þ;j j
ðxÞwð2Þ;i i X
ðxÞ j;i
ðxÞ i
ðxÞ j wð1Þ;jwð2Þ;i x2 wð1Þ;jwð2Þ;j
¼< wð1Þ ; wð2Þ > :
The fourth step is true since Fourier basis functions are orthonormal .
4 COMPUTING THE FOURIER TRANSFORM OF A
DECISION TREE
The Fourier spectrum of a given tree can be computed efficiently by traversing the tree . This section first reviews an algorithm to do that . It discusses aggregation of the multiple spectra computed from the base classifiers of an ensemble . It also extends the technique for dealing with non Boolean class labels . Kushilevitz and Mansour [ 16 ] considered the issue of learning the low order Fourier spectrum of the target function ( represented by a Boolean decision tree ) from a data set with uniformly distributed observations . Note that the current contribution is fundamentally different from their goal . This paper does not try to learn the spectrum directly from the data . Rather , it considers the problem of computing the spectrum from the decision tree generated from the data .
4.1 Schema Representation of a Decision Path For the sake of simplicity , let us consider a Boolean decision tree as shown in Fig 1 . The Boolean class labels correspond to positive and negative instances of the concept class . We can express a Boolean decision tree as a function f : X‘ ! f0 ; 1g . The function f maps positive and negative instances to one and zero , respectively . A node in a tree is labeled with a feature xi . A downward link from the node xi is labeled with an attribute value of the ith feature . The path from the root node to a successor node represents the subset of data that satisfies the different feature values labeled along the path . These subsets of the domain are
4
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL . 18 , NO . 7 , JULY 2006 essentially similarity based equivalence classes and we shall call them schemata ( schema in singular form ) . If h is a schema , then h 2 f0 ; 1 ; g‘ , where denotes a wildcard that matches any value of the corresponding feature . For example , the path fðx3!1 x2g in Fig 1 represents the schema 0 1 , since all members of the data subset at the final node of this path take feature values 0 and 1 for x1 and x3 , respectively . We shall use the term order to represent the number of nonwildcard values in a schema . The following section describes an algorithm to extract Fourier coefficients from a tree . x1 ; x1!0
4.2 Extracting and Calculating Significant Fourier the Fourier
Fig 2 . An instance of a Boolean decision tree that shows average output values at each subtree .
Lemma 5 . For any Fourier coefficient wj whose order is greater jðxÞ ¼ 0 . If the order than the depth of a leaf node li , of wj is greater than the depth of tree , then wj ¼ 0 . x2Sli
P fðxÞ jðxÞ
Proof . The proof immediately follows from Lemma 4 . tu
Coefficients from a Tree
Considering a decision tree as a function , transform of a decision tree can be defined as :
X
X x2Sl1 fðxÞ jðxÞ
X x2 fðxÞ jðxÞ ¼ 1 j j X wj ¼ 1 j j þ 1 j j ¼ jSl1j j j fðh1Þ jðh1Þ þ jSl2j fðhnÞ jðhnÞ ; x2Sl2 fðxÞ jðxÞ þ . . . þ 1 j j x2Sln j j fðh2Þ jðh2Þ þ . . . þ jSlnj j j where denotes the complete instance space , Sli is an instance subspace which the ith leaf node li covers , and hi is a schema defined by a path to li , respectively ( Note that any path to a node in a decision tree is essentially a subspace or hyperplane ; thus , it is a schema ) .
Lemma 3 . For any Fourier basis function j , x2 jðxÞ ¼ 0 . Proof . Since Fourier basis functions form an orthogonal set ,
P
X
X jðxÞ ¼ x2 x2
0ðxÞ jðxÞ ¼ 0 :
Here , 0 is the 0th Fourier basis function , which is tu constant ( one ) for all x .
Lemma 4 . Let hi be a schema defined by the path to a leaf node li . Then , if j has a nonzero attribute value at a position where hi has no value ( wild card ) ,
X
X fðxÞ jðxÞ ¼ fðhiÞ jðxÞ ¼ 0 ; x2Sli x2Sli where Sli is the subset that hi covers .
Proof . Let j ¼ ðjinjoutÞ , where jin are features which are included hi and jout are features not in hi , respectively . Since all values for jin are fixed in hi , jinðxÞ is constant for all x 2 Sli and Sli forms redundant ( multiples of ) complete domain with respect to jout . Therefore , for a leaf node li,X
X
X fðxÞ jðxÞ ¼ x2Sli x2Sli fðhiÞ jðxÞ ¼ fðhiÞ jinðxÞ joutðxÞ
X x2Sli joutðxÞ ¼ 0 :
¼ fðhiÞ jðhiÞ x2Sli tu
Thus , for an FC wj to be nonzero , there should exist at least one schema h that has nonwild card attributes for all nonzero attributes of j . In other words , there exists a set of nonzero FCs associated with a schema h . This observation leads us to a direct way of detecting and calculating all nonzero FCs of a decision tree : For each schema h ( or path ) from the root , we can easily detect all nonzero FCs by enumerating all FCs associated with h .
2
Before describing the algorithm , we need to introduce some notations . Let hk¼i be a vector that is generated by replacing the kth position of h with value i . Note that this notation will be used for both schema and partition . Let us consider a nonleaf node n that has d children . In other words , there exist d disjoint subtrees below n . If xk is the feature appearing in n , then FxkðiÞ denotes the average function value of domain members covered by a subtree accessible through the ith child of n . For example , in Fig 2 , Fx1ð0Þ is 1 and Fx2ð1Þ is one . Note that FxkðiÞ is equivalent to the average of schema h , where h denotes the path ( from the root node ) to the ith subtree of the node where xk appears . The algorithm starts with precalculating all FxkðiÞs ( This is essentially a recursive “ Tree Visit ” operation ) . Then , it incrementally finds nonzero FCs as it traverses the tree . If we let S denote the set of partitions that correspond to nonzero FCs , initially , S ¼ f000 . . . 0g and the corresponding w0000 is calculated with the overall average of output . In Fig 2 , it is : 1 8 . The algorithm continues to extract all remaining nonzero FCs in recursive fashion from the root . New nonzero FCs are identified by inducing their correponding partitions from the existing S . For any h 2 S , when a node with the feature xk is visited , partitions hk¼1 ; ; hk¼ k 1 are added into S , where k is the cardinality of xk . For the tree in Fig 2 , S is initially f000g . Then , 010 is added to S when x1 is visited . Note that 010 is found by replacing the first position ( starting from zero ) with 1 , ie , h1¼1 ¼ 010 is obtained from h ¼ 000 . w010 is computed using ( 1 ) :
2 1 ¼ 5
4 þ 1
2 1
KARGUPTA ET AL . : ORTHOGONAL DECISION TREES
5 applied to represent an ensemble of decision trees that uses voting as its aggregation scheme . The Fourier spectrum faithfully represents functions in closed forms and ensemble classifiers are not such functions . Therefore , we need a different approach to model a multiclass decision trees with the Fourier basis . Let us consider a decision tree that has k classifications . Then , let us define =i to be the Fourier spectrum of a decision tree whose class labels are all set to zero except the ith class . In other words , we treat the tree to have a Boolean classification with respect to the ith class label . If we define fðkÞðxÞ to be a partial function that computes the inverse Fourier transform using =k , classification of an input vector x is written as : fðxÞ ¼ c1fð1ÞðxÞ þ c2fð2ÞðxÞ þ þ clfðlÞðxÞ , where each ci corresponds to a mapped value for the ith classification . Note that if x belongs to jth class , fðiÞðxÞ ¼ 1 when i ¼ j , and 0 otherwise . Now , let us consider an ensemble of decision trees in weighted linear combination form . Then , fðkÞðxÞ can ðlÞ l ðxÞ , be written as : fðkÞðxÞ ¼ a1f ðxÞ represent the weight of ith tree in the where ai and f ensemble and its partial function for the kth classification , respectively . Finally , the classification of an ensemble , of a decision tree that adopts voting as its aggregation scheme can be defined as : fðxÞ ¼ argmaxkðfðkÞðxÞÞ .
ð2Þ 2 ðxÞ þ alf
ð1Þ 1 ðxÞ þ a2f
ðkÞ i
In this section , we discussed the Fourier representation of decision trees . We showed that the Fourier spectrum of a decision tree is very compact in size . In particular , we proved that the exponential decay property is also true for a Fourier spectrum of non Boolean decision trees . In the following section , we will describe how the Fourier spectrum of an ensemble can be used to construct a single tree .
5 CONSTRUCTION OF A DECISION TREE FROM
FOURIER SPECTRUM
This section discusses an algorithm to construct a tree from the Fourier spectrum of an ensemble of decision trees . The following section first shows that the information gain needed to choose an attribute at the decision nodes can be efficiently computed from the Fourier coefficients . in a decision tree .
5.1 Schema Average and Information Gain Consider a classification problem with Boolean class labels—f0 ; 1g . Recall that a schema h denotes a path to a node nk In order to compute the information gain introduced by splitting the node using a particular attribute , we first need to compute the entropy of the class distribution at that node . We do that by introducing a quantity called schema average . Let us define the schema average function value as follows :
X
ðhÞ ¼ 1 jhj x2h fðxÞ ; where fðxÞ is the classification value of x and jhj denotes the number of members in schema h . Note that the schema average ðhÞ is nothing but the frequency of all instances of the schema h with a classification value of 1 . Similarly , note that the frequency of the tuples with classification value of 0 is ð1   ðhÞÞ . It can therefore be used to compute the entropy at the node nk .
Fig 3 . Algorithm for obtaining the Fourier spectrum of a decision tree . k in xk implies that xk is the kth feature . k denotes the cardinality of xk and jnodej denotes the size of subspace node covers . j j is the size of the complete instance space . nodei is the ith child of node . w010 ¼ 1 2 ¼ 1 2 ¼ 1 2 fð 0 Þ 010ð 0 Þ þ 1 2 Fx1ð0Þ 010ð 0 Þ þ 1 2 1 2
1 þ 1 2
1 ð 1Þ ¼ 1 4 fð 1 Þ 010ð 1 Þ Fx1ð1Þ 010ð 1 Þ
  1 2
¼   1 4
:
For x2 , f001 ; 011g will be added into S . w001 and w011 are computed similarly as w010 . The pseudocode of the algorithm is presented in Fig 3 .
4.3 Fourier Spectrum of an Ensemble Classifier The Fourier spectrum of an ensemble classifier that consists of multiple decision trees can be computed by aggregating the spectra of the individual base models . Let fðxÞ be the underlying function computed by a tree ensemble where the output of the ensemble is a weighted linear combination of the outputs of the base tree classifiers . X fðxÞ ¼ a1f1ðxÞ þ a2f2ðxÞ þ . . . þ anfnðxÞ ðnÞ j jðxÞ ; w
ð1Þ j jðxÞ þ . . . þ an w
¼ a1
X j2J1 j2Jn
P where fiðxÞ and ai are ith decision tree and its weight , respectively . Ji is set of nonzero Fourier coefficients that are P ðiÞ detected by the ith decision tree and w is a Fourier j coefficient in Ji . Now , ( 2 ) is written as : fðxÞ ¼ j2J wj jðxÞ , where wj ¼ i¼1Ji . The following section extends the Fourier spectrum based approach to represent and aggregate decision trees to domains with multiple class labels . and J ¼ [ n
ðiÞ n i¼1 aiw j
4.4 Fourier Spectrum of Multiclass Decision Trees A multiclass decision tree has k > 2 different class labels . In general , we can assume that each label is again assigned a unique integer value . Since such decision trees are also functions that map an instance vector to numeric value , the Fourier representation of such a tree is essentially not any different . However , the Fourier spectrum cannot be directly
6
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL . 18 , NO . 7 , JULY 2006 confidenceðhÞ ¼ maxððhÞ ; 1   ðhÞÞ entropyðhÞ ¼  ðhÞ log ðhÞ   ð1   ðhÞÞ logð1   ðhÞÞ : The computation of ðhÞ using the above expression for a given ensemble is not practical since we need to evaluate all x 2 h . Instead , we can use the following expression that computes ðhÞ directly from the given FS :
X
X
ðhÞ ¼
: : wð0;;p1;;pm;;0Þ exp p1 pm
2 iðp1 b1 j1
þþpm bm jm
Þ
; where h ¼ b1 b2 bm that has m nonwildcard values bi at position ji and pi 2 f0 ; 1 ; . . . ; ji   1g . A similar Walsh analysis based approach for analyzing the behavior of genetic algorithms can be found elsewhere [ 17 ] . Note that the summations in ( 3 ) are defined only for the fixed ( nonwild card ) positions that correspond to the features defining the path to the node nk . the cost
Using ( 3 ) as a tool to obtain information gain , it is relatively straight forward to come up with a version of ID3 or C4.5 like algorithms that work using the Fourier spectrum . However , a naive approach may be computationally inefficient . The computation of ðhÞ requires an exponential number of FCs with respect to the order of h . involved in computing ðhÞ increases Thus , exponentially as the tree becomes deeper . Moreover , since the Fourier spectrum of the ensemble is very compact in size , most Fourier coefficients involved in computing ðhÞ are zero . Therefore , the evaluation of ðhÞ using ( 3 ) is not only inefficient , but also involves unnecessary computations . Construction of a more efficient algorithm to compute ðhÞ is possible by taking advantage of the recursive and decomposable nature of ( 3 ) . When computing the average of an order l schema h , we can reduce some computational steps if any of the order l   1 schemata which subsumes h is already evaluated . For a simple example in the Boolean domain , let us consider the evaluation of ð 1 0 Þ . Let us also assume that ð 1 Þ is precalculated . Then , ð 1 0 Þ is obtained by simply adding w000100 and  w010100 to ð 1 Þ . This observation leads us to an efficient algorithm to evaluate schema averages . Recall that the path to a node from the root in a decision tree can be represented as a schema . Then , choosing an attribute for the next node is essentially the same as selecting the best schema among those candidate schemata that are subsumed by the current schema whose orders are just one more than that of this schema . In the following section , we describe a tree construction algorithm that is based on these observations .
5.2 Bottom Up Approach to Construct a Tree Before describing the algorithm , we need to introduce some notations . Let hk¼i and h be two schemata . The order of hk¼i is one higher than that of h . Schema hk¼i is identical to h except at one position—the kth feature is set to i ( Note that we use similar notation for ExtractFS ) . For example , consider schemata h ¼ ð 1 2Þ and h3¼1 ¼ ð 1 12Þ . Here , we use an integer number based indexing of the features feature ) . ðhÞ denotes a set of ( zero for the leftmost partitions that are required to compute ðhÞ ( See ( 3) ) . A k fixed partition is a partition with a nonzero value at the kth position . Let ðkÞ be a set of order one k fixed partitions ; ðhk¼iÞ be the partial sum of ðhk¼iÞ which only includes k fixed partitions . Now , the information gain achieved by choosing the kth feature with a given h is redefined using these new notations :
Fig 4 . Algorithm for constructing a decision tree from the Fourier spectrum ( TCFS ) .
Gainðh ; kÞ ¼ entropyðhÞ   1 k entropyðhk¼iÞ entropyðhk¼iÞ ¼  ðhk¼iÞ logððhk¼iÞÞ   ð1   ðhk¼iÞÞ
X k 1 i¼0 logð1   ðhk¼iÞÞ ðhk¼iÞ ¼ ðhÞ þ ðhk¼iÞ ðhk¼iÞ ¼
X jðhk¼iÞwj ; j2 ðhÞ ðkÞ where is the Cartesian product and k is the cardinality of the kth feature , respectively .
Now , we are ready to describe the Tree Construction from Fourier Spectrum ( TCFS ) algorithm , which essentially notes the decomposable definition of ðhk¼iÞ and focuses on computing ðhk¼iÞ s . Note that with a given h ( the current path ) , selecting the next feature is essentially identical to choosing the kth feature that achieves the maximum Gainðh ; kÞ . Therefore , the basic idea of TCFS is to associate most up to date ðhk¼iÞ s with the kth feature . In other words , when TCFS selects the next node ( after some i is chosen for hk¼i ) , hk¼i becomes the new h . Then , it identifies a set of FCs ( We call these appropriate FCs ) that are required to compute all hk¼i s for each feature and computes the corresponding entropy . This process can be considered to update each ðhk¼iÞ for the corresponding kth feature as if it were selected . The reason is that such computations are needed anyway if a feature is to be selected in the future along the current path . This is essentially updating ðhk¼iÞ s for a feature k using bottom up approach ( following the flavor of dynamic programming ) . Note that ðhk¼iÞ is , in fact , computable by adding ðhk¼iÞ to ðhÞ . Here , ðhk¼iÞ s are partial sums that only current appropriate FCs contribute to . The detection of all appropriate FCs requires a scan over the FS . However , they are removed from the FS once they are used in computation , since they are no longer needed for the calculation of higher order schemata . Thus , it takes a lot less time to compute higher order schemata ; note that it is just opposite to what we encountered in the naive implementation . The algorithm stops growing a path when either the original FS becomes an empty set or the minimum confidence level is achieved . The depth of the resulting tree can be set to a predetermined bound . A pictorial description of the algorithm is shown in Fig 6 . Pseudocode of the algorithm is presented in Figs . 4 and 5 .
The TCFS uses the same criteria to construct a tree as that of the C45 Both of them require a number of informationgain tests that grows exponentially with respect to the depth of the tree . In that sense , the asymptotic running time of TCFS is the same as that of the C45 However , while the C4.5 uses original data to compute information gains , TCFS uses a Fourier spectrum . Therefore , in practice , a comparison of the running time between the two approaches will depend on the sizes of the original data and that of the
KARGUPTA ET AL . : ORTHOGONAL DECISION TREES
7
Fig 5 . Algorithm for constructing a decision tree from the Fourier spectrum ( TCFS ) . orderðhÞ returns the order of schema h . intersectðh ; iÞ returns the feature to be updated using wi , if such a feature exists . Otherwise , it returns  .
Fourier spectrum . The following section presents an extension of the TCFS for handling non Boolean class labels .
5.3 Extension of TCFS to Multiclass Decision Trees The extension of the TCFS algorithm to multiclass problems is immediately possible by redefining the “ entropy ” function . It should be modified to capture an entropy from the multiple class labels . For this , let us first define ðiÞðhÞ to be a schema average function that uses =i ( See Section 4.4 ) only . Note that it computes the average occurrence of the ith class label in h . Then , the entropy of a schema is redefined as follows : entropyðhÞ ¼  
ðiÞðhÞ log ðiÞðhÞ ;
X k i¼1 where k is the number of class labels .
This expression can be directly used for computing the information gain to choose the decision nodes in a tree for classifying domains with non Boolean class labels .
In this section , we discussed a way to assign a confidence to a node in a decision tree and considered a method to estimate information gain using it . Consequently , we showed that a decision tree construction from the Fourier spectrum is possible . In particular , we devised the TCFS algorithm that exploits the recursive and decomposable nature of tree building process in spectrum domain , thus constructing a decision tree efficiently . In the following section , we will discuss orthogonal decision trees that can be constructed using the Fourier spectrum of the trees in an ensemble .
6 REMOVING REDUNDANCIES FROM ENSEMBLES Existing ensemble learning techniques work by combining ( usually a linear combination ) the base the output of
Illustration of the Tree Construction from Fourier Spectrum Fig 6 . ( TCFS ) algorithm . It shows the constructed tree on the left . The schemata evaluated at different orders are shown in the middle . The rightmost tree shows the splitting of the set of all Fourier coefficients used for making the process of looking up the appropriate coefficients efficient .
8
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL . 18 , NO . 7 , JULY 2006 classifiers . They do not structurally combine the classifiers themselves . As a result , they often share a lot of redundancies . The Fourier representation offers a unique way to fundamentally aggregate the trees and perform further analysis to construct an efficient representation . Let feðxÞ be the underlying function representing the ensemble of m different decision trees , where the output is a weighted linear combination of the outputs of the base classifiers . Then , we can write ,
X feðxÞ ¼ 1 ð1ÞðxÞ þ 2 ð2ÞðxÞ þ þ m ðmÞðxÞ wðmÞ;j j ðxÞ þ þ m
¼ 1 wð1Þ;j
X j ðxÞ ; j2J 1 j2J m m
P
P j2J wðeÞ;j where i is the weight of the ith decision tree and J i is the set of all partitions with nonzero Fourier coefficients in its spectrum . Therefore , feðxÞ ¼ j ðxÞ , where i¼1J i . Therefore , the FourwðeÞ;j ¼ i¼1 iwðiÞ;j and J ¼ [ m ier spectrum of feðxÞ ( a linear ensemble classifier ) is simply the weighted sum of the spectra of the member trees . Consider the matrix D where Di;j ¼ ðjÞðxiÞ , where ðjÞðxiÞ is the output of the tree ðjÞ for input xi 2 . D is an j j m matrix where j j is the size of the input domain and m is the total number of trees in the ensemble .
An ensemble classifier that combines the outputs of the base classifiers can be viewed as a function defined over the set of all rows in D . If D ;j denotes the jth column matrix of D , then the ensemble classifier can be viewed as a function of D ;1 ; D ;2 ; D ;m . When the ensemble classifier is a linear combination of the outputs of the base classifiers , we have F ¼ 1D ;1 þ 2D ;2 þ mD ;m , where F is the column matrix of the overall ensemble output . Since the base classifiers may have redundancy , we would like to construct a compact low dimensional representation of the matrix D . However , explicit construction and manipulation of the matrix D is difficult , since most practical applications deal with a very large domain . We can try to construct an approximation of D using only the available training data . One such approximation of D and its Principal Component Analysis based projection is reported elsewhere [ 18 ] . Their technique performs PCA of the matrix D , projects the data in the representation defined by the eigenvectors of the covariance matrix of D , and then performs linear regression for computing the coefficients 1 ; 2 ; ; and m .
While the approach is interesting , it has a serious limitation . First of all , the construction of an approximation of D even for the training data is computationally prohibiting for most large scale data mining applications . Moreover , this is an approximation since the matrix is computed only over the observed data set of the entire domain . In the following , we demonstrate a novel way to perform a PCA of the matrix containing the Fourier spectra of trees . The approach works without explicitly generating the matrix D . It is important to note that the PCA based regression scheme [ 18 ] offers a way to find the weightage for the members of the ensemble . It does not offer any way to aggregate the tree structures and construct a new representation of the ensemble which the current approach does .
The following analysis will assume that the columns of the matrix D are mean zero . This restriction can be easily removed with a simple extension of the analysis . Note that the covariance of the matrix D is DT D . Let us denote this covariance matrix by C . The ði ; jÞth entry of the matrix ,
Ci;j ¼< Dð ; iÞ ; Dð ; jÞ >¼< ðiÞðxÞ ; ðjÞðxÞ >
X
¼ wðiÞ;pwðjÞ;p ¼< wðiÞ ; wðjÞ > :
ð4Þ p implies that
The fourth step is true by Lemma 2 . Now , let us consider the matrix W , where Wi;j is the coefficient corresponding to the ith member of the partition set J from the spectrum of the tree ðjÞ . Equation ( 4 ) the covariance matrices of D and W are identical . Note that W is an jJ j m dimensional matrix . For most practical applications , jJj << j j . Therefore , analyzing W using techniques like PCA is significantly easier . The following discourse outlines a PCA based approach . PCA of the covariance matrix of W produces a set of eigenvectors V1 ; V2 ; Vk . The eigenvalue decomposition constructs a new representation of the underlying domain . Note that since the eigenvectors are nothing but a linear combination of the original column vectors of W , each of them also form a Fourier spectrum and we can reconstruct a decision tree from this spectrum . Moreover , since they are orthogonal to each other , the tree constructed from them also maintain , the orthogonality condition and , therefore , they are redundancy free . They define a basis set and can be used to represent any given decision tree in the ensemble in the form of a linear combination . Orthogonal decision trees can be defined as an immediate extension of this framework . A pair of decision trees f1ðxÞ and f2ðxÞ are orthogonal to each other if and only if < faðxÞ ; fbðxÞ >¼ 0 when a 6¼ b and < faðxÞ ; fbðxÞ >¼ 1 otherwise . The second condition is actually a slightly special case of orthogonal functions —orthonormal condition . A set of trees are pairwise orthogonal if every possible pair of members of this set satisfy the orthogonality condition . that spans the entire function space of
The orthogonality condition guarantees that the representation is not redundant . These orthogonal trees form a basis set the ensemble . The overall output of the ensemble is computed from the output of these orthogonal trees . Specific details of the ensemble output computation depends on the adopted technique to compute the overall output of the original ensemble . However , for most popular cases considered here , it boils down to computing the average output . If we choose to go for weighted average , we may need to compute the coefficients corresponding to each Vq using linear regression or other similar techniques .
7 EXPERIMENTAL RESULTS
This section reports the experimental performance of orthogonal decision trees on the following data sets—SPECT , NASDAQ , DNA , House of Votes , and Contraceptive Method Usage Data . For each data set , the following three experiments are performed using known classification techniques :
1 . C4.5 : The C4.5 classifier is built on training data and validated over test data .
2 . Bagging : A popular ensemble classification technithe classification is used to test que , bagging , accuracy of the data set .
3 . Random Forest : Random forests are built on the training data , using approximately half the number of features in the original data set . The number of
KARGUPTA ET AL . : ORTHOGONAL DECISION TREES
9 trees in the forest is identical to that used in the bagging experiment.1
We then perform another set of experiments for comparing the techniques described in the previous sections in terms of error in classification and tree complexity .
1 . Reconstructed Fourier Tree ( RFT ) : The training set is uniformly sampled , with replacement and C4.5 trees built on each sample . The Fourier representation of each individual tree is obtained , preserving a certain percentage ( eg , 90 percent ) of the energy . This representation of a tree is used to reconstruct a decision tree using the TCFS algorithm described in Section 5 . The performance of a reconstructed Fourier tree is compared with the original C4.5 tree . The error in classification and tree complexity of each of the reconstructed trees is reported . The purpose of this experiment is to study the effect of “ noise removal ” from the ensemble on its classification accuracy by going to the Fourier domain and then coming back to the tree domain using the TCFS algorithm .
2 . Aggregated Fourier Tree ( AFT ) : The training set is uniformly sampled , with replacement and C4.5 decision trees built on each sample ( This is identical to bagging ) . A Fourier representation of each tree is obtained ( preserving a certain percentage of the total energy ) , and these are aggregated with uniform weighting to obtain the spectrum of an Aggregated Fourier Tree ( AFT ) . The AFT is reconstructed using the TCFS algorithm described before and the classification accuracy and the tree complexity of this aggregated Fourier tree is reported .
3 . Orthogonal Decision Trees : The matrix containing the Fourier coefficients of the decision trees is subjected to principal component analysis . Orthogonal trees are built using the corresponding eigenvectors . In most cases , it is found that the first principal eigenvector captures most of the variance and , thus , the orthogonal decision tree constructed from this eigenvector is of particular interest . We report the error in classification and tree complexity of the orthogonal decision tree obtained from the most dominant eigenvector . We also perform experiments where we keep k2 significant eigenvectors . The trees are combined by weighting them according to the coefficients obtained from a Least Square Regression . Each orthogonal decision tree is weighted using coefficients calculated from Least Square Regression . For this , we allow all the orthogonal decision trees to individually produce their classification on the test set . Thus , each ODT produces a column vector of its classification estimate . Since the class labels in the test set are already known , we use the least square regression to obtain the weights to assign to each ODT . The accuracy of the orthogonal decision trees is reported as ODT LR ( ODTs combined using Least Square Regression ) .
In addition to reporting the error in classification , we also report the tree complexity , the total number of nodes in
1 . We used the WEKA implementation ( http://wwwcswaikatoacnz/ ml/weka/ ) of Bagging and Random Forests .
2 . We select the value of k in such a manner that the total variance captured is more than 90 percent . One could potentially do cross validation to obtain a suitable value of k as pointed out in [ 19 ] , but this is beyond the current scope of the work and will be explored in future . the tree . Similarly , the term ensemble complexity reflects the total number of nodes in all the trees in the ensemble . A smaller ensemble tree complexity implies a compact representation of an ensemble and , therefore , it is desirable . Our experiments show that ODTs usually offer significantly reduced ensemble tree complexity without any reduction in the accuracy . The following section presents the results for the SPECT data set .
7.1 SPECT Data Set This section illustrates the idea of orthogonal decision trees using a well known binary data set . The data set , available from the University of California at Irvine , Machine Learning Repository , describes the diagnosing of cardiac Single Proton Emission Computed Tomography ( SPECT ) images into two categories , normal or abnormal . The database of 267 SPECT image sets ( patients ) is processed to extract features that summarize the original SPECT images . As a result , 44 continuous feature patterns are obtained for each patient , which are further processed to obtain 22 binary feature patterns . The training data set consists of 80 instances and 22 attributes . All the features are binary , and the class label is also binary ( depending on whether a patient is deemed normal or abnormal ) . The test data set consists of 187 instances and 22 attributes .
Table 1a shows the error percentage obtained in each of the different classification schemes . The root mean squared error for the 10 fold cross validation in the C4.5 experiment is found to be 0.4803 and the standard deviation is 23862 For Bagging , the number of trees in the ensemble is chosen to be 40 . Our experiments reveal that further increase in number of trees in the ensemble causes a decrease in accuracy of classification of the ensemble possibly due to overfitting of the data .
Fig 7a compares the accuracy of
For experiments with Random Forests , a forest of 40 trees , each constructed while considering 12 random features , is built . The average Out of bag error is reported to be 03245 the original C4.5 the Reconstructed Fourier Tree ensemble with that of ( RFT ) ensemble preserving 90 percent of the energy of the spectrum . The results reveal that if all of the spectrum is preserved , the accuracy of the original C4.5 tree and RFT are identical . When the higher order Fourier coefficients are removed , this becomes equivalent to pruning a decision tree . This explains the higher accuracy of the reconstructed Fourier tree preserving 90 percent of the energy of the spectrum . Fig 7b compares the tree complexity of the original C4.5 ensemble with that of the RFT ensemble .
In order to construct the orthogonal decision trees , the coefficient matrix is projected onto the first 15 most significant principal components . The most significant principal component captures 85.1048 percent of the variance and the tree complexity of the ODT constructed from this component is 17 with an accuracy of 91.97 percent . Fig 8 shows the variance captured by all the 15 principal components .
Tables 1a and 1b present the accuracy and the treecomplexity for this data set , respectively . The orthogonal trees are found to be smaller in complexity , thus reducing the complexity of the ensemble .
7.2 NASDAQ Data Set The NASDAQ data set is a semisynthetic data set with 1,000 instances and 100 discrete attributes . The original data set has three years of NASDAQ stock quote data . It is
10
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL . 18 , NO . 7 , JULY 2006
( a ) Classification Error and ( b ) Tree Complexity for SPECT Data
TABLE 1
Fig 7 . The accuracy and tree complexity of C4.5 and RFT for SPECT data . preprocessed and transformed to discrete data by encoding percentages of changes in stock quotes between consecutive days . For these experiments , we assign four discrete values , that denote levels of changes . The class labels predict whether the Yahoo stock is likely to increase or decrease based on attribute values of the 99 stocks . We randomly
Fig 8 . Percentage of variance captured by principal components for SPECT Data . select 200 instances for training and the remaining 800 instances forms the test data set .
Table 2a illustrates the classification accuracies of different experiments performed on this data set . The root mean squared error for the 10 fold cross validation in the C4.5 experiment is found to be 0.4818 and the standard deviation is 22247 C4.5 has the best classification accuracy , though the tree built has the highest tree complexity also . For the bagging experiment , C4.5 trees are built on the data set , such that the size of each bag ( used to build the tree ) as a percentage of the data set is 40 percent . Also , a Random Forest of 60 trees , each constructed while considering 50 random features , is built on the training data and tested with the test data set . The average out of bag error is reported to be 03165
Fig 9a compares the accuracy of the original C4.5 ensemble with that of the Reconstructed Fourier Tree ( RFT ) ensemble preserving 90 percent of the energy of the spectrum . Fig 9b compares the tree complexity of the original C4.5 ensemble with that of the RFT ensemble .
For the orthogonal trees , we project the data along the first 10 most significant principal components . Fig 10 illustrates the percentage of variance captured by the 10 most significant principal components .
Table 2b presents the tree complexity information for this set of experiments . Both the aggregated Fourier tree and the orthogonal trees performed better than the single
KARGUPTA ET AL . : ORTHOGONAL DECISION TREES
11
( a ) Classification Error and ( b ) Tree Complexity for NASDAQ Data
TABLE 2
Fig 9 . The accuracy and tree complexity of C4.5 and RFT for Nasdaq data .
C4.5 tree or bagging . The tree complexity result appears to be quite interesting . While a single C4.5 tree had 29 nodes in it , the orthogonal tree from the first principal component requires just three nodes , which is clearly a much more compact representation . butes of which 49.16 percent belongs to class 0 while the remaining 50.84 percent belongs to the class 1 . Table 3a reports the classification error . The root mean squared error for the 10 fold cross validation in the C4.5 experiment is found to be 0.2263 and the standard deviation is 06086
7.3 DNA Data Set The DNA data set3 is a processed version of the corresponding data set available from the UC Irvine repository . The processed StatLog version replaces the symbolic attribute values representing the nucleotides ( only A,C,T,G ) by three binary indicator variables . Thus , the original 60 symbolic attributes are changed into 180 binary attributes . The nucleotides A,C,G,T are given indicator values as follows : A ¼ 100 , C ¼ 010 , G ¼ 001 , and T ¼ 000 . The data set has three class values 1 , 2 , and 3 corresponding to exon intron boundaries ( sometimes called acceptors ) , intron exon boundaries ( sometimes called donors ) , and the case when neither is true . We further process the data such that , ie , class 1 represents either donors or acceptors , while class 0 represents neither . The training set consists of 2,000 instances and 180 attributes of which 47.45 percent belongs to class 1 , while the remaining 52.55 percent belongs to class 0 . The test data set consists of 1,186 instances and 180 attri there are are only two class labels ,
3 . Obtained from http://wwwliaccuppt/ML/statlog/data sets/dna .
It may be interesting to note that five eigenvectors are used in this experiment . Fig 11 shows the variance captured by these components . As before , the redundancy free trees are combined by the weights obtained from Least Square Regression . Table 3b reports the tree complexity for this data set . the first
Fig 12a compares the accuracy of the original C4.5 ensemble with that of the Reconstructed Fourier Tree ( RFT ) ensemble preserving 90 percent of the spectrum . Fig 12b compares the tree complexity of the original C4.5 ensemble with that of the RFT ensemble . the energy of
7.4 House of Votes Data The 1984 United States Congressional Voting Records Database is obtained from the University of California , Machine Learning Repository . This data set includes votes for each of the US House of Representatives Congressmen on the 16 key votes identified by the CQA including water project cost sharing , adoption of budget resolution , mxmissile , immigration , etc . It has 435 instances , 16 Boolean valued attributes , and a binary class label ( democrat or
12
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL . 18 , NO . 7 , JULY 2006
Fig 10 . Percentage of variance captured by principal components for
Nasdaq Data .
Fig 11 . Percentage of variance captured by principal components for DNA Data . republican ) . Our experiments use the first 335 instances for training and the remaining 100 instances for testing . In our experiments , missing values in the data are replaced by one . The results of classification are shown in Table 4a while the tree complexity is shown in Table 4b . The root mean squared error for the 10 fold cross validation in the C4.5 experiment is found to be 0.2634 and the standard deviation is 03862 For Bagging , 15 trees are constructed using the data set , since this produced the best classification results . The size of each bag was 20 percent of the training data set . Random Forest of 15 trees , each constructed by considering eight random features produces an average out of bag error of 005502 The accuracy of classification and the tree complexity of the original C4.5 and RFT ensemble are illustrated in Fig 13a and Fig 13b , respectively .
For orthogonal trees , the coefficient matrix is projected onto the first five most significant principal components . Fig 14a illustrates the amount of variance captured by each of the principal components .
7.5 Contraceptive Method Usage Data This data set is obtained from the University of California Irvine , Machine Learning Repository and is a subset of the 1987 National Indonesia Contraceptive Prevalence Survey . The samples were married women who were either not pregnant or did not know if they were at the time of interview . The problem is to predict the current contra ceptive method choice of a woman based on her demographic and socio economic characteristics . There are 1,473 instances and 10 attributes including a binary class label . All attributes are processed so that they are binary . Our experiments use 1,320 instances for the training set while the rest form the test data set .
The results of classification are tabulated in Table 5a while Table 5b shows the tree complexity . The root mean squared error for the 10 fold cross validation in the C4.5 experiment is found to be 0.5111 and the standard deviation is 18943 For a Random Forest built with 10 trees , considering five random features produces an average error in classification of about 45.88 percent and an average out of bag error of 042556 Fig 15a compares the accuracy of the original C4.5 ensemble with that of the Reconstructed Fourier Tree ( RFT ) ensemble preserving 90 percent of the energy of the spectrum . Fig 15b compares the tree complexity of the original C4.5 ensemble with that of the RFT ensemble .
For ODTs , the data is projected along the first ten principal components . Fig 14b shows the amount of variance captured by each principal component . It is interesting to note that the first principal component captures only about 61.85 percent of the variance and , thus , the corresponding ODT generated from the first principal component has a relatively high tree complexity .
( a ) Classification Error and ( b ) Tree Complexity for DNA Data
TABLE 3
KARGUPTA ET AL . : ORTHOGONAL DECISION TREES
13
Fig 12 . The accuracy and tree complexity of C4.5 and RFT for DNA data .
Fig 13 . The accuracy and tree complexity of C4.5 and RFT for House of Votes data .
( a ) Classification Error and ( b ) Tree Complexity for House of Votes Data
TABLE 4
8 CONCLUSIONS This paper introduced the notion of orthogonal decision trees and offered a methodology to construct them . Orthogonal decision trees are functionally orthogonal to each other and they provide an efficient redundancy free representation of large ensembles that are frequently produced by techniques like Boosting [ 2 ] , [ 3 ] , Bagging [ 4 ] ,
Stacking [ 5 ] , and Random Forests [ 6 ] . The proposed technique is also likely to be very useful in ensemble based mining of distributed [ 10 ] and stream data [ 7 ] , [ 8 ] .
14
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL . 18 , NO . 7 , JULY 2006
Fig 14 . Percentage of variance captured by principal components for ( a ) House of Votes data and ( b ) Contraceptive Method Usage data .
( a ) Classification Error and ( b ) Tree Complexity for Contraceptive Method Usage Data
TABLE 5
Fig 15 . The accuracy and tree complexity of C4.5 and RFT for Contraceptive Method Usage data .
The proposed approach exploits the earlier work done by Kargupta et al . [ 20 ] , [ 9 ] , which showed that the Fourier transform of decision trees can be efficiently computed . This work shows that we can compute the tree back from its
Fourier spectrum . The paper also offered a collection of new results regarding the properties of the multivariate Fourier spectrum of decision trees . Although the paper considers the Fourier representation , this is clearly not the only
KARGUPTA ET AL . : ORTHOGONAL DECISION TREES
15 available linear representation around . However , our work shows that is particularly suitable for representing it decision trees .
This work also opens up several new possibilities . Linear systems theory offers many tools for analyzing properties like stability and convergence . For example , eigenvalues of a linear system are directly associated with the stability of the system . Similar concepts may be useful in understanding the behavior of large ensembles . We plan to explore these issues in the future .
[ 17 ] D . Goldberg , “ Genetic Algorithms and Walsh Functions : Part I , a Gentle Introduction , ” Complex Systems , vol . 3 , no . 2 , pp . 129 152 , 1989 .
[ 18 ] CJ Merz and MJ Pazzani , “ A Principal Components Approach to Combining Regression Estimates , ” Machine Learning , vol . 36 , nos . 1 2 , pp . 9 32 , 1999 .
[ 19 ] C . Merz and M . Pazzani , “ A Principal Components Approach to Combining Regression Estimates , ” Machine Learning , vol . 36 , pp . 932 , 1999 .
[ 20 ] H . Kargupta , B . Park , D . Hershberger , and E . Johnson , “ Collective Data Mining : A New Perspective towards Distributed Data Mining , ” Advances in Distributed and Parallel Knowledge Discovery , H . Kargupta and P . Chan , eds . , AAAI/MIT Press , 2000 .
ACKNOWLEDGMENTS
The authors acknowledge supports from the US National Science Foundation CAREER award IIS 0093353 , US National Science Foundation grant IIS 0203958 , and NASA grant NAS2 37143 . The work of B H Park was partially funded by the Scientific Data Management Center ( http:// sdmcenterlblgov ) under the Department of Energy ’s Scientific Discovery through Advanced Computing ( DOE SciDAC ) program ( http://wwwscidacorg ) H . Kargupta is also affiliated to Agnik , LLC . , Columbia , MD . A four page version of this paper was published in the Proceedings of the 2004 IEEE International Conference on Data Mining .
REFERENCES [ 1 ]
JR Quinlan , “ Induction of Decision Trees , ” Machine Learning , vol . 1 , no . 1 , pp . 81 106 , 1986 .
[ 2 ] Y . Freund , “ Boosting a Weak Learning Algorithm by Majority , ”
Information and Computation , vol . 121 , no . 2 , pp . 256 285 , 1995 .
[ 3 ] H . Drucker and C . Cortes , “ Boosting Decision Trees , ” Advances in
Neural Information Processing Systems , vol . 8 , pp . 479 485 , 1996 .
[ 4 ] L . Breiman , “ Bagging Predictors , ” Machine Learning , vol . 24 , no . 2 , pp . 123 140 , 1996 .
[ 5 ] D . Wolpert , “ Stacked Generalization , ” Neural Networks , vol . 5 , pp . 241 259 , 1992 .
[ 6 ] L . Breiman , “ Random Forests , ” Machine Learning , vol . 45 , no . 1 , pp . 5 32 , 2001 .
[ 7 ] W . Fan , S . Stolfo , and J . Zhang , “ The Application of Adaboost for Distributed , Scalable , and On Line Learning , ” Proc . Fifth ACM SIGKDD Int’l Conf . Knowledge Discovery and Data Mining , 1999 .
[ 8 ] WN Street and Y . Kim , “ A Streaming Ensemble Algorithm ( Sea ) for Large Scale Classificaiton , ” Proc . Seventh ACM SIGKDD Int’l Conf . Knowledge Discovery and Data Mining , 2001 .
[ 9 ] H . Kargupta and B . Park , “ A Fourier Spectrum Based Approach to Represent Decision Trees for Mining Data Streams in Mobile Environments , ” IEEE Trans . Knowledge and Data Eng . , vol . 16 , no . 2 , pp . 216 229 , 2002 .
[ 10 ] B . Park , A . R , and H . Kargupta , “ A Fourier Analysis Based Approach to Learn Classifier from Distributed Heterogeneous Data , ” Proc . First SIAM Int’l Conf . Data Mining , 2001 .
[ 11 ] BH Park and H . Kargupta , “ Constructing Simpler Decision Trees from Ensemble Models Using Fourier Analysis , ” Proc . Seventh Workshop Research Issues in Data Mining and Knowledge Discovery , pp . 18 23 , 2002 .
[ 12 ] F . Chung , Spectral Graph Theory . Providence , RI : Am . Math . Soc . ,
1994 .
[ 13 ] H . Kargupta and B . Park , “ Mining Time Critical Data Stream Using the Fourier Spectrum of Decision Trees , ” Proc . IEEE Int’l Conf . Data Mining , pp . 281 288 , 2001 .
[ 14 ] H . Kargupta , B . Park , S . Pittie , L . Liu , D . Kushraj , and K . Sarkar , “ Mobimine : Monitoring the Stock Market from a PDA , ” ACM SIGKDD Explorations , vol . 3 , no . 2 , pp . 37 46 , Jan . 2002 .
[ 15 ] N . Linial , Y . Mansour , and N . Nisan , “ Constant Depth Circuits , Fourier Transform , and Learnability , ” J . ACM , vol . 40 , pp . 607 620 , 1993 .
[ 16 ] E . Kushilevitz and Y . Mansour , “ Learning Decision Trees Using the Fourier Spectrum , ” SIAM J . Computing , vol . 22 , no . 6 , pp . 13311348 , 1993 .
Hillol Kargupta received the PhD degree in computer science from the University of Illinois at Urbana Champaign in 1996 . He is an associate professor in the Department of Computer Science and Electrical Engineering , University of Maryland , Baltimore County . He is also a cofounder of Agnik LLC , an ubiquitous intelligence company . His research interests include mobile and distributed data mining and the computation in biological processes of gene expression . Dr . Kargupta won a US National Science Foundation CAREER award in 2001 for his research on ubiquitous and distributed data mining . He , along with his coauthors , received the best paper award at the 2003 IEEE International Conference on Data Mining for a paper on privacy preserving data mining . He won the 2000 TRW Foundation Award and the 1997 Los Alamos Award for Outstanding Technical Achievement . His research has been funded by the US National Science Foundation , the US Air Force , the US Department of Homeland Security , NASA , and various other organizations . He has published more than 90 peer reviewed articles in journals , conferences , and books . He has coedited two books : Advances in Distributed and Parallel Knowledge Discovery and Data Mining : Next Generation Challenges and Future Directions , both published by AAAI/MIT Press . He is an associate editor of the IEEE Transactions on Knowledge and Data Engineering and the IEEE Transactions on Systems , Man , and Cybernetics , Part B . He regularly serves in the organizing and program committee of many data mining conferences . More information about him can be found at http://wwwcsumbcedu/~hillol He is a senior member of the IEEE .
Byung Hoon Park received the MS and PhD degrees in computer science , both from Washington State University in 1996 and 2001 , respectively . He is currently a research scientist at the Computer Science and Mathematics Division of the Oak Ridge National Laboratory ( ORNL ) . His research areas include distributed data mining , computational biology , genetic computing , data stream analysis , and text mining . His research activities have been supported by the Genomes to Life program of the US Department of Energy ( DOE ) , Scientific Data Management ( SDM ) of DOE SciDAC program , and the Biodefense Knowledge Center projects of the US Department of Homeland Security . Before joining the ORNL , Dr . Park was with the University of Maryland Baltimore County as a postdoctoral research associate , where he was involved in a NASA EOS distributed data mining project . He served on the program committees of several data mining conferences and workshops . He also serves as a reviewer of numerous journals and conferences .
Haimonti Dutta received the BS degree in computer science from Jadavpur University , Kolkata , India , in 1999 and the MS degree in computer and information science from Temple University , Philadelphia , in 2002 . She worked for a year as a Software Consultant at iGate Global Solutions , Bangalore . She is currently a PhD student in the Department of Computer Science and Electrical Engineering at the University of Maryland , Baltimore County . Her research interests include distributed data mining , data stream monitoring , grid mining , and medical informatics .
