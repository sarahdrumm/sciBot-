Privacy Preserving Outlier Detection
Jaideep Vaidya
Rutgers University
180 University Avenue Newark , NJ 07102 1803 jsvaidya@rbsrutgersedu
Chris Clifton
Purdue University
250 N . University St .
W . Lafayette , IN 47907 2066 clifton@cspurdueedu
Abstract
Outlier detection can lead to the discovery of truly unexpected knowledge in many areas such as electronic commerce , credit card fraud and especially national security . We look at the problem of finding outliers in large distributed databases where privacy/security concerns restrict the sharing of data . Both homogeneous and heterogeneous distribution of data is considered . We propose techniques to detect outliers in such scenarios while giving formal guarantees on the amount of information disclosed .
1 . Introduction
Advances in information technology and the ubiquity of networked computers have made personal information much more available . This has lead to a privacy backlash . Unfortunately , “ data mining ” has been the whipping boy for much of this backlash , witness a United States Senate proposal to forbid all “ data mining activity ” by the US Department of Defense[7 ] . Much of this is based on a mistaken view of data mining ; the above cited act specifically discusses search for individuals . Most data mining is not about individuals , but generalizing information .
Data mining does raise legitimate privacy concerns ; the process of data mining often results in greater integration of data , increasing potential for misuse . If the data mining results do not pose an inherent privacy threat , privacypreserving data mining techniques enable knowledge discovery without requiring disclosure of private data . Privacypreserving methods have been developed for numerous data mining tasks ; many are described in [ 13 ] .
To our knowledge , privacy preserving outlier detection has not yet been addressed . Outlier detection has wide application ; one that has received considerable attention is the search for terrorism . Detecting previously unknown suspicious behavior is a clear outlier detection problem . The search for terrorism has also been the flash point for at tacks on data mining by privacy advocates ; the US Terrorism Information Awareness program was killed for this reason[18 ] .
Outlier detection has numerous other applications that also raise privacy concerns . Mining for anomalies has been used for network intrusion detection[1 , 17 ] ; privacy advocates have responded with research to enhance anonymity[20 , 10 ] . Fraud discovery in the mobile phone industry has also made use of outlier detection[6 ] ; organizations must be careful to avoid overstepping the bounds of privacy legislation[5 ] . Privacy preserving outlier detection will ensure these concerns are balanced , allowing us to get the benefits of outlier detection without being thwarted by legal or technical counter measures .
This paper assumes data is distributed ; the stewards of the data are allowed to use it , but disclosing it to others is a privacy violation . The problem is to find distance based outliers without any party gaining knowledge beyond learning which items are outliers . Ensuring that data is not disclosed maintains privacy , ie , no privacy is lost beyond that inherently revealed in knowing the outliers . Even knowing which items are outliers need not be revealed to all parties , further preventing privacy breaches .
The approach duplicates the results of the outlier detection algorithm of [ 14 ] . The idea is that an object O is an outlier if more than a percentage p of the objects in the data set are farther than distance dt from O . The basic idea is that parties compute the portion of the answer they know , then engage in a secure sum to compute the total distance . The key is that this total is ( randomly ) split between sites , so nobody knows the actual distance . A secure protocol is used to determine if the actual distance between any two points exceeds the threshold ; again the comparison results are randomly split such that summing the splits ( over a closed field ) results in a 1 if the distance exceeds the threshold , or a 0 otherwise .
For a given object O , each site can now sum all of its shares of comparison results ( again over the closed field ) . When added to the sum of shares from other sites , the result
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE is the correct count ; all that remains is to compare it with the percentage threshold p . This addition/comparison is also done with a secure protocol , revealing only the result : if O is an outlier .
We first discuss the problem we are facing : the different problems posed by vertically and horizontally partitioned datasets , and the formal definition of outlier detection . Section 3 gives privacy preserving algorithms for both horizontally and vertically partitioned data . We prove the security of the algorithms in Section 4 , and discuss the computational and communication complexity of the algorithms in Section 5 . We conclude with a discussion of areas for further work on this problem .
2 . Data Partitioning Models
The problem as we define it is that the data is inherently distributed ; it is sharing ( or disclosure to other parties ) that violates privacy . The way the data is distributed / partitioned results in very different solutions . We consider two different data partitions : horizontal and vertical . In either case , assume k different parties , P0 , . . . , Pk−1 ; m attributes ; and n total objects . We now describe the specifics of the different data models considered .
21 Horizontally Partitioned Data
With horizontally partitioned ( viz . distributed homogeneous ) data , different parties collect the same information ( features ) for different objects . Each party collects information about m attributes , A1 , . . . , Am . Party Pi coli=0 ni = n lects information about ni objects , such that ( different parties collect information about different entities ) . Consider the case of several banks that collect similar data about credit card transactions but for different clients . Clearly , the data is horizontally partitioned . Outlier detection is particularly useful in this case to determine potentially fraudulent transactions .
k−1
22 Vertically Partitioned Data
k−1
With vertically partitioned ( viz . distributed heterogeneous ) data , different parties collect different features for the same set of objects . Party Pi collects information about mi attributes , Ai,1 , . . . , Ai,mi . The total number of atp=0 mp = m . All of the parties also hold intributes , formation about the same n objects . Thus , there are a total of n transactions ( with data for each transaction really being split between the parties ) . Consider the case of an airline , banking institution and federal databases . By cross correlating information and locating outliers we may hope to spot potential terrorist activities .
23 Outlier Detection
Our goal is to find Distance Based Outliers . Knorr and Ng [ 14 ] define the notion of a Distance Based outlier as follows : An object O in a dataset T is a DB(p,dt) outlier if at least fraction p of the objects in T lie at distance greater than dt from O . Other distance based outlier techniques also exist[15 , 19 ] . The advantages of distance based outliers are that no explicit distribution needs to be defined to determine unusualness , and that it can be applied to any feature space for which we can define a distance measure . We assume Euclidean distances , although the algorithms are easily extended to general Minkowski distances . There are other non distance based techniques for finding outliers as well as significant work in statistics [ 2 ] , but we do not consider those in this paper and leave that for future work .
3 . Privacy Preserving Outlier Detection
We now present two algorithms for Distance Based Outliers meeting the definition given in Section 23 The first is for horizontally partitioned data , the second for vertically partitioned data . The algorithm is based on the obvious one : Compare points pairwise and count the number exceeding the distance threshold . The key is that all intermediate computations ( such as distance comparisons ) leave the results randomly split between the parties involved ; only the final result ( if the count exceeds p % ) is disclosed .
The pairwise comparison of all points may seem excessive , but early termination could disclose information about relative positions of points ( this will be discussed further in Section 5 . ) The asymptotic complexity still equals that of [ 14 ] .
Note that to obtain a secure solution , all operations are carried out modulo some field . We will use the field D for distances , and F for counts of the number of entities . The field F must be over twice the number of objects . Limits on D are based on maximum distances ; details on the size are given with each algorithm .
31 Horizontally Partitioned Data
The key idea behind the algorithm for horizontally partitioned data is as follows . For each object i , the protocol iterates over every other object j . If the party holding i also holds j , it can easily find the distance and compare against the threshold . If two different parties hold the two objects , the parties engage in a distance calculation protocol ( Section 311 ) to get random shares of the distance . A second protocol allows comparing the shares with the threshold , returning 1 if the distance exceeds the threshold , or 0 if it does not . The key to this second protocol is that the 1 or 0 is ac
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE tually two shares r(mod F ) . From one share , the party learns nothing . s , such that r q and r q + r s = 1 ( or 0 )
Once all points have been compared , the parties sum their shares . Since the shares add to 1 for distances exceeding the distance threshold , and 0 otherwise , the total sum ( mod F ) is the number of points for which the distance exceeds the threshold . The parties do not actually compute this sum ; instead all parties pass their ( random ) shares to a designate to add , and the designated party and the party holding the point engage in a secure protocol that reveals only if the sum of the shares exceeds p % . This ensures that no party learns anything except whether the point is an outlier .
Algorithm 1 gives the complete details . Steps 5 23 are the pairwise comparison of two points , giving each party random shares of a 1 ( if the points are far apart ) or 0 ( if the points are within the distance threshold dt ) . The random split of shares ensures that nothing is learned by either party . In steps 25 28 , the parties ( except the party Pq holding the object being evaluated ) sum their shares . Again , since each share is a random split ( and Pq holds the other part of the split ) , no party learns anything . Finally , Pq−1 and Pq add and compare their shares , revealing only if the object oi is an outlier . Note that the shares of this comparison are split , and could be sent to any party ( Pq in Algorithm 1 , but it need not even be one of the Pr ) . Only that party ( eg , a fraud prevention unit ) learns if oi is an outlier , the others learn nothing .
311 Computing distance between two points
Step 13 of Algorithm 1 requires computing a distance , but leaving random shares of that distance with two parties rather than revealing the result . For convenience , we actually compute shares of the square of the distance , and compare with the square of the threshold . ( This does not change the result , since squaring is a monotonically increasing function . ) We now give an algorithm based on secure scalar product for computing shares of the square of the Euclidean distance .
Formally , let there be two parties , P1 and P2 . All computations are over a field D larger than the square of the maximum distance . P1 ’s input is the point X , P2 ’s input is the point Y . The outputs are r1 and r2 respectively ( independently uniformly distributed over D ) , such that r1 + r2 = Distance2(X , Y ) ( mod D ) , where Distance(X , Y ) is the Euclidean distance between the points X and Y .
Let there be m attributes , and a point X be represented by its m dimensional tuple ( x1 , . . . , xm ) . Each co ordinate represents the value of the point for that attribute .
The square of the Euclidean distance between X and Y
Algorithm 1 Finding DB(p,D) outliers Require : k parties , P0 , . . . , Pk−1 ; each holding a subset of the objects O . squared , F larger than |O|
Require : Fields D larger than the maximum distance 1 : for all objects oi ∈ O {Let Pq be the party holding oi}
2 : 3 : 4 : 5 : 6 : 7 :
8 : 9 : 10 : 11 : 12 :
13 :
14 :
15 : 16 : 17 : 18 : 19 : 20 : 21 : 22 : 23 : 24 : 25 : 26 : 27 : 28 : 29 : 30 : 31 : 32 : 33 : 34 : 35 : 36 : do for all parties Pr do numr ← 0
( mod F ) {Initialize counters} end for for all objects oj ∈ O , oj fi= oi do if Pq holds oj then if Distance(oi , oj ) > dt {Computed locally at Pq} then
At Pq : numq ← numq + 1
( mod F ) else end if {Let Ps hold oj} {Using the distance computation protocol ( Section 311)} Pq ← rq and Ps ← rs such that rq + rs ( mod D ) = Distance2(oi , oj ) {Using the secure comparison protocol ( Section 3.3)} Pq ← rif rq + rs rq + relse rq + rend if q and Ps ← rs = 1 s such that : ( mod D ) > dt2 then
( mod F )
( mod F ) s = 0 end if At Pq : numq ← numq + rAt Ps : nums ← nums + r q s
( mod k ) do end for for all Pr except Pq and Pq−1 Pr sends numr to Pq−1 At Pq−1 : numq−1 ← end for {Using the secure comparison of Section 3.3} Pq ← tempq and Pq−1 ← tempq−1 such that : if numq + numq−1
( mod F ) > |O| ∗ p % then i.=q numi tempq + tempq−1 ← 1 {oi is an outlier} tempq + tempq−1 ← 0 else end if Pq−1 sends tempq−1 to Pq , revealing to Pq if oi is an outlier .
37 : end for
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE is given by
Distance2(X , Y ) = r=1 = x2 m .
( xr − yr)2 1 − 2x1y1 + y2 . . . + x2 m .
1 + . . . m − 2xmym + y2 m . r − m . r + x2 y2 m r=1 r=1 r=1
2xryr
= r y2
P1 can independently calculate r x2 r . Similarly , P2 can calculate r . As long as there is more than one attribute r(2xr)(−yr ) is simply ( ie , m > 1 ) , the remaining sum the scalar product of two m dimensional vectors . P1 and P2 engage in a secure scalar product protocol to get random shares of the dot product . This , added to their prior calculated values , gives each party a random share of the square of the distance . There are many scalar product protocols proposed in the literature [ 4 , 21 , 11 ] ; any of these can be used .
Assuming that the scalar product protocol is secure , applying the composition theorem of [ 8 ] shows that the entire protocol is secure .
32 Vertically Partitioned Data
Vertically partitioned data introduces a different challenge . Each party can compute a share of the pairwise distance locally ; the sum of these shares is the total distance . However , the distance must not be revealed , so a secure protocol is used to get shares of the pairwise comparison of distance and threshold . From this point , it is similar to horizontal partitioning : Add the shares and determine if they exceed p % .
An interesting side effect of this algorithm is that the parties need not reveal any information about the attributes they hold , or even the number of attributes . Each party locally determines the distance threshold for its attributes ( or more precisely , the share of the overall threshold for its attributes ) . Instead of computing the local pairwise distance , each party computes the difference between the local pairwise distance and the local threshold . If the sum of these differences is greater than 0 , the pairwise distance exceeds the threshold .
Algorithm 2 gives the full details . In steps 5 9 , the sites sum their local distances . The random x added by P0 masks the distances from each party . In steps 11 18 , Parties P0 and Pk−1 get shares of the pairwise comparison result , as in Algorithm 1 . The comparison is a test if the sum is greater than 0 ( since the threshold has already been subtracted . ) These two parties keep a running sum of their shares . At the end , these shares are added and compared with the percentage threshold , again as in Algorithm 1 .
Algorithm 2 Finding DB(p,D) outliers Require : k parties , P0 , . . . , Pk−1 ; each holding a subset of the attributes for all objects O .
Require : dtr : local distance threshold for Pr ( eg , dt2 + mr/m ) .
Require : Fields D larger than twice the maximum distance value ( eg , for Euclidean this is actually Distance2 ) , F larger than |O|
1 : for all objects oi ∈ O do 2 : m3 : 4 :
0 ← mfor all objects oj ∈ O , oj fi= oi do k−1 ← 0
( mod F )
5 : 6 :
7 : 8 : 9 :
10 :
11 : 12 : 13 : 14 : 15 : 16 : 17 : 18 : 19 : 20 : 21 : 22 : 23 : 24 : 25 : 26 : 27 :
P0 : Randomly choose a number x from a uniform distribution over the field D ; x ← x for r ← 0 , . . . , k − 2 do
At Pr : x ← x + Distancer(oi , oj ) − dtr ( mod D ) {Distancer is local distance at Pr} Pr sends x to Pr+1 end for At Pk−1 : x ← x +Distancek−1(oi , oj)−dtk−1 ( mod D ) {Using the secure comparison protocol ( Section 3.3)} P0 ← m0 and Pk−1 ← mk−1 such that : if 0 < x + ( −x ) m0 + mk−1 = 1
( mod D ) < |D|/2 then ( mod F )
( mod F ) else m0 + mk−1 = 0 0 ← m end if At P0 : mAt Pk−1 : m
0 + m0 k−1 ← m
( mod F ) k−1 + mk−1
( mod F ) end for {Using the secure comparison of Section 3.3} P0 ← temp0 and Pk−1 ← tempk−1 such that : ( mod F ) > |O| ∗ p % then if m0 + mtemp0 + tempk−1 ← 1 {oi is an outlier} temp0 + tempk−1 ← 0 else k−1 end if P0 and Pk−1 send temp0 and tempk−1 to the party authorized to learn the result ; if temp0 + temp1 = 1 then oi is an outlier .
28 : end for
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE
Theorem 1 Proof of Correctness : Algorithm 2 correctly returns as output the complete set of points that are global outliers .
PROOF . In order to prove the correctness of Algorithm 2 , it is sufficient to prove that a point is reported as an outlier if and only if it is truly an outlier . Consider point q . If q is an outlier , in steps 12 16 for at least p % ∗ |O| + 1 of the other points , m0 + mk−1 = 1 ( mod F ) . Since |F| > |O| , it k−1 > |O| ∗ p % . Therefore , point q follows that mwill be correctly reported as an outlier . If q is not an outlier , the same argument applies in reverse . Thus , in steps 12 16 at most p % ∗ |O| − 1 points , m0 + mk−1 = 1 ( mod F ) . Again , since |F| > |O| , it follows that mk−1 ≤ |O| ∗ p % . Therefore , point q will not be reported as an outlier .
0 + m
0 + m
33 Modified Secure Comparison Protocol
At several stages in the protocol , we need to securely compare the sum of two numbers , with the output split between the parties holding those numbers . This can be accomplished using the generic circuit evaluation technique first proposed by Yao[23 ] . Formally , we need a modified secure comparison protocol for two parties , A and B . The local inputs are xa and xb and the local outputs are ya and yb . All operations on input are in a field F1 and output are in a field F2 . ya + yb = 1 ( mod F2 ) if xa + xb ( mod F1 ) > 0 , otherwise ya + yb = 0 ( mod F2 ) . A final requirement is that ya and yb should be independently uniformly distributed over F ( clearly the joint distribution is not uniform ) .
This builds on the standard secure multiparty computation circuit based approach for solving this problem[8 ] . Effectively , A chooses ya with a uniform distribution over F , and provides it as an additional input to the circuit that appropriately computes yb . The circuit is then securely evaluated , with B receiving the output yb . The complexity of this is equivalent to the complexity of Yao ’s Millionaire ’s problem ( simple secure comparison ) .
4 . Security Analysis
The security argument for this algorithm uses proof techniques from Secure Multiparty Computation . The idea is that since what a party sees during the protocol ( its shares ) are randomly chosen from a uniform distribution over a field , it learns nothing in isolation . ( Of course , collusion with other parties could reveal information , since the joint distribution of the shares is not random . ) The idea of the proof is based on a simulation argument : If we can define a simulator that uses the algorithm output and a party ’s own data to simulate the messages seen by a party during a real execution of the protocol , then the real execution isn’t giving away any new information .
To formalize this , we will first give some definitions from [ 8 ] . We then give the proofs of security for Algorithms 1 and 2 .
4.1 Secure Multi Party Computation
Yao first postulated the two party comparison problem ( Yao ’s Millionaire Protocol ) and developed a provably secure solution[23 ] . This was extended to multiparty computations by Goldreich et al[9 ] They developed a framework for secure multiparty computation , and in [ 8 ] proved that computing a function privately is equivalent to computing it securely .
We start with the definitions for security in the semihonest model . A semi honest party follows the rules of the protocol using its correct input , but is free to later use what it sees during execution of the protocol to compromise security . A formal definition of private two party computation in the semi honest model is given below .
Definition 1 ( privacy wrt semi honest behavior):[8 ]
Let f : {0 , 1}∗ ×{0 , 1}∗ −→ {0 , 1}∗ ×{0 , 1}∗ be probabilistic , polynomial time functionality , where f 1 ( x , y ) ( respectively , f2 ( x , y ) ) denotes the first ( resp . , second ) element of f ( x , y) ) ; and let Π be two party protocol for computing f .
Let the view of
1 ( x , y ) ( resp . , viewΠ the first ( resp . , second ) party during an execution of protocol Π on ( x , y ) , denoted 2 ( x , y) ) , be ( x , r1 , m1 , . . . , mt ) viewΠ ( resp . , ( y , r2 , m1 , . . . , mt) ) . r1 represent the outcome of the first ( resp . , r2 the second ) party ’s internal coin tosses , and mi represent the ith message it has received .
The output of the first ( resp . , second ) party during an 1 ( x , y ) ( resp . , 2 ( x , y ) ) and is implicit in the party ’s view of the execution of Π on ( x , y ) is denoted outputΠ outputΠ execution .
Π privately computes f if there exist probabilistic poly nomial time algorithms , denoted S1 , S2 such that {(S1 ( x , f1 ( x , y ) ) , f2 ( x , y))}x,y∈{0,1}∗ ≡C viewΠ
1 ( x , y ) , outputΠ
2 ( x , y ) x,y∈{0,1}∗
{(f1 ( x , y ) , S2 ( x , f1 ( x , y)))}x,y∈{0,1}∗ ≡C outputΠ
1 ( x , y ) , viewΠ
2 ( x , y ) x,y∈{0,1}∗ where ≡C denotes computational indistinguishability . fi' fi' ff ff
As we shall see , our protocol is actually somewhat stronger than the semi honest model , although it does not meet the full malicious model definition of [ 8 ] .
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE
Privacy by Simulation The above definition says that a computation is secure if the view of each party during the execution of the protocol can be effectively simulated given the input and the output of that party . Thus , in all of our proofs of security , we only need to show the existence of a simulator for each party that satisfies the above equations . This does not quite guarantee that private information is protected . Whatever information can be deduced from the final result obviously cannot be kept private . For example , if a party learns that point A is an outlier , but point B which is close to A is not an outlier , it learns an estimate on the number of points that lie between the space covered by the hypersphere for A and hypersphere for B . Here , the result reveals information to the site having A and B . The key to the definition of privacy is that nothing is learned beyond what is inherent in the result .
A key result we use is the composition theorem . We state it for the semi honest model . A detailed discussion of this theorem , as well as the proof , can be found in [ 8 ] .
Theorem 2 ( Composition Theorem for the semi honest model ) : Suppose that g is privately reducible to f and that there exists a protocol for privately computing f . Then there exists a protocol for privately computing g .
PROOF . Refer to [ 8 ] .
42 Horizontally Partitioned Data
Theorem 3 Algorithm 1 returns as output the set of points that are global outliers , and reveals no other information to any party provided parties do not collude .
PROOF . Presuming that the number of objects |O| is known globally , each party can locally set up and run its own components of Algorithm 1 ( eg , a party only needs to worry about its local objects in the “ For all objects ” statements at lines 1 and 5 . ) In the absence of some type of secure anonymous send[20 , 10 ] ( eg , anonymous transmission with public key cryptography to ensure reception only by the correct party ) , the number of objects at each site is revealed . Since at least an upper bound on the number of items is inherently revealed by the running time of the algorithm , we assume these values are known .
The next problem is to simulate the messages seen by each party during the algorithm . Communication occurs only at steps 13 , 15 , 26 , 30 , and 36 . We now describe the simulation independently .
Step 13 : Pq and Ps each receive a share of the square of the distance . As can be seen in Section 311 , all parts of the shares are computed locally except for shares of the scalar product . Assume that the scalar product protocol chooses shares by selecting the share for Pq ( call it sq ) randomly from a uniform distribution over D . Then ∀x ∈ D , P r(sq = x ) = 1|D| . Thus , sq is easily simulated by simply choosing a random value from D . Let the result r(2xr)(−yr ) be fixed . Then ∀x ∈ F , P r(ss = x ) = r = P r(r − sq = y ) = P r(sq = r − y ) = 1|D| . Therefore , the simulator for Ps can simulate this message by simply choosing a random number from an uniform distribution over D . Assuming that the scalar product protocol is secure , applying the composition theorem shows that step 13 is secure .
Steps 15 and 30 : The simulator for party Pq ( respectively Ps ) again chooses a number randomly from a uniform distribution , this time over the field F . By the same argument as above , the actual values are uniformly distributed , so the probability of the simulator and the real protocol choosing any particular value are the same . Since a circuit for secure comparison is used , using the composition theorem , no additional information is leaked and step 15 is secure .
Step 26 : Pq−1 receives several shares numr . However , note that numr is a sum , where all components of the sum are random shares from Step 15 . Since Pq−1 receives only shares from the Ps in step 15 , and receives none from Pq , all of the shares in the sum are independent . The sum numr can thus be simulated by choosing a random value from a uniform distribution over F .
Step 36 : Since Pq knows the results ( 1 if oi is an outlier , 0 otherwise ) , and tempq was simulated in step 30 , it can simulate tempq−1 with the results ( 1 or 0 ) −tempq mod F . The simulator clearly runs in polynomial time ( the same as the algorithm ) . Since each party is able to simulate the view of its execution ( ie , the probability of any particular value is the same as in a real execution with the same inputs/results ) in polynomial time , the algorithm is secure with respect to Definition 1 .
While the proof is formally only for the semi honest model , it can be seen that a malicious party in isolation cannot learn private values ( regardless of what it does , it is still possible to simulate what it sees without knowing the input of the other parties . ) This assumes that the underlying scalar product and secure comparison protocols are secure against malicious behavior . A malicious party can cause incorrect results , but it cannot learn private data values .
43 Vertically Partitioned Data
Theorem 4 Algorithm 2 returns as output the set of points that are global outliers while revealing no other information to any party , provided parties do not collude .
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE
PROOF . All parties know the number ( and identity ) of objects in O . Thus they can set up the loops ; the simulator just runs the algorithm to generate most of the simulation . The only communication is at lines 7 , 11 , 21 , and 27 .
= is the sees xx where party Ps r=0 Distancer(oi , oj ) , x + s−1 Step 7 : Each ranP r(x = y ) = dom value chosen by P0 . P r(x + r=0 Distancer(oi , oj ) = y ) = P r(x = y − s−1 Thus we can simulate the value received by choosing a random value from a uniform distribution over D .
s−1 r=0 Distancer(oi , oj ) ) = 1|D| .
Steps 11 and 21 : Each step is again a secure comparison , so messages are simulated as in Steps 15 and 30 of Theorem 3 .
Step 27 : This is again the final result , simulated as in Step 36 of Theorem 3 . temp0 is simulated by choosing a random value , temp1 = result− temp0 . By the same argument on random shares used above , the distribution of simulated values is indistinguishable from the distribution of the shares . Again , the simulator clearly runs in polynomial time ( the same as the algorithm ) . Since each party is able to simulate the view of its execution ( ie , the probability of any particular value is the same as in a real execution with the same inputs/results ) in polynomial time , the algorithm is secure with respect to Definition 1 .
Absent collusion and assuming a malicious model secure comparison , a malicious party is unable to learn anything it could not learn from altering its input . Step 7 is particularly sensitive to collusion , but can be improved ( at cost ) by splitting the sum into shares and performing several such sums ( see [ 12 ] for more discussion of collusionresistant secure sum ) .
5 . Computation and Communication Analysis
Both Algorithms 1 and 2 suffer the drawback of having quadratic computation complexity due to the nested iteration over all objects .
Due to the nested iteration , Algorithm 1 requires O(n2 ) distance computations and secure comparisons ( steps 1220 ) , where n is the total number of objects . Similarly , Algorithm 2 also requires O(n2 ) secure comparisons ( steps 1016 ) . While operation parallelism can be used to reduce the round complexity of communication , the key practical issue is the computational complexity of the encryption required for the secure comparison and scalar product protocols .
Achieving lower than quadratic complexity is challenging . Failing to compare all pairs of points is likely to reveal information about the relative distances of the points that are compared . Developing protocols where such revelation can be proven not to disclose information beyond that revealed by simply knowing the outliers is a challenge . When there are three or more parties , assuming no collusion , we can develop much more efficient solutions that reveal some information . While not completely secure , the privacy versus cost tradeoff may be acceptable in some situations . An alternative ( and another approach to future work ) is demonstrating lower bounds on the complexity of fully secure outlier detection .
51 Horizontally Partitioned Data
With horizontally partitioned data , we can use a semitrusted third party to perform comparisons and return random shares . The two comparing parties just give the values to be compared to the third party to add and compare . As long as the third party does not collude with either of the comparing parties , the comparing parties learn nothing .
The real question is , what is disclosed to the third party ? Basically , since the data is horizontally partitioned , the third party has no idea about the respective locations of the two objects . All it can find out is the distance between the two objects . While this is information that is not a part of the result , by itself it is not very significant and allows a tremendous increase in efficiency . Now , the cost of secure comparison reduces to a total of 4 messages ( which can be combined for all comparisons performed by the pair , for a constant number of rounds of communication ) and insignificant computation cost .
52 Vertically Partitioned Data
The simple approach used in horizontal partitioning is not suitable for vertically partitioned data . Since all of the parties share all of the points , partial knowledge about a point does reveal useful information to a party . Instead , one of the remaining parties is chosen to play the part of completely untrusted non colluding party . With this assumption , a much more efficient secure comparison algorithm has been postulated by Cachin [ 3 ] that reveals nothing to the third party . The algorithm is otherwise equivalent , but the cost of the comparisons is reduced substantially .
6 . Conclusion
In this paper , we have presented privacy preserving solutions for finding distance based outliers in distributed data sets , and proven their security . One contribution of the paper is to point out that quadratic complexity is a necessity for secure solutions to the problem – at most constant time improvements are possible over the algorithms given .
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE
[ 15 ] E . M . Knorr , R . T . Ng , and V . Tucakov . Distance based outliers : algorithms and applications . The VLDB Journal , 8(34):237–253 , 2000 .
[ 16 ] E . M . Knorr , R . T . Ng , and R . H . Zamar . Robust space transformations for distance based operations . In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining , pages 126–135 , San Francisco , California , 2001 . ACM Press .
[ 17 ] A . Lazarevic , A . Ozgur , L . Ertoz , J . Srivastava , and V . Kumar . A comparative study of anomaly detection schemes in network intrusion detection . In SIAM International Conference on Data Mining ( 2003 ) , San Francisco , California , May 1 3 2003 .
[ 18 ] M . Lewis . Department of defense appropriations act , 2004 , July 17 2003 . Title VIII section 8120 . Enacted as Public Law 108 87 .
[ 19 ] S . Ramaswamy , R . Rastogi , and K . Shim . Efficient algorithms for mining outliers from large data sets . In Proceedings of the 2000 ACM SIGMOD international conference on Management of data , pages 427–438 . ACM Press , 2000 .
[ 20 ] M . K . Reiter and A . D . Rubin . Crowds : Anonymity for Web transactions . ACM Transactions on Information and System Security , 1(1):66–92 , Nov . 1998 .
[ 21 ] J . Vaidya and C . Clifton . Privacy preserving association rule mining in vertically partitioned data . In The Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 639–644 , Edmonton , Alberta , Canada , July 23 26 2002 .
[ 22 ] I . H . Witten and E . Frank . Data Mining : Practical Machine Learning Tools and Techniques with Java Implementations . Morgan Kaufmann , San Fransisco , Oct . 1999 .
[ 23 ] A . C . Yao . How to generate and exchange secrets . In Proceedings of the 27th IEEE Symposium on Foundations of Computer Science , pages 162–167 . IEEE , 1986 .
We are currently implementing these schemes and integrating them into software packages ( eg , Weka [ 22 ] ) to enable a practical evaluation of the computational cost . Another important problem is to figure out privacy preserving methods of space transformation[16 ] , allowing additional distance based operations to be done in a secure manner .
References
[ 1 ] D . Barbar´a , N . Wu , and S . Jajodia . Detecting novel network intrusions using bayes estimators . In First SIAM International Conference on Data Mining , Chicago , Illinois , Apr . 5 7 2001 .
[ 2 ] V . Barnett and T . Lewis . Outliers in Statistical Data . John
Wiley and Sons , 3rd edition , 1994 .
[ 3 ] C . Cachin . Efficient private bidding and auctions with an oblivious third party . In Proceedings of the 6th ACM conference on Computer and communications security , pages 120–127 . ACM Press , 1999 .
[ 4 ] W . Du and M . J . Atallah . Privacy preserving statistical analysis . In Proceeding of the 17th Annual Computer Security Applications Conference , New Orleans , Louisiana , USA , December 10 14 2001 .
[ 5 ] Directive 95/46/EC of the european parliament and of the council of 24 october 1995 on the protection of individuals with regard to the processing of personal data and on the free movement of such data . Official Journal of the European Communities , No I.(281):31–50 , Oct . 24 1995 .
[ 6 ] K . J . Ezawa and S . W . Norton . Constructing bayesian networks to predict uncollectible telecommunications accounts . IEEE Expert , 11(5):45–51 , Oct . 1996 .
[ 7 ] M . Feingold , M . Corzine , M . Wyden , and M . Nelson . Datamining moratorium act of 2003 . US Senate Bill ( proposed ) , Jan . 16 2003 .
[ 8 ] O . Goldreich . The Foundations of Cryptography , volume 2 , chapter General Cryptographic Protocols . Cambridge University Press , 2004 .
[ 9 ] O . Goldreich , S . Micali , and A . Wigderson . How to play any mental game a completeness theorem for protocols with honest majority . In 19th ACM Symposium on the Theory of Computing , pages 218–229 , 1987 .
[ 10 ] D . Goldschlag , M . Reed , and P . Syverson . Onion routing .
Commun . ACM , 42(2):39–41 , Feb . 1999 .
[ 11 ] I . Ioannidis , A . Grama , and M . Atallah . A secure protocol for computing dot products in clustered and distributed environments . In The 2002 International Conference on Parallel Processing , Vancouver , British Columbia , Aug . 18 21 2002 . [ 12 ] M . Kantarcıoˇglu and C . Clifton . Privacy preserving distributed mining of association rules on horizontally partitioned data . IEEE Transactions on Knowledge and Data Engineering , 16(9):1026–1037 , Sept . 2004 .
[ 13 ] Special section on privacy and security . SIGKDD Explo rations , 4(2):i–48 , Jan . 2003 .
[ 14 ] E . M . Knorr and R . T . Ng . Algorithms for mining distancebased outliers in large datasets . In Proceedings of 24th International Conference on Very Large Data Bases ( VLDB 1998 ) , pages 392–403 , New York City , NY , USA , Aug.2427 1998 .
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE
