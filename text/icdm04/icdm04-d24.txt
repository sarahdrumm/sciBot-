IRC : An Iterative Reinforcement Categorization
Algorithm for Interrelated Web Objects
Gui Rong Xue1 Dou Shen2 Qiang Yang4 Hua Jun Zeng3 Zheng Chen3
Yong Yu1 WenSi Xi5 Wei Ying Ma3
1Computer Science and Engineering
Shanghai Jiao Tong University Shanghai 200030 , PRChina
2Computer Science and Technology
TsingHua University
Beijing 100084 , PRChina
3Microsoft Research Asia
5F , Sigma Center , 49 Zhichun Road
Beijing 100080 , PRChina grxue@sjtueducn , yyu@cssjtueducn shendou@mailstsinghuaeducn
{hjzeng , zhengc , wyma}@microsoft.com
4Hong Kong University of Science and Technology
Clearwater Bay , Kowloon , Hong Kong qyang@csusthk
Virginia Polytechnic Institute and State University
5Computer Science
Virginia , USA xwensi@vt.edu
Abstract
Most existing categorization algorithms deal with homogeneous Web data objects , and consider interrelated objects as additional features when taking the interrelationships with other types of objects into account . However , focusing on any single aspects of these interrelationships and objects will not fully reveal their true categories . In this paper , we propose a novel categorization algorithm , the Iterative Reinforcement Categorization algorithm ( IRC ) , to exploit the full interrelationships between the heterogeneous objects on the Web . IRC attempts to classify the interrelated Web objects by iterative reinforcement between individual classification the interrelationships . Experiments on a clickthrough log dataset from MSN search engine show that , with the F1 measures , IRC achieves a 26.4 % improvement over a pure content based classification method , a 21 % improvement over a query metadata based method , and a 16.4 % improvement over a virtual document based method . Furthermore , our experiments show that IRC converges rapidly . 1 . Introduction results of different types via categorization
We are concerned with categorizing Web pages into meaningful semantic categories . Traditional contentbased simple representation schemes that are based on word occurrence statistics . However , several problems are concluded that may affect the performance of such algorithms . First , most Web pages contain noisy information . Sometimes non text objects , such as images and scripts that are approaches use unusable by text classifiers , are part of the page content . Second , the Web pages are created by different authors who may have no coherent page construction style , languages or even structures .
Our key observation here is that the Web consists of many types of objects such as queries , Web pages and users , while these types of objects are highly interrelated with each other . The intuitive approach is to augment the features of Web objects by using their interrelated Web objects as additional features since related objects are likely to have similar properties . For example , in Web pages categorization , the feature vector of a Web page can be constructed by combining its content and the queries that are associated with the page through the clickthrough log [ 1][19 ] . We denote these methods as virtual document categorization . In our experiment , this kind of categorization has a little improvement over the content based categorization approach . Thus there is still head room for harnessing such interrelationships between the heterogeneous objects . information of interrelated objects
Since the interrelated heterogeneous objects are likely to have similar topic , they are also likely to share similar category information . Therefore , after classifying the objects based on their content , is it possible to impose category in improving the categorization performance ? In fact , the heterogeneous objects form a graph within which the mapping is really many to many rather than one to many . In this paper , we propose a novel classification algorithm , the Iterative Reinforcement Categorization ( IRC ) , to fully exploit the many to many relationships on such graph . In this approach , the category information of one object is reinforced by its interrelated objects ; and the updated category information of the object will consequently reinforce the category information of all the category the information of its interrelated objects . That is to say , it is an iterative reinforcement process until it converges to a conclusive result . The difference of our method to virtualdocument method is that we use the category information instead of the content feature of queries . Furthermore , one Web page is usually interrelated to a few queries while the dimension of the query vector space is very high ( up to 46,000 ) , the virtual document categorization algorithm will suffer from severe data sparseness . The problem is also solved in IRC . interrelated data objects of different
The novelty of our work can be seen from several aspects . First , we extend the traditional classification methods to multi type interrelated data objects . We aim to classify types simultaneously using both their content features and their relationship with other types of objects . Second , we present a reinforcement algorithm to classify interrelated Web data objects on a bipartite graph . In this algorithm , the category of one type is propagated to reinforce the categorization of other interrelated data objects , vice versa , as an iterative process .
We perform comprehensive experiments on the clickthrough log dataset from MSN search engine to evaluate the proposed approach . Our experiments show that , with the F1 measurements , IRC achieves a 26.4 % improvement over a pure content based classification method , a 21 % improvement over a query metadata based method , and a 16.4 % improvement over a virtual document based method .
The rest of the paper is organized as follows . In Section 2 , we review some related work on Web classification and log analysis . In Section 3 , we explain the IRC algorithm and analyze the properties of the algorithm . Experimental results are reported in Section 4 . We give conclusions and discuss future works in Section 5 . 2 . Related work
The first collection of related works is about classification which can be divided into content based or link analysis based classification techniques . The former relies on the text representation of data objects being classified . Joachims [ 13 ] proposed a method of using Support Vector Machines ( SVMs ) to classify documents . Dumais and Chen [ 7 ] use text representation to organize search results into an existing hierarchical structure . They could not achieve high performance on Web pages in some cases due to the noise contained in the Web pages , especially for pages with non text information such as images and scripts that are unusable for text classifiers .
In the link analysis based classification techniques , learning algorithms are applied to handle both text components of the hyperlink relationship among them . Slattery et al . [ 16 ] explored the the Web pages and reduce that directly pages might incorporating words link based and content based hyperlink topology using an extended HITS algorithm . Similarly , Cohn et al . [ 6 ] and Glover et al . [ 9 ] showed that classification performance can be improved by combining techniques . Chakrabarti et al . [ 2 ] proposed a probabilistic model to utilize both text and linkage information to classify a database of patents and a small Web collection . They showed from neighboring classification performance . category information , such as hierarchical category prefixes , improves performance . Oh . et al . [ 14 ] reported similar results on a collection of encyclopedia articles . Getoor et al . [ 8 ] proposed the PRMs to combine the content feature and its relationship with other objects . Our work is different from the hyperlink based classification methods in that we could classify the heterogeneous data objects across different data types , such as Web pages , search queries and users by interrelated relationship through an iterative process . fully exploited incorporating
However ,
A second group of related work is query log analysis Beeferman et al . [ 1 ] proposed an innovative query clustering method based on clickthrough data . In their work , they treat clickthrough data sets as a bipartite graph and identify the mapping between queries and the associated URLs . Queries with similarly clicked URLs can be clustered together . In this work , they deal with the query clustering problem while our algorithm deals with categorization problem ( for both query and Web pages ) . Another difference is that we use the interrelationship to infer probability that a query/webpage belong to a class , while [ 1 ] only used the interrelationship as additional feature for clustering . The analogous method [ 12 ] is also used in finding similar objects , which iteratively calculate the similarities between the objects through the link relationship . Wen et al . [ 19 ] described a query clustering method that makes use of user logs . Chuang et al . [ 4 ] proposed a technique for categorizing Web query terms from the clickthrough logs into a pre defined subject taxonomy based on their popular search interests . Wang et al . [ 17 ] proposed a method of using query clickthrough log to iteratively reinforce query and Web page clusters . To the best of our knowledge , our work on multi type data objects classification on the Web is one of the first to integrate content information with relationships across different data types to improve the performance of classification . 3 . Categorization of interrelated objects
In this section , we first define the problem of interrelated objects categorization , followed by our IRC algorithm to iteratively exploit the relationship between the heterogeneous data objects . In this paper , two types of Web objects the query and Web page are considered , and we are concerned with the problem of categorizing Web pages into categories . 31 Problem definition
The Web could be modeled as a weighted directed bipartite graph G=(V , E ) , where the nodes in V represent queries and Web pages , and the edges E represent the clickthrough information from a query to a browsed Web page . We can divide V into two subsets Q={q1 , q2 , … , qm} and D={d1 , d2 , … , dn} where Q represents the queries and D represents the Web pages . A matrix M is used to represent the adjacency matrix , whose ( i , j) element is the weight from Web page i to query j . In this paper , we simply deem the weight as the frequency of the page being clicked to the query .
We divide the Web pages D into training set Dt and testing set Ds . We consider all queries as being part of the test set Qs , because we have no training set for the queries . Our iterative reinforcement algorithm is described as follows : Iterative Reinforcement Categorization Algorithm ( IRC ) Input : Ds , Dt , Qs Output : The categories of Ds , Qs 1 . Calculate the probability distribution of the Web pages in Ds on the k categories based on the content feature ; 2 . Calculate the probability distribution of queries on the k categories according to the probability distribution of the interrelated Web pages in Dt and Ds ; 3 . Label the query q with the category c where c=
) ]
; qcP i | (
[ max arg ci
Figure 1 . Interrelation between queries and Web pages Based on such a relationship graph , our problem is how to exploit the graph , which is constructed using two types of objects , the performance of categorization . Formally , given a bipartite graph G=(V , E ) , we want to classify the Web pages D and queries Q into a set of predefined categories C={c1 , c2 , … , ck} , where k is the number of categories . to enhance
0
1
≤
≤ ij rp , ij
We can use vectors Pi={pi1 , pi2,… , pik} and Ri={ri1 , ri2,… , rik} to represent the probabilities of the testing Web page di and the testing query qi belongs to each category , . Thus , a probability matrix respectively , where n×k ( Rm×k ) for all n Web pages ( m queries ) can be P constructed , where each entry pij ( rij ) is the probability that Web page di ( query qi ) belongs to category j . 32 Iterative reinforcement algorithm
First , we classify the Web pages according to their content feature , such as the plain text , the title and the metadata of the page , etc .
To fully utilize the relationship in the bipartite graph , we propose a novel iterative reinforcement classification method . The basic idea is to propagate the categories computed for one type of object to all related objects by updating their probability of belonging to a certain category . This process is iteratively performed until the classification results for all object types converge .
4 . Update the probability distribution of Web pages on the k categories according to the probability distribution of the interrelated queries in Qs of the moment from the probability distribution calculated in Step 1 ( see Equations 1 to 8 below ) ; 5 . Label the Web page p as the category c where c=
;
) ] p
|
[ max arg ci cP i (
6 . Update the probability distribution of queries on the k categories according to the probability distribution of the interrelated Web pages in Dt and Ds of the moment ; 7 . Repeat Steps 3 to 6 until the probability distributions reach a fixed point .
In the following , we illustrate the process in detail .
321 Classifying queries based on a bipartite graph . After initially classifying Web pages according to their contents , we could infer the probability of queries belonging to categories according to their interrelated Web pages through the M matrix . This propagation of probability is based on the assumption that the queries of a certain category are usually followed by clicks on Web pages of the same category .
In general , for any query , its interrelated Web pages in our experiment may be from two different sets : the training set Dt and the testing set Ds . Since the categories of Web pages in the training data set are known and the categories of Web pages in the testing data set are predicted , the role played by the training data and the testing data on the queries should be differentiated . We denote MT ( MS ) as the adjacency matrix between training Web pages ( testing Web pages ) and queries . In particular , of the query probability matrix R , denoting the entry the probability that query i belongs to category j , is computed through the following equation : ijr
=
α r ij
: m
∩∈
∑ pM ∑ mi T ODdm i mi M T ODdm ∩∈ i m
T
T
: mj
+
β n
∩∈
∑ pM ∑ ni S ODdn : i ni M S ODdn : ∩∈ i
S n
S nj
( 1 ) where Oi is the set of Web pages associated with the query qi ; α and β are the parameters to be tuned to reflect the relative importance between the training data and the testing data .
We denote the matrix T and S as the row normalized are the transpose of M matrix T and MS ) . Formula ( 1 ) could be re written with a matrix form : and and
' SM
' SM
' TM
' TM
(
( 2 ) where Pt and Ps are the probability matrices of Dt and
TPα T
SPβ S
R
+
=
S
Ds on k categories , respectively . 322 Classifying Web pages based on a bipartite graph . Just as the Web pages can affect the queries , the classification result on queries could also affect the Web pages . After acquiring the categorization of queries , we could re classify Web pages through the relationship between queries and pages . In this step , we both take the content and the categories of the associated queries into of the matrix P , denoting consideration . The element the probability that page i belongs to category j , is computed through the following equation : ijp p ij
= pα ' ij
+
β
'
( 3 ) qn :
∑ in rM nj S ∑
M in S
I ∈ n i qn : n
I ∈ i where Ii is the set of queries interrelated with the Web page pi ; α’ and β’ are the parameters we tune to show the relative importance of the content of the Web pages and their associated queries . We denote the matrix U as the .This equation could also be row normalized matrix re written in matrix form as follows :
SM
P S
=
Pα ' S
+
URβ '
S
( 4 )
323 Iterative reinforcement categorization ( IRC ) . Only using the above two steps , we still have not fully utilized the interrelationships between Web pages and queries . Therefore we continue to perform an iterative reinforcement on the categorization by exploiting the bipartite graph . Such calculation is an iterative process in which the categories are propagated from one denote the probability side to the other side . Let i matrix of interrelated queries and the SP th iteration . The probability matrix of Web pages after i algorithm could be re written in the following matrix : denote i SR
R i 1 =+ S
0 TPα T
+ i SPβ S
( 5 )
( 6 ) Based on the Equations ( 5 ) and ( 6 ) , we could derive
URβ '
0 Pα ' S i 1 + S i P S
+
=
1 +
' the following : 0 1+i Pα = ' SP S 0 αβ Pα ' ' + S
0 UTP ( 7 ) T P , the formula could be as written in the following form :
αβ ' + 0 UTP T
Taken i USP S
ββ
+ i P S
+=+1 i USPωP S
( 8 ) Where ω is equal to ββ’ . The equation implies that the probability matrix of Web pages is affected by the probability calculated on the content of Web pages and the relationship of the adjacent matrix .
Similar to Equation ( 6 ) , the computation of the probability matrix for queries is derived as follows :
0 0 SPβα TPα ' + T S 0 as SP ' βα S written in the following form : i 1 =+ R S 0 TP α + T
Taken
'
+
ββ
SUR
( 9 ) R , the formula could be i S
R i +=+1 S
SURωR
( 10 ) After several iterations , the PS and RS would reach a fixed point . Then , the category of p is taken to the category of q be
, while arg
) ] p i S
| cP i (
[ max ci is arg
[ max ci qcP i | (
) ]
.
The above computation is performed iteratively until || i the Euclidean length of the residual vector || P S becomes less than a predefined δ for some i>0 . 324 Convergence of IRC . We now give a claim that the iterative procedure converges to a fixed point eventually .
Lemma 1 . S and U are nonnegative matrices and sum
−+1 i P S of each row is equal to 1 .
Proof . It is a direct induction from definition of S and
Lemma 2 . US is a Markov matrix and therefore ( US)i is also a Markov matrix for any integer i .
Proof . US is a nonnegative square matrix and it is easy to verify that the entries in each row of US sum to 1 . So it is a Markov matrix . Also from the Markov Chain theory [ 10 ] , ( US)i is also a Markov matrix for any integer i .
Theorem 1 . The IRC algorithm can converge to a fixed
U . point .
Proof . Without loss of generality , we only prove that the matrix i SP can converge to a fixed point .
S , U are Markov matrices ( Lemma 1 ) , and ω ( 0<ω<1 ) From equation ( 8 ) , i USPωP S PUSω ( i P S USPωP =
USωPUSω
++
= +
+
+
+
1 +
1 +
)
(
)
(
)
2 i i
Now we see the convergence as following :
1 + i P S
− i P S
||
( ||
USω i ( )
USωP
+
−
E
|| )
|| i
USω
( i ( )
USωP
+
−
E
|| )
|| lim i ∞→ lim = i ∞→ lim i ∞→
=
ωi
)1
ω
=
<
<
0(
0 lim i ∞→ ( US ) is nonnegative and sum of each row is equal is a Markov matrix too ( Lemma 2 ) . i
Since ,
( US ) i
So to 1 .
Thus ,
Finally , lim i ∞→ lim i ∞→
( ||
USω
) i
||
=
|| i P S
1 −+ i P S
ω
|| lim i ∞→ 0|| = i
US (
) i
0|| =
It is obvious that i SP will converge to its respective can also i SR fixed point eventually . Analogously , converge to its respective fixed point eventually . 4 . Experiments
41 Data set To evaluate the performance of our algorithm , experiments were performed using a set of classified Web pages extracted from the Open Directory Project ( ODP ) ( http://dmozorg/ ) ODP contains about 1.2 million Web pages , in which each Web page is classified by human experts into 17 top level categories ( Arts , Business and Economy , Computers and Internet , Games , Health , Home , Kids and Teens , News , Recreation , Reference , Regional , Science , Shopping , Society , Sports , Adult and World ) . Because the Web pages in the regional category are also included in other categories and the Web pages in the category of the world are not written in English , these two categories are removed in our experiments . Accordingly , 15 categories in all are used in the experiments .
A real MSN query clickthrough log is collected as our experiment data set . The collect log contains about 1.2 million query requests recorded over 12 hours in August 2003 . The log we obtained has been already processed into a predefined format ; ie each query request is associated with one or more clicked Web pages , forming a “ query session ” , which can be defined as follows : Query Session : = query text [ clicked Web page * ] Some preprocessing steps are applied to queries and Web pages in the raw log . All queries are converted into lower case , and are stemmed using the Porter algorithm . The stop words are removed too . The query sessions sharing the same query and the same URL are merged into one query session , with the frequencies summed up . Since we only have 12 hours of query log on hand , some of Web pages in the ODP data set are not in our query clickthrough log . Hence , in our experiment , we only
11 10
9
8
14
15
13
12
1
7
6
5
1 Arts(19106 ) 2 shopping(19441 ) 3 science(6195 ) 4 Games(3526 ) 5 Business(20970 ) 6 Computers(8614 ) 7 Health(8261 ) 8 Sports(7802 ) 9 Society(15620 ) 10 New s(948 ) 11 Home(4787 ) 12 recreation(10934 ) 13 Kids_and_Teens(3039 ) 14 Adults(614 ) 15 Reference(5148 )
2
3
4
Figure 2 . Distribution of Web pages in the 15 categories deal with the common pages which appeared in both the ODP data set and the query clickthrough log . Finally , we got 131,788 Web pages in 15 top level categories , 199,564 associated queries and 468,696 query sessions . Figure 2 shows the distribution of the number of Web pages in 15 categories .
We test the relevancy of queries to the contents of Web pages from the users’ perspective . We randomly select three subsets which contain 600 query sessions in total . Ten volunteer graduate students are chosen as our evaluation subjects . They are asked to evaluate whether the queries are relevant to the Web pages according to the content of the pages . The result is shown in Table 1 , that about 81.7 % of queries on average are relevant to the contents of the Web pages . Table 1 . Relevance between the queries and Web page
Subset 1 2 3 Average
Session 300 300 300
Relevant 247 262 228
Ratio 0.82 0.87 0.76 0.817 softball hitting
Query URL software bugs
Table 2 . A sample of raw MSN clickthrough data Category http://wwwcpsrorg/program/y2k Computer Computer http://wwwbugnetcom/ http://wwwhitrangercom Shopping http://wwwdecatursportscom/sof tball_drills_page.htm http://wwwplayingmantiscom http://wwwjohnnylightningcom/ Recreation http://wwwsniperworldcom http://wwwsnipercentralcom http://wwwnorcalprecisioncom/
Sports Sports Sports
Games
Sports playing mantis sniper rifles information
Before the experiment , we also conducted another statistics test to see whether query terms can introduce extra for Web categorization . Several examples of Query session in the raw data are shown in Table 2 . From the examples we may come to a conclusion intuitively that the pages linked by the same query belong to the same category . Statistically 68.4 % Web pages fall into the same ODP category when they are clicked by the same query . 42 Other setting 421 Feature selection and Classifier . To speed up the classification , a simple feature selection method , known as “ Information Gain ( IG ) ” [ 20 ] , is applied in our experiments . SVM is a promising classification algorithm developed by Vapnik [ 3][13 ] . In this paper , we use linear SVM because of its high accuracy when used for text categorization . The SVMlight software package is used [ 15 ] . In all experiments , the trade off parameter C is set to 1 . The widely used one against all approach is used for the multi class case . In the rest of this paper , we only consider the probability of the data objects that belong to the categories , so we utilize the method in [ 18 ] to assign each category with a probability . 422 Evaluation Criteria . The performance of the proposed methods was evaluated using the conventional precision , recall and F1 measures . Precision p is defined as the proportion of correctly classified examples in the set of all examples assigned to the target class . Recall r is defined as the proportion of the correctly classified examples out of all the examples having the target class . F1 is a combination of precision and recall defined as follows :
( 11 )
F 1
= pr 2 rp +
Furthermore , micro averaging and macro averaging [ 20 ] were applied to get single performance values over all classification tasks . 43 Performance
The content based classification method is taken to be the baseline . Since most queries are relevant to the semantics of the corresponding Web pages , we can intuitively take the interrelated queries could be taken as an additional feature for their corresponding pages . Consider the example in Figure 1 . Web page di is clicked by users with different i1⋅q1+ Mi2⋅q2++ Mim⋅qm can be taken frequencies . Thus M as additional metadata for Web page di where Mik means the frequency that users click on di following query qk . We consider how to utilize the query metadata . First , we can use the query metadata as additional features of Web page directly . We denote this method as query metadata for queries qq , 2 1
, , mq based classification . Second , we can integrate the query metadata and the content of the Web page together and take them a virtual document of the Web page ; for the importance of the query metadata , we try different weights of the metadata and integrate them with the content of the Web pages . For example , if we set “ content : metadata ” as 1:2 , the query metadata is twice as important as the content of the Web page . After removing stop words and feature selection , the dimension of the Vector Space for the content of collection , the query metadata of collection and the virtual document collection are 258,669 , 46,002 , and 281,259 respectively .
We fixed several parameters for the rest of the experiments : the ratio between the content of Web pages and the query metadata is set as 1 : 2 when constructing the virtual documents ; α and β in equation 1 are set as 6 and 4 ; α’ and β’ in equation 3 are set as 7 and 3 ; and iteration five . These parameters are determined based on an experiment conducted on a subset of training data . times equals
Table 3 . Performance of the four algorithms
F1 0.561 0.586 0.609 0.709
Precision Recall 0.561 0.561 0.586 0.586 0.609 0.609 0.709 0.709
Precision Recall 0.470 0.642 0.554 0.523 0.583 0.575 0.671 0.664
MICRO
Content Query metadata Virtual document IRC MACRO
Content Query metadata Virtual document IRC
We start by analyzing how each source of information such as content , query metadata and virtual document performs without using the iterative algorithm . Table 3 shows the micro averaged F1 values for the different classifiers . The highest values for each classifier are shown in bold face . The content based classifiers , as expected , showed poor results , indicating that the text of the Web pages does not provide sufficient information to reliably classify the Web pages . Since queries are relevant to the Web page topics , which could be used to improve the performance .
F1 0.496 0.523 0.568 0.68
IRC achieves the higher performance than the other three methods in comparison . Relatively speaking , the IRC under the F1 micro averageing measure improved over the content method by 26.4 % , over the query metadata method by 21 % , and over the virtual document method by 164 % The reason for the improvement lies in the fact that our algorithm could fully exploit the relationship between the Web pages and the queries .
Content+IRC Query
Content+Query Content
)
% ( 1 F
72 70 68 66 64 62 60
)
% ( 1 F
75 70 65
60 55 50
0.1
0.2
0.3
0.4
0.5
0.6
0.8
0.7 Clickthrough Data Szie
0.9
1
Figure 3 . Performance on the different clickthrough data size
Error on Content
Error on IRC e t a R r o r r E
0.8
0.7
0.6
0.5
0.4
0.3
50
100
150
200
250
300
350
400
Length of file(by words ) Figure 4 . Error rate on different file length
We conducted a further experiment to show the effect of the clickthrough data by increasing the clickthrough data size as displayed in Figure 3 . We randomly selected 10 % data from the clickthrough data at the first time and 20 % at the second time and so on . We found that the performance of the content based categorization is quite poor . When clickthrough data the performance of the Web page categorization is improved . The page length has also an important effect on the performance of classification based on content feature . In order to verify such an intuition , two experiments are conducted and results are shown in Table 4 and Figure 4 . introduced , is
Table 4 . Effect of the length of file on classification
Number of files Ave.Length of files
Wrongly Classified 6060 180.85
Correctly Classified 7113 375.28
Total 13173 285.84
When processing the data , we find many pages that contain too few words to indicate their main topics though they are meaningful with plentiful non text resources such as picture and video . Thus it is hard to identify their labels only based on such words . However , these pages may still be retrieved and clicked by users when they are relevant to the query given by the users . In these cases , the query logs may be especially effective to predict the labels of these pages . In Figure 4 , we can see the error rate of the content based categorization gradually increasing in a large scale with the shorter length of pages while our IRC show a very small change in error rate .
1
2
3
4 Iterative Times
5
Figure 5 . Performance on different iteration counts p a G
800
600
400
200
0
1
3
5
7
9
Figure 6 . Convergence of the iterative algorithm
11
13
15 Iterative Times
) s m
( e m i
T
U P C
180000 150000 120000 90000 60000 30000 0
10 % 20 % 30 % 40 % 50 % 60 % 70 % 80 % 90 % 100 % Data Size
Figure 7 . Execution time on different data size i P S
Figure 5 shows the performance of the IRC algorithm with the iteration times . The implicit relationships are exploited more and more thoroughly with the increase of the iterations and these relationships contribute to the improvement of the classification performance .
The convergence curve of our iterative algorithm is shown in Figure 6 . The gap || || denotes the difference between the current iteration and the previous iteration . Figure 6 shows the IRC algorithm will converge after five iterations . i P −+1 S
We run the IRC algorithm on the Pentium 1.9G PC with 512M of memory . Figure 7 shows the execution time of the algorithm on different data size , where the CPU time is linear with the size of the clickthrough data , which shows that the algorithm scales up well with large data . 5 . Conclusions and future work
In this paper , we proposed an iterative reinforcement classification algorithm to categorize interrelated data objects from different data types simultaneously . The proposed algorithm considers both the content feature of in the can improve of Data , pages significantly data objects and the relationship across the different data types iterative classification process . The intermediate results of each type will be used to reinforce the classification of their related data types . Experiments on real MSN query clickthrough log datasets show that IRC the Web page classification under the F1 measure after introducing the relationships between the Web pages and queries . In the future , it would be interesting to see whether it is possible to effectively integrate multiple types of relationships such as hyperlinks and user access patterns to improve the classification effectiveness . 6 . References [ 1 ] D . Beeferman and A . Berger . Agglomerative clustering of a search engine query log . In Proceedings of the sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 407 415 , 2000 . [ 2 ] S . Chakrabarti , B . Dom , and P . Indyk . Enhanced hypertext categorization using hyperlinks . In Proceedings of the ACM SIGMOD International Conference on Management 307 318 , Seattle , Washington , June 1998 . [ 3 ] C.Cortes and V . Vapnik . Support Vector Networks . Machine Learning , 20:1 25 , 1995 . [ 4 ] S . L . Chuang and L . F . Chien . Enriching Web taxonomies through subject categorization of query terms from search engine logs . Decision Support System , Volume 35 , Issue 1 , April 2003 . [ 5 ] H . Cui , J . R . Wen , J . Y . Nie , and W . Y . Ma . Query Expansion by Mining User Logs , IEEE Transaction on Knowledge and Data Engineering , Vol . 15 , No . 4 , July/August 2003 . [ 6 ] D . Cohn and T . Hofmann . The missing link a probabilistic model of document content and hypertext connectivity . Information Processing Systems 13 , pages 430 436.MIT Press , 2001 . [ 7 ] S . Dumain and H . Chen . Hierarchical Classification of Web Content . In Proceedings of the 23rd annual international ACM SIGIR Conference on Research and Development in Information Retrieval , 2000 . [ 8 ] L . Getoor , N . Friedman , D . Koller , and B . Taskar . "Learning Probabilistic Models of Relational Structure," In Proceeding of the 18th International Conference on Machine Learning , 2001 . [ 9 ] E . J . Glover , K . Tsioutsiouliklis , S . Lawrence , D . M . Pennock , and G . W . Flake . Using Web structure for classifying and describing Web pages . In Proceedings of WWW 02 , International Conference on the World Wide Web , 2002 .
In Advances in Neural
[ 10 ] G . Grimmett and D . Stirzaker . Probability and Random Processes , 2nd ed . Oxford , England : Oxford University Press , 1992 . [ 11 ] C . K . Huang , L . F . Chien , and Y . J . Oyang . Relevant term suggestion in interactive Web search based on contextual information in query session logs . JASIST 54(7 ) : 638 649 , 2003 . [ 12 ] G . Jeh and J . Widom . SimRank : A Measure of Structural Context Similarity . Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 538 543 , Edmonton , Canada , July 2002 . [ 13 ] T . Joachims . Text categorization with support vector machines : learning with many relevant features . In Proceedings of ECML 98 , 10th European Conference on Machine Learning , pages 137 142 , Chemnitz , Germany , April 1998 . [ 14 ] H . J . Oh , S . H . Myaeng , and M . H . Lee . A practical hypertext categorization method using links and incrementally available class information . In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval , pages 264 271 . ACM Press , 2000 . [ 15 ] Sequential Minimal Optimization , http://research . micro softcom/~jplatt/smohtml [ 16 ] S . Slattery and M . Craven . Discovering test set regularities in relational domains . In Proceedings of ICML 00 , 17th International Conference on Machine Learning , pages 895 902 , Stanford , US , 2000 . [ 17 ] J . D . Wang , H . J . Zeng , Z . Chen , H . J . Lu , L . Tao , and W. Y Ma . ReCoM : reinforcement clustering of multitype interrelated data objects . In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval , pages 274 281 , Toronto , CA , July 2003 . [ 18 ] J . Platt . Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods . In A . Smola , P . Bartlett , B . Sch olkopf , and D . Schuurmans , editors , Advances in Large Margin Classi ers . MIT Press , 1999 . [ 19 ] J . R . Wen , J . Y . Nie , and H . J . Zhang . Clustering user queries of a search engine . In Proceedings of the Tenth International World Wide Web Conference , Hong Kong , May 2001 . [ 20 ] Y . Yang and JO Pedersen . A comparative study on feature selection in text categorization , in Proceeding of the Fourteenth International Conference of Machine Learning , 1997 .

