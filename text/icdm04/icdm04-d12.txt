Dynamic Classifier Selection for Effective Mining from Noisy Data Streams
Xingquan Zhu , Xindong Wu , and Ying Yang
Department of Computer Science , University of Vermont , Burlington VT 05405 , USA
{xqzhu , xwu , yyang}@csuvmedu
Abstract
Recently , mining from data streams has become an important and challenging task for many real world applications such as credit card fraud protection and sensor networking . One popular solution is to separate stream data into chunks , learn a base classifier from each chunk , and then integrate all base classifiers for effective classification . In this paper , we propose a new dynamic classifier selection ( DCS ) mechanism to integrate base classifiers for effective mining from data streams . The proposed algorithm dynamically selects a single “ best ” classifier to classify each test instance at run time . Our scheme uses statistical information from attribute values , and uses each attribute to partition the evaluation set into disjoint subsets , followed by a procedure that evaluates the classification accuracy of each base classifier on these subsets . Given a test instance , its attribute values determine the subsets that the similar instances in the evaluation set have constructed , and the classifier with the highest classification accuracy on those subsets is selected to classify the test instance . Experimental results and comparative studies demonstrate the efficiency and efficacy of our method . Such a DCS scheme appears to be promising in mining data streams with dramatic concept drifting or with a significant amount of noise , where the base classifiers are likely conflictive or have low confidence .
1 . Introduction
The ultimate goal of effective mining from data streams ( from the classification point of view ) is to achieve the best possible classification performance for the task at hand . This objective has traditionally led to an intuitive solution : separate stream data into chunks , and then integrate the classifiers learned from each chunk for a final decision [ 11 , 22 , 24 ] . Given a huge volume of data , such an intuitive solution can easily result in a large number of base classifiers , where the techniques from Multiple Classifier Systems ( MCS ) [ 1 2 ] are involved to integrate base classifiers . The fact behind the merit of MCS is from the following underlying assumption : Each participating classifier in the MCS has a merit that deserves exploitation [ 3 ] , ie , each base classifier has a particular subdomain from which it is most reliable , especially when different classifiers are built using different subsets of features , different subsets of the data , and/or different mining algorithms .
Roughly , techniques can be existing integration distinguished into two categories : 1 . Combine base classifiers for the final decision . When classifying a test instance , the results from all base classifiers are combined to work out the final decision . We refer it to Classifier Combination ( CC ) techniques .
2 . Select a single “ best ” classifier from base classifiers for the final decision , where each base classifier is evaluated with an evaluation set to explore its domain of expertise . When classifying an instance , only the “ best ” classifier is used to determine the classification of the test instance . We name it Classifier Selection ( CS ) techniques . In [ 4 ] , the CC techniques were categorized into three types , depending on the level of information being exploited . Type 1 makes use of class labels . Type 2 uses class labels plus a priority ranking assigned to each class . Finally , Type 3 exploits the measurements of each classifier and provides each classifier with some measure of support for the classifier ’s decision . The CS takes the opposite direction . Instead of adopting the combining techniques , it selects the “ best ” classifier to classify a test instance . Two types of techniques are usually adopted : 1 . Static Classifier Selection ( SCS ) . The selection of the best classifier is specified during a training phase , prior to classifying a test instance [ 5 6 ] .
2 . Dynamic Classifier Selection ( DCS ) . The choice of a classifier is made during the classification phase . We call it “ dynamic ” because the classifier used critically depends on the test instance itself [ 7 10 ] . Many existing data stream mining efforts are based on the Classifier Combination techniques [ 11 , 22 24 ] , and as they have demonstrated , a significant amount of improvement could be achieved through the ensemble classifiers . However , given a data stream , it usually results in a large number of base classifiers , where the classifiers from the historical data may not support ( or even conflict with ) the learner from the current data . This situation is compounded when the underlying concept of the data stream experiences dramatic changes or evolving , or when the data suffers from a significant amount of noise , because the data may vary dramatically in accuracy or in their domain of expertise ( ie , they appear to be conflictive ) . In these situations , choosing the most reliable one becomes more reasonable than relying on a whole bunch of likely contradictive base classifiers . the classifiers learned from
In this paper , we propose a new DCS mechanism for effective mining from noisy data streams . Our intuitive assumption is that the data stream at hand suffers from dramatic concept drifting , or a significant amount of noise , so the existing CC techniques become less effective . We will first review related work in Section 2 ; and then propose our new method in Section 3 . In Section 4 , we discuss about applying the proposed DCS scheme in noisy datasets . Our experimental results and comparative studies in Section 5 indicate that the proposed DCS scheme outperforms most CC or CS methods in many situations and appears to be a good solution for mining real world data . 2 . Related Work
The two main reasons of employing multiple classifiers for data stream mining are efficiency and accuracy . Although the efficiency could be the most attractive reason for adopting multiple classifiers , because a data stream can always involve a huge volume of data which turns to be a nightmare for any
0 7695 2142 8/04 $20.00  2004 IEEE
305 single learner . The accuracy of MCS in handling stream data is also remarkable : especially when the concept in the data stream is subject to evolving , changing or drifting [ 11 , 24 ] . Like many partitioning based or scale up learning algorithms ( eg , Bagging [ 12 ] , Boosting [ 13 ] and Meta learning [ 14 ] ) have demonstrated , by partitioning the whole dataset into subsets , the system efficiency can be dramatically improved , with a limited sacrifice of the accuracy .
When adopting MCS in stream data , the most intuitive ( and probably also the simplest ) scheme is simple voting which is also called Select All Majority ( SAM ) [ 7 ] , where the prediction from each base classifier is equally weighted to vote for the final prediction . Although simple , SAM has been proved to be effective to integrate multiple classifiers , and many revised versions [ 11 , 22 , 24 ] have been successfully developed to handle data streams . In comparison with CC based schemes , the CS schemes select one classifier for the final decision , where two kinds of techniques , SCS and DCS , are usually adopted . Among all SCS schemes , the most intuitive one is Cross Validation Majority ( CVM)[5 ] . In CVM , cross validation is adopted and the base classifier with the highest classification accuracy from the cross validation is selected to classify all test instances .
In comparison with SCS where the “ best ” classifier has been selected before the testing phase , DCS dynamically selects the “ best ” classifier for each test instance . Among different DCS schemes , the most representative one is Dynamic Classifier Selection by Local Accuracy ( DCS_LA ) [ 10 ] which explores a local community for each test instance to evaluate the base classifiers , where the local community is characterized as the k Nearest Neighbors ( kNN ) of the test instance in the evaluation set Z . Although DCS_LA has been widely integrated in many systems , it suffers from the following three disadvantages : ( 1 ) The selection of k and the adopted distance function critically affect the system performance .
( 2 ) The speed factor . Given a test instance , DCS_LA has to go through the whole evaluation set to find its neighborhood . Its time complexity is unbearable for stream data .
( 3 ) Sensitive to noise . Usually , the number of instances in a local community is relatively small , the existence of noise will critically affect the performance of DCS_LA .
Intuitively , for real world datasets , not all attributes have the same importance in classification . Instead of using all attributes to evaluate the base classifiers , a more reasonable way may consider the important attributes of each base classifier . Accordingly , a referee based dynamic classifier selection scheme was proposed in [ 9 ] where referees , in the form of decision trees , partition the whole evaluation set into subsets , and each base classifier is evaluated with these subsets to explore its domain of expertise . The advantage of the Referee is that it partitions the evaluation set into subsets by joining the features of the base classifiers . The less important attributes won’t be used in partitioning the evaluation set , and the partitioned subsets tend to be more reasonable in exploring the domain expertise . However , the drawbacks are threefold : ( 1 ) to learn each referee , one has to explore the features of each base classifier ; ( 2 ) it uses the learned decision trees to partition the original training set into subsets , and the system performance will critically depend on the quality of learned decision trees ; and ( 3 ) because each classifier has its own referee , and the reliabilities from different referees are evaluated from different subsets . Without the same measurements , the selected classifier might not work well .
306 to be
The above review shows that in order to explore the domain expertise of each base classifier , the DCS schemes have to evaluate the classifiers from either an entire or partial evaluation set to determine which classifier is the best for the test instance at the current stage . Such a mechanism inherently provides an adaptive scheme that is most suitable for mining data streams with dramatic changes , where most existing mining efforts less effective . However , all existing DCS appear approaches use either distance based schemes or classification trees ( or classification rules ) to partition the evaluation set into subsets , where the quality of the partitioned subsets critically depends on the performance of adopted distance functions or classification trees . In this paper , we present a new DCS scheme that evaluates base classifiers with subsets of the evaluation set , where the subsets are constructed with statistical information of attribute values . We believe such a partitioning scheme is more natural and intuitive in exploring the domain expertise of each base classifier for effective data stream mining . 3 . AO DCS : Attribute Oriented Dynamic Classifier Selection
In this section , we present a new dynamic classifier selection scheme , called AO DCS ( Attribute Oriented Dynamic Classifier Selection ) . We use attribute values of instances to partition the evaluation set into subsets for evaluation purpose . If the instances in a dataset have only one attribute , and we use this attribute to partition the evaluation set into disjoint subsets with each subset corresponding to one value of this attribute , the classification accuracy of each base classifier with these subsets is instances characterized by each attribute value . Then each base classifier ’s performance will reflect its domain of expertise . Our AO DCS takes the following three steps . 1 . Statically partition the evaluation set into subsets by using the attribute values of the instances . We denote the aggregation of all constructed subsets by Л . the performance of the classifier with the
2 . Evaluate the classification accuracy of each base classifier on all subsets in Л . We call this accuracy “ attributeoriented ” classification accuracy .
3 . For a test instance , use its attribute values to select the corresponding subsets from Л , and select the base classifier Cj that has the highest classification accuracy from the selected subsets as the “ best ” classifier to classify the test instance .
3.1 Constructing Subsets
Given a dataset D , let X , Y and Z be the training , test and evaluation set , with the numbers of instances in X , Y and Z denoted by NX , NY and NZ respectively , and C1 , C2 , , CL be the L base classifiers from X . The objective of AO DCS is to select the xIˆ in Y . Our “ best ” classifier C* to classify each instance algorithm first acquires statistical attribute information of all instances in D . Assuming the instances in D have M attributes A1 , A2,,AM , and each attribute Ai contains ni values V , , ( we will discretize numerical attributes into discrete 1 values ) . For an attribute Ai , we use its values to partition the evaluation , where . To partition Z , we design a procedure in Fig 1 . A A S S i i 1 n i set Z Z
=∪∪ into ni
A S , , i 1 subsets
A V i n i
A i n i
S
A i
PartitionEvaluationSet ( Ai ) Attribute Ai of the evaluation set Z . ni subsets , determined by Ai . ∅←
S S
A i n i A i n i
∅←
Procedure Input :
A S , , Output : i 1 ∅← A S , , i 1 For k =1 ; k ≤ NZ ; k++ For r =1 ; r ≤ ni ; r ++ S I {
End for
( 1 ) ( 2 ) ( 3 ) ( 4 )
∪
A i r
A i r
=
A i 2
S
S k
∈
|
I k
IZ ,
A i k
=
A i
V r
}
Fig 1 . Evaluation set partitioning by attribute values kI iA
In Fig 1 , denotes instance Ik ’s value on attribute Ai in the evaluation set Z . Given Ai , we can use its values to partition the evaluation set Z into ni disjoint subsets , and instances with the same values on Ai are put in the same subset . We partition the evaluation set Z by each attribute , and we can hereby M ni subsets from all attribute values . An example of construct ∑i evaluation set partitioning from a dataset containing only two attributes A1 and A2 is pictorially depicted in Fig 2 , where the xaxis denotes the values of attribute A1 , and the y axis represents the values of attribute A2 ( assuming A1 and A2 contain 3 and 4 values respectively ) . The x axis and y axis span the space ( ℜ ) of AV , all instances in this dataset . Using three attribute values 1 AS and AV and AV of A1 , we construct three subsets , 2 2 3 ℜ= AS , with . A S 1 3 3 Four subsets constructed by A2 are also depicted in Fig 2 . Any two subsets from A1 and A2 , eg , AS , have a small 3 portion of overlapping , and the overlapping region indicates the instances that have the same attribute values on A1 and A2 .
AS , 1 ∪ A S 1 2
AS and 1
∅= and
S A 1 1
A S 1 1
∩
∩
A 1 3
A 1 2
∪
S
S
1
1
1
1
1
2
1
1
Subset
AS 1
1
Subset
AS 2
1
Subset
AS 3
1
W hole attribute space ℜℜℜℜ
2AV
AV 4
2
AV 3
2
AV 2
2
AV 1
2
O
AV 1
1
AV 2
1
AV 3
1
Subset
AS 4
2
Subset
AS 3
2
Subset
AS 2
2
Subset
AS 1
2
1AV
(
Fig 2 . An example of subset partitioning with two attributes
1AV and After we have constructed ∑i
2AV ) M ni subsets by all attributes , we calculate the classification accuracy of each base classifier M ni subsets , and we evaluate the C1,Cj , ,CL on each of these ∑i classification accuracy of each base classifier on each subset with the procedure in Fig 3 . We denote the acquired accuracy matrix by
=
=
=
. lM
,,1
,,1
,,1
L i
;
, jn i
; iA S Acy l C j
Finally , the classification accuracy of each base classifier M ni ) can be worked out . An from all constructed subsets ( ∑i example accuracy matrix is given in Fig 4 , where the first column denotes the base classifiers , and the first row represents the subsets constructed from all attribute values . Each ( i , j)th cell indicates the classification accuracy of classifier Ci on subset Sj .
307
GenerateAccuracyMatrix( ) An evaluation set Z The accuracy matrix
S CAcy iA l j
Procedure Input :
Output : ( 1 ) For i=1 ; i ≤ M ; i++ ( 2 ) For each attribute Ai ( 3 ) ( 4 ) ( 5 ) ( 6 )
ParitionEvaluationSet(Ai ) ; For l=1 ; l ≤ ni ; l++ For j=1 ; j ≤ L ; j++ S CAcy
Calculate iA l j
, ie , the classification iA lS ; accuracy of classifier Cj on dataset End End for Fig 3 . Evaluating each base classifier on constructed subsets
Subsets from attribute AM
Subsets from attribute A1
Subsets from attribute Ai
1
A 1 1
AS … 1 nS … C1 0.81 … 0.67 … C2 0.79 … 0.71 … … … … … … CL 0.83 … 0.82 … iAS1 …
A i i nS …
MAS1 s r e i f i s s a l c
… nS A M M
Fig 4 . The classification accuracy from the base classifiers on
Output : ( 1 ) AverageAcy[ ] ← 0 ; ( 2 ) For j=1 ; j ≤ L ; j++ ( 3 ) For i=1 ; i ≤ M ; i++ ( 4 )
AverageAcy[j]= AverageAcy[j]+ A i x
S Acy C
V A { i b arg
=
=
ˆ I l
| b iA l j
; b
= n ,1 i
}
;
End for ( 5 ) AverageAcy[j]= AverageAcy[j ] / M ; End for ( 6 ) )ˆ( =
|)ˆ( max{
IC
IC arg
=
{ k
* k x x j
AverageAcy
= j
,2,1
,
L j }]}[
;
Fig 5 . Dynamic classifier selection of AO DCS
In Fig 5 , denotes sI x'ˆ value on Ai . Given xIˆ iA sI x'ˆ values on all attributes xIˆ , AOxIˆ iA , i=1,,M DCS first acquires These attribute values can be used to find specific subsets that were constructed by similar instances in the evaluation set . The classifier that receives the highest average accuracy with all selected subsets is identified as the “ best ” classifier for xIˆ . all constructed subsets 3.2 Dynamic Classifier Selection With the acquired classification matrix , l=1 , , ni ; j=1 , , L . given a test instance classifier selection with the procedure below : Procedure Input : iA S l j
CAcy , i=1 , , M , xIˆ , AO DCS performs
DynamicClassifierSelection( xIˆ ) A test instance iA S Acy ; ,,1 l C j The best classifier for instance xIˆ and a classification matrix lM ;
L ,,1 )ˆ(* xIC
= j xIˆ ,
,,1 n i
=
=
. i
,
3.3 Remarks on Relevant Research Efforts
Our method can accommodate both missing values and numerical attributes . A missing value is treated as a specific additional value . Numerical attributes can be converted into nominal ones using discretization techniques . In our system , we adopt to convert numerical attributes into nominal ones . the k means clustering algorithm [ 15 ]
By using attribute values to partition the evaluation set into subsets in advance , AO DCS works similar to the static dynamic classifier selection in [ 16 ] where it also partitions the evaluation set into subsets to evaluate the performance of base classifiers . However , there are two key distinctions : The method in [ 16 ] directly assigns the best classifier for each predefined region . AO DCS , however , creates some small regions by using the attribute values from the evaluation set , and uses each test instance to determine the final subsets for evaluating the base classifiers .
Instead of adopting a clustering technique that has to use distance functions to find small regions , we use attribute values to partition an evaluation subset , which is more natural and intuitive from the data viewpoint . By using attribute values to partition the evaluation dataset into subsets , our method is also somewhat similar to the rudimentary rule induction algorithm  1R [ 17 ] . This method generates a one level decision tree , which is expressed in the form of a set of rules that all test on one particular attribute . 1R is a simple , cheap method that often comes up with quite good rules for characterizing the structure in data . Perhaps this is because the structure underlying many real world datasets is rudimentary , and just one attribute is sufficient to determine the class of an instance accurately . With the surprising results from the 1R algorithm , we can find that using a single attribute to explore the domain of expertise might be more reasonable to some degree , especially in noisy environments . 4 . Applying AO DCS in Data Stream Mining In many applications , data is not static but arrives in data streams , and the stream data is also characterized by drifting concepts . In other words , the underlying data generation models , or the concepts that we try to learn from the data , are constantly evolving , even dramatically . Meanwhile , real world stream data is never perfect and can often suffer from corruptions ( noise ) that may impact interpretations of the data , models created from the data and decisions made based on the data [ 25 ] . Due to the inherent huge volume of the size , it is actually hard to apply general data cleansing mechanisms [ 18 ] to stream data for better data quality . Therefore , the above two facts ( concept drifting and noise ) imply that the classifiers learned from a small portion of the stream may vary significantly in performance , which makes DCS a promising solution for effective mining from a real world data stream . In this section , we propose a framework to apply AO DCS to mine noisy data streams . This framework is general enough to be incorporated to handle any real world data streams . to allow any exiting learning algorithms
As shown in Fig 6 , we first partition streaming data into a series of chunks , S1 , S2 , Si , , each of which is small enough to be processed by an induction algorithm at one time . Then we learn a base classifier Ci from each chunk Si . To evaluate all base classifiers ( in the case that the number of base classifiers is too large , we can keep only the most recent K classifiers ) and determine the “ best ” one for each test instance , we will dynamically construct an evaluation set Z ( using the most recent instances , because they are likely consistent with the current test instances ) . When classifying a test instance , Ik , we will employ AO DCS ( and the evaluation set Z ) to integrate existing base classifiers and select the “ best ” classifier to classify Ik .
In real world situations , many data streams contain a certain level of noise . There are two common types of noise : attribute noise and class noise [ 18 ] . In this paper , we assume the data stream suffers from a certain level of class noise ( which means the errors are introduced in the class labels ) , and will extensively evaluate the performance of different DCS schemes in handling noisy data , where various levels of class noise are manually introduced ( before the data partitioning ) to simulate real world scenarios . This should provide interested readers with valuable knowledge about mining from real world data streams .
Stream
• • • • •
The data stream with dramatic concept change
Evaluation
Set Z
Add class noise
Partition data stream into chunks
The most recent instances for classification
Sn
• • •
Si
• • •
S2
S1
Base Classifier
Cn
• • •
Base Classifier
Ci
• • •
Base Classifier
C2
Base Classifier
C1
Dynamic Classifier Selection
DCS
Testing instance Ik
Final class for Ik
Fig 6 . Applying DCS in noisy data streams
5 . Experimental Results and Comparisons
In this section , we design two sets of experiments , ( 1 ) DCS from classifiers that vary significantly in performance ; and ( 2 ) DCS in classifying noisy datasets , to evaluate the performance of our proposed scheme . We take 8 datasets ( including synthetic [ 19 ] and real world data from the UCI database repository [ 20 ] ) as benchmark data streams ( by assuming the data comes in a time series manner ) to evaluate the system performance .
Our purpose of the first set of experiments is to evaluate DCS in mining data streams with dramatic concept drifting . Unfortunately , although mining data streams with concept drifting has been addressed by many research efforts , we cannot find any benchmark dataset with dramatic concept changes ( most existing efforts evaluate their algorithms with synthetic data ) . So we use the following design to simulate the scenarios in this regard . Given a dataset X , we first execute c4.5rules [ 21 ] on X to learn a classifier C0 , and then split X into 2η chunks ( using proportional partitioning [ 14] ) , and randomly select one chunk to induce a base classifier Cη , until we get η base classifiers C1 , C2 , Cη . Normally , given a dataset X , the less the instances are used for training , the worse is the learned classifier in addressing the genuine concept of X . Therefore , from C1 to Cη , the classifier becomes weaker and weaker in performance ( we assume it is the result of the dramatic change of the underlying concept ) . We use C1 , C2 , , Cη to evaluate the proposed DCS algorithms . In the second set of experiments , DCS schemes are used to integrate classifiers learned from noisy datasets .
308
We compare the proposed AO DCS with four multiple classifier mechanisms : SAM [ 7 ] , CVM [ 5 ] , DCS_LA [ 10 ] and Referee [ 9 ] . For DCS_LA , we use the overall accuracy from the k nearest neighbors to evaluate the local accuracy . Meanwhile , we set k=10 for all experiments ( as recommended by the original authors ) . For Referee , we use decision rules [ 21 ] as the referee , because decision rules are easier to manage . To compare the results of DCS schemes in classifying noisy data streams , we implement the Arbiter scheme proposed by Chang [ 14 ] .
For each experiment , we execute 10 fold cross validation and use the final result . The classification accuracy in all tables and figures below denotes the accuracy evaluated from the test sets . the average accuracy as
To add class noise , we adopt a pairwise scheme [ 18 ] : given a pair of classes ( εx , εy ) and a noise level γ , an instance with its label εx has a γ⋅100 % chance to be corrupted and mislabeled as εy , so does an instance of class ε\y . We use this method because in realistic situations , only certain types of classes are likely to be mislabeled . With this scheme , the percentage of the entire training set that is corrupted will be less than γ⋅100 % because only some pairs of classes are considered problematic . In experiments below , we corrupt only one pair of classes ( usually the pair of classes with the highest proportion of instances ) in each dataset and only report the value γ in all tables and figures . 5.1 DCS from Classifiers Vary Significantly in Performance 511 Classification accuracy of base classifiers
To evaluate whether base classifiers actually vary significantly in performance , we add different levels of noise into the data and generate 5 ( η=5 ) base classifiers . We then compare the accuracy of each base classifier with SAM , CVM and AO DCS , and demonstrate the results in Table 1 , where the first column indicates the noise level , columns 2 to 6 represent the accuracy of each base classifier , and column 7 is the average accuracy of all base classifiers .
Table 1 illustrates that in most situations , the accuracy from C1 to C5 is getting worse . However , in a single run , there may have exceptions , Fig 7 shows the accuracy of the base classifiers from 10 runs ( with 15 % noise ) . One can find that from C1 to C5 , the overall accuracy is getting worse , but in a single run , the accuracy of the base classifiers can be different from this trend . Eg , in the third run of Fig 7 , the accuracy of C3 is better than C1 and C2 . Actually , the same phenomenon has been found from some of the other datasets . From Fig 7 , one can find that by dynamic selecting the “ best ” classifier , the AO DCS outperforms all base classifiers in any single run .
Table 1 also indicates that at most noise levels , the classification accuracy from the classifier selection schemes ( CVM and AO DCS ) is better than the combination scheme ( SAM ) . However , when the noise becomes extremely serious , the results from SAM become closer to ( or even better than ) CVM and AO DCS , as demonstrated on the eighth to tenth columns of Table 1 . One possible reason is that when the noise level increases , each base classifier becomes weaker . In the case that all base classifiers have a low confidence , combing results from base classifiers becomes more reasonable . Comparing the results from CVM and AO DCS , one can find that the latter outperforms the former at almost every noise level . As we analyzed before , CVM statistically selects the single best classifier by evaluating
309 the overall performance of the base classifiers . And by integrating the proposed DCS scheme , we can explore the merit of each base classifier and improve the system performance . Table 1 . Classification accuracy of base classifiers with various MCS schemes ( Car dataset , 5 base classifiers ) Noise Level
AOC1 DCS ( % ) % ( % ) 0 88.5 85.1 81.6 77.9 76.1 81.8 85.4 88.5 88.8 10 87.0 84.0 78.5 73.8 71.9 79.1 83.7 87.9 88.2 20 80.4 77.6 73.0 73.7 71.6 75.2 80.4 80.4 81.2 30 74.0 74.5 68.3 64.2 63.9 69.0 76.8 74.6 76.4 40 61.4 62.9 61.0 62.8 57.4 61.1 67.1 60.7 63.4
AVG ( % )
CVM ( % )
SAM ( % )
C2 ( % )
C4 ( % )
C5 ( % )
C3 ( % ) y c a r u c c A
90
85
80
75
70
65
60
1
2
C1
C2
C3
C4
C5
AO DCS
8
9
10
5
6
4
3 7 Execution times
Fig 7 . Classification accuracy of base classifiers and AO
DCS from 10 runs ( Car dataset , 15 % noise )
512 Comparative studies on accuracy
In this subsection , we compare AO DCS with three CS algorithms ( DCS_LA , Referee , and CVM ) and one CC scheme ( SAM ) . We add various levels of noise into original data ( and the evaluation set ) to evaluate the performance of various algorithms in noisy environments . Table 2 shows the results from the Car dataset , where the first column indicates the noise level and the other columns denote the accuracy from different methods .
From Table 2 , one can find that with five base classifiers varying in degrees of accuracy , the CS schemes achieve better performances than the CC scheme ( SAM ) at most noise levels . When the noise level is low , all CS methods outperform SAM with DCS_LA attaining the highest performance . However , with the increase of the noise level , DCS_LA receives the most dramatic decline in comparison with Referee and AO DCS . Meanwhile , although the accuracy of Referee is usually lower than AO DCS and DCS_LA , it was found to be the most noise tolerant , ie , it receives the lowest decline caused by the increase of the noise level . In Table 2 , when noise increases from 0 % to 40 % , the performance of DCS_LA and AO DCS drops 30.57 % ( from 90.65 % to 62.94 % ) and 29.58 % respectively . But , Referee receives only 21.14 % decrease . Actually , our analysis in Section 5.2 will indicate that Referee is more sensitive to the increase of the chunk number , and DCS_LA is more sensitive to noise . Table 2 . DCS accuracies from Car dataset ( 5 base classifiers ) AO DCS
DCS_LA
Referee
Noise Level ( % )
0 10 20 30 40
SAM ( % ) 85.72 82.33 78.45 71.86 61.62
CVM ( % ) 89.23 82.75 77.83 73.03 61.49
( % ) 89.56 85.03 80.31 77.43 70.63
( % ) 90.65 87.33 80.26 73.84 62.94
( % ) 89.73 85.46 78.15 74.65 63.19
513 Comparative studies on efficiency
The time complexity of DCS schemes comes from two phases : 1 ) evaluate the classification accuracy of base classifiers with predefined subsets , TS1 ; and 2 ) select the “ best ” classifier for test instances , TS2 . Given a dataset D with M attributes , assume the numbers of instances in the evaluation and test sets are NZ and NY respectively , and the number of induced base classifier is L . Assume further that DCS_LA selects k nearest neighbors for each test instance , and no indexing structure is adopted to facilitate the kNN search . For each test instance , TS1 of DCS_LA is zero , and TS2 is the time to go through all NZ evaluation instances to find the k nearest neighbors and rank L L ) log base classifiers accordingly , which is . For NY test instances , the total complexity is denoted by Eq ( 1 ) L )
Complexity
NO Z (
+ Lk
NN
( 1 ) log log log
O
=
+
DCS
_
LA
Y ' z
.
0( TS
1
+ LNk fi TS
Y
2
For the Referee method , TS1 is the time to construct the referee for each base classifier by using instances in the evaluation set , and we assume a quadratic complexity for such a procedure ( Although other better ( less that quadratic ) learning algorithms are available , we assume the quadratic complexity for the worst case ) , where the complexity to construct L referees is ZNLO ⋅ ( . TS2 is the time to pass all referees and select the . Therefore for NY test “ best ” classifier , which is instances , the total complexity denoted by Eq ( 2 ) . LN log TS
NLO TS
Complexity
fi' fi'
LO (
( 2 ) log
) ) 2
) ) 2 eferee
L
L
+
=
(
)
)
(
(
⋅
R
Y
Z
(
LMN
For AO DCS , TS1 is the time to evaluate the accuracy of each base classifier with the subsets constructed by attributes , where each classifier needs to go through the evaluation set for M times ( where M is the number of attributes ) . So TS1 of AO DCS is . TS2 for AO DCS is the time to select the “ best ” O classifier that has the highest accuracy with the subsets . For NY test determined by the test instance , which is instances the total complexity of AO DCS is denoted by Eq ( 3 ) . ( 3 )
LMO ) log
Complexity log
O
L
+
=
)
(
)
(
Z
AO
−
DCS
fi'fi'
2
1
LMN TS
1
Z
Y
MN TS
2
For normal datasets , it is obvious that ( M , L , k ) << ( NY , NZ ) , therefore complexity of AO DCS appears to be the lowest one .
In addition to the above theoretical analysis , we also perform an empirical analysis . Table 3 shows the execution times of these three schemes from the Mushroom dataset , where we present the actual execution time ( in seconds ) of each method and their ratio values ( We used a PC with Intel Pentium 4 with 2 GHz speed and 512 MB memory ) . In Table 3 , the first column indicates the percentage of the size of the evaluation set in
RatioA= TS of Referee / TS of AO DCS RatioB= TS of DCS_LA / TS of AO DCS comparison with the size of the whole dataset , and TS is the sum of TS1 and TS2 . RatioA and RatioB are given in Eqs . ( 4 ) and ( 5 ) . ( 4 ) ( 5 ) From the results in Table 3 , one can find that AO DCS comprehensively improves the system performance in terms of time efficiency . When the size of the evaluation set increases , both DCS_LA and Referee suffer from spending a lot of time on finding the nearest neighborhood or inducing referee rules from the evaluation set , which is obviously a nonlinear increase with the size of the evaluation set . But increase the size of the evaluation set has less influence on AO DCS . 5.2 DCS in Classifying Noisy Data Datasets 521 Experiments with noise levels
To evaluate the performances of DCS schemes in classifying partition based noisy data datasets , we equally split the dataset into 9 non overlapping chunks ( using proportional sampling ) . We use c4.5rules to construct a base classifier from each chunk , and apply DCS schemes to integrate these base classifiers . In addition , we also add certain levels of class noise into the original data . The experimental results are shown in Tables 4 and 5 , where the first column indicates the noise level , the second to seventh columns represent the classification accuracy of each scheme , and the eighth and ninth columns give the average and variance of the 9 base classifiers’ accuracy .
From Tables 4 and 5 , when we compare the performances of the three DCS schemes with SAM , CVM and Arbitrator , we can find that DCS schemes receive relatively better performances at various noise levels . If we compare the second and eighth columns , we can find that SAM ’s accuracy is higher than the average accuracy of all base classifiers , where the improvement from SAM can be 2 % to 10 % compared to the average accuracy of all base classifiers . This confirms that SAM works surprisingly ( or embarrassingly ) well in many circumstances . From the ninth column , one can find that when the noise level increases , the variance of the base classifiers’ accuracy becomes large , which indicates that the performance of the base classifier varies significantly . Usually , if the variance of the base classifiers’ accuracy becomes significant , DCS can receive relatively large improvement .
When we compare the fourth to sixth columns , we can find DCS_LA is more sensitive to noise . When the noise level increases , the performance of DCS_LA decreases dramatically . The reason is that this method uses the nearest neighbors to evaluate base classifiers , and in high noise level environments , the selected neighbors may be seriously corrupted by noise . Both Referee and AO DCS use statistical the evaluation set , so noise has less negative impact on them . information of
Table 3 . Execution time ( in seconds ) of different DCS schemes from Mushroom dataset ( 20 % noise , 5 base classifiers )
Evaluation Set
( % ) 10 30 50 70 90 100
Referee ( s )
DCS_LA ( s )
AO DCS ( s )
TS1 1.040 7.450 29.990 74.280 127.609 173.689
TS2 0.034 0.030 0.034 0.038 0.034 0.032
TS 1.074 7.480 30.024 74.318 127.643 173.721
TS1 0 0 0 0 0 0
TS2 0.788 2.178 3.63 5.210 6.842 7.824
TS 0.788 2.178 3.630 5.210 6.842 7.824
TS1 0.186 0.346 0.582 0.701 0.808 0.861
TS2 0.039 0.039 0.037 0.037 0.036 0.041
TS 0.225 0.385 0.619 0.738 0.844 0.902
RatioA
RatioB
4.77 19.43 48.50 100.70 151.25 192.60
3.50 5.66 5.86 7.06 8.11 8.67
310
When we compare SAM ( the second column ) and Arbiter ( the seventh column ) , one can find that Arbiter generally outperforms SAM in most situations . However , when the noise level reaches a relatively high level ( 30 % ∼ 40% ) , the improvement from Arbiter disappears . The reason is that when the noise level goes higher , more noisy instances are used to train the Arbiter . Consequently , the ability of the learned Arbiter becomes weaker . Table 4 . Experimental comparisons at different noise levels from Car dataset ( 9 chunks )
Noise level ( % ) 0 10 20 30 40
CVM SAM ( % ) ( % ) 84.4 84.6 83.1 82.2 80.1 80.5 76.0 76.9 71.6 67.1
Referee
( % ) 85.5 81.8 78.8 78.1 74.3
( % )
Arbiter
AODCS ( % )
DCA Avg . _LA ( % ) ( % ) 91.3 86.6 86.0 81.5 89.2 83.8 83.9 79.1 83.9 82.0 81.0 76.2 80.2 80.4 79.0 70.7 64.6 73.7 69.5 65.6
Var . ( % ) 2.4 3.8 6.2 8.7 11.3
Table 5 . Experimental comparisons at different noise levels from Krvskp dataset ( 9 chunks )
( % )
Arbiter
Referee
AODCS ( % )
( % ) 97.4 96.0 94.2 92.7 76.9
Noise level ( % ) 0 10 20 30 40
CVM SAM ( % ) ( % ) 97.1 97.2 95.1 95.8 93.9 93.8 92.2 92.4 72.9 73.0
DCA Avg . _LA ( % ) ( % ) 98.2 97.5 96.8 95.7 97.0 95.9 95.7 93.5 93.5 94.3 93.7 90.9 84.5 93.1 91.1 82.9 66.9 75.2 71.6 62.6
Var . ( % ) 0.3 0.9 1.2 10.1 13.2 522 Experiments with variable number of chunks In this subsection , we address the impact of the number of chunks on the performance of the proposed algorithms . We partition the dataset into a different number of chunks ( from 3 to up to 63 ) . Meanwhile , we run the experiments at two noise levels ( 0 % and 25% ) , and show the results in Figs . 9 to 14 . From Figs . 9 to 14 , we can find that when the number of chunks increases , the overall accuracy from all schemes decreases . However , in a certain range , the chunk number may have less impact on the classification accuracy ( and the increase of the number of chunks can even increase the classification accuracy ) . This phenomenon might come from the intrinsic characteristics of each dataset , eg , the level of redundancy .
For noise free datasets , DCS_LA can usually acquire the best performance . However , in noisy environments , DCS_LA receives less improvement , especially when the number of chunks is relatively large . As shown in Fig 9(b ) , where the noise level is 25 % , while the number of chunks increases , the performance of DCS_LA receives less improvement than AO DCS . The same phenomenon has been found from most other datasets ( except from Krvskp , in Fig 11 ( b ) , where the Referee scheme receives the highest classification accuracy ) .
When comparing the three DCS schemes , we find Referee is most sensitive to the number of chunks . Take Fig 9 as an example . When the chunk number increases from 3 to 63 , the performance of Referee drops 19.79 % ( from 90.24 % to 72.38 % ) which is the largest among all three DCS schemes ( the drop of DCS_LA and AO DCS in the same range is 3.93 % and 10.17 % respectively ) . The reason behind this phenomenon is that with the increase of the chunk number , the number of instances in each
311 its own referee ) , and without chunk is decreased . Each learned base classifier then tends to bias to only one or two attributes . Consequently , the learned referee of each classifier cannot comprehensively partition the evaluation set into small subsets to explore the domain expertise of each base classifier . Moreover , as we have mentioned in Section 2 , Referee uses different subsets to evaluate different base classifiers ( each classifier has the same measurements , the selected “ best ” classifier might not work well . The experimental results in Figs . 9 to 14 indicate that with a large number of chunks , the Arbiter scheme likely receives the same classification accuracy as SAM . The reason is that with the increase of the number of chunks ( the number of base classifiers ) , the learned Arbiter will have less influence on the classification accuracy , because the arbitration rules take effect only if the base classifiers cannot have a majority classification . Meanwhile , the existence of noise could also be fatal to the Arbiter , because the arbiter is learned from uncertain data collected from different chunks . In noisy datasets , there is no doubt that the uncertain data collected from different chunks usually contain significant noise . Then , the learned arbiter has a very limited ability to improve the system performance .
We have performed comparisons with different datasets at two noise levels ( 0 % and 25% ) . One can go through all figures below to get detailed comparisons . Clearly , these results indicate that DCS acquires much better performance than simple classifier combining , especially when a large number of base classifiers is adopted . This conclusion supports our initial motivation that each base classifier has its own merit that deserves exploitation , and finding the domain of expertise of each classifier supplies a new way to improve the system performance . Notes : Figs . 9 to 14 report the number of chunks and the system performance on 6 datasets , where ( a ) is tested on noise free data ; and ( b ) is tested with 25 % class noise . In Figs . 9 to 14 , the x axis denotes the number of chunks and the y axis represents the classification accuracy , and each curve denotes one method that is specified in Fig 8 .
9 5 9 0 8 5 8 0 7 5 7 0
SAM
DCS_LA
3
CVM
AO DCS
Referee
Arbitor
Fig 8 . The meaning of each curve in Figs 9 to 14
85
80
75
70
63
51
39
27 ( a ) ( b ) Fig 9 . Results from Car dataset
27
15
3
39
51
63
3
15
95
90
85
80
75
70
74
72
70
68
66
64
67
66
65
64
63
3
15
27
39
51
63
3
15
27
39
51
63
( a )
( b ) Fig 10 . Results from Connect 4 dataset
99
97
95
93
91
100
99.5
99
98.5
98
97.5
85
84
83
82
81
80
96
94
92
90
88
86
100
99
98
97
96
95
94
95
90
85
80
75
70
3
15
27
39
51
63
3
15
27
39
51
63
( a )
( b ) Fig 11 . Results from Krvskp dataset
51
63
3
15
27
39
51
63
3
15 27 ( a )
39
Fig 12 . Results from Mushroom dataset
( b )
3
15
27
39
51
63
( a )
( b ) Fig 13 . Results from Adult dataset
85
83
81
79
77
75
3
15
27
39
51
63
92
90
88
86
84
82
80
78
3
15
27
39
51
63
3
15
27
39
51
63
( a )
( b ) Fig 14 . Results from WDBC dataset
6 . Conclusions
Traditional data mining algorithms are challenged by two most important features of data streams : huge volumes of data and the underlying concept drifting . These two challenges raise the need for incorporating ensemble classifiers in existing stream mining efforts . This intuitive solution , however , ignores the fact that the base classifier learned from a portion of a data stream carries two important features : ( 1 ) weak ( even conflictive with others ) in overall performance ; but ( 2 ) still reliable in a specific domain . This fact becomes especially clear if the data stream suffers from dramatic concept drifting or a significant amount of instead of adopting any combination noise . Consequently , scheme , we have presented a new DCS algorithm that selects the “ best ” classifier once a time for each test instance in the data stream . We use each attribute to partition the evaluation set into disjoint subsets . We evaluate the classification accuracy of each base classifier on each subset . This accuracy indicates the ability of the base classifiers in classifying the instances characterized by each attribute , and hopefully , helps us explore the merit of each base classifier . Given a test instance , its attribute values will determine the subsets that have been constructed by similar
312 instances in the evaluation set . The base classifier that has the highest accuracy with all these determined subsets is selected to classify the test instance . Our experimental in results have demonstrated the DCS comparison with classifier combination schemes , algorithms can possibly acquire more accuracy improvement , especially when the performances of the base classifiers vary significantly . When the real world data suffers from dramatic concept drifting or a certain level of noise , AO DCS turns to be a better choice in integrating multiple classifiers to enhance the system performance . References [ 1 ] Ali K . & Pazzani M . , Error reduction through learning multiple that description , Machine Learning , vo.24 , no.3 , 1996 .
[ 2 ] Huang Y . S . & Suen C . Y . , A method of combining multiple experts for the recognition of unconstrained handwritten numerals , IEEE Trans . on PAMI , 17(1 ) , pp.90 94 , 1995 .
[ 3 ] Ueda N . , Optimal linear combination of neural networks for improving classification perform . , IEEE Trans . on PAMI , 22 , 2000 . [ 4 ] Xu L . , Krzyak A . & Suen C . , Methods of combining multiple classifiers and their application to handwriting recognition , IEEE Trans . on Sys . , Man and Cyber . , 22 , 1992 .
[ 5 ] Schaffer C . , Selecting a classification method by cross validation ,
Machine Learning , vol.13 , pp.135 143 , 1993 .
[ 6 ] Breiman L . , Friedman J . H . , Olshen RA & Stone C . J . ,
Classification and regression trees , Belmont , CA , 1984 .
[ 7 ] Merz C . J . , Dynamical selection of learning algorithms , In : from Data , Artificial
D.Fisher , H JLenz Intelligence and Statistics , Springer Verlag , NY ( 1996 ) .
( eds. ) , Learning
[ 8 ] Merz C . J . , Using correspondence analysis to combine classifiers ,
Machine Learning , vo.36 ( 1 2 ) , pp.33 58 , 1999 .
[ 9 ] Ortega J . , Koppel M . & Argamon S . , Arbitrating among competing classifiers using learned referees , Knowledge and Information Systems , vol.3 , no.4 , 2001 .
[ 10 ] Woods K . , Kegelmeyer W . P . & Bowyer K . , Combination of IEEE local accuracy estimation , multiple classifiers using Transactions on PAMI , vol.19 , no.4 , Apr . , 1997 .
[ 11 ] Wang H . , Fan W . , Yu P . & Han J . , Mining concept drifting data streams using ensemble classifiers , Proc . of KDD 2003 .
[ 12 ] Breiman L . , Stacked regressions , Machine Learning , 24 , 1996 . [ 13 ] Schapire R . , The strength of weak learnability , Machine Learning , vol.5 , no.2 , pp.197 227 , 1990 .
[ 14 ] Chan P . , An extensible meta learning approach for scalable and accurate inductive learning , Ph.D thesis , Columbia Univ . , 1996 .
[ 15 ] Jain Anil K . & Dubes R . C . , Algorithms for clustering data . Prentice
Hall , 1998 .
[ 16 ] Kucheva LI , Switching between selection and fusion in combining classifiers : An experiment , IEEE Trans . SMC , 32(2 ) , 2002 .
[ 17 ] Holte RC , Very simple classification rules perform well on most commonly used datasets , Machine Learning , 11 , 1993 .
[ 18 ] Zhu X . , Wu X . & Chen Q . , Eliminating class noise in large datasets ,
Prof . of 20th ICML Conf . , Washington DC , 2003 .
[ 19 ] IBM generator , http://wwwalmadenibmcom/software/quest/Resources/datasets/syndatahtml#classSynData
Research ,
Synthetic
Almaden data
[ 20 ] Blake C . L . & Merz , UCI Data Repository , 1998 . [ 21 ] Quinlan R . , C4.5 programs for machine learning , San Mateo , CA ,
Morgan Kaufmann publisher , 1993 .
[ 22 ] Domingos P . & Hulten G . , Mining high speed data streams , Proc . of
SIGKDD , 2000 .
[ 23 ] Nasraoui O . , Cardona C . , Rojas C . & González F . , TECNOSTREAMS : Tracking Evolving Clusters in Noisy Data Streams with a Scalable Immune System Learning Model , Proc . of ICDM , 2003 .
[ 24 ] Kolter J . & Maloof M . , Dynamic weighted majority : a new ensemble method for tracking concept drift , Proc . of ICDM , 2003
[ 25 ] Zhu X . & Wu X . , Class noise vs attribute noise : A quantitative study of their impacts , Artificial Intelligence Review , in press , 2004 .
