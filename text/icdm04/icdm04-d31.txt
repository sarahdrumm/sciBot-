Aligning Boundary in Kernel Space for Learning Imbalanced Dataset
Gang Wu & Edward Y . Chang
Department of Electrical & Computer Engineering {gwu@engineering , echang@ece}ucsbedu
University of California , Santa Barbara 93106
Abstract
An imbalanced training dataset poses serious problem for many real world supervised learning tasks . In this paper , we propose a kernel boundary alignment algorithm , which considers training data imbalance as prior information to augment SVMs to improve class prediction accuracy . Using a simple example , we first show that SVMs can suffer from high incidences of false negatives when the training instances of the target class are heavily outnumbered by the training instances of a non target class . The remedy we propose is to adjust the class boundary by modifying the kernel matrix , according to the imbalanced data distribution . Through theoretical analysis backed by empirical study , we show that our kernel boundary alignment algorithm works effectively on several datasets .
1 . Introduction
Support Vector Machines ( SVMs ) are a core machine learning technology . They have strong theoretical foundations and excellent empirical successes in many patternrecognition applications such as handwriting recognition , image retrieval [ 6 ] , and text classification [ 13 ] . However , for applications such as video event recognition [ 25 ] , fraud detection [ 10 ] , and medical image analysis , where the training instances of the target class are significantly outnumbered by the other training instances , the class boundary learned by SVMs can be severely skewed toward the target class . As a result , the false negative rate can be excessively high in identifying important target objects ( eg , a surveillance event or a disease causing agent ) , and can result in catastrophic consequences .
Skewed class boundary is a subtle but severe problem that arises from using an SVM classifier—in fact from using any classifier—for real world problems with imbalanced training data . To understand the nature of the problem , let us consider it in a binary classification setting ( positive vs . negative ) . We know that the Bayesian framework estimates
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE the posterior probability using the class conditional and the prior [ 11 ] . When the training data are highly imbalanced , it can be inferred that the state of the nature favors the majority class . Hence , when ambiguity arises in classifying a particular sample because of similar class conditional densities for the two classes , the Bayesian framework will rely on the large class prior in favor of the majority class to break the tie . Consequently , the decision boundary will skew toward the minority class .
While the Bayesian framework gives the optimal results ( in terms of the smallest average error rate ) in a theoretical sense , one has to be careful in applying it to realworld applications . In a real world application such as security surveillance and disease diagnosis , the risk ( or consequence ) of mispredicting a positive event ( a false negative ) far outweighs that of mispredicting a negative event ( a false positive ) . It is well known that in a binary classification problem , Bayesian risks are defined as :
R(αp|x ) = λppP ( ωp|x ) + λpnP ( ωn|x ) R(αn|x ) = λnpP ( ωp|x ) + λnnP ( ωn|x ) where p and n refer to the positive and negative events , respectively , λnp refers to the risk ( or cost ) of a false negative , and λpn the risk of a false positive . Which action ( αp or αn ) to take—or which action has a smaller risk—is affected not just by the event likelihood ( which directly influences the misclassification error ) , but also by the risk of mispredictions ( λnp and λpn ) . How can we factor risk into SVMs to compensate for the effect caused by P ( ωn|x ) >> P ( ωp|x ) ? Examining the class prediction function of SVMs [ 20 ] , n . fi sgn f(x ) = yiαiK(x , xi ) + b
,
( 1 ) i=1 we see that three parameters can affect the decision outcome : b , αi , and K . Our theoretical analysis backed up by empirical study will show that the the only effective method for improving SVMs is through adaptively modifying K based on the training data distribution . To modify K , we propose in this paper the kernel boundary alignment
( KBA ) algorithm , which addresses the imbalanced trainingdata problem in three complementary ways .
1 . Improving class separation . KBA increases intra class similarity and decreases inter class similarity through changing the similarity scores in the kernel matrix . As a consequence , instances in the same class are better clustered in the feature space F away from those in the other classes .
2 . Safeguarding overfitting . To avoid overfitting , KBA uses the existing support vectors to guide its boundaryalignment procedure .
3 . Improving imbalanced ratio . By adjusting the similarity scores between majority instances properly , KBA can reduce the number of support vectors at the majority side and hence improve the imbalanced supportvector ratio .
Our experimental results on both UCI and real world image/video datasets show the kernel boundary alignment algorithm to be effective in correcting the skewed boundary caused by imbalanced training data .
The rest of this paper is organized as follows . Section 2 discusses related work . In Section 3 , we describe the kernelboundary alignment algorithm for addressing the imbalanced training data problem . Section 4 presents the setup and the results of our empirical studies . We offer our concluding remarks in Section 5 .
2 . Related Work
Approaches for addressing the imbalanced training data problem can be divided into two main categories : the data processing approach and the algorithmic approach . The data processing approach can be further divided into two methods : under sample the majority class , and over sample the minority class . The one sided selection proposed by Kubat [ 16 ] is a representative under sampling approach which removes noisy , borderline , and redundant majority training instances . However , these steps typically can remove only a small fraction of the majority instances , so they might not be very helpful in a scenario with a majority to minority ratio of more than 100:1 ( which is becoming common in many emerging pattern recognition applications ) . Multi classifier training [ 5 ] and Bagging [ 3 ] are two other under sampling methods . These methods do not deal with noisy and borderline data directly , but use a large ensemble of sub classifiers to reduce prediction variance . is the opposite of
Over sampling [ 22 ] the undersampling approach . It duplicates or interpolates minority instances in the hope of reducing the imbalance . The over sampling approach can be considered as a “ phantomtransduction ” method . It assumes the neighborhood of a
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE positive instance to be still positive , and the instances between two positive instances positive . The validity of assumptions like these , however , can be data dependent .
The algorithmic approach , which is traditionally1 orthogonal to the data processing approach , is the focus of this paper . Nugroho [ 19 ] suggests combining a competitive learning network and a multilayer perceptron as a solution for the class imbalance problem . Kubat et al . [ 16 ] modify the decision tree generator to improve its learning performance on imbalanced datasets . For SVMs , few attempts [ 15 , 18 , 21 ] have dealt with the imbalanced trainingdata problem . These methods modify parameters b and αi in Eqn . 1 . Karakoulas et al . [ 15 ] proposed an approach to modify the bias ( or parameter b ) in the class prediction function ( Eqn . 1 ) . Veropoulos et al . [ 18 , 21 ] use different pre defined penalty constants ( based on some prior knowledge ) for different classes of data . The effectiveness of this method is limited since the Karush Kuhn Tucker ( KKT ) conditions [ 17 ] use the penalty constants as the upper bounds , rather than the lower bounds , of misclassification costs . Moreover , αiyi = 0 imposes an equal total inthe KKT condition n' i=1 fluence from the positive and negative support vectors . The increases in some αi ’s at the positive side will inadvertently increase some αi ’s at the negative side to satisfy the constraint . These constraints can make the increase of C + on minority instances ineffective . ( Validation is presented in Section 4 . )
Another algorithmic approach to improve the SVMs for imbalanced training is to modify the employed kernel function K . In kernel based methods , such as SVMs , the kernel K represents a pairwise similarity measurement among the data . Because of the central role of the kernel , a poor K will lead to a poor performance of the employed classifier [ 8 , 20 ] . Our prior work ACT [ 23 ] falls into this category by modifying the K using ( quasi ) conformal transformation so as to change the spatial resolution around the class boundary . However , ACT works only when data have a fixed dimensional vector space representation , since the algorithm relies on information in the input space . The kernelboundary alignment algorithm ( KBA ) that we propose in this paper is a more general approach , which modifies kernel matrix2 K instead of kernel function K , and hence does not require the data to have a vector space representation . This relaxation is important so that we can deal with a large class of sequence data ( motion trajectories , DNA sequences , sensor network data , etc. ) , which may have differ
1 Although our algorithmic approach focuses on aligning class boundary , it can effectively remove redundant majority instances as a byproduct . Details will be discussed in Section 3 . 2 Given a kernel function K and a set of instances Xtrain = {xi , yi}n i=1 , the kernel matrix ( Gram matrix ) is the matrix of all possible inner products of pairs from Xtrain , K = ( kij = K(xi , xj ) ) . ent length . Furthermore , KBA provides greater flexibility in adjusting the class boundary . ( We present details in Section 3 ) .
Recently , several kernel alignment algorithms [ 8 , 14 ] have been proposed in the Machine Learning community to learn a kernel function or a kernel matrix from the training data . The motivation behind these methods is that a good kernel should be data dependent , and a systematic method for learning a good kernel from the data is useful . All these methods are based on the notion of the kernel target alignment proposed by Cristianini et al . [ 8 ] . The alignment score is used for measuring the quality of a given kernel matrix . To address the imbalanced training data problem , Kandola et al . [ 14 ] propose an extension to kernel target align1 n+ to the posiment by giving the alignment targets of tive instances and − 1 n− to the negative instances . ( We use n+ and n to denote the number of minority and majority instances , respectively . ) Unfortunately , when n+ n− is small −) ) , the concentra(when n+ does not remain O(n+ + n tion property upon which that kernel target alignment relies may no longer hold . In other words , the proposed method can deal only with imbalanced data that are not very imbalanced . Our proposed KBA algorithm is based on maximizing the separation margin of the SVMs , and is more effective in its solution .
−
3 . Kernel Boundary Alignment
Let us consider a two class classification problem with i=1 , where xi ∈ .m and training dataset Xtrain = {xi , yi}n y ∈ {−1 , +1} . Kernel based methods , such as SVMs , introduce a mapping function Φ which embeds the I into a high dimensional F as a curved Riemannian manifold S where the mapped data reside [ 4 ] . A Riemannian metric gij(x ) is then defined for S , which is associated with the kernel function K(x , x ) by fi gij(x ) =
∂2K(x , x ) ∂xi∂x j
. x =x
( 2 )
The metric gij shows how a local area around x in I is magnified in F under the mapping of Φ . The idea of conformal transformation in SVMs is to enlarge the margin by increasing the magnification factor gij(x ) along the classification boundary and to decrease it around other areas . This could be implemented by a conformal transformation3 of the related kernel K(x , x ) according to Eqn . 2 , so that the spatial relationship between the data would not be affected too much [ 1 ] . Such a ( quasi ) conformal transformation can be
3 Usually , it is difficult to find a totally conformal mapping function to transform the kernel . As suggested in [ 1 ] , we can choose a quasiconformal mapping function for kernel transformation .
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE
Meaning Conformal transformation function Parameters of D(x ) Nearest neighborhood range Number of support instances Number of minority support instances Number of majority support instances
Symbol D(x ) τ 2 b M |SI| |SI+| |SI−| x+ , Φ(x+ ) A minority support instance x− , Φ(x− ) A majority support instance xb , Φ(xb ) β Xtrain X ∗ X + X −
An interpolated boundary instance Weight parameter of interpolation Set of the training instances Sets of the interpolated boundary instances Set of the misclassified minority test instances Set of the misclassified majority test instances b mis mis
Table 1 : Notations Used in ACT and KBA depicted as
˜K(x , x ) = D(x)D(x )K(x , x ) ,
( 3 ) where D(x ) is a properly defined positive ( quasi ) conformal function . D(x ) should be chosen in such a way that the new Riemannian metric ˜gij(x ) , associated with the new kernel function ˜K(x , x ) , has larger values near the decision boundary . ( More details are presented in [ 23] . )
To deal with the skew of the class boundary caused by imbalanced classes , we propose to modify the kernel , according to Eqn . 3 , by considering the imbalanced data distribution as the prior information . In our prior work [ 23 ] , we proposed an adaptive conformal transformation ( ACT ) in SVMs to modify the kernel function K directly in input space I , so as to deal with the imbalanced training dataset problem . ( Please refer to [ 23 ] for the details . ) ACT relies on the data information in I , and hence the fixed dimensional input space must exist .
However , for data that do not have a vector space representation ( eg , sequence data ) , it may not be feasible to transform kernel function K conformally by relying on the information in the input space I . Our kernel boundary alignment ( KBA ) algorithm bypasses this limitation by modifying the kernel matrix K by only relying on the mapped data information in the feature space F . Indeed , as long as the resulting kernel matrix K maintains the positive ( semi ) definite property , the modification is mathematically valid . We will at the end of this section point out the differences between KBA and ACT , and in particular , the additional flexibility hat KBA enjoys in adjusting similarity measures . To assist the reader , Table 1 lists key notations used in this section . KBA modifies kernel matrix K based on the trainingdata distribution in F . Kernel matrix K encodes all pairwise similarity information between the instances in the training dataset . Hence , modifying the kernel matrix transforms the kernel function indirectly . ( Notice that
Feature Space
Center Hyperplane
Interploated
Boudary
Majority SV Hyperplane
Minority SV Hyperplane
Φ x+(
)
B 1
B 2
η
Φ x
(
) f x( )= +1 f x( )= 0 f x( )= 1
Figure 1 : Estimate Boundary Instances in F .
KBA is certainly applicable to data that do have a vectorspace representation , since K = ( kij = K(xi , xj)) . ) Now , because a training instance x might not be a vector , we introduce a more general term , support instance4 , to denote x if its embedded point via K is a support vector in F . In the following subsections , we will first propose a datadependent way to estimate the “ ideal ” class boundary in F ( Section 31 ) We then choose a feasible alignment function D(x ) , which can assign a larger spatial resolution along the estimated “ ideal ” boundary in F ( Section 32 ) Finally , we present KBA ’s iterative training procedure ( Section 33 )
31 Estimation of Boundary
Performing transformation on K or K aims to magnify the spatial resolution along the decision boundary , thereby improving the class separation . According to the work of [ 1 , 23 ] , maximal magnification should be performed along the class boundary . Unfortunately , locating the class boundary in input space I is difficult [ 1 ] . Instead , KBA locates the class boundary in feature space F through interpolation . In F , the class boundary learned from the training data is the center hyperplane in the margin . When the training dataset is balanced , the center hyperplane approximates the “ ideal ” boundary well . However , when the training dataset is imbalanced , the decision boundary is skewed toward the minority class . To compensate for this skew , KBA gives the maximal magnification to an interpolated boundary between the center hyperplane and the hyperplane formed by the majority support instances in F . Figure 1 illustrates how the interpolation procedure works . Φ(x+ ) and Φ(x− ) in the figure denote a instance and a majority support minority support instance , respectively . A boundary instance Φ(xb ) on the “ ideal ” boundary should reside between the center hyperplane ( the thick line in the middle of Figure 1 ) and the majority support instance hyperplane ( the dash line on the right hand side of the figure ) . We can thus estimate the location of Φ(xb ) by interpolating the positions of Φ(x+ ) and Φ(x− ) as follows :
1 2
( 4 )
Φ(xb ) = ( 1 − β)Φ(x+ ) + βΦ(x− ) ,
≤ β ≤ 1 . 2 , and Φ(xb ) When the training dataset is balanced , β is 1 lies on the center hyperplane ( eg , point B1 in the figure ) . In this balanced case , the estimated “ ideal ” boundary coincides with the learned boundary . When the training dataset is imbalanced , however , we need to adjust β to estimate the “ ideal ” boundary . The key research question to answer is : “ How to determine β in a data dependent way ? ”
We propose a cost function for measuring the loss caused by false negatives and false positives when different values of β are introduced . We choose the β which can achieve the minimal cost . Let X + mis denote the set of the misclassified minority test instances and X − mis the set of the misclassified majority test instances . We define the cost functional C(· ) for any scalar decreasing loss functions cp(· ) and cn(· ) as follows :
|X + i=1
C(η ) = where f mis| . mis| . (xi ) = f(xi ) + η , 0 ≤ η ≤ 1
(xi ) ) + cp ( yif
|X − i=1 cn ( yif
(xi ) ) ,
In the equation above , f(xi ) is the SVM predication score for test instance xi , η is the offset of the interpolated boundary from the center hyperplane , as shown in Figure 1 , and (xi ) is the associated margin in F of the instance xi yif with respect to the interpolated class boundary . The loss functions cp(· ) and cn(· ) are used to penalize the misclassified5 minorities ( false negative ) and majorities ( false positive ) , respectively . Each loss function , cp(· ) or cn(· ) , can be chosen as any scalar decreasing function of the margin (xi ) according to the prior knowledge . When no prior yif knowledge is available , usually , we can choose the exponential loss function as cp(· ) and the log likelihood loss function as cn(· ) , i.e , cp(yif cn(yif
(xi ) ) = exp(−yif (xi) ) , (xi ) ) = ln(1 + exp(−yif
(xi)) ) .
The justification of choosing them as the loss functions comes from boosting [ 12 ] , where the exponential loss criterion concentrates much more influence ( exponentially ) on
4
In the KBA algorithm , if x is a support instance , we call both x and its embedded support vector via K in F support instance .
5
In KBA , we only consider the misclassified test instances in the margin so as to reduce the influence from the outliers . Their SVM scores f ( x ) range from −1 to +1 .
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE
(xi ) < 0 ) , observations with large negative margins ( yif and the log likelihood loss concentrates relatively less influence ( linearly ) on such observations . Since KBA aims to concentrate on false negatives , we use the exponential loss as cp(· ) and the log likelihood loss as cn(· ) .
The optimal η is then chosen by minimizing the total loss induced by all test instances falling into the margin of SVMs ,
∗
∗ = arg min
η
η
C(η ) , 0 ≤ η ≤ 1 .
∗ can be calculated from ∂C(η )
∂η = 0 and trunThe optimal η cated between 0 and 1 . After the optimal position η of the interpolated boundary is calculated , we can obtain β in Eqn . 4 as follows :
∗
β =
∗
.
1 + η
2
32 Selection of D(x )
After interpolating a boundary in the margin , we then magnify the spatial resolution along the boundary by modifying the Riemannian metric gij(x ) according to Eqn . 2 and Eqn . 3 . When given a prior kernel , gij(x ) is determined by the conformal function D(x ) . As what we discussed in the beginning of Section 3 , a good D(x ) function should be larger when x is closer to the boundary in F so as to achieve a larger spatial resolution around the boundary . According to this criteria , we choose D(x ) as a set of Gaussian functions :
. ff
D(x ) =
1 |X ∗ b | exp xb∈X ∗ b
−'Φ(x ) − Φ(xb)'2
τ 2 b
,
( 5 ) where τ 2 b is a parameter controlling the magnitude of each exponential function in D(x ) . For a given instance x , D(x ) is calculated as the average of all exponential functions , each of which is related with one interpolated boundary instance Φ(xb ) in X ∗ b set . In addition , 'Φ(x ) − Φ(xb)'2 is calculated via the kernel trick as follows
'Φ(x ) − Φ(xb)'2 ='Φ(x ) − ( 1 − β)Φ(x+ ) − βΦ(x−)'2 =kxx + ( 1 − β)2kx+x+ + β2kx−x−
− 2(1 − β)kxx+ − 2βkxx− + 2β(1 − β)kx+x− ,
( 6 ) where kxx is from the kernel matrix K . When the instance x is an unseen test instance , kxx is computed using the predefined similarity measurement which generates the kernel matrix K .
According to [ 1 , 20 ] , we have the following corollary to guarantee the kernel transformation induced by D(x ) , as defined in Eqn . 5 , performs a mathematically valid conformal transformation .
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE
(
Corollary 1 The function D(x ) defined in Eqn . 5 gives a valid conformal transformation on feature space F induced by the pre defined kernel matrix K . Proof . Please refer to [ 24 ] .
In KBA , we adaptively choose τ 2 b in a data dependent way as τ 2 b = AVGi∈{Dist2(xi,xb)<M
Dist2(xi , xb )
,
( 7 ) where the neighborhood range M is the margin value . The distance Dist2(xi , xb ) between two interpolated boundary instances xi and xb is 'Φ(xi ) − Φ(xb)'2 and can be computed using Eqn . 4 and Eqn . 6 . Notice that we do not need to scale τ 2 b as in ACT [ 23 ] for dealing with the imbalanced training data problem , since we have considered this factor when interpolating the class boundary and selecting D(x ) . We believe that our adjusted interpolation procedure and selection of D(x ) enjoy two benefits .
1 . Improved class prediction accuracy . In the imbalanced situation , most of misclassified minority instances fall into the margin area between the center hyperplane and the majority support vector hyperplane . By maximizing the spatial resolution in this area , we expect to move those ambiguous instances as far away from the decision boundary as possible , so as to improve classprediction accuracy .
2 . Improved imbalance ratio . Since the majority support instances are located nearer the interpolated bound2 ≤ β ≤ 1 ary than the minority support instances ( 1 in Eqn . 4 ) , by choosing a proper form of D(x ) as in Eqn.5 , we can increase the degree of similarity between majority support instances and make them close each other in feature space after kernel transformation . This increase can lead to a reduction of the number of majority support instances , and hence improve the imbalanced support instance ratio .
33 Retraining
After choosing D(x ) , KBA modifies the given kernel matrix K = ( kij ) in the following way .
˜kij = D(xi ) × D(xj ) × kij .
( 8 ) The new kernel matrix ˜K after modification is then put back into the regular SVM algorithm for retraining . We have the following corollary , supported by the work of [ 20 ] , to guarantee that the new kernel matrix after transformation in Eqn . 8 is a valid kernel matrix . Corollary 2 When given a positive ( semi )definite kernel matrix K , the kernel transformation defined in Eqn . 8 results in a new kernel matrix ˜K which is also positive ( semi ) definite . Proof . Please refer to [ 24 ] .
DATASET SEGMENTATION GLASS EUTHYROID CAR YEAST ABALONE
# ATTRIB 19 10 24 6 8 8
# POS 30 29 238 69 51 32
# NEG 180 185 1762 1659 1433 4145
|SI−| |SI+| 1.8:1 2.0:1 1.5:1 1.8:1 3.0:1 9.0:1
SVMs
98.1 ± 5.1 89.9 ± 6.3 92.8 ± 3.6 99.0 ± 2.2 59.0 ± 12.1 0.0 ± 0.0
SMOTE 98.1 ± 5.1 91.8 ± 6.5 92.4 ± 4.3 99.0 ± 2.3 69.9 ± 10.0 0.0 ± 0.0
ACT
98.1 ± 5.1 93.7 ± 6.7 94.5 ± 3.0 99.9 ± 0.2 78.5 ± 4.5 51.9 ± 7.6
KBA
98.1 ± 5.1 93.7 ± 6.6 94.6 ± 2.9 99.9 ± 0.2 82.2 ± 7.1 57.8 ± 5.4
Table 2 : Mean and standard deviation of g Means prediction accuracy on UCI datasets .
4 . Experimental Results
Our empirical study examined the effectiveness of the kernel boundary alignment algorithm in two aspects .
1 . Vector space evaluation . We compared KBA with other algorithms for imbalanced data learning . We used six UCI datasets and an image dataset to conduct this evaluation . ( We present the datasets shortly . ) 2 . Non vector space evaluation . We evaluated the effectiveness of KBA on a set of video surveillance data , which are represented as spatio temporal sequences that do not have a vector space representation . In our experiments , we used C SVMs as our yardstick to measure how other methods perform . We employed Laplacian kernels of the form exp(−γ|x− x | ) as K(x , x ) of CSVMs . Then we used the following procedure : The dataset was first randomly split into training and test subsets generated in a certain ratio which was empirically chosen to be optimal on each dataset for the regular C SVMs . Hyperparameters ( C and γ ) of K(x , x ) were obtained for each run using 7 fold cross validation . All training , validation , and test subsets were sampled in a stratified manner ensuring each of them had the same negative/positive ratio [ 16 ] . We repeated this procedure seven times , computed average class prediction accuracy , and compared the results . The detailed choices of parameters are presented in Sections 411 and 412
41 Vector space Evaluation
For this evaluation , we used six UCI datasets and a 116category image dataset . The six UCI datasets we experimented with are abalone ( 19 ) , car ( 3 ) , segmentation ( 1 ) , yeast ( 5 ) , glass ( 7 ) , and euthyroid ( 1 ) . The class label in the parentheses indicates the target class we chose . Table 2 shows the characteristics of these six datasets organized according to their negative to positive training instance ratios . The top three datasets ( segmentation , glass , and euthyroid ) are not too imbalanced . The middle two ( car and yeast ) are mildly imbalanced . The bottom dataset ( abalone ) is the most imbalanced ( the ratio is about 130:1 ) .
The image dataset contains 20K images in 116 categories collected from the Corel Image CDs6 . Each image is repre
6 We exclude from our testbed those categories that cannot be classified automatically , such as “ industry ” , “ Rome ” , and “ Boston ” . ( Eg ,
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE sented by a vector of 144 dimensions including color , texture , and shape features [ 6 ] . To perform class prediction , we employed the one per class ( OPC ) ensemble [ 9 ] , which trains 116 classifiers , each of which predicts the class membership for one class . The class prediction on a testing instance is decided by voting among the 116 classifiers . 411 Results on UCI Benchmark Datasets . Tables 2 and 3 report the experimental results with the six UCI datasets . In addition to conducting experiments with SVMs , ACT , and KBA , we also implemented and tested one popular minority oversampling strategy SMOTE [ 7 ] . We used the L2 norm RBF function for D(x ) in ACT . In each run , the training and test subsets were generated in the ratio 6:1 . For SMOTE7 , the minority class was over sampled at 200 % , 400 % and 1000 % for each of three groups of UCI datasets in Table 2 , respectively . a+ · a− , where a+ and a
We report in Table 2 using the Kubat ’s g means metric are positive ( the tardefined as get class ) and negative testing accuracy , respectively [ 16 ] . Means and standard deviations of the experimental results are both reported in the table . In all the six datasets , KBA achieves the highest or ties for the highest accuracy . ( The best results are marked in bold . ) When the data is very imbalanced ( the last row abalone of Table 2 ) , both SVMs and SMOTE cannot make accurate predictions . KBA achieves 57.8 % mean class prediction accuracy ( in g means ) , and shows 5.9 percentile points improvement over ACT .
√
−
We also report in Table 3 using AUC [ 2 ] defined as the area under an ROC curve to compare the four strategies on the six UCI datasets . An ROC curve demonstrates the tradeoff between true positive rate and false positive rate in binary classification problems as a function of varying a classification threshold . In our experiments , since all four strategies use Eqn . 1 as the decision function for classification , we varied the classification threshold θ by sgn(f(x ) − θ ) to determine the label of x , so as to draw the ROC curves . Means and standard deviations of the AUC scores are reported in the table . For readability , we report AUCs as percentages between 0 % and 100 % , instead of between 0 and
7 the Boston category contains various subjects , eg , architectures , landscapes , and people , of Boston . ) For the datasets in Table 2 from top to bottom , for SMOTE , the optimal γ was 0.002 , 0.003 , 0.085 , 0.3 , 0.5 , and 0.084 , respectively . For SVMs , ACT , and KBA , the optimal γ was 0.004 , 0.003 , 0.08 , 0.3 , 0.5 , and 0.086 , respectively . All optimal C ’s were 1 , 000 .
DATASET SEGMENTATION GLASS EUTHYROID CAR YEAST ABALONE
SVMs
100.0 ± 0.0 96.9 ± 3.0 96.6 ± 2.2 99.8 ± 0.2 89.2 ± 5.4 62.5 ± 12.1
SMOTE 100.0 ± 0.0 97.1 ± 3.1 96.0 ± 2.8 99.8 ± 0.2 91.1 ± 5.0 62.5 ± 12.1
ACT
100.0 ± 0.0 98.5 ± 2.5 98.2 ± 1.8 99.9 ± 0.1 93.8 ± 2.2 80.2 ± 7.1
KBA
100.0 ± 0.0 98.9 ± 2.6 98.8 ± 1.5 99.9 ± 0.1 95.2 ± 2.5 87.4 ± 6.8
Table 3 : Mean and standard deviation of AUCs ( in % ) on UCI datasets .
1 . Again , KBA achieves the highest mean AUCs in all six UCI datasets . Compared to ACT , KBA generated better results especially for the last datasets ( yeast and abalone ) , with 1.4 and 7.2 percentile points improvement , respectively . Such gains bear out the flexibility and superiority of KBA working in feature space F . Statistically , the higher AUCs from KBA means that our KBA algorithm will favor in classifying a positive ( target ) instance with a higher probability than other algorithms and hence could well tackle the imbalanced training dataset problem .
412 Results on 20K Image Dataset . The image dataset is more imbalanced than the UCI datasets . We first set aside 4K images to be used as the test subset ; the remaining 16K images were used for training and validation . We compared four schemes : SVMs , BP ( the biased penalty method of [ 18 , 21] ) , ACT , and KBA . Notice that in this experiment , we used the L1 norm RBF function for D(x ) in ACT , since the L1 norm RBF works best for the image dataset [ 6 ] .
Table 4 presents the prediction accuracy for twelve representative categories out of 116 , sorted by their imbalance ratios . KBA improves the accuracy over SVMs by 5.3 , 5.9 , and 15.5 percentile points on the three subgroup datasets , respectively . KBA achieves the best prediction accuracy for nine out of twelve categories among all schemes ( marked by bold font ) . BP outperforms SVMs , but only slightly . ( We have predicted BP ’s ineffectiveness , due to the KKT conditions , in Section 2 . ) Remark . From Table 4 , we can see that on this challenging dataset of several diversified classes , the results of all algorithms , including KBA , are not stellar ( class predication accuracy is less than 50 % for almost all classes ) . This low accuracy is caused partly by a large number of classes ( 116 ) , and partly by not so perfect image feature extraction ( due to the state of the available image processing technologies ) .
42 Non vector space Evaluation
For our multi camera video surveillance project , we recorded video data at a campus parking lot . We collected trajectories depicting five motion patterns : circling ( 30 instances ) , zigzag pattern or M pattern ( 22 instances ) , back forth ( 40 instances ) , go straight ( 200 instances ) , and parking ( 3 , 161 instances ) . We divided these events into benign and suspicious categories and aimed to detect suspicious events with high accuracy . The benign event category consists of patterns go straight and parking , and the
CATEGORY MOUNTAIN SNOW DESERT DOG WOMAN CHURCH LEAF LIZARD PARROT HORSE LEOPARD SHARK
RATIO 34:1 37:1 39:1 44:1 54:1 66:1 80:1 101:1 263:1 264:1 283:1 1232:1
SVMs BP 24.8 24.8 47.8 46.4 34.3 33.7 35.2 32.9 26.2 27.9 21.8 21.8 24.8 26.1 15.1 13.9 7.1 7.1 14.3 14.3 7.7 7.7 0.0 0.0
ACT KBA 33.3 34.5 52.3 54.6 36.8 39.1 42.7 41.5 39.1 35.3 20.0 20.6 37.2 32.6 25.4 22.2 18.4 14.3 28.6 32.9 23.1 23.1 16.6 16.6
Table 4 : Image dataset prediction accuracy . suspicious event category consists of the other three patterns .
For each experiment , we chose 60 % of the data as the training set , keeping the remaining 40 % to use as our testing data . We employed a sequence alignment kernel to compute similarity between two trajectories [ 25 ] . Figure 2(a ) reports the means and standard deviations of sensitivities of using SVMs and two methods of improving the SVMs . Both methods , BP and KBA , can improve sensitivity . However , our KBA achieves significantly larger magnitude of improvement over SVMs , around 30 percentile points . Figure 2(b ) shows that all methods maintain high specificity . Overall , BP does not work effectively , which bears out our prediction in Section 2 . 5 . Conclusion
We have proposed the kernel boundary alignment algorithm for tackling the imbalanced training data challenge . Through theoretical justifications and empirical studies , we show this method to be effective . We believe that kernelboundary alignment is attractive , not only because of its accuracy , but also because it can be applied to learning both vector data and sequence data ( eg , DNA sequences and spatio temporal patterns ) through modifying the kernel matrix directly .
6 . Acknowledgement
We would like to thank the support of two NSF grants
NSF Career IIS 0133802 and NSF ITR IIS 0219885 .
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE
)
%
( e g a t n e c r e P
100
90
80
70
60
50
40
30
20
10
0
SVMs BP KBA
)
%
( e g a t n e c r e P
100
90
80
70
60
50
40
30
20
10
0
Circling
M pattern
Back forth
Circling
M pattern
Back forth
( a ) Sensitivity ( b ) Specificity Figure 2 : Sensitivity vs . Specificity on trajectory dataset .
SVMs BP KBA
References
[ 1 ] S . Amari and S . Wu .
Improving support vector machine classifiers by modifying kernel functions . Neural Networks , 12(6):783–789 , 1999 .
[ 2 ] A . P . Bradley . The use of the area under the roc curve in the evaluation of machine learning algorithms . Pattern Recognition , 30:1145–1159 , 1997 .
[ 3 ] L . Breiman . Bagging predictors . Machine Learning , 24:123–
140 , 1996 .
[ 4 ] C . Burges . Geometry and invariance in kernel based methods . in adv . in kernel methods : Support vector learning . pages 89–116 , 1999 .
[ 5 ] P . Chan and S . Stolfo . Learning with non uniform class and cost distributions : Effects and a distributed multi classifier approach . Workshop Notes KDD 98 Workshop on Distributed Data Mining , pages 1–9 , 1998 .
[ 6 ] E . Y . Chang , K . Goh , G . Sychay , and G . Wu . Contentbased soft annotation for multimodal image retrieval using bayes point machine . IEEE Transactions on Circuits and Systems for Video Technology Special Issue on Conceptual and Dynamical Aspects of Multimedia Content Description , 13(1):26–38 , January 2003 .
[ 7 ] N . Chawla , K . Bowyer , L . Hall , and W . P . Kegelmeyer . Smote:synthetic minority over sampling technique . International Conference on Knowledge Based Computer Systems , 2000 .
[ 8 ] N . Cristianini , J . Kandola , A . Elisseeff , and J . Shawe Taylor . On kernel target alignment . Journal Machine Learning Research , 1 , 2002 .
[ 9 ] T . Dietterich and G . Bakiri . Solving multiclass learning problems via error correcting output codes . Journal of Artifical Intelligence Research , 2:263–286 , 1995 .
[ 10 ] T . Fawcett and F . Provost . Adaptive fraud detection . In Data
Mining and Knowledge Discovery , 1(3):291–316 , 1997 .
[ 11 ] K . Fukunaga .
Introduction to Statistical Pattern Recogni tion . Academic Press , Boston , MA , 2 edition , 1990 .
[ 12 ] T . Hastie , R . Tibshirani , and J . Friedman . The Elements of Statistical Learning : Data Mining , Inference , and Prediction . Springer , New York , 2001 .
[ 13 ] T . Joachims . Text categorization with support vector machines : learning with many relevant features . In Proceedings of ECML 98 , the tenth European Conference on Machine Learning , pages 137–142 , 1998 .
Proceedings of the Fourth IEEE International Conference on Data Mining ( ICDM’04 ) 0 7695 2142 8/04 $ 20.00 IEEE
[ 14 ] J . Kandola and J . Shawe Taylor . Refining kernels for regression and uneven classification problems . In Proceedings of the ninth International Workshop on Artificial Intelligence and Statistics , 2003 .
[ 15 ] G . Karakoulas and J . S . Taylor . Optimizing classifiers for In Advances in Neural Informa imbalanced training sets . tion Processing Systems , 1999 .
[ 16 ] M . Kubat and S . Matwin . Addressing the curse of imbalanced training sets : One sided selection . In Proceedings of the fourteenth International Conference on Machine Learning , pages 179–186 , 1997 .
[ 17 ] H . W . Kuhn and A . W . Tucker . Non linear programming . Proc . 2nd Berkeley Syrup . on Mathematical Statistics and Probability , Univ . Calif . Press , 1961 .
[ 18 ] Y . Lin , Y . Lee , and G . Wahba . Support vector machines for classification in nonstandard situations . Machine Learning , 46:191–202 , 2002 .
[ 19 ] A . Nugroho , S . Kuroyanagi , and A . Iwata . A solution for imbalanced training sets problem by combnet ii and its application on fog forecasting . IEICE Transaction on Information and Systems , E85 D(7):1165–1174 , July 2002 .
[ 20 ] B . Scholkopf and A . Smola . Learning with Kernels : Support Vector Machines , Regularization , Optimization , and Beyond . MIT Press , Cambridge , MA , 2002 .
[ 21 ] K . Veropoulos , C . Campbell , and N . Cristianini . Controlling the sensitivity of support vector machines . Proceedings of the International Joint Conference on Artificial Intelligence , pages 55–60 , 1999 .
[ 22 ] G . M . Weiss and F . Provost . The effect of class distribution on classifier learning : An empirical study . Technical Report ML TR 44 , Department of Computer Science , Rutgers University , August 2001 .
[ 23 ] G . Wu and E . Y . Chang . Adaptive feature space conformal transformation for imbalanced data learning . In Proceedings of the twentieth International Conference on Machine Learning , pages 816–823 , August 2003 .
[ 24 ] G . Wu and E . Y . Chang .
Kernel boundary alignment report , http://mmdbeceucsbedu/∼gangwu/Papers/kbapdf , May 2004 . imbalanced data training . for
Technical
[ 25 ] G . Wu , Y . Wu , L . Jiao , Y FWang , and E . Y . Chang . Multicamera spatio temporal fusion and biased sequence data learning for security surveillance . ACM International Conference on Multimedia , November 2003 .
