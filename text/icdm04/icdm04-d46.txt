SCHISM : A New Approach for Interesting Subspace Mining ⁄
Department of Computer Science , Rensselaer Polytechnic Institute , Troy , New York 12180
Karlton Sequeira and Mohammed Zaki
Abstract
High dimensional data pose challenges to traditional clustering algorithms due to their inherent sparsity and data tend to cluster in different and possibly overlapping subspaces of the entire feature space . Finding such subspaces is called subspace mining . We present SCHISM , a new algorithm for mining interesting subspaces , using the notions of support and Chernoff Hoeffding bounds . We use a vertical representation of the dataset , and use a depth first search with backtracking to find maximal interesting subspaces . We test our algorithm on a number of highdimensional synthetic and real datasets to test its effectiveness .
1 Introduction
Clustering is an unsupervised learning process , in which a multidimensional space is partitioned into disjoint regions , such that all points within any such region/cluster are similar to each other , but dissimilar with respect to points in other clusters . If the clustering is done using all available features , it is called a full dimensional clustering . Many such algorithms like BIRCH , DBSCAN , CURE [ 14 ] have been proposed for this task . While they show acceptable performance on lower dimensional datasets , a large number of dimensions poses problems [ 15 ] . One of the main reasons is that data is generally very sparse in high dimensional datasets . Also , most of the full dimensional algorithms use distance metrics , which treat every dimension with equal importance . For high dimensional spaces , it has been argued that under certain reasonable assumptions on the data distribution , the ratio of the distances of the nearest and farthest neighbors to a given target is almost 1 for a variety of distance functions and data distributions [ 7 ] . In such a scenario , many full dimensional clustering algorithms have little meaning , as the pairwise distances between the points in distinct clusters need not provide an acceptable contrast . Solutions proposed include , designing new distance metrics [ 1 ] and dimension reduction [ 12 ] . Dimension reduction techniques , such as the Karhunen Loeve transformation ( KLT ) or singular value decomposition ( SVD ) , project the dataset from the original d to a k dimensional space , where k ¿ d , and each new dimension is a linear combination of the original dimensions ; after this clustering is done using only the k dimensions . Such a strategy may be inappropriate since clusters in the transformed feature space may be hard to interpret . Also , data is only clustered in a single k dimensional space . [ 4 , 20 ] cite examples in which KLT does not reduce the dimensionality without trading off considerable information , as the dataset contains subsets of points which lie in different and sometimes overlapping lower dimensional subspaces . Another method of
⁄This work was supported by NSF Grant EIA 0103708 under the KDD program , NSF CAREER Award IIS 0092978 , and DOE Early Career PI Award DE FG02 02ER25538 . dimension reduction is feature selection , in which some of the dimensions are selected heuristically without transformation [ 8 ] . This removes the problem of interpretability , but still only a fixed subspace is used for clustering . These challenges have caused the focus of much recent work in clustering to shift towards finding the interesting subspaces within a high dimensional space[4 , 20 , 10 , 19 , 17 , 2 , 3 , 9 ] . Other challenges encountered in subspace mining are that subspaces may share dimensions as well as objects , etc . The subspace mining problem has wide applications , especially with datasets having ordinal/nominal values , eg , datasets found in bioinformatics , intrusion detection , etc .
In this paper , we tackle the problem of finding statistically ‘interesting’ subspaces in a high dimensional dataset using an algorithm called SCHISM ( Support and ChernoffHoeffding bound based Interesting Subspace Miner ) . We use the Chernoff Hoeffding bound to prune the search for interesting subspaces , as a nonlinear function of the number of dimensions in which the subspace is constrained . We use a vertical representation of the dataset and capitalize on various advances made in itemset mining . We use a depthfirst search with backtracking to find maximal interesting subspaces . We finally test our algorithm on a wide array of high dimensional datasets .
2 Related Work
Subspace clustering methods may be classified into two main categories : density based and projected clustering . Density based Clustering Agrawal et al . [ 4 ] , proposed CLIQUE , which discretizes the domain of each of the d dimensions into a user specified number , » , of equalwidth intervals . They use support ( the fraction of points that lie in a subspace ) to denote the density of a subspace ; only those subspaces above a minimum density threshold are mined . Using a bottom up Apriori like [ 5 ] approach , higher dimensional ‘dense’ maximal , hyperrectangular subspaces are mined . To prune their search at a faster rate , they use the minimum description length ( MDL ) principle as a heuristic , thereby making it an approximate search . They then merge ‘dense’ subspaces sharing common faces , and use covering algorithms to mine the minimal descriptions of the subspaces .
Instead of support , Cheng [ 10 ] , proposed using entropy as a measure of subspace interestingness . Subspaces satisfying an entropy threshold are mined . Nagesh et al . in MAFIA[19 ] , partition each dimension into variable width intervals , based on the distribution of points . An interval is considered ‘dense’ if the number of points in it exceeds the threshold ( fian)=Di , where n is the number of points in the dataset and fi is a user specified parameter , called the cluster dominance factor . Here , ( an)=Di corresponds to the number of points expected to lie inside the interval of width a in the i th dimension , which has range Di . Using adaptive width intervals minimizes rigidity of clusters obtained by CLIQUE .
Kailing et al [ 18 ] suggest using a sample of the points in the dataset . They generate dense subspaces enclosing each point of the sample if it is a core object , ie , if it has more than M inP ts , a user specified threshold , points within a threshold radius † . The subspaces are then assigned a quality rating , which takes into account the number of dimensions in which the subspace is constrained and this rating is used to prune lower quality subspaces . By providing a rating , it is only possible for the user to determine the relative interestingness of a subspace wrt another subspace . It is not easy for the user to know the absolute interestingness of the subspace .
Projected Clustering Aggarwal [ 2 , 3 ] uses projective clustering to partition the dataset into clusters occurring in possibly different subsets of dimensions in a high dimensional dataset . PROCLUS [ 2 ] seeks to find axis aligned subspaces by partitioning the set of points and then uses a hill climbing technique to refine the partitions . ORCLUS [ 3 ] , finds arbitrarily oriented clusters by using ideas related to singular value decomposition . Both the algorithms require the number of clusters and the expected number of dimensions for each cluster to be input .
In DOC [ 20 ] , Procopiuc et al . devise a Monte Carlo algorithm for finding projective clusters . They propose a mathematical formulation for the notion of optimal projective cluster based on the density of the points in the subspaces . In LDR [ 9 ] , Chakrabarti et al . search for local correlations in the data and perform dimensionality reduction on the locally correlated clusters of data individually .
3 Interestingness Measure
Let A = fA1 ; A2 ; : : : ; Adg be the set of dimensions . Each dimension Ai has a totally ordered and bounded domain . Then , S = A1 £ A2 £ : : : £ Ad is the highdimensional space . The input DB , is a set of n points , DB = fpiji 2 [ 1 ; n ] ; pi 2 Sgg . We partition S into nonoverlapping rectangular units , obtained by partitioning each dimension into » intervals of equal width . Definition 1 : A subspace is an axis aligned hyperrectangle , [ l1 ; h1 ] £ [ l2 ; h2 ] £ : : : £ [ ld ; hd ] , where li = ( aDi)=» , and hi = ( bDi)=» ; a ; b are positive integers , and a < b • » If hi ¡ li = Di , the subspace is unconstrained in dimension i whose range is given as Di . A m subspace is a subspace constrained in m dimensions , denoted as Sm .
Let Xp be the random variable(RV ) denoting the number of points in Sp . If the probability of finding np points in Sp , is bounded by a reasonably low , user specified threshold probability ¿ , Sp is considered to be interesting 1 , ie , P r(Xp ‚ np ) • ¿ implies that Sp is an interesting subspace . Accordingly , we have Definition 2 : A subspace is interesting if the number of points it contains is statistically significantly higher than that expected under the assumption that all dimensions are independent and uniformly distributed .
It is obvious that a dataset that is scattered uniformly and independently , spanning the entire S , is of least interest from a clustering view point , as the entropy is maximized .
1Typically ¿ is set to O( 1 n ) ¿ 0:05 , which is statistically significant
Lemma 1 ( Effect of monotonicity on thresh ) :
If a subspace deviates significantly from the uniform distribution , then it is potentially interesting 2 . If np points are found in Sp , CLIQUE considers Sp to be ‘dense’ if np=n ‚ s , where s is the user specified support threshold . MAFIA considers the subspace ‘dense’ if np=n ‚ ( fia)=Di where fi is the cluster dominance factor . In general , all density based subspace finding algorithms , use a threshold function thresh : Z + ! < where Z + is the set of positive integers , and denotes the number of constrained dimensions in a candidate subspace . The value of thresh 2 < corresponds to the density threshold that must be exceeded for the candidate subspace to be called ‘dense’ . For example , support based pruning in CLIQUE , threshCLIQU E(p ) = s;8p 2 [ 1 ; d ] , ie , no matter what the number of constrained dimensions of a subspace , the pruning threshold is a constant . The thresh function can intuitively be either constant ( as in CLIQUE ) or monotonically increasing or monotonically decreasing . If any subspace Sp+1 ‰ S is interesting , then every psubspace Sp , which encloses Sp+1 and is unconstrained in one of the ( p+1 ) constrained dimensions of Sp+1 , is always interesting if thresh(p + 1 ) ‚ thresh(p ) ; 1 • p • d ¡ 1 , for density function thresh . Proof : If Sp+1 is interesting , np+1 n ‚ thresh(p + 1 ) . But , np ‚ np+1 because Sp+1 ‰ Sp . Thus , np n ‚ np+1 n ‚ thresh(p + 1 ) . If , thresh(p + 1 ) ‚ thresh(p ) , then np n ‚ thresh(p ) and monotonicity is guaranteed . Lemma 2 For p=1 . . . d , let thresh be monotonically non decreasing and let thresh2(p ) = thresh(r ) , where r is the number of dimensions constrained in Sr , a maximally interesting subspace under thresh . If Sr ( cid:181 ) Sp , then Sp is also interesting under thresh2 . Proof : Since Sr is interesting , then nr n ‚ thresh(r ) . Also , as Sr ( cid:181 ) Sp ; 1 • p • r ¡ 1 ; np ‚ nr . Hence , np n ‚ nr n ‚ thresh(r ) = thresh2(p ) . Thus , Sp is also interesting under thresh2 . A consequence of the above lemmas is that it in order to find all the maximal interesting subspaces found by thresh2 , one must set thresh(1 ) to a very low value so that it converges to thresh2 . This causes generation of a number of candidate subspaces which do not yield maximal interesting subspaces but add to computation time . Hence , it makes little sense to have a monotonically increasing threshold function . The thresh function must be either constant or monotonically decreasing . However , it has been observed , that in order to find small subspaces , the constant support threshold function has to be set very low,which makes subspace mining very slow . Hence , we propose a non linear monotonically decreasing threshold function , which does not guarantee mining all interesting subspaces,3 but does mine in a more reasonable time . The intuition behind why this might work , is that as p increases , the subspace becomes constrained in more and more dimensions , making its volume smaller and smaller . Hence the threshold too , must decrease for the enclosed ( Sp+1 ) and enclosing ( Sp ) subspaces to have comparable interestingness .
2See Eq 3 and following comment 3For monotonically decreasing thresh , np+1 not be applicable . Consider , np thresh(p + 1 ) < thresh(p ) , then np n interesting although Sp+1 is . n = the Apriori principle may If n < thresh(p ) and Sp is not
= thresh(p + 1 ) .
3.1 Chernoff Hoeffding bound
We use the Chernoff Hoeffding bound [ 11 , 16 ] to bound the tail of the distribution of Xp and measure the level of interestingness . If Yi ; i = 1 : : : n ; are independently distributed RV , with 0 • Yi • 1 and V ar[Yi ] < 1 , then for Y = Pn i=1 Yi ; t > 0 ,
P r[Y ‚ E[Y ] + nt ] • e¡2nt2
( 1 ) where E[Y ] = Pn i=1 E[Yi ] by linearity of expectation .
Given a p subspace Sp , let Yi correspond to the RV that the ith point in DB , when projected onto the set of p constrained dimensions of Sp , lies within Sp . Then Y = Xp . Using Eq ( 1 ) and for some real tp > 0 , Sp is interesting if ,
P r[Xp ‚ np ] • e¡2nt2 p • ¿
( 2 ) where E[Xp ] + ntp = np , which implies that tp = np n ¡ n . Substituting tp in the right hand term of ( 2 ) , we have
E[Xp ] e¡2n‡ np n ¡
E[Xp ] n ·2
• ¿ which on simplification gives , np n ‚
E[Xp ] n
+r 1
2n ln(
1 ¿
)
( 3 )
Thus , for a p subspace to be interesting , ( 3 ) must hold . ( 3 ) makes no assumption , other than independence , about the comparative distribution and hence can be extended to find subspaces interesting wrt distributions , other than that having uniformly distributed dimensions .
E[Xp ]
2n ln( 1 n +q 1
In comparison with CLIQUE , the term q 1
Note that the interestingness measure , thresh(p ) = ¿ ) is a non linear monotonically decreasing function in the number of dimensions p , in which Sp is constrained . Also , note that thresh is analogous to the support and density threshold measures used to prune search in the CLIQUE [ 4 ] and MAFIA [ 19 ] algorithms respectively . ¿ ) corresponds to minimum density s set by the user . The interestingness threshold probability ( ¿ ) seems intuitively easier to set than s . The chief difference is the term E[Xp ] n , which makes pruning conscious of the volume of the subspace and hence conscious of the number of constrained dimensions of the subspace on which it is being carried out . Unlike earlier proposed interestingness measures [ 18 ] , this one gives the user a sense of absolute interestingness .
2n ln( 1
If we assume that each dimension in the d dimensional space is independent and uniformly distributed and discretized into » levels , then the probability that a point lies in a specific interval of any dimension is 1 » . Hence , the probability that a point lies in a specific p subspace ( assuming it is constrained to a single interval in each of the p constrained dimensions ) is ( 1 » )p . Thus , the probability of finding np points in any subspace Sp , is distributed as per the binomial distribution with mean E(Xp ) = n( 1 » )p . Thus , Sp is interesting if , np
2n ln( 1
¿ ) . n ‚ 1
»p +q 1 log(» ) e = v , we have n
Note that for p ‚ d log(n ) ¿ ) … q 1 »p + q 1
»p • 1 , and thus 1 ¿ ) . The threshold function thus converges to a constant when the number of constrained dimensions p ‚ v ; analogous to minimum threshold s in CLIQUE . To summarize ,
2n ln( 1
2n ln( 1 threshSCHISM ( d ‚ p ‚ v ) = r 1
2n ln(
1 ¿
)
From Lemma 1 , for p ‚ v , Sp+1 is interesting implies that Sp is interesting , as SCHISM is similar to CLIQUE and uses support based pruning for a large part of the subspace
2n ln¡ 1 mining process . Note that this constant q 1 ¿¢ varies inversely as pn and hence threshSCHISM converges to a higher threshold for smaller datasets , implying more pruning while yielding subspaces of equal interestingness . While threshSCHISM is constant for p ‚ v , we can gain some improvements in empirical results by changing the rate of change in threshSCHISM ( p < v ) to increase the likelihood of monotonic search . We do so by trading off some tightness of the bound by using a penalty term . If f ( p ) is the penalty term , such that 8p 2 [ 1 ; d ] ; f ( p ) • 1 , then e¡2nt2 p . Using this in 2 , P r(Xp ‚ np ) • e¡2nf ( p)t2 p • e¡2nf ( p)t2 p • ¿ . After simplification , threshSCHISM ( 0 < p < v ) = min(u ;
1
»p +s 1
2nf ( p ) ln(cid:181 ) 1
¿¶ )
The term u is used to upper bound threshSCHISM ( 1 ) , which is empirically too large for typical values of » . Typical values of f ( p ) are p ( c¡bp2 ) . The last penalty term provides a parabolic as opposed to exponential drop in the threshold as p increases . Typically , u = 0:05 . In summary , we have a ( a ‚ v = ) f ( p ) • 1 ) ;
1 threshSCHISM ( p ) =8< : minnu ; 1 »p +q 1 q 1 2n ln¡ 1 ¿¢ 4 SCHISM Algorithm
2nf ( p ) ln¡ 1
¿¢o if p < v if p ‚ v
( 4 )
A number of the subspace mining algorithms [ 4 , 10 , 19 ] use a bottom up , breadth first search . In contrast , SCHISM , which is based on the GenMax algorithm that mines maximal itemsets [ 13 ] , uses a depth first search with backtracking to mine the maximal interesting subspaces . The main steps in SCHISM are shown in Figure 1 ; we first discretize the dataset and convert it to a vertical format . Then we mine the maximal interesting subspaces . Finally , we assign each point to its cluster , or label it as an outlier .
Discretization In SCHISM , we first discretize all points ( figure 1 , line 1 ) . Given the original dataset DB , we divide each dimension into » bins , and give each interval a unique id ( for example , the intervals in dimension d0 are labeled from 0 to » ¡ 1 , those for d1 are labeled from » to 2» ¡ 1 , etc ) Consider the example dataset DB shown in Table 1
SCHISM ( DB ; s ; » ; ¿ ) : //s is the minimum support threshold //» is the number of intervals per dimension //¿ is the user specified interestingness threshold 1 . DDB = Discretize(DB ; » ) 2 . V DB=HorizontalToVertical(DDB ) 3 . M IS = MineSubspaces(V DB ; s ; » ; ¿ ) 4 . AssignPoints ( DB ; M IS )
Figure 1 . The SCHISM Algorithm
DB p1 p2 p3 p4 p5 p6 p7 p8 p9 d1 755 818 418 833 264 991 921 686 448 d2 689 166 159 173 960 972 963 965 146
DDB d1 7 p1 8 p2 4 p3 8 p4 2 p5 9 p6 9 p7 6 p8 4 p9 d4 482 302 260 236 985 986 976 993 205 d5 d3 838 306 378 494 139 499 948 484 70 465 72 118 71 910 68 623 605 984 ( a ) Original DB d5 48 43 41 49 40 40 40 40 49 ( b ) Discretized DB d3 23 24 24 24 24 21 29 26 26 d4 34 33 32 32 39 39 39 39 32 d2 16 11 11 11 19 19 19 19 11 d6 657 439 921 17 209 209 220 202 423 d6 56 54 59 50 52 52 52 52 54 d7 743 633 986 647 782 804 818 800 654 d8 980 805 780 781 309 341 317 287 983 d7 67 66 69 66 67 68 68 68 66 d8 79 78 77 77 73 73 73 72 79
Table 1 . Example DB : Real & Discretized d1 1 1 1 d2 1 163 949
I1 I2 I3 d1 1 1 1
I1 I2 I3 d3 1 475 1 d4 478 260 985
( a ) Original Subspaces d2 d6 1 56 11 1 19 52 ( b ) Discretized Subspaces d4 34 32 39 d3 1 24 1 d5 1 1 72 d5 1 1 40 d6 673 1 204 d7 774 1 806 d8 1 786 317 d7 67 1 68 d8 1 77 73
Table 2 . Seed Subspaces : Real & Discretized
( a ) , generated by our synthetic data generator ( see section 5.1 ) , with n = 9 ; » = 10 and d = 8 . The seed subspaces used to generate DB are shown in Table 2 ( a ) . Here ¡1 implies that the subspaces are unconstrained in that dimension . Thus , p1 is generated from subspace I1 , points p2 ; p3 ; p4 are generated from I2 , points p5 ; p6 ; p7 ; p8 are generated from I3 and p9 is an outlier . Table 1 ( b ) shows the discretized dataset DDB obtained from DB ; the corresponding discretized subspaces are shown in Table 2 ( b ) . Data Transformation The next step in SCHISM ( figure 1 , line 2 ) is to convert the dataset into a vertical tidset format [ 13 ] , which records for each subspace ( initially a single interval ) , the list of points that belong to it . Using a vertical format dataset gives us a number of advantages . Firstly , better memory utilization results from having only the relevant subspaces in memory at a time , as opposed to the horizontal format in which the entire dataset is scanned . Secondly , computing support of subspaces to be merged via tidset intersections is very fast . Figure 3 ( for p = 1 ) shows the tidsets for the initial ‘interesting’ intervals . For example , for interval 11 , its tidset is given as t(11 ) = f2 ; 3 ; 4 ; 9g .
MineSubspaces ( V DB ; » ; ¿ ) : 1 . Find IS1 and IS2 //sort IS1 as optimization 2 . MIS backtrack(` ; IS1 ; M IS ; 0 ; » ; ¿ ) 3 . return M IS
MIS backtrack(Sp ; Cp ; M IS ; l ; » ; ¿ ) 4 . 8Sx 2 Cp 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 .
Sp+1 = Sp [ Sx Pp+1 = fSy 2 Cp jSy > Sxg If MergeSubspaces(M IS ; ( Sp+1 [ Pp+1 ) ) return Cp+1 = IS candidate(Sp+1 ; Pp+1 ; l ; » ; ¿ ) If Cp+1 is empty
If Sp+1 has enclosed no subspace in M IS , else MIS backtrack(Sp+1 ; Cp+1 ; M IS ; p + 1 ; » ; ¿ )
M IS = M IS [ Sp+1
If minSf 2M IS Sim(Zp+1 ; Sf ) > ‰ £ min(f ; jZp+1j )
MergeSubspaces(M IS ; Zp+1 ) 13 . 14 . M IS = M IS ¡ Sf 15 . M IS = M IS [ ( Sf [ ( Zp+1 ) 16 . 17 . return true return false
IS candidate(Sp+1 ; Pp+1 ; l ; » ; ¿ ) 18 . Cp+1 = ` 19 . 8Sy 2 Pp+1 20 . 21 . 22 . 23 . y ) = t(Sp+1 ) \ t(Sy ) y )j n ‚ thresh(jS0
Cp+1 = Cp+1 [ Sy t(S0 If jt(S 0 return Cp+1 yj )
Figure 2 . Mining Interesting Subspaces
Mining Interesting Subspaces In SCHISM , interesting subspaces are mined ( figure 1 , line 3 ) , using a depth first search with backtracking , allowing us to prune a considerable portion of the search space . The pseudo code for MineSubspaces is shown in Figure 2 . The method first finds all interesting subspaces in one ( IS1 ) and two dimensions ( IS2 ) . We next call the recursive MIS backtrack procedure to mine the set of maximal interesting subspaces ( M IS ) .
MIS backtrack accepts as input , a single p subspace Sp , and a set Cp of candidate p subspaces that can be used to constrain ( or extend ) Sp in an interval of another dimension . Each Sx 2 Cp results in a potential new p + 1subspace Sp+1 = Sp [ Sx ( line 5 ) , for which we have to calculate the new candidate set Cp+1 ( line 8 ) . We do this by using a possible set Pp+1 ( line 6 ) of potential candidate subspaces , which are all the unprocessed subspaces in Sy > Sx in Cp . If Cp+1 is empty ( line 9 ) , then Sp+1 is potentially maximal ; it will be added to M IS if there is no maximal subspace that encompasses it ( lines 10 11 ) . If Cp+1 is nonempty , then we recursively call MIS backtrack .
The call to IS Candidate ( line 8 ) constructs the new candidate set Cp+1 for Sp+1 for the next level . The basic idea is to intersect tidset of Sp+1 with every possible subspace in Pp+1 ( line 20 ) . We keep only those subspace extensions that pass the thresh( ) function ( lines 21 22 ) .
Typically , a depth first search with backtracking produces a number of subspaces , which may overlap considerably , leading to redundant subspaces . To avoid this behavior , we prune the search tree by using MergeSubspaces
( line 7 ) . If the subspace Zp+1 = Sp+1 [ Pp+1 , resulting from constraining Sp+1 with all its remaining possible intervals in Pp+1 , is significantly similar ( typically , we set the merging threshold ‰ = 0:8 ) to some known Sf 2 M IS ( line 13 ) , we replace Sf with a typically more constrained subspace ( lines 14 15 ) , Sf[Zp+1 . As ‰ < 1 , we may merge subspaces , which are constrained to adjacent intervals in a few dimensions , thus compensating for uniform width intervals in each dimension . The Sim function ( line 13 ) used to calculate the similarity of two subspaces A and B is given as Sim(A ; B ) = Pd i=1 JaccardSimilarity(Ai ; Bi ) , where Ai ; Bi are the sets of interesting intervals spanned by A ; B in the i th dimension and JaccardSimilarity(X ; Y ) = jX\Y j jX[Y j . For example for point p1 2 DDB and seed subspace I2 ( discretized ) ( see Tables 1 and 2 ) , Sim(p1 ; I2 ) = 2 , as they are identically constrained in the second ( 11 ) and third ( 24 ) dimensions .
19,39,40,52,68,73
6 7
19,39,40,52,68
6 7 8
19,39,40,52
5 6 7 8
19,39,40
5 6 7 8
39 5 6 7 8
40 5 6 7 8
52 5 6 7 8
32 3 4 9
68 6 7 8
73 5 6 7
11,24,32
3 4
11,24
19,39
2 3 4
19 5 6 7 8
11 2 3 4 9
5 6 7 8
24 2 3 4 5 p=6 p=5 p=4 p=3 p=2 p=1
Figure 3 . Lattice of running example
Example Let ’s consider how SCHISM works on our example dataset DB . Let u = 0:25 ; ¿ = 4=n = 0:44 . Then , IS1 = f11 ; 19 ; 24 ; 32 ; 39 ; 40 ; 52 ; 68 ; 73g . Likewise we compute IS2 . The initial call to MIS backtrack is with Sp = ; , and Cp = IS1 , which results in a recursive call of MIS backtrack for each interval , with a new candidate set Cp+1 . For example , the candidate set for 11 is given by C1 = f24 ; 32 ; 39 ; 40 ; 52 ; 68 ; 73g , thus in the next level , we will try to extend 11 with 24 ; 32;¢¢¢ ; 73 , and so on , recursively . For our running example , when Sp+1 = 11 , Sy = 24 , then S0 y = f11 ; 24g . Also t(Sp+1 ) = f2 ; 3 ; 4 ; 9g , and t(Sy ) = f2 ; 3 ; 4 ; 5g , which gives t(S0 y ) = f2 ; 3 ; 4g ( see Figure 3 ) . As p = 1 , we use the interestingness measure for pruning the search ( line 21 ) , ie , the second case in Equation 4 . With threshSCHISM ( 1 ) = 0:21 , n1=n = 3=9 = 0:33 > 0:22 ; thus S0 y is interesting , and we add 24 to the candidate set . Proceeding in this manner , we get the lattice shown in Fig 3 . The rectangles shaded in gray are the elements of M IS .
Assigning Points to Clusters Let A correspond to the dsubspace surrounding a point pi , and let B correspond to a mined p subspace . Let Y and y be the random variable denoting the number and the true number , respectively , of
AssignPoints ( DB ; M IS ) : 1 . 8i 2 [ 1 ; n ] // for each point in DB 2 . If minj Sim(xi ; M ISj ) > T hresholdSim 3 . xi ! M ISargminj Sim(xi;M ISj ) else xi is an outlier 4 .
Figure 4 . Assign Points dimensions in which subspaces A and B are identically constrained . Under the default uniform distribution , each dimension is independent of the other and Y is a binomial random variable with mean E[Y ] = b=» , where b = E[jBj ] is the expected number of dimensions in which B is constrained . Using Chernoff Hoeffding bounds again , under a uniform distribution , if P r[Y ‚ y ] • exp(¡2bt2 ) • ¿ for reasonably small user specified threshold ¿ , it implies that the similarity between A and B is unusually high and the point in A is with high probability generated from the subspace B . Now , exp(¡2bt2 ) • ¿ implies that t ‚ q 1 2b ln¡ 1 » + bt , we ¿¢ . We set b = d=2 .
¿¢ . Substituting t in y = E[Y ] + bt = b
Fig 1 , line 4 of SCHISM assigns each point to the most similar maximal interesting subspace ( lines 2 3 ) , or else labels it as an outlier ( line 4 ) . Figure 4 shows these steps . Additionally , we have to examine if the similarity is statistically significant by using T hresholdSim computed above . 5 Experiments get T hresholdSim = b
» +q b
2 ln¡ 1
We perform tests on a range of synthetic highdimensional datasets using our data generator and a couple of real datasets . We evaluate SCHISM based on two metrics : i ) speed : the time taken to find the interesting subspaces in the dataset , and ii ) accuracy : measured in terms of entropy and coverage . For a clustering C , entropy is defined n Pi pij log(pij) ) , where pij = nij n , Cj is the jth cluster in C , nj is the number of points in Cj , and nij is the number of points of Cj that actually belong to subspace i . The lower the E(C ) , the better the clustering . Coverage is the fraction of points in DB which are accurately labeled as not being outliers . Ideally this is 1 . as E(C ) = ¡PCj
( nj
5.1 Synthetic Data Sets
We generate synthetic datasets using our own data generator which employs techniques similar to those mentioned in [ 2 , 3 , 20 ] . We embed k multivariate Gaussian clusters in a dataset of n points and d dimensions . Each dimension of a seed center is constrained to a single interval , with probability c ; integer values are chosen in [ 0 ; 1000 ] . By not setting all dimensions of a seed center , we produce subspaces . If a center is set in any dimension , then the next center to be generated has the same dimension constrained with probability o . This ensures that the subspaces have different volumes and they can overlap in some dimensions . If the points are normally distributed about their cluster centers , the standard deviation for each dimension , for each cluster , is distributed uniformly in the range [ 10,30 ] . Let x be the fraction of the points generated as outliers and let the fraction of points generated for the i th subspace be In order that the number of points in the subspaces differ , we use a parameter fii , such that x + Pk i=1 fii = 1 .
• = maxi fii ie , the ratio of the fiis of the subspace with mini fii the most points to the subspace with the least points . An outlier has each dimension chosen uniformly in [ 0,1000 ] . The points for each subspace are independently and normally distributed about its center and the coordinates for dimensions in which the center is unbounded are chosen uniformly in [ 0,1000 ] . Thus , the subspaces generated in the dataset are oriented parallel to the axes . For all experiments , unless otherwise mentioned , we set as parameters to our data generator , k = 5 ; n = 1000 ; d = 50 ; c = 0:5 ; o = 0:5 ; • = 4:0 ; x = 0:05 . Also , we set support threshold u = 0:05 ; ¿ = 1=n ; » = 10 as the parameters to SCHISM . Also , we use f ( p ) = p d .
Experiments were carried out on a Sun Sparc 650 MHz machine running on a Solaris O/S with 256 MB RAM . Since we have the seed subspaces we can easily evaluate the accuracy of SCHISM . We first mine the subspaces and partition the points in the space , so that they either belong to some interesting subspace or they are classified as outliers . Each of the following graphs , unless otherwise mentioned , shows the variation in performance , as a measure of two evaluation metrics : execution time and coverage ( shown on y axis ) , as a parameter of either SCHISM or the synthetic dataset is varied ( shown on x axis ) . The entropy for all these experiments is below 0.004 and hence not shown . This implies that SCHISM mines very pure clusters from our synthetic datasets . Ideally , the running time curve should be flat or linear and the coverage curve should be flat at 10
511 Effect of varying dataset parameters Effect of dataset size and dimensionality In Fig 5 , it is evident that as the dataset size increases , the coverage remains constant , while the running time grows linearly .
Note that in Fig 6 , as the dimensionality of the dataset increases , the coverage remains more or less constant , but the running time seems to grow exponentially initially , and then grows linearly from dimensions 200 300 . In the worst case , this algorithm has exponential complexity in the number of dimensions , but in practice as shown here , the DFS algorithm coupled with the varying threshold function , significantly prunes the search space .
Effect of subspace size and dimensionality In Fig 7 , we observe the variation in coverage and running time , as the ratio • = maxi fii increases from 2 to 12 . We observe mini fii that as the ratio increases , the coverage dips marginally and the running time remains constant . The coverage decreases because the subspaces which contain a smaller number of points have , on average , as large a volume as those containing a larger number of points , leading to a lower density . A smaller fraction of their enclosing subspaces are likely to be identified as ‘interesting’ and hence only a small fraction of their points are detected as non outliers , as compared to when the ratio is not so large .
In Fig 8 , we observe the variation in coverage and running time , as the probability of constraining a dimension in a subspace c , increases from 0:3 to 0:9 . We observe that as c increases , the running time remains constant but larger fractions of the dataset are constrained to smaller volumes , making them more ‘interesting’ and hence coverage improves somewhat .
Effect of subspace overlap Subspaces may overlap in terms of the dimensions in which they are constrained or in terms of the specific intervals they are constrained to . Accordingly , we have two experiments . In the first , we test the effect on performance due to increased overlap between constrained dimensions of subspaces generated consecutively . By increasing o from 0.1 to 0.9 , we increase the likelihood of different subspaces being constrained to the same intervals of the same dimensions . In Fig 9 , we observe that running time stays constant but coverage decreases as o increases . This occurs because it becomes more likely that points belong to multiple ‘interesting’ subspaces simultaneously and hence only one is discovered , which may not completely cover the other .
In the second experiment , if Ci;j is the jth co ordinate of the center of the ith subspace , then if we constrain the jth co ordinate of the ( i + 1)th center as well , we set it so Ci+1;j 2 fCi;j ¡ 2(cid:190)i;j ; Ci;j + 2(cid:190)i;jg , where ( cid:190)i;j is the standard deviation in the jth dimension of the points corresponding to the ith subspace . From Fig 10 , we observe that SCHISM does not perform too well in this test and fails to find some of the clusters as the dimensionality rises , again because a point may now belong to multiple subspaces .
Performance on datasets with less dense subspaces In this experiment we run SCHISM on Gaussian and hyperrectangular datasets . We decrease the density of the Gaussian datasets by increasing the standard deviation of each constrained dimension in each subspace . For hyperrectangular subspaces , each constrained dimension is constrained to an interval of width , chosen uniformly in [ 05w,15w ] Thus , the density is decreased by increasing w ; the volume of the subspace and keeping the number of points assigned to it is the same .
From Fig 11 and Fig 12 , it is clear that as density decreases , SCHISM ’s performance deteriorates . This is because a smaller percentage of the subspace ’s points tend to fall into the same interval as that of the subspace center , as the volume increases . In such a case , decreasing the number of intervals in the dimension ( » ) might help or we must search for less ‘interesting’ subspaces , ie , decrease ¿ .
Effect of number of clusters k Note from Fig 13 , that the running time remains constant as the number of embedded subspaces ( k ) increases , while coverage worsens after k = 8 as some clusters become very small and hence not ‘interesting’ .
512 Effects of varying algorithm parameters Effect of ¿ In Fig 14 , we decrease the user specified interestingness threshold ¿ from 10¡12 to 10¡:25 , and observe its effect on the coverage and running time . Note that the coverage increases rapidly , implying that ¿ is the main parameter which determines how much of the search space is mined and hence running time drops rapidly too . Our experiments on the effect of u on SCHISM performance ( which are not shown due to lack of space ) , indicate that ¿ has a more precise control on pruning than u .
Effect of » From Fig 15 , we observe that , varying the number of intervals into which each dimension is discretized ( » ) , has a small effect on SCHISM ’s performance for a considerable range of values for » . This is because the term » is incorporated into threshSCHISM . Outside this range however ( » > 15 ) , performance is severely degraded as the interval size becomes so small that very few contain enough points to be considered ‘interesting’ .
Effect of thresh( ) function on performance Here we compare the performance of the thresh( ) function given e r o c S e r o c S e r o c S e r o c S
Coverage Speed(minutes )
2
1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0
50
100
150
200
Number of dimensions(d )
250
300
Figure 6 . Data Dimensionality
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0.1
Coverage Speed(minutes )
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
P(same dimension in adjacent subspaces are constrained)=o
Figure 9 . % Overlapping Constrained Dimensions
Coverage Running time(minutes )
Coverage Speed(minutes )
50000
100000
150000
200000
250000
300000
Number of points(n )
Figure 5 . Dataset Size
Coverage Speed(minutes )
7
6
5
4
3
2
1
0
0
1
0.8
0.6
0.4
0.2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
P(dimension in subspace is constrained)=c
Figure 8 . Subspace Dimensionality
1
0.8
0.6
0.4
0.2
0
10
Coverage Speed(minutes )
15
20
25
30
35
40
Standard deviation of constrained dimension e r o c S e r o c S e r o c S
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
20
Coverage Speed(minutes )
4
6
8
10
12
Kappa
Figure 7 . Subspace Size
Coverage Speed(minutes )
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
2
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
P(adjacent subspaces with nearby means have overlapping constrained dimension )
Figure 10 . 2(cid:190 ) Constraining
Coverage Speed(minutes )
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1 e r o c S e r o c S e r o c S
4
6
8
10
12
14
16
Number of clusters(k )
Figure 13 . Num Clusters ( k )
Figure 11 . Gaussian Subspace Density
Figure 12 . Hyper rectangle Subspace Density
40
60
80
100
120
140
Average width of interval of constrained dimension
0
2
Coverage Speed(minutes )
1
0.8
0.6
0.4
0.2
0
12
10
8
6 log10(Tau )
4
2
0
Figure 14 . Interestingness Threshold
Class ‘0’ ‘1’ ‘2’ ‘3’ ‘4’ ‘5’ ‘6’ ‘7’ ‘8’ ‘9’
C1 0 0 0 0 0 322 1 1 22 0
C2 0 272 2 7 0 0 0 74 0 11
C3 0 0 0 0 38 0 11 0 0 3
C4 15 0 0 0 0 0 109 0 0 0
C5 0 0 2 0 0 0 0 24 36 0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1 e r o c S
0
5
C6 0 0 1 0 1 0 78 13 14 0
Coverage Speed(minutes ) e r o c S
0.075
0.07
0.065
0.06
0.055
0.05
0.045
0.04
0.035
0.03
0.025
70
CLIQUE Running time(Minutes ) SCHISM Running time(minutes )
80
90
100
110
120
130
140
150
Average width of interval of constrained dimension
10
15
20
25
30
Number of intervals per division(Xi )
Figure 15 . Effect of »
Figure 16 . thresh : CLIQUE vs . SCHISM
C7 0 0 0 53 0 6 4 0 1 24
C8 0 4 0 43 3 11 1 0 0 15
C9 1 9 11 405 6 78 9 1 2 160
C10 0 6 142 0 0 0 3 0 0 0
C11 0 39 284 0 0 0 0 0 0 1
C12 0 24 56 0 0 0 0 1 5 0
C13 0 0 0 0 80 0 9 0 0 3
C14 0 0 0 0 82 0 31 0 0 2
C15 0 3 7 0 0 0 0 16 24 0
C16 0 1 0 52 0 2 0 0 0 33
C17 0 0 0 0 0 0 0 94 8 0
Figure 17 . Confusion Matrix for PenDigits Data Set in Eq 4 , with that of CLIQUE on the synthetic datasets . From Fig 16 , we observe that as the density of the hyperrectangular clusters dips due to increase in the width of its constrained dimensions , the running time of CLIQUE 4 increases rapidly over that of SCHISM . Also , CLIQUE tends to split clusters into smaller ones . Its performance closely mirrors that of SCHISM for datasets having well defined distinct clusters . However , when clusters overlap in a number of dimensions , the coverage and entropy suffers . 5.2 Real Data Sets
We apply SCHISM to two well researched datasets from different domains . The PenDigits dataset 5 [ 6 ] contains 7,494 16 dimensional vectors . 30 different writers wrote approximately 250 digits , sampled randomly from the digit set [ 0,9 ] on a pressure sensitive tablet . Each vector corresponds to the ( x ; y ) coordinates of 8 points , spatially sampled from each of these handwritten digits . Note that the embedded subspaces , ie , the digits 0 9 , overlap considerably in the 16 dimensional space . SCHISM outputs 128 subspaces in 4 seconds , of which the 17 clusters with the highest entropies are shown in the confusion matrix in Figure 17 . It achieves a coverage of 69 % and an entropy of 0.365 ( u = :01 ; ¿ = 0:018 ) . CLIQUE achieves a coverage of 60.7 % and an entropy of 0.49 in approximately 4 seconds too . As in DOC , ORCLUS , we provide a confusion matrix ( Figure 17 ) , which is interpreted as follows : cell ( i ; j ) of the matrix denotes the number of points having true class i , which were clustered by SCHISM into subspace j . Ideally , each row and each column have only a single non zero entry implying E(C)=0 . Note that samples of the digits f3 ; 9g are both assigned by SCHISM to clusters C7 ; C8 ; C9 due to their similarity in structure . The clusters not shown typically have all their samples from the same digit class .
The other dataset is the gene expression data for the yeast cell cycle 6 , obtained for 2884 genes ( rows ) at 17 ( columns ) points in time . We obtained a number of clusters of which a few were highly populated and the others relatively empty . Ideally , clustering gene expression data should produce clusters of genes which are similar in function . However , almost all the genes have multiple functions and hence genes cannot be labeled by a single class . The highly populated clusters we mined using SCHISM , contained groups of genes which are known to have strong similarity in terms of function , eg , out of 5 genes in our dataset known ( see wwwyeastgenomeorg ) to be involved in ribonuclease MRP activity , 4 ( POP4,POP5,POP8,SNM1 ) are assigned to the same cluster , 4 genes ( SEC7,AGE1,SFT1,COG6 ) out of 6 involved in intra Golgi transport , are assigned to the same cluster , etc . ( » = 5 ; ¿ = 0:018 ) . While SCHISM finds 59 such groups of genes which are clustered together in larger clusters , CLIQUE finds only 33 , both doing so in approximately 8.5 seconds .
While , we attempted to compare our algorithm performance with that of SUBCLU [ 17 ] , we found default parameter setting for SUBCLU to be unsatisfactory and manual setting to be extremely hard , as it took an unreasonably long time ( on the order of a number of hours ) to produce output for our synthetic and real datasets . The clusters produced generally split the embedded clusters into distinct clusters . simply replacing implementation to implementation of CLIQUE involves threshSCHISM with threshCLIQU E in our test the significance of our threshold function
4Our
5See ftp://ftpicsuciedu/pub/machine learning databases/pendigits 6See http://arepmedharvardedu/biclustering/yeastmatrix
6 Conclusions
We define a new interestingness measure which provides absolute guarantees to the user about the interestingness of the subspaces reported , as per our definition of interesting . We use the interestingness measure itself to prune our search , as opposed to traditional methods[21 ] , which determine interestingness of patterns after the search is completed , making the process faster . We use an algorithm which requires parameters which are relatively easy to set intuitively . These contributions can also be applied to the problem of finding interesting itemsets . The code for SCHISM is available at wwwcsrpiedu/ sequek/schism
References
[ 1 ] C . Aggarwal . Towards systematic design of distance functions for data mining applications . In SIGKDD Conf , 2003 . [ 2 ] C . Aggarwal , C . Procopiuc , J . Wolf , P . Yu , and J . Park . A framework for finding projected clusters in high dimensional spaces . In SIGMOD Conf , 1999 .
[ 3 ] C . Aggarwal and P . Yu . Finding generalized projected clus ters in high dimensional spaces . In SIGMOD Conf , 2000 .
[ 4 ] R . Agrawal , J . Gehrke , D . Gunopulos , and P . Raghavan . Automatic subspace clustering of high dimensional data for data mining applications . In SIGMOD Conf , 1998 .
[ 5 ] R . Agrawal , T . Imielinski , and A . Swami . Mining association rules between sets ofitems in large databases . In SIGMOD Conf , 1993 .
[ 6 ] F . Alimoglu and E . Alpaydin . Methods of combining multiple classifiers based on different representations for penbased handwriting recognition . In Turkish AI and Neural Networks Symp , 1996 .
[ 7 ] K . Beyer , J . Goldstein , R . Ramakrishnan , U . Shaft . When is nearest neighbors meaningful ? ICDT Conf , 1999 .
[ 8 ] M . Brusco and J . Cradit . A variable selection heuristic for k means clustering . Psychometrika , 66:249–270 , 2001 .
[ 9 ] K . Chakrabarti and S . Mehrotra . Local dimensionality reduction : A new approach to indexing high dimensional spaces . In VLDB Conf , 2000 .
[ 10 ] C . Cheng , A . Fu , Y . Zhang . Entropy based subspace clus tering for mining numerical data . In SIGKDD Conf , 1999 .
[ 11 ] H . Chernoff . A measure of asymptotic efficienc y for tests of a hypothesis based on the sum of observations . Annals of Math . Statistics , 23:493–509 , 1952 .
[ 12 ] C . Ding , X . He , H . Zha , and H . Simon . Adaptive dimension In ICDM reduction for clustering high dimensional data . Conf , 2002 .
[ 13 ] K . Gouda and M . Zaki . Efficiently mining maximal frequent itemsets . In ICDM Conf , 2001 .
[ 14 ] J . Han and M . Kamber . Data Mining : Concepts and Tech niques . Morgan Kaufmann , San Francisco , CA , 2001 .
[ 15 ] A . Hinneburg and D . Keim .
Optimal grid clustering : Towards breaking the curse of dimensionality in highdimensional clustering . In VLDB Conf , 1999 .
[ 16 ] W . Hoeffding . Probability inequalities for sums of bounded J . American Statistical Association , random variables . 58:13–30 , 1963 .
[ 17 ] K . Kailing , H . Kriegel , and P . Kroger . Density connected In SIAM subspace clustering for high dimensional data . Data Mining Conf , 2004 .
[ 18 ] K . Kailing , H . Kriegel , P . Kroger , and S . Wanka . Ranking interesting subspaces for clustering high dimensional data . In European PKDD Conf , 2003 .
[ 19 ] H . Nagesh , S . Goil , and A . Choudhary . Adaptive grids for In SIAM Data Mining Conf , clustering massive data sets . 2001 .
[ 20 ] C . Procopiuc , M . Jones , P . Agarwal , and T . Murali . A montecarlo algorithm for fast projective clustering . In SIGMOD Conf , 2002 .
[ 21 ] P . Tan , V . Kumar , and J . Srivastava . Selecting the right interestingness measure for association patterns . In SIGKDD Conf , 2002 .
