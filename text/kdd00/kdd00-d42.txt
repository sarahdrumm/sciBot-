Identifying Prospective Customers
Paul B . Chou , Edna Grossman , Dimitrios Gunopulos* , Pasumarti Kamesam
IBM TJ Watson Research Center
*Department of Computer Science and Engineering
Route 134
Yorktown Heights , NY 10598
{pchou,egross,pvk}@usibmcom
University of California , Riverside
Riverside , CA 92521 dg@csucredu
ABSTRACT We describe data mining techniques designed to address the problem of selecting prospective customers from a large pool of candidates . These techniques cover a number of different scenarios , namely whether the marketing researchers have demographic information on the current customers , or the general market population , or people with propensity to become customers We also present a novel approach to the problem by exploiting the availability of a data sample from the general market population . Finally , we describe an on line lead management and delivery system that uses the mining approach described in this paper for insurance agents to obtain qualified customer leads .
Category and subject : ACM H28 Data Mining General Terms : customer prospecting
1 . INTRODUCTION An important aspect of marketing research is understanding demographic characteristics of those who are buying a product of interest . Developing such product market insight allows for the design of new products , the identification of cross selling opportunities , and more importantly , the identification of prospective new customers .
In this paper , we focus on the use of data mining techniques to address the problem of prospective customer selection .
1.1 Background The advent of information technology allows merchants to keep detailed records of their customers’ business transactions as well as relevant demographic data . For example , insurance companies keep records of each of their policy holders , including a customer ’s business transactions as well as demographic characteristics that are relevant to their insurance policies . To better understand their customers , this data may be enriched with additional demographic information , perhaps at a cost , from companies that provide various demographic data elements collected from disparate sources . These data providers also maintain large databases of demographic data of the general population , typically organized by household . ( For example , the Acxiom Corp ’s InfoBase contains demographic data for most US households . ) While the accuracy of the provided data is sometimes questionable , the data are certainly useful and beneficial for conducting marketing research and campaigns .
Many marketing campaigns are designed to reach new customers . In such cases , a contact resulting in any sale , thus creating a new customer , is considered a success . In some cases , the new customers’ business characteristics , such as contributions to revenue or profit , are also taken into account for measuring success . Obviously , the success of a marketing campaign is also governed by the cost of the campaign , including the cost of data and the cost of contacting the prospects . The objective of data mining analysis in this context can be generally defined as identifying prospective customers from a large pool of candidates ( eg all the households in the target market ) based on the available data in order to maximize the campaign objective while meeting the constraints on its cost .
1.2 Analysis Scenarios It is desirable to measure a subject ’s “ propensity to buy ” a particular product based on his demographic characteristics for two reasons . First , we can take advantage of the vast amount of demographic data available from the data providers of our prospect candidate pool , and , secondly , we will be able to target only those who have the highest propensity to buy thus increasing our chance of success . Developing such a measure using statistical methods requires training data which includes data for those who bought the product as well as for those who did not when given an opportunity to do so , or , in different words , data for those who responded to the product campaign and for those who did not . Such buyer vs . non buyer data are either nonexistent or expensive to collect , typically requiring well designed experimental marketing campaigns . On the other hand , as mentioned above , it is common for a merchant to have extensive collections of data of its customer base . Such customer data , does not directly indicate who would buy or not , but does provide hints as to who the customers are . Using such data for prospect selection makes practical sense but poses a significant challenge due to the lack of the negative ( eg nonbuyer ) data . When buyer vs . non buyer data are not available , we have devised a method that exploits the customer data in conjunction with sample demographic data from the overall market population . is
This paper is organized according to the strategies and techniques applicable to each of the following analysis scenarios : 1 . Only customer data , enriched with certain demographic information , are available . This the most frequently encountered scenario . 2 . A representative market sample is also available in addition to the customer data , consisting of household demographic data records from the market population . 3 . Buyer vs non buyer data are available . This is the ideal scenario for this problem . Much prior work in the area of creating response models can be applied to this scenario . We will only provide references and a summary of the existing techniques .
1.3 Data Mining Process for Customer Prospecting Figure 1 depicts the high level steps that may be involved in the overall prospecting process . Some of steps are optional , depending on the analysis scenarios summarized above . The steps are : Customer data extraction . This step typically involves the identification and extraction of customer records , for example , records related to insurance policies , mutual fund ownership , etc . , from a variety of business systems and databases . It is also important to identify attributes relevant for marketing purposes . Householding and enrichment of customer records . This step first creates household level records by aggregating transactionoriented data to the household level . Typically , new householdlevel attributes are created , summarizing various customer characteristics of the household with respect to the merchant ’s business . For example , the types and quantities of products purchased by the household , how long the household has been a customer , etc . , can be extracted from the transaction oriented data . Household records can then be enriched with additional demographic attributes , such as estimated income , number of children , etc . Prospect model development . This step refers to the application of data mining techniques to develop mathematical models that relate explanatory ( eg demographic ) attributes to the attribute of interest ( eg propensity to buy ) . The modeling techniques and their applicability are the main focus of this paper , and they will be described in more detail later . The resulting models serve the following purposes : a ) provide insight to the customer characteristics , and b ) provide quantitative index to differentiate prospects based on their desirability with respect to the marketing campaign . For example , a prospect model may consist of a set of market segments , and their descriptions , ranked by the average number of purchases per household in the segment . The higher the average number per household , the more desirable is the segment . Prospect extraction and scoring . This step refers to applying the result from the previous step to select and extract household records from the large household demographic database . One approach is to identify prospects according to the descriptions of selected segments , and to score them individually with respect to their desirability . One advantage of this approach is that , if there is a cost associated with the use of household data at this stage , scoring only the households in selected market segments ( based on the results of Step 3 ) would reduce much of the unnecessary cost . Alternatively , if cost is not a concern , all records in the database can be scored first , and only the higher scoring ones retained as campaign targets . Scoring individual households provides flexibility for the marketing campaign , as highly scored households can be pursued first in order to reach maximal campaign efficiency . The scores also provide a validation mechanism for the data mining analysis . For example , the top 10 % of households contacted should have a better response rate than the lowest 10 % of households , if they are ranked by the model ’s scores . Marketing campaign . One important part of this step , besides the actual contact of the target customers , is to collect response data of the contacted customers . The response data feedback can help develop more up to date , accurate models for use by future campaigns .
Enriched Customer Records
Market Segment
Descriptions
Scored Prospects
Customer Records
Household Demographic
Data
Exclusion
List
Buyer vs . Non
Buyer Records
1
Customer Data
Extraction
2 Householding
&
Enrichment
3
Prospect Model
Development
4
Prospect
Extraction and
Scoring
5
Marketing Campaign
Figure1.Overall Process
1.4 Practical Considerations Before going into the discussion of data mining techniques for the prospecting problem , it may be important to bring out some issues that should be considered first . Certainly , the foremost consideration is about data : the types of data that are available . Obviously , the choice of data mining technique for the problem should match the types of available data . In addition , the following aspects of the problem should also be taken into account . Total cost . Whether or not there is an incurred cost for every piece of data used for prospect model development , prospect scoring , or a marketing campaign , may be a deciding factor in designing a solution . For example , a solution that requires the scoring of all available prospect candidates may have a significantly higher overall cost than a solution that scores only the prospects in selected market segments . Interpretability of results . If customer segments could be accurately described in a natural language such as English , and if scores can be interpreted with , say , probability semantics , it would be easier for an end user ( eg a marketing expert ) to accept and validate the computer created decisions . Moreover , such customer segment descriptions can be used to query a database of potential customer records . For this reason , techniques that produces customer segments ( clusters ) that can be described as DNF ( Disjunctive Normal Form ) expressions are particularly attractive . It is important however to have descriptions as minimal as possible because the complexity of the shape of the clusters increases with the dimensionality of the space ( see also the discussion on high dimensionality below ) . High dimensionality . Household information from companies such as Axciom typically includes dozens and up to several hundred of attributes , and the ranges for some of the attributes are large . It is important for data mining techniques to have the ability to identify important attributes and to disregard the superfluous ones for the following reasons . First , related to result interpretability , a more concise description of the result including only a handful of necessary attributes is more desirable than one with dozens or hundreds of attributes . Secondly , models developed with properly selected attributes tend to be more accurate since superfluous attributes often introduce additional noise and dilute the information in the data . Thirdly , regarding finding market segment or customer clusters , it is not meaningful to look for clusters in such a highdimensional data space as the average density of points anywhere in the data space is likely to be quite low . Compounding this problem , many of these attributes or combinations of attributes can have values that are uniformly distributed , or noisy . Therefore distance functions that use all the dimensions of the data , such as the Euclidean distance , may not be effective . Moreover , clusters may exist in subspaces comprised of different combinations of attributes . Missing values . Many of the demographic data attributes are populated only for a small percentage of the households , and most of the attributes contain some percentage of missing values . It is important to deal with this problem effectively . Many existing techniques simply ignore any records containing a missing value in any of the attributes while others implement certain missing ( eg replacement value schemes constant/mode/mean replacement ) . Such approaches have shortcomings . Some newer data mining algorithms , such as the Demographic Clustering algorithm in Intelligent Miner , ProbE developed at IBM TJ Watson Research Center , and CLIQUE ( see Section 2.2 ) , treat missing values as a part of the algorithm and not as an exception . Such algorithms can exploit the available data even if the missing value problem is severe , but do not eliminate the need for judicious data understanding , cleansing , and transformation . Computation resource requirements . Even with the cost of computing coming down regularly over time , it is still important to understand the data mining resource requirements with respect to the problem . For example , regarding timeliness , if the data mining process needs to be repeated very frequently , eg weekly , then a model which takes 48 hours to run is obviously not very useful . Another important consideration is scalability .
2 . CUSTOMER PROFILING When only information about existing customers is available , a sensible approach to the prospecting problem is to profile the existing customers by segmenting or grouping them based on their traits . Once the customer segments are determined , each segment can then be further analyzed and profiled by their common characteristics and their impacts to the business . “ Good ” segments are then selected , and their profiles are used for identifying prospects . For example , one may consider a segment with a larger number of customers to be a better segment than one with fewer customers . Alternatively , a segment with a higher average revenue per customer is better than a segment with a lower one . While the above examples of “ goodness ” criteria are based on intuitive business reasoning , we caution the reader that there really is no sound justification as to why targeting those “ better ” segments would lead to more efficient campaigns .
Prospecting using the customer profiling approach works in two steps : Cluster the customer population into segments and derive customer profiles for each segment . Score the segments according to a scoring criterion and use the profiles of the highest scoring segments to search for new customers .
In this section we first provide a brief overview of clustering techniques in general . Given the nature of the clustering problem and the customer data we used , it is impossible to draw any objective comparison among the techniques with which we experimented during our study . We offer some of our observations along with some sample results in Section 5.1 , in which clusters resulting from CLIQUE are evaluated using a measurement market penetration rate , defined in Section 32
2.1 Clustering Techniques Current clustering techniques can be broadly classified into two categories : partitional and hierarchical . Given a set of customer records , partitional clustering obtains a partition of the records into K clusters such that the customers in a cluster are more similar to each other than to customers in different clusters . The value of K may or may not be specified and a clustering criterion such as mean square error must be adopted . The popular K means method ( 4,9,12 ) iteratively determines K representatives that minimize the clustering criterion and groups each record with the representative closest to it , creating K clusters . Mode seeking clustering methods identify clusters by searching for regions in the data space in which the density of customer records is large . Each dense region , called mode , is associated with a cluster center , and each customer is assigned to the cluster with the closest center ( 1,4 ) .
The two clustering algorithms in IBM Intelligent Miner use different partitioning schemes . The Kohonen Neural Net algorithm is a partitional technique similar to the K means technique described above . In addition to finding a partitioning of the data that minimizes the K means criterion , the neural net algorithm also presents a two dimensional map of the clusters with the property that clusters that have points that are close to each other appear in neighboring positions in the cluster map .
The Demographic Clustering algorithm in Intelligent Miner uses a different distance measure that is designed to work well with categorical attributes . The algorithm is incremental : the data records are considered one by one . Each new record is either inserted to an existing cluster , or a new cluster is created that initially includes the new point only . To decide which cluster the record belongs to , a global criterion is minimized . To minimize the risk of inserting a point to the wrong cluster , multiple passes are made over the data . We have observed in our studies the Demographic Clustering algorithm performs well on our datasets involving large numbers of attributes and missing values . that
Hierarchical clustering algorithms produce a nested sequence of partitions . A hierarchical clustering starts by placing each record in its own cluster and then merges these atomic clusters into larger and larger clusters until all records are in a single cluster . Divisive , hierarchical clustering reverses the process by starting with all records in one cluster and subdividing into smaller pieces .
These algorithms assume that there exists an appropriate distance function that evaluates how close two customer records are to each other . If all the attributes are numeric , then each customer record can be thought of as a point in a multidimensional numerical space , and the distance function is typically the Euclidean distance between the two points . This approach does not work however when some or all the attributes are categorical . following transformation is used to map the original space to a numerical space : a new dimension is added for each different value of each attribute . A record in the new space will have value 1 in the dimension that corresponds to the value of the categorical attribute in the original space , and 0 everywhere else . Two points in the new space have distance 0 if and only if the points in the original space had the same categorical value . this case , the
In as such previously
The traditional clustering techniques do not address the special considerations mentioned high dimensionality , missing values , and result interpretability adequately . For example , K means algorithms typically minimize the within cluster variation and dist(x,y ) is either the Euclidean distance between the points x and y or the L_1 or L_¥ distance measures . This assumes that distance metrics that use all the dimensions are reliable . However , when the data include sets of dimensions with uniform distributions or noise , this assumption is not justified . is for cluster analysis . However ,
To address the problem of having too many demographic attributes , users are required to specify a subset of the attributes such useridentification of subspaces is error prone . Another way to address high dimensionality to apply dimensionality reduction techniques to the dataset . Dimensionality reduction methods such as principal component analysis and factor analysis optimally transform the original data space into a lower dimensional space by forming dimensions that are linear combinations of given attributes . The new space has the property that distances between points remain approximately the same as before . While these techniques succeed in reducing the dimensionality , they have two caveats . First , the new dimensions can be difficult to interpret and the clusters are difficult to describe in the original space . Second , these techniques do not address the problem of choosing the right attributes for clustering
Most of the clustering techniques do not produce explicit descriptions of the clusters . To create profiles for the segments produced by any of those clustering techniques , a classification technique such as decision trees can be applied to the clustered customer data , using the cluster number resulted from the clustering analysis as the classification variable . The result is several DNF rules , one for each leaf in the resulting tree , that describe the clusters . A relatively large error can be expected since now we are using axis parallel rectangles to describe clusters of arbitrary shapes . In addition , this method does not find the minimal description of the clusters .
2.2 CLIQUE CLIQUE ( 1 ) is a new clustering technique that has been developed to address these problems . The primary problem is the high dimensionality of data . Because of the high dimensionality and the existence of noisy dimensions , almost every region in the space has a low density of points , and all points are far from each other . This technique automatically identifies ( in general several ) subspaces of the original data space which would allow better clustering of the data points than the original space . Restricting the search to only subspaces of instead of using new dimensions ( for example linear combinations of the original dimensions ) is an important factor in the solution because this restriction allows a much simpler , more comprehensible presentation of the results . Each of the original dimensions typically has a real meaning to the user , while even a simple linear combination of many dimensions may be very hard to interpret . the original space ,
The clustering model used is density based : intuitively , a cluster is a region that has a higher density of points than its surrounding region . The problem is to automatically identify projections of the input data into a subset of the attributes with the property that these projections include regions of high density .
The straightforward approach of enumerating all subspaces is clearly infeasible . However a high density cluster remains a cluster in all its projections , and this fact is used to discover the interesting subspaces in a bottom up fashion .
To compute an approximation of the density of the data points , the data space is partitioned into cells , and the number of points that lie inside each cell ( unit ) of the partitioning is found . The partitioning of the space is accomplished by regularly partitioning each dimension into the same number of intervals . This means that each unit has the same volume , and therefore the number of points inside the units can be used to approximate the density of the points inside it .
Once the appropriate subspaces are found , the task is to find clusters in the corresponding projections . The data points are separated according to the valleys of the density functions . To simplify the cluster descriptions , rectangular constraints are imposed on the geometry of clusters . The clusters are unions of connected high density units in subspaces .
Each unit in a k dimensional subspace can be described as a single AND term because it is the intersection of 2k axisparallel hyperplanes . Since each cluster is the union of such cells , it can be described by a DNF expression . A compact description is obtained by covering a cluster with a minimal number of maximal , possibly overlapping , rectangles and describing the cluster as a union of these rectangles .
Subspace clustering is tolerant to the problem of missing values in input data . A data point is considered to belong to a particular subspace if all the attribute values in this subspace are not missing , irrespective of the values of the rest of the attributes . Thus records with missing values can be used for clustering with more accurate results than is the case if missing values were replaced by values taken from a distribution .
2.3 Using the profiles Once the clusters are found , they should be evaluated for the purpose of prospecting . One technique is to use cluster properties such as size and density to sort the clusters . One may assume that a cluster that includes more points , and has a small diameter ( largest intra cluster distance ) is a better cluster than a smaller cluster of larger diameter . This approach can be used if the demographic attributes we are using for clustering are defining a numerical space , and therefore we can define distances between customers . In the case of CLIQUE , we can , in addition , define the average density of a given cluster , that is , the ratio of points in the cluster over the volume of the cluster description , and use this as a criterion to sort the clusters . This is a natural criterion to use because it rates the clusters according to how different the customer distribution inside the cluster description is from the uniform distribution . As a result , niche areas in the customer distribution will be receive a higher rating
Business properties can also be used for measuring the clusters . For example , average revenue or profit derived from the customers in a cluster may indicate the desirability of the cluster . Used in conjunction with the intrinsic cluster properties and the explicit DNF cluster descriptions , one can make educated judgments for choosing the market segments to pursue .
3 . PROSPECTING USING CUSTOMER AND MARKET INFORMATION Here we present a new method for identifying prospective customers without the need to conduct designed marketing campaigns . The method uses available customer data as well as demographic data the overall market population . It provides an intuitive measure to guide in the selection of marketing targets . that characterize
Similar to customer profiling , the approach here has two steps : 1 . Segment the population . 2 . Estimate to own" and penetration" for each segment of the population . the "propensity the "market
This approach is different from the previous section ’s approach because demographic data for a large sample of the overall market population are available , and the segmentation is done so as to optimally distinguish customers from the market sample . ( This distinction in methodology is sometimes referred to as doing supervised , as opposed to unsupervised , machine learning 2,7,10 ) It should also be noted that there are parametric methods from classical statistics , such as logistic regression ( 10 ) , which accomplish step 2 above without first going through step 1 . These have the drawback that no insightful descriptions result and DNF rules , which could eliminate unnecessary cost in the acquisition of lists of candidate households for prospecting , are not created . Using data from a recent project , we will later show a comparison of the results of our technique and logistic regression . The results will demonstrate that no predictive accuracy is sacrificed when using our data mining based approach .
3.1 Segmenting by SLIQ Given demographic data for a set of customers , and the same demographic data for a sample of the market population , a decision tree algorithm is used to distinguish the customers from the overall market population . In the examples we refer to in this paper , we have used the decision tree component in IBM ’s Intelligent Miner toolkit , known within IBM as SLIQ ( 5,8 ) . Starting with the entire population , SLIQ repeatedly divides ( sub )populations in two so as to maximally reduce the "impurity" with respect to the response variable , CUSTFLAG ( “ Customer or Market Sample ? ” ) in each of the two new subpopulations at each step . Each such division is made by optimally choosing a demographic variable and partitioning the sub population based on the values of that variable . This results in a partition of the space into axis parallel regions that contain either mostly customers or mostly points from the sample market population . The profiles for these regions are given by the decision tree ’s structure and satisfy the desiderata for simplicity and comprehensibility . Each leaf of the tree defines a population segment . Each segment is characterized by a set of conditions on demographic attributes , their number being equal to the depth of the tree from the starting node to the specific leaf . In addition , SLIQ determines an optimum stopping point to avoid overfitting to the training data ( the data from which the model is created ) . Such overfitting would produce a model which does not generalize to new unseen data . the is done by coding
Because we are interested in producing a continuous score , we run SLIQ in regression mode , as opposed to classification mode . This response variable CUSTFLAG as 1 and 0 , instead of “ Customer ” and “ Market Sample ” , and specifying that this variable is numeric rather than categorical . It is felt that , in a 0 1 problem such as this , the best results from a binary decision tree algorithm are obtained using a training set with roughly as many "0" examples as "1" examples . Therefore , for training ( constructing the model ) we use all the customer training records , but only a random sample of the sample market population records equal in number to the customers in the training set .
In an example taken from a life insurance prospecting model generated by SLIQ , the first split is made on the variable OCCUPATION . At the next stage , the white collar workers are split by the value of GENDER while the blue collar workers are split by the value of INCOME . After the next stage , three of the eight subsegments are marked by squares , indicating that the algorithm has determined that no further splitting should be done at these nodes and these nodes are declared to be leaves , ie final segments . The algorithm continues in this way until it determines that no more statistically valid splits can be made . Scores will then be computed for each leaf of the tree . The DNF rule corresponding to the first leaf created would be : IF OCCUPATION=White Collar AND GENDER=Male AND NEXTVAR=Value THEN Score=X .
3.2 Scoring The most natural candidate for a score to assign to each segment is the density of customers in that segment . This method has a serious drawback because we have artificially arranged for the overall density in the training data to be 50 % . Neither the test data ( which is held in reserve at the model creation phase ) nor the real world data to which the model will be applied have a comparable overall customer density . For each segment , we therefore compute the ratio of the percent of the customer population to the percent of the market sample population that falls into the segment . This ratio , the "market penetration rate" or MPR of the segment , is used to score the segments . For any segment S , we define its market penetration rate as
MPR(S)= ( % of all customer records which fall into S ) / ( % of all market sample records which fall into S )
This is a quantity which does not depend on the overall density of customers in a data set and is comparable in sets with differing overall customer densities .
If p(S ) is the density of customers in the segment S ( ie the probability that a record in the segment is a customer record ) , and r is the overall ratio of market sample records to customer records , it may be shown that p(S ) = MPR(S ) / ( r + MPR(S ) ) and that MPR(S ) = r * p(S ) / ( 1 p(S) ) .
We have constructed the training data so that r=1 . In the case of the training data , the above equations imply that if p(S ) is the probability of a record in S being a customer record , then MPR(S ) is the corresponding odds . It should be noted that in logistic regression it is precisely the logarithm of the odds which is modeled as a linear function of the explanatory variables .
4 . EVALUATING A SCORING MODEL The objective of predictive modeling is always to build a model which best predicts outcomes for future data . In order to make a comparative evaluation of competing models , we apply these models to test data ( data which was held in reserve during model creation ) and compare their predictions to the actual outcomes in the test data . The model that predicts the outcomes in the test set most accurately is the preferred one ( all other things being equal ) .
When the outcome of the model is a score , both the training and test data can be ranked by this score and the properties of the ranked data studied . If the score produced is a measure of customers' propensity to buy a product , the ranking , in descending order of the predicted score , in both the training and test sets , should contain at the top of the list a high proportion of those customers who did , in fact , purchase the product . The user can employ the ranked list to make tradeoff decisions between unnecessary marketing expense and missed customer opportunity . If he wants to be reasonably sure of missing at most 5 % of actual buyers , he can determine how far down the ranked list he needs to go to cover 95 % of the purchasers . If he needs to market to 50 % of the list to reach this target according to one model and to only 35 % according to another model , the second model is superior . Of course , this must hold for the test data as well as the training data .
The scoring models we create are , likewise , evaluated by sorting the test data in descending order of predicted score . Since there are as many distinct scores as there are segments or leaves in the decision tree , this can be done by constructing a table in which the rows correspond to the model's segments . For each segment , we tabulate its predicted score , and what percentage of all test records are in that segment We also tabulate the percentage of customer records that are in the segment and the percentage of the market sample records that are in the segment . In order to track how well the predicted ranking is doing in terms of giving the highest scores to customers , we sort the rows of the table in descending order of predicted score . We then tabulate the cumulative percentages of total records covered , customer records covered , and market sample records covered . A typical row of the table would inform us , for example , that by restricting marketing activity to prospects with a score of 1.4 or higher , the model would have selected 76 % of all records which in actuality came from customer data , but less than 28 % of the records which came from the market sample .
An evaluation table such as the table described above can be helpful in choosing a threshold score for data acquisition . Once this is done , the original tree may be pruned to a much simpler tree for the purpose of building DNF rules for data acquisition . For example , by choosing a threshold of 5 % for missed opportunity , we might see that it would be sufficient to acquire data only for households with a score greater than 333 By examining the full tree , we may safely prune it at any node where the leaves below it all have scores greater than .333 or all have scores less than 333 In our life insurance example , an original tree with 115 leaves is simplified to a tree with only 25 leaves . Thus 25 DNF rules decided what data records to acquire , and the full decision tree will then be used to score these data .
4.1 Visual Evaluation The data in the evaluation table described above is often plotted in a lift or gains chart ( Figure 2 ) to give a quick way to visually evaluate the results of the scoring procedure and to compare different scoring models . The x axis corresponds to the cumulative percentage of the total population . On the yaxis we plot the percentage of customers in the test data we would have obtained using this percentage as a threshold , as well as the percentage of the market sample in the test data we would have obtained . As a reference line , we plot the line y=x . This represents the percentage of either the market sample or the customer population that would be obtained by an entirely random sorting of the examples . The best models have a customer population curve considerably above the reference line and the market sample curve below it . Below is the lift chart corresponding to the evaluation table example above .
4.2 Quantitative Evaluation Two quantitative measures that are often used to evaluate a scoring model are described below .
1 . Concordance/Discordance For every pair of records ( c,m ) , in the test data , where c is a customer record and m is record from the market sample data , there are three possibilities . Either MPR(c)>MPR(m ) or MPR(c)<MPR(m ) or MPR(c)=MPR(m ) . In the first case , we call the pair concordant , while in the second case it is said to be discordant . In the third case the pair is tied . Obviously , the more concordant pairs the model produces , the better the model . The concordance is defined to be the percentage of all pairs ( c,m ) which are concordant . Similarly , the discordance is the percentage of all pairs which are discordant .
2 . Negative Log Likelihood This is an information theoretic measure which is usually used when the score can be interpreted as the probability of an event occurring . In our case , this event is the purchase of a product and its probability can be calculated from the score predicted by the model . A function is chosen to measure how much information loss occurs when the predicted probability for a record in the test data replaces the actual 0 or 1 value of the response variable , CUSTFLAG . For a customer record , one with a 1 value for the response variable , this loss function is taken to be log(p ) , where p is the probability predicted by the model . Note that since p is between zero and 1 , this is a positive number , which gets smaller as p approaches 1 . For a record from the market sample , the loss function is defined as log(1 p ) which , again , is positive , but gets small as p approaches 0 . Thus , the closer the probability is to the actual 0 or 1 outcome , the smaller the loss function will be . The total negative log likelihood measure of the model M , LL(M ) , is obtained by summing these functions over all the records in the test data .
LL(M ) = S cust log(p ) + S market sample log(1 p ) where p is the probability predicted by the model . The model M with the minimal LL(M ) is the preferred one .
5 . EXPERIMENTAL RESULTS We show the results of using our methods on actual prospecting data for the purchase of two life insurance products . We show a comparison between the performance on test data of our datamining methodology and logistic regression . For the logistic regression exercise , we used the algorithm supplied in the SAS statistical package .
The lift chart for one of the products is shown in Figure 2 , and it is almost impossible to distinguish the curves from SLIQ ’s model to those drawn from the logistic regression model . SLIQ ’s curve for cumulative customer percent is slightly higher than the corresponding curve for logistic regression , but this difference is probably not significant . table of measurements ( Table 1 ) , likewise , shows The extremely close results . The model labeled “ Constant ” , corresponding to a model in which the same score is given to all records , is included to illustrate how small the differences between the other two models are . The negative log likelihood measures should be compared on a per record basis only , as different models handle records with missing data differently , sometimes discarding these records .
Product 1 Model
SLIQ Train SLIQ Test LogitRegr Train LogitRegr Test Constant Train Constant Test
Log
Neg Likelihood 55788.972 160667.474 56744.596 161543.302 77276.000 214265.760
Per Record
Concordance % Ties %
Discordance %
0.501 0.527 0.515 0.537 0.693 0.701
82.536 82.264 81.873 81.994 0.000 0.000
1.053 1.067 0.000 0.000 100.000 100.000
16.413 16.678 18.127 18.006 0.000 0.000
Table 1 : Experimental results for Product 1 .
Figure 2 : Prospecting Model Lift Charts SLIQ vs Logistic Regression
100
80
60
40
20
% e v i t a l u m u C
0
0
20
40
60
80
100
Cumulative % of Total Population into
5.1 Scoring Results from Clustering Algorithms As mentioned previously , SLIQ falls the class of supervised machine learning methods . It creates the segments by maximizing the separation of the customer records from a market sample . This property helps create highly efficient prospecting models as measured by MPR while providing explicit DNF descriptions of the market segments . It is interesting to note that the same scoring and evaluation methodology described in the previous sections can be applied to segments ( clusters ) created by unsupervised clustering algorithms . We describe here some sample results using the clustering technique CLIQUE described in Section 22
First we give are some examples of the clusters discovered by CLIQUE for insurance customers of the same two life products discussed above . There were 16 demographic attributes associated with each customer record . CLIQUE found clusters in 9 dimensional subspaces . The algorithm used only the customer data to find the clusters . Then , for each cluster , its coverage of the market sample and its MPR was computed . Recall that after clustering the customer records , we find regions in the data space that have a large density of customer records . We also know the number of customer records in each cluster . We can now use the market sample information and compute , for each such cluster , the number of market sample records that falls into the same space . We can then calculate , for each cluster found by CLIQUE , the MPR of the cluster , which is the ratio of these two numbers normalized by the total number of customer records and the total number of market sample records .
For example , one of the clusters with high MPR ’s that was discovered using the dataset of customers of the first product had an MPR of 4.56 and covered 9.5 % of the customer population .
The demographic attributes used to define this cluster are : whether the head of household is a professional ( Professional ) , whether the head of household is retired ( Retired ) , whether the household is a buyer by mail ( Mail_Buyer ) , whether the household owns or rents their dwelling ( Own_or_Rent ) , the size of the household dwelling ( Dwelling_Size ) , the number of adults in the household ( Number_of_Adults ) , the age of the head of the marital status ( Marital_Status ) and the gender of the head of the household ( HOH_Gender ) . The DNF expresion that describes this cluster is : ( Professional=YES ) AND ( Retired=NO ) AND the household ( HOH_Age ) ,
( Mail_Buyer=YES ) AND ( Own_or_Rent =OWN ) AND AND ( Number_of_Adults=2 ) AND ( 32<HOH_Age<48 ) AND AND ( HOH_Gender =MALE )
( Marital_Status=MARRIED )
( Dwelling_Size=SMALL )
Once we have the clusters from CLIQUE and calculated their MPR ’s , we sort them according to their MPR values . For a given threshold , we can take all the clusters with market penetration rate above the threshold and find the percentage of the customer population and the percentage of the market sample population that this set of clusters covers . Note that the clusters may be overlapping , so individual records may be covered by more than one cluster . This can result to an overall MPR for the set of clusters that is lower than the threshold we set for each cluster .
Figure 3 : Prospecting Model
CLIQUE/SLIQ Lift Charts patterns in the parameter space . Above we have shown how we can use the segments , sorting them , and picking the top ones for a marketing campaign . the same methodology , namely measuring
6 . CAMPAIGN RESPONSE MODELING For conducting marketing campaigns , it is best to build a prediction model that characterizes who would respond to the
Data Mining Backend
Real time Internet
Delivery System
Data Mining
Identify Prospects
Optimized Prospects
US
Household
Demographics
Existing Customers
Cumulative % of Customers
100
80
60
40
20
0
0
SLIQ
CLIQUE
Prospecting
Server
Internet
Agent Desktop
20
40
60
80
100
Cumulative % of M arket Sample
The experimental results ( Figure 3 ) show that the clusters that CLIQUE finds in the customer data , if sorted using information about a sample of the market population , can be effectively used to target specific segments of the population . In the following lift chart we show the results we obtained after running experiments with the same dataset of life insurance customers modeled by SLIQ . To create this graph we first sort the clusters according to the market penetration rate . This creates an ordering of the clusters from 1 to n ( assuming n clusters ) . Then we find what percentage of the market sample and what percentage of the customer population is covered by all clusters with market penetration ratio of at least 3.5 ( for the first point ) , 3 , 2.5 , and 2 . For example , the clusters with market penetration of at least 3.5 cover 65 % of the customers and 20 % of the market sample . The corresponding line for the model generated by SLIQ is included for comparison purposes .
It is interesting to compare the supervised ( eg SLIQ ) and the unsupervised ( eg CLIQUE ) approaches in this context . In general , when a well defined objective/ measure is available ( MPR in our case ) , a supervised approach should be preferred because it is more likely to produce a better partition of the space with respect to the objective . In some cases , unsupervised clustering techniques may be appropriate for discovering data
Figure 4 End to End Solution Architecture campaign and who would not , and then target only those who have a high propensity to respond based on the model . Any technique for developing prediction models is applicable , including regression trees , logistic regression , neural networks , radial basis functions , etc In Section 5 we have shown a comparison of the results between SLIQ and logistic regression in both log likelihood measures based on our datasets . Given that the results are very close , and that SLIQ provides extra information ( eg DNF description of segments ) , we recommend that SLIQ be the preferred method for creating campaign response models . the concordance/discordance and negative
In practice , creating prediction models for targeted , niche products is rare due to the lack of sufficient responder vs . nonresponder data . In the previous sections , we have discussed approaches that can be deployed when such data are not available . These approaches can also be used in a bootstrapping manner : collecting response data while conducting the campaign and refining the models using the response data when sufficient number of data points have been collected . The problem of refining a prospecting model is an interesting research topic . One possibility is to develop a refined model for each segment in the original model so that subsegments with higher rates of responders could be identified . Another possibility is to develop a new model , based purely on the new data , to replace the original model . The new model would redefine the segments in the space defined by the selected segments from the original model .
9 . REFERENCES
7 . ON LINE PROSPECTING The methodology described in this paper was implemented as a part of an end to end solution that allows insurance agents to access optimized prospect information through the worldwide web ( 11 ) . Figure 4 depicts a high level architecture view the solution . The solution consists of two parts . The first part creates and maintains a large database of qualified prospects . This qualified prospect database was created starting from a general database of US households , and then applying the segment creation and scoring methodologies described in this paper . The segmentation and scoring were applied separately for each product of interest such as term life or annuity . The second part of the solution delivers information on the prospects in a manner that leverages the preferences and skills of each individual agents . An agent can specify the type of product , a preferred client profile and a geographic area in which he or she wishes to prospect using an easy to use browser based program . The Prospect Server ( Figure 4 ) dynamically compiles the prospect list for download based on the agent ’s preferences and the scores of the households from the data mining operation . Note that the agent preferences are applied to subselect from the qualified prospect database created by the segmentation and scoring process . The Prospect Sever also keeps track of the uses of the leads to ensure , for example , agents are not competing for the same prospect . With a few mouse clicks , the agent can download the optimized leads from the web . The solution has been piloted by an insurance company to support their independent agents . A modified version of the solution was successfully implemented for a cross selling application in the insurance industry . We refer to [ 3 ] for a more detailed description of the technique .
8 . CONCLUSION This paper described some results and observations based on a recent study with an insurance company for identifying prospects for two life product families . We experimented with several techniques during the study , and devised a methodology that exploits the existing customer data in conjunction with data from a sample of the general population . This methodology is effective in the absence of buyer vs non buyer data , and it is being deployed by an insurance industry client to support its agent sales force .
[ 1 ] "Automatic Subspace Clustering of High Dimensional Data for Data Mining Applications" by R . Agrawal , J . Gehrke , D . Gunopulos and P . Raghavan , in Proc . of the ACM SIGMOD Conference on Management of Data , Seattle , Washington , 1998 . [ 2 ] "Classification and Regression Trees" , by L . Breiman , J . H . Friedman , R . A . Olshen and C . J . Stone , Wadsworth , Belmont , 1984 . [ 3 ] “ Identifying prospective customers using data mining techniques ” , by Paul Chou , Edna Grosman , Dimitrios Gunopulos and Pasumarti Kamesam , RC 21661 , IBM TJ Watson Research Center , Yorktown Heights , NY . Jan . 2000 . [ 4 ] A Density Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise" , by Martin Ester , HansPeter Kriegel , Jorg Sander and Xiaowei Xu , in Proc . of the 2nd Int'l Conference on Knowledge Discovery in Databases and Data Mining , Portland , Oregon , 1995 . [ 5 ] "SLIQ : A Fast Scalable Classifier for Data Mining" , by Manish Mehta , Rakesh Agrawal and Jorma Rissanen in Proc . of the Fifth Int'l Conference on Extending Database Technology ( EDBT ) , Avignon , France , 1996 . [ 6 ] "Efficient and Effective Clustering Methods for Spatial Data Mining" by Raymond T . Ng and Jiawei Han , in Proc . of the VLDB Conference , Santiago , Chile , 1994 . [ 7 ] "C4.5 : Programs for Machine Learning" , by J . Ross Quinlan , Morgan Kaufman , 1993 . [ 8 ] "SPRINT : A Scalable Parallel Classifier for Data Mining" by John Shafer , Rakesh Agrawal and Manish Mehta , in Proc . of the 22nd Int'l Conference on Very Large Databases , Bombay , India , 1996 . [ 9 ] "SAS Manual" , SAS Institute , 1995 . [ 10 ] " “ Computer Systems That Learn ” , S . Weiss and C . Kulikowski , Morgan Kauffman , San Mateo , CA , 1991 . [ 11 ] “ Electronic Prospecting ” , by D . Yellin , G . Anderson , and P . Chou , GAMA , September October , 1998 , page 49 51 . [ 12 ] "BIRCH : An Efficient Data Clustering Method for Very Large Databases" , by Tian Zhang , Raghu Ramakrishnan and Miron Livny , in Proc . of the ACM SIGMOD Conference on Management of Data , Montreal , Canada , 1996 .
