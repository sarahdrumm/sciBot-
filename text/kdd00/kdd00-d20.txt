Data Mining Techniques for Optimizing Inventories for
Electronic Commerce
Massachusetts Institute of Technology
Massachusetts Institute of Technology
Massachusetts Institute of Technology
Anjali Dhond
Room E53 311
40 Wadsworth Street
617 253 8906
Amar Gupta
Room E53 311
40 Wadsworth Street
617 253 8906
Sanjeev Vadhavkar
Room 1 270
77 Massachusetts Avenue
617 253 6232 vada@mit.edu adhond@mit.edu agupta@mit.edu
ABSTRACT As part of their strategy for incorporating electronic commerce capabilities , many organizations are involved in the development of information systems that will establish effective linkages with their suppliers , customers , and other channel partners involved in transportation , distribution , warehousing and maintenance activities . These linkages have given birth to comprehensive data warehouses that integrate operational data with supplier , customer , channel partners and market information . Data mining techniques can now provide the technological leap needed to structure and prioritize information from these data warehouses to address specific end user problems . Emerging data mining techniques permit the semi automatic discovery of patterns , associations , changes , anomalies , rules , and statistically significant structures and events in data . Very significant business benefits have been attained through the integration of data mining techniques with current information systems aiding electronic commerce . This paper explains key data mining principles that can play a pivotal role in an electronic commerce environment . The paper also highlights two case studies in which neural network based data mining techniques were used for inventory optimization . The results from the data mining prototype in a large medical distribution company provided the rationale for the strategy to reduce the total level of inventory by 50 % ( from a billion dollars to half a billion dollars ) in the particular organization , while maintaining the same level of probability that a particular customer ’s demand will be satisfied . The second case study highlights the use of neural network based data mining techniques for forecasting hot metal temperatures in a steel mill blast furnace .
Keywords Inventory Optimization , Temporal Data Mining , Data Massaging .
1 . INTRODUCTION
The past two decades have witnessed a dramatic increase in information being stored in electronic format . This surge will be further compounded by an ever growing number of organizations embracing the paradigm of electronic commerce . The amount of information in the world is estimated to double every 20 months and the size and the numbers of databases are increasing at a still faster pace . The increase in the use of electronic data gathering devices , such as point of sale devices and remote sensing devices , is one factor for this explosive growth .
In electronic commerce environments , the rapidly escalating volume of data makes timely and accurate data analysis beyond the reach of the best human domain expert , even hordes of them working day and night . Instead , emerging data mining techniques offer far superior abilities to discover hidden knowledge , interesting patterns and new business rules hidden within huge repositories of electronic databases . Currently regarded as the key element of the more elaborate process of Knowledge Discovery in Database ( KDD ) , the data mining paradigm integrates theoretical perspectives from the realms of statistics , machine learning and artificial technology implementation , it relies on advances in data modeling , data warehousing and information retrieval . However , the more important challenges lie in organizing business practices around the knowledge discovery activity . As organizations gear towards a web enabled economy and increasingly rely on online information sources for a variety of decision support applications , one will witness a growing reliance on data mining techniques in the electronic commerce space . the standpoint of intelligence . From
Data mining involves the semi automatic discovery of patterns , associations , changes , anomalies , rules , and statistically significant structures and events in data . In other words , data mining attempts to extract knowledge from data . Data mining differs from traditional statistics in several ways : formal statistical inference is assumption driven in the sense that a hypothesis is formed and validated against the data . Data mining in contrast , is discovery driven in the sense that patterns and hypothesis are automatically extracted from large data sets . Further , the goal in data mining is to extract qualitative models , which can easily be translated rules or visual representations . Therefore , the results of the data mining process may be patterns , insights , rules , or predictive models that are frequently beyond the capabilities of the best human domain experts . into business patterns , logical segments ,
In the electronic commerce space , data mining techniques have the potential of providing companies with competitive advantages in optimizing their use of information . Potential applications include the following [ 16][17][18][19 ] : ♦ To manage customer relationships by predicting customer buying habits , calibrating customer loyalty and retention , analyzing customer target marketing and promotion effectiveness , customer profitability , customer lifetime value , and customer acquisition effectiveness .
♦ To enable financial management through analytical fraud detection , claims reduction , detection of high cost to serve orders or customers , risk scoring , credit scoring , audit targeting and enforcement targeting .
♦ To position products by product affinity analysis that shows opportunities for cross selling , up selling and strategic product bundling .
♦ To develop efficient and optimized inventory management system based on Web customer demand predictions .
♦ To implement more efficient supply chains with suppliers and contractors .
2 . CASE STUDIES 2.1 Medicorp – Pharmaceutical Distribution Company
Large organizations , especially geographically dispersed organizations , are usually obliged to carry large inventories of products ready for delivery on customer demand . Inventory optimization pertains to the problem of how much quantity of each product should be kept in the inventory at each store and each warehouse . If too little inventory is carried relative to demand , unsatisfied customers could turn to competitors . On the other hand , a financial cost is incurred for carrying excessive inventory . In addition , some products have short expiration periods and shelf life and therefore , must be replaced periodically . Inventories take a lot of money to maintain . The best way to manage an inventory is through the development of better techniques for predicting customer demands and managing stock inventories accordingly . In this way , the size and the constitution of the inventory can be optimized with respect to changing demands .
With hundreds of chain stores and with revenues of several billion dollars per annum , “ Medicorp ” is a large retail distribution company . Medicorp revenues exceeded $15 billion from over 4100 stores in 25 states in the United States . Medicorp dispenses approximately 12 % of all retail prescriptions in the United States . In keeping with its market leading position , Medicorp is forced to have a large standing inventory of products ready to deliver on customer demand . The problem is how much quantity of each drug should be kept in the inventory at each store and warehouse . Because of unfulfilled prescriptions , unsatisfied customers may switch company loyalties , relying on other pharmacy chains for their needs . On the other hand , Medicorp incurs a financial cost if it carries excessive inventories . In addition , pharmaceutical drugs have a short expiration date and must be renewed periodically .
Historically , Medicorp has maintained an inventory of approximately a billion dollars on a continuing basis , using traditional regression models to determine inventory levels for each drug item . The corporate policy of Medicorp is governed by two competing principles : minimize total inventory and achieve highest level of customer satisfaction . The former principle is not quantified in numerical terms . On the latter issue , Medicorp strives to achieve a 95 % fulfillment level . That is , if a random customer walks into a random store on a random day for a random drug , the probability for the availability of the particular item must be 95 % . The figure of 95 % is based on the type of goods that Medicorp carries , and levels offered by competitors of Medicorp for the same items . Medicorp has a corporate wide data warehouse system that maintains data on what was sold , at what price , and to whom at each store . the service
After reviewing various options , and using conventional inventory optimization techniques , Medicorp adopted a “ threeweeks of supply ” approach . This approach involved the regression study of historical data to compute a seasonally – adjusted estimate of the forecasted demand for the next three week period . This estimated demand is the inventory level that Medicorp keeps , or strives to keep , on a continuing basis . Each store within the Medicorp chain orders replenishments on a weekly basis and receives the ordered items 2 3 days later from a regional data warehouse . Historically , this model has yielded the 95 % target for customer satisfaction .
To find the best solution to the inventory problem , we analyzed data maintained within the transactional data warehouse at Medicorp . The Medicorp data warehouse is of the order of several gigabytes in size . In the modeling phase , we extracted a portion of the recent data fields , which was deemed to provide adequate raw data for a preliminary analysis : ♦ Date field – Indicates the date of the drug transaction ♦ NDC number – Uniquely identifies a drug ( equivalent to a drug name ) purchased
♦ Customer number – Uniquely identifies a customer ( useful in tracking repeat customers )
♦ Quantity number – Identifies the amount of the drug
♦ Sex field – Identifies the sex of the customer ♦ Days of Supply – Identifies how long that particular drug purchased will last of the particular drug
♦ Cost Unit Price – Establishes the per unit cost to Medicorp
♦ Sold Unit Price – Identifies per unit cost to the customer of the particular drug Before adopting neural network based data mining techniques , preliminary data analysis was utilized to help search for seasonal trends , correlation between field variables and significance of variables , etc . Our preliminary data provided evidence for the following patterns : ♦ Most sales of drug items showed minimal correlation to seasonal changes .
♦ Women are more careful about consuming medication than men , indicating that women customers were more likely to complete the prescription fully than men .
♦ Drug sales are heaviest on Thursdays and Fridays , indicating that inventory replenishment would be best ordered on Monday .
♦ Drug sales ( in terms of quantity of drug sold ) show differing degrees of variability :
♦ Maintenance type drugs ( for chronic ailments ) show low degrees of sales variability .
♦ Acute type drugs ( for temporary ailments ) show high degrees of sales variability .
There is no general theory that specifies the type of neural network , number of layers , number of nodes ( at various layers ) , or learning algorithm for a given problem . As such , data mining analysts must experiment with a large number of neural networks before converging upon the appropriate one for the problem in hand . In order to evaluate the relative performance of each neural network , we used statistical techniques to measure the error values in predictions . Most major neural network architectures and major learning algorithms were tested using sample data patterns from Medicorp . Multi Layer Perceptron ( MLP ) models and Time Delay Neural Network ( TDNN ) models yielded promising results and were studied in greater detail .
Modeling short time interval predictions is difficult , as it requires a greater number of forecast points , shows greater sales demand variability , and exhibits lesser dependence on previous sales history . Using MLP architectures and sales data for one class of products , we initially attempted to forecast sales demand on a daily basis . The results were unsatisfactory : the networks produced predictions with very low correlation ( generally below 20 % ) and very high absolute error values ( generally above 80 % ) . Hence , modeling for larger time intervals was attempted next . the accuracy of prediction and
As expected , forecasting for a week proved more accurate than for a day and forecasting for one month proved more accurate than for a week . Indeed , when predicting aggregate annual sales demand , we obtained average error values of only 2 % . Keeping a weekly prediction interval provided the best compromise between the usefulness of the predicted information for Medicorp . The weekly forecasts are useful for designing inventory management systems for individual Medicorp stores , while the yearly forecasts are useful for determining the performance of a particular item in a market and the overall financial performance of the organization . The neural network was trained with historic sales data using two methods : the standard method and the rolling method . The difference between these two methods is best explained with an example . Assume that weekly sales data ( in units sold ) were 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 , 100 , etc . In the standard method , we would present the data : “ 10 , 20 , 30 ” and ask the network to predict the fourth value : “ 40 ” . Then , we would present the network with “ 40 , 50 , 60 ” and ask it to predict the next value : “ 70 ” . We would continue this process until all training data were exhausted . On the other hand , using the rolling method , we would present historic data as “ 10 , 20 , 30 ” and ask the network to predict the fourth value : “ 40 ” ; then , we would present the network with “ 20 , 30 , 40 ” and ask it to predict the fifth value : “ 50 ” . We would continue using the rolling method until all the training data were exhausted .
The rolling method has an advantage over the standard method in that it produces a greater quantity of training examples from the same data sample , but at the expense of training data quality . The rolling method can “ confuse ” the neural network because of the close similarity between training samples . Using the previous example for instance , the rolling method would produce “ 10 , 20 , 30 ” ; “ 20 , 30 , 40 ” ; “ 30 , 40 , 50 ” . Each of these training samples differs from another data set by a single number only . This minuscule difference may reduce the neural network ’s ability to understand the underlying pattern in the data .
At Medicorp , some items sell infrequently . In fact , some of the specialized drugs may sell only twice or thrice a year at a particular store . This lack of sales data is a major problem in training neural networks . To solve it , we used other methods for transformation of data , reuse , and aggregation of data . The one we found most effective involved changing future data sets with some known fraction of past data sets . If X[i]’ represents the ith changed data set , X[i ] represents the ith initial data set , X[i 1 ] represents the initial ( i 1)th initial data set and µ is some numerical factor , then the new time series can be computed as X[i]’ = X[i ] + µ * X[i 1 ] , X[0]’ = X[0 ] . The modified time series thus has data elements that retain a fraction of the information of past elements . By modifying the actual time series with the proposed scheme , the memory of non zero sales items is retained for a longer period of time , making it easier to train the neural networks with the modified time series .
As mentioned before , the policies at Medicorp are governed by two competing principles : minimize drug inventories and enhance customer satisfaction via high availability of items in stock . As such , we calibrated the different inventory models using two parameters : “ undershoot ” and “ days of supply ” . The number of “ undershoots ” denotes the number of times a customer would be turned away if a particular inventory model were used over the “ test ” period . The “ days of supply ” statistic is the number of days the particular item in the inventory is expected to last . By using the latter parameter , one reduces the complexity and allows for equitable comparisons across different categories of items . For example , items in the inventory are measured in different ways : by weight or by volume or by number . If one talked in terms of raw amount , one would need to take into account different units of measure . However , the “ days of supply ” parameter allows all items to be specified in terms of one unit : days . The level of popularity of the item gets factored into the “ days of supply ” parameter . While maintaining a 95 % probability of customer , the MLP model reduces “ days of supply ” for items in the inventory by 66 % . On the average , the neural network “ undershoots ” only three times ( keeping the 95 % customer satisfaction policy of Medicorp ) .
Our models suggested that , as compared to the “ three weeks of supply ” thumb rule , the level of inventory needs to be “ reduced ” for popular items and “ increased ” for less popular or unpopular items . This inference appears counter intuitive at first glance . However , since fast moving items are already carried in large amounts , and since they can be replenished at weekly intervals , one can reduce the inventory level without adversely impacting the likelihood of availability when needed . This is the factor that permits significant reduction in the size of the total inventory , and has been highlighted by a number of observers in the popular press .
To summarize the effort , we developed the neural network based data mining model for reducing the inventory at Medicorp from over a billion dollars worth of drugs to about one half billion dollars ( reduction by 50 % ) while maintaining the original customer satisfaction level ( 95 % availability level ) . 2.2 Steelcorp – Iron and Steel Company
The blast furnace is the heart of any steel mill . Inside the blast furnace , the oxygen from the iron oxides is removed to yield nearly pure liquid iron . This liquid iron , or pig iron , is the raw material used in the steel plants . As with any product , the quality of this pig iron can vary . The most important determinants of the quality are ( 1 ) the amount and composition of any impurities , and ( 2 ) the temperature of the hot metal when it is tapped from the blast furnace [ 8 ] . The quality of the pig iron produced is important in determining how costly it will be to produce steel from the pig iron , as well as constraining what final types of steel into which the pig iron can be made . Therefore , it is crucial that hot metal temperature be maintained within an optimal range of values [ 9 ] . A blast furnace is very difficult to model due to the complex flow conditions with mass and heat transfer inside . For many years , blast furnace operators have been aware of the fact that there are no universally accepted methods for accurately controlling blast furnace operation and predicting the outcome . The Hot Metal Temperature and Silicon Content are important indicators of the internal state of a blast furnace as well as of the quality of the pig iron being produced . The production of pig iron involves complicated heat and mass transfers and introduces complex relationships between the various chemicals used .
This case study presents preliminary results from the use of Artificial Neural Networks ( ANNs ) as a means of modeling these complex inter variable relationships . The research is based on three months of operational data collected from the blast furnace of “ Steelcorp ” . Steelcorp is one of Asia ’s largest manufacturers of iron and steel and has multiple blast furnaces operating in tandem at multiple locations . Most of the blast furnaces are stateof the art and automatically collect and store data at periodic intervals on a number of input and output parameters for future analysis . the other data points were taken every five minutes . Linear interpolation between measurements of HMT was used to approximate values for the missing data points .
The raw data from the blast furnace contained a total of 9100 data points taken every five minutes . This five minute level data may see some inputs changing rapidly from one value to another , but since the temperature changes slowly over a longer period of time these short term changes do not have a noticeably affect on the output . Domain knowledge from Steelcorp indicated that an effective unit for considering the data would be in blocks of one hour . Therefore , groups of twelve data points were averaged to create one data point , which represented one hour block . While hourly averaging of the data improved the predictive ability of the network , it had a side effect of greatly reducing the number of data points available for training the networks . The hourly averaging reduced the number of data points to approximately 760 . A moving window technique was used to deter this problem . The moving window takes the first twelve data points and averages them , but in the next step it shifts over by a five minute interval and averages the new data point with the previous eleven data points . The window continues to slide one data point at a time , until the end of the set is reached . This technique allowed the use of almost the same number of data points as in the original dataset .
The initial data contained 35 input parameters . Analysis of the data revealed that some of the input variables were redundant and others were not useful in predicting HMT or Silicon content . In order to discover which variables were the most important , a sensitivity analysis was performed on all of the 35 input variables . The way this was done was to calculate the correlation coefficient between each input variable and the corresponding output variable ( HMT ) . The reasoning is that the higher the correlation between a particular input and HMT , the more ‘important’ that particular input variable must be in determining HMT . Therefore , such a variable should be included in the dataset . Using correlation relationships and information from the blast furnace experts at Steelcorp , the number of input variables was narrowed down from 35 to 11 . These 11 variables were : total coke , carbon oxide , hydrogen , steam , group 1 heat flux , group 2 heat flux , actual coke injection , % oxygen enrichment , ore/coke ratio , hot blast temperature ( degrees C ) , charge time for 10 semi charges , and the previous measured hot metal temperature ( HMT ) .
Two distinct types of data sets were created in order to model future silicon content . The first type of data set consisted of 38 of the 39 input/output columns of the five minute interval HMT data ( the only column omitted was the time column ) . These variables were used as the inputs in order to predict the lone output variable , Si % , which is a column that was extracted from hot metal chemistry data .
Since Si % was measured at a less frequent rate than the HMT input variables , the addition of the silicon column as the output column resulted in having large , contiguous regions of the output variable that had the same , constant value . Therefore , linear interpolation and hourly averaging were performed . In addition , the usual practices of implementing the best lags for each input column , filling of missing values with previous values , and normalization of each column were also implemented . The second type of silicon data set was processed in the same manner as above , but , includes additional variables as inputs . Specifically , the additional inputs used were taken from the Coke and Sinter
There have been many attempts by researchers to use AI techniques in order to predict different state variables of the blast furnace based on measured conditions within the furnace . However , modeling the relationships between various variables in the blast furnace has been quite difficult using standard statistical techniques [ 5 ] . The main reason is that non linearities exist between the different parameters used in pig iron ( hot metal ) production . Production of hot metal in a blast furnace is the result of complex chemical reactions that scientists have not been able to model explicitly . Therefore , many have turned to neural networks in order to predict various blast furnace parameters . For example , Bulsari and Saxen [ 5 ] used feed forward neural networks when trying to classify the state of a blast furnace based on the measurement of blast furnace temperatures . Bulsary et al [ 9 ] used multi layered feed forward artificial neural networks to predict the silicon content of hot metal from a blast furnace . Several different artificial neural network models were tried by Singh et al [ 7 ] in order to predict the silicon content of hot metal using : coke rate , hot blast temperature , slag rate , top pressure , slag basicity and the logarithm of blast kinetic energy . the blast raw data furnace was not consistent/enough for direct use during modeling . Reasons for this ranged from problems inherent to the data , such as missing or very anomalous values , to more subtle flaws such as not taking into account the effect of time lags in the production process . Several steps were involved in preprocessing the raw data into a dataset that would be suitable for training an artificial neural network .
Extremely abnormal data values were adjusted to make the data more consistent . Values that were more than two standard deviations from the mean were modified so that they would be two standard deviations away from the mean . In some cases a minimum value for a variable was specified . If two standard deviations below the mean is smaller than the minimum then the data was adjusted to the minimum value . This process removed any outliers from the dataset . A major problem with the original data was inaccurate values of HMT for many of the data points . HMT can be measured only approximately once every hour while from
The data sets , and include the following : Coke Ash , Coke VM , CSR , CRI , RDI , CaO , SiO2 , MgO , Al2O3 , FeO and Fe .
To present some of the key results from the modeling exercise , prediction results from modeling HMT 2 and 4 hours into the future are shown below ( Figures 1 and 2 ) . Figure 1 shows the graph of predicted Hot Metal Temperature , two hours ahead of time , against the observed value . The network with the lowest mean squared error ( MSE ) had 19 hidden nodes . A noticeable lag is present which indicates that the most important variable ( as far as the model is concerned ) for predicting future HMT is the previous known HMT . that the networks are not focusing on just the previous silicon value when predicting into the future .
Figure 3 : Silicon Content 2 Hour Network with coke and sinter as inputs Solid Predicted Dashed – Actual
Figure 1 : HMT 2 Hours Solid Predicted Dashed Actual
Figure 4 : Silicon Content 4 Hour Network with coke and sinter as inputs Solid Predicted Dashed Actual
The research group is currently looking at other artificial intelligence techniques such as genetic algorithms and pattern matching to control the conditions in the blast furnace . A prediction can indicate the future condition of the blast furnace based on the current conditions . This is extremely useful when the blast furnace operator can alter the current conditions in order to keep the future conditions within desirable range . In order to perform the task , the relationships between the variables being controlled and the variables affecting them must be known . But some characteristics of the problem make it difficult : 1 ) each variable being controlled is affected by a large number of variables , 2 ) the relationships between the variables being controlled and the variables affecting them are non linear , and 3 ) these non linear relationships change over time . The first step towards controlling the conditions in a blast furnace involves finding out which input variables are most influential in
Figure 2 : HMT 4 Hours Solid Predicted Dashed Actual
Similar analysis was performed for modeling silicon content . Preliminary results from the modeling are shown in Figures 3 and 4 . From Figures 1 4 and comparing the absolute error values from the analyses , our results indicate that the addition of coke and sinter as inputs did not provide any clear advantage in terms of predicting silicon content . One interesting point to notice is that , unlike in the HMT case , there is no deterioration in the quality of silicon predictions as the prediction horizon increases . In addition , the predictions do not seem to "lag" the actual values . This is a problem that we had with HMT estimation . This means
4 . CHECKLIST FOR POTENTIAL BENEFICIARIES producing an output . Our analysis uses HMT as the output variable .
Since we have been able to train networks that can predict HMT with a relatively high degree of accuracy , we now will find what kind of importance the neural network itself assigns each input variable when predicting HMT . The neural network will provide us with some insight into which variables we should pay special attention to when trying to predict and control HMT . We calculate the derivative of the output ( HMT ) with respect to each input variable using the formulas demonstrated by Takenaga et al [ 9 ] for the case when one hidden layer is present in the neural network . We have extended their work to the case of neural networks with two hidden layers . Since the weights of the network are fixed , each derivative will be a function of the weights and the input values at the given time . The derivative of the output with respect to a given input variable depends on the value of that input variable as well as the values of all the other input variables at that time . This means that the derivative of HMT with respect to an input variable will vary over time . This approach is expected to give us insights into which variables may be more influential in predicting HMT when compared to others .
3 . CONCLUSIONS
With the advent of electronic commerce , the rapid growth of business databases has overwhelmed the traditional , interactive approaches to data analysis and created a need for a new generation of tools for intelligent and automated discovery in data . The paper presented preliminary data from research efforts currently underway in MIT Sloan School of Management in strategic data mining towards that end . Prototype based on these tools was successful in reducing the total level of inventory by 50 % in Medicorp , while maintaining the same level of probability that a particular customer ’s demand will be satisfied .
The paper also highlighted many interesting challenges within the context of providing neural network based data mining tools for Inventory control . In neural network based data mining , the most difficult problems were encountered at the data preparation stage . The problem of too few and relatively irregular timing of data points was addressed in multiple ways . Linearization was used to overcome the erratic frequency with which the input variables were measured . The concept of “ moving windows ” was used to boost the number of data points reduced by the use of linearization . Both methods are good at resolving the problems they are intended for , but distort the way the input parameters are represented in the modeling stage . This is a side effect which one has to cope with , in situations involving missing and/or infrequent data .
In the modeling stage , different neural network algorithms were experimented with , along with different input hidden output node configurations , different randomizing algorithms and different learning rates . The variations on the number of nodes and the different algorithms did not produce very different results , indicating that these factors are not important . Even though in general , Time Delay family of neural networks are more powerful networks in time series analysis because of their ability to capture time dependencies in the data set , in this case , they did not outperform simple Feed Forward neural networks . For both the case studies , time dependencies of the data set were explicitly defined and compensated in the modeling stage .
Based on our experience with many data mining projects , we have the following suggestions to offer for using data mining techniques for electronic commerce related applications :
Have a clearly articulated business problem and then determine whether data mining is the proper solution technology . It might be tempting to use data mining to solve every business problem related to databases but some problems are unsuited to data mining . A question such as "What were my sales from Web customers in Massachusetts last month?" can be best answered with a database query or on line analytical processing tool . Data mining is about answering questions such as : “ What are the characteristics of my most profitable Web customers from Massachusetts ? ” “ How do I optimize my inventory for the next month ? ” In the electronic commerce space , data mining can be used effectively to increase market share by knowing who your most valuable customers are , defining their features and then using that profile to either retain them or target new , similar customers .
Have business division(s ) be intimately involved in the endeavor from the beginning . Data mining is gradually evolving from a technology driven concept to a business solution driven concept . Earlier , information technology consumers were eager to employ data mining technologies without much regard to its incumbent business processes and organizational disciplines . Now business divisions , technology divisions , are spearheading the data mining efforts in major corporations . technology firepower can
Understand and deliver the fundamentals . At the heart of any data mining effort , there must be a business process . No amount of its place . The fundamentals of the business must be incorporated seamlessly in the data mining effort . For example , it may be important to keep in mind that Web customers are different from non Web customers ; therefore , any data mining results derived from analyzing an entire customer base may not be applicable to a webcustomer base . In fact , data mining tools can be used to model the differences in the two types of customer bases thereby creating a more effective experience for the customer .
Have your technology folks be involved too . Software vendors are responding to the technology to business migration by growing emphasis on one button data mining products . Vendors can repackage data mining tools , enhancing their graphical user interface and automating some of their more esoteric aspects . However , it still falls on the analyst to acquire , clean , and feed data to the software ; make dynamic selection of appropriate algorithms ; validate and assimilate the results of the data mining runs ; and generate business rules from the patterns in the data . Most of the operational complexity , time consumption and potential benefits of data mining lie in performing these steps and performing them well . take rather than
[ 10 ] Takenaga , H et al . “ Input Layer Optimization of Neural Networks by Sensitivity Analysis and Its Application to Recognition of Names , ” in Electrical Engineering in Japan . Vol . 111 , No . 4 , 1991 .
[ 11 ] Elvers , B , ed . Ullman's Encyclopedia of Industrial
Chemistry . John Wiley and Sons , New York . 1996 .
[ 12 ] Smith , Murray .
Neural Networks for Statistical
Modeling . Van Nostrand Reinhold , New York . 1993 .
[ 13 ] Chauvin , Y .
“ Generalization Performance of Overtrained Back Propagation Networks," in "Neural Networks , ” Lecture Notes in Computer Science . , pp 46 55 . Springer Verlag , New York . 1990
[ 14 ] Bhattacharjee , D . , Dash SK , Das , AK Application of Artificial Intelligence in Tata Steel . Tata Search , 1999 . [ 15 ] Weigend , AS and Gershenfeld , NA “ Results of the time series prediction competition at the Santa Fe Institute , ” in IEEE International Conference on Neural Networks , pp 1786 1793 . IEEE Press , Piscataway , NJ , 1993 . applications ”
[ 16 ] Reyes , Carlos , Ganguly , A , Lemus , G and Gupta , A . “ A hybrid model based on dynamic programming , neural networks , and surrogate value for inventory optimization the Operational Research Society , Vol . 49 , 1998 , pp . 1 10 . [ 17 ] Bansal , K . , Gupta , A . , Vadhavkar , S . “ Neural Networks Based Forecasting Techniques for Inventory Control Applications , ” in Data Mining and Knowledge Discovery , Vol . 2 , 1998 .
Journal of in
[ 18 ] Gupta , A . , Vadhavkar , S . and Au , S . “ Data Mining for Electronic Commerce , ” in Electronic Commerce Advisor , Volume 4 , Number 2 , September/October 1999 , pp . 24 30 .
[ 19 ] Bansal , K . , Vadhavkar , S . and Gupta , A “ Neural Networks Based Data Mining Applications for Medical Inventory Problems ” , in International Journal of Agile Manufacturing , Volume 1 Issue 2 , 1998 , pp . 187 200 , Urvashi Press , India .
5 . ACKNOWLEDGMENTS
The authors would like to thank various members of the Data Mining research group at the Sloan School of Management for their help in building training data for the ANNs and testing various ANN models . Proactive support from the top management of Medicorp and Steelcorp throughout the research is greatly appreciated . 6 . REFERENCES
[ 1 ] Knoblock , C , ed . “ Neural networks in real world applications , ” in IEEE Expert , August 1996 , pp 4 10 .
[ 2 ] Bhat , N and McAvoy , TJ “ Use of Neural Nets For Dynamic Modeling and Control of Chemical Process Systems , ” in Computers in Chemical Engineering , Vol . 14 , No . 4/5 , pp 573 583 .
[ 3 ] Rumelhart D . and McClelland J . Parallel Distributed the Microstructure of
Processing : Exploration Cognition , Vol.1 , MIT Press 1986 . in
[ 4 ] Pal SK and Mitra S . , “ Multilayer Perceptron , fuzzy sets and classification ” , in IEEE Transactions on Neural Networks , Vol.3 , No.5 , September 1992 .
[ 5 ] Bulsari , A and Saxen , H . “ Classification of blast furnace probe temperatures using neural networks , ” in Steel Research . Vol . 66 . 1995 .
[ 6 ] Biswas AK , Principles of Blast Furnace Ironmaking ,
SBA Publications , 1984 .
[ 7 ] Singh , H and Sridhar , Nallamali and Deo , Brahma . “ Artificial neural nets for prediction of silicon content of blast furnace hot metal , ” in Steel Research , vol . 67 ( 1996 ) . No . 12 techniques
[ 8 ] Osamu , L . and Ushijima , Yuichi and Toshiro , Sawada . “ Application of AI to blast furnace operations , ” in Iron and Steel Engineer , October 1992 . [ 9 ] Bulsari , A and Saxen , Henrik and Saxen Bjorn . “ Timeseries prediction of silicon in pig iron using neural networks , ” in International Conference on Engineering Applications of Neural Networks ( EANN '92 ) .
