Probabilistic Bag Of Hyperlinks Model for Entity Linking
Octavian Eugen Ganea Dept . of Computer Science ETH Zurich , Switzerland ganeao@infethzch
Marina Horlescu
Google Inc . marinah@google.com
Aurelien Lucchi
Dept . of Computer Science
ETH Zurich , Switzerland alucchi@infethzch
Thomas Hofmann
Dept . of Computer Science ETH Zurich , Switzerland thomaho@infethzch
Keywords
Entity linking ; Wikification ; Probabilistic graphical mod els ; Approximate inference ; Loopy belief propagation
1 .
INTRODUCTION
Digital systems are producing increasing amounts of data every day . With daily global volumes of several terabytes of newly authored textual content , there is a growing need for automatic methods for text aggregation , summarization , and , eventually , understanding . Entity linking is a key step towards these goals as it establishes a mapping between surface forms , ie , spans of text , and their encapsulated semantics . In practice , this is achieved by associating a span of text with a canonical representation of an entity such as Wikipedia articles or Freebase entries . Figure 1 illustrates the difficulty of this task when dealing with real world data . The main challenges , arising from word ambiguities inherent to natural language , are surface form synonymy , ie , different spans of text referring to the same entity , and homonymy , ie , the same name being shared by multiple entities .
Carsten Eickhoff
Dept . of Computer Science
ETH Zurich , Switzerland ecarsten@infethzch
ABSTRACT
The goal of entity linking is to map spans of text to canonical entity representations such as Freebase1 entries or Wikipedia2 articles . It provides a foundation for various natural language processing tasks , including text understanding , summarization and machine translation . Name ambiguity , word polysemy , context dependencies , and a heavytailed distribution of entities contribute to the complexity of this problem .
We here propose a probabilistic approach that makes use of an effective graphical model for collective entity linking , which resolves entity links jointly across an entire document . Our model captures local information from linkable token spans ( ie , mentions ) and their surrounding context and combines it with a document level prior of entity co occurrences . The model is acquired automatically from entity linked text repositories with a lightweight computational step for parameter adaptation . Loopy belief propagation is then used as an efficient approximate inference algorithm .
Our method does not require extensive feature engineering but relies on simple sufficient statistics extracted from data , thus making it sufficiently fast for real time usage . We demonstrate its performance on a wide range of well known entity linking benchmark datasets , demonstrating that our approach matches , and in many cases outperforms , existing state of the art methods .
Categories and Subject Descriptors
Information Systems [ Information Extraction ] : Entity
Resolution
1https://wwwfreebasecom/ 2http://enwikipediaorg/
Figure 1 : An entity linking problem showcasing five mentions and their potential entity resolutions .
In this paper , we describe and evaluate a novel method for document level entity linking with Wikipedia ( known as Wikification [ 4] ) . Our starting point is the natural assumption that each entity depends ( i ) on its mention , ( ii ) its surrounding local context , and ( iii ) on other entities that appear in the same document . We capture the first two factors by constructing a likelihood function for a candidate entity , given the referring text span and neighboring context words . We exploit the third factor by acquiring and incorporating prior information on the co occurrence of entities within a document . Finally , the likelihood function and the prior are combined in a probabilistic model from which we collectively infer entities for all input mentions in the same document . In our example in Figure 1 , each highlighted mention constrains the set of possible entity candidates , yet leaves a significant level of ambiguity . However , there is only one collective way of linking that is jointly consistent and supported by contextual cues ( eg , words like “ team ” , “ goal ” ) .
The novel contributions of our work are the following : ( 1 ) We employ rigorous probabilistic semantics for the entity linking problem by introducing a principled light weight probabilistic graphical model . ( 2 ) At the core of our joint probabilistic model , we derive a set of potential functions that proficiently explain statistics of observed training data . ( 3 ) We further show how a robust entity linking procedure can be built upon our model using efficient approximate inference algorithms . ( 4 ) Throughout a range of experiments performed on several standard entity linking datasets using the Gerbil platform[39 ] , we demonstrate competitive or state of the art quality compared to some of the best existing approaches . ( 5 ) Moreover , our training is solely based on publicly available Wikipedia hyperlink statistics and the method does not require extensive feature engineering , making this paper a self contained manual of implementing an entity linking system from scratch .
The remainder of this paper is structured as follows : Section 2 briefly discusses relevant literature on entity linking based on local or global models , graph based and topic modelling methods . Section 3 formally introduces our probabilistic graphical model and details the initialization and learning procedure of all relevant model parameters . Section 4 details the actual inference process that is used for collective mention resolution . Section 5 empirically demonstrates the merits of the proposed method , first on a number of small collections of manually annotated documents , and then on a large scale subset of Wikipedia articles . Finally , in Section 6 , we conclude with a brief summary of our findings and an overview of ongoing and future work .
2 . RELATED WORK
There is a substantial body of existing work dedicated to the task of entity linking with Wikipedia ( Wikification ) . We can identify four major paradigms of how this challenge is approached .
Local models consider the individual context of each entity mention in isolation in order to reduce the size of the decision space . In one of the early entity linking papers , Mihalcea and Csomai [ 24 ] propose an entity disambiguation scheme based on similarity statistics between the mention context and the entity ’s Wikipedia page . Milne and Witten [ 25 ] further refine their scheme with special focus on the mention detection step ( identifying “ linkable ” spans of text ) . Bunescu and Pasca [ 3 ] present a Wikipedia driven approach , making use of manually created resources such as redirect and disambiguation pages . Dredze et al . [ 9 ] cast the entity linking task as a retrieval problem , treating mentions and their context as queries , and ranking candidate entities according to their likelihood of being referred to .
Global models attempt to jointly disambiguate all entity mentions in the document based on the assumption that its entities are correlated and consistent with the main topic . While this approach tends to result in superior accuracy , the space of possible entity assignments grows combinatorially . As a consequence , many approaches in this group rely on approximate inference mechanisms . Cucerzan [ 7 ] uses high dimensional vector space representations of candidate entities and attempts to iteratively choose candidates that optimize the mutual proximity to existing candidates . Kulkarni et al . [ 22 ] exploit topical information about candidate entities and try to harmonize these topics across all assigned entities . Ratinov et al . [ 30 ] prune the list of entity mentions using support vector machines trained on a range of similarity and term overlap features between entity representations . Ferragina and Scaiella [ 13 ] focus on short documents such as tweets or search engine snippets . Based on evidence across all mentions , the authors employ a voting scheme in which the respective candidate entities for each mention are selected . Recently , Cheng et al . [ 6 ] and Singh et al . [ 34 ] described models for jointly capturing the interdependence between the tasks of entity tagging , relation extraction and co reference resolution . Similarly , Durrett and Klein [ 10 ] describe a graphical model for collectively addressing the tasks of named entity recognition , entity disambiguation and co reference resolution . Fahrni and Strube [ 11 ] propose a hybrid inference scheme in which local and/or global contexts are used based on the assumption that depending on the concrete mention at hand , one or the other may be more indicative . Zuo et al . [ 43 ] investigate the use of cheap classifier ensembles for entity linking , finding considerable gains in effectiveness and efficiency as compared to single , more sophisticated classifiers .
Graph based models establish structural relationships between candidate entities and mentions using structural models . For inference , various approaches are employed , ranging from densest graph estimation algorithms ( Hoffart et al . [ 18 ] ) to graph traversal methods such as random graph walks ( Guo and Barbosa [ 14 ] , Han et al . [ 17 ] ) or Page Rank ( Hachey et al . [ 15] ) . In a similar fashion , densest graph estimation and random graph walks can be combined to enhance the quality of both entity linking and word sense disambiguation in a synergistic solution ( Moro et al . [ 26] ) .
The above approaches are limited because they assume a single topic per document . Thus , topic modelling can be used for entity linking by attempting to harmonize the individual distribution of latent topics across candidate entities . Houlsby and Ciaramita [ 19 ] and Pilz and Paaß [ 29 ] rely on Latent Dirichlet Allocation ( LDA ) and compare the resulting topic distribution of the input document to the topic distributions of the disambiguated entities’ Wikipedia pages . Han and Sun [ 16 ] propose a joint model of mention context compatibility and topic coherence , allowing them to simultaneously draw from both local ( terms , mentions ) as well as global ( topic distributions ) information . Kataria et al . [ 21 ] use a semi supervised hierarchical LDA model based on a wide range of features extracted from Wikipedia pages and topic hierarchies .
In this paper , we exclusively focus on the task of entity disambiguation , assuming a reasonable mention detection mechanism to be in place . In contrast to other probabilistic approaches to this problem ( eg [ 2] ) , our method exploits co occurrence statistics in a fully probabilistic manner using a graph based model that addresses collective entity linking across all mentions . It combines a clean and light weight probabilistic model with an elegant , real time inference algorithm . State of art accuracy is achieved without the need for special purpose computational heuristics .
3 . PROBABILISTIC MODEL
In this section , we formally define the entity linking task that we address in this work and describe our modeling approach in detail . 3.1 Problem Definition and Formulation Let E be a knowledge base ( KB ) of entities , V a finite dictionary of phrases or names and C a context representation . Formally , we seek a mapping F : ( V,C)n → E n , that takes as input a sequence of linkable mentions m = ( m1 , . . . , mn ) along with their contexts c = ( c1 , . . . , cn ) and produces a joint entity assignment e = ( e1 , . . . , en ) . Here n refers to the number of linkable spans in a document . Our problem is known as entity disambiguation or link generation in the literature . 3 We can construct such a mapping F in a probabilistic approach , by learning a conditional probability model p(e|m , c ) from data and then employing ( approximate ) probabilistic inference in order to find the maximum a posteriori ( MAP ) assignment , hence :
F ( m , c ) := arg max e∈En p(e|m , c ) .
( 1 )
In the sequel , we describe how to estimate such a model from a corpus of entity linked documents . Finally , we show in Section 4 how to apply belief propagation for approximate inference in this model . 3.2 Maximum Entropy Models
Assume a corpus of entity linked documents is available . Specifically , we have used the set of Wikipedia pages together with their respective Wiki hyperlinks4 . One can extract two kinds of basic statistics from such a corpus : First , counts of how often each entity was referred to by a specific name . Second , pairwise co occurrence counts for entities in documents . Our fundamental conjecture is that most of the relevant information about entity linking is contained in these counts , that they are sufficient statistics . We thus take these statistics and request that our probability model reproduces these counts in expectation . As this alone typically yields an ill defined problem , we follow the maximum entropy principle of Jaynes [ 20 ] : Among the feasible set of distributions we favor the one with maximal entropy . Formally , let D be an entity linked document collection . Ignoring mention contexts for now , we extract for each document d ∈ D a sequence of mentions m(d ) and their corresponding target entities e(d ) , both of length n(d ) . Assuming exchangeability of random variables within these sequences , we reduce each ( e , m ) to statistics ( or features )
3Note that we do not address the issues of mention detection or nil identification in this work . Rather , our input is a document along with a fixed set of linkable mentions corresponding to existing KB entities .
4Hyperlinks are considered ground truth annotations , where the mention is the linked span of text and the truth entity is the Wikipedia page it refers to . n i=1 about mention entity and entity entity co occurrence as follows :
φe,m(e , m ) :=
ψ{e,e}(e ) :=
1[ei = e]·1[mi = m ] , ∀(e , m)∈E×V ( 2 )
1[{ei , ej} = {e , e
} ] , ∀e , e
∈ E ,
( 3 ) i<j where 1[· ] is the indicator function . Note that we use the subscript notation {e , e} for ψ to take into account the symmetry in e , e as well the fact that one may have e = e .
The document collection provides us with empirical esti mates for the expectation of these statistics under an iid sampling model for documents , namely the averages
φe,m(D ) :=
ψ{e,e}(D ) :=
1 |D| 1 |D|
φe,m(e(d ) , m(d ) ) ,
ψ{e,e}(e(d ) ) .
( 4 )
( 5 ) d∈D d∈D
Note that in entity linking , the mention sequence m is always considered given , while we seek to predict the corresponding entity sequence e . It is thus not necessary to try to model the joint distribution p(e , m ) , but sufficient to construct a conditional model p(e|m ) . Following Berger et al . [ 1 ] this can be accomplished by taking the empirical distribution p(m|D ) of mention sequences and combining it with a conditional model via p(e , m ) = p(e|m)·p(m|D ) . We then require that : Ep[φe,m ] = φe,m(D ) and Ep[ψ{e,e} ] = ψ{e,e}(D ) ,
+|E| moment constraints on p(e|m ) . which yields |E|·|V|+,|E|
( 6 )
2
The maximum entropy distributions , fulfilling constraints as stated in Eq ( 6 ) form a conditional exponential family for which φ(· , m ) and ψ are sufficient statistics . We thus know that there are canonical parameters ρe,m and λ{e,e} ( formally corresponding to Lagrange multipliers ) such that the maximum entropy distribution can be written as exp [ ρ , φ(e , m ) + λ , ψ(e ) ] p(e|m ; ρ , λ ) =
( 7 )
1 where Z(m ) is the partition function
Z(m ) := exp [ ρ , φ(e , m ) + λ , ψ(e ) ] .
( 8 )
Z(m ) e∈En
Here we interpret ( e , m ) and {e , e} as multi indices and suggestively define the shorthands
ρ , φ :=
ρe,mφe,m , λ , ψ :=
λ{e,e}ψ{e,e} .
( 9 )
{e,e} e,m
Note that we can switch between the statistics view and the raw data view by observing that
ρ , φ(e , m ) =
ρei,mi ,
λ , ψ(e ) =
λ{ei,ej} .
( 10 ) i=1 i<j
While the maximum entropy principle applied to our fundamental conjecture restricts the form of our model to a finitedimensional exponential family , we need to investigate ways of finding the optimal or – as we will see – an approximately optimal distribution in this family . To that extent , we first re interpret the obtained model as a factor graph model . n m4
E4
E3 m3 m1
E1
E2 m2
Figure 2 : Proposed factor graph for a document with four mentions . Each mention node mi is paired with its corresponding entity node Ei , while all entity nodes are connected through entity entity pair factors .
3.3 Markov Network and Factor Graph
Complementary to the maximum entropy estimation perspective , we want to present a view on our model in terms of probabilistic graphical models and factor graphs . Inspecting Eq ( 7 ) and interpreting φ and ψ as potential functions , we can recover a Markov network that makes conditional independence assumptions of the following type : an entity link ei and a mention mj with i = j are independent , given mi and e−i , where e−i denotes the set of entity variables in the document excluding ei . This means that mentions mj only influence a variable ei through the intermediate variables ej . However , the functional form in Eq ( 7 ) goes beyond these conditional independences in that it limits the order of interaction among the variables . A variable ei interacts with neighbors in its Markov blanket through pairwise potentials . In terms of a factor graph decomposition , p(e|m ) decomposes into functions of two arguments only , modeling pairwise interactions between entities on one hand and between entities and their corresponding mentions on the other hand .
We emphasize the factor model view by rewriting ( 7 ) as p(e|m ; ρ , λ ) ∝ exp [ ρei,mi ] · exp.λ{ei,ej}fi
( 11 ) i i<j where we think of ρ and λ as functions
ρ : E × V → R , λ : E ∪ E 2 → R ,
( e , m ) → ρe,m {e , e
} → λ{e,e}
An example of a factor graph ( n = 4 ) is shown in Figure 2 . We will investigate in the sequel , how the factor graph structure can be further exploited . 3.4
( Pseudo–)Likelihood Maximization
While the maximum entropy approach directly motivates the exponential form of Eq ( 7 ) and is amenable to a plausible factor graph interpretation , it does not by itself suggest an efficient parameter fitting algorithm . As is known by convex duality , the optimal parameters can be obtained by maximizing the conditional likelihood of the model under the data ,
L(ρ , λ;D ) = log p(e(d)|m(d ) ; ρ , λ )
( 12 ) d putation of gradients of L , which involves evaluating expectations with regard to the model , since ∇ρ log Z(m ) = Epφ(e , m ) , ∇λ log Z(m ) = Epψ(e ) . ( 13 ) The exact inference problem of computing these model expectations , however , is not generally tractable due to the pairwise couplings through the ψ statistics .
As an alternative to maximizing the likelihood in Eq ( 12 ) , we have investigated an approximation known as , pseudolikelihood maximization [ 37 , 40 ] . Its main benefits are low computational complexity , simplicity and practical success . Switching to the Markov network view , the pseudo likelihood estimator predicts each variable conditioned on the value of all variables in its Markov blanket . The latter consists of the minimal set of variables that renders a variable conditionally independent of everything else . In our case the Markov blanket consists of all variables that share a factor with a given variable . Consequently , the Markov blanket of ei is N ( ei ) := ( mi , e−i ) . The posterior is then approximated in the pseudo likelihood approach as :
˜p(e|m ; ρ , λ ) := p(ei|N ( ei ) ; ρ , λ ) ,
( 14 ) n i=1 n(d ) d∈D i=1 which results in the tractable log likelihood function
˜L(ρ , λ;D ) := log p(e(d ) i
|N ( e(d ) i
) ; ρ , λ ) .
( 15 )
Introducing additional L2 norm penalties γ(λ2
2+ρ2 2 ) to further regularize ˜L , we have utilized parallel stochastic gradient descent ( SGD ) [ 31 ] with sparse updates to learn parameters ρ , λ . From a practical perspective , we only keep for each token span m parameters ρe,m for the most frequently observed entities e . Moreover , we only use λ{e,e} for entity pairs ( e , e ) that co occurred together a sufficient number of times in the collection D.5 As we will discuss in more detail in Section 5 , our experimental findings suggest this brute force learning approach to be somewhat ineffective , which has motivated us to develop simpler , yet more effective plug in estimators as described below . 3.5 Bethe Approximation
The major computational difficulty with our model lies in the pairwise couplings between entities and the fact that these couplings are dense : The Markov dependency graph between different entity links in a document is always a complete graph . Let us consider what would happen , if the dependency structure were loop free , ie , it would form a tree . Then we could rewrite the prior probability in terms of marginal distributions in the so called Bethe form . Encoding the tree structure in a symmetric relation T , we would get n {i,j}∈T p(ei , ej ) i=1 p(ei)di−1 , p(e ) = di := |{j : {i , j} ∈ T }| .
( 16 )
The Bethe approximation [ 41 ] pursues the idea of using the above representation as an unnormalized approximation for
However , specialized algorithms for maximum entropy estimation such as generalized iterative scaling [ 8 ] are known to be slow , whereas gradient based methods require the com
5For the Wikipedia collection , even after these pruning steps , we ended up with more than 50 million parameters in total . p(e ) , even when the Markov network has cycles . How does this relate to the exponential form in Eq ( 7 ) ? By simple pattern matching , we see that if we choose ∀e , e
λ{e,e} = log
∈ E
( 17 )
, we can apply Eq ( 16 ) to get an approximate distribution p(e ) p(e ) p(e , e ) n i=1 n i<j p(ei , ej ) i=1 p(ei)n−2 = i<j p(ei )
¯p(e ) ∝ p(ei , ej ) p(ei ) p(ej )
( 18 )
( 19 )
= exp log p(ei ) +
λ{ei,ej}
, i i<j where we see the same exponential form in λ appear as in Eq ( 10 ) . We complete this argument by observing that with
ρe,m = log p(e ) + log p(m|e )
( 20 ) we obtain a representation of a joint distribution that exactly matches the form in Eq ( 7 ) .
What have we gained so far ? We started from the assumption of constructing a model , which would agree with the observed data on the co occurrence probabilities of token spans and their linked entities as well as on the co link probability of entity pairs within a document . This has led to the conditional exponential family in Eq ( 7 ) . We have then proposed pseudo likelihood maximization as a way to arrive at a tractable learning algorithm to try to fit the massive amount of parameters ρ and λ . Alternatively , we have now seen that a Bethe approximation of the joint prior p(e ) yields a conditional distribution p(e|m ) that ( i ) is a member of the same exponential family , ( ii ) has explicit formulas for how to choose the parameters from pairwise marginals , and ( iii ) would be exact in the case of a dependency tree . We claim that the benefits of computational simplicity together with the correctness guarantee for non dense dependency networks outweighs the approximation loss , relative to the model with the best generalization performance within the conditional exponential family . In order to close the suboptimality gap further , we suggest some important refinements below . 3.6 Parameter Calibration
With the previous suggestion , one issue comes into play : The total contribution coming from the pairwise interac tions between entities will scale with,n
, while the entity– mention compatibility contributions will scale with n , the total number of mentions . This is a direct observation of the number of terms contributing to the sums in ( 10 ) . However , for practical reasons , it is somewhat implausible that , as n grows , the prior p(e ) should dominate and the contribution of the likelihood term should vanish . The model is not well calibrated with regard to n .
2
We propose to correct for this effect by adding a normal ization factor to the λ parameters by replacing ( 17 ) with :
λn e,e =
2 n − 1 log
∀e , e
∈ E
,
( 21 ) p(e , e ) p(e ) · p(e ) where now these parameters scale inversely with n , the number of entity links in a document , making the corresponding sum in Eq ( 7 ) scale with n . With this simple change , a substantial accuracy improvement was observed empirically , the details of which are reported in our experiments .
The re calibration in Eq ( 17 ) can also be justified by the following combinatorial argument : For a given set Y of random variables , define an Y cycle as a graph containing as nodes all variables in Y , each with degree exactly 2 , connected in a single cycle . Let Ξ be the set enumerating all possible Y cycles . Then , |Ξ|= ( n − 1)! , where n is the size of Y .
In our case , if the entity variables e per document would have formed a cycle of length n instead of a complete subgraph , the Bethe approximation would have been written as :
¯pπ(e ) ∝
( i,j)∈E(π ) p(ei , ej )
∀π ∈ Ξ
,
( 22 ) i p(ei ) where E(π ) is the set of edges of the e cycle π . However , as we do not desire to further constrain our graph with additional independence assumptions , we propose to approximate the joint prior p(e ) by the average of the Bethe approximation of all possible π , that is log ¯pπ(e ) .
( 23 )
Since each pair ( ei , ej ) would appear in exactly 2(n − 2)! e cycles , one can derive the final approximation : log ¯p(e ) ≈ 1 |Ξ|
π∈Ξ i<j p(ei , ej ) i p(ei )
¯p(e ) ≈
2 n−1
.
( 24 )
Distributing marginal probabilities over the parameters starting from Eq ( 24 ) results in Eq ( 21 ) . While the above line of argument is not a strict mathematical derivation , we believe this to shed further light on the empirically observed effectiveness of the parameter re scaling . 3.7
Integrating Context
The model that we have discussed so far does not consider the local context of a mention . This is a powerful source of information that a competitive entity linking system should utilize . For example , words like “ computer ” , “ company ” or “ device ” are more likely to appear near references of the entity Apple Inc . than of the fruit apple . We demonstrate in this section how this integration can be easily done in a principled way on top of the current probabilistic model , showcasing the extensibility of our approach . Enhancing our model with additional knowledge such as entity categories or word co reference can also be done in a rigorous way , so we hope that this provides a template for future extensions . As stated in Section 3.1 , for each mention mi in a document , we maintain a context representation ci consisting of the bag of words surrounding the mention within a window of length K 6 . Hence , ci can be viewed as an additional random variable with an observed outcome . We make additional conditional independence assumptions that seem reasonable and increase tractability of the model . Most importantly , we assume that the context words ci around a mention are independent of everything else , including the
6Throughout our experiments , we used a context window of size K = 100 , intuitively chosen and without extensive validation . mention token span mi , given the identity of the linked entity ei . This translates into a factorial form for the model p(e , m , c ) = p(e)p(m , c|e ) = p(e ) p(mi|ei)p(ci|ei ) ( 25 ) n i=1 and a simple extension of the previous factor graphs by factors connecting all pairs ( ci , ei ) . As is common for bag ofword representations , we also assume that context probabilities factorize further by assuming – as in na¨ıve Bayes – conditional independence of words in ci , given ei . p(ci|ei ) = p(wj|ei ) .
( 26 ) wj∈ci
While this completes the argument from a joint model point of view , we need to consider one more aspect for the conditional model p(e|m , c ) that is our main interest . If we cannot afford ( computationally as well as with regard to training data size ) a full blown discriminative learning approach , then how do we balance the relative influence of the context ci and the mention token span mi on ei ? For instance , the influence of ci will depend on the chosen window width K , which seems undesirable in a conditional model .
To address this issue , we resort to a hybrid approach , where , in the spirit of the Bethe approximation , we continue to express our model in terms of simple marginal distributions that we can easily estimate independently from data , yet that allow for a small number of parameters ( in our case “ small ” equals 2 ) to be chosen to optimize the conditional log likelihood p(e|m , c ) . We thus introduce weights ζ and τ that control the importance of the context factors and , respectively , of the entity entity interaction factors in the final model . We arrive at n i=1
log p(ei|mi ) + ζ p(ei , ej ) wj∈ci
+
2τ n − 1 log i<j p(ei ) p(ej ) log p(wj|ei )
+const .
 log p(e|m , c ) =
( 27 ) Note that here we have made use of the identity p(m|e)p(e ) = p(e|m)p(m ) and absorbed all log p(m ) terms in the constant . We use grid search on a validation set for the remaining problem of optimizing over the parameters ζ , τ . Details are provided in section 5 .
The above model will be subsequently referred to as the
PBoH model ( Probabilistic Bag of Hyperlinks ) . 3.8 Smoothing Empirical Probabilities
In order to estimate the probabilities involved in Eq ( 27 ) , we rely on an entity annotated corpus of text documents , eg , Wikipedia Web pages together with their hyperlinks which we view as ground truth annotations . From this corpus , we derive empirical probabilities for a name to entity dictionary ˆp(m|e ) based on counting how many times an entity appeared referenced by a given name7 . Additionaly , we also compute the pairwise probabilities ˆp(e , e ) obtained by counting the pairwise co occurrence of entities within the
7In our implementation we combined the mention entity counts from Wikipedia hyperlinks with the Crosswikis counts [ 35 ] same document , as well as the marginals ˆp(e ) = and the word entity statistics ˆp(w|e ) . e ˆp(e , e )
In the absence of huge amounts of data , estimating such probabilities from counts is subject to sparsity . For instance , in our statistics , there are 8 times more distinct pairs of entities that co occur in at most 3 Wikipedia documents compared to the total number of distinct pairs of entities that appear together in at least 4 documents . Thus , it is expected that the heavy tail of infrequent pairs of entities will have a strong impact on the accuracy of our entity linking system . Traditionally , various smoothing techniques are employed to address sparsity issues arising commonly in areas such as natural language processing . Out of the wealth of methods , we decided to use the absolute discounting smoothing technique [ 42 ] that involves interpolation of higher and lower order ( backoff ) models . In our case , whenever insufficient data is available for a pair of entities ( e , e ) , we assume the two entities are drawn from independent distributions . Thus , if we denote by N ( e , e ) the total number of corpus documents that link both e and e , and by Nep the total number of pairs of entities referenced in each document , then the final formula for the smoothed entity pairwise probabilities max(N ( e , e ) − δ , 0 )
) = is : p(e , e assures that
+ ( 1 − µe)ˆp(e)ˆp(e e
)
Nep
( 28 ) where δ ∈ [ 0 , 1 ] is a fixed discount and µe is a constant that ep(e , e ) = 1 . δ was set by performing a coarse grid search on a validation set . The best δ value was found to be 05 The word entity empirical probabilities ˆp(w|e ) were computed based on the Wikipedia corpus by counting the frequency with which word w appears in the context windows of size K around the hyperlinks pointing to e . In order to avoid memory explosion , we only considered the entitywords pairs for which these counts are at least 3 . These empirical estimates are also sparse , so we used absolute discounting smoothing for their correction by backing off to the unbiased estimates ˆp(w ) . The latter can be much more accurately estimated from any text corpus . Finally , we obtain : p(w|e ) = max(N ( w , e ) − ξ , 0 )
Nwp
+ ( 1 − µw)ˆp(w ) .
( 29 )
Again ξ ∈ [ 0 , 1 ] was optimized by grid search to be 05
4 .
INFERENCE
After introducing our model and showing how to train it in the previous section , we now explain the inference process used in our prediction algorithm . 4.1 Candidate Selection
At test time , for each mention to be disambiguated , we first select a set of potential candidates by considering the top R ranked entities based on the local mention entity probability dictionary ˆp(e|m ) . We found R = 64 to be a good compromise between efficiency and accuracy loss . Second , we want to keep the average number of candidates per mention as small as possible in order to reduce the running time which is quadratic in this number ( see the next section for details ) . Consequently , we further limit the number of candidates per mention by keeping only the top 10 entity candidates ranked by the local mention context entity compat ibility defined as log p(ei|mi , ci ) = log p(ei|mi ) + ζ wj∈ci log p(wj|ei ) +const .
( 30 )
These pruning heuristics result in a significantly improved run time at an insignificant accuracy loss . If the given mention is not found in our map ˆp(e|m ) , we try to replace it by the closest name in this dictionary . Such a name is picked only if the Jaccard distance between the set of letter trigrams of these two strings is smaller than a threshold that we empirically picked as 05 Otherwise , the mention is not linked at all . 4.2 Belief Propagation
Collectively disambiguating all mentions in a text involves iterating through an exponential number of possible entity resolutions . Exact inference in general graphical models is NP hard , therefore approximations are employed . We propose solving the inference problem through the loopy belief propagation ( LBP ) [ 27 ] technique , using the max product algorithm that approximates the MAP solution in a runtime polynomial in n . For the sake of brevity , we only present the algorithm for the maximum entropy model described by Eq ( 7 ) ; A similar approach was used for the enhanced model of Eq ( 27 ) .
Our proposed graphical model is a fully connected graph where each node corresponds to an entity random variable . Unary potentials exp(ρm,e ) model the entity mention compatibility , while pairwise potentials exp(λ{e,e} ) express entity entity correlations . Through the modelled posterior in Eq ( 7 ) , one can derive the update equation of the logarithmic message that is sent in round t + 1 from entity random variable Ei to the outcome ej of the entity random variable Ej : mt+1
Ei→Ej
( ej ) = max ei
ρei,mi +λ{ei,ej} +
1≤k≤n;k=j
( 31 )
 mt
Ek→Ei ( ei )
Note that , for simplicity , we skip the factor graph framework and send messages directly between each pair of entity variables . This is equivalent to the original BP framework . We chose to update messages synchronously : in each round t , each pair of entity nodes ( Ei , Ej ) exchanges messages . This is done until convergence or an allowed maximum number ( 15 in our experiments ) of iterations is reached . The convergence criterion is :
1≤i,j≤n;ej∈E|mt+1 max
Ei→Ej
( ej ) − mt
Ei→Ej ( ej)|≤
( 32 ) where = 10−5 . This setting was sufficient in most of the cases to reach convergence .
In the end , the final entity assignment is determined by :
ρei,mi +
1≤k≤n
∗ e i = arg max ei
 mt
Ek→Ei ( ei )
( 33 )
The complexity of the belief propagation algorithm is , in our case , O(n2 · r2 ) , with n being the number of mentions in a document and r being the average number of candidate entities per mention . While the maximum allowed number
Dataset
# non NIL mentions # documents
AIDA test A AIDA test B
MSNBC
AQUAINT
ACE04
4791 4485 656 727 257
216 231 20 50 35
Table 1 : Statistics on some of the used datasets of candidates per mention was initially chosen to be 64 , we eventually limited it to 10 as previously described . This brought an average 6 fold speed up of the inference procedure without significant accuracy loss . More details regarding the runtime and convergence of the loopy BP algorithm can be found in Section 5 .
5 . EXPERIMENTS
We now present the experimental evaluation of our method . We first uncover some practical details of our approach . Further , we show an empirical comparison between PBoH and well known or recent competitive entity linking systems . We use the Gerbil testing platform [ 39 ] with the D2KB setting in which a document together with a fixed set of mentions to be annotated are given as input . We run additional experiments that allow us to compare against more performant recent approaches , such as [ 19 ] and [ 14 ] .
Note that in this work we do not perform any mention detection step . In all the experiments we assume that an input set of linkable token spans for a document is given , and our main goal is to annotate most8 of them with Wikipedia entities .
Evaluation metrics . We quantify the quality of an entity linking system by measuring common metrics such as precision , recall and F1 scores . Let M∗ be the ground truth annotations made for a given set of mentions X , and let M be the output annotations of an entity linking system on the same input . Then our quality metrics are computed as :
|M∩M∗|
• Precision : P = • Recall : R = • F1 score : F1 = 2·P·R
|M| |M∩M∗|
|M∗|
P +R
We mostly report results in terms of F1 scores , namely macro averaged F1@MA ( aggregated accross documents ) , and micro averaged F1@MI ( aggregated accross mentions ) . For a fair comparison with Houlsby and Ciaramita [ 19 ] , we also report micro recall R@MI and macro recall R@MA on the AIDA test datasets .
In the formulas above , two annotations are considered equal if their mention strings and their annotated entities are the same . Note that the precision and recall are not necessarily identical since a method may not consider a particular candidate for certain mentions 8 .
Pseudo likelihood training . We briefly mention some of the practical issues that we encounter with the likelihood
8In PBoH , we refrain from annotating mentions m that do not appear in the p(e|m ) dictionary . e t e l p m o C L L N o C / A D A
I
60.27 56.97 78.00 73.81 58.84 60.59 48.46 45.29 46.6 42.86 73.39 73.26 54.62 52.35 72.07 71.19 83.82 83.59 64.72 66.17
86.72 86.85
A t s e T L L N o C / A D A
I
59.06 53.36 75.77 71.26 54.90 54.11 45.44 42.17 44.13 42.36 70.9 67.91 52.85 49.6 69.07 66.5 81.82 80.25 61.65 61.67
86.63 85.48
B t s e T L L N o C / A D A
I
58.32 58.03 80.36 74.52 57.69 61.34 48.59 46.20 44.02 41.31 72.64 73.31 52.59 51.34 70.62 70.38 84.34 84.12 60.71 63.19
87.39 86.32 g n i n i a r T L L N o C / A D A
I
61.05 57.53 78.01 74.22 60.04 62.23 49.25 45.85 47.83 43.36 74.22 74.47 55.55 53.23 73.2 72.45 84.21 84.22 66.48 67.93
86.59 87.30
4 0 0 2 E C A
65.83 77.63 63.20 76.71 70.38 80.02 18.72 16.97 12.74 12.3 80.08 87.57 54.89 72.22 81.93 89.09 80.0 86.49 77.14 86.36
87.19 90.40 t h g i l t o p S a i d e p B D
36.61 33.25 51.05 51.97 69.27 67.23 26.70 22.75 22.59 18.0 73.63 76.60 46.8 45.59 63.31 65.1 65.18 68.24 62.57 61.43
79.48 80.13
I
T N A U Q A
60.10 58.62 72.27 73.23 74.03 73.13 38.28 38.15 21.67 19.59 81.84 81.27 49.68 46.06 76.27 75.12 76.82 77.64 75.96 74.63
86.64 86.14 t s e T 4 1 0 2 s t s o p o r c i
M
42.43 61.08 47.20 62.11 56.43 71.63 31.27 44.02 29.12 39.53 63.4 76.54 38.65 57.91 56.81 71.66 59.56 73.89 54.88 69.29
74.19 84.48 n i a r T 4 1 0 2 s t s o p o r c i
M
50.39 62.87 50.60 61.02 56.26 67.99 35.21 42.07 32.69 38.41 64.67 74.32 39.83 53.74 59.14 70.45 61.96 72.65 55.93 67.0
73.08 81.25
8 2 1 s r e t u e R 3 N
67.95 75.52 58.61 59.87 56.44 58.77 32.74 31.85 28.4 24.84 63.2 64.45 54.96 62.9 59.32 67.55 64.38 65.81 60.05 66.51
76.54 83.31
0 0 5 S S R 3 N
59.88 70.80 69.17 76.00 57.63 65.03 31.11 33.55 21.77 22.2 69.29 75.93 61.22 67.3 78.05 83.2 68.21 76.0 64.54 72.23
71.24 78.33
C B N S M
75.42 73.82 78.17 75.73 69.27 69.82 36.86 39.42 41.24 40.3 85.49 87.4 64.03 67.28 75.96 77.05 77.72 79.08 64.25 64.68
89.54 89.62
0 5 E R O K
34.16 30.20 73.12 69.77 37.59 32.90 17.20 12.54 27.97 25.2 57.95 53.17 29.96 24.75 57.34 54.67 58.99 53.13 41.63 35.0
61.70 55.83
B T I I
41.23 43.38 57.13 55.36 65.44 62.81 28.53 28.48 18.46 19.54 72.03 70.52 51.08 49.91 57.23 55.8 61.14 59.36 58.59 56.98
62.47 61.04
F1@MI F1@MA
AGDISTIS
Babelfy
DBpedia Spotlight
Dexter
Entityclassifier.eu
Kea
NERD ML
TagMe 2
WAT
Wikipedia Miner
PBoH
Table 2 : Micro and macro F1 scores reported by Gerbil for 14 datasets and 11 entity linking systems including PBoH . For each dataset and each metric , we highlight in red the best system and in blue the second best system .
Systems LocalMention TagMe reimpl . AIDA S & Y’13 Houlsby’14 PBoH
Datasets
AIDA test A
AIDA test B
R@MI R@MA R@MI R@MA
69.73 76.89 79.29
79.65 85.70
69.30 74.57 77.00 84.22 76.61 85.26
67.98 78.64 82.54
84.89 87.61
72.75 78.21 81.66
83.51 86.44
Table 3 : AIDA test a and AIDA test b datasets results . maximization described in Section 34 From the practical perspective , for each mention m , we only considered the set of parameters ρm,e limited to the top 64 candidate entities e per mention , ranked by ˆp(e|m ) . Additionally , we restricted the set λe,e to entity pairs ( e , e ) that co occurred together in at least 7 documents throughout the Wikipedia corpus . In total , a set of 26 millions ρ and 39 millions λ parameters were learned using the above procedure and tested on a sample test set . Note that the universe of all Wikipedia entities is of size ∼ 4 million . For the SGD procedure , we tried different initializations of these parameters , including ρm,e = log p(e|m ) , λe,e = 0 , as well as the parameters given by Eq ( 17 ) . However , in all cases , the accuracy gain on a sample of 1000 Wikipedia test pages was small or negligible compared to the LocalMention baseline . One reason for this is the inherent sparsity of the data : the parameters associated with the long tail of infrequent entity pairs are updated rarely and expected to be defective at the end of the SGD procedure . However , these scattered pairs are crucial for the effectiveness and coverage of the entity linking system . To overcome this problem , we refined our model as described in Section 3.5 and subsequent sections .
PBoH training details . Wikipedia itself is a valuable resource for entity linking since each internal hyperlink can be considered as the ground truth annotation for the respective anchor text . In our system , the training is solely done on a subset of Wikipedia documents . Hyper parameters are grid searched such that the micro plus macro F1 scores are maximized over the combined held out set containing only the AIDA Test A dataset and a Wikipedia validation set of 1000 Web pages . As a preprocessing step in our training and testing procedure , we removed all annotations and hyperlinks that point to non existing , disambiguation or list Wikipedia pages .
The PBoH system used in the experimental comparison is the model given by Eq ( 27 ) for which grid search of the hyper parameters suggested ζ = 0.075 , τ = 0.5 , δ = 0.5 , ξ = 05
Datasets . In our experiments , we evaluate on 14 well known public entity linking datasets built from various sources . Statistics of some of them are shown in Table 1 , and their descriptions are provided below . For information on the other
Systems
F1@MI F1@MA
F1@MI F1@MA
F1@MI F1@MA new MSNBC
Datasets new AQUAINT new ACE2004
LocalMention
Cucerzan M & W Han’11 AIDA GLOW RI’13 REL RW’14
PBoH
73.64
88.34 78.43 88.46 78.81 75.37 90.22 91.37
91.06
77.71
87.76 80.37 87.93 76.26 77.33 90.87 91.73
91.19
87.33
78.67 85.13 79.46 56.47 83.14 87.72 90.74
89.27
86.80
78.22 84.84 78.80 56.46 82.97 87.74 90.58
88.94
84.75
79.30 81.29 73.48 80.49 81.91 86.60 87.68
88.71
85.70
78.22 84.25 66.80 84.13 83.18 87.13 89.23
88.46
Table 4 : Results on the newer versions of the MSNBC , AQUAINT and ACE04 datasets . datasets used only in the Gerbil experiments , refer to [ 39 ] . • The CoNLL AIDA dataset is an entity annotated corpus of Reuters news documents introduced by Hoffart et al . [ 18 ] . It is much larger than most of the other existing EL datasets , making it an excellent evaluation target . The data is divided in three parts : Train ( not used in our current setting for training , but only in the Gerbil evaluation ) , Test a ( used for validation ) and Test b ( used for blind evaluation ) . Similar to Houlsby and Ciaramita [ 19 ] and others , we report results also on the validation set test a .
• The AQUAINT dataset introduced by Milne and Witten [ 25 ] contains documents from a news corpus from the Xinhua News Service , the New York Times and the Associated Press .
• MSNBC [ 7 ] a dataset of news documents that includes many mentions which do not easily map to Wikipedia titles because of their rare surface forms or distinctive lexicalization .
• The ACE04 dataset [ 30 ] is a subset of ACE2004 coreference proceedings annotated by Amazon Mechanical Turkers in a similar standard as AQUAINT . Note that the ACE04 dataset contains mentions that are annotated with NIL entities , meaning that no proper Wikipedia entity was found . Following common practice , we removed all the mentions corresponding to these NIL entities prior to our evaluation .
Note that the Gerbil platform uses an old version of the AQUAINT , MSNBC and ACE04 datasets that contain some no longer existing Wikipedia entities . A new cleaned version of these sets9 was released by Guo & Barbosa [ 14 ] . We report results on both versions in Table 2 and Table 4 .
Systems . For comparison , we selected a broad range of competitor systems from the vast literature in this field . The Gerbil platform already integrates the methods of Agdistis [ 38 ] , Babelfy [ 26 ] , DBpedia Spotlight [ 23 ] , Dexter [ 5 ] , Kea [ 36 ] , Nerd ML [ 32 ] , Tagme2 [ 12 ] , WAT [ 28 ] , Wikipedia Miner [ 25 ] and Illinois Wikifier [ 30 ] . We furthermore compare against Cucerzan [ 7 ] – the first collective EL system that uses optimization techniques , M& W [ 25]– a popular machine learning approach , Han’11 [ 17 ] – a graph based disambiguation system that uses a random walk model to
Datasets
AIDA test A
AIDA test B
MSNBC
AQUAINT
ACE04
22.18
19.41
32.8
14.54
7.34
100 % 99.56 % 100 % 445.56 371.65
203.66
100 % 40.42
100 % 10.88
2.86
2.83
3.0
2.56
2.25
Avg . num mentions per doc Conv . rate runAvg . ning time ( ms/doc ) Avg . num . rounds
Table 5 : Loopy Belief Propagation statistics . Average running time , number of rounds and convergence rate of our inference procedure are provided . jointly disambiguate mentions , AIDA [ 18 ] – a performant graph based approach , GLOW [ 30 ] – a system that uses local and global context to perform joint entity disambiguation , RI [ 6 ] – a recent EL approach using relational inference for mention disambiguation , and REL RW [ 14 ] , the very recent system that iteratively solves mentions relying on an online updating random walk model . In addition , on the AIDA datasets we also compare against S& Y [ 33 ] – an apparatus for combining the NER and EL tasks , and Houlsby [ 19 ] – a topic modelling LDA based approach for EL .
In addition , in order to empirically assess the accuracy gain introduced by each incremental step of our approach , we ran experiments on several of our method ’s components , individually : LocalMention – links mentions to entities solely based on the token span statistics e∗ = arg maxe ˆp(e|m ) ; Unnorm – the unnormalized mention entity model described in Section 3.5 ; Rescaled – the rescaled model presented in Section 3.6 ; LocalContext – disambiguates an entity based on the mention and the local context probability given by Equation ( 30 ) , e∗ = arg maxe p(e|m , c ) . Note that Unnorm , Rescaled and PBoH use the loopy belief propagation procedure for inference . 5.1 Results
Results of the experiments run on the Gerbil platform are shown in Table 2 10 . We obtain the highest performance on 11 datasets and the second highest performance on 2 datasets , showing the effectiveness of our method .
Other results are presented in Table 3 and Table 4 . The highest accuracy for the cleaned version of AQUAINT , MSNBC
9http://wwwcsualbertaca/~denilson/data/ deos14_ualberta_experiments.tgz
10The full Gerbil experiment is available at http:// gerbilaksworg/gerbil/experiment?id=201510160025
Avg # mentions per doc Systems PBoH REL RW
Datasets
MSNBC
AQUAINT ACE2004
36.95
14.54
8.68
# entities # entities # entities
247.19
382773.6
95.38
66.66
242443.1
256235.49
Table 6 : Average number of entities that appear in the graph built by PBoH and by REL RW
Systems LocalMention Unnorm Rescaled LocalContext PBoH
Datasets
AIDA test A
AIDA test B
R@MI R@MA R@MI R@MA
69.73 69.77 75.09 82.50 85.53
69.30 69.95 74.25 81.56 85.09
67.98 75.87 74.76 85.46 87.51
72.75 75.12 78.28 84.08 86.39
Table 7 : Accuracy gains of individual PBoH components . and ACE04 was previously reported by Guo & Barbosa [ 14 ] , while Houlsby et al . [ 19 ] dominate the AIDA datasets . Note that the performance of the baseline systems shown in these two tables is taken from [ 14 ] and [ 19 ] .
All these methods are tested in the setting where a fixed set of mentions is given as input , without requiring the mention detection step .
Discussion . Several observations are worth noting here . First , the simple LocalMention component alone outperforms many EL systems . However , our experimental results show that PBoH consistently beats LocalMention on all the datasets . Second , PBoH produces state of the art results on both development ( test a ) and blind evaluation ( test b ) of the AIDA dataset . Third , on the AQUAINT , MSNBC and ACE04 datasets , PBoH outperforms all but one of the presented EL systems and is competitive with the state of art approaches . The method whose performance is closer to ours is REL RW [ 14 ] whose average F1 score is only slightly higher than ours ( +0.6 on average ) . However , there is a number of significant advantages of our method that make it easier to use for practitioners . First , our approach is conceptually simpler and only requires sufficient statistics computed from Wikipedia . Second , PBoH shows a superior computational complexity manifested in significantly lower run times ( Table 5 ) , making it a good fit for large scale real time entity linking systems ; this is not the case for REL RW qualified as “ time consuming ” by its authors [ 14 ] . Third , the number of entities in the underlying graph , and thus the required memory , is significantly lower for PBoH ( see statistics provided in Table 6 ) .
Incremental accuracy gains
To give further insight to our method , Table 7 provides an overview of the contribution brought step by step by each incremental component of the Full PBoH system . It can be noted that PBoH performs best , outranking all its individual components .
Reproducibility of the experiments
Our experiments are easily reproducible using the details provided in this paper . Our learning procedure is only based on statistics coming from the set of Wikipedia webpages together with the Wiki hyperlinks . As a consequence , one can implement a real time highly accurate entity linking system solely based on the details described in this paper .
Our code is publicly available at : https://github.com/ dalab/pboh entity linking
6 . CONCLUSION
In this paper , we described a light weight graphical model for entity linking via approximate inference . Our method employs simple sufficient statistics that rely on three sources of information : First , a probabilistic name to entity map ˆp(e|m ) derived from a large corpus of hyperlinks ; second , observational data about the pairwise co occurrence of entities within documents from a Web collection ; third , entity contextual words statistics . Our experiments based on a number of popular entity linking benchmarking collections show improved performance as compared to several wellknown or recent entity linking systems .
There are several promising directions of future work that we will continue to pursue . Currently , our model considers pairwise potentials . In the future , it would be interesting to investigate the use of higher order potentials and submodular optimization in an entity linking pipeline , thus allowing us to capture the interplay between entire groups of entity candidates ( eg , through the use of entity categories ) . Additionally , we will further enrich our probabilistic model with statistics from new sources of information . We expect some of the performance gains that other papers report from using entity categories or semantic relations to be additive with regard to our system ’s current accuracy .
7 . REFERENCES [ 1 ] Adam L Berger , Vincent J Della Pietra , and Stephen
A Della Pietra . A maximum entropy approach to natural language processing . Computational linguistics , 22(1):39–71 , 1996 .
[ 2 ] Roi Blanco , Giuseppe Ottaviano , and Edgar Meij . Fast and space efficient entity linking for queries . In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining , pages 179–188 . ACM , 2015 .
[ 3 ] Razvan C Bunescu and Marius Pasca . Using encyclopedic knowledge for named entity disambiguation . In EACL , volume 6 , pages 9–16 , 2006 .
[ 4 ] Zhiyuan Cai , Kaiqi Zhao , Kenny Q Zhu , and Haixun Wang .
Wikification via link co occurrence . In Proceedings of the 22nd ACM international conference on Conference on information & knowledge management , pages 1087–1096 . ACM , 2013 .
[ 5 ] Diego Ceccarelli , Claudio Lucchese , Salvatore Orlando , Raffaele Perego , and Salvatore Trani . Dexter : an open source framework for entity linking . In Proceedings of the sixth international workshop on Exploiting semantic annotations in information retrieval , pages 17–20 . ACM , 2013 .
[ 6 ] Xiao Cheng and Dan Roth . Relational inference for wikification . Urbana , 51:61801 , 2013 .
[ 7 ] Silviu Cucerzan . Large scale named entity disambiguation based on wikipedia data . In EMNLP CoNLL , volume 7 , pages 708–716 , 2007 .
[ 8 ] John N Darroch and Douglas Ratcliff . Generalized iterative scaling for log linear models . The annals of mathematical statistics , pages 1470–1480 , 1972 .
[ 9 ] Mark Dredze , Paul McNamee , Delip Rao , Adam Gerber , and Tim Finin . Entity disambiguation for knowledge base population . In Proceedings of the 23rd International Conference on Computational Linguistics , pages 277–285 . Association for Computational Linguistics , 2010 .
[ 10 ] Greg Durrett and Dan Klein . A joint model for entity analysis : Coreference , typing , and linking . Transactions of the Association for Computational Linguistics , 2:477–490 , 2014 .
[ 11 ] Angela Fahrni and Michael Strube . A latent variable model for discourse aware concept and entity disambiguation . EACL 2014 , page 491 , 2014 .
[ 12 ] Paolo Ferragina and Ugo Scaiella . Fast and accurate annotation of short texts with wikipedia pages . arXiv preprint arXiv:1006.3498 , 2010 .
[ 13 ] Paolo Ferragina and Ugo Scaiella . Tagme : on the fly annotation of short text fragments ( by wikipedia entities ) . In Proceedings of the 19th ACM international conference on Information and knowledge management , pages 1625–1628 . ACM , 2010 .
[ 14 ] Zhaochen Guo and Denilson Barbosa . Robust entity linking via random walks . In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management , CIKM ’14 , pages 499–508 , New York , NY , USA , 2014 . ACM .
[ 15 ] Ben Hachey , Will Radford , and James R Curran .
Graph based named entity linking with wikipedia . In Web Information System Engineering–WISE 2011 , pages 213–226 . Springer , 2011 .
[ 16 ] Xianpei Han and Le Sun . An entity topic model for entity linking . In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning , pages 105–115 . Association for Computational Linguistics , 2012 .
[ 17 ] Xianpei Han , Le Sun , and Jun Zhao . Collective entity linking in web text : a graph based method . In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval , pages 765–774 . ACM , 2011 .
[ 18 ] Johannes Hoffart , Mohamed Amir Yosef , Ilaria Bordino ,
Hagen F¨urstenau , Manfred Pinkal , Marc Spaniol , Bilyana Taneva , Stefan Thater , and Gerhard Weikum . Robust disambiguation of named entities in text . In Proceedings of the Conference on Empirical Methods in Natural Language Processing , pages 782–792 . Association for Computational Linguistics , 2011 .
[ 19 ] Neil Houlsby and Massimiliano Ciaramita . A scalable gibbs sampler for probabilistic entity linking . In Advances in Information Retrieval , pages 335–346 . Springer , 2014 .
[ 20 ] Edwin T Jaynes . On the rationale of maximum entropy methods . Proceedings of the IEEE , 70(9):939–952 , 1982 .
[ 21 ] Saurabh S Kataria , Krishnan S Kumar , Rajeev R Rastogi ,
Prithviraj Sen , and Srinivasan H Sengamedu . Entity disambiguation with hierarchical topic models . In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 1037–1045 . ACM , 2011 .
[ 22 ] Sayali Kulkarni , Amit Singh , Ganesh Ramakrishnan , and Soumen Chakrabarti . Collective annotation of wikipedia entities in web text . In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 457–466 . ACM , 2009 .
[ 23 ] Pablo N Mendes , Max Jakob , Andr´es Garc´ıa Silva , and
Christian Bizer . Dbpedia spotlight : shedding light on the web of documents . In Proceedings of the 7th International Conference on Semantic Systems , pages 1–8 . ACM , 2011 .
[ 24 ] Rada Mihalcea and Andras Csomai . Wikify! : linking documents to encyclopedic knowledge . In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management , pages 233–242 . ACM , 2007 .
[ 25 ] David Milne and Ian H Witten . Learning to link with wikipedia . In Proceedings of the 17th ACM conference on Information and knowledge management , pages 509–518 . ACM , 2008 .
[ 26 ] Andrea Moro , Alessandro Raganato , and Roberto Navigli .
Entity Linking meets Word Sense Disambiguation : a Unified Approach . Transactions of the Association for Computational Linguistics ( TACL ) , 2:231–244 , 2014 .
[ 27 ] Kevin P . Murphy , Yair Weiss , and Michael I . Jordan . Loopy belief propagation for approximate inference : An empirical study . In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence , UAI’99 , pages 467–475 , San Francisco , CA , USA , 1999 . Morgan Kaufmann Publishers Inc .
[ 28 ] Francesco Piccinno and Paolo Ferragina . From tagme to wat : a new entity annotator . In Proceedings of the first international workshop on Entity recognition & disambiguation , pages 55–62 . ACM , 2014 .
[ 29 ] Anja Pilz and Gerhard Paaß . From names to entities using thematic context distance . In Proceedings of the 20th ACM international conference on Information and knowledge management , pages 857–866 . ACM , 2011 .
[ 30 ] Lev Ratinov , Dan Roth , Doug Downey , and Mike
Anderson . Local and global algorithms for disambiguation to wikipedia . In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies Volume 1 , pages 1375–1384 . Association for Computational Linguistics , 2011 .
[ 31 ] Benjamin Recht , Christopher Re , Stephen Wright , and
Feng Niu . Hogwild : A lock free approach to parallelizing stochastic gradient descent . In J . Shawe taylor , Rs Zemel , P . Bartlett , Fcn Pereira , and Kq Weinberger , editors , Advances in Neural Information Processing Systems 24 , pages 693–701 . 2011 .
[ 32 ] Giuseppe Rizzo , Marieke van Erp , and Rapha¨el Troncy .
Benchmarking the extraction and disambiguation of named entities on the semantic web . In Proceedings of the 9th International Conference on Language Resources and Evaluation , 2014 .
[ 33 ] Avirup Sil and Alexander Yates . Re ranking for joint named entity recognition and linking . In Proceedings of the 22nd ACM international conference on Conference on information & knowledge management , pages 2369–2374 . ACM , 2013 .
[ 34 ] Sameer Singh , Sebastian Riedel , Brian Martin , Jiaping
Zheng , and Andrew McCallum . Joint inference of entities , relations , and coreference . In Proceedings of the 2013 workshop on Automated knowledge base construction , pages 1–6 . ACM , 2013 .
[ 35 ] Valentin I . Spitkovsky and Angel X . Chang . A cross lingual dictionary for English Wikipedia concepts . In Proceedings of the Eighth International Conference on Language Resources and Evaluation ( LREC 2012 ) , Istanbul , Turkey , May 2012 .
[ 36 ] Nadine Steinmetz and Harald Sack . Semantic multimedia information retrieval based on contextual descriptions . In The Semantic Web : Semantics and Big Data , pages 382–396 . Springer , 2013 .
[ 37 ] Charles Sutton and Andrew McCallum . Piecewise training for structured prediction . Machine Learning , 77(2 3):165–194 , 2009 .
[ 38 ] Ricardo Usbeck , Axel Cyrille Ngonga Ngomo , Michael
R¨oder , Daniel Gerber , Sandro Athaide Coelho , S¨oren Auer , and Andreas Both . Agdistis graph based disambiguation of named entities using linked data . In The Semantic Web–ISWC 2014 , pages 457–471 . Springer , 2014 .
[ 39 ] Ricardo Usbeck , Michael R¨oder , Axel Cyrille
Ngonga Ngomo , Ciro Baron , Andreas Both , Martin Br¨ummer , Diego Ceccarelli , Marco Cornolti , Didier Cherix , Bernd Eickmann , et al . Gerbil : General entity annotator benchmarking framework . In Proceedings of the 24th International Conference on World Wide Web , pages 1133–1143 . International World Wide Web Conferences
Steering Committee , 2015 .
[ 40 ] S . V . N . Vishwanathan , Nicol N . Schraudolph , Mark W . Schmidt , and Kevin P . Murphy . Accelerated training of conditional random fields with stochastic gradient methods . In Proceedings of the 23rd International Conference on Machine Learning , ICML ’06 , pages 969–976 , New York , NY , USA , 2006 . ACM .
[ 41 ] JS Yedidia , WT Freeman , and Y . Weiss . Generalized belief propagation . In Advances in Neural Information Processing Systems ( NIPS ) , volume 13 , pages 689–695 , December 2000 .
[ 42 ] Chengxiang Zhai and John Lafferty . A study of smoothing methods for language models applied to information retrieval . ACM Transactions on Information Systems ( TOIS ) , 22(2):179–214 , 2004 .
[ 43 ] Zhe Zuo , Gjergji Kasneci , Toni Gruetze , and Felix
Naumann . Bel : Bagging for entity linking . In Proceedings of COLING 2014 , the 25th International Conference on Computational Linguistics : Technical Papers , pages 2075–2086 . Dublin City University and Association for Computational Linguistics , 2014 .
