Information Theoretic Co clustering
Inderjit S . Dhillon
Dept . of Computer Sciences University of Texas , Austin inderjit@csutexasedu
Subramanyam Mallela Dept . of Computer Sciences University of Texas , Austin manyam@csutexasedu
Dharmendra S . Modha
IBM Almaden Research Center
San Jose , CA dmodha@usibmcom
ABSTRACT Two dimensional contingency or co occurrence tables arise frequently in important applications such as text , web log and market basket data analysis . A basic problem in contingency table analysis is co clustering : simultaneous clustering of the rows and columns . A novel theoretical formulation views the contingency table as an empirical joint probability distribution of two discrete random variables and poses the co clustering problem as an optimization problem in information theory — the optimal co clustering maximizes the mutual information between the clustered random variables subject to constraints on the number of row and column clusters . We present an innovative co clustering algorithm that monotonically increases the preserved mutual information by intertwining both the row and column clusterings at all stages . Using the practical example of simultaneous word document clustering , we demonstrate that our algorithm works well in practice , especially in the presence of sparsity and high dimensionality . Categories and Subject Descriptors E.4 [ Coding and Information Theory ] : Data compaction and compression ; G.3 [ Probability and Statistics ] : Contingency table analysis ; H33 [ Information Search and Retrieval ] : Clustering ; I53 [ Pattern Recognition ] : Clustering Keywords Co clustering , information theory , mutual information 1 . Clustering is a fundamental tool in unsupervised learning that is used to group together similar objects [ 14 ] , and has practical importance in a wide variety of applications such as text , web log and market basket data analysis . Typically , the data that arises in these applications is arranged as a contingency or co occurrence table , such as , word document co occurrence table or webpage user browsing data . Most clustering algorithms focus on one way clustering , ie , cluster one dimension of the table based on similarities along the second dimension . For example , documents may be clustered based upon their word distributions or words may be clustered based upon their distribution amongst documents .
INTRODUCTION
It is often desirable to co cluster or simultaneously cluster both dimensions of a contingency table [ 11 ] by exploiting the clear duality between rows and columns . For example , we may be interested in finding similar documents and their interplay with word clusters . Quite surprisingly , even if we are interested in clustering along one dimension of the contingency table , when dealing with sparse and high dimensional data , it turns out to be beneficial to employ co clustering .
To outline a principled approach to co clustering , we treat the ( normalized ) non negative contingency table as a joint probability distribution between two discrete random variables that take values over the rows and columns . We define co clustering as a pair of maps from rows to row clusters and from columns to column clusters . Clearly , these maps induce clustered random variables . Information theory can now be used to give a theoretical formulation to the problem : the optimal co clustering is one that leads to the largest mutual information between the clustered random variables . Equivalently , the optimal co clustering is one that minimizes the difference ( “ loss ” ) in mutual information between the original random variables and the mutual information between the clustered random variables . In this paper , we present a novel algorithm that directly optimizes the above loss function . The resulting algorithm is quite interesting : it intertwines both row and column clustering at all stages . Row clustering is done by assessing closeness of each row distribution , in relative entropy , to certain “ row cluster prototypes ” . Column clustering is done similarly , and this process is iterated till it converges to a local minimum . Coclustering differs from ordinary one sided clustering in that at all stages the row cluster prototypes incorporate column clustering information , and vice versa . We theoretically establish that our algorithm never increases the loss , and so , gradually improves the quality of co clustering .
We empirically demonstrate that our co clustering algorithm alleviates the problems of sparsity and high dimensionality by presenting results on joint word document clustering . An interesting aspect of the results is that our co clustering approach yields superior document clusters as compared to the case where document clustering is performed without any word clustering . The explanation is that co clustering implicitly performs an adaptive dimensionality reduction at each iteration , and estimates fewer parameters than a standard “ one dimensional ” clustering approach . This results in an implicitly “ regularized ” clustering .
A word about notation : upper case letters such as X , Y , ˆX , ˆY will denote random variables . Elements of sets will be denoted by lower case letters such as x and y . Quantities associated with clusters will be “ hatted ” : for example , ˆX denotes a random variable obtained from a clustering of X while ˆx denotes a cluster . Probability distributions are denoted by p or q when the random variable is obvious or by p(X , Y ) , q(X , Y , ˆX , ˆY ) , p(Y |x ) , or q(Y |ˆx ) to make the random variable explicit . Logarithms to the base 2 are used .
2 . PROBLEM FORMULATION Let X and Y be discrete random variables that take values in the sets {x1 , . . . , xm} and {y1 , . . . , yn} respectively . Let p(X , Y ) denote the joint probability distribution between X and Y . We will think of p(X , Y ) as a m× n matrix . In practice , if p is not known , it may be estimated using observations . Such a statistical estimate is called a two dimensional contingency table or as a two way frequency table [ 9 ] .
We are interested in simultaneously clustering or quantizing X into ( at most ) k disjoint or hard clusters , and Y into ( at most ) ‘ disjoint or hard clusters . Let the k clusters of X be written as : {ˆx1 , ˆx2 , . . . , ˆxk} , and let the ‘ clusters of Y be written as : {ˆy1 , ˆy2 , . . . , ˆy‘} . In other words , we are interested in finding maps CX and CY ,
CX : {x1 , x2 , . . . , xm} → {ˆx1 , ˆx2 , . . . , ˆxk} CY : {y1 , y2 , . . . , yn} → {ˆy1 , ˆy2 , . . . , ˆy‘} .
For brevity , we will often write ˆX = CX ( X ) and ˆY = CY ( Y ) ; ˆX and ˆY are random variables that are a deterministic function of X and Y , respectively . Observe that X and Y are clustered separately , that is , ˆX is a function of X alone and ˆY is a function of Y alone . But , the partition functions CX and CY are allowed to depend upon the entire joint distribution p(X , Y ) .
Definition 21 We refer to the tuple ( CX , CY ) as a co clustering .
Suppose we are given a co clustering . Let us “ re order ” the rows of the joint distribution p such that all rows mapping into ˆx1 are arranged first , followed by all rows mapping into ˆx2 , and so on . Similarly , let us “ re order ” the columns of the joint distribution p such that all columns mapping into ˆy1 are arranged first , followed by all columns mapping into ˆy2 , and so on . This row column reordering has the effect of dividing the distribution p into little two dimensional blocks . We refer to each such block as a co cluster .
A fundamental quantity that measures the amount of information random variable X contains about Y ( and vice versa ) is the mutual information I(X ; Y ) [ 3 ] . We will judge the quality of a co clustering by the resulting loss in mutual information , I(X ; Y )− I( ˆX ; ˆY ) ( note that I( ˆX ; ˆY ) ≤ I(X ; Y ) by Lemma 2.1 below ) .
Definition 22 An optimal co clustering minimizes
I(X ; Y ) − I( ˆX ; ˆY )
( 1 ) subject to the constraints on the number of row and column clusters .
( 2 )
( 3 )
For a fixed distribution p , I(X ; Y ) is fixed ; hence minimizing ( 1 ) amounts to maximizing I( ˆX , ˆY ) .
Let us illustrate the situation with an example . Consider the 6 × 6 matrix below that represents the joint distribution :
2666664 p(X , Y ) =
.05 .05 .05 .05 .05 .05
0 0
0 0 .04 .04 .04 .04 .04
0 0
0 0
0 0 0 .05 .05 .05 0 .05 .05 .05 0 .04 .04 .04 0 .04 .04
3777775
Looking at the row distributions it is natural to group the rows into three clusters : ˆx1 = {x1 , x2} , ˆx2 = {x3 , x4} and ˆx3 = {x5 , x6} . Similarly the natural column clustering is : ˆy1 = {y1 , y2 , y3} , ˆy2 = {y4 , y5 , y6} . The resulting joint distribution p( ˆX , ˆY ) , see ( 6 ) below , is given by :
24 .3
0 0 .3 .2 .2
35 . p( ˆX , ˆY ) =
It can be verified that the mutual information lost due to this co clustering is only .0957 , and that any other co clustering leads to a larger loss in mutual information .
The following lemma shows that the loss in mutual information can be expressed as the “ distance ” of p(X , Y ) to an approximation q(X , Y ) — this lemma will facilitate our search for the optimal co clustering .
Lemma 21 For a fixed co clustering ( CX , CY ) , we can write the loss in mutual information as
I(X ; Y ) − I( ˆX , ˆY ) = D(p(X , Y )||q(X , Y ) ) ,
( 4 ) where D(·||· ) denotes the Kullback Leibler(KL ) divergence , also known as relative entropy , and q(X , Y ) is a distribution of the form q(x , y ) = p(ˆx , ˆy)p(x|ˆx)p(y|ˆy ) , where x ∈ ˆx , y ∈ ˆy .
( 5 )
Proof . Since we are considering hard clustering , p(ˆx ) =P
I(X ; Y ) − I( ˆX ; ˆY ) y∈ˆy x∈ˆx p(ˆx , ˆy ) =
X X x∈ˆx p(x ) , and p(ˆy ) =P X X X 0@X −X X X X X X X
X X X X
X x∈ˆx x∈ˆx x∈ˆx y∈ˆy y∈ˆy y∈ˆy
ˆy
ˆy
ˆx
ˆy
ˆx p(x , y ) log p(x , y ) log
=
ˆx
= p(x , y ) ,
( 6 ) y∈ˆy p(y ) . By definition , p(x , y ) p(x)p(y )
1A log p(x , y ) p(ˆx , ˆy ) p(ˆx)p(ˆy ) p(x , y ) p(ˆx , ˆy ) p(x ) p(ˆx ) p(y ) p(ˆy )
= p(x , y ) q(x , y ) where the last step follows since p(x|ˆx ) = p(x ) and 0 otherwise , and similarly for p(y|ˆy ) . p(x , y ) log x∈ˆx y∈ˆy
ˆy
ˆx
, p(ˆx ) for ˆx = CX ( x ) tu
2666664
3777775 ,
( 7 )
Lemma 2.1 shows that the loss in mutual information must be non negative , and reveals that finding an optimal coclustering is equivalent to finding an approximating distribution q of the form ( 5 ) that is close to p in Kullback Leibler divergence . Note that the distribution q preserves marginals of p , that is , for ˆx = CX ( x ) and ˆy = CY ( y ) , q(x ) = q(x , y ) = p(ˆx , ˆy)p(x|ˆx)p(y|ˆy ) = p(x , ˆx ) = p(x ) .
X y
X
X y∈ˆy
ˆy
Similarly , q(y ) = p(y ) . In Section 4 we give further properties of the approximation q . Recall the example p(X , Y ) in ( 2 ) and the “ natural ” row and column clusterings that led to ( 3 ) . It is easy to verify that the corresponding approximation q(X , Y ) defined in ( 5 ) equals q(X , Y ) =
0 0
0 0
.054 .054 .042 .054 .054 .042
0 0 0 .042 .054 .054 0 .042 .054 .054 .036 .036 .028 .028 .036 .036 .036 .036 .028 .028 .036 .036
0 0
0 0 and that D(p||q ) = 0957 Note that the row and column sums of the above q are identical to those of p given in ( 2 ) .
We end this section by providing another motivation based on the theory of source coding and transmission . Let us set up an artificial data compression problem , where we want to transmit X and Y from a source to a destination . Let us insist that this transmission be done in twostages : ( a ) first compute ˆX = CX ( X ) and ˆY = CY ( Y ) , and transmit the cluster identifiers ˆX and ˆY jointly ; and ( b ) separately transmit X given that the destination already knows ˆX and transmit Y given that the destination already knows ˆY . The first step will require , on an average , at least , H( ˆX , ˆY ) bits , and , the second step will require , on an average , H(X| ˆX ) + H(Y | ˆY ) bits . For every fixed co clustering , the average number of bits that must be transmitted from the source to the destination is :
H( ˆX , ˆY ) + H(X| ˆX ) + H(Y | ˆY ) .
( 8 )
However , by noting the parallel between ( 5 ) and ( 8 ) , it easy to show that :
H( ˆX , ˆY ) + H(X| ˆX ) + H(Y | ˆY ) − H(X , Y )
= D(p(X , Y )||q(X , Y ) ) .
Thus , to find an optimal co clustering it is sufficient to minimize ( 8 ) subject to the constraints on the number of row and column clusters . Observe that ( 8 ) contains the cross term H( ˆX , ˆY ) that captures the interaction between row and column clusters . This underscores the fact that clustering of rows and columns must interact in a “ good ” co clustering . A naive algorithm that clusters rows without paying attention to columns and vice versa will miss this critical interaction that is the essence of co clustering .
3 . RELATED WORK Most of the clustering literature has focused on one sided clustering algorithms [ 14 ] . There was some early work on co clustering , such as in [ 11 ] which used a local greedy splitting procedure to identify hierarchical row and column clusters in matrices of small size . Co clustering has also been called biclustering and block clustering in [ 2 ] and [ 17 ] respectively . Recently [ 4 ] used a graph formulation and a spectral heuristic that uses eigenvectors to co cluster documents and words ; however , a restriction in [ 4 ] was that each word cluster was associated with a document cluster . We do not impose such a restriction in this paper ; see Section 5.3 for examples of different types of associations between row and column clusters .
Our information theoretic formulation of preserving mutual information is similar to the information bottleneck ( IB ) framework [ 20 ] , which was introduced for one sided clustering , say X to ˆX . IB tries to minimize the quantity I(X , ˆX ) to gain compression while maximizing the mutual information I( ˆX , Y ) ; the overall quantity considered in [ 20 ] is I(X , ˆX)− βI( ˆX , Y ) where β reflects the tradeoff between compression and preservation of mutual information . The resulting algorithm yields a “ soft ” clustering of the data using a deterministic annealing procedure . For a hard partitional clustering algorithm using a similar information theoretic framework , see [ 6 ] . These algorithms were proposed for one sided clustering .
An agglomerative hard clustering version of the IB method was used in [ 19 ] to cluster documents after clustering words . The work in [ 8 ] extended the above work to repetitively cluster documents and then words . Both these papers use heuristic procedures with no guarantees on a global loss function ; in contrast , in this paper we first quantify the loss in mutual information due to co clustering and then propose an algorithm that provably reduces this loss function monotonically , converging to a local minimum .
Recently , when considering a clustering framework using Bayesian belief networks , [ 10 ] proposed an iterative optimization method that amounts to a multivariate generalization of [ 20 ] , and , once again , uses deterministic annealing . A later paper [ 18 ] presented a hard agglomerative algorithm for the same problem that has advantages over [ 10 ] in that “ it is simpler , fully deterministic , and non parametric . There is no need to identify cluster splits which is rather tricky ” . However , [ 18 ] pointed out that their “ agglomeration procedures do not scale linearly with the sample size as top down methods do . . . ” . In this paper , we present a principled , top down hard clustering method that scales well . Also , the results in [ 18 ] amount to first finding a word clustering followed by finding a document clustering ( without any iteration ) , whereas we present a procedure that intertwines word and document clusterings at all stages and continually improves both until a local minimum is found , and , hence , is a true co clustering procedure .
By Lemma 2.1 , our co clustering procedure is intimately related to finding a matrix approximation q(X , Y ) . A soft version of our procedure is related to the PLSI scheme of [ 12 ] ; however the latter uses a single latent variable model . A two sided clustering model is given in [ 13 ] that uses maximum likelihood in a model based generative framework .
4 . CO CLUSTERING ALGORITHM We now describe a novel algorithm that monotonically decreases the objective function ( 1 ) . To describe the algorithm and related proofs , it will be more convenient to think of the joint distribution of X , Y , ˆX , and ˆY . Let p(X , Y , ˆX , ˆY ) denote this distribution . Observe that we can write : p(x , y , ˆx , ˆy ) = p(ˆx , ˆy)p(x , y|ˆx , ˆy ) .
( 9 )
By Lemma 2.1 , for the purpose of co clustering , we will seek an approximation to p(X , Y , ˆX , ˆY ) using a distribution q(X , Y , ˆX , ˆY ) of the form : q(x , y , ˆx , ˆy ) = p(ˆx , ˆy)p(x|ˆx)p(y|ˆy ) .
( 10 )
The reader may want to compare ( 5 ) and ( 10 ) : observe that the latter is defined for all combinations of x , y , ˆx , and ˆy . Note that in ( 10 ) if ˆx 6= CX ( x ) or ˆy 6= CY ( y ) then q(x , y , ˆx , ˆy ) is zero . We will think of p(X , Y ) as a two dimensional marginal of p(X , Y , ˆX , ˆY ) and q(X , Y ) as a two dimensional marginal of q(X , Y , ˆX , ˆY ) . Intuitively , by ( 9 ) and ( 10 ) , within the co cluster denoted by ˆX = ˆx and ˆY = ˆy , we seek to approximate p(X , Y | ˆX = ˆx , ˆY = ˆy ) by a distribution of the form p(X| ˆX = ˆx)p(Y | ˆY = ˆy ) . The following proposition ( which we state without proof ) establishes that there is no harm in adopting such a formulation .
Proposition 41 For every fixed “ hard ” co clustering , D(p(X , Y )||q(X , Y ) ) = D(p(X , Y , ˆX , ˆY )||q(X , Y , ˆX , ˆY ) ) . where the last equality follows from ( 15 ) . tu
Interestingly , q(X , Y ) also enjoys a maximum entropy property and it can be verified that H(p(X , Y ) ) ≤ H(q(X , Y ) ) for any input p(X , Y ) .
Lemma 2.1 quantified the loss in mutual information upon co clustering as the KL divergence of p(X , Y ) to q(X , Y ) . Next , we use the above proposition to prove a lemma that expresses the loss in mutual information in two revealing ways . This lemma will lead to a “ natural ” algorithm .
Lemma 41 The loss in mutual information can be expressed as ( i ) a weighted sum of the relative entropies between row distributions p(Y |x ) and “ row lumped ” distributions q(Y |ˆx ) , or as ( ii ) a weighted sum of the relative entropies between column distributions p(X|y ) and “ columnlumped ” distributions q(X|ˆy ) , that is ,
D(p(X , Y , ˆX , ˆY )||q(X , Y , ˆX , ˆY ) ) p(x)D(p(Y |x)||q(Y |ˆx) ) , x:CX ( x)=ˆx
D(p(X , Y , ˆX , ˆY )||q(X , Y , ˆX , ˆY ) )
X X
ˆx
=
=
X X p(y)D(p(X|y)||q(X|ˆy) ) .
We first establish a few simple , but useful equalities that highlight properties of q desirable in approximating p .
ˆy y:CY ( y)=ˆy
Proposition 42 For a distribution q of the form ( 10 ) , the following marginals and conditionals are preserved : q(ˆx , ˆy ) = p(ˆx , ˆy ) , q(x , ˆx ) = p(x , ˆx ) & q(y , ˆy ) = p(y , ˆy ) . ( 11 )
Thus , p(x ) = q(x ) , p(y ) = q(y ) , p(ˆx ) = q(ˆx ) , p(ˆy ) = q(ˆy ) , p(x|ˆx ) = q(x|ˆx ) , p(y|ˆy ) = q(y|ˆy ) , p(ˆy|ˆx ) = q(ˆy|ˆx ) , p(ˆx|ˆy ) = q(ˆx|ˆy )
( 14 ) ∀x , y , ˆx , and ˆy . Further , if ˆy = CY ( y ) and ˆx = CX ( x ) , then ( 15 ) q(y|ˆx ) = q(y|ˆy)q(ˆy|ˆx ) , q(x , y , ˆx , ˆy ) = p(x)q(y|ˆx ) , and , symmetrically , q(x|ˆy ) = q(x|ˆx)q(ˆx|ˆy ) , q(x , y , ˆx , ˆy ) = p(y)q(x|ˆy ) .
( 12 )
( 13 )
( 16 )
( 17 )
Proof . The equalities of the marginals in ( 11 ) are simple to show and will not be proved here for brevity . Equalities ( 12 ) , ( 13 ) , and ( 14 ) easily follow from ( 11 ) . Equation ( 15 ) follows from q(y|ˆx ) = q(y , ˆy|ˆx ) =
= q(y|ˆy , ˆx)q(ˆy|ˆx ) = q(y|ˆy)q(ˆy|ˆx ) . q(y , ˆy , ˆx ) q(ˆx )
Equation ( 16 ) follows from q(x , y , ˆx , ˆy ) = p(ˆx , ˆy)p(x|ˆx)p(y|ˆy )
= p(ˆx)p(x|ˆx)p(ˆy|ˆx)p(y|ˆy ) = p(x , ˆx)p(ˆy|ˆx)p(y|ˆy ) = p(x)q(ˆy|ˆx)q(y|ˆy ) = p(x)q(y|ˆx ) ,
Proof . We show the first equality , the second is similar . D(p(X , Y , ˆX , ˆY )||q(X , Y , ˆX , ˆY ) )
ˆx,ˆy
X X X
ˆx,ˆy
=
( a ) =
=
X X p(x , y , ˆx , ˆy ) log p(x)p(y|x ) log p(x , y , ˆx , ˆy ) q(x , y , ˆx , ˆy ) p(x)p(y|x ) p(x)q(y|ˆx ) x:CX ( x)=ˆx,y:CY ( y)=ˆy x:CX ( x)=ˆx,y:CY ( y)=ˆy
X
X
ˆx x:CX ( x)=ˆx y p(x ) p(y|x ) log p(y|x ) q(y|ˆx )
, where ( a ) follows from ( 16 ) and since p(x , y , ˆx , ˆy ) = p(x , y ) = p(x)p(y|x ) when ˆy = CY ( y ) , ˆx = CX ( x ) . tu
The significance of Lemma 4.1 is that it allows us to express the objective function solely in terms of the row clustering , or in terms of the column clustering . Furthermore , it allows us to define the distribution q(Y |ˆx ) as a “ row cluster prototype ” , and similarly , the distribution q(X|ˆy ) as a “ columncluster prototype ” . With this intuition , we now present the co clustering algorithm in Figure 1 . The algorithm works as follows . It starts with an initial co clustering ( C ( 0 ) X , C ( 0 ) Y ) and iteratively refines it to obtain a sequence of co clusterings : ( C ( 1 ) Y ) , . . Associated with a generic coclustering ( C ( t ) Y ) in the sequence , the distributions p(t ) and q(t ) are given by : p(t)(x , y , ˆx , ˆy ) = p(t)(ˆx , ˆy)p(t)(x , y|ˆx , ˆy ) and q(t)(x , y , ˆx , ˆy ) = p(t)(ˆx , ˆy)p(t)(x|ˆx)p(t)(y|ˆy ) . Observe that while as a function of four variables , p(t)(x , y , ˆx , ˆy ) depends upon the iteration index t , the marginal p(t)(x , y ) is , in fact , independent of t . Hence , we will write p(x ) , p(y ) , p(x|y ) , p(y|x ) , and p(x , y ) , respectively , instead of p(t)(x ) , p(t)(y ) , p(t)(x|y ) , p(t)(y|x ) , and p(t)(x , y ) .
Y ) , ( C ( 2 )
X , C ( 1 )
X , C ( 2 )
X , C ( t )
Algorithm Co Clustering(p,k,‘,C
† X ,C
† Y )
Input : The joint probability distribution p(X , Y ) , k the desired number of row clusters , and ‘ the desired number of column clusters .
Output : The partition functions C
† X and C
† Y .
1 . Initialization : Set t = 0 . Start with some initial partition functions C(0 )
Y . Compute
X and C(0 ) q(0)( ˆX , ˆY ) , q(0)(X| ˆX ) , q(0)(Y | ˆY ) and the distributions q(0)(Y |ˆx ) , 1 ≤ ˆx ≤ k using ( 18 ) .
2 . Compute row clusters : For each row x , find its new cluster index as
C(t+1 )
X
( x ) = argminˆx D
“ p(Y |x)||q(t)(Y |ˆx )
, resolving ties arbitrarily . Let C(t+1 )
Y
= C(t ) Y .
3 . Compute distributions q(t+1)( ˆX , ˆY ) , q(t+1)(X| ˆX ) , q(t+1)(Y | ˆY ) and the distributions q(t+1)(X|ˆy ) , 1 ≤ ˆy ≤ ‘ using ( 19 ) .
4 . Compute column clusters : For each column y , find its new cluster index as
C(t+2 )
Y
( y ) = argminˆy D
“ p(X|y)||q(t+1)(X|ˆy )
,
”
” resolving ties arbitrarily . Let C(t+2 )
X
= C(t+1 )
X
.
5 . Compute distributions q(t+2)( ˆX , ˆY ) , q(t+2)(X| ˆX ) , q(t+2)(Y | ˆY ) and the distributions q(t+2)(Y |ˆx ) , 1 ≤ ˆx ≤ k using ( 18 ) .
† X = C(t+2 )
X and C
6 . Stop and return C the change in objective function value ,
Y if that D(p(X , Y )||q(t)(X , Y ) ) − D(p(X , Y )||q(t+2)(X , Y ) ) , “ small ” ( say 10−3 ) ; Else set t = t + 2 and go to step 2 .
† Y = C(t+2 ) is , is
Figure 1 : Information theoretic co clustering algorithm that simultaneously clusters both the rows and columns
X , C ( 0 )
In Step 1 , the algorithm starts with an initial co clustering ( C ( 0 ) Y ) and computes the required marginals of the resulting approximation q(0 ) ( the choice of starting points is important , and will be discussed in Section 5 ) . The algorithm then computes the appropriate “ row cluster prototypes ” q(0)(Y |ˆx ) . While the reader may wish to think of these as “ centroids ” , note that q(0)(Y |ˆx ) is not a centroid , q(0)(Y |ˆx ) 6=
1 |ˆx| p(Y |x ) ,
X x∈ˆx where |ˆx| denotes the number of rows in cluster ˆx . Rather , by ( 15 ) , for every y , we write q(t)(y|ˆx ) = q(t)(y|ˆy)q(t)(ˆy|ˆx ) ,
( 18 ) where ˆy = CY ( y ) . Note that ( 18 ) gives a formula that would have been difficult to guess a priori without the help of analysis . In Step 2 , the algorithm “ re assigns ” each row x to a new row cluster whose row cluster prototype q(t)(Y |ˆx ) is closest to p(Y |x ) in Kullback Leibler divergence . In essence ,
Step 2 defines a new row clustering . Also , observe that the column clustering is not changed in Step 2 . In Step 3 , using the new row clustering and the old column clustering , the algorithm recomputes the required marginals of q(t+1 ) . More importantly , the algorithm recomputes the column cluster prototypes . Once again , these are not ordinary centroids , but rather by using ( 17 ) , for every x , we write q(t+1)(x|ˆy ) = q(t+1)(x|ˆx)q(t+1)(ˆx|ˆy ) ,
( 19 ) where ˆx = CX ( x ) . Now , in Step 4 , the algorithm “ reassigns ” each column y to a new column cluster whose columncluster prototype q(t+1)(X|ˆy ) is closest to p(X|y ) in KullbackLeibler divergence . Step 4 defines a new column clustering while holding the row clustering fixed . In Step 5 , the algorithm re computes marginals of q(t+2 ) . The algorithm keeps iterating Steps 2 through 5 until some desired convergence condition is met . The following reassuring theorem , which is our main result , guarantees convergence . Note that coclustering is NP hard and a local minimum does not guarantee a global minimum .
Theorem 41 Algorithm Co Clustering monotonically de creases the objective function given in Lemma 21
Proof . D(p(t)(X , Y , ˆX , ˆY )||q(t)(X , Y , ˆX , ˆY ) ) p(x ) p(y|x ) log p(y|x ) q(t)(y|ˆx ) p(y|x ) log q(t )
“ p(y|x ) y|C ( t+1 )
X
”
( x )
X X y p(x ) y p(x )
( x)=ˆx
ˆx
ˆx
=
ˆx x:C
ˆx x:C
ˆy y:C x:C
( a ) =
( c ) =
( t+1 ) X
( t+1 ) Y
( t ) X ( x)=ˆx
( t ) X ( x)=ˆx
X ( b)≥ X X X X | X
X X X X X X 0B@ X = I1 +X X “ X X X X
( d ) = I1 +
= I1 +
( t+1 ) X
( t+1 ) X
( t+1 ) X
( e)≥ I1 +
ˆx
ˆy
ˆx
ˆy x:C
ˆx x:C x:C
+
ˆx p(y|x ) log p(y|x ) q(t ) ( ˆy|ˆx ) q(t)(y|ˆy )
( y)=ˆy
( x)=ˆx y:C
( t+1 ) Y
( y)=ˆy
X X
{z
I1
X X
ˆy
ˆy p(x ) p(x )
( x)=ˆx
( t+1 ) Y
( y)=ˆy p(y|x ) log p(y|x ) q(t)(y|ˆy )
} p(y|x ) log
1 q(t)(ˆy|ˆx )
1CA log p(x)p(y|x )
1 q(t)(ˆy|ˆx ) y:C
X ”
( x)=ˆx y:C
( y)=ˆy
( t+1 ) Y q(t+1)(ˆx , ˆy ) log
1 q(t)(ˆy|ˆx )
X X
ˆy q(t+1)(ˆx ) q(t+1)(ˆx )
ˆx
ˆy q(t+1)(ˆy|ˆx ) log
1 q(t)(ˆy|ˆx ) q(t+1)(ˆy|ˆx ) log
1 q(t+1)(ˆy|ˆx ) x:C
( t+1 ) X
( x)=ˆx y:C
( t+1 ) Y
( y)=ˆy
X X X X
=
( f ) =
ˆx
X X X X
ˆy
ˆx x:C
( t+1 ) X
( x)=ˆx
ˆy y:C
( t+1 ) Y
( y)=ˆy p(x ) p(y|x ) log p(y|x ) q(t+1)(ˆy|ˆx)q(t)(y|ˆy ) p(x ) p(y|x ) log p(y|x ) q(t+1)(ˆy|ˆx)q(t+1)(y|ˆy )
( g )
= D(p(t+1)(X , Y , ˆX , ˆY )||q(t+1)(X , Y , ˆX , ˆY ) ) ,
( 20 ) where ( a ) follows from Lemma 4.1 ; ( b ) follows from Step 2 of the algorithm ; ( c ) follows by rearranging the sum and from ( 15 ) ; ( d ) follows from Step 3 of the algorithm , ( 6 ) and ( 11 ) ; ( e ) follows by non negativity of the Kullback Leibler divergence ; and ( f ) follows since we hold the column clusters fixed in Step 2 , that is , C ( t+1 ) Y , and ( g ) is due to ( 15 ) and Lemma 41 By using an identical argument , which we omit for brevity , and by using properties of Steps 4 and 5 , we can show that
= C ( t )
Y
D(p(t+1)(X , Y , ˆX , ˆY )||q(t+1)(X , Y , ˆX , ˆY ) )
≥ D(p(t+2)(X , Y , ˆX , ˆY )||q(t+2)(X , Y , ˆX , ˆY ) ) .
( 21 )
By combining ( 20 ) and ( 21 ) , it follows that every iteration of the algorithm never increases the objective function . tu
Corollary 41 The algorithm in Figure 1 terminates in a finite number of steps at a cluster assignment that is locally optimal , that is , the loss in mutual information cannot be decreased by either ( a ) re assignment of a distribution p(Y |x ) or p(X|y ) to a different cluster distribution q(Y |ˆx ) or q(X|ˆy ) , respectively , or by ( b ) defining a new distribution for any of the existing clusters .
Proof . The result follows from Theorem 4.1 and since the tu number of distinct co clusterings is finite .
Remark 41 The algorithm is computationally efficient even for sparse data as its complexity can be shown to be O(nz · τ · ( k + ‘ ) ) where nz is the number of nozeros in the input joint distribution p(X , Y ) and τ is the number of iterations ; empirically 20 iterations are seen to suffice .
Remark 42 A closer examination of the above proof shows that Steps 2 and 3 together imply ( 20 ) and Steps 4 and 5 together imply ( 21 ) . We show how to generalize the above convergence guarantee to a class of iterative algorithms . In particular , any algorithm that uses an arbitrary concatenations of Steps 2 and 3 with Steps 4 and 5 is guaranteed to monotonically decrease the objective function . For example , consider an algorithm that flips a coin at every iteration and performs Steps 2 and 3 if the coin turns up heads , and performs Steps 4 and 5 otherwise . As an another example , consider an algorithm that keeps iterating Steps 2 and 3 , until no improvement in the objective function is noticed . Next , it can keep iterating Steps 4 and 5 , until no further improvement in the objective function is noticed . Now , it can again iterate Steps 2 and 3 , and so on and so forth . Both these algorithms as well all algorithms in the same spirit are guaranteed to monotonically decrease the objective function . Such algorithmic flexibility can allow exploration of various local minima when starting from a fixed initial random partition in Step 1 .
Remark 43 While our algorithm is in the spirit of kmeans , the precise algorithm itself is quite different . For example , in our algorithm , the distribution q(t)(Y |ˆx ) serves as a “ row cluster prototype ” . This quantity is different from the naive “ centroid ” of the cluster ˆx . Similarly , the columncluster prototype q(t+1)(X|ˆy ) is different from the obvious centroid of the cluster ˆy . In fact , detailed analysis ( as is evident from the proof of Theorem 4.1 ) was necessary to identify these key quantities .
4.1 Illustrative Example We now illustrate how our algorithm works by showing how it discovers the optimal co clustering for the example p(X , Y ) distribution given in ( 2 ) of Section 2 . Table 1 shows a typical run of our co clustering algorithm that starts with a random partition of rows and columns . At each iteration Table 1 shows the steps of Algorithm Co Clustering , the resulting approximation q(t)(X , Y ) and the corresponding compressed distribution p(t)( ˆX , ˆY ) . The row and column cluster numbers are shown around the matrix to indicate the clustering at each stage . Notice how the intertwined row and column co clustering leads to progressively better approximations to the original distribution . At the end of four iterations the algorithm almost accurately reconstructs the original distribution , discovers the natural row and column partitions and recovers the ideal compressed distribution p( ˆX , ˆY ) given in ( 3 ) . A pleasing property is that at all iterations q(t)(X , Y ) preserves the marginals of the original p(X , Y ) .
5 . EXPERIMENTAL RESULTS This section provides empirical evidence to show the benefits of our co clustering framework and algorithm . In particular we apply the algorithm to the task of document clustering using word document co occurrence data . We show that the co clustering approach overcomes sparsity and highdimensionality yielding substantially better results than the approach of clustering such data along a single dimension . We also show better results as compared to previous algorithms in [ 19 ] and [ 8 ] . The latter algorithms use a greedy technique ( [19 ] uses an agglomerative strategy ) to cluster documents after words are clustered using the same greedy approach . For brevity we will use the following notation to denote various algorithms in consideration . We call the Information Bottleneck Double Clustering method in [ 19 ] as IB Double and the Iterative Double Clustering algorithm in [ 8 ] as IDC . In addition we use 1D clustering to denote document clustering without any word clustering i.e , clustering along a single dimension .
5.1 Data Sets and Implementation Details For our experimental results we use various subsets of the 20Newsgroup data(NG20 ) [ 15 ] and the SMART collection from Cornell ( ftp://ftpcscornelledu/pub/smart ) The NG20 data
ˆy1 .029 .036 .018 .018 .039 .039
ˆy1 .036 .036 .019 .019 .043 .025
ˆy1 .054 .054 .013 .013 .028 .017
ˆy1 .054 .054 0 0 .036 .036
ˆx3 ˆx1 ˆx2 ˆx2 ˆx3 ˆx3
ˆx1 ˆx1 ˆx2 ˆx2 ˆx3 ˆx2
ˆx1 ˆx1 ˆx2 ˆx2 ˆx3 ˆx2
ˆx1 ˆx1 ˆx2 ˆx2 ˆx3 ˆx3 q(t)(X , Y ) ˆy1 .022 .028 .014 .014 .030 .030
ˆy2 .019 .014 .028 .028 .025 .025
ˆy1 .029 .036 .018 .018 .039 .039
ˆy2 .024 .018 .036 .036 .032 .032
ˆy2 .024 .018 .036 .036 .032 .032
↓ steps 2 & 3 of Figure 1
ˆy1 .036 .036 .019 .019 .043 .025
ˆy2 .014 .014 .026 .026 .022 .035
ˆy1 .028 .028 .015 .015 .033 .020
ˆy2 .018 .018 .034 .034 .028 .046
ˆy2 .018 .018 .034 .034 .028 .046
↓ steps 4 & 5 of Figure 1
ˆy1 .054 .054 .013 .013 .028 .017
ˆy1 .042 .042 .010 .010 .022 .013
ˆy2 0 0 .031 .031 .033 .042
ˆy2 0 0 .041 .041 .043 .054
ˆy2 0 0 .041 .041 .043 .054
↓ steps 2 & 3 of Figure 1
ˆy1 .054 .054 0 0 .036 .036
ˆy1 .042 .042 0 0 .028 .028
ˆy2 0 0 .042 .042 .028 .028
ˆy2 0 0 .054 .054 .036 .036
ˆy2 0 0 .054 .054 .036 .036 p(t)( ˆX , ˆY )
0.10 0.10 0.30
0.05 0.20 0.25
0.20 0.18 0.12
0.10 0.32 0.08
0.30 0.12 0.08
0 0.38 0.12
0.30 0 0.20
0 0.30 0.20
Table 1 : Algorithm Co Clustering of Figure 1 gives progressively better clusterings and approximations till the optimal is discovered for the example p(X , Y ) given in Section 2 . set consists of approximately 20 , 000 newsgroup articles collected evenly from 20 different usenet newsgroups . This data set has been used for testing several supervised text classification tasks [ 6 ] and unsupervised document clustering tasks [ 19 , 8 ] . Many of the newsgroups share similar topics and about 4.5 % of the documents are cross posted making the boundaries between some news groups rather fuzzy . To make our comparison consistent with previous algorithms we reconstructed various subsets of NG20 used in [ 19 , 8 ] . We applied the same pre processing steps as in [ 19 ] to all the subsets , ie , removed stop words , ignored file headers and selected the top 2000 words by mutual information1 . Specific details of the subsets are given in Table 2 . The SMART
1The data sets used in [ 19 ] and [ 8 ] differ in their preprocessing steps . The latter includes subject lines while the former does not . So we prepared two different data sets one with subject lines and the other without subject lines . collection consists of MEDLINE , CISI and CRANFIELD sub collections . MEDLINE consists of 1033 abstracts from medical journals , CISI consists of 1460 abstracts from information retrieval papers and CRANFIELD consists of 1400 abstracts from aerodynamic systems . After removing stop words and numeric characters we selected the top 2000 words by mutual information as part of our pre processing . We will refer to this data set as CLASSIC3 .
Bow [ 16 ] is a library of C code useful for writing text analysis , language modeling and information retrieval programs . We extended Bow with our co clustering and 1D clustering procedures , and used MATLAB for spy plots of matrices .
5.2 Evaluation Measures Validating clustering results is a non trivial task . In the presence of true labels , as in the case of the data sets we use , we can form a confusion matrix to measure the effectiveness of the algorithm . Each entry(i , j ) in the confusion matrix represents the number of documents in cluster i that belong to true class j . For an objective evaluation measure we use micro averaged precision . For each class c in the data set we define α(c , ˆy ) to be the number of documents correctly assigned to c , β(c , ˆy ) to be number of documents incorrectly assigned to c and γ(c , ˆy ) to be the number of documents incorrectly not assigned to c . The micro averaged precision and recall are defined , respectively , as :
P
P
P
P
P ( ˆy ) = c α(c , ˆy )
, R(ˆy ) = c α(c , ˆy )
. c(α(c , ˆy ) + β(c , ˆy ) ) c(α(c , ˆy ) + γ(c , ˆy ) )
Note that for uni labeled data P ( ˆy ) = R(ˆy ) .
5.3 Results and Discussion First we demonstrate that co clustering is significantly better than clustering along a single dimension using worddocument co occurrence matrices . In all our experiments since we know the number of true document clusters we can give that as input to our algorithm . For example in the case of Binary data set we ask for 2 document clusters . To show how the document clustering results change with the number of word clusters , we tried k = 2 , 4 , 8 , . . . , 128 word clusters . To initialize a co clustering with k word clusters , we split each word cluster obtained from a co clustering run with k/2 word clusters . Note that this does not increase the overall complexity of the algorithm . We bootstrap at k = 2 by choosing initial word cluster distributions to be “ maximally ” far apart from each other [ 1 , 6 ] . Our initialization scheme alleviates , to a large extent , the problem of poor local minima . To initialize document clustering we use a random perturbation of the “ mean ” document , a strategy that has been observed to work well for document clustering [ 5 ] . Since this initialization has a random component all our results are averages of five trials unless stated otherwise .
Table 3 shows two confusion matrices obtained on the CLASSIC3 data set using algorithms 1D clustering and co clustering ( with 200 word clusters ) . Observe that co clustering extracted the original clusters almost correctly resulting in a micro averaged precision of 0.9835 while 1D clustering led to a micro averaged precision of 09432
Dataset
Newsgroups included
Binary & Binary subject Multi5 & Multi5 subject
Multi10 & Multi10 subject talkpoliticsmideast , talkpoliticsmisc comp.graphics , rec.motorcycles , recsportsbaseball , sci.space , talkpoliticsmideast alt.atheism , compsysmachardware , misc.forsale , recautos,recsporthockey , sci.crypt , sci.electronics , sci.med , sci.space , talkpoliticsgun
#documents Total per group 250 documents 500
100
50
500
500
Table 2 : Datasets : Each dataset contains documents randomly sampled from newsgroups in the NG20 corpus .
Co clustering
1D clustering
992 40 1
4
1452
4
8 7
1387
944 71 18
9
1431
20
98 5
1297
Table 3 : Co clustering accurately recovers original clusters in the CLASSIC3 data set .
Binary
Binary subject
Co clustering 244
4
6
246
1D clustering Co clustering 178 72
11 239
104 146
241
9
1D clustering 179 71
94 156
Table 4 : Co clustering obtains better clustering results compared to one dimensional document clustering on Binary and Binary subject data sets
Table 4 shows confusion matrices obtained by co clustering and 1D clustering on the more “ confusable ” Binary and Binary subject data sets . While co clustering achieves 0.98 and 0.96 micro averaged precision on these data sets respectively , 1D clustering yielded only 0.67 and 0648
Figure 2 shows how precision values vary with the number of word clusters for each data set . Binary and Binary subject data sets reach peak precision at 128 word clusters , Multi5 and Multi5 subject at 64 and 128 word clusters and Multi10 and Multi10 subject at 64 and 32 word clusters respectively . Different data sets achieve their maximum at different number of word clusters . In general selecting the number of clusters to start with is a non trivial model selection task and is beyond the scope of this paper . Figure 3 shows the fraction of mutual information lost using co clustering with varied number of word clusters for each data set . For optimal co clusterings , we expect the loss in mutual information to decrease monotonically with increasing number of word clusters . We observe this on all data sets in Figure 2 ; our initialization plays an important role in achieving this . Also note the correlation between Figures 2 & 3 : the trend is that the lower the loss in mutual information the better is the clustering . To avoid clutter we did not show error bars in Figures 2 & 3 since the variation in values was minimal .
Figure 4 shows a typical run of our co clustering algorithm on the Multi10 data set . Notice how the objective function value(loss in mutual information ) decreases monotonically . We also observed that co clustering converges quickly in about 20 iterations on all our data sets .
Table 5 shows micro averaged precision measures on all our
Figure 2 : Micro averaged precision values with varied number of word clusters using co clustering on different NG20 data sets .
Figure 3 : Fraction of mutual information lost with varied number of word clusters using co clustering on different NG20 data sets .
Binary subject
Binary
Multi5
Multi5 subject
Multi10
Multi10 subject
Co clustering
1D clustering
IB Double
IDC
0.98 0.96 0.87 0.89 0.56 0.54
0.64 0.67 0.34 0.37 0.17 0.19
0.70
0.5
0.35
0.85
0.88
0.55
Table 5 : Co clustering obtains better microaveraged precision values on different newsgroup data sets compared to other algorithms .
1248163264128040608Number of Word Clusters ( log scale)Micro Average PrecisionBinaryBinary_subjectMulti5Multi5_subjectMulti10Multi10_subject12481632641280.5 05506 06507 Number of Word Clusters ( log scale)Fraction of Mutual Information lostBinaryBinary_subjectMulti5Multi5_subjectMulti10Multi10_subject ˆx13 dod ride rear riders harleys camping carbs bikers tharp davet
ˆx14 pitching season players scored cubs fans teams yankees braves starters
ˆx16 graphics image mac ftp color cd package display data format
ˆx23 space nasa shuttle flight algorithm orbital satellite budget srb prototype
ˆx24 israel arab jewish occupied rights palestinian holocaust syria civil racist
ˆx47 army working running museum drive visit post cpu plain mass
ˆx13 ,
ˆx14 ,
Table 6 : Word Clusters obtained using coclustering on the Multi5 subject data set . The clusters represent rec.motorcycles , recsportbaseball , comp.graphics , sci.space and talkpoliticsmideast newsgroups respectively . For each cluster only top 10 words sorted by mutual information are shown .
ˆx23 and ˆx24
ˆx16 , information decreases Figure 4 : Loss in mutual monotonically with the number of iterations on a typical co clustering run on the Multi10 data set . data sets ; we report the peak IB Double and IDC precision values given in [ 19 ] and [ 8 ] respectively . Similarly , in the column under co clustering we report the peak precision value from among the values in Figure 2 . On all data sets co clustering performs much better than 1D clustering clearly indicating the utility of clustering words and documents simultaneously . Co clustering is also significantly better than IB Double and comparable with IDC supporting the hypothesis that word clustering can alleviate the problem of clustering in high dimensions .
We now show the kind of structure that co clustering can discover in sparse word document matrices . Figure 5 shows the original word document matrix and the reordered matrix obtained by arranging rows and columns according to cluster order to reveal the various co clusters . To simplify the figure the final row clusters from co clustering are ordered in ascending order of their cluster purity distribution entropy . Notice how co clustering reveals the hidden sparsity structure of various co clusters of the data set . Some word clusters are found to be highly indicative of individual document clusters inducing a block diagonal sub structure while the dense sub blocks at the bottom of the right panel of Figure 5 show that other word clusters are more uniformly distributed over the document clusters . We observed similar sparsity structure in other data sets .
While document clustering is the main objective of our experiments the co clustering algorithm also returns word clusters . An interesting experiment would be to apply co clustering to co occurrence matrices where true labels are available for both dimensions . In Table 6 we give an example to show that word clusters obtained with co clustering are meaningful and often representative of the document clusters . Table 6 shows six of the word clusters obtained with coclustering on Multi5 subject data set when a total of 50 word clusters and 5 document clusters are obtained . The clusters ˆx13 , ˆx14 , ˆx16 , ˆx23 and ˆx24 appear to represent individual newsgroups with each word cluster containing words indicative of a single newsgroup . This correlates well with the co cluster block diagonal sub structure observed in Figure 5 . Additionally there are a few clusters like ˆx47 which contained non differentiating words ; clustering them into a single cluster appears to help co clustering in overcoming noisy dimensions .
6 . CONCLUSIONS AND FUTURE WORK We have provided an information theoretic formulation for co clustering , and presented a simple to implement , top down , computationally efficient , principled algorithm that intertwines row and column clusterings at all stages and is guaranteed to reach a local minimum in a finite number of steps .
We have presented examples to motivate the new concepts and to illustrate the efficacy of our algorithm . In particular , word document matrices that arise in information retrieval are known to be highly sparse [ 7 ] . For such sparse highdimensional data , even if one is only interested in document clustering , our results show that co clustering is more effective than a plain clustering of just documents . The reason is that when co clustering is employed , we effectively use word clusters as underlying features and not individual words . This amounts to implicit and adaptive dimensionality reduction and noise removal leading to better clusters . As a side benefit , co clustering can be used to annotate the document clusters .
While , for simplicity , we have restricted attention to coclustering for joint distributions of two random variables , both our algorithm and our main theorem can be easily extended to co cluster multi dimensional joint distributions .
In this paper , we have assumed that the number of row and column clusters are pre specified . However , since our formulation is information theoretic , we hope that a informationtheoretic regularization procedure like MDL may allow us to select the number of clusters in a data driven fashion . Finally , as the most interesting open research question , we would like to seek a generalization of our “ hard ” co clustering formulation and algorithms to an abstract multivariate clustering setting that would be applicable when more complex interactions are present between the variables being clustered and the clusters themselves .
Acknowledgments . Part of this research was sup
02468101214162727528285Number of IterationsObjective_value(Loss in Mutual Information)multi10 Figure 5 : Sparsity structure of the Binary subject word document co occurrence matrix before(left ) and after(right ) co clustering reveals the underlying structure of various co clusters ( 2 document clusters and 100 word clusters ) . The shaded regions represent the non zero entries . ported by NSF CAREER Award No . ACI 0093404 and Texas Advanced Research Program grant 003658 0431 2001 .
7 . REFERENCES [ 1 ] P . Bradley , U . Fayyad , and C . Reina . Scaling clustering algorithms to large databases . In KDD’03 . AAAI Press , 1998 .
[ 2 ] Y . Cheng and G . Church . Biclustering of expression data . In Proceedings ISMB , pages 93–103 . AAAI Press , 2000 .
[ 3 ] T . Cover and J . Thomas . Elements of Information
Theory . John Wiley & Sons , New York , USA , 1991 .
[ 4 ] I . S . Dhillon . Co clustering documents and words using bipartite spectral graph partitioning . In Proceedings of The 7th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining(KDD 2001 ) , pages 269–274 , 2001 .
[ 5 ] I . S . Dhillon , J . Fan , and Y . Guan . Efficient clustering of very large document collections . In R . Grossman , C . Kamath , P . Kegelmeyer , V . Kumar , and R . Namburu , editors , Data Mining for Scientific and Engineering Applications , pages 357–381 . Kluwer Academic Publishers , 2001 .
[ 6 ] I . S . Dhillon , S . Mallela , and R . Kumar . A divisive information theoretic feature clustering algorithm for text classification . Journal of Machine Learning Research(JMLR ) : Special Issue on Variable and Feature Selection , 3:1265–1287 , March 2003 .
[ 7 ] I . S . Dhillon and D . S . Modha . Concept decompositions for large sparse text data using clustering . Machine Learning , 42(1):143–175 , January 2001 .
[ 8 ] R . El Yaniv and O . Souroujon . Iterative double clustering for unsupervised and semi supervised learning . In ECML 01 , pages 121–132 , 2001 .
[ 9 ] R . A . Fisher . On the interpretation of χ2 from the contingency tables , and the calculation of p . J . Royal Stat . Soc . , 85:87–94 , 1922 .
[ 10 ] N . Friedman , O . Mosenzon , N . Slonim , and N . Tishby .
Multivariate information bottleneck . In UAI 2001 , pages 152–161 , 2001 .
[ 11 ] J . A . Hartigan . Direct clustering of a data matrix .
Journal of the American Statistical Association , 67(337):123–129 , March 1972 .
[ 12 ] T . Hofmann . Probabilistic latent semantic indexing .
In Proc . ACM SIGIR . ACM Press , August 1999 .
[ 13 ] T . Hofmann and J . Puzicha . Latent class models for collaborative filtering . In Proceedings of the International Joint Conference in Artificial Intelligence ( IJCAI ) , 1999 .
[ 14 ] A . K . Jain and R . C . Dubes . Algorithms for Clustering
Data . Prentice Hall , Englewood Cliffs , NJ , 1988 .
[ 15 ] Ken Lang . News Weeder : Learning to filter netnews .
In ICML 95 , pages 331–339 , 1995 .
[ 16 ] A . K . McCallum . Bow : A toolkit for statistical language modeling , text retrieval , classification and clustering . wwwcscmuedu/ mccallum/bow , 1996 .
[ 17 ] B . Mirkin . Mathematical Classification and Clustering .
Kluwer Academic Publishers , 1996 .
[ 18 ] N . Slonim , N . Friedman , and N . Tishby .
Agglomerative multivariate information bottleneck . In NIPS 14 , 2001 .
[ 19 ] N . Slonim and N . Tishby . Document clustering using word clusters via the information bottleneck method . In ACM SIGIR , pages 208–215 , 2000 .
[ 20 ] N . Tishby , F . C . Pereira , and W . Bialek . The information bottleneck method . In Proc . of the 37 th Annual Allerton Conference on Communication , Control and Computing , pages 368–377 , 1999 .
0501001502002503003504004505000200400600800100012001400160018002000nz = 196260501001502002503003504004505000200400600800100012001400160018002000nz = 19626
