Online Novelty Detection on Temporal Sequences
Junshui Ma
Aureon Biosciences Corp Yonkers , NY 10701 USA
914 377 4034 , +1 jasonma@aureoncom
Simon Perkins
NIS 2 , Los Alamos National Lab MS D436 , NIS 2 , LANL
Los Alamos , NM 87544 USA
505 667 9558 , +1 sperkins@lanlgov
Abstract : Novelty detection , or anomaly detection , on temporal sequences has increasingly attracted attention from researchers in different areas . In this paper , we present a new framework for online novelty detection on temporal sequences . This framework includes a mechanism for associating each detection result with a confidence value . Based on this framework , we develop a concrete online detection algorithm , by modeling the temporal sequence using an online support vector regression algorithm . Experiments on both synthetic and real world data are performed to demonstrate the promising performance of our proposed detection algorithm .
Keywords : Novelty detection , Anomaly detection , Confidence , Online algorithm , Support vector regression .
1 . INTRODUCTION
Novelty detection , or anomaly detection , refers to the automatic identification of unforeseen or abnormal phenomena embedded in a large amount of normal data . [ 4 , 8 , 17 ] It can be applied in both time sensitive and time insensitive scenarios . This paper targets the time sensitive case of detecting novelty in temporal sequences [ 2 , 4 , 5 , 8 ] , which has many immediate applications . For example , in a safety critical environment , it is very helpful to have an automatic supervising system , which can screen the time series generated by monitoring sensors , and report any abnormal observations . Another promising application is to help scientists in different areas by liberating them from exhaustive examination of data , and drawing their attention only to unusual and “ interesting ” phenomena .
Novelty detection is a challenging topic , mainly because it is hard to obtain sufficient knowledge and an accurate representative of “ novelty ” given a problem [ 17 ] , which rules out the use of many supervised techniques . Despite its challenge , in the past over ten years it has been a topic acquiring increasing attention , and quite a few techniques have been proposed and investigated . These techniques were experimentally proven to be effective in some cases , while they can fail in other cases . For example , some methods were designed based on the assumption of possessing
Copyright 2003 Association for Computing Machinery . ACM acknowledges that this contribution was authored or co authored by a contractor or affiliate of the US Government . As such , the Government retains a nonexclusive , royalty free right to publish or reproduce this article , or to allow others to do so , for Government purposes only . SIGKDD ’03 , August 24 27 , 2003 , Washington , DC , USA . Copyright 2003 ACM 1 58113 737 0/03/0008…$500 interpreted as outlier detection . However , precise models of the underlying problem [ 9 ] , or knowing the novelty conditions [ 1 , 6 , 13 ] , which are generally not true in real world . In some other studies [ 3 , 7 , 10 , 14 ] , novelty detection was simply this simplification produces methods that cannot discover novel patterns formed by several continuous instances . In particular , the novelty detection method proposed in [ 14 ] and [ 3 ] is based on a technique called one class support vector machine , whose formulation forces it to “ identify ” some novel individuals no matter how normal the whole data set is . A wavelet based signal trend shift detection method is proposed in [ 15 ] . Nevertheless , this method cannot detect short novel patterns embedded in normal signals . An interesting idea for novelty detection , inspired by the negative selection mechanism immune system , was proposed in [ 4 ] . However , this method can fail when the negative set goes to null with the increasing diversity of the normal set . Another method for target time series novelty detection , called TARZAN [ 8 ] , is based on converting the time series into a symbolic string . However , the procedure for discretizing and symbolizing real values in time series , as well as the various choices of string representation parameters , can cause the loss of meaningful patterns in the original time series . in the
Furthermore , the ability to conduct online novelty detection is especially desirable for temporal sequences . However , few of the existing algorithms explicitly address this issue . The most relevant result is an incremental algorithm designed to detect normal events , instead of novel events . [ 5 ]
In this paper , we propose a direction to detect novelty in temporal sequences . As with other detection algorithms , it is impossible for our proposed direction to succeed in all scenarios . However , it can at least provide an alternative and complementary solution to some problems in which other available techniques may fail . The remainder of this paper is organized as follows . A general online novelty detection framework is proposed in Section 2 . Based on the framework , a concrete online detection algorithm is proposed . Because the new algorithm utilizes a technique called support vector regression ( SVR ) , Section 3 is devoted to a brief introduction to SVR . The new algorithm , as well as issues involved in its implementation , is presented in Section 4 . Two variants of the original detection algorithms are proposed in Section 5 to adapt the original algorithm to different application scenarios . Experiments are proposed in Section 6 .
The main contributions of this paper are ( a ) It proposes an online novelty detection framework for temporal sequences . This online framework is capable of associating a confidence level with each detection result . ( b ) It proposes a concrete online novelty detection algorithm based on the framework , and describes experiments to test the new algorithm .
2 . A FRAMEWORK FOR ONLINE NOVELTY DETECTION WITH CONFIDENCE
We first reflect on the concept of novelty , as well as that of novelty detection , philosophically . First of all , novelty is always a relative concept with regard to our current knowledge . Therefore , novelty should be defined in the context of a representation of our current knowledge . Such representation can be a database , a knowledge base , or a model . In our framework , we prefer representing our knowledge with a model , simply because of its mathematical neatness . Moreover , in online applications , it is desirable that the representation of our knowledge can also be updated with the acquisition of new data . Second , there is generally no clear cut line between novel events and normal events in real world application . Therefore , it is attractive to associate with each novel event a value to characterize how confident we are about our judgment . Finally , in a real world environment with noise the novel events in a temporal sequence are generally associated with segments , instead of individual time points . These understandings lay the foundation of a formal formulation of an online novel detection framework , which is defined in the remaining part of this section .
N
In this paper , a temporal sequences is represented as
( )tX , = L . They can be time series , video sequences , or 1t ( )tX is a is employed to represent one of its where some other objects that can be indexed by time t . stochastic process , and realizations .
( )tx
As we mentioned previously , we utilize a model to represent our knowledge about the underlying temporal sequence up to 0t . This model can be a physics based model provided by domain experts , or a model constructed from available data ( )tx , where
0( )txM
= L . 1t t
0
Surely , it is impossible for us to utilize a model with finite number of parameters to completely represent the information embedded in a random temporal sequence . Also , each model may only be good at representing certain types of information , because different models have different sensitivity to different type of information . For example , some models are more sensitive to extreme values , while some other models are more sensitive to unusual patterns . However , this selective property of model based knowledge representation , when applied to novelty detection , can become an advantage . In a particular application , we may only be interested in some types of novelties among all of the different possible novelties . Thus , utilizing different models provides us with the opportunities of selecting different types of interested novelties . Definition 1 ( Matching Value and Matching Function ) The matching function , denoted as xM function that can quantify how well the model temporal sequence . The match value defined as t , is a x ( ) ) ( 1 ) , 0 matches the 0( )txM ˛ R , is , where )V t 0(
)V t 0( .
( Ft
=
0
VtFt ( )( 00
( 1 ) ,
( ) ) t xM 0 x
In other words , the matching value how well our knowledge about the time series at represented by . 0( )tx is employed to measure t , 0 1
, can describe the instance
0( )V t
0(1)t xM
Definition 2 ( Occurrence ) Denoted by
0( )O t
, occurrence at 0t is defined as e e 0
( ))}
( ){ ( OtI Vtt 0
)(( ) , t 00 { }I g is the indicator function , and
( 1 ) > is a 0 where predefined tolerance width . can be determined based on the noise level , as well as the )te 0( precision requirement of the underlying problem . Note that 0( )O t is a random variable . te 2 ( ) 0
Definition 3 ( S urprise ) A surprise is observed if
O t = . 0( ) 1
That is , a surprise happens when a new instance outside of the tolerance range t 0 Definition 4 ( Event and Event Duration ) e ( ) , t 0 e ( ) )
.
(
Denoted by
0( nE t
)
, an event at time 0t is defined as
0( )tx falls
( )( ) 000
( 1)(1 ) n
[ = nEtOtOtOt where n is called the event duration . The 1 norm of denoted as
++ L
. That is
]
T
0
0( nE t
)
( 2a )
0( nE t
) is
( )( EtOt 0 n
= i n ) = i
1
0
+
0
( 2b )
) n n
)
)
)
)
.
) net
0( )
Ot
0(
+=
( ) ) 0
( or
( or
0( ne t ne t 0(
0( nE t
0( nE t
0( nE t
0( nE t
) is denoted as the occurrences is the number of surprises happened in the event
) is represented as i+ in ( 2b ) because 0( Ot ) 0( nE t nE t 0( Event duration n is a predetermined algorithmic parameter . We i+ omit the absolute operation on ) ) is nonnegative according to ( 1 ) . is a random binary vector with at most 2n different realizations , while is a random variable with at most n+1 different realizations . A realization of ) . The discrete nE t 0( density function of , ( pe t nE n= L . The formulation of where can be pe t ( 0 ( ) ) nE 0 L . For example , determined by the occurrences 1 n Otii 0(),0 L += if identical {(),01}Otii independent Bernoulli variables , becomes a binomial ) random variable . will be different if the occurrences ( ) ) 0 in are interdependent , such as following a Markov chain distribution . Definition 5 ( Novel Event with Confidence ) Given a confidence level is defined as a novel event with confidence satisfies ( a ) E ( )max( ,{()} ) 0 n fixed lower bound of nE t 0( . ( 3b )
, where { }E g is the mean , h is a with h ˛ N , and ( 3a )
, Event , if 0( c t pe t ( nE L n 0( nE t
0( ne t 0( ne t ethE t>
, where
{(),01}Otii are
)(0,1 )
0( c t
0( c t
( b )
+= in
0 )
) ) n
< ( ) ) 1 ( ( petc t nE n
0
)
0
0 n
)
0
) n
)
” ˇ ( cid:229 ) ˛ The condition ( 3a ) makes sure that at least certain number of surprises happen in the event , while the condition ( 3b ) ensures that the probability for the number of surprises to happen in the event is small enough to satisfy our confidence level . The h is an algorithmic parameter to define the lower 0( c t bound of the number of surprises .
0( ne t
0( ne t
)
)
)
0
)
,
( ) ) x xM
0( nE t t(1 ) , 0
0( )txM temporal sequence
Four items in this framework need to be instantiated before it to represent becomes a concrete algorithm . ( a ) The model ( )tX ; ( b ) The matching function the to quantify the disagreement between the Ft ( ( )tX observation at 0t ; model output and the temporal sequence 0t ; ( d ) The discrete density ( c ) The tolerance width = L . In Section 4 , function of 0m ( pe t nE a concrete algorithm is derived by instantiating all these four items in the framework .
3 . A BRIEF INTRODUCTION TO SUPPORT VECTOR REGRESSION at , where
2 ( )te 0 ( ) ) 0 regression
Because the algorithm proposed in Section 4 is extensively technique , called Support Vector based on a Regression ( SVR ) , we briefly introduce the basic concepts of SVR in this section . A more detailed presentation of SVR can be found in [ 16 ] . n n and to W and
˛x R , Given a training set iy ˛ R , we construct a linear regression function with regard
= ,),1} l
, where
= Tyi
{( x
D i i i
T
( 4 ) x ( ) + = ( )( ) x W x f where W and space F . Meanwhile , function , which maps ( 4 ) are obtained by solving an optimization problem : b x are vectors in a huge dimensional feature ( ) x can also be considered as a mapping ( ) ˛x R to a vector in F . The W and b in
D min( W b
,
P
( ( ) sty
1 i l
T
T
=
C
+ W W
1 = ) 2 +£ + W x b ) i + £ + ) b W x ( ( ) = x x i 1 ,0 , i i
T
*
+ x i
* x i x e i e x i l
* y i L
The optimization criterion penalizes data points whose yvalues differ from f ( x ) by more than e . The slack variables x and x * represent the size of this excess deviation for positive and negative deviations respectively . Introducing Lagrange multipliers a and a* , we get : min()( , a a
*
=
D
1 2
) l l
= i
1
= j
1
Q iji a a a a
* i j j l
++eaaa a
( iiii
= 1 i
0,1,( ) stCi aaa a 0 l
£=* iii i
*
) i
= l
= i
1 y
(
L where
=F
Q ij
= ( ) ( )( xxx x T j iji
,
)
K
*
*
) l
*
( 6 ) i
= 1 . Here
K x x is a kernel
(
)
, i j
( 5 )
) yx t
0
) =+== ( 1)()( tfK 0
, xx
) t b x 0 DiD
1 q t t
0
( 9 )
0 t
+(cid:229 )
D = t function [ 16 ] . Given the solution of ( 6 ) , the regression function ( 4 ) can be written :
= ( )( , xx x fK i l ) = 1 bq i
+ i
( 7 )
= j i i
D
,
)
(
* i q a a i
˛x R if the kernel function iq in ( 7 ) are finally nonzero . Those samples
. ( 7 ) is a nonlinear function with where the coefficient regard to K x x is chosen as a nonlinear function . Normally , only a small fraction of the coefficients ix with nonzero iq are called support vectors of the regression function , because it is these critical samples in the training set T that solely determine the formulation of ( 7 ) .
4 . AN SVR BASED ONLINE NOVELTY DETECTION ALGORITHM
In this section , we instantiate the four items in the framework listed in Section 2 to devise a novelty detection algorithm . First , SVR is employed to model a temporal sequence
( )tX . SVR has quite a few attractive features . For example , ( a ) the regression function f x obtained by SVR can approximate any ( ) nonlinear relationship between the input vector x and its associated target value y if a proper kernel function K x x is chosen , ( b ) SVR has good generalization properties , and ( c ) SVR can handle high dimensional data efficiently .
(
)
, i j t
(
)
){( t 0
( )tx L xL
More sp ecifically , given a realization of a temporal sequence , = L , we can construct a set of training or , where 1t ( )tx samples from 0( DT t Ttt D t== ,),1}t x y D 0 D =where called the embedding dimension of the training set According to Section 3 , from the training set algorithm can construct a regression function
, and D is . ) SVR training
, ( 8 )
+ tD [ (1)( ) ]
0( DT t
+ ( 1 ) t xx t D
=
, x y t
T
0 t
0( ) DT t ( )t f x 0 D
1 Naturally , the model
0( )txM ( )tX can be defined as sequence
== 0()()( xMxx x tfK
, t ) 0 DiD
0
+ t
1 t b q D = i E suggests representing the temporal t
0
( 10 )
( 10 ) that model
Equation essentially 0( )txM incorporates all the points of the temporal sequence , where ( )tx = L , and thus is a good candidate for representing our 1t acquired knowledge of t
0
( )tX .
Second , the matching function and the matching value are defined as ( )( VtFt 00
= ( 1 ) , ( ) ) M t 0 x = =()(1)()( xMx ttt 000 x
) t x 0 x
)
( 11 )
F F F F F F ‡ ( cid:229 ) £ ( cid:229 ) ( cid:229 ) ( cid:229 ) ( cid:229 ) ( cid:229 ) F ( cid:229 ) ( cid:229 ) Equation ( 11 ) indeed suggests that the matching value the residual of the regression function ( 9 ) at 0t .
0( )V t is at
2 ( )te 0
Third , we need to define the tolerance width
0t . Fortunately , SVR formulation generally adopts an e insensitive loss function [ 16 ] , which possesses exactly the same flavor as our tolerant range defined in Definition 2 . Thus , we just simply merge the concepts of two tolerant ranges together , and define the tolerant width in Definition 2 as 2e for any 0t , where e is the insensitive parameter in the e insensitive loss function of SVR . A direct consequence of this definition is that any sample
( ) in will be a 0( )txM support vector in the updated model [ 16 ] . that turns to be a surprise to model
0( DT t
Dx t 0 y
)
, t
0 xM
0(1)t +
)
0( nE t
Finally , in order to make our algorithm theoretically tractable , we simply define as a sequence of independent Bernoulli random variables with the same parameter , and the discrete density function of , can thus be formulated as [ 12 ] pe t E n
( pe t nE
0( nE t
( ( ) ) n
( ) ) 0
=
,
)
0 n n e t ( 0 n
) etn e t ) n
( )( ( )(1( ) ) qtq t 0 0
,
0 n
0
( 12 ) where et n
(
0
= ) 0
L n
0 , otherwise where nE t 0( ) as ) q t (
)
=
0
) q t 0( to be a surprise . The is the probability of an occurrence in the event can be approximately estimated
0( q t
)
, ( 13 )
)
SVN t ( 0 t D 0
) is the number of support vectors in model
0( SVN t is the number of training samples in
, 0( )txM according
0( DT t
) where and 0t Dto ( 8 ) .
This definition is valid under the following two assumptions ( a ) The occurrences in an event are independent ; ( b ) All occurrences in an event have approximately the same probability of being a surprise . Intuitively , the first assumption is reasonable if the regression function ( 9 ) can sufficiently capture the dependent relationship in a temporal sequence . This can be achieved by an adequate training stage to fully build the model before it is utilized for detection . The second assumption )txM 0( is sensible if the event duration n is not too large . More discussion of the role that event duration n plays in the algorithm will be presented in Subsection 51 Failure of meeting either assumption deteriorates the accuracy of the confidence level associated with the detection output .
We now have a complete online novelty detection algorithm has been proposed based the framework in Section 2 . This algorithm requires a set of algorithmic parameters , which include ( a ) Embedding dimension D ; ( b ) Event duration n ; ( c ) Tolerant K x x ; ( e ) Confidence level c ; width 2e ; ( d ) Kernel function
(
)
, i j and ( f ) Fixed lower bound of the number of surprises h . The algorithmic procedures can be summarized as follows : 1 ) At training stage , i ) When a new point in the temporal sequence available , the training set following ( 8 ) . Accordingly , the model updated to 0( DT t based on the
0( )txM
0(1 ) DT t
)
0(1)t xM . will be updated to
0( )tx becomes
) will also be
0( DT t ii )
If training stage does not finish , go to step i ) . Otherwise , move to the detection stage .
2 ) At detection stage , i ) When a new point in the temporal sequence 0( )V t available , calculate the matching value
0( )tx becomes following ( 11 ) . ii ) Based on
0( )V t and the tolerant width 2e , determine the value of occurrence
0( )O t following ( 1 ) . inEt
0
)
)
1
0
.
= to nEt v )
0(1 ) xM
0(1)t n + n +
0( DT t
)txM 0( iv ) The training set and ends at is then updated to iii ) Based on L n 1 the values of occurrence 0( Ot , determine the value of event is updated by following ( 8 ) . Accordingly , based on
, where ) n + i 0(1 ) meets the conditions in ( 3 ) , a following ( 2 ) . If 0t , is novel event , which starts at t detected with a confidence of c .
0( )tx
0(1 ) DT t considering the new point the model DT t 0( If the detection stage does not finish , go to step i ) . Otherwise , quit . Finally , it is worth mentioning that the SVR training algorithm introduced in Section 3 is a batch algorithm . That is , whenever a new sample is added into the training set , the existing regression function can only be updated by retraining the whole training set , which is not an efficient way to implement our detection algorithm . Fortunately , we have recently derived an incremental SVR training algorithm [ 11 ] , which can efficiently update the regression function whenever a sample is added to or removed from the training set .
5 . TWO VARIANTS OF THE NOVELTY DETECTION ALGORITHM
When we apply this algorithm to particular real world scenarios , it is sometimes necessary to fine tune some parts of the algorithms to meet the special requirements of an application . In this section , we propose two variants of the original algorithm .
5.1 Robust Online Novelty Detection
The event duration n is a critical parameter in the detection algorithm . In some cases where the novel events are outstanding , the original algorithm is not sensitive to event duration n . This can be illustrated by experiments in Section 6 . However , in the other cases , an improper choice of this parameter does lead to the deterioration of the algorithmic performance . Meanwhile , it is
( cid:236 ) ( cid:230 ) ( cid:246 ) ( cid:239 ) ( cid:231 ) ( cid:247 ) Ł ł ( cid:239 ) ( cid:239 ) ( cid:237 ) ( cid:239 ) ( cid:239 ) ( cid:239 ) ( cid:238 ) hard to know in advance what kind of novel events will be detected in a temporal sequence , which is indeed the nature of novelty detection .
Although , given a particular problem , how to select an optimal event duration n for the original detection algorithm is still an open topic , in some applications it is possible for us to know the range in which an optimal event duration n may fall in . Based on this assumption , a robust version of the original detection algorithm can be intuitively devised . In this variant , we evenly pick up r different event duration n ’s from the available range , and apply each of them to the original detection algorithm , and thus obtain r detection outputs . The final robust detection output is obtained by a voting procedure among all the generated outputs . When implementing this idea , it is not necessary to literally repeat the original detection algorithm for each event duration n . Repeating merely the step iii ) at the detection stage for each event duration n is adequate .
5.2 Fixed Resource Online Novelty Detection One problem with our original detection algorithm is the longer the prediction goes on , the bigger the training set
0( ) DT t will become , and the more SVs will be involved in the SVR regression function ( 9 ) . In some environments with limited memory and computational power , it is possible to stress out the system resources with the complexity of the SVR model ( 9 ) growing in this way . One way to deal with this problem is to impose a “ forgetting ” time W . When training set grows to this maximum W , then the SVR model ( 9 ) will first be trained to remove the oldest sample before the next new sample is used to update the model . Accordingly , the in ( 13 ) becomes ) ( q t is still the number of support
, where
SVN t 0(
0( DT t
0( q t
=
)
)
)
)
)
0
( SVN t W
0 vectors in model
0( )txM
.
This variant is intrinsically suitable for non stationary temporal sequences , as it can be updated in real time to fit the most recent behavior of a temporal sequence .
6 . EXPERIMENTS
Experiments based on both synthetic and measured data are presented to demonstrate the performance of our algorithm . According to our knowledge , a comparable automatic online novelty detection algorithm supported by confidence is still not available in other literatures when we prepare this paper . Therefore , it is difficult for us to implement comparative experiments to justify our performance . Thus , we here do it by comparing our detection results with visual detection by humans .
6.1 Experiments Based on Synthetic Data
Three synthetic time series are generated from the following stochastic processes respectively .
X
=+ p 40 + ( )sin()()()( ) ttntete t 31 N
2
+
( 14c )
N =
= L , 1t N where noise with zero mean and a SDT of 01 novel events ,
, and
1200
( )n t is an additive Gaussian e t are two 2 ( ) e t and 1( )
( cid:236)= ( cid:237 )
( ) e t 1 t nt ( ),[600,620 ] 1 0 , otheriwse where
1( )n t follows a normal distribution of
N
( 0,0.5 )
.
( cid:239)= ( cid:237 )
( ) e t 2
0.4*sin(),[820,870 ] t t p 40 N
0 , otheriwse
We use the first 400 points in the three time series to train our models , and conduct the novelty detection on the remaining 800 points . The algorithmic parameters are arbitrarily set as ( a ) Embedding dimension D=8 ; ( b ) Event duration n=6 ; ( c ) Tolerant width 2e =0.2 ; ( d ) Kernel function ; ( e ) Confidence level c = 95 % ; ( f ) Fixed lower bound of number of surprises h =n/2 . The experimental results are demonstrated in Figure 1 .
=)exp{ x i
, xxx iji
K
}
(
2
The blue curves in Figure 1 are the synthetic time series , and the peaks on the red lines indicate the positions and durations of the detected novel events . Because the first 400 points of each time series are taken out for training the support vector regression functions , no detection outputs are produced at that segment .
Figure 1 . Experimental Results on Synthetic Time Series
1( )tx and
2( )tx
The plots in Figure 1 show that , our detection algorithm successfully detects all the novel events in . 3( )tx Meanwhile , our algorithm properly figures out that no any part of the can be considered as a novel event . As suggested in Subsection 5.1 , because the novel events in these synthetic time series are fairly distinguishable , the detection output of the original detection algorithm is not sensitive to the choice of event duration n . It can be shown that the same detection output can be produced when event duration n is set to 8 or 10 .
X 1
X
2
= ttn t ( )sin()( ) p 40 N p 40 + ( )sin()()( ) ttnte t N
=+
+
( 14a )
( 14b )
1
6.2 An Experiment Based on Measured Data The experiment is to apply the original detection algorithm to the famous Santa Fe Institute Competition data , which is 1000point time series . We define the first 200 points as the train
˛ ( cid:238 ) ( cid:236 ) ˛ ( cid:239 ) ( cid:238 ) segment , and the remaining 800 points as the detection segment . The algorithmic parameters are set exactly the same as the experiments done in Subsection 61 The time series , along with the detection results , are plotted in Figure 2 .
Figure 2 . Experimental Results on Santa Fe Institute Time
Series
In this experiment our algorithm claims that two novel events happen in the detection segment of the time series . It is easy to visually examine its validity . Also , similar to the experiments done in Subsection 6.1 , this Santa Fe Institute Time Series is also insensitive to different choice of event duration n . The same result can be obtained if we set the event duration n to 8 or 10 .
7 . CONCLUSIONS
This paper proposes a new direction for online novelty detection on temporal sequences . Primitive experimental results demonstrate the promising performance of this algorithm .
Meanwhile , many topics brought up by this paper are still open . For example , we notice that some relationship exists among the algorithm parameters , such as event duration n , tolerant width 2e , and confidence level c . However , we still cannot figure out a method to make use of this relationship to define a set of optimal algorithmic parameters . Also , intuitively , for some temporal sequences , Markov chain can be a better model than Binomial distribution to describe the relationship among the occurrences . Thus , how to implement this intuition is another direction )O t 0( worth future investigation . Surely , new detection algorithms can also be devised by employing different temporal sequence models , such as data clustering and one class support vector machine .
8 . ACKNOWLEDGEMENTS
Most of this work was supported by the NASA project NRA00 01 AISR 088 . The final part of it was supported by Aureon Bioscience Corp .
9 . REFERENCES [ 1 ] Bishop , C . M . , Novelty Detection and Neural Network Validation , IEE Proceedings Vision , Image and Signal Processing , vol . 141 , no . 4 , pp . 217 222 , August , 1994 .
[ 2 ] Brotherton , Tom , Tom Johnson , and George Chadderdon , Classification and Novelty Detection Using Linear Models and a Class Dependent Elliptical Basis Function Neural Network , in Proceedings of the International Conference on Neural Networks , Anchorage , May 1998 .
[ 3 ] Campbell , Colin , Kristin P . Bennett , A Linear Programming Approach to Novelty Detection , in Advances in Neural Information Processing Systems , vol 14 , 2001 . in Time Series Data Using
[ 4 ] Dasgupta , Dipanker , and Stephanie Forrest , Novelty Detection from Immunology , In Proceedings of the 5th International Conference on Intelligent Systems , Reno , Nevada , June 1921 , 1996 .
Ideas
[ 5 ] Guralnik , Valery , Jaideep Srivastava , Event Detection from Time Series Data . In Proceedings of the International Conference Knowledge Discovery and Data Mining , San Diego , California , 1999 .
[ 6 ] Isermann , Rolf , Process Fault Detection Based on Modeling and Estimation Method A Survey , Automatica , vol . 20 , pp.387 404 , 1984 .
[ 7 ] Jagadish , H . V . , N . Kouda , and S . Muthukrishnan , Mining deviates in a time series database , in Proceedings of 25th International Conference on Very Large Data Bases , pp 102113 , 1999 .
[ 8 ] Keogh , E . , S Lonardi , and W Chiu , Finding Surprising Patterns in a Time Series Database In Linear Time and Space , In the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pp 550 556 , Edmonton , Alberta , Canada , July 23 26 , 2002 .
[ 9 ] Kozma , R . , M . Kitamura , M . Sakuma , and Y . Yokoyama , Anomaly Detection by Neural Network Models and Statistical Time Series Analysis , in Proceedings of IEEE International Conference on Neural Networks , Orlando , Florida , June 27 29 , 1994 .
[ 10 ] Lauer , Martin , A Mixture Approach to Novelty Detection Using Training Data With Outliers , Lecture Notes in Computer Science , vol . 2167 , pp . 300 310 , 2001 .
[ 11 ] Ma , Junshui , James Theiler , and Simon Perkins , "Accurate Online Support Vector Regression," to appear in Neural Computation , 2003 .
[ 12 ] Mood , A . M . , F . A . Graybill , and D . C . Boes , Introduction to The Thoery of Statistics , 3rd Edition , McGraw Hill , Inc , 1974 .
[ 13 ] Roberts , S . , and L . Tarassenko . A Probabilistic Resource for Novelty Detection , Neural
Allocating Network Computation , vol . 6 , pp . 270 284 , 1994 .
[ 14 ] Schölkopf , B . , RC Williamson , AJ Smola , J . ShaweTaylor , and J . Platt . Support vector method for novelty detection . In Neural Information Processing Systems , 2000 . [ 15 ] Shahabi . C . , X . Tian , and W . Zhao , Tsa tree : A Waveletbased Approach to Improve the Efficiency of Multi level Surprise and Trend Queries . In Proceedings of 12th International Conference on Scientific and Statistical Database Management , 2000 .
[ 16 ] Smola , A . J . , and B . Scholkopf ( 1998 ) . A Tutorial on Support Vector Regression , NeuroCOLT Technical Report NC TR98 030 , Royal Holloway College , University of London , UK . [ 17 ] Ypma , Alexander , and Rober P . Duin , Novelty Detection Using Self Organizing Maps , in Progress in ConnectionistBased Information Systems , pp 1322 1325 , London : Springer , 1997 .
