Similarity Analysis on Government Regulations Gio Wiederhold Gloria T . Lau Stanford University Stanford University
Kincho H . Law Stanford University
Computer Science Dept . Stanford , CA 94305 9040 gio@dbstanfordedu
Dept . of Civil & Environmental Eng .
Dept . of Civil & Environmental Eng .
Stanford , CA 94305 4020 glau@stanford.edu
Stanford , CA 94305 4020 law@stanford.edu
ABSTRACT Government regulations are semi structured text documents that are often voluminous , heavily cross referenced between provisions and even ambiguous . Multiple sources of regulations lead to difficulties in both understanding and complying with all applicable codes . In this work , we propose a framework for regulation management and similarity analysis . An online repository for legal documents is created with the help of text mining tool , and users can access regulatory documents either through the natural hierarchy of provisions or from a taxonomy generated by knowledge engineers based on concepts . Our similarity analysis core identifies relevant provisions and brings them to the user ’s attention , and this is performed by utilizing both the hierarchical and referential structures of regulations to provide a better comparison between provisions . Preliminary results show that our system reveals hidden similarities that are not apparent between provisions based on node content comparisons .
Categories and Subject Descriptors H31 [ Information Storage and Retrieval ] : Content Analysis [ Database and Management ] : Database Applications – data mining ; J.1 [ Administrative Data Processing ] : Law . linguistic processing ; H28
Indexing –
Keywords Regulations , Similarity Analysis , Legal Informatics , Text Mining .
1 . INTRODUCTION Government regulations are an important asset of our society ; ideally , they should be readily available and retrievable by the general public . Industrial productivity can be greatly increased if tools are provided locating and understanding regulations . For instance , building designers , although more knowledgeable than the general public , have yet to search through the continuously changing provisions and locate the relevant sections resolve potential ambiguities in their provisions . Inspectors have to go through a their projects , to aid related then in to similar evaluation process before a permit can be approved . The inherent nature of multiple issuing agencies also deserves attention . Regulations are typically specified by Federal as well as State governmental agencies and are amended and regulated by local counties or cities . These multiple sources of regulations sometimes complement and modify each other , and at times contradict one another . Designers often turn to reference handbooks that are independent of governing bodies ; as a result , the regulations , amending provisions and interpretive manuals together create a massive volume of semi structured documents with potential differences in formatting , terminology and context .
1.1 The Need for Regulatory Information Management To illustrate some of the research issues in legal informatics and the need for it , we present two examples below . The first example shows two provisions regulating curb ramps in accessible parking stalls [ 9 ] . The California Building Code ( CBC ) [ 7 ] allows curb ramps encroaching into accessible parking stall access aisles , while the Americans with Disabilities Act ( ADA ) Accessibility Guidelines [ 1 ] disallows encroachment into any portion of the stall . Here one provision is clearly more restrictive than another , making compliance a non trivial task without the knowledge of the existence of related provisions . Example 1 ADAAG Appendix A463 Parking Spaces
… a curb ramp opening must be located within the access aisle boundaries , not within the parking space boundaries .
CBC 1129B43 Equivalent facilitation for parking arrangements
Exceptions : Ramps located at the front of accessible parking spaces may encroach into the length of such spaces …
Example 2 presents two directly conflicting provisions from the ADAAG and the CBC . This conflict is due to the fact that the ADAAG focuses on wheelchair traversal while the CBC focuses on the visually impaired when using a cane , and is captured by the clash between the term “ flush ” and the measurement “ ½ inch lip beveled at 45 degrees ” . In his interpretive manual to California accessibility regulations , Gibbens [ 9 ] points out that “ when a state or local agency requires you to construct the California required ½ inch beveled lip , they are requiring you to break the federal law ” , and this clearly should be brought to the user ’s attention .
In this paper , we describe a regulatory document mining system that utilizes the structure of regulations to enhance a similarity comparison between sections . A brief literature review is presented in Section 2 ; feature extraction , which is one of the key elements of the proposed regulation analysis model , follows in Section 3 . Our similarity analysis is presented in Section 4 , and preliminary results are shown in Section 5 . Section 6 gives a brief discussion on future tasks . Example 2 ADAAG 472 Slope
Transitions from ramps to walks , gutters , or streets shall be flush and free of abrupt changes
CBC 1127B55 Beveled lip
The lower end of each curb ramp shall have a ½ inch ( 13mm ) lip beveled at 45 degrees as a detectable way finding
2 . RELATED WORK To aid legal research , one can use traditional textual comparison techniques from the field of Information Retrieval ( IR ) , such as the Boolean model or the Vector model [ 2 ] , with most being bagof word type of analysis ( ie word order insensitive ) . This is insufficient since it ignores the structure of regulations , namely that 1 ) regulations are organized into deep hierarchies , 2 ) sections are heavily cross referenced , and 3 ) terms are well defined within regulations . A good similarity analysis tool for a legal corpus should make use of the structure of regulations mentioned above to provide a better comparison . In addition , traditional IR techniques do not cater to our future development of conflict identification ( which is not discussed in this paper ) . Assuming that the contents of the conflicting sections are related , conflict analysis builds upon a solid similarity comparison between documents , and it requires a deeper understanding of documents rather than the traditional bag of word type of similarity analysis . Feature extraction provides some help to this end . Feature extraction is an important step in repository development when the data dimension is large . It is a form of pre processing , eg , combining input variables to form a new variable , and most of the time features are constructed by hand based on some understanding of the particular problem being tackled [ 3 ] . Automation of this process is also possible ; in particular , in the field of information retrieval , software tools exist to fulfill “ the task of feature extraction … to recognize and classify significant vocabulary items ” [ 3 ] . The IBM Intelligent Miner for Text [ 8 ] and the Semio Tagger [ 17 ] are both examples of fully automated key phrase extraction tools . In addition to comparing the body text of provisions , the heavily referenced nature of regulations provides extra information about provisions , and link analysis [ 5 ] is the natural improvement to the similarity measure . Academic citation analysis [ 4 ] is closest in this regard ; however the algorithm cannot be directly transported to our domain . Citation analysis assumes a pool of documents citing one another , while our problem here are separate islands of information where within island documents are highly referenced ; across islands they are not . We are therefore in search of a different algorithm that will better serve our needs .
3 . REPOSITORY DEVELOPMENT In order to develop a prototypic system , we focus on accessibility regulations , whose intent is to provide the same or equivalent access to a building and its facilities for disabled persons . Our corpus currently includes two Federal documents : the ADAAG [ 1 ] and the Uniform Federal Accessibility Standards ( UFAS ) [ 18 ] . In addition , parts of the International Building Code ( IBC ) [ 11 ] is included to reflect the similarity and dissimilarity between federal and private agency mandated regulations . Related sections from the British Standard BS8300 [ 6 ] are included as well to show the difference between American and European regulations .
3.1 Data Consolidation and Categorization Before regulations can be compared , documents are consolidated to a unified format and features that identify similarity are extracted as shown in Figure 1 . As for data format conversion , it suffices to say that a shallow parser is developed to consolidate different documents into eXtensible Markup Language ( XML ) [ 19 ] for its capability to handle semi structured data . The hierarchy of regulations is maintained by properly structuring the XML tags , for example , Section 1.1 is a child node of Section 1 , and is thus structured as a child element of Section 1 in the XML tree . References are extracted as well ; please see Section 324 for an example of a reference tag . As shown in Figure 1 , after the documents are parsed into XML format , features are extracted and added to the corpus as described in Section 32 Besides reading regulations based on its natural hierarchy , users might find it helpful to browse through an ontology [ 10 ] with documents categorized based on concepts as well . Semio Tagger is one of several software products that provide such a capability . It first identifies a list of noun phrases , or concept , that are central to the corpus through linguistic analysis . It also provides a concept latching tool to help knowledge engineer to categorize the concepts and create a taxonomy . Documents are thus clustered based on the taxonomy , and users can click through the structure to view relevant provisions classified according to concepts . regulations in HTML , PDF , plain text , etc shallow parser
XML regulations feature extractor exceptions measurements definitions features from regulations features from references/handbooks glossary terms author prescribed indices refined XML regulations
Semio concepts
Ontologist
OntoView
Figure 1 . Repository development schematic
3.2 Feature Extraction This process extracts from regulations the identified features that signal related or similar sections . Some of the features can be applied generically on other sets of regulations , while some are specific to the domain of accessibility ; for instance , numeric measurements only make sense in the domain of disabled access code but not in human rights law . In addition , what defines evidence in a certain domain of regulations is also subjected to the knowledge engineer ’s judgment . In this context , we strive to be as generic as possible , and all of the extracted features can be easily extended to other engineering domains as well .
Two different sources of features , namely features from within the regulation corpus and features from outside ( like those from reference books or engineering handbooks ) , are extracted with the help of software tools and parsers developed for this task . As shown in Figure 1 , features from within the corpus include exceptions , measurements , definitions and concepts , and features from outside domain , for example , engineering handbooks and references , provide domain specific glossary terms and authorprescribed indices . Each of the features will be discussed in the following sections with an example to illustrate the idea . An example with complete mark up of the features is shown in Section 324
321 Concept and Author Prescribed Index Tags The traditional Boolean model or Vector model in IR provides a mechanism for text analysis . Indexing the texts using all of the words , except stopwords ( which are very common terms ) , generates a huge multi dimensional space with one axis representing one word . Using singular value decomposition , in short SVD , as the dimensional reduction tool , similar words are pulled together as one reduced axis . However , it is still computationally intensive to perform SVD , and the initial sparseness of the matrix is destroyed after dimension reduction . In order to seek an alternative to the bag of word vector model and the SVD technique , we use concepts or key phrases , which are relatively simpler compared to traditional index terms and allow us to capture sequencing information on words . To extract noun phrases from the corpus , the software tool Semio Tagger is used to extract a list of concepts that are identified as important . In our case , the ADAAG and the UFAS together generate just over a thousand concepts . Each provision is tagged with its concepts along with the corresponding count of appearances of the concept ( num ) as shown below . To increase the number of matches , our system stems both the concepts and the texts in the provision with Porter ’s Algorithm [ 14 ] before matching . Below is an example of a concept and its count . <concept name="stationary wheelchair" num="2" /> Concepts are machine generated phrases that represent a good measure of important terms in the provisions . Another source of potentially important phrases comes from author prescribed indices at the back of reference books or even the regulation itself ; this type of human written information sometimes can be more valuable than machine generated phrases . Therefore , index terms from the accessibility chapter of the IBC [ 11 ] are tagged against the repository with identical syntax as a concept tag . A strict Boolean concept or index term match ignores synonyms which can convey important information at times , and work has been done to resolve terminological heterogeneity . As shown in [ 13 ] , a relatively high accuracy of concept matching is obtained by combining dictionary based and context based heuristics . As our corpus grows and so does the list of extracted concepts , matching techniques similar to this can be used to help consolidate future development of conflict identification . the vocabulary , which also aids our
322 Definition and Glossary Tags In regulation documents , there is often a designated section in an early chapter that defines the important terminologies used in the code , such as Section 3.5 in the ADAAG . These human generated terms are more likely to convey key concepts than machine extracted ones such as concepts shown above . In addition , the definition of a term gives the meaning to a term , which is useful for comparisons ; an example is shown below . <definition> <term> Clear </term> <definedAs> Unobstructed . </definedAs> </definition> Similarly , engineering handbooks always define in the glossary the important terms used in the field . For instance , the KidderParker Architects’ and Builders’ Handbook provides an 80 page glossary that defines “ technical terms , ancient and modern , used by architects , builders , and draughtsmen ” [ 12 ] . The difference between definition and glossary tags is that definition comes from the regulation itself , while glossary term comes from sources other than the regulation . Again the syntax is similar to that of definition tags .
323 Measurement Tag In accessibility provisions , measurements play a very important role ; in particular , they define most of the conflicts . For instance , one provision might ask for a clear width of 10 to 12 inches , while another one might require 13 to 14 inches . It is therefore crucial to identify measurements and the associated quantifiers if there is any . In our context , measurement is defined to be length , height , angle , and such . They are numbers preceding units . Quantifiers are noun phrases that modify a measurement , like “ at most ” , “ less than ” , “ maximum ” and so on . These can be reduced to a root of either “ max ” or “ min ” , for example , “ at most ” and “ less than ” are maximum requirements , thus both reduce to “ max ” . Our parser first identifies numbers followed by units , like the number 2 followed by the unit lbf as in 2 lbf . The quantifier is an optional attribute in the measurement tag and is identified if it appears in the vicinity of the measurement . Negation , if appearing right in front of the quantifier , is extracted as well and the final quantifier is reduced to its root “ max ” or “ min ” ; an example is shown below . <measurement unit="lbf" size="2" quantifier="max" />
324 Example with Complete Mark up An example is presented below with the complete set of feature mark ups that shows a typical provision from the UFAS , which contains exception , measurement , ref , concept and indexTerm tags in addition to the body text regText tag . All of the extracted information is encapsulated in a regElement node for each section . Example 3 Original Section 463 from the UFAS 463 Parking Spaces
… at least 96 in … and an adjacent access aisle EXCEPTION : If accessible parking spaces for vans
Refined Section 463 in XML format <regElement name="ufas463" title="parking spaces">
<concept name="access aisle" num="3" /> <indexTerm name="accessible circulation route" num="1" /> <measurement unit="inch" size="96" quantifier="min" /> <ref name="ufas45" num="1" /> … <regText> Parking spaces for disabled people </regText> <exception> If accessible parking spaces for </exception>
</regElement>
4 . SIMILARITY ANALYSIS As pointed out in the Introduction , it is rather difficult for anyone to locate any desired material within the jungle of regulations available . Even upon finding a relevant provision for a particular design scenario , clients have to search multiple codes with multiple terms to locate yet more related provisions if there are any . Thus , our goal is to provide a reliable measure of relatedness for pairs of provisions , and to suggest similar sections of a selected provision based on a similarity measure . Here , since a typical regulation can easily exceed thousands of pages , we do not attempt to compare a full set of regulations against one another ; rather , a section or a provision from one set of regulation is compared with another section from another set , such as a comparison between Section 4.3(a ) in ADAAG and Section 3.12 in UFAS . re fin e d XM L re g u la t io n s
S im ila rity A n a ly s is C o re fe a tu re m a tc h in g e x c e p tio n s g lo s s a ry te rm s b a s e s c o re n e ig h b o r in c lu s io n m e a s u re m e n ts re fin e d s c o re a u th o r p re s c rib e d in d ic e s re fe re n c e d is trib u tio n d e fin it io n s fin a l s c o re re la t e d p a irs tra s h b e lo w th re s h o ld p a irs
C o n flic t A n a ly s is C o re
Figure 2 . Similarity analysis core schematic
A schematic is shown below in Figure 2 for the similarity analysis core , which takes as an input the parsed regulations and the associated features , and produces as a result a list of the most similar pairs of provisions . The dissimilar pairs are discarded while the most related pairs form the analysis basis for conflict identification ( which is not discussed in this paper ) . The goal of the similarity analysis core is to produce a similarity score , denoted by f ∈ ( 0 , 1 ) , per pairs of provisions . As mentioned in Section 2 , our system combines feature matching with the structure of regulations to provide a better comparison in a legal corpus . The process starts with an initial similarity score obtained by feature matching , and the neighboring nodes are compared as well to modify their initial score . The influence of the not soimmediate neighbors is taken into account by a process called Reference Distribution . Details of each process follow in Sections 4.1 through 42
4.1 Base Score f0 The base score f0 is a linear combination of the scores fi from each of the features i . Scores from features can be weighted differently but for now equal weights are assigned to all features as in Equation 1 . The scoring scheme for each of the features essentially reflects how much resemblance can be inferred between the pair of sections based on that particular feature . Here we take concept matching as an example . f0 = ( Σi = features fi ) / # features i
( 1 )
Concepts are used exactly like the index terms in the vector model [ 15 ] , where the degree of similarity of documents is evaluated as the correlation between their index term vectors that represent the weights for each index term in the document . The regulations are indexed against these concepts . Each provision is represented as a k entry vector where k is the total number of concepts . A technique similar to the tf×idf measure [ 16 ] is used for normalization , where term frequency ( tf ) is replaced by concept frequency for intra cluster similarity , while the inverse document frequency ( idf ) remains the same to account for inter cluster dissimilarity . The formula to compute the idf component is taken to be log(n/ni ) where n is the total number of sections , and ni is the number of sections the particular concept appears . For two sections , the similarity score fconcept is obtained by comparing concepts given by the cosine similarity between the two concept vectors . Since the cosine similarity is normalized , it always produces a score between 0 and 1 . Scoring schemes for other features follow the same idea .
4.2 Refined Scores Score refinement utilizes the tree structure of regulations to refine the base score f0 between provisions in order to obtain a better and more complete comparison . The immediate neighbors of a node , ie , the parent , siblings and children of a provision A , are collectively termed the psc of A . To help define the terms in a solid sense , we take sections A and B as our point of comparison . By comparing the neighbors of A and B , additional similarity evidences might be revealed ; therefore section A itself is first compared with psc(B ) , and vice versa , to produce the score fs psc based on the initial score f0(A , B ) . The next refinement takes into account the comparison between psc(A ) and psc(B ) , which gives the score fpsc psc . The final score frd comes from reference distribution , which compares the referenced sections . Each step is briefly discussed in the follow sections . Before discussing the details of each refinement techniques , it is crucial to understand the assumption here : we are only interested in increasing the identified similarity but not reducing it . Thus , in the following sections we only consider neighbors or referenced sections that already have higher similarity scores than the pair of interest . The validity of this assumption is built upon what we intend to achieve , and in the case of legal informatics we aim to provide the end user with related provisions and possibly conflicting ones in the future . As a result , it is best to include as much evidence as possible to increase the number of matches , which explains why we are only interested in increasing the similarity score but not decreasing it . For instance , if two sections are entirely the same , but embedded in two completely different neighborhoods , it is important not to decrease their similarity score such that the end user is presented with all relevant provisions .
421 Neighbor Inclusions : Self vs . Psc We use an empirical formula to update the score from f0 to fs psc based on the near neighbors in the regulation tree . Starting from f0 , the comparison between a pair of provisions ( A , B ) is first refined by comparing the self node , ie node A , with the immediate surrounding of the other interested node , ie psc(B ) , and vice versa , to obtain fs psc(A , B ) . Here we are only interested in s psc scores higher than what A and B already share in f0 in order to reveal greater similarity from the neighbors . We have
Set S = f0(A , psc(B ) ) U f0(psc(A ) , B ) N = sizeof(S ) δGT = Σs>f0(A , B ) ( s – f0(A , B) ) , s ∈ S αs psc = discount factor of update if ( N != 0 ) fs psc(A , B ) = f0(A , B ) + αs psc × ( δGT / N ) else fs psc(A , B ) = f0(A , B )
Here , set S is the set of similarity scores between section A and psc(B ) , and between psc(A ) and section B . The total δGT sums over all s in set S which is greater than the original score ; thus δGT / N represents the average greater than score . Clearly α is always less than one , following our intuition that self self comparison is more important than self psc comparison .
Set S = fpsc psc(ref ( A ) , ref ( B ) ) N = sizeof(S ) δGT = Σs>fpsc psc(A , B ) ( s – fpsc psc(A , B) ) , s ∈ S αrd = discount factor of update if ( N != 0 ) frd(A , B ) = fpsc psc(A , B ) + αrd × ( δGT / N ) else frd(A , B ) = fpsc psc(A , B )
5 . RESULTS Preliminary results are obtained by taking the score from concept match as the base score , and the discount factor α is taken to be 0.5 for all cases . Due to the volume of documents involved , a number of sections from different regulations are randomly selected for comparison to assess system performance .
422 Neighbor Inclusion : Psc vs . Psc Based on fs psc , the second refinement is to account for the influence of psc psc on sections A and B . Here psc(A ) is compared against psc(B ) to refine f0(A , B ) , which implies that another layer of indirection is inferred and thus the weight of pscpsc should be less than that of s psc . We have
4131 4132
4133
UFAS
4.13 Doors
41312
BS8300
1254 Doors
12541 parent sibling
4139
Door Hardware
12542
Door Furniture
Set S = fs psc(psc(A ) , psc(B ) ) N = sizeof(S ) δGT = Σs>fs psc(A , B ) ( s – fs psc(A , B) ) , s ∈ S αpsc psc = discount factor of update if ( N != 0 ) fpsc psc(A , B ) = fs psc(A , B ) + αpsc psc × ( δGT / N ) else fpsc psc(A , B ) = fs psc(A , B )
By separating the process of comparing s psc and psc psc , we are enforcing the intuition that the comparison between self ( eg , section A ) and psc ( eg , psc(B ) ) should weigh more than that of psc ( eg , psc(A ) ) and psc ( eg , psc(B) ) . Therefore the comparison threshold here is raised to fs psc . that
423 Reference Distribution To understand the intuition behind reference distribution , we should note regulations are heavily self referenced documents , which contributes to the difficulty in reading and understanding them . Our documents , in particular ADAAG and UFAS , are heavily self referenced but not cross referenced : they do not reference each other or outside materials as much . For instance , sections in the ADAAG reference other sections in the ADAAG , but do not reference the UFAS or others . With this understanding in mind , it is easy to explain the process of reference distribution . The hypothesis is that two sections referencing similar sections are more likely to be related and should have their similarity score raised . Therefore , the process of reference distribution utilizes the heavily self referenced structure of the regulation to further refine the similarity score obtained from Section 422 One can visualize the problem as separate islands of information : within an island information is bridged with references ; across islands there are no connecting bridges . Therefore , similarity score between the referee sections is increased due to the similarity in the referenced sections , and this increase is proportional to the similarity score between the referenced sections . We deploy a system similar to the s psc and psc psc process , replacing psc with ref which represents the set of outlinks from a section :
Figure 3 . Score refinement based on neighboring nodes in tree Example 4 UFAS 4.13 Doors
4131 General … 4139 Door Hardware
Handles , pulls , latches , locks , and other
… 41312 Door Opening Force
BS8300 1254 Doors
12541 Clear Widths of Door Openings 12542 Door Furniture
Door handles on hinged and sliding doors …
Example 5 UFAS 412 Accessible Buildings : New Construction
( 4 ) Stairs connecting levels that are not connected
Scottish Technical Standards 3 Stairs and ramps
3.17 Pedestrian Ramps a raised kerb at least 100mm high on any … illustrate
To the similarity between American and British standards , we compare UFAS with BS8300 . Example 4 shows sections from the two regulations both focusing on doors . Given the relatively high similarity score between Sections 4139 and 12542 ( f0 = 0.425 ) , they are expected to be related , and in fact they are ; Section 4139 from the American code is titled “ Door Hardware ” while Section 12542 from the British standard is titled “ Door Furniture . ” As the American and British phrasing is different , concept comparison does not pick up the match between “ door hardware ” and “ door furniture ” ; however , by comparing the neighbors of the sections , we observe a higher similarity score ( fpsc psc = 0471 ) As shown in Figure 3 , similarities in neighboring nodes in the regulation trees imply a higher similarity between the compared Sections 4139 and 12542
Comparing fpsc psc with frd , we find it difficult to observe any major improvements after neighbor inclusion . This is possibly due to the relatively high threshold in the algorithm : frd is only updated from fpsc psc if the outlinks have higher similarities between them . However , some improvement still exists ; for instance , in Example 5 , both sections from the UFAS and the Scottish code are concerned about pedestrian ramps and stairs which are related accessible elements . Indeed , after reference distribution , these two provisions show a significant increase in the similarity score from fpscpsc of 0.094 to frd of 031 into XML format because of
6 . CONCLUSIONS AND FUTURE TASKS This project aims to develop an infrastructure for regulation management and comparative analysis . A repository is built by transforming regulations its capability to handle semi structured data . After all regulations are in a unified format , features are extracted from the corpus semiautomatically , in addition to features from reference materials such as engineering handbooks . A taxonomy is developed on top of the concepts identified by an text mining tool to allow for easy viewing following the classification . With the repository fully functional online , users can browse through regulatory documents according to the document hierarchy or based on concept clusters . We then perform a similarity analysis . It first computes a base score between pairs of provisions by combining similarity scores from each of the features . The base score is refined by taking immediate neighboring sections Reference distribution is performed to further refine the scores according to the reference structure in the regulations . Preliminary results are obtained by comparing several sets of accessibility regulations , and we have provided examples to show that our system does reveal hidden relatedness between provisions through neighbor inclusion and reference distribution . Once the prototype is thoroughly tested and evaluated on accessibility regulations , we anticipate the incorporation of environmental scalability and practicality of the system . In addition , due to the existence of multiple sources of regulations and thus potential conflicts between them , conflict identification becomes the natural next step to a complete regulatory document analysis . In the long run , we plan to study the formal representation derived from structured texts in order to perform automated analysis of overlaps , completeness and conflicts . to demonstrate into account . regulations
7 . ACKNOWLEDGMENTS This research project is sponsored by the National Science Foundation , Contract Numbers EIA 9983368 and EIA 0085998 . The authors would like to acknowledge a “ Technology for Education 2000 ” equipment grant from Intel Corporation . We would also like to acknowledge the support by Semio Corporation in providing the software for this research .
8 . REFERENCES [ 1 ] ADA Accessibility Guidelines for Buildings and Facilities .
The Access Board , 1998 .
[ 2 ] Baeza Yates , R . and Ribeiro Neto , B . Modern Information
Retrieval . ACM Press , New York , NY , 1999 .
[ 3 ] Bishop , C . Neural Networks for Pattern Recognition . Oxford
University Press ; Clarendon Press , New York , NY , 1995 . [ 4 ] Bollacker , KD , Lawrence , S . and Giles , CL CiteSeer : an autonomous web agent for automatic retrieval and identification of interesting publications . in Proceedings of the 2nd International Conference on Autonomous Agents ( Minneapolis , MN , 1998 ) , ACM Press , 116 123 . [ 5 ] Brin , S . and Page , L . The anatomy of a large scale hypertextual web search engine . in Proceedings of the 7th International World Wide Web Conference ( Brisbane , Australia , 1998 ) , 107 117 .
[ 6 ] British Standard 8300 . British Standards Institution ( BSI ) ,
2001 .
[ 7 ] California Building Code . California Building Standards
Commission , 1998 .
[ 8 ] Dorre , J . , Gerstl , P . and Seiffert , R . Text mining : finding nuggets in mountains of textual data . in Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( San Diego , CA , 1999 ) , 398 401 .
[ 9 ] Gibbens , MP California Disabled Accessibility Guidebook
2000 . Builder's Book , Canoga Park , CA , 2000 .
[ 10 ] Hovy , E . Using an ontology to simplify data access .
Communications of the ACM , 46 ( 1 ) . 47 49 .
[ 11 ] International Building Code 2000 . International Conference of Building Officials , 2000 .
[ 12 ] Kidder , F . and Parker , H . Kidder Parker Architects' and Builders' Handbook . John Willey & Sons , London , UK , 1931 .
[ 13 ] Mitra , P . and Wiederhold , G . Resolving terminological heterogeneity in ontologies . in Proceedings of Workshop on Ontologies and Semantic Interoperability at the 15th European Conference on Artificial Intelligence ( ECAI ) ( Lyon , France , 2002 ) .
[ 14 ] Porter , MF An algorithm for suffix stripping . Program : Automated Library and Information Systems , 14 ( 3 ) . 130137 .
[ 15 ] Salton , G . The smart retrieval system experiments in automatic document processing . Prentice Hall , Englewood Cliffs , NJ , 1971 .
[ 16 ] Salton , G . and Buckley , C . Term weighting approaches in automatic retrieval . Information Processing and Management , 24 ( 5 ) . 513 523 .
[ 17 ] Semio Tagger . Semio Corporation , 2002 . http://wwwsemiocom
[ 18 ] Uniform Federal Accessibility Standards ( UFAS ) . The
Access Board , 1986 .
[ 19 ] Extensible Markup Language ( XML ) . World Wide Web
Consortium ( W3C ) , 2003 . http://wwww3org/XML
