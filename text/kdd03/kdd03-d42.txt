Finding Recent Frequent Itemsets Adaptively over Online Data Streams Joong Hyuk Chang Won Suk Lee Department of Computer Science , Yonsei University
134 Shinchon dong Seodaemun gu Seoul , 120 749 , Korea
+82 2 2123 2716
{ jhchang , leewo }@amadeusyonseiackr
ABSTRACT A data stream is a massive unbounded sequence of data elements continuously generated at a rapid rate . Consequently , the knowledge embedded in a data stream is more likely to be changed as time goes by . Identifying the recent change of a data stream , specially for an online data stream , can provide valuable information for the analysis of the data stream . In addition , monitoring the continuous variation of a data stream enables to find the gradual change of embedded knowledge . However , most of mining algorithms over a data stream do not differentiate the information of recently generated transactions from the obsolete information of old transactions which may be no longer useful or possibly invalid at present . This paper proposes a data mining method for finding recent frequent itemsets adaptively over an online data stream . The effect of old transactions on the mining result of the data steam is diminished by decaying the old occurrences of each itemset as time goes by . Furthermore , several optimization techniques are devised to minimize processing time as well as main memory usage . Finally , the proposed method is analyzed by a series of experiments .
Categories and Subject Descriptors H28 [ Database Management ] : Application Data mining
General Terms Algorithm
Keywords Recent frequent Delayed insertion , Pruning of itemsets itemsets , Data stream , Decay mechanism ,
1 . INTRODUCTION A data stream is a massive unbounded sequence of data elements continuously generated at a rapid rate . Due to this reason , it is impossible to maintain all elements of a data stream . As a result , data stream processing should satisfy the following requirements [ 1 ] .
First , each data element should be examined at most once to analyze a data stream . Second , memory usage for data stream analysis should be restricted finitely although new data elements are continuously generated in a data stream . Third , newly generated data elements should be processed as fast as possible . Finally , the up to date analysis result of a data stream should be instantly available when requested . In order to satisfy these requirements , data stream processing sacrifices the correctness of its analysis result by allowing some error . The target application domains of a data stream are either a bulk addition of new transactions as in a data warehouse system or an individual addition of a continuously generated transaction as in a network monitoring system . The former is called as an offline data stream while the latter is called as an online data stream [ 2 ] . For an offline data stream , it is possible to enhance the performance of data mining through a batch operation by processing a considerable number of newly generated transactions together [ 2 ] . Due to this reason , the up to date mining result of an offline data stream is available only after a batch operation is finished . Therefore , the granularity of generating the most up to date result depends on the number of new transactions batch processed together . However , data mining over an online data stream should support flexible trade off between processing time and mining accuracy without any fixed granule of data mining in order to catch the sensitive change of its mining result as quickly as possible . Among the frequency counting algorithms [ 2,3 ] of data elements over a data stream , the Lossy Counting algorithm [ 2 ] is the most representative method . In the Lossy Counting algorithm , the set of frequent itemsets in a data stream is found when a maximum allowable error ε as well as a minimum support is given . A set of newly generated transactions in a data stream is loaded together into a fixed sized buffer in main memory and they are batchprocessed . The information about the previous mining result up to the latest batch operation is maintained in a data structure called D containing a set of entries of a form ( e , f , ∆ ) where e is an itemset , f is the count of the itemset e , and ∆ is the maximum possible error count of the itemset e . In order to update the information of the data structure D , all of its entries are looked up in sequence . For the entry ( e , f , ∆ ) of an itemset e in D , if the itemset e is one of the itemsets identified by the new transactions in the buffer , its previous count f is incremented by its count in the new transactions . Subsequently , if its estimated count ie , f+∆ is less than ε×N , it is pruned from D . On the other hand , when there is no entry in D for a new itemset e identified by the new transactions in the buffer , a new entry ( e , f , ∆ ) is inserted to D . Its maximum where N’ denotes the number possible error ∆ is set to 
N ′×ε
487 of transactions that were processed up to the latest batch operation . Generally , knowledge embedded in a data stream is more likely to be changed as time goes by . Identifying the recent change of a data stream quickly , specially for an online data stream , can provide valuable information for the analysis of the data stream . In addition , monitoring the continuous variation of a data stream enables to find the gradual change of embedded knowledge , so that it can be timely utilized . In order to achieve this , the effect of obsolete information in old transactions on the current mining result of a data stream should be eliminated effectively . As a simple solution , it is possible to consider a sliding window approach . It restricts the target transactions of data mining to those transactions that are generated within the most recent period of a fixed sized window . However , its current mining result totally depends on recently generated transactions in the range of the window . Due to this reason , this approach is a primitive way of disregarding obsolete information . In addition , all the transactions in the window need to be maintained in order to remove their effects on the current mining result when they are out of the range of a sliding window . In terms of information differentiation , the SWF algorithm [ 4 ] uses a sliding window to find frequent itemsets in the fixed number of recent transactions . The sliding window is composed of a sequence of partitions . Each partition maintains a number of transactions . The candidate 2 itemsets of all transactions in the window are maintained separately . When the window is advanced , the oldest partition is disregarded and a new partition containing newly generated transactions is appended to the window . At the same time , the candidate 2 itemsets of the advanced window are adjusted . Subsequently , all possible candidate itemsets are generated by these candidate 2 itemsets . The new set of frequent itemsets is identified by scanning the transactions of the slid window . A more flexible way of information differentiation is presented in [ 5 ] where correlations among co evolving time sequences are analyzed . The missing values of the sequences are estimated and their future values are predicted . In order to identify the recent change of correlations adaptively , a forgetting factor is used to diminish the effect of old correlations among sequences . A forgetting factor determines how fast the effect of old information is faded away . This type of an information decay model is also introduced in NIDES [ 6 ] for anomaly intrusion detection . NIDES models the historical behavior of a user ’s activities in terms of various measures and generates a long term profile containing a statistical summary for each measure . In order to concentrate on the recent behavior of the user , the statistics of old activities in the long term profile are decayed as new activities are performed by the user . This paper proposes a method of finding recent frequent itemsets adaptively over an online data stream . It examines each transaction in a data stream one by one without any candidate generation . The occurrence count of a significant itemset that appears in each transaction is maintained by a prefix tree lattice structure in main memory . The effect of old transactions on the current mining result the old occurrence count of each itemset as time goes by . In addition , the rate of decay old information is flexibly defined as needed . The total number of significant itemsets in main memory is minimized by delayed insertion and pruning operations of an itemset . As a result , its processing time is flexibly controlled while sacrificing its accuracy . is diminished by decaying
2 . PRELIMINARIES For finding frequent itemsets , a data stream can be defined as follows :
ⅰ ) Let I={i1 , i2 , … , in} be a set of current items that have ever been used as a unit information of an application domain . ⅱ ) An itemset e is a set of items such that e∈(2I {∅} ) where 2I is the power set of I . The length |e| of an itemset e is the number of items that form the itemset and it is denoted by an |e| itemset . An itemset {a,b,c} is denoted by abc .
ⅲ ) A transaction is a subset of I and each transaction has a unique transaction identifier TID . A transaction generated at the kth turn is denoted by Tk .
ⅳ ) When a new transaction Tk is generated , the current data stream Dk is composed of all transactions that have ever been generated so far ie , Dk = <T1 , T2 , … , Tk> and the total number of transactions in Dk is denoted by |D|k .
When a transactions Tk is generated currently , the current count Ck(e ) of an itemset e is the number of transactions that contain the itemset among the k transactions . Likewise , the current support Sk(e ) of an itemset e is the ratio of its current count Ck(e ) over |D|k . A decay rate means the reducing rate of a weight for a fixed decay unit . A decay unit determines the chunk of information to be decayed together . A decay rate is defined by two parameters : a decay base b and a decay base life h . A decay base b determines the amount of weight reduction per a decay unit and it is greater than 1 . When the weight of the current information is set to 1 , a decay base life h is defined by the number of decay units that makes the current weight be b 1 . Based on these two parameters , a decay rate d is defined as follows : d
−= b
/1( h
)
( b>1 , h≥1 , b 1≤ d< 1 )
( 1 )
/1( h
) d
  
| k
1 − d−
)
−= b
|D|k= d +× k if k if 1 as the value k increases
Theorem 1 . Given a decay rate ( b>1 , h≥1 , b 1≤ d< 1 ) , the total number of transactions |D|k in the current data stream Dk is found as follows :
1
D |
1 = 2 ≥ The value of |D|k converges to 1/(1 infinitely . ( Proof ) When the first transaction is looked up , the number of transactions |D|1 is obviously 1 since there is no previous transaction whose weight should be decayed . When the second transaction is looked up , the total number of transactions |D|2 is computed by |D|1×d+1 since the weight of the first transaction is decayed . Subsequently , when a new transaction is generated at the kth ( k≥2 ) turn , the total number of transactions |D|k=|D|k 1×d+1 . Consequently , it can be expressed by |D|k=dk 1+dk 2 + …+d+1= . Since b 1≤ d<1 , |D|k converges to as k increases infinitely . □ Similarly , the count Ck(e ) of an itemset e in the current data stream Dk is obtained as follows :
1/( )
1/(1 d−
)
1(
−
− d
) d k
Ck(e)=Ck 1(e)×d+Wk(e ) , Wk(e ) =
1 
 0  kTe ∈ if otherwise
3 . FINDING RECENT FREQUENT ITEMSETS In this section , a method of finding recent frequent itemsets adaptively over an online data stream is proposed based on the
488 decay mechanism described in Section 2 . The different combinations of items that appear in each transaction are maintained in a prefix tree lattice structure [ 7,8 ] called as a monitoring lattice . A node in a monitoring lattice contains an item and it denotes an itemset composed of items that are in the nodes of its path from the root .
3.1 Count Estimation of an Itemset In the Carma algorithm [ 9 ] , the maximum possible count of an itemset is estimated by the minimum value among the maximum possible counts of all of its subsets . A new itemset is inserted to a lattice of itemsets if it is potentially frequent and all of its subsets are maintained in the lattice . Similarly , the count of an itemset that are not maintained can be estimated by its subsets that are maintained in a monitoring lattice . For this purpose , the terms defined in Definition 1 and Definition 2 are used . Definition 1 . For an n itemset e ( n≥2 ) , a set of its subsets P(e ) , a set of its m subsets Pm(e ) and a set of counts for its m subsets PC m(e ) are formally defined as follows . ⅰ ) A set of its subsets P(e ) is composed of all possible itemsets that can be generated by one or more items of the itemset e ie , P(e)={α|∀α st α∈2e {e} and α≠∅ } .
ⅱ ) A set of its m subsets Pm(e ) is composed of those itemsets in P(e ) that have m items ( m<n ) ie , Pm(e)={α|∀α st α∈P(e ) and |α|=m } .
ⅲ ) A set of counts for its m subsets PC m(e ) is composed of the distinct counts of all itemsets in Pm(e ) ie , PC m(e)={C(α)| ∀α st α∈Pm(e ) } , where C(e ) denotes the count of an itemset e over a data stream . □ Definition 2 . For two itemsets e1 and e2 , a union itemset e1∪e2 and an intersection itemset e1∩e2 are defined as follows .
ⅰ ) A union itemset e1∪e2 is composed of all items that are members of either e1 or e2 . are members of both e1 and e2 .
ⅱ ) An intersection itemset e1∩e2 is composed of all items that □ For an itemset , each of its subsets appears in at least as many transactions as the itemset appears in . Furthermore , when all items of an itemset always appear together in each transaction , the count of the itemset should be identical to those of its subsets . Therefore , the count of an itemset depends on how often its items appear together in each transaction . Based on this observation , the possible range of the count of an itemset can be identified by two extreme distributions : least exclusively distributed ( LED ) and most exclusively distributed ( MED ) . When the items of an itemset are LED , they appear together in as many transactions as possible . On the other hand , the items of an itemset appear exclusively as many transactions as possible when they are MED . In a data set D , the count of an n itemset e can be estimated by the individual counts of its subsets . Its maximum count Cmax(e ) is found when all of its subsets are LED . It is the smallest value among the counts of all the subsets . However , since the set of its ( n 1) subsets can provide the most accurate information about the count of the n itemset , Cmax(e ) can be estimated by only its ( n 1)subsets . Therefore , when min(V ) denotes the smallest value in a set of values V , the maximum count Cmax(e ) of an itemset e is found as follows :
Cmax(e ) = min(PC
( 2 ) For two itemsets e1 and e2 , the minimum count Cmin(e1∪e2 ) of n 1(e ) ) their union itemset e1∪e2 can be estimated as follows .
Cmin(e1∪e2 ) eC ( ) 1 = )C(e 1 max max
,0( ( 0 ,
  
+ + eC ( ) 2 )C(e 2
∩ e eC ( − 2 1 |D| )
−
) ) e if 1 e if 1
∅≠∩ ∅=∩ e 2 e 2
( 3 ) where |D| denotes the total number of transactions in D and max(V ) denotes the largest value in a set of values V . Based on Equation ( 3 ) , the minimum count Cmin(e ) of an itemset e can also be estimated by the counts of its ( n 1) subsets . In other words , for each distinct pair ( αi , αj ) of its ( n 1) subsets ie , αi and αj∈Pn 1(e ) , the count of their union itemset αi∪αj can be estimated . Among the estimated counts for the itemset e , the largest count is the guaranteed appearance count ie , the minimum count Cmin(e ) of the itemset e as follows :
Cmin(e ) = max({Cmin(αi∪αj ) | ∀αi , αj ∈ Pn 1(e ) and i≠j} )
The maximum count Cmax(e ) of an itemset e is used as the estimated count of the itemset . Consequently , there may exist an estimation error count since Cmax(e ) is the largest possible count that the itemset can appear in the transactions of a data set . Let the difference between Cmax(e ) and Cmin(e ) be the estimation error E(e ) of the itemset .
3.2 estDec Method Not all of itemsets that appear in a data stream are significant for finding frequent itemsets . An itemset which has much less support than a predefined minimum support is not necessarily monitored since it cannot be a frequent itemset in the near future . Therefore , the insertion of a new itemset can be delayed until it can possibly be a frequent itemset in the near future . When the estimated support of a new itemset is large enough , it is regarded as a significant itemset and it is inserted to a monitoring lattice . On the other hand , an efficient pruning technique is obviously another way of reducing the usage of memory space . Although an itemset in a monitoring lattice was significant enough to be monitored in the past , if its current support becomes much less than a predefined minimum support , it can be eliminated from the monitoring lattice . This section proposes an estDec method for finding recent frequent itemsets adaptively over an online data stream . Every node in a monitoring lattice maintains a triple ( cnt , err , MRtid ) for its corresponding itemset e . The count of the itemset e is denoted by cnt . The maximum error count of the itemset e is denoted by err . Finally , the most recent transaction that contains the itemset e is denoted by MRtid . The estDec method is composed of four phases : parameter updating phase ( Phase Ⅰ ) , count updating phase ( Phase Ⅱ ) , delayedinsertion phase ( Phase Ⅲ ) and frequent itemset selection phase ( Phase Ⅳ ) . The detailed steps of these phases are illustrated in Figure 1 . When a new transaction Tk is generated in a data stream , the total number of transactions in the current data stream |D|k is updated in the parameter updating phase ( line 4 in Figure 1 ) as follows : identifier of transaction the
|D|k = |D|k 1×d+1
In the count updating phase ( line 5 9 ) , the counts of those itemsets in a monitoring lattice that appear in the new transaction are updated . All the paths of a monitoring lattice that are induced by the items of the transaction are traversed and the previous triple ( cntpre , errpre , MRtidpre ) of each node in the paths is updated to the current triple ( cntk , errk , MRtidk ) as follows : kd − (
MRtid
) pre
+1 , cntk = cntpre×
489 errk = errpre× kd − (
MRtid
) pre
, MRtidk = k | k k D |/ cnt of an itemset in a When the updated support ie , monitoring lattice becomes less than a predefined threshold , the itemset is regarded as an insignificant itemset , so that it is pruned from the monitoring lattice as in conventional lattice based data mining methods [ 7,8 ] . However , if a 1 itemset is pruned from a monitoring lattice , it is impossible to estimate its count later . Therefore , it should not be pruned . This mechanism is called as a pruning operation of an itemset . The threshold of this operation is defined as a threshold for pruning Sprn which should be less than a minimum support Smin . After all of these itemsets are updated , the delayed insertion phase ( line 10 22 ) is started in order to find any new itemset that has a high possibility to become a frequent itemset in the near future . A new itemset is inserted to a monitoring lattice only in the following two cases . The first case is when a new 1 itemset appears in a newly generated transaction . In this case , the itemset is instantly inserted to a monitoring lattice without any estimation process . Consequently , the count cnt of every 1 itemset in a monitoring lattice is not an estimated value but an actual value . The second case is when the estimated support of an n itemset ( n≥2 ) that is not in the monitoring lattice is large enough to be monitored . In this phase , among the items of the new transaction , the items whose supports are less than Sins are not considered . While navigating the lattice according to the remaining items of the new transaction , the count of an insignificant itemset that is composed of a significant itemset and one of the remaining items is estimated by its maximum count Cmax(e ) as described in Section 31 Due to the characteristics of a prefix lattice structure , there is no candidate itemset generation process . This is because such an itemset is identified systematically while navigating the lattice according to the remaining items in the new transaction . If any of its ( |e| 1) subsets in Pn 1(e ) is not currently maintained in the monitoring lattice , the count of the itemset e is not estimated . This is because its Cmax(e ) is always 0 in this case . Subsequently , the estimated support of the itemset can be found by the ratio of its count cnt over the current total number of transactions |D|k . If it is greater than or equal to a predefined threshold , the itemset is inserted to the monitoring lattice . This mechanism is called as a delayed insertion operation and the pre defined threshold for this insertion is defined as a threshold for delayed insertion Sins which should be also less than a minimum support Smin . When an itemset e is inserted , all of its ( |e| 1) subsets should be significant . Due to this reason , it is possible to find the upper bound Cupper(e ) of its actual count when it is inserted at the kth transaction . In other words , among the k transactions generated so far , at least |e| 1 transactions that contain the itemset e are required to insert all of its subsets to the monitoring lattice in advance . Therefore , its actual count is maximized when these |e| 1 transactions are most recently generated . The similar approach is used in [ 9 ] . The decayed count of the itemset e for the insertion of its subsets by these recent |e| 1 transactions is represented by a term cnt_for_subsets as follows : d e )1|(| −
( 4 ) In addition , the maximum possible decayed count of the itemset e before by max_cnt_before_subsets and it is represented as follows : e )1|(| cntt_for_subsets = transactions denoted recent
|e| 1 the
1/(} max_cnt_before_subsets =
( 5 ) is
1{
D
{|
−
−
×
× d d
S
}
)
−
| k
− e )1|(|
− ins
Consequently , Cupper(e ) can be found as follows :
Cupper(e ) = max_cnt_before_subsets+cnt_for_subsets ( 6 ) If Cmax(e ) in Equation ( 2 ) is greater than the upper bound Cupper(e ) , Cupper(e ) is used as its count cnt . Accordingly , the current triple ( cntk , errk , MRtidk ) of the itemset e in the corresponding node of the monitoring lattice is updated as follows : cntk = min{Cmax(e ) , Cupper(e)} errk = E(e ) = cntk – Cmin(e ) , MRtidk = k
Input : A data stream D Output : A complete set of recent frequent itemsets Lk
)
) kT2
; MRtid = k ;
( MRtid kd −
+1 ; err = err ×
{∅} ) and e∈ML { d : A given decay rate ML : A monitoring lattice for each new transaction in D { read current transaction Tk ;
// Parameter updating phase |D|k = |D|k 1×d+1 ; for all itemset e st e∈(
} // of for in line 5
// Count updating phase
// Delayed insertion phase / kT = ItemFiltering(Tk ) ; for all itemset e st e∈(
1 : ML = ∅ ; 2 : 3 :
4 :
5 : ( MRtid kd − cnt = cnt × 6 : 7 : if ( cnt/|D|k)<Sprn and |e|>1 // Pruning Eliminate e and it's child node from ML ; 8 : 9 :
10 : 11 : 12 : 13 : Insert e into ML ; cnt=1 ; err = 0 ; MRtid = k ; 14 : 15 : Estimate Cmax(e ) and Cmin(e ) ; 16 : if Cmax(e ) > Cupper(e ) 17 : Cmax(e ) = Cupper(e ) ; )| 18 : if k 19 : Insert e into ML ; cnt=Cmax(e ) ; err= Cmax(e) Cmin(e ) ; MRtid=k ; 20 : } // of if n line 18 } // of else in line 14 21 : 22 :
23 : 24 : 25 : cnt = cnt× 26 : 27 : 28 : } // of for in line 23 29 : } // of for in line 2
// Frequent itemset selection phase Lk = ∅ ; if ( cnt/|D|k)≥Smin Lk = Lk ∪ {e} ;
/2 kT {∅} ) and e∉ML {
} // of for in line 11 if |e| = 1 { } else { for all itemset e∈ML {
; MRtid = k ;
( MRtid kd −
( MRtid kd −
; err = err ×
≥Sins { e |/)(
C ( max
D
)
)
Figure 1 . The estDec method
An itemset pruned at present can be inserted into the monitoring lattice in the future by the delayed insertion operation if it appears frequently in new transactions . Consequently , Sprn should be less than Sins . As the gap between the two thresholds Sprn and Sins is enlarged , the possibility of repeating the insertion and pruning of the same itemset frequently is reduced . Furthermore , as the gap between these two thresholds is enlarged , the accuracy of frequent itemsets is improved while the size of a monitoring lattice is increased . The frequent itemset selection phase ( line 23 28 ) is performed only when the mining result of the current data set is required . It produces all current frequent itemsets in a monitoring lattice by the same way as in conventional mining methods [ 7,8 ] based on a prefix tree lattice structure . When this phase is performed in the current data stream Dk , an itemset e is frequent if its current support than a predefined minimum support Smin . Furthermore , its current support error err { can be found as well . is greater kD | cnt {
|/}
|/}
MRtid
MRtid
×
× d d k ( − k ( −
)
) kD |
490
5
4
3
2
1
0
) 0 0 0 0 0 0 1 x ( s t e s m e t i f o r e b m u N
Sins = 10 % Sins = 30 % Sins = 50 %
T10I4D1000K Sm in = 0.001 b=2 , h=10000
Sins = 10 % Sins = 30 % Sins = 50 %
T10I4D1000K Sm in = 0.001 b=2 , h=10000
30
20
10
0
) c e s m
( e m i t i g n s s e c o r p e g a r e v A
500
400
300
200
100
0
) c e s m
( e m i t i g n s s e c o r p e g a r e v A
Sins = 10 % Sins = 30 % Sins = 50 %
T10I4D1000K Sm in = 0.001 , b=2 , h=10000
0
~200000
~400000
~600000
~800000 ~1000000
0
~200000
~400000
~600000
~800000 ~1000000
0
~200000 ~400000
~600000 ~800000 ~1000000
TID
( a )
TID
( b )
Figure 2 . Performance of the estDec method for the data set T10I4D1000K
TID
( c )
)
5 0 1 x (
E S A
5
4
3
2
1
0
Sins = 10 % Sins = 30 % Sins = 50 %
T10I4D1000K Sm in = 0.001 b=2 , h=10000
0
~200000
~400000
~600000
~800000
~1000000
TID
Figure 3 . Accuracy of mining results
All insignificant itemsets in a monitoring lattice can be pruned together by examining the current support of every itemset in the monitoring lattice . This mechanism is called as a force pruning operation and can be performed periodically or when the current size of a monitoring lattice reaches a pre defined threshold value .
4 . STABILIZED ANALYSIS When the information of a transaction is rapidly decayed , the resulting set of recently frequent itemsets may be too sensitively varied since it is dominated by the itemsets of newly generated transactions . This type of fluctuation in the set of frequent itemsets is meaningless . In order to avoid this , a safety factor γ is introduced . It is defined by the maximum number of most recently generated consecutive transactions that contains a new itemset while the itemset remains as an infrequent itemset . The resulting set of recently frequent itemsets in a data stream Dk becomes stable when the decay base life h of a decay rate is set to be greater than or equal to its lower bound hLB defined in Theorem 2 . If a decay base life h is less than its lower bound hLB , an itemset whose current count is less than the safety factor γ may become a frequent itemset . Theorem 2 . Given a minimum support Smin , a safety factor γ and a ) decay base b , the decay base life h of a decay rate should be greater than or equal to its lower bound hLB defined as follows :
−= b
−= b
/1( h
/1( h d d
) hLB = 
{ − γ
/(log
1( b − minS
)}

1( a in data transactions
( Proof ) When a new itemset e appears in most recently generated γ its consecutive . Consequently , its support Sk(e ) is always Ck(e)= − γ d d ) 1/( ) − γ d 1 − since the maximum value greater than ( d 1 − of |D|k is . The itemset should not be a frequent itemset d− ) for the given safety factor γ , so that its support should be always less than the minimum support Smin . Therefore , <Smin stream Dk ,
1 d−1
γd−1
γd−1
) =
1/(1
/
100
80
60
40
20
0
T5I4D1000K AB S min = 0.001 S ins= 10 % , b = 2 h=100000 CR(A ) h=300000 CR(A ) h=500000 CR(A ) h=100000 CR(B ) h=300000 CR(B ) h=500000 CR(B )
)
%
( e t a r e g a r e v o C
0
200000
400000
TID
600000
800000
1000000
Figure 4 . Coverage rate for the data set T5I4D1000K AB
/1( h ) should be satisfied . Since decay base life hLB is found as follows :  . hLB = 
−= b
/(log
{ − γ
)} d
1( b − minS
, the lower bound of a
□
5 . EXPERIMENTAL RESULTS In this section , two data sets T10I4D1000K and T5I4D1000KAB are used to evaluate the performance of the estDec method . Each data set is generated by the same method as described in [ 10 ] and the total number of items is 1,000 . In all experiments , the transactions of each data set are looked up one by one in sequence to simulate the environment of an online data stream . In addition , every value of a decay base life h used in these experiments is above its lower bound . Each of two thresholds Sins and Sprn is assigned relatively to the value of a predefined minimum support Smin . These two thresholds are not necessarily to be the same in practice . However , in all experiments , they are set to be the same value . When the value of the threshold Sins is denoted by p % , the actual value of the threshold Sins is equal to Smin×(p/100 ) . Likewise , Sprn is also equal to Smin×(p/100 ) . All experiments are performed on a 1.8GHz Pentium PC machine with 512MB main memory running on Linux 7.3 and all programs are implemented in C . Figure 2 shows the performance of the estDec method for the data set T10I4D1000K Figure 2 (a ) shows the memory usage and it is represented by the maximum number of itemsets in a monitoring lattice . A minimum support Smin , a decay base b and a decay base life h are set to 0.001 , 2 and 10,000 respectively . The sequence of generated transactions is divided into 5 intervals each of which consists of 200,000 transactions and a force pruning operation is performed in every 1,000 transactions . Since only significant itemsets are maintained in a monitoring lattice by the delayed insertion and pruning of an itemset , the memory usage of the estDec method remains the same although new transactions are continuously generated . In addition , it decreases as the value of Sins is increased . Figure 2 (b ) shows the average processing time in each interval of the experiment in Figure 2 (a ) . The processing time is measured by a period from the generation of a new transaction to the delayed insertion phase ( PhasesⅠ Ⅲ ) of the estDec method . As
491 shown in this figure , the average processing time is less than about 15 msec . It is influenced by not only the number of new itemsets whose current support should be estimated for delayedinsertion but also the current size of a monitoring lattice . The number of itemsets maintained in a monitoring lattice is inversely proportional to the value of a threshold Sins as shown in Figure 2(a ) . Therefore , as the value of Sins is increased , the average processing time is decreased . Figure 2 (c ) shows the average processing time of the frequent itemset selection phase ( Phase Ⅳ ) in each interval of the experiment in Figure 2 (a ) . Compared with Figure 2 (b ) , the average processing time of this phase is considerably larger . This is because it requires to search the entire space of a monitoring lattice . However , it is also decreased as the value of Sins is increased . A term average support error is introduced to model the relative accuracy of the proposed method . When two sets of mining results R1={(ei , S1(ei ) ) | S1(ei)≥Smin} and R2={(ej , S2(ej ) ) | S2(ej)≥Smin} are given for the same data set , the average support error ASE(R2|R1 ) of R2 with respect to R1 is defined as follows :
ASE ( R2|R1 ) = eS { ( l 1 RRRe ∩−∈ l 2
∑
1
1
)
+
( | ∑ RRe ∩∈ l 1 2 eS ( l 2
)
− eS ( l 1
| ) )
+
∑ eS ( l 2 RRRe ∩−∈ l 2
1
2
|/} )
R 1
| support average
2 . The in Section where |R1| denotes the number of itemsets in R1 . As the average support error of R2 gets smaller , the mining result of R2 is more similar to R1 . To show the accuracy of the estDec method , Figure 3 shows ASE(RestDec|RdApriori ) for the experiment in Figure 2 . The mining results of the estDec method and the dApriori algorithm are denoted by RestDec and RdApriori respectively . The dApriori algorithm is the Apriori algorithm [ 10 ] with the decay mechanism proposed error ASE(RestDec|RdApriori ) of the estDec method is influenced by the value of Sins . As it becomes smaller , more itemsets are maintained in a monitoring lattice , which makes the mining result of the estDec method be more accurate . When the value of Sprn is set to be less than the value of Sins , the accuracy is slightly improved although this experiment is not presented in this paper . Figure 4 shows the adaptability of the estDec method for the change of information in a data stream . In this experiment , a data set T5I4D1000K AB is experimented . The data set is composed of two consecutive subparts . The front part is a set of 500,000 transactions generated by an item set A while the second part is a set of 500,000 transactions generated by an item set B . There are no common items in the item sets A and B . In this experiment , Smin and b are set to 0.001 and 2 respectively . In order to illustrate how rapidly the estDec method can adapt the change of information in a data stream , a coverage rate CR(X ) is introduced . It denotes the ratio of frequent itemsets induced by an item set X in all frequent itemsets as follows : of # frequent set item an
X
CR(X ) = induced itemsets by
R |
|
×
100
( % ) where |R| denotes the total number of frequent itemsets in a monitoring lattice . As the value of a decay base life h becomes smaller , the estDec method adapts more rapidly the transition of information between the two subparts of the data set . By varying a decay base life h , the adaptability of the estDec method for the recent change of a data stream can be controlled . The similar effect can be archived by varying a decay base b . As the value of a decay base b becomes larger , the estDec method adapts more rapidly the recent change of a data stream .
6 . CONCLUDING REMARKS Considering the continuity of a data stream , the general definition of finding frequent itemsets used in conventional data mining methodology may not be valid in a data stream . This is because the old information of a data stream may be no longer useful or possibly incorrect at present . In order to support various needs of data stream analysis , the interesting recent range of a data stream needs to be defined flexibly . Based on this range , a mining method can be able to identify when a transaction becomes obsolete and needs to be disregarded . The estDec method proposed in this paper finds recent frequent itemsets over an online data stream by decaying the weight of old transactions as time goes by . As a result , the recent change of information in a data stream can be adaptively reflected to the current mining result of the data stream . The weight of information in a transaction of a data stream is gradually reduced as time goes by while its reduction rate can be flexibly controlled . Due to this reason , no transaction needs to be maintained physically .
7 . REFERENCES [ 1 ] M . Garofalakis , J . Gehrke and R . Rastogi . Querying and mining data streams : you only get one look . In the tutorial notes of the 28th Int'l Conference on Very Large Databases , Hong Kong , China , Aug . 2002 .
[ 2 ] G . S . Manku and R . Motwani . Approximate frequency counts over data streams . In Proc . of the 28th Int'l Conference on Very Large Databases , Hong Kong , China , Aug . 2002 .
[ 3 ] M . Charikar , K . Chen and M . Farach Colton . Finding frequent items in data streams . In Proc . of the 29th Int'l Colloq . on Automata , Language and Programming , 2002 .
[ 4 ] C H Lee , C R Lin and M S Chen . Sliding window filtering : An efficient algorithm for incremental mining . In Proc . of the 10th Int’l Conference on Information and Knowledge Management , pages 263 270 , Atlanta , GE , Nov . 2001 .
[ 5 ] B K Yi , N . D . Sidiropoulos , T . Johnson , H . V . Jagadish , C . Faloutsos , and A . Biliris . Online data mining for co evolving time sequences . In Proc . of the 16th Int'l Conference on Data Engineering , pages 13 22 , San Diego , CA , Feb . 2000 .
[ 6 ] H . S . Javitz and A . Valdes . The NIDES statistical component description and justification . Annual report , March 1994 . [ 7 ] S . Brin , R . Motwani , J . D . Ullman , and S . Tsur . Dynamic itemset counting and implication rules for market basket data . In Proc . of the ACM SIGMOD Int'l Conference on Management of Data , pages 255 264 , Tucson , AZ , May 1997 .
[ 8 ] R . C . Agarwal , C . C . Aggarwal and VVV Prasad . Depth first generation of long patterns . In Proc . of the 6th ACM SIGKDD Int'l Conference on Knowledge Discovery and Data Mining , pages 108 118 , Boston , MA , Sept . 2000 .
[ 9 ] C . Hidber . Online association rule mining . In Proc . of the ACM SIGMOD Int'l Conference on Management of Data , pages 145 156 , Philadelphia , PA , May 1999 .
[ 10 ] R . Agrawal , and R . Srikant . Fast algorithms for mining association rules . In Proc . of the 20th Int'l Conference on Very Large Databases , Santiago , Chile , Sept . 1994 .
492
