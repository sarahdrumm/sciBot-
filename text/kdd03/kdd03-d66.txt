On Computing , Storing and Querying Frequent Patternsfi
Guimei Liu Hongjun Lu Wenwu Lou The Hong Kong Univ . of Science & Technology
Hong Kong , China fcslgm , luhj , wwloug@csusthk
ABSTRACT Extensive efforts have been devoted to developing efficient algorithms for mining frequent patterns . However , frequent pattern mining remains a time consuming process , especially for very large datasets . It is therefore desirable to adopt a \mining once and using many times" strategy . Unfortunately , there has been little work reported on managing and organizing a large set of patterns for future use . In this paper , we propose a disk based data structure , CFP tree ( Condensed Frequent Pattern Tree ) , for organizing frequent patterns discovered from transactional databases . In addition to an efficient algorithm for CFP tree construction , we also developed algorithms to efficiently support two important types of queries , namely queries with minimum support constraints and queries with item constraints , against the stored patterns , as these two types of queries are basic building blocks for complex frequent pattern related mining tasks . Comprehensive experimental study has been conducted to demonstrate the effectiveness of CFP tree and efficiency of related algorithms .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications Data Mining
Keywords data mining and data warehousing , frequent pattern mining
1 .
INTRODUCTION
Mining frequent patterns is an important problem in data mining area . During the past decade , extensive efforts have been devoted to developing efficient algorithms for mining frequent patterns [ 1 , 7 , 12 , 2 , 16 , 3 , 11 , 9 , 5 , 4 ] . Despite fiThis work was partly supported by the Research Grant Council of the Hong Kong SAR , China ( CUHK4229/01E , Grants DAG01/02.EG14 ) and National 973 Fundamental Research Program of China ( G1998030414 ) .
606607
Jeffrey Xu Yu
The Chinese University of Hong Kong
Hong Kong , China yu@secuhkeduhk all these efforts , frequent pattern mining remains a timeconsuming process . Therefore it is desirable to be conducted in a \mining once and using many times" fashion . However , there is a lack of studies in managing and organizing a large set of patterns on disk for future use . For this purpose , we propose a disk based data structure , called Condensed Frequent Pattern Tree , or CFP tree for short , to organize frequent patterns on disk , and develop efficient algorithms to retrieve patterns from it . Another motivation of our work is that frequent pattern mining requires a predefined minimum support threshold . In real applications , there are often no guidelines for choosing the proper minimum support , which makes frequent pattern mining a repeated process to find appropriate threshold for a given database and application . Furthermore , for the same database , different applications may require different thresholds . It is preferable to materialize the frequent patterns with a sufficiently low minimum support such that most , if not all , of the user requests can be answered by querying the materialized pattern set .
Given a minimum support threshold , the complete set of frequent patterns may be undesirably large , especially when long patterns exist . Recently , researchers have proposed to mine only the frequent closed patterns to reduce the output size [ 8 , 15 , 10 , 14 ] . A pattern is closed if none of its proper supersets has the same support as it has . The set of frequent closed patterns is the most concise representation of the whole frequent pattern set without information loss , and it could be significantly smaller than the complete set of frequent patterns . Inspired by this , only frequent closed patterns are included in the CFP tree structure . Furthermore , different patterns in the CFP tree structure can share the storage of their prefixes , which makes CFP tree a very compact data structure .
A CFP tree constructed with minimum support threshold min sup can efficiently support two types of important queries related to frequent pattern mining : ( 1 ) query with minimum support constraint , for example , \find all the patterns with support higher than s%" , where s min sup ; and ( 2 ) query with item constraint , eg \find all the frequent patterns containing items in I 0" , where I 0 is the set of items a user is interested in . These two types of queries are very common in practice , and are also essential for efficiently evaluating more complex queries .
The main contributions of our work can be summarized as follows . ( 1 ) We proposed a compact and efficient data structure , CFP tree , for storing and querying frequent patterns on disk . ( 2 ) We proposed a set of CFP tree related algorithms , including a fast tree construction algorithm , ef
TflIflDfl 1fl 2fl 3fl 4fl 5fl 6fl 7fl
Tflrflaflnflsflaflcfltflifloflnflsfl afl,fl flbfl,fl flcfl,fl flffl,fl flmfl,fl flpfl afl,fl fldfl,fl flefl,fl flffl,fl flgfl afl,fl flbfl,fl flffl,fl flmfl,fl flnfl afl,fl flcfl,fl flefl,fl flffl,fl flmfl,fl flpfl dfl,fl flffl,fl flnfl,fl flpfl afl,fl flcfl,fl flhfl,fl flmfl,fl flpfl afl,fl fldfl,fl flmfl,fl flsfl
Afllfllfl flFflrfleflqflefluflnfltfl flPflafltfltfleflrflnflsfl cfl:fl3fl,fl fldfl:fl3fl,fl flpfl:fl4fl,fl flffl:fl5fl,fl flmfl:fl5fl,fl flafl:fl6fl cflpfl:fl3fl,fl flcflmfl:fl3fl,fl flcflafl:fl3fl,fl flpflffl:fl3fl,fl flpflmfl:fl3fl,fl pflafl:fl3fl,fl flfflmfl:fl3fl,fl flfflafl:fl4fl,fl flmflafl:fl5fl cflpflmfl:fl3fl,fl flcflpflafl:fl3fl,fl flcflmflafl:fl3fl,fl pflmflafl:fl3fl,fl flfflmflafl:fl fl3fl cflpflmflafl:fl fl3fl mfliflnfl_flsfluflpfl fl=fl fl4fl0fl%fl
Fflrfleflqflufleflnfltfl
Cfllfloflsflefldfl flPflafltfltfleflrflnflsfl dfl:fl fl3fl,fl flpfl:fl fl4fl,fl flffl:fl fl5fl,fl flafl:fl fl6fl pflffl:fl fl3fl,fl flfflafl:fl fl4fl,fl flmflafl:fl fl5fl fflmflafl:fl fl3fl cflpflmflafl:fl3fl mfliflnfl_flsfluflpfl fl=fl fl4fl0fl%fl
1fl cfl:fl3fl dfl:fl3fl pfl:fl4fl ffl:fl5fl mfl:fl5fl afl:fl6fl
2fl pfl:fl3fl
5fl ffl:fl3fl
8fl afl:fl5fl mfl:fl3fl
3fl
4fl afl:fl3fl
6fl mfl:fl3fl afl:fl4fl
7fl afl:fl3fl
( a ) Database
( b )
( c )
( d ) CFP tree
Figure 1 : A CFP tree Example ficient algorithms for finding all frequent patterns wrt a minimum support threshold and efficient algorithms for finding all frequent patterns containing a given set of items . ( 3 ) Comprehensive experimental study has been conducted to demonstrate the effectiveness of CFP tree and efficiency of related algorithms .
The rest of the paper is organized as follows . Section 2 describes the CFP tree structure and its properties ; query processing algorithms are presented in Section 3 ; Section 4 presents the CFP tree construction algorithm ; Section 5 shows experiment results ; some related works are introduced in Section 6 ; finally , Section 7 concludes the paper .
2 . THE CFP›TREE STRUCTURE
In this section , we use an example to illustrate the CFP tree structure and describe its properties . 2.1 An Example
Given a transaction database D as shown in Figure 1(a ) and minimum support 40 % , the complete set of frequent patterns and the frequent closed patterns are shown in Figure 1(b ) and Figure 1(c ) respectively . For brevity , an itemset fi1 ; i2 ; ; img with support s is represented as i1i2 im : s . In this example the set of frequent closed patterns is substantially smaller than the complete pattern set . In particular , all the patterns containing c can be represented by a single frequent closed pattern cpma . Such case is quite common in real datasets .
The CFP tree constructed from D with minimum support 40 % is shown in Figure 1(d ) . The nodes in the tree are numbered according to their creation time . Each node in a CFP tree is represented as a variable length array , and all the items in a node are stored in ascending order of their frequencies . Both types of queries can benefit from this ordering method as explained later in next subsection . A path in the tree starting from an entry in the root node represents a frequent pattern . All the entries in the same node share the same prefix . For an entry E in a node , suppose the pattern represented by the path from root to E is p , the entry E stores four pieces of information : ( 1 ) the last item of p , ( 2 ) the support of p , ( 3 ) a pointer pointing to the root of the sub CFP tree that stores all the patterns having p as prefix , and ( 4 ) a hash bitmap which is not shown in Figure 1(d ) . In the rest of this paper , for an entry E in a CFP tree node , we will use E:item to denote the item stored in E , E:support to denote its support , E:child to denote the pointer to the root of its sub CFP tree , and E:bitmap to denote its hash bitmap .
2.2 Properties
The CFP tree structure is called condensed , not only because it stores merely frequent closed patterns , but also because patterns can share the storage of their prefixes in the tree . It can lead to great space saving because item sharing is quite common among patterns . The CFP tree structure has two important properties , which can be utilized in query processing .
Apriori Property . For an entry E in a CFP tree node , the support of any pattern in the subtree pointed by E cannot be greater than E:support . We can take advantage of this property when processing queries with minimum support constraints . If the support of an entry does not satisfy the minimum support constraint specified in the query , then there is no need to access the subtree pointed by the entry . Left Containment Property . For any entry E , E:item can only appear in the subtrees pointed by the entries before E or in E itself . This property can be utilized when answering queries with item constraint . To find all the patterns containing an item E:item , only the subtrees pointed by the entries before E and E itself needs to be accessed .
Now we show how the item ascending order can help query processing . The Left Containment Property implies that the subtrees pointed by entries at the beginning of a node have a higher chance to be visited than the subtrees pointed by entries at the end of a node . An entry with lower minimum support will very possibly point to a smaller subtree . All the entries in a node are sorted in ascending order of their frequencies , so every time only small trees are to be traversed . It results in great saving when processing queries with item constraints . For queries with minimum support constraints , the most infrequent item of every pattern is first checked based on the ascending ordering method , so those patterns that contain an item dissatisfying the minimum support constraint can be quickly discarded .
To further avoid unnecessary traversal cost when processing queries with item constraints , we maintain a hash bitmap at each entry . Given an entry E in a CFP tree node , for every item i in the subtree pointed by E , the j th bit of E:bitmap is set to 1 , where j=i mod N and N is the length of the bitmap . Thus before performing search on the subtree pointed by E:child , we can first check E:bitmap to see whether all the items being searched are in the sub CFPtree . If the hash bitmap indicates that all the items are in the sub CFP tree , we need to continue the search on that subtree , otherwise the search on that subtree can be terminated . Other hash functions can be used here as well , as long as the hash function does not introduce false dismissal .
607608
The length of the hash bitmap is a trade off between the size of the CFP tree and the search time . In our experiments , the length of the bitmap is set to 32 .
3 . QUERY PROCESSING ON CFP›TREE
In this section , we show how to utilize the two properties , the ascending frequency order and the hash bitmap to process queries . We consider two basic types of queries : ( 1 ) query with minimum support constraint , and ( 2 ) query with item constraint . At the end of this section , we briefly discuss how to process queries with both constraints . 3.1 Query with Minimum Support Constraint A query with minimum support constraint is to output all frequent patterns wrt a user specified minimum support min sup , where min sup has to be no less than the constructing support of the CFP tree . According to the Apriori property of the CFP tree , for such queries only the subtrees pointed by an entry with support no less than min sup should be searched . At each node items are sorted according to their frequencies , so a binary search can be performed to find the first entry whose support is no less than min sup . Suppose this entry is E , then only the subtrees pointed by entries after E and by E itself need to be accessed . Algorithm 1 shows the pseudo code for the search algorithm . BinarySearch(cnode , min sup ) procedure returns the first entry in cnode whose support is no less than min sup .
Algorithm 1 Search Minsup Algorithm Input : p is a frequent pattern cnode the cfp tree node pointed by p min sup is the minimum support threshold
Description : 1 : if p 6= ; AND ( p has more than one children OR support(p ) > the support of p ’s only child ) then output p and its support ; s = p SfE:itemg ; if E:child 6= NULL then
2 : 3 : end if 4 : E1 = BinarySearch(cnode , min sup ) ; 5 : for all entry E 2 cnode , E = E1 or E after E1 do 6 : 7 : 8 : 9 : 10 : 11 : end if 12 : end for output pattern s and its support E:support ;
Search Minsup(s , E:child , min sup ) ; else
CFP tree stores some patterns which are prefixes of some closed patterns , but themselves are not closed , eg pattern cp and cpm in Figure 1(d ) . To output only the closed patterns , we need to check whether a pattern has a greater support than its children before output it ( line 1 3 ) . Consider an example : find all the frequent patterns wrt 50 % . In the CFP tree shown in Figure 1(d ) , a binary search is performed on node 1 , and p is found to be the first item with support no less than 50 % . All the entries before p can be ignored . The node pointed by p ( node 5 ) is visited , and no item has support greater than 50 % . Next the node pointed by f ( node 6 ) is accessed . Again a binary search is performed and item a is found to be frequent . Finally , patterns ma and a are found to be frequent and closed .
608609
3.2 Query with Item Constraint
Based on the Left Containment property , the item of an entry E appears only in subtrees pointed by the entries before E or in E itself . A subtree pointed by an entry before E may not actually contain E:item . The hash bitmap maintained at each entry can be utilized to reduce unnecessary searching cost . Before a subtree pointed by an entry E is searched , the bitmap of E is first checked ( line 6 7 ) . If and only if it indicates that all the items being searched appear in that subtree , search on that subtree should be continued . Algorithm 2 shows the pseudo code for evaluating queries with item constraint . In Algorithm 2 , if I = ; , then all the subtrees under cnode should be accessed .
Algorithm 2 Search Item Algorithm Input : p is a frequent pattern cnode is the sub CFP tree pointed by p I is the set of items that must be contained in patterns
Description : 1 : if I = ; AND ( p has more than one children OR support(p ) > the support of p ’s only child ) then output p and its support ; check E:bitmap ; if all the items in I , fE:itemg are in E:child then
2 : 3 : end if 4 : E1 = the first entry of cnode that satisfies E1:item 2 I ; 5 : for all entry E 2 cnode , E before E1 or E=E1 do 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : end if 15 : end for s = p S E:item ; if E:child 6= NULL then
Search Item(s , E:child , I , fE:itemg ) ; else if I , fE:itemg is empty then output s and its support E:support ; end if
To process query \find all the patterns containing item p and f " on the CFP tree shown in Figure 1(d ) , only the subtrees pointed by entry c , d and p at node 1 need to be accessed . We start from entry c , and examine the hash bitmap of entry c , suppose it indicates that both items appear in the subtree rooted at node 2 ( the hash bitmap may introduce false alarm , but no false dismissal ) , then the search on node 2 should be continued . Node 2 contains only one entry p , we check its bitmap to see whether f exists in the subtree rooted at node 2 . Suppose the corresponding bit of f in the bitmap is 0 , the search at node 2 is stopped . Then we continue our search on entry d at node 1 . It points to an empty subtree . The next entry of node 1 is p , and its bitmap indicates that f exists in node 5 since there indeed exists a f in the subtree . Then node 5 is searched and we find f . Node 5 points to an empty tree , the search is finished . 3.3 Query with Both Constraints
Another type of query is to have both minimum support constraint and item constraint . The algorithm for evaluating such queries can also combine the pruning power of the Apriori property , the Left Containment property and the hash bitmap . Let us consider a query \find all the patterns with minimum support 50 % and containing item p and f " on the database in Figure 1(a ) . According to the item constraint , we only need to check the entries c , d and p at node
1 . And according to the minimum support constraint , only entry p and the entries after p need to be considered . Therefore , only the subtree pointed by entry p in node 1 needs to be accessed .
4 . CFP›TREE CONSTRUCTION
In this section , we briefly present the algorithm for constructing the CFP tree , which is similar to the AFOPT algorithm [ 4 ] except that we adopted several techniques to remove redundant patterns . 4.1 Construction Algorithm
Given a transactional database D and a minimum support threshold , the CFP tree structure can be constructed with only two database scans by adopting the pattern growth approach . In the first database scan , all the frequent items in the database are mined , and they are sorted in ascending order of their frequencies . Then a CFP tree node is created , which contains all the frequent items and their supports . Let the set of frequent items be F = fi1 ; i2 ; ; img . We perform another database scan , and construct a conditional database for each ij 2 F , denoted by Dij . During the second scan of the database , infrequent items in each transaction t are removed and the remaining items are sorted according to their orders in F . Transaction t is put into Dij if the first item of t is ij . The conditional databases contain the complete information for mining frequent patterns . Once they are built , the remaining mining will be performed on them . There is no need to access the original database .
We first perform mining on Di1 to mine all the patterns containing i1 . Mining on individual conditional database follows the same process as mining on the original database . After the mining on Di1 is finished , Di1 can be discarded . Since it also contains other items , the transactions in it will be inserted into the remaining conditional databases . Given a transaction t in Di1 , suppose the next item after i1 in t is ij , then t will be inserted into Dij . This step is called pushright . Sorting the items in ascending order of their frequencies ensures that every time , a small conditional database is pushed right : i1 is the most infrequent item , and Di1 contains fewest transactions , with the increasing of j , the number of transactions in Dij increases , but the number of distinct items in Dij shrinks and the transaction length decreases .
The pseudo code of the construction algorithm is shown in Algorithm 3 . Algorithm 3 is independent of the representation of the conditional databases . We choose to use the prefix tree structure , in which different transactions can share their prefixes . We chose this structure not only because it is compact , but also because it allows quick removal of redundant patterns , e.g the identification of single child can be very easy . Further optimizations can be made on the prefix tree structure , eg using arrays to store single branches . We do not discuss further details here . 4.2 Removing Redundant Patterns
Algorithm 3 generates all the frequent patterns and stores them in CFP tree . In this subsection , we describe how to remove redundant patterns during mining process . A pattern is called redundant if it is not closed . We have the following two lemmas .
Lemma 1 . In Algorithm 3 , a pattern p is closed if and only if two conditions hold : ( 1 ) there is no previously mined
Algorithm 3 CFP Construct Algorithm Input : p is a frequent pattern Dp is the conditional database of p Ep is the entry of p in CFP tree min sup is the minimum support threshold ;
Description : 1 : Scan Dp count frequent items , let F denotes them ; 2 : Sort items in F in ascending order of their frequencies ; 3 : Create a new CFP tree node cnode , put items in F and their supports in cnode ; set ( i mod N ) th bit of Ep:bitmap to 1 ;
4 : Ep:child = cnode ; 5 : for all item i 2 F do 6 : 7 : Dp Sfig = ; ; 8 : end for 9 : for all transaction t 2 Dp do 10 : remove infrequent items from t , and sort remaining items according to their orders in F ; let i be the first item of t , insert t into Dp Sfig .
11 : 12 : end for 13 : for all item i 2 F do 14 : 15 : 16 : 17 : 18 : end for s = p Sfig ; Es = the entry of i in cnode ; CFP Construct(s , Ds , Es , min sup ) ; PushRight(Ds ) ; pattern which is a superset of p and has the same support as p ; ( 2 ) all the items in Dp have a lower support than p .
Proof : If all the items in Dp has a lower support than p , then all the patterns mined from Dp must have a lower support than p according to the Apriori property . Suppose the last item of p is i , then all the conditional databases after Dp cannot contain i according to the construction of the conditional databases , so any pattern mined from them cannot be a superset of p . That means only patterns mined from previous conditional databases can be a superset of p , if no such pattern exists , then p is closed .
Lemma 2 . In Algorithm 3 , if a pattern p is not closed because condition ( 1 ) in Lemma 1 does not hold , then none of the patterns mined from Dp can be closed .
Proof : Pattern p is not closed because condition ( 1 ) does not hold , then there exists a previously mined pattern q , p fl q and p ; q have the same support . For every pattern s mined from Dp , the pattern t = ( s , p ) S q has the same support as s and s fl t , so s is not closed .
Based on Lemma 1 , there are two pruning conditions for a redundant pattern p : ( 1 ) Examine whether there exists a previously mined pattern q , which is a superset of p and has the same support as q . This checking can be done before the mining on a pattern ’s conditional database starts . If such q exists , then there is no need to perform mining on Dp based on Lemma 2 . Thus the identification of a redundant pattern not only reduces the size of the tree , but also avoids ( 2 ) Check whether there exists unnecessary mining cost . an item i , which appears in every transaction of Dp . If such i exists , then there is no need to consider the patterns that do not contain i when mining Dp . In other words , we can directly perform mining on Dp Sfig instead of Dp . The efforts for mining Dp Sfjg , j 6= i are saved .
609610
Data Sets BMS POS
Pumsb
T20I10D1000k
Size
#Trans #Items AvgTL
19.20M 515,597 49,046 14.75MB 89.57MB 987,139
1657 2113 8876
6.53 74.00 20.23
Table 1 : Datasets
To incorporate the above two pruning techniques , the items that have the same support as p are removed from F before the new CFP tree node for F is created ( line 3 ) . For each of such item i , a new CFP tree node nodei is created , which contains only item i itself . Ep or the most recently created CFP tree node points to nodei . At line 14 , before performing mining on Ds , condition ( 1 ) in Lemma 1 is checked . If it does not hold , then the mining on Ds can be skipped . 4.3 Closed Pattern Checking
When we do condition ( 1 ) pruning , p should be compared with previously mined frequent closed patterns . Since the CFP tree constructed during the mining process stores all the frequent closed patterns mined so far , we can use the search algorithms proposed in Section 3 to do the closeness checking . For a pattern p , we call Search Both( ; , root , support(p ) , p ) to find some previously mined pattern q such that p fl q and support(p)=support(q ) . If such q exists , then we can safely discard Dp . Previous algorithms for mining frequent closed patterns require that all the frequent closed patterns must be in memory to do this checking . Our closeness checking technique does not have this requirement since our search algorithm is very efficient on disk . Moreover , the CFP tree structure is a compact representation of the patterns , so it has a higher chance to be held in memory than the flat representation of the patterns .
5 . PERFORMANCE STUDY
We conducted a set of experiments to demonstrate the efficiency of the CFP tree structure . All the experiments were conducted on a 2.24Ghz Pentium IV with 512MB memory running Microsoft Windows XP . All codes were complied using Microsoft Visual C++ 60
Table 1 shows the several datasets used for performance study . BMS POS [ 17 ] is a large and sparse dataset containing click stream data . Pumsb is a dense dataset obtained from UCI machine learning repository . T20I10D1000k is a large synthetic dataset generated by IBM Quest Synthetic Data Generation Code . Table 1 lists some statistical information about the datasets , including the size of the dataset on disk , the number of transactions , the number of distinct items and the average transaction length . 5.1 Query Processing
We built a CFP tree for the three datasets respectively with minimum support 0.02 % , 0.05 % and 50 % . The size of the CFP tree for three datasets is 80.7MB , 199.5MB and 142.1MB respectively . We issued a set of queries with minimum support constraint or item constraint to see the efficiency of our query processing algorithms . We compared three algorithms : ( 1 ) the query processing algorithms proposed in this paper , denoted by \CFP" , ( 2 ) the sequential scan algorithm , denoted by \SCAN" , and ( 3 ) mining from scratch , denoted by \MINE" . For sequential scan algorithm , we stored all the closed patterns in a flat file . For every pattern , we kept its length , support and the set of items in it .
) c e s ( e m T i
1000
100
10
1
0.1
0.01
1000
100
10
1
0.1
) c e s ( e m T i
Data set : BMS POS ( 0.02 % )
Data set : BMS POS ( 0.02 % )
MINE SCAN CFP
0.05
0.1
0.15
0.2
0.25
Minimum Support ( % )
) c e s ( e m T i
10000
1000
100
10
1
0.1
0.01
MINE SCAN CFP
10 20 30 40 50 60 70 80 90
Item Frequency ( x1000 )
( a ) BMS POS
( b ) BMS POS
Data set : pumsb ( 50 % )
Data set : pumsb ( 50 % )
MINE SCAN CFP
MINE SCAN CFP
10000
1000
100
10
1
0.1
) c e s ( e m T i
0.01
55
60
65
75 Minimum Support ( % )
70
80
85
0.01
25
30
35
40
45
Item Frequency ( x1000 )
( c ) pumsb
( d ) pumsb
Data set : T20I10D1000k ( 0.05 % )
Data set : T20I10D1000k ( 0.05 % )
MINE SCAN CFP
MINE SCAN CFP
1000
100
10
1
0.1
) c e s ( e m T i
0.15 Minimum Support ( % )
0.2
0.25
0.3
0.01
0
5
10
20 Item Frequency ( x1000 )
15
25
1000
100
10
1
0.1
) c e s ( e m T i
0.01
0.1
( e ) T20I10D1000k
( f ) T20I10D1000k
Figure 2 : Query Processing Time
Then we sequentially scan the file to find all the patterns that satisfy the minimum support constraint or item constraint . The y axis in all figures is logarithmic . For queries with item constraint , we used only one item as the constraint . The selection of the item is based on its frequency . The less frequent of an item , the more pruning power of it in searching the CFP tree structure and also in the mining process . The x axis in Figure 2(b ) , 2(d ) and 2(f ) is the frequency of the selected item . For \MINE" algorithm , we set the minimum support to the building minimum support of the CFP tree . Figure 2 shows the time for processing two types of queries on three datasets . In most cases , searching patterns from pre computed results needs less time than mining from scratch . The time for sequential scan does not vary with the minimum support and the frequency of the items . The time for retrieving patterns from CFP tree increases with the minimum support threshold and the frequency of the items , but can hardly exceeds the time for sequential scan . 5.2 Construction Time and CFP›tree Size
The CFP tree can be constructed quickly using our proposed algorithm . We compared our construction algorithm with CLOSET+[14 ] , which is shown to consistently defeat
610611 other frequent closed pattern mining algorithms . The two algorithms show comparable performance on BMS POS . Figure 3 shows the running time on the other two datasets .
) c e s ( e m T i
100000
10000
1000
100
10
Data set : T20I10D1000k
CLOSET+ CFP
0.05
0.1
0.15
0.2
0.25
Minimum Support( % )
) c e s ( e m T i
10000
1000
100
10
1
0.1
Data set : pumsb
CLOSET+ CFP
45 50 55 60 65 70 75 80 85 90
Minimum Support( % )
( a ) T20I10D1000k
( b ) pumsb
Figure 3 : CFP tree Construction Time
We also compared the size of the CFP tree structure with the size of the closed pattern set stored in flat files . The size of the CFP tree structure can be much smaller than the flat representation , especially when dataset is dense . For example , on pumsb dataset with minimum support 50 % , the size of the CFP tree is almost one third of the size of flat representation . Due to the limitation of the space , detailed experiment results are not shown here .
6 . RELATED WORK
During the past decade , extensive efforts have been devoted to developing efficient algorithms for mining frequent patterns . They can be classified into two categories : the Apriori family algorithms[1 , 7 , 12 , 2 ] and the pattern growth based algorithms[16 , 3 , 11 , 9 , 5 , 4 ] . The pattern growth based algorithms differ mainly in the representation of the conditional databases . There were also works on mining only frequent closed patterns [ 8 , 15 , 10 , 14 ] .
However , there is a lack of studies on how to store and retrieve frequent patterns . Morzy et al . proposed a group bitmap index structure for retrieving association rules stored in a relation database [ 6 ] . This technique is similar to our hash bitmap . Tuzhilin et al . proposed a rule query language Rule QL for querying multiple rulebases and a number of efficient query evaluation techniques for Rule QL [ 13 ] . They use separated indexes for support/confidence constraint and item constraint . More specifically , they use B+ trees to index the support and the confidence of the rules , and use inverted lists for subset matching . We believe that the CFPtree structure can be an alternative for indexing association rules .
7 . DISCUSSION AND CONCLUSIONS
In this paper , we proposed a compact and efficient data structure , CFP tree , for storing and querying frequent patterns on disk . With CFP tree , frequent pattern mining can be conducted in \mining once and using many times" fashion by querying the stored CFP tree . Two types of important queries related to frequent itemset mining can be answered quickly using our proposed query processing algorithms , namely the query with minimum support constraint and the query with item constraint . We have also observed consistent results concerning queries with both constraints .
In fact , the benefit gained by the CFP tree structure was more significant because we can exploit the pruning powers of the two constraints at the same time . This part of experiment results are not shown in this paper due to the limitation of space .
In this paper we did not discuss how to update the CFPtree structure when underlying database changes . A simple but costly solution is to reconstruct the tree periodically . A more efficient approach is to adopt the idea of the incremental mining algorithms to minimize the scanning cost of the original database .
8 . REFERENCES [ 1 ] R . Agrawal , T . Imielinski , and A . N . Swami . Mining association rules between sets of items in large databases . In SIGMOD , 1993 .
[ 2 ] S . Brin , R . Motwani , J . D . Ullman , and S . Tsur .
Dynamic itemset counting and implication rules for market basket data . In SIGMOD , 1997 .
[ 3 ] J . Han , J . Pei , and Y . Yin . Mining frequent patterns without candidate generation . In SIGMOD , 2000 .
[ 4 ] G . Liu , H . Lu , Y . Xu , and J . X . Yu . Ascending frequency ordered prefix tree : Efficient mining of frequent patterns . In DASFAA , 2003 .
[ 5 ] J . Liu , Y . Pan , K . Wang , and J . Han . Mining frequent item sets by opportunistic projection . In SIGKDD , 2002 .
[ 6 ] T . Morzy and M . Zakrzewicz . Group bitmap index : A structure for association rules retrieval . In SIGKDD , 1998 .
[ 7 ] J . S . Park , M . Chen , and P . S . Yu . An effective hash based algorithm for mining association rules . In SIGMOD , 1995 .
[ 8 ] N . Pasquier , Y . Bastide , R . Taouil , and L . Lakhal . Discovering frequent closed itemsets for association rules . In ICDT , 1999 .
[ 9 ] J . Pei , J . Han , H . Lu , S . Nishio , S . Tang , and
D . Yang . H mine : Hyper structure mining of frequent patterns in large databases . In ICDM , 2001 .
[ 10 ] J . Pei , J . Han , and R . Mao . Closet : An efficient algorithm for mining frequent closed itemsets . In DMKD , 2000 .
[ 11 ] RCAgarwal , CCAggarwal , and VVVPrasad A tree projection algorithm for finding frequent itemsets . Journal on Parallel and Distributed Computing , 61(3):350{371 , 2001 .
[ 12 ] A . Savasere , E . Omiecinski , and S . B . Navathe . An efficient algorithm for mining association rules in large databases . In VLDB , 1995 .
[ 13 ] A . Tuzhilin and B . Liu . Querying multiple sets of discovered rules . In SIGKDD , 2002 .
[ 14 ] J . Wang , J . Pei , and J . Han . Closet+ : Searching for the best strategies for mining frequent closed itemsets . In SIGKDD , 2003 .
[ 15 ] M . J . Zaki and C . Hsiao . Charm : An efficient algorithm for closed itemset mining . 2002 .
[ 16 ] M . J . Zaki , S . Parthasarathy , M . Ogihara , and W . Li . New algorithms for fast discovery of association rules . In SIGKDD , 1997 .
[ 17 ] Z . Zheng , R . Kohavi , and L . Mason . Real world performance of association rule algorithms . In SIGKDD , 2001 .
611612
