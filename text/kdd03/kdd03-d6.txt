Accurate Decision Trees for Mining High speed Data
Streams
Jo˜ao Gama
Ricardo Rocha
LIACC , FEP , Univ . do Porto R . do Campo Alegre 823
Projecto Matem´atica Ensino Departamento de Matem ´atica
4150 Porto , Portugal jgama@liaccuppt
3810 Aveiro , Portugal ricardor@matuapt
Pedro Medas
LIACC , Univ . do Porto
R . do Campo Alegre 823
4150 Porto , Portugal pmedas@liaccuppt
ABSTRACT In this paper we study the problem of constructing accurate decision tree models from data streams . Data streams are incremental tasks that require incremental , online , and any time learning algorithms . One of the most successful algorithms for mining data streams is VFDT . In this paper we extend the VFDT system in two directions : the ability to deal with continuous data and the use of more powerful classiflcation techniques at tree leaves . The proposed system , VFDTc , can incorporate and classify new information online , with a single scan of the data , in time constant per example . The most relevant property of our system is the ability to obtain a performance similar to a standard decision tree algorithm even for medium size datasets . This is relevant due to the any time property . We study the behaviour of VFDTc in difierent problems and demonstrate its utility in large and medium data sets . Under a bias variance analysis we observe that VFDTc in comparison to C4.5 is able to reduce the variance component .
Categories and Subject Descriptors H28 [ Database Management ] : Database applications| data mining ; I26 [ Artiflcial Intelligence ] : Learning| classiflers design and evaluation
Keywords Data Streams , Incremental Decision Trees , Functional Leaves
1 .
INTRODUCTION
Databases are rich in information that can be used in the decision process . Nowadays , most of the companies and organizations possess gigantic databases , that grow to a limit of millions of registers per day . In traditional applications of data mining the volume of data is the main obstacle to the use of memory based techniques due the restrictions in the computational resources : memory , time or space in hard disk . Therefore in most of these systems , the use of all available data becomes impossible and can result in under fltting . The construction of KDD systems that use the entire amount of data and keep the accuracy of the traditional systems becomes problematic .
Decision trees , due to its characteristics , are one of the most used techniques for data mining . Decision tree models are non parametric , distribution free , and robust to the presence of outliers and irrelevant attributes . Tree models have high degree of interpretability . Global and complex decisions can be approximated by a series of simpler and local decisions . Univariate trees are invariant under all ( strictly ) monotone transformations of the individual input variables . Usual algorithms that construct decision trees from data use a divide and conquer strategy . A complex problem is divided into simpler problems and recursively the same strategy is applied to the sub problems . The solutions of sub problems are combined in the form of a tree to yield the solution of the complex problem .
Data streams are , by deflnition , problems where the training examples used to construct decision models come over time , usually one at time . A natural approach for this incremental task is incremental learning algorithms . In the fleld of incremental tree induction a successful technique maintains at each decision node a set of su–cient statistics and only make a decision ( install a split test in that node ) , when there is enough statistical evidence in favour of a particular split test . This is the case of [ 6 , 3 ] . In this paper we argue that incremental tree induction methods that only install a split test when there is enough statistical support , will large beneflt of using more appropriate classiflcation strategies at tree leaves . This is the main idea behind this paper .
We propose the VFDTc system , which incorporates two main extensions to the VFDT system : the ability to deal with numerical attributes and the ability to apply naive Bayes classiflers in tree leaves . The paper is organized as follows . The next section describes VFDT and other related work that is the basis for this paper . The section 3 presents our extensions to VFDT leading to system VFDTc . We detail the major options that we implemented and the difierences to VFDT and others available systems . The system has been implemented and evaluated . Experimental evaluation is done in Section 4 . Last section concludes the paper , resuming the main contributions of this work .
2 . RELATED WORK
In this section we analyse related work in two dimensions .
One dimension is related to the use of more powerful classiflcation strategies at tree leaves , the other dimension is related to methods for incremental tree induction .
Functional Tree Leaves The standard algorithm to construct a decision tree usually install at each leaf a constant that minimizes a given loss function . In the classiflcation setting , the constant that minimizes the 0 1 loss function is the mode of the target attribute of the examples that fall at this leaf . Several authors have studied the use of other functions at tree leaves [ 9 , 5 ] . One of the earliest works is the Perceptron tree algorithm [ 11 ] where leaf nodes may implement a general linear discriminant function . Also Kohavi [ 9 ] has presented the naive Bayes tree that uses functional leaves . NBtree is a hybrid algorithm that generates a regular univariate decision tree , but the leaves contain a naive Bayes classifler built from the examples that fall at this node . The approach retains the interpretability of naive Bayes and decision trees , while resulting in classiflers that frequently outperform both constituents , especially in large datasets .
In this paper we explore this idea in the context of learning from data streams . As we show in the experimental section , there are strong advantages in the performance of resulting decision models .
Incremental Tree Induction In many interesting domains , the information required to learn concepts is rarely available a priori . Over time , new pieces of information become available , and decision structures should be revised . This learning mode has been identifled and studied in the machine learning community under several designations : incremental learning , online learning , sequential learning , theory revision , etc . In the case of tree models , we can distinguish two main research lines . The flrst one , a tree is constructed using a greedy search . Incorporation of new information involves the re structuring the actual tree . This is the case of systems like ITI [ 12 ] , or ID5R[8 ] . The second research line does not use the greedy search of standard tree induction . It maintains a set of sufflcient statistics at each decision node and only make a decision , i . e . , install a split test at that node when there is enough statistical evidence in favour of a split test . This is the case of [ 6 , 3 ] . A notable example , is the VFDT system [ 3 ] . It can manage thousand of examples using few computational resources with a performance similar to a batch decision tree given enough examples .
In VFDT a decision tree is learned by recursively replacing leaves with decision nodes . Each leaf stores the su–cient statistics about attribute values . The su–cient statistics are those needed by a heuristic evaluation function that evaluates the merit of split tests based on attribute values . When an example is available , it traverses the tree from the root to a leaf , evaluating the appropriate attribute at each node , and following the branch corresponding to the attribute ’s value in the example . When the example reaches a leaf , the su–cient statistics are updated . Then , each possible condition based on attribute values is evaluated . If there is enough statistical support in favour of one test over the others , the leaf is changed to a decision node . The new decision node will have as many descendant leaves as the number of possible values for the chosen attribute ( therefore this tree is not necessarily binary ) . The decision nodes only maintain the information about the split test installed in this node . The initial state of the tree consists of a single leaf : the root of the tree . The heuristic evaluation function is the Information Gain ( denoted by H(¢))1 . The su–cient statistics for estimating the merit of a nominal attribute are the counts nijk , representing the number of examples of class k that reach the leaf , where the attribute j takes the value i . The Information Gain measures the amount of information that is necessary to classify an example that reach the node : H(Aj ) = inf o(examples ) ¡ inf o(Aj ) . The information of the attribute j is given by : inf o(Aj ) =
,
Pi Pi(cid:181)Pk ¡Pik log2(Pik)¶ where Pik = nijk Pa najk probability of observing the value of the attribute i given class k and Pi = Pa nija is the probability of observing Pa Pb the value of attribute i . is the najb
The main innovation of the VFDT system is the use of Hoefiding bounds to decide how many examples are necessary to observe before installing a split test at each leaf . Suppose we have made n independent observations of a random variable r whose range is R . The Hoefiding bound states , with probability 1 ¡ – , that the true average of r , „r , is at least
„r ¡ † and † = qR2 ln( 1 – ) 2n .
Let H(¢ ) be the evaluation function of an attribute . For the information gain , the range R , of H(¢ ) is log2#classes . Let xa be the attribute with the highest H(¢ ) , xb the attribute with second highest H(¢ ) and ¢H = H(xa ) ¡ H(xb ) , the difierence between the two better attributes . Then if ¢H > † with n examples observed in the leaf , the Hoefiding bound states with probability 1 ¡ – that xa is really the attribute with highest value in the evaluation function . In this case the leaf must be transformed into a decision node that splits on xa .
The evaluation of the merit function for each example could be very expensive . It turns out that it is not e–cient to compute H(¢ ) every time that an example arrives . VFDT only computes the attribute evaluation function H(¢ ) when a minimum number of examples has been observed since the last evaluation . This minimum number of examples is a user deflned parameter . When two or more attributes continuously have very similar values of H(¢ ) , even with a large number of examples , the Hoefiding bound will not decide between them . To solve this problem the VFDT uses a constant ¿ introduced by the user for run ofi , eg , if ¢H < † < ¿ then the leaf is transformed into a decision node . The split test is based on the best attribute .
3 . THE VFDTC SYSTEM
We implement a system based on the VFDT[3 ] . It uses the Information Gain as the evaluation function , deals with numerical attributes and uses functional leaves to classify test examples .
Numerical attributes Most real world problems contain numerical attributes . Practical applications of learning algorithms to real world problems should address this issue . For batch decision tree learners , this ability requires a sort operation that is the most
1The original description of VFDT is general enough for other evaluation functions ( eg GINI ) . Without loss of generality , we restrict here to the information gain .
Figure 1 : Algorithm to insert value xj of an example label with class y into a Binary Tree
Procedure InsertValueBtree(xj,y,Btree )
Begin if ( xj == Btree.i ) then
BtreeVE[y]++
Elseif ( xj • Btree.i ) then
BtreeVE[y]++ InsertValueBtree(xj,y,BtreeLeft )
Elseif ( xj > Btree.i ) then
BtreeVH[y]++ InsertValueBtree(xj,y,BtreeRight )
End
.
Figure 2 : Algorithm to compute #(Aj • i ) for a given attribute j and class k :
Procedure LessThan(i,k,BTree )
Begin if ( BTree == NULL ) return 0 . if ( BTree.i == i ) return V E[k ] . if ( BTree.i < i ) return
V E[k ] + LessT han(i ; k ; BT ree:Right ) . if ( BTree.i>i ) return LessThan(i,k,BTreeLeft )
End
. time consuming operation.In this section we provide an efflcient method to deal with numerical attributes in the context of online decision tree learning .
In VFDTc a decision node that contains a split test based on a continuous attribute has two descendant branches . The split test is a condition of the form attri • cut point . The two descendant branches corresponds to the values T RU E and F ALSE for the split test . The cut point is chosen from all the possible observed values for that attribute . In order to evaluate the goodness of a split , we need to compute the class distribution of the examples at which the attributevalue is less than or greater than the cut point . The counts nijk are fundamental for compute all necessary statistics . They are kept with the use of the following data structure : In each leaf of the decision tree we maintain a vector of the classes distribution of the examples that reach this leaf . For each continuous attribute j , the system maintains a binary tree structure . A node in the binary tree is identifled with a value i ( that is the value of the attribute j seen in an example ) , and two vectors ( of dimension k ) used to count the values that go through that node . These vectors , V E and V H contain the counts of values respectively • i and > i for the examples labelled with class k . When an example reaches a leaf , all the binary trees are updated . Figure 1 presents the algorithm to insert a value in the binary tree . Insertion of a new value in this structure is O(log n ) where n represents the number of distinct values for the attribute seen so far .
To obtain the Information Gain of a given attribute we use an exhaustive method to evaluate the merit of all possible cut points . In our case , any value observed in the examples so far can be used as cut point .
For each possible cut point , we compute the information of the two partitions using equation 1 . inf o(Aj(i ) ) =
P ( Aj • i ) ⁄ iLow(Aj(i ) ) + P ( Aj > i ) ⁄ iHigh(Aj(i ) )
( 1 ) where i is the split point , iLow(Aj(i ) ) the information of Aj • i ( equation 2 ) and iHigh(Aj(i ) ) ( equation 3 ) the information of Aj > i . So we choose the split point that minimizes ( 1 ) . iLow(Aj(i ) ) =
P ( K = kjAj • i ) ⁄ log2(P ( K = kjAj • i ) )
( 2 )
¡XK iHigh(Aj(i ) ) =
P ( K = kjAj > i ) ⁄ log2(P ( K = kjAj > i ) )
( 3 )
¡XK
These statistics are easily computed using the counts nijk , and using the algorithm presented in flgure 2 . For each attribute , it is possible to compute the merit of all possible cut points traversing the binary tree once . A split point for a numerical attribute is binary . The examples will be divided into two subsets : one representing the True value of the split test and the other the False value of the test installed at the decision node . VFDTc only considers a possible cut point if and only if the number of examples in each 2 percentage of the total of the subsets is higher than pmin number of examples seen in the node .
Discrete attributes The VFDTc does not need previous knowledge of all the possible values of a categorical attribute . The su–cient statistics for discrete attributes are counters of the number of occurrences of an observed attribute value per class . These statistics are enough to compute H(¢ ) . When a test on a discrete attribute is installed in a leaf , the leaf becomes a decision node with as many descendant branches as the number of observed distinct values 3 plus a branch that represents other , not yet observed , values and unknown values . Therefore when an example reaches this node with an unknown value for this attribute , the example follows the branch representing other values .
Functional tree leaves To classify a test example , the example traverses the tree from the root to a leaf . The example is classifled with the most representative class of the training examples that fall at that leaf . One of the innovations of our algorithm is the ability to use the na˜‡ve Bayes classiflers at tree leaves . That is , a test example is classifled with the class that maximizes the posterior probability given by Bayes rule assuming the independence of the attributes given the class .
There is a simple motivation for this option . VFDT only changes a leaf to a decision node when there are a su–cient number of examples to support the change . Usually hundreds or even thousands of examples are required . To classify a test example , the majority class strategy only use the information about class distributions and does not look for the attribute values . It uses only a small part of the available information , a crude approximation to the distribution of the examples . On the other hand , naive Bayes takes into account not only the prior distribution of the classes , but also 2Where pmin is a user deflned constant . 3Known until the moment .
Figure 3 : Algorithm to compute P ( xjjCk ) for numeric attribute xj and class k at a given leaf .
Procedure PNbc(xj,k,Btree , Xh , Xl,Nj )
Xh the highest value of xj observed at the Leaf Xl the lowest value of xj observed at the Leaf Nj the difierent values of xj observed at the Leaf Begin if ( BTree==NULL ) return 0 nintervals = min(10 ; Nj ) . // number of intervals inc = Xh¡Xl Let Counts[nintervals ] be the number nintervals . // interval range of examples between intervals
For i=1 to nintervals
Counts[i ] = LessT han(xl + inc ⁄ i ; k ; BT ree ) If ( i > 1 ) then
Counts[i]=Counts[i] Counts[i 1 ]
If ( xj • Xl + inc ⁄ i ) then return
Counts[i ]
Leaf:nrExs[k ]
ElseIf ( (i==nintervals ) then return
Counts[i ]
Leaf:nrExs[k ]
End the conditional probabilities of the attribute values given the class . In this way , there is a much better exploitation of the available information at each leaf . Moreover , naive Bayes is naturally incremental . It deals with heterogeneous data and missing values . It has been observed [ 4 ] that for small datasets naive Bayes is a very competitive algorithm .
Given the example ¡!e = ( x1 ; :: : ; xj ) and applying Bayes P ( ¡!e ) Q P ( xjjCk ) . To theorem , we obtain : P ( Ckj¡!e ) / P ( Ck ) compute the conditional probabilities P ( xjjCk ) we should distinguish between nominal attributes and continuous ones . In the former the problem is trivial using the su–cient statistics used to compute information gain . In the latter , there are two usual approaches : or assuming that each attribute follows a normal distribution or discretizing the attributes . Assuming a normal distribution , the su–cient statistics can be computed on the ( cid:176)y . Nevertheless , it is possible to compute the required statistics from the binary tree structure stored at each leaf before it comes a decision node . This is the method implemented in VFDTc . Any numerical attribute is discretized into min(10 ; N r : of dif f erent values ) intervals . To count the number of examples per class that fall at each interval we use the algorithm described in flgure 3 . This algorithm is computed only once in each leaf for each discretization bin . Those counts are used to estimate P ( xjjCk ) .
We should note , that the use of naive Bayes classiflers at tree leaves doesn’t introduce any overhead in the training phase . In the application phase and for nominal attributes , the su–cient statistics constitute ( directly ) the naive Bayes tables . For continuous attributes , the naive Bayes contingency tables are e–ciently derived from the Btree ’s used to store the numeric attribute values . The overhead introduced is proportional to depth of the Btree , that is at most log(n ) , where n is the number of difierent values observed for a given attribute .
4 . EVALUATION
In this section we empirically evaluate VFDTc . We con sider three dimensions of analysis : error rate , learning time , and tree size . The main goal of this section is to provide evidence that the use of functional leaves improve the performance of VFDT and most important that it improve its anytime characteristic . In a flrst set of experiments we analyze the efiects of two difierent strategies when classifying test examples : classifying using the majority class ( VFDTcMC ) and classifying using naive Bayes ( VFDTcNB ) at leaves .
The experimental work has been done using the Waveform and LED datasets . These are well known artiflcial datasets . We have used the two versions of the Waveform dataset available at the UCI repository [ 1 ] . Both versions are problems with three classes . The flrst version is deflned by 21 numerical attributes . The second one contains 40 attributes . It is known that the optimal Bayes error is 14 % . The LED problem has 24 binary attributes ( 17 are irrelevant ) and 10 classes . The optimal Bayes error is 26 % .
The choice of these datasets was motivated by the existence of dataset generators at the UCI repository . We could generate datasets with any number of examples and perform a set of learning curves able to evaluate our claim about the any time property of VFDTc .
We have done a set of experiments , using the LED and Waveform datasets . For all datasets , we generate training sets of a varying number of examples , starting from 10k till 1000k . The test set contains 250k examples . The VFDTc algorithm was used with the parameters values – = 5 £ 10e¡6 ; ¿ = 5 £ 10e¡3 and nmin = 200 .
All algorithms run on a Pentium IV at 1GHz with 256 MB of RAM and using Linux RedHat 71 Detailed results are presented in table 1 .
Classi£cation strategy : Majority Class vs . naive Bayes In this subsection , we compare the performance of VFDTc using two difierent classiflcation strategies at the leaves : naive Bayes and Majority Class . Our goal is to show that using stronger classiflcation strategies at tree leaves will improve , sometimes drastically , classifler ’s performance .
On these datasets VFDTcNB consistently out performs VFDTcMC ( Figure 4 ) . We should note , with the Waveform data , as the number of examples increases , the performance of VFDTcMC approximates VFDTcNB ( Figure 4 ) . The most relevant aspect in all learning curves is that the performance of VFDTcNB is almost constant , independently of the number of training examples . For example the difierence between the best and worst performance ( over all the experiments ) is :
Waveform ( 21 atts ) 4.28 Waveform ( 40 atts ) 3,97 0.41 Led
C4.5 VFDTcNB VFDTcMC 18.54 13,65 7.97
2.89 5 0.3
These experiments support our claim that the use of appropriate classiflcation schemes will improve the any time property .
With respect to the other dimensions of analysis , the size of the tree does not depend on the classiflcation strategy . With respect to the learning times , the use of naive Bayes classiflers introduces an overhead . The overhead is due to two factors . The flrst factor only applies when there are numeric attributes and is related to the construction of the contingency tables from the Btrees . The size of these tables is usually short ( in our case 10 £ #Classes ) and independent of the number of examples . In our experiments it
Error Rate ( % )
Learning Times ( seconds )
Tree Size
Training+Classiflcation
Training
Nr . Exs C4.5 VFDTcNB VFDTcMC C4.5 VFDTcNB VFDTcMC VFDTc C4.5 VFDTcNB Waveform dataset 21 Attributes
100k 19.98 500k 18.96 1000k 18.17
17.85 17.28 17.15
Waveform dataset 40 Attributes
100k 21.02 500k 19.63 1000k 19.12
18.50 17.17 17.03
LED dataset 24 Attributes
100k 26.53 500k 26.39 1000k 26.35
25.87 25.95 26.05
229.2 25.46 21.15 3975.1 20.16 14960.8
347.8 24.82 21.29 5570.8 19.82 22402.7
74.67 73.97 73.98
15.1 80.6 190.5
31.2 116.5 210.1
64.2 227.4 431.3
18.8 31.7 59.9
23.4 106.9 201.5
46.2 155.3 408.2
6.3 19.9 45.3
18.3
7901 101.5 37787 195.3 61323
36.2
9935 195.1 43725 397.4 80933
2.2
7923 14.1 35413 36.8 65119
51 249 495
51 243 491
39 207 403
Table 1 : Learning curves for Waveform 21 , Waveform 40 , and LED datasets . In all the experiments the test set have 250k examples .
Waveform − 21 Atts
Error Rate
LED Dataset
( Learning Times )
)
%
( e t a R r o r r
E
5 3
0 3
5 2
0 2
C4.5 VFDTcMC VFDTcNB
C4.5 VFDTcNB VFDTcMC VFDTc s d n o c e S n i e m T i
0 0 2
0 5 1
0 0 1
0 5
0
10k
20k
30k
40k
50k
75k
100k
150k
200k
300k
400k
500k
750k
1000k
10k
20k
30k
40k
50k
75k
100k
150k
200k
300k
500k
750k
1000k
Nr.Exs
Nr.Examples
4 :
Figure of VFDTcMC , VFDTcNB and C4.5 error rates on Waveform data ( 21 atts ) .
Learning Curves was the least important factor . The second factor is that the application of naive Bayes requires the estimation , for each test example , of #Classes£#Attributes probabilities . The majority class strategy only requires #Classes probabilities . When the number of test cases is large ( as in our experiments ) this is the most relevant factor . Nevertheless the impact of the overhead shrinks has the training time increases . It is why the overhead is more visible for reduced number of training examples ( Figure 5 ) .
From now on we focus our analysis in VFDTcNB .
C4.5 versus VFDTcNB In this subsection , we compare VFDTcNB against C4.5 [ 10 ] . VFDTc as VFDT was designed for fast induction of interpretable , and accurate models from large data streams using one scan of the data . The motivation for these experiments is the comparison of the relative performances of an online learning algorithm with a standard batch learner . We would expect , given enough examples , a faster convergence rate of VFDTcNB in comparison to VFDTcMC . The following table shows the mean of the ratios of the error rate ( VFDTcNB / C4.5 ) and the p ¡ value of the Wilcoxon test for the three datasets under study .
Figure 5 : Learning times ( train+test ) of C4.5 , VFDTcNB , and VFDTcMC as a function of the number of training examples .
Waveform ( 21 atts ) Waveform ( 40 atts ) LED
VFDTcNB / C4.5 p ¡ value 0.0011 0.0001 0.0017
0.89 0.90 0.98
On the LED dataset the performance of both systems are quite similar unrespectable to the dimension of the training set . On both Waveform VFDTcNB out performs C45
Tree size and Learning Times In this work , we measure the size of tree models as the number of decision nodes plus the number of leaves . We should note that VFDTcNB and VFDTcMC generates exactly the same tree model . In all the experiments we have done , VFDTc generates decision models that are , at least , one order of magnitude smaller than those generated by C45 The size of C4.5 trees grows much more with the number of examples , just as it would expect .
In another dimension , we measured the time needed by each algorithm to generate a decision model . The analysis we have done in a previous section , comparing VFDTcNB versus VFDTcMC applies in the comparison VFDTcNB versus C45 VFDTcNB is very fast in the training phase . It scans the entire training set once , and the time needed to process each example is negligible . In the application phase there is an overhead due to the use of naive Bayes at leaves . In Figure 5 we plot the learning plus classiflcation time as a function of the number of training examples . For small datasets ( less than 100k examples ) the overhead introduced in the application phase is the most important factor .
Bias Variance Decomposition of the Error An interesting analysis of the classiflcation error is given by the so called Bias Variance decomposition [ 2 ] . Several authors refer that there is a trade ofi between the systematic errors due to the representational language used by an algorithm ( the bias ) and the variance due to the dependence of the model to the training set . We have used the Waveform ( 21 attributes ) dataset . The experimental methodology was as follows : We generate a test set with 50k examples , and 10 independent training sets with 75k . VFDTc and C4.5 learn the training sets and the corresponding models were used to classify the test set . The predictions are used to compute the terms of the bias and variance equations using the deflnition presented in [ 2 ] . The flgures of bias and variance for C4.5 were 15.01 and 4.9 respectively , and for VFDTcNB the bias is 17.2 and variance 24 We observe that while VFDTc exhibits lower variance , C4.5 exhibits lower Bias . With respect to the variance , these results were the expected . Decision nodes in VFDTc should be much more stable than greedy decisions . Moreover , naive Bayes is known to have low variance . With respect to the bias component , these results are somewhat surprising . They indicate that sub optimal decisions could contribute to a bias reduction .
Other Results on Real data We have done some experiments on real data . We have used the Forest CoverType dataset from the UCI KDD archive . The goal is to predict the forest cover type from cartographic variables . The problem is deflned by 54 variables of difierent types : continuous and categorical . The dataset contains 581012 examples . Published results for this dataset , using the flrst 15120 examples for training and the reminder 565892 for test are : 70 % accuracy for backpropagation , 58 % for a linear discriminant , and 68.6 % for C45 Using the same training and test sets , the accuracy of VFDTcNB 4 is 62.4 % and 34.2 % for VFDTcMC .
5 . CONCLUSIONS
In this paper , we propose two major extensions to VFDT , one of the most promising algorithms for tree induction from data streams . The flrst one is the ability to deal with numerical attributes . The second one is the ability to apply naive Bayes classiflers in tree leaves . While the former extends the domain of applicability of the algorithm to heterogeneous data , the latter reinforces the any time characteristic , an important property for any learning algorithm for data streams . We should note that the extensions we propose are integrated . In the training phase only the su–cient statistics required to compute the information gain are stored at each leaf . In the application phase , and for nominal attributes , the su–cient statistics constitute ( directly ) the naive Bayes tables . For continuous attributes , naive Bayes tables are e–ciently derived from the Btree used to
4Results obtained with – = 5 £ 10¡6 , ¿ = 5 £ 10¡3 and nmin = 200 . store the numeric attribute values . Nevertheless the application of naive Bayes introduces an overhead with respect to the use of the majority class because the former requires the estimation of much more probabilities than the latter . VFDTc maintains all the desirable properties of VFDT . It is an incremental algorithm , new examples can be incorporated as they arrive , it works online , only see one example once , and using a small processing time per example . The experimental evaluation of VFDTc clear illustrates the advantages of using more powerful classiflcation techniques . In the datasets under study VFDTcNB is a very competitive algorithm even in comparison against the state of the art in batch decision tree induction . The bias variance analysis shows that VFDTcNB generates very stable predictive models concerning variations of the training set .
In this paper , we do not discuss the problem of timechanging concepts [ 7 ] . Nevertheless , our extensions could be applied to any strategy that takes into account concept drift .
Acknowledgments : The authors reveal its gratitude to the flnancial support given by the FEDER ( Plurianual support attributed to LIACC ) , and to project ALES .
6 . REFERENCES [ 1 ] C . Blake , E . Keogh , and C . Merz . UCI repository of
Machine Learning databases , 1999 .
[ 2 ] P . Domingos . A unifled bias variance decomposition and its applications . In P . Langley , editor , Machine Learning , Proc . of the 17th International Conference . Morgan Kaufmann , 2000 .
[ 3 ] P . Domingos and G . Hulten . Mining high speed data streams . In Knowledge Discovery and Data Mining , pages 71{80 , 2000 .
[ 4 ] P . Domingos and M . Pazzani . On the optimality of the simple Bayesian classifler under zero one loss . Machine Learning , 29:103{129 , 1997 .
[ 5 ] J . Gama . An analysis of functional trees . In
C . Sammut , editor , Machine Learning , Proc . of the 19th Int . Conference . Morgan Kaufmann , 2002 .
[ 6 ] J . Gratch . Sequential inductive learning . In Proc . of
Thirteenth National Conference on Artiflcial Intelligence , volume 1 , pages 779{786 , 1996 .
[ 7 ] G . Hulten , L . Spencer , and P . Domingos . Mining time changing data streams . In Knowledge Discovery and Data Mining , 2001 .
[ 8 ] D . Kalles and T . Morris . E–cient incremental induction of decision trees . Machine Learning , 24(3):231{242 , 1996 .
[ 9 ] R . Kohavi . Scaling up the accuracy of naive Bayes classiflers : a decision tree hybrid . In Proc . of the 2nd International Conference on Knowledge Discovery and Data Mining . AAAI Press , 1996 .
[ 10 ] R . Quinlan . C4.5 : Programs for Machine Learning .
Morgan Kaufmann Publishers , Inc . , 1993 .
[ 11 ] P . Utgofi . Perceptron trees a case study in hybrid concept representation . In Proc . of the Seventh National Conference on Artiflcial Intelligence . Morgan Kaufmann , 1988 .
[ 12 ] P . E . Utgofi , N . C . Berkman , and J . A . Clouse .
Decision tree induction based on e–cient tree restructuring . Machine Learning , 29(1):5{44 , 1997 .
