A Two Way Visualization Method for Clustered Data
[ Extended Abstract ]
∗
Yehuda Koren and David Harel
Dept . of Computer Science and Applied Mathematics The Weizmann Institute of Science , Rehovot , Israel {yehuda,dharel}@wisdomweizmannacil which we now turn .
ABSTRACT We describe a novel approach to the visualization of hierarchical clustering that superimposes the classical dendrogram over a fully synchronized low dimensional embedding , thereby gaining the benefits of both approaches . In a single image one can view all the clusters , examine the relations between them and study many of their properties . The method is based on an algorithm for lowdimensional embedding of clustered data , with the property that separation between all clusters is guaranteed , regardless of their nature . In particular , the algorithm was designed to produce embeddings that strictly adhere to a given hierarchical clustering of the data , so that every two disjoint clusters in the hierarchy are drawn separately .
1 .
INTRODUCTION
In the past decade , a number of technological developments have led to an explosion of raw data that has to be analyzed . Two significant examples are the World Wide Web which has grown enormously , and DNA microarray technology , which produces vast amounts of gene expression data [ 6 ] . All this has created a great interest in methods geared towards finding structure in large collections of experimental data , which sometimes discover unexpected relationships . Methods of this kind are concerned with exploratory data analysis . We are especially interested in two families of tools in this domain : clustering algorithms and data visualization methods . Clustering is a classical problem , applicable to a wide variety of fields . It calls for discovering natural groups in datasets , and identifying abstract structures that might reside there . Prior literature on clustering is extensive ; see eg , [ 10 ] . However , the problem remains largely elusive , and there is still a dire need for a clustering method that is natural and robust , yet very efficient in dealing with large datasets . Clustering methods can be broadly classified into hierarchical and partitional approaches . Partitional clustering algorithms obtain a single partition of the data , which optimizes a certain criterion . Hierarchical algorithms create a sequence of increasingly coarse partitions in which each partition is nested within the next partition in the sequence . Although the methods described in this paper can be applied to the results of partitional clustering ( see Section 5 ) , our main interest here is hierarchical clustering , to ∗ For a full version of this paper , see [ 11 ] .
The clustering hierarchy is often visualized as a dendrogram — a full binary tree in which each subtree is a cluster and the leaves are individual elements ; see Fig 1(a ) . Despite being very useful in visualizing the clustering decomposition , the dendrogram has a significant disadvantage : it does not provide exploratory visual representations of the data itself . In general , we would like to visualize the overall structure of the data , in a way that conveys relative positions of the points and the clusters , shows symmetries , emphasizes outliers , and provides hints regarding the shapes of the clusters . Another issue is that of cluster validity . Given n elements to be clustered , a dendrogram will always suggest n−1 different clusters of these elements , regardless of the specific data characteristics . Thus , we will still have to examine many clusters before we find the few that reflect a genuine property of the data .
An indispensable tool in exploratory data analysis are the various approaches to information visualization . We are particularly interested in methods for achieving a low dimensional embedding ( for short , an embedding ) of the data . Such methods convey the overall structure of the data by mapping it into a low dimensional space ( mostly 2 D or 3 D ) that can be quickly assessed by our own visual system ; see Fig 1(b ) . There are many techniques in this category , including principal component analysis ( PCA ) [ 7 ] , multidimensional scaling ( MDS ) [ 7 ] , and force directed placement [ 4 ] . Visualization techniques solve some of the aforementioned limitations of the dendrogram based rendition of clustering . However , in general , they cannot utilize external clustering information . In particular , to the best of our knowledge , no low dimensional embedding algorithm is able to guarantee visual separation between the clusters , so that each cluster will be drawn in a distinct area . Moreover , even when the clustering structure of the high dimensional data is apparent , embedding methods may give priority to displaying other properties of the data , at the expense of the clustering decomposition , which as a result might not be detectable .
For a demonstration of the relative merits of the two approaches — a dendrogram vs . a low dimensional embedding — see Fig 1 , which shows gene expression data related to the Lymphoma/ Leukemia molecular profiling project , taken from http://bioi nfocnioes/dnarray/ Fig 1(a ) shows a dendrogram produced using the Cluster program [ 6 ] . Although one gets to see the hierarchical structure of the data , we are left with many unanswered questions : Which are the significant clusters ? What is the shape of the clusters ? Which clusters and which elements ( in this case , genes ) are similar ? Note that two elements that are placed in proximity in the dendrogram are not necessarily similar , since the dendrogram has an exponential number of different orientations by exchanging the left and the right offspring of internal nodes .
Fig 1(b ) shows the same data using MDS . Here , the similarities between the genes show up clearly , and we get a feel for the structure of the data . However , nothing can be said about the clustering structure of the data . Note that such a structure may exist in the original high dimension version of the data , even though it is not preserved in the low dimensional embedding .
In this paper , we integrate the two approaches : hierarchical clustering depicted as a dendrogram , and low dimensional embedding . We suggest a novel embedding technique whose goal is to preserve the data in the low dimension while also guaranteeing the separation of every two disjoint clusters in the dendrogram . In this way , the data is visualized by two complementary approaches . We show the dendrogram , but choose an ordering of its nodes in such a way that similar elements are located close to each other . In addition , below the dendrogram we show an embedding of the data . The special structure of the embedding makes it possible to align the two views perfectly ( each point in the embedding is drawn exactly below its corresponding leaf in the dendrogram ) , thus enabling the user to mentally link the dendrogram to the embedding and reap the benefits of both approaches . Essentially , we compute an embedding of the points that strictly adheres to the hierarchical structure of the data found in the dendrogram , making it possible to simultaneously examine all n − 1 clusters suggested by the dendrogram , and to pick the interesting ones for further inspection .
Fig 2 shows the results of our approach , applied to the same Lymphoma gene expression of Fig 1 . As the algorithm shows , the data can be viewed as consisting naturally of four clusters ( which we have hand colored appropriately ) . The data analysis effort can now concentrate on checking the validity of these four clusters .
( a )
( b )
Figure 1 : Lymphoma gene expression data . ( a ) A dendrogram representing a hierarchical clustering of the data . ( b ) Low dimensional embedding of the data . expressed either by the distances dij 0 or by the similarities wij 0 . The similarities are a decreasing function of the distances and vice versa . As usual , we assume symmetry , ie dij = dji and wij = wji . Given with one kind of relationship ( similarities or distances ) there are various ways of deducing the other kind . We compute a 2 dimensional embedding of the data , which is formally defined by two vectors , x , y ∈ R n . Thus , the coordinates of element i are ( xi , yi ) . Note that the techniques described here are not restricted to 2 dimensions , and additional dimensions can be computed similarly to the way the y dimension is computed .
Our algorithm is based on the principle of axis separation , ie , the x coordinates and the y coordinates are computed separately . Accordingly , Section 3 describes the computation of the x coordinates , whereas Section 4 is devoted to the y coordinates .
3 . COMPUTING THE X COORDINATES
The embedding must place each element exactly below its corresponding leaf in the dendrogram . This means that the x coordinate of a point is nothing but the x coordinate of the corresponding leaf . Consequently , we face the problem of computing the x coordinates of the dendrogram leaves , in a way that preserves the relationships among the data as much as possible . Of course , being limited by the hierarchy structure of the dendrogram , many coordinates will not be valid . We exhaust all the existing degrees of freedom , opting for a twofold process : ( 1 ) Find the best orientation of the dendrogram . This step determines the ordering of the leaves . ( 2 ) Decide on the exact gaps between consecutive leaves in the ordering . Here we determine the final coordinates of the leaves . We now describe these two stages . 3.1 Dendrogram orientation
A dendrogram has 2n−1 different orientations , which may be significantly different in their quality . For example , consider Fig 3 . The points at Fig 3(a ) were hierarchically clustered . Two isomorphic dendrograms describing the clustering are shown in Fig 3(b,c ) . Whereas the dendrogram at Fig 3(b ) properly shows the similarities in the data , the dendrogram in Fig 3(c ) is misleading . For example , A and H , two of the most distant points in the data , are placed adjacently . Indeed , many researchers dealing with hierarchical clustering have felt the necessity to optimize the orientation of the dendrogram ; thus various local and greedy rules were suggested ; see , eg , [ 9 , 6 ] . Here we give a rigorous formulation for this problem coupled with a satisfactory optimization algorithm .
B
A
D
C
F
E
H
G
A
B
C
D
E
F
G
H
( a )
( b )
Figure 2 : Lymphoma gene expression data . The dendrogram was reordered by the algorithm so that similar elements are placed close together . Below the dendrogram we show a low dimensional embedding that separates different clusters of the dendrogram . Each point is placed exactly below its corresponding leaf in the dendrogram . For emphasis , we have hand colored the four salient clusters revealed by the embedding .
2 . BASIC NOTIONS Throughout the paper , assume we are given data about n elements {1 , . . . , n} . The relationships between pairs of elements are
D
C
B
A
H
G
F
E
A
B
C D
E F G H
( c )
( d )
Figure 3 : ( a ) A hierarchically clustered dataset . ( b ) A properly oriented dendrogram . ( c ) A badly oriented dendrogram . ( d ) Adjusting distances in the dendrogram of ( b ) to better reflect the data .
One way of defining formally what should be considered a ‘good’ ordering of the leaves is to associate a cost function with the den drogram , such that finding the best ordering is equivalent to optimizing this function . An appropriate choice would be the classical minimum linear arrangement problem [ 5 , 2 ] , which is based on the following cost function :
Denote the left and right children of an internal node v by v.left and v.right Another term that will be used in the algorithm is the inner cut of a dendrogram node v , defined as
LAsim(x ) def= wij · |xi − xj| .
( 1 )
InnerCut(v ) = wij . i∈Leaves(v.left ) , j∈Leaves(v.right ) i,j
In this problem , which is known to be NP hard , we want to find a vector x , which is a permutation of {1 , . . . , n} and which minimizes LAsim(x ) . The intuition behind this problem is that we want to order the elements so as to minimize the sum of weighted distances between them ; thus , we prefer putting similar elements ( having high similarity values ) close together .
In our particular problem , we are also faced with an ordering task , so the assumption that the x coordinates are a permutation of {1 , . . . , n} is without loss of generality . However , here we should not consider all possible permutations , but only those that agree with the dendrogram ’s structure , thus vastly reducing the size of the search space , from n! to “ only ” 2n−1 . This helps render the ordering problem tractable . We shall be describing an optimal algorithm of dynamic programming , whose running time is exponential in the dendrogram ’s height , but not in its size . Our algorithm is based on a linear arrangement method described in [ 2 ] .
Before describing the algorithm , we would like to introduce an additional form of the cost function , which is useful when we are given pairwise distances ( instead of similarities ) : dij · |xi − xj| .
LAdist(x ) def=
( 2 ) i,j
In this formulation , we want to find a permutation x that maximizes LAdist(x ) . In this section we address the problem of optimizing LAsim(x ) ; minor adjustments are needed for dealing with LAdist(x ) .
We begin by introducing some basic notions . We are given an ordered dendrogram , T , and a node v . Denote by Leaves(v ) the set of leaves in the subtree rooted by v . Let x be the ordering ( permutation ) on the leaves that the ordered dendrogram induces . Let S be Leaves(v ) , L be the set of leaves that are to the left of S , and R the set of leaves that are to the right of S . Thus , if |L| = l,|S| = s , we have x(L ) = {1 , . . . , l} , x(S ) = {l + 1 , . . . , l + s} , x(R ) = {l + s + 1 , . . . , n}.1
A key concept of the algorithm is the local arrangement cost , which measures the arrangement cost projected on a consecutive set of elements , and is defined as :
LocalLA
T
( v ) def= i,j∈S
+ i∈S,j∈R wij · |xi − xj| + wij · ( l + s − xi ) + i∈S,j∈L i∈L,j∈R wij · ( xi − l)+ wij · s . wij . i∈S,j∈R
Two additional related terms that we will use are :
LeftCut
T
( v ) def=
T wij , RightCut
( v ) def= i∈S,j∈L
When the context is clear , we will omit the T superscript , and use : LocalLA(v ) , LeftCut(v ) , RightCut(v ) .
The dynamic programming approach is based on the observation that the internal ordering of a set S that minimizes LocalLAT ( v ) is independent of the internal ordering of L and R . Thus , in order to compute LocalLAT ( v ) we only need the information regarding the orientation of the nodes in the path from the root to v , since this information completely determines the contents of L and R . Consequently , we can independently optimize disjoint subtrees . 1For a set S , x(S ) def= {xi | i ∈ S} .
All the inner cuts are computed in a preprocessing stage that takes O(n2 ) time .
The algorithm , which is given in Fig 4 , recursively reorders all subtrees of the dendrogram to achieve the best local cost . This way we get the optimal local cost of the root , which solves our original problem . The algorithm uses an auxiliary function called ComputeDirectly , which gets a leaf node v , and computes the magnitudes LocalLA(v ) , LeftCut(v ) and RightCut(v ) . For such a leaf , we take LocalLA(v ) def= LeftCut(v ) . ( Alternatively , we could take LocalLA(v ) def= RightCut(v) . ) A naive implementation of this function would run in time O(n ) , by going through all other leaves and computing the magnitudes according to the formulas . However , we have found a better way . Using O(n2 ) time for pre processing and a simple data structure , it is possible to compute the function ComputeDirectly in constant time .
.
Complexity : The orientation algorithm tries all possible orderings along all paths from the root to the leaves . Thus , its time comleaf v 2depth(v) ) . If the dendrogram is balanced , all plexity is O( the n leaves will be at depth log n , resulting in a time complexity of O(n2 ) , which is also the time needed both for computing all the pairwise distances and for constructing the dendrogram . However , for a less balanced tree , the time complexity is worse .
In practice , the algorithm runs in “ interactive ” time for dendrograms whose maximal depth is around 15 . In order to handle deeper dendrograms , we run the algorithm iteratively , where in each iteration we allow only the nodes at certain depths to change their ordering , while fixing the ordering of all other nodes . More precisely , let D be the maximal depth of the dendrogram and let us choose a fixed number r ( a typical value would be r = 5 ) . In one so called “ sweep ” , we employ D − r iterations of the ordering algorithm , where in iteration i we allow only the nodes at depths i , . . . , i + r to change their ordering . We may repeat this for a few more sweeps . In practice , we observed convergence of the process after about 3 sweeps , after which no additional changing of ordering was performed . Taking r as a constant , the running time of a single sweep is O(n ) , so we have a linear time algorithm , which is significantly faster in practice than the process of constructing the dendrogram in the first place . Our implementation uses a few more optimizations to improve running time . 3.2 Determining coordinates of the leaves
After we have determined the ordering of the leaves , we are left with the task of computing the exact gaps between each two consecutive leaves , which determines the final x coordinates ( up to scaling and translation ) ; see Fig 3(d ) . In order to simplify notation , we will assume in this subsection , without loss of generality , that the ordering of the leaves is just 1 , . . . , n .
A plausible approach is to take the gap between leaves i and i+1 , denoted by gapi , to be the distance between them , ie , di,i+1 . A better approach is to take a weighted average over all influenced leaf pairs , yielding :  
 
−1
· gapi = dkj k − j
.
1 k − j j i , k>i j i , k>i
The reasoning behind the weighting is that there are k − j consecutive gaps that separate a leaf pair j , k ( ie , gapj , . . . , gapk−1 ) . 1 Therefore , the contribution of a single gap is taken to be k−j .
Function Orient ( Dend , v , LCut , RCut ) % Input : An ordered dendrogram ( Dend ) and an internal node ( v ) % ( the initial call should use the root of the dendrogram as v ) . % The function reorders the subtree of v % so as to optimize the respected local cost . % Return value : LocalLA(v ) % Also computes Lef tCut(v ) ( LCut ) and RightCut(v ) ( RCut ) lar to those governing the calculation of the y coordinates . Hence , a careless computation might yield y coordinates similar to the x coordinates . Thus , the intrinsic dimensionality of the drawing would approach 1 , meaning that the y axis was wasted . In [ 11 ] we describe how four different embedding algorithms can be smoothly extended in order to draw the y axis while considering the presence of the x coordinates ; see also [ 12 , 13 ] .
5 . RESULTS
In this section we show the results of applying our algorithm to three datasets . 5.1 Odors dataset
The first example is an odor dataset taken from [ 3 ] . It consists of 30 volatile odorous pure chemicals . Each chemical was measured by an electronic nose in batches , with a single batch containing at least six successive measurements . Different batches of the same chemical were usually taken at totally different dates . In total , the dataset contains 262 elements that correspond to 262 measurements .
A natural clustering of this dataset is to decompose it into 30 groups . Each group contains all the measurements performed on one of the chemicals . We want to compute an embedding of this dataset that separates the 30 clusters , so we can learn about the shape of each cluster and the relations among the clusters .
Embedding of a partitional , hierarchy less , clustering : Our algorithm , as described so far , is appropriate only for hierarchically clustered data . Our way of using the algorithm on non hierarchical partitions is to impose a hierarchy on the data , in a way that obeys the given partition . We can apply any hierarchical clustering algorithm , but should require that each of the given clusters forms a separate subtree in the resulting dendrogram , so the dendrogram respects the given partition . Having such a dendrogram at hand makes it possible to apply our algorithm . Here , we have used a UPGMA agglomerative clustering to construct the dendrogram .
The result is given in Fig 5 , where each of the given 30 clusters gets a different ( hand prepared ) color . Note that the embedding algorithm worked with a dendrogram containing 261 clusters , and was not informed that 30 of these clusters are the “ natural ” ones . Nonetheless , we can see that many of these 30 clusters were clearly distinguished in the embedding , being well separated from the other clusters . This is an encouraging indication of the ability of our algorithm to emphasize the valid clusters in the dendrogram . if IsLeaf(v ) end if return ComputeDirectly(Dend , v , LCut , RCut )
% v is not a leaf . % Deduce LocalLA(v ) , recursively , using the current orientation of v LocalLA(v.left ) ← Orient(Dend , v.left , LCut1 , RCut1 ) LocalLA(v.right ) ← Orient(Dend , v.right , LCut2 , RCut2 ) Cut1 ← RCut1 − InnerCut(v ) Cut2 ← LCut2 − InnerCut(v ) CandidateCost1 ← LocalLA(v.left ) + LocalLA(v.right)+ Backup ← RecordOrientation(Dend , v )
+Cut1 · |Leaves(v.right)| + Cut2 · |Leaves(v.left)|
% Try a different orientation , by exchanging left and right subtrees v.left ↔ v.right LocalLA(v.left ) ← Orient(Dend , v.left , LCut1 , RCut1 ) LocalLA(v.right ) ← Orient(Dend , v.right , LCut2 , RCut2 ) Cut1 ← RCut1 − InnerCut(v ) Cut2 ← LCut2 − InnerCut(v ) CandidateCost2 ← LocalLA(v.left ) + LocalLA(v.right)+
+Cut1 · |Leaves(v.right)| + Cut2 · |Leaves(v.left)|
% Choose the orientation of v that minimizes LocalLA(v ) if CandidateCost1 < CandidateCost2 then RestoreOrientation(Dend , v , Backup ) LocalLA ← CandidateCost1 else LocalLA ← CandidateCost2 end if LCut ← LCut1 + Cut2 RCut ← RCut2 + Cut1 return LocalLA end
Figure 4 : An optimal dendrogram orientation algorithm
.
. j i , k>i j i , k>i
We compute all the gaps using a simple algorithm that maintains 1 the two components of gapi , namely , ai = k−j and dkj k−j . The computation of ai+1 and bi+1 can be bi = done in O(n ) time , based on the values of ai and bi . Thus , the time complexity for computing all the gaps is O(n2 ) . If we are using pairwise similarities , they should be converted into distances . Alternative , more fundamental approaches , which define an energy model whose minimization determines optimal coordinates , are described in the full version of the paper [ 11 ] .
4 . COMPUTING THE Y COORDINATES
After completing the computation of the x coordinates , we would like to convey additional information about the data using the yaxis . Clustering information should not be considered any longer , since it is assumed to be exhaustively taken care of by the x axis . However , we cannot simply use some classical low dimensional embedding algorithm because one dimension of the drawing has already been determined ; so what we seek here is an embedding algorithm that can take into account the presence of the x coordinates . Note that ignoring the x coordinates can be very problematic . The x coordinates were calculated in order to satisfy criteria simi
Figure 5 : Odors dataset containing 262 measurements by electronic nose over 30 different chemicals , suggesting a partitioning of the data into 30 clusters . Results are color coded according to the partitioning . In fact , we observed that the embedding conveys many properties of the odors . Here are some examples . In the upper left part of the embedding there are 3 clusters ( chemicals ) that are clearly separated from the rest of data . An inspection of these three reveals that they are true outliers , with some of their sensors showing an anomalous double peak time response . Another exceptional cluster is the blue one at the central bottom part of the embedding , which is represented by a large , heterogeneous cloud of points . This cluster is related to a specific chemical that was used as a reference , and hence was measured simultaneously with each of the other chemicals . It is known that the kind of chemical sensors used in this experiment suffer from time drifts . Consequently , identical samples measured with a significant time lag ( weeks ) will produce slightly different signals , which explains the heterogeneity of the blue cluster . The same time drift phenomenon explains the small breaks in some of the other clusters . For example , consider the top rightmost cluster . It is delicately broken into two parts , representing two batches of measurements that were taken at two different dates .
In contrast , some of the chemicals are not well separated . A partial explanation is that some of them are isomers , ie , compounds having the same molecular formula but different structures . 5.2 Iris dataset
The time honored Iris dataset was introduced by Fisher [ 8 ] as an example of discriminant analysis . The dataset reports four characteristics ( sepal width , sepal length , pedal width and pedal length ) of three species of the Iris flower : Setosa , Versicolor and Verginica . For each species there are 50 instances , so the dataset contains 150 elements naturally partitioned into 3 clusters . Hence , unlike the odors dataset that contained many small clusters , here we have a small number ( three , actually ) of large clusters . As we will show , our results not only facilitate a direct comparison between the three clusters , but also convey some true characteristics of the data .
Again , we constructed a dendrogram that respects the partition of the data , using UPGMA agglomerative clustering . Fig 6 shows results of running our algorithm on this data . We want to emphasize that the algorithm did not take into account any special importance of the three natural clusters , so all 149 clusters in the dendrogram were of equal importance . The Setosa samples ( left cluster ; brown color coded ) are known to be very different from the Versicolor and Verginica samples . However , almost all classification algorithms cannot completely discriminate between the Versicolor and Verginica samples . This situation is reflected in our result .
We can discern several interesting characteristics of the data . First note that one of the Verginica ( right cluster ; purple colorcoded ) samples is drawn much higher than the other samples and seems to be an outlier . This is sample no . 107 . We checked it and found both its sepal and petal lengths to be significantly shorter than in all other Verginica samples . The sepal length is 4.9 , whereas the average Verginica sepal length is 6.6 , with a standard deviation of 063 Another observation , is the saliency of the top left Versicolor ( middle cluster ; green color coded ) sub cluster . This sub cluster contains samples no . 58 , 61 , 94 and 99 . Their Sepal lengths are 4.9 , 5 , 5 , and 5.1 , respectively , significantly smaller than all other Versicolor samples . The average Versicolor Sepal length is 5.9 , with a standard deviation of 05 Another apparently interesting subcluster consists of four Setosa samples , and is placed much lower than all other Setosa samples . The members of this sub cluster are samples no . 15 , 16 , 33 , 34 . Their Sepal widths ( 4 , 4.4 , 4.1 , 4.2 ) are significantly larger than in all other Setosa samples . The average Sepal width for Setosa is 3.4 with a standard deviation of 038 Many more interesting insights can be learned from the Iris embeddings . The few insights we give here were intended to show the ability of our embeddings to convey the structure of the data . 5.3 Gene expression data : CDC15 synchro nized cell cycle
In this subsection we provide the results of our algorithm for a much larger dataset of gene expression data , containing 6113 elements . The dataset contains genome wide mRNA levels for 6113 genes , over about two and a half cell cycle periods , in a yeast culture synchronized by CDC15 , relative to a reference mRNA from
Figure 6 : The Iris dataset containing 150 samples over 3 equally sized classes , corresponding to 3 iris species : Setosa ( left cluster ; brown color coded ) , Versicolor ( middle cluster ; green color coded ) , and Verginica ( right cluster ; purple color coded ) an asynchronous yeast culture ; for more details see [ 1 ] . The dataset was taken from genome wwwstanfordedu/SVD
This dataset was heavily processed in [ 1 ] , filtering out the uninteresting information using singular value decomposition . It was shown there that before the processing , all the genes have about the same steady state expression . After the filtering , however , the data express oscillations of about two and a half periods during the cell cycle .
This raises a two way challenge for our algorithm . On the one hand , we did not want it to show too much clustering structure when applied to the raw data , which represent many genes in about the same steady state . On the other hand , when applied to the processed data , we expect to see interesting patterns .
Running our algorithm on this dataset took a few seconds . Fig 7(a ) shows the results for the raw data . Clearly the dataset is in some kind of a steady state , having only very few deviations on the boundaries . The situation is completely different with the filtered data , shown in Fig 7(b ) . Here , many of the dendrogram clusters seem anomalous and a sort of oscillatory behavior is observed . A closer study of the properties of these seemingly interesting clusters is beyond the scope of this paper , but given the results of the other datasets we have analyzed , it appears to be worthwhile . 6 . RELATED WORK
In recent years , hierarchical clustering has been used extensively for biological data ; see , eg , [ 6 , 14 ] . Consequently , many software packages for hierarchical clustering have been developed , offering several ways to enhance the bare dendrogram . A very popular way is to superimpose the dendrogram over a color coded matrix that displays the high dimensional coordinates of the data , or the pairwise similarities . An example , produced by TreeView [ 6 ] , is given in Fig 8(a ) , which shows Yeast gene expression data . Each of the genes was measured under seven conditions , and each such 7 D pattern is displayed adjacently to each of the leaves , using seven colored rectangles . Here , red means a relatively increased expression , while green means a relatively decreased expression . For a comparison we provide the results of our algorithm in Fig 8(b ) . The main difference between the methods is that ours reduces the dimensionality of the data , whereas the TreeView approach shows the full high dimensional coordinates of each element .
Embedding the data in a low dimension significantly reduces the burden on our visual system and facilitates direct inspection of the relations between the clusters . The reduction in dimensionality provided by our method also greatly improves the utilization of screen space . Since each element is represented by a single point only , our results can fit a display of over 10,000 elements on a standard computer screen . On the other hand , drawing a high dimensional color pattern below each leaf , wastes a lot of screen space , and far fewer elements can be effectively shown simultaneously . Nevertheless , we pay a price for the simplified display : A low dimensional embedding must make compromises and thus lose some of the high dimensional information . This is an advantage of the other
( a )
( b )
Figure 7 : CDC15 synchronized gene expression data ( 6113 elements ) . ( a ) Results for raw data , showing that the data is at a steady state . ( b ) Results for processed data , showing its rich structure . Salient clusters are color coded . approach that uses a virtually lossless representation of the data . Hence , we feel that ways should be found to use both approaches together .
( a )
( b )
Figure 8 : Yeast gene expression data ( 99 elements ) . ( a ) Results by TreeView , showing a 7 D color pattern to the right of each leaf . ( b ) Results of our algorithm for the same clustering ; salient clusters are color coded .
Another kind of related work has been carried out on low dimensional embedding of data in a way that emphasizes its clustering decomposition . One classical approach is Fishers ’s Linear Discriminant Analysis ( LDA ) and its generalizations ; see eg [ 7 ] . The objective of this method is to find the best linear combinations of the data coordinates so as to maximize the inter class scatter and minimize the intra class scatter . Being a linear transformation , LDA cannot guarantee separation between the clusters . Unlike our method , LDA can also utilize the y coordinates for separating clusters , which may be advantageous in some cases . However , our use of one axis only — the x axis — for separating clusters is necessary if we want to carry out the significant alignment with the dendrogram . 7 . DISCUSSION
We have integrated two key methods in exploratory data analysis : cluster analysis and low dimensional embedding of the data . We introduced an embedding algorithm that can preserve the structure of a predefined partitional or hierarchical clustering decomposition . The method was constructed so as to facilitate a direct linking of the embedding with a related dendrogram , enabling the user to examine all the clusters simultaneously . Each part of the algorithm was carefully designed to maximize the embedding quality . We tried to utilize all available degrees of freedom , in order to preserve the nature of the data without violating the clustering structure .
In general , our method has two unique properties , which , to the best of our knowledge , make it distinct from other low dimensional embedding methods : ( 1 ) Guaranteed separation between any kind of given clusters . ( 2 ) The ability to deal with a predefined hierarchical clustering .
8 . REFERENCES [ 1 ] O . Alter , P . O . Brown and D . Botstein , “ Processing and
Modeling Genome Wide Expression Data Using Singular Value Decomposition ” , Microarrays : Optical Technologies and Informatics , Proc . of SPIE 4266 , pp . 171 186 , 2001 .
[ 2 ] R . Bar Yehuda , G . Even , J . Feldman and S . Naor , “ Computing an Optimal Orientation of a Balanced Decomposition Tree for Linear Arrangement Problems ” , J . of Graph Algorithms and Applications 5 ( 2001 ) , 1–27 .
[ 3 ] L . Carmel , Y . Koren and D . Harel , “ Visualizing and
Classifying Odors Using a Similarity Matrix ” , Proc . of the 9th International Symposium on Olfaction and Electronic Nose ( ISOEN’02 ) , IEEE , to appear , 2003 .
[ 4 ] G . S . Davidson , B . N . Wylie and K . W . Boyack , “ Cluster
Stability and the Use of Noise in Interpretation of Clustering ” , Proc . IEEE Information Visualization , pp . 23–30 , 2001 .
[ 5 ] J . Diaz , J . Petit and M . Serna , “ A Survey on Graph Layout Problems ” , ACM Computing Surveys 34 ( 2002 ) , 313–356 .
[ 6 ] MB Eisen , PT Spellman , PO Brown and D . Botstein ,
“ Cluster Analysis and Display of Genome Wide Expression Patterns ” , Proc . National Academy of Sciences 95 ( 1998 ) , 14863–14868 .
[ 7 ] B . S . Everitt and G . Dunn , Applied Multivariate Data
Analysis , Arnold , 1991 .
[ 8 ] RA Fisher , “ The Use of Multiple Measurements in
Taxonomic Problems ” , Ann . Eugenics 7 ( 1936 ) , 179–188 .
[ 9 ] N . Gale , WC Halperin and C . M . Costanzo , “ Unclassed
Matrix Shading and Optimal Ordering in Hierarchical Cluster Analysis ” , Journal of Classification 1 ( 1984 ) , 75–92 . [ 10 ] A . K . Jain , MN Murty and PJ Flynn , “ Data Clustering : A
Review ” , ACM Computing Surveys 31 ( 1999 ) , 264–323 .
[ 11 ] Y . Koren and D . Harel , “ A Two Way Visualization Method for Clustered Data ” , Technical Report MCS03 05 , Faculty of Mathematics and Computer Science , The Weizmann Institute of Science , 2003 .
[ 12 ] Y . Koren and D . Harel , “ One Dimensional Graph Drawing :
Part I — Drawing Graphs by Axis Separation ” , to be published .
[ 13 ] Y . Koren , “ One Dimensional Graph Drawing : Part II —
Axis by Axis Stress Minimization ” , to be published . [ 14 ] J . Seo and B . Shneiderman , “ Interactively Exploring
Hierarchical Clustering Results ” , IEEE Computer 35 ( 2002 ) , 80–86 .
