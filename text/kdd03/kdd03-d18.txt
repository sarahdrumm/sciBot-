CARPENTER : Finding Closed Patterns in Long Biological
Datasets
Feng Pan , Gao Cong , Anthony K . H . Tung ∗† Natl . University of Singapore
{panfeng,conggao,atung}
@compnusedusg
University of Illinois , Urbana
Jiong Yang
Champaign jioyang@csuiucedu
Mohammed J . Zaki ‡ Rensselaer Polytechnic
Institute zaki@csrpiedu
Categories and Subject Descriptors H28 [ Database Management ] : Database ApplicationsData Mining
Keywords frequent pattern , closed pattern , row enumeration
ABSTRACT The growth of bioinformatics has resulted in datasets with new characteristics . These datasets typically contain a large number of columns and a small number of rows . For example , many gene expression datasets may contain 10,000100,000 columns but only 100 1000 rows .
Such datasets pose a great challenge for existing ( closed ) frequent pattern discovery algorithms , since they have an exponential dependence on the average row length . In this paper , we describe a new algorithm called CARPENTER that is specially designed to handle datasets having a large number of attributes and relatively small number of rows . Several experiments on real bioinformatics datasets show that CARPENTER is orders of magnitude better than previous closed pattern mining algorithms like CLOSET and CHARM .
1 .
INTRODUCTION
The growth of bioinformatics has resulted in datasets with new characteristics . These datasets typically contain a large number of columns and a small number of rows . For example , many gene expression datasets may contain 10,000100,000 columns or items but usually have only 100 1000 rows or transactions . Such datasets pose a great challenge
∗Contact Author †This work was supported in part by NUS ARF grant R252000 121 112 and R252 000 142 112 . ‡This work was supported in part by NSF CAREER Award IIS 0092978 , DOE Career Award DE FG02 02ER25538 , and NSF grant EIA 0103708 . for existing frequent pattern discovery algorithms . While there are a large number of algorithms that had been developed for frequent pattern mining [ 1 , 5 , 11 ] , their running time increases exponentially with increasing average row length , thus such high dimensional data renders most current algorithms impractical . This also holds for extant methods for mining closed patterns [ 6 , 2 , 7 , 10 ] .
Previous ( closed ) frequent pattern mining methods work well for datasets with small average row length , since if i is the maximum row size , there could be 2i potential frequent itemsets ; usually i < 100 . However , for the gene expresion datasets in bioinformatics domain i can be in the range of tens of thousands . As a result , search over the itemset space is impractical . Since these datasets have a small number of rows ( say m ) usually in the range of a hundred or a thousand ( ie , m ( cid:191 ) i ) , it appears reasonable to design an algorithm that searches the row set space instead of the usual itemset space .
In this paper , we describe a new algorithm called CARPENTER 1 , that is specially designed to handle datasets having a large number of items and relatively small number of rows . CARPENTER is a novel algorithm which discovers frequent closed patterns by performing depth first rowwise enumeration instead of the usual itemset enumeration , combined with efficient search pruning techniques , to yield a highly optimized algorithm . Our experiments show that this unconventional approach produces good results when mining long biological datasets and outperforms current methods like CHARM[10 ] and CLOSET[7 ] by more than an order of magnitude .
2 . PRELIMINARIES
Let F = {f1 , f2 , , fm} be a set of items , also called features . Our dataset D consists of a set of rows R = {r1 , , rn} , where each row ri is a set of features , ie , ri ⊆ F . Figure 1(a ) shows an example dataset in which the features are represented using alphabets a through t . There are altogether 5 rows , r1,,r5 The first row r1 contains the feature set {a , b , c , l , o , s} . For convenience , in the sequel , we drop set notation and denote a set of features {a , c , f } as acf , and we denote a row set {r2 , r3 , r5} as 235 .
Given a set of features F ⊆ F , we define the feature support set , denoted R(F ) ⊂ R , as the maximal set of rows that contain F . Likewise , given a set of rows R ⊂ R , we define the row support set , denoted F(R ) ⊂ F , as the maximal set of features common to all the rows in R .
1CARPENTER stands for Closed Pattern Discovery by Transposing Tables that are Extremely Long ; the “ ar ” in the name is gratuitous . ri a,b,c,l,o,s a,d,e,h,p,l,r a,c,e,h,o,q,t a,e,f,h,p,r b,d,f,g,l,q,s,t i 1 2 3 4 5 ( a ) Example Table fj R(fj ) 1,2,3,4 a 1,5 b 1,3 c 2,5 d 2,3,4 e 4,5 f 5 g 2,3,4 h 1,2,5 l 1,3 o 2,4 p 3,5 q 2,4 r 1,5 s 3,5 t ( b ) Transposed Table , T T
Figure 1 : Running Example fj R(fj ) a e h
4 4 4
Figure 2 : T T |{2,3}
As an exmaple consider Figure 1(a ) . Let F = aeh , then R(F ) = 234 since these are all the rows that contain F . Also let R = 23 , then F(R ) = aeh since it is the maximal set of features common to both r2 and r3 .
Given a set of features F , the number of rows in the dataset that contain F is called the support of F . By definition , the support of F is given as |R(F )| . A set of features F ⊆ F is called a closed pattern if there exists no F such that F ⊂ F and |R(F )| = |R(F )| , ie , there is no superset of F with the same support . Put another way , the row set that contains superset F must not be exactly the same as the row set of F . A feature set F is called a frequent closed pattern , if it is i ) closed , ii ) |R(F )| ≥ minsup , where minsup is a user specified lower support threshold . For example , given minsup = 2 , the feature set aeh is a frequent closed pattern in Figure 1(a ) since it occurs three times . ae , on the other hand , is not a frequent closed pattern , since it is not closed ( |R(aeh)| = |R(ae)| ) , although its support is more than minsup .
Given a dataset D which contains records that are subset of a set of features F , our problem is to discover all frequent closed patterns with respect to a user support threshold minsup . In addition we assume that the dataset satisfies the condition |R| ( cid:191 ) |F | .
3 . THE CARPENTER ALGORITHM
To illustrate CARPENTER , we will use the tables in Figure 1 as a running example . Table 1(b ) is a transposed version of Table 1(a ) , denoted T T . In T T , each tuple lists a feature , along with the row ids where that feature occurs in the original table . As an example 15 is the set of rows that contain feature b , which produces the second tuple in T T . In the sequel we always refer to entries of the transposed table as tuples , and entries of the original table as rows .
Unlike existing algorithms which perform their search by enumeration of feature sets [ 6 , 7 ] , CARPENTER performs search by enumeration of row sets . Figure 3 illustrates the
12345
{}
1234 {a}
1235 {}
1245 {}
1345 {}
2345 {}
123 {a} 124 {a}
125 {l} 134 {a}
135 {} 145 {}
234 {aeh}
235 {} 245 {}
345 {}
12 {al}
13 {aco}
14 {a}
15 {bls} 23 {aeh}
24
{aehpr}
25 {dl}
34 {aeh}
35 {q}
45 {f}
1
{abclos}
{}
2
{adehplr}
3
{acehoqt}
4
{aefhpr}
5
{bdfglqst}
Figure 3 : The Row Enumeration Tree . complete row set enumeration tree without application of any pruning strategies . Each node in the tree represents a row set R ; also shown is F(R ) . For example , the node 12 represents the row set {r1 , r2} , along with it supporting feature set F(12 ) = al .
To find frequent closed patterns , CARPENTER performs a depth first search ( DFS ) of the row set enumeration tree . By imposing a total order , such as lexicographic order on the row sets , we are able to perform a systematic search for closed patterns . For example , DFS on the row enumeration tree in Figure 3 will be {1 , 12 , 123 , 1234 , 12345 , 1235,,45 , 5} ( in absence of any optimization and pruning strategies ) .
Lemma 31 Let F be a closed pattern and R(F ) be the set of rows that contain F , then R(F ) is unique . In other words , there does not exist a closed pattern F , F = F , that satisfies R(F ) = R(F ) . Proof : We prove by contradiction . Assume there exists a closed pattern F that satisfies R(F ) = R(F ) but F = F . Let pattern CF = F ∪ F . Then R(CF ) = R(F ) = R(F ) . Since F ⊂ CF , this contradicts the fact that F is closed . 2
By Lemma 3.1 , each closed pattern corresponds to a unique set of rows . By enumerating all combinations of rows as shown in Figure 3 , we ensure that all closed patterns in the datasets are enumerated . It is obvious that a complete traversal of the row enumeration tree is not efficient and pruning techniques must be introduced to prune off unnecessary searches .
Let X be a subset of rows . Given the transposed table T T , a X conditional transposed table , denoted as T T |X , is a subset of tuples from T T such that : 1 ) For each tuple x in T T , there exist a corresponding tuple x in T T |X . 2 ) x contains all rows in x with row ids larger than any row in X . As an example , let the transposed table in Figure 1(b ) be T T and let X = 23 . The X conditional transposed table , T T |X is as shown in Figure 2 .
Our formal algorithm is shown in Figure 4 . For clarity , we assume that the database is already transposed with infrequent features removed . This pre processing step is trivial
Algorithm CARPENTER Input : Transposed table T T , features set F and support level minsup Output : Complete set of frequent closed patterns , F CP Method :
1 . Initialization . F CP = ∅ . Let R be the ( numerically sorted ) set of rows in the original table ;
2 . Mine Frequent Closed Pattern . MinePattern(T T |∅,R ,
F CP ) ;
Subroutine : MinePattern(T T |X ,R , F CP ) . Parameters :
• T T |X : A X conditional transposed table ; • R : A subset of rows which have not been considered in the enumeration ;
• F CP : The set of frequent closed patterns that have been found ;
Method :
1 . Scan T T |X and count the frequency of occurrences for each row , ri ∈ R . Y = ∅ .
2 . Pruning 1 : Let U ⊂ R be the set of rows in R which occur in at least one tuple of T T |X . If |U |+|X| ≤ minsup , then return ; else R = U ;
3 . Pruning 2 : Let Y be the set of rows which are found in every tuple of the X conditional transposed table . Let R = R − Y and remove all rows of Y from T T |X ;
4 . Pruning 3 : If F ( X ) ∈ F CP , then return ;
5 . If |X| + |Y | ≥ minsup , add F ( X ) into F CP ; 6 . For each ri ∈ R ,
R = R − {ri} MinePattern(T T |X |ri , R , F CP ) ;
Figure 4 : The CARPENTER Algorithm and take negligible time since our datasets usually fit in main memory . CARPENTER does recursive generation of conditional transposed tables , performing a depth first traversal of the row enumeration tree . Each computed conditional table represents a node in the enumeration tree of Figure 3 . For example , the 23 conditional table represents the node 23 . After initializing F CP , the set of frequent closed pattern , to be empty and letting R to be the set of rows in the original table , CARPENTER calls the subroutine M ineP attern to recursively generate X conditional tables .
The subroutine M ineP attern takes in three parameters T T |X , R and F CP . T T X is a X conditional table . R contains the set of rows that will be used to enumerate the next level of conditional transposed table while F CP contains the frequent closed patterns that have been discovered so far .
Steps 1 to 4 in the subroutine perform the counting and pruning . They are extremely important for efficiency of CARPENTER . Before we explain these 4 steps , we will first show that the M ineP attern subroutine will only output a pattern if and only if it is a frequent closed patterns ( in the absence of these 4 steps ) . This is done at Step 5 which checks whether F(X ) is a frequent closed pattern before inserting F(X ) into F CP , and at Step 6 which continues the next level of enumeration in the search tree . We prove the correctness of the two steps by two lemmas as follows :
Lemma 32 Let X be a subset of rows from the original table , then F(X ) must be a closed pattern ( not necessarily frequent ) . Proof : We will prove by contradiction . Assuming F(X ) is not a closed pattern , then there exists a feature fi such that R(F(X ) ) = R(F(X ) ∪ fi ) . Since X contains all features of F(X ) , then X ⊂ R(F(X) ) . This means that fi belongs to in every row in X , which contradicts the definition that F(X ) is the maximal set of features common to rows of X . 2
Lemma 3.2 ensures that Step 5 only inserts closed patterns that are frequent into F CP . The main observation used in the proof is that F(X ) cannot be a maximal feature set that is common in all rows of X unless it is a closed pattern . A check on |X| + |Y | is needed to ensure that the minimum support threshold is satisfied ( Note that Y is an empty set if the Steps 1 to 4 of M ineP attern are not executed ) . Together with Lemma 3.1 , we know that the complete and correct set of frequent closed patterns will be in F CP .
Lemma 33 T T |X |ri = T T |X∪ri
2
Lemma 3.3 is useful for explaining Step 6 . It simply states that a X ∪ ri conditional transposed table can be computed from a X conditional transposed table , T T |X , by selecting those tuples that contain ri in T T |X . This is utilized in Step 6 where a recursive call on M ineP attern is called with T T |X |ri as the conditional transposed table . This is in fact generating the X + ri conditional transposed table that is needed to represent the next level of row set enumeration .
Note that Step 6 implicitly represents a form of pruning too since it is possible to have R = ∅ . It can be observed from the enumeration tree that there exist some combinations of rows , X , such that F(X ) = ∅ ( an example is node “ 134 ” ) . This implies that there is no feature which exists in all the rows in X . When this occurs , R will be empty and no further enumeration will be performed .
We next look at the pruning techniques that are used in CARPENTER to enhance its efficiency . Our emphasis here is to show that our pruning steps do not prune off any frequent closed patterns , while preventing unnecessary traversal of the enumeration tree . This guarantees correctness .
The first pruning step is executed in Step 2 of M ineP attern .
The pruning is essentially aimed at removing search branches which can never yield closed patterns that satisfy the minsup threshold . The following lemma is applied in the pruning .
Lemma 34 Let T T |X be a X conditional transposed table . Let U be a set of rows which occur in at least one tuple of T T |X . If |U | + |X| < minsup , then it is not possible that for any U ⊂ U , F(X ∪ U ) is a frequent closed pattern . Proof : By definition , any row not in T T |X cannot be combined with X to produce a non empty closed pattern . Thus X can only be combined with some U ⊂ U in order to continue the enumeration . It is clear that the maximum support is bounded by |U |+|X| . If |U |+|X| < minsup , we can safely conclude that all the patterns in further enumeration will not be frequent . 2
In Step 3 of M ineP attern , our second pruning strategy is applied . This pruning deals with rows that occur in all tuples of the X conditional transposed table . Such rows are immediately removed from T T |X because of the following lemma
Lemma 35 Let T T |X be a X conditional transposed table and Y be a set of rows which occur in every tuple of T T |X . Given any subset R ⊂ R , we have F(X ∪ R ) = F(X ∪ Y ∪ R ) . Proof : By definition , F(X ∪ R ) contains a set of features which occur in every row of X∪R . Since the rows in Y occur in every tuple of T T |X , this means that these rows also occur in every tuples of T T |(X∪R ) ( Note : T T |(X∪R ) ⊂ T T |X ) . Thus , the set of tuples in T T |(X∪R ) is exactly the set of tuples in T T |(X∪R∪Y ) . From this , we can conclude that F(X ∪ R ) = F(X ∪ Y ∪ R ) .
2
As an example to illustrate Lemma 3.5 , let us consider the 23 conditional transposed table in Figure 2 . Since row 4 occurs in every tuple of T T |23 , we can conclude that F(23)=F(234 ) =aeh . Thus , we need not create T T |234 in our search and row 4 need not be considered for further enumeration down that branch of the enumeration tree .
Our final and most complex pruning strategy is shown in Step 4 of M ineP attern . This step will prune off any further search down the branch of node X if it is found that F(X ) was already discovered previously in the enumeration tree . The inituitive reasoning which we will prove later is as follows : the set of closed patterns that will be enumerated from the descendants of node X must have been enumerated previously .
Unlike feature based mining algorithms , such as CHARM and CLOSET , we need not perform detection of supersetsubset relationship among the patterns since Lemma 3.2 already shows that only closed patterns will be enumerated in our search tree . For example , in Figure 3 , it is not possible for the pattern {a , c} to be enumerated although both {a} and {a , c , o} are closed patterns with support of 80 % and 40 % respectively . This is unlike CHARM and CLOSET , both of which will enumerate {a , c} and check that it has the same support as a superset {a , c , o} before discarding it as a non closed pattern .
Another important thing to note here is that the correctness of the third pruning strategy ( Step 3 ) is dependent on the second pruning criteria . This is essential because of the following lemma .
Lemma 36 Let X be the set of rows in the current search node and X be the set of rows that result in F(X ) ( which is the same as F(X’ ) ) being inserted into F CP in earlier enumeration . If pruning strategy 2 is applied consistently in the algorithm , then the node representing X in the enumeration tree will not be the descendent of the node representing X in the enumeration tree . Proof : Assume otherwise , then X ⊂ X . Let Z = X − X . Since F(X ) = F(X ) , all rows in Z must be contained in all tuples of the X conditional transposed table . Based on pruning strategy 2 , the rows in Z would be added to X and will be removed from subsequent transposed table down that search branch . Thus the node representing X will not be visited , which contradicts the fact that node X is currently being processed in the enumeration tree . 2
Consider the node 23 in Figure 3 . As mentioned earlier , its descendant node 234 will not be visited since row 4 occurs in every tuple of 23 conditional transposed table . Without pruning strategy 2 , this will not be the case .
We next try to prove that all branches from a node X in the enumeration tree can be pruned off if F(X ) is already in F CP . We have the following lemma .
Lemma 37 Let T T |X be the conditional transposed table in the current search node . Let X be the set of rows which result in F(X ) ( which equals to F(X ) ) being inserted into F CP in earlier enumeration . Let xfi and x fi be the two tuples that represent feature fi in T T |X and T T |X respectively . We will have xfi ⊂ x Proof : We know that F(X ) = F(X ) which implies that the fi for all fi ∈ F(X ) . set of features represented by tuples in both the conditional transposed tables are the same .
Let the maximal set of rows that contains the feature set max = {r1 , , rn} which is sorted in numerical m} be the first row set that causes m} is also
F(X ) be R order . Let X = {r F(X ) to be inserted into F CP and X = {r sorted in numerical order .
1 , , r
1 , , r
Based on Lemma 3.6 , X cannot be a descendent of X in the enumeration tree . Thus , X must be of the form ( X − A ) ∪ B where A ⊂ X and B ⊂ R max − X , A = ∅ , B = ∅ . By lexicographic row set search , we can conclude from here that there exists a row r i such that i > m and r i ∈ X .
By definition of a conditional transposed table , we know m will be removed in T T |X . that all rows which occur before r Likewise , all rows occurring before r i will be removed in T T fi representing feature fi in T T |X will have less rows being removed than the corresponding tuple xfi representing feature fi in T T |X . Hence the proof . 2
X . Since i > m , a tuple x
In a less formal term , Lemma 3.7 shows that if X is the first combination of rows that cause F(X ) to be inserted into F CP , then the conditional transposed table T T |X will be more “ general ” than any other conditional transposed table T T |X in which F(X ) = F(X ) . “ General ” in this case , refers to the fact that each tuple in T T |X is in fact a superset of the corresponding tuple in T T |X . We will now formalize our third pruning strategy as a theorem .
Theorem 31 Given a node representing a set of rows X in the enumeration tree , if F(X ) is already in F CP , then all enumeration down that node can be pruned off . Proof Let X be the combination of rows that first cause F(X ) to be inserted into F CP . From Lemma 3.7 , we know that any tuple x fi in the X conditional table will be a superset of a corresponding tuple xfi in the X conditional table and X conditional table has the same number of tuples with X conditional table .
Since we know that the next level of search at node X in the enumeration tree is based on the set of rows in the X conditional transposed table , it is easy to conclude that the possible enumeration at the node X is a subset of the possible enumeration at node X . Since X had been visited , it is thus not necessary to perform any enumeration from the node X onwards . 2
Consider the node 23 in Figure 3 which is the first node that results in the insertion of aeh based on the enumeration order . Thus , R min = 23 . Next look at node 34 . We have F(34 ) = F(23 ) = aeh . It can be seen that node 34 is not a descendant of node 23 and that 34 satisfies the formula ( R In this case , pruning strategy 3 can be applied and no further enumeration will be done from node 34 . min − A ) ∪ B , A = 2 , B = 4 .
CARPENTER is implemented by adopting the in memory , pointer based approach in BUC [ 3 ] 2 . With the in memory pointer , CARPENTER does not construct conditional transposed table physically , thus saving space . Due to space limitation , we will not give details on the implementation of CARPENTER .
2We note also that there are other alternatives for implementation including building a FP tree [ 4 ] on the transposed table and adopting the vertical data representation in the row wise manner [ 10 ] . However , our central theme of row enumeration is independant of these techniques and we leave it to interested readers to explore them during their own implementation .
4 . PERFORMANCE STUDIES
In this section , we compare the performance of CARPENTER against other algorithms . All experiments were performed on a PC with a Pentium III 1.4 Ghz CPU , 1GB RAM and a 80GB hard disk . Algorithms were coded in Standard C .
Algorithms : We compare CARPENTER against two other closed pattern discovery algorithms , CHARM [ 10 ] and CLOSET [ 7 ] Experiments in [ 7 , 10 ] have shown that depth first mining algorithms like CHARM and CLOSET are substantially better than levelwise mining algorithms like Close[6 ] and Pascal [ 2 ] . To make a fair comparison , CHARM and CLOSET are also run in the main memory after one disk scan is done to load the datasets . The run time for CARPENTER includes the time for transposing the datasets .
Datasets : We choose 3 real life gene/protein expression datasets to analyze the performance of CARPENTER . The Lung Cancer ( LC ) dataset 3 is a gene expression dataset . The rows in the dataset represent sample tissues and these tissues can come from either malignant pleural mesothelioma ( MPM ) or adenocarcinoma ( ADCA ) of the lung . There are 181 tissue samples and each sample is described by the activity level of 12533 genes or features . The Acute Lymphoblastic Leukemia ( ALL ) dataset 4 is also a gene expression dataset containing tissues from cancerous/noncancerous cells . There are 215 tissue samples described by activity level of 12533 genes . The Ovarian Cancer ( OC ) dataset 5 is for identifying proteomic patterns in serum that distinguish ovarian cancer from non cancer cases . There are 253 samples each described by activity level of 15154 proteins . These expression datasets have real valued entries , which have to be discretized to obtain binary features ; we do an equal depth partition for each attribute using 20 buckets . 6
Parameters : Two parameters are varied in our experiment , minimum support ( minsup ) and length ratio ( l ) . We use a default value of minsup = 4 % . The parameter length ratio , l , has a value between 0 and 1 . It is used to generate new datasets with different average row size from the original datasets . A dataset with a length ratio of l retains on average l ∗ 100 % of the columns in the original dataset . Columns to be retained are randomly selected for each row . The default value used is l = 0.6 , unless otherwise stated . 4.1 Varying Minimum Support
Figure 5 shows how CARPENTER compares against CHARM and CLOSET as minsup is varied with l = 06 Note that the y axis is in logarithmic scale . There is a large variation in the running time for both CHARM and CLOSET even though the variation in absolute minsup is small . This is because the average length of each row after removing the infrequent features can increase ( decrease ) substantially due to a small decrease ( increase ) in minsup value . This increases ( decreases ) the search space of both CHARM and CLOSET substantially , resulting in a large difference in running time .
Among the three algorithms , we find that CLOSET is the slowest and has the steepest increases in run time as minsup
3available from http://wwwchestsurgorg 4http://wwwstjuderesearchorg/data/ALL1/ 5http://clinicalproteomicssteemcom/ 6Fewer buckets results in a extremely high running time ( up to a few days ) for CHARM and CLOSET . is decreased . CHARM on the other hand is generally 2 to 3 orders of magnitude slower than CARPENTER and only outperforms CARPENTER at higher support level , where the difference in time is under 10 seconds . 4.2 Varying Length Ratio
Figure 6 shows the performance comparison of the methods as we vary l , with minsup = 4 . The growth in run time of all the algorithms is exponential with respect to the length ratio ( note the log scaled y axes ) . CARPENTER is however substantially faster than CHARM and CLOSET . While CHARM outperforms CLOSET , CARPENTER can be up to a 100 times faster than CHARM and 1000 times faster than CLOSET .
As we can see , in all the experiments we conducted , CARPENTER outperforms CHARM and CLOSET in most cases . These results clearly demonstrate that CARPENTER is very efficient in finding frequent closed patterns on datasets with small number of rows and large number of features .
5 . RELATED WORK
Frequent pattern mining [ 1 , 5 , 11 , 8 ] as a vital topic has received a significant amount of attention during the past decade . The number of frequent patterns in a large data set can be very large and many of these frequent patterns may be redundant . To reduce the frequent patterns to a compact size , mining frequent closed patterns has been proposed . The followings are some new advances for mining closed frequent patterns .
Close [ 6 ] and Pascal [ 2 ] are two algorithms which discover closed patterns by performing breadth first , column enumeration . Close [ 6 ] is an Apriori like algorithm to determine a closed itemset . Due to the level wise approach of Close and Pascal , the number of feature sets enumerated will be extremely large when they are run on long biological datasets . In [ 7 ] , the CLOSET algorithm was proposed for mining closed frequent patterns . Unlike Close and Pascal , CLOSET performs depth first , column enumeration . CLOSET uses a novel frequent pattern tree ( FP structure ) for a compressed representation of the datasets . It then performs recursive computation of conditional tables to simulate the search on the column enumeration tree . CLOSET is unable to handle long biological datasets because of two reasons . First , the FP tree is unable to give good compression for long rows . Second , there are too many combinations when performing column enumerations . CLOSET+ [ 9 ] is a recent improvement on CLOSET . Our study show that CARPENTER still outperforms CLOSET+ on average by around 500 600 times . 7
Another algorithm for mining frequent closed pattern is CHARM [ 10 ] . Like CLOSET , CHARM performs depthfirst , column enumeration . However , unlike CLOSET , CHARM stores the dataset in a vertical format where a list of row ids is stored for each feature . These row id lists are then merged during the column enumeration to generate new rows id lists that represent nodes in the enumeration tree . In addition , a technique called diffset is used to reduce the size of the row id lists and the computational complexity for merging them . Although performance studies in [ 10 ] shows that CHARM is substantially faster than all other algorithm on most datasets , CHARM is still unable to handle long biolog
7We will like to thank Jianyong Wang and Jiawei Han for making the executable code of CLOSET+ available to us . They have indicated that the version of CLOSET+ that they pass to us at press time is not optimized for our datasets and an optimized version will be released in the future .
Figure 5 : Varying minsup with l=0.6 : a ) LC , b ) ALL , c ) OC
Figure 6 : Varying l with minsup = 4 % : a ) LC , b ) ALL , c ) OC ical dataset efficiently because it performs feature enumeration .
6 . CONCLUSION
In this paper , we proposed an algorithm called CARPENTER for finding frequent closed patterns in long biological datasets . CARPENTER makes use of the special charateristic of biological datasets to enhance its efficiency . It adopts the novel approach of performing row enumeration instead of the conventional column enumeration so as to overcome the extremely high dimensionality of many biological datasets . Experiments show that this bold approach yields good payoff as CARPENTER outperforms exisiting closed pattern discovery algorithms like CHARM and CLOSET by a large order of magnitude when they are running on long biological datasets . In the future , we will look at how CARPENTER can be extended to work on other datasets by using a combination of column and row enumerations .
7 . REFERENCES [ 1 ] R . Agrawal and R . Srikant . Fast algorithms for mining association rules . In Proc . 1994 Int . Conf . Very Large Data Bases ( VLDB’94 ) , pages 487–499 , Santiago , Chile , Sept . 1994 .
[ 2 ] Y . Bastide , R . Taouil , N . Pasquier , G . Stumme , and
L . Lakhal . Mining frequent closed itemsets with counting inference . In SIGKDD Explorations , 2(2 ) , Dec . 2000 .
[ 3 ] K . Beyer and R . Ramakrishnan . Bottom up computation of sparse and iceberg cubes . In Proc . 1999 ACM SIGMOD Int . Conf . Management of Data ( SIGMOD’99 ) , pages 359–370 , Philadelphia , PA , June 1999 .
[ 4 ] J . Han , J . Pei , and Y . Yin . Mining partial periodicity using frequent pattern trees . In Computing Science
Techniqcal Report TR 99 10 , Simon Fraser University , July 1999 .
[ 5 ] H . Mannila , H . Toivonen , and A . I . Verkamo . Efficient algorithms for discovering association rules . In Proc . AAAI’94 Workshop Knowledge Discovery in Databases ( KDD’94 ) , pages 181–192 , Seattle , WA , July 1994 . [ 6 ] N . Pasquier , Y . Bastide , R . Taouil , and L . Lakhal . Discovering frequent closed itemsets for association rules . In Proc . 7th Int . Conf . Database Theory ( ICDT’99 ) , pages 398–416 , Jerusalem , Israel , Jan . 1999 .
[ 7 ] J . Pei , J . Han , and R . Mao . CLOSET : An efficient algorithm for mining frequent closed itemsets . In Proc . 2000 ACM SIGMOD Int . Workshop Data Mining and Knowledge Discovery ( DMKD’00 ) , pages 11–20 , Dallas , TX , May 2000 .
[ 8 ] P . Shenoy , J . Haritsa , S . Sudarshan , G . Bhalotia ,
M . Bawa , and D . Shah . Turbo charging vertical mining of large databases . In Proc . 2000 ACM SIGMOD Int . Conf . Management of Data ( SIGMOD’00 ) , pages 22–23 , Dallas , TX , May 2000 . [ 9 ] J . Wang , J . Han , and J . Pei . Closet+ : Searching for the best strategies for mining frequent closed itemsets . In Proc . 2003 ACM SIGKDD Int . Conf . on Knowledge Discovery and Data Mining ( KDD’03 ) , Washington , DC , Aug 2003 .
[ 10 ] M . Zaki and C . Hsiao . Charm : An efficient algorithm for closed association rule mining . In Proc . of SDM 2002 , 2002 .
[ 11 ] M . J . Zaki , S . Parthasarathy , M . Ogihara , and W . Li . New algorithms for fast discovery of association rules . In Proc . 1997 Int . Conf . Knowledge Discovery and Data Mining ( KDD’97 ) , pages 283–286 , Newport Beach , CA , Aug . 1997 .
Sample Wise Enumeration Methods for Mining Microarray Datasets
Anthony K . H . Tung
Department of Computer Science National University of Singapore
A Microarray Dataset
1000 100,000 columns
Gene1
Gene2 Gene3
Gene4 Gene5 Gene6 Ge
Sample1 Sample2
Class Cancer Cancer
100500 rows
. . .
SampleN 1 SampleN
~Cancer ~Cancer
• Find closed patterns which occur frequently among genes . • Find rules which associate certain combination of the columns that affect the class of the rows – Gene1,Gene10,Gene1001 > Cancer
Challenge I
• Large number of patterns/rules
– number of possible column combinations is extremely high
• Solution : Concept of a closed pattern
– Patterns are found in exactly the same set of rows are grouped together and represented by their upper bound and 4
• Example : the following patterns are found in row 2,3 Class
1 a ,b,c,l,o,s C 2 a ,d , e , h ,p,l,r C 3 a ,c , e , h ,o,q,t C 4 a , e ,f , h ,p,r ~C 5 b,d,f,g,l,q,s,t ~C upper bound ( closed pattern ) aeh ah i ri ae eh e h lower bounds
“ a ” however not part of the group
Challenge II
• Most existing frequent pattern discovery algorithms perform searches in the column/item enumeration space ie systematically testing various combination of columns/items
• For datasets with 1000 100,000 columns , this search space is enormous
• Instead we adopt a novel row/sample enumeration algorithm for this purpose . CARPENTER ( SIGKDD’03 ) is the FIRST algorithm which adopt this approach
Column/Item Enumeration Lattice
• Each nodes in the lattice represent a combination of columns/items
• An edge exists from node A to B if A is subset of B and A differ from B by only 1 column/item • Search can be done breadth first a,b,c,e a,b,c a,b,c a,b,e a,c,e a,b,e a,c,e b,cb,c b,c b,cb,c b,c a,b a,b a,b a,b a,c a,c a,c a,c a,c a,c a,e a,e a,e a,e b,c b,c b,c b,c b,c b,c b b i ri
Class C 1 a,b,c,l,o,s C 2 a,d,e,h,p,l,r C 3 a,c,e,h,o,q,t 4 a,e,f,h,p,r ~C 5 b,d,f,g,l,q,s,t ~C a a a a a a b b b b b b c c c c c c start
{}
Column/Item Enumeration Lattice
• Each nodes in the lattice represent a combination of columns/items
• An edge exists from node A to B if A is subset of B and A differ from B by only 1 column/item
• Search can be done depth first • Keep edges from parent to child only if child is the prefix of parent a,b,c,e a,b,c a,b,c a,b,e a,b,e a,c,e a,c,e b,cb,c b,c a,b a,b a,b a,c a,c a,c a,c a,e a,e b,c b,c b,c b b i ri
Class C 1 a,b,c,l,o,s C 2 a,d,e,h,p,l,r C 3 a,c,e,h,o,q,t 4 a,e,f,h,p,r ~C 5 b,d,f,g,l,q,s,t ~C a a a a b b b c c c start
{}
General Framework for Column/Item
Enumeration
Read based
Write based
Point based
Association Mining
Apriori[AgSr94 ] ,
DIC
Sequential Pattern
Discovery
GSP[AgSr96 ]
Eclat ,
MaxClique[Zaki01 ] ,
FPGrowth [ HaPe00 ]
SPADE
[ Zaki98,Zaki01 ] ,
PrefixSpan [ PHPC01 ]
Hmine
Iceberg Cube
Apriori[AgSr94 ]
BUC[BeRa99 ] , HCubing [ HPDW01 ]
A Multidimensional View types of data or knowledge associative pattern others other interest measure constraints pruning method sequential pattern compression method iceberg cube closed/max pattern lattice trans main operations versal/ read write point
Sample/Row Enumeration Algorihtms • To avoid searching the large column/item enumeration space , our mining algorithm search for patterms/rules in the sample/row enumeration space
• Our algorithms does not fitted into the column/item enumeration algorithms
• They are not YAARMA ( Yet Another Association
Rules Mining Algorithm )
• Column/item enumeration algorithms simply does not scale for microarray datasets
Existing Row/Sample Enumeration Algorithms
• CARPENTER(SIGKDD'03 )
– Find closed patterns using row enumeration
• FARMER(SIGMOD’04 )
– Find interesting rule groups and building classifiers based on them
• COBBLER(SSDBM'04 )
– Combined row and column enumeration for tables with large number of rows and columns
• FARMER's demo ( VLDB'04 ) • Balance the scale : 3 row enumeration algorithms vs >50 column enumeration algorithms
Concepts of CARPENTER i ri
Class C 1 a,b,c,l,o,s 2 a,d,e,h,p,l,r C 3 a,c,e,h,o,q,t C 4 a,e,f,h,p,r ~C 5 b,d,f,g,l,q,s,t ~C
Example Table
C
~C
5 4 4,5 5 4 5 ij R ( ij ) a 1,2,3 4 b 1 5 c 1,3 d 2 e 2,3 f g h 2,3 l 1,2 o 1,3 p 2 q 3 r 2 s 1 t 3
4 5 4 5 5
Transposed Table,TT
C 1,2,3 a 2,3 e h 2,3
~C 4 4 4
TT|{2,3}
Row Enumeration
{a}123 {a}124 125 {l} 134 {a} {}135 {}145 234 {aeh} {}235 {}245 345 {}
12 {al} 13 {aco} 14 {a} 15 {bls}
23 {aeh} 24
{aehpr}
25 {dl} 34 {aeh}
35 {q}
45 {f}
{}
1
{abclos}
2
{adehplr}
3
{acehoqt}
4
{aefhpr}
5
{bdfglqst}
12345
{}
TT|{1}
1234 {a} 1235 {}
1245 {}
1345 {}
2345 {}
TT|{123} TT|{124} ij R ( ij )
C
~C a 1,2,3 4 b 1 5 c 1,3 l 1,2 o 1,3 s
5
5
1
TT|{12} ij R ( ij ) ij R ( ij )
C C
~C ~C a 1,2,3 4 a 1,2,3 4 ij R ( ij )
C
~C
5 4 4,5 5 4 5 a 1,2,3 4 b 1 5 c 1,3 d 2 e 2,3 f g h 2,3 l 1,2 o 1,3 p 2 q 3 r 2 s 1 t 3
4 5 4 5 5 ij R ( ij )
C
~C a 1,2,3 4 l 5
1,2
Pruning Method 1
• Removing rows that appear in all tuples of transposed table will not affect results r2 r3 {aeh} r2 r3 r4 {aeh} r4 has 100 % support in the conditional table of “ r2r3 ” , therefore branch “ r2 r3r4 ” will be pruned .
C a 1,2,3 e 2,3 h 2,3
~C 4 4 4
TT|{2,3}
Pruning method 2
1234 {a} 1235 {}
1245 {}
1345 {}
2345 {}
{a}123 {a}124 125 {l} 134 {a} {}135 {}145 234 {aeh} {}235 {}245 345 {}
12 {al} 13 {aco} 14 {a} 15 {bls}
23 {aeh} 24
{aehpr}
25 {dl} 34 {aeh}
35 {q}
45 {f}
{}
1
{abclos}
2
{adehplr}
3
{acehoqt}
4
{aefhpr}
5
{bdfglqst}
• 12345 {} if a rule is discovered before , we can prune enumeration below this node – Because all rules below this node has been discovered before
– For example , at node
34 , if we found that {aeh} has been found , we can prune off all branches below it
C a 1,2,3 e 2,3 h 2,3
~C 4 4 4
TT|{3,4}
Pruning Method 3 : Minimum Support
• Example : From TT|{1} , we can see that the support of all possible pattern below node {1} will be at most 5 rows .
TT|{1} ij R(ij )
C
~C a 1,2,3 4 b 1 5 c 1,3 l 1,2 o 1,3 s 1
5
5
From CARPENTER to FARMER
• What if classes exists ? What more can we do ?
• Pruning with Interestingness Measure
– Minimum confidence – Minimum chi square
• Generate lower bounds for classification/ prediction
Interesting Rule Groups
• Concept of a rule group/equivalent class
– rules supported by exactly the same set of rows are grouped together
• Example : the following rules are derived from row 2,3 and 4 with 66 % confidence aeh > C(66 % ) upper bound ae >C ( 66 % ) ah > C(66 % ) eh >C ( 66 % ) e >C ( 66 % ) h >C ( 66 % ) lower bounds i ri
Class C 1 a ,b,c,l,o,s 2 a ,d , e , h ,p,l,r C 3 a ,c , e , h ,o,q,t C 4 a , e ,f , h ,p,r ~C 5 b,d,f,g,l,q,s,t ~C a >C however is not in the group
Pruning by Interestingness Measure
• In addition , find only interesting rule groups
( IRGs ) based on some measures : – minconf : the rules in the rule group can predict the class on the RHS with high confidence
– minchi : there is high correlation between LHS and
RHS of the rules based on chi square test
• Other measures like lift , entropy gain , conviction etc . can be handle similarly
Ordering of Rows : All Class C before ~C
{a}123 {a}124 125 {l} 134 {a} {}135 {}145 234 {aeh} {}235 {}245 345 {}
12 {al} 13 {aco} 14 {a} 15 {bls}
23 {aeh} 24
{aehpr}
25 {dl} 34 {aeh}
35 {q}
45 {f}
{}
1
{abclos}
2
{adehplr}
3
{acehoqt}
4
{aefhpr}
5
{bdfglqst}
12345
{}
TT|{1}
1234 {a} 1235 {}
1245 {}
1345 {}
2345 {}
TT|{123} TT|{124} ij R ( ij )
C
~C a 1,2,3 4 b 1 5 c 1,3 l 1,2 o 1,3 s
5
1
5
TT|{12} ij R ( ij ) ij R ( ij )
C C
~C ~C a 1,2,3 4 a 1,2,3 4 ij R ( ij )
C
~C
5 4 4,5 5 4 5 a 1,2,3 4 b 1 5 c 1,3 d 2 e 2,3 f g h 2,3 l 1,2 o 1,3 p 2 q 3 r 2 s 1 t 3
4 5 4 5 5 ij R ( ij )
C
~C a 1,2,3 4 l 5
1,2
Pruning Method : Minimum Confidence
• Example : In TT|{2,3} on the right , the maximum confidence of all rules below node {2,3} is at most 4/5
C ~C 1,2,3,6 4,5 a 4,9 e 2,3,7 h 2,3 4
TT|{2,3}
Pruning method : Minimum chi square
• Same as in computing maximum confidence
C
~C
Total
C ~C 1,2,3,6 4,5 a 4,9 e 2,3,7 h 2,3 4
TT|{2,3}
A ~A min=1 max=5 Computed Computed Computed Computed Constant Constant
Constant
Finding Lower Bound , MineLB a,b,c,d,e abc cde ad ae bd be a b c d e
Candidate lower bound : ad , ae , bd , be Candidate lower bound : ad , ae , bd , be , cd , ce Kept since no lower bound override them Removed since d,e are still lower bound
– Example : An upper bound rule with antecedent A=abcde and two rows ( r1 : abcf ) and ( r2 : cdeg )
– Initialize lower bounds
{a , b , c , d , e}
– add “ abcf ” new lower {d ,e}
– Add “ cdeg ” new lower bound{ad , bd , ae , be}
Implementation
• In general , CARPENTER
FARMER can be implemented in many ways : – FP tree – Vertical format
• For our case , we assume the dataset can be fitted into the main memory and used pointerbased algorithm similar to BUC
C
~C
5 4 4,5 5 4 5 ij R ( ij ) a 1,2,3 4 b 1 5 c 1,3 d 2 e 2,3 f g h 2,3 l 1,2 o 1,3 p 2 q 3 r 2 s 1 t 3
4 5 4 5 5
Experimental studies
• Efficiency of FARMER – On five real life dataset
• lung cancer ( LC ) , breast cancer ( BC ) , prostate cancer
( PC ) , ALL AML leukemia ( ALL ) , Colon Tumor(CT )
– Varying minsup , minconf , minchi – Benchmark against
• CHARM [ ZaHs02 ] ICDM'02 • Bayardo ’s algorithm ( ColumE ) [ BaAg99 ] SIGKDD'99
• Usefulness of IRGs
– Classification
Example results Prostate
FARM ER ColumnE CHARM
100000
10000
1000
100
10
1
3
4
5
6
7
8
9 minimum sup p o rt
Example results Prostate
1200
1000
800
600
400
200
0
FARM ER:minsup=1:minchi=10
FARM ER:minsup =1
0
50
70
80
85
90
99 minimum confidence( % )
Naive Classification Approach
• Generate the upper bounds of IRGs • Rank the upper bounds , thus ranking the IRGs ; • Apply coverage pruning on the IRGs ; • Predict the test data based on the IRGs that it covers .
Classification results
Summary of Experiments
• FARMER is much more efficient than existing algorithms
• There are evidences to show that IRGs is useful for classification of microarray datasets
COBBLER : Combining Column and Row Enumeration
• Extend CARPENTER to handle datasets with both large number of columns and rows
• Switch dynamically between column and row enumeration based on estimated cost of processing
Single Enumeration Tree r1r2r3r4
{ } r1r2r3 {c} r1r2r4 { } r1r3r4 { } r2r3r4 { }
{ } ab {r1} ac {r1r2} ad {r2} bc {r1r3} bd { } cd {r2 } a
{r1r2} b
{r1r3} c
{r1r2r3} d
{r2r4} abcd { } abc {r1} abd { } acd { r2} bcd { }
{ } r1 r2 r3 r4 a b c a c d b c d r1 {abc} r2 {acd} r3 {bc} r4{d} r1r2 {ac} r1r3 {bc} r1r4 { } r2r3 {c} r2r4{d } r3r4 { }
Feature enumeration
Row enumeration
Dynamic Enumeration Tree abcd { } abc {r1} abd { } acd { r2} r1 r2 bc cd
{ } b c d r1 r1 r2 r2 r1 {bc} r2 {cd} r1 {c} r3 { c} r2 {d } a
{r1r2} b
{r1r3} c
{r1r2r3} d
{r2r4} r1r2 {c} r1r3 { c} a
{r1r2} ab {r1} ac {r1r2} ad {r2} abc : {r1} ac : {r1r2} acd : {r2}
Feature enumeration to Row enumeration
Dynamic Enumeration Tree ab {} ac { r2} bc {r3 } ac{r1 } ad{ } cd { } bc {r1 } a{r2} b{r3} c{r2r3 } a{r1} c {r1r3} d {r4 } b{r1 } c{r1r2 } r1r2 {ac} r1r2r3r4
{ } r1r2r3 {c} r1r2r4 { } r1r3 {bc} r1r3r4 { } r1 {abc} acd { } r1r4 { } ac : {r1r2} bc : {r1r3} c : {r1r2r3}
Row enumeration to Feature Enumeration
{ } r1 {abc} r2 {acd} r3 {bc} r4 {d}
Switching Condition
• Naïve idea of switching based on row number and feature number does not work well
• to estimate the required computation of an enumeration sub tree , ie , row enumeration sub tree or feature enumeration sub tree . – Estimate the maximal level of enumeration for each children sub tree
• Example of estimating the maximal level of enumeration :
– Suppose r=10 , S(f1)=0.8 , S(f2)=0.5 , S(f3)=0.5 , S(f4)=0.3 and minsup=2
– S(f1)*S(f2)*S(f3)*r =2 ≥ minsup – S(f1)*S(f2)*S(f3)*S(f4)*r =0.6 < minsup – Then the estimated deepest node under f1 is f1f2f3
Switching Condition
Switching Condition
To estimate for a node : To estimate for a path :
To sum up estimation of all paths as the final estimation
Length and Row ratio
) . c e s ( e m i t n u R
14000 12000 10000 8000 6000 4000 2000 0 0.75
COBBLER CLOSET+ CHARM
0.8
0.85
0.9
0.95
1
1.05
Length Ratio
80000
70000
60000
50000
40000
30000
20000
10000
) . c e s ( e m i t n u R
0
0.5
COBBLER CLOSET+ CHARM
1 Row Ratio
1.5
2
Synthetic data
Extension of our work by other groups
( with or without citation )
• [ 1 ] Using transposition for pattern discovery from microarray data , Francois Rioult ( GREYC CNRS ) , Jean Francois Boulicaut ( INSA Lyon ) , Bruno Cremileux ( GREYC CNRS ) , Jeremy Besson ( INSA Lyon )
• See the presence and absence of genes in the sample as a binary matrix . Perform a transposition of the matrix which is essentially our transposed table . Enumeration methods are the same otherwise .
Extension of our work by other groups
( with or without citation ) II
•
[ 2 ] Mining Coherent Gene Clusters from Gene Sample Time Microarray Data . D . Jiang , Jian Pei , M . Ramanathan , C . Tang and A . Zhang . ( Industrial full paper , Runner up for the best application paper award ) . SIGKDD’2004
Gene1 Gene
2
Gene3 Gene
4
Sample1 Sample2
. . .
SampleN1 SampleN
Extension of our work by other groups
( with or without citation ) III
Gene
3
Gen 4
Gene
1
Gene
2 1.23 1.34
1.52
S1 S2
. . .
SN 1 SN
A gene in two samples are say to be coherent if their time series satisfied a certain matching condition
In CARPENTER , a gene in two samples are say to be matching if their expression in the two samples are almost the same
Extension of our work by other groups
( with or without citation ) IV
[ 2 ] Try to find a subset of samples S such that a subset of genes G is coherent for each pair of samples in S . |S|>mins , |G|>ming
In CARPENTER , we try to find a subset of samples S in which a subset of genes G is similar in expression level for each pair of samples in S . |S|>mins , |G|>0
Gene1 Gene2 Gene
3
Gene4
S1 S2
SN 1 SN
. . .
1.23 1.34
1.52
Extension of our work by other groups
( with or without citation ) V
12
13 {aco} 14 {a} 15 {bls}
23 {aeh} 24
{aehpr}
25 {dl}
1
{abclos}
2
{adehplr}
3
{acehoqt}
{}
12 {a 12 {
12 {
13 {
23 {
{a}123 {a} 125 {l} 134 {a} {}135 {}145 234 {aeh} {}235 245
[ 2 ] Perform sample wise enumeration and remove genes that are not pairwise coherent across the samples enumerated
CARPENTER : Perform samplewise enumeration and remove genes that does not have the same expression level across the samples enumerated
Extension of our work by other groups
( with or without citation ) VI
From [ 2 ] : Pruning Rule 3.1 ( Pruning small sample sets ) . At a node v = fsi1 ; : : : ; sikg , the subtreeof v can be pruned if ( k + jTailj ) < mins
TT|{1}
• Pruning Method 3 in CARPENTER :
From TT|{1} , we can see that the support of all possible pattern below node {1} will be at most 5 rows . ij R(ij )
C
~C a 1,2,3 4 b 1 5 c 1,3 l 1,2 o 1,3 s 1
5
5
Extension of our work by other groups
1234 {a} 1235 {}
1245 {}
1345 {}
2345 {}
•
[ 2 ] Pruning Rule 3.2 ( Pruning subsumed sets ) . At a node v = {si… sik} if {si1,…sik} U Tail is a subset of some maximal coherent sample set , then the subtree of the node can be pruned .
( with or without citation ) VII {a}123 {a}124 125 {l} 134 {a} {}135 {}145 234 {aeh} {}235 {}245 345 {}
25 {dl} 34 {aeh}
12 {al} 13 {aco} 14 {a} 15 {bls}
23 {aeh} 24
{acehoqt}
2
3
4
1
{abclos}
{adehplr}
{aehpr}
{}
• CARPENTER Pruning
Method 2 : if a rule is discovered before , we can prune enumeration below this node
{aefhpr}
35 {q}
5
{bdfglqst}
45 {f}
C ~C 1,2,3 a 4 e 2,3 4 h 2,3 4 TT|{3,4}
Extension of our work ( Conclusion ) • The sample/enumeration framework had been successfully adopted by other groups in mining microarray datasets
• We are proud of our contribution as the group the produce the first row/sample enumeration algorithm CARPENTER and is happy that other groups also find the method useful
• However , citations from these groups would have been nice . After all academic integrity is the most important things for a researcher .
Future Work : Generalize Framework for
Row Enumeration Algorithms ? types of data or knowledge associative pattern others other interest measure constraints sequential pattern pruning method
Only if real life applications require it . compression method iceberg cube closed/max pattern lattice transversal/ main operations read write point
Conclusions
• Many datasets in bioinformatics have very different characteristics compared to those that has been previously studied
• These characteristics can either work against you or for you
• In the case of microarray datasets with large number columns but small number of rows/samples , we turn what is against us to our advantage – Row/Sample enumeration – Pruning strategy
• We show how our methods have been modified by other groups to produce useful algorithm for mining microarray datasets
Thank you!!! atung@compnusedusg wwwcompnusedusg/~atung/sfu_talkpdf
