Learning Query and Document Similarities from Click through
Bipartite Graph with Metadata
Wei Wua , Hang Lib , Jun Xub aDepartment of Probability and Statistics , Peking University bMicrosoft Research Asia
Abstract We consider learning query and document similarities from a click through bipartite graph with metadata on the nodes . The metadata consists of multiple types of features of queries and documents . We aim to leverage both the click through bipartite and the features to learn similarity functions that can effectively measure query document , document document , and query query similarities . The challenges include how to model the similarity functions based on the graph data , and how to accurately and efficiently learn the similarity functions . We propose a method that can solve the problems in a principled way . Specifically , we use two different linear mappings to project queries and documents in two different feature spaces into the same latent space , and take the dot product in the latent space as the similarity function between queries and documents . Query query and document document similarities can also be naturally defined as dot products in the latent space . We formalize the learning of similarity functions as learning of the mappings that maximize the similarities of the observed query document pairs on the enriched click through bipartite . When queries and documents have multiple types of features , the similarity function is defined as a linear combination of multiple similarity functions , each based on one type of features . We further solve the learning problem by using a new technique called Multi view Partial Least Squares ( M PLS ) developed by us . We prove that the learning problem has a global optimal solution . The global optimum can be efficiently obtained through Singular Value Decomposition ( SVD ) . We also theoretically demonstrate that the proposed method is also capable of finding high quality similar queries . We conducted large scale experiments on enterprise search data and web search data . The experimental results on relevance ranking and similar query finding demonstrate that the proposed method works significantly better than the baseline methods . Key words : Similarity Learning , Multi view Partial Least Squares , Click through Bipartite
1 . Introduction
Many tasks in Information Retrieval ( IR ) rely on similarities between pairs of objects . In relevance ranking , given a query , one retrieves the most relevant documents and ranks them based on their degrees of relevance to the query . The relevance between a query and a document can be viewed as a kind of similarity . In query reformulation or rewriting , queries that convey similar search intents but in different forms are created to refine the original query , so that the documents better meeting the users’ information need can be properly retrieved and ranked . The original query and refined queries are in fact similar queries . In query suggestion , queries with Microsoft Research Technical Report November 18 , 2011 related intents are recommended to the user , to help the user to search other useful information . Those queries are also similar queries . In all these tasks , we need to measure similarities between two objects , either query document or query query .
Similarity functions are usually defined based on the features of queries and documents . The relevance models in IR , including Vector Space Model ( VSM ) [ 23 ] , BM25 [ 19 ] , and Language Models for Information Retrieval ( LMIR ) [ 18 , 36 ] can all be viewed as similarity functions between query and document feature vectors [ 34 , 32 ] . Similarity between a query and a document is calculated as similarity between term vectors or n gram vectors . Similarly , queries are represented as vectors in a term space or n gram space , and the dot product or cosine is taken as a similarity function between them [ 37 , 35 ] . All the methods utilize features to calculate similarity functions and we call them feature based methods .
Recently , mining query and document similarities from click through bipartite graph has also been proposed ( cf . , [ 8 , 17] ) . Click through bipartite , which represents users’ implicit judgments on query query , document document , and query document relevance relations , has been proved to be a very valuable source for measuring the similarities . For example , queries which share many co clicked documents may represent similar search intents , and they can be viewed as similar queries [ 5 , 30]1 . These methods only rely on the structure of bipartite graph . We call them graph based methods .
In this paper , we consider leveraging information from both click through bipartite and features to measure query and document similarities . In other words , this is about how to combine the feature based methods and graph based methods . As far as we know , this problem was not well studied previously . We formalize the issue as that of learning query and document similarities from a click through bipartite graph with metadata on the nodes representing multiple types of features . The features may come from multiple sources . For example , for queries , the content of queries , a taxonomy of semantic classes , and the user information associated with queries can be defined as features ; for documents , features can be extracted from the URLs , the titles , the bodies , and the anchor texts of web documents . Formally , we assume that there is a query document bipartite graph . The bipartite graph consists of triplets ( q,d,t ) , where q denotes a query , d denotes a document , and t denotes the number of clicks between q and d . Besides , we assume that queries and documents on the bipartite graph have multiple types of features . For each type i ( i 1 ) , queries and documents are represented as vectors in query space Qi and document space Di . Qi and Di are subspaces of the Euclidian spaces 4sqi and 4sdi , where sqi and sdi represent the dimensionalities of the Euclidean spaces . Qi and Di may or may not be the same space , which means that queries and documents may be either homogeneous or heterogeneous data . Figure 1 illustrates the relationships .
Our goal is to accurately and efficiently learn the similarity functions from the enriched clickthrough bipartite graph . There are several challenges : 1 ) how to model the similarity functions based on the complicated graph data , particularly when there are multiple types of features ; 2 ) how to accurately learn the similarity functions ; 3 ) how to efficiently perform the learning task . We propose a method that can solve all the problems in a theoretically sound way .
1 ) Specifically , for each type of features , we use two linear mappings to project query vectors in the query space and document vectors in the document space into the same latent space . We take the dot product of the images in the latent space as the similarity function between the query and document vectors . Figure 2 illustrates the method . The dot product in the latent
1There are different ways to define query similarity . In this paper query similarity and document similarity are defined from the viewpoint search intents .
2
Figure 1 : Click through bipartite with metadata on nodes representing queries and documents in feature spaces . space is also taken as the similarity function between query and query and the similarity function between document and document . The two mappings are supposed to be different , because of the difference between the query space and document space . The final similarity functions are defined as linear combinations of similarity functions from different feature types .
2 ) We learn the mappings and combination weights using the enriched click through bipartite data . The numbers of clicks between queries and documents represent similarities between queries and documents . We formalize the learning method as an optimization problem , in which the objective is to maximize the similarities of the observed query document pairs on the clickthrough bipartite . We regularize the combination weights with L2 norm and we make orthogonal assumptions on the mappings . We propose a new machine learning technique called Multi view Partial Least Squares ( M PLS ) to learn the linear mappings . M PLS is an extension of the Partial Least Squares technique into multi view ( multiple feature types ) . When there is only one type of features ( one view ) , then M PLS degenerates to the conventional PLS . Moreover , if there is no metadata used , then M PLS becomes equivalent to Latent Semantic Indexing ( LSI ) and thus our method can also be regarded as an extension of LSI . We prove that although the optimization problem is not convex , the solution of M PLS is global optimal . We also conduct theoretical analysis on the method , and point out that it has two properties that enable the method to capture query query similarity as well ( this is also true for document document similarity ) , although the learning of it is not explicitly incorporated into the formulation .
3 ) The learning task can be efficiently performed through Singular Value Decomposition ( SVD ) . First , we employ the power method to build an SVD solver . After that , we solve a simple quadratic program to learn the optimal combination weights . We show that the quadratic program has a close form solution .
We conducted experiments on large scale enterprise search data and web search data . The results on relevance ranking and similar query finding show that our method significantly outperforms the baseline methods . Specifically , in relevance ranking , we compare our method with the state of the art feature based methods such as BM25 , graph based methods such as random walk and their linear combinations . Our method not only significantly outperforms the feature based methods and graph based methods , but also significantly performs better than their linear combinations . In similar query finding , we use examples to demonstrate the capability of our method on finding high quality similar queries . When compared with the baseline methods such as random walk and cosine similarity , our method significantly outperforms the feature based methods and graph based methods , and is comparable with the best linear combination . Particularly , we find our method performs better on tail queries , and the results also strongly support
3
Figure 2 : Projecting queries and documents from the query space and document space into a latent space using two linear mappings LQi and LDi . The dot product in the latent space is taken as the similarity function . the conclusions of our theoretical analysis .
Our contributions in this paper include : 1 ) proposal of a new problem on learning query and document similarities from an enriched click through bipartite graph ; 2 ) proposal of a new learning method called M PLS to perform the learning task ; 3 ) theoretical proof of the properties of the proposed method and empirical demonstration of the effectiveness of the method .
The rest of the paper is organized as follows : in Section 2 , we conduct a survey on related work . Then , we define the similarity learning problem in Section 3 . After that , we explain our method based on M PLS in Section 4 . Experimental results are reported in Section 5 , and Section 6 concludes the paper and provides some future research directions . The proof of theorem is given in Appendix .
2 . Related Work
Partial Least Squares ( PLS ) [ 20 ] refers to a class of algorithms in statistics that aims to model the relations between two or more sets of data by projecting them into a latent space . The underlying idea of PLS algorithms is to model collinearity between different data sets . Since the first work by Wold [ 31 ] , many variants of PLS have been developed and used in many tasks such as regression [ 26 ] , classification [ 4 ] and dimension reduction [ 25 ] . In practice , PLS has been successfully applied in chemometrics [ 20 ] , biometrics [ 7 ] , computer vision [ 25 ] and graph mining [ 22 ] . In this paper , we extend PLS to M PLS and employ M PLS in web search . If there is only one type of feature in the query as well as in the document space , our similarity learning problem degenerates to a problem which can be directly solved by PLS .
Canonical Correlation Analysis ( CCA ) [ 14 ] or its kernelized version KCCA [ 12 , 13 ] is an alternative method to PLS . Both attempt to learn linear mapping functions to project objects in two spaces into the same latent space . The difference between CCA and PLS is that CCA learns cosine as the similarity function and PLS learns dot product as the similarity function . In our work , we choose PLS instead of CCA , because it is easier to enhance the efficiency In CCA it is necessary to compute the inverse of large matrices 2 , which is for the former . computationally expensive .
Measuring query and document similarities is always an important research topic in IR . Existing work on query and document similarities can be divided into two groups : feature based methods and graph based methods . In the former group , Vector Space Model ( VSM ) [ 23 ] , BM25 [ 19 ] , and Language Models for Information Retrieval ( LMIR ) [ 18 , 36 ] make use of features ,
2http://enwikipediaorg/wiki/Canonical_correlation
4 particularly , n gram features to measure query document similarity . As pointed out by Xu et al . [ 34 ] and others that these models can be viewed as models using the dot product between a query vector and a document vector as the query document similarity function . Similarly , queries can also be represented as n grams , and the cosine or dot product can be utilized as the similarity function between them [ 37 , 35 ] . In [ 35 ] , queries are represented as n gram vectors , and a cosine similarity function is learned by using distance metric learning . In [ 6 ] , the authors propose calculating query similarity with term and n gram features enriched with a taxonomy of semantic classes . In [ 3 ] , queries are represented as vectors in a high dimensional space with each dimension corresponding to a URL . The click frequency on a URL is used as the value of the corresponding dimension . In the latter group , graph based methods exploit the structure of click through bipartite to learn the query query , document document , and query document similarities . For example , Latent Semantic Indexing ( LSI ) [ 10 ] can be employed , which uses SVD to project queries and documents in a click through bipartite into a latent space , and calculates query document , query query , and document document similarities through the dot product of their images in the latent space . In [ 16 , 1 ] , the authors propose determining the similarity of a pair of objects based on the similarity of other pairs of objects , and the final similarity measure is iteratively calculated on a bipartite graph . A click through bipartite can also be used in clustering of similar queries [ 5 , 30 ] . Craswell and Szummer [ 8 ] extend the idea and propose adopting a backward random walk process on a click through bipartite to propagate similarity through probabilistic transitions . Our method aims to leverage both features and click through bipartite to more accurately learn the similarities .
Methods for learning similarities between objects by utilizing bipartite graphs built from multiple sources have also been studied . In [ 9 ] , term relations and document relations are integrated into a document term bipartite . In [ 28 ] , the authors extend LSI when information from different types of bipartite graphs is available . In [ 33 ] , the authors use a unified relationship matrix to represent different types of objects and their relations . In [ 17 ] , matrix factorization is simultaneously conducted on two bipartites , a user query bipartite and a query document bipartite . In [ 11 ] , similarity scores are first calculated by the LMIR model and then the scores are propagated on a click through bipartite to find similar queries . The method proposed in this paper is different from these existing methods . In our method , we make use of both click through bipartite and multiple types of features on the nodes . In that sense , we combine a feature based method and graph based method . In contrast , the existing methods are all graph based methods and they use either more than one bipartite or a graph with more than one type of relation .
5
The method proposed in this paper is also related to multi view learning [ 21 ] in machine learning . In multi view learning , instances are assumed to have multiple types of features and the goal is to exploit the different types of features to learn a model . In our work , we assume that queries and documents on a click through bipartite graph have multiple types of features , which is similar to the assumption in multi view learning . Our work is unique in that we perform multi view similarity learning on an enriched click through bipartite graph . Our method can be regarded as Multi view PLS , an extension of PLS to multi view learning .
The similarity learning problem studied in this paper is not limited to search , and it can be potentially extended to other applications such as recommendation system and online advertisement . Recently , Wang et al . [ 27 ] propose a method for advertisement in a setting similar to ours . We learn query and document similarities from a bipartite graph with metadata , while they predict possible clicks between users and advertisements on a network with metadata . Note that there is significant difference between our method and their method . First , they only measure similarity between users and advertisements ( corresponding to queries and documents ) , while we also measure query query similarity and document document similarity . Second , their method assumes that the latent space is predefined and is independent from the similarity function , while in our method the latent space as well as the similarity function are learned from data .
3 . Problem Formulation
Let G = ( V,E ) denote a click through bipartite . V = Q D is the set of vertices , which i=1 . E is the set of edges between i=1 and a document set D = {di}n consists of a query set Q = {qi}m query vertices and document vertices . The edge ei j ∈ E between query vertex qi and document vertex d j is weighted by the click number ti j . We assume that G is an undirected graph . Besides , we assume that there exists rich metadata on the vertices of the bipartite . The metadata consists of l types of features ( l 1 ) . The features may stand for the content of queries and documents and the clicks of queries and documents on the bipartite [ 3 ] , as will be seen in Section 5 . For each type i ( 1 i l ) , query q ∈ Q and document d ∈ D are represented as vectors in space Qi and space Di , respectively , where Qi and Di are subspaces of the Euclidean spaces 4sqi and 4sdi . Figure 1 illustrates the relationships . Our goal is to leverage both the click through bipartite and the features to learn query and document similarities . The similarities may include query document similarity , query query similarity , and document document similarity . In this paper we only study query document similarity and query query similarity , and the same method can be applied to learning of documentdocument similarity . Formally , we learn two similarity functions f ( q , d ) and g(q , q ) given G and i=1 . Similarity function f ( q , d ) measures the similarity between query q ∈ Q and docu{(Qi,Di)}l ment d ∈ D , and similarity function g(q , q ) measures the similarity between queries q , q ∈ Q . The question is then how to model the similarity functions f ( q , d ) and g(q , q ) , and how to accurately and efficiently learn the similarity functions . We propose learning the similarity functions by linearly projecting the queries and documents in the query spaces and document spaces into latent spaces . For each type of feature spaces ( Qi,Di ) , we learn two linear mappings LQi and LDi . LQi is an sqi × ki dimensional matrix which can map a query q from Qi to the ki dimensional latent space Ki , and LDi is an sdi×ki dimensional matrix which can map a document d from Di to the ki dimensional latent space Ki . We assume that ∀i , ki min(sqi , sdi ) . LQi and LDi are different mapping functions due to the difference between Qi and Di . Given q ∈ Q and d ∈ D , the images in the latent space Ki are L d , respectively . We define similarity Qi q . In other words , functions fi(q , d ) and gi(q , q ) as fi(q , d ) = qLQi L Di l l we take the dot products of images of queries and documents in the latent spaces as similarity functions . The final similarity functions f ( q , d ) and g(q , q ) are defined as linear combinations of i=1 and {gi(q , q)}l { fi(q , d)}l i=1 αigi(q , q ) , where αi 0 i=1 : f ( q , d ) = is a combination weight . We learn the mappings {(LQi , LDi)}l i=1 and combination weights {αi}l i=1 from the click through bipartite G and the features {(Qi,Di)}l i=1 . Specifically , we view the click number of a querydocument pair as an indicator of their similarity . We learn {(LQi , LDi)}l i=1 and {αi}l i=1 by maximizing the similarities of the observed query document pairs on the click through bipartite . The underlying assumption is that the higher the click number is , the more similar the query and the document are in the latent spaces . In graph based methods ( cf . , [ 8] ) , click numbers are usually directly used for query and document similarity calculation . In this paper , we take a transformation of click numbers and use the logarithm of them , which is verified to be effective . d , gi(q , q ) = qLQi L Qi q and L Di i=1 αi fi(q , d ) , g(q , q ) =
6
Finally , we consider the following learning problem : l euv∈E i=1 log(tuv ) · qi
αi · u LQi L
Didi v
,
( 1 ) arg max
{(LQi ,LDi )}l i=1,{αi}l i=1 u and di v represent the feature vectors of query qu and document dv in Qi and Di , respecwhere qi tively . Note that an alternative method for learning query and document similarities from G and {(Qi,Di)}l i=1 is to concatenate different types of feature vectors of queries as well as different types of feature vectors of documents and learning two mappings for queries and documents with the two concatenated vectors . The method is a special case of the learning method ( 1 ) . We compare the performances of our proposed method ( 1 ) and this alternative method in Section 5 . Objective function ( 1 ) may go to infinity , since there are no constraints on the mappings v ) | euv ∈ E , 1 i l} are also not {(LQi , LDi)}l bounded . We consider adding proper constraints to the mapping functions {(LQi , LDi)}l i=1 and the weights {αi}l i=1 and the weights {αi}l i=1 , as shown in the next section . i=1 . The features {(qi u , di
4 . Our Method
We further formalize the learning problem in ( 1 ) as a constrained optimization problem . We propose a new learning technique called Multi view Partial Least Squares ( M PLS ) to solve the problem , which is a multi view learning version of PLS . We prove that the problem has a global optimal solution . The optimal mappings can be obtained by Singular Value Decomposition ( SVD ) and the optimal weights can be obtained by quadratic programming . We present the algorithm and its complexity . We also conduct theoretical analysis to demonstrate that our method has the capability to find high quality similar queries , although query query similarity is not directly represented in the formulation .
First , we normalize the feature vectors such that ∀u , v , i , ||qi
41 Constrained Optimization Problem we add orthogonal constraints on the mapping matrices {(LQi , LDi)}l regularization on the weights {αi}l
The similarity learning method is re formalized as l i=1 .
αi · log(tuv ) · qi u LQi L
Didi v arg max i=1 i=1,{αi}l
{(LQi ,LDi )}l subject to L Qi i=1 euv∈E LQi = Iki×ki , L
Di LDi = Iki×ki , αi 0 , u|| = 1 and ||di v|| = 1 . Second , i=1 . Finally , we introduce L2
( 2 ) l i=1 i 1 , α2 where ki min(sqi , sdi ) is a parameter and Iki×ki is an identity matrix . A larger ki means preserving more information in the projection for the type of features . Although problem ( 2 ) is not convex , we can prove that the global optimum exists . Moreover , the global optimal mappings {(LQi , LDi)}l i=1 can be obtained by Singular Value Decomposition ( SVD ) , and the global optimal weights {αi}l i=1 can be obtained by quadratic programming having a close form solution .
7 euv∈E i=1 l l l l i=1 i=1
=
=
= i=1 log(tuv ) · qi
αi · u LQi L
Didi v
  u LQi L
Didi v log(tuv ) · qi log(tuv)di vqi u )LQi euv∈E
αi · Trace
αi · Trace
 L euv∈E
Di(
αi · Trace
L Di MiLQi
, vqi u . arg max Trace LQi ,LDi subject to L Qi
L Di MiLQi LQi = Iki×ki , L
Di LDi = Iki×ki where Mi is defined as euv∈E log(tuv)di
Suppose that ∀i , ki min(sqi , sdi ) , the following theorem indicates that the optimization problem
( 3 ) ki
2 . . . λi p 0 , p = min(sqi , sdi ) , Ui = ( ui has a global optimum and the global optimal solution can be obtained through SVD of Mi : Theorem 41 ∀ki min(sqi , sdi ) , the global optimal solution of problem ( 3 ) exists . Furtheri , where Σi is an sdi × sqi diagonal matrix with singular values more , suppose that Mi = UiΣiV j} are left singular sdi ) where {ui 1 λi λi j} are right singular vectors . The global maximum sqi ) where {vi vectors , and Vi = ( vi ) and is given by ki ˆLDi = ( ui 1 , ui j and the global optimal ˆLQi and ˆLDi are given by ˆLQi = ( vi j=1 λi 2 , . . . , ui ki
The proof is given in Appendix . With Theorem 4.1 , if we define Λi = re write problem ( 2 ) as j , then we can
) , respectively .
2 , . . . , vi ki l
2 , . . . , vi
1 , ui
2 , . . . , ui j=1 λi
1 , vi
1 , vi
42 Global Optimal Solution
Basically , there are two steps in finding the global optimum . First , for each type of features , we find the optimal mappings through solving SVD . Second , we determine the optimal combination weights .
Specifically , the objective function ( 2 ) can be re written as log(tuv ) · qi αi · u LQi L
Didi v
αi
L Qi max {(LQi ,LDi )}l LQi =LDi i=1
LDi =Iki×ki euv∈E log(tuv ) · qi u LQi L
Didi v max {(LQi ,LDi )}l l i=1,{αi}l i=1 LDi =Iki×ki ,αi0 , i=1 α2 i 1
L Qi
LQi =LDi euv∈E i=1 l l i=1
= max l {αi}l i=1 i=1 α2
αi0 , i 1
= max l {αi}l i=1 i=1 α2
αi0 , i 1
αiΛi . i=1
8
( 4 )
It is easy to prove that problem ( 4 ) has a close form global optimum . The optimal weights are given by
Λil i=1 Λ2 i
ˆαi =
1 i l .
,
( 5 )
It is easy to verify that when l = 1 , problem ( 2 ) becomes a problem solvable by Partial Least Squares ( PLS ) [ 20 , 24 ] . The orthogonal assumptions on the mapping matrices make it feasible to employ PLS to solve the problem . Therefore , our method of Multi view PLS is an extension of PLS .
If we assume that only the click through bipartite is used , ie , no feature is used , then problem ( 2 ) becomes equivalent to Latent Semantic Indexing ( LSI ) [ 10]3 . In other words , LSI is a special case of our method . Specifically , in such case l = 1 , both query vectors and document vectors are orthogonal , and in each query vector and each document vector , only one element equals to one that indicates the index of a query or a document and the other elements are zero . As a result , matrix Mi for SVD is exactly the one in LSI .
In problem ( 2 ) , we consider using L2 norm to regularize the weights {αi}l i=1 . An alternative i=1 αi 1 ( ie , L1 norm ) to regularize them . However , such a regularmethod would be to use ization will make the final solution become αi = 1 , if Λi = max1 jl Λ j , and αi = 0 , otherwise . This is a degenerative solution and is not desirable . The L2 regularization in our method does not suffer from the problem . l
43 Algorithm
The learning algorithm is described in Algorithm 1 . The algorithm contains two steps . First , for each type of features , it calculates Mi , and solves SVD of Mi to learn the linear mappings . Then , it calculates the combination weights using ( 5 ) . At Step 2.a , it is necessary to calculate Mi . Suppose that there are nq queries on the clickthrough bipartite G . Each query has on average κq clicked documents . Then for each type of features , the worst case time complexity of calculating Mi is of order O(nq · κq · sqi · sdi ) , where sqi and sdi denote the dimensionalities of query space Qi and document space Di , respectively . Although sqi and sdi can be very large , the query vectors and document vectors are usually very sparse . This can make the average time complexity much smaller than the worst case time complexity . Suppose that each dimension of query vectors has on average cqi non zero values , and each document vector has on average cdi non zero values . The average time complexity of calculating Mi becomes of order O(sqi · cqi · κq · cdi ) . Since cqi and cdi are much smaller than nq and sdi , Mi can be calculated very efficiently .
We empirically find in our experiments that sparse query and document feature vectors also make the matrix Mi sparse . In our experiments , the ratio of non zero elements in the matrix Mi is not larger than 05 % Therefore , we can employ the power method ( cf . , [ 29 ] ) to build an SVD solver . It is an iterative algorithm , and after i iterations , the ith largest singular values can be obtained . Suppose that there are C non zero elements in matrix Mi . The time complexity for calculating each singular value is O(C + ki · max(sqi , sdi) ) , and the total time complexity is O(ki · C + k2 i · max(sqi , sdi) ) . Since Mi is sparse and C is much smaller than sqi · sdi , when ki is small , SVD of Mi can be solved efficiently .
3LSI is usually used for learning the similarity between term and document from a term document bipartite graph .
9
Algorithm 1 1 : Input : click through bipartite G = ( V,E ) , feature spaces {(Qi,Di)}l 2 : For each type of feature spaces ( Qi,Di ) i=1 , parameters {ki}l i=1 .
1 , ui
2 , . . . , ui ki
) as ˆLDi , and first ki right singular vectors ki a . Calculate Mi using b . Calculate SVD of Mi . c . Take the first ki left singular vectors ( ui euv∈E log(tuv)di vqi u .
( vi
1 , vi
2 , . . . , vi ki
) as ˆLQi . d . Calculate Λi using j=1 λi j .
3 : Calculate combination weight ˆαi using equation ( 5 ) . 4 : Output : similarity functions : a . ∀q , d , f ( q , d ) = b . ∀q , q , g(q , q ) = l l i=1 ˆαi · qi ˆLQi ˆL di . Di i=1 ˆαi · qi ˆLQi qi . ˆL Qi
44 Query Similarity
Problem ( 2 ) is formalized as learning of query document similarity . Actually the optimization can also help learning of query query similarity ( equivalently document document similarity ) . Here , we show that our method is able to find high quality similar queries as well . Under the orthogonal assumptions , the results of problem ( 2 ) have two properties which can guarantee that the performance of the method on similar query finding is high as well . For a specific feature type i , if the similarity between q , q ∈ Qi is high , then the similarity between images of q and q in the latent space is also high , provided that ki is sufficiently large . Formally , we assume that there is a small number ε/2 > 0 such that qq 1 − ε/2 ( ie , they have high similarity because qq 1 ) . Since ||q|| = ||q|| = 1 , we have ||q − q||2 = ||q||2 + ||q||2 − 2qq ε . Suppose that LQi = ( lQi
) . We have
1 , . . . lQi ki
||L Qi q − L Qi lQi j lQi j
( q − q ) =
|lQi j , q − q|2 . q||2 = ( q − q ) ki ki j=1
Since L Qi
LQi = Iki×ki , we have
||L Qi q − L Qi q||2 =
|lQi j , q − q|2 ||q − q||2 ε . j=1 q||2 − ε)/2 . This inequality indicates that for a Thus , we have qLQi L Qi specific feature type i , if the similarity between two queries q and q is high in the feature space Qi , then the similarity between their images in the latent space is determined by their norms in the space . The square of the norm of query q in the latent space can be calculated as q ( ||L Qi q||2 + ||L Qi
||L Qi q||2 = qLQi L Qi lQi j lQi j q =
|lQi j , q|2 .
( 6 )
Since usually ki sqi , we have ||L Qi that ||L Qi q||2 1 − δ , and ||L Qi q||2 ||q||2 = 1 . With a sufficiently large ki , we can assume q||2 1 − δ , where δ 0 is a small number . Thus , we obtain
10 q = q ki j=1 ki j=1 ki j=1 q 1 − δ − ε q2 , d
1 and d
1 , q d2 . If we assume that q
Second , suppose that q1 , q2 ∈ Qi , d1 , d2 ∈ Di , and q 2 = L 2 and d Di qLQi L 2 . Note that δ monotonically decreases when ki increases . In an extreme Qi case , when ki = sqi , δ = 0 holds . We call this property inheritance . This property ensures that when sufficient information is preserved in the latent space ( ie , ki is sufficiently large ) , similar queries in the original feature space will also be similar in the latent space . 1 = L q1 , q 1 = L 2 = L d1 , Di Qi Qi 1 and d 2 , and d and d 2 are similar pairs with high similarity , then when ki is sufficiently large , we can obtain high similarity between q 1 and 1 1 − ε/18 , 2 . Formally , we assume that there is a small number ε > 0 such that q q 1 d 1|| + ||d 1 − d 2|| + ||q 2 − d 1 − d 2|| . 2 1 − ε/18 . We have ||q 2 1 − ε/18 , and d q 2 d 1 d d1 , similar to the analysis of equation ( 6 ) , we have ||q 1|| ||q1|| = 1 q1 and d 1 = L 1 = L Since q Di Qi 1 − d and ||d 1||2 − 2q 1|| ||d1|| = 1 . We know that ||q 1||2 = ||q 1 d 1 ε/9 . Similarly , we can get ||q 2||2 ε . Similarly 2||2 ε/9 . Thus , we obtain ||q 1 − q 2 − d 2||2 ε/9 and ||d 1 − d to the analysis above , we can say that with a sufficiently large ki , we can have the similarity between q 1 and q 2 large enough . We call this property transitivity . The property ensures that when sufficient information is preserved in the latent space , we can derive similar query pairs from similar query document pairs . The property needs high similarity between documents in the latent space . The condition can be easily met , because document similarity can be preserved with the “ inheritance ” property .
2|| ||q
1 − q 1||2 + ||d
In our experiments , we find that the inheritance property and the transitivity property can really help us on finding high quality similar queries . We also give an example to support our theoretical analysis .
5 . Experiments
We conducted experiments to test the performance of the proposed method on relevance ranking and similar query finding . We used two data sets : enterprise search data and web search data .
51 Data Sets
We collected one year click through data from an enterprise search engine of an IT company and one week click through data from a commercial web search engine . We built two clickthrough bipartite graphs with the data . If there are more than 3 clicks between a query and a URL , we added a link between them . There are 51 , 699 queries and 81 , 186 URLs on the bipartite of enterprise search data . There are 94 , 022 queries and 111 , 631 URLs on the bipartite of web search data . That means that we discarded the links between queries and URLs whose click frequency is lower than or equal to 3 . Each query has on average 2.44 clicked URLs in the enterprise bipartite and each query has on average 1.74 clicked URLs in the web bipartite .
We extracted features for both data sets . Two types of the features were extracted and attached to the nodes of the bipartite graphs . First , we took the words in queries and the words in URLs as features , referred to as “ word features ” . Words were stemmed and stop words were removed . With word features , each query is represented by a tf idf vector in the query space , and each URL is also represented by a tf idf vector in the document space . Each dimension of the query space corresponds to a unique term and so does each dimension of the document space . Note that the two spaces are of high dimension and very sparse . For the enterprise search data , there are 9 , 958 unique terms . For the web search data , the dimensionality of term space is 10 , 791 . Next , we took the numbers of clicks of URLs as the features of queries and the numbers of clicks of
11 queries as features of URLs . We call the features “ graph features ” , because they are derived from the bipartite graph . Each query is represented by a vector of log click numbers in the query space and each document is represented by a vector of log click numbers in the document space . Each dimension of the query space corresponds to a URL on the click through bipartite , and similarly each dimension of the document space corresponds to a query on the click through bipartite . The dimensionalities of the two spaces are also very high , while the densities of the two spaces are very low .
We randomly sampled queries and their associated URLs from the click through bipartites of the two data sets . The relevance between the queries and associated URLs were made by human experts . There are five level judgments , including “ Perfect ” , “ Excellent ” , “ Good ” , “ Fair ” , and “ Bad ” . For the enterprise search data , 1 , 701 queries and their associated URLs have judgments . Each query has on average 16 judged URLs . For the web search data , the number of judged queries is 4 , 445 , and each query has on average 11.34 judged URLs .
52 Experiment Setup
We consider four alternatives to learn a similarity function using our method : 1 ) Only word features are used . We denote the model as M PLSWord ; 2 ) Only graph features are used . We refer to the model as M PLSBipar ; 3 ) The vectors from the word feature space and the vectors from the graph feature space are concatenated to create long vectors . The corresponding space is the Cartesian product of the word feature space and the graph feature space . We call the model M PLSConca ; 4 ) We apply our method to learn the similarity function assuming that queries and documents have two types of features and we learn a linearly combined similarity function . We call the model M PLSCom .
As baselines , we consider feature based methods , graph based methods , and their linear combinations . In relevance ranking , we take BM25 as an example of feature based methods . We choose LSI on click through bipartite graph and random walk ( RW for short ) [ 8 ] as examples of graph based methods . We also use a linear combination of BM25 and LSI as well as a linear combination of BM25 and RW . In similar query finding , besides LSI and RW , we adopt as baseline methods cosine similarity of two query vectors represented with graph features and cosine similarity of two query vectors represented with word features . We denote them as CosB and CosW , respectively . We also linearly combine CosB , LSI , and RW with CosW .
To evaluate the performances of different methods in relevance ranking , we employ MAP [ 2 ] and NDCG [ 15 ] at positions of 1 , 3 , and 5 as evaluation measures . For similar query finding , qualitative evaluation and quantitative evaluation were made on the discovered similar queries . In the qualitative evaluation , we randomly sampled examples of similar queries found by M PLSCom and evaluated their quality . In quantitative evaluation , we randomly sampled 500 queries from each data set , retrieved the top 3 most similar queries for each query , and manually judged the quality of the similar queries found by each method . The final results are presented in pos comneg graphs , where “ pos ” represents the ratio of queries for which our method provides higher quality similar queries , “ com ” represents the ratio of queries on which the performances of our method and baseline methods are comparable , and “ neg ” represents the ratio of queries on which the baseline methods do a better job .
Note that all the methods in our experiments are ‘unsupervised methods’ . We tested the results for all the methods on the data sets at different parameter settings , and report the best performances of the methods .
12
Table 1 : Weights in combination methods on two data sets
Model
Components
Combination weights Enterprise
Web
Relevance ranking
LSI+BM25 RW+BM25
CosB+CosW LSI+CosW RW+CosW
53 Parameter Setting
( 0.9 , 0.1 ) ( 0.9 , 0.1 )
( LSI,BM25 ) ( RW,BM25 ) Similar query finding ( CosB,CosW ) ( LSI,CosW ) ( RW,CosW )
( 0.9 , 0.1 ) ( 0.9 , 0.1 ) ( 0.9 , 0.1 )
( 0.8 , 0.2 ) ( 0.8 , 0.2 )
( 0.9 , 0.1 ) ( 0.9 , 0.1 ) ( 0.9 , 0.1 )
We set the parameters of the methods in the following way . In BM25 , the default setting is used . There are two parameters in random walk ( RW for short ) : the self transition probability and the number of transition steps . Following the conclusion in [ 8 ] , we fix the self transition probability as 0.9 and choose the number of transition steps from {1 , . . . , 10} . We found that RW reaches a “ stable ” state with a few steps walk . In the experiments on both data sets , after 5 steps we saw no improvement on all the evaluation measures . Therefore , we set 5 as the number of transition steps of RW on both data sets . In LSI , M PLSWord , M PLSBipar , M PLSConca , and M PLSCom , the parameter is the dimensionality of the latent space . We set the dimensionalities in the range of {100 , 200 , . . . , 1000} for the enterprise search data and in the range of {100 , 200 , . . . , 3000} for the web search data . We found that when we increase the dimensionality of the latent space , the performances of LSI , M PLSWord , M PLSBipar , M PLSConca , and M PLSCom are all improved . The larger the dimensionality is , the better the performance is . On the other hand , a large dimensionality means that we need to calculate more singular values and use more computation power . Therefore , we finally choose 1000 as the dimensionalities of the latent spaces for LSI , M PLSWord,M PLSBipar , M PLSConca,M PLSCom for the enterprise data . We choose 3000 as the dimensionalities of the latent spaces for the web data .
In the combination models , the weights are also parameters . In LSI+BM25 an RW+BM25 for relevance ranking , LSI+CosW , RW+ CosW , CosB+ CosW for similar query finding , we choose the combination weights within {0.1 , 0.2 , . . . , 09} We report the best performance of the combination methods . Table 1 shows the weights in the best performing combination models . In M PLSCom , the combination weights are chosen automatically using equation ( 5 ) .
Table 2 shows the properties of matrices for SVD in each method . We can see that although the matrices have high dimensionalities , they are really sparse , and we can efficiently conduct SVD on them .
54 Experimental Results 541 Relevance Ranking Results
Table 3 and Table 4 give the results on relevance ranking on two data sets . We can see that on both data sets , our method M PLSCom not only outperforms the state of the art feature based methods such as BM25 and graph based methods such as RW , but also performs better than their linear combinations . We conducted sign test on the improvement of M PLSCom over the baseline methods . The results show that the improvements of M PLSCom are statistically
13
Table 2 : Properties of SVD matrices in each method
Enterprise search data
9958 × 9958 , 81186 × 51699
Dimension 9958 × 9958 81186 × 51699 91144 × 61657 81186 × 51699 Web search data
10791 × 10791 , 111631 × 94022
Dimension 10791 × 10791 111631 × 94022 122422 × 104813 111631 × 94022
Density
0.4 % , 0.08 %
0.4 % 0.08 % 0.5 % 0.003 %
Density
0.3 % , 0.01 %
0.3 % 0.01 % 0.03 % 0.002 %
M PLSCom M PLSWord M PLSBipar M PLSConca
LSI
M PLSCom M PLSWord M PLSBipar M PLSConca
LSI
Table 3 : Relevance ranking result on enterprise search data
M PLSCom M PLSConca M PLSWord M PLSBipar BM25 RW RW+BM25 LSI LSI+BM25
MAP NDCG@1 NDCG@3 NDCG@5 0.644 0.624 0.628 0.589 0.543 0.603 0.608 0.577 0.605
0.742 0.734 0.725 0.689 0.650 0.684 0.686 0.681 0.702
0.754 0.748 0.742 0.707 0.655 0.702 0.702 0.699 0.712
0.730 0.717 0.711 0.673 0.646 0.657 0.666 0.666 0.688 significant ( p < 0.01 ) , except NDCG@1 compared with RW + BM25 on the web search data . Improvement of M PLSCom on the web search data is slightly lower than the improvement of M PLSCom on the enterprise search data . This may be because the bipartite of web search data is more sparse than that of enterprise search data ( cf . , Table 2 ) .
M PLSConca also performs well on both data sets . The higher complexity of it makes it less attractive than M PLSCom ( Note that the matrix of M PLSConca for SVD has a high dimensionality and is dense ) . M PLSBipar performs worst among the alternatives of our method . This may be because 1 ) the features of M PLSBipar are more sparse ( cf . , Table 2 ) ; 2 ) there is some overlap between the features of M PLSBipar and the similarities between queries and documents in learning .
Figure 3 and Figure 4 show the the performances of M PLSCom with respect to different dimensionalities of the latent spaces on the two data sets . We can see that the performance of M PLSCom will increase when the dimensionalities of the latent spaces increase . On the other hand , the efficiency of the method will decrease , because the time complexity of it is the order of the square of dimensionality ( O(ki · C + k2 i · max(sqi , sdi)) ) . We find that after the dimensionality reaches 1000 in the enterprise data and 3000 in the web data , the improvement of performance becomes slower . That is why we chose 1000 and 3000 as the dimensionalities of M PLSCom for
14
Table 4 : Relevance ranking result on web search data
M PLSCom M PLSConca M PLSWord M PLSBipar BM25 RW RW+BM25 LSI LSI+BM25
MAP NDCG@1 NDCG@3 NDCG@5 0.533 0.521 0.519 0.461 0.461 0.491 0.498 0.448 0.484
0.724 0.718 0.712 0.669 0.682 0.705 0.712 0.663 0.699
0.730 0.725 0.719 0.683 0.684 0.706 0.711 0.675 0.702
0.667 0.658 0.654 0.593 0.626 0.656 0.661 0.584 0.644
Table 5 : Performances of combination of M PLSCom and BM25 on relevance ranking
Enterprise search data
Web search data
MAP NDCG@1 NDCG@3 NDCG@5 0.651 0.537
0.741 0.729
0.753 0.732
0.732 0.678 the two data sets in the experiments . The same thing can be said to the other methods .
Finally , an interesting thing is that when we linearly combine our method M PLSCom with BM25 , the performances of our method can be further improved , indicating that our method M PLSCom is complementary to BM25 . Table 5 shows the results .
542 Similar Query Finding Results
We show the performance of the proposed method on similar query finding . We compare M PLSCom with other baselines . We first show some examples , and then we use pos com neg graphs to make the comparisons .
Qualitative Evaluation . Table 6 gives some examples . For each query , we show the top 3 most similar queries . We can see that M PLSCom is able to find high quality similar queries by effectively using the enriched click through bipartite graph , even for some “ difficult ” queries , which contain typos , abbreviations , concatenations and rare words . For the tail queries consisting of rare words , usually it is very hard for the baseline methods to find their similar queries , because they only have a few clicks . Nonetheless , M PLSCom is still able to find similar queries for them , which is very impressive .
Quantitative Evaluation . We compared the performances of M PLSCom and the baseline methods on the two data sets . In each data set , we evaluated the quality of similar queries of 500 random queries found by each method .
Figure 5 and Figure 6 present the results . We can see that M PLSCom significantly outperforms the feature based methods and graph based methods , including CosB , CosW , LSI and RW . Our method performs better on more than 15 % queries from the enterprise data and on more than 25 % queries from the web data . Only on less than 7 % queries from the enterprise data and less than 3 % queries from the web data , our method performs worse . We conducted sign test , and the results show that all the improvements are statistically significant ( p < 001 ) Among the combined models , CosB + CosW and RW + CosW perform comparably well , and our method
15
Figure 3 : Performances of M PLSCom with respect to different dimensionalities of the latent space on the enterprise data
Figure 4 : Performances of M PLSCom with respect to different dimensionalities of the latent space on the web data still outperforms them . However , the improvement of our method over those methods is not so significant .
We investigated the reasons that the methods can or cannot achieve high performance in similar query finding . First , we found that CosB , LSI and RW are good at handling head queries , since they all rely on co clicks on the click through bipartite to calculate query similarity . Among them , RW performs a little better than CosB . This may be because similarity on the bipartite can be propagated by RW . In contrast , for tail queries , we cannot expect these methods to have good performances . In an extreme case , if a query only has one clicked document and the document is only clicked by the query , then no similar queries can be found for the query . We call this kind of query “ isolated island ” . Second , no matter whether two queries have co clicked documents , if they share some words , they are likely to be judged as similar queries by CosW . Therefore , CosW is good at handling queries sharing terms . This is especially true for tail queries , since tail queries tend to be longer . However , if two similar queries do not share any term , their similarity cannot be captured by CosW . The problem is called “ term mismatch ” , and becomes serious when the queries have spelling errors , abbreviations , and concatenations . The two types of methods ( either use click graph or use terms ) are complementary , and thus when combined together , it is possible to find similar queries with higher probability .
We found that on most queries our method M PLSCom performs equally well with the combination baseline CosB + CosW . Sometimes , the two methods even give the same top 3 most similar queries . It indicates that click through bipartite and features are really useful for similar query finding . On the other hand , we also found that our method can work very well for some really difficult queries on which graph based methods and word based methods fail . The result demonstrates that our method really has the capability to leverage the enriched click through bipartite to learn query similarities .
Finally , we particularly investigated the performance of our method on tail queries on the web search data , since the web search data set is larger and more sparse than the enterprise search data set . First , Table 7 shows an example . The query ‘walmartmoneycard’ is a concatenation , and
16
0505506065070750802004006008001000MAPNDCG@1NDCG@3NDCG@50405060708050010001500200025003000MAPNDCG@1NDCG@3NDCG@5 Figure 5 : Similar query evaluation on enterprise search data .
Figure 6 : Similar query evaluation on web search data . is also an isolated island . As a result , for CosB , LSI and RW , no similar queries can be found . CosW suffers from term mismatch , and thus only those queries that can exactly match ‘walmartmoneycard’ are returned . The combination methods can only use CosW , and thus return the same results with CosW . These baseline methods cannot find similar queries for ‘walmartmoneycard’ . In contrast , M PLSCom can work better than the baseline methods . We found that the key reason is that our method can effectively use the click bipartite graph , specifically similar URLs on the graph . The query ‘walmartmoneycard’ has a clicked document https://wwwwalmartmoneycardcom/ , and its similar query ‘wal mart prepaid visa activation’ also has a clicked document https://wwwwalmartmoneycardcom/walmart/homepageaspx Through our optimization , both the similarity between ‘walmartmoneycard’ and https://wwwwalmartmoneycard com/ and the similarity between ‘wal mart prepaid visa activation’ and https://wwwwalmartmoneycard com/walmart/homepage.aspx are maximized in the latent space ( they are 0.71 and 0.53 , respectively ) . Moreover , since the two documents share a common term ‘walmartmoneycard’ in the term space , their similarity is also captured in the latent space through the learning process of our method ( with similarity 047 ) Therefore , with the two similar documents as a bridge , our method can connect the two queries together and treat them as similar queries .
We also quantitatively evaluated our method on the tail queries from the web search data . We treat queries with total click numbers less than 10 on the click through bipartite as tail queries . There are totally 212 tail queries in the 500 samples from the web data . Figure 7 gives the “ poscom neg ” comparison results . We can see that on tail queries , our method M PLSCom performs even better than itself on the whole sample set . The results indicate the capability of our method on finding high quality similar queries in the tail .
17
0%10%20%30%40%50%60%70%80%90%100%Cos_BCos_WCos_B+Cos_WLSILSI+Cos_WRWRW+Cos_WPosComNeg0%10%20%30%40%50%60%70%80%90%100%Cos_BCos_WCos_B+Cos_WLSILSI+Cos_WRWRW+Cos_WPosComNeg Figure 7 : Similar query evaluation on the tail queries from web search data .
6 . Conclusion and Future Work
In this paper , we have studied the issue of learning query and document similarities from a click through bipartite with metadata . The click through bipartite represents the click relations between queries and documents , while the metadata represents multiple types of features of queries and documents . We aim to leverage both the click through bipartite and features to perform the learning task . We have proposed a method that can solve the problem in a principled way . Specifically , for each type of features , we use two different linear mappings to project queries and documents into a latent space . Then we take the dot product in the latent space as the similarity function between query document pairs . Similarities between query query and document document pairs are simultaneously defined . The final similarity function is defined as a linear combination of similarity functions from different types of features . We learn the mappings and combination weights by maximizing the similarities of the observed query document pairs on the click through bipartite , and make orthogonal assumptions on the mappings and regularize the weights using L2 norm . We have proved that the learning problem has a global optimum . The mappings can be obtained efficiently through Singular Value Decomposition ( SVD ) and the weights can be obtained from a close form solution of a quadratic program . It turns out to be a new learning method as an extension of Partial Least Squares , referred to as Multi view PLS . We have theoretically analyzed the proposed method , and demonstrated its capability on finding high quality similar queries ( also similar documents ) . We have conducted experiments on large scale enterprise search and web search data to test the performance of our method . The results not only indicate that our method can significantly outperform the state of the art methods on relevance ranking and similar query finding , but also verify the correctness of our theoretical analysis .
As future work , we want to further enhance the efficiency of our method and test its performance on larger data sets . To achieve the goal , we may need to parallelize the learning algorithm . We also want to study the kernelization of our method , and thus our method is still applicable when kernel matrices instead of features are available .
References
[ 1 ] I . Antonellis , HG Molina , and CC Chang . Simrank++ : Query rewriting through link analysis of the click graph .
[ 2 ] R . Baeza Yates and B . Ribeiro Neto . Modern Information Retrieval . Addison Wesley Longman Publishing Co . ,
VLDB , 1(1):408–421 , 2008 .
Inc . , Boston , MA , USA , 1999 .
[ 3 ] R . Baeza Yates and A . Tiberi . Extracting semantic relations from query logs . In SIGKDD , pages 76–85 , 2007 . [ 4 ] M . Barker and W . Rayens . Partial least squares for discrimination . Journal of chemometrics , 17(3):166–173 , 2003 . [ 5 ] D . Beeferman and A . L . Berger . Agglomerative clustering of a search engine query log . In KDD , pages 407–416 ,
2000 .
18
0%10%20%30%40%50%60%70%80%90%100%Cos_BCos_WCos_B+Cos_WLSILSI+Cos_WRWRW+Cos_Wposcomneg [ 6 ] A . Broder , P . Ciccolo , E . Gabrilovich , V . Josifovski , D . Metzler , L . Riedel , and J . Yuan . Online expansion of rare queries for sponsored search . In WWW’09 , pages 511–520 , 2009 .
[ 7 ] H . Chun and S . Kelecs . Sparse partial least squares regression for simultaneous dimension reduction and variable selection . Journal of the Royal Statistical Society : Series B ( Statistical Methodology ) , 72(1):3–25 , 2010 .
[ 8 ] N . Craswell and M . Szummer . Random walks on the click graph . In SIGIR , pages 239–246 , 2007 . [ 9 ] BD Davison . Toward a unification of text and link analysis . In SIGIR’03 , 2003 . [ 10 ] S . Deerwester , S . Dumais , G . Furnas , T . Landauer , and R . Harshman . Indexing by latent semantic analysis . JASIS ,
[ 11 ] H . Deng , MR Lyu , and I . King . A generalized co hits algorithm and its application to bipartite graphs . In KDD’09 ,
41:391–407 , 1990 .
2009 .
[ 12 ] DR Hardoon and J . Shawe Taylor . Kcca for different level precision in content based image retrieval . In Proceed ings of Third International Workshop on Content Based Multimedia Indexing , IRISA , Rennes , France , 2003 .
[ 13 ] DR Hardoon and J . Shawe Taylor . Sparse canonical correlation analysis . stat , 1050:19 , 2009 . [ 14 ] DR Hardoon , S . Szedmak , and J . Shawe Taylor . Canonical correlation analysis : An overview with application to learning methods . Neural Computation , 16(12):2639–2664 , 2004 .
[ 15 ] K . Jarvelin and J . Kekalainen . Ir evaluation methods for retrieving highly relevant documents . In SIGIR’ 00 , pages
[ 16 ] G . Jeh and J . Widom . Simrank : a measure of structural context similarity . In SIGKDD , pages 538–543 , 2002 . [ 17 ] H . Ma , HX Yang , I . King , and M . R . Lyu . Learning latent semantic relations from clickthrough data for query suggestion . In CIKM , pages 709–718 , 2008 .
[ 18 ] JM Ponte and WB Croft . A language modeling approach to information retrieval . In SIGIR’ 98 , pages 275–281 ,
[ 19 ] S . E . Robertson , S . Walker , S . Jones , M . Hancock Beaulieu , and M . Gatford . Okapi at trec 3 . In TREC , 1994 . [ 20 ] R . Rosipal and N . Kr¨amer . Overview and recent advances in partial least squares . Subspace , Latent Structure and
Feature Selection , pages 34–51 , 2006 .
[ 21 ] S . R¨uping and T . Scheffer . Learning with multiple views . In Proc . ICML Workshop on Learning with Multiple
[ 22 ] H . Saigo , N . Kr¨amer , and K . Tsuda . Partial least squares regression for graph mining . In SIGKDD , pages 578–586 ,
[ 23 ] G . Salton and M . McGill . Introduction to Modern Information Retrieval . McGraw Hill , Inc . , New York , NY , USA ,
41–48 , 2000 .
1998 .
Views , 2005 .
2008 .
1986 .
[ 24 ] PJ Schreier . A unifying discussion of correlation analysis for complex random vectors . Signal Processing , IEEE
[ 25 ] WR Schwartz , A . Kembhavi , D . Harwood , and LS Davis . Human detection using partial least squares analysis .
Transactions on , 56(4):1327–1336 , 2008 .
In ICCV , pages 24–31 , 2009 .
[ 26 ] RD Tobias . An introduction to partial least squares regression . In Proceedings of the Twentieth Annual SAS Users
Group International Conference , pages 1250–1257 , 1995 .
[ 27 ] C . Wang , R . Raina , D . Fong , D . Zhou , J . Han , and G . Badros . Learning relevance from heterogeneous social network and its application in online targeting . In SIGIR’11 , 2011 .
[ 28 ] X . Wang , JT Sun , Z . Chen , and CX Zhai . Latent semantic analysis for multiple type interrelated data objects . In
SIGIR , pages 236–243 , 2006 .
[ 29 ] JA Wegelin . A survey of partial least squares ( pls ) methods , with emphasis on the two block case . Technical
Report , No.371 , Seattle : Department of Statistics , University of Washington , 2000 .
[ 30 ] JR Wen , JY Nie , and HJ Zhang . Query clustering using user logs . ACM Trans . Inf . Syst . , 20(1):59–81 , 2002 . [ 31 ] H . Wold . Path models with latent variables : the nipals approach . Quantitative sociology : International perspectives on mathematical and statistical modeling , pages 307–357 , 1975 .
[ 32 ] W . Wu , J . Xu , H . Li , and O . Satoshi . Learning a robust relevance model for search using kernel methods . JMLR ,
12:1429–1458 , 2011 .
[ 33 ] W . Xi , EA Fox , W . Fan , B . Zhang , Z . Chen , J . Yan , and D . Zhuang . Simfusion : measuring similarity using unified relationship matrix . In SIGIR , pages 130–137 . ACM , 2005 .
[ 34 ] J . Xu , H . Li , and ZL Zhong . Relevance ranking using kernels . In AIRS ’10 , 2010 . [ 35 ] JF Xu and G . Xu . Learning similarity function for rare queries . In WSDM , pages 615–624 , 2011 . [ 36 ] CX Zhai and J . Lafferty . A study of smoothing methods for language models applied to information retrieval .
ACM Trans . Inf . Syst . , 22(2):179–214 , 2004 .
[ 37 ] Z . Zhang and O . Nasraoui . Mining search engine query logs for query recommendation . In WWW , pages 1039–
1040 , 2006 .
19
We show the proof of Theorem 4.1 here .
A . Proof of Theorem 4.1 ki Proof . Suppose that LQi = MilQi re written as j=1lDi ki j=1 lDi j . Since Mi = lDi
, MilQi
= j j j
. Objective function ( 3 ) can be wui λi w lDi j j w lQi , vi w , lQi j | + w − λi ( λi ki)|ui w , lDi j ||vi j | w , lQi w , lDi j | + w}p w=1 are orthonormal , we have j ||vi j | w , lQi j
, ki w=1 w=1 λi
1 , . . . , lQi lQi p p j = lDi , MilQi p w|ui λi p p p w − λi ( λi j ||vi j ||vi ki)|ui w , lDi w , lDi
|ui w=1 w=1 w=ki+1 j
+
= λi ki
λi ki w=1
|ui j ||vi w , lQi w , lQi ( w}p w=1 and {vi p ki w=1 w=1(λi
+ j | w , lQi j λi ki ki
1 , . . . , lDi lDi p w=1
, and LDi = wvi wui wui λi j | w , lQi w lQi wvi w , we have j = ki w , lDi j ||vi w=1 j | w , lQi ki ui w , lDi ki w=1 w=1
)|ui ki)|ui w − λi ( λi p j 2)( w − λi ki vi j 2 ) w , lQi j ||vi w , lDi ki ki)(1 − ki w − λi ( λi ki j 2)( w − λi ( λi vi w , lQi j 2 )
|ui ki)( w=1 j=1 j=1 ki w=1 ui w , lDi j=1
 1
2
 1
2 j ( ki j=1
, MilQi j kiλi ki + w , lDi j ||vi w , lQi j | ) . lDi j
, MilQi
|ui w , lDi j ||vi w , lQi j | ) .
Since ||lQi j
|| = ||lDi
|| = 1 and {ui j
Thus , we know lDi both sides , we obtain j p w=1
|ui w , lDi j ||vi , MilQi ki w − ki lDi
λi j=1 j ki w=1 j=1
From this inequality , we obtain
||lQi j
|| · ||lDi j
|| = 1 . w , lQi j | . By taking summation on
Since L Qi
LQi = Iki×ki and L Di
LDi = Iki×ki , we have ki j=1 ki j=1lDi j
|ui w , lDi w , lQi j | j ||vi j ki
, MilQi
Particularly , letting lDi w=1 λi w . j and lQi j = ui
Thus ,
||ui w|| · ||vi w|| = 1 . j = vi j , we can obtain the global maximum .
20
Table 6 : Examples of similar queries found by M PLSCom
Original Query
Similar Queries
Spelling Errors dictonary wickapedia gooole.com fl.lottery ym bofa online banking dickssportinggoods googlenews peoplesearch star wars anniversary edition lego darth vader fighter american express online account summary read full books online free web dictionary onlinedictionary dictionery wwwwikipediaorg wikepedia wwwwikipediacom wwwwgooglecom www.gooogle wwwgoolglecom
Abbreviations lottery florida florida lottery numbers florida lottery results yahoo messanger yahoomessenger yahoo im messenger bank america online banking bank of america online www.bank of america online banking
Concatenations dicks sporting goods dicks sporting good store dicks sporting goods coupons google news goggle news newsgooglecom search people people search yahoo people search
Tail Queries www.star wars legos.com star wars legos star wars lego american express account online american express account american express online read books online free free online books to read read books online
21
Table 7 : Result on a difficult query
Query : walmartmoneycard
M PLSCom
CosB , LSI , RW
CosW
CosB + CosW , LSI + CosW , RW+ CosW wwwwalmartmoneycardcom walmartmoneycard.com wal mart prepaid visa activation
N / A N / A N / A wwwwalmartmoneycardcom walmartmoneycard.com
N / A wwwwalmartmoneycardcom walmartmoneycard.com
N / A
22
