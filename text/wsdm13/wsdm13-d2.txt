TYPiMatch : Type specific Unsupervised Learning of Keys and Key Values for Heterogeneous Web Data Integration
Yongtao Ma Institute AIFB
Karlsruhe Institute of Technology
76128 Karlsruhe , Germany yongtaoma@kitedu
Thanh Tran Institute AIFB
Karlsruhe Institute of Technology
76128 Karlsruhe , Germany ductran@kitedu
ABSTRACT Instance matching and blocking , a preprocessing step used for selecting candidate matches , require determining the most representative attributes of instances called keys , based on which similarities between instances are computed . We show that for the problem of learning blocking keys and key values , both generic techniques that do not exploit type information and supervised learning techniques optimized for one single predefined type of instances do not perform well on heterogeneous Web data capturing instances for which the predefined type is too general . That is , they actually belong to some subtypes that are not explicitly specified in the data . We propose an unsupervised approach for learning these subtypes and the subtype specific blocking keys and key values . Compared to state of the art supervised and unsupervised learning approaches that are optimized for one single type , our approach improves efficiency as well as result quality . In particular , we show that the proposed strategy of learning subtype specific blocking keys and key values improves both blocking and instance matching results .
Categories and Subject Descriptors H28 [ Database Management ] : Database Application— Data Mining
Keywords Information Integration ; Heterogeneous Web Data ; Unsupervised Learning
1 .
INTRODUCTION
Especially due to recent trends in Web data publishing , there is a large amount of heterogeneous datasets that are publicly available . Linked Data alone comprises billions of RDF triples that reside in hundreds of datasets [ 4 ] .
One main problem towards the effective usage of Web data is integration . Besides schema matching , the other main problem in this regard is to resolve differences in the data representation of the same real world object . It is called instance matching ( also known as entity resolution , entity co reference , or record linkage ) , which is about finding instances ( eg Products ) that refer to the same object . One main challenge behind this is to find a few representative attributes ( eg title and price ) , based on which the similarity between instances can be computed . These attributes and their values ( so called keys and key values ) are not only needed for blocking , which uses a few simple blocking keys to quickly determine candidates and group them together in blocks , but also for the instance matching task in general . Blocking is usually performed as a quick candidate selection step that involves boolean matching , while instance matching employs fuzzy similarity matching with fine tuned similarity thresholds to further refine the candidates in the blocks .
State of the art approaches for finding blocking keys assume a given schema , which describes the types and attributes associated with instances of these types . Given a particular type , its attributes , and training data for that type , existing approaches derive type specific keys or more complex similarity matching function predicates that capture the keys as well as the similarity metrics and thresholds [ 5 , 12 , 3 , 22 , 21 ] . Not relying on training data , an unsupervised approach has been proposed recently to select attributes as keys based on their discriminability and coverage [ 18 ] . Basically , the discriminability of an attribute is measured by the diversity of its values such that lowdiscriminability means that many instances have the same values on that attribute . The coverage of an attribute refers to the number of instances that have that attribute . Intuitively , attributes shall be selected and used as keys when they are covered by many instances and also , discriminate them well . Finding type specific keys in this supervised or unsupervised fashion is however problematic when the given instances actually belong to multiple unknown subtypes , as illustrated by the following example :
Example 1 . One example of data that lacks type information is Linked Data as shown in Fig 1 . Based on a 3M sample of instances extracted from several Linked Data sources crawled from the Web , we found that 393,503 of them lack type information , eg it is not captured in the data that p1 is a Camera ( dotted edges indicate the type information that is not captured ) . Another example is data stored in big tables of enterprise databases . Tab . 1 shows real product data from a data integration project of a product
325 Table 1 : Product table ; some words later used as features for illustration purposes , are highlighted .
ID Title p1
Sony Cyber Shot DSC W650
Price Brand Description 129
Sony p2 p3 p4 p5 p6
Sony DSC W620 Cyber Shot
112
Sony
Sony Cyber Shot DSC W650 camera owns14 megapixel sensor Optical Steady Shot Image stabilization reduces blur , whilephotos Effect modes allow you to get more creative with your photos . Capture true to life photos with the Sony Cyber shot DSC W620 digital camera . Boasting a 5x zoom , this 14.1 megapixel digital camera captures the most detailed photos
Sony Cyber Shot DSC W65
Sony PRS 600SC Sony PRS 600SC
Reader
Reader
Sony PRS T1 6 ” eBook Reader
138
118
93
129
Sony
Sony Get high quality images with the Sony Cyber shot DSC W650 digital camera . Boasting a 5x optical zoom , this 16.1 megapixel digital camera produces detailed photos . The Sony PRS 600E Book Reader allows readers to store 350 books from anywhere . Using e link technology for a paper like display The Reader Touch Edition utilizes E Ink screen technology to deliver an amazing , paper like displayTake your favorite books to go with the Reader Touch Edition by Sony . The Sony reader equipped E Ink Pearl V220 glare free screen Books from your local public library or download Google public domain books .
Sony
Sony management company1 . Here , we face the situation where 7M descriptions acquired from different sources are stored in one single product table . The products captured by this data actually belong to many different subtypes . Using four attributes , this table captures instances of type Product , which actually belong to subtypes such as Camera and eReader .
This example illustrates the cases where the type is either not specified or too general to be useful . The current supervised solution [ 21 , 22 , 3 , 12 , 5 ] to that is to treat instances as belonging to one single type and learn blocking keys that are applicable to all of them , eg keys for all Product instances . Intuitively , when dealing with instances of the type Camera ( p1 , p2 , and p3 ) , keys specific to Camera might be more useful ( eg Model ) than keys learned for Product ( eg Title ) . However , when learned for a single big Product table , such camera specific keys might not be selected because they are not applicable to many table records .
As an alternative to these schema based type specific supervised learning solutions , a schema and type agnostic approach has been recently proposed to deal with heterogeneous Web data . This approach does not exploit attributes ( of specific types ) for building keys , but instead , uses unstructured bags of features that can be extracted from the attribute values as key values [ 14 ] . They are simply composed of features that do not come with attribute information . Instances are considered similar when their key values overlap , ie they have some features in common .
Contributions . In this work , we show that considering the schema and the attributes it captures significantly improves the quality of the results achievable with the schemaagnostic approach [ 14 ] . In particular , the quality improves when instead of using the general type [ 19 , 20 , 5 ] , blocking keys are learned for specific subtypes of instances in the data . To this end , we propose an unsupervised approach that relies entirely on the data for learning both the subtypes , and the keys and key values specific to these subtypes . In experiments , we show how this approach can be used for blocking and instance matching . Compared to state of the art instance matching approaches , our solution greatly improves result quality ( up to 201.56 % improvement in terms of Fmeasure ) . Results are also promising when considering the 1http://wwwrepositocom/ blocking task only . Our approach yields up to 32.62 % improvement in terms of reduction ratio over existing solutions for blocking [ 19 ] . It is also time efficient when compared against approaches that achieve similar result quality . Compared to the approach that yields second best result quality , our approach is up to several times faster wrt blocking , and achieves similar performance wrt instance matching . Outline . We provide an overview of the problem and solutions in Sec 2 . The learning of types is presented in Sec 3 . The learning of keys and key values specific to these types is discussed in Sec 4 . We present experiments in Sec 5 , related work in Sec 6 , and conclusions in Sec 7 .
2 . OVERVIEW
We consider general Web data , which includes different types such as relational , XML and RDF data . In literatures , this data is often conceived as a graph :
Definition 1 . The data G is a tuple ( N , E , L ) where • N is a finite set of nodes . Thereby , N is conceived as the disjoint union NI NC NV with NI representing instances , NC stands for classes , and NV stands for attribute values .
• L is a finite set of labels , subdivided by L = LN LE type , where LN are node labels and LE type are edge labels .
• E is a finite set of edges e(n1 , n2 ) with n1 , n2 ∈ N and e ∈ L . In particular , we distinguish attribute edges EA ⊂ E , with e(n1 , n2 ) ∈ EA iff n1 ∈ NE and n2 ∈ NV from type edges , type(n1 , n2 ) ∈ E , n1 ∈ NI and n2 ∈ NC .
An instance description of ni refers to the set of attributevalue pairs E(ni ) = {e(ni , nj ) ∈ E} associated with ni .
This graph structured model resembles the RDF model , where classes correspond to RDF classes , instances are RDF resources , and attribute values are RDF literals . A record in a relational database captures an instance and its attribute values . For clarity , we omit relationships , eg relations between RDF resources and foreign key relationships between
326 Figure 1 : A data graph . records , because they are not used for instance matching here . The graph for our data example is shown in Fig 1 .
Problem . The overall problem in this work is instance matching : given a set of instances and their descriptions , the goal is to detect those that refer to the same real world object . Existing solutions can be characterized through the tuple ( ∼I , σ ) , where ∼I : NI × NI → R+ maps a pair of instances to a similarity value , and they are considered as being the same when that similarity value is higher than the threshold σ . Typically , this instance similarity ∼I is defined based on the similarity of the attribute values , ∼V : NV × NV → R+ . Effective instance matching involves determining ( 1 ) ( combinations of ) attributes ( Title , Price etc . ) and for each of them , ( 2 ) the weight indicating its importance , ( 3 ) the value representation function , simply referred to as values , ( eg selecting only the most important words in the values of Title and using only the first 3 tokens of each word ) , ( 4 ) the similarity measures ( eg edit distance and Jaccard similarity ) and ( 5 ) the similarity thresholds [ 21 , 22 , 3 , 12 , 5 ] .
Often performed as a candidate selection step , blocking is typically limited to finding attributes also called blocking keys ( task 1 ) and their values ( task 3 ) [ 12 , 2 , 24 , 14 ] . They are used without weights and thresholds . The similarity is determined through a predefined boolean function predicate ( eg exact value matching or value overlap ) . Previous works [ 14 , 18 ] use the best ranked attribute [ 18 ] as key and all features from its values as key values ( schemabased ) , or the values of attributes combined [ 14 ] ( omitting attribute information , hence called schema agnostic ) . Then , instances are considered as matches and placed into one block when there is an overlap in their value representation ( ie value overlap is the similarity metric ) . Resembling the blocking problem , we focus on the tasks 1 and 3 in this instance matching context , ie finding attributes and their their value representation :
( Find Blocking Keys and Key Values ) .
V × N e
V → {true , f alse} , N e
Definition 2 function predicates ( called blocking scheme ) ( cid:86 )
Given the data G(N , E , L ) , we find a conjunction of boolean e∈L∗ ∼e V ⊆ NV ⊂ N where ∼e denotes the values of some keys e ∈ L∗ , and L∗ ⊂ L is the set of blocking keys . Blocking maps every instance ni ∈ NI to a subset NBi ⊆ NI ⊂ N , an equivalence class of instances ( instances , that according to the blocking scheme , are equivalent to ni ) called block : NBi = [ ni ] = {nj ∈ NI :
V
V : N e ni , nj ∈ NI ,(cid:86 ) e∈L∗ ∼e
V ( ni , nj ) = true} .
Example 2 . Tab . 2 shows two blocks obtained using Title as key and words in Title as key values . Note instances in the same block overlap on the word Sony and Reader . In addition , our approach recognizes that instances belong to the two subtypes Camera and eReader .
Our Solution . We consider the case where the data might contain instances belonging to multiple unknown subtypes .
Table 2 : Example blocks for different subtypes .
Block block1 block2
ID p1 p2 p3 p4 p5 p6
Title
Sony Cyber Shot DSC W650 Sony DSC W620 Cyber Shot Sony Cyber Shot DSC W650
Sony Reader PRS 600SC Sony Reader PRS 600SC
Sony PRS T1 6 ” eBook Reader
Subtype Camera Camera Camera eReader eReader eReader
Our solution to that is based on the following main ideas : we observe that instances belonging to the same type share the same attributes in the schema , eg instances of the type Product have Title and Price as attributes . However , as noted in the example , these attributes we call schema features , are too few to distinguish the different subtypes of instances that exist in our Product example . Yet , following this intuition , we can also observe that besides schema features , instances of the same type also have some features in their values in common . For instance , instances of the type Camera commonly have Cyber Shot and camera in the Description . We infer features in values as so called pseudo schema features when they commonly co occur in instance descriptions . From these features and the instances in which they co occur , we then infer different subtypes . Intuitively , a subtype is a set of instances that share the same set of pseudo schema features . Both pseudo schema features and subtypes learned from the data are then finally exploited for blocking and instance matching : representative attributes are learned for specific subtypes , and as pseudoschema features characterize a set instead of individual instances , they are excluded from the value representation .
We compare our approach with existing solutions for blocking [ 19 , 14 ] . Subtype specific schemes are used in our solution . This represents a major depart from existing blocking and instance matching techniques . Several schemes might be used for blocking ( ie multiple run blocking [ 5] ) . However , they are always applied to all instances . Instance matching is applied separately to candidates within blocks for reason of efficiency . However , we are not aware of any existing instance matching work , which uses different instance matching strategies for instances of different blocks . We use different strategies concerning tasks 1 and 3 for different subtypes to recognize that for improved result quality , their instances have different characteristics , and thus deserve different treatments . Further , we note that while also representing a set of instances , blocks are not the same as subtypes used in this work . Instances in the same blocks refer to the same real world object , hence share object specific features . Instances belonging to the same subtype refer to the same class , thus might have different object specific features but share class specific features ( ie pseudo schema features ) .
To form a blocking scheme , only the best ranked attribute determined for every subtype is used as key and non pseudoschema features are extracted from its value as key values . For finding blocks using such a scheme , there are two strategies : one is to iterate through the instances , as proposed recently [ 18 ] ; using the features in the key value of a given instance , candidates are retrieved for that instance ( instancebased ) ; the alternative is to iterate through all features that appear in the instances’ key values , and to retrieve all the
Titlep4Price118Sony Reader Digital Book PRS 600SCTypeScreene Inkp6TypeTitleony PRS T1 Pearl Red 6" eBook ReaderPrice129e InkScreeneReaderProductCameraModelp1TitleSony Cyber Shot DSC W650Price129sensor14 mega'pi'x'e'sW650 TypeTypeBrandSonyPrice13814 mega'pi'x'e'ssensorSonyBrandSonyBrandSonyBrandSony DSC W620 Cyber ShotTitleW620Model327 candidates for a given feature ( feature based ) . For our approach , we use the feature based strategy and apply value overlap as similarity measure .
Additionally , we show that the subtype specific attributes and key values are also useful for instance matching . Similar to blocking , different attributes are values are used for different subtypes . Concerning tasks 2 , 4 and 5 , we assign uniform weights to attributes ( ie no weighting ) and use Jaccard similarity as measure where the threshold is learned using an existing technique [ 23 ] .
3 . LEARNING PSEUDO SCHEMA FEA
TURES AND THEIR TYPES
The idea of pseudo schema features and their use for learning the type semantics of structured data has been presented in our previous work [ 11 ] . The focus of that work , called TYPifier , lies in the clustering techniques that can be used for computing a hierarchy of types . In this work , we apply the technique used by TYPifier for identifying pseudoschema features . However , we note that the hierarchy of types learned by TYPifier is not suitable . Note only does the learning of hierarchy requires more computational effort , it also yields types too fine grain that have only low coverage of the instances . In this section , we firstly revisit the technique used to identify pseudo schema features , and then introduce the technique used by TYPimatch to learn a set of types , as opposed to a hierarchy of types . 3.1 Learning Pseudo schema Features
We observe that many values such as Cyber Shot and camera appear in the descriptions of several instances . We aim to find those values , which commonly appear in descriptions of instances that belong to the same type . We call these values pseudo schema features because just like schema attributes such as Title and Price , the are specific to one particular type . For identifying these features , we exploit two intuitions : first , candidate pseudo schema features are those , which appear in a large number of instance descriptions ; those which co occur in a large number of instance descriptions , form a specific type . We capture co occurrences through the notion of conditional co occurrence probability ( CCP ) , and groups of co occurring features through the feature co occurrence graph ( FCG ) .
Recall we define the instance description E(n ) as a set of attribute value pairs . From the label of a value node , words can be extracted such that every value can be conceived as a bag of words . E(n ) contains a feature f when it has a bag of words ( a value ) that contains f ( we use f ∈ E(n ) to denote this containment ) . Then , we say a feature fi cooccurs with another fj when there is an entity description E(n ) that contains both . Through counting co occurrences of features , we compute the CCP as follows :
Definition 3
The probability that fi and fj co occur , given fj , p(fi|fj ) = p(fi , fj)/p(fj ) = count(fi , fj)/|N fj I = {n ∈ NI N fj having fj as feature , and count(fi , fj ) = |N fi co occurrence count of fi and fj .
( Conditional Co Occ . Probability ) . is I | , where : fj ∈ E(n)} denotes all instances I | , the
I ∩ N fj
This co occurrence relation between features is used to construct the FCG :
Definition 4
( Feature Co Occurrence Graph ) . features V , vocabulary of all
Given the feature co occurrence graph is a labeled directed graph G = ( N , E , L ) , with nodes n ∈ N stand for features in V and edges w(ni , nj ) ∈ E capture the co occurrence between the two features fi and fj , where edge labels L stand for the conditional co occurrence probabilities , w = p(fi|fj ) . the
Example 3 . The FCG according to the example in Tab . 1 is shown in Fig 2 . For example , because the features E Ink and Sony co occur , there are the corresponding edges w(E Ink , Sony ) and w(Sony , E Ink ) in the graph . Note count(E Ink , Sony ) = |{p4 , p5 , p6}| = 3 and there are six instances that have Sony as feature . Thus , we have 0.5(E Ink , Sony ) because p(E Ink|Sony ) = Sony count(E Ink , Sony)/N In the same way , we I obtain 1.00(Sony , E Ink ) ( and other edges and weights not shown in the figure ) .
= 05
Figure 2 : FCG for data in Table 1 .
We propose to identify features that are characteristic for sets of instances by searching for maximal cliques in the FCG , ie clusters of features that pairwise , highly co occur as measured in terms of CCP . The computation of pseudoschema features is performed in two steps , namely computing G from G ( F CG ) and extracting maximal cliques from G ( M axCliques ) as shown in Alg . 1 . For computing G , Alg . 1 firstly calculates the probabilities p(fi|fj ) , which requires a maximum of |NI|×|NI| steps ( simple counting ) for computing the co occurrences of fi and fj ( because |NI| is the maximum number of instances that can have fi or fj ) . This has to be performed for all possible feature pairs , a number bounded by |V |×|V | . An undirected edge θ(fi , fj ) is created for G only when both p(fi|fj ) and p(fj|fi ) are greater then the given parameter θ . This parameter is used to control the desired amount of pseudo schema features generated by this procedure . Note that this is not the same as using the joint probability p(fi , fj ) , which is often high when either p(fi ) or p(fj ) is high . This twoways conditional probability gives a smoother estimate of co occurrence .
Then , an existing maximal cliques algorithm [ 13 ] is applied to this undirected version of the FCG . While MaxCliques is known to be NP hard , optimized algorithms such as the one we apply can provide fast runtime performance . The amount of edges in the FCG is bounded by |V |2 . The size of V is reduced through text processing techniques such as stop words elimination . Further , a large amount of feature pairs is pruned during the construction of the FCG , especially when θ is high .
Finally , only features corresponding to nodes contained in the resulting maximal cliques are kept as pseudo schema features , the set of nodes F∗
P S .
ReaderE InkbookscameramegapixelCyber ShotphotosSony050100328 Algorithm 1 : Computing maximal feature cliques
Input : G(N , E , L ) , V . Data : Undirected FCG G(N , E , L ) . Result : F∗ foreach fi ∈ V do
P S . foreach fj ∈ V do p(fi|fj ) := count(fi , fj)/|N fj I | ; p(fj|fi ) := count(fi , fj)/|N fi I | ; //Construct undirected edges . ; if p(fi|fj ) ≥ θ ∧ p(fj|fi ) ≥ θ then
G := G ∪ θ(fi , fj ) ;
//Extract features from maximal cliques . F∗ P S :=Features ( MaxCliques ( G’) ) ; return F∗
P S ;
3.2 Learning Types
Every maximal clique represents a cluster of features ( ie a set of features ) that are specific to a particular set of instances . We observe that these clusters are too fine grained , hence do not directly correspond to the types given in the data . We propose to merge them to form larger clusters capturing types .
Previously , we use feature co occurrence to determine features that are characteristic for sets of instances . To merge clusters , we leverage a similar intuition : we use pairwise cluster co occurrence , ie feature sets co occurrence , to determine the distance between clusters . Then , we merge those that frequently co occur , ie are close to each other in terms of this distance metric :
Definition 5
( Cluster Distance ) . Let count(fi , fj ) I | be the count be the co occurrence count of fi and fj and |N f of instances having f as feature , the distance d(Fi , Fj ) between the set of features Fi and Fj is the conditional probability of co occurrence of features in Fi and Fj , d(Fi , Fj ) = p(Fi|Fj ) = fi∈Fi,fj∈Fj f∈Fj count(fi , fj ) |N f I |
|Ni ∩ Nj|
|Nj|
,
= where Ni ( Nj ) denotes instances that have at least one element in Fi ( Fj ) as feature in their description , ie for every ni ∈ Ni ( nj ∈ Nj ) there exists one fi ∈ Fi ( fj ∈ Fj ) , where fi ∈ E(ni ) ( fj ∈ E(nj) ) .
Two clusters Fi and Fj are merged when they highly cooccur . Equivalently , this co occurrence means there is a strong overlap between the instances Ni that have some element in Fi and the instances Nj that have some element in Fj as features . Using this distance metric , we merge two clusters Fi and Fj when d(Fi , Fj ) > and d(Fj , Fi ) > , where is a parameter used to control the granularity respectively the number of clusters . Clusters resulting from this merging are used as types . In particular , given the resulting cluster Fi , the set of instances that belong to the type captured by Fi is simply Ni , ie instances that have some elements in Fi as features .
We note that just like TYPifier [ 11 ] , the learning of types as proposed above also makes use of pseudo schema features . However , TYPifier requires complex hierarchical relations between and operators on clusters to perform hierarchical clustering . The method proposed above only uses the distance metric and applies cluster merging as the only operator . When doing hierarchical clustering , not only the features but also the instances representing the clusters have to be processed . For this , features and instances not only have to be merged but also might be added or spitted during the hierarchy construction process . Here , we only need to compute a set of clusters , where the computation stops after merging the clusters represented by the features . Only then , instances have to be considered , ie we simply group instances together that are associated with pseudo schema features representing the same cluster .
We will now learn keys for a specific cluster and use them for blocking instances that belong to that cluster .
4 . LEARNING KEYS AND VALUES
The keys are selected based on their ability to discriminate instances , measured in terms of a notion we call instance similarity , while the value representation excludes the pseudo schema features computed before because they characterize a set instead of individual instances .
4.1 Blocking Key Selection
Intuitively , attributes that are more useful in discriminating instances shall be selected as keys . Because keys capture similarity ( thus are called similarity attributes ) , we introduce a notion of instance similarity that distinguishes attributes by their ability to discriminate instances , measured by the number of similarity evidences they provide :
Definition 6
( Attr. specific Instance Similarity ) . Given G(N , E , L ) and the similarity relations ∼I and ∼V on the instances NI ⊆ N and attribute values NV ⊆ N , the instance similarity of an attribute e is respectively , measured as the number of that are similar , given they all have a similar value for e , ie s(e ) = |{n : n , m ∈ NI , m ∼I n , v(e , m ) ∼V v(e , n)}| , where v(e , x ) refers to the value of the attribute e of the instance x . instances in NI
Note that this notion reflects the intuition behind existing learning based approaches [ 21 , 22 , 3 , 12 , 5 ] , which aim to learn keys that maximally separate positive examples from negative ones , ie keys that lead to a large amount of correct matches ( high s(e ) ) when applied against training data . However , obtaining representative examples ( for unknown types ) is difficult . Thus , we propose to estimate instance similarity based on the dependencies among attribute values that can be observed in the data :
Definition 7
( Dependency ) . The strength of dependency of an attribute ei from an attribute ej , denoted d(ei , ej ) , is inverse to the amount of information carried by values of ei , given the values of ej are known . Given G = ( N , E , L ) , d(ei , ej ) is measured as the entropy of the V = {ny|ei(nx , ny ) ∈ E} , conditional on valvalues of ei , N ei
329 V = {ny|ej(nx , ny ) ∈ E} : ues of ej , N ej d(ei , ej ) = ( 1 + H(N ei
−1
1 + 1 +
V |N ej
V ) ) nj∈N ej V ni∈N
V ,nj∈N ei ej V
=
=
−1
( 1 )
−1
( 2 ) p(nj)H(N ei
V |N ej
V = nj ) p(ni , nj)log p(ni ) p(ni , nj )
Example 4 . For the attributes given in Tab . 1 , we obtain d(Brand , T itle ) = 1 and d(T itle , Brand ) = 043 It can be easily observed in the data that given Title , it is easier to predict the value for Brand , while given Brand there are many products with different values for Title .
When an attribute ej is strongly dependent on an attribute ei ( d(ej , ei ) is high ) , it means that if instances have the same value for ei then the probability they also have the same value for ej is high . In particular , the conditional entropy used above implies that for any two instances ni and nj , d(ex , ei ) > d(ex , ej ) means the probability ni and nj have the same value for ex when they have the same value for ei , is higher than the probability ni and nj have the same value for ex when they have the same value ie p ( v(ni , ex ) = v(nj , ex)|v(ni , ei ) = v(nj , ei ) ) > for ej , p ( v(ni , ex ) = ( nj , ex)|v(ni , ej ) = v(nj , ej) ) .
We note this notion of dependency correlates with instance similarity : namely , the more other attributes are dependent on e , the higher the amount of evidences e can provide ( higher s(e ) ) and hence , the less the amount of additional evidences the other attributes can contribute . We exploit this correlation to estimate s(e ) as follows :
Proposition 1 . Let Agg : 2
R+ → R+ be a monotonic aggregation function and LF E be the set of attributes associated with the type F . For two attributes ei , ej ∈ LF E , ( d(e , ej ) ) then we have if Agge∈LF s(ei ) ≥ s(ej ) .
( d(e , ei ) ) ≥ Agge∈LF
E
E
Alg . 2 computes the instance similarity for every attribute in LF E that is associated with a given type F , and stores it in S . Selecting the set of blocking keys L∗ is simply done by sorting S and keeping the top ones with highest instance similarity . In this work , we simply use the top 1 attribute as the blocking key , ie L∗ consists of exactly one attribute . First , Alg . 2 computes d(ei , ej ) for every pair of attributes to construct the matrix M . Every combination of values for ei and ej has to be considered to derive the probabilities p(ni ) and p(ni , nj ) . At most , these steps of probability computation have to be performed |N emax | times ( emax denotes the attribute in the data that is associated with the largest number of values ) . We choose the sum of dependencies as an aggregation function for selecting similarity attributes . Then , instance similarities stored in S can be derived from the sums of row values in M . Since probabilities as well as the sums of dependency values in M can be computed in time linear to the size of G , we have this as overall complexity :
|×|N emax
V
V
Theorem 1 . Given G = ( N , E , L ) , S can be computed in
O((|N emax
| × |N emax
|)|LF
E|×|LF
E| ) .
V
V
Algorithm 2 : Computing instance similarities E| matrix M .
E| × |LF
Input : G(N , E , L ) . V , N ej Data : H(N ei Result : S . foreach ei ∈ LF
V ),|LF E ⊂ L do foreach ni ∈ N ei foreach ej ∈ LF
E ⊂ L do foreach nj ∈ N ej do
V = {ny|ei(nx , ny ) ∈ E} do V = {ny|ej(nx , ny ) ∈ E}
H(N ei H(N ei
V , N ej V , N ej
V ) = V ) + p(ni , nj)log p(ni ) p(ni,nj ) ;
Mij = ( 1 + H(i , j))−1 ; foreach e ∈ LF
Se = s(e ) =
E ⊂ L do 1≤i≤|LF
E| Mei ; return S ;
Thus , when excluding the effect of maximal clique finding , which operates against the relatively small graph of attributes , both the computation of pseudo schema features ( and the types based on them ) and the selection of blocking key can be performed in polynomial time , bounded by the maximum number of instances and values associated with an attribute , and the amount of attributes . In practice , these bounds can be reduced substantially : not all attributes are relevant for blocking . For instance , strategies for ranking and filtering attributes ( eg attributes too general or too type specific to be useful for blocking ) can be applied . Further , we note that just like other learning algorithms , a smaller amount of representative samples may be chosen instead of using all instances and values . Even without these strategies , the proposed algorithms scale well to large datasets in our experiment . 4.2 Key Value Selection
The idea behind selecting keys also applies for the selection of key values : they should help to discriminate instances . Pseudo schema features characterize a set of instances well but are less useful in discriminating individual instances . As a result , while they help to identify types , they are less useful in identifying blocks of similar instances , as illustrated by the following example :
Example 5 . There are two Product instances with the Title “ Sony Cyber Shot DSC W650 ” and “ Sony DSC W620 Cyber Shot ” . When using all the words in Title as key values , these two instances exhibit an overlap and thus , would be placed in one block . However , after excluding pseudoschema features such as Sony and Cyber Shot , features that remain such as DSC W650 and DSC W620 are more specific to these two instances ( rather than the class they belong to ) .
Thus , pseudo schema features associated with a type are excluded from the value representation of keys that have been selected for that type .
In summary , the output of the two steps discussed before is a blocking scheme for every subtype F , which consists the attribute as key that has highest instance similarity . The key value representation contains all features from its value except those that are in the set of pseudo schema features .
330 Table 3 : For each dataset pair : number of instances , attributes , words that appear in attribute values , pseudo schema features ( PS ) , and mappings indicating two instances are same ( ground truth , GT ) .
Task
AB DS
Instances
Dataset1 Dataset2
Words PS
GT
1,081 2,616
1,092 64,263
7,040 89,189
163 386
1,097 5,347
5 . EXPERIMENTAL EVALUATION
Our work mainly addresses scenarios where type information is missing . To study the proposed solution , we employ a recent instance matching benchmark [ 10 ] that captures data from enterprise databases . We show that recognizing subtypes and learning type specific attributes and values improve both efficiency and the quality of results . Compared against the best blocking baseline , our approach is up to several times faster and improves result quality measured in terms of reduction ratio ( RR ) by up to 32.62 % , while maintaining similar results for pair completeness ( PC ) . Considering the instance matching task , our solution , when used in combination with standard solutions for other tasks , improves F measure result by up to 20156 %
5.1 Datasets and Matching Tasks
We now briefly describe the datasets and matching tasks in the benchmark [ 10 ] that are used here . They cover the bibliographic and e commerce domains . Tab . 3 provides an overview of these datasets .
DBLP Scholar ( DS ) . These datasets include data from DBLP and Google Scholar . The attributes and values in Google Scholar are automatically extracted from full text documents , hence contain a number of misspellings and heterogeneous representations of authors and venues . Because one instance in DBLP can be mapped to multiple instances in Google Scholar , the number of GT is larger than the number of instances in DBLP .
Abt Buy ( AB ) . This matching task is performed between instances of the product dataset from http://abt.com and http://buycom Although all the products are stored in the same tables , they actually represent different subtypes of products , including computer , cell phone , and wash machine . Compared to the other matching tasks featured by this benchmark , the one captured by this pair of datasets is in fact most representative for our scenario of heterogeneous data with multiple unknown types . 5.2 Experimental Setting
In the experiments , we select the four approaches [ 14 , 19 ,
5 , 20 ] as discussed before as baselines .
Systems . The first two approaches are recent approaches proposed for blocking , while the other twos are state of theart solutions targeting the instance matching task . The first one is the type and schema agnostic approach ( Agnostic ) , which treats instances as unstructured bags of words extracted from attribute values [ 14 ] . The second approach is the unsupervised solution for learning keys based on ranking attributes by discriminability and coverage [ 19 ] ( Unsupervised ) . It requires setting two parameters : α is used to control the discriminability of a key and β determines if a key should be removed . We swept over parameters and used the optimal configurations with α = 0.9 and
β = 05 For instance matching , we use a supervised learning approach , which infers the attributes , similarity thresholds etc . from positive and negative examples [ 5 ] ( Supervised ) . We use 10 % of the ground truth mappings as positive examples , and negative examples are generated by combining instances in positive examples [ 9 ] . Finally , we use a recent approach , which iteratively cross fertilizes instance matching results with schema matching results [ 20 ] until convergence ( PARIS ) . The maxinum number of iterations is set to 5 , while PARIS always terminated in less than four iterations in the experiments . PARIS ’s implementation is available for download . We use that PARIS Java implementation for the experiment , while the other systems were implemented in Java 5 .
Figure 3 : The Effect of θ on Effectiveness .
Figure 4 : The Effect of on Effectiveness .
We compare these approaches against our solution , called TYPiMatch . As discussed , to solve the task of finding similarity threshold , TYPiMatch applies an existing mechanism that exploits redundancy between thresholds [ 23 ] . Also for TYPiMatch , we swept over parameters to find the optimal configuration such that the results presented in these experiments , represent best performances of the studied approaches . As parameters , TYPiMatch uses θ and to control the amount of pseudo schema features and clusters , respectively . The effect of these parameters are shown in Figs . 3 and 4 . Higher θ results in less pseudo schema features . We observed that as θ increases , precision decreases and while recall decreases in the beginning ( for small values of θ ) , it also increases with larger values for θ . With less pseudoschema features available for pruning key values , instance representations contain more features for classes . Using them to find similarity yields more matches ( high recall ) including incorrect ones ( low precision ) . High prevents clusters from being merged . Thus , as increases , the number of clusters increases . To accommodate the increasing number of clusters that are also more fine grained , TYPiMatch employs a large number of schemes . As a result of this , precision increases while recall decreases . Thus , these two parameters can be used to control the tradeoff between precision and recall . The results presented in the following are for the configuration = 0.07 and θ = 01
Metrics . We use standard metrics proposed for evaluating blocking and instance matching results . Let S be the result generated by blocking or instance matching approaches , M be the matching pairs , ie the ground truth GT , and N the non matching pairs in the dataset , ie all possible pairs of instances but the ones in GT . PC captures the ratio between correct matches as captured in the blocks |S∩M| . RR and all matches in the ground truth , ie P C = |M| measures the relative reduction in the size of the comparison space accomplished by blocking , ie RR = 1 − |S| |M∪N| . Pre
40  45  50  55  60  65  70  75  0.07  0.1  0.2  0.3  0.4  0.5  0.6  Percentage  ( % )  theta  Precision  Recall  F ­‐measure  40  45  50  55  60  65  70  75  0.07  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  Percentage  ( % )  epsilon  Precision  Recall  F ­‐measure  331 |S|
|S∩M| |M| cision ( P ) captures the ratio between correct matches and all generated matches , ie P = . Recall ( R ) is the
|S∩M| same as P C , ie R = . F measure ( F ) considers both P and R , calculated as F = 2P R P +R .
All experiments were run on a server with two Intel Xeon 2.8GHz Dual Core CPUs , using 8GB of main memory , running Linux with kernel version 2618 5.3 Efficiency of Blocking
Table 4 : Performance of learning blocking keys in ms .
TYPiMatch Type TYPiMatch Key Unsupervised
Abt Buy
DBLP Scholar
4,572
42,262
1,041 895 1,679 10,639
1,069 1,192 1,952 9,661
Tab . 4 shows learning times only for TYPiMatch and Supervised because Agnostic simply uses all attributes as keys . Both approaches yield good and fairly comparable performances , requiring only a few seconds to output the keys . We can see that TYPiMatch was slightly faster in learning keys ( TYPiMatch Key ) , which excludes the times needed to infer the subtypes in the data ( TYPiMatch Type ) . Unsupervised iterates through the data graph and considers all instances to calculate the discriminability and coverage for each property . Thus , learning time for Unsupervised increases with the size of the dataset . This is also the case with TYPiMatch . It uses dependencies for key selection . Given an attribute , it inspects values associated with instances to calculate dependencies . However , only values owned by more than one instance are relevant . Thus , processing was faster for datasets that contain less duplicate values . Also , we can see that inferring types could be done efficiently . As shown in the TYPiMatch Type column , this could be done in 4,572 ms for AB , and 42,262 ms for DS .
Table 5 : Performance of blocking in ms . TYPiMatch Agnostic Unsupervised
AB DS
9,635 95,254
27,208 882,626
18,243 491,569
Tab . 4 shows blocking times for all three approaches . Note that blocking performance depends on two tasks : ( 1 ) retrieving candidates from the index and ( 2 ) performing pair wise similarity computation to filter these candidates . As discussed , an instance based [ 18 ] or a feature based strategy can be used for candidate retrieval . While Unsupervised uses the former , the other two approaches employ the latter .
TYPiMatch yields best performance while Agnostic is clearly worst . The latter is slow because it employs all words extracted from all attribute values , whereas the other two approaches only use words extracted from the keys . Compared to the baselines , TYPiMatch benefits from decomposing the dataset into n subtypes . Because blocking is performed separately for every subtype , the number of retrieval operations needed equals the number of distinct features associated with every subtype . The total number of features for all subtypes combined is higher than the number of distinct features for the single general type . Accordingly , the total number of retrieval operations is higher for TYPiMatch . However , the total number of candidates retrieved
Table 6 : Effectiveness of blocking in terms of PC and RR , * indicates statistically significant improvements of TYPiMatch over the best baseline , Unsupervised ( paired t test , p<005 )
TYPiMatch RR PC AB 86.14 98.33 DS
94.17 99.64
Agnostic
PC 85.96 98.43
RR 26.45 93.16
PC
Unsupervised RR 71.01 96.20
87.05 99.93 chg % over second best
PC
1.04* 1.60*
RR
+32.62* +3.58* for all subtype features was lower than the total number of candidates retrieved for the general type features . Therefore in total , TYPiMatch spends less time in computing matches . In fact , this can be seen as a “ blocking within blocking ” strategy , where the data is partitioned into blocks corresponding to subtypes , and subsequently , is further decomposed into more fine grained blocks . However , we already pointed out in Section 2 that subtypes and blocks resulting from standard blocking are different in many aspects . 5.4 Effectiveness of Blocking
Tab . 6 presents total results for blocking effectiveness for the comparison between our solution and the other two unsupervised approaches . TYPiMatch largely outperforms these two in terms of RR and achieves comparable results for PC . Compared to the second best approach in every setting , TYPiMatch results in 1.32 % decrease in PC and and +18.10 % increase in RR on average . Intuitively , instances which refer to the same real world object should belong to the same type . Thus , learning the blocking scheme for every type should be possible without losing PC . However , the scheme obtained for every type is more specific and thus , can improve RR . The results show this is indeed true for the matching tasks in the benchmark , as TYPiMatch yields high quality types that can be used to preserve PC and improve RR . In particular , we note it produces the largest improvement for the AB task . This is because for products captured by these datasets , there exist many different subtypes that exhibit different vocabularies of features . TYPiMatch is able to infer these subtypes and to adopts distinguished words for matching instances with these types . For the DS task , which involves datasets that are well maintained and fairly homogeneous ( contain only instances of type Publication ) , TYPiMatch achieves a small improvement . 5.5 Efficiency of Instance Matching
Table 7 : Performance of learning instance matching schemes and executing them in ms .
Learning
Instance Matching
TYPiMatch
Supervised TYPiMatch
AB
DBLP
2,369 6,281
2,784 5,726
24,527 101,070
Supervised PARIS 18,373 70,479
66,856 229,833
For instance matching , this version of TYPiMatch also includes the mechanism for learning threshold [ 23 ] . The overall time needed by TYPiMatch for learning keys , key values and thresholds and the time needed by Supervised to learn instance matching scheme , which additionally , also captures the weights , are shown in Tab . 7 . These two approaches achieve similar performance . However , Supervised ’s learning process is non deterministic . In our experiments , it often did not finish after several hours . This problem has been discussed before [ 23 ] , which occurs especially when there is a
332 large numbers of attributes that can be used to form the scheme . The results presented for Supervised is an average over five successful runs .
Tab . 7 also shows the performance for instance matching . The process implemented by TYPiMatch for this is similar to the one used for blocking : it consists of feature based retrieval of candidates and then , for matching them , Jaccard similarity as well as the learned thresholds are employed . As discussed , due to the use of types , this processing resembles a “ blocking within instance matching ” strategy . TYPiMatch retrieves candidates for subtype features and match these subtype candidates . We can see this strategy yields better performance when analyzing the differences between TYPiMatch and Supervised . While the learning techniques they employ are different , the resulting scheme employed by both these approaches for matching are of the same type , ie keys , values and thresholds ( and weights ) . Just like TYPiMatch , Supervised applies feature based candidate retrieval and uses such a scheme for matching . However , TYPiMatch needs only half of the time taken by Supervised . This is because TYPiMatch retrieves and matches instances belonging to subtypes ( using different schemes ) , Supervised applies its scheme to all candidates . Even though TYPiMatch requires a higher number of retrieval operations , it yields better overall performance because it reduces the number of pairwise similarity computation . However , we note this strategy employed by TYPiMatch is different to the standard pipeline of computing candidates via blocking and refining candidates using an instance matcher . TYPiMatch does not physically partition the data into different blocks . Using features from a subtype specific scheme , candidates retrieved might belong to that corresponding subtype as well as some other subtypes . Hence , blocks are actually formed online during feature based retrieval and also , they might be overlapping . Moreover , they are more coarse grained , corresponding to classes of entities instead of individual real world objects .
PARIS is the fastest . We observed that this is mainly because PARIS is very aggressive in pruning candidates . Often , candidates are only kept when they have identical attribute values . While this largely increases performance , it leads to low recall when the data contains more noises , as discussed in the following .
5.6 Effectiveness of Instance Matching
Tab . 8 presents results for instance matching effectiveness . TYPiMatch largely outperforms the other two approaches in terms of F measure . We can see from the results that the product matching task AB is more difficult , where Fmeasures values obtained by all approaches are lower than for the other task . This is because the product descriptions contain a large amount of noisy text , while the bibliographic data employed in that benchmark is more structured .
Because Supervised randomly picks positive examples , the schemes it learns vary in different runs . We calculate the average for five runs . We observed that Supervised ’s strategy of learning the threshold from the skyline of negative examples tends to produce low values . As the result , Supervised covers most positive matches and thus , leads to high recall . However , this comes at the cost of very low precision .
The poor performance on the AB matching task suggests that PARIS is not successful with dealing with noises in textual descriptions . PARIS can propagate evidences when the same values at the data or schema level can be found . However , this task involves many products , which refer to the same object , but due to noises in the data , these matching instances greatly vary in their values for name and price . PARIS ’s performance was better for the DS task . As a general observation , PARIS tends to be more aggressive in pruning results compared to the other approaches . This leads to high precision but low recall .
6 . RELATED WORK
Throughout the paper , we have discussed our previous work on TYPifier [ 11 ] and the most related approaches applicable to the problem of learning blocking keys and key values [ 14 , 18 ] as well as more complex instance matching schemes that also include similarity metrics and thresholds [ 23 , 5 , 3 , 12 ] . We pointed out that as opposed to existing solutions , we learn keys and key values that are specific for subtypes derived from the data . Here , we provide a broader overview of solutions for the instance matching problem .
Efficient Instance Matching . For efficiency , there exist several similarity join algorithms for efficient similarity comparison [ 6 , 1 , 16 ] . They can be used to compute candidate matches based on the blocking scheme learned by our approach , where the conjunction of similarity function predicates acts as the join predicate . By filtering candidates , blocking approaches aim to reduce the number of similarity comparisons [ 12 , 2 , 24 , 14 ] . The attributes learned by our approach can be used for instance matching in general and as blocking keys that can be quickly processed to produce candidates , in particular .
Attribute based Matching . For improving the quality of matching results , there are approaches that specifically focus on the matching at the level of fields ( attributes ) . To this category belong the various proposals for measuring similarities , from character based metrics ( eg edit distance ) to token based metrics that consider the rearrangements of words [ 7 ] . A recent approach measures the degrees of matching based on probability estimates [ 20 ] ( and crossfertilizes them by incorporating and propagating evidences at the data and schema level ) . In this work , we simply assume a given similarity metric that can be used for blocking . Supervised Learning . For matching at the level of instances ( records ) , there are supervised learning techniques , which use decision trees to learn combinations of similarity functions , both for effective instance matching [ 21 , 22 ] and for quick blocking [ 12 ] . This class of algorithms performs worst than SVM models [ 3 ] . The quality of instance matching results achieved through SVM models could be further improved by a recent approach that reduces the learning of similarity function predicates to the maximum rectangle problem [ 5 ] . We compare our approach to this learningbased approach [ 5 ] in the experiment .
Semi Supervised Learning . There are also semisupervised techniques based on probabilistic graphical models . In fact , it has been shown that many existing instance matching approaches including the blocking solution here can be specified in terms of Markov Logic formulas and reformulated as a Markov Logic learning problem [ 17 ] . However , this involves learning both the structure ( formulas ) and their weights . The current solution [ 17 ] still requires the logic for matching to be manually specified as Markov Logic formulas .
Unsupervised Learning . Probabilistic graphical mod
333 Table 8 : Effectiveness of instance matching in terms R , P and F ; * indicates statistically significant improvements of TYPiMatch over the best baseline , Supervised for AB , and PARIS for DS ( paired t test , p<005 )
TYPiMatch
Supervised
R
AB 56.70 75.28 DS
P
72.49 76.49
F
63.63 75.88
R
55.79 79.66
P
13.01 5.76
F
21.10 10.74
R
2.55 41.18
PARIS
P
12.44 89.08
F chg % over second best F
P
R
4.24 56.32 +82.81*
+1.63* +457.19* +201.56* +34.73*
14.13* els were also used for unsupervised instance matching . Common to all these approaches is the observation that instance matching can be formulated as a standard classification task , where instances are classified as matching or non matching [ 8 ] . For instance , latent binary variables in a hierarchical graphical model were used to model whether attributes match or not [ 15 ] . While in principle , the problem of learning blocking keys tackled here could be formulated as a classification problem , different learning techniques may be needed due to the large amount of classes . 7 . CONCLUSION
For the problem of matching instances , we provide a solution to solve the subtasks of selecting discriminative attributes and representing their values . This solution is derived for every subtype learned from the data to recognize that in some datasets , instances cannot be assumed to be of one particular type but may actually belong to multiple unknown subtypes . We showed in experiments that our approach of using subtype specific keys and values improved both blocking and instance matching . In experiments , we show how this approach can be used for blocking and instance matching . Compared to state of the art instance matching approaches , our solution greatly improves result quality ( up to 201.56 % improvement in terms of F measure ) . Considering the blocking task , our approach yields 32.62 % improvement in terms of reduction ratio . It is also time efficient when compared against approaches that achieve similar result quality . Currently , we focus on selecting keys and their values . One direction for future work is to apply this subtype specific strategy to other tasks in instance matching such as learning similarity metrics and thresholds . 8 . REFERENCES [ 1 ] A . Arasu , V . Ganti , and R . Kaushik . Efficient exact set similarity joins . In VLDB , pages 918–929 , 2006 .
[ 2 ] M . Bilenko , B . Kamath , and R . J . Mooney . Adaptive blocking : Learning to scale up record linkage . In ICDM , pages 87–96 , 2006 .
[ 3 ] M . Bilenko and R . J . Mooney . Adaptive duplicate detection using learnable string similarity measures . In KDD , pages 39–48 , 2003 .
[ 4 ] C . Bizer , T . Heath , and T . Berners Lee . Linked data the story so far . International Journal of Semantic Web and Information System , 5(3):1–22 , 2009 .
[ 5 ] S . Chaudhuri , B C Chen , V . Ganti , and R . Kaushik .
Example driven design of efficient record matching queries . In VLDB , pages 327–338 , 2007 .
[ 6 ] S . Chaudhuri , V . Ganti , and R . Kaushik . A primitive operator for similarity joins in data cleaning . In ICDE , page 5 , 2006 .
[ 7 ] W . W . Cohen . Integration of heterogeneous databases without common domains using queries based on textual similarity . In SIGMOD Conference , pages 201–212 , 1998 .
[ 8 ] I . Fellegi and A . Sunter . A Theory for Record Linkage .
Journal of the American Statistical Association , 64(328):1183–1210 , 1969 .
[ 9 ] R . Isele and C . Bizer . Learning linkage rules using genetic programming . In OM , 2011 .
[ 10 ] H . K¨opcke , A . Thor , and E . Rahm . Evaluation of entity resolution approaches on real world match problems . PVLDB , 3(1):484–493 , 2010 .
[ 11 ] Y . Ma , T . Tran , and V . Bicer . Typifier : Inferring the type semantics of structured data . In ICDE , to appear , 2013 .
[ 12 ] M . Michelson and C . A . Knoblock . Learning blocking schemes for record linkage . In AAAI , 2006 .
[ 13 ] P . R . J . ¨Osterg˚ard . A fast algorithm for the maximum clique problem . Discrete Applied Mathematics , 120(1 3):197–207 , 2002 .
[ 14 ] G . Papadakis , E . Ioannou , C . Nieder´ee , and
P . Fankhauser . Efficient entity resolution for large heterogeneous information spaces . In WSDM , pages 535–544 , 2011 .
[ 15 ] P . D . Ravikumar and W . W . Cohen . A hierarchical graphical model for record linkage . In UAI , pages 454–461 , 2004 .
[ 16 ] S . Sarawagi and A . Kirpal . Efficient set joins on similarity predicates . In SIGMOD Conference , pages 743–754 , 2004 .
[ 17 ] P . Singla and P . Domingos . Entity resolution with markov logic . In ICDM , pages 572–582 , 2006 .
[ 18 ] D . Song and J . Heflin . Automatically generating data linkages using a domain independent candidate selection approach . In International Semantic Web Conference ( 1 ) , pages 649–664 , 2011 .
[ 19 ] D . Song , J . Heflin , and J . Heflin . Automatically generating data linkages using a domain independent candidate selection approach . In International Semantic Web Conference ( 1 ) , pages 649–664 , 2011 .
[ 20 ] F . M . Suchanek , S . Abiteboul , and P . Senellart . Paris :
Probabilistic alignment of relations , instances , and schema . PVLDB , 5(3):157–168 , 2011 .
[ 21 ] S . Tejada , C . A . Knoblock , and S . Minton . Learning object identification rules for information integration . Information System , 26(8):607–633 , 2001 .
[ 22 ] S . Tejada , C . A . Knoblock , and S . Minton . Learning domain independent string transformation weights for high accuracy object identification . In KDD , pages 350–359 , 2002 .
[ 23 ] J . Wang , G . Li , J . X . Yu , and J . Feng . Entity matching : How similar is similar . PVLDB , 4(10):622–633 , 2011 .
[ 24 ] S . E . Whang , D . Menestrina , G . Koutrika ,
M . Theobald , and H . Garcia Molina . Entity resolution with iterative blocking . In SIGMOD Conference , pages 219–232 , 2009 .
334
