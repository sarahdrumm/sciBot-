Web Usage Mining for Enhancing Search Result Delivery and Helping Users to Find Interesting Web Content
Sapienza , University of Rome
Ida Mele
Rome , Italy mele@disuniroma1it
ABSTRACT Web usage mining is the application of data mining techniques to the data generated by the interactions of users with web servers . This kind of data , stored in server logs , represents a valuable source of information , which can be exploited to optimize the document retrieval task , or to better understand , and thus , satisfy user needs .
Our research focuses on two important issues : improving search engine performance through static caching of search results , and helping users to find interesting web pages by recommending news articles and blog posts .
Concerning the static caching of search results , we present the query covering approach . The general idea is to populate the cache with those documents that contribute to the result pages of a large number of queries , as opposed to caching the top documents of most frequent queries .
For the recommendation of web pages , we present a graphbased approach , which leverages the user browsing logs to identify early adopters . These users discover interesting content before others , and monitoring their activity we can find web pages to recommend .
Categories and Subject Descriptors H33 [ Information Search and Retrieval ] : [ Information filtering][Search process ] ; H28 [ Database Applications ] : [ Data mining ]
General Terms Algorithms ; Experimentation .
Keywords Web usage mining ; Caching ; Recommendation .
1 . MOTIVATIONS FOR THE RESEARCH
The World Wide Web gives an abundance of information to the users , and this makes the identification of relevant content difficult . Web mining tries to address this problem . It consists in the application of machine learning and data mining techniques , which allow the automatic extraction of meaningful patterns and relationships from large collections of web data . Web mining can be divided into three main subareas : ( i ) web content mining , which infers knowledge from the content ( ie text and graphics ) of web pages , ( ii ) web structure mining , which extracts information from data describing the organization of web content , and ( iii ) web usage mining , which captures usage patterns by analyzing data generated from the interactions of the users with the Web . There is not a clear cut distinction among these categories , and all three mining tasks can be combined .
Our research work is strongly related to the field of web usage mining . In particular , we exploit the implicit information given by users , that are stored in query and browsingactivity logs , in order to help web users to find information easily and quickly . The contributions we propose are twofold : First , we face the problem of improving searchengine performance . Second , we tackle the problem of page recommendation .
Improving search engine performance . Optimization of search engine performance is obviously of paramount importance , given that a typical search engine receives a huge number of queries every second , and users expect very low response times . To this end , search engines employ a variety of caching techniques as a means to provide results timely and with minimal reduction in quality . Several works suggest using a static cache of documents . This cache stores results that are relevant to the most frequent queries [ 23 , 12 ] . Although this approach is very simple , it may require a lot of cache space . In our research work , we propose strategies for optimizing the space utilization of the static cache .
Web page recommendation . Search engines help people to search for information on the Internet , but the web search is effective only when the users have a clear idea of what they want . Often , people have no specific information need , for example they surf the Web to read news , interesting blog posts , etc . Recommender systems produce suggestions , and they are effective in static and relatively noise free environments . The design of recommender systems for web content poses significant challenges due to the dynamic nature of web pages , and the high level of noise introduced by the analysis of the user browsing data . We handle these problems by proposing a graph based approach , which allows to find interesting news articles soon in order to recommend them to the web users .
765 2 . RELATED WORK
In this Section , we present the state of the art for the main topics of our research .
Web usage mining . Web usage mining aims to capture , model , and analyze the behavioral patterns and profiles of users interacting with the Web . Data stored in usage logs can be used for solving navigational problem [ 24 ] , improving web search [ 3 ] , recommending queries [ 5 ] , suggesting authoritative web sites [ 22 ] , and enhancing performance of search engines [ 23 ] . A good survey of web usage mining can be found in [ 21 ] .
Query log analysis . Several studies have analyzed querylog data to understand the behavior of search engine users . In [ 20 ] , Spink et al . point out that users tend to submit short queries and look at few pages of results . They prefer to refine queries rather than to navigate answer pages deeper , and they do not exploit advanced search features .
Another important finding is that the frequency of queries is power law distributed : a lot of queries are submitted once or twice , while a few queries are submitted many times [ 19 ] . These queries are also popular , so they are shared by different users reflecting a spatial locality in the query stream [ 23 ] . Further studies emphasize a temporal locality in query stream : repeated submissions of queries on the same topic are separated by a small number of other queries [ 15 , 12 ] .
Caching of query results . The locality properties , observed in the query stream , suggest caching pre computed answers or partial data used in the computation of new answers . The work [ 17 ] exploits frequent queries to enhance the retrieval process . However , the simple popularity based cache may poorly address the issue of temporal locality [ 15 ] . In [ 12 ] , the authors present a Static Dynamic Cache ( SDC ) , where the static portion has fixed entries to store results of most frequent queries , and the dynamic portion is managed by a replacement policy to intercept the variations in the query stream . In [ 4 ] , the authors study the impact of the tail of the query distribution on caches of search engines .
Diversification of search results . Result diversification is of use for ambiguous queries ( eg , “ python ” ) and broad queries with many potential user intents ( eg , “ java programming language ” ) . Some diversification strategies are based on removing documents that are similar in order to avoid the redundancy of information [ 6 , 8 ] . Others make explicit use of the knowledge about the different topics covered by queries and documents [ 1 , 7 ] .
Web page recommendation . Recommender systems allow to learn user preferences and to make recommendations . They can be employed to recommend products ( eg , books , movies , music , etc . ) and web content ( eg , news , photos , etc ) Recommender systems can be : ( i ) content based , the system recommends items similar to the ones the user preferred in the past , ( ii ) collaborative filtering based , the system recommends items that people with similar tastes liked in the past , and ( iii ) hybrid , the system combines content and collaborative filtering based methods .
In the last years , a lot of attention has been devoted to the recommendation of web news . In [ 18 ] , the authors present a recommender system based on human judgments . In [ 11 ] , Das et al . identify clusters of users with similar interests by exploiting the user click data .
3 . PROPOSED RESEARCH
In this Section , we describe the research issues which we want to focus on . Then , we present our approaches to address these issues , and we show some preliminary results , which corroborate our proposals . 3.1 Improving search engine performance
Every day millions of users submit queries to search engines and the amount of data involved in processing a query is huge . The users desire to receive a list of relevant and authoritative results rapidly , therefore the search engines employ caching techniques to provide high quality results in short time .
Modern search engines cache posting lists of most frequent query terms or the results of the queries . In the second case , depending on the available space , the cache can store the entire page of results or single documents . A drawback of the latter approach with respect to the former is that query response times will be higher , since result pages must be constructed by using the documents in cache . On the other hand , this approach presents the advantage that the same document can be used to serve multiple queries , so that the cache space is used efficiently . The search result cache can be static or dynamic , and a combination of both techniques can be employed [ 12 ] . The static cache is subject to periodical updates and the documents to cache are precomputed using historical information . The dynamic cache stores results according to the query sequence .
For our research work , we consider the scenario of the static caching of query results , where the cache stores single documents ( snippet and satellite information returned to users in response to their queries ) . We propose query covering as a strategy for effectively and efficiently caching results of queries . The idea is to use statistical information about the user querying behavior , so as to populate the cache with a set of documents that in the average will maximize the number of queries that are entirely served by the cache . We examine the problem of query covering under a simple information retrieval model , in which each document is either relevant or irrelevant to a query [ 2 ] . Then , we improve this approach by proposing a weighted version of query covering , where weights capture the degree of relevance of the documents to the queries . The weighted approach selects high ranked documents , which appear in the result lists of several queries . This allows to optimize the cache space , ensuring the quality of the results . Finally , we prove that the weighted approach can be used for different purposes , such as the diversification of cached documents .
311 Approach We consider a simple framework in which users submit queries to a document retrieval system over time . The document retrieval system hosts a document collection . In addition , to improve response time , the system uses a document cache . Whenever a query is submitted , the system checks the cache , if all requested documents are stored in the cache , the system builds the final page of results , otherwise it incurs a cache miss , that is , a penalty reflecting that the system has to carry out a time expensive operation , such as the retrieval of documents from the document collection .
We assume that we have full knowledge of the document set , and in particular we know what documents are relevant to what queries . Furthermore , we assume to have no knowl
766 edge about queries that will be submitted in the future , but we have a relatively good estimate of their distribution . We want to create a universal map from each query to a set of relevant documents for the query , so that , when a query is issued , the system can fetch the documents from the cache and use them to serve the current query as well as possibly future ones .
The problem can be seen as a stochastic generalization of the set cover problem [ 14 ] , in which elements correspond to queries and documents correspond to sets . We need to define a fixed mapping from elements to covering sets without knowledge of the elements to cover , since we have no a priori knowledge of the future queries .
Our theoretical finding is that knowing the query distribution suffices to provide algorithms with logarithmic approximations of the optimal solution . In [ 2 ] , we prove that there exists a set of documents that covers a large fraction of the queries and a simple greedy approach is able to find it . Moreover , we present a greedy algorithm , which selects the most cost effective documents , namely , the documents appearing in the result lists of a large number of uncovered queries . This greedy algorithm ignores the degree of relevance of the document to the query . Hence , we propose to overcome this limitation by using a new approach , which associates weights to query document pairs .
The weighted version of the greedy algorithm gives preference to the documents that cover the largest total weight among the uncovered queries . We believe that the weighted approach is general and versatile , and it may be used for different optimization criteria by suitably defining weights . In particular , we propose two definitions of weights :
• Document relevance . The weights reflect the degree of relevance of the document to the query . The weight of the document d for the query q decreases as the position of d in the result list of q increases .
• Document diversification . We want to cache documents maximizing the coverage of all possible intents behind user queries . Hence , the weight of a document depends on the position of the document within the result list and on the extent to which the document covers different intents for the query . Following Capannini et al . [ 7 ] , we interpret the specializations of a given query as possible subtopics ( intents ) of that query . The weight of the document d for the query q is computed as the sum of the values of its utility score for every specialization q of q , each weighted by the probability of that transition between q and q . The utility score captures the similarity between the document , that is a result of the query , and each document appearing in the result list of the query specializations .
312 Experiments For the experiments , we use a real world query log dataset . We split the dataset into time periods ( eg , one day or one week ) . Queries submitted in the ith time period form the training set , and they are used to learn the query distribution and to select a static collection of documents for the cache . Queries of the ( i + 1)th time period , constitute the test set , and they are exploited to assess the performance of the query covering approach .
To evaluate the performance we use : ( i ) recall , the percentage of queries from test set covered by cached documents , ( ii ) num_doc , the number of cached documents , and ( iii ) p@n ( precision at n ) , given a query in the test set , it is the proportion of the top n results that are found in the cache . Then , we compute the average over all covered queries from test set .
We denote with Unweighted , the implementation of the greedy algorithm presented in [ 2 ] . For the weighted approach , we propose to design two different implementations which correspond to different definitions of covering : the query is covered if the overall weight of its results is at least a given threshold W ( Threshold algorithm ) , or if the number of results is at least a given cardinality k ( Cardinality algorithm ) . We compare the performance of our approaches against Topk performance . Topk selects first k results for most frequent queries . This approach is na¨ıve , but it gives us an upper bound of the coverage that can be achieved with a given collection of documents .
The preliminary results of recall and num_doc for k = 10 , and W = 10.0 , varying the percentage of queries from training set we desire to cover , are shown in Figure 1(a ) and ( b ) . As expected Topk has the best recall , however greedy approaches are following closely . Moreover , if we observe the dimension of the cache , for Topk the required space grows linearly with the number of queries in the training set that are covered , while the num_doc is optimized with covering approaches , for which we observe a steep increase after 40 % of covered queries . This leads us to an important observation : we can just cover a smaller fraction of queries from training set , for example 30 % or 40 % , to have a good recall and at the same time a limited utilization of cache space .
The precision ( p@n ) is the proportion of the top n results found in the cache . In Figure 1(c ) is reported the p@n , for n = 5 , 10 , of the covered queries from the test set . As expected , good precisions are achieved with Topk , Threshold and Cardinality , while Unweighted performs worse .
The performance of the diversification approach are evaluated following the guidelines of the TREC Diversity Task [ 9 ] . We consider two intent aware metrics : α NDCG [ 10 ] with α = 0.5 and IA P [ 1 ] . The greedy approach allows to achieve good values of intent aware metrics , and at the same time it saves a lot of cache space . In particular , the cache usage is approximately one order of magnitude less than the na¨ıve heuristic .
3.2 Web page recommendation
The systems for web page recommendation are based on collaborative filtering approaches . The underlying idea is to exploit the user browsing logs ( eg the click information ) , in order to identify users with similar interests and tastes [ 11 ] . These recommender systems deal with very high levels of noise , since visiting a web page is not a clear indication of interest as renting a dvd or buying a book . Further , the web pages to recommend are not an input of the recommendation algorithm , and the systems have to discover the interesting pages to recommend . We address these issues by leveraging not only the user similarity , but also the latent temporal patterns of user visits to web pages . In [ 16 ] , we present a graph based approach called early adopter graph . We propose to use this graph to identify those users who discover interesting pages before others . These users are called early adopters , a term we borrow from social sciences ,
767 ( a ) Recall
( b ) Dim . Cache
( c ) Precision
Figure 1 : Recall , dimension of the cache , and precision at n vs . % of covered queries of training set . economics , and marketing research , in which early adopters are people who embrace new technologies before others , buy new products soon after their release , and play an important role in influencing others to adopt innovations . By tracking the browsing activity of early adopters we can identify new interesting pages early , and recommend these pages to users who share interests with the early adopters . In particular , we consider news and blog pages , as these pages are more dynamic and more appropriate for recommendation tasks . 321 Approach The early adopter graph G is an attributed , directed and weighted graph , where nodes correspond to users and an edge between two users , u and v , expresses the fact that the two users visited the same page and that the temporal rule “ u visited the page before v ” holds . We build the graph by extracting from a toolbar log the triples ( u , p , t ) , where : ( i ) u is the anonymous identifier of the user , ( ii ) p is the url of the visited page , and ( iii ) t is the timestamp of the visit of u at p . Given a page pj , we organize the visits that pj received by all users in a chronologically sorted access list A(pj ) : ( u1 , t1j ) , ( u2 , t2j ) , . . . , ( un , tnj ) , where tij is the timestamp of the first visit of user i to the page j . Obviously , the early adopters tend to appear at the beginning of the access list . We propose different measures of the early adoption . More in detail , the early adopter score σ(u ) is computed by considering the position of the user in the access list ( relative position ) , or the timestamp of the visit ( time distance ) . In both cases , the more a user u exhibits an early adopter behavior , the higher is the value of σ(u ) . Furthermore , we model each user u by a topic distribution θu : given a set of topics T , the z th coordinate θuz denotes the interest of the user u in the topic z ∈ T , and it is computed using the empirical frequency that a user visits the pages whose topic is z .
The edge weight w(u , v ) represents the likelihood that a page visited by u is then visited by v . We propose and evaluate different edge weighting schemes to compute the strength of the connections . The definitions of the edge weights are inspired by the Bernoulli and Jaccard measures described in [ 13 ] . The Bernoulli measure interprets each visit of u to a page as a hypothetical attempt of u to influence v in visiting the same page . The Jaccard measure considers also the pages visited by v and not by u , and it captures whether v follows mostly the actions of u and not many more .
Figure 2 : Precision of recommendations : earlyadopter graph ( EAG ) vs . collaborative filtering approaches ( CF ) .
Our recommendation approach leverages the information found in the early adopter graph G . Given an arc ( u , v ) we consider suggesting to user v pages that have been visited by the user u . To improve the relevance of our recommendations , we rank recommendations by using the early adopter score σ(u ) of the user u from whom the recommendation originates , as well as the edge weight w(u , v ) . Additionally , we use page topics to boost scores of pages whose topics match the interests of the user v . When a page p is suggested to v by different early adopters , the final recommendation score s(v , p ) is the sum of the contributions of the early adopters for the user v .
322 Experiments We evaluate our recommendation algorithm using Yahoo! Toolbar dataset . We split our dataset , at the level of pages , in two portions : training and test sets . The training subset is used to build the early adopter graph , learn the earlyadopter scores , and the topics of interests of the users . Given a user v and the set of v ’s early adopters , namely , the in neighborhood of v ( N−(v) ) , the algorithm recommends to v the pages visited by u ∈ N−(v ) . Then , it ranks the pages by the recommendation scores .
The recommendations are evaluated with precision atk ( p@k ) for k = 1 , 5 , 10 , 15 , which gives an indication of the percentage of recommended pages that are actually visited by v . As we can see from Figure 2 , the resulting system outperforms other out of the shelf recommendation systems based on collaborative filtering , and where the user similarity is given by Tanimoto or Log Likelihood coefficients .
15 20 25 30 35 40 45 10 20 30 40 50 60 70 80 90 100 % Covered Queries from Test Set % Covered Queries from Training Set ( x)Thres . W=100Card k=10Unweig . k=10Top1000*10020*10540*10560*10580*10510*10612*10614*10616*106 10 20 30 40 50 60 70 80 90 100Dim Cache % Covered Queries from Training Set ( x)Thres . W=100Card k=10Unweig . k=10Top10 0.1 0.2 0.3 0.4 0.5 0.6 0.7105090Precision Covered Queriesp at 5Thres . W=100Card k=10Unweig . k=10Top10 0.1 0.2 0.3 0.4 0.5 0.6 0.7105090Precision Covered Queriesp at 10 0 0.05 0.1 0.15 0.2 0.25 0.3p@1p@5p@10p@15PrecisionsDefinition of edge weights : BernoulliEAG : time distanceEAG : relative positionCF : TanimotoCF : Log Likelihood768 [ 9 ] C . L . Clarke , N . Craswell , and I . Soboroff . Overview of the TREC 2009 Web Track . In TREC , 2009 .
[ 10 ] C . L . Clarke , M . Kolla , G . V . Cormack ,
O . Vechtomova , A . Ashkan , S . B¨uttcher , and I . MacKinnon . Novelty and diversity in information retrieval evaluation . In SIGIR , 2008 .
[ 11 ] A . Das , M . Datar , A . Garg , and S . Rajaram . Google news personalization : scalable online collaborative filtering . In WWW , 2007 .
[ 12 ] T . Fagni , R . Perego , F . Silvestri , and S . Orlando . Boosting the performance of web search engines : Caching and prefetching query results by exploiting historical usage data . ACM Trans . Inf . Syst . , 24(1):51–78 , 2006 .
[ 13 ] A . Goyal , F . Bonchi , and L . V . S . Lakshmanan .
Learning influence probabilities in social networks . In WSDM , 2010 .
[ 14 ] F . Grandoni , A . Gupta , S . Leonardi , P . Miettinen , P . Sankowski , and M . Singh . Set covering with our eyes closed . In FOCS ’08 , pages 347–356 . IEEE Computer Society , 2008 .
[ 15 ] E . P . Markatos . On caching search engine query results . Computer Communications , 24(2):137–143 , 2001 .
[ 16 ] I . Mele , F . Bonchi , and A . Gionis . The early adopter graph and its application to web page recommendation . In CIKM , 2012 .
[ 17 ] V . V . Raghavan and H . Sever . On the reuse of past optimal queries . In SIGIR , 1995 .
[ 18 ] P . Resnick , N . Iacovou , M . Suchak , P . Bergstrom , and
J . Riedl . Grouplens : an open architecture for collaborative filtering of netnews . In CSCW , 1994 .
[ 19 ] C . Silverstein , M . Henzinger , H . Marais , and
M . Moricz . Analysis of a very large web search engine query log . In ACM SIGIR Forum , pages 6–12 , 1999 .
[ 20 ] A . Spink , D . Wolfram , M . B . J . Jansen , and
T . Saracevic . Searching the web : the public and their queries . J . Amer . Soc . Inform . Sci . Tech . , 52(3):226–234 , 2001 .
[ 21 ] J . Srivastava , R . Cooley , M . Deshpande , and P N
Tan . Web usage mining : discovery and applications of usage patterns from Web data . SIGKDD Explor . Newsl . , 1(2):12–23 , 2000 .
[ 22 ] R . W . White , M . Bilenko , and S . Cucerzan . Studying the use of popular destinations to enhance web search interaction . In SIGIR , 2007 .
[ 23 ] Y . Xie and D . O’Hallaron . Locality in search engine queries and its implications for caching . In IEEE Infocom 2002 , pages 1238–1247 , 2002 .
[ 24 ] J . Zhu , J . Hong , and J . G . Hughes . Pagecluster :
Mining conceptual link hierarchies from web log files for adaptive web site navigation . ACM Trans . Internet Technol . , 4(2 ) , 2004 .
4 . OPEN RESEARCH ISSUES
Open problems are related to the optimization of searchengine performance . First , it would be interesting to design and analyze a dynamic caching policy able to cope with spikes in the query distribution . These spikes could be due to sudden events . Second , we propose to create a twolevel static cache , where a first level cache stores documents relevant to most frequent queries , while a second level cache stores documents chosen by the weighted greedy approach . For the diversification purpose , we aspire to find a dynamic approach which decrements the weights of documents that satisfy topics already covered , in order to favor those documents that are relevant to uncovered topics .
Moreover , we propose to investigate the application of covering approaches to the optimization of other aspects of the web search . For example , an important search functionality is represented by the phrase queries . Such queries are characterized by quotation marks , and can be issued to the search engine in order to find documents that contain an exact sequence of words . The standard way to process a phrase query is using an index with positional information . However , processing phrase queries is complex and rather expensive , especially if the phrase presents stop words , which cannot be ignored . There are different techniques to optimize the phrase query processing , and one of them is using a phrase index . For large collection of documents , indexing all possible phrases is prohibitive , and a challenging task consists in identifying the set of phrases to index . We believe that the covering strategies can help to take this decision : given a set of possible queries , we extract phrases from them , and then we can use a greedy approach to identify those phrases which cover as many queries as possible . These phrases represent good candidates to be indexed .
Concerning the early adopter graph , we plan to use different influence models to learn the edge weights . Then , we would like to investigate the application of the early adopter model to other domains .
5 . REFERENCES [ 1 ] R . Agrawal , S . Gollapudi , A . Halverson , and S . Ieong .
Diversifying search results . In WSDM , 2009 .
[ 2 ] A . Anagnostopoulos , L . Becchetti , S . Leonardi ,
I . Mele , and P . Sankowski . Stochastic query covering . In WSDM , 2011 .
[ 3 ] R . Baeza Yates , C . Hurtado , and M . Mendoza .
Improving search engines by query clustering . J . Am . Soc . Inf . Sci . Technol . , 58(12):1793–1804 , 2007 .
[ 4 ] R . Baeza Yates , F . Junqueira , V . Plachouras , and
H . F . Witschel . Admission policies for caches of search engine results . In SPIRE , 2007 .
[ 5 ] P . Boldi , F . Bonchi , C . Castillo , D . Donato , A . Gionis , and S . Vigna . The query flow graph : model and applications . In CIKM , 2008 .
[ 6 ] A . Bookstein . Information retrieval : A sequential learning process . Journal of the American Society for Information Science , 34(5):331–342 , 1983 .
[ 7 ] G . Capannini , F . M . Nardini , R . Perego , and
F . Silvestri . Efficient diversification of web search results . Proc . VLDB Endow . , 4:451–459 , 2011 .
[ 8 ] J . Carbonell and J . Goldstein . The use of MMR , diversity based reranking for reordering documents and producing summaries . In SIGIR , 1998 .
769
