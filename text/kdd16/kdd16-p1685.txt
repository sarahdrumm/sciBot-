Lightweight Monitoring of Distributed Streams
Arnon Lazerson
Technion – Israel Institute of
Technology
Haifa 32000 Israel lazerson@cstechnionacil
Daniel Keren Haifa University Haifa 31905 Israel dkeren@cshaifaacil
Assaf Schuster
Technion – Israel Institute of
Technology
Haifa 32000 Israel assaf@cstechnionacil
ABSTRACT As data becomes dynamic , large , and distributed , there is increasing demand for what have become known as distributed stream algorithms . Since continuously collecting the data to a central server and processing it there incurs very high communication and computation complexities , it is advantageous to define local conditions at the nodes , such that – as long as they are maintained – some desirable global condition holds . A generic algorithm which proved very useful for reducing communication in distributed streaming environments is geometric monitoring ( GM ) . Alas , applying GM to many important tasks is computationally very demanding , as it requires solving a notoriously difficult problem – computing the distance between a point and a surface , which is often very time consuming even in low dimensions . Thus , while useful for reducing communication , GM often suffers from exceedingly heavy computational burden at the nodes , which renders it very problematic to apply , especially for “ thin ” , battery operated sensors , which are prevalent in numerous applications , including the “ Internet of Things ” paradigm .
Here we propose a very different approach , designated CB ( for Convex/Concave Bounds ) . CB is based on directly bounding the monitored function by suitably chosen convex and concave functions , that naturally enable monitoring distributed streams . These functions can be checked on the fly , yielding far simpler local conditions than those applied by GM . CB ’s superiority over GM is demonstrated in reducing computational complexity , by several orders of magnitude in some cases . As an added bonus , CB also reduced communication overhead in all application scenarios we tested .
Keywords Disributed Streams ; Distributed Monitoring ; Resource Limited Devices
1 .
INTRODUCTION
The following is a canonical problem in distributed systems and databases : given are distributed nodes , and a function which depends on the data at all of them . How can its value be computed , or approximated , with minimal communication ? Typically , the trivial solution ( collecting all data to a central node and computing the function there ) is impractical .
With the advent of data stream systems and their increasing importance in quickly evolving fields such as social networks , a more difficult variant of this problem began attracting considerable interest : assume that the data at the nodes is also dynamic . Continuously and exactly computing the function ’s value is typically infeasible , as real world data consists of many nodes , each holding a large , rapidly changing data stream . This led to the introduction of the distributed monitoring problem ( also referred to as the functional monitoring problem , [ 29 , 7 , 34 ] ; see also the survey in [ 9] ) , which can be broadly defined as follows :
Definition 1 . Given is a distributed system , with nodes N1Nk , with Ni holding a dynamic data vector vi(t ) ( t will be suppressed hereafter to reduce equation clutter ) . Also given is a function f , which depends on all the vi ’s , and a threshold T . The goal is to define local conditions at the nodes , such that :
• Correctness : As long as all local conditions hold , the global condition f ( v1vk ) ≤ T is also guaranteed to hold .
• Communication efficiency : The local conditions are “ lenient ” , ie the number of times in which they are violated is minimal .
• Computational efficiency : The complexity of checking the local conditions is minimal .
Case study .
As a motivating real life example [ 26 ] , which applies the Pearson Correlation Coefficient function ( treated in this paper ) , consider a distributed sensor network used to monitor air quality . Often , not only the information on the individual pollutants is important , but the correlations between them as well . For example , if an event i is defined as pollutant i crossing a certain threshold , one may wish to know whether there exists a correlation between events i , j for two different pollutants . A commonly used such measure , the Pearson Correlation Coefficient ( PCC ) , quantifies such a correlation by the value , where x , y , z are resp .
√ z−xy
√ x−x2 y−y2 the probabilities of event i , event j , and both events simultaneously . For a distributed system , the global probabilities are averaged over the nodes . It is easy , however , to see that the PCC value of the global probabilities can be above a
1685 given threshold T , while the local value at some of the nodes is below T , and vice versa ( for example , in a system with two nodes and local values x1 = 0.8 , y1 = 0.2 , z1 = 0.17 and x1 = 0.2 , y1 = 0.7 , z1 = 0.15 , the local PCC values are 0.062 and 0.054 , and the global value is −026 ) This is because , for arbitrary functions , there is generally no correlation between the average of the values and the value at the average .
For general functions , defined over a distributed system , it is typically impossible to determine the position of their global values vis a vis T , when given just the local values . The distributed monitoring problem is to impose local conditions which will guarantee that the global value did not cross T .
This problem is known to be rather difficult ( NP complete even in very simple scenarios ; see [ 21] ) . Nonetheless , considerable progress has been made for real life problems ( Section 2 ) .
, ie f = f ( v1++vk
Geometric monitoring ( GM ) , introduced in [ 32 ] , deals with the case in which the monitored function can be expressed as the application of an arbitrary function to the aggregated vector v1++vk ) . This model turns out to be rich enough to be applicable to a wide range of problems ( see Section 21 ) To the best of our knowledge , GM is the only completely generic method for monitoring arbitrary functions over the aggregated data ; in this paper , the most recent version of GM [ 22 ] is the baseline for comparison . k k
While a more complete description of GM is deferred to Section 2.1 , we note that in order to apply it , the following problem should be repeatedly solved : let S be the hypersurface ( or threshold surface ) defined by S = {u|f ( u ) = T} . Then , it is required that each node , at every time step , calculate the distance of a certain point ( unique to that node ) from S . This problem can be exceedingly difficult even for surfaces in low dimensional Euclidean space , and it can render GM unsuitable for monitoring even relatively simple functions , such as the cosine similarity between two vectors ( more on this in Section 22 ) 1.1 The convex bound ( CB ) method k
We propose here a very different , simpler , and more direct method to solve the distributed monitoring problem . It relies on the simple observation that , if f is a convex function , then , if f ( vi ) ≤ T holds at every node , it also holds that ) ≤ T . Thus , monitoring the value of a convex f ( v1++vk function ( from above ) is trivial – just monitor its value at every node . To handle a general f , we propose to search for a convex function c such that c(u ) ≥ f ( u ) for all vectors u , and monitor the condition c ≤ T . This yields a far simpler monitoring condition , whose correctness implies the correctness of the desired condition f ≤ T . Naturally , the following conditions should hold :
• c should be easy to derive and calculate . • In order to avoid a high ratio of “ false alarms ” , c should tightly bound f .
We refer to the proposed method as convex bound ( CB ) . Clearly , monitoring f ≥ T can be similarly achieved by finding a concave lower bound .
1.2 Contributions
We offer the following contributions : • Introducing the CB method and applying it to monitor four popular functions : the Pearson correlation coefficient ( PCC hereafter ) , inner product , cosine similarity , and PCA Score . These functions were chosen both for their great practical importance and since they do not fall into any category for which there exist simple , efficient solutions ( they are not linear , convex , concave , or monotonic ) .
• Experimentally validating against state of the art methods . CB proved to be far less demanding in terms of local computation at the nodes . Local computation was reduced by one to six orders of magnitude . As an added bonus CB reduced communication overheads for all datasets , functions , and thresholds we tested .
• Proving that every solution GM arrives at can also be obtained with CB .
• Providing a general approach for calculating the convex bound function and proving that it is optimal up to second order Taylor expansion .
2 . PREVIOUS WORK
Much of the early work on monitoring distributed streams dealt with the simpler cases of linear functions [ 20 , 19 ] . Distributed sensor networks were studied in [ 28 , 27 ] . Other work included top k monitoring [ 4 ] ; distributed monitoring of the value of a single variable polynomial [ 30 ] , and perturbative analysis of eigenvalues , which was applied to determine local conditions on traffic volume data at the nodes of a distributed system , in order to monitor system health [ 17 ] . In [ 33 ] , the monitoring problem is studied in a probabilistic setting , and in addition to the function ’s score , a probability threshold is applied ; see also [ 25 ] . Monitoring entropy was studied in [ 3 ] . Ratio queries are handled in [ 15 ] . In [ 35 ] the norm of the average vector is monitored .
While some problems in monitoring over distributed systems were treated in the past , we are not aware of any general method ( capable of handling arbitrary non linear , non monotonic , non convex functions ) except for GM and its derivatives , which are surveyed next . 2.1 Previous work on geometric monitoring
In [ 31 , 32 ] a general approach , geometric monitoring ( GM ) , was proposed for tracking the value of a general function over distributed streams . GM rests on a geometric result , the so called bounding lemma ( details follow in this subsection ) , which makes it possible to “ break up ” a global threshold query into conditions that can be checked locally at each site . Followup work [ 22 , 13 ] proposed various extensions to the basic method . Recent work on GM includes efficient outlier detection in sensor networks [ 8 ] and sketch based monitoring of norm , range aggregate , and join aggregate queries over distributed streams [ 12 ] .
While GM achieved state of the art results in reducing communication overhead for a nice range of central problems , its application is typically hampered by high computational overhead at the nodes . It is this problem which the proposed CB approach aims to alleviate .
1686 Since GM is our baseline for comparison , and it also shares some basic terminology with CB , we next briefly describe it . Proofs and further details can be found in [ 22 ] ( which is the version that was implemented ) .
A brief view of GM . Recall that the distributed mon ) ≤ T , where itoring problem considers whether f ( v1++vk {vi} denote the local dynamic data vectors at the nodes . GM rests on the following geometric interpretation of this question : define the admissible region , A , by A {u|f ( u ) ≤ T} . Then , the question is whether the con∈ A holds . The first step in answering this dition v1++vk question is the following : k k
Lemma 1 . [ 32 ] Let vi(0 ) denote the initial value of the data vector at the i th node , and let the so called reference point , p0 , be equal to the average of these initial values : p0 = v1(0)++vk(0 ) . We assume that , during the initial synchronization , a coordinator node broadcasts p0 to all nodes . Denote the change in the data vector at the i th node , ie vi − vi(0 ) , by di ( it will be referred to as the i th drift vector ) . Then , the following holds : k v = v1 + + vk k
=
( p0 + d1 ) + + ( p0 + dk ) k fi
Now , the i th node can independently compute p0 + di ; and since the global vector v is equal to the average of p0 + di , i = 1k , it obviously lies in their convex hull . This can be used to impose local conditions on p0 + di , which will guarantee that v ∈ A . This is achieved by the bounding lemma :
Theorem 1 . [ 32 ] Let Bi denote the ( solid ) sphere with k center p0 + di/2 and radius ||di||/2 . Then the union
Bi contains the convex hull of the vectors p0 , p0 + d1p0 + dk ; hence it contains v . i=1
As a result of the bounding lemma , the local condition used in GM is the following : node i remains silent as long as its sphere Bi is contained in A ( Fig 1 ) . If this condition is violated , the system enters a violation recovery phase [ 32 , 12 ] .
T} . Computing the distance from a point to the threshold surface of a general function is a notoriously difficult problem , and a closed form solution very rarely exists . Worse , there exist no algorithms which guarantee that the distance will be recovered . Even for the case of a polynomial f , the solution may require an inordinate amount of time ; closedform solutions are often impossible to derive , and iterative schemes are slow and not guaranteed to converge . Known upper bounds on the complexity are extremely high ( doubly exponential in the number of variables [ 2] ) .
In [ 24 ] , GM was extended by the convex decomposition ( CD ) approach , which works by decomposing ¯A into convex subsets . However , the resulting algorithm has to be specifically tailored to each monitored function , and it suffers from the need to solve the same type of problem as GM ( finding the closest point on a surface ) . For the inner product function , the solution was quite complicated , and we could not apply CD to the PCC or to cosine similarity , which are easily treated by CB .
Clearly , run times as those often incurred by GM and its derivatives are unacceptable for many distributed streaming systems . We now introduce CB , and demonstrate its advantage for monitoring four popular functions .
3 . THE CB METHOD
As noted in the Introduction , it is easy to define local conditions for the monitoring problem ( Def . 1 ) when f is convex : every node i must only check the condition f ( p0 + di ) ≤ T ( correctness follows immediately from Lemma 1 ) . We propose to extend this simple observation to monitor arbitrary functions , using an approach which works directly in the realm of functions , as opposed to seeking a geometric solution . The proposed solution works by “ relaxing ” f to a convex function c that bounds f from above , and monitoring the condition c ≤ T . This condition both implies f ≤ T , and is also easy to monitor . We shall refer to c as a convex bound for f . Fig 2 schematically demonstrates the idea behind CB .
Figure 1 : Applying local conditions in GM . The drift vector di causes a violation , since the sphere it defines with p0 intersects the inadmissible region ¯A ; however , dj does not cause a violation .
2.2 Computational complexity of GM
To apply GM , it must be repeatedly checked whether a certain sphere intersects with ¯A – that is , whether its radius is smaller than the distance from its center to A ’s boundary , which is defined by the threshold surface {u|f ( u ) =
Figure 2 : x2 + 10 ( blue curve ) is a convex bound for x2 + 10 sin(x ) ( dark curve ) .
The next theorem states that every solution which GM provides is also realizable as a solution provided by CB . The proof is omitted due to lack of space .
Theorem 2 . For every monitoring problem , there is a solution obtained with CB which is exactly identical to the solution obtained with GM – that is , it imposes exactly the same local conditions . 3.1 Choosing convex bounds
There are , of course , an infinite number of convex bounds
1687 for f , and the question is which of them to choose . To this end , we first propose the following definition .
Definition 2 . Let f be the monitored function . A tight family of convex bounds for f , denoted CB(f ) , is a set of convex functions satisfying the following requirements :
• g ∈ CB(f ) implies that g is convex and , for every u , g(u ) ≥ f ( u ) ( the last condition will be denoted g f ) . • Let c be any convex function such that c f . Then there exists g ∈ CB(f ) such that c g .
• If g1 , g2 ∈ CB , then neither g1 g2 nor g2 g1 .
Clearly , if g1 , g2 are both convex bounds for f , and g1 g2 , it is better to use g2 when monitoring f ( since the condition g2(v ) ≤ T is weaker than g1(v ) ≤ T ) . Therefore we have the following :
Lemma 2 . When applying CB to monitor f , the convex bound should belong to some family of tight bounds of f . In the following case , it is possible to define CB(f ) :
Lemma 3 . Let f be a concave function . Then the family of all tangent planes to f defines a family of tight bounds.1
Proof . Every tangent plane is linear , hence convex . Further , it is known that a concave function lies under any of its tangent planes . Now , let g be convex and g f . Denote by U ( g ) the set of all points above gs graph , and by B(f ) all points below fs graph . Then both U ( g ) , B(f ) are convex , and the minimal distance between them is therefore obtained at points on their boundaries . The tangent plane at the point on f ’s boundary is the desired element of CB(f ) . The idea of the proof is outlined in Fig 3 .
Figure 3 : A convex function g and concave function f such that g f . S is the segment connecting the two closest points on the graphs . The tangent at the closest point on f ’s graph , L , satisfies g L f , proving that the set of f ’s tangent planes is a tight family of convex bounds . 3.2 The convexity gap and dependence on the reference point
Replacing the monitored condition T ≥ f by T ≥ g f , for a convex g , enables efficient monitoring – alas , it might also result in potential false alarms ( ie vectors u for which T ≥ f ( u ) but T < g(u) ) . We refer to this problem as the convexity gap , or simply the gap ( referring to the gap between 1We deal here with differentiable functions , which include many functions of practical interest . Further , nondifferentiable functions can be arbitrarily approximated by differentiable functions on any bounded domain . f and g ) . Figuratively speaking , the “ system price ” , one must pay in order to allow distributed monitoring , is reflected in the “ convexity price ” , which is the gap between the monitored function and its convex bound . To minimize the number of false alarms , the gap should be minimized . However , as the following simple example demonstrates , it is often impossible to choose a single optimal g to achieve this goal . As depicted
Figure 4 : The impossibility of choosing a single best convex bound for the function f ( dark curve ) . g1 ( resp . g2 ) is better in the vicinity of p1 ( resp . p2 ) . in Fig 4 , it is evident that , loosely speaking , different bounds are better at different regions of the data space , and there is typically no hope of finding a unique bound that is always better than all the others . We formalize this observation with the following definition , which is both realizable and appropriate for the monitoring problem :
Definition 3 . A convex bound g1 is better than g2 at a point p iff there exists a neighborhood of p in which g2 g1 . Thus , in Fig 4 , gi is better at pi for i = 1 , 2 . Def . 3 is appropriate for the following reason . Recall that the local condition at the i th node is g(p0 +di ) ≤ T . Initially , the drift vector di is equal to zero ; assuming that the data at the nodes behaves continuously , or can be approximated by a random walk ( [18 , 14 , 21 , 22] ) , it follows that the local vector p0 + di can be modeled by a continuous process which starts at p0 and gradually wanders away from it . Therefore , a bound is sought which is optimal ( ie smaller than all other bounds ) in a certain neighborhood of p0 . For the case of a concave f , such a bound is provided by the following result :
Lemma 4 . If f is concave , the tangent plane at a point p is the best convex bound at p .
The proof is trivial , since the tangent plane ’s value at p is equal to f ( p ) , but all other tangent planes lie above f . Thus the deviation of the tangent plane at p from the point ( p , f ( p ) ) is quadratic ; hence , locally , it is smaller than that of the tangent planes at other points , which is linear . Consequently , when bounding a concave function from above ( or , equivalently , a convex function from below ) , we will replace it with its tangent plane at p0 . This is next used to transform a threshold condition on general functions to a convex condition . 3.3 “ Convexizing ” threshold conditions
If the monitored f is itself convex , the choice of a convex If f is concave , then , bound c is trivial – choose c = f .
1688 following Lemma 4 , the tangent plane at p0 is the optimal candidate for c . We next handle a more general case .
Definition 4 . : Assume that f = c1 − c2 , where both c1 , c2 are convex . The convexization of the condition f ≤ T is defined by c = c1 − Lc2 ( p0 ) ≤ T , where Lc2 ( p0 ) is the tangent plane of c2 at p0 .
Note that the c defined in Def . 4 is convex , bounds f from above , and that its definition is motivated by the special cases where f is convex or concave . The lower bound case is similarly handled : the inequality f ≥ T is replaced by the condition Lc1 ( p0)− c2 ≥ T ( note that Lc1 ( p0)− c2 is concave and bounds f from below ) .
We next prove that , for a very wide class of real problems , it is always possible to express f as the difference of two convex functions . First we recall a definition from calculus that comes in handy for testing convexity :
Definition 5 . Let f be a function of x1xn Its Hessian
Hf is the n × n matrix Hf ( i , j ) = ∂2f
.
∂xi∂xj
It is well known that a function is convex in a given domain D iff its Hessian is positive semidefinite ( PSD ) at every point in D2 .
Lemma 5 . If f possesses bounded second derivatives in a domain D , it can be expressed as the difference of two convex functions .
Proof . Since the elements of Hf are bounded over D , there is an upper bound , Λ , on the absolute values of Hf ’s 2 ||u||2 , c2(u ) = negative eigenvalues . Define c1(u ) = f ( u ) + Λ 2 ||u||2 . Clearly f = c1 − c2 and c2 is positive definite . Also , Λ Hc1 = Hf + Hc2 = Hf + ΛI ( where I is the identity matrix ) . Hence all the eigenvalues of Hc1 are ≥ 0 and c1 is convex .
All the functions we deal with in this paper either have bounded second derivatives , or their derivatives are continuous and the domain of interest is bounded ; hence , Lemma 5 is applicable . We will apply it for monitoring cosine similarity ( Section 43 )
The process outlined in Lemma 5 can be extended to provide a convex bound which is optimal to second order Taylor expansion . First , let us formalize the concept of “ annihilating ” negative eigenvalues : decomposition [ 6 ] A = P ( A ) = i max{λi , 0}eiet i .
Definition 6 . Given a symmetric matrix A with spectral i ( where λi are A ’s eigenvalues and ei its eigenvectors ) , define its positive part by i λieiet
331 Convexizing inequality constraints Since c1 − c2 ≤ T iff c1 ≤ c2 + T c3 , we can assume that the monitored condition is given as an inequality between two convex functions , c1 ≤ c3 . This condition is especially amenable to convexization : we replace it with c1 ≤ Lc3 ( p0 ) , where , as before , Lc3 ( p0 ) is c3 ’s tangent plane at p0 . We will use this form of convexization for the inner product , cosine similarity , and PCA Score functions ( Section 4 ) .
4 . APPLYING CB : THEORY
We now apply CB to monitor four popular functions : Pearson correlation coefficient , inner product , cosine similarity , and PCA Score ( “ effective dimension ” ) . In Section 5 we compare the run time and communication overhead of CB and GM in a variety of real scenarios .
To apply CB , we follow the method described in Section 331 If the monitored function cannot be directly written as the difference of two convex functions ( as in the case of cosine similarity ) , we apply Lemma 5 . 4.1 PCC
Let x , y denote the frequency of appearances of two items in elements of a certain set , and z the frequency of their joint appearances . A very typical example is when x , y denote the ratio of documents in which certain terms appear , and z the same for appearances of both terms simultaneously . The range over which PCC is defined is therefore 0 ≤ x , y ≤ 1 and z ≤ x , y . The function measures the strength of correlation between the appearances of x and y , and is defined by z − xy x − x2y − y2
√
√
P ( x , y , z ) =
The condition P ( x , y , z ) ≤ T can be written as x − x2y − y2 . We convexize it as follows .
( 1 ) We will assume T > 0 ; the case T ≤ 0 is treated similarly . z ≤ xy + T First , note that xy is neither convex nor concave ; it is simple to verify that the Hessian ’s eigenvalues for xy are always 1 and −1 ( ie every point on the function ’s surface is a saddle 4 − ( x−y)2 point ) . We therefore use the identity xy = ( x+y)2 . , Q2 = ( x−y)2 Denote Q1 = ( x+y)2 ( note that Q1 , Q2 are convex ) . We also need the following :
4
4
4 x − x2y − y2 is concave .
√
Lemma 6 . The function
The proof is omitted due to lack of space .
The condition P ( x , y , z ) ≤ T can therefore be written as x − x2
( z − T y − y2 + Q2 ) − Q1 ≤ 0
( 2 )
Theorem 3 ( whose proof is omitted due to lack of space ) enables to define a convex bound which is optimal to second order .
Theorem 3 . For a function f and reference point p0 , define a convex bound by copt(p ) = f ( p0 ) + ∇f ( p0 ) , p − u0 + ( 1/2)(p − p0)P ( Hf ( p0))(p − p0)t . Then , for any other convex bound g of f which satisfies g(p0 ) = f ( p0 ) , it holds that Hg(p0 ) ≥ Hcopt ( p0 ) ( where ≥ holds for both the operator and Frobenius norms of the respective Hessians ) . That is – up to second order , copt(p ) is an optimal convex bound at the vicinity of p0 . 2A matrix B is PSD iff uBut ≥ 0 for every vector u . A symmetric matrix is PSD iff all its eigenvalues are ≥ 0 .
√ x − x2y − y2 , but that is just an exercise in mul and , since this last expression is the difference of two convex functions3 , we can proceed by applying the paradigm described in Def . 4 . The lower bound case is similarly handled . It remains to calculate the tangent planes of Q1 , Q2 , tivariate calculus . The bounds are depicted , for some typical values , in Fig 5 . 411 Monitoring PCC with GM As explained in Section 2.2 , to apply GM we must be able to solve the closest point problem for the surface defined by z = xy + T software [ 16 ] , which first reduces the surface ’s equation to √ x − x2y − y2 . To this end we used dedicated x − x2y − y2 is concave , its negative is convex .
3Since T
√
1689 found , it still incurs the overhead of computing the quartic ’s coefficients , solving it , and checking the solutions to see which one yields the closest point on the surface . GM ’s overall run time was about 20 times higher than CB ’s . 4.3 Cosine similarity
Another very popular measure of similarity is cosine similarity ( referred to as Csim hereafter ) , which resembles the inner product function , but normalizes it by the length of the vectors . For example , if we have two histograms of word frequencies , derived from two document corpora , Csim will “ neutralize ” the effect of the corpus size when measuring the histogram similarity ; the inner product function , however , is biased towards larger corpora .
As in the inner product case , the data vector p is [ x , y ] , the concatenation of two n dimensional vectors x , y , and the reference point will be denoted p0 = [ x0 , y0 ] . Then , Csim x,y is defined by Csim(p ) = ||x||||y|| . Thus , to monitor a lower bound , ie Csim(p ) ≥ T ( we assume T > 0 ; the case of negative T is similarly treated ) , we need to monitor the condition x , y ≥ T||x||||y|| . This problem is more complicated than the inner product case , since there is no obvious way to decompose it into an inequality between two convex functions ; this is due to the fact that , while representing x , y as the difference of two convex functions is relatively easy , it is more difficult to derive such a representation for ||x||||y|| . We therefore resort to using the method outlined in Lemma 5 . We must first determine the smallest eigenvalue of the Hessian of ||x||||y|| . It follows from the following lemma that it equals −1 :
Lemma 7 . At a point x , y , the eigenvalues of H(||x||||y|| ) are 1 , −1 ( each with multiplicity one ) and ||x||/||y||,||y||/||x|| ( each with multiplicity n − 1 ) .
The proof is omitted due to lack of space . Now we can proceed to convexize the problem . First , we write the inequality x , y ≥ T||x||||y|| as ||x + y||2 ≥ ||x− y||2 + 4T||x||||y|| . Next , to make both sides convex , we add 2T ( ||x||2 +||y||2 ) to them , to obtain
||x + y||2 + 2T ( ||x||2 + ||y||2 ) ≥ ||x − y||2 + 4T||x||||y|| + 2T ( ||x||2 + ||y||2 )
Lastly , the inequality is convexized by replacing the RHS with its tangent plane at p0 . This step is straightforward , requiring only computation of the gradient , and is omitted for brevity . 431 Monitoring Csim with GM The problem of calculating the distance of a point to the Csim surface {[x , y]|x , y = T||x||||y||} is exceedingly difficult . No closed form solution exists , and three different software packages we applied took about three minutes to complete the task for a single point . 4.4 PCA Score
PCA ( Principal Component Analysis ) is a fundamental dimension reduction technique with numerous applications . Given a set of vectors in Euclidean space , PCA seeks a lowdimensional subspace which , on the average , well approximates the vectors in the set . Formally :
Definition 7 . Given 1 > T > 0 ( typically T ≈ 0.9 ) and a finite set of vectors S ⊂ Rm , the effective dimension of S
( a ) Upper Bound
( b ) Lower Bound
Figure 5 : Left : a convex upper bound ( blue ) for PCC ( green ) . The reference point ( in red ) is x0 = 0.3 , y0 = 0.6 , and T = 04 Right : a concave lower bound . an algebraic one , and then solves for the closest point . This incurred a run time far higher than the simple CB solution ( by more than three orders of magnitude ) , and also resulted in higher communication overhead ; results are provided in Section 521 4.2 Inner product
The inner product function is also extensively applied in data mining and monitoring tasks as a measure of similarity . We assume that the monitored function f is over vectors of length 2n , and is equal to the inner product of the first and second halves of the vector ; denoting the concatenation of vectors x , y by [ x , y ] , we have f ( [x , y ] ) = x , y . To express f as the difference of two convex functions , note that 4x , y = ||x + y||2 − ||x − y||2 . Since the norm squared function is convex , the condition x , y ≤ T is convexized by
||x + y||2 ≤ 4T + ||x0 − y0||2 + 2[x0 − y0 , y0 − x0 ] , [ x − x0 , y − y0 ]
( 3 ) where the reference point p0 = [ x0 , y0 ] , and the gradient of ||x − y||2 is equal to 2[x − y , y − x ] ( recall that , for a multivariate function f , the tangent plane at a point u0 is given by f ( u0 ) + ∇f ( u0 ) , u − u0 ) . 421 Monitoring inner product with GM In order to apply GM , one must be able to solve the closest point problem for the threshold surface , x , y = T . If the point outside the surface is denoted [ x0 , y0 ] , the problem can be formulated as
Minimize ( ||x − x0||2 + ||y − y0||2 ) such that x , y = T .
This problem can be solved with Lagrange multipliers . Defining F ||x− x0||2 +||y− y0||2 + 2λ( x , y− T ) The equations ∂x , ∂F
∂y , ∂F ( x − x0 ) + λy = 0 , ( y − y0 ) + λx = 0 , x , y = T
( 4 )
∂F
∂λ = 0 assume the form
These equations can be manipulated to obtain a quartic equation in λ :
T λ4−(2T + x0 , y0 ) λ2+,||x0||2 + ||y0||2 λ−x0 , y0+T = 0
After solving for λ , it is easy to solve for x , y .
While the inner product case is the only one addressed here for which a relatively simple solution for GM could be
1690 u∈S u∈S is defined as the smallest dimension of a sub space V ⊂ Rm , ||u||2 , where PV ( u ) is the such that
||PV ( u)||2 ≥ T projection of u on V 4 .
It is well known that the effective dimension , denoted k hereafter , can be computed as follows :
1 . Construct the m× m scatter matrix M = uut . Note u∈S that in the distributed setup , S is equal to the sum of local scatter matrices at the nodes .
2 . Compute M ’s eigenvalues , λ1 ≥ λ2 ≥ ≥ λm . i ≥ T λ2
3 . Determining the smallest k such that
λ2 i .
1≤i≤k
1≤i≤m
In [ 23 ] , PCA was applied to measure the health of a system consisting of distributed nodes . This proceeds as follows : at each timestep , a vector of various system parameters is associated with each node ( typically , the vectors’ components are various traffic volume indicators ) . System wide anomalies ( ie DDOS attacks ) are highly correlated with an increase in the effective dimension of the union of the parameter vectors over all nodes , in a sliding window of pre determined length . Hence , the condition to monitor is that the PCA Score , deλ2 i ) , is greater than some threshold fined by (
λ2 i )/(
1≤i≤k
1≤i≤m
T . As for the previous functions we handled , the difficulty lies in that λi are the eigenvalues of a global matrix which is equal to the sum of the local matrices , hence its exact computation at every timestep will incur a huge communication overhead . In order to apply CB for distributed monitoring , we must express the PCA Score as a function of the average matrix , as opposed to the sum ; however , since ( i ) eigenvalues scale linearly when the matrix is multiplied by a scalar , and ( ii ) the PCA Score is defined as the ratio of sums of squares of eigenvalues , its values on the average and sum matrices are equal .
What remains is to “ convexize ” the inequality i ≥ T λ2
λ2 i .
1≤i≤k
1≤i≤m
We rely on the following two lemmas :
Lemma 8 . For an m×m scatter matrix S ,
( 5 )
1≤i≤m
λ2 i equals
Tr2(S ) , and is a convex function of S . The proof follows immediately from the fact that every scatter matrix is symmetric .
Lemma 9 . For a symmetric S ,
λ2 i is convex .
1≤i≤k
Proof . This follows from the famous Fan identities , specif ically Theorem 2 in [ 11 ] .
Since both sides of Eq 5 are convex , we can proceed as in Section 331 , by replacing the LHS with the tangent plane at the reference scatter matrix S0 . All that is required is to compute the gradient of the LHS ; for that , we use the following result from linear algebra . 4We assume that S is centralized , ie its average is zero . The general case proceeds along the same lines and is omitted due to lack of space .
Lemma 10 . The derivative of λi with respect to S is equal i , where ei is the eigenvector of S corresponding to λi . to eiet
Hence , the monitored condition in Eq 5 is convexized by
λi(S0)ei(S0)et i(S0 ) , S−S0 ≥ T ( Tr(S2 ) ) i ( S0)+2
λ2
1≤i≤k
1≤i≤k
( 6 ) where S0 is the reference matrix , and S the local matrix . 441 Monitoring PCA Score with GM In order to apply GM , we must be able to compute the minimal PCA Score over all matrices in a sphere in the m2dimensional space of m × m matrices . This can be done using perturbative bounds that were applied in [ 17 ] , which also addressed monitoring the health of a distributed system . We also tested a simpler method , analogous to the ones used in [ 17 ] , in which the safe zone is defined by the maximal sphere around the reference matrix which is contained in the admissible region . Both methods require bounding the change in the eigenvalues , given the magnitude of change in the matrix . Two such perturbative bounds can be applied , which relate the change in the eigenvalues to the Frobenius norm or the spectral norm of the change in the matrix . We refer to the algorithms which use the Frobenius norm resp . spectral norm as FN resp . SN ( a detailed description is impossible due to lack of space ) . We note , however , that all these methods ( GM , FN , SN ) require solving the difficult problem of finding the closest point on the surface of matrices whose PCA Score equals T ; this renders them slower than CB . Further , CB was better in reducing communication overhead . Details are provided in Section 524
5 . EXPERIMENTAL EVALUATION
In the experiments , CB was applied to the tasks of monitoring the functions discussed in Section 4 , over a few datasets and for different threshold values . For the PCC , Csim and inner product functions we compared CB to GM . To the best of our knowledge , GM represents the state of the art in monitoring threshold queries over distributed streams . We are not aware of any other work on monitoring cosine similarity and the PCC , and while there is other work on monitoring the inner product [ 10 ] , GM improved on it [ 12 ] . For the PCA score function we compared CB to GM as well as to the Frobenius norm ( FN ) and spectral norm ( SN ) perturbative bounds described in [ 17 ] .
We examined the sliding window scenario , in which the data of interest are the last m records for some pre defined m ( or the last records received within a certain period ) ; for example , one may wish to continuously monitor only the last 1000 tweets in a tweet stream . The sliding window case corresponds to the turnstile model , in which the data vector ’s entries can both increase and decrease , and is more general than the cash register model , in which the entries can only increase .
In all the experiments , CB outperformed the other methods in both communication reduction and run time , with the improvement factor in run time being orders of magnitude for PCC , cosine similarity , and PCA Score . 5.1 Data
We used three data sets : the Reuters Corpus ( RCV1 v2 , REU ) , a Twitter crawl ( Dataset UDI TwitterCrawl Aug2012 , TWIT ) , and the 10 percent sample supplied as part of KDD
1691 Cup 1999 Data ( KC ) . The overall sizes of these data sets were : REU 374MB , TWIT 691MB , KC 46MB .
REU consists of 804,414 news stories , produced by Reuters between August 20 , 1996 , and August 19 , 1997 . Each story was categorized according to its content . A total of 47,236 features were extracted from the documents and indexed . Each document is represented as a vector of the features it contains .
TWIT is a subset of Twitter , containing 284 million follower relationships , 3 million user profiles , and 50 million tweets . We filtered the dataset to obtain only hashtagged tweets , which left us with 9 million tweets from 140,000 users . For each tweet , the dataset contains information about the tweet content , ID , creation time , re tweet count , favorites , hashtags and URLs .
KC was used in the “ Third International Knowledge Discovery and Data Mining Tools Competition ” . It contains information about TCP connections . Each connection is described by 41 features , such as duration , protocol , bytes sent , bytes received etc .
For all data sets , in order to simulate multiple streams , we distributed the data between the nodes in round robin fashion . Results are presented for 10 streams , and in Section 5.3 we present some results for communication reduction for up to 1,000 streams ( the reduction in computational overhead does not depend on the number of streams ) . 5.2 Computational overhead reduction
Next we summarize the main results of this paper – the reduction in running time for monitoring the four functions discussed in Section 4 . Then we briefly summarize the communication reduction results .
In Fig 6 we present a summary of the running times for GM and CB , on the various functions and data sets ; details are provided in Sections 521 to 524
Figure 6 : Running times for a local condition check . “ SN ” and “ FN ” stand for previous methods to monitor PCA Score ( see Section 524 ) Note logarithmic scale .
521 Pearson correlation coefficient We evaluated PCC on REU , where every document may be labeled as belonging to several categories . The most frequent category is “ CCAT ” ( the “ CORPORATE/INDUSTRIAL ” category ) . In the experiments our goal was to select features that are most relevant to this category , ie whose PCC with the category is above a given T . Each node holds a sliding window containing the last 6,700 documents it received ( this is roughly the number of documents received in a month ) . We monitored the correlation of “ CCAT ” with the features “ Bosnia ” and “ Febru ” . Run time evaluation . The majority of GM ’s run time is spent on testing for sphere intersection with the PCC surface . To solve this problem we used the Gloptipoly global optimization package [ 16 ] . In CB , the local conditions for PCC monitoring are very simple , and only require computing the functions composing the PCC and their derivatives ( Section 41 )
The experiments demonstrated that the run time of checking the local condition a single time , for the CB method , is almost four orders of magnitude lower than for GM ( see Table 1 ) . Note – to reduce space , the tables also include results for inner product and Csim5 .
Function
Dim
PCC
Inner Prod Inner Prod
Csim
3
2050 1250 100
Run time ( sec . ) GM CB 0.58
27.4E 04 18.2E 04
170
0.67E 04 1.35E 04 0.89E 04 1.67E 04
Speedup
8657.7
20.3 20.45
1,020,000
Table 1 : Run time for checking the local condition CB vs . GM .
Inner product
522 We monitored the inner product on REU and TWIT . As in [ 12 ] , we calculated the inner product of feature vectors from two streams ( created by splitting the records ) . For REU , we used the top 2050 features left after removing features which appear in less than 1 % of the documents . We used the NLTK [ 5 ] package to tokenize and stem the tweets in TWIT , and then selected the top 1250 features , ignoring features appearing in less than 0.1 % of the tweets .
In the REU experiment , each node held a sliding window of the last 6,700 documents , while in TWIT each node held a sliding window containing the last 1000 tweets . We used threshold values between 7000 and 17000 for TWIT , and between 4.9E7 to 5.5E7 for REU .
Run time evaluation .
Although GM requires no optimization to find the closest point on the surface , but only to solve a quartic equation , CB checks local conditions about 20 times faster than GM ( see Table 1 ) ; this is due to the time required to construct and solve the equation , and then check the distinct solutions to see which one yields the closest point . Checking the local conditions requires more time for the REU , since the feature vectors are longer ( 2050 vs . 1250 ) .
523 Cosine similarity To evaluate the computational overhead for the cosine similarity function , we monitored both REU and TWIT . Data was the same as for the inner product experiments ( see Section 522 for more details ) .
Run time evaluation .
The run time of checking a local condition a single time in GM is almost 3 minutes , while for CB it is less than 0.2 milliseconds ( See Table 1 ) .
5In the PCA Score experiments ( Section 524 ) we compared CB to three different methods , hence the results are provided separately ; see Table 2 .
1100100001000000100000000REUREUTWITREUTWITKCPCCIPCsimPCARuntime ( microsecondes)CBGMSNFN1692 524 PCA Score For monitoring the PCA Score function , we compared CB with GM as well as methods based on the Frobenius norm ( FN ) and spectral norm ( SN ) perturbative bounds , described in [ 17 ] ( see also Section 44 ) All methods except CB require solving complex optimization problems , which were implemented using Matlab and the CVXOPT package [ 1 ] .
We monitored the PCA Score over KC using 10 nodes , each holding a sliding window of the last 100 feature vectors . We ran experiments with threshold values T ranging between 0.8 and 0.95 , and effective dimension values ranging from 3 to 6 .
The experiments show that the three methods which were compared with CB – SN , FN and GM – offer a trade off between communication cost and run time .
GM achieves the best communication cost of the three but is also the slowest method . FN is faster than GM but its communication cost is slightly higher . SN is the faster of the three by far , but it achieves relatively poor communication reduction . CB improves on all three methods , achieving better communication cost than GM and better run time than SN .
Run time evaluation . Run time results for monitoring the PCA Score are displayed in Table 2 . The table shows average run time of a single round of each method the as well as the speedup factor achieved by CB .
CB is about 3 times faster than SN , two orders of magnitude faster than FN , and three orders of magnitude faster than GM . Note that while SN ’s runtime results are better than GM ’s , it achieves a rather poor reduction in communication ( Fig 7 )
CB run time
0.0086
CB speedup
1
SN 0.027 3.20
FN 2.01
GM 9.57
232.81
1105.95
Table 2 : Run times ( seconds ) for monitoring the PCA Score over KC . 5.3 Communication overhead reduction
While the work presented here focuses on reducing computational overhead , we also briefly provide results on its performance in reducing overall communication . To evaluate the communication cost , we measured the number of messages sent . The naive method , in which every message is sent to the coordinator , is used as a common baseline . At the opposite extreme , we compared to a hypothetical algorithm , which alerts only when the threshold condition is locally violated , ie when f ( vi ) ≥ T for some local vector vi . Clearly , every monitoring algorithm will have to alert in such a case . However , to maintain correctness , the local conditions have to adhere to the more restrictive constraint ) ≤ T . Since the constraints of every correct f ( v1++vk algorithm are more restrictive , it will issue more alerts , leading to a higher communication cost ( unless , of course , f is convex ) . We refer to this super optimal bound – the number of local violations – as RLV ( real local violations ) ; if the ratio between the number of messages sent by a certain algorithm and the number RLV sent is close to 1 , then this algorithm can hardly be improved . k
Figure 7 shows a summary of the communication required by CB , GM , and RLV for the functions we studied as well as
SN and FN for the PCA Score function . Each bar represents the results across multiple thresholds and datasets . CB is always better than GM . In most cases CB is close to the super optimal lower bound RLV , meaning it can be hardly be improved . Note that while FN and SN achieved better runtimes than GM ( Table 2 ) they have higher communication costs . CB did better than the competing methods ( GM , FN , SN ) in both runtime and communication costs .
Figure 7 : Communication reduction summary ( Lower is better ) . y axis is ratio to naive . Each bar represents the results across multiple thresholds and datasets . The SN bar is cropped ( actual ratio is 1.4 )
We also tested the effect of the window size on the communication cost ( Fig 8 ) . The results can be explained by the slower change in the function ’s value when the window size increases , thus making the monitoring problem easier .
Figure 8 : Communication cost as a function of window size for Inner Prod on TWIT ( lower is better ) .
To test the scalability of CB , we ran experiments with up to 1,000 nodes . Fig 9 shows the results . The advantage of both CB and GM ( and RLV ) over the naive grows with the number of nodes , while CB maintains its superiority over GM .
Figure 9 : Relative communication cost for up to 1000 nodes ( lower is better ) .
000501015020250303504CSIMINNER PRODPCCPCARatio to naiveRLVCBGMFNSN00020040060080101201401601000200030004000500060007000Ratio to naiveWindow sizeGMCBRLV0001002003004005006020040060080010001200Ratio to naiveNumber of nodesGMCBRLV1693 6 . CONCLUSIONS
We presented a new method , CB , to monitor threshold functions over distributed streams . The novelty lies in that the monitoring takes place directly over functions , as opposed to previous methods which require solving very difficult optimization problems .
We presented a general paradigm for implementing CB and demonstrated its superiority over previously known methods for four important functions , achieving one to six orders of magnitude run time improvement , while also reducing the communication cost .
With the move towards the Internet of things , smart home , smart cities etc . , the deployment of resource constrained devices is expected to exponentially increase . Systems composed of these devices will have to perform complex monitoring tasks in real time ; hence , the need for computationally efficient solutions , such as the one presented here , is expected to increase .
Future work will concentrate on further applications , as well as on more theoretical directions , eg studying alternative methods to “ convexize ” monitoring problems .
7 . ACKNOWLEDGMENTS
The research leading to these results has received funding from the [ European Union ’s ] Seventh Framework Programme [ FP7 ICT 2013 11 ] under grant agreement N◦619491 and N◦619435 and from the European Commission Horizon 2020the Framework Programme for Research and Innovation ( 2014 2020 ) under grant agreement N◦688380 .
The authors are very grateful to four anonymous reviewers . All their remarks will be incorporated in the planned journal submission .
8 . REFERENCES [ 1 ] http://cvxoptorg/ [ 2 ] See survey in http://tinyurlcom/lr4zhrk [ 3 ] C . Arackaparambil , J . Brody , and A . Chakrabarti .
Functional monitoring without monotonicity . In ICALP ( 1 ) , 2009 .
[ 4 ] B . Babcock and C . Olston . Distributed top k monitoring . In
SIGMOD , New York , NY , USA , 2003 . ACM .
[ 5 ] S . Bird . Nltk : The natural language toolkit . In
COLING/ACL , COLING ACL ’06 , pages 69–72 , 2006 .
[ 6 ] S . Boyd and L . Vandenberghe . Convex Optimization .
Cambridge University Press , 2004 .
[ 7 ] J . Brody and A . Chakrabarti . A multi round communication lower bound for gap hamming and some consequences . In IEEE CCC , pages 358–368 , 2009 .
[ 8 ] S . Burdakis and A . Deligiannakis . Detecting outliers in sensor networks using the geometric approach . In ICDE , 2012 .
[ 9 ] G . Cormode . The continuous distributed monitoring model .
SIGMOD Record , 42(1):5–14 , 2013 .
[ 10 ] G . Cormode and M . N . Garofalakis . Approximate continuous querying over distributed streams . ACM Trans . Database Syst . , 33(2 ) , 2008 .
[ 11 ] K . Fan . On a theorem of weyl concerning eigenvalues of linear transformations i . In Proceedings of the National Academy of Sciences , volume 35(11 ) , pages 652–655 , 1949 .
[ 12 ] M . N . Garofalakis , D . Keren , and V . Samoladas .
Sketch based geometric monitoring of distributed stream queries . PVLDB , 6(10):937–948 , 2013 .
[ 13 ] N . Giatrakos , A . Deligiannakis , M . Garofalakis , I . Sharfman , and A . Schuster . Prediction based geometric monitoring over distributed data streams . In Proceedings of the 2012 ACM
SIGMOD International Conference on Management of Data , pages 265–276 . ACM , 2012 .
[ 14 ] R . Gupta , K . Ramamritham , and M . K . Mohania . Ratio threshold queries over distributed data sources . In ICDE , 2010 .
[ 15 ] R . Gupta , K . Ramamritham , and M . K . Mohania . Ratio threshold queries over distributed data sources . PVLDB , 6(8):565–576 , 2013 .
[ 16 ] D . Henrion , J B Lasserre , and J . Lfberg Gloptipoly 3 : moments , optimization and semidefinite programming . Optimization Methods and Software , 24(4 5):761–779 , 2009 . [ 17 ] L . Huang , X . Nguyen , M . N . Garofalakis , J . M . Hellerstein ,
M . I . Jordan , A . D . Joseph , and N . Taft . Communication efficient online detection of network wide anomalies . In INFOCOM , 2007 .
[ 18 ] B . Kanagal and A . Deshpande . Online filtering , smoothing and probabilistic modeling of streaming data . In ICDE , 2008 .
[ 19 ] S . R . Kashyap , J . Ramamirtham , R . Rastogi , and P . Shukla . Efficient constraint monitoring using adaptive thresholds . In ICDE , pages 526–535 , 2008 .
[ 20 ] R . Keralapura , G . Cormode , and J . Ramamirtham .
Communication efficient distributed monitoring of thresholded counts . In SIGMOD , 2006 .
[ 21 ] D . Keren , G . Sagy , A . Abboud , D . Ben David , A . Schuster , I . Sharfman , and A . Deligiannakis . Geometric monitoring of heterogeneous streams . IEEE Trans . Knowl . Data Eng . , 26(8):1890–1903 , 2014 .
[ 22 ] D . Keren , I . Sharfman , A . Schuster , and A . Livne . Shape sensitive geometric monitoring . IEEE Trans . Knowl . Data Eng . , 24(8 ) , 2012 .
[ 23 ] A . Lakhina , M . Crovella , and C . Diot . Diagnosing network wide traffic anomalies . In ACM SIGCOMM 2004 , pages 219–230 , 2004 .
[ 24 ] A . Lazerson , I . Sharfman , D . Keren , A . Schuster , M . N . Garofalakis , and V . Samoladas . Monitoring distributed streams using convex decompositions . PVLDB , 8(5):545–556 , 2015 .
[ 25 ] F . Li , K . Yi , and J . Jestes . Ranking distributed probabilistic data . In SIGMOD , 2009 .
[ 26 ] C . G . OJ Okunola , A . Uzairu and G . Ndukwe . Assessment of gaseous pollutants along high traffic roads in kano , nigeria . International Journal of Environment and Sustainability .
[ 27 ] T . Palpanas . Real time data analytics in sensor networks . In
Managing and Mining Sensor Data , pages 173–210 . 2013 .
[ 28 ] T . Palpanas , D . Papadopoulos , V . Kalogeraki , and
D . Gunopulos . Distributed deviation detection in sensor networks . SIGMOD Record , 32(4):77–82 , 2003 .
[ 29 ] J . M . Phillips , E . Verbin , and Q . Zhang . Lower bounds for number in hand multiparty communication complexity , made easy . In SODA , pages 486–501 , 2012 .
[ 30 ] S . Shah and K . Ramamritham . Handling non linear polynomial queries over dynamic data . In ICDE , 2008 .
[ 31 ] I . Sharfman , A . Schuster , and D . Keren . A geometric approach to monitoring threshold functions over distributed data streams . In SIGMOD , 2006 .
[ 32 ] I . Sharfman , A . Schuster , and D . Keren . A geometric approach to monitoring threshold functions over distributed data streams . ACM Trans . Database Syst . , 32(4 ) , 2007 .
[ 33 ] M . Tang , F . Li , J . M . Phillips , and J . Jestes . Efficient threshold monitoring for distributed probabilistic data . In ICDE , 2012 .
[ 34 ] R . Wolff . Distributed convex thresholding . In ACM PODC
2015 , pages 325–334 , 2015 .
[ 35 ] R . Wolff , K . Bhaduri , and H . Kargupta . A generic local algorithm for mining data streams in large distributed systems . IEEE TKDE , 21(4 ) , 2009 .
1694
