Sampling of Attributed Networks from
Hierarchical Generative Models
Pablo Robles Purdue University
West Lafayette , IN USA problesg@purdue.edu
Sebastian Moreno
Universidad Adolfo Ibañez
Viña del Mar , Chile sebastianmoreno@uaicl
Jennifer Neville Purdue University
West Lafayette , IN USA neville@cspurdueedu
ABSTRACT Network sampling is a widely used procedure in social network analysis where a random network is sampled from a generative network model ( GNM ) . Recently proposed GNMs , allow generation of networks with more realistic structural characteristics than earlier ones . This facilitates tasks such as hypothesis testing and sensitivity analysis . However , sampling of networks with correlated vertex attributes remains a challenging problem . While the recent work of [ 16 ] has provided a promising approach for attributed network sampling , the approach was developed for use with relatively simple GNMs and does not work well with more complex hierarchical GNMs ( which can model the range of characteristics and variation observed in real world networks more accurately ) . In contrast to simple GNMs where the probability mass is spread throughout the space of edges more evenly , hierarchical GNMs concentrate the mass to smaller regions of the space to reflect dependencies among edges in the network—this produces more realistic network characteristics , but also makes it more difficult to identify candidate networks from the sampling space .
In this paper , we propose a novel sampling method , CSAG , to sample from hierarchical GNMs and generate networks with correlated attributes . CSAG constrains every step of the sampling process to consider the structure of the GNM—in order to bias the search to regions of the space with higher likelihood . We implemented CSAG using mixed Kronecker Product Graph Models and evaluated our approach on three real world datasets . The results show that CSAG jointly models the correlation and structure of the networks better than the state of the art . Specifically , CSAG maintains the variability of the underlying GNM while providing a ≥ 5X reduction in attribute correlation error .
Keywords Complex Networks ; Network Sampling ; Statistical Network Models ; Attributed Networks
1 Introduction Since many complex systems can be represented as networks ( where nodes and edges represent entities and their relations , respectively ) , there has been increased scientific interest in how to model observed network structure effectively—to analyze the properties of the associated systems . Recently many network analyses have focused on data that comprise a single large network ( eg , Facebook , Twitter ) . Moreover , algorithms are often developed using a small number of network datasets gathered from different domains . Thus , it is critical to have generative network models that can produce data samples that exhibit the natural characteristics of real world networks . Models that generate realistic networks can be used for a variety of analytic tasks . For instance , they enable hypothesis testing ( by facilitating assessment of data variability ) , sensitivity analysis ( by facilitating model evaluation under different settings ) , and benchmark testing ( by providing semi synthetic datasets that preserve privacy ) [ 5 ] . The ability to generate network structure has served to support research in the basic area of network science but to date there are few methods to generate networks with correlated node attributes [ 10][16 ] . Furthermore , few can replicate the range of complex characteristics that occur in real world networks . More precisely , the problem of modeling graph structure with correlated node attributes corresponds to modeling the joint distribution P ( G , X ) where P ( G ) and P ( X ) refer to the marginal distributions of the network structure G and node attributes X , respectively . Most of current methods in network science focus on the problem of modeling P ( G ) without considering node attributes P ( X)[11][18 ] . Other methods in statistical relational learning focus on the problem of modeling the attribute distribution conditioned on the network structure [ 4 ] . However , no method effectively represents and samples from the joint distribution P ( G , X ) , due to the complexities of modeling both structure and correlated node attributes ( eg , high dimensionality , sparse graphs , few sample graphs , etc ) While P ( G , X ) could be described by combining these efforts to use P ( X|G)P ( G ) , it is typically impractical to draw samples from P ( X|G ) due to cyclic attribute dependencies . in this work we explore the following– Problem Definition : Let GIN = ( V , E ) be an input network with node attributes XIN , where xi refers to the attribute value of node i ( eg , xi ∈ {0 , 1} for binary attributes ) . Let P = P ( G , X ) be the unknown target distribution which generated < GIN , XIN > . If P ( G ) is the marginal distribution of the network structure estimated with a generative network model M with parameter ΘG , and P ( X ) is
More formally ,
1155 the marginal distribution of node attributes with estimated parameter ΘX , then our objective is to learn a model using the marginals P ( X ) , P ( G ) to construct a proposal distribution Q and sample a pair < GOU T , XOU T >∼ Q such that it is likely to have been drawn from P .1
The Attributed Graph Model ( AGM [ 16 ] ) is a new approach to solve this problem , which approximates sampling from the joint distribution of structure and attributes P ( G , X ) using rejection sampling . The AGM uses Q = P ( G)P ( X ) as a proposal distribution—by first sampling the node attributes from P ( X ) and then using a probabilistic generative network model ( GNM ) to sample edges efficiently from P ( G ) . The proposed edges are accepted with probability proportional to P ( G|X ) ( learned from observed data ) . The AGM is simple to use and performs well with several edge based GNMs , such as the Chung Lu ( CL ) model [ 2 ] and the Kronecker product graph model ( KPGM ) [ 11 ] .
However , the AGM approximation is only accurate when all edge probabilities in the associated GNM are non zero , and performance degrades if the edge probability mass is more concentrated over a subset of node pairs , instead of being spread more evenly over all pairs of nodes . As we will discuss later , some hierarchical GNMs that model more complex dependencies among graph edges produce more concentrated probability mass over smaller regions of node pairs . Thus , the AGM will not perform as well with these GNMs . In this paper , we explore the issue of using hierarchical GNMs to generate attributed networks with correlated attributes and network structure that reflects the characteristics of real world graphs . Namely , we use the mixed Kronecker product graph model ( mKPGM [ 14] ) , a sub class of hierarchical generative models that overcome known limitations [ 19 ] of KPGMs . This hierarchical GNM creates networks by iteratively sampling from a hierarchy of latent variables that represent edges , blocks , and superblocks of edges . The hierarchical process restricts the space of possible networks , conditioned on the sampling of the blocks . Specifically , if a block is not sampled , the associated edges have zero probability ( ie , they cannot be sampled ) . This has significant impact on the model ’s effective sampling space and makes it difficult to use AGM ’s rejection sampling process . for Attributed Graphs ( CSAG ) , a new method to approximate sampling structure and attributes from P ( G , X ) using mKPGMs as the generative network model in the proposal distribution . As in AGM , we use a proposal Q composed by the network distribution and the attribute distribution , ie Q = P ( G)P ( X ) . However , due to the complex GNM hierarchy , we cannot model the target joint P = P ( G|X)P ( X ) directly ( ie , we cannot efficiently calculate the likelihood of a sample , nor can we effectively use rejection sampling ) . Instead , we develop a two stage constrained sampling process to generate from a distribution Q ∼ P : a block sampling stage to generate from the hierarchy , and an edge sampling stage to generate edges , using constraints based on the maximum entropy principle [ 7 ] . The combination increases the accuracy of the sampling process , producing samples with correlated node attribute values and complex network structure that match the marginal models . propose Constrained Sampling
We
To summarize , our contributions are twofold . First , we show both a discussion and empirical evidence of the reduc
1This likelihood depends on the goodness of fit of P ( G ) and P ( X ) . tion in sampling space that results from the use of a hierarchical GNM such as mKPGM—which makes it difficult to model P ( G , X ) with the recently developed AGM sampling method . Second , to address the issue above , we propose CSAG , a 2 stage constrained sampling method that samples a set of blocks from a region of feasibility and then samples edges from the selected block space . CSAG is appropriate to use with mKPGM and other hierarchical GNMs . We provide a detailed implementation of CSAG for mKPGMs . Our experiments on three real datasets show that CSAG outperforms the state of the art by jointly matching attribute correlations and network structure—using orders of magnitude fewer parameters than competing methods . The experiments also show a ≥ 5X reduction in the attribute correlation error with respect to the state of the art .
2 Background and Related Work 2.1 Generative Network Models Let G = ( V , E ) be a graph with set of vertices V and edges E ⊂ V × V . Generative network models sample random networks G from a distribution P ( G ) . Let M(ΘG ) refer to a generative network model with parameters ΘG , which generates a network G through a random sampling process as follows . We define Eij to be a binary random variable , where Eij = 1 indicates that an edge between Vi,Vj ∈ V exists ( ie , eij ∈ E ) , and Eij = 0 ⇒ eij /∈ E . Many GNMs sample networks from a |V|×|V| probability matrix P , where Pij = pij is the probability of the edge eij existing in the network , ie , P ( Eij = 1 ) = pij . We will denote as P + the subset Pij > 0 . Some examples of generative network models appear below . Unique Bernoulli probabilities . An important abstraction/property of GNMs that use the probability matrix P is the fact that they can be simply represented with the pair < U , T > . U is the set of unique Bernoulli probabilities that appear in the matrix P , ie U = {π1 , π2 , . . . , πu , . . .} = unique(P ) . The function unique returns the set of all pij ( ie without repetitions ) . T is a set of lists Tu , which lists the positions where the probability πu appears in P . Later we will see that T can be indexed also by other criteria ( eg by the node attributes ) . For now , it is enough to understand that u indexes T ( Tu ) . Erd˝os R´enyi Model ( ER ) [ 3 ] . In an ER model , random graphs are formed by creating edges between each pair of nodes independently with probability p . It is easy to see that U = {π = p} has only one unique probability and T consists of a single list with all the positions in P . Chung Lu Model ( CL ) [ 2 ] . wiwj/ wk for a sequence of expected degrees of nodes k < wk so that w = ( w1 , . . . , w|V| ) ( CL assumes maxk w2 0 ≤ Pij ≤ 1 ) . In the worst case , although very unlikely , there could be |V|2 unique probabilities πi and T would consist of |V|2 lists of a single element each . Transitive Chung Lu Model ( TCL ) [ 17 ] . TCL is a generalization of CL . Both CL and TCL model the expected degree via a set of weights wi proportional to degree of node i . However , while CL samples each edge independently with probability proportional to wiwj , TCL rewires edges to match clustering coefficient of the original network . The same criteria applies for the size of U and T as for CL . Block two level Erd˝os R´enyi Model ( BTER ) [ 18 ] . Sampling for BTER is done in two phases . First , affinity blocks
In a CL model Pij =
1156 are created in a preprocessing step , where edges in each block are linked with an ER model . Second , connections between blocks are created with a CL model . Since the phase 2 uses a CL model U and T could have the same worst case scenario than CL .
K b2 + K − 1
Kronecker Product Graph Model ( KPGM ) [ 11 ] . Given a b×b parameter matrix ΘG where ∀ i,j θG ij ∈ [ 0 , 1 ] , and K ( the number of Kronecker products ) , KPGM samples graphs as follows . First , it computes P a matrix equal to K−1 Kronecker products of ΘG with itself . Second , it samples G = ( V , E ) from P by sampling each cell independently from Bernoulli(Pij ) . In KPGM , U is |U| = and T consists of |U| lists of edges ( more details in [ 15] ) . mixed Kronecker Product Graph Model ( mKPGM ) [ 14 ] . mKPGM is a hierarchical generative network model that overcomes known limitations [ 19 ] of the KPGM . Given a b×b parameters ΘG where ∀ i,j θG ij ∈ [ 0 , 1 ] , the number of Kronecker products K , and the number of untied levels , mKPGM samples a network as follows . First , it computes P [ 0 ] , a b×b matrix , equal to −1 Kronecker products of ΘG with itself , ie , it computes a KPGM model with parameters ΘG , . Second , it samples G[0]=(V[0 ] , E[0 ] ) from P [ 0 ] by sampling each cell independently via Bernoulli(P [ 0 ] ij ) . Note , |V[0]| = b . Third , it calculates P [ l ] = G[l−1 ] ⊗ ΘG and samples G[l ] for l=1 . . K− The iterations increase the variability in the sampled network G[K− ] ( where |V[K−]| = bK ) . Since G[0 ] . . . G[K−−1 ] represent auxiliary graphs ( in matrices of increasing size ) , where each edge influences a block of possible edges in the next iteration of the hierarchical sampling process ( and they are not present in the final output network ) we refer to them as block matrices : B[0 ] . . . B[K−−1 ] . To simplify notation we will refer to the final sampled network G[K− ] = ( V[K− ] , E[K− ] ) as GOU T = ( VOU T , EOU T ) . Figure 1 illustrates the hierarchical sampling process from Ps to Bs , of increasing size . In mKPGMs , the size of U is b2 for each level of the hierarchy and T consists of b2 lists of edges .
2.2 Attributed Networks and Related Work When developing models to sample attributed networks , it is worth highlighting the importance of accurately modeling the correlation observed among the attributes of linked nodes ( or autocorrelation ) . Autocorrelation is typically observed in networks due to processes such as social influence and homophily [ 13 , 20 ] , and is an important feature exploited in statistical relational learning to improve classification and other statistical inference tasks in network domains [ 4 ] .
The Attributed Graph Model ( AGM ) [ 16 ] approximates sampling from the joint distribution of network structure and node attributes P ( G , X ) using rejection sampling . AGM combines structure and attributes independently in a proposal distribution Q = P ( G)P ( X ) , by first sampling the node attributes from a distribution P ( X ; ΘX ) with parameters ΘX and then using a generative network model P ( G ; ΘG ) with parameters ΘG to sample attributed edges , based on :
P ( Eij|X ) ≈ P ( Eij|ΘG ) · A(f ( xi , xj)|ΘG , ΘX ) where P ( Eij = 1|ΘG ) = Pij is the edge probability defined by the generative network model and A(· ) is the acceptance
Figure 1 : mKPGM sampling process with b = 2 , = 3 , K = 5 . P [ 0 ] is generated as a KPGM . Left : Matrices of probabilities P [ l ] ( white : Pij = 0 black : Pij = 1 ) . Right : Adjacency matrices B[0 ] . . . B[K−−1 ] , EOU T ( black ⇒ block/edge sampled ) . probability based on features between nodes f ( xi , xj ) . The acceptance probability ensures that we add edges eij with correlated attribute values . For training , AGM uses an input network GIN and attributes XIN , to learn ΘX , ΘG , and A(· ) . To illustrate AGM ’s process consider a network where nodes have a single attribute . AGM learns a GNM model from the network structure ( ie , P ( G) ) , fits a Bernoulli distribution to the observed attribute values ( ie , P ( X) ) , and learns A(· ) by comparing the attribute correlations observed in GIN to those produced by an independent sampling process ( ie , P ( G)P ( X) ) . Then , AGM produces a new set of nodes , samples attribute values from P ( X ) for each node , generates candidate edges from the GNM , and accepts/rejects based on the attributes of the incident nodes using A(· ) . AGM has been implemented for GNMs : FCL [ 17 ] , TCL , and KPGM . Another approach to generate samples of attributednetworks is to develop a probabilistic model that jointly incorporates both attributes and structure . There is work in this direction . One example is the multiplicative attribute graph ( MAG ) model [ 10 ] . However , the evaluation of AGM [ 16 ] showed that MAG underperformed AGM for sampling new networks because MAG considers latent node attributes instead of learning from observed node attributes . As we will describe below , the insight of our CSAG method is that sampling from the full joint is difficult due to its high dimensional space , but much of the space has essentially zero probability in some hierarchical GNMs , such as mKPGMs . Our CSAG approach uses constrained sampling to effectively explore only parts of the network space that are likely . It discards random variables with zero probability and exploits information about the desired level of autocorrelation to further constrain the search using the maximum entropy ( MaxEnt ) principle . We will compare CSAG only to AGM since [ 16 ] showed that AGM was significantly better than MAG and other methods .
Finally , it is important to note some existing applications of MaxEnt to related problems . While it has not been used for sampling attributed networks , MaxEnt has been exten
BlocksEdgesB[1]B[0]EOUTPP[1]P[0]1157 sively used in sampling of structure alone , and for inference in attributed networks . The earliest example of MaxEnt for sampling network structure is the Erd˝os R´enyi model [ 3 ] . Modeling and sampling of scale free networks with MaxEnt was explored by [ 8 ] , [ 5 ] studied anomaly detection ( ie , inference ) , and [ 21 ] learned the structure in attributed networks . We note that [ 21 ] is , however , not easy to extend to model the joint distribution for sampling because it prunes away data and scales poorly without this pruning .
3 Sampling Attributed Networks Naive application of AGM with mKPGM A full description of the joint distribution of structure and attributes P ( G , X ) remains an open problem [ 16 ] and AGM is one of the first methods to approximate attributednetwork sampling ( ie sampling from P ( G , X) ) . Thus , one could naively apply AGM using mKPGMs as the component GNM as follows . First , learn an mKPGM from GIN to model P ( G ) , fit a distribution P ( X ) to the observed attributes XIN , and learn A(· ) as usual . Next , produce a new set of nodes and sample their attribute values from P ( X ) . Then , sample blocks from the mKPGM and generate candidate edges based on the sampled blocks ( ie , the results of the penultimate iteration of mKPGM sampling process : B[K−−1] ) . Finally , accept/reject the candidate edges based on the attribute values of the incident nodes .
While this implementation will preserve the network structure modeled by the mKPGM , it will be difficult for the process to match the correlations observed in GIN . AGM assumes the set of non zero edge probabilities in P where pij > 0 , which we will denote P + , has a large cardinality and is as close as possible to |V|2 ( ie , |P +| |P| ) . However , in mKPGMs there is a high probability that many blocks will not be sampled ( see Fig 1 ) , thus E[|P +| ] << |P| , as shown by Lemma 1 below . The restricted space of possible edges in mKPGM limits the possibilities considered in the AGM ’s rejection process and as a result degrades its ability to match the targeted attribute correlations .
Impact of the Generative Sampling Space As we discussed before , mKPGM sampling comprises a hierarchical structure of edges , blocks , and super blocks of edges . A block ij at level l has an associated random variable ( rv ) ij which represents its state : sampled ( B[l ] B[l ] ij = 1 ) or not ij = 0 ) . Here l ∈ {0 , . . . , K − } , with l = 0 correspond(B[l ] ing to the root of the hierarchy and l = K − to the level of edges . A block or edge can be sampled iff its parent is sampled—if a superblock is not sampled , sampling of subblocks or edges is inhibited . In this case , the conditional P ( Eij | pa(Eij ) = 0 ) = 0 , but the marginal P ( Eij ) > 0 . As a consequence , mKPGMs have a reduced conditional space . Specifically , |P +| < |P| given the set of sampled blocks in the hierarchy of an mKPGM model : Lemma 1 . Let P be the matrix of edge probabilities and P + the matrix of non zero edge probabilities conditioned on the sampled blocks in the level K−−1 of an mKPGM . If
ΘG < b2 , then E[|P +| ] << |P| , where K−−1 is the last Proof . Let Θs = ΘG be the sum of the mKPGM param block iteration ( before the generation of edges ) ( figure 1 ) . eters , then E[|P +| ] = ( Θs)K−1 · b2 and |P| = ( b2)K . Then
|P| = ( Θs)K−1 E[|P+| ]
( b2)K−1 . Since Θs < b2 , then E[|P+| ]
|P| << 1 , ie mKPGM has a reduced sampling space . In sparse networks |E| = O(|V| ) and for KPGM family models E[|E| ] = ( Θs)K . Thus , ( Θs)K= O(|V| ) ⇒ ( Θs)K−1 ≈ |V| . Θs Consequently , for mKPGMs E[|P+| ] Θs|V| . Our empirical analysis in Table 2 shows that |P +| for mKPGMs is up to 6 orders of magnitude smaller than |P +| for other GNM models . In general , for non hierarchical GNMs : |P| |P +||V|2 .
|P| ≈ |V|/Θs
|V|2 = 1
The reduced sampling space for mKPGMs constrains the ability to generate edges with correlation between the attributes of the incident nodes ( eg , autocorrelation ) . This is because the correlation is jointly produced by the attribute distribution and the sampling of edges from the GNM . When |P +| < |P| , the potential edges may not exhibit the combinations of attribute values needed to produce ( auto)correlation . For instance , if an unsampled block contain nodes with particular values of an attribute , the correlation that can be obtained will be determined by the values in the remaining blocks . In comparison to GNMs with matrices where |P +| |P| , for iterative sampling methods like AGM , it will be difficult to produce the target level of correlation when using hierarchical GNMs like mKPGM . Our approach As we describe next , we develop a 2 stage constrained sampling method to address the problem of sampling from P ( G , X ) with hierarchical GNMs . First , we use linear programming ( LP ) to sample blocks from the latent space . Then , we sample edges from the sampled blocks . We use MaxEnt [ 7 ] via constraints , maintained through the method of moments .
4 CSAG Sampling Method To solve the problem defined in Section 1 , we propose a novel sampling method to draw the network structure conditioned on the attribute values . Specifically , we fit a distribution P ( X ) to the attributes XIN , and estimate the structure P ( G ) by learning a model M from GIN , where GIN and XIN are the input data by problem definition . Then we use the structure conditioned on the attributes , as a proposal Q = P ( G)P ( X ) . Although Q is easy to sample from ( because assumes independence ) it is not a good approximation of the joint P because P ( G ) and P ( X ) are not independent . To solve this , we calculate the moments f ={fo(GIN , XIN )} of the training data ( attributes and structure ) , and sample a network from Q that matches those moments ie we sample from Q⊂ Q : Q = P ( G|X , f )P ( X ) .
The general goal of the algorithm is to maintain the marginals P ( G ) and P ( X ) and keep only the sampled networks that maintain the characteristics of networks in the joint distribution P . In other words , from the candidate edges we keep only those that match the moments f .
Here we describe the intuition behind our 2 stage sampling algorithm while its full description is provided in 41 First , let us define the moments used for sampling . The set of attributes X = {X1 , X2 , , Xm} create the j ) for eij ∈ E ∀i , j , m , m . We refer the set pairs ( xm of unique labels ( xm j ) as the edge types Ψ ( eg , for an undirected network with a single binary node attribute Ψ = {00 , 01 , 11} ) , and the fraction of edges of each type Ψ i , xm i , xm
1158 the space with blocks of edges that make the target correlations more likely . Specifically , when the basic unconstrained sampling of the GNM2 results in a space ( of sampled blocks ) where it is possible to match f , then the first stage sampling ( with MaxEnt moment matching ) will be sufficient to sample edges with the target correlations . Otherwise , sampling will require a second stage linear program ( LP sampling ) , to sample blocks before the first stage—replacing the basic sampling of that iteration . This is repeated for as many previous iterations as needed ( see Figure 2 ) . Since the number of iterations where second stage sampling is needed is generally small ( ≤ K/2 ) , the process is relatively efficient . The first stage sampling , described by Algorithm MEEdgeSampling , is a general procedure to sample edges and works under the assumption that β can be achieved with the sampled blocks . Consider the case where sampling from an mKPGM leads to a space where a network with the target correlation is feasible . In such case , the edges can be sampled from a set of blocks BK−−1 sample sampled at the iteration K− − 1 of the sampling process . The constraints that must be enforced to sample edges ( at iteration K− ) are two fold : ( 1 ) the structure of the sampled network is the one defined by the GNM , and ( 2 ) the correlations obtained are the same as the input correlations . CSAG constrains every step of the sampling process by including these two restrictions . The former restriction ( to maintain the structure of the GNM ) is guaranteed by using group probability sampling [ 15 ] as follows . Given the set of unique Bernoulli probabilities πu from the GNM3 , we need to guarantee that the resulting samples follow the exact same Bernoulli distribution of the edges . This is accomplished by ensuring the number of edges sampled per πu follows a binomial distribution , as proved in [ 15 ] . The later restriction ( to maintain the correlation ) is guaranteed by matching the moments βj of edges of each type ψj to the moments of the input data . The second stage sampling , described in Algorithm LPBlockSearch , is used to draw blocks , ie it can be applied to all iterations except the last , as opposed to the previous stage that only applies to the last iteration for sampling edges . This stage is necessary to deal with cases where the network space obtained using the generative network model does not allow for networks with the target correlation . In such cases the set of blocks from which edges will be sampled must be realized from regions of high(er ) likelihood such that the edges sampled fulfill the constraints that produce the target correlation .
The objective in the second stage is to find the set χ of blocks to sample in a particular iteration . As we will describe , χ must fulfill constraints so that the blocks selected will maintain both the network structure of the GNM and the attribute structure of P ( X ) , based on the training data . To express these two types of constraints mathematically , we will consider certain conditions that the sampled blocks must follow . Thus , our objective function f ( χ ) is an error function ( to minimize ) that measures how far the samples are from a region that satisfies the conditions .
Since both sampling stages respect the original distribution of the GNM , the structure and the variability of the GNM is maintained in the output network . Both stages are also designed to sample from a region that has non trivial
2Section 2.1 describes mKPGM ’s basic unconstrained sampling . 3As described in 2.1 , GNMs are equivalent to a set of unique Bernoulli probabilities πu ∈ U and their positions Tu .
Figure 2 : Example illustration of the sampling process which combines basic sampling with MaxEnt sampling , and LP search at different levels of the hierarchy , depending on whether the output network meets the desired constraints .
Algorithm GraphSampling 1 : Input : GIN = ( V , E ) , node attrs XIN , GNM M , error 2 : Ouput : GOU T , XOU T , ρOU T 3 : [ Ψ , β , ΘX , ΘG ] ← LearnParameters(GIN , XIN ) 4 : Sample XOU T from P ( X|ΘX ) 5 : Initialize ρOU T = ∞ and lo = K− − 1 6 : while ( |ρIN − ρOU T | > ) AND ( lo ≥ 0 ) do 7 : 8 : 9 :
Sample B[lo ] for l = lo + 1 to K − − 1 do sample ∼M(ΘG ) using basic sampling 2 sample ←LPBlockSearch(M , ΘG , B[l ] B[l+1 ] GOU T ←MEEdgeSampling(M , ΘG , B[K−−1 ] Calculate ρOU T using GOU T and XOU T lo = lo − 1 sample , Ψ , β ) sample , Ψ , β )
10 : 11 : 12 :
={xm in G as β = {β1 , . . . , β|Ψ|} . The moments f = β model the i } Pearson correlation ρ of the attribute vectors Xm = {xm and Xm j } over the pairs eij ∈ E ∀i , j . When m = m , this corresponds to the autocorrelation of a single attribute across edges in the network . We will denote the target correlation as ρIN and the sample correlation as ρOU T . Since ρ is function of β ( and |E| ) we will use β instead of ρ in our discussion . It is worth to note that ρ is jointly defined by the GNM and the distribution of node attribute values . Thus in hierarchical GNMs where the graph structure is constrained , the possible values of ρ in the sample are also constrained .
Given an input graph and its attributes ( GIN , XIN ) , and a GNM M , we sample in a two stage process to generate the attributed network ( GOU T , XOU T ) with correlation ρOU T , as outlined in Algorithm GraphSampling . The task of GraphSampling is to sample the network structure conditioned on the attribute values to match ρIN . The general idea of this two stage sampling method is to apply moment matching whenever possible while sampling edges . When the sampling space is too restrictive however ( ie , it is not possible to sample edges with the target correlations ) , a second stage sampling is used to steer the search towards parts of
G[0]G[K l 1]G[K l]ME samplingBasic samplingIf ρOUT ~ ρIN G[K l]returnelseG[K l 2]Basic samplingG[K l 1]G[K l]ME samplingLP samplingG[K l 2]If ρOUT ~ ρIN G[K l]returnelsereturnLP samplingG[K l 1]G[K l]ME samplingLP samplingG[K l 2]If ρOUT ~ ρIN G[K l]returnelse1159 mass in the joint distribution—because many networks with high likelihood wrt the GNM will typically not exhibit the desired attribute correlations . Learning Learning ΘX and ΘG is straightforward . In our evaluation , we estimate ΘX using the MLE of the multinomial distribution for X , while for ΘG we use the simulated method of moments of [ 14 ] . Likewise , estimating ρIN is a trivial MLE . In our example , for instance , it simply consists in calculating the fraction of edges with labels Ψ ={00 , 01 , 11} .
Algorithm LearnParameters 1 : Input : GIN = ( V , E ) , node attrs XIN , model M 2 : Ouput : Ψ , β , ΘX , ΘG 3 : Compute Ψ and β ( that define ρIN ) from GIN and XIN 4 : Compute the MLE of ΘX using XIN ∼ f ( X ) 5 : Estimate ΘG from GIN ∼M
4.1 Algorithmic details Algorithm GraphSampling describes CSAG ’s overall framework . GraphSampling first determines the correlation ρIN of the input graph ( the edge types Ψ , and the fraction of edges per type β ) , and learns ΘX and ΘG for the attributes P ( X ) and model M(ΘG ) ( line 3 ) . ΘX is estimated using the MLE of the multinomial distribution of X , while ΘG is estimated with the learning algorithm of the GNM ( eg [ 14 ] is used to learn ΘG for mKPGM ) . The algorithm samples attribute values XOU T for the output nodes with ΘX ( line 4 ) . GOU T is sampled using the two stage sampling based on ΘG . The LP search starts from the set of blocks B[lo ] sample sampled from M at layer lo ( line 7 ) . Then , it searches through the hierarchy ( l = lo . . .K − − 1 ) to ultimately sample B[K−−1 ] sample ( lines 8 9 ) . Then the method of moments uses B[K−−1 ] sample to sample edges ( line 10 ) . lo moves one level up in the hierarchy if the space cannot match ρIN . This process is repeated until |ρIN −ρOU T | ≤ , for some predefined error . The LP search is not realized if ρIN is feasible to obtain ( eg in nonhierarchical GNMs)—then GOU T is sampled directly . It is important to note that there are configurations of attribute values , and target correlations , for which sampling may not be possible . If this is the case , GraphSampling will not satisfy the constraints and the algorithm will terminate ( with lo = 0 ) . In practice , because the networks are sparse and the attributes are low dimensional it is generally possible to find a sample of edges that satisfy the constraints .
Algorithm LPBlockSearch describes the LP block sampling stage . This stage focuses the sampling on relevant regions of the space where networks with the target correlation of node attributes are possible to be drawn . Given the GNM M with parameters ΘG , the set of sampled blocks B[l ] sample , the list of edge types Ψ , and the fraction of edges of each type β , LPBlockSearch returns the set of sample blocks B[l+1 ] sample at level l + 1 such that all possible edges descendent from B[l+1 ] sample have the same proportions as β . LPBlockSearch first calculates U ={π1 , . . . , πk} , and T—the set of unique probabilities of M and the locations of possible sample blocks per unique probability πu , respectively , which can be generated by B[l ] sample . The FOR loop ( lines 4 14 ) performs a linear search per πu . Specifically , it calculates the number of blocks to be sampled ( line 5 ) . The next FOR loop computes sample , Ψ , β sample sampled blocks in l + 1 :
Algorithm LPBlockSearch 1 : Input : M , ΘG , B[l ] 2 : Ouput : B[l+1 ] 3 : ( U , T ) =getUniquePr BLocations(M , ΘG , B[l ] 4 : for u = 1 to |U| do 5 : 6 : 7 : 8 :
Draw nu ∼ Bin(|Tu| , πu ) {# of blocks to sample per πu} for j = 1 to |Ψ| do ej = βj × nu {fraction of possible edges leading to ρIN} Determine Ajk {# of descendent edges of type ψj ∈ Ψ per position tk ∈ Tu} for j = 1 to NΩ do sample ) k=1 Ajk {max # of sampled blocks per A·k} Solve the LP of eq 1 : find min χ using nu , e , A , and ub for j = 1 to NΩ do ubj =|Ψ|
B B[l+1 ] sample= Randomly sample χj blocks from ubj places sample = B[l+1 ] sample ∪ B sample
9 : 10 : 11 : 12 : 13 :
14 : the fraction of edges per type ψj ∈ Ψ that would lead to ρIN ( lines 6 7 ) . Then , the algorithm determines the matrix A with the number of descendent edges of type ψj ∈ Ψ per position tk ∈ Tu ( line 8 ) . Thus , A is a look ahead structure for the possible edges of the blocks sampled with probability πu . In theory the dimension of A is |Ψ| × |Tu| . However , in practice we take only the set of unique columns which makes A much smaller ( |Ψ| × NΩ ) . Let χ be the indices of the blocks to be sampled , then A · χ is ( a |Ψ| dimensional vector with ) the total number of possible descendent edges per edge type ψj ∈ Ψ . Next , the algorithm computes the maximum number of blocks that can be sampled per column in A , which we refer to as a configuration ( lines 9 10 ) . Using these restrictions , the number of edges to sample χ is obtained through LP search ( line 11 , we used Matlab ’s interior point algorithm fmincon [ 9] ) . Given χ , the last FOR loop ( lines 12 14 ) samples the desired number of blocks per configuration ( line 13 ) , generating B[l+1 ] sample ( line 14 ) .
In summary , LPBlockSearch applies the following process per probability πu .
( a ) Number of samples according to the GNM P ( G ) . Since πu is Bernoulli , the number of blocks to sample per πu is nu ∼ Bin(|Tu| , πu ) , where T is the set of locations of possible blocks per unique probability πu
( b ) Number of samples according to correlation in P ( X ) . Let ej = βj × nu be the fraction of edges per type ψj ∈ Ψ that would lead to ρIN . Let A · χ be ( a |Ψ| dimensional vector with ) the total number of possible descendent edges per edge type ψj ∈ Ψ , where matrix A is a look ahead structure on the possible edges for the blocks sampled with probability πu . Then , ej − Aj· · χ must be as small as possible ∀j ∈ [ 1,|Ψ| ] ; where χ is a NΩ dimensional vector with the set of blocks to sample per configuration A·k . The value of χ can be found by minimizing the objective function :
( ej−Aj· · χ ) × 1[(ej − Aj· · χ ) > 0 ]
|Ψ| j minimize subject to f ( χ ) = j χj = nu
0 ≤ χj ≤ ubj ∀j
Minimizing f ( χ ) gives a solution where the fraction of possible descendent edges per type is similar to the fraction of edges β of the input network , which leads to the target correlation ρIN . Notice f ( χ ) is the summation of the differences
( 1 )
1160 sample , Ψ , β
Algorithm MEEdgeSampling 1 : Input : M , ΘG , B[K−−1 ] 2 : Output : GOU T 3 : ( U , T ) =getUniquePr ELocations(M , ΘG , B[l ] 4 : for u = 1 to |U| do Draw nu ∼ Bin(|Tu·| , πu ) {# edges per unique probabil} 5 : Set Ne = Ne + nu {Total # edges to be sample} 6 : 7 : Draw Γ = [ γ1 , . . . , γ|Ψ| ] ∼ M ult ( Ne ; β ) {# edges per edgetype to match ρIN} 8 : for u = 1 to |U| do 9 :
{# edges per edge sample , Ψ ) nu ; Γ Ne
Draw Y = [ Y1 , , Y|Ψ|]∼ M ult type for πu} for j = 1 to |Ψ| do
E = Sampling Yj edges at random from Tuj locations . EOU T = EOU T ∪ E γj = γj − Yj {Adaptative process to match the moments} Ne = Ne − Yj
10 : 11 : 12 : 13 :
14 : between the expected number of possible edges according to β and the actual number of descendent edges per edge type . If ( ej −Aj··χ ) is negative , the value ( 1[(ej−Aj··χ ) > 0 ] ) is zero and the term is not considered ( ie , enough edges exist satisfying the condition ) . This forces the function to apply constraints for all conditions where not enough edges exist .
The constraint j χj = nu guarantees that the sum of the number of blocks to sample , per edge type , is equal to the desired number of blocks to sample nu . This constraint forces sampling a network with a high likelihood with respect to P ( G|M(ΘG) ) , because nu edges are sampled from the unique πu . The inequalities 0 ≤ χj ≤ ubj ∀j ensure that χ is a valid solution , ie each χj is greater than zero and less than the number of possible blocks .
Algorithm MEEdgeSampling describes the edge sampling stage . In this stage of the sampling process , the network is sampled by drawing edges from the graph model M conditioned on the node attributes . The edges accepted must respect a sampling invariant of the sampling process , namely , that the fraction of edges of each type are the same as those in the input network . Given the GNM M with parameter ΘG , the set of sample blocks B[K−−1 ] sample , the list of edge types Ψ , and the fraction of edges of each type β , MEEdgeSampling returns the sampled graph GOU T . MEEdgeSampling calculates U = {π1 , . . . , πk} and T—the set of unique probabilities and the locations of possible sample edges . In order to optimize sampling we transform T from a vector to a matrix indexed not only per unique probability πu but also per edge type ψj ∈ Ψ . Thus , Tuj contains a matrix with the position of edges sampled with probability πu that correspond to edge type ψj , i.e dim(T ) = |U|×|Ψ| . Then , MEEdgeSampling computes the number of edges nu per unique probability and the total number of edges Ne to sample in the final network ( lines 4 6 ) . With Ne and β , it determines Γ ( the total number of edges per type that must be sampled to match ρIN ) . Edge sampling is realized by the FOR loop ( lines 8 14 ) . This loop calculates the number of edges per edge type Y to be sampled given a unique probability πu ( line 9 ) . This matches moments by sampling networks with the original proportion of edges β , ie ρIN . Lines 10 to 14 sample edges per edge type given Y . Specifically , Yj edges are sampled from the |Tuj| possible locations with probability πu and edge type Ψj . The sampled edges are added to the sampled GOU T . Finally , the number of edges per edge type
γj , and Ne are updated to avoid possible errors given the multinomial sampling process ( lines 13 14 ) . Lines 13 and 14 are crucial to generate networks able to match ρIN for all unique probabilities and edge types , unless the number of edges to sample ( Y ) is smaller than the possible positions Tuj . For a unique probability πu consider Yj > Tuj , ( ie it is impossible to sample the required number of edges ) . These lines distribute Yj − Tuj edges among the other edge types , to obtain the number of edges nu for πu . Since other edgetypes may be oversampled for πu , the lines recalculate the proportion of edges for the next unique probabilities such that ρOU T ≈ ρIN .
In summary , algorithm MEEdgeSampling performs the edge sampling stage , as follows :
( a ) The structure of the GNM P ( G ) is maintained by sampling the target number of edges nu ∼ Bin(|Tu·| , πu ) , because πu is Bernoulli distributed [ 15 ] .
( b ) The attribute distribution P ( X ) is guaranteed by maintaining the correlation as defined by β , Ne : Γ = [ γ1 , . . . , γ|Ψ| ] ∼ M ult ( Ne ; β ) .
These two criteria are combined to sample the number of edges per unique probability πu and per type ψj : Y = [ Y1 , , Y|Ψ|]∼ M ult
. nu ; Γ Ne
As in LPBlockSearch , GOU T has high probability with respect to P ( G|M(ΘG) ) . By respecting the original distribution of the GNM , the structure is maintained as well as the variability of the sample networks .
5 Experimental Results We compare our sampling method , CSAG , using mixed Kronecker Product Graph Model ( CSAG mKPGM ) , with AGMmKPGM , AGM Chung Lu ( AGM CL ) , and AGM Transitive Chung Lu ( AGM TCL)4 . We note that AGM TCL is the state of the art , outperforming AGM CL and MAG [ 16 ] . However , AGM TCL can not be applied to directed networks [ 17 ] . The results confirm that CSAG mKPGM accurately captures the structure and correlation of the input networks , while creating networks with enough variability . 5.1 Datasets We evaluate our framework on three real world datasets : CoRA citations network [ 12 ] , Facebook wall postings from Purdue University , and the US patent citation network [ 6 ] . The CoRA network comprises 11,881 papers with 31,482 citations between them . We use the categorical attribute AI ( AI =1 iff a paper ’s topic is in the field of Artificial Intelligence ) . The Facebook network contains wall postings of 449,748 users with 1,016,621 posting edges . We use two profile attributes : Religion ( R=1 indicates a user ’s Religious Views contains “ christ ” ) and Politics ( P=1 indicates a user ’s Political Views contains “ conservative ” ) . The US patent citation network contains 3,774,768 patents with 16,528,948 citation edges . We use the attribute Category with six possible values : Chemical , Computers and Communications , Drugs and Medical , Electrical and Electronic , Mechanical , and Others . For all networks , we model the attribute distributions with multinomials and use MLE estimation .
4Code available rdppd1qvqwjgj29/CSAG.zip?dl=0 data and at wwwdropboxcom/s/
1161 Table 1 : Correlation modeled for CoRA , Facebook , and Patents using AGM and CSAG . Best performers : Values closest to original are bolded . Values corresponding to nonrejected hypothesis under α = 0.01 are marked with *
Model
Original
AGM TCL AGM CL mKPGM
AGM mKPGM CSAG mKPGM
CoRA
AI
0.8373 0.8388*
–
0.2884 0.6945
Correlations
Facebook
R
0.1319 0.1369
–
0.0311 0.1253
P
0.2231 0.2364
–
0.0076 0.0286
RP
0.1873 0.1945
–
0.0391 0.0807
0.8370*
0.1317*
0.2225*
0.1870*
Patent
0.7097
–
0.7111 0.6310 0.7260 0.7074
5.2 Models Since AGM is the best performer in [ 17 ] , we compare CSAGmKPGM against AGM mKPGM and AGM TCL ( AGM CL was used for directed network ) . We use TCL as described in [ 17 ] . We trained mKPGM using the simulated method of moments [ 14 ] with the following features : average number of edges , clustering coefficient , geodesic distance ( approximated by a sample of nodes ) , size of largest connected component , and number of non zero degree nodes . Table 3 shows the values of the learned parameters . As we will discuss , due to its use of mKPGM , CSAG requires significantly fewer parameters than AGM .
Our sampling method introduces a constraint on the order of attribute values that are generated for the output nodes . To understand the rationale for this choice , we note that in practice random attribute samples can result in a scenario where networks with the desired structure and correlation have probability close to zero due to the specifics of ΘG . This is because the GNMs create edges at particular locations in the adjacency matrix . Thus it can be difficult for CSAG to produce high correlations with arbitrary orderings of the attribute values ( over the random nodes ) . While this issue could be solved more generally by iteratively reassigning attribute values as part of the search process , for this work we randomly generated the attribute values and then partially sorted ( over the nodes in the matrix ) before randomly sampling the edges . Sorting starts the sampling from a location that might likely , although not always , result in a successful search . Although this process introduces a bias in a selection of edges geared to match the target correlation , sorting the labels does not affect the distribution nor the sampled graph because the nodes are all random .
5.3 Evaluation and Results With the learned parameters ( Table 3 ) , we sampled 50 networks for CoRA with attribute AI , 20 networks for Facebook with both attributes ( Religion and Politics ) , and a single network for Patent because of AGM ’s run time limitations . We analyze both the attribute correlations and the graph structures of the sampled networks .
Figure 3 summarizes both network structure and attribute correlation errors in a single plot . The x axis comprises the 3 dimensional Kolmogorov Smirnov distance of the CDFs of the three network characteristics described before , between the models and the original data . The y axis comprises the relative error of the correlation , ie |ρIN − ρOU T|/ρIN . As the figure shows , CSAG ( enclosed by the triangle ) has the lowest errors in all three datasets for both graph structure and attribute correlation . As we will discuss , CSAG also provides relatively faster performance than AGM and uses orders of magnitude fewer parameters .
Figure 3 : Performance of methods ( shape coded ) per dataset ( color coded ) . Values towards the bottom left are better .
Table 2 : Non zero edge probabilities for mKPGM and TCL/CL modeling the three datasets .
Dataset
CoRA
Facebook
Patent count of p(eij ) = 0 mKPGM CL and TCL 1.41 × 108 2.60 × 105 6.35 × 106 2.02 × 1011 4.53 × 107 1.42 × 1013
531 Feature Correlations Our first set of evaluations are based on the mean attributecorrelation of the sampled networks . The measurements of the mean correlation are shown in Table 1 . These results show that , though mKPGM has a very restricted sample space ( Table 2 ) , in all cases CSAG mKPGM can capture attribute correlation . Indeed , CSAG mKPGM is the best model to capture correlation except for AGM CL in Patents data where CSAG mKPGM gives competitive performance . The results confirm the importance of our CSAG framework to search over the space of possible networks to sample edges that match the target correlations . Moreover , we can observe that AGM mKPGM performs badly when AGM ’s assumptions are violated , producing quite different correlations from the input . We verified these results statistically . Since population variance is unknown for the original networks , we used a T test with α = 0.01 and null hypothesis H0 : true correlation=model ’s correlation ( assuming the true correlation to be that of the original network ) . H0 is never rejected for our framework while H0 is rejected for TCL ( values with non rejected H0 are marked with ∗ in Table 1 ) . The test cannot be applied to Patents because there is only one network available . This confirms that CSAG in combination with mKPGM provides the closest match to the correlations in the original network datasets .
532 Graph Structure In our second set of evaluations , we investigated whether the models capture three important structure characteristics of real networks : degree , clustering coefficient ( CC ) , and hopplot ( HP ) . To evaluate the ability of the models to capture these characteristics , we compared the cumulative distribution functions ( CDFs ) of the sampled vs . the original networks , visualizing not only the median but the variability of the sampled networks through the plot of the first and ninety nine percentiles ( in the case of CoRA and Facebook ) . Additionally , we compared two more undirected network characteristics , not included in the mKPGM learning process : the K core distribution [ 1 ] and eigenvalue distances . We measured the maximum absolute distance between the K core
00204060812500012500125012KS distance of network characteristicsCorrelation Relative ErrorCORA AGM TCLCORA AGM mKPGMCORA CSAGFACEBOOK AGM TCLFACEBOOK AGM mKPGMFACEBOOK CSAGPATENT AGM CLPATENT AGM mKPGMPATENT CSAG1162 Figure 4 : CDF of network characteristics ( median , 1st , and 99th percentile ) . Left to right : degree , cluster coefficient , and hop plot . Top to bottom : CoRA , Facebook , and Patents .
Table 3 : Learned parameters for mKPGMs in CoRA : K = 9 , = 6 , Facebook : K = 12 , = 9 , and Patent : K = 14 , = 10
 .94
ΘG
CoRA .01 .86
.03 .40 .95
  .99
ΘG
F acebook
.01 .01
.07 .65 .98
  .99
.01 .46
ΘG
P atents .01 .81 .01
.01 .01 .98

CDFs of the sampled vs . the original networks , and compared the largest 10 eigenvalues of each network using the standardized absolute distance between them .
Figure 4 shows the cumulative distribution function ( CDF ) of three different characteristics . We compare CSAG with the best performing AGM ( ie AGM TCL , which outperforms MAG [ 10] ) . The results show CSAG with mKPGM captures most characteristics of the real networks using only 9 parameters . Moreover , the results also show variability in the generated networks , as can be observed in the dashed lines of the plots in the first two rows , which represent the 1st and 99th percentile of the set of samples . ( Recall , we generated only a single network for Patents due to AGM ’s restrictive sampling time . )
While AGM also captures the characteristics of the original networks , AGM requires 11881 , 449748 , and 3774768 parameters to model CoRA , Facebook , and Patents respectively . One exception is the surprisingly poor fit of AGM for Patents HP and CC ( recall , we did not use TCL because it is only defined for undirected networks ) . As expected , AGMTCL also shows significantly less variability than CSAG with mKPGM .
We further analyze the variability in network characteristics in two ways . First , we compare CSAG mKPGM ’s with
Table 4 : K core and eigVal distances [ undirected networks ] CoRA and Facebook ( smallest distances to original bolded ) .
Model
AGM TCL mKPGM
AGM mKPGM CSAG mKPGM
CoRA
Facebook
K core 0.1069 0.0838 0.1073 0.1255 eigenvalue
0.5438 0.2737 0.3059 0.1647
K core 0.3281 0.3398 0.3458 0.3409 eigenvalue
0.6608 0.2707 0.2825 0.3008 mKPGM ’s variance , using an F test ( α = 0.01 ) for distinct values ( vertical slice ) of the characteristics in Figure 4 . We assume normality for each slice . In this case H0 : the variance of a model is equal to the variance of mKPGM . For CC , HP , and degree CSAG fails to reject H0 ( ie maintains mKPGM ’s variance ) for 100 % 80 % and 91 % of the slices for CoRA . Facebook ’s results were similar . Second , we asses the variance reduction of AGM vs . CSAG . AGM ’s average variance reduction ( compared to CSAG ) is as follows : ( 1 ) For CoRA , the CC , degree , and HP , is reduced by 88 % , 76 % , and 70 % respectively ; ( 22 ) For Facebook , CC is reduced by 19 % , but degree and HP are similar for AGM and CSAG .
Table 4 shows the K core and eigenvalue distances for CoRA and Facebook using all models . The K core distances for CoRA and Facebook are similar across models , confirming that both models ( mKPGM and TCL ) show similar connectivity and community structure in the sampled graphs compared to the original network . In contrast , mKPGMs have considerably smaller eigenvalue distance than AGMTCL . In fact , CSAG mKPGM obtains the lowest eigenvalue distance across models in CoRA , which indicates the similarity of the most important nodes compared to the original network . In contrast , AGM TCL shows surprisingly high
1163 eigenvalue distances for both CoRA and Facebook . Once again , there are no results for Patents dataset because these measures are calculated only for undirected networks . Finally , sampling with CSAG is faster than with AGM . For Facebook the mean ±1 stdev runtimes using a MATLAB implementation on a laptop with 2.8 GHz Intel Core i7 and 16 GB in RAM are : CSAG:12.42± 0.55s vs AGM:59.15± 4.07s , respectively . In the more restricted scenario of CoRA , AGM is faster ( CSAG:9.9 ± 0.65s vs AGM:1.5 ± 0.15s ) but AGM does not match the correlation . CSAG is an order of magnitude faster in Patents ( CSAG:100.7s vs AGM : 13616s )
In summary , the results show that CSAG achieves : ( 1 ) Overall best fit of attribute correlation ( smallest relative error ) and network structure ( smallest KS distance ) . Specifically , CSAG produces ≥ 5X reduction in attribute correlation error compared to AGM TCL . Moreover , AGM TCL cannot model correlation consistently and cannot be used with directed networks ; ( 2 ) More effective sampling—the exponential complexity of sampling from the full joint is relieved in CSAG by having a proposal distribution and few rejected samples . This is reflected in the improvement in runtime compared to AGM ; ( 3 ) Variability of the GNM is maintained for 80 % − 100 % of the values of the characteristics ; ( 4 ) Fewer parameters required compared to AGM .
6 Discussion and Conclusion There are two main contributions of this paper . First , we showed that hierarchical GNMs , such as mKPGM , have a limited size of the matrix of conditional edge probabilities which restricts iterative sampling of edges with targeted attribute correlation . This constrains AGM ’s performance as a result . Second , we proposed CSAG , a novel method to samples networks using mKPGM as the structural component of a proposal distribution . Our method uses a two stage constrained sampling : an LP stage to sample blocks and a moment matching stage to sample edges . While the LP stage is strictly for models with constrained space , the moment matching is general enough to work with other types of models as well . CSAG ’s implementation with mKPGM provides the following benefits , as verified experimentally with 3 real datasets : ( 1 ) It jointly models network structure and correlated attributes , obtaining cumulative distribution functions ( CDFs ) and correlations close to the original networks . ( 2 ) It does not affect the network variability of the original GNM , as observed in the CDFs . ( 3 ) It captures the attribute correlations better than AGM , when using the same GNM . Acknowledgements The authors thank the reviewers for their useful comments . This research is supported by NSF under contracts IIS1149789 , IIS 1219015 , and CCF 0939370 . Sebastian Moreno acknowledges the support of “ CONICYT + PAI/Concurso nacional de apoyo al retorno de investigadores/as desde el extranjero , convocatoria 2014 + folio 82140043 ” .
7 References
[ 1 ] J . I . Alvarez Hamelin , A . Barrat , A . Vespignani , and et al . k core decomposition of internet graphs : hierarchies , self similarity and measurement biases . Networks and Heterogeneous media , 3(2):371 , 2008 .
[ 2 ] F . Chung and L . Lu . The average distances in random graphs with given expected degrees . PNAS , 99(25):15879–15882 , 2002 .
[ 3 ] P . Erdos and A . Renyi . On the evolution of random graphs .
In Publication of the Mathematical Institute of the Hungarian Academy of Sciences , pages 17–61 , 1960 .
[ 4 ] L . Getoor , N . Friedman , D . Koller , and A . Pfeffer . Learning probabilistic relational models . In Relational Data Mining , pages 307–335 . Springer Verlag , 2001 .
[ 5 ] Y . Gu , A . McCallum , and D . Towsley . Detecting anomalies in network traffic using maximum entropy estimation . In Proceedings of the 5th ACM SIGCOMM Conference on Internet Measurement , IMC ’05 , pages 32–32 , Berkeley , CA , USA , 2005 . USENIX Association .
[ 6 ] B . Hall , A . Jaffe , and M . Trajtenberg . The NBER patent citation data file : Lessons , insights and methodological tools . NBER working paper 8498 , 2001 .
[ 7 ] E . T . Jaynes . On the rationale of maximum entropy methods . Proceedings of the IEEE , 70(9):939–952 , June 1982 .
[ 8 ] K . Judd , M . Small , and T . Stemler . What exactly are the properties of scale free and other networks ? EPL ( Europhysics Letters ) , 103(5):58004 , 2013 .
[ 9 ] N . Karmarkar . A new polynomial time algorithm for linear programming . In Proceedings of the 16th Annual ACM Symposium on Theory of Computing , STOC ’84 , pages 302–311 , 1984 .
[ 10 ] M . Kim and J . Leskovec . Multiplicative attribute graph model of real world networks . In Algorithms and Models for the Web Graph , volume 6516 of LNCS , pages 62–73 , 2010 .
[ 11 ] J . Leskovec , D . Chakrabarti , J . Kleinberg , C . Faloutsos , and Z . Ghahramani . Kronecker graphs : An approach to modeling networks . JMLR , 11(Feb):985–1042 , 2010 .
[ 12 ] A . McCallum , K . Nigam , J . Rennie , and K . Seymore . Automating the construction of internet portals with machine learning . Information Retrieval , 3(2):127–163 , 2000 .
[ 13 ] M . McPherson , L . Smith Lovin , and J . Cook . Birds of a feather : Homophily in social networks . Annual Review of Sociology , 27:415–445 , 2001 .
[ 14 ] S . Moreno , J . Neville , and S . Kirshner . Learning mixed kronecker product graph models with simulated method of moments . In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 1052–1060 , 2013 .
[ 15 ] S . Moreno , J . Pfeiffer III , S . Kirshner , and J . Neville . A scalable method for exact sampling from kronecker family models . In IEEE 14th International Conference on Data Mining ( ICDM ) , Dec 2014 .
[ 16 ] J . J . Pfeiffer , III , S . Moreno , T . La Fond , J . Neville , and
B . Gallagher . Attributed graph models : Modeling network structure with correlated attributes . In Proceedings of the 23rd International Conference on World Wide Web , WWW ’14 , pages 831–842 , 2014 .
[ 17 ] J . J . Pfeiffer III , T . La Fond , S . Moreno , and J . Neville .
Fast generation of large scale social networks while incorporating transitive closures . In 4th ASE/IEEE International Conference on Social Computing , 2012 .
[ 18 ] C . Seshadhri , T . Kolda , and A . Pinar . Community structure and scale free collections of Erd˝os R´enyi graphs . Physical Review E , 85(5 ) , 2012 .
[ 19 ] C . Seshadhri , A . Pinar , and T . Kolda . An in depth analysis of stochastic Kronecker graphs . Journal of the ACM , 60(2 ) , 2013 .
[ 20 ] C . R . Shalizi and A . C . Thomas . Homophily and contagion are generically confounded in observational social network studies . Sociological methods & research , 40(2):211?239 , May 2011 .
[ 21 ] E . Spyropoulou , T . De Bie , and M . Boley . Interesting pattern mining in multi relational data . Data Mining and Knowledge Discovery , 28(3):808–849 , 2014 .
1164
