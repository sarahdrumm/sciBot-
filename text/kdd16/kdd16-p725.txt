Communication Efficient Distributed Kernel Principal
Component Analysis
Maria Florina Balcan∗ School of Computer Science Carnegie Mellon University ninamf@cscmuedu
Yingyu Liang Computer Science Princeton University yingyul@csprincetonedu
Le Song
College of Computing
Georgia Institute of Technology lsong@ccgatechedu
David Woodruff
Almaden Research Center dpwoodru@usibmcom
IBM Research
Bo Xie
College of Computing
Georgia Institute of Technology boxie@gatechedu
ABSTRACT Kernel Principal Component Analysis ( KPCA ) is a key machine learning algorithm for extracting nonlinear features from data . In the presence of a large volume of high dimensional data collected in a distributed fashion , it becomes very costly to communicate all of this data to a single data center and then perform kernel PCA . Can we perform kernel PCA on the entire dataset in a distributed and communication efficient fashion while maintaining provable and strong guarantees in solution quality ?
In this paper , we give an affirmative answer to the question by developing a communication efficient algorithm to perform kernel PCA in the distributed setting . The algorithm is a clever combination of subspace embedding and adaptive sampling techniques , and we show that the algorithm can take as input an arbitrary configuration of distributed datasets , and compute a set of global kernel principal components with relative error guarantees independent of the dimension of the feature space or the total number of data points . In particular , computing k principal components with relative error over s workers has communication cost ˜O(sρk/ + sk2/ 3 ) words , where ρ is the average number of nonzero entries in each data point . Furthermore , we experimented the algorithm with large scale real world datasets . The experimental results showed that the algorithm produces a high quality kernel PCA solution while using significantly less communication than alternative approaches .
Keywords Kernel method ; Principal Component Analysis ; distributed computing ∗The authors are listed in alphabetical order .
1 .
INTRODUCTION
Kernel Principal Component Analysis ( KPCA ) is a key machine learning algorithm for extracting nonlinear features from complex datasets , such as image , text , healthcare and biological data [ 27 , 26 , 28 ] . The original kernel PCA algorithm is designed for a batch setting , where all data points need to fit into a single machine . However , nowadays large volumes of data are being collected increasingly in a distributed fashion , which poses new challenges for running kernel PCA . For instance , a large network of distributed sensors can collect temperature readings from geographically distant locations ; a system of distributed data centers in an Internet company can process user queries from different countries ; a fraud detection system in a bank needs to perform credit checks on people opening accounts from different branches ; and a network of electronic healthcare systems can store patient records from different hospitals . It is very costly in terms of network bandwidth and transmission delays to communicate all of the data collected in a distributed fashion to a single data center , and then run kernel PCA on the central node . In other words , communication now becomes the bottleneck to the nonlinear feature extraction pipeline . How can we leverage the aggregated computing power in a large distributed system ? Can we perform kernel PCA on the entire dataset in a distributed and communication efficient fashion while maintaining provable and strong guarantees in solution quality ?
While recent work shows how to do linear PCA in a communication efficient and distributed fashion [ 8 ] , the kernel setting is significantly more challenging . The main problem with previous work is that it achieves communication proportional to the dimension of the data points , which if implemented straightforwardly in the kernel setting would give communication proportional to the dimension of the feature space which can be very large or even infinite . Kernel PCA uses the kernel trick to avoid going to the potentially infinite dimensional kernel feature space explicitly , so intermediate results are often represented by a function ( eg , a weighted combination ) of the feature mapping of some data points . Communicating such intermediate results requires communicating all the data points they depend on . To lower the communication , the intermediate results should only depend on a small number of data points . A distributed algorithm then needs to be carefully designed to meet this constraint .
725 In this paper , we propose a communication efficient algorithm for distributed KPCA in a master worker setting where the dataset is arbitrarily partitioned and each portion sits in one worker , and the workers can communicate only through the master . Our key idea is to design a communication efficient way of generating a small representative subset of the data , and then performing kernel PCA based on this subset . We show that the algorithm can compute a rank k subspace in the kernel feature space using just a representative subset of size O(k/ ) built in a distributed fashion . For polynomial kernels , it achieves a ( 1 + ) relative error approximation to the best rank k subspace , and for shiftinvariant kernels ( such as the Gaussian kernel ) , it achieves ( 1 + ) approximation with an additive error term that can be made arbitrarily small . In both cases , the total communication for a system of s workers is ˜O(sρk/ + sk2/ 3 ) words , where ρ is the average number of nonzero entries in each data point , and is always bounded by the dimension of the data d and independent of the dimension of the kernel feature space . This for constant nearly matches the lower bound Ω(sdk ) for linear PCA [ 8 ] . As far as we know , this is the first algorithm that can achieve provable approximation with such communication bounds .
As a subroutine of our algorithm , we have also developed an algorithm for the distributed Column Subset Selection ( CSS ) problem , which can select a set of O(k/ ) points whose span contains ( 1 + ) approximation , with communication O(sρk/ + sk2 ) . This is the first algorithm that addresses the problem for kernels , and it nearly matches the communication lower bound Ω(sρk/ ) for this problem in the linear case [ 10 ] . The column subset selection problem has various applications in big data scenarios , so this result could be of independent interest .
Furthermore , our algorithm also leads to some other distributed kernel algorithms : the data can then be projected onto the subspace found and processed by downstream applications . For example , an immediate application is for distributed spectral clustering , that first computes KPCA to rank k/ and then does k means on the data projected on the subspace found by KPCA ( eg , [ 17] ) . This can be done by combining our algorithm with any efficient distributed k means algorithms ( eg , [ 6] ) .
We evaluate our algorithm on datasets with millions of data points and hundreds of thousands of dimensions where non distributed algorithms such as batch KPCA are impractical to run . Furthermore , comparing to other distributed algorithms , our algorithm requires less communication and fewer representation data points to achieve the same approximation error .
2 . RELATED WORK
There has been a surge of recent work on distributed machine learning , eg , [ 5 , 30 , 20 , 6 ] . In this setting , the data sets are typically large , and small error rate is required . This is because if only a coarse error is needed then there is no need to use large scale data sets ; a small subset of the data will be sufficient . Furthermore , one prefers relative error rates instead of additive error rates , since the latter is worse and harder to interpret without knowing the optimum . Our algorithm can achieve small relative error with limited communication .
Since there exist communication efficient distributed linear PCA algorithms [ 6 , 20 ] , it is tempting to adopt the ran dom feature approach for distributed kernel PCA : first construct m random features and then solve PCA in the primal form , ie , apply distributed linear PCA on the random features . However , the communication of this method is too high . One needs m = ˜O(d/ 2 ) random features to preserve the kernel values up to additive error , leading to a communication of O(skm/ ) = O(skd/ 3 ) . Another drawback of using random features is that it only produces a solution in the space spanned by the random features , but not a solution in the feature space of the kernel .
The Nystr¨om method is another popular tool for largescale kernel methods : sample a subset of data points uniformly at random , and use them to construct an approximation of the original kernel matrix . However , it also suffers from high communication cost , since one needs O(1/ 4 ) sampled points to achieve additive error in the Frobenius norm of the kernel matrix [ 21 ] . A closely related method is incomplete Cholesky decomposition [ 3 ] , where a few pivots are greedily chosen to approximate the kernel matrix . It is unclear how to design a communication efficient distributed version since it requires as many rounds of communication as the number of pivots , which is costly .
Leverage score sampling is a related technique for lowrank approximation [ 29 ] . A prior work of Boutsidis et al . [ 8 ] gives the first distributed protocol for column subset selection . [ 11 ] gives a distributed PCA algorithm with optimal communication cost , but only for linear PCA . In comparison , our work is the first communication efficient distributed algorithm for low rank approximation in the kernel space .
3 . BACKGROUNDS For any vector v , let v denote its Euclidean norm . For any matrix M ∈ Rd×n , let Mi : denote its i th row and M:j its j th column . Let MF denote its Frobenius norm , and M2 denote its spectral norm . Let its rank be r ≤ min{n , d} , and denote its SVD as M = U ΣV where U ∈ Rd×r , Σ ∈ Rr×r , and V ∈ Rn×r . Let [ M ]k denote its best rank k approximation . Finally , denote its number of nonzero entries as nnz(M ) . i=1 ni ) . concatenation of the local data ( n =s
In the distributed setting , there are s workers that are connected to a master processor . Worker i has a local data set Ai ∈ Rd×ni , and the global data set A ∈ Rd×n is the Kernels and Random Features . For a kernel κ(x , x ) , let H denote its feature space , ie , there exists a feature mapping φ(· ) ∈ H such that κ(x , x ) = φ(x ) , φ(x)H . Let φ(A ) ∈ Hn denote the matrix obtained by applying φ on each column of A and concatenating the results . Throughout the paper , we regard any M ∈ Hn as a matrix whose columns are elements in H and define matrix operations accordingly . For example , for any M ∈ Hn and N ∈ Hm , let B = MN ∈ Rn×m where Bij = M:i , N:jH , and let the subscript H .
M2H = tr,MM . When there is no ambiguity , we omit m
The random feature approach is a recent technique to scale up kernel methods . Many kernels can be approximated by 1 i=1 ξωi ( x)ξωi ( y ) where ωi ’s are randomly m sampled . These include Gaussian RBF kernels and other shift invariant kernels , inner product kernels , etc ( [24 , 15] ) . For example , Gaussian RBF kernels , κ(x , y ) = exp(−x − y2/2σ2 ) , can be approximated by 1 √ i=1 zωi,bi ( x)zωi,bi ( y ) 2 cos(ωx + b ) and ωi is from a Gaussian where zω,b(x ) = m m
726 distribution with density proportional to exp(−σ2 ω2 /2 ) and bi is uniform over [ 0 , 2π ] .
In this paper , we provide guarantees for shift invariant kernels using Fourier random features ( the extension to other kernels/random features is straightforward ) . We assume the kernel satisfies some regularization conditions : it is defined over bounded compact domain in Rd , with κ(0 ) ≤ 1 and bounded ∇2k(0 ) [ 24 ] . Such conditions are standard in practice , and thus we assume them throughout the paper . Kernel PCA . An element u ∈ H is an eigenfunction of φ(A)φ(A ) with the corresponding eigenvalue λ if u = 1 and φ(A)φ(A)u = λu . Given eigenfunctions {ui} of φ(A)φ(A ) and eigenvectors {vi} of φ(A)φ(A ) , φ(A ) has the singular decomposition U ΣkV + U⊥Σ⊥V ⊥ , where U , V are the lists of top k eigenfunctions/vectors , Σk is a diagonal matrix with the corresponding singular values , U⊥ , V⊥ are the lists of the rest of the eigenfunctions/vectors , and Σ⊥ is a diagonal matrix with the rest of the singular values . Kernel PCA aims to identify the top k subspace U , since the best rank k approximation [ φ(A)]k = U ΣkV = U Uφ(A ) . Typically , the goal is to find a good approximation to this subspace . Formally ,
Definition 1 . A subspace L ∈ Hk is a rank k ( 1 + , ∆ ) approximation for kernel PCA on A if LL = Ik and flflfl2 ≤ ( 1 + )flflφ(A ) − [ φ(A)]k flfl2 + ∆ . flflflφ(A ) − LL
φ(A )
Kernel PCA leads to immediate solutions for some other nonlinear component analysis ( eg , kernel CCA ) , and provides needed subroutines for tasks like spectral clustering .
Subspace Embeddings . Subspace embeddings are a useful technique that can improve the computational and space costs by embedding data into lower dimension while preserving interesting properties . They have been extensively studied in recent years [ 25 , 1 , 13 ] . The recent fast sparse subspace embeddings [ 13 ] and its optimizations [ 22 , 23 ] are particularly suitable for large scale sparse datasets , since their running time is linear in the number of non zero entries in the data matrix . They also preserve the sparsity of the input data . Formally ,
Definition 2 . An subspace embedding of M ∈ Rm×n is a matrix S ∈ Rt×m such that for any x , SM x = ( 1 ± )M x . ie , S ∈ Rn×t andflflxM Sflfl = ( 1 ± )flflxMflfl .
Subspace embeddings can also be done on the right hand side ,
M x is in the column space of M and SM x is its embedding , so the definition means that the norm of any vector in the column space of M is approximately preserved . This then provides a way to do dimensional reduction for problems depending on inner products of vectors . Our algorithm repeatedly makes use of subspace embeddings . In particular , the embedding we use is the concatenation of the following known sketching matrices : CountSketch and iid Gaussians ( or the concatenation of CountSketch , fast Hadamard and iid Gaussians ) . The details can be found in [ 29 ] ; we only need the following fact .
Lemma 1 . For M ∈ Rd×n , there exist sketching matrices S ∈ Rt×d with t = O(n/ 2 ) that are subspace embeddings . Furthermore , SM can be successfully computed in time ˜O(nnz(M ) ) with probability at least 1 − δ .
The work of [ 2 ] shows that a fast computational approach , TensorSketch , is indeed a subspace embedding for the polynomial kernel . However , there are no previously known subspace embeddings for other kernels . We develop efficient and provable embeddings for a large family of kernels including Gaussian kernel and other shift invariant kernels . These embeddings will be a key tool used by our algorithm .
4 . OVERVIEW
In view of the limitations of the related work , we instead take a different approach , which first selects a small subset of points whose span contains an approximation with relative error rate , and then find a low rank approximation in their span . It is important to keep the size of the subset small and also guarantee that their span contains a good approximation ( this is also called kernel column subset selection ) . A well known technique is to sample according to the statistical leverage scores .
Challenges . However , this immediately raises the fol lowing technical challenges .
I . Computing the statistical leverage scores is prohibitively expensive . Na¨ıvely computing them requires communicating all data points . There exist non trivial fast algorithms [ 18 ] , but they are designed for the non distributed setting . Using them in the distributed setting leads to communication linear in the number of data points , or linear in the number of random features if one uses random features and computes the leverage scores for them . Our key idea is that it is sufficient to compute the ( generalized ) leverage scores of the data points , ie , the leverage scores of another matrix whose row space approximates that of the original data matrix . So the problem is reduced to designing kernel subspace embeddings that can approximate the row space of the data .
II . Even given the embedded data , it is unclear how to compute its leverage scores in a communication efficient way . Although the dimension of the embedded data is small , existing algorithms will lead to communication linear in the number of data points , which is impractical .
III . Simply sampling according to the generalized leverage scores does not give the desired results : a good approximation can only be obtained using a much larger rank , specifically , O(k/ ) .
IV . After selecting the small subset of points , we need to design a distributed algorithm to compute a good low rank approximation in their span .
Algorithm . We have designed a distributed kernel PCA algorithm that computes an approximated solution with relative error rate using low communication . The algorithm operates in following key steps , each of which addresses one of the challenges mentioned above ( See Figure 1 ) :
I . Kernel Subspace Embeddings . To approximate the subspace of the original data matrix , we propose subspace embeddings for a large family of kernels . For polynomial kernels we improve the prior work by reducing the embedding dimension and thus lowering the communication . Furthermore , we propose new subspace embeddings for kernels with random feature expansions , allowing PCA for these kernels to be computed in a communication efficient manner . See Section 5.1 for the details .
II . Distributed Leverage Scores . To compute the leverage scores , sampling with constant approximations is sufficient . We can thus drastically reduce the number of data points :
727 ( a ) Compress data and compute leverage scores
( b ) Leverage score sampling
( c ) Adaptive sampling
( d ) Project data and compute KPCA
Figure 1 : Algorithm overview . The black machine at the center is the master and the gray machines are the workers . Each worker stores its portion of the dataset , and the algorithm computes the top k principle components on the whole dataset . The arrows between the machines denote the direction of communications . In each round , the communication always starts from the workers to the master ( lighter arrows ) and then from the master to the workers ( darker arrows ) . ( a ) Each worker compresses its data by using ( kernel ) subspace embeddings and sends it to the master . The master aggregates the data and computes intermediate results for leverage scores and sends back to the workers . ( b ) Each worker computes the leverage scores , samples data points ( denoted by circles ) and then sends them to the master . The master distributes back the union of the sampled data points . ( c ) Each worker conducts adaptive sampling and sends newly sampled points to the master . The master distributes back the union of all sampled points . ( d ) Each worker projects its data onto the subspace spanned by the sampled data points and sends the compressed projections to the master . The master computes coefficients for the top k principle components by running SVD , and then sends them back to the workers . ( best viewed in color ) first do another ( non kernel ) subspace embeddings on the embedded data , and then send the result to the master for computing the scores . See Figure 1(a ) for an illustration and Section 5.2 for the details .
III . Sampling Representative Points . We take a two step approach as leverage scores alone is not good enough : first sample according to generalized leverage scores , and then sample additional points according to their distances to the span of the points sampled in the first step . The first step gains some coarse information about the data , and the second step use it to get the desired samples . The two steps are illustrated in Figure 1(b ) and 1(c ) , respectively , while the details are in Section 53
IV . Computing an Approximation . After projecting the data to the span of the representative points , we sketch the projections by ( non kernel ) subspace embeddings . We then send the compressed projections to the master and compute the solution there . See Figure 1(d ) for an illustration and Section 5.4 for the details .
Main Theoretical Results . Given as input the local datasets , the rank k and error parameters , ∆ , our algorithm outputs a ( 1 + , ∆) approximation to the optimum with large probability . Formally , we have the following theorem .
Theorem 1 . Algorithm 4 produces a subspace L for ker nel PCA on A that with probability ≥ 0.99 satisfies :
1 . L is a rank k ( 1 + , 0) approximation when applied to polynomial kernels .
The total communication is ˜O( sρk the average number of nonzero entries in one data point .
3 ) words , where ρ is
+ sk2
The constant success probability can be boosted up to any high probability 1−δ by repetition , which adds only an extra O(log 1
δ ) term to communication and computation .
The output subspace L is represented by ˜O(k/ ) sampled points Y from A ( ie , L = φ(Y )C for some coefficient matrix C ) , so L can be easily communicated and the projection of any point on L can be easily computed by the kernel trick . The communication has linear dependence on the dimension and the number of workers , and has no dependence on the number of data points , which is crucial for big data scenarios . Moreover , it does not depend on ∆ ( but the computation does ) , so the additive error can be made arbitrarily small with more computation .
The theorem also holds for other properly regularized kernels with random feature expansions ( see [ 24 , 15 ] for more such kernels ) ; the extension of our proof is straightforward . We also make the following contributions : ( i ) Subspace embedding techniques for many kernels . ( ii ) Distributed algorithm for computing generalized leverage scores with low communication . ( iii ) Distributed algorithm for kernel column subset selection .
5 . DISTRIBUTED KERNEL PRINCIPAL
COMPONENT ANALYSIS
2 . L is a rank k ( 1 + , ∆) approximation when applied to shift invariant kernels with regularization .
Our algorithm first computes the ( generalized ) leverage scores that measure the non uniform structure , then samples
SVD( )728 a subset of points whose span contains a good approximated solution , and finally finds such a solution in the span .
Leverage scores are critical for importance sampling in many fast randomized algorithms . The leverage scores are defined as follows .
Definition 3 . For E ∈ Rt×n with SVD E = U ΣV , the leverage score j for its j th column is j = Vj:2 .
2
Their importance is reflected in the following fact : suppose E has rank at most k , and suppose P is a subset of O( k log k ) columns obtained by repeatedly sampled from the columns of E according to their leverage scores , then the span of P contains an ( 1 + , 0) approximation subspace for E with probability ≥ 0.99 ( see,eg , [ 19] ) . Here , sampling one col4 umn according to the leverage scores j means to define sampling probabilities pj such that pj ≥ j for all j , and probability pj . Note that setting pj = j then pick one column where the j th column is picked with is clearly sufficient , but a constant variance of pj is allowed at the expense of an extra constant factor in the sample size . This means that it is sufficient to compute constant approximations ˜j for j , and then sample according to pj = j j j j
˜j
.
˜j j
However , even computing constant approximations of the leverage scores are non trivial : na¨ıve approaches require SVD , which is expensive . Actually , SVD is more expensive than the task of PCA itself . Even ignoring computation cost , na¨ıve SVD is prohibitive in the distributed setting due to its high communication cost . Fortunately , it turns out that the leverage scores are an over kill for our purpose ; it suffices to compute the generalized leverage scores , ie , the leverage scores of a proxy matrix .
Definition 4 . If E has rank q and can approximate the row space of M up to ( 1 + , ∆ ) , ie , there exists X with
XE − MF ≤ ( 1 + )M − [ M ]kF + ∆ , then the leverage scores of E are called the generalized leverage scores of M with respect to rank q .
This generalizes the definition in [ 18 ] by allowing the rank of E to be larger than k and allowing additive error ∆ , which are important for our application . The generalized leverage scores can act as the leverage scores for our purpose in the following sense .
Lemma 2 . Let P be O( q log q
) columns sampled from M according to their generalized leverage scores wrt rank q . Then with probability ≥ 0.99 , the span of P has a rank s ( 1 + 2 , 2∆) approximation subspace for M .
2
Proof . It follows from combining Theorem 5 in [ 19 ] and the definition of the generalized leverage scores .
Computing the generalized scores with respect to rank q could be much more efficient , since the intrinsic dimension now becomes q , which can be much smaller than the ambient dimension ( the number of points or the dimension of the feature space ) . However , as noted in the overview , there are still a few technical challenges .
• Efficiently find a smaller matrix E that can approxi mate the row space of the original data .
• Compute the leverage scores of E in a communication efficient way .
• The approximation solution in the span of P has the same rank as E , which is O(k/ ) when we use kernel subspace embedding to obtain E . This is not satisfying since our final goal is to compute a rank k solution .
• Find a good approximation in the span of φ(Y ) with low communication .
Our final algorithm consists of four key steps , each of which addresses one of the above challenges . They are elaborated in the following four subsections respectively , and the final subsection presents the overall algorithm . 5.1 Kernel Subspace Embeddings Recall that a subspace embedding S for a matrix M is such that SM x ≈ M x , ie , the norm of any vector in the column space of M is approximately preserved . Subspace embeddings can also be generalized for the feature mapping of kernels , simply by setting M = φ(A ) , S a linear mapping from H → Rt and using the corresponding inner product . If the data after the kernel subspace embedding is sufficient for solving the problem under consideration , then only Sφ(A ) in much lower dimension is needed . This is especially interesting for distributed kernel methods , since directly using the feature mapping or the kernel trick in this setting will lead to high communication cost , while the data after embedding can be smaller and lead to lower communication cost .
A sufficient condition for solving many problems ( in particular , kernel PCA ) is to preserve the low rank structure of the data . More precisely , the row space of Sφ(A ) is a good approximation to that of φ(A ) , where the error is comparably to the best rank k approximation error . Then Sφ(A ) can be used to compute the generalized leverage scores for φ(A ) , which can then be utilized to compute kernel PCA as mentioned above .
More precisely , we would like Sφ(A ) to approximate the row space of φ(A ) up to ( 1 + , ∆ ) , as required in the definition of the generalized leverage scores . We give such embeddings a particular name .
Definition 5 . S is called a ( 1 + , ∆) good subspace em bedding for φ(A ) ∈ Hn , if there exists X such that
X(Sφ(A ) ) − φ(A)2 ≤ ( 1 + )φ(A ) − [ φ(A)]k2 + ∆ .
We now identify the sufficient conditions for ( 1 + , ∆)good subspace embeddings , which can then be used in constructing such embeddings for various kernels .
Lemma 3 . S is a ( 1 + , ∆) good subspace embedding for φ(A ) ∈ Hn if it satisfies the following . P1 ( Subspace Embedding ) : For any orthonormal V ∈ Hk
( ie , V V is the identity ) , for all x ∈ Rk ,
SV x = ( 1 ± c)V x where c is a sufficiently small constant .
P2 ( Approximate Product ) : for any M ∈ Hn , N ∈ Hk , N2 M2 + ∆ .
( SM ) − N
M flflfl(SN ) flflfl2
F
≤ k
Polynomial Kernels . For polynomial kernels , there exists an efficient algorithm TensorSketch to compute the embedding [ 2 ] . However , the embedding dimension has a
729 i=1
, k )
Algorithm 1 Distributed Leverage Scores : {˜i 1 : Each worker i : do 1 j} = disLS((Ei)s 2 : Master : QR factorize.E1T 1 , . . . , EsT sfi flflfl,(Z)−1Ei with p = O(t ) ; send EiT i to Master .
3 : Each worker i : compute ˜i send Z to all workers . j =
= U Z ; flflfl2
2
.
:j
4 subspace embedding EiT i ∈ Rt×p quadratic dependence on the rank k , which will increase the communication . Fortunately , subspace embedding can be concatenated , so we can further apply another known subspace embedding such as one of those in Lemma 1 which , though not fast for feature mapping , is fast for the already embedded data and has lower dimension . In this way , we can enjoy the benefits of both approaches .
The guarantee of TensorSketch in [ 2 ] and the property of the subspace embeddings in Lemma 1 can be combined to verify P1 and P2 . So we have
Lemma 4 . For polynomial kernels κ(x , y ) = ( x , y)q , there exists a ( 1 + , 0) good subspace embedding matrix S : Rdq → Rt with t = O(k/ ) .
Kernels with Random Feature Expansions . Polynomial kernels have finite dimensional feature mappings , for which the sketching seems natural . It turns out that it is possible to extend subspace embeddings to kernels with infinite dimensional feature mappings . More precisely , we propose subspace embeddings for kernels with random feature expansions , ie , κ(x , y ) = Eω [ ξω(x)ξω(y ) ] for some function ξ(· ) . Therefore , one can approximate the kernel by using m features zω(x ) on randomly sampled ω . Such random feature expansion can be exploited for subspace embeddings : view the expansion as the “ new ” data points and apply a sketching matrix on top of it . Compared to polynomial kernels , the finite random feature expansion leads to an additional additive error term . Our analysis shows that bounding the additive error term only requires sufficiently large sampled size m , which affects the computation but does not affect the final embedding dimension and thus the communication . In summary , the embedding is Sφ(x ) = T R(φ(x) ) , where R(φ(x ) ) ∈ Rm is m random features for x and T ∈ Rt×m is an embedding as in Lemma 1 . The properties P1 and P2 can be verified by combining Lemma 1 and the guarantees of random features .
Lemma 5 . For a continuous shift invariant kernels κ(x , y ) =
κ(x − y ) with regularization , there exists a ( 1 + , ∆) good subspace embedding S : H → Rt with t = O(k/ ) . 5.2 Computing Leverage Scores
Given the matrix E obtained from kernel subspace embedding , we would like to compute the leverage scores of E . First note that this cannot be done simply in a local manner : the leverage score of a column in Ei is different from the leverage score of the same column in E . Furthermore , though data in E have low dimension , communicating all points in E to the master is still impractical , since it leads to communication linear in the total number of points .
Fortunately , we only need to compute constant approximations of the scores , which allows us to use subspace embedding on E to greatly reduce the number of data points .
Y = RepSample((Ai)s
,{˜i j} , k , ) i=1
Algorithm 2 Sampling Representative Points :
1 : Workers : sample O(k log k ) points according to {˜i j} ; send to Master ;
2 : Master : send all the sampled points P to the workers ; sample O(k/ ) points ˜Y according to the 3 : Workers : square distances to P in the feature space ; send to Master ; 4 : Master : send Y = ˜Y ∪ P to all the workers .
4 subspace embedding T i ( eg , In particular , we apply a 1 one of those in Lemma 1 ) on each local data set Ei , and then send them to the master . Let ET denote all the embedded data , and do QR factorization ( ET ) = U Z . Now , ET are a set of basis for ET . the rows of U = ,Z −1 Then , think of UT † =,Z −1 suffices to compute the norms of the columns in,Z −1
E . The details are described in Algorithm 1 and Figure 1(a ) shows an illustration . The algorithm is guaranteed to output constant approximations of the leverage scores of E .
E as the basis for E , so it
Lemma 6 . Let i j be the true leverage scores of E . Then j = ( 1 ± 1/2)i j .
Algorithm 1 outputs ˜i
Proof . The algorithm can be viewed as applying an em bedding T = diag,T 1 , . . . , T s on E to approximate the flflflx scores while saving the costs . Each T i is an 1 bedding matrix , then for any x ,
4 subspace em
E2T 2 , . . . , x flflfl2
ET
= flflfl2 ( 1 ± 1/4)2flflflx
EsT s ]
Eiflflfl2
E1T 1 , x flflfl[x EiT iflflfl2 s =(1 ± 1/4)2flflflx flflflx i=1
=
E
= s flflfl2 i=1
.
So T is also 1 4 subspace embedding . Such a scheme of using embedding for approximating the scores has been analyzed ( Lemma 6 in [ 18] ) , and the lemma follows .
We note that though a constant approximation is suffij = 4 ) , cient for our purpose , but the algorithm can output ˜i ( 1 ± )i 2 subspace embedding ( instead of 1 which can be useful for other applications . 5.3 Sampling Representative Points j by doing an
Sampling directly to the leverage scores can produce a set of points P such that the span of φ(P ) contains a ( 1 + , ∆)approximation to φ(A ) . However , the rank of that approximation can be as high as O(k/ ) , since its rank is the same as that of the embedded data ( see Lemma 2 ) , which will be O(k/ ) to achieve error . To get a rank k approximation and also enjoy the advantage of leverage scores , we propose to combine leverage score sampling and the adaptive sampling algorithm in [ 16 , 9 ] .
The details are presented in Algorithm 2 . We first sample a set P of O(k log k ) points according to the leverage scores , so that the span of φ(P ) contains a ( 2 , ∆) approximation . Then we use the adaptive sampling method : sample O(k/ ) points according to the square distances from the points to their projections on P and then add them to P to get the
730 1 : Each worker i : do a ( 1/4 , ∆) good subspace embedding
, k , , ∆ )
Algorithm 4 Distributed Kernel PCA :
L = disKPCA((Ai)s j} = disLS((Ei)s i=1
2 : Compute the leverage scores :
Ei = S(φ(Ai ) ) ∈ Rt×ni , t = O(k ) ; {˜i
3 : Sample points : Y = RepSample((Ai)s 4 : Output L = disLR((Ai)s
, Y , k , , ∆ ) .
, k ) ; i=1 i=1
,{˜i j} , k , ) ; i=1
Table 1 : Dataset specification : d is the original feature dimension , n is the number of data points , and s is the total number of workers storing the dataset distributedly . Among them , bow and 20news are sparse datasets . All datasets except mnist8m are taken from UCI repository [ 4 ] and [ 7 ] .
Dataset bow higgs mnist8m susy yearpredmsd ctslice 20news protein har insurance d
100,000 28 784 18 90 384 61,118 9
561 85 n
8,000,000 11,000,000 8,000,000 5,000,000 463,715 53,500 11,269 41,157
10,299 9,822 s
200 200 100 100 10 10 5 5
5 5
Combining this with T 2 , and applying Pythagorean Theorem again , we know that the error is roughly flflflQ[Q
φ(A)]k − φ(A ) flflfl2
.
Now , by assumption , there is a rank k ( 1+ , ∆) approximation subspace X in the span of φ(Y ) . Since [ Qφ(A)]k is the best rank k approximation to Qφ(A ) , flflflQ[Q flflflQ[Q ≤flflflX − QQ
=
φ(A)]k − φ(A ) φ(A)]k − QQ
φ(A )
+ flflfl2 flflfl2 flflflQQ
φ(A ) flflfl2
=X − φ(A)2 . flflflQQ
+ φ(A ) − φ(A ) flflfl2 flflfl2
φ(A ) − φ(A )
The lemma then follows . 5.5 Overall Algorithm
Now , putting things together , we obtain our final algorithm for distributed kernel PCA ( Algorithm 4 ) . Our main result , Theorem 1 , follows by combining all the lemmas in the previous subsections ( with properly adjusted and ∆ ) .
Algorithm 3 Computing an Approximation :
L = disLR((Ai)s 2 : Master : concatenate ΠT =.Π1T 1 , . . . , ΠsT sfi and send
1 : Each worker i : compute the basis Q for φ(Y ) and Πi = Qφ(Ai ) ; do an subspace embedding ΠiT i ∈ R|Y |×w with w = O(|Y |/ 2 ) , and send ΠiT i to Master ;
, Y , k , , ∆ ) i=1 the top k singular vectors W of ΠT to the workers .
3 : Each worker i : set L = QW . desire set Y of representative points . Figure 1(b ) and 1(c ) demonstrate the two steps of the algorithm .
Adaptive sampling has the following guarantee :
Lemma 7 . Suppose there is a ( 2 , ∆) approximation for φ(A ) in the span of φ(P ) . Then with probability ≥ 0.99 , the span of φ(Y ) has a rank k ( 1 + , ∆) approximation .
Therefore , we solves the column subset selection problem for kernels in the distributed setting , with O(k log k+k/ ) selected columns and with a communication of only O(sρk/ + sk2 ) . This also provides the foundation for kernel PCA task . 5.4 Computing an Approximation
To compute a good approximation in the span of φ(Y ) , the na¨ıve approach is to project the data to the span and compute SVD there . However , the communication will be linear in the number of data points . Subspace embedding can be used to sketch the projected data , so that the number of data points is greatly reduced .
Algorithm 4 describes the details and Figure 1(d ) shows an illustration . To compute the best rank k approximation for the projected data Π , we do a subspace embedding on the right hand side , ie , compute ΠT =.Π1T 1 , . . . , ΠsT sfi .
Then the algorithm computes the best rank k approximation W for ΠT , which is then a good approximation for Π and thus φ(A ) . It then returns L , the representation of W in the coordinate system of φ(A ) . The output L is guaranteed to be a good approximation . subspace in the span of φ(Y ) , then
Lemma 8 . If there is a rank k ( 1 + , ∆) approximation flflfl2 ≤ ( 1+ )2flflφ(A ) − [ φ(A)]k flfl2+(1+ )∆ .
φ(A ) − φ(A ) flflflLL
Proof Sketch . For our choice of w , T i is an subspace embedding matrix for Πi . Then their concatenation B is an subspace embedding for Π , the concatenation of Πi . Then we can apply the idea implicit in [ 20 ] .
By Pythagorean Theorem , the error can be factorized into flflfl2 flflfl2 flflflφ(A ) − QQ flflfl2
φ(A )
T 2
φ(A )
.
T 1 flflflW W
Since LL = QW W Q ,
T 1 =
φ(A ) − Q
Q
Note that Π = Qφ(A ) , and W is the best rank k subspace for its embedding ΠT . By property of T ( Theorem 7 in [ 20] ) , it is also a good approximation for Π . So
φ(A)]k − Q
φ(A )
=
φ(A)]k − QQ
φ(A ) flflfl2 flflflQ[Q
T 1 ≈flflfl[Q flflflLL
φ(A ) − QQ
φ(A )
+
.
6 . EXPERIMENTS 6.1 Datasets
We use ten datasets to evaluate our algorithm . They contain both sparse and dense data and come from a variety of different domains , such as text , images , high energy physics and biology . We use two smaller ones to benchmark against the single machine batch KPCA algorithm while the rest are large scale datasets with up to tens of millions of data points and hundreds of thousands dimensions . Refer to Table 1 for detailed specifications .
. flflfl2
731 ( a ) error on insurance Figure 2 : KPCA for polynomial kernels on small datasets : low rank approximation error and runtime
( b ) runtime on insurance
( d ) runtime on har
( c ) error on har
( a ) error on insurance Figure 3 : KPCA for Gaussian kernels on small datasets : low rank approximation error and runtime
( b ) runtime on insurance
( d ) runtime on har
( c ) error on har
Each dataset is partitioned on different workers according to the power law distribution with exponent 2 to simulate the distribution of the data over large networks [ 14 ] . Depending on the size of the dataset , the number of workers used ranges from 5 to 200 ( see Table 1 for details ) .
6.2 Experiment Settings
Since our key contribution is sampling a small set of data points intelligently , the natural alternative is uniformly sampling . We compare with two variants of uniform sampling algorithms : 1 ) uniformly sampling representative points and use Algorithm 3 to get KPCA solution ( denoted as uniform+disLR ) ; 2 ) uniformly sampling data points and apply batch KPCA ( denoted as uniform+batch KPCA ) .
For both algorithms , we compare the tradeoff of low rank approximation error and communication cost . Particularly , we compare the communication needed to achieve the same error . Each method is run 5 times and the mean and the standard deviation are reported .
For polynomial kernel , the degree is q = 4 and for Gaussian RBF kernel , the kernel bandwidth σ is set to 0.2 of the median pairwise distance among a subset of 20000 randomly chosen data points ( aka , the “ median trick ” ) . For Gaussian random feature expansion , we use 2000 random features .
In all experiments , we set the number of principle components k = 10 , which is the same number for k means . The algorithm specific parameters are set as follows : 1 ) The subspace embedding dimension for the feature expansion t is 50 ; 2 ) The subspace embedding dimension for the data points p is 250 ; 3 ) We vary the number of adaptively sampled points | ˜Y | from 50 to 400 to simulate different communication cost ; 4 ) The subspace embedding dimension w is set to equal |Y | .
6.3 Comparison with Batch Algorithm
We compare to the “ ground truth ” solutions produced by batch KPCA on two small datasets where it is feasible . The experiment results for the polynomial kernel and the Gaussian RBF kernel are presented in Figures 2 and 3 , respectively . In both cases , the approximation error of disKPCA
Figure 6 : KPCA results for arc cos kernels decreases as more communication is allowed . It can nearly match the optimum low rank approximation error with much fewer data points . In addition , it is much faster : we gain a speed up of 10× by using five workers . 6.4 Communication Efficiency
In these experiments , we compare the tradeoff between communication cost and approximation accuracy on largescale datasets . The alternative , uniform + batch KPCA , is stopped short in many experiments due to its excessive computation cost for larger number of sampled data points . Figure 4 demonstrates the performance on polynomial kernels on four large datasets . On all four datasets , our algorithm outperforms the alternatives by significant margins . Especially on bow , which is a sparse dataset , the usage of kernel embeddings takes advantage of the sparsity structure and leads to much smaller error . On other datasets , uniform + disLR cannot match the error achieved by our algorithm even when using much more communication .
Figure 5 shows the performance on Gaussian kernels . On mnist8m , the error for uniform + batch KPCA is so large ( almost twice of the errors in the figure ) that it is not shown . On other datasets , disKPCA achieves significant smaller error . For example , on higgs dataset , to achieve the same error , uniform + disLR requires more than 5 times communication . Since it does not have the communication of computing leverage scores , this means that it needs to sample much more points to get similar performance . Therefore , our algorithm is very efficient in communication .
01234x 1064800500052005400Communication costLow−rank approximation errorinsurance disKPCAbatch KPCA01234x 10605101520Communication costRun timeinsurance 345678x 1063638442x 106Communication costLow−rank approximation errorhar 345678x 1060102030Communication costRun timehar 0123456x 1064354444545455Communication costLow−rank approximation errorinsurance distKPCAbatch KPCA0123456x 10605101520Communication costRun timeinsurance 123456x 1063637383940Communication costLow−rank approximation errorhar 123456x 10605101520Communication costRun timehar 0123456x 106152253354x 106Communication costLow−rank approximation error20news disKPCAuniform + disLRuniform + batch KPCA01234x 106300040005000600070008000Communication costLow−rank approximation errorprotein 732 Figure 4 : KPCA for polynomial kernels on larger datasets
Figure 5 : KPCA for Gaussian kernels on larger datasets
Figure 7 : KPCA scaling results
Besides polynomial and Gaussian kernels , we have also conducted experiments using arc cos kernel [ 12 ] . The arccosine kernels have random feature bases similar to the Rectified Linear Units ( ReLU ) used in deep learning . We use degree n = 2 and Figure 6 shows the results . Our algorithm consistently achieves better tradeoff between communication and approximation and the benefit is especially more pronounced on sparser dataset such as 20news . 6.5 Scaling Results
In Figure 7 , we present the scaling results for disKPCA . In these experiments , we vary the number of workers and record the corresponding computation time ( communication time excluded ) . On both datasets , the runtime decreases as we use more workers , and it eventually plateaus . Our algorithm gains about 2× speedup by using 4× more workers . Note that our algorithm is designed to strike a good balance between communication and approximation . Even though computation complexity is not our first priority , the experiments show disKPCA still enjoys favorable scaling property . 6.6 Distributed Spectral Clustering
We have also experimented a form of spectral clustering ( KPCA followed by k means clustering ) . We project the data onto the top k principle components and then apply a distributed k means clustering algorithm [ 6 ] . The evaluation criterion is the k means objective , ie , average distances to the corresponding centers , in the feature space .
Figure 8(a ) presents results for polynomial kernels on the 20news and susy and Figure 8(b ) presents results for Gaussian kernels on ctslice and yearpredmsd . Our algorithm compares favorably with the other methods and achieves a better tradeoff of communication and error . This means that although the other methods require similar communication , they need to sample more data points to achieve the same loss , demonstrating the effectiveness of our algorithm .
7 . CONCLUSION
This paper proposes a communication efficient distributed algorithm for kernel Principal Component Analysis with theoretical guarantees . It computes a relative error approximation compared to the best rank k subspace , using communication that nearly matches that of the state of the art algorithms for distributed linear PCA . This is the first distributed algorithm that can achieve such provable approximation and communication bounds . The experimental results show that it can achieve better performance than the baseline using the same communication budget .
8 . ACKNOWLEDGMENTS
M F B . and Y . L . were supported in part by NSF grants CCF 0953192 and CCF 1101283 , ONR N00014 09 1 0751 , AFOSR grant FA9550 09 1 0538 , a Microsoft Faculty Fellowship , and a Google Research Award . Y . L . was also supported in part by NSF grants CCF 1527371 , DMS 1317308 , Simons Investigator Award , Simons Collaboration Grant , and ONR N00014 16 1 2329 . L . S . and B . X . were supported in part by NSF/NIH BIGDATA 1R01GM108341 , ONR N00014 15 1 2340 , NSF IIS 1218749 , and NSF CAREER IIS 1350983 . DW acknowledges the support from XDATA program of the Defense Advanced Research Projects Agency ( DARPA ) , administered through Air Force Research Laboratory contract FA8750 12 C 0323 .
9 . REFERENCES [ 1 ] N . Ailon and B . Chazelle . The fast johnson lindenstrauss transform and approximate nearest neighbors . SIAM Journal on Computing , 39(1):302–322 , 2009 .
[ 2 ] H . Avron , H . Nguyen , and D . Woodruff . Subspace embeddings for the polynomial kernel . In Advances in Neural Information Processing Systems , pages 2258–2266 , 2014 .
[ 3 ] F . Bach and M . Jordan . Predictive low rank decomposition for kernel methods . In Proceedings of the International Conference on Machine Learning , 2005 .
0051152x 1084567891011x 106Communication costLow−rank approximation errorsusy disKPCAuniform + disLRuniform + batch KPCA01234x 108060811214x 107Communication costLow−rank approximation errorhiggs 01234x 1085657585966162x 108Communication costLow−rank approximation errorbow 005115225x 10766676869771x 105Communication costLow−rank approximation errorctslice 051015x 1072200220522102215Communication costLow−rank approximation errorsusy disKPCAuniform + disLRuniform + batch KPCA0051152253x 10831595316031605316131615Communication costLow−rank approximation errorhiggs 005115225x 10818241826182818301832Communication costLow−rank approximation errormnist8m 051152x 1072172172217421762178Communication costLow−rank approximation errorctslice 2040608010030405060708090Number of workersRun timesusy501001502006080100120140160180Number of workersRun timebow733 ( a ) Polynomial kernels
( b ) Gaussian kernels
Figure 8 : KPCA + k means clustering
[ 4 ] K . Bache and M . Lichman . UCI machine learning repository , 2013 . and statistical leverage . The Journal of Machine Learning Research , 13(1):3475–3506 , 2012 .
[ 5 ] M F Balcan , A . Blum , S . Fine , and Y . Mansour .
[ 19 ] P . Drineas , M . W . Mahoney , and S . Muthukrishnan .
Distributed learning , communication complexity and privacy . COLT , 2012 .
[ 6 ] M F Balcan , V . Kanchanapally , Y . Liang , and
D . Woodruff . Improved distributed principal component analysis . In Advances in Neural Information Processing Systems 27 , pages 3113–3121 . Curran Associates , Inc . , 2014 .
[ 7 ] P . Baldi , P . Sadowski , and D . Whiteson . Searching for exotic particles in high energy physics with deep learning . Nature Communications , 2014 .
[ 8 ] C . Boutsidis , M . Sviridenko , and D . P . Woodruff .
Optimal distributed principal component analysis . In manuscript , 2015 .
[ 9 ] C . Boutsidis and D . P . Woodruff . Optimal cur matrix decompositions . arXiv preprint arXiv:1405.7910 , 2014 .
[ 10 ] C . Boutsidis and D . P . Woodruff .
Communication optimal distributed principal component analysis in the column partition model . CoRR , abs/1504.06729 , 2015 .
[ 11 ] C . Boutsidis , D . P . Woodruff , and P . Zhong . Communication optimal distributed principal component analysis in the column partition model . In ACM Symposium on Theory of Computing , 2015 . [ 12 ] Y . Cho and L . K . Saul . Kernel methods for deep learning . In NIPS , pages 342–350 , 2009 .
[ 13 ] K . L . Clarkson and D . P . Woodruff . Low rank approximation and regression in input sparsity time . In Proceedings of the Annual ACM Symposium on Theory of Computing , 2013 .
[ 14 ] A . Clauset , C . R . Shalizi , and M . E . Newman .
Power law distributions in empirical data . SIAM review , 51(4):661–703 , 2009 .
[ 15 ] B . Dai , B . Xie , N . He , Y . Liang , A . Raj , M F F . Balcan , and L . Song . Scalable kernel methods via doubly stochastic gradients . In Advances in Neural Information Processing Systems , pages 3041–3049 , 2014 .
[ 16 ] A . Deshpande and S . Vempala . Adaptive sampling and fast low rank matrix approximation . Algorithms and Techniques in Approximation , Randomization , and Combinatorial Optimization , pages 292–303 , 2006 .
[ 17 ] I . S . Dhillon , Y . Guan , and B . Kulis . Kernel kmeans , spectral clustering and normalized cuts . In Conference on Knowledge Discovery and Data Mining , 2004 . [ 18 ] P . Drineas , M . Magdon Ismail , M . Mahoney , and
D . Woodruff . Fast approximation of matrix coherence
Relative error cur matrix decompositions . SIAM Journal on Matrix Analysis and Applications , 30(2):844–881 , 2008 .
[ 20 ] R . Kannan , S . Vempala , and D . Woodruff . Principal component analysis and higher correlations for distributed data . In Proceedings of The 27th Conference on Learning Theory , pages 1040–1057 , 2014 .
[ 21 ] S . Kumar , M . Mohri , and A . Talwalkar . Sampling methods for the nystr¨om method . Journal of Machine Learning Research , 13:981–1006 , 2012 .
[ 22 ] X . Meng and M . W . Mahoney . Low distortion subspace embeddings in input sparsity time and applications to robust linear regression . In Proceedings of the Annual ACM symposium on Symposium on Theory of Computing , 2013 .
[ 23 ] J . Nelson and H . L . Nguyˆen . Osnap : Faster numerical linear algebra algorithms via sparser subspace embeddings . In IEEE Annual Symposium on Foundations of Computer Science , 2013 .
[ 24 ] A . Rahimi and B . Recht . Random features for large scale kernel machines . In J . Platt , D . Koller , Y . Singer , and S . Roweis , editors , Advances in Neural Information Processing Systems 20 . MIT Press , Cambridge , MA , 2008 .
[ 25 ] T . Sarl´os . Improved approximation algorithms for large matrices via random projections . In IEEE Symposium on Foundations of Computer Science , 2006 .
[ 26 ] B . Sch¨olkopf and A . J . Smola . Learning with Kernels .
MIT Press , Cambridge , MA , 2002 .
[ 27 ] B . Sch¨olkopf , A . J . Smola , and K R M¨uller . Kernel principal component analysis . In Artificial Neural Networks ICANN’97 , volume 1327 , pages 583–588 , Berlin , 1997 .
[ 28 ] B . Sch¨olkopf , K . Tsuda , and J P Vert . Kernel Methods in Computational Biology . MIT Press , Cambridge , MA , 2004 .
[ 29 ] D . P . Woodruff . Sketching as a tool for numerical linear algebra . Theoretical Computer Science , 10(1 2):1–157 , 2014 .
[ 30 ] Y . Zhang , M . J . Wainwright , and J . C . Duchi .
Communication efficient algorithms for statistical optimization . In Advance in Neural Information Processing Systems , 2012 .
024681012x 1061152253354x 1020Communication costk−means cost20news disKPCA−kmeansuniform + disKPCA−kmeans012345x 10815225335x 107Communication costk−means costsusy 0051152253x 10709809850990995Communication costk−means costctslice 0051152x 107091091509209250930935Communication costk−means costyearpredmsd 734
