Graph Wavelets via Sparse Cuts
Arlei Silva † , Xuan Hong Dang † , Prithwish Basu ∗ , Ambuj Singh † , Ananthram Swami ‡
†Computer Science Department , University of California , Santa Barbara , CA , USA
∗Raytheon BBN Technologies , Cambridge , MA , USA
‡Army Research Laboratory , Adelphi , MD , USA
{arlei,xdang,ambuj}@csucsbedu , phasu@bbn.com , ananthramswamiciv@mailmil
‘
ABSTRACT Modeling information that resides on vertices of large graphs is a key problem in several real life applications , ranging from social networks to the Internet of things . Signal Processing on Graphs and , in particular , graph wavelets can exploit the intrinsic smoothness of these datasets in order to represent them in a compact and accurate manner . However , how to discover wavelet bases that capture the geometry of the data with respect to the signal as well as the graph structure remains an open problem . In this paper , we study the problem of computing graph wavelet bases via sparse cuts in order to produce low dimensional encodings of data driven bases . This problem is connected to known hard problems in graph theory ( eg multiway cuts ) and thus requires an efficient heuristic . We formulate the basis discovery task as a relaxation of a vector optimization problem , which leads to an elegant solution as a regularized eigenvalue computation . Moreover , we propose several strategies in order to scale our algorithm to large graphs . Experimental results show that the proposed algorithm can effectively encode both the graph structure and signal , producing compressed and accurate representations for vertex values in a wide range of datasets ( eg sensor and gene networks ) and significantly outperforming the best baseline . Categories and Subject Descriptors : H28 [ Database Management ] : Database applications − data mining General Terms : Algorithms , Experimentation Keywords : Graph mining , Spectral Theory , Wavelets
1 .
INTRODUCTION
Graphs are the model of choice in several applications , ranging from social networks to the Internet of things ( IoT ) . In many of these scenarios , the graph represents an underlying space in which information is generated , processed and transferred . For instance , in social networks , opinions propagate via social interactions and might produce large cas cades across several communities . In IoT , different objects ( eg cars ) collect data from diverse sources and communicate with each other via the network infrastructure . As a consequence , exploiting the underlying graph structure in order to manage and process data arising from these applications has become a key challenge .
Signal processing on graphs ( SPG ) is a framework for the analysis of data residing on vertices of a graph [ 28 , 26 ] . The idea generalizes traditional signal processing ( eg compression , sampling ) as means to support the analysis of highdimensional datasets . In particular , SPG has been applied in the discovery of traffic events using speed data collected by a sensor network [ 22 ] . Moreover , graph signals are a powerful representation for data in machine learning [ 13 , 11 ] . As in traditional signal processing , the fundamental operation in SPG is the transform , which projects the graph signal in the frequency ( or other convenient ) domain . Real signals are expected to be smooth with respect to the graph structure—values at nearby vertices are similar—and an effective transform should lead to rapidly decaying coefficients for smooth signals . The most popular transform in SPG , known as Graph Fourier Transform [ 25 , 28 ] , represents a signal as a linear combination of the eigenvectors of the graph Laplacian . However , as its counterpart in traditional signal processing , Graph Fourier fails to localize signals in space ( ie within graph regions ) . This limitation has motivated recent studies on graph wavelets [ 6 , 16 , 5 ] .
An open issue in SPG is how to link properties of the signal and underlying graph to properties of the transform [ 28 ] . Gavish et al . [ 13 ] makes one of the first efforts in this direction , by relating the smoothness of the signal with respect to a tree structure of increasingly refined graph partitions and the fast decay of the wavelet coefficients in a Haar like expansion . However , as explicitly stated in their paper , their approach “ raises many theoretical questions for further research , in particular regarding construction of trees that best capture the geometry of these challenging datasets ” .
In this paper , we study the problem of computing wavelet trees that encode both the graph structure and the signal information . A wavelet tree defines a hierarchical partitioning used as basis for a graph wavelet transform . Good wavelet trees should produce fast decaying coefficients , which support a low dimensional representation of the graph signal . The particular application scenario we consider is the lossy graph signal compression . This task arises in many relevant data management and analytics applications , including IoT and social networks , where values associated to interconnected entities have to be represented in a compact form .
1175 optimal wavelet trees are related to hard cut problems , such as graph bisection [ 2 ] and multiway cuts [ 7 ] .
In Figure 1 , we also show the cuts associated to each level of the wavelet tree together with the signal approximation for the respective level . Basis A can be approximately encoded by the cutting four edges : {(b , d ) , ( c , d)} ( level 1 ) and {(e , f ) , ( e , g)} ( level 2 ) . The resulting compact wavelet tree can effectively represent the graph signal using only the two top wavelet coefficients , leading to a relative L2 error of 1 % . On the other hand , basis B does not have such a compact approximation with small error via sparse cuts .
In this paper , we formalize the problem of computing sparse wavelet bases ( or trees ) for graph wavelet transforms . This problem , which we call sparse graph wavelet transform ( SWT ) consists of identifying a sequence of sparse graph cuts that leads to the minimum error in the reconstruction of a given graph signal . We show that this problem is NPhard , even to approximate by a constant . In fact , we are able to show that computing each individual cut in the tree construction is an NP hard problem .
As the main contribution of this paper , we propose a novel algorithm for computing an SWT via spectral graph theory . The algorithm design starts by formulating a relaxation of our problem as an eigenvector problem , which follows the lines of existing approaches for ratio cuts [ 15 ] , normalizedcuts [ 27 ] and max cuts [ 33 ] . We further show how the proposed relaxation leads to a regularization of pairwise values by the graph Laplacian , which relates to existing work on graph kernels [ 31 , 19 ] . In order to improve the computational efficiency of our algorithm , we design a fast graph wavelet transform ( FSWT ) using several techniques including Chebyshev Polynomials and the Power method .
2 . RELATED WORK
Generalizing the existing signal processing framework to signals that reside on graphs is the main focus of Signal Processing on Graphs ( SPG ) [ 28 , 26 ] . Operations such as filtering , denoising , and downsampling , which are well defined for signals in regular Euclidean spaces , have several applications also when signals are embedded in sparse irregular spaces that can be naturally modeled as graphs . For instance , sensor networks [ 22 ] , brain imaging [ 20 ] , computer network traffic [ 6 ] , and statistical learning [ 31 , 19 , 11 ] , are examples of scenarios where graph signals have been studied . The main idea in SPG is the so called Graph Fourier Transform ( GFT ) [ 25 ] , which consists of applying eigenvectors of the Laplacian matrix of a graph as a basis for graph signals . Laplacian eigenvectors oscillate at different frequencies over the graph structure , capturing a notion of frequency similar to complex exponentials in the standard Fourier Transform . As is the case for its counterpart for Euclidean spaces , GFT fails to localize graph signals , ie capture differences within graph regions . This aspect has motivated the study of graph wavelets [ 6 , 13 , 16 , 5 ] . Crovella and Kolaczyk [ 6 ] introduced wavelets on graphs for the analysis of network traffic . Their design extracts differences in values within a disc ( ie a center node and a fixed radius ) and a surrounding ring as means to identify traffic anomalies . Coiffman and Maggioni [ 5 ] proposed a more sophisticated design , known as diffusion wavelets , based on compressed representations of dyadic powers of a diffusion operator . In [ 16 ] , Hammond et al . present a wavelet design using kernel functions that modulate eigenvectors around vertices at multiple scales .
( a ) Wavelet basis A
( b ) Wavelet basis B
Figure 1 : Graph wavelet transforms for two different wavelet trees and the same piecewise smooth graph signal ( values set to vertices ) . A wavelet tree contains one average coefficient and several weighted difference coefficients associated with vertex partitions . Basis A is better than B because it produces fast decaying difference coefficients . Moreover , basis A can be approximately encoded as a sequence of sparse graph cuts ( first {(b , d ) , ( c , d)} then {(e , f ) , ( e , g)} ) , which leads to a compact and accurate representation of the graph signal .
Figure 1 shows two wavelet trees , A and B , and their respective transforms for a piecewise smooth graph signal defined over seven vertices . The wavelet transform contains a single average coefficient and a set of weighted difference coefficients associated to each node of the tree . Weighted difference coefficients are computed as a function of the values in each partition and the partition sizes ( see Equation 2 for a formal definition ) . Notice that these two bases produce very different wavelet transforms for the same signal . While tree A is characterized by fast decaying difference coefficients , tree B has relatively large coefficients at every level . This indicates that tree A supports a better representation for the signal than does tree B . However , good wavelet trees must also capture properties of the graph structure .
We measure the relationship between a wavelet tree and the graph structure using the notion of sparse cuts . A graph cut is a set of edges that connect two disjoint sets of vertices and sparse cuts ( ie those with a small number of edges ) are a natural way to model graph partitions [ 9 ] . As each node of the wavelet tree separates a set of vertices into two subsets , a sparse wavelet tree can be approximately encoded by a sequence of sparse cuts . This work is the first effort to connect graph cuts and graph signal processing . In particular , we show how problems that arise in the construction of
8 4 610 9 910average a0,0 = 0 , difference a1,1 = 28a1,2 = 1.3 a2,2 = 4a2,3 = 1a3,3 = 08a1,3= 0 1010 4 6 9 9abcdefgCumulative cut and corresponding signalapproximationweighted difference:(4*28 ( 3*28 ) ) / 7(2*( 10 ) 2*( 18 ) ) / 4a0,0 = 0 , a1,1 = 24a2,1 = 6 a2,2 = 1a3,3 = 1.5 9a3,1 = 1a3,2 = 7 1010 4 6 981176 An assumption shared by existing work on graph wavelets is that good bases can be computed based solely on the graph structure . However , as shown in Figure 1 , a proper choice of graph wavelet bases can lead to significantly more effective transforms . In this paper , we study the problem of computing optimal graph wavelet bases for a given signal via sparse graph cuts . A graph cut partitions the vertices of a graph into two disjoint subsets and optimization problems associated with graph cuts are some of the most traditional problems in graph theory [ 8 , 12 ] . In particular , graph cuts ( eg min cut , max cut ) are a natural way to formulate graph partitioning problems [ 9 ] . Here , we constraint the size of the cut , in number of edges , associated to a graph wavelet basis in order to discover bases that are well embedded in the graph . A similar constraint also appears in the min cut [ 8 ] , graph bisection [ 2 ] , and multiway cut [ 7 ] problems .
Learning bases tailored for classes of signals is an important problem in signal processing , known as dictionary learning [ 32 ] . This problem differs from ours since our wavelet bases are adapted to each signal , which leads to more compact representations . In [ 29 ] , the authors show how importance sampling can support the discovery of center radius partitions for attribute compression . However , their approach does not generalize to arbitrarily shaped partitions . Many relevant problems on graphs have been solved using the framework of Spectral Graph Theory ( SPG ) [ 4 ] , which studies combinatoric graph properties via the spectrum of matrices associated with them . For instance , the relationship between eigenvectors of the Laplacian and graph partitions can be traced back to Cheeger ’s inequality [ 3 ] . More recently , SPG has led to efficient graph partitioning algorithms ( eg ratio cuts [ 15 ] , normalized cuts [ 27] ) . In this paper , we propose a spectral algorithm for computing sparse graph wavelet bases . Interestingly , our analysis show that these bases are related to existing work on graph kernels [ 31 , 19 ] , including the wavelet design by Hammond et al . [ 16 ] .
3 . WAVELETS ON GRAPHS i j and X +1 i ∪ X +1 at level + 1 such that X +1
A graph is a tuple G(V , E ) , where V is a set of n vertices and E is a set of m ( unweighted ) edges , respectively . A signal W :V → R is a real valued function defined on the set of vertices V . In other words , W ( v ) is the value of the signal for a vertex v ∈ V . In Figure 1 we show an example of a graph G for which we define a signal W . A graph wavelet tree is a binary tree structure X ( G ) that partitions the graph recursively as follows . A root node X 1 1 contains all the vertices in the graph ( ie X 1 In k ⊆ V is the k th node at level with children general , X j = ∅ X +1 and X +1 k . We focus on binary trees since they have the same encoding power as n ary trees in this model . The tree X ( G ) defines spaces of functions , V and W , analogous to Haar wavelet spaces in harmonic analysis [ 21 ] . The space V1 contains functions that are constant on V . And , in general , V contains functions that are piecewise k at the level of X ( G ) . Let V be constant on nodes in X the space of functions that are constant on individual nodes in V . Bases to span such spaces can be constructed using functions 1X k and 0 , otherwise ( box functions ) . This formulation leads to a multiresolution V1 ⊂ V2 ⊂ . . .V for function spaces . Another set of function spaces in the form W contains wavelet functions ψk , with equal to 1 for v ∈ X
1 = V ) . ∩ X +1 j = X k i j
, ( 2 ) are orthogonal to 1X the following properties : ( 1 ) are piecewise constant on X +1 and X +1 k and ( 3 ) are 0 everywhere else . It follows that any function in W can be represented using V+1 . Also , for any level , V ⊥ W and V ⊕ W = V+1 , where ⊕ is the orthogonal sum . defined on X k i
We combine wavelet functions with 1V to produce an orthonormal basis for G . Intuitively , this basis supports the representation of any graph signal W as a linear combination of the average µ(W ) plus piecewise functions defined on recursive partitions of the vertices V ( see Figure 1 ) . A graph wavelet transform ϕW is a set of difference coefficients ak , : ak , =
µ(W ) , W , ψk , , otherwise if = k = 0
( 1 )
In particular , except for a0,0 , we can write ak , as : j
|
| ak , =
|X +1 k| |X v∈X+1 The sizes |X k| , |X +1
W ( v ) − |X +1 |X k| | are taken into account | and |X +1 because partitions might be unbalanced . Analogously , the wavelet inverse ϕ−1W is defined as : v∈X+1
W ( v )
( 2 ) j i i j i
−1W ( v ) = a0,0 +
ϕ where : k
1/|X +1
| , −1/|X +1 0 , j i
| ,
νk,(v)ak ,
( 3 ) i if v ∈ X +1 if v ∈ X +1 j otherwise
( 4 )
νk,(v ) =
Figure 1a shows the graph wavelet transform for a toy example . For instance , the value of a2,2 = ( 2.(−4 + ( −6 ) ) − 2.(−9 + ( −9)))/4 = 4 and the inverse ϕ−1W ( e ) = 0 + ( −28)/4 + 4/2 + ( −1)/1 = −6 = W ( e ) . An important property of the graph wavelet transform , known as Parseval ’s relation , is that the signal and its transform are equivalent representations ( ie ϕ−1ϕW = W ) for any signal W and wavelet tree X ( G ) . More formally , we can define the L2 energy of a graph wavelet coefficient as :
||ak,||2 =
|X +1 |.a2 |X +1 |2 i i k ,
|.a2 |X +1 |X +1 |2 j j k ,
+
= a2 k , |X +1 i
| + a2 k , |X +1 j
| ( 5 )
Using Equation 2 , we can show the Parseval ’s relation :
||ak,||2 =
|W ( v)|2
( 6 ) k v
In particular , a lossy compressed representation of W can be constructed by the following procedure : ( 1 ) Compute transform ϕW , ( 2 ) set the lowest energy coefficients ak , to 0 , ( 3 ) return the non zero wavelet coefficients ϕW from ϕW . In this setting , the error of the compression is the sum of the energies of the dropped coefficients . If W has a sparse representation in the transform ( frequency domain ) , where most of the energy is concentrated in a few high level coefficients , it can be compressed with small error .
Figure 1a illustrates a sparse representation of a graph signal W ( basis A ) . The fast decay of the difference coefficients
1177 ak , in the wavelet transform as the level increases leads to a high compression using the aforementioned algorithm . The signal can be approximated within L2 error of 1 % using the top coefficients a1,1 and a2,2 . However , by keeping the top coefficients for basis B ( Figure 1b ) , the error is 22 % .
In [ 13 ] ( see theorems 1 3 ) , the authors show that , if the energy of a wavelet coefficient ak , is bounded as a function of the size of its corresponding vertex set X k and the tree X ( G ) is almost balanced , then there is a sparse representation of W as a wavelet transform . Here , we tackle the problem from a more practical and data driven perspective , where a tree X ( G ) that leads to a sparse representation of W is unknown . Moreover , we add sparsity constraints to the description size of X ( G ) in order to enforce wavelet bases that are embedded in the graph structure . In the next section , we formalize the problem of computing wavelet basis using sparse cuts and characterize its hardness .
4 . WAVELET BASES VIA SPARSE CUTS
The existence of a good basis ( or tree ) for a signal W in a graph G provides relevant information about both W and G . We measure the description length of a wavelet tree X ( G ) as the size |X ( G)|E of its edge cut . The edge cut of a wavelet tree is the number of edges in the set E ⊆ E that , if removed , separates the leaf nodes of X ( G ) . In other words , i , v ∈ X b there is no path between any pair of vertices u ∈ X a in G(V , E − E ) whenever X a j are leaves of X ( G ) . A tree X ( G ) associated with a sparse cut requires a few edges to be removed in order to disconnect its leaf nodes . If |X ( G)|E < |E| , the energy of at least one coefficient a k of any transform ϕW will be always set to 0 and , as a consequence , the inverse ϕ−1ϕW ( v ) will be the same for any vertex v ∈ X k . As graphs have a combinatorial number of possible cuts , we formalize the problem of finding an optimal sparse wavelet basis in terms of ( L2 ) error minimization . i and X b j
Definition 1 . Optimal graph wavelet basis via sparse cuts . Given a graph G(V , E ) , a signal W , and a cut size q compute a wavelet tree X ( G ) with a cut |X ( G)|E of size q that minimizes ||W − ϕ−1ϕW||2 .
Figure 2 shows two candidate wavelet trees with cut size q = 4 for the same graph signal shown previously in Figure 1a . While the tree from Figure 2b achieves an error of 22 % , the one from Figure 2a is the optimal basis of cut size 4 for our example , with an error of 1 % . As discussed in Section 3 , a good basis generates sparse transforms , which maximize the amount of energy from the signal that is conserved in a few coefficients . In the remainder of this section , we analyze the hardness of computing sparse wavelet bases by connecting it to well known problems in graph theory .
Theorem 1 . Computing an optimal graph wavelet basis is NP hard .
Please refer to the extended version of this paper [ 30 ] for proofs of Theorems 1 , 2 and 5 . Theorem 1 shows that finding an optimal basis is NP hard using a reduction from the 3 multiway cut problem [ 7 ] , which leads to the question of whether such problem can be approximated within a constant factor in polynomial time . Theorem 2 shows that our problem is also NP hard to approximate by any constant .
Theorem 2 . Computing an optimal graph wavelet basis is NP hard to approximate by a constant .
( a ) Optimal wavelet basis
( b ) Alternative wavelet basis
Figure 2 : Two graph wavelet bases with cut of size 4 for the same signal . Reconstructed values are set to leaf nodes . The basis from Figure 2a achieves 1 % error and is optimal . An alternative basis with 22 % error is shown in Figure 2b .
Connecting the construction of sparse wavelet basis to a hard problem such as the 3 multiway cut is a key step for proving Theorems 1 and 2 . However , these constructions assume wavelet trees X ( G ) with a number of levels strictly larger than 2 ( ie more than two partitions are generated ) . A final question we ask regarding the hardness of our problem is whether there is an efficient algorithm for partitioning a set of nodes X . If so , one could apply such an algorithm recursively in a top down manner in order to construct a reasonably good wavelet basis . We can pose such a problem using the notion of L2 energy of graph wavelet coefficients from Equation 5 . k into children X +1 and X +1 j i
Definition 2 . Optimal graph wavelet cut . Given a graph G(V , E ) , a signal W , a constant k , and a set of nodes k ⊆ V , compute a partition of X and X +1 X that maximizes ||ak,||2 . k into X +1 j i
Theorem 3 rules out the existence of an efficient algorithm that solves the aforementioned problem optimally .
Theorem 3 . Computing an optimal sparse graph wavelet cut is NP hard .
Our proof ( in the appendix ) is based on a reduction from the graph bisection [ 12 ] and raises an interesting aspect of good graph wavelet bases , which is balancing . The problem of finding balanced partitions in graphs has been extensively studied in the literature , specially in the context of VLSI design [ 15 ] , image segmentation [ 27 ] and other applications
8 4 610 9 910a0,0 = 0 , a1,1 = 28a1,2 = 0 a2,2 = 4a2,3 = 0a3,3 = 09.3a1,3 = 0 9393 5 5 9 9abcdefgCumulative cut and corresponding signalapproximationa0,0 = 0 , a1,1 = 24a1,2 = 6 a2,2 = 0a3,3 = 0 8a1,3 = 0a2,3 = 0 933 8 891178 of spectral graph theory [ 4 ] . In the next section , we propose a spectral algorithm for computing graph wavelet bases .
5 . SPECTRAL ALGORITHM
Our approach combines structural and signal information as a vector optimization problem . By leveraging the power of spectral graph theory , we show how a relaxed version of this formulation is a regularized eigenvalue problem , which can be solved using 1 D search and existing eigenvalue computation procedures . Our discussion focuses on computing a single cut ( Definition 2 ) and extends to the computation of a complete basis . Section 5.3 is focused on performance . 5.1 Formulation First , we introduce some notation . The degree dv of a vertex v is the number of vertices u ∈ V such that ( u , v ) ∈ E . The degree matrix D of G is an n × n diagonal matrix with Dv,v = dv for every v ∈ V and Du,v = 0 , for u = v . The adjacency matrix A of G is an n × n matrix such that Au,v = 1 if ( u , v ) ∈ E and Au,v = 0 , otherwise1 . The Laplacian of G is defined as L = D − A . We also define a second matrix C = nI − 1n×n , where I is the identity matrix and 1n×n is an n × n matrix of 1 ’s . The matrix C can be interpreted as the Laplacian of a complete graph with n vertices . The third matrix , which we call S , is a matrix of pairwise squared differences with Su,v = ( W ( u)− W ( v))2 for any pair of nodes u , v ∈ V . Notice that these matrices k , E ) , can also be computed for an induced subgraph G(X where E = {(u , v)|u ∈ X In order to formulate the problem of finding an optimal k| dimensparse wavelet cut in vectorial form , we define a |X k into X +1 sional indicator vector x for the partition of X k , xv = −1 if v ∈ X +1 and X +1 and xv = 1 if v ∈ X +1 . By combining the matrices ( C , S , L ) and the indicator vector x , the following Theorem shows how the problem from Definition 2 can be rewritten as an optimization problem over vectors ( see appendix for the proof ) .
. For any v ∈ X k ∧ v ∈ X k} . j j i i
Theorem 4 . The problem of finding an optimal sparse graph wavelet partition ( Definition 2 ) can be written as : x∗ = min a(x ) st . x
Lx ≤ 4q
( 7 ) x∈{−1,1}n where a(x ) = x CSCx xCx and q is the maximum cut size .
Theorem 4 does not make the problem of computing an optimal wavelet basis any easier . However , we can now define a relaxed version of our problem by removing the constraint that xi ∈ {−1 , 1} . Once real solutions ( xi ∈ R ) are allowed , we can compute an approximate basis using eigenvectors of a well designed matrix . The next corollary follows directly from a variable substitution and properties of Lagrange multipliers in eigenvalue problems [ 10 , chapter 12 ] .
Corollary 1 . A relaxed version of the problem from Definition 2 can be solved as a regularized eigenvalue problem : x∗ = min x a(x )
= min x x
CSCx xCx + βxLx
( 8 ) where y∗ = miny βL)+ ) of ( C + βL ) and β is a regularization factor . yy , M = ( (C + βL)+ )
1 2 CSC((C + 2 x , ( C + βL)+ is the pseudoinverse
1 2 , y = ( C + βL )
M y y
1
This eigenvalue problem is well defined due to properties of the matrix M , which is real and symmetric . In fact , M is negative semidefinite , since the energy ||ak,||2 of a wavelet coefficient is non negative . We apply the pseudoinverse ( C + βL)+ because C and L are positive semidefinite and thus their standard inverses are not well defined—they both have at least one zero eigenvalue .
At this point , it is not clear how the matrix M captures both signal and structural information as means to produce high energy sparse wavelet cuts . In particular , we want to provide a deeper insight into the role played by the regularization factor β in preventing partitions that are connected by many edges in G . To simplify the notation and without loss of generality , let us assume that X k = V and that V has 0 mean . The next theorem gives an explicit form for the entries of M based on the node values and graph structure :
Theorem 5 . The matrix M is in the form : n n n v=1 u=1 r=2
Mij = 2n2 er,ier,u
1√ n
λr r=2
W ( u).W ( v )
1√ λr er,ver,j
( 9 ) where ( λr , er ) is an eigenvalue eigenvector pair of the matrix ( C + βL ) such that λr > 0 .
Based on Theorem 5 , we can interpret M as a Laplacian regularized matrix and Expression 8 as a relaxation of a maximum cut problem in a graph with Laplacian matrix −M . In this setting , the largest eigenvalue of −M is known to be a relaxation of the maximum cut in the corresponding graph . The matrix ( C + βL ) is the Laplacian of a graph G associated to G with the same set of vertices but edge weights wu,v = 1 + β if ( u , v ) ∈ G , and wu,v = 1 , otherwise . Intuitively , as β increases , G becomes a better representation of a weighted version of G with Laplacian matrix βL . For instance , if β = 0 , G is a complete graph with all nonzero eigenvalues equal to n and G has no effect over the weights of the cuts in M . In other words , the wavelet cut selected will simply maximize the sum of ( negative ) products −W ( u).W ( v ) and separate nodes with different values . On the other hand , for large β , the eigenvalues λr will capture the structure of G and have a large magnitude . The relative importance of a product −W ( u).W ( v ) will be reduced whenever u and v are well connected to nodes i and j , respectively , in G . As a consequence , the cuts selected will rather cover edge pairs ( i , j ) for which far away nodes u and v in G have different values for the signal W .
Expressions in the form i define regularizations via the Laplacian , which have been studied in the context of kernels on graphs [ 31 , 19 ] and also wavelets [ 16 , 20 ] . Notice that the regularization factor β is not known a priori , which prevents the direct solution of the relaxation given by Expression 8 . However , we can apply a simple 1 D search algorithm ( eg golden section search [ 18 ] ) in order to compute an approximate optimal β within a range [ 0 , βmax ] . r g(λr)eie
= ( (C + βL)+ )
1
2 y∗
1Our method can be generalized to weighted graphs .
( x∗ , β∗ ) = min
β min x a(x ) st . x
Lx ≤ 4q
( 10 )
1179 Algorithm 1 Spectral Algorithm
Require : Graph G , values W , set X k , regularization constant β , cut size q i j and X +1
Ensure : Partitions X +1 1 : C ← n × n Laplacian of complete graph 2 : L ← n × n Laplacian of G 3 : S ← n × n squared difference matrix of G 4 : x∗ ← minx a(x ) 5 : ( X1 , X2)z ← cut ( {1 , 2 . . . z} , {z + 1 . . . n} ) 6 : ( X +1
) ← max(X1,X2)j
, X +1 j i
||ak,||2 st . cut size |(X1 , X2)| ≤ q
( a ) Graph signal
( b ) Eigenvector/cut
Figure 3 : Example of a cut of size q = 2 found by the spectral algorithm . The eigenvector x is rounded using a sweep procedure and the best wavelet cut is selected .
5.2 Algorithm i j
, X +1
) that partitions X
Algorithm 1 describes our spectral algorithm for computing sparse graph wavelet cuts . Its inputs are the graph G , the signal W , a set of nodes X k from G , the regularization constant β , and the cut size q . As a result , it returns a cut ( X +1 k by maximizing the energy ||ak,||2 and has at most q edges . The algorithm starts by constructing matrices C , L and S based on G and W ( lines 1 3 ) . The best relaxed cut x∗ is computed using Equation 8 ( line 4 ) and a wavelet cut is obtained using a standard sweeping approach [ 27 ] ( lines 5 6 ) . Vertices in X k are sorted in non decreasing order of their value in x∗ . For each value xu , the algorithm generates a candidate cut ( X1 , X2)j by setting xv = −1 if v < u , and xv = 1 , otherwise ( line 5 ) . The cut with size |(X +1 )| at most q that maximizes the energy ||ak,|| is selected among the candidate ones ( line 6 ) and is returned by the algorithm .
, X +1 j i
Figure 3 illustrates a wavelet cut of size q = 2 discovered by our spectral algorithm . The input graph and its signal are given in Figure 3a . Moreover , we show the value of the eigenvector x that maximizes Expression 8 for each vertex and the resulting cut after rounding in Figure 3b . Notice that x captures both signal and structural information , assigning similar values to vertices that have small difference regarding the signal and are near in the graph . The energy ||ak,||2 associated with the cut is 457 ( 96 % of the energy of the signal ) , which is optimal in this particular setting .
We evaluate Algorithm 1 using several datasets in our experiments . However , an open question is whether such an algorithm provides any quality guarantee regarding its solution ( for a single cut ) . One approach would be computing a lower bound on the L2 energy of the wavelet cuts generated by the rounding algorithm , similar to the Cheeger ’s inequality for the sparsest cut [ 4 ] . Unfortunately , proving such a bound has shown to be quite challenging and will be left as future work . For a similar proof regarding an approximation for the max cut problem , please refer to [ 33 ] .
We apply Algorithm 1 recursively in order to construct a complete graph wavelet basis . Starting with the set of nodes V , we repeatedly compute new candidate wavelet cuts and select the one with maximum L2 energy ( ie it is a greedy algorithm ) . Once there is no feasible cut given the remaining budget of edges , we compute the remaining of the basis using ratio cuts , which do not depend on the signal . 5.3 Efficient Approximation
Here , we study the performance of the algorithm described in the previous section and describe how it can be approximated efficiently . Although performance is not the main focus of this paper , we still need to be able to compute wavelets on large graphs . The most complex step of Algorithm 1 is computing the matrix M ( see Corollary 1 ) , which involves ( pseudo )inverting and multiplying dense matrices . Moreover , the algorithm also requires the computation of the smallest eigenvalue/eigenvector of M .
A naive implementation of our spectral algorithm would take O(n3 ) time to compute the pseudo inverse ( C + βL)+ , O(n3 ) time for computing matrix products , and other O(n3 ) time for the eigen decomposition of M . Assuming that the the optimal value of β ( Equation 10 ) is found in s iterations , the total complexity of this algorithm is O(sn3 ) , which would hardly enable the processing of graphs with more than a few thousand vertices . Therefore , we propose a fast approximation of our algorithm by removing its dependence of β and using Chebyshev polynomials and the Power Method . Our original algorithm searches for the optimal value of the regularization constant β using golden search , which requires several iterations of Algorithm 1 . However , our observations have shown that typical values of β found by the search procedure are large , even for reasonable values of q , compared to the number of edges in G . Thus , we propose simplifying Equation 8 to the following : x
CSCx xLx
( 11 )
As a consequence , we can compute a wavelet cut with a single execution of our spectral algorithm . Using Theorem 5 , we can show that dropping the matrix C from the denominator has only a small effect over the resulting matrix M . First , consider the eigenvalue eigenvector pairs ( λr , er ) of ( C + βL ) and let ( λl , el ) and ( λc , ec ) be the eigenvalueeigenvector pairs for non zero eigenvalues of L and C , respectively . Given that C is the Laplacian of a complete graph , we know that λc = n , for any c , and every vector orthogonal to the constant vector 1n is an eigenvector of C . In particular , any eigenvector el of L is an eigenvector of C . From the definition of eigenvalues/eigenvectors , we get that ( C + βL)el = ( n + βλl)el and thus ( n + βλl , el ) is an eigenvalue eigenvector pair of ( C + βL ) .
Nevertheless , computing all the eigenvalues of the graph Laplacian L might still be prohibitive in practice . Thus , we avoid the eigen decomposition by computing an approximated version of M using Chebyshev polynomials [ 16 ] . These polynomials can efficiently approximate an expression in the r g(λr)er,ier,j and f is a real vector . We can apply the same approach to approximate the product ( (L+ ) form υ , f , where υi =
2 × CSC)i,j by setting g and f as :
1
10 4 610 9 98abcdefg0802 0608 1 11abcdefg1180 ( a ) Scalability
( b ) Energy
( c ) Noise
( d ) Sparsity
Figure 4 : Scalability and L2 energy associated to the cuts discovered by the sparse wavelet transform ( SWT ) and its fast approximation ( FSWT p ) for different number of polynomial coefficients ( p ) and varying the graph size ( a ) , the energy of the cut in the data ( b ) , the noise level ( c ) , and the sparsity of the cut ( d ) using synthetic datasets . Our fast approximation is up to 100 times faster than the original algorithm and achieves accurate results even when p is relatively small ( 20 ) .
1√ λr
,
1 g(λr ) = f = CSC:,j
( 12 ) where λr ∈ [ 1 , n ] and : , j is an index for a matrix column . Chebyshev polynomials can be computed iteratively with cost dominated by a matrix vector multiplication by L . By truncating these polynomials to p terms ( ie iterations ) , each one with cost O(mn ) , where m is the number of edges , and n is the number of nodes , we can approximate this matrix product in O(pmn ) time . For sparse matrices ( m = O(n ) ) and small p , pmn n3 , which leads to significant performance gains over the naive approach . In order to compute M , we can repeat the same process with f = 2 × CSC)j, : , where j , : is an index for a matrix row . ( (L+ ) Once the matrix M is constructed , it remains to compute its eigenvector associated to the smallest eigenvalue . A trivial solution would be computing all the eigenvectors of M , which can be performed in time O(n3 ) . However , due to the fact that our matrix is negative semidefinite , its smallest eigenvector can be approximated more efficiently using the Power Method [ 14 ] , which requires a few products of a vector and M . Assuming that such method converges to a good solution in t iterations , we can approximate the smallest eigenvalue of M in time O(tn2 ) . Moreover , the compu1 tation of x from y using ( L+ ) 2 can also be performed via Chebyshev polynomials in time O(pm ) .
The time taken by our improved algorithm to compute a single cut is O(pmn + tn2 ) , where p is the number of terms in the Chebyshev polynomial , m = |E| , n = |V | , and t is the number of iterations of the Power method . This complexity is a significant improvement over the O(sn3 ) time taken by its naive version whenever p , m , and t are small compared to n . For computing all the cuts , the total worst case time complexity of the algorithm is O(qpmn + qtn2 ) , where q is the size of the cut of the wavelet tree X ( G ) . However , notice that good bases tend to be balanced ( see Theorem 3 ) and in such case our complexity decreases to O(pmn + tn2 ) . 6 . EXPERIMENTS
We evaluate our algorithms for computing sparse wavelet bases using synthetic and real datasets . We start by analyzing the scalability and quality of our efficient approximation compared to the original algorithm . Next , we compare our approach against different baselines and using four real datasets in the signal compression task . This section ends with some visualizations of the sparse wavelet formulation , which provides further insights into our algorithm . All the implementations are available as open source and we also provide the datasets applied in this evaluation2 . 6.1 Scalability and Approximation
The results discussed in this section are based on a synthetic data generator for both the graph and an associated signal . Our goal is to produce inputs for which the best wavelet cut is known . The data generator can be summarized in the following steps : ( 1 ) Generate sets of nodes V1 and V2 such that |V1| = |V2| ; ( 2 ) Generate m edges such that the probability of an edge connecting vertices in V1 and V2 is given by a sparsity parameter h ; ( 3 ) Assign average values µ1 and µ2 to V1 and V2 , respectively , so that the energy of the cut ( V1 , V2 ) is equal to an energy parameter α ; ( 4 ) Draw values from a Gaussian distribution N ( µi , σ ) for each vertex set Vi , where σ is a noise parameter .
Proper values for the averages are computed using Equation 13 . We set default values for each parameter as follows : number of vertices n = 500 and edges m = 3n , sparsity h = .5 , and noise σ = |µi| . These parameters are varied in each experiment presented in Figure 4 . For SWT , we fix the value of βmax in the golden search to 1000 and , for the fast approximation ( FSWT ) , we vary the number of Chebyshev polynomials applied ( 5 , 20 , and 50 ) . The number of iterations of the Power method to approximate the eigenvectors of M is fixed at 10 , which achieved good results in our experiments . Figure 4a compares FSWT and the original algorithm ( SWT ) varying the graph size ( n ) , showing that FSWT is up to 100 times faster than SWT . In Figures 4b 4d , we compare the approaches in terms of the energy ||a1,1||2 of the first wavelet cut discovered varying the synthetic signal parameters . The results show that FSWT achieves similar or better results than SWT for relatively few coefficients ( p = 20 ) in all the settings . 6.2 Compression
We evaluate our spectral algorithm for sparse wavelet bases in the signal compression task . Given a graph G and a signal W , the goal is to compute a compact representation W that minimizes the L2 error ( ||W − W ||2 ) . For the baselines , the size of the representation is the number of coefficients of the transform kept in the compression , relative to the size of the
2https://github.com/arleilps/sparse wavelets
1181 ( a ) Traffic
( b ) Human
( c ) Wiki
( d ) Blogs
Figure 5 : Compression results for the Traffic , Human , Wiki , and Blogs . Our approach ( FSWT ) outperforms the baselines in most of the settings considered . In particular , FSWT achieves up to 80 times lower error than the best baseline ( GWT ) . dataset . We also take into the account the representation cost of the cuts ( log(m ) bits/edge ) for our approach .
Datasets : Four datasets are applied in our evaluation . Small Traffic and Traffic are road networks from California for which vehicle speeds –measured by sensors– are modeled as a signal , with n = 100 and m = 200 , and n = 2K and m = 6K , respectively [ 23 ] . Human is a gene network for Homo Sapiens with expression values as a signal where n = 1K and m = 1K [ 24 ] . Wiki is a sample of Wikipedia pages where the ( undirected ) link structure defines the graph and the signal is the number of page views for each page with n = 5K and m = 25K . Blogs is a network of blogs with political leaning ( 1 for left and 1 for right ) as vertex attributes [ 1 ] ( n = 1K and m = 17K ) . Notice that these graphs have sizes in the same scale as the ones applied by existing work on signal processing on graphs [ 28 , 13 , 11 ] . We normalize the values to the interval [ 0 , 1 ] to make the comparisons easier .
Baselines : We consider the Graph Fourier Transform ( FT ) [ 25 , 28 ] and the wavelet designs by Hammond et al . ( HWT ) [ 16 ] and Gavish et al . ( GWT ) [ 13 ] as baselines . Instead of the original bottom up partitioning algorithm proposed for GWT , we apply ratio cuts [ 15 ] , which is more scalable and achieves comparable results in practice .
Figure 6a shows compression results for Small Traffic . The best baselines ( GWT and FT ) incur up to 5 times larger error than our approaches ( SWT and FSWT ) . Figures 5a5d show the results for FSWT , GWT , and FT using the remaining datasets . Experiments for HWT and SWT took too long to finish and were terminated . FSWT outperforms the baselines in most of the settings , achieving up to 5 , 6 , 2 , and 80 times lower error than the best baseline ( GWT ) for Traffic , Human , Wikipedia , and Blogs , respectively . FT performs surprisingly well for Blogs because vertex values are almost perfectly separated into two communities , and thus some low frequency eigenvectors are expected to approximately match the separation ( see [ 1 , Fig 3] ) . As the size of the representation increases , FSWT is the only method able to separate values at the border of the communities .
These results offer strong evidence that our sparse wavelet bases can effectively encode both the graph structure and the signal . The main advantage of our approach is building bases that are adapted to the signal by cutting few edges in the graph . The compression times of our algorithm are comparable with the baselines , as shown in Table 6b . 6.3 Visualization
Finally , we illustrate some interesting features of our sparse wavelet formulation using graph drawing . Eigenvectors of
( a ) Compression for Small Traffic
Small Traffic Traffic Human Wiki Blogs
HWT
FT
GWT SWT FSWT
8 1 1 1 1
35 5 18
2 11 14
381 386
425
7 47 38
( b ) Average compression times ( in secs ) .
Figure 6 : Compression results for Small Traffic and compression times for all methods and the datasets . Our approaches ( SWT and FSWT ) outperform the baselines while taking comparable compression time . the Laplacian matrix are known to capture the community structure of graphs , and thus can be used to project vertices in space . In particular , if e2 and e3 are the second ( Fiedler ) and the third eigenvectors of the Laplacian matrix , we can draw a graph in 2 D by setting each vertex vi ∈ V to the position ( e2(i ) , e3(i) ) . Following the same approach , we apply the smallest eigenvectors of the matrix M ( see Corollary 1 ) to draw graphs based on both the structure and a signal . Figure 7 presents drawings for two graphs , one is the traditional Zachary ’s Karate club network with a synthetic heat signal starting inside one community and the other is Small Traffic . Three different drawing approaches are applied : ( 1 ) The Scalable Force Directed Placement ( SFDP ) [ 17]3 , the Laplacian eigenvectors , and the wavelet eigenvectors . Both SFDP and the Laplacian are based on the graph structure only . The drawings demonstrate how our wavelet formulation separates vertices based on both values and structure .
7 . CONCLUSION
Signal Processing in Graphs ( SPG ) is a powerful framework for modeling complex data arising from several applica
3Implemented by GraphViz : http://wwwgraphvizorg/
1182 be interpreted as representing the official policies , either expressed or implied , of the Army Research Laboratory or the US Government . The US Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on .
( a ) SFDP
( b ) Laplacian
( c ) Wavelet
Zachary ’s karate club .
8 . REFERENCES
[ 1 ] L . A . Adamic and N . Glance . The political blogosphere and the 2004 us election : divided they blog . In Workshop on Link Discovery , 2005 .
[ 2 ] T . Bui , S . Chaudhuri , F . Leighton , and M . Sipser . Graph bisection algorithms with good average case behavior . Combinatorica , 7:171–191 , 1987 .
[ 3 ] J . Cheeger . A lower bound for the smallest eigenvalue of the laplacian . Problems in analysis , 625:195–199 , 1970 .
[ 4 ] F . R . Chung . Spectral graph theory . American
Mathematical Society , 1997 .
[ 5 ] R . Coifman and M . Maggioni . Diffusion wavelets .
Applied and Computational Harmonic Analysis , 21:53–94 , 2006 .
[ 6 ] M . Crovella and E . Kolaczyk . Graph wavelets for spatial traffic analysis . In INFOCOM , 2003 . [ 7 ] E . Dahlhaus , D . Johnson , C . Papadimitriou ,
P . Seymour , and M . Yannakakis . The complexity of multiway cuts . In STOC , 1992 .
[ 8 ] J . Edmonds and R . M . Karp . Theoretical improvements in algorithmic efficiency for network flow problems . Journal of the ACM , 19:248–264 , 1972 . [ 9 ] S . Fortunato . Community detection in graphs . Physics
Reports , 486:75–174 , 2010 .
[ 10 ] J . Friedman , T . Hastie , and R . Tibshirani . The elements of statistical learning . Springer , 2001 .
[ 11 ] A . Gadde , A . Anis , and A . Ortega . Active semi supervised learning using sampling theory for graph signals . In SIGKDD , 2014 .
[ 12 ] M . R . Garey and D . S . Johnson . Computers and intractability . WH Freeman , 2002 .
[ 13 ] M . Gavish , B . Nadler , and R . Coifman . Multiscale wavelets on trees , graphs and high dimensional data . In ICML , 2010 .
[ 14 ] G . H . Golub and C . F . Van Loan . Matrix computations . JHU Press , 2012 .
[ 15 ] L . Hagen and A . B . Kahng . New spectral methods for ratio cut partitioning and clustering . IEEE Transactions on Computer aided Design of Integrated Circuits and Systems , 11(9):1074–1085 , 1992 .
[ 16 ] D . Hammond , P . Vandergheynst , and R . Gribonval .
Wavelets on graphs via spectral graph theory . Applied and Computational Harmonic Analysis , 30:129–150 , 2011 .
[ 17 ] Y . Hu . Efficient , high quality force directed graph drawing . Mathematica , 10:37–71 , 2005 .
[ 18 ] J . Kiefer . Sequential minimax search for a maximum .
In Proceedings of the AMS , 1953 .
[ 19 ] J . Lafferty and G . Lebanon . Diffusion kernels on statistical manifolds . JMLR , 6:129–163 , 2005 .
[ 20 ] N . Leonardi and D . Van De Ville . Tight wavelet frames on multislice graphs . IEEE Transactions on Signal Processing , 61:3357–3367 , 2013 .
( d ) SFDP
( e ) Laplacian
( f ) Wavelet
Traffic .
Figure 7 : Drawing graphs using SFDP ( a,d ) and Laplacian ( b,e ) and wavelet eigenvectors ( c,f ) . Vertices are colored based on values ( red=high , green=average and blue=low ) . Different from the other schemes , wavelet eigenvectors are based on both signal and structure ( better seen in color ) . tions . A major challenge in SPG is relating properties of the graph signal , the graph structure and the transform . Graph wavelets are able to effectively model a smooth graph signal conditioned to the existence of a hierarchical partitioning of the graph that captures the geometry of the graph structure as well as the signal . Our work is the first effort to build such hierarchies in a compact fashion . We first introduced the problem of computing graph wavelet bases via sparse cuts and show that it is NP hard—even to approximate by a constant—by connecting it to existing problems in graph theory . Then , we proposed a novel algorithm for computing sparse wavelet bases by solving regularized eigenvalue problems using spectral graph theory . While naively considering both structure and values can lead to computationally intensive operations , we introduced an efficient solution using several techniques . These approaches are extensively evaluated using real and synthetic datasets and the results provide strong evidence that our solution produces compact and accurate representations for graph signals in practice .
This work opens several lines for future investigation : ( i ) It remains an open question whether approximating a single optimal wavelet cut is NP hard . ( ii ) The wavelet design applied in this work maps only to a particular type of wavelets ( Haar ) ; extending our approach to other wavelet functions ( eg Mexican hat , Meyer [ 21 ] ) might lead to better representations for particular classes of signals . ( iii ) Generalizing the ideas presented here to time varying graph signals might lead to novel algorithms for anomaly detection , event discovery , and data compression .
Acknowledgment . Research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF 09 2 0053 ( the ARL Network Science CTA ) . The views and conclusions contained in this document are those of the authors and should not
1183 [ 21 ] S . Mallat . A wavelet tour of signal processing .
Academic Press , 1999 .
[ 22 ] D . Mohan , M . T . Asif , N . Mitrovic , J . Dauwels , and
P . Jaillet . Wavelets on graphs with application to transportation networks . In ITSC , 2014 .
[ 23 ] M . Mongiovi , P . Bogdanov , and A . Singh . Mining evolving network processes . In ICDM , 2013 .
[ 24 ] F . Moser , R . Colak , A . Rafiey , and M . Ester . Mining cohesive patterns from graphs with feature vectors . In SDM , 2009 .
[ 25 ] A . Sandryhaila and J . Moura . Discrete signal processing on graphs . In ICASSP , 2013 .
[ 26 ] A . Sandryhaila and J . Moura . Big data analysis with signal processing on graphs . IEEE Signal Processing Magazine , 31:80–90 , 2014 .
[ 27 ] J . Shi and J . Malik . Normalized cuts and image segmentation . IEEE Transactions on Pattern Analysis and Machine Intelligence , 22:888–905 , 2000 .
[ 28 ] D . Shuman , S . Narang , P . Frossard , A . Ortega , and
P . Vandergheynst . The emerging field of signal processing on graphs . IEEE Signal Processing Magazine , 2013 .
[ 29 ] A . Silva , P . Bogdanov , and A . K . Singh . Hierarchical in network attribute compression via importance sampling . In ICDE , 2015 .
[ 30 ] A . Silva , X H Dang , P . Basu , A . Singh , and
A . Swami . Graph wavelets via sparse cuts . http://arxivorg/abs/160203320 , 2016 .
[ 31 ] A . Smola and R . Kondor . Kernels and regularization on graphs . In Learning theory and kernel machines , volume 2777 , pages 144–158 . 2003 .
[ 32 ] I . Toˇsi´c and P . Frossard . Dictionary learning . IEEE
Signal Processing Magazine , 28:27–38 , 2011 .
[ 33 ] L . Trevisan . Max cut and the smallest eigenvalue . SIAM Journal on Computing , 41:1769–1786 , 2012 .
APPENDIX Proofs for Theorems 1 , 2 and 5 are provided in [ 30 ] .
Proof of Theorem 3 Proof . We use a reduction from graph bisection , which given a graph G(V , E ) and a constant q , asks whether there is a set of q edges in E that , if removed , would break G into two equal parts ( assume |V | is even ) . Graph bisection is NP complete [ 12 ] . By substituting Expression 2 in Expression 5 , we obtain the following expression :
||ak,||2 = ( µ(X +1 i
) − µ(X +1 j
||X +1 ))2 |X +1 |X k| j i
|
( 13 )
For a given instance of the graph bisection problem , we generate |V |(|V |−1)/2 instances of the sparse wavelet basis problem , one for each pair of vertices ( u , v ) in V . Set V = V ∪ {s , t} , E = E ∪ {(s , u ) , ( v , t)} , W ( s ) = 1 , W ( t ) = −1 , and W ( u ) = 0 for u ∈ V . From Expression 13 , we get that s and t have to be separate from each other in an optimal partitioning . Moreover , since |X +1 | is fixed , the energy is maximized when the partitions have equal size , with value 4(|V | + 1)2/(|V | + 2)2 .
| + |X +1 j i
Proof of Theorem 4 Proof . We start by rewriting Expression 5 in terms of pairwise differences ( we drop the index ) :
||ak||2 =
−2|Xi||Xj|  u∈Xi ( W ( u ) − W ( v))2
( W ( u ) − W ( v))2
−1
2|Xi||Xj||Xk|
+ |Xj|2 +|Xi|2 u,v∈Xi u,v∈Xj v∈Xj
( W ( u ) − W ( v))2
( 14 )
|Xk| is a constant and can be dropped from the denomiCx is the quadratic form of the Laplacian C , thus : nator . x u,v∈C x
Cx =
( xu − xv)2 = 4|Xi||Xj|
Similarly , x
Lx is the standard quadratic form for the size of the cut between two partitions in G : x
Lx =
( xu − xv)2 = 4|{(u , v ) ∈ E|u ∈ Xi ∧ v ∈ Xj}| u,v∈E

−1 −1
. . . . . . . . . . . .
−1
( n − 1 )
( n − 1 )
−1
( n − 1 )
Regarding x x
C = [ x1 . . . xn ] ×
CSCx :
  x1(n − 1 ) − xn(n − 1 ) −
=
( x xb(n − 1 ) − xi =
Also Cx = ( x
C i=b
)
−1 −1 i=1 xi
 −2|Xj| , i=n xi
2|Xi| , a quadratic form for the matrix S : z
Sz =
( wv − wu)2zvzu
C)b can take two possible values , depending on xb :
( 15 ) if xb = −1 otherwise
( 16 )
. Therefore , x
CSCx is also
( wv − wu)2|Xj|2
( wv − wu)2|Xi|2
( 17 )
( wv − wu)2|Xi||Xj|
= ( x
C ) v∈Xk u∈Xj u∈Xi v∈Xj v∈Xi u∈Xi v∈Xj u∈Xk = −4
− 4
+ 8 where z = Cx . This ends the proof .
1184
