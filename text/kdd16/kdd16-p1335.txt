Partial Label Learning via Feature Aware Disambiguation
Min Ling Zhang 1,2
Bin Bin Zhou 1,2
Xu Ying Liu 1,2
1 School of Computer Science and Engineering , Southeast University , Nanjing 210096 , China
2 Key Laboratory of Computer Network and Information Integration ( Southeast University ) ,
Ministry of Education , China
{zhangml , zhoubinbin , liuxy}@seueducn
ABSTRACT Partial label learning deals with the problem where each training example is represented by a feature vector while associated with a set of candidate labels , among which only one label is valid . To learn from such ambiguous labeling information , the key is to try to disambiguate the candidate label sets of partial label training examples . Existing disambiguation strategies work by either identifying the ground truth label iteratively or treating each candidate label equally . Nonetheless , the disambiguation process is generally conducted by focusing on manipulating the label space , and thus ignores making full use of potentially useful information from the feature space . In this paper , a novel two stage approach is proposed to learning from partial label examples based on feature aware disambiguation . In the first stage , the manifold structure of feature space is utilized to generate normalized labeling confidences over candidate label set . In the second stage , the predictive model is learned by performing regularized multi output regression over the generated labeling confidences . Extensive experiments on artificial as well as real world partial label data sets clearly validate the superiority of the proposed feature aware disambiguation approach .
Keywords weak supervision ; partial label learning ; disambiguation ; manifold
1 .
INTRODUCTION
Partial label ( PL ) learning refers to the problem where each training example is represented by a single instance ( feature vector ) while associated with a set of candidate labels [ 8 , 26 ] . Among the candidate label set , only one label is assumed to be valid and not directly accessible to the learning algorithm . The need to learn from data with partial labeling information naturally arises in many real world applications such as automatic image annotation [ 7 , 25 ] , web mining [ 14 ] , ecoinformatics [ 16 ] , etc.1
1In some literatures , the partial label learning framework is also named as ambiguous label learning [ 5 , 13 ] or superset label learning [ 17 ] .
⊤
Formally speaking , let X = Rd denote the d dimensional feature space and Y = {y1 , y2 , . . . , yq} denote the label space with q class labels . The task of partial label learning is to learn a multiclass classifier f : X 7→ Y from the partial label training set D = {(xi , Si ) | 1 ≤ i ≤ m} . Here , for each PL training examis a d dimensional feature ple ( xi , Si ) , xi = ( xi1 , xi2 , . . . , xid ) vector and Si ⊆ Y is the set of candidate labels associated with xi . Partial label learning takes the basic assumption that the groundtruth label yi for xi resides in its candidate label set ( ie yi ∈ Si ) but unknown to the learning algorithm .
Apparently , the available labeling information in the training set is ambiguous as the ground truth label is concealed in the candidate label set . The key for successful partial label learning is therefore trying to disambiguate the set of candidate labels , where existing strategies include disambiguation by identification or disambiguation by averaging . For identification based disambiguation , the ground truth label is regarded as latent variable and identified through iterative refining procedure such as EM [ 5 , 15 , 16 , 18 , 24 ] . For averaging based disambiguation , all the candidate labels are treated equally and the prediction is made by averaging their modeling outputs [ 8 , 13 , 27 ] .
′
By taking specific views on the candidate labels , both of the existing strategies conduct disambiguation by only focusing on the manipulation of label space . Nonetheless , it is natural to postulate that the potentially useful information from feature space should also be exploited to facilitating the disambiguation process . Specifically , to help disambiguate the candidate label set , one might make use of the smoothness assumption that examples close to each other in the feature space will tend to share identical label in the label space . For instance , suppose we have three PL training examples ( x,{y2 , y3} ) , ( x ,{y3 , y4} ) where x is shown ′ to be close to x in the feature space . Then , it is reasonable to assign higher labeling confidence on y2 than y3 for x ( ie P(y2 | x ) > P(y3 | x) ) , as y2 is a shared candidate label ′ between close instances ( x and x ) while y3 is a shared candidate ′′ label between distant instances ( x and x
,{y1 , y2} ) and ( x ′′ ′′ while far from x
In light of the above observation , a novel two stage approach named PL LEAF , ie Partial Label LEArning via Feature aware disambiguation , is proposed in this paper . In the first stage , the manifold structure among training examples is analyzed in the feature space and then utilized to generate normalized labeling confidences over candidate label set . In the second stage , a multi class predictive model is learned by fitting a regularized multi output regressor with the generated labeling confidences . Extensive experiments on controlled UCI data sets as well as real world PL data sets clearly show the effectiveness of feature aware disambiguation for partial label learning .
) .
The rest of this paper is organized as follows . Section 2 briefly
1335 reviews related work in partial label learning . Section 3 introduces the proposed PL LEAF approach . Section 4 reports experimental results of comparative studies . Finally , Section 5 concludes the paper and discusses future research issues .
2 . RELATED WORK
Due to the ambiguous labeling information conveyed by PL training examples , partial label learning can be regarded as a weaklysupervised learning framework . It situates between two ends of the supervision spectrum , ie standard supervised learning with explicit supervision and unsupervised learning with blind supervision . Furthermore , partial label learning is related to other wellstudied weakly supervised learning frameworks , including semisupervised learning , multi instance learning and multi label learning , while the weak supervision scenario for partial label learning is different to those counterpart frameworks . Semi supervised learning [ 4 , 29 ] aims to learn a predictive model f : X 7→ Y from few labeled data together with abundant unlabeled data . For unlabeled data the ground truth label assumes the entire label space , while for PL data the ground truth label is confined within its candidate label set . Multi instance learning [ 1 , X 7→ Y from training 9 ] aims to learn a predictive model f : 2 examples each represented as a labeled bag of instances . For multiinstance data the label is assigned to bag of instances , while for PL data the label is assigned to single instance . Multi label learning [ 11 , 28 ] aims to learn a predictive model f : X 7→ 2 from training examples each associated with multiple labels . For multi label data the associated labels are all valid ones , while for PL data the associated labels are only candidate ones .
Y m
)
∑
F ( xi , y ;
Existing approaches learn from PL training examples by trying to disambiguate their candidate label sets . One disambiguation strategy is to assume certain parametric model F ( x , y ; ) where the ground truth label is regarded as latent variable and identified as ^yi = arg maxy∈Si F ( xi , y ; ) . Generally , the latent variable ( ∑ is refined iteratively via EM procedure which optimizes objective ∑ function defined according to the maximum likelihood criterion : m [ 15 , 16 ] , or the maximum margin y∈Si i=1 log i=1(maxy∈Si F ( xi , y ; ) − maxy /∈Si F ( xi , y ; ) ) criterion : [ 18 , 24 ] . One potential drawback of the identification based disambiguation strategy lies in that , rather than recovering the groundtruth label yi , the identified label ^yi might turn out to be false positive label in the candidate label set ( ie Si \ {yi} ) . ∑
Another disambiguation strategy is to assume equal importance of each candidate label and then make prediction by averaging their modeling outputs . Under discriminative learning setting , the averaged output from all candidate labels , ie F ( xi , y ; ) , is distinguished from the outputs from non candidate labels , ie F ( xi , y ; ) ( y /∈ Si ) [ 8 ] . Under instance based learning setting , the predicted label for unseen instance is determined by averaging the candidate labeling information from its neighboring examples in the PL training set [ 13 , 27 ] . One potential drawback of the averaging based disambiguation strategy lies in that the essential modeling output from ground truth label yi might be overwhelmed by the distractive outputs from false positive labels . y∈Si
1|Si|
In the next section , a novel partial label learning approach named PL LEAF will be introduced . Other than disambiguation by identification or averaging , PL LEAF facilitates the disambiguation process by making use of local topological information from the feature space . The candidate label set is disambiguated in the form of normalized labeling confidences , which differs from the crispstyle disambiguation of identifying a single candidate label or the uniform style disambiguation of averaging all candidate labels .
3 . THE PROPOSED APPROACH As shown in Section 1 , the task of partial label learning is to learn a multi class classifier f : X 7→ Y from the PL training set D = {(xi , Si ) | 1 ≤ i ≤ m} . For the proposed PL LEAF approach , its key novelty lies in how the disambiguation procedure is conducted . Specifically , for each PL training example ( xi , Si ) , PL LEAF aims to disambiguate its candidate label set Si via a normalized realvalued vector i = ( λi1 , λi2 , . . . , λiq ) . Here , each component λik ( 1 ≤ k ≤ q ) represents the labeling confidence of yk being the ground truth label for xi , which satisfies the following constraints :
⊤
• λik = 0 ( ∀ yk /∈ Si ) • λik ≥ 0 ( ∀ yk ∈ Si ) and
∑ yk∈Si
λik = 1
Once the normalized labeling confidence vectors have been generated , the predictive model will be induced by utilizing the disambiguation results .
In the next subsections , the two basic stages of PL LEAF , ie feature aware disambiguation and predictive model induction , will be scrutinized respectively . 3.1 Feature Aware Disambiguation
In the first stage , PL LEAF aims to disambiguate the candidate label set by exploiting useful information from the feature space . To fulfill this task , the popular smoothness assumption is adopted to enable information exploitation from the feature space to the label space . Given the PL training set D = {(xi , Si ) | 1 ≤ i ≤ m} , a weighted graph G = ( V , E , W ) is constructed over the training examples to characterize the manifold structure of feature space . Here , V = {xi | 1 ≤ i ≤ m} corresponds to the set of vertices and E = {(xi , xj ) | xi ∈ KNN(xj ) , i ̸= j} corresponds to the set of directed edges from xi to xj iff xi is among the K nearest neighbors of xj . Furthermore , W = [ Wij]m×m corresponds to the nonnegative weight matrix where Wij = 0 if ( xi , xj ) /∈ E . For the ⊤ weight matrix , its j th column W·j = ( W1j , W2j , . . . , Wmj ) is determined by solving the following linear least square problem : fifififififi2 fifififififixj −∑ ∑ ( xi,xj )∈E Wij = 1 Wij ≥ 0 ( ∀ ( xi , xj ) ∈ E )
( xi,xj )∈E Wij · xi min W j st :
( 1 )
( 2 )
Conceptually , the weight value Wij ( (xi , xj ) ∈ E ) characterizes the relative importance of neighboring example xi in reconstructing xj . According to the constraint ( xi,xj )∈E Wij = 1 , the above optimization problem ( OP ) can be re written as :
∑ st : 1
⊤ ·j Gj W·j
W
W·j = 1 min W j ⊤ Wij ≥ 0 ( ∀ ( xi , xj ) ∈ E ) Wij = 0 ( ∀ ( xi , xj ) /∈ E ) ab = ( xj − xa ) ⊤ ab]m×m is the local Gram matrix for xj with elHere , Gj = [ Gj ( xj − xb ) . Apparently , OP(2 ) correements Gj sponds to a standard quadratic programming ( QP ) problem whose optimal solution can be obtained by any off the shelf QP solver . The weight matrix W is constructed by solving OP(2 ) columnwisely and the resulting matrix is generally not symmetric . Accordingly , based on the local topological information embodied in G , the manifold structure in the feature space will be exploited to help disambiguate the candidate label set .
Here , in order to generate the labeling confidence vectors fi = [ 1 , 2 , . . . , m ] , PL LEAF exploits the smoothness assumption
1336 Table 1 : The pseudo code of PL LEAF . the partial label training set {(xi , Si ) | 1 ≤ i ≤ m} ( xi ∈ X , Si ⊆ Y,X = Rd,Y = {y1 , y2 , . . . , yq} ) the number nearest neighbors used for weighted graph construction the regularization parameters for regression loss function the unseen instance ( x ∈ X ) the predicted label for x
Inputs : D : K : C1 , C2 : x :
Outputs : y :
(
)
1|Si| · 1Si
Instantiate the j th column W·j of the weight matrix W by solving OP(2 ) with QP procedure ;
Process : 1 : Set the weighted graph G = ( V , E , W ) with V = {xi | 1 ≤ i ≤ m} and E = {(xi , xj ) | xi ∈ KNN(xj ) , i ̸= j} ; 2 : for j = 1 to m do 3 : 4 : end for 5 : Generate the labeling confidence vectors fi by solving OP(4 ) with QP procedure , or by solving OP(5 ) with alternating optimization ; 6 : Calculate the kernel matrix K = [ κ(x,xj)]m×m over training examples ; − 1| ^Si| · 1 ^Si 7 : Calculate ) = [ 1 , 2 , . . . , m ] with i = − 8 : Set t = 0 ; 9 : Initialize .(0 ) and b(0 ) with ff(0 ) 10 : repeat 11 : 12 : 13 : 14 : 15 : 16 : 17 : 18 : until L(.(t−1 ) , b(t−1 ) ) − L(.(t ) , b(t ) ) < τ · L(.(t−1 ) , b(t−1 ) ) 19 : Set the final predictive model with . 20 : Return y = f ( x ) according to Eq ( 18 ) . end for Set the descending direction P(t ) according to Eq ( 16 ) ; Update {.(t+1 ) , b(t+1)} by invoking line search procedure from {.(t ) , b(t)} along P(t ) ; t = t + 1 ; according to Eq ( 11 ) , and set Dρ = [ dij]m×m with dij = ρiδij ;
Calculate fl = [ ρ1 , ρ2 , . . . , ρm ] for k = 1 to q do
Obtain the solution ~ffk and ~bk by solving Eq ( 17 ) ; k = 0 ( 1 ≤ k ≤ q ) ;
∗ = .(t ) and b
( 1 ≤ i ≤ m ) k = 0 and b(0 )
= b(t ) ;
⊤
∗
( 3 ) we can choose to solve OP(4 ) with alternating optimization strategy where a series of QP subproblems with q variables and q + 1 constraints are optimized iteratively . Without loss of generality , in each alternating optimization iteration , one labeling confidence vector i is optimized by fixing the values of other labeling confidence vectors j ( j ̸= i ) :
)
( ∑ m j=1,j̸=i ( rij + rji ) ·
⊤ j i
( 5 )
( xi,xj )∈E Wij = 1 , the above OP(3 ) that the manifold structure in the feature space should also be preserved in the label space : fifififififi2
( xi,xj )∈E Wij · i fi min m j=1 fifififififij −∑ ∑ st : λjk = 0 ( ∀ 1 ≤ j ≤ m , yk /∈ Sj ) ∑ λjk ≥ 0 ( ∀ 1 ≤ j ≤ m , yk ∈ Sj ) ∑ λjk = 1 ( ∀ 1 ≤ j ≤ m ) ∑
∑ j=1 γij · yk∈Sj min m
According to the constraint can be re written as : fi
( 4 ) m i=1
⊤ i j st : λjk = 0 ( ∀ 1 ≤ j ≤ m , yk /∈ Sj ) ∑ λjk ≥ 0 ( ∀ 1 ≤ j ≤ m , yk ∈ Sj ) λjk = 1 ( ∀ 1 ≤ j ≤ m ) yk∈Sj ·i 1m×m W·i − 2Wij where Wi· , ⊤ ⊤ Here , γij = Wi· W j· + δijW δij and 1m×m represent the i th row of weight matrix W , the Kronecker ’s delta and the m × m matrix of 1 ’s respectively .
Note that OP(4 ) corresponds to a standard QP problem with mq variables and m(q +1 ) constraints , whose computational complexity would be demanding when mq is large . To improve efficiency , min i
⊤ i i + rii · st : λik = 0 ( ∀ yk /∈ Si ) ∑ λik ≥ 0 ( ∀ yk ∈ Si ) yk∈Si
λik = 1
It is interesting to notice that , to some extent , the disambiguation results returned by existing strategies can be viewed as simplified versions of PL LEAF ’s normalized labeling confidence vectors . For identification based disambiguation [ 16 , 18 ] , a single label y^k in Si is identified as the ground truth label leading to unimodal labeling confidence vector with λi^k = 1 and λik = 0 ( k ̸= ^k ) . For averaging based disambiguation [ 8 , 13 ] , all candidate labels in Si are treated equally leading to uniform labeling confidence vector with λik = 1|Si| ( yk ∈ Si ) . Therefore , the normalized labeling confidence vector considered by PL LEAF would accommodate
1337 Controlled UCI Data Sets
# Examples
Data set vehicle segment abalone satimage usps pendigits
846 2,310 4,177 6,345 9,298 10,992
Table 2 : Characteristics of the experimental data sets .
# Features
18 18 7 36 256 16
# Class Labels
4 7 29 7 10 10
Configurations
( I ) p = 1 , r = 1 , ε ∈ {0.1 , 0.2 , . . . , 0.7} [ Figure 1 ] ( II ) r = 1 , p ∈ {0.1 , 0.2 , . . . , 0.7} [ Figure 2 ] ( III ) r = 2 , p ∈ {0.1 , 0.2 , . . . , 0.7} [ Figure 3 ] ( IV ) r = 3 , p ∈ {0.1 , 0.2 , . . . , 0.7} [ Figure 4 ]
Real World Data Sets
Data set FG NET
Lost
MSRCv2 BirdSong
Soccer Player Yahoo! News
# Examples
# Features
# Class Labels
Avg . # CLs
1,002 1,122 1,758 4,998 17,472 22,991
262 108 48 38 279 163
78 16 23 13 171 219
7.48 2.23 3.16 2.18 2.09 1.91
Task Domain facial age estimation [ 20 ] automatic face naming [ 8 ] object classification [ 16 ] bird song classification [ 3 ] automatic face naming [ 25 ] automatic face naming [ 12 ] more flexibility in modeling the disambiguation results compared to the unimodal or uniform setup . 3.2 Predictive Model Induction Following the first stage of feature aware disambiguation , the original PL training set D has been transformed into its disambiguated counterpart : Ddis = {(xi , i ) | 1 ≤ i ≤ m} . In the second stage , PL LEAF aims to induce the predictive model f : X 7→ Y based on Ddis . Considering that the response variables ( normalized labeling confidences ) for each training example in Ddis are actually real valued , it is natural to induce the predictive model by performing multi output regression . Among various choices of multi output regression techniques , we choose to adapt the multiregression support vector machines ( MSVR ) [ 6 , 21 , 23 ] such that kernel trick can be readily incorporated to accommodate nonlinear modeling . Let ϕ(· ) : Rd 7→ RH be the ( implicit ) nonlinear mapping from the original feature space to the higher dimensional Reproducing Kernel Hilbert Space ( RKHS ) via kernel function κ : X ×X 7→ R . Furthermore , let {(k , bk ) | 1 ≤ k ≤ q} denote the multi output regression model in the RKHS with one linear predictor ( k , bk ) for each class label yk ∈ Y . Then , PL LEAF induces the regression model by minimizing the following loss function : q∑ m∑
L( . , b ) =
1 2
||k||2 + C1 k=1 i=1
L1(ui ) + C2 vi
( 6 ) m∑ i=1 ⊤ represent Here , . = [ 1 , 2 , . . . , q ] and b = [ b1 , b2 , . . . , bq ] the regression model ’s weight matrix and bias vector respectively . As shown in Eq ( 6 ) , the first term of L( . , b ) controls the com plexity of the induced model . In addition , the second term of L( . , b ) is defined based on the ϵ insensitive loss function :
{ 0 , u < ϵ ( u − ϵ)2 , u ≥ ϵ
L1(u ) =
( 7 ) For each example ( xi , i ) in Ddis , the corresponding input to the ϵ insensitive loss function L1(· ) is set as : ui = ||ei|| = ⊤ e i ei with ei = i − . ϕ(xi)− b . In this way , the outputs of all linear ⊤ predictors are considered simultaneously to yield a unique input to L1(· ) such that the dependencies among all the class labels can
√ be exploited by the ϵ insensitive term . The third term of L( . , b ) considers the partial label loss for each example which is set as :
)(
)
( vi = −
1|Si| · 1
⊤ Si
| ^Si| · 1 − 1
⊤ ^Si
⊤
.
ϕ(xi ) + b
( 8 )
Here , for candidate label set Si and its complementary set ^Si in Y , ) corresponds to a q dimensional vector whose k th ele1Si ( 1 ^Si ment equals to 1 if yk ∈ Si ( yk ∈ ^Si ) and 0 otherwise . In other words , the third term enforces the property that the average output from candidate labels should be larger than the average output from non candidate ones , which has been widely used in designing effective partial label learning algorithms [ 8 , 13 , 26 ] .
To minimize L( . , b ) , PL LEAF employs the gradient based iterative method named Iterative Re Weighted Least Square ( IRWLS ) [ 21 , 23 ] . Specifically , in each iteration the descending direction is determined analytically by solving linear systems of equations . Let {.(t ) , b(t)} denote the current model after t th iteration , the ϵinsensitive function L1(ui ) is firstly approximated by its first order Taylor expansion : fifififi
⊤
( e(t ) i ) u(t ) i
( t ) i u
)
( ei − e(t ) i
( 9 )
L1(ui ) = L1(u(t ) i ) + dL1(u ) du and e(t ) are calculated based on the current model {.(t ) , Here , u(t ) b(t)} . Then , a quadratic approximation to L(ui ) is further constructed to ease analytical solution to the descending direction : i i
~L1(ui ) = L1(u(t ) i ) + dL1(u ) du i − ( u(t ) i )2 u2 2u(t ) i
( t ) i
( 10 ) u
=
1 2 where
ρiu2 i + Ti fifififi
0 ,
2
(
ρi =
L1(u )
1 u(t ) i u(t ) i < ϵ ≥ ϵ and Ti is a constant which does not depend on { . , b} .
, u(t )
( t ) u i
−ϵ du
( t ) i
( t ) i
= u u i
( 11 ) fifififi
)
1338 ( a ) vehicle
( b ) segment
( c ) abalone
( d ) satimage
( e ) usps
( f ) pendigits
Figure 1 : Classification accuracy of each comparing algorithm changes as ε ( co occurring probability of the coupling label ) increases from 0.1 to 0.7 ( with 100 % partially labeled examples [ p = 1 ] and one false positive candidate label [ r = 1] ) .
Based on Eqs.(10 ) and ( 11 ) , the objective function L( . , b ) can be re written as :
~L( . , b ) = q∑ ∑
1 2 k=1
||k||2 + C1 m∑ i=1 m∑ i=1 vi + C1
ρi 2 u2 i + C2
( 12 )
Ti m∑ ∑ i=1 m q k=1 i=1 u2
||k||2 In contrast to standard least square objective function 1 2 i , the minimization of Eq ( 12 ) can be regarded as a +C1 weighted least square problem along with partial label loss regularization . Minimization of ~L( . , b ) can be decoupled for each class label , whose solution for each ( k , bk ) ( 1 ≤ k ≤ q ) is found by equating the corresponding gradient to zero : ~L( . , b ) = k− ∇k m∑ m∑
( 13 )
C1 i=1
ρiϕ(xi)(λik − ϕ(xi ) ⊤ m∑
~L( . , b ) =
ρi(λik − ϕ(xi )
⊤
∇bk −C1 k − bk ) + C2 m∑
ψikϕ(xi ) = 0 i=1
( 14 )
ψik = 0 i=1 k − bk ) + C2 )
− 1| ^Si| · 1 ^Si ][ ] [ k bk ⊤ fl fl
C1(
C1fl
Here , ψik corresponds to the k th component of the q dimensional vector i = − [ . Accordingly , Eqs.(13 ) and ( 14 ) can be expressed as a linear system of equations :
1|Si| · 1Si
C1(
⊤
⊤ Dρ( + I C1( ⊤ C11
(
⊤
C1fl
]
( 15 )
= Dρk − C2( k − C21 ⊤ ⊤
⊤ k k i=1
(
Here , ( = [ ϕ(x1 ) , ϕ(x2 ) , . . . , ϕ(xm ) ] dij = ρiδij , fl = [ ρ1 , ρ2 , . . . , ρm ] k = [ ψ1k , ψ2k , . . . , ψmk ]
⊤
⊤
.
, Dρ = [ dij]m×m with ⊤ ,
, k = [ λ1k , λ2k , . . . , λmk ]
Let ~ . = [ ~1 , ~2 , . . . , ~q ] and ~b = [ ~b1 , ~b2 , . . . , ~bq ] be the solution obtained by solving Eq ( 15 ) for each class label , the descending direction for the next iteration would be :
⊤
⊤
]
[
P(t ) =
~ . − .(t ) ( ~b − b(t ) ) ⊤
( 16 )
The subsequent model {.(t+1 ) , b(t+1)} is then updated by invoking line search procedure from {.(t ) , b(t)} along the descending direction P(t ) [ 19 ] . ∑ [
According to the Representer Theorem [ 22 ] , under fairly general conditions , the predictive model can be expressed by a linear combination of the training examples in the RKHS , ie k = ffk . By introducing the kernel trick into Eqs.(13 ) and ( 14 ) , the linear system of Eq ( 15 ) can be expressed as follows : m i=1 αikϕ(xi ) = (
][
]
]
[
⊤
−1 C1K + D ρ K
C1fl
⊤
C11 ⊤
C11 ffk bk
= fl
C1k − C2D −1 ρ k k − C21 ⊤ k C1fl
⊤
( 17 )
Here , K = [ κ(xi , xj)]m×m is the kernel matrix over training examples . Based on the kernel trick , the line search procedure can be readily performed in terms of ffk and bk as well . be the resulting model after the whole iterative optimization process , PL LEAF makes prediction on the class label of unseen instance x as follows :
∗ Let .
∗ and b f ( x ) = arg maxyk∈Y
= arg maxyk∈Y
∗ ϕ(x ) + b k
∗ ik κ(xi , x ) + b
α
( 18 )
∗ k
⊤
∑m
∗ k i=1
1339 ( a ) vehicle
( b ) segment
( c ) abalone
( d ) satimage
( e ) usps
( f ) pendigits
Figure 2 : Classification accuracy of each comparing algorithm changes as p ( proportion of partially labeled examples ) increases ( with one false positive candidate label [ r = 1] ) .
Table 1 summarizes the pseudo code of PL LEAF.2 Given the PL training set , a weighted graph is constructed to characterize the manifold structure of feature space which is then utilized to disambiguate the candidate label set ( Steps 1 5 ) . After that , a predictive model based on kernelized multiregression SVR is learned via gradient based iterative optimization ( Steps 6 18).3 Finally , prediction on the unseen instance is made via the learned predictive model ( Steps 19 20 ) .
4 . EXPERIMENTS 4.1 Experimental Setup
To evaluate the performance of PL LEAF , two series of comparative experiments are conducted on controlled UCI data sets [ 2 ] as well as real world partial label data sets . Characteristics of the experimental data sets are summarized in Table 2 .
Following the widely used controlling protocol in partial label learning research [ 5 , 8 , 16 , 24 , 27 ] , an artificial PL data set can be generated from a multi class UCI data set with three controlling parameters p , r and ε . Here , p controls the proportion of examples which are partially labeled ( ie |Si| > 1 ) , r controls the number of false positive labels in the candidate label set ( ie |Si| = r + 1 ) , and ε controls the co occurring probability between one coupling candidate label and the ground truth label . As shown in Table 2 , a total of 28 ( 4x7 ) parameter configurations have been considered for each UCI data set .
In addition to artificial data sets , a number of real world PL data
2Code package for PL LEAF is publicly available at : http://cseseu educn/PersonalPage/zhangml/Resourceshtm#kdd16 3In this paper , the ϵ insensitive function L1(· ) is instantiated with ϵ = 0.1 and the convergence condition in Step 18 is instantiated with τ = 10
−10 . sets have been collected from several task domains.4 For the task of facial age estimation ( FG NET [ 20] ) , human faces with landmarks are represented as instances while ages annotated by ten crowdsourced labelers together with the ground truth age are regarded as candidate labels . For the task of automatic face naming ( Lost [ 8 ] , Soccer Player [ 25 ] and Yahoo! News [ 12] ) , faces cropped from an image or video frame are represented as instances while names extracted from the associated captions or subtitles are regarded as candidate labels . For the task of bird song classification ( BirdSong [ 3] ) , singing syllables of the birds are represented as instances while bird species jointly singing during a 10 seconds period are regarded as candidate labels . For the task of object classification ( MSRCv2 [ 16] ) , image segmentations are represented as instances while objects appearing within the same image are regarded as candidate labels . The average number of candidate labels ( Avg . #CLs ) for each real world PL data set is also recorded in Table 2 .
To show the effectiveness of feature aware disambiguation , PLLEAF is compared against four state of the art partial label learning approaches with diverse properties , each configured with parameters suggested in respective literatures :
• PL KNN [ 13 ] : A K nearest neighbor approach to partial label learning via averaging based disambiguation [ suggested configuration : K = 10 ] ;
• CLPL [ 8 ] : A discriminative approach to partial label learning via averaging based disambiguation [ suggested configuration : SVM with squared hinge loss ] ;
• PL SVM [ 18 ] : A maximum margin approach to partial label learning via identification based disambiguation [ suggested 4These data sets are publicly available at : http://cseseueducn/ PersonalPage/zhangml/Resources.htm#partial_data
1340 ( a ) vehicle
( b ) segment
( c ) abalone
( d ) satimage
( e ) usps
( f ) pendigits
Figure 3 : Classification accuracy of each comparing algorithm changes as p ( proportion of partially labeled examples ) increases ( with two false positive candidate labels [ r = 2] ) . configuration : regularization parameter pool with {10 103} ] ;
−3 , . . . ,
• LSB CMM [ 16 ] : A maximum likelihood approach to partial label learning via identification based disambiguation [ suggested configuration : q mixture components ] .
Parameters for PL LEAF ( Table 1 ) are set as K = 10 , C1 = 10 and C2 = 1.5 In this paper , ten runs of 50%/50 % random train/test splits are performed on each artificial as well as real world PL data set . Accordingly , the mean predictive accuracies ( with standard deviation ) are recorded for all comparing algorithms . 4.2 Experimental Results
′
421 Controlled UCI Data Sets In Figure 1 , the classification accuracy of each comparing algorithm is illustrated where the co occurring probability ε varies from 0.1 to 0.7 with step size 0.1 ( p = 1 , r = 1 ) . For any ground truth label y ∈ Y , one extra label y ̸= y is designated as the coupling label which co occurs with y in the candidate label set with probability ε . Otherwise , any other class label would be chosen to co occur with y . In Figures 2 to 4 , the classification accuracy of each comparing algorithm is illustrated where the proportion p ) varies from 0.1 to 0.7 with step size 0.1 ( r = 1 , 2 , 3 ) . Together with the ground truth label , r class labels in Y will be random− ||xi−xj||2 5For PL LEAF , Gaussian kernel κ(xi , xj ) = exp is employed in Step 6 with σ being the average distance among each pair of training examples . Furthermore , on the two artificial ( usps , pendigits ) and two real world ( Soccer Player , Yahoo! News ) data sets with large scale , alternating optimization is employed in Step 5 . Sensitivity analysis on PL LEAF ’s parameter configuration is conducted in Subsection 43
(
σ2
Table 3 : Win/tie/loss counts ( pairwise t test at 0.05 significance level ) on the classification performance of PL LEAF against each comparing algorithm .
PL LEAF against PL KNN 26/7/9 28/7/7 28/7/7 29/6/7
CLPL 31/11/0 42/0/0 40/2/0 39/3/0 152/16/0
111/27/30
[ Figure 1 ] [ Figure 2 ] [ Figure 3 ] [ Figure 4 ] In Total
PL SVM LSB CMM 20/16/6 27/13/2 23/16/3 35/7/0 23/12/7 33/9/0 26/12/4 32/10/0 127/39/2 92/56/20 ly picked up to constitute the candidate label set for each partially labeled example .
As shown in Figures 1 to 4 , in most cases , PL LEAF achieves superior or competitive performance against the comparing algorithms . Based on pairwise t test at 0.05 significance level , Table 3 summarizes the win/tie/loss counts between PL LEAF and each comparing algorithm . Out of the 168 statistical tests ( 28 configurations × 6 UCI data sets ) , it is shown that : • Comparing to averaging based disambiguation approaches , PL LEAF achieves superior performance against PL KNN and CLPL in 66.0 % and 90.4 % cases respectively . Furthermore , the performance of PL LEAF is inferior to PL KNN in only 17.9 % cases and has never been outperformed by CLPL .
• Comparing to identification based disambiguation approaches , PL LEAF achieves superior performance against PL SVM and LSB CMM in 75.5 % and 54.7 % cases . Furthermore , the performance of PL LEAF is inferior to LSB CMM in only 11.9 % cases and has been outperformed by PL SVM in only 2 out of 168 cases .
1341 ( a ) vehicle
( b ) segment
( c ) abalone
( d ) satimage
( e ) usps
( f ) pendigits
Figure 4 : Classification accuracy of each comparing algorithm changes as p ( proportion of partially labeled examples ) increases ( with three false positive candidate labels [ r = 3] ) .
Table 4 : Classification accuracy ( mean±std ) of each comparing algorithm on the real world partial label data sets . In addition , •/◦ indicates whether the performance of PL LEAF is statistically superior/inferior to the comparing algorithm on each data set ( pairwise t test at 0.05 significance level ) .
FG NET
FG NET ( MAE3 ) FG NET ( MAE5 )
Lost
MSRCv2 BirdSong
Soccer Player Yahoo! News
PL LEAF 0072±0010 0411±0012 0550±0018 0664±0020 0459±0013 0706±0012 0515±0004 0597±0004
PL KNN
0037±0008• 0284±0035• 0438±0033• 0332±0030• 0417±0012• 0637±0009• 0494±0004• 0403±0004•
CLPL
0047±0017• 0240±0045• 0343±0055• 0670±0024 0375±0020• 0624±0009• 0347±0004• 0457±0005•
PL SVM
0058±0010• 0343±0022• 0473±0016• 0639±0056 0417±0027• 0671±0018• 0430±0004• 0615±0002◦
LSB CMM 0056±0008• 0344±0026• 0478±0025• 0591±0019• 0431±0008• 0692±0015• 0506±0006• 0594±0007
422 Real World Data Sets Table 4 reports the detailed predictive performance of each comparing algorithm on the real world PL data sets , where the outcomes of pairwise t tests at 0.05 significance level are also recorded . Note that the average number of candidate labels ( Avg . #CLs ) for the FG NET data set ( ie 7.48 as shown in Table 2 ) is quite large , which makes the task of facial age estimation ( based on training examples with partial labels ) rather challenging . Furthermore , the state of the art performance on this data set ( based on training examples with ground truth labels ) corresponds to more than 3 years of mean average error ( MAE ) between the predicted age and the ground truth age [ 20 ] . In Table 4 , two extra classification accuracies are reported on the FG NET data set where an unseen example is regarded to be correctly classified if the difference between the predicted age and the ground truth age is less than 3 years ( MAE3 ) or 5 years ( MAE5 ) .
As shown in Table 4 , it is impressive to observe that :
• On the FG NET ( with its MAE3 and MAE5 variants ) , MSRCv2 , BirdSong and Soccer Player data sets , the performance of PL LEAF is superior to all the other comparing algorithms . • On the Lost data set , the performance of PL LEAF is superior to PL KNN and LSB CMM , and comparable to CLPL and PL SVM .
• On the Yahoo! News data set , the performance of PLLEAF is superior to PL KNN and CLPL , comparable to LSBCMM , and inferior to PL SVM .
4.3 Further Analysis
In addition to Table 4 reporting inductive performance on test examples , it is also interesting to study the transductive performance of each comparing algorithm on classifying training examples [ 8 , 24 ] . For each PL training example ( xi , Si ) , its ground truth label is predicted by consulting the candidate label set Si , ie predicting ^yi ∈ Si with largest modeling output . In other words , transductive
1342 Table 5 : Transductive accuracy ( mean±std ) of each comparing algorithm on the real world partial label data sets . In addition , •/◦ indicates whether the performance of PL LEAF is statistically superior/inferior to the comparing algorithm on each data set ( pairwise t test at 0.05 significance level ) .
FG NET
FG NET ( MAE3 ) FG NET ( MAE5 )
Lost
MSRCv2 BirdSong
Soccer Player Yahoo! News
PL LEAF 0148±0009 0567±0015 0710±0014 0809±0022 0645±0015 0822±0014 0702±0003 0827±0002
PL KNN 0173±0017◦ 0559±0023 0721±0021 0596±0018• 0603±0015• 0766±0015• 0674±0002• 0720±0003•
CLPL
0158±0018 0548±0017• 0684±0021• 0855±0007◦ 0612±0035• 0820±0014 0742±0005◦ 0832±0002◦
PL SVM 0136±0021 0502±0030• 0644±0021• 0814±0033 0656±0026 0831±0013 0733±0007◦ 0861±0003◦
LSB CMM 0138±0019 0541±0022• 0684±0018• 0755±0018• 0603±0016• 0827±0017 0688±0003• 0861±0002◦
† PL LEAF 0142±0010 0556±0014 0698±0010 0788±0025 0629±0016 0787±0016 0669±0003 0822±0002
( a ) Varying K ( C1 = 10 , C2 = 1 )
( b ) Varying C1 ( K = 10 , C2 = 1 )
( c ) Varying C2 ( K = 10 , C1 = 10 )
Figure 5 : Parameter sensitivity analysis for PL LEAF on the Lost , MSRCv2 and BirdSong data sets . ( a ) Classification accuracy of PL LEAF changes as K increases from 6 to 14 with step size 2 ; ( b ) Classification accuracy of PL LEAF changes as C1 increases from 6 to 14 with step size 2 ; ( c ) Classification accuracy of PL LEAF changes as C2 increases from 0.6 to 1.4 with step size 02 performance of the partial label learning algorithm reflects its ability in disambiguating the candidate label set . Accordingly , Table 5 reports the transductive accuracy of each comparing algorithm together with the outcomes of pairwise t tests at 0.05 significance level . Out of the 32 statistical tests ( 8 data sets × 4 comparing algorithm ) , it is shown that :
• Comparing to averaging based disambiguation approaches , PL LEAF is outperformed by PL KNN on the FG NET data set , and outperformed by CLPL on the Lost , Soccer Player and Yahoo! News data sets . In the rest 12 statistical tests , the performance of PL LEAF is superior or at least comparable to PL KNN and CLPL .
• Comparing to identification based disambiguation approaches , PL LEAF is outperformed by PL SVM on the Soccer Player and Yahoo! News data sets , and outperformed by LSB CMM on the Yahoo! News data set . In the rest 13 statistical tests , the performance of PL LEAF is superior or at least comparable to PL SVM and LSB CMM .
As the feature aware disambiguation stage of PL LEAF finishes , the generated labeling confidence vector i can also be used to predict the ground truth label of xi as ^yi = arg maxyk∈Si λik . The resulting transductive performance is reported in Table 5 ( denoted ) for referencee purpose . In most cases , the transducas PL LEAF tive performance of PL LEAF is close to PL LEAF indicating the usefulness of generated labeling confidence vectors .
As shown in Table 1 , to study the sensitivity of PL LEAF wrt its parameters K , C1 and C2 , Figure 5 illustrates how PL LEAF
†
† performs under different parameter configurations . For clarity of illustration , three data sets ( Lost , MSRCv2 and BirdSong ) are chosen here for sensitivity analysis while similar observations also hold on other data sets .
It is obvious from Figure 5 that the performance of PL LEAF is stable across a broad range of each parameter . This property is quite desirable as one can make use of PL LEAF to achieve robust classification performance without the need of parameter fine tuning . Therefore , the parameter configuration for PL LEAF in Subsection 4.1 ( K=10 , C1=10 , C2=1 ) naturally follows from these observations . 5 . CONCLUSION
In this paper , a novel approach named PL LEAF is proposed to learning from partial label examples . Different from existing strategies , PL LEAF aims to disambiguate the candidate label set by manipulating useful information in the feature space . Specifically , PL LEAF generates normalized labeling confidence vectors based on manifold relationships among training examples , and then induces the predictive model based on multi output regression analysis . Comparative studies across comprehensive partial label data sets clearly verify the effectiveness of the proposed approach .
For PL LEAF , an important future work is to investigate ways to perform manifold structure discovery and labeling confidence generation simultaneously . Secondly , it is worth studying whether there are better techniques to exploit the generated labeling confidence vectors , such as fitting probabilistic models [ 10 ] . Thirdly , it is also interesting to explore other ways to fulfill the feature aware disambiguation strategy .
1343 Acknowledgements The authors wish to thank the anonymous reviewers for their constructive comments and suggestions . This work was supported by the National Science Foundation of China ( 61222309 , 61573104 , 61473087 ) , the Natural Science Foundation of Jiangsu Province ( BK20141340 ) , the MOE Program for New Century Excellent Talents in University ( NCET 13 0130 ) , and the Collaborative Innovation Center of Wireless Communications Technology .
6 . REFERENCES [ 1 ] J . Amores . Multiple instance classification : Review , taxonomy and comparative study . Artificial Intelligence , 201:81–105 , 2013 .
[ 2 ] K . Bache and M . Lichman . UCI machine learning repository . School of Information and Computer Sciences , University of California , Irvine , 2013 .
[ 3 ] F . Briggs , X . Z . Fern , and R . Raich . Rank loss support instance machines for MIML instance annotation . In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 534–542 , Beijing , China , 2012 .
[ 4 ] O . Chapelle , B . Schölkopf , and A . Zien , editors .
Semi Supervised Learning . MIT Press , Cambridge , MA , 2006 .
[ 5 ] Y C Chen , V . M . Patel , R . Chellappa , and P . J . Phillips . Ambiguously labeled learning using dictionaries . IEEE Transactios on Information Forensics and Security , 9(12):2076–2088 , 2014 .
[ 6 ] W . Chung , J . Kim , H . Lee , and E . Kim . General dimensional multiple output support vector regressions and their multiple kernel learning . IEEE Transactions on Cybernetics , 45(11):2572–2584 , 2015 .
[ 7 ] T . Cour , B . Sapp , C . Jordan , and B . Taskar . Learning from ambiguously labeled images . In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition , pages 919–926 , Miami , FL , 2009 .
[ 8 ] T . Cour , B . Sapp , and B . Taskar . Learning from partial labels .
Journal of Machine Learning Research , 12(May):1501–1536 , 2011 .
[ 9 ] T . G . Dietterich , R . H . Lathrop , and T . Lozano Pérez .
Solving the multiple instance problem with axis parallel rectangles . Artificial Intelligence , 89(1 2):31–71 , 1997 .
[ 10 ] X . Geng , C . Yin , and Z H Zhou . Facial age estimation by label distribution learning . IEEE Transactions on Pattern Analysis and Machine Intelligence , 35(10):2401–2412 , 2013 .
[ 11 ] E . Gibaja and S . Ventura . A tutorial on multilabel learning .
ACM Computing Surveys , 47(3):Article 52 , 2015 .
[ 12 ] M . Guillaumin , J . Verbeek , and C . Schmid . Multiple instance metric learning from automatically labeled bags of faces . In K . Daniilidis , P . Maragos , and N . Paragios , editors , Lecture Notes in Computer Science 6311 , pages 634–647 . Springer , Berlin , 2010 .
[ 13 ] E . Hüllermeier and J . Beringer . Learning from ambiguously labeled examples . Intelligent Data Analysis , 10(5):419–439 , 2006 .
[ 14 ] L . Jie and F . Orabona . Learning from candidate labeling sets .
In J . Lafferty , C . K . I . Williams , J . Shawe Taylor , R . S . Zemel , and A . Culotta , editors , Advances in Neural Information Processing Systems 23 , pages 1504–1512 . MIT Press , Cambridge , MA , 2010 .
[ 15 ] R . Jin and Z . Ghahramani . Learning with multiple labels . In
S . Becker , S . Thrun , and K . Obermayer , editors , Advances in Neural Information Processing Systems 15 , pages 897–904 . MIT Press , Cambridge , MA , 2003 .
[ 16 ] L . Liu and T . Dietterich . A conditional multinomial mixture model for superset label learning . In P . Bartlett , F . C . N . Pereira , C . J . C . Burges , L . Bottou , and K . Q . Weinberger , editors , Advances in Neural Information Processing Systems 25 , pages 557–565 . MIT Press , Cambridge , MA , 2012 .
[ 17 ] L . Liu and T . Dietterich . Learnability of the superset label learning problem . In Proceedings of the 31st International Conference on Machine Learning , pages 1629–1637 , Beijing , China , 2014 .
[ 18 ] N . Nguyen and R . Caruana . Classification with partial labels .
In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 381–389 , Las Vegas , NV , 2008 .
[ 19 ] J . Nocedal and S . Wright . Numerical Optimization . Springer ,
Berlin , 2nd edition , 2006 .
[ 20 ] G . Panis and A . Lanitis . An overview of research activities in facial age estimation using the FG NET aging database . In L . Agapito , M . M . Bronstein , and C . Rother , editors , Lecture Notes in Computer Science 8926 , pages 737–750 . Springer , Berlin , 2015 .
[ 21 ] M . Sánchez Fernández , M . de Prado Cumplido ,
J . Arenas García , and F . Pérez Cruz . SVM multiregression for nonliear channel estimation in multiple input multiple output systems . IEEE Transactions on Signal Processing , 52(8):2298–2307 , 2004 .
[ 22 ] B . Schölkopf and A . J . Smola . Learning with Kernels :
Support Vector Machines , Regularization , Optimization and Beyond . MIT Press , Camridge , MA , 2001 .
[ 23 ] D . Tuia , J . Verrelst , L . Alonso , F . Pérez Cruz , and
G . Camps Valls . Multioutput support vector regression for remote sensing biophysical parameter estimation . IEEE Geoscience and Remote Sensing Letters , 8(4):804–808 , 2011 .
[ 24 ] F . Yu and M L Zhang . Maximum margin partial label learning . In Proceedings of the 7th Asian Conference on Machine Learning , pages 96–111 , Hong Kong , China , 2015 .
[ 25 ] Z . Zeng , S . Xiao , K . Jia , T H Chan , S . Gao , D . Xu , and
Y . Ma . Learning by associating ambiguously labeled images . In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition , pages 708–715 , Portland , OR , 2013 .
[ 26 ] M L Zhang . Disambiguation free partial label learning . In Proceedings of the 14th SIAM International Conference on Data Mining , pages 37–45 , Philadelphia , PA , 2014 .
[ 27 ] M L Zhang and F . Yu . Solving the partial label learning problem : An instance based approach . In Proceedings of the 24th International Joint Conference on Artificial Intelligence , pages 4048–4054 , Buenos Aires , Argentina , 2015 .
[ 28 ] M L Zhang and Z H Zhou . A review on multi label learning algorithms . IEEE Transactions on Knowledge and Data Engineering , 26(8):1819–1837 , 2014 .
[ 29 ] X . Zhu and A . B . Goldberg . Introduction to semi supervised learning . In R . J . Brachman and T . G . Dietterich , editors , Synthesis Lectures to Artificial Intelligence and Machine Learning , pages 1–130 . Morgan & Claypool Publishers , San Francisco , CA , 2009 .
1344
