A Truth Discovery Approach with Theoretical Guarantee
Houping Xiao1 , Jing Gao1 , Zhaoran Wang2 , Shiyu Wang3 , Lu Su1 , and Han Liu2
{houpingx,jing}@buffalo.edu , zhaoran@princeton.edu swang86@illinois.edu , lusu@buffalo.edu , hanliu@princeton.edu
1SUNY Buffalo , Buffalo , NY USA 2Princeton University , Princeton , NJ USA
3University of Illinois , Urbana Champaign , IL USA
ABSTRACT In the information age , people can easily collect information about the same set of entities from multiple sources , among which conflicts are inevitable . This leads to an important task , truth discovery , ie , to identify true facts ( truths ) via iteratively updating truths and source reliability . However , the convergence to the truths is never discussed in existing work , and thus there is no theoretical guarantee in the results of these truth discovery approaches . In contrast , in this paper we propose a truth discovery approach with theoretical guarantee . We propose a randomized gaussian mixture model ( RGMM ) to represent multi source data , where truths are model parameters . We incorporate source bias which captures its reliability degree into RGMM formulation . The truth discovery task is then modeled as seeking the maximum likelihood estimate ( MLE ) of the truths . Based on expectation maximization ( EM ) techniques , we propose population based ( ie , on the limit of infinite data ) and sample based ( ie , on a finite set of samples ) solutions for the MLE . Theoretically , we prove that both solutions are contractive to an ϵ ball around the MLE , under certain conditions . Experimentally , we evaluate our method on both simulated and real world datasets . Experimental results show that our method achieves high accuracy in identifying truths with convergence guarantee .
CCS Concepts •Information systems → Data mining ;
Keywords Truth Discovery ; Mixture Model ; Asymptotic Consistency
1 .
INTRODUCTION
With the increase in our capabilities in collecting data from the physical world , an important feature of the data collection is its wide variety , ie , data about the same object can be obtained from various sources . For example , cus tomer information can be found from multiple databases in a company , a patient ’s medical records may be scattered at different hospitals , and product specifications are typically listed at different websits ( eg , Amazon.com , ShopZillacom ) Conflicts among information from different sources are commonly observed . Therefore , it is an important task to discover the truth ( ie , correct information ) out of conflicting multi source data , which is referred to as truth discovery .
A trivial approach of accomplishing the truth discovery task is to treat the average of the data as the truth . The drawback of this simple averaging approach is that it treats each source equally reliable , which is often violated in real practice . Usually there exist sources with low quality information , such as faulty sensors that keep emanating erroneous information , or spammers who propagate false information on the Web . To address this challenge , researchers propose a variety of approaches that infer both source reliability and truths from multi source data . These approaches , referred to as truth discovery approaches , are developed based on heuristic principles [ 3 , 5–8 , 13 , 16 , 24 ] , optimization [ 9 , 10 , 14 , 27 ] , or probabilistic models [ 2 , 4 , 12 , 15 , 17 , 19–21 , 25 , 26 ] . Despite the difference in the techniques , the underlying principle is the same : The sources which often provide truths should be reliable , and the information from reliable sources usually represent the truths . Based on this principle , existing truth discovery approaches usually start with an initialization of source reliability , and then conduct the following two steps iteratively until convergence :
( i ) Based on source reliability , obtain the truth by assigning high weights to reliable sources in the aggregation of sources’ inputs ;
( ii ) Based on the truths , calculate the reliability degree of each source by measuring the deviation between truths and sources’ inputs .
Although these algorithms differ in the specific ways to compute truths or source reliability , they typically follow this iterative procedure . However , the convergence of such an iterative procedure has not been discussed in existing work , and there is no theoretical guarantee in the results of existing truth discovery approaches .
In contrast , in this paper we propose an effective truth discovery approach with theoretical guarantee in convergence . Compared with existing approaches , the proposed approach follows the same principle that reliable sources provide truths and truths are stated by reliable sources , but the major advantage of the proposed approach is its theoretical guarantee . Specifically , we introduce the bias of each source which is a
1925 random variable measuring its reliability degree . To represent multi source data with various reliability degrees , we propose a Randomized Gaussian Mixture Model ( RGMM ) formulation , which consists of a gaussian mixture model [ 18 ] with sources’ biases incorporated . We cast the truth discovery problem as inferring a maximum likelihood estimator ( MLE ) of the unknown parameters in RGMM . To solve the problem , we derive both population based and samplebased Expectation Maximization ( EM ) solutions [ 4 ] for the limit of infinite data and a finite set of samples respectively . We name the two approaches as population and sampleEMrgmm ( ie , EM for RGMM ) , respectively .
Theoretically , we prove that the output of the proposed EMrgmm approaches converge to a MLE of the truth as the number of sources increases . The proof is first derived for population EMrgmm . We first bound the distance between the population EMrgmm and the MLE . In Property 1 , we present the conditions under which population EMrgmm estimate is contractive to the MLE , in an ϵ ball around the MLE . Then to prove that the output of sample EMrgmm converges to an ϵ ball around MLE , we bound the deviation from sample EMrgmm to population EMrgmm . To achieve this goal , we introduce the definition of the covering number [ 22 ] of a metric space , and show that the distance between population EMrgmm and sample EMrgmm is upper bounded by the number of sources ( Corollary 1 ) . Combining these theoretical results and applying triangle inequality , we prove that the distance between sample EMrgmm and the MLE is bounded , under certain conditions .
Experimentally , we evaluate the effectiveness of the proposed sample EMrgmm and verify the theoretical results on several simulated data as well as three real world application datasets : Weather Forecast , Indoor Floorplan , and Stock data . Experimental results show that the proposed sampleEMrgmm approach are able to estimate truths from conflicting multi source data . Compared with the state of the art truth discovery algorithms , the proposed sample EMrgmm can achieve comparable performance . Moreover , the estimate is asymptotically consistent to an ϵ ball around the MLE under the stated conditions . ,
The remainder of the paper is organized as follows . We discuss related work in Section 2 . In Section 3 , we mathematically formulate the task setting , introduce the randomized Gaussian mixture model , and derive the closed form EM algorithms for MLE . Then we present two important theoretical results for the convergence of the proposed approaches ( Peoperties 1 and 2 ) in Section 4 . The detailed proofs are presented in Appendix . In Section 5 , we report experimental results on both simulated and real world datasets . We conclude the paper in Section 6 .
2 . RELATED WORK
The problem of truth discovery has attracted much attention recently . People develop various approaches to extract true information from multiple sources of conflicting data . Initial studies [ 6 , 24 ] were motivated by the observation that source reliability and truths are highly relevant–Truths are often stated by many reliable sources and reliable sources tend to tell truths more often . Based on this principle , approaches were developed to iteratively update source reliability and true facts . Later , various approaches were further developed to capture various factors that affect truth discovery [ 3 , 5 , 7 , 8 , 13 , 16 ] .
In recent work , this principle is formulated as an optimization framework [ 10 ] . The objective is to minimize the overall distance between source observations and truths in which sources are weighted by their reliability degrees . Then this framework was extended to handle data with long tail distributions by calculating the confidence of source reliability estimates [ 9 ] . In [ 27 ] , an optimization framework was developed based on min max entropy . The solutions to these optimization formulations usually involve an iterative update of truths and source reliability as well .
In addition , probabilistic approaches [ 2,4,15,19–21,25,26 ] were developed to tackle the truth discovery task . The basic idea is to formulate multi source data as certain mixture of distributions and incorporate source reliability as some random variable into the probabilistic models . The approaches differ in the way of selecting proper distributions and capture source reliability to handle various scenarios in truth discovery , such as the existence of multiple truths , various difficulty levels of the task , and various data types . To obtain the truths , these approaches try to maximize likelihood or posterior distributions , which leads to iteratively updates in model parameters and truth inference . In particular , Dawid & Skene were the first to develop a maximum likelihood formulation and an expectation maximization based approach to solve the problem . This approach was then adapted to social sensing scenarios in [ 19 ] . In [ 20 ] , the authors developed a maximum likelihood estimator for source reliability and approximately quantified confidence in its estimation based on an asymptotic Cramer Rao lower bound , but the convergence to the truth is not analyzed and cannot be guaranteed . In summary , although the topic of truth discovery has been widely studied and most approaches achieve success in realworld applications , there is no theoretical guarantee that the results of these approaches converge to the truths . In this paper , we propose a novel truth discovery approach which can not only achieve comparable effectiveness in identifying the truths but also has theoretical guaranteed in the convergency to the truths .
3 . METHODOLOGY
In this section , we mathematically formulate the truth discovery task , propose a randomized gaussian mixture model to represent multi source data with various reliability degrees , derive both population based and samplebased expectation maximization solutions ( ie , populationEMrgmm and sample EMrgmm ) , and finally present the complete algorithm of sample EMrgmm . 3.1 Problem Formulation
We first introduce the notations that will be used throughInput . Consider a set of entities N := {n}N out the paper and then state the target problem . n=1 that we are interested in , and there are S := {s}S 1 sources which provide information about all N entities . For the s th source , denote 1,··· , xs its claims on N entities as X s = ( xs N ) , where xs represents its claim on the n th entity . Then , X = {X s}S n represents the whole set of claims over the sources S . 1 Output . The truths for all entities are denoted as = ( µ1,··· , µN ) , which is unknown a priori . Let ’s also denote approach as b . the estimator of the truth obtained from a truth discovery
Truth Discovery Task . The truth discovery task is formally defined as follows : Given the data collection X , the
1926 goal of a truth discovery method is to obtain an estimate b for all entities’ truths as close to as possible .
We summarize the notations in Table 1 . Some of the no tations will be introduced later in the paper .
Notation
N S
∗ b s X s X σ2
Table 1 : Notations Definition the set of entities the set of sources the vector of entities’ truths the maximum likelihood estimator of the estimator of truths the bias for the source s the data collected from source s the whole claims over S sources the variance of data collection in the model
3.2 Randomized Gaussian Mixture Model
Now we propose a probabilistic model with parameters whose estimate will be inferred via maximum likelihood estimator technique . In truth discovery , different sources are treated differently depending on the quality of their data . To do this , we assume that the reliability of the s th source is captured by a random variable s that measures bias ( ie , deviation from the truth ) . The smaller absolute value the bias s , the more reliable the s th source . Considering all the S sources together , their reliability degrees {s}S 1 can be assumed to follow certain distributions , for example , a uniform distribution s ∼ Uniform(−C , C)N . This distribution models the overall quality of the collection of sources .
For the s th source , we assume that the claims made by this source follow a multi variate gaussian distribution with variance σ2IN , where IN is an identity matrix of size n . This assumption is typically adopted in many existing truth discovery work [ 9,10,27 ] . The mean of the gaussian distribution is − s in which is the truth and s is the source bias . Therefore , a more reliable source ’s mean is closer to the truth and thus its claims are more likely to be close to the truth . With these notations , we can write the distribution of the claims made by the s th source as :
X s|s ∼ Normal( − s , σ2IN ) .
( 1 )
We further model the whole data collection using the following mixture model [ 23 ] : p(X ) =
1 S
Normal( − s , σ2IN ) ,
( 2 )
S∑ s=1 where we assume equal weights among components . Different from traditional gaussian mixture model with fixed mean in each component , the proposed model ’s mean is random because we incorporate the random variables ηs as the source bias . Thus , we name the proposed model as Randomized Gaussian Mixture Model ( RGMM ) . We introduce a latent variable Y ∈ {s}S 1 as a source indicator , which is also an indicator of the underlying mixture component . Namely , the claims from the s th component of RGMM is denoted as :
X|Y = s ∼ Normal( − s , σ2IN ) .
( 3 )
We assume that the pair ( X , Y ) are random variables in the sample spaces X×Y . Based on the RGMM , we formulate the problem of truth discovery as the task of estimating the model parameter . Specifically , the objective is to obtain the estimate of which maximizes the likelihood of observ∗ ing the multi source input . We define as the maximum likelihood estimator of in the proposed RGMM formulation . In the following sections , we propose effective solutions ∗ to estimate and then demonstrate the approaches’ convergence guarantee in Section 4 . 3.3 EM Solutions for RGMM The objective of the proposed approach is to obtain an es∗ timate of the unknown parameter which maximizes the likelihood of RGMM on the data . An effective approach for deriving MLE is expectation maximization ( EM ) method . We develop EM solutions for two versions of MLE estimates in this task : population based MLE ( assuming the limit infinite data ) and sample based MLE ( assuming a finite set of samples ) . We name these solutions as population and sample EMrgmm , respectively . Data is finite in real practice , so we should use sample EMrgmm to identify entities’ truths , but the introduction of population EMrgmm enables us to conduct the convergence analysis for both solutions . In this subsection , we first introduce the general EM procedure , and then derive the EM updates of both population and sampleEMrgmm . First , let us briefly review the EM algorithm . Given the lower bounds on the log likelihood Q(·|· ) , EM algorithm successively maximizes the lower bound and then reevaluates the lower bound at the new parameter value . The update procedure is as follows . EM updates : Given t−1 obtained at the ( t− 1) th iteration , the t th iteration of EM algorithm can be summarized in the following two steps :
• E Step . Calculate the lower bound of the log likelihood • M Step . Compute the maximizer as follows :
Q(·|t−1 ) . t = arg max
′∈Ω
′|t−1 ) .
Q(
An EM based method iteratively conduct these steps until some conditions are satisfied . Next , we introduce both the population and sample EMrgmm .
∫ population EMrgmm . When deriving the E step and Mstep for population based MLE of RGMM , we assume that there is an infinite set of samples . The population based Q function Q(·| ) takes the form that
( ∫ Y f(y|X ) log p′ ( X , y)dy
Q( where f(y|X ) denotes the conditional density of Y given X and g(X ) is the density function of the observed variable X . The population based EM operator M : RN → RN is defined as follows : g(X)dX ,
′| ) =
)
X
( 4 )
′| ) .
Q(
M ( ) := arg max ′∈RN
( 5 ) M ( · ) is to find the maximizer of the Q function given the parameters obtained in previous step . sample EMrgmm . sample EMrgmm is derived on a finite set of the claims , ie , X = {X s}S 1 . We assume that each sample given by every source is drawn iid from the mixture density Eq ( 2 ) . Under this assumption , we define the sample based Q function as QS , which is shown as follows :
)
′| ) =
QS(
1 S
P
′|X s y = s log p′ ( X s , y )
. ( 6 )
(
S∑
S∑ s=1 s′
[
]
1927 ′|X s ] is the probability that a sample X belongs to P [ y = s ′ source s , and its value is defined by the following function : P [ y = s|X ] := e
− ∥X−−s e 2σ2
− ∥X−−s∥2
)−1
S∑
(
′ ∥2
2σ2
.
2
2
( 7 ) s′=1
S∑ ′| ) takes the form :
S∑
To simplify the notation , we denote Eq ( 7 ) as ω(X , s ) . Substituting ω(X , s ) ( Eq ( 7 ) ) into Eq ( 6 ) and ignoring terms that do not contain , we show that the sample based function QS(
′
′ − s ∥X s − 2σ2
′∥2
′| ) = − 1 S
ω(X s , s
) s=1 s′=1
QS( We denote the sample based EM operator as Mn : RN → RN , which is to maximize the sample based Q function . ′| ) . According to Namely , MS( ) := arg max′∈RN QS( Eq ( 8 ) , we have that
.
2
( 8 )
S∑
S∑
1 S
′
)(X s − s
′
ω(X s , s s=1 s′=1
MS( ) =
( 9 ) Moreover , Eq ( 9 ) implies that M ( ) = E [ ω(X , )(X − ) ] , where the expectation is taken over X × . 3.4
Sample EMrgmm Algorithm
) .
As discussed , sample EMrgmm which deals with finite samples , is typically adopted in real practice . We summarize this algorithm in Algorithm 1 . We will show its performance on both simulated and real world datasets in Section 5 .
Algorithm 1 sample EMrgmm Input : Entities N = {n}N Output : Truth estimates b n}S;N collection X = {xs s=1;n=1 ,
1 , Sources S = {s}S
1 , and data
Calculate its variance ˆσ2 Initialize µold
1 : for entity n ( n = 1,··· , N ) do 2 : 3 : 4 : end for 5 : Estimate model variance : ˆσ2 = 1 N 6 : Estimate upper bound of biases C using the maximum n using the mean of claims over sources ; n over S sources ; n=1 ˆσ2 n ;
∑
N absolute value of X ;
For each source s , generate s ∼ Uniform(−C , C)N ;
7 : while convergence criterion is not satisfied do 8 : 9 : Update truth estimator according to Eq ( 9 ) :
11 : return b = new
10 : end while new = Mn(old ) ;
4 . THEORETICAL ANALYSIS
In this section , we theoretically present convergence analysis for the proposed solutions : population and sample EMrgmm . The outline of this section is : ( 1 ) In Property 1 , we first provide conditions under which the distance between the population EMrgmm ’s result and the MLE is bounded ; ( 2 ) Based on the concept of covering number of a metric space , we bound the distance between the result of population and that of sample EMrgmm ( Corollary 1 ) ; ( 3 ) Based on Property 1 and Corollary 1 , applying triangle inequality , the error between the sample EMrgmm ’s result and MLE is upper bounded by the number of samples ( Property 2 ) .
Convergence of population EMrgmm Let ’s first introduce the convergence property of the popula∗ tion EMrgmm . Recall that represents the maximizer of the population likelihood . [ 1 ] introduces the self consistency property for the maximum likelihood estimator , that is ,
∗
= arg max
′ Q(
′|
∗
) .
( 10 )
Eq ( 10 ) implies that the maximum likelihood estimator should maximize the population based Q function . Combin∗ ing with Eq ( 5 ) , it is obvious that ) . For the proposed RGMM ( Eq ( 2) ) , we have the following property .
= M (
∗
Property 1 . Given the RGMM with a sufficiently small and a sufficiently large signal to
C∥∗∥2
, there is a universal constant c > 0 and a bias to mean ratio ∗∥2 ∥ 2 noise ratio constant λ ∈ ( 0 , 1 ) with λ ≤ exp(−c ∗∥2 ∥ 2 ∗ − ∥2 ∗∥2 2 ≤ λ∥ 2 , ∗∥2 ≤ ∥ ∗∥2
∥M ( ) − holds for all if ∥ −
.
) , such that
( 11 )
Proof . Please refer to §A for a detailed proof .
4
The idea of the proof for Property 1 is adopted from [ 1 ] . However , two major differences are : ( 1 ) we incorporate a random variable for each latent component whose mean share the same sign , and ( 2 ) we consider arbitrary number of latent components more than 2 . Both differences make the proof more complicated comparing with Corollary 1 in [ 1 ] . The detailed proof is deferred in Appendix A .
Property 1 establishes the conditions under which the convergence of the population EMrgmm M ( ) is guaranteed . Namely , the proposed M ( ) is contractive over a small ball ∗ around , a maximum likelihood estimator of the truths . Given an initial 0 , an immediate result from Property 1 is :
∥t −
∗∥2 2 = ∥M ( t−1 ) −
∗∥2 2 ≤ λ∥t−1 − ∗∥2 ∗∥2 2 .
∗∥2 2 ≤ ··· ≤ λt∥0 −
2
≤ λ∥M ( t−2 ) −
( 12 )
Eq ( 12 ) implies that M ( ) is linear convergence . Moreover , It shows that given any initialization , the proposed M ( ) is able to modify it as the iteration increases . Convergence of sample EMrgmm To prove the convergence property of the sample EMrgmm , we first measure the deviation of its result from that of population EMrgmm . As the covering number is used in the proof , we formally introduce its definition from [ 22 ] .
Definition 1 . Let G be a subset of a metric space . ∀ϵ > 0 , the covering number N ( G , ϵ ) is defined to be the minimal integer n ∈ N such that these n balls with radius ϵ cover G . Based on Definition 1 , the difference between populationEMrgmm ( M ( ) ) and sample EMrgmm ( Mn( ) ) is upper bounded as follows .
Corollary 1 . Given the population and sample based EM operator Mn and M , there exists a constant cffi such that sup ∈Ω
∥Mn( ) − M ( )∥2 ≤ cffi;N ( Ω;ffi)S
− 1
2
( 13 ) holds with probability at least 1− δ , where Ω is the parameter space .
Proof . Please refer to §B for a detailed proof .
1928 , which is stated in the following property .
Based on Property 1 and Corollary 1 , we can bound the distance between the result of sample EMrgmm Mn( ) and ∗ the MLE Property 2 . Under the conditions of Property 1 , ∀0 such that ∥0 − ∗∥2 ≤ r , if there are enough sources , then {t}∞ t=0 obtained by the sample EM algorithm satisfies that ∥t −
∗∥2 ≤ λt∥0 −
∗∥2 + cffi;N ( Ω;ffi)S
( 14 )
− 1 holds with probability of at least 1 − δ , where λ ∈ ( 0 , 1 ) .
Proof . Please refer to §C for a detailed proof .
1 1 − λ
2
− 1
−1=2 ) .
After conduct enough iterations , Eq ( 14 ) shows that the main component of the upper bound is the second terms ( ie , 1 1− cffi;N ( Ω;ffi)S 2 ) . Namely , Property 2 states that the performance of the proposed sample EMrgmm is upper bounded by Ω(S Intuitively , in truth discovery tasks the more the sources , the better the performance of methods . In Property 2 , we theoretically present that the convergence rate of the sample EMrgmm is Ω(S 5 . EXPERIMENTS
−1=2 ) .
Note that , only finite number of samples can be obtained in real world applications , which fits the sample Emrgmm setting . Therefore , all experiments are conducted using sample EMrgmm1 . In this section , we test the sampleEMrgmm on both simulated and real world data sets . The experimental results show the effectiveness of the proposed sample EMrgmm in identifying truths as well as its convergence . We first introduce baselines and performance measures in Subsections 5.1 and 5.2 , respectively . Experimental results on simulated data are presented in Subsection 53 In Subsection 5.4 , we show experimental results on three real world application datasets : Weather Forecast , Indoor Floorplan , and Stock Data . 5.1 Baselines
A variety of truth discovery methods have been developed to identify each object ’s truth . As we consider applications of continuous data in this paper , we compare the sampleEMrgmm with three state of the art truth discovery methods CRH , CATD , GTM , and two naive methods : Mean and Median . Details of the baselines are shown as follows : • CRH : In [ 10 ] , truth discovery task is formulated as an optimization problem , seeking the optimal truth estimators and weights to minimize the weighted distance between claims and the truths . CRH is proposed to iteratively update the truths and source weights . • CATD : CATD [ 9 ] is a statistical method that has been proposed for long tail phenomenon in truth discovery , where confidence interval is incorporated in source weight estimation . • GTM : [ 25 ] proposes a probabilistic graphical model In based method to solve the truth discovery task . their framework , source reliability and truths are model parameters to estimate . • Mean : The average of claims provided by multiple sources is treated as the final estimator of the truth . • Median : The median of claims provided by multiple sources is defined as the final estimator of the truth .
1We will use RGMM and sample EMrgmm interchangeably in the experiment section
5.2 Performance Measures
In the experiments , we have continuous input obtained from multiple sources . Although the ground truths are available , we conduct all methods in an unsupervised manner and the ground truths will only be used in evaluation . To evaluate the performance of sample EMrgmm as well as baselines , we adopt the following measures : • M AE : It measures the mean of absolute error between the output of methods and the ground truths . As L1norm is applied , M AE penalizes more on small errors . • RM SE : It measures the root of mean squared error between output and groundtruth . RM SE penalizes more on big errors because of the involved L2 norm . • ErrorRate(ϵ ) : It is defined as the percentage of the estimated truths falling outside an ϵ ball2 of the ground truth .
Note that a lower measure value means that the truth estimates are closer to the ground truths . Thus , for all measures , the lower the value , the better the method ’s performance . 5.3 Simulated Data
The advantage of using simulated data is that we can simulate different truth discovery scenarios to compare the performance of the proposed sample EMrgmm with that of the baselines . In this section , we first introduce the procedure of generating simulated data . Then , we show the performance of the sample EMrgmm as well as the comparison with baselines in terms of M AE , RM SE , and ErrorRate(ϵ ) .
Data Generation .
In each experiment , we generate N = 200 entities and S = 100 sources . For each source , its bias ( s ) is drawn iid from a distribution F . We assume that the ground truth for all entities are 0s . Thus , the s th component ( ie , source ) of the mixture model follows a multivariate normal distribution , Normal(−s , σ2IN ) . To generate the sample of claims {X s}S 1 , we first randomly generate a source index s from [ 1,··· , S ] , and then X s is drawn from Normal(−s , σ2IN ) , where σ2 = 1 . We use MAE and RMSE , and Error Rate ( 0.1 ) for evaluation . We simulate three different scenarios involving different distri′ butions of source biases : Uniform , Normal , and Student s tdistribution , and then evaluate the performance of all truth discovery methods .
Scenario 1 : ηs ∼ Uniform(−c , c ) .
In this scenario , sources’ biases are drawn from a uniform distribution with c = 2 . The source reliability degrees are uniformly distributed . We report the results on experiments with different source number S = {10 , 20,··· , 100} in terms of all methods in Figure 1 . In Figure 1 , the solid and dark line represents 2 , which is the dominatthe value of function f ( S ) = S ed term in upper bound of the proposed sample EMrgmm ( Eq ( 14) ) . From Figures 1(a ) , 1(b ) and 1(c ) , we can see that the convergence of the proposed sample EMrgmm is similar to the S 2 , which confirms the result in Property 2 . Moreover , we can see that the performance of the proposed RGMM is better when comparing with baselines in terms of RMSE and Error Rate . It means that most of truth estimates from the sample EMrgmm have smaller errors compared with that of baselines . For MAE , all truth discovery methods have the same performance . Scenario 2 : ηs ∼ Normal(0 , δ2 ) . In this scenario , sources’ biases are drawn from a Normal distribution with variance 2ϵ is chosen based on the scale of the multi source data .
− 1
− 1
1929 ( a ) MAE vs . S
( b ) RMSE vs . S
( c ) Error Rate(.1 ) vs . S
Figure 1 : Simulated data on Scenario 1 : Performance with respect to the Number of Sources ( S ) .
( a ) MAE vs . S
( b ) RMSE vs . S
( c ) Error Rate(.1 ) vs . S
Figure 2 : Simulated data on Scenario 2 : Performance with respect to the Number of Sources ( S ) .
( a ) MAE vs . S
( b ) RMSE vs . S
Figure 3 : Simulated data on Scenario 3 : Performance with respect to the Number of Sources ( S ) .
2
2
, 3√
δ2 = 05 Based on three sigma rule of thumb , sources biases fall into the interval [ − 3√ ] with probability of 9973 % Meanwhile , as the variance of the normal distribution is small , there are many samples closer to the mean 0 . Therefore , there are many reliable sources than unreliable sources . The results are shown in Figure 2 . The convergence performance of the sample EMrgmm with respect to the number of sources is similar to that in Scenario 1 .
Scenario 3 : ηs ∼ Student ′ s t (ν ) .
In this scenario , sources’ biases are drawn from a student ’s t distribution with ′ freedom ν = 2 . Compared with previous scenarios , Student s t distribution has heavier tails , ie , it is more prone to producing values that fall far from its mean 0 . Consequently , there are more unreliable sources . The results in terms of MAE , RMSE , and Error Rate(.1 ) are shown in Figure 3 . Result Analysis . Comparing the performance on Scenarios 1 ∼ 3 , we can see that all methods perform best in Scenario 3 and worst in Scenario 3 . In Property 1 , the conditions show that the performance is better if the upper bound of data C is smaller and the variance σ2 is larger . In Sce nario 1 , bC = 6.3476 and ˆσ2 = 2.4502 , while bC = 5.74 , and
ˆσ2 = 2.0886 in Scenario 2 . As student t distribution is longtail , there are some sources which have very large bias . Thus , in Scenario 3 , the upper bound of original simulated data is 1614553 The claims provided these sources can be treat ed as outliers . After removing them , we have bC = 19.9335
( c ) Error Rate(.1 ) vs . S and ˆσ2 = 58458 Based on Property 11 , the performance in Scenario 2 should be the best , as shown in Figure 2 .
The convergence rate of the sample EMrgmm is Ω(S
2 ) as shown in the Section 4 . Experimentally , the convergence 2 ) , as shown rate in Scenarios 1 and 2 is nicely fit to Ω(S in Figures 1 and 2 . In Scenario 3 , the convergence is not as clear as that in Scenarios 1 and 2 , as there are more outliers . 5.4 Real World Data
− 1
− 1
Data Description . We test the proposed sampleRGMM and baselines on real world data . The detailed description of each dataset and their tasks are shown as follows : • Weather Forecast [ 10 ] : Temperature forecasts information for 88 cities in US are collected from three websites : HAM weather3 , Wundergound4 , and World Weather Online5 . Besides , the real temperature for all cities are also crawled as ground truths for evaluation . The goal is to estimate the true temperature for each city from the conflicting data provided by different sources . • Indoor Floorplan [ 9 ] : An Andriod App is designed for smart phone users to collect their estimates of hallway
3http://wwwhamweathercom 4http://wwwwundergroundcom 5http://wwwworldweatheronlinecom
204060801000102030405Number of SourcesMAE CRHCATDGTMMeanMedianFittingRGMM204060801000010203040506Number of SourcesRMSE CRHCATDGTMMeanMedianFittingRGMM20406080100005115Number of SourcesError Rate CRHCATDGTMMeanMedianFittingRGMM2040608010001015020250303504Number of SourcesMAE CRHCATDGTMMeanMedianFittingRGMM2040608010000102030405Number of SourcesRMSE CRHCATDGTMMeanMedianFittingRGMM20406080100002040608112Number of SourcesError Rate CRHCATDGTMMeanMedianFittingRGMM204060801000102030405060708Number of SourcesMAE CRHCATDGTMMeanMedianFittingRGMM20406080100020406081121416Number of SourcesRMSE CRHCATDGTMMeanMedianFittingRGMM20406080100005115Number of SourcesError Rate CRHCATDGTMMeanMedianFittingRGMM1930 distance . There are totally 308 claims from 44 users on 7 indoor hallways . The ground truths are obtained via manually measuring the hallways by tape . The goal is to estimate the distance of indoor hallways from the data provided by a crowd of users . • Stock Data : The stock data in [ 11 ] contains the price information for 1000 stocks from 55 sources over 21 days . To fit our scenarios , we preprocess the data and obtain a full dense dataset which contains the price information of 300 stocks from 36 sources over 19 days . In this task , we only focus on the open price , so the goal is to estimate the true open price for each stock .
Note that we have a different task setting compared with [ 9,10 ] on the real world datasets reported in this paper . We consider a scenario where all entities are claimed by all sources while CATD [ 9 ] and CRH [ 10 ] were applied to entities that are observed by a subset of sources . To fit the full observation scenario , we preprocess the data used in [ 9,10 ] by deleting those entities which have not been claimed by all sources . In addition , our model in this paper tackles continuous data only . Therefore , we select continuous attributes in the Weather Forecast dataset , and the ” Price ” attribute ( ie , a continuous attribute ) in the Stock dataset ( [11] ) , which also differs from the setting in [ 10 ] ( ie , using both categorical and continuous attributes ) . The statistics of three real world datasets are summarized in Table 2 .
Dataset
Table 2 : Statistics of real world datasets claims 3 , 344 308
Weather Forecast Indoor Floorplan
88 7 sources objects
30 44 36
Stock Data
300 ∗ 19
108 , 000 ∗ 19
Result Analysis . On the Weather Forecast data , we reduce the scale of the observations in preprocess step . For example , the original 77 Fahrenheit is changed to 77 We evaluate all methods on different scenarios in which the number of sources increases from 4 to 10 by the stepsize of 2 . In Figure 4 , we report the experimental results in terms of MAE and RMSE . We can see that the performance of the proposed RGMM improves as the number of sources increases . When the number of sources is relatively small , the performance of the proposed RGMM is worse than baselines . however , given a plenty of sources , RGMM converges to other baselines . The experimental results with respect to Error Rate(ϵ ) is presented in Table 3 . Table 3 shows that the performance of RGMM is comparable to that of baselines .
( a ) MAE vs . S
( b ) RMSE vs . S
Figure 4 : Weather Forecast dataset : Performance with respect to the Number of Sources ( S ) .
On Indoor Floorplan dataset , we test all truth discovery methods on different scenarios where the number of sources increases from 6 to 44 by the stepsize of 1 . In each scenario , we randomly choose the pre fixed number of sources . To
Table 3 : Error Rate(ϵ ) on Weather Forecasts
Method
Error Rate(ϵ )
.05
.06
.07
.08
.09
.10
RGMM .2159 CRH .2045 CATD .4205 GTM .4091 .2045 Mean Median .2045
.1591 .1477 .3523 .3409 .1477 .1477
.1477 .1023 .2955 .2386 .1023 .1023
.1023 .0682 .2273 .1932 .0682 .0682
.0795 .0568 .1705 .1591 .0568 .0455
.0455 .0341 .1364 .1023 .0341 .0341
( a ) MAE vs . S
( b ) RMSE vs . S
Figure 5 : with respect to the Number of Sources ( S ) .
Indoor Floorplan dataset : Performance reduce the randomness , we repeat experiments 20 times and report the average of evaluation measures . The performance comparison between the proposed RGMM and baselines in terms of MAE and RMSE are presented in Figures 5(a ) and 5(b ) , respectively . Note that the decrease rate is not exactly fit to the exponential function with power to − 1 2 . For both measures , although the performance of all methods decrease as the number of sources increases , the proposed RGMM is the best . Namely , the truth estimates obtained by RGMM is closer to the truths . We also report experimental results in terms of Error Rate(ϵ ) . We change ϵ from .5 to 1.0 and show the results in Table 4 . We can see that the performance of the proposed RGMM is the best on all scenarios exept ϵ = 6
Table 4 : Error Rate(ϵ ) on Indoor Floorplan
Method
Error Rate(ϵ )
.5
.6
.7
.8
0.9
1.0
RGMM .5714 CRH .4286 CATD .4286 GTM .4286 Mean .5714 .7143 Median
.4286 .4286 .4286 .2857 .4286 .2857
.2857 .2857 .2857 .2857 .4286 .2857
.2857 .2857 .2857 .2857 .2857 .2857
.1429 .2857 .2857 .2857 .2857 .1429
.1429 .2857 .2857 .2857 .2857 .1429
On Stock data , we follow the similar experiment design . We report the results in terms of MAE and RMSE measures in Table 5 . Table 5 shows that the performance of RGMM comparable with baselines . We also test the performance with respect to the number of sources . Due to the page limit , we only show the results on the data collected on day 1 in Figure 6 . To better confirm the theoretical results obtained in Section 4 , we report the convergence of the RGMM with respect to the number of sources on all days’ Stock data in Figure 7 . Each blue line represents a experiment conducted in a single day Stock data . We also fit each line into a function a ∗ S 2 + c where a and c are coefficients . The red line is plot using the average of a and c over 19 days . From Figures 6 and 7 , we can see that the convergence of 2 ) , which perfectly the RGMM ’s performance is indeed Ω(S confirms our theoretical results .
− 1
− 1
5101520253000102030405Number of SourcesMAD CRHCATDGTMMeanMedianFittingRGMM5101520253000102030405Number of SourcesRMSD CRHCATDGTMMeanMedianFittingRGMM102030400204060811214Number of SourcesMAE CRHCATDGTMMeanMedianFittingRGMM102030400204060811214Number of SourcesRMSE CRHCATDGTMMeanMedianFittingRGMM1931 Table 5 : Performance comparison on Stock data MAE
RMSE
RGMM CRH CATD GTM Mean Median
RGMM CRH CATD GTM Mean Median
.0162 .0161 .0155 .0175 .0227 .0308 .0351 .0394 .0505 .0522 .0543 .0479 .0576 .0472 .0506 .0539 .0605 .0761 .0794
.0144 .0149 .0238 .0221 .0234 .0240 .0273 .0303 .0370 .0402 .0425 .0436 .0461 .0438 .0453 .0500 .0544 .0659 .0678
.0145 .0149 .0238 .0222 .0235 .0240 .0273 .0304 .0370 .0403 .0427 .0437 .0462 .0439 .0454 .0501 .0546 .0660 .0679
.0145 .0149 .0238 .0222 .0235 .0240 .0273 .0304 .0370 .0403 .0427 .0437 .0462 .0439 .0454 .0501 .0545 .0660 .0678
.0138 .0144 .0225 .0220 .0231 .0233 .0268 .0296 .0365 .0397 .0412 .0419 .0440 .0422 .0439 .0486 .0525 .0645 .0666
.0145 .0150 .0238 .0222 .0235 .0240 .0273 .0304 .0370 .0403 .0427 .0437 .0462 .0439 .0454 .0501 .0546 .0660 .0679
.0200 .0200 .0200 .0245 .0316 .0337 .0469 .0414 .0525 .0532 .0663 .0608 .0600 .0612 .0600 .0652 .0723 .0856 .0901
.0175 .0199 .0303 .0287 .0316 .0337 .0399 .0414 .0524 .0548 .0558 .0561 .0600 .0585 .0598 .0642 .0703 .0849 .0856
.0175 .0199 .0303 .0287 .0316 .0337 .0400 .0414 .0525 .0548 .0560 .0562 .0601 .0585 .0598 .0643 .0705 .0849 .0857
.0175 .0199 .0303 .0287 .0316 .0337 .0400 .0414 .0525 .0548 .0560 .0562 .0600 .0585 .0598 .0642 .0705 .0849 .0857
.0170 .0189 .0288 .0279 .0304 .0330 .0393 .0407 .0520 .0542 .0545 .0546 .0585 .0575 .0590 .0632 .0690 .0833 .0842
.0175 .0199 .0303 .0288 .0316 .0337 .0400 .0414 .0525 .0548 .0560 .0562 .0600 .0586 .0598 .0643 .0705 .0850 .0857
Data
Day1 Day2 Day3 Day4 Day5 Day6 Day7 Day8 Day9 Day10 Day11 Day12 Day13 Day14 Day15 Day16 Day17 Day18 Day19
( a ) MAE vs . S
( b ) RMSE vs . S
Figure 6 : Stock data on Day 1 : Performance with respect to the number of sources ( S ) .
( a ) MAE vs . S
( b ) RMSE vs . S
Figure 7 : Convergence wrt the number of sources on all Stock data
In summary , the experimental results on both simulated and real world datasets demonstrate that : ( 1 ) the performance of the proposed sample EMrgmm is comparable compared with several state of the art methods , and ( 2 ) the convergence rate of its performance is Ω(S 2 ) as proved in Section 4 ( Property 2 ) .
− 1
6 . CONCLUSIONS
With the increasing possibilities to collect data from multiple sources in the real world , it is critical to identify true facts from conflicting data . To solve this problem , many algorithms were developed based on heuristic principles , optimizations , or probabilistic model . However , in existing literature on truth discovery , the convergence analysis is missing , and thus there is no theoretical guarantee that the results of these algorithms converge to the truths . In this paper , we proposed an effective truth discovery approach with theoretical guarantee . We first introduced the randomized biases of sources to measure their reliability degrees . Then we proposed a novel model ( RGMM ) to represent multisource data with various reliability degrees , which consists of a Gaussian mixture model with the randomized biases incorporated . The parameters of interests in this model are the truths to be identified . We then derive both populationand sample EMrgmm to the MLE of the truth parameter of RGMM . Theoretically , we prove that population EMrgmm converges in probability to an ϵ ball around the MLE with the increasing number of sources , under certain conditions . Moreover , we prove that sample EMrgmm also converges to the ϵ ball around the MLE with more iterations . In the experiments , we evaluate the effectiveness of the proposed sample EMrgmm on both simulated and real world datasets . Experimental results demonstrate that the proposed sample EMrgmm is able to identify reliable information from multi source data , and the estimator converges to an ϵ ball around the MLE under the stated conditions .
7 . ACKNOWLEDGEMENTS
This work was sponsored in part by US National Science Foundation under grant IIS 1319973 , IIS 1553411 and CNS1566374 . The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agency .
8 . REFERENCES [ 1 ] S . Balakrishnan , M . J . Wainwright , and B . Yu . Statistical guarantees for the EM algorithm : From population to sample based analysis . arXiv:1408.2156 , 2014 .
[ 2 ] L . Blanco , V . Crescenzi , P . Merialdo , and P . Papotti .
Probabilistic models to reconcile complex data from inaccurate data sources . In Proc . of CAiSE , pages 83{97 , 2010 .
510152025303500102030405Number of SourcesMAE CRHCATDGTMMeanMedianFittingRGMM510152025303500102030405Number of SourcesRMSE CRHCATDGTMMeanMedianFittingRGMM5101520253000050101502Number of SourcesMAE RGMMFitting5101520253000050101502Number of SourcesMAE RGMMFitting1932 [ 3 ] C . Dai , D . Lin , E . Bertino , and M . Kantarcioglu . An approach to evaluate data trustworthiness based on data provenance . In Proc . of SDM , pages 82{98 , 2008 .
[ 4 ] A . P . Dawid and A . M . Skene . Maximum likelihood estimation of observer error rates using the EM algorithm . Appl . Stat . , pages 20{28 , 1979 .
[ 5 ] X . Dong , E . Gabrilovich , G . Heitz , W . Horn , N . Lao ,
K . Murphy , T . Strohmann , S . Sun , and W . Zhang . Knowledge vault : A web scale approach to probabilistic knowledge fusion . In Proc . of KDD , pages 601{610 , 2014 .
[ 6 ] X . L . Dong , L . Berti Equille , and D . Srivastava . Integrating conflicting data : the role of source dependence . PVLDB , pages 550{561 , 2009 .
[ 7 ] A . Galland , S . Abiteboul , A . Marian , and P . Senellart .
Corroborating information from disagreeing views . In Proc . of WSDM , pages 131{140 , 2010 .
[ 8 ] F . Li , M . L . Lee , and W . Hsu . Entity profiling with varying source reliabilities . In Proc . of KDD , pages 1146{1155 , 2014 .
[ 9 ] Q . Li , Y . Li , J . Gao , L . Su , B . Zhao , M . Demirbas , W . Fan , and J . Han . A confidence aware approach for truth discovery on long tail data . PVLDB , 2014 .
[ 10 ] Q . Li , Y . Li , J . Gao , B . Zhao , W . Fan , and J . Han . Resolving conflicts in heterogeneous data by truth discovery and source reliability estimation . In Proc . of SIGMOD , pages 1187{1198 , 2014 .
[ 11 ] X . Li , X . L . Dong , K . Lyons , W . Meng , and D . Srivastava .
Truth finding on the deep web : is the problem solved ? PVLDB , pages 97{108 , 2012 .
[ 12 ] F . Ma , Y . Li , Q . Li , M . Qiu , J . Gao , S . Zhi , L . Su , B . Zhao ,
H . Ji , and J . Han . Faitcrowd : Fine grained truth discovery for crowdsourced data aggregation . In Proc . of KDD , pages 745{754 , 2015 .
[ 13 ] A . Marian and M . Wu . Corroborating information from web sources . Data Eng . Bull . , pages 11{17 , 2011 .
[ 14 ] C . Meng , W . Jiang , Y . Li , J . Gao , L . Su , H . Ding , and
Y . Cheng . Truth discovery on crowd sensing of correlated entities . In Proc . of SenSys , pages 169{182 , 2015 .
[ 15 ] S . Mukherjee , G . Weikum , and C . Danescu Niculescu Mizil .
People on drugs : credibility of user statements in health communities . In Proc . of KDD , pages 65{74 , 2014 .
[ 16 ] J . Pasternack and D . Roth . Making better informed trust decisions with generalized fact finding . In Proc . of IJCAI , pages 2324{2329 , 2011 .
[ 17 ] G J Qi , C . C . Aggarwal , J . Han , and T . Huang . Mining collective intelligence in diverse groups . In Proc . of WWW , pages 1041{1052 , 2013 .
[ 18 ] R . A . Redner and H . F . Walker . Mixture densities , maximum likelihood and the EM algorithm . SIAM review , pages 195{239 , 1984 .
[ 19 ] D . Wang , L . Kaplan , H . Le , and T . Abdelzaher . On truth discovery in social sensing : A maximum likelihood estimation approach . In Proc . of IPSN , pages 233{244 , 2012 .
[ 20 ] D . Wang , L . M . Kaplan , T . F . Abdelzaher , and C . C .
Aggarwal . On scalability and robustness limitations of real and asymptotic confidence bounds in social sensing . In Proc . of SECON , pages 506{514 , 2012 .
[ 21 ] P . Welinder , S . Branson , P . Perona , and S . J . Belongie . The multidimensional wisdom of crowds . In Proc . of NIPS , pages 2424{2432 , 2010 .
[ 22 ] Q . Wu and D X Zhou . SVM soft margin classifiers : linear programming versus quadratic programming . Neural Comput . , pages 1160{1187 , 2005 .
[ 23 ] J . Yin and J . Wang . A dirichlet multinomial mixture model based approach for short text clustering . In Proc . of KDD , pages 233{242 , 2014 .
[ 24 ] X . Yin , J . Han , and P . S . Yu . Truth discovery with multiple conflicting information providers on the web . TKDE , pages 796{808 , 2008 .
[ 25 ] B . Zhao and J . Han . A probabilistic model for estimating real valued truth from conflicting sources . Proc . of QDB , 2012 .
[ 26 ] B . Zhao , B . I . P . Rubinstein , J . Gemmell , and J . Han . A bayesian approach to discovering truth from conflicting sources for data integration . PVLDB , pages 550{561 , 2012 . [ 27 ] D . Zhou , J . C . Platt , S . Basu , and Y . Mao . Learning from the wisdom of crowds by minimax entropy . In Proc . of NIPS , pages 2204{2212 , 2012 .
APPENDIX A . PROOF OF PROPERTY 1 We follow the similar procedure of the proof in [ 1 ] : Decompose M ( ) − ∗ into two separate functions : E[Γ1 ffi(X ) ] and Γ2 ffi(X ) , and then bound Γ1 , ∀δ ∈ [ 0 , 1 ] . ∗ Define u = Before applying Taylor ’s property to the function → ω(X , ) , we first take a look at its derivative and upper
+ δ∆ where ∆ := − ∗ ffi(X ) and Γ2 ffi(X ) . bound it as follows :
@!s
( X ; ) @
≤ α exp(
⟨X,X⟩+⟨δ ,δ
⟩
σ2
2⟨X,δ
⟩
)
σ2 exp(
) where
S2 e
α , 2C C2 2σ2 . We first apply Taylor property to the function → ω(X , ) and take the expectation over X . Combining ∫ ∫ with the upper bound , we have that E [ (ω(X , ) − ω∗ ( X , ))(X − ) ] | | ≤ E
αΨ(X , ffi)X T
αΨ(X , ffi )
{z
{z
∆dδ
∆dδ
[
]
[
]
}
}
E
+
0
1
0
1
, ( 15 )
Γ1
δ ( X )
Γ2
δ ( X )
⟨X,X⟩+⟨δ ,δ
⟩
) exp(
σ2
2⟨X,δ where Ψ(X , ffi ) = [ ∥E [ (ω(X , ) − ω∗ ( X , ))(X − )]∥2 ≤ ∥Γ1 ffi(X)∥op + supffi∈[0;1 ] supffi∈[0;1 ]
∥Γ2 exp(
σ2
⟩
)
] ffi(X)∥op
∥∆∥2 .
( 16 )
. Thus , we have that
The following two Lemmas provide the upper bounds for E[Γ2 ffi(X) ] , respectively . The proofs are deferred in A.1 and A2 ffi(X ) ] and E[Γ2
]
[
[
Lemma 1 . There exist β1 , β2 , β3 and β4 , such that ffi(X ) ≤ α ∗∥3 + β4∥ Γ1
∗∥2 + β3
+ β2∥
β1
2
2
2 ∥∗∥2
∥∗∥3
2
− ∥ e
∗∥2 8σ2 .
2
Lemma 2 . There exist λ1 , λ2 and λ3 , such that ∗∥2 8σ2 . ffi(X ) ≤ α Γ2
∗∥2 2 + λ3
− ∥ e
λ1 + λ2
∥∗∥2
∥
2
2
2
]
2
Applying Lemmas 1 and 2 , it is easy to prove Theorem 1 . Specifically , substituting Equations ( 20 ) , ( 21 ) , ( 23 ) , and ( 24 ) into ( 16 ) , we have that
E [ ω(X , ) − ω∗ ( X , )(X − ) ] ≤ c1(1 + ϕ + ϕρ + ϕρ2 + 1 −c2 ρ2 fl ) e
S
∥ −
∗∥2 ,
( 17 )
2
∗∥2 ∥ 2
≥ 16/3 . Based on this fact , the bound ( 11 ) is suffi , is sufficiently whenever holds provided that the single to noise ratio C∥∗∥2 ciently large , and the bias to mean ratio small . So far , we have finished the proof for Property 1 . A.1 Proof of Lemma 1 We first apply Taylor ’s property to the function X → exp(
⟨X;X⟩+⟨δ ;δ⟩
) , which yields
∗∥2 ∥ 2
2
1933 ffi(X ) = E Γ1
 α(1 + |
)X T
∥δ∥2 2 exp( 2⟨X;δ⟩
2
2
{z
)
 }
[ |
+E
] }
. ( 18 )
α⟨X , X⟩X T exp( 2⟨X;δ⟩ )
{z
2 fl1 1 ( X ) fl1 2 ( X )
Based on ( 18 ) , it is easy to obtain that
∥Γ1 ffi(X)∥op ≤ sup ffi∈[0;1 ]
∥E(γ1
1 ( X))∥op + sup ffi∈[0;1 ] sup ffi∈[0;1 ]
∥E(γ1
2 ( X))∥op .
1 ( X))∥op and ∥E(γ1
The remainder of the proof is to show a sufficient uniform upper bound of ∥E(γ1 2 ( X))∥op over δ ∈ [ 0 , 1 ] . Based on the discussion before , the distribution of X is symmetric around ∥ ∗∥2 . Let us define A = {X ≤ 1 ( X))∥op , ∗∥ ∥ 4
} . Note that ∥ffi∥2 ≤ ( 1 + r)∥∗∥2 . For ∥E(γ1
)
σ2 exp( exp( exp( ff(1+
1 ( X ) ) ff(1+ therefore , we have that E(γ1 ∥δ ∥2 |A]P[A ] + E[ ≤ E[ )XT 2 σ2 ⟩ 2⟨X,δ ) σ2 ∗∥2 ∥ ≤ E[ ff(1+ 1+r 2)XT σ2 ∥2X 2∥δ σ2 ∗∥2 ∥ ≤ ff2(1+ 1+r 2 ) 2e∥δ∥2 Based on the fact that ∥ffi∥2 = ∥ ∥ P[A ] ≤ exp(− ∥ further : E(γ1
|A]P[A ] + E[ ∥ ∥δ
1 ( X ) ) ≤ 2ff2(1+ 1+r σ2 e∥∗∥2 exp( ff(1+ 1+r σ2 4 exp(
∗∥2 − 1
P[A ] +
∗∥2 ∥ 2 ) ∥∗∥2 2 32σ2 )
∥
∗∥2
+
4
σ2
∥δ ∥2 |Ac ] )XT 2 σ2 2⟨X,δ ⟩ ) ∗∥2 ∥ ff(1+ 1+r 2)XT σ2 2∥δ ∥2X σ2 ∗∥2
) exp( ∗∥2 2)∥ ∥2∥∗∥2 2σ2 ∗
.
)
|Ac ]
( 19 )
)∥2 ≥ ∗∥2 , and the standard Gaussian tail bounds , 322 ) , we can narrow the upper bound ( 19 )
+ δ( − ∗
2 ff(1+ 1+r σ2
4 exp(
∗∥2 ∥ 2)∥ ∥∗∥2 2 8σ2 )
∗∥2
. ( 20 )
Similarly , we have that E(γ1
2 ( X ) ) ≤ E[ ff⟨X;X⟩XT |A]P[A ] + E[ ff⟨X;X⟩XT 2∥δ ∥2X 2∥δ ∥2X ) σ2 σ2 ∗∥2 ∗∥2 − ∥ 32σ2 + ff∥ 2 8σ2
≤ 27ff6 4e3∥∗∥3
∗∥3 43
− ∥ exp( exp( e e
2
2
2
|Ac ]
)
( 21 )
2
+E
{z
] }
[ |
 } ffi(X ) = E Γ2
α |
∥δ∥2 1 + 2 exp( 2⟨X;δ⟩
Therefore , Lemma 1 holds based on ( 20 ) and ( 21 ) , where β1 , β2 , β3 and β4 are chosen properly . A.2 Proof of Lemma 2 Similar to the proof in Corollaries 1 , we have that α⟨X , X⟩ {z 1 ( X))∥op and To derive the uniform upper bounds of ∥E(γ2 2 ( X))∥op , let δ ∈ [ 0 , 1 ] be arbitrarily given and e1 ∈ RN ∥E(γ2 denotes the first canonical basis vector . We can construct an orthonormal matrix , Q , such that Qffi = ∥ffi∥2e1 . Assume that Y = QX , which makes Y ∼ N ( Q , σ2IN ) . Note that 1 ( X))∥op ≤ ∥ffi∥2 ≤ ( 1 + r)∥ ∗ ∥2 . Thus , we have ∥E(γ2 ] . Conditioned on A := {Y1 ≤ ∥ ∗∥2 E[α
σ2 exp( 2⟨X;δ⟩ fl2 2 ( X ) fl2 1 ( X )
} ,
2
2
∗
)
)
.
2
( 22 )
) exp(
∗∥2 ∥ 1+(1+r ) σ2 2∥δ ∥2X σ2 ∥E(γ2 1 ( X))∥op ∗∥2 ∥ |A]P[A ] + E[α ≤ E[α 1+(1+r ) σ2 2∥δ ∥2X ) σ2 ∗∥2 ≤ α(1 + ( 1 + r ) ∥ 2 ∗∥2 ≤ α(1 + ( 1 + r ) ∥ 2
∗∥2 ∥ 1+(1+r ) σ2 ∥2X 2∥δ σ2 )[P[A ] + exp(− ∥δ∥2∥ ∗∥2 )[e 8σ2 ] .
32σ2 + e
− ∥
− ∥
22 2
∗∥2 exp( exp(
2
2
2
2
2
)
∗∥2
( 23 )
) ]
4
|Ac ]
[
]
)
⟩
σ2
162
∗∥2
2 exp(
2⟨X,δ ff⟨X;X⟩
− ∥ e
2 ( X ) ) = E
= : E . Based on [ 1 ] , it is easy ∗∥2 ∗∥2 2 ≥ 16σ2/3 . Moreover , for any index j ̸= 1 ,
Based on the constructed orthonormal projection matrix , the operator norm of the matrix is shown as follows E(γ2 to obtain that E11 ≤ ασ2( whenever ∥ we have Ejj = E[ tion of the uniform upper bound of ∥E(γ2 Ejj ≤ 2α exp(− ∥ Ejj , we have that ∥E(γ2
1 ( X))∥op , we have 322 ) . Combining the results on E11 and
2 ( X))∥op ≤ α(2 + 162 9e2∥∗∥2
∗∥2 162 ) exp(− ∥ ∥
Similar to the deriva ff ∥2Y1 2∥δ σ2
∥ 162 e
− 3∥ 8σ2
9e2∥∗∥2
32σ2 +
∗∥2
∗∥2
∗∥2 exp(
+
] .
)
)
2
2
2
2
2
2
2
2
322 ) . ( 24 )
Therefore , the corollary holds based on Equations ( 23 ) and ( 24 ) , where λ1 , λ2 and λ3 are chosen properly . ∑ B . PROOF OF COROLLARY 1 Define g(X , ) = ω(X , )(X − ) . Therefore , Mn( ) = s=1 g(X s , s ) and M ( ) = E(g ) . Thus , G = {g| ∈ Ω} . Define X , sups∈[S ] ∥X s∥2 . Based on Theorem 1 , we ∗∥2 , and E(g2 ) ≤ can show that ∥g − E(g)∥2 ≤ X + C + λr∥ ( X + C)2 . As for the special case with τ = 0 in Lemma 5.1 in [ 22 ] , for ϵ > 0 we have that
1 S
S
∥Mn( ) − M ( )∥2 > 4ϵ ]
∑ s=1 g(X s , s ) − E(g)∥2 > 4ϵ ]
S
2((C+X)2+ 1 3
−Sϵ2 ( C+X+λr∥∗∥2 )ϵ ) ,
P[sup∈Ω = P[supg∈G ∥ 1 ≤ cN ( Ω , ϵ)e
S
( 25 )
− 1 t )S t ;N ( Ω ; δ ∗∥2 > ϵ ]
2 } − 1 t i=0 Ai . Corollary 1 states that for each t we have 2 ] ≥ 1− δ . Therefore , for any ϵ > 0 , − 1 where c is a positive constant . Define the right hand side 2 for some as δ . Then we can derive that ϵ = cffi;N ( Ω;ffi)S constant cffi;N ( Ω;ffi ) . ∩ C . PROOF OF PROPERTY 2 Define event At = {∥Mn(t ) − M ( t)∥2 ≤ cffi;N ( Ω;ffi)S and A = thatP[At ≤ c δ we have that P[∥t+1 − ∗∥2 > ϵ|A]P[A ] = P[∥Mn(t ) − M ( t)∥2 + ∥M ( t ) − ∑ +P[∥Mn(t ) − M ( t)∥2 + ∥M ( t ) − ∗∥2 > ϵ|Ac]P[Ac ] ∗∥2 > ϵ|A ] + P[Ac ] ≤ P[∥Mn(t ) − M ( t)∥2 + ∥M ( t ) − ≤ P[λt+1∥0 − − 1 P[Ac i ] t )S 2 > ϵ ] + ≤ P[λt+1∥0 − − 1 2 > ϵ ] + t ffi t )S t The first inequality holds because P(A ) ≤ 1,∀A . To obtain the second inequality , we first show that ∗∥2
∥t+1 − ≤ ∥Mn(t ) − M ( t)∥2 + ∥M ( t ) −
∗∥2 = ∥Mn(t ) −
∗∥2 + 1 ∗∥2 + 1 t ;N ( Ω ; δ t ;N ( Ω ; δ
1− c δ 1− c δ
≤ δ .
∗∥2 t i=0 t i=0 Ai , we have that ∥Mn(t ) − M ( t)∥2 ≤ δ . Applying the same procedure on ∥M ( t ) − ∗∥2 , we can obtain the first part . The second part is easy to obtain by using the Boole ’s Inequality . The last inequality holds provided that ϵ ≥ λt+1∥0 −
− 1 2 . Therefore , in probability at least 1 − δ , we have that ∥t+1 − − 1
∗∥2 ≤ λt+1∥0 −
∗∥2 + 1
∗∥2 + 1 t ;N ( Ω ; δ
1− c δ t )S
2 . ( 26 )
As At ⊆ ∩
1− c δ t ;N ( Ω ; δ t )S which completes the proof .
1934
