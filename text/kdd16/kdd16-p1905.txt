Transfer Knowledge between Cities
Ying Wei∗§ , Yu Zheng† , Qiang Yang§
†Microsoft Research , Beijing , China
§Hong Kong University of Science and Technology , Hong Kong
§{yweiad,qyang}@cseusthk , †yuzheng@microsoft.com
ABSTRACT The rapid urbanization has motivated extensive research on urban computing . It is critical for urban computing tasks to unlock the power of the diversity of data modalities generated by different sources in urban spaces , such as vehicles and humans . However , we are more likely to encounter the label scarcity problem and the data insufficiency problem when solving an urban computing task in a city where services and infrastructures are not ready or just built . In this paper , we propose a FLexible multimOdal tRAnsfer Learning ( FLORAL ) method to transfer knowledge from a city where there exist sufficient multimodal data and labels , to this kind of cities to fully alleviate the two problems . FLORAL learns semantically related dictionaries for multiple modalities from a source domain , and simultaneously transfers the dictionaries and labelled instances from the source into a target domain . We evaluate the proposed method with a case study of air quality prediction .
CCS Concepts •Information systems → Data mining ; Geographic information systems ; •Computing methodologies → Transfer learning ; Keywords Urban Computing ; Multi modality ; Transfer Learning
1 .
INTRODUCTION
The rapid progress of urbanization has modernized people ’s lives , but also engendered many challenges in cites , such as traffic congestion and air pollution . Recently , the proliferation of big data in cities has fostered unprecedented opportunities to tackle these urban challenges by data science and computing technology , aka , urban computing [ 33 ] . Given the complex setting of a city , we usually need to harness the diversity of data ( ie , multi modality ) to solve an urban computing problem . For example , to predict and tackle air pollution , we need to check air quality data from monitoring stations , pollution emission from factories and vehicles , land ∗The paper was done when the first author was an intern in Microsoft Research under the supervision of the second author . uses and meteorological data of different locations [ 36 , 34 ] . To diagnose a city ’s noise situation , we need to consider human mobility , traffic conditions and layout of a neighborhood [ 35 ] . Thus , to unlock the power of knowledge from multiple disparate datasets ( ie , multi modalities ) is a key research problem in urban computing .
The problem becomes more challenging when we conduct urban computing in a “ new ” city where infrastructures and services are not ready or just built , thus the data required by a task are insufficient . For example , when we conduct air quality prediction in Baoding , we face the following two challenges as shown in Figure 1 . 1 ) The label scarcity problem : the ground truth labels , ie , air quality data , are very scarce because there exist only a few air quality monitoring stations in Baoding . 2 ) The data insufficiency problem : there are two types of insufficiency . One refers to structured modality missing . The taxi trajectory data ( D4 ) , characterizing the pollution emission from vehicles , are existing in Beijing but missing in Baoding . The other is within modality insufficiency . The meteorology data ( D3 ) in Baoding are not that sufficient as in Beijing due to limited weather stations . ffi(cid:30 ) ) ( cid:36)fl (cid:36) (cid:30 ) fl
( cid:36 ) fl fl ( cid:36 ) ff*fl fffl
)ffi'fi(' . fffflfl ffi('fiff'' . fifflfl* fl ffiff''fiff(' . fifflfl* ffiff('fi'' . fl* fflfl* ffi''fi'' . ffl(cid:30)ffl ffi''fi('' . fl . flfi fl' flff
( cid:36) (cid:30 )
Figure 1 : An example of transferring knowledge from Beijing to Baoding city .
An interesting question arises : can we transfer knowledge from a city where data are sufficient , to a city which faces either the label scarcity or the data insufficiency problem ? As demonstrated in Figure 1 , based on Beijing ’s data , we can learn the knowledge about underlying connections between different modalities ; eg , air pollution might be related to traffic congestion which would be caused by a dense road network structure . With such knowledge transferred from Beijing , we may be able to infer Baoding ’s air pollution based on road network structures even if there exists no traffic data like taxi trajectories . In this example , Beijing is a source domain where knowledge comes from , and Baoding is a target domain that we transfer knowledge to .
To transfer knowledge between different cities ( referred to as domains in the rest of this paper ) is a challenging task , as data from different cities may have different distributions in feature and label
1905 spaces . Using the air quality inference as an example , as shown in Figure 2(a ) , the distributions of humidity ( ie , a kind of feature ) in four cities are very different . The distributions of the four cities’ air quality ( ie , labels ) are also different . Though transfer learning [ 16 ] has been proposed to tackle this challenge , none of existing work can solve our problem given the following three unique challenges .
( a ) Humidity distribution .
( b ) Air quality distribution .
Figure 2 : Distribution differences across domains .
First , we transfer knowledge between source and target domains with multi modality data rather than single modality data . Multimodality data have incommensurable representations . For example , the Point Of Interests ( D2 ) in Figure 1 is characterized as Bool ean values indicating categories of a venue , while the meteorology ( D3 ) is featured as real values . Simply concatenating features extracted from datasets of different modalities into a single modality compromises the performance of a transfer learning model [ 19 , 34 ] . Thus , most transfer learning models [ 16 , 28 ] designed for a singlemodality dataset are not applicable to our problem .
Second , though a few multi view transfer learning algorithms [ 5 , 20 , 26 , 25 , 30 ] support multi modality data , none of them can tackle the data insufficiency problem mentioned in Figure 1 . Because of within modality insufficiency , different instances may have different modalities in a target domain . Thus , the instances cannot be treated equally . When facing the structured modality missing , we need to complement a missing modality with its knowledge representation from a source domain .
Third , data of different modalities should have different weights when transferring between different source and target domains . For example , when transferring knowledge from Beijing to Shanghai for air quality prediction , road networks may play a more important role than other modalities ( like weather ) as the two cities have a very similar structure of road networks ( but different weather conditions ) . When transferring between Beijing and Tianjin ( which are geographically close ) , however , weather conditions of the two cities are more similar than other modalities , thereby playing a more important role in the transfer . Existing transfer learning methods cannot well learn the weights for data of different modalities .
To tackle the three challenges , we propose a FLexible multimOdal tRAnsfer Learning ( FLORAL ) method with the following three contributions : • It enforces multi modalities to share knowledge and representation structures by learning semantically related dictionaries each modality has a dictionary which consists of atoms encoding latent semantic meanings ; different modalities have different dictionaries but all modalities’ dictionaries share the size and latent semantic space ; eg , the third atoms of all modalities’ dictionaries semantically mean “ good air quality ” . • It settles the data insufficiency problem , by transferring semantically related dictionaries learnt from a source to enrich feature representations of a target domain . Moreover , an algorithm called Multimodal Transfer AdaBoost ( MTAB ) , capable of learning and differentiating different modalities’ weights , is proposed to leverage labelled source instances to alleviate the label scarcity problem .
•We evaluate our method on air quality prediction in three cities , with performances outperforming six baselines .
2 . RELATED WORK
In this section , we briefly review the related work in two categories : some representative research on multimodal data fusion , and state of the art transfer learning methods . 2.1 Multimodal Data Fusion
There have been many attempts made towards fusing multimodal data . Some of them perform model level fusion , ie , generating a model for each data modality and unifying these models’ outputs as the final result . Co training [ 34 ] and multi kernel learning [ 32 , 36 ] belong to this category . The other line of research fuses different data modalities in feature level . The most naive way is to directly concatenate features from different modalities [ 23 ] . However , the performance of this method is usually inferior because it introduces overfitting and ignores non linear interactions between modalities according to [ 19 ] . The majority of feature level fusion devote to extract a semantic latent subspace or build a translator to align different modalities . The techniques capable of aligning embrace translation [ 4 , 21 ] , canonical correlation analysis [ 6 ] , matrix factorization [ 17 ] , manifold alignment [ 35 ] , coupled dictionary learning [ 29 ] , and multimodal deep learning [ 10 , 19 ] . Either model level or feature level multimodal data fusion methods require sufficient data in each modality , as well as abundant correspondence between instances across modalities . To solve urban computing tasks in a city facing the data insufficiency problem , which our work focuses on , these methods become powerless and even infeasible ( imagining that a modality is missing ) . 2.2 Transfer Learning
Transfer learning [ 16 ] leverages knowledge from a source domain to facilitate learning in a target domain . Almost all work in this field have been motivated by the scarcity of labelled data in a target domain . Until recently , Yang et . al [ 28 ] initiated the setting called heterogeneous transfer learning which enriches the modality in a target domain with the other modality from a source by providing complementary views . This work and its follow up [ 17 , 22 ] , however , can only handle the case where both source and target domains contain single modality only .
Two strands of research , ie , multi task multi view learning and multi view transfer learning , enable knowledge transfer between domains with multimodal data . Nevertheless , we first emphasize the difference between multi task learning and transfer learning : multi task learning assumes sufficient annotated data in each task and treats all tasks equally ; while transfer learning cares only the target domain with scarce labelled data . Besides , most multi task multi view learning algorithms transfer model parameters , thus ignore the differences between tasks [ 31 ] or rely on enough labelled data in all tasks to learn the differences [ 14 , 18 , 24 ] . Though some work [ 7 , 9 , 27 ] transfer knowledge in feature level , IteM2 [ 7 ] can only tackle non negative feature values , and MAMUDA [ 9 ] and HiMLS [ 27 ] cannot fully handle the data insufficiency problem , especially the within modality insufficiency .
To the best of our knowledge , there are only a few attempts on multi view transfer learning . Zhang et . al [ 30 ] first proposed the MVTL LM algorithm that transfers both model parameters and instances between domains with multi views . The Multi transfer [ 20 ] and DISMUTE [ 5 ] extend it to multiple source domains and multiclass classification , respectively . Blitzer et . al [ 2 ] pointed out the limitations of parameter and instance transfer in dealing with a target domain whose distribution distinctly differs from a source ’s .
1906 The IMAM [ 26 ] and MDT [ 25 ] alleviate the limitations by performing feature level knowledge transfer . Unfortunately , none of these work tackles the within modality insufficiency , and differentiates different modalities’ weights when transferring .
3 . FLEXIBLE MULTIMODAL TRANSFER
LEARNING
In this section , we present our method in detail . We first introduce the general framework in Figure 3 , which involves two major pipelines , ie , learning semantically related dictionaries from a source domain ( represented by broken blue arrows ) , and transferring dictionaries and instances from a source to a target domain ( shown in red solid arrows ) . After we introduce the notations and problem definitions , we detail how to learn semantically related dictionaries , and transfer the dictionaries and instances . The complexity analysis is given at the end of this section . n i a m o D e c r u o S d e r r e f s n a r T e g d e l w o n K n i a m o D t e g r a T
SDB 1
SDB 2 M disparate modalities
SDB M
Original feature extraction
Graph construction
Graph clustering
1S
2S
MS
Sparse coding model
Dictionary Inference
1ˆS
2ˆS
MSˆ
1D
2D
MD
Multimodal Transfer AdaBoost
Sparse coding model
1ˆT
3ˆT
1T
3T
MT
Max pooling
MTˆ
MPTˆ
Original feature extraction
Prediction disparate modalities
1M TDB 3
TDB 1
TDB M Learning dictionaries
Results
Final classifier
Transferring dictionaries/instances
Figure 3 : The framework of our proposed FLORAL method .
3.1 Overview
Learn semantically related dictionaries : To learn commensurable representations for multi modalities , we first learn semantically related dictionaries from a source domain through a dictionary learning approach . In this approach , we build a graph that connects instances across different modalities and those in each modality . We then cluster the graph into K clusters , while ensuring that each cluster encodes a latent semantic meaning and contains instances from all modalities . Subsequently , for each modality , we build a dictionary by taking the K cluster centres of the modality as atoms . Obviously , different modalities’ dictionaries have the same size K , and share the K dimensional latent semantic space .
Transfer dictionaries and instances : To address the data insufficiency problem in a target domain , we transfer the semantically related dictionaries learnt from a source . For each modality in a target domain , we extract original features , and learn enriched representations over this modality ’s dictionary by sparse coding . Enriched representations make an instance more informative , thus alleviate within modality insufficiency . As the M dictionaries may influence each other by sharing semantic meanings , the knowledge of those missing modalities ( eg , the second modality illustrated here ) are preserved in the dictionaries and enriched representations of existing modalities . Therefore structured modality missing is addressed .
Transferring the dictionaries is not enough to address the label scarcity problem in a target domain . We also transfer labelled instances from a source . Before transferring , we meet the following two prerequisites : 1 ) learn enriched representations of labelled source instances by sparse coding , in order to make representations of source and target instances consistent ; 2 ) perform max pooling for each target instance to aggregate enriched representations of all existing modalities , so that target instances can be treated equally regardless of within modality insufficiency . Once these prerequisites are satisfied , we apply the Multimodal Transfer AdaBoost algorithm to transfer labelled source instances . The output of the algorithm is a classifier that can predict any target instances . l l li li li l j l j l j ui ui ui
}Nt l i=Nt l
}N s l j=N s l l and Nt u j u j , N s
,··· , tm
,··· , tM
3.2 Notations and Problem Formulation Suppose that in the target domain we are provided a very few la}Nt i=1 with labels y = {yi}Nt belled instances Tl = {t1 ,··· , tM l i=1 and some unlabelled instances Tu = {t1 ,··· , tm +Nt u +1 , where ∈ Rpm denote the feature vector of the mth modality of the li , tm tm ui ith labelled and unlabelled instance , respectively . Nt u indicate the number of labelled and unlabelled instances , respectively . Meanwhile , there exists a source domain in which sufficient la}N s ,··· , sM ,··· , sm belled instances Sl = {s1 j=1 with labels g = {g j}N s l j=1 ,··· , sm and unlabelled instances Su = {s1 , ··· , sM +N s u +1 are availu j able . The meanings of sm l j , sm l , N s u , are similar to those in the target domain . Note that M is the total number of modalities li or tm in the source domain , while in the target domain tm ui could be missing for some 1 ≤ m ≤ M of some 1 ≤ i ≤ Nt + Nt u l as a result of the data insufficiency . Our goal is first to learn M dictionaries D1,··· , Dm,··· , DM for all M modalities from Sl and Su , where Dm ∈ Rpm×K . Subsequently , we transfer these dictionaries to the target domain , and obtain enriched representation}Nt s ˆTl = {ˆt1 i=1 of Tl over the dictionaries , where }Nt ˆtm l i=1 li is the max pooling result of ˆTl , by aggregating all existing modalities {ˆtm li could be missing . ) into ˆtMP li for any ith instance . The same applies to ˆTMP . Finally , we learn u a classifier h f ( ˆTMP ) by Multimodal Transfer AdaBoost to transfer u labelled source instances , ie , ˆSl , and adapt to target instances , ie , ˆTl and ˆTu . For brevity , we summarize these notations in Table 1 .
∈ RK . We obtain ˆTu and ˆSl in the same fashion . ˆTMP
}M m=1 ( for some 1 ≤ m ≤ M , ˆtm
,··· , ˆtM
,··· , ˆtm
= {ˆtMP u j li li li li li l l
Table 1 : Definition of Notations
Notation
Description
No .
Input tm li tm ui sm l j sm u j yi g j
Dm ˆtm li ˆtMP li ˆtm ui
ˆtMP ui
ˆsm f ( · ) l j mth modality of ith labelled instance in the target domain mth modality of ith unlabeled instance in the target domain mth modality of jth labelled instance in the source domain mth modality of jth unlabeled instance in the source domain label of ith labelled instance in the target domain label of jth labelled instance in the source domain
Output li m=1
}M dictionary for mth modality enriched representation for tm li max pooling of {ˆtm enriched representation for tm ui max pooling of {ˆtm enriched representation for sm l j classifier for ˆTMP
}M m=1 ui u
Nt l Nt u
N s l N s u
Nt l N s l
M Nt l Nt l Nt u
Nt u
N s l li l j ui u j l i=1 l j=1
Set Notation Tl = {t1 Tu = {t1 Sl = {s1 Su = {s1 y = {yi}Nt g = {g j}Ns D = {Dm}M ˆTl = {ˆt1 ˆTMP l ˆTu = {ˆt1 u = {ˆtMP ˆTMP ˆSl = {ˆs1
= {ˆtMP ui li l j
+Nt u +1 li ui l i=1
,··· , tM }Nt }Nt ,··· , tM l i=Nt l }Ns ,··· , sM l j=1 ,··· , sM }Ns l j=Ns l u j l j
+Ns u +1 m=1 li l i=1
}Nt li l i=1
,··· , ˆtM }Nt ,··· , ˆtM }Nt l i=Nt ui l }Nt +Nt u l i=Nt +1 l ,··· , ˆsM
}Ns ui l j=1 l j
+Nt u +1
1907 3.3 Learn Semantically Related Dictionaries Sparse coding , a technique widely used in machine learning , represents data vectors as sparse linear combinations of basis elements . The set of basis elements is called dictionary . Sparse coding provides an effective way to homogenize representation structures of multi modalities , by enforcing all modalities’ dictionaries semantically related and learning linear combination coefficients over the corresponding dictionary for each modality as new representations . There are three main categories of techniques to learn dictionaries : probabilistic learning , reconstruction error minimization , and clustering . Here we prefer clustering because of its advantage in extracting semantically related dictionaries . However , directly clustering multi modalities in incommensurable representation structures is impossible . Instead , we propose a graph clustering algorithm as shown in Figure 4 , in which we build a weighted graph to model pairwise similarities between vertices across different modalities and within each modality . Though the work [ 8 ] also learns dictionaries by graph clustering , it learns a dictionary for single modality only . Next , we will detail the graph construction on multi modalities , the graph clustering with highly efficient submodular optimization , and the dictionaries inference .
Intra edge Inter edge
1
1
1
1
Graph Construction
4
4
4
4
3
3
3
3
6
6
6 i and sn
As for a pair of vertices sm j in the mth and nth modality , respectively , we connect them with an inter edge whose weight equals to 1 , ie , wm,n = 1 , if the ith and jth instances are known to i j be correlated . The correlation depends on specific applications . In air quality prediction , a region ( denoted by an eclipse in Figure 4 ) is an instance . Therefore the ith and jth instances are correlated if the two corresponding regions are geographical neighbours .
Submodular Graph Clustering
332 A natural idea of graph clustering is to partition sparsely connected dense subgraphs from each other based on the notion of intra cluster density versus inter cluster sparsity . Given a graph G(V , E ) , we select A ⊆ E , so that the resulting graph G(V , A ) contains exactly K connected components . Obviously , this is a discrete optimization problem . Submodularity , oftentimes viewed as a discrete analog of convexity , is the key to effectively and efficiently solve discrete optimization problems in machine learning . Thus , we design the objective function to satisfy the "submodularity" condition . Before proceeding to the objective function , we first introduce the definitions of submodularity and monotonicity .
Definition 1 . ( Submodularity [ 12 ] ) Let E be a finite set . A set function F : 2E → R is submodular if F(A ∪ {a1} ) − F(A ) ≥ F(A ∪ {a1 , a2} ) − F(A ∪ {a2} ) , for all A ⊆ E and a1 , a2 ∈ E \ A . This property , also named diminishing marginal gains , states that the impact of adding an element to a larger set is less .
Submodular Graph Clustering
6
6 6
6 6
6 6
Definition 2 . ( Monotonically Increasing ) A set function F is mono tonically increasing if F(I1 ) ≤ F(I2 ) for any I1 ⊆ I2 .
= exp wm i j i and sm
The more similar sm edge connecting them is .
2
−sm − sm j 2δ2 i
.
( 1 )
( b ) Homogeneity
( d ) Modality diversity j are , the larger the weight of the intra
Figure 5 : Illustrations of the four criteria met by our objective function . An eclipse represents a cluster .
1
1
1 1
1
1
1 1
1 1
1
1d 1
1d 2
1d 3
1d 4
2d 1
2d 2
2d 3
2d 4
Dictionary Inferrence Kd 1
Kd 3
Kd 4
Kd 2
1 D
Kp ( cid:143 ) 1
4 D
Kp ( cid:143 ) 4
Figure 4 : The procedures of dictionary learning . Different shapes represent different modalities , while the eclipses enclosing shapes denote instances . The eclipses with numbered shapes are labelled instances .
331 Graph Construction We first build an undirected graph G = ( V , E ) . The vertex set V consists of all modalities of all instances in the source domain , ie , V = Sl ∪ Su . We denote |V| , |Vm| , |Vl| and |Vu| as the number of all vertices , vertices in the mth modality , labelled vertices and unlabelled vertices , respectively . The edge set E models pairwise relations between vertices within each modality , ie , intra edges , and across different modalities , ie , inter edges . i and sm
For a pair of vertices sm j in the mth modality , we measure their similarity with the Euclidean distance between their feature vectors . The ith and jth vertices are connected with an intra edge if each of them is among the top k similar vertices of the other vertex . This way of constructing intra edges , ie , mutual k NN , has been proved to outperform traditional k NN in semi supervised clustering [ 15 ] . To weight each intra edge , we apply Gaussian kernels to the similarity between two end vertices of the edge :
In order to introduce the criteria met by our objective function , we compare a pair of graph clustering results ( C1 , C2 ) for each criterion in Figure 5 . C2 more closely complies with each criterion by enforcing O(C1 ) < O(C2 ) . We have determined the following four criteria . 1 ) The compactness guards the basic idea of graph clustering , ie , intra cluster density . Maximizing the objective ensures that densely rather than sparsely connected vertices constitute a cluster . 2 ) The homogeneity requires each cluster to be homogeneous for labelled vertices , ie , a cluster should not mix vertices belonging to different categories . 3 ) The label balance states that the number of labelled vertices in each cluster stays “ balanced ” . This constraint avoids to produce clusters without category labels , and thereby supports the homogeneity . 4 ) The modality diversity ensures that each cluster contains vertices from all modalities . The compactness equips the dictionaries with representation effectiveness . The homogeneity and label balance enforce each dictionary atom , ie , each cluster center , to encode a latent semantic meaning and be discriminative . The modality diversity is crucial to couple all modalities’ dictionaries to be semantically related .
1
1
2
1
1
1
1 6
1
1
1
22 2
2
1
2
1
1
1
1
2
1
1 6
( a ) Compactness
1
1
1
22 2
2
1
2
1 1
1
1
1
1
1
1
1 1
1
1
1
1
1
1
( c ) Label balance
1 1
1
1
1
1 1
1
1
1
1908 1 − j:ei j∈A wi j wi
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
Compactness : A random walk , starting at a vertex and then randomly travelling to a connected vertex , is more likely to stay within a cluster than travelling between . Therefore conducting random walks on the graph can discover clusters where the flow tends to gather . The transition probability from a vertex vi to a vertex v j is defined as a set function Pi j(A ) : 2E → R for the graph G(V , A ) : if i = j , i . j , ei j ∈ A , i . j , ei j fi A , if if wi j wi 0
Pi j(A ) =
( 2 ) which encourages random walks within clusters ( ei j ∈ A ) and eliminates those between clusters ( ei j fi A ) . wi = j:ei j∈E wi j is the total weights incident to vi . We add a self loop transition ( i = j ) to maintain the total transition probability out of vi to be 1 .
To satisfy compactness , we define the objective as the entropy rate of a random walk [ 3 ] to measure the uncertainty of the walk :
C(A ) = −
μi
Pi j(A ) log Pi j(A ) ,
( 3 ) i j y k=1 k=1 wall
( 4 ) max
NA
NA
, w2 wall
) with wall =
,··· , w|V| wall
|Vl(y)k| − NA .
|V| where μi is the ith element of the stationary distribution μ = ( μ1 , μ2 , ··· , μ|V| ) = ( w1 i=1 wi . Intuitively , random walks on dense subgraphs are more uncertain than on sparse subgraphs . Hence maximizing the entropy rate ensures the compactness , and enforces that the edges selected into A from E can make each cluster as dense as possible . C(A ) has been proved to be monotonically increasing and submodular in [ 11 ] . Homogeneity : Suppose that for the graph G(V , A ) given by current A , we have NA connected components , ie , G1,··· , GNA . In the kth connected component Gk , we denote the number of labelled vertices as |Vlk| , and the number of labelled vertices carrying the label y as |Vl(y)k| . Gk ’s homogeneity can be defined as H(Gk ) = 1|Vlk| maxy |Vl(y)k| , which computes the percentage of those vertices carrying the mostly assigned label in Gk . The objective function of homogeneity for the whole graph G(V , A ) wrt A is straightforward by averaging over all NA connected components : H(A ) = Maximizing Equation ( 4 ) encourages homogeneity ( the first term ) , but avoids a trivial solution where each cluster contains a single vertex by restricting NA to be as small as possible ( the second term ) . The monotonicity and submodularity of H(A ) are proved in [ 8 ] . Label balance : Motivated by the fact that the information entropy of a random variable achieves the maximum if this random variable is uniformly distributed , we consider the percentage of labelled vertices across clusters as a random variable and propose the objective function for label balance as :
|Vlk| |Vl| × H(Gk ) − NA = 1|Vl|
L(Gk ) log L(Gk)−NA = − NA
L(A ) = − NA |Vlk| |Vl| −NA . ( 5 ) So that maximizing L(A ) enforces labelled vertices to scatter uniformly across NA clusters . In [ 11 ] , the authors prove that − k pA(k ) log pA(k ) − NA satisfies monotonicity and submodularity . Modality diversity : We again employ the information entropy to formulate the objective for modality diversity , but consider the percentage of vertices in each modality across clusters as a random variable . By averaging over all M modalities , we obtain : M(Gk ) log M(Gk ) − NA | log
( 6 ) Maximizing M(A ) encourages each cluster to be diverse , i.e , inmonotonicity and submodularity of − cluding vertices from all M modalities . As mentioned above , the k pA(k ) log pA(k)− NA have been proved in [ 11 ] . Meanwhile , we are provided with the fact that
( − NA |Vm| |V| NA M |Vm
|Vm |Vm| − NA .
|Vlk| |Vl| log
= − 1|V|
M(A ) =
M m=1 m=1 k=1 k=1 k=1 k=1
)
| k k linear combination with nonnegative coefficients preserves monotonicity and submodularity [ 13 ] . M(A ) , therefore , is also guaranteed to be monotonically increasing and submodular .
Combining the four objective functions introduced , the overall
A optimization problem can be written as : max st A ⊆ E and NA ≥ K ,
O(A ) = C(A ) + λH(A ) + γL(A ) + μM(A )
( 7 ) where λ , γ , and μ are three trade off parameters to balance the importance of the four terms . O(A ) , a linear combination of C(A ) , H(A ) , L(A ) , and M(A ) , is monotonically increasing and submodular . Solving the optimization problem in Equation ( 7 ) is NP hard . Fortunately , the submodularity of O(A ) contributes a greedy approximation algorithm with effectiveness and efficiency guarantee . It initiates A = ∅ and iteratively selects the edge e ∈ E \ A to maximize the marginal gain O(A ∪ e ) − O(A ) . Fisher et al . [ 13 ] showed that the algorithm gives a 1/2 approximation bound on the optimality of the solution . Besides , the algorithm is highly efficient thanks to the diminishing marginal gains property of submodular functions . In each iteration it computes the marginal gain for only the edge who holds the second largest gain in the previous iteration , instead of all edges in the set E \ A . The implementation details and time complexity will be discussed in Section 35 333 Dictionary Inference In the kth cluster , we calculate the center of vertices in the mth modality as the dictionary atom dm k . The final dictionary of the mth modality Dm combines K dictionary atoms inferred from all K clusters , ie , Dm = [ dm K ] . To wrap up , we present the pseudo 1 code of learning semantically related dictionaries in Algorithm 1 . Algorithm 1 Learn Semantically Related Dictionaries ( LSRD ) Input :
Sl , Su – the labelled and unlabelled instances in the source domain ; g – the label vector in the source domain ; λ , γ , μ – trade off parameters for initialization ; K – the dictionary size ;
Output : D = {Dm}M 1 : Construct the graph G = ( V , E ) ; 2 : Initialize A ← ∅ , D1,··· , DM ← ∅ , λ = ( maxe∈E C(e)−C(∅ ) maxe∈E H(e)−H(∅ ) )λ , γ =
,··· , dm m=1 maxe∈E M(e)−M(∅ ) )μ ; maxe∈E L(e)−L(∅ ) )γ , μ = ( maxe∈E C(e)−C(∅ ) ( maxe∈E C(e)−C(∅ ) 3 : while NA > K do ˆe ← arg maxe∈E\A O(A ∪ e ) − O(A ) ; 4 : A ← A ∪ ˆe ; 5 : 6 : end while 7 : for m = 1,··· , M do for k = 1,··· , K do 8 : 9 : 10 : 11 : end for
Dm = Dm ∪ {(1/|Vm end for
∈Gk sm j j:sm j
} ;
| ) k
3.4 Transfer Dictionaries and Instances
341 Transfer Dictionaries We learn M semantically related dictionaries from the source domain to unlock the power of sparse coding in homogenizing different modalities as stated in Section 33 More importantly , we transfer the M semantically related dictionaries to the target domain , and apply them to learn enriched representations of target instances , in order to address the data insufficiency problem . Mathematically , for the mth modality of the ith labelled instance in the target domain ( if available ) , ie , tm li , we transfer the mth dictionary learnt from the source domain , ie , Dm , and apply sparse coding to learn the enriched representation ˆtm li by 2 − Dmˆtm + αˆtm F
≥ 0 , tm
1 st
( 8 )
ˆtm li li li li min ˆtm li
1909 where α controls the sparsity of enriched representations . We obtain the enriched representation ˆtm ui for the mth modality of the ith unlabelled instance , ie , tm ui , in a similar fashion .
342 Transfer Instances After transferring the dictionaries , the label scarcity problem necessitates a much more powerful solution transferring abundant labelled instances from the source into the target domain . To enable instance transfer , the following two prerequisites have to be met first . 1 ) Learn enriched representations for labelled source instances . Mathematically , for the mth modality of the jth labelled source instance , ie , sm l j by performing sparse coding over the mth dictionary Dm : l j , we learn the enriched representation ˆsm − Dmˆsm
+ αˆsm
≥ 0 . sm st
( 9 )
2
1
ˆsm l j l j l j
F l j min ˆsm l j
Only in this way can the representation structures of labelled source instances , ie , ˆSl , stay consistent with those of target instances , ie , ˆTl and ˆTu . 2 ) Aggregate enriched representations of all existing modalities for each target instance . We adopt max pooling , widely applied in image processing , to aggregate . For the ith labelled target instance , max pooling maximizes each feature of the enriched representation over all existing modalities , ie ,
{ˆtm li ( k)} , k = 1 , 2,··· , K ,
ˆTMP l for all m=1,2,··· ,M
, similarly .
ˆtMP ( k ) = max ( 10 ) li li could be missing for some 1 ≤ m ≤ M . We obtain the where ˆtm aggregated representation for the ith unlabelled target instance , ie , ˆtMP In this case , we obtain a uniform representation ui Algorithm 2 Multimodal Transfer AdaBoost ( MTAB ) Input :
– enriched representations of labelled target instances ; ˆTMP – enriched representations of unlabelled target u instances ; ˆSl – enriched representations of labelled source instances ; y – the label vector in the target domain ; g – the label vector in the source domain h f – the final hypothesis for ˆTMP u
Output : 1 : Initialize the weight of the ith ( 1 ≤ i ≤ Nt l ) instance : vi(1 ) in 2 : Initialize the weight of the mth ( 1 ≤ m ≤ M ) modality of the 3 : for r = 1,··· , R do 4 : the target domain : vi(1 ) ; jth ( 1 ≤ j ≤ N s for m = 1,··· , M do l ) instance in the source domain : wm j ( 1 ) ;
Set pm i ( r ) =
⎧⎪⎪⎨⎪⎪⎩vi(r)/Bm(r ) , wm i−Nt N s Nt j=1 wm i=1 vi(r ) +
( r)/Bm(r ) , Nt l where Bm(r ) = j ( r ) . Train WeakLearner hm(r,· ) on [ ˆTMP l i ( r)}Nt pm(r ) = {pm
+N s l l i=1
; l l l
1 ≤ i ≤ Nt
, l
+ 1 ≤ i ≤ Nt
+ N s l
, l
; ˆSm l ] weighted by
Nt l
, l i=1
).yi ] vi(r ) maxM
: ε(r ) = end for m=1 I[hm(r,ˆtMP Define the error on ˆTMP li Nt l i=1 vi(r ) where I[a ] = 1 ifa is true and I[a ] = 0 otherwise ; Define the consistency of M weak learners on ˆTMP : l consistency(r ) = 1 − ).hm2 ( r,ˆtMP ) ] Set ( r ) = ε(r ) ∗ consistency(r ) ( ( r ) < 0.5 is compulsory ) ; Set β(r ) = ( r ) Update the weights : vi(r + 1 ) = vi(r)β(r)1−maxM j ( r)βI[hm(r,ˆsm j ( r + 1 ) = wm wm
Nt i=1 I[hm1 ( r,ˆtMP l li ×(M 2 )
2 ln N s l ).yi ] , 1 ≤ i ≤ Nt l ; m=1 I[hm(r,ˆtMP li l j).g j ] , 1 ≤ j ≤ N s l .
1− ( r ) and β = 1/(1 +
/R ) ;
M m1
M m2
Nt l
; li for all target instances regardless of within modality insufficiency . Besides , the representation is robust to unreliable modalities , since max pooling chooses the most responsive modality for each feature . Afterwards , we propose the Multimodal Transfer AdaBoost algorithm to leverage labelled source instances . The algorithm is based on TrAdaBoost [ 4 ] in terms of the basic idea , ie , reduce the distribution differences between domains by adjusting the weights of instances for training in an adaptively boosting fashion . Specifically , the weights of mis classified target instances increase to make sure that these instances draw enough attention to be classified right in the next iteration , while the mis classified source instances are down weighted because they are likely the most different in distribution from target instances . However , our algorithm differs from TrAdaBoost [ 4 ] in the following two aspects : 1 ) for each iteration it learns M weak learners to handle M modalities , and skilfully combines M learners’ results to boost the prediction accuracy ; 2 ) more importantly , it learns and differentiates weights for different modalities besides instances . Algorithm 2 details the whole algorithm . 3.5 Complexity Analysis The computational cost of the FLORAL method comprises two parts . 1 ) Learn semantically related dictionaries in O(M|Vm| log|Vm| +Mk|Vm| + c|V| + |V| log|V| ) , where c is a constant . The first two terms together are the cost of constructing the mutual k NN graph within each modality implemented by KD tree [ 1 ] , a space partition based approach . The third term is the cost to build interedges across different modalities . The last term corresponds to submodular graph clustering implemented by a max heap which stores marginal gains of all edges . Taking the full advantage of the diminishing marginal gains property , submodular clustering is highly computationally efficient by retrieving the top of the heap , re maximizing the heap , and updating the marginal gain of the top only . 2 ) Transfer dictionaries and instances in O(M(K + Z2)(N s + l Nt l ) ) , where Z is the number of non zeros in the l enriched representation . The first term is the cost to solve sparse coding in Equation ( 8 ) ( 9 ) with SPAMS1 , while Algorithm 2 runs in O(RM(N s l ) ) by training each weak learner with LIBLINl EAR2 . In conclusion , FLORAL scales linearly with the number of instances as well as the number of modalities . u ) +RM(N s l
+ Nt
+ Nt
+ Nt
4 . EXPERIMENTS
In this section , we evaluate the FLORAL method with the case study of air quality prediction . In the case study , FLORAL transfers knowledge from a source city , ie , Beijing , to improve accuracies of air quality prediction in three target cities , namely Shanghai , Tianjin and Baoding , which face either the label scarcity or the data insufficiency problem . 4.1 Datasets
We collected the following four data modalities in Beijing : 1 ) road networks from Bing Maps contain road segments each of which is described with its end points , length and level of capacity ; 2 ) Point Of Interests ( POI ) from Bing Maps indicate the name , address , coordinates , category of a venue ; 3 ) Meteorological data crawled from a public website every hour include weather , temperature , humidity , barometer pressure , wind strength , and etc ; 4 ) Taxi trajectories generated by over 32,000 taxicabs in Beijing from February 2nd to May 26th , 2014 . In the three target cities , however , only the first three modalities are available . Table 2 details the statistics of the first three modalities for all cities .
5 :
6 :
7 : 8 :
9 :
10 : 11 : 12 :
13 : end for 14 : h f ( ˆtMP ui ) = arg minc( ff R r=.R/2fi β(r )
− maxM m=1 I[hm(r,ˆtMP ui
).c] ) ;
1http://spams develgforgeinriafr/indexhtml 2https://wwwcsientuedutw/~cjlin/liblinear/
1910 Table 2 : The statistics of three modalities for all cities .
Modalities
Road
POI
# . Segments Highways
Roads
# . of POIs
Meteorology
Time span(2014 )
Cities
Beijing 249,080 994km
Shanghai 313,736 2,016km
Tianjin 97,258 1,681km
Baoding 69,383 795km
24,643km 40,944km 18,595km 17,884km 379,022 2/1 5/31
152,797 9/10 11/30
88,698 8/1 11/30
433,016 8/1 9/10
As air quality in a city varies with time and location simultaneously , we characterize a grid region in an hour of a day as an instance by partitioning each city into grid regions in the size of 15km×15km For each instance , we extract its features in all modalities . The feature construction for each modality follows [ 34 ] in which road network features Fr , POI features Fp , meteorological features Fm , and taxi traffic features Ft are extracted . Specifically , Fr and Fp are spatio features , and Fm and Ft are temporal features . Note that some modalities of some instances are not available , and the modality of taxi trajectories is missing for all instances of the three target cities . We label an instance with Air Quality Index ( AQI ) values which are collected from ground based air quality monitor stations in the four cities every hour . The AQI values range from one to six , corresponding to six air quality states , ie , “ Good ” , “ Moderate ” , “ Unhealthy for sensitive groups ” , “ Unhealthy ” , “ Very unhealthy ” , and “ Hazardous ” , respectively .
We measure the distributional difference in each modality between a source and a target domain with KL divergence . The larger the KL divergence is , the more different the feature distributions of a source and a target domain in a modality are . Table 3 and Figure 6 present the distributional differences between Beijing and the three target cities in the three shared modalities .
Table 3 : KL divergence in the distributions of road network and POI features .
Target cities
Modalities
Shanghai
Road POI
0.541 0.7618
Tianjin Baoding 0.7361 1.1439 1.1387 0.889
4.2 Baselines
Figure 6 : KL divergence in the distributions of meteorological features , differentiated by hours .
We compare our proposed method FLORAL with the following six baselines , evaluated by prediction accuracy : Original . This method trains a classifier for each modality in a target domain . Among all classifiers , this method selects the one with the best prediction accuracy . U Air . This model [ 34 ] combines different modalities by co training spatio and temporal features . LSRD . We learn semantically related dictionaries from a source domain by applying Algorithm 1 , transfer the dictionaries to enrich feature representations in a target domain according to Equation ( 8)(10 ) , and train classifiers on ˆTMP Orig+TAB . This method performs TrAdaBoost [ 4 ] , a state of theart algorithm that transfers instances , on each modality with original features , and outputs the best result among all modalities . LSRD+TAB . We perform TrAdaBoost on each modality with enriched features , and output the best result among all modalities . O
. l riginal features of both source and target domains are enriched by the semantically related dictionaries according to Equation ( 8)(9 ) . MDT . Multi view Discriminant Transfer learning ( MDT ) [ 25 ] transfers knowledge between domains with multiple views . We adapt MDT to solve our problem which faces the within modality insufficiency , by discarding those instances with modalities missing .
In summary , Original and U Air do not transfer . LSRD and Orig+TAB perform feature and instance transfer , respectively . LSRD+TAB directly combines feature and instance transfer . To make Orig+TAB and MDT applicable to our problem , we discard the modalities which are existing in a source but missing in a target domain . We use linear SVM as the base classifier . Given different feature representations for different models , the trade off parameter C of linear SVM is set according to 10 fold cross validation .
4.3 Results
Performance comparison : We differentiate the performance comparison by hours for the following two reasons : 1 ) distributions of temporal features for different hours , eg , traffic features in 0am and 8am , could be distinct ; 2 ) different numbers of instances are available in different hours . For each hour in a target domain , we first select an hour from a source so that transferring labelled instances in the hour maximizes the performance . Second , we randomly select 10 % of labelled instances as training data , and the rest as test . In Figure 7 , we report the average accuracy over ten such random partitions for each hour .
From Figure 7 , we have the following observations . First , combining different modalities outperforms using single modality only . Compared to Original , U Air unlocks the power of spatio and temporal features collectively in a co training fashion , and thereby partially addresses the label scarcity problem . Especially , our proposed LSRD algorithm is highly effective , since it addresses the data insufficiency problem in a target domain by enriching feature representations . Second , transferring source labelled instances is also critical to improve the performance . Even though we apply TradaBoost on each modality ’s original features individually , ie , Orig+TAB , we see the performance improvement . Third , performances of the multi view transfer learning algorithm MDT are not that satisfactory , probably because it fails to tackle the structured modality missing and within modality insufficiency . Fourth , directly combining feature and instance transfer ie , LSRD+TAB , still falls behind our method FLORAL . LSRD+TAB cannot learn and differentiate different modalities’ weights as FLORAL does . Generally speaking , FLORAL outperforms all the baselines in almost all hours of all target cities up to 50 % .
The improvement of FLORAL over other baselines achieves the most significant when transferring from Beijing to Tianjin according to Figure 7(b ) ; transferring to Baoding takes second while transferring to Shanghai ranks third . Table 3 , Figure 2(b ) , and Figure 6 provide the explanations . The KL divergence values between Tianjin and Beijing are averagely small for all the three modalities , ie , road , POI , and meteorology . The distribution of labels , ie , air quality , in Tianjin is also similar to that in Beijing . However , the feature distributions of Baoding in road and POI largely differ from those of Beijing , considering that Baoding is a small city . In this case , the meteorology which is similar for the two geographically close cities primarily accounts for the transfer . Figure 9 further confirms the fact : the smaller the KL divergence between Baoding and Beijing in meteorology , the better FLORAL performs .
Figure 10 shows the correspondence between each hour in Baoding and the hour in Beijing selected by FLORAL . To maximize the prediction accuracy , it is expected that the hour selected from a source is the most similar to each hour in a target domain in dis
1911 y c a r u c c a y c a r u c c a y c a r u c c a
0.7 0.6 0.5 0.4 0.3 0
0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
0.5 0.4 0.3 0.2 0.1 0
0.5 0.45
0.35 y c a r u c c a
Original
U−Air
LSRD
Orig+TAB
LSRD+TAB
MDT
FLORAL
1
2
3
4
5
6
7
8
9
10
11
13
12 hour
14
15
16
17
18
19
20
21
22
23
24
25
( a ) Hourly air quality prediction accuracies in Shanghai .
Original
U−Air
LSRD
Orig+TAB
LSRD+TAB
MDT
FLORAL
1
2
3
4
5
6
7
8
9
10
11
13
12 hour
14
15
16
17
18
19
20
21
22
23
24
25
( b ) Hourly air quality prediction accuracies in Tianjin .
Original
U−Air
LSRD
Orig+TAB
LSRD+TAB
MDT
FLORAL
1
2
3
4
5
6
7
8
9
10
11
13
12 hour
14
15
16
17
18
19
20
21
22
23
24
25
( c ) Hourly air quality prediction accuracies in Baoding .
Figure 7 : Performance comparison of hourly air quality prediction in different target cities .
Baoding Beijing
25
20
15
10
5 r u o h
0.25 0
5
20
10 15 hour
25 Figure 9 : Hourly air quality prediction accuracies in Baoding , with the boxes denoting the scaled KLdivergence values between Baoding and Beijing in meteorology .
0 0
5
20
15
10 hour
25 Figure 10 : The correspondence between each hour in Baoding and the hour in Beijing selected by FLORAL . tributions . Consequently , we conclude that during 5am 9am and 13pm 17pm Beijing is the most synchronously similar to Baoding . Effectiveness of semantically related dictionaries : The success of FLORAL highly depends on the quality of semantically related dictionaries learnt by LSRD . In Figure 11 , we examine and visualize the dictionary learnt from Beijing during 11am 12pm for the modality of meteorology with the size K = 10 . Each dictionary atom is labelled as the mostly assigned label in the cluster which we infer the atom from . The label of an atom is regarded as the latent semantic meaning it encodes . The figure tells that the semantic meanings do make a lot of sense , and thereby the learnt dictionary is effective . For example , as the level of humidity increases and the wind speed reduces , the labels of dictionary atoms tend to increase , meaning that the air quality gets worse . It is noted that the rainfall stays unchanged across all atoms , because there is a lack of rain in Beijing and exists seldom raining days in our training data .
Figure 11 : The meteorology dictionary learnt from Beijing during 11am 12pm . The x axis denotes the features while the y axis labels a dictionary atom with the mostly assigned label in the cluster .
Dealing with the label scarcity and data insufficiency problems : In Figure 8 , we verify that FLORAL is capable of dealing with the label scarcity and data insufficiency problems . We focus on the performance of air quality prediction in Tianjin during 17pm 18pm . First , we vary the percentage of labelled instances for training in the target domain , ie , Tianjin . The smaller the percentage is , the scarcer the labelled data are . Figure 8(a ) shows that when the percentage of training data increases , all algorithms perform better . Especially , when the labelled data are very scarce , say the percentage equals to 0.1 , FLORAL even improves the most over the baselines . Thus we conclude that FLORAL can successfully handle the label scarcity problem , and that is why we select 10 % of labelled instances as training data for performance comparison . Second , we compare different algorithms’ capabilities to tackle the structured modality missing in Figure 8(b ) . We vary available modalities in Tianjin , ranging from single modality to three modalities together . Figure 8(b ) shows that the perfor
1912 0.8
0.7
0.6
0.5
0.4 y c a r u c c a
0.1
0.3 percentage of training data
0.5
0.7
0.9 meterology 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 road poi meteorology &poi&road poi
&road meteorology
&poi meteorology
&road
Original U Air LSRD Orig+TAB LSRD+TAB MDT FLORAL
( a ) Varying the percentage of labelled target instances .
( b ) Varying available modalities in the target domain .
0.6
0.5
0.4
0.3 y c a r u c c a
0.1 0.2 0.3 0.4 0.5
0.2 0 the random dropping percentage ( c ) Varying the percentage of random dropping in each modality .
Original U−Air LSRD Orig+TAB LSRD+TAB MDT FLORAL
Figure 8 : Dealing with the label scarcity and data insufficiency problems . mance gap between FLORAL and the baselines based on LSRD , ie , LSRD and LSRD+TAB , stays consistent , while the gap between FLORAL and the other baselines increases as more modalities are missing . Therefore we prove that learning semantically related dictionaries fully takes the advantage of the modalities which are missing in a target domain but existing in a source , and thereby effectively addresses the structured modality missing . Note that U Air cannot handle the cases where only spatio or temporal features are available , and MDT is not applicable in the cases where only single modality is provided . Third , we investigate the capabilities of all algorithms to deal with the within modality insufficiency in Figure 8(c ) , by randomly dropping a percentage of data for each modality . Reasonably , as the dropping percentage increases , the performances of all algorithms decrease . However , the performances of FLORAL and the baselines based on LSRD decrease much slower than those of the other baselines . The semantically related dictionaries complement the within modality insufficiency by enriching feature representations .
Learning and differentiating different modalities’ weights : The major reason why FLORAL wins over LSRD+TAB is that FLORAL has the ability to learn and differentiate different modalities’ weights when transferring , which is further validated in Figure 12 . No matter which target city FLORAL transfers to , the distribution of labelled source instances’ weights in meteorology significantly differs from that in traffic . Besides , for each modality , y t i l i b a b o r p
0.8
0.6
0.4
0.2
0
Shanghai Tianjin Baoding
0−0.1
01−09 bins
0.9−1
( a ) in meteorology y t i l i b a b o r p
0.8
0.6
0.4
0.2
0
Shanghai Tianjin Baoding
0−0.1
01−09 bins ( b ) in traffic
0.9−1
Figure 12 : Comparison of the distributions of labelled source instances’ weights in two modalities when transferring to different target cities to predict air quality during 17pm 18pm . the distributions of labelled source instances’ weights differ for different target cities . Specifically , the modality of meteorology plays the most important role when transferring from Beijing to Tianjing because the weights are the most likely to lie in 0.9 − 1 . The geographical closeness of the two cities explains this . However , the modality of traffic is weighted the highest while transferring from Beijing to Shanghai , the two of which are top two cities in China . We would also clarify why the modality of meteorology seems more important than the modality of traffic for all target cities . It is because the modality of traffic is missing in all target cities so that the meteorology is more likely to dominate .
Varying the percentage of labelled source instances : The performances of FLORAL also rely on the amount of labelled instances we transfer from a source domain . Figure 13 presents the performances of FLORAL in predicting air quality in Tianjin during 17pm 18pm , while we vary the percentage of labelled instances in the target domain , ie , rt , and that in the source , ie , rs , simultaneously . Reasonably , larger rt and rs lead to better performances . Besides , when rs = 0.6 , the performances of FLORAL start to saturate , meaning that 60 % of the labelled source instances have been sufficient to improve the target domain .
Figure 13 : Varying the percentage of labelled instances in the target and source domain simultaneously .
Parameter sensitivity : We also study the influence of different parameter settings on the performances of FLORAL when transferring from Beijing to Tianjin during 17pm 18pm . We investigate three parameters : K , the size of semantically related dictionaries , λ and γ , the trade off parameters’ initialization in Equation ( 7 ) . For space limitation , we do not include the result for μ , the other trade off parameter ’s initialization . We perform grid search on λ and γ in the range of {10−3 , 10−2 , 10−1 , 100 , 101 , 102 , 103} by fixing the dictionary size K . FLORAL gains the best accuracy at λ = 100 and γ = 10 as Figure 14(a ) shows . In Figure 14(b ) , by fixing λ = 100 and γ = 10 , we obtain the best dictionary size K = 500 .
0.6
0.55
0.5
0.45
0.4 y c a r u c c a
200 400 600 800 1000 dictionary size
( a ) Grid search of λ and γ .
( b ) Varying the dictionary size . Figure 14 : Study of parameter sensitivity on air quality prediction .
1913 Scalability : We evaluate the scalability of our LSRD algorithm , which is the major computational bottleneck of FLORAL . By using KD tree for graph construction and submodular optimization for graph clustering , LSRD is highly efficient and capable of handling extremely large graphs involving massive vertices and hyper edges as Figure 15 shows .
) n l ( e m i i t g n n r a e l y r a n o i t c d i
10
8
6
4
2 0
5
# of vertices in the graph
15 x 104 Figure 15 : Scalability of FLORAL .
10
5 . CONCLUSIONS
In this paper , we propose a novel method called FLORAL to transfer knowledge between domains with multimodal data . Particularly , FLORAL enriches feature representations in a target domain with semantically related dictionaries learnt from a source , and transfers labelled instances from the source . Extensive experimental results in the case study of air quality prediction demonstrate the superiority of FLORAL over other state of the art methods . Besides air quality prediction , FLORAL could be applied whenever the target domain encounters the label scarcity and data insufficiency problems . In the future , we would like to extend FLORAL to transfer from multiple source domains . Although finding a source domain which contains all modalities in a target is not that difficult , FLORAL can be more flexible by transferring from multiple source domains .
6 . ACKNOWLEDGMENTS
We thank the reviewers for their valuable comments to improve this paper . We also thank the support of China National 973 project 2014CB340304 , and Hong Kong CERG projects 16211214 and 16209715 .
7 . REFERENCES [ 1 ] J . L . Bentley . Multidimensional binary search trees used for associative searching . Communications of the ACM , 18(9):509–517 , 1975 .
[ 2 ] J . Blitzer , S . Kakade , and D . P . Foster . Domain adaptation with coupled subspaces . In AISTATS , pages 173–181 , 2011 .
[ 3 ] T . M . Cover and J . A . Thomas . Elements of information theory . 2012 . [ 4 ] W . Dai , Q . Yang , G R Xue , and Y . Yu . Boosting for transfer learning . In ICML , pages 193–200 , 2007 .
[ 5 ] Z . Fang and Z . M . Zhang . Discriminative feature selection for multi view cross domain learning . In CIKM , pages 1321–1330 , 2013 .
[ 6 ] D . R . Hardoon , S . Szedmak , and J . Shawe Taylor . Canonical correlation analysis : An overview with application to learning methods . Neural computation , 16(12):2639–2664 , 2004 .
[ 7 ] J . He and R . Lawrence . A graph based framework for multi task multi view learning . In ICML , pages 25–32 , 2011 .
[ 8 ] Z . Jiang , G . Zhang , and L . S . Davis . Submodular dictionary learning for sparse coding . In CVPR , pages 3418–3425 , 2012 .
[ 9 ] X . Jin , F . Zhuang , H . Xiong , C . Du , P . Luo , and Q . He . Multi task multi view learning for heterogeneous tasks . In CIKM , pages 441–450 , 2014 .
[ 10 ] R . Kiros , R . Salakhutdinov , and R . Zemel . Multimodal neural language models . In ICML , pages 595–603 , 2014 .
[ 11 ] M Y Liu , O . Tuzel , S . Ramalingam , and R . Chellappa . Entropy rate clustering : Cluster analysis via maximizing a submodular function subject to a matroid constraint . PAMI , 36(1):99–112 , 2014 .
[ 12 ] S . T . McCormick . Submodular function minimization . Handbooks in operations research and management science , 12:321–391 , 2005 . [ 13 ] G . L . Nemhauser , L . A . Wolsey , and M . L . Fisher . An analysis of approximations for maximizing submodular set functions . Mathematical Programming , 14(1):265–294 , 1978 .
[ 14 ] L . Nie , L . Zhang , Y . Yang , M . Wang , R . Hong , and T S Chua . Beyond doctors : future health prediction from multimedia and multimodal observations . In MM , pages 591–600 , 2015 .
[ 15 ] K . Ozaki , M . Shimbo , M . Komachi , and Y . Matsumoto . Using the mutual k nearest neighbor graphs for semi supervised classification of natural language data . In CoNLL , pages 154–162 , 2011 .
[ 16 ] S . J . Pan and Q . Yang . A survey on transfer learning . Knowledge and Data Engineering , IEEE Transactions on , 22(10):1345–1359 , 2010 .
[ 17 ] X . Shi , Q . Liu , W . Fan , and P . S . Yu . Transfer across completely different feature spaces via spectral embedding . Knowledge and Data Engineering , IEEE Transactions on , 25(4):906–918 , 2013 .
[ 18 ] X . Song , L . Nie , L . Zhang , M . Liu , and T S Chua . Interest inference via structure constrained multi source multi task learning . In IJCAI , pages 2371–2377 , 2015 .
[ 19 ] N . Srivastava and R . R . Salakhutdinov . Multimodal learning with deep boltzmann machines . In NIPS , pages 2222–2230 , 2012 .
[ 20 ] B . Tan , E . Zhong , E . W . Xiang , and Q . Yang . Multi transfer : Transfer learning with multiple views and multiple sources . In SDM , 2013 .
[ 21 ] Y . Wei , Y . Song , Y . Zhen , B . Liu , and Q . Yang . Scalable heterogeneous translated hashing . In SIGKDD , pages 791–800 , 2014 .
[ 22 ] Y . Wei , Y . Zhu , C . W k Leung , Y . Song , and Q . Yang . Instilling social to physical : Co regularized heterogeneous transfer learning . In AAAI , 2016 .
[ 23 ] D . Yang , D . Zhang , Z . Yu , and Z . Yu . Fine grained preference aware location search leveraging crowdsourced digital footprints from LBSNs . In UbiComp , pages 479–488 , 2013 .
[ 24 ] H . Yang and J . He . Learning with dual heterogeneity : a nonparametric bayes model . In SIGKDD , pages 582–590 , 2014 .
[ 25 ] P . Yang and W . Gao . Multi view discriminant transfer learning . In
IJCAI , pages 1848–1854 , 2013 .
[ 26 ] P . Yang , W . Gao , Q . Tan , and K F Wong . Information theoretic multi view domain adaptation . In ACL , pages 270–274 , 2012 .
[ 27 ] P . Yang and J . He . Model Multiple Heterogeneity via Hierarchical
Multi Latent Space Learning . In SIGKDD , pages 1375–1384 , 2015 .
[ 28 ] Q . Yang , Y . Chen , G R Xue , W . Dai , and Y . Yu . Heterogeneous transfer learning for image clustering via the social web . In ACL , pages 1–9 , 2009 .
[ 29 ] Z . Yu , F . Wu , Y . Yang , Q . Tian , J . Luo , and Y . Zhuang .
Discriminative coupled dictionary hashing for fast cross media retrieval . In SIGIR , pages 395–404 , 2014 .
[ 30 ] D . Zhang , J . He , Y . Liu , L . Si , and R . Lawrence . Multi view transfer learning with a large margin approach . In SIGKDD , pages 1208–1216 , 2011 .
[ 31 ] J . Zhang and J . Huan . Inductive multi task learning with multiple view data . In SIGKDD , pages 543–551 , 2012 .
[ 32 ] Z . Y . Zhao , M . Xie , and M . West . Dynamic dependence networks : Financial time series forecasting and portfolio decisions . Applied Stochastic Models in Business and Industry , 2016 .
[ 33 ] Y . Zheng , L . Capra , O . Wolfson , and H . Yang . Urban computing : concepts , methodologies , and applications . ACM Transactions on Intelligent Systems and Technology ( TIST ) , 5(3):38 , 2014 .
[ 34 ] Y . Zheng , F . Liu , and H P Hsieh . U air : When urban air quality inference meets big data . In SIGKDD , pages 1436–1444 , 2013 .
[ 35 ] Y . Zheng , T . Liu , Y . Wang , Y . Zhu , Y . Liu , and E . Chang . Diagnosing new york city ’s noises with ubiquitous data . In UbiComp , pages 715–725 , 2014 .
[ 36 ] Y . Zheng , X . Yi , M . Li , R . Li , Z . Shan , E . Chang , and T . Li . Forecasting Fine Grained Air Quality Based on Big Data . In SIGKDD , pages 2267–2276 , 2015 .
1914
