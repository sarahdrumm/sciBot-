An Efficient Approach for Predictive Pattern Mining
Safe Pattern Pruning :
Kazuya Nakagawa
Shinya Suzumura
Nagoya Institute of Technology
Nagoya Institute of Technology
Nagoya , Japan nakagawakmllabnit
@gmail.com
Nagoya , Japan suzumuramllabnit
@gmail.com
Masayuki Karasuyama
Nagoya Institute of Technology
Nagoya , Japan karasuyama@nitechacjp
Koji Tsuda
University of Tokyo
Tokyo , Japan tsuda@ku tokyoacjp
Ichiro Takeuchi
Nagoya Institute of Technology
Nagoya , Japan takeuchiichiro@nitechacjp
∗
ABSTRACT In this paper we study predictive pattern mining problems where the goal is to construct a predictive model based on a subset of predictive patterns in the database . Our main contribution is to introduce a novel method called safe pattern pruning ( SPP ) for a class of predictive pattern mining problems . The SPP method allows us to efficiently find a superset of all the predictive patterns in the database that are needed for the optimal predictive model . The advantage of the SPP method over existing boosting type method is that the former can find the superset by a single search over the database , while the latter requires multiple searches . The SPP method is inspired by recent development of safe feature screening . In order to extend the idea of safe feature screening into predictive pattern mining , we derive a novel pruning rule called safe pattern pruning ( SPP ) rule that can be used for searching over the tree defined among patterns in the database . The SPP rule has a property that , if a node corresponding to a pattern in the database is pruned out by the SPP rule , then it is guaranteed that all the patterns corresponding to its descendant nodes are never needed for the optimal predictive model . We apply the SPP method to graph mining and item set mining problems , and demonstrate its computational advantage .
Keywords Predictive pattern mining , Graph mining , Item set mining , Sparse learning , Safe screening , Convex optimization
∗Corresponding author
1 .
INTRODUCTION
In this paper , we study predictive pattern mining . The goal of predictive pattern mining is discovering a set of patterns from databases that are needed for constructing a good predictive model . Predictive pattern mining problems can be interpreted as feature selection problems in supervised machine learning tasks such as classifications and regressions . The main difference between predictive pattern mining and ordinal feature selection is that , in the former , the number of possible patterns in databases are extremely large , meaning that we cannot naively search over all the patterns in databases . We thus need to develop algorithms that can exploit some structures among patterns such as trees or graphs for efficiently discovering good predictive patterns .
To be concrete , suppose that there are D patterns in a database , which is assumed to be extremely large . For the i th transaction in the database , let zi1 , . . . , ziD ∈ {0 , 1} represent the occurrence of each pattern . We consider linear predictive model in the form of wjzij + b , j∈A
( 1 ) where A ⊆ {1 , . . . , D} is a set of patterns that would be selected by a mining algorithm , and {wj}j∈A and b are the parameters of the linear predictive model . Here , the goal is to select a set of predictive patterns in A and find the model parameters {wj}j∈A and b so that the predictive model in the form of ( 1 ) has good predictive ability .
Existing predictive pattern mining studies can be categorized into two approaches . The first approach is two stage approach , where a mining algorithm is used for selecting the set of patterns A in the first stage , and the predictive model is fitted by only using the selected patterns in A in the second stage . Two stage approach is computationally efficient because the mining algorithm is run only once in the first stage . However , two stage approach is suboptimal as predictive model building procedure because it does not directly optimize the predictive model . The second approach is direct approach , where a mining algorithm is integrated in a feature selection method . An advantage of direct approach is that a set of patterns that are useful for predictive modeling is directly searched for . However , the computational cost of
1785 existing direct approach is usually much greater than twostage approach because the mining algorithm is run multiple times . For example , in a stepwise feature selection method , the mining algorithm is run at each step in order to find the pattern that best improves the current predictive model .
In this paper , we study a direct approach for predictive pattern mining based on sparse modeling . In the literature of machine learning and statistics , sparse modeling has been intensively studied in the past two decades . An advantage of sparse modeling is that the problem is formulated as a convex optimization problem , and it allows us to investigate several properties of solutions from a wide variety of perspectives . In addition , many efficient solvers that can be applicable to high dimensional problems ( although not as high as the number of patterns in databases as we consider in this paper ) have been developed .
Predictive pattern mining algorithms based on sparse modeling have been also studied in the literature [ 12 , 14 , 13 ] . All these studies rely on a technique developed in the context of boosting [ 4 ] . Roughly speaking , in each step of the boostingtype method , a feature is selected based on a certain criteria , and an optimization problem defined over the set of features selected so far is solved . Therefore , when the boosting type method is used for predictive pattern mining tasks , one has to search over the database as many times as the number of steps in the boosting type method .
Our main contribution in this paper is to propose a novel method for sparse modeling based predictive pattern mining . Denoting the set of patterns that would be used in the optimal predictive model as A∗ , the proposed method can find a set of patterns ˆA ⊇ A∗ , ie , ˆA contains all the predictive patterns that are needed for the optimal predictive model . It means that , if we solve the sparse modeling problem defined over the set of patterns ˆA , then it is guaranteed that the resulting predictive model is optimal . The main advantage of the proposed method over the above boostingtype method is that a mining algorithm is run only once for finding the set of patterns ˆA .
The proposed method is inspired by recent safe feature screening studies [ 5 , 20 , 18 , 1 , 9 , 17 , 19 , 6 , 10 ] . In ordinary feature selection problems , safe feature screening allows us to identify a set of features that would never be used in the optimal model before actually solving the optimization problem It means that these features can be safely removed from the training set . Unfortunately , however , it cannot be applied to predictive pattern mining problems because it is computationally intractable to apply safe feature screening to each of extremely large number of patterns in a database for checking whether the pattern can be safely removed out or not .
In this paper , we develop a novel method called safe pattern pruning ( SPP ) . Considering a tree structure defined among patterns in the database , the SPP method allows us to prune the tree in such a way that , if a node corresponding to a pattern in the database is pruned out , then it is guaranteed that all the patterns corresponding to its descendant nodes would never be needed for the optimal predictive model . The SPP method can be effectively used in predictive pattern mining problems because we can identify an extremely large set of patterns that are irrelevant to the optimal predictive model by exploiting the tree structure among patterns in the database , A superset ˆA ⊇ A∗ can be obtained by collecting the set of patterns corresponding to the nodes that are not pruned out by the SPP method . 1.1 Notation and outline We use the following notations in the rest of the paper . For any natural number n , we define [ n ] := {1 , . . . , n} . For an n dimensional vector v and a set I ⊆ [ n ] , vI represents a sub vector of v whose elements are indexed by I . The indicator function is written as I(· ) , ie , I(z ) = 1 if z is true , and I(z ) = 0 otherwise . Boldface 0 and 1 indicate a vector of all zeros and ones , respectively . Here is the outline of the paper . §2 presents problem setup and existing methods . §3 describes our main contribution where we introduce safe pattern pruning ( SPP ) method . §4 covers numerical experiments for demonstrating the advantage of the SPP method . §5 concludes the paper .
2 . PRELIMINARIES
We first formulate our problem setting .
2.1 Problem setup
In this paper we consider predictive pattern mining problems . Let us consider a database with n records , and denote the dataset as {(Gi , yi)}i∈[n ] , where Gi is a labeled undirected graph in the case of graph mining , while it is a set of items in the case of item set mining . The response variable yi is defined on R and on {±1} for regression and classification problems , respectively . Let T be the set of all patterns in the database , and denote its size as D := |T | . For example , T is the set of all possible subgraphs in the case of graph mining , while T is the set of all possible item sets in the case of item set mining . Alternatively , Gi is represented as a D dimensional binary vector xi ∈ {0 , 1}D whose t th element is defined as xit := I(t ⊆ Gi ) , ∀t ∈ T .
The number of patterns D is extremely large in all practical pattern mining problems . It implies that any algorithms that naively search over all D patterns are computationally infeasible .
In order to study both regression and classification problems in a unified framework , we consider the following class of convex optimization problems : min w,b
Pλ(w , b ) := i w + βib + γi ) + λw1 , f ( α
( 2 ) i∈[n ] i∈[n ] where f : R → R is a gradient Lipschitz continuous loss function and λ > 0 is a tuning parameter . We refer the problem ( 2 ) as primal problem and write the optimal solution as w∗ . When f ( z ) := 1 2 z2 and αi := xi , βi := 1 , γi := −yi ∀i ∈ [ n ] , the general problem ( 2 ) is reduced to the following L1 penalized regression problem defined over D + 1 variables : min w∈RD ,b∈R
1 2 i w + b − yi)2 + λw1 .
( 3 )
( x
2 max{0 , 1−z}2 and αi := On the other hand , when f ( z ) := 1 yixi , βi := yi , γi := 0 ∀i ∈ [ n ] , the general problem ( 2 ) is reduced to the following L1 penalized classification problem
1786 ( a ) Graph mining
( b ) Item set mining
Figure 1 : Two examples of tree structures defined among patterns in databases . defined over D + 1 variables : min w∈RD ,b∈R
1 2 max{0 , 1 − yi(x i w + b)}2 + λw1 . ( 4 ) i∈[n ]
Remembering that D is extremely large , we cannot solve these L1 penalized regression and classification problems in a standard way .
The dual problem of ( 2 ) is defined as θ2
Dλ(θ ) := − λ2 2 max θ∈Rn fifififififi
θ
2 + λδ fifififififi ≤ 1,∀t ∈ T , st
αitθi i∈[n ] θ = 0 , θi ≥ ε , ∀i ∈ [ n ] ,
β
( 5 ) where δ = y , ε = −∞ for regression problem in ( 3 ) , and δ = 1 , ε = 0 for classification problem in ( 4 ) . The dual optimal solution is denoted as θ∗ .
The key idea for handling an extremely large number of patterns in the database is to exploit the tree structure defined among the patterns . Figure 1 shows tree structures for graph mining ( left ) and item set mining ( right ) . As shown in Figure 1 , each node of the tree corresponds to each pattern in the database . Those trees are constructed in such a way that , for any pair of a node t and one of its descendant node t , they satisfy the relation t ⊆ t , ie , the pattern t is a superset of the pattern t . It suggests that , for such a pair of t and t , and , conversely xit = 1 ⇒ xit = 1 ∀i , xit = 0 ⇒ xit = 0 ∀i .
2.2 Existing method To the best of our knowledge , except for the boostingtype method described in §1 and its extensions or modifications [ 12 , 14 , 13 ] , there is no other existing method that can be used for solving the convex optimization problem ( 2 ) for predictive pattern mining problems defined over an extremely large number of patterns D . The boosting type method solves the dual problem ( 5 ) . The difficulty in the dual problem is that there are extremely large number of i∈[n ] αitθi| ≤ 1,∀t ∈ T . Starting from the optimization problem ( 5 ) without these constraints , in each step of the boosting type method , the most violating constraint is added to the problem , and an optimization problem only with the constraints added so far is constraints in the form of | solved . In optimization literature , this approach is generally known as the cutting plane method , for which its effectiveness has been also shown in some machine learning problems ( eg , [ 7] ) . The key computational trick used by [ 12 , 14 , 13 ] is that , for finding the most violating constraint in each step , it is possible to efficiently search over the database by using a certain pruning strategy in the tree as depicted in Figure 1 . This method is terminated when there is no violating constraints in the database .
In each single step of the boosting type method , one first has to search over the database by a mining algorithm , and then run a convex optimization solver for the problem with the newly added constraint . Boosting type method is computationally expensive because these steps must be repeated until all the constraints corresponding to all the predictive patterns in A∗ are added . In the next section , we propose a novel method called safe pattern pruning , by which the optimal model is obtained by a single search over the database and a single run of convex optimization solver .
3 . SAFE PATTERN PRUNING
In this section , we present our main contribution .
3.1 Basic idea
It is well known that L1 penalization in ( 2 ) makes the solution w∗ sparse , ie , some of its elements would be zero . The set of patterns which has non zero coefficients are called active and denoted as A∗ ⊆ T , while the rest of the patterns are called non active . A nice property of sparse learning is that the optimal solution does not depend on any non active patterns . It means that , after some non active patterns are removed out from the dataset , the same optimal solution can be obtained . The following lemma formally states this well known but important fact .
Lemma 1 . Let ˆA be a set such that A∗ ⊆ ˆA ⊆ T , and P ˆA λ ( w ˆA , b ) be the objective function of ( 2 ) in which wT \ ˆA = 0 is substituted :
ˆA λ ( w ˆA , b ) :=
P f ( α
ˆA,iw ˆA + βib + γi ) + λw ˆA1 .
( 6 ) i∈[n ]
Then , the optimal solution of the original problem ( 2 ) is given by
( w
∗ ˆA , b ) = arg ∗ T \ ˆA = 0 . w min w ˆA∈R| ˆA|,b∈R
ˆA λ ( w ˆA , b ) ,
P
{1} {2} {3} {1,2} {1,3} {3,4} { } {4} 1787 Lemma 1 indicates that , if we have a set of patterns ˆA ⊇ A∗ , we have only to solve a smaller optimization problem defined only with the set of patterns in ˆA . It means that , if such an ˆA is available , we do not have to work with extremely large number of patterns in the database . In the rest of this section , we propose a novel method for finding such a set of patterns ˆA ⊇ A∗ by searching over the database only once . Specifically , we derive a novel pruning condition which has a property that , if the condition is satisfied at a certain node , then all the patterns corresponding to its descendant nodes and the node itself are guaranteed to be non active . After traversing the tree , we simply define ˆA be the set of nodes which are not pruned out . Then , it is guaranteed that ˆA satisfies the condition in Lemma 1 . The proposed method is inspired by recent studies on safe feature screening . We thus call our new method as safe pattern pruning ( SPP ) . 3.2 Main theorem for safe pattern pruning
The following theorem provides a specific pruning condition that can be used together with any search strategies on a tree . Let Tsub(t ) ⊆ T be a set of nodes in a subtree of T having t as a root node and containing all descendant nodes of t . We derive a condition for safely screening the entire Tsub(t ) out , which is computable at the node t without traversing the descendant nodes . This means that , our rule , called safe pattern pruning rule , tells us whether a pattern t ∈ Tsub(t ) has a chance to be active or not based on the information available at the root node of the subtree t . An important consequence of the condition below is that if the condition holds , ie , any t ∈ Tsub(t ) cannot be active , then we can stop searching over the subtree ( pruning the subtree ) .
Theorem 2
( Safe pattern pruning ( SPP ) rule ) .
Given an arbitrary primal feasible solution ( ˜w , ˜b ) and an arbitrary dual feasible solution ˜θ , for any node t ∈ Tsub(t ) , the following safe pattern pruning criterion ( SPPC ) provides a rule
SPPC(t ) := ut + rλ
√ vt < 1 ⇒ w
∗ t = 0 ,
 , vt := i∈[n ]
α2 it ,
αit ˜θi where ut := max
 i:βi
˜θi>0 for t ∈ [ D ] , and
αit ˜θi , − i:βi
˜θi<0 rλ :=
2(Pλ( ˜w , ˜b ) − Dλ( ˜θ ) )
.
λ
The proof of Theorem 2 is presented in §33
SPPC(t ) depends on three scalar quantities ut , vt and rλ . The first two quantities ut and vt are obtained by using information on the pattern t , while the third quantity rλ does not depend on t . Noting that all these three quantities are non negative , the SPP rule would be more powerful ( have more chance to prune the subtree ) if these three quantities are smaller . The following corollary is the consequence of the simple fact that the first two quantities ut and vt at a descendant node are smaller than those at its ancestor nodes .
Corollary 3 . For any node t ∈ Tsub(t ) ,
SPPC(t ) ≥ SPPC(t
)
The proof of Corollary 3 is presented in Appendix . This corollary suggests that the SPP rule would be more powerful at deeper nodes .
The third quantity rλ represents the goodness of the pair of primal and dual feasible solutions measured by the duality gap , the difference between the primal and dual objective values . It means that , if sufficiently good pair of primal and dual feasible solutions are available , the SPP rule would be powerful . We will discuss how to obtain good feasible solutions in §34 3.3 Proof of Theorem 2 In order to prove Theorem 2 , we first clarify the condition for any pattern t ∈ T to be non active by the following lemma .
Lemma 4 . For a pattern t ∈ T , fifififififi < 1 ⇒ w dicates that , if an upper bound of | | In order to derive an upper bound of |
Proof of Lemma 4 is presented in Appendix . Lemma 4 ini | is smaller i∈[n ] αitθ∗ than 1 , then we can guarantee that w∗ t = 0 . In what follows , we actually show that SPPC(t ) is an upper bound of i | , we use a technique developed in a recent safe feature screening study [ 10 ] . The following lemma states that , based on a pair of a primal feasible solution ( ˜w , ˜b ) and a dual feasible solution ˜θ , we can find a ball in the dual solution space Rn in which the dual optimal solution θ∗ exists . i | for ∀t ∈ Tsub(t ) . i∈[n ] αit θ∗ i∈[n ] αitθ∗
∗ t = 0 . fifififififi
∗ i
αitθ i∈[n ]
Lemma 5
( Theorem 3 in [ 10] ) . Let ( ˜w , ˜b ) be an arbitrary primal feasible solution , and ˜θ be an arbitrary dual feasible solution . Then , the dual optimal solution θ∗ is within a ball in the dual solution space Rn with the center ˜θ and the radius rλ :=
2(Pλ( ˜w , ˜b ) − Dλ( ˜θ))/λ .
|
See Theorem 3 and its proof in [ 10 ] . This lemma tells that , given a pair of primal feasible and dual feasible solutions , we can bound the dual optimal solution within a ball .
Lemma 5 can be used for deriving an upper bound of i | . Since we know that the dual optimal solui∈[n ] αitθ∗ tion θ∗ is within the ball in Lemma 5 , an upper bound of any t ∈ T can be obtained by solving the following convex optimization problem : fifififififi fifififififi flflfl2 flflflθ − ˜θ i∈[n ]
αitθi
β
UB(t ) := arg max θ∈Rn st
2(Pλ( ˜w , ˜b ) − Dλ( ˜θ))/λ ,
≤
θ = 0 .
( 7 )
Fortunately , the convex optimization problem ( 7 ) can be explicitly solved as the following lemma states .
1788 Lemma 6 . The solution of the convex optimization prob lem ( 7 ) is given as fifififififi i∈[n ] fifififififi + rλ i∈[n ] it − (
α2
UB(t ) =
αit ˜θi i∈[n ] αitβi)2 β2
2
.
Proof of Lemma 6 is presented in Appendix . Although UB(t ) provides a condition to screen any t ∈ T , calculating UB(t ) for all t ∈ T is computationally prohibiting in our extremely high dimensional problem setting . In the next lemma , we will show that SPPC(t ) ≥ UB(t ) for ∀t ∈ Tsub(t ) , ie , SPPC(t ) in Theorem 2 is an upper bound of UB(t ) , which enables us to efficiently prune subtrees during the tree traverse process .
Lemma 7 . For any t ∈ Tsub(t ) ,
UB(t
) =
αit ˜θi fifififififi i∈[n ] fifififififi + rλ it − (
α2 i∈[n ] ≤ ut + rλ
√ i∈[n ] αit βi)2
β2
2 vt = SPPC(t ) .
Finally , by combining Lemmas 4 , 5 , 6 and 7 , we can prove
Theorem 2 . Proof of Theorem 2 .
Proof . From Lemmas 5 , 6 and 7 , fifififififi i∈[n ] fifififififi ≤ UB(t
αit θ
∗ i
) ≤ SPPC(t ) ,
∀t
∈ Tsub(t ) .
( 8 )
From Lemma 4 and ( 8 ) ,
SPPC(t ) < 1 ⇒ w
∗ t = 0 ,
∀t
∈ Tsub(t ) .
3.4 Practical considerations
Safe pattern pruning rule in Theorem 2 depends on a pair of a primal feasible solution ( ˜w , ˜b ) and a dual feasible solution ˜θ . Although the rule can be constructed from any solutions as long as they are feasible , the power of the rule depends on the goodness of these solutions . Specifically , the criterion SPPC(t ) depends on the duality gap Pλ( ˜w , ˜b)−Dλ( ˜θ ) which would vanish when these primal and dual solutions are optimal . Roughly speaking , it suggests that , if these solutions are somewhat close to the optimal ones , we could expect that the SPP rule is powerful .
In practical predictive pattern mining tasks , we need to find a good penalty parameter λ based on a model selection technique such as cross validation . In model selection , a sequence of solutions with various different penalty parameters must be trained . Such a sequence of solutions is sometimes referred to as a regularization path [ 11 ] . Regularization path of the problem ( 2 ) is usually computed from larger λ to smaller λ because more sparse solutions would be obtained for larger λ . Let us write the sequence of λs as λ0 > λ1 > . . . > λK . When computing such a sequence of solutions , it is reasonable to use warm start approach where the previous optimal solution at λk−1 is used as the initial starting point of the next optimization problem at λk . In such a situation , we can also make use of the previous solution at λk−1 as the feasible solution for the safe pattern pruning rule at λk .
In sparse modeling literature , it is custom to start from the largest possible λ at which the primal solution is given as w∗ = 0 and b∗ = ¯y , where ¯y is the sample mean of {yi}i∈[n ] . The largest λ is given as fifififififi i∈[n ] fifififififi .
λmax := max t∈T xit(yi − ¯y )
In order to solve this maximization problem over the database , for a node t and t ∈ Tsub(t ) , we can use the following upper boundfifififififi i∈[n ] fifififififi  i|yi−¯y>0 xit ( yi − ¯y )
≤ max xit(yi − ¯y),− i|yi−¯y<0
 , xit(yi − ¯y ) and this upper bound can be exploited for pruning the search over the tree .
Algorithm 1 shows the entire procedure for computing the regularization path by using the SPP rule . fififi i∈[n ] xit(yi − ¯y ) fififi and ( w0 , b0 ) ← ( 0 , ¯y )
Algorithm 1 Regularization path computation algorithm Input : {(Gi , yi)}i∈[n ] , {λk}k∈[K ] 1 : λ0 ← maxt∈T 2 : for k = 1 , . . . , K do 3 :
Find ˆA(λk ) ⊇ A∗(λk ) by searching over the tree with the SPP rules based on ( w∗(λk−1 ) , b∗(λk−1 ) ) and θ∗(λk−1 ) as the primal and dual feasible solutions , respectively . Solve a small optimization problems in ( 6 ) with ˆA = ˆA(λk ) , and obtain the primal solution ( w∗(λk ) , b∗(λk ) ) and the dual solution θ∗(λk ) . 5 : end for Output : {(w∗(λk ) , b∗(λk))}k∈[K ] and {(θ∗(λk)}k∈[K ]
4 :
4 . EXPERIMENTS
In this section , we demonstrate the effectiveness of the proposed safe pattern pruning ( SPP ) method through numerical experiments . We compare SPP with the boosting based method ( boosting ) discussed in §22 4.1 Experimental setup We considered regularization path computation scenario described in §34 Specifically , we computed a sequence of optimal solutions of ( 2 ) for a sequence of 100 penalty parameters λ evenly allocated between λ0 = λmax and 0.01λ0 in logarithmic scale . For solving the convex optimization problems , we used coordinate gradient descent method [ 16 ] . The optimization solver was terminated when the duality gap felled below 10−6 . In both of SPP and boosting , we used warm start approach . In addition , the solution at the previous λ was also used as the feasible solution for constructing the SPP rule at the next λ . We used gSpan algorithm [ 21 ] for mining subgraphs . We wrote all the codes ( except gSpan part in graph mining experiment ) in C++ . All the computations were conducted by using a single core of an Intel Xeon CPU E5 2643 v2 ( 3.50GHz ) with 64GB MEM .
1789 ( a ) CPDB
( b ) mutagenicity
( c ) Bergstrom
( d ) Karthikeyan
Figure 2 : Computation time comparison for graph classification and regression . Each bar contains computational time taken in the tree traverse ( traverse ) and the optimization procedure ( solve ) respectively .
( a ) splice
( b ) a9a
( c ) dna
( d ) protein
Figure 3 : Computation time comparison for item set classification and regression . Each bar contains computational time taken in the tree traverse ( traverse ) and the optimization procedure ( solve ) respectively .
4.2 Graph classification/regression
We applied SPP and boosting to graph classification and regression problems . For classification , we used CPDB and mutagenicity datasets , containing n = 648 and n = 4377 chemical compounds respectively , for which the goal is to predict whether each compound has mutagenicity or not . For regression , we used Bergstrom and Karthikeyan datasets where the goal is to predict the melting point of each of the n = 185 and n = 4173 chemical compounds . All datasets are downloadable from http://cheminformaticsorg/datasets/ We considered the cases with maxpat ∈ {5 , 6 , 7 , 8 , 9 , 10} , where maxpat indicates the maximum number of edges of subgraphs we wanted to find .
Figure 2 shows the computation time of the two methods . In all the cases , SPP is faster than boosting , and the difference gets larger as maxpat increases . Figure 2 also shows the computation time taken in traversing the trees ( traverse ) and that taken in solving the optimization problems ( solve ) . The results indicate that traverse time of SPP are only slightly better than that of boosting . It is because the most time consuming component of gSpan is the minimality check of the DFS ( depth first search ) code , and the traverse time mainly depends on how many different nodes are generated in the entire regularization path computation process1 . In terms of solve time , there are large differences between SPP and boosting . In SPP , we have only to solve a single convex optimization problem for each λ . In boosting , on the other hand , convex optimization problems must be repeatedly solved every time a new pattern is added to the working set . Figure 4 shows the total number of tra versed nodes in the entire regularization path computation process . Total number of traversed nodes in SPP is much smaller than those of boosting , which is because one must repeat searching over trees many times in boosting .
4.3 Item set classification/regression
We applied SPP and boosting to item set classification and regression problems . For classification , we used splice dataset ( n = 1000 and the number of items d = 120 ) and a9a dataset ( n = 32561 and d = 123 ) . For regression , we used dna dataset ( n = 2000 and d = 180 ) and protein dataset ( n = 6621 and d = 714)2 . All datasets were obtained from LIBSVM Dataset site [ 3 ] . We considered the cases with maxpat ∈ {3 , 4 , 5 , 6} , where maxpat here indicates the maximum size of item sets we wanted to find .
Figure 3 compares the computation time of the two methods . In all the cases , SPP is faster than boosting . Here again , Figure 3 also shows the computation time taken in traversing the trees ( traverse ) and that taken in solving the optimization problems ( solve ) . In contrast to the graph mining results , traverse time of SPP are much smaller than that of boosting because it simply depends on how many nodes are traversed in total . Figure 5 shows the total number of traversed nodes in the entire regularization path computation process . Especially when λ is small where the number of active patterns are large , boosting needed to traverse large number of nodes , which is because the number of steps of boosting is large when there are large number of active patterns .
1A common trick used in graph mining algorithms with gSpan is to keep the minimality check results in the memory for all the nodes generated so far .
2This dataset is provided for classification . We used it for regression simply by regarding the class label as the scalar response variable .
0 10 20 30 40 50 6056789105678910Time ( sec)traversesolveSPPboosting 0 1000 2000 3000 4000 5000 600056789105678910Time ( sec)traversesolveSPPboosting 0 2 4 6 8 10 12 14 16 18 2056789105678910Time ( sec)traversesolveSPPboosting 0 200 400 600 800 1000 1200 1400 1600 1800 200056789105678910Time ( sec)traversesolveSPPboosting 0 50 100 150 200 250 300 35034563456Time ( sec)traversesolveSPPboosting 0 500 1000 1500 2000 250034563456Time ( sec)traversesolveSPPboosting 0 500 1000 1500 2000 250034563456Time ( sec)traversesolveSPPboosting 0 500 1000 1500 2000 2500 3000 3500 4000 450034563456Time ( sec)traversesolveSPPboosting1790 ( a 1 ) maxpat 6
( a 2 ) maxpat 7
( a 3 ) maxpat 8
( a 4 ) maxpat 10
( a ) CPDB
( b 1 ) maxpat 6
( b 2 ) maxpat 7
( b 3 ) maxpat 8
( b 4 ) maxpat 10
( b ) mutagenicity
( c 1 ) maxpat 6
( c 2 ) maxpat 7
( c 3 ) maxpat 8
( c 4 ) maxpat 10
( c ) Bergstrom
( d 1 ) maxpat 6
( d 2 ) maxpat 7
( d 3 ) maxpat 8
( d 4 ) maxpat 10
( d ) Karthikeyan
Figure 4 : # of traversed nodes for graph classification and regression .
0 5000 10000 15000 20000 25000 30000 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 10000 20000 30000 40000 50000 60000 70000 80000 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 20000 40000 60000 80000 100000 120000 140000 160000 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 50000 100000 150000 200000 250000 300000 350000 400000 450000 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 20000 40000 60000 80000 100000 120000 140000 160000 180000 200000 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 100000 200000 300000 400000 500000 600000 700000 800000 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 500000 1e+06 1.5e+06 2e+06 2.5e+06 3e+06 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 5e+06 1e+07 1.5e+07 2e+07 25e+07 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 2000 4000 6000 8000 10000 12000 14000 16000 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 5000 10000 15000 20000 25000 30000 35000 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 10000 20000 30000 40000 50000 60000 70000 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 20000 40000 60000 80000 100000 120000 140000 160000 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 20000 40000 60000 80000 100000 120000 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 50000 100000 150000 200000 250000 300000 350000 400000 450000 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 200000 400000 600000 800000 1e+06 1.2e+06 14e+06 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 1e+06 2e+06 3e+06 4e+06 5e+06 6e+06 7e+06 8e+06 9e+06 1e+07 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP1791 ( a 1 ) maxpat 3
( a 2 ) maxpat 4
( a 3 ) maxpat 5
( a 4 ) maxpat 6
( a ) splice
( b 1 ) maxpat 3
( b 2 ) maxpat 4
( b 2 ) maxpat 5
( b 3 ) maxpat 6
( b ) a9a
( c 1 ) maxpat 3
( c 2 ) maxpat 4
( c 3 ) maxpat 5
( c 4 ) maxpat 6
( c ) dna
( d 1 ) maxpat 3
( d 2 ) maxpat 4
( d 3 ) maxpat 5
( d 4 ) maxpat 6
( d ) protein
Figure 5 : # of traversed nodes for item set classification and regression .
0 500000 1e+06 1.5e+06 2e+06 2.5e+06 3e+06 3.5e+06 4e+06 4.5e+06 5e+06 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 1e+06 2e+06 3e+06 4e+06 5e+06 6e+06 7e+06 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 1e+06 2e+06 3e+06 4e+06 5e+06 6e+06 7e+06 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 1e+06 2e+06 3e+06 4e+06 5e+06 6e+06 7e+06 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 100000 200000 300000 400000 500000 600000 700000 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 500000 1e+06 1.5e+06 2e+06 2.5e+06 3e+06 35e+06 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 1e+06 2e+06 3e+06 4e+06 5e+06 6e+06 7e+06 8e+06 9e+06 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 2e+06 4e+06 6e+06 8e+06 1e+07 1.2e+07 1.4e+07 16e+07 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 5e+06 1e+07 1.5e+07 2e+07 2.5e+07 3e+07 35e+07 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 1e+07 2e+07 3e+07 4e+07 5e+07 6e+07 7e+07 8e+07 9e+07 1e+08 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 2e+07 4e+07 6e+07 8e+07 1e+08 1.2e+08 1.4e+08 1.6e+08 1.8e+08 2e+08 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 2e+07 4e+07 6e+07 8e+07 1e+08 1.2e+08 1.4e+08 1.6e+08 1.8e+08 2e+08 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 2e+07 4e+07 6e+07 8e+07 1e+08 1.2e+08 1.4e+08 1.6e+08 1.8e+08 2e+08 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 2e+07 4e+07 6e+07 8e+07 1e+08 1.2e+08 1.4e+08 1.6e+08 1.8e+08 2e+08 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 5e+07 1e+08 1.5e+08 2e+08 25e+08 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP 0 5e+07 1e+08 1.5e+08 2e+08 25e+08 2 18 16 14 12 1 08 06 04 02 0# of traversed nodeslog λ / λmaxboostingSPP1792 5 . CONCLUSIONS
In this paper , we introduced a novel method called safe pattern pruning ( SPP ) method for a class of predictive pattern mining problems . The advantage of the SPP method is that it allows us to efficiently find a superset of all the predictive patterns that are used in the optimal predictive model by a single search over the database . We demonstrated the computational advantage of the SPP method by applying it to graph classification/regression and item set classification/regression problem As a future work , we will study how to integrate the SPP method with a technique for providing the statistical significances of the discovered patterns [ 15 ] .
6 . ACKNOWLEDGEMENT
For this work , MK was partially supported from JSPS KAKENHI 26280083 and 26730120 , KT was partially supported from JST CREST 15656320 , IT was partially supported from JST CREST 15656320 , and JSPS KAKENHI 26280083 , 16H00886 .
7 . REFERENCES
[ 1 ] A . Bonnefoy , V . Emiya , L . Ralaivola , and
R . Gribonval . A dynamic screening principle for the lasso . In Signal Processing Conference ( EUSIPCO ) , 2014 Proceedings of the 22nd European , pages 6–10 . IEEE , 2014 .
[ 2 ] S . Boyd and L . Vandenberghe . Convex optimization .
Cambridge university press , 2004 .
[ 11 ] M . Y . Park and T . Hastie . L1 regularization path algorithm for generalized linear models . Journal of the Royal Statistical Society : Series B ( Statistical Methodology ) , 69(4):659–677 , 2007 .
[ 12 ] H . Saigo , T . Kadowaki , and K . Tsuda . A linear programming approach for molecular qsar analysis . In International workshop on mining and learning with graphs ( MLG ) , pages 85–96 . Citeseer , 2006 .
[ 13 ] H . Saigo , S . Nowozin , T . Kadowaki , T . Kudo , and
K . Tsuda . gboost : a mathematical programming approach to graph classification and regression . Machine Learning , 75(1):69–89 , 2009 .
[ 14 ] H . Saigo , T . Uno , and K . Tsuda . Mining complex genotypic features for predicting hiv 1 drug resistance . Bioinformatics , 23(18):2455–2462 , 2007 .
[ 15 ] S . Suzumura , K . Nakagawa , M . Sugiyama , K . Tsuda , and I . Takeuchi . Selective inference approach for statistically sound predictive pattern mining . arXiv preprint arXiv:1602.04601 , 2016 .
[ 16 ] P . Tseng and S . Yun . A coordinate gradient descent method for nonsmooth separable minimization . Mathematical Programming , 117(1 2):387–423 , 2009 .
[ 17 ] J . Wang , J . Zhou , J . Liu , P . Wonka , and J . Ye . A safe screening rule for sparse logistic regression . In Advances in Neural Information Processing Systems , pages 1053–1061 , 2014 .
[ 18 ] J . Wang , J . Zhou , P . Wonka , and J . Ye . Lasso screening rules via dual polytope projection . In Advances in Neural Information Processing Systems , pages 1070–1078 , 2013 .
[ 3 ] C C Chang and C J Lin . LIBSVM : A library for
[ 19 ] Z . J . Xiang , Y . Wang , and P . J . Ramadge . Screening support vector machines . ACM Transactions on Intelligent Systems and Technology , 2:27:1–27:27 , 2011 .
[ 4 ] A . Demiriz , K . P . Bennett , and J . Shawe Taylor .
Linear programming boosting via column generation . Machine Learning , 46(1 3):225–254 , 2002 .
[ 5 ] L . El Ghaoui , V . Viallon , and T . Rabbani . Safe feature elimination for the lasso and sparse supervised learning problems . Pacific Journal of Optimization , 8(4):667–698 , 2012 .
[ 6 ] O . Fercoq , A . Gramfort , and J . Salmon . Mind the duality gap : safer rules for the lasso . In Proceedings of the 32nd International Conference on Machine Learning , pages 333–342 , 2015 .
[ 7 ] T . Joachims . Training linear svms in linear time . In
Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 217–226 , 2006 .
[ 8 ] T . Kudo , E . Maeda , and Y . Matsumoto . An application of boosting to graph classification . In Advances in neural information processing systems , pages 729–736 , 2004 .
[ 9 ] J . Liu , Z . Zhao , J . Wang , and J . Ye . Safe Screening with Variational Inequalities and Its Application to Lasso . In Proceedings of the 31st International Conference on Machine Learning , 2014 .
[ 10 ] E . Ndiaye , O . Fercoq , A . Gramfort , and J . Salmon .
Gap safe screening rules for sparse multi task and multi class models . In Advances in Neural Information Processing Systems , pages 811–819 , 2015 . tests for lasso problems . arXiv preprint arXiv:1405.4897 , 2014 .
[ 20 ] Z . J . Xiang , H . Xu , and P . J . Ramadge . Learning sparse representations of high dimensional data on large scale dictionaries . In Advances in Neural Information Processing Systems , pages 900–908 , 2011 .
[ 21 ] X . Yan and J . Han . gspan : Graph based substructure pattern mining . In Data Mining , 2002 . ICDM 2003 . Proceedings . 2002 IEEE International Conference on , pages 721–724 . IEEE , 2002 .
APPENDIX A . PROOFS
Proof . For any pair of nodes t and t ∈ Tsub(t ) , i:βi
˜θi>0
Proof of Corollary 3 .
αit ˜θi ≥ αit ˜θi ≤ First consider the case where ut = ut = −
˜θi<0 αit ˜θi , from ( 10 ) , i:βi
˜θi<0 i:βi
˜θi<0 i:βi
˜θi>0 i:βi
αit ˜θi ,
αit ˜θi .
( 9 )
( 10 )
˜θi>0 αit ˜θi . When ˜θi>0 αit ˜θi , from ( 9 ) , ut ≥ ut . When ut = i:βi i:βi ut ≥ − i:βi
˜θi<0
αit ˜θi ≥ ut .
1793 It suggests that
Then , from ( 16 ) , the solution of ( 12 ) is given as
∗ i
αitθ t = 0 , ∀t ∈ T . ∗
θ = ˜θ −
Proof of Lemma 6 .
:,t
α
˜θ − rλ
α:,t2
˜θi<0 αit ˜θi . When i:βi
Next , consider the case where ut = − ut = When ut = − ut ≥
˜θi>0 αit ˜θi , from ( 9 ) ,
˜θi<0 i:βi i:βi
αit ˜θi ≥ ut .
˜θi<0 αit ˜θi , from ( 10 ) , ut ≥ ut . Furthermore , it is clear that vt ≥ vt . Since rλ > 0 , SPPC(t ) ≥ SPPC(t ) . i:βi
Proof of Lemma 4 .
Proof . Based on the convex optimization theory ( see , eg , [ 2] ) , the KKT optimality condition of the primal problem ( 2 ) and the dual problem ( 5 ) is written as n i=1 i ∈ ∗
αitθ fififififi n i=1 sign(w∗ t ) [ −1 , +1 ] fififififi < 1 ⇒ w if w∗ if w∗ t = 0 , t = 0 ,
∀t ∈ T .
Proof . Let α:,t := [ α1t , . . . , αnt ] . First , note that the objective part of the optimization problem ( 7 ) is rewritten as
:,tθ fififi fififiα
− min max
θ
θ max ⇔ max ⇔ max
θ
:,tθ,−α
:,tθ α θ,− min ( −α:,t )
θ
:,tθ
α
( 11 )
Thus , we consider the following convex optimization problem :
:,tθ st θ − ˜θ2
2 ≤ r2
λ , β
θ = 0 .
( 12 ) min
θ
α
Let us define the Lagrange function :,tθ + ξ(θ − ˜θ2
L(θ , ξ , η ) = α
2 − r2
λ ) + ηβ
θ , and then the optimization problem ( 12 ) is written as min
θ max ξ≥0,η
L(θ , ξ , η ) .
The KKT optimality conditions are summarized as
θ − ˜θ2
ξ(θ − ˜θ2
ξ > 0 , λ ≤ 0 , θ = 0 ,
λ ) = 0 ,
2 − r2
β 2 − r2
( 13 )
( 14a )
( 14b )
( 14c )
( 14d ) where note that ξ > 0 because the problem does not have a minimum value when ξ = 0 . Differentiating the Lagrange function wrt θ and using the fact that it should be zero ,
θ = ˜θ − 1 2ξ
( α:,t + ηβ ) .
( 15 )
By substituting ( 15 ) into ( 13 ) , α:,t + ηβ2 max ξ>0,η
− 1 4ξ
2 + ( α:,t + ηβ )
˜θ − ξr2 λ .
Since the objective function is a quadratic concave function wrt η , we obtain the following by considering the condition ( 14c ) :
.
η = − α :,tβ β2
2
2
α:,t − α :,tβ β2 α:,t2
2β2
By substituting this into ( 15 ) ,
θ = ˜θ − 1 2ξ
β
.
( 16 )
Since ξ > 0 and ( 14d ) indicates θ − ˜θ2 substituting ( 16 ) into this equality ,
2 − r2
λ = 0 , by
ξ =
1
2β2rλ
α:,t2
β2rλ 2β2 2 − ( α
:,tβ)2
:,tβ)2 .
2 − ( α
α:,t − α :,tβ β2
2
β
,
2 − ( α :,tβ)2 β2
2
.
( 17 ) and the minimum objective function value of ( 12 ) is
Then , substituting ( 17 ) into ( 11 ) , the optimal objective value of ( 7 ) is given asfififiα
:,t
˜θ fififi + rλ
α:,t2
2 − ( α :,tβ)2 β2
2
. fififififi n i=1
Proof of Lemma 7 .
Proof . First , using the bound introduced in [ 8 ] ,
αit ˜θi
αit ˜θi + fififififi = fifififififi
˜θi>0
  i:βi i:βi
≤ max
≤ max fifififififi i:βi
˜θi<0
αit ˜θi
αit ˜θi , − αit ˜θi , − i:βi i:βi
˜θi>0 i:βi
˜θi<0
˜θi>0
˜θi<0
 
αit ˜θi
αit ˜θi
Next , it is clear that i=1 n fififififi n i=1
α2
= : ut , it − ( n n fififififi + rλ i=1
By combining them ,
αit ˜θi i=1 αit βi)2 β2
2
≤ n it − ( n i=1
α2 it ≤ n
α2 i=1
α2 it := vt . i=1 αit βi)2 β2
2
≤ ut + rλ
√ vt .
1794
