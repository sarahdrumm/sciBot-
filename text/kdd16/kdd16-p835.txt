A Subsequence Interleaving Model for Sequential Pattern Mining
Jaroslav Fowkes
Charles Sutton
School of Informatics
University of Edinburgh , Edinburgh , EH8 9AB , UK
{jfowkes , csutton}@edacuk
ABSTRACT Recent sequential pattern mining methods have used the minimum description length ( MDL ) principle to define an encoding scheme which describes an algorithm for mining the most compressing patterns in a database . We present a novel subsequence interleaving model based on a probabilistic model of the sequence database , which allows us to search for the most compressing set of patterns without designing a specific encoding scheme . Our proposed algorithm is able to efficiently mine the most relevant sequential patterns and rank them using an associated measure of interestingness . The efficient inference in our model is a direct result of our use of a structural expectation maximization framework , in which the expectation step takes the form of a submodular optimization problem subject to a coverage constraint . We show on both synthetic and real world datasets that our model mines a set of sequential patterns with low spuriousness and redundancy , high interpretability and usefulness in real world applications . Furthermore , we demonstrate that the quality of the patterns from our approach is comparable to , if not better than , existing state of the art sequential pattern mining algorithms .
1 .
INTRODUCTION
Sequential data pose a challenge to exploratory data analysis , as large data sets of sequences are difficult to visualise . In applications such as healthcare ( patterns in patient paths [ 10] ) , click streams ( web usage mining [ 18] ) , bioinformatics ( predicting protein sequence function [ 27 ] ) and source code ( API call patterns [ 30] ) , a common approach has been sequential pattern mining , to identify a set of patterns that commonly occur as subsequences of the sequences in the data .
A natural family of approaches for sequential pattern mining is to mine frequent subsequences [ 2 ] or closed frequent subsequences [ 26 ] , but these suffer from the well known problem of pattern explosion , that is , the list of frequent subsequences is typically long , highly redundant , and difficult to understand . Recently , researchers have introduced methods to prevent the problem of pattern explosion based on the minimum description length ( MDL ) principle [ 12 , 25 ] . These methods define an encoding scheme which describes an algorithm for compressing a sequence database based on a library of subsequence patterns , and then search for a set of patterns that lead to the best compression of the database . These MDL methods provide a theoretically principled approach that results in better patterns than frequent subsequence mining , but their performance relies on designing a coding scheme .
In this paper , we introduce an alternate probabilistic perspective on subsequence mining , in which we develop a generative model of the database conditioned on the patterns . Then , following Shannon ’s theorem , the length of the optimal code for the database under the model is simply the negative logarithm of its probability . This allows us to search for the set of patterns that best compress the database without designing a specific coding scheme . Our approach , which we call the Interesting Sequence Miner ( ISM)1 , is a novel sequential pattern mining algorithm that is able to efficiently mine the most relevant sequential patterns from a database and rank them using an associated measure of interestingness . ISM makes use of a novel probabilistic model of sequences , based on generating a sequence by interleaving a group of subsequences . It is these learned component subsequences that are the patterns ISM returns .
An approach based on probabilistic machine learning brings a variety of benefits , namely , that the probabilistic model allows us to declaratively incorporate ideas about what types of patterns would be most useful ; that we can easily compose the ISM model with other types of probabilistic models from the literature ; and that we are able to bring to bear powerful tools for inference and optimization from probabilistic machine learning . Inference in our model involves approximate optimization of a non monotone submodular objective subject to a submodular coverage constraint . The necessary partition function is intractable to construct directly , however we show that it can be efficiently computed using a suitable lower bound . The set of sequential patterns under our model can be inferred efficiently using a structural expectation maximization ( EM ) framework [ 8 ] . This is , to our knowledge , the first use of an expectation maximization scheme for the subsequence mining problem .
On real world datasets ( Section 4 ) , we find that ISM returns a notably more diverse set of patterns than the recent MDL methods SQS and GoKrimp ( Table 2 ) , while retain
1https://github.com/mast group/sequence mining
835 ing similar quality . A more diverse set of patterns is , we suggest , especially suitable for manual examination during exploratory data analysis . Qualitatively , the mined patterns from ISM are all highly correlated and extremely relevant , eg representing phrases such as oh dear or concepts such as reproducing kernel hilbert space . More broadly , this new perspective has the potential to open up a wide variety of future directions for new modelling approaches , such as combining sequential pattern mining methods with hierarchical models , topic models , and nonparametric Bayesian methods .
2 . RELATED WORK
Sequential pattern mining was first introduced by Agrawal and Srikant [ 2 ] in the context of market basket analysis , which led to a number of other algorithms for frequent subsequence , including GSP [ 23 ] , PrefixSpan [ 22 ] , SPADE [ 29 ] , and SPAM [ 3 ] . Frequent sequence mining suffers from pattern explosion : a huge number of highly redundant frequent sequences are retrieved if the given minimum support threshold is too low . One way to address this is by mining frequent closed sequences , ie , those that have no subsequences with the same frequency , such as via the BIDE algorithm [ 26 ] . However , even mining frequent closed sequences does not fully resolve the problem of pattern explosion . We refer the interested reader to Chapter 11 of [ 1 ] for a survey of frequent sequence mining algorithms .
In an attempt to tackle this problem , modern approaches to sequence mining have used the minimum description length ( MDL ) principle to find the set of sequences that best summarize the data . The GoKrimp algorithm [ 12 ] directly mines sequences that best compress a database using a MDL based approach . The goal of GoKrimp is essentially to cover the database with as few sequences as possible , because the dictionary based description length that is used by GoKrimp favours encoding schemes that cover more long and frequent subsequences in the database . In fact , finding the most compressing sequence in the database is strongly related to the maximum tiling problem , ie , finding the tile with largest area in a binary transaction database .
SQS Search ( SQS ) [ 25 ] also uses MDL to find the set of sequences that summarize the data best : a small set of informative sequences that achieve the best compression is mined directly from the database . SQS uses an encoding scheme that explicitly punishes gaps by assigning zero cost for encoding non gaps and higher cost for encoding larger gaps between items in a pattern . While SQS can be very effective at mining informative patterns from text , it cannot handle interleaving patterns , unlike GoKrimp and ISM , which can be a significant drawback on certain datasets eg patterns generated by independent processes that may frequently overlap .
In related work , Mannila and Meek [ 15 ] proposed a generative model of sequences which finds partial orders that describe the ordering relationships between items in a sequence database . Sequences are generated by selecting a subset of items from a partial order with a learned inclusion probability and arranging them into a compatible random ordering . Unlike ISM , their model does not allow gaps in the generated sequences and each sequence is only generated from a single partial order , an unrealistic assumption in practice . There has also been some existing research on probabilistic models for sequences , especially using Markov models . Gwadera et al . [ 9 ] use a variable order Markov model to iden tify statistically significant sequences . Stolcke and Omohundro [ 24 ] developed a structure learning algorithm for HMMs that learns both the number of states and the topology . Landwehr [ 13 ] extended HMMs to handle a fixed number of hidden processes whose outputs interleave to form a sequence . Wood et al . developed the sequence memoizer [ 28 ] , a variable order Markov model with a Pitman Yor process prior . Also , Nevill Manning and Witten [ 20 ] infer a contextfree grammar over sequences using the Sequitur algorithm .
3 . MINING SEQUENTIAL PATTERNS
In this section we will formulate the problem of identifying a set of interesting sequences that are useful for explaining a sequence database . First we will define some preliminary concepts and notation . An item i is an element of a universe U = {1 , 2 , . . . , n} that indexes symbols . A sequence S is simply an ordered list of items ( e1 , . . . , em ) such that ei ∈ U ∀i . A sequence Sa = ( a1 , . . . , an ) is a subsequence of another sequence Sb = ( b1 , . . . , bm ) , denoted Sa ⊂ Sb , if there exist integers 1 ≤ i1 < i2 < . . . < in ≤ m such that a1 = bi1 , a2 = bi2 , . . . , an = bin ( ie , the standard definition of a subsequence ) . A sequence database is merely a list of sequences X(j ) . Further , we say that a sequence S is supported by a sequence X in the sequence database if S ⊂ X . Note that in the above definition each sequence only contains a single item as this is the most important and popular sequence type ( cf . word sequences , protein sequences , click streams , etc)2 A multiset M is a generalization of a set that allows elements to occur multiple times , ie , with a specific multiplicity #M(· ) . For example in the multiset M = {a , a , b} , the element a occurs twice and so has multiplicity #M(a ) = 2 . 3.1 Problem Formulation Our aim in this work is to infer a set of interesting subsequences I from a database of sequences X(1 ) , . . . , X(N ) . Here by interesting , we mean a set of patterns that are useful for helping a human analyst to understand the important properties of the database , that is , interesting subsequences should reflect the most important patterns in the data , while being sufficiently concise and non redundant that they are suitable for manual examination . These criteria are inherently qualitative , reflecting the fact that the goal of data mining is to build human insight and understanding . To quantify these criteria , we operationalize the notion of interesting sequence as those sequences that best explain the underlying database under a probabilistic model of sequences . Specifically we will use a generative model , ie , a model that starts with a set of interesting subsequences I and from this set generates the sequence database X(1 ) , . . . , X(N ) . Our goal is then to infer the most likely generating set I under our chosen generative model . We want a model that is as simple as possible yet powerful enough to capture correlations between items in sequences . A simple such model is as follows : iteratively sample subsequences S from I and randomly interleave them to form the database sequence X . If we associate each subsequence S ∈ I with a probability πS , we can sample the indicator variable zS ∼ Bernoulli(πS ) 2Note that we can easily extend our algorithm to mine sequences of sets of items ( as defined in the original sequence mining paper [ 2 ] ) by extending the subsequence operator ⊂ to handle these more general ‘sequences’ .
836 and include it in X if zS = 1 . However , we may wish to include a subsequence more than once in the sequence X , that is , we need some way of sampling the multiplicity of S in X . The simplest way to do this is to change our generating distribution from Bernoulli to eg Categorical and sample the multiplicity zS ∼ Categorical(πS ) where πS is now a vector of probabilities , with one entry for each multiplicity ( up to the maximum in the database ) . We define the generative model formally in the next section . 3.2 Generative Model
As discussed in the previous section , we propose a simple directed graphical model for generating a database of sequences X(1 ) , . . . , X(N ) from a set I of interesting sequences . The generative story for our model is , independently for each sequence X in the database : 1 . For each interesting sequence S ∈ I , decide independently the number of times S should be included in X , ie , sample the multiplicity zS ∈ N0 as zS ∼ Categorical(πS ) , where πS is a vector of multiplicity probabilities . For clarity we present the Categorical distribution here but one could use a more general distribution if desired . 2 . Set S to be the multiset with multiplicities zS of all the sequences S selected for inclusion in X :
S := {S | zS ≥ 1} .
3 . Set P to be the set of all possible sequences that can be generated by interleaving together all occurrences of the sequences in the multiset S , ie ,
P := {X |S partition of X , S ⊂ X ∀S ∈ S} .
Here by interleaving we mean the placing of items from one sequence into the gaps between items in another whilst maintaining the orders of the items imposed by each sequence .
4 . Sample X uniformly from P , ie , X ∼ P .
Note that we never need to construct the set P in practice , since we only require its cardinality during inference , and we show in the next section how we can efficiently compute an approximation to |P| . We can , however , sample from P efficiently by merging subsequences S ∈ S into X one at a time as follows : splice the elements of S , in order , into X at randomly chosen points ( here by splicing S into X we mean the placing of items from S into the gaps between items in X ) . For example , S = {(1 , 2 ) , ( 3 , 4)} will generate the set of sequences P = {(3 , 4 , 1 , 2 ) , ( 3 , 1 , 4 , 2 ) , ( 3 , 1 , 2 , 4 ) , ( 1 , 3 , 4 , 2 ) , ( 1 , 3 , 2 , 4 ) , ( 1 , 2 , 3 , 4)} . We could of course learn a transition distribution between subsequences in our model , but we choose not to do so because we want to force the model to use I to explain the sequential dependencies in the data . 3.3 Inference Given a set of interesting sequences I , let z denote the vector of zS for all sequences S ∈ I and similarly , let Π denote the list of πS for all S ∈ I . Assuming z , Π are fully determined , it is evident from the generative model that the probability of generating a database sequence X is p(X , z|Π ) =
Q fl 1
|P| 0
Q|πS|−1 m=0 π
S∈I
[ zS=m ] Sm if X ∈ P , otherwise , where |πS| is the length of πS and [ zS = m ] evaluates to 1 if zS = m , 0 otherwise . Intuitively , it helps to think of each πS as being an infinite vector and each S ∈ I as being augmented with a Kleene star operator , so that , for example , one can use ( 1 , 2)∗ and ( 3)∗ to generate the sequence ( 1 , 2 , 1 , 3 , 2 ) . Calculating the normalization constant |P| is problematic as we have to count the number of possible distinct sequences that could be generated by interleaving together subsequences in S . This is further complicated by the fact that S is a multiset and so can contain multiple occurrences of the same subsequence , which makes efficient computation of |P| impractical . However , it turns out that we can compute a straightforward upper bound since |P| is clearly bounded above by all possible permutations of all the items in all the subsequences S ∈ S , and this bound is attained when S contains only distinct singleton sequences without repetition . Formally ,
|P| ≤,P
S∈S|S| !
Conveniently , this gives us a non trivial lower bound on the posterior p(X , z|Π ) which , as we will want to maximize the posterior , is precisely what we want . Moreover , the lower bound acts as an additional penalty , strongly favouring a non redundant set of sequences ( see Section 42 )
Now assuming the parameters Π are known , we can infer z for a database sequence X by maximizing the log of the lower bound on the posterior p(X , z|Π ) over z :
X
|πS|−1X
S∈I m=0 max z st X ∈ P .
P S∈S|S|X j=1
[ zS = m ] ln(πSm ) − ln j
( 3.1 )
This is an NP hard problem in general and so impractical to solve directly in practice . However , we will show that it can be viewed as a special case of maximizing a submodular function subject to a submodular constraint and so approximately solved using the greedy algorithm for submodular function optimization . Now strictly speaking the notion of a submodular function is only applicable to sets , however we will consider the following generalization to multisets :
Definition 1 . ( Submodular Multiset Function ) Let Ω be a finite multiset and let N0 Ω denote the set of all possible Ω → R multisets that are subsets of Ω , then a function f : N0 is submodular if for for every C ⊂ D ⊂ Ω and S ∈ Ω with #C(S ) = #D(S ) it holds that f(C ∪ {S} ) − f(C ) ≥ f(D ∪ {S} ) − f(D ) .
Let us now define a function f for our specific case : let T be the multiset of supported interesting sequences , ie , sequences S ∈ I st S ⊂ X with multiplicity given by the maximum number of occurrences of S in any partition of X . Now , define f : N0
T → R as
[ #C(S ) = m ] ln(πSm ) − ln j
P S∈C|S|X j=1 f(C ) :=X
|πS|−1X
S∈C m=0
837 and g(C ) := |∪S∈CS| . We can now re state ( 3.1 ) as : Find a non overlapping multiset covering C ⊂ T that maximizes f(C ) , ie , such that g(C ) = g(T ) and f(C ) is maximized . Note that g(T ) = |X| by construction . Now clearly g is monotone submodular as it is a multiset coverage function , and we will show that f is non monotone submodular . To see that f is submodular observe that for C ⊂ D , #C(S ) = #D(S ) f(D ∪ {S} ) − f(D ) = ln(πS#D(S)+1 ) − ln(πS#D(S ) )
P D∈D|D|+|S|X j=P P C∈C|C|+|S|X j=P
D∈D|D|+1
C∈C|C|+1
−
− ln j ln j
= f(C ∪ {S} ) − f(C )
≤ ln(πS#C(S)+1 ) − ln(πS#C(S ) ) which is precisely Definition 1 . To see that f is non monotone observe that
πS#C(S)+1
πS#C(S )
P C∈C|C|+|S|X j=P
C∈C|C|+1
− ln j f(C ∪ {S} ) − f(C ) = ln whose sign is indeterminate .
Maximizing the posterior ( 3.1 ) is therefore a problem of maximizing a submodular function subject to a submodular coverage constraint and can be approximately solved by applying the greedy approximation algorithm ( Algorithm 1 ) . The greedy algorithm builds a multiset covering C by repeatedly choosing a sequence S that maximizes the profit f(C ∪ {S} ) − f(C ) of adding S to the covering divided by the number of items in S not yet covered by the covering g(C∪{S})−g(C ) = |S| . In order to minimize CPU time spent solving the problem , we cache the sequences and coverings for each database sequence as needed .
Algorithm 1 Greedy Algorithm Input : Database sequence X , supported sequences T
Initialize multiset C ← ∅ while g(C ) 6= |X| do
Choose S ∈ T maximizing f(C∪{S})−f(C ) C ← C ∪ {S}
|S| end while return C
Note that while there are good theoretical guarantees on the approximation ratio achieved by the greedy algorithm when maximizing a monotone submodular set function subject to a coverage constraint ( eg ln|X| + 1 for weighted set cover [ 5 , 7 ] ) the problem of maximizing a non monotone submodular set function subject to a coverage constraint has , to the best of our knowledge , not been studied in the literature . However , as our submodular optimization problem is an extension of the weighted set cover problem , the greedy algorithm is a natural fit and indeed we observe good performance in practice .
Algorithm 2 Hard EM Input : Set of sequences I and initial estimates Π(0 ) k ← 0 do k ← k + 1 E step : ∀ X(i ) solve ( 3.1 ) to get z M step : π
PN
← 1 i=1[z
( k ) Sm
N while kΠ(k−1 ) − Π(k)kF > ε Remove from I sequences S with πS0 = 1 return I , Π(k )
( i )
S ∀ S ∈ Ti
( i )
S = m ] ∀ S ∈ I , ∀ m
3.4 Learning
Given a set of interesting sequences I , consider now the case where both variables z , Π in the model are unknown . In this case we can use the hard EM algorithm [ 6 ] for parameter estimation with latent variables . The hard EM algorithm in our case is merely a simple layer on top of the inference algorithm ( 31 ) Suppose there are N database sequences X(1 ) , . . . , X(N ) with multisets of supported interesting sequences T ( 1 ) , . . . ,T ( N ) , then the hard EM algorithm is given in Algorithm 2 ( note that k·kF denotes the Frobenius norm and πS0 is the probability that S does not explain any sequence in the database ) . To initialize Π , a natural choice is simply the support ( relative frequency ) of each sequence . 3.5 Inferring new sequences We infer new sequences using structural EM [ 8 ] , ie , we add a candidate sequence S0 to I if doing so improves the optimal value p of the problem ( 3.1 ) averaged across all database sequences . Interestingly , there are two implicit regularization effects here . Firstly , observe from ( 3.1 ) that when a new candidate S0 is added to the model , a corresponding is added to the log likelihood of all database seterm ln πS0 0 quences that S0 does not support . For large sequence databases , this amounts to a significant penalty on candidates in practice . Secondly , observe that the last term of ( 3.1 ) acts as an additional penalty , strongly favouring a non redundant set of sequences . To get an estimate of maximum benefit to including candidate S0 , we must carefully choose an initial value of πS0 that is not too low , to avoid getting stuck in a local optimum . To infer a good πS0 , we force the candidate S0 to explain all database sequences it supports by initializing πS0 = ( 0 , 1 , . . . , 1)T and update πS0 with the probability corresponding to its actual usage once we have inferred all the coverings . Given a set of interesting sequences I and corresponding probabilities Π along with database sequences X(1 ) , . . . , X(N ) , each iteration of the structural EM algorithm is given in Algorithm 3 below . Occasionally the Hard EM algorithm may assign zero probability to one or more singleton sequences and cause the greedy algorithm to not be able to fully cover a database sequence X using just the interesting sequences in I . In this case we simply re seed I with the necessary singletons . Finally , in practice we store the set of candidates that have been rejected by Structural EM and check each potential candidate against this set for efficiency . 3.6 Candidate generation
The Structural EM algorithm ( Algorithm 3 ) requires a
838 Algorithm 3 Structural EM ( one iteration ) Input : Sequences I , Π , optima p(i ) of ( 3.1 ) ∀ X(i )
PN i=1 p(i )
Set profit p ← 1 do
N
Generate candidate S0 using Candidate Gen I ← I ∪ {S0} , πS0 ← ( 0 , 1 , . . . , 1)T E step : ∀ X(i ) solve ( 3.1 ) to get z S ∀ S ∈ Ti S = m ] ∀ S ∈ I , ∀ m M step : π0 i=1[z ∀ X(i ) , solve ( 3.1 ) using π0 S ∀ S ∈ Ti S , z to get the optimum p(i )
Sm ← 1
PN PN
( i )
( i )
( i )
N
Set new profit p0 ← 1 I ← I \ {S0}
N i=1 p(i ) while p0 ≤ p {until one good candidate found} I ← I ∪ {S0} return I , Π0 method to generate new candidate sequences S0 that are to be considered for inclusion in the set of interesting sequences I . One possibility would be to use the GSP algorithm [ 23 ] to recursively suggest larger sequences starting from singletons , however preliminary experiments found this was not the most efficient method . For this reason we take a slightly different approach and recursively combine the interesting sequences in I with the highest support first ( Algorithm 4 ) . In this way our candidate generation algorithm is more likely to propose viable candidate sequences earlier and in practice we find that this heuristic works well . Algorithm 4 Candidate Gen Input : Sequences I , cached supports σ , queue length q if ( cid:64 ) priority queue Q for I then
Initialize σ ordered priority queue Q Sort I by decreasing sequence support using σ for all ordered pairs S1 , S2 ∈ I , highest ranked first do
Generate candidate S0 = S1S2 Cache support of S0 in σ and add S0 to Q if |Q| = q break end for end if Pull highest ranked candidate S0 from Q return S0
3.7 Mining Interesting Sequences
Our complete interesting sequence mining ( ISM ) algorithm is given in Algorithm 5 . Note that the Hard EM
Algorithm 5 ISM ( Interesting Sequence Miner ) Input : Database of sequences X(1 ) , . . . , X(N )
Initialize I with singletons , Π with their supports while not converged do Add sequences to I , Π using Structural EM Optimize parameters for I , Π using Hard EM end while return I , Π parameter optimization step need not be performed at every iteration , in fact it is more efficient to suggest several candidate sequences before optimizing the parameters . As all operations on database sequences in our algorithm are trivially parallelizable , we perform the E and M steps in both the hard and structural EM algorithms in parallel . 3.8 Interestingness Measure Now that we have inferred the model variables z , Π , we are able to use them to rank the retrieved sequences in I . There are two natural rankings one can employ , and both have their strengths and weaknesses . The obvious approach is to rank each sequence S ∈ I according to its probability under the model πS , however this has the disadvantage of strongly favouring frequent sequences over rare ones , an issue we would like to avoid . An alternative is to rank the retrieved sequences according to their interestingness under the model , that is the ratio of database sequences they explain to database sequences they support . One can think of interestingness as a measure of how necessary the sequence is to the model : the higher the interestingness , the more supported database sequences the sequence explains . Thus interestingness provides a more balanced measure than probability , at the expense of missing some frequent sequences that only explain some of the database sequences they support . We define interestingness formally as follows .
Definition 2 . The interestingness of a sequence S ∈ I re trieved by ISM ( Algorithm 5 ) is defined as S ≥ 1 ]
( i ) int(S ) = i=1[z supp(S )
PN and ranges from 0 ( least interesting ) to 1 ( most interesting ) . Any ties in the ranking can be broken using the sequence probability p(S ⊂ X ) = p(zS ≥ 1 ) = 1 − πS0 . 3.9 Correspondence to Existing Models
There is a close and well known connection between probabilistic modelling and the minimum description length principle used by SQS and GoKrimp ( see MacKay [ 14 ] , §28.3 for a particularly nice explanation ) . Given a probabilistic model p(X|Π,I ) of a single database sequence X , by Shannon ’s theorem the optimal code for the model will encode X using approximately − log2 p(X|Π,I ) bits . So by finding a set of patterns that maximizes the probability of the data , we are also finding patterns that minimize description length . Conversely , any encoding scheme implicitly defines a probabilistic model . Given an encoding scheme E that assigns each transaction X to a string of L(X ) bits , we can define p(X|E ) ∝ 2−L(X ) , and then E is an optimal code for p(X|E ) . Interpreting the previous subsequence mining methods in terms of their implicit probabilistic models provides interesting insights into these methods . The encoding of a database sequence used by SQS can be interpreted as a probabilistic model p(X , z|Π,I ) , where the SQS analog of p(X , z|Π,I ) is similar to ( 3.1 ) with
PN P PN i=1 z
I∈I
( i ) S
,
( j ) I j=1 z
!m
πSm = along with additional terms that correspond to the description lengths for indicating the presence and absence of gaps in the usage of a sequence S . Additionally , SQS contains an explicit penalty for the encoding of the set of patterns I ,
839 that encourages a smaller number of patterns . In a probabilistic model , this can be interpreted as a prior distribution p(I ) over patterns . There is also a prior distribution on the content of the patterns , similar to a unigram model , which encourages the patterns to contain more common elements . Similarly , GoKrimp uses a variant of the above model , where instead we have
PN P PN i=1 z
I∈I
πSm =
!m
.
( i )
S + |{T ∈ I | S ⊂ T}| j=1 z
I + |{T ∈ I | I ⊂ T}|
( j )
The differences between these models and ISM are :
In addition , the description length used by GoKrimp also has a gap cost that penalizes sequences with large gaps . GoKrimp employs a greedy heuristic to find the most compressing sequence : an empty sequence S is iteratively extended by the most frequent item that is statistically dependent on S . ISM , by contrast , iteratively extends sequences by the most frequent sequence in its candidate generation step which enables it to quickly generate large candidate sequences ( Section 36 ) We did consider performing a statistical test between a sequence and its extending sequence , however this proved computationally prohibitive . • Interleaving . SQS cannot mine subsequences that are interleaved and thus struggles on datasets which consist mainly of interleaved subsequences ( for illustration , see Section 44 ) GoKrimp handles interleaving using a pointer scheme that explicitly encodes the location of the subsequence within the database . In ISM , the partition function |P| allows us to handle interleaving of subsequences without needing to explicitly encode positions , and also serves as an additional penalty on the number of elements in the subsequences used to explain a database sequence . • Gap penalties . Both SQS and GoKrimp explicitly punish gaps in sequential patterns . Adding such a penalty would require only a trivial modification to the algorithm , namely , updating the cost function in Algorithm 1 . We did not pursue this as we observe excellent results without it ( Section 4 ) . • Encoding the set of patterns . Both SQS and GoKrimp contain an explicit penalty term for the description length of the pattern database , which corresponds to a prior distribution p(I ) over patterns . In our experiments with ISM , we did not find in practice that an explicit prior distribution p(I ) was necessary for good results . It would be possible to incorporate it with a trivial change to the ISM algorithm , in particular , when computing the score improvement of a new candidate in the structural EM step . • Encoding pattern absence . Also , observe that , if we view ISM as an MDL type method , not only the presence of a pattern , but also the absence of it is explicitly encoded ( in the form of πS0 in ( 31 ) ) As a result , there is an implicit penalty for adding too many patterns to the model and one does not need to use a code table which would serve as an explicit penalty for greater model complexity .
4 . NUMERICAL EXPERIMENTS
In this section we perform a comprehensive quantitative and qualitative evaluation of ISM . On synthetic datasets we show that ISM returns a list of sequential patterns that is largely non redundant , contains few spurious correlations and scales linearly with the number of sequences in the dataset . On a set of real world datasets we show that ISM
Figure 1 : ISM scaling as the number of sequences in our synthetic database increases . finds patterns that are consistent , interpretable and highly relevant to the problem at hand . Moreover , we show that ISM is able to mine patterns that achieve good accuracy when used as binary features for real world classification tasks . Datasets We use ten real world datasets in our numerical evaluation ( see Table 1 ) . The Alice dataset consists of the text of Lewis Carrol ’s Alice in Wonderland , tokenized into 1 , 638 sentences using the Stanford Document Preprocessor [ 17 ] with stop words and punctuation deliberately retained . The Gazelle dataset consists of 59 , 601 sequences of clickstream data from an e commerce website used in the KDD CUP 2000 competition [ 11 ] . The JMLR dataset consists of 788 abstracts from the Journal of Machine Learning Research and has previously been used in the evaluation of the SQS and GoKrimp algorithms [ 12 , 25 ] . Each sequence is a list of stemmed words from the text with stop words removed . The Sign dataset is a list of 730 American sign language utterances where each utterance contains a number of gestural and grammatical fields [ 21 ] . The last six datasets listed in Table 1 were first introduced in [ 19 ] to evaluate classification accuracy when mined sequential patterns are used as features . The datasets were converted from time interval sequences into sequences of items by considering the start and end of each unique interval as distinct items and ordering the items according to time . ISM Results We ran ISM on each dataset for 1 , 000 iterations with a priority queue size of 100 , 000 candidates . The runtime and number of non singleton sequential patterns returned by ISM is given in the right hand side of Table 1 . We
Dataset Uniq . Items 2 , 619 Alice 497 Gazelle 3 , 846 JMLR Sign 267 250 aslbu 94 aslgt 16 auslan2 context 94 178 pioneer skating 82
Sequences 1 , 638 59 , 601 788 730 424 3 , 464 200 240 160 530
Subseq.† Runtime 114 min 582 min 230 min 31 min 4 min 19 min >1 min 7 min 3 min 9 min
123 727 361 159 144 57 10 19 86 70
Table 1 : Summary of the real datasets used and ISM results after 1 , 000 iterations . † excluding singleton subsequences .
103104105106No . Sequences101102103104105Time ( s)840 original dataset due to the nature of our ‘subsequence interleaving’ generative model . This not only provides a good validation of ISM ’s inference procedure and underlying generative model but also demonstrates that ISM returns few spurious patterns . For comparison , SQS returned a very small set of generating patterns and GoKrimp returned many patterns that were not generating . The set of top k patterns mined by BIDE contained successively less generating patterns as k increased . It is not our intention to draw conclusions about the performance of the other algorithms as this experimental setup naturally favours ISM . Instead , we compare the patterns from ISM with those from SQS and GoKrimp on real world data in the next sections . 4.2 Pattern Redundancy
We now turn our attention to evaluating how redundant the sets of sequential patterns returned by ISM , SQS , GoKrimp and BIDE actually are . A suitable measure of redundancy for a single sequence is the minimum edit distance between it and the other mined sequences in the set . Averaging this distance across all sequences in the set , we obtain the average inter sequence distance ( ISD ) . Similarly , we can also calculate the average number of sequences containing other mined sequences in the set ( CS ) , which provides us with another measure of redundancy . Finally , we can also look at the number of unique items present in the set of mined sequences which gives us an indication of how diverse it is . We ran ISM , SQS , GoKrimp and BIDE on all the datasets in Table 1 and report the results of the three aforementioned redundancy metrics on the top 50 non singleton sequential patterns for each algorithm in Table 2 . One can see that on average the top ISM sequences have a larger inter sequence distance , smaller number of containing sequences and larger number of unique items , clearly demonstrating they are less redundant than SQS , GoKrimp and BIDE . Predictably , the top BIDE sequences are the most redundant , with an average inter sequence distance of 100 4.3 Classification Accuracy
A key property of any set of patterns mined from data is its usefulness in real world applications . To this end , in keeping with previous work [ 12 ] , we will focus on classification tasks as they are some of most important applications of pattern mining algorithms . Specifically we will consider the task of classifying sequences in a database using mined sequential patterns as binary features . We therefore performed 10 fold cross validation using a Support Vector Machine ( SVM ) classifier on the six classification datasets from Table 1 with the top k patterns mined by ISM , SQS , GoKrimp and BIDE as features . We used the linear classifier from the libSVM library [ 4 ] with default parameters . Additionally , we used the top k most frequent singleton patterns as a baseline for the classification tasks . The resulting plots of k against classification accuracy for all the datasets and algorithms are given in Figure 3 . One can see that the patterns mined by SQS perform best , exhibiting the highest classification accuracy on four out of the six datasets , closely followed by ISM and GoKrimp , which performs surprisingly well considering it struggles to return more than 50 patterns . All three consistently outperform BIDE and the singletons baseline which exhibit similar performance to each other . We therefore conclude that the sequential patterns mined by ISM can indeed be useful in real world applications .
Figure 2 : Precision against recall for each algorithm on our synthetic database , using the top k patterns as a threshold . Note that SQS is a single point at the top left and GoKrimp has near zero precision and recall . Each plotted curve is the 11 point interpolated precision3 . also investigated the scaling of ISM as the number of sequences in the database increases , using the model trained on the Sign dataset from Section 4.1 to generate synthetic sequence databases of various sizes . We ran ISM for 100 iterations on these databases and one can see in Figure 1 that the scaling is linear as expected . All experiments were performed on a machine with 16 Intel Xeon E5 2650 2.60Ghz CPUs and 128GB of RAM . Evaluation criteria We will evaluate ISM along with SQS , GoKrimp and BIDE according to the following criteria : 1 . Spuriousness – to assess the degree of spurious correlation in the mined set of sequential patterns .
2 . Redundancy – to measure how redundant the mined set
3 . Classification Accuracy – to measure the usefulness of the of patterns is . mined patterns .
4 . Interpretability – to informally assess how meaningful and relevant the mined patterns actually are .
4.1 Pattern Spuriousness
The sequence cover formulation of the ISM algorithm ( 3.1 ) naturally favours adding sequences to the model whose items co occur in the sequence database . One would therefore expect ISM to largely avoid suggesting sequences of uncorrelated items and so return more meaningful patterns . To verify this is the case and validate our inference procedure , we check if ISM is able to recover the sequences it used to generate a synthetic database . To obtain a realistic synthetic database , we sampled 10 , 000 sequences from the ISM generative model trained on the Sign dataset ( cf . Section 32 ) We were then able to measure the precision and recall for each algorithm , ie , the fraction of mined patterns that are generating and the fraction of generating patterns that are mined , respectively . Figure 2 shows the precision recall curve for ISM , SQS , GoKrimp and BIDE using the top k mined sequences ( according to each algorithms ranking ) as a threshold . One can clearly see that ISM was able to mine almost all the generating patterns and almost all the patterns mined were generating , despite the fact that the generated database will contain many subsequences not present in the 3ie , the interpolated precision at 11 equally spaced recall points between 0 and 1 ( inclusive ) , see [ 16 ] , §8.4 for details .
000204060810Recall000204060810PrecisionISMSQSGoKrimpBIDE841 real world larg scale high dimension state art first second reproduc kernel hilbert space maximum likelihood wide rang gene express princip compon analysi random field maximum entropi low dimension blind separ wide varieti acycl graph turn out markov chain leav out machin learn state art data set bayesian network larg scale nearest neighbor decis tree neural network cross valid featur select graphic model real world high dimension mutual inform sampl size learn algorithm logist regress model select machin learn real world state art high dimension reproduc hilbert space experiment result supervis learn neural network compon analysi well known support vector base result paper investig data demonstr hilbert space such paper learn result learn experi learn learn learn algorithm algorithm learn data data learn data model model problem problem learn result problem algorithm method method algorithm result data set learn learn learn learn problem learn method algorithm data learn set problem learn
Alice CS 0.00 0.10 0.10 0.36 aslgt CS 0.20 0.28 *0.00 0.00
Items 94 72 52 29
Items 94 86 *89 22
Gazelle CS 0.00 0.38 *0.05 0.36 auslan2 CS *1.0 *1.17 *0.25 *3.16
ISD 3.36 4.24 *4.51 1.00
ISD *2.40 *1.42 *2.00 *1.00
Items 167 183 *176 26
Items *14 *12 *8 *6
JMLR CS 0.00 0.02 *0.10 0.18 context CS *0.47 0.90 *0.52 1.72
ISD 1.84 1.82 *1.40 1.00
ISD *2.16 2.14 *2.07 1.00
Items 96 92 *30 12
Items *35 64 *51 12
Sign CS 0.00 0.94 0.24 0.60 pioneer CS 0.00 0.40 *0.00 0.06
Items 113 57 63 15
Items 102 78 *33 32
ISD 3.64 1.26 1.72 1.00
ISD 2.04 1.64 *1.82 1.02 aslbu CS 0.00 *0.11 *0.00 0.00 skating CS 0.72 0.84 *0.29 1.06
Items 110 *61 *18 26
Items 73 46 *64 17
ISD 2.24 *1.89 *2.00 1.00
ISD 2.12 1.62 *1.90 1.00
ISD 2.00 1.76 1.24 1.00
ISD 2.08 1.96 *2.00 1.00
ISM SQS GoKrimp BIDE
ISM SQS GoKrimp BIDE
Table 2 : Average inter sequence distance ( ISD ) , average no . containing sequences ( CS ) and no . unique items for the top 50 non singleton sequences returned by the algorithms from the datasets . Larger inter sequence distances and smaller no . containing sequences indicate less redundancy . * returned less than 50 non singleton sequences .
ISM
SQS
GoKrimp
BIDE support vector machin support vector machin support vector machin algorithm algorithm princip compon analysi algorithm demonstr algorithm algorithm algorithm
Table 3 : The top twenty non singleton sequences as found by ISM , SQS , GoKrimp and BIDE for the JMLR dataset .
4.4 Pattern Interpretability
For the two text datasets in Table 1 we can directly interpret the mined patterns and informally assess how meaningful and relevant they are . JMLR Dataset We compare the top 20 non singleton patterns mined by ISM , SQS , GoKrimp and BIDE in Table 3 . It is immediately obvious from the table that the BIDE patterns are almost exclusively permutations of frequent items and so uninformative . For this reason we omit BIDE from consideration on the next dataset . The patterns mined by ISM , SQS and GoKrimp are all very informative , containing technical concepts such as support vector machine and commonly used phrases such as state ( of the ) art . Alice Dataset We compare the top twenty 20 non singleton patterns mined by ISM , SQS and GoKrimp in the first three columns Table 4 . This time , one can clearly see that the patterns mined by ISM are considerably more informative . They contain collocated words and phrases such as mock turtle and oh dear , correlated words such as as spoke and off head , as well as correlated punctuation such as ( ) and “ ” . Both SQS and GoKrimp on the other hand mine collocated words with spurious punctuation and stop words , eg prepending the to nouns and commas to phrases . To further illustrate this notable difference , we also show the top 20 non singleton patterns that are exclusive to each algorithm ( ie , found by ISM but not SQS/GoKrimp , etc . ) in the last three columns of Table 4 . One can clearly see that GoKrimp has the least informative exclusive patterns , predominantly combinations of stop words and punctuation , SQS mostly prepends and appends informative exclusive patterns with punctuation and stop words , whereas ISM is the only algorithm that just returns purely correlated words . Note that SQS in particular struggles to return patterns such as balanced parentheses , since it punishes the large gaps between them and cannot handle interleaving them with the patterns they enclose . Here we can really see the power of the statistical model underlying ISM as it is able to discern spurious punctuation from genuine phrases . Parallel Dataset Finally , we consider a synthetic dataset that demonstrates the ability of ISM to handle interleaving patterns . Following [ 12 ] , we generate a synthetic dataset where each item in the sequence is generated by five independent parallel processes , ie , each process i generates one item from a set of five possible items {ai , bi , ci , di , ei} in or
842 Figure 3 : Linear SVM classification accuracy using the top k sequences returned by each algorithm as binary features . ISM shows consistently good performance , comparable to SQS and GoKrimp .
ISM she herself mock turtle
( ) went on
“ ” ca n’t he his looked at had been must be at last as spoke looking at had back just when off head she at once oh dear more than never before
SQS ! ’ , and ? ’ . ’ the mock turtle the march hare
* * * *
, ’ said alice the queen , you know went on it was the white rabbit
, ’ ; and a little i ’m do n’t she had beau – ootiful soo – oop !
GoKrimp
Exclusive ISM
Exclusive SQS
Exc . GoKrimp
‘ ’ , and , said . mock turtle said the . in a of the . i n’t ’ i ’m march hare went on a little
! ’ alice . to herself as she what ? you know the queen the hatter she herself
( ) he his as spoke had back just when off head she at once oh dear never before join dance might well if ’d such thing ’ve seen going into too much soon found took its do n’t know what
? ’ the mock turtle the march hare
* * * *
, ’ said alice , you know it was the white rabbit
, ’ ; and she had i ’ve minute or two there was ‘ well ,
– ’ in a tone soo – oop of the e – e –
, ’ said the king beau – ootiful soo – oop !
‘ ’
, said . said the . of the . i n’t ’ march hare alice . what ? you know
‘ ! you ? , , , oh , ! i ’ alice ;
, ! the . alice , it : and she
Table 4 : The top twenty non singleton sequences as found by ISM , SQS and GoKrimp for the Alice dataset as well as those found by ISM but not SQS/GoKrimp , SQS not ISM/GoKrimp and GoKrimp not ISM/SQS .
020406080100top k046048050052054056058060062Classification AccuracyaslbuISMSQSGoKrimpBIDESingletons020406080100top k030405060708Classification Accuracyaslgt020406080100top k022024026028030032034Classification Accuracyauslan2020406080100top k020304050607080910Classification Accuracycontext020406080100top k070075080085090095100Classification Accuracypioneer020406080100top k018020022024026028030Classification Accuracyskating843 [ 5 ] V . Chvátal . A greedy heuristic for the set covering problem . Math . OR , 4(3):233–235 , 1979 .
[ 6 ] A . Dempster , N . Laird , and D . Rubin . Maximum likelihood from incomplete data via the EM algorithm . Journal of the Royal Statistical Society : Series B , pages 1–38 , 1977 .
[ 7 ] U . Feige . A threshold of ln n for approximating set cover .
Journal of the ACM , 45(4):634–652 , 1998 .
[ 8 ] N . Friedman . The Bayesian structural EM algorithm . In
UAI , pages 129–138 , 1998 .
[ 9 ] R . Gwadera , M . J . Atallah , and W . Szpankowski . Markov models for identification of significant episodes . In SDM , pages 404–414 , 2005 .
[ 10 ] N . Jay , G . Herengt , E . Albuisson , F . Kohler , and A . Napoli .
Sequential pattern mining and classification of patient path . In MEDINFO , page 1667 , 2004 .
[ 11 ] R . Kohavi , C . E . Brodley , B . Frasca , L . Mason , and
Z . Zheng . KDD Cup 2000 organizers’ report : Peeling the onion . SIGKDD Explorations Newsletter , 2(2):86–93 , 2000 .
[ 12 ] H . T . Lam , F . Moerchen , D . Fradkin , and T . Calders .
Mining compressing sequential patterns . Statistical Analysis and Data Mining , 7(1):34–52 , 2014 .
[ 13 ] N . Landwehr . Modeling interleaved hidden processes . In
ICML , pages 520–527 , 2008 .
[ 14 ] D . J . C . MacKay . Information Theory , Inference , and
Learning Algorithms . Cambridge University Press , 2003 .
[ 15 ] H . Mannila and C . Meek . Global partial orders from sequential data . In KDD , pages 161–168 , 2000 .
[ 16 ] C . D . Manning , P . Raghavan , and H . Schütze . Introduction to Information Retrieval . Cambridge University Press , 2008 .
[ 17 ] C . D . Manning , M . Surdeanu , J . Bauer , J . Finkel , S . J .
Bethard , and D . McClosky . The Stanford CoreNLP natural language processing toolkit . In ACL System Demonstrations , pages 55–60 , 2014 .
[ 18 ] B . Mobasher , H . Dai , T . Luo , and M . Nakagawa . Using sequential and non sequential patterns in predictive web usage mining tasks . In ICDM , pages 669–672 , 2002 . [ 19 ] F . Moerchen and D . Fradkin . Robust mining of time intervals with semi interval partial order patterns . In SDM , pages 315–326 , 2010 .
[ 20 ] C . G . Nevill Manning and I . H . Witten . Identifying hierarchical structure in sequences : A linear time algorithm . Journal of Artificial Intelligence Research , 7:67–82 , 1997 . [ 21 ] P . Papapetrou , G . Kollios , S . Sclaroff , and D . Gunopulos .
Discovering frequent arrangements of temporal intervals . In ICDM , pages 82–89 , 2005 .
[ 22 ] J . Pei , J . Han , B . Mortazavi Asl , H . Pinto , Q . Chen ,
U . Dayal , and M C Hsu . PrefixSpan : Mining sequential patterns efficiently by prefix projected pattern growth . In ICDE , pages 0215–0215 , 2001 .
[ 23 ] R . Srikant and R . Agrawal . Mining sequential patterns :
Generalizations and performance improvements . In EDBT , pages 3–17 , 1996 .
[ 24 ] A . Stolcke and S . Omohundro . Hidden Markov model induction by Bayesian model merging . In NIPS , pages 11–18 , 1993 .
[ 25 ] N . Tatti and J . Vreeken . The long and the short of it : summarising event sequences with serial episodes . In KDD , pages 462–470 , 2012 .
[ 26 ] J . Wang and J . Han . BIDE : Efficient mining of frequent closed sequences . In ICDE , pages 79–90 , 2004 .
[ 27 ] M . Wang , X Q Shang , and Z H Li . Sequential pattern mining for protein function prediction . In ADMA , pages 652–658 . Springer , 2008 .
[ 28 ] F . Wood , J . Gasthaus , C . Archambeau , L . James , and
Y . W . Teh . The sequence memoizer . Communications of the ACM , 54(2):91–98 , 2011 .
[ 29 ] M . J . Zaki . SPADE : An efficient algorithm for mining frequent sequences . Machine Learning , 42(1 2):31–60 , 2001 .
[ 30 ] H . Zhong , T . Xie , L . Zhang , J . Pei , and H . Mei . MAPO :
Mining and recommending API usage patterns . In ECOOP , pages 318–343 . 2009 .
Figure 4 : Recall for each algorithm on the synthetic parallel dataset , using the top k ( first k for SQS ) patterns as a threshold . Note that SQS maintains a recall level of 0.6 for the remaining patterns ( up to k = 403 , not shown for clarity ) . der . In each step , the generator chooses i at random and generates an item using process i , until the sequence has length 1 , 000 , 000 . The sequence is then split into 10 , 000 sequences of length 100 . For this dataset we know that all mined sequences containing a mixture of items from different processes are spurious . This enables us to calculate recall , ie , the fraction of processes present in the set of true patterns mined by each algorithm . We plot the recall for the top k patterns mined by ISM and GoKrimp in Figure 4 and the first k patterns mined by SQS ( as it was still running after seven days ) . One can see that while ISM and GoKrimp are able to mine true patterns from all processes , SQS only returns patterns from 3 of the 5 processes .
5 . CONCLUSIONS
In this paper , we have taken a probabilistic machine learning approach to the subsequence mining problem . We presented a novel subsequence interleaving model , called the Interesting Sequence Miner , that infers subsequences which best compress a sequence database without having to design a MDL encoding scheme . We demonstrated the efficacy of our approach on both synthetic and real world datasets , showing that ISM returns a more diverse set of patterns than previous approaches while retaining comparable quality . In the future we would like to extend our approach to the many promising application areas as well as considering more advanced techniques for parallelization .
Acknowledgments This work was supported by the Engineering and Physical Sciences Research Council ( grant number EP/K024043/1 ) .
References [ 1 ] C . Aggarwal and J . Han . Frequent Pattern Mining .
Springer , 2014 .
[ 2 ] R . Agrawal and R . Srikant . Mining sequential patterns . In
ICDE , pages 3–14 , 1995 .
[ 3 ] J . Ayres , J . Flannick , J . Gehrke , and T . Yiu . Sequential pattern mining using a bitmap representation . In KDD , pages 429–435 , 2002 .
[ 4 ] C C Chang and C J Lin . LIBSVM : A library for support vector machines . ACM TIST , 2:27:1–27:27 , 2011 . http://wwwcsientuedutw/~cjlin/libsvm
01020304050top k000204060810RecallISMSQSGoKrimp844
