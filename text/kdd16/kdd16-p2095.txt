Efficient Shift Invariant Dictionary Learning
Guoqing Zheng
School of Computer Science Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh , PA 15213 , USA gzheng@cscmuedu
Yiming Yang
School of Computer Science Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh , PA 15213 , USA yiming@cscmuedu
Jaime Carbonell
School of Computer Science Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh , PA 15213 , USA jgc@cscmuedu
ABSTRACT Shift invariant dictionary learning ( SIDL ) refers to the problem of discovering a set of latent basis vectors ( the dictionary ) that captures informative local patterns at different locations of the input sequences , and a sparse coding for each sequence as a linear combination of the latent basis elements . It differs from conventional dictionary learning and sparse coding where the latent basis has the same dimension as the input vectors , where the focus is on global patterns instead of shift invariant local patterns . Unsupervised discovery of shift invariant dictionary and the corresponding sparse coding has been an open challenge as the number of candidate local patterns is extremely large , and the number of possible linear combinations of such local patterns is even more so . In this paper we propose a new framework for unsupervised discovery of both the shift invariant basis and the sparse coding of input data , with efficient algorithms for tractable optimization . Empirical evaluations on multiple time series data sets demonstrate the effectiveness and efficiency of the proposed method .
CCS Concepts •Computing methodologies → Factor analysis ; Learning latent representations ;
Keywords dictionary learning ; sparse coding ; time series
1 .
INTRODUCTION
Sparse representation models , such as those in dictionary learning , have been proposed for obtaining both a succinct set of vectors as the new basis ( also called the dictionary ) from unlabeled input sequences , and for representing each sequence as a sparse linear combination of the basis elements , ie , the sparse coding of the data . Successful applications include those in dimensionality reduction [ 6 ] , image restoration [ 13 , 2 , 18 ] , signal compression [ 15 , 17 ] and compressed sensing [ 4 , 9 ] .
Conventional dictionary learning models assume that the induced basis is of the same dimensionality of the input data , thus the basis elements ( vectors ) and the input sequences ( also vectors ) are strictly aligned from coordinate to coordinate . This simplistic assumption enables efficient algorithms for finding the optimal basis from input data ; however , it also significantly limits the scope of potential applications of those models . Consider the three time series example from [ 24 ] reproduced in Figure 1 for instance . Time series No . 1 and series No . 3 share the bumps ( as plotted in red ) which are similar in shape but appear at different locations . We would consider this pair ( 1 and 3 ) more similar than the other pairs but such similarity cannot be captured , obviously , using a global non shift invariant pattern ( corresponding to a basis element ) with the full span over the time series . On the other hand , such similarity could be captured if we allow the basis elements to have a shorter span and to occur at different locations in the time series . Such a desirable kind of basis is called shift invariant basis , and finding such a basis from data is referred as solving the shift invariant dictionary learning ( SIDL ) problem .
Figure 1 : Importance of local patterns ( plotted in red ) versus that of the entire time series .
As a challenging problem with a potentially broad range of applications , SIDL has received increasing attention in recent machine learning research [ 8 , 23 , 14 , 24 ] . Existing work and potential strengths/limitations can be grouped and outlined as follows : First , methods such as [ 24 ] , which focus on exhaustive or heuristic search over all possible local pattern candidates , are either highly inefficient or sub optimal , or both . Second , methods such as [ 7 ] , which rely on the availability of sufficiently labeled data in order to learn discriminant local patterns , are not applicable when labeled data are lacking or are extremely sparse . Third , methods such as [ 8 ] , which use convolution to model invariant shift of local patterns , has the drawback of not offering any method for sparse coding of input data . In other words , those methods are capable of learning shift invariant local patterns as a new
1232095 basis , but lack of the ability to combine the shift invariant patterns in a sparse coding of the original data . Recall that sparse coding is a highly desirable for scalable data analysis tasks ( including classification and clustering of time series ) , as well as for the interpretability of the analysis and the robustness of system predictions .
In this paper , we propose a new approach to SIDL which combines the strengths and addresses the aforementioned limitations of the previous methods . Specifically , our model allows the basis elements to be much shorter than the length of input series , to occur at different locations in the input series , and to be optimally combined into a sparse coding of observed data , thus representing each input series as as a sparse linear combination of the short basis elements . More importantly , both the shift invariant basis and the sparse coding are jointly optimized in an unsupervised learning framework with our new algorithms for highly efficient computation . Empirical results on multiple times series demonstrate the effectiveness and efficiency of the proposed approach .
2 . PRELIMINARIES Suppose we have a set of n input data points with dimension p , {xi ∈ Rp}n i=1 , we want to learn a set of K basis elements , ie , a dictionary , D = [ d1 , , dK ] in Rq×K and a sparse coding αi ∈ RK for each input data point xi . In classic sparse encoding and dictionary learning , the dimension of the basis is the same as that of the data point , ie q = p . 2.1 Sparse Coding
Given a fixed dictionary D , for an input data point sparse encoding aims to find a sparse linear combination of the basis vectors as its representation . Formally , sparse coding with 1 regularization amounts to solving arg min α∈RK
1 2 x − Dα2
2 + λα1
( 1 ) where λ is controls the balance between restoration error and coding sparsity . It is well known that 1 penalty yields a sparse solution , and the above LASSO problem can be efficiently solved with coordinate descent [ 19 , 20 ] . Other sparsity penalties such as 0 regularization can be used well , however solving sparse coding with 0 penalty is often intractable . In this paper , we focus on the 1 regularized setting . 2.2 Dictionary Learning
When we lack a pre existing dictionary a priori or we wish the dictionary to reflect directly the properties of the data , dictionary learning can induce both the sparse coding for the input data points and the dictionary by solving the following optimization problem st dk2 ≤ c , for k = 1 , , K
( 2 ) where c is a constant and the norm constraint on dj is necessary to avoid degenerate solutions . The above problem is not convex in ( D , α ) jointly , but is convex if we fix either variable . Hence alternate optimization between both sets of n i=1
1 2 arg min D∈Rp×K αi∈RK xi − Dαi2
2 + λαi1 variables is a common approach to address dictionary learning .
When D is fixed , the above problem is essentially a sparse coding as described in Section 21 When all αs are fixed , the above problem is quadratic programming with quadratic constraints ( QCQP ) and there exist many algorithms to solve it efficiently .
3 . SHIFT INVARIANT DICTIONARY LEARN
ING
In this section , we present our shift invariant dictionary learning ( SIDL ) to capture both the locality of representative patterns as well as preserve a sparse representations for the data points .
Unlike classic dictionary learning which enforces the same dimensionality for the basis and the data point ( q = p ) , shift invariant dictionary relaxes this constraint by q ≤ p , allowing the basis to slide along the support of the data point . For real problems , we may often set q to be much smaller than p ; however we emphasize that classic DL is a special case of SIDL with the setting q = p . For a data point xi ∈ Rp and shift invariant basis dk ∈ Rq , we introduce a variable tik to denote the location1 where dk is matched to xi , with tik = 0 indicating that dk is aligned to the beginning of xi and that tik = p − q indicating the largest shift dk can be aligned to xi without running beyond the boundary of xi , as shown in Figure 3 . Hence the possible values for tik are all integers2 in [ 0 , p − q ] .
( a ) Basis d is aligned to the beginning of x , ie the shift variable t = 0
( b ) Basis d is aligned to the end of x , ie the shift variable t = p − q
Figure 2 : The same basis with different shifts
Obviously , shifting a basis element d by an amount of t is
1The idea of shift invariant basis can also be applied to data with more than one “ directions ” , such as image . For example , for images , the basis now tunrs to be a rectangular area and the shift is represented by two variables on each direction , the idea proposed in this paper can be easily extended to this case . For brevity , we focus on data with only one “ direction ” in this paper and leave the extensions as future work . 2As we only consider “ discretized ” data points , the shift can only take integer values .
2096 equivalent to defining a new vector as Tp(d , t ) v ∈ Rp where vi = di−t 0 if 1 ≤ i − t ≤ q otherwise
( 3 )
( 4 )
Given a set of input data points {xi}n i=1 , shift invariant dictionary learning aims to learn the dictionary D = [ d1 , , dK ] , the sparse coefficients {αik} and the corresponding shifts {tik} entirely from the data in an unsupervised way :
Algorithm 1 Shift invariant sparse coding
Input : data point x ∈ Rp , dictionary D = [ d1 , , dK ] Output : sparse coding α∗ , matching offsets {t∗ Initialize α randomly repeat k}K k=1 j=k αjT ( dj , tj ) for k = 1 to K do x ← x − tk ← Eq ( 9 ) αk ← Eq ( 10 ) end for until convergence n i=1
1 2 flflflflflxi − K k=1
SIDL : arg min D∈Rq×K αi∈RK tik∈[0,p−q ] flflflflfl2
2
αikT ( dk , tik ) st dk2 ≤ c , for k = 1 , , K
( 5 ) is n i=1
+ λ
αi1
( 6 ) . However , by plugging Eq ( 7 ) back to Problem ( 6 ) and with simple math manipulation , we arrive at the following theorem which is much more efficient to compute .
4 . MODEL LEARNING FOR SIDL
In this section , we present an efficient algorithm to solve SIDL . Similar to classic DL , the SIDL problem is non convex ; hence , we employ an anternative optimization scheme . 4.1 Shift invariant sparse coding
Given the dictionary D fixed , Problem ( 5 ) turns to solving for the basis matching location tik and the the coefficients α . Also note that with D fixed , Problem ( 5 ) can be decomposed to learning the coefficients and the basis shift for every xi independently . Hence in this subsection we drop the subscript and simply use x to represent any single input data point from the data set for clarity . To learn α and {tk} for input x , we adopt coordinate descent to estimate the coefficients α and a greedy approach to estimate {tk} . Minimizing over αk and tk with {αj}j=k and {tj}j=k fixed : arg min
αk,tk
1 2
αkT ( dk , tk ) +
αjT ( dj , tj ) − x2 + λ|αk|
( 6 ) j=k
αk = S λdk2 k)T ( dk , t∗ k ) wherex
= S λdk2 x − is the residue of fitting x with all basis except for dk and S(· ) is the shrinkage operator defined as j=k αjT ( dj , tj )
( 7 ) dk2
T ( dk , t∗
T ( dk , t∗ k)x T ( dk , t∗ k)x
x − a if x ≥ a
0 x + a if x ≤ −a
Sa(x ) = if x ∈ ( −a , a )
( 8 )
Proposition 41 The optimal solution for Problem ( 6 )
∗ k = arg max t tk
|T ( dk , tk ) x| k)x| > λ k)x if |T ( dk , t∗ otherwise and
∗ k = a
T ( dk , t∗ 0
( 9 )
( 10 ) fi Proof . See Appendix A . Proposition 4.1 suggests that we only need to compute the dot product between the basis element and the segment from the input data point ; all updates are exact which do not involve parameter tuning . Algorithm 1 outlines the suggested procedure for solving shift invariant sparse coding . 4.2 Shift invariant dictionary update
When the coefficients α and basis shifts {tik} are fixed , updating the dictionary requires solving the following optimization problem : flflflflfl2
αikT ( dk , tik ) k i=1
1 2 n n arg min d1,,dk∈Rq st dk2 ≤ c,∀k ∈ [ 1 , K ] flflflflflxi − K flflflflflflxi − αikT ( dk , tik ) − K n wherexi xi −K st dk2 ≤ c arg min dk∈Rq
L(dk , u ) = j=k
1 2
1 2 i=1 i=1
This is a least square problem with quadratic constraints , which we can solve via its dual problem . The Lagrangian is : xi − αikT ( dk , tik)2 + u(dk2 − c )
αijT ( dk , tij )
( 11 ) flflflflflfl2
( 12 )
( 13 )
( 14 )
For this one dimensional optimization problem , its solution for αk ( as a function of the optimal basis shift t∗ k ) is
Coordinate descent is used to solve for d as well , when minimizing over dk with {dj}j=k fixed :
The above solution for αk depends on the shift tk of the kth basis element , and since it can only take integer values from [ 0 , p − q ] , a naive approach is to enumerate all possible tk , compute the corresponding αk and pick the pair of ( t∗ k ) which yields the minimum objective defined in k , α∗ j=k αijT ( dk , tij ) is the residue for xi and u ≥ 0 is the Lagrangian multiplier . Minimizing L(dk , u ) over dk , we get an analytic form for d∗ n i=1 αikx[1+tik,q+tik ] 2u +n k as i=1 α2 ik i
∗ d k =
2097 Algorithm 2 Shift invariant dictionary update i=1 , sparse coding α , matching
Input : data point {xi}n offsets {tk}K Output : dictionary D = [ d1 , , dK ] Initialize D randomly repeat k=1 x ←n i=1 αikx[1+tik,q+tik ] i for k = 1 to K do dk ← Eq ( 15 ) end for until convergence
Algorithm 3 SIDL i=1 , desired basis dimension q ,
Input : data points {xi}n desired number of basis K Output : dictionary D = [ d1 , , dK ] , sparse coding {αi}n Initialize D , {αi}n repeat i=1 , basis shifting location {tik} i=1 and {tik} randomly
Sparse coding : call Alg . 1 Dictionary updating : call Alg . 2 until convergence denotes the slice ofxi from index 1 + tik wherex[1+tik,q+tik ] of all residues {xi} . Plugging it back to the Lagrangian and through q + tik . Eq ( 14 ) suggests that the optimum dk is a weighted average of the corresponding matching segments i maximizing the dual problem we finally get
 xn x otherwise if c
√ xx 1n i=1 αikx[1+tik,q+tik ] i=1 α2 ik i=1 α2 ik
∗ d k = wherex n
≥ √ c
( 15 ) i
. Refer to Appendix B for a detailed proof of Eq ( 15 ) . Algorithm 2 outlines the learning procedure for shift invariant dictionary updating . 4.3 Complete algorithm
Given unlabeled input data points , SIDL alternates between optimization of the sparse coding and updating the dictionary . Algorithm 3 outlines the learning procedure for shift invariant ditionary updating . 431 Algorithm analysis The time complexity of SIDL consists of two parts , one for shift invariant sparse coding and the other for shift invariant dictionary update . For shift invariant sparse coding ( Algorithm 1 ) , the cost for one outer iteration , ie , finding the optimum shifting locations and basis coefficients for K basis , is O(Kq(p− q + 1) ) , where O(q ) comes from the cost of computing inner product of two vectors of length q and p− q + 1 is the possible integer scope for solving tk by Proposition 4.1 , hence the total complexity for one call to Algorithm 1 is O(M1Kq(p − q ) ) where M1 is the maximum number of iterations allowed in Algorithm 1 .
The core part of shift invariant dictionary update ( Algorithm 2 ) is the computing for dk , which takes O(nq ) oper ations to compute thex and scale it to get dk according to
Eq ( 15 ) . Thus the total complexity for one call to Algorithm 2 is O(M2Knq ) where M2 is the maximum number of iterations allowed in Algorithm 2 .
In fact , we do not necessarily require Algorithm 1 and 2 to converge in every call from 3 , since as long as the objective function decreases in both shift invariant sparse coding and shift invariant dictionary update , the entire algorithm is still guaranteed to converge . Hence M1 and M2 can be set to be quite small , eg , 10 to 20 iterations in both Algorithm 1 and 2 would suffice . Therefore , the total time complexity for SIDL is O(M Kq(n + p − q) ) , where M is the total number of iterations allowed or needed for Algorithm 3 to converge . 432 Comparison to classical dictionary learning As we mentioned , classical dictionary learning is a special case of the proposed framework of SIDL by setting q = p , hence the time complexity for classical dictionary learning is O(M Kpn ) . By comparing the complexity of the proposed SIDL framework to classical DL , because
M Kq(n + p − q ) < M Knp
( 16 ) holds for any q < min(n , p ) , hence in terms of asympotic complexity , the proposed SIDL is more efficient than standard classical DL in terms of time complexity . If q is close to min(n , p ) , then the proposed SIDL will be of about the same compelexity in computation . Empirical timing of SIDL can also be found in Section 53
5 . EXPERIMENTS
In this section , we evaluate the proposed Shift Invariant Dictionary Learning ( SIDL ) from two aspects . One is to investigate its performance as a data reconstruction algorithm in reconstructing unseen data with the dictionary learned from training data , and the other is to evaluate the quality of learned sparse representations as features for downstream tasks , specifically for classification . 5.1 Data sets
Table 1 : Dataset information
Dataset
#(Training ) #(Testing )
Length #(classes )
Trace synthetic control ECGFiveDays Gun Point MedicalImages Chlorine .
100 300 23 50 381 467
100 300 861 150 760 3840
275 60 136 150 99 166
4 6 2 2 10 3
Before presenting our experimental results , we first list the data sets on which SIDL is performed . We use several real world time series data sets in all evaluations from the UCR time series archives3 [ 5 ] , which are listed below with brief descriptions .
• Trace : Synthetic dataset designed to simulate failures in nuclear power plants ;
• Synthetic Control : Synthetically generated charts by the process in [ 1 ] ;
• ECGFiveDays : Measurements of the abnormality of heartbeat recorded by an electrode ;
• GunPoint : Video tracking data of a human test subject ’s right hand , recording whether or not he/she is pulling out a gun ;
3http://wwwcsucredu/˜eamonn/time series data/
2098 • MedicalImages : Histograms of pixel intensity of med ical images of different human body regions ;
• ChlorineConcentration : Chlorine concentration in drinking water versus time collected from eight households .
Table 1 lists related statistics about the data sets mentioned above . 5.2 SIDL on data reconstruction
We first investigate how SIDL can be used as a data reconstruction method by learning the shift invariant basis on the training set and use the learned dictionary to encode the signals from the test set . We report the reconstruction error on all data sets , as well how its performance changes with different values of the sparsity regularization parameter λ , basis vector length q and dictionary size K .
We train SIDL with different parameters on the training portion of each data set and use the learned dictionary to reconstruct the testing data . Specifically , on the training data , we use SIDL the learn the shift invariant dictionary D , and then given the testing data points , we solve for their sparse codings with D fixed . The Mean Squared Error ( MAE ) for reconstruction is measured as flflflflflxi − K k=1 ntest i=1
M AE =
1 ntest flflflflfl2
∗ ikT ( dk , t
∗ ik )
α
( 17 ) ik and t∗ where α∗ ik are obtained by solving the shift invariant sparse coding problem as described in Section 4.1 with dks trained from the training data points .
Figure 3 presents the complete reconstruction performance of SIDL on all time series data sets with different parameter configurations . It ’s worth noting that when q p equals 1 , our proposed model reduces to classical dictionary learning . The dictionary sizes in Figure 3 are all set to be K = 50 as we have examined different values of K and observed similar patterns , so we omit the detailed graphs .
On five out of the six datasets , we can see that SIDL actually achieves ( ie when q p < 1 ) significantly lower reconstruction error than classical dictionary learning , given the same number of basis elements and same sparsity regularization strength . This suggests that not only SIDL produces a better dictionary to generalize to unseen data but also it results in smaller dictionary ( since our basis elements are shorter than those of classical dictionary learning ) . Results from Figure 3 demonstrate that SIDL can be an effective alternative to yield unsupervised sparse representations compared to classical dictionary learning .
The above results show the reconstruction performance of SIDL doing a grid search of the parameters . When applied pratically , the optimal choice of q p and the degree of sparsity regularization λ can be obtained via cross validation on splits from the training set , as we will show in the evaluation for using codings from SIDL as features for classification in Section 54 5.3 Computational efficiency of SIDL
We plot the running time of SIDL with different parameter setting in Figure 4 . As disscussed in Section 432 , when the length of the basis is quite smaller than that of the input signal , SIDL will be much faster than classical DL , such as q = 0.1p and p = 0.25p in Figure 4 . When q gets closer to p , as we can see from 4 , the running time for SIDL is getting closer to that of DL , especially for larger K .
We also plot how the training objective of SIDL and DL decreases in terms of iterations . We set the convergence threshold , which is defined as the relative function objective change , to be 10−5 , ie convergence is achieved when
|F ( i+1 ) − F ( i)|
F ( i )
≤ 10
−5
( 18 ) where F ( i ) is the objective function for training after the ith iteration . It ’s clear to see when λ = 1 , all the SIDL runs converge in fewer iterations than DL , except for q = 0.75p which takes about 200 more iterations to converge . When λ = 10 , which encourages sparser solutions , all the SIDL runs converge in fewer iterations than DL . These imply that the proposed SIDL method can be at least as efficient as classical DL , with the flexibility to model shift invariant basis present in the data .
It ’s also worth emphasizing that , from Figure 4 , although the training loss minimum DL achieved is slightly better than those of SIDLs , this does not necessarily mean that the basis found by DL is any better than those found by SIDL . In fact , as we have shown in the data restruction and we will show in the classification tasks , the basis found by SIDL actually are better than those found by DLs . 5.4 Sparse coding from SIDL as features for classification
In this section , we evaluate the encodings output by SIDL as feature representations for time series classification tasks . For all data sets , we train the dictionary on the training set and apply it onto the test data points to get their encodings . The dictionary size K , the length of basis element q and the sparsity regularization parameter λ are chosen via 3 fold cross validation on the training set . We compare the classification results using sparse coding output by SIDL versus using raw input as features . Below is a list of classification algorithms we used for the evaluation .
• 1 Nearst Neighbor with Euclidean distance ( 1NN Euclidean ) .
It is widely accepted in the time series mining community that 1NN with eucledian distance [ 22 ] is a strong though naive baseline . We run this algorithm on the raw representation of the time series .
• 1 Nearst Neighbor with Dynamic Time Warping ( 1NNDTW ) . It is by far a strong benchmark method for time series classfication . The distance of two time series is computed by performing DTW on the pair .
• Support Vector Machine with raw representation ( SVMRaw ) . This method trains an SVM classifier based on the raw representation of time series , without any transformation or processing of the input signals . We use the libsvm package to implement this method [ 3 , 21 ] .
• Support Vector Machine with classical dictionary learning features ( SVM DL ) . This method trains an SVM classifier based on the encodings from classical dictionary learning and then makes predictions on the encodings of testing data .
• Support Vector Machine with SIDL ( SVM SIDL ) . We train SVM on the shift invariant sparse codings given
2099 Figure 3 : Reconstruction performance of SIDL wrt sparsity regularization parameter λ and the relative length of the basis element q p = 1 denotes the classical dictionary learning setting . p and a fixed dictionary size of K = 50 . Note that q
( a ) Time taken for SIDL with different q and DL to converge ( convergence threshold = 10−5 ) when trained on the Trace data set with sparsity regularization parameter λ = 1 .
( b ) Training loss versus iteration for SIDL and DL when trained on the Trace data set with sparsity regularization parameter λ = 1 . ( convergence threshold = 10−5 )
( c ) Training loss versus iteration for SIDL and DL when trained on the Trace data set with sparsity regularization parameter λ = 10 . ( convergence threshold = 10−5
Figure 4 : Running time comparisons by SIDL , and report the performance on the encodings of the testing data points with the learned shiftinvariant dictionary .
Table 2 presents the classification accuracies of various classification methods of SIDL on all data sets , with comparison to the baseline method . One interesting result to point out is that , without sparse encodings ( such as DL and SIDL ) , classification with raw time series representations works always worse than naive 1NN method with Euclidean distance except on the ECGFiveDays data set . This suggests for data with temporal dependencies , such as time series , using raw representation as features for SVM is not a good idea .
Nevertheless , sparse codings of the sequential data does help improve classification , compared to SVM Raw .
SVM SIDL achieves either the best or second best results on five data sets out of six , demonstrating the benefits of modeling shift invariant local patterns in the data and though classical dictionary learning alleviate the problem suffered by SVM Raw , it still lacks the flexibility to model local patterns . This further validates our motivation that a method that models local patterns in sequential data will better represent and explain the data .
As also validated by previous literature , 1NN DTW is indeed a strong baseline method for time series classification , achieving three times of best and once of second best ac
020304050607080910qp0005101520253035MSETrace(K=50)λ=01λ=10λ=100020304050607080910qp0246810MSEsyntheticcontrol(K=50)λ=01λ=10λ=100020304050607080910qp000510152025MSEGunPoint(K=50)λ=01λ=10λ=100020304050607080910qp0005101520253035MSEECGFiveDays(K=50)λ=01λ=10λ=100020304050607080910qp00051015202530MSEMedicalImages(K=50)λ=01λ=10λ=100020304050607080910qp0123456MSEChlorineConcentration(K=50)λ=01λ=10λ=100K020406080100Time to converge ( s)02000400060008000SIDL ( q=0.10p)SIDL ( q=0.25p)SIDL ( q=0.50p)SIDL ( q=075p)DLiteration0200400600800avg training loss100101102SIDL ( q=0.10p)SIDL ( q=0.25p)SIDL ( q=0.50p)SIDL ( q=075p)DLiteration050100150avg training loss101102103SIDL ( q=0.10p)SIDL ( q=0.25p)SIDL ( q=0.50p)SIDL ( q=0.75p)DL2100 Table 2 : Classification accuracies ( Best results are printed in both bold and italic ; second best results are printed in bold only . All parameters of SIDL are selected via 3 fold cross validation on splits of the training data with the following range K ∈ {10 , 20 , 50 , 100} , λ ∈ {0.1 , 1 , 10 , 100} , q ∈ {0.1p , 0.25p , 0.5p , 075p} The same parameter ranges are also used for parameter tunning for DL . The parameter C for all linear SVMs are chosen through cross validation from {0.001 , 0.01 , 0.1,1,10,100,1000} )
Dataset Trace synthetic control GunPoint ECGFiveDays MedicalImages Chlorine .
1NN Euclidean 0.760 0.880 0.913 0.797 0.684 0.650
1NN DTW SVM Raw SVM DL 0.680 0.960 0.933 0.497 0.653 0.533
1.000 0.993 0.907 0.768 0.737 0.648
0.430 0.930 0.747 0.965 0.649 0.533
SVM SIDL 0.950 0.967 0.953 0.999 0.688 0.533 curacies over all data sets , while the proposed SVM SIDL twice of best and three times of second best . It ’s worth mentioning that the proposed method is formulated to reduce reconstruction error , instead of classification error . Besides , it ’s worth pointing out that nearest neighbor based methods suffer from expensive computational burden from pairwise DTW between the test instance and each of all training examples when doing prediction .
Also it is worth mentioning that SVM SIDL performs best on the “ toughest ” dataset in the collection , ie , ECGFiveDays , with a large margin over the baseline methods , including 1NN DTW . This again reinforces the benefits of modeling shift invariant local patterns for sequential data mining . 5.5 Learned Dictionary
In this section , we showcase the basis elements learned by SIDL without any human supervision . Figure 5 presents sample time series from each of the four classes and plots the learned basis elements together with the time series for the Trace data set . The two largest basis of each time series ( in terms of the absolute value of the coefficient of that basis ) are plotted with their shift locations in Figure 5 . Also the degree of sparsity of the resulting codings ( propotions of zero coefficients ) for each time series are shown in the figure titles . It can be observed from the plots that the learned basis do capture local and discriminative patterns of the time series from different classes . This further validates the idea of representing time series with local basis elements and the proposed method can be used to efficiently learn them , even in the absence of true class labels . All the basis elements are learned with random initialization at the start of SIDL ; it is possible that the algorithm might work even better given meaningful initializations .
Similarly , sample time series for Gun Point are plotted in Figure 6 . Again , though the dictionary and the encodings are learned in an unsupervised fashion , it is clear that the learned representations for the time series do represent meaningful local patterns and more importantly , these patterns are relevant indicators for classification , as shown empirically by the example time series .
More sample time samples for ECGFiveDays are shown in Figure 7 .
6 . CONCLUSIONS AND FUTURE WORK This paper presents a new framework for shift invariant dictionary learning to capture local patterns from input signals . This framework relies on the assumption that the basis vectors may be be shorter than the full input signal . In other words useful temporal patterns may be embedded in different locations of a longer time series . We also presenet an efficient learning algorithm to estimate the shift invariant sparse coding as well as a set of effective basis vectors . In our experiments on benchmark time series datasets for classification evaluations , the proposed method produces basis vectors that are more useful for signal reconstruction , exhibiting a lower reconstruction error . These same basis vectors also produce comparable or even more accurate classifications than several state of the art baseline methods
There are several promising directions in extending the work . First , the formulation of SIDL does not make any assumptions about the input signals , such as smoothness , sparsity or bounded norms [ 10 ] . If such assumptions are true for the input signals , we can exploit these characteristics to further improve the proposed method . Second , temporal patterns may compress or stretch in time , and hence incorporating dynamic time warping ( DTW ) [ 16 , 11 ] into the framework may further improve the process by normalizing the basis vectors . Third , it is possible to extend the framework to take advantage of supervision such as partial labeling of the input signals [ 12 ] , to help identify local patterns that are pertinent to the learning task . In addition , one may also want to investigate initialization strategies of the dictionaries and sparse codings based on priors ( such domain knowledge ) about the input signal to improve the resulting coding quality . Acknowledgements We thank the anonymous reviewers for their helpful comments . 7 . REFERENCES [ 1 ] R . J . Alcock and Y . Manolopoulos . Time series similarity queries employing a feature based approach . In 7 th Hellenic Conference on Informatics , Ioannina , pages 27–29 , 1999 .
[ 2 ] C . Bao , J . Cai , and H . Ji . Fast sparsity based orthogonal dictionary learning for image restoration . In IEEE International Conference on Computer Vision , ICCV 2013 , Sydney , Australia , December 1 8 , 2013 , pages 3384–3391 , 2013 .
[ 3 ] C . Chang and C . Lin . LIBSVM : A library for support vector machines . ACM TIST , 2(3):27 , 2011 . [ 4 ] X . Chen , Z . Du , J . Li , X . Li , and H . Zhang .
Compressed sensing based on dictionary learning for extracting impulse components . Signal Processing , 96:94–109 , 2014 .
2101 Figure 5 : Sample time series in different classes from the Trace dataset ( plotted in black ) . The reconstructed signal with the learned basis is plotted in green . The most 2 active basis are also shown in their matching location with the time series in red and blue .
Figure 6 : Sample time series in different classes from the Gun Point dataset ( plotted in black ) . The reconstructed signal with the learned basis is plotted in green . The most 2 active basis are also shown in their matching location with the time series in red and blue .
[ 5 ] Y . Chen , E . Keogh , B . Hu , N . Begum , A . Bagnall ,
A . Mueen , and G . Batista . The ucr time series classification archive , July 2015 . wwwcsucredu/˜eamonn/time series data/ .
[ 6 ] I . Gkioulekas and T . E . Zickler . Dimensionality reduction using the sparse linear model . In Advances in Neural Information Processing Systems 24 : 25th Annual Conference on Neural Information Processing Systems 2011 . Proceedings of a meeting held 12 14 December 2011 , Granada , Spain . , pages 271–279 , 2011 .
[ 7 ] J . Grabocka , N . Schilling , M . Wistuba , and
L . Schmidt Thieme . Learning time series shapelets . In The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’14 ,
New York , NY , USA August 24 27 , 2014 , pages 392–401 , 2014 .
[ 8 ] R . B . Grosse , R . Raina , H . Kwong , and A . Y . Ng .
Shift invariance sparse coding for audio classification . In UAI 2007 , Proceedings of the Twenty Third Conference on Uncertainty in Artificial Intelligence , Vancouver , BC , Canada , July 19 22 , 2007 , pages 149–158 , 2007 .
[ 9 ] Y . Huang , J . Paisley , Q . Lin , X . Ding , X . Fu , and
X . Zhang . Bayesian nonparametric dictionary learning for compressed sensing MRI . IEEE Transactions on Image Processing , 23(12):5007–5019 , 2014 .
[ 10 ] W . Jiang , F . Nie , and H . Huang . Robust dictionary learning with capped l1 norm . In Proceedings of the
0100200300 3 2 101234TS 20 , Class 1 , Sparsity 0.25DataBasis 2Basis 11Reconstruction0100200300 3 2 101234TS 47 , Class 1 , Sparsity 0.30DataBasis 11Basis 18Reconstruction0100200300 3 2 101234TS 67 , Class 2 , Sparsity 0.10DataBasis 7Basis 11Reconstruction0100200300 3 2 101234TS 77 , Class 2 , Sparsity 0.05DataBasis 5Basis 7Reconstruction0100200300 3 2 101234TS 26 , Class 3 , Sparsity 0.10DataBasis 1Basis 8Reconstruction0100200300 3 2 101234TS 95 , Class 3 , Sparsity 0.20DataBasis 6Basis 20Reconstruction0100200300 3 2 101234TS 28 , Class 4 , Sparsity 0.25DataBasis 6Basis 20Reconstruction0100200300 3 2 101234TS 52 , Class 4 , Sparsity 0.05DataBasis 1Basis 17Reconstruction050100150 2 15 1 050051152TS 11 , Class 1 , Sparsity 0.76DataBasis 27Basis 40Reconstruction050100150 2 15 1 050051152TS 38 , Class 1 , Sparsity 0.80DataBasis 27Basis 28Reconstruction050100150 2 15 1 050051152TS 49 , Class 1 , Sparsity 0.78DataBasis 39Basis 48Reconstruction050100150 2 15 1 050051152TS 67 , Class 1 , Sparsity 0.78DataBasis 12Basis 27Reconstruction050100150 2 15 1 050051152TS 26 , Class 2 , Sparsity 0.82DataBasis 5Basis 21Reconstruction050100150 2 15 1 050051152TS 52 , Class 2 , Sparsity 0.72DataBasis 10Basis 44Reconstruction050100150 2 15 1 050051152TS 104 , Class 2 , Sparsity 0.72DataBasis 5Basis 40Reconstruction050100150 2 15 1 050051152TS 108 , Class 2 , Sparsity 0.74DataBasis 19Basis 36Reconstruction2102 Figure 7 : Sample time series in different classes from the ECGFiveDays dataset ( plotted in black ) . The reconstructed signal with the learned basis is plotted in green . The most 2 active basis are also shown in their matching location with the time series in red and blue .
Twenty Fourth International Joint Conference on Artificial Intelligence , IJCAI 2015 , Buenos Aires , Argentina , July 25 31 , 2015 , pages 3590–3596 , 2015 . [ 11 ] E . J . Keogh and M . J . Pazzani . Scaling up dynamic time warping for datamining applications . In Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining , Boston , MA , USA , August 20 23 , 2000 , pages 285–289 , 2000 . learned dictionaries by rls dla and compared with k svd . In Acoustics , Speech and Signal Processing ( ICASSP ) , 2011 IEEE International Conference on , pages 1517–1520 . IEEE , 2011 .
[ 18 ] C . Studer and R . G . Baraniuk . Dictionary learning from sparsely corrupted or compressed signals . In Acoustics , Speech and Signal Processing ( ICASSP ) , 2012 IEEE International Conference on , pages 3341–3344 . IEEE , 2012 .
[ 12 ] J . Mairal , F . R . Bach , J . Ponce , G . Sapiro , and
[ 19 ] R . Tibshirani . Regression shrinkage and selection via
A . Zisserman . Supervised dictionary learning . In Advances in Neural Information Processing Systems 21 , Proceedings of the Twenty Second Annual Conference on Neural Information Processing Systems , Vancouver , British Columbia , Canada , December 8 11 , 2008 , pages 1033–1040 , 2008 .
[ 13 ] J . Mairal , F . R . Bach , J . Ponce , G . Sapiro , and
A . Zisserman . Non local sparse models for image restoration . In IEEE 12th International Conference on Computer Vision , ICCV 2009 , Kyoto , Japan , September 27 October 4 , 2009 , pages 2272–2279 , 2009 .
[ 14 ] A . Mueen , E . J . Keogh , and N . E . Young .
Logical shapelets : an expressive primitive for time series classification . In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , San Diego , CA , USA , August 21 24 , 2011 , pages 1154–1162 , 2011 .
[ 15 ] R . Rubinstein , M . Zibulevsky , and M . Elad . Double sparsity : learning sparse dictionaries for sparse signal approximation . IEEE Transactions on Signal Processing , 58(3):1553–1564 , 2010 .
[ 16 ] H . Sakoe and S . Chiba . Dynamic programming algorithm optimization for spoken word recognition . Acoustics , Speech and Signal Processing , IEEE Transactions on , 26(1):43–49 , 1978 .
[ 17 ] K . Skretting and K . Engan . Image compression using the lasso . J . R . Statist . Soc . B , 58:267–288 , 1996 .
[ 20 ] T . T . Wu and K . Lange . Coordinate descent algorithms for lasso penalized regression . The Annals of Applied Statistics , 2:1–21 , 2008 .
[ 21 ] Y . Wu and E . Y . Chang . Distance function design and fusion for sequence data . In Proceedings of the 2004 ACM CIKM International Conference on Information and Knowledge Management , Washington , DC , USA , November 8 13 , 2004 , pages 324–333 , 2004 .
[ 22 ] X . Xi , E . J . Keogh , C . R . Shelton , L . Wei , and C . A . Ratanamahatana . Fast time series classification using numerosity reduction . In Machine Learning , Proceedings of the Twenty Third International Conference ( ICML 2006 ) , Pittsburgh , Pennsylvania , USA , June 25 29 , 2006 , pages 1033–1040 , 2006 .
[ 23 ] L . Ye and E . J . Keogh . Time series shapelets : a new primitive for data mining . In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , Paris , France , June 28 July 1 , 2009 , pages 947–956 , 2009 .
[ 24 ] J . Zakaria , A . Mueen , and E . J . Keogh . Clustering time series using unsupervised shapelets . In 12th IEEE International Conference on Data Mining , ICDM 2012 , Brussels , Belgium , December 10 13 , 2012 , pages 785–794 , 2012 .
050100150 4 3 2 101234TS 6 , Class 1 , Sparsity 0.16DataBasis 25Basis 34Reconstruction050100150 4 3 2 101234TS 19 , Class 1 , Sparsity 0.10DataBasis 31Basis 42Reconstruction050100150 4 3 2 101234TS 137 , Class 1 , Sparsity 0.20DataBasis 34Basis 42Reconstruction050100150 4 3 2 101234TS 278 , Class 1 , Sparsity 0.16DataBasis 21Basis 43Reconstruction050100150 4 3 2 101234TS 52 , Class 2 , Sparsity 0.20DataBasis 9Basis 28Reconstruction050100150 4 3 2 101234TS 132 , Class 2 , Sparsity 0.20DataBasis 9Basis 41Reconstruction050100150 4 3 2 101234TS 201 , Class 2 , Sparsity 0.22DataBasis 11Basis 36Reconstruction050100150 4 3 2 101234TS 472 , Class 2 , Sparsity 0.16DataBasis 18Basis 36Reconstruction2103 APPENDIX A . PROOF OF PROPOSITION 4.1 we have
Proof . Plugging Eq ( 7 ) back to Eq ( 6 ) , there are three
Taking the derivative of L wrt u yields x + dk4x2
L
( u ) = −
( 19 )
=
From the KKT conditions , we also have u ≥ 0 and x
T ( dk , tk )
( 23 )
∗ t k = arg min tk
Aggregating the above three cases , since no matter which range T ( dk , tk)x lies in , Eq ( 19 ) and Eq ( 22 ) are both up2x2 , we arrive at the statement in Propo per bounded by 1 sition 41 fi
B . PROOF OF EQUATION ( 15 )
Proof . Plugging Eq ( 14 ) of dk back to the Lagrangian
Eq ( 13 ) , we have
L(u ) = flflflflfl2
, tik j=1 α2 jk
,2u +n
2 i=1 α2 ik
( 24 )
2x x
( 25 ) ik i=1
1 2
+ u j=1 α2 i=1 2α2 i=1 α2 jk i=1 α2 i=1 α2 ik flflflflflxi − αikT n x 2u +n x x ( 2u +n jk)2 − c n 3x x +
2u +n −2u +n 3x x − c ,2u +n 2x x − c ,2u +n
2x ,2u +n xn ≥ √ x2√
1 i=1 α2 ik
1 i=1 α2 ik i=1 α2 ik u =
+ u ik
α2 ik x − c
1 2 c
= 0
( 26 )
≥ 0
( 27 )
( 28 ) i=1
− n xx
Hence • if c , L(u ) can reach 0 when and the corresponding dk is √ c dk = xn x
• If
√ c , L(u ) will always be negative , hence to maximize the Lagrangian , u = 0 and the corresponding dk is i=1 α2 ik
<
1n i=1 α2 ik x dk =
( 29 ) fi cases : k = 0 and the corresponding
1
=
1 2
1 2
+ λ dk2 dk2 value for Eq ( 6 ) is now minimize the following objective over tk :
• If |T ( dk , tk)x| ≤ λ , α∗ x2 ; • If T ( dk , tk)x > λ , then αk = T ( dk,tk)x−λ flflflfl T ( dk , tk)x − λ flflflfl2 T ( dk , tk ) −x flflflflT ( dk , tk)T ( dk , tk ) flflfl2 + dk4x2 x x − 2dk2x T ( dk , tk ) + 2λdk2x + λ2dk2 + 2λdk2x
− dk2x = − T ( dk , tk)x − λ2
2dk4 + 2λdk2T ( dk , tk ) x − λ2dk2 x2
2dk4 − 2λdk2T ( dk , tk )
− 2λ2dk2
T ( dk , tk)T ( dk , tk ) fl
=
+
1
2dk2
1 2 and we
T ( dk , tk)x − λ dk2 x
T ( dk , tk)T ( dk , tk )
T ( dk , tk )
( 21 ) and like
T ( dk , tk)x + λ dk2 where we extensively used the fact
T ( dk , tk ) = dk2
T ( dk , tk )
Since T ( dk , tk)x > λ , Eq ( 19 ) is monotonically de creasing in T ( dk , tk ) , therefore to minimize Eq ( 19 ) we have
( 20 ) t
1 tk
=
1 2 dk2
T ( dk , tk ) dk2 wise we have :
∗ k = arg max
2dk4 + 2λdk2T ( dk , tk ) x • If T ( dk , tk)x < −λ , then αk = T ( dk,tk)x+λ flflflfl T ( dk , tk)x + λ flflflfl2 − λ T ( dk , tk ) −x flflflflT ( dk , tk)T ( dk , tk ) flflfl2 x + dk4x2 x − 2dk2x + λ2dk2 − 2λdk2x T ( dk , tk ) − 2λdk2x
− dk2x x − λ2dk2 = − T ( dk , tk)x + λ2 x2 Since T ( dk , tk)x < −λ , Eq
2dk4 − 2λdk2T ( dk , tk )
− 2λ2dk2
T ( dk , tk)T ( dk , tk )
2dk2 fl
1 2
=
+
1
( 22 ) is monotonically increasing in T ( dk , tk ) , therefore to minimize Eq ( 22 )
T ( dk , tk)T ( dk , tk )
T ( dk , tk ) x + dk4x2
( 22 )
2104
