Absolute Fused Lasso and Its Application to
Genome Wide Association Studies
Tao Yang
Arizona State University
Tempe , AZ 85287 tyang@asuedu Ruiwen Zhang SAS Institute Inc . Cary , NC 27513 ruiwenzhang@sascom
Jun Liu
SAS Institute Inc . Cary , NC 27513 junliu@sascom Xiaotong Shen
University of Minnesota Minneapolis , MN 55455 xshen@statumnedu
Pinghua Gong
University of Michigan Ann Arbor , MI 48109 gongp@umich.edu
Jieping Ye
University of Michigan Ann Arbor , MI 48109 jpye@umich.edu
ABSTRACT In many real world applications , the samples/features acquired are in spatial or temporal order . In such cases , the magnitudes of adjacent samples/features are typically close to each other . Meanwhile , in the high dimensional scenario , identifying the most relevant samples/features is also desired . In this paper , we consider a regularized model which can simultaneously identify important features and group similar features together . The model is based on a penalty called Absolute Fused Lasso ( AFL ) . The AFL penalty encourages sparsity in the coefficients as well as their successive differences of absolute values—ie , local constancy of the coefficient components in absolute values . Due to the non convexity of AFL , it is challenging to develop efficient algorithms to solve the optimization problem . To this end , we employ the Difference of Convex functions ( DC ) programming to optimize the proposed non convex problem . At each DC iteration , we adopt the proximal algorithm to solve a convex regularized sub problem . One of the major contributions of this paper is to develop a highly efficient algorithm to compute the proximal operator . Empirical studies on both synthetic and real world data sets from Genome Wide Association Studies demonstrate the efficiency and effectiveness of the proposed approach in simultaneous identifying important features and grouping similar features .
Keywords Absolute Fused Lasso ; Non convex Optimization ; Proximal Operator ; GWAS
1 .
INTRODUCTION
Regularized learning methods have recently attracted increasing attention in various applications . A common scenario that occurs in many studies is that the data sets we investigated are of some natural ( eg , spatial or temporal ) order ; examples include the comparative genomic hybridization data [ 18 ] , prostate cancer data [ 17 ] and neuroimaging data [ 23 ] . For those classes of studies , it is often the case that the adjacent samples/features are similar and even identical . Moreover , in Genome Wide Association Studies ( GWAS ) , a causal single nucleotide polymorphism ( SNP ) often exhibits high similarity with its nearby SNPs . It is thus desired to group nearby SNPs together . However , due to the ambiguity choice of reference allele during genotype coding [ 8 ] , we should group adjacent SNPs if their absolute values are close to each other .
Previous works [ 22 , 26 , 1 , 25 , 20 ] indicate that utilizing the inherent structure information in the data can potentially be beneficial for model construction and interpretation . Thus if the data exhibits some sequential order , we can potentially incorporate such prior knowledge into the model to improve performance . Meanwhile , due to the curse of dimensionality in the high dimensional scenario , identifying the most relevant features is of crucial importance . In such a case , the traditional Lasso [ 16 ] model is insufficient to produce desired results since it tends to select only one of those highly correlated features [ 29 ] . There are mainly two approaches in the literature to address the problem . One approach adopts the fused penalty ( eg , fused Lasso ) , which can yield a sparse solution in both the coefficients and their successive differences [ 17 , 18 , 9 ] . However , it does not consider the case that adjacent features have high similarity but opposite signs . Another approach utilizes the graph structure among features ( eg , OSCAR ) during model construction [ 3 , 23 , 28 ] . However , such an approach is too general and does not make full use of the specific structure of the genome data .
Generally , a GWA study focuses on investigating associations between genotypes ( SNPs ) and disease phenotypes . Previous studies [ 8 , 28 ] have shown that incorporating the linkage disequilibrium ( LD ) information [ 13 ] between adjacent SNPs is beneficial in delineating association SNPs with smoothness and less randomness than individual SNP analysis . The studies in [ 8 ] also argue that the fused Lasso is not effective due to the ambiguity choice of coding reference . Thus , it is desired to penalize successive SNPs whose absolute values are close or identical .
In this paper , we consider a regularized model which uses a penalty called “ Absolute Fused Lasso ” ( AFL ) to solve such a problem . The AFL penalty encourages sparsity in the co
1955 efficients as well as their successive differences of absolute values—ie , local constancy of the coefficient components in absolute value . With AFL , highly similar features can potentially be grouped together even when their signs are different . Since the AFL problems are non convex , it is challenging to develop efficient optimization algorithms . To this end , we employ the Difference of Convex functions ( DC ) programming to solve the non convex problem . At each DC iteration , we adopt the proximal algorithm to efficiently solve the convex subproblem , which iteratively solves a proximal operator problem ; we further use the Barzilai Borwein ( BB ) rule for line search to accelerate convergence . One of the major contributions of this paper is to show that the proximal operator problem can be solved efficiently . Specifically , by exploiting the special structure of the regularizer , we first convert the computation of such proximal operator to an equivalent optimization problem via a Euclidean projection onto a special polyhedron . We then develop a gradient descent algorithm based on a novel restart technique by utilizing the optimality condition to efficiently solve the projection problem . We have conducted empirical evaluations on both synthetic data and real data . Experimental results demonstrate that the proposed DC Proximal approach can achieve up to 50x speedup over general DC ADMM ( alternating direction method of multipliers ) method—it allows us to perform efficient AFL modeling on large scale genome data that contains tens of thousands SNPs .
2 . THE AFL FORMULATION
In this paper , we consider the following Absolute Fused
Lasso ( AFL ) regularization model : min x∈Rp loss(x ) + afl(x ) ,
( 1 ) where loss(x ) is a convex empirical loss function ( eg , the least squares loss or the logistic loss ) and the AFL penalty is defined as : afl(x ) = λ1x1 + λ2
||xi| − |xi+1|| ,
( 2 ) p−1 i=1 p−1 i=1 |xi − xi+1| ) , where λ1 and λ2 are non negative regularization parameters . The second term penalizes differences of successive coefficients’ magnitudes and can be considered as a grouping penalty . By imposing both the l1 and the grouping penalties , the AFL model can simultaneously identify important features as well as group similar ( identical ) features together . Different from the fused Lasso that penalizes the l1 norm on successive differences of coefficients ( ie , λ2 the AFL encourages the smoothness of adjacent coefficients whose absolute values are close or even identical . Thus , strong successive signals can be identified by Eq ( 1 ) even when their signs are different . In general , adopting the AFL penalty is expected to be more effective than the fused Lasso ( See an example in Fig 1 ) . Note that in GWAS , the SNPs data we obtain through genotype coding are strongly affected by the choice of reference allele . Thus it is insufficient to just penalize the successive differences without considering the absolute values . In [ 8 ] , the authors use the l2 norm on the absolute difference , and apply coordinate descent to solve the proposed formulation . However , due to the use of l2 norm , the fused property , ie , the absolute values of nearby terms tend to be identical , does not hold any more .
Figure 1 : Comparison of the solutions of the AFL and the fused Lasso on a simulated data . The AFL ( red line ) provides better recovery of the original signals ( green ) than the fused Lasso ( blue ) . See Supplement A for more details about this experiment .
In this paper , we propose to adopt the DC programming to solve the AFL problem ( 1 ) and apply the proximal algorithm to solve the sub problem at each DC iteration . One of our main technical contributions is to develop an algorithm to efficiently solve the proximal operator problem by exploiting the special structure of the regularizer , which is a key building block of the proximal algorithm .
3 . DC PROGRAMMING FOR SOLVING THE
AFL PROBLEM
The AFL formulation in Eq ( 1 ) is a non convex optimization problem . We propose to use the Difference of Convex functions ( DC ) programming [ 15 , 14 ] to solve it , where a key step is to decompose the objective function in Eq ( 1 ) into the difference of two convex functions . By noting that ||xi| − |xi+1|| = |xi + xi+1| + |xi − xi+1| − ( |xi| + |xi+1| ) , we decompose the objective function in Eq ( 1 ) into the difference of the following two functions : f1(x ) = loss(x ) + λ1x1 + λ2
( |xi + xi+1| + |xi − xi+1| ) , p−1 p−1 i=1
( |xi| + |xi+1| ) . f2(x ) = λ2 i=1
By linearization of f2(x ) , the per iteration subproblem of the DC algorithm can be written as : loss(x ) − ( ck)T x min x p−1
+ λ1x1 + 2λ2 max ( |xi|,|xi+1| ) ,
( 3 ) where i=1 ck i = λ2di sgn ( xk d1 = dp = 1 , di = 2 , 2 ≤ i ≤ p − 1 i ) with
( 4 ) and sgn(· ) is the signum function ( Detailed derivation is provided in Supplement B ) . We summarize the DC programming in Algorithm 1 . A key building block in this algorithm is how to efficiently solve the subproblem ( 3 ) . Next , we show that ( 3 ) can be efficiently solved via a proximal algorithm .
1956 Algorithm 1 DC algorithm for solving the AFL Problem Input : data matrix A ∈ Rn×p , response vector y ∈ Rn×1 , regularizes λ1 , λ2 , and tolerance
Output : x 1 : Initialization : x0 ← 0 , k = 0 2 : while f ( xk ) − f ( xk+1 ) > do 3 : Update ck according to Eq ( 4 ) . 4 : Update xk+1 according to Eq ( 3 ) . 5 : 6 : end while k ← k + 1 .
4 . THE PROXIMAL ALGORITHM
In this paper , we adopt the proximal framework [ 21 ] to solve the sub optimization problem ( 3 ) at each DC iteration . Problem ( 3 ) is equivalent to min x∈Rp h(x ) = l(x ) + m(x ) ,
( 5 ) where where σ ∈ ( 0 , 1 ) is a constant . To further accelerate the convergence speed of the proximal algorithm , as suggested by the studies in [ 21 , 5 ] , we adopt the Barzilai Borwein ( BB ) rule to initialize the line search step size as 1/tk,0 , where tk,0 = ak , bk ak , ak with ak = xk − xk−1 and bk = ∇l(xk ) − ∇l(xk−1 ) .
Notice that a key step in the proximal algorithm is how to efficiently solve the proximal operator problem ( 7 ) . In the next section , we introduce our efficient approach to solve ( 7 ) by exploiting the special structure of the regularizer . 5 . EFFICIENT COMPUTATION OF THE
PROXIMAL OPERATOR
For discussion convenience , we absorb tk into the regularization parameters λ1 and λ2 , and omit the superscript k in Eq ( 7 ) . Then the proximal operator problem ( 7 ) can be simplified as follows : l(x ) = loss(x ) − ( ck)T x , p−1 m(x ) = λ1x1 + 2λ2 max ( |xi|,|xi+1| ) . i=1
The proximal algorithm solves problem ( 3 ) by generating a sequence {xk} by solving : xk+1 = arg min x∈Rp
{l(xk ) + ∇l(xk ) , x − xk
+ m(x ) + x − xk2 2} ,
( 6 ) tk 2 where tk > 0 is chosen by some rule introduced below . It is easy to show that ( 6 ) is equivalent to the following proximal operator problem :
1 2 x − uk2 +
1 tk m(x ) , xk+1 = arg min x∈Rp
( 7 ) where uk = xk − ∇l(xk)/tk . Thus , it can be viewed as the gradient descent along the direction −∇l(xk ) with the step size 1/tk plus computing the proximal operator problem ( 7 ) . The pseudo codes of the algorithm are summarized in Algorithm 2 .
Algorithm 2 The Proximal Algorithm
Choose tk ∈ [ tmin , tmax ]
Input : A , y , λ1 , λ2 Output : x 1 : Choose η > 1 , tmax > tmin > 0 2 : Initialization : x0 , k = 0 3 : while some stopping criterion is not satisfied do 4 : 5 : while line search criterion is not satisfied do Update xk+1 according to Eq ( 7 ) . 6 : tk ← ηtk . 7 : end while 8 : k ← k + 1 . 9 : 10 : end while
πλ1 λ2
( u ) = arg min x∈Rp
{ 1 2 x − u2 p−1
+λ1x1 + 2λ2 max ( |xi|,|xi+1|)} .
( 8 )
By applying the procedure discussed in [ 4 ] , we have the i=1 following theorem : Theorem 1 . For any λ1 , λ2 ≥ 0 , we have
πλ1 λ2
( u ) = sgn(π0
λ2 ( u ) ) fi max(|π0
λ2 ( u)| − λ1 , 0 ) .
( 9 )
Theorem 1 implies that we can solve problem ( 8 ) in two steps : first solve ( 8 ) with λ1 = 0 and then applying ( 9 ) to obtain the final result . Let λ = 2λ2 and λ1 = 0 , Eq ( 8 ) can be rewritten as :
πλ(u ) = arg min x∈Rp
{ 1 2 x − u2 p−1 i=1
+ λ max ( |xi|,|xi+1|)} .
( 10 )
We propose to solve problem ( 10 ) efficiently by converting the proximal operator to a Euclidean projection onto a special polyhedron . To perform this transformation , we utilize some important properties of ( 10 ) as summarized in Lemma 1 , where a detailed proof is provided in Supplement C . Lemma 1 . Let x∗ = πλ(u ) be the optimal solution to ( 10 ) . ∀λ > 0 , we have : i ) if ui ≥ 0 , then ui ≥ x∗ i ≥ 0 , ii ) if ui < 0 , then ui ≤ x∗ i ≤ 0 , iii ) πλ(u ) = sgn(u ) fi πλ(|u| ) , iv ) if |ui| ≥ |ui+1| , then |x∗ v ) if |ui| < |ui+1| , then |x∗ i | ≥ |x∗ i | ≤ |x∗ i+1| , i+1| .
5.1 Equivalent Euclidean Projection Problem Assume u ≥ 0 , we define a sparse matrix R ∈ R(p−1)×p as
To guarantee convergence , we use a line search criterion to choose an appropriate step size . Specifically , we accept the step size 1/tk if the following inequality holds : tkxk+1 − xk2 , h(xk+1 ) ≤ h(xk ) − σ 2 follows :
Rij =
1 1 −1 −1 0 ui < ui+1 , j = i ui ≥ ui+1 , j = i + 1 ui ≥ ui+1 , j = i ui < ui+1 , j = i + 1 otherwise .
( 11 )

1957 In addition , we denote a vector w ∈ Rp with the j th entry defined as :
 2 0
1 wj = i Rij = 2 i Rij ≤ −1 otherwise .
( 12 )
With Lemma 1 and the above definitions of R and w , we next present the following theorem which converts the proximal operator problem to an equivalent Euclidean projection problem . Theorem 2 . Let u ≥ 0 and λ > 0 . Let
The support set S(z ) is motivated by the optimality of the problem ( 20 ) , and shall be used for defining a nonlinear and discontinuous mapping from z to x . ∀z∗ ≥ 0 , it is a minimizer of ( 20 ) if and only if z − z∗ , φ(z∗ ) ≥ 0,∀z ≥ 0 . From the optimality condition , we can build the relationship between the minimizer and its gradient , as summarized in the following lemma : Lemma 2 . Let z∗ be the optimal solution to ( 20 ) and g∗ = φ(z∗ ) . We have : i ) if z∗ i > 0 , then z∗ i = 0 , and ii ) if g∗ i > 0 then g∗ i = 0 . and v = u − λw
P = {x|Rx ≤ 0 , x ≥ 0} .
Define the Euclidean projection of v onto P as :
πP λ ( v ) = arg min x∈P x − v2 .
1 2
We have
πλ(u ) = πP
λ ( v ) .
( 13 )
( 14 )
( 15 )
( 16 )
The matrix RRT is very special , and it can be shown that its eigenvalues are 2 − 2cos(iπ/p ) , i = 1 , 2 , . . . , p − 1 , and thus it is positive definite . Note that RRT is the Hessian of φ(z ) , which implies that the minimizer of ( 20 ) is unique . 522 A Nonlinear Mapping ω(· ) from z to x Let s0 = 0 denote the smallest entry in S(z ) , and s|S| = p denote the largest entry in S(z ) . In addition , we denote the j th largest entry in the set S − {0 , p} by sj , j = 1 , 2 , . . . ,|S| − 2 . It is clear that 1 ≤ s1 and s|S|−2 ≤ p − 1 . With s0 , s1 , . . . , s|S|−1 , the indices in [ 1 : p ] can be divided into |S| − 1 non overlapping groups :
The above theorem implies that , the proximal operator in ( 10 ) can be solved by solving the Euclidean projection problem in ( 15 ) . To further simplify , our next theorem shows that , such a Euclidean projection problem can be solved by a simplified problem without the non negative constraint . Theorem 3 . Let u ≥ 0 , λ > 0 , and
We have
Q = {x|Rx ≤ 0} ,
πQ λ ( v ) = arg min x∈Q x − v2 .
1 2
πP λ ( v ) = max(πQ
λ ( v ) , 0 ) .
( 17 )
( 18 )
( 19 )
Detailed proofs of Theorem 2 and Theorem 3 are provided in Supplements D & E . In the next section , we discuss a restart technique to efficiently solve the Euclidean projection problem ( 18 ) . 5.2 The Restart Technique
Introducing the dual variable z ∈ Rp−1 for the inequality constraints in ( 18 ) , we can obtain the Lagrangian in Supp . ( E 48 ) . The dual problem of ( 18 ) is equivalent to
{φ(z ) = min z≥0
1 2
RT z − v2} .
( 20 )
We propose to solve ( 18 ) by simultaneously using the information of the primal and dual problems . The novelty lies in the usage of the so called restart technique for fast convergence . 521 Optimality Condition and the Support Set Our proposed restart technique is built on the introduction of the support set . Specifically , ∀z ≥ 0 and denote g = φ(z ) , we define the support set as follows :
S(z ) = {i : i ∈ [ 1 , p − 1 ] , zi = 0 , gi > 0} ∪ {0 , p} .
( 21 )
Gj = {i : sj−1 + 1 ≤ i ≤ sj} , 1 ≤ j ≤ |S| − 1 .
( 22 ) Let e ∈ Rp be a vector composed of 1 ’s , and eGj and vGj be the j th group of e and v corresponding to the indices in Gj , respectively . For discussion convenience , assume z0 = zp = 0 , then we can define the nonlinear mapping x = ω(z ) based on the support set S as : eGj , vGj − zsj−1 + zsj
, i ∈ Gj , 1 ≤ j ≤ |S|−1 . ( 23 ) xi =
|Gj|
With Lemma 2 and the definition of support set in ( 21 ) , it is easy to show that the optimal solution to problem ( 18 ) can be exactly recovered by the support set S(z∗ ) , as stated in the following theorem . Theorem 4 . Let z∗ be the minimizer of the dual problem ( 20 ) , and x∗ be the minimizer of primal problem ( 18 ) . Then x∗ can be recovered by x∗ = ω(z∗ ) .
523 The Restart Technique and Properties By introducing the support set S , Theorem 4 provides an alternative efficient way to computing x∗ from z∗ . Specifically , we can exactly obtain x∗ = ω(˜z ) , where ˜z is an appropriate solution with S(˜z ) = S(z∗ ) even if ˜z = z∗ . The intuition is that , for a given appropriate solution ˜z = z∗ , if S(˜z ) is close to S(z∗ ) , x = ω(˜z ) can be a better approximation than ˜x = v − RT ˜z for the primal .
We then present a gradient projection algorithm based on the proposed restart technique , as summarized in Algorithm 3 . Given an iterative solution zk , we do not perform the gradient projection at the point z = zk . Instead , we first compute xk = ω(zk ) . Then , we compute a restart point zk by xk = v − RT zk 0 0 can be solved by an equivalent 0 = Rv − Rxk . Finally , we perform the linear system RRT zk gradient projection at the restart point z = zk 0 . Note that P0(x ) is an operator that projects x onto the non negative orthant .
0 , where zk
1958 Algorithm 3 Gradient Projection Algorithm with a Restart Technique
Input : v , λ , R Output : z 1 : Initialization : z0 ← 0 , L = 2− 2 cos(π(p− 1)/p ) , k = 0 ; 2 : Compute g0 = φ(z0 ) = RRT z0 − Rv ; and set z0 = P0(z0 − g0/L ) ;
3 : while not converge do 4 : Update the support set S(zk ) according to ( 21 ) ; 5 : Update xk = ω(zk ) according to ( 23 ) ; 0 as the solution to RRT zk 6 : 7 : Update zk+1 = P0(zk 8 : end while
0 ) , and set k ← k + 1 ;
Compute zk
0 = Rv − Rxk ;
5.3 Discussion
To end this section , we summarize our methodology for solving the proximal operator problem ( 8 ) as follows . We first show that a minimizer of problem ( 8 ) can be obtained by applying a soft thresholding ( 9 ) on the solution of an alternative optimization problem ( 10 ) . By applying the properties of ( 10 ) introduced in Lemma 1 and two variables R and w defined in ( 11 ) and ( 12 ) , we show that the proximal operator problem ( 10 ) can be convert to an equivalent problem ( 15 ) . In the sequel , we present to optimize an alternative problem ( 18 ) without the non negative constraint through ( 19 ) . To solve problem ( 18 ) , we develop a novel restart technique by introducing the support set ( 21 ) and a nonlinear mapping ( 22 ) . We propose to use Algorithm 3 to solve ( 18 ) for efficient computation .
6 . EXPERIMENTS
In this section , we evaluate the AFL model ( with the leastsquares loss ) and the proposed algorithm on both synthetic and real world data . We first evaluate the efficiency of our proposed algorithm in §611 , and then compare the AFL with the fused Lasso in §612 Next , we evaluate the prediction performance of the AFL in two GWA studies in §621 and §622 Finally , we show the effectiveness of the AFL in identifying genetic risk factors in §622 For all experiments , we use the following two stopping criteria for our algorithm : 1 ) the relative difference of function values between two iterations is less than a tolerance of 10−5 , and 2 ) the algorithm exceeds the maximum iterations ( 1000 iterations ) . 6.1 Evaluation on Synthetic Data
611 Efficiency of AFL We present the empirical studies on the efficiency of our proposed algorithm by comparing our method with the approach that adopts the alternating direction method of multipliers ( ADMM ) to solve the sub problem at each DC iteration . The experiments are carried out on a collection of randomly generated data sets A ∈ Rn×p and outcomes In addition , denote ¯λ = AT y∞ . We then y ∈ Rn×1 . conduct the evaluations in the following two scenarios :
Scenario 1 . Varying the number of features p with a fixed sample size and fixed regularization parameters λ1 and λ2 . We fix the number of samples n = 500 and vary the number of features p from 1,000 to 20,000 . We set the regularizers as λ1 = λ2 = 10−3 ¯λ .
Scenario 2 . Varying regularization parameters λ1 and λ2 with a fixed sample/feature size . We fix the n = 500 and p = 10 , 000 . We choose the values of ( λ1 , λ2 ) from the following set : {(10−4 ¯λ , 10−4 ¯λ ) , ( 10−3 ¯λ , 10−3 ¯λ ) , ( 0.01¯λ , 001¯λ)}
Figure 2 summarizes the running time ( in seconds ) and speedup of AFL ( proximal algorithm ) over ADMM in the above two scenarios . From these figures , we have the following observations : ( 1 ) Our proposed algorithm is much more efficient than ADMM in both scenarios . ( 2 ) The speedup of AFL over ADMM increases as the feature size increases . This indicates that our proposed approach using DC programming and the proximal algorithm is capable of handling large scale learning problems . ( 3 ) The speedup of AFL over ADMM increases as the regularized parameters become larger . Thus , our method is expected to be superior over ADMM in real world applications , ie , only a small number of features are relevant—a relatively large regularized parameter value is preferred .
( a ) Scenario 1
( b ) Scenario 2
Figure 2 : Comparison of running times and speedups of AFL over ADMM .
612 Comparison of AFL and Fused Lasso In this study , we compare the AFL model with the fused Lasso . Recall that the AFL is designed to encourage the smoothness of adjacent coefficients whose absolute values are close or even identical . Thus if the adjacent features exhibit different signs in the model , the AFL approach is expected to be more effective than the fused Lasso . We generate the synthetic data via the linear model y = A¯x + , where the design matrix A ∈ R500×5000 and the noise term ∈ Rn are randomly generated from normal distributions . The ground truth ¯x ∈ Rn contains 10 % of the signals , which are evenly partitioned into 5 groups . Specifically , within each group , we first continuously assign the same value for all the signals ; and then , we randomly pick {0 % , 1 % , 2 % , 5 % , 10%} of the signals and change their signs to the opposite . The regularization parameters λ1 and λ2 are chosen from the interval [ 10−4¯λ , 0.9¯λ ] using five fold crossvalidation for both the AFL and the fused Lasso . We then evaluate the models on a 100 iid samples testing set . The
05101520253002004006008001K3K5K7K9K11K13K15K17K19KSpeedupTime4(s)ALFADMMSpeedup0102030405060050100150200(1e=4,1e=4)(1e=3,1e=3)(001,001)SpeedupTime4(s)ALFADMMSpeedup1959 SLEP package [ 7 , 9 ] is utilized to solve the fused Lasso problem . We report the averaged prediction performance of 10 replications in Table 1 .
We observe from Table 1 that the AFL approach provides better prediction performance than the fused Lasso in most If the ground truth ¯x does not contain too many cases . opposite adjacent signals , both AFL and the fused Lasso can recover the original signal accurately . However , when the number of opposite signals increases , AFL outperforms the fused Lasso significantly . The reason is that , with the AFL penalty , the model tends to select those highly similar adjacent features even if their signs are different . Therefore , the AFL approach is more robust than the fused Lasso in such cases .
Table 1 : Averaged prediction performance of AFL and fused Lasso on synthetic data ( standard deviation is shown in the bracket ) . FL refers to the fused Lasso . MSE refers to the mean squared error . Corr X is the Pearson correlation between the model x and the ground truth ¯x .
Neg % Method
MSE Y
MSE X
Corr X
0 %
1 %
2 %
5 %
10 %
AFL FL
AFL FL
AFL FL
AFL FL
AFL FL
0.0001 ( 0.00 ) 0.0003 ( 0.00 )
0.0000 ( 0.00 ) 0.0000 ( 0.00 )
1.00 ( 0.00 ) 1.00 ( 0.00 )
0.0157 ( 0.02 ) 0.0051 ( 0.00 )
0.0000 ( 0.00 ) 0.0000 ( 0.00 )
1.00 ( 0.00 ) 1.00 ( 0.00 )
0.0179 ( 0.01 ) 0.0227 ( 0.01 )
0.0000 ( 0.00 ) 0.0000 ( 0.00 )
1.00 ( 0.00 ) 1.00 ( 0.00 )
15.16 ( 11.09 ) 51.75 ( 23.55 )
0.0029 ( 0.00 ) 0.0103 ( 0.00 )
0.98 ( 0.01 ) 0.92 ( 0.04 )
86.32 ( 28.21 ) 125.98 ( 19.85 )
0.0200 ( 0.00 ) 0.0242 ( 0.00 )
0.81 ( 0.03 ) 0.78 ( 0.01 )
6.2 Real world Applications
621 GLT1D1 Data Study In this study , we evaluate the AFL approach on a realworld GWAS data called GLT1D1 . The data set containing 210 samples and 1,782 SNPs [ 28 ] . The major objectives in this study are predicting the gene expression level of GLT1D1 as well as identifying disease risk SNPs . To construct our predictive models , we randomly pick 2/3 of the samples to form the training set and use the same method in § 612 to choose the best parameters . We compare the AFL model with the fused Lasso on the remaining 1/3 of the data . The averaged results of 10 replications are summarized in Table 2 .
Table 2 shows that the AFL approach achieves better prediction performance in terms of MSE . In addition , our model selects a smaller number of SNPs , which demonstrates the need of considering the absolute values in GWAS due to the ambiguity choice of reference allele during genotype coding .
Table 2 : Averaged prediction performance of AFL and fused Lasso on GLT1D1 Data . “ # of nonzeros ” refers to the number of nonzero regression coefficients .
Method
MSE
# of nonzeros
AFL FL
0.8952 ( 0.05 ) 0.9182 ( 0.07 )
15.25 ( 14.57 ) 34.78 ( 48.07 )
622 ADNI WGS Data Study In this study , we evaluate the AFL model on the Alzheimer ’s
Disease Neuroimaging Initiative ( ADNI ) whole genome sequence ( WGS ) data . Particularly , we investigate imaging genetics associations between imaging phenotypes and SNPs ( within the 19th chromosome ) using the regression model with the AFL penalty . We follow the procedure in [ 24 ] to process the SNP data and the data set contains 717 subjects and 131,670 SNPs . The baseline entorhinal cortex ( EC ) volume and hippocampal ( HIPP ) volume are chosen to be the responses , as these are two major brain regions affected by the Alzheimer ’s disease ( AD ) .
Comparison of Prediction Performance We first compare the predictive performance of the AFL model with the fused Lasso . We randomly pick 90 % of the samples to form the training set and the remaining 10 % of the samples to form the testing set . We perform five fold cross validation to choose the best regularization parameters from the interval [ 10−4¯λ , 09¯λ ] We report the mean squared error and the number of nonzero coefficients of 10 replications in Table 3 .
Table 3 shows that both approaches achieve similar predictive performance in terms of MSE . Specifically , in EC task , AFL achieves a slightly lower MSE by selecting a smaller number of features . In HIPP task , AFL selects more features than the fused Lasso . We take a careful look at the SNPs identified by AFL . The AFL detects several SNPs located in three gene including PVRL2 ( rs12972156 , rs12972970 , rs34342646 ) , APOE ( rs769449 , rs769450 , rs429358 ) and APOC1 ( rs12721051 , rs56131196 , rs4420638 ) and assign them into correct groups . Note that the sign of the model coefficient of SNP rs769450 is different from its two adjacent SNPs ( ie , rs769449 and rs429358 ) ; the fused Lasso approach fails to group those three SNPs appropriately .
Table 3 : Averaged prediction performance of AFL and fused Lasso on ADNI WGS Data .
Method
MSE
# of nonzeros
EC
HIPP
AFL FL
AFL FL
0.9422 ( 0.13 ) 0.9440 ( 0.12 )
19.4 ( 18.51 ) 66.3 ( 23.78 )
0.9804 ( 0.13 ) 0.9996 ( 0.13 )
81.22 ( 86.21 ) 5822(2326 )
Detecting Risk Genetic Factors using AFL Inspired by the idea of interaction testing introduced in [ 2 ] , we finally conduct a study on detecting AD risk genetic factors with the AFL model . Specifically , on Chromosome 19 , we first calculate the Pearson correlation between each coded SNP and the response phenotype vector . Then , we plug the correlation coefficients vector into our model ( 1 ) . To identify the most association SNPs , we vary the regularization parameters and record each model .
Figure 3 shows the study results of EC and HIPP . In the experiment , we can observe that the AFL model can successfully capture AD risk genes including PVRL2 [ 10 ] , TOMM40 [ 12 , 6 , 11 ] , APOE [ 10 , 12 , 11 , 19 ] and APOC1 [ 27 , 19 ] . Moreover , the AFL is capable of performing automatic feature grouping even when the signs are different , eg , rs769449 , rs769450 and rs429358 in APOE exhibit high similarity in absolute values . However , the fused Lasso fails to correctly group SNPs like rs769450 since their signals are
1960 Table 4 : Statistical scores of selected SNPs on Chr19 P EC refers to the p value associated with the EC task . P HIPP refers to the p value associated with the HIPP task . OR refers to the odds ratio associated with MCI&AD .
RS ID
Gene
P EC
P HIPP
OR
PVRL2 rs12972156 PVRL2 rs12972970 PVRL2 rs34342646 PVRL2 rs283815 PVRL2 rs6857 PVRL2∼TOMM40 rs76692773 PVRL2∼TOMM40 rs71352238 TOMM40 rs184017 TOMM40 rs2075650 rs157581 TOMM40 rs34095326 TOMM40 rs34404554 TOMM40 rs11556505 TOMM40 rs157582 TOMM40 rs59007384 TOMM40 APOE rs769449 APOE rs769450 APOE rs429358 rs10414043 APOE∼APOC1 APOE∼APOC1 rs7256200 APOE∼APOC1 rs483082 rs12721051 APOC2 rs56131196 APOC3 rs4420638 APOC4 rs78959900 APOC1 rs73052341 APOC1
1.03E 04 1.24E 04 1.18E 04 1.98E 04 8.07E 06 3.86E 01 9.20E 05 2.72E 05 5.33E 04 5.43E 05 4.14E 02 1.59E 04 1.60E 04 8.06E 05 5.20E 05 1.54E 05 9.99E 03 2.13E 08 1.49E 05 1.96E 05 1.30E 04 1.73E 07 3.44E 08 3.40E 08 3.28E 02 4.65E 05
1.23E 05 9.98E 06 9.51E 06 1.17E 03 2.05E 06 2.64E 01 1.32E 05 8.31E 04 3.15E 04 1.39E 03 6.25E 02 4.42E 05 4.23E 05 1.96E 03 5.13E 04 3.30E 06 2.87E 03 2.50E 07 3.17E 05 6.68E 05 1.55E 03 8.62E 06 5.11E 05 6.77E 05 1.27E 01 3.97E 05
1.947 1.984 1.809 1.436 1.914 0.912 1.767 1.414 1.791 1.436 1.511 1.842 1.857 1.435 1.541 2.646 0.897 2.409 2.447 2.447 1.690 1.914 1.739 1.712 0.899 1.978 different . In Table 4 , we further present some statistical scores of SNPs selected by the AFL model , including the p value1 ( P ) and odds ratio ( OR ) association score . We can observe that most of the selected SNPs achieve high statistical significance .
7 . CONCLUSIONS
In this paper , we study a regularized learning model based on absolute fused Lasso penalty . The AFL penalty encourages sparsity in the coefficients as well as penalizes differences of successive coefficients’ magnitudes . Due to the nonconvexity of the proposed model , we propose to use the DC programming to solve it . At each DC iteration , we solve a convex regularized sub problem via the proximal algorithm . The proximal algorithm iteratively solves a proximal operator problem and adopts the Barzilai Borwein rule for line search . One of our main technical contributions is to develop a highly efficient algorithm to solve the proximal operator problem via a Euclidean projection based on a novel restart technique . Experimental results on both synthetic and realworld data demonstrate the effectiveness and efficiency of the proposed algorithm .
8 . ACKNOWLEDGMENTS
This work was supported in part by research grants from NIH ( R01 LM010730 and RF1 AG051710 ) and NSF ( IIS0953662 and III 1421057 ) .
References [ 1 ] F . Bach , R . Jenatton , J . Mairal , G . Obozinski , et al . Structured sparsity through convex optimization . Statistical Science , 27(4):450–468 , 2012 .
1Those p values are obtained from Pearson correlation analysis between SNPs and the selected imaging phenotype .
[ 2 ] J . Bien , N . Simon , R . Tibshirani , et al . Convex hierarchical testing of interactions . The Annals of Applied Statistics , 9(1):27–42 , 2015 .
[ 3 ] H . D . Bondell and B . J . Reich . Simultaneous regression shrinkage , variable selection , and supervised clustering of predictors with OSCAR . Biometrics , 64(1):115–123 , 2008 .
[ 4 ] J . Friedman , T . Hastie , H . H¨ofling , R . Tibshirani , et al . Pathwise coordinate optimization . The Annals of Applied Statistics , 1(2):302–332 , 2007 .
[ 5 ] P . Gong , C . Zhang , Z . Lu , J . Huang , and J . Ye . A general iterative shrinkage and thresholding algorithm for non convex regularized optimization problems . In The 30th International Conference on Machine Learning ( ICML ) , pages 37–45 , 2013 .
[ 6 ] R . J . Guerreiro and J . Hardy . TOMM40 association with Alzheimer disease : tales of APOE and linkage disequilibrium . Archives of neurology , 69(10):1243–1244 , 2012 .
[ 7 ] J . Liu , S . Ji , and J . Ye . SLEP : Sparse Learning with
Efficient Projections . Arizona State University , 2009 .
[ 8 ] J . Liu , K . Wang , S . Ma , and J . Huang . Regularized regression method for genome wide association studies . BMC Proceedings , 5(9):1–5 , 2011 .
[ 9 ] J . Liu , L . Yuan , and J . Ye . An efficient algorithm for a class of fused lasso problems . In Proceedings of the 16th ACM SIGKDD International Conf . on Knowledge Discovery and Data Mining , pages 323–332 . ACM , 2010 .
[ 10 ] M . W . Logue et al . A comprehensive genetic association study of Alzheimer disease in african americans . Archives of neurology , 68(12):1569–1579 , 2011 .
[ 11 ] D . M . Lyall et al . Alzheimer ’s disease susceptibility genes APOE and TOMM40 , and brain white matter integrity in the lothian birth cohort 1936 . Neurobiology of aging , 35(6):1513–e25 , 2014 .
[ 12 ] A . Maruszak et al . TOMM40 rs10524523 polymorphism ’s role in late onset Alzheimer ’s disease and in longevity . Journal of Alzheimer ’s Disease , 28(2):309– 322 , 2012 .
[ 13 ] D . E . Reich , M . Cargill , S . Bolk , J . Ireland , P . C . Sabeti , D . J . Richter , T . Lavery , R . Kouyoumjian , S . F . Farhadian , R . Ward , et al . Linkage disequilibrium in the human genome . Nature , 411(6834):199–204 , 2001 . [ 14 ] P . D . Tao and L . T . H . An . Convex analysis approach to dc programming : Theory , algorithms and applications . Acta Mathematica Vietnamica , 22(1):289–355 , 1997 .
[ 15 ] P . D . Tao et al . Duality in dc ( difference of convex functions ) optimization . subgradient methods . In Trends in Mathematical Optimization , pages 277–293 . Springer , 1988 .
[ 16 ] R . Tibshirani . Regression shrinkage and selection via the lasso . Journal of the Royal Statistical Society . Series B ( Methodological ) , pages 267–288 , 1996 .
[ 17 ] R . Tibshirani , M . Saunders , S . Rosset , J . Zhu , and K . Knight . Sparsity and smoothness via the fused lasso . Journal of the Royal Statistical Society : Series B ( Statistical Methodology ) , 67(1):91–108 , 2005 .
[ 18 ] R . Tibshirani and P . Wang . Spatial smoothing and hot spot detection for cgh data using the fused lasso . Biostatistics , 9(1):18–29 , 2008 .
[ 19 ] B . Tycko et al . APOE and APOC1 promoter polymorphisms and the risk of Alzheimer disease in african
1961 ( a ) Entorhinal
Figure 3 : Regression coefficients learned by each AFL model . Each color in the graph represents a learned model based on a pair of regularizers ( λ1 , λ2 ) . SNPs ( named by RS IDs ) are presented in their order on Chr19 “ ” indicates the gaps between SNPs . AD risk genes are marked in red .
( b ) Hippocampus american and caribbean hispanic individuals . Archives of neurology , 61(9):1434–1439 , 2004 .
[ 20 ] Y . Wang , S . Wang , J . Tang , H . Liu , and B . Li . PPP : Joint pointwise and pairwise image label prediction . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2016 .
[ 21 ] S . Wright , R . Nowak , and M . Figueiredo . Sparse reconstruction by separable approximation . IEEE Transactions on Signal Processing , 57(7):2479–2493 , 2009 .
[ 22 ] S . Yang , Z . Lu , X . Shen , P . Wonka , and J . Ye . Fused multiple graphical lasso . SIAM Journal on Optimization , 25(2):916–943 , 2015 .
[ 23 ] S . Yang , L . Yuan , Y C Lai , X . Shen , P . Wonka , and J . Ye . Feature grouping and selection over an undirected graph . In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 922–930 . ACM , 2012 .
[ 24 ] T . Yang , J . Wang , Q . Sun , D . P . Hibar , N . Jahanshad , L . Liu , Y . Wang , L . Zhan , P . Thompson , and J . Ye . Detecting genetic risk factors for Alzheimer ’s disease in whole genome sequence data via Lasso screening . In IEEE Intl . Symposium on Biomedical Imaging , 2015 .
[ 25 ] T . Yang , X . Zhao , B . Lin , T . Zeng , S . Ji , and J . Ye . Automated gene expression pattern annotation in the mouse brain . In Pacific Symposium on Biocomputing , page 144 , 2015 .
[ 26 ] J . Ye and J . Liu .
Sparse methods for biomedical data . ACM SIGKDD Explorations Newsletter , 14(1):4– 15 , 2012 .
[ 27 ] Q . Zhou , F . Zhao , Z p Lv , C g Zheng , W d Zheng , L . Sun , N n Wang , S . Pang , F . M . de Andrade , M . Fu , et al . Association between APOC1 polymorphism and Alzheimerˆa ˘A´Zs disease : A case control study and metaanalysis . PloS one , 9(1):e87017 , 2014 .
[ 28 ] Y . Zhu , X . Shen , and W . Pan . Simultaneous grouping pursuit and feature selection over an undirected graph . Journal of the American Statistical Association , 108(502):713–725 , 2013 .
[ 29 ] H . Zou and T . Hastie . Regularization and variable selection via the elastic net . Journal of the Royal Statistical Society : Series B ( Statistical Methodology ) , 67(2):301– 320 , 2005 .
APPENDIX In this supplement , we present all of the details we mentioned in the main text .
A . SETTING FOR EXAMPLE 1 The synthetic data set is generated via y = Ax + , where the data matrix A ∈ R500×500 and the noise term are randomly generated from normal distributions . The ground truth x is designed as :
• x1,,100,401,,500 = 0 ,
012 01 008 006 004 0020002004006RS8105340RS3112439RS3112440…RS421812RS3865427…RS71352237RS34224078RS35879138…RS12972156RS12972970RS34342646RS283810RS283811…RS283815RS6857RS71352238RS184017RS157580RS2075649RS2075650RS157581RS34095326RS34404554RS11556505RS157582RS59007384…UNKNOWNRS1160985RS760136UNKNOWNRS1038025RS1038026RS1305062RS10119RS7259620…RS769449RS769450RS429358RS7412RS1081105RS80125357…RS10414043RS7256200RS483082RS59325138RS438811…RS12721046RS12721056RS12721051RS56131196RS4420638RS78959900RS73052341RS4803770CoefficientsPVRL2TOMM40APOEAPOC1near 008 006 004 0020002004006RS8105340RS3112439RS3112440…RS34278513…RS3865427…RS71352237RS34224078RS35879138…RS12972156RS12972970RS34342646…RS283811RS283815RS6857RS76692773RS71352238RS184017…RS2075650RS157581RS34095326RS34404554RS11556505RS157582RS59007384RS157584RS157585RS157588…RS1160985RS760136…RS741780…RS1038025RS1038026RS1305062RS10119RS7259620…RS405509…RS769449RS769450RS429358…RS75627662…RS10414043RS7256200RS483082RS59325138…RS438811…RS12721046…RS12721051RS56131196RS4420638RS78959900CoefficientsPVRL2TOMM40APOEAPOC11962 • x101,,190,201,,290,301,,390 = 1 , • x191,,200,291,,300,391,,400 = −1 .
B . DC PROGRAMMING FOR SOLVING AFL
The AFL formulation in Eq ( 1 ) is a non convex optimization problem . We propose to use the DC programming to solve it . By noting that ||xi| − |xi+1|| = |xi + xi+1| + |xi − xi+1| − ( |xi| + |xi+1| ) , we decompose the objective function in Eq ( 1 ) into the difference of the following two functions : f1(x ) = loss(x ) + λ1x1 + λ2
( |xi + xi+1| + |xi − xi+1| ) , p−1 p−1 i=1
( |xi| + |xi+1| ) . f2(x ) = λ2 i=1
Denote the affine minorization of f2(x ) as f k
2 ( x ) = f2(xk)+ x − xk , ∂f2(xk ) , where ·,· refers to the inner product . Then the DC programming solves problem ( 1 ) by iteratively solving : f1(x ) − f k min x∈Rp
( 24 ) Since xk , ∂f2(xk ) is a constant , problem ( 24 ) is equivalent to :
2 ( x ) . f1(x ) − x , ∂f2(xk ) . min x∈Rp
( 25 ) and let ck = ∂f2(xk ) , problem ( 24 ) can be rewritten as : loss(x ) − ( ck)T x min x∈Rp p−1
+ λ1x1 + λ2
( |xi + xi+1| + |xi − xi+1| ) .
( 26 )
Note that i=1 i ) , ck i = λ2di sgn ( xk
( 27 ) where d1 = dp = 1 , di = 2 , 2 ≤ i ≤ p − 1 ; sgn(· ) is the signum function . In addition , since max ( |xi|,|xi+1| ) = 2 ( |xi + xi+1| + |xi − xi+1| ) , problem ( 26 ) is equivalent to max ( |xi|,|xi+1| ) . loss(x ) − ( ck)T x + λ1x1 + 2λ2 p−1
1 min x∈Rp i=1
C . PROOF FOR LEMMA 1
Proof : We prove these properties of the proximal operator problem ( 10 ) as follows . i ) If ui ≥ 0 and x∗ i < 0 , we can construct ˜x∗ as follows : j ,∀j = i . ∗ ∗ ∗ j = x i = 0 , ˜x ˜x
It can easily be shown that φ(˜x∗ ) < φ(x∗ ) . This contradicts with the fact that x∗ is the minimizer to ( 10 ) . If ui ≥ 0 and x∗ i > ui , we can construct ˜x∗ as follows : ∗ ˜x i = ui , ˜x j ,∀j = i . ∗
∗ j = x
It can easily be shown that φ(˜x∗ ) < φ(x∗ ) . This contradicts with the fact that x∗ is the minimizer to ( 10 ) . ii ) This property can be proved in a similar way as i ) . iii ) Let ˜x∗ = πλ(|u| ) . We have p−1 i=1 p−1
φ(sgn(u ) fi ˜x ∗
) =
1 2
+ λ
=
1 2
+ λ
=
1 2 sgn(u ) fi ˜x
∗ − u2 max(| sgn(ui)˜x i |,| sgn(ui+1)˜x i+1| ) ∗ ∗ sgn(u ) fi ( ˜x
∗ − |u|)2 max(|˜x i |,|˜x i+1| ) ∗ ∗ i=1
˜x
∗ − |u|2 + λ p−1 i=1 max(|˜x i |,|˜x i+1| ) . ∗ ∗
Since ˜x∗ = πλ(|u| ) and the minimizer is unique , it follows that sgn(u ) fi ˜x∗ needs to minimize φ(x ) . iv ) We only focus on the case ui ≥ ui+1 ≥ 0 in the proof and the results can be generated to the rest the cases using property iii ) . With properties i ) and ii ) , we have ui ≥ x∗ i+1 ≥ 0 . If this property does not hold , we have : i ≥ 0 and ui+1 ≥ x∗ ui ≥ ui+1 ≥ x ∗ i+1 > x i+1 > x∗ Next , we show that x∗ With a non negative and assuming x∗ construct ¯x∗ and ˜x∗ as follows : i ≥ 0 . ∗ i leads to a contradiction . i , we i+1 − > x∗
( 28 ) j ,∀j = i + 1 , i+1 − , ¯x ∗ ∗ ∗ ∗ j = x i+1 = x ¯x j ,∀j = i . ∗ ∗ ∗ ∗ i = x i+1 + , ˜x j = x
˜x
( 30 ) where the i + 1 entry of x∗ is decreased by in constructing ¯x∗ and the i entry of x∗ is increased by in constructing ˜x∗ . Denote
( 29 ) d = − 1 2 i+1 < x∗
If x∗ i+2 , we have i+1 − ui+1)2 + ∗ ( x i+1 − − ui+1)2 . ∗
( x
1 2
φ(¯x ) − φ(x
∗
) = d − λ .
If x∗ i+1 − > x∗ i+2 , we have φ(¯x ) − φ(x ∗
) = d − 2λ .
If x∗ i+1 ≥ x∗ i+2 ≥ x∗ i+1 − , we have
∗
φ(¯x ) − φ(x φ(¯x ) − φ(x
∗
) ≤ d − λ . ) ≥ d − 2λ .
( 31 )
( 32 )
( 33 )
( 34 )
In summary , we have
φ(¯x ) − φ(x
∗
) ≤g1( ) = − 1 2 i+1 − ui+1)2 ∗ ( x i+1 − − ui+1)2 − λ . ∗ ( x
( 35 )
+
1 2
Similarly , we have φ(˜x ) − φ(x
∗
) ≤ g2( ) = − 1 2 1 2
+ i − ui)2 ∗ ( x i + − ui)2 + λ . ( 36 ) ∗ ( x
It is hard to directly prove either g1( ) or g2( ) is negative in the case of ( 28 ) . To arrive at the contradiction ,
1963 we let
G( ) = g1( ) + g2( )
= − 1 ( x 2 − 1 2 i+1 − ui+1)2 + ∗ i − ui)2 + ∗ 1 2
( x
( x i+1 − − ui+1)2 ∗ 1 ( x 2 i + − ui)2 ∗
( 37 )
The derivative of G( ) is
G
( ) = 2 + ( ui+1 − ui ) + ( x i − x ∗ ∗ i+1 ) .
( 38 ) Making use of ( 28 ) , we can arrive at G( ) < 0 when i+1 − x∗ x∗ i
∈ ( 0 ,
( 39 ) For any satisfying ( 39 ) , we have G( ) < 0 , since G(0 ) = 0 and G( ) < 0 . Therefore , there exists that satisfies ( 39 ) . Hence
) .
2
∗ ( φ(¯x
) − φ(x
∗
∗ ) ) + ( φ(˜x
) − φ(x
∗
) ) < 0 .
( 40 )
∗
This leads to the fact that at least one of the following two inequalities holds : ∗ ( φ(¯x ∗ ( φ(˜x
) − φ(x ) − φ(x
) ) < 0 ,
) ) < 0 .
This contradicts with the fact that x∗ is the minimizer to ( 10 ) . Therefore , we cannot have x∗ i in the case ui ≥ ui+1 ≥ 0 . i+1 > x∗
∗ v ) This property can be proved in a similar way as iv ) .
This ends the proof to Lemma 1 .
Based on Lemma 1 , we also have the following remark that summarizes the properties of w : Remark . wi = 2 indicates 1 < i < p , ui−1 < ui ≤ ui+1 . wi = 1 holds in one of the following four cases : 1 ) i = 1 , u1 ≥ u2 ; 2 ) i = p , up−1 < up ; 3 ) 1 < i < p , ui ≥ ui+1 , ui ≤ ui−1 ; 4 ) 1 < i < p , ui < ui+1 , ui > ui−1 . wi = 0 holds in one of the following three cases : 1 ) i = 1 , u1 < u2 ; 2 ) i = p , up−1 ≥ up ; 3 ) 1 < i < p , ui < ui+1 , ui ≥ ui−1 .
In addition , it is easy to get thatp i=1 wi = p − 1 .
Proof : According to Lemma 1 i ) and ii ) , we have that the optimal solution to ( 10 ) is non negative , ie , x∗ ≥ 0 . Incorporating the definition of R in ( 11 ) and Lemma 1 iv ) and v ) , we have Rx∗ ≤ 0 . Therefore , we have x∗ ∈ P , where P is defined in ( 14 ) . It is easy to verify that P is a closed convex and nonempty polyhedron . Thus x∗ = πλ(u ) is the optimal solution to min x∈P x − u2 + λ
1 2 max(|xi|,|xi+1| )
.
Making use of the definitions of R and w in ( 11 ) and ( 12 ) , ∀x ∈ P , we have p max(|xi|,|xi+1| ) = wixi . i=1 i=1 p−1 i=1 p−1
D . PROOF FOR THEOREM 2
Let
Therefore , x∗ = πλ(u ) is the optimal solution to the problem : min x∈P x − u2 + λ
1 2 wixi
.
( 41 ) p i=1
Incorporating ( 13 ) , we can easily verify that , πP λ ( v ) , the optimal solution to(15 ) is also the optimal solution to ( 41 ) . Thus , ( 16 ) holds .
E . PROOF FOR THEOREM 3
Proof : We prove ( 19 ) by the technique of KKT optimality conditions . By introducing the dual variables w ∈ Rp for the inequality x ≥ 0 , and z ∈ Rp−1 for the inequality Rx ≤ 0 , we can write the Lagrangian of ( 15 ) as :
L(x , w , z ) =
1 2 x − v2 − wT x + zT Rx .
( 42 )
The inequality constraint functions in ( 15 ) are affine , and thus Slater ’s condition holds , which indicates strong duality . Let ¯x and ( ¯w , ¯z ) be any primal and dual optimal points with zero gap for ( 15 ) . The KKT optimality conditions require the following necessary and sufficient conditions :
¯x ≥ 0 , R¯x ≤ 0 , ¯w ≥ 0 , ¯z ≥ 0 , ¯x = v + ¯w − R¯z .
( 43 )
( 44 )
( 45 )
( 46 )
( 47 )
Following a similar analysis , we introduce the dual variable z ∈ Rp−1 for the inequality Rx ≤ 0 , and write the Lagrangian of ( 18 ) as : L(x , z ) = x − v2 + zT Rx .
( 48 )
1 2
Let ˜x and ˜z be any primal and dual optimal points with zero gap for ( 18 ) . The KKT optimality conditions requires the following necessary and sufficient conditions :
R˜x ≤ 0 , ˜z ≥ 0 , ˜x = v − R˜z . x w
∗ ∗ ∗
= max(˜x , 0 ) , = max(˜x , 0 ) − ˜x , = ˜z .
( 49 )
( 50 )
( 51 )
( 52 )
( 53 ) z
( 54 ) Next , we show that x∗ and ( w∗ , z∗ ) satisfy the KKT conditions ( 44) (47 ) . It is easy to verify the relationships in formulations ( 44 ) , ( 46) (47 ) . Rx∗ ≤ 0 holds as : 1 ) R˜x ≤ 0 , 2 ) x∗ = max(˜x , 0 ) , and 3 ) each row of R only contains two entries 1 and 1 . As the objectives of ( 15 ) and ( 18 ) are strictly convex , ¯x and ˜x are both unique . Therefore , it follows from ( 53 ) that ( 19 ) holds .
1964
