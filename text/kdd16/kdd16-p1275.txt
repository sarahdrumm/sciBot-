Robust Extreme Multi label Learning
Chang Xu∗† xuchang@pkueducn
Dacheng Tao†
DachengTao@utseduau
Chao Xu∗ xuchao@cispkueducn
Key Laboratory of Machine Perception , Cooperative Medianet Innovation Center , Peking University∗ , Beijing , China
Centre for Quantum Computation and Intelligent Systems , University of Technology† , Sydney
ABSTRACT Tail labels in the multi label learning problem undermine the low rank assumption . Nevertheless , this problem has rarely been investigated . In addition to using the low rank structure to depict label correlations , this paper explores and exploits an additional sparse component to handle tail labels behaving as outliers , in order to make the classical low rank principle in multi label learning valid . The divideand conquer optimization technique is employed to increase the scalability of the proposed algorithm while theoretically guaranteeing its performance . A theoretical analysis of the generalizability of the proposed algorithm suggests that it can be improved by the low rank and sparse decomposition given tail labels . Experimental results on real world data demonstrate the significance of investigating tail labels and the effectiveness of the proposed algorithm .
CCS Concepts •Computing methodologies → Supervised learning ;
Keywords Multi label Learning ; Low rank Algorithm
1 .
INTRODUCTION
In contrast to conventional single label learning , in which each example is assigned only one label , multi label learning evaluates examples with multiple labels . Many real world applications use multi label learning [ 31 ] including text categorization and image/video annotation [ 7 ] . The rapid evolution of information techniques has fueled the emergence of large scale multi label applications with huge numbers of labels . For example , given over a million labels ( categories ) on Wikipedia , one might wish to build a classifier to annotate a new article or web page with a subset of the most relevant categories . In another example , taking billions of YouTube videos as distinct labels , a task might be to recommend a ranked list of labels to a single user . Hence , extreme multilabel learning with an extremely large number of labels has become an important research focus .
Binary relevance ( BR ) [ 24 ] seeks to independently train a classifier for each label . BR is a straightforward approach for multi label learning , but due to prohibitive training and prediction costs arising from large numbers of labels , this method becomes less useful . A number of embedding based approaches have been proposed to overcome this extreme multi label learning problem that reduce the effective number of labels . The approaches assume that the label matrix is low rank . Different techniques can be used to compress and decompress label vectors , including compressed sensing [ 15 ] , Bloom filters [ 9 ] , SVD [ 23 ] , landmark labels [ 2 , 5 ] , and output codes [ 32 ] .
The low rank label matrix assumption in embedding methods is violated in many real world applications due to the presence of tail labels occurring in a handful of data points . Histograms of the label matrices on the wiki10 , DeliciousL and Amazon datasets are shown in Figure 1 . There are more than 104 labels which occur in at most 2 examples on each dataset , such that they are not well approximated by any linear low dimensional basis . This tail label issue is frequently and persistently neglected in multi label learning . Recently , instead of projecting label vectors into a linear low rank subspace , [ 4 ] addressed the tail label problem by learning embeddings that non linearly captured label correlations by preserving the pairwise distances between label vectors . Although this provides an alternative approach to handling tail labels , we must also ask whether low rank based approaches really are now redundant .
Here we revisit the classical low rank principle in multilabel learning and suppress the influence of tail labels . Tail labels can be regarded as label matrix outliers , inspiring us to decompose the label matrix into a low rank part that depicts label correlations and a sparse part that captures tail labels . Various effective embedding techniques are then applicable . To improve scalability and leverage the growing availability of parallel computing architectures , the divideand conquer approach is adopted to optimize the resulting objective function , whose performance can be theoretically guaranteed with high probability . Our theoretical analysis shows that the proposed low rank and sparse decomposition is useful for improving the generalizability of multilabel learning algorithms . Experiments on real world data demonstrate the significance of studying tail labels in multilabel learning and the effectiveness of the proposed algorithm .
1275 Figure 1 : The number of examples in which each label is present on the ( a ) Wiki10 ( b ) Delicious L and ( c ) Amazon datasets . The horizontal axis indicates the index of label , while the vertical axis indicates the number of associated examples .
2 . RELATED WORK
The multi label learning problem has been extensively studied in practice [ 31 ] and theory [ 13 , 27 ] . Broadly speaking , existing multi label learning algorithms can be categorized into two groups : algorithm adaptation and problem transformation methods . Algorithm adaptation methods adapt , extend and customize an existing machine learning algorithms for the task of multi label learning , while problem transformation methods transform the multi label learning problem into one or more single label classification or regression problems . Representative examples include boosting [ 20 , 26 , 16 ] , decision trees [ 25 , 17 ] , neural networks [ 29 , 10 ] , support vector machines [ 12 , 14 ] and k nearest neighbors classifier [ 30 , 21 ] .
The key in multi label learning is modeling inter label correlations and using them for label vector prediction . In many applications such as text categorization and functional genomics , the labels are often organized in the form of a tree or directed acyclic graph , so that label relationship can be exploited from the knowledge resources as prior knowledge for multi label learning [ 6 , 14 ] . In most real world tasks , however , prior knowledge of label relationship is often unavailable . It is thus necessary to model the label relationship directly . For example , Sun et al . [ 22 ] proposed to employ hypergraphs to exploit the higher order relations among multiple instances sharing the same label in multi label learning . They constructed a hyperedge for each label , and included all instances annotated with a common label into one hyperedge , thus capturing their joint similarity . Zhang and Zhang [ 28 ] used a Bayesian network to characterize the dependence structure between multiple labels , and learned a binary classifier for each label by treating its parental labels in the dependence structure as additional input features . However , as the number of labels keeps growing , these algorithms are usually computationally infeasible .
Embedding based approaches are significant for handling extreme multi label learning problem by reducing the effective number of labels . Generally , they assume that the label matrix is low rank , and then project label vectors into a lowdimensional subspace . Hence , instead of directly learning to predict the original high dimensional label vector of each example , the training complexity is largely decreased by first learning the predictor of embedded label vectors , and then a decompression operation is employed to lift the embedded label vectors back to the original label space .
Various compression and decompression techniques have been exploited by existing embedding methods . Hsu et al . [ 15 ] addressed classification problem with a large number of labels via a three step approach . First , random transformation is used to project the high dimensonal label vector into a low dimensional space ; next , a regression model is trained to predict each dimension of the transformed label vector ; finally , for a test example , its predicted label vector in the low dimensional space is projected back to the original label space . Considering the drawback of random transformation in [ 15 ] , Tai and Lin [ 23 ] proposed the principal label space transformation , which uses principal component analysis ( PCA ) to accomplish the compression operation on the high dimensional label vector . Since PCA in the label space only focuses on minimizing the encoding error between high dimensional feature vectors and their low dimensional representations [ 23 ] , Chen and Lin [ 8 ] proposed conditional principal label space transformation , which improves [ 23 ] by simultaneously considering the label encoding error and training error in the low dimensional label space . Based on canonical correlation analysis ( CCA ) , Zhang and Schneider [ 32 ] also took both feature matrix and label matrix into consideration . After that , a maximum margin formulation was developed to learn an output coding , which is predictive and discriminative so that the codings for different label vectors are easy to predict and significantly different from each other . Instead of using label transformation , Balasubramanian and Lebanon [ 2 ] proposed to train only a small subset of the labels , which come from the original labels , so that the difficulty of the learning problems can be decreased . Supposing that the non selected labels are to be faithfully and easily constructed from the selected ones , a group sparse learning problem is investigated to discover the optimal label subset [ 2 ] . However , the structured sparsity optimization problem in [ 2 ] is computationally expensive , especially when there are a lot of labels to select from . Bi and Kwok [ 5 ] alleviated this problem by proposing an efficient label selection method based on randomized sampling . Following the assumption in [ 2 ] , Bi and Kwok [ 5 ] designed the sampling
100101102103104105100101102103104105106100101102103104105100101102103104(a) (b) (c) 1276 probability of each label using its leverage score in the best rank k subspaces of the label matrix .
Recently , Yu et al .
[ 27 ] modeled multi label classification as a general empirical risk minimization ( ERM ) problem with a low rank constraint , which generalizes both label and feature dimensionality reduction . Given squaredL2 loss , LEML algorithm in [ 27 ] has a closed form solution , and can be reduced to the conditional principal label space transformation algorithm in [ 8 ] . Various loss functions and regularizers are applicable in this ERM framework for preventing overfitting and increasing scalability . However , as suggested by [ 4 ] , the low rank assumption in embedding based approaches is easily violated by tail labels in real world datasets with a large number of labels . Instead of globally projecting high dimensional label vectors into a low rank subspace , SLEEC algorithm in [ 4 ] learns low dimensional embedding which non linearly capture label correlations by preserving the pairwise distances between only the closest label vectors . Regressors are then trained to predict the embedded label vector for each example . During prediction , rather than using a decomposition matrix , SLEEC uses a k nearest neighbor ( kNN ) classifier in the embedding space , which leverages the fact that nearest neighbors have been preserved during training .
3 . PROBLEM FORMULATION Given a training data set {(x1 , y1),··· , ( xn , yn)} , where xi ∈ Rd is the feature vector of the i th example , and yi ∈ {−1 , 1}L is the corresponding label vector , the feature matrix is denoted as X = [ x1;··· ; xn ] ∈ Rn×d and the label matrix is Y = [ y1;··· ; yn ] ∈ {−1 , 1}n×L . If Yij = 1 , example xi will have label j ; otherwise , there is no label i for example xj . Multi label learning aims to learn a hypothesis f : Rd → {−1 , 1}L that accurately predicts the label vector for a given example .
There are a large number of labels in the extreme multilabel learning setting . Tail labels , which only occur with several examples , cannot be ignored , and a number of rows in the label matrix Y will have plenty of −1 valued entries while occasionally being dotted with several 1 valued entries . Due to the existence of these rows , the classical low rank assumption on the label matrix is violated . We attempt to modernize the low rank principle by carefully formulating the tail labels in the extreme multi label learning problem . 3.1 Robust Extreme Multi label Learning
One reasonable approach to interpret the label matrix in multi label learning is to assume that the label matrix can be well approximated using a low dimensional subspace . This assumption has been well justified in many practical situations . A general model can be written as
F + λ(f ( X , W ),Y )
Y −Y 2 minY ,W st rank(Y ) ≤ k .
( 1 ) where Y is the low rank approximation of Y , and loss functechniques have been used to formulate the low rank Y , the tion ( · ) is employed to penalize the loss generated by a multi label predictor parameterized by W .
Problem ( 1 ) has been studied for decades , and various predictor f ( · ) and the loss function ( · ) . However , as mentioned above , tail labels damage the low rank assumption over Y and render the estimated Y arbitrarily far from the true Y ; the learned multi label predictor is , therefore , seriously influenced as a result . A method that can extract the low rank components from Y even in the presence of tail labels would be desirable .
To achieve this , we treat tail labels as outliers and decom pose the label matrix to where YL is of low rank and depict label correlations and YS is the sparse component capturing the influence of tail
( 2 ) labels . These two components can be obtained by solving the following objective :
Y = YL +YS ,
( 3 )
Y −YL −YS2 minYL,YS st rank(YL ) ≤ k ,
F card(YS ) ≤ s .
Given the low rank and sparse components YL and YS , we the input features . That is , we require that YL ≈ W X and YS ≈ HX , where W , H ∈ Rd×L . Hence , problem ( 3 ) can be expect to learn regression models that predict them using reformulated as :
Y − XW − XH2 min W,H st rank(XW ) ≤ k ,
F card(XH ) ≤ s .
( 4 )
Since the rank and card constraints tend to increase the optimization complexity , we employ two popular matrix factorization heuristics ( to encourage low rankness ) and L1 norm minimization ( to encourage sparsity ) to relax the constraints in problem ( 4 ) , such that min U,V,H
Y − XU V − XH2 F + V 2
+ λ2(U2
F + λ1H2 F ) + λ3XH1 ,
F
( 5 ) where {λ1 , λ2 , λ3} are positive constants , W is supposed to have W = U V given U ∈ Rd×k and V ∈ Rk×L , and an L2 regularization has been included for H . It is expected that by solving problem ( 5 ) , we can obtain a low rank function ( ie , W = U V ) and a sparse function ( ie , H ) , which together are used for multi label prediction .
4 . OPTIMIZATION
In this section , we first present the basic optimization method for problem ( 5 ) and then adopt the divide andconquer strategy to develop an optimization method applicable to extreme multi label learning . 4.1 Basic Optimization Method
Problem ( 5 ) can be solved by alternatively solving the following three subproblems until convergence :
V = arg min
U = arg min
H = arg min
H
V
U
Y − XU V − XH2 Y − XU V − XH2 Y − XU V − XH2 + λ3XH1 .
F
F + λ2V 2 F + λ2U2 F + λ1H2
F
F
( 6a )
( 6b )
( 6c )
In the following , we illustrate the optimization of these three subproblems respectively .
1277 Solving V
411 In problem ( 6a ) , updating V is simple since each column vj of V can be independently updated :
Yij − xiU vj − ( XH)ij2 + λ2vj2 2 ,
( 7 ) n i=1 min vj which is easy to solve in a closed form as the dimension of vj ( ie k ) is generally small . Setting the gradient of Eq ( 7 ) wrt vj to zero , the optimal v∗ j corresponding to the j th label can be obtained as
−1 n
,Yij − ( XH)ij
(xiU )T
( xiU )T xiU + λ2I n
∗ j = v i=1 i=1
412 Solving U Problem ( 6b ) is equivalent to n
L min
Yij − uT Xij − ( XH)ij2 + λ2u2 , u i=1 j=1 where u ∈ Rdk denotes vec(U ) , and Xij = vec(xivj ) . There ,Yij−(XH)ij Xij
Xij(Xij)T +λ2I is a closed form solution for this problem ,
−1 n n
L
L
= u
∗ i=1 j=1 i=1 j=1
However , it is inefficient to compute the closed form solution for the above problem when d is large due to the huge computational cost of inverting a dk × dk matrix . Instead , it is more appropriate to employ efficient gradient descent methods ( eg conjugate gradient descent ) for optimization . 413 Solving H
In problem ( 6c ) , given Y = Y − XU V , each column Hj can be independently solved , n i=1 min Hj
Yij − xiHj2
F + λ1Hj2
F + λ3XHj1 .
( 9 )
The L1 norm in problem ( 9 ) involves both Hj and X , which makes the problem non smooth and disallows the standard proxy function based optimization methods . One way to circumvent this difficulty is by introducing an auxiliary variable Zj = XHj and transforming problem ( 9 ) into n
Yij − xiHj2 i=1
+ ρXHj − Zj + µ2 F , min
Hj ,Zj ,µ
F + λ1Hj2
F + λ3Zj1
( 10 ) where ρ > 0 is the penalty parameter , and µ is the scaled dual variable .
Fixing Hj and µ , problem ( 10 ) is reduced to Zj1 .
XHj − Zj + µ2
F + min Zj
λ3 ρ
( 11 )
The optimal Z∗ ation [ 11 ] , j can be obtained via soft thresholding oper
∗ j = sof t(XHj + µ ,
Z
λ3 ρ
) , where sof t(a , b ) = sign(a ) max(|a| − b , 0 ) .
( 12 )
( 13 )
( 8 ) min Hj
Algorithm 1 Robust Extreme Multi label Learning
Input : X , Y , t ≥ 1 For i = 1,··· , t do In Parallel Sample ( Y )i ⊆ Y repeat
Solve ( V )i from Problem ( 6a ) Solve ( U )i from Problem ( 6b ) Solve ( H)i from Problem ( 6c ) ( W )i = ( U )i(V )i ColumnProjection ( [(W )1,··· , ( W )t ] , ( W )1 ) until Convergence
End
Fixing Zj and µ , Hj can be solved from the following objective function : n Yij−xiHj2 ∇Hj J = 2 , n i=1
F +λ1Hj2
F +ρXHj−Zj +µ2
F . ( 14 ) xT i xi + λ1I + ρX T X Hj − 2 , n i Yij + ρX T ( Zj − µ) . xT i=1
( 15 )
The gradient wrt Hj is calculated as i=1
By setting Eq ( 15 ) to zero , Hj can be easily solved out in a closed form . On the other hand , if the dimension of Hj is great , cheap gradient descent optimization method can be applied . µ can be updated via
µ ← µ + XH − Z .
( 16 )
Through alternatively updating Hj , Zj and µ , the optimal Hj for problem ( 9 ) can be achieved . 4.2 Divide and Conquer Optimization
The divide and conquer strategy can be employed to increase the algorithm ’s capability for handling extremely large numbers of labels . The original optimization problem is first divided into cheaper sub problems that can be efficiently solved in parallel . The solutions to these subproblems can then be combined to achieve the final solution . The whole optimization procedure is summarized in Algorithm 1 . Divide Step . Given the label matrix Y ∈ {−1 , 1}n×L , we randomly partition the matrix Y into t m column submatrices {(Y )i}t i=1 , where we suppose L = tm and each ( Yi ) ∈ {−1 , 1}n×m . Hence , the original problem is divided into t sub problems regarding {(Y )1,··· , ( Y )t} , respectively . The basic optimization method described in Section 4.1 can be adopted to solve these sub problems in parallel , which
) . outputs the solutions(,(W )1 , ( H)1 obtained by projecting [ (W )1,··· , ( W )t ] onto the column space of ( W )1 . After obtaining W , we can then launch the optimization over each each column of H in parallel to obtain
Conquer Step . This conquer step exploits column projection to integrate the solutions of sub problem solutions . The final approximation W to problem ( 5 ) can thus be
,··· ,,(W )t , ( H)t the sparse component of the resulting multi label predictor .
1278 5 . THEORETICAL ANALYSIS
In this section , we prove the upper bounds on the estimation error of the basic and divide and conquer optimization methods , respectively . The generalizability of our learning model is also analyzed . 5.1 Estimation Error Given a feature matrix X ∈ Rn×d and the corresponding label matrix Y ∈ {−1 , 1}n×L for n training points of L labels , we suppose that
Y = XW0 + YS + ,
( 17 ) where W0 ∈ Rd×L is the ground truth weight , YS is the sparse component to formulate the influence of tail labels , and is a data independent noise term . Given this training data , we aim to estimate W0 by performing empirical risk minimization:W = arg min where Y = Y − YS . Note that although the method in
= Y − XW2
F + λW∗ ,
( 18 )
W
Eq ( 4 ) uses a regularized rank constrained formulation , we analyze the trace norm regularized version without the rank constraint for simplicity . Since the class of rank constrained matrices is smaller than the class of trace norm constrained matrices , we can in fact expect better theoretical results here .
We generically denote the estimator for ΣXX by X T X , and the estimator for ΣXY by X TY . We require ΣXX to be positive semidefinite . Thus , the estimator for W naturally becomes,W = arg min We use the following theorem to show thatW solved in Eq
W , ΣXX W − 2ΣXY , W + λW∗ .
( 19 )
W
( 19 ) is an approximated estimation of the true W0 .
Theorem 1 . Suppose the smallest eigenvalue of ΣXX is bounded by σmin(ΣXX ) ≥ σ > 0 . The estimation error satisfies
,2ΣXY − ΣXX W0∗ − λ .
W − W0F ≤ 1 Proof . Let ∆ =W − W0 . Considering the optimality of W for problem ( 19 ) , we have W0 + ∆ , ΣXX ( W0 + ∆ ) − 2ΣXY , W0 + ∆ + λW0 + ∆∗ ≤W0 , ΣXX W0 − 2ΣXY , W0 + λW0∗ ,
( 20 )
σ which can be rearranged to
∆ , ΣXX ∆ ≤ 2ΣXY − ΣXX W0 , ∆ − λ∆∗ .
( 21 )
Since the smallest eigenvalue of ΣXX is bounded , we have
∆ , ΣXX ∆ ≥ σ∆2 F .
( 22 ) Given ∆∗ ≥ ∆F , the right hand side of Eq ( 21 ) can be upper bounded by
2ΣXY −ΣXX W0 , ∆ − λ∆∗
Combining all the above results , we get
≤2ΣXY − ΣXX W0∗∆F − λ∆F . F ≤ 2ΣXY − ΣXX W0∗∆F − λ∆F .
σ∆2
( 23 )
( 24 )
The result then follows .
According to Theorem 1 , the estimation error bound de pends on ΣXY = X T ( Y − YS ) , and thus YS can be interpreted as the perturbations of ΣXY . Since the magnitude of YS is no greater than that of Y , the tail labels will not overwhelm the estimation over W .
We next analyze the estimation error in the divide andconquer optimization method . Suppose the compact singular value decomposition ( SVD ) of W is UW ΣV T W , where Σ is diagonal and contains k non zero singular values of W , and UW ∈ Rd×k and VW ∈ RL×k are the corresponding left and right singular vectors of W . We assume the true weight W0 is ( µ , k) coherent , whose definition is given as below , dL
Definition 1 . Given µ0(VW ) = d k max1≤i≤d ( VW )i2 and k maxi,j |eT
µ1(W ) = rank(W ) = k , max(µ0(VW ) , µ0(VW ) ) ≤ µ and µ1(W ) ≤ µ , we call W is ( µ , k) coherent . i UW V T
W ej| , for any µ > 0 , if
We first invoke a lemma from [ 18 ] to show that column projection can produce an approximation that is nearly as good as a given rank k target by sampling a number of columns proportional to the coherence .
Lemma 1 . [ 18 ] . Given a matrix W ∈ Rd×L and a rank k approximation A ∈ Rd×L , choose m ≥ ckµ log(L ) log(1/δ)/ 2 , where c is a fixed positive constant , and let ( cid:102)W ∈ Rd×m be a matrix of m columns of W sampled uniformly without replacement . Then , with probability at least 1 − δ ,
W −(cid:102)W projF ≤ ( 1 + )W − AF , where ( cid:102)W proj ∈ Rd×L is derived by projecting ( cid:102)W onto the
( 25 ) space of W .
Recall that the true weight W0 has been partitioned into t sub matrices {(W0)1,··· , ( W0)t} that are solved by optimizing distinct sub problems in parallel and correspond to t estimated sub matrices {(W )1,··· , ( W )t} . We employ colhelp of W , and denote it as W proj . The difference between W0 and W proj can be bounded by the following theorem . umn projection to derive the approximation of W0 with the
Theorem 2 . For m ≥ ckµ log(L ) log(1/δ)/ 2 , where c is a fixed positive constant . The original problem has been divided into t sub problems . If a basic optimization method yields the estimation error satisfying Theorem 1 for each sub problem , then with probability at least 1 − δ , the estimation error in the divide and conquer method is bounded by
W0 −W projF ≤ 2 +
σ
,2ΣX(Y )i
− ΣXX ( W0)i∗ − λ t i=1
Proof . According to Lemma 1 , with probability at least
1 − δ , the following inequality holds :
W −W projF ≤ ( 1 + )W − W0F .
By adding W − W0F to both sides of the above inequality , ( 2 + )W − W0F ≥ W −W projF + W − W0F we get
( 26 )
≥ W0 −W projF ,
1279 which implies that
W0−W projF ≤ ( 2 + )W − W0F t ( W0)i − ( W )iF t ,2ΣX(Y )i
≤ ( 2 + )
≤ 2 + σ i=1 i=1
− ΣXX ( W0)i∗ − λ .
This ends the proof .
Compared to the estimation error bound derived by applying a basic optimization method to estimate W0 in Theorem 1 , Theorem 2 exhibits an approximate recovery error with appropriate probability . It is instructive to note that the divide and conquer approach provides a controlled increase in error and a controlled decrease in the probability of success . Users can , therefore , adjust the optimization speed and accuracy . 5.2 Generalization Error from the distribution Q = X × Y , the proposed model aims to learn
( W , H ) ∈ F = W × H by performing ERM as follows :
Given n multi label points sampled iid
L(W , H ) = n
1 n inf rank(W )≤k card(XH)≤s
( yi , f ( xi , W , H) ) ,
( 27 ) where L(W , H ) is the empirical risk of the predictor ( W , H ) . Our goal would be to show that ( W , H ) has good general i=1 ization properties , that is :
L(W , H ) ≤
L(W , H ) + ,
( 28 ) inf rank(W )≤k card(XH)≤s where L(W , H ) = E(x,y)[(y , f ( x , W , H) ) ] is the population risk of the predictor . The Rademacher complexity is an effective way to measure the richness ( complexity ) of the function class F , based on which the generalization error bound of the learning algorithm can easily be obtained using standard approaches [ 3 ] .
Definition 2 . Given a sample S = {x1,··· , xn} ∈ X n and a real valued function class F defined on a space X , the empirical Rademacher complexity of F is defined as
Rn(F ) = Eσ sup f∈F fififififi 2 n n j=1
σjf ( xj ) where σ = ( σ1,··· , σn ) are independent uniform {±1} valued Rademacher random variables . The Rademacher complexity of F is fififififi x1,··· , xn fififififi 2 n L j=1 n
, fififififi
σjf ( xj )
. sup f∈F n
The Rademacher complexity of the proposed multi label learning algorithm can thus be written as :
Rn(F ) = Es(Rn(F ) ) = Ex,σ
Rn(F ) =
Ex,σ
2 n sup
( W,H)∈F
σi xi(wj + hj )
, i=1 j=1 where wj and hj correspond the j th columns of W and H , respectively , and together determine the predictor for
.
( 30 ) nkLΘΛ + L3/2Ω n sup
W + H , X W , X + H , X W∗XF + XH∗ √ kWFXF + n
√
LXH1
.
2 ≤ nLΛ2 ,
=
2 n ≤ 2 n ≤ 2 n
Ex,σ
Ex,σ
Ex,σ sup
( W,H)∈F sup
( W,H)∈F sup
( W,H)∈F the j th label . We denotex as the weighted summarization n i=1 σixi . L copies ofx are then stacked into X . Hence , the multi label Rademacher complexity can be simply written as
W + H , X
,
( 29 )
Rn(F ) =
Ex,σ
2 n sup
( W,H)∈F whose upper bound can be revealed by the following theorem .
Theorem 3 . The proposed algorithm learns W and H sampled from over n training points with L labels iid distribution Q = X × Y . For any data point , x2 ≤ Λ . The learning algorithm encourages that rank(W ) ≤ k and XH1 ≤ Ω . WF is assumed to be upper bounded by Θ . Then , the Rademacher complexity of F is
√
Rn(F ) ≤ 2
Proof . According to Eq ( 29 ) , we have
Rn(F ) =
Ex,σ
2 n
( W,H)∈F
Then the following bounds can be easily obtained ,
Ex,σX2 and
This proves
2 = Ex,σL[
F = Ex,σLx2 Ex,σXH1 ≤ ExLXH1 ≤ LΩ .
σixi]2 i=1
√
Rn(F ) ≤ 2 nkLΘΛ + L3/2Ω n
.
( 31 )
, and thus the large rank(W corresponding Rademacher complexity of O(rank(W
To make conclusions of Theorem 3 , consider a typical algorithm that neglects tail labels and attempts to minimize the = W +H for multi label learning and with trace norm of W )/n ) . As shown above , tail labels will significantly violate the lowrank assumption on W ) will lead to a greater Rademacher complexity . In contrast , the bound revealed in Theorem 3 is composed of two components corresponding to the low rank W and sparse XH , respectively . By separating the influence of tail labels , the rank of W ( ie , k ) will be smaller . Although tail labels might influence multi label learning , its significance on the bound can be dramatically decreased by constraining the sparsity of XH ( ie XH1 ≤ Ω ) . Hence , decomposing the multilabel model into low rank and sparse parts to handle tail labels is helpful for decreasing the Rademacher complexity of the function class , which in turn improves the generalization error bound of the algorithm .
1280 Table 1 : Statistics of the datasets used in experiments .
Dataset Bibtex Delicious S Mediamill Wiki10 Delicious L Amazon
#training #test #features #labels #card label #card feature
4,880 12 , 920 30 , 993 14 , 146 196,606 490,449
2,515 3,185 12,914 6,616
100,095 153,025
1,836 500 120
101,938 782,585 135,909
159 983 101
30,938 205,443 670,091
2.40 19.03 4.38 18.64 75.54 5.45
68.74 18.17 120.00 673.45 301.17 75.68
Figure 2 : Top k precision and nDCG@k of multi label learning algorithms on the Wiki10 dataset .
Figure 3 : Top k precision and nDCG@k of multi label learning algorithms on the Delicious L dataset .
Figure 4 : Top k precision and nDCG@k of multi label learning algorithms on the Amazon dataset .
6 . EXPERIMENTS
In this section , we evaluate the proposed algorithm on six benchmark multi label datasets : Bibtex , Delicious S , Mediamill , Wiki10 , Delicious L and Amazon . All these datasets were provided by [ 4 ] , and have already been pre separated into training and test sets . A summary of the statistics of datasets is shown in Table 1 . #training is the number of training examples ; #test is the number of test examples ; #features is the number of features ; #labels is the number of labels ; #card label is the average number of positive labels per example ; #card feature is the average number of nonzero features per example . The first three datasets with less than 1,000 labels are regarded as small datasets , while the last three are large datasets . We set the embedding dimension in REML algorithm as 0.8L for the small datasets , and 200 for the large datasets . Since the Delicious L and Amazon datasets are rather large , in optimizing REML we divided the original problems into 2 and 4 subproblems respectively using the divide and conquer strategy . For the other datasets , we directly solve REML using the basic optimization method .
In experiments , we compared our proposed Robust Extreme Multi label Learning ( REML ) algorithm with LEML [ 27 ] , which is the state of the art label embedding method based on the low rank assumption over label matrix , and SLEEC [ 4 ] , which is a recently developed method using the neighborhood embedding technique to handle tail labels . Other representative multi label learning algorithms , such as CS [ 15 ] , CPLST [ 8 ] , ML CSSP [ 5 ] and 1 vs All [ 1 ] , which are only applicable for small datasets , are included in comparison experiments as well .
We used two metrics to evaluate the multi label classification performance in the experiments , both of which have been widely used in the fields of multi label learning and ranking . Precision at k measures the fraction of true positive predictions in the top k scoring labels , given the predicted score vectory ∈ RL . nDCG at k measures the usefulness , or gain , of a label based on its position in the predicted label list . We refer readers to [ 19 ] for more detailed information .
6.1 Results on Large Datasets
We compare the classification performance of the proposed REML algorithm with those of leading methods on three large datasets : Wiki10 , Delicious L and Amazon . The classification results measured in Precision@k and nDCG@k are presented in Figures 2 4 . It can be seen that REML stably performs better than LEML on these large datasets . For example , REML improves over LEML by as much as
Precision@17274767880828486REMLSLEECLEMLFastXMLLPSR Precision@3586062646668707274REMLSLEECLEMLFastXMLLPSR Precision@5485052545658606264REMLSLEECLEMLFastXMLLPSR nDCG@1727476788082848688REMLSLEECLEMLFastXMLLPSR nDCG@360626466687072747678REMLSLEECLEMLFastXMLLPSR nDCG@5545658606264666870REMLSLEECLEMLFastXMLLPSR nDCG@515202530354045REMLSLEECLEMLFastXMLLPSR Precision@11520253035404550REMLSLEECLEMLFastXMLLPSR Precision@315202530354045REMLSLEECLEMLFastXMLLPSR Precision@510152025303540REMLSLEECLEMLFastXMLLPSR nDCG@11520253035404550REMLSLEECLEMLFastXMLLPSR nDCG@315202530354045REMLSLEECLEMLFastXMLLPSR Precision@110203040REMLSLEECLEMLFastXMLLPSR Precision@310203040REMLSLEECLEMLFastXMLLPSR Precision@510203040REMLSLEECLEMLFastXMLLPSR nDCG@110203040REMLSLEECLEMLFastXMLLPSR nDCG@310203040REMLSLEECLEMLFastXMLLPSR nDCG@510203040REMLSLEECLEMLFastXMLLPSR1281 Figure 5 : Top k precision of multi label learning algorithms on the Bibtex dataset .
Figure 6 : Top k precision of multi label learning algorithms on the Delicious S dataset .
Figure 7 : Top k precision of multi label learning algorithms on the MediaMill dataset .
18 % and 20 % in terms of Precision@3 and nDCG@3 on the Wiki10 dataset . This is because that the success of LEML mainly depends on the low rank assumption , which tends to be violated on the large dataset with lots of tail labels ( see Figure 1 ) . REML provides an appropriate approach to preserve the validity of low rank assumption by elegantly handling the tail labels as outliers . Hence , REML is able to achieve satisfactory classification results when faced with a number of tail labels , which is evidenced by its comparable performance with respect to the SLEEC method .
6.2 Results on Small Datasets
We next conduct multi label classification over three small datasets : Bibtex , Delicious S , MediaMill , which can be handled by more leading multi label learning algorithms , such as CPLST and CS . The classification results in Precision@k are shown in Figures 5 7 , while Table 2 presents the results in nDCG@k . Since the tail label problem is not acute on these small datasets , LEML and other comparison algorithms can achieve fine classification results . However , LEML is inferior to SLEEC , which uses neighborhood embedding to get rid of the influence of tail labels . Hence , SLEEC improves LEML
Precision@13040506070REMLSLEECLEMLWSABIECPLSTCSML CSSPLPSROneVsALLKNNPrecision@13040506070REMLSLEECLEMLWSABIECPLSTCSML CSSPLPSROneVsALLKNNPrecision@13040506070REMLSLEECLEMLWSABIECPLSTCSML CSSPLPSROneVsALLKNNPrecision@13040506070REMLSLEECLEMLWSABIECPLSTCSML CSSPLPSROneVsALLKNNPrecision@13040506070REMLSLEECLEMLWSABIECPLSTCSML CSSPLPSROneVsALLKNNPrecision@13040506070REMLSLEECLEMLWSABIECPLSTCSML CSSPLPSROneVsALLKNNPrecision@13040506070REMLSLEECLEMLWSABIECPLSTCSML CSSPLPSROneVsALLKNNPrecision@13040506070REMLSLEECLEMLWSABIECPLSTCSML CSSPLPSROneVsALLKNNPrecision@13040506070REMLSLEECLEMLWSABIECPLSTCSML CSSPLPSROneVsALLKNNPrecision@13040506070REMLSLEECLEMLWSABIECPLSTCSML CSSPLPSROneVsALLKNNPrecision@13040506070REMLSLEECLEMLWSABIECPLSTCSML CSSPLPSROneVsALLKNNPrecision@13040506070REMLSLEECLEMLWSABIECPLSTCSML CSSPLPSROneVsALLKNNPrecision@13040506070REMLSLEECLEMLWSABIECPLSTCSML CSSPLPSROneVsALLKNNPrecision@13040506070REMLSLEECLEMLWSABIECPLSTCSML CSSPLPSROneVsALLKNNPrecision@13040506070REMLSLEECLEMLWSABIECPLSTCSML CSSPLPSROneVsALLKNNPrecision@13040506070REMLSLEECLEMLWSABIECPLSTCSML CSSPLPSROneVsALLKNNPrecision@13040506070REMLSLEECLEMLWSABIECPLSTCSML CSSPLPSROneVsALLKNNPrecision@13040506070REMLSLEECLEMLWSABIECPLSTCSML CSSPLPSROneVsALLKNNPrecision@13040506070REMLSLEECLEMLWSABIECPLSTCSML CSSPLPSROneVsALLKNNPrecision@13040506070REMLSLEECLEMLWSABIECPLSTCSML CSSPLPSROneVsALLKNN1282 Table 2 : nDCG@k of multi label learning algorithms on the Bibtex , Delicious S , Mediamill datasets .
Algorithm nDCG@1 nDCG@3 nDCG@5 nDCG@1 nDCG@3 nDCG@5 nDCG@1 nDCG@3 nDCG@5
Bibtex
Delicious S
Mediamill
KNN
OneVsAll
LPSR
ML CSSP
CS
CPLST WSABIE
LEML SLEEC REML
57.81 62.62 62.98 56.86 59.60 61.99 55.03 63.10 64.49 65.13
52.36 59.44 57.11 52.54 53.08 57.66 50.26 58.84 59.90 60.01
54.57 61.73 58.76 54.81 53.74 59.71 52.33 61.06 62.29 62.46
64.80 64.90 64.46 63.96 61.26 65.65 63.67 64.96 67.41 66.30
60.71 60.94 60.47 59.07 57.85 61.52 59.47 61.80 62.46 62.65
57.02 56.54 56.19 54.86 54.44 58.00 56.37 58.42 59.02 59.10
82.59 83.67 83.65 83.21 83.97 83.79 79.86 83.09 86.61 86.73
75.62 73.90 74.12 72.67 75.29 74.44 72.51 75.23 80.04 82.67
72.76 67.75 69.12 65.05 71.99 70.49 69.31 71.96 77.71 78.32
Table 3 : Time and performance of REML with varying numbers of subproblems .
#subproblems Time ( s ) Precision@1 Precision@3 Precision@5 nDCG@1 nDCG@3 nDCG@5
1 2 5 8 10
3,739 2,190 1,495 923 764
86.17 84.07 82.80 78.45 76.68
74.30 72.70 71.20 69.53 67.75
64.37 62.92 62.78 58.76 56.68
88.09 85.26 83.79 81.92 80.01
78.44 76.52 75.47 72.07 70.30
69.38 68.94 66.23 64.42 63.32 by 4.8 % in terms of Precision@1 on the Bibtex dataset and 7.8 % in terms of Precision@3 on the MediaMill dataset . We suggest that the performance gap between LEML and SLEEC can be bridged by the REML algorithm , which is able to suppress the influence of tail labels and activate the low rank assumption on multiple labels . On the Delicious S dataset , REML is the closest competitor of SLEEC , while on the Bibtex dataset , REML takes a lead ahead of SLEEC in terms of Precision@5 . This demonstrates that the lowrank assumption is powerful for processing multiple labels , and its performance can be further strengthened by carefully investigating the tail labels as outliers in the low rank formulation . 6.3 Algorithm Analysis
We next perform experiments on the Wiki10 dataset , to explore the trade off between computation and accuracy when using divide and conquer technique to optimize the proposed REML algorithm ( denoted as ‘DC REML’ ) . Table 3 presents the time required to solve REML with different numbers of subproblems ( ie , t ) , and the corresponding classification results in Precision@k and nDCG@k . From this table , we find that DC REML performs comparably to REML for smaller values of t , and the performance gradually degrades for larger subproblem number , which is consistent with the theoretical analysis in Section 51 Most importantly , DCREML have significantly sped up REML by dividing the original problem into 5 subproblems , with an acceptable performance degradation of 4 % relative to REML in terms of Precision@3 . Hence , given the scalability provided by the divide and conquer optimization technique , we can flexibly manage the optimization accuracy and time cost in solving large scale multi label learning problems .
7 . CONCLUSION
In this paper , we study the tail labels problem in multilabel learning , where the scarce labels associate with limited number of examples . To prevent the damage of tail labels on the low rank assumption over multiple labels , we treat tail labels as outliers and develop a robust extreme multi label learning algorithm . Divide and conquer approach applied to optimize the resulting objective function is beneficial for improving the scalability , and its advantages in balancing accuracy and computation have been theoretically demonstrated . We analyze the generalization error of the proposed algorithm , and suggest that it can be improved by the low rank and sparse decomposition given tail labels . Experimental results on real world datasets demonstrate the significance of investigating tail labels and the promising performance of the proposed algorithm .
Acknowledgment We greatly thank anonymous reviewers for their positive support and constructive comments for improving the paper quality . The work was supported in part by Australian Research Council Projects FT 130101457 , DP 140102164 and LE140100061 , NSFC61375026 and 2015BAF15B00 .
References [ 1 ] S . V . N . V . B . Hariharan and M . Varma . Efficient max margin multi label classification with applications to zero shot learning . Machine Learning , 2012 .
[ 2 ] K . Balasubramanian and G . Lebanon . The landmark selection method for multiple output prediction . In ICML , 2012 .
[ 3 ] P . L . Bartlett and S . Mendelson . Rademacher and gaussian complexities : Risk bounds and structural results . The Journal of Machine Learning Research , 3:463–482 , 2003 .
[ 4 ] K . Bhatia , H . Jain , P . Kar , M . Varma , and P . Jain .
Sparse local embeddings for extreme multi label classification . In Advances in Neural Information Processing Systems , pages 730–738 , 2015 .
1283 [ 5 ] W . Bi and J . Kwok . Efficient multi label classification with many labels . In Proceedings of the 30th International Conference on Machine Learning ( ICML 13 ) , pages 405–413 , 2013 .
[ 6 ] W . Bi and J . T . Kwok . Multi label classification on tree and dag structured hierarchies . In Proceedings of the 28th International Conference on Machine Learning ( ICML 11 ) , pages 17–24 , 2011 .
[ 7 ] G . Carneiro , A . B . Chan , P . J . Moreno , and
N . Vasconcelos . Supervised learning of semantic classes for image annotation and retrieval . Pattern Analysis and Machine Intelligence , IEEE Transactions on , 29(3):394–410 , 2007 .
[ 8 ] Y N Chen and H T Lin . Feature aware label space dimension reduction for multi label classification . In Advances in Neural Information Processing Systems , pages 1538–1546 , 2012 .
[ 9 ] M . M . Cisse , N . Usunier , T . Artieres , and P . Gallinari . Robust bloom filters for large multilabel classification tasks . In Advances in Neural Information Processing Systems , pages 1851–1859 , 2013 .
[ 10 ] K . Crammer and Y . Singer . A family of additive online algorithms for category ranking . The Journal of Machine Learning Research , 3:1025–1058 , 2003 .
[ 11 ] D . L . Donoho . De noising by soft thresholding .
Information Theory , IEEE Transactions on , 41(3):613–627 , 1995 .
[ 12 ] A . Elisseeff and J . Weston . A kernel method for multi labelled classification . In Advances in neural information processing systems , pages 681–687 , 2001 .
[ 13 ] W . Gao and Z H Zhou . On the consistency of multi label learning . Artificial Intelligence , 199:22–44 , 2013 .
[ 14 ] B . Hariharan , L . Zelnik Manor , M . Varma , and
S . Vishwanathan . Large scale max margin multi label classification with priors . In Proceedings of the 27th International Conference on Machine Learning ( ICML 10 ) , pages 423–430 , 2010 .
[ 15 ] D . Hsu , S . Kakade , J . Langford , and T . Zhang .
Multi label prediction via compressed sensing . In NIPS , volume 22 , pages 772–780 , 2009 .
[ 16 ] S J Huang , Y . Yu , and Z H Zhou . Multi label hypothesis reuse . In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 525–533 . ACM , 2012 .
[ 17 ] C L Li and H T Lin . Condensed filter tree for cost sensitive multi label classification . In Proceedings of the 31st International Conference on Machine Learning ( ICML 14 ) , pages 423–431 , 2014 .
[ 18 ] L . W . Mackey , M . I . Jordan , and A . Talwalkar .
Divide and conquer matrix factorization . In Advances in Neural Information Processing Systems , pages 1134–1142 , 2011 .
[ 19 ] C . D . Manning , P . Raghavan , H . Sch¨utze , et al . Introduction to information retrieval , volume 1 . Cambridge university press Cambridge , 2008 .
[ 20 ] R . E . Schapire and Y . Singer . Boostexter : A boosting based system for text categorization . Machine learning , 39(2 3):135–168 , 2000 .
[ 21 ] E . Spyromitros , G . Tsoumakas , and I . Vlahavas . An empirical study of lazy multilabel classification algorithms . In Artificial Intelligence : Theories , Models and Applications , pages 401–406 . Springer , 2008 .
[ 22 ] L . Sun , S . Ji , and J . Ye . Hypergraph spectral learning for multi label classification . In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 668–676 . ACM , 2008 .
[ 23 ] F . Tai and H T Lin . Multilabel classification with principal label space transformation . Neural Computation , 24(9):2508–2542 , 2012 .
[ 24 ] G . Tsoumakas , I . Katakis , and I . Vlahavas . Mining multi label data . In Data mining and knowledge discovery handbook , pages 667–685 . Springer , 2010 .
[ 25 ] C . Vens , J . Struyf , L . Schietgat , S . Dˇzeroski , and
H . Blockeel . Decision trees for hierarchical multi label classification . Machine Learning , 73(2):185–214 , 2008 .
[ 26 ] R . Yan , J . Tesic , and J . R . Smith . Model shared subspace boosting for multi label classification . In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 834–843 . ACM , 2007 .
[ 27 ] H F Yu , P . Jain , and I . S . Dhillon . Large scale multi label learning with missing labels . In Proceedings of the twenty first international conference on Machine learning , 2014 .
[ 28 ] M L Zhang and K . Zhang . Multi label learning by exploiting label dependency . In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 999–1008 . ACM , 2010 .
[ 29 ] M L Zhang and Z H Zhou . Multilabel neural networks with applications to functional genomics and text categorization . Knowledge and Data Engineering , IEEE Transactions on , 18(10):1338–1351 , 2006 .
[ 30 ] M L Zhang and Z H Zhou . Ml knn : A lazy learning approach to multi label learning . Pattern recognition , 40(7):2038–2048 , 2007 .
[ 31 ] M L Zhang and Z H Zhou . A review on multi label learning algorithms . Knowledge and Data Engineering , IEEE Transactions on , 26(8):1819–1837 , 2014 .
[ 32 ] Y . Zhang and J . G . Schneider . Multi label output codes using canonical correlation analysis . In International Conference on Artificial Intelligence and Statistics , pages 873–882 , 2011 .
1284
