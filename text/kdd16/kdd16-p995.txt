Dynamic Clustering of Streaming Short Documents
Shangsong Liang†
Emine Yilmaz†
Evangelos Kanoulas‡ ekanoulas@uvanl shangsongliang@uclacuk emineyilmaz@uclacuk
†University College London , London , United Kingdom ‡University of Amsterdam , Amsterdam , The Netherlands
ABSTRACT Clustering technology has found numerous applications in mining textual data . It was shown to enhance the performance of retrieval systems in various different ways , such as identifying different query aspects in search result diversification , improving smoothing in the context of language modeling , matching queries with documents in a latent topic space in ad hoc retrieval , summarizing documents etc . The vast majority of clustering methods have been developed under the assumption of a static corpus of long ( and hence textually rich ) documents . Little attention has been given to streaming corpora of short text , which is the predominant type of data in Web 2.0 applications , such as social media , forums , and blogs . In this paper , we consider the problem of dynamically clustering a streaming corpus of short documents . The short length of documents makes the inference of the latent topic distribution challenging , while the temporal dynamics of streams allow topic distributions to change over time . To tackle these two challenges we propose a new dynamic clustering topic model DCT that enables tracking the time varying distributions of topics over documents and words over topics . DCT models temporal dynamics by a short term or long term dependency model over sequential data , and overcomes the difficulty of handling short text by assigning a single topic to each short document and using the distributions inferred at a certain point in time as priors for the next inference , allowing the aggregation of information . At the same time , taking a Bayesian approach allows evidence obtained from new streaming documents to change the topic distribution . Our experimental results demonstrate that the proposed clustering algorithm outperforms state of the art dynamic and non dynamic clustering topic models in terms of perplexity and when integrated in a clusterbased query likelihood model it also outperforms state of the art models in terms of retrieval quality . CCS Concepts •Information systems → Clustering ;
Keywords Clustering ; Topic Models ; Streaming Text ; Cluster based Retrieval Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than the author(s ) must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee . Request permissions from permissions@acmorg KDD ’16 , August 13 17 , 2016 , San Francisco , CA , USA cfl 2016 Copyright held by the owner/author(s ) . Publication rights licensed to ACM . ISBN 978 1 4503 4232 2/16/08 . . . $15.00 DOI : http://dxdoiorg/101145/29396722939748
1 .
INTRODUCTION
New media applications and the increasing prevalence of mobile devices have facilitated the collection and rapid dissemination of news and information by anyone connected to the Internet . Massive amounts of user generated content , often in the form of short text ( eg microblog posts ) , typically clustered around reallife events [ 6 , 20 ] , are streaming in , and being consumed by interconnected users . Organizing large text collections around concise topics ( or clusters ) allows effective summarization and retrieval of information [ 5 , 7 , 13 ] . In the case of a rapidly streaming short text , however , traditional clustering algorithms are either not applicable , or do not tackle the temporal and sparse nature of the text corpus [ 7 , 25 ] . In this paper we propose a dynamic clustering topic model method – DCT – for short length streaming text and we demonstrate that it can effectively model both the temporal nature of topics in streaming text and the sparsity problem of short text , improving the performance of clustering and ad hoc search .
One of the key challenges in clustering streaming data is the dynamic nature of topics ( or clusters ) : topic distributions change with time , with previously salient topics “ fading off ” and vice versa [ 1 , 6 , 7 , 10 , 22 ] . Therefore , techniques developed ought to allow for changes in the topic distribution along time . For example , Twitter posts about Apple Inc . on September 9 , 2015 , when Apple introduced iPhone 6s plus , are expected to be clustered around Apple iPhone 6s plus , while this may not be the case on December 3 , 2015 , when Apple swift was announced . The problem of clustering documents in streams has been widely investigated in the past [ 1 , 6 , 10 , 22 ] . However , most of the previous work makes the assumption that the content of documents is rich enough to infer a per document multinomial distribution of topics . The second key challenge in clustering streaming data is that this assumption does not hold for short text , as the number of words in each document is limited , which prohibits the accurate inference of a topic distribution over the document . Our method tackles the two challenges by introducing a collapsed Gibbs sampling algorithm that ( a ) assigns a single topic to all the words of a short document , and ( b ) uses the inferred topic distribution of past documents as a prior of the topic distribution of the current documents , while at the same time allowing new evidence ( newly streamed documents ) to change the posterior distribution of topics . Based on the exact definition of the prior the proposed model enables both short term and long term dependencies between the previously and currently inferred distributions .
In this paper , we take a special interest in the application of topic models in the area of information retrieval . Our goal is to infer the relevance of each cluster to a user query by calculating the dynamic topic distribution over short documents and incorporating that in a query likelihood model for ad hoc retrieval . We evaluate our pro
995 posed clustering model on a publicly available Twitter dataset by comparing the retrieval performance achieved with state of the art methods and demonstrate the superiority of our algorithm .
The contribution of this work is threefold :
( 1 ) We propose a Dynamic Dirichlet Multinomial Mixture Model that captures short and long term temporal dependencies , tracking dynamic topic distributions over short document streams .
( 2 ) We propose a collapsed Gibbs sampling algorithm for our Dynamic Dirichlet Multinomial Mixture Model to infer the changes in topic and document probability distributions .
( 3 ) We analyze the effectiveness of the proposed clustering models by using the produced clusters to improve the performance of short text ad hoc retrieval against a Twitter dataset , and demonstrate that our method significantly outperforms state of the art methods .
The remainder of the paper is organized as follows : §2 discusses related work ; §3 details the problem ; §4 describes the proposed clustering model ; §5 describes our experimental setup ; §6 is devoted to our experimental results and we conclude the paper in §7 .
2 . RELATED WORK
There are two lines of work related to our work , topic modeling and clustering , with a rich literature available on both topics . In the following sections we only discuss the most related models and algorithms . 2.1 Topic Modeling
Topic modeling provides a suite of algorithms to discover hidden thematic structure in a collection of documents . A topic model takes a collection of documents as input , and discovers a set of “ latent topics ” —recurring themes that are discussed in the collection— and the degree to which each document exhibits those topics [ 3 ] . Latent Dirichlet Allocation ( LDA ) [ 3 ] is one of the simplest topic models , and it decomposes a collection of documents into topics— biased probability distributions over terms—and represents each document with a subset of these topics .
Many models that extend LDA have been proposed , such as topic over time model [ 20 ] , dynamic mixture model [ 22 ] , topic tracking model [ 10 ] , online multi scale dynamic topic model [ 11 ] and more recently , ( static ) Dirichlet multinomial mixture model [ 25 ] , Dirichlet hawkes topic model [ 6 ] and user aware sentiment topic model [ 24 ] . These models can either infer topics in static collections of short text , eg the ( static ) Dirichlet multinomial mixture model [ 25 ] , or infer dynamic topics in long documents , eg the dynamic mixture model [ 22 ] , the topic tracking model [ 10 ] , and the online multi scale dynamic topic model [ 11 ] . Instead , we propose two dynamic Dirichlet multinomial mixture topic models for short text streams : one for short term dependency of the current inference of the topics and another for long term dependency . Based on these topic models we can infer the dynamic changes of the multinomial distribution of the documents in a stream , and the document probabilities to the topics , and we use the inferred topics to cluster the documents . Hence , our model can both infer topics in short text streams and track the dynamic changes in clusters . 2.2 Clustering
Clustering is one of the main technologies that has been applied to tackle many challenges in data mining , text mining , and information retrieval [ 5 ] . For instance , [ 21 ] proposed a cluster based document retrieval model where the clusters are generated by LDA .
Table 1 : Main notation used in dynamic clustering topic model . Notation Gloss d z t v V Z d t dt αt βt Θt Φt document topic time word total number of words total number of latent topics set of documents arriving at time t document stream up to time t , shorten for d≤t parameter of topic Dirichlet prior at time t parameter of word Dirichlet prior at time t dynamic topic distribution at time t dynamic word distribution at time t
[ 13 ] presented a burst aware approach to fusing document lists retrieved in response to a query via integrating information used by fusion methods with that induced from time sensitive clusters of documents . Efron et al [ 7 ] found that relevant documents tend to cluster together in time and utilizing some existing clustering algorithms can boost the performance of tweet search . In terms of data mining , Botezatu et al . [ 4 ] proposed a multi view incident ticket clustering algorithm for optimal ticket dispatching .
To this date , a large number of clustering algorithms have been proposed with KNN ( K Nearest Neighbours ) and K Means as some of the most famous ones . Among those , given that we want to tackle the problem of clustering short documents in streams , we focus on Dirichlet multinomial mixture clustering model [ 25 ] , that performs well on short text , and which is based on topic modeling . This model acknowledges that as the number of words in short documents is limited , and thus each word in the same document can be assigned to one topic . Then documents assigned to the same topic are in the same cluster . Their experimental results validated the effectiveness of this assumption . However , this model and more recent short document clustering model based on convolutional neural networks [ 23 ] can only cluster a static collection of short documents . Other clustering technologies based on topic modeling include dynamic mixture model [ 22 ] , topic over time model [ 20 ] and topic tracking model [ 10 ] . However , until now all of the dynamic topic models make a strong assumption that documents arriving in a data stream are long and provide rich context for the inference . To the best of our knowledge , our proposed clustering algorithm is the first attempt to cluster streams of short text documents .
3 . TASK DESCRIPTION
The task we address in this work is the clustering of short text streaming documents , with clusters changing dynamically , as new documents stream in . The dynamic clustering algorithm is essentially a function f that satisfies : d≤t = { . . . , d t} f−→ c≤t = {c t−2 , d t−1 , d
Z} ,
2 , . . . , c
1 , c where d≤t represents the stream of documents with d t being the most recent set of short documents , arrived at time t , and c≤t is the resulting set of clusters of documents up to time t , with c z being the z th cluster in ct and Z the total number of clusters . d t comprises a set of short text documents , with each document d being represented by a sequence of words appearing in d , coming from a vocabulary V = {v1 , v2 , . . . , vV } . We assume that the length of d is no more than a predefined small length ( for instance , 140 characters in the case of Twitter ) . For brievity , in the remainder of the paper , we denote d≤t and c≤t with dt and ct , respectively .
Table 1 summarizes the main notation used in our dynamic clus tering topic model .
996 4 . DYNAMIC CLUSTERING MODEL
In this section , we describe our proposed dynamic clustering topic model , DCT , aiming at the effective clustering of short document streams . 4.1 Preliminaries
The goal of the dynamic clustering topic model is to infer the dynamically changing topic distribution and document distribution over topics at any given time t . That is , we want to infer the temporal word probability for a topic , P ( v|t , z ) , and the temporal topic probability over a document , P ( z|t , d ) . Previous work [ 25 ] has demonstrated that algorithms that assign a single topic to all the words in a short document outperform those that assign different topics to different words in terms of clustering quality . The intuition behind this observation is that the number of words in short documents is limited and a short document is likely to be associated with one topic . Following [ 25 ] , our proposed Gibbs sampling – as it will be described in the following sections – assigns a single topic to all the words in each short document . Following the notation of past topic modeling work [ 2 , 3 , 10 , 20 ] , we let Θt = {θt,z}Z z=1be the topic distribution at time t with z=1 θt,z = 1 . We also let Φt = {φt,z}Z z=1be the word distribution over topics at time t . φt,z = {φt,z,v}V v=1 is the ( multinomial ) distribution of words for topic z at time t , while the probability of a word v belonging to z at t , v=1 φt,z,v = 1 ; V is the size of the vocabulary V . In fully bayesian non dynamic topic models ( such as LDA [ 3] ) , there is an underlying assumption that the per document topic distribution is independent of the past distributions , and have a Dirichlet prior with a static set of parameters κ = {κz}Z
θt,z = P ( z|t ) > 0 , and Z φt,z,v = P ( v|t , z ) > 0 , and V z=1 , with κz > 0 ,
θκz−1 t,z
,
( 1 )
P ( Θt|κ ) ∝ Z z=1
P ( φt,z|γ ) ∝ V v=1
Similarly , the per topic word distribution φt,z also has a Dirichv=1 , with γv > 0 , let prior with a static set of parameters γ = {γv}V
φγv−1 t,z,v ,
( 2 )
The assumptions made in ( 1 ) and ( 2 ) are not realistic when it comes to a data stream setting , where the distributions at time t are dependent on past distributions . In the following subsections , we infer Θt and Φt by modeling short term dependency ( Section 4.2 ) and long term dependency ( Section 43 ) 4.2 Short term Dependency DCT Modeling short term dependency . To model the temporal dependencies of the topics in a document stream , and by following the work of past dynamic topic models [ 10 , 11 , 22 ] , we propose a short term dependency DCT model . According to this model , the topic distribution at time t remains the same as the one at time t− 1 if no new documents are observed , while it is updated on the basis of new evidence when a new set of documents is observed at time t . To achieve that we factorize the parameter κ in ( 1 ) into the mean of the distribution at the previous time step , θt−1,z , and a set of precision values αt = {αt,z}Z z=1 . Hence , κ = αtΘt−1 , which allows the mean of the current distribution Θt to depend on the mean of the previous distribution Θt−1 ,
P ( Θt|Θt−1 , αt ) ∝ Z z=1
θ(αt,z θt−1,z )−1 t,z
,
( 3 ) where the precision value αt,z represents the topic persistency , that is how salience is topic z at time t compared to that at time t − 1 . The distribution is a conjugate prior of the Multinomial distribution , hence the inference can be performed by Gibbs sampling [ 17 ] . In a similar way , to model the dynamic changes of the multinomial distribution of words specific to topic z , we assume a Dirichlet prior , in which the mean of the current distribution Φt evolves from the mean of the previous distribution Φt−1 with the precision being βt ,
P ( φt,z|φt−1,z , βt,z ) ∝ V
φ(βt,z,v φt−1,z,v )−1 t,z,v
,
( 4 ) v=1 where as before , the Dirichlet prior parameter γ in ( 2 ) is factorized into the mean and precision , γ = βt,zφt−1,z , with βt = {βt,z}Z z=1 being the set of precision values at time t for the topics . Here βt,z = {βt,z,v}V v=1 , with βt,z,v representing the persistency of word v in topic z at time t , a measure of how consistently word v belongs to topic z at time t compared to that at the previous time t− 1 . We describe the inference for Θt , Φt , αt and βt in later part of this section . Assuming that we know the topic distribution at time t − 1 , Θt−1 , and the word distribution over topics at time t − 1 , Φt−1 , the proposed Dynamic Dirichlet Multinomial Mixture Model is a generative topic model that depends on Θt−1 and Φt−1 . We can initialize ( at time t = 0 ) the means of the two distributions to θ0,z = 1/Z and φ0,z,v = 1/V . The generative process ( used by the Gibbs sampler for parameter estimation ) of our model for documents in stream dt at time t , is as follows , i . Draw a multinomial distribution Θt from a Dirichlet prior distribution αtΘt−1 ; from a Dirichlet prior distribution βt,zφt−1,z ; ii . Draw Z , one for each topic z , multinomial distributions φt,z iii . For each document d ∈ dt , draw a topic zd from the multinomial distribution Θt and for each word vd in the document d : ( a ) Draw a word vd from multinomial φt,zd ;
Fig 1 illustrates the graphical representation of our Dynamic Dirichlet Multinomial Mixture Model ; given that documents are short , and following [ 25 ] , all words in the same document d are drawn from the Multinomial distribution associated with the same topic zd . The parameterization of the proposed dynamic topic model is as follows :
Θt ∼ Dirichlet(αtΘt−1 )
φt,z|βt,zφt−1,z ∼ Dirichlet(βt,zφt−1,z ) zd ∼ Multinomial(Θt ) vd|φt,zd ∼ Multinomial(φt,zd )
Note that in the generative process described above , there is a fixed number of latent topics Z . A non parametric Bayes version of our dynamic topic model that automatically integrates over the number of topics is possible , but we leave this as future work . Inference for the short term dependency DCT . The inference of the distribution parameters of the model is intractable . Following [ 14 , 15 , 19 , 20 ] we employ a collapsed Gibbs sampler [ 9 ] for an approximate inference . We adopt a conjugate prior ( Dirichlet ) for the multinomial distributions , and thus we can easily integrate out the uncertainty associated with φt,z and Θt . In this way we enable sampling since we do not need to sample φt,z or Θt . In the Gibbs sampling procedure we need to calculate the conditional distribution P ( zd|zt,−d , dt , Φt−1 , Θt−1 , αt , βt ) , at time t ,
997 αt−2
θt−2 z d
αt−1
θt−1 z d
αt
θt z d
|dt−2|
|dt−1|
|dt|
φt−2
Z
βt−2
φt−1
Z
βt−1
φt
Z
βt
Figure 1 : Graphical representation of our dynamic Dirichlet multinomial mixture clustering topic model , DCT . Note that short term dependence DCT model excludes the two blue curved lines ; while long term dependence DCT model does include these two lines . The figure is best viewed in color . where zt,−d represents the topic assignments for all documents in dt except document d . We begin with the joint probability of the current document set dt , P ( dt , zt|Φt−1 , Θt−1 , αt , βt ) ( see Appendix A for the detail of the join probability ) , and using the chain rule , we can obtain the following conditional probability , P ( zd|zt,−d , dt , Φt−1 , Θt−1 , αt , βt ) ∝
Z mt,z + αt,zθt−1,z − 1
Nd,v z=1(mt,z + αt,zθt−1,z ) − 1 i=1(nt,z,−d + i − 1 +V Nd j=1 ( nt,z,v,−d + βt,z,vφt−1,z,v + j − 1 ) ( 5 ) v=1 βt,z,vφt−1,z,v ) v∈d
×
, where mt,z is the total number of documents in dt assigned to topic z , Nd,v is the number of word v in the document d , nt,z,v,−d is the total number of the word v assigned to topic z except that in d , and nt,z,−d is the total number of documents assigned to z except d . Detailed derivation of Gibbs sampling for our proposed DCT model is provided in Appendix A . During sampling , at each iteration , the precision parameters αt and βt can be estimated by maximizing the joint distribution P ( dt , zt|Φt−1 , Θt−1 , αt , βt ) . We apply fixedpoint iteration to get the optimal αt and βt at time t . The following update rule of αt for maximizing the joint distribution in our fixedpoint iteration is derived by applying two bounds in [ 18 ] , αt,z ← αt,z ( Ψ(mt,z + αt,zθt−1,z ) − Ψ(αt,zθt−1,z ) ) where Ψ(· ) defined by Ψ(x ) = ∂ log Γ(x ) whereas the following update rule of βt is , z=1 mt,z + αt,zθt−1,z ) − Ψ(Z is the digamma function ;
Ψ(Z z=1 αt,zθt−1,z )
∂x
,
Z Z
βt,z,v ← z=1 βt,z,vφt−1,z,vAt,z,v
, where At,z,v = Ψ(nt,z,v + βt,z,vφt−1,z,v ) − Ψ(βt,z,vφt−1,z,v ) ,
Bt,z,v = Ψ(V z=1 φt−1,z,vBt,z,v v=1 nt,z,v+βt,z,vφt−1,z,v)−Ψ(V and nt,z,v is the number of word v assigned to topic z in stream dt . v=1 βt,z,vφt−1,z,v )
Algorithm 1 : Inference for the Dynamic Dirichlet Multinomial Mixture Model at time t . Input
: Previous topic distribution Θt−1 Previous word distribution specific to topics Φt−1 A set of short documents dt at time t Initialized αt and βt Number of iterations Niter
Output : Current topic distribution Θt
Current word distribution specific to topics Φt Documents’ probabilities to each topic at time t , P ( z|t , d )
1 Initialize topic assignments randomly for all documents in dt 2 for iteration = 1 to Niter do 3 4 5 draw zd from P ( zd|zt,−d , dt , Φt−1 , Θt−1 , αt , βt ) update mt,zd and nt,zd,v for d = 1 to |dt| do update αt and βt
6 7 Compute the posterior estimates Θt and Φt 8 Compute P ( z|t , d ) l=1 ) ∝ Z
,L
θ t,z
−1
Our derivation of the update rules for αt and βt , and the two bounds used in deviating the update rules are detailed in Appendix B . An overview of our proposed collapsed Gibbs sampling algorithm , including the input and output , is shown in Algorithm 1 . 4.3 Long term Dependency DCT Modeling long term dependency . So far the distributions Θt and Φt depend on the previous time step distributions . Research has shown that topic distributions or the interests of a user on a topic when searching for information may depend on a longer time step history . We model such a long term ( L steps ) dependency DCT model on the basis of the distribution priors as follows :
P ( Θt|{Θt−l , αt,l}L l=1 αt,z,lθt−l,z
.
( 6 ) z=1
The mean in this case is proportional to the weighted sum of the past L “ topic trends ” in the documents , and αt,l = {αt,z,l}Z z=1 represents how the topics at time t are related to the l previous topics . For a comparison with the short term dependency model refer to Eq ( 3 ) and Eq ( 6 ) . Further , long term dependency reduces the information loss and the bias of the inference due to the multiple estimates . Similarly , the Dirichlet prior of the topic trends φt,z at t can be modified such that φt,z depends on the past L topic trends {φt−l,z}L as well . By doing so , we can make the inference more robust . Thus , we have : P ( φt,z|{φt−l,z , βt,z,l}L l=1 ) ∝ V
,L l=1 βt,z,v,lφt−l,z,v
−1
φ
, t,z,v l=1 v=1
( 7 ) v=1 represents how the word distribution where βt,z,l = {βt,z,v,l}V over topics at time t are related to the l previous one . Inference for the long term dependency DCT . The parameters Θt and φt,z in Eq ( 6 ) and Eq ( 7 ) can be integrated in the exact same way as before ( since priors are still Dirichlet distributed ) and Θt and φt,z at time t are inferred using the proposed Gibbs sampling in Algorithm 1 . The only difference lies in the way we sample the latent topic for each document ( step 4 in Algorithm 1 ) and the update rules for the priors ( step 6 in Algorithm 1 ) . Similar
998 to Eq ( 5 ) , we sample a latent topic for a document d by :
P ( zd|zt,−d , dt,{Φt−l , Θt−l , αt,l , βt,l}L l=1 ) ∝ × mt,z +L z=1(mt,z +L Z Nd,v j=1 ( nt,z,v,−d +L i=1(nt,z,−d + i − 1 +V Nd l=1 αt,z,lθt−l,z − 1 l=1 αt,z,lθt−l,z ) − 1 L l=1 βt,z,v,lφt−l,z,v + j − 1 ) l=1 βt,z,v,lφt−l,z,v ) v∈d v=1
( 8 )
.
, t,z z=1
L with fixed point iteration by : l=1 αt,z,lθt−l,z ) and D
The derivation of Eq ( 8 ) is similar to that of Eq ( 5 ) ( see Appendix A ) . Again , we update αt,z,l in Eq ( 8 ) using the two bounds in [ 18 ] with fixed point iteration such that : αt,z,l ← αt,z,lCt,z Dt,z − D l=1 αt,z,lθt−l,z ) , t,z = l=1 αt,z,lθt−l,z ) . Similarly , we update βt,z,v,l in ( 8 ) l=1 αt,z,lθt−l,z)−Ψ(L where Ct,z = Ψ(mt,z+L z=1 mt,z +L and Dt,z = Ψ(Z Ψ(Z Z Z z=1 βt,z,v,lφt−l,z,vA t,z,v = Ψ(nt,z,v+L l=1 βt,z,v,lφt−l,z,v)−Ψ(L t,z,v = Ψ(V v=1 nt,z,v +L L l=1 βt,z,v,l l=1 βt,z,v,lφt−l,z,v)− l=1 βt,z,v,lφt−l,z,v ) . Given the space limitation , we do not show the derivations of the update rules for αt,z,l and βt,z,v,l , as they are similar to those for αt,z and βt,z,v in our short term dependency DCT model ( see Appendix B ) . where A φt−l,z,v ) , and B
Ψ(V z=1 φt−l,z,vB
βt,z,v,l ← t,z,v t,z,v v=1
,
4.4 Clustering
Now , we can infer the dynamic topic distribution at time t , Θt in our short term dependency DCT model as ,
Z
θt,z = mt,z + αt,zθt−1,z z=1 mt,z + αt,zθt−1,z
= mt +Z mt,z + αt,zθt−1,z z=1 αt,zθt−1,z
, where mt is the total number of documents in dt , and infer a multinomial distribution over words for topic z at time t as , where nt,z is the number of words assigned to topic z at time t . Similarly , we can infer the dynamic topic distribution at time t in our long term dependency DCT model as , where P ( zd|zt,−d , dt,{Φt−l , Θt−l , αt,l , βt,l}L l=1 ) can be obtained by Eq ( 5 ) and Eq ( 8 ) for the short and long term dependency DCT model , respectively . Finally , the document d in stream dt at t z , ie , the topic z = arg maxzd P ( zd|t , d ) . is clustered to cluster c 5 . EXPERIMENTAL SETUP
Ideally , we would like to evaluate the performance of our dynamic clustering model by directly comparing the clustering result with ground truth labels in a streaming short text corpus . However , to the best of our knowledge , there is no such collection available to this date ; obtaining cluster labels for all documents in a stream and all points in time is rather expensive . Instead , we perform an extrinsic evaluation of the proposed model : ( a ) we incorporate the clustering algorithm derived by the DCT model into a cluster based query likelihood model for ad hoc retrieval [ 5 , 21 ] , and test the clustering quality on the basis of retrieval performance , and ( b ) we test the ability of the DCT generative model to predict the observed data on the basis of perplexity [ 2 , 3 ] . We compare the performance of our model with other state of the art clustering models .
The cluster based ad hoc retrieval model [ 21 ] used in our exper imental setup is the following : P ( q|t , d ) =
P ( v|t , d)n(v,q ) ,
( 12 ) v∈q where n(v , q ) is the term frequency of term v in query q , and P ( v|t , d ) is the probability of document d ∈ dt being relevant to the query term v , which is computed by using a Dirichlet smoothing language model [ 5 ] as , P ( v|t , d ) = λPCluster(v|t , d)+ ( 1 − λ )
PML(v|t , d ) +,1 − Nd
PML(v|dt )
Nd
Nd + µ
Nd + µ
( 13 ) where λ is a free parameter , µ is a Dirichlet prior in language model [ 5 ] , and PML(v|t , d ) , PML(v|dt ) and PCluster(v|t , d ) are the maximum likelihood estimates of word v in the document d , in the current short document stream dt and in the document d in terms of clusters at time t , respectively . According to the cluster based retrieval model proposed in [ 21 ] , PCluster(w|t , d ) is computed by ,
Z z=1 where P ( v|t , d , z ) is the probability of word v being relevant to topic z at time t , and P ( z|t , d ) the probability of document d being assigned to topic z . When applying the proposed DCT clustering model , for instance , we set P ( v|t , d , z ) = φt,z,v , where φt,z,v is defined in Eq ( 9 ) and Eq ( 10 ) for the short and long term dependence DCT models , respectively , while P ( z|t , d ) is defined in Eq ( 11 ) , for the two models , respectively . 5.1 Research Questions
The research questions we investigate in experimental section of the paper are : On the basis of ranking performance :
RQ1 : How does the proposed DCT clustering model perform compared to state of the art clustering algorithms in searching a short text document stream ?
RQ2 : Is the performance consistent across different user queries ? RQ3 : How does the performance of the long term dependence DCT model compares to that of the short term dependence DCT model ?
φt,z,v = nt,z,v + βt,z,vφt−1,z,v v=1 βt,z,vφt−1,z,v
,
( 9 )
PCluster(v|t , d ) =
P ( v|t , d , z)P ( z|t , d ) , nt,z +V mt,z +L mt +Z L nt,z,v +L nt,z +V L z=1 v=1
θt,z =
φt,z,v = and the multinomial distribution over words for topic z at time t , l=1 αt,z,lθt−l,z l=1 αt,z,lθt−l,z
, l=1 βt,z,v,lφt−l,z,v l=1 βt,z,v,lφt−l,z,v
.
( 10 )
As one can observe in all equations for the two models , the shortterm model is just a special case of the long term one for L = 1 .
Having computed θt,z and φt,z,v , we can compute the probability that a document d is relevant to topic zd at time t in the stream dt , P ( zd|t , d ) as : P ( zd|t , d ) =
Z P ( zd|zt,−d , dt,{Φt−l , Θt−l , αt,l , βt,l}L d=1 P ( z z l=1 ) d|zt,−d , dt,{Φt−l , Θt−l , αt,l , βt,l}L l=1 )
( 11 )
,
999 RQ4 : What is the impact of the free parameter λ in Eq ( 13 ) when applying the DCT model on cluster based ad hoc retrieval ?
RQ5 : Is the performance of cluster based ad hoc retrieval sensitive to the number of clusters used in the DCT model ?
On the basis of the generative model : temporal features are extracted from an initial ranked list of documents and then reranks this list to produce a final ranking .
Laten Dirichlet Allocation ( LDA ) [ 21 ] : Clusters documents based on LDA and ranks them by a cluster based document retrieval model ( Eq ( 12 ) and ( 13) ) , in the same way DCT ranks documents .
RQ6 : What is the performance of the generative DCT model compared to other baseline topic models in terms of the likelihood of generating the top k documents ( measured by perplexity [ 3] ) ?
Dirichlet Multinomial Mixture Model ( DMM ) [ 25 ] : Clusters documents based on a vanilla Dirichlet multinomial mixture model ( without the temporal dependencies introduced by this paper ) and ranks them by Eq ( 12 ) and ( 13 ) .
5.2 Data Set
One of the key criteria for a suitable test collection for our ad hoc retrieval task is the dynamic nature of the intent of a users’ query . That is we make the assumption that for the same query , eg Egypt , the intent may change over time ( something that we hope to be reflected in the identified dynamic topic distribution ) . Publicly available labeled corpora , such as Tweets2011 and Tweets2013 used for ad hoc retrieval in TREC 2011–2015 Microblog track [ 16 ] , have been constructed however by judging documents against a static query intent ; furthermore the time span of the collection is relatively small ( 16 and 59 days , respectively ) .
To allow for a dynamic query intent we construct a new test collection based on a publicly available corpus of Twitter posts ( an 1 % sample of all tweets ) . 1 The corpus has been collected between February 1 , 2015 and April 30 , 2015 , covering a period of 90 days . Most of the tweets are written in English ; we remove non English tweets and retweets to end up with 369 million tweets . We follow Fisher et al . [ 8 ] and generated queries and relevance labels as follows : ( a ) Manual selected hashtags on topics of general interest , such as “ #Apple ” and “ #Egypt ” are transformed into keyword queries . ( b ) Given a query at time t , we label the top k documents retrieved by a time sensitive language model ( see Section 5 ) , resulting in the query document ground truth used in our experiments . Assessors are university students employed remotely , while no specific intent for a query was provided to them . Therefore , it was up to their own judgment to decide what constitutes relevant and what not . To enable the possibility of query intent drifting relevance judgments were not obtained retrospectively ( ie at the end of the 90 days period ) but we simulated a streaming scenario and obtained labels at 20 day intervals2 . This resulted in 5 sets of ground truth data : on February 9th , March 1st , March 21st , April 10th , and April 30th of 2015 . Our test collection includes 107 queries , and 5 sets of ( disjoint ) ground truth labels for each one of them . 5.3 Baselines , Evaluation Metrics , and Setting We compare the DCT 3 model with a number of baselines and state of the art algorithms : Language Model ( LM ) [ 5 ] : Directly ranks documents by their relevance scores computed by a multinomial query likelihood model Eq ( 12 ) and ( 13 ) after removing PCluster(v|t , d ) .
Time aware Microblog Search ( TMS ) [ 7 ] : Based on the temporal cluster hypothesis that relevant documents tend to cluster together in time,first adopts a feedback framework where
1https://archive.org/details/twitterstream 2Applying shorter intervals requires more efforts of manual labeling and we found that it yielded not significantly different results in many cases .
3The code of the DCT topic model is available at https:// bitbucketorg/sliang1/dct/get/DCTzip
Topic Tracking Model ( TTM ) [ 10 ] : Clusters documents based on a dynamic topic tracking model that captures temporal dependencies between long text streams , and ranks them by Eq ( 12 ) and ( 13 ) .
LDA , DMM , TTM and our proposed DCT use the same retrieval model , ie , Eq ( 13 ) , to compute the relevant scores for the documents ; they only differ in the way they perform clustering . For all methods including the vanilla LM , we define the probability of a term given a document and a point in time as PML(v|t , d ) = PML(v|d)·b−(t−td ) , where b is a base parameter that determines the rate of the recency decay and td is the creation time of document d . In the remainder of this paper we refer to the cluster based retrieval models with the name of the clustering method they employ , that is , LDA , DMM , TTM and DCT .
The evaluation metrics used to assess the performance of the ranking algorithms are the ones widely used in TREC 2011—2015 Microblog tracks [ 16 ] : NDCG [ 12 ] , MAP [ 5 ] , Recall , R prec , and P@k ( Precision at k ) [ 5 ] . R Prec is the precision after R documents have been retrieved , where R is the total number of relevant document for the query . We set k to 30 to align with the cut off used in the TREC Microblog tracks [ 16 ] . The statistical significance of the observed differences between the performance of two ranking algorithms across the 107 queries is tested using a two tailed paired t test and is denoted using ( cid:78 ) ( or ( cid:72 ) ) for α = .01 , and ( cid:77 ) ( and ( cid:79 ) ) for α = 05
Experiments are run as follows : First , we obtain the top k documents , dt , in response to a user ’s query using a vanilla query likelihood model at time t . In our experiments we used k = 500 , but we experimented with other values for k ; for any k > 100 the results of our experiments remained stable . For cluster based retrieval , topics are then inferred over the documents in dt , and ( a ) documents are re ranked based on the cluster based query likelihood model , and rankings are evaluated on the basis of different information retrieval metrics , and ( b ) we calculate the likelihood of observing these documents in the collection on the basis of the underlying generative model . We use a 60/30/10 split of our collection for training , validation and testing , respectively . We train the vanilla LM , LDA , DMM , TTM , and DCT for different values of the parameters λ , and µ in Eq ( 13 ) ; λ varies from 0 to 1.0 and µ from 0 to 1000 . The optimal λ and µ values are decided based on the validation set , and evaluated on the test set . The training/validation/test splits are permuted until all 107 queries have been chosen once for the test set . We repeat the experiments 10 times and report the average evaluation measures .
6 . RESULTS
We start by comparing the retrieval performance of DCT with the rest of the methods in Section 5.3 ( RQ1 ) , and the persistence of the performance across queries ( RQ2 ) . We then analyse the effect of
1000 Table 2 : Mean performance over the five test cutoff days . The best performance per metric is in boldface . Statistically significant differences between DCT and the best baseline , TTM , are marked in the upper right hand corner of DCT ’s performance scores .
NDCG MAP .2241 .4446 LM .5018 .2749 LDA .2991 TMS .5286 .3416 DMM .5715 TTM .5936 .3638 .6421(cid:78 ) .4132(cid:78 ) DCT
Recall R prec .3534 .3092 .3439 .3976 .4152 .3604 .4460 .3854 .4019 .4648 .4298(cid:78 ) .5021(cid:78 )
P@30 .2772 .3171 .3353 .3685 .3928 .4277(cid:78 ) various parameters in our model : the dependency length ( RQ3 ) , the mixture parameter λ ( RQ4 ) , and the predefined number of topics ( RQ5 ) . Last , we test the generalisability of the proposed generative model in terms of perplexity ( RQ6 ) . 6.1 The Ranking Performance of DCT
RQ1 : We compare the ranking performance of the short term DCT cluster based retrieval model with the rest of the methods in Section 53
Table 2 reports the performance averaged across all five testing time cutoffs . The ranking of models with respect to the retrieval performance is consistent across the different evaluation measures , and in particular the following order is observed : DCT > TTM > DMM > TMS ≥ LDA > LM . Here > denotes statistically significantly better performance at a significance level of 99 % , and ≥ denotes statistically significantly better performance at a significance level of 95 % . To get a better insight on the persistence of the results across the five testing time cutoffs , we compare the performance of the six algorithms on a per cutoff basis . We visualise the results in Fig 2 in terms of five heat maps , one per metric , so that the relative performance per model and per time cutoff can be observed , by examining the intense of the color ( dark blue translates to high measure values , and light blue to low measure values ) . The five heat maps lead to the exact same findings per time cutoff to those when the average values were considered : in most cases , DCT statistically significantly outperforms TTM , which is followed by DMM , TMS , LDA , and LM .
The finding DCT > TTM in both Table 2 and Fig 2 illustrates that the way we track the changes of topics specific to a query in DCT works better than the way it is done by TTM which focuses on long documents . The finding DCT > DMM illustrates that DCT integrates time information better in the inference of topics distribution at time t compared to DMM , which ignores time information . An interesting observation in Fig 2 is that as time progresses , the performance of both DCT and all other baselines slightly decreases due to the fact that more and new intents underlying a given query appear and make the retrieval task more challenging . Instead the performance of DCT remains stable across all the test time cutoffs . 6.2 Query level Analysis
RQ2 : We take an in depth view of the improvements of DCT performance over the best baseline ( TTM ) on a per query basis .
Fig 3 shows the per query performance differences in terms of all the metrics , averaged across all the test days . The number of queries on which DCT outperforms TTM is larger than the number of queries on which TTM outperforms DCT , for every metric . Further , the positive differences of DCT against TTM are larger than the negative differences in most case . Both of these findings further support the conclusion that DCT can effectively capture the topic distribution at a given time and query for clustering short documents in streams compared to state of the art dynamic or nondynamic clustering topic models for long or short text documents . There are only very few cases in which DCT performs worse than TTM . 6.3 Impact of Dependency Length
RQ3 : We compare the short term DCT with the long term DCT ( DCT L with L being the length of dependency under consideration ) . We vary the length of dependency from 1 to 8 time steps .
Fig 4 shows the performance on the metrics , averaged across the five test cutoff days . It is clear from the figure that the longer the dependences captured by the model the better the performance of the ranking algorithm . This is especially true for L = 1 , . . . , 4 , while after that the performance reaches a plato . This illustrates that our DCT model can enhance the performance of clustering when past distribution information is integrated in the model .
In the remaining of the analysis we will focus on the short term DCT model so that we can study the performance of our dynamic topic model independently of the length of the dependency . The performance of DCT L is at least as good as the performance of the short term DCT . 6.4 Contribution of Clustering Ingredient
RQ4 : We vary λ in Eq ( 13 ) and measure the average performance of our model to analyze the contribution of clustering ingredient in the cluster based retrieval model .
Fig 5 depicts the performance on all metrics . For λ = 0 , the performance of DCT and the rest of the methods is identical with the time sensitive language model ( LM ) performance , as expected . As λ increases from 0 to 0.6 , giving more weight to the cluster terms , the performance of all cluster based methods improves , with the DCT clusters providing more relevant to the query terms . This leads to a faster improvement of DCT compared to the rest of the methods ( TTM , DMM , and LDA ) , which demonstrates the homogeneity of clusters on the query topic . The performance of all algorithms drops as expected when larger weights are given to the cluster terms . However , even when the query is completely ignored ( λ = 1 ) the DCT clusters continue to provide good on topic terms outperforming the language model method in the task of reranking . Again , these findings strengthen our conclusion that integrating high quality clustering information , as provided by our dynamic clustering model , can enhance the performance of ad hoc retrieval in short document streams . 6.5 Effect of the Number of Topics
RQ5 : We examine the effect of the number of latent topics passed as an input parameter to DCT and the rest of the clustering models on the overall retrieval performance . We vary the number of latent topics from 2 to 16 for each query , and compare the performance in terms of all the metrics , averaged across all five test cutoff days .
As illustrated by Fig 6 when only two latent topics are modeled , the four clustering models yields almost the same performance ; if the number of available topics to be inferred is small DCT does not offer any improvements compared to other methods . With the number of latent topics increasing to 4 and 8 , the positive performance differences between DCT and baseline methods also increases . When the number of latent topics further increases ( eg between 8 to 16 ) , the performance of all the clustering models reaches a plato . This also demonstrates the merit of the proposed DCT model : it is robust and insensitive to the number of latent topics and once enough latent topics are used it is able to improve the
1001 ( a ) NDCG
( b ) MAP
( c ) Recall
( d ) R prec
( e ) P@30
Figure 2 : Heat maps of retrieval performance ( one per metric ) ; columns represent testing cutoff days ( February 9 , March 1 , March 21 , April 10 , and April 30 , respectively ) ; rows represent methods ( DCT , TTM , DMM , TMS , LDA , and LM , going from top to bottom ) .
Figure 3 : Per query retrieval performance differences between DCT and TTM , averaged across all test days . A bar above the line y=0 indicates that DCT outperforms TTM , while the opposite is true for bars below y=0 .
Figure 4 : Mean retrieval performance of the short term and the long term DCT with the dependency length L varying between 2 and 8 . performance of the cluster based retrieval model and work better than the state of the art dynamic and non dynamic clustering models in short document streams . 6.6 Perplexity
RQ6 : Last , we evaluate the performance of DCT and the baseline models in terms of perplexity , which is widely used as an evaluation metric in previous topic modeling work [ 2 , 3 ] . The perplexity used in language modeling , is monotonically decreasing with the likelihood of the documents , and is algebraically equivalent to the inverse of the geometric mean per word likelihood . The perplexity [ 3 ] that is widely used to evaluate the generalization performance of many topic models is computed as Perplexity(dt ) = exp , − words in document d , and p(v|t , d ) =
|dt| v∈d log p(v|t , d ) d=1 Nd
, where Nd is the number of
|dt| d=1 z p(v|t , d , z)p(z|t , d ) . A lower perplexity score indicates better generalization performance . Fig 7 shows the mean perplexity performance of DCT and the baseline models , over the five test cutoff days with the number of latent topics ranging between 2 and 16 for each query . As it can be observed , DCT consistently performs better than the rest of the models , with the performance flattening out when the number of topics is equal or more than 8 .
7 . CONCLUSION
Clustering technologies have been widely used in a number of text related applications including information retrieval , and summarization . In this work we studied the problem of clustering short document streams , and proposed a new dynamic Dirichlet multi nomial mixture clustering topic model , DCT , to effectively handle both the textual sparsity of short documents , and the dynamic nature of topics across time . The proposed clustering model can capture short term and long term trends in topics . We evaluated the performance of the proposed model in terms of retrieval effectiveness . We conducted experiments over a Twitter streaming dataset , which was manually labeled . To allow possible drifts in the query intent across time we did not provide any static query intent description to the assessors . We compared the performance of the proposed model with a state of the art dynamic topic model that infers clusters in the context of long documents , a static topic model that infers clusters in static short document sets , a state of the art time aware microblog search model , an LDA topic model , and a time sensitive language model . Our experimental results demonstrate the effectiveness of the proposed dynamic clustering model . As future work we intent to automatically estimate the ( dynamic ) number of topics in our clustering model in the context of short document streams , and use the proposed model to improve the performance of other text related applications such as tweet summarization , sentiment analysis , and query suggestion in the context of short document streams .
Acknowledgments . We thank Prof . W . Bruce Croft and Stephen M . Harding at the University of Massachusetts , Amherst for helpful discussion and preprocessing the dataset , respectively . This research was partially supported by the UCL Big Data Institute and Elsevier .
0.6 0 06∆NDCGqueries 06 0 06∆APqueries 06 0 06∆Recallqueries 06 0 06∆Rprecqueries 06 0 0.6∆P@30queriesDependent lengthNDCGDCT−LDCT2468062068074Dependent lengthMAPDCT−LDCT2468040045050Dependent lengthRecallDCT−LDCT2468040044Dependent lengthR precDCT−LDCT2468046050054Dependent lengthP@30DCT−LDCT24680420460501002 Figure 5 : Mean retrieval performance of DCT and the state of the art topic models when varying the smoothing parameter λ .
Figure 6 : Mean retrieval performance of DCT and the state of the art topic models with the number of latent topics ranging from 2 to 16 . search . Information Processing & Management , pages 89–113 , 2015 .
[ 14 ] S . Liang , Z . Ren , and M . de Rijke . Fusion helps diversification . In
SIGIR , pages 303–312 , 2014 .
[ 15 ] S . Liang , Z . Ren , and M . de Rijke . Personalized search result diversification via structured learning . In KDD , pages 751–760 , 2014 .
[ 16 ] J . Lin , M . Efron , Y . Wang , and G . Sherman . Overview of the TREC
2014 Microblog track . In TREC 2015 . NIST , 2015 .
[ 17 ] J . S . Liu . The collapsed Gibbs sampler in Bayesian computations with applications to a gene regulation problem . J . Am . Stat . Assoc . , 89(427):958–966 , 1994 .
[ 18 ] T . Minka . Estimating a dirichlet distribution , 2000 . [ 19 ] X . Quan , Q . Wang , Y . Zhang , L . Si , and L . Wenyin . Latent discriminative models for social emotion detection with emotional dependency . ACM Trans . Inf . Syst . , 34(1):2:1–2:19 , 2015 .
[ 20 ] X . Wang and A . McCallum . Topics over time : a non markov continuous time model of topical trends . In KDD , pages 424–433 , 2006 .
[ 21 ] X . Wei and W . B . Croft . LDA based document models for ad hoc retrieval . In SIGIR , pages 178–185 , 2006 .
[ 22 ] X . Wei , J . Sun , and X . Wang . Dynamic mixture models for multiple time series . In IJCAI , pages 2909–2914 , 2007 .
[ 23 ] J . Xu , P . Wang , G . Tian , B . Xu , and J . Zhao . Short text clustering via convolutional neural networks . In NAACL HLT , pages 62–69 , 2015 .
[ 24 ] Z . Yang , A . Kotov , A . Mohan , and S . Lu . Parametric and non parametric user aware sentiment topic models . In SIGIR , pages 413–422 , 2015 .
[ 25 ] J . Yin and J . Wang . A dirichlet multinomial mixture model based approach for short text clustering . In KDD , pages 233–242 , 2014 .
APPENDIX A . GIBBS SAMPLING DERIVATION FOR
DYNAMIC CLUSTERING TOPIC MODEL
We begin with the joint distribution P ( dt , zt|Φt−1 , Θt−1 , αt , βt ) . We can take advantage of conjugate priors to simplify the integrals . We use the abbreviation φ = φt−1,z,v in the following . All other symbols are defined in Section 3 and 4 . Due to space limitation , we only provide the derivation for the short term dependence DCT model , and the derivation for the long term DCT model is actually similar . P ( dt , zt|Φt−1 , Θt−1 , αt , βt ) = P ( dt|zt , Φt−1 , βt)P ( zt|Θt−1 , αt )
=
P ( dt|zt , Φt)P ( Φt|Φt−1 , βt)dΦt
P ( zt|Θt)P ( Θt|Θt−1 , αt)dΘt
Figure 7 : Mean perplexity of DCT and state of the art topic models with varying numbers of latent topics . 8 . REFERENCES
[ 1 ] N . Begum , L . Ulanova , J . Wang , and E . Keogh . Accelerating dynamic time warping clustering with a novel admissible pruning strategy . In KDD , pages 49–58 , 2013 .
[ 2 ] D . M . Blei and J . D . Lafferty . Dynamic topic models . In ICML , pages 113–120 , 2006 .
[ 3 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent dirichlet allocation . J .
Mach . Learn . Res . , 3:993–1022 , 2003 .
[ 4 ] M . Botezatu , J . Bogojeska , I . Giurgiu , H . Voelzer , and D . Wiesmann .
Multi view incident ticket clustering for optimal ticket dispatching . In KDD , pages 1711–1720 , 2015 .
[ 5 ] W . B . Croft , D . Metzler , and T . Strohman . Search engines :
Information retrieval in practice . Addison Wesley Reading , 2015 .
[ 6 ] N . Du , M . Farajtabar , A . Ahmed , A . J . Smola , and L . Song . Dirichlet hawkes processes with applications to clustering continuous time document streams . In KDD , pages 347–362 , 2015 .
[ 7 ] M . Efron , J . Lin , J . He , and A . de Vries . Temporal feedback for tweet search with non parametric density estimation . In SIGIR , pages 33–42 , 2014 .
[ 8 ] D . Fisher , A . Jain , M . Keikha , W . B . Croft , and N . Lipka . Evaluating ranking diversity and summarization in microblogs using hashtags . Technical report , University of Massachusetts , 2015 .
[ 9 ] T . L . Griffiths and M . Steyvers . Finding scientific topics . PNAS , 101 :
5228–5235 , 2004 .
[ 10 ] T . Iwata , S . Watanabe , T . Yamada , and N . Ueda . Topic tracking model for analyzing consumer purchase behavior . In IJCAI , volume 9 , pages 1427–1432 , 2009 .
[ 11 ] T . Iwata , T . Yamada , Y . Sakurai , and N . Ueda . Online multiscale dynamic topic models . In KDD , pages 663–672 . ACM , 2010 .
[ 12 ] K . Järvelin and J . Kekäläinen . Cumulated gain based evaluation of
IR techniques . ACM Trans . Inf . Syst . , 20(4):422–446 , 2002 .
[ 13 ] S . Liang and M . de Rijke . Burst aware data fusion for microblog
λNDCGDCTTTMDMMLDA000408040608λMAPDCTTTMDMMLDA00040802030405λRecallDCTTTMDMMLDA000408025040055λR precDCTTTMDMMLDA000408030045060λP@30DCTTTMDMMLDA000408025040055Number of topicsNDCGDCTTTMDMMLDA481216035050065Number of topicsMAPDCTTTMDMMLDA481216010025040Number of topicsRecallDCTTTMDMMLDA481216025035045Number of topicsR precDCTTTMDMMLDA481216025035045055Number of topicsP@30DCTTTMDMMLDA481216020030040Number of topicsPerplexityDCTTTMDMMLDA4812163000600090001003 P ( vt,di|φt,zdi )
Z z=1
P ( φt,z|φt−1,z , βt)dΦt
=
Z mt,z + αt,zθt−1,z − 1
Nd,v z=1(mt,z + αt,zθt−1,z ) − 1 Nd i=1(nt,z,−d + i − 1 +V j=1 ( nt,z,v,−d + βt,z,vφ + j − 1 ) v=1 βt,z,vφ ) v∈d
×
=
=
=
=
= i=1 z=1 d=1 d=1 d=1 v=1
×
×
|dt| Nd |dt| Z V |dt| Z V Z Γ(V Z V × Γ(Z Z Γ(V Z V × Γ(Z Z
× v=1 z=1 z=1 z=1 z=1
θ
P ( zt,d|θt)P ( θt|θt−1 , αt)dΘt
φ nt,z,v t,z,v
P ( φt,z|φt−1,z , βt)dΦt
Z z=1
P ( zt,d|θt)P ( θt|θt−1 , αt)dΘt dΦt
βt,z,v φ−1 t,z,v
φ v=1 βt,z,vφ ) v=1 Γ(βt,z,vφ )
V Z v=1 z=1 z=1 αt,zθt−1,z ) z=1 Γ(αt,zθt−1,z )
αt,z θt−1,z−1 t,z dΘt
θ
φ nt,z,v t,z,v mt,z t,z z=1
Γ(V V
Z Γ(Z Z V Z Z V Z Γ(V Z Γ(Z v=1 z=1 z=1 z=1 v=1 βt,z,vφ ) v=1 Γ(βt,z,vφ ) z=1 αt,zθt−1,z ) z=1 Γ(αt,zθt−1,z ) v=1 βt,z,vφ ) v=1 Γ(βt,z,vφ ) z=1 αt,zθt−1,z ) z=1 Γ(αt,zθt−1,z ) nt,z,v +βt,z,v φ−1 t,z,v
φ dΦt mt,z +αt,z θt−1,z−1 θ t,z v=1 Γ(nt,z,v + βt,z,vφ ) v=1 nt,z,v + βt,z,vφ ) z=1 Γ(mt,z + αt,zθt−1,z ) z=1 mt,z + αt,zθt−1,z )
.
B . DERIVATION OF THE UPDATE RULES We apply a fixed point iteration for estimating the parameters αt and βt by maximizing the joint distribution P ( dt , zt|Φt−1 , Θt−1 , αt , βt ) . Here we only show the derivation for short term dependence DCT model , while that for long term dependence DCT model is similar . Instead of maximizing the joint distribution directly , we try to maximize the following :
= log Γ( nt,z,v + βt,z,vφ ) log P ( dt , zt|Φt−1 , Θt−1 , αt , βt )
Z Z z=1
+ z=1
+ log Γ(
Z z=1 v=1 v=1 log Γ(
V V βt,z,vφ ) − Z V log Γ(nt,z,v + βt,z,vφ ) − Z Z Z log Γ(mt,z + αt,zθt−1,z ) − Z
αt,zθt−1,z,v ) − log Γ( v=1 z=1 z=1 z=1
V v=1 log Γ(βt,z,vφ ) mt,z + αt,zθt−1,z ) log Γ(αt,zθt−1,z ) z=1 z=1
Using the bounds [ 18 ] : for any x∗ ∈ R+ , n ∈ Z+ and x∗ ’s estimation x : log Γ(x∗ ) − log Γ(x∗ + n ) ≥ log Γ(x ) − log Γ(x + n )
+ ( Ψ(x + n ) − Ψ(x ) ) ( x − x∗ ) , and log Γ(x∗ + n ) − log Γ(x∗ ) ≥ log Γ(x + n ) − log Γ(x )
+ x ( Ψ(x + n ) − Ψ(x ) ) ( log x∗ − log x ) , t,z is the optimal parameter in the next fixed point iteration , it supposing α∗ follows that log P ( dt , zt|Φt−1 , Θt−1 , {αt,1 , . . . α∗ =αt,zθt−1,z ( Ψ(mt,z + αt,zθt−1,z ) − Ψ(αt,zθt−1,z ) ) log α∗ t,z , . . . , αt,Z} , βt ) ≥ B(α∗ t,z ) t,zθt−1,z
− α∗ t,zθt−1,z
Ψ( mt,z + αt,zθt−1,z )
+ C ,
Z z=1 where C is function not containing the term α∗ out by taking ∂(· ) ∂α∗ ∂B(α∗ αt,zθt−1,z ( Ψ(mt,z + αt,zθt−1,z ) − Ψ(αt,zθt−1,z ) ) ∂α∗ t,z . Then , we let t,z and thus will be integrated to α∗ t,z )
α∗
= t,z t,z t,z
Z
− θt−1,z
=0 ,
Z
Ψ( mt,z + αt,zθt−1,z ) − Ψ(
αt,zθt−1,z ) z=1 z=1 which results in
α∗ t,z =
Ψ(Z
αt,z ( Ψ(mt,z + αt,zθt−1,z ) − Ψ(αt,zθt−1,z ) ) z=1 mt,z + αt,zθt−1,z ) − Ψ(Z z=1 αt,zθt−1,z )
, where Ψ(· ) is the digamma function defined by Ψ(x ) = ∂ log Γ(x )
Following the same derivation , again supposed β∗
∂x
. t,z,v is the optimal pa rameter in the next fixed point iteration , we have v=1 nt,z,v + βt,z,vφ ) − Ψ(V
Ψ(nt,z,v + βt,z,vφ ) − Ψ(βt,z,vφ ) v=1 βt,z,vφ )
. z=1 βt,z,vφ
Ψ(V
Z Z z=1 φ
β∗ t,z,v = dΘt
+
( cid:30 )
P ( zt,dt|Φt−1,Θt−1,αt,βt ) P ( zt,−d,dt|Φt−1,Θt−1,αt,βt ) z=1 Γ(mt,z + αt,zθt−1,z ) z=1 mt,z + αt,zθt−1,z ) z=1 Γ(mt,z,−d + αt,zθt−1,z ) z=1 mt,z,−d + αt,zθt−1,z )
.
=
= z=1 z=1
×
×
Because document d is associated with its own topic z , it becomes v=1 Γ(nt,z,v + βt,z,vφ ) v=1 nt,z,v + βt,z,vφ ) v=1 Γ(nt,z,v + βt,z,vφ ) v=1 nt,z,v + βt,z,vφ ) v=1 Γ(nt,z,v,−d + βt,z,vφ ) v=1 nt,z,v,−d + βt,z,vφ )
Z Γ(Z Z Γ(Z
Applying the chain rule , we can obtain the conditional probability : P ( zd = z|zt,−d , dt , Φt−1 , Θt−1 , αt , βt ) = ∝ P ( zt , dt|Φt−1 , Θt−1 , αt , βt ) V P ( zt,−d , dt,−d|Φt−1 , Θt−1 , αt , βt ) Z Γ(V V Z Γ(V V Γ(V V Γ(V V V
Γ(Z Γ(Z Γ(Z Γ(Z Γ(V Γ(V Γ(Z Γ(Z Γ(nt,z,−d +V Γ(nt,z,−d + Nd +V m
Γ(mt,z + αt,zθt−1,z − 1 ) ×
Γ(mt,z + αt,zθt−1,z − 1 ) × v=1 Γ(nt,z,v,−d + βt,z,vφ ) v=1 nt,z,v,−d + βt,z,vφ )
Γ(mt,z + αt,zθt−1,z ) z=1 mt,z + αt,zθt−1,z )
Applying Γ(x ) = ( x − 1)Γ(x − 1 ) and Γ(x + m ) = v=1 Γ(nt,z,v,−d + βt,z,vφ ) v∈d Γ(nt,z,v,−d + βt,z,v ) z=1 mt,z + αt,zθt−1,z ) z=1 mt,z + αt,zθt−1,z ) v=1 Γ(nt,z,v + βt,z,vφ ) v∈d Γ(nt,z,v + βt,z,v )
Γ(mt,z + αt,zθt−1,z )
Γ(mt,z + αt,zθt−1,z )
×
=
= v=1 nt,z,v + βt,z,vφ ) z=1(mt,z + αt,zθt−1,z ) − 1 )
Γ(mt,z,−d + αt,zθt−1,z ) z=1 mt,z,−d + αt,zθt−1,z )
× z=1(mt,z + αt,zθt−1,z ) − 1 ) v=1 nt,z,v,−d + βt,z,vφ ) v=1 βt,z,vφ ) ( x + i − 1)Γ(x ) , v=1 βt,z,vφ )
( cid:30 )
. the above becomes
Z mt,z + αt,zθt−1,z − 1 z=1(mt,z + αt,zθt−1,z ) − 1
= i=1 i=1(nt,z,−d + i − 1 +V Nd v∈d Γ(nt,z,v +βt,z,v φ ) v∈d Γ(nt,z,v,−d+βt,z,v φ ) v=1 βt,z,vφ )
1004
