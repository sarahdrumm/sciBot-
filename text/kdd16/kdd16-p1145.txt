ABRA : Approximating Betweenness Centrality in Static and Dynamic Graphs with Rademacher Averages
Matteo Riondato
Two Sigma Investments , LP matteo@twosigma.com
New York , NY , USA
ΑΒΡΑΞΑΣ ( ABRAXAS ) : Gnostic word of mystic meaning ABSTRACT We present ABRA , a suite of algorithms to compute and maintain probabilistically guaranteed , high quality , approximations of the betweenness centrality of all nodes ( or edges ) on both static and fully dynamic graphs . Our algorithms use progressive random sampling and their analysis rely on Rademacher averages and pseudodimension , fundamental concepts from statistical learning theory . To our knowledge , this is the first application of these concepts to the field of graph analysis . Our experimental results show that ABRA is much faster than exact methods , and vastly outperforms , in both runtime and number of samples , state of the art algorithms with the same quality guarantees .
Keywords betweenness ; centrality ; pseudodimension ; Rademacher averages ; sampling ;
1 .
INTRODUCTION
Centrality measures are fundamental concepts in graph analysis : they assign to each node or edge in the network a score that quantifies some notion of importance of the node/edge in the network [ 21 ] . Betweenness Centrality ( bc ) is a very popular centrality measure that , informally , defines the importance of a node or edge z in the network as proportional to the fraction of shortest paths in the network that go through z [ 2 , 13 ] ( see Sect . 3 for formal definitions ) . Brandes [ 9 ] presented an algorithm ( denoted BA ) to compute the exact bc values for all nodes or edges in a graph G = ( V , E ) in time O(|V ||E| ) if the graph is unweighted , or time O(|V ||E| + |V |2 log |V | ) if the graph has positive weights . The cost of BA is excessive on modern networks with millions of nodes and tens of millions of edges . Moreover , having the exact bc values may often not be needed , given the exploratory nature of the task , and a high quality
Dept . of Computer Science – Brown University
Eli Upfal
Providence , RI , USA eli@csbrownedu approximation of the values is usually sufficient , provided it comes with stringent guarantees .
Today ’s networks are not only large , but also dynamic : edges are added and removed continuously . Keeping the bc values up to date after edge insertions and removals is a challenging task , and proposed algorithms [ 15 , 17–19 ] have worst case time and space complexities not better than fromscratch recomputation using BA . Maintaining a high quality approximation up to date is more feasible and more sensible : there is little added value in keeping track of exact bc values that change continuously .
Contributions . We focus on developing algorithms for approximating the bc of all nodes and edges in static and dynamic graphs . Our contributions are the following . • We present ABRA ( for “ Approximating Betweenness with Rademacher Averages ” ) , the first family of algorithms based on progressive sampling for approximating the bc of all nodes in static and dynamic graphs , where node and edge insertions and deletions are allowed . The bc approximations computed by ABRA are probabilistically guaranteed to be within an user specified additive error from their exact values . We also present variants with relative error ( ie , within a multiplicative factor ε of the true value ) for the top k nodes with highest bc , and variants that use refined estimators to give better approximations with a slightly larger sample size . • Our analysis relies on Rademacher averages [ 28 ] and pseudodimension [ 23 ] , fundamental concepts from the field of statistical learning theory [ 30 ] . Exploiting known and novel results using these concepts , ABRA computes the approximations without having to keep track of any global property of the graph , in contrast with existing algorithms [ 4 , 6 , 24 ] . ABRA performs only “ real work ” towards the computation of the approximations , without having to obtain such global properties or update them after modifications of the graph . To the best of our knowledge , ours is the first application of Rademacher averages and pseudodimension to graph analysis problems , and the first to use progressive random sampling for bc computation . Using pseudodimension , we derive new analytical results on the sample complexity of the bc computation task , generalizing previous contributions [ 24 ] , and formulating a conjecture on the connection between pseudodimension and the distribution of shortest path lengths . • The results of our experimental evaluation on real networks show that ABRA outperforms , in both speed and
1145 number of samples , the state of the art methods offering the same guarantees [ 24 ] . Due to space constraints , some details and additional results have been deferred to the extended online version [ 25 ] .
2 . RELATED WORK
The definition of Betweenness Centrality comes from the sociology literature [ 2 , 13 ] , but the study of efficient algorithms to compute it started only when graphs of substantial size became available to the analysts , following the emergence of the Web . The BA algorithm by Brandes [ 9 ] is currently the asymptotically fastest algorithm for computing the exact bc values for all nodes in the network . A number of works also explored heuristics to improve BA [ 12 , 27 ] , but retained the same worst case time complexity . The use of random sampling to approximate the bc values in static graphs was proposed independently by Bader et al . [ 3 ] and Brandes and Pich [ 10 ] , and successive works explored the tradeoff space of sampling based algorithms [ 4–6 , 24 ] . We focus here on related works that offer approximation guarantees similar to ours . For an in depth discussion of previous contributions approximating bc on static graphs , we refer the reader to [ 24 , Sect . 2 ] .
Riondato and Kornaropoulos [ 24 ] present algorithms that employ the Vapnik Chervonenkis ( VC ) dimension [ 30 ] to compute what is currently the tightest upper bound on the sample size sufficient to obtain guaranteed approximations of the bc of all nodes in a static graph . Their algorithms offer the same guarantees as ours but , to compute the sample size , they need to compute an upper bound on a characteristic quantity of the graph ( the vertex diameter , namely the maximum number of nodes on any shortest path ) . Thanks to our use of Rademacher averages in a progressive random sampling setting , ABRA does not need to compute any characteristic quantity of the graph , and instead uses an efficient to evaluate stopping condition to determine when the approximated bc values are close to the exact ones . This allows ABRA to use smaller samples and be much faster than the algorithms by Riondato and Kornaropoulos [ 24 ] . A number of works [ 15 , 17–19 ] focused on computing the exact bc for all nodes in a dynamic graph , taking into consideration different update models . None of these algorithm is provably asymptotically faster than a complete computation from scratch using Brandes’ algorithm [ 9 ] and they all require significant amount of space ( more details about these works can be found in [ 4 , Sect . 2] ) . In contrast , Bergamini and Meyerhenke [ 4 , 5 ] built on the work by Riondato and Kornaropoulos [ 24 ] to derive an algorithm for maintaining high quality approximations of the bc of all nodes when the graph is dynamic and both additions and deletions of edges are allowed . Due to the use of the algorithm by Riondato and Kornaropoulos [ 24 ] as a building block , the algorithm must keep track of the vertex diameter after an update to the graph . Our algorithm for dynamic graphs , instead , does not need this piece of information , and therefore can spend more time in computing the approximations , rather than in keeping track of global properties of the graph . Moreover , our algorithm can handle directed graphs , which is not the case for the algorithms by Bergamini and Meyerhenke [ 4 , 5 ] . Hayashi et al . [ 16 ] recently proposed a data structure called Hypergraph Sketch to maintain the shortest path DAGs between pairs of nodes following updates to the graph . Their algorithm uses random sampling and this novel data struc ture allows them to maintain a high quality , probabilistically guaranteed approximation of the bc of all nodes in a dynamic graph . Their guarantees come from an application of the simple uniform deviation bounds ( ie , the union bound ) to determine the sample size , as previously done by Bader et al . [ 3 ] and Brandes and Pich [ 10 ] . As a result , the resulting sample size is excessively large , as it depends on the number of nodes in the graph . Our improved analysis using the Rademacher averages allows us to develop an algorithm that uses the Hypergraph Sketch with a much smaller number of samples , and is therefore faster .
Progressive random sampling with Rademacher Averages has been used by Elomaa and Kääriäinen [ 11 ] and Riondato and Upfal [ 26 ] in completely different settings .
3 . PRELIMINARIES
We now introduce the formal definitions and basic results that we use throughout the paper . 3.1 Graphs and Betweenness Centrality
Let G = ( V , E ) be a graph . G may be directed or undirected and may have non negative weights on the edges . For any ordered pair ( u , v ) of different nodes u 6= v , let Suv be the set of Shortest Paths ( SPs ) from u to v , and let σuv = |Suv| . Given a path p between two nodes u , v ∈ V , a node w ∈ V is internal to p iff w 6= u , w 6= u , and p goes through w . We denote as σuv(w ) the number of SPs from u to v that w is internal to .
Definition 1
( Betweenness Centrality ( bc ) [ 2 , 13] ) .
Given a graph G = ( V , E ) , the Betweenness Centrality ( bc ) of a node w ∈ V is defined as b(w ) =
1
|V |(|V | − 1 )
σuv(w )
σuv
( ∈ [ 0 , 1 ] ) .
X
( u,v)∈V ×V u6=v
Many variants of bc have been proposed in the literature , including one for edges [ 21 ] . Our results can be extended to these variants , following the reduction by Riondato and Kornaropoulos [ 24 , Sect . 6 ] , but we do not discuss them here due to space constraints . of the collection B = {b(w ) , w ∈ V } .
In this work we focus on computing an ( ε , δ) approximation
Definition 2
( (ε , δ) approximation ) . Given ε , δ ∈ ( 0 , 1 ) , an ( ε , δ) approximation to B is a collection ˜B = {˜b(w ) , w ∈ V } such that
Pr(∀w ∈ v,|˜b(w ) − b(w)| ≤ ε ) ≥ 1 − δ .
In Sect . 4.2 we discuss a relative ( ie , multiplicative ) error variant for the top k nodes with highest bc . 3.2 Rademacher Averages
Rademacher Averages are fundamental concepts to study the rate of convergence of a set of sample averages to their expectations . They are at the core of statistical learning theory [ 30 ] but their usefulness extends way beyond the learning framework [ 26 ] . We present here only the definitions and results that we use in our work and we refer the readers to , eg , the book by Shalev Shwartz and Ben David [ 28 ] for in depth presentation and discussion .
While the Rademacher complexity can be defined on an arbitrary measure space , we restrict our discussion here to
1146 a sample space that consists of a finite domain D and a uniform distribution over that domain . Let F be a family of functions from D to [ 0 , 1 ] , and let S = {c1 , . . . , c‘} be a sample of ‘ elements from D , sampled uniformly and independently at random . For each f ∈ F , the true average and the sample average of f on a sample S are , respectively , mD(f ) = 1 |D| and mS(f ) = 1
‘X
‘X f(ci ) . f(c )
( 1 )
‘
Given S , we are interested in bounding the maximum deviation of mS(f ) from mD(f ) among all f ∈ F , ie , sup f∈F
|mS(f ) − mD(f)| .
( 2 ) For 1 ≤ i ≤ ‘ , let σi be a Rademacher rv , ie , a rv that takes value 1 with probability 1/2 and −1 with probability 1/2 . The rv ’s σi are independent . Consider the quantity c∈D i=1
#
"
‘X i=1
R(F,S ) = Eσ
1 ‘ sup f∈F
σif(ci )
,
( 3 ) where the expectation is taken wrt the Rademacher rv ’s , ie , conditionally on S . The quantity R(F,S ) is known as the ( conditional ) Rademacher average of F on S . The following is a key result in statistical learning theory , connecting R(F,S ) to the maximum deviation ( 2 ) .
Theorem 1
( Thm . 26.5 [ 28] ) . Let η ∈ ( 0 , 1 ) and let S be a collection of ‘ elements of D sampled independently and uniformly at random . Then , with probability at least 1 − η , r2 ln(2/η )
‘
.
( 4 )
|mS(f ) − mD(f)| ≤ 2R(F,S ) + sup f∈F
Thm . 1 is how the result is classically presented , but better although more complex bounds than ( 4 ) are available [ 22 ] . ( Thm . 3.11 [ 22] ) . Let η ∈ ( 0 , 1 ) and let S be a collection of ‘ elements of D sampled independently and uniformly at random . Let
Theorem 2 q,2‘R(F,S ) + ln 2 ln 2
η
η
ln 2
η
α = ln 2
η +
,
( 5 ) rln 2
η
η ln 2 then , with probability at least 1 − η , |mS(f ) − mD(f)| ≤ R(F,S ) 1 − α sup f∈F
+
2‘
2‘α(1 − α ) +
. ( 6 ) Computing , or even estimating , the expectation in ( 3 ) wrt the Rademacher rv ’s is not straightforward and can be computationally expensive , requiring a time consuming Monte Carlo simulation [ 7 ] . For this reason , upper bounds to the Rademacher average are usually employed in ( 4 ) and ( 6 ) in place of R(F,S ) . A powerful and efficient to compute bound is presented in Thm . 3 . Given S , consider , for each f ∈ F , the vector vf,S = ( f(c1 ) , . . . , f(c‘) ) , and let VS = {vf , f ∈ F} be the set of such vectors ( |VS| ≤ |F| ) .
Theorem 3 ln X
( [26] ) . Let w : R+ → R+ be the function ( 7 )
2kvk2 exp(s
/(2‘
2) ) , w(s ) = 1 s v∈VS where k · k denotes the Euclidean norm . Then
R(F,S ) ≤ min s∈R+ w(s ) .
( 8 )
The function w is convex , continuous in R+ , and has first and second derivatives wrt s everywhere in its domain , so it is possible to minimize it efficiently using standard convex optimization methods [ 8 ] . In future work , we plan to explore how to obtain a tighter bound than the one presented in Thm . 3 using recent results by Anguita et al . [ 1 ] .
4 . STATIC GRAPH BC APPROXIMATION We now present and analyze ABRA s , our progressive sampling algorithm for computing an ( ε , δ) approximation to the collection of exact bc values in a static graph . Many of the details and properties of ABRA s are shared with the other ABRA algorithms we present . Progressive Sampling . Progressive sampling algorithms are intrinsically iterative . At a high level , they work as follows . At iteration i , the algorithm extracts an approximation of the values of interest ( in our case , of the bc of all nodes ) from a collection Si of Si = |Si| random samples from a suitable domain D ( in our case , the samples are pairs of different nodes ) . Then , the algorithm checks a specific stopping condition that uses information obtained from the sample Si and from the computed approximation . If the stopping condition is satisfied , then the approximation has , with the required probability , the desired quality ( in our case , it is an ( ε , δ) approximation ) . The approximation is then returned in output and the algorithm terminates . If the stopping condition is not satisfied , ABRA s builds a collection Si+1 by adding random samples to Si until it has size Si+1 . Then it computes a new approximation from the socreated collection Si+1 , and checks the stopping condition again and so on . There are two main challenges for the algorithm designer : deriving a “ good ” stopping condition and determining good choices for the initial sample size S1 and the subsequent sample sizes Si+1 , i ≥ 1 . 1 . when satisfied , guarantees that the computed approximation has the desired quality properties ( in our case , it is an ( ε , δ) approximation ) ; and
Ideally , a good stopping condition is such that :
2 . can be evaluated efficiently ; and 3 . is tight , in the sense that is satisfied at small sample sizes . The stopping condition for ABRA s is based on Thm . 2 and Thm . 3 , and has all the above desirable properties .
The second challenge is determining the sample schedule ( Si)i>0 . Any monotonically increasing sequence of positive numbers can act as sample schedule , but the goal in designing a good sample schedule is to minimize the number of iterations that are needed before the stopping condition is satisfied , while minimizing the sample size Si at the iteration i at which this happens . The sample schedule may be fixed in advance , but an adaptive approach that ties the sample schedule to the stopping condition can give better results , as the sample size Si+1 for iteration i + 1 can be computed using information obtained in ( or up to ) iteration i . ABRA uses such an adaptive approach ( see Sect . 411 ) 4.1 Algorithm Description and Analysis
ABRA s takes as input a graph G = ( V , E ) , which may be directed or undirected , and may have non negative weights
1147 puts a collection eB = {eb(w ) , w ∈ V } that is an ( ε , δ ) on the edges , and two parameters ε , δ ∈ ( 0 , 1 ) . It outapproximation of the betweenness centralities B = {b(w ) , w ∈ V } . The algorithm samples from D = V × V , u 6= v} .
For each node w ∈ V , let fw : D → [ 0 , 1 ] be the function fw(u , v ) = σuv(w )
σuv
,
( 9 ) ie , fw(u , v ) is the fraction of shortest paths ( SPs ) from u to v that go through w . Let F = {fw , w ∈ V } be the set of these functions . Given this definition , we have that
X mD(fw ) = 1 |D| fw(u , v )
X
( u,v)∈V ×V u6=v
( u,v)∈D
=
1
|V |(|V | − 1 )
= b(w ) .
σuv(w )
σuv
Let now S = {(ui , vi ) , 1 ≤ i ≤ ‘} be a collection of ‘ pairs ( u , v ) from D . For the sake of clarity , we define eb(w ) = mS(fw ) = 1
‘
‘X i=1 For each w ∈ V consider the vector fw((ui , vi ) ) . vw = ( fw(u1 , v1 ) , . . . , fw(u‘ , v‘ ) ) .
It is easy to see thateb(w ) = kvwk1/‘ . Let now VS be the set of these vectors :
VS = {vw , w ∈ V }
( |VS| ≤ |V | ) .
If we have complete knowledge of this set of vectors , then we can compute the quantity exp,s ln X v∈VS
2kvk2
ω
∗ = min s∈R+
1 s then use ω∗ in ( 5 ) in place of R(F,S ) to obtain α , and combine ( 6 ) , ( 7 ) , and ( 8 ) to obtain
∆S = ω∗ 1 − α
+ ln 2
η
2‘α(1 − α ) + and finally check whether ∆S ≤ ε . This is ABRA s ’s stopping condition . When it holds , we can just return the col lection eB = {eb(w ) = kvwk1/‘ , w ∈ V } since , from the definition of ∆S and Thms . 2 and 3 , we have that eB is an
( ε , δ) approximation to the exact betweenness values .
ABRA s works as follows . Suppose for now that we fix a priori a monotonically increasing sequence ( Si)i>0 of sample sizes ( we show in Sect . 411 how to compute the sample schedule adaptively on the fly ) . The algorithm builds a collection S by sampling pairs ( u , v ) independently and uniformly at random from D until S has size S1 . After each pair of nodes has been sampled , ABRA s performs an s − t SP computation from u to v and then backtracks from v to u along the SPs just computed , to keep track of the set VS of vectors ( details given below ) . For clarity of presentation , let S1 denote S when it has size exactly S1 , and analogously for Si and Si , i > 1 . Once Si has been built , ABRA s computes ∆Si using η = δ/2i and checks whether ∆Si is at most
/(2‘
2) , rln 2
η
2‘
,
ε . If so , then it returns eB . Otherwise , ABRA s iterates and continues adding samples from D to S until it has size S2 , and so on until ∆Si ≤ ε holds . The pseudocode for ABRA s is presented in Alg . 1 , including the steps to update VS and to adaptively choose the sample schedule ( Sect . 411 ) We now prove the correctness of the algorithm .
Algorithm 1 : ABRA s : absolute error approximation of bc on static graphs input : Graph G = ( V , E ) , accuracy parameter ε ∈ ( 0 , 1 ) , output : Set eB of bc approximations for all nodes in V confidence parameter δ ∈ ( 0 , 1 )
1+16ε ) ln(2/δ ) 4ε2
1 D ← {(u , v ) ∈ V × V , u 6= v} √ 2 S0 ← 0 , S1 ← ( 1+8ε+ 3 0 = ( 0 ) 4 V = {0} 5 foreach w ∈ V do M[w ] = 0 6 c0 ← |V | 7 i ← 1 , j ← 1 8 while True do 9 10 11 12 13 14 for ‘ ← 1 to Si − Si−1 do end
39 return eB ← {eb(w ) ← kM[w]k1/Si , w ∈ V }
Theorem 4
( correctness ) . The collection eB returned F , fw for w ∈ V ,eb(w ) , ∆Si , Thm . 3 , and from the fact that ,
Proof . The claim follows from the definitions of S , VS , by ABRA s is a ( ε , δ) approximation . at each iteration i , Thm . 2 holds with probability δ/2i .
Computing and maintaining the set VS . We now discuss in details how ABRA s efficiently maintain the set VS of
( u , v ) ← uniform_random_sample(D ) compute_SPs(u , v ) //Truncated SP computation if reached v then foreach z ∈ Pu[v ] do σzv ← 1 foreach node w on a SP from u to v , in reverse order by d(u , w ) do σuv(w ) ← σuwσwv v ← M[w ] v0 ← v ∪ {(j , σuv(w)} if v0 6∈ V then cv0 ← 1 V ← V ∪ {v0} else cv0 ← cv0 + 1 M[w ] ← v0 if cv > 1 then cv ← cv − 1 else V ← V \ {v} foreach z ∈ Pu[w ] do σzv ← σzv + σwv end end j ← j + 1 s lnP δ +p(2Siω∗ end i ← mins∈R+ 1 ω∗ αi ← ∆Si ← ω∗ + ( i+1 ) ln 2 i1−αi if ∆Si ≤ ε then break else
( i+1 ) ln 2 i v∈VS exp,s2kvk2/(2S2 i )
( i+1 ) ln 2 δ
+(i+1 ) ln 2 q
2Siαi(1−αi ) +
δ
δ )(i+1 ) ln 2 ( i + 1 ) ln 2 δ2Si
δ
Si+1 ← nextSampleSize( ) i ← i + 1
15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
31
32
33 34 35 36 37 38 end
1148 valueseb(w ) = kvwk1/|S| in eB . In addition to VS , ABRA s vectors , which is used to compute the value ∆S and the also maintains a map M from V to VS ( ie , M[w ] is a vector vw ∈ VS ) , and a counter cv for each v ∈ VS , denoting how many nodes w ∈ V have M[w ] = v . At the beginning of the execution of the algorithm , we have S = ∅ and also VS = ∅ . Nevertheless , ABRA s initializes VS to contain one special empty vector 0 , with no components , and M so that M[w ] = 0 for all w ∈ V , and c0 = |V | ( lines 3 and following in Alg . 1 ) . After having sampled a pair ( u , v ) from D , ABRA s updates VS , M and the counters as follows . First , it performs ( line 11 ) a s − t SP computation from u to v using any SP algorithm ( eg , BFS or Dijkstra ) modified , as discussed by Brandes [ 9 , Lemma 3 ] , to keep track , for each node w encountered during the computation , of the SP distance d(u , w ) from u to w , of the number σuw of SPs from u to w , and of the set Pu(w ) of ( immediate ) predecessors of w along the SPs from u.1 Once v has been reached ( and only if it has been reached ) , the algorithm starts backtracking from v towards u along the SPs it just computed ( line 14 ) . During this backtracking , the algorithm visits the nodes along the SPs in inverse order of SP distance from u , ties broken arbitrarily . For each visited node w different from u and v , ABRA s computes the value fw(u , v ) = σuv(w ) of SPs from u to v that go through w , which is obtained as
σuv(w ) = σuw × X
σzv z : w∈Pu(z ) where the value σuw is obtained during the s− t SP computation , and the values σzw are computed recursively during the backtracking ( line 25 ) [ 9 ] . After computing σuv(w ) , the algorithm takes the vector v ∈ VS such that M[w ] = v and creates a new vector v0 by appending σuv(w ) to the end of v.2 Then it adds v0 to the set VS , updates M[w ] to v0 , and increments the counter cv0 by one ( lines 16 to 22 ) . Finally , the algorithm decrements the counter cv by one , and if cv becomes equal to zero , ABRA s removes v from VS ( line 24 ) . At this point , the algorithm moves to analyzing another node w0 with distance from u less or equal to the distance of w from u . It is easy to see that when the backtracking reaches u , the set VS , the map M , and the counters , have been correctly updated .
We remark that to compute ∆Si and eB and to keep the map M up to date , ABRA s does not actually need to store the vectors in VS ( even in sparse form ) , but it is sufficient to maintain their ‘1 and Euclidean norms , which require much less space . 411 Computing the sample schedule We now discuss how to compute the initial sample size S1 at the beginning of ABRA s ( line 2 of Alg . 1 ) and the sample size Si+1 at the end of iteration i of the main loop ( line 35 ) . We remark that any sample schedule ( Si)i>0 can be used , and our method is an heuristic that nevertheless exploits 1Storing the set of immediate predecessors is not necessary . By not storing it , we can reduce the space complexity from O(|E| ) to O(|V | ) , at the expense of some additional computation at runtime . 2ABRA s uses a sparse representation for the vectors v ∈ VS , storing only the non zero components of each v as pairs ( j , g ) , where j is the component index and g is the value of that component .
√ all available information at the end of each iteration to the most possible extent , with the goal of increasing the chances that the stopping condition is satisfied at the next iteration .
As initial sample size S1 we choose
S1 ≥ ( 1 + 8ε +
1 + 16ε ) ln(4/δ ) 4ε2
.
( 10 )
To understand the intuition behind this choice , recall ( 6 ) , and consider that , at the beginning of the algorithm , we obviously have no information about R(F,S1 ) , except that it is non negative . Consequently we also cannot compute α as in ( 5 ) using η = δ/2 , but we can easily see that α ∈ [ 0 , 1/2 ] . From the fact that R(F,S ) ≥ 0 , we have that , for the rhs of ( 6 ) to be at most ε ( ie , for the stopping condition to be satisfied after the first iteration of the algorithm ) , it is necessary that rln 4
δ
2S1 ln 4
2S1α(1 − α ) +
δ
≤ ε .
Then , using the fact that the above expression decreases as α increases , we use α = 1/2 , ie , its maximum attainable value , to obtain the following inequality , where S1 acts as the unknown : rln(4/δ )
2S1
≤ ε .
2 ln(4/δ )
+
S1
Solving for S1 under the domain constraints S1 ≥ 1 , δ ∈ ( 0 , 1 ) , and ε ∈ ( 0 , 1 ) gives the unique solution in ( 10 ) . Computing the next sample size Si+1 at the end of iteration i ( in the pseudocode in Alg . 1 , this is done by calling nextSampleSize( ) on line 35 ) is slightly more involved . The intuition is to assume that ω∗ i , which is an upper bound on R(F,Si ) , is also an upper bound on R(F,Si+1 ) , whatever Si+1 will be , and whatever size it may have . At this point , we can ask what is the minimum size Si+1 = |Si+1| for which ∆Si+1 would be at most ε , under the assumption that R(F,Si+1 ) ≤ ω∗ i . More formally , we want to solve the inequality
!
×
+
1 + p(2Si+1ω∗ r(i + 2 ) ln 2
∗ i +
ω
( i + 2 ) ln 2
δ
≤ ε
2Si+1
δ
( i + 2 ) ln 2 i + ( i + 2 ) ln 2
δ +p(2Si+1ω∗
δ )(i + 2 ) ln 2
δ i + ( i + 2 ) ln 2 2Si+1
δ )(i + 2 ) ln 2
δ
!
( 11 ) where Si+1 acts as the unknown . The lhs of this inequality is obtained by plugging ( 5 ) into ( 6 ) and using ω∗ i in place of R(F,S ) , Si+1 in place of ‘ , δ/2i+1 in place of η , and slightly reorganize the terms for readability . Finding the solution to the above inequality requires computing the roots of the cubic equation ( in x ) −8
∗ i + ( 1 + 4ε)2)x
( −16ω
3
2
( i + 2 ) ln 2 − 4
+ ( i + 2 ) ln 2
δ
( i + 2 ) ln 2 i − ε)2(1 + 4ε)x ∗
δ
δ
( ω
2 + 4(b − f)4
3 = 0 . ( 12 ) One can verify that the roots of this equation are all reals . The roots are presented in Table 1 . The solution to inequality ( 11 ) is that Si+1 should be larger than one of these roots , x
1149 i , δ , and ε . but which of the roots it should be larger than depends on the values of ω∗ In practice , we compute each of the roots and then choose the smallest positive one such that , when Si+1 equals to this root , then ( 11 ) is satisfied .
The assumption R(F,Si+1 ) ≤ ω∗ i , which is not guaranteed to be true , is what makes our procedure for selecting the next sample size an heuristics . Nevertheless , using information available at the current iteration to compute the sample size for the next iteration is more sensible than having a fixed sample schedule , as it tunes the growth of the sample size to the quality of the current sample . Moreover , it removes from the user the burden of choosing a sample schedule , effectively eliminating one parameter of the algorithm . 4.2 Relative error Top k Approximation
In practical applications it is usually necessary ( and sufficient ) to identify the nodes with highest bc , as they act , in some sense , as the “ primary information gateways ” of the network . In this section we present a variant ABRA k of ABRA s to compute a high quality approximation of the set TOP(k , G ) of the top k nodes with highest bc in a graph
G . The approximationeb(w ) returned by ABRA k for a node w is within a multiplicative factor ε from its exact value b(w ) , rather than an additive factor ε as in ABRA s . This higher accuracy has a cost in terms of the number of samples needed to compute the approximations . Formally , assume to order the nodes in the graph in decreasing order by bc , ties broken arbitrarily , and let bk be the bc of the k th node in this ordering . Then the set TOP(k , G ) is defined as the set of nodes with bc at least bk , and can contain more than k nodes :
TOP(k , G ) = {(w , b(w ) : v ∈ V and b(w ) ≥ bk} .
The algorithm ABRA k follows the same approach as the algorithm for the same task by Riondato and Kornaropoulos [ 24 , Sect . 5.2 ] and works in two phases . Let δ1 and δ2 be such that ( 1−δ1)(1−δ2 ) ≥ ( 1−δ ) . In the first phase , we run ABRA s with parameters ε and δ1 . Let ‘0 be the k th highest let ˜b0 = ‘0 − ε . valueeb(w ) returned by ABRA s , ties broken arbitrarily , and
In the second phase , we use a variant ABRA r of ABRA s with a modified stopping condition based on relative error versions of Thms . 1 and 3 ( Thms . 11 and 12 from Appendix D of the extended online version [ 25 ] ) , which take ε , δ2 , and λ = ˜b0 as parameters . The parameter λ plays a role in the stopping condition . Indeed , ABRA r is the same as ABRA s , with the only crucial difference in the definition of the quantity ∆Si , which is now : lnX v∈V exp s2kvk2
λ2S2 i r
+ 3
λ ln(4/δ )
2Si i
.
∆Si = 2 min s∈R+
1 s
Theorem 5 . Let eB = {eb(w ) , w ∈ V } be the output of ABRA r . Then eB is such that
( 13 )
|eb(v ) − b(v)|
Pr
∃w ∈ V : max{λ , b(v)} > ε
< δ .
The proof follows the same steps as the proof for Thm . 4 , using the above definition of ∆Si and applying Thms . 11 and 12 from Appendix D of the extended online version [ 25 ] instead of Thms . 2 and 3 .
Let ‘00 be the kth highest valueeb(w ) returned by ABRA r TOP(k , G ) = {(w,eb(w ) ) : w ∈ V andeb(w ) ≥ ˜b
( ties broken arbitrarily ) and let ˜b00 = ‘00/(1 + ε ) . ABRA k then returns the set
00} .
We have the following result showing the properties of the collection TOP(k , G ) . is such that :
Theorem 6 . With probability at least 1−δ , the set TOP(k , G )
1 . for any pair ( v , b(v ) ) ∈ TOP(k , G ) , there is one pair ( v,eb(v ) ) ∈ with highest betweenness ) and this pair is such that |eb(w)− 2 . for any pair ( w,eb(w ) ) ∈ TOP(k , G ) such that ( w , b(w ) ) 6∈ TOP(k , G ) ( ie , any false positive ) we have thateb(w ) ≤
TOP(k , G ) ( ie , we return a superset of the top k nodes b(w)| ≤ εb(w ) ;
( 1 + ε)bk ( ie , the false positives , if any , are among the nodes returned by ABRA k with lower bc estimation ) .
The proof and the pseudocode for ABRA k can be found in Appendix A of the extended online version [ 25 ] . 4.3 Special Cases
In this section we consider some special restricted settings that make computing an high quality approximation of the bc of all nodes easier . One example of such restricted settings is when the graph is undirected and every pair of distinct nodes is either connected with a single SP or there is no path between the two nodes ( because they belong to different connected components ) . Examples of these settings are many road networks , where the unique SP condition is often enforced [ 14 ] . Riondato and Kornaropoulos [ 24 , Lemma 2 ] showed that , in this case , the number of samples needed to compute a high quality approximation of the bc of all nodes is independent of any property of the graph , and only depends on the quality controlling parameters ε and δ . The algorithm by Riondato and Kornaropoulos [ 24 ] works differently from ABRA s , as it samples one SP at a time and only updates the bc estimation of nodes along this path , rather than sampling a pair of nodes and updating the estimation of all nodes on any SPs between the sampled nodes . Nevertheless , as shown in the following theorem , we can actually even generalize the result by Riondato and Kornaropoulos [ 24 ] , as shown in Thm . 7 . The statement and the proof of this theorem use pseudodimension [ 23 ] , an extension of the Vapnik Chervonenkis ( VC ) dimension to real valued functions . Details about pseudodimension and the proof of Thm . 7 can be found in Appendix B of the extended online version [ 25 ] . Corollary 1 shows how to modify ABRA s to take Thm . 7 into account .
Theorem 7 . Let G = ( V , E ) be a graph such that it is possible to partition the set D = {(u , v ) ∈ V × V , u 6= v} in two classes : a class A = {(u∗ , v∗)} containing a single pair of different nodes ( u∗ , v∗ ) such that σu∗v∗ ≤ 2 ( ie , connected by either at most two SPs or not connected ) , and a class B = D\ A of pairs ( u , v ) of nodes with σuv ≤ 1 ( ie , either connected by a single SP or not connected ) . Then the pseudodimension of the family of functions {fw : D → [ 0 , 1 ] , w ∈ V } , where fw is defined as in ( 9 ) , is at most 3 .
1150  z = 48ω∗
Let
Root 1 Root 2 Root 3 i + ( 1 + 4ε)2 √ 3| − 1 + 2ω∗ i + 2ε|p−(27(ω∗ w = −1 − 12ε + 8(27(ω∗ y = 12 θ = arg(−w + jy)/3 where j is the imaginary unity and arg(‘ ) is the argument of the complex number ‘ i )2 + ( 21 − 8ε)ε2 + 18ω∗ i ( 1 + 18ε ) ) i ( 1 + ε ) i )2 − ε2(1 + 16ε ) − ω∗ δ )((1 + 4ε ) − √
1 3 ( i + 2)(ln 2
1 6 ( i + 2)(ln 2 1 6 ( i + 2)(ln 2
δ )(2(1 + 4ε ) + δ )(2(1 + 4ε ) + z cos θ)(ω∗
√ √
√ z(cos θ + z(cos θ − √ i − ε)−2 3 sin θ))(ω∗ 3 sin θ))(ω∗ i − ε)−2 i − ε)−2
Table 1 : Roots of the cubic equation ( 12 ) for the computation of the next sample size .
Corollary 1 . Suppose to augment ABRA s with the additional stopping condition instructing to return the set ˜B =
{eb(w ) , w ∈ V } after a total of r = c ε2
3 + ln 1
δ pairs of nodes have been sampled from D . The set ˜B is an ( ε , δ) approximation .
The bound in Thm . 7 is strict , ie , there exists a graph for which the pseudodimension is exactly 3 [ 24 , Lemma 4 ] . Moreover , as soon as we relax the requirement in Thm . 7 and allow two pairs of nodes to be connected by two SPs , there are graphs with pseudodimension 4 ( Lemma 4 in Appendix B of the extended online version [ 25] ) .
For the case of directed networks , it is currently an open question whether a high quality ( ie , within ε ) approximation of the bc of all nodes can be computed from a sample whose size is independent of properties of the graph , but it is known that , even if possible , the constant would not be the same as for the undirected case [ 24 , Sect . 41 ] We conjecture that , given some information on how many pair of nodes are connected by x shortest paths , for x ≥ 0 , it should be possible to derive a strict bound on the pseudodimension associated to the graph . 4.4 Improved Estimators
Geisberger et al . [ 14 ] present an improved estimator for bc using random sampling . Their experimental results show that the quality of the approximation is significantly improved , but they do not present any theoretical analysis . Their algorithm , which follows the work of Brandes and Pich [ 10 ] differs from ours as it samples nodes and performs a Single Source Shortest Paths ( SSSP ) computation from each of the sampled nodes . We can use an adaptation of their estimator in a variant of our algorithm , and we can prove that this variant is still probabilistically guaranteed to compute an ( ε , δ) approximation of the bc of all nodes , therefore removing the main limitation of the original work , which offered no quality guarantees . We now present this variant considering , for ease of discussion , the special case of the linear scaling estimator by Geisberger et al . [ 14 ] . This technique can be extended to the generic parameterized estimators they present . The intuition behind the improved estimator is to increase the estimation of the bc for a node w proportionally to the ratio between the SP distance d(u , w ) from the first component u of the pair ( u , v ) to w and the SP distance d(u , v ) from u to v . Rather than sampling pairs of nodes , the algorithm samples triples ( u , v , d ) , where d is a direction , ( either ← or → ) , and updates the betweenness estimation differently depending on d , as follows . Let D0 = D ×{←,→} and for each w ∈ V , define the function gw from D0 to [ 0 , 1 ] as :
( σuv(w )
σuv
σuv(w )
σuv d(u,w ) d(u,v ) 1 − d(u,w ) d(u,v ) if d =→ if d =← gw(u , v , d ) =
Let S be a collection of ‘ elements of D0 sampled uniformly and independently at random with replacement . Our esti mationeb(w ) of the bc of a node w is gw(u , v , d ) = 2mS(fw ) . eb(w ) = 2
‘
X
( u,v,d)∈S
The presence of the factor 2 in the estimator calls for a single minor adjustment in the definition of ∆Si which , for this variant of ABRA s , becomes ( i + 1 ) ln 2 2Siαi(1 − αi ) +
∆Si = ω∗ 1 − αi
2 ln 2 Si r
( i + 1 )
+
δ
δ i ie , wrt the original definition of ∆Si , there is an additional factor 4 inside the square root of the third term on the rhs The output of this variant of ABRA s is still a high quality approximation of the bc of all nodes , ie , Thm . 4 still holds with this new definition of ∆Si . This is due to the fact that the results on the Rademacher averages presented in Sect . 3.2 can be extended to families of functions whose codomain is an interval [ a , b ] , rather than just [ 0 , 1 ] [ 28 ] .
5 . DYNAMIC GRAPH BC APPROXIMATION
In this section we present an algorithm , named ABRA d , that computes and keeps up to date an high quality approximation of the bc of all nodes in a fully dynamic graph , ie , in a graph where nodes and edges can be added or removed over time . Our algorithm builds on the recent work by Hayashi et al . [ 16 ] , who introduced two fast data structures called the Hypergraph Sketch and the Two Ball Index : the Hypergraph Sketch stores the bc estimations for all nodes , while the Two Ball Index is used to store the SP DAGs and to understand which parts of the Hypergraph Sketch needs to be modified after an update to the graph ( ie , an edge or node insertion or deletion ) . Hayashi et al . [ 16 ] show how to populate and update these data structures to maintain an ( ε , δ) approximation of the bc of all nodes in a fully dynamic graph . Using the novel data structures results in orders ofmagnitude speedups wrt previous contributions [ 4 , 5 ] . The algorithm by Hayashi et al . [ 16 ] is based on a static random sampling approach which is identical to the one described for ABRA s , ie , pairs of nodes are sampled and the bc estimation of the nodes along the SPs between the two nodes are updated as necessary . Their analysis on the number of samples necessary to obtain an ( ε , δ) approximation of the
1151 bc of all nodes uses the union bound , resulting in a number of samples that depends on the logarithm of the number of nodes in the graph , ie , O(ε−2(log(|V |/δ) ) ) pairs of nodes must be sampled .
ABRA d builds and improves over the algorithm presented by Hayashi et al . [ 16 ] as follows . Instead of using a static random sampling approach with a fixed sample size , we use the progressive sampling approach and the stopping condition that we use in ABRA s to understand when we sampled enough to first populate the Hypegraph Sketch and the Two Ball Index . Then , after each update to the graph , we perform the same operations as in the algorithm by Hayashi et al . [ 16 ] , with the crucial addition , after these operation have been performed , of keeping the set VS of vectors and the map M ( already used in ABRA s ) up to date , and checking whether the stopping condition is still satisfied . If it is not , additional pairs of nodes are sampled and the Hypergraph Sketch and the Two Ball Index are updated with the estimations resulting from these additional samples . The sampling of additional pairs continues until the stopping condition is satisfied , potentially according to a sample schedule either automatic , or specified by the user . As we show in Sect . 6 , the overhead of additional checks of the stopping condition is minimal . On the other hand , the use of the progressive sampling scheme based on the Rademacher averages allows us to sample much fewer pairs of nodes than in the static sampling case based on the union bound : Riondato and Kornaropoulos [ 24 ] already showed that it is possible to sample much less than O(log |V | ) nodes , and , as we show in our experiments , our sample sizes are even smaller than the ones by Riondato and Kornaropoulos [ 24 ] . The saving in the number of samples results in a huge speedup , as the running time of the algorithms are , in a first approximation , linear in the number of samples , and in a reduction in the amount of space required to store the data structures , as they now store information about fewer SP DAGs .
Theorem 8 . The set eB = {eb(w ) , w ∈ V } returned by
ABRA d after each update has been processed is such that
Pr(∃w ∈ V st |eb(w ) − b(w)| > ε ) < δ .
The proof follows from the correctness of the algorithm by Hayashi et al . [ 16 ] and of ABRA s ( Thm . 4 ) . 6 . EXPERIMENTAL EVALUATION
In this section we presents the results of our experimental evaluation . We measure and analyze the performances of ABRA s in terms of its runtime and sample size and accuracy , and compared them with those of the exact algorithm BA [ 9 ] and the approximation algorithm RK [ 24 ] , which offers the same guarantees as ABRA s ( computes an ( ε , δ)approximation the bc of all nodes ) .
Implementation and Environment . We implement ABRAs and ABRA d in C++ , as an extension of the NetworKit library [ 29 ] . The code is available from http://matteorionda to/software/ABRA radebetwtbz2 We performed the experiments on a machine with a AMD PhenomTM II X4 955 processor and 16GB of RAM , running FreeBSD 11 .
Datasets and Parameters . We use graphs of various nature ( communication , citations , P2P , and social networks ) from the SNAP repository [ 20 ] . The characteristics of the graphs are reported in the leftmost column of Table 2 .
In our experiments we varied ε in the range [ 0.005 , 0.3 ] , and we also evaluate a number of different sampling schedules ( see Sect . 62 ) In all the results we report , δ is fixed to 01 We experimented with different values for this parameter , and , as expected , it has a very limited impact on the nature of the results , given the logarithmic dependence of the sample size on δ . We performed five runs for each combination of parameters . The variance between the different runs was essentially insignificant , so we report , unless otherwise specified , the results for a random run . 6.1 Runtime and Speedup
Our main goal was to develop an algorithm that can compute an ( ε , δ) approximation of the bc of all nodes as fast as possible . Hence we evaluate the runtime and the speedup of ABRA s wrt BA and RK . The results are reported in columns 3 to 5 ( from the left ) of Table 2 ( the values for ε = 0.005 are missing for Email Enron and Cit HepPh because in these case both RK and ABRA s were slower than BA ) . As expected , the runtime is a perfect linear function of the sample size ( column 9 ) , which in turns grows as ε−2 . The speedup wrt the exact algorithm BA is significant and naturally decreases quadratically with ε . More interestingly ABRA s is always faster than RK , sometimes by a significant factor . At first , one may think that this is due to the reduction in the sample size ( column 10 ) , but a deeper analysis shows that this is only one component of the speedup , which is almost always greater than the reduction in sample size . The other component can be explained by the fact that RK must perform an expensive computation ( computing the vertex diameter [ 24 ] of the graph ) to determine the sample size before it can start sampling , while ABRA s can immediately start sampling and rely on the stopping condition , whose computation is inexpensive , as we will discuss . The different speedups for different graphs are due to different characteristics of the graphs : when the SP DAG between two nodes has many paths , ABRA s does more work per sample than RK , which only backtracks along a single SP of the DAG , hence the speedup is smaller .
Runtime breakdown . The main challenge in designing a stopping condition for progressive sampling algorithm is striking the right balance between the strictness of the condition ( ie , it should stop early ) and the efficiency in evaluating it . We now comment on the efficiency , and will report about the strictness in Sect . 6.2 and 63 In columns 6 to 8 of Table 2 we report the breakdown of the runtime into the main components . It is evident that evaluating the stopping condition amounts to an insignificant fraction of the runtime , and most of the time is spent in computing the samples ( selection of nodes , execution of SP algorithm , update of the bc estimations ) . The amount in the “ Other ” column corresponds to time spent in logging and checking invariants . We can then say that our stopping condition is extremely efficient to evaluate , and ABRA s is almost always doing “ real ” work to improve the estimation . 6.2 Sample Size and Sample Schedule
We evaluate the final sample size of ABRA s and the performances of the “ automatic ” sample schedule ( Sect . 411 ) The results are reported in columns 9 and 10 of Table 2 .
1152 Speedup wrt
Runtime Breakdown ( % )
Graph
Directed
Soc Epinions1 |V | = 75 , 879 |E| = 508 , 837
Directed
P2p Gnutella31 |V | = 62 , 586 |E| = 147 , 892
Email Enron Undirected |V | = 36 , 682 |E| = 183 , 831
Cit HepPh Undirected |V | = 34 , 546 |E| = 421 , 578
ε
0.005 0.010 0.015 0.020 0.025 0.030 0.005 0.010 0.015 0.020 0.025 0.030 0.010 0.015 0.020 0.025 0.030 0.010 0.015 0.020 0.025 0.030
Runtime
( sec . ) 483.06 124.60 57.16 32.90 21.88 16.05 100.06 26.05 11.91 7.11 4.84 3.41 202.43 91.36 53.50 31.99 24.06 215.98 98.27 58.38 37.79 27.13
BA 1.36 5.28 11.50 19.98 30.05 40.95 1.78 6.85 14.98 25.09 36.85 52.38 1.18 2.63 4.48 7.50 9.97 2.36 5.19 8.74 13.50 18.80
RK 2.90 3.31 4.04 5.07 6.27 7.52 4.27 4.13 4.03 3.87 3.62 3.66 1.10 1.09 1.05 1.11 1.03 2.21 2.16 2.05 2.02 1.95
Sampling
99.983 99.956 99.927 99.895 99.862 99.827 99.949 99.861 99.772 99.688 99.607 99.495 99.984 99.970 99.955 99.932 99.918 99.966 99.938 99.914 99.891 99.869
Stop Cond . Other 0.002 0.014 0.009 0.035 0.018 0.054 0.074 0.031 0.046 0.092 0.062 0.111 0.010 0.041 0.036 0.103 0.074 0.154 0.191 0.121 0.174 0.220 0.243 0.262 0.003 0.013 0.006 0.024 0.010 0.035 0.052 0.016 0.021 0.061 0.004 0.030 0.008 0.054 0.013 0.073 0.091 0.018 0.023 0.108
Sample
Size
110,705 28,601 13,114 7,614 5,034 3,668 81,507 21,315 9,975 5,840 3,905 2,810 66,882 30,236 17,676 10,589 7,923 32,469 14,747 8,760 5,672 4,076
Reduction wrt RK 2.64 2.55 2.47 2.40 2.32 2.21 4.07 3.90 3.70 3.55 3.40 3.28 1.09 1.07 1.03 1.10 1.02 2.25 2.20 2.08 2.06 1.99
Absolute Error ( ×105 ) max 70.84 129.60 198.90 303.86 223.63 382.24 38.43 65.76 109.10 130.33 171.93 236.36 145.51 253.06 290.30 548.22 477.32 129.08 226.18 246.14 289.21 359.45 avg 0.35 0.69 0.97 1.22 1.41 1.58 0.58 1.15 1.63 2.15 2.52 2.86 0.48 0.71 0.93 1.21 1.38 1.72 2.49 3.17 3.89 4.45 stddev 1.14 2.22 3.17 4.31 5.24 6.37 1.60 3.13 4.51 6.12 7.43 8.70 2.46 3.62 4.83 6.48 7.34 3.40 5.00 6.39 7.97 9.53
Table 2 : Runtime , speedup , breakdown of runtime , sample size , reduction , and absolute error
As expected , the sample size grows with ε−2 . We already commented on the fact that ABRA s uses a sample size that is consistently ( up to 4× ) smaller than the one used by RK and how this is part of the reason why ABRA s is much faster than RK . In Fig 1 we show the behavior ( on P2p Gnutella31 , figures for other graphs can be found in Appendix C of the extended online version [ 25 ] ) of the final sample size chosen by the automatic sample schedule in comparison with static geometric sample schedules , ie , schedules for which the sample size at iteration i+1 is c times the size of the sample size at iteration i . We can see that the automatic sample schedule is always better than the geometric ones , sometimes significantly depending on the value of c ( eg , more than 2× decrease wrt using c = 3 for ε = 005 ) Effectively this means that the automatic sample schedule really frees the user from having to selecting a parameter whose impact on the performances of the algorithm may be devastating ( larger final sample size implies higher runtime ) . Moreover , thanks to the automatic sample schedule , ABRAs always terminates after just two iterations , while this was not the case for the geometric sample schedules ( taking even 5 iterations in some cases ) : this means that effectively the automatic sample schedules “ jumps ” directly to a sample size for which the stopping condition will be verified . We can sum up the results and say that the stopping condition of ABRA s stops at small sample sizes , smaller than those used in RK , and the automatic sample schedule we designed is extremely efficient at choosing the right successive sample size .
6.3 Accuracy
We evaluate the accuracy of ABRA s by measuring the absolute error |eb(v ) − b(v)| . The theoretical analysis guar antees that this quantity should be at most ε for all nodes , with probability at least 1 − δ . A first important result is that in all the thousands of runs of ABRA s , the maximum
Figure 1 : Final sample size for different sample schedules on P2p Gnutella error was always smaller than ε ( not just with probability > 1−δ ) . We report statistics about the absolute error in the three rightmost columns of Table 2 and in Fig 2 ( figures for the other graphs are in Appendix C of the extended online version [ 25 ] . The minimum error ( not reported ) was always 0 . The maximum error is an order of magnitude smaller than ε , and the average error is around three orders of magnitude smaller than ε , with a very small standard deviation . As expected , the error grows as ε−2 . In Fig 2 we show the behavior of the maximum , average , and average plus three standard deviations ( approximately corresponding to the 95 % percentile ) for Soc Epinions1 ( the vertical axis has a logarithmic scale ) , to appreciate how most of the errors are almost two orders of magnitude smaller than ε .
All these results show that ABRA s is very accurate , more than what is guaranteed by the theoretical analysis . This can be explained by the fact that the bounds to the sampling size , the stopping condition , and the sample schedule are conservative , in the sense that ABRA s may be sampling more than necessary to obtain an ( ε , δ) approximation . Tightening any of these components would result in a less
000050010015002002500300E+0040E+0480E+0412E+0516E+0520E+05autoc=12c=15c=2c=3epsilonSample Size1153 [ 3 ] D . A . Bader , S . Kintali , K . Madduri , and M . Mihail . Approximating betweenness centrality . Algorithms and Models for the Web Graph , 124–137 , 2007 .
[ 4 ] E . Bergamini and H . Meyerhenke . Fully dynamic approximation of betweenness centrality . ESA’15 . [ 5 ] E . Bergamini and H . Meyerhenke . Approximating betweenness centrality in fully dynamic networks . Internet Mathematics , to appear .
[ 6 ] E . Bergamini , H . Meyerhenke , and C . L . Staudt .
Approximating betweenness centrality in large evolving networks . ALENEX’15 , 2015 .
[ 7 ] S . Boucheron , O . Bousquet , and G . Lugosi . Theory of classification : A survey of some recent advances . ESAIM : Probability and Statistics , 9:323–375 , 2005 .
[ 8 ] S . Boyd and L . Vandenberghe . Convex optimization .
Cambridge university press , 2004 .
[ 9 ] U . Brandes . A faster algorithm for betweenness centrality . J .
Math . Sociol . , 25(2):163–177 , 2001 .
[ 10 ] U . Brandes and C . Pich . Centrality estimation in large networks . Int . J . Bifurc . Chaos , 17(7):2303–2318 , 2007 .
[ 11 ] T . Elomaa and M . Kääriäinen . Progressive Rademacher sampling . AAAI’2001 , 2001 .
[ 12 ] D . Erdős , V . Ishakian , A . Bestavros , and E . Terzi . A divide and conquer algorithm for betweenness centrality . SDM’15 , 2015 .
[ 13 ] L . C . Freeman . A set of measures of centrality based on betweenness . Sociometry , 40:35–41 , 1977 .
[ 14 ] R . Geisberger , P . Sanders , and D . Schultes . Better approximation of betweenness centrality . ALENEX’08 , 2008 . [ 15 ] O . Green , R . McColl , and D . Bader . A fast algorithm for streaming betweenness centrality . PASSAT’12 , 2012 .
[ 16 ] T . Hayashi , T . Akiba , and Y . Yoshida . Fully dynamic betweenness centrality maintenance on massive networks . VLDB’16 , 2015 .
[ 17 ] M . Kas , M . Wachs , K . M . Carley , and L . R . Carley .
Incremental algorithm for updating betweenness centrality in dynamically growing networks . ASONAM’13 , 2013 .
[ 18 ] N . Kourtellis , G . D . F . Morales , and F . Bonchi . Scalable online betweenness centrality in evolving graphs . IEEE Trans . Knowl . Data Eng . , 27(9):2494–2506 , 2015 .
[ 19 ] M J Lee , J . Lee , J . Y . Park , R . H . Choi , and C W
Chung . QUBE : A quick algorithm for updating betweenness centrality . WWW’12 , 2012 .
[ 20 ] J . Leskovec and A . Krevl . SNAP Datasets http://snapstanfordedu/data , 2014 .
[ 21 ] M . E . J . Newman . Networks – An Introduction . Oxford
Springer Verlag , 1999 .
University Press , 2010 .
[ 22 ] L . Oneto , A . Ghio , D . Anguita , and S . Ridella . An improved analysis of the Rademacher data dependent bound using its self bounding property . Neur . Netw . , 44:0 , 2013 .
[ 23 ] D . Pollard . Convergence of stochastic processes .
Springer Verlag , 1984 .
[ 24 ] M . Riondato and E . M . Kornaropoulos . Fast approximation of betweenness centrality through sampling . Data Mining Knowl . Disc . , 30:(2 ) 438–475 , 2015 .
[ 25 ] M . Riondato and E . Upfal . Approximating betweenness centrality in static and dynamic graphs with Rademacher averages ( extended version ) http://riondato/papers/RiondatoUpfal ABRA extpdf , 2016 .
[ 26 ] M . Riondato and E . Upfal . Mining frequent itemsets through progressive sampling with Rademacher averages . KDD’15 , 2015 .
[ 27 ] A . E . Sarıyüce , E . Saule , K . Kaya , and U . V . Çatalyürek .
Shattering and compressing networks for betweenness centrality . SDM’15 , 2015 .
[ 28 ] S . Shalev Shwartz and S . Ben David . Understanding
Machine Learning : From Theory to Algorithms . Cambridge University Press , 2014 .
[ 29 ] C . Staudt , A . Sazonovs , and H . Meyerhenke . NetworKit : A tool suite for high performance network analysis . Network Science , to appear .
[ 30 ] V . N . Vapnik . The Nature of Statistical Learning Theory .
Figure 2 : Absolute error evaluation – Soc Epinions1 conservative algorithm that offers the same approximation quality guarantees , and is an interesting research direction . 6.4 Dynamic BC Approximation
We did not evaluate ABRA d experimentally , but , given its design , it is reasonable to expect that , when compared to previous contributions offering the same quality guarantees [ 5 , 16 ] , it would exhibit similar or even larger speedups and reductions in the sample size than what ABRA s had wrt RK . Indeed , the algorithm by Bergamini and Meyerhenke [ 4 ] uses RK as a building block and it needs to constantly keep track of ( an upper bound on ) the vertex diameter of the graph , a very expensive operation . On the other hand , the analysis of the sample size by Hayashi et al . [ 16 ] uses very loose simultaneous deviation bounds ( the union bound ) . As already shown by Riondato and Kornaropoulos [ 24 ] , the resulting sample size is extremely large and they already showed how RK can use a smaller sample size . Since we built over the work by Hayashi et al . [ 16 ] and ABRA s improves over RK , we can reasonably expect it to have better performances than the algorithm by Hayashi et al . [ 16 ]
7 . CONCLUSIONS
We presented ABRA , a family of sampling based algorithms for computing and maintaining high quality approximations of ( variants of ) the bc of all nodes in static and dynamic graphs with updates ( both deletions and insertions ) . We discussed a number of variants of our basic algorithms , including finding the top k nodes with higher bc , using improved estimators , and special cases when there is a single SP . ABRA greatly improves , theoretically and experimentally , the current state of the art . The analysis relies on Rademacher averages and on pseudodimension . To our knowledge this is the first application of these concepts to graph mining . In the future we plan to investigate stronger bounds to the Rademacher averages , give stricter bounds to the sample complexity of bc by studying the pseudodimension of the class of functions associated to it , and extend our study to other network measures . Acknowledgements . This work was supported in part by NSF grant IIS 1247581 and NIH grant R01 CA180776 .
8 . REFERENCES [ 1 ] D . Anguita , A . Ghio , L . Oneto , and S . Ridella . A deep connection between the Vapnik Chervonenkis entropy and the Rademacher complexity . IEEE Trans . Neural Netw . Learn . Sys . , 25(12):2202–2211 , 2014 .
[ 2 ] J . M . Anthonisse . The rush in a directed graph . TR BN
9/71 , Stichting Mathematisch Centrum , 1971 .
00005001001500200250031E 061E 051E 041E 031E 02maxavg+3stddevavgepsilonabsolute error1154
