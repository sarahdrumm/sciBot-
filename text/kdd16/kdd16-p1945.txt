Online Feature Selection : A Limited Memory Substitution
Algorithm and Its Asynchronous Parallel Variation
Haichuan Yang
Department of Computer
Science
University of Rochester Rochester , NY 14627 hyang@rochesteredu
Ryohei Fujimaki
NEC
Cupertino , CA 95014 rfujimaki@nec labs.com
Yukitaka Kusumura
NEC
Cupertino , CA 95014 ykusumura@nec labs.com
Ji Liu
Science
Department of Computer
University of Rochester Rochester , NY 14627 jiliuuwsic@gmailcom
ABSTRACT This paper considers the feature selection scenario where only a few features are accessible at any time point . For example , features are generated sequentially and visible one by one . Therefore , one has to make an online decision to identify key features after all features are only scanned once or twice . The optimization based approach is a powerful tool for the online feature selection .
However , most existing optimization based algorithms explicitly or implicitly adopt L1 norm regularization to identify important features , and suffer two main disadvantages : 1 ) the penalty term for L1 norm term is hard to choose ; and 2 ) the memory usage is hard to control or predict . To overcome these two drawbacks , this paper proposes a limited memory and model parameter free online feature selection algorithm , namely online substitution ( OS ) algorithm . To improve the selection efficiency , an asynchronous parallel extension for OS ( Asy OS ) is proposed . Convergence guarantees are provided for both algorithms . Empirical study suggests that the performance of OS and Asy OS is comparable to the benchmark algorithm Grafting , but requires much less memory cost and can be easily extended to the parallel implementation .
Keywords Feature Selection ; Online Learning ; Asynchronous Parallel Optimization
1 .
INTRODUCTION
Feature selection plays a key role in many learning and mining tasks [ 12 ] . Substantial research efforts have focused on the batch selection , where all features are assumed to be accessible at any time point and can be accessed for arbitrarily many times if needed eg works in [ 19 , 3 , 25 , 6 ] . However , the batch selection may meet the bottleneck in terms of computation and memory for the big data application , while the ultimately wanted features only occupy a tiny space . Some works [ 18 , 28 ] can handle large number of features , but all the features are assumed to be accessible in the selection process .
Online feature selection [ 14 , 32 ] relaxes the requirement in the batch selection and makes a series of decisions to identify key features , to fit important applications where features cannot be accessed at one time , for example , • features are too many to store locally ; • features can only be queried remotely and one needs to identify important features after scanning all the features one time immediately . There are usually two types of online feature selection algorithms : statistical algorithms [ 32 , 23 , 33 , 22 ] and optimization based algorithms ( or embedded methods ) [ 14 , 34 ] , which have different application scenarios . The statistical algorithms usually do not have a given objective and features are selected based on certain statistical quantity , eg mutual information or correlation . Thus , selected features can be used for many different tasks , but are usually sub optimal for any specific task ( eg a given objective function ) .
This paper mainly focuses on the optimization based approaches , which are target oriented with a clear objective ( eg loss function ) in feature selection , for example , minimizing square loss or classification error . The key difference to statistical methods is that optimization based methods give different selection result for different objective .
Most existing optimization based algorithms such as Grafting [ 14 ] or its variation [ 34 ] essentially solve an L1 norm minimization problem similar to batch methods , for example , LASSO [ 19 ] and L1 logistic regression [ 12 ] . There are two main drawbacks in practice : 1 ) As pointed out in [ 21 ] , it is hard to choose the model parameter ( that is , L1 norm penalty ) for online methods . The hyper parameter in batch methods is usually decided by cross validation ( CV ) . How
1945 ever , the CV strategy is unavailable for the online feature selection scenario , since we can only see a few features . 2 ) The L1 norm based online method cannot strictly control the sparsity of the solution path . In the worst case , the required space is comparable to the batch methods .
To overcome these two practical disadvantages , this paper proposes an online feature selection algorithm , namely online substitution ( OS ) algorithm . OS essentially aims at solving an L0 norm constraint problem : st w0 ≤ s . w min
L(Xw , y )
( 1 ) where X ∈ Rn×p is a feature matrix of n samples with p features , y ∈ Rn is the label vector , and s is the total number of features we want to select . Comparing to Grafting types approaches , it has the following advantages : • ( Limited memory ) The memory usage is strictly controlled up to O(ns ) . The memory restriction has a very special meaning for big data applications . Imagine the following scenario : Given p = 2M features and n = 0.5M samples , one wants to select top s = 100 important features . Using the batch method , one needs terabytes of space to save it and solve an optimization to identify top 100 key features using the batch selection . Typically , this work needs to involve parallel computation to solve the batch problem ( eg , L1 norm minimization ) . If one can reduce the memory cost to s × n ≤ 1GB to obtain top 100 features , the whole work can be done on a single PC . • ( Model parameter free ) One only needs to specify the total number of desired features s ( that is much easier to decide the L1 norm penalty ) , thus it avoids the dilemma of setting hyper parameters ( depending all features ) . To improve the computational efficiency in big data application , we propose an asynchronous parallel variation for OS ( Asy OS ) . Asynchronous parallelism is proven to be more efficient than synchronous parallelism in solving many large scale problems in deep learning , sparse learning , etc . It is mainly because that asynchronous parallelism substantially reduces the communication and coordination cost which cannot be avoided in synchronous parallelism . This paper applies this efficient scheme to parallelize the proposed OS algorithm . Besides of improving the computational efficiency , the memory cost can be shared by multiple machines in AsyOS , which is able to tackle even larger scale applications .
The contribution of our work can be summarized as follows : • We propose a limited memory and model parameter free online feature selection algorithm , which avoids disadvantages of some benchmark algorithms and can be easily extended to the parallel implementation ; • We propose a novel asynchronous parallel algorithm to improve the efficiency of the proposed OS algorithm , while only slightly sacrificing the accuracy . To the best of our knowledge , this is the first parallel algorithm for online feature selection . • Theoretical convergence guarantees are provided for both algorithms .
Notations and Definitions Throughout this paper , we use the following notations and definitions . • n is the number of samples . • p is the number of features . • y(i ) is the label for the i th sample .
[ x(1),··· , x(n) ] .
• x(i ) ∈ Rp is a column vector representing i th sample . • xj ∈ Rn is a column vector representing the j th feature . • X ∈ Rn×p is the data matrix : X := [ x1,··· , xp ] or • S ⊂ {1,··· , p} is the index set of features , and S denotes • XS is the matrix containing features in set S . • w is the p dimensional vector containing coefficients of all • wj ∈ Rp is the coefficient of the j th feature . • wS is the coefficients of the features in set S . • wk is the coefficient vector in the k th iteration . its complementary set . the features .
2 . RELATED WORK
Our work relates to topics including batch feature selection , streaming feature selection , and asynchronous parallel optimization . In this section , we will introduce methods on all the areas and emphasize the ones which are closely related to ours . 2.1 Batch Feature Selection
Batch feature selection is the prototype used in most traditional feature selection methods . The entire set of features is given before doing selection . Most batch feature selection methods can be classified as statistical methods or optimization methods . Statistical methods select features according to certain criteria which are usually built based on statistical rules . For example , method based on mutual information [ 13 ] , selecting features based on relevance and redundancy [ 24 ] , and method based on dependency [ 16 ] .
Optimization based feature selection methods are also called embedded methods in some literature . Its basic idea is doing feature selection and learning the model concurrently . These methods usually have a linear model w , and require it to be sparse , ie w0 ≤ s . Different feature selection methods handle the sparsity requirement differently . For instance , L1 regularization based methods [ 19 , 12 , 3 ] relax the hard constraint to L1 penalty , and transform the problem to be convex . Besides using an overall sparse regularization , there are many works [ 25 , 2 , 6 ] proposed for structured sparsity . Unsupervised objective is also used in some works [ 8 , 9 ] . However , we cannot directly control the number of selected features by using the L1 regularized methods . Different with the original L0 constraint , they use a regularization parameter λ to control the sparsity , but there is no explicit relation between s and λ , which means we need to tune λ if we want to get a reasonable result , ie will not select too much or too little features .
An alternative way to handle the sparsity requirement is optimizing the problem just with hard L0 constraint [ 5 , 26 ] . Although this leads to an NP hard problem and we may never get the global optimal solution , there is still theoretical guarantee for the error bound [ 26 ] . The basic approach of these methods is projected gradient descent . Thanks to the simplicity of projection onto L0 ball , their efficiency is somehow satisfactory . 2.2 Streaming Feature Selection
The basic problem setting of streaming feature selection is we only have access to a small part of features , eg , the input feature . Previously input features are allowed to be retained , but we also should consider the limit on memory . This obstacle is an issue when feature selection method considers the
1946 relation between different features . Many streaming feature selection methods can be seen as extension of some batch feature selection approach , so they also belong to statistical methods or optimization methods . For example , Zhou etal [ 32 ] proposed the alpha investing criterion to select features . OSFS [ 22 ] dynamically selects the features based on the online analysis and the online redundancy analysis . Both of their criteria are based on statistical quantities .
In this paper , we focus on optimization based streaming feature selection methods . Grafting method [ 14 ] is built based on the L1 regularized formulation , and it can be used in any problem with L1 regularization theoretically . It mainly select features at two time points . When the feature just come , grafting test it with a simple criterion and reject it if it fails . When the feature pass the test , grafting will include it into the other selected features and solve a small scale batch sparse learning problem , finally only the features with nonzero coefficient can be retained . Since grafting is actually the online version of L1 regularization method , it has all the problems of L1 based method . What ’s worse , tuning λ seems harder since we even cannot access all the features , and we do not know how much space we need in the process of feature selection . Besides that , it lacks theoretical analysis about convergence rate and error bound with respect to the number of iteration . It also has another practical problem that it takes too much time if the feature pass the test , since a full optimization need to be conduct . This defect has been pointed out and improved by grafting light [ 34 ] . 2.3 Asynchronous Parallel Optimization
Asynchronous parallelism is a new parallel mechanism to speedup the optimization efficiency and received broad attention recently . It avoids the coordination and synchronization cost comparing the traditional synchronous parallelism and received remarkable successes in optimization and machine learning for solving deep learning [ 7 , 30 , 31 ] , SVM [ 11 ] , linear programming [ 17 ] , linear equations [ 1 ] , LASSO [ 10 ] , matrix completion [ 15 ] , and many others [ 29 ] . 3 . ALGORITHM
The online feature selection problem considered in this paper can be formally defined in the following : while features come one by one , given a memory budget and a target ( eg , sample labels or measurements on samples ) , we need to decide what features to retain .
The loss function L in ( 1 ) can be in many different forms . For linear regression , the most common L is in the squared loss form
Lr :=
Xw − y2 .
1 2n
( 2 ) n i=1
Lc :=
1 2n
For classification task , it could be the squared hinge loss max(0 , 1 − y(i)w x(i))2
( 3 ) where y(i ) ∈ {−1 , 1} . L could also be logistic regression loss , SVM regression loss , etc . In this section , we do not restrict the form of L .
There are several methods [ 5 , 26 ] that work on analyzing and solving problem ( 1 ) . The main idea of their algorithm is projected gradient descent ( Proj GD ) , which makes a normal gradient descent step and projects the current point onto the L0 ball . Yuan etal [ 26 ] derive the convergence rate and error bound for general convex and smooth loss function . In a word , Proj GD works very well on the L0 constrained problem .
However , in order to use Proj GD to do feature selection , we must have the access to all the features . For streaming feature selection , grafting [ 14 ] and grafting light [ 34 ] uses the stage wise strategy to update the model w . Since they are all based on the L1 regularized formulation , it is easy to guarantee descent on objective value , and the global optimum can be attained if we can scan all the features again and again . But L0 constraint based formulation is another story , even batch method Proj GD can only get the approximate solution [ 26 ] . Furthermore , how can we guarantee descent and convergence is not clear . 3.1 Online Substitution Algorithm
The basic reason that we cannot do Proj GD for streaming feature selection is that we can nether get the gradient , nor update the coefficients for the unseen feature . We are only allowed to access couple of features at the same time , which are actually the new coming feature , and some features temporarily retained . Since new features keep coming , and we cannot retain all of them , this means we need to make decision on rejecting or accepting features in this process .
We propose a method which is motivated by Proj GD . We maintain a set S containing features temporarily retained . The maximum size of S is s . When S is not full , new features are always accepted . If the set S is full , we have to reject an existing feature in S if we decide to accept a new feature , that is , the new feature will substitute the old feature . We call this process “ online substitution ” . The criteria for substitution is comparing the potential of the new feature with the worst features in S . At iteration k , if the coming feature has index j , then the procedure is :
η
1 . Update coefficients of the retained features with step length m : wS = wS − η feature with step length η : wj = −η∇jL ; m∇SL , and update coefficient of the new
2 . Project w onto Ω(s ) : w = PΩ(s ) ( w ) . where m ≥ 1 is a parameter , PΩ(s)(w ) means projecting w onto the set Ω(s ) := {w|w0 ≤ s} , that is , retain top k largest ( in the sense of magnitude ) elements in w and set the rest to zero . In section 4.2 , We will see that the parameter m is useful to the convergence guarantee of our asynchronous parallel method . Since w in step 2 has at most s + 1 nonzero elements , so the projection step is just setting the minimum ( in magnitude ) nonzero element as zero . The pseudo code is shown in Algorithm 1 . From the algorithm we can see that the partial gradient ∇SL is written as X ∂u can be formed as a function of u , for least square loss ( 2 ) , it is :
∂u , where u := Xw . Actually ∂L
∂L
S
∂L ∂u
=
1 n
( u − y )
( 4 ) and for the squared hinge loss ( 3 ) , it is
∂L ∂u
= − 1 n max(0 , 1 − u ◦ y ) ◦ y where ◦ means element wise multiplication . 3.2 Asynchronous Online Substitution Algo
( 5 ) rithm
Because we only make one partial gradient step ( WRT S ∪ {j} ) , our method OS is much efficient for handling each
1947 Algorithm 1 : Online Substitution ( OS ) algorithm .
Algorithm 3 : Asy OS : procedure of worker t .
Data : Label y . Result : Set S consisting of selected features .
Data : Label y . Result : Set S(t ) consisting of selected features .
1 set S(t ) = ∅ ; 2 repeat 3
1 set S = ∅ ; 2 repeat 3
4
5
6
7
8
9
Receive a feature xj from the pool S with index j ; u := XSwS ; wS = wS − ( η/m)X wj = −ηx ∂L ∂u ; update S = S ∪ {j} ; if |S| > s then
∂L ∂u ;
S j wk∗ = 0 , S = S \ {k∗} , where k∗ = arg mink∈S |wk| end
10 11 until Reach convergence ; coming feature when compare with grafting . However , if the new features are input too frequently , the online substitution speed may not catch up its speed . One promising way to improve the efficiency is using parallel optimization . We extend our method OS to an asynchronous method .
The asynchronous parallelism implementation has a similar procedure with Algorithm 1 , but here we have multiple workers selecting features . Each worker t uses set S(t ) with size s/q , where q is the number of workers . In addition , computing the gradient needs information from other workers , we can use a central machine to collect all updates in different workers as in Algorithm 2 . The procedure for the central machine is very simple : it just receives all the ∆u(t ) sent by each worker t , and add it to the central state variable uC , which actually represents Xw .
The procedure for workers is shown in Algorithm 3 . Firstly , pull the central state variable uC from the central machine , then save the current local state variable u(t ) . Secondly , use uC to compute the partial gradient WRT wS(t ) and wj , and perform update according to the gradient and projection . At last , calculate the difference variable ∆u(t ) and send it to the central machine . In the whole procedure of central machine and workers , there is no synchronization process .
Algorithm 2 : Asy OS : procedure of the central machine . 1 set uC = 0 ; 2 repeat 3 if Receive ∆u(t ) from a certain worker t . then
4 uC = uC + ∆u(t ) ; end
5 6 until Workers stop pushing ∆u(t ) ;
4 . THEORETICAL ANALYSIS
In this section , we show the main result of our theoretical analysis , including convergence of OS and Asy OS algorithm , and the property of their local minimum . Proofs are provided in the Appendix A .
Firstly , we make some certain Lipschitzian assumptions to prepare the following theoretical analysis . Lipschitzian assumptions are commonly used in analyzing optimization algorithms . Define function f ( w ) as f ( w ) := L(Xw , y ) and
4
5
6
7
8
9
10
11
12
Receive a feature xj from the pool S with index j ; Read uC from the central machine ; u(t ) = XS(t)wS(t ) ; wS(t ) = wS(t ) − ( η/m)X wj = −ηx update S(t ) = S(t ) ∪ {j} ; if |S(t)| > s/q then
∂L ∂uC
∂L ∂uC
S(t )
;
; j wk∗ = 0 , S(t ) = S(t ) \ {k∗} , where k∗ = arg mink∈S(t ) |wk| end Push ∆u(t ) = XS(t)wS(t ) − u(t ) to the central machine ;
13 until Reach convergence ; construct function Fw(t , r ) by :
Fwk ( t , r ) := f ( wk + i∈S
1 m tiei + rej ) where t ∈ R|S| , r ∈ R . Assume we have :
Assumption 1 . ( Lipschitz Gradient . ) There exists con stant LF < +∞ which satisfies : ∀t ∈ R|S| and r ∈ R
Fwk ( t , r ) − Fwk ( 0 , 0 ) ≤∇SFwk ( 0 , 0 ) , t + r∇jFwk ( 0 , 0 ) +
LF 2
( t2/m2 + r2 ) . ( 6 )
There exists a constant Lf < +∞ satisfying : ∀u , v ∈ Rp
∇f ( u ) − ∇f ( v ) ≤ Lfu − v .
( 7 )
4.1 Convergence of the OS Algorithm
Then we are ready to present the convergence rate for the proposed OS algorithm ( Algorithm 1 ) with the following Theorem :
Theorem 1 . For K iterations , the average distance of w between two successive iterations in the OS algorithm satisfies wk+1 − wk2 ≤ 2,f ( w1 ) − f ( wK+1 )
K
( 8 )
K(1 − ηLF )η
1
Kη2 k=1 when η < 1/LF establishes . The left hand side of ( 8 ) is nothing but the ∇f ( wk)2 if using gradient descent to solve an unconstrained optimization . Theorem 1 basically suggests that the sequence wk+1 − wk2 converges and its average rate is O(1/k ) . It is also worth to point out that the optimal choice for η is 0.5/LF . 4.2 Convergence of Asy OS Algorithm
Next we establish the convergence of the proposed AsyOS algorithm . For the asynchronous version in Algorithm 2 and 3 , the convergence is guaranteed under some conditions . We assume that we have bounded staleness , ie
1948 K k=1
1
Kη2
Ewk+1 − wk2 ≤ f ( w1 ) − Ef ( wK+1 )
Kη2∆
L2
1
1
2m2q
+
2(p − s )
τ 2 max
> 0 . f LF
∆ :=
− LF −
1 2η
Assumption 2 . ( Bounded staleness . ) In one update iteration of any worker , the central state variable uC will not update more than τmax times . Thus uC updates at most τmax times between the each pair of line 4 and line 12 in Algorithm 3 .
Formally , we have the following theorem :
Theorem 2 . The average distance of two successive iterations the Asy OS algorithm with q workers converges in the following sense : if the step length η is appropriately chosen such that
Note that w consists of all disjoint pieces from q workers and the iteration counts the change happening to w in any single worker . Form the above theorem we can see the relation between the step length η and the staleness bound τmax . Larger τmax requires that η to be smaller . 4.3 Local Optimality
Assume that we have the following Restricted Strong
Convexity assumption :
Assumption 3 . There exists ρ−(s ) > 0 which satisfies : f ( ¯w ) − f ( w ) ≥ ∇f ( w ) , ¯w − w + ∀ w ∈ Ω(s ) , ¯w ∈ Ω(s )
¯w − w2
ρ−(s )
2
Then our local optimum has the property
Theorem 3 . If w∞ is the local optimal point that we get by our method . Then
∞
) − f ( w
∗ f ( w
( f ( 0 ) − f ( w
∗
) )
) ≤ α
1 + α
2 where α =
1
( ρ−(s))η where w∗ is the global optimal point . Theorem 3 shows that our local minimum is better with smaller value of α , which also has a relationship with step length η . Larger step length η leads to better local optimum . However , as shown in Theorem 1 and 2 , there is an upper bound for η to guarantee convergence .
5 . EXPERIMENT
In this section , we demonstrate the empirical performance and the effects of some parameters . The experiments are conducted on two different models : linear regression and hinge loss classification .
Linear regression . Feature selection in linear regression aims at recovering the sparse vector w∗ ∈ Rp , given the training data X ∈ Rn×p and the corresponding observations y = Xw∗ + , where ∈ Rn is the noise . In our experiment , the training data X are generated from standard iid Gaussian distribution . We force the ground truth vector w∗ to be sparse , ie w∗0 ≤ s . The nonzero positions of w∗ are randomly selected and their values follow the standard iid Gaussian distribution . Noise is generated from iid Gaussian distribution with mean 0 and variance 012 We adopt the cost function ( 2 ) for linear regression .
Hinge loss classification . Besides regression , we also try our method for classification model . The data matrix X and the sparse ground truth vector w∗ are generated exactly the same way as in linear regression settings . Class labels y ∈ {−1 , +1}n are generated by taking the sign of Xw∗ . Another data matrix Xtest ∈ Rn×p and vector ytest are generated exactly the same way as X and y . We use them as the test data . We adopt the squared hinge loss function ( 3 ) . In addition , we also test our method on the privacy image classification problem [ 27 ] . Its target is to distinguish images which could contain some privacy information from the public images . In the experiment , we use a collected data set in [ 20 ] with roughly 3400 images in both classes ie privacy and public . We combine them as a dataset with 6914 images , and randomly select one percent data as training set , leave the rest as testing set . Each image is represented by 7488 features generated by several image feature extraction methods including color histogram , linear binary pattern , histogram of oriented gradients , etc .
Baseline Methods . To demonstrate our online feature selection method , we compare it with L0 constrained batch method ( Proj GD ) , L1 regularized batch method ie LASSO and L1 hinge loss classification , grafting method , and two naive online implementations which are based on coordinate descent method and projected or proximal operation . We adopt proximal gradient descent to solve LASSO , and use the implementation of LIBLINEAR [ 4 ] to solve L1 hinge loss classification .
Performance Measure . To compare the performance , we use feature selection recall , estimation error and classification error . Feature selection recall is the ratio that correctly selected features ( at most s ) over the total amount of used features in the true model . For linear regression , we show the recovery error which is the normalized distance between the recovered model and the true model . For classification problem , we evaluate the classification error . For all the methods , we use the selected features to fit the training data again .
To demonstrate the efficiency improvement of our parallel method Asy OS , we show the running time and training error ( ie fitting error for linear regression and classification error for hinge loss classification ) . We also show the speedup compared with the sequential version .
Implementation Details . In our experiment , we focus on methods based on two basic sparsity formulations : L0 constraint and L1 regularization . For L0 methods including our method OS , the objective function is just equation ( 1 ) . The L1 regularized formulation is minw L + λw1 . In this formulation , L is the specific loss function , ie Lr in ( 2 ) or Lc in ( 3 ) . L1 regularization parameter λ is set to guarantee the batch methods selecting at least s features . In our synthetic data experiments , the total number of features p is set to 2000 ∼ 6000 for linear regression and 1000 ∼ 5000 for classification . The number of nonzero elements is set s = 100 both . The number of samples n is set to 1.2s log2 p . For all the online feature selection method , we make the limitation that we can only scan all the features twice . The results are averaged on 30 repeated experiments with different random data .
1949 5.1 Performance Comparison
6 . CONCLUSIONS
In the first column of Figures 1 and 2 , we show the performance comparison on linear regression and classification problems respectively . We can see that for both problems , L0 based methods ie Proj GD and our method OS , have higher feature selection recall . For linear regression , our method OS has almost the same performance with the L0 batch counterpart Proj GD . But we can find that the estimation error and the feature selection recall are not very consistent . The L1 methods also have low recovery error , although they do not have the highest feature selection recall . In the first column of Figure 4 , we can find that the L1 batch method has higher error when s is small , and other methods are not influenced too much .
5.2 Sensitivity about Scanning Features
One important issue in practice is how many times of scanning features is sufficient . To demonstrate this , we test our method by scanning features for different number of times , and compare the performance in the second column of Figures 1 , 2 and 4 , which suggests that 2 or 3 times would be enough since the error does not clearly improve after that .
5.3 Asynchronously Parallel Algorithm
In this section , we study the performance of Asy OS . We first simulate Asy OS on a single machine , such that the staleness value τmax can be easily controlled . We set τmax to be the number of workers , that is , each worker uses the information from other workers “ #Workers ” iterations iterates ago . From the right column of Figures 1 ( for linear regression ) , 2 ( for classification ) and 4 , we can observe that the recall almost monotonically degrade while the number of workers increases . One interesting finding is that the classification problem seems to be more robust to the staleness ( or #Workers ) .
Next we implement Asy OS on a computer network consisting of 6 machines . We use Message Passing Interface ( MPI ) to implement the parallel mechanism . The central node ( ie Algorithm 2 ) runs on a single machine , and every two worker processes run on a specific machine . All the machines have the same hardware ( Intel Xeon E5 2430 CPU with 24 cores , 64 GB RAM ) . The data are generated with the similar way but here we set n = 1000 , p = 8192 , s = 64 for linear regression and s = 32 for classification . To compute the speedup , we set an error threshold and record the time used for attaining such error . We repeat the experiment 10 times and report the average result .
In Figure 3 , we show the running time of Asy OS . In the first two sub figures we can observe that the error decreases rapidly when more workers are involved . When the number of workers is larger than 8 , there is almost no improvement of efficiency . We think this is caused by two reasons . First , since every worker will communicate with the central machine , the communication cost over the network will increase when the number of workers becomes larger . Second , the staleness upper bound τmax will be larger when using more workers . In the speedup result , we can find we have more than linear speedup for both applications . The reason is that we separate both the total iterations and the workload for each iteration . For each iteration , each worker only needs to compute its local XS(t)wS(t ) and retrieves uC from the central machine ( shown in Algorithm 3 ) .
This paper proposes a limited memory and model parameter free online feature selection algorithm , namely OS , which overcomes the disadvantages of existing optimization based online feature selection algorithm such as Grafting and its variation . To improve the computational efficiency and solve problems with huge scale problem , we propose an asynchronous parallel OS algorithm . Theoretical convergence analysis is provided for both algorithms . Empirical study suggests that the performance of OS and Asy OS is comparable to the benchmark algorithm Grafting , but requires much less memory cost and is much easier to set sparsity parameter .
7 . ACKNOWLEDGMENT
We thank the reviewers for constructive comments . This project is supported by the NEC fellowship and the NSF grant CNS 1548078 . The implementation of Asy OS significantly benefits from the first author ’s course project on Professor Sandhya Dwarkadas ’s Parallel and Distributed Systems course at University of Rochester .
8 . REFERENCES [ 1 ] H . Avron , A . Druinsky , and A . Gupta . Revisiting asynchronous linear solvers : Provable convergence rate through randomization . Journal of the ACM , 62(6):51 , 2015 .
[ 2 ] F . Bach , R . Jenatton , J . Mairal , G . Obozinski , et al .
Structured sparsity through convex optimization . Statistical Science , 27(4):450–468 , 2012 .
[ 3 ] E . Candes and T . Tao . The dantzig selector : statistical estimation when p is much larger than n . The Annals of Statistics , pages 2313–2351 , 2007 .
[ 4 ] R E Fan , K W Chang , C J Hsieh , X R Wang , and C J Lin . Liblinear : A library for large linear classification . JMLR , 9:1871–1874 , 2008 .
[ 5 ] S . Foucart . Hard thresholding pursuit : an algorithm for compressive sensing . SIAM Journal on Numerical Analysis , 49(6):2543–2563 , 2011 .
[ 6 ] D . Kong , R . Fujimaki , J . Liu , F . Nie , and C . Ding .
Exclusive feature learning on arbitrary structures via 1,2 norm . In NIPS , pages 1655–1663 , 2014 .
[ 7 ] M . Li , L . Zhou , Z . Yang , A . Li , F . Xia , D . G . Andersen , and A . Smola . Parameter server for distributed machine learning . ArXiv , 2013 . [ 8 ] Z . Li , J . Liu , Y . Yang , X . Zhou , and H . Lu .
Clustering guided sparse structural learning for unsupervised feature selection . TKDE , 26(9):2138–2150 , 2014 .
[ 9 ] Z . Li and J . Tang . Unsupervised feature selection via nonnegative spectral analysis and redundancy control . IEEE Transactions on Image Processing , 24(12):5343–5355 , 2015 .
[ 10 ] J . Liu and S . J . Wright . Asynchronous stochastic coordinate descent : Parallelism and convergence properties . SIAM Journal on Optimization , 25(1):351–376 , 2015 .
[ 11 ] J . Liu , S . J . Wright , C . R´e , V . Bittorf , and S . Sridhar .
An asynchronous parallel stochastic coordinate descent algorithm . JMLR , 16(1):285–322 , 2015 .
1950 Figure 1 : Recovery error and feature selection recall of linear regression on synthetic data sets . The left column shows the comparison of feature selection methods with different number of features ; the middle column shows the result with different times of scanning features ; and the right column shows the result with different number of workers for Asy OS .
Figure 2 : Classification error and feature selection recall of hinge loss classification on synthetic data sets . The left column shows the comparison of feature selection methods with different number of features ; the middle column shows the result with different times of scanning features ; and the right column shows the result with different number of workers for Asy OS .
#Features200025003000350040004500500055006000Recall0640660680707207407607808Feature selection recall with #featuresLASSOProj GDGraftingL1 coordinate descentL0 coordinate descentOS#Epoch of scanning feature12345Recall06206406606807072074076Feature selection recall with #Epoch of scanning featurep=2000p=2500p=3000p=3500p=4000p=4500p=5000p=5500p=6000#Workers1251020Recall06706806907071072073074075076077Feature selection recall with #Workersp=2000p=2500p=3000p=3500p=4000p=4500p=5000p=5500p=6000#Features200025003000350040004500500055006000Error0202102202302402502602702802903Error with #featuresLASSOProj GDGraftingL1 coordinate descentL0 coordinate descentOS#Epoch of scanning feature12345Error02202402602803032Error with #Epoch of scanning featurep=2000p=2500p=3000p=3500p=4000p=4500p=5000p=5500p=6000#Workers1251020Error021022023024025026027028Error with #Workersp=2000p=2500p=3000p=3500p=4000p=4500p=5000p=5500p=6000#Features100015002000250030003500400045005000Recall040450505506065Feature selection recall with #featuresL1 batch methodProj GDGraftingL1 coordinate descentL0 coordinate descentOS#Epoch of scanning feature12345Recall035040450505506065Feature selection recall with #Epoch of scanning featurep=1000p=1500p=2000p=2500p=3000p=3500p=4000p=4500p=5000#Workers1251020Recall0460480505205405605806062064Feature selection recall with #Workersp=1000p=1500p=2000p=2500p=3000p=3500p=4000p=4500p=5000#Features100015002000250030003500400045005000Error01201401601802022024Error with #featuresL1 batch methodProj GDGraftingL1 coordinate descentL0 coordinate descentOS#Epoch of scanning feature12345Error01201401601802022Error with #Epoch of scanning featurep=1000p=1500p=2000p=2500p=3000p=3500p=4000p=4500p=5000#Workers1251020Error01201250130135014014501501550160165017Error with #Workersp=1000p=1500p=2000p=2500p=3000p=3500p=4000p=4500p=50001951 Figure 3 : Evaluation on running time for Asy OS . The left and middle columns show the training error with elapsed time , the right column shows the speedup compared with the sequential method .
Figure 4 : Classification error of privacy image classification . The left column shows the comparison of feature selection methods with different value of s ; the middle column shows the result with different times of scanning features ; and the right column shows the result with different number of workers for Asy OS .
[ 12 ] A . Y . Ng . Feature selection , l 1 vs . l 2 regularization , and rotational invariance . In ICML , page 78 , 2004 . streaming feature selection . In ICML , pages 1159–1166 , 2010 .
[ 13 ] H . Peng , F . Long , and C . Ding . Feature selection
[ 23 ] K . Yu , X . Wu , W . Ding , and J . Pei . Towards scalable based on mutual information criteria of max dependency , max relevance , and min redundancy . TPAMI , 27(8):1226–1238 , 2005 .
[ 14 ] S . Perkins and J . Theiler . Online feature selection using grafting . In ICML , pages 592–599 , 2003 . and accurate online feature selection for big data . In ICDM , pages 660–669 , 2014 .
[ 24 ] L . Yu and H . Liu . Efficient feature selection via analysis of relevance and redundancy . JMLR , 5:1205–1224 , 2004 .
[ 15 ] B . Recht , C . Re , S . Wright , and F . Niu . Hogwild : A
[ 25 ] M . Yuan and Y . Lin . Model selection and estimation lock free approach to parallelizing stochastic gradient descent . In NIPS , pages 693–701 , 2011 .
[ 16 ] L . Song , A . Smola , A . Gretton , K . M . Borgwardt , and
J . Bedo . Supervised feature selection via dependence estimation . In ICML , pages 823–830 , 2007 .
[ 17 ] S . Sridhar , S . Wright , C . Re , J . Liu , V . Bittorf , and C . Zhang . An approximate , efficient lp solver for lp rounding . In NIPS , pages 2895–2903 , 2013 .
[ 18 ] M . Tan , I . W . Tsang , and L . Wang . Towards ultrahigh dimensional feature selection for big data . JMLR , 15(1):1371–1429 , 2014 .
[ 19 ] R . Tibshirani . Regression shrinkage and selection via the lasso . Journal of the Royal Statistical Society . Series B ( Methodological ) , pages 267–288 , 1996 .
[ 20 ] L . Tran , D . Kong , H . Jin , and J . Liu . Privacy cnh : A framework to detect photo privacy with convolutional neural network using hierarchical features . In AAAI , 2016 . in regression with grouped variables . Journal of the Royal Statistical Society : Series B ( Statistical Methodology ) , 68(1):49–67 , 2006 .
[ 26 ] X T Yuan , P . Li , and T . Zhang . Gradient hard thresholding pursuit for sparsity constrained optimization . ICML , 2014 .
[ 27 ] S . Zerr , S . Siersdorfer , J . Hare , and E . Demidova . Privacy aware image classification and search . In SIGIR , pages 35–44 , 2012 .
[ 28 ] Y . Zhai , M . Tan , Y . S . Ong , and I . W . Tsang .
Discovering support and affiliated features from very high dimensions . In ICML , pages 1455–1462 , 2012 . [ 29 ] R . Zhang and J . Kwok . Asynchronous distributed admm for consensus optimization . In ICML , pages 1701–1709 , 2014 .
[ 30 ] S . Zhang , A . E . Choromanska , and Y . LeCun . Deep learning with elastic averaging sgd . In NIPS , pages 685–693 , 2015 .
[ 21 ] J . Wang , M . Wang , P . Li , L . Liu , Z . Zhao , X . Hu , and
[ 31 ] W . Zhang , S . Gupta , X . Lian , and J . Liu .
X . Wu . Online feature selection with group structure analysis . TKDE , 27(11):3029–3041 , 2015 .
Staleness aware async sgd for distributed deep learning . IJCAI , 2016 .
[ 22 ] X . Wu , K . Yu , H . Wang , and W . Ding . Online
[ 32 ] J . Zhou , D . Foster , R . Stine , and L . Ungar . Streaming
Time ( in seconds)51015Fitting error000500100150020025003Fitting error with cost time2 workers4 workers6 workers8 workers10 workersTime ( in seconds)2468Classification error00501015020250303504045Classification error with cost time2 workers4 workers6 workers8 workers10 workers#Workers246810Speedup5101520253035Speedup with #Workerslinear regressionclassifications50100150200Error0050101502025Classification Error with sparsityL1 Hinge Loss ClassificationProj GDGraftingL1 coordinate descentL0 coordinate descentOS#Epoch of scanning feature12345Error001001500200250030035004Error with #Epoch of scanning featureSparsity s=50Sparsity s=100Sparsity s=150Sparsity s=200#Workers1251020Error005010150202503Error with #WorkersSparsity s=50Sparsity s=100Sparsity s=150Sparsity s=2001952 feature selection using alpha investing . In SIGKDD , pages 384–393 . ACM , 2005 .
[ 33 ] J . Zhou , D . P . Foster , R . A . Stine , and L . H . Ungar .
Streamwise feature selection . JMLR , 7:1861–1885 , 2006 .
[ 34 ] J . Zhu , N . Lao , and E . P . Xing . Grafting light : fast , incremental feature selection and structure learning of markov random fields . In SIGKDD , pages 303–312 , 2010 .
APPENDIX A . PROOFS A.1 Proof of Theorem 1
Define Sk as the set of selected features and jk as the new input feature in the ( k + 1) th iteration . The OS algorithm following the update rule that : wk+1 = PΩ(s)(wk − η m
∇if ( wk ) − η∇jk f ( wk) ) . i∈Sk
Suppose that ( wk+1 − wk)Sk = t , ( wk+1 − wk)jk = r , and we already know that wk+1 and wk are the same at other positions . So we have : f ( wk+1 ) = Fwk ( t , r ) .
According to inequality ( 6 ) from the Assumption 1 , we have : f ( wk+1 ) − f ( wk ) =Fwk ( t , r ) − Fwk ( 0 , 0 ) ≤∇Sk Fwk ( 0 , 0 ) , t + r∇jk Fwk ( 0 , 0 ) +
( t2/m2 + r2 ) ∇Sk f ( wk ) , wk+1 − wk + ( wk+1 − wk)jk∇jk f ( wk ) LF 2
LF 2 wk+1 − wk2 − 1 2η wk+1 − wk2
=
≤
1 m
+
LF
2
( 9 ) where the last inequality comes from the fact that i∈Sk
1 m i∈Sk wk+1 − ( wk − η
≤wk − ( wk − η
1 m i∈Sk
=η
1 m
∇if ( wk ) − η∇jk f ( wk))2
≤
∇if ( wk ) − η∇jk f ( wk))2
A.2 Proof of Theorem 2
To analyze the proposed asynchronous algorithm , we monitor the values of w concatenating all disjoint pieces from q workers . The central node actually records the value of u = Xw . To be convenient , we define a vector function gk(w ) =
1 m
∇if ( w)+∇j f ( w ) =
∇Sk(tk)f ( w)+∇jk f ( w ) .
1 m i∈Sk(tk ) where tk is the worker index which makes the update at the ( k + 1) th iteration , and Sk(tk ) denotes the set of selected features at worker tk .
Then the ( k + 1) th update happens in the central node follows :
Sk(tk)∪{jk} = PΩ(s/q)[(wk − ηgk( ˆwk))Sk(tk)∪{jk} ] wk+1 wk+1 Sk(tk)∪{jk} = wk
Sk(tk)∪{jk} . where Sk(tk ) ∪ {jk} denotes the complementary set of Sk(tk)∪ {jk} .
In the asynchronous parallelism , we can not guarantee ˆwk = wk in general , but we have the following equation when the staleness is limited :
ˆwk = wk − l∈τ ( k )
( wl+1 − wl ) . where τ ( k ) ⊂ {k − 1 , k − 2 , , k − τmax} . Since wk+1 and wk only differ in Sk(tk ) ∪ {jk} , we have the inequality : wk+1 − ( wk − ηgk( ˆwk))2 ≤ wk − ( wk − ηgk( ˆwk))2 , so we get : gk( ˆwk ) , wk+1 − wk ≤ − 1 2η wk+1 − wk2 .
( 10 )
With similar steps of getting inequality ( 9 ) , we have f ( wk+1 ) − f ( wk ) ≤gk(wk ) , wk+1 − wk + =gk( ˆwk ) , wk+1 − wk + wk+1 − wk2 wk+1 − wk2
LF 2 LF 2
+ gk(wk ) − gk( ˆwk ) , wk+1 − wk
LF
2
− 1 2η wk+1 − wk2 + gk(wk ) − gk( ˆwk ) , wk+1 − wk
.
=:T1
( 11 )
∇if ( wk ) + η∇jk f ( wk)2 . where the last inequality uses ( 10 ) . Introduce α > 0 , β > 0 , we obtain :
1
K k=1
− LF 2
2η
1 2η − LF
2
Summing up inequality ( 9 ) from k = 1 to K , we get f ( w1 ) − f ( wK+1 ) ≥ wk+1 − wk2 .
Since we know that function f is bounded below , ie , f ( w ) > −∞,∀w ∈ Ω(s ) . If
> 0 , we get wk+1 − wk2 ≤
2η
K(1 − ηLF ) f ( w1 ) − f ( wK+1 )
K k=1
1 K
It completes the proof .
T1 ≤ 1 2α
( wk+1 − wk)Sk ( tk )2 +
1 2β
( wk+1 − wk)jk2+
α 2
1 m
∇Sk ( tk )f ( ˆwk)2 +
∇Sk ( tk )f ( wk ) − 1
We use Uk to denote the union of Sk(tk ) : Uk = ∪q Then we take the expectation of T2 :
=:T2
β 2 m
∇jk f ( wk ) − ∇jk f ( ˆwk)2 t=1Sk(tk ) .
.
E(T2 ) ≤ α 2m2
E∇Sk(tk)f ( wk ) − ∇Sk(tk)f ( ˆwk)2
1953 + ≤ α 2m2q
β
2(p − s )
E∇Sk(tk)f ( wk ) − ∇Sk(tk)f ( ˆwk)2
E∇Uk f ( wk ) − ∇Uk f ( ˆwk)2 β
E∇Uk f ( wk ) − ∇Uk f ( ˆwk)2
+
2(p − s )
≤ α 2m2q ≤ α 2m2q fEwk − ˆwk2 + L2 fEwk − ˆwk2 + L2 fEwk − ˆwk2 L2 fEwk − ˆwk2 L2
=
≤
≤
αL2 f 2m2q
αL2 f 2m2q
+
+
βL2 f 2(p − s )
βL2 f 2(p − s )
αL2 f 2m2q
+
βL2 f 2(p − s )
β
2(p − s )
β l∈τ ( k )
2(p − s )
E τmaxE k−1 l∈τ ( k )
τmaxE
( wl+1 − wl)2 wl+1 − wl2 wl+1 − wl2 l=k−τmax where the second inequality comes from ( 7 ) in Assumption 1 . Combine with equation ( 11 ) and choose α = β to be 1/LF , then
1 f ( wk+1 ) − f ( wk ) ≤ −
− LF
2η wk+1 − wk2 + T2 .
If the condition
− LF −
1 2η
1
2m2q
+
1
2(p − s )
τ 2 max
> 0
L2 f LF
1
2m2q + 1
2(p−s )
.
L2 f LF
τ 2 max
Finally , we get is satisfied , that is ,
η <
1
2
LF +
K 2η − LF − 1 k=1
1
1
η2K
≤
Ewk+1 − wk2 f ( w1 ) − Ef ( wK+1 )
2m2q + 1
2(p−s )
L2 f LF
.
Kη2
τ 2 max
It completes the proof . A.3 Proof of Theorem 3
Let us prove a lemma first .
Lemma 4 . We have f ( ¯w ) ≥ f ( wk ) − s
2ρ−(s )
∇f ( wk)2∞,∀ ¯w , wk ∈ Ω(s )
Proof . From the restricted strong convexity , ie As sumption 3 , we have : f ( ¯w ) − f ( wk ) ≥∇f ( wk ) , ¯w − wk +
ρ−(s )
2
≥ min supp(w)⊆ ¯Ω
∇f ( wk ) , w − wk + w − wk2
¯w − wk2 ρ−(s )
2 wl+1 − wl2 .
( 12 )
2ρ−(s )
1
= − ≥ − | ¯Ω|
2ρ−(s )
[ ∇f ( wk ) ] ¯Ω2
[ ∇f ( wk ) ] ¯Ω2∞ ≥ − s
2ρ−(s )
∇f ( wk)2∞ where ¯Ω = supp( ¯w ) It completes the proof .
Then we are ready to prove Theorem 3 . Proof . From Lemma 4 , we have :
∞ f ( w where |w∞
) − f ( w
∗
) ≤ s
∇f ( w 2ρ−(s ) i | . min| = mini |w∞
∞
)2∞ ≤ s
2ρ−(s )
∞ min)2 w
(
1 η
K k−1 k=1 l=k−τmax
τmax LF
Ewl+1 − wl2 w
From Assumption 3 , we also have : ρ−(s )
∞2 ≤ f ( 0 ) − f ( w ∞2 ≤ 2 ( f ( 0 ) − f ( w∞ ) )
⇒ ( w w
∞
2
ρ−(s )
∞
∞
)(0 − w
) − ∇f ( w min)2 ≤ 2 ( f ( 0 ) − f ( w∞ ) )
)
∞ sρ−(s ) then we have
∞
) − f ( w f ( w
∞
) − f ( w ∗ ) − f ( w ∞ f ( w
It completes the proof .
( f ( 0 ) − f ( w
∞
) )
∗
) − f ( w
∞
) )
∗
2
1 η2
2ρ−(s )
) ≤ s ≤α ( f ( 0 ) − f ( w ≤α ( f ( 0 ) − f ( w ) ) ≤α ( f ( 0 ) − f ( w ) ≤ α ∗ sρ−(s ) ∞ ) ) ∗ ∗
) + f ( w ) ) ( f ( 0 ) − f ( w
) ) .
∗
1 + α
So we have
E 1
2η
+ f ( wk+1 ) − f ( wk )
≤ −
− LF
Ewk+1 − wk2
L2 f 2m2q
L2 f
+
2(p − s )
E
τmax LF k−1 l=k−τmax
Summing ( 12 ) from k = 1 to K , we get f ( wK+1 ) − f ( w1 )
≤ −
− LF
2η
Ewk+1 − wk2
K k=1 L2 f
2(p − s )
K k=1 L2 f
2(p − s )
1
+
E 1
1
K
+ k=1
L2 f 2m2q
+
L2 f 2m2q
+
Ewk+1 − wk2 .
≤ −
− LF
2η
Ewk+1 − wk2
K k=1
τ 2 max LF
Ewk+1 − wk2
L2 f LF
= −
− LF −
1 2η
+
1
2(p − s )
2m2q
τ 2 max
( 1 + α ) ( f ( w
1954
