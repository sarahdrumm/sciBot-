Streaming LDA : A Copula based Approach to Modeling
Topic Dependencies in Document Streams
Hesam Amoualian
University of Grenoble Alps CNRS / LIG hesamamoualian@imagfr
Marianne Clausel
University of Grenoble Alps CNRS / LJK marianneclausel@imagfr
Eric Gaussier
University of Grenoble Alps CNRS / LIG ericgaussier@imagfr
Massih Reza Amini
University of Grenoble Alps CNRS / LIG massih rezaamini@imagfr
ABSTRACT We propose in this paper two new models for modeling topic and word topic dependencies between consecutive documents in document streams . The first model is a direct extension of Latent Dirichlet Allocation model ( LDA ) and makes use of a Dirichlet distribution to balance the influence of the LDA prior parameters wrt to topic and word topic distribution of the previous document . The second extension makes use of copulas , which constitute a generic tools to model dependencies between random variables . We rely here on Archimedean copulas , and more precisely on Franck copulas , as they are symmetric and associative and are thus appropriate for exchangeable random variables . Our experiments , conducted on three standard collections that have been used in several studies on topic modeling , show that our proposals outperform previous ones ( as dynamic topic models and temporal LDA ) , both in terms of perplexity and for tracking similar topics in a document stream .
CCS Concepts •Computing methodologies → Latent Dirichlet allocation ; •Mathematics of computing → Bayesian computation ; Gibbs sampling ; Metropolis Hastings algorithm ;
Keywords Latent Dirichlet allocation , Copulas , Document Streams , Topic Dependencies
1 .
INTRODUCTION
The recent proliferation of temporal textual data on the Internet such as Tweets or comments on Youtube has brought new challenges for learning with interdependent data . Though important progress has been made in some directions [ 8 ] , popular approaches for most of these tasks are designed to deal with static collections of documents . This is specially the case for latent topic modeling , albeit analyzes of social content have gained much attention in recent years for different aspects of daily life , such as latent health related topic analysis [ 19 ] or buzz detection [ 20 ] .
Although the main goal of probabilistic modeling is to find word topics , an equally interesting objective is to examine topic evolutions and transitions . The seminal work of [ 4 ] proposed to model the dynamic evolution of topics by first grouping documents into time slices and then to chain the evolution of both the word topic and topic mixture distributions via a Gaussian process . In some cases , the Gaussian distribution was not found to be the appropriate distribution in modeling the topic shifts and some studies considered other probability distributions for capturing the evolution of topics over time [ 22 ] . However , the idea of grouping documents into epochs for modeling topic evolution was echoed in a number of studies . For example , [ 24 ] estimated a transition matrix over topic vectors between two predefined epochs and they showed that the LDA model [ 5 ] can be enhanced by considering directly the evolution of the topics over time .
In this paper we propose two extensions of LDA for modeling the dependency between two consecutive documents in a stream . In our first model , we suppose that the dependency between topic distributions of two consecutive documents follows a Dirichlet distribution controlled by an hyperparameter . This model is similar to the one of [ 4 ] with time slices equal to 1 , but it offers a more precise mechanism for controlling the dependencies and is based on a framework encompassing all the situations ( from complete independence to plain equality ) . This first study paves the way for a more general topic model in which the dependencies between the topics of two consecutive documents are captured by copulas which constitute generic tools to model dependencies between random variables [ 6 ] . Among the several families of copulas that have been defined in the literature , our choice fell on Archimedean copulas [ 13 , 14 ] as they are symmetric and associative , necessary conditions when dealing with exchangeable random variables [ 18 ] . More particularly , we use Franck copulas , a special case of Archimedean copulas that rely on a single parameter , easier to estimate and more robust to sparse data . Using three collections with different characteristics , we show that our approaches are faster and improve over state of the art topic models . We also analyze
695 the precision of our models to track the topics on a labeled dataset .
The outline of this paper is as follows . In the next section , we present our models . In Section 3 , we introduce an efficient procedure to estimate the most important , in terms of size , parameters . We then describe in Section 4 the experimental results obtained with our approaches on three distinct datasets . In Section 5 , we position our work with respect to the state of the art . Finally , Section 6 concludes our study by summarizing its main results and by giving some pointers to future research . 2 . STREAMING LDA
Latent Dirichlet Allocation ( LDA , [ 5 ] ) is a probabilistic Bayesian model used to describe a corpus of D documents , associated with a vocabulary of size V . In this model , latent variables , indexed in {1,··· , K} , are used to represent the hidden ( in the sense non observed ) topics underlying each document . LDA is associated to the following generative model1 :
• Generate , for each topic k , 1 ≤ k ≤ K , a distribution over the words : φk ∼ Dir(β ) , where φk and β are V dimensional vectors ; • For each document d : where θd and α are K dimensional vectors ;
– Choose a distribution over the topics : θd ∼ Dir(α ) , – For each position ( indexed by n , 1 ≤ n ≤ N ) in d : n ∼ mult(1 , θd ) ; n with
( a ) Choose a topic assignment : zd ( b ) Choose the word wd n = v|zd probability P ( wd n from the topic zd n = k ) = φk,v ; where N is the length of each document and φk,v is the vth coordinate of φk . α and β correspond to the priors of the model . They are usually fixed , following [ 5 ] . Furthermore , in almost all previous studies on LDA , the priors are considered to be symmetric , each coordinate of the vector being equal : α1 = ··· = αK . If one assumes a broad Gamma prior for both α and β , then their value can be easily learned from data by maximum a posteriori [ 1 ] or Markov Chain Monte Carlo [ 15 ] methods . One can also envisage learning asymmetric Dirichlet priors [ 21 ] , which raises no particular difficulties for the models we are considering . For clarity sake , we however assume here fixed , symmetric priors ; the extension to their learning through Gamma priors or through asymmetric priors is purely technical . In the remainder , we will denote by α and β the priors for the Dirichlet distributions as well the constant value taken by each coordinate of these priors , the context being sufficient to determine which element is referred to .
An important characteristic of LDA is that each document is generated independently from the previous ones . This is not a realistic assumption in different settings , as document streams , and we introduce below two extensions of LDA that model such dependencies . 2.1 Dirichlet based dependencies
We introduce here a first extension of LDA , that we refer to as ST LDA D . 1For simplification and following standard practice , we do not model here the length of each document , assumed to be fixed and equal to N .
211 Presentation of the model In this first model , we rely on a direct extension of the LDA model to take into account dependencies between the document specific topic distributions of two sequential documents , denoted ( d − 1 ) and d ( 2 ≤ d ≤ D ) . This extension uses , as the standard LDA model , Dirichlet distributions for the document specific topic distributions , the parameters of which are linear combination of the standard prior α and the topic distribution estimated in the previous document :
θd|θd−1 ∼ Dir(α + λdθd−1 )
( 1 ) where λd is a uniformly distributed parameter that controls the influence of the topics of document ( d − 1 ) on the topics of document d ( see Figure 1 ) . The expectation of each component of θd is given by : i |θd−1
E[θd
] =
( 2 ) i
α + λdθd−1 Kα + λd i i i
. i |θd−1
] ≈ θd−1
Hence , if λd is high , ie if document d covers the same topics as document ( d − 1 ) , then E[θd We furthermore assume that the previous document , ( d− 1 ) , can influence the word topic distributions of the current document d . This assumption , also made in dynamic topic models [ 4 ] and topic tracking models [ 11 ] , is motivated by the fact that , within a given topic , if word distributions evolve over time , they tend to do so in a smooth way . As before , one can use a direct extension of the LDA model to account for dependencies between word topic distributions in sequential documents : ∀k , 1 ≤ k ≤ K , φd k|φd−1 ∼ Dir(β + µdφd−1
( 3 )
) k
Here µd is again a uniformly distributed parameter that controls the tradeoff between the prior β and the learned topicword distributions φd−1 . As usual φd−1 is the word distribution of topic k . The conditional mean of each component of φd k is given by : k
E[φd k|φd−1 ] =
β + µdφd−1 k V β + µd
( 4 ) and is approximately the value of the same component of document ( d − 1 ) when the two documents are strongly dependent . Lastly , as one can note , by setting λd = µd = 0 , ∀d , 2 ≤ d ≤ D , one “ forgets ” the dependencies between consecutive documents . The streaming model is in this case identical to the standard LDA model . 212 As mentioned before , the parameters α and β are considered fixed . The other parameters can be estimated through Gibbs sampling , with Metropolis Hasting updates for the parameters λd and βd . We give here the update formulas of each parameter . For θ , one has :
Inference with Gibbs sampling
θd ∼ P ( θ|θd−1 , zd , wd , α , β , λd , φd−1 , φd , µd )
=
B(α)B(α + λdθd−1 + Ωd ) B(α + Ωd)B(α + λdθd−1 ) Dir(Ωd + α + λdθd−1 )
×
( 5 )
696 θd−1 z w
θd z w
N
N
φ
β
α
θd−1 z w
N
φd−1
β
α
θd z w
φd
β
N
αd−1
θd−1 z w
N
φd−1
αd
θd z w
φd
N
.
θd−h
.
.
.
λd−1
α
λd
α
θd−1 z w
N
φd−1
T Matrix
θd z w
φd
N
θd−1 z w
N
φd−1
θd z w
φd
N
β
µd−1
β
µd
( a ) DMM
( b ) TTM
( c ) DTM
( d ) TM LDA
( e ) ST LDA [D|C ]
Figure 1 : Graphical models for Dynamic Mixture Models ( DMM , [ 25] ) , Topic Tracking Models ( TTM , [ 11] ) , Dynamic Topic Models ( DTM , [ 4] ) , Temporal LDA ( TM LDA , [ 24 ] ) and Streaming LDA ( ST LDA [D|C ] ) where Ωd is defined as in [ 23 ] and represents the dth row of the D × K count matrix Ω , with Ωd,k being the number of times that topic k is assigned to words in document d .
The update for φd k , 1 ≤ k ≤ K is similar : k ∼ P ( φk|θd−1 , θd , zd , wd , α , β , λd , φd−1 , µd ) φd
=
B(β)B(β + µdφd−1 k + Ψk ) B(β + Ψk)B(β + µdφd−1 ) Dir(Ψk + β + µdφd−1 k ) k
×
( 6 ) where Ψk is again defined as in [ 23 ] and represents the kth row of a K × V count matrix , Ψk,v being the number of times that topic k is assigned to word v in the documents seen so far .
The Gibbs update for z is the same as the one for the standard LDA model :
∀k , 1 ≤ k ≤ K , P ( zd v = k|θd , φd ) = k × φd θd j × φd j θd k,v j,v
( 7 )
Finally , for λd and µd , one can not directly compute Gibbs updates as the normalizing factor for the distribution of λ given all the other parameters can not be computed exactly . One can nevertheless rely on a Metropolis Hasting procedure , detailed in Appendix A . 2.2 Copula based dependencies
Model ST LDA D captures topic and word topic dependencies through Dirichlet distributions , which allow one to balance the influence of the priors ( α and β ) and of the topic and topic word distributions of the previous document . We introduce now another extension of LDA in which the dependencies between the topics of consecutive documents are modeled through copulas , which constitute a generic tool to model dependencies and do not rely on a specific distribution . We first provide a brief overview of copulas , prior to describe our model . 221 Basics on copulas For every p ≥ 2 , a p–dimensional copula is a p–variate density function on [ 0 , 1]p , whose univariate marginals are uniformly distributed on [ 0 , 1 ] . Copulas are particularly useful when modeling dependencies between random variables . Indeed , the joint cumulative distribution function ( CDF )
FX1,··· ,Xp of any random vector X = ( X1,··· , Xp ) can be written as a function of its marginals , as follows :
Theorem 1 ( Sklar ’s theorem Theorem 233 of [ 16 ] ) Let FX1,··· ,Xp be a p–dimensional distribution function with marginals FX1 ,··· , FXp . Then there exists a copula C with uniform marginals such that :
FX1,··· ,Xp ( x1,··· , xp ) = C(FX1 ( x1),··· , FXp ( xp ) )
Furthermore , when the CDF FX1,··· ,Xp is continuous , the copula is unique .
Copulas represent a general way of modeling the dependencies between random variables , from complete independence to equality . If the random variables X1,··· , Xp are pairwise independent , their copula is the so–called independency copula :
FX1,··· ,Xp ( x1,··· , xp ) = FX1 ( x1)··· FXp ( xp ) whereas in the case X1 = ··· = Xd , one gets the comonotonicity copula :
FX1,··· ,Xp ( x1,··· , xp ) = min i∈{1,··· ,p} FXi ( xi )
Several copula families have been defined in the literature , among which the Archimedean copulas ( [16 , Ch . 4] ) , particularly interesting in our case . A p–dimensional Archimedean copula C with generator ψ is defined as :
Cp(u ; ψ ) := ψ(ψ
−1(u1 ) + ··· + ψ
−1(up) ) , u ∈ [ 0 , 1]p where ψ is a continuous , decreasing function , from [ 0,∞ ] to ( 0 , 1 ) , strictly decreasing on [ 0 , inf{t : ψ(t ) = 0} ] , and satisfying :
ψ(0 ) = 1 , ψ(∞ ) = lim t→∞ ψ(t ) = 0
Archimedean copulas have the following interesting properties :
• They are symmetric , that is invariant by any permutation of their coordinates , which is important when dealing with exchangeable random variables , as is the case here2 ;
2The LDA model is based on the assumption that topics are infinitely exchangeable within a document .
697 • They are associative : for any ( u1,··· , up ) ∈ [ 0 , 1]p , one has :
Cp−1(C2(u1 , u2 ; ψ ) , u3,··· , up ; ψ ) = Cp−1(u , ··· , up−2 , C2(up−1 , up ; ψ ) ; ψ )
This means that the dependency properties are the same whatever the way we group the random variables .
In this study , we further consider a particular case of the Archimedean copulas , namely the one–parameter family of Franck copula , defined , for any λ ∈ R \ {0} , as :
( e−λu − 1)(e−λv − 1 )
)
Cλ(u , v ) = −(1/λ ) ln(1 + e−λ − 1
( 8 ) When λ → 0 , one approaches the independency copula , whereas λ = ∞ yields the comonotonicity copula . Lastly , for any λ ∈ R \ {0} , Cλ is twice differentiable on [ 0 , 1]2 so that the copula function admits a density , denoted in the sequel cλ . By varying λ from 0 to ∞ , Franck copula allows one to model all the possible dependencies between two random variables , from complete independency to equality . Dependency/independency is furthermore controlled by a single parameter , λ , which makes parameter estimation both easier and more robust . 222 Generative process Instead of generating the topic distribution of each document θd independently , as is done in standard LDA we bind , as for our first model , ST LDA D , the topic distributions θd−1 and θd of consecutive documents , this time by using copulas , and more precisely Franck copula .
One can not however directly use Sklar ’s theorem as it does not extend to joint distributions over random vectors . This means that if we are given two random vectors X1 , X2 , one can not claim that there exists a copula C such that , for any ( x1 , x2 ) ∈ [ 0 , 1]p1 × [ 0 , 1]p2 :
FX1,X2 ( x1 , x2 ) = C(FX1 ( x1 ) , FX2 ( x2 ) ) except in very specific situation as when X1 and X2 are independent for example . One can nevertheless relate latent topics θd−1 and θd through their components . Indeed , the topic Dirichlet distribution can be decomposed into univariate Gamma distributions with parameters ( α , 1 ) , denoted Ga(α ) :
Theorem 2 ( from Theorem 2.1 of [ 17 ] ) A random vector θ follows a Dirichlet distribution Dir(α ) iff there exists a random vector T ∼ Ga(α ) ⊗ ··· ⊗ Ga(α ) such that :
( L ) =
θ
T T 1
( 9 )
( L ) where = means “ equality in distribution ” . In addition , if we are given θ ∼ Dir(α ) and R ∼ Ga(Kα ) independent , then T = Rθ ∼ Ga(α ) ⊗ ··· ⊗ Ga(α ) .
To bind the topic distributions θd−1 and θd of two consecutive documents , we thus consider the associated vectors T d−1 and T d , and bind them coordinate per coordinate using Franck copula . For the word topic distributions , we use the same coupling between consecutive documents as the one used in model ST LDA D , as a tighter coupling through copulas would be too costly . We will come back to this issue in Section 3 .
In the sequel for any γ > 0 , fγ ( resp . Fγ ) denotes the pdf ( resp . cdf ) of the Gamma distribution with parameters ( γ , 1 ) . The global generative model is thus as follows :
1 . Generate the first document according to the standard
LDA model
2 . For each document d , 2 ≤ d ≤ D :
( a ) Generate λd ∼ U [ 0 , τλ ] ( b ) Generate µd ∼ U [ 0 , τµ ] ( c ) For each topic k , 1 ≤ k ≤ K :
• Generate T d is : k
T d−1 P ( T d k whose conditional density wrt k |T d−1 • Generate φd
) = fα(T d k ) cλd ( Fα(T d−1 k|φd−1 ∼ Dir(β + µdφd−1
) , Fαk ( T d k ) ) ) k k k
( d ) Set θd = T d/T d1 ( e ) For each word n , 1 ≤ n ≤ N in d : • Choose a topic assignment : zd • Choose the word wd n|zd probability P ( wd n ∼ mult(1 , θd ) n with n from the topic zd n ) = φd zd n,wd n where T d k represents the kth coordinate of the vector T d , and follows a distribution Ga(α ) according to Theorem 9 . We refer to the corresponding model as ST LDA C . Figure 1 provides a graphical representation of this model , together with the ones of previous models . 223 The updates for zd , φd and µd are identical to the ones
Inference with Gibbs sampling for model ST LDA D . For λd , one gets :
K
P ( λd|T d−1,T d , zd , wd , α , β , φd−1 , φd , µd ) ∝ ) , Fα(T d k ) ) k )cλ(Fα(T d−1 fα(T d−1
)fα(T d k k
P ( λd ) k=1
The same Metropolis Hasting procedure as the one used for model ST LDA D and detailed in Appendix A can then be used . For θd , one needs first to estimate the conditional probability of the random vector T d with respect to the other parameters . This expression can be factored as follows :
P ( T d|T d−1 , zd , wd , α , β , λd , φd−1 , φd , µd ) = P ( T d|T d−1 , α , λd)P ( zd|T d )
P ( zd|α )
As in the classical context of LDA , one has P ( zd|α ) = B(Ωd + α)/B(Ωd ) where Ωd is defined as before . By assumption on the distribution of the random vectors ( T d−1,T d ) :
P ( T d|T d−1 , α , λd ) = fα(T d k )cλ(Fα(T d−1 k
) , Fα(T d k ) )
K k=1
Developing P ( zd|T d ) as detailed in Appendix B , finally leads to : P ( T d|T d−1 , zd , wd , α , β , λd , φd−1 , φd , µd ) ∝ (
K
−N
T d k )
× K f(Ωd,k+α−1)(T d k ) × cλ(Fα(T d−1 k
) , Fα(T d k ) ) ( 10 ) k=1 k=1
Each T d k can then be estimated through the MetropolisHasting procedure presented in Appendix A ; θd is finally obtained from T d through Eq 9 .
698 For model ST LDA C , the word topic distributions φd
3 . COMPUTATIONAL CONSIDERATIONS k ( 1 ≤ k ≤ K ) could be estimated in the same way as θd is estimated , as mentioned in Section 22 However , this would entail running K × V Metropolis Hasting procedures , which is problematic as soon as the collections considered are relatively large . We thus proposed in Section 2.2 to estimate it through Eq 6 , as done for ST LDA D . This time , K × V Gibbs sampling updates are required . If this estimation procedure is faster , it may still be too slow for really large collections . Theorem 2 nevertheless suggests a way to approxik ( 1 ≤ k ≤ K , 2 ≤ d ≤ D ) through Gamma updates , mate φd as follows :
1 . For each word v in d , generate tk,v ∼ Ga(β + φd−1 k,v ) k,v ← tk,v 2 . For each word v in the vocabulary V , φd v∈V tk,v where β corresponds to the real parameter ( ie , the constant value that makes up the V dimensional vector of priors ) . The quantities tk,v are first initialized through tk,v ∼ Ga(β ) , and updated each time a new document is encountered . As one can note , this update primarily concerns the words present in the current document ( step 1 ) , the components for the other words being just renormalized ( step 2 ) . This contrasts with Eq 6 in which the contribution of all words is resampled for each document via a multivariate Dirichlet distribution . The above procedure simplifies this by relying on the univariate equivalent of the Dirichlet distribution , namely the Gamma distribution , and by binding the variables through the renormalization step . It is faster as it involves only K × N samplings from a Gamma distribution instead of K samplings from a multivariate , V ( V >> N ) dimensional Dirichlet distribution ( the K × V renormalizations in step 2 do not really harm the procedure and are negligible compared to the Dirichlet samplings ) . We have
Algorithm 1 : Inference process for ST LDA [D|C ] Input : Stream of D documents of length N ; number of topics K
Output : For each document d , topic distribution θd , k ( 1 ≤ k ≤ K ) ; for word topic distributions φd each word v in d , topic assignment zd v
// Initialization
1 for k = 1 to K , v ∈ V do tk,v ∼ Ga(β )
2
3 for d = 1 to D do 4
Random initialization of λd , µd and zd n , 1 ≤ n ≤ N
5 λ1 = µ1 = 0
// Document processing
6 for d = 1 to D do 7 repeat
8
9
10
11
12
13
14
15
For ST LDA D : update θd acc . to Eq 5 For ST LDA C : ( a ) update T d ( Metropolis Hasting ) ( b ) obtain θd from T d through Eq 9
Update φd k acc . φ procedure Update λd and µd ( Metropolis Hasting ) , d > 2 n acc . to Eq 7 , 1 ≤ k ≤ K , 1 ≤ n ≤ N Update zd until estimates are stable observed in practice no difference , in terms of performance measures we consider ( see Section 4 ) , between this procedure and the more complex ones mentioned before , and make use of it in the remainder of the paper . In terms of speed , this procedure performed 1.5 times faster on the NIPS collection , which contains long documents and a relatively small vocabulary ( ca . 12,000 words ) , and 2 times faster for the TDT4 and Tweets collections , which contain shorter documents with a larger vocabulary ( up to 42,000 words ) .
Algorithm 1 summarizes the inference process we rely on . It makes use of the above procedure to estimate φ , referred to as φ procedure .
4 . EXPERIMENTAL STUDY
We conducted a number of experiments aimed at evaluating how the proposed models behave on different collections by analyzing their stability , convergence time and performance .
Datasets . We performed experiments on three datasets with different characteristics . The NIPS dataset contains 1,500 scientific papers with no time dependency between them . The size of the vocabulary is 12,375 and documents contain 500 unique words in average . The collection was collected from the NIPS proceedings and is relatively homogeneous in terms of the topics covered . It allows us to assess whether topic dependencies are still useful in a ” loose ” context in which there is no more temporal dependency . It is available at the UCI ML Repository [ 12 ] .
The Multilingual Text and Annotations data set ( TDT4)3 proposed for topic detection and tracking , has 3,190 original documents in English and a vocabulary size of 22,965 . Documents here are newswires extracted from different broadcasts and the number of unique words per document is 100 in average . Even though newswires are not extracted from the same source , they are ranked by the time .
The Tweets dataset is collected using Twitter ’s streaming API during 20 days from 8/10/2014 to 27/10/2014 . The collection contains 72,592 tweets and a vocabulary of size 42,336 . Tweets have been sequenced by time and are filtered over health issues using an SVM classifier trained over MeSH categories4 .
Each dataset was separated into training and test sets . The NIPS collection was randomly splitted into training ( 90 % of the collection ) and test ( 10 % of the collection ) sets . For TDT4 , we used the first 2800 newswires released in time for training , and the last 390 ones for testing . For the Tweets dataset , we used the tweets issued in the first 17 days for training ( 60,000 documents ) and those of the last 3 days ( 12,000 documents ) for testing . Table 1 summarizes the characteristics of these collections .
Evaluation .
Results are evaluated over the test set using the widely used perplexity measure that can be approximated by [ 5 ] . k × φd θd log Dtest × N k k,vd n n
− d
 perplexity(C test ) = exp
3Linguistic Data Consortium , The Trustees of the University of Pennsylvania https://catalogldcupennedu/ LDC2005T16 . 4https://wwwnlmnihgov/mesh/
( 11 )
699 NIPS
TDT4
2,100
2,000
1,900
1,800
1,700
1,600
1,500
1,400 y t i x e l p r e P
LDA1 TM LDA
LDAall DTM
ST LDA D
ST LDA C y t i x e l p r e P
8
10 12 14 16 18 20 22 24 26 28 30
1,100
1,050
1,000
950
900
850
800
5
LDA1 TM LDA
LDAall DTM
ST LDA D
ST LDA C
6
7
8
9
10
11
12
13
14
15
Time ( minutes )
Time ( minutes )
Figure 2 : Perplexity curves with respect to time for all methods on NIPS and TDT4 collections ( 80 topics ) .
Table 1 : Datasets used in our experiments along with their properties .
Documents in Train set Documents in Test set Vocabulary size # of unique words per doc . Words in total
NIPS 1,350 150 12,375 500 1,900,000
TDT4 2,800 390 22,965 100 779,000
Tweets 60,000 12,000 42,336 15 904,262 k and φd where C test denotes the test collection , Dtest is its size and vd n represents the word at position n in document d . The parameters θd k are estimated on the training set . Furthermore , for the TDT4 collection we use the available semantic labels of newswires in the test set in order to evaluate the ability of the models to find documents of the same semantic labels using only their predicted topic distributions ( Section 42 ) To this aim , we measure ROC curves and AUC of different topic models on TDT4 .
Settings and comparisons .
For all models , both hyperparameters α and β were fixed to 05 Documents of the NIPS dataset are initially stoplisted , we did not perform further preprocessing of the data nor removed stop words from the TDT4 and Tweets documents as for all methods best results are obtained when collections are not filtered .
To validate the streaming LDA models described in the previous section , we test the following six methods . The first two are LDA models [ 5 ] : ( a ) LDA1 , which consists in training an LDA model on the whole training data , then fixing φ and updating θ for each document in the test set , ( b ) LDAall , which consists in training an LDA model on the whole on training data and updating both φ and θ for each document in the test set . In addition , we consider two stateof the art latent models that take into account dependencies between topics : Dynamic Topic Model ( DTM ) [ 4 ] and Temporal LDA ( TM LDA ) [ 24 ] . DTM is certainly the most popular model to take into account topic dependencies . It is furthermore complete in the sense that it integrates both topic and word topic distributions . TM LDA is a very recent proposal with nice features . Lastly , we also consider the two streaming LDA models we have introduced ( ST LDA D and ST LDA C ) . For these last two models , τλ ( see Appendix A ) is set to 30,0005 . All the algorithms were implemented in Python with Numpy and Scipy6 except DTM that is a C++ implementation tool from [ 3 ] . For both training and test , DTM is used considering that each document corresponds to a time slice . 4.1 The effect of streams of documents
We start our evaluation by analyzing the gains provided by modeling dependencies between topics by streaming ( as with ST LDA D and ST LDA C ) compared to other approaches on the different datasets . Figure 2 shows the evolution of perplexities of different models over the test set with respect to the training time of each model on NIPS and TDT4 datasets . The code program of DTM ( in C++ ) generally executes faster than the other code programs ( written in pyhton ) , nevertheless we ignore this detail and consider all the curves identically . To measure the perplexity for each model , we estimate θ and φ over respectively all documents and all words of the training set . These estimates are then used to evaluate iteratively new φ and θ distributions for each document in the test set . This iterative update of φ and θ is done for all of the methods except LDA1 which updates the distributions θ and φ over the whole documents in the test set with the last parameters that were obtained from the training set .
As expected , all perplexity curves decrease monotonically with respect to time . On both datasets , perplexity curves 5This value , upper bounding λd , corresponds to a regime of the Franck copula close to comonotonicity . 6We are working to release all the programs developed in this study publicly available for research purpose .
700 Table 2 : Perplexity with respect to different number of topics in {20 , 40 , 60} .
Models
LDA1 LDAall TM LDA DTM ST LDA D ST LDA C
20
2068.4 1625.4 2038.7 1737.5 1620.4 1612.8
NIPS
40
2034.5 1534.7 2025.4 1551.2 1520.9 1497.6
60
1986.4 1458.1 1985.3 1450.7 1450.2 1434.5
20
900.8 723.1 876.7 869.1 724.4 720.6
TDT4
40
930.2 768.4 900.3 836.7 758.1 752.5
60
960.4 792.7 916.3 820.9 784.4 780.8
20
470.8 431.8 455.1 559.45 393.9 388.2
Tweets
40
580.3 508.6 520.1 578.25 480.1 474.1
60
615.5 577.1 585.2 607.41 552.7 546.8
ST LDA D and ST LDA C lower bound the other curves on all iterations . On the NIPS dataset , DTM becomes competitive with the two others , at the end of the iterations , while on TDT4 , where test documents come in a stream , ST LDA C stands clearly as the best model . These results show the ability of ST LDA C to capture dependencies between topics in document streams . Further , we note that at the beginning of iterations where dependencies are not yet apparent , the perplexity curves of both models are very similar to the one of LDAall . This is in line with our assertion of the previous section supporting that both models reduce to LDA in the case where topics are independent . TM LDA is not competitive in this setting as it does really not make advantage of the fact that the words in the new , arriving documents are known . Its ability to predict future topics is not exploited in this setting .
The evolution of perplexity on Tweets from the three last consecutive days considered in our experiments is shown in Figure 3 . The behavior of perplexity curves here are accentuated with the total stream characteristics of Tweets ; the curve of LDAall gets away from those of ST LDA C and STLDA D , while DTM comes close . In order to see if the number of topics , that we fixed for all models to 80 , have an impact on these results or not , we repeated the experiments by varying the number of topics in the set {20 , 40 , 60} .
Tweets
TM LDA
LDAall DTM
ST LDA D
ST LDA C y t i x e l p r e P
680
660
640
620
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 1.2 ·104
Number of Tweets in the test set
Figure 3 : Perplexity of each method by number of tweets that are added to the test set ( 80 topics ) .
Table 2 depicts the perplexities of all models on the three collections at the end when the parameters φ and θ have been estimated over all the test documents . In all experiments , best results are obtained with ST LDA C and STLDA D , followed by DTM on NIPS and TDT4 and by LDAall on Tweets . These results are consistent with those of the figures 2 and 3 . Again , TM LDA does not perform well ( as explained before ) ; LDAall which is a standard LDA model , performs relatively well ; however , both DTM and the ST LDA [D|C ] models outperform it by taking into account dependencies between topics . We see here that the extra flexibility of the ST LDA [D|C ] models allow them to outperform DTM . 4.2 Ability to detect semantic correlations
We further investigate on the ability of models to find topics that can detect documents of the same semantic class . For doing so , we used the TDT4 collection for which some documents are assigned semantic classes by experts . We hence use the cosine measure or the λd parameter of STLDA C , to detect consecutive documents in the test set of this collection that are found similar on the basis of their topic distributions ; two consecutive documents are considered as similar if the cosine measure of their topic distributions ( resp . estimated λd line 13 Algorithm 1 ) is higher than a given threshold . If two consecutive and similar documents share the same semantic label , we count them as a true positive ; if they do not share the same semantic label , we count them as false positive . By changing the threshold , we can plot the ROC curves for the corresponding method . Figure 4 depicts ROC curves of DTM , TM LDA and ST LDAC defined over 8 different thresholds taken in the set [ 0.2 0.5 0.7 0.86 0.89 0.92 0.95 0.98 ] for the cosine measure and [ 0.5 1 2 5 10 15 20 50 ] for λd when the number of topics is fixed to 20 and to 80 .
In order to compare between the different ROC curves , we estimated the area under them , shown in Table 3 . From these results it comes clear , that topic distributions found by ST LDA C are more able to detect these semantic classes than topic distributions of DTM and TM LDA .
Table 3 : Areas under the ROC curves of figure 4 .
Methods ST LDA C with λd ST LDA C with cosine TM LDA with cosine DTM with cosine
20 ( Fig 4 , left )
80 ( Fig 4 , right )
0.7982 0.8004 0.7652 0.7357
0.8306 0.7755 0.7349 0.6301
Finally , to further illustrate the role of λd , we pictorially illustrate the correlation between the estimated λd and the topic distributions of three consecutive documents ( Figure 5 ) with identical labels in the TDT4 collection . As one can
701 TDT4
TDT4
) l l a c e R ( e t a R e v i t i s o P e u r T
1
0.8
0.6
0.4
0.2
0
0
) l l a c e R ( e t a R e v i t i s o P e u r T
1
0.8
0.6
0.4
0.2
0
0
ST LDA C with λd ST LDA C with cosine TM LDA with cosine DTM with cosine
0.2
0.4
0.6
0.8
1
ST LDA C with λd ST LDA C with cosine TM LDA with cosine DTM with cosine
0.2
0.4
0.6
0.8
1
False Positive Rate ( Fall Out )
False Positive Rate ( Fall Out )
Figure 4 : ROC curves of ” semantic class matching ” methods working over the topic distributions found by DTM , TM LDA and ST LDA C , for the number of topics fixed to 20 ( left ) and 80 ( right ) . see , the distributions of topics in the three pairs of consecutive documents with high λd are similar . In addition , the two most probable topics of the document pairs retained in Figure 6 , also taken from TDT4 , do not share any word when λd is small and are almost identical when λd is high . These examples illustrate the fact that λd is a good indicator of the topic dependencies between documents . 5 . RELATED WORK
Some studies have considered the possibility to model different streams of documents , as in [ 10 ] , trying to leverage standard models ( as LDA ) by considering topics common to the different streams . In such studies the evolution of topics over time is not considered . The study presented in [ 22 ] aims at modeling , through an extension of LDA , the timestamp associated with each token in a document . If dependencies between topics are not explicitly modeled , topics tend to specialize over different time periods through the joint dependence of each word and timestamp on the topic variable ( z in LDA ) . Other studies have addressed the problem of topic evolution and dependencies within a single document , as the recent sequential LDA model described in [ 7 ] . We rather focus in this study on explicitly modeling topic dependencies across documents , for both topic and wordtopic distributions . Several studies have addressed a similar problem . One of the first proposals corresponds to the Dynamic Topic Model ( DTM ) , introduced in [ 4 ] and illustrated in Figure 1 . An interesting feature of DTM is its use of time slices ; we have not considered time slices in this study , but our models ( as most dynamic models ) can be extended to deal with them . DTM captures dependencies for both topic and word topic distributions . These dependencies are however captured through Gaussian distributions , the expectation of which corresponds to the previous parameters . This entails that new parameter values are constrained to be distributed around the values observed previously . In contrast , even in model ST LDA D , the expectations of the new topic and word topic distributions ( Eqs . 2 and 4 ) can be uncor related to the previous distributions in the absence of dependencies . Our models thus offer additional flexibility over the presence or absence of dependencies between consecutive documents in a stream . The Dynamic Mixture Model ( DMM , see Fig 1 ) introduced in [ 25 ] is similar to DTM except that topic dependencies are directly considered at the topic level ( as is the case for ST LDA D and ST LDA C but not for DTM which operates at the prior level ) and that word topic dependencies are dropped . As for DTM , the expectation of a new topic distribution is given by the values obtained in the previous document . This again contrasts with our proposal that introduces additional flexibility , as mentioned before . The Topic Tracking Model ( TTM , see Fig 1 ) introduced in [ 11 ] is similar to our models in the sense that both topic and word topic ( more precisely interest topic ) dependencies are considered . However , as for DTM and DMM , the mean of the current topics and interests are the same as the ones of the previous topics and interests . The model is thus again limited in its ability to model the presence or absence of dependencies between consecutive documents . A more recent proposal , called Temporal LDA ( TM LDA , see Fig 1 ) , was introduced in [ 24 ] . TM LDA differs from the previous models as it also aims at predicting future topics even in the situation where future documents are not seen . It thus assumes a strong dependency between consecutive documents , which is not always realistic , even on such collections as Tweets . Furthermore , TM LDA does not consider dependencies for the word topic distributions .
6 . CONCLUSION
We have proposed in this paper two new models for modeling topic and word topic dependencies between consecutive documents in document streams . The first model is a direct extension of Latent Dirichlet Allocation model ( LDA ) and makes use of a Dirichlet distribution to balance the influence of the LDA prior parameters wrt to topic and wordtopic distribution of the previous document . The second extension makes use of copulas , which constitute a generic
702 5 t n e m u c o D f o t s i D c i p o T
6 t n e m u c o D f o t s i D c i p o T
1
0.8
0.6
0.4
0.2
0
0
1
0.8
0.6
0.4
0.2
0
0
5
10
15
20
5
10
15
20
1
0.8
0.6
0.4
0.2
4 2 t n e m u c o D
0
0
1
0.8
0.6
0.4
0.2
5 2 t n e m u c o D
0
0
5
10
15
20
5
10
15
20
1
0.8
0.6
0.4
0.2
2 3 t n e m u c o D
0
0
1
0.8
0.6
0.4
0.2
3 3 t n e m u c o D
0
0
5
10
15
20
5
10
15
20
Lambda Dependency = 1730
Lambda Dependency = 5184
Lambda Dependency = 26881
Figure 5 : Topic distribution of three pairs consecutive documents that have the same topic ( Olympic left , Election middle , Sport right ) and subject labels in TDT4 dataset ( 20 topics ) .
DocId=307
DocId=308
DocId=362
DocId=363 mutual merk medtronic support financial heavy palestinian israeli army force bush voter gore american governor voter campaign campaign american bush
λ = 1
λ = 525
Figure 6 : 5 most frequent words of the most probable topic ( 20 topics ) . tool to model dependencies between random variables . Our experiments , conducted on three standard collections that have been used in several studies on topic modeling , show that our proposals outperform previous ones ( as dynamic topic models and temporal LDA ) , both in terms of perplexity and for tracking similar topics in a document streams . Compared to previous proposals , our models have extra flexibility and can adapt to situations where there is in fact no dependencies between the documents .
In the future , we plan to develop non parametric extensions as well as versions of these models that scale well , following the improvements on the inference methods for LDA , proposed in streams [ 26 ] or in online settings [ 9 , 2 ] .
7 . ACKNOWLEDGMENTS
We thank the anonymous reviewers for their useful com ments . This work was partly supported by the LabEx PERSYVALLab ANR 11 LABX 0025 .
8 . REFERENCES [ 1 ] A . Asuncion , M . Welling , P . Smyth , and Y . W . Teh .
On smoothing and inference for topic models . In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence , UAI , 2009 .
[ 2 ] A . Banerjee and S . Basu . Topic models over text streams : A study of batch and online unsupervised learning . In Proceedings of the 7th SIAM conference on Data Mining , SDM , 2007 .
[ 3 ] D . M . Blei . Free C++ implementation for dtm . https :
//wwwcsprincetonedu/˜blei/topicmodelinghtml
[ 4 ] D . M . Blei and J . D . Lafferty . Dynamic topic models . In ACM International Conference Proceeding Series , ICML , 2006 .
[ 5 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent dirichlet allocation . J . Mach . Learn . Res . , 2003 .
[ 6 ] S . Derrode and W . Pieczynski . Unsupervised data classification using pairwise markov chains with automatic copulas selection . Computational Statistics & Data Analysis , 2013 .
[ 7 ] L . Du , W . L . Buntine , and H . Jin . Sequential latent dirichlet allocation : Discover underlying topic structures within a document . In IEEE Computer Society , ICDM , 2010 .
[ 8 ] M . M . Gaber , A . Zaslavsky , and S . Krishnaswamy .
Mining data streams : A review . ACM SIGMOD Record , 2005 .
[ 9 ] M . D . Hoffman , D . M . Blei , and F . Bach . Online learning for latent dirichlet allocation . In NIPS , 2010 .
[ 10 ] L . Hong , B . Dom , S . Gurumurthy , and
K . Tsioutsiouliklis . A time dependent topic model for multiple text streams . In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD , 2011 .
[ 11 ] T . Iwata , S . Watanabe , T . Yamada , and N . Ueda .
Topic tracking model for analyzing consumer purchase behavior . In Proceedings of the 21st International Jont Conference on Artifical Intelligence , IJCAI , 2009 .
[ 12 ] M . Lichman . UCI machine learning repository , 2013 . [ 13 ] A . J . McNeil . Sampling nested Archimedean copulas .
Journal of Statistical Computation and Simulation , 2008 .
703 [ 14 ] A . J . McNeil and J . Neˇslehov`a . Multivariate
Archimedean copulas , D monotone functions and 1 norm symmetric distributions . Annals of Statistics , 2009 .
[ 15 ] R . Neal . Slice sampling . Annals of Statistics , 2000 . [ 16 ] R . B . Nelsen . An introduction to copulas . Springer
Science & Business Media , 2007 .
[ 17 ] K . W . Ng , G L Tian , and M L Tang . Dirichlet and related distributions : Theory , methods and applications . John Wiley & Sons , 2011 .
[ 18 ] O . Ostap , O . Yarema , and S . Wolfgang . Properties of hierarchical Archimedean copulas . Statistics & Risk Modeling , 2013 .
[ 19 ] M . J . Paul and M . Dredze . You are what you tweet : Analyzing twitter for public health . In International Conference on Weblogs and Social Media , 2011 .
[ 20 ] T . Sakaki , M . Okazaki , and Y . Matsuo . Earthquake shakes twitter users : Real time event detection by social sensors . In International Conference on World Wide Web , 2010 .
[ 21 ] H . M . Wallach , D . M . Mimno , and A . McCallum .
Rethinking LDA : why priors matter . In Advances in Neural Information Processing Systems Conference , 2009 .
[ 22 ] X . Wang and A . McCallum . Topics over time : A non markov continuous time model of topical trends . In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD , 2006 .
[ 23 ] Y . Wang . Distributed gibbs sampling of latent topic models : The gritty details . Technical report , 2008 .
[ 24 ] Y . Wang , E . Agichtein , and M . Benzi . TM LDA :
Efficient online modeling of latent topic transitions in social media . In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD , 2012 .
[ 25 ] X . Wei , J . Sun , and X . Wang . Dynamic mixture models for multiple time series . In Proceedings of the 20th International Joint Conference on Artificial Intelligence , IJCAI , 2007 .
[ 26 ] L . Yao , D . Mimno , and A . McCallum . Efficient methods for topic model inference on streaming document collections . In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD , 2009 .
APPENDIX A . METROPOLIS HASTING PROCEDURE The Metropolis Hasting procedure is based on the follow ing steps :
1 . Generate an initial value of x : draw x1 ∼ Pprior(x )
2 . Initialize j = 1
3 . Repeat till sequence is stable
( a ) Draw x ∼ q , where q represents the ” jump ” function ( b ) Draw u ∼ U [ 0 , 1 ] ( c )
 Π(xj )q(x )
Π(x)q(xj ) Π(x)q(xj ) Π(xj )q(x )
α =
( d ) If u ≤ α , then xj+1 = x ; xj+1 = xj otherwise
For x = λd , one has :
P ( λd|θd−1 , θd , zd , wd , α , β , φd−1 , φd , µd ) ∝ Pprior(λd)P ( θd|θd−1 , α , λd ) := Π(λd ) where Pprior(λd ) ∼ U [ 0 , τλ ] . As λd should be higher when θd−1 and θd are more similar ( as in such a case the influence of θd−1 on θd is more important ) , we make use of the following jump function , based on the exponential distribution : q(λd ) = ( 1 − cos(θd−1 , θd ) ) × e
−(1−cos(θd−1,θd))×λd
For x = µd , the same distribution is used for the jump function , the cosine being taken between the vectors that correspond to the column wise concatenation of the columns of each matrix φd−1 and φd . The prior this time is P ( µd ) ∼ U [ 0 , τµ ] . Lastly , for x = T d k ) ∼ Ga(α ) , the jump function corresponds to Franck copula , and Π(T d k ) corresponds to the kth contribution in Eq 10 . k , Pprior(T d
B . GIBBS SAMPLING UPDATES ( ST LDA C ) We provide here the complete derivation of Eq 10 . For any d ≥ 2 , one has : T d ∼ P ( T d|T d−1 , zd , wd , α , β , λd , φd−1 , φd , µd )
P ( T d−1|α)P ( T d|T d−1 , α , λd)P ( zd|T d)P ( wd|zd )
P ( T d−1|α)p(zd|α)P ( wd|zd )
=
=
P ( T d|T d−1 , α , λd)P ( zd|T d )
P ( zd|α )
Let Fα ( resp fα ) denote the cdf ( resp pdf ) of the Gamma distribution with parameters ( α , 1 ) . By assumption :
P ( T d|T d−1 , α , λd ) = fα(T d k )cλ(Fα(T d−1
) , Fα(T d k ) ) k
K K N k=1
θd zd n k=1 T d k
=
K
, :
−N N
T d k
T d zd n n=1 k=1 n=1
P ( zd|θd)P ( θd|α)dθd =
B(Ωd + α )
B(Ωd )
−NN k=1 T d k=1 Γ(α ) k n=1 T d zd n
×
B(Ωd + α)/B(Ωd )
α−1
−T d k cλ(Fα(T d−1
) , Fα(T d k ) ) k exp
−NK
K k=1 k k=1 T d k=1 Γ(α ) k=1 T d k
Ωd,k+α−1
×
B(Ωd + α)/B(Ωd ) k=1 T d k cλ(Fα(T d−1 k
) , Fα(T d k ) ) and , since θd = T d/
P ( zd|T d ) =
Further , as usual [ 23 ] :
P ( zd|α ) =
Hence : p(T d|T d−1 , zd,··· ) =
=
K K K K K −K
T d k=1 k exp if Π(xj)q(x ) < Π(x)q(xj ) otherwise leading to the desired result .
704
