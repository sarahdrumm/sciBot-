Generalized Hierarchical Sparse Model for Arbitrary Order Interactive Antigenic Sites Identification in Flu Virus Data
Lei Han† , Yu Zhang‡ , Xiu Feng Wan§ , Tong Zhang†¶
†Department of Statistics , Rutgers University
‡Department of Computer Science and Engineering , Hong Kong University of Science and Technology
§College of Veterinary Medicine , Mississippi State University lhan@statrutgersedu ; zhangyu@cseusthk ; wan@cvmmsstateedu ; tzhang@statrutgersedu
¶Baidu Inc . Beijing , China
ABSTRACT Recent statistical evidence has shown that a regression model by incorporating the interactions among the original covariates ( features ) can significantly improve the interpretability for biological data . One major challenge is the exponentially expanded feature space when adding high order feature interactions to the model . To tackle the huge dimensionality , Hierarchical Sparse Models ( HSM ) are developed by enforcing sparsity under heredity structures in the interactions among the covariates . However , existing methods only consider pairwise interactions , making the discovery of important high order interactions a non trivial open problem . In this paper , we propose a Generalized Hierarchical Sparse Model ( GHSM ) as a generalization of the HSM models to learn arbitrary order interactions . The GHSM applies the 1 penalty to all the model coefficients under a constraint that given any covariate , if none of its associated kth order interactions contribute to the regression model , then neither do its associated higher order interactions . The resulting objective function is non convex with a challenge lying in the coupled variables appearing in the arbitrary order hierarchical constraints and we devise an efficient optimization algorithm to directly solve it . Specifically , we decouple the variables in the constraints via both the GIST and ADMM methods into three subproblems , each of which is proved to admit an efficiently analytical solution . We evaluate the GHSM method in both synthetic problem and the antigenic sites identification problem for the flu virus data , where we expand the feature space up to the 5th order interactions . Empirical results demonstrate the effectiveness and efficiency of the proposed method and the learned high order interactions have meaningful synergistic covariate patterns in the virus antigenicity .
Keywords High Order Interaction ; Heredity Structure ; Hierarchical Sparsity
1 .
INTRODUCTION
Fitting a linear regression model to the response based on a number of covariates ( features ) is a commonly used tool in statistical analysis . However , in numerous situations , a linear model on the covariates may be not sufficiently enough to provide comprehensive explanations for the data and to make accurate predictions . For example , in the influenza antigenic sites identification problem , a mutation at an individual antigenic site ( covariate ) is less deterministic to change the phenotypic behavior ( antigenic change ) of the influenza virus . Instead , multiple simultaneous mutations at different antigenic sites will significantly enhance the antigenic drift , and strong interactions among the antigenic sites are observed during the virus evolution .
Actually , recent statistical results have shown that studying the feature interactions in a learning model can significantly enhance its interpretability for the data and improve the prediction accuracy [ 6 , 15 , 13 ] . Generally , the interaction effects are represented as the elementwise product among the covariates and for example , the second order interaction between two covariates xi and xj is represented by their elementwise product xi fi xj . Hence , the interactions can encourage capturing the nonlinearity in the data . The interactions among covariates have been found to play an important role in various areas . For example , strong evidences have been found in [ 2 ] that the genetic environmental interactions have significant effects on conduct disorders , and similar results are reported in [ 5 ] that the genetic environmental interactions in serotonin system are highly correlated with the adolescent depression . Moreover , in [ 19 ] , considering the interaction between the continuance commitment and affective commitment is shown to be effective in predicting the absenteeism . Recently , in the antigenic sites identification problem [ 23 ] , interactions among co evolved antigenic sites are proved to be critical to quantify the impact of multiple simultaneous mutations .
As the study of the interactions among covariates gains increasing attentions , a major challenge is the exponentially expanded feature space . That is , when considering the kth order interactions among covariates , the number of interactions is O(dk ) with respect to the d covariates . Such a large number of interactions make the learning model computationally demanding even when d and k are very small . One promising strategy is to exploit sparse structure under this scenario , since only a subset of the covariates and the interactions could be of interest . A simple way is to directly apply the Lasso [ 21 ] method by treating all the covariates and the interactions equally , which is referred to the all pairs Lasso [ 1 , 23 ] . Furthermore , since the interaction effects are generated from the covariates and the higher order interaction effects originate from lower order ones , logical heredity relationship among those effects could be taken into account instead of treating them equally .
In order to make use of the heredity structure , statisticians favor the sparsity which obeys certain logical heredity constraints ,
865 referring to the situation that if a set of parameters are estimated as zeros , then the set of its dependent parameters based on some certain heredity relationship should also be set to zeros . Accordingly , a number of Hierarchical Sparse Models ( HSM ) have been developed . For example , in [ 15 ] , a convex Lasso style method named VANISH is proposed by enforcing the strong heredity constraint for the second order interactions that if a second order interaction is added to the model , then both the corresponding covariates must be included as well . Consequently , many convex formulations , including the glinternet [ 11 ] , GRESH [ 16 ] , FAMILY [ 9 ] , and the hierarchical sparse model [ 22 ] , incorporate the strong heredity into the second order interactions with the heredity structure achieved via the group sparsity [ 25 ] , where the covariate and interactions restricted by a heredity constraint form a group . Similar considerations are discussed by [ 26 , 24 ] . On the other hand , in contrast to those convex models , the SHIM method [ 3 ] adopts a non convex formulation to achieve the strong heredity by decomposing the coefficient of each interaction into a product of the coefficients for the covariates . In addition to the strong heredity , there is another type of hierarchical relation , the weak heredity , which introduces a constraint that a pairwise interaction is considered if either of its corresponding covariates was included . Both the strong and weak heredity are investigated in [ 1 ] and an efficient algorithm to handle the weak heredity is introduced in [ 12 ] .
So far , many interests have been focused on exploring the sparse heredity in the interaction model but none of them can deal with general hierarchies , since all of them study only the second order interactions and their algorithms are particularly designed for the second order interactions . On the other hand , there have been sufficient evidences to indicate that higher order interactions are more important in many applications . For example , in psychological analysis , the third order interactions among covariates have been shown to be important [ 4 ] . Specifically , in antigenicity analysis of influenza virus , a recent study on proteins of the H3N2 influenza virus shows that more than two of the amino acid positions could mutate simultaneously [ 17 ] and biological evidences in [ 14 ] also demonstrate that the co evolved antigenic sites are more likely to be physically close in the 3D structure of the protein .
Unfortunately , due to the difficulty in defining and learning with the high order heredity , we are unaware of any existing work that can deal with general hierarchies with the order of feature interactions larger than two and there is even no formal definition for the arbitrary order heredity . In this paper , we propose a Generalized Hierarchical Sparse Model ( GHSM ) to tackle arbitrary order interactions among features . We first introduce the definition of the arbitrary order heredity , which makes an assumption that given any covariate , if none of its associated kth order interaction effects contribute to a learning model , then neither do its associated higherorder interaction effects . Based on this definition , we formulate the GHSM model by applying the 1 penalty to all the coefficients under certain hierarchical chain constraints , which guarantee the arbitrary order heredity . The resulting problem is non convex and not easy to be optimized since the number of variables in the optimization problem increases dramatically when the order of interactions becomes bigger , which poses a computational challenge . To optimize the objective function , we use the GIST method [ 7 ] where the proximal operator is solved by the ADMM method . In the three subproblems of the ADMM method , the first two need to solve quadratic programming problems and the last one is a least square problem with a hierarchical chain constraint . After analysis , we show that all the three subproblems admit efficiently analytical solutions . In the experiments , we evaluate the GHSM method in both synthetic problem and the antigenic sites identification prob lem in influenza virus data , and empirical results show that the GHSM method can capture meaningful synergistic covariate patterns , which can be well explained by biological knowledge .
2 . PRELIMINARIES
Throughout this paper , we use regular letters to denote scalars , bold face and lowercase letters for vectors , and bold face and uppercase letters for matrices or tensors . Suppose the data matrix for training is denoted by X = ( x1,··· , xd ) ∈ Rn×d , where n is the number of samples , d is the feature dimensionality , and xi records the values for the ith covariate in the n data samples . The response vector is y ∈ Rn . The second order interaction models [ 15 , 3 , 1 , 11 , 12 , 16 , 9 , 22 ] commonly consider the following regression model : d i=1 d i=j y =
βixi +
φi,j ( xi fi xj ) + ε ,
( 1 ) where fi denotes the elementwise product between vectors , β ∈ Rd with βi as its ith element is the coefficient vector for the covariates , Φ ∈ Rd×d with φi,j as its ( i , j)th element is the coefficient matrix for the pairwise interaction effects , and ε ∼ N ( 0 , σ2I ) is a Gaussian noise vector . In the existing works , two types of heredity structure are considered for the second order interactions , ie the strong heredity and the weak heredity , whose definitions are as follows : strong heredity : φi,j = 0 =⇒ βi = 0 and βj = 0 , or equivalently , βi = 0 or βj = 0 =⇒ φi,j = 0 ; weak heredity : φi,j = 0 =⇒ βi = 0 or βj = 0 ,
( 2 )
( 3 ) or equivalently , βi = 0 and βj = 0 =⇒ φi,j = 0 .
Based on Eqs . ( 2 ) and ( 3 ) , the HSM methods introduced in [ 1 , 12 ] , named as the strong and weak hierNet , explicitly enforce the heredity structure by adding inequality or symmetry constraints to the Lasso method as strong hierNet : min β,Φ weak hierNet : min β,Φ
λ 2 l(β , Φ|X , y ) + λβ1 +
Φ1 st Φ = Φ , |βi| ≥ Φi,·1 ∀i ∈ Nd ; Φ1 l(β , Φ|X , y ) + λβ1 + st |βi| ≥ Φi,·1 ∀i ∈ Nd ,
λ 2
( 4 )
( 5 ) where l(· ) is a loss function based on Eq ( 1 ) , λ is a regularization parameter that controls the sparsity , Nd denotes the set of integers {1,··· , d} , · 1 denotes the 1 norm of a vector or matrix , and Φi,· denotes the ith row of Φ . The only difference between problems ( 4 ) and ( 5 ) is the existence of the symmetry constraint on Φ . It is not hard to see that the constraints in problems ( 4 ) and ( 5 ) can guarantee the strong and weak heredity defined in Eqs . ( 2 ) and ( 3 ) . The strong and weak hierNet methods are representatives of the second order HSM methods which are explicitly enforced to obey the heredity structure .
In the hierNet models , only the second order interaction is considered . As discussed in the previous section , higher order interactions are important to model the biological data . To the best of our knowledge , there is no work to even define the high order interactions . In the next section , we will first provide a formal definition of the arbitrary order heredity and then introduce our method to model the arbitrary order interactions .
866 3 . THE GHSM covariates ( K d ) , and the regression model is formulated as
Here we consider up to the Kth order interactions among the
θ(2 ) i1,i2z(2 ) i1,i2 + · · ·
( 6 ) i1,i2,··· ,iKz(K ) θ(K ) i1,i2,··· ,iK + ε , y = d i=1
+ d i1<i2
θ(1 ) i z(1 ) i + d i1<i2<···<iK
Table 1 : The ordering of elements in θ(k ) .
The index in θ(k ) d − k + 1 d − k + 2
1 2 · · ·
,d
· · · k
The corresponding interaction index
1 , · · · , k − 1 , k
· · ·
1 , · · · , k − 1 , k + 1 1 , · · · , k − 2 , k − 1 , d 1 , · · · , k − 2 , k , k + 1 d − k + 1 , · · · , d − 1 , d
· · · i = xi , z(k ) length,d i1,i2,··· ,ik = xi1 fi xi2 fi ··· fi xik dewhere z(1 ) notes a data vector for the kth order interaction corresponding to i1,··· , ik , an interaction index i1,··· , ik , where i1 < ··· < ik , is an index to uniquely indicate the interaction among the cok ) for k = 1,··· , K is a vector of variates i1,··· , ik , θ(k ) ∈ R(d k!(d−k)! with θ(k ) i1,··· ,ik as its element corresponding to the index i1,··· , ik . In Eq ( 6 ) , each interaction term only corresponds to one model coefficient and hence the number of model parameters is reduced from O k=1 dk K
K
=
,d to O k=1 d!
. k k
3.1 Arbitrary Order Heredity
Based on the regression model in Eq
( 6 ) , we will define the arbitrary order heredity in this section . We first introduce some notations . For i1 < ··· < ik and j ∈ {i1,··· , ik} , i1,··· , ik ∪j and j ∪ i1,··· , ik are the indices for the interaction effect among covariates i1,··· , ik and j , which adds j into i1,··· , ik by preserving the ascending order . Similarly , i1,··· , ik\j defines the index by removing the element j from i1,··· , ik , where j ∈ {i1,··· , ik} . If we follow the concept of the strong and weak heredity for the second order case in Eqs . ( 2 ) and ( 3 ) , a straightforward definition for the strong arbitrary order heredity can be formulated as i1,··· ,ik = 0 =⇒ ∀j ∈ {i1 , · · · , ik} , θ(k−1 ) θ(k ) or equivalently , i1,··· ,ik = 0 =⇒ ∀j ∈ {i1 , · · · , ik} , θ(k+1 ) i1,··· ,ik∪j = 0 , ( k ≥ 1 ) . θ(k ) K−1 Similar extension can be made for the weak heredity for arbitraryorder case as well . Unfortunately , the above definition will lead to
constraints if the interactions up to the Kth order are i1,··· ,ik\j = 0 , ( k ≥ 2 ) ,
,d considered , which makes the problem intractable to be solved .
So , in order to represent the heredity structure in arbitrary order case , we propose a more concise and intuitive definition as follows . First we define an ordering of the elements in θ(k ) in a way following the index principle in Table 1 . That is , the 1st element in θ(k ) indicates the element with the interaction index 1,··· , k − 1 , k , the 2nd element in θ(k ) indicates the one with the interaction index 1,··· , k − 1 , k + 1 , and so on . Based on this ordering , we define i ∈ R(d k ) as a 0/1 binary vector for the ith covariate where if e(k ) i appears in an interaction index of θ(k ) , then the corresponding element in e(k ) is set to 1 while the rest entries in e(k ) are 0 . k=1 k i i
Then we introduce the definition for the arbitrary order heredity . DEFINITION 1
( ARBITRARY ORDER HEREDITY ) . Given the regression model in Eq ( 6 ) , when up to the Kth order interactions among the covariates ( K ≥ 2 ) are considered , the heredity among the K orders is defined as fi θ(k+1 ) = 0 , ∀i ∈ Nd , k ∈ NK−1 . i fi θ(k ) = 0 =⇒ e(k+1 ) e(k ) In Definition 1 , for any covariate i , if none of its associated kthorder ( k < K ) interaction terms contribute to the regression model , i
Figure 1 : A pictorial illustration of the relationship among the strong , weak and arbitrary order heredity . k then neither do its associated higher order interaction terms . The arbitrary order heredity poses constraints on sets of variables associated with each covariate i instead of each individual interaction coefficient and hence it only leads to ( K − 1)d constraints , whose size is much smaller thanK−1
, which is the size of
,d constraints induced by the straightforward strong arbitrary order heredity discussed before , making the learning model have much lower complexity as we will see later . k=1
When K is set to 2 , it is easy to see that the arbitrary order heredity in Definition 1 degenerates to the strong heredity in Eq ( 2 ) and hence the arbitrary order heredity is a generalization of the strong second order heredity . Fig 1 gives an example to explain the relationship among the strong , weak and arbitrary order heredity where the circle denotes the coefficient of a covariate ‘1’ , ovals denote the coefficients of the interactions involving the covariates ‘1’ , and the red arrows indicate the heredity . From the figure , we see that when K = 2 , the top 2 layers in the arbitrary order heredity degenerates to the strong heredity structure by eliminating the redundancy among the model coefficients . 3.2 The Model K λ αk i fi θ(2)1 ≥ · · · ≥ e(K )
( 6 ) and Definition 1 , we formulate the GHSM model for up to the Kth order interactions as i fi θ(K)1 , i ∈ Nd ,
Based on Eq
θ(k)1 ,
| ≥ e(2 ) st |θ(1 )
L(Θ ) + min Θ
( 7 ) k=1 i d
2y −K where λ and α are two regularization parameters controlling the sparsity and the decay in the coefficients for interactions of different orders , Θ denotes the set of parameters {θ(k)}K k=1 , and L(· ) is a loss function for regression such as the square loss defined as 2 where L(Θ ) = 1 · 2 denotes the 2 norm of a vector . In problem ( 7 ) , the constraints associated with each covariate i have a chain of inequality constraints , which contains ( K−1 ) inequality constraints and there are a total of d chains . It is easy to see that these constraints achieve the arbitrary order hierarchy in Definition 1 . i1,··· ,ik2
θ(k ) i1,··· ,ikz(k ) i1<···<ik k=1
867 K k=1
K k=1
Problem ( 7 ) is non convex due to the chains of inequality constraints . Moreover , the variables are coupled in different chains of constraints , eg , the variable θ(3 ) 1,2,3 appears in three chains associated with the covariates 1 , 2 , 3 respectively , which makes the problem more complex . In the next section , we propose an efficient algorithm to solve problem ( 7 ) .
4 . OPTIMIZATION ALGORITHM
In this section , we introduce the optimization algorithm to solve problem ( 7 ) . The main idea is to combine proximal gradient methods and the ADMM . Since problem ( 7 ) is non convex , we adopt the GIST method [ 7 ] whose entire algorithm is shown in Algorithm 1 . We define r(Θ ) as r(θ ) =
λ
αk θ(k)1 , if the constraint in Eq ( 7 ) is satisfied ; otherwise . k=1
+∞ , fl K
Then , the proximal operator at the ( t + 1)th iteration in the GIST method solves the following problem :
Θ(t+1 ) = arg min Θ
τt 2
θ(k ) − v(k)2
2 + r(Θ ) ,
( 8 )
τt ily computed as [ Z(k)](K where [ θ(k)](t ) denotes the estimation of θ(k ) in the tth iteration , v(k ) = [ θ(k)](t ) − 1 ∇θ(k ) L(Θ(t) ) , ∇θ(k ) L(Θ(t ) ) denotes the gradient of L(Θ(t ) ) with respect to θ(k ) at Θ(t ) and it can be eask=1 Z(k)θ(k ) − y ) for k ∈ NK , Z(k ) ∈ Rn×(d k ) is the matrix containing all the kth order interaction effects , and τt is a step size determined via a line search method by satisfying the following condition :
F ( Θ(t+1 ) ) ≤ F ( Θ(t ) ) − ϕτt 2
[ θ(k)](t+1 ) − [ θ(k)](t)2 2 ,
( 9 ) where F ( Θ ) = L(Θ ) + r(Θ ) and ϕ is a constant in ( 0 , 1 ) . Then , the GIST algorithm iteratively solves the proximal problem ( 8 ) until convergence .
Algorithm 1 The GIST algorithm for solving problem ( 7 ) . Input : X , y , K , = 10−4 ; Output : ˆΘ ; 1 : Initialize Θ(0 ) , η > 1 , 0 < τmin < τmax , ϕ ∈ ( 0 , 1 ) , t = 0 ; 2 : repeat 3 : 4 : 5 : 6 : 7 : 8 : 9 : until F ( Θ(t ) ) − F ( Θ(t+1 ) ) < ; 10 : ˆΘ = Θ(t ) ;
Solve the proximal problem ( 8 ) with Θ(t ) and τt ; τt = ητt ; until condition ( 9 ) is satisifed ; t = t + 1 ;
τt ∈ [ τmin , τmax ] ; repeat
4.1 Solving The Proximal Problem The key problem in Algorithm 1 is to solve the proximal problem ( 8 ) . Since r(· ) is an extended real value function , problem ( 8 ) can be reformulated as
K
K min Θ
τ 2
θ(k ) − v(k)2
2 +
θ(k)1 ,
λ αk
( 10 ) k=1 k=1 st |θ(1 ) i
| ≥ e(2 ) i fi θ(2)1 ≥ · · · ≥ e(K ) i fi θ(K)1 ∀i ∈ Nd , where we omit the iterative index t for notational simplicity . Problem ( 10 ) is still non convex due to the chains of inequality constraints . However , the following theorem shows that problem ( 10 ) admits an equivalently convex formulation.1
THEOREM 1 . Let ¯θ(k ) = |θ(k)| , where the operator | · | denotes the elementwise absolute operator on a vector . Then problem ( 10 ) is equivalent with the following convex optimization problem : min
¯θ(1),··· , ¯θ(K )
τ 2
¯θ(k ) − ¯v(k)2 2 ,
K i ≥ [ e(2 ) ¯θ(k ) 0 , ∀k ∈ NK , k=1 i st ¯θ(1 )
] ¯θ(2 ) ≥ · · · ≥ [ e(K ) i
( 11 )
] ¯θ(K ) , ∀i ∈ Nd , where 1 denotes a column vector of all ones with appropriate size , ¯v(k ) = |v(k)|− λ αkτ 1 , and denotes the elementwise ‘no smaller than’ operator . The solution of problem ( 10 ) can be obtained as θ(k ) = sign(v(k ) ) fi ¯θ(k ) for k ∈ NK , where sign(· ) is the elementwise sign operator .
It is easy to find that problem ( 11 ) is a quadratic programming problem , hence many off the shelf solvers for convex programming can be used directly to obtain the optimal solution . Nevertheless , instead of using these tools , we propose an efficient algorithm to solve problem ( 11 ) by taking advantage of the chain structure in the constraints . In problem ( 11 ) , the variables are coupled together in the chains of inequality constraints . In order to decouple these parameters , we use the ADMM method to solve problem ( 11 ) by introducing new variables .
We define p(k ) = ¯θ(k ) for k ∈ NK , δi = ( [e(2 )
· · · , [ e(K ) Then , problem ( 11 ) can be reformulated as
]p(K ) ) ∈ RK−1 for i ∈ Nd , and qi = δi ∈ RK−1 . i
]p(2 ) , [ e(3 )
]p(3 ) , i i min
{ ¯θ(k)},{p(k)},{qi} st
K
( 12 ) k=1
¯θ(k ) − ¯v(k)2 2 ,
τ 2 ¯θ(k ) 0 , ∀k ∈ NK p(k ) = ¯θ(k ) , ∀k ∈ NK qi = δi , ∀i ∈ Nd i ≥ q1,i ≥ · · · ≥ qK−1,i , ∀i ∈ Nd , p(1 ) d i=1 where p(1 ) is the ith element in p(1 ) and qj,i is the jth element in qi . Based on problem ( 12 ) , we define the augmented Lagrangian function as i
K k=1
τ 2
¯L( ¯Θ , P , Q ) =
K
K k=1 d
¯θ(k ) − ¯v(k)2
2 + p(k ) − ¯θ(k)2
2
ρ1 2
+
[ a(k)](p(k ) − ¯θ(k ) ) + k=1 i=1 qi − δi2
2 +
ρ2 2 b i ( qi − δi ) . where ¯Θ denotes the set of parameters { ¯θ(k)}K k=1 , P and Q denote i=1 respectively , {a(k)}K the sets of variables {p(k)}K k=1 and {bi}d i=1 are the Lagrangian multipliers , and ρ1 and ρ2 are two penalty parameters . Then we need to solve the following problem : k=1 and {qi}d
¯θ(k ) 0 , k ∈ NK ; min
¯Θ,P,Q
¯L( ¯Θ , P , Q ) st i ≥ q1,i ≥ · · · ≥ qK−1,i , i ∈ Nd . p(1 )
The above problem can be solved via the ADMM algorithm presented in algorithm 2 , in which three subproblems in steps 4 , 5 and 6 need to be solved . In the next two sections , we will show how to solve those subproblems efficiently .
1We put all the proofs in the supplementary material at http://wwwstatrutgers edu/home/lhan .
868 Algorithm 2 The ADMM algorithm for solving problem ( 12 ) . Input : X , y , K ; Output : ˆΘ ; 1 : Initialize ¯Θ(0 ) , Q(0 ) and A(0 ) ; 2 : Set ρ = 0.1 and t = 0 ; 3 : repeat 4 : 5 : 6 : 7 : 8 : 9 : 10 : until Some convergence criterion is satisfied ;
Solve ¯Θ(t+1 ) with fixed P(t ) and Q(t ) ; Solve {P(t+1)\[p(1)](t+1)} with fixed ¯Θ(t ) , Q(t ) and [ p(1)](t ) ; Solve Q(t+1 ) and [ p(1)](t+1 ) with fixed ¯Θ(t ) , {P(t)\[p(1)](t)} ; [ a(k)](t+1 ) = [ a(k)](t ) + ρ1([p(k)](t ) − [ ¯θ(k)](t ) ) for k ∈ NK ; b(t+1 ) i t = t + 1 ;
) for i ∈ Nd ; i + ρ2(q(t ) i − δ(t )
= b(t ) i
4.2 Analytical Solutions in Steps 4 and 5
In step 4 of Algorithm 2 , with fixed P and Q , the problem with respect to ¯Θ can be rewritten as
K k=1 min ¯Θ
¯θ(k ) − τ ¯v(k ) + ρ1p(k ) + a(k )
τ + ρ1
( 13 )
2 , st ¯θ(k ) 0 , 2
, ∀k ∈ NK . which can be easily solved via the following analytical solution :
τ ¯v(k ) + ρ1p(k ) + a(k )
[ ¯θ(k)]∗ = max
0 ,
τ + ρ1
( 14 ) Given fixed ¯Θ , Q and p(1 ) , the problem with respect to p(2),··· , p(K ) corresponding to step 5 of Algorithm 2 can be decomposed into K − 1 separable problems with the problem for p(k ) formulated as min p(k )
1 2
[ p(k)]H(k)p(k ) − [ c(k)]p(k ) , k = 2 , · · · , K ,
( 15 )
2 ,··· , e(k ) where I is an identity matrix with appropriate size , the matrix E(k ) d ) ∈ R(d k)×d , and H(k ) , c(k ) are defined as 1 , e(k ) = ( e(k ) H(k ) =I + E(k)[E(k) ] , d bk−1,i + qk−1,i)e(k )
. i c(k ) = ¯θ(k ) − 1 ρ1 a(k ) +
(
1 ρ2 i=1 k k
( 16 )
E(k ) )
[ E(k ) ]
= [ H(k ) ]
−1c(k )
∗ [ p(k ) ] problem ( 15 ) can be computed as the original,d
×,d = c(k ) −
Note that H(k ) is positive definite ( PD ) . Hence , H(k ) is invertible and its inverse can be computed efficiently as [ H(k)]−1 = I − E(k)(I + [ E(k)]E(k))−1[E(k) ] , where the matrix inverse is actually taken on a d × d matrix I + [ E(k)]E(k ) instead of
matrix H(k ) . Then , the optimal solution of c(k ) ) time complexity for each
. Moreover , given the data , E(k)(I + [ E(k)]E(k))−1 ∈ R(d k)×d is a constant matrix and so it can be computed only once and stored prior to the model learning . As a consequence , the analytical so lution in Eq ( 16 ) only takes O(d,d
−1
E(k)(I + [ E(k ) ]
With fixed ¯Θ and p(2),··· , p(K ) , the problem wrt Q and p(1 )
[ ¯θ(k)]∗ , which is almost linear with respect to the number of the kth order interactions . 4.3 Efficient Solution in Step 6 d i ≥ q1,i ≥ · · · ≥ qK−1,i , ∀i ∈ Nd . p(1 ) − ( ¯θ(1 ) − a(1 ) ρ1 qi − ( δi − bi ρ2 can be formulated as min Q,p(1 ) st p(1 )
ρ2 2
ρ1 2
)2
)2
( 17 )
2 + i=1 k
2
Problem ( 17 ) can be decomposed into d independent problems with the ith one formulated as min ( 1 ) qi,p i i − a(1 ) i − ( ¯θ(1 ) p(1 )
ρ1 ρ2 i ≥ q1,i ≥ · · · ≥ qK−1,i , i ρ1
) st p(1 )
2
K−1 k=1
+ qk,i − ( δk,i − bk,i ρ2
)
2
( 18 )
K k=1 min s i i
, a(1 ) where ¯θ(1 ) and δk,i , bk,i are the kth elements in δi and bi respectively . are the ith elements in ¯θ(1 ) and a(1 ) respectively ,
In problem ( 18 ) , the chain of inequality constraints still exists .
We first rewrite this problem into a more general formulation as
ωk ( sk − uk)2 st s1 ≥ s2 ≥ · · · ≥ sK ,
( 19 ) i where problem ( 18 ) is a special case of problem ( 19 ) by setting s = ,··· , ( p(1 ) δK−1,i − bK−1,i
, q1,i,··· , qK−1,i ) ∈ RK , u = ( ¯θ(1 )
, δ1,i− b1,i i − a , 1,··· , 1 ) ∈ RK .
) ∈ RK and ω = ( ρ1
( 1 ) i ρ1
In the following , we generalize our previous results in [ 8 ] to show that an efficient solution exists for problem ( 19 ) . We first introduce two useful lemmas to reveal some interesting properties of problem ( 19 ) .
ρ2
ρ2
ρ2
LEMMA 1 . In problem ( 19 ) , the following properties hold : ( 1 ) If u1 ≥ u2 ≥ ··· ≥ uK , then the optimal solution ( s∗ 1 , s∗ 2 , K ) is ( u1 , u2,··· , uK ) ; ( 2 ) If u1 ≤ u2 ≤ ··· ≤ uK , then ··· , s∗ K K ) is ( u∗,··· , u∗)|K , where the optimal solution ( s∗ K and ( u∗,··· , u∗)|K denotes a sequence with K u∗ = identical elements u∗ .
2 , ··· , s∗
1 , s∗ k=1 ωkuk k=1 ωk
LEMMA 2 . For any two sets of inputs {(u1,··· , ul ) , ( ω1,··· , ωl)} and {(ul+1 , ··· , un ) , ( ωl+1,··· , ωn)} , which define two instances of problem ( 19 ) , if the optimal solutions for them are ( ˙u∗ , ··· , ˙u∗)|l and ( ¨u∗,··· , ¨u∗)|n−l respectively , then we have : ( 1 ) If ˙u∗ ≥ ¨u∗ , the optimal solution for the problem defined by the concatenated sequence ( u1,··· , ul ) ( ul+1,··· , un ) and concatenated weights ( ω1,··· , ωl ) ( ωl+1,··· , ωn ) is ( ˙u∗,··· , ˙u∗)|l ( ¨u∗,··· , ¨u∗)|n−l ; ( 2 ) Otherwise , ie , ˙u∗ < ¨u∗ , the optimal solution for the problem defined by the concatenated sequence is ( u∗,··· , u∗)|n , where u∗ = n n
. i=1 ωiui i=1 ωi
Lemma 2 implies that we can immediately obtain the solution of problem ( 19 ) defined by the input ( u1,··· , un ) and ( ω1,··· , ωn ) , if ( u1,··· , un ) is a concatenation from two sub sequences and the optimal solutions corresponding to problems defined by the two sub sequences have solutions with identical values . Therefore , based on Lemma 2 , we devise Algorithm 3 to solve problem ( 19 ) with its optimality guaranteed by the following theorem .
THEOREM 2 . For problem ( 19 ) defined by ( u1,··· , uK ) and
( ω1,··· , ωK ) , Algorithm 3 finds its optimal solution .
4.4 Time Complexity
We analyze the time complexity of the whole optimization procedure for solving the GHSM model . We first discuss the time complexity of the inner most Algorithm 3 . In Algorithm 3 , step 1 only needs to scan the input sequence ( u1,··· , uK ) once and thus it needs O(K ) time . Although there exist two loops from step 3 to step 14 , the maximum number of the concatenation operations in step 8 is M − 1 . For the concatenation operation , according to Lemma 2 , it only needs to compute the weighted average of the
869 1 , · · · , s∗
Algorithm 3 The algorithm for solving problem ( 19 ) . Input : ( u1 , · · · , uK ) and ( ω1 , · · · , ωK ) ; Output : ( s∗ 1 : Scan ( u1 , · · · , uK ) once to split it into M non decreasing subsequences ( t1 , · · · , tM ) and meanwhile split ( ω1 , · · · , ωK ) accordingly . Then calculate the solutions for the problems defined by those sub sequences and sub weights based on Lemma 1 ;
K ) ;
2 : Push t1 into a stack ; 3 : for m = 2 : M do 4 : 5 : 6 :
Push tm into the stack ; while there are at least two sequences in the stack do
Pop the first and second sequences from the stack and denote the solutions for their associated problems by ¨u∗ and ˙u∗ separately ; if ˙u∗ < ¨u∗ then
Concatenate the two sequences under the second condition in Lemma 2 and then push the concatenated sequence into the stack ;
7 : 8 :
9 : 10 : else
Push the two sequences back into the stack without any operation ; Break ;
11 : 12 : 13 : 14 : end for 15 : Concatenate the solutions of the sequences in the stack from bottom to end if end while top and output the concatenated solution ; entries in two sequences and we just need to record the weighted average and the sum of the weights in each sequence , making each concatenation operation cost O(1 ) . Since M ≤ K , the complexity of Algorithm 3 is O(K ) .
In Algorithm 2 , solving the subproblem in step 4 by using Eq time . The computation of step 5 by
( 14 ) requires O
K
,d k=1 using the closed form solution in Eq ( 16 ) takes O time . The computation in step 6 needs to execute Algorithm 3 for d times , and hence the time cost is O(dK ) . Usually , we have K d k=2 k k dK
,d . So the total time complexity of each ,d dK N1N2dK
,d
,d k=2
. k and hence dK < dK k iteration in Algorithm 2 is O k=2 k=2
By assuming that Algorithms 1 and 2 need N1 and N2 iterations to converge respectively , the total time complexity for solving the , which is almost linear GHSM model is O with respect to the total number of interaction effectsK
. In
,d our experiments , we empirically find that both N1 and N2 are small when convergence . Hence , the overall algorithm for solving the GHSM model is very efficient . Moreover , according to Sections 4.2 and 4.3 , the problems in steps 4 , 5 and 6 of the ADMM algorithm can be decomposed into a number of independent problems , which can be parallelized to further improve the efficiency . k=2 k k
5 . EXPERIMENTS
In this section , we empirically evaluate the proposed GHSM method and compare with a large number of the state of the art methods for hierarchical sparse modeling . Specifically , the competitors include ( 1 ) Lasso [ 21 ] , which is the sparse model using covariates only by applying the 1 penalty on the model coefficients ; ( 2 ) All Interactions Lasso ( AIL k ) , which is the sparse interaction model using up to the kth order interactions based on the 1 penalty . We concatenate all the effects together to form a new data matrix and treat it as a Lasso problem ; ( 3 ) weak hierNet ( w hierNet ) [ 1 ] , which is the HSM method using up to the 2ndorder interaction effects with the weak heredity , ie , solving the problem in Eq ( 5 ) . Its R package ‘hierNet’ is available in ‘CRAN’ ;
( 4 ) eWHL [ 12 ] , which is an efficient implementation for the whierNet method proposed in [ 1 ] ; ( 5 ) strong hierNet ( s hierNet ) [ 1 ] , which is the HSM method using up to the 2nd order interaction effects with the strong heredity , ie , solving the problem in Eq ( 4 ) . Its R package ‘hierNet’ is available in ‘CRAN’ ; ( 6 ) FAMILY [ 9 ] , which is a convex HSM model using up to the 2nd order interaction effects with the strong heredity , where the sparsity is achieved by using the group lasso . Many methods [ 11 , 16 , 9 , 22 ] have similar ideas and we select the FAMILY method as a representative . Its R package ‘FAMILY’ is available in ‘CRAN’ ; ( 7 ) GHSM k , which is the proposed GHSM models that can deal with up to the arbitrary kth order interaction effects .
The experimental platform is a 64 bit machine with 2.2 GHz quad core Intel Core i7 CPU and 16 GB memory . 5.1 Synthetic Study
In this section , we conduct experiments on synthetic datasets .
Settings
511 To study different orders of interactions , we generate 3 synthetic datasets with the highest order being K = 3 , 4 , 5 respectively . In all the datasets , the number of training samples n is set to 200 and the number of covariates d is 20 . Each entry in the data matrix for training , X ∈ Rn×d , is sampled from the standard normal distribution . The kth order interaction matrix Z(k ) ∈ Rn×(d k ) is then generated with its column indices following Table 1 . All the columns of matrices X and Z(k ) ’s are normalized to have zero mean and unit variance .
In all the 3 datasets , we set the first half of the coefficients with respect to θ(1 ) to be 1 and the rest to be 0 . For the first dataset with K = 3 , the indices of the non zero coefficients in the second order interactions are set to be 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 and 9 , 10 , ie , there are 5 non zero coefficients in θ(2 ) . Similarly , we set the indices of the non zero coefficients in the third order interactions as 1 , 2 , 3 , 2 , 3 , 4 , 4 , 5 , 6 , 5 , 6 , 7 , 7 , 8 , 9 and 8 , 9 , 10 , ie , 6 entries in θ(3 ) are non zero . All the non zero entries in θ(2 ) and θ(3 ) are set to 05 For the second dataset with K = 4 , we keep the settings for the orders up to the third order as in the case K = 3 and set the indices of the non zero coefficients in the 4th order interactions as 1 , 2 , 3 , 4 , 2 , 3 , 4 , 5 , 5 , 6 , 7 , 8 and 6 , 7 , 8 , 9 , leading to 4 non zero coefficients in θ(4 ) . All the non zero entries in θ(4 ) are also set to 05 For the third dataset with K = 5 , we adopt the same settings for up to the fourth order as in the case K = 4 and set the indices of non zero coefficients in the 5th order interactions as 1 , 2 , 3 , 4 , 5 and 2 , 3 , 4 , 5 , 6 with the corresponding entries having values of 05 It is easy to check that the above settings of the model coefficients satisfy the arbitrary order heredity . Finally , k=1 Z(k)θ(k ) + where Z(1 ) = X and ∼ N ( 0 , I/4 ) . The statistic of the interaction effects used in the synthetic data is given in Table 3 . For all the methods in comparison , we choose their regularization parameters from a set {0.1 , 0.3 , 0.5 , 1 , 3 , 5 , 10 , 30 , 50} by using additional 200 data samples as a validation set . For the GHSM method , there is another regularization parameter α , which is selected from a set {1 , 2 , 10} . To measure the performance of different methods , we use the sensitivity ( Sen . ) and the specificity ( Spe . ) [ 12 ] , where non zero entries in the corresponding coefficient vector are treated as positive and zero entries are negative , for each order of interactions to test the recovery performance on the model coefficients and use the root mean square error ( RMSE ) on a test set having 200 samples for each setting . For each setting , we repeat each configuration for 10 times and report the average results . the response vector y is constructed as y = K
870 Table 2 : The recovery and prediction performance of different methods averaged over 10 repetitions on the synthetic data . ‘−’ indicates the value is not available in the corresponding setting . Higher values of Sen . and Spe . indicates better recovery performance on non zero and zero entries , respectively . The numbers in bold denote the best results .
Synthetic data 1 : with up to the 3rd order interactions
Synthetic data 2 : with up to the 4th order interactions
Recovery on training data
1st order
Method Lasso AIL 2 w hierNet eWHL s hierNet FAMILY GHSM 2
AIL 3
GHSM 3
AIL 4
GHSM 4
Sen . 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 − −
Spe . 0.86 0.94 0.85 0.92 0.79 0.93 0.97 0.93 0.98 − −
2nd order Sen . Spe . − − 1.00 0.92 1.00 0.95 1.00 0.94 1.00 0.96 1.00 0.97 0.99 1.00 1.00 0.94 1.00 0.97 − − − −
3rd order Sen . Spe . − − − − − − − − − − − − − − 0.96 0.81 0.97 1.00 − − − −
Testing RMSE
2910±0283 1773±0244 1702±0272 1718±0241 1655±0257 1639±0211 1672±0214 1027±0224 0965±0246
− −
1st order
Sen . 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00
Spe . 0.87 0.93 0.82 0.88 0.60 0.83 0.88 0.88 0.97 0.95 0.96
Recovery on training data 3rd order 2nd order Sen . Spe . Sen . Spe . − − − − − − 1.00 0.90 − − 1.00 0.94 − − 1.00 0.95 − − 1.00 0.89 − − 1.00 0.92 − − 0.99 1.00 1.00 0.90 0.95 0.86 1.00 1.00 0.91 0.91 0.96 1.00 0.93 0.94 0.98 0.86 0.92 0.91
4th order Sen . Spe . − − − − − − − − − − − − − − − − − − 0.98 0.85 0.93 0.99
Testing RMSE
3010±0225 2006±0308 1929±0290 1907±0273 1868±0267 1891±0299 1856±0249 1704±0161 1506±0168 1484±0226 1333±0271
Synthetic data 3 : with up to the 5th order interactions
Recovery on training data
Method Lasso AIL 2 w hierNet eWHL s hierNet FAMILY GHSM 2
AIL 3
GHSM 3
AIL 4
GHSM 4
AIL 5
GHSM 5
1st order
Sen . 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00
Spe . 0.85 0.95 0.83 0.90 0.46 0.78 0.86 0.82 0.94 0.84 0.88 0.61 0.91
2nd order Sen . Spe . − − 1.00 0.88 1.00 0.94 0.95 1.00 1.00 0.89 1.00 0.90 1.00 0.95 1.00 0.85 1.00 0.92 1.00 0.89 1.00 0.89 1.00 0.68 1.00 0.81
3rd order Sen . Spe . − − − − − − − − − − − − − − 0.88 0.92 0.95 0.93 0.94 0.90 0.92 0.88 0.84 0.92 0.96 0.65
4th order Sen . Spe . − − − − − − − − − − − − − − − − − − 0.97 0.70 0.98 0.88 0.96 0.58 0.98 0.78
5th order Sen . Spe . − − − − − − − − − − − − − − − − − − − − − − 0.99 0.25 0.80 0.99
Testing RMSE
3239±0366 2279±0398 2157±0388 2111±0372 2083±0398 1979±0361 2081±0336 2087±0280 1907±0326 1835±0296 1813±0369 2074±0327 1793±0377
Table 3 : The statistic of the effects in the synthetic study . ‘#’ indicates ‘the number of’ .
# effects
# non zeros
1st 20 10
2nd 190 5
3rd 1140
6
4th 4845
4
5th
15504
2
512 Results and Analysis The detailed results are shown in Table 2 . The compared methods can be divided into 5 groups , ie , {Lasso} , {AIL 2 , w hierNet , eWHL , s hierNet , FAMILY , GHSM 2} , {AIL 3 , GHSM 3} , {AIL4 , GHSM 4} and {AIL 5 , GHSM 5} , according to the highest order of interactions that they can handle . From the results , we have several observations : ( 1 ) All the methods in comparison can correctly detect the useful covariates , since all the Sen . values in the column for the 1st order interactions are 1 ’s . Similar results are also observed for the 2nd order interaction models ; ( 2 ) In each synthetic dataset , the GHSM method has better recovery performance for different orders of interactions compared with other methods ; ( 3 ) The prediction performance of each method is usually proportional to its capacity . That is , the methods that can learn higherorder interactions will achieve lower RMSE for prediction ; ( 4 ) In all the three datasets , the GHSM method always has the best prediction performance and it significantly outperforms the Lasso and the existing second order interaction models ; ( 5 ) In the third synthetic dataset , the AIL 5 method performs even worse than the AIL 4 method . One possible reason is that the AIL method does not capture the sparse heredity structure , making it hardly detect the correct higher order interactions . One evidence is that the AIL 5 method only recovers 58 % and 25 % of the correct 4th and 5th or der interactions , respectively , while our GHSM 5 method achieves 78 % and 80 % .
The synthetic study demonstrates that as long as significant highorder interaction effects exist in the data , the high order interaction methods will have better performance compared with the conventional second order interaction methods , and the proposed GHSM method with high order interactions can learn the sparse heredity structure and accurately detect those interactions , leading to improved prediction performance . 5.2 H3N2 Influenza A Virus Data
In this section , we study the application of the GHSM method on the antigenic sites identification problem .
Settings
521 Seasonal influenza A viruses pose great threats to public health , while the vaccination is the primary way to reduce this risk . An effective vaccination program requires an antigenic match between circulating viruses and vaccine strains to be used , and hence a timely identification of emerging influenza virus antigenic variants is critical to the success of influenza vaccination programs . Recent studies have suggested that multiple interactive antigenic sites mutations will significantly enhance the antigenic drift of the influenza viruses to new variants [ 17 , 10 , 23 ] . However , discovering the important interactive patterns among the antigenic sites is not trivial . In this problem , each site is treated as a covariate of the antigenic distances which are the responses , hence identifying interactive patterns among multiple antigenic sites can be formulated as an interaction model . Here we apply the proposed GHSM method
871 to identify interactive antigenic sites on an influenza H3N2 virus dataset [ 20 , 23].2 This dataset collects the results from the hemagglutination inhibition ( HI ) assays , which is a matrix recording the reaction values between 192 viruses in rows and a number of test serums in columns . The 192 H3N2 influenza A viruses are collected during year 2004 to 2007 . These reaction values in the HI matrix describe the virus antigenicities , and the Euclidian difference between the reaction values of two viruses describes the antigenic distance between them . A large antigenic distance may induce an antigenic drift and sometimes cause influenza outbreaks due to viral escape from existing immunity . Therefore , accurately predicting the antigenic distances among viruses is a fundamental task . In this dataset , for each virus , its hemagglutinin ( HA ) protein sequence , ie , a sequence of amino acid sites ( covariates ) that are responsible for antigenic changes , is also collected . The number of amino acid sites in the HA sequence of each virus is d = 329 . By comparing the sites in any two HA sequences , we could obtain a difference vector , in which the unchanged positions have zero values and the mutated positions have integer values between 1 and 5 , which is computed via the pattern induced multi sequence alignment ( PIMA ) scheme [ 20 , 23 ] . Then , each pairwise difference vector is treated as a data sample in the data matrix X , and each pairwise antigenic distance obtained from the HI reaction values is used as a response value in the target y . So there are a total number
= 18336 samples . of,192
2
Table 4 : Statistics of the influenza virus data .
1st 329 329 100 %
2nd
53,956 4368 8.1 %
# effects
# valid effects rate
# samples
3rd
4th
5,881,204
479,318,126
37,857 0.64 %
18,336
157,462 0.03 %
Since there are 329 features , the number of interaction effects increases drastically with respect to the order . Table 4 gives statistics of the effects in this data . When we consider the 3rd and 4th order interactions , the dimension of the entire feature space is around hundreds of millions , which leads to heavy computational demand . Fortunately , in each data sample , we find that only few mutated positions have non zero values and hence the data matrix X is sparse . As a consequence , a large number of the interaction effects are useless zero vectors since they are obtained via the product among sparse inputs and we can eliminate them before learning . The number of valid effects for each order is given in Table 4 . Here we focus on up to the 4th order interactions since higher order interactions cannot bring too much performance improvement .
The entries in X and Z(k ) ’s are normalized into [ 0 , 1 ] and each entry in y is log transformed and normalized into [ 0 , 1 ] as well . The regularization parameters for different methods are selected from a set {10−5 , 10−4,··· , 103} via the 5 fold cross validation . The parameter α in GHSM methods is chosen in the same way as that in the synthetic data . We randomly split the dataset into a training set and a test set by varying the training ratio from 10 % to 90 % at an interval of 20 % . Each setting is repeated for 10 times . We report both the predictive RMSE on the test set and the running time for all the methods . 522 Results and Analysis The predictive RMSE ’s for different methods are presented in Table 5 . From the results , we observe that all the interaction models remarkably outperform the Lasso method , implying that incorporating the feature interactions is important in this dataset . Different from the synthetic case , the AIL methods do not show much
2http://sysbiocvmmsstateedu/research/resources improvement over the Lasso and this is possibly because the AIL methods can not make use of the sparse heredity structure and the interactions among the antigenic sites are much more complex than the synthetic case . For the second order interaction models , the strong heredity based methods , ie , the s hierNet , FAMILY , and GHSM 2 , show better performance than the weak heredity based methods including the w hierNet and eWHL methods . This could be an evidence that the strong heredity structure is more useful for this problem . Similar to the results in the synthetic case , the GHSM method with higher order interactions obtain more accurate prediction results and the GHSM 4 method performs the best in all settings .
In addition to the RMSE , we also provide visualizations for the prediction results of different methods via the antigenic cartography , which is an approach to visualize the virus antigenic evolution process on a 2D/3D space by using the antigenic distance among the viruses and has been widely used for virus antigenicity explanation since its first use by [ 18 ] . The idea in the antigenic cartography is to utilize the multi dimensional scaling technique to obtain the coordinates of each virus on a 2D/3D space given the antigenic distance among them . We plot the embedding results in Fig 2 when the training rate is 10 % and use the predicted antigenic distance to reconstruct the cartography of all the viruses . In Fig 2(a ) , the cartography using the true antigenic distance among the viruses is plotted in a 2D space , where each circle represents a virus and all the 192 viruses are divided into four different groups according to the year of their appearance . Figs . 2(b) 2(h ) show the cartographies of different methods respectively . Since the cartographies of the AIL methods are similar to that of the Lasso and the eWHL and w hierNet methods are two solutions of the same model , we omit their cartographies . By comparing with Fig 2(a ) , we observe that the Lasso method can hardly reconstruct the antigenic distances among the viruses . The second order interaction models including the w hierNet , s hierNet , FAMILY and GHSM2 methods show clearer reconstruction but the viruses in different years are still hard to be distinguished . The group structure in the cartographies of the GHSM 3 and GHSM 4 methods is much better than others , which can be confirmed by the Pearson correlation coefficients reported in Fig 2 .
One advantage of the GHSM method is that we can identify important high order interactions based on the model parameters . Specifically , we take the 4th order interactions learned from the GHSM 4 method for example to see which 4th order interactions are detected by the algorithm . We sort the magnitude of the coefficients for the 4th order interactions in a descending order and then select the top 5 interactions , which are 157 , 159 , 242 , 246 , 186 , 193 , 242 , 246 , 94 , 145 , 189 , 219 , 145 , 189 , 198 , 219 and 94 , 145 , 189 , 198 . It is well known in the H3N2 virus antigenicity analysis that there are 135 important antigenic sites out of the total 329 positions identified as the antibody binding sites , since these sites locate at the surface of the H3N2 virus protein structure and they are more likely to react with the sera . These antibody binding sites are further divided into 5 binding areas A E according to their locations . Promisingly , we find that all of these detected positions in the 4th order interactions belong to the binding areas . More interestingly , when we tag the binding areas for these sites in the selected interactions , we get the following patterns : B , B , D , D , B , B , D , D , E , A , B , D , A , B , B , D and E , A , B , B , respectively , from which we see that the antigenicity of the H3N2 virus is more likely to be controlled by the interactions among the sites in different binding regions simultaneously instead of in the same binding region . This observation is reasonable since multiple sites from different binding areas can accurately capture the 3D
872 ( a ) Ground Truth
( b ) Lasso ( 0.59 )
( c ) w hierNet ( 0.60 )
( d ) s hierNet ( 0.61 )
( e ) FAMILY ( 0.62 )
( f ) GHSM 2 ( 0.64 )
( g ) GHSM 3 ( 0.67 )
( h ) GHSM 4 ( 0.70 )
Figure 2 : Cartographies of the prediction results of different methods when using 10 % samples for training . The number in each bracket of figures ( b) (h ) denotes the Pearson correlation coefficient between the true coordinates and the coordinates obtained from each method .
Table 5 : The predictive RMSE ’s on the influenza virus data . ‘−’ indicates that the corresponding algorithm does not return a result after running over 5 hours . The numbers in bold face denotes the best results .
Training
Rate 10 % 30 % 50 % 70 % 90 %
Covariates
Lasso
03938±00076 03914±00038 03935±00020 03927±00011 03926±00027
AIL 2
03648±00150 03622±00056 03625±00027 03634±00022 03629±00024 w hierNet
02025±00011 02018±00019 02013±00013 01989±00024 01958±00011
Up to 2nd order interactions eWHL s hierNet
02024±00023 02021±00010 02019±00009 02015±00013 02013±00022
01869±00040 01855±00016 01844±00040 01835±00024 01815±00018
FAMILY
01823±00029 01818±00016
− − −
GHSM 2
01887±00018 01884±00014 01875±00014 01873±00022 01868±00022
Training
Rate 10 % 30 % 50 % 70 % 90 %
Up to 3rd order interactions AIL 3 GHSM 3
03655±00143 03638±00066 03641±00031 03640±00022 03620±00022
01742±00014 01735±00007 01733±00015 01732±00013 01723±00031
Up to 4th order interactions AIL 4 GHSM 4
03658±00163 03642±00065 03635±00034 03636±00025 03631±00029
01737±00013 01725±00013 01714±00011 01718±00015 01697±00039 structure ( shape ) of the virus . Fig 3 shows the 3D structure of these binding areas with the sites from the selected interactions . So far , all of the above analysis is considerably useful in influenza vaccine strain selection , and it can significantly reduce the human labor efforts for serological characterization and will increase the probability of correct influenza vaccine candidate selection .
Moreover , we compare the training time of different HSM models on the entire influenza virus dataset and report the results in Table 6 . For the 2nd order HSM , the strong heredity based methods , ie , the s hierNet and FAMILY , are computationally much more expensive than the weak heredity based methods such as the w hierNet and eWHL . The FAMILY method fails to give the solution in reasonable time . Actually , the FAMILY method is computationally intractable even when using only 50 % of the samples for training ( refer to Table 5 ) . The eWHL method is very efficient since it is specifically designed for the w hierNet model . The proposed GHSM 2 method , a strong heredity based method , is much more efficient than the s hierNet and FAMILY methods and comparable with the eWHL algorithm . By increasing the order of interactions , the GHSM 3 and GHSM 4 methods are still very efficient compared with the w hierNet , s hierNet and FAMILY methods , while it has much better performance than those methods .
6 . CONCLUSION
In this paper , we proposed a generalized hierarchical sparse model to learn arbitrary order interactions contained in the data via the proposed arbitrary order heredity structure . An efficient algorithm was developed by decoupling the variables in the complex constraint and all the subproblems have efficient analytical solutions . Empirical results show the effectiveness of the proposed method .
When considering high order interactions , if the data matrix are not sparse like the influenza virus data , the number of high order interactions still increases exponentially with respect to the order and solving the GHSM will become intractable even for small d and K . One possible direction to solve this problem is to conduct dimensionality reduction methods before learning the GHSM model via , for example , the feature screening technique . We are also interested in applying the GHSM methods to other biological problems , such as the cancer microarray analysis , to detect important interactions .
Acknowledgments This work was partially supported by NSF IIS 1250985 , NSF IIS1407939 , NIH R01AI116744 , NSFC 61305071 , NSFC 61473087 and NSF of Jiangsu Province ( BK20141340 ) .
873 Table 6 : Training time of all HSMs on the entire flu virus data . ‘−’ indicates that the algorithm does not return a result after running over 5 hours .
2nd order interaction models eWHL − 11.3 s hierNet 5876.9
FAMILY GHSM 2 GHSM 3
21.3
34.3
Training time ( in seconds ) w hierNet
205.6
Higher order interaction models
GHSM 4
76.4
[ 10 ] J W Huang , C C King , and J M Yang . Co evolution positions and rules for antigenic variants of human influenza a/h3n2 viruses . BMC Bioinformatics , 10(Suppl 1):S41 , 2009 .
[ 11 ] M . Lim and T . Hastie . Learning interactions through hierarchical group lasso regularization . arXiv preprint arXiv:1308.2719 , 2013 .
[ 12 ] Y . Liu , J . Wang , and J . Ye . An efficient algorithm for weak hierarchical lasso . In KDD , pages 283–292 , 2014 . [ 13 ] D . C . Montgomery , E . A . Peck , and G . G . Vining .
Introduction to Linear Regression Analysis , volume 821 . John Wiley & Sons , 2012 .
[ 14 ] D . D . Pollock , W . R . Taylor , and N . Goldman . Coevolving protein residues : maximum likelihood identification and relationship to structure . Journal of Molecular Biology , 287(1):187–198 , 1999 .
[ 15 ] P . Radchenko and G . M . James . Variable selection using adaptive nonlinear interaction structures in high dimensions . Journal of the American Statistical Association , 105(492):1541–1553 , 2010 .
[ 16 ] Y . She and H . Jiang . Group regularized estimation under structural hierarchy . arXiv preprint arXiv:1411.4691 , 2014 .
[ 17 ] A . C C Shih , T C Hsiao , M S Ho , and W H Li .
Simultaneous amino acid substitutions at antigenic sites drive influenza a hemagglutinin evolution . PNAS , 104(15):6283–6288 , 2007 .
[ 18 ] D . J . Smith , A . S . Lapedes , J . C . de Jong , T . M . Bestebroer , G . F . Rimmelzwaan , A . D . Osterhaus , and R . A . Fouchier . Mapping the antigenic and genetic evolution of influenza virus . Science , 305(5682):371–376 , 2004 .
[ 19 ] M . J . Somers . Organizational commitment , turnover and absenteeism : An examination of direct and interaction effects . Journal of Organizational Behavior , 16(1):49–58 , 1995 .
[ 20 ] H . Sun , J . Yang , T . Zhang , L P Long , K . Jia , G . Yang , R . J .
Webby , and X F Wan . Using sequence data to infer the antigenicity of influenza virus . MBio , 4(4):e00230–13 , 2013 .
[ 21 ] R . Tibshirani . Regression shrinkage and selection via the lasso . Journal of the Royal Statistical Society . Series B ( Methodological ) , pages 267–288 , 1996 .
[ 22 ] X . Yan and J . Bien . Hierarchical sparse modeling : a choice of two regularizers . arXiv preprint arXiv:1512.01631 , 2015 . [ 23 ] J . Yang , T . Zhang , and X F Wan . Sequence based antigenic change prediction by a sparse learning method incorporating co evolutionary information . PLOS One , 9(9):e106660 , 2014 .
[ 24 ] M . Yuan , V . R . Joseph , and H . Zou . Structured variable selection and estimation . The Annals of Applied Statistics , 3(4):1738–1757 , 2009 .
[ 25 ] M . Yuan and Y . Lin . Model selection and estimation in regression with grouped variables . Journal of the Royal Statistical Society : Series B ( Statistical Methodology ) , 68(1):49–67 , 2006 .
[ 26 ] P . Zhao , G . Rocha , and B . Yu . The composite absolute penalties family for grouped and hierarchical variable selection . The Annals of Statistics , 37(6A):3468–3497 , 2009 .
Figure 3 : The 3D structure of the H3N2 virus . The red regions denote the anti body binding regions A E . The sites from the selected interactions are also labeled .
7 . REFERENCES [ 1 ] J . Bien , J . Taylor , and R . Tibshirani . A lasso for hierarchical interactions . The Annals of Statistics , 41(3):1111–1141 , 2013 .
[ 2 ] R . J . Cadoret , W . R . Yates , G . Woodworth , and M . A .
Stewart . Genetic environmental interaction in the genesis of aggressivity and conduct disorders . Archives of General Psychiatry , 52(11):916–924 , 1995 .
[ 3 ] N . H . Choi , W . Li , and J . Zhu . Variable selection with the strong heredity constraint and its oracle property . Journal of the American Statistical Association , 105(489):354–364 , 2010 .
[ 4 ] J . F . Dawson and A . W . Richter . Probing three way interactions in moderated multiple regression : development and application of a slope difference test . Journal of Applied Psychology , 91(4):917 , 2006 .
[ 5 ] T . C . Eley , K . Sugden , A . Corsico , A . M . Gregory , P . Sham , P . McGuffin , R . Plomin , and I . W . Craig . Gene–environment interaction analysis of serotonin system markers with adolescent depression . Molecular Psychiatry , 9(10):908–915 , 2004 .
[ 6 ] J . Friedman , T . Hastie , and R . Tibshirani . The Elements of
Statistical Learning . Springer , Berlin , 2001 .
[ 7 ] P . Gong , C . Zhang , Z . Lu , J . Z . Huang , and J . Ye . A general iterative shrinkage and thresholding algorithm for non convex regularized optimization problems . In ICML , 2013 .
[ 8 ] L . Han and Y . Zhang . Learning tree structure in multi task learning . In KDD , pages 397–406 , 2015 .
[ 9 ] A . Haris , D . Witten , and N . Simon . Convex modeling of interactions with strong heredity . arXiv preprint arXiv:1410.3517 , 2014 .
874
