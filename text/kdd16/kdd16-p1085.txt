Structural Neighborhood Based Classification of Nodes in a Network
Sharad Nandanwar
M . N . Murty
Department of Computer Science and
Department of Computer Science and
Automation
Indian Institute of Science
Bangalore , India sharadnandanwar@csaiiscernetin
Automation
Indian Institute of Science
Bangalore , India mnm@csaiiscernetin
ABSTRACT Classification of entities based on the underlying network structure is an important problem . Networks encountered in practice are sparse and have many missing and noisy links . Statistical learning techniques have been used in intranetwork classification ; however , they typically exploit only the local neighborhood , so may not perform well . In this paper , we propose a novel structural neighborhood based classifier learning using a random walk . For classifying a node , we take a random walk from the node and make a decision based on how nodes in the respective kth level neighborhood are labeled . We observe that random walks of short length are helpful in classification . Emphasizing role of longer random walks may cause the underlying Markov chain to converge to a stationary distribution . Considering this , we take a lazy random walk based approach with variable termination probability for each node , based on the node ’s structural properties including its degree . Our experimental study on real world datasets demonstrates the superiority of the proposed approach over the existing stateof the art approaches .
CCS Concepts •Computing methodologies → Semi supervised learning settings ; Statistical relational learning ; •Humancentered computing → Social networks ;
Keywords Graph based semi supervised learning ; Relational learning ; Collective classification
1 .
INTRODUCTION
With widespread availability and exponential growth of network data in recent years , there has been a surge of interest in mining network and graph data ; this has shown up
ACM acknowledges that this contribution was authored or co authored by an employee , contractor or affiliate of a national government . As such , the Government retains a nonexclusive , royalty free right to publish or reproduce this article , or to allow others to do so , for Government purposes only .
KDD ’16 , August 13 17 , 2016 , San Francisco , CA , USA
© 2016 ACM . ISBN 978 1 4503 4232 2/16/08 . . . $15.00 DOI : http://dxdoiorg/101145/29396722939782 in many guises . Classification , although a well defined problem in machine learning and pattern recognition literature , has not been studied extensively in the context of networks . In the network domain , the classification problem , often addressed as relational classification , is to assign each node of the network to a well defined community ( based on interests , demographics , concepts , etc . ) in the presence of its intrinsic features as well as the link structure . Some of the popular applications of node classification are , Wikipedia page categorization based on intra wiki links , genre identification of movies in a movie actor network , social circle learning in a friendship network etc . [ 25 , 17 ]
To motivate the problem , we use a toy example of a citation network shown in figure 1 , where nodes in the network represent papers published in the past proceedings of a conference , and edges depict the citations made by these papers . Each track in the conference where papers are published represents a class , which are distinguished by the color in figure 1 . Nodes with missing color information correspond to new submissions which are unlabeled . For a new submission in an upcoming conference , the classification task is to identify the track based on the citations made by the authors in the paper . Networks generally observe homophily , ie , nodes have a tendency to connect to other nodes that are similar to them . Because of this , it can be claimed that citations made in a paper are from an identical or a correlated track . Further , citations made by authors may range over multiple publishers who opt for different schema standards . This leads to heterogeneity in the metadata . Obtaining precise track information for such papers may have its own cost overheads . In the real world , getting labeled data for learning a supervised model is always expensive and the process involved is also tedious . So , the problem boils down to learning from a small set of labeled examples .
Generally , the study of networks revolves around social networking websites because of their popularity . However , in this paper , by “ network ” we refer to a generic network , be it a co citation network , friendship network , biological network , hyper link structure of the Internet , astronomical network , etc . Network Analysis is not only limited to data showing linked structure ; other unstructured data expressed in high dimensional space can also be transformed into weighted graphs so as to understand the topology of the data in a better way . Several approaches have been suggested in the literature for transforming non graph data into graph data , by using measures like Gaussian similarity [ 15 ] .
1085 of connectivity , can be used to formulate the problem as a semi supervised or transductive learning problem in graph . To exploit homophily in networks , we take a comprehensive view of classification , where a node is classified based on how other nodes in its extended neighborhood are labeled . A random walk in the limit gets attracted towards dense sub networks in the graph . Hence , a majority of nodes in such a dense sub network share the same class label .
The principal contributions of our work are as follows :
• We propose a novel approach for intra network classification , which enriches the adjacency structure of a node by exploiting its global neighborhood .
• We formulate this as an optimization problem that ex ploits the network structure .
• We provide a learning algorithm based on the stochas tic gradient descent approach for the same .
• We show effectiveness of the proposed approach by making an exhaustive comparative study with state of theart approaches for intra network classification .
Rest of the paper is organized as follows : section 2 describes some of the state of the art techniques in the field of relational classification including collective classification , graph kernels , and social representation learning . The proposed approach for discriminative learning , viz . Structural Neighborhood Based Classification , is described in section 3 . Section 4 describes how to extend the proposed framework from exploiting the local neighborhood to that of the global neighborhood . A detailed comparative study with existing baseline approaches is made in section 5 , followed by discussion and conclusion in section 6 and section 7 respectively . All the source code and datasets used for evaluation are available at https://githubcom/sharadnandanwar/snbc
2 . STATE OF THE ART
Traditional statistical relational learning methods may not work well in cases where the data is sparsely labeled . In the past decade , several approaches that exploit the relational dependency among the nodes have been proposed . We summarize some of the influential methods below .
Lu and Getoor [ 12 ] proposed an approach on the lines of logistic regression , which computes the conditional probability of a class as the product of posterior probabilities conditioned on the node attributes and the node links respectively , and finds a MAP estimate for the class variable . However , similar to the other local classifiers like the k Nearest Neighbor Classifier and SVM , this approach too cannot view beyond the local neighborhood of the nodes and is hence not robust to noise .
Collective Classification : Collective classification approaches utilize additional knowledge about attributes and class information , based on relationships among entities . Macskassy and Provost [ 13 ] introduced a simple relational classifier for network data , the weighted vote relational neighbor ( wvRN ) classifier . It computes the class membership as a weighted average of the estimated class membership probabilities of the neighboring nodes . Multi Rank Walk [ 11 ] , yet another relational classifier , is based on the principle of random graph walks similar to Page Rank [ 16 ] . The class of
Figure 1 : Illustration of multi class classification in a
Citation Network
Conventionally , networks are represented by graphs , defined as follows .
Definition 1 . A network is modeled as a graph G(V , E ,W ) , where V is the set of |V| = n interacting units ( nodes or actors ) , and E ⊆ V × V is the set of edges indicating relationship or interactions among the nodes . W ∈ Rn×n , where Wij indicates affinity or strength of the relationship between nodes vi , vj ∈ V . For a sparsely labeled network , the classification problem is formally stated as follows .
Definition 2 . Given a graph G(V , E ) and a set of labeled nodes Vl ( ( V ) with corresponding ( ordered ) set of labels Yl ∈ C|Vl| , where C = {C1,C2 , . . . ,Ck} , the set of k labels C1 to Ck . The objective is to learn a model for inferring labels of the unlabeled nodes Vu = V \ Vl . Many times , structure of the graph is noisy because of missing and noisy edges . While using traditional statistical learning tools like Support Vector Machines ( SVM ) , we may be losing crucial information if we learn on the basis of a node ’s local neighborhood alone . Such locally oriented models may not be able to capture global or long distance relationships . Techniques from link prediction ( another sub domain in network analysis ) can be leveraged to make the representation more precise . On these lines , many graph kernels [ 8 , 9 , 19 ] were proposed , which compute global structural similarity between pairs of nodes . However , when the network size is in the order of millions , such graph kernel based approaches also suffer due to the high computational requirements and overhead involved in searching over the parameter space .
Unlike the traditional supervised learning techniques , the features in a relational data scenario correspond to the data instances themselves . Hence , since features and instances are interdependent , so are the data instances , rendering the otherwise usual iid assumption invalid . Even though the network is sparsely labeled ( |Vl| << |V| ) , the connectivity between nodes imparts additional knowledge , strengthening the case for a classifier . In accordance with the principle of homophily [ 1 ] , neighboring nodes in a network are supposed to be similar to each other . Also , as per the clustering hypothesis , nodes forming a dense sub network should be correlated [ 1 ] . These assumptions , along with the knowledge
1086 any unlabeled node is decided as the one which has the highest probability of containing terminal nodes of the random walk .
Social Dimensions for Classification : Networks encountered in practice generally exhibit a community structure , ie a certain group of nodes or entities have stronger connections among themselves , as compared to rest of the network . SocioDim [ 24 ] is a framework which exploits the cluster hypothesis , which posits that the nodes in the same community are likely to be of the same class . SocioDim attempts to find the latent social dimensions of the network corresponding to the communities and performs classification in these dimensions . However , in networks where community structure has a high conductance , the obtained social dimensions would be inexact , leading to poor classification performance . Wang et al . [ 25 ] propose relational learning in a multi label setting by extracting social context based features . The extracted social context features correspond to hidden causes which make nodes collaborate among themselves . Based on these features , an iterative probabilistic process similar to wvRN is adopted . DeepWalk [ 17 ] and LINE [ 22 ] are recently proposed approaches for Social Representation Learning based on random walks . These methods try to capture neighborhood similarity and community membership in latent representations . These label independent representations are then used in multi label classification .
Random Walk based approaches : Zhou et al .
[ 27 ] proposed a globally consistent learning approach on the lines of spectral clustering . The proposed iterative method updates the label of a node using information the node receives from its neighbors . Label propagation algorithm [ 28 ] adapts a probabilistic perspective using Markov random walks , wherein structure is used to compute transition probabilities . Hidden labels of a node are thereafter inferred using its ancestors’ hidden/observed labels . In [ 2 ] , Baluja et al . proposed a controlled random walk over a graph , with three possible actions ( inject , continue , abandon ) at each step . Inject causes the random walk to stop and return its current label , continue continues the random walk to its neighbors , and abandon terminates the walk without performing any labeling . [ 21 ] proposed a modification to this by defining a well behaved objective function which such a framework minimizes , thus guaranteeing convergence .
Learning using Graph Kernels : In addition to the above schemes , many graph kernels that exploit structural information have also been proposed . Link prediction techniques based on Common Neighbors , Adamic Adar , Resource Allocation Index , etc . [ 10 ] provide means for computing similarity between nodes . They , however , consider only the local neighborhood of nodes and neglect long range relationships . Graph kernels , on the other hand , capture the notion of global similarity between the nodes . A simple kernel would count the number of paths of fixed length between the nodes . It is well known that longer paths between two nodes are less significant compared to shorter ones . Exponential and von Neumann graph kernels [ 8 ] compute a weighted mean of all such path counts giving higher weights to shorter paths . The number of paths of length n between nodes vi and vj are reflected in An ij , where A is the adjacency matrix . Then , a von Neumann kernel is defined as ,
∞
γiAi = ( I − γA)−1 ,
Xi=0
γi i!
Ai = exp(γA ) .
∞
Xi=0 where γ ∈ ( 0 , 1 ) . Exponential graph kernel , very similar to the above , is defined as ,
Laplacian counterparts of the above kernels have also been defined , namely , regularized Laplacian kernel [ 19 ] and heat diffusion kernel [ 9 ] respectively . Graph kernels usually work well on smaller networks . For learning on large networks , these approaches require inverting large matrices , which is not feasible when the resources are limited .
On a side note , relational classifiers described above do not have good generalization abilities and suffer when the given network is sparsely labeled .
A heterogeneous information network [ 20 ] is a network composed of multiple types of nodes and relationships between them . Study of these networks has lately gained prominence . Chakrabarti et al . [ 3 ] showed that making use of hyper links along with hypertext while classifying linked text content leads to an enhanced performance . Ming Ji [ 7 ] proposed a framework for transductive learning et al . in heterogeneous networks based on two assumptions : local consistency ( class assignment of neighbors is similar ) and ground truth ( pre assigned class labels are correct ) . In [ 6 ] , ranking and classification in heterogeneous information networks are combined based on the intuition that highly ranked objects within a class should play a more important role in classification .
3 . STRUCTURAL NEIGHBORHOOD BASED
CLASSIFICATION ( SNBC )
Traditional learning approaches like the k Nearest Neighbor classifier and SVM use local adjacency information in classification . In a sparsely labeled network , it becomes difficult to classify a node if there are not sufficient number of labeled nodes in its vicinity . In accordance with homophily , two nodes connected by an edge are expected to be similar to each other . We assume that the behavior of a node is defined by the average behavior of its neighboring nodes , and come up with a novel way of learning in networks . Most of the statistical classifier learning problems are inherently binary classification problems , which can be easily extended for multi class classification using standard approaches like one vs one and one vs all [ 14 ] . Henceforth , in this paper , we will be working with a binary classification problem , which can be extended to deal with multi class classification like others .
Let C+ and C− represent the sets of positive and negative examples respectively in the binary classification problem under consideration . A simple approach to classify a node would be to count the number of neighbors from the respective classes and label the node based on majority voting . However , this approach has its own limitations . If the network is sparsely labeled , nodes may lack sufficient number of labeled neighbors . Macskassy [ 13 ] showed that instead of counting votes , taking a weighted vote of each neighbor is more useful . The weight of a vote expresses the confidence with which a neighbor can be assigned to a class . In a probabilistic model , this confidence can be measured using conditional class probabilities . In classifiers where a real decision function is learned , a larger distance from the decision boundary signifies a higher weight .
1087 amazon wikipedia cora youtube imdb pubmed
0.35
0.30
0.25 n o i t a n m i i r c s i D
0.20
0.15
0.10
0.05
( a )
0.00
0
2
4
10
12
14
6 8 Log Degree
( b )
Figure 2 : ( a)Toy network of books modeling similarity relationship among books based on user preferences . ( b)Plot showing expected contribution of nodes with degree d towards discrimination ie ∆(d )
We start by defining the adjacency based representation of a graph . Given an undirected and binary weighted graph G(V , E ) , where V = {v1 , v2 , . . . , vn} and E ⊆ V × V , the corresponding adjacency matrix A is defined as follows ,
Aij =(1 , if ( vi , vj ) ∈ E
0 , otherwise
.
In the case of a weighted graph G(V,E ,W ) , links carry a non negative weight specified by W . Adjacency Matrix A for these graphs is modified as Aij = Wij . Based on the link attributes of a node , we define node vi in vector space notation as ai = [ Aji]n×1 where j ∈ {1 , 2 , ··· , n} . For node vi , we define the first level/local neighborhood as follows :
Definition 3 . First Level Neighborhood of a node vi if and only if there exists an edge is the set N 1 in the graph connecting vi and vj , ie , ( vi , vj ) ∈ E . i st vj ∈ N 1 i
Further , let N 1 i,+ ⊆ C+ and N 1 i,− ⊆ C− denote the sets of neighboring nodes from positive class and negative class respectively in the binary labeled network . In this setting , we assume a linear separating hyperplane ( like the one in perceptron , logistic regression , SVM ) of the form wT · x + b . Then , given the parameters w and b of an optimal decision boundary , for an unknown example xu , the predicted label is given by
ˆyu = sign(wT · xu + b )
Because of homophily in the network , a node is expected to have its class label ( defined by its properties ) similar to majority of its neighbors . For a target node vi , the above scoring function assigns a positive or negative value ( ˆyj ) to its neighboring node ( vj ) . As most of the neighboring nodes are expected to have their class label same as that of the target node , the aggregated sum of scores from these neighboring nodes should also have the same sign as the target label ( yi ) .
Also , in real networks , variability in the node degree is inherent and follows a power law distribution . It is observed that high degree nodes are generally the source of linkage noise in networks . For example consider the toy network of books shown in figure 2a . The network models “ users who liked this also liked ” similarity relation among books . In the graph , Angels and Demons shares a similarity relation with most of the books in the Thriller category . This entices one to become prejudiced that any book that shares such a relation with Angels and Demons will belong to the Thriller category . However , contrary to this intuition , the same book shares many relations with books in other categories too , as is visible in the graph . This can be attributed to the “ rich getting richer ” phenomenon in networks , ie , the book went popular and was enjoyed by readers across the genres . Based on this observation , we argue that a link to a low or medium degree node should be considered more reliable in comparison with a link to a high degree node . Thus , while learning the weight vector w , if an almost equivalent performance can be achieved by ( 1 ) assigning larger weights to a large number of medium degree nodes , than ( 2 ) assigning larger weights to a small number of high degree nodes , then the former should be preferred . Figure 2b shows the expected discriminatory power ( defined below ) with varying degree . The graph shows how the relative average mutual information varies for nodes lying in different degree zones . We define a measure of discrimination ∆ for degree d as follows :
∆(d ) =
1
Z Xi∈{j:dj =d} max c∈C
M I(ai , yc ) , where Z is the normalization constant , and M I(X , Y ) computes the mutual information of random variables X and Y , dj is the degree of node vj , and yc refers to the label vector of class c . We observe a consistent behavior across all datasets considered in our experiments , however the peaking range is different for each dataset , which would depend on the network size and its structure , more specifically on its sparsity . Based on these observations , we state the following conjectures :
1088 Conjecture 1 . For a node v , its neighbors Nv and the decision boundary ( H ) , sum of distances between H and neighbors N ( s ) similar to v ( ie , of the same class as v ) should be greater than the same for other neighbors ( Nv \ N ( s ) v ) . v
Conjecture 2 . The role of high degree nodes in discrimination is smaller compared to that of medium degree nodes .
We formalize our intuition as follows : ( 1 ) Structural Neighborhood : For a node vi having class label yi ∈ {−1 , 1} , the aggregated scores from the nodes having label yi in the first level neighborhood ( N 1 i ) should be more than the aggregated scores from rest of the nodes in the first level neighborhood . Thus , for optimal w and b , we have , for all i , yi Xj∈N 1 i,yi
A(vi , vj)(wT·aj +b ) ≥ −yi Xj∈N 1 i,−yi
A(vi , vj )(wT·aj+b ) , where A(vi , vj ) = Aij indicates the weight of edge joining nodes vi and vj . This can be equivalently rewritten as , yi  Xj∈N 1
Aij ( wT · aj + b) i,yi
Aij(wT · aj + b ) + Xj∈N 1 =⇒ yi Xj∈V i,−yi
Aij(wT · aj + b ) ! ≥ 0 .
 ≥ 0 ,
Rearranging the terms , we get , yi , wT · A · ai + di b ≥ 0 ,
Aij . where di = Xj∈V di wT · A · ai + b ≥ 0
=⇒ yi 1 Let M = [ m1 , m2 , ··· , mn ] = A2D−1 , where D ∈ Rn × n defined as Dij =fl di
Then , we have the following in a linearly separable case :
0 otherwise if i = j
. yi(wTmi + b ) ≥ 0 . ai di
The above can be interpreted as mapping the adjacency information ai to a new space as mi = A , and then learning a decision boundary . For node vi , we define empirical loss εi ( ≥ 0 ) such that yi(wTmi + b ) ≥ 1 − εi .
Mean empirical loss , that is to be minimized , is given by f ( w , b ) =
1
|Vl| Xi∈Vl
εi .
( 1 )
( 2 ) Degree Dependent Regularization : Weight wi for node vi with degree di should be affected in a manner directly proportional to a monotonically increasing function of di . Let g : ( R+S{0} ) × Z+ → R be the penalty function such that penalty for node vi is pi = g(wi , di ) . We define penalty for the network as a whole by the ℓ2 norm of vector p = ( p1 , p2 , . . . , pn ) , and try to minimize the squared penalty . In this work we consider the following penalty functions :
• Linear Weighted Degree ( LWD ) : g(wi , di ) := |wi|di
• Linear Weighted Root Degree ( LWRD ) : g(wi , di ) := |wi|√di
• Linear Weighted Root Log Degree ( LWRLD ) : g(wi , di ) := |wi|plog2 di
Regularizing the objective in ( 1 ) corresponding to empirical loss with the above penalty using regularization parameter λ would lead to , min w,b
λ
2||p||2 +
1
|Vl| Xi∈Vl
εi such that yi(wTmi + b ) ≥ 1 − εi , εi ≥ 0 , and ai mi = A di
( 2 )
4 . DIVING DEEP INTO THE NETWORK
The objective in ( 2 ) attempts to learn parameters ( w and b ) , while labeling the node under consideration based on the collective behavior of its first level neighborhood . We further explore the significance of far away neighbors in this setting . We begin by giving a recursive definition of the rth level neighborhood of a node : i i i i i
} . i st vk ∈ N r
, and multiplicity of vk in N r
Definition 4 . rth Level Neighborhood of a node vi is defined as a multiset N r if and only if there is an edge in graph G connecting nodes vk and vj where node vj ∈ N r−1 is given by the cardinality of set {vj|(vk , vj ) ∈ E and vj ∈ N r−1 Note that the multiplicity of a node vk in multiset N r indicates the number of paths of length r possible between nodes vi and vk . Proceeding on similar lines as in section 3 , we deal with the second level neighborhood , which can be generalized for any depth in the network . While considering the first level neighborhood N 1 , we were interested in how nodes in N 1 get classified based on adjacency information they have . When using second level neighborhood , we first accumulate scores at first level neighbors using decision values of second level neighbors . These scores from first level neighbors are further propagated to the node under consideration to perform classification . Figure 3 illustrates the involved process ( 3a ) for first level neighborhood based classification , and ( 3b ) when using second level neighborhood in classification .
In section 3 we have derived the representation mi for ai . di node vi based on its first level neighborhood as mi = A
We carry forward this representation while delving into higher level neighborhoods . Using similar homophily arguments at each node , we have , yi  Xj∈N 1 which simplifies to
Aij(wT · mj + b ) + Xj∈N 1 i,−yi i,yi
Aij(wT · mj + b) ∀i = 1 , . . . , n
 ≥ 0 ,
1089 a5 a4 a6 a3 a12 a13 a11 a2 a8 a1 m3 m4 m5 q0 = X mj j∈N0
1 d0 m2 = 1 X aj d2 j∈N2 m1 a10 a9 m6 m8 m7 m0 = 1 X aj d0 j∈N0 a7
( a ) First Level Neighborhood
( b ) Second Level Neighborhood
Figure 3 : For an unweighted graph Figure 3(a ) illustrates how representation for node v0 based on its first level neighbors is derived while 3(b ) illustrates the representation for the same based on its second level neighbors . Ni denotes the set of first level neighbors for node vi with degree di = |Ni| yi 1 di wT · A · mi + b ≥ 0 ,
∀i = 1 , . . . , n
Aij . where , di = Xj∈V Let Q = [ q1 , q2,··· , qn ] = M AD−1 = A(AD−1)2 , where M and D are defined in section 3 . Then , for the linearly separable case , we have , yi(wTqi + b ) ≥ 0 ,
∀i = 1 , . . . , n based on second level neighborhood of where qi = A2D−1 ai di a node . We can argue using inductive logic that while using r level neighbors , the same will be mapped to A(AD−1)r−1 ai di For classification using r level neighbors , the adjacency information contained in the matrix A as a whole gets mapped to A(AD−1)r .
.
Random Walk Based View : The above scheme can also be viewed as taking random walks of fixed length r starting from the node under consideration , and classifying the node by accumulating the decision values for respective neighbor nodes . With higher values of r , the representation derived above tends to become more global . However , with increasing values of r , the transition probabilities between any pair of nodes start converging towards the stationary probability distribution . Because of this , in a connected graph for sufficiently large values of r , every node will eventually end up having a similar representation . Because of this , one can expect some loss in discrimination for higher values of r .
Lazy Random Walk : To avoid the situation where all nodes have identical representation , we consider taking lazy random walks of arbitrary lengths instead of a fixed length r . We start by introducing a dampening factor at each hop , which controls the termination of the random walk . If dampening factor is denoted by γ ∈ [ 0 , 1 ] , the random walk terminates at each hop with probability ( 1 − γ ) , and continues to the next hop with probability γ . The representation qi for node vi obtained using the above procedure is given by qi = ( 1−γ)Aei+γ ai di
+γ2(AD−1 ) ai di
+γ3(AD−1)2 ai di
+ . . . ,
( 3 ) ei being a n dimensional unit vector with ith entry as 1 and remaining as zeros . Other notations used have already been defined . This closely resembles the regularized graph laplacian kernel [ 19 ] using unnormalized graph laplacian . However , instead of computing structure based similarity matrix , we are interested in a structural similarity based representation . In equation 3 , if γ is chosen to be close to zero , it is equivalent to learning with the given adjacency representation alone . On the other extreme , if gamma is large ( ≈ 1 ) , higher powers will dominate . This may lead to every node having similar representation as mentioned previously . We empirically find that taking such an approach does not lead to a significant improvement in the performance of the learned classifier .
Structured Random Walk : As stated earlier , networks observed in practice exhibit a variability in degree . It is easier to classify higher degree nodes than it is to classify low degree nodes , since the former have a richer local neighborhood . Low degree nodes have limited information owing to their smaller local neighborhood . In addition , if the dampening factor is high , then continuing from a high degree node may lead to accumulation of more noise at each successive hop . Considering these consequences , we decide upon having a separate dampening factor specific to a node , based on its structural property . Intuitively , on encountering a high degree node , a random walk should observe higher termination probability ( 1 − γ ) , and the same should be lower in case of low degree nodes . We use randomness of a node as a measure to derive respective dampening factors , which is defined for node vi as follows :
Hi = − Xj∈Ni pij log2(pij ) , where Ni denotes the set of neighbors of node vi , as defined earlier . Assuming uniform transition probabilities for all neighbors of a node , we have pij = 1
|Ni| , which gives
1090 Hi = log2 |Ni| = log2 di
As stated earlier , higher randomness implies a higher termination probability and a lesser dampening factor . Taking this into account , we choose
Algorithm 1 Training Structural Neighborhood Based Classifier with LWRLD penalty using stochastic gradient descent 1 : procedure snbc train ( A , y , λ , imax , k )
γi =
1 log2 di
.
We define matrix Γ as ,
Γ = diag(γ1 , γ2 , . . . , γ|V| ) .
The representation then becomes qi = Aei+Γ ai di
+(AD−1)Γ2 ai di
+(AD−1)2Γ3 ai di
+ . . ( I−Γ )
Let Q be the matrix obtained by stacking column vectors qi corresponding to all the nodes . Then , we have Q = A(I − AD−1Γ)−1(I − Γ ) .
4.1 Structural Neighborhood Based Classifi cation ( SNBC ) Algorithm
Taking structured random walk into account the objective in ( 2 ) is modified as : min w,b st
εi
1
λ
2 ||p||2 +
|Vl| Xi∈Vl yi(wTqi + b ) ≥ 1 − εi ,
εi ≥ 0 , and
+(AD−1)Γ2 ai di
+(AD−1)2Γ3 ai di
( 4 )
+ . . .(I−Γ ) qi = Aei +Γ ai di
Subgradient of the above is given by ,
∇t = λp
∂p ∂w −
1
|Vl| Xi∈Vl
1[yiwT t qi < 1]yiqi , where 1 denotes indicator function . Using gradient descent , the iterative update rule for w is given by wt+1 = wt − ηt∇t where ηt is the learning rate for the tth iteration . We use stochastic gradient descent mini batch update algorithm with a variable learning rate ηt given by ηt =
. Algorithm
1 provides the pseudocode for the adopted approach with a LWRLD penalty function .
1
2 + λt
5 . EXPERIMENTS
We empirically evaluate the results of the proposed approach SNBC over various real world datasets . In order to show the effectiveness of SNBC we compare it with some of the recent and state of the art intra network classification schemes .
5.1 Datasets :
We used some of the popular relational datasets described below .
Input :
An×n yn×1
λ imax k
Output :
⊲ Adjacency matrix representation of network ⊲ Label vector , yi ∈ {−1 , 1} if ith node is labeled , yi = 0 otherwise ⊲ Regularization parameter ⊲ Maximum number of iterations for SGD ⊲ Sample size for SGD wimax , b
⊲ Learned decision function parameter
2 : 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 :
11 :
12 : 13 : 14 :
15 : 16 : 17 :
⊲ Set of training points ⊲ Degree Vector
T r ← {i : yi 6= 0} dn×1 ← A 1 D ← diag(d ) Define log degree vector ln×1 st li ← log2(2 + di ) L ← diag(l ) Γ ← diag−1(l ) X ← A(I − AD−1Γ)−1(I − Γ ) w0 ← 0n×1 , b ← 0 for t = 1 , 2 , . . . , imax do
⊲ Initialization
1
T k Xi∈T r i wt−1 )
( yi − x b ← Choose Smp ⊆ T r , where |Smp| = k , at random M c ← {i ∈ Smp : yi(xT wt ← ( I − ηtL)wt−1 + i wt−1 + b ) < 1} ηt k Xi∈M c yixi end for return wimax , b end procedure models “ knows ” friendship relations amongst users who are assigned to multiple groups according to their interests .
PubMed : The PubMed data set2 consists of scientific publications from the PubMed database pertaining to diabetes , where each publication is classified into one of three classes : “ Diabetes Mellitus , Experimental ” , “ Diabetes Mellitus Type 1 ” , “ Diabetes Mellitus Type 2 ” .
CoRA : A collection of research articles3 in the computer science domain classified into pre defined research topics . For our study , we ignored the citations for which complete information is not available .
IMDb : We crawled information about movies from IMDb4 . For generating the graph , we took a subset of English movies released after the year 1990 . Then , we defined the similarity between the movies based on the top 5 billed stars in them . Genre assigned to the movies is used as the target variable . Amazon Books : We extracted a subset of books from the amazon co purchasing network data5 . Books having less than 5 reviews were ignored . For each book , the dataset also provides a list of other similar books , which is used to build a network . The Genre of books gives a natural
Youtube : A subset of Youtube users with grouping information made available by Lei Tang1 is used . The graph
1leitangnet/code/social_dimension/data/youtubemat
2linqscsumdedu/projects/projects/lbc/indexhtml 3peoplecsumassedu/~mccallum/data/CoRA classifytargz 4wwwimdbcom/interfaces 5snapstanfordedu/data/amazon metahtml
1091 categorization , which we use as class labels in our learning problem .
Wikipedia : We use a crawled dump of Wikipedia pages from different areas of computer science using the Wikimedia API6 . For crawling , we choose 16 top level category pages , and recursively crawled subcategories up to a depth of 3 . The top level categories are used as class labels .
Some of the statistics of the datasets are summarized in
Table 1 .
Dataset #Nodes #Edges #Classes
Amazon CoRA IMDb PubMed Wikipedia Youtube
83742 24519 19359 19717 35633 22693
190097 92207 362079 44324 495388 96361
30 10 21 3 16 47
Label
Cardinality
1.546 1.004 2.300 1.000 1.312 1.707
Table 1 : Datasets used for experiments
5.2 Comparative Study :
There is a lot of work on semi supervised learning in graphs and on collective classification . However , we focus on comparing our work with the state of the art approaches . We restrict our study to some of the recent advances in the field of intra network classification , and use linear Support Vector Machine ( SVM ) as the baseline classifier . We briefly describe below , the approaches considered by us for the empirical study .
Linear SVM [ 4 ] : We learn from local adjacency vector using Support Vector Machine(SVM ) and use this as a benchmark .
SocioDim Modularity [ 23 ] : This approach uses modularity matrix and tries to extract social dimensions hidden in relations among nodes by computing eigenvectors of the modularity matrix . We fix the number of social dimensions as 200 for our experiments .
SocioDim EdgeClustering [ 24 ] : As an extension to SocioDim Modularity , this approach performs edge clustering by using incidence matrix of the given graph . The hard clusters for edges thus obtained are used to compute latent dimensions of the node . In our experiment , we partition edges into 5000 clusters .
SCRN [ 25 ] : SCRN , an approach for collective classification , exploits social context features in relational learning . The social features are computed using edge clustering , in a way similar to SocioDim EdgeClustering .
Deepwalk [ 17 ] : Deepwalk is a recently proposed approach for deep learning in networks . The approach is useful for learning low dimensional embeddings in large networks . We obtain 128 dimensional embeddings for a node using Deepwalk , and use these embeddings further in classification .
5.3 Setup and Parameter tuning :
To study the robustness of the proposed framework on a sparsely labeled network , we learn a model by holding out the labels of 90 % , 70 % , 50 % , 30 % and 10 % of nodes respectively . However , because of lack of space here , we show results obtained on using 10 % of the labeled nodes for training . Set of nodes used for training is sampled using the Snowball sampling [ 5 ] technique . For datasets having directed edges ,
6enwikipediaorg/w/apiphp we remove directionality by adding an edge in the opposite direction also . The datasets used in experiments are multilabeled , ie , a node can belong to more than one class . To train SVM in this setting , we use the one vs all approach for training a model corresponding to each class . For tuning linear SVM , we restricted the search space to 2−10 < λ < 25 . For each node in the test set , decision values are obtained from the respective class models . We assign s most probable classes to the node using these decision values , where s is equal to the number of labels assigned to the node originally . However , the decision values given by different SVM models cannot be compared directly . We use Platt ’s Scaling [ 18 ] to convert these decision values into probability scores . Probability scores from different models , being on the same scale , can be compared . For validating our results , we use three popular evaluation measures for multi label classification : Hamming Score , Micro F1 Score , and Macro F1 Score [ 26 ] . If for the ith node , Ti is the set of true labels , and Pi is set of predicted labels , then we have
Micro F1 Score =
|Ti ∩ Pi| |Ti ∪ Pi|
Hamming Score =Xi 2Xi |Ti ∩ Pi| |Ti| +Xi Xi 2Xi∈Cj |Ti ∩ Pi| |Ti| + Xi∈Cj Xi∈Cj
|Pi|
|Pi|
Macro F1 Score =
1 k k
Xj=1
To verify robustness of various approaches , we take 100 random splits of the network into test and train datasets . We report average Hamming Score , Micro F1 Score , and MacroF1 Score over these random splits .
6 . RESULTS AND DISCUSSION
In this section , we explain the behavior of the proposed approach SNBC based on a systematic experimental study described above . Table 2 reports the results using different penalty functions . We choose the functions LWD , LWRD , and LWRLD as described in section 3 . Although the penalty function can have many more forms , our purpose of choosing these functions is to demonstrate the impact of rate of increase of function value with degree . In all the cases , we found LWRLD penalty to be outperforming . The reason is that the other two functions , in addition to penalizing high degree nodes , also heavily penalize medium degree nodes that carry maximum discriminative power . This heavy penalization inhibits learning larger weights for medium degree nodes . However , LWRD was found to be competent in case of small networks like IMDb and PubMed , where the average degree is small . The behavior can be seen as an implication of pdavg ≈ log2(davg ) for small average degree ( davg ) .
Figures 4 , 5 , and 6 show respectively the Hamming Score , Macro F1 Score , and Micro F1 score for multi label classification of the datasets . In most of the cases , the proposed approach improves over the existing ones . We investigate the reason behind this . As we move from local neighborhood to global neighborhood , the ratio of number of between class
1092 Dataset →
Penalty Func . ↓
LWD
LWRD
LWRLD
Measure
Youtube
PubMed
CoRA
IMDb
Amazon
Wikipedia
Hamming Sc . Micro F1 Sc . Macro F1 Sc .
Hamming Sc . Micro F1 Sc . Macro F1 Sc .
Hamming Sc . Micro F1 Sc . Macro F1 Sc .
18.27 ± 0.09 24.29 ± 0.16 8.09 ± 0.08 35.05 ± 2.79 40.64 ± 1.12 33.43 ± 2.20 36.15 ± 2.58
41.13 ± 1.28 41.12 ± 1.37 21.63 ± 2.39 78.55 ± 0.37 78.55 ± 0.37 76.35 ± 0.77 80.04 ± 0.19
37.74 ± 0.10 37.71 ± 0.10 6.33 ± 0.11 60.46 ± 2.56 60.45 ± 2.55 49.33 ± 4.38 67.39 ± 2.23
32.77 ± 0.15 40.93 ± 0.55 9.84 ± 0.25 32.76 ± 0.12 40.91 ± 0.46 9.82 ± 0.22
32.90 ± 0.36
15.18 ± 0.03 17.60 ± 0.15 2.84 ± 0.02 61.08 ± 0.36 61.53 ± 0.45 59.67 ± 0.74 61.24 ± 0.19
16.13 ± 0.04 21.32 ± 0.11 7.32 ± 0.19 16.08 ± 0.16 21.26 ± 0.22 7.22 ± 0.27
69.93 ± 0.16
41.26 ± 1.34
80.04 ± 0.19
67.39 ± 2.24
41.30 ± 0.44
61.71 ± 0.25
71.32 ± 0.12
35.66 ± 3.31
78.28 ± 0.24
57.47 ± 3.96
16.56 ± 0.31
59.69 ± 0.31
61.37 ± 1.58
Table 2 : Evaluation of multi label classification with different penalty functions , using 10 % of labeled data for training .
90
Linear SVM
SocioDim Modularity
SocioDim EdgeClus
80
SCRN
Deepwalk
SNBC
70
60
50
40
30 e r o c S g n m m a H i
20.67
19.64
20
18.72
69.92
67.39
65.94
66.11
61.88
62.53
56.14
50.03
50.92
47.99
53.06
80.04
76.33
66.11
65.80
61.24
57.38
57.39
54.86
47.36
47.72
36.15
36.57
23.18
20.73
33.09
31.90
32.90
29.19
28.58
26.35
19.26
14.12
10
0
Youtube
CoRA
Wikipedia
IMDb
Amazon
PubMed e r o c S
1 F o r c a M
80
70
60
50
40
30
20
10
0
Linear SVM
SocioDim Modularity
SocioDim EdgeClus
SCRN
Deepwalk
SNBC
61.38
57.47
46.72
52.12
51.23
48.95
47.17
41.29
35.67
34.80
30.18
29.20
25.10
14.54
14.52
12.60
7.41
9.87
10.41
78.28
73.79
63.61
64.54
59.69
54.15
54.19
51.70
43.85
42.75
33.49
18.36
16.57
13.08
14.75
14.77
13.70
Youtube
CoRA
Wikipedia
IMDb
Amazon
PubMed
Figure 4 : Hamming score for multi label classification compared with the state of the art approaches using
Figure 5 : Macro F1 score for multi label classification compared with the state of the art approaches using
10 % of nodes for training
10 % of nodes for training to within class paths keeps increasing , ie , the boundary between classes starts diminishing in an expected sense . Despite all this , SNBC is able to perform well . The reason is that even though inter class separability diminishes in an expected sense , low degree nodes which earlier had very less information for classification , now have a better representation .
Citation networks are generally sparse , and many low degree nodes can be expected to occur in such a network . In our experimental results , we find that SNBC was able to improve remarkably over all other techniques in both CoRA and PubMed citation networks . In our setting , where the network is sparsely labeled , some of the features important to low degree nodes got ignored , leading to misclassification . While learning using global neighborhood , these features are extended by exploiting neighborhood information , which leads to improved performance . Similar to citation networks , in case of the Youtube social network also , we record a significant improvement for all metrics . Youtube data being a real world data , has a similar behavior , ie , presence of large number of low degree nodes , which get aided when we look into global neighborhood . However , networks like Amazon and IMDb are synthesized based on similarity . These networks have enough link information for most of the nodes . For example , in the Amazon network , for each book , we have a list of five similar books ; this makes most of the books to have almost the same degree . Expanding representation using global neighborhood in such a case will only lead to accumulation of noise and hence , will show only marginal or no improvement . Similar is the case for IMDb movie network . The Wikipedia hyperlink network is a highly noisy network with lots of between class edges . We find performance of the proposed approach to be significantly better in this case also . In the Wikipedia network , most of the noisy links come out of high degree nodes . But the adopted structured walk approach causes the random walk not to move further from high degree nodes , thus inhibiting accumulation of noise .
7 . CONCLUSION
The work here proposes and demonstrates an approach for classifying network data based on homophily , cluster hypothesis , and structural properties including degrees of nodes . We propose a structured random walk based ap
1093 80.04
76.33
66.30
65.80
61.71
55.69
57.38
57.39
48.08
46.88
41.94
40.94
41.30
37.33
34.05
26.21
30.32
36.57 e r o c S
1 F o r c i M
70
60
50
40
30
20
10
0
90
Linear SVM
SocioDim Modularity
SocioDim EdgeClus
80
SCRN
Deepwalk
SNBC
71.32
67.39
66.68
66.85
62.96
62.14
56.14
55.39
50.01
50.88
47.98
41.26
25.60
25.57
24.63
21.01
19.60
23.21
Youtube
CoRA
Wikipedia
IMDb
Amazon
PubMed
Figure 6 : Micro F1 score for multi label classification compared with the state of the art approaches using 10 % of nodes for training proach to classification , while emphasizing the role of medium degree nodes in classification . A regularizer that underplays the importance of high degree and low degree nodes is implicitly used . This helps us in achieving robust classification by regulating the noise . A comparative study is made using some of the state of the art intra network classification approaches , and a baseline classifier demonstrates the effectiveness of our approach .
8 . REFERENCES
[ 1 ] A . Anagnostopoulos , R . Kumar , and M . Mahdian .
Influence and correlation in social networks . In KDD , pages 7–15 . ACM , 2008 .
[ 2 ] S . Baluja , R . Seth , D . Sivakumar , Y . Jing , J . Yagnik ,
S . Kumar , D . Ravichandran , and M . Aly . Video suggestion and discovery for youtube : taking random walks through the view graph . In WWW , pages 895–904 . ACM , 2008 .
[ 3 ] S . Chakrabarti , B . Dom , and P . Indyk . Enhanced hypertext categorization using hyperlinks . 27(2):307–318 , 1998 .
[ 4 ] R E Fan , K W Chang , C J Hsieh , X R Wang , and C J Lin . Liblinear : A library for large linear classification . JMLR , 9:1871–1874 , 2008 .
[ 5 ] L . A . Goodman . Snowball sampling . The annals of mathematical statistics , pages 148–170 , 1961 .
[ 6 ] M . Ji , J . Han , and M . Danilevsky . Ranking based classification of heterogeneous information networks . In KDD , pages 1298–1306 . ACM , 2011 .
[ 7 ] M . Ji , Y . Sun , M . Danilevsky , J . Han , and J . Gao .
Graph regularized transductive classification on heterogeneous information networks . In ECML PKDD , pages 570–586 . Springer , 2010 .
[ 8 ] J . Kandola , N . Cristianini , and J . S . Shawe taylor .
Learning semantic similarity . In NIPS , pages 657–664 , 2002 .
[ 9 ] R . I . Kondor and J . Lafferty . Diffusion kernels on graphs and other discrete input spaces . In ICML , pages 315–322 , 2002 .
[ 10 ] D . Liben Nowell and J . Kleinberg . The link prediction problem for social networks . JASIST , 58(7):1019–1031 , 2007 .
[ 11 ] F . Lin and W . W . Cohen . Semi supervised classification of network data using very few labels . In ASONAM , pages 192–199 . IEEE , 2010 .
[ 12 ] Q . Lu and L . Getoor . Link based classification . In
ICML , pages 496–503 , 2003 .
[ 13 ] S . A . Macskassy and F . Provost . A simple relational classifier . In MRDM at KDD , 2003 .
[ 14 ] C . Manning , P . Raghavan , and H . Sch¨utze .
Introduction to Information Retrieval . Cambridge University Press , 2008 .
[ 15 ] A . Y . Ng , M . I . Jordan , Y . Weiss , et al . On spectral clustering : Analysis and an algorithm . In NIPS , pages 849–856 . MIT ; 1998 , 2002 .
[ 16 ] L . Page , S . Brin , R . Motwani , and T . Winograd . The pagerank citation ranking : bringing order to the web . 1999 .
[ 17 ] B . Perozzi , R . Al Rfou , and S . Skiena . Deepwalk : Online learning of social representations . In KDD , pages 701–710 . ACM , 2014 .
[ 18 ] J . C . Platt . Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods . Advances in large margin classifiers , 10(3):61–74 , 1999 .
[ 19 ] A . J . Smola and R . Kondor . Kernels and regularization on graphs . In Learning theory and kernel machines , pages 144–158 . Springer , 2003 .
[ 20 ] Y . Sun and J . Han . Mining heterogeneous information networks : a structural analysis approach . ACM SIGKDD Explorations Newsletter , 14(2):20–28 , 2013 .
[ 21 ] P . P . Talukdar and K . Crammer . New regularized algorithms for transductive learning . In ECML PKDD , pages 442–457 . Springer , 2009 .
[ 22 ] J . Tang , M . Qu , M . Wang , M . Zhang , J . Yan , and
Q . Mei . Line : Large scale information network embedding . In WWW , pages 1067–1077 , 2015 .
[ 23 ] L . Tang and H . Liu . Relational learning via latent social dimensions . In KDD , pages 817–826 . ACM , 2009 .
[ 24 ] L . Tang and H . Liu . Leveraging social media networks for classification . DMKD , 23(3):447–478 , 2011 .
[ 25 ] X . Wang and G . Sukthankar . Multi label relational neighbor classification using social context features . In KDD , pages 464–472 . ACM , 2013 .
[ 26 ] M L Zhang and Z H Zhou . A review on multi label learning algorithms . TKDE , 26(8):1819–1837 , 2014 .
[ 27 ] D . Zhou , O . Bousquet , T . N . Lal , J . Weston , and
B . Sch¨olkopf . Learning with local and global consistency . In NIPS , pages 321–328 , 2004 .
[ 28 ] X . Zhu and Z . Ghahramani . Learning from labeled and unlabeled data with label propagation . Technical report , Citeseer , 2002 .
1094
