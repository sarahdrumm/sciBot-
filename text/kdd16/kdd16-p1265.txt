Revisiting Random Binning Features : Fast Convergence and Strong Parallelizability
∗
Lingfei Wu
College of William and Mary
Williamsburg , VA 23185 lfwu@cswmedu
Jie Chen
IBM Research
Yorktown Heights , NY 10598 chenjie@usibmcom
Ian EH Yen * Austin , TX 78712
Unversity of Texas at Austin ianyen@csutexasedu
Rui Yan Baidu Inc .
Beijing 100085 , China yanrui02@baidu.com
ABSTRACT Kernel method has been developed as one of the standard approaches for nonlinear learning , which however , does not scale to large data set due to its quadratic complexity in the number of samples . A number of kernel approximation methods have thus been proposed in the recent years , among which the random features method gains much popularity due to its simplicity and direct reduction of nonlinear problem to a linear one . Different random feature functions have since been proposed to approximate a variety of kernel functions . Among them the Random Binning ( RB ) feature , proposed in the first random feature paper [ 21 ] , has drawn much less attention than the Random Fourier ( RF ) feature proposed also in [ 21 ] . In this work , we observe that the RB features , with right choice of optimization solver , could be orders of magnitude more efficient than other random features and kernel approximation methods under the same requirement of accuracy . We thus propose the first analysis of RB from the perspective of optimization , which by interpreting RB as a Randomized Block Coordinate Descent in the infinitedimensional space , gives a faster convergence rate compared to that of other random features . In particular , we show that by drawing R random grids with at least κ number of non empty bins per grid in expectation , RB method achieves a convergence rate R ) rate from of O(1/(κR) ) , which not only sharpens its O(1/ Monte Carlo analysis , but also shows a κ times speedup over other random features under the same analysis framework . In addition , we demonstrate another advantage of RB in the L1 regularized setting , where unlike other random features , a RB based Coordinate Descent solver can be parallelized with guaranteed speedup proportional to κ . Our extensive experiments demonstrate the superior performance of the RB features over other random features and kernel approximation methods . ∗Both authors contributed equally to this manuscript
√
Keywords Kernel approximation , Random Binning Features , large scale machine learning , faster convergence , strong parallelizability
1 .
INTRODUCTION
Kernel methods have great promise for learning non linear model from simple data input representations and have been demonstrated successful for solving various learning problems such as regression , classification , feature extraction , clustering and dimensionality reduction [ 26 , 30 ] . However , they are typically not first choice for large scale nonlinear learning problems , since large number of samples ( N ) presents significant challenges in terms of computation and memory consumptions to Kernel methods for computing the dense kernel matrix K ∈ RN×N which requires at least a O(N 2 ) complexity . To scale up the kernel methods , there have been great efforts addressing this challenge from various perspectives such as numerical linear algebra , sampling approximation , optimization and functional analysis [ 4 , 5 , 7 , 21 , 27 , 33 ] . A line of research [ 7 , 27 , 28 , 31 ] approximates the kernel matrix K using low rank factorizations , K ≈ Z T Z , where Z ∈ RN×R matrix with R N . Among them , Nyström method [ 5 , 8 , 11 , 27 , 31 ] is probably one of the most popular approaches , which reduces the total computational costs to O(N Rd + N R2 + R3 ) or O(N Rd + N Rm ) depending whether the algorithm performs on K explicitly or implicitly through Z , where d and m are the input data dimension and the number of iterations of an iterative √ solver respectively . However , the convergence of the low rank apN ) [ 5 , 33 ] , which proximation is proportional to O(1/ implies that the rank R may need to be near linear to the number of data points in order to achieve comparable generalization error compared to the vanilla kernel method . For large scale problems , the low rank approximation could become almost as expensive as the exact kernel method to maintain competitive performance [ 29 ] . Another popular approach for scaling up kernel method is random features approximation [ 21 , 22 ] . Unlike previous approach that approximates kernel matrix , Random Features approximate the kernel function directly via sampling from an explicit feature map . Random Fourier ( RF ) is one of the feature maps that attracted considerable interests due to its easy implementation and fast execution time [ 4 , 12 , 20 , 22 , 32 ] , which has total computational cost and storage requirement as O(N Rd + N Rm ) and O(N R ) respectively , for computing feature matrix Z and operating the subsequent algorithms on Z . A Fastfood approach and its extension [ 12 , 32 ] was
√ R + 1/
1265 proposed to reduce the time of computing Fourier features from O(Rd ) to O(R log d ) by leveraging Hadamard basis functions , which improves the efficiency for prediction but not necessarily for training if d m . Although RF has been successfully applied to speech recognition and vision classifications on very large datasets [ 3 , 10 , 17 ] , a drawback is that a significant large number of random features are needed to achieve a comparable performance to exact kernel method . This is not surprising since the convergence of apN ) [ 4 , 22 ] , which proximation error is in the order O(1/ is the same as that of low rank kernel approximations .
√ R+1/
√
Mercer ’s theorem [ 19 ] guarantees that any positive definite kernel permits a feature map decomposition . However , the decomposition is not unique . One may find different feature maps to construct the same kernel function [ 21 , 33 ] . Therefore , we ask following question : do some of the feature maps lead to faster convergence than the others in terms of approximation ? In this paper , we address this question by reconsidering the Random Binning ( RB ) feature map , which was proposed in the first Random Feature paper [ 21 ] but has drawn much less attentions since then compared to the RF feature . Our main contributions are fourfold .
First , we propose the first analysis of RB from the perspective of optimization . By interpreting RB as a Randomized Block Coordinate Descent ( RBCD ) in the infinite dimensional space induced from the kernel , we prove that RB enjoys faster convergence than other random features . Specifically , by drawing R grids with expected number of non empty bins per grid lower bounded by κ , RB can achieve a solution comparable to exact kernel method with O(1/(κR ) ) precision in terms of the objective function , which is R ) rate from Monte Carlo not only better than the existing O(1/ analysis [ 21 ] , but also shows a κ times speedup over the rate of other random features under the same analysis framework [ 33 ] .
√
Second , we exploit the sparse structure of the feature matrix Z , which is the key to rapidly transform the data features into a very high dimension feature space that is linearly separately by any regressors and classifiers . In addition , we discuss how to efficiently perform the computation for a large , sparse matrix by using stateof the art iterative solvers and advanced matrix storage techniques . As a result , the computational complexity and storage requirements in training are still O(N Rd + N Rm ) and O(N R ) , respectively . Third , we show that Random Binning features is particularly suitable for Parallel Coordinate Descent solver . Unlike other random features , RB guarantees a speedup proportional to κ due to a sparse feature matrix . This is particularly useful in the Sparse Random Feature setting [ 33 ] , where L1 regularization is used to induce a compact nonlinear predictor and Coordinate Descent is presumably the state of the art solver in such setting .
Finally , we provide extensive experiments to demonstrate the faster convergence and better parallelizability of RB in practice . Compared to other popular low rank approximations , RB shows superior performance on both regression and classification tasks under the same computational budgets , and achieves same performance with one to three orders of magnitude reduction in time and memory consumptions . When combined with Coordinate Descent to solve an L1 regularized objective , RB shows an almost linear speedup , in contrast to RF that has almost no speedup .
2 . RANDOM BINNING FEATURE AS KER
NEL APPROXIMATION
In this work , we consider the problem of fitting a nonlinear prediction function f : X → Y in Reproducing Kernel Hilbert Space H from training data pairs {(xn , yn)}N n=1 via regularized Empiri cal Risk Minimization ( ERM )
∗ f
= argmin f∈H f2H +
λ 2
1 N
N n=1
L(f ( xn ) , yn ) ,
( 1 ) where L(z , y ) is a convex loss function with Lipschitz continuous derivative satisfying |L(z1 , y ) − L(z2 , y)| ≤ β|z1 − z2| , which includes several standard loss functions such as the square loss 2 ( z − y)2 , square hinge loss L(z , y ) = max(1 − L(z , y ) = 1 zy , 0)2 and logistic loss L(z , y ) = log(1 + exp(−yz) ) . 2.1 Learning with RKHS
The RKHS H can be defined via a positive definite ( PD ) kernel function k(x1 , x2 ) that measures similarity between samples as
H = f ( · ) =
αik(xi,· ) | αi ∈ R , xi ∈ X
.
( 2 )
K i=1 fl f ( · ) =
One can also define the RKHS via a possibly infinite dimensional feature map { ¯φh(x)}h∈H with each h ∈ H defining a feature function ¯φh(x ) : X → R . The space can be expressed as H = w(h ) ¯φh(·)dh = w , ¯φ(·)H | f2H < ∞ ( 3 ) where w(h ) specifies weights over the set of features {φh(x)}h∈H . The Mercer ’s theorem [ 19 ] connects the above two formulations of RKHS by stating that every PD kernel k( . , . ) can be expressed as an integration over some basis functions {φh(.)}h∈H h∈H
, h∈H k(x1 , x2 ) = p(h)φh(x1)φh(x2)dh = ¯φ(x1 ) , ¯φ(x2)H , ( 4 ) However , the decomposition ( 4 ) is not unique , so one can find different feature maps { ¯φh(.)}h∈H satisfying ( 4 ) for the same kernel k( . , ) In particular , as an example used extensively in this work , the Laplacian Kernel
−x1 − x21
σ k(x1 , x2 ) = exp
,
( 5 ) allows decomposition based on ( i ) Fourier basis map [ 21 ] , ( ii ) RB map [ 21 ] , and also ( iii ) map based on infinite number of decision trees [ 15 ] to name a few . On the other hand , different kernels can be constructed using the same set of basis function {φh(.)} with different distribution p(h ) . For example , the RB feature map can be used to construct any shift invariant kernel of the form [ 21 ]
K(x1 , x2 ) = K(x1 − x2 ) = kj(x1j − x2j ) ,
( 6 ) j=1 by sampling the "width" of bins δj for each feature j from a distribution proportional to δk j ( δ ) is the second derivative of kj(δ ) , assuming the kernel has a non negative second derivative . 2.2 Random Binning Features j ( δ ) , where k
In this section , we describe the Random Binning ( RB ) feature map , which has decomposition of the form d
K(x1 , x2 ) = p(δ)φBδ
( x1)T φBδ
( x2 ) dδ
( 7 )
δ where Bδ is a grid parameterized by δ = ( δ1 , u1 , , δd , ud ) that specifies the width and bias of the grid wrt the d dimensions , and φBδ
( x ) is a vector which has
φb(x ) = 1 , if b = ( x1 − u1
, , xd − ud
) ,
δ1
δd
1266 Figure 1 : Generating process of RB features .
Algorithm 1 Random Binning Features
Given a kernel function k(x1 , x2 ) =d j ( δ ) be a distribution over δ . pj(δ ) ∝ δk for r = 1R do j=1 kj(|x1j −x2j| ) . Let
1 . Draw δrj ∼ pj(δ ) , ∀j ∈ [ d ] . urj ∈ [ 0 , δrj],∀j ∈ [ d ] . 2 . Compute feature zr(xn ) as the the indicator vector of bin index ( xn1−u1
) , for ∀n ∈ [ N ] .
, , xnd−ud
δd
[ z1(xn ) ; ; zD(xn ) ] ∀n ∈ [ N ] as the
δ1 end for . Return z(xn ) = 1√ D data with RB Features . and φb(x ) = 0 otherwise for any b ∈ Bδ . Note for each grid Bδ , the number of bins |Bδ| is countably infinite , so φBδ ( x ) has infinite dimension but only 1 non zero entry ( at the bin x lies in ) . Figure 1 illustrates an example when the raw dimension d = 2 . The kernel K(x1 , x2 ) is thus interpreted as the collision probability that two data points x1 , x2 fall in the same bin , when the grid is generated from distribution p(δ ) . In [ 21 ] , it is pointed out for any kernel of form ( 6 ) with nonnegative second derivative k j ( δ ) , one j=1 pj(δj)U ( uj ; 0 , δj ) , where j ( δj ) and U ( · , a , b ) is uniform distribution in the can derive distribution p(δ ) = d pj(δj ) ∝ δk range [ a , b ] .
To obtain a kernel approximation scheme from the feature map ( 7 ) , a simple Monte Carlo method can be used to approximate ( 7 ) by averaging over R grids {Bδr}R r=1 with each grid ’s parameter δr drawn from p(δ ) . The procedure for generating R RB features from raw data {xn}N n=1 is given in Algorithm 1 .
Using a Monte Carlo analysis , one can show the approximation R ) . From the to ( 7 ) yields approximation error of order O(1/ Representer theorem , one can further bound error of the learned predictor
√ fififi = fififififi N n=1 n z(xn)T z(x ) − N
αRF
∗ nk(xn , x )
α n=1 fififiwT
RF z(x ) − f
∗
( x ) as shown in [ 21 ] ( appendix C ) . Unfortunately , the rate of convergence suggests that to achieve small approximation error , one needs significant amount of random features proportional to Ω(1/ 2 ) , and furthermore , the Monte Carlo analysis does not explain why empirically RB feature achieves faster convergence than other random feature map like Fourier basis by orders of magnitude .
3 . FASTER CONVERGENCE OF RANDOM
BINNING
In this section , we first illustrate the sparse structure of the feature matrix Z of RB and discuss how to make efficient computa
Figure 2 : Example of the sparse feature matrix ZN×D generated by RB . In this special case , Z has the number of rows N = 200 and columns D = 395 , respectively . The number of grids R = 10 and the nnz(Z ) = 2000 . Note that for ith row of Z , nnz(Z(i , : ) ) = R and R ≤ D ≤ N R . tion and storage format of Z . Then by interpreting RB features as Randomized Block Coordinate Descent in the infinite dimensional space , we prove that RB has a faster convergence rate than other random features . We illustrate them accordingly in the following sections . 3.1 Sparse Feature Matrix & Iterative Solvers A special characteristic of RB compared to other low rank approximations is the fact that the feature matrix generated by RB is typically a large , sparse binary matrix Z ∈ RN×D , where the value of D is determined by both number of grids R and the kernel width parameter ( ex . σ in the case of Laplacian Kernel ) . Different from other random features , D , rather than R , is the actual number of columns of Z . A direct connection between D and R is that the matrix has each row i satisfying nnz(Z(i , : ) ) = R and therefore R ≤ D ≤ N R . Intuitively speaking , RB has more expressive power than RF since it generates a large yet sparse feature matrix to rapidly transform the data space to a very high dimension space , where data could become almost linearly separable by the classifiers . Fig 2 gives an example to illustrate the sparse structure of Z .
In the case of Kernel Ridge Regression ( L2 regularization with square loss ) , if using RB feature to approximate the RKHS , one can solve ( 1 ) directly in its primal form . The weighting vector is simply the solution of the linear system :
( Z T Z + λI)wRB = Z T y .
( 8 )
Note since Z is a large sparse matrix , there is no need to explicitly compute the covariance matrix Z T Z , which is much denser than Z itself . One can apply state of the art sparse iterative solvers such as Conjugate Gradient ( CG ) and GMRES to directly operate on Z [ 25 ] . The main computation in CG or GMRES is the sparse matrixvector products . Let m be the number of iterations , then the total computational complexity of iterative solver is O(m nnz(Z ) ) = O(mN R ) . In addition , since most elements in Z are zeros , the Compressed Sparse Row type matrix storage format should be employed for economically storing Z [ 9 ] , which gives computational cost and memory requirement as O(mN R ) and O(N R ) respectively , a similar cost to that of other low rank approximations despite its much higher dimension . In testing phase , each point produces a sparse feature vector z(x ) ∈ RD based on the grids stored during training , yielding a sparse vector z(x ) with nnz(z(x) ) ) = fififififi
D = 395050100150200250300350N = 200050100150200R = 10 , nnz = 20001267 R and computing the decision function z(x)T wRB onlyl requires O(dR + R ) .
When the ERM is smooth but not quadratic , a Newton CG method that solves smooth problem via a series of local quadratic approximation gives the same complexity per CG iteration [ 14 ] , and note that most of state of the art linear classification algorithms have complexity linear to nnz(Z ) , the number of nonzeros of feature matrix [ 6 ] . In section 4 , we further discuss cases of L1 regularized problem , where a Coordinate Descent algorithm of cost O(nnz(Z ) ) per iteration is discussed . 3.2 Random Binning Features as Block Coor dinate Descent
In [ 33 ] , a new approach of analysis was proposed , which interpreted Random Features as Randomized Coordinate Descent in the infinite dimensional space , and gives a better O(1/R ) rate in the convergence of objective function . In this section , we extend the approach of [ 33 ] to show that , RB Feature can be interpreted as RBCD in the infinite dimensional space , which by drawing a block of features at a time , produces a number of features D significantly more than the number of blocks R , resulting a provably faster convergence rate than other RF . While at the same time , by exploiting state of the art iterative solvers introduced in section 3.1 , the computational complexity of RB does not increase with number of features D but only with the number of blocks R . Consequently , to achieve the same accuracy , RB requires significantly less training and prediction time compared to other RF .
A key quantity to our analysis is an upper bound on the collision probability νδ which specifies how unlikely data points will fall into the same bin , and its inverse κδ := 1/νδ which lower bounds the number of bins containing at least one data point . We define them as follows . Definition 1 . Define collision probability of data D on bin b ∈ Bδ as
|{n ∈ [ N ] | φb(xn ) = 1}|
νb :=
N
.
( 9 )
Let νδ := maxb∈Bδ νb be an upper bound on ( 9 ) , and κδ := 1/νδ be a lower bound on the number of nonempty bins of grid δ .
κ := Eδ[κδ ] = Eδ[1/νδ ]
( 10 ) is denoted as the lower bound on the expected number of ( used ) bins wrt the distribution p(δ ) .
In the RB matrix , the empirical collision probability is simply the average number of non zeros per column , divided by N , a number much smaller than 1 as in the example of Fig 2 . Our analysis assumes a smooth loss function satisfying the following criteria .
Assumption 1 . The loss function L(z , y ) is smooth wrt response z so difference between function difference and its linear approximation can be bounded as
L(z2 , . ) − L(z1 , . ) ≤ ∇L(z1 , .)(z2 − z1 ) + for some constant 0 ≤ β ≤ ∞ .
This assumption is satisfied for a wide range of loss such as square loss ( β = 1 ) , logistic loss ( β = 1/4 ) and L2 hinge loss ( β = 1 ) .
We interpret RB as a Fully Corrective Randomized Block Coor dinate Descent ( FC RBCD ) on the objective function F ( ¯w ) := R( ¯w ) + Loss( ¯w ; φ ) min
¯w
( z2 − z2)2 .
β 2
Loss( ¯w(r ) + ηB ) − Loss( ¯w(r ) ) ≤ 1 N iφbi η +
L
β 2
( ηbi φbi )2
N i=1 where Loss( ¯w ; φ ) = 1 N √
¯φ :=
N p ◦ φ = ( p(δ)φBδ n=1 L( ¯w , φ(xn ) , yn ) and
( .))δ∈H with "◦" denoting the component wise product . The goal is to show that , by performing R steps of FC RBCD on ( 11 ) , one can obtain a ¯wR with comparable regularized loss to that from optimal solution of ( 1 ) . Note one advantage of analysis from this optimization perspective is : it does not rely on Representer theorem , and thus R(w ) 2 w2 or L1 regularizer λw1 , where the can be L2 regularizer λ latter has advantage of giving sparse predictor of faster prediction [ 33 ] . The FC RBCD algorithm maintains an active set of blocks A(r ) which is expanded for R iterations . At each iteration r , the FC RBCD does the following :
1 . Draw δ from p(δ ) ( derived from the kernel k( . , . ) ) . 2 . Expand active set A(r+1 ) := A(r ) ∪ Bδ . 3 . Minimize ( 11 ) subject to a limited support supp( ¯w ) ⊆ A(r+1 ) . Note this algorithm is only used for analysis . In practice , one can draw R blocks of features at a time , and solve ( 11 ) by any optimization algorithm such as those mentioned in section 3.1 or the CD method we introduce in section 4 . Due to space limit , here we prove the case when R( . ) is the non smooth L1 regularizer λ ¯w1 . The smooth case for R(w ) = 2 ¯w2 can be shown in a similar way . Note the objective function λ ( 11 ) can be written as
( 12 )
¯F ( w ) := F (
√ √ p ◦ w ) = R( p ◦ w ) + Loss(w , ¯φ ) . √ p ◦ w . by a scaling of variable ¯w = The below theorem states that , running FC RBCD for R iterations , it generates a solution ¯wR close to any reference solution w∗ in terms of objective ( 12 ) with their difference bounded by O( 1 κR ) . Theorem 1 . Let R be the number of blocks ( grids ) generated by FC RBCD , and w∗ be any reference solution , we have
) ≤ βw∗2 κR for R := R − c > 0 , where c = ( cid:100 ) 2κ( ¯F ( 0)− ¯F ( w∗ ) )
E[ ¯F ( w(R) ) ] − ¯F ( w
∗
βw∗2
( 13 )
.
Proof . Firstly , we obtain an expression for the progress made by each iteration of FC RBCD . Let B := Bδ(r ) be the block drawn at step 1 of FC RBCD , and ¯w(r+1 ) be the minimizer of ( 11 ) subject to support supp( ¯w ) ⊆ A(r+1 ) given by the step 3 . Since B ⊆ A(r+1 ) , we have
F ( ¯w(r+1 ) ) − F ( ¯w(r ) ) ≤ F ( ¯w(r ) + ηB ) − F ( ¯w(r ) )
( 14 ) for any ηB : supp(η ) ⊆ B . Then denote bi as the bin xi falling i = ∇L( ¯w(r)T φ(xi ) , yi ) , by smoothness of the loss ( Asin and L sumption 1 ) , we have
≤ gB , ηB +
βνδ(r )
2
ηB2 where the second inequality uses the fact φbi = 1 and gB := ∇BLoss( ¯w(r ) , φ ) .
( 15 )
( 11 )
Now consider the regularization term , note since block B is drawn from an inifinite dimensional space , the probability that B is in
1268 active set is 0 . Therefore , we have B ∩ A(r ) = ∅ , ¯w(r ) RB( ¯w(r )
B ) = 0 . As a result ,
B = 0 and
F ( ¯w(r ) + ηB ) − F ( ¯w(r ) ) ≤ RB(ηB ) + gB , ηB +
βνδ(r )
2
ηB2
( 16 )
Let ηB be the minimizer of RHS of ( 16 ) . It satisfies ρB + gB + βvδ(r ) ηB = 0 for some ρB ∈ ∂R(ηB ) , and thus ,
F ( ¯w(r ) + ηB ) − F ( ¯w(r ) ) ≤ ρB , ηB + gB , ηB + ρB + gB2 = − 1
2βνδ(r )
βνδ(r )
2
ηB2
( 17 )
E
νδ(r )
1 1
E.ρB + gB2fi
Now taking expectation wrt p(δ ) on both sides of ( 17 ) , we have
E[F ( ¯w(r ) + ηB ) ] − F ( ¯w(r ) ) ≤ − 1 ρB + gB2 2β ≤ − 1 E 2β ≤ − κ ¯ρB + ¯gB2 2β √ ( 18 ) p ◦ g , and the second inequality where ¯ρ := uses the fact that the number of used bins κδ(r ) = 1/νδ(r ) has non negative correlation with the discriminative power of block B measured by the magnitude of gradient with soft thresholding ¯ρB + ¯gB ( ie fewer collisions on grid B implies B to be a better block of features ) .
√ p ◦ ρ , ¯g :=
νδ(r )
The result of ( 18 ) expresses descent amount in terms of the proximal gradient of the reparameterized objective ( 12 ) . Note for B : B = 0 , and RB(¯η)−RB(0 ) = ¯ρ , ¯η ; B∩A(r ) = ∅ , we have w(r ) on the other hand , for B ⊆ A(r ) , we have
0 ∈ arg min
¯ηB
RB(
√ pBwB + ¯ηB ) + ¯gB ,
√ pBwB + ¯ηB
√
β 2κ since they are solved to optimality in the previous iteration . Then E[F ( ¯w(r ) + η ) ] − F ( ¯w(r ) ) ¯ρB + ¯gB2 = ¯ρ , ¯η + ¯g , ¯η + ≤ − κ 2β √ = R( p ◦ ( w(r ) + ¯η ) ) − R(
¯η ¯A(r)2 p ◦ w(r ) ) + ¯g , ¯η + β 2κ
¯η ¯A(r)2 ( 19 ) p◦η . Thus the final where ¯η ¯A(r ) := ( ¯ηB)B:B∩A(r)=∅ and ¯η := step is to show the descent amount given by RHS of ( 19 ) decreases the suboptimality ¯F ( w(r ) ) − ¯F ( w∗ ) significantly . This can be achieved by considering ¯η of the form α(w∗ − w(r ) ) for some α ∈ [ 0 , 1 ] as follows : E[ ¯F ( w(r ) + ¯η ) ] − ¯F ( w(r ) ) ≤ min p ◦ ( w(r ) + ¯η ) ) − R(
√ R(
√
¯F ( w(r ) + ¯η ) − ¯F ( w(r ) ) +
¯F ( (1 − α)w(r ) + αw
∗
√ p ◦ w(r ) ) + ¯g , ¯η + ¯η ¯A(r)2 ) − ¯F ( w(r ) ) +
β 2κ w
βα2 2κ
∗2
−α( ¯F ( w(r ) ) − ¯F ( w
∗
) ) +
βα2 2κ w
∗2 ,
¯η
≤ min
¯η
≤ min α∈[0,1 ] ≤ min α∈[0,1 ]
βw∗2
2βw∗2 where the second and fourth inequalities are from convexity of ¯F ( ) The α minimizing ( 20 ) is α∗ := min( κ( ¯F ( w(r))− ¯F ( w∗ ) ) , 1 ) , which leads to E[ ¯F ( w(r ) + ¯η ) ] − ¯F ( w(r ) ) ≤ − κ( ¯F ( w(r ) ) − ¯F ( w∗))2 ( 21 ) κw∗2 ; otherwise , we have E[ ¯F ( w(r ) + if ¯F ( w(r))− ¯F ( w∗ ) ≤ β ¯η ) ] − ¯F ( w(r ) ) ≤ − β 2κw∗2 . Note the latter case cannot happen more than c = ( cid:100 ) 2κ( ¯F ( 0)− ¯F ( w∗ ) ) times since FC RBCD is a deβw∗2 scent method . Therefore , for r := r−c > 0 , solving the recursion ( 21 ) leads to the conclusion . Note we have √ pw∗ = w∗ in the L1regularized case , and thus the FC RBCD guarantees convergence of the L1 norm objective to the ( non square ) L2 norm objective . The convergence result of Theorem 1 is of the same form to the rate proved in [ 33 ] for other random features , however , with an additional multiplicative factor κ ≥ 1 that speeds up the rate by κ times . Recall that κ is the lower bound on the expected number of bins being used by data samples for each block of features Bδ , which in practice is a factor much larger than 1 , as shown in the Figure 2 and also in our experiments . In particular , in case each grid Bδ has similar number of bins being used , we have D ≈ κR , and thus obtain a rate of the form p ◦ w∗1 ≤ √
E[ ¯F ( w(R) ) ] − ¯F ( w
∗
) βw∗2
D
.
( 22 )
Note for a fixed R , the total number of features D is increasing with kernel parameter 1/σ in the case of Laplacian Kernel , which means the less smooth the kernel , the faster convergence of RB . A simple extreme case is when σ → 0 , where one achieves 0 training loss , and the RB , by putting each sample in a separate bin , converges to 0 loss with R = 1 , D = N . On the other hand , other random features , such as Fourier , still require large R for convergence to 0 loss . In practice , there are many data that require a small kernel bandwidth σ to avoid underfitting , for which RB has dramatically faster convergence than other RF .
4 . STRONG PARALLELIZABILITY OF RAN
DOM BINNING FEATURES
In this section , we study another strength of RB Features in the context of Sparse Random Feature [ 33 ] , where one aims to train a sparse nonlinear predictor that has faster prediction and more compact representation through an L1 regularized objective . In this case , the CD method is known as state of the art solver [ 23 , 34 ] , and we aim to show that the structure of RB allows CD to be parallelized with much more speedup than that of other random features . 4.1 Coordinate Descent Method
Given the N × D data matrix produced by the RB Algorithm 1 , min w∈RD
λw1 +
1 N
L(wT zi , yi )
( 23 )
N n=1
¯η ¯A(r)2
β 2κ a RCD Method solves by minimizing ( 23 ) wrt a single coordinate j
( 20 )
λ|wj + dj| + gjdj +
Mj 2 d2 j min dj
( 24 )
1269 Algorithm 2 Sparse Random Binning Features via Parallel RCD
0 . Generate RB feature matrix Z by Algorithm 1 1 . z1 = 0 , w1 = 0 . for t=1T ( with τ threads in parallel ) do 2 . Draw j from [ D ] uniformly at random . 3 . Compute d∗ 4 . wt+1 := wt + d∗ 5 . Maintain ˆyi,∀i ∈ [ N ] to satisfy ( 27 ) . j by ( 26 ) . j ej . end for at a time , where
N
( ∇jL(wT zi , yi))zij
( 25 ) n=1
1 N gj :=
N is the gradient of loss term in ( 23 ) wrt the j th coordinate , and ij is an upper bound on ∇jjL( ) Note , by Mj := β 1 N focusing on single coordinate , ( 24 ) has a tighter quadratic upper bound than other algorithms such as Proximal Gradient Method , and allows simple closed form solution i=1 z2
∗ j := proxR/Mj
( wj − gj Mj
) − wj
( 26 ) d where
 0 , vj − λ , vj + λ ,
|vj| ≤ λ vj > λ vj < λ
. proxR(vj ) :=
To have efficient evaluation of the gradient ( 25 ) , a practical implementation maintain the responses
ˆyi := wT zi
( 27 ) j ej , so the cost for each coordinate after each update wt+1 := wt+d∗ wise minimization takes O(nnz(zj ) ) time for both gradient evaluation and maintenance of ( 27 ) , where zj := ( zij)i∈[N ] . The algorithm is summarized in Alg . 2 , which just like the iterative solver introduced in section 3.1 , has cost O(nnz(Z ) ) for one pass of all variables j ∈ [ D ] . 4.2 Parallel Randomized Coordinate Descend on Random Binning Features
The RCD , however , is hard to parallelize [ 23 ] . It is known that simultaneous updates of two coordinates j1 , j2 could lead to divergence , and although one can enforce convergence by shortening , the convergence rate will not be improved the step size 1 Mp with parallelization without additional assumption [ 1 , 24 ] .
1
Mj
On the other hand , in [ 24 ] , it is shown that a function with par tially separable smooth term plus a separable non smooth term min w∈RD
F ( w ) := Ω(w ) + fi(w )
( 28 ) can be parallelized with guaranteed speedup in terms of overall complexity , where Ω(w ) is a non smooth separable function and each function fi(w ) is a smooth depends only on at most ω number of variables . The form ( 28 ) , fortunately , fits our objective ( 23 ) with features zi generated by RB . In particular , the generating process of RB guarantees that , for each block of feature Bδ , the i th ) , sample can fall in exactly one bin b = ( xn1−u1 therefore each sample inolves at most R features out of D . Specifically , let Ω(w ) := λw1 and 1 N
, , xnd−ud
L(wT zi , yi ) , fi(w ) :=
δd
δ1
N i=1 we have ω = R . Then by Theorem 19 of [ 24 ] , a parallel RCD of τ threads that selects coordinate j uniformly at random achieves a speed up ( ie time of sequential/time of parallel ) of speedup ratio =
τ
1 + ( R−1)(τ−1 )
D−1
.
( 29 )
When D , R * 1 , and τ = a¯κ+1 where ¯κ := D/R , ( 29 ) becomes a¯κ + 1 1 + a
, speedup ratio =
( 30 ) which equals ( ¯κ + 1)/2 when a = 1 and approaches ¯κ when a → ∞ . Therefore , it is guaranteed in theory that parallelization can speedup RCD significantly as long as ¯κ = D/R * 1 . We give our sparse RB Features algorithm based on parallel RCD in Alg . 2 . Note for other Random Features , there is no speedup guaranteed and our experiment shows that Parallel RCD performed on Random Fourier features could even have no speedup .
Note that the speedup achieved in this section is orthogonal to the faster convergence rate achieved in section 3 , so by increasing κ , the advantage of RB over other Random Features is superlinearly increasing if a parallel RCD is used . Note also that the results ( 29 ) , ( 30 ) also apply to algorithms that utilize Coordinate Descent as subproblem solvers such as Proximal ( Quasi ) Newton Method [ 13 , 35 ] . Those methods are typically employed for computationally expensive loss functions .
5 . EXPERIMENTS
In this section , we present extensive sets of experiments to demonstrate the efficiency and effectiveness of RB . The datasets are chosen to overlap with those in other papers in the literature , where the details are shown in the table 1 . All sets except census are available at LIBSVM data set [ 2 ] . All computations are carried out on a DELL dual socket with Intel Xeon processors at 2.93GHz for a total of 16 cores and 250 GB of memory running the SUSE Linux operating system . We implemented all methods in C++ and all dense matrix operations are performed by using the optimized BLAS and LAPACK routines provided in the OpenBLAS library . Due to the limited space , we only choose subsets of our results to present in each subsection . However , these results are objective and unbiased .
Name cadata census ijcnn1 cod_rna covtype SUSY mnist acoustic letter
Table 1 : Properties of the datasets . C : Classes
1 1 2 2 2 2 10 3 26 d : Features N : Train M : Test 4,128 2,273 91,701 271,617 116,203 1,000,000
16,512 18,186 35,000 49,437 464,809 4,000,000
60,000 78,823 10,500
10,000 19,705 5,000
8 119 22 8 54 18 780 50 16
5.1 Effects of σ and R on Random Binning
We perform experiments to investigate the characteristics of RB by varying the kernel parameter λ and the rank R , respectively . We use a regularization λ = 0.01 to make sure the reasonable performance of RB and other low rank kernels , although we found that RB is not sensitive to this parameter . We increase the σ in the large interval from 1e−2 to 1e2 so that the optimal σ locates within the interval . We apply CG iterative solver to operate on Z directly .
1270 In order to make fair runtime comparison in each run , we set the tol = 1e − 15 to force similar CG iterations with different σ .
We evaluate the training and testing performance of regression and classification , when varying σ with fixed R . In [ 21 ] , it does not consider the effect of σ in their analysis , which however has a large impact on the performance since D depends on the number of bins which is controlled by σ . Fig 3 shows that the training and testing performance coincidentally decrease ( increase ) before they diverge when D grows by increasing σ . This confirms with our analysis in Theorem 1 that the larger κ , the faster convergence of RB Feature ( recall that the convergence rate is O(1/(κR)) ) .
Second , one should not be surprised that the empirical training time increases with D . The operations involving the weighting vector wRB could become as expensive as a sparse matrix vector operation in an iterative solver . However , the total computational costs are still bounded by O(N R ) but the constant factor may vary with different datasets . Fortunately , in most of cases , the training time corresponding to the peak performance is just slightly higher than the smallest one . In practice , there are several ways to improve the computation costs by exploiting more advanced sparse matrix techniques such as preconditioning and efficient storage scheme , which is out scope of this paper and left for future study .
Finally , we evaluate the training and testing performance when varying R with fixed σ . Fig 4 shows that the training and testing performance converge almost linearly with D , which again confirms our analysis in Theorem 1 . In addition , we observe that RB has strong overfit ability which turns out to be a strong attribute , especially when the hypothesis space has not yet saturated . 5.2 Performance Comparisons of All Methods We present a large sets of experiments to compare RB with other most popular low rank kernel approximations , including RF [ 21 ] , Nyström [ 31 ] , and recently proposed independent block approximation [ 29 ] . We also compare all methods with the exact kernel as a benchmark [ 26 ] . We do not report the results of the vanilla kernel on covtype and SUSY since the programs run out of memory . To make a fair comparison , we also apply CG on RB and Nyström directly on Z to admit similar computational costs . Since the independent block kernel approximation approximates the kernel matrix directly , we employ direct solver of dense matrix for this method . In practice , the CG iterative solver has no need to solve in high precision [ 3 ] , which has also been observed in our experiments . Thus , we set the tolerance to 1e − 3 .
Fig 5 clearly demonstrates the superiority of RB compared to other low rank kernels . For example , in the first column , RB significantly outperforms other methods in testing performance on all of these datasets , especially when R is relatively small . This is because RB enjoys much faster convergence rate to the optimal function than other methods . The advantage generally diminishes when R increases to reasonably large . However , for some large datasets such as covtype and SUSY , increasing number of random features or R boosts the performance extremely slow . This is consistent with our analysis that RB enjoys its fast convergence rate √ of O(1/(κR ) ) while other methods has slow convergence rates R ) . The third and fourth columns further promote the inO(1/ sights about how many number of random features or how large rank R that is needed for achieving similar performance of RB . In particular , RB is often between one and three orders of magnitude faster and less memory consumptions than other methods .
In the second column , we also observe that the training time of all low rank kernels are linear with R , which is expected since all these methods has computational complexity of O(kN R ) . The difference in training time between these low rank kernels is only within some constant factors . However , we point out that the computations of RF , Nyström and independent block approximation are mainly carried out by the high optimized BLAS library since they are dense matrices . In contrast , the computations of RB are most involved in sparse matrix operations , which are self implemented and not yet optimized . In addition , more advanced sparse matrix techniques such as preconditioning can be explored to significantly accelerate the computation , which we leave it as future work . 5.3 Parallel Performance of Random Binning and Random Fourier
We perform experiments to compare RB with RF when using RCD to solve L1 regularized Lasso and kernel SVM for both regression and binary classification problems . Since the goal is to demonstrate the strong parallel performance of RB , we implement the basic parallel implementation of RCD based on simple shared memory parallel programming model with OpenMP . We leave the high performance distributed RCD implementation as one of the future works . We define the speedup of RCD on multicore implementation as follows : speedup = runtime of RCD using single core runtime using P cores
As shown in Fig 6 , when the sparsity level of the feature matrix Z is high , the near linear speedup can be achieved [ 16 , 18 ] . This is because the minimization problem can almost be separated along the coordinate axes , then higher degrees of parallelism are possible . In contrast , if Z is lack of sparsity , then the penalty for data correlations slows the speedup to none . This is confirmed by no gain of parallel speedup of RF since Z is always fully dense . Obviously , in order to empower strong parallel performance of RB , a very large D is expected , which interestingly coincides with power of its faster convergence . Therefore , one can enjoy the double benefits of fast convergence and strong parallelizability of RB , which is especially useful for very large scale problems .
6 . CONCLUSIONS
In this paper , we revisit RB features , an overlooked yet very powerful random features , which we observe often to be orders of magnitude faster than other random features and kernel approximation methods to achieve the same accuracy . Motivated by these impressive empirical results , we propose the first analysis of RB from the perspective of optimization , to make a solid attempt to quantify its faster convergence , which is not captured by traditional Monte Carlo analysis . By interpreting RB as a RBCD in the infinite dimensional space , we show that by drawing R grids with at least κ expected number of non empty bins per grid , RB achieves a convergence rate of O(1/(κR) ) . In addition , in the L1regularized setting , we demonstrate the sparse structure of RB features allows RCD solver to be parallelized with guaranteed speedup proportional to κ . Our extensive experiments demonstrate the superior performance of the RB features over other random feature and kernel approximation methods .
7 . ACKNOWLEDGEMENT
This work was done while L . Wu was a research intern at IBM Research . J . Chen is supported in part by the XDATA program of the Advanced Research Projects Agency ( DARPA ) , administered through Air Force Research Laboratory contract FA8750 12C 0323 .
1271 ( a ) cadata
( b ) cadata
( c ) census
( d ) census
( e ) ijcnn1
( f ) ijcnn1
( g ) covtype
( h ) covtype
( i ) SUSY
( j ) SUSY
( k ) mnist
( l ) mnist
( m ) acoustic
( n ) acoustic
( o ) letter
( p ) letter
Figure 3 : Train and test performance , and train time when varying σ with fixed R . The black line and square box represent the best test performance of the exact kernel and RB respectively .
( a ) cadata
( b ) ijcnn1
( c ) acoustic
( d ) letter
Figure 4 : Train and test performance when varying R with fixed σ .
References [ 1 ] J . K . Bradley , A . Kyrola , D . Bickson , and C . Guestrin . Parallel coordinate descent for l1 regularized loss minimization . CoRR , abs/1105.5379 , 2011 .
[ 3 ] J . Chen , L . Wu , K . Audhkhasi , B . Kingsbury , and B . Ramabhadran . Efficient one vs one kernel ridge regression for speech recognition . In ICASSP , 2016 .
[ 2 ] C . Chang and C . Lin . Libsvm : a library for support vector machines .
ACM Transactions on Intelligent Systems and Technology , 2:27:1–27:27 , 2011 .
[ 4 ] B . Dai , B . Xie , N . He , Y . Liang , A . Raj , M F F . Balcan , and
L . Song . Scalable kernel methods via doubly stochastic gradients . In NIPS . Curran Associates , Inc . , 2014 .
102104106D ( Varying σ)0020406081Errorcadata : Train and Test ErrorTrainR=128TestR=128LowestError102104106D ( Varying σ)10 1100101Time ( Seconds)cadata : Train TimeTestR=128LowestError104106D ( Varying σ)0020406081Errorcensus : Train and Test ErrorTrainR=128TestR=128LowestError104106D ( Varying σ)10 1100101Time ( Seconds)census : Train TimeTestR=128LowestError102104106D ( Varying σ)020406080100Accuracy %ijcnn1 : Train and Test AccuracyTrainR=128TestR=128PeakAccuracy102104106D ( Varying σ)10 1100101102Time ( Seconds)ijcnn1 : Train TimeTestR=128PeakAccuracy102104106108D ( Varying σ)30405060708090100Accuracy %covtype : Train and Test AccuracyTrainR=128TestR=128102104106108D ( Varying σ)100101102103Time ( Seconds)covtype : Train TimeTestR=128PeakAccuracy1021041061081010D ( Varying σ)020406080100Accuracy %SUSY : Train and Test AccuracyTrainR=128TestR=1281021041061081010D ( Varying σ)101102103104Time ( Seconds)SUSY : Train TimeTestR=128PeakAccuracy104106D ( Varying σ)020406080100Accuracy %mnist : Train and Test AccuracyTrainR=128TestR=128PeakAccuracy104106D ( Varying σ)100101102103Time ( Seconds)mnist : Train TimeTestR=128PeakAccuracy102104106D ( Varying σ)20406080100Accuracy %acoustic : Train and Test AccuracyTrainR=128TestR=128PeakAccuracy102104106D ( Varying σ)100101102Time ( Seconds)acoustic : Train TimeTestR=128PeakAccuracy102104106D ( Varying σ)020406080100Accuracy %letter : Train and Test AccuracyTrainR=128TestR=128PeakAccuracy102104106D ( Varying σ)100101102Time ( Seconds)letter : Train TimeTestR=128PeakAccuracy102103104D ( Varying R)020250303504045Errorcadata : Train and Test ErrorTrain σ=0.56Test σ=0.56Lowest Error105106D ( Varying R)93949596979899100Accuracy %ijcnn1 : Train and Test AccuracyTrain σ=1.12Test σ=1.12Peak Accuracy103104105D ( Varying R)6570758085Accuracy %acoustic : Train and Test AccuracyTrain σ=0.28Test σ=0.28Peak Accuracy103104105D ( Varying R)5060708090100Accuracy %letter : Train and Test AccuracyTrain σ=0.28Test σ=0.28Peak Accuracy1272 ( a ) cadata
( b ) cadata
( c ) cadata
( d ) cadata
( e ) ijcnn1
( f ) ijcnn1
( g ) ijcnn1
( h ) ijcnn1
( i ) covtype
( j ) covtype
( k ) covtype
( l ) covtype
( m ) SUSY
( n ) SUSY
( o ) SUSY
( p ) SUSY
( q ) mnist
( r ) mnist
( s ) mnist
( t ) mnist
( u ) acoustic
( v ) acoustic
( w ) acoustic
( x ) acoustic
Figure 5 : Comparisons among RB , RF , Nyström and Independent Block approximation . The first and second columns plot test performance and train time when increasing R . The third and fourth columns plot the train time and memory consumptions when achieving the desired test performance .
101102103R02202402602803032034036Test Errorcadata : Test Error VS RRandBinFourierNystromBlockDiagKernel101102103R10 410 2100102104Train Time ( Seconds)cadata : Train Time VS RRandBinFourierNystromBlockDiagKernel02402502602702802903Test Error %10 2100102104Train Time ( Seconds)cadata : Train Time VS Test ErrorRandBinFourierNystromBlockDiag02402502602702802903Test Error %100101102103104105Normalized Memory Estimatecadata : Memory Estimate VS Test ErrorRandBinFourierNystromBlockDiag101102103R889092949698100Test Accuracy %ijcnn1 : Test Accuracy VS RRandBinFourierNystromBlockDiagKernel101102103R10 2100102104Train Time ( Seconds)ijcnn1 : Train Time VS RRandBinFourierNystromBlockDiagKernel959559696597975Test Accuracy %10 1100101102103Train Time ( Seconds)ijcnn1 : Train Time VS AccuracyRandBinFourierNystromBlockDiag959559696597975Test Accuracy %100101102103104105Normalized Memory Estimateijcnn1 : Memory Estimate VS AccuracyRandBinFourierNystromBlockDiag101102103R5060708090100Test Accuracy %covtype : Test Accuracy VS RRandBinFourierNystromBlockDiag101102103R10 1100101102103Train Time ( Seconds)covtype : Train Time VS RRandBinFourierNystromBlockDiag60708090100Test Accuracy %10 1100101102103104Train Time ( Seconds)covtype : Train Time VS AccuracyRandBinFourierNystromBlockDiagTest Accuracy %6065707580859095100Normalized Memory Estimate100101102103104105covtype : Memory Estimate VS AccuracyRandBinFourierNystromBlockDiag101102103R55606570758085Test Accuracy %SUSY : Test Accuracy VS RRandBinFourierNystromBlockDiag101102103R100101102103104Train Time ( Seconds)SUSY : Train Time VS RRandBinFourierNystromBlockDiag556065707580Test Accuracy %100101102103104105Train Time ( Seconds)SUSY : Train Time VS AccuracyRandBinFourierNystromBlockDiag556065707580Test Accuracy %100101102103104Normalized Memory EstimateSUSY : Memory Estimate VS AccuracyRandBinFourierNystromBlockDiag101102103R405060708090100Test Accuracy %mnist : Test Accuracy VS RRandBinFourierNystromBlockDiagKernel101102103R10 2100102104106Train Time ( Seconds)mnist : Train Time VS RRandBinFourierNystromBlockDiagKernel86889092949698Test Accuracy %100101102103104Train Time ( Seconds)mnist : Train Time VS AccuracyRandBinFourierNystromBlockDiag86889092949698Test Accuracy %100101102103104105Normalized Memory Estimatemnist : Memory Estimate VS AccuracyRandBinFourierNystromBlockDiag101102103R556065707580Test Accuracy %acoustic : Test Accuracy VS RRandBinFourierNystromBlockDiagKernel101102103R10 2100102104106Train Time ( Seconds)acoustic : Train Time VS RRandBinFourierNystromBlockDiagKernel747576777879Test Accuracy %10 1100101102103104Train Time ( Seconds)acoustic : Train Time VS AccuracyRandBinFourierNystromBlockDiag747576777879Test Accuracy %100101102103104105Normalized Memory Estimateacoustic : Memory Estimate VS AccuracyRandBinFourierNystromBlockDiag1273 ( a ) ijcnn1
( b ) cod_rna
( c ) covtype
( d ) SUSY
Figure 6 : Comparisons of parallel performance between RB and RF using RCD when increasing the number of threads .
[ 5 ] P . Drineas and M . W . Mahoney . On the nyström method for approximating a gram matrix for improved kernel based learning . JMLR , 6:2153–2175 , Dec . 2005 .
[ 22 ] A . Rahimi and B . Recht . Weighted sums of random kitchen sinks : Replacing minimization with randomization in learning . In NIPS . Curran Associates , Inc . , 2008 .
[ 6 ] R E Fan , K W Chang , C J Hsieh , X R Wang , and C J Lin .
Liblinear : A library for large linear classification . JMLR , 9:1871–1874 , 2008 .
[ 23 ] P . Richtárik and M . Takáˇc . Iteration complexity of randomized block coordinate descent methods for minimizing a composite function . Mathematical Programming , 144(1 2):1–38 , 2014 .
[ 7 ] S . Fine and K . Scheinberg . Efficient svm training using low rank kernel representations . JMLR , 2:243–264 , Mar . 2002 .
[ 8 ] A . Gittens and M . W . Mahoney . Revisiting the nyström method for improved large scale machine learning . CoRR , abs/1303.1849 , 2013 .
[ 9 ] G . H . Golub and C . F . Van Loan . Matrix Computations . Johns
Hopkins University Press , Baltimore , MD , USA , 1996 .
[ 10 ] P S Huang , H . Avron , T . Sainath , V . Sindhwani , and
B . Ramabhadran . Kernel methods match deep neural networks on timit . In ICASSP , 2014 .
[ 11 ] S . Kumar , M . Mohri , and A . Talwalkar . Sampling methods for the nyström method . JMLR , 13:981–1006 , Apr . 2012 .
[ 12 ] Q . V . Le , T . Sarlós , and A . J . Smola . Fastfood : Approximate kernel expansions in loglinear time . ICML , 2013 .
[ 13 ] J . Lee , Y . Sun , and M . Saunders . Proximal newton type methods for convex optimization . In Advances in Neural Information Processing Systems , pages 836–844 , 2012 .
[ 14 ] C J Lin , R . C . Weng , and S . S . Keerthi . Trust region newton method for logistic regression . JMLR , 9:627–650 , 2008 .
[ 15 ] H T Lin and L . Li . Support vector machinery for infinite ensemble learning . JMLR , 9:285–312 , 2008 .
[ 16 ] J . Liu , S . J . Wright , C . Ré , and V . Bittorf . An asynchronous parallel stochastic coordinate descent algorithm . JMLR , 32(1):469–477 , 2014 .
[ 17 ] Z . Lu , A . May , K . Liu , A . B . Garakani , D . Guo , A . Bellet , L . Fan , M . Collins , B . Kingsbury , M . Picheny , and F . Sha . How to scale up kernel methods to be as good as deep neural nets . CoRR , abs/1411.4000 , 2014 .
[ 18 ] J . Mareˇcek , P . Richtárik , and M . Takáˇc . Distributed block coordinate descent for minimizing partially separable functions . Numerical Analysis and Optimization , 134:261–288 , 2014 .
[ 19 ] J . Mercer . Functions of positive and negative type , and their connection with the theory of integral equations . Royal Society London , A 209:415–446 , 1909 .
[ 20 ] M . Raginsky and S . Lazebnik . Locality sensitive binary codes from shift invariant kernels . In Y . Bengio , D . Schuurmans , J . D . Lafferty , C . K . I . Williams , and A . Culotta , editors , Advances in Neural Information Processing Systems 22 , pages 1509–1517 . Curran Associates , Inc . , 2009 .
[ 21 ] A . Rahimi and B . Recht . Random features for large scale kernel machines . In NIPS . Curran Associates , Inc . , 2007 .
[ 24 ] P . Richtárik and M . Takáˇc . Parallel coordinate descent methods for big data optimization . Mathematical Programming , pages 1–52 , 2015 .
[ 25 ] Y . Saad . Iterative Methods for Sparse Linear Systems . Society for Industrial and Applied Mathematics , Philadelphia , PA , USA , 2nd edition , 2003 .
[ 26 ] B . Scholkopf and A . J . Smola . Learning with Kernels : Support
Vector Machines , Regularization , Optimization , and Beyond . MIT Press , Cambridge , MA , USA , 2001 .
[ 27 ] S . Si , C . Hsieh , and I . S . Dhillon . Memory efficient kernel approximation . In ICML , 2014 .
[ 28 ] A . J . Smola and B . Schökopf . Sparse greedy matrix approximation for machine learning . In ICML , ICML ’00 , San Francisco , CA , USA , 2000 . Morgan Kaufmann Publishers Inc .
[ 29 ] M . L . Stein . Limitations on low rank approximations for covariance matrices of spatial data . Spatial Statistics , 8:1 – 19 , 2014 . Spatial Statistics Miami .
[ 30 ] B . Taskar , C . Guestrin , and D . Koller . Max margin markov networks .
In NIPS . MIT Press , 2004 .
[ 31 ] C . K . I . Williams and M . Seeger . Using the nyström method to speed up kernel machines . In NIPS . MIT Press , 2001 .
[ 32 ] Z . Yang , A . G . Wilson , A . J . Smola , and L . Song . A la carte learning fast kernels . In AISTATS , 2015 .
[ 33 ] I . E H Yen , T W Lin , S D Lin , P . K . Ravikumar , and I . S . Dhillon .
Sparse random feature algorithm as coordinate descent in hilbert space . In JMLR , 2014 .
[ 34 ] G X Yuan , K W Chang , C J Hsieh , and C J Lin . A comparison of optimization methods and software for large scale l1 regularized linear classification . JMLR , 11:3183–3234 , 2010 .
[ 35 ] K . Zhong , I . E H Yen , I . S . Dhillon , and P . K . Ravikumar . Proximal quasi newton for computationally intensive l1 regularized m estimators . In Advances in Neural Information Processing Systems , pages 2375–2383 , 2014 .
051015Number of Threads0246810Speedup ( over Sequential)ijcnn1 : Speedup in Train timeRandBinFourier051015Number of Threads0123456Speedup ( over Sequential)cod_rna : Speedup in Train timeRandBinFourier051015Number of Threads024681012Speedup ( over Sequential)covtype : Speedup in Train timeRandBinFourier051015Number of Threads012345Speedup ( over Sequential)SUSY : Speedup in Train timeRandBinFourier1274
