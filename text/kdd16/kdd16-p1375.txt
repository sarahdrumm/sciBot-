Accelerating Online CP Decompositions for
Higher Order Tensors
Shuo Zhou1 , Nguyen Xuan Vinh1 , James Bailey1 , Yunzhe Jia1 , Ian Davidson2
1Dept . of Computing and Information Systems , The University of Melbourne , Australia
1{zhous@student . , vinh.nguyen@ , baileyj@ , yunzhej@student}unimelbeduau
2Dept . of Computer Science , University of California , Davis , USA
2davidson@csucdavisedu
ABSTRACT Tensors are a natural representation for multidimensional data . In recent years , CANDECOMP/PARAFAC ( CP ) decomposition , one of the most popular tools for analyzing multi way data , has been extensively studied and widely applied . However , today ’s datasets are often dynamically changing over time . Tracking the CP decomposition for such dynamic tensors is a crucial but challenging task , due to the large scale of the tensor and the velocity of new data arriving . Traditional techniques , such as Alternating Least Squares ( ALS ) , cannot be directly applied to this problem because of their poor scalability in terms of time and memory . Additionally , existing online approaches have only partially addressed this problem and can only be deployed on third order tensors . To fill this gap , we propose an efficient online algorithm that can incrementally track the CP decompositions of dynamic tensors with an arbitrary number of dimensions . In terms of effectiveness , our algorithm demonstrates comparable results with the most accurate algorithm , ALS , whilst being computationally much more efficient . Specifically , on small and moderate datasets , our approach is tens to hundreds of times faster than ALS , while for large scale datasets , the speedup can be more than 3,000 times . Compared to other state of the art online approaches , our method shows not only significantly better decomposition quality , but also better performance in terms of stability , efficiency and scalability .
Keywords Tensor Decomposition ; CP Decomposition ; Online Learning
1 .
INTRODUCTION
Numerous types of data are naturally represented as multidimensional structures . The Tensor , a multi way generalization of the matrix , is useful for representing such data . Similar to matrix analysis tools , such as PCA and SVD , tensor decomposition ( TD ) is a popular approach for feature extraction , dimensionality reduction and knowledge discovery on multi way data . It has been extensively studied and widely applied in various fields of science , including chemometrics [ 1 ] , signal processing [ 7 ] , computer vision [ 14 , 29 ] , graph and network analysis [ 10 , 17 ] and time series analysis [ 5 ] .
In the era of big data , data is often dynamically changing over time , and a large volume of new data can be generated at high velocity . In such dynamic environments , a data tensor may be expanded , shrunk or modified on any of its dimensions . For example , given a network monitoring ten sor structured as sourceˆ destinationˆ portˆ time , a large number of network transactions are generated every second , which can be recorded by appending new slices to the tensor on its time mode . Additionally , new IP addresses may be added and invalid addresses may be removed from the data tensor . Overall , this data tensor is highly dynamic .
As TD is usually the first and necessary step for analyzing multi way data , in this work , we aim to address the problem of how to adaptively track the decompositions for such timeevolving tensors . Specifically , we are particularly interested in dynamic tensors that are incrementally growing over time , while the other dimensions remain unchanged . These are the most common type of dynamic tensors that occur in practice . We refer to such tensors as online tensors , also known as tensor streams and incremental tensors [ 27 , 28 ] .
Finding the decompositions for large scale online tensors is challenging . The difficulty mainly arises from two factors . First , as online tensors are growing with time , their overall size is potentially unbounded . Thus , TD techniques for such tensors need to be highly efficient and scalable , from both time and space perspectives . Second , a high data generation rate demands decomposition methods providing real time or near real time performance [ 6 ] . However , traditional TD techniques , such as Tucker and CANDECOMP/PARAFAC ( CP ) decompositions , cannot be directly applied to this scenario because : ( i ) they require the availability of the full data for the decomposition , thus having a large memory requirement ; and ( ii ) their fitting algorithms , Higher Order SVD ( HOSVD ) for Tucker [ 9 ] , and Alternating Least Squares ( ALS ) or other variants for CP [ 8 ] , are usually computationally too expensive for large scale tensors .
A recipe for addressing the above challenges is to adapt existing approaches using online techniques . In recent years , several studies have been conducted on tracking the Tucker decomposition of online tensors by incorporating online techniques , such as incremental SVD [ 14 , 18 , 26 ] , and incremental update of covariance matrices [ 27 , 28 ] . However , there is a limited amount of work on tracking the CP decomposition of an online tensor . The only work in the literature , pro
1375 posed by Nion and Sidiropoulos [ 20 ] , specifically only deals with third order tensors and there is no provision for higherorder tensors with more than 3 dimensions . To fill this gap , we propose an efficient algorithm to find the CP decomposition of large scale high order online tensors , with low space and time usage . We summarize our contributions as follows :
‚ We propose a scalable algorithm for efficiently tracking the CP decompositions of online tensors . Not limited to basic third order tensors , our model can also handle higher order tensors that have more than 3 dimensions .
‚ Through experimental evaluation on seven real world datasets , we show that our approach can provide more accurate results and better efficiency , compared with state of the art approaches .
‚ Based on empirical analysis on synthetic datasets , our algorithm produces more stable decompositions than existing online approaches , as well as better scalability .
The rest of this paper is organized as follows . Section 2 gives a review of current techniques and Section 3 introduces background knowledge . Our proposal is discussed in Section 4 . We start from third order tensors and then extend this model to general tensors that have an arbitrary number of dimensions . After that , the performance of our approach is evaluated on both real world and synthetic datasets in Section 5 . Lastly , Section 6 concludes and discusses future research directions .
2 . RELATED WORK
The problem of decomposing online tensors was originally proposed by Sun et al . [ 27 , 28 ] , wherein they refer to this problem as Incremental Tensor Analysis ( ITA ) . Three variants of ITA are discussed in their work : ( 1 ) dynamic tensor analysis ( DTA ) modifies the covariance matrices calculation step in typical HOSVD in an incremental fashion ; ( 2 ) stream tensor analysis ( STA ) is an approximation of DTA by the SPIRIT algorithm [ 21 ] ; and ( 3 ) window based tensor analysis ( WTA ) uses a sliding window strategy to improve the efficiency of DTA . The main issue of these techniques is that they have not fully optimized the most time consuming step , ie , diagonalizing the covariance matrix for each mode , which limits their efficiency . To overcome this issue , Liu et al . [ 17 ] propose an efficient algorithm that enforces the diagonalization on the core tensors only . Additionally , another trend for improving the efficiency of HOSVD on online tensors is to replace SVD with incremental SVD algorithms . Several applications of this idea can be found in computer vision [ 14 , 18 , 26 ] and anomaly dectection [ 25 ] fields .
The major difference between the aforementioned techniques and our approach is that they are online versions of Tucker decomposition . Although CP decomposition can be viewed as a special case of Tucker with super diagonal core tensor , none of the above methods provides a way to enforce this constraint . As a result , these algorithms are not suitable for tracking the CP decompositions of online tensors .
Unlike the existing extensive studies on online Tucker decomposition , there is a limited research reported for online CP decomposition . The most related work to ours was proposed by Nion and Sidiropoulos [ 20 ] , which introduced two adaptive algorithms that specifically focus on CP decomposition : Simultaneous Diagonalization Tracking ( SDT ) that incrementally tracks the SVD of the unfolded tensor ; and Recursive Least Squares Tracking ( RLST ) , which recursively updates the decomposition factors by minimizing the mean squared error . However , the major drawback of this work is that they work on third order tensors only , while in contrast we propose a general approach that can incrementally track the CP decompositions of tensors with arbitrary dimensions . In another related area , among the studies that focus on improving CP decomposition for handling large scale tensors , GridTF , a grid based tensor factorization algorithm [ 22 ] is particularly related to our problem . The main idea of GridTF is to partition the large tensor into a number of small grids . These grids are then factorized by a typical ALS algorithm in parallel . Finally , the resulting decompositions of these sub tensors are combined together using an iterative approach . In fact , as stated by the authors , if such partitioning is enforced on the time mode only , then GridTF can be used for tracking the CP decomposition of tensor streams .
3 . PRELIMINARIES 3.1 Notation and Basic Operations
Following the notation in [ 16 ] , vectors are denoted by boldface lowercase letters , eg , a , matrices by boldface uppercase letters , eg , A , and tensors by boldface Euler script letters , eg , XXX . The order of a tensor , also known as the number of ways or modes , is its number of dimensions . For example , vectors and matrices are tensors of order 1 and 2 . A tensor XXX P R I1ˆ¨¨¨ˆIN is an N th order one consisting of real numbers and the cardinality of its i th order , i P r1 , Ns is Ii . We refer to tensors with more than 3 modes as higher order ones . The elements of a tensor are retrieved by their indices , and a slice of an N th order tensor is anpN ´ 1qth
J piq piq
, A pNq
: and
. .A
Let A
, . . . ,A
, A p1q order tensor with the index of a particular mode fixed . . . denote the transpose , inverse , Moore Penrose pseudoinverse and Frobenius norm of A . Let A represent a sequence of N matrices . The Khatri Rao and Hadamard products [ 16 ] and
´1 , A p2q andfN A when the n th matrix is not included in above operations , tively . Furthermore , the Khatri Rao and Hadamard products of a sequence of N matrices A are . Note that the superscripts element wise division are denoted by d , f and m , respecdenoted bydN A in the sequence are inverted and the subscript i ‰ n is used as dN Tensor unfolding , or matricization , is a process to transform a tensor into a matrix [ 15 ] . Generally , given an N thI1ˆI2ˆ¨¨¨ˆIN , its mode n unfolding Xpnq P order tensor XXX P R InˆśN permuted tensor into a matrix of size In ˆśN i‰n Ii can be obtained by permuting the dimensions of XXX as rIn , I1 , . . . , In´1 , In`1 , . . . , INs and then reshaping the and fN i‰nA pN´1q
, . . . ,A i‰nA piq
. pNq
, A p1q piq
R i‰n Ii .
3.2 CP Decomposition data . Basically , given an N th order tensor XXX P R
CP decomposition is a widely used technique for exploring and extracting the underlying structure of the multi way I1ˆ¨¨¨ˆIN , CP decomposition approximates this tensor by N loading matrices A
, such that
, . . . ,A pNq pn`1q d A pn´1q ¨¨¨ d A p1qqJ p1q pnqpA pnqpÄN p1q pNq d ¨¨¨ A piqqJ i‰n A pNqfi
, . . . ,A
Xpnq « A “ A “ .A
( 1 )
1376 . piq p1q
, . . . ,A
To find the CP decomposition .A where .‚fi is defined as the CP decomposition operator and , i P r1 , Ns is of size Ii ˆ R , where each loading matrices A R is the tensor rank , indicating the number of latent factors . pNqfi for an N th order tensor XXX , the objective is to minimize the estimation error L , which is defined as pnqpdN piqqJ . i‰nA However , directly minimizing L over A p1q is difficult , since L is not convex wrt A . As a result , a widely applied approach is ALS . The main idea is to divide the above optimization problem into N sub problems and the n th one , n P r1 , Ns fixes all variables but A , and then minimizes the convex objective L wrt A pnq , that is piqqJ .
.Xpnq ´ A pnqpdN
L “ 1 2
.Xpnq ´ A i‰nA
, . . . ,A
, . . . ,A pNq pNq pnq p1q
2
.
2
.
A pnq Ð arg min Apnq
1 2
.
( 2 )
3.3 Online CP Decomposition
IˆJˆptold`tnewq
Here we briefly introduce the main idea of existing online CP decomposition algorithms . A third order online tensor XXX P R is used as a running example , where XXX is expanded from XXXold P R IˆJˆtold by appending a new chunk of data XXXnew P R IˆJˆtnew at its last mode . Considering that in most online systems , the size of the new incoming data is usually much smaller than that of all ex isting historical data , thus we assume tnew ! told . The CP decomposition of XXXold is written as .Aold , Bold , Coldfi and the aim is to find the CP decomposition .A , B , Cfi of XXX .
SDT and RLST [ 20 ] : Both SDT and RLST transform the online tensor decomposition problem into an incremental matrix factorization problem , by letting D “ B d A , so that equation ( 1 ) can be written as Xp3q “ CD
. Then the
J problem is how to estimate C and D .
J
, where UΣV
Different strategies are used in SDT and RLST for calculating C and D . SDT chooses to do this by making use J of the SVD of Xoldp3q , UoldΣoldV old . Specifically , there will always be a matrix Wold that Cold “ UoldW ´1 old and Dold “ VoldΣoldW J old . Similarly , a matrix W can be found ´1 and D “ VΣW to make C “ UW J is the SVD of Xp3q , which can be efficiently calculated by incremental SVD algorithms . Furthermore , the authors assume that there is only a tiny difference between D and Dold so that the first told rows of C are approximately equal ´1 can be calculated as to Cold . Under this assumption , W old , where rU is the first told rows of U . Conse rU In contrast , RLST follows a more direct approach to get C and D . Recall that Xp3q “ CD , firstly , Cnew is calculated as Xnewp3qpD and C is updated by appending Cnew to Cold . Then D is incrementally estimated with Xnewp3q and Cnew based on the matrix inversion and pseudo inversion lemmas . Due to the page limit , we refer interested readers to the original papers [ 20 ] for more details . quently , W , C and D can be obtained . oldq : J
UoldW
´1
J
:
After getting C and D , the last step for both SDT and RLST is to estimate A and B from D . This process is done by applying SVD on the matrix formed by each column of D , and then putting the left and right principal singular vectors into A and B , respectively .
Overall , SDT and RLST both deal with the online CP decomposition problem by flattening the non temporal modes . However , this is the main limitation to their performance .
Firstly , it is time consuming due to the cost of SVD . Although the authors replaced the traditional SVD with the
Bi SVD algorithm , the complexity of this is still OpR2IJq , which limits their applications on large scale tensors . Additionally , this flattening process makes SDT and RLST not easy to extend to higher order tensors , since the flattened matrix will be much larger and it has to be recursively decomposed to loading matrices in the end , which is also costly . GridTF [ 22 ] : As mentioned earlier , the basic idea of GridTF is to partition the whole tensor into smaller tensors and then combine their CP decompositions together . In regards to the example here , XXX is the online tensor , XXXold and XXXnew are the two partitions . To obtain the CP decomposition of XXX , the first step of GridTF is to decompose XXXnew by ALS as .Anew , Bnew , Cnewfi , while the decomposition of XXXold is already known from the last time step .
The combination step is a recursive update procedure such that in every iteration , each mode is updated with a modified ALS rule , until the whole estimation converges , or the maximum number of iterations has been reached . In our notation , the update rules are given as follows , of which further details can be found in [ 22 ] .
1 ) For non temporal modes A and B oldAqq A Ð AoldpPold m pA J Q m pAJAq B Ð BoldpPold m pB oldBqq J Q m pBJBq newAqq ` AnewpPnew m pA J Q m pAJAq ` BnewpPnew m pB newBqq J Q m pBJBq
2 ) For temporal mode C
»——– ColdpPold m pC oldCqq J Q m pCJCq CnewpPnew m pC newCqq J Q m pCJCq fiffiffifl
C Ð where A , B , C are randomly initialized at the beginning and
Pold “ pA pB newBq f pC J oldAq f pB J oldBq f pC oldCq , Pnew “ pA J J newCq , andQ “ pA J J
Aq f pB newAq f J Cq .
Bq f pC
J
J
The main issue of applying GridTF to online CP decomposition is its efficiency . Firstly , even though only the new data need to be decomposed , this is still expensive , especially when the size of new data is large . Secondly , for esJ timating C and calculating Pold and Q , C oldC needs to be calculated , costing R2told operations . This means the time complexity of the update procedure is linear in the length of the existing data , told , which can be huge , thus significantly limiting its ability for processing online tensors .
4 . OUR APPROACH
In this section , we introduce our proposal for tracking the CP decomposition of online multi way data in an incremental setting . For presentation clarity , initially a third order case will be discussed . Then , we further extend to more general situations , where our proposed algorithm is able to handle tensors that have arbitrary number of modes . Without loss of generality , we assume the last mode of a tensor is always the one growing over time , while the size of the other modes are kept unchanged with time . 4.1 Third order Tensors
Following the same notation introduced in Section 3.3 , similar to the classic ALS algorithm , our approach handles the problem in an alternating update fashion . That is , we
1377 first fix A and B , to update C , and then sequentially update A and B , by fixing the other two .
411 Update Temporal Mode C By fixing A and B , from ( 2 ) we have
2
C Ð arg min “ arg min
C
C
“ arg min
C
1 2
1 2
1 2
.
.
„ j j „ .Xp3q ´ CpB d AqJ . p1q „ p2q
´ Xoldp3q Xnewp3q Xoldp3q ´ C Xnewp3q ´ C
C C
. . . .
. . . . p1qpB d AqJ p2qpB d AqJ j . pB d AqJ
2
. . .
2
. . . .
( 3 )
It is clear that the norm of the first row is minimized with Cold , since A and B are fixed as Aold and Bold from the last time step . The optimal solution to minimize the second row . As a result , C is updated by is C appending the projection Cnew of Xnewp3q via the loading matrices A and B of previous time step , to Cold , ie , p2q “ Xnewp3qppBd AqJq : „ Xnewp3qppB d AqJq :
Cold Cnew
C “ j
„ j
Cold
“
( 4 )
412 Update Non temporal Modes A and B First , we update A . By fixing B and C , the estimations .Xp1q ´ ApC d BqJ . error L can be written as 1 , and the derivative of L wrt A is
.
.
2
2
“ Xp1qpC d Bq ´ ApC d BqJpC d Bq
BL BA
By setting the derivative to zero and letting P “ Xp1qpCd Bq and Q “ pC d BqJpC d Bq , we have
A “ PQ
´1
( 5 )
Directly calculating P and Q is costly . This is mainly be cause the output of pCd Bq is a huge matrix of size Jptold` tnewq ˆR , where R is the tensor rank . It further results in OpRIJptold ` tnewqq and OpR2Jptold ` tnewqq operations to Bq product can be avoided by calculating it as pC J [ 16 ] , which has a complexity of OpR2pJ ` told ` tnewqq , this get P and Q , respectively . Although for Q , the Khatri Rao
CqfpB
J is still expensive since told is usually quite large . As a result , in order to improve the efficiency , we need a faster approach . Firstly , let us look atP . By representing Xp1q and C with the old and new components , we have
“ “ “ “
P “ Xp1qpC d Bq
Xoldp1q , Xnewp1q
Xoldp1q , Xnewp1q
˙
‰ˆ„ ‰„ j Cold d B Cnew d B
Cold Cnew j d B
“ Xoldp1qpCold d Bq ` Xnewp1qpCnew d Bq
( 6 ) recall that B has been fixed asB old , so that the first part of the last line of equation ( 6 ) only contains components from the previous time step . Suppose we know this part already and denote it by Pold , then ( 6 ) can be rewritten as
P “ Pold ` Xnewp1qpCnew d Bq
( 7 )
This means that by keeping a record of the previous P , the large computation can be avoided and it can be efficiently updated in an incremental way . Specifically , suppose
P is initialized with a small partition XXXpτq P R IˆJˆτ that contains the first τ slices of the data , where τ ! told , we only need OpRIJτq operations to construct P . Afterwards , the cost of OpRIJtnewq , which is independent to told . whenever new data comes , P can be efficiently updated at
Likewise , Q can be estimated as
Q “ Qold ` pCnew d BqJpCnew d Bq
“ Qold ` pC newCnewq f pB J
J
Bq
( 8 )
Thus , by storing the information of previous decomposition with complementary matrices P and Q , we achieve the update rule for A as follows ,
The update rule for B can be derived in a similar way as
P Ð P ` Xnewp1qpCnew d Bq newCnewq f pB Q Ð Q ` pC J J A Ð PQ ´1
Bq
U Ð U ` Xnewp2qpCnew d Aq V Ð V ` pC newCnewq f pA J J B Ð UV ´1
Aq
( 9 )
( 10 ) where U “ Xp2qpC d Aq , V “ pC d AqJpC d Aq are the two complementary matrices of mode 2 .
To sum up : For a third order tensor that grows with time , we propose an efficient algorithm for tracking its CP decomposition on the fly . We name this algorithm as OnlineCP , comprising the following two stages :
1 ) Initialization stage : for non temporal modes , complementary matrices P , Q , U and V are initialized with the initial tensor XXXinit and its CP decomposition .A , B , Cfi as
P “ Xinitp1qpC d Bq , Q “ pC Cq f pB Bq J J Aq Cq f pA U “ Xinitp2qpC d Aq , V “ pC J J
2 ) Update stage : XXXnew , it is processed as for each new incoming data chunk a ) for the temporal mode 3 , C is updated with ( 4 ) b ) for non temporal modes 1 and 2 , A is updated with ( 9 ) and B is updated with ( 10 ) , respectively . 4.2 Extending to Higher Order Tensors We now show how to extend our approach to higher order cases . Let XXXold P R I1ˆ¨¨¨ˆIN´1ˆtold be an N th order tensor , pNq p1q pN´1q .A old fi be its CP decomposition , theN th , A old , . . . ,A mode be the time . A new tensor XXXnew P R I1ˆ¨¨¨ˆIN´1ˆtnew is old added to XXXold to form a tensor XXX P R I1ˆ¨¨¨ˆIN´1ˆptold`tnewq , where told " tnew . In addition , two sets of complementary p1q pN´1q , n P r1 , N ´ 1s , are the complementary matrices P pnq where P matrices for mode n . We are interested in finding the CP decomposition .A pNqfi of XXX .
, . . . ,P pnq are stored , pN´1q pN´1q
, . . . ,A
, . . . ,Q and Q and Q
, A p1q p1q
421 Update Temporal Mode Similar to the third order case , the loading matrix of the , is updated at first by fixing the other time mode , A loading matrices and minimizing the estimation error L pNq
A pNq Ð arg min ApNq
1 2
.
.XpNq ´ A pNqpdN´1A piqqJ .
.
2
1378 Basically , the above equation has the same structure as ( 3 ) , so we have a similar update rule for A pNq
« ff
„ pNq Ð
A pNq pNq old new
A A
“
XnewpNqppdN´1A
A pNq old piqqJq : j
2
2
.
.
P
Q
A
A pnq pnq
, and i‰nA
422 Update Non temporal Modes For each non temporal mode n P r1 , N´1s , the estimation pnqpdN error L on mode n is 1 similar update rule as equation ( 9 ) can be applied , that is ¯ new d K pNq A f H pNq new
.Xpnq ´ A ´ ´ pnq ` Xnewpnq J pnq ` pNq A new pnqpQ pnqq´1 pnq Ð P pnq Ð Q pnq Ð P piqqJ . ¯
, as K pnq i‰n A piq A where we denote the Khatri Rao product of the first N ´ 1 but the n th loading matrices , dN´1 Hadamard product , fN´1 423 Avoid Duplicated Computation for each n P In fact , if we were to compute every K r1 , N ´ 1s , there would be some redundant computation among them . Take a 5th order tensor for example , where p1q “ A p1q K , p3q “ A K . It p3q is clear that both K , p3q p1q and K . These redundant Khatri Rao products are computationally expensive to calculate , and more importantly , the amount of redundancy will dramatically increase with the number of modes N , since more common components are shared . p3q d A p4q d A p2qdA p4qdA p1q p1q p4q have computed A share a common computation A p3q d A p2qdA p1q p4qdA p2qdA p2q “ A p4q “ A p4q d A p3qdA
, K , and K p2q i‰n A and the and K and K
, asH piqJ pnq pnq p2q piq
. pnq
To overcome this issue , we use a dynamic programming ’s in one run , by making strategy to compute all the K good use of the intermediate results and avoiding duplicated operations . This process is detailed in Algorithm 1 and an illustrating example of a 6th order tensor is given in Figure 1 . The main idea is to go through the loading matrix list A from both ends , until the algorithm p1q reaches the results of K ( lines 3 to 8 ) . After piq that , for the rest of K computed as the Khatri Rao products of the intermediate results from the last loop ( lines 11 to 15 ) . where i P r2 , N ´ 2s , they are pN´1q pN´1q
, . . . ,A and K
, A p2q p1q pnq ual H
For the H
@i , j P r1 , N ´ 1s , H
’s , it is obvious that calculating each individby itself is inefficient . Exploiting the fact that for pjqfpA pjqq “ in each round of update , H is calculated first , then pnqq , where m is the is obtained as H m pA piqq “ H pnqJ piqfpA
H , each H element wise division . pjqJ piqJ pnq
A
A
A
Finally , by putting everything together , we obtain the general version of our OnlineCP algorithm1 , as presented in Algorithm 2 and 3 . 4.3 Complexity Analysis tensor rank , S “ śN´1 i “ 1 Ii , and J “ řN´1 ations to get all the K
Following the same notation as Section 4.2 , let R be the i “ 1 Ii . To process a new chunk of data XXXnew , it takes up to pN ´ 1qS oper , n P r1 , N ´ 1s , and H can be obtained in R2J ` pN ´ 2qR2 operations ( lines 1 and 2 in 1We provide our Matlab implementation of OnlineCP at http://shuo zhouinfo pnq pnq
Algorithm 1 : Get a list of Khatri Rao products
Input : A list of loading matrices rA pN´1q
, . . . ,A p1qs
Output : A list of Khatri Rao products pN´1qs
, . . . ,K rK p1q 1 lef t Ð rA pN´1qs 2 right Ð rA p1qs 3 if N ą 3 then for n Ð 2 to N ´ 2 do lef trns Ð lef trn ´ 1s d A rightrns ÐA pN´nq pnq d rightrn ´ 1s
4
5
6
12
13 end
7 8 end 9 K 10 K p1q Ð lef trN ´ 2s pN´1q Ð rightrN ´ 2s for n Ð 2 to N ´ 2 do
11 if N ą 3 then pnq Ð lef trN ´ n ´ 1s d rightrn ´ 1s
K end
14 15 end
A(5)A(5 )
A(4)A(4 )
A(3)A(3 )
A(2)A(2 )
A(1)A(1 )
A(5 ) . A(4 ) A(5 ) . A(4 )
A(5 ) . A(4 ) . A(3 ) A(5 ) . A(4 ) . A(3 )
A(2 ) . A(1 ) A(2 ) . A(1 )
A(j)A(j )
A(i)A(i )
A(3 ) . A(2 ) . A(1 ) A(3 ) . A(2 ) . A(1 )
A(j ) . A(i ) A(j ) . A(i )
K(1)K(1 )
K(2)K(2 )
K(3)K(3 )
K(4)K(4 )
K(5)K(5 )
Figure 1 : A 6th order example to get all K ’s together . A Khatri Rao product is represented by two arrows , of which the solid one linked to the first input . The two lists , lef t and right , are indicated by the two columns on the graph . pnq
Algorithm 3 ) . To update the time mode , RS , Stnew , and RStnew ` R2tnew ` R3 operations are required to get K pNq , pNq XnewpNq , and A new , respectively ( lines 3 , 4 ) . Note that the pseudoinverse ppK : as pA d Bq : “ ppA J [ 16 ] . For each non temporal mode n ( lines 7 to 10 ) , Stnew , RStnew{In ( « Stnew , since R is usually smaller than In ) , RStnew and can be replaced byK J
Bqq:pA d BqJ pNqqJq : Aq f pB pNq
H pnq pnq pnq
RIn operations are required for the unfolding , Khatri Rao , multiplication and addition in the step to update P ; and Q the updated A tions . Thus , to update the loading matrix A takes R2In ` R2tnew ` 3R2 operations to update ; then can be calculated in R3 ` R2In operap2R2`RqIn`pR`2qStnew`R3`p3`tnewqR2 operations is modes takes p2R2 ` RqJ ` pN ´ 1qpR ` 2qStnew ` pN ´ 1qpR3 ` p3 ` tnewqR2q operations . Overall , as S is usually lineCP can be written as OpN RStnewq , which is constant much larger than other factors , the time complexity of On required and the whole update procedure for non temporal of mode n , pnq wrt the length of processed data told .
In terms of space consumption , unlike ALS that needs to store all the data , OnlineCP is quite efficient since only the new data , previous loading matrices and complementary matrices need to be recorded . Hence , the total cost of space is Stnew ` p2J ` toldqR ` pN ´ 1qR2 .
1379 Table 1 : Complexity comparison between OnlineCP and existing methods .
OnlineCP ALS SDT RLST GridTF
Time
OpN RSptold ` tnewqq
OpN RStnewq OpR2ptold ` Sqq
OpR2Sq
OpN RStnew ` R2pJ ` told ` tnewqq
Space
Stnew ` p2J ` toldqR ` pN ´ 1qR2 Stnew ` pJ ` S ` 2toldqR ` 3R2 Stnew ` pJ ` told ` 2SqR ` 2R2
Sptold ` tnewq
Stnew ` pJ ` toldqR
Source
[ 4 ] [ 20 ] [ 20 ] [ 22 ]
Algorithm 2 : Initialization stage of OnlineCP
Input : Initial tensor XXXinit , loading matrices
Output : complementary matrices P p1q pN´1q
, . . . ,P pNq p1q
A
, . . . ,A p1q and Q pN´1q
, . . . ,Q pN´1q p1q p2q piqJ piq
, K
, . . . ,K
1 Get K
2 H Ð fN A 3 for n Ð 1 to N ´ 1 do pnq Ð XinitpnqpA pnqJ pnq Ð H m pA
A
Q
P
4
5 6 end pnqq pNq d K pnqq
A by Algorithm 1
We summarize the complexity of our approach in Table 1 , along with other existing approaches . Note that the complexities of SDT and RLST are based on the exponential window [ 20 ] , which considers all existing data while leverages their importance by a forgetting factor λ . In addition , as they only work on third order tensors , when other methods are compared to them , N should be set to 3 . Another remark is that the time complexities of ALS and GridTF are based on one iteration only , in reality they would take a few iterations until convergence .
5 . EMPIRICAL ANALYSIS
In this section , we evaluate our OnlineCP algorithm , compared to existing techniques . We first examine their effectiveness and efficiency on seven real world datasets . After that , based on the investigation on synthetic tensors , we further analyze the critical factors that can affect the performance of our approach , along with other baselines . 5.1 Real World Datasets
511 Experimental Specifications Datasets : The experiments are conducted on seven real world datasets of varying characteristics , all are naturally of multi way structures . These are two image datasets : ( i ) Columbia Object Image Library ( COIL ) ; ( ii ) ORL Database of Faces ( FACE ) ; three human activity datasets : ( iii ) Daily and Sports Activities Data Set ( DSA ) ; ( iv ) University of Southern California Human Activity Dataset ( HAD ) ; ( v ) Daphnet Freezing of Gait Data Set ( FOG ) ; one chemical laboratory dataset : ( vi ) Gas sensor array under dynamic gas mixtures Data Set ( GAS ) ; and a ( vii ) road traffic dataset collected from loop detectors in Victoria , Australia ( ROAD ) . Each dataset is represented by a tensor with its most natu ral structure . For instance , FACE is represented by a pixelˆ pixel ˆ shot third order tensor , while DSA is stored as an
Algorithm 3 : Update stage of OnlineCP pNq p1q
Input : Loading matrices A
,
, . . . ,A p1q pN´1q p1q complementary matrices P Q pN´1q
, . . . ,Q
, , and new data tensor XXXnew pNq ,
, . . . ,P p1q
Output : Updated loading matrices A
, . . . ,A and updated complementary matrices P pN´1q
, . . . ,Q
, . . . ,P
, Q p1q p1q p2q p1q pN´1q pN´1q by Algorithm 1
A
, K
1 Get K
2 H Ð fN´1A // update A pNq Ð K « p1q new Ð XnewpNqppK pNq pNq Ð
, . . . ,K piqJ piq pNq p1q d A ff
3 K 4 A
5 A pNq pNq old new
A A pNqqJq :
7
P
// update other modes
6 for n Ð 1 to N ´ 1 do pnq Ð P pnq ` XnewpnqpA pnqJ pnq Ð H m pA pnqq A J pnq ` pA pnq Ð Q pNq A new pnqpQ pnq Ð P pnqq´1
Q A
H
9
8 new d K pNq pnqq newq f H pNq pnq
10 11 end
4th order tensor of subjectˆ trailˆ sensorˆ time . Further more , since some of our baselines can only work with thirdorder tensors , for tensors with higher order , DSA , GAS , and HAD , we randomly extract third order sub tensors from them . Conversely , to enlarge the number of higher order tensors , image datasets , COIL and FACE have been transformed into 4th order tensors by treating each image as a collection of small patches , which forms the extra order . As a result , there are five datasets having two versions of representation : a third order one , indicated by suffix 3D , and a higher order form with suffix HD . The details of these datasets can be found in Table 2 .
Baselines : In this experiment , five baselines have been selected as the competitors to evaluate the performance .
( i ) Batch Cold : an implementation of ALS algorithm in
Tensor Toolbox [ 4 ] without special initialization .
( ii ) Batch Hot : the same ALS algorithm as above but the CP decomposition of the last time step is used as the initialization for decomposing the current tensor .
( iii ) SDT [ 20 ] : an adaptive algorithm based on incremen tally tracking the SVD of the unfolded tensor .
( iv ) RLST : another online approach proposed in [ 20 ] . Instead of tracking the SVD , recursive updates are performed to minimize the mean squared error on new data .
1380 Table 2 : Details of datasets
Datasets
Size
COIL 3D COIL HD DSA 3D DSA HD FACE 3D FACE HD FOG GAS 3D GAS HD HAD 3D HAD HD ROAD
128 ˆ 128 ˆ 240 64 ˆ 64 ˆ 25 ˆ 240 8 ˆ 45 ˆ 750 19 ˆ 8 ˆ 45 ˆ 750 112 ˆ 92 ˆ 400 28 ˆ 23 ˆ 16 ˆ 400 10 ˆ 9 ˆ 1000 30 ˆ 8 ˆ 2970 30 ˆ 6 ˆ 8 ˆ 2970 14 ˆ 6 ˆ 500 14 ˆ 12 ˆ 5 ˆ 6 ˆ 500 4666 ˆ 96 ˆ 1826
S “ śN´1
Slice Size i “ 1 Ii
16,384 102,400
360 6,840 10,304 10,304
90 240 1,440
64
3,840
447,936
Source
[ 19 ]
[ 2 ]
[ 23 ]
[ 3 ]
[ 12 ]
[ 30 ]
[ 24 ]
( v ) GridTF [ 22 ] : an divide and conqure based algorithm . To find CP decompositions for online tensors , the partitioning is enforced on the time mode only .
Evaluation metrics : Two performance metrics are used in our evaluation . Fitness is the effectiveness measurement defined as
¨˝1 ´ fitness fi
˛‚ˆ 100 %
. .
. ˆXXX ´ XXX
. . .
. .XXX
. . where XXX is the ground truth , ˆXXX is the estimation and . denotes the Frobenius norm . In addition , the average running time for processing one data slice , measured in seconds , is used to validate the time efficiency of an algorithm .
.
‚
Experimental setup : The experiments are divided into two parts . The first part is to decompose the third order tensors with all baselines . For the second part , only Batch Hot , GridTF and our approach are used . This is because both SDT and RLST work on third order tensors only , and Batch Cold does not show better performance compared to Batch Hot , while taking a much longer time to run .
Apart from the difference in the number of competitors , the experimental protocol is the same for both third and higher order tests . Specifically , for a given dataset , the first 20 % of the data is decomposed by ALS and its CP decomposition is used to initialize all algorithms . After that , the remaining 80 % of the data is appended to the existing tensor by one slice at a time . At each time step , after processing the appended data slice , all methods calculate the fitness of their current decomposition with their updated loading matrices , as well as their processing time for this new slice . The same experiment is replicated 10 times for all datasets on a workstation with dual Intel Xeon processors , 64 GB RAM . The final results are averaged over these 10 runs .
There are some settings of parameters that need to be clarified . Firstly , since we only care about the relative performance comparison among different algorithms , it is not necessary to pursue the best rank decomposition for each dataset . As a result , the rank R is fixed to 5 for all datasets . Additionally , for the initial CP decomposition , the tolerance
ε is set to 1e ´ 8 and the maximum number of iterations maxiters is set to 100 to ensure a good start , as the performance of all online algorithms depends on the quality of the initial decomposition .
In terms of method specific parameters , for the two batch algorithms , the default settings , ε “ 1e´ 4 and maxiters “
50 are used . For GridTF , which contains an ALS procedure for the new data slice and a recursively update procedure for estimating the whole current tensor , the same default parameters are chosen for the ALS step ; while ε “ 1e´2 and maxiters “ 50 are used for the update phase . Additionally , since batch algorithms do not provide a weighting strategy to differentiate the importance of data , in order to make a fair comparison , all the data slices are equally treated and there is no difference between older and newer ones in terms of their weights , which means the exponential window is used with λ “ 1 in SDT and RLST .
512 Results Given a particular dataset and a specific algorithm , its fitness and processing time are two time series ( averaged over 10 runs ) . We take the mean values of them and report the results for third order tensors in Table 3 and the higherorder tensors in Table 4 . In addition , for the four online approaches , SDT , RLST , GridTF and OnlineCP , their relative performances compared to Batch Hot are also shown in the parenthesis . Finally , the best results among these four are indicated by boldface .
As can be seen from Table 3 , for the two batch methods , there is no significant difference on their effectiveness . However , on all third order datasets , the fitness of Batch Cold is slightly worse than that of Batch Hot . The main reason is that using the previous results as initialization can provide the ALS algorithm with a descent seeding point . In contrast , every time Batch Cold totally discards this useful information and starts to optimize from the beginning , which cannot guarantee a better or even same quality estimation in the end . In fact , this also results in the longer running time of Batch Cold compared with Batch Hot , where the former is usually more than 10 times slower than the latter . On the other hand , even though Batch Hot improves the efficiency , its time cost is still considerably high , especially for large scale datasets . For example , on average it takes more than 20 seconds to process one additional data slice on the ROAD dataset , while OnlineCP takes only 0.0068 seconds . As the earliest studies of online CP decomposition , both SDT and RLST address this efficiency issue very well . Compared with Batch Hot , they shorten the mean running time by up to 400 times . RLST , in particular , was the most efficient online algorithm on 4 out of 7 third order tensor datasets . In fact , the efficiency of SDT is quite close to RLST , except for the GAS dataset , whose length of time mode is significantly higher than other datasets . This shows that SDT is more sensitive to the growth of time . However , the main issue of SDT and RLST is their estimation accuracy . For some datasets , such as COIL and HAD , they work fine , while for some others like DSA , they exhibit fairly poor accuracy , achieving only nearly half of the fitness of batch methods . The same accuracy problem can be observed in GridTF as well . In terms of efficiency , there is no significant difference between GridTF and Batch Hot , at least on the small size datasets . In fact , we notice that a substantial amount of time of GridTF is consumed by decomposing the new data slice and this cost is particularly dominant when the tensor size is not large enough . This can be confirmed by observing its generally better efficiency on higher order datasets , compared to the third order ones .
1381 Table 3 : Experimental results of third order datasets
( a ) Mean fitness of third order datasets over time ( in % , the higher the values the better ) . For SDT , RLST , GridTF and OnlineCP , ratios of their fitness to the result of Batch Hot are shown in parenthesis . Boldface indicates the best result among these four online approaches .
Datasets
Batch Cold Batch Hot
SDT
RLST
GridTF
OnlineCP
COIL 3D DSA 3D FACE 3D FOG 3D GAS 3D HAD 3D ROAD*
58.31 57.50 75.35 48.38 86.56 28.97 79.25
58.76 57.88 75.69 48.95 87.06 29.34 79.94
5131(087 ) 2630(045 ) 7064(093 ) 4090(084 ) 4393(050 ) 2614(089 ) 1545(021 )
5627(096 ) 2744(047 ) 4684(062 ) 4128(084 ) 5792(067 ) 2833(097 ) 6123(077 )
5413(092 ) 4885(084 ) 7191(095 ) 2594(053 ) 4864(056 ) 2756(094 )
N/A
5743(098 ) 5751(099 ) 7531(099 ) 4439(091 ) 8494(098 ) 2854(097 ) 7972(100 )
( b ) Mean running time of third order datasets for processing one data slice ( in seconds ) . For SDT , RLST , GridTF and OnlineCP , the ratios between the running time of Batch Hot and theirs are shown in parenthesis . Boldface indicates the best result among these four online approaches .
Datasets
Batch Cold Batch Hot
SDT
RLST
GridTF
OnlineCP
3.0255 0.3627 1.2616 0.2805 0.4095 0.1867
309.5064
0.1944 0.0246 0.1430 0.0198 0.0314 0.0154 23.6683
COIL 3D DSA 3D FACE 3D FOG 3D GAS 3D HAD 3D ROAD* * The result is based on only 1 run , due to the huge time consumption of batch methods . Figures of Batch Cold are estimated based on its average performance on other datasets , compared to Batch Hot . GridTF failed on this dataset because of the singular matrix problem , resulting from the large chunks of missing data in some sensors .
00041(4779 ) 00003(7245 ) 00037(3847 ) 00003(7201 ) 00003(9153 ) 00003(6094 ) 00573(4130593 )
00037(5206 ) 00005(5241 ) 00034(4206 ) 00005(3943 ) 00015(2082 ) 00004(4144 ) 00582(40667 )
00446(436 ) 00332(074 ) 00439(326 ) 00285(070 ) 00265(118 ) 00158(098 )
00017(11536 )
00004(5728 ) 00019(7387 ) 00004(5333 ) 00005(6491 ) 00004(4376 )
N/A
00068(348063 )
Table 4 : Experimental results of higher order datasets .
( a ) Mean fitness of higher order datasets over time ( in % , the higher the values the better ) . For GridTF and OnlineCP , ratios of their fitness to the result of Batch Hot are also shown in parenthesis . Boldface indicates the best result between these two online approaches .
Datasets
Batch Hot
GridTF
OnlineCP
COIL HD DSA HD FACE HD GAS HD HAD HD
57.43 62.76 74.78 79.38 67.36
4269(074 ) 6133(098 ) 6747(090 ) 6105(077 ) 6557(097 )
5683(099 ) 6254(100 ) 7445(100 ) 7571(095 ) 6728(100 )
( b ) Mean running time of higher order datasets for processing one data slice ( in seconds ) . For GridTF and OnlineCP , the ratios between the running time of Batch Hot and theirs are shown in parenthesis . Boldface indicates the best result between these two online approaches .
Datasets
Batch Hot
GridTF
OnlineCP
COIL HD DSA HD FACE HD GAS HD HAD HD
2.1264 0.2217 0.3795 0.3154 0.1750
01935(1099 ) 00896(248 ) 01438(264 ) 01807(175 ) 00922(190 )
00076(28044 ) 00029(7543 ) 00040(9415 ) 00016(20347 ) 00041(4223 )
Our proposed algorithm , OnlineCP , shows very promising results in both accuracy and speed . On every dataset , both third order and higher order ones , our method reaches the best fitness among all online algorithms . More importantly , the estimation performance of our approach is quite stable and very comparable to the results of batch techniques . In most of the cases , the fitness of OnlineCP is less than 3 % lower than that of the most accurate algorithm , Batch Hot . However , the speed of OnlineCP is orders of magnitudes faster than Batch Hot . On small and moderate size datasets , OnlineCP can be tens to hundreds of times faster than Batch Hot ; and on the largest dataset , ROAD , OnlineCP improves the efficiency of Batch Hot by more than 3,000 times . Additionally , compared with another fast approach , RLST , although OnlineCP is outperformed on four datasets , its speed on these datasets is quite close to the best . On the other hand , we notice that all these datasets have fairly small slice size . In contrast , on those datasets with larger slices , such as COIL and FACE , the time consumption of our method clearly grows slower than that of RLST , showing that OnlineCP is less sensitive to the size of the data , and thus , having better scalability . 5.2 Sensitivity to Initialization
Throughout our experiments , an interesting observation was made : for all adaptive algorithms that make use of the previous step results , namely Batch Hot , SDT , RLST , GridTF , and OnlineCP , their best results are usually linked to a good initial fitness , while poor quality initializations often lead them to subsequent under fitting . To explore the impact of initialization to each algorithm , the following ex
1382 Table 5 : The final fitness averaged over 200 runs with dif ferent initial fitness . Results are displayed as mean ˘ std , where mean is the average final fitness and std is the standard deviation , both in % ( the higher the values the better ) .
Batch Hot SDT RLST GridTF OnlineCP
Final Fitness
8257˘101474 768˘555205 3315˘369270 5743˘152148 6767˘129846 sor XXX P R periment has been conducted . We generate a synthetic ten20ˆ20ˆ100 by constructing from random loading matrices and then downgrade it by a Gaussian noise with a Signal to Interference Rate ( SIR ) of 20 dB . The best fitness to XXX in 10 runs of ALS is 9014 % This tensor is then repeatedly decomposed by the above five methods for 200 times . At the beginning of each run , half of the data is used for initialization by ALS with a random tolerance from 9e ´ 1 to 1e ´ 4 , to produce different level of initial fitness . Then the rest of data is sequentially added and processed by each online algorithm . The averaged final fitness over all runs is used as the effectiveness indicator , as well as the standard deviation . Table 5 shows the experimental results with average initial fitness as 65.78 % and standard deviation as 153704 %
As shown in Table 5 , overall , the low quality initialization has a negative impact on all algorithms . Even the most powerful one , Batch Hot , cannot always reach the best fitness and shows a decline of 10 % . For SDT and RLST , it turns out that both algorithms are significantly dependent on the initial fitness . When the initial fitness is lower than the best value , their performance can quickly drop to an unacceptable level . In addition , their results are also highly unstable , as demonstrated by a large variance . While both GridTF and OnlineCP exhibit much more stable performance , the final fitness of GridTF is considerably lower than our approach OnlineCP . This experimental evidence demonstrates that the proposed algorithm , OnlineCP , is less sensitive to the quality of the initialization , compared with exiting online methods . However , it can be seen that good initialization still plays an important role to our algorithm . Thus , for applying our method , we suggest to validate the goodness of the initialization at the beginning , in order to obtain the best subsequent effectiveness . 5.3 Scalability Evaluation
According to Section 4.3 , the time complexity of each algorithm is mainly determined by the slice size and the length of processed data . To confirm our analysis and evaluate the 20ˆ20ˆ105 scalability of our algorithm , firstly , a tensor XXX P R of small slice size but long time dimension is decomposed . After initializing with data of the first 100 timestamps , each method ’s running time for processing one data slice at each time step is measured and displayed in Figure 2 . In addition , to examine the impact of slice size to efficiency , we fix the time mode to 100 , and generate a group of tensors of different slice sizes , ranging from 100 to 9 ˆ 106 . For each tensor , its first 20 % of data is used for initialization and the average running time for processing the rest data slices is i e m T g n n n u R i
10 1
10 0
10 1
10 2
10 3
10 4
0
Batch Hot SDT RLST GridTF GridTF update OnlineCP
2
4
6
8
Length of Time Mode
10 ×10 4
( a ) i e m T g n n n u R i
0.025
0.02
0.015
0.01
0.005
0
0
SDT RLST GridTF GridTF update OnlineCP
2
4
6
8
Length of Time Mode
10 ×10 4
( b )
Figure 2 : Running time ( in seconds ) for adding one slice to a 20 ˆ 20 ˆ pt ´ 1q tensor at time t . Two figure represents the same information , differing only in the y axis scale . i e m T g n n n u R i
10 2
10 0
10 2
10 4
0
2
4 6 Slice Size
( a )
Batch Hot SDT RLST GridTF OnlineCP
8
10 ×10 6 i e m T g n n n u R i
12
10
8
6
4
2
0
Batch Hot SDT RLST GridTF OnlineCP
0
2
8
10 ×10 6
4 6 Slice Size ( b )
Figure 3 : Running time ( in seconds ) for processing different size of tensors . Two figure represents the same information , differing only in the y axis scale . shown in Figure 3 . For better comparison , both Batch Hot and GridTF are forced to execute 1 iteration only in these two experiments . Note that the y axis of Figures 2a and 3a is displayed in log scale and in Figure 2b , Batch Hot has been removed for better visibility .
As can be seen from Figure 2 , both RLST and OnlineCP show constant complexities and the increasing length of processed data has no impact on them . For the other approaches , a clear linear growth with time can be observed in Batch Hot and SDT , which makes them less feasible for online learning purposes . The change of time consumption in GridTF is less obvious compared with Batch Hot and SDT . However , after removing the time used by its inner ALS procedure , similar linear trend can be seen , marked as GridTF update in the figure .
In terms of slice size , it turns out that the time consumption of all approaches are linearly increasing as the slice size grows . However , their slopes vary . Both Batch Hot and SDT show quicker growth compared to others . This is reasonable since the impact of slice size to them is also leveraged by the time mode . On the other hand , GridTF outperforms SDT and RLST when the slice gets larger . This is because the growth of slice size has impact only on its ALS procedure , which is scaled by R times , while the coefficients in the complexities of SDT and RLST wrt slice size contain an R2 term . Once again , our proposed algorithm illustrates the best performance in this experiment and even a large 3000 ˆ 3000 data slice can be efficiently processed in 0.1 second .
1383 6 . CONCLUSIONS AND FUTURE WORK To conclude , in this paper , we address the problem of tracking the CP decomposition of online tensors . An online algorithm , OnlineCP , is proposed , which can efficiently track the new decomposition by using complementary matrices to temporally store the useful information of the previous time step . Furthermore , our method is not only applicable to third order tensors , but also suitable for higher order tensors that have more than 3 modes . As evaluated on both real world and synthetic datasets , our algorithm demonstrates comparable effectiveness with the most accurate batch techniques , while significantly outperforms them in terms of efficiency . Additionally , compared with the state of art online techniques , the proposed algorithm shows advantages in many aspects , including effectiveness , efficiency , stability and scalability .
There is still room for improving our method . One direction is to further extend it for more general dynamic tensors that may be changed on any modes [ 11 ] . Another potential direction is to incorporate constraints , such as nonnegativity [ 13 ] , so that our method can be more suitable for applications such as computer vision .
Acknowledgments This work is supported by the Australian Research Council via grant number DP140101969 .
7 . REFERENCES [ 1 ] E . Acar , et al . Scalable tensor factorizations for incomplete data . Chemometrics and Intelligent Laboratory Systems , 106(1):41–56 , March 2011 .
[ 2 ] K . Altun , et al . Comparative study on classifying human activities with miniature inertial and magnetic sensors . Pattern Recognition , 43(10):3605–3620 , 2010 . [ 3 ] M . B¨achlin , et al . Wearable assistant for parkinson ’s disease patients with the freezing of gait symptom . IEEE Trans . Inf . Tech . Biomed . , 14(2):436–446 , 2010 .
[ 4 ] B . W . Bader , T . G . Kolda , et al . Matlab tensor toolbox version 26 Available online , February 2015 .
[ 5 ] Y . Cai , et al . Facets : Fast comprehensive mining of coevolving high order time series . In SIGKDD , 2015 .
[ 6 ] A . Cichocki . Era of big data processing : a new approach via tensor networks and tensor decompositions . arXiv preprint arXiv:1403.2048 , 2014 .
[ 7 ] A . Cichocki , et al . Tensor decompositions for signal processing applications : From two way to multiway component analysis . Signal Processing Magazine , IEEE , 32(2):145–163 , 2015 .
[ 8 ] P . Comon , X . Luciani , and A . L . De Almeida . Tensor decompositions , alternating least squares and other tales . J . Chemometrics , 23(7 8):393–405 , 2009 . [ 9 ] L . De Lathauwer , et al . On the best rank 1 and rank (r 1 , r 2 , , rn ) approximation of higher order tensors . SJMAEL , 21(4):1324–1342 , 2000 .
[ 10 ] D . M . Dunlavy , T . G . Kolda , and E . Acar . Temporal link prediction using matrix and tensor factorizations . TKDD , 5(2):10 , 2011 .
[ 11 ] H . Fanaee T and J . Gama . Multi aspect streaming tensor analysis . Knowledge Based Systems , 89:332–345 , 2015 .
[ 12 ] J . Fonollosa , et al . Reservoir computing compensates slow response of chemosensor arrays exposed to fast varying gas concentrations in continuous monitoring . Sens . Actuator B Chem . , 215:618–629 , 2015 .
[ 13 ] C . Hu , et al . Scalable bayesian non negative tensor factorization for massive count data . In ECML PKDD , 2015 .
[ 14 ] W . Hu , et al . Incremental tensor subspace learning and its applications to foreground segmentation and tracking . IJCV , 91(3):303–327 , 2011 .
[ 15 ] T . G . Kolda . Multilinear operators for higher order decompositions . Tech . Report SAND2006 2081 , Sandia National Laboratories , April 2006 .
[ 16 ] T . G . Kolda and B . W . Bader . Tensor decompositions and applications . SIAM review , 51(3):455–500 , 2009 .
[ 17 ] W . Liu , et al . Utilizing common substructures to speedup tensor factorization for mining dynamic graphs . In CIKM , 2012 .
[ 18 ] X . Ma , et al . Dynamic updating and downdating matrix svd and tensor hosvd for adaptive indexing and retrieval of motion trajectories . In ICASSP , 2009 .
[ 19 ] S . A . Nene , et al . Columbia Object Image Library
( COIL 20 ) . Tech . report , Feb 1996 .
[ 20 ] D . Nion , et al . Adaptive algorithms to track the parafac decomposition of a third order tensor . IEEE Trans . Sig . Process . , 57(6):2299–2310 , 2009 .
[ 21 ] S . Papadimitriou , et al . Streaming pattern discovery in multiple time series . In VLDB , 2005 .
[ 22 ] A . H . Phan , et al . Parafac algorithms for large scale problems . Neurocomputing , 74(11):1970–1984 , 2011 .
[ 23 ] F . S . Samaria , et al . Parameterisation of a stochastic model for human face identification . In WACV , 1994 . [ 24 ] F . Schimbinschi , et al . Traffic forecasting in complex urban networks : Leveraging big data and machine learning . In Big Data , 2015 .
[ 25 ] L . Shi , et al . Stensr : Spatio temporal tensor streams for anomaly detection and pattern discovery . KAIS , 43(2):333–353 , 2015 .
[ 26 ] A . Sobral , et al . Incremental and multi feature tensor subspace learning applied for background modeling and subtraction . In ICIAR , 2014 .
[ 27 ] J . Sun , D . Tao , and C . Faloutsos . Beyond streams and graphs : dynamic tensor analysis . In SIGKDD , 2006 .
[ 28 ] J . Sun , et al . Incremental tensor analysis : Theory and applications . TKDD , 2(3):11 , 2008 .
[ 29 ] M . A . O . Vasilescu , et al . Multilinear analysis of image ensembles : Tensorfaces . In ECCV , 2002 .
[ 30 ] M . Zhang and A . A . Sawchuk . Usc had : A daily activity dataset for ubiquitous activity recognition using wearable sensors . In Ubicomp SAGAware , 2012 .
1384
