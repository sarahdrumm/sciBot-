Collaborative Multi View Denoising
Lei Zhang1
∗
, Shupeng Wang1
∗
, Xiaoyu Zhang1 , Yong Wang1 , Binbin Li1 , Dinggang Shen2 , and
Shuiwang Ji3
1Institute of Information Engineering , Chinese Academy of Sciences , Beijing 100093 , China 2Department of Radiology and BRIC , University of North Carolina , Chapel Hill , NC 27599
3School of Electrical Engineering and Computer Science , Washington State University , Pullman , WA 99164
{zhanglei1,wangshupeng,zhangxiaoyu,wangyong,libinbin}@iieaccn,dgshen@meduncedu,sji@eecswsuedu
ABSTRACT In multi view learning applications , like multimedia analysis and information retrieval , we often encounter the corrupted view problem in which the data are corrupted by two different types of noises , ie , the intra and inter view noises . The noises may affect these applications that commonly acquire complementary representations from different views . Therefore , how to denoise corrupted views from multi view data is of great importance for applications that integrate and analyze representations from different views . However , the heterogeneity among multi view representations brings a significant challenge on denoising corrupted views . To address this challenge , we propose a general framework to jointly denoise corrupted views in this paper . Specifically , aiming at capturing the semantic complementarity and distributional similarity among different views , a novel Heterogeneous Linear Metric Learning ( HLML ) model with low rank regularization , leave one out validation , and pseudo metric constraints is proposed . Our method linearly maps multiview data to a high dimensional feature homogeneous space that embeds the complementary information from different views . Furthermore , to remove the intra and inter view noises , we present a new Multi view Semi supervised Collaborative Denoising ( MSCD ) method with elementary transformation constraints and gradient energy competition to establish the complementary relationship among the heterogeneous representations . Experimental results demonstrate that our proposed methods are effective and efficient .
CCS Concepts •Information systems → Data mining ; •Computing methodologies → Machine learning ;
Keywords Multi view ; denoising ; heterogeneity ; metric learning ∗Corresponding Author .
KDD ’16 , August 13 17 , 2016 , San Francisco , CA , USA cfl 2016 ACM . ISBN 978 1 4503 4232 2/16/08 . . . $15.00 DOI : http://dxdoiorg/101145/29396722939811
Inter view
Noise
View X
View Y
View X
Correlation
Correlation
Correlation
Correlation
Multi view
Mono view
Datum
Datum
Multi view
Mono view
Datum
Datum
Multi view
Mono view
Datum
Datum
Intra view
Noise
Multi view
Mono view
Datum
Datum
Figure 1 : Intra View Noise and Inter View Noise .
1 .
INTRODUCTION
With the rapid development of modern information technology , a large number of high tech digital products appear in real world . The multi view data produced by these electronic equipments become available in various fields , including medical diagnosis , webpage classification , and multimedia analysis . These multi view data show heterogeneous characteristics of low level features and the correlation of high level semantics .
Generally , due to inappropriate data processing , manmade mistakes , random events , and the like , not all instances are a prefect reflection of objective reality , resulting in the corrupted views of multi view data . Rather , the corrupted view problem in multi view learning is essentially different from single view one . The reason is that multi view data are always corrupted by two different types of noise . One refers to the intra view noise that makes the instances from different categories in the same view grouped together while keeping the samples of the same class away from each other simultaneously . The other represents the noise existing among different views , ie , the inter view noise , leading to false complementary relationship among the heterogeneous representations of the same object . For example , as shown in Fig 1 , the existence of intra view noise causes the zebra photo is incorrectly grouped with the tiger images ; additionally , the unmatched white tiger picture is wrongly correlated with the Siberian tiger image from man made mistakes , leading to intra view noise .
More notably , these noise levels are high enough to affect the performance of multi view data , leading to false classification , clustering , retrieval and analysis . Thus before extracting vital information from multi view data or next level of processing , it is essential to denoise them to improve the quality of multi view data for a more accurate and rigorous assessment [ 1 , 2 , 3 , 4 , 5 ] . Furthermore , to the best of our k
2045 Feature 1
Semantic Complementarity
Feature 1
Feature 2
Zebra
Feature 2
Zebra
Feature 3
Feature 4
Feature 4
Distributional Similarity
Feature 5
Tiger
View X
Tiger
Feature 3
View Y
Figure 2 : Complementarity and Distributivity Restraints on Multi View Data . nowledge , no existing efforts have focused on denoising the corrupted views of multi view data . Consequently , the abovementioned applications face great challenge in the real world . Thus , it is necessary to develop an effective denoising method for multi view corrupted data .
However , it is a challenging task to denoise the corrupted views of multi view data . First of all , since different views span heterogeneous low level feature spaces , there is no explicit correspondence among the heterogeneous representations from different views . For example , in the Alzheimer ’s Disease Neuroimaging Initiative ( ADNI ) [ 6 ] database , objects not only have Positron Emission Tomography ( PET ) scan , but also own Magnetic Resonance Imaging ( MRI ) measurement . Therefore , to denoise the corrupted views of multi view data , an issue to be first addressed is to learn a couple of heterogeneous metrics through multi view uncorrupted data to capture the semantic complementarity among different views .
Meanwhile , for multi view data , it can be assumed as illustrated in Fig 2 that they are under both complementarity and distributivity constraints . The complementarity constraint refers to the semantic complementarity among different views that makes much more the complementary information from different views fully contained in the multi view data . Unlike the complementarity constraint , the distributivity constraint takes high distributional similarity which can group the samples of the same class from the same view together . Hence , another issue we need further to deal with for denoising corrupted views is to refine multi view corrupted data under both the complementarity and distributivity constraints .
1.1 Main Contributions
The key contributions of this paper are highlighted as fol lows :
• A general framework for denoising the corrupted views of multi view data is proposed to obtain the complex representations for multi view data . In this framework , multiple heterogeneous linear metrics are learned to build a bridge between multiple heterogeneous lowlevel feature spaces . ferent views . To the best of our knowledge , no other existing efforts have focused on this type of mapping .
• A new Multi view Semi supervised Collaborative Denoising ( MSCD ) method with elementary transformation constraints and Gradient Energy Competition ( GEC ) criterion is proposed to remove the intra and interview noise . It is worth to note that no similar method has been proposed in the past .
1.2 Organization
The remainder of this paper is organized as follows : We present a general framework for denoising the corrupted views of multi view data in Section 21 In Section 2.2 , a novel Heterogeneous Linear Metric Learning ( HLML ) model is developed for correlating different views . We build a new Multi view Semi supervised Collaborative Denoising ( MSCD ) method to remove the intra and inter view noises under both complementarity and distributivity constraints in Section 23 Furthermore , Section 3 provides two efficient algorithms to solve the proposed framework . Section 4 gives a broad overview of some related works . Experimental results and analyses are reported in Section 5 . Section 6 concludes this paper .
1.3 Notations
We establish some notations to be used throughout this paper in Table 1 .
Table 1 : Notations
Notation
Vx Vy
Description View X View Y
XU ∈ Rn1×dx Uncorrupted samples in Vx YU ∈ Rn1×dy Uncorrupted samples in Vy LU ∈ Rn1×m Label indicator matrix xi ∈ Rdx yi ∈ Rdy n1 dx dy m
( xi , yi )
The i th sample from Vx The i th sample from Vy Number of uncorrupted instances Dimensionality of Vx Dimensionality of Vy Number of labels The i th multi view datum
XC ∈ Rn2×dx Corrupted representations in Vx YC ∈ Rn2×dy Corrupted representations in Vy Number of corrupted instances Frobenius norm Trace norm Inner product of matrices Positive semi definite matrices Gradient of smooth function f ( · ) Absolute value Identity matrix
|| · ||F || · ||∗ h· , ·i Sk×k ▽f ( · )
Ik ∈ Rk×k n2
+
| · |
2 . COLLABORATIVE MULTI VIEW
DENOISING
• We propose a novel Heterogeneous Linear Metric Learning ( HLML ) model , which linearly maps multiple heterogeneous low level feature spaces to a high dimensional feature homogeneous one , to capture the semantic complementarity and distributional similarity among dif
Here we propose a general framework to denoise the corrupted views of multi view data . To facilitate the understanding of our proposed framework , Fig 3 gives an overall illustration of the proposed framework . More details are presented in the following subsections .
2046 Multi View Heterogeneous Uncorrupted Data
Heterogeneous Linear Metric Learning
View X
View Y x1 x2 x3 x4 x5 x6 x7 x8 x9
XU
XC
Correlation
Correlation
Correlation
Correlation
Correlation
Correlation
Inter View Noise
Inter View Noise
Inter View Noise y1 y2 y3 y4 y5 y6 y7 y8 y9
Zebra x4 x5
Metric
Metric y4
Zebra y5 y6
Metric
Heterogeneous
View Y
Linear
Transformation
View X x6
YU x1 x3
Complementary
Metric y1
Metric y3
Metric y2
Tiger
Tiger x2
High dimensional Feature homogeneous Space
View X
View Y
Collaborative
Denoising
YC
Switch x8 y8 x10 y10
P u s h x7 x9 x8 x10 y9 y8 y7 y10 y7
After Denoising
Intra View Noise
Intra View Noise x10
Correlation y10 x9 x7
Before Denoising y9
Switch
Multi View Heterogeneous Corrupted Data
Multi view Semi supervised Collaborative Denoising
Figure 3 : The proposed framework for collaborative multi view denoising .
2.1 Overview of the Proposed Framework
We provide an overview of the proposed formulations by using the example in Fig 3 . In this example , a set of multiview data consists of View X and View Y . There are a certain amount of multi view uncorrupted data such as ( x1 , y1 ) . However , some multi view data are corrupted . For instance , the zebra representations x9 and y10 are wrongly grouped into the tiger category , and the co occurring heterogeneous representations in the multi view data ( x7 , y7 ) , ( x8 , y8 ) , and ( x9 , y9 ) have incorrect complementary relationships .
To denoise the corrupted views of multi view data , multiple heterogeneous linear metrics are learned by HLML model to build a high dimensional feature homogeneous subspace among multiple heterogeneous low level feature spaces in the proposed framework . Specifically , to fully exploit the semantic complementarity and distributional similarity among different views , multiple heterogeneous linear metrics A and B are learned using the multi view uncorrupted data XU and YU to eliminate the heterogeneity across them . Thus , a feature homogeneous subspace is obtained , in which the correlated representations from different views are coupled together to capture much more complementary information from different views . At the same time , the samples of the same class from the same view can be grouped together while keeping the instances from different categories away from each other simultaneously . For example , the zebra heterogeneous representations x6 and y6 are matched together to capture much more complementary information from different views . Furthermore , the tiger co occurring heterogeneous representations ( x1 , y1 ) , ( x2 , y2 ) , and ( x3 , y3 ) and the zebra co occurring heterogeneous representations ( x4 , y4 ) , ( x5 , y5 ) , and ( x6 , y6 ) are grouped together respec tively to mine the distributional similarity among different views .
Meanwhile , by exploiting multiple heterogeneous metrics learned by HLML model , the intra view noises existing in the multi view corrupted data XC and YC are removed by MSCD model to a large extent in the feature homogeneous space on the basis of both semantic complementarity and distributional similarity among different views . Moreover , the MSCD method utilizes the proposed elementary transformation constraints based on GEC criterion to establish the complementary relationship among the heterogeneous representations of the same object according to the learned multiple heterogeneous metrics . The constraints will switch the positions of corresponding representations in the corrupted matrix XC and YC to eliminate the inter view noises . For instance , the zebra representation x9 in the View X is pulled out the group composed of the tiger representations x7 and x8 ; besides , the zebra representation y10 in the View Y is pushed closer to the cluster consisting of the zebra representations to remove intra view noise ; and the zebra representations y7 and y9 in the View Y are switched respectively to match the appropriate representations to eliminate interview noise effectively . After denoising , the heterogeneous representations from different views are correctly matched and grouped together in the feature homogeneous space .
2.2 The Proposed HLML Model
In the following , a novel multi view metric learning method is developed for capturing both semantic complementarity and distributional similarity among different views in this subsection . Our work is motivated by a few prior studies . Recently , Sun et al [ 7 ] have proved that the homogeneous transformations can significantly capture the com
2047 plementarity among different views . Moreover , Weinberger et al [ 8 ] have pointed out that the pseudo metric based on Mahalanobis distance can be used to effectively eliminate the intra view noise . Furthermore , Goldberger et al [ 9 ] have proved that the Mahalnobis distance metric based on leave one out validation can exploit the characteristics of sample distribution to improve the performance of classification . Additionally , Liu et al [ 10 ] have pointed out that the rank is a powerful tool to capture between class differences in the matrix case . Nevertheless , “ rank(• ) ” is not a convex function , which leads to the difficulty in finding the optimal solution . Fortunately , Cand`es and Recht [ 11 ] , Recht et al [ 12 ] , and Cand`es and Tao [ 13 ] have theoretically justified that the trace norm of a matrix can be used to approximate the rank of the matrix .
Following the above mentioned strong theoretical supports [ 7 , 8 , 9 , 10 , 11 , 12 , 13 ] , we propose a novel HLML model with low rank regularization , leave one out validation , and pseudo metric constraints to learn multiple heterogeneous linear transformations for multi view data , as shown in Fig 4 . Particularly , the existing uncorrupted heterogeneous representations XU and YU are utilized in HLML model to learn multiple well defined pseudo metrics A and B to mine the distributional similarity among different views . Then , to make transformed data MU and RU maximally linearly separable , it is essential to impose the low rank regularization on the transformed data . As a consequence , the heterogeneous representations are linearly projected into a featurehomogeneous space , in which the correlated representations from different views are coupled together to capture the semantic complementarity among different views .
More specifically , the new distance metrics are defined as follows to learn a Mahalanobis distance :
DMX ( xi , xj ) = ( xi − xj)T MX ( xi − xj ) ,
DMY ( yi , yj ) = ( yi − yj)T MY ( yi − yj ) ,
( 1 )
( 2 )
Assuming Ct where MX = AT A and MY = BT B are two positive semidefinite matrices . Thus , the linear transformations A and B can be applied to each pair of co occurring heterogeneous representations ( xi , yi ) . X and Ct
Y be the sample sets of t th class from the views Vx and Vy , respectively . We define each sample xi or yi selects another sample yj or xj in another view as its neighbor with the probability pij or qij . By using a softmax under the Euclidean distance in the transformed featurehomogeneous space , pij and qij are defined as follows : pij = qij = exp(− k Axi − Byj k2 )
Pk exp(− k Axi − Byk k2 ) Pk exp(− k Byi − Axk k2 ) exp(− k Byi − Axj k2 )
,
.
( 3 )
( 4 )
Under this definition , we can compute the probabilities pi and qi that the sample i will be correctly classified :
& yj ∈Ct Y
X pi = qi =
Pxi∈Ct P yi∈Ct Y
& xj ∈Ct X pij , qij ,
( 5 )
( 6 )
Heterogeneous Matrix
Pseudometric Matrix
Feature Homogenous Matrix
Rank
Class 1
Class 2
Uncorrupted
Linear
Representations XU
Transformation A
Linearly Separable Representations MU
Heterogeneous Matrix
Pseudometric Matrix
Feature Homogenous Matrix
Rank
Class 1
Class 2
Uncorrupted
Linear
Representations YU
Transformation B
Linearly Separable Representations RU
Figure 4 : Heterogeneous Linear Metric Learning .
Then the proposed approach can be formulated as follows :
Ψ1 : k XU A − YU B k2 min A,B st AT A 0 and BT B 0 ,
F −αg(A , B ) + βh(A , B )
( 7 ) where A ∈ Rdx×k , B ∈ Rdy×k , k is the dimensionality of the feature homogeneous subspace , the positive semidefinite constraints AT A 0 and BT B 0 are added into the optimization to ensure a well defined pseudo metric , and α and β are two trade off parameters . The first term in the objective function is used to capture the semantic complementarity among different views . The motivation of introducing the leave one out validation g(A , B ) consisting of the classification accuracies of different views is to mine the distributional similarity among different views . In addition , the third term h(A , B ) in the objective function h(A , B ) =k XU A k∗ + k YU B k∗ ,
( 9 ) is a low rank regularization based on trace norm to make transformed data MU and RU carrying more between class differences information .
It is worth to note that no similar numerical method has been yet proposed . Our proposed HLML model is greatly different from well known kernel methods [ 14 , 15 ] without an explicit high dimensional projection and classical linear algorithms [ 7 , 16 ] to reduce dimensionality . HLML can linearly project the multi view data into a feature homogeneous space of even higher dimensions . That is to say , k may be greater than both dx and dy , ie , k ≥ max(dx , dy ) .
Furthermore , the Parallel Feature Fusion Strategy ( PFFS ) [ 17 , 18 ] is adopted to establish the common representations . The details is as follows : for the i th pair of heterogeneous representations ( xi , yi ) , we can obtain their own Homogeneous Correlated Representations ( HCR ) with the optimal A∗ and B∗ by :
µxi = A∗T xi and µyi = B∗T yi .
( 10 )
Consequently , we can obtain a Complex Representations ( CR ) µi in the feature homogeneous subspace based on µxi and µyi :
µi = ( µxi + µyi )/2 .
( 11 )
In Section 3.1 , an efficient algorithm is proposed to solve the problem Ψ1 . g(A , B ) =P pi +P qi ,
( 8 )
2048 ( cid:79)(cid:72)(cid:72)(cid:87)(cid:85)(cid:92)fifi ( cid:85)(cid:86)(cid:73)(cid:85)(cid:87)(cid:76)fi
( cid:76)(cid:86)(cid:72)(cid:85)(cid:85)(cid:87)(cid:72)(cid:71)fifi
( cid:87)(cid:85)(cid:76)fifi
( cid:72)(cid:76)(cid:86)(cid:72)(cid:71)fifi problem Ψ1 . Then the proposed approach can be formulated as follows :
( cid:76)(cid:87)(cid:75)(cid:76)(cid:74 )
( cid:76)(cid:87)(cid:75)(cid:76)(cid:74 )
( cid:79)(cid:72)(cid:72)(cid:87)(cid:85)(cid:92)fifi ( cid:85)(cid:86)(cid:73)(cid:85)(cid:87)(cid:76)fi(cid:43 )
( cid:76)(cid:86)(cid:72)(cid:85)(cid:85)(cid:87)(cid:72)(cid:71)fifi
( cid:87)(cid:85)(cid:76)fifi
( cid:72)(cid:76)(cid:86)(cid:72)(cid:71)fifi
Figure 5 : Multi view Semi supervised Collaborative Denoising .
2.3 The Proposed MSCD Model
In the above subsection , we have built a high dimensional feature homogeneous space by multiple learned heterogeneous linear metrics to capture both semantic complementarity and distributional similarity among different views . Furthermore , to eliminate the intra and inter view noise , it is essential to recover the complementary relationship among the heterogeneous representations of the same object in the multi view corrupted data on the basis of the learned heterogeneous metrics .
In [ 19 ] , it has been pointed out that an elementary row transformation matrix can be used to exchange any rows of a matrix . Additionally , Rubinstein et al [ 20 ] have proposed recently a forward looking energy function that measures the effect of seam carving on the retargeted image , not the original one . They have shown how the new measure can be used in either graph cut or dynamic programming and demonstrated the effectiveness of their contributions on several images and video sequences . Moreover , it has been testified in [ 21 ] by Muslea et al that the robust performance of multi view learning depends on interleaving active and semi supervised learning on the basis of view correlation . Furthermore , Blum and Mitchell [ 22 ] have proved that for a problem with two views the target concept can be learned based on a few labeled and many unlabeled examples , provided that the views are compatible and heterogeneous .
Based on the above mentioned strong theoretical supports [ 19 , 20 , 21 , 22 ] , we propose a new MSCD model with elementary transformation constraints and GEC criterion to eliminate the intra and inter view noises according to the learned semantic complementarity and distributional similarity among different views in Section 22 As shown in Fig 5 , MSCD model eliminates the intra and inter view noises by means of semi supervised learning . It firstly makes use of the uncorrupted linearly separable representations MU and RU with labels to learn a decision matrix W . Then through the learned elementary row transformation matrices T and H , MSCD switches the positions of the noise corrupted samples in the matrices MC and RC ; meanwhile , the decision matrix W is applied to predict the classification of the unlabeled noisy representations MC and RC to establish the complementary relationship among the heterogeneous representations of the same object .
Specifically , let ( A∗ , B∗ ) be the optimal solutions of the
Ω1 : min T,H,W st k T MC W − HRCW k2
F +
γ kMU
RU W −LU
LU k2
F +τ k W k2 F
( 12 )
T , H ∈ En2 and W T W = I , where T ∈Rn2 ×n2 and H ∈Rn2×n2 are two elementary row transformation matrices , W ∈ Rk×m is a decision matrix , MC = XC A∗ and RC = YCB∗ are the noise corrupted matrices in View X and View Y , respectively , MU = XU A∗ and RU = YU B∗ are the uncorrupted linearly separable representations in the feature homogeneous space , En2 ∈ Rn2×n2 is a set of elementary row transformation matrices , and γ and τ are two regularization parameters . The first term in the objective function takes advantage of the learned W , T , and H to recover the complementary relationship among the heterogeneous representations of the same object . The second term in the objective function is a linear least square loss to learn an excellent decision matrix W using the uncorrupted linearly separable representations MU and RU with labels . The goal of imposing the orthogonal constraints on W is to effectively remove the correlations among different classes . The motivation of introducing the elementary transformation constraints is to ensure the matrices T and H be two standard elementary row transformation matrices to switch the positions of the noise corrupted samples in the matrices MC and RC .
Recently , the gradient energy measure has been widely used in dynamic programming and demonstrated its effectiveness on graph cut or seam carving[20 ] . Based on the above strong theoretical supports , we propose a Gradient Energy Competition ( GEC ) criterion to build an elementary row transformation matrix .
In detail , in a gradient matrix G obtained by gradient descent method , every internal element Gij is connected to its four neighbors Gi−1,j , Gi+1,j , Gi,j−1 , and Gi,j+1 . Following the ℓ1 norm gradient magnitude energy [ 20 ] , we define the between sample energy Ebs of Gij in the vertical direction as
Ebs =
∂ ∂x
G =| G(i + 1 , j ) − G(i , j ) | +
| G(i , j ) − G(i − 1 , j ) | ,
( 13 ) and the within sample energy Ews in the horizontal direction as
Ews =
∂ ∂y
G =| G(i , j + 1 ) − G(i , j ) | +
| G(i , j ) − G(i , j − 1 ) | .
( 14 )
The global energy of Gij can be obtained via Ebs and Ews :
Eglobe = δ ∗ Ebs + ( 1 − δ ) ∗ Ews ,
( 15 ) where δ is a trade off parameter . We use Eq ( 15 ) to compute
Gradient Matrix G
Energy Matrix E
Row Transformation T
Between Sample
Energy Ebs
Gi 1,j
Gi,j 1 Gi,j Gi,j+1
Gi+1,j
Competition
Circulation
Within Sample
Energy Ews
Winner
Elementary Matrix
Figure 6 : Gradient Energy Competition .
2049 Algorithm 1 : Heterogeneous Linear Metric Learning ( HLML )
Input : F ( · ) , D(· ) , h(· ) , Z0 = [ AZ0 BZ0 ] , β , XU , YU ,
γ1 > 0 , t0 = 1 , and max−iter .
Output : Z ∗ . 1 : Define Fγ,S(Z ) = D(S)+h▽D(S ) , Z −Si+γkZ −Sk2
F /2+
βh(Z )
Set ai = ( ti−1 − 1)/ti−1 . Compute ASi = ( 1 + αi)AZi − αiAZi−1 . Compute BSi = ( 1 + αi)BZi − αiBZi−1 . Set Si = [ ASi BSi ] . Compute ▽AS D(ASi ) and ▽BS D(BSi ) . while ( true )
2 : Set AZ1 = AZ0 and BZ1 = BZ0 . 3 : for i =1,2,· · ·,max−iter do 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : 17 : 18 : 19 : 20 : end for 21 : Set Z ∗ = Zi+1 .
Update ti = 1+q1+4t2
Compute cAS = ASi − ▽AS D(ASi )/γi . Compute [ AZi+1 ] = PSP(cAS ) . Compute cBS = BSi − ▽BS D(BSi )/γi . Compute [ BZi+1 ] = PSP(cBS ) .
Set Zi+1 = [ AZi+1 BZi+1 ] . if F ( Zi+1 ) ≤ Fγi,Si ( Zi+1 ) , then break ; else Update γi = γi × 2 . end if end while i−1 /2 , γi+1 = γi . global energy of every element in the matrix G , and then an energy matrix E can be obtained . Furthermore , the global energies of every element in the matrix E are compared . As shown in Fig 6 , the winner which owns the greatest energy will be set to 1 , the rest of the elements in the same row and column to 0 . And the cycle repeats until a standard elementary transformation matrix T is established . It is worth to note that no similar method has been yet proposed . Section 3.2 presents an efficient algorithm to compute the optimum for the problem Ω1 .
3 . EFFICIENT ALGORITHMS FOR THE
PROPOSED FRAMEWORK
Here we provide two efficient algorithms to solve the proposed framework . Specifically , an iterative algorithm for solving the HLML model Ψ1 ( see Section 2.2 ) is presented in the Section 31 Moreover , the Section 3.2 shows how to solve the MSCD model Ω1 proposed in Section 23
3.1 An Efficient Solver for Ψ1
For notational simplicity , we denote the optimization prob lem Ψ1 in Eq ( 7 ) by : min Z∈C
F ( Z ) = D(Z ) + βh(Z ) ,
( 16 ) where D(· ) = k · k2 F − αg(· ) is a smooth objective function , Z = [ AZ BZ ] symbolically represents the optimization variables , and C is the closed and separately convex domain with respect to each variable :
C = {Z|AT
Z AZ 0 , BT
Z BZ 0} .
( 17 )
As D(· ) is continuously differentiable with Lipschitz contin uous gradient L [ 23 ] : k▽D(Zx)−▽D(Zy)kF ≤ LkZx−ZykF , ∀Zx , Zy ∈ C ,
( 18 ) thus it is appropriate to adopt the Accelerated Projected Gradient ( APG ) [ 23 , 24 , 25 ] method to solve Eq ( 16 ) .
The APG algorithm is a first order gradient method , which can accelerate each gradient step on the feasible solution to obtain an optimal solution when minimizing a smooth function [ 26 ] . This method will construct a solution point sequence {Zi} and a searching point sequence {Si} , where each Zi is updated from Si .
Note that , in the APG algorithm , the Euclidean projection of a given point s onto the convex set C can be defined by : projC(s ) = arg min z∈C kz − sk2
F /2 .
( 19 )
Weinberger et al . proposed a Positive Semi definite Projection ( PSP ) [ 8 ] to minimize a smooth function while remaining positive semi definite constraints . Then we can use the PSP to solve the problem in Eq ( 19 ) .
Finally , when applying the APG method for solving the problem in Eq ( 16 ) , the projection Z = [ AZ BZ ] of a given point S = [ AS BS ] onto the set C is defined by : projC(S ) = arg min Z∈C kZ − Sk2
F /2 .
( 20 )
By combining APG and PSP , we can solve the problem in Eq ( 20 ) . The details are given in Algorithm 1 . if i−1 ≤ 0 && i+1 ≤ n && j −1 ≤ 0 && j +1 ≤ n
Ebs = |Gi+1,j − Gi,j | , Ews = |Gi,j+1 − Gi,j | . elseif i−1≤0 && i+1≤n && j−1>0 && j+1≤n
Ebs = |Gi+1,j − Gi,j | . Ews = |Gi,j+1 − Gi,j | + |Gi,j − Gi,j−1| . elseif i−1≤0 && i+1≤n && j−1>0 && j+1>n Ebs = |Gi+1,j − Gi,j| , Ews = |Gi,j − Gi,j−1| . elseif i−1>0 && i+1≤n && j−1≤0 && j+1≤n
Ebs = |Gi+1,j − Gi,j | + |Gi,j − Gi−1,j | . Ews = |Gi,j+1 − Gi,j | . elseif i−1>0 && i+1≤n && j−1>0 && j+1≤n
Ebs = |Gi+1,j − Gi,j | + |Gi,j − Gi−1,j | . Ews = |Gi,j+1 − Gi,j | + |Gi,j − Gi,j−1| . elseif i−1>0 && i+1≤n && j−1>0 && j+1>n
Ebs = |Gi+1,j − Gi,j | + |Gi,j − Gi−1,j | . Ews = |Gi,j+1 − Gi,j | . for j =1,2,· · ·,n do
Algorithm 2 : Energy Input : G ∈ Rn , δ . Output : an energy matrix E ∈ Rn . 1 : for i =1,2,· · ·,n do 2 : 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 : 14 : 15 : 16 : 17 : 18 : 19 : 20 : 21 : 22 : 23 : 24 : 25 : 26 : 27 : 28 : 29 : end for end for elseif i−1>0 && i+1>n && j−1≤0 && j+1≤n Ebs = |Gi,j − Gi−1,j | , Ews = |Gi,j+1 − Gi,j | . elseif i−1>0 && i+1>n && j−1>0 && j+1≤n
Ebs = |Gi,j − Gi−1,j | . Ews = |Gi,j+1 − Gi,j | + |Gi,j − Gi,j−1| . elseif i−1>0 && i+1>n && j−1>0 && j+1>n Ebs = |Gi,j − Gi−1,j | , Ews = |Gi,j − Gi,j−1| . end if Compute Eij = δ ∗ Ebs + ( 1 − δ ) ∗ Ews .
2050 3.2 An Efficient Solver for Ω1
This subsection provides an efficient algorithm to solve the model Ω1 proposed in Section 23 Similarly , the optimization problem Ω1 can be simplified as : min Z∈Q
Q(Z ) ,
( 21 )
F is a smooth objective function , Z = where Q(· ) = k · k2 [ TZ HZ WZ ] symbolically represents the optimization variables , and Q is the closed domain set with respect to each variable :
Q = {Z|TZ ∈ En2 , HZ ∈ En2 , and W T
Z WZ = I} .
( 22 )
Similarly , as Q(· ) is continuously differentiable with Lipschitz continuous gradient L [ 23 ] in the Eq ( 18 ) , it is also appropriate to adopt the Accelerated Projected Gradient ( APG ) [ 23 ] method to solve the problem in Eq ( 21 ) .
In like manner , we can define the Euclidean projection of a given point s onto the closed set Q in the APG algorithm as : projQ(s ) = arg min z∈Q kz − sk2
F /2 ,
( 23 )
To solve the Eq ( 23 ) , we use the proposed GEC criterion in Section 2.3 to project the approximate solution of the problem into the elementary transformation constraint Q . Two new functions Energy(· ) and Competition(· ) are designed in this subsection to implement the GEC criterion .
The proposed function Energy(· ) in Algorithm 2 will compute the global energy of every internal element in the gradient matrix G obtained by gradient descent method on the basis of the position of each element according to Eq(13,14,15 ) Thus , an energy matrix E can be obtained .
In addition , we also develop a Competition(· ) function in Algorithm 3 to establish a standard elementary transformation matrix according to the energy matrix E produced by Algorithm 2 .
Algorithm 3 : Competition Input : E ∈ Rn . Output : Z ∈ Rn . 1 : Build a zero matrix Z ∈ Rn . 2 : for i =1,2,· · ·,n do 3 :
Find the row and column coordinates r and c of the maximum value of the matrix E . Set Zr,c = 1 .
4 : 5 : Replace the other components of r th row and c th column in the matrix E with 0 .
6 : end for
Note that the orthogonality constraints are included in Eq(21 ) Recently , the Gradient Descent Method with Curvilinear Search ( GDMCS ) [ 27 ] proposed by Wen and Yin can effectively deal with these difficulties . Thus , we can use the GDMCS to preserve the orthogonality of a given point s in Eq ( 21 ) . By combining APG , Algorithm 2 and 3 , and GDMCS , we can solve the problem in Eq ( 21 ) . The details are given in Algorithm 4 , where the function Schmidt(· ) [ 19 ] denotes the GramSchmidt process .
4 . RELATED WORK
This section reviews some related works . We begin by discussing some prior methods for mining the correlation between different views in multi view learning . And then s
Algorithm 4 : Multi view Semi supervised Collaborative Denoising ( MSCD )
F /2 .
Input : Q(· ) , TZ0 = In , HZ0 = In , WZ0 , Z0 = [ TZ0 HZ0 WZ0 ] , δ , γ1 > 0 , t0 = 1 , τ1 , 0 < ρ = [ ρ1 ρ2 ] < 1 , and max−iter . Output : Z ∗ . 1 : Define Qγ,S(Z ) = Q(S)+h▽Q(S ) , Z −Si+γkZ −Sk2 2 : Compute [ WZ0 ] = Schmidt(WZ0 ) . 3 : Set TZ1 = TZ0 , HZ1 = HZ0 , and WZ1 = WZ0 . 4 : for i =1,2,· · ·,max−iter do 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : 13 :
Set ai = ( ti−1 − 1)/ti−1 . Compute TSi = ( 1 + αi)TZi − αiTZi−1 . Compute HSi = ( 1 + αi)HZi − αiHZi−1 . Compute WSi = ( 1 + αi)WZi − αiWZi−1 . Set Si = [ TSi HSi WSi ] . Derive ▽TS Q(TSi ),▽HS Q(HSi ) , and▽WS Q(WSi ) . while ( true )
Compute [ TZi+1 ] = Competition(\TZi+1 ) .
Compute cTS = −▽TS Q(TSi)/γi . Compute [ \TZi+1 ] = Energy(cTS , δ ) . Compute cHS = −▽HS Q(HSi)/γi . Compute [ \HZi+1 ] = Energy(cHS ) . Compute dWS = WSi − ▽WS Q(WSi)/γi . Compute [ \WZi+1 ] = Schmidt(dWS ) .
Compute [ HZi+1 ] = Competition(\HZi+1 ) .
14 : 15 : 16 :
17 : 18 : 19 :
Compute [ WZi+1 ] = GDMCS ( \WZi+1 , τ1 , ρ ) . Set Zi+1 = [ TZi+1 HZi+1 WZi+1 ] . if Q(Zi+1 ) ≤ Qγi ,Si ( Zi+1 ) , then break ; else Update γi = γi × 2 . end if end while
Update ti = 1+q1+4t2 i−1 /2 , γi+1 = γi .
20 : 21 : 22 : 23 : 24 : 25 : 26 : 27 : end for 28 : Set Z ∗ = Zi+1 . ome representative linear metric learning technologies for mono view data are investigated to show the correlation between them and our proposed work . Finally , some multiview semi supervised learning algorithms to bootstrap classifiers in each view are studied .
4.1 Feature Homogeneous Methods
To eliminate the heterogeneity across different views , many feature homogeneous techniques based on Subspace Learning ( SL ) [ 7 , 16 , 14 , 15 ] have been proposed recently .
CCA ( Canonical Correlation Analysis ) [ 7 ] and PLS ( Partial Least Squares ) [ 16 ] are two classical statistical analysis techniques for modeling correlation between sets of observed variables . They both compute low dimensional embedding of sets of variables simultaneously . The main difference of them is that CCA maximizes the correlation between variables in the embedded space , while PLS maximizes their covariance . Kernel CCA ( KCCA ) [ 14 ] offers a nonlinear alternative solution for CCA by implicitly mapping multiview data into a high dimensional feature homogeneous space . Unlike KCCA , Deep CCA ( DCCA ) in [ 15 ] does not require an inner product , which provides a flexible nonlinear alternative to KCCA .
4.2 Linear Metric Learning
Many representative metric learning algorithms , such as
2051 Large Margin Nearest Neighbors ( LMNN ) [ 8 ] , Information Theoretic Metric Learning ( ITML ) [ 28 ] , Neighborhood Component Analysis ( NCA ) [ 9 ] , Logistic Discriminative Metric Learning ( LDML ) [ 29 ] , and Linear Discriminant Analysis ( LDA ) [ 30 ] , are based on linear transformation and distance metric .
Weinberger et al . [ 8 ] proposed a linear metric learning algorithm called LMNN based Mahalanobis distance [ 8 ] for kNearest Neighbors ( kNN ) [ 31 ] classification . ITML [ 28 ] uses a one to one correspondence between the Mahalanobis distance to minimize the differential relative entropy between two multivariate Gaussians under constraints on a distance function . NCA [ 9 ] is another linear metric learning method to find a distance metric that maximizes the performance of kNN classification , measured by Leave One Out ( LOO ) validation . A logistic discriminant approach based marginal probability named LDML was presented by Guillaumin et al . [ 29 ] to learn a metric from a set of labelled image pairs . LDA [ 30 ] is widely used as a form of linear preprocessing for pattern classification , which is operated in a supervised setting and uses the class labels of the inputs to derive informative linear projections .
4.3 Multi View Semi Supervised Learning
Recently , some researchers have investigated many semisupervised learning methods [ 21 , 32 , 33 , 34 ] to deal with various multi view problems .
Muslea et al . pointed out in [ 21 ] that the robustness of multi view learning came from the combination of semisupervised and active learning . In [ 32 ] , Qian et al . presented a joint learning framework based on reconstruction error , namely Semi Supervised Dimension Reduction for Multilabel and Multi view Learning ( SSDR MML ) , to perform optimization for dimension reduction and label inference in multi label and multi view learning settings . Yan and Naphade [ 33 ] proposed a novel multi view semi supervised learning algorithm called Semi supervised Cross Feature Learning ( SCFL ) for detecting the video semantic concepts , which can handle additional views of unlabeled data even when these views were absent from the training data . A MultiView Vector Valued Manifold Regularization ( MV3MR ) algorithm was developed in [ 34 ] to integrate multiple features from different views in the learning process of the vectorvalued function .
5 . EXPERIMENTAL EVALUATION
In this section , we evaluate and analyze the effectiveness of the proposed formulations and algorithms for denoising the corrupted views of multi view data .
5.1 Datasets
Our experiments are conducted on three publicly available multi view datasets , namely , UCI Multiple Features ( UCI MFeat ) [ 35 ] , COREL 5K [ 36 ] , and Alzheimer ’s Disease Neuroimaging Initiative ( ADNI ) [ 6 ] .
5.2 Experimental Setup
Note that all the data are normalized to unit length . Each dataset is randomly separated into a training set and a test set . The training samples account for 80 percent of each original dataset , and the remaining ones act as the test data . Such a partition of each dataset is repeated five times and the average performance is reported . In the training and test sets , 10 percent of multi view data have corrupted view ( We rearrange the corresponding relationships among these multi view data in random order , and corrupt them by white Gaussian noise )
Some key parameters of all the methods in our experiments are tuned using the 5 fold cross validation based on the AUC ( area under the receiver operating characteristic curve ) on the training set . Particularly , the LIBSVM classifier serves as the benchmark for the tasks of classification in the experiments .
5.3 Comparison of Feature Homogeneous Al gorithms
Since the proposed HLML model and other classical feature homogeneous methods such as CCA [ 7 ] , PLS [ 16 ] , KCCA [ 14 ] , and DCCA [ 15 ] are based on subspace learning , we compare their classification performance to show the importance of mining the distributional similarity among different views . Here , the dimensionality k of the featurehomogeneous space is specified by min(dx , dy ) for CCA and PLS . For KCCA and DCCA , we tune the dimensionality k on the candidate set {i × 200|i = 1 , 2 , 3 , · · · , 10} , and Gaussian kernel is used in KCCA . The dimensionality k of the feature homogeneous subspace is set to max(dx , dy ) in HLML , and the trade off parameters α and β are tuned on the sets {10i|i = −2 , −1 , 0 , 1 , 2}
Due to their inherent limitations , PLS and CCA can only project the multi view data into a low dimensional space according to Eq ( 11 ) without the full consideration of distributional similarity among different views . Therefore , the feature homogeneous spaces learned by PLS and CCA may contain much more noise , which groups the instances from different categories together while keeping the samples of the same class away from each other simultaneously . Additionally , KCCA and DCCA offer an alternative solution by nonlinearly mapping the multi view data into a featurehomogeneous space . However , it is very difficult for KCCA and DCCA to capture much distributional information without leave one out validation and low rank regularization .
Table 2 : Classification Performance of Feature Homogeneous Methods in terms of AUC
Method
CCA PLS
KCCA DCCA HLML
Dataset
UCI MFeat COREL 5K ADNI 0.7519 0.7846 0.8096 0.8196 0.8339
0.7936 0.8016 0.6371 0.8494 0.9536
0.5376 0.5597 0.5738 0.5393 0.7213
The proposed HLML model linearly maps multiple heterogeneous low level feature spaces to a high dimensional feature homogeneous one using pseudo metric constraints . As shown in Table 2 , the superiority of HLML over CCA , PLS , KCCA , and DCCA in the classification performance is quite clear . For example , nearly 20 percent gain is achieved for the COREL 5K dataset . It means that HLML can learn the distributional similarity among different views more effectively than CCA , PLS , KCCA , and DCCA .
2052 0.75
0.7
0.65
0.6
0.55
0.5
20 %
HLML
LMNN ITML
NCA LDA
40 %
60 %
80 %
100 %
Number of samples fi
Figure 7 : Comparisons of Classification Performance of Metric Learning Methods .
5.4 Analysis of Metric Learning Methods
To validate the heterogeneous metrics learned by the proposed HLML method , we analyze HLML on the task of classification with other four representative and state of the art metric learning methods such as LMNN [ 8 ] , ITML [ 28 ] , NCA [ 9 ] , and LDA [ 30 ] . This experiment is conducted in the larger COREL 5K dataset . We randomly sample data in the ratio {20 % , 40 % , 60 % , 80 % , 100%} from the training set as the training instances and fix the testing set . For LMNN , ITML , NCA , and LDA , the experiment settings follow the original works [ 8 , 28 , 9 , 30 ] , respectively . ITML uses identity matrix as initial metric matrix . Moreover , we used the codes provided by the authors for LMNN , ITML , and NCA . Similar to LMNN and ITML , the proposed HLML model is also a metric method based on Mahalanobis distance . But the major difference of HLML with the other models lies in that it fully takes into account the distributional similarity among different views . In addition , though NCA also use leave one out validation to exploit the characteristics of sample distribution , the correlation among heterogeneous representations in multi view data is not utilized fully . Moreover , since LDA is originally developed for handling mono view problem , it can only learn some limited distributional information among different views .
We can see from Fig 7 that HLML is superior to other metric learning methods in classification performance . This observation further confirms that HLML can effectively capture both semantic complementarity and distributional similarity among different views . Furthermore , with the increasing of training sample , the performance of HLML will improve . Thus , HLML also has some limitations that it need a certain number of existing samples to learn a set of excellent metrics .
5.5 Comparison of Multi View Semi Supervised
Learning
In essence , like SSDR MML [ 32 ] , SCFL [ 33 ] , and MV3MR [ 34 ] , the proposed MSCD model is also a multi view semisupervised learning method using both labeled and unlabeled data simultaneously . But the explicit difference of MSCD from the former models lies in that it fully takes into account the semantic complementarity among different views . So the latter will be more favorable to reduce intreview noise for reestablishing the complementary relationship among heterogeneous representations .
To validate this point , we first use HLML to project the multi view data into a feature homogeneous space and then apply SSDR MML , SCFL , MV3MR , and MSCD to denoise corrupted view . The performances of the classifiers learned by MSCD and other methods are compared in three multiview dataset . For MSCD , the elementary row transformation matrices T and H are set to identity matrices . We tune the regularization parameters γ and τ on the set {10i| = −2 , −1 , 0 , 1 , 2} . The parameter δ in GEC criterion ( see Eq.(15 ) ) is specified by 01 The decision matrix W ∈Rk×m is randomly initialized . The regularization parameter λ in SSDR MML is set to 1 , and following the original work [ 32 ] , the maximization of learning success measure is adopted to determine the importance of each label . For SCFL [ 33 ] , SV M Light [ 37 ] serves as the underlying classifier where the linear kernel is applied for View Vx and the RBF kernel for View Vy . The parameter setting in MV3MR is the same as in its original reference [ 34 ] . Additionally , we also compare the classification performances of the methods in each iteration round in COREL 5K dataset to verify the convergence of the proposed MSCD algorithm .
Table 3 : Classification Performance of SSDR MML , SCFL , MV3MR and MSCD in terms of AUC
Method
SSDR MML
SCFL
MV3MR MSCD
Dataset
UCI MFeat COREL 5K ADNI 0.8039 0.8172 0.8169 0.8219
0.6738 0.6393 0.6857 0.7087
0.7371 0.8494 0.7826 0.9149
As shown in Table 3 , the superiority of MSCD over SSDRMML , SCFL , and MV3MR in the classification performance is quite clear . This result shows that , in contrast to the compared approaches , MSCD is effective on reducing intreview noise . Moreover , it can be observed from Fig 8 that MSCD shows an obvious advantage over the other methods in every iteration and converges as well . This observation indicates that the MSCD is superior to other multi view semi supervised learning methods in rebuilding the semantic complementarity among different views .
6 . CONCLUSION
In this paper , we have investigated the corrupted views problem in multi view learning . We developed a general framework to denoise corrupted views to obtain CR for multi view data . Within this framework , multiple heterogeneous linear metrics are learned by the proposed HLML model with pseudo metric constraints , leave one out validation , and low rank regularization to unfold the shared infor
0.75
0.70
C U A
0.65
0.60
0.55
0.50
0
SSDR MML
SCFL MV3MR MSCD
50
100
150
200
Number of iteration fi
Figure 8 : Comparison in each iteration round .
2053 mation from different views . Meanwhile , we also proposed a MSCD method with elementary transformation constraints and GEC criterion to remove the intra and inter view noises by using the semantic complementarity and distributional similarity among different views .
Acknowledgments This work was supported in part by the National Natural Science Foundation of China ( No.61271275 and No.61202067 ) , the National High Technology Research and Development Program of China ( No.2013AA013204 ) , and National Science Foundation ( DBI 1147134 and DBI 1350258 ) .
7 . REFERENCES [ 1 ] N . Rasiwasia , J . Costa Pereira , E . Coviello , G . Doyle , G . R . Lanckriet , R . Levy , and N . Vasconcelos . A new approach to cross modal multimedia retrieval . In ACM ICMM , 2010 .
[ 2 ] A . Sharma , A . Kumar , H . Daume III , and D . W .
Jacobs . Generalized multiview analysis : A discriminative latent space . In IEEE CVPR , 2012 .
[ 3 ] M . Xiao and Y . Guo . Semi supervised matrix completion for cross lingual text classification . In AAAI , 2014 .
[ 4 ] D . Tao , X . Li , X . Wu , and S . J . Maybank . Geometric mean for subspace selection . IEEE Trans . Pattern Anal . Mach . Intell . , 31(2):260–274 , 2009 .
[ 5 ] D . Tao , X . Li , X . Wu , and S . J . Maybank . General tensor discriminant analysis and gabor features for gait recognition . IEEE Trans . Pattern Anal . Mach . Intell . , 29(10):1700–1715 , 2007 .
[ 6 ] M . Liu , D . Zhang , and D . Shen . Ensemble sparse classification of alzheimer ’s disease . NeuroImage , 60(2):1106–1116 , 2012 .
[ 7 ] L . Sun , S . Ji , and J . Ye . Canonical correlation analysis for multilabel classification : A least squares formulation , extensions , and analysis . IEEE TPAMI , 33(1):194–200 , 2011 .
[ 8 ] K . Q . Weinberger and L . K . Saul . Distance metric learning for large margin nearest neighbor classification . JMLR , 10:207–244 , 2009 .
[ 9 ] J . Goldberger , G . E . Hinton , S . T . Roweis , and R . Salakhutdinov . Neighbourhood components analysis . In NIPS , 2004 .
[ 10 ] J . Liu , P . Musialski , P . Wonka , and J . Ye . Tensor completion for estimating missing values in visual data . IEEE TPAMI , 35(1):208–220 , 2013 .
[ 11 ] E . J . Cand`es and B . Recht . Exact matrix completion via convex optimization . Found . Comput . Math . , 9(6):717–772 , 2009 .
[ 12 ] B . Recht , M . Fazel , and P . A . Parrilo . Guaranteed minimum rank solutions of linear matrix equations via nuclear norm minimization . SIAM Rev . , 52(3):471–501 , 2010 .
[ 13 ] E . J . Cand`es and T . Tao . The power of convex relaxation : Near optimal matrix completion . IEEE TIT , 56(5):2053–2080 , 2010 .
[ 14 ] D . R . Hardoon , S . Szedmak , and J . Shawe Taylor . Canonical correlation analysis : An overview with application to learning methods . Neural Computat . , 16(12):2639–2664 , 2004 .
[ 15 ] G . Andrew , R . Arora , J . Bilmes , and K . Livescu . Deep canonical correlation analysis . In ACM ICML , 2013 .
[ 16 ] H . Wold . Partial least squares . Encyclopedia of
Statistical Sciences , 1985 .
[ 17 ] J . Yang , J . Yang , D . Zhang , and J . Lu . Feature fusion : Parallel strategy vs . serial strategy . Pattern Recognit . , 36(6):1369–1381 , 2003 .
[ 18 ] Q . Sun , S . Zeng , Y . Liu , P . Heng , and D . Xia . A new method of feature fusion and its application in image recognition . Pattern Recognit . , 38(12):2437–2448 , 2005 .
[ 19 ] C . D . Meyer . Matrix Analysis and Applied Linear
Algebra . Siam , 2000 .
[ 20 ] M . Rubinstein , A . Shamir , and S . Avidan . Improved seam carving for video retargeting . 27(3):16 , 2008 .
[ 21 ] I . Muslea , S . Minton , and C . A . Knoblock . Active + semi supervised learning = robust multi view learning . In ACM ICML , 2002 .
[ 22 ] A . Blum and T . Mitchell . Combining labeled and unlabeled data with co training . In ICCLT , 1998 .
[ 23 ] Y . Nesterov . Introductory Lectures on Convex
Optimization , volume 87 . Springer , 2004 .
[ 24 ] S . Ji and J . Ye . An accelerated gradient method for trace norm minimization . In ICML , pages 457–464 , 2009 .
[ 25 ] J . Liu , S . Ji , and J . Ye . Multi task feature learning via efficient ℓ2,1 norm minimization . In UAI , pages 339–348 , 2009 .
[ 26 ] Y . Nesterov . Smooth minimization of non smooth functions . Math . Program . , 103(1):127–152 , 2005 .
[ 27 ] Z . Wen and W . Yin . A feasible method for optimization with orthogonality constraints . Math . Program . , 142(1 2):397–434 , 2013 .
[ 28 ] J . V . Davis , B . Kulis , P . Jain , S . Sra , and I . S .
Dhillon . Information theoretic metric learning . In ACM ICML , 2007 .
[ 29 ] M . Guillaumin , J . Verbeek , and C . Schmid . Is that you ? metric learning approaches for face identification . In IEEE CVPR , 2009 .
[ 30 ] R . A . Fisher . The use of multiple measures in taxonomic problems . Ann . Eugenics , 7:179–188 , 1936 . [ 31 ] T . M . Cover and P . E . Hart . Nearest neighbor pattern classification . IEEE TIT , 13(1):21–27 , 1967 . [ 32 ] B . Qian , X . Wang , J . Ye , and I . Davidson . A reconstruction error based framework for multi label and multi view learning . IEEE TKDE , 27(3):594–607 , 2015 .
[ 33 ] R . Yan and M . Naphade . Semi supervised cross feature learning for semantic concept detection in videos . In IEEE CVPR , 2005 .
[ 34 ] Y . Luo , D . Tao , C . Xu , D . Li , and C . Xu .
Vector valued multi view semi supervised learning for multi label image classification . In AAAI , 2013 . [ 35 ] R . P . Duin . UCI repository of machine learning databases . 1998 .
[ 36 ] M . Guillaumin , J . Verbeek , and C . Schmid .
Multimodal semi supervised learning for image classification . In IEEE CVPR , 2010 .
[ 37 ] T . Joachims . Making large scale svm learning practical . Technical report , Universit¨at Dortmund , 1999 .
2054
