FINAL : Fast Attributed Network Alignment
Si Zhang
Arizona State University szhan172@asu.edu
Hanghang Tong
Arizona State University hanghangtong@asuedu
ABSTRACT Multiple networks naturally appear in numerous high impact applications . Network alignment ( ie , finding the node correspondence across different networks ) is often the very first step for many data mining tasks . Most , if not all , of the existing alignment methods are solely based on the topology of the underlying networks . Nonetheless , many real networks often have rich attribute information on nodes and/or edges . In this paper , we propose a family of algorithms ( FINAL ) to align attributed networks . The key idea is to leverage the node/edge attribute information to guide ( topology based ) alignment process . We formulate this problem from an optimization perspective based on the alignment consistency principle , and develop effective and scalable algorithms to solve it . Our experiments on real networks show that ( 1 ) by leveraging the attribute information , our algorithms can significantly improve the alignment accuracy ( ie , up to a 30 % improvement over the existing methods ) ; ( 2 ) compared with the exact solution , our proposed fast alignment algorithm leads to a more than 10× speed up , while preserving a 95 % accuracy ; and ( 3 ) our on query alignment method scales linearly , with an around 90 % ranking accuracy compared with our exact full alignment method and a near real time response time .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— Data Mining
Keywords Attributed network alignment ; Alignment consistency ; Onquery alignment
1 .
INTRODUCTION
Multiple networks naturally appear in many high impact application domains , ranging from computer vision , bioinformatics , web mining , chemistry to social network analysis . More often than not , network alignment ( ie , to find node correspondence across different networks ) is virtually the very first step for any data mining task in these applications . For example , by linking users from different social network sites , we could recommend the products from one site ( eg , eBay ) to the users from another site ( eg , Facebook ) [ 29 ] . In bioinformatics , integrating different tissuespecific protein protein interaction ( PPI ) networks has led to a significant improvement for candidate gene prioritization [ 16 ] .
Despite the extensive research on network alignment ( see Section 6 for a review ) , most , if not all , of those work focus on inferring the node correspondence solely based on the topology . For instance , IsoRank [ 21 ] propagates pairwise topology similarities in the product graph . NetAlign utilizes max product belief propagation based on the network topology [ 2 ] . BigAlign and UniAlign [ 11 ] aim to infer the soft alignment based on the assumption that the adjacency matrix of one network is a noisy permutation of another network . A fundamental assumption behind these existing methods is the topology consistency . That is , the same node has a consistent connectivity structure across different networks ( eg , connecting to the same or similar set of the neighbors ) . However , such an assumption could be easily violated in some applications . For example , a user might be very active on one social network site ( eg , Facebook ) , but behaves more quietly on another site ( eg , LinkedIn ) [ 11 ] ; the same gene might exhibit dramatically different behaviors across different tissue specific PPI network [ 16 ] . In these cases , the topology based methods could lead to sub optimal or even misleading alignment results .
At the same time , many real networks often have rich accompanying attributes on the nodes and/or edges ( eg , demographic information of the users , the communication types between different users ) , which might provide a complementary solution to address the node topology consistency assumption violation . Nonetheless , it remains a daunting task to align such attributed networks . To be specific , the following questions have largely remained open . First ( Q1 . Formulation ) , it is not clear how to assimilate node/edge attribute information into the topology based network alignment formulation . For instance , many topologybased alignment approaches can often be formulated from the optimization perspective , yet it is unknown what its attributed counterpart is . Second ( Q2 . Algorithms ) , the optimization problem behind the topology based network alignment is often non convex or even NP hard . Introducing attributes into the alignment process could only further complicate the corresponding optimization problem . How can we develop an effective solver for the attributed network alignment , with a similar or comparable time complexity to its topology only counterpart ? Third ( Q3 . Scalable Computation ) , how can we scale up the attributed network align
1345 ment process by taking advantage of some intrinsic properties ( eg , low rank ) of real networks ? For some applications ( eg , cross network search ) , we might be interested in finding similar nodes across different networks ( eg , to find similar users on LinkedIn for a given user on Facebook ) . How can we further speed up the computation for such an onquery attributed network alignment process , without solving the full alignment problem ?
In this paper , we address the attributed network alignment problem , aiming to answer all these questions . The main contributions of this paper are as follows :
1 . Formulation . We formulate the attributed network alignment from the optimization perspective . The key idea behind the proposed formulation is to leverage the node/edge attribute information to guide ( topologybased ) alignment process based on the alignment consistency principle . As a side product , our formulation helps reveal the quantitative relationships between the ( attributed ) network alignment problem and several other network mining problems ( eg , graph kernel , node proximity ) .
2 . Algorithms and Analysis . We propose a family of algorithms FINAL to solve the attributed network alignment problem . Our analysis shows that the proposed FINAL algorithms are both effective and efficient they converge to the global optima with a complexity that is comparable to the topology only counterpart .
3 . Computations . We further develop ( 1 ) an approximate algorithm FINAL N+ to solve the full attributed network alignment problem , which reduces the time complexity from O(mn ) to O(n2 ) ( where m and n are number of edges and nodes in the network , respectively ) ; and ( 2 ) a linear algorithm to solve the on query attributed network alignment , which achieves a good quality speed trade off .
4 . Evaluations . We perform extensive experiments to validate the effectiveness and the efficiency of the proposed algorithms . Our experimental evaluations demonstrate that ( 1 ) our FINAL algorithms significantly improve the alignment accuracy by up to 30 % over the existing methods ; ( 2 ) the proposed FINAL N+ algorithm leads to a more than 10× speed up , while preserving a 95 % accuracy compared with the exact method ; and ( 3 ) our on query alignment method scales linearly , with an around 90 % ranking accuracy compared with the exact full alignment method and a near real time response time .
The rest of the paper is organized as follows . Section 2 defines the attributed network alignment problem and the on query attributed network alignment problem . Section 3 presents the proposed optimization formulation of FINAL and its solutions . Section 4 proposes two speed up methods for approximate full alignment and on query alignment . Section 5 presents the experimental results . Related work and conclusion are given in Section 6 and Section 7 .
2 . PROBLEM DEFINITIONS
Table 1 summarizes the main symbols and notations used throughout the paper . We use bold uppercase letters for
Table 1 : Symbols and Notation
Symbols
G = {A , N , E}
A N E n1 , n2 m1 , m2
K , L a , b x , y v , w k , l I , 1 H S r , p α a = vec(A )
Q = mat(q , n2 , n1 )
A
D = diag(d )
⊗ fi
Definition an attributed network the adjacency matrix of the network the node attribute matrix of the network the edge attribute matrix of the network # of nodes in G1 and G2 # of edges in G1 and G2 # of the node and edge labels node/edge indices of G1 node/edge indices of G2 node pair indices of the vectorized alignment s = vec(S ) node/edge label indices an identity matrix and a vector of 1s , respectively n2 × n1 prior alignment preference n2 × n1 alignment matrix reduced ranks the parameter , 0 < α < 1 vectorize a matrix A in column order reshape vector q into a n2 × n1 matrix in column order symmetrically normalize matrix A diagonalize a vector d Kronecker product element wise matrix product matrices ( eg , A ) , bold lowercase letters for vectors ( eg , s ) , and lowercase letters ( eg , α ) for scalars . For matrix indexing , we use a convention similar to Matlab ’s syntax as follows . We use A(i , j ) to denote the entry at the intersection of the i th row and j th column of matrix A , A(i , : ) to denote the i th row of A and A( : , j ) to denote the j th column of A . We denote the transpose of a matrix by the superscript T ( eg , AT is the transpose of A ) . We use ( eg , A = D−1/2AD−1/2 , where D is the degree matrix of on top to denote the symmetric normalization of a matrix
A ) . The vectorization of a matrix ( in the column order ) is denoted by vec(. ) , and the resulting vector is denoted by the corresponding bold lowercase letter ( eg , a = vec(A) ) . We represent an attributed network by a triplet : G = {A , N , E} , where ( 1 ) A is the adjacency matrix , and ( 2 ) N and E are the node attribute matrix and the edge attribute matrix , respectively1 . The attribute of node a corresponds to the value of N(a , a ) , and E(a , b ) describes the edge attribute of the edge between node a and node b . For a given node attribute value k , we define Nk as a diagonal matrix with the same size as N , where Nk(a , a ) = 1 if node a has the attribute value k and Nk(a , a ) = 0 otherwise . For a given edge attribute value l , we define El as a matrix of the same size with E , where El(a , b ) = 1 if edge ( a , b ) has the attribute value l and El(a , b ) = 0 otherwise .
Figure 1 presents an illustrative example . We can see from Figure 1(a ) , the set of nodes ( 2 , 3 , 4 and 5 ) from the first network share the exact same topology with another set of nodes ( 2 , 3 , 4 and 5 ) . The topology alone would be inadequate to differentiate these two sets . On the other hand , we can see that ( 1 ) 2 , 2 , 5 and 5 share the same node attribute value ; ( 2 ) 3 , 3 , 4 and 4 share the same node attribute value ; and ( 3 ) the two edges incident to 3 share the same edge attribute value with those incident to 4 . These node/edge attributes could provide vital information to establish the accurate node level alignment ( ie , 2 aligns to 5 , 5 aligns to 2 , 3 aligns to 4 and 4 aligns to 3 ) . This is exactly what this paper aims to address . Formally , the attributed network alignment problem is defined as follows .
Problem 1 . Attributed Network Alignment . Given : ( 1 ) two attributed networks G1 = {A1 , N1 , E1} 1In this paper , we use ‘graph’ and ‘network’ interchangeably , and ‘(categorical ) attribute’ and ‘label’ interchangeably .
1346 ( a ) Input Attributed Networks .
( b ) Matrix Representation .
( c ) Alignment Output .
Figure 1 : An illustrative example of the attributed network alignment problem . ( a ) : two input attributed networks . ( b ) : the matrix representation for attributed networks , where the upper matrices represent the adjacency matrices , and the bottom matrices represent the node attribute ( the diagonal positions ) and the edge attribute ( the off diagonal entries ) matrices . ( c ) : the desired alignment output ( denoted by the red dashed lines ) . and G2 = {A2 , N2 , E2} with n1 and n2 nodes respectively , ( 2 optional ) a prior alignment preference H . Output : the n2×n1 alignment/similarity matrix S , where S(x , a ) represents the alignment/similarity between node a in G1 and node x in G2 .
In the above definition , we have an optional input , to encode the prior knowledge of pairwise alignment preference H , which is an n2 × n1 matrix . An entry in H reflects our prior knowledge of the likelihood to align two corresponding nodes across the two input networks . When such prior knowledge is absent , we set all entries of H equal , ie , a uniform distribution . Without loss of generality , we assume that A1 and A2 share a comparable size , ie , O(n1 ) = O(n2 ) = O(n ) and O(m1 ) = O(m2 ) = O(m ) . This will also help simplify the complexity analysis in the next two sections .
Notice that the alignment matrix S in Problem 1 is essentially a cross network node similarity matrix . In some applications , we might be interested in finding a small number of similar nodes in one network wrt a query node from the other network . For instance , we might want to find the top 10 most similar LinkedIn users for a given Facebook user . We could first solve Problem 1 and then return the corresponding row or column in the alignment matrix S , which might be computationally too costly as well as unnecessary . Having this in mind , we further define the on query attributed network alignment problem as follows :
Problem 2 . On Query Attributed Network Alignment . Given : ( 1 ) two attributed networks G1 = {A1 , N1 , E1} and G2 = {A2 , N2 , E2} , ( 2 optional ) a prior alignment preference H , ( 3 ) a query node a in G1 . Output : an n2 × 1 vector sa measuring similarities between the query node a in G1 and all the nodes in G2 efficiently . 3 . TOPOLOGY MEETS ATTRIBUTES
In this section , we present our solutions for Problem 1 . We start by formulating Problem 1 as a regularized optimization problem , and then propose effective algorithms to solve it , followed by some theoretic analysis . 3.1 FINAL : Optimization Formulation
The key idea behind our proposed formulation lies in the alignment consistency principle , which basically says that the alignments between two pairs of nodes across the two input networks should be consistent if these two pairs of nodes themselves are “ similar/consistent ” with each other . Let us elaborate this using the following example . In Figure 2 , we are given two pairs of nodes : ( 1 ) node a in G1 and node x in G2 ; and ( 2 ) node b in G1 and node y in G2 . By the alignment consistency principle , we require the alignment between a and x , and that between b and y to be consistent ( ie , small S(x , a ) − S(y , b) ) , if all of the following conditions hold , including C1 Topology Consistency . a and b are close neighbors in G1 ( ie , large A1(a , b) ) , and x , y are also close neighbors in G2 ( ie , large A2(x , y) ) ;
C2 Node Attribute Consistency . a and x share the same node attribute value , and so do b and y ;
C3 Edge Attribute Consistency . Edge ( a , b ) and ( x , y ) share the same edge attribute value .
The intuition behind the alignment consistency principle is as follows . If we already know that node a is aligned to node x ( ie , large S(x , a) ) , then their close neighbors ( eg , b and y ) with same node attribute value should have a high chance to be aligned with each other ( ie , large S(y , b) ) , where we say that b and y are close neighbors of a and y respectively if they are connected by the same edge attribute value , with large edge weights ( ie , large A1(a , b ) and A2(x , y) ) . This naturally leads to the following objective function which we wish to minimize in terms of the alignment matrix S : a,b,x,y f ( x , a )
S(x , a )
[
J1(S ) =
× 1(N1(a , a ) = N2(x , x))1(N1(b , b ) = N2(y , y ) )
C1 : Topology Consistency
C2 : Node Attribute Consistency
× 1(E1(a , b ) = E2(x , y ) )

− S(y , b)f ( y , b )
]2 A1(a , b)A2(x , y )
( 1 )
C3 : Edge Attribute Consistency where ( 1 ) a , b = 1 , , n1 , and x , y = 1 , , n2 ; ( 2 ) 1(· ) is the indicator function , which takes 1 if the condition inside the parenthesis is true and zero otherwise ; and ( 3 ) f ( · ) is a node pair normalization function that is defined as f ( x , a ) =
1(E1(a , b ) = E2(x , y))A1(a , b ) if N1(a , a ) = N2(x , x ) b,y
× A2(x , y)1(N1(b , b ) = N2(y , y ) )
( 2 )
1 otherwise
The function f ( x , a ) measures how many ( weighted ) neighborpairs a and x have that ( 1 ) share the same node attribute
1347 value between themselves ( eg , b and y ) , and ( 2 ) connect to a and x via the same edge attribute value , respectively . Notice that the indicator function 1(· ) reflects whether the two input nodes/edges share the same attribute value2 , we factorize it as follows
1(N1(a , a ) = N2(x , x ) ) =
1(E1(a , b ) = E2(x , y ) ) =
Nk
1 ( a , a)Nk
2 ( x , x )
( 3 )
El
1(a , b)El
2(x , y ) k=1
K L − S(y , b)f ( y , b ) l=1
]2
L K k,k=1 1 ( b , b)Nk 2(x , y)Nk
Nk l=1 a,b,x,y
[
S(x , a ) f ( x , a )
K L
Substitute Eq ( 3 ) into Eq ( 1 ) and Eq ( 2 ) , we have
J1(S ) =
1 ( a , a)Nk
2 ( x , x )
×A1(a , b)El
( 4 ) and for nodes with the same attribute value , ie , N1(a , a ) = N2(x , x )
1(a , b)A2(x , y)El
2 ( y , y ) f ( x , a ) =
Nk
1 ( b , b)Nk
2 ( y , y)El
1(a , b)El
2(x , y )
( 5 ) b,y k=1 l=1
×A1(a , b)A2(x , y )
[
J1(s ) =
Next , we present an equivalent matrix form of J1 , which is more convenient for the following algorithm description and the theoretic proof . By vectorizing the alignment matrix S ( ie , s = vec(S) ) , and with the notation of element wise product and Kronecker product , Eq ( 1 ) can be re written as s(v)D(v , v ) =sT ( I −(cid:102)W)s where v = n2(a− 1 ) + x , w = n2(b− 1 ) + y , N =K 2 , E = L ( cid:102)W = D− 1 K
1 ⊗ 2 and W = N[E fi ( A1 ⊗ A2)]N . 1 ⊗ El 2 is the symmetric normalized matrix of
W . The diagonal degree matrix D of W is defined as s(w)D(w , w ) l=1 El 2 WD− 1
]2W(v , w )
L k=1 Nk
Nk
−
( 6 ) v,w
1 fi A1)Nk
1 1)⊗ ( Nk
2 ( El
( Nk
1 ( El
2 fi A2)Nk
2 1 ) )
D = diag( k,k=1 l=1
Note that some diagonal elements in D could be zero ( eg , D(v , v ) = 0 ) . For such elements , we define the corresponding D(v , v)−1/2 0 .
F problem can be stated as follows .
Putting everything together , our proposed optimization argmins J(s ) = αsT ( I −(cid:102)W)s + ( 1 − α ) s − h 2
( 7 ) where · F denotes the Frobenius norm , and α is the regularization parameter . Notice that compared with J1 , we have an additional regularization term s − h2 F to reflect the prior alignment preference , where h = vec(H ) . When no such prior information is given , we set h as a uniform column vector . From the optimization perspective , this additional regularization term would also help prevent a trivial solution of J1 with a zero alignment matrix S . 3.2 FINAL : Optimization Algorithms
The objective function in Eq ( 7 ) is essentially a quadratic function wrt s . We seek to find its fixed point solution by setting its derivative to be zero
= 2(I − α(cid:102)W)s + 2(1 − α)h = 0
∂J(s )
2We remark that by replacing the indicator function 1(· ) by an attribute value similarity function , the proposed formulation can be naturally generalized to handle the numerical attributes on nodes and/or edges .
∂s
Figure 2 : An illustration of alignment consistency . which leads to the following equation s = α(cid:102)Ws + ( 1 − α)h
= αD
2 N(E fi ( A1 ⊗ A2))ND − 1
2 s + ( 1 − α)h − 1
( 8 )
L
We could directly develop an iterative algorithm based on Eq ( 8 ) . However , such an iterative procedure involves the Kronecker product between A1 and A2 whose time complexity is O(m2 ) . In order to develop a more efficient algorithm , thanks to a key Kronecker product property ( ie , vec(ABC ) = ( CT ⊗ A)vec(B) ) , we re write Eq ( 8 ) as follows
− 1
( El
2fiA2)Q(El
1fiA1)T )+(1−α)h ( 9 ) l=1
2 Nvec( s = αD where Q is an n2 × n1 matrix reshaped by q = ND− 1 2 s in column order , ie , Q = mat(q , n2 , n1 ) . We can show that Eq ( 8 ) and Eq ( 9 ) are equivalent with each other ( detailed proofs are omitted due to space ) . The advantage of Eq ( 9 ) is that it avoids the expensive matrix Kronecker product , which leads to a more efficient iterative algorithm FINALNE ( summarized in Algorithm 1 ) . Algorithm 1 FINAL NE : Attributed Network Alignment . Input : ( 1 ) G1 = {A1 , N1 , E1} and G2 = {A2 , N2 , E2} , ( 2 ) optional prior alignment preference H , ( 3 ) the regularization parameter α , and ( 4 ) the maximum iteration number tmax .
Compute vector q = ND− 1 Reshape q as Q = mat(q , n2 , n1 ) ; Initiate an n2 × n1 zero matrix T ; for l = 1 → L do
Output : the alignment matrix S between G1 and G2 . 1 : Construct degree matrix D and node attribute matrix N ; 2 : Initiate the alignment s = h = vec(H ) , and t = 1 ; 3 : while t ≤ tmax do 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 : end while 13 : Reshape s to S = mat(s , n2 , n1 ) . end for Update s ← αD− 1 Set t ← t + 1 ;
2 Nvec(T ) + ( 1 − α)h ;
Update T ← T + ( El
2 fi A2)Q(El
1 fi A1)T ;
2 s ;
Variants of FINAL NE .
Our proposed FINAL NE algorithm assumes that the input networks have both node and edge attributes . It is worth pointing out that it also works when the node and/or the edge attribute information is missing .
First , when only node attributes are available , we can set all entries in the edge attribute matrix E to 1 where an edge indeed exists . The intuition is that we treat all the edges in the networks to share a common edge attribute value . In this case , the fixed point solution in Eq ( 8 ) becomes s = αD where DN = diag(K
= αD
2
2
2
− 1 N s + ( 1 − α)h
− 1 N WN D − 1 − 1 N s + ( 1 − α)h N N(A1 ⊗ A2)ND 1 A1Nk 2 A2Nk 1 1 ) ⊗ ( Nk k,k=1(Nk
2
2 1 ) ) denotes the degree matrix of WN . Similar to Eq ( 9 ) , we can
( 10 )
1348 use the vectorization operator to accelerate the computation . We refer to this variant as FINAL N , and omit the detailed algorithm description due to space .
Second , when only the edge attributes are available , we treat all nodes to share one common node attribute value by setting N to be an identity matrix . In this case , the fixed point solution in Eq ( 8 ) becomes
− 1 E ( E fi ( A1 ⊗ A2))D where DE = diag(L
( 11 ) 2 fi A2)1] ) . Again , we omit the detailed algorithm description due to space , and refer to this variant as FINAL E .
1 fi A1)1 ] ⊗ [ (El
− 1 E s + ( 1 − α)h l=1[(El s = αD
2
2
Finally , if neither the node attributes nor the edge at tributes are available , Eq ( 8 ) degenerates to
− 1 U ( A1 ⊗ A2)D
2
− 1 U s + ( 1 − α)h
2 s = αD
( 12 ) where DU = D1 ⊗ D2 , D1 and D2 are the degree matrix of A1 and A2 respectively . This variant is referred to as FINAL P . 3.3 Proofs and Analysis
In this subsection , we first analyze the convergency , the optimality and the complexity of our FINAL algorithms . Due to the space limit , we only present the results for the most general case ( ie , FINAL NE ) . Then we analyze the relationships between FINAL and several classic graph mining problems .
We start with Theorem 1 , which says the proposed FINALNE algorithm converges to the global optimal solution of Eq ( 7 ) .
Theorem 1 . Convergency and Optimality of FINAL
NE . Algorithm 1 converges to the closed form global mini
Proof . To prove the convergency of Eq ( 8 ) , we first similar to the stochastic matrix WD−1 = D mal solution of J(s ) : s = ( 1 − α)(I − α(cid:102)W)−1h . show the eigenvalues of α(cid:102)W are in ( −1 , 1 ) . Since ( cid:102)W is eigenvalues of ( cid:102)W are within [ −1 , 1 ] . Since 0 < α < 1 , the eigenvalues of α(cid:102)W are in ( −1 , 1 ) . s(t ) = αt(cid:102)Wth + ( 1 − α )
Since the eigenvalues of α(cid:102)W are in ( −1 , 1 ) , we have that t−1 i=0 αi(cid:102)W i = ( I − α(cid:102)W)−1 . Putting t→+∞ αt(cid:102)Wt = 0 and lim
We denote the alignment vector s in the t th iteration as
2(cid:102)WD− 1
αi(cid:102)Wih s(t ) . We have that t−1
2 , the lim i=0
1 these together , we have that t→+∞ t→+∞ s(t ) = ( 1 − α)(I − α(cid:102)W ) s = lim
−1h
Next , we prove that the above result is indeed the global minimal solution of the objective function defined in Eq ( 7 ) . We prove this by showing that J(s ) in Eq ( 7 ) is convex . To see this , we have that the Hessian matrix of Eq ( 7 ) is
( cid:79)2J = 2(I − α(cid:102)W ) . By the Weyl ’s inequality theorem [ 4 ] , all eigenvalues of 2(I − α(cid:102)W ) are greater than 0 . In other words , we have that ( cid:79)2J is positive definite . Therefore , the objective function defined in Eq ( 7 ) is convex , and its fixed point solution by Algorithm 1 corresponds to its global minimal solution , which completes the proof .
The time and space complexity of Algorithm 1 are summarized in Lemma 1 . Notice that such a complexity is comparable to the complexity of topology alone network alignment methods , such as IsoRank [ 21 ] . In the next section , we will propose an even faster algorithm .
Lemma 1 . Complexity of FINAL NE . The time complexity of Algorithm 1 is O(Lmntmax + LK 2n2 ) , and its space complexity is O(n2 ) . Here , n and m are the orders of the number of nodes and edges of the input networks , respectively ; K , L denote the number of unique node and edge attributes respectively , and tmax is the maximum iteration number .
Proof . Omitted for space . Finally , we analyze the relationships between the proposed FINAL algorithms and several classic graph mining problems . Due to the space limit , we omit the detailed proofs and summarize the major findings as follows . A FINAL vs . Node Proximity .
An important ( single ) network mining task is the node proximity , ie , to measure the proximity/similarity between two nodes on the same network . By ignoring the node/edge attributes and setting A1 = A2 , our FINAL algorithms , up to a scaling operation D1/2 , degenerate to SimRank [ 12 ] a prevalent choice for node proximity . Our FINAL algorithms are also closely related to another popular node proximity method , random walk with restart [ 24 ] . That is , Eq ( 8 ) can be viewed as random walk with restart on the attributed Kronecker graph with h being the starting vector . Note that neither the standard SimRank nor random walk with restart considers the node or edge attribute information . B FINAL vs . Graph Kernel .
The alignment result s by our FINAL algorithms is closely related to the random walk based graph kernel [ 25 ] . To be specific , if k(G1,G2 ) is the random walk graph kernel between the two input graphs and p is the stopping vector , we can show that k(G1,G2 ) = pT s . This intuitively makes sense , as we can view the graph kernel/similarity as the weighted aggregation ( by the stopping vector p ) over the pairwise cross network node similarities ( encoded by the alignment vector s ) . We also remark that in the original random walk graph kernel [ 25 ] , it mainly focuses on the edge attribute information . C FINAL vs . Existing Network Alignment Methods .
If we ignore all the node and edge attribute information , our FINAL P algorithm is equivalent to IsoRank [ 21 ] by scaling the alignment result and alignment preference by D1/2 . We would like to point out that such a scaling operation is important to ensure the convergence of the iterative procedure . Recall that the key idea behind our optimization formulation is the alignment consistency . When the attribute information is absent , the alignment consistency principle is closely related to the concept of “ squares ” behind NetAlign algorithm [ 2 ] . Like most , if not all of the , existing network alignment algorithms , the node or the edge attribute information is ignored in IsoRank and NetAlign .
We remark that these findings are important in the following two aspects . First , they help establish a quantitative relationship between several , seemingly unrelated graph mining problems , which might in turn help better understand these existing graph mining problems . Second , these findings also have an important algorithmic implication . Take SimRank as an example , it was originally designed for plain graphs ( ie , without attributes ) , and was formulated from random walk perspective and it is not clear what the algorithm tries to optimize . By setting G1 = G2 and ignoring the attribute information , our objective function in Eq ( 7 )
1349 Proof . Omitted for space .
Algorithm 2 FINAL N+ : Low Rank Approximation of FINAL N . Input : ( 1 ) G1 = {A1 , N1} and G2 = {A2 , N2} , ( 2 ) optional prior alignment preference H , ( 3 ) the regularization parameter α , and ( 4 ) the rank of eigenvalue decomposition r . Output : approximate alignment matrix S between G1 and G2 . 1 : Construct degree matrix DN and node attribute matrix N ; 2 : Construct alignment preference vector h = vec(H ) ; 2 ← A2 3 : Eigenvalue decomposition U1Λ1UT 4 : Compute U = U1 ⊗ U2 ; 5 : Compute Λ = [ (Λ1 ⊗ Λ2)−1 − αUT ND−1 6 : Compute s by Eq ( 15 ) ; 7 : Reshape vector s to S = mat(s , n2 , n1 ) .
1 ← A1 , U2Λ2UT
N NU]−1 ;
Proposed Solution for Problem 2
4.2 In Problem 2 , we want to find an n2 × 1 vector sa which measures the similarities between the query node a in G1 and all the n2 nodes in G2 ( ie , cross network similarity search ) . It is easy to see that sa is essentially the a th column of the alignment matrix S , or equivalently a certain portion of the alignment vector s , ie , where v = ( a − 1)n2 + 1 and w = an2 . sa = S( : , a ) = s(v : w )
However , if we call FINAL N or FINAL N+ to find S ( or s ) and then return the ranking vector sa , it would take at least O(n2 ) time . Next , we propose an approximate algorithm ( FINAL On Query ) which directly finds the ranking vector sa in linear time , without solving the full alignment matrix S . We first relax the degree matrix DN to its upper bound ˆDN = D1 ⊗ D2 . There are two reasons for taking such a relaxation . First , it would take O(n2 ) time to compute the DN matrix directly . On the other hand , ˆDN can be indirectly expressed by the Kronecker product between D1 and D2 , each of which only takes O(m ) time . Second , since ˆDN is an upper bound of the DN matrix , such a relaxation will not affect the convergence of FINAL N . By this relaxation , the fixed point solution in Eq ( 10 ) can be approximated as s = αN ˆD
− 1 N ( A1 ⊗ A2 ) ˆD
2
− 1 N Ns + ( 1 − α)h
2
( 16 ) provides a natural way to interpret SimRank from an optimization perspective . By setting G1 = G2 alone ( ie , keeping the attribute information ) , our FINAL algorithms can be directly used to measure node proximity on an attributed network . Finally , our upcoming FINAL On Query algorithm also naturally provides an efficient way ( ie , with a linear time complexity ) for on query SimRank with or without attribute information ( ie , finding the similarity between a given query node and all the remaining nodes in the same network ) .
4 . SPEED UP COMPUTATION
In this section , we address the computational issue . To be specific , we will focus on two scenarios . First , to solve Problem 1 , our proposed FINAL algorithms in Section 3 have a time complexity of O(mn ) , where we have dropped the lower order terms . We will propose an effective approximate algorithm that reduces the time complexity to O(n2 ) . Second , for Problem 2 , solving the full alignment problem not only still requires O(n2 ) time , but also is unnecessary , as we essentially only need a column or a row from the alignment matrix S . To address this issue , we will propose an effective algorithm for Problem 2 with a linear time complexity . For presentation clarity , we restrict ourselves to the case where there is only node attribute information , although our proposed strategies can be naturally applied to the more general case where we have both node and edge attributes . 4.1 Speed up FINAL N
According to Theorem 1 , the alignment vector s in FINAL
N converges to its closed form solution as follows . s = ( 1 − α)(I − α(cid:102)WN )
−1h
= ( 1 − α)(I − αD
− 1 N N(A1 ⊗ A2)ND
2
− 1 N )
2
−1h
( 13 )
The key idea to speed up FINAL N is to efficiently approximate such a closed form solution . To be specific , we first approximate the two adjacency matrices by top r eigenvalue decomposition : A1 = U1Λ1UT 1 and A2 = U2Λ2UT 2 . Then the rank r approximation of WN can be defined as follows
ˆWN = N[(U1Λ1UT
1 ) ⊗ ( U2Λ2UT = N(U1 ⊗ U2)(Λ1 ⊗ Λ2)(UT
2 )]N 1 ⊗ UT
2 )N
Substitute Eq ( 14 ) into Eq ( 13 ) , we can approximate the alignment vector s as s ≈(1 − α)[I − αD =(1 − α)(I + αD
2
− 1 N NU(Λ1 ⊗ Λ2)UT ND − 1 N NUΛUT ND
− 1 N )h
2
2
− 1 N ]−1h
2
2 )ND
1 ⊗ UT where U = U1 ⊗ U2 , and Λ is an r2 × r2 matrix computed by Sherman Morrison Lemma [ 18 ] : Λ = [ (Λ1 ⊗ Λ2)−1 − α(UT
N N(U1 ⊗ U2)]−1 . −1
Based on Eq ( 15 ) , our proposed FINAL N+ algorithm is summarized in Algorithm 2 . The time complexity of FINAL N+ is summarized in Lemma 2 . Notice that we often have r n , m n2 and K n . Therefore , compared with FINAL N , FINAL N+ is much more efficient in time complexity .
Lemma 2 . Time Complexity of FINAL N+ . FINALN+ takes O(n2r4 +Kn2 ) in time ,where n is the order of the number of nodes , r is the rank of eigenvalue decomposition and K is the number of node attributes .
( 14 ) where ˆDN = D1 ⊗ D2 . proximate solution for s is
By a similar procedure in FINAL N+ , the low rank aps ≈ ( 1 − α)h + α(1 − α ) ˆD
( 17 )
− 1 N h
2
2
− 1 N NU ˆΛUT N ˆD −1 N U]−1 .
( 15 ) where ˆΛ = [ (Λ1 ⊗ Λ2)−1 − αUT N ˆD
Since both ˆDN and N are diagonal matrices , the ranking vector for node a is sa =(1 − α)[h(v : w ) + α[ ˆD
− 1 N NU ˆΛUT N ˆD
2
− 1 N h](v : w ) ]
2
2 ) ]
Nk
1 ( a , a)Nk
ˆΛ
×[(U1(a , : ) ⊗ U2 )
=(1 − α)[H( : , a ) + α[(D1(a , a)D2)− 1 2 ( decomposition ( SVD ) on H , ie , H =p
Notice that Eq ( 18 ) still needs O(n2 ) time due to the last two terms . We reduce the time cost for computing g = − 1 UT N ˆD N h as follows . First , we take a rank p singular value i . Then , k=1 − 1 ] N h
UT N ˆD
O(n2r4+r6 ) i=1 σiuivT
O(n2r2 )
O(nr2 )
( 18 )
2 by the vectorization operator , we have that
K
2
1350 p
K i=1 k=1 g =
O(nr )
O(nr )
UT
2 Nk
2 D
− 1 2 ui )
2
− 1 1 vi ) ⊗ (
2
O(nr+r2)=O(nr )
σi(
UT
1 Nk
1 D
( 19 )
We can see that the time cost for Eq ( 19 ) is reduced to
O(pKrn ) , which is linear wrt the number of nodes n .
We reduce the time cost for computing ˆΛ by reformulating as follows , whose time complexity is O(Knr2 + Kr4 + r6 )
ˆΛ = [ (Λ2 ⊗ Λ1)−1
−α
UT (
1 Nk
O(r2 )
K k=1
O(nr2 )
1 D−1
1 U1 ) ⊗ (
O(nr2+r4 )
O(nr2 )
2 D−1
UT
2 Nk
2 U2 )
]−1
( 20 ) Putting everything together , the ranking vector of node a now becomes sa =(1 − α)H( : , a ) + α(1 − α)[
×[(U1(a , : ) ⊗ U2 )
O(n )
( D1(a , a)D2)− 1 g ˆΛ
2
]
K k=1
O(Kn )
Nk
1 ( a , a)Nk 2 ]
( 21 )
O(nr2 )
O(Knr2+Kr4+r6 )
O(pKnr )
Based on Eq ( 21 ) , our proposed FINAL On Query algorithm is summarized in Algorithm 3 . The time complexity of FINAL On Query is summarized in Lemma 3 . Notice that we often have r , p n , mH m n2 and K n . FINAL On Query has a linear time complexity wrt the size of the input network , which is much more scalable than both FINAL N and FINAL N+ .
Algorithm 3 FINAL On Query : Approximate On Query Algorithm for Node Attributed Networks . Input : ( 1 ) G1 = {A1 , N1} and G2 = {A2 , N2} , ( 2 ) optional prior alignment preference H , ( 3 ) the regularization parameter α , ( 4 ) the rank of eigenvalue decomposition r , and ( 5 ) the rank of SVD for H p . Output : approximate ranking vector sa between node a in G1 and all nodes in G2 . Pre Compute :
1 : Compute degree matrices D1 and D2 ;
2 : Compute Da = D1(a , a)D2 , and Na =K 5 : Rank p singular value decompositionp
3 : Rank r eigenvalue decomposition U1Λ1UT 4 : Rank r eigenvalue decomposition U2Λ2UT k=1 Nk 1 ← A1 ; 2 ← A2 ; i=1 σiuivT i ← H ;
1 ( a , a)Nk 2 ;
6 : Compute g by Eq ( 19 ) ; 7 : Compute ˆΛ by Eq ( 20 ) ;
Online Query :
8 : Compute sa by Eq ( 21 ) .
Lemma 3 . Time complexity of FINAL On Query . The time complexity of FINAL On Query is O(r6 + mr + nr2 + mH p + np2 + Knr2 + Kr4 + pKnr ) . where n , m are the orders of the number of nodes and edges respectively , r , p is the rank of eigenvalue decomposition and SVD , respectively , K is the number of node attributes and mH is the number of non zero elements in H .
Proof . Omitted for brevity .
5 . EXPERIMENTAL RESULTS
In this section , we present the experimental results and analysis of our proposed algorithms FINAL . The experiments are designed to evaluate the following aspects :
• Effectiveness : How accurate are our algorithms for • Efficiency : How fast are our proposed algorithms ? aligning attributed networks ?
5.1 Experimental Setup Datasets . We evaluate our proposed algorithms on six realworld attributed networks .
• DBLP Co Authorship Network : This dataset contains 42,252 nodes and 210,320 edges [ 19 ] . Each author has a feature vector which represents the number of publications of the author in each of 29 major conferences . • Douban : This Douban dataset was collected in 2010 and contains 50k users and 5M edges [ 31 ] . Each user has rich information , such as the location and offline event participation . Each edge has an attribute representing whether two users are contacts or friends . • Flickr : This dataset was collected in 2014 and consists of 215,495 users and 9,114,557 friend relationships . Users have detailed profile information , such as gender , hometown and occupation , each of which can be treated as the node attributes [ 30 ] . • Lastfm : This dataset was collected in 2013 and contains 136,420 users and 1,685,524 following relationships [ 30 ] . A detailed profile of some users is also provided , including gender , age and location , etc . • Myspace : This dataset contains 854,498 users and 6 ,489,736 relationships . The profile of users includes gender , hometown and religion , etc . [ 30 ] . • ArnetMiner : ArnetMiner dataset consists of the information up to year 2013 . The whole dataset has 1,053,188 nodes and 3,916,907 undirected edges [ 30 ] .
Based on these datasets , we construct the following five alignment scenarios for evaluations . S1 . DBLP vs . DBLP . We extract a subnetwork with 9,143 users/nodes from the original dataset , together with their publications in each conference . We randomly permute this subnetwork with noisy edge weights and treat it as the second network . We choose the most active conference of a given author as the node attribute , ie , the conference with the most publications . We construct the prior alignment preference H based on the node degree similarity . For this scenario , the prior alignment matrix H alone leads to a very poor alignment result , with only 0.6 % one to one alignment accuracy . S2 . Douban Online vs . Douban Offline . We construct an alignment scenario for Douban dataset in the same way as [ 31 ] . We construct the offline network according to users’ co occurrence in social gatherings . We treat people as ( 1 ) ‘contacts’ of each other if they participate in the same offline events more than ten times but less than twenty times , and ( 2 ) ‘friends’ if they co participate in more than twenty social gatherings . The constructed offline network has 1,118 users and we extract a subnetwork with 3,906 nodes from the provided online network that contains all these offline users . We treat the location of a user as the node attribute , and ‘contacts’/‘friends’ as the edge attribute . We use the degree similarity to construct the prior alignment preference H . The prior alignment matrix H alone leads to 7.07 % one to one alignment accuracy . S3 . Flickr vs . Lastfm . We have the partial ground truth alignment for these two datasets [ 30 ] . We extract the subnetworks from them that contain the given ground truth
1351 nodes . The two subnetworks have 12,974 nodes and 15,436 nodes , respectively . We consider the gender of a user as node attribute . For those users with the missing information of gender , we treat them as ‘unknown’ . Same as [ 30 ] , we sort nodes by their pagerank scores and label 1 % highest nodes as ‘opinion leaders’ , the next 10 % nodes as ‘middle class’ and remaining nodes as ‘ordinary users’ . Edges are attributed by the level of people they connect to ( eg , leader with leader ) . We use the username similarity as the prior alignment preference by the Jaro Winkler distance [ 6 ] . The username similarity alone can correctly align 61.50 % users . S4 . Flickr Myspace . Same as S3 , we have the partial groundtruth alignment for these two datasets . We extract two subnetworks that contain these ground truth nodes . The subnetwork of Flickr has 6,714 nodes and the subnetwork of Myspace has 10,733 nodes . We use the same way as S3 for node attributes , edge attributes and the prior alignment preference . The username similarity alone can correctly align 61.80 % users . S5 . ArnetMiner ArnetMiner . We use the same method as S1 to construct the alignment scenario as well as the prior alignment preference . This scenario contains the largest networks , and therefore is used for efficiency evaluations . Comparison Methods . For the proposed FINAL algorithms , we test the following variants , including ( 1 ) FINALNE with both node and edge attributes ; ( 2 ) FINAL N with node attributes only ; ( 3 ) FINAL E with edge attributes only ; ( 4 ) FINAL N+ , a low rank based approximate algorithm of FINAL N . We compare them with the following existing network alignment algorithms including ( 1 ) IsoRank [ 21 ] , ( 2 ) NetAlign [ 2 ] , ( 3 ) UniAlign [ 11 ] and ( 4 ) Klau ’s Algorithm [ 2 , 9 ] . Machines and Repeatability . All experiments are performed on a Windows machine with four 3.6GHz Intel Cores and 32G RAM . The algorithms are programmed with MATLAB using a single thread . We intend to release the source code as well as all the non proprietary datasets after the paper is published .
5.2 Effectiveness Analysis
We first evaluate the impact of the permutation noise on the alignment accuracy . We use a heuristic greedy matching algorithm [ 10 ] as a post processing step on the similarity matrix to obtain the one to one alignments between the two input networks , and then compute the alignment accuracy with respect to the ground truth . The results are summarized in Figure 3 . We have the following observations . First , all of our proposed methods outperform the three existing alignment methods . Specifically , FINAL NE achieves a 20% 30 % improvement in terms of the alignment accuracy over the existing methods ( ie , IsoRank , NetAlign and UniAlign ) . Second , FINAL N and FINAL E both outperform the existing methods , yet are not as good as FINALNE , suggesting that node attributes and edge attributes might be complementary in terms of improving the alignment accuracy . Third , the alignment accuracy of FINALN+ is very close to its exact counterpart FINAL N ( ie , with a 95 % accuracy compared with FINAL N ) . Fourth , by jointly considering the attributes and the topology of networks , our methods are more resilient to the permutation noise . Finally , for the two networks whose topologies are dramatically different from each other ( eg , Douban onlineoffline networks ) , the accuracy gap between FINAL N+ and
( a ) DBLP co author networks . ( α = 08 )
( b ) Douban online offline networks . ( α = 06 )
Figure 3 : ( Higher is better . ) Alignment accuracy vs . the noise level in networks . ( tmax = 30 , r = 5 ) .
( a ) Flickr Lastfm networks .
( b ) Flickr Myspace networks . Figure 4 : ( Higher is better . ) Alignment accuracy vs . the noise level in H . ( α = 0.3 , tmax = 30 , r = 5 ) . the existing methods is even bigger ( Figure 3(b) ) . This is because in this case , the topology information alone ( IsoRank , NetAlign and Klau ) could actually mislead the alignment process .
Second , we evaluate the impact of the noise in the prior alignment preference ( ie , H ) on the alignment results , which is summarized in Figure 4 . As expected , a higher noise in H has more negative impacts on the alignment accuracy for most of the methods . Nonetheless , our FINAL algorithms still consistently outperform all other four existing methods across different noise levels . 5.3 Efficiency Analysis
( a ) DBLP co author networks . α = 08
( b ) Flickr Lastfm networks . α = 03
Figure 5 : Balance between the accuracy and the speed . tmax = 30 , r = 5 . Quality Speed Trade off . We first evaluate how different methods balance the alignment accuracy and the running time for the full network alignment problem ( ie , Problem 1 ) . The results are summarized in Figure 5 . As we can see , the running time of our proposed exact methods ( eg , FINAL N , FINAL E ) is only slightly higher than its topology alone counterpart ( ie , IsoRank ) , and in the meanwhile , they all achieve a 10% 20 % accuracy improvement . FINAL N+ and UniAlign are the fastest , yet the proposed FINAL N+ produces a much higher alignment
Noise on Weight00050101502Accuracy0020406081FINAL NFINAL EFINAL NEFINAL N+IsoRankNetAlignUniAlignKlauNoise on Weight00050101502Accuracy0010203040506FINAL NFINAL EFINAL NEFINAL N+IsoRankNetAlignUniAlignKlauNoise on Alignment Preference00050101502Accuracy0020406081FINAL NFINAL EFINAL NEFINAL N+IsoRankNetAlignUniAlignKlauNoise on Alignment Preference00050101502Accuracy0020406081FINAL NFINAL EFINAL NEFINAL N+IsoRankNetAlignUniAlignKlauRunning Time ( second)20 200 500 10001500Accuracy0020406081FINAL NFINAL EFINAL NEFINAL N+IsoRankNetAlignUniAlignRunning Time ( second)20 200 500 20003000Accuracy0020406081FINAL NFINAL EFINAL NEFINAL N+IsoRankNetAlignUniAlign1352 A classic alignment approach can be attributed to IsoRank algorithm [ 21 ] , which is in turn inspired by PageRank [ 17 ] . The original IsoRank algorithm propagates the pairwise node similarity in the Kronecker product graph . Several approximate algorithms have been proposed to speed up its computation . Kollias et al . [ 10 ] propose an efficient method based on uncoupling and decomposition . SpaIsoRank modifies IsoRank to maximize the number of “ squares ” for sparse networks [ 2 ] . In addition , IsoRankN [ 13 ] extends the original IsoRank algorithm and uses a similar approach as PageRank Nibble [ 1 ] to align multiple networks .
Bayati et al . [ 3 ] propose a maximum weight matching algorithm for graph alignment using the max product belief propagation [ 26 ] . Bradde et al . [ 5 ] propose another distributed message passing algorithm based on belief propagation for protein protein interaction network alignment . Recently , NetAlign [ 2 ] is proposed by formulating the network alignment problem as an integer quadratic programing problem to maximize the number of “ squares ” . A nearoptimal solution is obtained by finding the maximum a posteriori ( MAP ) assignment using message passing approach . BigAlign formulates the bipartite network alignment problem and uses the alternating projected gradient descent to solve it [ 11 ] . Zhang et al . solve the multiple anonymized network alignment in two steps , ie , unsupervised anchor link inference and transitive multi network matching [ 28 ] .
A related problem is to identify users from multiple social networks ( ie , the cross site user identification problem ) . Zafarani et al . identify users by modeling user behavior patterns based on human limitations , exogenous and endogenous factors [ 27 ] . Tan et al . [ 23 ] propose a subspace learning method , which models user relationship by a hypergraph . Liu et al . propose a method to identify same users by behavior modeling , structure consistency modeling and learning by multi objective optimization [ 14 ] . COSNET [ 30 ] considers both local the global consistency and uses an energy based model to find connections among multiple heterogeneous networks .
7 . CONCLUSION
In this paper , we study the attributed network alignment problem , including the full alignment version as well as its on query variant . We propose an optimization based formulation based on the alignment consistency principle . We propose a family of effective and efficient algorithms to solve the attributed network alignment problem . In detail , we first propose exact algorithms ( FINAL ) which are proved to converge to the global optima , with a comparable complexity with their topology alone counterparts . We then propose ( 1 ) an approximate alignment algorithm ( FINALN+ ) to further reduce the time complexity ; and ( 2 ) an effective alignment algorithm ( FINAL On Query ) to solve the on query network alignment problem with a linear time complexity . We conduct extensive empirical evaluations on real networks , which demonstrate that ( 1 ) by assimilating the attribute information during the alignment process , the proposed FINAL algorithms significantly improve the alignment accuracy by up to 30 % over the existing methods ; ( 2 ) the proposed approximate alignment algorithm ( FINALN+ ) achieves a good balance between the running time and the alignment accuracy ; and ( 3 ) the proposed on query alignment algorithm ( FINAL On Query ) ( a ) preserves an around 90%+ ranking accuracy , ( b ) scales linearly wrt the
Figure 6 : Alignment accuracy vs . running time for on query alignment .
Figure 7 : FINAL N+ .
Scalability of accuracy . NetAlign takes the longest running time as it involves a time consuming , greedy/max weight matching process during each iteration . We do not show the balance of Klau ’s Algorithm because the running time is usually several hours which is not comparable with other methods . Overall , FINAL N+ achieves the best trade off between the alignment accuracy and the running time .
Second , we evaluate the quality speed trade off for onquery alignment problem ( ie , Problem 2 ) . Here , we treat the top 10 ranking results by FINAL N as the ground truth , and compare the average ranking accuracy of 500 random nodes with two proposed approximate algorithms ( FINALN+ and FINAL On Query ) . The results are summarized in Figure 6 . We can see , that ( 1 ) FINAL N+ preserves a 95 % ranking accuracy , with a more than 10× speedup over FINAL N , ( 2 ) FINAL On Query preserves an about 90 % ranking accuracy , and it is 100× faster than the exact FINAL N . Scalability . We first evaluate the scalability of FINALN+ , which is summarized in Figure 7 . We can see that the running time is quadratic wrt the number of nodes of the input networks , which is consistent with the time complexity results in Lemma 2 . Second , we evaluate the scalability of FINAL On Query , for both its pre compute phase and online query phase . As we can see from Figure 8 , the running time is linear wrt the number of nodes in both stages , which is consistent with Lemma 3 . In addition , the actual online query time on the entire ArnetMiner data set ( with r = 10 ) is less than 1 second , suggesting that the proposed FINAL On Query method might be well suitable for the real time query response .
( a ) Pre compute phase .
( b ) Online query phase .
Figure 8 : Scalability of FINAL On Query .
6 . RELATED WORK
The network alignment has attracted lots of research interests with extensive literatures . It appears in numerous domains , ranging from database schema matching [ 15 ] , bioinformatics [ 21 , 13 , 8 ] , chemistry [ 22 ] , computer vision [ 7 , 20 ] , to data mining [ 11 , 2 ] .
Log of Time ( second)100101102103Relative Accuracy0020406081Flickr Myspace Flickr LastfmFINAL On QueryFINAL N+Exact FINAL NNumber of Nodes×1040051152253Running Time ( second)0200400600800r=1r=2r=5r=10r=15Number of Nodes in Networks×1050246810Running Time ( second)0102030405060rank=1rank=2rank=5rank=10rank=15Number of Nodes in Networks×1050246810Online Query Time ( second)005115225rank=1rank=2rank=5rank=10rank=151353 size of the input networks , and ( c ) responds in near real time . Future work includes generalizing FINAL algorithms to handle dynamic networks . 8 . ACKNOWLEDGEMENTS
This work is partially supported by the National Science Foundation under Grant No . IIS1017415 , by DTRA under the grant number HDTRA1 16 0017 , by Army Research Office under the contract number W911NF 16 1 0168 , by National Institutes of Health under the grant number R01LM 011986 , Region II University Transportation Center under the project number 49997 33 25 and a Baidu gift . We would like to sincerely thank Dr . Jie Tang and Dr . Yutao Zhang for their generosity to share the datasets , and anonymous reviewers for their insightful and constructive comments . 9 . REFERENCES [ 1 ] R . Andersen , F . Chung , and K . Lang . Local graph partitioning using pagerank vectors . IEEE , 2006 .
[ 2 ] M . Bayati , M . Gerritsen , D . F . Gleich , A . Saberi , and
Y . Wang . Algorithms for large , sparse network alignment problems . IEEE , 2009 .
[ 3 ] M . Bayati , D . Shah , and M . Sharma . Maximum weight matching via max product belief propagation . IEEE , 2005 .
[ 4 ] R . Bhatia . Linear algebra to quantum cohomology : the story of alfred horn ’s inequalities . The American Mathematical Monthly , 108(4):289–318 , 2001 .
[ 5 ] S . Bradde , A . Braunstein , H . Mahmoudi , F . Tria , M . Weigt , and R . Zecchina . Aligning graphs and finding substructures by a cavity approach . EPL ( Europhysics Letters ) , 89(3):37009 , 2010 .
[ 6 ] W . Cohen , P . Ravikumar , and S . Fienberg . A comparison of string metrics for matching names and records . 2003 .
[ 7 ] D . Conte , P . Foggia , C . Sansone , and M . Vento .
Thirty years of graph matching in pattern recognition . International journal of pattern recognition and artificial intelligence , 18(03):265–298 , 2004 .
[ 8 ] S . Hashemifar and J . Xu . Hubalign : an accurate and efficient method for global alignment of protein–protein interaction networks . Bioinformatics , 30(17):i438–i444 , 2014 .
[ 9 ] G . W . Klau . A new graph based method for pairwise global network alignment . BMC bioinformatics , 10(Suppl 1):S59 , 2009 .
[ 10 ] G . Kollias , S . Mohammadi , and A . Grama . Network similarity decomposition ( nsd ) : A fast and scalable approach to network alignment . Knowledge and Data Engineering , IEEE Transactions on , 24(12):2232–2243 , 2012 .
[ 11 ] D . Koutra , H . Tong , and D . Lubensky . Big align : Fast bipartite graph alignment . In Data Mining ( ICDM ) , 2013 IEEE 13th International Conference on , pages 389–398 . IEEE , 2013 .
[ 12 ] C . Li , J . Han , G . He , X . Jin , Y . Sun , Y . Yu , and
T . Wu . Fast computation of simrank for static and dynamic information networks . ACM , 2010 .
[ 13 ] C S Liao , K . Lu , M . Baym , R . Singh , and B . Berger .
Isorankn : spectral methods for global alignment of multiple protein networks . Bioinformatics , 25(12):i253–i258 , 2009 .
[ 14 ] S . Liu , S . Wang , F . Zhu , J . Zhang , and R . Krishnan .
Hydra : Large scale social identity linkage via heterogeneous behavior modeling . ACM , 2014 .
[ 15 ] S . Melnik , H . Garcia Molina , and E . Rahm . Similarity flooding : A versatile graph matching algorithm and its application to schema matching . IEEE , 2002 .
[ 16 ] J . Ni , H . Tong , W . Fan , and X . Zhang . Inside the atoms : ranking on a network of networks . In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 1356–1365 . ACM , 2014 .
[ 17 ] L . Page , S . Brin , R . Motwani , and T . Winograd . The pagerank citation ranking : bringing order to the web . 1999 .
[ 18 ] W . W . Piegorsch and G . Casella . Erratum : inverting a sum of matrices . SIAM review , 32(3):470 , 1990 . [ 19 ] A . Prado , M . Plantevit , C . Robardet , and J F
Boulicaut . Mining graph topological patterns : Finding covariations among vertex descriptors . Knowledge and Data Engineering , IEEE Transactions on , 25(9):2090–2104 , 2013 .
[ 20 ] C . Schellewald and C . Schn¨orr . Probabilistic subgraph matching based on convex relaxation . Springer , 2005 . [ 21 ] R . Singh , J . Xu , and B . Berger . Global alignment of multiple protein interaction networks with application to functional orthology detection . Proceedings of the National Academy of Sciences , 105(35):12763–12768 , 2008 .
[ 22 ] A . Smalter , J . Huan , and G . Lushington . Gpm : A graph pattern matching kernel with diffusion for chemical compound classification . IEEE , 2008 .
[ 23 ] S . Tan , Z . Guan , D . Cai , X . Qin , J . Bu , and C . Chen . Mapping users across networks by manifold alignment on hypergraph . 2014 .
[ 24 ] H . Tong , C . Faloutsos , and J Y Pan . Fast random walk with restart and its applications . 2006 . [ 25 ] S . V . N . Vishwanathan , N . N . Schraudolph ,
R . Kondor , and K . M . Borgwardt . Graph kernels . The Journal of Machine Learning Research , 11:1201–1242 , 2010 .
[ 26 ] J . S . Yedidia , W . T . Freeman , and Y . Weiss .
Understanding belief propagation and its generalizations . Exploring artificial intelligence in the new millennium , 8:236–239 , 2003 .
[ 27 ] R . Zafarani and H . Liu . Connecting users across social media sites : a behavioral modeling approach . In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 41–49 . ACM , 2013 .
[ 28 ] J . Zhang and S . Y . Philip . Multiple anonymized social networks alignment . Network , 3(3):6 , 2015 .
[ 29 ] Y . Zhang . Browser oriented universal cross site recommendation and explanation based on user browsing logs . ACM , 2014 .
[ 30 ] Y . Zhang , J . Tang , Z . Yang , J . Pei , and P . S . Yu . Cosnet : Connecting heterogeneous social networks with local and global consistency . ACM , 2015 .
[ 31 ] E . Zhong , W . Fan , J . Wang , L . Xiao , and Y . Li . Comsoc : adaptive transfer of user behaviors over composite social network . ACM , 2012 .
1354
