Convex Optimization for Linear Query Processing under Approximate Differential Privacy
Ganzhao Yuan1
Yin Yang2
Zhenjie Zhang3
Zhifeng Hao4
1School of Mathematics
South China University of Technology yuanganzhao@gmail.com
2College of Science and Engineering
Hamad Bin Khalifa University yyang@qforgqa
3Advanced Digital Sciences Center
Illinois at Singapore Pte . Ltd . zhenjie@adsccomsg
4School of Mathematics and Big Data
Foshan University mazfhao@scuteducn
ABSTRACT Differential privacy enables organizations to collect accurate aggregates over sensitive data with strong , rigorous guarantees on individuals’ privacy . Previous work has found that under differential privacy , computing multiple correlated aggregates as a batch , using an appropriate strategy , may yield higher accuracy than computing each of them independently . However , finding the best strategy that maximizes result accuracy is non trivial , as it involves solving a complex constrained optimization program that appears to be non convex . Hence , in the past much effort has been devoted in solving this non convex optimization program . Existing approaches include various sophisticated heuristics and expensive numerical solutions . None of them , however , guarantees to find the optimal solution of this optimization problem .
This paper points out that under ( , δ) differential privacy , the optimal solution of the above constrained optimization problem in search of a suitable strategy can be found , rather surprisingly , by solving a simple and elegant convex optimization program . Then , we propose an efficient algorithm based on Newton ’s method , which we prove to always converge to the optimal solution with linear global convergence rate and quadratic local convergence rate . Empirical evaluations demonstrate the accuracy and efficiency of the proposed solution .
1 .
INTRODUCTION
Differential privacy [ 4 , 2 ] is a strong and rigorous privacy protection model that is known for its generality , robustness and effectiveness . It is used , for example , in the ubiquitous Google Chrome browser [ 6 ] . The main idea is to publish randomized aggregate information over sensitive data , with the guarantee that the adversary cannot infer with high confidence the presence or absence of any individual in the dataset from the released aggregates . An important goal in the design of differentially private methods is to maximize the accuracy of the published noisy aggregates with respect to their exact values .
Besides optimizing for specific types of aggregates , an important generic methodology for improving the overall accuracy of the released aggregates under differential privacy is batch processing , first proposed in [ 12 ] . Specifically , batch processing exploits the correlations between multiple queries , so that answering the batch as a whole can lead to higher overall accuracy than answering each query individually . For example , if one aggregate query Q1 ( eg , the total population of New York State and New Jersey ) can be expressed as the sum of two other queries ( the population of New York and New Jersey , respectively ) , ie , Q1 = Q2 + Q3 , then we can simply answer Q1 by adding up the noisy answers of Q2 and Q3 . Intuitively , answering two queries instead of three reduces the amount of random perturbations required to satisfy differential privacy , leading to higher overall accuracy for the batch as a whole [ 12 , 23 ] . In this paper , we focus on answering linear aggregate queries under differential privacy . Given a batch of linear aggregate queries ( called the workload ) , we aim to improve their overall accuracy by answering a different set of queries ( called the strategy ) under differential privacy , and combining their results to obtain the answers to the original workload aggregates .
As shown in [ 12 , 13 , 23 , 24 , 10 ] , different strategy queries lead to different overall accuracy for the original workload . Hence , an important problem in batch processing under differential privacy is to find a suitable strategy that leads to the highest accuracy . Such a strategy can be rather complex , rendering manual construction and brute force search infeasible [ 23 , 24 ] . On the other hand , the problem of strategy searching can be formulated into a constrained optimization program , and it suffices to find the optimal solution of this program . However , as we show later in Section 2 , the program appears to be non convex ; hence , solving it is rather challenging . To the best of our knowledge , existing approaches resort to either heuristics or complex , expensive and unstable numerical methods . To our knowledge , no existing solutions guarantee to find the optimal solution .
This paper points out that under the ( , δ) differential privacy definition ( also called approximate differential privacy , explained in Section 2 ) , the constrained optimization program for finding the best strategy queries can be re formulated into a simple and elegant convex optimization program . Note that although the formulation itself is simple , its derivation is rather complicated and non trivial . Based on this new formulation , we propose the first polynomial solution COA that guarantees to find the optimal solution to the original constrained optimization problem in search of a suitable strategy for processing a batch of arbitrary linear aggregate queries under approximate differential privacy . COA is based on Newton ’s
2005 method and it utilizes various non trivial properties of the problem . We show that COA achieves globally linear and locally quadratic convergence rate . Extensive experiments confirm the effectiveness and efficiency of the proposed method .
The rest of the paper is organized as follows . Section 2 provides necessary background on differential privacy and overviews related work . Section 3 presents our convex programming formulation for batch linear aggregate processing under approximate differential privacy . Section 4 describes the proposed solution COA . Section 5 contains a thorough set of experiments . Section 6 concludes the paper with directions for future work . In this paper , boldfaced lowercase letters denote vectors and uppercase letters denote realvalued matrices . We summarize the frequent notations in Table 1 .
2 . BACKGROUND
Using the notations summarized in Table 1 , a common definition of differential privacy is ( , δ) differential privacy [ 4 ] , as follows : DEFINITION 1 . Two databases D and D are neighboring iff they differ by at most one tuple . A randomized algorithm M satisfies ( , δ) differential privacy iff for any two neighboring databases D and D and any measurable output S in the range of M , we have
Pr[M(D ) ∈ S ] ≤ e · Pr[M(D
) ∈ S ] + δ .
When δ = 0 , the above definition reduces to another popular definition : differential privacy ( also called “ exact differential privacy ” ) . This work focuses on the case where δ > 0 , which is sometimes called approximate differential privacy . Usually , δ is set to a value smaller than 1|D| , where |D| is the number of records in the dataset D . Both exact and approximate definitions of differential privacy provide strong and rigorous privacy protection to the users . Given the output of a differentially private mechanism , the adversary cannot infer with high confidence ( controlled by parameters and δ ) whether the original database is D or any of its neighbors D , which differ from D by one record , meaning that each user can plausibly deny the presence of her tuple . An approximately differentially private mechanism can be understood as satisfying exact differential privacy with a certain probability controlled by parameter δ . Hence , it is a more relaxed definition which is particularly useful when the exact definition is overly strict for an application , leading to poor result utility .
One basic mechanism for enforcing approximate differential privacy is the Gaussian mechanism [ 3 ] , which injects Gaussian noise to the query results calibrated to the 2 sensitivity of the queries . Note that the Gaussian mechanism cannot be applied to exact differential privacy . Since the proposed method is based on the Gaussian mechanism , it is limited to query processing under approximate differential privacy as well . Specifically , for any two neighbor databases D and D , the 2 sensitivity Θ(Q ) of a query set Q is defined as Θ(Q ) = maxD,D Q(D ) , Q(D)2 . Given a database D and a query set Q , the Gaussian mechanism outputs a random result that follows the Gaussian distribution with mean Q(D ) and magnitude σ = Θ(Q)/h( , δ ) , where h( , δ ) = /2 ln(2/δ ) .
This paper focuses on answering a batch of m linear aggregate queries , Q = {q1 , q2 , . . . , qm} , each of which is a linear combination of the unit aggregates of the input database D . For simplicity , in the following we assume that each unit aggregate is a simple count , which has an 2 sensitivity of 1 . Other types of aggregates can be handled by adjusting the sensitivity accordingly . The query set Q can be represented by a workload matrix W ∈ Rm×n with m rows and n columns . Each entry Wij in W is the weight in query
Table 1 : Summary of notations .
Symbol
W m n V X A A†
Definition W ∈ Rm×n , Workload matrix Number of queries ( ie , rows ) in W Unit counts ( ie , columns ) in W V ∈ Rn×n , Covariance matrix of W X ∈ Rn×n , Solution matrix A ∈ Rp×n , Strategy matrix A† ∈ Rn×p , pseudo inverse of matrix A vec(X ) ∈ Rn2×1 , Vectorized listing of X vec(X ) mat(x ) mat(x ) ∈ Rn×n , Convert x ∈ Rn2×1 into a matrix F ( X ) G(X ) H(X ) HX(D ) HX(D ) ∈ Rn×n , Equivalent to mat(H(X)vec(D ) )
F ( X ) ∈ R , Objective value of X G(X ) ∈ Rn×n , Gradient matrix of X H(X ) ∈ Rn2×n2 , Hessian matrix of X
1/0 I X 0 X 0 λ(X )
All one column vector/All zero column vector Identity matrix Matrix X is positive semidefinite Matrix X is positive definite Eigenvalue of X ( increasing order ) Diagonal matrix with x as the main diagonal entries Column vector formed from the main diagonal of X Spectral norm : the largest eigenvalue of X Smallest eigenvalue of X Sum of the elements on the main diagonal X diag(x ) diag(X ) X χ(X ) tr(X ) X , Y X ⊗ Y Kronecker product of X and Y X fi Y Hadamard ( aka entry wise ) product of X and Y X∗ XF XN
Euclidean inner product , ie , X , Y = Frobenius norm : ( ij X2 Generalized vector norm : XN = ( vec(X)T Nvec(X))1/2
Nuclear norm : sum of the singular values of matrix X ij )1/2 ij Xij Yij qi on the j th unit count xj . Since we do not use any other information of the input database D besides the unit counts , in the following we abuse the notation by using D to represent the vector of unit counts . Therefore , we define D x ∈ Rn , Q W ∈ Rm×n ( “ ” means define ) . The query batch Q can be answered directly by :
T
∈ Rm×1
Q(D ) Wx =
W1jxj , . . . ,
Wmjxj j j
Given a workload matrix W , the worse case expected squared error of a mechanism M is defined as [ 12 , 14 , 17 ] : E[M(x ) − Wx2 2 ] err(M ; W ) max x∈Rn where the expectation is taken over the randomness of M . Without information of the underlying dataset , the lowest error achievable by any differentially private mechanism for the query matrix W and database is : opt(W ) = minM err(M ; W )
( 1 ) where the infimum is taken over all differentially private mechanisms . If a mechanism M minimizes the objective value in Eq ( 1 ) , it is the optimal linear counting query processing mechanism , in the sense that without any prior information of the sensitive data , it achieves the lowest expected error .
Matrix Mechanism . The first solution for answering batch linear aggregate queries under differential privacy is the matrix mechanism [ 12 ] . The main idea is that instead of answering the workload queries W directly , the mechanism first answers a different
2006 set of r queries under differential privacy , and then combine their results to answer W . Let matrix A represent the strategy queries , where each row represent a query and each column represent a unit count . Then , according to the Gaussian mechanism , A can be answered using Ax + ˜b under ( , δ) differentially privacy , where ˜b denotes an m dimensional Gaussian random variable with scale
||A||2,∞2 ln(2/δ)/ , and Ap,∞ is the maximum p norm a mong all column vectors of A . Accordingly , the matrix mechanism answers W by :
M(x ) = W(x + A
†˜b )
( 2 ) where A† is the Moore Penrose pseudo inverse of A .
Based on Eq ( 2 ) , Li et al . [ 12 ] formalize the strategy searching problem for batch linear counting query processing in Eq(1 ) into the following nonlinear optimization problem :
A\{0} J(A ) A2 min
† p,∞tr(WA
†T WT ) .
A
( 3 ) min B,L
In the above optimization program , p can be either 1 or 2 , and the method in [ 12 ] applies to both exact and approximate differential privacy . This optimization program , however is rather difficult to solve . The pseudoinverse of A† of A involved in Program ( 3 ) is not a continuous function , as it jumps around when A is illconditioned . Therefore , A† does not have a derivative , and we cannot solve the problem with simple gradient descent . As pointed out in [ 24 ] , the solutions in [ 12 ] are either prohibitively expensive ( which needs to iteratively solve a pair of related semidefinite programs that incurs O(m3n3 ) computational costs ) , or ineffective ( which rarely obtains strategies that outperform naive methods ) .
Low Rank Mechanism . Yuan et al . [ 24 ] propose the Low Rank Mechanism ( LRM ) , which formulates the batch query problem as the following low rank matrix factorization problem : tr(BT B ) st W = BL , Lp,∞ ≤ 1
( 4 ) where B ∈ Rm×r , L ∈ Rr×n . It can be shown that Program ( 4 ) and Program ( 3 ) are equivalent to each other ; hence , LRM can be viewed as a way to solve the Matrix Mechanism optimization program ( to our knowledge , LRM is also the first practical solution for this program ) . The LRM formulation avoids the pseudo inverse of the strategy matrix A ; however , it is still a non linear , non convex constrained optimization program . Hence , it is also difficult to solve . The solution in LRM is a sophisticated numeric method based first order augmented Lagrangian multipliers ( ALM ) . This solution , however , cannot guarantee to find the globally optimal strategy matrix A , due to the non convex nature of the problem formulation . Further , the LRM solution may not converge at all . Specifically , it iteratively updates B using the formula : B ⇐ ( βWLT + πLT )(βLLT + I)−1 , where β is the penalty parameter . When L is low rank , according to the rank inequality for matrix multiplication , it leads to : rank(B ) ≤ rank(L ) . Therefore , the equality constraint W = BL may never hold since we can never express a full rank matrix W with the product of two low rank ones . When this happens , LRM never converges . For this reason , the initial value of L needs to be chosen carefully so that it is not low rank . However , this problem cannot be completed avoided since during the iterations of LRM , the rank of L may drop . Finally , even in cases where LRM does converge , its convergence rate can be slow , leading to high computational costs as we show in the experiments . In particular , the LRM solution is not necessarily a monotone descent algorithm , meaning that the accuracy of its solutions can fluctuate during the iterations .
Adaptive Mechanism . In order to alleviate the computational overhead of the matrix mechanism , adaptive mechanism ( AM ) [ 13 ] considers the following optimization program : min λ∈Rn
, st ( Q fi Q)(λ fi λ ) ≤ 1
( 5 ) d2 i λ2 i where Q ∈ Rm×n is from the singular value decomposition of the workload matrix W = QDP with D ∈ Rn×n , P ∈ Rn×n , and d = diag(D ) ∈ Rn , ie , the diagonal values of D . AM then computes the strategy matrix A by A = Qdiag(λ ) ∈ Rm×n , where diag(λ ) is a diagonal matrix with λ as its diagonal values .
The main drawback of AM is that it searches over a reduced subspace of A , since it is limited to a weighted nonnegative combination of the fixed eigen queries Q . Hence , the candidate strategy matrix A solved from the optimization problem in ( 5 ) is not guaranteed to be the optimal strategy . In fact it is often suboptimal , as shown in the experiments .
Exponential Smoothing Mechanism . Based on a reformulation of matrix mechanism , the Exponential Smoothing Mechanism ( ESM ) [ 23 ] considers solving the following optimization program : ( 6 ) max(diag(X ) ) · tr(WX
−1WT ) st X 0 min
X∈Rn×n n i=1 authors use the soft max function smax(v ) = µ logn where max is a function that retrieves the largest element in a vector . This function is hard to compute since it is non smooth . The µ ) ) to smooth this term and employ the non monotone spectral projected gradient descent for optimizing the non convex but smooth objective function on a positive definiteness constraint set . i ( exp( vi
One major problem with this method is that Program ( 6 ) involves matrix inverse operator , which may cause numerical instability when the final solution ( ie , the strategy matrix ) is of low rank . Further , since the problem is not convex , the ESM solution does not guarantee to converge to the global optimum , either .
The proposed solution , presented next , avoids all the drawbacks of previous solutions : it is fast , stable , numerically robust , and most importantly , it guarantees to find the optimal solution . 3 . A CONVEX PROBLEM FORMULATION This section presents the a convex optimization formulation for finding the best strategy for a given workload of linear aggregate queries . The main idea is that instead of solving for the strategy matrix A that minimizes result error directly , we first solve the optimal value for X = AAT , and then obtain A accordingly . Note that there can be multiple strategy matrices A from a given X = AAT , in which case we simply output an arbitrary one , since they all lead to the same overall accuracy for the original workload W . As we show soon , the objective function with respect to X is convex ; hence , the proposed solution is guaranteed to find the global optimum . The re formulation of the optimization program involves a non trivial semi definite programming lifting technique to remove the quadratic term , presented below .
First of all , based on the non convex model in Program ( 3 ) , we have the following lemma1 .
LEMMA 1 . Given an arbitrary strategy matrix A , we can always construct another strategy A satisfying ( i ) Ap,∞ = 1 and ( ii ) J(A ) = J(A ) , where J(A ) is defined in in Program ( 3 ) . By Lemma 1 , the following optimization program is equivalent to Program ( 3 ) .
A
†
A
†T , WT W , st Ap,∞ = 1
( 7 ) min
A
1All proofs can be found in the full version of the paper [ 22 ] .
2007 This paper focuses on approximate differential privacy where p = 2 . Moreover , we assume that V = WT W is full rank . If this assumption does not hold , we simply transform V into a full rank matrix by adding an identity matrix scaled by θ , where θ approaches zero . Formally , we have :
V = WT W + θI 0 min
F ( X ) = X
( 8 ) Let X = AT A 0 . Using the fact that ( A2,∞)2 = diag(X)∞ and A†A†T = X−1 , we have the following matrix inverse optimization program ( note that X and V are both full rank ) : −1 , V , st diag(X ) ≤ 1 , X 0 .
( 9 ) Interestingly , using the fact that ||X/n|| ≤ tr(X/n ) ≤ 1 , one can approximate the matrix inverse via Neumann Series 2 and rewrite the objective function in terms of matrix polynomials 3 . Although other convex semi definite programming reformulations/relaxations exist ( discussed in the full version of the paper [ 22] ) , we focus on Program ( 9 ) and provide convex analysis below .
X
Convexity of Program ( 9 ) . Observe that the objective function of Program ( 9 ) is not always convex unless some conditions are imposed on V and X . For instance , in the the one dimensional x , case , it reduces to the inversely proportional function f ( x ) = k with k > 0 . Clearly , f ( x ) is convex on the strictly positive space and concave on the strictly negative space .
The following lemma states the convexity of Program ( 9 ) under appropriate conditions .
LEMMA 2 . Assume that X 0 . The function F ( X ) = X−1 , V is convex ( resp . , strictly convex ) if V 0 ( resp . , V 0 ) .
Since V is the covariance matrix of W , V is always positive semidefinite . Therefore , according to the above lemma , the objective function of Program ( 9 ) is convex . Furthermore , since V is strictly positive definite , the objective function F ( X ) is actually strictly convex . Therefore , there exists a unique optimal solution for Program ( 9 ) .
Dual program of Program ( 9 ) . The following lemma describes the dual program of Program ( 9 ) .
LEMMA 3 . The dual program of Program ( 9 ) is the following :
− y , 1 , st Xdiag(y)X − V 0 , X 0 , y ≥ 0 . max X,y where y ∈ Rn is associated with the inequality constraint diag(X ) ≤ 1 .
Lower and upper bounds for Program ( 9 ) . Next we establish a lower bound and an upper bound on the objective function of Program ( 9 ) for any feasible solution .
LEMMA 4 . For any feasible solution X in Program ( 9 ) , its ob jective value is sandwiched as max(2W∗ − n , W2∗/n ) + θ ≤ F ( X ) ≤ ρ2(W2 F + θn ) where ρ = maxi S( : , i)2 , i ∈ [ n ] , and S comes from the SVD decomposition that W = UΣS .
The parameter θ ≥ 0 serves as regularization of the convex problem . When θ > 0 , we always have V 0 . As can be seen in our subsequent analysis , the assumption that V is strictly positive definite is necessary in our algorithm design .
Problem formulation with equality constraints . We next re formulate Program ( 9 ) in the following lemma .
2X−1 =∞ 3F ( X ) = ( X/n)−1 , V/n = ∞ k=0(I − X)k , ∀ X ≤ 1 k=0(I − X/n)k , V/n
LEMMA 5 . Assume V 0 . The optimization problem in Pro gram ( 9 ) is equivalent to the following optimization program :
F ( X ) = X
−1 , V , st diag(X ) = 1 , X 0 .
( 10 ) min
X
Program ( 10 ) is much more attractive than Program ( 9 ) since the equality constraint is easier to handle than the inequality constraint . As can be seen in our algorithm design below , this equality constraint can be explicitly enforced with suitable initialization . Hence , in the rest of the paper , we focus on solving Program ( 10 ) . First order and second order analysis . It is not hard to verify that the first order and second order derivatives of the objective function F ( X ) can be expressed as ( see page 700 in [ 1] ) :
G(X ) = −X
−1VX
H(X ) = −G(X ) ⊗ X
−1 , −1 − X
−1 ⊗ G(X )
( 11 )
Since our method ( described soon ) is a greedy descent algorithm , we restrict our discussions on the level set X which is defined as : X {X|F ( X ) ≤ F ( X0 ) , and diag(X ) = 1 , and X 0} We now analyze bounds for the eigenvalues of the solution in Program ( 10 ) , as well as bounds for the eigenvalues of the Hessian matrix and the gradient matrix of the objective function in Program ( 10 ) . The following lemma shows that the eigenvalues of the solution in Program ( 10 ) are bounded .
LEMMA 6 . For any X ∈ X , there exist some strictly positive constants C1 and C2 such that C1I X C2I where C1 = λ1(V ) − 1 + 1 ( F ( X0 ) The next lemma shows the the eigenvalues of the Hessian matrix and the gradient matrix of the objective function in Program ( 10 ) are also bounded . n )−1 and C2 = n .
LEMMA 7 . For any X ∈ X , there exist some strictly positive constants C3 , C4 , C5 and C6 such that C3I H(X ) C4I and C5I G(X ) C6I , where C3 = λ1(V ) , C5 = 2 ( X )
, C4 = λn(V ) 1 ( X )
, C6 = λn(V ) 1 ( X )
λ1(V ) C2 2 ( X ) A self concordant function [ 16 ] is a function f : R → R for which |f(x)| ≤ 2f(x)3/2 in the affective domain . It is useful in the analysis of Newton ’s method . A self concordant barrier function is used to develop interior point methods for convex optimization .
Self Concordance Property . The following lemma establishes
C2
C3
C3
. the self concordance property of Program ( 10 ) .
.
LEMMA 8 . The objective function ˜F ( X ) = C2
4 · 4 F ( X ) = C2 X−1 , V with X ∈ X is a standard self concordant function , 2 tr(V)−1/2 where C is a strictly positive constant with C 6C3 23/2C3 1 The self concordance plays a crucial role in our algorithm design and convergence analysis . First , self concordance ensures that the current solution is always in the interior of the constraint set X 0 [ 16 ] , which makes it possible for us to design a new Cholesky decomposition based algorithm that can avoid eigenvalue decomposition4 . Second , self concordance controls the rate at which the second derivative of a function changes , and it provides a checkable sufficient condition to ensure that our method converges to the global solution with ( local ) quadratic convergence rate . 4Although Cholesky decomposition and eigenvalue decomposition share the same computational complexity ( O(n3 ) ) for factorizing a positive definite matrix of size n , in practice Cholesky decomposition is often significantly faster than eigenvalue decomposition ( eg by about 50 times for a square matrix of size n = 5000 ) .
2008 Algorithm 1 Algorithm COA for Solving Program ( 10 ) 1 : Input : θ > 0 and X0 such that X0 0 , diag(X0 ) = 1 2 : Output : Xk 3 : Initialize k = 0 4 : while not converge do 5 :
Solve the following subproblem by Algorithm 2 :
Dk ⇐ arg min
∆ f ( ∆ ; Xk ) , st diag(Xk + ∆ ) = 1
( 12 )
6 : 7 : 8 : 9 : 10 : 11 :
Perform step size search to get αk such that :
( 1 ) Xk+1 = Xk + αkDk is positive definite and ( 2 ) there is sufficient decrease in the objective . if Xk is an optimal solution of Program ( 10 ) then terminate and output Xk
Increment k by 1 maximum iteration T ∈ N
Algorithm 2 A Modified Conjugate Gradient for Finding D as in Program ( 15 ) 1 : Input : Z = ( Xk)−1 , and current gradient G = G(Xk ) , Specify the 2 : Output : Newton direction D ∈ Rn×n 3 : D = 0 , R = −G + ZDG + GDZ 4 : Set Dij = 0 , Rij = 0 , ∀i = j , i , j ∈ [ n ] 5 : P = R , rold = R , R 6 : for l = 0 to T do 7 : B = −G + ZDG + GDZ , α = rold P,B 8 : D = D + αP , R = R − αB Set Dij = 0 , Rij = 0 , ∀i = j , i , j ∈ [ n ] 9 : rnew = R , R , P = R + rnew 10 : 11 : return D
P , rold = rnew rold
4 . OPTIMIZATION ALGORITHM
In this section , we provide a Newton like algorithm COA to solve Program ( 10 ) . We first show how to find the search direction and the step size in Sections 4.1 and 4.2 , respectively . Then we study the convergence property of COA in Section 43 Finally , we present a homotopy algorithm to further accelerate the convergence . For notational convenience , we use the shorthand notation F k = F ( Xk ) , Gk = G(Xk ) , Hk = H(Xk ) , and D = D(Xk ) to denote the objective value , first order gradient , hessian matrix and the search direction at the point Xk , respectively .
Following the approach of [ 20 , 9 , 25 ] , we build a quadratic ap proximation around any solution Xk for the objective function F ( X ) by considering its second order Taylor expansion : f ( ∆ ; Xk ) = F k + ∆ , Gk +
1 2 vec(∆)T Hkvec(∆ ) .
( 13 )
Therefore , the Newton direction Dk for the smooth objective functon F ( X ) can then be written as the solution of the following equality constrained quadratic program :
Dk = arg min ∆ f ( ∆ ; Xk ) , st diag(Xk + ∆ ) = 1 ,
( 14 )
After the direction Dk is computed , we employ an Arimijo rule based step size selection to ensure positive definiteness and sufficient descent of the next iterate . We summarize our algorithm COA in Algorithm 1 . Note that the initial point X0 has to be a feasible solution , thus X0 0 and diag(X0 ) = 1 . Moreover , the positive definiteness of all the following iterates Xk will be guaranteed by the step size selection procedure ( refer to step 7 in Algorithm 1 ) . 4.1 Computing the Search Direction This subsection is devoted to finding the search direction in Eq ( 14 ) . With the choice of X0 0 and diag(X0 ) = 1 , Eq(14 ) reduces to the following optimization program : min ∆
∆ , Gk +
1 2 vec(∆)T Hkvec(∆ ) , st diag(∆ ) = 0 ( 15 )
At first glance , Program ( 15 ) is challenging . First , this is a constrained optimization program with n × n variables and n equality constraints . Second , the optimization problem involves computing and storing an n2 × n2 Hessian matrix Hk , which is a daunting task in algorithm design .
We carefully analyze Problem ( 15 ) and propose the following solutions . For the first issue , Eq ( 15 ) is actually a unconstrained quadratic program with n × ( n − 1 ) variable . In order to handle the diagonal variables of ∆ , one can explicitly enforce the diagonal entries of current solution and its gradient to 0 . Therefore , the constraint diag(∆ ) = 0 can always be guaranteed . This implies that linear conjugate gradient method can be used to solve Problem ( 15 ) . For the second issue , we can make good use of the Kronecker product structure of the Hessian matrix . We note that ( A ⊗ B ) vec(C ) = vec(BCA),∀A , B , C ∈ Rn×n . Given a vector vec(D ) ∈ Rn2×1 , using the fact that the Hessian matrix can be computed as H = −G⊗ X−1 − X−1 ⊗ G , the Hessian vector product can be computed efficiently as : Hvec(D ) = vec(−GDX−1 −X−1DG ) , which only involves matrix matrix computation . Our modified linear conjugate gradient method for finding the search direction is summarized in Algorithm 2 . Note that the algorithm involves a parameter T controlling the maximum number of iterations . The specific value of T does not affect the correctness of COA , only its efficiency . Through experiments we found that a value of T = 5 usually leads to good overall efficiency of COA . 4.2 Computing the Step Size After the Newton direction D is found , we need to compute a step size α ∈ ( 0 , 1 ] that ensures positive definiteness of the next iterate X + αD and leads to a sufficient decrease of the objective function . We use Armijo ’s rule and try step size α ∈ {β0 , β1 , } with a constant decrease rate 0 < β < 1 until we find the smallest t ∈ N with α = βt such that X + αD is ( i ) positive definite , and ( ii ) satisfies the following sufficient decrease condition [ 20 ] :
F ( Xk + αkDk ) ≤ F ( Xk ) + αkσGk , Dk ,
( 16 ) where 0 < σ < 05 We choose β = 0.1 and σ = 0.25 in our experiments .
We verify positive definiteness of the solution while computing 3 n3 flops ) . We remark that the Cits Cholesky factorization ( takes 1 holesky factorization dominates the computational cost in the stepsize computations . To reduce the computation cost , we can reuse the Cholesky factor in the previous iteration when evaluating the objective function ( that requires the computation of X−1 ) . The decrease condition in Eq ( 16 ) has been considered in [ 20 ] to ensure that the objective value not only decreases but also decreases by a certain amount αkσGk , Dk , where Gk , Dk measures the optimality of the current solution .
The following lemma provides some theoretical insights of the line search program . It states that a strictly positive step size can always be achieved in Algorithm 1 . This property is crucial in our global convergence analysis of the algorithm .
C1 C7
LEMMA 9 . There exists a strictly positive constant α < min(1 , , C8 ) such that the positive definiteness and sufficient descent conditions ( in step 7 8 of Algorithm 1 ) are satisfied . Here C7 are some constants which are inde and C8 2(1−σ)C3
2λn(V ) C2 1 C3 pendent of the current solution Xk .
C4
2009 The following lemma shows that a full Newton step size will be selected eventually . This is useful for the proof of local quadratic convergence .
Algorithm 1 is used as the initial solution X0 of the next iteration . In this paper , Algorithm 3 starts from a large θ0 = 1 , and it stops when the preferred θ ≤ 10−10 arrives .
LEMMA 10 . If Xk is close enough to global optimal solution C6C2 ) , the line search condition such that Dk ≤ min( 3.24 , ( 2σ+1)2 C2C4 will be satisfied with step size αk = 1 . 4.3 Theoretical Analysis
First , we provide convergence properties of Algorithm 1 . We prove that Algorithm 1 always converges to the global optimum , and then analyze its convergence rate . Our convergence analysis is mainly based on the proximal point gradient method [ 20 , 9 ] for composite function optimization in the literature . Specifically , we have the following results ( proofs appear in the full version [ 22] ) : THEOREM 1 . Global Convergence of Algorithm 1 . Let {Xk} be sequences generated by Algorithm 1 . Then F ( Xk ) is nonincreasing and converges to the global optimal solution .
THEOREM 2 . Global Linear Convergence Rate of Algorithm 1 . Let {Xk} be sequences generated by Algorithm 1 , Then {Xk} converges linearly to the global optimal solution .
THEOREM 3 . Local Quadratic Convergence Rate of Algorithm 1 . Let {Xk} be sequences generated by Algorithm 1 . When Xk is sufficiently close to the global optimal solution , then {Xk} converges quadratically to the global optimal solution .
It is worth mentioning that Algorithm 1 is the first polynomial algorithm for linear query processing under approximate differential privacy with a provable global optimum guarantee .
Next we analyze the time complexity of our algorithm . Assume that COA converges within Ncoa outer iterations ( Algorithm 1 ) and Tcoa inner iterations ( Algorithm 2 ) . Due to the O(n3 ) complexity for Cholesky factorization ( where n is the number of unit counts ) , the total complexity of COA is O(Ncoa · Tcoa · n3 ) . Note that the running time of COA is independent of the number of queries m . In contrast , the current state of the art LRM has time complexity O(Nlrm · Tlrm · min(m , n)2 · ( m + n ) ) ( where Nlrm and Tlrm are the number of outer and inner iterations of LRM , respectively ) , which involves both n and m . Note that ( Ncoa · Tcoa ) in the big O notation is often much smaller than ( Nlrm · Tlrm ) in practice , due to the quadratic convergence rate of COA . According to our experiments , typically COA converges with Ncoa ≤ 10 and Tcoa ≤ 5 . 4.4 A Homotopy Algorithm
In Algorithm 1 , we assume that V is positive definite . If this is not true , one can consider adding a deceasing regularization parameter to the diagonal entries of V . We present a homotopy algorithm for solving Program ( 9 ) with θ approaching 0 in Algorithm 3 .
The homotopy algorithm used in [ 19 , 5 ] have shown the advantages of continuation method in speeding up solving large scale optimization problems . In continuation method , a sequence of optimization problems with deceasing regularization parameter is solved until a sufficiently small value is arrived . The solution of each optimization is used as the warm start for the next iteration .
In Eq ( 8 ) , a smaller θ is always preferred because it results in more accurate approximation of the original optimization in Program ( 9 ) . However , it also implies a slower convergence rate , according to our convergence analysis . Hence the computational cost of our algorithm is high when small θ is selected . In Algorithm 3 , a series of problems with decreasing regularization parameter θ are solved by using Algorithm 1 , and the solution of each run of
Algorithm 3 A Homotopy Algorithm for Solving Eq ( 9 ) with θ approaching 0 . 1 : Input : workload matrix W 2 : Output : X 3 : Specify the maximum iteration T = 10 4 : Initialize X0 = I , θ0 = 1 5 : for i = 0 to T do 6 : 7 :
Apply Algorithm 1 with θi and Xi to obtain Xi+1 θi+1 = θi × 0.1
5 . EXPERIMENTS
This section experimentally evaluates the effectiveness of the proposed convex optimization algorithm COA for linear aggregate processing under approximate differential privacy . We compare COA with six existing methods : Gaussian Mechanism ( GM ) [ 15 ] , Wavelet Mechanism ( WM ) [ 21 ] , Hierarchical Mechanism ( HM ) [ 7 ] , Exponential Smoothing Mechanism ( ESM ) [ 23 , 12 ] , Adaptive Mechanism ( AM ) [ 13 , 12 ] and Low Rank Mechanism ( LRM ) [ 23 , 24 ] . Qardaji et al . [ 18 ] proposed an improved version of HM by carefully selecting the branching factor . Similar to HM , this method focuses on range processing , and there is no guarantee on result quality for general linear aggregates . A detailed experimental comparison with [ 18 ] is left as future work . Moreover , we also compare with a recent hybrid data and workload aware method [ 11 ] which is designed only for range queries and exact differential privacy . Since a previous study [ 24 ] has shown that LRM significantly outperforms MWEM , we do not compare with Exponential Mechanism with Multiplicative Weights update ( MWEM ) . Although the batch query processing problem under approximate differential privacy in Program ( 9 ) can be reformulated as a standard semi definite programming problem which can be solved by interior point solvers , we do not compare with it either since such method requires prohibitively high CPU time and memory consumption even for one single ( Newton ) iteration .
For AM , we employ the Python implementation obtained from the authors’ website : http://csumassedu/~chaoli We use the default stopping criterion provided by the authors . For ESM and LRM , we use the Mablab code provided by the authors : http:// yuanganzhaoweeblycom/ For COA , we implement the algorithm in Matlab ( refer to the Appendix in [ 22 ] ) and only report the results of Algorithm 1 with the parameter θ = 10−3 . We performed all experiments on a desktop PC with an Intel quad core 2.50 GHz CPU and 4GBytes RAM . In each experiment , every algorithm is executed 20 times and the average performance is reported .
Following the experimental settings in [ 24 ] , we use four realworld data sets ( Search Log , Net Trace , Social Network and UCI Adult ) and fours different types of workloads ( WDiscrete , WRange , WMarginal and WRelated ) . In WDiscrete , each entry is a random variable follows the bernoulli distribution ; in WRange , each query sums the unit counts in a range whose start and end points are randomly generated following the uniform distribution . WMarginal contains queries uniformly sampled from the set of all two way marginals . For WRelated , we generate workload matrix by lowrank matrix multiplication [ 24 ] . Moreover , we measure average squared error and computation time of all the methods . Here the average squared error is the average squared 2 distance between the exact query answers and the noisy answers . The privacy parameters are set to = 0.1 , δ = 0.0001 in our experiments for
2010 ( a ) WDiscrete
( b ) WMarginal
( c ) WRange
( d ) WRelated
Figure 1 : Convergence behavior of the proposed convex optimization algorithm ( Algorithm 1 ) .
( a ) Search Log
( b ) Net Trace
( c ) Social Network
( d ) UCI Adult
Figure 2 : Effect of varying domain size n with m = 1024 on workload WDiscrete .
( a ) Search Log
( b ) Net Trace
( c ) Social Network
( d ) UCI Adult
Figure 3 : Effect of varying domain size n with m = 1024 on workload WMarginal .
( a ) Search Log
( b ) Net Trace
( c ) Social Network
( d ) UCI Adult
Figure 4 : Effect of varying domain size n with m = 1024 on workload WRange .
( a ) Search Log
( b ) Net Trace
( c ) Social Network
( d ) UCI Adult
Figure 5 : Effect of varying domain size n with m = 1024 on workload WRelated . all methods , except for DAWA , which has = 0.1 , δ = 0 since it answers queries under exact differential privacy . Convergence Behavior of COA : Firstly , we verify the convergence property of COA using all the datasets on all the workloads . We record the objective value ( ie the expected error ) , the optimality measure ( ie GkF ) and the test error on four datasets at every iteration k and plot these results in Figure 1 .
We make three important observations from these results .
( i ) The objective value and optimality measure decrease monotonically . This is because our method is a greedy descent algorithm . ( ii ) The test errors do not necessarily decrease monotonically but tend to decrease iteratively . This is because we add random gaussian noise to the results and the average squared error is expected to decrease . ( iii ) The objective values stabilize after the 10th iteration , which means that our algorithm has converged , and the decrease of the error is negligible after the 10th iteration . This implies that one may use a looser stopping criterion without sacrificing accuracy . Impact of Varying Number of Unit Counts : We now evaluate the accuracy performance of all mechanisms with varying domain size n from 64 to 8192 , after fixing the number of queries m to 1024 . We report the results of all mechanisms on the 4 different workloads in Figures 2 , 3 , 4 and 5 , respectively . We have the following observations . ( i ) COA obtains comparable results with LRM , the current state of the art . Part of the reason may be that , the random initialization strategy makes LRM avoid undesirable local minima . In addition , COA and LRM achieve the best performance in all settings . Their improvement over the naive GM is over two orders of magnitude , especially when the domain size is large . ( ii ) WM and
123456789101112131415161718190051152253354x 109IterationSquared Error 10−410−2100102104106OptimalityExpected ErrorTest Error on Sea LogTest Error on Net TraTest Error on Soc NetTest Error on UCI AduFrobenius Norm of G12345678910111213141516170246810121416x 108IterationSquared Error 10−610−410−2100102104106OptimalityExpected ErrorTest Error on Sea LogTest Error on Net TraTest Error on Soc NetTest Error on UCI AduFrobenius Norm of G1234567891011121314151617181920012345678x 108IterationSquared Error 10−410−2100102104106OptimalityExpected ErrorTest Error on Sea LogTest Error on Net TraTest Error on Soc NetTest Error on UCI AduFrobenius Norm of G123456789101112131415161718190020406081121416182x 109IterationSquared Error 10−810−610−410−2100102104106OptimalityExpected ErrorTest Error on Sea LogTest Error on Net TraTest Error on Soc NetTest Error on UCI AduFrobenius Norm of G6412825651210242048409681921081091010Domain Size nAvg . Squared ErrorGMWMHMESMLRMCOAAM6412825651210242048409681921081091010Domain Size nAvg . Squared ErrorGMWMHMESMLRMCOAAM6412825651210242048409681921081091010Domain Size nAvg . Squared ErrorGMWMHMESMLRMCOAAM6412825651210242048409681921081091010Domain Size nAvg . Squared ErrorGMWMHMESMLRMCOAAM6412825651210242048409681921081010Domain Size nAvg . Squared ErrorGMWMHMESMLRMCOAAM6412825651210242048409681921081010Domain Size nAvg . Squared ErrorGMWMHMESMLRMCOAAM6412825651210242048409681921081010Domain Size nAvg . Squared ErrorGMWMHMESMLRMCOAAM6412825651210242048409681921081010Domain Size nAvg . Squared ErrorGMWMHMESMLRMCOAAM6412825651210242048409681921071081091010Domain Size nAvg . Squared ErrorGMWMHMESMLRMCOAAM6412825651210242048409681921071081091010Domain Size nAvg . Squared ErrorGMWMHMESMLRMCOAAM6412825651210242048409681921071081091010Domain Size nAvg . Squared ErrorGMWMHMESMLRMCOAAM6412825651210242048409681921071081091010Domain Size nAvg . Squared ErrorGMWMHMESMLRMCOAAM6412825651210242048409681921071081091010Domain Size nAvg . Squared ErrorGMWMHMESMLRMCOAAM6412825651210242048409681921071081091010Domain Size nAvg . Squared ErrorGMWMHMESMLRMCOAAM6412825651210242048409681921071081091010Domain Size nAvg . Squared ErrorGMWMHMESMLRMCOAAM6412825651210242048409681921071081091010Domain Size nAvg . Squared ErrorGMWMHMESMLRMCOAAM2011 ( a ) Search Log
( b ) Net Trace
( c ) Social Network
( d ) UCI Adult
Figure 6 : Effect of varying number of queries m with n = 512 on workload WDiscrete .
( a ) Search Log
( b ) Net Trace
( c ) Social Network
( d ) UCI Adult
Figure 7 : Effect of varying number of queries m with n = 512 on workload WMarginal .
( a ) Search Log
( b ) Net Trace
( c ) Social Network
( d ) UCI Adult
Figure 8 : Effect of varying number of queries m with n = 512 on workload WRange .
( a ) Search Log
( b ) Net Trace
( c ) Social Network
( d ) UCI Adult
Figure 9 : Effect of varying number of queries m with n = 512 on workload WRelated .
HM obtain similar accuracy on WRange and they are comparable to COA and LRM . This is because they are designed for range queries optimization . ( iii ) AM and ESM have similar accuracy and they are usually strictly worse than COA and LRM . Moreover , the accuracy of AM and ESM is rather unstable on workload WMarginal . For ESM , this instability is caused by numerical errors in the matrix inverse operation , which can be high when the final solution matrix is low rank . Finally , AM searches in a reduced subspace for the optimal strategy matrix , leading to suboptimal solutions with unstable quality . Impact of Varying Number of Queries : In part of the section , we test the impact of varying the query set cardinality m from 32 to 8192 with n fixed to 512 . The accuracy results of all mechanisms on the 4 different workloads are reported in Figures 6 , 7 , 8 and 9 . We have the following observations . ( i ) COA and LRM have similar performance and they consistently outperform all the other methods in all test cases . ( ii ) On WDiscrete and WRange workloads , AM and ESM show comparable performance , which is much worse performance than COA and LRM . ( iii ) On WDiscrete , WRange and WRelated workload , WM and HM improve upon the naive Gaussian mechanism ; however , on WMarginal , WM and HM incur higher errors than GM . AM and ESM again exhibit similar performance , which is often better than that of WM , HM , and GM . Impact of Varying Rank of Workload : Past studies [ 23 , 24 ] show that it is possible to reduce the expected error when the workload matrix has low rank . In this set of experiments , we manually control the rank of workload W to verify this claim . Recall that the parameter s determines the size of the matrix C ∈ Rm×s and the size of the matrix A ∈ Rs×n during the generation of the WRelated workload . When C and A contain only independent rows/columns , s is exactly the rank of the workload matrix W = CA . In Figure 10 , we vary s from 0.1×min(m , n ) to 1×min(m , n ) . We observe that both LRM and COA outperform all other methods by at least one order of magnitude . With increasing s , the performance gap gradually closes . Meanwhile , COA ’s performance is again comparable to LRM . Running Time Evaluations : We now demonstrate the efficiency of LRM , ESM and COA for the 4 different types of workloads . Other methods , such as WM and HM , requires negligible time since they are essentially heuristics without complex optimization computations . From our experiments we obtain the following results . ( i ) In Figure 11 , we vary m from 32 to 8192 and fix n to 1024 . COA requires the same running time regardless of the number of queries m , whereas the efficiency of LRM deteriorates with increasing m . ( ii ) In Figure 12 , we vary n from 32 to 8192 and fix m to 1024 . We observe that COA is more efficient than LRM when n is relatively small ( ie , n < 5000 ) . This is mainly because COA converges with much fewer iterations than LRM . Specifically , we found through manual inspection that COA converges within about Ncoa = 10 outer iterations ( refer to Figure 1 ) and Tcoa = 5 inner
326412825651210242048409681921061081010Query Size mAvg . Squared ErrorGMWMHMESMLRMCOAAM326412825651210242048409681921061081010Query Size mAvg . Squared ErrorGMWMHMESMLRMCOAAM326412825651210242048409681921061081010Query Size mAvg . Squared ErrorGMWMHMESMLRMCOAAM326412825651210242048409681921061081010Query Size mAvg . Squared ErrorGMWMHMESMLRMCOAAM326412825651210242048409681921061081010Query Size mAvg . Squared ErrorGMWMHMESMLRMCOAAM326412825651210242048409681921061081010Query Size mAvg . Squared ErrorGMWMHMESMLRMCOAAM326412825651210242048409681921061081010Query Size mAvg . Squared ErrorGMWMHMESMLRMCOAAM326412825651210242048409681921061081010Query Size mAvg . Squared ErrorGMWMHMESMLRMCOAAM32641282565121024204840968192106107108109Query Size mAvg . Squared ErrorGMWMHMESMLRMCOAAM32641282565121024204840968192106107108109Query Size mAvg . Squared ErrorGMWMHMESMLRMCOAAM32641282565121024204840968192106107108109Query Size mAvg . Squared ErrorGMWMHMESMLRMCOAAM32641282565121024204840968192106107108109Query Size mAvg . Squared ErrorGMWMHMESMLRMCOAAM32641282565121024204840968192106108Query Size mAvg . Squared ErrorGMWMHMESMLRMCOAAM32641282565121024204840968192106108Query Size mAvg . Squared ErrorGMWMHMESMLRMCOAAM326412825651210242048409681921061081010Query Size mAvg . Squared ErrorGMWMHMESMLRMCOAAM326412825651210242048409681921061081010Query Size mAvg . Squared ErrorGMWMHMESMLRMCOAAM2012 ( a ) Net Trace
( b ) Search Log
( c ) Social Network
( d ) UCI Adult
Figure 10 : Effect of varying s and fixed m = 1024 , n = 1024 on different datasets .
( a ) WDiscrete
( b ) WMarginal
( c ) WRange
( d ) WRelated
Figure 11 : Running time comparisons with varying m and fixed n = 1024 for different workloads .
( a ) WDiscrete
( b ) WMarginal
( c ) WRange
( d ) WRelated
Figure 12 : Running time comparisons with varying n and fixed m = 1024 for different workloads .
( a ) WDiscrete
( b ) WMarginal
( c ) WRange
( d ) WRelated
Figure 13 : Running time comparisons with varying n and fixed m = 2048 for different workloads .
( a ) Social Network
( b ) Random Uniform
( c ) Social Network
( d ) Random Uniform
Figure 14 : ( a,b ) Effect of varying domain size n with m = 1024 on workload WRange . ( c,d ) Effect of varying number of queries m with n=1024 on workload WRange . iterations ( refer to our Matlab code in the full version [ 22] ) . In contract , LRM often takes about Nlrm = 200 outer iterations and about Tlrm = 50 inner iterations to converge . When n is very large ( eg , when n = 8192 ) and m is relatively small ( 1024 ) , COA may run slower than LRM due to the former ’s cubic runtime complexity with respect to the domain size n . ( iii ) In Figure 13 , we vary n from 32 to 8192 and fix m to a lager value 2048 . We observe that COA is much more efficient than LRM for all values of n . This is because the runtime of COA is independent of m while LRM scale quadratically with min(m , n ) , and COA has quadratic local convergence rate . These results are consistent with the convergence rate analysis and complexity analysis in Section 43 COA vs DAWA : DAWA [ 11 ] targets very different application s compared to the proposed solution COA . In particular , DAWA focuses on range processing under exact ( ie , ) differential privacy , whereas COA addresses arbitrary linear counting queries under approximate ( ie , ( , δ) ) differential privacy . Adapting DAWA to approximate differential privacy is non trivial , because at the core of DAWA lies a dynamic programming algorithm that is specially designed for 1 cost and the Laplace mechanism ( refer to Section 3.2 in [ 11] ) . Further , DAWA replies on certain assumptions of the underlying data , eg , adjacent counts are similar in value , whereas COA is data independent . Hence , their relative performance depends on the choice of parameter δ , as well as the dataset .
We compare COA with different values of δ ranging from 0.01 to 0.00001 against DAWA on workload WRange , since DAWA focus
0102030405060708091107108109rank ratioAvg . Squared ErrorGMWMHMESMLRMCOAAM0102030405060708091107108109rank ratioAvg . Squared ErrorGMWMHMESMLRMCOAAM0102030405060708091107108109rank ratioAvg . Squared ErrorGMWMHMESMLRMCOAAM0102030405060708091107108109rank ratioAvg . Squared ErrorGMWMHMESMLRMCOAAM32641282565121024204840968192020040060080010001200Query Size mTime ( seconds ) ESMLRMCOA32641282565121024204840968192020040060080010001200Query Size mTime ( seconds ) ESMLRMCOA32641282565121024204840968192020040060080010001200Query Size mTime ( seconds ) ESMLRMCOA32641282565121024204840968192020040060080010001200Query Size mTime ( seconds ) ESMLRMCOA12825651210242048409681920100020003000400050006000Domain Size nTime ( seconds ) ESMLRMCOA128256512102420484096819201000200030004000500060007000Domain Size nTime ( seconds ) ESMLRMCOA128256512102420484096819201000200030004000500060007000Domain Size nTime ( seconds ) ESMLRMCOA12825651210242048409681920100020003000400050006000Domain Size nTime ( seconds ) ESMLRMCOA12825651210242048409681920051152x 104Domain Size nTime ( seconds ) ESMLRMCOA128256512102420484096819202000400060008000Domain Size nTime ( seconds ) ESMLRMCOA128256512102420484096819202000400060008000Domain Size nTime ( seconds ) ESMLRMCOA128256512102420484096819202000400060008000Domain Size nTime ( seconds ) ESMLRMCOA641282565121024204840968192107Domain Size nAvg . Squared Error DAWACOA(δ=10−2)COA(δ=10−3)COA(δ=10−4)COA(δ=10−5)641282565121024204840968192107108109Domain Size nAvg . Squared Error DAWACOA(δ=10−2)COA(δ=10−3)COA(δ=10−4)COA(δ=10−5)32641282565121024204840968192106107108Query Size mAvg . Squared Error DAWACOA(δ=10−2)COA(δ=10−3)COA(δ=10−4)COA(δ=10−5)32641282565121024204840968192106107108Query Size mAvg . Squared Error DAWACOA(δ=10−2)COA(δ=10−3)COA(δ=10−4)COA(δ=10−5)2013 es on range queries . For brevity , we only present the experimental result on a real dataset ( Social Network ) and a synthetic one called Random Uniform . Specifically , the sensitive data Random Uniform consists of a random vector x ∈ Rn drawn from the uniform distribution with mean zero and variance 10 . Experimental comparisons of COA and DAWA on other datasets can be found in the full version [ 22 ] . Figure 14 shows the results with varying domain size n and the number of queries m . We have the following observations . ( i ) On the Random Uniform dataset , the performance of DAWA is rather poor , since this synthetic dataset does not satisfy the assumption that adjacent aggregates have similar values . ( ii ) COA generally achieves better performance than DAWA when δ ≥ 00001 ( iii ) With a fixed number of queries m = 1024 , COA significantly outperforms DAWA when n is large . ( iv ) DAWA outperforms COA only when δ is very small , and the dataset happens to satisfy its assumptions . In such situations , one potential way to improve COA is to incorporate data dependent information through a postprocessing technique ( eg , [ 8 , 10] ) , which is outside of the scope of this paper and left as future work .
6 . CONCLUSION AND FUTURE WORK
In this paper we introduce a convex re formulation for optimizing batch linear aggregate queries under approximate differential privacy . We provide a systematic analysis of the resulting convex optimization problem . In order to solve the convex problem , we propose a Newton like method , which is guaranteed to achieve globally linear convergence rate and locally quadratic convergence rate . Extensive experiment on real world data sets demonstrate that our method is efficient and effective .
There are several interesting directions for future research . Firstly , it is worthwhile to extend the proposed method to develop hybrid data and workload aware differentially private algorithms [ 11 , 10 ] . Secondly , it is interesting to investigate convex relaxations/reformulations to handle the squared/absolute sum error under differential privacy . Acknowledgments We would like to thank Prof . Shaohua Pan ( SCUT ) for her helpful discussions and the reviewers for their valuable comments . This research is partly supported by the research grant for the Human Centered Cyber physical Systems Programme at the Advanced Digital Sciences Center from Singapore ’s A*STAR . This research is also supported in part by Postdoctoral Science Foundation of China ( 2015M572317 ) , Fundamental Research Funds for Central Universities , NSFC of Guangdong Joint Found ( U1501254 ) , NSF of China ( 61402182 , 61472089 and 61572143 ) , NSF of Guangdong ( 2014A030306004 and 2014A030308008 ) , and Science and Technology Planning Project of Guangdong ( 2013B051000076 , 2015B010108006 and 2015B010131015 ) . 7 . REFERENCES [ 1 ] J . Dattorro . Convex Optimization & Euclidean Distance
Geometry . Meboo Publishing USA , 2011 .
[ 2 ] C . Dwork . A firm foundation for private data analysis .
Communications of the ACM , 54(1):86–95 , 2011 .
[ 3 ] C . Dwork , K . Kenthapadi , F . McSherry , I . Mironov , and
M . Naor . Our data , ourselves : Privacy via distributed noise generation . In EuroCRYPT , pages 486–503 , 2006 . [ 4 ] C . Dwork , F . McSherry , K . Nissim , and A . Smith .
Calibrating noise to sensitivity in private data analysis . In TCC , pages 265–284 , 2006 .
[ 5 ] B . Efron , T . Hastie , I . Johnstone , and R . Tibshirani . Least angle regression . Annals of Statistics , 32:407–499 , 2002 .
[ 6 ] Ú . Erlingsson , V . Pihur , and A . Korolova . Rappor :
Randomized aggregatable privacy preserving ordinal response . In CCS , 2014 .
[ 7 ] M . Hay , V . Rastogi , G . Miklau , and D . Suciu . Boosting the accuracy of differentially private histograms through consistency . PVLDB , 3(1):1021–1032 , 2010 .
[ 8 ] J . Lee , Y . Wang , and D . Kifer . Maximum likelihood postprocessing for differential privacy under consistency constraints . In SIGKDD , pages 635–644 , 2015 . [ 9 ] J . D . Lee , Y . Sun , and M . A . Saunders . Proximal newton type methods for minimizing composite functions . SIAM Journal on Optimization , 24(3):1420–1443 , 2014 .
[ 10 ] C . Li . Optimizing linear queries under differential privacy .
PhD Thesis , University of Massachusetts , 2013 .
[ 11 ] C . Li , M . Hay , G . Miklau , and Y . Wang . A data and workload aware algorithm for range queries under differential privacy . PVLDB , 7(5):341–352 , 2014 .
[ 12 ] C . Li , M . Hay , V . Rastogi , G . Miklau , and A . McGregor .
Optimizing linear counting queries under differential privacy . In PODS , pages 123–134 , 2010 .
[ 13 ] C . Li and G . Miklau . An adaptive mechanism for accurate query answering under differential privacy . PVLDB , 5(6):514–525 , 2012 .
[ 14 ] C . Li and G . Miklau . Optimal error of query sets under the differentially private matrix mechanism . In ICDT , pages 272–283 , 2013 .
[ 15 ] F . McSherry and I . Mironov . Differentially private recommender systems : Building privacy into the netflix prize contenders . In SIGKDD , pages 627–636 , 2009 .
[ 16 ] Y . Nesterov and A . Nemirovski . Interior point Polynomial Algorithms in Convex Programming . Society for Industrial Mathematics , 1994 .
[ 17 ] A . Nikolov , K . Talwar , and L . Zhang . The geometry of differential privacy : the sparse and approximate cases . In STOC , pages 351–360 , 2013 .
[ 18 ] W . Qardaji , W . Yang , and N . Li . Understanding hierarchical methods for differentially private histograms . PVLDB , 6(14):1954–1965 , 2013 .
[ 19 ] R . Tibshirani . Regression shrinkage and selection via the lasso . Journal of the Royal Statistical Society , Series B , 58:267–288 , 1994 .
[ 20 ] P . Tseng and S . Yun . A coordinate gradient descent method for nonsmooth separable minimization . Mathematical Programming , 117(1 2):387–423 , 2009 .
[ 21 ] X . Xiao , G . Wang , and J . Gehrke . Differential privacy via wavelet transforms . In ICDE , pages 225–236 , 2010 .
[ 22 ] G . Yuan , Y . Yang , Z . Zhang , and Z . Hao . Convex optimization for linear query processing under approximate differential privacy . arXiv preprint , url : http://arxivorg/abs/160204302 , 2016 .
[ 23 ] G . Yuan , Z . Zhang , M . Winslett , X . Xiao , Y . Yang , and
Z . Hao . Low rank mechanism : Optimizing batch queries under differential privacy . PVLDB , 5(11):1352–1363 , 2012 .
[ 24 ] G . Yuan , Z . Zhang , M . Winslett , X . Xiao , Y . Yang , and Z . Hao . Optimizing batch linear queries under exact and approximate differential privacy . ACM Transactions on Database Systems , 40(2):11 , 2015 .
[ 25 ] S . Yun , P . Tseng , and K . Toh . A block coordinate gradient descent method for regularized convex separable optimization and covariance selection . Mathematical Programming , 129(2):331–355 , 2011 .
2014
