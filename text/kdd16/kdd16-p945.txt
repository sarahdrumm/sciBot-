CatchTartan : Representing and Summarizing Dynamic
Multicontextual Behaviors
Meng Jiang1 , Christos Faloutsos2 , Jiawei Han1
1Department of Computer Science , University of Illinois Urbana Champaign , IL , USA
2Computer Science Department , Carnegie Mellon University , PA , USA mjiang89@illinois.edu , christos@cscmuedu , hanj@illinois.edu
ABSTRACT Representing and summarizing human behaviors with rich contexts facilitates behavioral sciences and user oriented services . Traditional behavioral modeling represents a behavior as a tuple in which each element is one contextual factor of one type , and the tensorbased summaries look for high order dense blocks by clustering the values ( including timestamps ) in each dimension . However , the human behaviors are multicontextual and dynamic : ( 1 ) each behavior takes place within multiple contexts in a few dimensions , which requires the representation to enable non value and set values for each dimension ; ( 2 ) many behavior collections , such as tweets or papers , evolve over time . In this paper , we represent the behavioral data as a two level matrix ( temporal behaviors by dimensionalvalues ) and propose a novel representation for behavioral summary called Tartan that includes a set of dimensions , the values in each dimension , a list of consecutive time slices and the behaviors in each slice . We further develop a propagation method CATCHTARTAN to catch the dynamic multicontextual patterns from the temporal multidimensional data in a principled and scalable way : it determines the meaningfulness of updating every element in the Tartan by minimizing the encoding cost in a compression manner . CATCHTARTAN outperforms the baselines on both the accuracy and speed . We apply CATCHTARTAN to four Twitter datasets up to 10 million tweets and the DBLP data , providing comprehensive summaries for the events , human life and scientific development . Categories and Subject Descriptors H35 [ Information Systems ] : Information Storage and Retrieval On line Information Services ; J.4 [ Computer Applications ] : Social and Behavioral Sciences Keywords Behavior Representation ; Behavior Summarization ; Minimum Description Length 1 .
INTRODUCTION
Behavioral representation and summarization is a fundamental component of behavioral scientific discovery : it supports the systematic analysis and investigation of human behaviors . It is also a
( a ) Representing a behavior with dimensional values including nonvalue and set values , instead of one guaranteed value .
( b ) Tartans in a “ two level matrix ” : dimensions and values on the columns , time slices and behaviors on the rows . Figure 1 : The representation and summarization of dynamic multicontextual behaviors : it takes every behavior while the tensor fails . fundamental problem in many user oriented applications for a better understanding of the event from news , human life from tweets , and the scientific development from publications . However , it is rather challenging for the following two characteristics of the behaviors [ 10 , 4 ] . ( Terms and their definitions are given in Table 1 . ) First , human behaviors are multicontextual : a behavior consists of one or multiple types ( ie , dimensions ) of contextual factors [ 12 ] , and it has one or multiple values in each dimension . Take the “ Super Bowl ” tweet in Figure 1a as an example : it has several dimensions such as the user , phrase , hashtag and shorten URL , and this behavior has one user , one hashtag , several phrases and no URL . The publishing paper behavior also has multiple values in the author , keyword and cited paper dimensions . The representation should enable different combinations of the dimensions and a non value/set value setting of the dimensional values .
Second , human behaviors are dynamic . They naturally evolve with the changing of personality , physical environment and so
Tweetingbehavior(Twitter)20:03:09@ebekahwsm:this better be the best halftime show ever in the history of halftimes shows . ever . #SuperBowlTimesliceUserLocationPhraseHashtagURL20:00 20:30@ebekahwsm∅{besthalftimeshow,inthehistory,halftimesshows}{#SuperBowl}∅TimesliceAuthorVenueKeywordCitedpapers2009{PMelville,WGryc,RLawrence}SIGKDD{sentimentanalysis,lexicalknowledge,textclassification}{p81623 , p84395 , p95393 , p95409 , p99073 , p116349 …}Publishing paperbehavior(DBLP)SIGKDD2009 “ Sentiment analysis of blogs by combining lexical knowledge with text classification ” UserPhraseURLLoc.Hashtag……………11111121…1…111…1…1…1……1…1…1……1…20…1……1111…1…111…11…111…1………………1…22…1……1111…………… “ User Phrase URL ” Tartan(Advertisingcampaign ) “ Phrase Location Hashtag ” Tartan(Localevent)Timeslicett+1t+2Behavior(tweeting)Multicontextual(dimensions,dimensionalvalues)Dynamic(consecutivetimeslices)945 Figure 2 : CATCHTARTAN captures multiple phases ( eg , score prediction , half time show ) in the “ Super Bowl 2013 ” event by representing the dynamic and multicontextual patterns with “ Tartans ” ( consecutive time slices , a set of dimensions and dimensional value sets ) .
Term Dimension
Definition The type of a contextual factor ( eg , location , phrase ; author , keyword ) ( Dimensional ) value The contextual factor in the dimension Time slice Behavior
The period for consecutive behaviors A set of dimensions , a set of values in each dimension , a time slice for the timestamp
Table 1 : Terms used throughout the paper and their definitions . cial interaction [ 34 ] . For example , the crowds predicted the score before the Super Bowl , commented the singers and bands at the half time show , and expressed their happiness or sadness after the match . Therefore , the representation should make the behaviors sortable by their time dimensional values ( ie , timestamps ) , while the other dimensions are not required to be compared .
Traditional behavior modeling used the “ tensor ” [ 15 , 30 , 11 , 9 ] to represent the multidimensional behaviors and proposed a great line of block detection methods [ 5 , 17 , 24 ] to capture the dense blocks as interesting patterns . Why not Tensor ? FEMA [ 11 ] and CROSSSPOT [ 9 ] represented the tweets as ( user , phrase , hashtag , URL ) tuples and used the 4mode tensor to define the tweet data . However , when the tweet has neither a hashtag nor a URL , it either has to be moved out or individually creates a 2 mode dense block . Therefore , the tensor representation either loses a large amount of such information or overweights the meaningfulness of the tweet . Why not Block Detection ? SVD and tensor decompositions have been widely used for multidimensional clustering , subgraph mining and community/block detection [ 17 , 24 , 9 ] . However , they mix all the values into one dimension even including the timestamp values . Their blocks cannot select the meaningful dimensions ; the grouped timestamps cannot capture the dynamic patterns .
In this paper , we propose novel representations for the behav iors and summaries ( see Figure 1b ) : a “ two level matrix ” for the behaviors in which the columns are the dimensional values of the contextual types , and the rows are the behaviors in the time slices ; a “ Tartan ” for the behavioral summary that includes ( 1 ) a set of meaningful dimensions , the meaningful values in each dimension to define the multicontextual patterns ; and ( 2 ) a list of consecutive time slices and the representative behaviors in each slice to define the dynamic patterns . To address the problem of catching the Tartans ( ie , summarizing the behavioral data ) , we propose a propagation method called CATCHTARTAN that defines the meaningfulness metric of including or excluding a value , a dimension , a behavior and a time slice by leveraging the Minimum Description Length ( MDL ) principle . The general philosophy is that saving more bits in compression indicates a more important element in the Tartan . Moreover , CATCHTARTAN is carefully developed with several desired properties : it requires no user defined parameters , runs in parallel and adapts to the dynamic environment .
Figure 2 shows seven of the Tartans that CATCHTARTAN catches from tweets about the “ Super Bowl 2013 ” event . They summarize its five phases such as the score prediction , first half , half time show , second half and sentiments after a win/loss . The Tartans consist of different numbers of dimensions from 1 ( “ Phrase ” ) to 3 ( “ Location Phrase Hashtag ” , “ Phrase RT@User URL ” ) and different consecutive time slices from 5pm , 8pm to 10pm , indicating the advertising campaigns , local trends and topical discussions . It is worthwhile to highlight our contributions as follows . • The Tartan concept : we propose a novel representation for behavioral summary to capture the dynamic and multicontextual patterns . It enables the non value/set values , the temporal ordering of behaviors and the dimension selectivity . • Scalable , parameter free algorithm : we propose a scalable and parameter free method CATCHTARTAN for behavioral summarization , iteratively updating the Tartans with an
16:3017:0017:3018:0018:3019:0019:3020:0020:3021:0021:3022:0016:30:31My predictionRavens 34 Niners3116:30:57Ready for the big game :D , my prediction24 20 SF #SuperBowl16:31:14My predictionfor superbowl 48 Jets over Bears 17 13 Mark Sanchez MVP16:32:24I predictBaltimore Ravenswill win 27 to 24 or 25 or 26 . Basically it will be a close game.userphrasehashtagURL3,397tweets(3,325)226(0)(0)17:30:51RT @LMAOTWlTPICS : Make Your Prediction . RetweetFor 49ershttp://t.co/KKksEist17:31:01RT @LMAOTWlTPICS : Make Your Prediction . RetweetFor 49ershttp://t.co/KKksEist17:31:16RT @LMAOTWlTPICS : Make Your Prediction . RetweetFor 49ershttp://t.co/KKksEist17:31:19RT @LMAOTWlTPICS : Make Your Prediction . RetweetFor 49ershttp://t.co/KKksEistuserphraseRT@userURL196tweets(196)41118:55:03RT @49ers : Kaepernickis sacked on 3rd and goal . #49ers K David Akersmakes 36 yard FG . Baltimoreleads 7 3 with 3:58 left in 1st Qtr . #SB4718:55:04RT @49ers : Kaepernickis sacked on 3rd and goal . #49ers K David Akersmakes 36 yard FG.Baltimoreleads 7 3 with 3:58 left in 1st Qtr . #SB4718:55:44RT @Ravens : David Akersis good from 36 yards to make the score 7 3 Ravens . Nice jobby the defenseto tighten up in the red zone.userphraseRT@userURL215tweets(213)213(0)20:20:01RT @ExtraGrumpyCat : No Superbowlhalftime show will ever surpass this . http://t.co/0VSy7Cv620:20:02RT @WolfpackAlan : No Superbowlhalftime showwill ever surpass this . http://t.co/6BlloPXs20:20:04RT @ExtraGrumpyCat : No Superbowlhalftime show will ever surpass this . http://t.co/0VSy7Cv620:20:05RT @WolfpackAlan : No Superbowlhalftime showwill eversurpass this . http://t.co/6BlloPXsuserphraseRT@userURL617tweets(617)114420:20:47(Manhattan , NY)and every one of those girlstook #ballet #Beyonce#superbowl20:22:01(New York , NY)I have the biggest lady boner for Beyonce#BeyonceBowl#DestinyBowl#DestinysChild#SuperBowl20:24:32 ( Manhattan , NY ) No one can ever top that performance by BeyonceEVER #Beyonce#superbowl#halftimeshowlocationphrasehashtagURL166tweets25517(0 ) “ myprediction ” “ 7 3 ” , “ 1stQtr ” “ beyonce ” ,#beyonce,#superbowl,#DestinysChild21:44:42Ahorasi pff#49ers23 28#Ravens21:44:44Baltimore #Ravens28 23San Francisco #49ers21:44:50FG Akers #49ers23 28#Ravens 3Q 3:10 #SuperBolwXLVII#SuperBowl#NFLuserphrasehashtagURL653tweets(650)6911(0 ) “ 28 23 ” ,#49ers,#Ravens22:42:27 CongratulationsRavens!!!!22:42:43CongratulationsRay Lewis and the Ravens.22:42:43Game over.! Ravenswon raygot his retirement ringnow all y'all boys and girlsgo to sleep .!22:42:52 “ @LetThatBoyTweet : Game over . Ravenswin the Super Bowl . ” userphrasehashtagURL1,950tweets(1942)248(0)(0 ) “ congratulations ” , “ gameover ” Tartan#1:(1dim)16:30 17:30 “ makeyourprediction ” halftimeshow ” Tartan#2:(3dims)17:00 18:00Tartan#3:(2dims)18:30 19:30Tartan#4:(3dims)20:00 21:00Tartan#5:(3dims)20:00 21:00Tartan#6:(2dims)21:00 22:00Tartan#7:(1dim)22:00 23:30946 FSG GRAPH[20 ] CUBE [ 35 ]
EVENT MDC [ 22 ] CUBE [ 30 ]
BOW FEMA COM2 [ 7 ] [ 2 ]
[ 11 ]
CATCHTIMECRUNCH TARTAN
SCOPE [ 28 ] √ √
CROSS GRAPH VOG [ 18 ] SPOT [ 9 ] √ √ √ √ √ √
√ √
√ √ √ √
[ 27 ] √ √
√ √
√ √ √ √ √ √
√
√
√ √
√ √
√ √
√ √
Principled scoring Parameter free Multidimensional Multicontextual Timestamp value Dynamics Table 2 : Feature based comparison of CATCHTARTAN with alternative approaches : it gives a straight line of checks ( blanks for “ × ” ) . Dataset Time Period 113 days NYC14 113 days LA14 25 half hours SPB13 52 half hours GRM13 Dataset Time Period DBLP 35 years Table 3 : Four Twitter datasets ( New York 2014 , Los Angeles 2014 , Super Bowl 2013 and Grammy Awards 2013 ) and the DBLP data .
#URL # RT @User 24,439 795 284,647 235,097
#Hashtag 587,527 24,711 105,473 81,582 #Cited paper 62,710
#Tweet 10,111,725 402,036 2,072,402 2,606,933 #Paper 112,157
#Phrase 1,082,463 257,301 416,461 433,548 #Keyword 33,285
#User 329,779 14,949 1,456,992 1,457,664 #Author 117,934
#Loc 690 55 9,306 5,750 #Venue 55
#@User 955,764 42,951 223,261 160,184
2,766,557 76,950 140,874 334,707 information theoretically principled metric that defines the meaningfulness of including or excluding any element . • Effectiveness : we evaluate the scalable CATCHTARTAN on synthetic data , four Twitter datasets and the DBLP data . We show both quantitative and qualitative results : CATCHTARTAN provides comprehensive behavioral summaries .
2 . RELATED WORK
Traditional approaches model behaviors in three ways : graphs , tensors/cubes and multidimensional itemsets . However , none of the above can represent the dynamic multicontextual patterns in the human behavioral data . Table 2 gives a visual feature based comparison of CATCHTARTAN with the existing methods . Graph data summarization . Graph is common to represent the binary relations inside human behaviors . GRAPHSCOPE [ 28 ] uses graph search for hard partitioning of temporal graphs to find dense temporal cliques and bipartite cores . VOG [ 18 ] and TIMECRUNCH [ 27 ] use MDL to label subgraphs in terms of stars , ( near ) cliques , ( near ) bipartite cores and chains : the former approach works on static graphs , while the latter focuses on dynamic graphs . SLASHBURN [ 16 ] is a recursive node reordering approach to leverage runlength encoding for graph compression . Toivonen et al . [ 31 ] uses structural equivalence to collapse nodes/edges to simplify graph representation . These approaches work on flat representations , while the behavioral dataset itself is naturally multidimensional . Tensor decomposition and cube analysis . Tensor decompositions [ 29 , 17 , 11 ] conduct multidimensional analysis ; COM2 [ 2 ] uses CP/PARAFAC tensor decomposition with MDL . However , the tensor has a big flaw : it has to drop the behaviors in which some dimension is missing . On the cube side , TOPICCUBE [ 15 ] proposes a topic concept cube that supports online multidimensional mining of query log . GRAPHCUBE [ 35 ] defines analysis cubes and OLAP operations on cubes over graphs . EVENTCUBE [ 30 ] performs multidimensional search and analysis of large collections of free text . Our CATCHTARTAN proposes a totally different representation for the behaviors and it has a principled scoring function to select the dimensions for the summaries . Frequent pattern mining and multidimensional clustering . We can adopt the concept of itemsets in both the frequent pattern mining [ 20 , 8 ] and multidimensional data clustering [ 23 , 14 , 1 , 19 ] to represent the behavioral contexts . F . Cordeiro et al . [ 7 ] proposes BOW method for clustering very large and multidimensional datasets with MAPREDUCE . However , the mixture of the dimensional values ( itemsets in the above methods ) kills the selectivity of meaningful dimensions and thus fails to describe the multicontextual patterns . The timestamp clustering cannot describe the dynamic patterns either . MDL theory and applications . Rissanen [ 25 ] proposes optimal encoding for integers greater than or equal to 1 , which minimizes the description length one obtains estimates of the integer valued structure parameters . Cilibrasi et al . [ 3 ] proposes a hierarchical clustering method on compression using the non computable notion of Kolmogorov complexity . Faloutsos et al . [ 6 ] demonstrates that compression and Kolmogorov complexity can measure structure and order . The MDL principle aims to be a practical version of Kolmogorov Complexity [ 21 ] . Vreeken et al . [ 32 ] uses the MDL principle to catch large groups of patterns essentially describing the same set of transactions in the data .
To summarize , our CATCHTARTAN is unique for its ( 1 ) novel representations for behaviors and summaries to capture dynamic multicontextual patterns ; ( 2 ) principled scoring function with no user defined parameters ; and ( 3 ) scalable propagation algorithm .
3 . BEHAVIORAL REPRESENTATION AND
SUMMARIZATION
In this section , we first introduce several datasets of behaviors and preliminarily analyze the multicontextual characteristic . Then we propose our representations for the behaviors and summaries , following by the problem definition of behavioral summarization . 3.1 The Multicontextual Behaviors Datasets . We use four large Twitter datasets as well as the DBLP data ( see Table 3 ) . The tweets were collected from different sources : ( 1 ) NYC14 and LA14 were crawled using Twitter Streaming API1 from August 1st to November 30th 2014 . The NYC14 dataset consists of 10 million tweets in New York and the LA14 consists of 0.4 million in the Greater Los Angeles Area . ( 2 ) SPB13 ( Super Bowl 2013 ) and GRM13 ( the Grammy Awards 2013 ) were collected by Techtunk2 , each of which has over 2 million tweets . We extract many hard encoded dimensions such as location , hashtag , URL ,
1https://devtwittercom/streaming/overview 2http://wwwtechtunkcom/
947 Symbol Definition
X The “ two level ” matrix : the behavioral data A The Tartan : the behavioral summary X A The first level submatrix of X that includes A D Number of dimensions in the data T Number of time slices in the data d The dimension index t The time slice index
The consecutive time slices in A : [ tstart,tend ] The set of values on the d th dimension in A The set of behaviors at the t th time slice in A
The size of the d th dimension The size of the t th time slice The volume of X A
Nd E(t ) V C The sum of non zero entries in X A D The set of dimensions in A T Vd B(t ) DA The number of dimensions in A T A The number of time slices in A nd e(t ) v c The sum of non zero entries in A
The number of values on the d th dimension in A The number of behaviors at the t th time slice in A The volume of the Tartan A
Table 4 : Symbols and their definitions .
3.3 The Behavioral Summarization Problem In this paper , the ultimate goal is to summarize the behaviors , in other words , to find the behavioral summaries in the temporal multidimensional data . With the above representations for the behavior and summary , we define the problem of behavioral summarization as follows , equally as catching Tartans in the two level matrix .
PROBLEM 1
( BEHAVIORAL SUMMARIZATION ) . Given the bet=1} , havioral data ( a two level matrix ) X = {D , Nd|D find a list of behavioral summaries ( Tartans ) ˜A = { . . . ,A , . . .} ordered by a principled metric function f ( A,X ) which defines how well the sets of meaningful dimensions , values , time slices and behaviors are partitioned and how well the meaningful subset of data is summarized , where A = {D,Vd|d∈D,T ,B(t)|t∈T } . d=1 , T , E(t)|T
Good summarization including good partitions will be determined in an information theoretic manner . We would like to emphasize that we solve the problem in a parameter free and scalable way .
4 . PROPOSED METHOD : CATCHTARTAN Our CATCHTARTAN method is based on the Minimum Descrip tion Length ( MDL ) principle and employs a lossless encoding scheme for the temporal multidimensional data . Our objective function estimates the number of bits that encoding the Tartan can save from merging this meaningful knowledge into the data . In this section , we will address the proposed problem in Section 3 by answering three questions : ( 1 ) how to derive the cost of encoding the Tartan ; ( 2 ) how to define the principled scoring function for optimization ; ( 3 ) how to develop a scalable algorithm to catch the Tartans .
Encoding the Tartan .
Figure 4 illustrates the MDL based scheme for encoding the five components of the Tartan . The components are ( 1 ) first level columns ( dimensions ) , ( 2 ) second level columns ( dimensional values ) , ( 3 ) first level rows ( time slices ) , ( 4 ) second level rows ( behaviors ) , and ( 5 ) the behavior value entries . Encoding the dimensions . Suppose D = 5 and the set of dimensions in the Tartan A is D = {1 , 2 , 4} ( DA = 3 ) , the binary string
Figure 3 : The distribution of #dimensions in human behaviors .
@User , as well as the rich phrase dimension . For the DBLP data , we have the author , venue , keyword and cited paper dimensions . Dimension distributions . Given the number of dimensions in a behavior , Figure 3 shows the percentage of the behaviors of that many dimensions in the datasets . The most frequent number in all the datasets is 3 : ( User , Phrase , Location/Hashtag/URL ) in the Twitter data , and ( Author , Venue , Keyword ) in the DBLP . The behaviors are allowed to have various dimensions and for each dimension , they are allowed to have multiple values ( a few phrases or a few keywords ) . A specific behavioral intention shares a set of specific contextual factors and creates a pattern of specific dimensions and values . For example , advertisers often generate tweets of similar phrases and the same URL ; local events often share a group of hashtags and phrases . This is so called “ multicontextual ” . 3.2 “ Two level Matrix ” and the Tartan
Now we know that the behavioral data include temporal and contextual information , because every behavior has its timestamp and a set of contexts , or called dimensional values . On the contextual side , suppose the data have D dimensions , and for each dimension d ∈ [ 1 , D ] , there are Nd values . On the temporal side , suppose the data can be divided into T time slices , and for each time slice t ∈ [ 1 , T ] , there are E(t ) behaviors . The symbols are their definitions are given in Table 4 .
Figure 1b has illustrated our proposed “ two level matrix ” to rep
DEFINITION 1 resent human behaviors . The formal definition is as follows .
A two level matrix X consists ofD values ) andT
( TWO LEVEL MATRIX ( BEHAVIORAL DATA) ) . d=1 Nd columns ( dimensional d ( b , i ) denotes how many times the i th value in the d th dimension appears in the b th behavior at the t th time slice . The top level consists of D dimensions and T time slices . t=1 E(t ) rows ( behaviors ) , in which X ( t )
Note that our definition can represent any dimensional setting and any type of values including non value and set values . A behavioral summary is a subset of the data that creates a representative pattern . Specifically , the definition is as follows .
( TARTAN ( BEHAVIORAL SUMMARY) ) . A be
DEFINITION 2 havioral summary A has five components : • a set of dimensions D ⊆ {1 , . . . , D} ; • a set of values Vd ⊆ {1 , . . . , Nd} in the dimension d ∈ D ; • a list of consecutive time slices T = [ tstart , tend ] ⊆ [ 1 , T ] ; • a set of behavior entries B(t ) ⊆ {1 , . . . , E(t)} in the time • the behavior value entries {X ( t ) d ( b , i)|d ∈ D , t ∈ T , b ∈ The size of the first four components are denoted by 1 ≤ DA ≤ D , 1 ≤ nd ≤ Nd , 1 ≤ T A ≤ T and 1 ≤ e(t ) ≤ E(t ) . slice t ∈ T ; B(t ) , i ∈ B(t)}
As shown in Figure 1b and 4 , the Tartan is named after its par ticular shape in the two level matrix .
0%10%20%30%40%50%123456PercentageofbehaviorsNumberofdimensionsinabehaviorNYC14LA14SPB13GRM13DBLP948 Figure 4 : Encoding the 5 components when encoding a Tartan .
( a ) Update the set of behaviors .
( b ) Update the set of values . to encode the set is 11010 . The length of this string is D and the number of 1s is DA . To further save space , we can adopt Huffman coding or arithmetic coding to encode the binary string , which formally can be viewed as a sequence of realizations of a binomial random variable X . We denote by HD(X ) the entropy :
HD(X ) = − = − , DA x∈{0,1}P ( X = x ) log P ( X = x ) D log D−DA D log DA
D + D−DA
D
.
Additionally , two integers need to be stored : D and DA . The cost for storing these integers is ( log∗ D + log∗ DA ) bits , where log∗ x is the universal code length for an integer x [ 26 ] . Therefore , the description length is
LD(A ) = log∗ D + log∗ DA + D · HD(X ) = log∗ D + log∗ DA + g(D , DA ) , where g(x , y ) = x log x− ( x− y ) log ( x − y)− y log y , y ≤ x ; x is the total number of values and y is the number of selected values . Encoding the dimensional values . For each dimension d ∈ D , the binary string is of the length Nd and has nd 1s . Therefore , the entropy is
HVd ( X ) = − , nd LV ( A ) =
. , log∗ Nd + log∗ nd + g(Nd , nd) . log Nd−nd
+ Nd−nd
Nd The total description length is log nd Nd
Nd
Nd d∈D
Encoding the time slices . The set of consecutive time slices in the Tartan A is T = [ tstart , tend ] ⊆ [ 1 , T ] , where tend = tstart + T A − 1 . Thus , the description length is
LT ( A ) = log∗ T + log∗ T A + log∗ tstart
Encoding the behaviors . For each time slice t ∈ T , the binary string is of the length E(t ) and has e(t ) 1s . The entropy is
HB(t ) ( X ) = − , e(t ) LB(A ) =
. , log∗ E(t ) + log∗ e(t ) + g(E(t ) , e(t)) .
E(t ) + E(t)−e(t ) log E(t)−e(t )
The total description length is
E(t ) log e(t )
E(t )
E(t )
Encoding the entries in the Tartan . The entries in the Tartan A are non negative counts instead of binary values . The volume , ie , the length of the non negative integer string , is t∈T
, t∈T e(t) . v = , c = d∈D nd d∈D,t∈T b∈B(t),i∈Vd
X ( t ) d ( b , i ) .
The sum of the non negative counts is
It is straightforward to add bits in order to store the integer string as a binary string . For example , if the string is 2 1 0 4 0 , it can be stored as 110 10 0 11110 0 , where 1··· 10 encodes a non negative integer x with x 1s . Therefore , the binary string is of the length v + c and has c 1s . The entropy is
HA(X ) = − , c v+c log c v+c + v v+c log v v+c
.
( c ) Update the consecutive time slices . ( d ) Update the set of dimensions . Figure 5 : Updating the four elements of the Tartan till convergence : time slices , behaviors , dimensions and dimensional values .
The description length is
LA(A ) = ( v + c)HA(X ) = g(v + c , c ) .
The entire encoding cost of the Tartan A is L(A ) = LD(A ) + LV ( A ) + LT ( A ) + LB(A ) + LA(A ) .
The principled scoring function .
The goal is to find the Tartan with high “ meaningfulness score ” . The scoring function is defined as the number of bits ( description length ) that encoding the Tartan A saves from encoding every individual entry in the first level matrix X A : f ( A,X ) = L(X A
) − L(A ) − L(X A\A ) .
( 1 ) where X A\A is the individual entries in X A except the Tartan A . Encoding the individual entries in the first level matrix . X A includes every value from the dimension in the set D and every behavior from the time slice in the set T : X A = {X ( t ) The volume of this first level matrix is d ( b , i)|d ∈ D , t ∈ T , i ∈ {1 , . . . , Nd} , b ∈ {1 , . . . , E(t)}} .
V = , d∈D,t∈T d∈D Nd Its sum of the non negative counts is
C =
, t∈T E(t) . b∈{1,,E(t)},i∈{1,,Nd} X ( t ) d ( b , i ) .
Therefore , the description length of X A is
L(X A ) = g(V + C , C ) + LD(A ) + LT ( A )
+ d∈D log∗ Nd + t∈T log∗ E(t ) .
Given the A and X A , the #bits to encode the individual entries is
L(X A\A ) = g(V + C − v − c , C − c ) ;
Our proposed scoring function encodes different partitions including the dimensions , dimensional values , as well as the time slices and behaviors in the time slice , in order to achieve a concise description of the data . The fundamental trade off that decides the
EncodingthedimensionsEncodingthedimensionalvaluesEncodingthetimeslicesEncodingthebehaviors110011EncodingtheentriesintheTartantstart 1tstarttendtend+1949 “ best ” summaries is between ( 1 ) the number of bits needed to describe the Tartan , and ( 2 ) the number of bits needed to describe the individual entries in the data . Properties . We list several good properties that agree with intuition of the function f ( A,X ) , which directs us to a propagation algorithm that updates the Tartan for a high score . These properties are proved in the Appendix . Property 1 . A Tartan of a higher sum c saves more bits , when other variables are fixed ( which is assumed for all properties ) . Property 2 . A Tartan of a smaller volume v saves more bits . Property 3 . The first level data of a smaller sum C saves more bits . Property 4 . The data of a bigger volume V saves more bits .
7 : 8 : 9 : 10 : 11 :
The scalable algorithm to catch the Tartans .
We propose a greedy search algorithm for optimal partitions in the Tartans . However , finding the optimal solution is NP hard3 . So we present an iterative alternating optimization where we find the optimal set of dimensions , values , time slices and behaviors while holding other variables in the Tartan . We run this sequence of updates until convergence . The algorithm is scalable to run on multiple threads sharing the memory of the dataset . Algorithm 1 CATCHTARTAN : Catching the dynamic multicontextual Tartans for behavioral summaries Require : the behavioral data X = {D , Nd|D 1 : ˜A = {} 2 : while the threads run do 3 : 4 : 5 : 6 : generate a seed Tartan A = {D,Vd|d∈D,T ,B(t)|t∈T } while not converged do
Update the set of behaviors B(t ) ( see Figure 5a ) by for each time slice t ∈ T = [ tstart , tend ] do d=1 , T , E(t)|T t=1} maximizing the scoring function f ( A,X ) end for for each dimension d ∈ D do
Update the set of values Vd ( see Figure 5b ) end for Update the consecutive time slices : check if includes for each dimension d /∈ D do the ( tstart 1) th and ( tend+1) th slices ( see Figure 5c )
Check if includes the dimension ( see Figure 5d ) end for end while ˜A ← ˜A ∪ A sorted in descending order by f ( A,X )
12 : 13 : 14 : 15 : 16 : 17 : end while 18 : return ˜A : the list of Tartans in X Seed selection . We recommend three ways of generating seed Tartans : ( 1 ) one or several random behaviors in a single time slice , ( 2 ) several popular dimensional values , and ( 3 ) high order SVD on the partial data . Experimental results in Section 5 show that the first , simple setting performs well and runs fast . Complexity . The properties with guarantees ensure that the top behaviors in B(t ) and top dimensional values in Vd . Intuitively , a higher score looks for a better compression , ie , few behaviors/values in the time slice/dimension that give large sums of counts . Therefore , the optimization can be solved via a quick sorting of the valt E(t ) log E(t) ) . ues . The time complexity is O( d Nd log Nd+
5 . EXPERIMENTS
In this section , we evaluate CATCHTARTAN and seek to answer the following questions . Can it accurately catch the Tartans from 3It is NP hard since , even allowing only column re ordering , a reduction to the TSP problem can be found [ 13 ] . the temporal multidimensional data ? Is it scalable ? For real world behaviors , are their patterns dynamic and multicontextual ? If they are , what Tartan structures do we see and what do they mean ?
5.1 Quantitative Analysis
It is impracticable to evaluate the behavioral summaries in real datasets . Thus , we generate the synthetic datasets and report the quantitative results . Synthetic datasets and experimental setup . We generate random “ behavioral ” data and inject a Tartan into it . We set up extensive experiments with many parameters on ( 1 ) the Tartan distribution : 1 . T A ∈ [ 2 , 9 ] , the number of consecutive time slices in the 2 . e(t ) ∈ [ 100 , 2 , 000 ] , the number of behaviors in the time 3 . DA ∈ [ 2 , 9 ] , the number of dimensions in A , 3 as default ; 4 . nd ∈ [ 50 , 200 ] , the number of values per dimension in A , 5 . ρ ∈ [ 1 , 10 ] , the average number of values per dimension in
Tartan A , 4 as default ; slice , 1 , 000 as default ;
100 as default ; the behaviors , 3 as default ; and ( 2 ) the data distribution : as default ;
6 . T ∈ [ 5 , 30 ] , the total number of time slices in the dataset , 10 7 . E(t ) ∈ [ 1 , 000 , 10 , 000 ] , the number of behaviors per time 8 . Nd ∈ [ 1 , 000 , 2 , 000 ] , the number of values per dimension slice in the dataset , 5 , 000 as default ; in the data , 1 , 000 as default .
Our task is to catch the Tartan , which has two binary classification subtasks : ( 1 ) detecting the set of behaviors in the Tartan , ( 2 ) detecting the set of dimensional values ( contexts ) in the Tartan .
We adopt the following methods as the baselines : • FSG ( Frequent Subgraph Discovery ) [ 20 ] : this is a frequent itemset discovery algorithm that discovers subgraphs that occur frequently over the entire set of graphs . • EIGENSPOKE [ 24 ] : this is a SVD based method that can detect communities from large graphs by reading the singular vectors of the adjacency matrix . • NMF ( Nonnegative Matrix Factorization ) [ 33 ] : it factorizes the matrices of complex networks to find the close relationship between clustering methods .
The experiments were conducted on a machine with 20 cores of Intel(R ) Xeon(R ) CPU E5 2680 v2 @ 280GHz We set up 10 threads and each thread searches the Tartans with 2 seeds .
Note that none of the above methods selects the dimensions nor time slices . The tensor based methods including decompositions [ 29 , 11 ] and the CROSSSPOT [ 9 ] fail to represent the multicontextual data . They lose lots of information and gave poor performances in the Tartan detection , so we do not show their results in this paper . We evaluate the performance of our CATCHTARTAN and the baselines from two perspectives , ( 1 ) accuracy : F1 score that is the harmonic mean of precision and recall , ( 2 ) efficiency : the cost of running time . A high F1 score indicates accurate performance and a small time cost indicates high efficiency . Accuracy and efficiency . Figure 6 presents the extensive experimental results . As every parameter varies in a big range , it shows the accuracy ( on the 1st and 3rd columns in the figure ) and the time cost ( on the 2nd and 4th columns ) of our CATCHTARTAN and the baseline methods . The solid lines are for the behavior detection , and the dashed lines are for the dimensional value detection . The symbols of our CATCHTARTAN is the red triangles . We have the following observations . • CATCHTARTAN consistently outperforms the baselines and the F1 score is close to the perfect 1 . FSG performs well
950 ( a ) F1 score vs T A .
( b ) Time cost vs T A .
( c ) F1 score vs e(t ) .
( d ) Time cost vs e(t ) .
( e ) F1 score vs DA .
( f ) Time cost vs DA .
( g ) F1 score vs nd .
( h ) Time cost vs nd .
( i ) F1 score vs ρ .
( j ) Time cost vs ρ .
( k ) F1 score vs T .
( l ) Time cost vs T .
( m ) F1 score vs E(t ) .
( n ) Time cost vs E(t ) .
( o ) F1 score vs Nd .
( p ) Time cost vs Nd .
Figure 6 : Synthetic experimental results demonstrate the effectiveness and efficiency of our CATCHTARTAN : the red triangle gives high and stable F1 scores on the tasks of both catching the behaviors and catching the values ; it also costs much less running time than the baselines .
( a ) Taking few iterations .
( b ) Taking few seeds .
Figure 7 : Our CATCHTARTAN takes fewer than 5 iterations in over 90 % runs ; it requires 4 seeds to reach a higher than 0.92 F1 score . when the number of time slices T A or the behaviors e(t ) in the Tartan is big , however , CATCHTARTAN can catch the Tartan when the time period is short , which is a common case in real data . When the number of dimensions DA is bigger or the number of values in the dimension becomes smaller , the F1 score of CATCHTARTAN gradually decreases since the Tartan becomes more like a high dimensional block but it is still higher than the baselines . Moreover , from Figure 6k , 6m and 6o , we demonstrate that CATCHTARTAN is robust to the data distributions , especially when the baselines are inaccurate for the too big data scale of both the temporal and contextual dimension . • CATCHTARTAN consistently costs much less time than the baselines . Our method is cheaper for its counting and sorting operations instead of the high order decomposition . It has a quasi linear complexity while the complexity of the traditional approaches are quadratic . From the 2nd and 4th columns of the figure , we spot that CATCHTARTAN spends less time to reach a much better performance . For the default setting , the CATCHTARTAN uses 0.155 second , while FSG , EIGENSPOKE and NMF use 3.25 , 3.08 and 3.98 seconds , respectively : our method has a 20× speed .
The robustness to the number of iterations and seeds . Figure 7a shows that over 92 % of the processes of catching the Tartans take fewer than 5 iterations . In Figure 7b , we spot that in the default setting , CATCHTARTAN requires only 4 seeds to reach an as highas 0.92 F1 score for both the detection tasks of behaviors and dimensional values . CATCHTARTAN requires no user defined parameters , and it is robust to the number of iterations and seeds . 5.2 Qualitative Analysis
In this section , we discuss qualitative results from applying CATCHTAR
TAN to the tweet datasets and DBLP data mentioned in Table 3 . “ Super Bowl 2013 ” event summaries . We have introduced Figure 2 in which the seven of the Tartans summarize the behavioral patterns in the data . They present five phases of the event such as the score prediction , first half , half time show , second half and sentiments after a win/loss . The Tartans consist of different numbers of dimensions and different consecutive time slices , which indicates the advertising campaigns , local trends and topical discussions . “ Grammys Award 2013 ” event summaries . Figure 9 presents ten Tartans caught by our method from the GRM13 data . The Tartans have meaningful dimensional settings : ( User , Phrase , @User ) in
002040608123456789F1 scoreNumber of time slices in the TartanCatchTartan BehaviorFSG BehaviorEigenSpoke BehaviorNMF BehaviorCatchTartan DimValueFSG DimValueEigenSpoke DimValueNMF DimValue01234523456789Time cost ( second)Number of time slices in the TartanCatchTartanFSGEigenSpokeNMF0020406081F1 scoreNumber of behaviors per slice in the TartanCatchTartan BehaviorFSG BehaviorEigenSpoke BehaviorNMF BehaviorCatchTartan DimValueFSG DimValueEigenSpoke DimValueNMF DimValue012345Time cost ( second)Number of behaviors per slice in the TartanCatchTartanFSGEigenSpokeNMF002040608123456789F1 scoreNumberofdimensionsintheTartanCatchTartan BehaviorFSG BehaviorEigenSpoke BehaviorNMF BehaviorCatchTartan DimValueFSG DimValueEigenSpoke DimValueNMF DimValue01234523456789Time cost ( second)NumberofdimensionsintheTartanCatchTartanFSGEigenSpokeNMF0020406081507090110130150170190F1 scoreNumber of values per dim in the TartanCatchTartan BehaviorFSG BehaviorEigenSpoke BehaviorNMF BehaviorCatchTartan DimValueFSG DimValueEigenSpoke DimValueNMF DimValue012345507090110130150170190Time cost ( second)Number of values per dim in the TartanCatchTartanFSGEigenSpokeNMF002040608112345678910F1 scoreThe average number of values in the TartanCatchTartan BehaviorFSG BehaviorEigenSpoke BehaviorNMF BehaviorCatchTartan DimValueFSG DimValueEigenSpoke DimValueNMF DimValue012345612345678910Time cost ( second)The average number of values in the TartanCatchTartanFSGEigenSpokeNMF002040608157911131517192123252729F1 scoreNumber of time slices in the dataCatchTartan BehaviorFSG BehaviorEigenSpoke BehaviorNMF BehaviorCatchTartan DimValueFSG DimValueEigenSpoke DimValueNMF DimValue05101557911131517192123252729Time cost ( second)Number of time slices in the dataCatchTartanFSGEigenSpokeNMF0020406081F1 scoreNumber of behaviors per slice in the dataCatchTartan BehaviorFSG BehaviorEigenSpoke BehaviorNMF BehaviorCatchTartan DimValueFSG DimValueEigenSpoke DimValueNMF DimValue051015Time cost ( second)Number of behaviors per slice in the dataCatchTartanFSGEigenSpokeNMF002040608110001100120013001400150016001700180019002000F1 scoreNumber of values per dim in the dataCatchTartan BehaviorFSG BehaviorEigenSpoke BehaviorNMF BehaviorCatchTartan DimValueFSG DimValueEigenSpoke DimValueNMF DimValue012345Time cost ( second)Number of values per dim in the dataCatchTartanFSGEigenSpokeNMF10%25%28%18%13%2%4%2%0%5%10%15%20%25%30%12345678PercentageNumberofiterations002040608112345678910F1scoreNumberofseedsF1 BehaviorF1 DimValue951 Figure 8 : Tweet summaries of the NYC14 data : four Tartans of different sets of dimensions indicate behavioral patterns ( eg , advertising ) .
Figure 9 : Tweet summaries of the GRM13 ( Grammys ) : Tartans of different consecutive half hours and different dimensions .
Figure 10 : Tweet summaries of the LA14 ( Los Angeles ) : Tartans of different consecutive days and different dimensions . dicating that a group of users communicated with each other about the same words , ( User , Phrase , Hashtag ) indicating that the users discussed similar topics with a group of phrases ; besides these three dimensional Tartans , there are two four dimensional ones . All these Tartans include a list of consecutive time slices for their dynamic behavioral patterns . “ Los Angeles 2014 ” tweet summaries . Figure 10 presents eight Tartans from the LA14 data . The Tartans have various dimensional settings : three are five dimensional , ( User , Location , Phrase , Hashtag , URL ) for well designed advertising campaigns ; two are fourdimensional , ( User , Location , Phrase , Hashtag ) for local topics and ( User , Location , Phrase , URL ) for advertisements ; one is threedimensional , ( User , Location , @User ) ; and two are two dimensional , ( User , Location ) . These Tartans show not only the multicontextual view but also the dynamic patterns for 3 days , 5 days or even one
( a ) CATCHTARTAN takes fewer than 15 iterations .
( b ) The time cost is linear in the number of behaviors . Figure 11 : The scalability in real data experiments . week . CATCHTARTAN provides comprehensive behavioral summaries about the information at the Greater Los Angeles Area . “ New York City 2014 ” tweet summaries . Figure 8 takes more space to introduce four Tartans in the NYC14 data . The first Tartan has four dimensions . It has 18 users from 9 locations , talking about 42 phrases and 19 hashtags during 4 days in August , 2014 . It has as many as 1,734 tweets . The tweets show that this Tartan encodes a campaign that promotes the band 5SOS ( 5 Seconds of Summer ) . The second Tartan has 1,632 tweets with 22 users , 8 locations and 26 phrases . This 3 dimensional Tartan also takes 4 days but in October , 2014 . The tweets encode the positive sentiments of the New York citizens .
The third Tartan has four dimensions , user , location , phrase and @user . The volume is quite small : only 2 users , 3 locations , 11 phrases and 3 users who were mentioned , but the sum of tweet counts is big . The 1,585 tweets were generated from November 14 to November 18 , which promoted the EP by the band TKLband ( The Killing Lights ) . The messages are too similar to be generated by the two “ legitimate ” users : they are high probably created by some scripts . We even doubt whether the message “ preorder our EP for only $3.99 ” is true or false or fraudulent .
The last example has five dimensions : 2 users , 1 location , 4
Aug15Sep1Sep15Oct1Oct15Nov1Nov15UserLocationPhraseHashtagURL#Tweet2@KTPJobs@TFATechRecruit1Manhattan,NY4 “ KaplanTestPrep ” “ TeachforAmerica ” …16#Job,#NewYork,#TweetMyJob…10http://t.co/Wt1xUbqheR…80Nov16 Nov23UserLocationPhrase@User#Tweet2@bryanallantkl@DaniellaGates3Elmwood Park , NJLyndhurst , NJBelleville , NJ11 “ play bass in tklbandnew single ” “ what ’s up ” “ preorder our ep for only ” 3@tklband@xbrooke_alexis@ashton5sos1,585Nov14 Nov182014 11 14 17:04:18bryanallantkl:@jjnnniWhat‘s up!?? ? ? I play bass in @TKLbandNEW SINGLE! http://t.co/Rrn6bTqHVJ preorder our EP for only $3.99! https://t.co/2YiGs7XILe2014 11 14 17:04:36bryanallantkl:@officiallizelyWhat's up!????I play bass in @TKLbandNEW SINGLE! http://t.co/PIlTd39LM9 preorder ourEPfor only$3.99! https://t.co/M9EeHdR6P22014 11 16 06:41:22KTPJobs:KaplanTest Prep : Director of Employee Relations ( #NewYork , NY ) http://t.co/Wt1xUbqheR #HR #Job #Jobs #TweetMyJobs2014 11 16 07:06:52TFATechRecruit:#OpenSource#Job alert : Director , JavaScript Front End Developer Teach For America #NewYork , NY http://t.co/5UDjsb0Bdk #Jobs2014 11 16 08:17:09TFATechRecruit:TeachFor America #IT #Job : Assistant , Information Technology ( #NewYork , NY ) http://t.co/l2MCuCpeU9 #Jobs #TweetMyJobsUserLocationPhrase#Tweet22@DateMeJackDail@picomarlon_pico8Rahway , NJManhattan , NY26 “ Iloveyou ” , “ newyork ” , “ thankyou ” , “ dreamcometrue ” …1,632Oct1 Oct4UserLocationPhraseHashtag#Tweet18@queen_toni_@nachoiall9Bronx,NYStatenIsland,NY42 “ havepizza ” , “ eatcandy ” …19#vote2sos,#bsmg,#coast2coast…1,734Aug16 Aug192014 10 02 03:56:06JossethHenry:Thank You , Lord , for every blessing and favor in my life God never forgets God is not a human being that He should change His mind2014 10 02 09:26:35picomarlon_pico:@ShopOnThePorchthank you2014 10 02 12:35:39omgimsoawesome:I love you baby @primetime_joe2014 08 16 04:35:17queen_toni_:AND WE CAN HAVE PIZZA BLAST MUSIC EAT CANDY AND WATCH MOVIES @Michael5SOS#vote5sos http://t.co/qZGv93B82P2014 08 16 04:35:33queen_toni_:AND WE CAN HAVE PIZZA BLAST MUSIC EAT CANDY AND WATCH MOVIES @Michael5SOS#vote5sos http://t.co/mrhy1dckG82014 08 16 04:36:10queen_toni_:AND WE CAN HAVE PIZZA BLAST MUSIC EAT CANDY AND WATCH MOVIES @Michael5SOS#vote5sos http://t.co/VDIEGzPZhD17:30 19:00(3)530tweets145users53phrases32@Users18:0020:0022:0000:0002:0004:0006:0008:0010:0012:0014:0016:0017:30 22:00(9)5,432tweets1,730users354phrases148hashtags268@Users19:30 23:30(8)2,299tweets823users111phrases89hashtags20:30 00:00(7)2,109tweets774users185phrases56hashtags109@Users03:00 06:30(7)2,690tweets823users263phrases242hashtags04:30 08:00(7)1,913tweets680users164phrases152@Users07:30 10:30(6)2,002tweets714users178phrases199hashtags10:30 14:00(7)2,439tweets888users182phrases300@Users13:00 16:00(6)1,287tweets466users132phrases181@Users15:30 18:00(5)2,206tweets688users172phrases173@UsersAug6 Aug13(8)534tweets9users5locations55@UsersAug1Aug15Sep1Sep15Oct1Oct15Nov1Nov15Dec1Aug15 Aug18(9)481tweets26users7locations28phrases48@UsersSept7 Sept11(5)362tweets7users4locationsSept17 Sept24(8)481tweets3users2locations15phrases22hashtags75URLsOct13 Oct19(7)189tweets7users1location10phrases21hashtags65URLsNov13 Nov20(8)455tweets8users3locationsNov18(1)43tweets1user1location2phrases1hashtag1URLNov7 Nov9(3)152tweets7users2locations5phrases20hashtag0%10%20%30%40%24681012PercentageThenumberofiterationsinreal dataexperimentsNYC14LA14SPB13GRM13DBLP0200400600800024681012Timecost(second)Thenumberofbehaviorsinthedataset(million)NYC14GRM13SPB13LA14DBLP952 Figure 12 : Publishing paper behavioral summaries : seven Tartans about DM and ML show the dynamics and contexts of our community . phrases , 16 hashtags and 10 URLs . The messages are a large group of interesting news about job hunting in the Manhattan , NY . The number of the messages is 80 . It looks small but the messages are from a even smaller number of users and locations . Compressing these messages as a whole can save lots of bits from compressing the messages individually . “ DBLP ” summaries . Figure 12 presents seven ( but not the least ) Tartans in the DBLP data . They are shown for their relatedness to the area of database , data mining , machine learning and so on .
The sixth Tartan presents the behaviors by the “ web search ” community . It has four dimensions . A group of 12 authors studied 3 keywords “ web search ” , “ click through data ” and “ sponsored search ” and published 32 papers on the SIGIR , WWW , WSDM , CIKM conferences from 2006 to 2013 . These conferences are the major information retrieval conferences . There are 12 highlycited papers . The most representative paper is p82630 ( “ Optimizing search engines using clickthrough data ” ) .
Finally , the small but meaningful Tartan represents the “ transfer learning ” community . 8 authors , eg , Dr . Qiang Yang and Dr . Dou Shen , published 17 papers about “ transfer learning ” and “ data mining ” on the KDD and AAAI conferences . This is a 3 dimensional Tartan during the year 2007 2010 . Efficiency . Figure 11a shows the distributions of the number of iterations until convergence when we apply CATCHTARTAN to different datasets : CATCHTARTAN takes fewer than 15 iterations , and the most frequent number is consistently smaller than 10 . Figure 11b shows that the time cost is linear in the number of behaviors in the real data experiments , which demonstrates the scalability . 6 . CONCLUSIONS
In this paper , we uncovered the dynamic and multicontextual patterns of human behaviors and focused on the problem of behavioral summarization . We proposed a novel representation called the Tartan that includes a set of dimensions , sets of dimensional values , consecutive time slices , and sets of behaviors in the slices . We proposed a parameter free and scalable method CATCHTARTAN to capture the Tartan summaries with a principled scoring function . We applied our CATCHTARTAN to the synthetic data , DBLP data and Twitter datasets . The experimental results including the comprehensive event summaries have demonstrated the effectiveness and efficiency of our proposed CATCHTARTAN . 7 . ACKNOWLEDGEMENTS
We thank the reviewers for their insightful comments . This work was sponsored in part by the US Army Research Lab . under Cooperative Agreement No . W911NF 09 2 0053 ( NSCTA ) , National Science Foundation CNS 1314632 , IIS 1408924 , IIS 1017362 , IIS1320617 , IIS 1354329 , HDTRA1 10 1 0120 , and grant 1U54GM114838 awarded by NIGMS through funds provided by the trans NIH Big Data to Knowledge ( BD2K ) initiative ( wwwbd2knihgov ) , and
The first Tartan has four dimensions , author , venue , keyword and cited paper . From 2003 to 2007 , 76 authors published papers about “ information retrieval ” , “ data integration ” and “ text classification ” on the SIGIR , VLDB conferences and TKDE journal . The paper p56743 ( “ A language modeling approach to information retrieval ” ) was frequently cited by these papers . The authors have all read this paper and explored new techniques to address the problems .
The second Tartan has only two dimensions , the venue and keyword . There were 40 papers about “ reinforcement learning ” on the ICML and NIPS conferences from 1997 to 2002 . The author and cited paper dimensions are missing as many scholars cited many related papers in their publications . The Tartan automatically excluded both the dimensions because no leading authors nor cited papers could be summarized .
The third Tartan has three dimensions , author , venue and cited paper . A group of six famous researchers , eg , Dr . Jiawei Han and Dr . Xifeng Yan , published 22 papers on the SIGMOD conference which cited the same paper p76095 ( “ Frequent subgraph discovery ” ) from 2004 to 2010 . The 22 papers used different phrases because the area of “ subgraph mining ” or “ frequent subgraph pattern mining ” was promising but not mature : many different methods , models and algorithms were proposed .
The forth Tartan has two dimensions , venue and keyword . 25 papers about “ anomaly detection ” were published in the ICDM , AAAI conferences and TKDE journal from 2005 to 2013 . The papers were written by a large number of authors to address the detection problem in many applications with different methods .
The fifth Tartan is relatively large . It has 3 dimensions of 27 authors , 6 venues and 12 keywords . These 70 papers were published from 2006 to 2013 . The group of researchers studied the “ large graphs ” , “ data streams ” , “ evolving data ” and “ evolving graphs ” on the KDD , ICDM , ICDE conferences and TKDE journal .
199720002003200620092012AuthorVenueKeyword#Paper8QiangYangDouShenSinnoPan3KDDPAKDDAAAI6 “ transferlearning ” “ datamining ” “ localizationmodels ” 172007 2010AuthorVenueKeywordCited#Paper12RyenWhiteHangLiTie YanLiuZhaohuiZheng…5SIGIRWWWWSDMCIKM3 “ websearch ” “ click throughdata ” “ sponsoredsearch ” 12p826303p116290p103899p106191…322006 20133 “ Optimizing search engines using clickthroughdata ” AuthorVenueKeyword#Paper27CFaloutsosJPeiPSYuXLinCAggarwal6KDDICDMICDETKDE…12 “ largegraphs ” “ datastreams ” “ evolvingdata ” “ evolvinggraphs ” …702006 2013VenueKeyword#Paper3ICDMAAAITKDE1 “ anomalydetection ” 252005 2013AuthorVenueCited#Paper6JiaweiHanXifengYan1SIG MOD1p760952222004 20102 “ Frequentsubgraphdiscovery ” AuthorVenueKeywordCited#Paper76Cheng xiangZhaiHuiFangS.Kambhampati7SIGIRVLDBTKDE7 “ informationretrieval ” “ dataintegration ” “ textclassification ” 68p567431p62995p76869322003 20071 “ A language modeling approach to information retrieval ” VenueKeyword#Paper5ICMLNIPS…6 “ reinforcementlearning ” “ machinelearning ” 401997 2002953 MIAS , a DHS IDS Center for Multimodal Information Access and Synthesis at UIUC . The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies , either expressed or implied , of the Army Research Laboratory , the US Government , or other funding parties . The US Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on .
8 . REFERENCES [ 1 ] R . Agrawal , J . Gehrke , D . Gunopulos , and P . Raghavan . Automatic subspace clustering of high dimensional data . DMKD’05 .
[ 2 ] M . Araujo , S . Papadimitriou , S . Günnemann , C . Faloutsos ,
P . Basu , A . Swami , E . E . Papalexakis , and D . Koutra . Com2 : fast automatic discovery of temporal ( “ comet ” ) communities . In PAKDD’14 .
[ 3 ] R . Cilibrasi and P . Vitanyi . Clustering by compression .
TIT’05 .
[ 4 ] P . Cui , H . Liu , C . Aggarwal , and F . Wang . Computational modeling of complex user behaviors : Challenges and opportunities . IEEE Intelligent Systems , 31(2):78–81 , 2016 .
[ 5 ] L . De Lathauwer , B . De Moor , and J . Vandewalle . A multilinear singular value decomposition . SIAM journal on Matrix Analysis and Applications , 2000 .
[ 6 ] C . Faloutsos and V . Megalooikonomou . On data mining , compression , and kolmogorov complexity . DMKD’07 .
[ 7 ] R . L . Ferreira Cordeiro , C . Traina Junior , A . J .
Machado Traina , J . López , U . Kang , and C . Faloutsos . Clustering very large multi dimensional datasets with mapreduce . In KDD’11 .
[ 8 ] J . Han , H . Cheng , D . Xin , and X . Yan . Frequent pattern mining : current status and future directions . DMKD’07 .
[ 9 ] M . Jiang , A . Beutel , P . Cui , B . Hooi , S . Yang , and
C . Faloutsos . A general suspiciousness metric for dense blocks in multimodal data . In ICDM’15 .
[ 10 ] M . Jiang , P . Cui , and C . Faloutsos . Suspicious behavior detection : Current trends and future directions . Intelligent Systems , IEEE , 31(1):31–39 , 2016 .
[ 11 ] M . Jiang , P . Cui , F . Wang , X . Xu , W . Zhu , and S . Yang .
Fema : flexible evolutionary multi faceted analysis for dynamic behavioral pattern discovery . In KDD’14 .
[ 12 ] M . Jiang , P . Cui , F . Wang , W . Zhu , and S . Yang . Scalable recommendation with social contextual information . TKDE’14 .
[ 13 ] D . Johnson , S . Krishnan , J . Chhugani , S . Kumar , and
S . Venkatasubramanian . Compressing large boolean matrices using reordering techniques . In VLDB’04 .
[ 14 ] K . Kailing , H P Kriegel , and P . Kröger . Density connected subspace clustering for high dimensional data . In SDM’04 . [ 15 ] D . Kang , D . Jiang , J . Pei , Z . Liao , X . Sun , and H J Choi .
Multidimensional mining of large scale search logs : a topic concept cube approach . In WSDM’11 .
[ 16 ] U . Kang and C . Faloutsos . Beyond “ caveman communities ” :
Hubs and spokes for graph compression and mining . In ICDM’11 .
[ 17 ] T . G . Kolda and B . W . Bader . Tensor decompositions and applications . SIAM review , 2009 .
[ 18 ] D . Koutra , U . Kang , J . Vreeken , and C . Faloutsos . Vog : Summarizing and understanding large graphs . SDM’14 .
[ 19 ] H P Kriegel , P . Kröger , M . Renz , and S . Wurst . A generic framework for efficient subspace clustering of high dimensional data . In ICDM’05 .
[ 20 ] M . Kuramochi and G . Karypis . An efficient algorithm for discovering frequent subgraphs . TKDE’04 .
[ 21 ] M . Li and P . Vitányi . An introduction to Kolmogorov complexity and its applications . 2013 .
[ 22 ] S . Padmanabhan , B . Bhattacharjee , T . Malkemus ,
L . Cranston , and M . Huras . Multi dimensional clustering : a new data layout scheme in db2 . In SIGMOD’03 .
[ 23 ] L . Parsons , E . Haque , and H . Liu . Subspace clustering for high dimensional data : a review . KDD’04 .
[ 24 ] B . A . Prakash , A . Sridharan , M . Seshadri , S . Machiraju , and C . Faloutsos . Eigenspokes : Surprising patterns and scalable community chipping in large graphs . In PAKDD’10 . [ 25 ] J . Rissanen . Modeling by shortest data description .
Automatica , 14(5 ) , 1978 .
[ 26 ] J . Rissanen . A universal prior for integers and estimation by minimum description length . The Annals of statistics , 1983 . [ 27 ] N . Shah , D . Koutra , T . Zou , B . Gallagher , and C . Faloutsos . Timecrunch : Interpretable dynamic graph summarization . In KDD’15 .
[ 28 ] J . Sun , C . Faloutsos , S . Papadimitriou , and P . S . Yu .
Graphscope : parameter free mining of large time evolving graphs . In KDD’07 .
[ 29 ] J . Sun , D . Tao , and C . Faloutsos . Beyond streams and graphs : dynamic tensor analysis . In KDD’06 .
[ 30 ] F . Tao , K . H . Lei , J . Han , C . Zhai , X . Cheng , M . Danilevsky ,
N . Desai , B . Ding , J . G . Ge , H . Ji , et al . Eventcube : multi dimensional search and mining of structured and text data . In KDD’13 .
[ 31 ] H . Toivonen , F . Zhou , A . Hartikainen , and A . Hinkka .
Compression of weighted graphs . In KDD’11 .
[ 32 ] J . Vreeken , M . Van Leeuwen , and A . Siebes . Krimp : mining itemsets that compress . DMKD’11 .
[ 33 ] F . Wang , T . Li , X . Wang , S . Zhu , and C . Ding . Community discovery using nonnegative matrix factorization . DMKD’11 . [ 34 ] T . Zhang , P . Cui , C . Song , W . Zhu , and S . Yang . A multiscale survival process for modeling human activity patterns . PloS one , 11(3):e0151473 , 2016 .
[ 35 ] P . Zhao , X . Li , D . Xin , and J . Han . Graph cube : on warehousing and olap multidimensional networks . In SIGMOD’11 . APPENDIX Proofs of the Properties in Section 4 . Suppose the partitions have much smaller encoding cost than the data entries , the scoring function can be written as f ( A,X ) = g(V + C , C ) − g(v + c , c ) − g(V + C − v − c , C − c ) . Since g(x ) = log x − log ( x − y ) and g(y ) = log ( x − y ) − log y , we have the derivatives of the function as follows . V −v−c V −v−c
( C−c−v)c ( C−c−v)c
= log
= log
V v
,
∂f ∂v
∂f ∂c
∂f ∂C
= log
V v
( V +C−c)C ( V +C−c)C
V v
V v
+ c v + C V − c − C v
V
V
V + C V + c − C − c
C+V −v C+V −v
V v v v v
.
,
∂f ∂V
= log
In a summary , the density of the Tartan is higher than the data : v > C c ∂v < 0 ( Property 2 ) , ∂f
∂c > 0 ( Property 1 ) , ∂f ∂V < 0 ( Property 4 ) .
V . Thus , we obtain that ∂f
∂C > 0 ( Property 3 ) , and ∂f
954
