Structured Doubly Stochastic Matrix for
Graph Based Clustering
Xiaoqian Wang
Department of Computer Science and Engineering
University of Texas at Arlington xqwang1991@gmail.com
Texas , USA
Feiping Nie
Department of Computer Science and Engineering
Heng Huang
Department of Computer Science and Engineering
∗
University of Texas at Arlington
University of Texas at Arlington
Texas , USA feipingnie@gmail.com
Texas , USA heng@uta.edu
ABSTRACT As one of the most significant machine learning topics , clustering has been extensively employed in various kinds of area . Its prevalent application in scientific research as well as industrial practice has drawn high attention in this day and age . A multitude of clustering methods have been developed , among which the graph based clustering method using the affinity matrix has been laid great emphasis on . Recent research work used the doubly stochastic matrix to normalize the input affinity matrix and enhance the graph based clustering models . Although the doubly stochastic matrix can improve the clustering performance , the clustering structure in the doubly stochastic matrix is not clear as expected . Thus , postprocessing step is required to extract the final clustering results , which may not be optimal . To address this problem , in this paper , we propose a novel convex model to learn the structured doubly stochastic matrix by imposing low rank constraint on the graph Laplacian matrix . Our new structured doubly stochastic matrix can explicitly uncover the clustering structure and encode the probabilities of pair wise data points to be connected , such that the clustering results are enhanced . An efficient optimization algorithm is derived to solve our new objective . Also , we provide theoretical discussions that when the input differs , our method possesses interesting connections with K means and spectral graph cut models respectively . We conduct experiments on both synthetic and benchmark datasets to validate the performance of our proposed method . The empirical results demonstrate that our model provides an approach to better solving the K mean clustering problem . By using the cluster indicator provided by our model as initialization , Kmeans converges to a smaller objective function value with better clustering performance . Moreover , we compare the clustering performance of our model with spectral clustering and related double stochastic model . On all datasets , our method performs equally or better than the related methods . ∗ To whom all correspondence should be addressed . This work was partially supported by US NSF IIS 1117965 , NSF IIS 1302675 , NSF IIS 1344152 , NSF DBI 1356628 , NIH R01 AG049371 .
CCS Concepts •Theory of computation → Unsupervised learning and clustering ;
Keywords Doubly Stochastic Matrix ; Graph Laplacian ; K means Clustering ; Spectral Clustering
1 .
INTRODUCTION
Graph based learning is one of the central research topics in machine learning . These models use similarity graph as input and perform learning tasks over the graph , such as spectral clustering [ 17 ] , manifold based dimensional reduction [ 2 , 12 ] , graph based semisupervised learning [ 30 , 31 ] , etc . Because the input data graph is often associated to the affinity matrix ( the pair wise similarities between data points ) , graph based learning methods show promising performance and have been introduced to many applications in data mining [ 20 , 19 , 18 , 1 , 6 ] . The graph based learning methods depend on the input affinity matrix , thus the quality of the input data graph is crucial in achieving the final superior learning solutions .
To enhance the learning results , graph based learning methods often need pre processing on the affinity matrix . The doubly stochastic matrix ( also called bistochastic matrix ) was utilized to normalize the affinity matrix and showed promising clustering results [ 29 ] . A doubly stochastic matrix S ∈ ffn×n is a square matrix and all elements in S satisfy : sij ≥ 0 , sij = 1 , 1 ≤ i , j ≤ n . n . n . sij = 1 ,
( 1 ) j=1 i=1
Some previous works have been proposed to learn the best doubly stochastic approximation to a given affinity matrix [ 29 , 26 , 11 ] . Imposing the double stochastic constraints can properly normalize the affinity matrix such that the data graph is more suitable for clustering tasks . However , even with using the doubly stochastic matrix , the final clustering structure is still not obvious in the data graph . The graph based clustering methods often use K means algorithm to post process the clustering results to get the clustering indicators , thus the final clustering results are dependent on and sensitive to the initializations .
To address these challenges , in this paper , we propose a novel model to learn the structured doubly stochastic matrix , which encodes the probability of each pair of data points to be connected . We explicitly constrain the rank of the graph Laplacian and guarantee the number of connected components in the graph to be exactly k , ie the number of clusters . Thus , the clustering results
1245 can be directly obtained without the post processing step , such that the clustering results are superior and stable . To solve the proposed new objective , we introduce an efficient optimization algorithm . Meanwhile , we also provide theoretical analysis on the connection between our method and K means clustering as well as spectral clustering , respectively .
We conduct extensive experiments on both synthetic and benchmark datasets to evaluate the performance of our method . We find that our method provides a good initialization for K means clustering such that a smaller objective function value can be achieved for the K means problem . Also , we compare the clustering performance of our model with other methods . On all datasets , our method performs equally or better than related methods .
Notations : Throughout this paper , matrices are all written as uppercase letters while vectors as bold lower case letters . For a matrix M ∈ ffd×n , its i th row , j th column and ij th element are denoted by mi , mj and mij respectively . The Frobenius norm of M is defined as ( M( min{d,n}' known as the nuclear norm ) is defined as ( M(∗ = σi , n' where σi is the i th singular value of M . For a vector v ∈ ffn , when p )= 0 , its fip norm ( v( |vi|p ) p . Specially , 1 represents a vector whose elements are 1 consistently and I stands for the identity matrix . p is defined as ( v( ij . The trace norm ( also d' n' fi p = (
F = m2 j=1 i=1 i=1 i=1
1
2 . CLUSTERING WITH NEW STRUCTURED
DOUBLY STOCHASTIC MATRIX
To achieve good clustering results , doubly stochastic matrix is usually utilized to approximate the given affinity matrix such that the clustering structure in data graph can be maintained . However , final clustering structures are still not obvious in the data graph and post processing step has to be employed , which leads the clustering results to be not optimal . To address this problem and learn a more powerful doubly stochastic matrix , we propose a new structured doubly stochastic model to capture clustering structure and encode probabilistic data similarity . 2.1 New Structured Doubly Stochastic Matrix The ideal clustering structure of a graph with n data points is to have exactly k connected components , where k is the number of clusters . If we reshuffle the n data points in the similarity matrix of the ideal graph such that points in the same connected components are arranged together , then the k connected components form k blocks ranging along the diagonal of the similarity matrix . With such ideal structure we can immediately obtain clustering indicators without post processing steps . Thus , we propose a novel method to learn the new structured doubly stochastic matrix which has such ideal clustering structure with exactly k blocks . From [ 15 , 8 ] , we know that each connected component in the graph W corresponds to an eigenvalue 0 in the Laplacian matrix LW = DW − W . DW is the degree matrix of graph W defined as DW = wij ) , where Diag(a ) denotes a diagonal matrix with diDiag(
' j agonal elements formed by the entries of vector a . If W has exactly k blocks , then k eigenvalues of LW are zeros , ie the rank of LW is equal to n − k . Therefore , given the affinity matrix W , we propose to learn a new structured doubly stochastic matrix M ∈ ffn×n such that its Laplacian matrix LM = DM −M is restricted to be rank(LM ) = n − k . With this constraint , the learnt M has the clustering block structure along the diagonal with proper permutation , based on which we can directly partition the data points into k clusters .
Moreover , to make M encode the probability of each pair of data points to be connected in the graph , we normalize M1 = 1 . As a result , the entire probability for a point to connect with others is 1 . Consequently , we have DM = I . Meanwhile , since M represents the probability of each pair of data points to be connected , we naturally expect the learnt M is symmetric and nonnegative . These constraints validate the doubly stochastic property of matrix M .
Under our new constraints , we learn a structured doubly stochastic matrix M to best approximate the affinity matrix W by solving : ( 2 )
( M − W(2
F st M ≥ 0 , M = M T , M1 = 1 , rank(LM ) = n − k . min M
Theoretically , in the ideal case the probability of a certain point to correlate with points in the same cluster should be the same . That is , suppose there are ni points in the i th cluster , for any two points ps and pn in the i th cluster , the probability of ps and pn to be connected is msn = 1 ni . Consistently , for point ps , we have mss = 1 F , where according to Lemma 1 a large enough parameter r forces the elements in each block of matrix M to be the same . ni . Toward this end we add another term r ( M(2
LEMMA 1 . In the following problem : F + r ( M(2
( M − W(2
F min M st M ≥ 0 , M = M T , M1 = 1 , rank(LM ) = n − k , if the value of r tends to be infinity , the matrix M is block diagonal with elements in each block to be the same . The number of blocks in matrix M is k .
Proof : If r tends to infinity , the following optimization problem ( 3 )
( M − W(2
F + r ( M(2
F st M ≥ 0 , M = M T , M1 = 1 , rank(LM ) = n − k min M is equivalent to
( M(2
F min M
( 4 ) st M ≥ 0 , M = M T , M1 = 1 , rank(LM ) = n − k .
According to the previous discussion , with proper rearrangement of rows and columns of M , the constraint rank(LM ) = n − k ⎡ require the structure of M to be k blocks diagonally arranged like : ⎢⎢⎢⎣
⎤ ⎥⎥⎥⎦ .
M1
M2
0
0
Mk
Meanwhile , let ’s we denote a random row ( or column ) of one
'
. mi)2 i ≥ ( m2 i block in M as mT ( or m ) . Since length(m ) , and only when all mi are of the same value makes the left and right sides be equal . Thus , the solution to minimize Problem ( 4 ) is that M is a block diagonal matrix with elements in each block to be the same . 2 In addition , to highlight the constraint that M has exactly k i blocks , we add another constraint as T r(M ) = k and have :
( M − W(2
F + r ( M(2 st M ≥ 0 , M = M T , M1 = 1 , min M
F rank(LM ) = n − k , T r(M ) = k .
( 5 )
1246 The constraint rank(LM ) =n − k makes Problem ( 5 ) nonconvex . Therefore , we use the trace norm of LM as a relaxation form of rank(LM ) . Our final objective is to solve :
( M − W(2
F + γ ( LM(∗ + r ( M(2 Jopt = min M st M ≥ 0 , M = M T , M1 = 1 , T r(M ) = k .
( 6 ) It is not trivial to solve our new objective Jopt in Eq ( 6 ) . We will propose a novel algorithm to solve the new objective . Before that , we first show the interesting connection between our model and spectral clustering . 2.2 Connections to Spectral Graph Cut
F
Models
THEOREM 1 . In Problem ( 5 ) , if r → ∞ and W is doubly stochastic , then Problem ( 5 ) is equivalent to spectral clustering . Proof : As illustrated in [ 25 ] , given a graph G with n points and its affinity matrix W ∈ ffn×n , the definition of graph cut is :
. k .
1≤p≤q≤k s(Cl , ¯Cl )
ρ(Cl ) l=1
J =
= s(Cp , Cq )
ρ(Cp )
+ s(Cp , Cq )
ρ(Cq )
,
( 7 ) where k is the number of clusters , Cl is the l th cluster and ¯Cl is the complement subset of cluster Cl in graph G , s(M , N ) =
'
' m∈M n∈N
Wmn .
For Ratio Cut , the function ρ(Cl ) in Eq ( 7 ) is defined as :
ρRCut ( Cl ) = |Cl| .
( 8 ) We introduce an indicator vector ql ∈ ffn(l = 1 , 2,··· , k ) , such that the i th element of ql equals to 1 if the i th point in graph G belongs to the l th cluster , and 0 otherwise .
With the indicator vector ql , we have : s(Cl , ¯Cl ) =
Wij = qT l ( DW − W )ql .
Along with Eq ( 8 ) , we can rewrite the cut function of Ratio Cut in Eq ( 7 ) as : l ( DW − W )ql qT
JRCut =
( 9 ) Define matrix G ∈ ffn×k such that gl = ql . We introduce an l ql qT l=1
. indicator matrix F :
− 1
F = G(GT G )
( 10 ) Assume xi ∈ Ci and xj ∈ Cj . For the F matrix in Eq ( 10 ) , we ni ; otherwise fij = 0 , can observe that if Ci = Cj , then fij = 1√ where ni denotes the number of data points in the i th cluster .
2 ,
Thus , the cut function ( 9 ) can be written as :
JRCut = T r(F T ( DW − W )F ) .
( 11 ) If W is doubly stochastic , then DW = I . In this case , Normalized Cut is equivalent to Ratio Cut and tackles the following problem :
.
. i∈Cl j∈ ¯Cl k .
Let M = F F T , the Problem ( 12 ) becomes :
( M − W(2 F . min M
( 13 )
We can directly find that mij = 1 mij = 0 when Ci )= Cj , where xi ∈ Ci and xj ∈ Cj . ni when Ci = Cj ; while
Thus , the spectral clustering problem is to find a matrix M minimizing Problem ( 13 ) . From the definition of M , we can obtain some properties of M that M ≥ 0 , M T = M and M1 = 1 , thus it is doubly stochastic . As for T r(M ) , we have :
T r(M ) = T r(F F T ) = T r(G(GT G )
−1G ) = k .
( 14 )
2 In practice , matrix W may not always be doubly stochastic . Given affinity matrix W0 , we can learn a doubly stochastic matrix W as the initialization by solving : min
W≥0,W =W T ,W 1=1
( W − W0(2 F .
( 15 )
The above problem is the same as the problem solved in [ 29 ] and also a special case of our proposed objective in Eq ( 6 ) when both parameter γ and r are set as 0 .
Previous method in [ 29 ] can only learn a doubly stochastic matrix without clear clustering structure . After adding the regularization terms , our new objective can achieve a better doubly stochastic matrix with clear clustering structure to improve the clustering results .
3 . OPTIMIZATION ALGORITHM strategy [ 4 ] to solve our new objective Jopt in Eq ( 6 ) .
We use the Augmented Lagrange Multiplier ( ALM ) optimization Here we introduce a slack variable L that L = I − M , then
Problem ( 6 ) can be rewritten as : ( M − W(2 min M
F + γ ( L(∗ + r ( M(2
F st M ≥ 0 , M = M T , M1 = 1 , T r(M ) = k ,
I − M = L , and Problem ( 16 ) is equivalent to : ( M − W(2
F + γ ( L(∗ ffffffffI − M − L + ffffffff2
F
1 μ
Λ min M
+
μ 2
( 16 )
+ r ( M(2
F
( 17 ) st M ≥ 0 , M = M T , M1 = 1 , T r(M ) = k , where Λ ∈ ffn×n is the Lagrange multiplier and μ is the penalty parameter for Eq ( 17 ) . Compared with Problem ( 6 ) , Problem ( 17 ) is easier to solve since the trace norm term γ ( L(∗ is now independent to M . We introduce an efficient alternating algorithm to tackle Problem ( 17 ) . The first step is fixing L and solving M , thus Problem ( 17 ) becomes :
( M − W(2
F +
μ 2 min M +r ( M(2
F ffffffffI − M − L + ffffffff2
1 μ
Λ
F
( 18 ) st M ≥ 0 , M = M T , M1 = 1 , T r(M ) = k .
F min =⇒ max =⇒ min
F
F
T r(F T ( I − W )F ) T r(F T W F ) ffffffF F T − W ffffff2
F
Let
.
( 12 )
T =
1
μ + 2r
( 2W + μ(I − L +
1 μ
Λ ) ) ,
1247 Algorithm 1 Proposed Algorithm Input :
The given affinity matrix W ∈ ffn×n ; The number of clusters k ;
Output :
∗
;
The learnt similarity matrix M Initialization : Let the count number of iteration t = 0 . Randomly initialize matrix L(0 ) ∈ ffn×n and set the Lagrange multiplier matrix Λ(0 ) = 0 ∈ ffn×n . Set the penalty parameter μ(0 ) = 0.1 , and the increment step parameter ρ > 1 ; Preprocessing : Solve Problem ( 15 ) to pre process W and get a doubly stochastic matrix M ( 0 ) . Let W = M ( 0 ) . while not converge do
1 . Update M ( t+1 ) using Eq ( 22 ) Eq ( 23 ) alternatively via the successive projection strategy ; 2 . Update L(t+1 ) by Eq ( 26 ) ; 3 . Update Λ(t+1 ) = Λ(t ) + μ(t)(I − M ( t+1 ) − L(t+1) ) ; 4 . Update μ(t+1 ) = ρμ(t ) ; 5 . Update t = t + 1 ; end while Return : M
∗
; refer to the literature therein [ 3 , 22 ] . Because our new objective is convex , our algorithm converges to the global optimum .
In Algorithm 1 , the slowest step is Step 2 for updating L .
It requires O(n3 ) time to implement the singular vector decomposition , where n is the number of samples in the dataset . This time complexity is comparable to that of spectral clustering .
4 . CONNECTIONS TO K MEANS CLUSTER
ING
Here in this section , we will further discuss the connection be tween our model and the K means clustering problem .
THEOREM 2 . In Problem ( 5 ) , if r → ∞ and W = X T X , then
Problem ( 5 ) is equivalent to K means clustering . Proof : Given a set of data points X = [ x1 , x2 , , xn ] ∈ ffd×n , the K means clustering problem is meant to partition X into k ( 1 ≤ k ≤ n ) clusters C = {C1 , C2 , , Ck} such that the sum of the within cluster variance is minimized [ 13 ] . That is to say , the objective function of the K means problem is :
( xj − μi(2
( 27 )
( 19 )
( 20 )
( 21 ) the Problem ( 18 ) can be rewritten as :
( M − T(2
F min M st M ≥ 0 , M = M T , M1 = 1 , T r(M ) = k .
Since the constraint on T r(M ) is only concerned with the diagonal elements , Problem ( 19 ) can be divided into two subproblems :
( M − T(2 min M
F , st M = M T , M1 = 1 , and
( M − T(2
F , st M ≥ 0 , mT 1 = k , min M where m = diag(M ) and diag(M ) denotes a vector formed by the diagonal elements of M .
Our strategy is to solve two subproblems , Problem ( 20 ) and Problem ( 21 ) alternately , and let their solutions project mutually . In each iteration , we solve Problem ( 20 ) first and let its solution M1 to be the T matrix in Problem ( 21 ) , afterwards we solve Problem ( 21 ) and let its solution M2 play the role of matrix T in Problem ( 20 ) . We solve these two problems alternately and iteratively until M converges .
According to Von Neumann ’s successive projection lemma [ 16 ] , this mutual projection strategy we use will converge to the cross of two subspaces formed by Problems ( 20 ) and ( 21 ) . The lemma theoretically ensures that the solution of the alternate projection strategy ultimately converges onto the global optimal solution of Problem ( 19 ) .
According to Lemma 2 in Appendix A , the optimal solution of
Problem ( 20 ) is as follows : n + 1T K1
M = K +
11T − 1 n
K11T − 1 n
11T K ,
( 22 ) n2 where K = T +T 2
T
.
As far as Problem ( 21 ) is concerned , firstly we let matrix T in Problem ( 21 ) equal to the solution of M to Problem ( 20 ) ( shown in Eq ( 22) ) . Then according to Lemma 3 in Appendix B , the optimal solution of Problem ( 21 ) is :
M = T+ , m = ( t − λ1)+ .
( 23 )
Alternately we solve Problems ( 20 ) and ( 21 ) till M converges onto its global optimal solution .
The second step is fixing M and solving L , then Problem ( 17 ) becomes : ffffffffI − M − L + ffffffff2
F
1 μ
Λ
γ ( L(∗ +
μ 2 min
L
Let N = ( I − M + 1
μ Λ ) , Problem ( 24 ) becomes γ μ
( L − N(2 F .
( L(∗ +
1 2 min
L
According to [ 5 ] , the solution of Problem ( 25 ) is :
L = U Diag((σi − γ μ
)+)V T ,
.
( 24 )
( 25 )
( 26 ) where the singular value decomposition of N is N = U ΣV T . Diag((σi − γ μ )+ ) is a diagonal matrix with i th diagonal element as ( σi − γ μ )+ .
Our algorithm to solve the new objective is summarized in Al k .
. min
C i=1 xj∈si ffffffX − U GT ffffff2
F where μi is the mean of data points belonging to Ci .
If we introduce two matrix U ∈ ffd×k and G ∈ ffn×k , where U = [ μ1 , μ2 , , μk ] and G indicates the clustering indices , then Eq ( 27 ) can be reformulated as : min G∈Ind,U ⇐⇒ min G∈Ind,U
T r(GU T U GT ) − 2T r(X T U GT )
( 28 ) gorithm 1 .
Convergence and Complexity Analysis : The convergence of ALM algorithm was proved and discussed in previous papers . Please
Since the solution of U wrt X and G is U = XG(GT G )
−1 , we have T r(GU T U GT ) = T r(X T U GT ) , thus Eq ( 28 ) can be
1248 written as :
G∈Ind,U max ⇐⇒ max G∈Ind ⇐⇒ max
F
T r(X T U GT )
T r(F T ( X T X)F )
T r((GT G )
− 1
2 GT ( X T X)G(GT G )
− 1
2 )
( 29 ) where F = G(GT G )
− 1 2 .
Note that the T r(F F T F F T ) = T r(F F T ) = T r(G(GT G )
= k , so Problem ( 29 ) is equivalent to :
−1G )
( 30 ) min
F ffffff2 ffffffF F T − X T X ffffff2 ffffffM − X T X
F
Let M = F F T , then Problem ( 30 ) can be rewritten as :
F min M∈D
( 31 ) where M ∈ D indicates some constraints on the M matrix . So the K means clustering problem is to find a matrix M meeting some requirements such that M can minimize Problem ( 31 ) .
Let ’s take further observe the properties of M . From the definition of M , where M = F F T , we can directly find that mij = 1 ni , if xi and xj belongs to the same cluster ; and mij = 0 otherwise . Also , it ’s apparent that M ≥ 0 , M T = M and M1 = 1 , that is to say , M is doubly stochastic . Moreover , we have T r(M ) = T r(F F T ) = k . 2
5 . EXPERIMENTAL RESULTS
Our structured doubly stochastic model ( SDS ) can uncover the clustering structure and directly provide the clustering results , thus the clustering performance using doubly stochastic matrix can be enhanced . In this section we evaluate the clustering performance of our method on both synthetic and benchmark datasets , and compare them to the related doubly stochastic model and spectral clustering methods .
Moreover , according to the discussion in the previous section , our model possesses interesting connection with K means clustering , we also conduct experiments to test whether our model provides an approach to better solving the K means clustering problem . 5.1 Experiments on Clustering
In this subsection , we conduct clustering experiments on our method and several related method . Our goal is to test whether the structured doubly stochastic matrix learned in our model is beneficial to improve the clustering performance under different circumstances .
511 Experimental Settings on Clustering To evaluate the clustering performance of our method , we compare with spectral clustering , i.e , Ratio Cut and Normalized Cut , as well as the doubly stochastic normalization ( DSN ) method [ 29 ] .
All comparing methods require an affinity matrix as the input . We construct the input affinity matrix with the self tune Gaussian method [ 7 ] , where the number of neighbors is set to be 5 and the value of σ is self tuned . Moreover , we let the input matrix W of our method to be initialized as shown in Eq ( 15 ) such that W is doubly stochastic . In the experiment , we set the number of clusters to be the ground truth in each dataset . In our method , we set parameter μ = 0.1 , ρ = 1.1 and r to be tuned in the range of {100 , 100.5 , , 105} .
For all methods requiring K means as the post processing step , including Ratio Cut , Normalized Cut and DSN , we give them the same 100 random initializations and compute their respective best initialization vector wrt K means objective function value . Since their performance is unstable with different initialization , we only report their respective best results in the 100 times repetition . For DSN method , we set the number of iteration as 3000 so as to get a good doubly stochastic matrix for clustering .
All experiments are conducted on a Windows system with Intel
Core i7 3770 Processor ( 8M Cache , 3.40 GHz ) .
The evaluation of different methods is based on two clustering metrics : accuracy and NMI ( Normalized Mutual Information ) .
Accuracy is the percentage of the correctly assigned labels . NMI is short for the normalized mutual information . Let L denote the denotes the predicted real label vector in a certain dataset , while L one , then
. N M I(L , L
) =
. I(L , L
) max(H(L ) , H(L
,
.
) )
( 32 )
. where I(L , L
) is the mutual information between L and L
I(L , L
.
) =
.
. li∈L
.
. j∈L l
. j ) log p(li , l
. j ) p(li , l . p(li)p(l j )
,
.
:
( 33 ) and H(L ) is the entropy of L :
H(L ) = − n . i=1 p(li ) log p(li ) .
( 34 )
512 Clustering Experiments on Synthetic Data First of all , we conduct clustering experiments on the synthetic data as a sanitary check . The synthetic dataset is a 100 × 100 matrix with four 25×25 block matrices diagonally arranged . The data within each block denotes the probability of two corresponding points from one same cluster to be connected ; while the data outside all the blocks denotes the probability of pair wise data points from different clusters to be connected , ie , noise ( which should be 0 in the ideal clustering data ) . The probability values within each block are randomly generated in the range of ( 0 , 1 ) ; while the noise data is randomly generated in the range of ( 0 , c ) , where c is set to be 0.5 and 0.6 respectively . What ’s more , to make this clustering task more challenging , we randomly pick out 25 noise data and set their value to be 1 .
Fig 1 shows the original random matrix and corresponding clustering results of SDS . We can notice that our model performs well in this task . In our approach , we successfully learn a structured doubly stochastic matrix with explicit block structure , which divides the data into exactly four clusters . After adding high level disturbance in the random data , our method still effectively recovers the clustering structure , which indicates the robustness of our model .
When the noise ratio is 0.5 , our method works out an almost perfect structured doubly stochastic matrix with four clear blocks . As the noise increases , the block structure in the original data blurs , but our model is still able to detect the intrinsic cluster structure from the data .
513 Clustering Experiments on Benchmark Datasets We evaluated the proposed double stochastic method on 7 benchmark datasets : AR [ 14 ] , FERET [ 21 ] , Yale [ 9 ] , ORL [ 23 ] , Carcino
1249 0.8 0.6 0.4 0.2
( a ) Original Graph , noise = 0.5
0.8 0.6 0.4 0.2
( c ) Original Graph , noise = 0.6
0.2 0.1 0 ( b ) SDS Result , noise = 0.5
0.2 0.1 0 ( d ) SDS Result , noise = 0.6
Figure 1 : Illustration of our clustering results on the block diagonal synthetic data with different settings of noise . On the left column shows the graph structure of the original data generated in the experiment . Figures on the right denote the structure of the doubly stochastic matrix obtained in our SDS model . graph constructed via self tune Gaussian method , which is the input for spectral clustering . To explicitly view the graph structure , here we use the two datasets with relatively small number of classes and samples as the example , ie , LEUML and SRBCTML . We present the graphs in Fig 2 . In each graph , row and columns are reshuffled such that samples from the same cluster are put together , which makes the cluster structure more clear to observe in the graph . For LEUML and SRBCTML , the r value we set for SDS is 100 and 100.5 , respectively . We can notice that the graph learned by SDS maintains the most clear block structure , and elements in each cluster tend to have similar value . These observations coincide with our theoretical analysis . Especially in the LEUML dataset , the doubly stochastic matrix learned by DSN is quite noisy , which leads to bad clustering performance shown in Table 2 . Whereas , due to the lowrank constraint on the graph Laplacian matrix , the doubly stochastic matrix learned in our SDS model has more clear block structure , which accounts for the better clustering results obtained from SDS . 5.2 Experiments on K means Task
In the K means clustering problem , a ” better" solution signifies a smaller objective function value as well as a higher clustering accuracy . Since K means problem is non convex , the quality of initialization is crucial in performing K means clustering . In this subsection , we conduct experiments on both synthetic and real benchmark datasets to demonstrate the contribution of our method in better solving the K means problem .
Table 1 : Descriptions of benchmark datasets used in our experiments .
Number of Instances Dimensions Classes
Datasets
AR
FERET
Yale ORL
Carcinomas SRBCTML
LEUML
840 1400 2414 400 174 83 72
768 1296 1024 1024 9182 2308 3571
120 200 38 40 11 4 2 mas [ 24 , 28 ] , SRBCTML [ 10 ] and LEUML1 . The detailed description of these datasets is summarized in Table 1 .
The clustering performance comparison is summarized in Table 2 . Results in Table 2 suggest that our method works very well on real benchmark datasets . Our SDS method maintains a high potential to outperform other methods on these distinct datasets . The theoretical proof in the methodology section indicates the connection between our our model and spectral clustering problem , while the experimental results here verify SDS ’s better clustering performance . This suggests SDS has the ability to better solve the spectral clustering problem . Compared with the up to date method , our method gains an obvious advantage over DSN . DSN only holds constraints on the doubly stochastic property of the learned graph but not its cluster structure , thus cannot get the optimal clustering results . On the contrary , our model learns a novel doubly stochastic matrix with explicit block structure , which performs better in clustering .
514 Clustering Results Analysis To further analyze the clustering performance , we draw the graph learned from different methods and compare their structure . We compare the graphs represented by the doubly stochastic matrix learned in DSN and SDS , respectively , and also , we display the
1http://www2statdukeedu/courses/Spring01/sta293b/datasetshtml
521 Experimental Settings on K means Task The experimental settings are similar to the settings in the clustering experiments . The different part is that in this section , we assign the clustering indicator obtained in our method as an initialization for K means and see if the K means clustering problem can be better solved with our initialization . For our method , we use W = X T X as the input matrix .
Still , the number of clusters is set to be the ground truth in each dataset . When implementing K means clustering , unless specified otherwise , the following settings are adopted : we use 100 random initializations and record the average as well as best result wrt K means objective function value in the 100 times repetition .
The evaluation is based on three metrics : accuracy , NMI and the
K means objective function value .
522 K means Experiments on Synthetic Data In the synthetic experiment , our toy data is a randomly generated multi cluster matrix . Data points in each cluster are sampled iid from the Gaussian distribution N ( 0 , 1 ) . In our experiment , we set the number of clusters to be 100 , number of samples to be 1000 , while the dimensionality to be {2 , 50 , 1000} respectively . Our goal is to partition these clusters apart with K means method . In the beginning we run K Means for 10000 times and record the minimum K means objective value and the corresponding clustering accuracy . Then we run our method once by setting the input matrix to be W = X T X and use the obtained clustering results as an initialization index vector for K means and compute the same metrics . Comparison results are summarized in Table 3 , which indicates apparent superiority of our method over K means . It shows that even after 10000 times run , the minimum K means objective value and clustering accuracy obtained by K means are still far behind the result obtained by our method with just one run . This verifies that our method is able to better solve the K means problem .
523 K means Experiments on Benchmark Datasets Still , we evaluate our model on the 7 benchmark datasets shown
1250 Table 2 : Experimental results comparison of clustering on benchmark datasets .
ACCURACY
NMI
AR
FERET
Yale ORL
Carcinomas SRBCTML
LEUML
AR
FERET
Yale ORL
Carcinomas SRBCTML
LEUML
0.358 0.255 0.396 0.625 0.695 0.434 0.903
0.358 0.249 0.387 0.653 0.724 0.434 0.903
Ratio Cut Normalized Cut DSN 0.382 0.279 0.439 0.605 0.690 0.410 0.542 Ratio Cut Normalized Cut DSN 0.705 0.682 0.604 0.783 0.707 0.132 0.079
0.677 0.647 0.561 0.799 0.719 0.169 0.547
0.700 0.674 0.570 0.794 0.697 0.160 0.547
SDS 0.404 0.280 0.448 0.663 0.718 0.446 0.917 SDS 0.706 0.683 0.605 0.814 0.712 0.187 0.585
Table 3 : K means objective function value and clustering results comparison on synthetic datasets .
K means Min_obj with SDS Initialization K means Accuracy ( min_obj )
K means Accuracy with SDS Initialization
K means Min_obj d = 2 d = 50 d = 1000
1.60 420.08 10292.00
1.15 0.14 2.7305
0.713 0.861 0.863
0.822 1.000 1.000
Table 4 : Experimental results comparison for K means problem on benchmark datasets .
K means Objective Function
Value
Accuracy
NMI
AR
FERET
Yale ORL
Carcinomas SRBCTML
LEUML
AR
FERET
Yale ORL
Carcinomas SRBCTML
LEUML
AR
FERET
Yale ORL
Carcinomas SRBCTML
LEUML
7982.58 28684.82 39578.65 6223.80 47160.00 5747.30 11364.00
K means Min_obj K means Average 8395.09 ± 124.98 29101.97 ± 215.69 40398.72 ± 337.14 663350±15662 4868200±69263 598200±16118 1139800±7878 K means Min_obj K means Average 0.285 ± 0.011 0.200 ± 0.005 0.110 ± 0.006 0490±0031 0571±0050 0446±0070 0740±0058 K means Min_obj K means Average 0.621 ± 0.009 0.581 ± 0.006 0.163 ± 0.009 0711±0018 0591±0042 0187±0081 0225±0112
0.640 0.598 0.170 0.754 0.648 0.106 0.182
0.310 0.206 0.111 0.568 0.672 0.374 0.708
K means with
SDS Initialization
7562.62 26420.99 39381.64 5943.80 46787.00 5747.30 11364.00
SDS 0.343 0.234 0.114 0.638 0.695 0.458 0.736 SDS 0.688 0.638 0.178 0.781 0.704 0.261 0.237 in Table 1 . We summarize the K means performance comparison of K means clustering and our method in Table 4 . From Table 4 , we can notice that our method improves the performance of Kmeans on real benchmark datasets . On all dataset , our SDS method performs equally or even better than K means clustering . These re sults demonstrates that our model makes a good way to better solve the K means clustering problem . By adopting the cluster indicator learned in our model as the initialization , not only is the K means objective function value reduced , but the clustering performance is also boosted to a large extent .
1251 5.3 Experiments on Convergence Analysis
In this subsection , we analyze the influence of parameter r in Eq ( 6 ) to the convergence of our algorithm . To save space , we just take two datasets , Carcinomas and ORL , as an example . We apply our method to these benchmark datasets with three different r values ( ie , 10 , 103 and 105 ) and record the objective value of our model in each iteration .
The convergence results are presented in Fig 3 . We can notice that no matter what the r value is , our model always converges within about 80 iterations , which indicates the fast convergence of our algorithm .
6 . CONCLUSIONS
In this paper , we proposed a novel structured doubly stochastic model with rank constraint on the graph Laplacian matrix . The doubly stochastic matrix learned in our model possesses explicit clustering structure , from which we can immediately partition data points into k connected components , where k is the number of clusters . The doubly stochastic property guarantees the effectiveness of the learnt similarity matrix while the rank constraint on the graph Laplacian matrix enhances the clustering ability . The quality of the learnt graph was verified by extensive experimental results , which suggested the feasibility of our model . What ’s more , we theoretically and empirically proved that our method made its own contribution in better solving the K means and spectral clustering problem .
Appendix A
LEMMA 2 . The following gives the global optimal solution to
Problem ( 20 ) : n + 1T K1
M = K +
K =
T + T T
2 n2
.
11T − 1 n
K11T − 1 n
11T K ,
Proof : With the Lagrangian function , Problem ( 20 ) can be rewrit ten as : 1 2
( M − T(2
−(λT ( M1−1))−T r(ΛT ( M T −M ) ) , ( 35 ) min M where Λ ∈ ffn×n and λ ∈ ffn are Lagrange multipliers .
F
Taking derivative wrt M and set it to 0 , we have : M − T − ( ΛT − Λ ) − λ1T = 0 .
Compute transpose on both sides of Eq ( 36 ) , we have :
M − T T + ( ΛT − Λ ) − 1λT = 0 .
Subtracting Eq ( 36 ) from Eq ( 37 ) , we get :
T − T T + 2(ΛT − Λ ) + λ1T − 1λT = 0
=⇒ −(ΛT − Λ ) = 1
2 ( T − T T + λ1T − 1λT ) .
Combining Eq ( 36 ) with Eq ( 38 ) , we further get :
2(M − T ) + ( T − T T + λ1T − 1λT ) − 2λ1T = 0
=⇒ 2M − T − T T − λ1T − 1λT = 0 .
Multiply the vector 1 on both sides of Eq ( 39 ) , we have :
21 − T 1 − T T 1 − nλ − 1λT 1 = 0 .
( 40 ) Since λT 1 is a number , it is apparent that ( λT 1)T = λT 1 , thus :
21 − T 1 − T T 1 − nλ − 11T λ = 0 .
From Eq ( 41 ) we can obtain the solution of λ as follows :
λ = ( 11T + nI )
−1(21 − T 1 − T T 1 ) .
( 42 )
To enhance the computing speed , we compute the inverse term in Eq ( 42 ) by means of the Woodbury formula [ 27 ] : −1U )
−1V A Thus the inverse term in Eq ( 42 ) can be rewritten as :
−1+V A
−1 = A
−1U ( C
−1−A
( A+U CV )
−1 . ( 43 )
( 11T + nI )
−1 = ( − 1
2n2 11T +
1 n
I ) .
( 44 )
From Eq ( 44 ) , we can rewrite Eq ( 42 ) as follows :
λ = ( − 1
2n2 11T + 1 − ( − 1
I)(21 − T 1 − T T 1 )
1 n
=
1 n
2n2 11T +
1 n
I)(T + T T )1 .
( 45 )
Plugging the solution of λ in Eq ( 45 ) to Eq ( 39 ) , we get :
2M = T + T T + λ1T + 1λT
= T + T T +
−11T ( T + T T )(− 1
11T − ( − 1
2 n
2n2 11T + 1 I ) n
2n2 11T + 1T T T 1
= T + T T +
2 n T 11T − 1 n
11T + n2 T T 11T − 1 n
1T T 1 n2 11T 11T T
11T + 11T T T − 1 n
− 1 n
1 n
I)(T + T T )11T
Let K = T +T 2
T
, then we can rewrite the above equation as :
M = K + n + 1T K1 n2
11T − 1 n
K11T − 1 n
11T K ,
( 46 ) which is the global optimal solution of Problem ( 20 ) . Obviously , M is symmetric and
M1 = K1 + n + 1T K1 n
1 − K1 − 1 n
11T K1 = 1 ,
( 47 ) which means that the solution of M in Eq ( 46 ) meets the requirements of Problem ( 20 ) .
Specially , when T is symmetric , the solution of M is :
M = T + n + 1T T 1 n2
11T − 1 n
T 11T − 1 n
11T T .
( 48 )
2
Appendix B
LEMMA 3 . The following gives the global optimal solution to
Problem ( 21 ) :
M = T+ , m = ( t − λ1)+ . where m = diag(M ) .
Proof : Require M = T+ , then Problem ( 21 ) could be rewritten as follows :
( m − t(2
2 min m st m ≥ 0 , mT 1 = k ,
( 49 )
( 50 )
( 36 )
( 37 )
( 38 )
( 39 )
( 41 ) where m = diag(S ) and t = diag(T ) .
1252 [ 14 ] A . Martinez and R . Benavente . The ar face database .
Technical report , CVC Technical report , 1998 .
[ 15 ] B . Mohar . The laplacian spectrum of graphs . In Graph
Theory , Combinatorics , and Applications , pages 871–898 . Wiley , 1991 .
[ 16 ] J . V . Neumann . Functional Operators , volume 2 . 1950 . [ 17 ] A . Y . Ng , M . I . Jordan , and Y . Weiss . On spectral clustering :
Analysis and an algorithm . In NIPS , pages 849–856 , 2001 . [ 18 ] F . Nie , H . Wang , H . Huang , and C . Ding . Unsupervised and semi supervised learning via l1 norm graph . In IEEE Conference on Computer Vision , pages 2268–2273 , 2011 . [ 19 ] F . Nie , X . Wang , and H . Huang . Clustering and projected clustering via adaptive neighbor assignment . The 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining ( KDD 2014 ) , pages 977–986 , 2014 .
[ 20 ] F . Nie , X . Wang , M . Jordan , and H . Huang . The constrained laplacian rank algorithm for graph based clustering . Thirtieth AAAI Conference on Artificial Intelligence ( AAAI 2016 ) , pages 1969–1976 , 2016 .
[ 21 ] P . Phillips , H . Wechsler , J . Huang , and P . Rauss . The FERET database and evaluation procedure for face recognition algorithms . Image and Vision Computing , 16(5):295–306 , 1998 .
[ 22 ] M . J . D . Powell . A method for nonlinear constraints in minimization problems . In R . Fletcher , editor , Optimization . Academic Press , London and New York , 1969 .
[ 23 ] F . S . Samaria and A . C . Harter . Parameterisation of a stochastic model for human face identification . In Applications of Computer Vision , 1994 . , Proceedings of the Second IEEE Workshop on , pages 138–142 . IEEE , 1994 .
[ 24 ] A . I . Su , J . B . Welsh , L . M . Sapinoso , S . G . Kern ,
P . Dimitrov , H . Lapp , P . G . Schultz , S . M . Powell , C . A . Moskaluk , H . F . Frierson , and G . M . Hampton . Molecular Classification of Human Carcinomas by Use of Gene Expression Signatures . Cancer Res . , 61(20):7388–7393 , 2001 .
[ 25 ] U . Von Luxburg . A tutorial on spectral clustering . Statistics and computing , 17(4):395–416 , 2007 .
[ 26 ] F . Wang , P . Li , and A . Konig . Learning a bi stochastic data similarity matrix . IEEE International Conference on Data Mining , pages 551–560 , 2010 .
[ 27 ] M . A . Woodbury . Inverting modified matrices . 1950 . [ 28 ] K . Yang , Z . Cai , J . Li , and G . Lin . A stable gene selection in microarray data analysis . BMC bioinformatics , 7(1):228 , 2006 .
[ 29 ] R . Zass and A . Shashua . Doubly stochastic normalization for spectral clustering . In NIPS , pages 1569–1576 , 2006 .
[ 30 ] D . Zhou , O . Bousquet , T . N . Lal , J . Weston , and
B . Schölkopf . Learning with local and global consistency . In NIPS , 2004 .
[ 31 ] X . Zhu , Z . Ghahramani , and J . D . Lafferty . Semi supervised learning using Gaussian fields and harmonic functions . In ICML , pages 912–919 , 2003 .
Similarly , we can use the Lagrangian function to solve Problem
( 49 ) and define :
G(m , λ , η ) =
( m − t(2
2 − λ(mT 1 − k ) − ηmT ,
( 51 )
1 2 where λ ∈ ffn and η ∈ ffn are Lagrange multipliers .
Taking derivative wrt m and set it to 0 , then we can solve m in Problem ( 51 ) as follows : m − t − λ1 − η = 0
=⇒ m = ( t − λ1)+ .
( 52 )
2
7 . REFERENCES [ 1 ] R . Angelova and G . Weikum . Graph based text classification : Learn from your neighbors . annual international ACM SIGIR conference on Research and development in information retrieval , pages 485–492 , 2006 .
[ 2 ] M . Belkin and P . Niyogi . Laplacian eigenmaps for dimensionality reduction and data representation . Neural Computation , 15(6):1373–1396 , 2003 .
[ 3 ] D . P . Bertsekas . Constrained optimization and lagrange multiplier methods . Athena Scientific , 1996 .
[ 4 ] D . P . Bertsekas et al . Augmented lagrangian and differentiable exact penalty methods . 1981 .
[ 5 ] J F Cai , E . J . Candès , and Z . Shen . A singular value thresholding algorithm for matrix completion . SIAM Journal on Optimization , 20(4):1956–1982 , 2010 .
[ 6 ] A . Celikyilmaz , M . Thint , and Z . Huang . A graph based semi supervised learning for question answering . the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP , pages 719–727 , 2009 .
[ 7 ] W Y Chen , Y . Song , H . Bai , C J Lin , and E . Y . Chang . Parallel spectral clustering in distributed systems . IEEE Transactions on Pattern Analysis and Machine Intelligence , 33(3):568–586 , 2011 .
[ 8 ] F . R . K . Chung . Spectral Graph Theory . CBMS Regional
Conference Series in Mathematics , No . 92 , American Mathematical Society , February 1997 .
[ 9 ] A . Georghiades , P . Belhumeur , and D . Kriegman . From few to many : Illumination cone models for face recognition under variable lighting and pose . IEEE Trans . Pattern Anal . Mach . Intelligence , 23(6):643–660 , 2001 .
[ 10 ] J . Khan , J . S . Wei , M . Ringner , L . H . Saal , M . Ladanyi ,
F . Westermann , F . Berthold , M . Schwab , C . R . Antonescu , C . Peterson , et al . Classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks . Nature medicine , 7(6):673–679 , 2001 .
[ 11 ] D . Luo , C . Ding , and H . Huang . Forging The Graphs : A Low
Rank and Positive Semidefinite Graph Learning Approach . Advances in Neural Information Processing Systems ( NIPS ) , pages 2969–2977 , 2012 .
[ 12 ] D . Luo , C . Ding , F . Nie , and H . Huang . Cauchy graph embedding . International Conference on Machine Learning , pages 553–560 , 2011 .
[ 13 ] J . MacQueen et al . Some methods for classification and analysis of multivariate observations . In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability , volume 1 , pages 281–297 . Oakland , CA , USA . , 1967 .
1253 0.2
0.15
0.1
0.05
0
( a ) Graph for Normalized Cut in LEUML
0.3
0.2
0.1
0.06
0.04
0.02
0 ( b ) Graph Learned by DSN in LEUML
0 ( c ) Graph Learned by SDS in LEUML
0.25
0.2
0.15
0.1
0.05
0
( d ) Graph for Normalized Cut in SRBCTML
0.5
0.4
0.3
0.2
0.1
0 ( e ) Graph Learned by DSN in SRBCTML
0.25
0.2
0.15
0.1
0.05
0
( f ) Graph Learned by SDS in SRBCTML
Figure 2 : Illustration of the graph learned from different methods on LEUML and SRBCTML datasets . Rows and columns of the graph are reshuffled respectively such that data points belonging to the same cluster are put together . l e u a V n o i t c n u F e v i t c e b O j
900
800
700
600
500
400
300
200
0
50
100
Carcinomas
150
200
250
300
350
Number of Iterations
2400
2300
2200
2100
2000
1900
1800 l e u a V n o i t c n u F e v i t c e b O j
400
450
500
1700
0
50
100
Carcinomas x 105
1.587
Carcinomas l e u a V n o i t c n u F e v i t c e b O j
1.586
1.585
1.584
1.583
1.582
1.581
1.58
400
450
500
1.579
0
50
100
150
200
250
300
Number of Iterations
350
400
450
500
150
200
250
300
Number of Iterations
350
( a ) r = 10 in Carcinomas
( b ) r = 103 in Carcinomas
( c ) r = 105 in Carcinomas
2000
1800
1600
1400
1200
1000
800
600
400
0
50
100
ORL
6800
6600
6400
6200
6000
5800
5600
5400
ORL x 105
4.832
ORL l e u a V n o i t c n u F e v i t c e b O j
4.83
4.828
4.826
4.824
4.822
4.82
4.818 l e u a V n o i t c n u F e v i t c e b O j
200
150 350 Number of Iterations
250
300
400
450
500
5200
0
50
100
200
150 350 Number of Iterations
250
300
400
450
500
4.816
0
50
100
200
150 350 Number of Iterations
250
300
400
450
500 l e u a V n o i t c n u F e v i t c e b O j
( d ) r = 10 in ORL
( e ) r = 103 in ORL
( f ) r = 105 in ORL
Figure 3 : Objective function value of Eq ( 6 ) with different r parameters in each iteration on Carcinomas and ORL datasets .
1254
