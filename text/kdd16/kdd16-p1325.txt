Annealed Sparsity via Adaptive and Dynamic Shrinking
Kai Zhang§ , Shandian Zhe† , Chaoran Cheng , Zhi Wei , Zhengzhang Chen§
Haifeng Chen§ , Guofei Jiang§ , Yuan Qi† , Jieping Ye‡
§NEC Laboratories America , Princeton NJ †Dept . Computer Science , Purdue University
Dept . Computer Science , New Jersey Institute of Technology
‡Dept . Computational Medicine & Bioinformatics , University of Michigan , Ann Arbor §{kzhang,zchen,haifeng,gfj}@nec labs.com , †{szhe,alanqi}@purdue.edu
{cc424,zhiwei@njitedu} , ‡jpye@umich.edu
ABSTRACT Sparse learning has received tremendous amount of interest in high dimensional data analysis due to its model interpretability and the low computational cost . Among the various techniques , adaptive 1 regularization is an effective framework to improve the convergence behaviour of the LASSO , by using varying strength of regularization across different features . In the meantime , the adaptive structure makes it very powerful in modelling grouped sparsity patterns as well , being particularly useful in high dimensional multi task problems . However , choosing an appropriate , global regularization weight is still an open problem . In this paper , inspired by the annealing technique in material science , we propose to achieve “ annealed sparsity ” by designing a dynamic shrinking scheme that simultaneously optimizes the regularization weights and model coefficients in sparse ( multi task ) learning . The dynamic structures of our algorithm are twofold . Feature wise ( “ spatially ” ) , the regularization weights are updated interactively with model coefficients , allowing us to improve the global regularization structure . Iteration wise ( “ temporally ” ) , such interaction is coupled with gradually boosted 1 regularization by adjusting an equality norm constraint , achieving an “ annealing ” effect to further improve model selection . This renders interesting shrinking behaviour in the whole solution path . Our method competes favorably with state of the art methods in sparse ( multi task ) learning . We also apply it in expression quantitative trait loci analysis ( eQTL ) , which gives useful biological insights in human cancer ( melanoma ) study .
Keywords Sparse regression , adaptive LASSO , multi task LASSO , regularization path , sparse multi task learning
1 .
INTRODUCTION
With the rapid development of data acquisition technolo gies in various science and engineering domains such as imaging , physics , biology , and computer networks , we are having access to digital data of unprecedented amount and quality . In this modern paradigm , a significant challenge for data discovery is the huge number of features in representing objects . For example , a high resolution image is composed of millions of pixels ; the micro array data in human genomic study typically includes tens of thousands of gene expressions ; in movie recommendation systems the number of movies can be tens of millions . How to identify truly relevant features in the huge feature pools for accurate learning and prediction has become one of the key challenges in data mining .
Sparse regression has recently emerged as a powerful tool for high dimensional data analysis , especially in removing irrelevant variables and identifying a parsimonious subset of covariates for predicting the target [ 29 , 38 , 39 ] . Given a response vector y = [ y1 , y2 , , yn ] and predictors X ∈ Rn×D , where without loss of generality the data is centered , sparse regression and in particular the LASSO solves the following regularized linear regression problem :
Xβ − y2
2 + λ|β|1 ,
β min
( 1 ) where β ∈ RD×1 is the regression coefficient vector . The 1norm |β|1 is used to enforce the sparsity of solution , making the model easy to interpret . In the meantime , recent advances in solving the non smooth , convex LASSO problem has made it computationally extremely efficient [ 14 , 12 ] . Therefore the LASSO and related methods have been applied with great success in a number of domains including bioinformatics [ 17 , 24 , 35 , 36 ] , imaging and computer vision [ 33 , 21 , 9 ] , and signal processing [ 5 , 6 ] .
It is shown that LASSO can perform automatic variable selection because the 1 penalty is singular at the origin [ 11 ] . It was also shown that variable selection with the LASSO is consistent only under certain conditions [ 22 , 37 ] . Namely , there are scenarios in which the LASSO selection is not consistent . To fix this problem , [ 38 ] proposes to use adaptive weights to regularize the model coefficients along different features , as
Xβ − y2
2 + λ · | ˆw fi β|1 ,
β min
( 2 ) where ˆw ∈ RD×1 is the regularization weight , and | ˆwfiβ|1 = i ˆwi|βi| , namely each dimension of β is penalized differently instead of sharing a single regularization parameter λ as in LASSO ( 1 ) . The regularization weights ˆw can be chosen as ˆw = |βols|−γ , where βols is the ordinal least square solution ,
1325 and γ is a positive number . Such choice renders the oracle property of the adaptive LASSO estimator in simultaneous variable selection and prediction .
Besides improving the asymptotic behaviour of sparse model estimation , the adaptive LASSO can also be quite useful in imposing structured sparsity patterns , in particular in highdimensional multi task learning by sharing the same adaptive weight across different tasks . Therefore it has gained a lot of research interest from various domains [ 10 , 17 , 19 ] . However , choosing an optimal regularization weight vector ˆw ( 2 ) can be much more challenging than selecting a single regularization parameter λ ( 1 ) . The former has a significantly larger search space , and is the key to the superior performance of adaptive LASSO .
In this paper , we propose a novel approach to simultaneously compute the model coefficients and adaptive regularization weights in 1 regularized regression , in comparison to most existing methods that address them separately . The basic idea is to adopt an alternating optimization framework to establish the closed form relations between model coefficients and regularization weights ( under an equality norm constraint of the latter ) . By doing this , the two sets of parameters can then be optimized iteratively , until an equilibrium state is obtained upon convergence .
The interactive updating scheme can acquire greater flexibility in tuning the sparse model . In the meantime , to further improve its convergence and reduce the sensitivity on initial conditions , we borrow the idea from material science and propose an “ annealed ” shrinking procedure . Specifically , throughout the interactive updates between model coefficients and regularization weights , we gradually strengthen the global magnitude of 1 penalization by adjusting the equality norm constraint on the regularization weight vector . Then , by starting from a dense solution , the system will go through a series of micro stages that continuously sparsify and ameliorate itself . In this “ annealing ” process , features are like particles : in the “ high temperature ” beginning , all features have the freedom to compete with each other and position themselves in the model ; however , when the system gradually cools down , fewer and fewer features could preserve their energy ; finally , only those features that survive the dynamic competing process will be selected .
We find that such a dynamic shrinking scheme leads to an interesting mechanism of feature selection and competition , which favors the choice of truly relevant features . Through extensive experiments , we compare our approach with a number of state of the art techniques in sparse learning , and obtain promising results . The contribution of the paper is summarized as follows :
1 . We introduce the concept of “ annealing ” in sparse learning , and propose an annealed , dynamic shrinking framework to improve the model selection in 1 regression ;
2 . We extend our approach to solve high dimensional multi task learning problems to improve state of the art ;
3 . We apply our approach in eQTL , which successfully identifies significant and relevant pathways to help understand the P53 regulation mechanism for melanoma .
The rest of the paper is structured as follows . Section 2 discusses the proposed method . Section 3 extends it to multitask learning scenario . Section 4 describes related methods .
Empirical results are presented in Section 5 , and the last section concludes the paper .
2 . METHOD Consider the following linear regression problem with the design matrix X ∈ Rn×D , where n is the sample size and D is the dimensionality , and the target response vector y ∈ Rn×1 . We use an adaptive weight vector w = [ w1 , w2 , , wD ] to regularize over different covariates , as −γ fi β| 2 + |w wd = ω , wd ≥ 0 .
Xβ − y2 min w,B st
( 3 )
( 4 ) d
−γ d
−γ d=1 w d parameters will be optimized in our learning procedures .
Here , β ∈ RD×1 is the model , w ∈ RD×1 is the regulariza· |βd| . Both tion weight vector , and |w−γ fi β| =D The equality norm constraint d wd = ω is quite useful in controlling the global strength of regularization ( in an average sense ) . To see this , note that the regularization imposed on the dth feature is w ( 3 ) . Therefore , if γ is positive : then the larger the ω , the smaller the average strength of the 1 penalty ; on the other hand , if γ is negative : then the larger the ω , the larger the average strength of 1 penalty . Later we shall see that , it is exactly because of this equality norm constraint ( 4 ) that we acquire the flexibility of “ annealing ” the whole system to improve the state of solution . The power parameter γ can be chosen either as a positive or negative real number , in comparison to the power parameter that can only be positive in the adaptive LASSO [ 39 ] . 2.1 Interactive Update
We first consider ω as a pre defined constant . Then the problem ( 1 ) can be solved by alternating optimization . Namely we first fix w and solve β , and then fix β and solve w , and keep iterating until convergence . Here we use βd to denote the dth entry of β .
Fix w and solve β . Then this becomes an adaptive
LASSO problem ,
Xβ − y2
2 + |w
−γ fi β| , min
β
( 5 ) which can be computationally converted to a standard LASSO problem [ 38 ] .
Fix β and solve w . This then become the following d
 θ dD wd =
1
1+γ j=1 θ
1
1+γ j constrained optimization problem βd · w −γ d , θd = |βd| . min w
Problem ( 6 ) has a closed form solution ,
 ω .
( 6 )
( 7 )
The derivations are in the appendix .
Choice of the γ Parameter . Based on equation ( 7 ) , we can examine the relation between the actual regularization imposed in ( 3 ) , w−γ , and the ( absolute ) value of the model coefficient , θd ( 4 ) . We discuss the following scenarios :
1 . γ > 0 : if θd is larger ( compared with θd , d = d ) , then wd ( 7 ) will also be larger due to the positive power term
1326 1 1+γ , and as a result the regularization term w in ( 3 ) will be smaller , leading to a weaker regularization on the d feature in the next iteration ;
−γ d
2 . γ < −1 :
= d ) , if θd is larger ( compared with θd , d 1 then wd will be smaller due to the negative power 1+γ ; d will also be smaller since −γ > 0 , −γ as a result w leading to a weaker regularization in the next iteration ; 3 . −1 < γ < 0 : if θd is larger ( compared with θd , d = d ) , 1 1+γ ; so then wd will be larger due to the positive power d will be larger since −γ > 0 , leading to a stronger −γ w regularization in the next iteration .
As can be seen , in case γ > 0 or γ < −1 , the regularization −γ and the model coefficient θd are inversely related term w d to each other : larger θd will lead to smaller regularization −γ coefficient w d , and vice versa . In practice , we update wd and θd iteratively . As a result , important features in the current iteration will tend to be penalized less in the next iteration ; on the contrary , less important features will be confronted with strong penalty in future iterations . The system reaches a stationary point upon convergence .
In case −1 < γ < 0 , however , w
−γ d and θd will be favorably associated with each other . In other words , relevant features in the current step will be strongly penalized in the next iteration . This obviously leads to unstable iterations and therefore we will exclude it from our parameter choice .
In the adaptive LASSO [ 38 ] , the regularization weight is also inversely related to some pre computed model coefficient . The difference of our method is that , first , our weights are carefully tuned based on previous model coefficients through norm regularization ( 7 ) ; second , we keep alternating instead of using a single update ; third , as will be discussed , we have the freedom of annealing the strength of global regularization via the equality norm constraint ( 4 ) . 2.2 Multi Stage Shrinking
The interactive updates between models and the adaptive weights mimic a self adapting process that is expected to drive the whole system to a desired state . However , this optimization problem is non convex , therefore in case of bad initialization , the iterations might quickly get stuck into a local optimum . In this case , dimensions with large model coefficients will keep being dominant and and dimensions with small coefficients may never have a chance to regain their magnitudes .
In order to prevent pre mature convergence , we propose a multi stage shrinking procedure . The basic idea is to introduce strong perturbations in the beginning , such that all features have the chance to be selected and compete with each other . Then we gradually “ cool down ” the system by using stronger and stronger 1 penalties . Namely fewer and fewer features can survive in the progressive shrinking . By doing this , the system will go a series of self adjusting microstages sequentially before reaching the final solution . Suppose we initialize w with |w| = ωτ , τ = 0 . Then we interactively update β ( 5 ) and w ( 6 ) under this equality norm constraint until convergence . When this stage ends , we will start next stage of iterations with an updated norm constraint |w| = ωτ , τ = 1 , which imposes a stronger 1penalty . Then we iterate between β and w until the second stage ends . By repeating this , we keep strengthening the global 1 norm regularization stage by stage , achieving an
“ annealing ” effect . Here each stage is indexed by τ and is composed of iterations under |w| = ωτ . Depending on the choice of γ , in order to guarantee that the global regularization strength w−γ will gradually increase , we need different strategies in tuning the ω parameter . ( 1 ) γ > 0 : ω will start from a large value ( corresponding to a weak regularization ) and gradually decrease ; ( 2 ) γ < −1 : ω will start from a small value and gradually increase throughout the iterations . 2.3 Relation with Annealing
In material science , annealing is a powerful heat treatment technique [ 31 ] to improve physical and chemical properties of a metal . It heats the metal to a high temperature , which gives the energy for its atoms to break the bond and diffuse actively within crystal lattices ; a suitable temperature is then maintained and gradually cooled down , allowing the material to progress towards equilibrium state . Annealing can reduce the Gibbs Free Energy of the metal .
The dynamic shrinking method in Algorithm 1 very much resembles ( and is indeed inspired by ) an annealing process . The strength of the 1 regularization can be deemed as controlling the temperature of the system : in the beginning stages when regularization is weak , all features have the freedom to compete and position themselves in the model , meaning that the solution is dense and the system has a high energy . When the regularization gradually enhances , the system begins to cool down , model coefficients start shrinking progressively , and the system energy decreases as well . The norm constraint |w| = ω can be deemed exactly as the as controlling the “ temperature ” of the system : a larger ω imposes a weak regularization , meaning high temperature and energy status ; a smaller ω , on the contrary , enforces low temperature and energy status .
The initial temperature of annealing should be higher than metal recrystallization temperature . Similarly , we also start from a high temperature , ie , a weak regularization such that initial model parameters are dense . This allows different features to fully compete with each other ; if the solution is already sparse in the beginning , the iterations will quickly get trapped into a poor local optima . In our context , the densest initial solution is the ordinary least square solution , namely a sparse regression with vanishing 1 penalties .
It is worthwhile to point out the difference between our method and simulated annealing [ 15 ] . Simulated annealing is a probabilistic searching technique that can be applied to any pre defined objective function to find its global optimum [ 25 ] ; in comparison , we target on more effective sparse regression and feature selection by reformulating the adaptive LASSO with a progressive , multi stage shrinking mechanism , thus bearing an analogy to the “ annealing ” process .
3 . MULTI TASK REGRESSION
Suppose we have a number of k tasks , each task is composed of the design matrix Xk ∈ Rnk×D and target yk ∈ Rnk×1 ; we use shared adaptive weight w = [ w1 , w2 , , wD ] to regularize over all the K tasks , as
+ |w
−γ fi βk| flflflXkβk − ykflflfl2
2 wd = ω , wd ≥ 0 .
K k=1 st d min w,B
( 8 )
( 9 )
1327 Algorithm 1 : Adaptive LASSO + dynamic shrinking Input : multi task data Z = {Xk , yk}K k=1 ; initial norm constraint ω0 ; shrinking factor δ ; τ = 0 ; initial regularization weight w0
D , ω0 Output : solution path for all the k tasks
0 = [ ω0
D , ω0 D ] ;
1 begin 2 while model is unempty do
3
4
5
6
7
8
9
10 t = 0 ; while Convergence do t+1 =ModelUpdate(wτ t+1 =WeightUpdate(Bτ t , Z ) ; t+1 , ωτ ) ;
Bτ wτ t = t + 1 ; ωτ +1 = ωτ · δ ; wτ +1 0 = wτ t ; τ = τ + 1
Here , βk ∈ RD×1 is the model coefficients for the kth task for k = 1 , 2 , , K , and B = [ β1 , β2 , , βk ] . Through similar derivations , we have the following procedures .
Fix w and solve βk ’s . Then this becomes K indepen dent adaptive LASSO problems , for k = 1 , 2 , , K
−γ fi βk|
( 10 ) which can be easily converted to a standard LASSO .
Fix βk ’s and solve w . This then becomes the following
2 min βk
+ |w flflflXkβk − ykflflfl2 constrained optimization problem θd · w θd =K k=1 |βk d| .  θ dD wd =
−γ d min
1+γ w d
1
1 j=1 θ
1+γ j
 ω .
Problem ( 11 ) has a closed form solution ,
( 11 )
( 12 )
( 13 )
As can be seen , the proposed method can conveniently handle multi task learning scenarios , thanks to the flexibility of using an adaptive regularization weight . In the following we introduce two routines to simplify our presentation of the algorithm .
• B =ModelUpdate(w , Z ) . This denotes training an adaptive LASSO with weights w ( 10 ) for each of the k tasks in Z = {Xk , yk}K k=1 , independently , and obtaining the model coefficients B = [ β1 , β2 , , βK ] ;
• w =WeightUpdate(B , ω ) . This denotes the process of using current models B and a specified value of ω to update the regularization weights w , as in ( 11 ) to ( 13 ) .
Using these notations , we summarize the algorithm in Algorithm 1 , which is applicable to both single and multiple tasks . Here the upper index τ denotes outer iterations , where each iteration τ corresponds to a stage with distinct value of ω ; the lower index t indexes the inner iterations inside each stage . The δ is a shrinking factor that is smaller than 1 when γ > 0 , and a growing factor that is larger than 1 when γ < −1 . The iteration will keep going until all features are removed from the model . Then a cross validation can be used to select the best model along the solution path . min−n
In case of classification tasks with high dimensional features , one can consider the sparse logistic regression [ 27 ] , i=1 ln,1 + exp[−βxi · yi] +|wfiβ|1which can ben efit from our dynamic shrinking approach as well . Similarly , the iterative procedures will decompose into two subproblems : when fixing w , it becomes a standard logistic regression with adaptive 1 regression ; and when fixing β , the problem is identical to ( 6 ) and can be solved accordingly .
4 . RELATED METHODS 4.1 Re weighted LASSO i i i
| + ) , and then use w(t+1 )
In [ 4 ] , an interesting , re weighted LASSO algorithm was proposed to improve the sparsity of LASSO . After solving a standard LASSO at time t ( starting from t = 0 ) , it will compute a set of adaptive regularization weights w(t+1 ) = 1/(|β(t ) ’s to adaptively penalize the 1 regularization . Here is a small number to ensure that a zero component in β does not strictly prohibit a nonzero estimate at the next step . As can be seen , the algorithm repeatedly performs the adaptive LASSO by using the absolute value of the inverse of previous model coefficients as the regularization weights for the next iteration . Such iterations may easily get trapped in local optimal solution due to the sensitivity of the convergence on initial values . In comparison , our approach avoids pre mature convergence by continuously adjusting the global regularization strength . 4.2 Mixed norm Regularization 421 Univariate Regression Cases In case there exists grouping structures among input variables , the LASSO algorithm has been extended to recover such grouping . For example , the elastic net algorithm penalizes both the 1 and 2 norm of the model [ 39 ] , which encourages a grouping effect such that strongly correlated predictors tend to be in or out of the model together . When the groupings of the inputs are available as prior knowledge , the group LASSO [ 34 ] penalizes the 2 norm of each group as a unit for variable selection , using the following optimization framework , flflflL l=1 min flflfl2
F
L i=1
Xlβl − y
+ λ
·√ plβl2 .
Here , the predictors are assumed to have l groups with group size pl ; Xl represents predictors of the lth group , with corresponding coefficient βl . The group LASSO achieves sparse feature selection at the group level : depending on λ , an entire group of predictors is either selected simultaneously in the model , or will be removed together . 422 Multi variate/Multi task Cases Multi task learning has drawn considerable interest in data mining [ 2 , 3 ] . It assumes that different tasks share some common structures , and enforcing the task relatedness can help improve the learning performance . We focus on sparse multi task learning [ 16 , 23 , 34 , 17 ] , namely joint feature selection in multiple tasks needs to be performed .
The 1/2 penalty of group lasso has been used to recover inputs that are jointly relevant to all of the outputs , or tasks , by applying the 2 norm to outputs instead of groups of inputs.For example , [ 23 ] proposed to penalize the sum of the q norms of the blocks of coefficients associated with each
1328 feature across tasks , which is called mixed norm or 1/q regularization . One appealing property is that it encourages multiple predictors from different tasks to share similar parameter sparsity patterns . Let B = [ β1 , β2 , , βk ] , and define Bi ∈ R1×K as the ith row in the model coefficient matrix B . Then the objective function of the 1/q regularization is as follows :
K Here B1/q min
B k=1 flflflXkβk − ykflflfl + λ D K
D
= is the block 1/q norm i=1
Bij
.
B1/q q 1 q
.
B1/q
= i=1 j=1
When q = 2 , we have a block 1/2 norm , which is identical to the group LASSO [ 34 ] . Other choices have also been studied such as 1/∞ [ 30 ] . The mixed norm regularization encourages simultaneous feature selection across tasks . Namely , a given feature is either selected as relevant for all the tasks’ output simultaneously , or is excluded all together for all the tasks . Such regularization is very effective if the underlying task relation satisfies such assumption . However it can be too restrictive in some other applications .
In [ 17 ] , an adaptive multi task LASSO framework was proposed which combines adaptive regularization with the mixed norm regularization , as
D
K
θj
D
L(β ) + λ1 min β,θ,ρ
|βk j | + λ2
ρjβj2 + log Z(θ , ρ ) . j=1 i=1 j=1
Here L is the loss function ; the second term is an adaptive LASSO that imposes 1 norm penalty with strength λ1·θj on |βj|1 from all tasks ; the third term is a mixed norm regularization together with an adaptive weights , which imposes the penalty λ2 · ρj βj2 ; the last term is a normalization factor on the conditional probability p(β|θ , ρ ) . The whole framework has an elegant Bayesian interpretation . It achieves sparsity both across tasks and within each task . However , the regularization weights are assumed to be spanned by features from extra domains with prior knowledge , which might not be available in general multi task learning ; on the other hand , it separates the learning of the model and the regularization profile . 4.3 Regularization Path
The dynamic shrinking process of the proposed algorithm is illustrated in Figure 1 , where the strength of the 1regularization gradually increases , leading to a solution path . Due to the interplay between adaptive weights w and models coefficients B , the whole solution path of B is connected : each solution B is affected by its predecessor . This means , the effect of system evolution is inherited from one stage τ to the next stage τ + 1 , or from one iteration t to the next iteration t + 1 inside a single stage . In other words , the solutions have to be obtained in a sequential manner . For the standard LASSO , in comparison , the solution path can actually be obtained by training a number of independent LASSO ’s with different λ ’s .
Note that the solution path of LASSO can also be obtained in a sequential manner by using the least angle regression [ 8 ] , which fully exploits the piecewise linear structures of the solutions . However , an important difference is that , our approach will re define the LASSO regression in each iteration . To see this , note that any adaptive LASSO problem minXβ − y2 2 + |w fi β|1 can be converted to a LASSO minXW−1β − y2 2 + |β|1 where β = w fi β and W = diag(w ) . In our approach , since the regularization weight vector w keeps updating , therefore each iteration is equivalent to a LASSO problem with continuously rectified data XW−1 , making it different from traditional solution path . It will be a very interesting topic to explore the solution path structures of our dynamic shrinking approach , so as to make it more computationally efficient .
5 . EMPIRICAL RESULTS
In this section , we perform extensive experiments to examine the performance of our approach , in both simulation data sets and real world bioinformatics application . 5.1 Competing Methods
Altogether , we implement and compare the following al gorithms :
1 . Standard LASSO algorithm [ 29 ] : We use the LARS algorithm to generate the solution path ;
2 . Adaptive LASSO [ 38 ] : We use inverse of ridge regression coefficient to compute w for each task and average them as the shared regularization . Then we rescale w to generate the solution path ;
3 . Adaptive LASSO II [ 13 ] : We use inverse of the marginal regression coefficient to compute w for each task and average them as the shared regularization , then we rescale the regularization to generate the solution path ;
4 . Multi task LASSO [ 23 ] : We choose different values of the initial λ to compute the solution path ;
5 . Re weighted LASSO [ 4 ] : We choose different values of the initial λ ( each initiates a series of iterations till convergence ) to generate the solution path ;
6 . Our approach : We simply choose γ = 1 , an initial norm ω0 = 1e8 , and shrinking factor δ = 0.8 ; we can generate a solution path throughout the iterations until all features are removed . Results on using negative power γ < −1 is similar and therefore removed due to space limit .
We use the following measurements to evaluate the per formance of different methods :
1 . Specificity ( SPC ) versus true positive rate ( TPR ) ( SPC VS TPR ) curve based on solution paths from different algorithms ;
2 . Cross validated mean squared error ( CV MSE ) : we report 5 fold cross validated error of different methods ;
3 . Cross validated F score ( CV Fscore ) : we comput the
5 fold cross validated F score for different methods ;
We use the SLEP sparse learning package [ 18 ] to implement our approach . All codes are written in Matlab and run on a cluster server with 2.2 ∼ 2.8 GHz CPU .
1329 Figure 1 : Illustration of the regularization path of our approach . Here , {ω0 , ω1 , ω2 , } is a sequence such that the global strength of 1 regularization grows stronger .
5.2 Single Task Regression
First we use single task sparse regression problem to test the performance of different methods . Following the details in [ 4 ] , we simulate the data set of n = 100 samples with dimensionality D = 256 , and the design matrix is an n by d iid Gaussian entries . Among the 256 features , only p = 20 are relevant features with randomly chosen non zero β entries from a zero mean unit variance Gaussian distribution . We then use the linear relation yi = xiβ + N ( 0 , σ2 ) to generate the response y .
Results are shown in Figure 2 , where each algorithm is marked by their indexes specified in Section 51 In this data set , multi task LASSO ( method ( 4 ) ) is identical to standard LASSO ( method ( 1 ) ) and therefore is removed . As can be seen , our approach is superior in terms of picking out relevant covariates throughout the whole solution path , demonstrating the effectiveness of annealed sparsity in improving the sparse model selection . In the meantime , the cross validated mean squared error and F score of our approach are also the best among competing methods . 5.3 Multi task Regression
In this experiment we simulate data with K = 5 tasks , each task has n = 40 samples with dimension D = 100 . For each task design matrix is an iid Gaussian distribution , and i βk +N ( 0 , σ2 ) , and for we assume the linear relation yk the relevant features , the corresponding entries in βk ’s are randomly chosen from the distribution 3 +N ( 0 , 1 ) . Here we generate two types of multi task data . i = xk
1 . Multitask I : strict group wise sparsity . We choose p = 20 relevant features for all tasks , and each row of the model coefficient matrix B is either all zeros or all nonzeros , meaning that one feature is either relevant to all tasks , or excluded from all tasks ;
2 . Multitask II : mixed sparsity patterns . We then introduce a perturbation on the model coefficients B : for each non zero row of B , we randomly pick one entry and set it to zero ; in this case , each row of B can have both zero and non zero entries . Namely it has a mixed sparsity pattern ( across group and within group ) .
We report the results in Figure 3 and Figure 4 . We can observe that our approach has the best performance in terms of both feature selection ( F score ) and regression ( predicting error ) , on both types of multi task data sets . The adaptive LASSO II [ 13 ] using the inverse of the marginal regression coefficients as adaptive weights seems to perform better than the adaptive LASSO using ordinary least squares coefficients . The LASSO considers each task separately and can be less accurate . Another observation is that , in case of mixed sparsity patterns , all algorithms perform worse than in the case of strict group wise sparsity , in particularly judged by feature selection accuracy ( F score ) . Nevertheless , our approach still performs the best among competing methods .
We also experiment with different noise levels to test the noise tolerance shown in Figure 5 . As can be observed , our approach is competitive under different noise levels . 5.4 Algorithm Behaviors
In this section , we study properties of the proposed method from different perspectives .
Solution Path
541 First , to have a direct picture on the shrinking behaviour of our method , we plot the solution path of our approach in Figure 6 . Here we use the multi task simulation data with group wise sparsity under the highest noise level ( δ = 3 ) . To prevent visual cluttering , we only plot the solution path for one task , and we only demonstrate 10 of the 20 relevant features and all the rest 80 irrelevant features .
We have several interesting observations . First , note that when the regularization is relatively weak , the solution paths are all smooth ; when the regularization becomes stronger , solution paths begin to show clear stage wise behaviour : the coefficient value is relatively stable within each stage , but may change significantly across stages ( due to the change of ω ) , indicating that the system state goes through significant changes . Second , the solution path is quite non monotonic . With the growing strength of regularization ( 1 ω ) , we can observe that the model coefficients first expand and then gradually shrink . This is in sharp contrast to the solution path of the LASSO , whose solution path almost monotonically shrinks with growing regularization .
The non monotonic shrinking can be quite beneficial in practice . Note that in the beginning stage , both relevant and irrelevant features have large model coefficients , meaning that they are difficult to differentiate . When the regularization grows stronger , interestingly , we can see that most relevant features begin to expand , while most irrelevant features begin to shrink . This becomes particularly obvious around 1 tures suddenly shrink to zero , while relevant features have a jump increase in their coefficients . This is quite beneficial in practical feature selection problems .
ω ≈ 10−4.5 , where the majority of irrelevant fea
Competing mechanism of annealing . In the beginning , under weak global 1 penalty , model coefficients are dense , indicating that the “ energy ” of the system is distribut
1330 ( a ) SPC VS TPR
( b ) Cross validated MSE
( c ) Cross validated F score
Figure 2 : Performance for different methods on single task data , with noise σ = 1 .
( a ) SPC VS TPR
( b ) Cross validated MSE
( c ) Cross validated F score
Figure 3 : Results on multitask I data ( strict group sparsity ) , with noise σ = 1 .
( a ) SPC VS TPR
( b ) Cross validated MSE
( c ) Cross validated F score
Figure 4 : Results on multitask II data ( mixed sparsity pattern ) , with noise σ = 1 .
( a ) Multitask I , σ = 2
( b ) Multitask I σ = 3
( c ) Multitask I , σ = 4
( d ) Multitask II , σ = 2
( e ) Multitask II σ = 3
( f ) Multitask II , σ = 4
Figure 5 : Results on different noise levels for multitask I ( 1st row ) and multitask II ( 2nd row ) .
( 1)(2)(5)Ours(3)(1)(2)(5)Ours(3)(1)(2)(5)Ours(4)(3)(1)(2)(5)Ours(4)(3)(1)(2)(5)Ours(4)(3)(1)(2)(5)Ours(4)(3)1331 low initial temperature fails to start the whole system with sufficient energy and as a result the iterations could quickly stop at a local optima . In practice , we simply choose ω0 as a large value such as 1e8 .
In Figure 7(b ) , we examine the performance of our approach wrt the shrinking factor . As can be observed , more aggressive shrinking scheme ( δ → 0 ) makes the performance worse ; in comparison , milder shrinking scheme ( δ → 1 ) allows the system to evolve slowly such that the “ annealing ” is sufficient , but it is computationally more expensive . In practice , we find that 0.2 < δ < 0.8 can strike a balance between efficiency and the quality of annealing . 5.5 Bioinformatics Application
In this section , our task of expression quantitative trait loci ( eQTL ) is to identify genes whose DNA copy ( DNA copy number data as input ) are associated with the mRNA expression level of six P53 target genes ( normalized expression data as response ) . Note that P53 is a well known tumor suppressor gene . The data set is obtained from Cancer Cell Line Encyclopedia ( CCLE ) project1 , with DNA copy number of 23316 genes across 1011 samples . The six target genes include CDKN1A , PMAIP1 , BBC3 , MSH2 , PML and PRKAA2 , which are of particular relevance to melanoma as suggested by biological experts [ 32 ] . The regulatory genes identified through our regression analysis will then help understand the whole P53 regulation mechanism for cancer , and in particular melanoma .
In the application , we treat the eQTL of six P53 target genes as six tasks , since we believe that the regulating processes on all these melanoma related genes should share some underlying mechanism . We have used the 5 fold crossvalidated error to select the best model . Table 1 reports the 5 fold CV MSE for all competing methods , from which we can see that our method achieves the lowest fitting error . This fully illustrates the superior performance of the proposed dynamic shrinking scheme in high dimensional , realworld multi task learning problems .
We further explore whether the selected genes by our method makes biological sense , by following the common practice of gene set enrichment analysis ( GSEA ) [ 28 ] . Specifically , we rank the selected genes in each task based on the regression coefficients and feed the ranking to GSEA2 222 software . We consider the canonical pathways/gene sets provided by the Molecular Signatures Database2 . For each task , GSEA returns a number of significant pathways/gene sets under false discovery rate ( FDR ) 5 % , and we pick one example pathway to illustrate in Figure 8 . Here , the pathway name is marked on top of each figure ; the red bar denotes the ranking of the β coefficient for each task , and the black lines mark the genes belonging to selected pathway . Our approach identifies more than 100 significant pathways for each task , which is much larger than other methods .
These significant pathways based on our computed gene ranking are very relevant to cancers and/or melanoma , as discussed below : • The gene CDKN1A , cyclin dependent kinase inhibitor 1A , itself is relevant to cell cycle . The significant pathway “ REACTOME_P53_INDEPENDENT_G1_S ” includes genes in p53 Independent G1/S DNA damage checkpoint , which has been shown to be quite relevant to dysfunctional cell cycle causing cancer [ 20 ] .
1
2 http://wwwbroadinstituteorg/ccle/home http://softwarebroadinstituteorg/gsea/msigdb/collectionsjsp
Figure 6 : Solution path for dynamic shrinking . Thick colored lines represent relevant features , and dashed lines for irrelevant features . Vertical line in the middle marks change of display scales ( for visual clarity ) . Regularization increases from left to right . ed somewhat uniformly among competing features . As the regularization grows , the system begins cooling toward a lower energy state ; in the meantime , energy distribution becomes more concentrated . That is , competitive features ( in terms of better prediction in the least square ) will attract more energy from irrelevant features , making the latter shrink . This is why we observe significant growth of some feature magnitude even though the global sparsity enhances . Such energy re allocation through annealing solves the feature selection problem in an effective manner . 542 Parameter Selection In this section , we study how the performance of our approach is affected by the following two parameters : the ω0 that controls the initial “ temperature ” of the system ; the shrinking factor δ that controls the “ cooling rate ” of the system . The performance is measured by the F score using the multitask I data set with σ = 1 .
( a ) Initial temperature ( ω0 )
( b ) Shrinking factor ( δ )
Figure 7 : Performance of our method versus different parameters .
First , we examine the performance wrt ω0 chosen from some grid points {1010 , 109 , , 10−3} . As can be seen from Figure 7(a ) , in a wide range of high initial temperatures , the performance of our approach is quite satisfactory ; when the initial temperature is below a certain value , the performance quickly drops . This coincides with our expectation , since a
10−610−510−4−2024!k ( k = 1)1/"10−310−21332 ( a ) CDKN1A
( b ) BBC3
( c ) MSH2
( d ) PMAIP
( e ) PML
( f ) PRKAA2
Figure 8 : Our ranking of the genes , as well as one example of the identified pathways for each task based on this ranking , via gene enrichment analysis on the CCLE data .
Table 1 : CV MSE on CCLE data set Method LASSO
5 fold Cross validated MSE
Adaptive LASSO
Adaptive LASSO II Multitask LASSO
Re weighted LASSO
Ours
3.2530 1.3078 1.3701 3.2451 1.5507 1.1165
• The gene BBC3 is a protein that cooperates with direct activator proteins to induce mitochondrial outer membrane permeabilization and apoptosis . The pathway “ REACTOME_AUTODEGRADATION_OF_THE ” include genes involved in autodegradation of the E3 ubiquitin ligase COP1 . Destruction of COP1 results in abrogation of the ubiquitination and degradation of p53 [ 7 ] .
• The gene MSH2 is involved in cyclin A/B1 associated events during G2/M transition and is a protein coding gene . In literatures , its related pathways are all about cancer and cell cycle , or checkpoint control . The pathway we find , “ REACTOME_CYCLIN_A_B1_ASSOCIATED ” , is also a cell cycle gene set . It is responsible for phosphorylation of nuclear lamins and histones [ 26 ] , which in turn regulates G2/M transition , thus controlling cell cycle progression by cyclin dependent protein kinases in G1/S and G2/M transitions .
• The gene PAMIP promotes activation of caspases and apoptosis . It contributes to p53/TP53 dependent apoptosis after radiation exposure . In the proteasome pathway “ BIOCARTA_PROTEASOME_PATHWAY ” , the regulated proteolysis of proteins by proteasomes removes damaged or improperly translated proteins from cells , and aids caspases and apoptosis [ 1 ] .
The above results show that our approach not only predicts target gene expressions more accurately , but also identifies biologically meaningful molecular predictors .
6 . CONCLUSIONS
In this paper , we propose a dynamic shrinking framework to compute adaptive regularization in sparse ( multi task ) regression . Our key contribution is to introduce the concept of annealing in sparse model estimation and feature selection , through an iterative , self adapting and self competing mechanism . Empirically , the annealing process can improve the accuracy of models in particular in multi task problems . In the future , we will study how to explore underlying structures of the dynamic solution path to make it computationally more efficient ; we also want to incorporate explicit , task level constraints to make the learned model coefficients more useful for subsequent learning tasks . Finally , we are trying to build a more rigorous , mathematical connection between our approach and annealing so as to fully characterize the behaviour of system evolutions .
Appendix To derive ( 7 ) , we use the Lagrangian of ( 6 ) . We first drop the non negativity constraint . Then the Lagrangian can be written as
J =
θdw
−γ d + α wd − ω
. d d
By setting ∂J ∂wd
= 0 , we have
Plugging the above relation in the constraint
α =
.
θdγ w1+γ d
( 14 ) d wd = ω , then we have
α =
1+γ
, d(θdγ )
1
1+γ
ω
1333 so we have w1+γ d =
θdγ α
=
1+γ .
θdγ · ω1+γ d(θdγ )
1+γ
1
Plugging the above equation in ( 14 ) , we finally have wd =
1
1+γ
θ d
1
1+γ d θ d
ω .
Since θd = k| ≥ 0 , and ω ≥ 0 , the solution will satisfy the non negative constraints automatically . This completes the proof of solution ( 13 ) . k |βd
References
[ 1 ] I . Amm , T . Sommer , and D . H . Wolf . Protein quality control and elimination of protein waste : The role of the ubiquitin–proteasome system . Biochimica et Biophysica Acta ( BBA) Molecular Cell Research , 1843(1):182–196 , 2014 .
[ 2 ] A . Argyriou , T . Evgeniou , and M . Pontil . Convex multi task feature learning . Machine Learning , 73(3):243–272 , 2008 .
[ 3 ] S . Ben david and R . Schuller . Exploiting task relatedness for multiple task learning . In Proceedings of the 16th Annual Conference on Learning Theory , pages 567–580 , 2003 .
[ 4 ] E . J . Candes , M . B . Wakin , and S . Boyd . Enhancing sparsity by reweighted l1 minimization . Journal of Fourier Analysis and Applications , special issue on sparsity , 14(5):877–905 , 2008 .
[ 17 ] S . Lee , J . Zhu , and E . P . Xing . Adaptive multi task lasso : with application to eqtl detection . In Advances in Neural Information Processing Systems , pages 1306–1314 , 2010 .
[ 18 ] J . Liu , S . Ji , and J . Ye . Slep : Sparse learning with efficient projections . Arizona State University , 2009 .
[ 19 ] A . C . Lozano and G . ´Swirszcz . Multi level lasso for sparse multi task regression . In Proceedings of the International Conference on Machine Learning , 2012 .
[ 20 ] N . Mailand , J . Falck , C . Lukas , R . G . Sylju˚asen , M . Welcker ,
J . Bartek , and J . Lukas . Rapid destruction of human cdc25a in response to dna damage . Science , 288(5470):1425–1429 , 2000 .
[ 21 ] J . Mairal , F . Bach , J . Ponce , G . Sapiro , and A . Zisserman .
Supervised dictionary learning . In Advances in Neural Information Processing Systems , 2009 .
[ 22 ] N . Meinshausen and P . B¨uhlmann . High dimensional graphs and variable selection with the lasso . The Annals of Statistics , 34(3):1436–1462 , 2006 .
[ 23 ] G . Obozinski , M . J . Wainwright , and M . I . Jordan .
High dimensional union support recovery in multivariate regression . In Neural Information Processing Systems , pages 1217–1224 , 2008 .
[ 24 ] S . Puniyani , S . Kim , and E . P . Xing . Multi population gwa mapping via multi task regularized regression . Bioinformatics , 26(12):208–216 , 2010 .
[ 25 ] S . Raman and V . Roth . Sparse point estimation for bayesian regression via simulated annealing . Lecture Notes in Computer Science , 7476:317–326 , 2012 .
[ 26 ] B . M . Sefton and S . Shenolikar . Overview of protein phosphorylation . Current Protocols in Protein Science , pages 13–1 , 2001 .
[ 5 ] S . Chen , D . L . Donoho , and M . A . Saunders . Atomic
[ 27 ] S . K . Shevade and S . S . Keerthi . A simple and efficient decomposition by basis pursuit . SIAM Review , 43(1):129–159 , 2001 . algorithm for gene selection using sparse logistic regression . Bioinformatics , 19(17):2246–2253 , 2003 .
[ 6 ] D . Donoho . Compressed sensing . IEEE Transactions on
Information Theory , 52(4):1289–1304 , 2006 .
[ 7 ] D . Dornan , H . Shimizu , A . Mah , T . Dudhela , M . Eby ,
K . O’Rourke , S . Seshagiri , and V . M . Dixit . Atm engages autodegradation of the e3 ubiquitin ligase cop1 after dna damage . Science , 313(5790):1122–1126 , 2006 .
[ 8 ] B . Efron , T . Hastie , I . Johnstone , and R . Tibshirani . Least angle regression . Annals of Statistics , 32(2):407–499 , 2004 .
[ 9 ] M . Elad and M . Aharon . Image denoising via sparse and redundant representations over learned dictionaries . IEEE Transactions on Image Processing , 37(4):3736 – 3745 , 2006 .
[ 10 ] J . Fan , Y . Feng , and Y . Wu . Network exploration via the adaptive lasso and scad penalties . The Annals of Applied Statistics , 3(2):521–541 , 2009 .
[ 11 ] J . Fan and R . Li . Variable selection via nonconcave penalized like lihood and its oracle properties . Journal of the American Statistical Association , 96:1348–1360 , 2001 .
[ 12 ] M . Figueiredo , R . Nowak , and S . Wright . Gradient projection for sparse reconstruction : Application to compressed sensing and other inverse problems . IEEE Journal on Selected Topics in Signal Processing , 1(4):586 –597 , 2007 .
[ 13 ] J . Huang , S . Ma , and C H Zhang . Adaptive lasso for sparse high dimensional regression models . Statistica Sinica , pages 1603–1618 , 2008 .
[ 14 ] S . Kim , K . Koh , M . Lustig , S . Boyd , and D . Gorinevsky . An interior point method for large scale l1 regularized least squares . IEEE Transactions on Signal Processing , 1(4):606–617 , 2007 .
[ 15 ] S . Kirkpatrick , C . Gelatt , and M . Vecchi . Optimization by simulated annealing . Science , 220(4598):671–680 , 1993 .
[ 16 ] M . Kowalski . Sparse regression using mixed norms . Applied and Computational Harmonic Analysis , 27(3):303–324 , 2009 .
[ 28 ] A . Subramanian et al . Gene set enrichment analysis : a knowledge based approach for interpreting genome wide expression profiles . Proceedings of the National Academy of Sciences , 102(43):15545–15550 , 2005 .
[ 29 ] R . Tibshirani . Regression shrinkage and selection via the lasso .
Journal of the Royal Statistical Society . Series B ( Methodological ) , 58(1):267–288 , 1996 .
[ 30 ] J . Tropp , A . Gilbert , and M . Strauss . Algorithms for simultaneous sparse approximation , part ii : Convex relaxation . Signal Processing , 86:572–588 , 2006 .
[ 31 ] L . H . V . Vlack . Elements of Materials Science and
Engineering . Addison Wesley , 1985 .
[ 32 ] C . Wei et al . A global map of p53 transcription binding sites in the human genome . Cell , 124(1):207–219 , 2006 .
[ 33 ] J . Wright , A . Yang , A . Ganesh , S . Sastry , and Y . Ma . Robust face recognition via sparse representation . IEEE Transactions on Pattern Analysis and Machine Intelligence , 2009 .
[ 34 ] M . Yuan and Y . Lin . Model selection and estimation in regression with grouped variables . Journal of the Royal Statistical Society Series B , 68(1):49–67 , 2006 .
[ 35 ] J . Zhang , W . Cheng , Z . Wang , Z . Zhang , W . Lu , G . Lu , and J . Feng . Pattern classification of large scale functional brain networks : Identification of informative neuroimaging markers for epilepsy . PLoS ONE , 7(5):e36733 , 2012 .
[ 36 ] K . Zhang , J . Gray , and B . Parvin . Sparse multitask regression for identifying common mechanism of response to therapeutic targets . Bioinformatics , 26(12):97 – 105 , 2010 .
[ 37 ] P . Zhao and B . Yu . On model selection consistency of lasso . Journal of Machine Learning Research , 7:2541–2563 , 2007 .
[ 38 ] H . Zou . The adaptive lasso and its oracle properties . Journal of the American Statistical Association , 101(476):1418–1429 , 2006 .
[ 39 ] H . Zou and T . Hastie . Regularization and variable selection via the elastic net . Journal of the Royal Statistical Society : Series B ( Statistical Methodology ) , 67(2):301–320 , 2005 .
1334
