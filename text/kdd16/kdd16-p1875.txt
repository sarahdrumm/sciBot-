Scalable Partial Least Squares Regression on
Grammar Compressed Data Matrices
Yasuo Tabei
Japan Science and
Technology Agency , Japan tabeiyaa@mtitechacjp
Yoshihiro Yamanishi Kyushu University , Japan yamanishi@bioreg.kyushu uacjp
ABSTRACT With massive high dimensional data now commonplace in research and industry , there is a strong and growing demand for more scalable computational techniques for data analysis and knowledge discovery . Key to turning these data into knowledge is the ability to learn statistical models with high interpretability . Current methods for learning statistical models either produce models that are not interpretable or have prohibitive computational costs when applied to massive data . In this paper we address this need by presenting a scalable algorithm for partial least squares regression ( PLS ) , which we call compression based PLS ( cPLS ) , to learn predictive linear models with a high interpretability from massive high dimensional data . We propose a novel grammar compressed representation of data matrices that supports fast row and column access while the data matrix is in a compressed form . The original data matrix is grammarcompressed and then the linear model in PLS is learned on the compressed data matrix , which results in a significant reduction in working space , greatly improving scalability . We experimentally test cPLS on its ability to learn linear models for classification , regression and feature extraction with various massive high dimensional data , and show that cPLS performs superiorly in terms of prediction accuracy , computational efficiency , and interpretability .
1 .
INTRODUCTION
Massive data are now abundant throughout research and industry , in areas such as biology , chemistry , economics , digital libraries and data management systems . In most of these fields , extracting meaningful knowledge from a vast amount of data is now the key challenge . For example , to remain competitive , e commerce companies need to constantly analyze huge data of user reviews and purchasing histories [ 24 ] .
Hiroto Saigo
Kyushu University , Japan saigo@infkyushu uacjp
Simon J . Puglisi
University of Helsinki , Finland simonpuglisi@cshelsinkifi
In biology , detection of functional interactions of compounds and proteins is an important part in genomic drug discovery [ 29 , 7 ] and requires analysis of a huge number of chemical compounds [ 3 ] and proteins coded in fully sequenced genomes [ 4 ] . There is thus a strong and growing demand for developing new , more powerful methods to make better use of massive data and to discover meaningful knowledge on a large scale .
Learning statistical models from data is an attractive approach for making use of massive high dimensional data . However , due to high runtime and memory costs , learning of statistical models from massive data — especially models that have high interpretability — remains a challenge .
Partial least squares regression ( PLS ) is a linear statistical model with latent features behind high dimensional data [ 26 , 35 , 36 ] that greedily finds the latent features by optimizing the objective function under the orthogonal constraint . PLS is suitable for data mining , because extracted latent features in PLS provide a low dimensional feature representation of the original data , making it easier for practitioners to interpret the results . From a technical viewpoint , the optimization algorithm in PLS depends only on elementary matrix calculations of addition and multiplication . Thus , PLS is more attractive than other machine learning methods that are based on computationally burdensome mathematical programming and complex optimization solvers . In fact , PLS is the most common chemoinformatics method in pharmaceutical research .
However , applying PLS to massive high dimensional data is problematic . While the memory for the optimization algorithm in PLS depends only on the size of the corresponding data matrix , storing all high dimensional feature vectors in the data matrix consumes a huge amount of memory , which limits large scale applications of PLS in practice . One can use lossy compression ( eg , PCA [ 14 , 9 ] and b bit minwise hashing [ 12 , 20 ] ) to compactly represent data matrices and then learn linear models on the compact data matrices [ 21 ] . However , although these lossy compression based methods effectively reduce memory usage [ 21 , 30 ] , their drawback is that they cannot extract informative features from the learned models , because the original data matrices cannot be recovered from the compressed ones .
Grammar compression [ 2 , 27 , 16 ] is a method of lossless compression ( ie , the original data can be completely re
1875 Table 1 : Summary of scalable learning methods of linear models .
Approach
Compression Type # of parameters Interpretability Optimization
PCA SL [ 14 , 9 ]
Orthogonal rotation bMH SL [ 21 ] SGD [ 33 , 8 ]
Hashing Sampling
Lossy Lossy cPLS ( this study ) Grammar compression
Lossless
2 3 1 1
Limited Unable Limited
High
Stable Stable
Unstable
Stable covered from grammar compressed data ) that also has a wide variety of applications in string processing , such as pattern matching [ 37 ] , edit distance computation [ 13 ] , and q gram mining [ 1 ] . Grammar compression builds a small context free grammar that generates only the input data and is very effective at compressing sequences that contain many repeats . In addition , the set of grammar rules has a convenient representation as a forest of small binary trees , which enables us to implement various string operations without decompression . To date , grammar compression has been applied only to string ( or sequence ) data ; however , as we will see , there remains high potential for application to other data representations . A fingerprint ( or bit vector ) is a powerful representation of natural language texts [ 23 ] , bio molecules [ 32 ] , and images [ 11 ] . Grammar compression is expected to be effective for compressing a set of fingerprints as well , because fingerprints belonging to the same class share many identical features .
Contribution .
In this paper , we present a new scalable learning algorithm for PLS , which we call lossless compressionbased PLS ( cPLS ) , to learn highly interpretable predictive linear models from massive high dimensional data . A key idea is to convert high dimensional data with fingerprint representations into a set of sequences and then build grammar rules for representing the sequences in order to compactly store data matrices in memory . To achieve this , we propose a novel grammar compressed representation of a data matrix capable of supporting row and column access while the data matrix is in a compressed format . The original data matrix is grammar compressed , and then a linear model is learned on the compressed data matrix , which allows us to significantly reduce working space . cPLS has the following desirable properties :
1 . Scalability : cPLS is applicable to massive high dimensional data .
2 . Prediction Accuracy : cPLS can achieve high predic tion accuracies for both classification and regression .
3 . Usability : cPLS has only one hyper parameter , which enhances the usability of cPLS .
4 . Interpretability : Unlike lossy compression based methods , cPLS can extract features reflecting the correlation structure between data and class labels/response variables .
We experimentally test cPLS on its ability to learn linear models for classification , regression and feature extraction with various massive high dimensional data , and show that cPLS performs superiorly in terms of prediction accuracy , computational efficiency , and interpretability .
2 . LITERATURE REVIEW
Several efficient algorithms have been proposed for learning linear models on a large scale . We now briefly review the state of the art , which is also summarized in Table 1 .
Principal component analysis ( PCA ) [ 14 ] is a widely used machine learning tool , and is a method of lossy compression , ie , the original data cannot be recovered from compressed data . There have been many attempts to extend PCA [ 31 , 28 ] and present a scalable PCA in distributed settings for analyzing big data [ 9 ] . For classification and regression tasks , a data matrix is compressed by PCA , and linear models are learned on the compressed data matrix by a supervised learning method ( SL ) , which is referred to as PCA SL . Despite these attempts , PCA and its variants do not look at the correlation structure between data and output variables ( ie , class labels/response variables ) , which results in not only the inability of feature extractions in PCA but also the inaccurate predictions by PCA SL .
Li et al . [ 21 ] proposed a compact representation of fingerprints for learning linear models by applying b bit minwise hashing ( bMH ) . A d dimensional fingerprint is conceptually equivalent to the set si ⊂ {1 , , d} that contains element i if and only if the i th bit in the fingerprint is 1 . Li et al . ’s method works as follows . We first pick h random permutations πi , i = 1 , , h , each of which maps [ 1 , d ] to [ 1 , d ] . We then apply a random permutation π on a set si , compute the minimum element as min(π(si) ) , and take as a hash value its lowest b bits . Repeating this process h times generates h hash values of b bits each . Expanding these h values into a ( 2b × h) dimensional fingerprint with exactly h 1 ’s builds a compact representation of the original fingerprint .
Linear models are learned on the compact fingerprints by SL , which is referred to as bMH SL . Although bMH SL is applicable to large scale learning of linear models , bMH is a method of lossy compression and cannot extract features from linear models learned by SL . Other hashing based approaches have been proposed such as Count Min sketch [ 5 ] , Vowpal Wabbit [ 34 ] , and Hash SVM [ 25 ] . However , like bMH SL , these algorithms cannot extract features , which is a serious problem in practical applications .
Stochastic gradient descent ( SGD ) [ 8 , 33 ] is a computationally efficient algorithm for learning linear models on a large scale . SGD samples ν feature vectors from an input dataset and computes the gradient vector from the sampled feature vectors . The weight vector in linear models is updated using the gradient vector and the learning rate µ , and this process is repeated until convergence . Unfortunately however , learning linear models using SGD is numerically unstable , resulting in low prediction accuracy . This is because SGD has three parameters ( ν , µ , and C ) that must be optimized if high classification accuracy is to be attained . Online learning is a specific version of SGD that loads an input dataset from the beginning and updates the weight vector in a linear model for each feature vector . AdaGrad [ 8 ] is an efficient online learning that automatically tunes parameters of ν and µ in SGD . Although online learning is space efficient ( owing to its online nature ) , it is also numerically unstable . Even worse , AdaGrad is applicable only
1876 new non terminal . Each iteration of the algorithm consists of the following two steps : ( i ) find the most frequent pair of symbols in the current sequence , and then ( ii ) replace the most frequent pair with a new non terminal symbol , generating a new grammar rule and a new ( and possibly much shorter ) sequence . Steps ( i ) and ( ii ) are then applied to the new sequence and iterated until no pair of adjacent symbols appears twice .
Apart from the dictionary D that stores the rules as they are generated , Re Pair maintains a hash table and a priority queue that together allow the most frequent pair to be found in each iteration . The hash table , denoted by H , holds the frequency of each pair of adjacent symbols ab in the current sequence , ie , H : ab → N . The priority queue stores the symbol pairs keyed on frequency and allows the most frequent symbol to be found in step ( i ) . In step ( ii ) , a new grammar rule Z1 → ab is generated where ab is the most frequent symbol pair and Z1 is a new non terminal not appearing in a sequence . The rule is stored in the dictionary D . Every occurrence of ab in the sequence is then replaced by Z1 , generating a new , shorter sequence . This replacement will cause the frequency of some symbol pairs to change , so the hash table and priority queue are then suitably updated . Let sc denote a sequence generated at c th iteration in the Re Pair algorithm . For input sequence s in Figure 1 , the most frequent pair of symbols is 12 . Thus , we generate rule Z1 → 12 to be added to the dictionary D and replace all the occurrences of 12 by non terminal Z1 in s . After four iterations , the current sequence s4 has no repeated pairs , and thus the algorithm stops . Dictionary D has four grammar rules that correspond to a forest of two small trees .
As described by Larsson and Moffat [ 19 ] , Re Pair can be implemented to run in linear time in the length of the input sequence , but it requires the use of several heavyweight data structures to track and replace symbol pairs . The overhead of these data structures ( at least 128 bits per position ) prevents the algorithm from being applied to long sequences , such as the large data matrices .
Another problem that arises when applying Re Pair to long sequences is the memory required for storing the hash table : a considerable number of symbol pairs appear twice in a long sequence , and the hash table stores something for each of them , consuming large amounts of memory .
In the next section , we present scalable Re Pair algorithms that achieve both space efficiency and fast compression time on large data matrices . Specifically , our algorithms need only constant working space .
4 . OUR GRAMMAR COMPRESSED DATA
MATRIX
Our goal is to obtain a compressed representation of a data matrix X of n rows and d columns . Let xi denote the ith row of the matrix represented as a fingerprint ( ie binary vector ) . An alternative view of a row that will be useful to us is as a sequence of integers si = ( p1 , p2 , , pm ) , p1 < p2 < ··· < pm , where pi ∈ si if and only if xi[pi ] = 1 . In other words the sequence si indicates the positions of the 1 bits in xi .
In what follows we will deal with a differentially encoded form of si in which the difference for every pair of adjacent elements in si is stored , ie , si = ( p1 , p2 , , pm ) is encoded as sgi = ( p1 , p2 − p1 , p3 − p2 , , pm − pm−1 ) . This differ
Figure 1 : Illustration of grammar compression . to differentiable loss functions , which limits its applicability to simple linear models , eg , SVM and logistic regression , making the learned model difficult to interpret .
Despite the importance of scalable learning of interpretable linear models , no previous work has been able to achieve high prediction accuracy for classification/regression tasks and high interpretability of the learned models . We present a scalable learning algorithm that meets both these demands and is made possible by learning linear models on grammarcompressed data in the framework of PLS . Details of the proposed method are presented in the next section .
3 . GRAMMAR COMPRESSION
Given a sequence of integers S , a grammar compressor generates a context free grammar ( CFG ) that generates S and only S . The grammar consists of a set of rules1 . Each rule is of the form Zi → ab . Symbols that appear on the lefthand side of any rule are called non terminals . The remaining symbols are called terminals , all of which are present in Informally , a rule Zi → ab indicates the input sequence . that on the way to recovering the original sequence from its grammar compressed representation , occurrences of the symbol Zi should be replaced by the symbol pair ab ( the resulting sequence may then be subject to yet more replacements ) . A data structure storing a set of grammar rules is called a dictionary and is denoted by D . Given a nonterminal , the dictionary supports access to the symbol pair on the right hand of the corresponding grammar rule , ie , D[Zi ] returns ab for rule Zi → ab . The original sequence can be recovered from the compressed sequence and D . The set of grammar rules in D can be represented as a forest of ( possibly small ) binary trees called grammar trees , where each node and its left/right children correspond to a grammar rule . See Figure 1 for an illustration .
The size of a grammar is measured as the number of rules plus the size of compressed sequence . The problem of finding the minimal grammar producing a given string is known to be NP complete [ 2 ] , but several approximation algorithms exist that produce grammars that are small in practice ( see , eg , [ 27 , 19 , 16] ) . Among these is the simple and elegant Re Pair [ 19 ] algorithm , which we review next . 3.1 Re Pair Algorithm
The Re Pair grammar compression algorithm by Larsson and Moffat [ 19 ] builds a grammar by repeatedly replacing the most frequent symbol pair in an integer sequence with a
1In this paper we assume without loss of generality that the grammar is in Chomsky Normal Form .
( i ) Input sequence ( ii ) Compressed sequence and dictionary . The dictionary corresponds to a forest of grammar trees.1877 ential encoding tends to increase the number of repeated symbol pairs , which allows the sequences sgi to be more effectively compressed by the Re Pair algorithm . A grammar compressor captures the underlying correlation structure of data matrices : by building the same grammar rules for the same ( sequences of ) integers , it effectively compresses data matrices with many repeated integers . 4.1 Re Pair Algorithms in Constant Space
We now present two ideas to make Re Pair scalable without seriously deteriorating its compression performance . Our first idea is to modify the Re Pair algorithm to identify topk frequent symbol pairs in all rows sc gi in step ( i ) and replace all the occurrences of the top k symbol pairs in all rows sc gi in step ( ii ) , generating new k grammar rules and new rows sc+1 . This new replacement process improves scalability by gi reducing the number of iterations required by roughly a factor of k .
Since we cannot replace both frequent symbol pairs ab and bc in triples abc in step ( ii ) , we replace the first appearing symbol pair ab , preferentially . However , such preferential replacement can generate a replacement of a pair only once and can add redundant rules to a dictionary , adversely affecting compression performance . To overcome this problem , we replace the first and second appearances of each frequent pair at the same time and replace the next successive appearance of the frequent pair as usual , which guarantees generating grammar rules that appear at least twice .
Our second idea is to reduce the memory of the hash table by removing infrequent symbol pairs . Since our modified Re Pair algorithm can work storing compressed sequences sc gi at each iteration c in a secondary storage device , the hash table consumes most of the memory in execution . Our modified Re Pair generates grammar rules from only top k frequent symbol pairs in the hash table , which means only frequent symbol pairs are expected to contribute to the compression . Thus , we remove infrequent symbol pairs from the hash table by leveraging the idea behind stream mining techniques originally proposed in [ 15 , 6 , 22 ] for finding frequent items in data stream . Our method is a counter based algorithm that computes the frequency of each symbol pair and removes infrequent ones from the hash table at each interval in step ( i ) . We present two Re Pair algorithms using lossy counting and frequency counting for removing infrequent symbol pairs from the hash table . We shall refer to the Re Pair algorithms using lossy counting and frequency counting as Lossy Re Pair and Freq Re Pair , respectively . 4.2 Lossy Re Pair
The basic idea of lossy counting is to divide a sequence of symbols into intervals of fixed length and keep symbol pairs in successive intervals in accordance with their appearance frequencies in a hash table . Thus , if a symbol pair has appeared h times in the previous intervals , it is going to be kept in the next h successive intervals .
Let us suppose a sequence of integers made by concatenating all rows sgi of X and let N be the length of the sequence . We divide the sequence into intervals of fixedlength . Thus , the number of intervals is N/ . We use hash table H for counting the appearance frequency of each symbol pair in the sequence . If symbol pair ab has count H(ab ) , it is ensured that ab is kept in hash table H until the next H(ab) th interval . If symbol pair ab first appears in the q th interval , H(ab ) is initialized as qN/ + 1 , which ensures that ab is kept until at least the next interval , ie , the ( qN/ + 1) th interval . Algorithm 1 shows the pseudo code of lossy counting .
The estimated number of symbol pairs in the hash table is O( ) [ 22 ] , resulting in O( log ) bits consumed by the hash table .
N = N + 1 if H(ab ) = 0 then
Algorithm 1 Lossy counting . H : hash table , N : length of an input string at a time point , : length of each interval . Note that lossy counting can be used in step ( i ) in the RePair algorithm . 1 : Initialize N = 0 and ∆ = 0 2 : function LossyCounting(a , b ) 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : 12 :
H(ab ) = ∆ + 1 = ∆ then ∆ = N for each symbol pair ab in H do if H(ab ) < ∆ then
H(ab ) = H(ab ) + 1
Remove ab from H else if N
4.3 Freq Re Pair
The basic idea of frequency counting is to place a limit , v , on the maximum number of symbol pairs in hash table H and then keep only the most frequent v symbol pairs in H . Such frequently appearing symbol pairs are candidates to be replaced by new non terminals , which generates a small number of rules .
The hash table counts the appearance frequency for each symbol pair in step ( i ) of the Re Pair algorithm . When the number of symbol pairs in the hash table reaches v , FreqRe Pair removes the bottom percent of symbol pairs with respect to frequency . We call the vacancy rate . Algorithm 2 shows the pseudo code of frequency counting . The space consumption of the hash table is O(v log v ) bits . Algorithm 2 Frequency counting . H : hash table , |H| : number of symbol pairs in H , v : the maximum number of symbol pairs in H , : vacancy rate . Note that frequency counting can be used in step ( i ) in the Re Pair algorithm . 1 : function FrequencyCounting(a , b ) 2 : 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : 11 : while v(1 − /100 ) < |H| do H(ab ) = H(ab ) − 1 if H(ab ) = 0 then
H(ab ) = H(ab ) + 1 if |H| ≥ v then for each symbol pair ab in H do
Remove ab from H if H(ab ) = 0 then else
H(ab ) = 1
5 . DIRECT ACCESS TO ROW AND COL
UMN
In this section , we present algorithms for directly accessing rows and columns of a grammar compressed data matrix ,
1878 which is essential for us to be able to apply PLS on the compressed matrix in order to learn linear regression models . 5.1 Access to Row
Accessing the i th row corresponds to recovering the original si from grammar compressed sc gi . We compute this operation by traversing the grammar trees . For recovering the i th row si , we start traversing the grammar tree having a node of the q th symbol sc gi[q ] as a root for each q from 1 gi| . Leaves encountered in the traversal must have inteto |sc gers in sequence sgi , which allows us to recover sgi via tree traversals , starting from the nodes with non terminal sc gi[q ] for each q ∈ [ 1,|sc gi| ] . We recover the original i th row si from sgi by cumulatively adding integers in sgi from 1 to |sgi| , i.e , si[1 ] = sgi[1 ] , si[2 ] = sgi[2 ] + si[1],,si[|sgi| ] = sgi[|sgi| ] + si[|sgi| − 1 ] . 5.2 Access to Column
Accessing the j th column of a grammar compressed data matrix requires us to obtain a set of row identifiers R such that xij = 1 for i ∈ [ 1 , n ] , ie , R = {i ∈ [ 1 , n ] ; xij = 1} . from This operation enables us to compute the transpose X X in compressed format , which is used in the optimization algorithm of PLS .
P [ Zi ] stores a summation of terminal symbols as integers at the leaves under the node corresponding to terminal symbol Zi in a grammar tree . For example , in Figure 1 , P [ Z1 ] = 3 , P [ Z2 ] = 6 , P [ Z3 ] = 8 and P [ Z4 ] = 4 . P can be implemented as an array that is randomly accessed from a given non terminal symbol . We shall refer to P as the weight array . The size of P depends only on the grammar size . gi[1 ] ] + P [ sc gi[2 ] ] +··· + P [ sc
The j th column is accessed to check whether or not xij = gi , for each i ∈ [ 1 , n ] . We effi1 in compressed sequence sc ciently solve this problem on grammar compressed data matrix by using the weight array P . Let uq store the summation of weights from the first symbol sc gi[1 ] to the q th symbol gi[q ] , ie , uq = P [ sc sc gi[q] ] , and let u0 = 0 . If uq is not less than j , the grammar tree with the node corresponding to a symbol sc gi[q ] as a root can encode j at a leaf . Thus , we traverse the tree in depth first order from the node corresponding to symbol sc gi[q ] as follows . Suppose Z = sc gi[q ] and u = uq−1 . Let Z ( resptively Zr ) be a ( respectively b ) of Z → ab in D . ( i ) if j < u , we go down to the left child in the tree ; ( ii ) otherwise , ie , j ≥ u , we add P [ Z ] to u and go down to the right child . We continue the traversal until we reach a leaf . If s = j at a leaf , this should be xij = 1 at row i ; thus we add i to solution set R . Algorithm 3 shows the pseudo code for column access .
6 . CPLS
In this section we present our cPLS algorithm for learning PLS on grammar compressed data matrices . We first review the PLS algorithm on uncompressed data matrices . NIPALS [ 35 ] is the conventional algorithm for learning PLS and requires the deflation of the data matrix involved . We thus present a non deflation PLS algorithm for learning PLS on compressed data matrices . 6.1 NIPALS Let us assume a collection of n data samples and their output variables ( x1 , y1 ) , ( x2 , y2 ) , , ( xn , yn ) where yi ∈ . The i=1 yi = output variables are assumed to be centralized asn gi[q ] ] for i in 1n do u0 = 0 for q in 1|sc gi| do uq = uq−1 + P [ Sc if j ≤ uq then
Algorithm 3 Access to the j th column on grammarcompressed data matrix . R : solution set of row identifiers i at column j st xij = 1 . 1 : function AccessColumn(j ) 2 : 3 : 4 : 5 : 6 : 7 : 8 : 1 : function Recursion(i,j,Z,u ) 2 : 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 :
Set Zl ( resp . Zr ) as a ( resp . b ) of Z → ab in D if u + P [ Zl ] > j then
Recursion(i,j,Zr,u + P [ Zl ] ) Go to right child
Recursion(i , j , sc break if Z is a terminal symbol then
Recursion(i,j,Zl,u ) if u + Z = j then
Go to left child gi[q ] , uq−1 )
Add i to R return else
0 . Denote by y ∈ n the vector of all the training output variables , ie , y = ( y1 , y2 , , yn )
.
The regression function of PLS is represented by the fol lowing special form , f ( x ) =
αiw i x , m fl 1 i = j i=1
0 i = j where the wi are weight vectors reducing the dimensionality of x ; they satisfy the following orthogonality condition : i X w
Xwj =
.
( 1 )
We have two kinds of variables wi and αi to be optimized . Denote by W ∈ d×m the weight matrix i th column of which is weight vector wi , ie , W = ( w1 , w2 , , wm ) . Let α ∈ m be a vector whose i th element is αi , ie , α =
( α1 , α2 , , αm ) . Typically , W is first optimized and then α is determined by minimizing the least squares error without regularization ,
||y − XWα||2 2 . min
α
( 2 )
By computing the derivative of equation ( 2 ) with respect to α and setting it to zero , α is obtained as follows :
α = ( W
X
XW )
−1W
X y .
( 3 )
The weight vectors are determined by the following greedy algorithm . The first vector w1 is obtained by maximizing the squared covariance between the mapped feature Xw and the output variable y as follows : w1 = argmaxw cov2(Xw , y ) , subject to w Xw . The problem can be analytically solved as w1 = X
Xw = 1 , where cov(Xw , y ) = y y .
X
For the i th weight vector , the same optimization problem is solved with additional constraints to maintain orthogonality , cov2(Xw , y ) , wi = argmax
( 4 ) wj = 0 , j = 1 , , i − 1 . subject to w The optimal solution of this problem cannot be obtained analytically , but NIPALS solves it indirectly . Let us define the
Xw = 1 , w
X
X
X w
1879 i th latent vector as ti = Xwi . The optimal latent vectors ti are obtained first and the corresponding wi is obtained later . NIPALS performs the deflation of design matrix X to ensure the orthogonality between latent components ti as follows , X = X − tit i X . Then , the optimal solution has the form , wi = X
Due to the deflation , X = X− tit i X , NIPALS completely destroys the structure of X . Thus , it cannot be used for learning PLS on grammar compressed data matrices . 6.2 cPLS Algorithm y .
We present a non deflation PLS algorithm for learning PLS on grammar compressed data matrices . Our main idea here is to avoid deflation by leveraging the connection between NIPALS [ 35 ] and the Lanczos method [ 18 ] which was originally proposed for recursive fitting of residuals without changing the structure of a data matrix .
We define residual ri+1 = ( ri − ( y ti−1)ti−1 ) that is initialized as r1 = y . The i th weight vector is updated as wi = ( ri−1 − ( y ti−1)ti−1 ) , which means wi can be computed X without deflating the original data matrix X . The i th latent vector is computed as ti = Xwi and is orthogonalized by applying the Gram Schmidt orthogonalization to the i th latent vector ti and previous latent vectors t1,t2,,ti−1 as fol lows , ti = ( I−Ti−1T i−1)Xwi , where Ti−1 = ( t1 , t2 , , ti−1 ) ∈ n×(i−1 ) . The non deflation PLS algorithm updates the residual ri instead of deflating X , thus enabling us to learn PLS on grammar compressed data matrices . cPLS is the non deflation PLS algorithm that learns PLS on grammar compressed data matrices . The input data matrix is grammar compressed and then the PLS is learned on the compressed data matrix by the non deflation PLS algorithm . Our grammar compressed data matrix supports row and column accesses directly on the compressed format for computing matrix calculations of addition and multiplication , which enables us to learn PLS by using the nondeflation PLS algorithm . Let XG be the grammar compressed data matrix of X . Algorithm 4 shows the pseudo code of cPLS . Since our grammar compression is lossless , the cPLS algorithm on grammar compressed data matrices learns the same model as the non deflation PLS algorithm on uncompressed data matrices and so achieves the same prediction accuracy .
Algorithm 4 The cPLS algorithm . XG : the grammarcompressed data matrix of X . 1 : r1 = y 2 : for i = 1 , , m do 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : Compute the coefficients α using equation ( 3 ) . wi = X Gri if i = 1 then t1 = XGwi ti = ( I − Ti−1T i−1)XGwi else ti = ti/||ti||2 ri+1 = ri − ( y access to column access to row access to row ti)ti
We perform feature extraction after line 3 at each iteration in Algorithm 4 . The features corresponding to the top u largest weights wi are extracted . Due to the orthogonality condition ( 1 ) , the extracted features give users a novel insight for analyzing data , which is shown in Section 7 .
The cPLS algorithm has three kinds of variables to be optimized : wi , ri , and ti . The memory for wm is O(md ) and the memory for tm and ri is O(mn ) . Thus , the total memory for the variables in cPLS is O(m min(n , d ) ) highly depending on parameter m . The parameter m controls the amount of fitting of the model to the training data and is typically chosen to optimize the cross validation error . Since the cPLS algorithm learns the model parameters efficiently , m can be set to a small value , which results in overall spaceefficiency .
7 . EXPERIMENTS
In this section , we demonstrate the effectiveness of cPLS with massive datasets . We used five datasets , as shown in Table 2 . ” Book review ” consists of 12,886,488 book reviews in English from Amazon [ 24 ] . We eliminated stop words from the reviews and then represented them as 9,253,464 dimensional fingerprints , where each dimension of the fingerprint represents the presence or absence of a word . ” Compound ” is a dataset of 42,682 chemical compounds that are represented as labeled graphs . We enumerated all the subgraphs of at most 10 vertices from the chemical graphs by using gSpan [ 38 ] and then converted each chemical graph into a 52,099 dimensional fingerprint , where each dimension of the fingerprint represents the presence or absence of a chemical substructure . ” Webspam ” is a dataset of 16,609,143 fingerprints of 350,000 dimensions2 . ” CP interaction ” is a dataset of 216,121,626 compound protein pairs , where each compound protein pair is represented as a 3,621,623 dimensional fingerprint and 300,202 compound protein pairs are interacting pairs according to the STITCH database [ 17 ] . We used the above four datasets for testing the binary classification ability . ” CP intensity ” consists of 1,329,100 compoundprotein pairs represented as 682,475 dimensional fingerprints , where the information about compound protein interaction intensity was obtained from several chemical databases ( eg , ChEMBL , BindingDB and PDSP Ki ) . The intensity was observed by IC50 ( half maximal ( 50 % ) inhibitory concentration ) . We used the ” CP intensity ” dataset for testing the regression ability . The number of all the nonzero dimensions in each dataset is summarized in the #nonzero column in Table 2 , and the size for storing fingerprints in memory by using 32bits for each element is written in the memory column in Table 2 . We implemented all the methods by C++ and performed all the experiments on one core of a quad core Intel Xeon CPU E5 2680 ( 28GHz ) We stopped the execution of each method if it had not finished within 24hours in the experiments . In the experiments , cPLS did not use a secondary storage device for compression , ie , cPLS compressed data matrices by loading all data in memory . 7.1 Compression Ability and Scalability
First , we investigated the influence on compression performance of the top k parameter in our Re Pair algorithms . For this setting , we used the Lossy Re Pair algorithm , where parameter is set to the total length of all rows in an input data matrix in order to keep all the symbols in the hash table . We examined k = {1× 104 , 2.5× 104 , 5× 104 , 7.5× 104 , 10× 104} for the Book review , Compound and Webspam datasets and examined k = {1× 105 , 2.5× 105 , 5× 105 , 7.5× 105 , 10× 105} for the CP interaction and CP intensity datasets .
2The dataset is downloadable from http://wwwcsientu edutw/˜cjlin/libsvmtools/datasets/binaryhtml
1880 Table 2 : Summary of datasets .
Dataset Book review Compound Webspam CP interaction CP intensity
Label type binary binary binary binary real
Number Dimension 9,253,464 52,099,292 16,609,143 3,621,623 682,475
12,886,488 42,682 350,000 216,121,626 1,329,100
#nonzeros Memory ( mega bytes ) 698,794,696 2,665 3,489 914,667,811 4,977 1,304,697,446 125,243 32,831,736,508 28,865,055,991 110,111
Book review
Compound
Webspam
CP interaction
CP intensity
Figure 2 : Compression size in mega bytes ( MB ) and compression time in seconds ( sec ) for various top k
Figure 2 shows compression size and compression time for various top k . We observed a trade off between compressed size and compression time for all the datasets . The smaller the compressed size , the larger the compression time for larger values of k . In particular , significantly faster compression time was possible at the cost of only slightly worse compression . For example , Lossy Re Pair took 57,290 seconds to compress the Book review dataset and its size was 1,498 mega bytes ( MB ) for k=10000 . When k=100000 , compression time dropped to 20,004 seconds ( less than half ) , while compressed size increased negligibly to 1,502MB .
The same trends for the Book review dataset were observed in the other datasets , which suggests that in practice a large value of k can be chosen for fast compression , without adversely affecting compression performance . Notably , we observed our compression method to be particularly effective for the larger datasets : CP interaction and CP intensity . The original sizes of CP interaction and CPintensity were 125GB and 110GB , respectively , while the compressed sizes of CP interaction and CP intensity were at most 5GB and at 535MB , respectively . Our compression method thus achieved compression rates of 4 % and less than 1 % for CP interaction and CP intensity , respectively . Such significant reductions in data size enable the PLS algorithm to scale to massive data . Next , we evaluated the performance of Lossy Re Pair and Freq Re Pair , where parameters ={1MB , 10MB , 100MB , 1000MB} were examined for Lossy Re Pair , and parameters v ={1MB , 10MB , 100MB , 1000MB} and = {30} were examined for Freq Re Pair . Table 3 shows the compressed size , compression time and the working space used for the hash table in Lossy Re Pair and Freq Re Pair . We observed that both Lossy Re Pair and Freq Re Pair achieved high compression rates using small working space . Such efficiency is crucial when the goal is to compress huge data matrices that exceed the size of RAM ; our Re Pair algorithm can compress data matrices stored in external memory ( disk ) . For compressing the CP interaction dataset , Lossy Re Pair and Freq Re Pair consumed 16GB and 13GB , respectively , achieving a compressed size of 5GB . We observed the same tendency for the other datasets ( See Table 3 ) .
7.2 Prediction Accuracy
We evaluated the classification and regression capabilities of cPLS , PCA SL , bMH SL and SGD . Following the previous works [ 39 , 21 ] , we randomly selected 20 % of samples for testing and used the remaining 80 % of samples for training . cPLS has one parameter m , so we selected the best parameter value among m = {10 , 20 , , 100} that achieved the highest accuracy for each dataset . The PCA phase of PCASL has one parameter deciding the number of principal components m , which was chosen from m = {10 , 25 , 50 , 75 , 100} whose maximum value of 100 is the same as that of cPLS ’s parameter m . Linear models were learned with LIBLINEAR [ 10 ] , one of the most efficient implementations of linear classifiers , on PCA ’s compact feature vectors , where the hinge loss of linear SVM for classification and the squared error loss for regression were used with L2 regularization . The learning process of PCA SL [ 14 , 9 ] has one parameter C for L2 regularization , which was chosen from C = {10−5 , 10−4 , , 105} . For PCA SL [ 14 , 9 ] , we examined all possible combinations of two parameters ( m and C ) and selected the best combination achieving the highest accuracy for each dataset . The hashing process of bMH SL [ 21 ] has two parameters ( the number of hashing values h and the length of bits b ) , so we examined all possible combinations of h = {10 , 30 , 100} and b = {8 , 16} . As in PCA SL , linear models were learned with LIBLINEAR [ 10 ] on bMH ’s compact feature vectors , where the hinge loss of linear SVM for classification and the squared error loss for regression were used with L2regularization . The learning process of bMH SL [ 21 ] has one parameter C for L2 regularization , which was chosen from C = {10−5 , 10−4 , , 105} . For bMH SL , we examined all possible combinations of three parameters ( h , b , and C ) and selected the best combination achieving the highest accuracy for each dataset . We implemented SGD on the basis of the AdaGrad algorithm [ 8 ] using the logistic loss for classification and the squared error loss for regression with L2 regularization . SGD [ 8 ] has one parameter C for L2 regularization , which was also chosen from C = {10−5 , 10−4 , , 105} . We measured the prediction accuracy by the area under the ROC curve ( AUC ) for classification and Pearson correlation coefficient ( PCC ) for regression . Note that AUC and PCC return 1 for perfect inference in classification/regression , while AUC returns 0.5 for random inference and PCC returns 0 for random inference . We report the best test accuracy under the above experimental settings for each method below .
( cid:724)(cid:724)(cid:724)(cid:724)(cid:724)Top−kCompression Size ( MB)2e+044e+046e+048e+041e+05100012001400160018002000Compression Time ( sec)2000030000400005000060000Compression Size ( MB)Compression Time ( sec)(cid:724)(cid:724)(cid:724)(cid:724)(cid:724)Top−kCompression Size ( MB)2e+044e+046e+048e+041e+0502004006008001000Compression Time ( sec)10000150002000025000Compression Size ( MB)Compression Time ( sec)(cid:724)(cid:724)(cid:724)(cid:724)(cid:724)Top−kCompression Size ( MB)2e+044e+046e+048e+041e+05600800100012001400Compression Time ( sec)100001500020000250003000035000Compression Size ( MB)Compression Time ( sec)(cid:724)(cid:724)(cid:724)(cid:724)Top−kCompression Size ( MB)2e+054e+056e+058e+051e+0646004800500052005400Compression Time ( sec)50000600007000080000Compression Size ( MB)Compression Time ( sec)(cid:724)(cid:724)(cid:724)(cid:724)(cid:724)Top−kCompression Size ( MB)2e+054e+056e+058e+051e+0602004006008001000Compression Time ( sec)6600680070007200740076007800Compression Size ( MB)Compression Time ( sec)1881 Table 3 : Compression size in mega bytes ( MB ) , compression time in seconds ( sec ) , and working space for hash table ( MB ) for varying parameter in Lossy Re Pair and v in Freq Re Pair for each dataset . compression size ( MB ) compression time ( sec ) working space ( MB )
Lossy RePair ( MB )
Freq RePair v(MB )
Book review
1
1836 9904 1113
10
1685 12654 1931
100 1502 19125 7988
1
1000 2021 1501 2956 20004 8603 292 Compound
10
1816 2355 616
100 1680 3165 3856
1000 1501 21256 6724
Lossy RePair ( MB )
Freq RePair v(MB ) compression size ( MB ) compression time ( sec ) working space ( MB )
1
1288 5096 1113
10 859 7053 1926
100 825 7787 5030
1
1000 825 7946 5030 Webspam
1523 1362 292
10
1302 1587 616
100 825 8111 3535
1000 825 8207 3535 compression size ( MB ) compression time ( sec ) working space ( MB )
Lossy RePair ( MB )
Freq RePair v(MB )
1
1427 6953 1112
10 948
10585 1923
100 940
10584 7075
1000 940
10964 7075
1
2328 2125 292
10
2089 2799 616
100 1050 7712 3856
1000 940
11519 5539
Lossy RePair ( MB )
Freq RePair v(MB )
CP interaction compression size ( MB ) compression time ( sec ) working space ( MB )
10
24hours
100 5199 55919 9914
1000 5139 44853 16650
10000 5036 43756 16635
616 CP intensity
10
20307 24565
100 9529 39647 3856
1000 5136 47230 13796
10000 5136 48653 13796 compression size ( MB ) compression time ( sec ) working space ( MB )
Lossy RePair ( MB )
Freq RePair v(MB )
10 558 8103 1936
100 543 6479 3552
1000 540 6494 3722
10000
535 6657 3738
10 588 5423 616
100 535 5848 2477
1000 535 5859 2477
10000
535 5923 2477
Table 4 : Results of cPLS , PCA SL , bMH SL and SGD for various datasets . Dspace : the working space for storing data matrix ( MB ) , Ospace : the working space for optimization algorithm and Ltime : learning time ( sec ) .
Data Book review Compound Webspam CP interaction CP intensity
Data Book review Compound Webspam CP interaction CP intensity m 100 20 60 40 60 m/C 100/1 25/0.1 50/1
100/0.1
Dspace(MB ) Ospace(MB ) Ltime(sec ) AUC/PCC cPLS
1288 786 890 4367 472
15082 7955 7736 53885 10683 PCA SL
21628 1089 4171 35880 33969
0.96 0.83 0.99 0.77 0.67
Dspace(MB ) Ospace(MB ) Ltime(sec ) AUC/PCC
14747
12 200
1521
110
1 2 11 bMH SL
6820
6
129
>24hours
42
0.70 0.65 0.99
0.11
Data b/h/C
Dspace(MB ) Ospace(MB ) Ltime(sec ) AUC/PCC
Book review 100/16/0.01
Compound Webspam CP interaction CP intensity
30/16/10 30/16/10 30/16/0.1 100/16/0.1
2457
2 20
12366
253
110
1 2
1854
11 SGD
1033
1 2
10054
45
0.95 0.62 0.99 0.77 0.54
Data Book review Compound Webspam CP interaction CP intensity
C 10 10 10 1 0.1
Dspace(MB ) Ospace(MB ) Ltime(sec ) AUC/PCC
1694 9539 3041 663 124
57 83 85
3163 280
0.96 0.82 0.99 0.75 0.04
1882 Figure 3 : Extracted features for the top 10 latent components in the application of cPLS to the Compound dataset . Each column represents the highly weighted features ( chemical substructures ) of a latent component .
Table 4 shows the prediction accuracy , working space and training time of cPLS , PCA SL , bMH SL , and SGD . The working space for the storing data matrix and the working space needed for optimizations were separately evaluated . While PCA SL and bMH SL significantly reduced the working space for storing data matrices , the classification and regression accuracies were low . Since PCA SL and bMH SL compress data matrices without looking at the correlation structure between data and output variables for compressing data matrices , high classification and regression accuracies were difficult to achieve .
SGD significantly reduced the working space , since it did not store data matrices in memory . Classification and regression accuracies of SGD were not high , because of the instability of the optimization algorithm . In addition , SGD is applicable only to simple linear models , making the learned model difficult to interpret .
Our proposed cPLS outperformed the other methods ( PCA
SL , bMH SL , and SGD ) in terms of AUC and PCC and significantly reduced the working space . The results showed cPLS ’s efficiency for learning PLS on compressed data matrices while looking at the correlation structure between data and output variables . Such a useful property enables us to extract informative features from the learned model . 7.3 Interpretability
Figure 3 shows the top 10 highly weighted features that were extracted for each component in the application of cPLS to the Compound dataset , where one feature corresponds to a compound chemical substructure . It was observed that structurally similar chemical substructures were extracted together as important features in the same component , and the extracted chemical substructures differed between components . This observation corresponds to a unique property of cPLS . Analysing large scale compound structure data is of importance in pharmaceutical applications , especially for rational drug design . For example , the extracted chemical substructures are beneficial for users who want to identify important chemical fragments involved in therapeutic drug activities or adverse drug reactions .
8 . CONCLUSIONS AND FUTURE WORK
We presented a scalable algorithm for learning interpretable linear models — called cPLS — which is applicable to largescale regression and classification tasks . Our method has the following appealing properties :
1 . Scalability : cPLS is applicable to large numbers of high dimensional fingerprints ( see Sections 7.1 and 72 ) 2 . Prediction Accuracy : The optimization of cPLS is numerically stable , which enables us to achieve high prediction accuracies ( see Section 72 )
3 . Usability : cPLS has only one hyperparameter to be tuned in cross validation experiments ( see Section 62 )
4 . Interpretability : Unlike lossy compression based meth ods , cPLS can extract informative features reflecting the correlation structure between data and class labels ( or response variables ) , which makes the learned models easily interpretable ( see Section 73 )
In this study , we applied our proposed grammar compression algorithm to scaling up PLS , but in principle it can be used for scaling up other machine learning methods or data mining techniques . An important direction for future work is therefore the development of scalable learning methods and data mining techniques based on grammar compression techniques . Such extensions will open the door for machine learning and data mining methods to be applied in various large scale data problems in research and industry .
9 . ACKNOWLEDGMENTS
This work was supported by MEXT/JSPS Kakenhi ( 24700140 ,
25700004 and 25700029 ) , the JST PRESTO program , the Program to Disseminate Tenure Tracking System , MEXT and Kyushu University Interdisciplinary Programs in Education and Projects in Research Development , and the Academy of Finland via grant 294143 .
fi 'ff( fi'ff( fi 'ff( fi 'ff( fi'ff( fi'ff fi'ff( fi'ff( fi'ff( fi'ff( fi 'ff() fi 'ff(( fi 'ff() fi 'ff() fififi'ff() fi'ff fi'ff( fi'ff fi'ff( fi'ff fi'ff() fi 'ff() fi'ff() fi 'ff(( fi'ff() fi'ff fi'ff( fi'ff( fi'ff( fi'ff fi 'ff() fi'ff() fi 'ff() fi 'ff() fi 'ff() fi'ff( fi'ff( fi'ff( fi'ff( fi'ff( fi 'ff() fi 'ff() fi 'ff() fi 'ff() fi 'ff() fi'ff( fi'ff( fi'ff( fi'ff( fi'ff(Component1Component2Component3Component4Component5Component6Component7Component8Component9Component10 fi fi'ff( fifi 'ff( fifi'ff( fi'ff fi 'ff() fi'ff( fi'ff( fi'ff( fi'ff( fi'ff( fi 'ff() fi 'ff() fifi fi 'ff() fi 'ff() fi'ff( fi'ff( fi'ff( fi'ff( fi'ff( fi'ff( fi'ff() fi'ff() fi'ff() fi'ff() fi'ff() fi'ff( fi'ff( fi'ff( fi'ff( fi'ff( fi'ff(( fi 'ff() fi 'ff() fi'ff() fi'ff() fi'ff( fi'ff fi'ff( fi'ff( fi'ff() fi'ff( fi 'ff(( fi 'ff() fi 'ff() fi'ff( fi'ff( fi'ff( fi'ff fi'ff(( fi'ff(1883 10 . REFERENCES [ 1 ] P . Bille , P . H . Cording , and I . L . Gortz . Compact q gram profiling of compressed strings . In CPM , pages 62–73 , 2013 .
[ 2 ] M Charikar , E . Lehman , D . Liu , R . Panigrahy , M . Prabhakaran , A . Sahai , and A . Shelat . The smallest grammar problem . IEEE Transactions on Information Theory , 51:2554–2576 , 2005 .
[ 3 ] B . Chen , D . Wild , and R . Guha . PubChem as a source of polypharmacology . JCIM , 49:2044–2055 , 2009 .
[ 4 ] The Uniprot Consortium . The universal protein resource ( uniprot ) in 2010 . NAR , 38:D142–D148 , 2010 . [ 5 ] G . Cormode and Muthukrishnan S . An improved data stream summary : the count min sketch and its applications . Journal of Algorithms , 55:58–75 , 2005 .
[ 6 ] D . Demaine , A . L´opez Ortiz , and I . Munro . Frequency estimation of internet packet streams with limited space . In ESA , pages 348–360 , 2002 .
[ 7 ] CM Dobson . Chemical space and biology . Nature ,
432(7019):824–828 , 2004 .
[ 8 ] J . Duchi , E . Hazan , and Y . Singer . Adaptive subgradient methods for online learning and stochastic optimization . JMLR , 12:2121–2159 , 2011 . [ 9 ] T . Elgamal , M . Yabandeh , A . Aboulnaga ,
W . Mustafa , and M . Hefeeda . sPCA : Scalable principal component analysis for big data on distributed platforms . In SIGMOD , 2015 .
[ 10 ] R . Fan , K . W . Chang , C . J . Hsieh , X . R . Wang , and
C . J . Lin . LIBLINEAR : A library for large linear classification . JMLR , pages 1871–1874 , 2008 .
[ 11 ] D . A . Forsyth and J . Ponce . Computer Vision : A
Modern Approach . Prentice Hall Professional Technical Reference , 2002 .
[ 12 ] A . Gionis , P . Indyk , and R . Motwani . Similarity search in high dimensions via hashing . In VLDB , 1999 . [ 13 ] D . Hermelin , D . H . Landau , and O . Weimann . A unified algorithm for accelerating edit distance computation via text compression . In STACS , pages 529–540 , 2009 .
[ 14 ] I . T . Jolliffe . Principal Component Analysis . Springer ,
1986 .
[ 15 ] R . Karp , S . Shenker , and C . Papadimitriou . A simple algorithm for finding frequent elements in sets and bags . TODS , 28:51–55 , 2003 .
[ 16 ] J . C . Kieffer , E . Yang , G . J . Nelson , and P . C .
Cosman . Universal lossless compression via multilevel pattern matching . IEEE Transactions on Information Theory , 46(4):1227–1245 , 2000 .
[ 17 ] M . Kuhn , D . Szklarczyk , A . Franceschini ,
M . Campillos , C . von Mering , LJ Jensen , A . Beyer , and P . Bork . STITCH 2 : An interaction network database for small molecules and proteins . NAR , 38(suppl 1):D552–D556 , 2010 .
[ 18 ] C . Lanczos . An iteration method for the solution of the eigen value problem of linear differential and integral operators . Journal of Research of the National Bureau of Standards , 45:255–282 , 1950 .
[ 19 ] J . Larsson and A . Moffat . Offline dictionary based compression . In DCC , pages 296–305 , 1999 .
[ 20 ] P . Li and A . C . K¨onig . b bit minwise hashing . In
WWW , pages 671–680 , 2010 .
[ 21 ] P . Li , A . Shrivastava , J . L . Moore , and A . C . K¨onig . Hashing algorithms for large scale learning . In NIPS , pages 2672–2680 , 2011 .
[ 22 ] G . Manku and R . Motwani . Approximate frequency counts over data stream . In VLDB , volume 5 , pages 346–357 , 2002 .
[ 23 ] C . D . Manning and H . Sch¨utze . Foundations of
Statistical Natural Language Processing . The MIT Press , 1999 .
[ 24 ] J . McAuley and J . Leskovec . Hidden factors and hidden topics : understanding rating dimensions with review text . In RecSys , 2013 .
[ 25 ] Y . Mu , G . Hua , W . Fan , and S . Chang . Hash SVM :
Scalable kernel machines for large scale visual classification . In CVPR , pages 979–986 , 2014 .
[ 26 ] R . Rosipal and N . Kr¨amer . Overview and recent advances in partial least squares . LNCS , pages 34–51 . Springer , 2006 .
[ 27 ] W . Rytter . Application of Lempel Ziv factorization to the approximation of grammar based compression . TCS , 302(1–3):211–222 , 2003 .
[ 28 ] B . Sch¨olkopf , A . Smola , and K . R . M¨uller . Nonlinear component analysis as a kernel eigenvalue problem . Neural Computation , 10:1299–1319 , 1998 .
[ 29 ] B . Stockwell . Chemical genetics : Ligand based discovery of gene function . Nature Reviews Genetics , 1:116–125 , 2000 .
[ 30 ] Y . Tabei and Y . Yamanishi . Scalable prediction of compound protein interactions using minwise hashing . BMC Systems Biology , 7:S3 , 2013 .
[ 31 ] M . E . Tipping and C . M . Bishop . Mixtures of probabilistic principal component analysers . Neural Computation , 11:443–482 , 1999 .
[ 32 ] R . Todeschini and V . Consonni . Handbook of
Molecular Descriptors . Wiley VCH , 2002 .
[ 33 ] Y . Tsuruoka , J . Tsujii , and S . Ananiadou . Stochastic gradient descent training for l1 regularized log linear models with cumulative penalty . In ACL and AFNLP , pages 477–485 , 2009 .
[ 34 ] K . Weinberger , A . Dasgupta , J . Langford , A . Smola , and J . Attenberg . Feature hashing for large scale multitask learning . In ICML , pages 1113–1120 , 2009 .
[ 35 ] H . Wold . Path models with latent variables : The
NIPALS approach . In Quantitative Sociology : International Perspectives on Mathematical and Statistical Model Building , pages 307–357 . Academic Press , 1975 .
[ 36 ] S . Wold , M . Sj¨ostr¨om , and L . Eriksson .
PLS regression : a basic tool of chemometrics . Chemometrics and Intelligent Laboratory Systems , 58:109–130 , 2001 .
[ 37 ] T . Yamamoto , H . Bannai , S . Inenaga , and M . Takeda . Faster subsequence and don’t care pattern maching on compressed texts . In CPM , pages 309–322 , 2011 .
[ 38 ] X . Yan and J . Han . gSpan : graph based substructure pattern mining . In ICDM , pages 721–724 , 2002 .
[ 39 ] HF Yu , CJ Hsieh , KW Chang , and CJ Lin . Large linear classification when data cannot fit in memory . In KDD , pages 833–842 , 2010 .
1884
