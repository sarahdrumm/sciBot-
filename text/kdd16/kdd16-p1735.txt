Multi Task Feature Interaction Learning
Kaixiang Lin1 , Jianpeng Xu1 , Inci M . Baytas1 , Shuiwang Ji2 , Jiayu Zhou1 1Computer Science and Engineering , Michigan State University , East Lansing , MI 48824
2Electrical Engineering and Computer Science , Washington State University , Pullman , WA 99164
{linkaixi , xujianpe , baytasin , jiayuz}@msu.edu , sji@eecswsuedu
ABSTRACT Linear models are widely used in various data mining and machine learning algorithms . One major limitation of such models is the lack of capability to capture predictive information from interactions between features . While introducing high order feature interaction terms can overcome this limitation , this approach dramatically increases the model complexity and imposes significant challenges in the learning against overfitting . When there are multiple related learning tasks , feature interactions from these tasks are usually related and modeling such relatedness is the key to improve their generalization . In this paper , we propose a novel Multi Task feature Interaction Learning ( MTIL ) framework to exploit the task relatedness from high order feature interactions . Specifically , we collectively represent the feature interactions from multiple tasks as a tensor , and prior knowledge of task relatedness can be incorporated into different structured regularizations on this tensor . We formulate two concrete approaches under this framework , namely the shared interaction approach and the embedded interaction approach . The former assumes tasks share the same set of interactions , and the latter assumes feature interactions from multiple tasks share a common subspace . We have provided efficient algorithms for solving the two formulations . Extensive empirical studies on both synthetic and real datasets have demonstrated the effectiveness of the proposed framework .
CCS Concepts •Computing methodologies → Multi task learning ; •Information systems → Data mining ;
Keywords muti task learning ; feature interaction ; structured regularization ; tensor norm
1 .
INTRODUCTION
Linear models are simple yet powerful machine learning and data mining models that are widely used in many applications . Due to the additive nature of the linear models , it can fully unleash the power of feature engineering , allowing crafted features to be easily integrated into the learning system . This is a desired property in many practical applications , in which high quality features are the key to predictive performance . Moreover , efficient parallel algorithms are readily available to learn linear models from large scale datasets . Despite its attractive properties , one apparent limitation of such models is that they can only learn a set of individual effects of features contributing to the response , due to its linear additive property . Thus when a part of the response is derived from interactions between features , such models would not be able to detect such non linear predictive information , thereby leading to poor predictive performance .
In practice , high order feature interactions are common in many domains . For example , in genetics studies , environmental effects and genetic environmental interaction are found to have strong relationship with the variability in adoptee aggressivity , conduct disorder and adult antisocial behavior [ 7 ] . Similarly , the interaction effects between continuance commitment and affective commitment was found in predicting annexed absences [ 28 ] . Also , a recent study of depression found that genotype , sex , environmental risk and their interaction have combined influence on depression symptoms [ 12 ] . It is also reported that the interaction of brain derived neurotrophic factor and early life stress exposure are identified in predicting syndromal depression and anxiety , and associated alterations in cognition [ 16 ] . In biomedical studies , many human diseases are a result of complicated interactions among genetic variants and environmental factors [ 19 ] . One intuitive solution to overcome this limitation is to augment interaction terms into linear models , explicitly modeling the effects from the interactions . However , this will dramatically increase the model complexity and lead to poor generalization performance when there is limited amount of data [ 9 , 11 , 23 , 26 , 35 ] .
On the other hand , when there are multiple related learning tasks , the multi task learning ( MTL ) paradigm [ 1 , 4 , 8 ] has offered a principled way to improve the generalization performance of such learning tasks by leveraging the relatedness among tasks and performing inductive transfer among them . The past decade has witnessed a great amount of success in applying MTL to tackle problems where large amount of labeled data are not available or creating such
1735 datasets incurs prohibitive cost . Such problems are especially prevalent in biological and medical domains , where MTL has achieved significant success , including data analysis on genotype and gene expression [ 21 ] , breast cancer diagnosis [ 37 ] and progression modeling of Alzheimer ’s Disease [ 18 ] , etc . The MTL improves generalization performance by learning a shared representation from all tasks , which serves as the agent for knowledge transfer . Structured regularization has provided an effective means of modeling such shared representation and encoding various types of domain knowledge on tasks [ 1 , 20 , 24 , 33 ] . The attractive benefits provided by MTL make it an ideal scheme when learning problems involve multiple related tasks with feature interactions , because tasks may be related with each other by shared structures on feature interactions . For example , predicting various cognitive functions may involve a shared set of interactions among brain regions .
However , many existing MTL frameworks are based on linear models [ 1 ] in the original input space . Thus they cannot be directly applied to explore task relatedness in the form of high order feature interactions . On the other hand , although traditional nonlinear MTL methods based on neural networks ( eg , [ 2 ] ) can exploit non linear feature interactions to some extends , it is generally difficult to encode prior knowledge on task relatedness to such models . In this paper , we propose a novel multi task feature interaction learning framework , which learns a set of related tasks by exploiting task relatedness in the form of shared representations in both the original input space and the interaction space among features . We study two concrete approaches under this framework , according to different prior knowledge about the relatedness via feature interactions . The shared interaction approach assumes that there are only a small number of interactions that are relevant to the predictions , and all tasks share the same set of interactions ; the embedded interaction approach assumes that , for each task , the feature interactions are derived from a low dimensional subspace that is shared across different tasks . We have provided formulations and efficient algorithms for both approaches . We conduct empirical studies on both synthetic and real datasets to demonstrate the effectiveness of the proposed framework on leveraging feature interactions from tasks . The contributions of this paper are three folds :
• Our novel framework has extended the MTL paradigm , for the first time , to allow high order representations to be shared among tasks , by exploiting predictive information from feature interactions .
• We proposed two novel approaches under our framework to model different task relatedness over feature interactions .
• Our comprehensive empirical studies on both synthetic and real data have led to practical insights of the proposed framework .
The remainder of this paper is organized as follows : Section 2 reviews related work of MTL and models involving feature interactions . Section 3 introduces the framework for MTIL . The two approaches under MTIL have been given in 4 . Section 5 presents the experimental results on both synthetic and real datasets . Section 6 concludes the paper .
2 . RELATED WORK The proposed research is related to existing work on MTL and feature interaction learning . In this section , we briefly summarize the these related work and show how our work advances these areas . 2.1 Multi Task Learning
MTL has been extensive studied over the last two decades . In the center of most MTL algorithms is how task relationships are assumed and encoded into the learning formulations . The concept of learning multiple related tasks in parallel was first introduced in [ 8 ] . It was demonstrated in multiple real world applications that adding a shared representation in neural network tasks can help others get better models . Such discovery had inspired many subsequent research efforts in the community and applications in diverse application domains . Among these studies , the regularized MTL framework has been pioneered by [ 13 ] . The regularization scheme can easily integrate various task relationship into existing learning formulations to couple MTL , thus providing a flexible multi task extension to existing algorithms . It is well adopted and is soon generalized to a rich family of MTL algorithms . MTL via Regularization . Among the work in the regularization based MTL scheme , there are many different assumptions about how tasks are related , leading to different regularization terms in the formulation . For example , one common assumption is that the tasks share a subset of features , and the task relatedness can be captured by imposing a group sparsity penalty on the models to achieve simultaneous feature selection across tasks [ 33 , 24 ] . Another common assumption is that the models of tasks come from the same subspace , leading to a low rank structure within the model matrix . Directly penalizing the rank function leads to NPhard problems , and one convex alternative is to penalize the convex envelop of the rank function , ie , trace norm . This encourages low rank by introducing sparsity to the singular values of the model matrix [ 20 ] . In [ 1 ] , the authors studied a MTL formulation that learns a common feature mapping for the tasks and assumed all tasks share the same features after the mapping . The authors have shown that this assumption can also be equivalently expressed by a low rank regularization on the model . There are many more formulations that fall into this category of formulation to capture task relatedness by designing different shared representation and regularization terms , such as cluster structures [ 38 ] , tree/graph structures [ 21 , 10 ] , etc . However , to the best of our knowledge , all of these formulations do not consider feature interactions in the model , and extensions to consider interactions are not straightforward . In this work , we will extend the MTL framework to enable knowledge transfer not only in the original input space , but also in higher order feature interaction space . Multilinear MTL . The use of tensor in MTL has shown to be very effective in representing structural information underlying in MTL problems . In [ 27 ] , Romera Paredes et al . proposed a multilinear multitask ( MTMTL ) framework that arranges parameters of linear effects from all tasks into a tensor W , by which they are able to represent the multi modal relationships among tasks . In a dataset containing multimodal relationships , tasks can be referenced by multiple indices . In MTMTL , the authors employed a regularizer on W to induce a low rank structure to transfer knowledge among
1736 d
Q
Feature Interaction of one task d
T d a ) tensor representation of feature interactions
T d d
Task 1
Task 2
Task T b ) structured sparsity of an interaction tensor
T d d d r r 1 × 1
T r
1 × r 3 d c ) low rank structure of an interaction tensor
Figure 1 : Illustration of MTL with feature interactions . ( a ) the feature interactions from multiple tasks can be collectively represented as a tensor Q ; group sparse structures ( c ) and low rank structures ( b ) in feature interactions can be used to facilitate multi task models . tasks . The optimization problem contains the minimization of tensor ’s rank , which leads to solving a non convex problem . Thus the authors develop an alternating algorithm , employing the Tucker decomposition and convex relaxation using tensor trace norm . Although the authors also used a tensor representation in MTL , the learning formulations , implications , as well as the meaning of such the tensor is fundamentally different from those in our work . The proposed MTIL framework utilizes tensor to capture the relatedness among tasks and transfer knowledge through high order feature interactions , which cannot be achieved by any existing MTL formulations . Note that the tensor in MTMTL is indexed by multi modal tasks . In MTIL , the tensor is indexed by features and tasks , which is clearly different from the aforementioned work . In the proposed embedded interaction approach for MTIL , however , we face a similar challenge in MTMTL to seek a solution involving a low rank tensor . 2.2 Feature Interaction
In many machine learning tasks , we are interested in learning a linear predictive model . Given the input feature vector of a sample , the response is given by a linear combination of these features , ie , a weighted sum of the features . Because of this reason we call them linear effects . There are strong evidences found in many complex applications that , in addition to the linear effects , there are also effects from high order interactions between such features . As a result , there are considerable efforts from both academia and industry aiming at addressing this limitation by removing the additive assumption and including interaction effects .
To overcome the dimensionality issues introduced by interaction effects , two types of heredity constraints have been studied [ 5 ] ; namely strong hierarchy in which an interaction effect can be selected into the model only if both of its corresponding linear effects have been selected , and weak hierarchy , in which an interaction effect can be selected if at least one of its corresponding linear effects has been selected . In [ 11 ] , the authors proposed an approach known as SHIM to identify the important interaction effects . SHIM extends the classical Lasso [ 29 ] and enforces a strong hierarchy . An iterative algorithm was proposed based on Lasso , which may not scale to problems with high dimensional feature space . Radchenko et . al proposed the VANISH method to address the problem [ 26 ] . They developed a convex formulation with a refined penalty that can not only learn the sparse solution , but also treat the linear and interaction effects using different weights . This way , the main effect could have more influence on the prediction . In [ 5 ] , a hierarchical lasso was proposed to search for interactions with large main effects instead of considering all possible interactions . The authors proposed an algorithm based on ADMM for strong hierarchy lasso and a generalized gradient descent for weak hierarchical lasso . More recently , Liu et al . [ 23 ] proposed an efficient algorithm for solving the non convex weak hierarchical Lasso directly , based on the framework of general iterative shrinkage and thresholding ( GIST ) [ 17 ] . The authors proposed a closed form solution of proximal operator and further improved the efficiency of solving the subproblem of proximal operator from quadratic to linearithmic time complexity .
In many real work applications there are multiple related tasks . When those these tasks involve interaction effects , the tasks could be related via the high order feature interactions . In our paper , we propose to address the model complexity issue from interaction effects using a new perspective , by leveraging such relatedness .
3 . TASK RELATEDNESS IN HIGH ORDER
FEATURE INTERACTIONS
In this section , we present the framework of Multi Task feature Interaction Learning ( MTIL ) . For completeness , we give a self contained introduction of our work . We will derive concrete learning algorithms under this framework in Section 4 . Linear and Interaction Effects . Consider the traditional linear models . For an input feature vector x ∈ R and a scalar response y , we have assumed the following underlying linear generative model : d d . i=1 y = xiwi + , d
) is a Gaussian noise . A linear model f ( x ; w ) = x is the weight vector for linear effects , and ∼ where w ∈ R N ( 0 , σ2 w can be a quite effective prediction function . However , if the underlying generative model includes effects from feature interactions , ie ,
T d . d . d . y = xiwi + xixjQi,j + , i=1 i=1 j=1 where xixjQi,j is the joint effect between the ith feature and the jth feature , and Qi,j is the weight for this joint effect . This type of feature interactions have been commonly found in many applications . If the training data follow this distribution then the linear model is not enough to capture the relationship between input features and output responses .
1737 One of the approaches is to introduce non linear feature interaction terms into the linear model . That is , we can denote it as a quadratic function : w + x
T
Qx ,
( 1 )
T f ( x ; w , Q ) = x and Q ∈ R d d×d i Qi,i . where w ∈ R collectively represent the parameters for linear effects and interaction effects , respectively . We note that Q is typically symmetric because this representation includes two terms involving feature i and j : xixj(Qi,j + Qj,i ) and it also includes second order feature transformations of the original features x2 In supervised Discussions on Feature Interactions . learning , we seek a predictive function that maps an into a corresponding output y ∈ R . Let put vector x ∈ R ( X , y ) = {(x1 , y1 ) , ( x2 , y2 ) , ( xn , yn)} be a training dataset , in which each data point is drawn from certain iid distribution μ . The goal of learning is to find the best predictor ˆf ∈ H so that the predicted value ˆyi for the input data xi is as close as possible to the ground truth yi , ∀(xi , yi ) ∈ ( X , y ) , given a loss function L( . , ) We hope that the predictor f learned in this way is close to the optimal model that minimizes the expected loss according to the μ : d
R(f ) = E(X,y)∼μL(f ( X ) , y ) .
( 2 )
Such predictor is given by the minimum of the empirical risk :
ˆf = arg min f∈H
L(f ( xi ) , yi ) . n . i=1
The error caused by learning the best predictor in the training dataset is called the estimation error . The error caused by using a restricted H is called the approximation error . For a fixed data size , the smaller the hypothesis space H , the larger the approximation error , and vice versa . The trade off between approximation error and estimation error is controlled by selecting the size of H . By including feature interactions we would enlarge the hypothesis space , and we may be able to dramatically minimize the approximation error compared to the traditional hypothesis space for linear models . On the other hand , we note that given a limited amount of data , a large hypothesis space may result in models with poor generalization performance . We will need to either increase our training data , or provide effective regularizations to narrow down the hypothesis space . Multi task Feature Interactions . We consider the setting that there are multiple learning tasks which are related not only in the original feature space , but also in terms of feature interactions . The propose framework simultaneously learns all related tasks and provides an effective regularization on the hypothesis space using relatedness on the interactions . Let D = ( X1 , y1 ) , . . . ,( XT , yT ) be the training data for the T learning tasks , and the iid training samples for mt , where mt is the number of task t is drawn from ( μt ) fiT data points available for task t . We collectively denote the distribution as D ∼ μ = mt . All tasks have a ddimensional feature space ( ie , xi ∈ R ) . The corresponding features are homogeneous and have the same semantic meaning . The total training data points are : ( Xt , yt ) = {(x1t , y1t ) , ( x2t , y2t ) , . . . ,( xmt , ymt)} , t = 1 , . . . , T , The goal of MTL is to learn T functions for the tasks such t=1(μt ) d that ft(xit ) =y it , based on the assumption that all task functions are related to some extent .
In order to consider interactions for each task , we use the quadratic predictive function in Eq 1 for all tasks . We collectively represent the linear effects from all tasks as a matrix W = [ w1 , . . . ,w T ] ∈ R and the interaction effects as a tensor Q ∈ R , in which the t th frontal slice Qt ∈ R represents the interaction effects for task t . We illustrate this interaction tensor in Figure 1(a ) .
, wi ∈ R d×T d×d×T d×d d
Given specific loss functions ˆ' for samples from one task , ( eg , square loss for regression and logistic loss for classification , see Table 1 ) , the loss function for each task is i=1 ˆ'(f ( xi ; w , Q ) , yi ) . Our multi task 't(f , w , Q ; X , y ) = feature interaction loss function is given by :
'mt
L(W,Q ; f , X , Y ) =
't(f , wt,Qt ; Xt , Yt ) .
( 3 )
T . t=1
Note that it is not necessary for all tasks to have the same loss function . In MTL , the learning of each task benefits from the knowledge from other tasks , which effectively reduces the hypothesis space for all tasks . In order to achieve knowledge transfer among tasks , we would like to impose shared representations via designing regularization terms on both W and Q , which specify how tasks are related in the original feature space and features interactions , respectively . The MTIL Framework . The proposed Multi Task feature Interaction Learning ( MTIL ) framework is then given by the following learning objective :
W,Q L(W,Q ; f , X , Y ) +λ RRF ( W ) +λ I RI ( Q ) , min
( 4 ) where RF ( W ) is the regularization providing task relatedness in the original feature space , RI ( Q ) is the regularization encoding our knowledge about how feature interactions are related among tasks , λR and λI are the corresponding regularization coefficients . For λI → ∞ , the problem reduces to traditional MTL , when RI is chosen properly . In this paper , we formulate two concrete approaches to capture the feature interaction patterns :
• Shared Interaction Approach .
In many applications , even though we have a large number of feature interactions , only a few interactions may be related to the response [ 5 , 11 ] . When learning with multiple tasks , different tasks may share exactly the same set of feature interactions , but with different effects . As such , we can design MTIL formulations that learns a set of common feature interactions , which could effectively reduce the hypothesis space . During the learning process the selected feature interactions for one task will be task ’s knowledge , contributing to the share representation : a set of indices of common interactions . An analogy in traditional MTL is the joint feature learning approach [ 24 , 33 ] , in which tasks share the same set of features . One way to achieve this approach is by using the structured sparsity to induce the same sparsity patterns on the interaction effects . An illustration of this approach is given in Figure 1(b ) .
• Embedded Interaction Approach . When the response from one task is related to complicated feature interactions , the patterns of such interactions may be captured by a low dimensional space , resulting in a
1738 low rank interaction matrix . When there are multiple related tasks , they could have a shared low dimensional space , ie , different interaction matrices may share the same set of rank 1 basis matrices , but have different weights associated with these basis matrices . When collectively represented by a tensor , we end up with a low rank tensor . During the learning process , each task contributes their subspace information to facilitate learning of the share low dimensional subspace , which in turn , improves the feature space . The analogy in traditional MTL is the low rank based models [ 1 , 20 ] . However , there are challenging questions such as : How to define a proper rank function for tensor ? Are there tractable algorithms to induce low rank structure in tensor ? In the next section we will discuss these important questions and propose efficient algorithms . We illustrate this approach in Figure 1(c ) .
We note that even though we only provided two specific approaches in this paper , the proposed MTIL framework could offer broader class of formulations . The proposed framework allows many other possible ways to define task relatedness on feature interactions , leading to a brand new research area of MTL .
4 . FORMULATIONS AND ALGORITHGMS
OF THE TWO MTIL APPROACHES
In this section , we will study how the formulations and algorithms of the shared interaction approach and embedded interaction approach under the proposed MITL framework . We note that extension of multi task learning to feature interactions is not trivial because of the involvement of tensors . We start with formulating the shared interaction approach by incorporating a group Lasso penalty to introduce structured sparsity on the tensor , which would select only a set of common feature interactions across different tasks that are relevant to the prediction . For the embedded interaction approach , we propose both a convex formulation and a non convex formulation . While the convex formulation leads to efficient optimization algorithms and global solutions , the non convex formulation provides reduced storage complexity for large scale problems . 4.1 Preliminary In this paper , we use the following basic definition of tensor : Mode n fiber is a vector defined by fixing every index but one . We may see it as the higher order analogue of matrix rows ( mode 2 fibers ) and columns ( mode 1 fibers ) . For example , in a three way tensor Q ∈ R n1×n2×n3 , the mode 3 fiber is Qi,j , : ∈ R Mode n unfolding is the process of reordering the elements of an N way tensor Q ∈R n1×n2×,,×nN into a matrix . The mode k unfolding of tensor Q is denoted by Q(k ) ∈ nk×Jk , where Jk = i=1,i'=k . The matrix is arranged by R concatenating all mode k fibers of the tensor . Rank n in our paper denotes the rank of tensor ’s mode n unfolding . It ’s actually the dimension of the space spanned by the mode n fibers of tensor . Specifically , rankn(Q ) = rank(Q(n) ) . When Q is a matrix ( ie 2 way tensor ) , this becomes the regular definition of rank , since rank1(Q ) = rank2(Q ) = rank(Q ) . fiN n3 .
4.2 Shared Interaction Approach
The goal of the shared interaction approach is to identify a set of common and relevant feature interactions across different tasks . The interaction tensor Q in our framework has provided a convenient representation to encode such information , and we are able to incorporating a group Lasso penalty [ 14 ] to induce a special type of structured sparsity on the tensor , coupling the same interactions for all tasks . Recall that the sparsity implies that only the significant interaction effects are captured in the model . For the purpose of shared interaction , a sparse tensor norm is defined as :
.d
.d
||Q||GL Sym ≡ ff.K i=1 j≥i k=1
Q2 i,j,k + Q2 j,i,k
.
( 5 )
Note that this norm enforces a symmetric sparsity by over the tensor , so that the one group is defined to include coefficients of one interaction between feature i and feature j , from all tasks . Penalizing the tensor sparse norm leads to the following formulation : min w,Q L(W,Q ; f , X , Y ) +λ F RF ( W ) +λ I||Q||GL Sym , ( 6 ) where the parameter λI control the sparsity of tensor Q , a larger μ will end up with a more sparseQ . The solution to formulation delivers a tensor such that the mode 3 fibers are either all zeros vectors or non zero vectors , ie , interaction effects between 2 features xi , xj either exists on all tasks , or irrelevant for all tasks . Note that even the sparsity patterns is same for all tasks , their interactions may have different weights . It is easy to see that , this approach subsumes the traditional multi task learning as a special case : when λI → ∞ by setting regularization parameter on tensor Q to infinity , all the elements in of Q in the solution will be zeros , and the model only considers linear effects .
When the loss function L chosen is convex and continuously differentiable with Lipschitz continuous gradient [ 26 ] , then we can use proximal based gradient methods , such as first order FISTA [ 3 ] , SpaRSA [ 34 ] or second order Proximal Newton [ 22 ] to solve it efficiently . Because that the linear effects and interaction effects are decoupled in the predictive function , a major class of loss functions belong to this category , and we give a few examples of common loss functions in Table 1 . Note that even when L is non convex , a local optimal solution can be efficiently obtained using the GIST framework [ 17 ] . The key to apply these algorithms is to efficiently compute the proximal operator that associates to the problem ( refer to [ 25 ] for more details about proximal ) :
1 2
( (W − ˆW(2
F + ( Q − ˆQ(2
F ) +ρ 1RF ( W ) +ρ 2||Q||GL Sym , min W,Q where ˆW and ˆQ are intermediate solutions at each step , ρ1 and ρ2 are regularization parameters augmented with step size . Note that we have extend the Forbenius norm from matrix to tensor . We see that the problem is decoupled for W and Q . And the tensor proximal :
( Q − ˆQ(2
F + ρ2||Q||GL Sym , minQ
1 2 can be solved in the same way as the group Lasso proximal operator [ 36 ] . Moreover , we find that when the gradient is symmetric , we don’t need to enforce a symmetric tensor
1739 Table 1 : Examples of three common smooth loss functions and their gradients with the interaction augmented predictive function given in Eq ( 1 ) .
Gradient | Linear Eff . ∇WLi Gradient | Interaction Eff . ∇Qt Li
Loss with Interaction
∗ Logistic Loss Squared Loss Squared Hinge ∗ †
†
−[log(g(xi))yti + ( 1− yti)(log(1 − g(xi)) ) ]
Loss function Li
2 i Qtxi − yti||2 i Qtxi ) ) . 0 forz ≥ 1}
1 +exp ( −(xT
1 2 i wt + xT
||xT h(yti(xT i wt + xT z − 1 for 0 < z < 1 , ff.K
Q2 i,j,k , i,j k=1 for z ≤ 0 , .
||Q||GL = g(x ) is the sigmoid function defined as g(xi ) = 1/ ( z ) ={−1 h sparse norm , and we could simply use a simple alternative : and initialize the algorithm with a symmetric tensor as the starting point . The reason that symmetry holds can be explained by two parts . First , the gradient of Q is symmetric , therefore the gradient descent step won’t change the symmetry of tensor Q . Second , the proximal operator associated to sparse tensor norm won’t change the symmetry of matrix . To see this , the proximal operation is performed by vectorizing the matrix into a vector and shrink each element of the vector with respect to a input vector , which is obtained by the last gradient descent step . Since the input vector represents an symmetric matrix , the element and its symmetric element will always shrink to the same new value . Therefore , the symmetry of Q holds . The sparse tensor norm is equivalent to perform the l1 projection of vectors where each element is the l2 norm of mode 3 fiber in tensor Q . 4.3 Embedded Interaction Approach
The share interaction approach has enforced a very restrictive form of how tasks are supposed to relate to each In many applications , the prediction may be a reother . sult of complicated feature interactions , instead only involves a few interactions . Even though the prediction may involve all feature interactions , it is usually a reasonable assumption that there are patterns among these interactions . Numerically , existence of patterns imply a low dimensional subspace , which is reflected by a low rank structure in the matrix . When there are multiple related learning tasks , one way for these tasks relate to others via a shared lowdimensional subspace , which gives us a low rank tensor . As such , we may design a structured regularization to encourage the matrix Q to be a low rank tensor . In this paper we describe one convex formulation that encourages low rank structure by penalizing a tensor norm and one non convex formulation that directly learns a low rank representation . 431 Convex Formulation One way to obtain a low rank tensor is to augment our formulation with a rank penalty . One problem associates to tensor is that there is no consistent way to define the rank of a tensor . One way is to use the average rank of unfolding on different mode [ 15 ] :
N . n=1
1 N rankn(Q ) =
1 N rank(Q(n) ) ,
N . n=1 where N is the total number of mode of the tensor ( N = 3 when only pair wise interactions ) , and Q(n ) is unfold on n mode . Since minimizing the rank function is proven to be NP hard , we could penalize the trace norm instead , which is the convex envelope of the rank function . The trace norm
( g(xi ) − yti)xi i wt + xT i Qtxi − yti ) ( xT i Qtxi ) fi i Qtxi ) ) i wt + xT xi(xT ytixih i wt + xT
( g(xi ) − yti)xixT i wt + xT ( xT i Qtxi − yti)xT i i wt + xT i Qtxi ) i h i xi(xT ytixixT n=1
λI N
3 .
L(W,Q ; f , X , Y ) +λ RR1(W ) + is defined as the sum of singular values of the matrix variable [ 20 ] . We then obtain the following convex formulation : ||Q(n)||∗ , ( 7 ) min W,Q where ( .(∗ denotes the trace norm . However , this convex formulation penalizes every mode of tensor Q to be jointly low rank , which may be too restricted in practice , which may lead to suboptimal performance . Moreover , the practical way to solve the formulation in Eq ( 7 ) is to use the alternating direction methods of multipliers ( ADMM ) [ 6 ] , which introduces auxiliary variables and equality constraints , in order to decouple the three tensor trace norm terms . However , ADMM algorithm in practice is shown to have a slow convergence rate , and less preferred when composite proximal methods such as FISTA can be applied . inf
N .
||Q(n )
( n)||∗ ,
Q(1)+Q(2)++Q(N )=Q
One alternative way to address these issues is to use the latent trace norm [ 30 , 31 ] , which is defined as following for a N−way tensor : ||Q||latent = where Q(1 ) . . .Q(N ) are a set of low rank auxiliary tensors , which states that the original tensor can be decomposed into the sum of a set of tensors that are low rank in different modes . Finally , we proposed to drop the equality constraint that each auxiliary tensor equal to the original one , but we directly use the mixture of tensors to represent the original tensor , so the problem becomes a unconstrained optimization problem . The predictive function of task t with such mixture is given by : n=1 fmix(x ; wt,{Q(i)}3 i=1 ) = x
T wt + x
T
(
.3
Q(i ) t )x , i=1 d×d×K , ∀j = 1 , 2 , 3 are the auxiliary tensors where Q(j ) ∈ R for replacing the original tensor Q , matrix Q(j ) is the mode j unfolding of tensor Q(j ) t ∈ R frontal slice of tensor Q(j ) under embedded interaction approach is given by :
( j ) ∈ R d×d is the tth . Finally , our convex formulation
, Q(j )
( n1n2n3/nj )×nj
L(W,{Q(i)}3 min
W,{Q(i)}3 i=1 i=1 ; fmix , X , Y ) 3 .
+ λF RF ( W ) +λ I
||Q(j )
( j)||∗ . j=1
The convexity of this formulation holds since both the loss function and the penalty are convex . We note that this formulation can be solved in the same way as the formulation in Eq ( 7 ) , and the model is much more flexible to model the complicated interactions among the features , leveraging the advantages of such auxiliary tensors .
1740 432 Non Convex Formulation Although using proximal gradient methods we are able to secure an optimal solution for the convex formulation , the time complexity and storage cost are unacceptable in practice as the dimension of data increase . To see this , we note that the proximal operator associated to a trace norm regularized objective requires singular projections [ 20 ] , which requires cubic complexity singular value decomposition . Recall in each iteration of the gradient methods could involve more than one computation of proximal operator [ 3 ] , and thus the computation may be prohibitive when dimension grows larger . On the other hand , we have to maintain 3 dense tensors of size d× d× T which means the storage cost is at O(d2 ) , where T is the number of tasks and typically we have T ) d . Also the mixture of three low rank auxiliary tensors may lead to some difficulty when it comes to analyzing the predictive model itself . To this end , we propose to use a tensor with a explicit lowrank structure . Consider the interaction effects matrix Q ∈ d×d for one task , we assume the low rank decomposition r×r
R Q = B ˜QB is a small matrix , capturing the information of the original tensor under the set of bases ( columns ) in B . To see this , we T j , meaning the matrix Q can expand Q = is a result of interactions among bases in B and also spanned by the columns of B . We thus can use a predictive function that explicitly considers this low rank structure : is a basis matrix , ˜Q ∈ R
, where B ∈ R i,j=1 ˜Q(i,j)BiB
'r d×r
T
T fnvc(x ; w , B , ˜Q ) = x w + x
T
B ˜QB
T x .
When there are multiple tasks , our assumption for embedded interaction approach is the shared basis , meaning B is restricted to be same as all other tasks . The multi task loss function is thus given by : L(W,{B} , ˜Q ; fnvc , X , Y ) =
't(fnvc , wt , B , ˜Qt ; Xt , Yt ) ,
T . t=1 r×r×T where ˜Q ∈ R collective denotes the set of matrices ˜Q from all tasks . This loss function is not convex because of the x . This loss function multiplication of variables in x leads to our final non convex formulation for embedded :
B ˜QB
T
T
L(W,{B} , ˜Q ; fnvc , X , Y ) min
W,{B} , ˜Q
+ λF RF ( W ) +λ I RI ( {B} , ˜Q ) , where the regularization RI ( {B} , ˜Q ) can be Forbenius norm or other structural information ( eg '1 norm ) . The dimension r of B can be chosen according to the need of specific application demands , and can be selected by cross validation . In general , we choose r ) d . We note that the storage complexity for the feature interaction effects ( eg , tensor Q ) is reduce from O(d2K ) toO ( dr + r2K ) , which is dramatically smaller than the full tensor , especially in the high dimensional settings . We could use the family of block coordinate descent algorithms [ 32 ] to alternatively solve the variables W,{B} , and ˜Q , to get a local optimal solution .
5 . EXPERIMENTS
In this section , we perform experiments on both synthetic datasets and two real world datasets to evaluate the effectiveness of our proposed MTIL framework .
Single Task Synthetic Dataset
STIL 1k STIL 5k RR 1k RR 5k
1.1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
E S M R
0.1
10
20
30
50
70 40 Dimension of features
60
80
90
100
Figure 2 : RMSE comparison between RR and STIL on two synthetic datasets with sample size of 1k and 5k , respectively . 5.1 Synthetic Dataset
In order to justify the effectiveness of modeling the feature interactions and MTIL framework , we test our methods on synthetic datasets . 511 Effectiveness of modeling feature interactions In this subsection , we test whether the interactions between features can be properly handled by adding the interaction term Q . To do so , we create a single task synthetic dataset by assuming :
) + ,
( 8 ) is the feature matrix , y ∈ R d×1 n×1 is the weight vector , Q ∈ R y = Xw + diag(XQX where X ∈ R n×d is the responses , w ∈ R d×d is a symmetric , low rank sparse matrix , which represents the feature interactions in the dataset , and ∼ N ( 0 , 0.01In ) is the additive noise term . We generate 20 synthetic datasets with different sizes ( 1000 or 1k and 5000 or 5k ) and different feature dimensions ( varying from 10 to 100 , stepped by 10 ) by randomly selecting X , w , and Q and computing y according to Eq ( 8 ) .
We use single task feature interaction learning model ( STIL ) to evaluate the effectiveness of the interaction term Q :
T
λ i=1 min w,Q
||x
T i w + x i Qxi − yi||2
1 2 where w ∈ R ture interaction matrix , and ( Q(1,1 = the '1,1 norm .
2 + μ||Q||1,1 , ||w||2 ' ' is the weight vector , Q ∈ R d×d j |Qi,j| denotes is the fea d×1
2 +
2 i
We compared the Root Mean Square Error ( RMSE ) between the Ridge Regression(RR ) and STIL on both of the synthetic datasets . As the results show in Figure 2 , STIL outperforms RR on both of the datasets , which shows the effectiveness of modeling the feature interaction in the data . Besides , STIL 5k ( RR 5k ) performs better than STIL 1k ( RR 1k ) , which demonstrates that the learning models will capture the underlining models of the data better with larger training size . Also note that with the number of dimensions increases , STIL will gradually overfit the data , because of the dramatic increase of the interactions between features . n .
1741 1.1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
E S M R
Multi Task Synthetic Dataset
RR STIL MTL L MTIL L S MTIL S S MTIL L Lc MTIL L Ln MTIL S Ln MTIL S Lc
40
45
50
0.1
10
15
20
25
30
Dimension of features
35
Figure 3 : Synthetic dataset ( Multi task ) : Root Mean Square Error ( RMSE ) comparisons among all the methods . The Y axis is RMSE , X axis is dimension of features .
512 Effectiveness of MTIL In order to test the effectiveness of MTIL , we generate a multi task synthetic data by assuming :
T t ) , t = 1 , 2 , 3 , , T , n×d is the feature matrix of task t , yt ∈ R yt = Xtwt + diag(XtQtX where Xt ∈ R n×1 is the responses of task t , W ∈ R = [ w1 , w2 , w3 , , wT ] is the weights for tasks . As described in Section 4.3 , we generate feature interaction matrix Qt = BqtB and project it into a sparse , symmetric space . d×T
T
In this experiment , we generate 5 datasets with different feature dimensions from 10 to 50 , stepped by 10 , by randomly selecting Xt , wt , B and qt .
The predictive performance of the methods outlined below are examined on the synthetic multi task datasets : dently .
• Ridge Regression ( RR ) : We choose this model as the baseline and make neither assumptions of feature interaction nor the relation among all the tasks . • STIL : We perform STIL on each of the task indepen• MTL L : This approach refers to the traditional MTL method regularized by the trace norm of the weight matrix W[1 ] . It does not make assumptions on feature interactions . • MTIL L S : This approach , refers to multi task feature interaction learning regularized by the trace norm of the weight matrix W and the tensor group lasso norm of tensor Q ( see section 42 ) • MTIL S S : This approach is similar to MTIL L S except that the regularization term on W is '2,1 norm . • MTIL L Lc : This approach refers to multi task feature interaction learning regularized by the trace norm of the weight matrix W and latent trace norm of tensor Q ( see section 43 ) • MTIL S Lc : This approach is similar to MTIL L Lc except for that the regularization term on W is '2,1 norm .
• MTIL L Ln : This approach refer to multi task feature interaction learning regularized by the low rank norm of tensor Q and the trace norm of the weight matrix W ( see section 432 ) • MTIL S Ln : This approach is similar to MTIL L Ln except for that the regularization term on W is '2,1 norm .
Figure 3 compares the RMSE of the above methods on the 5 synthetic datasets . We can see that MTIL L Ln and MTIL S Ln are not that sensitive to the change of feature dimensions , thanks to the low rank assumption on the feature interaction . Also , RR and MTL L share a similar performance , which is consistent with the fact that we did not assume any low rank structure in this synthetic dataset . Note that although STIL performs almost the best on low dimensional data , its performance deteriorates rapidly compared with other MTIL methods , due to the incapability of learning the feature interactions across tasks . 5.2 School Dataset
This dataset contains the examination records of 15362 students with 28 features from 139 schools in years of 1985 , 1986 and 1987 , provided by the Inner London Education Authority(ILEA ) . In this dataset , each task is to predict exam scores for students in one out of the 139 schools . We perform 4 sets of experiments by varying the amount of training size , from 20 % to 50 % of the total sample size . We test the approaches summarized in section 512 and tune the parameters on λR in set [ 10 ] . For MTIL L Ln and MTIL S Ln methods , the rank of matrix r for each task are tuned in [ 2 , 3 , , 19 , 20 ] . For MTIL L S and MTIL L Lc , we tune the regularization parameters λI in [ 10
−1 , 10
−1 , 10
0 , , 10
0 , , 10
9 , 10
10
9 , 10
10
] .
The experimental results are shown in Table 2 . First , for most of the methods , RMSE will decrease when the training size increases . This means that providing more data in the training set will help overcome the overfitting problem . Also , we found that the performance of embedded feature approaches ( ie MTIL L Lc , MTIL L Ln , MTIL SLn ) are worse than the single task learning approach . The reason behind this is that embedded feature approaches do not have sparse constraints on the interaction term , which will severely overfit the data when there is not sufficient training samples . Additionally , the MTL L and MTIL L S obtain better performance than single task learning , which indicates that the low rank structure shared by tasks are effectively captured by the low rank assumption in these two methods . Moreover , MTIL L S method outperforms all other methods , which empirically proves the effectiveness of learning the shared interactions with sparse constraints . 5.3 Modeling Alzheimer ’s Disease
The Alzheimer ’s Disease Neuroimaging Initiative ( ADNI ) database(adniloniuclaedu ) , which was launched in 2003 as a 5 year public private partnership , is aimed to test whether the positron emission tomography ( PET ) , serial magnetic resonance imaging ( MRI ) , other biological markers , and clinical and neuropsychological assessments can be combined to measure the progression of mild cognitive impairment ( MCI ) and early Alzheimer ’s disease ( AD ) . We follow the procedure of preprocessing mentioned in [ 39 ] and obtain 648 subjects and 305 MRI features . The parameters are tuned in the same way as we described in 52
1742 Table 2 : Performance comparison of competing methods on the School dataset in terms of RMSE . The MTILL S method consistently outperforms all other methods , showing the effectiveness of the shared interactions .
RR STIL MTL L
MTIL L S MTIL S S MTIL L Lc MTIL S Lc MTIL L Ln MTIL S Ln
Training 20 % 0.9149 ± 0.0031 0.9149 ± 0.0031 0.8998 ± 0.0044 0.8623 ± 0.0048 0.8999 ± 0.0063 0.9252 ± 0.0090 0.9353 ± 0.0133 1.0084 ± 0.0180 1.0026 ± 0.0368
Training 30 % 0.9025 ± 0.0058 0.9025 ± 0.0057 0.8807 ± 0.0052 0.8506± 0.0038 0.8907 ± 0.0049 0.8893 ± 0.0037 0.9139 ± 0.0053 0.9758 ± 0.0097 0.9585 ± 0.0059
Training 40 % 0.8885 ± 0.0067 0.8885 ± 0.0067 0.8657 ± 0.0032 08511±00043 0.8832 ± 0.0077 0.8859 ± 0.0037 0.8941 ± 0.0024 0.9328 ± 0.0267 0.9297 ± 0.0253
Training 50 % 0.8722 ± 0.0059 0.8721 ± 0.0058 0.8503 ± 0.0070 0.8404 ± 0.0067 0.8686 ± 0.0046 0.8720 ± 0.0044 0.8761 ± 0.0062 0.9041 ± 0.0140 0.8965 ± 0.0066
Table 3 : Performance comparison of different methods on the ADNI dataset in terms of RMSE . All of the MTLs outperform the single task learning approaches ( RR and STIL ) and MTIL S Lc method outperforms all other methods , which demonstrates the effectiveness of embedded feature interactions .
RMSE ± standard deviation
RR STIL MTL L MTIL L S MTIL S S MTIL L Lc MTIL S Lc MTIL L Ln MTIL S Ln
0.9418 ± 0.0023 0.9417 ± 0.0021 0.9031 ± 0.0007 0.9030 ± 0.0007 0.9162 ± 0.0017 0.8941 ± 0.0050 0.8909 ± 0.0059 0.8926 ± 0.0009 0.9085 ± 0.0028
The RMSE comparison result is shown in Table 3 . First , we found that all of the MTLs outperform the single task learning approaches ( RR and STIL ) , which demonstrates the effectiveness of learning multiple tasks jointly by exploring the relatedness between tasks , as well as the existence of the underlying relatedness between tasks in the ADNI dataset . Second , the RMSE results of MTIL L S and MTLL are comparable with each other , which indicates that the multiple tasks in this dataset do not share the same feature interaction structure . Finally , the result of MTIL S Lc method outperforms all other methods , which shows superiority of our feature interaction framework . Through a mixture of 3 low rank tensor , we are able to learn the feature interaction pattern in this dataset . 5.4 Discussion
The proposed multi task feature interaction learning framework has provided us a way to bridge related tasks using interaction effects . By employing different types of regularizations on the interaction effects tensor , the formulations under this framework have very different characteristics .
For the shared interaction approach : we utilize Group Lasso on the interaction tensor to control the model complexity . The proximal operator admits a closed form solution , and thus the overall computational cost is very low . We are able to obtain interpretable results from the model , showing what are important interactions that are relevant to the prediction tasks . The main drawback is that we assume all tasks share the same set of interaction effects , which may not be the case for many data sets . One way to further improve the formulation is by extending the strong or weak heredity properties [ 5 , 23 ] to the proposed MTIL framework .
For the embedded interaction approach : we can easily obtain the global optimal for the convex formulation . Though we are able to tune the regularization parameter on the trace norms to control the rank of the interaction tensor , it is usually very hard to decide the value unless cross validation is used . A rank larger than necessary may lead to over fitting when training samples are insufficient . On the other hand , the obtained mixture of 3 tensor is hard to interpret . The non convex formulation provides a better model decomposition , from which we can see the combination of basis for different tasks and identify embedded bases that are shared among the set of tasks . The drawback of this formulation is that we may easily trapped in a bad local optimal unless we carefully choose the initial value ( eg , using the solution from the convex formulation ) .
In general , this framework can be generalized into many other possible relatedness on feature interactions by incorporating different regularization terms . Different approaches of this framework should be carefully chosen according to the application domain . In the future work we plan to study the statistical properties of the proposed model , which may lead to deeper understanding of these interaction models . 6 . CONCLUSIONS One major limitation of linear models is the lack of capability to capture predictive information from interactions between features . While introducing high order feature interaction terms can overcome this limitation , this approach tremendously increases the model complexity and imposes significant challenges in the learning against overfitting . In this paper , we proposed a novel Multi Task feature Interaction Learning ( MTIL ) framework to exploit the task relatedness from high order feature interactions , which provides better generalization performance by inductive transfer among tasks via shared representations of feature interactions . We formulate two concrete approaches under this framework and provide efficient algorithms : the shared interaction approach and the embedded interaction approach . The former assumes tasks share the same set of interactions , and the latter assumes feature interactions from multiple tasks come from a shared subspace . We have provided efficient algorithms for solving the two approaches . Extensive empirical studies on both synthetic and real datasets have demonstrated the effectiveness of the proposed framework . Acknowledgments This material is based in part upon work supported by the National Science Foundation under Grant Numbers IIS 1565596 and Office of Naval Research N00014 14 1 0631 .
1743 References [ 1 ] A . Argyriou , T . Evgeniou , and M . Pontil . Convex multi task feature learning . Machine Learning , 73(3):243–272 , 2008 .
[ 2 ] B . Bakker and T . Heskes . Task clustering and gating for bayesian multitask learning . The Journal of Machine Learning Research , 4:83–99 , 2003 .
[ 3 ] A . Beck and M . Teboulle .
A fast iterative shrinkagethresholding algorithm for linear inverse problems . SIAM journal on imaging sciences , 2(1):183–202 , 2009 .
[ 4 ] S . Ben David and R . Schuller . Exploiting task relatedness In Learning Theory and Kernel for multiple task learning . Machines , pages 567–580 . Springer , 2003 .
[ 5 ] J . Bien , J . Taylor , and R . Tibshirani . A lasso for hierarchical interactions . Annals of statistics , 41(3):1111 , 2013 .
[ 6 ] S . Boyd , N . Parikh , E . Chu , B . Peleato , and J . Eckstein . Distributed optimization and statistical learning via the alternating direction method of multipliers . Foundations and Trends Rff in Machine Learning , 3(1):1–122 , 2011 .
[ 7 ] R . J . Cadoret , W . R . Yates , G . Woodworth , and M . A . Stewart . Genetic environmental interaction in the genesis of aggressivity and conduct disorders . Archives of General Psychiatry , 52(11):916–924 , 1995 .
[ 8 ] R . Caruana . Multitask learning . Machine learning , 28(1):41–
75 , 1997 .
[ 9 ] S . Chang , G J Qi , C . C . Aggarwal , J . Zhou , M . Wang , and T . S . Huang . Factorized similarity learning in networks . In ICDM , pages 60–69 . IEEE , 2014 .
[ 10 ] X . Chen , X . Shi , X . Xu , Z . Wang , R . Mills , C . Lee , and J . Xu . A two graph guided multi task lasso approach for eqtl mapping . In AISTATS , pages 208–217 , 2012 .
[ 11 ] N . H . Choi , W . Li , and J . Zhu . Variable selection with the strong heredity constraint and its oracle property . JASA , 105(489):354–364 , 2010 .
[ 12 ] T . C . Eley , K . Sugden , A . Corsico , A . M . Gregory , P . Sham , P . McGuffin , R . Plomin , and I . W . Craig . Gene–environment interaction analysis of serotonin system markers with adolescent depression . Molecular psychiatry , 9(10):908–915 , 2004 .
[ 13 ] T . Evgeniou and M . Pontil . Regularized multi–task learning .
In SIGKDD , pages 109–117 . ACM , 2004 .
[ 14 ] J . Friedman , T . Hastie , and R . Tibshirani . A note on the group lasso and a sparse group lasso . arXiv preprint arXiv:1001.0736 , 2010 .
[ 15 ] S . Gandy , B . Recht , and I . Yamada . Tensor completion and low n rank tensor recovery via convex optimization . Inverse Problems , 27(2):025010 , 2011 .
[ 16 ] J . Gatt , C . Nemeroff , C . Dobson Stone , R . Paul , R . Bryant , P . Schofield , E . Gordon , A . Kemp , and L . Williams . Interactions between bdnf val66met polymorphism and early life stress predict brain and arousal pathways to syndromal depression and anxiety . Molecular psychiatry , 14(7):681–695 , 2009 .
[ 17 ] P . Gong , C . Zhang , Z . Lu , J . Z . Huang , and J . Ye . A general iterative shrinkage and thresholding algorithm for nonconvex regularized optimization problems . In ICML , volume 28 , page 37 , 2013 .
[ 18 ] P . Gong , J . Zhou , W . Fan , and J . Ye . Efficient multi task feature learning with calibration . In SIGKDD , pages 761–770 . ACM , 2014 .
[ 19 ] K . Hemminki , J . L . Bermejo , and A . F¨orsti . The balance between heritable and environmental aetiology of human disease . Nature Reviews Genetics , 7(12):958–965 , 2006 .
[ 20 ] S . Ji and J . Ye . An accelerated gradient method for trace norm minimization . In ICML , pages 457–464 . ACM , 2009 .
[ 21 ] S . Kim and E . P . Xing . Tree guided group lasso for multi task regression with structured sparsity . ICML , 2010 .
[ 22 ] J . Lee , Y . Sun , and M . Saunders . Proximal newton type In NIPS , pages 836–844 , methods for convex optimization . 2012 .
[ 23 ] Y . Liu , J . Wang , and J . Ye . An efficient algorithm for weak hierarchical lasso . In SIGKDD , pages 283–292 . ACM , 2014 .
[ 24 ] G . Obozinski , B . Taskar , and M . I . Jordan . Joint covariate selection and joint subspace selection for multiple classification problems . Statistics and Computing , 20(2):231–252 , 2010 .
[ 25 ] N . Parikh and S . P . Boyd . Proximal algorithms . Foundations and Trends in optimization , 1(3):127–239 , 2014 .
[ 26 ] P . Radchenko and G . M . James . Variable selection using adaptive nonlinear interaction structures in high dimensions . Journal of the American Statistical Association , 105(492):1541–1553 , 2010 .
[ 27 ] B . Romera Paredes , H . Aung , N . Bianchi Berthouze , and In ICML , pages
M . Pontil . Multilinear multitask learning . 1444–1452 , 2013 .
[ 28 ] M . J . Somers . Organizational commitment , turnover and absenteeism : An examination of direct and interaction effects . Journal of Organizational Behavior , 16(1):49–58 , 1995 .
[ 29 ] R . Tibshirani . Regression shrinkage and selection via the Journal of the Royal Statistical Society . Series B lasso . ( Methodological ) , pages 267–288 , 1996 .
[ 30 ] R . Tomioka , K . Hayashi , and H . Kashima . Estimation of arXiv preprint low rank tensors via convex optimization . arXiv:1010.0789 , 2010 .
[ 31 ] R . Tomioka and T . Suzuki . Convex tensor decomposition via structured schatten norm regularization . In NIPS , pages 1331–1339 , 2013 .
[ 32 ] P . Tseng . Convergence of a block coordinate descent method for nondifferentiable minimization . Journal of optimization theory and applications , 109(3):475–494 , 2001 .
[ 33 ] B . A . Turlach , W . N . Venables , and S . J . Wright . Simultaneous variable selection . Technometrics , 47(3):349–363 , 2005 .
[ 34 ] S . J . Wright , R . D . Nowak , and M . A . Figueiredo . Sparse reconstruction by separable approximation . Signal Processing , IEEE Transactions on , 57(7):2479–2493 , 2009 .
[ 35 ] J . Xu , P N Tan , and L . Luo . Orion : Online regularized multi task regression and its application to ensemble forecasting . In ICDM , pages 1061–1066 . IEEE , 2014 .
[ 36 ] L . Yuan , J . Liu , and J . Ye . Efficient methods for overlapping group lasso . In NIPS , pages 352–360 , 2011 .
[ 37 ] Y . Zhang , D Y Yeung , and Q . Xu . Probabilistic multi task feature selection . In NIPS , pages 2559–2567 , 2010 .
[ 38 ] J . Zhou , J . Chen , and J . Ye . Clustered multi task learning via alternating structure optimization . In NIPS , pages 702– 710 , 2011 .
[ 39 ] J . Zhou , J . Liu , V . A . Narayan , J . Ye , A . D . N . Initiative , et al . Modeling disease progression via multi task learning . NeuroImage , 78:233–248 , 2013 .
1744
