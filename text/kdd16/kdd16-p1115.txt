PTE : Enumerating Trillion Triangles On Distributed
Systems
Ha Myung Park
KAIST hamyungpark@kaistackr
Sung Hyon Myaeng myaeng@kaistackr
KAIST
U Kang∗
Seoul National University ukang@snuackr
ABSTRACT How can we enumerate triangles from an enormous graph with billions of vertices and edges ? Triangle enumeration is an important task for graph data analysis with many applications including identifying suspicious users in social networks , detecting web spams , finding communities , etc . However , recent networks are so large that most of the previous algorithms fail to process them . Recently , several MapReduce algorithms have been proposed to address such large networks ; however , they suffer from the massive shuffled data resulting in a very long processing time .
In this paper , we propose PTE ( Pre partitioned Triangle Enumeration ) , a new distributed algorithm for enumerating triangles in enormous graphs by resolving the structural inefficiency of the previous MapReduce algorithms . PTE enumerates trillions of triangles in a billion scale graph by decreasing three factors : the amount of shuffled data , total work , and network read . Experimental results show that PTE provides up to 47× faster performance than recent distributed algorithms on real world graphs , and succeeds in enumerating more than 3 trillion triangles on the ClueWeb12 graph with 6.3 billion vertices and 72 billion edges , which any previous triangle computation algorithm fail to process .
CCS Concepts •Information systems → Data mining ; •Theory of computation → Parallel algorithms ; Distributed algorithms ; MapReduce algorithms ; Graph algorithms analysis ;
Keywords triangle enumeration ; big data ; graph algorithm ; scalable algorithm ; distributed algorithm ; network analysis
∗Corresponding Author Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee . Request permissions from permissions@acmorg KDD ’16 , August 13 17 , 2016 , San Francisco , CA , USA cfl 2016 ACM . ISBN 978 1 4503 4232 2/16/08 . . . $15.00 DOI : http://dxdoiorg/101145/29396722939757
1 .
INTRODUCTION
How can we enumerate trillion triangles from an enormous graph with billions of vertices and edges ? The problem of triangle enumeration is to discover every triangle in a graph one by one where a triangle is a set of three vertices connected to each other . The problem has numerous graph mining applications including detecting suspicious accounts like advertisers or fake users in social networks [ 29 , 16 ] , uncovering hidden thematic layers on the web [ 8 ] , discovering roles [ 5 ] , detecting web spams [ 3 ] , finding communities [ 4 , 24 ] , etc . A challenge in triangle enumeration is handling big real world networks , such as social networks and WWW , which have millions or billions of vertices and edges . For example , Facebook and Twitter have 1.39 billion [ 9 ] and 300 million active users [ 27 ] , respectively , and at least 1 trillion unique URLs are on the web [ 1 ] .
Even recently proposed algorithms , however , fail to enumerate triangles from such large graphs . The algorithms have been proposed in different ways : I/O efficient [ 13 , 21 , 19 ] , distributed memory [ 2 , 11 ] , and MapReduce algorithms [ 6 , 22 , 23 , 26 ] . These algorithms have a limited scalability . The I/O efficient algorithms use only a single machine , and thus they cannot process a graph exceeding the external memory space of the machine . The distributed memory algorithms use multiple machines , but they also cannot process a graph whose intermediate data exceed the capacity of distributed memory . The state of the art MapReduce algorithm [ 23 ] , named CTTP , significantly increases the size of processable dataset by dividing the entire task into several sub tasks and processing them in separate MapReduce rounds . Even CTTP , however , takes a very long time to process an enormous graph because , in every round , CTTP reads the entire dataset and shuffles a lot of edges . Indeed , shuffling a large amount of data in a short time interval causes network congestion and heavy I/O to disks which decrease the scalability and the fault tolerance , and prolong the running time significantly . Thus , it is desirable to shrink the amount of shuffled data .
In this paper , we propose PTE ( Pre partitioned Triangle Enumeration ) , a new distributed algorithm for enumerating triangles in an enormous graph by resolving the structural inefficiency of the previous MapReduce algorithms . We show that PTE successfully enumerates trillions of triangles in a billion scale graph by decreasing three factors : the amount of shuffled data , total work , and network read . The main contributions of this paper are summarized as follows :
1115 Table 1 : Table of symbols .
Symbol Definition
G = ( V , E ) Simple graph with the set V of vertices and the set E u , v , n i , j , k ( u , v )
( u , v , n ) of edges . Vertices . Vertex colors . Edge between u and v where u ≺ v . Triangle with vertices u , v , and n where u ≺ v ≺ n .
( i , j , k ) , ( i , j ) Subproblems . d(u ) id(u ) ≺ ρ ξ
Eij E(cid:63 ) ij M P
Degree ( number of neighbors ) of u . Vertex number of u , a unique identifier . Total order on V . u ≺ v means u precedes v . Number of vertex colors . Coloring function : V → {0 , · · · , ρ − 1} . ξ(u ) is the color of vertex u . Set of edges ( u , v ) where ( ξ(u ) , ξ(v ) ) = ( i , j ) or ( j , i ) . Set of edges ( u , v ) where ( ξ(u ) , ξ(v ) ) = ( i , j ) . Available memory size of a machine . Number of processors in a distributed system .
2.1 I/O Efficient Triangle Algorithms
Recently , several triangle enumeration algorithms have been proposed in I/O efficient ways to handle graphs that do not fit into the main memory [ 13 , 21 , 19 ] . Hu et al . [ 13 ] propose Massive Graph Triangulation ( MGT ) which buffers a certain number of edges on the memory and finds all triangles containing one of these edges by traversing every vertex . Pagh and Silvestri [ 21 ] propose a cache oblivious algorithm which colors the vertices of a graph hierarchically so that it does not need to know the cache structure of a system . Kim et al . [ 19 ] present a parallel external memory algorithm exploiting the features of a solid state drive ( SSD ) .
These algorithms , however , cannot process a graph exceeding the external memory space of a single machine . Moreover , these algorithms cannot output all the triangles in a large graph containing numerous triangles ; for example , the ClueWeb12 graph ( see Section 5 ) has 3 trillion triangles requiring 70 Terabytes of storage . 2.2 Distributed Memory Triangle Algorithms The triangle enumeration problem has been recently targeted in the distributed memory model which assumes a multi processor system where each processor has its own memory . We call a processor with a memory a machine . Arifuzzaman et al . [ 2 ] propose a distributed memory algorithm based on Message Passing Interface ( MPI ) . The algorithm divides a graph into overlapping subgraphs and finds triangles in each subgraph in parallel . GraphLab PowerGraph ( shortly GraphLab ) [ 11 ] , which is an MPI based distributed graph computation framework , provides an implementation for triangle enumeration . GraphLab copies each vertex and its outgoing edges γ times on average to multiple machines where γ is determined by the characteristic of the input graph and the number of machines . Thus , γ|E| data are replicated in total , and GraphLab fails when γ|E|/P ≥ M where P is the number of processors and M is the available memory size of a machine . GraphX , a graph computation library for Spark , also provides an implementation of the same algorithm as in GraphLab ; thus it has the same limitation in scalability . PDTL [ 10 ] is a parallel and distributed extension of MGT . The experiment of the paper shows impressive speed of PDTL , but it has limited scalability : 1 ) every machine must hold a copy of an entire graph , 2 ) a part of PDTL runs on a single machine , which can be a performance bottleneck , and 3 ) it stores entire triangles in a single machine . In summary , all the previous distributed memory algorithms are limited in handling large graphs .
Figure 1 : The running time of proposed methods ( PTESC , PTECD , PTEBASE ) and competitors ( CTTP , MGT , GraphLab ) on real world datasets ( log scale ) . GraphX is not shown since it failed to process any of the datasets . Missing methods for some datasets mean they failed to run on the datasets . PTESC shows the best performances outperforming CTTP and MGT by up to 47× and 17× , respectively . Only the proposed algorithms succeed in processing the ClueWeb12 graph containing 6.3 billion vertices and 72 billion edges . in O(|E| )
• We propose PTE , a new distributed algorithm for enumerating triangles in an enormous graph , which is designed to minimize the amount of shuffled data , total work , and network read . • We prove the efficiency of the proposed algorithm : √ the algorithm operates shuffled data , M ) network read , and O(|E|3/2 ) total O(|E|3/2/ work , the worst case optimal , where |E| is the number of edges of a graph and M is the available memory size of a machine . • Our algorithm is experimentally evaluated using large real world networks . The results demonstrate that our algorithm outperforms the best previous distributed algorithms by up to 47× ( see Figure 1 ) . Moreover , PTE successfully enumerates more than 3 trillion triangles in the ClueWeb12 graph containing 6.3 billion vertices and 72 billion edges . Any previous algorithms , including GraphLab , GraphX , and CTTP , fail to process the graph because of massive intermediate data .
The codes and datasets used in this paper are provided in http://datalabsnuackr/pte The remaining part of the paper is organized as follows . In Section 2 , we review the previous researches related to the triangle enumeration . In Section 3 , we formally define the problem and introduce important concepts and notations used in this paper . We introduce the details of our algorithm in Section 4 . The experimental results are given in Section 5 . Finally , we conclude in Section 6 . The symbols frequently used in this paper are summarized in Table 1 . 2 . RELATED WORK
In order to handle enormous graphs , several triangle enumeration algorithms have been proposed recently . In this section , we introduce the distinct approaches of the algorithms , including recent MapReduce algorithms related to our work . We also outline the MapReduce model and emphasize the importance of reducing the amount of shuffled data in improving the performance .
100101102103104TWTSDYWCW09CW12Running Time ( min)PTESCPTECDPTEBASECTTPMGTGraphLab40x15x47x17x37x27x1116 2.3 MapReduce
MapReduce [ 7 ] is a programming model supporting parallel and distributed computation to process large data . MapReduce is highly scalable and easy to use , and thus it has been used for various important graph mining and data mining tasks such as radius [ 18 ] , graph queries [ 17 ] , triangle [ 16 , 22 , 23 ] , visualization [ 15 ] , and tensors [ 14 ] . A MapReduce round transforms an input set of key value pairs to an output set of key value pairs by three steps : it first transforms each pair of the input to a set of new pairs ( map step ) , groups the pairs by key so that all values with the same key are aggregated together ( shuffle step ) , and processes the values by each key separately and outputs a new set of key value pairs ( reduce step ) .
The amount of shuffled data significantly affects the performance of a MapReduce task because shuffling includes heavy tasks of writing , sorting , and reading the data [ 12 ] . In detail , each map worker buffers the pairs from the map step in memory ( collect ) . The buffered pairs are partitioned into R regions and written to local disk periodically where R is the number of reduce workers ( spill ) . Each reduce worker remotely reads the buffered data from the local disks of the map workers via a network ( shuffle ) . When a reduce worker has read all the pairs for its partition , the reduce worker sorts the pairs by keys so that all values with the same key are grouped together ( merge and sort ) . Because of such heavy I/O and network traffic , a large amount of shuffled data decreases the performance significantly . Thus , it is desirable to shrink the amount of shuffled data as much as possible . 2.4 MapReduce Triangle Algorithms
Several triangle computation algorithms have been designed in MapReduce . We review the algorithms in terms of the amount of shuffled data . The first MapReduce algorithm is proposed by Cohen [ 6 ] . The algorithm is a variant of node iterator [ 25 ] , a well known sequential algorithm . It shuffles O(|E|3/2 ) length 2 paths ( also known as wedges ) in √ a graph . Suri and Vassilvitskii [ 26 ] reduce the amount of shuffled data to O(|E|3/2/ M ) by proposing a graph partitioning based algorithm Graph Partition ( GP ) . Considering types of triangles , Park and Chung [ 22 ] improve GP by a √ constant factor in their algorithm , Triangle Type Partition ( TTP ) . That is , TTP also shuffles O(|E|3/2/ M ) data during the process . Aforementioned algorithms cause an out of space error when the size of shuffled data is larger than the total available space . Park et al . [ 23 ] avoid the out of space error by introducing a multi round algorithm , namely Colored TTP ( CTTP ) . CTTP limits the shuffled data size of a round , and thus significantly increases the size of processable √ data . However , CTTP still shuffles the same amount of data as TTP does , that is , O(|E|3/2/ M ) . Note that our proposed algorithm in this paper reduces the amount of shuffled data to O(|E| ) , improving the performance significantly .
3 . PRELIMINARIES
In this section , we define the problem that we are going to solve , introducing several terms and notations formally . We also describe two previously introduced major algorithms for distributed triangle enumeration . 3.1 Problem Definition
We first define the problem of triangle enumeration .
Definition 1 . ( Triangle enumeration ) Given a simple graph G = ( V , E ) , the problem of triangle enumeration is to discover every triangle in the graph G one by one , where a simple graph is an undirected graph containing no loops or no duplicate edges , and a triangle is a set of three vertices fully connected to each other .
Note that we do not require the algorithm to retain or emit each triangle ( u , v , n ) into any memory system , but to call a local function enum(· ) with the triangle as the parameter . On a vertex set V , we define a total order to uniquely express an edge or a triangle .
Definition 2 . ( Total order on V ) The order of two ver tices u and v is determined as follows :
• u ≺ v if d(u ) < d(v ) or ( d(u ) = d(v ) and id(u ) < id(v ) ) where d(u ) is the degree , and id(u ) is the unique identifier of a vertex u .
We denote by ( u , v ) an edge between two vertices u and v , and by ( u , v , n ) a triangle consisting of three vertices u , v , and n . Unless otherwise noted , the vertices in an edge ( u , v ) have the order of u ≺ v , and we presume it has a direction , from u to v , even though the graph is undirected . Similarly , the vertices in a triangle ( u , v , n ) also has the order of u ≺ v ≺ n , and we give each edge a name to simplify the description as follows ( see Figure 2 ) :
Definition 3 . For a triangle ( u , v , n ) where the vertices are in the order of u ≺ v ≺ n , we call ( u , v ) pivot edge , ( u , n ) port edge , and ( v , n ) starboard edge .
Figure 2 : A triangle with directions by the total order on the three vertices . 3.2 Triangle Enumeration in TTP and CTTP Park and Chung [ 22 ] propose a MapReduce algorithm , named Triangle Type Partition ( TTP ) , for enumerating tri√ angles . We introduce TTP briefly because of its relevance to our work , and we show that it shuffles O(|E|3/2/ M ) data . The first step of TTP is to color the vertices with ρ = 1} . Let Eij with i , j ∈ {0,··· , ρ − 1} and i ≤ j be the set {(u , v ) ∈ E | i = min(ξ(u ) , ξ(v ) ) and j = max(ξ(u ) , ξ(v))} . A triangle is classified as type 1 if all the vertices in the triangle have the same color , type 2 if there are exactly two vertices with the same color , and type 3 if no vertices have
O(|E|/M ) colors by a hash function ξ : V → {0,··· , ρ − the same color . TTP divides the entire problem into,ρ
+,ρ
2
3 subproblems of two types : ( i , j ) subproblem , with i , j ∈ {0,··· , ρ − 1} and i < j , is to enumerate triangles in an edge induced subgraph on ij = Eij ∪ Eii ∪ Ejj . It finds every triangle of type 1 E and type 2 where the vertices are colored with i and ( i , j , k ) subproblem , with i , j , k ∈ {0,··· , ρ − 1} and i < j < k , is to enumerate triangles in an edge induced subgraph on E It finds every triangle of type 3 where the vertices are colored with j . There are,ρ i , j and k . There are,ρ
subproblems of this type .
subproblems of this type . ijk = Eij ∪ Eik ∪ Ejk .
2
3 uvnportedgestarboardedgepivotedge1117 Algorithm 1 : Graph Partitioning
Algorithm 2 : Triangle Enumeration ( PTEBASE )
/* ψ is a meaningless dummy key Map
: input ψ ; ( u , v ) ∈ E
1 emit ( ξ(u ) , ξ(v) ) ; ( u , v )
Reduce 2 emit E(cid:63 )
: input ( i , j ) ; E(cid:63 ) ij ij to a distributed storage
*/ ij and E
Each map task of TTP gets an edge e ∈ E and emits ( i , j ) ; e and ( i , j , k ) ; e for every E ijk containing e , respectively . Thus , each reduce task gets a pair ( i , j ) ; E ij or ( i , j , k ) ; E ijk , and finds all triangle in the edge induced subgraph . For each edge , a map task emits ρ − 1 key value pairs ( Lemma 2 in [ 22] ) ; that is , O(|E|ρ ) = O(|E|3/2/ M ) pairs are shuffled in total . TTP fails to process a graph when the shuffled data size is larger than the total available space . CTTP [ 23 ] avoids the failure by dividing the tasks into multiple rounds and limiting the shuffled data size of a round . However , CTTP still shuffles exactly the same pairs as TTP ; hence CTTP also suffers from the massive shuffled data resulting in a very long running time . 4 . PROPOSED METHOD
√
In this section , we propose PTE ( Pre partitioned Triangle Enumeration ) , a distributed algorithm for enumerating triangles in an enormous graph . There are several challenges in designing an efficient and scalable distributed algorithm for triangle enumeration .
1 . Minimize shuffled data . Massive data are shuffled for generating subgraphs by the previous algorithms . How can we minimize the amount of shuffled data ?
3 . Minimize network read .
2 . Minimize computations . The previous algorithms contain several kinds of redundant operations ( details in Section 42 ) How can we remove the redundancy ? In previous algorithms , each subproblem reads necessary sets of edges via network , and the amount of network read is determined by the number of vertex colors . How can we decrease the number of vertex colors to minimize network read ? We have the following main ideas to address the above challenges , which are described in detail in later subsections . 1 . Separating graph partitioning from generating √ subgraphs decreases the amount of shuffled data to O(|E| ) from O(|E|3/2/ M ) of the previous MapReduce algorithms ( Section 41 )
2 . Considering the color direction of edges removes redundant operations and minimizes computations ( Section 42 )
3 . Carefully scheduling triangle computations in subproblems shrinks the amount of network read by decreasing the number of vertex colors ( Section 43 ) In the following we first describe PTEBASE which exploits pre partitioning to decrease the shuffled data ( Section 41 ) Then , we describe PTECD which improves on PTEBASE to remove redundant operations ( Section 42 ) After that , we explain our desired method PTESC which further improves on PTECD to shrink the amount of network read ( Section 43 ) The theoretical analysis of the methods and implementation issues are discussed in the end ( Sections 4.4 and 45 ) Note that although we describe PTE using MapReduce primitives for simplicity , PTE is general enough to be implemented in any distributed framework ( discussions in Section 4.5 and experimental comparisons in Section 523 )
/* ψ is a meaningless dummy key Map
: input ψ ; problem = ( i , j ) or ( i , j , k )
1 initialize E 2 if problem is of type ( i , j ) then ij ∪ E(cid:63 ) /* Eij = E(cid:63 ) read Eij , Eii , Ejj E ← Eij ∪ Eii ∪ Ejj
3
4 ji , and Eii = E(cid:63 ) ii
5 else if problem is of type ( i , j , k ) then ik ∪ E(cid:63 ) ji , Eik = E(cid:63 ) ki , and
/* Eij = E(cid:63 ) Ejk = E(cid:63 ) ij ∪ E(cid:63 ) jk ∪ E(cid:63 ) kj read Eij , Eik , Ejk E ← Eij ∪ Eik ∪ Ejk 8 enumerateTriangles(E )
7
6
*/
*/
*/
/* enumerate triangles in the edge induced subgraph on E */
9 Function enumerateTriangles(E ) foreach ( u , v ) ∈ E do foreach n ∈ {nu|(u , nu ) ∈ E} ∩ {nv|(v , nv ) ∈ E} do
10
11
12
13
14
15
16 if ξ(u ) = ξ(v ) = ξ(n ) then if ( ξ(u ) = i and i + 1 ≡ j mod ρ ) or ( ξ(u ) = j and j + 1 ≡ i mod ρ ) then enum((u , v , n ) ) else enum((u , v , n ) )
4.1 PTEBASE : Pre partitioned Triangle Enu meration
In this section we propose PTEBASE which rectifies the massive shuffled data problem of previous MapReduce algorithms . The main idea is partitioning an input graph into sets of edges before enumerating triangles , and storing the sets in a distributed storage like Hadoop Distributed File System ( HDFS ) of Hadoop , or Resilient Distributed Dataset ( RDD ) of Spark . We observe that the subproblems of TTP require each set Eij of edges as a unit . It implies that if each edge set Eij is directly accessible from a distributed storage , we need not shuffle the edges like TTP does . Con sequently , we partition the input graph into ρ +,ρ = ρ(ρ+1 ) and,ρ are for Eij when i = j and i < j , respectively . Each distributed . PTEBASE sets ρ to ( cid:100)6|E|/M to fit the three sets of edges according to the vertex colors in each edge ; ρ edge ( u , v ) ∈ Eij keeps the order u ≺ v . Each vertex is colored by a coloring function ξ which is randomly chosen from a pairwise independent family of functions [ 28 ] . The pairwise independence of ξ guarantees that edges are evenly
2
2
2 edge sets for an ( i , j , k ) subproblem into the memory of size M in a processor : the expected size of an edge set is 2|E|/ρ2 , and the sum 6|E|/ρ2 of the size of the three edge sets should be less than or equal to the memory size M .
After the graph partitioning , PTEBASE reads edge sets and finds triangles in each subproblem . In each ( i , j ) subproblem , it reads Eij , Eii , and Ejj , and enumerates triangles in the union of the edge sets . In each ( i , j , k ) subproblem , similarly , it reads Eij , Eik , and Ejk , and enumerates triangles in the union of the edge sets . The edge sets are read from a distributed storage via a network , and the total amount of network read is O(|E|ρ ) ( see Section 32 ) Note that the network read is different from the data shuffle ; the data shuffle is a much heavier task since it requires data collecting and writing in senders , data transfer via a network , and data merging and sorting in receivers ( see Section 23 ) However , the network read contains data transfer via a network only .
1118 3
4
5
6
7
8
13
14
17
18
( a )
( b )
Figure 3 : ( a ) An example of finding type 3 triangles containing an edge ( u , v ) ∈ Eij in an ( i , j , k ) subproblem . PTEBASE finds the triangle ( u , v , b ) by intersecting u ’s neighbor set {v , a , b} and v ’s neighbor set {n , b} . However , it is unnecessary to consider the edges in Eij since the other two edges of a type 3 triangle containing ( u , v ) must be in Eik and Ejk , respectively . ( b ) enumerateTrianglesCD(E(cid:63 ) jk ) in PTECD finds every triangle whose pivot edge , port edge , and starboard edge have the same colordirections as those of E(cid:63 ) jk , respectively . The arrows denote the color directions . ik , and E(cid:63 ) ik , E(cid:63 ) ij , E(cid:63 ) ij , E(cid:63 )
Pseudo codes for PTEBASE are listed in Algorithm 1 and Algorithm 2 . The graph partitioning is done by a pair of map and reduce steps ( Algorithm 1 ) . In the map step , PTEBASE transforms each edge ( u , v ) into a pair ( ξ(u ) , ξ(v) ) ; ( u , v ) ( line 1 ) . The edges of the pairs are aggregated by the keys ; and for each key ( i , j ) , a reduce task receives E(cid:63 ) ij and emits it to a separate file in a distributed storage ( line 2 ) , where ij is {(u , v ) ∈ E|(ξ(u ) , ξ(v ) ) = ( i , j)} . Note that the union E(cid:63 ) of E(cid:63 ) ji is Eij , that is , E(cid:63 ) ij ∪ E(cid:63 ) ij and E(cid:63 ) ji = Eij .
Thanks to the pre partitioned edge sets , the triangle enumeration is done by a single map step ( see Algorithm 2 ) . Each map task reads edge sets needed to solve a subproblem ( i , j ) or ( i , j , k ) ( lines 3 , 6 ) , makes the union of the edge sets ( lines 4 , 7 ) , and enumerates triangles with a sequential algorithm enumerateTriangles ( line 8 ) . Although any sequential algorithm for triangle enumeration can be used for enumerateTriangles , we use CompactForward [ 20 ] , one of the best sequential algorithms , with a modification ( lines 9 16 ) : we skip the vertex sorting procedure of CompactForward because the edges are already ordered by the degrees of its vertices . Given a set E of edges , enumerateTriangles runs in O(|E|3/2 ) total work , the same as that of CompactForward . Note that we propose a specialized algorithm to reduce the total work in Section 42 Note also that although every type 1 triangle appears ρ − 1 times , PTEBASE emits for each type 1 triangle of color i , the triangle only once : PTEBASE emits the triangle if and only if i + 1 ≡ j mod ρ given a subproblem ( i , j ) or ( j , i ) ( lines 12 14 ) . Still , a type1 triangle is computed ρ− 1 times which is unnecessary . We completely resolve the issue in Section 42 4.2 PTECD : Reducing the Total Work
PTECD improves on PTEBASE to minimize the amount of computations by exploiting color direction . We first give an example ( Figure 3(a ) ) to show that the function enumerateTriangles in Algorithm 2 performs redundant operations . Let us consider finding type 3 triangles containing an edge ( u , v ) ∈ Eij in an ( i , j , k ) subproblem . enumerateTriangles finds such triangles by intersecting the two outgoing neighbor sets of u and v . In the figure , the neighbor sets are {v , a , b} and {n , b} ; and we find the triangle ( u , v , b ) . However , it is unnecessary to consider edges in Eij ( that is , ( u , v ) , ( u , a ) , ( v , n ) ) since the other two edges of a type 3 tri
Algorithm 3 : Triangle Enumeration ( PTECD )
/* ψ is a meaningless dummy key Map
: input ψ ; problem = ( i , j ) or ( i , j , k )
1 if problem is of type ( i , j ) then 2 read E(cid:63 ) /* enumerate Type 1 triangles if i + 1 = j then ii , E(cid:63 ) jj ij , E(cid:63 ) ji , E(cid:63 ) enumerateTrianglesCD(E(cid:63 ) else if j = ρ − 1 and i = 0 then ii , E(cid:63 ) ii , E(cid:63 ) ii ) enumerateTrianglesCD(E(cid:63 ) jj , E(cid:63 ) jj , E(cid:63 ) jj )
/* enumerate Type 2 triangles foreach ( x , y , z ) ∈ {(i , i , j ) , ( i , j , i ) , ( j , i , i ) , ( i , j , j ) , ( j , i , j ) , ( j , j , i)} do
*/ enumerateTrianglesCD(E(cid:63 ) xy , E(cid:63 ) xz , E(cid:63 ) yz )
*/
*/
*/
9 else if problem is of type ( i , j , k ) then enumerateType3Triangles((i , j , k ) )
10
/* enumerate every triangle ( u , v , n ) such that ξ(u ) = i ,
11 Function enumerateTrianglesCD(E(cid:63 ) 12
ξ(v ) = j and ξ(n ) = k foreach ( u , v ) ∈ E(cid:63 ) ij , E(cid:63 ) ik , E(cid:63 ) jk ) ij do foreach n ∈ {nu|(u , nu ) ∈ E(cid:63 ) enum((u , v , n ) ) ik} ∩ {nv|(v , nv ) ∈ E(cid:63 ) jk} do
15 Function enumerateType3Triangles(i , j , k ) 16 read E(cid:63 ) ik , E(cid:63 ) foreach ( x , y , z ) ∈ {(i , j , k ) , ( i , k , j ) , ( j , i , k ) , ( j , k , i ) , ( k , i , j ) , ( k , j , i)} do ki , E(cid:63 ) kj jk , E(cid:63 ) ij , E(cid:63 ) ji , E(cid:63 ) enumerateTrianglesCD(E(cid:63 ) xy , E(cid:63 ) xz , E(cid:63 ) yz ) angle containing ( u , v ) must be in Eik and Ejk , respectively . The redundant operations can be removed by intersecting u ’s neighbors only in Eik and v ’s neighbors only in Ejk instead of looking at all the neighbors of u and v . In the figure , the two neighbor sets are both {b} ; and we find the same triangle ( u , v , b ) . PTECD removes the redundant operations by adopting a new function enumerateTrianglesCD ( lines 11 14 in Algorithm 3 ) . We define the color direction of an edge ( u , v ) to be from ξ(u ) to ξ(v ) ; and we also define the color direction of E(cid:63 ) ij to be from i to j . Then , enumerateTrianglesCD(E(cid:63 ) ij , E(cid:63 ) ik , E(cid:63 ) jk ) finds every triangle whose pivot edge , port edge , and starboard edge have the same color directions as those of E(cid:63 ) jk , respectively ( see Figure 3(b) ) . Note that the algorithm does not look at any edges in Eij for the intersection in the example of Figure 3(a ) by separating the input edge sets . ik , and E(cid:63 ) ij , E(cid:63 )
Redundant operations of another type appear in ( i , j ) subproblems ; PTEBASE outputs a type 1 triangle exactly once but still computes it multiple times : type 1 triangles with a color i appears ρ − 1 times in ( i , j ) or ( j , i ) subproblems for j ∈ {0,··· , ρ − 1} \ {i} . PTECD resolves the duplicate computation by performing enumerateTrianglesCD(E(cid:63 ) ii ) exactly once for each vertex color i ; and thus , every type 1 triangle appears only once . ii , E(cid:63 ) ii , E(cid:63 )
Algorithm 3 shows PTECD using the new function enumerateTrianglesCD . To find type 3 triangles in each ( i , j , k ) subproblem , PTECD calls the function enumerateTrianglesCD 6 times for every possible color direction ( lines 10 , 15 18 ) ( see Figure 4(a) ) . To find type 2 triangles in each ( i , j ) subproblem , similarly , PTECD calls enumerateTrianglesCD 6 times ( lines 7 8 ) ( see Figure 4(b) ) . For type 1 triangles
EikEjkEijuvnabE?ikE?jkE?ijuvn1119 ( a ) The six color directions of a type 3 triangle in an ( i , j , k ) subproblem .
( b ) The six color directions of a type 2 triangle in an ( i , j ) subproblem . ii , E(cid:63 ) ii , E(cid:63 )
Figure 4 : The color directions of a triangle according to its type . The function enumerateTrianglesCD is called for each color direction ; that is , PTECD calls it 6 times for type 3 triangles and type 2 triangles , respectively . We add O(ρ ) because ( d(cid:63)(u ) + d(cid:63)(v))/ρ can be less than 1 whose vertices have a color i , PTECD performs enumerii ) only if i + 1 ≡ j mod ρ ateTrianglesCD(E(cid:63 ) but d(cid:63 ) ξ(v)ξ(u)(v ) is always larger than or equal given a subproblem ( i , j ) or ( j , i ) so that enumerateTrianto 1 . Meanwhile , the number of operations by PTECD for glesCD(E(cid:63 ) ( u,v)∈E ( d(cid:63)(u ) + d(cid:63)(v ) ) as we ii ) operates exactly once ( lines 3 6 ) . As a result , the algorithm emits every triangle exactly once . will see in Theorem 5 . Thus , PTECD reduces the number of operations by more than 2 − 2 ρ times from PTEBASE in expectation . 4.3 PTESC : Reducing the Network Read intersecting neighbor sets is
ξ(u)ξ(v)(u ) + d(cid:63 ) ii , E(cid:63 ) ii , E(cid:63 )
Removing the two types of redundant operations , PTECD decreases the number of operations for intersecting neighbor sets by more than 2− 2 ρ times from PTEBASE in expectation . As we will see in Section 521 , PTECD decreases the operations by up to 6.83× than PTEBASE on real world graphs . Theorem 1 . PTECD decreases the number of operations ρ times com for intersecting neighbor sets by more than 2− 2 pared to PTEBASE in expectation . Proof . To intersect the sets of neighbors of u and v in an edge ( u , v ) such that ξ(u ) = i , ξ(v ) = j and i = j , the function enumerateTriangles in PTEBASE performs d(cid:63 ) ij(u)+d(cid:63 ) jk(v ) operations while enumerateTrianglesCD in PTECD performs d(cid:63 ) jk(v ) operations for each color k ∈ {0,··· , ρ − 1} \ {i , j} where d(cid:63 ) ij(u ) is the number of u ’s neighbors in E(cid:63 ) ij . Thus , PTEBASE performs ( ρ − 2 ) × ( d(cid:63 ) ξ(v)ξ(u)(v ) ) additional operations compared to PTECD for each ( u , v ) ∈ Eout where Eout is the set of edges ( u , v ) ∈ E such that ξ(u ) = ξ(v ) ; that is ,
ξ(u)ξ(v)(u ) + d(cid:63 ) ik(u ) + d(cid:63 ) ik(u)+d(cid:63 ) ji(v)+d(cid:63 ) d(cid:63 ) ξ(u)ξ(v)(u ) + d(cid:63 )
ξ(v)ξ(u)(v )
( 1 )
( ρ − 2 ) ×
( u,v)∈Eout
PTESC further improves on PTECD to shrink the amount of network read by scheduling calls of the function enumerij in ρ − 1 subproblems , ateTrianglesCD . Reading each E(cid:63 ) PTECD ( as well as PTEBASE ) reads O(|E|ρ ) data via a network in total . For example , E(cid:63 ) 01 is read in every ( 0 , 1 , k ) subproblem for 2 ≤ k < ρ , and the ( 0 , 1 ) subproblem . It implies that the amount of network read depends on ρ , the number of vertex colors . PTEBASE and PTECD set ρ to In PTESC , we
( cid:100)6|E|/M as mentioned in Section 41 reduce it to ( cid:100)5|E|/M by setting the sequence of trian ij , E(cid:63 ) ik , E(cid:63 ) gle computation as in Figure 5 which represents the schedule of data loading for an ( i , j , k ) subproblem . We denote by ∆ijk the set of triangles enumerated by enumerateTrianglesCD(E(cid:63 ) jk ) . PTECD handles the triangle sets one by one from left to right in the figure . The check marks ( ) show the relations between edge sets and triangle sets . For example , E(cid:63 ) jk should be retained in the memory together to enumerate ∆ijk . When we restrict to read an edge set only once in a subproblem , the shaded areas represent when the edge sets are in the memory . For example , E(cid:63 ) ij is read before ∆ijk , and is released after ∆kij . Then , we can easily see that the maximum number of edge sets which are retained in the memory together at a time is 5 , and it leads to setting ρ to ( cid:100)5|E|/M . The pro ik , and E(cid:63 ) ij , E(cid:63 )
∆ijk ∆ikj ∆jik ∆jki ∆kij ∆kji
E(cid:63 ) ij E(cid:63 ) ik E(cid:63 ) ji E(cid:63 ) jk E(cid:63 ) ki E(cid:63 ) kj
Figure 5 : The schedule of data loading for an ( i , j , k ) subproblem . PTESC enumerates triangles in columns one by one from left to right . Each checkmark ( ) shows the relations between edge sets and triangle types . Each shaded area denotes when the edge set is in the memory , when we restrict to read an edge set only once in a subproblem . ii(v ) + d(cid:63 ) ij(u ) + d(cid:63 )
Given an edge ( u , v ) such that ξ(u ) = ξ(v ) = i , PTEBASE ii(u ) + d(cid:63 ) performs d(cid:63 ) ij(v ) operations for each color j ∈ {0,··· , ρ − 1} \ {i} ; meanwhile , PTECD performs ij(v ) operations for each color j ∈ {0,··· , ρ − 1} . d(cid:63 ) ij(u ) + d(cid:63 ) Thus , PTEBASE performs ( ρ−2)×(d(cid:63 ) ξ(v)ξ(u)(v ) ) more operations than PTECD for each ( u , v ) ∈ Ein where Ein is E \ Eout ; that is , ( ρ − 2 ) ×
ξ(u)ξ(v)(u)+d(cid:63 ) d(cid:63 ) ξ(u)ξ(v)(u ) + d(cid:63 )
ξ(v)ξ(u)(v )
( 2 )
( u,v)∈Ein
Then , the total number of additional operations performed by PTEBASE , compared to PTECD , is the sum of ( 1 ) and ( 2 ) :
( ρ − 2 ) ×
ξ(u)ξ(v)(u ) + d(cid:63 ) d(cid:63 )
ξ(v)ξ(u)(v )
( 3 )
( u,v)∈E The expected value of d(cid:63 ) ξ(u)ξ(v)(u ) is d(cid:63)(u)/ρ where d(cid:63)(u ) is the number of neighbors v of u such that u ≺ v , since the coloring function ξ is randomly chosen from a pairwise independent family of functions . Thus , ( 3 ) becomes as follows :
×
( u,v)∈E
( ρ − 2 )
ρ
,d(cid:63)(u ) + d(cid:63)(v ) + O(ρ )
( 4 ) i≺j≺kj≺i≺kk≺i≺ji≺k≺jj≺k≺ik≺j≺iEijEikEjkEiiEjjEiji≺i≺ji≺j≺ij≺i≺ij≺j≺ij≺i≺ji≺j≺j1120 Algorithm 4 : Type 3 triangle enumeration in PTESC 1 Function enumerateType3Triangles(i , j , k ) 2 read E(cid:63 ) foreach ( x , y , z ) ∈ {(i , j , k ) , ( i , k , j ) , ( j , i , k)} do jk , E(cid:63 ) kj ik , E(cid:63 ) ij , E(cid:63 ) ji , E(cid:63 )
3
4
5
6
7
8 enumerateTrianglesCD(E(cid:63 ) xy , E(cid:63 ) xz , E(cid:63 ) yz ) release E(cid:63 ) ik read E(cid:63 ) foreach ( x , y , z ) ∈ {(j , k , i ) , ( k , i , j ) , ( k , j , i)} do ki enumerateTrianglesCD(E(cid:63 ) xy , E(cid:63 ) xz , E(cid:63 ) yz ) cedure of type 3 triangle enumeration with the scheduling method is described in Algorithm 4 which replaces the function enumerateType3Triangles in Algorithm 3 . Note that the number 5 of edge sets loaded in the memory at a time is optimal as shown in the following theorem .
Theorem 2 . Given an ( i , j , k ) subproblem , the maximum number of edge sets retained in the memory at a time cannot be smaller than 5 , if each edge set can be read only once .
Proof . Suppose that there is a schedule to make the maximum number of edge sets retained in the memory at a time less than 5 . Then , we first read 4 edge sets in the memory . Then , 1 ) any edge set in the memory cannot be released until all triangles containing an edge in the set have been enumerated , and 2 ) we cannot read another edge set until we release one in the memory . Thus , for at least one edge set in the memory , it should be able to process all triangles containing an edge in the edge set without reading an additional edge set . However , it is impossible because enumerating all triangles containing an edge in an edge set requires 5 edge sets but we have only 4 edge sets . For example , the triangles in ∆ijk , ∆ikj , and ∆kij , which are related kj , and E(cid:63 ) to an edge set E(cid:63 ) ki . Thus , there is no schedule to make the maximum number of edge sets retained in the memory at a time less than 5 . 4.4 Analysis ij , require E(cid:63 ) jk , E(cid:63 ) ik , E(cid:63 ) ij , E(cid:63 ) i = j . Without loss of generality , we assume i < j . Then , ij is read ρ − 2 times in ( i , j , k ) or ( i , k , j ) or ( k , i , j ) subE(cid:63 ) problems where k ∈ {0,··· , ρ − 1} \ {i , j} , and once in an ( i , j ) subproblem ; ρ − 1 times in total . The total amount of data read by PTE is as follows :
 = O
|E|3/2√
M
5|E| M
− 1
( ρ−1 )
|E(cid:63 ) ij| = |E|(ρ−1 ) = |E|

ρ−1
ρ−1 i=0 j=0 where |E(cid:63 ) ij| is the number of edges in E(cid:63 ) ij .
Finally , we prove the claimed total work of the proposed algorithm .
Theorem 5 . PTE requires O(|E|3/2 ) total work . Proof . Intersecting two sets requires comparisons as many times as the number of elements in the two sets . Accordingly , the number of operations performed by enumerateTrianglesCD(E(cid:63 ) ij , E(cid:63 ) ik , E(cid:63 ) jk ) is
( u,v)∈E(cid:63 ) ij d(cid:63 ) ik(u ) + d(cid:63 ) jk(v ) + O(1 ) i=0 j=0
ρ−1
ρ−1 ik(u ) + d(cid:63 ) ik(u ) is the number of u ’s neighbors in E(cid:63 ) where d(cid:63 ) ik . We put O(1 ) since d(cid:63 ) jk(v ) can be smaller than 1 . PTESC ( as well as PTECD ) calls enumerateTrianglesCD for every possible triple ( i , j , k ) ∈ {0,··· , ρ − 1}3 ; thus the total number of operations is as follows :
ρ−1
,d(cid:63)(u ) + d(cid:63)(v)√ The left term O(|E|ρ ) is O(|E|3/2/ M ) for checking all edges in each subproblem , which occurs also in PTEBASE . The right summation is the number of operations for intersecting neighbors , and it is O(|E|3/2 ) when the vertices are ordered by Definition 2 because the maximum value of d(cid:63)(u ) for every u ∈ V is 2|E| as proved in [ 25 ] . d(cid:63 ) ik(u ) + d(cid:63 ) jk(v ) + O(1 )
( u,v)∈E(cid:63 ) ij
=O(Eρ ) +
( u,v)∈E k=0
In this section we analyze the proposed algorithm in terms of the amount of shuffled data , network read , and total work . We first prove the claimed amount of shuffled data generated by the graph partitioning in Algorithm 1 .
Theorem 3 . The amount of shuffled data for partitioning a graph is O(|E| ) where |E| is the number of edges in the graph .
Proof . The pairs emitted from the map operation is exactly the data to be shuffled . For each edge , a map task emits one pair ; accordingly , the amount of shuffled data is the number |E| of edges in the graph .
√ We emphasize that while the previous MapReduce algorithms shuffle O(|E|3/2/ M ) data , we reduce it to be O(|E| ) . Instead of data shuffle requiring heavy disk I/O , network read , and massive intermediate data , we only require the same amount of network read , that is O(|E|3/2/
Theorem 4 . PTE requires O(|E|3/2/ Proof . We first show that every E(cid:63 ) ij
√ M ) network read . for ( i , j ) ∈ {0,··· , ρ − 1}2 are read ρ − 1 times . It is clear that E(cid:63 ) such that i = j is read ρ− 1 times in ( i , k ) or ( k , i ) subprobij lems for k ∈ {0,··· , ρ − 1} \ {i} . We now consider E(cid:63 ) ij for
√ M ) .
Note that it is the worst case optimal and the same as one of the best sequential algorithms [ 20 ] . 4.5 Implementation
In this section , we discuss practical implementation issues of PTEs . We focus on the most famous distributed computation frameworks , Hadoop and Spark . Note that PTEs can be implemented for any distributed framework which supports map and reduce functionalities . PTE on Hadoop . We describe how to implement PTE on Hadoop which is the de facto standard of MapReduce framework . The graph partitioning method ( Algorithm 1 ) of PTE is implemented as a single MapReduce round . The result of the graph partitioning method has a custom output format that stores each edge set as a separate file in Hadoop Distributed File System ( HDFS ) ; and thus each edge set is accessible by the path . The triangle enumeration method ( Algorithms 2 or 3 ) of PTE is implemented as a single map step where each map task processes an ( i , j ) or ( i , j , k ) subproblem . For this purpose , we generate a text file where each line is ( i , j ) or ( i , j , k ) , and make each map task read a line and solve the subproblem . PTE on Spark . We describe how to implement PTE on Spark , which is another popular distributed computing
1121 Table 2 : The summary of datasets .
Dataset Twitter ( TWT)1 SubDomain ( SD)2 YahooWeb ( YW)3 ClueWeb09 ( CW09)4 ClueWeb12 ( CW12)5 ClueWeb12/256 ClueWeb12/128 ClueWeb12/64 ClueWeb12/32 ClueWeb12/16 ClueWeb12/8 ClueWeb12/4 ClueWeb12/2
Vertices
Edges
Triangles
42M 101M 1.4B 4.8B 6.3B 287M 457M 679M 947M 1.3B 1.8B 2.6B 4.0B
1.2B 1.9B 6.4B 7.9B 72B 260M 520M 1.0B 2.1B 4.2B 8.2B 17B 35B
34824916864 417761664336 85782928684 31012381940 3064499157291 184494 1458207 11793453 94016252 750724625 6015772801 48040237257 384568217900 framework . The reduce operation of Algorithm 1 is replaced by a pair of partitionBy and mapPartitions operations of general Resilient Distributed Dataset ( RDD ) . The partitionBy operation uses a custom partitioner that partitions edges according to their vertex colors . The mapPartition operation outputs an RDD ( namely edgeRDD ) where each partition contains an edge set . A special RDD where each partition is in charge of a subproblem ( i , j ) or ( i , j , k ) wraps the edgeRDD ; the edge sets used in each partition of the special RDD are loaded directly from the edgeRDD . 5 . EXPERIMENTS
In this section , we experimentally evaluate our algorithms and compare them to recent single machine and distributed algorithms . We aim to answer the following questions .
Q1 How much do the three methods of PTE contribute to the performance improvement ? ( Section 521 )
Q2 How does PTE scale up in terms of the number of ma chines ? ( Section 522 )
Q3 How does the performance of PTE change depending on the underlying distributed framework ( MapReduce or Spark ) ? ( Section 523 )
We first introduce datasets and experimental environment in Section 51 After that , we answer the questions in Section 5.2 presenting the result of the experiments . 5.1 Setup 511 Datasets We use real world datasets to evaluate the proposed algorithms . The datasets are summarized in Table 2 . Twitter is a followee follower network in the social network service Twitter . SubDomain is a hyperlink network among domains where an edge exists if there is at least one hyperlink between two subdomains . YahooWeb , ClueWeb09 , and ClueWeb12 are page level hyperlink networks on the Web . ClueWeb12/k is a subgraph of ClueWeb12 . We use random edge sampling to make ClueWeb12/k , with the sampling probability 1/k where k varies from 2 to 256 . Each dataset is preprocessed to be a simple graph . We reorder the vertices in each edge ( u , v ) to be u ≺ v using an algorithm in [ 6 ] . These tasks are done in O(E ) . 1 http://ankaistackr/traces/WWW2010html
2
3
4 http://webdatacommons.org/hyperlinkgraph http://webscopesandboxyahoocom http://bostonlticscmuedu/clueweb09/wiki/tiki indexphp ? page=Web+Graph 5 http://wwwlemurprojectorg/clueweb12/webgraphphp
512 Experimental Environment We implement PTE on Hadoop ( open source version of MapReduce ) and Spark . Results described in Sections 521 and 522 are from Hadoop implementation of PTE ; we also describe the Spark results in Section 523 We compare PTEs with previous algorithms : CTTP [ 23 ] , MGT [ 13 ] and the triangle counting implementations on GraphLab and GraphX . CTTP is the state of the art MapReduce algorithm . MGT is an I/O efficient external memory algorithm . GraphX is a graph processing API on Spark , a distributed computing framework . GraphLab is another distributed graph processing framework using MPI .
All experiments were conducted on a cluster with 41 machines where each machine is equipped with an Intel Xeon E3 1230v3 CPU ( quad core at 3.30GHz ) , and 32GB RAM . The cluster runs Hadoop v121 , and consists of a master node , and 40 slave nodes . Each slave node can run 3 mappers and 3 reducers ( 120 mappers and 120 reducers in total ) concurrently . The memory size for each mapper and reducer is set to 4GB . Spark v151 is also installed at the cluster where each slave is set to use 30GB memory and 3 cores of CPU ; one core is for system maintenance . We operate GraphLab PowerGraph v2.2 on OpenRTE v154 at the same cluster servers . 5.2 Experimental Results
In this section , we present experimental results to answer the questions listed at the beginning of Section 5 . 521 Effect of PTE ’s three methods Effect of pre partitioning . We compare the amount of shuffled data between PTE and CTTP to show the effect of pre partitioning ( Section 41 ) Figure 6(a ) shows the results on ClueWeb12 with various numbers of edges . It shows that PTE shuffles far fewer data than CTTP , and the difference gets larger as the data size increases ; as the number of edges varies from 0.26 billions to 36 billions , the difference increases from 30× to 245× . The slopes for PTE and CTTP are 0.99 and 1.48 , respectively . They reflect the claimed complexity of shuffled data size , O(|E| ) of PTE and √ O(|E|1.5/ M ) of CTTP . Figure 6(b ) shows the results on real world graphs ; PTE shuffles far fewer data by up to 175× than CTTP does . Effect of color direction . To show the effect of colordirection ( Section 4.2 ) , we count the number of all oper
( a )
( b )
Figure 6 : The shuffled data size of PTE and CTTP ( a ) on ClueWeb12 with various numbers of edges , and ( b ) on real world graphs . PTE shuffles up to 245× fewer data than CTTP on ClueWeb12 ; the gap grows when the data size increases . On real world graphs , PTE shuffles up to 175× fewer data than CTTP on ClueWeb09 .
10010110210310410510610 1100101102Shuffled Data ( GBytes)Number of edges ( ×109)slope=148slope=099PTECTTP245x30x101102103104105TWTSDYWCW09CW12Shuffled Data ( GBytes)PTECTTP66x85x162x175x1122 Table 3 : The number of operations by PTECD and PTEBASE on various graphs . PTECD decreases the number of operations by up to 6.85× from PTEBASE .
Dataset
PTEBASE 1.5 × 108 CW12/256 6.2 × 108 CW12/128 2.7 × 109 CW12/64 1.2 × 1010 CW12/32 4.9 × 1010 CW12/16 2.1 × 1011 CW12/8 8.9 × 1011 CW12/4 3.7 × 1012 CW12/2 1.6 × 1013 CW12 TWT 1.1 × 1012 SD 8.3 × 1012 YW 7.4 × 1011 2.7 × 1011
CW09
PTECD 2.1 × 107 1.0 × 108 4.6 × 108 2.2 × 109 1.1 × 1010 5.4 × 1010 2.6 × 1011 1.2 × 1012 4.9 × 1012 5.4 × 1011 4.0 × 1012 2.9 × 1011 6.9 × 1010
PTEBASE
PTECD
6.85 6.22 5.83 5.10 4.44 3.90 3.47 3.18 3.14 2.03 2.06 2.55 3.94
Table 4 : The amount of data read via a network on ) is about 1.10 ≈ various graphs . The ratio ( PTECD PTESC
6/5 for all datasets , as expected .
Dataset
PTECD
CW12/256 CW12/128 CW12/64 CW12/32 CW12/16 CW12/8 CW12/4 CW12/2 CW12 TWT SD TW CW09
22.5GB 58.8GB 174GB 485GB 1.3TB 3.7TB 9.9TB 26.9TB 72.9TB 102GB 206GB 1.9TB 2.9TB
PTESC
18.7GB 51.5GB 162GB 426GB 1.2TB 3.3TB 9.0TB 24.1TB 64.3TB 83GB 191GB 1.7TB 2.6TB
PTECD PTESC 1.20 1.14 1.08 1.14 1.09 1.10 1.10 1.12 1.13 1.23 1.08 1.13 1.11
) is about 1.10 ≈6/5 . ations in intersecting two neighbor sets ( line 13 in Algorithm 3 ) in Table 3 . PTECD reduces the number of comparisons by up to 6.85× from PTEBASE by the color direction . Effect of scheduling computation . We also show the effect of scheduling calls of the function enuerateTrianglesCD ( Section 4.3 ) by comparing the amount of data read via a network by PTECD and PTESC in Table 4 . As expected , for every dataset , the ratio ( P T ECD P T ESC Running time comparison . We now compare the running time of PTEs , and competitors ( CTTP , MGT , GraphLab , GraphX ) in Figure 7 . PTESC shows the best performance and PTECD follows it very closely . CTTP fails to run when edges are more than 20 billions within 5 days . GraphLab , GraphX and MGT also fail when the number of edges is larger than 600 million , 3 billion and 10 billion , respectively , because of out of memory or out of range error . The out of range error occurs when a vertex id exceeds the range of 32 bit integer limit . Note that , even when MGT can treat vertex ids exceeding the integer range , the performance of MGT would worsen as the graph size increases since MGT performs massive I/O ( O(|E|2/M ) ) when the input data size is large . The slope of the PTEs is 1.26 , which is smaller than 15 It means that the practical time complexity is smaller than the worst case O(|E|3/2 ) .
Figure 1 shows the running time of various algorithms on real world datasets . PTESC shows the best performances outperforming CTTP and MGT by up to 47× and 17× , respectively . Only the proposed algorithms succeed in processing ClueWeb12 with 6.3 billion vertices and 72 billion edges while all other algorithms fail to process the graph .
Figure 7 : The running time on ClueWeb12 with various numbers of edges . oom : out of memory . PTESC shows the best data scalability ; only PTEs succeed in processing the subgraphs containing more than 20B edges . The pre partitioning ( CTTP vs PTEBASE ) significantly reduces the running time while the effect of the scheduling function ( PTECD vs PTESC ) is relatively insignificant . 522 Machine Scalability We evaluate the machine scalability of PTEs by measuring the running time of them and CTTP on YahooWeb varying the number of machines from 5 to 40 in Figure 8 . Note that GraphX and GraphLab are omitted because they fail to process YahooWeb on our cluster . Every PTE shows strong scalability : the slope 0.94 of the PTEs is very close to the ideal value 1 . It means that the running time decreases 20.94 = 1.92 times as the number of machines is doubled . On the other hand , CTTP shows weak scalability when the number of machine increases from 20 to 40 . 523 PTE on Spark We implemented PTESC on Spark as well as on Hadoop to show that PTEs are general enough to be implemented in any distributed system supporting map and reduce functionalities . We compare the running time of the two implementations in Figure 9 . The result indicates that the Spark implementation does not show a better performance than the Hadoop implementation even though the Spark implementation uses a distributed memory as well as disks . The reason is that PTE is not an iterative algorithm ; thus , there are little data to reuse from memory . Even reading data
Figure 8 : Machine scalability of PTEs and CTTP on YahooWeb . GraphLab and GraphX are excluded because they failed to process YahooWeb . PTEs show very strong scalability with exponent 0.94 which is very close to 1 , the ideal .
102103104105106 1 10 100Running time ( sec)Number of edges ( ×109)slope=1.26Timeout ( >120h)oomoomOut OfRangeCTTPGraphXPTEBASEPTECDPTESCMGTGraphLab103104105106 5 10 20 40Running time ( sec)Number of machinesslope= 0.94PTESCPTECDPTEBASECTTP1123 [ 7 ] Jeffrey Dean and Sanjay Ghemawat . Mapreduce :
Simplified data processing on large clusters . In OSDI , pages 137–150 , 2004 .
[ 8 ] Jean Pierre Eckmann and Elisha Moses . Curvature of co links uncovers hidden thematic layers in the world wide web . PNAS , 99(9):5825–5829 , 2002 .
[ 9 ] Facebook . http://newsroomfbcom/company info , 2015 . [ 10 ] Ilias Giechaskiel , George Panagopoulos , and Eiko Yoneki .
PDTL : parallel and distributed triangle listing for massive graphs . In ICPP , 2015 .
[ 11 ] Joseph E . Gonzalez , Yucheng Low , Haijie Gu , Danny
Bickson , and Carlos Guestrin . Powergraph : Distributed graph parallel computation on natural graphs . In OSDI , pages 17–30 , 2012 .
[ 12 ] Herodotos Herodotou . Hadoop performance models . arXiv , 2011 .
[ 13 ] Xiaocheng Hu , Yufei Tao , and Chin Wan Chung . Massive graph triangulation . In SIGMOD , pages 325–336 , 2013 .
[ 14 ] ByungSoo Jeon , Inah Jeon , Lee Sael , and U Kang . Scout :
Scalable coupled matrix tensor factorization algorithm and discoveries . In ICDE , 2016 .
[ 15 ] U Kang , Jay Yoon Lee , Danai Koutra , and Christos
Faloutsos . Net ray : Visualizing and mining billion scale graphs . In PAKDD , 2014 .
[ 16 ] U Kang , Brendan Meeder , Evangelos E . Papalexakis , and
Christos Faloutsos . Heigen : Spectral analysis for billion scale graphs . TKDE , pages 350–362 , 2014 .
[ 17 ] U Kang , Hanghang Tong , Jimeng Sun , Ching Yung Lin , and Christos Faloutsos . Gbase : an efficient analysis platform for large graphs . VLDB J . , 21(5):637–650 , 2012 .
[ 18 ] U Kang , Charalampos E . Tsourakakis , and Faloutsos
Faloutsos . Pegasus : A peta scale graph mining system implementation and observations . ICDM , 2009 .
[ 19 ] Jinha Kim , Wook Shin Han , Sangyeon Lee , Kyungyeol
Park , and Hwanjo Yu . OPT : A new framework for overlapped and parallel triangulation in large scale graphs . In SIGMOD , pages 637–648 , 2014 .
[ 20 ] Matthieu Latapy . Main memory triangle computations for very large ( sparse ( power law ) ) graphs . Theor . Comput . Sci . , pages 458–473 , 2008 .
[ 21 ] Rasmus Pagh and Francesco Silvestri . The input/output complexity of triangle enumeration . In PODS , pages 224–233 , 2014 .
[ 22 ] Ha Myung Park and Chin Wan Chung . An efficient mapreduce algorithm for counting triangles in a very large graph . In CIKM , pages 539–548 , 2013 .
[ 23 ] Ha Myung Park , Francesco Silvestri , U Kang , and
Rasmus Pagh . Mapreduce triangle enumeration with guarantees . In CIKM , pages 1739–1748 , 2014 .
[ 24 ] Filippo Radicchi , Claudio Castellano , Federico Cecconi ,
Vittorio Loreto , and Domenico Parisi . Defining and identifying communities in networks . PNAS , 101(9):2658–2663 , 2004 .
[ 25 ] Thomas Schank . Algorithmic aspects of triangle based network analysis . Phd thesis , University Karlsruhe , 2007 .
[ 26 ] Siddharth Suri and Sergei Vassilvitskii . Counting triangles and the curse of the last reducer . In WWW , pages 607–614 , 2011 .
[ 27 ] Twitter . https://abouttwittercom/company , 2015 . [ 28 ] Mark N . Wegman and Larry Carter . New hash functions and their use in authentication and set equality . J . Comput . Syst . Sci . , 22(3):265–279 , 1981 .
[ 29 ] Zhi Yang , Christo Wilson , Xiao Wang , Tingting Gao ,
Ben Y . Zhao , and Yafei Dai . Uncovering social network sybils in the wild . TKDD , 2014 .
Figure 9 : The running time of PTESC on Hadoop and Spark . There is no significant difference between them . from distributed memory , not from remote disk , does not contribute to performance improvement of PTE ; they take similar time since disk reading and network transmission occur simultaneously . 6 . CONCLUSION
In this paper , we propose PTE , a scalable distributed algorithm for enumerating triangles in very large graphs . We carefully design PTE so that it minimizes the amount of √ shuffled data , total work , and network read . PTE operates in O(|E| ) shuffled data , O(|E|3/2/ M ) network read , and O(|E|3/2 ) total work , the worst case optimal . PTE shows the best performances in real world data : it outperforms the state of the art scalable distributed algorithm by up to 47× . Also , PTE is the only algorithm that successfully enumerates more than 3 trillion triangles in ClueWeb 12 graph with 72 billion edges while all other algorithms including GraphLab , GraphX , MGT , and CTTP fail . Future research directions include extending the work to support general subgraphs . Acknowledgments This work was supported by IT R&D program of MSIP/IITP [ R0101 15 0176 , Development of Core Technology for Humanlike Self taught Learning based on Symbolic Approach ] . This work was also funded by Institute for Information communications Technology Promotion ( IITP ) grant funded by the Korea government ( MSIP ) ( No.R0190 15 2012 , ” High Performance Big Data Analytics Platform Performance Acceleration Technologies Development ” ) . The ICT at Seoul National University provides research facilities for this study . 7 . REFERENCES [ 1 ] Jesse Alpert and Nissan Hajaj . http://googleblog . blogspotkr/2008/07/we knew web was bightml , 2008 .
[ 2 ] Shaikh Arifuzzaman , Maleq Khan , and Madhav V .
Marathe . PATRIC : a parallel algorithm for counting triangles in massive networks . In CIKM , 2013 .
[ 3 ] Luca Becchetti , Paolo Boldi , Carlos Castillo , and
Aristides Gionis . Efficient algorithms for large scale local triangle counting . TKDD , 2010 .
[ 4 ] Jonathan W Berry , Bruce Hendrickson , Randall A LaViolette , and Cynthia A Phillips . Tolerating the community detection resolution limit with edge weighting . Phys . Rev . E , 83(5):056119 , 2011 .
[ 5 ] Bin Hui Chou and Einoshin Suzuki . Discovering community oriented roles of nodes in a social network . In DaWaK , pages 52–64 , 2010 .
[ 6 ] Jonathan Cohen . Graph twiddling in a mapreduce world .
CiSE , 11(4):29–41 , 2009 .
100101102103TWTSDYWCW09CW12Running Time ( min)PTESC ( Hadoop)PTESC ( Spark)1124
