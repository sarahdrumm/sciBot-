Accelerated Stochastic Block Coordinate Descent with Optimal Sampling
Aston Zhang
Dept . of Computer Science
University of Illinois at Urbana Champaign
IL , USA 61801 lzhang74@illinois.edu
Dept . of Systems and Information Engineering
Quanquan Gu
University of Virginia qg5w@virginia.edu
VA , USA 22904
ABSTRACT We study the composite minimization problem where the objective function is the sum of two convex functions : one is the sum of a finite number of strongly convex and smooth functions , and the other is a general convex function that is non differentiable . Specifically , we consider the case where the non differentiable function is block separable and admits a simple proximal mapping for each block . This type of composite optimization is common in many data mining and machine learning problems , and can be solved by block coordinate descent algorithms . We propose an accelerated stochastic block coordinate descent ( ASBCD ) algorithm , which incorporates the incrementally averaged partial derivative into the stochastic partial derivative and exploits optimal sampling . We prove that ASBCD attains a linear rate of convergence . In contrast to uniform sampling , we reveal that the optimal non uniform sampling can be employed to achieve a lower iteration complexity . Experimental results on different large scale real data sets support our theory .
CCS Concepts •Information systems → Data mining ; •Computing methodologies → Machine learning ;
Keywords Stochastic block coordinate descent ; Sampling
1 .
INTRODUCTION
We consider the problem of minimizing a composite function , which is the sum of two convex functions :
P ( w ) = F ( w ) + R(w ) ,
( 1.1 )
∗ w where F ( w ) = n−1n
= argmin w∈Rd i=1 fi(w ) is a sum of a finite number of strongly convex and smooth functions , and R(w ) is a block separable non differential function . To explain block separability , let {G1 , . . . ,Gk} be a partition of all the d coordinates where Gj is a block of coordinates . A subvector wGj is [ wk1 , . . . , wk|Gj| ] , m where Gj = {k1 , . . . , k|Gj|} and 1 ≤ j ≤ m . The fact that R(w ) is block separable is equivalent to
R(w ) = rj(wGj ) .
( 1.2 ) j=1
The above problem is common in data mining and machine learning , such as the regularized empirical risk minimization , where F ( w ) is the empirical loss function averaged over the training data sets , and R(w ) is a regularization term . For example , suppose that for a data mining problem there are n instances in a training data set {(x1 , y1 ) , ( x2 , y2 ) , . . . , ( xn , yn)} . By choosing the squared loss fi(w ) = ( w , xi − yi)2/2 and R(w ) = 0 , a least square regression is obtained . If R(w ) is chosen to be the sum of the absolute value of each coordinate in w , it becomes a lasso regression [ 46 ] . In general , the problem in ( 1.1 ) can be approximately solved by proximal gradient descent algorithms [ 32 ] and proximal coordinate descent algorithms [ 23 ] .
Coordinate descent algorithms have received increasing attention in the past decade in data mining and machine learning due to their successful applications in high dimensional problems with structural regularizers [ 12 , 11 , 28 , 2 , 47 ] . Randomized block coordinate descent ( RBCD ) [ 31 , 36 , 26 , 39 , 4 , 14 , 21 ] is a special block coordinate descent algorithm . At each iteration , it updates a block of coordinates in vector w based on evaluation of a random feature subset from the entire training data instances . The iteration complexity of RBCD was established and extended to composite minimization problems [ 31 , 36 , 26 ] . RBCD can choose a constant step size and converge at the same rate as gradient descent algorithms [ 31 , 36 , 26 ] . Compared with gradient descent , the periteration time complexity of RBCD is much lower . This is because RBCD computes a partial derivative restricted to only a single coordinate block at each iteration and updates just a single coordinate block of vector w . However , it is still computationally expensive because at each iteration it requires evaluation of the gradient for all the n component functions fi : the per iteration computational complexity scales linearly with the training data set size n .
In view of this , stochastic block coordinate descent was proposed recently [ 8 , 51 , 48 , 35 ] . Such algorithms compute the stochastic partial derivative restricted to one coordinate block with respect to one component function , rather than the full partial derivative with respect to all the component functions . Essentially , these algorithms employ sampling of both features and data instances at each iteration . However , they can only achieve a sublinear rate of convergence .
We propose an algorithm for stochastic block coordinate descent using optimal sampling , namely accelerated stochastic block coordinate descent with optimal sampling ( ASBCD ) . On one hand , ASBCD employs a simple gradient update with optimal non
2035 Algorithm 1 ASBCD : Accelerated Stochastic Block Coordinate Descent with Optimal Sampling 1 : Inputs : step size η and sampling probability set P =
{p1 , . . . , pn} of component functions f1 , . . . , fn i = w(0 ) ∈ Rd
2 : Initialize : φ(0 ) 3 : for t = 1 , 2 , . . . do 4 :
5 : 6 :
Sample a component function index i from {1 , . . . , n} at probability pi ∈ P with replacement i ← w(t−1 ) φ(t ) Sample a coordinate block index j from {1 , . . . , m} uniformly at random with replacement i ) −
− η.(npi)−1∇Gj fi(φ(t ) )fi
,w(t−1 ) ) + n−1n
← proxη,j k=1 ∇Gj fk(φ(t−1 )
Gj k
7 : w(t)Gj
( npi)−1∇Gj fi(φ(t−1 ) 8 : w(t ) \Gj 9 : end for
← w(t−1 ) \Gj i uniform sampling , which is in sharp contrast to the aforementioned stochastic block coordinate descent algorithms based on uniform sampling . On the other hand , we incorporate the incrementally averaged partial derivative into the stochastic partial derivative to achieve a linear rate of convergence rather than a sublinear rate .
O n
1
To be specific , given error and number of coordinate blocks m , for strongly convex fi(w ) with the convexity parameter µ and the Lipschitz continuous gradient constant Li ( LM = max Li ) , the iteration complexity of ASBCD is
Notation . Here we define and describe the notation used through this paper . Let wk be the kth element of a vector w =
[ w1 , . . . , wd ] ∈ Rd . We use w = w2 =,d denote the 2 norm of a vector w and w1 = d k=1 w2 k=1 |wk| . The subvector of w excluding wGj is denoted by w\Gj . The simple proximal mapping for each coordinate block , also known as the proximal operator , is defined as
1/2 to
Li µ
+ n log
1 m n i=1 k
. i proxη,j(w ) = argmin u∈Rd
1 2η w − u2 + rj(u ) .
( 1.3 )
2 . THE PROPOSED ALGORITHM
We propose ASBCD ( Algorithm 1 ) , an accelerated algorithm for stochastic block coordinate descent with optimal sampling . It starts with known initial vectors φ(0 ) i = w(0 ) ∈ Rd for all i .
In sharp contrast to stochastic block coordinate descent with uniform sampling , ASBCD selects a component function according to non uniform probabilities ( Line 4 of Algorithm 1 ) . In Algorithm 1 , we define the gradient of any function f ( φ ) with respect to a coordinate block Gj of φ as ∇Gj f ( φ ) = = [ ∂f ( φ)/∂φk1 , . . . , ∂f ( φ)/∂φk|Gj| ] , where Gj = [ ∇f ( φ)]Gj {k1 , . . . , k|Gj|} . Algorithm 1 has a lower computational cost than either proximal gradient descent or RBCD at each iteration . The update at each iteration of Algorithm 1 is restricted to only a sampled component function ( Line 4 ) and a sampled block of coordinates ( Line 6 ) . third term n−1n
The key updating step ( Line 7 ) with respect to a stochastic block of coordinates incorporates the incrementally averaged partial derivative into the stochastic partial derivative with the ) within the square bracket . this summation
At each iteration with i and j sampled , k=1 ∇Gj fk(φ(t−1 ) k term n k=1 ∇Gj fk(φ(t−1 ) k
) from itself while adding ∇Gj fi(φ(t−1 )
) is efficiently updated by subtracting ) to itself . i
∇Gj fi(φ(t−2 ) i
REMARK 21 For many empirical risk minimization problems with each training data instance ( xi , yi ) and a loss function , the gradient of fi(w ) with respect to w is a multiple of xi : ∇fi(w ) = ( w , xi , yi)xi . Therefore , ∇fi(φi ) can be compactly saved in memory by only saving scalars ( φi , xi , yi ) with the same space cost as those of many other related algorithms MRBCD , SVRG , SAGA , SDCA , and SAG described in Section 5 .
REMARK 22 The sampling probability of component functions fi in Line 4 of Algorithm 1 is according to a given probability set P = {p1 , . . . , pn} . The uniform sampling scheme employed by stochastic block coordinate descent methods fits under this more generalized sampling framework as a special case , where pi = 1/n . We reveal that the optimal non uniform sampling can be employed to lower the iteration complexity in Section 3 .
When taking the expectation of the squared gap between the iterate w(t ) and the optimal solution w∗ in ( 1.1 ) with respect to the stochastic coordinate block index , the obtained upper bound does not depend on such an index or the proximal operator . This property may lead to additional algorithmic development and here it is important for deriving a linear rate of convergence for Algorithm 1 . We prove the rate of convergence bound in Appendix A after presenting and discussing the main theory in Section 3 .
3 . MAIN THEORY
In this section , we present and discuss the main theory of our proposed algorithm ( Algorithm 1 ) . The proof of the main theory is presented in the appendix .
We begin with the following assumptions on F ( w ) and R(w ) in the composite objective optimization problem as characterized in ( 11 ) These assumptions are mild and can be verified in many regularized empirical risk minimization problems in data mining and machine learning .
ASSUMPTION 3.1
( LIPSCHITZ CONTINUOUS GRADIENT ) . Each gradient ∇fi(w ) is Lipschitz continuous with the constant Li , ie , for all w ∈ Rd and u ∈ Rd we have
∇fi(w ) − ∇fi(u ) ≤ Li w − u .
ASSUMPTION 3.2
( STRONG CONVEXITY ) . Each function fi(w ) is strongly convex , ie , there exists a positive constant µ such that for all w ∈ Rd and u ∈ Rd we have fi(u ) − fi(w ) − ∇fi(w ) , u − w ≥ µ 2 u − w2 .
Assumption 3.2 implies that F ( w ) is also strongly convex , ie , there exists a positive constant µ such that for all w ∈ Rd and u ∈ Rd we have
F ( u ) − F ( w ) − ∇F ( w ) , u − w ≥ µ 2 u − w2 .
ASSUMPTION 3.3
( BLOCK SEPARABILITY ) . The regularization function R(w ) is convex but non differentiable , and a closedform solution can be obtained for the proximal operator defined in ( 13 ) Importantly , R(w ) is block separable as defined in ( 12 )
With the above assumptions being made , now we establish the linear rate of convergence for Algorithm 1 , which is stated in the following theorem .
2036 Li and pI = min
THEOREM 34 Let LM = max pi . Suppose that Assumptions 31—33 hold . Based on Algorithm 1 and with w∗ defined in ( 1.1 ) , by setting η = max npi/[2(nµ + Li) ] , ζ = npI /(LM η)− 1 > 0 , κ = L2 M m/[2nη(LM − µ + LM ηµζ ) ] > 0 , and 0 < α = 1 − ηµ/m < 1 , it holds that i i i
Ei,j[w(t ) − w
≤ αt.w(0 ) − w
∗2 ]
− ∇F ( w
∗
[ F ( w(0 ) ) − F ( w
∗
)
1 κ
∗2 + ) , w(0 ) − w
∗]fi .
REMARK 35 Theorem 3.4 justifies the linear rate of convergence for Algorithm 1 . Parameter α depends on the number of coordinate blocks m . It may be tempting to set m = 1 for faster convergence . However , this is improper due to lack of considerations for the computational cost at each iteration . When m = 1 , at each iteration the gradient is updated with respect to all coordinates . When m > 1 , at each iteration of Algorithm 1 the gradient is updated with respect to only a sampled coordinate block among all coordinates , so the computational cost is lower than that of m = 1 per iteration . Therefore , comparing algorithms that update the gradient with respect to different numbers of coordinates per iteration should be based on the same number of entire data passes ( the least possible iterations for passing through the entire data instances with respect to all coordinates ) . We perform experiments to compare such different algorithms in Section 4 .
REMARK 36 Theorem 3.4 implies a more generalized itera tion complexity of Algorithm 1 , which is
O m min i
Li/µ + n npi log
1 given the error > 0 . The uniform sampling scheme fits this more generalized result with pi = 1/n . With LM = max Li , by setting pi = 1/n , η = 1/ [ 2(LM + nµ ) ] > 0 , ζ = ( LM + 2nµ ) /LM > 0 , κ = m/ [ 2nη(1 − ηµ ) ] > 0 , and 0 < α = 1 − µ/[2m(LM + nµ ) ] < 1 , Theorem 3.4 still holds . The iteration complexity of ASBCD with uniform sampling is i
O m
LM
µ
1
+ n log
.
( 3.2 )
Now we show that the iteration complexity in ( 3.2 ) can be further improved by optimal sampling . To begin with , minimizing α can be achieved by maximizing η with respect to pi . It is easy to show that k=1(n + Lk/µ ) . Then ,
η is maximized when pi = ( n + Li/µ)/n i=1(nµ + Li)fi > 0 we obtain the iteration by setting η = n/.2n
1 n complexity of ASBCD with optimal sampling :
O m
Li µ n i=1
+ n log
.
1
( 3.3 ) i
−1 i=1 L
COROLLARY 37 Let LM = max i=1(nµ+Li ) ] < 1 , we chose η = n/[2n
Li . Suppose that Assumptions 31—33 hold . Based on Algorithm 1 and with w∗ defined k=1(n + Lk/µ ) , ζ = i=1(2nµ + 2Li)−1 − 1 > 0 , and 0 < α = i=1(nµ+ in ( 1.1 ) , by setting pi = ( n + Li/µ)/n n i /n 1−nµ/[2mn ≤ αtw(0 ) − w
Li ) ] > 0 and it holds that
[ F ( w(0 ) ) − F ( w
Ei,j[w(t ) − w
∗2 ]
∗
) n
∗2 + ) , w(0 ) − w
∗ ]
. m(LM + nµ )
− ∇F ( w
∗ net is used :
∗ w
( 3.1 )
= argmin w∈Rd
P ( w ) n
Comparing the iteration complexity of ASBCD in ( 3.3 ) and ( 3.2 ) , it is clear that the optimal sampling scheme results in a lower iteration complexity than uniform sampling .
4 . EVALUATION
We conduct experiments to evaluate the performance of our proposed ASBCD algorithm in comparison with different algorithms on large scale real data sets . 4.1 Problems and Measures
We define the problems and measures used in the empirical evaluation . Classification and regression are two corner stone data mining and machine learning problems . We evaluate the performance of the proposed ASBCD algorithm in solving these two problems . 411 Classification and Regression Problems As a case study , the classification problem is 1,2 regularized logistic regression :
∗ w
= argmin w∈Rd
= argmin w∈Rd λ2 2
+ log.1 + exp(−yiw , xi)fi
P ( w ) n 1 n w2 + λ1 w1 . i=1
For the the regression problem in this empirical study , the elastic
( w , xi − yi)2
= argmin w∈Rd
1 n i=1
2
+
λ2 2 w2 + λ1 w1 .
The regularization parameters λ1 and λ2 in both problems are tuned by proximal gradient descent using five fold cross validation on the training data sets . 412 Measures for Convergence and Testing Accur acy
Recall the problem of composite function minimization as formalized in ( 11 ) In evaluation of the algorithm performance on the convergence effect , we use the measure of objective gap value : P ( w ) − P ( w∗ ) .
To further study model prediction capabilities that are trained by different algorithms , we evaluate testing accuracy using two different measures : • AUC : For the classification problem , area under receiver operating characteristic curve ( AUC ) is measured [ 13 ] . Note that a higher testing accuracy can be reflected by a higher AUC .
• MSE : For the regression problem , mean squared error ( MSE ) is compared . Note that a higher testing accuracy can be reflected by a lower MSE .
4.2 Large Scale Real Data Sets
The empirical studies are conducted on the following four real data sets that are downloaded using the LIBSVM software [ 3 ] : • KDD 2010 : Bridge to Algebra data set from KDD Cup 2010
Educational Data Mining Challenge [ 44 ] .
• COVTYPE : Data set for predicting forest cover type from car tographic variables [ 22 ] .
2037 Table 1 : Summary statistics of four large scale real data sets in the experiments . These data sets are used for evaluating performance of algorithms in solving two corner stone data mining and machine learning problems : classification and regression .
Data Set KDD 2010 COVTYPE RCV1 E2006 TFIDF
#Training Instances
#Testing Instances
19,264,097
290,506 20,242 16,087
748,401 290,506 677,399 3,308
Problem
#Features 29,890,095 Classification Classification Classification Regression
47,236 150,360
54
Measure for Testing Accuracy ( ↑ )
AUC ( ↑ ) AUC ( ↑ ) AUC ( ↑ ) MSE ( ↓ )
• RCV1 : Reuters Corpus Volume I data set for text categorization research [ 20 ] .
• E2006 TFIDF : Data set for predicting risk from financial re ports from thousands of publicly traded US companies [ 16 ] . Each of these real data sets has a large size in either its instance count or feature size , or both . For instance , the KDD 2010 data set has over 19 million training instances with nearly 30 million features . Summary statistics of these data sets are provided in Table 1 . 4.3 Algorithms for Comparison
We evaluate the performance of ASBCD in comparison with recently proposed competitive algorithms . To comprehensively evaluate ASBCD , we also compare variants of ASBCD with different sampling schemes .
Below are the seven algorithms for comparison .
• SGD ( SG ) : Proximal stochastic gradient descent . This algorithm has a sublinear rate of convergence . To ensure the high competitiveness of this algorithm , the implementation is based on a recent work [ 1 ] .
• SBCD ( SB ) : Stochastic block coordinate descent . It is the same as SGD except that SBCD updates the gradient with respect to a randomly sampled block of coordinates at each iteration . SBCD also converges at a sublinear rate .
• SAGA ( SA ) : Advanced stochastic gradient method [ 9 ] . This algorithm is based on uniform sampling of component functions . It updates the gradient with respect to all coordinates at each iteration . SAGA has a linear rate of convergence .
• SVRG ( SV ) : ( Proximal ) stochastic variance reduced gradient [ 15 , 50 ] . This algorithm is based on uniform sampling of component functions . It updates the gradient with respect to all coordinates at each iteration . Likewise , SVRG converges to the optimum at a linear rate .
• MRBCD ( MR ) : Mini batch randomized block coordinate descent [ 54 ] . This algorithm uses uniform sampling of component functions . MRBCD converges linearly to the optimum .
• ASBCD U ( U ) : The proposed ASBCD algorithm with uniform sampling of component functions . The sampling probability pi for component function fi is pi = 1/n . The sampling probabil ity pi for component function fi : pi = Li/n ity pi for component function fi is pi = ( n+Li/µ)/n
• ASBCD O ( O ) : The proposed ASBCD algorithm with optimal sampling as described in Corollary 37 The sampling probabilk=1(n+ k=1 Lk .
Lk/µ ) .
4.4 Experimental Setting
Note that algorithms SBCD , MRBCD , and ASBCD update the gradient with respect to a sampled block of coordinates at each iteration . In contrast , SGD , SAGA , and SVRG update the gradient with respect to all the coordinates per iteration . Recalling Remark 3.5 , comparison of these algorithms is based on the same entire data passes .
441 Equipment Configuration We evaluate convergence and testing accuracy with respect to training time . The experiments on the KDD 2010 data set are conducted on a computer with two 14 core 2.4GHz CPUs and a 256GB RAM while the experiments on the other data sets are conducted on a computer with an 8 core 3.4GHz CPU and a 32GB RAM . 442 Parameter Setting Different from the other algorithms in comparison , the SVRG and MRBCD algorithms both have multiple stages with two nested loops . The inner loop counts in SVRG and MRBCD are set to the training data instance counts as suggested in a few recent studies [ 15 , 50 , 54 ] .
For each algorithm , its parameters , such as the step size ( η in this paper ) , are chosen around the theoretical values to give the fastest convergence under the five fold cross validation . Here we describe the details . The training data set is divided into five subsets of approximately the same size . One validation takes five trials on different subsets : in each trial , one subset is left out and the remaining four subsets are used . The convergence effect in one cross validation is estimated by the averaged performance of the five trials . 4.5 Experimental Results
All the experimental results are obtained from 10 replications . Both the mean and standard deviation values are reported in Tables 2—6 . For clarity of exposition , Figures 1—4 plot the mean values of the results from all these replications . 451 Results on KDD 2010 For classification on KDD 2010 , Figure 1 compares convergence of all the algorithms for the same number of entire data passes . In general , among all the seven algorithms in comparison , ASBCD with optimal sampling converges fastest to the optimum for the same number of entire data passes . Sublinearly convergent algorithms SGD and SBCD converge much more slowly than the other linearly convergent algorithms .
We also observe from Figure 1 that , stochastic block coordinate descent algorithms generally converge faster than those algorithms without using stochastic block coordinates . For instance , SBCD converges faster than SGD while MRBCD converges faster than SVRG for the same number of entire data passes .
The varied convergence effects across all the seven algorithms can be visualized more clearly when they are compared for the same training time . Figure 2 exhibits such performance variations . Clearly , ASBCD with optimal sampling achieves the fastest convergence for the same training time . Similar to the results in Figure 1 , for the same training time , stochastic block coordinate descent algorithms still generally converge faster than those algorithms without using stochastic block coordinates .
It is not surprising that the convergence effects influence the testing accuracy . Tables 2 and 3 report the AUC comparison of algorithms for the same entire data passes and training time . Consist
2038 ASBCD with optimal sampling generally achieves the highest testing accuracy for the same training time .
5 . RELATED WORK
The first line of research in modern optimization is randomized block coordinate descent ( RBCD ) algorithms [ 11 , 49 , 25 , 39 , 36 ] . These algorithms exploit the block separability of regularization function R(w ) . With separable coordinate blocks , such algorithms only compute the gradient of F ( w ) with respect to a randomly selected block at each iteration rather than the full gradient with respect to all coordinates : they are faster than the full gradient descent at each iteration [ 11 , 49 , 25 , 39 , 36 ] . However , such algorithms still compute the exact partial gradient based on all the n component functions per iteration , though accessing the entire component functions is computationally more expensive when the training data set has a larger number of instances [ 52 ] .
Recently , an MRBCD algorithm was proposed for randomized block coordinate descent using mini batches [ 54 ] . At each iteration , both a block of coordinates and a mini batch of component functions are sampled but there are multiple stages with two nested loops . For each iteration of the outer loop , the exact gradient is computed once ; while in the follow up inner loop , gradient estimation is computed multiple times to help adjust the exact gradient . MRBCD has a linear rate of convergence for strongly convex and smooth F ( w ) only when the batch size is “ large enough ” although batches of larger sizes increase the per iteration computational cost [ 54 ] ( Theorem 42 ) Similar algorithms and theoretical results to those of MRBCD were also proposed [ 48 , 18 ] . Chen and Gu further considered related but different sparsity constrained non convex problems and studied stochastic optimization algorithms with block coordinate gradient descent [ 6 ] .
Our work departs from the related work in the above line of research by attaining a linear convergence using optimally and nonuniformly sampling of a single data instance at each of iterations . The second line of research in modern optimization is proximal gradient descent . In each iteration , a proximal operator is used in the update , which can be viewed as a special case of splitting algorithms [ 24 , 5 , 35 ] . Proximal gradient descent is computationally expensive at each iteration , hence proximal stochastic gradient descent is often used when the data set is large . At each iteration , only one of the n component functions fi is sampled , or a subset of fi are sampled , which is also known as mini batch proximal stochastic gradient [ 43 ] . Advantages for proximal stochastic gradient descent are obvious : at each iteration much less computation of the gradient is needed in comparison with proximal gradient descent . However , due to the variance in estimating the gradient by stochastic sampling , proximal stochastic gradient descent has a sublinear rate of convergence even when P ( w ) is strongly convex and smooth .
To accelerate proximal stochastic gradient descent , variance reduction methods were proposed recently . Such accelerated algorithms include stochastic average gradient ( SAG ) [ 38 ] , stochastic dual coordinate ascent ( SDCA ) [ 42 ] , stochastic variance reduced gradient ( SVRG ) [ 15 ] , semi stochastic gradient descent ( S2GD ) [ 19 ] , permutable incremental gradient ( Finito ) [ 10 ] , minimization by incremental surrogate optimization ( MISO ) [ 27 ] , and advanced stochastic gradient method ( SAGA ) [ 9 ] . There are also some more recent extensions in this line of research , such as proximal SDCA ( ProxSDCA ) [ 40 ] , accelerated mini batch SDCA ( ASDCA ) [ 41 ] , adaptive variant of SDCA ( AdaSDCA ) [ 7 ] , randomized dual coordinate ascent ( Quartz ) [ 34 ] , mini batch S2GD ( mS2GD ) [ 17 ] , and proximal SVRG ( ProxSVRG ) [ 50 ] .
Besides , several studies show that non uniform sampling can be used to improve the rate of convergence of stochastic optimization
Figure 1 : Classification on KDD 2010 : Convergence comparison of algorithms for the same number of entire data passes . In general , ASBCD with optimal sampling ( O ) converges fastest to the optimum for the same number of entire data passes .
Figure 2 : Classification on KDD 2010 : Convergence comparison of algorithms for the same training time . In general , ASBCD with optimal sampling ( O ) converges fastest to the optimum for the same training time . ent with the observed convergence performance in Figures 1 and 2 , ASBCD with optimal sampling generally achieves the highest testing accuracy for both the same number of entire data passes and the same training time . 452 Results on More Data Sets We further compare the algorithms on three more data sets COVTYPE , RCV1 , and E2006 TFIDF as described in Section 4.2 and summarized in Table 1 . COVTYPE and RCV1 are used for the classification problem , while E2006 TFIDF is for the regression problem . All the results are reported in Tables 4—6 , Figure 3 , and Figure 4 . To begin with , we describe the convergence effects . Figures 3 and 4 compare convergence of algorithms for the same entire data passes and for the same training time . In general , ASBCD with optimal sampling ( O ) converges fastest to the optimum for both the same number of entire data passes and the same training time .
Tables 4—6 present the testing accuracy comparison results for the same training time . Note that a lower MSE for regression on E2006 TFIDF indicates a higher testing accuracy . These testing accuracy results agree with the varied convergence effects of different algorithms on the same data set . Among all the algorithms ,
0 2 4 6 8 10Number of Entire Data Passes10 410 310 210 1100101Objective Gap ValueSGSBSASVMRUOTraining Time ( in Seconds)0 600 1200180024003000Objective Gap Value10 1100101SGSBSASVMRUO2039 Table 2 : Classification on KDD 2010 : AUC comparison of algorithms for the same entire data passes . The boldfaced results with symbol denote the highest AUC among all the algorithms for the same number of entire data passes .
#Data Passes = 2 Method ±Std . Mean ±0.0214 SGD 0.8341 ±0.0278 SBCD 0.8352 ±0.0103 SAGA 0.8360 ±0.0111 SVRG 0.8349 ±0.0301 MRBCD 0.8352 ±0.0153 ASBCD U 0.8367 ±0.0112 ASBCD O 0.8374 *Std . : Standard Deviation
#Data Passes = 4 ±Std . Mean ±0.0188 0.8343 ±0.0264 0.8353 ±0.0105 0.8401 ±0.0146 0.8393 ±0.0232 0.8395 ±0.0127 0.8407 ±0.0133 0.8429
#Data Passes = 6 ±Std . Mean ±0.0153 0.8348 ±0.0315 0.8354 ±0.0305 0.8481 ±0.0178 0.8438 ±0.0270 0.8449 ±0.0212 0.8494 ±0.0109 0.8525
#Data Passes = 8 ±Std . Mean ±0.0123 0.8350 ±0.0288 0.8355 ±0.0154 0.8528 ±0.0097 0.8514 ±0.0162 0.8517 ±0.0165 0.8530 ±0.0098 0.8542
#Data Passes = 10 ±Std . Mean ±0.0179 0.8351 ±0.0352 0.8357 ±0.0097 0.8542 ±0.0087 0.8531 ±0.0183 0.8535 ±0.0168 0.8551 ±0.0076 0.8562
Table 3 : Classification on KDD 2010 : AUC comparison of algorithms for the same training time . The boldfaced results with symbol denote the highest AUC among all the algorithms for the same training time . Time = 1800s ±Std . ±0.0146 ±0.0245 ±0.0153 ±0.0166 ±0.0146 ±0.0153 ±0.0103
Time = 2400s ±Std . ±0.0154 ±0.0256 ±0.0151 ±0.0083 ±0.0170 ±0.0134 ±0.0087
Time = 1200s ±Std . ±0.0153 ±0.0233 ±0.0241 ±0.0198 ±0.0296 ±0.0150 ±0.0097
Time = 3000s ±Std . ±0.0112 ±0.0356 ±0.0099 ±0.0087 ±0.0153 ±0.0113 ±0.0081
Mean 0.8344 0.8353 0.8379 0.8368 0.8390 0.8415 0.8432
Mean 0.8342 0.8350 0.8351 0.8340 0.8356 0.8367 0.8371
Mean 0.8344 0.8352 0.8365 0.8353 0.8370 0.8385 0.8396
Time = 600s
Method Mean SGD 0.8339 SBCD 0.8348 SAGA 0.8306 SVRG 0.8293 MRBCD 0.8309 ASBCD U 0.8311 ASBCD O 0.8314 *Std . : Standard Deviation
±Std . ±0.0163 ±0.0265 ±0.0223 ±0.0287 ±0.0280 ±0.0148 ±0.0122
Mean 0.8341 0.8350 0.8337 0.8320 0.8339 0.8346 0.8351 algorithms [ 45 , 31 , 29 , 50 , 34 , 53 , 37 , 33 ] . However , the proposed sampling schemes in these studies cannot be directly applied to our algorithm , because they are limited in at least one of the following two aspects : ( 1 ) the algorithm does not apply to composite objectives with a non differentiable function ; ( 2 ) it does not support randomized block coordinate descent .
6 . CONCLUSION
Research on big data is increasingly important and common . Training data mining and machine learning models often involve minimizing empirical risk or maximizing likelihood over the training data set , especially in solving classification and regression problems . Thus , big data research may rely on optimization algorithms , such as proximal gradient descent algorithms . At each iteration , proximal gradient descent algorithms have a much higher computational cost due to updating gradients based on all the data instances and features . Randomized block coordinate descent algorithms are still computationally expensive at each iteration when the data instance size is large . Therefore , we focused on stochastic block coordinate descent that samples both data instances and features at every iteration .
We proposed the ASBCD algorithm to accelerate stochastic block coordinate descent . ASBCD incorporates the incrementally averaged partial derivative into the stochastic partial derivative . For smooth and strongly convex functions with non differentiable regularization functions , ASBCD is able to achieve a linear rate of convergence . The optimal sampling achieves a lower iteration complexity for ASBCD . The empirical evaluation with both classification and regression problems on four large scale real data sets supported our theory . Acknowledgement . We would like to thank the anonymous reviewers for their helpful comments . This research was partially supported by Quanquan Gu ’s startup funding at Department of Systems and Information Engineering , University of Virginia .
7 . REFERENCES [ 1 ] L . Bottou . Stochastic gradient descent tricks . In Neural Networks : Tricks of the Trade , pages 421–436 . Springer , 2012 .
[ 2 ] P . Breheny and J . Huang . Coordinate descent algorithms for nonconvex penalized regression , with applications to biological feature selection . The annals of applied statistics , 5(1):232 , 2011 .
[ 3 ] C C Chang and C J Lin . Libsvm : a library for support vector machines . ACM Transactions on Intelligent Systems and Technology ( TIST ) , 2(3):27 , 2011 .
[ 4 ] K W Chang , C J Hsieh , and C J Lin . Coordinate descent method for large scale l2 loss linear support vector machines . The Journal of Machine Learning Research , 9:1369–1398 , 2008 .
[ 5 ] G . H . Chen and R . Rockafellar . Convergence rates in forward–backward splitting . SIAM Journal on Optimization , 7(2):421–444 , 1997 .
[ 6 ] J . Chen and Q . Gu . Accelerated stochastic block coordinate gradient descent for sparsity constrained nonconvex optimization . In Conference on Uncertainty in Artificial Intelligence , 2016 .
[ 7 ] D . Csiba , Z . Qu , and P . Richtárik . Stochastic dual coordinate ascent with adaptive probabilities . arXiv:1502.08053 , 2015 .
[ 8 ] C . D . Dang and G . Lan . Stochastic block mirror descent methods for nonsmooth and stochastic optimization . SIAM Journal on Optimization , 25(2):856–881 , 2015 .
[ 9 ] A . Defazio , F . Bach , and S . Lacoste Julien . Saga : A fast incremental gradient method with support for non strongly convex composite objectives . In Advances in Neural Information Processing Systems , pages 1646–1654 , 2014 .
[ 10 ] A . J . Defazio , T . S . Caetano , and J . Domke . Finito : A faster , permutable incremental gradient method for big data problems . In Proceedings of the International Conference on
2040 Table 4 : Classification on COVTYPE : AUC comparison of algorithms for the same training time . The boldfaced results with symbol denote the highest AUC among all the algorithms for the same training time . Time = 300s
Time = 100s
Time = 200s
Time = 400s
Time = 500s
Table 5 : Classification on RCV1 : AUC comparison of algorithms for the same training time . The boldfaced results with symbol denote the highest AUC among all the algorithms for the same training time .
Time = 100s
Time = 200s
Time = 300s
Time = 400s
Time = 500s
Method Mean SGD 0.6998 SBCD 0.7021 SAGA 0.7259 SVRG 0.7234 MRBCD 0.7241 ASBCD U 0.7302 ASBCD O 0.7296 *Std . : Standard Deviation
±Std . ±0.0211 ±0.0528 ±0.0165 ±0.0176 ±0.0313 ±0.0243 ±0.0258
Method Mean SGD 0.9192 SBCD 0.9193 SAGA 0.9215 SVRG 0.9201 MRBCD 0.9206 ASBCD U 0.9216 ASBCD O 0.9218 *Std . : Standard Deviation
±Std . ±0.0143 ±0.0198 ±0.0204 ±0.0251 ±0.0203 ±0.0166 ±0.0199
Mean 0.7004 0.7024 0.7321 0.7265 0.7268 0.7329 0.7325
Mean 0.9194 0.9194 0.9231 0.9210 0.9223 0.9228 0.9232
±Std . ±0.0309 ±0.0352 ±0.0323 ±0.0233 ±0.0254 ±0.0249 ±0.0223
Mean 0.7007 0.7026 0.7318 0.7288 0.7295 0.7372 0.7390
±Std . ±0.0160 ±0.0312 ±0.0298 ±0.0236 ±0.0216 ±0.0156 ±0.0149
Mean 0.7010 0.7027 0.7332 0.7298 0.7306 0.7393 0.7438
±Std . ±0.0187 ±0.0319 ±0.0075 ±0.0182 ±0.0249 ±0.0175 ±0.0104
Mean 0.7010 0.7028 0.7351 0.7305 0.7313 0.7424 0.7483
±Std . ±0.0255 ±0.0334 ±0.0135 ±0.0208 ±0.0185 ±0.0123 ±0.0112
±Std . ±0.0213 ±0.0214 ±0.0280 ±0.0250 ±0.0144 ±0.0072 ±0.0063
Mean 0.9194 0.9196 0.9240 0.9221 0.9226 0.9242 0.9247
±Std . ±0.0202 ±0.0224 ±0.0198 ±0.0132 ±0.0139 ±0.0069 ±0.0048
Mean 0.9195 0.9197 0.9257 0.9224 0.9233 0.9259 0.9268
±Std . ±0.0143 ±0.0188 ±0.0143 ±0.0049 ±0.0199 ±0.0045 ±0.0031
Mean 0.9196 0.9199 0.9278 0.9228 0.9242 0.9282 0.9291
±Std . ±0.0138 ±0.0167 ±0.0321 ±0.0099 ±0.0168 ±0.0057 ±0.0040
Table 6 : Regression on E2006 TFIDF : MSE comparison of algorithms for the same training time . The boldfaced results with symbol denote the lowest MSE among all the algorithms for the same training time .
Method Mean SBCD 0.2852 SGD 0.2850 SAGA 0.2386 SVRG 0.2723 MRBCD 0.2402 ASBCD U 0.2183 ASBCD O 0.2158 *Std . : Standard Deviation
±Std . ±0.0219 ±0.0142 ±0.0136 ±0.0123 ±0.0144 ±0.0043 ±0.0078
Time = 300s
Time = 600s
Time = 900s
Mean 0.2806 0.2829 0.1892 0.2324 0.1854 0.1653 0.1647
±Std . ±0.0118 ±0.0136 ±0.0139 ±0.0134 ±0.0098 ±0.0050 ±0.0064
Mean 0.2794 0.2824 0.1629 0.2008 0.1635 0.1628 0.1621
±Std . ±0.0132 ±0.0136 ±0.0109 ±0.0043 ±0.0130 ±0.0087 ±0.0047
Time = 1200s ±Std . ±0.0143 ±0.0187 ±0.0129 ±0.0085 ±0.0092 ±0.0054 ±0.0057
Mean 0.2790 0.2815 0.1611 0.1643 0.1607 0.1605 0.1597
Time = 1500s ±Std . ±0.0164 ±0.0238 ±0.0109 ±0.0048 ±0.0096 ±0.0042 ±0.0054
Mean 0.2773 0.2808 0.1598 0.1620 0.1594 0.1589 0.1583 problems . In Proceedings of the International Conference on Machine Learning , 2014 .
[ 11 ] J . Friedman , T . Hastie , H . Höfling , R . Tibshirani , et al .
Pathwise coordinate optimization . The Annals of Applied Statistics , 1(2):302–332 , 2007 .
[ 12 ] W . J . Fu . Penalized regressions : the bridge versus the lasso .
Journal of computational and graphical statistics , 7(3):397–416 , 1998 .
[ 13 ] J . A . Hanley and B . J . McNeil . The meaning and use of the area under a receiver operating characteristic ( roc ) curve . Radiology , 143(1):29–36 , 1982 .
[ 14 ] C J Hsieh , K W Chang , C J Lin , S . S . Keerthi , and S . Sundararajan . A dual coordinate descent method for large scale linear svm . In Proceedings of the 25th international conference on Machine learning , pages 408–415 . ACM , 2008 .
[ 15 ] R . Johnson and T . Zhang . Accelerating stochastic gradient descent using predictive variance reduction . In Advances in Neural Information Processing Systems , pages 315–323 , 2013 .
[ 16 ] S . Kogan , D . Levin , B . R . Routledge , J . S . Sagi , and N . A .
Smith . Predicting risk from financial reports with regression .
In Proceedings of Human Language Technologies : The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics , pages 272–280 . Association for Computational Linguistics , 2009 .
[ 17 ] J . Koneˇcn`y , J . Liu , P . Richtárik , and M . Takáˇc . ms2gd :
Mini batch semi stochastic gradient descent in the proximal setting . arXiv:1410.4744 , 2014 .
[ 18 ] J . Koneˇcn`y , Z . Qu , and P . Richtárik . Semi stochastic coordinate descent . arXiv preprint arXiv:1412.6293 , 2014 .
[ 19 ] J . Koneˇcn`y and P . Richtárik . Semi stochastic gradient descent methods . arXiv:1312.1666 , 2013 .
[ 20 ] D . D . Lewis , Y . Yang , T . G . Rose , and F . Li . Rcv1 : A new benchmark collection for text categorization research . The Journal of Machine Learning Research , 5:361–397 , 2004 .
[ 21 ] Y . Li and S . Osher . Coordinate descent optimization for â ˇD¸S
1 minimization with application to compressed sensing ; a greedy algorithm . Inverse Problems and Imaging , 3(3):487–503 , 2009 .
[ 22 ] M . Lichman . UCI machine learning repository . http://archiveicsuciedu/ml , 2013 .
[ 23 ] Q . Lin , Z . Lu , and L . Xiao . An accelerated proximal
2041 ( a ) Classification on COVTYPE
( b ) Classification on RCV1
( c ) Regression on E2006 TFIDF
Figure 3 : Convergence comparison of algorithms for the same number of entire data passes for classification and regression on three data sets . In general , ASBCD with optimal sampling ( O ) converges fastest to the optimum for the same number of entire data passes .
( a ) Classification on COVTYPE
( b ) Classification on RCV1
( c ) Regression on E2006 TFIDF
Figure 4 : Convergence comparison of algorithms for the same training time for classification and regression on three data sets . In general , ASBCD with optimal sampling ( O ) converges fastest to the optimum for the same training time . coordinate gradient method and its application to regularized empirical risk minimization . arXiv preprint arXiv:1407.1296 , 2014 .
[ 24 ] P L Lions and B . Mercier . Splitting algorithms for the sum of two nonlinear operators . SIAM Journal on Numerical Analysis , 16(6):964–979 , 1979 .
[ 25 ] H . Liu , M . Palatucci , and J . Zhang . Blockwise coordinate descent procedures for the multi task lasso , with applications to neural semantic basis discovery . In Proceedings of the 26th Annual International Conference on Machine Learning , pages 649–656 . ACM , 2009 .
[ 26 ] Z . Lu and L . Xiao . On the complexity analysis of randomized block coordinate descent methods . Mathematical Programming , 152(1 2):615–642 , 2015 .
[ 27 ] J . Mairal . Incremental majorization minimization optimization with application to large scale machine learning . arXiv:1402.4419 , 2014 .
[ 28 ] R . Mazumder , J . H . Friedman , and T . Hastie . Sparsenet :
Coordinate descent with nonconvex penalties . Journal of the American Statistical Association , 2012 .
[ 29 ] D . Needell , R . Ward , and N . Srebro . Stochastic gradient descent , weighted sampling , and the randomized kaczmarz algorithm . In Advances in Neural Information Processing Systems , pages 1017–1025 , 2014 .
[ 30 ] Y . Nesterov . Introductory lectures on convex optimization : A
Basic Course . Springer Science & Business Media , 2004 . [ 31 ] Y . Nesterov . Efficiency of coordinate descent methods on huge scale optimization problems . SIAM Journal on Optimization , 22(2):341–362 , 2012 .
[ 32 ] Y . Nesterov et al . Gradient methods for minimizing composite objective function . Technical report , Center for Operations Research and Econometrics , 2007 .
[ 33 ] Z . Qu and P . Richtárik . Coordinate descent with arbitrary sampling i : Algorithms and complexity . arXiv preprint arXiv:1412.8060 , 2014 .
[ 34 ] Z . Qu , P . Richtárik , and T . Zhang . Randomized dual coordinate ascent with arbitrary sampling . arXiv preprint arXiv:1411.5873 , 2014 .
[ 35 ] S . Reddi , A . Hefny , C . Downey , A . Dubey , and S . Sra .
Large scale randomized coordinate descent methods with non separable linear constraints . arXiv preprint arXiv:1409.2617 , 2014 .
[ 36 ] P . Richtárik and M . Takáˇc . Iteration complexity of
0 2 4 6 8 10Number of Entire Data Passes10 410 310 210 1Objective Gap ValueSGSBSASVMRUO0 2 4 6 8 10Number of Entire Data Passes10 510 410 310 210 1Objective Gap ValueSGSBSASVMRUO0 2 4 6 8 10Number of Entire Data Passes10 310 210 1100101102Objective Gap ValueSGSBSASVMRUOTraining Time ( in Seconds)0 100200300400500Objective Gap Value10 310 210 1SGSBSASVMRUOTraining Time ( in Seconds)0 100200300400500Objective Gap Value10 310 210 1SGSBSASVMRUOTraining Time ( in Seconds)0 300 600 900 12001500Objective Gap Value10 1100101102SGSBSASVMRUO2042 randomized block coordinate descent methods for minimizing a composite function . Mathematical Programming , 144(1 2):1–38 , 2014 .
[ 37 ] M . Schmidt , R . Babanezhad , M . O . Ahmed , A . Defazio ,
A . Clifton , and A . Sarkar . Non uniform stochastic average gradient method for training conditional random fields . arXiv preprint arXiv:1504.04406 , 2015 .
[ 38 ] M . Schmidt , N . L . Roux , and F . Bach . Minimizing finite sums with the stochastic average gradient . arXiv:1309.2388 , 2013 .
[ 39 ] S . Shalev Shwartz and A . Tewari . Stochastic methods for l
1 regularized loss minimization . The Journal of Machine Learning Research , 12:1865–1892 , 2011 .
[ 40 ] S . Shalev Shwartz and T . Zhang . Proximal stochastic dual coordinate ascent . arXiv:1211.2717 , 2012 .
[ 41 ] S . Shalev Shwartz and T . Zhang . Accelerated mini batch stochastic dual coordinate ascent . In Advances in Neural Information Processing Systems , pages 378–385 , 2013 .
[ 42 ] S . Shalev Shwartz and T . Zhang . Stochastic dual coordinate ascent methods for regularized loss . The Journal of Machine Learning Research , 14(1):567–599 , 2013 .
[ 43 ] S . Sra , S . Nowozin , and S . J . Wright . Optimization for machine learning . Mit Press , 2012 .
[ 44 ] J . Stamper , A . Niculescu Mizil , S . Ritter , G . Gordon , and
K . Koedinger . Bridge to algebra dataset from kdd cup 2010 educational data mining challenge . http://pslcdatashop . webcmuedu/KDDCup/downloadsjsp , 2010 .
[ 45 ] T . Strohmer and R . Vershynin . A randomized kaczmarz algorithm with exponential convergence . Journal of Fourier Analysis and Applications , 15(2):262–278 , 2009 .
[ 46 ] R . Tibshirani . Regression shrinkage and selection via the lasso . Journal of the Royal Statistical Society . Series B ( Methodological ) , pages 267–288 , 1996 .
[ 47 ] R . Tibshirani , J . Bien , J . Friedman , T . Hastie , N . Simon , J . Taylor , and R . J . Tibshirani . Strong rules for discarding predictors in lasso type problems . Journal of the Royal Statistical Society : Series B ( Statistical Methodology ) , 74(2):245–266 , 2012 .
[ 48 ] H . Wang and A . Banerjee . Randomized block coordinate descent for online and stochastic optimization . arXiv preprint arXiv:1407.0107 , 2014 .
[ 49 ] T . T . Wu and K . Lange . Coordinate descent algorithms for lasso penalized regression . The Annals of Applied Statistics , pages 224–244 , 2008 .
[ 50 ] L . Xiao and T . Zhang . A proximal stochastic gradient method with progressive variance reduction . SIAM Journal on Optimization , 24(4):2057–2075 , 2014 .
[ 51 ] Y . Xu and W . Yin . Block stochastic gradient iteration for convex and nonconvex optimization . SIAM Journal on Optimization , 25(3):1686–1716 , 2015 .
[ 52 ] A . Zhang , A . Goyal , R . Baeza Yates , Y . Chang , J . Han , C . A .
Gunter , and H . Deng . Towards mobile query auto completion : An efficient mobile application aware approach . In Proceedings of the 25th International Conference on World Wide Web , pages 579–590 , 2016 .
[ 53 ] P . Zhao and T . Zhang . Stochastic optimization with importance sampling . arXiv preprint arXiv:1401.2753 , 2014 . [ 54 ] T . Zhao , M . Yu , Y . Wang , R . Arora , and H . Liu . Accelerated mini batch randomized block coordinate descent method . In Advances in Neural Information Processing Systems , pages 3329–3337 , 2014 .
APPENDIX A . PROOF OF THE MAIN THEORY We provide the proof for the main theory delivered in Section 3 . Note that all the expectations are taken conditional on w(t−1 ) and each φ(t−1 ) unless otherwise stated . For brevity , we define i gi =
1 npi
∇fi(φ(t ) i ) − 1 npi
∇fi(φ(t−1 ) i
) +
1 n n
∇fk(φ(t−1 ) k
) . k=1
( A.1 )
Let us introduce several important lemmas . To begin with , since Algorithm 1 leverages randomized coordinate blocks , the following lemma is needed for taking the expectation of the squared gap between the iterate w(t ) and the optimal solution w∗ in ( 1.1 ) with respect to the coordinate block index j .
LEMMA A1 Suppose that Assumption 3.3 holds . Let j be a coordinate block index . With gi defined in ( A.1 ) and w∗ defined in ( 1.1 ) , based on Algorithm 1 we have
Ej [ w(t ) − w∗2 ] ≤ 1 m
.(m − 1)w(t−1 ) − w∗2 +w(t−1 ) − ηgi − w∗ + η∇F ( w∗)2fi .
Lemma A.1 takes the expectation of the squared gap between the iterate w(t ) and the optimal solution w∗ in ( 1.1 ) with respect to the randomized coordinate block index . The obtained upper bound does not have a randomized coordinate block index or the proximal operator . Block separability and non expansiveness of the proximal operator are both exploited in deriving the upper bound . This upper bound is used for deriving a linear rate of convergence for Algorithm 1 .
LEMMA A2 Based on Algorithm 1 and as defined in ( A.1 ) , we have Ei [ gi ] = ∇F ( w(t−1) ) .
Lemma A.2 guarantees that gi is an unbiased gradient estimator of F ( w ) . The proof is strictly based on the definition of gi in ( A1 ) LEMMA A3 With gi defined in ( A.1 ) and w∗ defined in ( 1.1 ) , based on Algorithm 1 and for all ζ > 0 we have
. gi − ∇F ( w∗)2fi ≤ ( 1 + ζ)Ei
Ei
∇fi(w(t−1 ) ) flflfl 1 npi flflfl2 flflfl 1
− 1 npi
∇fi(w∗ )
+ ( 1 + ζ−1)Ei
− ζ∇F ( w(t−1 ) ) − ∇F ( w∗)2
∇fi(φ(t−1 ) i
) − 1 npi npi
∇fi(w∗ ) flflfl2
.
Lemma A.3 makes use of the property that E[x2 ] = E[x − E[x]2 ] + E [ x]2 for all x and the property that x + y2 ≤ ( 1 + ζ)x2 + ( 1 + ζ−1)y2 for all x , y , and ζ > 0 .
LEMMA A4 Let f be strongly convex with the convexity parameter µ and its gradient be Lipschitz continuous with the constant L . For all x and y , it holds that
∇f ( y ) , x − y ≤ f ( x ) − f ( y ) −
1
∇f ( x ) − ∇f ( y)2
2(L − µ ) ∇f ( x ) − ∇f ( y ) , y − x −
− µ
L − µ
Lµ
2(L − µ ) y − x2 .
Lemma A.4 leverages properties of strongly convex functions with Lipschitz continuous gradient .
2043 LEMMA A5 Algorithm 1 implies that
Li npi fi(φ(t ) i ) fi(w(t−1 ) ) +
Li n n i=1
1 n
( 1 − pi)Li npi fi(φ(t−1 ) i
) .
1 n
1 n n n i=1 i=1
Ei
=
Lemma A.5 is obtained according to the non uniform sampling of component functions in Algorithm 1 .
REMARK A6 Similar to Lemma A.5 , we have
1 n
1 n
Ei
= i=1 npi
∇fi(w∗ ) , φ(t ) i − w∗ Li n ∇fi(w∗ ) , w(t−1 ) − w∗ Li n ( 1 − pi)Li n i=1 n
+
1 n i=1 npi
∇fi(w∗ ) , φ(t−1 ) i
− w∗
.
( A.2 )
Now we develop the main theorem of bounding the rate of con vergence for Algorithm 1 .
PROOF OF THEOREM 34 By applying Lemma A.1 , A.2 , and mw(t−1 ) − w∗2 + 2η∇F ( w∗ ) , w(t−1 ) − w∗
Lemma A.3 ,
.w(t ) − w∗2fi
Ei,j ≤ 1 m − 2η∇F ( w(t−1) ) , w(t−1 ) − w∗ + η2(1 + ζ)Ei flflfl 1 flflfl 1 npi
+ η2(1 + ζ−1)Ei
∇fi(w(t−1 ) ) − 1 npi ∇fi(φ(t−1 )
) − 1 npi i npi
∇fi(w∗ )
∇fi(w∗ ) flflfl2 flflfl2
− η2ζ∇F ( w(t−1 ) ) − ∇F ( w∗)2
.
( A.3 )
Substituting x , y , and f with w∗ , w(t−1 ) , and fi in Lemma A.4 , and taking average on both sides of the inequality in Lemma A.4 , we obtain − 2η∇F ( w(t−1) ) , w(t−1 ) − w∗ ≤ 2η n
[ fi(w∗ ) − fi(w(t−1) ) ]
Li − µ
∇fi(w∗ ) , w(t−1 ) − w∗ − ηµw∗ − w(t−1)2 .
1 Li
( A.4 )
Ei flflfl 1 n npi i=1
≤ 2 n
Recall the property of any function f that is convex and has a Lipschitz continuous gradient with the constant L : f ( y ) ≥ f ( x ) + ∇f ( x ) , y− x +∇f ( x ) − ∇f ( y)2 /(2L ) for all x and y [ 30 ] ( Theorem 215 ) Taking average on both sides , we have
∇fi(φ(t−1 ) i
.fi(φ(t−1 )
∇fi(w∗ )
) − 1 npi ) − fi(w∗ ) − ∇fi(w∗ ) , φ(t−1 ) i i
− w∗fi
Li npi after substituting y , x , and f with φ(t−1 ) arranging terms . i
( A.5 ) , w∗ , and fi while re flflfl2
Before further proceeding with the proof , we define
1 n
.fi(φ(t ) n i − w∗fi + κw(t ) − w∗2 .
Li npi i=1 φ(t ) i ) − fi(w∗ ) − ∇fi(w∗ ) ,
H ( t ) =
( A.6 )
Following the definition in ( A.6 ) , for all α > 0 ,
1 n
Ei,j [ H ( t ) ] − αH ( t−1 ) = Ei,j
Li npi i − w∗ ∇fi(w∗ ) , φ(t ) fi(φ(t ) i ) i=1 n
1 n n .κw(t ) − w∗2fi − αH ( t−1 ) . fi(w∗ ) − Ei,j
Li npi i=1 i=1 n
− 1 n
+ Ei,j
·
Li npi ckTk ,
( A.7 )
Recall the property of any strongly convex function f with the convexity parameter µ that f ( y ) ≤ f ( x ) + ∇f ( x ) , y − x + ∇f ( x ) − ∇f ( y)2/(2µ ) for all x and y [ 30 ] ( Theorem 2110 ) We can obtain −∇fi(w(t−1 ) ) − ∇fi(w∗)2 ≤
−2µ.fi(w(t−1 ) ) − fi(w∗ ) − ∇fi(w∗ ) , w(t−1 ) − w∗fi .
Combining ( A.3 ) with a positive constant κ , ( A.4 ) , and ( A.5 ) , after simplifying terms , by Lemma A.5 and ( A.2 ) , with defining LM = max
Li and pI = min pi we have i where the four constant factors are − 1 LM
κη mn c1 = npI k=1 i
Ei,j [ H ( t ) ] − αH ( t−1 ) ≤ 4 η(1 + ζ )
LM − 2κη(LM − µ )
2κη2(1 + ζ−1 ) − α
1 − ηµ m
LM m
1 n n
,
, mpI c2 = c3 = κ c4 =
LM n2
,
− 2βκη2µ m
− 1
,
1 − α pI
+ and the four corresponding terms are
T1 =
T2 = i=1 n n n i=1
∇fi(w(t−1 ) ) − ∇fi(w∗)2 ,
.fi(w(t−1 ) ) − fi(w∗ ) − ∇fi(w∗ ) , w(t−1 ) − w∗fi , .fi(φ(t−1 ) − w∗fi .
) − fi(w∗ ) − ∇fi(w∗ ) , φ(t−1 ) i i
T3 = w(t−1 ) − w∗2 , i=1 i
M m/.2nη(LM − µ + LM ηµζ)fi > 0 , and c3 = 0 with
There are four constant factors associated with four terms on the right hand side of ( A7 ) Among the four terms , obviously T1 ≥ 0 and T3 ≥ 0 . By the convexity property of fi , we have T2 ≥ 0 and T4 ≥ 0 . We choose η = max npi/[2(nµ + Li) ] . By setting c1 = 0 with ζ = npI /(LM η ) − 1 > 0 , c2 = 0 with κ = L2 0 < α = 1 − ηµ/m < 1 , it can be verified that c4 ≤ 0 . With the aforementioned constant factor setting , Ei,j[H ( t ) ] − αH ( t−1 ) ≤ 0 , where the expectation is conditional on information from the previous iteration t − 1 . Taking expectation with this previous iteration gives Ei,j[H ( t ) ] ≤ αEi,j[H ( t−1) ] . By chaining over t iteratively , Ei,j[H ( t ) ] ≤ αtH ( 0 ) . Since the sum of the first three terms in ( A.6 ) is non negative by the convexity of F , we have κw(t ) − w∗2 ≤ H ( t ) . Together with the aforementioned results by chaining over t , the proof is complete . i=1
Li n n n i=1
1 Li
− η n
− 2ηµ n i=1
∇fi(w∗ ) − ∇fi(w(t−1))2
T4 =
2044
