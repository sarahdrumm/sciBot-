FUSE : Full Spectral Clustering
Wei Ye† , Sebastian Goebl† , Claudia Plant§ , Christian Böhm†
†Ludwig Maximilians Universität München , Munich , Germany
§ University of Vienna , Vienna , Austria {ye , goebl , boehm}@dbsifilmude claudiaplant@univieacat
ABSTRACT Multi scale data which contains structures at different scales of size and density is a big challenge for spectral clustering . Even given a suitable locally scaled affinity matrix , the first k eigenvectors of such a matrix still cannot separate clusters well . Thus , in this paper , we exploit the fusion of the cluster separation information from all eigenvectors to achieve a better clustering result . Our method FUll Spectral ClustEring ( FUSE ) is based on Power Iteration ( PI ) and Independent Component Analysis ( ICA ) . PI is used to fuse all eigenvectors to one pseudo eigenvector which inherits all the cluster separation information . To conquer the clustercollision problem , we utilize PI to generate p ( p > k ) pseudoeigenvectors . Since these pseudo eigenvectors are redundant and the cluster separation information is contaminated with noise , ICA is adopted to rotate the pseudo eigenvectors to make them pairwise statistically independent . To let ICA overcome local optima and speed up the search process , we develop a self adaptive and self learning greedy search method . Finally , we select k rotated pseudo eigenvectors ( independent components ) which have more cluster separation information measured by kurtosis for clustering . Various synthetic and real world data verifies the effectiveness and efficiency of our FUSE method .
Keywords Spectral clustering ; Power iteration ; ICA ; Givens rotation ; Multiscale data
1 .
INTRODUCTION
Clustering is a basic technique in data analysis and mining . Two commonly used methods are k means and Expectation Maximization clustering ( EM ) which pre assume that data fits a Gaussian model . Such model based clustering methods perform well if data fits the model . However , in most cases , we do not know the distribution of data . It is hard to decide which model to adopt . Spectral clustering , on the other hand , does not pre assume any model . It only uses local information ( point to point similarity ) to achieve global clustering . Thus it is very elegant and popular in data ming and machine learning . Spectral clustering transforms the clustering of a set of data points with pairwise similarities into a graph partitioning problem , ie , partitioning a graph such that the intra group edge weights are high and the inter group edge weights are low . There are three kinds of similarity graphs , ie , the ε neighborhood graph , the k nearest neighbor graph and the fully connected graph [ 20 ] . Luxburg [ 20 ] emphasized that “ theoretical results on the question how the choice of the similarity graph influences the spectral clustering result do not exist ” . However , the parameters ( ε,k,σ ) of these similarity graphs highly affect the clustering results , especially in cases where data contains structures at different scales of size and density . One usually used objective function in spectral clustering is normalized cut [ 16 ] . As pointed out in [ 14 ] , the normalized cut criterion does not always work even given a proper affinity matrix .
Consider three clusters of different geometry shapes and densities in Figure 1(a ) . Both Gaussian clusters have 100 data points . The rectangular stripe cluster has 400 data points sampled from a uniform distribution . Conventional spectral clustering algorithms tend to fail on this multi scale data . Self tuning spectral clustering ( ZP ) [ 23 ] proposes to use the locally scaled affinity matrix to solve the limitation . Further , ZP rotates the eigenvectors to create the maximally sparse representation to estimate the number of clusters automatically . However , such proposals still do not work on multi scale data because of the unsuitability of the normalized cut criterion only using local information . Such an argument can be inferred from Figure 1(c ) . ZP fails to correctly separate the three clusters . Both cuts are along the stripe . Intuitively , it is not difficult to understand . The normalized cut criterion tries to make clusters “ balanced ” as measured by the number of vertices or edge weights . Since each of the two Gaussian clusters only has 100 data points and they are so close to the stripe cluster , cuts between the Gaussian clusters and the stripe cluster have a higher penalty than those along the stripe .
Differing from other spectral clustering algorithms , our method combines the cluster separation information from all eigenvectors to achieve a better clustering result . As can be seen from Figure 1(d ) , only some controversial data points lying on the boundaries are clustered incorrectly . The fusion of the cluster separation information from all eigenvectors is accomplished by exploiting truncated Power Iteration ( PI ) . To yield good clustering , spectral clustering uses the first k eigenvectors of the graph Laplacian matrix . Similarly , we use PI to generate p ( p > k ) pseudoeigenvectors . Each pseudo eigenvector is a linear combination of all original eigenvectors , including the information not only from the “ informative ” eigenvectors but from the “ noise ” eigenvectors . Note that the pseudo eigenvectors are redundant to each other . One main question is how to make the information from the “ informative ” eigenvectors stand out and suppress the information from the
1985 ( a ) SYN1
( b ) eigenvalues
( c ) ZP
( d ) FUSE
Figure 1 : Clustering results of ZP and FUSE on our SYN1 data ( (b ) gives the top 10 eigenvalues of the normalized affinity matrix ) .
“ noise ” eigenvectors ? In this paper , we use Independent Component Analysis ( ICA ) to reduce the redundancy , ie , to make the pseudo eigenvectors statistically independent ( non redundant ) to each other . After whitening ( more details in Section 3.1 ) , ICA rotates the pseudo eigenvectors to find the direction in which the entropy is minimized . Subsequently , a kurtosis based selection strategy is exploited . Such a minimum entropy rotation plus a kurtosisbased selection improve the cluster separation . Our contributions are as follows , Contributions 1 ) We achieve the eigenvector fusion by using Power Iteration ( PI ) . The generated pseudo eigenvectors include information from all eigenvectors . 2 ) We improve the cluster separation by applying ICA combined with a kurtosis based selection strategy . Since the generated pseudo eigenvectors are redundant to each other , which is not beneficial to good clustering , we apply ICA to make them statistically independent . Then , a kurtosis based selection strategy is exploited to improve the cluster separation . To the best of our knowledge , we are the first to apply ICA on spectral clustering . 3 ) We develop a greedy search method to render searching for statistically independent components more efficient and effective . The greedy search strategy discriminates the search order to let ICA not get easily trapped into local optimal . In addition , during the search process , the greedy search makes use of self adaptive and self learning strategies to balance the efficiency and effectiveness .
2 . PRELIMINARIES
In this section , we give some major notations used in this paper and some background techniques our algorithm is based on to make this paper self contained . 2.1 Notations
We use lower case Roman letters ( eg a , b ) to denote scalars . Upper case Roman letters ( eg X , Y ) are used for continuous random variables . We denote vectors ( row ) by boldface lower case letters ( eg x ) . Matrices are denoted by boldface upper case letters ( eg X ) . We denote entries in a matrix by non bold lower case letters , such as xij . Row i of matrix X is denoted by the vector xi· , column j by the vector x·j . We use [ x1,··· , xn ] to denote a row created by stacking n continuous random variables ; similarly , we use X = [ x1;··· ; xm ] to denote creating a matrix by stacking xi along the rows . A set is denoted by calligraphic capital letters ( eg S ) . A cluster C is a set of data objects , ie , C = {o1 , o2,··· , on} . An undirected graph is denoted by G = ( V,E ) , where V is a set of graph vertices and E is a set of graph edges . An affinity matrix of the vertices is denoted by A ∈ Rn×n with aij ≥ 0 , aij = aji ( the sociated with A with dii = graph is undirected ) . The degree matrix D is a diagonal matrix asj aij . A normalized affinity matrix W is defined as D−1A . Thus , a normalized graph random walk Laplacian matrix is L = I − W = I − D−1A according to Meila and Shi [ 13 ] . We list common symbols used throughout this paper in Table 1 .
Table 1 : Common symbols used throughout the paper
Symbol I A W D L U Λ V E M I(X ; Y ) Mutual information between two random variables
Description Identity matrix Affinity matrix Normalized affinity matrix Degree matrix Graph Laplacian matrix Eigenvector matrix Eigenvalue matrix Pseudo eigenvector matrix Fused eigenvector matrix Demixing matrix
2.2 Power Iteration
Although spectral clustering has gained its popularity and success in data mining and machine learning fields , its high time complexity ( O(n3 ) for computing the eigenvectors of the graph Laplacian matrix L ) limits its practical use in real world data . To address the difficulty , Lin and Cohen [ 11 ] used truncated power iteration to find a pseudo eigenvector on the normalized affinity matrix W with time complexity O(n ) , which is very efficient and attractive . Note that the k largest eigenvectors of W are also the k smallest eigenvectors of L . Power Iteration ( PI ) is an efficient and popular method to compute the dominant eigenvector of a matrix . PI starts with a random vector v0 and iteratively updates as follows [ 11 ] , vt =
Wvt−1 Wvt−11
( 1 )
Suppose W has eigenvectors U = [ u1 ; u2;··· ; un ] with eigenvalues Λ = [ λ1 , λ2,··· , λn ] , where λ1 = 1 and u1 is constant . We have WU = ΛU and in general WtU = ΛtU . When ignoring renormalization , Equation 1 can be written as vt = Wvt−1 = W2vt−2 = ··· = Wtv0
= c1Wtu1 + c2Wtu2 + ··· + cnWtun = c1λt
2u2 + ··· + cnλt
1u1 + c2λt nun
( 2 )
00204060810020406081024681008809092094096098100204060810020406081002040608100204060811986 u2 + ··· +
λi t
According to Equation 2 , we have
λ2 t
λ1 vt c1λt 1
= u1 + c2 c1
λn t
λ1 cn c1 un
( 3 )
λ1
So the convergence rate of PI towards the dominant eigenvector ( 2 i n ) . PI will u1 depends on the significant terms finally converge to the dominant eigenvector u1 which is of little use in clustering . PIC [ 11 ] defines the velocity at t to be the vector δt = vt − vt−1 and defines the acceleration at t to be the vector t = δt − δt−1 and stops PI when tmax is below a threshold ˆ . 2.3 ICA Let s1,··· , sm be m one dimensional independent sources . Each has n iid samples denoted by si = [ si1,··· , sin](1 ≤ i ≤ m ) . Let S = [ s1;··· ; sm ] ∈ Rm×n and we assume S is hidden and only a matrix X of mixed sources can be observed . The task of ICA is to find a demixing matrix M ∈ Rm×m such that S = MX and every two components si and sj ( 1 ≤ i , j ≤ m , i = j ) are statistically independent . Without loss of generality , we assume data has been whitened , which means ( 1 ) the expectation value is zero and the covariance matrix is an identity matrix ( I ) , ( 2 ) the demixing matrix is square , orthogonal ( M · M = I ) and full rank .
3 . FULL SPECTRAL CLUSTERING 3.1 Fusing eigenvectors
For real world data , a single pseudo eigenvector is not enough when the number of clusters is large . The reason is we need more eigenvectors to discriminate clusters when the cluster count increases . Thus , the cluster collision problem may happen on onedimensional pseudo eigenvector . However , using PI p ( p > k ) times with random generated starting vectors to generate p pseudoeigenvectors is not sufficient either , which can only alleviate the situation a little because of the redundant information provided by these pseudo eigenvectors . It is just the first step of the eigenvector fusion . We also need to reduce the redundancy in these pseudo eigenvectors . Our goal is twofold : 1 ) generate p pseudoeigenvectors , 2 ) reduce redundancy to make the cluster separation information stand out and suppress the noise information . The goal can also be rephrased as fusing the cluster separation information from all original eigenvectors to improve clustering . Why do we need to fuse the information from all eigenvectors ? The analysis in [ 14 ] shows that “ when confronted with clusters of different scales , corresponding to a multi scale landscape potential , standard spectral clustering which uses the first k eigenvectors to find k clusters will fail ” . Even given a locally scaled affinity matrix [ 23 ] , ZP still cannot overcome the limitation if clusters have comparable densities .
For example , Figure 2(a ) demonstrates the eigenvector space ( consists of the eigenvectors associated with the top three minimum eigenvalues ) found by ZP ( other spectral clustering algorithms yield similar ones ) on the running example ( SYN1 ) in Section 1 . It demonstrates that the three clusters are connected together in the eigenvector space , which is the reason for k means’ difficulty in separating them . The cluster separation information is not provided by the first three eigenvectors . However , we can see from Figure 2(b ) that the blue and red clusters have fewer overlapped data points with the black cluster . If we fuse the information from the eigenvectors in Figure 2(b ) to those in Figure 2(a ) , we can achieve a better clustering result . In this paper , we use truncated Power Iteration ( PI ) to fuse eigenvectors . Figure 2 ( c ) shows the four pseudoeigenvectors returned by running PI four times with randomly gen erated starting vectors . The resulting pseudo eigenvectors are very similar and redundant , eg , the pseudo eigenvectors vt 1 and vt 4 . Thus , the cluster separation information is not standing out . Figure 2(d ) gives the pseudo eigenvector space returned by our algorithm . In such space , the blue and red clusters have much fewer close data points to the black cluster compared to those in Figure 2(a ) , which makes k means easily distinguish them from each other .
Consider that each pseudo eigenvector generated by PI is a linear combination of all eigenvectors of the normalized affinity matrix W and every pair of distinct pseudo eigenvectors is redundant . One way to reduce redundancy is to make p pseudoeigenvectors statistically independent , which can be accomplished by ICA . Mathematically speaking , the problem formulation is as follows ,
Problem : Statistically Independent Pseudo eigenvectors . Given a pseudo eigenvector matrix V ∈ Rp×n generated by running PI p times , find a demixing matrix M ∈ Rp×p such that E = MV and the sum of mutual information between pairwise components of E is minimized , where E ∈ Rp×n is a resulting independent pseudo eigenvector matrix .
JI ( M ) := min
I(ei· ; ej· )
( 4 )
1≤i,j≤p,i=j
The demixing matrix M can be derived by determining the directions of minimal entropy . To find such directions , ICA requires to whiten data , ie , the expectation value of data is zero and the covariance matrix of data is an identity matrix , by applying PCA . The demixing matrix M is orthonormal in white space . After whitening data , ICA finds those directions of minimal entropy by rotating the whitened data . In this paper , instead of using fastICA [ 6 ] , we use Jacobi ICA [ 8 , 7 ] to find statistically independent pseudoeigenvectors for the reasons that 1 ) we can choose different kinds of contrast functions , 2 ) we can make it escape from local optima more easily . Note that Equation 4 can be solved by iteratively optimizing every pairwise mutual information . We rewrite Equation 4 as the following objective function : min I(ei· ; ej· ) subject to E = MV , 1 ≤ i , j ≤ p , i = j
( 5 )
Now it comes to how to select k independent components . Since ICA is interested in searching for non Gaussian directions , in which negentropy is minimized . Non Gaussian may be superGaussian as well as sub Gaussian . We are only interested in subGaussian directions , in which clusters are as much separated as possible . Such directions can be best modeled by uniform distributions [ 2 ] . In this paper , we use kurtosis to measure the distance of the probability distribution of an independent component to Gaussian distribution . The kurtosis is the fourth standardized moment , defined as ,
E,(X − µ)4
( E ( (X − µ)2))2
Kurt(X ) =
µ4 σ4 =
( 6 ) where µ4 is the fourth moment of the mean and σ is the standard deviation .
The kurtosis of any univariate Gaussian distribution is 3 . The kurtosis of any sub Gaussian distribution is below 3 and the kurtosis of any sup Gaussian distribution is above 3 . We prefer the independent components associated with the top k minimum kurtosis values .
1987 ( a )
( b )
( c )
( d )
Figure 2 : Demonstration of the ( pseudo )eigenvector space generated by ZP and FUSE on SYN1 data . ( a ) the eigenvector space ( consists of the first three eigenvectors ) returned by ZP , ( b ) the eigenvector space consists of the fourth and fifth eigenvectors returned by ZP , ( c ) four pseudo eigenvectors generated by running PI four times with random initial vectors , ( d ) the pseudo eigenvector space returned by FUSE , in which the clusters are well separated .

· · · . . . · · · . . . · · · . . . · · ·
1
0
0
0
0
cos θ
sin θ
0
0
· · · . . . · · · − sin θ . . . · · · . . . · · · cos θ
0
· · · . . . · · · . . . · · · . . . · · ·
0
0

0
1
3.2 Givens Rotation
The objective function in Equation 5 is difficult to solve . Inspired by Learned Miller et . al [ 8 ] and Kirshner et . al [ 7 ] in which they used Givens rotation to estimate a demixing matrix for independent component analysis by sequentially rotating every two mixture components . The reason behind Givens rotation is : 2d pairwise distances are not changing after rotation , thus joint distribution remains the same , whereas marginal distributions change after rotation . Thus , any metric based on joint distribution and marginal distributions varies when the rotation angle θ varies . We can find the maximal or minimal values of metrics with respect to θ . For d dimensional data , a Givens rotation of angle θ for dimensions i and j is defined as :
G(i , j , θ ) = where sin θ is on the j th row and i th column and − sin θ is on the i th row and j th column of G .
The demixing matrix M can be estimated as
∗
) , 1 ≤ i , j ≤ p , i = j
M =
G(i , j , θ
( 7 ) where G(i , j , θ∗ ) is a Givens rotation of the best angle θ∗ for optimizing the mutual information of the dimensions i and j of a data matrix .
4 . ALGORITHM 4.1 Greedy Search
2
.0 , π
Learned Miller et . al [ 8 ] and Kirshner et . al [ 7 ] optimized Equation 5 by exhaustively search over K = 150 values of θ in the range fi which is time consuming . Besides , they did not discriminate the order of optimization for different pairwise dimensions , which results in getting easily trapped in local optima . Considering the two limitations , we propose a new optimization method which is very efficient and effective . over K = 150 values of θ in the range.0 , π fi . In contrast , we use
To speed up the search process , we do not exhaustively search the history search as anchors to guide the search process . From this perspective , the greedy search is a self learning method based on its
2
2
2 learned knowledge . The strategy of adjusting the search resolution ( see the following ) makes our greedy search self adaptive . Note that we only need to consider θ in the interval.0 , π fi because the effectiveness of any 90 degree rotation is equivalent as explained in [ 8 ] . In this paper , we adopt kernel generalized variance ( KGV ) proposed by Bach and Jordan [ 1 ] to estimate pairwise mutual information considering its linear complexity and especially its smoothness wrt log function . For a detailed description , please see [ 1 , 17 ] . Now we give an example to demonstrate our greedy search method . Because in practical use we can choose different contrast functions , here we just give a generalized function f to demonstrate the main idea . The curve in Figure 3 ( a ) is a function of θ . The goal is to find the best θ∗ achieving the maximal objective function f in maximal −f ) . Our method is described as follows : fi ( the minimal f can be achieved by finding the the interval.0 , π
Case 2 : We compute f ( θ3 ) as depicted in Figure 3 ( b ) .
Case 1 : As depicted in Figure 3 ( a ) , we set the ascending and 2K . We choose three different initial θ ( in descending step size to π our example is θ1 , θ2 and θ3 ) with the same interval ( eg π 2K ) and compute their objective function ( f ( θ1 ) , f ( θ2 ) and f ( θ3) ) . If f ( θ3 ) ≤ f ( θ2 ) ≤ f ( θ1 ) , we assume that the function is continuously decreasing and we multiply the descending step size by τ = 2 ( the search resolution ) and update θ3 for the following iteration . If τ is two large , the method may skip some important search niche , while if τ is too small , the efficiency will be decreased . We update θ1 = θ2 , θ2 = θ3 , f ( θ1 ) = f ( θ2 ) and f ( θ2 ) = f ( θ3 ) as history reference values for the following search . If f ( θ2 ) ≤ f ( θ3 ) and f ( θ2 ) ≤ f ( θ1 ) , we assume the function is continuously increasing . We set the descending step size to its initial value and multiply the ascending step size by τ and update θ3 . Also , we update θ1 = θ2 , θ2 = θ3 , f ( θ1 ) = f ( θ2 ) and f ( θ2 ) = f ( θ3 ) as history reference values for the following search . If f ( θ3 ) ≤ f ( θ2 ) ≤ f ( θ1 ) , go to case 1 . If f ( θ1 ) ≤ f ( θ2 ) ≤ f ( θ3 ) , we assume the function is continuously increasing . We set the descending step size to its initial value and multiply the ascending step size by τ and update θ3 . Also , we update θ1 = θ2 , θ2 = θ3 , f ( θ1 ) = f ( θ2 ) and f ( θ2 ) = f ( θ3 ) as history reference values for the following search . if not , go to case 4 . Case 4 : In this case ( Figure 3 ( d) ) , f ( θ1 ) ≤ f ( θ2 ) and f ( θ3 ) ≤ f ( θ2 ) , we assume there may be some peaks in the interval . We 2K . Fiexhaustively search in the interval [ θ1 , θ3 ] with a step size π nally , we update θ1 = θ2 , θ2 = θ3 , f ( θ1 ) = f ( θ2 ) , f ( θ2 ) = f ( θ3 ) and set the ascending step size to its initial value . Since now f ( θ2 ) ≤ f ( θ1 ) , we assume the function is continuously decreas
Case 3 : We compute f ( θ3 ) as depicted in Figure 3 ( c ) .
00500500eigenvector 30.05eigenvector 1 00501eigenvector2 01 0020002004006008 01 005000501eigenvector 4 015 01 005000501eigenvector 51234100200300400500600 34 2 12IC30IC201IC1 23210 1 21988 We repeat the above four cases until θ3 ≥ π ing . We multiply the descending step size by τ and update θ3 . If f ( θ3 ) ≤ f ( θ2 ) , go to case 1 ; if f ( θ3 ) ≥ f ( θ2 ) , go to case 2 . case , we update the best objective function value fb and θ∗ . 4.2 FUSE
2 . Note that , in each
As said before , in [ 8 ] and [ 7 ] , the authors do not differ between the order of optimizing pairwise dimensions which results in the algorithm ’s getting easily trapped in local optima . In this paper , we use a greedy selection method , ie , computing the objective function values for each pairwise dimensions and then optimizing pairs according to their objective function values from the worst to the best , to make our method not easily get trapped in local optima . Our pseudo code is given in Algorithm 1 .
Steps 1 – 2 initialize the demixing matrix M to an identity matrix , compute the affinity matrix A and normalize it to W . Steps 3 – 8 generate p = k + 1 pseudo eigenvectors using power iteration ( PI ) . In step 4 , the starting vector is randomly generated following a Gaussian distribution with mean 0 and variance 1 . In Steps 5 – 8 , each starting vector is iteratively updated by power iteration until the acceleration threshold ˆ or the maximum iteration number is reached . Step 10 whitens the pseudo eigenvector matrix V to let it have zero expectation value and an identity covariance matrix . In step 11 , c includes the indices of each pairwise components in E and their mutual information values ( ai ∈ {1 , 2,··· , p} ) . To let the algorithm escape from local optima , step 14 sorts c in descending order wrt mutual information values . Steps 15 – 19 use the greedy search to find the best θ∗ for each pairwise components of E to make them statistically independent and update components’ pairwise mutual information value stored in cj3 , and also update E and M . We set the mutual information threshold to 0.1 for a balance between the efficiency and effectiveness . If we set it lower , the efficiency will be decreased but effectiveness will be increased and vice versa . Step 20 returns the selected independent components which will be fed to k means to find clusters .
( a )
( c )
( b )
( d )
Figure 3 : Greedy search strategy erating one pseudo eigenvector costs O(n ) time [ 12 ] , and thus the runtime complexity to generate p = k + 1 pseudo eigenvectors by power iteration is O((k + 1)n ) . In step 10 , as a preprocessing step , whitening data costs O((k + 1)2n ) time . In this paper , we adopt Kernel Generalized Variance ( KGV ) using incomplete Cholesky decomposition proposed in [ 1 ] to estimate mutual information . The complexity of KGV is O(m2M 2n)[1 ] , where m is data dimension and M is the maximal rank considered by the low rank decomposition algorithms for the kernels . In step 11 , the computation time for all pairwise mutual information values is k(k+1 ) 1 n ) . To make FUSE escape from local optimal , we sort c using quick sort algorithm . The runtime complexity of the ordering process is 3(k + 1 ) · O(l log l ) = ) = O(k4 log k ) . The time cost 3(k + 1 ) · O( k(k+1 ) of finding independent pseudo eigenvectors is 3(k + 1 ) · k(k+1 ) · K ·O(22M 2 2 n ) . Finally , we use k means to cluster on the selected independent pseudo eigenvectors . The total runplus the time complexity of k means , ie , O(nk)×(# k means iterations ) [ 3 ] . time complexity of FUSE is O,k2(M 2
2 + k2 log k )
1 n ) = O(k2M 2
2 n ) = O(k3M 2
· O(22M 2
1 n + knM 2 log k(k+1 )
2
2
2
2
Algorithm 1 : FUSE Input : Data X ∈ Rm×n Output : cluster indicator vectors
1 T ← 1000 , levels ← 3 , sweeps ← p ; 2 Initialize M ∈ Rp×p to an identity matrix and compute the 3 for j ← 1 to p do 4 normalized affinity matrix W ∈ Rn×n ;
/* p = k + 1 */
/* vj ∈ R1×n */ */ t ← 0 , v0 j ← randn ( 1 , n ) ; /* power iteration repeat j until δt+1
5 j ← Wvt 6 ; vt+1 j1 Wvt δt+1 ← |vt+1 j − vt j| ; 7 t ← t + 1 ; 8 jmax ≤ ˆ or t ≥ T ; j − δt 9 10 V = [ v1 ; . . . ; vp ] ; 11 V ← whiten ( V),E ← MV ; 12 c ← ( (a1 , a2 , I1 = I(va1· ; va2·) ) , . . . , ( al−1 , al , Il = 13 for level ← 1 to levels do 14 15
/* c has l =,p for sweep ← 1 to sweeps do
tuples
I(val−1· ; val·)) ) ; c ← order_descending_by_I_value ( c ) ; /* minimize pairwise mutual for j ← 1 to l do information
2 if cj3 > 0.1 then
[ θ∗ , cj3 ] ← greedy_search ( cj1 , cj2 , E ) ; E ← G(cj1 , cj2 , θ∗)E ; M ← G(cj1 , cj2 , θ∗)M ;
16 17 18 19 20
*/
*/
21 compute kurtosis of each pseudo eigenvector in E and return the pseudo eigenvectors associated with the top k minimum values ;
4.3 Complexity Analysis
We omit the runtime for computing the affinity matrix which is a common step in all spectral clustering methods . The analysis of runtime complexity of FUSE is as follows : In steps 3 – 8 , gen
5 . EXPERIMENTAL EVALUATION Competitors : To evaluate the performance of FUSE , we adopt three spectral clustering methods NCut [ 16 ] , NJW [ 15 ] and ZP f(θ1)θ1θ3θ2fθf(θ2)f(θ3)f(θ1)θ1θ3θ2fθf(θ2)f(θ3)f(θ1)θ1θ3θ2fθf(θ2)f(θ3)f(θ1)θ1θ3θ2fθf(θ2)f(θ3)1989 [ 23 ] and power iteration based clustering methods PIC [ 11 ] , PIC k [ 10 ] , DPIC [ 18 ] and DPIE [ 5 ] as competitors . FUSE and all the comparison methods are written in Matlab . All experiments are run on the same machine with an Intel Core Quad i7 3770 with 3.4 GHz and 32 GB RAM . The code of FUSE and all synthetic and real world data used in this paper are available at the website1 . Parameters : The parameters for spectral clustering , poweriteration based clustering methods are set according to their original papers . For FUSE , we set ˆ i = i · ( cid:100)log(k ) · 1e−5 as adopted n by DPIE [ 5 ] . For text data , we use cosine similarity ( ) xi2xj2 to compute the affinity matrix . For network data , the element aij of the affinity matrix A is simply 1 if blog i has a link to j or vice versa , otherwise aij = 0 . For all other data , the locally scaled affinity matrix is computed as the way proposed in [ 23 ] with KN N = 7 . The original ZP method automatically chooses the number of clusters . For a fair comparison , we give ZP the correct number of clusters . xi·xj
Since all the comparison algorithms use k means in the last step , in each experiment k means is run 100 times with random starting points and the most frequent cluster assignment is used [ 11 ] . We run each experiment 50 times and report the mean and standard deviation of Adjusted Mutual Information ( AMI ) [ 19 ] . For AMI , higher value means better clustering . 5.1 Synthetic Data
511 Quality Synthetic dataset SYN1 is the running example used in Section 1 . SYN1 has three clusters . Each of the two Gaussian clusters has 100 data points and the stripe cluster has 400 data points . We have shown the results before .
Synthetic dataset SYN2 has three clusters as well depicted in Figure 4(a ) . Both Gaussian clusters have 100 data points and the rectangular cluster has 400 data points . Both Gaussian clusters have some very close data points to the rectangular cluster making them hard to be separated correctly . The mean AMI of FUSE is 0.750 and the highest of the comparison algorithms’ is 0.574 achieved by PIC k . ZP only has a value 0483 Figure 4(b)– ( d ) give us an intuitive demonstration . FUSE just wrongly clustered a few data points in the magenta rectangular cluster to the blue Gaussian cluster , while ZP and PIC k wrongly clustered about a half of the data points in the magenta rectangular cluster to the blue Gaussian cluster . Our algorithm is superior to the competing algorithms .
SYN3 in Figure 5(a ) also has three clusters . Two Gaussian clusters each has 90 and 92 data points , respectively . The blue ring cluster has 130 data points . SYN3 is very interesting because the density of the blue ring cluster is lower than those of the Gaussian clusters . And the ring cluster is very close to the Gaussian clusters , which could make the KN N = 7 neighbors of some points in the blue ring cluster be belonging to the Gaussian clusters . SYN3 is also difficult to cluster correctly . However , FUSE achieved the best compared to all the comparison algorithms . PIC k even clustered several data points belonging to two Gaussian clusters to the ring cluster , which did not make sense .
SYN4 in Figure 6(a ) contains five clusters , each of the two Gaussian clusters has 100 data points , each of the two square clusters has 82 and 100 data points respectively and the ring cluster has 56 data points . The ring cluster is very close to the square clusters and even has some overlap with the two Gaussian clusters . Still , FUSE achieved the best result , only not distinguishing between the overlapped data points from the Gaussian clusters and the ring cluster .
ZP wrongly detected a half of the data points in the ring cluster to the green Gaussian cluster . PIC k wrongly clustered several data points in the ring cluster to the black square cluster although the densities of these two clusters are significantly different .
For all these multi scale synthetic datasets , our algorithm FUSE outperforms all the competing algorithms . FUSE is even superior to the spectral clustering algorithms ZP , NCut and NJW , which proves that the normalized cut criterion is not alway suitable for clustering . FUSE E is our algorithm using the exhaustive search strategy over θ proposed in [ 8 , 7 ] which does not discriminate the order of optimization of the pseudo eigenvectors generated by PI . We can see that sometimes it gets trapped in local optimal ( the result on SYN3 ) . Our algorithm FUSE adopting the greedy search strategy achieves quite similar or even better results than using the exhaustive search strategy .
Scalability
512 In this experiment , we want to test the runtime against the number of data points using data SYNC5 . Synthetic dataset SYNC5 is generated as follows : Firstly , we generate two 2D clusters sampled from uniform distributions with the number of data points 4,000 and 1,000 respectively . The two clusters are our basis clusters . Then at each step we increase the data points in each cluster by the size of its basis . Finally , we have data points varying from 5,000 to 30,000 by a step size 5,000 . We feed each algorithm with the same affinity matrix . Thus , the runtime does not include the computation time for the affinity matrix . The results are demonstrated in Figure 7 . Since the runtime of NJW and NCut are similar , here we only show the runtime of NCut for a clearer demonstration . Figure 7 shows that the runtime of FUSE becomes much lower than that of ZP , NCut and DPIC when increasing the number of data points . Compared to PIC , we can see their slope variances are quite similar . The runtime difference between FUSE and PIC is owing to that FUSE needs to determine the directions in which the entropy of pseudo eigenvectors is minimized . Compared with PIC k , the runtime of FUSE becomes close to that of PIC k when the number of data points increases to 30,000 . Note that FUSE E is our algorithm using the exhaustive search strategy . FUSE is faster than FUSE E as can be seen from the figure . Our algorithm is of more practical use than ZP , NCut , NJW and DPIC .
Table 2 : AMI on Synthetic Data ( mean ± standard deviation )
SYNC4
SYNC3
SYNC2
AMI SYNC1 0715±0131 0750±0127 0702±0048 0891±0016 FUSE FUSE E 0735±0142 0750±0120 0688±0044 0886±0018 0.882±0 0.528±0 0.483±0 0.374±0 ZP 0.522±0 0.370±0 0.479±0 0874±0002 NCUT 0.379±0 0.451±0 0879±0002 0.533±0 NJW 0355±0088 0.309±0 0494±0059 0840±0034 PIC 0324±0048 0574±0105 0508±0055 08544±0022 PIC k 0324±0094 0465±0113 0499±0107 0482±0064 DPIC 0350±0056 0128±0097 0.310±0 DPIE
0.630±0
5.2 Real world Data
521 Clustering Now we demonstrate the effectiveness of our FUSE on seven real world datasets . PENDIGITS is available from UCI machine learning repository2 . The original datasets MNIST , 20NEWSGROUPS , REUTERS21578 , TDT2 and RCV1 are available at this
1https://github.com/yeweiysh/FUSE
2http://archiveicsuciedu/ml/
1990 ( a ) SYN2
( b ) FUSE
( c ) ZP
( d ) PIC k
Figure 4 : Clustering results on SYNC2 ( shown are the most frequent clusterings )
( a ) SYN3
( b ) FUSE
( c ) NJW
( d ) PIC k
Figure 5 : Clustering results on SYNC3 ( shown are the most frequent clusterings )
( a ) SYN4
( b ) FUSE
( c ) ZP
( d ) PIC k
Figure 6 : Clustering results on SYNC4 ( shown are the most frequent clusterings ) website3 . 20NGD from [ 11 ] is a subset of 20NEWSGROUPS , and MNIST0127 from [ 18 ] is a subset of MNIST . TDT2_3CLASSES , REUTERS_4CLASSES and RCV1_4CLASSES are samples from original REUTERS21578 , TDT2 and RCV1 corpus using random indices from the website3 . AGBLOG is a connected network dataset of 1222 liberal and conservative political blogs mined from blog homepages [ 11 ] . For text datasets , we use the preprocessed document term matrix to compute the TF IDF matrix . Then each feature vector is normalized to have unity norm . Finally , we use cosine similarity to compute the affinity matrix . The statistics of all datasets are given in Table 3 .
From Table 4 , we can see that on all datasets , FUSE achieves the best results , even outperforms self tuning spectral clustering algorithm ( ZP ) and the conventional spectral clustering algorithms ( NCut and NJW ) . Compared to PIC and PIC k , FUSE improves AMI on each dataset . The most likely reason is the pseudoeigenvectors found by FUSE are statistically independent ( nonredundant ) , which make every cluster stand out in each pseudoeigenvector . Compared to DPIC and DPIE which also aim at reducing redundancy in pseudo eigenvectors generated by PI , FUSE
3http://wwwcadzjueducn/home/dengcai/Data/datahtml
Table 3 : Statistics of Datasets
Dataset PENDIGITS MNIST0127 AGBLOG 20NGD TDT2_3CLASSES REUTERS_4CLASSES RCV1_4CLASSES
#instances 7494 4189 1222 800 314 649 1000
#features 16 784 498 26214 36761 18933 29985
#clusters 10 4 2 4 3 4 4 improves AMI much on most datasets . One reason is that finding directions in which the entropy is minimized is much more beneficial to clustering . results are interesting
Two most on AGBLOG and TDT2_3CLASSES datasets . All comparison methods except PIC and PIC k fail on AGBLOG dataset . AGBLOG dataset has two balanced clusters with the number of instances 586 and 636 , respectively . We show the most frequent data embeddings of FUSE and ZP ( the results of NJW and NCut are very similar ) in Figure 8(a ) , ( b ) and ( c ) . We can see that most data points ( blue ones in Figure 8(b ) and ( c ) ) are assigned to one cluster , which
1991 makes the results of ZP and PIC k not appealing . However , our algorithm finds the embedding space in which two clusters are separated evenly . arate the two elephants , but the sky is well segmented . DPIE can also distinguish the two elephants but the segmentation is worse than FUSE ’s . In addition , DPIE does not segment the sky well .
6 . RELATED WORK
Spectral clustering . Spectral clustering is very popular in data ming owing to its ability to detect arbitrary shape clusters in data spectrum space . Spectral clustering can be divided into three categories by the type of Laplacian matrix , ie , unnormalized spectral clustering , normalized spectral clustering proposed by Shi and Malik ( NCut ) [ 16 ] and another normalized spectral clustering proposed by Ng , Jordan and Weiss ( NJW ) [ 15 ] . After deciding the type of Laplacian matrix , it computes the first k eigenvectors of the Laplacian matrix and then uses k means to cluster in the space formed by these eigenvectors . Spectral clustering is very elegant . However , the computation cost is very high for large scale data . Finding eigenvectors takes O(n3 ) in general . Recently , researchers have proposed many fast approximating techniques , such as IRAM and sampling techniques [ 4 , 22 ] . Spectral clustering assumes that the “ informative ” eigenvectors are those associated with the smallest k eigenvalues , which seems not to be successful on some realworld data with much noise or multi scale density . And this promotes many researchers work on how to select much informative eigenvectors , and how to estimate the local scale of data with varying densities , shapes and levels of noise [ 23 , 9 , 21 , 3 ] . ZP [ 23 ] is a representative of all these algorithms . ZP takes local scaling into consideration and constructs a locally scaled affinity matrix which proves beneficial to clustering especially for multi scale data or data with irregular background clutter . ZP also exploits the structure of eigenvectors to improve clustering . As NCut , NJW and other spectral based clustering methods , ZP only uses the first k eigenvectors to cluster , which is not appropriate in some cases . However , our algorithm FUSE exploits all “ informative ” eigenvectors and fuses all their information to accomplish better clustering . Power iteration based clustering . Power Iteration Clustering ( PIC ) [ 11 ] uses truncated power iteration on a normalized affinity matrix of the data points to find a very low dimensional data embedding which is a linear combination of the major eigenvectors for clustering . It is very elegant and efficient . However , the assumptions it bases on are very strict and it returns only one pseudoeigenvector which prevents its performance on data with large number of clusters , where cluster collision problem is easy to happen . PIC k [ 10 ] has been proposed to alleviate the situation but actually it still cannot solve the cluster collision problem due to much similarity exists in the returned pseudo eigenvectors . Another clustering algorithm based on power iteration is Deflation Power Iteration Clustering ( DPIC)[18 ] which uses Schur complement deflation to generate multiple orthogonal pseudo eigenvectors . However , the pseudo eigenvectors still contain noise together with clusterseparation information . Diverse Power Iteration Clustering ( DPIE ) [ 5 ] normalizes the residue ( regression ) error which is obtained by subtracting the effects of the already found DPIEs from the embeddings returned by PIC . However , DPIE cannot guarantee to find diverse embeddings in every iteration and it bases on the assumption that clear eigen gap exists between every two successive eigenvalues which is also very strict . Our method FUSE does not make any assumptions and finds statistically independent pseudo eigenvectors , each of which is a different linear combination of the original eigenvectors . Besides , each statistically independent pseudo eigenvector eliminates noise and only keeps cluster separation information which makes FUSE much more advanced and effective .
Figure 7 : Runtime comparison
Figure 9 shows the clustering results on the TDT2_3CLASSES dataset . Figure 9(b ) demonstrates the eigenvector space found by ZP , in which the red square cluster and the dot magenta cluster are connected together . ZP only achieves 0.673 in terms of AMI . PIC k found two pseudo eigenvectors . Also in its found embedding space , two clusters ( blue and magenta ) are not well separated . However , in the embedding space detected by our algorithm , three clusters are well separated , which makes the value of AMI much higher than those of the competing methods as can be seen in Table 4 . Thus , the embedding space found by our algorithm is much more attractive and effective .
If we look into Table 5 , we can find that we have six datasets on which the runtime of our method is much lower than that of NCUT and NJW . And we also have four datasets on which our method is faster than ZP . Our method is very efficient and promising for practical use . However , on PENDIGITS dataset , FUSE is slower than the conventional spectral clustering algorithms because the maximal rank M considered by KGV is close to n which costs much time to compute the pairwise mutual information . 522 Image Segmentation In this section , we apply our algorithm on image segmentation . Figure 10 shows two examples from the Berkeley Segmentation Dataset and Benchmark 4 . Each pixel is represented as a five dimensional vector of its pixel coordinates x and y , and the color intensities [ 3 ] . We set the number of clusters in Figure 10(a ) three and four for Figure 10(e ) . Since the results returned by ZP , NCut and NJW are very similar , we only show the results of ZP here . For other methods , we show such methods whose results are more interpretable . Figure 10(b ) shows that FUSE separates the deer , the grass and the forest very well . ZP correctly segments the grass , but does not distinguish the deer from the forest , while PIC k recognizes the deer but not the grass or the forest . Compared to Figure 10(a ) , Figure 10(e ) demonstrates a more challenging task because the two elephants are very similar in terms of the color and position . However , our method FUSE successfully distinguishes these two similar elephants and also recognizes the sky . ZP cannot sep4https://wwweecsberkeleyedu/Research/Projects/CS/vision/bsds/ BSDS300/html/dataset/images.html
5k10k15k20k25k30knumber of data points100101102103104105runtime ( sec)FUSEFUSE EZPNCUTPICPIC kDPICDPIE1992 ( a ) FUSE
( b ) ZP
( c ) PIC k
Figure 8 : The embedding space found by FUSE , ZP and PIC k on AGBLOG data .
( a ) FUSE
( b ) ZP
( c ) PIC k
Figure 9 : The embedding space found by FUSE , ZP and PIC k on TDT2_3CLASSES data .
Table 4 : AMI on Real world Data ( mean ± standard deviation )
AMI FUSE ZP NCUT NJW PIC PIC k DPIC DPIE
PENDIGITS 0828±0009 0.813±0 0.800±0 0.800±0 0.680±0 0773±0024 0616±0044 0624±0034
AGBLOG 0729±0001 0.017±0 0.002±0 0.017±0 0226±0317 0227±0288 0.330±0 0.050±0
20NGD 0348±0017 0.293±0 0325±0022 0.265±0 0318±0004 0284±0051 0046±0028 0271±0046
MNIST0127 0594±0043 0.444±0 0.418±0 0496±0040 0446±0034 0456±0006 0331±0006 0011±0003
REUTERS_4CLASSES 0593±0031 0585±002 0539±0004 0567±0005 0.310±0 0366±0069 0234±0028 0434±0198 Table 5 : Runtime ( sec ) on Real world Data ( mean ± standard deviation )
TDT2_3CLASSES 0951±0005 0.673±0 0.670±0 0.670±0 0308±0287 0400±0300 0606±0019 0135±0197
RCV1_4CLASSES 0493±0015 0.452±0 0.405±0 0.402±0 0335±0034 0351±0021 0150±0044 0404±0043
Runtime FUSE ZP NCUT NJW PIC PIC k DPIC DPIE
PENDIGITS 90644±13336 50250±3105 50375±2066 51±3.406 5197±0017 13745±5020 299556±30740 0099±0520
MNIST0127 3831±0890 41.188±0 62.697±0 59.940±0 0249±0031 0263±0036 20270±1933 1816±3822
AGBLOG 0325±0074 0.781±0 2.318±0 0.374±0 0063±0010 0035±0006 0521±0047 0003±0002
20NGD 0474±0132 0.271±0 0.957±0 0.807±0 0013±0003 0002±0002 0468±0078 0097±0009
TDT2_3CLASSES 0826±0376 0.032±0 51429±3824 50286±3302 0035±0042 0005±0015 0099±0078 0183±0019
REUTERS_4CLASSES 0394±0201 0.438±0 51±3.512 51±3.873 0010±0006 0.002±0 0231±0042 0104±0015
RCV1_4CLASSES 0391±0148 52714±4310 52143±3891 52143±4100 0.003±0 0.003±0 0502±0074 3287±0041
7 . CONCLUSION
We have proposed FUSE to handle multi scale data on which the normalized cut criterion tends to fail even given a suitable locally scaled affinity matrix . FUSE exploits PI and ICA to fuse all “ informative ” eigenvectors to yield better clustering . Since the pseudoeigenvectors fused by PI are redundant and the cluster separation information does not stand out , ICA is adopted to reduce the redundancy . Then , a kurtosis based selection strategy is used to improve cluster separation . To speed up the search process , we have developed a greedy search method which learns from its history search records and also adaptively adjusts its search resolution . Extensive experiments and evaluations on various synthetic and real world data show FUSE ’s promising in dealing with multi scale data . Future work would go to explore how to adaptively select the number of pseudo eigenvectors for different datasets .
Acknowledgement Warm thanks to Hao Huang for his DPIE code and anonymous reviewers . This work has been supported by the China Scholarship Council ( CSC ) .
IC 1IC 2eigenvector 1eigenvector 2050010001500indices of data points817281748176817881881828184pseudo eigenvector#10 4IC 1IC 2IC 3eigenvector 3eigenvector 2eigenvector 1pseudo eigenvector 1pseudo eigenvector 21993 ( a )
( e )
( b ) FUSE
( c ) ZP
( d ) PIC k
( f ) FUSE
( g ) ZP
( h ) DPIE
Figure 10 : Image segmentation ( shown are the most frequent clusterings of each method )
8 . REFERENCES [ 1 ] F . R . Bach and M . I . Jordan . Kernel independent component analysis . The Journal of Machine Learning Research , 3:1–48 , 2003 .
[ 2 ] C . Böhm , C . Faloutsos , and C . Plant . Outlier robust clustering using independent components . In Proceedings of the 2008 ACM SIGMOD international conference on Management of data , pages 185–198 . ACM , 2008 .
[ 3 ] C . D . Correa and P . Lindstrom . Locally scaled spectral clustering using empty region graphs . In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 1330–1338 . ACM , 2012 . [ 4 ] C . Fowlkes , S . Belongie , F . Chung , and J . Malik . Spectral grouping using the nystrom method . Pattern Analysis and Machine Intelligence , IEEE Transactions on , 26(2):214–225 , 2004 .
[ 5 ] H . Huang , S . Yoo , D . Yu , and H . Qin . Diverse power iteration embeddings and its applications . In Data Mining ( ICDM ) , 2014 IEEE International Conference on , pages 200–209 . IEEE , 2014 .
[ 6 ] A . Hyvärinen . Fast and robust fixed point algorithms for independent component analysis . Neural Networks , IEEE Transactions on , 10(3):626–634 , 1999 .
[ 7 ] S . Kirshner and B . Póczos . Ica and isa using schweizer wolff measure of dependence . In Proceedings of the 25th international conference on Machine learning , pages 464–471 . ACM , 2008 .
[ 8 ] E . G . Learned Miller et al . Ica using spacings estimates of entropy . The Journal of Machine Learning Research , 4:1271–1295 , 2003 .
[ 9 ] Z . Li , J . Liu , S . Chen , and X . Tang . Noise robust spectral clustering . In Computer Vision , 2007 . ICCV 2007 . IEEE 11th International Conference on , pages 1–8 . IEEE , 2007 .
[ 10 ] F . Lin . Scalable methods for graph based unsupervised and semi supervised learning . PhD thesis , Carnegie Mellon University , 2012 .
[ 11 ] F . Lin and W . W . Cohen . Power iteration clustering . In
Proceedings of the 27th International Conference on Machine Learning ( ICML 10 ) , June 21 24 , 2010 , Haifa , Israel , pages 655–662 , 2010 .
[ 12 ] F . Lin and W . W . Cohen . A very fast method for clustering big text datasets . In ECAI , pages 303–308 , 2010 .
[ 13 ] M . Meila and J . Shi . A random walks view of spectral segmentation . 2001 .
[ 14 ] B . Nadler and M . Galun . Fundamental limitations of spectral clustering . In Advances in Neural Information Processing Systems , pages 1017–1024 , 2006 .
[ 15 ] A . Y . Ng , M . I . Jordan , Y . Weiss , et al . On spectral clustering : Analysis and an algorithm . Advances in neural information processing systems , 2:849–856 , 2002 .
[ 16 ] J . Shi and J . Malik . Normalized cuts and image segmentation . Pattern Analysis and Machine Intelligence , IEEE Transactions on , 22(8):888–905 , 2000 .
[ 17 ] Z . Szabó , B . Póczos , and A . L˝orincz . Undercomplete blind subspace deconvolution . The Journal of Machine Learning Research , 8:1063–1095 , 2007 .
[ 18 ] N . D . Thang , Y K Lee , S . Lee , et al . Deflation based power iteration clustering . Applied intelligence , 39(2):367–385 , 2013 .
[ 19 ] N . X . Vinh , J . Epps , and J . Bailey . Information theoretic measures for clusterings comparison : Variants , properties , normalization and correction for chance . The Journal of Machine Learning Research , 11:2837–2854 , 2010 .
[ 20 ] U . Von Luxburg . A tutorial on spectral clustering . Statistics and computing , 17(4):395–416 , 2007 .
[ 21 ] T . Xiang and S . Gong . Spectral clustering with eigenvector selection . Pattern Recognition , 41(3):1012–1029 , 2008 .
[ 22 ] D . Yan , L . Huang , and M . I . Jordan . Fast approximate spectral clustering . In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 907–916 . ACM , 2009 .
[ 23 ] L . Zelnik Manor and P . Perona . Self tuning spectral clustering . In Advances in neural information processing systems , pages 1601–1608 , 2004 .
1994
