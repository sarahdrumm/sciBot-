Evolutionary Structure Learning Algorithm for Bayesian Network and Penalized
Mutual Information Metric
Gang LI
Fu TONG *
Honghua DAI
School of Computing and Mathematics , Deakin University , Victoria 3168 , Australia
*School of Computer , Shanghai University , Shanghai 200072 , China gangli@deakineduau ftong@onlineshcn hdai@deakineduau
Abstract that best formulates
This paper approximates the problem of learning Bayesian network structures from data as determining the structure probability distribution indicated by the data . A new metric , Penalized Mutual Information metric , is proposed , and a evolutionary algorithm is designed to search for the best structure among alternatives . The experimental results show that this approach is reliable and promising . the
1 . Problem Definition and PMI Metric a is network powerful
Bayesian knowledge representation and reasoning tool under uncertainty [ 1 ] . However , the construction of a Bayesian network manually is usually time consuming and subject to mistakes . Therefore , algorithms for automatic learning , that occasionally use the information provided by an expert , can be of great help [ 2 ] . Considering the fact that any Bayesian network for domain U uniquely determines a joint probability function over the domain U , the problem of structure learning of Bayesian network , can be viewed as finding the best approximate decomposition of the target distribution determined by the data . Let pˆ be the probability distribution function represented by a Bayesian network , the KL difference [ 3 ] between pˆ and the target probability distribution function p over the domain is , ( vpKL v 1 n
( vp 1
) )
) , v
(
L
L
,
,
, n
)
= v
1
∑ , ,
L v
( vp 1
,
L
, v n
)
⋅ log
( vp 1
,
L
, v n
)
− n n
∑ = 1 i
( vMI i
,
( vPa i
) )
− n
∑∑ = 1 v i i
⋅
( vp i
) log
( vp i
) between node iv and its parents
( ivPa
)
. Since values of the other terms are independent of the Bayesian network structure , the KL difference between the Bayesian network structure and target distribution is minimized just when the total mutual information is maximized . However , it can be proved that this principle could lead to more complex structures . Our response to this problem is to incorporate some form of penalty for model complexity into the total mutual information like this : n ∑ = 1 i where
( vMI i
,
( vPa
) ) i
−
( Nf
)
× dim(
S
) dim(S
) is the dimension , ie number of parameters needed to specify the Bayesian network with structure S , is a non negative penalization function . Based on the Bayesian Information Criterion [ 4 ] ,
( Nf
) we set
( Nf
)
= log N 2 N
, so we can get Penalized Mutual
Information metric as follows : n ∑ = 1 i
( vMI i
,
( vPa
) ) i
− log N 2 N
× dim(
S
)
3 . Structure Learning by Evolutionary Algorithm
In this section , we use evolutionary search methods to identify network structures with the highest score by selected metric . For a domain with n variables , a Bayesian network structure can be , represented by an connectivity matrix
=
2 ijrM
[
× nn
]
L
,{ vv 1
, nv
}
=
U
,
Where
( vMI
,
( vPa
) ) i i is the mutual information
Proceedings of the 2001 IEEE International Conference on Data Mining ( ICDM(cid:146)01 ) 0 7695 1119 8/01 $17.00 ' 2001 IEEE jv is a parent of where ijr equals 1 when iv , or 0 if not . Based on this representation , we defined two reproduction operators : Intersection results in a structure consists of the common of arcs in the parents ; Union results in a structure consists of the union of the arcs in the parents .
In addition to these , five mutation operators are designed , they are : Simple Mutation randomly adds or removes an arc from the structure ; MI Guided Mutation , randomly selects arc to add or remove according its corresponding mutual information ; Arc Reversion randomly selects an arc and reverses its direction ; Parent Shift randomly selects an arc , and changes its starting point to another node ; Child Shift randomly selects an arc , and changes its ending point to another node .
To assure the closeness of the reproduction and mutation operators , three repair operators , DAG Repair , Max Parents Repair , and Partial Order Repair , are introduced to assure that each resulting structure during the evolution is a valid DAG , and do not violate the prior knowledge .
4 . Experiment Results
In order to evaluate the performance of proposed algorithm , we present our experimental comparisons of structure learning by evolutionary algorithm and PMI metric ( PMI EA ) , with a genetic structure learning algorithm ( MDL GA ) described in [ 5 ] . Both algorithms attempt to induce a Bayesian network structure from a database . Four databases are used in the experiment . Six values related to the behavior of algorithms are considered : Average PMI metric ( APMI ) , Average MDL metric ( AMDL ) , Average Total Mutual Information ( ATMI ) , Average Number of Error Arcs ( ANE ) , Average Number of Generations performed before the best structure is found ( ANG ) , Times of Finding Original structure ( TFO ) We give the result of ALARM databases in table 1 . From the table we can see that the ANG for PMI EA is significantly smaller than that of MDL GA , and the ANE , AMDL for PMI EA are smaller than those of the MDLGA . This indicates that PMI EA is capable of finding a better Bayesian network structure from the given data set . On the other hand , for the algorithm PMI EA , we observe from the table that when use reproduction operators with a lower rate ( 0.1 ) the ANG is smaller than the ones that only use mutation the introduction of reproduction improves the convergence speed of the algorithm . operators . This indicates that
Table 5 . Comparison result for ALARM database
AMI
TFO
ANE 10.3 2.6 4.3 3.1 4.7
ANG 2102 3404 3260 2573 3081
17.01948 17.52483 17.48529 17.52091 17.46157 [ 2 ] Jordan , MI Learning in Graphical Models . MIT Press ,
1 12 7 11 9
1999
[ 3 ] Kullback , S . and Leibler , RA On Information and Sufficiency . Annals of Mathematic Statistics . 22:79~86 , 1951 [ 4 ] Schwarz , G . Estimating the Dimension of a Model . Annals of Statistics , 7(2):461~464 , 1978
[ 5 ] Larranaga , P . et al . Searching for the best ordering n the structure learning of Bayesian networks . IEEE Transactions on Systems , Man and Cybernetics , 26c(4)487~493 , 1996
[ 6 ] Wong , ML Lam , W . and Leung , KS Using Evolutionary Computation and Minimum Description Length Principle for Data mining of Probabilistic Knowledge . IEEE Pattern Analysis and Machine Intelligence , 21(2 ) , 174~178 , 1999 .
[ 7 ] Wallace , C . , Korb , KB and Dai , H . Causal Discovery via MML . Proceedings of the 13th International Conference on Machine learning ( ICML’96 ) , San Francisco : Morgan Kaufmann Publishers , 516~524 , 1996
MDL GA
PMI EA ( Pc=0.1 Pm=0.2 ) PMI EA ( Pc=0 Pm=0.2 ) PMI EA ( Pc=0.1 Pm=0.5 ) PMI EA ( Pc=0 Pm=0.5 )
APMI
16.73291 17.23317 17.21954 17.23116 17.20188
AMDL
1384269.41 1357988.28 1365633.73 1362748.25 1365975.11
The experimental results are very encouraging , which indicates that we are on the right way towards automated structure learning of Bayesian network . Clearly , what we have accomplished so far , is preliminary . In the future , we intend to incorporate other kinds of prior knowledge to increase the efficiency , as well as incorporate possibility theory to reduce the learning complexity especially when missing values exist .
Acknowledgements Related work is supported by
Natural Sciences Foundation of China 69873031 .
References
[ 1 ] Pearl , J . Probabilistic Reasoning in Intelligent Systems .
Morgan Kaufmann Publishers , 1988
Proceedings of the 2001 IEEE International Conference on Data Mining ( ICDM(cid:146)01 ) 0 7695 1119 8/01 $17.00 ' 2001 IEEE
