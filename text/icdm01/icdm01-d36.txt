A Synchronization Based Algorithm for Discovering Ellipsoidal Clusters in
Large Datasets
Hichem Frigui
Dept . of Electrical & Computer Eng .
University of Memphis hfrigui@memphis.edu
Mohamed Ben Hadj Rhouma Department of Mathematics
Georgia Institute of Technology rhouma@mathgatechedu
Abstract
This paper introduces a new scalable approach to clustering based on synchronization of pulse coupled oscillators . Each data point is represented by an integrate and fire oscillator , and the interaction between oscillators is defined according to the relative similarity between the points . The set of oscillators will self organize into stable phase locked subgroups . Our approach proceeds by loading only a subset of the data and allowing it to self organize . Groups of synchronized oscillators are then summarized and purged from memory . We show that our method is robust , scales linearly , and can determine the number of clusters . The proposed approach is empirically evaluated with several synthetic data sets and is used to segment large color images .
1 . Introduction
Clustering is an effective technique for exploratory data analysis , and has been studied for several years . It has found applications in a wide variety of areas such as pattern recognition , data mining , and statistical data analysis . Most existing methods can be categorized into the following categories : partitioning methods , hierarchical methods , and locality based methods . Partitional clusters generate a partition of the data such that objects in a cluster are more similar to each other than they are to objects in other clusters . The k Means[25 ] , EM[10 ] , and k medoids[19 ] are examples of partitional methods . Fuzzy partitional algorithms , such as the Fuzzy C Means[3 ] , generate a “ fuzzy partition ” where objects can belong to more than one cluster with different degrees . Hierarchical clustering procedures yield a nested sequence of partitions that corresponds to a graphical representation known as the dendrogram . Hierarchical procedures can be either agglomerative or divisive . Locality based methods group objects based on local relationships . Some locality based algorithms are density based , while others assume a random distribution .
Recently , the advent of World Wide Web search engines , the problem of organizing massive multimedia databases , and the concept of “ data mining ” large databases has lead to renewal of interest in clustering and the development of new algorithms[17 ] . Some of these methods are evolutionary and introduce enhancements and combination of traditional methods , others are revolutionary and introduce new concepts . In this paper , we describe a new approach that can cluster very large data sets while operating within a limited memory ( RAM ) buffer . The proposed algorithm is based on the synchronization of pulse coupled oscillators , and is an extension of our recently introduced clustering algorithm , called Self Organization of Oscillators Network ( SOON)[24 ] , to efficiently cluster huge data sets .
The organization of the rest of the paper is as follows . In section 2 , we briefly review related work . In section 3 , we describe SOON , and in section 4 , we instantiate it for the case of hype ellipsoidal clusters . In section 5 , we extend SOON to handle very large data sets . In section 6 , we evaluate the performance of the proposed algorithm . Finally , section 7 contains the conclusions .
2 . Related Work
Recently , few algorithms that can cluster large data sets have been proposed . In [ 19 ] , Kaufman and Rousseeuw proposed CLARA , which is based on finding k representative objects that minimize the sum of the within cluster dissimilarities . Ng and Han [ 23 ] proposed a variation of CLARA called CLARANS , that makes the search for the k medoids more efficient . CLARANS may require several passes over the database making the run time cost prohibitive for very large databases . The ScaleKM[4 ] and ScaleEM[5 ] are two other scalable partitional algorithms . These algorithms are based on loading only a subset of the data into the main memory , then using the K Means [ 25 ] or EM [ 10 ] to partition it . Both ScaleKM and ScaleEM inherit the drawbacks
Proceedings of the 2001 IEEE International Conference on Data Mining ( ICDM(cid:146)01 ) 0 7695 1119 8/01 $17.00 ' 2001 IEEE of K Means and EM algorithms , namely , sensitivity to noise and initial model parameters .
DBSCAN [ 12 ] is a density based algorithm that was developed to identify arbitrarily shaped clusters . DBSCAN is not suitable for high dimensional data since the intrinsic structure of all clusters cannot be characterized by global density parameters . Moreover , because DBSCAN is density based , random sampling may not be used to scale it . Other density based algorithms include DBCLASD[32 ] , which assumes that points within a cluster are uniformly distributed , STING[30 ] , an enhancement of DBSCAN , WaveCluster[26 ] , a method based on wavelets , and DENCLUE[16 ] which uses influence functions to model the points density .
CLIQUE[1 ] is a region grouping algorithm that can find clusters embedded in subspaces of the original high dimensional data space . In the first step , CLIQUE identifies subspaces that contain clusters . Then , it identifies the clusters and generates their description . The clusters generated by CLIQUE depend on two parameters which are difficult to determine : the number of intervals in every dimension ( ) , and the threshold density in each unit ( ) . Moreover , CLIQUE is scalable with respect to the number of records , but not with respect to the number of attributes . Finally , like most region growing approaches , CLIQUE cannot handle overlapping clusters .
BIRCH[33 ] and CURE[15 ] are two hierarchical algorithms that use region grouping techniques . BIRCH first performs a pre clustering phase in which dense regions are identified and represented by compact summaries . Second , it treats each of the sub cluster summaries as representative points and applies an agglomerative hierarchical clustering . Similar to CLARANS , BIRCH works satisfactory only when the clusters can be represented well by separated hyper spheres of similar sizes , and when the clusters are spanned in the whole space . CURE represents each cluster by a given number of ” well scattered ” points to capture its shape and extent . During the merging step , the chosen scattered points are ” shrunk ” towards the centroid of the cluster , and the clusters with the closest pair of representative points are merged . The multiple representative points allow CURE to recognize non spherical clusters , and the shrinking process reduces the effect of outliers . To handle large databases , CURE employs a combination of random sampling and partitioning .
3 . Synchronization of Coupled Oscillators
31 Background
Mutual synchronization of coupled oscillators is a widespread phenomenon that manifests itself in many fields [ 21 , 22 , 31 ] . One of the most cited examples is that of the southeastern fireflies that gather in large numbers on trees . First , these insects start flashing in random order . Then , they self organize and start flashing in total synchrony [ 7 ] . A characteristic feature of a population of biological oscillators is that they interact with each other by firing sudden impulses . For example , fireflies communicate through light flashes . The mathematical analysis of the details of such interactions is a complex task . An alternative approach is to neglect the details of the shape of the oscillations and model the population by a set of identical Integrate and Fire ( IF ) oscillators [ 2 ] . In this case , each oscillator is characterized by a state variable , which is assumed to be monotonically increasing toward a threshold . When this threshold is reached , the oscillator fires a pulse to the other oscillators , jumps back to a basal level , and a new period begins .
The dynamics of a population of N coupled oscillators has been investigated by several researchers . It has been shown that a population of N oscillators can exhibit a chaotic behavior , synchronize , or self organize into phaselocked sub groups . The interest in self organization behavior has increased considerably since evidence that this technique is used by the visual cortex in the mammalian brain have been discovered [ 11 ] . Strogatz and Mirollo [ 22 ] proved that a population of N identical concave down IF oscillators with constant excitatory coupling synchronizes for almost all initial conditions . The effect of delay , inhibitory coupling , and local coupling on similar IF models have been examined by several authors[6 , 29 , 18 ] .
In [ 24 ] , we introduced a model that combines synchronization and clustering concepts and resulted in an efficient and robust clustering approach . In addition to helping the model self organize into stable structured groups , the synergy between clustering and synchronization reduces the computational complexity significantly . This is because the number of competing oscillators shrinks progressively as synchronized oscillators get summarized by a single oscillator . A brief description of this model is presented in the following section .
32 Self Organization of Oscillators Network
Let Y = fyjjj = ; ; N g be a set of N objects , where each object , yj Rp . We represent each object ( yj ) by an oscillator ( Oj ) which is characterized by a phase variable ( cid:30)j and a state variable xj , which evolves according to : xj = f ( (cid:30)j ) = b lnh + ( eb , )(cid:30)ji :
( 1 ) In ( 1 ) , b is a constant that measures the extent to which f is concave down . The function f plays the role of an amplitude function , while ( cid:30)i [   ; ] represents the phase , which in case of no coupling with other oscillators corresponds to the normalized time elapsed since the last firing of Oi . Whenever xi reaches a threshold at xi= , the ith oscillator fires ( ie , excites/inhibits other oscillators ) and ( cid:30)i and
Proceedings of the 2001 IEEE International Conference on Data Mining ( ICDM(cid:146)01 ) 0 7695 1119 8/01 $17.00 ' 2001 IEEE xi are instantaneously reset to zero , after which the cycle repeats . As a consequence , the state variables of all other oscillators Oj ( j = i ) will change by an amount ffli((cid:30)j ) , ie , xj(t+ ) = xj(t ) + ffli((cid:30)j ) ;
( 2 ) where ffli((cid:30)j ) is a coupling function that is positive if Oi and Oj are similar , and negative otherwise . We use
1
2
3
4
5
6
7
8
9
1.0
0.8
0.6
0.4
0.2
) φ ( f
18
7
16
13
14
1
10
11
12
13
14
15
16
17
18
8
15
3
9
12
17
5
2
11
4
10
1.0
0.8
0.6
0.4
0.2
) φ ( f
1
4,7 4,7
6
16 2,3,5 2,3,5 2,3,5
10,11,13 10,11,13 10,11,13 8,98,9
12,14,15,17,18 12,14,15,17,18 12,14,15,17,18 12,14,15,17,18 12,14,15,17,18
6
0.0
0.0
0.2
0.4
φ
0.6
0.8
1.0
0.0
0.0
0.2
0.4
φ
0.6
0.8
1.0
( b )
( c )
( a )
1.0
0.8
0.6
0.4
0.2
) φ ( f
1,4,7 1,4,7 1,4,7
2,3,5,6,8,9 2,3,5,6,8,9 2,3,5,6,8,9 2,3,5,6,8,9 2,3,5,6,8,9 2,3,5,6,8,9
10,11,13 10,11,13 10,11,13
1.0
0.8
0.6
0.4
0.2
) φ ( f
1,2,,9 1,2,,9 1,2,,9 1,2,,9 1,2,,9 1,2,,9 1,2,,9 1,2,,9 1,2,,9
10,11,,18 10,11,,18 10,11,,18 10,11,,18 10,11,,18 10,11,,18 10,11,,18 10,11,,18 10,11,,18
12,14,15,16,17,18 12,14,15,16,17,18 12,14,15,16,17,18 12,14,15,16,17,18 12,14,15,16,17,18 12,14,15,16,17,18 0.2
0.0
0.0
0.6
0.8
1.0
0.4
φ
( d )
0.0
0.0
0.2
0.4
φ
0.6
0.8
1.0
( e ) ffli((cid:30)j ) = ij =ffi fi
,ffi 
>< CE . , d ,CI . d ffi ,ffi  fi > :
,CI ij ij ffi  ffi  < d if d if otherwise ij ffi
( 3 ) dij is the dissimilarity between Oi and Oj , CE and CI ( typically [  :  ;   : ] ) are the maximum excitatory and inhibitory coupling . Eq ( 3 ) states that a firing oscillator pulls similar ones closer by an amount proportional to their degree of similarity , and pushes non similar oscillators farther by an amount proportional to the degree of dissimilarity . ffi  can be regarded as a resolution parameter , and ffi ( typically ffi = .ffi  ) is a constant that is used to indicate that if an oscillator is too far , then it should simply be maximally inhibited . The SOON algorithm is summarized below . The mathematical analysis and a proof of convergence for the case of two oscillators can be found in [ 24 ] .
Self Organization of Oscillators Network ( SOON )
Select a dissimilarity measure d( ) ; Initialize phases ( cid:30)i randomly for i = ; ; N ; Repeat
Identify next oscillator to fire=fOi : ( cid:30)i = maxN Compute dij Bring ( cid:30)i to threshold , and adjust other phases : for j = N , j = i ; j= ( cid:30)jg ;
( cid:30)j = ( cid:30)j + ( , ( cid:30)i ) for j = ; ; N ;
For all oscillators Oj ( j = i ) Do
Compute state variable xj = f ( (cid:30)j ) using ( 1 ) ; Compute coupling ffli((cid:30)j ) using ( 3 ) ; Adjust state variables using ( 2 ) ; Compute new phases using ( cid:30)j(t+ ) = f , ,xj(t+) ;
Identify synchronized oscillators ; Update the parameters of the synchronized group ; Reset phases of oscillators that synchronized in this iteration ;
Until ( Synchronized groups stabilize ) ;
Fig 1 illustrates the evolution of the phases for the objects of a simple 2 D data set . The L norm is used and ffi  was set to   : ( the choice of ffi  will be discussed later ) . Fig 1(b ) displays the initial random phases . Fig 1(c ) , which displays the state of the system after 10 iterations , 5 groups have formed : G =fy ; y ; y ; y ; y g , G =fy ; y g , G =fy   ; y ; y g , G =fy ; y ; y g , and G =fy ; y g . Oscillators 1 , 6 , and 16 are not assigned to any group yet . As the system evolves further , new groups keep forming , and existing groups keep getting bigger by bringing other oscillators to threshold along with them , and
Figure 1 . Evolution of SOON . ( a)Objects in the feature space . ( b)Initial phases . Phases after ( b ) 10 , ( c ) 20 , and ( d ) 25 iterations . absorbing them . Moreover , a group can also bring another group to threshold , and the two groups will be merged into one . After a total of 25 iterations , only two groups ( the 2 actual clusters ) are present . The iteration process is stopped when the constant phase difference is detected .
33 Discussion
At first glance , SOON seems reminiscent of the standard Hierarchical Agglomerative Clustering ( HAC ) [ 27 ] and other neural networks based clustering algorithms such as VQ[14 ] , SOM[20 ] , and ART[8 ] . However , there are two major properties that distinguish SOON . First , SOON provides an efficient selection mechanism for candidate groups to be updated in every iteration . Most similar algorithms will either select all points in a sequential or in a random order . These simple selection schemes are inefficient since they do not consider the distribution of the data . The selection process in SOON is data driven and relies on the historical behavior ( accumulated in the phase ) . For instance , objects that are similar and are expected to belong to the same cluster will excite each other often , and thus will be selected ( ie , reach the threshold and fire ) more frequently . On the other hand , noise objects will get inhibited very often and will rarely reach the threshold .
The second property that distinguishes SOON is that the decision to merge objects is not based on a simple thresholding of the inter point distances . In SOON , points are merged based on their phase values which are accumulated over several iterations ( since CE<< ) . This property prevents the “ bridging effect ” which is a common drawback
Proceedings of the 2001 IEEE International Conference on Data Mining ( ICDM(cid:146)01 ) 0 7695 1119 8/01 $17.00 ' 2001 IEEE of hierarchical clustering . Fig 2 illustrates this property . Let ffi be the threshold used in HAC to decide if two points should be merged . If d(x ; x )<ffi , then points from the two clusters can be lumped in the same cluster . On the other hand , if ffi<d , then no groups can form . SOON does not suffer from this drawback . This is because the decision to merge two points depends not only on the distance between them , but also on the behavior of the neighboring points in the previous iterations . For instance , if d is less than ffi , then x will receive excitatory pulses from x and x , and inhibitory pulses from all the other points . On the other hand , x will receive excitatory pulses from x and most of the points in the left cluster . Thus , it is expected that after a few iterations the states of x , x and x will be different ( even if they had similar initial values ) , and therefore , it is unlikely for these points to be lumped together . within cluster distances have a probability distribution with p ( dimensionality of the data ) degrees of freedom . This desirable feature will ( i ) automate the choice of ffi  ; and ( ii ) make the neighborhood of the excitatory region dynamic and cluster dependent . We use ffi = ( ff ; p ) , that is , the ffth percentile of the probability distribution . In other words , if the probability that a given point belongs to the firing cluster is greater than ( ,ff ) , then its phase would be incremented . We use ff = % .
Initially , each data point is represented by one oscillator and constitute a cluster by itself . The initial center of the oscillator ( ie , cluster ) is the point itself , and the covariance matrix is initialized to ffl Ip.p , where Ip.p is the identity matrix , and ffl is a constant that depends on the dynamic range of the data . The center and the covariance matrix of each cluster will be updated using the features of the synchronized oscillators . d
X2
X1
X3
Figure 2 . Bridging Effect of HAC
The SOON clustering framework is generic and can be used to cluster feature vectors composed of numerical as well as categorical attributes . Moreover , it can incorporate a multitude of similarity measures including subjective and non metric ones . In this paper , we instantiate and numerically illustrate SOON for the case of hyper ellipsoidal clusters . We also introduce an incremental version of SOON to cluster very large data that cannot be fully loaded into the main memory .
4 . Identification of Hyper Ellipsoidal clusters
We assume that each cluster , k , has a multivariate normal distribution with mean ck and covariance matrix Ck . The synchronization process involves computing the distance between two oscillators , where each oscillator represents a cluster . We use a modified Mahalanobis distance : ij = min((ci ,cj)T C , d i
( ci ,cj ) ; ( ci ,cj)T C , j ( ci ,cj) ) ;
( 4 )
( a )
( b )
( c )
Figure 3 . Detection of ellipsoidal clusters . Results at the end of ( a ) 5 , ( b ) 500 , and ( c ) 1200 iterations .
( a )
( b )
( c )
Figure 4 . Detection of ellipsoidal clusters in a noisy data set . Results at the end of ( a ) 5 , ( b ) 500 , and ( c ) 1700 iterations . where ci ; cj ; Ci ; Cj are the centers and covariance matrces of oscillators ( clusters ) Oi and Oj . By using a different covariance matrix for each cluster , the above distance can be used to detect ellipsoidal clusters of various shapes and orientations . The shape of each cluster k is generated by the eigenstructure of the matrix Ck . If the clusters are assumed to come from a multivariate Gaussian distribution , then the
Fig 3 illustrates the evolution of SOON using a 2 D synthetic Gaussian mixture . Fig 3(a ) displays the results after 5 iterations where 4 small groups have formed . The center of each group is indicated by the “ + “ sign and the ellipses enclose points within the ffi  neighborhood . The remaining points indicate oscillators that did not synchronize yet . Fig 3(b ) displays the results after 500 iterations , where most os
Proceedings of the 2001 IEEE International Conference on Data Mining ( ICDM(cid:146)01 ) 0 7695 1119 8/01 $17.00 ' 2001 IEEE cillators have synchronized and formed groups of various sizes . Fig 3(c ) shows the result after 1200 iterations where the remaining groups are phase locked . Fig 4 illustrates the robustness of SOON to noise . As can be seen , noise points will either form very small clusters or will not synchronize at all . This is because noise points are located in non dense regions and they get inhibited by most of the other points . Even if these points reach the threshold , they will excite a few if any other oscillators .
The previous two examples might indicate that SOON is not efficient since it takes more than 1500 iterations to obtain the final partition , and that a simple K means or EM algorithm can converge in less than few hundred iterations . However , this is not the case . Unlike the K means and EM which compute the distance between all points and all clusters in every iteration , SOON computes the distance to only one of the clusters ( the one that fires ) in every iteration . Thus , to provide a fair comparison , the number of iterations in SOON should be divided by the number of clusters . Moreover , K means and EM would need many more iterations if the number of clusters is not known , and the computation in each iteration gets more complex for k means type algorithms that are robust to noise [ 13 ] .
To provide a quantitative indication of the efficiency of our approach , we compare the running time with two other algorithms . The first algorithm is the Fuzzy C Means ( FCM ) [ 3 ] using the Mahalanobis distance which is similar to the EM algorithm . The FCM is very simple , however , it cannot handle noisy data , and requires the specification of the number of clusters . The second algorithm is the Robust Competitive Agglomeration ( RCA ) [ 13 ] . The RCA can cluster noisy data and find the optimal number of clusters . It only requires the specification of an over specified number of clusters ( Cmax ) . Table 1 displays the average CPU time of the three algorithms over several runs using the data sets in Fig 3 and Fig 4 . We should note here that the FCM is sensitive to the initial parameters , and that there are few instances were the small cluster was not detected . Both the RCA and SOON are not as sensitive because they start with a much larger number of initial prototypes . Notice that for the data in Fig 4 , the FCM cannot generate a meaningful partition , and thus the CPU time is not recorded .
Table 1 . CPU time of 3 clustering algorithms
Image
Fig 3 Fig 4
Algorithm
FCM ( C=6 ) 2 Sec *****
RCA
RCA
SOON
( Cmax=25 )
( Cmax=50 )
6 Sec 7 Sec
10 Sec 11 Sec
3 Sec 4 Sec
5
Incremental Clustering using SOON
During the evolution of SOON , once a group of oscillators synchronize , they will share the same center and covariance matrix . Thus , a group of synchronized oscillators can be treated as a single oscillator . Our goal is to exploit this fact , and design an algorithm that can efficiently cluster huge data sets while operating within a limited memory ( RAM ) buffer in one scan of the data set . Our approach , called ScaleSOON , is outlined as follows :
1 . Get a sample from the data set , and fill the memory buffer .
2 . Apply the algorithm to the data contents in the buffer .
3 . Summarize each synchronized group by a single oscillator with equivalent sufficient statistics , and purge the synchronized oscillators from the buffer .
4 . If there are any points that have not been previously loaded , go to step 1 .
51 Data Compression and Sufficient Statistics
Let G be a group of synchronized oscillators .
If the distance defined in ( 4 ) is used , then the sufficient statistics for G are the triplet ( NG ; SG ; SSG ) , where NG is the number of oscillators in group G , SG = Py G y , and SSG = Py G yyT . Note that SSG is symmetric , and there is no need to store the entire matrix . Moreover , when the dimensionality of the feature space is large , it is common in practice to assume diagonal covariance matrices . In this case , the shape of SSG reduces to a vector . Initially , each feature vector ( or oscillator ) yj has its initial sufficient statistics , ie , ( Nyj ; Syj ; SSyj ) = ( ; yj ; yj yT j ) .
52 Purging and Filling the Memory Buffer
When the data set resident in the memory buffer reaches a stable state , each group of synchronized oscillators , Gk , will be compressed as follows :
1 . Summarization : Add a new oscillator with sufficient statistics :
>< NGk = Py Gk = Py Gk SGk > : SSGk = Py Gk
Ny Sy SSy :
( 5 )
2 . Purging : Purge the oscillators that belong to Gk from the memory buffer .
Proceedings of the 2001 IEEE International Conference on Data Mining ( ICDM(cid:146)01 ) 0 7695 1119 8/01 $17.00 ' 2001 IEEE
In ( 5 ) , y can represent a single oscillator or a group of oscillators that have been synchronized and summarized in previous iterations . After summarizing and purging all of the synchronized groups , the memory buffer is filled with feature vectors that have not been previously loaded . Since the features of the purged oscillators are not available in the main memory , the center and covariance matrix should be updated using the sufficient statistics of the group they belong to . That is , we use ck =
NGk
SGk and Ck =
NGk
SSGk , ckcT k the remainder of this section , we will assume that the number of clusters is always much smaller than 500 , and let the buffer size be fixed to 500 oscillators .
1400
1000
) c e S
( i e m T g n n n u R i
500
100 50
50
100
250
500
1K
Buffer Size ( Records )
2.5K
5K
10K
6 . Experimental Results
Figure 5 . Effect of the buffer size .
The performance of ScaleSOON is evaluated with several data sets . We demonstrate the scalability of ScaleSOON with respect to the number of records and the number of attributes using several synthetically generated data sets . All experiments were performed on an Ultra Sparc IIi 300 Mhz workstation with 256 MB RAM . Several data sets were generated by sampling from k=   multivariate Gaussian distributions . Gaussian means and diagonal covariance matrices were chosen uniformly on [   ;   ] , and [   : ; : ] respectively . The number of attributes varied from 10 to 100 , and the number of records varied from 10,000 to 1,000,000 .
61 Effect of the Buffer Size
Initially , ScaleSOON was designed to cluster very large data sets that cannot be entirely loaded into the main memory , and we planned on setting the buffer size to be close to the size of the main memory . However , after running ScaleSOON on several data sets with different buffer sizes , we have discovered that it runs faster with smaller buffer sizes . Fig 5 shows the running time for an increasing sequence of buffer sizes varying from 100 to 10,000 . The data used in this experiment has 25 attributes and K records . There are two reasons that explain this behavior . First , the algorithm exhibits temporal locality and the frequency of cache misses would be minimized when the buffer size is less than the cache size . Second , we have noticed that ScaleSOON reaches a stable state in fewer iterations when the buffer size is smaller .
When the buffer size becomes very small , the running time does not decrease any further . This is probably due to the more frequent accesses to the secondary storage , and also because not all of the available cache memory is used . In addition to inefficient use of cache memory , very small buffer sizes cannot maintain a variety of sub groups , and may lead to an unsatisfactory partition . Ideally , the buffer size should be much larger than the expected number of clusters to allow a smooth hierarchical agglomeration . In
62 Scalablity
To illustrate the scalability of ScaleSOON with respect to the number of records , we use data sets that have 25 attributes , 10 clusters , and 10K , 100K , 300K , 500K , and 1M records . For all data sets , ScaleSOON found 10 clusters and identified their parameters correctly . Fig 6(a ) displays the running time versus the number of records . As expected , ScaleSOON scales linearly . To illustrate the scalability with respect to the number of attributes , we use data sets that have 10 clusters , 500K records , and 10 , 25 , 50 , and 100 attributes . For all data sets , ScaleSOON found 10 clusters and identified their parameters correctly . Fig 6(b ) displays the running time versus the number of records . As can be seen , ScaleSOON scales linearly with respect to the number of attributes .
1200
900
600
300
100
1750
1000
500
) c e S
( i e m T g n n n u R i
) c e S
( i e m T g n n n u R i
0
10K 100K
300K
500K
Number of Records
1M
100
10
25
50
Number of Attributes
100
( a )
( b )
Figure 6 . Scalability of ScaleSOON : ( a ) with respect to the number of records , and ( b ) with respect to the number of attributes .
63 Effect of the Initial Phases
ScaleSOON assigns a random number ( initial phase ) to each record when it gets loaded . To study the sensitivity of the algorithm with respect to these random phases , we ran ScaleSOON on the same data set with several different
Proceedings of the 2001 IEEE International Conference on Data Mining ( ICDM(cid:146)01 ) 0 7695 1119 8/01 $17.00 ' 2001 IEEE initial phases ( using different seed values ) . The data used in this experiment has 25 attributes , 100K records , and 10 clusters . Fig 7 ( a ) displays the total distance between the true Gaussian means and the means of the identified clusters for the 10 different initializations . As can be seen , the total distance does not exceed 0.11 , which means that for all 10 initialization , ScaleSOON was able to identify the 10 correct clusters . Moreover , As shown in Fig 7(b ) , the variation in the run time for these 10 different runs is small ( less than % ) . This indicates that the total number of iterations required for the algorithm to reach a stable state is not sensitive to the initial phases .
0.15
0.10
98
) c e S
(
96 i s n a e m n a s s u a G e u r t o t e c n a t s D i
0.05
0.00
1
2
3
4
5
6
Seed Number i e m T g n n n u R i
94
92
90
1
2
3
4
7
8
9
10
5
6
Seed Number
( a )
( b )
Figure 8 . Color image segmentation . ( a ) Original images , ( b ) Edges of detected objects
7
8
9
10
( a )
( b )
Figure 7 . Effect of the Initial phases . ( a ) Total Euclidean distance from the true Gaussian means . ( b ) Running time .
64 Applications to Color Image Segmentation
ScaleSOON has been used to segment large color images . Each image contains . pixels , and each pixel is mapped to an 8 D vector consisting of 3 color , 3 texture , and 2 position features [ 9 ] . Typically , when using a clustering algorithm to segment large images , sampling is used to reduce the number of pixels . Unfortunately , sampling may result in the loss of small and/or thin objects . Fig 8(a ) shows 2 color images from the Corel image database . ScaleSOON was applied to each image ( without sampling ) to segment it into several homogeneous regions by clustering the feature vectors mapped from the image pixels . Fig 8(b ) shows the edges of the clustered objects .
7 . Conclusions
We have introduced a scalable clustering framework by combining clustering techniques with the dynamics of simple and discrete Integrate and Fire oscillators . The proposed approach , ScaleSOON , associates one oscillator with each data point and defines the interaction between oscillators according to the relative similarity between the points . As a result , ScaleSOON reaches a stable state where similar oscillators synchronize and dissimilar ones phase lock .
ScaleSOON achieves scalability by loading only a subset of the data into main memory at a time , and allowing it to self organize . Each group of oscillators is summarized by a single oscillators with sufficient statistics and are purged from memory . New data points are then loaded into memory and self organize with the existing groups . Our empirical evaluation has shown that ScaleSOON scales linearly with respect to the number of records and the number of attributes . Our experiments have also indicated that even if large memory is available to hold the entire data , it is more efficient to process it incrementally . This is because the problem is much simpler when fewer oscillators are interacting , and also the cache memory is used more efficiently . ScaleSOON does not require the specification of the actual number of clusters . This number is determined automatically depending on the resolution parameter ffi  . We have shown that using the Mahalanobis distance , ffi  can be fixed by exploiting the probability distribution of the distances in each cluster . Thus , each cluster can have its own adaptive resolution . The synchronization mechanism of ScaleSOON makes it robust to noise and outliers . Oscillators that represent noise points will synchronize with few if any other oscillators . Thus , they can be simply identified as those points that did not synchronize , or that belong to very small groups .
In this paper , we have instantiated our clustering framework using the Mahalanobis distance . This , however , does not constitute a limitation of our method . In fact , since ScaleSOON does not explicitly optimize an objective function , it can incorporate non metric similarity measures . Moreover , it can be easily adapted to cluster discrete valued measurements . This represents a very desirable features in
Proceedings of the 2001 IEEE International Conference on Data Mining ( ICDM(cid:146)01 ) 0 7695 1119 8/01 $17.00 ' 2001 IEEE many data mining applications where discrete data is very common and the most appropriate dissimilarity measures tend to be rather subjective , and thus non metric .
Acknowledgment
Partial support of this research was provided by the Woodrow W . Everett , Jr . SCE EE Development Fund in cooperation with the Southeastern association of electrical engineering department heads .
References
[ 1 ] R . Agrawal , J . Gehrke , D . Gunopulos , and P . Raghavan . Automatic subspace clustering of high dimensional data for data mining aplications . In Proc . of the ACM SIGMOD , 1999 .
[ 2 ] J . B´elair . Periodic pulsatile stimulation of a nonlinear oscil lator . J . Math . Biol , 24:74–85 , 1988 .
[ 3 ] J . C . Bezdek . Pattern Recognition with Fuzzy Objective
Function Algorithms . Plenum Press , New York , 1981 .
[ 4 ] P . Bradley , U . Fayyad , and C . Reina . Scaling clustering algorithms to large databases . In Proc . of the 4th Int . conf . on Knowledge Discovery and Data Mining , 1998 .
[ 5 ] P . Bradley , U . Fayyad , and C . Reina .
Scaling EM ( Expectation Maximization ) clustering to large databases . Technical Report MSR TR 98 35 , Microsoft Research , 1998 .
[ 6 ] B . Bressloff and S . Coombes . Symmetry and phase locking in a ring of pulse coupled oscillators with distributed delays . Physica D , 126:99–122 , 1999 .
[ 7 ] J . Buck and E . Buck . Synchronous fireflies . Scientific Amer ican , 234:74–85 , 1976 .
[ 8 ] G . Carpenter and S . Grossberg . Adaptive resonance theory ( art ) . In M . Arbib , editor , Handbook of Brain Theory and Neural Networks , pages 79–82 . MIT Press , 1996 .
[ 9 ] C . Carson , S . Belongie , H . Greenspan , and J . Malik . Colorand texture based image segmentation usin em and its application to image querying and classification . Submitted to : IEEE Trans . on Pattern Analysis and Machine Intelligence . [ 10 ] A . P . Dempster , N . M . Laird , and D . B . Rubin . Maximum likelihood from incomplete data via the EM algorithm . Journal of the Royal Statistical Society Series B , 39(1):1–38 , 1977 .
[ 11 ] R . Eckhorn , R . Bauer , W . Jordan , M . Brosch , W . Kruse , M . Munk , and H . J . Reitboeck . Coherent oscillations : a mechanism of feature linking in the visual cortex ? Biol . Cybern . , 60:121–130 , 1988 .
[ 12 ] M . Ester , H . P . Kriegel , J . Sander , and X . Xu . A densitybased algorithm for discovering clusters in large spatial databases with noise . In Proc . of the 2nd international conf . on Knowledge Discovery and Data Mining ( KDD96 ) , Portland Oregon , 1996 .
[ 13 ] H . Frigui and R . Krishnapuram . A robust competitive clustering algorithm with applications in computer vision . IEEE Trans . Patt . Analysis Mach . Intell . , 21(5):450–465 , 1999 .
[ 14 ] A . Gersho and R . M . Gray . Vector Quantization and Signal
Compression . Kluwer Academic Publishers , 1992 .
[ 15 ] S . Guha , R . Rastogi , and K . Shim . CURE : An efficient clustering algorithm for large data databases . In Proc . of the ACM SIGMOD conference on Management of Data , Seattle Washington , 1998 .
[ 16 ] A . Hinneburg and D . Keim . An efficient approach to clustering in large multimedia databases with noise . In Proc . of the 4th int . conf . on Knowledge Discovery and Data Mining , pages 58–65 , New York , NY , 1998 .
[ 17 ] A . Hinneburg and D . Keim . Clustering techniques for large data sets : From the past to the future . In Tutorial Notes for ACM SIGKDD int . conf . on Knowledge Discovery and Data Mining , pages 58–65 , New York , NY , 1999 .
[ 18 ] D . Horn and I . Opher . Collective excitaion phenomena and their applications . In W . Maass and C . M . Bishop , editors , Pulsed Neural Networks , pages 297–320 . MIT Press , 1999 . [ 19 ] L . Kaufman and P . J . Rousseeuw . Finding Groups in Data : An Introduction to Cluster Analysis . Addison Wesley , NEW York , 1990 .
[ 20 ] T . Kohonen . Self Organization and Associative Memory .
Springer Verlag , 1989 .
[ 21 ] Y . Kuramoto . Chemical Oscillations , Waves and Turbu lence . Springer , Berlin , 1984 .
[ 22 ] R . Mirollo and S . Strogatz . Synchronization of pulse coupled biological oscillators . SIAM J . Appl . Math , 50:1645– 1662 , 1990 .
[ 23 ] R . T . Ng and J . Han . Efficient and effective clustering methods for spatial data mining . In Proc . of the VLDB conference , Santiago Chile , 1994 .
[ 24 ] M . Rhouma and H . Frigui . Self organization of a population of coupled oscillators with application to clustering . IEEE Trans . Patt . Analysis Mach . Intell . , 23(2 ) , Feb . 2001 .
[ 25 ] RODuda and P . E . Hart . Pattern Classification and Scene
Analysis . John Wiley and Sons , 1973 .
[ 26 ] G . Sheikholeslami , S . Chatterjee , and A . Zhang . Wavecluster : A multi resolution clustering approach for very large spatial databases . In Proc . of the 24th Very Large Databases Conf . , pages 428–439 , New York , NY , 1998 .
[ 27 ] P . H . A . Sneath and R . R . Sokal . Numerical Taxonomy The Principles and Practices of Numerical Classification . W . H . Freeman , San Francisco , CA , 1973 .
[ 28 ] D . Terman and D . Wang . Global competition and local cooperation in a network of neural oscillators . Physica D . , 81:148–176 , 1995 .
[ 29 ] W . Wei , J . Yang , and R . Muntz . Sting : A statistical infromation grid approach to spatial data mining . In Proc . of the 23rd Very Large Databases Conf . , pages 186–195 , Athens , Greece , 1997 .
[ 30 ] A . T . Winfree . Biological rhythms and the behavior of populations of coupled oscillators . Journal Theoret . Biol . , 16:15– 42 , 1967 .
[ 31 ] X . Xiaowei , M . Ester , H . Kriegel , and J . Sander . A distribution based clustering algorithm for mining in large spatial databases . In Proc . of the 14th Int . Conf . on Data Engineering , pages 324–331 , Orlando , Florida , 1998 .
[ 32 ] T . Zhang , R . Ramakrishnan , and M . Livny . BIRCH : An efficient data clustering method for very large databases . In Proc . of the ACM SIGMOD conference on Management of Data , Montreal Canada , 1996 .
Proceedings of the 2001 IEEE International Conference on Data Mining ( ICDM(cid:146)01 ) 0 7695 1119 8/01 $17.00 ' 2001 IEEE
