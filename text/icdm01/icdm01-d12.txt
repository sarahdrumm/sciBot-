Theory Comput Syst DOI 10.1007/s00224 007 9094 6
Provably Fast Training Algorithms for Support Vector Machines
José L . Balcázar · Yang Dai · Junichi Tanaka · Osamu Watanabe
© Springer Science+Business Media , LLC 2008
Abstract Support Vector Machines are a family of algorithms for the analysis of data based on convex Quadratic Programming . We derive randomized algorithms for training SVMs , based on a variation of Random Sampling Techniques ; these have been successfully used for similar problems . We formally prove an upper bound on the expected running time which is quasilinear with respect to the number of data points and polynomial with respect to the other parameters , ie , the number of attributes and the inverse of a chosen soft margin parameter . [ This is the combined journal version of the conference papers ( Balcázar , JL et al . in Proceedings of 12th International Conference on Algorithmic Learning Theory ( ALT’01 ) , pp . 119–134 , 2001 ; Balcázar , JL et al . in Proceedings of First IEEE International Conference on
The first and the fourth authors started this research while visiting the Centre de Recerca Matemàtica of the Institute of Catalan Studies in Barcelona . The first author was supported by IST Programme of the EU under contract number IST 1999 14186 ( ALCOM FT ) , Spanish Government TIC2004 07925 C03 02 , and CIRIT 2001SGR 00252 . The second author conducted this research while she was with Department of Mathematical and Computing Sciences , Tokyo Institue of Technology , and was supported by a Grant in Aid ( C 13650444 ) from Japanese Goverment . The fourth author was supported in part by a Grant in Aid for Scientific Research on Priority Areas “ Discovery Science ” 1998–2000 from Japanese Goverment .
JL Balcázar Departament de Llenguatges i Sistemes Informàtics , Univ . Politècnica de Catalunya Campus Nord , Jordi Girona Salgado 1 3 , 08034 Barcelona , Spain
Y . Dai Department of Bioengineering ( MC063 ) , University Illinois at Chicago , 851 S . Morgan Str , Chicago , IL 60607 7052 , USA J . Tanaka · O . Watanabe ( . ) Department of Mathematical and Computing Sciences , Tokyo Institute of Technology , Meguro ku Ookayama , Tokyo 152 8552 , Japan e mail : watanabe@istitechacjp
Theory Comput Syst
Data Mining ( ICDM’01 ) , pp . 43–50 , 2001 ; and Balcázar , JL et al . in Proceedings of SIAM Workshop in Discrete Mathematics and Data Mining , pp . 19–29 , 2002) . ] Keywords Support Vector Machine · SVM training algorithm · Random sampling technique · Combinatorial dimension
1 Introduction
The Support Vector Machine ( SVM in short ) is a modern mechanism for two class classification and regression problems . Since the present form of SVM was proposed [ 15 ] , SVMs have been used in various application areas , and their classification power has been investigated in depth from both experimental and theoretical points of view [ 16 ] . An important feature is that their way of working , by identifying the socalled “ support vectors ” among the data , offers important contributions to a number of problems related to Data Mining . The purpose of this paper is to give a fast and theoretically justified algorithm—SVM training algorithm—that finds such support vectors from a huge input data set . Our algorithm is based on a variation of Random Sampling Techniques , which have been successfully used for similar problems .
The currently employed forms of SVM can be applied to nonseparable data as well , by means of the so called “ soft margin ” formulation , which will be described precisely below . In this case , outliers ( ie , erroneous data or exceptions ) can be handled in the same way , while being penalized , as the other normal input data . The influence of the outliers is adjusted by the use of the so called “ soft margin parameter ” , which is denoted as D ≤ 1 throughout this paper .
Algorithmically , the SVM training amounts to the identification of the solution of a convex quadratic programming ( QP in short ) problem . ( It was proved in [ 30 ] that a similar technique is able to help choosing an appropriate kernel . ) Though convex QP problems are polynomial time solvable , they are still not so easy . According to [ 6 ] and [ 19 ] , even the currently best method requires in the worst case TQP(n , m ) = m3/2n2 , where , in the context of the SVM training , n is the number of attributes1 and m is that of examples for a given data set . Thus , to scale up to really large data sets , the standard QP algorithms alone are inappropriate since their running times grow fast in m . Therefore , many algorithms and implementation techniques have been developed for training SVMs efficiently ; see , eg , [ 11 , 12 , 19 , 23 , 26 , 33 ] .
Among the proposed speed up techniques , the “ subset selection ” [ 33 ] has been used as an effective heuristic [ 10 ] , whose motivation is the same as our algorithm . Roughly speaking , the subset selection is a technique for the speed up of SVM training by dividing the original QP problem into small pieces , thereby reducing the size of each QP problem . Well known variations of subset selection techniques are chunking , decomposition , and sequential minimal optimization ( SMO in short ) ; see [ 15 , 16 , 27 , 29 ] for the details . In particular , variants of SMO have become popular because they outperform the others in several experiments . The performance of these active subset techniques has been extensively examined , but only a few number of
1Precisely , n here is the number of attributes plus 1 , but we ignore this additive difference here .
Theory Comput Syst theoretical analysis have been reported . The convergence of some of such algorithms has been theoretically shown in [ 22 , 25 ] ; more recently , some polynomial bounds on the number of subproblem solving steps have been obtained theoretically for the SVM light ( under a certain assumption ) [ 24 ] and for some variation of the subset selection technique [ 21 ] . In this paper , we propose another variation of the subset selection technique and prove that it converges on average within quasilinear subproblem solving steps and , in total , time quasilinear in m and polynomial in the other parameters . We also discuss the extension of our approach to the support vector regression , the regression problem under insensitive loss by means of SVM .
Our algorithm seems similar to the one proposed by Pavlov et al . [ 28 ] , which also involves a distribution of probability on the data points and samples like we will do . While our approach is quite similar to boosting , theirs is literally boosting , with the same constants and normalizations . However , boosting was designed with the purpose of improving the generalization error , whereas for the SVM training , the classifier to be found is always the same and the sampling only serves as an algorithmic means of faster computation ; this is indeed the way of using randomness in our algorithm .
We follow a soft margin SVM formulation by Bennett and Bredensteiner [ 5 ] . With this formulation , they gave a nice geometric interpretation of the soft margin parameter D ≤ 1 . This formulation is the same as the standard soft margin SVM formulation , provided that D is small enough , more technically , small enough so that nontrivial solution exists . In other words , such a small D should be used [ 5 ] in this formulation ; this criteria is in fact recommended in the context of linear programming type classification [ 7 ] . We will also show a way to determine appropriate D algorithmically . It should be also noted here that the formulation is similar to ν SVM [ 31 , 32 ] . In the ν SVM formulation , D is simply fixed to 1/m , while another parameter ν is introduced to give a weight to θ+ − θ− ( see ( P5 ) in our Sect . 2 ) ; then the choice of ν becomes important for the ν SVM formulation . See [ 5 ] for discussion on the relationship between the formulation of Bennett and Bredensteiner and the ν SVM formulation .
The purpose of our research is to provide a fast and theoretically justified SVM training algorithm . We propose the use of the random sampling techniques that have been developed and used for combinatorial optimization problems ; see , eg , [ 1 , 13 , 20 ] . It is straightforward to apply some of the random sampling techniques [ 20 ] for the determination of a maximum margin separating hyperplane for the two class classification problem , if the data set is linearly separable in the original feature space . On the other hand , the problem becomes complicated when the given data set is not linearly separable . Here the geometric interpretation of Bennett and Bredensteiner [ 5 ] is used for applying the same random sampling technique . Our algorithm solves a relatively small QP subproblem at each iteration . We prove that the expected number of these iteration steps is O(δ ln m ) , where each iteration needs O(m ) time besides solving some QP subproblem . On the other hand , the size of each QP subproblem is bounded by O(δ2 ) . Hence , the overall time bound is O((δ ln m)(m + TQP(n , δ2 ) ) = O(ln m(δm + δ4n2) ) . Here δ is a new parameter called a “ combinatorial dimension ” ; this δ is n+ 1 if no outlier exists , but it could be large if there are many outliers . Therefore , our algorithm has an advantage ( over simply solving the original QP problem ) if n fi m and δ can be bounded by O(n ) . In this sense , our algorithm as it is
Theory Comput Syst may not be practical . But we hope that our approach and analysis of the obtained algorithm could be used for theoretical investigation on the performance of the other similar algorithmic approaches .
Although the above worst case time bound is not appealing , it is still possible that our algorithm performs well even in some practical cases . Unfortunately , our proved upper bound for δ is min((n+1)/D , m ) ; but we conjecture that it is small ( ie , almost linearly bounded by n ) in some cases , in particular , after removing very bad outliers . Also although our theoretical analysis is made on some fixed QP subproblem size ( ie , O(δ2) ) , we may choose much smaller subproblems . We can guarantee that the output of our algorithm ( if it yields ) is correct no matter how δ is set and/or the subproblem size is chosen ; the only problem is that the algorithm does not terminate in the expected number of steps . Thus , by running the algorithm from some small δ and some small subproblem size , and by doubling these parameters if the algorithm does not seem to terminate , we can execute the algorithm with quite reasonable parameters . This type of experimental studies are our future work .
2 Support Vector Machines , Optimization , and Random Sampling
Here we explain basic notions on SVM and random sampling techniques . We mainly explain those necessary for our discussion . For SVM , see , eg , the textbook [ 16 ] or the survey [ 6 ] ; and for random sampling techniques , see the survey [ 20 ] .
The SVM training for the two class classification problem can be phrased as follows . Given a set of labeled examples , we have to determine a hyperplane separating positive and negative examples with the largest possible margin , ie , maximal separation from all the data points . A possible formulation for the problem is presented as follows . Suppose that we are given a set of m examples xi , 1 ≤ i ≤ m , in an n dimensional space , say Rn . Each example xi is labeled by yi ∈ {1,−1} denoting the classification of the example . We assume here that the data set is linearly separable , ie , a hyperplane separating the two classes of examples exists ; the nonseparable case , which is our main object , will be discussed shortly . The SVM training problem that will be discussed in this paper is essentially to solve the following optimization problem ( P1 ) .
Max Margin ( P1 ) min . wrt w = ( w1 , . . . , wn ) , θ+ , and θ− , and st ffwff2 − ( θ+ − θ− ) 1 2 w · xi ≥ θ+ , w · xi ≤ θ− , if yi = 1 , if yi = −1 .
Here we follow [ 5 ] and use their formulation . The problem can be restated with a single threshold parameter as given in the original paper [ 15 ] .
Remarks on Notations Throughout this paper , we use X to denote the set of examples , and let n and m denote the dimension of the example space and the number of examples respectively . Also we use i for indexing examples ( and their labels ) , and xi
Theory Comput Syst and yi to denote the ith example and its label respectively . The range of i is always {1 , . . . , m} .
By the solution of ( P1 ) , we mean the hyperplane that achieves the minimum cost . We sometimes consider a partial problem of ( P1 ) that minimizes the target cost under some subset of constraints . A solution to such a partial problem of ( P1 ) is called a local solution of ( P1 ) for the subset of constraints . Given a solution , its support vectors are the data points xi for which , at the solution , the corresponding inequality is tight ; that is , w · xi = θ+ if yi = 1 , and w · xi = θ− if yi = −1 .
Intuitively , the maximal margin separator does not necessarily lie towards either class , and therefore provides an idea why it could generalize better . Formal discussions can be found in [ 16 ] , where it is proved that , in the linearly separable case , the generalization error is bounded by a term that depends on the margin but not on the dimensionality of the space . In fact , the dimension could be actually infinite if a way to operate with the corresponding vectors would exist , and the trained SVM would still obey a reasonable bound on the generalization error .
An important feature of SVM is that it is also applicable for the nonseparable case . More precisely , for nonseparable data we can take two positions : ( i ) the case where we consider that a hyperplane is too weak to be a classifier for our given examples , and that we should be able to fit them better nonlinearly ; and ( ii ) the case where we consider that there are some erroneous examples or exceptions , ie , “ outliers ” , which should be somehow identified and allowed to be misclassified . Of course , it would be better if we can use a nonlinear classifier . Nevertheless , the second approach is important as well if we suspect that outliers exist in a given set of examples . The usability of SVM is due to the fact that we can use both .
The first subcase is solved by the SVM approach by mapping examples into a much higher dimension space ; we come back to this point later on . The second subcase is solved by relaxing constraints by introducing slack variables or “ soft margin error ” . That is , we consider the following generalization of the problem ( P1 ) , corresponding to the soft margin hyperplane separation problem . ffwff2 − ( θ+ − θ− ) + D · . min . wrt w = ( w1 , . . . , wn ) , θ+ , θ− , and ξ1 , . . . , ξm , st and
ξi i if yi = 1 , if yi = −1 , and
Max Soft Margin ( P2 )
1 2 w · xi ≥ θ+ − ξi , w · xi ≤ θ− + ξi , ξi ≥ 0 .
Again this formulation from [ 5 ] is different from the standard one [ 15 ] . But it is shown [ 5 ] that these two formulations are equivalent provided that D is small enough so that a nontrivial solution exists .
For a given set X of examples , suppose we solve the problem ( P2 ) and obtain the optimal hyperplane . Then an example in X is called an outlier if it is misclassified with this hyperplane . On the other hand , examples other than outliers are called normal examples . Throughout this paper , we use to denote the number of outliers .
Theory Comput Syst
Notice that this definition of outlier is relative both to the hypothesis class and to the soft margin parameter D , which determines the degree of influence of the outliers . Note that D should be fixed in advance ; that is , D is a constant throughout the training process . This point will be discussed later . ties : that is , w · xi = θ+ − ξi if yi = 1 , and w · xi = θ− + ξi if yi = −1 .
Again , the concept of support vector for ( P2 ) is defined in terms of tight inequali
For the soft margin case , some generalization error bounds exist but tend to be weaker , and/or depend on particular properties of the underlying probability distribution . The formulation , however , turns out to be close to those obtained from applying regularization theory to the problem of fitting the examples as well as possible [ 17 , 18 ] : the sum of slacks corresponds to minimizing the error , whereas the norm of the vector corresponds to a regularization term , with regularizer factor 1/D ; except that the bias terms do not correspond to such a regularized problem .
2.1 LP type Optimization Problems and the Sampling Lemma
We explain now , briefly , the essentials of the abstract framework for discussing randomized sampling techniques that was given by Gärtner and Welzl [ 20 ] . ( The idea , and its algorithmic application as in our Theorem 2.2 below , can be found already in the paper by Clarkson [ 13 ] , where a randomized algorithm for linear programming has been proposed . Indeed , a similar idea has been used [ 1 ] to design an efficient randomized algorithm for quadratic programming . Here we explain the framework and the algorithm following [ 20] . ) Randomized sampling techniques , particularly , the Sampling Lemma below , are applicable for many “ LP type ” problems . Here we use ( D , φ ) to denote an abstract LP type problem , where D is a set of elements and φ is a function mapping any R ⊆ D to some value space . In the case of our problem ( P1 ) , for example , we can consider an LP type problem ( D1 , φ1 ) , where D1 is X , and φ1 is a mapping from a given subset XR of X to the local solution of ( P1 ) for the subset of constraints corresponding to XR . Of course , there is a certain set of conditions [ 20 ] that any LP type problem must satisfy . But we omit the explanation here and simply mention that our ( D1 , φ1 ) clearly satisfies these conditions . For any R ⊆ D , a basis of R is an inclusion minimal subset B of R such that φ ( B ) = φ ( R ) . The combinatorial dimension of ( D , φ ) is the size of the largest basis of D . We will use δ to denote the combinatorial dimension . For the problem ( P1 ) , each basis is a minimal set of support vectors . Hence , the combinatorial dimension of ( P1 ) is at most n + 1 , which is due to the fact that the number of support vectors for ( P1 ) is at most n + 1 Consider any LP type problem , and any subset R of D . A violator of R is an element e of D such that φ ( R ∪ {e} ) )= φ ( R ) . An element e of R is extreme in R if φ ( R − {e} ) )= φ ( R ) . In our case , for any subset XR of X , let ( w , θ+ , θ− ) be a local solution of ( P1 ) obtained for XR . Then xi ∈ X is a violator of XR ( or , more directly , a violator of ( w , θ+ , θ− ) ) if the constraint corresponding to xi is not satisfied by ( w , θ+ , θ− ) . Consider again any LP type problem ( D , φ ) . Let U be a set consisting of u elements of D . U may be a multiple set , ie a set containing possibly some elements
Theory Comput Syst more than once . In order to discuss the case when elements of D are chosen into R according to some possibly nonuniform probability , we will use U as domain instead of D , and will consider simply that R is a subset of U . Though obvious , the following relation is important for our discussion . e violates R ⇐⇒ e is extreme in R ∪ {e} .
( 1 ) Define vR and xR to be the number of violators and extremes of R in U respectively . The following bound , which is also easy from the definition , is important . xR ≤ δ ( = the combinatorial dimension of ( D , φ) ) .
( 2 )
We are ready to state the Sampling Lemma . ( Here , for the completeness , we present the proof given in [ 20] . ) Lemma 2.1 Let ( D , φ ) be any LP type problem . Assume some weight scheme u on D that gives an integer weight to each element of D . Let u(D ) denote the total weight . For a given r , 0 ≤ r < u(D ) , we consider the situation where a set of r elements of D has been chosen randomly , according to their weights . Let R denote the set of chosen elements , and let vR be the weight of violators of R . Then we have the following bound on the expected value of vR :
Exp(vR ) ≤ u(D ) − r r + 1
· δ .
( 3 ) Proof For a given weight scheme u on D , we define U as a multiple set containing exactly u(x ) copies of each element x in D . Then choosing a set R of r elements randomly from D according to their weights is essentially the same as choosing one set R from all possible For randomly selected R , consider the values vR and xR . Let vr and xr denote their expected values Exp(vR ) and Exp(xR ) . Then we prove
( multi)subsets of U uniformly at random . fi ' u(D ) r
( r + 1)vr = xr+1(u(D ) − r ) .
( 4 )
' fiU r to
The bound of the lemma follows from this and the bound ( 2 ) . denote the set of all r element subsets of U .
The above relation ( 4 ) is derived easily from the following . Here we use ff u(D ) r
. e∈U−R . e∈U−R . vr = . R∈( U = . r ) R∈( U r ) = . Q∈( U r+1 ) e∈Q
[ e violates R ]
[ e is extreme in R ∪ {e} ]
[ e is extreme in Q ] = xr+1 ff u(D ) r + 1
.
.
See [ 20 ] for additional explanations , variations for other sampling schema , important related results such as tail bounds , and a large number of incarnations of this Sampling Lemma .
Theory Comput Syst
2.2 Preliminary Algorithmics
Consider first the separable case ( P1 ) . We can solve this optimization problem by using a standard general quadratic programming algorithm . In some applications , however , the number m of examples is much larger than the dimension n ( in other words , many more constraints than variables ) . This is the situation where randomized sampling techniques are effective .
We first describe how to adapt the general purpose randomized algorithm from [ 20 ] , which works for arbitrary LP type problems . The adaptation of both the algorithm and its analysis is straightforward , but it serves here the purpose of a preliminary easier case that simplifies later on the discussion of our new algorithm .
The idea is simple . Pick up a certain number of examples from X and solve ( P1 ) under the set of constraints corresponding to these examples . We choose examples randomly according to their “ weights ” , where initially all examples are given the same weight . Clearly , the obtained local solution is , in general , not the global solution , and it does not satisfy some constraints ; in other words , some examples are misclassified by the local solution . Then double the “ weight ” of such misclassified examples , and then pick up some examples again randomly according to their weights . If we iterate this process several rounds , the weight of “ important examples ” , which are support vectors in our case , grows exponentially fast , and hence , they are likely to be chosen . Note that once all support vectors are chosen at some round , then the local solution of this round is the true one , and the algorithm terminates at this point . By using the Simple Sampling Lemma , we can prove that the algorithm terminates in O(n log m ) rounds on average . Now we present in more detail the algorithm in Fig 1 . We use u to denote a weight scheme that assigns some integer weight u(xi ) to each xi ∈ X . For this weight Note that U has u(X ) ( = scheme u , consider a multiple set U containing each example xi exactly u(xi ) times . i u(xi ) ) elements . Then by “ choose r examples randomly ' u(X ) from X according to u ” , we mean to select a set of examples randomly from all subsets of U with equal probability . fi r
For analyzing the efficiency of this algorithm , we use the Simple Sampling Lemma 21 From it , we can prove the following bound . ( Again we state the proof for the completeness , though it is immediate from the general argument given in [ 20] . ) procedure OptMargin set weight u(xi ) to be 1 for all examples xi in X ; r ← 6δ2 ; % δ ≤ n + 1 . repeat XR ← choose r examples from X randomly according to u ; ( w , θ+ , θ− ) is a solution of ( P1 ) for XR ; V ← the set of violators in X of the solution ; if u(V ) ≤ u(X)/(3δ ) then double the weight u(xi ) for all xi ∈ V ; until V = ∅ ; return the last solution ; end procedure .
Fig 1 A first randomized SVM training algorithm
Theory Comput Syst
Theorem 2.2 The average number of iterations executed in the OptMargin algorithm is bounded by 6δ ln m = O(n ln m ) . ( Recall that |X| = m and δ ≤ n + 1 . )
Proof We say a repeat iteration is successful if the if condition holds in the iteration . We first bound the number of successful iterations . For this , we analyze how the total weight u(X ) increases . Consider the execution of any successful iteration . Since u(V ) ≤ u(X)/3δ , by doubling the weight of all examples in V , ie , all violators , u(X ) increases by at most u(X)/(3δ ) . Since u(X ) is initially m , after t successful iterations , we have u(X ) ≤ m(1 + 1/(3δ))t . Let X0 ⊆ X be a fixed minimal set of support vectors of ( P1 ) . X0 constitutes a basis , and thus it defines the same solution hyperplane as X ; thus , if all elements of X0 are chosen into XR , ie , X0 ⊆ XR , then there is no violator for XR . At each successful iteration ( if it is not the end ) some xi of X0 must not be in XR , which implies that it is a violator of XR by the fact that X0 is a basis . Hence , u(xi ) gets doubled . Since |X0| ≤ δ , there is some xi in X0 that gets doubled at least once every δ successful iterations . Therefore , after t successful iterations , u(xi ) ≥ 2t /δ .
Therefore , we have the following upper and lower bounds for u(X ) .
2t /δ ≤ u(X ) ≤ m(1 + 1/(3δ))t .
This implies that t < 3δ ln m as long as we have not reached the last iteration . That is , the algorithm terminates within less than 3δ ln m successful iterations .
Next we must estimate how often a successful iteration occurs . Here we use the Sampling Lemma . Consider the execution of any repeat iteration . Let u be the current weight on X , and let R and V be the set chosen at this iteration and the set of violators of XR . Then this XR corresponds to R in the Sampling Lemma , and we have u(V ) = vR . Hence from the above bound 3 , we can bound the expectation of u(V ) by ( u(X ) − r)δ/(r + 1 ) , which is smaller than u(X)/(6δ ) by our choice of r . Thus , the if condition is satisfied if u(V ) is less than double its own average , which happens with probability at least 1/2 . This implies that the expected number of iterations is at most twice as large as the number of successful iterations . Therefore , the algorithm terminates on average within 2 · 3δ ln m steps . .
Thus , while this randomized OptMargin algorithm needs to solve some subproblem of ( P1 ) for about 6n ln m times on average , the number of constraints needed to consider at each time is about 6n2 . Hence , if n is much smaller than m , then this algorithm is faster than solving ( P1 ) directly .
We want to apply a similar technique for the nonseparable case . It seems that we can simply apply the same sampling technique for solving ( P2 ) . But this approach is not so straightforward . For example , we need to redefine the notion of “ violator ” . Notice that for any solution hyperplane ( even the optimal one ) , there must be some examples that are misclassified by it . Thus , we cannot simply regard a misclassified example as a violator . Intuitively , it seems reasonable to consider an example as a violator to the current local solution if it is misclassified by the solution and it is not used to obtain the solution . It turns out that this intuitive approach works . Our first main technical contribution is to give a formal justification to this intuition and derive a random sampling based algorithm for ( P2 ) . We will do so by using alternative
Theory Comput Syst formulations of ( P2 ) given in [ 5 ] , which also helps us to understand the structure of the optimal solution of ( P2 ) .
For designing SVM training algorithms , it is also important that the computation can be done in such a way that the only operations acting on the data points are scalar products , and that the output hyperplane can be defined as a linear combination of the data points . These are necessary for us to use kernels mapping into feature spaces and to be able to use much more complex classifiers [ 16 ] . We will see that our derived algorithm satisfies these requirements .
3 Alternative Formulations
To go on we need an alternative formulation of ( P2 ) , which is derived based on an intuitive geometric interpretation of ( P2 ) that has been given by Bennett and Bredensteiner [ 5 ] .
3.1 Derivation of the Formulation
For the completeness , we review the alternative formulations of ( P2 ) given in [ 5 ] . First , it is shown [ 5 ] that ( P2 ) is equivalent to the following problem ( P3 ) . ( More precisely , the problem ( P3 ) is the Wolfe dual of ( P2) . )
Reduced Convex Hull ( P3 ) min . st
Note that
2
( ((( yi si xi si = 1 ,
.
( ((( i
1 2 . i : yi=1 ( (((
.
( ((( 2 = yi sixi i and 0 ≤ si ≤ D . si = 1 , wrt s1 , . . . , sm , . i : yi=−1 ( ((( . i : yi=1 si xi − . i : yi=−1
2
( (((
. sixi
That is , this is the distance between two points in the convex hulls of positive and negative examples . In the separable case , it is the distance between two closest points . On the other hand , in the nonseparable case , we give some restriction to the influence of each example and consider only points defined by examples in such a way that each example cannot contribute more than D . As mentioned in [ 5 ] , the meaning of D is intuitively explained by considering its inverse k = 1/D . ( Here we assume that 1/D is an integer . Throughout this paper , we use k to denote this constant . ) Then our objective can be regarded as the distance between the convex hulls of “ composed examples ” , points that are defined as a center of k examples . Then resulting convex hulls are reduced ones and they may be separable by some hyperplane . In the extreme case where k = m+ = m− ( where m+ and m− are respectively the number s of positive and negative examples ) , we have only one
Theory Comput Syst positive and one negative composed examples , which are clearly separable ( unless they are the same ) . Remarks on the Choise of D Although we will assume that D = 1/ k for some integer k > 0 , this is simply for our explanation and this restriction is not essential for executing our algorithm . Also note that we do not have to use a single constant for the soft margin parameter D . In particular , it would be more reasonable to use two D+ and D− ( hence , k+ and k− ) for positive and negative examples . But for simplifying our notation , we will discuss with a single soft margin parameter D throughtout the paper . The modification of our algorithm to a two or more soft margin parameter cases is easy .
We state the above intuition formally by reformulating ( P3 ) . Let Z be the set of composed examples zI that is defined by + xi2 zI = xi1
+ ··· + xik k
,
= with some k distinct elements xi1 , xi2 , . . . , xik of X with the same label ( ie , yi1 = ··· = yik ) . That is , each composed example is the mass center of a group of yi2 k homogeneously labeled initial data points . The label yI of the composed example zI inherits its members’ . Throughout this note , we use I for indexing elements of Z ' and their labels . The range of I is {1 , . . . , M} , where M . For each zI , we use zI to denote the set of original examples from which zI is composed . ( For distinguishing from composed examples zI , we will call xi an original example . ) def= |Z| . Note that M ≤ fi m k
Then it is easy to see that ( P3 ) is equivalent to the following ( P4 ) .
Convex Hull of Reduced Points ( P4 )
2
( ((( min . st
.
( (((
I
1 2 . I : yI=1 yI sI zI sI = 1 , wrt s1 , . . . , sM , . I : yI=−1 sI = 1 , and 0 ≤ sI ≤ 1 .
More formally , the same optimal value is achieved by solutions {s {s ∗
}I=1,,M satisfying the following for all i , 1 ≤ i ≤ m .
∗ i
I
1 k
∗ s I
= s ∗ i . ff . I:xi∈zI }i=1,,m , we can find {s ∗
This is because for any solution {s above for all i . i
}I=1,,M satifying the
∗
I
}i=1,,m and
( 5 )
In this paper , we further consider the Wolfe primal of this problem again . Then we come back to the one almost identical to ( P1)!
Theory Comput Syst
Max Margin for Composed Examples ( P5 )
1 2 ffwff2 − ( η+ − η− ) min . wrt w = ( w1 , . . . , wn ) , η+ , and η− , st and w · zI ≥ η+ , w · zI ≤ η− , if yI = 1 , if yI = −1 .
In general , composed examples may not be separable . But they are separable for sufficiently large k , in which case it can be shown that ( P2 ) and ( P5 ) are essentially equivalent . More precisely , from the observation given in [ 5 ] , we can show the following relationship between ( P2 ) and ( P5 ) .
Theorem 3.1 Suppose that k is large enough so that the obtained composed examples are linearly separable . Then the weight vector for the optimal separating ∗ , coincides for ( P2 ) and ( P5 ) . ( Note that the margin parameters hyperplane , ie , w ∗+ , θ ∗+ , η ∗− ) are usually different . ) ( θ
∗− ) and ( η
Note that the problem ( P5 ) is essentially the same as ( P1 ) ; thus , we can use our randomized algorithm OptMargin of Fig 1 for ( P5 ) . In fact , the problem ( P5 ) is regarded as the LP type problem ( D5 , φ5 ) similar to ( D1 , φ1 ) for ( P1 ) , where D5 is Z , and φ5 is a mapping from a given subset ZR of Z to the local solution of ( P5 ) . In particular , the combinatorial dimension of ( P5 ) is n + 1 , the same as that of ( P1 ) . Only the difference is that there are now M = O(mk ) composed examples ( hence , so many constraints ) . This number is quite large , but this is the situation suitable for the sampling technique . Suppose now that we use OptMargin for solving ( P5 ) . From our analysis , the expected number of iterations is O(n ln M ) = O(kn ln m ) . That is , we need to solve QP problems with n+ 2 variables and O(n2 ) constraints for O(kn ln m ) times . Although this is not bad at all , there are unfortunately some serious problems . The algorithm needs , at least as it is , a large amount of time and space for “ book keeping ” computation . First of all , we have to keep weights of all M composed examples in Z . Secondly , for finding violators and for modifying weights , we have to go through Z , which takes at least O(M ) steps . Also it is not so easy to choose composed examples randomly according to their weights . Some solution to these problems were proposed in [ 2 ] , but the proof of the running time of the resulting algorithm depended on an unproven hypothesis .
In this paper , instead of using OptMargin directly to ( P5 ) , we derive an algorithm , based on a nontrivial geometric lemma , that handles only m weights and avoids searching for violators on all of Z . In fact , this algorithm is a natural generalization of OptMargin for ( P2 ) . Also since it uses only scalar products on data points , it combines with any desired kernel .
Theory Comput Syst
3.2 Properties of the Solutions
Before deriving the algorithm , let us first examine solutions to ( P2 ) and ( P5 ) . Throughout this subsection , we assume that k is large enough so that the set of composed examples are linearly separable .
∗
∗− , ξ
∗+ , θ
∗+ , η
For a given example set X , let Z be the set of composed examples . Let ∗ ∗ ∗− ) be the solutions of ( P2 ) for X and ( P5 ) for Z ) and ( w ( w , θ , η ∗ as explained above . Let Xerr,+ and Xerr,− denote the sets of respectively , sharing w positive/negative outliers . That is , xi belongs to Xerr,+ ( resp . , Xerr,− ) if and only if ∗+ ( resp . , yi = −1 and w ∗ · xi < θ yi = 1 and w ∗− ) . We use + and − to denote the number of positive/negative outliers . Under the LP type interpretation ( D5 , φ5 ) of ( P5 ) , consider any fixed basis Z0 of Z . In order to facilitate understanding , we assume nondegeneracy throughout the following discussion . Note that every element of the basis is extreme in Z . Hence , we call elements of Z0 final extremers . By definition , the solution of ( P5 ) for Z is defined by the constraints corresponding to these final extremers .
∗ · xi > θ
By analyzing the Karush–Kuhn–Tucker ( in short , KKT ) condition for ( P2 ) , we can prove the following lemma . ( For the sake of simplicity , this lemma is stated only for the positive examples ; but the corresponding properties clearly hold for the negative examples . )
Lemma 3.2 We use symbols defined above . In particular , assume that composed examples in Z are linearly separable under the current choice of k . Let zI be any positive final extremer , ie , an element of Z0 such that yI = 1 . Then the following properties hold . ∗ · zI = η ∗+ . ( a ) w ( b ) Xerr,+ ⊆ zI . Hence , + ≤ k . ( c ) We may consider that Xerr,+ )= zI , ie , + < k . ( d ) For every xi ∈ zI , if xi /∈ Xerr,+ , then we have w ∗+ . ( e ) If Xerr,+ is not empty , then we have η
∗ · xi = θ
∗+ < θ
∗+ . ffl
∗ · zJ > η indeed contained all positive examples in Z0 , then we could set η+ with η
Proof ( a ) Since Z0 is the set of final extremers , ( P5 ) can be solved only with the ∗+ for some constraints corresponding to elements in Z0 . Suppose that w positive zJ ∈ Z0 including zI of the lemma . Let Z be the set of such zJ ’s of Z0 . If ∗+ − ffl Z for some > 0 and still satisfy all the constraints , which contradicts the optimality of the solution . Hence , we may assume that Z0 − Z still has some positive example . Then it is well known ( see , eg , [ 8 ] ) that a local optimal solution to the problem ( P5 ) with the constraints corresponding to elements in Z0 is also locally optimal to the problem ( P5 ) with the constraints corresponding to only elements in Z0 − Z . Furthermore , since ( P5 ) is a convex programming , a local optimal solution is globally optimal . Thus , the original problem ( P5 ) is solved with the constraints corresponding to elements in Z0 − Z . This contradicts our assumption that Z0 is the set of final extremers . ffl ffl ffl
( b ) Since ( P2 ) is a convex minimization problem , the KKT point of ( P2 ) is ob ) , the tuple
) . That is , with some ( s tained from its solution ( w
∗+ , θ
∗− , ξ
, u
, θ
∗
∗
∗
∗
Theory Comput Syst
∗
∗+ , θ
∗− , ξ
∗
∗
∗ , u
, θ
, s
) satisfies the following so called KKT condition . ( Below we ( w use i to denote indices of examples , and let P and N respectively denote the set of indices i of examples such that yi = 1 and yi = 0 . We use e to denote the vector with 1 at every entry . ) i xi = 0 , ∗ s w si xi + . i∈N = 0 , ∗ ∗ · xi − θ
∗ − . i∈P −1 + . i∈P ∀i ∈ P [ s ∗ i ( w ∗ · ξ s i u i ) = 0 ] , ∗ ) · ξ ∗ = 0 ( which means ( De − s ∗
∗+ + ξ
∗
De − s ∗ − u −1 + . i∈N ∀i ∈ N [ s ∗ i ( w ∗ = 0 ) , and ξ ∗ s i
∗ = 0 ,
= 0 , ∗ · xi − θ ∗ ≥ 0 . ∗
, s
, u
∗− − ξ i ) = 0 ] , ∗
From these requirements , we have the following relation . ( Note that the condition ∗ ≤ De below is derived from the requirements De − s s
∗ = 0 and u
∗ ≥ 0 . )
∗ − u s
∗ = . w i∈P . = 1 , i∈P i xi − . ∗ i∈N . ∗ i∈N s i s i
∗
∗ i xi , s = 1 , and 0 ≤ s
∗ ≤ De .
∗ i i
I
ξ
∗
∗
∗
∗ s I
= 0 ; that is , s
∗ = 0 and that De− s
I:xi∈zI = 1 . Hence , s ∗
= 1/ k . Then from ( 5 ) we have I:yI=1 s is exactly the optimal solution of ( P3 ) . )· ξ
In fact , s Consider any xi ∈ Xerr,+ ; we will show below that xi ∈ zI for any zI ∈ Z0 . Since i > 0 , from the requirements that ( De− s ∗ ≥ 0 it follows ∗ that D − s = 1 . On ∗ the other hand , from the constraint of ( P4 ) , we have I > 0 implies xi ∈ zI for any I . Now by the equivalence of ( P4 ) and ( P5 ) , we see that the final extremers are exactly points contributing to the solution of ( P4 ) . That is , for any I , it holds that zI ∈ Z0 if and only if s I > 0 . Therefore , for any zI ∈ Z0 , we have ∗ xi ∈ zI . ( c ) If Z0 contains more than one positive element , then the relation Xerr,+ )= zI is immediate from the above . A subtle case is when zI is the unique positive element of Z0 . Suppose that all xi ∈ zI are outliers wrt ( w ∗+ , θ ∗− , ξ ∗ ) . In this case , how∗ · xi as θ ever , the same optimal value is obtained by using θ+ = w ∗+ for any xi ∈ zI . ∗ · xi , in which case there is at least Thus , we may assume that θ one element not in Xerr,+ . zI ∈ Z0 . Since s from the requirement si ( w
( d ) Consider any index i in P such that xi appears in some of the final extremer ∗ ∗ i > 0 by using the equation ( 5 ) . Hence , I > 0 , we can show that s i ) = 0 , we have ∗+ + ξ ∗ ∗+ + ξ ∗ · xi − θ Thus , if xi )∈ Xerr , ie , it is not an outlier or ξ ∗
= 0 . ∗ = 0 , then we have w
∗+ is the smallest w
∗ · xi = θ
∗ · xi − θ
∗+ .
, θ w
∗ i i
( e ) From ( a ) , we have
∗+ = w
η
∗ · xi1
+ xi2
+ ··· + xik k
.
Theory Comput Syst
Since Xerr,+ is not empty , some of these xi1 , . . . , xik are outliers ; for such an out∗+ . On the other hand , as shown in ( c ) , we have lier xij , it holds that w ∗ · xij w
∗ · xij < θ
= θ
∗+ for all normal examples in Z0 . Thus , + ··· + xik
+ xi2
∗ · xi1
∗+ = w
η
∗+ kθ k
<
= θ
∗+ .
. k
∗ · z = 0 is η
Let us give some intuitive interpretation to the facts given in this lemma . ( Again we only consider , for the simplicity , the positive examples . ) The fact ( a ) of the lemma shows that all final extremers are located on some hyperplane whose distance from ∗+ . Essentially , ( b ) means that , when the final the base hyperplane w zI , the set of extremers are obtained , the set of outliers can be obtained as examples appearing in all final extremers . On the other hand , the fact ( d ) states that all original normal examples , ie , examples not in Xerr,+ , appearing in some final extremer are all located again on some hyperplane whose distance from the base ∗+ . ( Here we assume that k > + and each final extremer contains hyperplane is θ k − + normal examples . ) Now consider the point ff . xi∈Xerr,+ v+ def= 1 +
∗+ > η zI∈Z0
) xi
·
,
∗+ = w
∗+ ≥ namely , the center of positive outliers . Define μ ∗+ ≥ μ ∗+ ; that is , the hyperplane defined by the final extremers is located between η the one having all normal examples in the final extremers and the one having the center v+ of outliers . More specifically , since every final extremer is composed from all + positive outliers and k − + normal examples , we have ∗+ = + : k − + .
∗ · v+ . Then we have θ
∗+ − μ
∗+ − η
∗+ : η
θ
4 Our Algorithm
Now we are ready to derive the algorithm for ( P2 ) . First we give a rather rough outline of the algorithm , and then discuss its details formally . Throughout this section , we again assume that k ( = 1/D ) is large enough so that the set of composed examples is linearly separable .
We use the same notations as in the previous sections . X is the set of original examples xi , ie , input data , and Z is the set of composed examples zI made up from X . Our goal is to find a maximal margin separator of these composed examples . As explained in the previous section , the application of the algorithm OptMargin to ( P5 ) requires some heavy task such as book keeping of all weights of composed examples . This problem may be solved by associating weights to original examples xi rather than to composed examples . Instead of generating composed examples according to their weights , we first generate original examples according to their weights , and define the set of composed examples that are made up from the generated original of examples . More specifically , we define Pk to be a mapping from any subset X . Then X to the set of all composed examples consisting only of k elements of X ffl ffl
Theory Comput Syst procedure OptMargin_Composed set weight u(xi ) to be 1 for all examples xi in X ; r ← 6δ2 ; loop XR ← choose r elements from X randomly according to their weights ; ZR ← Pk(XR ) ; ( w , η+ , η− ) ← the solution of ( P5 ) for ZR ; if no composed example is misclassified by ( w , η+ , η− ) then exit ; V ← the set of “ violator ” under the interpretation of ( D2 , φ2 ) ; double the weight of examples in V ( unless u(V ) is too large ) ; end loop ; compute ( w , θ+ , θ− ) from the last solution ( w , η+ , η− ) ; return ( w , θ+ , θ− ) ; end procedure .
Fig 2 A new randomized SVM training algorithm ( outline ) we consider the following procedure : ( 1 ) generate a set XR of examples from X , ( 2 ) compute the set ZR = Pk(XR ) , ( 3 ) solve ( P5 ) on ZR , and ( 4 ) if the obtained local solution is not satisfiable , then increase the weight of examples in X that cause a violator of the current solution . For the implementation of this idea , we introduce a new LP type problem ( D2 , φ2 ) , where D2 is X , and φ2 is a mapping from any subset XR of X to the solution of ( P5 ) on Pk(XR ) . Based on this new interpretation , we can outline our new sampling algorithm as presented in Fig 2 . There are several points that have to be clarified . First , we need to show that the defined ( D2 , φ2 ) is indeed an LP type problem . Intuitively , ( D2 , φ2 ) is parallel to the LP type problem ( D5 , φ5 ) defined for ( P5 ) . In fact , φ2(XR ) = φ5(Pk(XR) ) , and ( D2 , φ2 ) can be considered as a special case of ( D5 , φ5 ) , where we focus on the sets defined by Pk(XR ) with XR ⊆ X ( = D2 ) for a subset of Z ( = D5 ) . In order to show that ( D2 , φ2 ) is indeed parallel to ( D5 , φ5 ) and hence it satisfies the conditions for LP type problems , it is important to establish the following relationship : for any XR , ZR = Pk(XR ) has a violator in the sense of φ5 if and only if XR has a violator in the sense of φ2 . This relation is proved by using Lemma 32 Lemma 4.1 For any XR ⊆ X , let ZR = Pk(XR ) . Also let ( w , θ+ , θ− ) and ( w , η+ , η− ) be respectively the solution of ( P2 ) on XR and ( P5 ) on ZR . Then the following three statements are equivalent . ( i ) There exists a composed example zI ∈ Z that is misclassified by ( w , η+ , η− ) . ( Thus , this zI is a violator of ZR under the interpretation of ( D5 , φ5) . ) ( ii ) There exists an original example xi ∈ X− XR such that Pk(XR ∪{xi} ) contains a composed example that is misclassified by ( w , η+ , η− ) . ( Thus , this xi is a violator of XR under the interpretation of ( D2 , φ2) . ) ( w , θ+ , θ− ) .
( iii ) There exists an original example xi ∈ X − XR that is misclassified by
Theory Comput Syst
Remark Precisely speaking , for the solution ( w , θ+ , θ− ) , we consider the one defined in the proof of Lemma 32(c ) Proof Since ( ii ) ⇒ ( i ) is trivial , we show that ( i ) ⇒ ( iii ) and ( iii ) ⇒ ( ii ) . Here we consider only positive examples , the negative case being analogous . In the following proof , we will make use of Lemma 32 Notice that ZR is the set of all composed examples made from XR . Thus , Lemma 3.2 holds with X = XR and Z = ZR . ( i ) ⇒ ( iii ) : Suppose that there exists a positive composed example zI that is misclassified by ( w , η+ , η− ) ; that is , w · zI < η+ . Pick any final extremer z0 of ZR . Then we argue first that zI contains some misclassified example xi , ie , an example with w · xi < θ+ , that is not in z0 . Suppose otherwise ; that is , all misclassified original examples of zI are accounted for in z0 . Since some correctly classified example xj exists in z0 , and it satisfies w · xj = θ+ ( Lemma 3.2(c ) , ( d) ) , this would imply w· zI ≥ w· z0 . But this contradicts the assumption w· zI < η+ , because w· z0 = η+ . ( iii ) ⇒ ( ii ) : Suppose that there exists a positive example xi ∈ X − XR that is misclassified by ( w , θ+ , θ− ) . This means that w · xi < θ+ . Consider any final extremer z0 of ZR . Then its corresponding inequality is tight ; that is , w · z0 = η+ . Also it follows from Lemma 3.2(b ) and ( c ) that z0 contains all misclassified original examples of XR and that the remaining examples xj ∈ z0 fulfill w · xj = θ+ . Construct zI by replacing one of such xj ’s with xi . Then we have w · zI < w · z0 = η+ . Hence zI is misclassified by ( w , η+ , η− ) ; furthermore , zI is in Pk(XR ∪ {xi} ) . .
This lemma provides us with a way to find violators of the generated set XR in our algorithm . That is , if and only if it is not in XR and it is misclassified by the solution of ( P2 ) on XR . Thus , by solving ( P2 ) on XR and by using its solution , we can compute the set V violators of XR . Furthermore , the lemma also guarantees that no composed example is misclassified ( by the current solution for ZR ) if and only if V = ∅ . Hence , the stopping condition can be checked by testing whether V = ∅ . This means that we do not have to solve ( P5 ) on ZR ; in fact , it is even not necessary to compute ZR! These observations lead to our algorithm stated in Fig 3 . ( We use the same condition as before for determining when weights should get increased . ) procedure OptSoftMargin set weight u(xi ) to be 1 for all examples xi in X ; r ← 6δ2 ; % See Lemma 4.2 for the bound for δ . repeat XR ← choose r examples from X randomly according to u ; ( w , θ+ , θ− ) is a solution of ( P2 ) for XR ; V ← the set of examples in X − XR that are misclassified by ( w , θ+ , θ− ) ; if u(V ) ≤ u(X)/(3δ ) then double the weight u(xi ) for all xi ∈ V ; until V = ∅ ; return the last solution ; end procedure .
Fig 3 The final randomized SVM training algorithm
Theory Comput Syst
Finally , we estimate the combinatorial dimension δ of ( D2 , φ2 ) . ( See a remark at the end of this section for the tightness of this bound . ) Lemma 4.2 The combinatorial dimension δ of ( D2 , φ2 ) is at most k(n + 1 ) . Proof Recall that the combinatorial dimension of ( D1 , φ1 ) is at most n + 1 ; then , by the same argument , we can bound the combinatorial dimension of ( D5 , φ5 ) by n+ 1 . That is , every basis of D5 consists of at most n + 1 composed examples . Now assume to the contrary that some basis X1 of ( D2 , φ2 ) has more than k(n+ 1 ) elements . Let Z1 = Pk(X1 ) . We may assume that both X1 and Z1 contain examples from both positive and negative ones so that the problems ( P2 ) and ( P5 ) are nontrivial.2 Consider any basis Z2 of Z1 . Then by definition , we have φ2(Z2 ) = φ2(Z1 ) ; in other words , the partial solution of ( P2 ) for Z2 is the same as the one for Z1 . Let X2 be a subset of X1 such that Z2 = Pk(X2 ) . As explained above , since Z2 has at most n + 1 composed examples , X2 has at most k(n + 1 ) examples . Hence , there exists some element x0 in X1 − X2 , which must be a violator of X2 , since X1 is a basis . In other words , x0 is misclassified by the partial solution of ( P2 ) on X2 . But then from Lemma 4.1 , there must be some violator of Z2 in Z1 , which contradicts the fact φ2(Z2 ) = φ2(Z1 ) . .
Now we conclude our analysis and estimate the total running time of the algorithm . This can be made in exactly the same way as the one for the separable case ( Theorem 2.2 ) , and we have the following theorem .
Theorem 4.3 The average number of iterations executed in the OptSoftMargin algorithm is bounded by 6δ ln m = O(n ln m ) , where |X| = m and δ ≤ k(n + 1 ) .
Notice here that our obtained algorithm uses solutions of ( P2 ) for selected original examples . Thus , any algorithm solving ( P2 ) should work ; in particular , we can use any kernel method so long as it works for solving the original soft margin problem ( P2 ) . But note also that the combinatorial dimension δ may get larger depending on the feature space .
Some Remark on the Combinatorial Dimension The combinatorial dimension δ is an important complexity parameter in our algorithm . First as stated above , it is used to bound the average number of iterations . Also recall that the number r of samples at each iteration—the sampling parameter—is determined by r = 6δ2 . ( Though this choice of r is sufficient for proving our bound , our preliminary experiments show that much smaller value of r is sufficient . It is our future work to study how to select the sampling parameter r , in particular , for popular kernels . ) In the above lemma , we give only a simple bound for the combinatorial dimension δ ; that is , δ ≤ k(n + 1 ) . Our experiments , however , indicate that actual δ seems to be much smaller , and it is quite likely that we can give much better bounds for δ . In
2We can define our LP type problems , eg , ( D2 , φ2 ) so that X1 has no extreme element ( hence , X1 is not a basis ) if its corresponding subproblem of ( P2 ) is trivial .
Theory Comput Syst
An example of reduced convex hulls and separating hyperplanes for n = 3 and k = 3 . We assume that all composed negative examples are on the surface of the cone ( top ) , while all composed positive examples ( except three explained below ) are under the shadowed triangular plane ( bottom ) . Here we focus on three composed positive examples that are located at vertices of the shadowed triangle . We assume that two original positive examples are located at a , b , and c respectively , and that one original example exists at d . Then the three composed examples are those cosisitng of two points at a ( resp . , b and c ) and one point at d . The soft margin or the distance between reduced convex hulls is the distance of two points indicated as + . Fig 4 An example for indicating δ cannot be n + 1 + k ( part 1 ) fact , one might argue as follows to bound δ ≤ n+1+ k : Suppose that for a given set X of examples , the number of outliers to each local solution of ( P2 ) is at most , where we may assume3 that ≤ k ( Lemma 32 ) Then there are at most n+ 1+ ≤ n+ 1+ k examples for which the constraint inequality of ( P2 ) is tight . ( At most examples xi with ξi > 0 , and at most n+ 1 examples xi with ξi = 0 . ) In other words , the solution hyperplane is determined by these at most n+ 1+ k examples ; hence , it seems that δ is bounded by n + 1 + k . Unfortunately , however , an example given in Figs . 4 and 5 suggests that this easy argument would not be correct . ( The idea of this example has been pointed out by Léonard Rodriguez . )
Nevertheless , the example is somewhat unconclusive , and we still think that one could show a much better bound for δ . We also leave this problem for our future research topics .
5 Extension to Support Vector Regression
The regression problem under insensitive loss by means of SVM can be phrased as follows . We are given a set of points in a product space X× Y , where the coordinates
3What we have from Lemma 3.2 is + , − < k , which guarantees only < 2k . But by more precise argument considering positive and negative examples separately , we can indeed show that ≤ k ; the detail is left to the reader .
Theory Comput Syst
Suppose that the combinatorial dimension of ( D2 , φ2 ) for the examples explained in Fig 4 is n + 1 + k = 7 . Then the same soft margin must be derived by considering some composed examples consisting of only 7 points . Clearly 3 ( = k ) original examples are needed for the negative side ; hence , we can choose at most 4 original examples from the positive side . The shadowed triangle in the figure shows the reduced convex hull when we choose one example from each of a , b , c , and d points . As shown in the figure the distance between two reduced convex hulls is longer than the distance between two + points . The same problem occurs for any choice of 4 original positive examples . Fig 5 An example for indicating δ cannot be n + 1 + k ( part 2 ) in X act as predictor ( or given ) information and the coordinate in Y act as response ( or desired ) variable . Also given the value of for the loss function . We have to determine a hyperplane approximating the cloud of points , in the sense that the hyperplane passes near all the points , missing them all by no more than on the Y coordinate ; thus all are within a so called tube around the hyperplane . Of course , if is too small or there are some exceptions , this might be impossible , in the same way as classifying with a hyperplane a linearly inseparable dataset . The solution for this case is analogous to the classification approach ; that is , soft margins become here slack quantities that allow , but penalize , points that are too far from the hyperplane . There are several formalizations of the SVM regression problem ; here we consider the one studied in [ 9 ] , and explain briefly the reduction of regression to classification suggested in [ 9 ] . ( See the references in [ 9 ] for the other formulations . )
The formulation we consider here is based on shifting the cloud of points along the coordinate corresponding to the response variable . Indeed , from the data points and from the value received for the loss function , we construct two new datasets , one by shifting the response up by , and the other by shifting it down by . For a large enough for which hard tubes exist , these shifts amount to move the first dataset to stand fully above the regression hyperplane , and the second dataset to stand fully below it . Thus , the regression hyperplane has been effectively transformed into a classification hyperplane , as was desired . To apply the support vectors approach , we
Theory Comput Syst simply compute the maximal margin separating hyperplane between both translated sets . Now let us give a precise formulation of the above idea . As before , suppose that we are given a set of m examples xi , 1 ≤ i ≤ m , in some n dimensional space Rn . Each example xi is associated to a value yi ∈ R of the response variable . Also , the value for the loss function is given . Based on the above idea , we formulate the ( hard tube ) SV regression problem ( SVR ) as the optimization problem HardTubeSVR ( P6 ) . Note that we are assuming here that is large enough for the problem to have a solution .
HardTubeSVR ( P6 ) ffwff2 + 1 2
1 2
σ 2 − ( θ+ − θ− ) min . wrt w = ( w1 , . . . , wn ) , σ , θ+ , and θ− , st w · xi + σ ( yi + ) ≥ θ+ , w · xi + σ ( yi − ) ≤ θ− . and
In the same way that the SVM classifiers are also applicable to the nonseparable case , an important feature of SVR is that it can be applied also when the value of is too small or there exist some exceptions or erroneous examples . This is done in classification by relaxing constraints by introducing slack variables or “ soft margin error ” . The reduction of regression to classification provides a similar solution , and we formulate the soft tube SV regression problem as the following SoftTubeSVR ( P7 ) . ( These problems ( P6 ) and ( P7 ) are respectively problems ( 5 ) and ( 7 ) in [ 9] . )
1 2 ff .
SoftTubeSVR ( P7 ) ffwff2 + 1 2
σ 2 − ( θ+ − θ− ) + D · min . wrt w = ( w1 , . . . , wn ) , σ , θ+ , θ− , ξ + 1 , . . . , ξ + st i , − i , w · xi + σ ( yi + ) ≥ θ+ − ξ w · xi + σ ( yi − ) ≤ θ− + ξ +
−
ξ i
≥ 0 ,
ξ i and ξ i
≥ 0 .
−
+ .
+ i i
+ m , and ξ i
,
ξ − 1 , . . . , ξ
− m ,
By the solution of the problems HardTubeSVR and SoftTubeSVR , we mean the hyperplane that achieves the minimum cost . Given a solution of HardTubeSVR , we have some examples xi—support vectors—for which , at the solution , at least one of the corresponding inequalities is tight : w · xi + σ ( yi + ) = θ+ or w · xi + σ ( yi − ) = θ− . On the other hand , for a solution of SoftTubeSVR , there may exist some examples xi—outliers—whose response variables are off the value predicted by the hyperplane by more than . The softness parameter D determines the degree of influence of the outliers as in the formulation of ( P2 ) .
It is easy to transform these optimization problems to those corresponding to the classification problems that we have studied in the previous sections . Then we can simply apply the algorithmics we have developed , and the analysis of the obtained solution can be made similarly . We explain below the transformation for the hard tube SVR and the soft tube SVR .
Theory Comput Syst
Hard Tubes
A slight adjustment of the HardTubeSVR ( P6 ) gives the following equivalent specific classification problem ( P1* ) , where we can see that our examples have both their predictor and response coordinates and that they are considered as positive examples in the + case and as negative examples in the − case .
Max Margin ( P1* ) ffwff2 + 1 2
1 2
σ 2 − ( θ+ − θ− ) min . wrt w = ( w1 , . . . , wn ) , σ , θ+ , and θ− , st and
( w , σ ) · ( xi , yi + ) ≥ θ+ , ( w , σ ) · ( xi , yi − ) ≤ θ− .
Notice that this formulation is essentially the same as ( P1 ) with n+ 1 dimensionality . Thus , we can solve this ( P1* ) by using the algorithm OptMargin in Fig 1 , within the running time given by the following theorem .
Theorem 5.1 The average number of iterations executed in the OptMargin algorithm for ( P1* ) is bounded by 6(n + 2 ) ln m = O(n ln m ) .
∗
∗
As shown in [ 9 ] , a solution ( w
∗− ) for ( P1* ) gives not only the separating hyperplane but also the two biases that indicate both borders ( up and down ) of the solution hard tube . Bi and Bennett proved ( Theorem 3.1 [ 9 ] ) that the optimal value ∗ = − ( θ defines the borders of the of is obtained by hard tube fitting all examples .
∗−)/(2σ
) ; that is ,
∗+ − θ
∗+ , θ
, σ
, θ
∗
∗
Soft Tubes
We can transform , in the same way as above , the SoftTubeSVR ( P7 ) into the following classification problem ( P2* ) . ff .
+ .
+ i
ξ
,
− i
ξ i i
Max Soft Margin ( P2* )
σ 2 − ( θ+ − θ− ) + D · ffwff2 + 1 2
1 2 min . wrt w = ( w1 , . . . , wn ) , σ , θ+ , θ− , + − − 1 , . . . , ξ 1 , . . . , ξ m , ( w , σ ) · ( xi , yi + ) ≥ θ+ − ξ ( w , σ ) · ( xi , yi − ) ≤ θ− + ξ +
+ m , and ξ st
−
ξ
+ i , − i , and ξ i
≥ 0 .
≥ 0 ,
ξ i
Theory Comput Syst
Now it is easy to see that ( P2* ) can be solved similarly by using our OtpSoftMar gin algorithm for ( P2 ) , within the following time bound . Theorem 5.2 Supposed that k ( = 1/D ) is large enough so that two classes of shifted examples of ( P2* ) are linearly separable . Then we can solve ( P2* ) with the OptSoftMargin algorithm , and the average number of iterations executed is bounded by 6δ ln m = O(n ln m ) , where |X| = m and δ ≤ n + 2 + 2k .
∗
∗
, σ
Let ( w
∗+ , θ ∗− ) be a solution for ( P2* ) . Again as in the HardTubeSVM , we , θ ∗ can consider the ) ( see Theorem 3.2 [ 9] ) . But tube with this means here that all normal examples fit in this soft tube , while the outliers lie its outside .
∗ = − ( θ
∗−)/(2σ
∗+ − θ
∗
6 Procedure for Determining the Parameter k
So far we have discussed by assuming that the parameter k is large enough so that the set of composed examples Z made up from X is linearly separable . But then , the choice of the parameter k becomes crucial , and we should provide some way to determine it . Here we give a way to decide whether k is sufficiently large , and propose some algorithms for choosing k .
Indeed , other formulations of SVMs always have at least one parameter that has to be tuned separately . In fact , changing the formulation into an equivalent one ( like we have done for the Bennett and Bredensteiner case ) does not always provide values for those parameters unless the correct solution is known . Thus , a procedure for determining a valid value for k remains necessary . In fact , as far as we know , very little is known about methods to choose the right values of these parameters in various formulations : the most standard way [ 16 ] is to try several choices and use the one with the best performance on the training set , using some sort of cross validation mechanism .
Since we have assumed that k is chosen so that the set of composed examples is linearly separable , let us first consider the “ linear separability ” as a criteria for choosing k . We do not know , however , whether this is an appropriate criteria for k .
We hope that our discussion here would open up some algorithmic approach for determining the influence of erroneous examples .
In the context of linear programming type classification , Bennet and Mangasarian [ 7 ] proposed to use this “ linear separability ” for choosing weights that are similar to our influence parameter . There they provide a way to decide whether examples are linearly separable under a given choice of weights . Here we prove that a similar way works in our situation . Technically , we prove the following characterization .
Theorem 6.1 Let X be any set of examples , and for any k , let Z be the set of composed examples made up from k examples from X . Let ( w ) be a solution of ( P2 ) on X . Then ffw ∗ )= 0 ) if and only if k is large enough so that Z is linearly separable .
∗ff > 0 ( or , equivalently w
∗+ , θ
∗− , ξ
, θ
∗
∗
Remark The problem ( P5 ) does not make sense if k > m+ or k > m− , where m+ and m− are respectively the total number of positive and negative examples in X . Similarly , the problem ( P2 ) is unbounded if D < 1/m+ or D < 1/m− . Thus , we assume below that k ≤ m+ and k ≤ m− .
Theory Comput Syst
First we note the following facts .
∗
∗+ , θ
∗− , ξ
∗
, θ
Fact 1 Let ( w itive , respectively negative outliers wrt ( w ∗− , ξ
∗+ , θ
, θ
∗
∗
∗
) be an optimal solution of ( P2 ) . Then the number of pos
∗+ , θ
∗− ) is at most k .
, θ
) satisfies the KKT condition described Proof The optimal solution ( w ∗ . in the proof of Lemma 3.2 ( b ) . That is , the following holds for some s ( Recall that P and N are used respectively to denote the set of indices of positive and negative examples . We use e to denote the vector with 1 at every entry . ) and u
∗ i xi = 0 , ∗ s w si xi + . i∈N = 0 , ∗ ∗ · xi − θ
∗ − . i∈P −1 + . s i i∈P ∀i ∈ P [ s ∗ i ( w ∗ · ξ u
De − s ∗ − u −1 + . ∗ s i i∈N ∀i ∈ N [ s ∗ i ( w ∗ = 0 ) , and ξ ∗
∗ = 0 , = 0 , ∗ · xi − θ ∗ ≥ 0 . ∗
, u
, s i
∗ i i∈P s
∗+ + ξ
∗− − ξ i ) = 0 ] , ∗
Now outliers are those xi with ξ ie , si = 1/ k , for every outlier xi . On the other hand , s must satisfy i∈N s i ) = 0 ] , ∗ ∗ = 0 ( which means ( De − s ) · ξ ∗ i > 0 . Then from the above , we have D − si = 0 , ∗ = ∗ = 1 . Thus , there are at most k outliers among positive and negative exam . ples respectively . Fact 2 Let ( w , θ+ , θ− , ξ ) be any feasible solution of ( P2 ) such that w = 0 . Then the objective value corresponding to this solution is more than or equal to 0 . Proof The objective value is clearly positive if θ+ ≤ θ− . Thus , we consider here the case that θ+ > θ− . Since w = 0 , it follows from the constraints of ( P2 ) that θ+ ≤ ξi for every positive xi , and −θ− ≤ ξi for every negative xi . Then by using the assumption ( see the remark of Theorem 6.1 ) that Dm+ ≥ 1 and Dm− ≥ 1 , we have ff ≤ D · . i : yi=+1 ff ≤ D · . i : yi=−1
ξi ≤ D · m+ · ξi ≤ D · m− ·
θ+ ≤ min i : yi=+1 −θ− ≤ min i : yi=−1 min i : yi=+1 min i : yi=−1 and
( 6 )
( 7 )
ξi ,
ξi .
ξi
ξi
Hence , by adding both sides , we have θ+ − θ− ≤ D · value of ( P2 ) for this solution is nonnegative . i ξi ; therefore , the objective .
Theory Comput Syst
Proof of Theorem 6.1 Let Z+ and Z− respectively denote the set of all positive and negative composed examples of Z . By “ Z is linearly separable ” , we formally mean that the following holds for some w . zI∈Z+ w · zI > max zJ∈Z− w · zJ . min
( 8 )
∗ )= 0 .
( The if direction ) Since Z is separable , there must be some w satisfying ( 8 ) . Clearly w )= 0 . We construct a feasible solution from w with the negative objective value , which , with Fact 2 , proves that w First note that for any θ+ and θ− , there exists some ξ such that ( w , θ+ , θ− , ξ ) satisfies the constraints of ( P2 ) . In particular , we can define θ+ and θ− so that ( a ) + ≤ k − 1 , and there are k − + positive examples xi such that w · xi = θ+ ; and ( b ) − ≤ k − 1 , and there are k − − negative examples xi such that w · xi = θ− , where + and − are respectively the number of positive and negative examples xi such that ξi > 0 . ( We also use Xerr,+ and Xerr,− to denote respectively the set of positive and negative examples xi such that ξi > 0 . )
Then we have min zI∈Z+ w · zI ≤ ( k − +)θ+ + · . i∈Xerr,+
= θ+ − 1 k k i∈Xerr,+ ( θ+ − ξi ) ξi = θ+ − D · . i∈Xerr,+
ξi .
Similarly , we have zJ∈Z− w · zJ ≥ θ− + D · . i∈Xerr,− max
ξi .
Then it follows from ( 8 ) that
θ+ − D · . i∈Xerr,+
ξi > θ− + D · . i∈Xerr,+
ξi , and hence ,
( θ+ − θ− ) − D · .
ξi > 0 . i
Now since the above value is not zero , there must be some b > 0 that satisfies
( θ+ − θ− ) − D · .
ξi = bffwff2 . i
On the other hand , it is clear that ( aw , aθ+ , aθ− , aξ ) is also feasible for any a > 0 , and the objective value of this solution is now computed as ffawff2 − ( aθ+ − aθ− ) + D · . i
1 2 aξi = a2 2 ffwff2 − abffwff2 = a2ffwff2 ff
.
1 2
− b a
Theory Comput Syst
∗
, θ
∗− , ξ satisfies w
Thus , by choosing a appropriately , we can make the objective value negative .
∗ ∗+ , θ ∗ · zI > maxzJ∈Z− w ∗ · zJ .
( The only if direction ) Suppose that an optimal solution ( w
∗ )= 0 . We show that minzI∈Z+ w
Consider a composed example z+ with the smallest w ∗ · xi < w
) of ( P2 ) ∗ · z+ . Note that for any ∗ · xj . Furthermore , the outlier xi and any normal example xj , we have w ∗ · z+ is number of positive outliers + is at most k ( Fact 1 ) . Hence , the value w minimized if z+ contains all positive outliers and the other ( normal ) elements xj of ∗+ . Thus , letting Xerr,+ be the set of positive outliers , we have z+ satisfies w ∗ · z+ ≥ ( k − +)θ ∗+ − 1 xi∈Xerr,+ ( θ k
∗ · xj = θ
∗+ + min z+∈Z+ w
∗+ − ξ
= θ
= θ
∗ i )
· . xi∈Xerr,+
∗
ξ i k
∗+ − D · . xi∈Xerr,+
∗ i .
ξ
Similarly , letting Xerr,− be the set of negative outliers , we have
∗ · z− ≤ θ max z−∈Z− w
∗− + D · . xi∈Xerr,−
∗ i .
ξ
Thus ,
∗ · z+ − max
∗ · z− ≥ ( θ
∗+ − θ min z+∈Z+ w z−∈Z− w ∗ )= 0 is an optimal solution and the objective value becomes 0 by the solution ( w , θ+ , θ− , ξ ) = ( 0 , 0 , 0 , 0 ) , the objective value corresponding to w
On the other hand , since w
ξ i
∗− ) − D · .
∗ i .
∗ )= 0 must be at most 0 . Thus , we have ∗+ ) + D · .
∗ff2 − ( θ
∗+ − θ ffw
ξi ≤ 0 , i
1 2 which implies ( noting ffw
∗− ) − D · .
∗ff2 > 0 ) ∗+ − θ ∗ · zI > maxzJ∈Z− w
( θ i
ξi > 0 . ∗ · zJ .
Therefore , we have minzI∈Z+ w
.
Assume , as our working hypothesis , that the separability of composed examples is an appropriate criterion for choosing the parameter k . Then this theorem provides us with an algorithmic way to determine k under this criterion . The simplest and straightforward way is to use the binary search method to find the smallest k ( = 1/D ) such that ( P2 ) has a solution with w )= 0 . For this , we need to solve ( P2 ) several times with different k , which may be computationally hard if there are huge number of examples . Here again our random sampling approach could be useful . Notice that k is not large enough even if we have w = 0 for the local solution of ( P2 ) . Thus , one possible approach is to execute the algorithm of Fig 3 with small k . If k is too small ,
Theory Comput Syst we may be able to find it in earlier steps , and then we can revise ( eg , double ) k . Notice that we do not have to restart the algorithm ; we can resume the computation with the revised k . By this way , we can combine the search for an appropriate choice of k with the random sampling iteration .
Notice that it is not necessary to use the same parameter k for all examples . In fact , it seems more reasonable [ 7 ] to use at least two independent parameters k+ and k− for positive and negative examples . Or we may be able to vary the influence parameter D ( = 1/ k ) on each example . For example , one can use small Di for an example xi that seems like an outlier . It would be very interesting if this adjustment of Di ’s can be coordinated with the modification of the weight u(xi ) in our randomized sampling algorithm . This approach including theoretical and experimental investigations on the choice of D is left as our future work .
Acknowledgements We would like to thank Mr . Léonard Rodriguez for pointing out the problem in our earlier argument for estimating the combinatorial dimension of ( D2 , φ2 ) . Thanks are due to Emo Welzl for his help in understanding the applications of the Simple Sampling Lemma , to Jorge Castro for proofreading , and to Norbert Martinez for sharing some experimentation with us .
References
1 . Adler , I . , Shamir , R . : A randomized scheme for speeding up algorithms for linear and convex pro gramming with high constraints to variable ratio . Math . Program . 61 , 39–52 ( 1993 )
2 . Balcázar , JL , Dai , Y . , Watanabe , O . : A Random sampling technique for training support vector machines : for a primal form maximal margin classifiers . In : Proceedings of 12th International Conference on Algorithmic Learning Theory ( ALT’01 ) . Lecture Notes in Computer Science , vol . 2225 , pp . 119–134 . Springer , London ( 2001 )
3 . Balcázar , JL , Dai , Y . , Watanabe , O . : Provably fast training algorithms for support vector machines . In : Proceedings of First IEEE International Conference on Data Mining ( ICDM’01 ) , pp . 43–50 . IEEE , Los Alamitos ( 2001 )
4 . Balcázar , JL , Dai , Y . , Watanabe , O . : Provably fast support vector regression using random sampling . In : Proceedings of SIAM Workshop in Discrete Mathematics and Data Mining , pp . 19–29 . SIAM , Philadelphia ( 2002 )
5 . Bennett , KP , Bredensteiner , EJ : Duality and geometry in SVM classifiers . In : Proceedings of 17th International Conference on Machine Learning ( ICML’2000 ) , pp . 57–64 . Morgan Kaufmann , San Mateo ( 2000 )
6 . Bennett , KP , Campbell , C . : Support Vector Machines : Hype or Hallelujah ? SIGKDD Explorations
2 , 2 ( 2000 )
7 . Bennett , KP , Mangasarian , OL : Robust linear programming discrimination of two linearly insepa rable sets . Optim . Methods Softw . 1 , 23–34 ( 1992 )
8 . Bertsekas , DP : Nonlinear Programming . Athena Scientific , Belmont ( 1995 ) 9 . Bi , J . , Bennett , KP : Duality , geometry , and support vector regression . In : Proceedings of Advances in Neural Information Processing Systems ( NIPS’02 ) , pp . 539–600 . MIT Press , Cambridge ( 2002 )
10 . Boser , BE , Guyon , IM , Vapnik , VN : A training algorithm for optimal margin classifiers . In : Proceedings of 5th Annual Conference on Computational Learning Theory ( COLT’92 ) , pp . 144–152 . ACM , New York ( 1992 )
11 . Bradley , PS , Mangasarian , OL , Musicant , DR : Optimization methods in massive datasets . In : Abello , J . , Pardalos , PM , Resende , MGC ( eds . ) Handbook of Massive Datasets , pp . 439–471 . Kluwer Academic , Dordrecht ( 2002 )
12 . Cauwenberghs , G . , Poggio , T . : Incremental and decremental support vector machine learning . In : Proceedings of Advances in Neural Information Processing Systems ( NIPS’00 ) , pp . 409–415 . MIT Press , Cambridge ( 2000 )
13 . Clarkson , KL : A Las Vegas algorithm for linear programming when the dimension is small . In : Proceedings of 29th IEEE Symposium on Foundations of Computer Science ( FOCS’88 ) , pp . 452– 456 . IEEE , Los Alamitos ( 1988 ) . ( See [ 15 ] for a better version . )
Theory Comput Syst
14 . Clarkson , KL : Las Vegas algorithms for linear and integer programming . J . ACM 42 , 488–499
( 1995 )
15 . Cortes , C . , Vapnik , V . : Support vector networks . Mach . Learn . 20 , 273–297 ( 1995 ) 16 . Cristianini , N . , Shawe Taylor , J . : An Introduction to Support Vector Machines . Cambridge University
Press , Cambridge ( 2000 )
17 . Cucker , F . , Smale , S . : On the mathematical foundations of learning . Bull . Am . Math . Soc . 39(1 ) , 1–49
( 2001 )
18 . Evgeniou , T . , Pontil , M . , Poggio , T . : A unified framework for regularization networks and support vector machines . AI Memo No . 1654 , AI Lab MIT ( 1999 )
19 . Fine , S . , Scheinberg , K . : Efficient SVM training using low rank kernel representations . J . Mach .
Learn . Res . 2 , 243–264 ( 2001 )
20 . Gärtner , B . , Welzl , E . : A simple sampling lemma : Analysis and applications in geometric optimiza tion . Discrete Comput . Geom . 25(4 ) , 569–590 ( 2001 )
21 . Hush , D . , Scovel , C . : Polynomial time decomposition algorithms for support vector machines . Mach .
Learn . 51 , 51–71 ( 2003 )
22 . Keerthi , SS , Gilbert , EG : Convergence of a generalized SMO algorithm for SVM classifier design .
Mach . Learn . 46(1–3 ) , 351–360 ( 2002 )
23 . Lee , Y . , Mangasarian , OL : RSVM : Reduced Support Vector Machines . In : CD Proceedings of the SIAM International Conference on Data Mining , Chicago , 5–7 April 2001 . SIAM , Philadelphia ( 2001 ) . ( Available from http://wwwcswiscedu/ olvi/olvihtml )
24 . Lin , CJ : Linear convergence of a decomposition method for support vector machines . Technical
Report . http://wwwcsientuedutw/~cjlin/papers/linearconvpdf ( 2001 )
25 . Lin , CJ : On the convergence of the decomposition method for support vector machines . IEEE Trans .
Neural Networks 14 , 1267–1281 ( 2002 )
26 . Martín , M . : On line support vector machine regression . In : Proceedings of 13th European Conference on Machine Learning ( ECML’02 ) , pp . 282–294 ( 2002 ) .
27 . Osuna , E . , Freund , R . , Girosi , F . : An improved training algorithm for support vector machines . In :
Proceedings of IEEE Workshop on Neural Networks for Signal Processing , pp . 276–285 ( 1997 )
28 . Pavlov , D . , Mao , J . , Dom , B . : Scaling up support vector machines using boosting algorithm . In : Pro ceedings of International Conference Pattern Rec . , pp . 2219–2222 ( 2000 )
29 . Platt , J . : Fast training of support vector machines using sequential minimal optimization . In : Schölkopf , B . , Burges , CJC , Smola , AJ ( eds . ) Advances in Kernel Methods—Support Vector Learning , pp . 185–208 . MIT Press , Cambridge ( 1999 )
30 . Schölkopf , B . , Burges , C . , Vapnik , V . : Extracting support data for a given task . In : Proceedings of First International Conference on Knowledge Discovery and Data Mining ( KDD’95 ) , pp . 252–257 ( 1995 )
31 . Schölkopf , B . , Smola , A . , Williamson , RC : A new support vector regression algorithm . In : Proceedings of Advances in Neural Information Processing Systems ( NIPS’99 ) , pp . 330–336 . MIT Press , Cambridge ( 1999 )
32 . Schölkopf , B . , Smola , A . , Williamson , RC , Bartlett , PL : New support vector algorithms . Neural
Comput . 12 , 1207–1245 ( 2000 )
33 . Smola , AJ , Schölkopf , B . : A tutorial on support vector regression . NeuroCOLT Technical Report
NC TR 98 030 , Royal Holloway College , University of London ( 1998 )
