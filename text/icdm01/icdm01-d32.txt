Document Clustering and Cluster Topic Extraction in Multilingual Corpora
Joaquim Silva
FCT / Universidade Nova de Lisboa 2725 Monte da Caparica , Portugal jfs@difctunlpt Agra Coelho
ISA / Universidade T´ecnica de Lisboa
Tapada da Ajuda , Portugal coelho@madpetisautlpt
Jo˜ao Mexia
FCT / Universidade Nova de Lisboa 2725 Monte da Caparica , Portugal
Gabriel Lopes
FCT / Universidade Nova de Lisboa 2725 Monte da Caparica , Portugal gpl@difctunlpt
Abstract
A statistics based approach for clustering documents and for extracting cluster topics is described . Relevant ( meaningful ) Expressions ( REs ) automatically extracted from corpora are used as clustering base features . These features are transformed and its number is strongly reduced in order to obtain a small set of document classification features . This is achieved on the basis of Principal Components Analysis . Model Based Clustering Analysis finds the best number of clusters . Then , the most important REs are extracted from each cluster and taken as document cluster topics .
1 . Introduction
We aimed at developing a computational approach for automatically organizing documents from multilingual corpora into clusters . We required no prior knowledge about document subject matters or language . So , we cleaned every keyword that might influence the behavior of our algorithm . Since we want a language independent system , no morpho syntactic information is used . Moreover , we also want to extract the documents main topics of each cluster . Available software for clustering , usually needs a matrix of objects characterized by a set of features . So we need to evaluate the documents according to features/subjects . In order to obtain those features , we used the LocalMaxs algorithm [ 6 ] to automatically extract Multiword Lexical Units or Relevant Expressions ( REs ) from corpora , ie meaningful expressions denoting important concepts such as Common agriculture policy , Common Customs , produits agricoles , etc Since these REs provide important information about the content of the documents , we use them as base features for clustering . The most informative REs correspond to cluster topics , as we will show . This paper is organized as follows : features extraction is explained in section 2 ; clustering and summarization in sections 3 and 4 ; section 5 discusses the results obtained ; related work and limitations are presented in section 6 and conclusions are drawn in section 7 . 2 . Extracting multiword features from corpora Three tools working together , are used forextractingREs from any corpus : the LocalMaxs algorithm , the Symmetric Conditional Probability ( SCP ) statistical measure and the Fair Dispersion Point Normalization ( FDPN ) . A full explanation of these tools is given in [ 6 ] . However , a brief description is presented here . Thus , let us consider that an gram is a sequence of words in text1 . For example the word president is an gram ; the sequence President of the Republic is a gram . LocalMaxs is based on the idea that each gram has a kind of “ glue ” or cohesion sticking the words together within the gram . Different grams usually have different cohesion values . One can intuitively accept that there is a strong cohesion within the gram ( Giscard , d’Estaing ) ie between the words Giscard and d’Estaing . However , one cannot say that there is a strong cohesion within the gram ( or , unincohesion terrupted ) or within the ( of , two ) . So , the value of a generic bigram (
) is obtained by
( 1 ) are the probabilities of occurand in the corto refer an gram of or and unigrams and where , rence of bigram
1We use the notation length . pus ; stands for the conditional probability of occurrence of in the first ( left ) position of a bigram , given that appears in the second ( right ) position of the same bigram . Similarly stands for the probability of occurrence of in the second ( right ) position of a bigram , given that appears in the first ( left ) position of the same bigram . However , in order to measure the cohesion value of each gram of any size in the corpus , the FDPN concept is applied to the measure and a new cohesion measure ,
, is obtained . where
( 2 )
( 3 ) is the probability of the where gram in the corpus . So , any gram of any length is “ transformed ” into a pseudo bigram2 that reflects the average cohesion between each two adjacent contiguous sub gram . Then , LocalMaxs algorithm grams of the original elects every gram whose 3 cohesion value is a salient one ( a local maximum ) , ie , the cohesion of the gram is greater or equal than the cohesion of any of its contiguous sub grams , and greater than the cohesion gram of which it is a sub sequence of words of any ( see [ 6 ] for details ) . See some examples of elected grams , ie , REs : Human Rights , Human Rights in East Timor , common agricultural policy , economia energ´etica and publication au Jounal officiell des Communaut´es . 21 The number of features
Since we want to cluster documents , we must build a matrix of documents “ classified ” in accordance with the smallest possible set of variables and convey that matrix to clustering software . In order to test our approach , we used a multilingual 872,795words corpus with 324 documents 4 . LocalMaxs algorithm extracted 25,838 REs from that corpus . Although , we do not use such a high number of features for distinguishing such a small number of objects ( 324 documents ) — in the worst case we would need no more discriminants to do it ( see [ 4 , pages 517 524 ] than for details ) . However , these REs ( base features ) provide the basis for building a new and reduced set of features .
,
, etc . seem to be “ tailored ” just for grams for any value of
2Roughly we can say that known statistical cohesion/association measures such as grams . However , by applying FDPN to those measures , it is possible to use them for measuring the cohesion values of [ 6 ] . 3LocalMaxs has been used in other applications with other statistical measures , as it is shown in [ 6 ] . However , for Information Retrieval ( IR ) purposes , very interesting results were obtained by using , in comparison with other measures [ 7 ] . 4This is part of the European Legislation in Force ( sub ELIF ) corpus : htp://europaeuint/eur lex
22 Principal Components analysis . Reducing the number of features
Let us take the following extracted REs containing the word agricultural : agricultural products , processing of agricultural products , agricultural products receiving refunds and common agricultural policy . of the original variables , ie , those
For document clustering purpose , there is redundancy in these REs , since , for example , whenever processing of agricultural products is in a document , agricultural products is also in the same document and it may happen that , common agricultural policy is also in that document . Let us show how these redundancies can be used to reduce the number of features . According to Principal Components analysis , often the original correlated random variables ( features ) can be “ replaced ” by a subset new uncorrelated variables ( components ) , each one being a linear combination of the principal components provide most of the information of the origivariables [ 4 , pages 340 350 ] . The original data set , nal consisting of measurements of variables , is reduced to another one consisting of measurements of principal components . Principal components depends solely on ( or the correlation matrix ) of the covariance matrix the original random variables . Now we state variables as being the original ( REs ) of the sub ELIF corpus . Then , for a reduced set of new variables ( principal components ) we would have to estimate the associated covariance matrix of the variables be the estimator of
. So , let the sample covariance5 matrix
.
( 4 )
6 of the is . the estimator covariance where can be seen as a matrix of “ similarities ” between REs . Unfortunately , it is impracticable to obtain principal components from this matrix using available software , due to the computational effort associ ) . Furthermore it ated to its huge size ( seems unlikely that Principal Components analysis could original achieve the strong reduction we need : from ( the number of documents ) new features to features ( principal components ) .
5When we say covariance matrix , we mean variance covariance matrix , to denote the estimator of the covariance 6In this paper , we use since between two random variables . for
.
23 Geometrical representations
We can associate to the th document , the vector
7 whose components are the original ( or transformed ) numbers of occurrences of the REs in the th document . Now we have a smaller ( ) covariance matrix
where the generic element in matrix
is given by
( 5 )
( 6 )
, meaning the average number of occurrences per and RE in the th document , is given by8
( 7 )
Then will be a matrix of similarities between documents . Escoufier and L’Hermier [ 2 ] proposed an approach , based on Principal Components analysis , to derive geometrical representations from similarity matrices . Since orthogonal is symmetric we have , the matrix of normalized eigenvectors ( of are the ) and eigenvalues . Thus diagonal . The principal elements of
, with of and with
( 8 ) The elements of the th line of will be the coordinates of the point associated with the th document . When there are leading eigenvalues we may consider only the corresponding coordinates since the remaining are near . Then , to assess how much of the total information is carried out columns of , we by the first may use components , ie the first
9
( 9 ) columns of matrix So , by taking the first such that or more ( depending on the criteequals , say rion used ) , we can reduce the initial large number of feanew features ( components ) . However , contures to sidering the 324 documents of the sub ELIF corpus , if we for the number of REs of the corpus and 7From now on we will use for the number of documents . 8When we replace an index by a dot , a mean value has been obtained . 9PTV are initials for cumulative Proportion of the Total Variance . of the initial use the original number of occurrences of the th RE in the ) to obtain “ similarities ” ( see ( 6) ) , we th document ( need the first 123 components to provide 0.85 of the total . Although it correinformation , ie sponds to features , this means we have 123 components for distinguishing 324 documents . Besides , to minimize computational effort of the clustering process , it is important to avoid large number of components . So , this number must be reduced without much loss of information . Thus , in order to “ stimulate ” similarities between documents , the original occurrences of the REs in documents will be transformed . 24 Transformed occurrences
As referred above , the geometrical representation may be obtained from transformed occurrences . The technique we used has four phases . In the first phase we standardize in order to correct document heterogeneity . This heterogeneity is measured by the variation between the occurrence numbers of the different REs inside each document . With the number of times that the th RE occurs in the th document , this variation may be assessed by
( 10 ) where is given by ( 7 ) . The standardized values will be
( 11 )
In the second phase we evaluate the variation between documents for each RE . It is important to evaluate this variation as if the documents had the same size , ie each RE associated variation must reflect how much the RE occurrences vary in different documents , due to document content , not due to document size . Therefore we use normalized values to calculate this variation :
( 12 )
These values are important since we found that , generally , the higher , the more information is carried out by the th RE . On the other hand , it was observed that REs constituted by long words , usually are more informative from the IR/Text Mining point of view ( eg agricultural products or communaut´e economique europ´eenne are more informative than same way , plus au moins or reach the level ) . Thus , in a third phase we define weighted occurrences as
( 13 )
, and where is the average length of the words in the th RE . This is measured by the number of characters of the words . Lastly , in the fourth phase we carry out a second standardization considering the weighted occurrences . This is for correcting document size heterogeneity , since we do not want that the document size affects its relative importance . Thus
( 14 )
( 15 ) These standardizations are transformed occurrences and are used to obtain the matrix of similarities between documents . In this matrix , the generic element , and
, is given by
, where
( 16 )
25 Non informative REs
Some high frequency REs appearing in most documents written in the same language are not informative from a IR/TextMining point of view , eg , locutions such as Consid´erant que ( having regard ) , tendo em conta ( having regard ) , and in particular , or other expressions which cannot be considered correct REs , such as of the or dans les ( in the ) . Although these expressions are useless to identify document topics , they are informative for distinguishing different languages . As a matter of fact they occur in most documents of the same language , and their associated variation ( see ( 12 ) ) is usually high or very high , ie , they are relevant to “ approximate ” documents of the same language for calculating similarities between documents ( see ( 13 ) , ( 14 ) and ( 16) ) . So , it seems that either they should be removed to distinguish topics in documents written in the same language , or they should be kept for distinguishing documents of different languages . To solve this problem , we use the following criterion : the REs having at least one extremity ( the leftof most or the rightmost word ) that exists in at least the documents we are working with , are removed from the initial set of REs . We follow that criterion since these expressions usually begin or end with words occurring in most documents of the same language , eg , of , les , que , etc As we will see in Subsect . 3.3 , the documents and REs with which the system is working , depends on the node of the clustering tree . in this section we obtained a matrix where a small set of components classifies a group of documents , which will be given as input to the clustering software . For this purpose , the matrix of similarities between
To summarize , was obtained by ( 8 ) and the first
) was previously calculated ( see ( 5) ) , and the documents ( generic element of this matrix was given by ( 16 ) . Then , from , columns of . Finally , the latter matrix will be taken as input to clustering software . REs for the 324 documents of the sub ELIF corpus , we obtained . was taken , such that Considering the initial ; and
3 . Clustering documents
We need to split documents into meaningful subgroups . However we do not know how many subgroups ( clusters ) should be obtained . Furthermore , though we have obtained features ( components ) to evaluate the documents , we do not know neither the composition of each cluster , nor its volume , shape and orientation in the axes space . 31 The Model Based Cluster Analysis
Considering the problem of determining the structure of clustered data , without prior knowledge of the number of clusters or any other information about their composition , Fraley and Raftery [ 3 ] developed the Model Based Clustering Analysis ( MBCA ) . By this approach , data are represented by a mixture model where each element corresponds to a different cluster . Models with varying geometric properties are obtained through different Gaussian parameterizations and cross cluster constraints . Partitions ( clusters ) are determined by the EM ( expectation maximization ) algorithm for maximum likelihood , with initial agglomerative hierarchical clustering ( see [ 3 ] for details ) . This clustering methodology is based on multivariate normal ( Gaussian ) mixtures . So the density function associated to cluster has the form belongs to cluster
. The covariance matrix
( 17 ) Clusters are ellipsoidal , centered at the means ; element determines other geometric characteristics . This clustering methodology is based on the parameterization of the covariance matrix in terms of eigenvalue decomposition in the is the orthogonal maform trix of eigenvectors , determining the orientation of the prinis the diagonal matrix whose cipal components of elements are proportional to the eigenvalues of , determining the shape of the ellipsoid . The volume of the ellip . Characteristics ( orientasoid is specified by the scalar tion , shape and volume ) of distributions are estimated from the input data , and can be allowed to vary between clusters , or constrained to be the same for all clusters . Considering
, where
.
Table 1 . Parameterizations of the covariance in the Gaussian model and their matrix geometric interpretation .
Distrib . Volume Equal Spher . Spher . Vari . Equal Ellips . Ellips . Vari . Equal Ellips . Ellips . Vari .
Shape Orient . Equal Equal Equal Equal Vari . Vari . Equal Vari . Equal Vari . s n o i t a v r e s b o d e r e d r O
2.5 2 1.5 1 0.5 0 0.5 1 1.5 2 2.5
2.5 2 1.5 1 0.5
1 Standard normal quantiles
0.5
0
1.5
2
2.5
, long known as
( see ( 8 ) and ( 9) ) . documents , corresponding to the components our application , input data is given by the columns “ classifying ” of matrix In Table 1 , Distrib . , Orient . , Spher . , Ellips . and Vari . abbreviate Distribution , Orientation , Spherical , Ellipsoidal and Variable . MBCA subsumes the approach with means , where sum of squares criterion is used , based on the assumption that all clusters are spherical and have the same volume ( see Table 1 ) . How means , the number of clusters has to ever , in the case of be specified in advance , and considering many applications , real clusters are far from spherical in shape . Therefore we have chosen MBCA for clustering documents . Then , function emclust has been used with S PLUS package , which is available for Windows and Linux . During the cluster analysis , emclust shows the Bayesian Information Criterion ( BIC ) , a measure of evidence of clustering , for each “ pair ” model number of clusters . These “ pairs ” are compared using BIC : the bigger the BIC , the stronger the evidence of clustering . The problem of determining the number of clusters is solved by choosing the best model . Table 1 shows the different models used during the calculation of the best model . Models must be specified as a parameter of the function emclust . However , usually there is no prior knowledge about the model to choose . Then , by specifying all models , emclust gives us BIC values for each pair model numberof clusters and proposes the best model which indicates what cluster must be assigned to each object ( document ) . 32 Assessing normality of data
MBCA works based on the assumption of normality of data . Then , Gaussian distribution must be checked for the marginal distributions of the documents on each component . Thus , a QQ plot is made for each component . So , ( see ( 8 ) ) is staneach of the first axis of the QQ plot . Then , dardized , ordered and put on standardized normal quantiles are generated and put on columns of matrix
Figure 1 . QQ plot for assessing normality of data for cluster 1.2 on 2th component . axis of the QQ plot ( see [ 4 , pages 146 162 ] for details ) . Fig 1 represents the QQ plot for the 2th component , assessing the normality of data of cluster 1.210 . We consider this QQ plot representative , since most of the components for other clusters produced similar QQ plots . Although the straightness of this QQ plot is not perfect , as it should be for a high certainty of normality , it is close enough to encourage us to use MBCA as an adequate approach for clustering this kind of data . 33 Sub clusters
As we will see in Sect . 5 , our approach organizes subELIF corpus in 3 main clusters : English , Portuguese and French documents . However , we can distinguish different subjects in different documents of the same cluster . So , a hierarchical tree of clusters is built as follows : let us consider that every cluster in the tree is a node . For every node , non informative REs are removed ( see Subsect . 2.5 ) from the set of REs contained in the documents of that node ( a subset of the original REs ) , in order to obtain a new matrix of similarities between documents ( see ( 16) ) . Then , the first columns of the new matrix are taken ( see ( 8 ) and ( 9) ) , and new clusters are proposed by MBCA . 34 Choosing the best number of clusters
As has been said , MBCA calculates the best model based on the first columns of matrix ( components ) . A large number of components means no significant information loss , which is important for a correct clustering ( best model ) to be proposed by MBCA . On the other hand , a large number of components must be avoided , since it takes MBCA 10In Sect . 5 we will deal with specific clusters . to estimate large covariance matrices — during the internal computation for different models — which can be judged to be close to singularity ( see [ 3] ) . Therefore , we use the folcomponents are chosen in such lowing criterion : the first a way that ( see ( 9) ) . Then , based on those components , MBCA produces a list of BIC values for each model : VVV ( Variable volume , Variable shape , Variable orientation ) , EEV , etc . ( see Table 1 ) . Each list may have several local maxima . The largest local maximum over all models is usually proposed as the best model . However , a heuristic that works well in practice ( for further discussion , see [ 3 ] ) — and has been followed by us — chooses the number of clusters corresponding to the first decisive local maximum over all the models considered . 4 . Summarization
Summarizing a document and summarizing a cluster of documents are different tasks . As a matter of fact , documents of the same cluster have common relevant expressions such as Community agricultural Regulations or Common agricultural policy , rather than long sentences which are likely to occur in just one or two documents . Then , summarizing topics seems adequate to disclose the core content of each cluster . Cluster topics correspond to the most important REs in the cluster . Let the cluster from where we want to extract its topics be the “ target cluster ” . In order to extract it , first the REs of the parent node of the target cluster are ordered according to the following criterion : the value given by is assigned to the th RE of the parent node , where
( 18 ) have the same meaning as in ( 13 ) ; and is given by otherwise
( 19 ) Thus , corresponds to a filter that “ eliminates ” REs cohesion value ( see Sect . 2 ) is lower whose than . These REs , eg , Consid´erant que , and in particular , etc . , are not informative for IR/TextMining11 . Most of the times , these REs are previously eliminated , when selecting the informative REs for calculating the covariance matrix ; however it may not happen in case of a multilingual set of documents ( see Subsect . 25 ) corresponds to the most important RE . For example , according with this criterion , the 15 most important REs of the initial cluster ( the 11The empirically defined value for
So , the largest was
. one containing all documents ) are the following : Nomenclatura Combinada ( Combined Nomenclature ) , nomenclature combin´ee , Member States , combined nomenclature , ´Etats membres ( Member states ) , Council Regulation , produtos agr´ıcolas ( agricultural products ) , produits agricoles , utilizac¸˜ao racional ( rational use ) , nomenclature tarifaire ( tariff nomenclature ) , autoridades aduaneiras ( customs authorities ) , estˆancia aduaneira ( customs office ) , Common Customs , indicados na coluna ( indicated in column ) and agricultural products . Now , considering for instance the “ French documents ” as the target cluster , whose parent is the “ initial cluster ” , we cannot “ guarantee ” that produits agricoles will be a topic , since not every French document content is about produits agricoles . On the other hand , the same topic often appears in different documents , written in different forms , ( eg produits agricoles and Produits Agricoles ) . Hence , according , the 15 most important REs of the target cluster to occurring in at least of its documents are put in a list . From this list , the REs with value not lower than value obtained from that 1/50 of the maximum list , are considered topics . 5 . Results
Sub ELIF is a multilingual corpus with 108 documents per language ( English , Portuguese and French ) . For each document there are two other documents which are translations to the other languages . From Table 2 we can see the hierarchical tree of clusters obtained by this approach . Custer 0 indicates the “ initial cluster ” containing all documents of the corpus . Now we present the topics extracted from each cluster . Cluster 1 : European Communities , Member States , EUROPEAN COMMUNITIES , Council Regulation , Having regard to Council Regulation , Having regard to Council and Official Journal . Cluster 2 : Comunidade Europeia ( European Community ) , Nomenclatura Combinada , COMUNIDADES EUROPEIAS and directamente aplic´avel ( directly applicable ) . Cluster 3 : Communaut´e europ´eenne , nomencla´Etats membres , COMMUNAUT ´ES EUture combin´ee , ROP ´EENNES and directement applicable . Cluster 1.1 : rational use of energy , energy consumption and rational use . Cluster 1.2 : agricultural products , Official Journal , detailed rules , Official Journal of the European Communities , proposal from the Commission , publication in the Official Journal and entirely and directly . Cluster 1.3 : combined nomenclature , Common Customs , customs authorities , No 2658/87 , goods described , general rules , appropriate CN , Having regard to Council Regulation , tariff and statistical and Customs Code .
Cluster
1 2 3 1.1 1.2 1.3 2.1 2.2 2.3 3.1 3.2 3.3
Main topic european communities Comunidade Europeia Communaut´e europ´eenne rational use of energy agricultural products combined nomenclature economia de energia produtos agr´ıcolas politique ´energ´etique produits agricoles nomenclature combin´ee
Nomenclatura Combinada
Table 2 . Evaluation of the clusters .
Correct# Total# Actcorr# 108 107 108 20 21 51 21 21 52 22 21 53
108 107 109 23 27 58 26 25 56 26 27 56
108 108 108 23 27 58 23 27 58 23 27 58
Prec.( % ) Rec.( % ) 100 99.1 100 86.9 77.8 87.9 91.3 77.8 89.7 95.7 77.8 91.4
100 100 99.1 86.9 77.8 87.9 80.8 84 92.9 84.6 77.8 94.6
Cluster 2.1 : economia de energia ( energy saving ) , utilizac¸˜ao racional , racional da energia ( rational of energy ) and consumo de energia ( energy consuming ) . Cluster 2.2 : produtos agr´ıcolas , Comunidades Europeias , Jornal Oficial ( Official Journal ) , directamente aplic´avel , COMUNIDADES EUROPEIAS , Jornal Oficial das Comunidades ( Official Journal of the Communities ) , directamente aplic´avel em todos os Estados membros ( directly applicable to all Member States ) , publicac¸ ˜ao no Jornal Oficial ( publication in the Official Journal ) , publicac¸ ˜ao no Jornal Oficial das Comunidades and Parlamento Europeu ( European Parliament ) . Cluster 2.3 : Nomenclatura Combinada , autoridades aduaneiras , indicados na coluna , mercadorias descritas ( goods described ) , informac¸ ˜oes pautais vinculativas ( binding tariff informations ) , Aduaneira Comum ( Common Customs ) , regras gerais ( general rules ) , c´odigos NC ( NC codes ) and COMUNIDADES EUROPEIAS . Cluster 3.1 : politique ´energ´etique ( energy policy ) , rationnelle de ´energie and l’utilization rationnelle . Cluster 3.2 : produits agricoles , organisation commune ( common organization ) , organisation commune des march´es ( common organization of the markets ) , directment applicable , Journal officiel , Journal officiel des Communaut´es and COMMUNAUT ´ES EUROP ´EENNES . combin´ee , autorit´es douani`eres ( customs authorities ) , nomenclature tarifaire , No 2658/87 , marchandises d´ecrites ( goods described ) , tarifaire et statistique ( tariff and statistical ) and COMMUNAUT ´ES EUROP ´EENNES . 51 Discussion
Cluster 3.3 : nomenclature
Clusters 1 , 2 and 3 were proposed by MBCA ( function mclust ) considering EEV model ( Equal volume , Equal shape , Variable orientation ) the best model , ( see Table 1 ) . EEV model was also considered best model when the subclusters of clusters 1 , 2 and 3 were calculated .
In Table 2 , column Main topic means the most relevant topic according to ( 18 ) obtained for the cluster indicated by column Cluster ; by Correct # we mean the correct number of documents in the corpus for the topic in Main topic ; Total# is the number of documents considered to belong to the cluster by our approach ; Act . corr.# is the number of documents correctly assigned to the cluster ; Prec.( % ) and Rec.( % ) are Precision and Recall .
Although the original texts of the sub ELIF corpus are classified by main topic areas , eg , Agriculture , Energy — Rational use , etc . , we have removed that information from the documents before extracting the REs using LocalMaxs algorithm , as we wanted to test our approach for clustering usual documents . Although , the topics shown in the previous lists capture the core content of each cluster . However , each cluster is not a perfect “ translation ” of the “ corresponding ” clusters in the other languages . Some reasons may help to explain why . Thus , because in Portuguese we write Estado Membro ( just one word ) for Member State , this concept is not extracted in Portuguese since LocalMaxs does not extract unigrams . Then , Estado Membro is not in cluster 2 . On the other hand , not every RE has the same number of occurrences of the corresponding RE in the other languages . For example , there are 55 occurrences of Nomenclatura Combinada , 54 of nomenclature combin´ee , 45 of combined nomenclature and 10 of Combined Nomenclature . Then , under the criterion used for extracting topics ( see Sect . 4 ) , 45 occurrences is less than 50 % of the 108 documents of the cluster 1 . Therefore combined nomenclature is not considered a topic in cluster 1 .
As we keep the original words in text , the same topic may be shown in different forms , eg , EUROPEAN
COMMUNITIES and European Communities in cluster 1 . Though some REs are not topics or just weakly informative expressions , such as Journal officiel or general rules , we think that about 80 % of these REs can be considered topics/subtopics , exposing the core content of the clusters . Since our approach is not oriented for any specific language , we believe that different occurrences for the same concept in the three different languages , are the main reason for different Precision and Recall scores comparing “ corresponding ” clusters . Precision and Recall values for clusters 1.2 , 2.2 and 3.2 are relatively low . However , we believe that using larger corpora , better results will be obtained . As has been said in Sect . 3.1 , the bigger the BIC value , the stronger the evidence of clustering . Then BIC can be used to decide whether to “ explore ” sub clusters or not . However we did not work on a criterion to decide that yet . 6 . Related work
Some known approaches for extracting topics and relevant information from documents use morpho syntactic information , eg , TIPSTER [ 8 ] . So , these approaches would need specific morpho syntactic information in order to extract topics from documents in other languages , and that information might not be available . In [ 1 ] , a topic detection system is presented and Natural Language Processing techniques are also used to identify key entities . In [ 5 ] , a multi document summarizer , called MEAD is presented . It generates summaries using cluster topic detection and tracking system . However , in this approach , topics are unigrams . Though many uniwords have precise meanings , multiword topics are usually more informative and specific than unigrams . For example , human is too generic and vague , but human rights is much more precise . Joe Zhou [ 9 ] suggests automatic topic oriented twoword terms , based on mutual information scores for adjacent words . For multiword terms , mutual information score is calculated for non adjacent words . A threshold is set to decide if terms are important or not . We prefer to avoid thresholds in this phase , by using LocalMaxs approach , since there are relevant terms with lower mutual information scores than other terms and then , a threshold may reject a relevant term and elect a non relevant one . In our approach , the size of the covariance matrix for calculating the principal components , depends on the number of documents to cluster . This may be a limitation . We will work on this problem . 7 . Conclusions
This paper presents an unsupervised statistics based and language independent approach for document clustering and topic extraction . We applied it to multilingual corpus , using just information extracted from it . No pre defined topics , features or descriptors were used . Thousands of REs were extracted by LocalMaxs algorithm . They were then transformed and aggregated into a small set of new features ( components ) , which — according to the results obtained — showed good document classification power . The best number of clusters was automatically calculated by Model Based Cluster Analysis and the results obtained led to a rather precise clustering of the documents . Thus , the number of clusters was not left to the user choice , as it might correspond to an unnatural clustering . Although we tested this approach on a small corpus ( 872,795words ) , the results are encouraging , since about 80 % of the clusters REs are sufficiently informative for being taken as topics of documents in the obtained document clusters . This lead us to believe that topics , rather than long sentences belonging to just one or two documents , are adequate to define clusters core content . In future work , we want to include informative unigrams in order to enrich the base feature set and therefore obtain better results . References [ 1 ] C . Clifton and R . Cooley . Topcat : Data mining for topic identification in a text corpus . In Principles of Data Mining and Knowledge Discovery , pages 174–183 , 1999 . [ 2 ] Y . Escoufier and H . L’Hermier . A propos de la comparaison graphique des matrices de variance . Biometrischc Zeitschrift , 20(5):477–483 , 1978 . [ 3 ] C . Fraley and A . E . Raftery . How many clusters ? which clustering method ? answers via model based clustering analysis . The computer Journal , 41:578–588 , 1998 . [ 4 ] R . A . Johnson and D . W . Wichern . Applied Multivariate Statistical Analysis . Prentice Hall International , second edition edition , 1988 . [ 5 ] D . R . Radev , J . Hongyan , and B . Makgorzata . Centroid based summarization of multiple documents : sentence extraction , utility based evaluation , and user studies . In Proceedings of the ANLP/NAACL Workshop on Summarization , 2000 . [ 6 ] J . F . Silva , G . Dias , S . Guillor´e , and G . P . Lopes . Using LocalMaxs algorithm for the extraction of contiguous and noncontiguous multiword lexical units . In Progress in Artificial Intelligence , volume 1695 of Lecture Notes in Artificial Intelligence , pages 113–132 . Springer Verlag , 1999 . [ 7 ] J . F . Silva and G . P . Lopes . A local maxima method and a fair dispersion normalization for extracting multiword units . In Proceedings of the 6th Meeting on the Mathematics of Language , pages 369–381 , Orlando , July 1999 . [ 8 ] Y . Wilks and R . Gaizauskas . Lasie junps the gate . In Natural Language Information Retrieval , pages 200–214 . Kluwer Academic Publishers , 1999 . [ 9 ] J . Zhou . Phrasal terms in real world ir applications . In Natural Language Information Retrieval , pages 225–259 . Kluwer Academic Publishers , 1999 .
