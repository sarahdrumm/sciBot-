Text Clustering Based on Good Aggregations
Andreas Hotho and Alexander Maedche and Steffen Staab
Institute AIFB , University of Karlsruhe , 76128 Karlsruhe , Germany faho , ama , sstg@aifbuni karlsruhede http://wwwaifbuni karlsruhede/WBS
Abstract
Text clustering typically involves clustering in a high dimensional space , which appears difficult with regard to virtually all practical settings . In addition , given a particular clustering result it is typically very hard to come up with a good explanation of why the text clusters have been constructed the way they are . In this paper , we propose a new approach for applying background knowledge during preprocessing in order to improve clustering results and allow for selection between results . We preprocess our input data applying an ontology based heuristics for feature selection and feature aggregation . Thus , we construct a number of alternative text representations . Based on these representations , we compute multiple clustering results using K Means . The results may be distinguished and explained by the corresponding selection of concepts in the ontology . Our results compare favourably with a sophisticated baseline preprocessing strategy .
1 Introduction
With the abundance of text documents through World Wide Web and corporate document management systems , the dynamic partitioning of texts into previously unseen categories ranks top on the priority list for all business intelligence systems . However , current text clustering approaches still suffer from major problems that greatly limit their practical applicability .
First , text clustering is mostly seen as an objective method , which delivers one clearly defined result , which needs to be “ optimal ” in some way . This , however , runs contrary to the fact that different people have quite different needs with regard to clustering of texts , because they may view the same documents from completely different perspectives ( eg , a business view vs . a technical view ; also cf . ( Macskassy et al . , 1998) ) . Thus , what is needed are subjective criteria that allow for a diversity of views from which to look at the clustering task .
Second , text clustering typically is a clustering task working in a high dimensional space where each word is seen as a potential attribute for a text . Empirical and mathematical analysis , however , has shown that — in addition to computational ineffiencies — clustering in high dimensional spaces is very difficult , because every data point tends to have the same distance from all other data points ( cf . ( Beyer et al . , 1999) ) .
Third , text clustering per se is often rather useless , unless it is combined with an explanation of why particular texts were categorized into a particular cluster . Ie one output desired from clustering in practical settings is the explanation of why a particular cluster result was produced rather than the result itself . A common method for producing explanations is the learning of rules based on the cluster results . Again , however , this approach suffers from the high number of features chosen for computing clusters .
1
Though there are of course different approaches for clustering , simple ones like K Means or sophisticated ones ( like ( Bradley et al . , 1998) ) , based on the consideration just mentioned we found that virtually all algorithms working on large feature vectors will eventually face the same principal problems without really approaching the matters of subjectivity and explainability . Hence , our aim was to consider different views onto the data , ie different representations1 of the same set of texts , from which alternative clustering results may be derived .
The principal idea of our approach , COSA ( Concept Selection and Aggregation ) , is to use a simple , core ontology for restricting the set of relevant document features and for automatically proposing good aggregations . The aggregations are then exploited by the standard clustering algorithm K Means . More precisely , we have compiled a heterarchy of concepts2 that is used by a heuristic search algorithm to automatically construct a set of aggregations . The basic criteria of COSA include the computation of support for particular concepts and the top down navigation of the heterarchy in a greedy manner . Based on COSA , a set of clustering results is produced without interaction by a human user of the system . The user may then decide to prefer the one over the other clustering result based on the actual concepts used for clustering as well as on standard quality measures ( such as the silhouette measure ( Kaufman & Rousseeuw , 1990) ) .
In this paper , we first explain our overall approach working through a simplifying example of one of our applications . Then , we briefly formalize our notion of ontology ( Section 3 ) . We continue describing COSA as well as two baseline preprocessing strategies ( Section 4 ) . These three are compared in our experimental evaluation ( Section 5 ) . Furthermore , from our experiments we have derived some less formal results that we describe in our “ Lessons Learned ” section . Before we conclude , we relate to other work .
2 Application example
Our ontology based text clustering approach has been implemented in a web based knowledge management application supporting information brokering , viz . the FZI Broker ( cf . ( Hotho et al . , 2001) ) . The underlying idea of FZI Broker is that members of a research group ( like FZI ) share a common ontology ( or vocabulary ) with common concepts and conceptual relations . FZI Broker supports the collaborative instantiation of these common interests by ( i ) collecting relevant documents for a given ontology , ( ii ) presenting the collected documents to the users , and ( iii ) allowing the definition of metadata of documents . As an important component of this application we provide the user a set of different views onto a repository of crawled documents taking advantage of supervised document categorization and — relevant here — ontology based text clustering .
Though standard mechanism for text clustering are well known , in our application they suffered from the principal problems we described above : First , background knowledge does not influence the interpretation of structures found by the clustering algorithms . Rather , irrelevant of the background knowledge existing clustering methods only compute one “ optimal ” result . Second , the term/concept vectors are too large . Therefore clustering takes place within a high dimensional vector space leading to undesirable mathematical consequences , viz . all document pairs are similarly ( dis )similar . Thus , clustering becomes impossible and yields no recognizable results ( Beyer et al . , 1999 ) . Third , it is hard for the user to understand the differences between clusters .
Let us work through a detailed example to show you the problems and to give you an intuition
1Motivated by the database point of view , we also call derived text representations “ aggregations ” . 2A heterarchy of concepts is a kind of “ taxonomy ” where each term may have multiple parents and — of course — multiple children .
2
Figure 1 : Sample web pages for the proposed solution . In Table 1 you find a sample of ( abbreviated ) concept vectors representing the web pages depicted in Figure 1 ( some of the matching concepts are highlighted in the web pages ) . In Figure 2 one may recognize the corresponding concepts highlighted in an excerpt of the FZI Broker ontology . Our simplifying example shows the principal problem of vector representations of documents : The tendency that spurious appearance of concepts ( or terms ) rather strongly affects the clustering of documents . The reader may bear in mind that our simplification is so extensive that practically it does not appear in such tiny settings , but only when one works with large representations and large document sets . In our simplifying example the appearance of concepts PUBLICATION , KNOWLEDGE MANAGEMENT , and DISTRIBUTED ORGANIZATION is spread so evenly across the different documents that all document pairs exhibit ( more or less ) the same similarity . Corresponding squared Euclidian distances for the example document pairs ( 1,2 ) , ( 2,3 ) , ( 1,3 ) leads to values of 2 , 2 , and 2 , respectively , and , hence , to no clustering structure at all .
Document # PUBLICATION
KNOWLEDGE MANAGEMENT DISTRIBUTED ORGANIZATION
1 ( “ OTK ” )
2 ( “ AIFB Publications ” )
3 ( “ IICM Publications ” )
0 2 1
1 2 0
1 1 1
Table 1 : Concept vector representations for 3 sample documents ( cf . Fig 1 )
When one reduces the size of the representation of our documents , eg by projecting into an sub
3
ROOT
PUBLICATION
EVENT
PERSON
TOPIC
RESEARCH TOPIC
THESIS
JOURNAL
KNOWLEDGE MANAGEMENT
DISTRIBUTED ORGANIZATION
Figure 2 : A sample ontology space , one focuses on particular concepts and one may focus on the significant differences that documents exhibit with regard to these concepts . For instance , when we project into a document vector representation that only considers the two dimensions PUBLICATIONS and KNOWLEDGE MANAGEMENT , we will find that document pairs ( 1,2 ) , ( 2,3 ) , ( 1,3 ) have squared Euclidean distances of 1 , 1 , and 2 . Thus , axis parallel projections like in this example may improve the clustering situation . In addition , we may exploit the ontology . For instance , we select features according to the taxonomy , choosing , eg , RESEARCH TOPIC instead of its subconcepts KNOWLEDGE MANAGEMENT and DISTRIBUTED ORGANIZATION to built our aggregation . Then , the entries for KNOWLEDGE MANAGEMENT and DISTRIBUTED ORGANIZATION are added into one vector entry resulting in squared Euclidean distances between pairs ( 1,2 ) , ( 2,3 ) , ( 1,3 ) of 2 , 0 , and 2 , respectively . Thus , documents 2 and 3 can be clustered together , while document 1 falls into a different cluster .
The algorithm GenerateConceptViews described below acts as a preprocessing step for clustering . GenerateConceptViews chooses a set of interpretable and ontology based aggregations leading to modified text representations . Conventional clustering algorithms like K Means may work on these modified representations producing improved clustering results . Because the size of the vector representation is reduced , it becomes easier for the user to track the decisions made by the clustering algorithms . Because there are a variety of aggregations , the user may choose between alternative clustering results . For instance , there are aggregations such that publication pages are clustered together and the rest is set aside or aggregations such that web pages about KNOWLEDGE MANAGEMENT are clustered together and the rest is left in another cluster . The choice of concepts from the taxonomy thus determines the output of the clustering result and the user may use a view like Figure 2 in order to select and understand differences between clustering results .
In the following we first formalize our notion of a core ontology , before we demonstrate our preprocessing and clustering strategy for obtaining subjective aggregations using an ontology .
3 Heterarchy and Core Ontology
A core ontology in our framework is defined by :
Definition 1 ( Core Ontology ) A core ontology is a sign system := ; F ; C ;   ; ROOT , which consists of ffl A lexicon : The lexicon contains a set of natural language terms .
4 ffl A set of concepts C . ffl The reference function F with F : 2 7! 2C . F links sets of terms f ig fl to the set of concepts they refer to . In general , one term may refer to several concepts and one concept may be refered to by several terms ( eg , “ boat hire ” and “ boat rental ” may refer to the concept BOATHIRE ) . The inverse of F is F 1 . ffl A heterarchy   : Concepts are taxonomically related by the directed , acyclic , transitive , reflexive relation   , (   fl C C ) .   HOTEL ; ACCOMODATION means that HOTEL is a subconcept of ACCOMODATION . ffl A top concept ROOT 2 C . For all C 2 C it holds :   C ; ROOT .
The core ontology defines the background knowledge used for preprocessing and selection of relevant views ( ie aggregations ) onto the set of texts . The formulation we have used here roughly corresponds to the basic structures used in the famous WordNet ( Miller , 1995 ) , but the actual ontology we have used is domain specific rather than general as WordNet . In order to easily build new ontologies we have developed a rich framework , which includes support for semi automatic constructing of ontologies from various input ( cf . ( Maedche & Staab , 2001) ) .
4 Document Preprocessing
Documents may be represented by a wide range of different feature descriptions . The most straightforward description of documents relies on term vectors . A term vector for one document specifies how often each term from the document set occurs in that document . The immediate drawback of this basic approach for document preprocessing for clustering is the size of the feature vectors . In our example evaluation , the feature vectors computed by this method were of size 46,947 , which made clustering inefficient and difficult in principle , as described above .
While for supervised learning tasks there exist quite a number of evaluations of how document preprocessing strategies perform ( cf . , eg , ( Fuernkranz et al . , 1998) ) , there are only few corresponding results for unsupervised knowledge discovery tasks like document clustering ( cf . Section 7 ) .
To evaluate our approach , which takes advantage of the background knowledge we provide with our core ontology , we did of course compare against that basic approach for document preprocessing ( referred to by Simple Vector Representation or SiVeR in the following ) . We were aware that due to the problems with clustering in high dimensional space , SiVeR would be handicapped from the very beginning . In order to perform a more competitive comparison , we have decided to include another preprocessing approach in the evaluation .
Hence , in the following we develop , ( i ) , a preprocessing strategy ( cf . Section 4.1 ) based on term vectors reduced to terms considered “ important ” by information retrieval measures , viz . a preprocessing strategy based on term selection ; ( ii ) , a more comprehensive approach using the background knowledge available in the ontology . In particular , we apply techniques from natural language processing to map terms to concepts ( cf . Section 4.2 ) and select between aggregations navigating topdown in the heterarchy .
4.1 Preprocessing Strategy : Term Selection ( TES )
Term selection , the second approach we use here for preprocessing , is based on the feature vectors from SiVeR , but focuses on few terms , hence , it produces a low dimensional representation . Selection
5 of terms is based on the information retrieval measure f idf :
Definition 2 ( tfidf ) Let tf i ; j be the term frequency of term j in a document di 2 D ; i = 1 ; : : : ; . Let df(j ) be the document frequency of term j that counts in how many documents term j appears . Then f idf ( term frequency / inverted document frequency ) of term j in document i is defined by : tfidf i ; j = tf i ; j   g df j :
( 1 )
Tfidf weighs the frequency of a term in a document with a factor that discounts its importance when it appears in almost all documents . Therefore terms that appear too rarely or too frequently are ranked lower than terms that hold the balance and , hence , are expected to be better able to contribute to clustering results .
For TES , we produce the list of all terms contained in one of the documents from the corpus D except of terms that appear in a standard list of stopwords . Then , TES selects the d best terms j that maximize W j ,
W j := Xi:=1: : : f idf i ; j ;
( 2 ) and produces a d dimensional vector for document di containing the f idf values , f idf i ; j , for the d best terms .
4.2 Preprocessing Strategy : Concept Selection and Aggregation ( COSA )
Our approach for preprocessing , concept selection and aggregation ( COSA ) , involves two stages . First , COSA maps terms onto concepts using a shallow and efficient natural language processing system . Second , COSA uses the concept heterarchy to propose good aggregations for subsequent clustering .
421 Mapping terms to concepts
The mapping of terms to concepts in our approach relies on some modules of SMES ( Saarbr¨ucken Message Extraction System ) , a shallow text processor for German ( cf . ( Neumann et al . , 1997) ) . SMES components exploited by COSA comprise a tokenizer based on regular expressions and a lexical analysis component including a word and a so called domain lexicon ( the domain specific part of the lexicon as defined in Definition 1 ) .
The tokenizer scans the text in order to identify boundaries of words and complex expressions like “ $20.00 ” or “ United States of America ” , and to expand abbreviations . The word lexicon contains more than 120,000 stem entries . Lexical analysis uses the word lexicon , ( i ) , to perform morphological analysis of terms , i . e . the identification of the canonical common stem of a set of related word forms and the analysis of compounds and , ( ii ) , to recognize named entities . Thus , as described in Definition 1 is a set defined by the tokenizer , the word lexicon and the analysis procedures of the lexical analysis component . The domain lexicon contains the mappings from word stems to concepts , ie together with the other modules it represents the function F as defined in Definition 1 . By this way , eg , the expression “ Hotel Schwarzer Adler ” is associated with the concept HOTEL .
Based on this input , each document is represented by a vector of concepts , each entry specifying the frequency that a concept occurs in the document .
6
422 A heuristic for generating good aggregations
Because synonyms are mapped to common concepts and because in all realistic document sets there are more terms than concepts , the sizes of concept vectors representing documents are already considerably smaller than the sizes of term vectors produced by SiVeR . Still , realistic settings require at least some hundreds or thousands of concepts , which yields simply too many dimensions for practical clustering .
Therefore , we have looked for ways to heuristically reduce the number of features and , thus , the number of dimensions of the clustering space . The principal idea of our algorithm GenerateConceptViews is to navigate the heterarchy top down and build aggregations from the concept parts that achieve good overall support ( cf . Algorithm 1 below ) . The underlying idea of using the notion of an overall support is that an important concept is typically supported by its occurrence in documents . Thus , in the algorithm we propose that a refinement of a given important concepts into its subconcepts enables a more detailed description , viz . new , important and clustering relevant aggregations may be derived by this strategy .
For instance ,
The variable Age da is defined to describe the current features used in concept vectors , hence the the current agenda could be current aggregation onto the document set . [ ACCOMODATION , VACATION , SIGHT SEEING ] . A aggregation is altered , by taking the frontmost , ie the concept with the most support , from the agenda ( lines 4 and 5 ) and branching — if it is not a leaf concept — into its subconcepts ( line 10 ) . In order to restrict branching , we only perform binary splits at a time . Continuing the example just given , the first feature for the input space is described by the concept ACCOMODATION and when ACCOMODATION has the subconcepts [ HOTEL ; GUEST HOUSE ; YOUTH HOSTEL ] , we will select the subconcept that has the highest support of these three ( line 11 ) , eg HOTEL , and aggregate the other two subconcepts into one feature , viz . [ GUEST HOUSE , YOUTHHOSTEL ] ( line 12 ) . The list [ GUEST HOUSE ; YOUTH HOSTEL ] is then treated almost like a proper , atomic concept . HOTEL and [ GUEST HOUSE ; YOUTH HOSTEL ] are both inserted into Age da ordering all elements according to their support ( lines 13 14 ) . The result might be , eg , [ VACATION ; [ GUEST HOUSE ; YOUTH HOSTEL ] ; HOTEL ; SIGHT SEEING ] .
Thereby , direct support of a concept C in a document di is defined by the concept frequency cf i ; C that one of the terms F 1 fCg appears in di . Complete support includes also consideration of all the subconcepts : and
Support i ; C := XB2fBj  B;C g cf i ; B ;
Support C := Xi:=1: : :
Support i ; C :
( 3 )
( 4 )
If the age da has length d 1 due to the last binary split of one of its elements , age da is shortened by the element with least support ( line 15 ) . If the age da has the correct number of features , it is added to the output set describing a selection of concepts , hence an aggregation that represents documents by d dimensional concept vectors ( line 17 ) .
Thus , Algorithm 1 zooms into those concepts that exhibit strongest support , while taking into account the support of subconcepts . Finally , it proposes sets of aggregations for clustering that imply a d dimensional representation of documents by concept vectors . Each entry of a vector specifies how often the concept ( or its subconcepts ) appears in the corresponding document .
7
Algorithm 1 ( GenerateConceptViews ) Input : number of dimensions d ; Ontology with top concept ROOT ; document set D
1 begin 2 Age da := [ ROOT ] ; 3 repeat
4
5
7
8
9
10
11
12
13
14
15
16
17
E e := First Age da ; Age da := Rest Age da ; if Leaf E e then c i e := FALSE ; else if Atom E e then E e := Subconcepts E e ; fi ; ewE e := BestSupportElem E e ; Re E e := E e ewE e ; if :Empty Re E e then Age da := SortInto Re E e ; Age da ; fi ; Age da := SortInto ewE e ; Age da ; if Length Age da > d then Age da := Butlast Age da ; fi ; fi ; if Length Age da = d then Output Age da ; fi ; until c i e = FALSE ;
18 19 end
Output : Set of lists consisting of single concepts and lists of concepts , which describe aggregations onto the document corpus D . Auxiliary functions used :
Subconcepts C Support C Support i C SortInto E e e ; i 2
BestSupportElem(List ) [ E e e ] [ E e e ; i ] First i ; Rest i Atom E Leaf E i E Length i Butlast i returns an arbitrarily ordered list of direct subconcepts of C . cf . equation 4 . is the sum over all concepts C in i C of Support C . sorts E e e , which may be a single concept or a list of concepts , as a whole into i 2 ordering according to Support E e e and removing redundant elements . returns the E e e of i with maximal Support E e e . constructs list with one E e e . list constructor extending i such that E e e is first . are the common list processing functions . returns true when E is not a list . returns true when E is a concept without subconcepts . removes element E from i . returns the length of i . returns a list identical to i , but excluding the last element .
8
4.3 A note on absolute vs . logarithmic values
The document representations described so far used absolute frequency values for concepts or terms ( possibly weighted by idf ) . Considering that the occurrence of terms forms a hyperbolic distribution and , hence , most terms appear only rarely , using the logarithmic value   g x 1 instead of the absolute value x itself seemed reasonable to improve clustering results . Indeed , for all preprocessing strategies given here , we found that results were only improved compared to absolute values . Hence , all results presented subsequently assume the logarithmic representation of term or concept frequencies .
5 Evaluation
This section describes the evaluation of applying K Means to the preprocessing strategies SiVeR , TES , and COSA introduced above .
Setting
We have performed all evaluations on a document set from the tourism domain ( cf . ( Staab et al . , 1999) ) . For this purpose , we have manually modeled an ontology consisting of a set of concepts C ( j C j = 1030 ) , and a word lexicon consisting of 1950 stem entries ( the coverage of different terms by SMES is much larger! ) . The heterarchy   has an average depth of 4:6 , the longest uni directed path from root to leaf is of length 9 .
Our document corpus D has been crawled from a WWW provider for tourist information ( URL : http://wwwall in allde ) consisting now of 2234 HTML documents with a total sum of over 16 million words . The documents in this corpus describe actual objects , like locations , accomodations , facilities of accomodations , administrative information , and cultural events .
Silhouette Coefficient
Our aim was to compare SiVeR , TES , and COSA for a wide range of parameter settings . In order to be rather independent from the number of features used for clustering and the number of clusters produced as result , our main comparisons refer to the silhouette coefficient ( cf . ( Kaufman & Rousseeuw , 1990) ) :
Definition 3 ( Silhouette Coefficient ) Let D = fD1 ; : : : ; Dkg describe a clustering result , ie it is an exhaustive partitioning of the set of documents D . The distance3 of a document d 2 D to a cluster Di 2 D is given as di d ; Di = 2Di di d ; j Di j
:
( 5 )
Let further be a d ; D = di d ; D  the distance of document d to its cluster D  ( d 2 D  ) , and di d ; Di the distance of document d to the nearest neighbouring b d ; D = i Di2D ;d =2Di cluster .
The silhouette d ; D of a document d 2 D is then defined as : axfa d ; D ; b d ; D g 3We use the standard euclidean distance for computing the silhouette coefficient . d ; D = b d ; D   a d ; D
:
( 6 )
9
The silhouette coefficient SC D as :
SC D = 2D ; D j D j
:
( 7 )
The silhouette coefficient is a measure for the clustering quality , that is rather independent from the number of clusters , k . Experiences , such as documented in ( Kaufman & Rousseeuw , 1990 ) , show that values between 0.7 and 1.0 indicate clustering results with excellent separation between clusters , viz . data points are very close to the center of their cluster and remote from the next nearest cluster . For the range from 0.5 to 0.7 one finds that data points are clearly assigned to cluster centers . Values from 0.25 to 0.5 indicate that cluster centers can be found , though there is considerable “ noise ” , ie there are many data points that cannot be clearly assigned to clusters . Below a value of 0.25 it becomes practically impossible to find significant cluster centers and to definitely assign the majority of data points .
For comparison of the three different preprocessing methods we have used standard K Means4 . However , we are well aware that for high dimensional data approaches like ( Bradley et al . , 1998 ) may improve results — very likely for all three preprocessing strategies . However , in preliminary tests we found that in the low dimensional realms where the silhouette coefficient indicated reasonable separation between clusters , quality measures for standard and improved K Means coincided .
The general result of our evaluation using the silhouette measure was that K Means based on COSA preprocessing excelled the comparison baseline , viz . K Means based on TES , to a large extent . K Means based on SiVeR was so strongly handicapped by having to cope with overly many dimensions that its silhouette coefficient always approached 0 — indicating that no reasonable clustering structures could be found .
One exemplary , but overall characteristic diagramm depicted in Figure 3 shows the silhouette coefficient for a fixed number of features used ( namely 15 ) and a fixed number of clusters produced ( namely 10 ) . It does so for K Means based on SiVeR , for K Means based on TES , and for K Means based on COSA . The results for SiVeR are strictly disappointing . TES is considerably better , but it still yields a silhouette coefficient that indicates practically non existent distinctions between clusters . COSA produces for this parameter setting 89 aggregations . We found that the best aggregations produced from COSA delivered clustering results with silhouette measures of up to 0.48 — indicating indeed very reasonable separation between clusters .
Mean Squared Error
To base our comparisons not only on one single measure we also did a study , evaluating the mean squared error ( MSE ) for our approach and the corresponding baseline results . The mean squared error is a measure of compactness of a given clustering and is defined as follows .
Definition 4 ( MSE ) The overall mean squared error SE for a given clustering D = fD1 ; : : : ; Dkg is defined as
SE D = k
Xi=1
SE Di :
The mean squared error for one cluster SE Di is defined as
4We use several well know heuristics to derive a good starting solution for K Means .
SE Di = X 2Di di ; Di
2 :
10
( 8 )
( 9 )
C S
0,5
0,45
0,4
0,35
0,3
0,25
0,2
0,15
0,1
0,05
0
1 8
5 1
2 2
COSA
TES
SiVeR
1 7
8 7
5 8
3 4
6 3
9 2 aggregation number
0 5
7 5
4 6
Figure 3 : Comparing TES with 89 aggregations produced by COSA for k = 10 ; d = 15 .
E S M
6000
5000
4000
3000
2000
1000
0
1 8
5 1
2 2
COSA
TES
1 7
8 7
5 8
3 4
6 3
9 2 aggregation number
0 5
7 5
4 6
Figure 4 : Comparing TES and 89 aggregations produced by COSA ; k = 10 ; d = 15 .
( where Di is the centroid of cluster Di )
We found that also for the MSE measure K Means based on COSA compared favorably against K Means based on TES . 49 of COSA ’s aggregations are worse , but 40 of them are considerably better than the TES baseline . Figure 4 shows the corresponding results with a baseline for TES at 3240 , but the best values for COSA exhibit a MSE of only 1314 .
Varying number of features d and clusters k
Then we explored how COSA and TES would fare when varying the number of features used and the number of clusters produced by K Means .
Figure 5 depicts the dependency between the number of features , d , used and the preprocessing method for a fixed number of clusters produced by K Means , viz . k = 10 . The line for COSA shows the silhouette coefficient for the best aggregation from the ones generated by GenerateConceptViews . We see that for TES and COSA the quality of results decreases — as expected — for the higher dimensions ( cf . ( Beyer et al . , 1999) ) , though COSA still compares favorably against TES . We have not included the lower bound of COSA in Figure 5 . The reason is that — so far — we have not been very attentive to optimize GenerateConceptViews in order to eliminate the worst ag
11
C S
0,7
0,6
0,5
0,4
0,3
0,2
0,1
0
COSA
TES
10
15
20
30
50
100 dimension
Figure 5 : Comparing TES and the best aggregation of COSA ; k = 10 ; d = 10 ; 15 ; 30 ; 50 ; 100 .
C S
0,7
0,6
0,5
0,4
0,3
0,2
0,1
0
1 8
5 1
2 2
9 2
6 3
COSA
TES
4 6
1 7
8 7
5 8
2 9
9 9
3 4
0 5
7 5 cluster#
Figure 6 : Comparing TES and the best aggregation produced by COSA ; k = 2 : : : 100 ; d = 15 . gregations up front . This , however , should be easily possible , because we observed that the bad results are produced by aggregations that contain too many overly general concepts like MATERIALTHING or INTANGIBLE .
In our real world application we experienced that it is useful to include the users viewpoint for deriving the number of dimensions with respect to the actual problem . In general one may propose the following upper bound for the number of useable dimensions : The silhouette coefficient decreases below 0.25 using more than 30 dimensions . Thus , using more than 30 dimensions may not be useful , because no meaningful clustering structure may be discovered .
In the next experiments , we have varied the number of clusters , k , between 2 and 100 , while d remained at 15 ( cf . Figure 6 ) . The general result is that the number of clusters does not affect the results produced by COSA and TES very much . The slight increase of the silhouette coefficient is due to a growing number of documents ( up to 30 % ) that cluster exactly at one point . There , viz . at 0 ; : : : ; 0 , all features disappear .
12
Example for Interpretation
In order to provide a more concrete intuition of the type of results returned by GENERATECONCEPTVIEW , we here show the list of concepts that corresponds to the best aggregation for parameters k = 10 and d = 10 and in a silhouette coefficient of 0:598 :
SAUNA , SOLARIUM , TERRACE , BEACH , SEA RESSORT , ACTION AT OBJECT , OVERNIGHT STAY ,
WATER SPORTS , TRAVELING , HOTEL CATEGORY
Comparing some plain lists may already give the user an intuition of how clustering results might be distinguishable ( or not distinguishable if the aggregations are very similar! ) . A better grip at interpretation is however achieved by depicting the relevant parts of the heterarchy as shown in Figure 7 .
Thing
Material_Thing
Facilities_of Accomodation
NonPrivate_ Facilities_of Accomodation
Sauna
Solarium
Terrace
Root
Spatial_Concept
Area
City
Intangible
Event
Action
Information
Hotel_Category
Sea Ressort
Human_Action
Action_at_Object
Beach
Overnight Stay
Traveling
Go_In_For_Sports
Water_Sports
Figure 7 : An example aggregation generated by COSA
Here , one may recognize that NONPRIVATE FACILITIES OF ACCOMODATION and ACTIONs were important concepts used for clustering and distinguishing documents . Interpreting results , we may conjecture that HOTEL CATEGORY ( three , four , five star hotels , etc . ) is a concept , which might perhaps correlate with facilities of the accomodation — a correlation that happens to be not described in the given ontology . Finally , we see SEA RESSORT in this aggregation , which might play a role for clustering or which might occur just because of uninterpretable “ noise ” .
We currently explore GUI possibilities in order to tie the interpretation of clustering results with the navigation of the heterarchy in order to give the user a good grip at different clustering aggregations .
6 Lessons Learned
From our experiments we have learned quite a number of interesting lessons that may be summarized by the following points . First of all , a number of expectations we had , eg , based on our literature research were fulfilled :
13 ffl Clustering in high dimensional space is worse than in fewer dimensions . ffl Clustering results produced in high dimensional space are hard to interpret for humans . Every effort at producing interpretations from there seemed to require projections , aggregation or any other techniques for reducing the dimensions in the end — even though these techniques may incur a loss of information . ffl The labelling of clustering results with selected concepts seems to facilitate the interpretation task for the user . Though we have not performed usability studies to prove this claim , we believe our experiences are strong enough to warrant plausibility .
Some results were not at all obvious from the beginning : ffl Aggregations into a low dimension space do not improve clustering per se . Bad evaluation re sults have been due to aggregations with too many overly general concepts ( like MATERIAL THING or INTANGIBLE ) . ffl Aggregations based on leaf of near leaf concepts yield good evaluation results . For these aggregations we find that a substantial share of documents are represented by~0 , a concept vector where all entries are 0 . The reason is that in particular aggregations , ie seen from a particular part of the ontology , this share of documents is simply irrelevant . ffl Analyzing single clusters that stem from COSA clustering results with the silhouette coefficient , we find clusters that are very well separated from the rest . However , quite regularly we find clusters that cannot be separated from the rest with good quality . We assume that texts in such clusters can only be reasonably interpreted in alternative views .
Our results support the general statement that structure can mostly be found in a low dimensional space ( cf . ( Beyer et al . , 1999) ) . Our proposal is well suited to provide a selected number of aggregations in subspaces exploiting standard K Means and comparing favorably with baselines , like clustering based on d terms ranked by f idf measures . The selected concepts may be used to indicate to the user , which text features were most relevant for the particular clustering results and to distinguish different aggregations .
7 Related Work
All clustering approaches based on frequencies of terms/concepts and similarities of data points suffer from the same mathematical properties of the underlying spaces ( cf . ( Beyer et al . , 1999 ; Hinneburg et al . , 2000) ) . These properties imply that even when “ good ” clusters with relatively small mean squared errors can be built , these clusters do not exhibit significant structural information as their data points are not really more similar to each other than to many other data points . Therefore , we derive the high level requirement for text clustering approaches that they either rely on much more background knowledge ( and thus can come up with new measures for similarity ) or that they cluster in subspaces of the input space .
In general , existing approaches ( eg , ( Agrawal et al . , 1998 ; Hinneburg & Keim , 1999 ) ) on subspace clustering face the dual nature of “ good quality ” . On the one hand , there are sound statistical measures for judging quality . State of the art methods use them in order to produce “ good ” projections and , hence , “ good ” clustering results , for instance :
14 ffl Hinneburg & Keim ( Hinneburg & Keim , 1999 ) show how projections improve the effectiveness and efficiency of the clustering process . Their work shows that projections are important for improving the performance of clustering algorithms . In contrast to our work , they do not focus on cluster quality with respect to the internal structures contained in the clustering . ffl The problem of clustering high dimensional data sets has been researched by Agrawal et al . ( Agrawal et al . , 1998 ) : They present a clustering algorithm called CLIQUE that identifies dense clusters in subspaces of maximum dimensionality . Cluster descriptions are generated in the form of minimized DNF expressions . ffl A straightforward preprocessing strategy may be derived from multivariate statistical data analysis known under the name principal component analysis ( PCA ) . PCA reduces the number of features by replacing a set of features by a new feature representing their combination . ffl In ( Schuetze & Silverstein , 1997 ) , Schuetze and Silverstein have researched and evaluated projection techniques for efficient document clustering . They show how different projection techniques significantly improve performance for clustering , not accompanied by a loss of cluster quality . They distinguish between local and global projection , where local projection maps each document onto a different subspace , and , global projection selects the relevant terms for all documents using latent semantic indexing ( introduced by ( Deerwester et al . , 1990) ) .
Now , on the other hand , in real world applications the statistically optimal projection , such as used in the approaches just cited , often does not coincide with the projection most suitable for humans to solve a particular task , such as finding the right piece of knowledge in a large set of documents . Users typically prefer explicit background knowledge that indicates the foundations on which a clustering results has been achieved .
Hinneburg et al . ( Hinneburg et al . , 1999 ) consider this general problem a domain specific optimization task . Therefore , they propose to use a visual and interactive environment to derive meaningful projections involving the user . Our approach may be seen to solve some part of task they assign to the user environment automatically , while giving the user some first means to explore the result space interactively in order to select the projection most relevant for her particular objectives .
Finally , we want to mention an interesting proposal for feature selection made in ( Devaney & Ram , 1998 ) . Devaney and Ram describe feature selection for an unsupervised learning task , namely conceptual clustering . They discuss a sequential feature selection strategy based on an existing COBWEB conceptual clustering system . In their evaluation they show that feature selection significantly improves the results of COBWEB . The drawback that Devaney and Ram face , however , is that COBWEB is not scalable like K Means . Hence , for practical purposes of clustering in large document repositories , COSA seems better suited .
8 Conclusion
In this paper we have shown how to include background knowledge in form of a heterarchy in order to generate different clustering aggregations onto a set of documents . We have compared our approach against a sophisticated baseline , achieving a result favourable for our approach . In addition , we have shown that it is possible to automatically produce results for diverging aggregations onto the same input . Thereby , the user can rely on a heterarchy to control and possibly interpret clustering results .
The preprocessing method , COSA , that we propose is a very general one . We have applied our techniques on a high dimensional data set that is not based on text documents , but on a real world
15 customer database with 24:156 customers in the telecommunications domain ( cf . ( Maedche et al . , 2000))5 . Preliminary results of applying our method on this complex transaction oriented database show similar positive results as could be presented here for text clustering .
References
Agrawal , R . , Gehrke , J . , Gunopulos , D . , & Raghavan , P . ( 1998 ) . Automatic subspace clustering of high dimensional data for data mining applications . In Proc . of the ACM SIGMOD . ACM Press .
Beyer , K . , Goldstein , J . , Ramakrishnan , R . , & Shaft , U . ( 1999 ) . When is ‘nearest neighbor’ meaning ful . In Proc . of ICDT 1999 , pages 217–235 .
Bradley , P . , Fayyad , U . , & Reina , C . ( 1998 ) . Scaling clustering algorithms to large databases . In Proc . of KDD 1998 , pages 9–15 . AAAI Press .
Deerwester , S . C . , Dumais , S . T . , Landauer , T . K . , Furnas , G . W . , & Harshman , R . A . ( 1990 ) . Indexing by latent semantic analysis . Journal of the American Society of Information Science , 41(6):391– 407 .
Devaney , M . & Ram , A . ( 1998 ) . Efficient feature selection in conceptual clustering .
In Proc . of
ICML 1997 . Morgan Kaufmann .
Fuernkranz , J . , Mitchell , T . , & Riloff , E . ( 1998 ) . A Case Study in Using Linguistic Phrases for Text Categorization on the WWW . In Proc . of AAAI/ICML Workshop Learning for Text Categorization . AAAI Press .
Hinneburg , A . , Aggarwal , C . , & Keim , D . ( 2000 ) . What is the nearest neighbor in high dimensional spaces ? In Proc . of VLDB 2000 , pages 506–515 . Morgan Kaufmann .
Hinneburg , A . & Keim , D . ( 1999 ) . Optimal grid clustering : Towards breaking the curse of dimen sionality in high dimensional clustering . In Proc . of VLDB 1999 . Morgan Kaufmann .
Hinneburg , A . , Wawryniuk , M . , & Keim , D . ( 1999 ) . Visual mining of high dimensional data . Com puter Graphics & Applications Journal .
Hotho , A . , Maedche , A . , Staab , S . , & Studer , R . ( 2001 ) . SEAL II — the soft spot between richly structured and unstructured knowledge . Universal Computer Science ( JUCS ) In Print .
Kaufman , L . & Rousseeuw , P . ( 1990 ) . Finding Groups in Data : An Introduction to Cluster Analysis .
Wiley , New York .
Macskassy , S . A . , Banerjee , A . , Davison , B . , & Hirsh , H . ( 1998 ) . Human performance on clustering web pages : a preliminary study . In Proc . of KDD 1998 , pages 264–268 . AAAI Press .
Maedche , A . & Staab , S . ( 2001 ) . Ontology learning for the semantic web . IEEE Intelligent Systems ,
16(2 ) .
Maedche , A . , Hotho , A . , & Wiese , M . ( 2000 ) . Enhancing preprocessing in data intensive domains In Data Warehousing and Knowledge Discovery , Second using online analytical processing . International Conference , DaWaK 2000 , London , UK , LNCS , pages 258–264 . Springer .
5The data given in the telecommunications domain has also the property of describing high dimensional spaces .
16
Miller , G . ( 1995 ) . WordNet : A lexical database for english . CACM , 38(11):39–41 .
Neumann , G . , Backofen , R . , Baur , J . , Becker , M . , & Braun , C . ( 1997 ) . An information extraction core system for real world german text processing . In Proc . of ANLP 1997 , pages 208–215 .
Schuetze , H . & Silverstein , C . ( 1997 ) . Projections for efficient document clustering .
In Proc . of
SIGIR 1997 , pages 74–81 . Morgan Kaufmann .
Staab , S . , Braun , C . , D¨usterh¨oft , A . , Heuer , A . , Klettke , M . , Melzig , S . , Neumann , G . , Prager , B . , Pretzel , J . , Schnurr , H P , Studer , R . , Uszkoreit , H . , & Wrenger , B . ( 1999 ) . GETESS — searching the web exploiting german texts . In Proceedings of the 3rd Workshop on Cooperative Information Agents , Uppsala , Sweden , LNCS , pages 113–124 . Springer .
17
