Classification with Degree of Membership : A Fuzzy Approach
Wai Ho Au
Keith CC Chan
Department of Computing
The Hong Kong Polytechnic University
Hung Hom , Kowloon , Hong Kong
E mail : {cswhau , cskcchan}@comppolyueduhk
Abstract terms linguistic to represent
Classification is an important topic in data mining research . It is concerned with the prediction of the values of some attribute in a database based on other attributes . To tackle this problem , most of the existing data mining algorithms adopt either a decision tree based approach or an approach that requires users to provide some userspecified thresholds to guide the search for interesting rules . In this paper , we propose a new approach based on the use of an objective interestingness measure to distinguish interesting rules from uninteresting ones . Using the revealed regularities and exceptions , this approach is especially useful when the discovered rules are presented to human experts for examination because of the affinity with the human knowledge representation . The use of fuzzy technique allows the prediction of attribute values to be associated with degree of membership . Our approach is , therefore , able to deal with the cases that an object can belong to more than one class . For example , a person can suffer from cold and fever to certain extent at the same time . Furthermore , our approach is more resilient to noise and missing data values because of the use of fuzzy technique . To evaluate the performance of our approach , we tested it using several real life databases . The experimental results show that it can be very effective at data mining tasks . In fact , when compared to popular data mining algorithms , our approach can be better able to uncover useful rules hidden in databases .
1 . Introduction
Classification is an important topic in data mining research [ 2 , 9 12 , 24 , 26 ] . The problem is concerned with the mining of a set of production rules that can allow the values of an attribute in a database to be accurately predicted based on those of other attributes [ 1 2 , 16 , 19 , 22 , 24 ] . For example , we are given a customer database with each record characterized by such attributes as income , car owned , and plan subscribed , a rule that could be discovered can be “ 90 % of high income customers who own a Jeep are subscribes of Plan B ; 3 % of all customers have both characteristics . ” The discovery of such a rule could be important to a marketing manager who may , as a result , concentrate on promoting Plan B among high income Jeep owners .
For data mining to be effective , an algorithm should be able to handle linguistic or fuzzy variables . This is because the ability to do so would allow some interesting patterns to be more easily discovered and expressed . For example , if crisp boundaries are defined for “ highincome ” in the above rule , there is a possibility that it may not be interesting at all as the support and confidence measure is dependent to a large extent on the definitions of boundaries . Despite its importance , many data mining algorithms ( eg , [ 1 4 , 16 , 18 19 , 22 , 24 25 ] ) were not developed to handle fuzzy data or fuzzy rules . They were used mainly to deal with categorical and quantitative attributes . In particular , when dealing with quantitative attributes , their domains are usually divided into equalwidth or equal frequency intervals . In most cases , the resulting intervals are not too meaningful and are hard to understand . terms linguistic
To deal with fuzzy data and fuzzy rules , we present a new approach , which employs to represent regularities and exceptions discovered , in this paper . These linguistic terms can be defined as fuzzy sets so that , based on their membership functions , either categorical or quantitative data , can be transformed by fuzzification . To deal with these fuzzified data so as to discover fuzzy rules , this approach utilizes the idea of residual analysis [ 5 8 ] . With it , our approach is able to reveal interesting associations hidden in the database without the need for users’ to supply some subjective thresholds . In other words , unlike many data mining algorithms ( eg , [ 1 4 , 16 , 18 19 , 22 , 24 25 ] ) that only discover rules with consequent consisting only of categorical or discretized crisp boundary quantitative attributes , our approach is able to discover rules with consequent composing of linguistic terms . This allows the prediction of attribute values to be associated with degree of membership . Consequently , our approach is able to deal with the cases that an object can belong to more than one class . For example , a person can suffer from cold and fever to certain extent at the same time . Furthermore , the use of linguistic terms to represent the discovered rules also allows quantitative values to be inferred .
The rest of this paper is organized as follows . In Section 2 , we provide a brief description of how existing algorithms can be used for classification and how fuzzy techniques can be applied to the data mining process . The details of our approach are given in Section 3 . To evaluate the performance of this approach , we applied it to several real life databases . The results of the experiments are discussed in Section 4 . Finally , in Section 5 , we provide a summary of the paper .
2 . Related work
Among the different approaches to solving the classification problem , decision tree based algorithms ( eg , [ 1 2 , 19 , 21 22 ] ) are the most popular . Other than the use of decision tree based algorithms , techniques that have been developed to mine association rules can also be used for classification ( eg , [ 16] ) .
It is important to note that the intervals involved in quantitative association rules may not be concise and meaningful enough for human experts to obtain nontrivial knowledge . Linguistic summaries introduced in [ 27 ] express knowledge in linguistic representation , which is natural for people to comprehend . In addition to linguistic summaries , the applicability of fuzzy modeling techniques to data mining has been discussed in [ 13 ] . Furthermore , an information theoretic fuzzy approach has been proposed in [ 17 ] to discover unreliable data in databases . Nevertheless , these fuzzy techniques have not been developed for classification .
An approach , which combines symbolic decision trees with approximate reasoning offered by fuzzy representation , has been proposed in [ 14 ] for building fuzzy decision trees . Based on a set of predefined fuzzy linguistic variables , a method for constructing fuzzy decision trees and a number of inference procedures based on conflict resolution in rule based systems and efficient approximate reasoning methods have been presented in [ 14 ] . Given a database , this approach can be used to build a fuzzy decision tree and the resulting tree can be used for inference .
An empirical comparison of our approach with C4.5 [ 21 ] ( a decision tree based approach ) , CBA [ 16 ] ( an association rule mining approach ) , and FID [ 14 ] ( a fuzzy decision tree approach ) on several real life databases will be given in Section 4 below .
3 . A fuzzy approach for data mining
A fuzzy rule describes an
Our approach is capable of mining fuzzy rules in large databases without any need for user specified thresholds or mapping of quantitative into binary attributes . interesting relationship between two or more linguistic terms . The definition of linguistic terms is presented in Section 31 The details of this approach are then given in Section 32 In Section 3.3 , we describe how interesting fuzzy rules can be identified . A confidence measure , called weight of evidence [ 5 8 ] measure , is then defined in Section 3.4 to provide a means for representing the uncertainty associated with the fuzzy rules . In Section 3.5 , we describe how to predict unknown values using the discovered fuzzy rules .
31 Linguistic terms
J , let dom(Iv ) = [ lv , uv ] ˝
Given a set of records , D , each of which consists of a set of attributes J = {I1 , … , In} , where Iv , v = 1 , … , n , can be quantitative or categorical . For any record , d ˛ D , d[Iv ] denotes the value iv in d for attribute Iv . For any quantitative attribute , Iv ˛ denote the domain of the attribute . Based on the fuzzy set theory , a set of linguistic terms can be defined over the domain of each quantitative attribute . Let us therefore terms associated with some denote linguistic quantitative attribute , Iv ˛ J as Lvr , r = 1 , … , sv , so that a corresponding fuzzy set , Lvr , can be defined for each Lvr . The membership function of the fuzzy set is denoted as m and is defined as : the
´ vrL m
L vr
:
( dom
I
) v
]1 ,0[
The fuzzy sets Lvr , r = 1 , … , sv , are then defined as : m
( i v
)
L vr
=
L vr dom
( v
) I m
L vr i ( i v v dom
(
I
) v i v if
I v is discrete
) if
I v is continuous for all iv ˛ value iv ˛ m by . vrL Note that Iv ˛ dom(Iv ) . The degree of membership of some dom(Iv ) with some linguistic term Lvr is given
=
J can also be categorical and crisp . In such case , let denote the domain ( dom of Iv . In order to handle categorical and quantitative attributes in a uniform manner , we can also define a set of linguistic terms , Lvr , r = 1 , … , mv , for each categorical
, , i vvm
{ i
}
1 v
I v
) fi    
    attribute , Iv ˛ Lvr , such that
J , where Lvr is represented by a fuzzy set ,
L vr
1= i vr
Using the above technique , we can represent the original attribute , J , using a set of linguistic terms , L = {Lvr | v = 1 , … , n , r = 1 , … , sv} where sv = mv for categorical attributes . Since each linguistic term is represented by a fuzzy set , we have a set of fuzzy sets , L = {Lvr | v = 1 , … , n , r = 1 , … , sv} . Given a record , d ˛
D , and a linguistic terms , Lvr ˛ L , which is , in turn , represented by a fuzzy set , Lvr ˛ L , the degree of membership of the values in d with respect to Lvr is given by . In other words , d is characterized by the
[ ( Id
] ) m
L vr v
L vr
[ ( Id
] ) term Lvr to the degree is completely characterized by m
, d 1 If , d is not characterized by the term Lvr at , d is partially characterized by [ ( Id
] ) L vr term Lvr .
. If the
] ) v < m
[ ( Id
[ ( Id
] )
=
<
L vr
1
0 v v v
0 all . If the term Lvr .
L vr m m
=
Realistically , d can also be characterized by more than one linguistic term . Let j be a subset of integers such that j = {v1 , … , vm} where v1 , … , vm ˛ {1 , … , n} , v1 „ 1 . We further suppose that j } . Given any Jj , it is associated with a set Jj = {Iv | v ˛ of linguistic terms , Lj r , r = 1 , … , sj , where . vm and |j | = h ‡
… „
= s j vs j v rv 11
L ,
,
Each Lj r is defined by a set of linguistic terms , L , to which d is characterized by the term Lj r is defined as :
. The degree ,
)(d mmrv
L
L rj l l
)( d
L j r
= m min(
[ ( Id v 1
L rv 11 m
,
, ] )
[ ( Id v m
] ) )
L mrmv
D can then be represented by a set of fuzzy data , F , which is characterized by a set of linguistic attributes , L = {L1 , … , Ln} . For any linguistic attribute , Lv ˛ L , the value of Lv in a record , t ˛ F , is a set of ordered pairs such that
L = ] v t
[
{(
L
,
1 v m
) ,
( ,
L
1 v vs v m ,
)} vs v where Lvk and m vk , k ˛ and its degree of membership , respectively .
{1 , … , sv} , are a linguistic term
For any record , t ˛ be the degree to which t is characterized by the linguistic terms Lpq and Lj k , p ˇ is defined as :
F , let j . jLL o o pq k jLL pq k o
LL pq j k
= m min( m ,
)
L j k
L pq
( 1 ) pq jLL deg
We further suppose that is the sum of degrees to which records in F characterized by the linguistic terms Lpq and Lj k . is given by : deg
( 2 ) deg jLL
= o pq k k
LL j pq k
LL j pq k t
F
Based on the linguistic terms , we can apply our approach to mine fuzzy rules in fuzzy data and present them to human users in a way that is much easier understood . Due to the use of fuzzy technique blurring the boundaries of adjacent intervals of numeric qualities , our approach is resilient to noise such as inaccuracies in physical measurements of real life entities .
311 An illustrative example . In this section , we illustrate how a relation in a relational database can be transformed to a fuzzy relation based on the linguistic terms . Let us consider a sample relation shown in Figure 1 .
MaritalStatus
Age 23 29 33 35 55 Figure 1 . A sample relation .
Salary 40,000 43,000 55,000 64,000 62,000
U M M U M
Let us further suppose that the Married attribute , which is a categorical attribute , is represented by two linguistic terms defined as :
Unmarried
1= U and
Married
1= M
For the remaining two quantitative attributes , Age and Salary , they are represented by the linguistic terms given in Figure 2 .
Based on these linguistic terms , the sample relation is transformed to a fuzzy relation shown in Figure 3 . Instead of mining interesting rules from the original relation , we perform data mining in the resulting fuzzy relation .
( cid:213 ) ˛ ˛   ˛ 1 i p h s r e b m e M f o e e r g e D
0
0
20
40
Age
60
80
( a ) The Age attribute
Low
Medium
High
1
0
0
20000
40000 60000
80000 100000
Salary i p h s r e b m e M f o e e r g e D
( b ) The Salary attribute
Figure 2 . The definitions of linguistic terms .
Age
{(Young , 0.85 ) ,
( Middle Aged , 0.15)}
MaritalStatus {(Unmarried ,
1)}
{(Young , 0.55 ) ,
{(Married , 1)}
Salary
{(Low , 0.5 ) ,
( Medium , 0.5)} {(Low , 0.35 ) ,
( Middle Aged , 0.45)}
{(Young , 0.35 ) ,
( Middle Aged , 0.65 )
{(Married , 1)}
{(Young , 0.25 ) ,
{(Unmarried ,
( Middle Aged , 0.75)}
1)}
{(Middle Aged ,
{(Married , 1)}
0.25 ) , ( Old , 0.75)}
( Medium , 0.65)} {(Medium , 0.75 ) , ( High , 0.25)} {(Medium , 0.3 ) , ( High , 0.7)} {(Medium , 0.4 ) , ( High , 0.6)}
Figure 3 . The resulting fuzzy relation .
32 The fuzzy data mining algorithm
It is important to note that a fuzzy rule can be of different orders . A first order fuzzy rule can be defined to be a rule involving one linguistic term in its antecedent ; a second order rule can be defined to have two ; and a thirdorder rule can be defined to have three linguistic terms , etc . Our approach is given in Figure 4 below .
To mine interesting first order rules , our approach makes use of an objective interestingness measure introduced in Section 3.3 below . After these rules are discovered , they are stored in R1 ( Figure 4 ) . Rules in R1 are then used to generate second order rules that are then stored in R2 . R2 is then used to generate third order rules
Young
Middle Aged
Old that are stored in R3 and so on for 4th and higher order . Our approach iterates until no higher order rule can be found .
The function , interesting(Lpq , Lj k ) , computes an objective measure to determine whether the relationship between Lpq and Lj k is interesting . If interesting(Lpq , Lj k ) returns true , a fuzzy rule is then generated by the rulegen function . For each rule generated , this function also returns an uncertainty measure associated with the rule ( see Section 34 ) All fuzzy rules generated by rulegen are stored in R that will then be used later for prediction or for the users to examine .
1 ) R1 = {first order fuzzy rules} ; 2 ) 3 ) begin for(m = 2 ; |Rm – 1| „ f ; m + + ) do forall j composed of m elements in C do
C = {each condition in the antecedent of r | r ˛ begin forall t ˛
F do
Rm – 1} ; t[Lp ] , ( Lj k , m j k ) ˛ =+ m min( t[Lp ] , ( Lj k , m j k ) ˛ t[Lj ] , p ˇ jm , ) pq t[Lj ] , p ˇ
; k k
LL j pq j do j do forall ( Lpq , m pq ) ˛ deg forall ( Lpq , m pq ) ˛ if interesting(Lpq , Lj k ) then
Rm = Rm ¨ rulegen(Lpq , Lj k ) ;
4 ) 5 ) 6 ) 7 )
8 )
9 )
10 ) 11 ) 12 ) 13 ) end
14 ) end =R 15 )
Figure 4 . The fuzzy data mining algorithm .
33 Discovering interesting rules in fuzzy data
In order to decide whether a relationship between a linguistic term , Lj k , and another linguistic term , Lpq , is interesting , we determine whether records characteri
( 3 ) which sum to of sum degrees of degrees to which
L L and by zed j k L characteri zed by records j k pq
L Pr( pq
|
L j
) k
= is significantly different from degrees sum of to which
L Pr( pq
=
) characteri records M zed
L by pq
( 4 ) where
M
= s p s j u
1
= deg jLL pu i
= 1 i
. If this is the case , we consider the relationship between Lj k and Lpq interesting . The significance of the difference can be objectively evaluated based on the idea of an adjusted residual [ 5 8 ] defined as : mR
;
  m d
LL j pq k
= z LL j pq g
LL j pq k k
( 5 ) where z given by : jLL pq is the standardized residual [ 5 8 ] and is k where e jLL pq k z
LL j pq k
= egd
LL j pq k e
LL pq j k e
LL j pq k
( 6 ) is the sum of degrees to which records are expected to be characterized by Lj k and Lpq . It is defined as : s j s p
== 1 i e
LL j pq k deg
LL j pq i u M deg
LL j pu k
= 1
( 7 ) is the maximum likelihood estimate [ 5 8 ] of
LL pq j k and g the variance of z jLL pq k and is given by : g
LL pq j k
=
1 s j
= 1 i deg
LL j pq i
M
1 s p
= 1 u deg
LL j pu k
M
( 8 )
96.1> k pq d
If jLL
( the 95 percentiles of the normal distribution ) , we can conclude that the discrepancy between Pr(Lpq | Lj k ) and Pr(Lpq ) is significantly different and hence the relationship between Lj k and Lpq is interesting . Specifically , the presence of Lj k implies the presence of Lpq . In other words , it is more likely for a record having both Lj k and Lpq .
34 Uncertainty representation
Given that a linguistic term Lj k is associated with another linguistic term Lpq , we can form the following fuzzy rule .
L j k
L wpq
[
LL j pq
] k where defined as follows . jLL w pq k is the weight of evidence measure that is
Since the relationship between Lj k and Lpq is interesting , there is some evidence for a record to be characterized by Lpq given it has Lj k . The weight of evidence measure is defined in terms of an information theoretic measure known as mutual information . Mutual information measures the change of uncertainty about the presence of Lpq in a record given that it has Lj k is and , in turn , defined as :
I
(
L pq
:
L j k
)
= log
L Pr( pq L Pr(
|
L j ) pq
) k
( 9 )
Based on mutual information , the weight of evidence measure is defined in [ 5 8 ] as : w
LL pq j k
=
I
(
L pq
:
L j k
)
I
(
(
L pi
:
L j k
) )
= log j
L Pr( L Pr( j k k |
|
L qi qi )
) pi pq L
( 10 ) k pq w jLL can be interpreted intuitively as a measure of the difference in the gain in information when a record with Lj k characterized by Lpq and when characterized by Lpi , i „ q . The weight of evidence measure can be used to weigh the significance or importance of fuzzy rules . kv 11
Given that Lj k is defined by a set of linguistic terms , L L , we have a high order fuzzy rule as ,
, follows : mmkv
L
L kv 11
L
, , kv mm pqL
[ w jLL pq k
]
… ·
… · j . where v1 , … , vm ˛
35 Predicting unknown values using fuzzy rules dom(Ip ) · dom(I1 ) ·
Given a record , d ˛
( pl dom dom(In ) , let d be characterized by n attribute values , a 1 , … , a p , … , a n , where a p is the value to be predicted . Let Lp , p = 1 , … , sp , be the linguistic terms corresponding to the class attribute , Ip . We further let lp be a linguistic term . The value of a p with domain is given by the value of lp . To predict the correct value of lp , our approach searches rules with Lpq ˛ dom(lp ) as consequents . For any combination of j , of d , it is characterized by a attribute values , a j , p ˇ linguistic term , Lj k , to a degree of compatibility , , )(d for each k ˛
{1 , … , sj} . Given those rules implying the
L= { ) fuzzy
L
, , the pps
L kj l
1 p
}
   
„ „ ˛
L j k
L wpq
[
LL j pq
]
, k for all
{1 , … , sj} , the evidence for such assignment is z ˝ assignment of Lpq , k ˛ given by : w
= a pq
L j w
LL j pq k l z k
)(L d j k
( 11 )
Suppose that , of the n – 1 attribute values excluding a p , only some combinations of them , a [ 1 ] , … , a [ j ] , … , a [ b ] with a [ j ] = {a i | i ˛ {1 , … , n} – {p}} , are found to match one or more rules , then the overall weight of evidence for the value of lp to be assigned to Lpq is given by :
= w q w a pq
L
[ j
]
( 12 ) b j
=
1
1 p
L
, w 1
As a ) , result , ( , , the value of a p ) , )} w is given by . When a crisp {( value is to be assigned to a p , the following methods are used depending on Ip is categorical or quantitative .
L ( , w
L pq
, ps q s p p
In case that Ip is categorical , lp is assigned to Lpc if
( £ c ps¢ ps¢ and g „ wc > wg , g = 1 , … , where by the rules . a p is therefore assigned to ipc ˛ If Ip is quantitative , a new method is used to assign an appropriate value to a p . Given the linguistic terms , L , and their overall weight of evidence , sp ) is the number of linguistic terms implied dom(Ip ) .
( 13 )
( p i
) be the weighted degree of pu dom(Ip ) to the fuzzy set Lpu , u ˛ is given by :
{1 ,
( 14 )
( i p
)
= w u m
( i p
)
L pu m ¢ L
L ,1
, p pps m ¢
, ,1 psw w , let L membership of ip ˛ … , sp} . ) where ip ˛
( p i m
L pu pu
(
1
= 1 u for a p is then defined as : dom(Ip ) and u = 1 , … , sp . The defuzzified ps value ,
F puL
)
, which provide an appropriate value
=
L pu
) dom
(
I
) p m
L p
1
L pps
( i p
) i p di p m
L p
1
L pps
( i p
) di p dom
(
I
) p
( 15 )
F
1
( s p
= 1 u m
= m max( m
Y
X
)( i
YX
))( i
,)( i for any fuzzy sets X where and Y . For quantitative predictions , we use root meansquared error as a performance measure . Given a set of test records , D , let n be the number of records in D . For any record , r ˛ denote the domain of the class attribute . We further let tr be the target value of the class attribute in r and or be the value predicted by our approach . The root mean squared error , rms , is defined as
D , let [ l , u ] ( cid:204 )
´
= rms
1 n
Dr l t r lu o r u l l
2
( 16 )
4 . Performance Analysis
To evaluate the effectiveness of our approach , we tested it using several real life databases : a credit card database , a diabetes database , and a social database . For each experiment , each of these databases was divided into two datasets with records in each dataset randomly selected . The mining of rules was performed on one of them . The other dataset was reserved for testing . For each such testing dataset , the values of the attributes to be predicted were deleted . The rules discovered by mining the other dataset was then used to predict the deleted attribute values . The predicted values are then compared against the original values to see if they are the same . If it is the case , an accuracy count is incremented . Based on this accuracy count , the percentage accuracy for our approach , C4.5 [ 21 ] ( a decision tree based approach ) , CBA [ 16 ] ( an association rule mining approach ) , and FID [ 14 ] ( a fuzzy decision tree approach ) are computed . The experiments performed for each of the databases were repeated ten times and the percentage accuracy , averaged over the ten trials , were recorded and are presented in the following sections .
41 The credit card database
The credit card database [ 20 ] contains data about credit card applications . It consists of 15 attributes of which one of them , the Success attribute , is concerned with whether or not an application is successful . The meaning of these attributes are not known as the names of the attributes and their values were changed by the donor of the database to meaningless symbols to protect confidentiality of the data . Out of the 15 attributes , 6 are quantitative and 9 are categorical . Each of the 6 quantitative attributes was represented by 4 linguistic terms for our approach and FID .
There are altogether 690 records in the database . For experiments , we randomly selected 30 % ( ie , 207 in total ) of them for testing by deleting from them the values of
 
˛ ( cid:215 ) ( cid:215 ) ¢
¨ ¨ ¨ ¨ ¢ ( cid:215 ) ¢
¢ ¢ ¢ ¨
˛ the Success attribute . Each of our approach , C4.5 , CBA , and FID , was then used to mine rules from the rest of the database ( 70 % or 483 records ) . The discovered rules were then used to predict the missing Success values in the test records . This procedure of randomly selecting different sets of records for data mining and testing was repeated ten times . The percentage accuracy was computed for each trial and the percentage accuracy averaged over these ten trials is given in Table 1 . Of the four different approaches , our approach performed better then C4.5 , CBA , and FID by 6.3 % , 3.9 % , and 30.9 % , respectively .
42 The diabetes database
The diabetes database [ 23 ] contains 768 patient records . These records are characterized by 9 attributes including one denoted Test results . Test results contains either a “ 1 ” ( tested positive for diabetes ) or a “ 2 ” ( tested negatively for diabetes ) . The other attributes are all quantitative . Each of these quantitative attributes was represented by 4 linguistic terms for our approach and FID . A total of 30 % of the records were randomly selected from the database and the values of Test results in these records deleted . Using each of our approach , C4.5 , CBA , and FID , rules were mined from the remaining 70 % of the data . These rules were then used to determine the values of Test results in the test dataset . This testing procedure was repeated ten times for each of our approach , C4.5 , CBA , and FID and the percentage accuracy , averaged over the ten trials , were determined . Of these different approaches , our approach performed better than C4.5 , CBA , and FID by 3.8 % , 3.2 % , and 15.6 % , respectively ( Table 1 ) .
43 The social database
The social database [ 15 ] contains data collected by the US Census Bureau . The data in the database were divided into two sets by the donor of the data . The first dataset , which consists of 32,561 records , where used for data mining whereas the second dataset , which consists of 16,281 records , were used for testing . The records in the database are characterized by 15 attributes . Of these attributes , 6 are quantitative . Each of these quantitative attributes was represented by 4 linguistic terms for our approach and FID . The remaining 9 attributes are all categorical .
Using each of our approach , C4.5 , CBA , and FID , predictive modeling rules were mined from the dataset for data mining . These rules were then used to predict the values of the Salary attribute in the test data . The percentage accuracies for the four approaches are given in Table 1 . Of these different approaches , our approach performed better than C4.5 , CBA , and FID by 0.5 % , 1.7 % , and 62.3 % , respectively .
Unlike the case with the credit card and the diabetes databases , it should be noted that testing was not repeated for this particular database . This is because the records for data mining and for testing were fixed by the donor rather than randomly selected .
44 Discussion
In summary , our approach performed better than C4.5 , CBA , and FID in the above cases . It achieved an average accuracy of 84.1 % and is better than C4.5 by 3.5 % , CBA by 2.9 % , and FID by 362 % If we define a baseline accuracy to mean the accuracy obtained by simply assigning the most frequently occurring values to the attributes being predicted , the baseline accuracy for the credit card database is 55.5 % , the diabetes database is 65.1 % , and the social database is 761 % For all the databases , the accuracies of the rules discovered by our approach , C4.5 , and CBA are significantly higher than the baseline accuracy . For the credit card database , the accuracy of FID is only marginally higher than the respective baseline accuracy . For the diabetes database , the accuracy of FID is marginally lower than the baseline accuracy . For the social database , the accuracy of FID is significantly lower than the baseline accuracy .
Table 1 . Average percentage accuracy .
Databases credit card diabetes social Average
Percentage Accuracy
Our
Approach
88.9 % 77.6 % 85.9 % 84.1 %
C4.5 82.6 % 73.8 % 85.4 % 80.6 %
CBA 85.0 % 74.4 % 84.2 % 81.2 %
FID 58.0 % 62.0 % 23.6 % 47.9 %
5 . Conclusions
In summary , we have presented a fuzzy approach that can be used for mining interesting rules for classification with degree of membership . This approach represents the revealed regularities and exceptions using linguistic terms . The use of linguistic terms allows human users to better understand the discovered rules because of the the human knowledge representation . affinity with Furthermore , our approach finding interesting relationships among attributes without any subjective input required of the users . The effectiveness of it has been evaluated using several real life databases . The experimental results show that our approach can be very effective at data mining tasks . In fact , in the experiments we performed , our approach was found to be able to predict more accurately than C4.5 , CBA , and FID . is capable of
[ 16 ] B . Liu , W . Hsu , and Y . Man , “ Integrating Classification and Association Rule Mining , ” in Proc . of the 4th Int’l Conf . on Knowledge Discovery and Data Mining , New York , NY , 1998 .
[ 17 ] O . Maimon , A . Kandel , and M . Last , “ InformationTheoretic Fuzzy Approach to Data Reliability and Data Mining , ” Fuzzy Sets and Systems , vol . 117 , pp . 183 194 , 2001 .
[ 18 ] H . Mannila , H . Toivonen , and AI Verkamo , “ Efficient Algorithms for Discovering Association Rules , ” in Proc . of the AAAI Workshop on Knowledge Discovery in Databases , Seattle , Washington , 1994 , pp . 181 192 .
[ 19 ] M . Mehta , J . Rissanen , and R . Agrawal , “ SLIQ : A Fast Scalable Classifier for Data Mining , ” in Proc . of the 5th Int’l Conf . on Extending Database Technology , Avignon , France , 1996 .
[ 20 ] JR Quinlan , “ Simplifying Decision Trees , ” Int’l J . of Man
Machine Studies , vol . 27 , pp . 221 234 , 1987 .
[ 21 ] JR Quinlan , C4.5 : Programs for Machine Learning , San
Mateo , CA : Morgan Kaufmann , 1993 .
[ 22 ] J . Shafer , R . Agrawal , and M . Mehta , “ SPRINT : A Scalable Parallel Classifier for Data Mining , ” in Proc . of the 22nd Int’l Conf . on Very Large Data Bases , Mumbai ( Bombay ) , India , 1996 .
[ 23 ] JW Smith , JE Everhart , WC Dickson , WC Knowler , and RS Johannes , “ Using the ADAP Learning Algorithm to Forecast the Onset of Diabetes Mellitus , ” in Proc . of the Symp . on Computer Applications and Medial Cares , 1983 , pp . 422 425 .
[ 24 ] P . Smyth and RM Goodman , “ An Information Theoretic Approach to Rule Induction from Databases , ” IEEE Trans . on Knowledge and Data Engineering , vol . 4 , no . 4 , 1992 , pp . 301 316 .
[ 25 ] R . Srikant and R . Agrawal , “ Mining Quantitative Association Rules in Large Relational Tables , ” in Proc . of the ACM SIGMOD Int’l Conf . on Management of Data , Montreal , Canada , 1996 , pp . 1 12 .
[ 26 ] SM Weiss , Predictive Data Mining : A Practical Guide ,
San Francisco , CA : Morgan Kaufmann , 1998 .
[ 27 ] RR Yager , “ On Linguistic Summaries of Data , ” in G . Piatetsky Shapiro and WJ Frawley ( Eds. ) , Knowledge Discovery in Databases , Menlo Park , CA : AAAI/MIT Press , 1991 , pp . 347 363 .
[ 28 ] L . Zadeh , “ Fuzzy Sets , ” Inform . Control , vol . 8 , pp . 338
353 , 1965 .
References
[ 1 ] R . Agrawal , S . Ghost , T . Imielinski , B . Iyer , and A . Swami , “ An Interval Classifier for Database Mining Applications , ” in Proc . of the 18th Int’l Conf . on Very Large Data Bases , Vancouver , British Columbia , Canada , 1992 , pp . 560 573 .
[ 2 ] R . Agrawal , T . Imielinski , and A . Swami , “ Database Mining : A Performance Perspective , ” IEEE Trans . on Knowledge and Data Engineering , vol . 5 , no . 6 , pp . 914925 , 1993 .
[ 3 ] R . Agrawal , T . Imielinski , and A . Swami , “ Mining Association Rules between Sets of Items in Large Databases , ” in Proc . of the ACM SIGMOD Int’l Conf . on Management of Data , Washington DC , 1993 , pp . 207 216 . [ 4 ] R . Agrawal and R . Srikant , “ Fast Algorithms for Mining Association Rules , ” in Proc . of the 20th Int’l Conf . on Very Large Data Bases , Santiago , Chile , 1994 , pp . 487 499 .
[ 5 ] W H Au and KCC Chan , “ An Effective Algorithm for Discovering Fuzzy Rules in Relational Databases , ” in Proc . of the 7th IEEE Int’l Conf . on Fuzzy Systems , Anchorage , Alaska , 1998 , pp . 1314 1319 .
[ 6 ] W H Au and KCC Chan , “ FARM : A Data Mining System for Discovering Fuzzy Association Rules , ” in Proc . of the 8th IEEE Int’l Conf . on Fuzzy Systems , Seoul , Korea , 1999 , pp . 1217 1222 .
[ 7 ] KCC Chan and W H Au , “ Mining Fuzzy Association Rules , ” in Proc . of the 6th Int’l Conf . on Information and Knowledge Management , Las Vegas , Nevada , 1997 , pp . 209 215 .
[ 8 ] KCC Chan and W H Au , “ Mining Fuzzy Association Rules in a Database Containing Relational and Transactional Data , ” in A . Kandel , M . Last , and H . Bunke ( Eds. ) , Data Mining and Computational Intelligence , Heidelberg , Germany ; New York , NY : Physica Verlag , 2001 , pp . 95 114 .
[ 9 ] M S Chen , J . Han , and PS Yu , “ Data Mining : An Overview from a Database Perspective , ” IEEE Trans . on Knowledge and Data Engineering , vol . 8 , no . 6 , pp . 866883 , 1996 .
[ 10 ] UM Fayyad , “ Mining Databases : Towards Algorithms for Knowledge Discovery , ” Bulletin of the Technical Committee on Data Mining , vol . 21 , no . 1 , 1998 , pp . 335341 .
[ 11 ] UM Fayyad , G . Piatetsky Shapiro , and P . Smyth , “ From Data Mining to Knowledge Discovery : An Overview , ” in UM Fayyad , G . Piatetsky Shapiro , P . Smyth , and R . Uthurusamy ( Eds. ) , Advances in Knowledge Discovery and Data Mining , Menlo Park , CA : AAAI/MIT Press , 1996 , pp . 1 34 .
[ 12 ] J . Han and M . Kamber , Data Mining : Concepts and
Techniques , Morgan Kaufmann , 2001 .
[ 13 ] K . Hirota and W . Pedrycz , “ Fuzzy Computing for Data Mining , ” Proc . of the IEEE , vol . 87 , no . 9 , pp . 1575 1600 , 1999 .
[ 14 ] CZ Janikow , “ Fuzzy Decision Trees :
Issues and Methods , ” IEEE Trans . on Systems , Man , and Cybernetics – Part B : Cybernetics , vol . 28 , no . 1 , pp 1 14 , 1998 .
[ 15 ] R . Kohavi , “ Scaling Up the Accuracy of Naive Bayes Classifiers : A Decision Tree Hybrid , ” in Proc . of the 2nd Int’l Conf . on Knowledge Discovery and Data Mining , Portland , Oregon , 1996 .
