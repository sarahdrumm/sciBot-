Learning Automatic Acquisition of Subcategorization Frames using
Bayesian Inference and Support Vector Machines
M . Maragoudakis , K . Kermanidis , N . Fakotakis , G . Kokkinakis
Dept . of Electrical & Computer Engineering University of Patras , 26500 Patras , Greece
Telephone : +30 61 991722
Fax : +30 61 991855
{mmarag,kerman,fakotaki,gkokkin}@wcleeupatrasgr
Abstract
Learning Bayesian Belief Networks ( BBN ) from corpora and Support Vector Machines ( SVM ) have been applied to the automatic acquisition of verb subcategorization frames for Modern Greek . We are incorporating minimal linguistic resources , ie basic morphological tagging and phrase chunking , to demonstrate that verb subcategorization , which is of great significance for developing robust natural language human computer interaction systems , could be achieved using large corpora , without having any general purpose syntactic parser at all . In addition , apart from BBN and SVM , which have not previously used for this task , we have experimented with three well known machine learning methods ( Feed Forward BackPropagation Neural Networks , Learning Vector Quantization and Decision Tables ) , which are also being applied to the task of verb subcategorization frame detection for the first time . We argue that both BBN and SVM are well suited for learning to identify verb subcategorization frames . Empirical results will support this claim . Performance has been methodically evaluated using two different corpora types , one balanced and one domain specific in order to determine the unbiased behaviour of the trained models . Limited training data are proved to endow with satisfactory results . We have been able to achieve precision exceeding 91 % on the identification of subcategorization frames which were not known beforehand .
1 Introduction
Verb subcategorization is an important issue especially for parsing and grammar development as it provides the parser with syntactic and/or semantic information on a verb ’s arguments , ie the set of restrictions the verb imposes on its arguments . In many natural language interface applications , the syntactic semantic information extracted from subcategorization frames ( SF ) could prove to be essential since it often clarifies the agent and the receiver of an action . Nowadays , with the impressive increase in the number of available text corpora and language resources in general , the need for fully annotated syntactic parsers could be alleviated by “ mining ” subcategorization frame information from large text corpora . Since building verb subcategorization classifiers is difficult and time consuming , learning classifiers from examples is advantageous Besides , machine readable dictionaries listing SF usually provide only expected frames rather than actual ones and thus being incomplete or in many cases unavailable for some languages .
Previous work on learning SF focuses mainly on English ( Brent , 1993 ; Manning , 1993 ; Briscoe and Carroll , 1998 ; Gahl , 1998 ) . Basili et al . ( 1997 ) deal with Italian , De Lima ( 1997 ) and Eckle and Heid ( 1996 ) with German , Kawahara et al . ( 2000 ) with Japanese , Zeman 2000 ) with Czech . In most of the above approaches , the input text corpus is fully parsed and in some of them only a small number of frames are learned ( Brent , 1993 ; Manning , 1993 ) .
For the present work , we are incorporating two machine learning methods that have revealed great potential for learning classification functions and have not been previously used for the detection of verb SFs . We apply Bayesian Belief Networks ( BBN ) learning from corpora and use the extracting network as an inference tool that enables automatic acquisition of SF for Modern Greek . Furthermore , we experiment with Support Vector Machines ( SVM ) , a recently well founded technique in terms of computational learning theory that has been successfully applied in numerous classification problems including text categorization ( Joachims ) , pattern recognition , face detection ( Osuna et al ) etc .
In order to obtain an inclusive view of the behaviour of the proposed methods , experiments with three additional machine learning techniques have been carried out . Namely Feed Forward Back Propagation Neural Network ( FF BP NN ) , Learning Vector Quantization ( LVQ ) and Decision Tables ( DT ) . BBN outperforms all the other techniques and SVM follow with a variation of 1,5 3 % while by introducing supplementary parameters , precision increases considerably . 10 fold cross validation is used in order to retrieve results .
Modern Greek ( MG ) is a “ free word order ” language . Position of the syntactic constituents in a sentence is a very weak indicator of the syntactic role of the constituent . Moreover , the existence of adverbs in the neighbourhood of a verb is a major source of noise . The corpora used for our study are the balanced ILSP/ELEFTHEROTYPIA1 Greek Corpus ( consisting of 1.6 million words of political , social and sports content , taken from a wide circulation newspaper ) , and the corpus of economic news created for DELOS2 project consisting of 1.7 million words ( Sintichakis et al . , 2000).The complete set of frames for a particular verb was not known beforehand but it is learned automatically through the training process .
This paper is organized as follows : Initially , we present certain linguistic properties that are relevant to the task of SF acquisition , which characterize MG and need to be taken into consideration ( section 2 ) . The pre processing stage is covered in section 3 and the detection of the environments of a verb in section 4 . Our criteria for the selection of the features of our data are stated in section 5 . Some theoretical background for BBN and SVM is provided in section 6 while three other machine learning techniques used to benchmark our idea are presented in section 7 . The results obtained by these methods are tabulated in section 8 , with a summary of concluding remarks in section 9 .
2 Properties of Modern Greek
MG is a ‘free word order’ language . The arguments of a verb do not have fixed positions . They are basically determined by their morphology and especially by their case . Noun phrases ( NP ) ( a ) , prepositional phrases ( PP ) ( b ) , adverbs ( c ) and secondary clauses ( d ) may function as arguments to verbs . Weak personal pronouns ( WPP ) may also function as arguments to verbs ( e ) when occurring within the verb phrase either in the genitive or accusative case . The examples in Table 1 illustrate how a single verb ( πιστεύω to believe ) can take as arguments all the above syntactic constituents . The brackets located next to a word describe the type of phrase and the case of the word .
Table 1 . Examples of various syntactic constituents of the verb “ πιστεύω to believe ” . a . Πιστεύω την Ελένη Believe Helen[NPacc ] I believe Helen b . Πιστεύω στο Θεό Believe in[PREP ] God[acc ] I believe in God c . Έτσι πιστεύω So[ADV ] believe I believe so d . Πιστεύω πως θα έρθει Believe that[CONJ ] come I believe that he will come
1 Copyright 1999 , ILSP . http://corpusilspgr 2 DELOS Project Nr:EPET II , 98 LE 12
As their arguments , verbs select specific prepositions ( introducing a PP complement ) , specific types of secondary clauses , and/or specific cases ( accusative or genitive ) for their NP complements .
3 Corpus pre processing
Given that we want to avoid using a wide coverage syntactic parser , limited linguistic resources have been utilized . Thus , pre processing of the corpus consisted of the following tasks :
• Basic morphological tagging ( Sgarbas et al . , 1999 ) . • Chunking ( Stamatatos et al . , 2000 ) . • Detecting the headword of the noun and prepositional phrases , ie the word the grammatical properties of which are inherited by the phrase .
• Filtering
Morphological tagging includes part of speech tagging for all words , case tagging for nouns , adjectives and pronouns , voice tagging for verbs , type tagging for verbs ( distinguishing between personal and impersonal verb types ) , type tagging for pronouns ( distinguishing among relative , interrogative and the rest of the pronouns ) and type tagging for conjunctions ( distinguishing between coordinate and subordinate conjunctions ) . The chunker is based on a small keyword lexicon containing some 450 keywords ( closed class words ) and a suffix lexicon of 300 of the most common word suffixes in MG . It detects the boundaries of intrasentential noun phrases , prepositional phrases , verb phrases , adverbial phrases and conjunctions .
A filtering stage follows which frees the corpus from noise like abbreviations , certain punctuation marks and other constituents that do not contribute to the SF detection task . A noun phrase or a prepositional phrase inherits its case tag from the case value of the headword of the phrase . Noun phrases inherit their part of speech tag from the part of speech value of their headword . Prepositional phrases and secondary clauses are labelled according to the reposition/conjunction introducing them respectively . Hence the tags used are as follows : e . Σε πιστεύω You[WPPacc ] believe I believe you
• Nouns : Nn , Na , Ng • Adjectives : An , Ag , Aa • Pronouns : ppn , ppa , ppg , prn , pta , … • Prepositional phrases : P1 ( ) , P2 ( ) , P3 ( ) , … • Secondary clauses : Cαν(if ) , Cνα(to ) , Cπου(where ) ,…
In our approach , a verb occurring in the corpus in both active and passive voice is treated as two distinct verbs . The same applies when the same verb appears with both a personal and an impersonal structure .
4 Detecting verb environments
We have carried out a number of experiments concerning the window size of the environment of a verb , ie the number of phrases preceding and following the verb . Windows of sizes ( 2+3 ) , ie two phrases preceding and three phrases following the verb , ( 2+2 ) and ( 1+2 ) were proved to provide better results . This phenomenon can be explained since it is rather unusual for an argument of a verb to be far away from it . This observation found to be true in both corpora used , since only in some literature texts we could found such verb argument anomalies . For almost every environment , not the entire environment , but a subset of the environment is a correct frame of the verb . Therefore all possible subsets ( Sarkar and Zeman ,
2000 ) of the above environments were produced and their frequency in the corpus recorded as shown in Figure 1 .
In Figure 1 , we could distinguish two environments for the verb “ αγγίζω ” ( to touch ) , environment A and B respectively . Environment A is consisted of a noun phrase of category one ( N1 ) a prepositional phrase ( P3 ) and another noun phrase of category three ( N3 ) , while B is consisted of two noun phrases , N1 and N2 . The parentheses next to each environment symbolize the number of occurrences in the corpus . For environment A , we calculate every single permutation of its subsets taking a left to right orientation , excluding duplicate ones ( eg N1 P5 and not P5 N1 ) . This is done in two steps , calculating the counts of the pair subsets first and then permutating them to obtain counts for the smallest subset available . The same procedure takes place for environment B . As a final stage , counts for every subset that exists in both environments are added ( eg N1 occurred two times in environment A after the permutations and one time in environment B ) .
E n v i r o n m e n t A N 1 P 5 N 3 ( 1 )
N 1 N 1 P 5
P 5 N 3 N 3
( 1 ) ( 1 ) ( 1 )
E n v i r o n m e n t B N 1 N 2 ( 1 )
N 1 P 5 N 3
( 1 ) + ( 1 ) ( 1 ) + ( 1 ) = 2 ( 1 ) + ( 1 ) = 2
+ ( 1 ) = 3 ( 1 ) N 2
Figure 1 . Subsets and their counts of environments A ( in plain font ) and B ( in bold ) . By calculating every permutation of environments A and B respectively , we obtain the frequency of occurrence of every subset ( shown in parentheses ) . As a last phase , frequencies of subsets of both environments are added .
Upon completion of the procedure described above , we were able to formulate input data . We consider as potential SF both the original environment extracted from the corpora and all computed subsets as well .
5 Feature Selection
Features of our training data were categorized in grammatical and numerical ones . Grammatical Features consisted of the window size , varying from ( 2+3 ) to ( 1+2 ) , along with seven more morphosyntactic categories , characterizing the type of phrase , the case , the preposition used , the presence or absence of an adverb , the type of verb , the tense , and category ( personal or impersonal ) of the structure . As for the numerical features , our goal is to determine if a candidate SF is highly associated with a specific verb . In order to obtain our features , we proceed to the following postulation . Making the hypothesis that the distribution of verb environment in the corpus is independent of the distribution of the verb , we compare the frequency of co occurrence of a given environment with a certain verb ( described by probability p1 ) to the frequency of its cooccurrence with the rest of the verbs ( described by p2 ) and to its expected frequency in the input data ( described by p ) . To this end the following counts are required :
• • • • the count of a given environment of a given verb v , ( k1 ) . the count of a given verb v , ( n1 ) . the count of a given environment with every other verb except for v , ( k2 ) . the count of every other verb except for verb v , ( n2 ) .
Using the above values :
• p1 = k1 /n1 • p2 = k2 /n2 • p = ( k1+k2)/(n1+n2 )
We have also included two additional numerical features , namely the Log Likelihood Ratio ( LLR ) and the Number of Distinct Elements ( NDE ) which are calculated as :
LLR=[logL(p1,k1,n)+logL(p2,k2,n) logL(p,k1,n) logL(p,k2,n) ] , where logL(a,b,c)=clog(a)+(b c)log(1 a )
NDE=number of distinct elements each environment has . eg NDE(N1 P5 N3)=3
We should denote the initial features as “ Standard Features ” and as “ Enhanced Features ” the feature set added by LLR and NDE . Actually , concerning the LLR parameter , is not an addition since we omitted features p1 , p2 and p when we used it . As we should discuss in the results section these two additional features contribute to the improvement of the performance by a factor of about 3,1 %
6 BBN and SVM background
6.1 Bayesian Networks
A Bayesian Belief Network ( BBN ) is a significant knowledge representation and reasoning tool , under conditions of uncertainty . ( Mitchell ) . Given a set of variables D = <X1 , X2…XN> , where each variable Xi could take values from a set Val(Xi ) , a BBN describes the probability distribution over this set of variables . We use capital letters as X,Y to denote variables and lower case letters as x,y to denote values taken by these variables . Formally , a BBN is an annotated directed acyclic graph ( DAG ) that encodes a joint probability distribution . We denote a network B as a pair B=<G,Θ> , ( Pearl , 1988 ) where G is a DAG whose nodes symbolize the variables of D , and Θ refers to the set of parameters that quantifies the network . G embeds the following conditional independence assumption : Each variable Xi is independent of its non descendants given its parents . Θ includes information about the probability distribution of a value xi of a variable Xi , given the values of its immediate predecessors . The unique joint probability distribution over <X1 , X2…XN> that a network B describes can be computed using :
XP B
(
1
X
N
)
∏ = N
= 1 i xP ( i
| parents
(
X
) ) i
( 1 )
Learning BBNs from data
In the process of efficiently detecting verb SF , prior knowledge about the impact each feature has on the classification of a candidate SF as valid or not , is not straightforward . Thus , a BBN should be learned from the training data provided . Learning a BBN unifies two processes : learning the graphical structure and learning the parameters Θ for that structure . In order to seek out the optimal parameters for a given corpus of complete data , we directly use the empirical conditional frequencies extracted from the data ( Cooper and Herskovits , 1992 ) . The selection of the variables that will constitute the data set is of great significance , since the number of possible networks that could describe these variables equals to :
−NN
(
)1
2
2 where N is the number of variables ( Jensen , 1996 ) .
( 2 )
We use the following equation along with Bayes theorem to determine the relation r of two candidate networks B1 and B2 respectively : DBPr = ) ( | DBP ( ) |
1
2
BPBDPDBP | )( ) ( DP ) (
=
(
)
| where :
• P(B|D ) is the probability of a network B given data D . • P(D|B ) is the probability the network gives to data D . • P(D ) is the ‘general’ probability of data . • P(B ) is the probability of the network before seen the data .
Applying equation ( 3 ) to ( 4 ) , we get : r =
BPBDP ( 1 BPBDP ( 2
( (
) )
| |
2
1
) )
( 3 )
( 4 )
( 5 )
Having not seen the data , no prior knowledge is obtainable and thus no straightforward method of computing P(B1 ) and P(B2 ) is feasible . A common way to deal with this is to assume that every network has the same probability with all the others , so equation ( 5 ) becomes : BDPr = ( | 1 BDP ( | 2
( 6 )
) )
The probability the model gives to the data can be extracted using the following formula ( Glymour and Cooper , 1999 ) : ΞΓ ( q +ΞΓ (
∏ ∏
BDP
∏
( 7 )
N
N
= ijk
)
)
)
(
)
)
=
1 qi
=
1 i
=
1 j ri
| n i k ij
+ΞΓ ( qr i ΞΓ ( qr i i i q i where : take . i:th variable .
• Γ is the gamma function . • n is the number of variables . • • qi is the number of possible different value combinations the parent variables can ri is the number of values in i:th variable .
• Nij depicts the number of rows in data that have j:th value combinations for parents of
• Nijk corresponds to the number of rows that have k:th value for the i:th variable and which also have j:th value combinations for parents of i:th variable .
• Ξ is the equivalent sample size , a parameter that determines how readily we change our beliefs about the quantitative nature of dependencies when we see the data . In our study , we follow a simple choice inspired by Jeffrey ’s prior . Ξ equals to the average number of values variables have , divided by 2 .
Given the great number of possible networks produced by the learning process , a search algorithm has to be applied . We follow greedy search with one modification : instead of comparing all candidate networks , we consider investigating the set that resembles the current best model most .
In general , a BBN is capable of computing the probability distribution for any partial subset of variables , given the values or distributions of any subset of the remaining variables .
Note that the values have to be discretised , and different discretisation size affects the network . As we shall discuss in the result section , BBN are a significant tool for knowledge representation , visualising the relationships between features and subsets of them . This fact has a significant result on identifying which features are actually affect the class variable , thus reducing training data size without any significant impact in the performance .
6.2 Support Vector Machines
Support Vector Machines ( SVM ) is based on the Structural Risk Minimization principle ( Vapnik ) . The basic idea of this theory is to find a hypothesis h for which we could guarantee the lowest true error . By “ true error ” we denote the probability that a hypothesis will make an error when classifying a random unseen test vector . SVM is a new machine learning technique that has been applied to numerous classification and pattern recognition problems such as text classification , shallow parsing and face recognition with noteworthy results . Let us denote a training set D as a pair {(xi,yi)} , i=1 to N with each input vector xi∈ℜm and each label vector yi ∈{ 1,+1} . SVM performs a mapping φ from the input space ℜm to the “ feature ” space ℜn . In the case where data are linearly separable in ℜn , a vector w ∈ℜn ban be defined such that ≥+ b wTφ if yi =1 x )(
( 8 )
1
( 9 )
+)(φ if yi = 1 xwTφ −≤+ b )( 1 where b∈ℜ is a scalar .
A hyperplane is assembled for which the distance between itself and the positive and negative examples is maximized . We should also take into consideration that a hyperplane in space ℜn may represent a nonlinear decision surface in space ℜm . It can be shown ( Cortes and Vapnik ) that the vector w which will produce the “ optimal ” hyperplane , can be computed by minimizing ||w||2 and the resultant equation could be written as a linear combination of φ(x ) ’s . Thus , we obtain the following mathematical formulation : w ∑ Ν = Let us symbolize the vector of ai ’s as A=(a1…ai…aN ) . A can be found by solving the following quadratic programming ( QP ) problem : Maximize W(A)=AT 1 1/2 AT QA
, where ai>=0
χφi i ya
( 10 )
(
)
ι
( 11 ) wT x b
= ι 1
Subject to : A≥0 , ATY=0 where Y=(y1…yn ) is a symmetric matrix with elements Qij = yiyjφ(xi)Τφ(xj )
In order to obtain Qij , we can find a kernel K( , ) such that K(xi,xj)=φ(xi)Tφ(xj ) . In that way Qij becomes yiyjK(xi,xj ) . As an example , note that the kernel of a polynomial classifier is k(xi,yj)=(xiTxj +1)d . Besides , notice that there are no local optima in ( 11 ) since Q is always positive and semi definite . There are numerous kernel functions with good generalization capabilities . In our approach we investigate the use of polynomials and RBF networks . For those examples from the training data along the margins of the decision boundary the corresponding ai ’s are greater than zero ( taken from Kuhn Tucker theorem ) , these examples are called support vectors .
For the testing process , given a test vector x∈ℜn , we first compute : h= wT
+)(φ x b
=
∑Ν
= 1ι xxKya , i
( i
+ b
) j
( 12 )
|| w
2||
= 1 i
ξ i
∑ + N C if s>=s0 if otherwise
The class label for each xi is assigned by applying the following empirical rule : Label=1 ( 13 ) Label= 1 The threshold s0 is of course user defined . The SVM algorithm tries to minimize : 1 2 Where ξ should satisfy equations ( 8),(9 ) when added to the right part of each and C is a user defined parameter . The most important advantage of SVM is that contrary to other machine learning techniques , it behaves robustly even in high dimensional feature problems .
7 Neural Nets , LVQ and Decision Tables
Three different machine learning methods were also applied to facilitate unbiased comparison of the outcome :
( 14 )
• A Feed Forward ( FF ) , Back Propagation Neural Network using the LevenbergMarquardt algorithm . This algorithm was selected because of its fast training time and the memory reduction feature it incorporates when dealing with large training sets . Since the environment is used as an additional feature , we encoded its nominal values into numerical ones using the following transformation rule :
Num
(
S
S
1
N
)
N
∑ == 1 i
Ascii
(
S
) i
N
( 15 )
Where <S1…SN> corresponds to the characters used to represent an environment and Ascii(Si ) is the ASCII number of character Si . By using the above transformation , different environments impose insignificant influence to the neural network , unless <S1…SN> is large ( which actually decreases the probability of being a valid SF ) .
• Optimized Learning Vector Quantization ( OLVQ1 ) algorithm ( Kohonen 1987 ) , which was integrated with LVQ pack software . The decision borders concerning the class are approximated by the Nearest Neighbour rule . Again , the above transformation was applied to the nominal values .
• Decision Tables ( DT ) ( Kohavi , 1995 ) provided by the WEKA machine learning workbench . They are easy for humans to understand since they use a default rule that maps to the majority class .
8 Experimental Results
Since a valence dictionary for MG does not exist , it is theoretically impossible to determine with objectivity the entire set of frames that each verb can take . An objective recall value is therefore very hard to obtain . For this reason , our recall value was extracted by asking a linguist to provide the entire set of frames for 47 verbs which frequently occurred in both corpora . Denote this frame set as S47 and a verb belonging to this set as Vi , we tested our methods against this set . Recall is given by the following relation :
R=
Correct(v i Length(v i
) )
( 16 ) where Correct(Vi ) corresponds to the number of frames for verb Vi that are correctly identified and Length(Vi ) is the total number of frames Vi can take . Precision is defined by :
P=
Correct(S ) Length(S )
( 17 ) where Correct(S ) is the number of correctly classified instances found in test set S and Length(S ) corresponds to the total number of instances . Both metrics were extracted by using 10 fold cross validation . The primary training set used to learn a BBN was constructed by manually tagging the class ( Y for a valid SF , N for an invalid SF in the particular environment these candidate frames appear in ) for 4700 instances from the ILSP corpus ( after formulation of the subsets ) and 3655 instances from DELOS corpus , with a window size of 2+3 . The process of finding instances of this set within the remaining window sizes along with those found in DELOS corpus is shown in Table 2 .
Table 2 . Size of training data for different corpora and window sizes . The number of verbs is included in parentheses .
WINDOWS 2+3 2+2 1+2
DELOS 3655(531 ) 2706(442 ) 2181(389 )
4700(802 ) 3480(512 ) 2805(432 )
CORPUS
ILSP
In each experiment two types of input data have been tested : a complete training corpus and a training corpus where all adverbs have been omitted . As adverbs in MG tend to behave far more like adjuncts than arguments , precision increased in the second case , as expected . Tables 3 to 6 , tabulate the precision recall metric for each of the machine learning methods used . Moreover , two different feature sets were experimented with . The former , is the standard set of grammatical and numerical features while the latter is enhanced with the LLR and NDE parameters .
Table 3 . ILSP CORPUS . Results ( Precision recall ) obtained by all methods for all window sizes with different feature sets without taking adverbs into account . METHOD
ILSP CORPUS
WITHOUT ADVERBS
BBN
SVM
FFNN
LVQ
DT
WINDOW SIZE 1+1 2+2 2+3
1+1 2+2 2+3
87,4 67,2 89,7 70,2 88 67,9
89,6 68,5 92 71,7 90,2 68,3
Standard Feature Set
83,6 63,5 86,9 68,4 85,6 64,8
69 58,3 69,4 59,1 69,6 59,7
70 58 73,4 59 73,2 59
Enhanced Feature Set
86,6 65,2 89,7 70 88,5 66
71,1 59,5 71,3 61,4 71,7 61,8
71,9 59,1 75,2 60 75,2 60,1
77,5 62 79,1 64,2 78,5 67,2
80 64,2 81,3 69 80,7 68,7
Table 4 . ILSP CORPUS . Results ( Precision recall ) obtained by all methods for all window sizes with different feature sets including adverbs .
ILSP CORPUS
INCLUDING ADVERBS WINDOW SIZE 1+1 2+2 2+3
1+1 2+2 2+3
BBN
SVM
LVQ
DT
METHOD
FFNN
Standard Feature Set
81,8 61,6 84,1 64,6 82,4 62,3
78,3 58,1 81,5 63 80,2 54,4
62,6 51,9 63 52,7 63,2 53,3
63,4 51,4 66,8 52,4 66,6 52,3
70,9 55,4 72,5 57,6 71,9 66,6
Enhanced Feature Set
84 62,9 86,4 66,1 84,6 62,7
81,4 59,9 84,3 64,6 83,1 66,6
64,7 53,1 64,9 55 65,2 55,4
65,3 52,5 68,6 53,4 68,6 53,5
73,4 57,6 74,7 62,4 74,1 62,1
Table 5 . DELOS CORPUS . Results ( Precision recall ) obtained by all methods for all window sizes with different feature sets without taking adverbs into account . METHOD
DELOS CORPUS
WITHOUT ADVERBS
BBN
SVM
FFNN
LVQ
DT
WINDOW SIZE 1+1 2+2 2+3
1+1 2+2 2+3
Standard Feature Set
82,4 64,2 84,7 67,2 83 64,9
79 60,8 82,1 65,5 80,7 59,6
62,6 54 63,1 54,6 63,3 53,9
63,4 53,4 66,8 54,5 66,6 54,3
71,6 58,1 73,2 59,3 72,6 58,6
Enhanced Feature Set
84,6 65,5 87 67,7 85,2 65,3
82,2 63,6 85 65.9 83,8 65,4
64,7 54,1 64,9 54,5 65,2 55,1
65,3 54,5 68,6 55,4 68.7 55,4
74 59.4 75,4 61,2 74,8 61,9
Table 6 . DELOS CORPUS . Results ( Precision recall ) obtained by all methods for all window sizes with different feature sets including adverbs .
DELOS CORPUS
INCLUDING ADVERBS WINDOW SIZE 1+1 2+2 2+3
1+1 2+2 2+3
METHOD
FFNN
BBN
SVM
LVQ
DT
77,6 59,4 79,8 62,3 78,2 60
79,8 59,6 82,2 63 80,4 61,8
Standard Feature Set
74,2 57 78,4 61 76 60.8
57,7 48.9 57,9 50,6 58 50,2
58,3 48,3 61,7 49,3 61,5 49,1
Enhanced Feature Set
77,2 57,6 80,8 63,6 79,1 64,6
59,6 52,5 59,8 53,9 61,2 54,3
60,2 48,4 63,5 49,9 63,4 49,9
66,5 54 68,1 55,2 67,5 55
69 55.1 70,3 61 69,7 60,7
All tables depict that BBN actually outperforms all other methods by a varying factor of 1,5 3 % compared to the performance of SVM to almost 11 12 % when confronting with the other three . However , training time of BBN was much slower than that of SVM . Especially when using the enhanced feature set , the time for constructing the new Bayesian network was 4,3 times more than SVM ’s training time . The size of the window is also found to be of great importance when dealing with verb subcategorization . As proved by the results , a window size of 2+3 is the best choice for SF detection . Another interesting observation is that DELOS corpus performs worse than ILSP . There are two logical explanations for that . DELOS corpus is an economic corpus with a morphologically narrower set of elements surrounding a verb . For that reason , instances are much less than those in ILSP for a same verb . The other reason is that economists use to develop their own terms and expressions , thus making it difficult for an automatic system to adjust , unless these terms co occur with the verbs many times .
As stated in section 6.1 , BBN are capable of visually representing the impact each feature has to the class parameter of a model . Taking this information into account , one could proceed to reducing the features of the input data . As Figure 2 depicts , no relation between the verb ’s features ( tense , personal or impersonal structure , voice ) and the class of a candidate frame is found . In other words , we could have omitted these parameters during the training process without having to concern about loss of precision . When applying this parameter reduction to the SVM algorithm , the Bayesian independence assumption was verified since precision dropped only by 0.25 % on average using all window sizes . The network was constructed using the “ Enhanced Feature ” set of the 4700 instances of the ILSP corpus . Approximately similar observations occur when constructing a Bayesian Network using the 3655 training examples of DELOS corpus .
Phrase Type
Win Size
Verb Type
Verb Tense
Verb Cat
Case
Prep
FRAME CLASS
Advb
NDE
LLR
Figure 2 . Bayesian Network learned from the “ Enhanced Feature ” set of the 4700 instances of the ILSP corpus . The class node is symbolised as “ FRAME CLASS ”
9 Conclusion
The identification of verb SF could help to the significant improvement of natural language human computer interaction systems since they could embed important information about the syntactic semantic constituents of a verb . In the present paper we introduced the idea of applying BBN and SVM , two machine learning techniques that have proven to perform well in various classification problems to text corpora in order to automatically identify SF . Our idea can be used for other languages as well with slight modifications . For example , for a not “ free word order ” language one would only have to calculate permutations of an environment treating duplicate pairs as different ( N1 P5≠P5 N1 ) . An obvious advantage of using BBN and SVM in corpora and not using a fully syntactic parsed tree bank is that the number of frames learned would grow as the corpus size increases . Since more SF are learned , a dialogue system based on this information would be more robust . Using minimal linguistic resources , ie basic morphological tagging and phrase chunking , verb environments were identified and every environment subset was formulated . Apart from BBN and SVM , Feed Forward Neural Networks , optimized Learning Vector Quantization and Decision Tables were additionally applied to the task at hand , using balanced as well as domain specific corpora and performance was evaluated . Experimental results supported the argument that BBN and SVM are well suited techniques for this task . Contrary to conventional verb subcategorization methods , BBN and SVM found to be very robust , thus eliminating the need for manual adjustment and expensive parameter tuning . Bayesian inference seemed to outperform all other approaches . The difference between SVM is estimated at about 1,5 3 % while it reaches almost 12 % when compared to the other three .
Various window sizes were experimented with and performance with a window size of 1+2 was slightly worse than with that of 2+2 and 2+3 . It is also been demonstrated that adverbs appearing in verb environments are a significant cause of noise , since when they are included in the candidate environments , precision is reduced . New frames not known beforehand were learned throughout the training process .
References
Basili R . , Pazienza MT and Vindigni M . 1997 . Corpus driven Unsupervised Learning of Verb Subcategorization frames . Proceedings of the Conference of the Italian Association for Artificial Intelligence , AI*IA 97 , Rome . Brent M . 1993 . From Grammar to Lexicon : Unsupervised Learning of Lexical Syntax . Computational Linguistics , vol . 19 , no . 3 , pp . 243 262 . Briscoe T . and Carroll J . 1997 . Automatic Extraction of Subcategorization from Corpora . Proceedings of the 5th ANLP Conference , pp356 363 ACL , Washington DC Cooper J . and Herskovits E . 1992 . A Bayesian method for the induction of probabilistic networks from data . Machine Learning , 9 , pp309 347 Cortes C . and Vapnik V . 1996 . Support vector networks . Machine Learning , 20,pp 273 297 De Lima F . 1997 . Acquiring German Prepositional Subcategorization frames from Corpora . Proceedings of the 5thWorkshop on Very Large Corpora ( WVLC 5 ) . Dunning T . 1993 . Accurate Methods for the Statistics of Surprise and Coincidence . Computational Linguistics . vol.19 , no . 1 , pp . 61 74 . Eckle J . and Heid U . 1996 . Extracting raw material for a German subcategorization lexicon from newspaper text . Proceedings of the 4th International Conference on Computational Lexicography , COMPLEX'96 , Budapest , Hungary . Gahl S . 1998 . Automatic extraction of subcorpora based on subcategorization frames from a part of speech tagged corpus . Proceedings of COLING ACL 1998 , pp428 432 Glymour C . and Cooper G . ( eds ) 1999 . Computation , Causation & Discovery . AAAI Press/The MIT Press , Menlo Park . Jeffreys H . 1939 . Theory of Probability . Clarendon Press , Oxford . Jensen F . 1996 . An Introduction to Bayesian Networks . New York : Springer Verlag . Joachims T 1996 . A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization . Carnegie Mellon University , Technical Report , pp 96 118 Kawahara D . , Kaji N . and Kurohashi S . 2000 . Japanese Case Structure Analysis by Unsupervised Construction of a Case Frame Dictionary . Proceedings of COLING 2000 . Kohavi R . 1995 . The power of Decision Tables . The European Conference on Machine Learning ( ECML ) Kohonen T . 1987 . Self Organization and Associative Memory . 2nd Edition , Berlin : SpringerVerlag . Manning C . 1993 . Automatic Acquisition of a Large Subcategorization Dictionary from Corpora . Proceedings of 31st Meeting of the ACL 1993 , pp . 235 242 . Mitchell T . 1997 . Machine Learning . Mc Graw Hill Osuna E . , Freud R . and Girosi F1997 Training support vector machines : an application to face detection . Proceedings of computer vision and pattern recognition .Puerto Rico Pearl J . 1988 . Probabilistic Reasoning in Intelligent Systems : Networks of Plausible Inference . San Mateo , CA : Morgan Kaufmann . Sarkar A . and D . Zeman , 2000 . Automatic Extraction of Subcategorization Frames for Czech . In Proceedings of the 18th International Conference on Computational Linguistics , pp691697 Sgarbas K . , Fakotakis N . and Kokkinakis G . 1999 . A Morphological Description of Modern Greek using the Two Level Model ( in Greek ) . Proceedings of the 19th Annual Workshop , Division of Linguistics , University of Thessaloniki , Greece , April 23 25 , 1999 , pp419 433 Sintichakis M . , Kermanidis K . , Kalamboukis T . 2000 . Corpus Analysis for Applied Lexicography . Proceedings of COMLEX 2000 , pp . 121 126 , Kato Achaia , Greece .
Stamatatos E . , Fakotakis N . and Kokkinakis G . 2000 . A Practical Chunker for Unrestricted Text . Proceedings of the 2nd International Conference of Natural Language Processing ( NLP2000 ) , pp . 139 150 . Vapnik V . 1995 The Nature of Statistical Learning Theory . Springer , New York
