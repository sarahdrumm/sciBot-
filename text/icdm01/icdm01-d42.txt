Incremental Learning of Bayesian Networks with Hidden Variables
Fengzhan Tian
Hongwei Zhang
Yuchang Lu
Chunyi Shi
The State Key Laboratory of Intelligent Technology and System
The Department of Computer Science and Technology
Tsinghua University , 100084 , Beijing , China
Abstract
In this paper , an incremental method for learning Bayesian networks based on evolutionary computing , IEMA , is put forward . IEMA introduces the evolutionary algorithm and EM algorithm into the process of incremental learning , can not only avoid getting into local maxima , but also incrementally learn Bayesian networks with high accuracy in presence of missing values and hidden variables . In addition , we improved the incremental learning process by Friedman et al . The experimental results verified the validity of IEMA . In terms of storage cost , IEMA is comparable with the incremental learning method of Friedman et al , while it is more accurate .
1 . Introduction
In recent years there has been a growing interest in learning Bayesian networks from data[4][5 ] . Because of the disadvantages of the batch learning algorithm[3 ] , it is of great importance and practical value to study incremental learning of Bayesian networks . Previous work on the incremental learning of Bayesian networks has been mostly restricted to updating the parameters assuming a fixed structure[7 ] . The approaches by Buntine[1 ] and by Lam and Bacchus[6 ] are two notable exceptions . While the most important contribution is the incremental learning algorithm by Friedman and Goldszmidt[3 ] .
Nevertheless , there are some disadvantages in the incremental approach of Friedman et al . In this paper , a new method , called IEMA , is put forward . Compared with the work before , the following two improvements were made : learning process . ( 2 ) introduce the evolutionary algorithm and EM algorithm into the process of incremental learning . improve Friedman ’s incremental
( 1 )
The incremental learning method by Friedman et al defines a search frontier at first . This frontier consists of all the networks it compares in the next iteration . We use F to denote this set of networks . We set S to contain all the sufficient statistics needed to evaluate the networks in F . After a new instance is received their procedure updates the values of the sufficient statistics in S and every receiving k instances , checks whether one of the networks in the frontier F is deemed more suitable than the current model . If this is the case , it adopts the new model as current model . And then it invokes the search procedure to determine the next frontier , and updates S accordingly[3 ] .
However , when the initial network is far away from the generating network of the current dataset , the algorithm may stop before reaching the generating network and could not ensure to learn the golden network structure on the current dataset because it seeks only one step forward after reading k data .
Furthermore , through analyzing the algorithm above , the newly added k cases dominate the we know that evaluation results of the networks in the new search frontier and thereby lead the search direction . In this way , when k is larger , the efficiency of the algorithm is very low . When k is very small , the algorithm runs not very robust the characteristic of these k cases . So , there is a contradiction in selecting the value of k . This accords with the experimental results by Friedman et al . performance depends because on its
As for the problems mentioned above , we improved the incremental learning procedure as follows :
Set G to be initial network . Let F initial search frontier for G . While Not Empty ( D )
Read k cases Repeat
∈' FG
Suff
Let S = Suff(G ) ∪ U Update each sufficient statistic record in S using the k cases . max Let G =
SGS ) arg
( G
)'
S
(
|'
∈ '
G
Nets
(
)
2 . Incremental Algorithm IEMA learning with Evolutionary
Proceedings of the 2001 IEEE International Conference on Data Mining ( ICDM(cid:146)01 ) 0 7695 1119 8/01 $17.00 ' 2001 IEEE
Update the frontier F
Until a stop condition is reached .
Compute optimal parameters Θ for G from S . Output ( G ; Θ ) .
In the above procedure , Suff(G ) denotes the set of sufficient statistics for G . Nets(S ) is the set of network structures that can be evaluated using the records in S .
From the above procedure , the value of k in the iteration should be as big as possible . In ideal situation , when the new dataset can be put into memory at one time , the while circulation in the above flow can be carried out only once and the time cost of the algorithm is only up to the repeat circulation . At this time , we can get the best incremental learning performance .
To avoid getting into a local maximum and learn Bayesian networks incrementally from incomplete data , we bring the evolutionary algorithm and EM algorithm into the process of incremental learning , transform the incomplete dataset to complete dataset by EM algorithm , and then learn Bayesian networks with evolutionary algorithms . In addition , for the purpose of learning the network structure with hidden variables , we introduce a new mutation operator and expand the function of the traditional crossover operator [ 2 ] .
The whole process of the IEMA method is as follows :
Set the original evolutionary group F . Select a good network G from F as the initial network . While Not Empty ( D )
Read k cases Repeat
Evolve group F using crossover or mutation operations , and get the evolved group F’ . Let S = U
Suff
G
)'
(
.
∈ ' FG
'
If there are missing values in the k cases , then compute the expected values of the sufficient statistics in S using these k cases by the current network G and EM algorithm . Otherwise , update statistic records in S directly using these k cases . Calculate the fitness
'GF of each network G’ sufficient the in F’ . Choose λ individuals having the highest fitness from F’ to form the next generation F . Let G = arg
'max
FG F∈ G
.
'
Until a terminative condition is reached .
Compute optimal parameters Θ for G from S . Output ( G ; Θ ) .
3 Experimental Evaluation
We compared our method with the method by Friedman et al in terms of the normalized loss and the storage cost on condition of learning on the complete dataset . We use the same method as that by Friedman et al . to generate datasets .
The experimental results show that the normalized loss of IEMA is smaller than the method by Friedman et al . While in terms of storage cost , IEMA needs more storage than the method by Friedman et al , but there is not significant difference between these two procedures .
In addition , we also evaluated the increment learning ability of IEMA in presence of incomplete data and hidden variables . We took out respectively a certain percentage of values and several variables from the original datasets at random . The experiments illustrate that IEMA can learn networks with high accuracy in presence of incomplete data and hidden variables .
At last , in all the experiments , as expected , IEMA can learn more accurate networks and can be more robust by running with larger k .
4 . Conclusions
The results of the experiments verified the validity of IEMA . In terms of storage cost , IEMA is comparable with the incremental learning method of Friedman et al , while it is more accurate .
It is expected that IEMA will cost more time than the method by Friedman et al . However , we believe that this time cost of IEMA can be decreased significantly by setting bigger value of k .
References
[ 1 ] Buntine , W . : Theory refinement on Bayesian networks . In
UAI ’91 .
[ 2 ] Fengzhan Tian , Yuchang Lu , Chunyi Shi . : Learning Bayesian Networks With Hidden Variables Using the Combination of EM and Evolutionary Algorithm ” . In PAKDD 2001 , Aril 2001 , pp568 574
[ 3 ] Friedman , N . , Goldszmidt , M . : Sequential update of Bayesian network structure . In Proc . 13th Conf . on Uncertainty in Artificial Intelligence , 1997
[ 4 ] Heckerman , D . : A tutorial on learning Bayesian network . Technical Report MSR TR 95 06 , Microsoft research ( 1995 ) [ 5 ] Lam , W . , Bacchus , F . : Learning Bayesian belief networks : An Computational approach based on the MDL principle . Intelligence 10 ( 1994 ) 269 293
[ 6 ] Lam , W . , Bacchus , F . : Using new data to refine a Bayesian network . In UAI ’94 .
[ 7 ] Spiegelhalter , D . J . , Lauritzen , S . L . : Sequential updating of conditional probabilities on directed graphical structures . Networks , 20 , 1990 .
Proceedings of the 2001 IEEE International Conference on Data Mining ( ICDM(cid:146)01 ) 0 7695 1119 8/01 $17.00 ' 2001 IEEE
