Combining labeled and unlabeled data for text classification with a large number of categories
Center for Automated Learning and Discovery , Carnegie Mellon University
Rayid Ghani
RayidGhani@cscmuedu
Abstract
A major concern with supervised learning techniques for text classification is that they often require a large number of labeled examples to learn accurately . One way to reduce the amount of labeled data required is to develop algorithms that can learn effectively from a small number of labeled examples augmented with a large number of unlabeled examples . In this paper , we develop a framework to incorporate unlabeled data in the Error Correcting Output Coding ( ECOC ) setup by decomposing multiclass problems into multiple binary problems and then use Co Training to learn the individual binary classification problems . We show that our method is especially useful for classification tasks involving a large number of categories where Co training doesn’t perform very well by itself and when combined with ECOC , outperforms several other algorithms that combine labeled and unlabeled data for text classification .
1 Introduction
The enormous growth of on line information has led to a comparable growth in the need for methods that help users organize such information . One area in particular that has seen much recent research activity is the use of automated learning techniques to categorize text documents . A major difficulty with supervised learning techniques for text classification is that they often require a large number of labeled examples to learn accurately . Collecting a large number of labeled examples can be very expensive , thus emphasizing the need for algorithms that can provide accurate classifications after labeling only a few examples .
One way to reduce the amount of labeled data required is to develop algorithms that can learn effectively from a small number of labeled examples augmented with a large number of unlabeled examples . Unlabeled examples , especially in the case of text classification problems , are much less expensive and easier to obtain than labeled examples . The Web contains a huge amount of text data that can serve as unlabeled data for many classification tasks . Collecting this text is often trivial and quick since web spiders and crawlers can be programmed to automatically do this task . If unlabeled data can be exploited , then the process of building text classification systems will be significantly faster and less expensive than before .
There has been recent work in supervised learning algorithms that combine information from labeled and unlabeled data . Such approaches include using Expectation
Maximization to estimate maximum a posteriori parameters of a generative model for text classification ( Nigam et al . , 2000 ) , using a generative model built from unlabeled data to perform discriminative classification ( Jaakkola & Haussler , 1999 ) , and using transductive inference for support vector machines to optimize performance on a specific test set ( Joachims , 1999 ) . These studies have shown that using unlabeled data can significantly decrease classification error , especially when labeled training data are sparse .
A related body of research uses labeled and unlabeled data in problem domains where the features naturally divide into two disjoint sets . For example , Blum and Mitchell ( 1998 ) present an algorithm for classifying web pages that builds two classifiers : one over the words that appear on the page , and another over the words appearing in hyperlinks pointing to that page . Datasets whose features naturally partition into two sets , and algorithms that use this division , fall into the co training setting ( Blum & Mitchell , 1998 ) . They show that PAC like guarantees on learning with labeled and unlabeled data hold under the assumptions that ( 1 ) each set of features is sufficient for classification , and ( 2 ) the two feature sets of each instance are conditionally independent given the class .
Published studies on text classification with Co training type algorithms ( Blum & Mitchell , 1998 ; Nigam & Ghani , 2000 ) have focused on small , often binary , problems and it is not clear whether their conclusions would generalize to real world classification tasks with a large number of categories . On the other hand , Error Correcting Output Codes ( ECOC ) are well suited for classification tasks with a large number of categories . However , most of the earlier work has focused neither on text problems ( except our earlier work ( Ghani , 2000 ) and ( Berger , 1999) ) , nor on problems with a large number of categories .
In this paper , we develop a framework to incorporate unlabeled data in the ECOC setup to decompose multiclass problems into multiple binary problems and then use Co Training to learn the individual binary classification problems . We show that our approach is especially useful for text classification problems involving a large number of categories and outperforms several other algorithms that are designed to combine labeled and unlabeled data . 2 Error Correcting Output Coding Error Correcting Output Coding ( ECOC ) ( Dietterich & Bakiri , 1995 ) has been shown to perform extraordinarily well for text classification ( Ghani , 2000 ; Berger , 1999 ) . ECOC works by converting a k class supervised learning problem into a large number L of twoclass supervised learning problems . Any learning algorithm that can handle two class learning problems can then be applied to learn each of these L problems . The algorithm works by assigning each class a unique binary string of length n refered to as codewords ( Dietterich & Bakiri , 1995 ) . Then we train n classifiers to predict each bit of the string . The predicted class is the one whose codeword is closest to the codeword produced by the classifiers . The distance metric we use in our experiments is the Hamming distance which counts the number of bits that the two codewords differ by . This process of mapping the output string to the nearest codeword is identical to the decoding step for error correcting codes ( Bose and Ray Chaudhri , 1960 ; Hocuenghem , 1959 ) .
1 . Given a problem with m classes , create an m x n binary matrix M . 2 . Each class is assigned one row of M . 3 . Train the base classifier to learn the n binary functions ( one for each column since each column divides the dataset into two groups ) . ffl Training Phase ffl Test Phase
1 . Apply each of the n single bit classifiers to the test example . 2 . Combine the predictions to form a binary string of length n . 3 . Classify to the class with the nearest codeword ffl Training Phase ffl Test Phase
3 Combining ECOC and Co Training We propose a new algorithm that aims at combining the advantages that ECOC offers for supervised classification with a large number of categories and that of Co Training for combining labeled and unlabeled data . Since ECOC works by decomposing a multiclass problem into multiple binary problems , we can incorporate unlabeled data into this framework by learning each of these binary problems using Co training .
The algorithm we propose is as follows :
1 . Given a problem with m classes , create an m x n binary matrix M . 2 . Each class is assigned one row of M . 3 . Train n Co trained classifiers to learn the n binary functions ( one for each column since each column divides the dataset into two groups ) .
1 . Apply each of the n single bit Co trained classifiers to the test example . 2 . Combine the predictions to form a binary string of length n . 3 . Classify to the class with the nearest codeword
Of course , an n class problem can be decomposed naively into n binary problems and co training can then learn each binary problem , but our approach is more efficient since by using ECOC we reduce the number of models that our classifier constructs and our approach sclaes up sulninearly with the number of classes ( More details about using eCOC for efficient text classification using ECOC can be found in ( Ghani , 2001 ) . We also believe that our approach will perform better than the nave approach under the conditions that :
1 . ECOC can outperform Nave Bayes on a multiclass problem ( which actually learns one model for every class )
2 . Co Training can improve a single Nave Bayes classifier on a binary problem by using unlabeled data
The complication that arises in fulfilling condition 2 is that unlike normal binary classification problems where Co Training has been shown to work well , the use of Co Training in our case involves binary problems which themselves consist of multiple classes . Since the two classes in each bit are created artificially by ECOC and consist of many ” Real ” classes , there is no guarantee that Co Training can learn these arbitrary binary functions . Each of the two classes in every ECOC bit consists of multiple classes in the original dataset . Let ’s take a sample classification task consisting of classes C1 through C10 where one of the ECOC bits partitions the data such that classes C1 through C5 are in one class ( B0 ) and C6 through C10 are in the other class ( B1 ) . The actual classes C1 through C10 contain different number of training examples and it is possible that the distribution is very skewed . If we pick our initial labeled examples randomly from the two classes B0 and B1 , there is no guarantee that we will have at least one example from all of the original classes C1 through C10 . If Co training does not contain at least one labeled example from one of the original classes , it is likely that it will never be confident about labeling any unlabeled example from that class . Under the conditions that :
1 . the initial labeled examples cover every ” original ” class , 2 . the target function for the binary partition is learnable by the underlying classifier , 3 . the feature split is redundant and independent so that the co training algorithm can utilize unlabeled data , theoretically , our combination of ECOC and Co Training should result in improved performance by using unlabeled data .
4 Descriptions of Algorithms Used We use Nave Bayes as the base classifier in our experiments to learn each of the binary problems in ECOC and also as the classifier within Co Training . We also use ExpectationMaximization ( EM ) algorithm to compare with our proposed approach .
4.1 Nave Bayes Naive Bayes is a simple but effective text classification algorithm for learning from labeled data alone ( McCallum & Nigam , 1998 ; Lewis , 1998 ) . We use the multinomial model as defined in ( McCallum & Nigam , 1998 ) where each word in a document is assumed to be generated independently of the others given the class and use Laplace smoothing to calculate word probabilities .
4.2 Expectation Maximization We use the EM algorithm to compare with our approach when learning with both labeled and unlabeled data . It has been shown by Nigam et al . ( 2000 ) that this technique can significantly increase text classification accuracy when given limited amounts of labeled data and large amounts of unlabeled data . However , on datasets where the assumption correlating the classes with a single multinomial component is badly violated , basic EM performance suffers .
EM is an iterative statistical technique for maximum likelihood estimation in problems with incomplete data ( Dempster et al . , 1977 ) . Given a model of data generation , and data with some missing values , EM will locally maximize the likelihood of the parameters and give estimates for the missing values . The naive Bayes generative model allows for the application of EM for parameter estimation . In our scenario , the class labels of the unlabeled data are treated as the missing values . We use the same implementation as used in ( Nigam et al . , 2000 ) . 5 Datasets The datasets used in the experiments are described in this section .
5.1 Hoovers Dataset This corpus of web pages was collected by the WebKB Group at CMU using the Hoovers Online Web resource ( wwwhooverscom ) by crawling 4285 companies on the web and , examining just over 108,000 Web pages . The set of categories consists of 255 classes and label each company with the industry sector it belongs to . Each web site is classified into one category only for each classification scheme . The most populous ( majority ) class contains 2 % of the documents . This dataset has previously been used to compare hytertext classification algorithms ( Ghani et al . , 2001 ) . Since there is no natural feature split available in this dataset , we randomly divide the vocabulary in two equal parts and apply Co Training to the two feature sets . We have previously ( Nigam & Ghani , 2000 ) shown that this random partitioning works reasonably well in the absence of a natural feature split .
5.2 Jobs Dataset We also use a dataset obtained from WhizBang! Labs consisting of Job titles and Descriptions organized in a two level hierarchy with 15 first level categories and 65 leaf categories . In all , there are 132000 examples and each example consists of a Job Title and a corresponding Job Description . We consider the Job title and Job Description as two independent and redundant feature sets for Co Training .
6 Experimental Results All the codes used in the following experiments are BCH codes ( 31 bit codes for the Jobs dataset and 63 bit codes for the Hoovers Dataset ) and are similar to those used in ( Ghani , 2000 ) .
Table 1 : Classification accuracies for the two datasets . Naive Bayes and ECOC do not use any unlabeled data whereas all the other algorithms have access to the same amount of labeled and unlabeled data
Dataset
Naive Bayes
ECOC
10 %
Labeled
100 % Labeled Labeled
10 %
100 % Labeled Labeled
Jobs 65
Hoovers 255
50.1 15.2
68.2 32.0
59.3 24.8
71.2 36.5
58.2 9.1
EM 10 %
Co Training ECOC + Co Training
10 %
Labeled
54.1 10.2
10 %
Labeled
64.5 27.6
6.1 Does Combining ECOC and Co Training Work ? Table 2 shows the results of the experiments comparing our proposed algorithm with EM and Co Training . The baseline results with Nave Bayes and ECOC using no unlabeled data are also given , as well as those when all the labels are known . The latter serve as an upper bound for the performance of our algorithm .
From results reported in recent papers ( Blum & Mitchell , 1998 ; Nigam & Ghani , 2000 ) , it is not clear whether co training will give us any leverage out of unlabeled data on a dataset consisting of a large number of classes . We can see that both Co Training and EM did not improve the classification accuracy by using unlabeled data on the Hoovers255 dataset ; rather they had a negative effect and resulted in decreased accuracy . The accuracy reported for EM and Co Training was decreasing at every iteration and since the experiments were stopped at different times , they are not comparable to each other . On the other hand , our proposed combination of ECOC and Co Training does indeed take advantage of the unlabeled data much better than EM and Co Training and outperforms both of those algorithms on both datasets . It is also worth noting that ECOC outperforms Nave Bayes for both datasets and this is more pronounced when the number of labeled examples is small .
Figure 1 shows the performance of our algorithm in terms of precision recall tradeoff . Precision and Recall are both standard evaluation measures in text classification and Information Retrieval literature . As we can see from the figure , both Nave Bayes and EM are not very good at giving high precision results . This is not surprising since the resulting classifier after learning with EM is a Nave Bayes classifier which gives very skewed scores to test examples and is not good at providing accurate probabilistic estimates . Interestingly , Nave Bayes used within ECOC results in high precision classification at reasonable levels of recall . This result is very encouraging and of enormous value in applications which require high precision results such as search engines and hypertext classification systems .
6.2 Can algorithms other than Co Training be used to learn the Binary problems created by ECOC ?
The framework presented in this paper to incorporate unlabeled data into ECOC , it is not necessary to use Co Training to learn the individual binary functions . Theoretically , any learning algorithm that can learn binary functions from labeled and unlabeled examples can be used . In this section , instead of Co Training , we employ an algorithm named Co EM which is a hybrid of EM and Co Training to learn the binary problems . 621 Co EM Co EM ( Nigam & Ghani , 2000 ) is an iterative algorithm that uses the feature split in a similar fashion as Co Training does . Given a feature split with two feature sets A and B , it trains two classifiers ( one for each feature set ) . It proceeds by initializing the A featureset naive Bayes classifier from the labeled data only . Then , A probabilistically labels all the unlabeled data . The B feature set classifier then trains using the labeled data and the unlabeled data with A ’s labels . B then relabels the data for use by A , and this process iterates until the classifiers converge . A and B predictions are combined together as co n o s i i c e r P
80
70
60
50
40
30
20
10
0
0
Naive Bayes
EM
ECOC + CoTrain
20
40
60
80
100
Recall
Figure 1 : Precision Recall Graph for the Jobs 65 Dataset training embedded classifiers are . In practice , co EM converges as quickly as EM does , and experimentally we run co EM for 10 iterations . The co EM algorithm can be thought of as a closer match to the theoretical argument of ( Blum & Mitchell , 1998 ) than the cotraining algorithm . The essence of their argument is that an initial A classifier can be used to generate a large sample of noisily labeled data to train a B classifier . The co EM algorithm does exactly this using one learner to assign labels to all the unlabeled data , from which the second classifier learns . In contrast , the co training algorithm learns from only a single example at a time . 622 Results Using Co EM instead of Co Training within ECOC performs better for the Jobs Dataset ( 66.1 % accuracy ) and worse for the Hoovers one ( 22.1 % accuracy ) . The key difference between the two algorithms is that Co EM re labels all the unlabeled examples at every iteration while Co Training never re labels an example after adding it to the labeled set . The better performance of Co EM on the Jobs Dataset may be due to the fact that the relabeling prevents the algorithm from getting stuck in local minima and makes it less sensitive to the choice of initial examples .
7 Discussion and Future Work We noted while running our experiments that our approach was very sensitive to the initial documents that are provided as labeled examples . This leads us to believe that some form of active learning combined with this method to pick the initial documents should perform better than picking random documents .
As mentioned in Section 4 , there is no guarantee that Co Training can learn these arbitrary binary functions where the two classes are created artificially . If Co training does not have at least one labeled example from one of the original classes , it is likely that it will never be confident about labeling any unlabeled example from that class . We ran some experiments using training examples that did not cover all ” original ” classes and as expected , the results were much worse than the ones reported in the previous section where a certain number of examples were chosen initially from every class .
There are several other ways in which ECOC and Co Training can be combined , eg training two ECOC classifiers on the two feature sets separately and combining them using Co Training . It will be interesting to pursue the other approaches in future work .
One potential drawback of any approach using Co Training type algorithms is the need for redundant and independent feature sets . In the experiments reported in this paper , we split our feature sets in a random fashion ( for the Hoovers dataset ) . In previous work ( Nigam & Ghani , 2000 ) , we have shown that random partitions of the feature set can result in reasonable performance and some preliminary work has also been done for developing algorithms that can partition a standard feature set into two redundantly sufficient feature sets . This would extend the applicability of our proposed approach to regular datasets .
Although the results shown in Table 1 are encouraging and the combination of ECOC and Co Training does indeed improve clasification acuracy using unlabeled data , there is a lot of room for improvement . The columns for Naive Bayes and ECOC with 100 % labeled data give an upper bound on how well the classifier could do if it had the correct labels for all the unlabeled data . Our approahc deos not reach this level of performance and leaves room for improved algorithms based on this approach . 8 Conclusions
The results described in this paper lead us to believe that the combination of ECOC and Co Training algorithms is indeed useful for learning with labeled and unlabeled data . We have shown that our approach outperforms both Co Training and EM algorithms , which have previously been shown to work well on several text classification tasks . Our approach not only performs well in terms of accuracy but also provides a smooth precision recall tradeoff which is useful in applications requiring high precision results . Furthermore , we have shown that the framework presented in this paper is general enough that any algorithm that can learn binary functions from labeled and unlabeled data can be used successfully within ECOC . References Berger , A . ( 1999 ) . Error correcting output coding for text classification . IJCAI 99 : Workshop on machine learning for information filtering .
Blum , A . , & Mitchell , T . ( 1998 ) . Combining labeled and unlabeled data with co training . Proceed ings of the 11th Annual Conference on Computational Learning Theory ( pp . 92–100 ) .
Dempster , A . P . , Laird , N . M . , & Rubin , D . B . ( 1977 ) . Maximum likelihood from incomplete data via the EM algorithm . Journal of the Royal Statistical Society , Series B , 39 , 1–38 .
Dietterich , T . , & Bakiri , G . ( 1995 ) . Solving multiclass learning problems via error correcting output codes . Journal of Artificial Intelligence Research , 2 , 263–286 .
Ghani , R . ( 2000 ) . Using error correcting codes for text classification . Proceedings of ICML 00 , 17th International Conference on Machine Learning ( pp . 303–310 ) . Stanford , US : Morgan Kaufmann Publishers , San Francisco , US .
Ghani , R . ( 2001 ) . Using error correcting codes for efficient text classification with a large number of categories . masters thesis ( Technical Report ) . Center for Automated Learning and Discovery , Carnegie Mellon University .
Ghani , R . , Slattery , S . , & Yang , Y . ( 2001 ) . Hypertext categorization using hyperlink patterns and meta data . Proceedings of ICML 01 , 18th International Conference on Machine Learning . Williams College , US : Morgan Kaufmann Publishers , San Francisco , US .
Jaakkola , T . , & Haussler , D . ( 1999 ) . Exploiting generative models in discriminative classifiers . Ad vances in Neural Information Processing Systems 11 ( pp . 487–493 ) .
Joachims , T . ( 1999 ) . Transductive inference for text classification using support vector machines . Proceedings of ICML 99 , 16th International Conference on Machine Learning ( pp . 200–209 ) . Bled , SL : Morgan Kaufmann Publishers , San Francisco , US .
Lewis , D . D . ( 1998 ) . Naive ( Bayes ) at forty : The independence assumption in information retrieval .
Machine Learning : ECML 98 , Tenth European Conference on Machine Learning ( pp . 4–15 ) .
McCallum , A . , & Nigam , K . ( 1998 ) . A comparison of event models for naive bayes text classification .
AAAI 98 Workshop on Learning for Text Categorization .
Nigam , K . , & Ghani , R . ( 2000 ) . Analyzing the applicability and effectiveness of co training . Proceedings of CIKM 00 , 9th ACM International Conference on Information and Knowledge Management ( pp . 86–93 ) . McLean , US : ACM Press , New York , US .
Nigam , K . , McCallum , A . K . , Thrun , S . , & Mitchell , T . M . ( 2000 ) . Text classification from labeled and unlabeled documents using EM . Machine Learning , 39 , 103–134 .
