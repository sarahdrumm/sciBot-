A comparison of stacking with meta decision trees to bagging , boosting , and stacking with other methods
Bernard ˇZenko , Ljupˇco Todorovski , and Saˇso Dˇzeroski Department of Intelligent Systems , Joˇzef Stefan Institute {Bernard.Zenko , Ljupco.Todorovski , Saso Dzeroski}@ijs.si
Jamova 39 , Ljubljana , Slovenia
ABSTRACT Meta decision trees ( MDTs ) are a method for combining multiple classifiers . We present an integration of the algorithm MLC4.5 for learning MDTs into the Weka data mining suite . We compare classifier ensembles combined with MDTs to bagged and boosted decision trees , and to classifier ensembles combined with other methods : voting and stacking with three different meta level classifiers ( ordinary decision trees , naive Bayes , and multi response linear regression MLR ) .
Meta decision trees . Techniques for combining predictions obtained from multiple base level classifiers can be clustered in three combining frameworks : voting ( used in bagging and boosting ) , stacked generalization or stacking [ 7 ] and cascading . Meta decision trees ( MDTs ) [ 5 ] adopt the stacking framework of combining base level classifiers . The difference between meta and ordinary decision trees ( ODTs ) is that MDT leaves specify which base level classifier should be used , instead of predicting the class value directly . The attributes used by MDTs are derived from the class probability distributions predicted by the base level classifiers for a given example . An example MDT , induced in the image domain from the UCI Repository , is given below . The leaf denoted by an asterisk ( * ) specifies that the IBk classifier is to be used to classify an example , if the entropy of the class probability distribution predicted by IBk is smaller than or equal to 0002369
IBk:Entropy <= 0.002369 : IBk ( * ) IBk:Entropy > 0.002369 | |
J48:maxProbability <= 0.909091 : IBk J48:maxProbability > 0.909091 : J48 the C4.5 [ 3 ] algorithm for
The original algorithm MLC4.5 [ 5 ] for inducing MDTs was an extension of induction of ODTs . We have integrated the algorithm for inducing MDTs in the Weka data mining suite [ 6 ] . We have implemented MLJ4.8 , a modification of J4.8 ( the Weka re implementation of C4.5 ) : the differences between MLJ4.8 and J4.8 closely mirror the ones between MLC4.5 and C45 Integrating MDTs into Weka lets us perform a variety of experiments in combining different sets of base level classifiers , as well as comparisons to other methods for combining classifiers .
Experimental setup . In order to compare the performance of MDTs with that of other combining schemes , we perform experiments on a collection of twenty one data sets from the UCI Repository of Machine Learning Databases and Domain Theories . Three learning algorithms are used in the base level experiments : the tree learning algorithm J4.8 , which is a re implementation of C4.5 [ 3 ] , the k nearest neighbor ( k NN or IBk ) algorithm and the naive Bayes ( NB ) algorithm . In all experiments , classification errors are estimated using 10 fold stratified cross validation . Cross validation is repeated ten times using different random generator seeds resulting in ten different sets of folds .
At the meta level , the performances of seven algorithms for combining classifiers are compared . These are bagging and boosting of decision trees , voting , stacking with three different meta level learning algorithms ( J4.8 , naive Bayes , and MLR ) , and stacking with MDTs . The performance of each of these algorithms is assessed in terms of its error rate . The performance of MDTs is compared to that of the other combining approaches . The relative accuracy improvement of classifier C1 as compared to classifier C2 is 1− error(C1)/error(C2 ) ( in our case C1 = MDTs ) . The average relative improvement is calculated using geometric mean : 1 − geometric mean(error(C1)/error(C2) ) . The statistical significance of the difference in classification errors is tested using the paired t test ( exactly the same folds are used for C1 and C2 ) with significance level of 95 % .
Results . Stacking with MDTs performs better than bagging and boosting of decision trees , which are the state of the art methods for learning ensembles of classifiers : In both cases MDTs are significantly better in 11 and worse in 3 domains , with a 20 % and 15 % relative accuracy improvement , respectively . A previous study of MDTs [ 5 ] shows that MDTs
Proceedings of the 2001 IEEE International Conference on Data Mining ( ICDM(cid:146)01 ) 0 7695 1119 8/01 $17.00 ' 2001 IEEE
Table 1 . The performance of stacking with MDTs ( error rate in % ) ; the relative improvement in accuracy ( in % ) achieved by stacking with MDTs as compared to bagging , boosting , voting , stacking with J4.8 , naive Bayes and MLR ; and its significance ( +/– : significantly better/worse , x : insignificant ) .
Data set australian balance breast w bridges td car chess diabetes echo german glass heart hepatitis hypo image ionosphere iris soya tic tac toe vote waveform wine Average W/L
Sta . MDT abs . err . 1377±038 851±019 269±007 1608±084 502±027 060±005 2474±054 2771±076 2560±030 3178±119 1604±046 1587±084 079±007 253±009 883±062 473±042 706±014 096±006 354±017 1440±011 326±060 1117±039
Bag . J48 rel . im . 0.74 50.83 45.98 7.89 25.96 1.55 0.48 12.53 2.92 22.08 18.91 10.22 1.62 0.68 12.73 17.44 2.43 85.87 9.94 20.00 36.26 19.89 sig . x + + – + x x + + – + x x x – + x + + + +
Boo . J48 rel . im . 11.63 60.39 27.41 17.17 20.75 56.55 13.28 18.24 12.42 37.10 26.36 13.07 24.62 37.65 37.78 18.39 0.21 72.04 21.03 22.50 19.44 14.78 sig . x + + + – x x + + – + x x x – + x + + + +
Voting rel . im . 0.31 4.49 22.31 1.86 22.73 59.10 3.04 5.22 1.63 7.09 6.28 8.89 40.09 13.72 23.02 12.70 4.55 89.60 50.16 9.44 87.10 18.34 sig . x + + – + x x + – – + x x x – – x + + + –
Sta . J48 rel . im . 5.75 41.49 3.09 4.09 208.54 20.42 3.85 4.31 0.51 17.68 8.84 16.04 4.56 22.92 44.86 22.83 12.04 130.00 12.99 0.15 14.71 4.24 sig . x – + + – x x – – + + x x x – + x – + – +
Sta . NB rel . im . 4.04 7.16 6.93 7.34 89.30 20.42 2.01 1.09 5.50 37.21 5.25 8.55 32.34 61.16 24.00 5.33 7.59 20.69 30.00 4.20 6.45 10.59 sig . x + + + – x x + + + + x x x – + x + + + +
Sta . MLR rel . im . 2.76 10.14 1.57 13.89 10.62 0.00 4.05 3.20 5.09 2.72 4.84 1.23 9.61 10.82 20.16 5.97 2.23 64.29 0.00 0.53 13.73 4.07 sig . x + + – + x x + – – – x x x – – x – x – –
11+/3–
11+/3–
8+/6–
7+/7–
12+/2–
4+/9– perform better than voting and stacking with ODTs . Our study confirms these findings and proves that they are independent of a specific implementation ( we used their reimplementation in Java programming language ) and the set of base level classifiers ( we used a different and smaller set ) . ( Comparing MDTs to ODTs shows a 4 % decrease in accuracy , but this is mostly due to the data sets car and tictac toe , where all combining methods perform very well : if we exclude these two data sets a 7 % increase is obtained ; MDTs are also much smaller than ODTs ) .
Stacking with naive Bayes performs poorly . Stacking with MLR slightly outperforms stacking with MDTs ( a 4 % relative improvement in accuracy ) . Note that stacking with MDTs performs comparably while using less information ( only aggregate data on the class probability distribution is used by MDTs , while the complete class probability distribution is used by MLR ) . The attributes used in MDTs are domain independent once we fix the set of base level classifiers and the language of MDTs is the same for all domains . Another advantage of the MDTs is their understandability : they provide information about the relative areas of expertise of the base level classifiers .
References [ 1 ] Breiman , L . ( 1996 ) Bagging predictors . Machine
Learning , 24(2 ) : 123–140 .
[ 2 ] Freund , Y . and Schapire , R . E . ( 1996 ) Experiments with a new boosting algorithm . In Proceedings of the Thirteenth International Conference on Machine Learning , pages 148 156 . Morgan Kaufmann , San Francisco .
[ 3 ] Quinlan , J . R . ( 1993 ) C4.5 : Programs for Machine
Learning . Morgan Kaufmann , San Francisco .
[ 4 ] Ting , K . M . and Witten , I . H . ( 1999 ) Issues in stacked generalization . Journal of Artificial Intelligence Research , 10 : 271–289 .
[ 5 ] Todorovski , L . and Dˇzeroski , S . ( 2000 ) Combining multiple models with meta decision trees . In Proceedings of the Fourth European Conference on Principles of Data Mining and Knowledge Discovery , pages 54– 64 . Springer , Berlin .
[ 6 ] Witten , I . H . and Frank , E . ( 1999 ) Data Mining : Practical Machine Learning Tools and Techniques with Java Implementations . Morgan Kaufmann , San Francisco .
[ 7 ] Wolpert , D . ( 1992 ) Stacked generalization . Neural Net works 5(2 ) : 241–260 .
Proceedings of the 2001 IEEE International Conference on Data Mining ( ICDM(cid:146)01 ) 0 7695 1119 8/01 $17.00 ' 2001 IEEE
