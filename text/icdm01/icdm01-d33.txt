A Min max Cult Algorithm for Graph Partitioning and Data Clustering
Chris HQ Ding" , Xiaofeng He",* , Hongyuan Zhab , Ming Gu' , Horst D . Simon"
University of California , Berkeley , CA 94720 a NERSC Division , Lawrence Berkeley National Laboratory * De artment of Computer Science and En ineerin !i Pennsy F vania State University , University Par , PA 1&02
Department of Mathematics
{ chqding ,hdsimon} @lbl.gov , { xhe,zha} @csepsuedu , mgu@mat hberkeleyedu
University of California , Berkeley , CA 94720
Abstract
An important application of graph partitioning is data clustering using a,graph model the pairwise similarities between all data objects form a weighted graph adjacency matrix that contains all necessary information for clustering . Here we propose a new algorithm for graph partition with an objective function that follows the min mas clustering principle . The relaxed version of the optimization of the min mas cut objective function leads to the Fiedler vector in spectral graph partition . Theoretical analyses of min mas cut indicate that it leads to balanced partitions , and lower bonds are derived . The min max cut algorithm is tested on newsgroup datasets and is found t o outperform other current popular partitioning/clustering methods . The linkagebased refinements in the algorithm further improve the quality of clustering substantially . We also demonstrate that the linearized search order based on linkage differential is better than that based on the Fiedler vector , providing another effective partition metshod .
1 1ntroductio.n
Graph partitioning has very broad range of applications . At one end are the near regular graphs , the mesh of a 2D surface of an airfoil or a 3D engine cylinder . Partitioning such a mesh into subdomains for distributed memory processors is a common task . Popular software packages for this partitioning task are developed [ 17 , 161 . At another end are the graphs generated from the World Wide Web . These graphs are highly irregular or random , and node degrees vary dramatically . Partitioning the web graph is useful to automatically identify topics from the retrieved webpages for a user query [ 15 ] . Here we emphasize graph partition as data clustering using a graph model . Given the attributes ( coordinates ) of the data points in a dataset and the similaritmy or affinity metric between any two points , the symmetric matrix containing similarities between all pairs of points forms a weighted adjacency matrix ( weight matrix ) of an undirected graph . Thus the data clustering problem becomes a graph partition problem .
The data clustering point of view of graph partfitioning helps to define more appropriate criteria for partitioning . In t,he simplest MINcut algorithm , a connected graph is partitioned into two subgraphs wit,h the cutsize ( cut set ) minimized . However , MINcut often result,s in a skewed cut , ie , a very small subgraph is cut . away [ 4 ] . Various constraints are introduced , such as the rntio cut [ 4 , 141 , the normalized cut [ 22 ] , etc . t,o circumvent the problem . However , skewed cuts st,ill occur when the overlaps between clusters are large .
In this paper , we propose a new graph part,itioii method based on the min mas clustering principle : the similarity or association betwieen two subgraphs ( cut , set ) is minimized , while the similarity or association within each subgraph ( summation of similarity between all pairs of nodes within a subgraph ) is maximized . These two requirements can be satisfied simultaneously with a simple min mas cut function . We present a number of theoretical analyses of min mas cut , and show that min max cut always leads t,o more balanced cuts t,han the ratio cut and the normalized cut .
Like many other methods , the optimal solution to the graph partition problem is NP complete because of the combinatoric nature of the problem . An effective approach is to consider continuous relaxat,ion of such problems . An example is to compute a principal direction/component ( principal eigenvect,or of t,he weight matrix ) , and find a cut point along this direction so that all points 011 one side belong to one subgraph , and all points on the other side belong to anotlier subgraph . This establishes a linear search order on which the minm a s cut can be efficiently applied to search the optimal cut .
The relaxed version of the min mas cut function optimization leads to a generalized eigenvalue problem . The second lowest eigenvector , also called the Fiedler vector , provides a linear search order ( Fiedler order ) . Thus the min max cut algorithm ( we call it Mcut algorithm ) provides both a well defined objective and a clear procedure to search for the optimal solution . We test the algorithm on a number of newsgroup text datasets and compare it with several current methods . The Mcut algorithm outperforms ot,hers .
We introduce a linkage difference metric that effec
0 7695 1 119 8101 $17.00 0 2001 IEEE
107 tively identifies nodes near the cut . We find many nodes sitting on the wrong side of the optimal cutpoint , ie , they have higher linkage to the other cluster than the one they are currently assigned to . Swapping them to the correct side , the objective function is reduced and the clustering accuracy is improved substantially .
It is generally believed that the Fiedler order provides the best known linearized order to search for the optimal cut . Here we find a linkage differential order that provides a better ordination than the Fiedler order . Searching based on linkage differential order consistently outperforms those based on the Fiedler order . The linkage differential order start from any existing clustering results and iteratively improves the ordering and therefore the clustering .
2 Min max cut
Given a weighted graph G = G ( E , V ) with node set V , edge set E and weight matrix W , we wish to partition it into two subgraphs A , B using the min max clustering principle minimize similarity between clusters and maximize similarity within a cluster . This is a souiid principle well established in statistics , data mining and machine learning areas The similarity or association between two nodes is t.lieir edge weight , W( , , . Thus the siniilarit,y between subgraphs A , B is the cutsize cut(. l , B ) = w(A4 , B )
( 1 ) where
I.t’(A,B ) =
Wti,, , , W( 4 ) = W(A,A ) .
( 2 )
UEA,L‘EB
Similarity or association within a cluster ( subgraph A ) is the suni of all edge weight,s within A : W(=l ) . Note that , the weight W , , on node z i is included in IV(,4 ) , which is important for some applicat,ions . Thus the mininax clust,ering principle requires we minimize cut(A , B ) while maximizing W ( A ) and W ( B ) at the same time . All t,liese requirements can be siniulta.neously satisfied by the following objective function ,
We call this new objective function the min max cut function or Mcut for short . Mcut , is inspired by previous works on spectral graph partition [ 2l , 14 , 221 ( see sect,ion 3 ) . It turns out that the continuous relaxation of Eq ( 3 ) must be solved in a way that is different from existing graph partition relaxations [ 21 , 14 , 221 . To reveal the solution , we reorder the rows and columns of W conformally with subgraphs A and B such that
It follows from Eqs.(1,2 ) that cut(A , B ) = xT ( D W ) x = yT ( D W ) y , ( 5 )
W ( A ) = x T W x , W ( B ) = y T W y .
Hence the objective function ( 3 ) can be rewritten as
XT(D W)x . y T ( D W ) y
+
Mcut =
XT w x
YT WY
’
( 6 )
Observe that in Eq ( 6 ) , Mcut is invariant under changes of IIx112 and llYll2 , and xT D y = 0 and xT W x > O,yT W y > 0 .
Taking these relations into account , we obtain a useful lower bound on ( 3 ) in Theorem 1 below . Observe that the problem Eq ( 6 ) can be relaxed into the following optimization problem subject to ll%112 = 1 1 ? 1 1 ~ = 1 , ST 5 = 0 , ZT @ 2 > 0 , YT i ? ? > 0 , where W = D lf’ T/V D ’/’ and 2 = 9 = D1f’y/lD11/2 . The conditions that D’f’~/lDl’f’ , F F 2 > o and 7’ @ ? > o are necessary since w in general is an indefinite matrix . Let the largest 2 eigenvalues of i ? be X I , A ? . XI = 1 by construction . We have Theorem 1 . Assume that A1 + A 2 ,> 0 . Let vectors 2 the following ( proof omitted ) . and ? solve problem Eq ( 7 ) . Choose U to be any column orthogonal matrix such that Q = ( 2 , 7 , U ) is an n x n h h
A orthogonal matrix . Then QT W Q =
0
W
It follows from Theorem 1 that both ratios of ( 7 ) are equal at the optimal solution :
( 8 )
27 ( I @ ) 2 ? T ( I i? ) ?
2 ?q$ ? X 1 + &
% T @ 2
1 , and the optimal value is Mcut = 4/(X1 + A , ) 2 . Since Eq ( 7 ) is a continuous relaxation of Eq ( 3 ) , the fact that the two terms in ( 7 ) are equal at optimal solution suggests that the two terms of Eq ( 3 ) should also be rather “ close ” to each other , implying W ( A ) should be “ close ” to W ( B ) . Hence the resulting clusters tend to have similar weights and are thus balanced . This is one indication that Mcut is a desired objective function for data clustering ( see sections 4 and 7 for more discussions ) .
Let x and y be vectors conformally partitioned with A y = ( OO,ll)T and B , ie , x = ( l ~ ~ ~ l , O ~ ~ ~ O ) T ,
( 4 )
2.1 Fiedler linear search order The solution to partition problem can be represented by an indicator vector q , where the nodal value of q on
108 node U is qu = { a , b } , depending on U E A or B . Finding the optimal partition is NP complete . A wellknown and effective solution is to first compute a linear search order and then find a cut point along this index order that minimizes the Mcut objective . Theorem 1 implies that t h z solution vectors x , y must lie in the eigenspace of W . The first eigenvector z1 = D112 e , e = ( l , . . . , 1)* with the largest eigenvalue A 1 = 1 does not match q . The second eigenvector z2 of W satisfies zTz1 = 0 aiid has positive and negative elements , therefore is a good approximation of q . h
More directly , we can show that where
Relaxing qu t,o rea.1 number in [ 1 , 13 , the solution for minimizing Rayleigh quot,ient Jn ; ( 9 ) is given by
( D W)q = i D q ,
( 10 ) subject , t.0 qTe = 0 . The solution to t.his generalized eigenvalue problem is t,he second eigenvector q2 , called the Fiedler vector , and the corresponing eigenvalue ( 2 is called t,he Fiedler value . Sort,ing the Fiedler vector provides t.he desired linear search order . Furthermore . we oht.ain a lower bound for t,he Mcut . objective ,
Note this bouncl is t8he same as the optimal hlcut , value in Eq ( 8 ) , because < , = 1 A , , aiid 4/(A1 + A , ) 2 = 4/(2 < 2 ) 2 = <2/( 1 < 2 / 2 ) .
3 Related work on spectral graph part it ion
Spectral graph part,itioning is based on the propert,ies of eigenvectors of t,he Lap1acia.n matrix L = D W , first developed by Donath and Hoffnian [ 8 ] and Fiedler [ ll , 121 , and recently populated by tlie work of Pot,hen , Simon and Liu [ ‘ill . The object,ive of the part,itioning is to niininiize tjhe cut size J(*4,.B ) = cut.(*4 , B ) with the requirement that two subgraphs have the same number of nodes : IAl = IBI . Using indicator variable x u , x u = ( 1 , 1} depending 011 U E 9 or B , the cutsize is
Since the trivial x1 = e is associated with A1 = 0 ; the second eigenvector x2 , the Fiedler vector , is the solution .
Hagen and Kahng [ 14 ] remove the requirement IAI = IBI and show that the Fiedler vector provides a good linear search oerder t o the ratio cut ( Rcut ) partitioning criteria[4 ]
Rcut =
+ cut(A , B ) cut(A , B ) .
PI
I At
( 14 )
The use of generalized eigensystem from Eq ( 13 ) t o Eq ( 10 ) has been studied by a number of authors [ 9 , 5 , 221 . Chung [ 5 ] especially emphasizes the advantage of using normalized Laplacian matrix which leads t o Eq ( 10 ) . Shi and Malik [ 22 ] propose the normalized cut , where deg(,4 ) = CtrEA d , is the sum of node degrees , which is also called the zrolumn [ 5 ] of subgraph A , in contrast . to the size of A . They show that Ncut can be reduced to Ncut(A,B ) = J N ( q ) in Eq ( 9 ) . Therefore , Ncut uses t.he same linear search order based on qs as Mcut objective . Following the same analysis , we obtain a lower bound for the Ncut objective , minNcut(‘4 , B ) 2 i z . 9
( 16 )
Here Rcut , , Ncut and Mcut object.ive functions are first , prescribed by motivating corisidera.tioiis and then t,he linear order of the Fiedler vector of ( normalized ) Laplacian matrix is argued to be the appropriate search order ( by relaxing discret.e indicator variables ) . It is import,ant t,o notme t,lia,t tlie same objective functions can be aut80mat,ically obtained as the eigenvalues of t,he Fiedler vect,or using a perturbation analysis on the ( normalized ) La.placian matrix [ GI . This further strengt,hens t.he connection between objective function and the Fiedler vector .
Beside spectra,l partitioning methods , ot,her recent part,it,ioiiing methods seek to minimize t.he sum of subgra.ph diameters , see [ ’i ] or k cent,er problem [ l ] for examples . There are other clustering methods that use singular value decompositions , for example [ lo ] .
4 Random graph model
Perhaps the most important feature of the Mcut method is that it tends to produce balanced cut , ie , the resulting clusters ( subgraphs ) have similar sizes . Here we use the random graph model [ 3 , 41 to illustrate this point . Suppose we have a uniformly distributed random graph with n nodes . For this random graph , any two nodes are connected with probability p . We consider t,he four objective functions , the MINcut , Rcut , Ncut and Mcut . We have the following Theorem 2 . For random graphs , MINcut favors highly cut(A,B ) = e,,EE
( xu x , ) ?
GV , , = xT( D W)X
2
( 12 ) Relax x , from ( 1 , 1} to continuous value in [ 1,1 ] , mininiing cut(A , B ) is equivelent to solve the eigensystem
( D W)x = Ax .
( 13 )
109 skewed cuts , ie , very uneven sizes . Mcut favors balanced cut , ie , both subgraphs have the same sizes . Rcut and Ncut show no size preferences . Proof . We compute the object functions for the partition of G into A and B . Note that tlie number of edges between A and B are plAllBl on average . We have
MINcut(A , B ) = plAllBl
For Rcut , we have
For Ncut , since all nodes have the same degree ( n l)y ,
For Mcut , we have
We now minimize these objectives . Clearly , MINcut favors 1.41 = n 1 and IBI = 1 or IBI = n 1 and 1.41 = 1 , both are skewed cuts . Minimizing R/Icut( l , B ) , we obt,ain a ba.lanced cut : 1.41 = IBI . Rcut , and Nciit , object,ives have no size dependency and no size preference , which also implies possible unstable resu1ts This complet.es t,he proof .
5 Mcut algorithm
The algorithiii for partitioning a graph int,o two subgraphs becomes t.he following .
1 . Compute the Fiedler vector from Eq ( lO ) . Sort nodal values t,o o b k n tlie Fiedler order .
2 . Sea.rch for t,he optimal cut point corresponding to the lowest Mcut based on the Fiedler order .
3 . Do linkage based refinements ( see section 8 ) .
Tlie computation of t,he Fiedler vector can be quickly done via the Lanczos method [ 20 ] . A fast soft.ware package for this calculation , LANSO , is available online ( http://wwwnerscgov/Nkewu/plansohtnil ) T h e Lanczos iteration has computational complexity of O( 1/31 + IVlb 6 Experiments
Document clustering has been popular in analyzing text information . Here we perform experiments on newsgroup articles in 20 newsgroups . We focus on three datasets , each has two newsgroups :
1/2 : altatheism/compgraphics lO/ll : recsportbaseball/recsporthockey 18/19 : talkpoliticsmideast/talkpoliticsmisc
( The newsgroup dataset together with the bow toolkit for processing is available online[l9] ) . Word document matrix X is first constructed . 2000 words are selected according to the mutual information between words and documents
I(tU ) = CP(W d ) l o g , b ( ~ , rl)/P(,4P@)l d where 'U ) represents a word and d represents a document . Words are st.enimed using [ 19 ] . Standard tf . idf scheme for term weighting is used and standard cosine siinilarity between two document,s d l , dz sim( d l , d ? ) = d l . d?/ldllld?l is used . When each document , column of X , is normalized to 1 using Lz norm , documentdocument similarities are d c u l a t e d as PI' = S T A Y . 1.V is interpreted as t,he weight/affinit.y matrix of t,he undirected graph . From this similaritmy matrix , we perform t,he clustering as explained above .
For comparison purpose , we also consider three ot,lier clustering methods : the ra.tio cut [ 4 , 141 , t,he normalized cut [ D ] ( see sect,ion 3 ) and tlie principle direction divisive part,itioning ( P D D P ) [ 2 ] . P D D P is based on t,he itlea of principle component , a.nalysis ( P C A ) applied to t,lie vect,or spa.ce model on S . First . S is centered , ie , the average of ea.ch row ( a word ) is suhtract,ed . Then t,he first , principle component is computed Tlie loadings of t,he docuiiients ( t8he project.ion of each docunient , on t,he principle axis ) form a 1 dim linear search order . This provides a lieurist,ic very similar t,o t,he linear search order provicletl by t8he Fiedler vector . Inst,eacl of sea.rching t.hrough t,o find a minimum based 011 some ohjective function , , P D D P part,itions d a h into two parts a t the cent,er of mass .
To increase st.at,ist,ics , we perform these two clust,er esperinient,s in a way similar t.0 cross validation . iVe divide one newsgroup 4 randonily into IC1 subgroups and the ot,lier newsgroup B randomly into I< ? subgroups . Then one of t,he Iil subgroups of A is mixed with one of t.he I< ? subgroups of B t,o produce a dataset G . The graph partitmion methods are run on t8his dataset . G' t,o produce two clusters . Since t,he true label of each newsgroup article is known , we use accuracy , percentage of newsgroup art.icles correctly clustered , as a measure of success . This is repeated for all K1Ji ? pairs bet.ween A and B , and the accuracy is averaged . In this way , every newsgroup articles is used the same number of times . The mean and standard deviation of accuracy are listed .
In Table 1 , t~he clustering results are listed for balanced cases , ie , both subgroups have about 200 newsgroup articles . Mcut performs about the same as Ncut for newsgroups NGl/NG2 , where the cluster over1a.p is small . Mcut performs substantially better than Ncut for newsgroups NG1O/NG11 and newsgroups NG18/NG19 , where the cluster overlaps are large . Mcut performs slightly better than PDDP . Rcut always performs the worst among the 4 methods and will not be studied further .
In Table 2 , the clustering results are listed for unbal . anced cases , ie , one subgroup has about 300 newsgroup articles and another subgroup has about 200 . This is
110 generally a harder problem due t o the unbalanced prior distributions . In this case , both Mcut and Ncut perform reasonably well , no clear deterioration is seen , while the performance of PDDP clearly deteriorated . This indicates the strength of Mcut method using graph model . Mcut consistently performs better than Ncut for cases where the cluster overlaps are large .
Uataset N G I / N G 2 N G l 0 ) N G l l NG18/NG19
Ncut
I M cut t I 972441 972f08 632f162 964f12 795f110 744j204 549f25 891f47 836f25 575f09 536f31 719*54
R
M cut
I Uataset N G l / N G 2 197.6 f 0.8 % 97.2f 0.8 % 90.6 f 2.1 % 85.7 & 8.3 % 73.8f 16.6 % 87.4 f 2.6 % N G l 0 ) N G l l 78.8 f 4.5 % 65.7f 0.5 % 59.6 f 2.4 % NG18/NG19
PUUP
N cut
7 Skewed cut
We further study the reasons that Mcut consistently outperforms Ncut in large overlap cases . A key observation is that Ncut can be written as cut(A , B ) cut(A , B )
+ W ( B ) + cut(.4 , B ) ’
W ( A ) + cut(A , B ) Ncut = ( 17 ) u E G VVuu = W ( A ) + cut(A , B ) . since deg(A ) E CuEA , Thus , Ncut sometimes cuts out a set with a very sniall weight , ie , a skewed cut , because cut(A , B ) in the denominators help to produce a smaller Ncut value .
We examine several cases and one specific case is shown in Figure 1 . The cut points for Mcut and Ncut and relevant quantity are listed in Table 3 . Ncut has two pronounced valleys , and produces a skewed cut . while Mcut has a very flat valley and gives balanced cuts . Further examination shows that in both cases , the cutsizes obtained in Ncut are equal or bigger than the weight ( self similarity ) of the smaller cluster as listed in Table 3 . In the example of Figure 1 , Ncut produces a cutsize of 262.7 , much larger than the weight W ( B , B ) = 169 . In these cases , clearly the Ncut objective [ see Eq ( 17 ) ] is not appropriate . In the Mcut objective , the cutsize is absent in the denominators ; this provides a balanced cut .
These case studies provide some insights t o these graph partition methods . Prompted by these studies , here we provide further analysis and derive general conditions under which a . skewed cut will occur . Consider
111
0.05 ,
I
I
I
1
0.05‘
0
50
I 100
I 150
I 200
250
300
350
I
50
100
150
200
I 250
300
350
0.6‘ 0
20 ,
I 400
I
I 400 I
0
50
100
150
200
Sorted Nodes
250
300
350
400
Figure 1 : Top : Nodal values of sorted Fiedler vector . Middle : Ncut , values as the cut point moves froni icut = I , 2 , n . Bott,om : Mcut values . A dataset froni NG18/NG19 in Table 1 .
Method Ncut Mciit , zcut 364 141 cut(A , B ) W ( A , A ) W(B,B ) 2 2 2
1 0 4 L Zk : ;
2 L.7
$1
,
Table 3 : Cut , point , cutsize . within cluster similarities for the case in Figure 1 . the balanced cases where W(A ) N W ( B ) . Let cut(A,B ) = f . ( W ) , ( W ) = i ( W ( A ) + bt’(B) ) , where f > 0 is the average fraction of cut relat,ive to within cluster associations .
In the case when tthe partition is optimal , 24 and B are exactly the partitioning result . The corresponding Ncut value is
+
Ncuto = cut(A , B ) cut(A , B )
2f W ( A ) + cut(A , B ) W ( B ) + cut(A , B ) 1 + f ( 18 ) For askewed partitionA1 , B1 , we have W(A1 ) << bV(Bl’ ) , and therefore cut(Al , B1 ) << W(B1 ) . The corresponding Ncut value is
N
Using Ncut , a skewed or incorrect cut will happen if Ncutl < Ncuto . Using Eqs.(l8 , 19 ) , this condition is satisfied if
We can repeat the same analysis using Mcut and calculating Mcuto and Mcutl . T h e condition for a skewed cut using Mcut is Mcutl < Mcuto , which is
Mcut : W(A1 ) 2 cut(A1,Bi ) .
1 2f
For large overlap case , say , f = 1/2 , the conditions for possible skewed cut are :
Ncut : W(Al ) 2 c ~ t ( A l , B 1 ) / 2 Mcut : W ( A 1 ) 2 cut(A1,Bi )
The relevant quantity is listed in Table 4 . For datasets newsgroups 10 1 1 , and newsgroups 18 19 , the condition for skewed Ncut is satisfied most of the time , leading to many skewed cuts and therefore lower clustering accuracy in Tables 1,2 . For the same datasets , condition for skewed hIcut . is not satisfied most of time , leading to more correct cuts and therefore higher clustering accuracy .
Dataset N G l / N G I N G 1 b/N ( 1 ; 1 1 NGlX/NG19
I c u t ( A , B ) W ( X , A ) W(B.B ) I 1412.5 1581.0 1665.5
1766.4 1372.8 2093.9
549.1 772.8 1049.5 f
0.346 0.523 0.558
8 Linkage based refinements
T h e heurist.ic linear search order provided by the’Fiedler vect,or is genera,lly a good heuristic , as t,he resu1t.s shown above . Nevert.heless , it may not necessarily be t,he perfect , one . Here we explore t,liis point and find an effec t,i ve refine men t method which subs t , an t,i a1 1 y imp roves the quality of gmph partitioning .
T h e 1inea.r search order provided by sorting the Fiedler vector q implies t.hat nodes on one side of the cut point must . belong t.0 one cluster : if qu 2 q , q , , where U , t i ! ui are nodes , t,lien t,he linear search will not allow t,he sit,uat,ion h a t U , zu belong to one cluster and pi belongs to the ot.her cluster . Such a strict order is not In fact , in large overlap cases , we expect necessary . some nodes could be moved to the other side of the cut while lowering the overall objective function .
How to identify those nodes near the cut ? For this purpose , we define linkage ! as a closeness or similarity measure between two subgraphs ( clusters ) :
!(il , B ) = W ( A , B ) / W ( A ) W ( B )
( 20 ) here W(A ) , W ( B ) are for normalization purpose so that !(A , B ) is insensitive to cluster weights ( this is motivated by the average linkage !(A , B ) = D‘(A , B)/1A11B1 in hierarchical agglomerative clustering . Following the spirit of min max cut , we replaced [ AI , IBI by W ( A ) , W ( B ) ) . For a single node U , it.s linkage to subgraph A is ! ( A , u ) = W ( A , u ) / W ( A ) . Now we can identify the nodes near the cut . If a node U is well inside a cluster , U will have a large linkage with the cluster , and a small linkage with the other cluster . If U is near the cut , its linkages with both clusters should be close . Therefore , we define the linkage difference
A!(u ) = ! ( u , A ) ! ( U , B ) .
( 21 ) should be near the cut and is a
A node with small A! possible candidate to be moved to the other cluster .
6
4
2
0
.8 I m
2
4
6
50
100
150
I 200
Sorted Ncdes
250
300
350
0
Figure 2 : Linkage difference of all nodes . The vertical line indicates the cutpoint using Mcut . Nodes on the left forms clust.er A and nodes on the right forms cluster B .
In Figure 2 , we show linkage difference A! for all nodes . The vertical line is the cut point . It is interesting to observe that not only many nodes have small A! , but quite a number of nodes whose A t have the wrong signs ( eg , A!(?! ) < 0 if U E A , or , Ae(w ) > 0 if w E B ) . For example , node #G2 has a relatively large negative 4! . This implies node #62 has a larger linkage to cluster B even though it is located in cluster A ( left of the cutpoint ) . Indeed , if we move node #G2 to cluster B , the objective function is reduced . Therefore we find a better solution .
After moving node #G2 to cluster B , we try to move another node with negative A t from cluster A to cluster B depending on whether the objective function is lowered . In fact , we move all nodes in cluster A with negative At to cluster B if the objective function is lowered . Similarly we move all nodes in cluster B with positive Ae to cluster A . This procedure of swapping nodes is called the “ linkage based swap ” . It is implemented by sorting the array s(u)A!(u ) [ s(u ) = 1 if U E A and s ( u ) = 1 if U E B ] in decreasing order to provide a priority list and then moving the nodes , one by one . T h e greedy move starts from the top of the list to the last node U where s(u)A!(u ) 2 0 . This swap
112 reduces the objective function and increases the partitioning quality . In Table 5 , the effects on clustering accuracy due t o the swap are listed . In all cases , the accuracy increases . Note that in the large overlap cases , NG9/NG10 , NG18/NG19 , the accuracy increase about 10 % over the Mcut without refinement . If s(u)At(u ) < 0 but close to 0 , node U is in the correct cluster , although it is close to the cut . Thus we select the smallest 5 % of the nodes with s(u)At(u ) < 0 as the candidates , and niove those which reduce hIcut objective t o the other cluster . This is done in both cluster A and B . We call this procedure "linkage based move" . Again , these moves reduce Mcut objective and therefore improve the solution . In Table 5 , their effects 011 improving clustering accuracy are shown . Adding toget,her , the linkage bassed refinements improve the accuracy by 20 % . Note the final Rilcut result,s are about 3040 % better than Ncut and about G 25 % better than PDDP ( see Tables 5 and 1 ) .
Uat,aset NG1/NC:2 N C l 0 ) N G l l NG18/NG19
RI cut
+Swap
1 197.2 i= 1.1 % 97.5 zt 0.8 % 79.5 4=ll.0 % 85.0 & 8.9 % 83.6 f 2.5 % 87.8 & 2.0 %
+Swap+Move 97.8 f 0 . 7 % 94.1 4=2.2 % 90.0 4=1.4 %
9 Linkage differential order
It is generally believed t,hat the Fiedler order provides t,he hest , kno1vn linearized order to search for t.he optimal cut , ( although delicate counter esamples exist . [ 13 , 231 ) . Is there a linear search order bet,t,er t.lian the Fiedler order' ?
Our analysis in previous sect,ions suggest,s a new linear search order . Given t,he linkage difference in Figure 2 , we see tohat quit,e a few nodes far away from t,he cut point have wrong A t sigiis , t,liat is , they belong t,o the other subgraph . This st8rongly suggesh that the Fiedler order is not necessarily the best linear search order . In fact , we can sort linkage difference At to obtain a linear order different from the Fiedler order , which will be referred t,o as linknge differential ( LD ) order . The search to find the best Mcut cut point based 011 this new LD order represents another improvement over the standard Mcut met,hod .
The results are given in Table 6 . We see that the Mcut values obtained on t,liis new order are lower t,han that based on the Fiedler order . The clustering accuracy also increases substantially . The quality of the clustering based 011 this new order is slightly bet,ter than the results obtained by using Mcut+swap in Table 5 . Therefore , we find a new 1inea.r order that leads t,o better graph part,itioning than that provided by the Fiedler order .
113
Note that the LD order does not depend on the Fiedler order . For example , we can obtain the LD order based on the principal direction in PDDP . Furthermore , the LD order can be recursively applied t o the clustering results obtained from an earlier LD order for further improvements . I
Acc( LD ) Min( F ) Min( LD )
Acc( F )
Dataset NGI/NG2 197.2 f 1.1 % 976*08 % 79.5 j z l l . O % 872*80 % N G l 0 ) N G l l 83.6 f 2.5 % 892*18 % NG18/NG19
0.698 1.186 1.126
0.694 1.087 1.057
10 Hierarchical divisive Mcut
So far in this paper , we focus on bisection a graph into two subgraphs . If more subgraphs or clusters are desired , one can recursively apply Mcut and related refinement to each subgraph , until certain stopping criteria is met , either the desired nuniber of clusters is reached or min( Mcut ) value is above certain pre defined value . Once the recursive division is stopped , some refinements along the lines discussed in section 8 should be applied . This is because even if during each bisection step , all nodes are optiiiially partitioned , the final partition is not necessarily optimal , since they are riot ohtaiiied directly according to the optimal Mcut objective when G is part,itioned into I< suhgra.phs , GI , . . . , G K . Note t,hat for I< 2 3 , each t,erm in McutK will be larger t.lian t,liat in Ii = 2 cases because cut(GP , G , ) , p = 1 , . . . Ii will increase on average while the weight ( selfsimilarity ) W(G' , ) will decrease . Thus , Mcuth . would differ from Ncuth [ 22 ] much more than in the I< = 2 cases [ cf . Eq ( li) ] . From the analysis regarding halanced cuts in previous sect,ions , Ncut is more likely t,o produce skewed cuts Therefore . Mcut is essential in Iy way parttition . When applying the refinementss on K 2 3 clusters , one may apply the 2 way linkage based refinement yairwisely on all pairs of clusters[l8 ] . However , a direct K way linkage based refinement procedure may be adopted : Assume a node U currently belongs to cluster Ci . The linkage difference Atij(u ) = !(,u,Gj ) !(u,Gi ) for all other I< 1 clust,ers are computed . The smallest Atij(u ) and the corresponding cluster id are stored as an entry in a priority list . This is repeated for all nodes so every entry of the list is filled . The list is then sorted according to Aeij(u ) t.0 obt,ain the final priority list . Following the list , nodes are then moved one after another to the appropriate clusters if the overall Mcuth objective is reduced . This completes one pass . For I< 2 3 , several passes may be necessary . 11 Summary
We introduce the Mcut algorithm for graph partit,ion . It is shown that the min max objective function follows t,he clustering principle and produces balanced partitions ; compared to many skewed cuts produced by Ratio cut , Nornmalized cut and P D D P algorithms in cases of large cluster overlaps . The linkage difference metric effectively identifies those nodes near the cut , which leads to effective refinement procedures . Finally , the new linkage differential order is shown to provide a het'ter linear search order than the best known Fierller order . Many datasets such as t,est information are represented by bipartite graphs . h4cut algorithm can also be applied t,o the hi clustering mode1[24 ] on these bipa.rt,ite graph problems . Acknowledgements . This work is supported in part hy Office of Science , Office of Laboratory Policy and Infrastructure , of Depart.ment of Energy under cont,ract DE AC03 TGSF00098 through an LDRD grant and NSF G r ant C CR 9 0 0 19 8 6 . References
PK Agarwal and CM Procopiuc . Exact ancl allproximation algorithms for clust,ering . Proc . 9th A CM SIA hf Sy m pos i II in o 11 Disc re i e A ig o rit h 711 .ti , pages 658 667 , 1998 . D . Bole? ; . Principal direct,ion divisive partit,ioning . Dala m,ining and knowledge discouery , 21325 344 , 1998 . B . Bollohas . Random Gmph,s . Academic Press , 1985 C Ii Cheng and YA Wei . An improved two way partitioning algorithm wit,h stable performance . IEEE . Tram . on Compuied Aided Desgin , 10 : 1.5021511 , 1991 . FRK Chung . Spectral Graph Theory . Amer . Math . Society , 1997 . C . Ding , X . He , aiid H . Zha . A spect.ral met,liod to separate disconnected and nearly disconnected Proc . 7th ACM In.2'1 web graph components . Conf Knowledge Discoriery and Data Mining ( KDD fool ) , pages 275 280 , August 2001 . S . Doddi , MV Marathe , S . S . Ravi , D . S . Taylor , and P . Widmayer . Approximation algorithms for clustering to minimize the sum of diameters . Nordic Journal of Computing , 7(3):185 , Fall 2000 , Fall 2000 . WE Donath and A . J . Hoffman . Lower bounds for partitioning of graphs . IBM J . Res . Develop . , 171420 425 , 1973 .
[ 9 ] R . V . Driessche and D . Roose . An improved spectral bisection algorithm and its application to dynamic load balancing . Parallel Computing , 21 , 1995 .
[ lo ] P . Drineas , A . Frieze , R . Kannan,'S . Vempala , and V . Vinay . Clust,ering in large graphs and matrices . In Proc . 19th ACM SIAM S,ymposiun ? . on Discreie Algori2hmsl 19'39 .
[ ll ] hl . Fiedler . Algebraic connect8ivit,y of graphs .
Czech . Math . J , 23:298 30.5 , 1973 .
[ 12 ] M . Fiedler . A property of eigenvectors of no~inegat,ive symmetric niat,rices arid its applicat.ion to graph theory . Czech . Maih . J . , 25:619 633 , 1975 . [ 13 ] S . Guattery and G . L . Miller . On the qualit , ; of spectral separators . SIAM Journal of i2latri:c Aiinl . A p p l . , 19(3 ) : 1998 .
[ l4 ] L . Hagen and XB Iiahng . New spectral met,hods for ratio cut . partitioning aiid clusteriiig . IEE'E . Trans . 011 Conzpcited Aided Desgin , 11:1074 1085 . 1992 .
[ 15 ] S . He . H . Zha , C . Ding , and HD Sinion . Web docu men t . cl us t er iiig using 11 y p er 1 ink s t r ti c t II res . Tt ch Report CSE 01 006 . April 2001 .
[ l6 ] U , .
Hendrickson and
C haco h i i p : / / i ~ ' i ~ . i ~ . CS.S(I n din . go u/CR F/ch ( I c . hinil . part,itioning mesh
R .
Leland . soft\vare
[ 17 ] ( 1 ; .
Iiarypis and V .
Iiuniar . gr a p 11 I I . S E ~ . S . cs . t r m n . e d u / karypis/iiirfis/ . part it ion i ng sof t.w a r e .
Netis htfp://cou:ri~
[ l8 ] B . \V . Iierniglian and S . Lin . An efficient , heurist,ic procedure for part,it.ioning graphs . T h e Bell Tech . J . , 1970 .
[ 19 ] A . hicCallum . Bow : A toolkit for statist,ical language modeling . text ret.rieva1 , classificat.ion and clust ering . h it p://mu~ ur . cs . ciii U . ed u/m ccall u m/bo iu , 1996 .
[ 20 ] B . N . Parlett . The Synrnietric Eigenaaiue Probleiri .
SIAhI Press , 1998 .
[ 21 ] A . Pothen , H . D . Simon , and Ii . P . Liou . P a r 6 tioniiig sparse matrices wit,h egenvectors of graph . SIAM Joumal of Matrir Anal . Appl . , 11:430 452 , 1990 .
[ 22 ] I Shi and J Malik . Normalized cuts and image segnientat,ion . IEEE . Trans . o n Pattern Analysis a n d Machine Intelligence , 2000 . [ 23 ] DA Spielman and S H Teng .
Spectral partitioning works : Planar graphs and finite element meshes . Proc . 37th IEEE Coiiference on' Foundations of Compuier Science , , 1996 .
[ 24 ] H . Zha , X . He , C . Ding , M . Gu , and HD Simon . Bipartite graph partitioning and data clustering . Proc . 10th Int '1 Conf . Information and Iinowledge Management ( CIICM 2001 ) , Nov . 2001 .
114
