Object Matching in Tweets with Spatial Models
Nilesh Dalvi
Bo Pang
Ravi Kumar Yahoo! Research 701 First Avenue
Sunnyvale , CA 94089 .
{ndalvi,ravikumar,bopang}@yahoo inc.com
ABSTRACT Despite their 140 character limitation , tweets embody a lot of valuable information , especially temporal and spatial . In this paper we study the geographic aspects of tweets , for a given object domain . We propose a user level model for spatial encoding in tweets that goes beyond the explicit geocoding or place name mentions ; this model can be used to match objects to tweets . We illustrate our model and methodology using restaurants as the objects , and show a significant improvement in performance over using standard language models . En route , we obtain a method to geolocate users who tweet about geolocated objects ; this may be of independent interest .
Categories and Subject Descriptors . H3m [ Information Storage and Retrieval ] : Miscellaneous
General Terms . Algorithms , Experimentation , Measurements
Keywords . Tweets , Spatial model , Language model , Object matching
1 .
INTRODUCTION
Tweets are steadily being treated as information sources and there is a growing body of research work to understand and use them in web applications . Tweets provide information that , in a sense , is complementary to what other web sources provide . For example , a user may review a restaurant only once , but may tweet about the same restaurant multiple times . A particular Twitter application that is becoming popular is topic and object recognition : identify which object the tweet is about . The broad success of tweetbeat.com during the 2010 World Cup Soccer , where the aboutness of tweets had to be determined , is a testament to the importance of this application . Motivated by applications such as these , in this paper we study the object matching problem in tweets .
Most of the research so far has been to study individual tweets from an information retrieval and content analysis points of view , to use the set of tweets from a user for recommendation purposes , and to get insights into the social processes behind tweeting . The 140 character limitation on tweets makes many of these tasks already challenging : there is hardly any room to provide complete information about anything , even if the author of the tweet so desired . Given that many of the tweets are content free,1 there is a real limitation on how far tweets by themselves can be used for interesting applications such as object matching .
Fortunately , there are two redeeming aspects to tweets : their volume and the context in which they are generated . By context , we mean the metadata about the author of the tweet and the spatial/temporal information associated with each tweet . A judicious filtering of the content free tweets combined with a careful use of its context opens up new possibilities for using the information tweets contain .
Geographic information is a vital component of the context of a tweet . More than one third of all Twitter users have a valid geographical location and more than half of them have a geographical location that is at a city level ; of course , these are based on self reporting and hence could be erroneous . ( More on these numbers in Section 32 ) And , 3–4 % of all tweets have a latitude longitude ( lat lon ) associated with them . This fraction will grow with the proliferation of GPS enabled devices and social applications such as Foursquare or Gowalla.2 Furthermore , even when such information is not explicitly declared , it can be possible to infer it from the location of places a user typically tweets about . All of these raise the possibility of using geographic information in an intrinsic manner in order to improve the usability of tweets as information .
In this paper we consider a genre of applications , where the goal is to match a tweet to an object , if possible , where the object is from a list of objects in a given domain . We assume that the geographic location of the entities is known . For this object matching task , we develop a user model that utilizes geographic information in an intrinsic way : the probability of a user tweeting about the object depends on the distance between the user ’s location and the object ’s location . We illustrate our model and method by using restaurants as the object domain . There have been a few recent attempts to incorporate geography into topic modeling and study from 2009
1A Pear Analytics pearanalytics.com/blog/wp content/uploads/2010/ 05/Twitter Study August 2009.pdf ) argues 40 % of the tweets are small talk . 2According http://wwwpewinternetorg/~/media/ Files/Reports/2010/PIP Location based services.pdf , on each day , 1 % of the online population uses these services .
( http://www . that over to
43 related tasks but none of these is particularly suitable for our application ( more on this in Section 2 ) .
Main contributions . We formulate a simple user model for generating a tweet about an object . In our model , there are three quantities : each user is associated with an interest in the set of objects ( say , restaurants ) that captures the propensity of the user to tweet about an object from the set , each object has an associated popularity , and each user and each object have their own geographic locations ( locations of objects are given as input to the model , whereas locations of users need to be inferred ) . Now , the probability that the user tweets about an object is a product of the user ’s interest in the object , the popularity of the object , and a decreasing function of the distance between the user and object locations .
With this model , we show an estimation procedure based on the EM algorithm . Unfortunately , the estimation can be computationally challenging . We then show that under reasonable simplifying assumptions on the distribution of users and objects , a computationally efficient method can be used to learn the parameters of the model . As a byproduct , we also obtain a principled method to geolocate tweets : infer the location of a user based on the tweets about geolocated objects .
Our experiments on a large scale collection of tweets that show our geography enabled model is able to achieve significant performance gains over geography less models . We also show that our method to infer geolocation of the users performs well in practice : in our restaurant related dataset , for users with three or more tweets , the median error between the user ’s self reported location and the location inferred by our method is around 10 miles .
Organization . In Section 2 , we discuss the related work on topic models that incorporate geography , matching objects in short user generated content , and geolocating usergenerated content . In Section 3 , we argue why our problem is hard and why traditional methods will not be applicable . In Section 4 , we present the basic model that consists of a distance component based on the user model described above and a language model component that models textual information . In Section 5 , we develop a method to estimate the parameters of the model in a principled manner ; here , we make certain simplifying assumptions that make the estimation procedure computationally tractable so that it can be applied to a large domain such as tweets . In Section 6 , we describe the data source and its properties . In Section 7 , we examine properties of our distance model and in Section 8 , we study the performance of the full model on the matching task , focusing on the performance improvements enabled by the combination of the distance model with the language model . Concluding remarks are presented in Section 9 .
2 . RELATED WORK
The line of work closest to ours is that of topic models that incorporate geography . O’Connor et al . study the geographic linguistic variation in tweets [ 8 , 11 ] . They propose a geographic topic model that incorporates regional vocabulary variations in the way the tweet text is generated . They use the geotagged tweets in their study and their model can be used to classify a tweet into a geographic region . Extending topic modeling to incorporate geographic information has been recently addressed in general information retrieval and NLP contexts . Mei et al . [ 10 ] postulate a model in which the distribution of topics is conditioned on geographical location ; however they assume that the set of locations is given . Sizov [ 14 ] combines text and geographic information , where each topic generates latitude and longitude from two topic specific Gaussian distributions . He shows that this improves tasks such as tag recommendation and clustering . Yin et al . [ 17 ] also study a similar problem but the geographic topic regions are discovered in a more systematic manner . Our work differs from these in the following ways : their granularities are much coarser than what we can afford , they do not incorporate the location of a user , and their goal is not to do object matching .
The question of matching objects to short text such as reviews and blogs was recently addressed in [ 6 , 5 ] , where simple translation models were proposed for the matching ; such models served as motivation behind our language model . These papers , however , do not attempt to model the geographic aspects of the data , which is unique to our setting . The geographic information that is present in tweets and in other user generated content has lent itself to many interesting data mining studies . Backstrom et al . [ 1 ] studied the role of geography in search engine queries . Crandall et al . [ 4 ] studied similar questions for geotagged images . They used text tags of images and geospatial data to find out the location of photos ; see also the work of Rattenbury et al . [ 12 ] on Flickr tags . Backstrom et al . [ 2 ] proposed a method to infer the geographic location of users based on the stated locations of their social connections ; their method is based on postulating a friendship creation model that resembles the geographic component of our model . Recently , Wing and Baldridge [ 15 ] proposed a text based supervised method for geolocating tweets ; their method obtains a median error of 479km.3 All these papers focus only on locating an object in the space using several clues about them . We , on the other hand , make this location detection as part of our model , with the end goal of matching entities to tweets . Finin et al . [ 9 ] tried to use crowd sourcing in order to extract named entities in tweets ; they do not use any geographic information . Tweets are being studied from a sociological point of view : there is a growing literature on this and we refer the readers to [ 3 , 16 , 13 ] and the references therein .
3 . A CLOSER LOOK AT THE PROBLEM
In this section , we discuss motivations of our modeling choices . 3.1 Is text enough ?
We can cast our problem as matching unstructured text such as tweets with structured entities in a database listing ( eg , restaurants with attributes like name , address , and cuisine ) . This can be seen as a cross of two problems : entity matching in databases , which matches pairs of structured entities , and document retrieval in IR , which matches unstructured queries with unstructured documents . Previous work [ 5 , 6 ] has looked into matching unstructured text reviews with structured entities . The main idea behind these
3As we will see in Section 7 , our geolocation method has a median error of below 10 miles ; however , these results are not comparable since [ 15 ] use all tweets of the user whereas we focus only on restaurant related tweets .
44 Figure 1 : A sample restaurant tweet . approaches is to treat the text as a bag of words , which is generated from the mixture of two distributions : an entityindependent review language model and an entity specific model . In the simpler case [ 5 ] , the entity specific model is a distribution over words in the attributes of the entity . The matching problem is to find the entity that maximizes the probability of a given text being generated from it . These previous models focused on the textual information in reviews and did not incorporate contextual information such as metadata about reviewers .
A fundamental challenge in applying these techniques to tweets is the lack of information redundancy since tweets are so short . Figure 1 shows an example of a tweet about a restaurant named Ronnarong . Based on text matching alone , this tweet could potentially match three businesses is our database : Yum Thai , Thai Tapas , and Ronnarong . In fulllength reviews , term frequency can help reveal the correct entity . Eg a full length review of Ronnarong might mention the name ( or other attributes ) several times . But in a short tweet , this does not hold . Using a bag of words model , it is difficult to distinguish between the three matches , each appearing exactly once in the tweet .
One way to combat this is to go for a deeper analysis of the text and take advantage of more structured representations such as a dependency parse of the text . However , given the informal nature of the medium , a full blown parser is unlikely to fare well . Identifying noun phrases using a lightweight part of speech tagging would be insufficient , as we need to distinguish noun phrases that are menu items or location names from noun phrases that are restaurant names . We develop two techniques to address the challenge posed by short tweets . First , we consider a generative language model based on bigrams . Note that the bigram mixture model we consider here is not just allowing each component model to include bigrams ; in a way , we also model transitions between different components ( see Section 422 for more details ) . As an example , our bigram mixture model can be implicitly distinguishing between different types of noun phrases . Eg , we can learn that at is a common word preceding a restaurant name , in commonly succeeds a restaurant name , yum commonly precedes a menu item but not a restaurant , etc . Thus , the more fine grained modeling of the textual information partially compensates for the lack of repetition in tweets .
The second , and the main technique of this paper , involves incorporating locations of the users in the matching . Using our language model , we might be able to correctly identify Ronnarong as the restaurant name . However , we still need to figure out which specific entity in the database it refers to— there can be several restaurants with the same name ( but in different cities ) in our database . Given the short nature of the tweets , the full address of the business , even at the city level , is often missing from the text.4 As such , knowledge
4The tweet in Figure 1 mentions a landmark Union Sq . , but of user locations can immensely help us in disambiguating between the matching restaurants .
As an example , suppose we know the location of the user in the tweet above to be Somerville , MA . Then , we can match the tweet to Ronnarong in Somerville with higher confidence than Ronnarong in other locations . This can also help us further lower the confidence over Yum Thai and Thai Tapas , if those restaurants are not near Somerville . 3.2 Can users be geolocated ?
In the previous section , we argued that knowledge over user locations can be very helpful for the matching task . Is it trivial to identify obtain this information at a useful granularity ? One obvious source seems to be the profile information , which is part of the meta information accompanying each tweet . However , not all users report their locations in their profiles . Based on an analysis of one month of tweet feed ( June , 2010 ) , out of the 22 million unique users who tweeted in this period , 44 % of the users did not specify location in their profile . This accounted for 19.4 % of the total tweet traffic .
Among the users who do have non empty location field , not all of the specified location values correspond to valid geographical locations , eg , Next to you . We applied the Yahoo! Geolocation API , and obtained the country and city information from the profile locations when the API was able to recognize one.5 We found that overall , 33 % of the profiles contain recognizable country names , and only 20 % ( accounting 25 % of the tweet traffic ) contain city names . There are also twitter clients that let users with a geo enabled device to set an exact latitude and longitude as their location , but they amount to less than 2 % of users . If we were to depend on the self reported locations in user profiles , clearly we miss out on 80 % of the users ( 75 % of the tweets ) , not to mention tweets where the self reported locations do not reflect the actual place the user resides in .
In this work we take a different approach . Instead of relying on self reported locations , we try to infer the location of a user based on the geo locations of entities the user tweets about . This presents a technical challenge : we need the user location to compute accurate matches , and we need correct matches to infer the location . Our approach is to use a probabilistic model that combines the language model with the distance model , and jointly infer the location of the users and the matches between tweets and entities . We experimentally demonstrate that our model is very effective in inferring user locations , and for many of the users we can identify the location to within 10 miles . Thus , our technique for location inference helps even for users with valid reported locations . Eg , we might identify that a user with a reported location in New York is in fact in Lower Manhattan . In the next section , we describe our joint model .
4 . MODEL
Let U be a set of users , where each user u ∈ U has a location ( u ) . Let E be a set of entities , where each entity e ∈ E has a set of attributes including its location ( e ) Finally , let T be a set of tweets , where each tweet t ∈ T it does not help as it is not part of the address we have in our database . 5Since this API does not work with foreign text , we removed those profile location that contained non ASCII characters .
45 consists of a piece of text , and is associated with a user u ∈ U , and needs to be matched to an entity e ∈ E . We assume there is an underlying distribution P over the set T × U × E . Our goal is to model P . This will enable us to compute P(e | t , u ) for a triple ( t , u , e ) drawn from this distribution , so that we can predict the best e∗ for an observed tweet–user pair ( t , u ) .
There are two components of the probabilistic model . ( i ) The distance model captures the relationship between U and E . It is given by P(e , u ) , which captures the propensity of a user u tweeting about an entity e .
( ii ) The language model captures the relationship between It is given by P(t | e , u ) , which captures the E and T . distribution of words in a tweet t corresponding to a user u and an entity e . bution over T × U × E can be expressed as P(e , t , u ) = P(t | e , u)P(e , u ) .
The complete model that captures the probability distri
( 1 )
We now discuss the distance model ( Section 4.1 ) and the language model ( Section 4.2 ) in more detail . 4.1 Distance model
In this section we present our model for P(e , u ) , which captures the propensity of a user tweeting about an entity . We expect the geographical distance between a user u and an entity e to play an important role in this model , and use d(e , u ) to denote the distance between ( u ) and ( e ) .
We assume that each user u is associated with an interest α(u ) with respect to the entity set E , eg , if E is the set of restaurants , then α(u ) will capture the interest of the user u in tweeting about restaurants . Similarly , we assume that each entity e has a popularity β(e ) . Finally , the probability of observing a pair ( e , u ) is directly proportional to the interest of the user and the popularity of the entity and monotonically decreases with the distance between them . Assuming a polynomial decay , we have
P(e , u ) ∝ α(u)β(e)(d0 + d(e , u ) )
−k ,
( 2 ) where d0 and k are parameters that govern how fast the probability drops with distance .
We can normalize the above equation by summing over all pairs of entities and users . Let g(e , u ) = α(u)β(e)(d0 + d(e , u ) )
−k , we have
P(e , u ) = g(e , u )
( e,u)∈E×U g(e , u )
.
( 3 )
( 4 )
Note that we use a distance function that exhibits a polynomial decay with respect to the distance between the user and the entity . Alternatively , we could have used , say , an exponentially decaying function . Here , we give the theoretical rationale for our distance function ; Section 7 provides more empirical evidence for our choice .
In theory , exponentially decaying function does not lead to intuitive properties that we expect . For instance , suppose we have a model where the probability of a user u visiting an entity e is proportional to e−kd(e,u ) . Consider the case when we have a user u and two entities e1 and e2 that are d1 and d2 away from u . Suppose they are all located on If y is a constant log 2 k , the same line , with d2 = d1 + y . P ( e2,u ) = e−k(d1−d2 ) = eky = 2 . That is , the we have P ( e1,u ) user is twice as likely to visit e1 than e2 , irrespective of d1 . This behavior is counter intuitive : when a user is really far away from both entities ( d1 >> y ) , we expect them to visit the two entities with almost equal probability . This property makes an exponential model ( or , for that matter , any superpolynomial model ) undesirable . 4.2 Language models
In this work we do not attempt to model the differences in the language usage of different users . That is , we assume that the language model is independent of the user . Formally , given the entity , the probability of observing a given text is independent of the user :
P(t | e , u ) = P(t | e ) .
( 5 )
This is a reasonable assumption — otherwise we will run into data sparsity issues since most of the users will not have sufficiently many tweets to learn user specific language models . Under this assumption , Equation ( 1 ) is reduced to
P(e , t , u ) = P(t | e , u)P(e , u ) = P(t | e)P(e , u ) .
( 6 ) Given an entity e and a tweet t , P(t | e ) denotes the probability of observing the words in the textual content of the tweet , given that the tweet is about entity e . For example , if E is the set of restaurants and e is a specific restaurant , then the tweet is likely to include words from the attributes such as name , address , and cuisine of e , as well as generic words such as dinner and delicious that are associated with restaurants .
Several unigram based language models have been considered in the past for entity matching , including simple termfrequency based models [ 5 ] as well as more advanced translation models [ 6 ] . We consider a model that is an extension of the language model proposed by Dalvi et al . [ 5 ] , which we refer to as the unigram mixture model . We briefly describe the model here .
421 Unigram mixture model Given a set of known entities from a specific category , eg , a set of restaurants , the unigram mixture model represents the probability distribution of documents ( in our case , tweets ) that are about these entities . Here , both the document and the entities are modeled as a bag of words . Given a document t about an entity e , each word in t is drawn randomly from a mixture of two language models : a generic unigram language model Plm for the domain which is independent of e ( eg , words related to restaurants ) and an entity specific language model Pe which is a distribution over words in e ( eg , the name and address of e ) . Given a mixture coefficient θ ∈ ( 0 , 1 ) , it assigns w∈t
P(t | e ) =
( θ · Pe(w ) + ( 1 − θ ) · Plm(w ) ) .
Plm can be learnt by counting the word frequencies of documents in the collection , while Pe can simply be taken as the uniform distribution over the set of words in e.6
Modeling documents and entities as bags of words ( uniIn the grams ) leads to the loss of sequential information .
6If we have different attributes in e , we can give different weights to words in different attributes in Pe , eg , words in names can have more weight than the words in addresses .
46 case of full length documents , the redundancy in the document compensates for this information loss in representation . However , in our setting , since the tweets are very short and most of the times entity names are mentioned just once , we need to make use of the context of name mentions to improve accuracy . Thus , we consider an extension which we will refer to as the bigram mixture model . 422 Bigram mixture model Bigram language models ( more generally , n gram language models ) are well studied , but combining them into a generative mixture model for entities is tricky . Standard bigram language models represents the probability of a sequence of words w1 , . . . , wn as
P(w1 , . . . , wn ) =
P(wi | wi−1 ) . i
Let f ( wi | wi−1 ) be the frequentist estimation , which is the number of occurrences of the sequence ( wi−1 , wi ) divided by the number of occurrences of wi−1 in the data . Usually this is smoothed with a unigram term due to data sparsity ,
( 7 )
P(wi | wi−1 ) = δ · f ( wi | wi−1 ) + ( 1 − δ ) · f ( wi ) . Let Plm and Pe be the bigram language models for the generic language and entity e . We need to adapt the standard bigram language models to account for the fact that wi and wi−1 can be drawn from either Plm or Pe . For each wi , there are four possibilities : it is a continuation of a bigram from Plm , it is a continuation of a bigram from Pe , it is the start of a new bigram from Plm while the previous word was from the entity e , or it is the start of a new bigram from Pe while the previous word was from the generic language .
To take this into account in defining the bigram mixture model , we introduce a new token called entity , which stands for the entity words . In Plm , in addition to the usual bigram sequences ( w , w ) , we also have bigrams of the form ( w , entity ) and ( entity , w ) . Thus , in our model , in addition to better modeling the sequential information in language usage , we model to an extent the context for an entity mention . For instance , for restaurants , we expect a high probability for bigrams such as ( at , entity ) , ( in , entity ) , and ( entity , has ) .
Let qe(w ) denote the posterior of w being drawn from Pe .
( qe(wi−1 ) · A + ( 1 − qe(wi−1 ) ) · B ) ,
We have
P(t | e ) = i where
A = θ · Pe(wi | wi−1 ) + ( 1 − θ ) · Plm(wi | entity ) , B = θ · Plm(entity | wi−1 ) + ( 1 − θ ) · Plm(wi | wi−1 ) , qe(wi ) = qe(wi−1 ) · θ · Pe(wi | wi−1 )
+(1 − qe(wi−1 ) ) · θ · Plm(entity | wi−1 ) . 4.3 The estimation and inference problems Let ∆ denote the parameters of the distance model that include the set {α(u ) | u ∈ U} , the set {β(e ) | e ∈ E} , d0 , and k ( from Equation ( 2) ) . Let Λ denote the parameters of the language model that include Plm , Pe , and θ . Note that ∆ and Λ completely specify the probability model . We assume that we know the location ( e ) of each entity e ∈ E , which can be obtained by geomapping its physical address . However , the locations ( · ) of users are unknown ; let L denote the mapping of all users to locations . Let T be a set of tweets consisting of ( t , u ) pairs ( note that we do not have any ( entity ) labels for the tweets ) . We consider the following estimation problem .
Problem 1 . ( Estimation ) Given T , determine parameters ∆ , Λ , and the locations of users L that maximizes the likelihood of observing T .
Once we learn the model , we want to use it to match tweets with entities . Given a tweet t and a user u , we want to find the most likely restaurant ( if at all ) the tweet is about . This leads to the following inferencing problem .
Problem 2 . ( Inference ) Given a tweet t and a user u , compute
P(e | t , u ) , arg max e∈E
( 8 ) ie , the most likely entity given t and u .
5 . MODEL ESTIMATION
Given a set T of tweet–user pairs , we want to estimate parameters ∆ , Λ , and the locations L of all users that maximize the likelihood of observing T , ie , find given by Equation ( 6 ) . Thus , we want e∈E P(e , t , u ) , where P(e , t , u ) is
P(t , u ) .
( t,u)∈T arg max ∆,Λ,L
We have P(t , u ) = arg max ∆,Λ,L
( t,u)∈T e
P(t | e)P(e , u ) .
( 9 )
The main difficulty in solving Equation ( 9 ) lies in the normalization term in Equation ( 4 ) . Since the term depends on the specific location of each user and each entity in relation to each other , it is hard to compute it exactly . We therefore introduce a set of simplifying assumptions that lead to reasonable interpretations of the model in order to make the problem tractable . 5.1 Making the estimation tractable
We make the following smoothness assumptions to make the model tractable while remaining interesting and useful .
1 . We assume that the density of entities in any small neighborhood is proportional to the density of users in that neighborhood .
2 . Given any region , the α values of the users come from a distribution Γu , which is independent of the region .
3 . Given any region , the β values of the entities come from a distribution Γe , which is independent of the region .
The first assumption is intuitive , as densely populated regions have more businesses . The second assumption states that each region has a similar mix of users in terms of their interests in tweeting about restaurants . Likewise , the third assumption states that each user has access to a similar mix of popular and unpopular entities . These are intuitively true
47 .
π(t , u , e ) = if ( t , u ) is about e , otherwise .
Then , we can write our objective function as fl 1
0 e for any given region . Now , we describe how these assumptions lead to a natural interpretation for the α and β parameters , and how they can be estimated efficiently .
Recall the definition of g(e , u ) given in Equation ( 3 ) . For a user u , define
F ( u ) =
β(e)(d0 + d(e , u ) )
−k .
Similarly , for an entity e , define
G(e ) =
α(u)(d0 + d(e , u ) )
−k . u
Also , while the entities and users are discrete points in space , we approximate their distributions Γu and Γe by assuming them as continuous functions . Let γu and γe be the means of the distributions Γu and Γe respectively . For simplicity , the rest of this section assumes that the densities of the entities and the users are constant , and given by τe and τu respectively . The analysis can be extended to the case when the ratio of the user density to the restaurant density is a global constant .
First , we show that under our assumptions , F ( u ) is a con stant independent of u .
Lemma 3 .
F ( u ) =
2πτeγe
( k − 1)(k − 2)dk−2
0
∞
Proof . Since we assume a continuous distribution of entities in the plane , to compute F ( u ) , we integrate it over the plane . Consider a small disc centered at ( u ) of radius x and infinitesimal width dx . The area of the disc is 2πxdx . Total number of entities in the disc equals τe · 2πxdx . The average value of β(e ) in this region is γe . Thus , we have
F ( u ) =
γe(d0 + x )
−k · τe · 2πxdx
0
Solving the above integral gives us the expression in the lemma .
Likewise , we show that under our assumptions , G(e ) is a constant independent of e .
Lemma 4 .
G(e ) =
2πτuγu
( k − 1)(k − 2)dk−2
.
0
Proof . Similar to the above .
The main conclusion of the above two lemmas is that the terms F ( u ) and G(e ) can be assumed to be constant over the set of users and set of entities respectively . This allows us to interpret α(u ) and β(e ) naturally . For example ,
P(e ) =
α(u)β(e)(d0 + d(e , u ) )
−k = β(e)G(e ) ∝ β(e ) , u since G(e ) is independent of e by Lemma 4 . Also , the parameters α and β are scale invariant model parameters , ie , if we scale each α by some constant and , independently , each β by another constant , we get back the same model . Thus , we can take β(e ) to be exactly equal to P(e ) , ie , the probability that a random tweet is about the entity e . Similarly , α(u ) can be taken as simply the probability that a random tweet is from the user u .
5.2 Learning the parameters
In this section we describe how to solve Equation ( 9 ) . We can write the objective function as
P(t | e)P(e , u ) =
P(u )
P(t | e)P(e | u ) . u
( t,u)∈T e
( t,u)∈T e
Note that P(u ) = α(u ) as we approximated in the previous section . Similarly , P(e | u ) = β(u)(d0 +d(u , e))−k/G(e ) , which does not depend on the α parameters . Thus , we can independently optimize the first product and the rest of the expression , since they do not share parameters . Since α(u ) = P(u ) , we can estimate it as the fraction of all tweets that are by the user u , which can be directly computed from the set T of tweets . For the rest of the section we focus on the following optimization :
( t,u)∈T e arg max ∆,Λ,L
P(t | e)P(e | u ) .
Since the optimization function is in a product of sum form which is difficult to optimize directly , we apply the expectation maximization ( EM ) method to simplify its form . We define a hidden random variable π(t , u , e ) for each triplet : arg max ∆,Λ,L
( t,u)∈T e
( P(t | e)P(e | u))π(t,u,e ) .
Now we can apply EM iteratively .
E step . In the E step , we assume some initial value of the parameters , say ∆0 , Λ0 , L0 and find the expected value of the hidden variables . We have ,
E(π(t , u , e ) ) = P(e | t , u ) =
P(e , t , u ) e P(e , t , u )
.
Using the current values of the parameters , we can compute the above expression . In theory , this can be done by iterating over each entity for each tweet ( t , u ) . This , however , will be infeasible in practice . Instead , for each tweet , we maintain a set of candidate entities that have a non trivial overlap with the text of the tweet , and assume the probability of match to be 0 for all other entities .
M step . In the M step , we use the values f ( t , u , e ) = E(π(t , u , e ) ) and find the parameters ∆ , Λ , L that maximize the following function :

( t,u),e
( P(t | e)P(e | u))f ( t,u,e )

 .(10 )
P(e | u)f ( t,u,e )
=
P(t | e)f ( t,u,e )
( t,u),e
( t,u),e
We see that the first product only involves the language parameters Λ , while the second product only involves ∆ and L , the parameters of the distance model . Thus , we can separately optimize the language model and the distance model .
48 Learning the distance model . We learn d0 and k by directly fitting the observed data , as described in Section 7 . For user locations , we take all possible locations of the tweets ( ie , locations of all possible candidate entity matches for the tweet ) from the user and compute the second product in Equation ( 10 ) from each of the locations to find the optimal location of the user .
Learning the language model . We can learn the bigram language model by a simple frequency counting . We keep a count of all the bigrams . Consider a ( t , e ) pair , along with the value f ( t , u , e ) as computed in the E step . First , we look for the name of entity e in the tweet , and replace it by a single token called entity . If there is no exact match , we look at the maximal common substring of tokens between the tweet and the entity , and treat that as the reference to the entity name . After the replacement , for each bigram in the tweet , we add an amount of f ( t , u , e ) to the global count of that bigram . We process all the ( t , e ) pairs , and finally , normalize all the bigram counts to sum up to 1 . The resulting distribution is the language model Plm that maximizes the first product term in the M step .
6 . DATA
In this section we first describe the data , which consists of tens of billions of tweets recorded over a year . The set of entities we consider are the restaurants in the continental United States .
We use the Twitter data , which consists of all the tweets with the associated metadata such as the name of the user , location of the user ( if available ) , time , etc . We considered the data from December 2009 to January 2011 . We consider the set of restaurants in the US , where each restaurant is associated with metadata such as the name , address , phone number , cuisine , etc . This data is obtained from Yahoo! Local ( localyahoocom ) and consists of about 750K restaurants .
To illustrate the efficacy of our methods , we focus on tweets that pertain to restaurants . Given the large volume of tweets , only a small portion of them will be about restaurants . Ideally we would like to focus only on tweets that have a non negligible chance of being about restaurants . Developing a highly accurate classifier for this purpose is not easy . Previous work has exploited the hashtags in tweets to generate labels for certain classification tasks [ 7 ] ; for example , tweets with the hashtag #hcr can largely be assumed to be related to the health care reform . Most of these hashtags have evolved by convention and usage . Unfortunately , we are not aware of a set of good hashtags that can help in our restaurant classification task . In fact , the top hashtags for the restaurant related tweets ( as determined by the filtering method described below ) are #Yelp , #coupon , #food , #restaurant , #coupon , and they cover less than 15 % of the tweets . Since the main focus in this paper is to investigate the effectiveness of our model , we consider developing such classifiers as an orthogonal ( but interesting ) effort to solve the final task , and opted for a simple keyword based filtering scheme as described below .
We obtained the textual content of about 300K Yelp restaurant reviews used in [ 5 ] , and picked the top occurring words in the reviews ( discarding the stopwords ) as our keyword list . The keywords are : {cafe , restaurant , club , bar , lounge , dinner , lunch , food , pizza , grill , coffee , sushi , tavern , kitchen , breakfast , diner , yelp , taco , burger , drinks , drink , brunch , bbq , bakery , yummy , burrito , cheese , tasty , delicious , crowded , eat , eating} . Tweets that do not contain any of these keywords were filtered out from our collection . As we will discuss in Section 8 , even this seemingly strict filtering scheme results in many tweets that are not about restaurants .
We collected all tweets with a non trivial match to a restaurant name from our set of restaurants ( ie , matching words more uncommon than food , which is the name of three restaurants in our set of restaurants ) . This yielded a dataset of over 200 million tweets , authored by close to 14 million users .
7 . EXPERIMENTS : DISTANCE MODEL
In this section we examine the distance model with which we infer the geographic locations of users . We perform two experiments . The first examines how well our polynomial distance model fits the data . The second examines how accurate the inferred user locations are .
We first look at how our distance model fits the observed data in terms of the distribution of user–entity distance . More specifically , suppose we have an oracle tweet–location mapping ( eg , using tweets that are geotagged ) , does the polynomial decay function in the distance model accurately explains the observed data ? We consider the subsets of tweets that are geotagged ; around 3 % of all tweets fall into this category . For each unique user who authored one of these tweets , we compute the user location by optimizing the EM objective function given by our distance model . We then compute the distance of each tweet to the inferred location of its author . Figure 2 shows the resulting distribution . We also plot the distribution as predicted by the model , ie , ( d0 + x)−k · x : the probability that a user tweets about a restaurant at a distance x is proportional to ( d0 + x)−k , and the number of entities at distance x is proportional to x , assuming a uniform distribution of entities . We compute the parameters d0 and k that best fit the data . Figure 2 plots both the observed and the theoretical distributions , and we see that predicted distribution fits the observed distribution closely ; here , d0 = 6 and k = 3 .
Figure 2 : Observed probability distribution vs . distance model .
In the second experiment , we measure the performance of our distance model in inferring the actual user locations .
0 0.1 0.2 0.3 0.4 0.5 0.6 0.01 0.1 1 10 100 1000probabilitydistance ( miles)observedpredicted49 To this end , we focus on users who have issued at least one restaurant related tweet ( of a restaurant in our collection ) and have self reported their geolocations by explicitly specifying a latitude longitude pair . Since each qualifying tweet carries a user location in the metadata , we obtain a distribution over locations for each such user ; let ( u,· ) denote this distribution . This serves as the ground truth . We then apply the distance model to infer the location of each user in this collection , withholding the ground truth location from the model . We measure the error of the inferred location ( u ) against the ground truth in two ways . In the min measure , the error is the minimum distance between ( u ) and p , where p is in the support of ( u,· ) , ie , ( u , p ) > 0 . In the mode measure , it is the distance between ( u ) and p where p is the mode of the distribution ( u,· ) .
Figure 3 shows the performance curves as a cumulative fraction : the x axis is the distance in miles and the y axis is the fraction of users whose error is within the distance . We study the error for users with at least 3 and at least 10
Figure 3 : User locations : self reported vs inferred . restaurant tweets . For users with at least 3 tweets , about 50 % of the users have an error of at most 10 miles under the stricter mode measure . The fraction jumps to 70 % if we consider the min measure for users with at least 10 restaurant tweets . Note that our model can still be quite accurate when the user moves about . Figure 4 shows the restaurants tweets for a peripatetic user with a self reported location in Atlanta . Even though the restaurant tweets are emanating from disparate locations , the inferred user location is within a mile of the self reported location .
Figure 4 : Location of the restaurant tweets for a user from Atlanta , GA .
These experiments illustrate that our distance model not only accurately captures the true distance distribution of tweet user pairs but also is useful in obtaining the actual location of the user fairly accurately .
8 . EXPERIMENTS : MATCHING 8.1 Experimental setup
In this section we evaluate the performance of the model for the matching task . Note that our model outputs the entity e that maximizes the probability of generating a given tweet . However , for some tweets , we want to return a “ no match ” , if the most likely entity results in a low probability . To this end , we add a dummy entity in our collection , NonRestaurant , which has no distance model but has a language model consisting of the entity independent language Plm . In addition , we assign a prior probability c to this object . The higher the value of c , the more likely a random tweet is non restaurant . If during matching , the most likely entity is found to be NonRestaurant , we say that the tweet has “ no match ” . As default , we set θ = 0.2 , c = 0999
Test data . If we randomly sample from the tweet collection that has been filtered by the keyword list as described in Section 6 , then most are either non restaurant or do not contain a match to any of the candidate restaurants in our restaurant set . On the other hand , if we take only the tweet– match pairs with the highest confidence , then we do not get to evaluate our performance on the low confidence end . In order to obtain a reasonable amount of positive data points with limited human labeling effort , we need to select across the board . To this end , we took the raw score produced by the model in [ 5 ] , ranked the tweet–match pairs according to this raw matching score7 , and selected the same amount of examples to label from each of the following buckets : top 2 % , 2–5 % , 5–30 % , and the rest . We filtered out users with more than 10K tweets as these users are less likely to be real users . 490 tweets sampled this way were labeled in the following procedure . Each tweet , along with all candidate restaurants , is presented to annotators , who either label the tweet as not about any of the candidates ( eg , not a restaurant tweet ) , or choose one of the candidates as the correct match . The location information of the user , if available , is also presented . In order to give more contextual information
7Note that the original model was developed to select the best matching restaurant for documents that are known to be restaurant reviews , thus the raw score is not optimal for this purpose . Nonetheless , it is a good surrogate to obtain a reasonable sample from the dataset .
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0 10 20 30 40 50cumulative fractiondistance ( miles)tweets >= 3 , users = 11256minmode 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0 10 20 30 40 50cumulative fractiondistance ( miles)tweets >= 10 , users = 3018minmode 120˚ 120˚ 105˚ 105˚ 90˚ 90˚ 75˚ 75˚30˚30˚45˚45˚center50 Figure 5 : Performance : θ = 0.2 , c = 0999
Figure 7 : Effect of different values of c for θ = 02 about the user , we also provide other tweets written by the user in the past .
We compare different methods by comparing the precision– recall curves . We compute the precision–recall curve in the following manner . For each tweet t , we take the candidate restaurant r with the highest matching probability p and form a t , r , p tuple . For a given threshold τ , we output r for t if p > τ and for all tweets with p ≤ τ , output “ no match ” . If r is the match chosen for t by annotators , we consider this to be a correct match . Note that this slightly differs from a standard precision–recall curve in that the recall may not reach 1 as we lower τ : if a tweet has an incorrect candidate ranked ahead of the correct one , it will never be correctly “ retrieved ” , no matter how low τ is . 8.2 Results
First , we compare the performance of the language model with and without the distance model . As we can see in Figure 5 , with our default parameter setting ( θ = 0.2 , c = 0.999 ) , we observe clear improvements in the precision–recall curve by utilizing the distance model . This trend is consistent for other c values as well ( Figures 6(a ) and 6(b) ) .
In addition , we investigate the effect of using different parameters . This set of comparison is done without using the distance model . First , we fix θ = 0.2 and experiment with different values of c . As shown in Figure 7 , we achieve better performance with large values of c . If we hope to maintain at least 50 % in recall , then the optimal value for c ranges between 1 − 10−7 and 1 − 10−9 . As we increase c to 1− 10−10 , we observe a clear degradation in performance on the higher recall range . Indeed , for c ranging from 1−10−6 to 1−10−10 , we observe higher c values performing better at the lower end of recall and lower c values performing better at the higher end of recall . This is a reasonable behavior since higher c values are in effect favoring tweets with extremely high confidence in the matching , and user locations are in turn computed with more accurate matches . We then compare different values of θ for c = 1 − 10−9 . Figure 8 shows that for a wide range of θ values ( 02–04 ) , the performance is quite stable .
Finally , We observe that the performance degrade if we switch to the unigram model . Using the default parameter setting , we see that the bigram model outperforms the unigram model when used on its own ( Figure 9(a ) ) as well as when used in conjunction with the distance model ( Figure
Figure 8 : Effect of different values of θ for c = 099999999
9(b) ) . While the distance model improves both the unigram and the bigram models , the gap between them is larger in Figure 9(b ) . This trend is more pronounced for a higher value of c ; we omit these plots due to space constraints .
9 . CONCLUSIONS
In this paper we formulated a user model for generating a tweet about an object . Our model depends on three intuitive quantities : the propensity of a user to tweet about an object , the object ’s inherent popularity , and the distance between the user and object ’s geographic location . We obtained an efficient estimation algorithm that can infer the model parameters . We then used this algorithm to match objects to tweets , focusing on the restaurant domain . Our experiments indicate that the model and methodology we developed work well in practice . As a byproduct of our modeling , we also obtained a principled method to infer the location of a user using the restaurant tweets ; this inference method may be of independent interest .
This object matching primitive is essential for applications that associate an object with tweets . These applications promise to be useful to the web community at large . Future work includes fine tuning the model to take into account multiple locations for a user ( home vs office , travel , etc ) and including a user specific popularity for a restaurant ( preference for a cuisine , decor , etc ) .
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6precisionrecallbigram modelbigram + distance models 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 06precisionrecallc=09099909999909999999099999999909999999999 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 06precisionrecallθ=00010020102030451 ( a ) θ = 0.2 , c = 0.999999
( b ) θ = 0.2 , c = 0.999999999
Figure 6 : Effect of distance models .
( a )
( b )
Figure 9 : Unigram vs . Bigram model , θ = 0.2 , c = 0999
10 . REFERENCES [ 1 ] L . Backstrom , J . M . Kleinberg , R . Kumar , and J . Novak .
Spatial variation in search engine queries . In WWW , pages 357–366 , 2008 .
[ 2 ] L . Backstrom , E . Sun , and C . Marlow . Find me if you can :
Improving geographical prediction with social and spatial proximity . In WWW , pages 61–70 , 2010 .
[ 3 ] E . Bakshy , J . M . Hofman , W . A . Mason , and D . J . Watts .
Everyone ’s an influencer : Quantifying influence on Twitter . In WSDM , pages 65–74 , 2011 .
[ 4 ] D . J . Crandall , L . Backstrom , D . P . Huttenlocher , and
J . M . Kleinberg . Mapping the world ’s photos . In WWW , pages 761–770 , 2009 .
[ 5 ] N . N . Dalvi , R . Kumar , B . Pang , and A . Tomkins .
Matching reviews to objects using a language model . In EMNLP , pages 609–618 , 2009 .
[ 6 ] N . N . Dalvi , R . Kumar , B . Pang , and A . Tomkins . A translation model for matching reviews to objects . In CIKM , pages 167–176 , 2009 .
[ 7 ] D . Davidov , O . Tsur , and A . Rappoport . Enhanced sentiment learning using Twitter hashtags and smileys . In COLING , pages 241–249 , 2010 .
[ 8 ] J . Eisenstein , B . O’Connor , N . A . Smith , and E . P . Xing . A latent variable model for geographic lexical variation . In EMNLP , pages 1277–1287 , 2010 .
[ 9 ] T . Finin , W . Murnane , A . Karandikar , N . Keller ,
J . Martineau , and M . Dredze . Annotating named entites in twitter data with crowdsourcing . In Workshop on Creating Speech and Language Data with Mechanical Turk at NAACL HLT , 2010 .
[ 10 ] Q . Mei , C . Liu , H . Su , and C . Zhai . A probabilistic approach to spatiotemporal theme pattern mining on weblogs . In WWW , pages 533–542 , 2006 .
[ 11 ] B . O’Connor , J . Eisenstein , E . P . Xing , and N . A . Smith . Discovering demographic language variation . In Workshop on Machine Learning for Social Computing at NIPS , 2010 .
[ 12 ] T . Rattenbury , N . Good , and M . Naaman . Towards automatic extraction of event and place semantics from flickr tags . In SIGIR , pages 103–110 , 2007 .
[ 13 ] D . Romero , B . Meeder , and J . Kleinberg . Differences in the mechanics of information diffusion across topics : Idioms , political hashtags , and complex contagion on Twitter . In WWW , pages 695–704 , 2011 .
[ 14 ] S . Sizov . Geofolk : Latent spatial semantics with Web 2.0 social media . In WSDM , pages 281–290 , 2010 .
[ 15 ] B . Wing and J . Baldridge . Simple supervised document geolocation with geodesic grids . In ACL , pages 955–964 , 2011 .
[ 16 ] S . Wu , J . M . Hofman , W . Mason , and D . J . Watts . Says what to whom on Twitter . In WWW , pages 705–714 , 2011 . [ 17 ] Z . Yin , L . Cao , J . Han , C . Zhai , and T . Huang . Geographic topic discovery and comparison . In WWW , pages 247–256 , 2011 .
0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7precisionrecallbigram modelbigram + distance models 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7precisionrecallbigram modelbigram + distance models 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6precisionrecallunigram modelbigram model 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6precisionrecallunigram + distance modelbigram + distance model52
