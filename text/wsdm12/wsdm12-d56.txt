Learning Evolving and Emerging Topics in Social Media : A Dynamic NMF approach with Temporal Regularization
Ankan Saha∗
Department of Computer Science
University of Chicago , Chicago IL 60637 ankans@csuchicagoedu
Vikas Sindhwani
IBM TJ Watson Research Center
Yorktown Heights , NY 10598 vsindhw@usibmcom
ABSTRACT As massive repositories of real time human commentary , social media platforms have arguably evolved far beyond passive facilitation of online social interactions . Rapid analysis of information content in online social media streams ( news articles , blogs,tweets etc . ) is the need of the hour as it allows business and government bodies to understand public opinion about products and policies . In most of these settings , data points appear as a stream of high dimensional feature vectors . Guided by real world industrial deployment scenarios , we revisit the problem of online learning of topics from streaming social media content . On one hand , the topics need to be dynamically adapted to the statistics of incoming datapoints , and on the other hand , early detection of rising new trends is important in many applications . We propose an online nonnegative matrix factorization framework to capture the evolution and emergence of themes in unstructured text under a novel temporal regularization framework . We develop scalable optimization algorithms for our framework , propose a new set of evaluation metrics , and report promising empirical results on traditional TDT tasks as well as streaming Twitter data . Our system is able to rapidly capture emerging themes , track existing topics over time while maintaining temporal consistency and continuity in user views , and can be explicitly configured to bound the amount of information being presented to the user . Categories and Subject Descriptors H33 [ Information Storage and Retrieval ] : Information Search and RetrievalRetrieval Models General Terms Algorithms , Experimentation Keywords Dictionary Learning , NMF , Topic Models , Time Series Analysis ∗Work done as a summer intern at IBM Research
1 .
INTRODUCTION
Over the last few years , the growth and ease of internet access accompanied by the advent of various facets of online social media viz . blogs , social networks and lately , twitter , has provided a vast continuous supply of dynamic diverse information content . When analyzed with appropriate statistical and computational tools , social media content can be turned into invaluable scientific and business insights . A recent large scale study [ 9 ] of 500 million tweets generated in 2009 , for example , concluded that the appearance of flu related topics in Twitter was highly predictive of future influenza rates in the general population .
While the early years of social media platforms were focussed on developing software infrastructure for connecting people , the emphasis has only very recently begun to shift towards understanding collective public opinion by using deep , data driven social media analytics [ 21 ] . One of the most basic and necessary tasks that arises in this setting is to organize streaming social media into coherent threads of discussion that can be easily analyzed and utilized to improve various services that are affected by social media . In this paper , we argue that traditional topic detection and tracking methodologies historically rooted in Information Retrieval literature need to be revisited in the context of the demands of these emerging applications .
Over regular intervals of time ( which might be per day or even every few hours ) , any user who is continuously mining social media content , has the following natural expectations from the system : ( 1 ) to be alerted to any new emerging themes of discussion that is fast gathering steam in social media , ( 2 ) to be able to follow the evolution of existing topics that have already been identified as being of particular interest and ( 3 ) not to be overloaded with excessive bits of information that is time consuming to sift through . Several tradeoffs become naturally evident in the design of a satisfactory system . By definition , an emerging theme is one that has not been observed before and is somewhat of an anomaly in the data stream , not necessarily distinguishable from noise when first encountered . Yet , not every anomaly can be presented to the user as an “ early warning ” as this would lead to excessive information overload in a large scale setting . At the same time , information presented in the past sets up expectations for what the user expects in the future , eg , the ability to clearly see how a topic has evolved possibly in response to marketing or PR interventions . What is needed , is an ability to distinguish valid emerging topics ( with steep information content ) from “ noise ” and some method to continuously summarize essential data character istics in terms of a small number of human interpretable components .
Several social media monitoring tools rely on communicating individual keywords whose usage has rapidly increased in recent time , as proxies for emerging topics . Such a methodology has obvious limitations in characterizing the separate strands of conversations that may have simulatenously Joemerged in the data stream . In this paper , we describe a system for online analysis of streaming text using more rigorous machine learning and optimization methodologies in the form of powerful topic modeling and non negative matrix factorization techniques . Unlike previously proposed IR techniques , our approach incorporates special algorithmic constructs to attempt to detect emerging topics early , maintains temporal continuity while evolving existing topics in response to the statistics of incoming data , and allows the amount of information being presented to the user to be explictly configured . While we demonstrate these ideas in the context of online topic modeling , our methods apply much more broadly to early detection problems in more general signal separation and decomposition settings . A preliminary version of this work appeared in the NIPS 2010 workshop on Social Computing .
2 . RELATED WORK AND OVERVIEW
Over the last decade , different methods have been used for topic modeling and detection from a corpus of documents available as a batch or an online stream . Early impetus in this research was provided by DARPA sponsorship of Topic Detection and Tracking evaluations which led to the design of several TDT engines [ 26 , 1 ] . One of the best performing engines , GAC INCR [ 26 ] , uses a clustering algorithm ( GAC ) to cluster incoming new data in the first phase , and then , based on a similarity/novelty threshold , either merges each of these clusters with those discovered in the past or treat them as a new cluster/topic to be tracked going forward .
Probabilistic latent semantic indexing ( pLSI ) [ 16 ] and Latent Dirichlet Allocation ( LDA ) [ 5 ] are probabilistic methods that have found remarkable success in building topic models of text . Both of them characterize topics as a multinomial distribution over a vocabulary of words rather than clusters of documents . The topics are treated as latent variables and the joint probability of the terms and documents is represented as the mixture of conditional probabilities over the latent topics which are typically inferred by maximum likelihood or Bayesian procedures that involve either variational inference or Gibbs sampling techniques . The notion of a “ topic ” is then communicated to a user via keywords that have highest mass in these learnt distributions.The two models are essentially equivalent [ 12 ] , the key difference being that while the PLSI approach may be viewed as maximum likelihood estimation of model parameters , LDA applies a Dirichlet prior on them . Variants of pLSI and LDA have been proposed for online and dynamic topic modeling ( see [ 6 , 13 , 15 , 4 , 2 ] and references therein ) .
Another line of seemingly unrelated work which finds use in topic modeling is that of dictionary learning and nonprobabilistic matrix factorizations[20 ] . Dictionary Learning is the problem of estimating a collection of basis vectors over which a given data collection can be accurately reconstructed , often with sparse encodings . It may be formulated in terms of uncovering low rank structure in the data using matrix factorizations possibly with sparsity inducing priors [ 20 ] . These are closely related to probabilistic topic models ( pLSI , LDA ) for textual datasets .
In this paper , we propose a framework for online topic detection to handle streaming non negative data matrices with possibly growing number of components . Our methods are rooted in non negative matrix factorizations ( NMF ) [ 18 , 25 ] whose unregularized variants for ( generalized ) KL divergence minimization can be shown to be equivalent to pLSI [ 10 ] . For squared loss , NMF finds a low rank approximation to a data matrix X by minimizing the Frobenius norm of X − WH2 f ro under non negativity and scaling constraints on the factors W and H . Finding the minimum rank NMF of X is a non convex problem and the general algorithm used is due to the multiplicative weight methods of [ 18 ] . It is common to add some form of l1/l2 regularization , generally to encourage sparse factors and prevent overfitting . If X is an N × D document term matrix , then W is a N × K matrix of topic encodings of documents where each column corresponds to a topic and represents the contribution of the documents to the particular topic . H is a K × D matrix of topic word associations , whose rows are the dictionary elements learnt by the NMF approach .
Figure 1 : A snapshot of the online NMF system for tracking and capturing topics
We give a preview of our online learning framework in Figure 1 . At any given timepoint , our system consumes the incoming data together with recently seen documents over a short time window . The output is an NMF that yields a new set of topics together with encodings for the most recently seen documents . These topics can be divided into two sets , which we call evolving and emerging sets . The evolving set is a smooth evolution of previously discovered topics . This evolution is constrained to prevent excessive drift or change that can negatively affect user interpretability . The emerging set comprises of a small number of topics injected into the model for the purpose of detecting emerging themes . This is done by finding the optimal word distributions that show rising temporal trends after correcting for spurious discontinuities . We show that constrained topic evolution and trend estimation can be posed naturally as extended matrix factorization problems that infact also have a link to margin based t−1tt−2t−3X(t)time}X(t−1)evolvingemergingH(t)W(t)INPUTsliding windowωOUTPUT learning methods such as SVMs . We then develop scalable alternating optimization algorithms using efficient schemes to solve rank one subproblems . Once the online model is learnt , the emerging set ( whose size can be configured ) is presented to the user who may choose to explicitly discard a subset from current consideration . Going forward in time , the emerging set becomes part of the evolving set , and new emerging bandwidth is introduced for the next timepoint . The use of explicit temporal regularizers for emerging topic detection in a matrix factorization framework in this manner is novel to the best of our knowledge . Prior work on online matrix factorization has not dealt with low rank approximations with gradually increasing rank . In the next section , we describe the details of our online models .
Notation : In the sequel we abuse notation to denote hi as the ith row of H and hij = Hij . ∆D denotes the D [ K ] refers to the set {1 , 2 . . . K} and dimensional simplex . 0d , 1d refers to the vector of all 0 ’s and 1 ’s of dimension d .
3 . DYNAMIC NMF FRAMEWORK
Let {X(t ) ∈ RN ( t)×D(t ) , t = 1 , 2t , } denote a sequence of streaming matrices where each row of X(t ) represents an observation whose time stamp is t . For simplicity in notation and exposition , we will assume that D(t ) = D for all t . In topic modeling applications over streaming documents , X(t ) will represent the highly sparse document term matrix observed at time t 1 . We will use the conventional vector space model [ 23 ] used in the information retrieval literature . Terms in a document are statistically weighted using standard measures Term Frequency(T F ) and Inverse Document Frequency(IDF ) . The ( d , r) th entry of X(t ) corresponding to document d and term r is given by
X(t)(d , r ) =
( 1 + log2 T F ( d , r ) ) × log2 IDF ( r )
C where C normalizes the representation to unit norm l2 norm vectors . We use X(t1 , t2 ) to denote the document term matrix formed by vertically concatenating {X(t ) , t1 ≤ t ≤ t2} . Since we operate in an online framework , we introduce a short sliding time window ω over which trends are estimated at every time point .
At the current time point t , our model consumes the incoming data X(t ) and appends to documents seen in a recent ω window , X(t − ω + 1 , t ) . It generates a factorization ( W(cid:63 ) , H(t ) ) comprising of K(t ) topics ( see equation ( 1) ) . Each column of W(cid:63 ) and each row of H(t ) corresponds to a topic . Note that W(cid:63 ) has the same number of rows as all the documents accumulated over the ω window ending at time point t . The last N ( t ) rows of W(cid:63)(corresponding to X(t ) ) represent the weight matrix W(t ) at time t . Furthermore , since H(t ) is the matrix of topic term dependence at time t , we normalize each row of H(t ) so that it resembles a probability distribution of words over the corresponding topic . This online factorization mechanism is designed to satisfy two considerations :
• The first K(t − 1 ) topics in H(t ) must be smooth evolutions of the K(t − 1 ) topics found upto the previous 1As new documents come in and new terms are identified , we expand the vocabulary and zero pad the previous matrices so that at the current time t , all previous and the current documents have a representation over the same vocabulary space . time point in H(t − 1 ) . These topics are assumed to be gradually changing over time ( with steady temporal profile ) . We call this the evolving set and introduce an evolution parameter , δ , which constrains the evolving set so that each entry of these K(t − 1 ) topics in H(t ) resides within a box of size δ on the probability simplex around their previous values in H(t − 1 ) ( see equation ( 4) ) . With minor modifications , δ can also be made topic or word specific eg , to take topic volatility or word dominance into account .
• The second consideration is the fast detection of emerging topics . At each time point , we inject additional topic bandwidth for this purpose which constitute the K(t ) − K(t − 1 ) remaining rows of H(t ) . This represents the emerging set .
Thus the topic variable H(t ) can be partitioned into an evolving set of K(t − 1 ) topics , Hev , and an emerging set of K em topics Hem where K(t ) = K(t − 1 ) + K em . It should be noted that the set Hev is an increasing set over time . Older topics that are no longer active may be removed for efficiency , but for simplicity , we do not discuss such a removal process in this paper . As new discussions take place over social forums , new topics emerge and are added to the system . Furthermore , we assume that emerging topics can be distinguished from noise based on their temporal profile . In other words , the number of documents that a true emerging topic associates with rapidly increases . This may occur due to a sudden large increase in discussions about the topic in documents over time .
The discussion above motivates the following objective function that is optimized at every time point t . X(t − ω + 1 , t ) − WH2 ( W(cid:63 ) , H(t ) ) = argmin f ro + µΩ(W )
W,H
( 1 ) where Ω plays the role of a temporal emergence regularizer ( described in section 4 ) which penalizes static temporal profiles and encourages the discovery of topics whose temporal profiles exhibit steep increase . These intuitively correspond to the emerging topics . This objective function is minimized under the following non negativity , normalization and evolution constraints as discussed above .
W , H ≥ 0
D
Hij = 1
∀i ∈ [ K(t − 1 ) + K em ]
( 2 ) j=1 min(Hij(t − 1 ) − δ , 0 ) ≤ Hij ≤ max(Hij(t − 1 ) + δ , 1 ) ,
( 3 )
∀i ∈ [ K(t − 1)],∀j ∈ [ D ]
( 4 )
The last equation ( 4 ) enforces the smoothness condition for the evolving topics Hev by forcing the individual topic components at time t to stay within δ of their value at the previous time step .
We then extract W(t ) from the bottom rows of W(cid:63 ) that correspond to X(t ) which came in at time t . Since W(t ) stores the weights assigned to each topic by the documents , the ith document ( row ) in X(t ) is tagged in the following way :
πsystem(i ) = argmax
W(t)(i , j )
( 5 ) j
Figure 2 :
. Thus each document at time t is assigned to the corresponding most dominating topic by the system . Note that this gives a clustering of the documents per topic . In the next section , we define the emergence regularization operator Ω(W ) that forms an essential component of our optimization algorithm . 4 . EMERGENCE REGULARIZATION
Let {yt}T
In this section , we formulate the temporal regularization operator Ω(W ) by chaining together trend extraction with a margin based loss function to penalize static or decaying topics . We begin with a brief introduction to trend filtering . 4.1 Hodrick Prescott ( HP ) Trend Filtering t=1 be a univariate time series which is composed of an unknown , slowly varying trend component {xt}T t=1 perturbed by random noise {αt}T t=1 . Trend Filtering is the task of recovering the trend component {xt} given the observations {yt} . The Hodrick Prescott filter is an approach to estimate the trend assuming that it is smooth and that the random residual is small . It is based on solving the following optimization problem :
( yi − xi)2 + λ
( (xt+1 − xt ) − ( xt − xt−1))2 i=1 t=2
( 6 ) The first term tries to minimize the reconstruction error while the second term penalizes the change in the underlying time series over successive time points thus enforcing the concept that the underlying trend component is smooth . Let us introduce the second order difference matrix D ∈ R(T−2)×T such that Dii = 1 , Di,i+1 = −2 and Di,i+2 = 1 for all i ∈ [ T − 2 ] . It is easy to see that the solution to the optimization problem of Equation 6 is given by : x = [ I + 2λD
−1y
D ] where we use the notation y = ( y1 . . . yT ) , x = ( x1 . . . xT ) . In the sequel , we use F to denote [ I + 2λDD]−1 , the linear smoothing operator associated with the Hodrick Prescott Filter . Given the time series y , the Hodrick Prescott ( HP ) trend estimate simply is x = F y . Figure 2 captures the notion of Hodrick Prescott trend filtering where a smooth reconstruction of the observed signal is demonstrated for different values of the parameter λ . In our experiments , we use λ = 10 . 4.2 Loss Function for Emerging Trends Let x = F y be the HP trend of the time series y . Let D be the forward difference operator , ie , the only non zero
T argmin
{xt}
1 2
T−1
T−1 i=1 entries of D are : Di,i = −1 and Di,i+1 = 1 . If z = Dx , then zi = xi+1 − xi reflects the discrete numerical gradient in the trend x . Given zi , we define a margin based loss function ( the 2 hinge loss ) , L(zi ) = ci max(0 , ν − zi)2 . If the growth in the trend at time i is sufficient , ie , greater than ν , the loss evaluates to 0 . If the growth is insufficient , the loss evaluates to ci(ν − zi)2 where ci is the weight of timepoint i . We observed experimentally that the best results are given by weights ci ’s which typically increase with i . For a vector z , the loss is added over the time components . In terms of the original time series y , this loss function is ,
L(y ) = ci max(0 , ν − ( DF y)i)2
( 7 )
Optimization Problem : As documents arrive over t ∈ {1 , 2 , . . . T} , we use S to denote a T × N time document matrix , where S(i , j ) = 1 if the document j has time stamp i . Noting that each column w of W , denotes the document associations for a given topic , Sw captures the time series of total contribution of the topic corresponding to w , which is analogous to the temporal profile of the topic and is expected to rapidly grow for emerging topics . Finally , we concretize ( 1 ) as the following optimization problem
X − WH2 f ro + µ argmin W,H≥0
L(Swi )
( 8 ) wi∈Wem subject to constraints in equations 3 and 4 . Note that the sum in the penalization term only runs over the emerging topic variables .
5 . OPTIMIZATION ALGORITHMS
We approximate X as the sum of rank one matrices wih i and optimize cyclically over individual wi and hi variables while keeping all other variables fixed . This results in three specific sub problems , each of which requires an efficient projection of a vector onto an appropriate space . Optimization of rank one subproblems has been previously shown to be very effective for standard NMFs [ 14 , 7 ] and is also reminiscent of the K SVD approach for dictionary learning [ 11 ] .
Optimization over hi : Firstly note that since the regularization term is independent of hi , it does not contribute to this optimization problem . Holding all variables except hi fixed and omitting additive constants independent of hi , ( 8 ) can be reduced to argminhi∈C flflR − wih flfl2 where i f ro
R = X − j=i wjh j
( 9 ) is the residual matrix independent of hi . Note that R is the difference of a sparse matrix and rank one matrices . While it can possibly be a dense matrix , we never need to evaluate it explicitly . In particular , our algorithm only needs to compute matrix vector products against R , namely
Rhi = Xhi − j hi ) wj(h wi − j=i j=i and hj(w j wi )
R wi = X
We use sparse matrix computations to evaluate Xhi and Xwi thus allowing us to efficiently compute the updates without evaluating R explicitly .
12345678910040608112141618TimeHodrick−Prescott Trend Filtering originalλ=100λ=10λ=01λ=001 Algorithm 1 : Online Learning Algorithm for Topic Evolution and Emergence .
Input : New data X(t ) ∈ RN ( t)×D , Old data X(t − ω + 1 , t − 1 ) , Previous topic matrix H(t − 1 ) of size K(t − 1 ) × D , Emerging Topic Bandwidth B , Hyperparameters : Evolution δ , Emergence µ , Hinge ν . Output : W(t ) ∈ RN ( t)×K(t ) , H(t ) ∈ RK(t)×D = 10−6 Set K(t ) = K(t − 1 ) + B Define X = X(t − ω + 1 , t ) ∈ RN×D Initialize W = Winit , H = Hinit where Winit ∈ RN×K(t ) , Hinit ∈ RK(t)×D ( details in Section 5 ) . while not(converge ) do for i = 1 , . . . K(t ) do if i ≤ K(t − 1 ) then
Rhi = Xhi − j=i wj(h 0 , Evolving wi : wij = max j hi )
.
1hi2 ( Rhi)j j wi)2 ( see Section 5 ) .
µ else flflw − Rhi/hi2flfl2
T−1 j=1 cj max(0 , νj − q
Emerging wi ( use Projected Gradient ) : + wi = argminw≥0 hi2 end if end for for i = 1 , . . . K(t ) do
Rwi = Xwi − if i ≤ K(t − 1 ) then j wi ) j=i hj(w lij = max ( 0 , hij(t − 1 ) − δ ) , uij = min ( hij(t − 1 ) + δ , 1 ) . Evolving hi ( Simplex projection with box constraints ) : hi = argminh∈C1 where C1 = {h : h ∈ ∆D , flflh − Rwi/wi2flfl2 flflh − Rwi/wi2flfl2
Emerging hi ( use Simplex Projection ) : hi = argminh∈∆D else lij ≤ hj ≤ uij} . end if end for Convergence Check : Relative change in Objective value < end while H(t ) = H and W(t ) is last N ( t ) rows of W .
Simple linear algebraic operations yield that the above is equivalent to argmin hi∈C flflflhi − R wi/wi2flflfl2
( 10 )
Note that the domain of dependence C changes according to different constraints on hi depending on whether it is an emerging or evolving topic .
Evolving hi : For an evolving topic , the optimization needs to be performed under the smoothness and the normalization constraints ( (4 ) and ( 3 ) respectively ) . Thus the optimum h(cid:63 ) i is obtained by optimizing the above objective over the set C = {hi : hi ∈ ∆D , lj ≤ hij ≤ uj} for appropriate lower and upper bounds lj and uj determined by ( 4 ) . This is equivalent to a projection onto a simplex with box constraints . Adapting a method due to [ 22 ] , we can find the minimizer in O(D ) time ie linear in the number of coordinates . Emerging hi : For an emerging topic , we do not have the smoothness constraints and the domain C = {hi : hi ∈ ∆D} is just the D dimensional simplex . The optimization ( 10 ) becomes equivalent to a projection onto the simplex ∆D . The same algorithm [ 22 ] again gives us the minimizer in linear time O(D ) . Evolving wi : When wi ∈ Wev , the regularization term in ( 8 ) does not contribute and the corresponding optimization problem boils down to w(cid:63 ) . Similar to ( 10 ) , simple algebraic operations yield that the above minimization is equal to the following simple projection problem , i = argminwi≥0 flfl2 i
. argmin wi≥0 flflR − wih flflwi − Rhi/hi2flfl2 flflflR − wih flflwi − Rhi/hi2flfl2
1hi2 ( Rhi)j argmin wi≥0 flflfl2 i
.
+ µL(Swi )
The projection set now is just the non negative orthant , for which there is a closed form minimizer : wij = max
Emerging wi : When wi ∈ Wem , the second term in ( 8 ) is active and the corresponding optimization problem looks like
0 ,
Omitting the terms independent of wi , simple algebra yields that the above is equivalent to argmin wi≥0
+ µL(Swi)/hi2
( 11 )
Noting that we choose L to be the 2 hinge loss , ( 11 ) can be rewritten as flflwi − Rhi/hi2flfl2
T−1
µ hi2 cj max(0 , νj − q j wi)2
+ argmin wi≥0 j = ( DF S)j, : , the jth row of DF S where the opwhere q erators are defined in ( 7 ) and ( 8 ) respectively . Assimilating the µ/hi2 into the constant ci , the above optimization problem can be written in the following generic form j=1
J(w ) where min w≥0 i
J(w ) = max ( 0 , ci(νi − w , xi))2 + w − w02 ( 12 )
λ 2 where w0 refers to the term Rhi/hi2 . Note that this is the same as the L2 SVM optimization problem with additional non negativity constraints on wi and the regularizer measuring distance from the vector w0 instead of the origin . This objective is minimized using a projected gradient algorithm ( due to the non negativity constraint ) [ 19 ] on the primal objective directly , as it is smooth and therefore the gradient is well defined . The update is given by ( w(k ) − ηk∇J(w(k) ) ) where is the projection operator(s ) = max(s , 0 ) and w(k+1 ) =
νi −
∇J(w(k ) ) = −2 max ci w(k ) , xi
, 0 xi i
+λ(w(k ) − w0 )
∇J(w(k) ) , w(k+1 ) − w(k )
The main trick lies in choosing the best rate ηk at the kth step . Following [ 19 ] , we start with η0 = 1 and at every step hot start ηk = ηk−1 . If ηk satisfies J(w(k+1 ) ) − J(w(k ) ) ≤ σ ( 13 ) we continuously increase ηk ← ηk/β as long it satisfies ( 13 ) . If ηk initially does not satisfy ( 13 ) , we continuously decrease ηk ← ηkβ until the condition is satisfied . For our experiments , we choose β = 0.05 and σ = 001
Our online learning algorithm can now be composed as in the table labeled Algorithm 1 .
Initializations : Note that in the algorithm above , Hinit = where Hev , the evolving set is initialized from
H(t − 1 ) and Hem is initialized to random distributions .
W11 W12
W21 W22 are
The corresponding parts of Winit =
Hev
Hem initialized in the following way : W11 is extracted from previous runs ( previous association of old documents with existing topics ) , W12 = 0 ( old documents have weak association with emerging topics ) , W21 optimizes
X(t ) − ˆWHev2 f ro argmin ˆW≥0
In other words , existing topics are allowed to first reconstruct new documents . Finally , W22 is chosen to be the columns of W corresponding to emerging topics where W is chosen randomly and scaled appropriately with
α = argmin
β>0
X(t ) − βWHinit2 f ro
( see [ 14 ] for simple expressions for α ) .
Convergence : Using a general result on convergence of Block Coordinate Descent , from [ 3 ] , we can show that the limit point ( W∗ , H(t ) ) generated by algorithm 1 is a stationary point of the objective function ( 1 ) . This follows from the uniqueness of the projection onto a closed convex set ( simplex , or simplex with box constraints ) and the strict convexity of Eq 12 .
6 . PERFORMANCE EVALUATION
We conducted a comprehensive empirical evaluation of our system on both traditional topic detection and tracking datasets comprising of streaming news stories , as well as a twitter stream filtered for tweets relevant to IBM that was collected over a span of 6 weeks .
It should be noted that the goal of our experiments is to empirically understand the effectiveness of the temporal regularizers . As a result , most of our experiments try to demonstrate the role of these temporal regularizers on traditional news datasets as well as twitter streams as opposed to exhaustive comparison with existing TDT algorithms . However we still perform comparisons with a simple baseline TDT model to put our models into perspective . We begin with a discussion on appropriate evaluation metrics , some of which are new to the best of our knowledge . 6.1 Metrics and Methodology
For performance evaluation , we assume that documents in the corpus have been manually identified with a set of E events . For simplicity , we assume that each document i is tagged with a single , most dominant event that it associates with πtrue(i ) ∈ {1 . . . E} . In the description below , we call these human labeled topics as events or “ true topics ” and assume them to be the ground truth .
Microaveraged F1 : This measure has been commonly reported in topic detection and tracking ( TDT ) literature ( see , eg , [ 6] ) . Let us assume that the system generates S topics where in general S = E ie , the system is allowed to generate any number of topics . We first construct the E × S confusion matrix CM between events and system topics , ie , CMe,s is the number of documents that were tagged e by the human and tagged s by the system . From this matrix , for each event e , we identify topk(e ) – the set of top k most frequently co occuring system topics . We can then compute the microaveraged F1 measure as follows :
E E e=1 |Dtrue(e ) ∩ Dsystem(e)| E e=1 |Dsystem(e)| E e=1 |Dtrue(e ) ∩ Dsystem(e)| e=1 |Dtrue(e)|
P recision ∗ Recall P recision + Recall
( 14 )
( 15 )
( 16 )
P recisionk =
Recallk =
F 1k = where Dtrue(e ) is the set of documents human tagged e and Dsystem(e ) is the set of documents system tagged with a topic in topk(e ) , |·| denotes the cardinality and ∩ denotes intersection operators on sets . Note that while k in topk is typically chosen to be 1 , higher values may also be meaningful to study particularly when multiple system topics attempt to capture the same semantic event . Also note that the F1 score also implicitly measures topic continuity which is important in providing a stable , consistent association across time between events and system topics .
Miss Rate @ First Detection : This measure attempts to intuitively capture the following notion : how much of an event has been “ missed ” before the system is able to communicate its existence to the user via the word distribution of a topic . It is a direct measure of the quality of an online topic detection model to serve as an early warning system for emerging themes . To compute this measure , we find the first detection timepoint : the earliest timepoint when the word distribution of a system generated topic gets sufficiently near the word distribution of the event e ( which is itself evolving temporally ) . We then compute the percentage of documents tagged with e that have streamed away prior to first detection . This fraction is called the miss rate @ first detection . To operationalize this measure , we define the following : ( a ) Word distribution of an event e at time t : the normalized centroid of all documents on event e upto time t . Note that this distribution is also obtained from the solution to the optimization problem
Htrue(t ) = argmin
H≥0,H1D =1E
X(1 : t ) − WtrueH2 f ro where Wtrue is simply the matrix encoding for πtrue , ie , Wtrue(i , e ) = 1 if πtrue(i ) = e and 0 otherwise . Htrue(t ) serves as a proxy for the word distributions of the true topics . ( b ) Similarity measure for Topic nearness : Given two discrete distributions p and q , we use the Jensen Shannon Divergence ( JSD ) as a measure of topic proximity ,
JSD(p , q ) =
( KL(p||m ) + KL(q||m ) )
1 2
Figure 3 : Metrics for TDT2 ( note that F1 score for independent models ( not plotted ) is 021 ) where m = 1 2 ( p+q ) . Other measures ( KL divergence , Symmetric KL divergence , Overlap in top words etc ) are also possible . However , we chose JSD because it is always numerically well defined and bounded between 0 and 1 . ( c ) First detection Time for an event e is defined as : tdetect(e ) = argmin t t : min s
JSD ( htrue(e ; t ) , hsys(s ; t ) ) < θ where θ is a detection threshold and htrue(e ; t ) is the word distribution for event e at time t ie the row corresponding to event e in Htrue(t ) , and hsys(s ; t ) is a row of H(t ) , ie , a system topic s at time t . Hence , tdetect simply measures the first timepoint at which a system topic comes θ close to an event as measured by JSD . In our experiments , we either set θ to a small value ( 0.2 ) or study variation with respect to its choice . The miss rate for event e can now be defined as :
|d : d ∈ Dtrue(e ) , timestamp(d ) < tdetect(e)| ∗ 100
( 17 )
|Dtrue(e)|
We study miss rates for individual events or report an average across all events .
Topic Continuity : We adapt a direct measure of topic continuity suggested in [ 6 ] . This measure essentially reports the JSD between word distributions in consecutive timepoints both for events ( true topics ) as well as for systemtopics . It therefore measures temporal smoothness in topic distributions .
Emerging Bandwidth : The topics in the emerging set can a ) either all become part of the evolving set going forward in time , b ) can be manually selected with some of them being discarded as noise by the user or c ) can be selected using criteria such as net current strength ( sum of the document weights of a topic from the corresponding column of W ) . In our experiments , we choose ( a ) and retain all topics in the emerging set . 6.2 Threshold Based TDT model
We adopt a simple baseline model ( Threshold Based TDT ) based on a body of work in topic detection [ 1 , 8 ] which corresponds to one of the classic topic detection frameworks . We adapt these body of algorithms for comparison to our experimental setup by using analogous concepts like adhoc period and emerging topics . In particular , we learn an initial set of topics by k means clustering of the documents over an adhoc period . New documents then stream in one time unit at a time and if their similarity to the existing topics exceed a certain threshold ( Yes/No threshold ) , they are allocated to those topics . We also incorporate adaptation [ 8 ] so that if the similarity of an incoming document to a topic exceeds a certain adaptation threshold τ , we modify the corresponding cluster ( topic ) center to incorporate the effect of the new document . The threshold values are treated as tunable parameters . The remaining documents which are not similar to the existing topics are then considered as belonging to emerging topics . We perform another k means clustering to cluster these documents into a set of emerging topics . The documents which are very far from these cluster centers are assigned to singleton topics . Similar to our temporal model , the emerging topics are absorbed into the evolving set going forward in time . 6.3 TDT2 Dataset
Our first dataset is drawn from the NIST Topic Detection and Tracking ( TDT2 ) corpus2 which consists of news stories in the first half of 1998 . In our evaluation , we used a set of 9394 documents represented over 19528 terms and distributed into the top 30 TDT2 human labeled topics over a period of 27 weeks . We choose the hinge parameter to be ν = 20 and emerging bandwidth of 2 per week for this dataset . In our experiments , we use a sliding window of ω = 4 weeks . The left panel of Figure 3 shows tracking performance ( F 1 ) as a function of the evolution parameter δ for various values of µ . When δ = 0 , the system freezes a topic as soon as it is detected not allowing the word distributions to change as the underlying topic drifts over time . When δ = 1 , the system has complete freedom in retraining topic distributions causing no single channel to remain consistently associated with an underlying topic . It can be seen that both these extremes are suboptimal . Tracking is much more effective when topic distributions are allowed to evolve under sufficient constraints in response to the statistics of incoming data . Moreover , the presence of emergence regularizer µ > 0 tends to lead to higher F1 score also . In the middle panel of Figure 3 we turn to the miss rate at first detection . Higher values of µ typically reduces miss rates helping emerging topics to be detected early . As δ is increased , topics become less constrained and therefore provide additional bandwidth to drift towards emerging topics , therefore lowering the miss rate curves . However , this comes at the price of reduced tracking performance . Thus , for a fixed amount of available topic bandwidth ( which also corresponds to user information overload ) , there is a tradeoff between tracking and early detection that can be navigated
2http://wwwnistgov/speech/tests/tdt/tdt98/indexhtm with the choice of µ and δ . The rightmost panel of Figure 3 shows topic continuity . Choosing δ = 0 leads to no evolution of word distributions while δ = 1 can lead to abrupt changes than can cause user confusion . For other values of δ we see that the system allows drift that is of the same scale as the drift in the true word distributions . Figure 3 also plots the metrics for a simple online methodology where a fresh NMF is learnt at every timepoint over all the data seen so far , independent of previous runs . This model is labeled independent in the Figure . As can be seen , independent runs lead to high miss rate ( see middle panel ) since small emerging topics can be swamped by the goal of reconstructing the ever expanding set of historical documents . Due to complete temporal independence , no system topic can maintain a stable association with an event leading to low F1 score ( 0.20 , not plotted in the left panel ) and high temporal discontinuity ( see right panel ) . In Figure 4 , we see that , as expected , training online models across short sliding windows takes much lesser time overall than training fresh batch models at every timepoint .
Figure 4 : Training time on TDT2
We perform baseline experiments with the Threshold Based TDT model over the TDT2 dataset . Due to space constraints , we just report the tracking performance using F1 score ( left panel on Figure 5 ) as a function of the Yes/No threshold for various values of the Adaptation threshold , τ . The F1 scores obtained by our temporal model are clearly much better than the simple baseline .
The first 7 days are used to build an initial adhoc model after which the data is streamed into our models for online analysis . In order to facilitate quantitative benchmarking , we labeled approximately 23 % of this data in a semi automatic fashion . First , we learnt a batch NMF model over the entire dataset with 100 topics . From this model , we identified 10 salient events in this time period concerning IBM , listed in the table below together with an estimate of date of peak strength from the model .
Table 1 : 10 events comprising roughly 23 % of 198029 tweets between Dec 21 , 2010 and Feb 2 , 2011 .
Lotusphere 2011 Conference
ID Event 1 2 Watson Jeopardy Contest 3 4 5 6 7 8 9 10
Graphene Transistors IBM Patents IBM Virtual Desktop Offering IBM Cloud Computing Data Center IBM Centennial IBM Quarterly Earnings IBM Analytics Study IBM ARM Partnership
Size 5020 16388 874 2459 1889 2823 2558 6944 3413 3168
Peak Date Jan 31 , 2011 Jan 14 , 2011 Jan 24 , 2011 Jan 10 , 2011 Jan 24 , 2011 Jan 26 , 2011 Jan 22 , 2011 Jan 18 , 2011 Jan 14 , 2011 Jan 20 , 2011
Several topics in the batch model were found to cover these events and induced a partial clustering of tweets which was treated as ground truth . Note , the online model needs to make a real time judgement of emerging topics and online tracking of evolving ones , and does not have the benefit of retrospective hindsight that the batch model does . It is important to note that our online models are trained on the entire dataset , and only the evaluation is restricted to these labeled events . In other words , these events need to each be teased apart from the entire data collection as they emerge . These 10 events are quite diverse and cover the victory of the IBM Watson supercomputer in a practice Jeopardy! round in January , IBM ’s quarterly earnings statement , announcements such as creation of the largest cloud computing data center in Asia and a new virtual desktop offering , new partnerships , market studies , patent leadership , centennial celebrations and super fast Graphene transistors . In our empirical study , we fixed the hinge parameter ν = 20 , evolution parameter δ = 0.001 , sliding window of one week ( ω = 7 ) and emerging topic bandwidth to be 4 . regularization . Figure 6 shows that the miss rate at first detection
Figure 5 : Metrics for TDT Adapt : F1 score on TDT2 and Twitter
Figure 6 : Miss Rate at First Detection as a function of Detection Threshold on IBM Twitter data
6.4
IBM Twitter Archive
We used the Twitter Search API to collect all tweets mentioning “ IBM ” in the time period Dec 21 , 2010 to Feb 2 , 2011 . Our dataset comprises of 198029 tweets spread over 43 days . curve as a function of detection threshold ( θ in Equation 17 ) is significantly lower when the emergence regularizer is used .
Table 2 : Tracking Performance on Twitter
Figure 7 : Emerging Topics in IBM Tweets
Measure Precision
Recall
F1 top1 µ = 0 µ = 1000 92.10 36.72 52.51
91.50 30.73 46.00 top5 µ = 0 µ = 1000 71.03 65.05 67.91
82.09 62.01 70.65
In Figures 7 , 8 we see the effect of emergence regularization : on several events , the online model with µ = 1000 shows a much sharper dip ( see bottom panel for each event ) as compared to µ = 0 in terms of JSD with respect to the true word distribution , around the time the event emerges ( see top panel for each event ) . These results clearly show the effectiveness and potential value of emergence regularization . The training time is 3.8 minutes per day for µ = 0 and 4.5 minutes per day for µ = 1000 on a commodity desktop running MATLAB . In the Table 2 , we report F 1 scores at k = 1 and k = 5 respectively . We see that the models tend to have high precision and in particular , the presence of the emergence regularizer also significantly improves precision when upto 5 best matched system topics are associated with each event . Finally , in Figure 9 , we show a visualization of topic temporal rivers [ 24 ] populated by keyword clouds ( the words with highest mass ) associated with system topics generated by our model . We show two different time points each spanning 1 week that has some coverage for most of the events . We see that the system is able to communicate the semantic essence ( in terms of words ) as well as the temporal profile of each event very effectively .
We perform baseline experiments with Threshold Based TDT as described in section 6.2 for both k = 1 and k = 5 just like our model . For k = 1 , the maximum F1 score for the thresholds considered is 41.12 obtained for precision and recall values of 71.65 and 28.83 respectively . The corresponding numbers for k = 5 is F1 of 48.16 with precision 39.30 and recall 62.19 respectively . We plot the F1 score in the same way as for the TDT2 dataset for the k = 1 case(Figure 5 right panel ) . 7 . CONCLUSION AND FUTURE WORK
We have developed a new framework for modeling the evolution of topics and aiding the fast discovery of emerging themes in streaming social media content . We have shown the effectiveness and value of novel temporal regularizers in analyzing twitter streams . There are several avenues for future work including a detailed large scale empirical study of the interplay between the model parameters as well as the tradeoff between evolution and emergence , coming up with convex formulations to avoid local minima problems possibly using nuclear norm regularization[17 ] , and effective means to do model selection in the online setting . Other fascinating directions include incorporating topic volatility into the evolution constraints , and building sparser models using l0/l1 regularizers .
8 . REFERENCES [ 1 ] J . Allan , editor . Topic Detection and Tracking : Event based Information Organization . Kluwer Academic Publ , 2002 .
[ 2 ] L . AlSumait , D . Barbara , and C . Domeniconi . On line lda : Adaptive topic models for mining text streams . In ICDM , 2008 .
[ 3 ] D . Bertsekas . Non linear Programming . Athena
Scientific , 1999 .
[ 4 ] D . Blei and J . Lafferty . Dynamic topic models . In
ICML , 2006 .
[ 5 ] D . Blei and MJordan Latent dirichlet allocation .
JMLR , 3:993–1022 , 2003 .
[ 6 ] Tzu Chuan Chou and Meng Chang Chen . Using Incremental PLSI for Treshhold Resilient Online Event Analysis . IEEE transactions on Knowledge and Data Engineering , 2008 .
[ 7 ] A . Cichocki , R . Zdunek , A . H . Phan , and S . Amari .
Non negative and Tensor Factorizations : Applications to Exploratory Multiway Data Analysis and Blind Source Separation . Wiley , 2009 .
Figure 8 : Emerging Topics in IBM Tweets
Figure 9 : TIARA visualizations [ 24 ] : Topics are organized as stacked rivers with width proportional to strength . Top keywords for each topic are also show .
[ 8 ] Margaret Connell , Ao Feng , Giridhar Kumaran , Hema
Raghavan , Chirag Shah , and James Allan . UMass at TDT 2004 . 2004 .
[ 9 ] Aron Culotta . Towards detecting influenza epidemics by analyzing twitter messages , 2010 .
[ 10 ] C . Ding , T . Li , and W . Peng . On the equivalence between non negative matrix factorizations and probabilistic latent semantic analysis . Computational Statistics and Data Analysis , 2008 .
[ 11 ] M . Elad . Sparse and Redundant Representations : From Theory to Applications in Signal and Image Processing . Springer , 2010 .
[ 12 ] Mark Girolami and A . Kaban . On an equivalence between plsi and lda . SIGIR .
[ 13 ] A . Gohr , A . Hinneburg , R . Schult , and
M . Spiliopoulou . Topic evolution in a stream of documents . In SDM , 2009 .
[ 14 ] Ngoc Diep Ho , Paul Van Dooren , and Vincent D . Blondel . Descent methods for nonnegative matrix factorization . Numerical Linear Algebra in Signals , abs/0801.3199 , 2007 .
[ 15 ] Matthew D . Hoffman , David M . Blei , and Frances
Bach . Online learning for latent dirichlet allocation . In NIPS , 2010 .
[ 16 ] T . Hoffman . Probabilistic latent semantic analysis . In
UAI , 1999 .
[ 17 ] M . Jaggi and M . Sulovsk´y . A simple algorithm for nuclear norm regularized problems . In ICML , 2010 .
[ 18 ] D . Lee and HS Seung . Learning the parts of objects using non negative matrix factorizations . Nature , 1999 .
[ 19 ] CJ Lin . Projected gradient methods for non negative matrix factorization . In Neural Computation , 2007 . [ 20 ] J . Mairal , F . Bach , J . Ponce , and G . Sapiro . Online learning for matrix factorization and sparse coding . JMLR , 2010 .
[ 21 ] P . Melville , V . Sindhwani , and R . Lawrence . Social media analytics : Channeling the power of the blogosphere for marketing insight . Workshop on Information in Networks , 2009 .
[ 22 ] P . M . Pardalos and N . Kovoor . An algorithm for singly constrained class of quadratic programs subject to upper and lower bounds . Mathematical Programming , 46:321–328 , 1990 .
[ 23 ] G . Salton . Automatic Text Processing : The Transformation , Analysis , and Retrieval of Information by Computer . Addison Wesley , 1989 .
[ 24 ] Furu Wei Shimie Pan Michelle X . Zhou Weihong Qian Lei Shi Li Tan Qiang Zhang Shixia Liu , Yangqiu Song . Tiara : Visually analyzing topic evolution in large text collections . In KDD , 2010 .
[ 25 ] Wei Xu , Xin Liu , and Yihong Gong . Document clustering based on non negative matrix factorization . In SIGIR , 2003 .
[ 26 ] Yiming Yang , Tom Pierce , and James Carbonell . A
Study on Retrospective and Online Event Detection . In SIGIR , 1998 .
