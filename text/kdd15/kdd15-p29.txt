Estimating Local Intrinsic Dimensionality
Laurent Amsaleg Equipe LINKMEDIA ,
CNRS/IRISA Rennes , France
Campus Universitaire de
Beaulieu
35042 Rennes Cedex , France laurentamsaleg@irisafr
Stéphane Girard Equipe MISTIS , INRIA
Grenoble , France
Inovallée , 655 , Montbonnot 38334 Saint Ismier Cedex , stephanegirard@inriafr
France
Teddy Furon
Equipe LINKMEDIA ,
INRIA/IRISA Rennes , France
Campus Universitaire de
Beaulieu
35042 Rennes Cedex , France teddyfuron@inriafr
Ken ichi Kawarabayashi
National Institute of Informatics , Japan 2 1 2 Hitotsubashi ,
Chiyoda ku
Tokyo 101 8430 , Japan k_keniti@niiacjp
Oussama Chelly National Institute of Informatics , Japan 2 1 2 Hitotsubashi ,
Chiyoda ku
Tokyo 101 8430 , Japan chelly@niiacjp Michael E . Houle National Institute of Informatics , Japan 2 1 2 Hitotsubashi ,
Chiyoda ku
Tokyo 101 8430 , Japan meh@niiacjp Michael Nett Google , Japan
6 10 1 Roppongi , Minato ku
Tokyo 106 6126 , Japan mnett@google.com
ABSTRACT This paper is concerned with the estimation of a local measure of intrinsic dimensionality ( ID ) recently proposed by Houle . The local model can be regarded as an extension of Karger and Ruhl ’s expansion dimension to a statistical setting in which the distribution of distances to a query point is modeled in terms of a continuous random variable . This form of intrinsic dimensionality can be particularly useful in search , classification , outlier detection , and other contexts in machine learning , databases , and data mining , as it has been shown to be equivalent to a measure of the discriminative power of similarity functions . Several estimators of local ID are proposed and analyzed based on extreme value theory , using maximum likelihood estimation ( MLE ) , the method of moments ( MoM ) , probability weighted moments ( PWM ) , and regularly varying functions ( RV ) . An experimental evaluation is also provided , using both real and artificial data .
Categories and Subject Descriptors G.3 [ Mathematics of Computing ] : Probability and Statistics—Distribution Functions ; I26 [ Computing Methodologies ] : Artificial Intelligence—Learning , Parameter Learning
August 10 13 , 2015 ,
Sydney , Australia
,
Keywords intrinsic dimension , indiscriminability , manifold learning
1 .
INTRODUCTION
In an attempt to improve the discriminability of similarity measures , and the scalability of methods that depend on them , much attention has been given in the areas of machine learning , databases , and data mining to the development of dimensional reduction techniques . Linear techniques for dimensionality reduction include Principal Component Analysis ( PCA ) and its variants [ 4 , 24 ] . Non linear dimensionality reduction methods — also known as manifold learning techniques — include Isometric Mapping [ 36 ] , MultiDimensional Scaling [ 35,37 ] , Locally Linear Embedding and its variants [ 30 ] , and Non Linear Component Analysis [ 32 ] . Most reduction techniques require that a target dimension be provided by the user , although some attempt to determine the dimension automatically . Ideally , the supplied dimension should depend on the intrinsic dimensionality ( ID ) of the data . This has served to motivate the development of models of ID , as well as accurate estimators .
Over the past few decades , many practical models of the intrinsic dimensionality of data sets have been proposed . Examples include the previously mentioned Principal Component Analysis and its variants [ 4 , 24 ] , as well as several manifold learning techniques [ 26 , 30 , 32 , 37 ] . Topological approaches to ID estimate the basis dimension of the tangent space of the data manifold from local samples [ 5,38 ] . Fractal methods such as the Correlation Dimension ( CD ) estimate an intrinsic dimension from the space filling capacity of the data [ 6 , 14 ] . Graph based methods use the k nearest neighbors graph along with density in order to estimate ID [ 8 ] .
The aforementioned intrinsic dimensionality measures can be described as ‘global’ , in that they consider the dimension
29 ality of a given set as a whole , without any individual object being given a special role . In contrast , ‘local’ ID measures are defined in this paper as those that involve only the knearest neighbor distances of a specific location in the space . Several local intrinsic dimensionality models have been proposed recently , such as the expansion dimension ( ED ) [ 25 ] , the generalized expansion dimension ( GED ) [ 19 ] , the minimum neighbor distance ( MiND ) [ 31 ] , and local continuous intrinsic dimension ( which we will refer to here as LID ) [ 17 ] . These models quantify ID in terms of the rate at which the number of encountered objects grows as the considered range of distances expands from a reference location .
Local approaches can be very useful when data is composed of heterogeneous manifolds . In addition to applications in manifold learning , measures of local ID have been used in the context of similarity search , where they are used to assess the complexity of a search query [ 22 , 25 ] , or to control the early termination of search [ 20 , 21 ] . They have also found applications in outlier detection , in the analysis of a projection based heuristic [ 9 ] , and in the estimation of local density [ 39 ] . The efficiency and effectiveness of the algorithmic applications of intrinsic dimensional estimation ( such as [ 20 , 21 ] ) depends greatly on the quality of of the estimators employed .
Distances from a query point can be seen as realizations of a continuous positive random variable . In this case , the smallest distances encountered would be ‘extreme events’ associated with the lower tail of the underlying distance distribution . In Extreme Value Theory ( EVT ) , a discipline of statistics concerned with the study of tails of continuous probability distributions , the random variable associated with nearest neighbor distances can be assumed to follow a power law distribution [ 7 ] . Continuous lower bounded random variables are known to asymptotically converge to the Weibull distribution as the sample size grows , regardless of the original distance measure and its distribution . In an equivalent formulation of EVT due to Karamata , the cumulative distribution function of a tail distribution can be represented as a regularly varying ( RV ) function whose dominant factor is a polynomial in the distance [ 7,18 ] ; the degree ( or ’index’ ) of this polynomial factor determines the shape parameter of the associated Weibull distribution , or equivalently the exponent of the associated power law . The index has been interpreted as a form of intrinsic dimension [ 7 ] . Maximum likelihood estimation of the index leads to the well known Hill estimator for power law distributions [ 16 ] . While EVT provides an asymptotic description of tail distributions , in the case of continuous distance distributions , the distribution can be exactly characterized in terms of LID [ 18 ] . The LID model introduces a function that assesses the discriminative power of the distribution at any given distance value [ 17 , 18 ] . A distance measure is described as ‘discriminative’ when an expansion in the distance results in a relatively small increase in the number of observations . This function is shown to fully characterize the cumulative distribution function without the explicit involvement of the probability density [ 18 ] . The limit of this function yields the skewness of the Weibull distribution ( or equivalently , the Karamata representation index , or power law exponent ) associated with the lower tail . It is the estimation of this limit that is the main focus of this paper .
In addition to the more traditional applications stated earlier , LID has the potential for wide application in many ma chine learning and data mining contexts , as it makes no assumptions on the nature of the data distribution other than continuity .
The main original contributions of this paper are : • a framework for the estimation of local continuous intrinsic dimension ( LID ) using well established techniques : the maximum likelihood estimation ( MLE ) , the method of moments ( MoM ) , and the method of probability weighted moments ( PWM ) . In particular , we verify that applying MLE to LID leads to the wellknown Hill estimator [ 16 ] .
• a new family of estimators based on the extreme valuetheoretic notion of regularly varying functions . Several existing dimensionality models ( ED , GED , and MiND ) are shown to be special cases of this family .
• confidence intervals for the variance and convergence of the estimators we propose .
• an experimental study using artificial data and synthetic distance distributions , in which we compare our estimators with state of the art global and local estimators . We also show that the empirical variance and convergence rates of the MLE ( Hill ) and MoM estimators are superior to those of the other local estimators studied .
• experiments showing that local estimators are more robust than global ones in the presence of noise in nonlinear manifolds . Our experiments show that our approaches are very competitive in this regard with other methods , both local and global .
• profiles of several real world data sets in terms of LID , illustrating the degree of variability of complexity from region to region within a dataset . The profiles demonstrate that a single ‘global’ ID value is in general not sufficient to fully characterize the complexity of realworld data .
2 . CONTINUOUS INTRINSIC DIMENSION LID [ 17 ] aims to quantify the local ID of a feature space exclusively in terms of the distribution of inter point distances . Formally , let ( Rm , d ) be a domain equipped with a non negative distance function d . Let us consider the distribution of distances within the domain with respect to some fixed point of reference . We model this distribution in terms of a random variable X with support [ 0,∞ ) . X is said to have probability density fX , where fX is a non negative Lebesgue integrable function , if and only if
Pr[a ≤ X ≤ b ] = fX(x ) dx , b x=a x for any a , b ∈ [ 0,∞ ) such that a ≤ b . The corresponding cumulative density function FX is canonically defined as
FX(x ) = Pr[X ≤ x ] = fX(u ) du . u=0
Accordingly , whenever X is absolutely continuous at x , FX is differentiable at x and its first order derivative is fX(x ) .
Definition 1
( Houle [ 17] ) . Given an absolutely continuous random distance variable X , for any distance threshold x such that FX(x ) > 0 , the local continuous intrinsic
30 dimension of X at distance x is given by
IDX(x ) lim →0+ ln FX ( (1 + )x ) − ln FX(x ) ln(1 + ) wherever the limit exists .
With respect to the generalized expansion dimension [ 19 ] , a precursor of LID , the above definition of IDX(x ) is the outcome of a dimensional test of neighborhoods of radii x and ( 1+ )x in which the neighborhood cardinalities are replaced by the expected number of neighbors . LID also turns out to be equivalent to a formulation of the ( lack of ) discriminative power of a distance measure , as both formulations have the same closed form :
Theorem 1
( Houle [ 17] ) . Let X be an absolutely continuous random distance variable . If FX is both positive and differentiable at x , then
IDX(x ) = xfX(x ) FX(x )
.
3 . EXTREME VALUE THEORY
Extreme value theory is concerned with the modeling of what can be regarded as the extreme behavior of stochastic processes . Its best known theorem , attributed in parts to Fisher and Tippett [ 10 ] , and Gnedenko [ 13 ] , states that the maximum of N independent identically distributed random variables ( after proper renormalization ) converges in distribution to a generalized extreme value distribution as N goes to infinity . 3.1 Threshold excesses
Consider the following two definitions . Definition 2 . Let ξ ∈ R and σ > 0 . The family of generalized Pareto distributions is defined by its cumulative distribution function :
− 1
ξ
.
FX ( x ) = 1 −
1 +
ξx σ
Definition 3 . Let X be a random variable whose distribution FX has the upper endpoint x+ ∈ R ∪ {∞} . Given w < x+ , the conditional excess distribution FX,w of X is the distribution of X − w conditioned on the event X > w :
FX,w ( x ) =
FX(w + x ) − FX(w )
1 − FX(w )
.
We are now in a position to introduce a powerful theorem due to Balkema and de Haan [ 1 ] , and Pickands [ 28 ] , which can be regarded as the counterpart to the central limit theorem for extremal statistics .
Theorem 2
( Balkema de Haan [ 1 ] , Pickands [ 28] ) . Let ( Xi)i∈N be a sequence of independent random variables with identical distribution function FX satisfying the conditions of the Fisher Tippett Gnedenko Theorem . As w → x+ , FX,w ( x ) converges to a distribution in FGPD .
In the following we demonstrate a direct relation between local ID and extreme value theory , which arises as an implication of Theorem 2 . Note that any choice of distance threshold w corresponds to a neighborhood of radius w based at the reference point , or equivalently , to the tail of the distribution of distances on [ 0 , w ) . As discussed in [ 7 ] , Theorem 2 also applies to lower tails : one can reason about minima using the transformation Y = −X . The distribution of the excess Y − ( −w ) ( conditioned on Y > −w ) then tends to a distribution in FGPD , as w tends to the lower endpoint of FX located at zero . Accordingly , as w tends to zero , the distribution in the tail [ 0 , w ) can be restated as follows [ 7 ] . Lemma 1 . Let X be an absolutely continuous random distance variable with support [ 0,∞ ) and cumulative distribution function FX such that FX(x ) > 0 if x > 0 . Let c ∈ ( 0 , 1 ) be an arbitrary constant . Let w > 0 be a distance threshold , and consider x restricted to the range [ cw , w ) . As w tends to zero , the distribution of X restricted to the tail [ cw , w ) satisfies , for some fixed ξ < 0 :
− 1
ξ
( x/w ) FX,w ( x )
→ 1
Note that the distribution of excess distance w − X is bounded from above by w which , according to [ 7 ] , enforces that ξ < 0 .
To summarize , whenever Theorem 2 applies to a distance variable X , the cumulative distribution of distances within a radius w neighborhood is asymptotically determined by a single parameter ξ < 0 . We can prove the following statement concerning LID .
Theorem 3 . Let X be an absolutely continuous random distance variable with support [ 0,∞ ) , satisfying the conditions of Theorem 2 , and w > 0 be a distance threshold . Then , as w tends to zero ,
IDX(w ) → − 1 ξ
= : IDX .
Proof . Omitted due to space limitations .
Note that together Lemma 1 and Theorem 3 allow us to restate the asymptotic cumulative distribution of distances in the tail [ cw , w ) as
( x/w)IDX FX,w ( x )
→ 1 .
( 1 )
3.2 Regularly varying functions
The Fisher Tippett Gnedenko Theorem and the PickandsBalkema de Haan Theorem have been shown to be equivalent to a third characterization of the tail behavior , in terms of regularly varying ( RV ) functions . The asymptotic cumulative distribution of X in the tail [ 0 , w ) can be expressed as FX(x ) = xκX(1/x ) , where X is differentiable and slowly varying ; that is , for all c > 0 , X satisfies lim t→∞
X(ct ) X(t )
= 1 .
FX restricted to [ 0 , w ) is itself said to be regularly varying with index κ . In particular , a cumulative distribution F ∈ FGEV has ξ < 0 if and only if F is RV and has a finite endpoint . Note that the slowly varying component X(1/x ) of FX is not necessarily constant as x tends to zero . For a detailed account of RV functions , we refer the reader to [ 2 ] . The following corollary is a straightforward extension of the examples given in Section 2 .
Corollary 1 . Let X be a random distance variable restricted to [ 0 , w ) with distribution FX(x ) = xκX(1/x ) . As w tends to zero , the index κ converges to IDX .
31 4 . ESTIMATION
This section is concerned with practical methods for the estimation of the local intrinsic dimension of a random distance variable X . In particular , we adapt known GPD parameter estimators such as the maximum likelihood estimator ( in Section 4.1 ) and moment based estimators ( in Sections 4.2 and 4.3 ) , and propose a new estimator based on regularly varying functions ( in Section 44 )
For the remainder of this discussion we assume that we are given a sequence x1 , . . . , xn of observations of a random distance variable X with support [ 0 , w ) , in ascending order — that is , x1 ≤ x2 ≤ ··· ≤ xn . 4.1 Maximum Likelihood Estimation
Using the asymptotic expression of the distance distribution given in Equation 1 , we see that the log likelihood of IDX for the sample is L(IDX ) = n ln n Accordingly , the maximum likelihood estimate IDX is
+ n ln IDX + ( IDX − 1 )
FX,w ( w ) xi w i=1 ln w
.
IDX = −
1 n n i=1 ln xi w
−1
, which follows the form of the well known Hill estimator for the scaling exponent of a power law tail distribution [ 16 ] .
The MLE model ensures the usual regularity conditions that guarantee the consistency , the asymptotic normality and the efficiency of this estimator . The variance is asymptotically given by the inverse of the Fisher information defined as :
I = E
− ∂2L(IDX )
∂ ID2 X
= n ID2 X
, where E[· ] denotes the expectation . Therefore , if the number / n ) . Accordingly , with probability 1 − β , a sample of n dis of samples n is sufficiently large , we haveIDX ∼ N ( IDX , ID2 tances in [ 0 , w ) provides an estimate IDX lying within
X
IDX ± IDX√ n
−1
Φ
1 − β 2
In other words , the 1 − β confidence interval is
IDX
.
IDX
1 + n−1/2Φ−1(1 − β/2 )
,
1 − n−1/2Φ−1(1 − β/2 )
.
4.2 Method of Moments
For any choice of k ∈ N , the k th order non central mo ment µk of the random distance X is
µk = E
= xkfX(x ) dx = wk
Xk w x=0
IDX
IDX + k
.
,
µk n wk
Solving for the intrinsic dimension gives
IDX = −k
µk
µk − wk = g with g(x ) = k x by its empirical counterpart ˆµk = 1 n E[ˆµk ] = µk and E[ˆµ2
1−x . When estimating the order k moment i , we see that r ] = ( nµ2k + n(n − 1)µ2 k)n−2 , so that i=1 xk
Var[ˆµ2 k ] =
µ2k − µ2 k n
= w2kIDXk2 n(IDX + 2k)(IDX + k)2 . mk,l,m = E
IDX
Therefore , the distribution of ˆµk with wk is asymptotically normal
ˆµk wk ∼ N
;
IDXk2 n(IDX + 2k)(IDX + k)2
IDX + k
According to [ 29 , Th . 6a2.9 ] , if x ∼ N ( µ ; σ2n−1 ) asymptotically , then g(x ) ∼ N ( g(µ ) ; σ2n−1g(µ)2 ) , where g is the first order derivative of g . Therefore , asymptotically
IDX ∼ N
.
.
IDX ;
ID2 X n
1 +
( k/IDX)2
ID2
X(1 + 2k/IDX )
This variance is monotonically increasing in k/IDX , which indicates that we should use moments of small order k . When k/IDX tends to zero , the variance converges to ID2 X/n , the variance of the maximum likelihood estimator ( see Section 41 ) Note that an upper bound on IDX implies that the variance is bounded . In this case we can derive confidence intervals similar to Section 41 4.3 Probability Weighted Moments
General probability weighted moments are defined as
FX(x)k(1 − FX(x))lXm w
.
We restrict here our attention to a subfamily : for any choice of k ∈ N , νk is defined as FX(x)kX
FX(x)kxfX(x ) dx
νk E
=
=
IDX w
IDX k + IDX + 1 x=0
; solving for the intrinsic dimension yields
IDX =
νk w − νk(k + 1 )
= h
,
νk w x
1−(k+1)x . where h(x ) = 4.4 Estimation Using Regularly Varying Func tions
In this section we introduce an ad hoc estimator for the intrinsic dimensionality based on the characterization of distribution tails as regularly varying functions ( as discussed in Section 3 ) . Consider the empirical distribution function ˆFX , defined as
ˆFX(x ) = where ( cid:74)ϕ(cid:75 ) refers to the Iverson bracket which evaluates to
1 if ϕ is true , and 0 otherwise . We propose the following estimator for the index κ of FX . n j=1(cid:74)xj < x(cid:75 ) ,
1 n
Definition 4 . Let X be an absolutely continuous random distance variable restricted to [ 0 , w ) . The local intrinsic dimension IDX can be estimated as
ˆFX((1 + τjδn)xn)/ ˆFX(xn ) J j=1 αj ln(1 + τjδn )
,
IDX = ˆκ =
J j=1 αj ln under the assumption that xn , δn → 0 as n → ∞ , where ( αj)1≤j≤J and ( τj)1≤j≤J are sequences .
We will refer to this family of estimators as RV , for ‘regularly varying’ . Note that since RV estimators involve only the products τjδn for 1 ≤ j ≤ J , we may assume without loss of generality that τ1 + ··· + τJ = 1 . The estimators are based on the observation that , for all 1 ≤ j ≤ J ,
32 ln [ FX((1 + τjδn)xn)/FX(xn ) ] = κ ln(1 + τjδn ) + ln [ X((1 + τjδn)xn)/X(xn ) ] κ ln(1 + τjδn ) .
The RV family covers several of the known local estimators of intrinsic dimensionality . For the parameter choices J = 1 and = τ δn , the RV estimator reduces to the GED formulation proposed in [ 19 ] :
ˆFX((1 + )xn)/ ˆFX(xn ) ln
IDX = ln(1 + )
,
By setting = 1 , Karger & Ruhl ’s expansion dimension is obtained , while by setting xn as the distance to the knearest neighbor and such as ( 1 + )xn as the distance to the nearest neighbor , we find a special case of the MiND family ( MiNDml1 ) [ 31 ] . Alternatively , by setting J = n , αi = 1 for all i ∈ [ 1n ] , , the RV and choosing the vector τ such that 1 + τiδn = xi xn estimator becomes n n
IDX = j=1 ln [ j/n ] j=1 ln [ xj/xn ] n
≈ ln
√ 2πn − n j=1 ln [ xj/xn ]
As n → ∞ , this converges to the MLE ( Hill ) estimator presented in Section 4.1 , with w = xn .
We now turn our attention to an analysis of the variation of RV estimators . First , we introduce an auxiliary function which drives the speed of convergence of the estimator proposed in Definition 4 . For x ∈ R let εX(x ) be defined as
εX(x ) x
X(x ) X(x )
.
In [ 11 , 12 ] , the auxiliary function is assumed to be regularly varying , and the estimation of the corresponding regular variation index is addressed . Within this article , so as to prove the following results , we limit ourselves to the assumption that εX is ultimately non increasing .
Theorem 4 . Let X be a random distance variable over [ 0 , w ) with distribution function FX(x ) = xκX(1/x ) , and let τmax max1≤j≤J τj . Furthermore , let δn , xn → 0 so that n FX(xn)δn → ∞ andnFX(xn)δnεX(1/[(1+τmaxδn)xn ] ) → ultimately non increasing , then nFX(xn)δn ·[IDX −IDX ]
0 as n approaches infinity . If the auxiliary function εX is converges to a centered Gaussian with variance
IDXVα,τ = IDX
αSα ( ατ )2 , where Sa,b = ( |τa| ∧ |τb|)(cid:74)τaτb > 0(cid:75 ) for ( a , b ) ∈ {1 , . . . , J}2 .
( A ∧ B denotes the minimum of A and B . )
Note that the requirement nFX(xn)δn → ∞ can be interpreted as a necessary and sufficient condition for the almost sure presence of at least one distance sample in the interval [ xn , ( 1 + τjδn)xn) ] . In addition , the condition nFX(xn)δnεX(1/[rn(1 + τmaxδn) ] ) → 0 enforces that the approximation bias εX(1/[(1 + δn)xn ] ) is negligible compared to the standard deviation of the esti mate , 1/nFX(xn)δn . We continue the analysis by propos ing choices of α that minimize the variance in Theorem 4 .
Lemma 2 . The weight vector α = ( α1 , . . . , αJ ) minimizing Vα,τ is proportional to α0 = S−1τ = ( 1 , 0 , . . . , 0 ) , and the associated optimal variance is given by V0(τ ) =
,τS−1τ −1
.
Proof . Omitted due to space limitations . For the case J = 1 , we see that τ = ( 1 ) and V0(1 ) = 1 . This indicates that the GED minimizes the variance of estimation . However , different choices can be made regarding the weight vector τ and regarding the criterion to use in order to optimize the choice of α . Minimizing variance is one choice explored in this paper , but other criteria can be used . In general , however , the following confidence interval holds for RV estimators :
Lemma 3 . Let β ∈ ( 0 , 1 ) , and assume that the assumptions of Theorem 4 hold with α = S−1τ . Let uβ = Φ−1((1 + β)/2 ) , where Φ is the cumulative distribution function of the standard Gaussian distribution . Then nδnV0(τ )IDX ˆFX(xn )
−1/2
IDX ± uβ are the boundaries of the asymptotic confidence interval of level β for IDX .
Proof . Lemma 3 is a direct consequence of the asymptotic distribution established in Theorem 4 and the convergence of ˆFX(xn ) to FX(xn ) as n → ∞ . 5 . EXPERIMENTAL FRAMEWORK 5.1 Methods
The methods used in this study include MLE , MoM , PWM , and RV . The RV estimators are evaluated for the choices J = 1 and J = 2 , as follows : ln n−lnn/2 ln xn−ln xn/2 , lnn/j−(p−1 ) lni/j ln xn/xj +(p−1 ) ln xi/xj if J = 1
, if J = 2 ,
IDRV = where p = ( xi − 2xj + xn)/(xn − xj ) , i = n/2 , and j = 3n/4 . Note that the estimator RV for J = 1 is a form of generalized expansion dimension ( GED ) [ 19 ] . For every dataset , we report the average of ID estimates across all the points in the dataset . All estimators in our study can be computed in time linear in the number of sample points .
Parameters threshold = 0.025 k = 100 , γ = 1 , M = 1 , N = 10 k = 100 , γ = 1 , M = 10 , N = 1
Method PCA kNNG1 kNNG2 MiNDml1 None MiNDmli k = 100
Table 1 : Parameter choices used in the experiments .
Our experimental framework includes several state of theart intrinsic dimensionality measures . The global estimators consist of a projection method ( PCA ) , fractal methods ( CD [ 6 ] , Hein [ 15 ] , Takens [ 34] ) , and graph based methods ( kNNG1 , kNNG2 [ 8] ) . The local distance based estimators are MiNDml1 and MiNDmli [ 31 ] . Table 1 summarizes the parameter choices for every method , except for the fractal methods , which do not involve any parameter .
The MiND variants makes more restrictive assumptions than our methods : they assume the data to be uniformly distributed on a hypersphere , with a locally isometric smooth
33 ( a ) ID = 2
( b ) ID = 8
( c ) ID = 32
( d ) ID = 128
Figure 1 : Comparison of the mean and standard deviation of LID estimates provided by MLE , MoM and RV ( for J = 1 and J = 2 ) on increasingly large samples drawn from artificially generated distance distributions . The results cover target dimensionality values of 2 , 8 , 32 , and 128 . The values are marked in the corresponding plots . map between the hypersphere and the representational space . MiND uses only the two extreme samples ( smallest and largest ) , and requires knowledge of the dimension of the space ( D ) . In contrast , our approach assumes only that the nearest neighbor distances are in the lower tail of the distance distribution , where EVT estimation can be performed . 5.2 Artificial Distance Distributions
In the following we propose a set of experiments concerning artificial data , and describe the method employed for the generation of test data .
First , consider a point P drawn uniformly at random from within the m dimensional unit sphere , for some choice of m ∈ N . According to the method of normal variates , we define P = Z1/mYY−1 , where Z is uniformly distributed on [ 0 , 1 ] , and Y is a random vector in Rm whose coefficients follow the standard normal distribution . The distance of P , with respect to our choice of reference point at location 0 ∈ Rm , is distributed as follows . Z1/mY
X =
Y = Z1/m .
Note that , by measuring LID purely based on distance values with respect to a reference point , the model does not require that the data have an underlying spatial representation . As such , non integer values of m ∈ R can be selected for the generation of distances , if desired . For choices of m ∈ {2 , 8 , 32 , 128} , we draw 100 independent sequences of sample distance values from the distribution described above , and record the estimates produced by each of our methods for sample sizes n between 10 and 104 . 5.3 Artificial Data
The data sets used in our experiments have been proposed in [ 31 ] . They consist of 15 manifolds of various stuctures and intrinsic dimensionalities ( d ) represented in spaces of different dimensions ( D ) . They are summarized in Table 2 . These datasets were generated in different sizes ( 103 , 104 , and 105 points ) in order to evaluate the effect of the num
Manifold d D Description
1 2 3
4 5 6 7 8 9
10a 10b 10c 11 12 13
10 3 4
4 2 6 2 12 20 10 17 24 2 20 1
11 Uniformly sampled sphere .
5 Affine space . 6 Concentrated figure confusable with a 3d one .
8 Non linear manifold . 3
2 d Helix
36 Non linear manifold .
3
Swiss Roll .
72 Non linear manifold . 20 Affine space . 11 Uniformly sampled hypercube . 18 Uniformly sampled hypercube . 25 Uniformly sampled hypercube . 3 M¨obius band 10 times twisted .
Isotropic multivariate Gaussian .
20 13 Curve .
Table 2 : Artificial datasets used in the experiments . ber of points on the quality of the different estimators . For each dataset and for each of the three sizes , we average the estimates over 20 instances .
In order to evaluate the robustness of the estimators , we also prepared versions of these datasets with noise added . For each attribute f , we added normally distributed noise with mean equal to zero and standard deviation σn = p · σf where σf is the standard deviation of the attribute itself , and p ∈ {0.01 , 0.04 , 0.16 , 064} For attributes with σf = 0 , the noise was generated with standard deviation σn = p· σ∗ f where σ∗ f is the minimum of the nonzero standard deviations over all attributes .
5.4 Real Data
Not only can a reliable estimation of ID greatly benefit the practical performance of many applications , it also serves as a characterization of high dimensional data sets and the potential problems associated with their use in practice . To this end , we investigate the distribution of LID estimates on
1.8 2 2.2 2.4 2.6 2.8 3 3.2 3.4 3.6101102103104Mean Estimated IDNumber of samplesIDMLEMoMRV ( J=1)RV ( J=2 ) 0 0.2 0.4 0.6 0.8 1 1.2101102103104Standard Deviation / IDNumber of samplesMLEMoMRV ( J=1)RV ( J=2 ) 7 8 9 10 11 12 13101102103104Mean Estimated IDNumber of samplesIDMLEMoMRV ( J=1)RV ( J=2 ) 0 0.2 0.4 0.6 0.8 1 1.2101102103104Standard Deviation / IDNumber of samplesMLEMoMRV ( J=1)RV ( J=2 ) 30 32 34 36 38 40 42 44 46 48101102103104Mean Estimated IDNumber of samplesIDMLEMoMRV ( J=1)RV ( J=2 ) 0 0.2 0.4 0.6 0.8 1 1.2101102103104Standard Deviation / IDNumber of samplesMLEMoMRV ( J=1)RV ( J=2 ) 120 130 140 150 160 170 180 190 200 210 220101102103104Mean Estimated IDNumber of samplesIDMLEMoMRV ( J=1)RV ( J=2 ) 0 0.2 0.4 0.6 0.8 1 1.2101102103104Standard Deviation / IDNumber of samplesMLEMoMRV ( J=1)RV ( J=2)34 Figure 2 : Plots of the distribution of LID values across 104 distinct query locations for each data set . The LID values were obtained using the MLE estimator on the size 1000 neighborhoods of the individual reference points .
Figure 3 : Histograms of LID values across 104 distinct query locations for each data set , obtained using the MLE estimator on the size 1000 neighborhoods of the individual reference points . the following data sets , each taken from a real world application scenario .
The ALOI ( Amsterdam Library of Object Images ) data set contains a total of 110250 color photos of 1000 different objects taken from varying viewpoints under various illumination conditions . Each image is described by a 641dimensional vector of color and texture features [ 3 ] .
The MNIST database [ 27 ] contains of 70000 recordings of handwritten digits . The images have been normalized and discretized to a 28 × 28 pixel grid . The gray scale values of the resulting 784 pixels are used to form the feature vectors . The ANN SIFT1B data set consists of 128 dimensional SIFT descriptors extracted from a collection of ∼ 109 images . This set has been created for the evaluation of nearestneighbor search strategies at very large scales [ 23 ] .
For each data set , we estimate LID with respect to 104 distinct reference points , based on the distribution of distances to their respective 103 nearest neighbors . For ANN SIFT1B we use a selection of 104 query points that is provided with the data . In the case of ALOI and MNIST , we computed distance samples with respect to 104 points selected uniformly at random .
6 . EXPERIMENTAL RESULTS 6.1 Artificial Distance Distributions
We begin our experimental study with an assessment — in terms of bias , variance , and convergence — of the ability of each estimator to identify the ID of a sample of distance values generated according to different choices of target ID . Note that for these trials , the distributional model asserted in Lemma 1 holds everywhere on the range [ 0 , w ) by construction ( with w = 1 ) .
Fig 1 shows the behavior of MLE , MoM , and RV ( for choices of J = 1 and J = 2 ) . The convergence to the target ID value observed in every case empirically confirms the consistency of these estimators . Likewise , PWM is consis tent however , one should beware of PWM ’s susceptibility to the effects of numerical instability .
We also note that the RV estimator with J = 1 ( GED ) — which asymptotically minimizes variance according to Lemma 2 — is not the choice that minimizes variance when the number of samples is limited . Faster initial convergence favors the choice of MLE and MoM for applications where the number of available query to neighbor distances is limited , or where time complexity is an issue . 6.2 Artificial Data
In Tables 3 and 4 , due to space limitations , we present only a representative selection of the experimental results , averaged over 20 runs each . It should be noted that as PCA and MiNDmli estimates are restricted to integer values , their bias is lower for examples having integer ground truth intrinsic dimension , especially when this dimensionality is small . Also , unlike the other estimators tested , MiND estimators also require that an upper bound on the ID be supplied ( set to D in these experiments ) . PCA requires a threshold parameter to be supplied , the value of which can greatly influence the estimation .
The experimental results indicate that local estimators tend to over estimate dimensionality in the case of non linear manifolds ( sets m3 , m4 , m5 , m6 , m7 , m8 , m11 and m13 ) and to under estimate it in the case of linear manifolds ( sets m1 , m2 , m9 , m10a , m10b , m10c and m12 ) . For highly non linear manifolds , such as the Swiss Roll ( m7 ) , global estimators have difficulty in identifying the intrinsic dimension . The experimental results with higher sampling rates confirm the reduction in bias that would be expected with smaller knearest neighbor distances , as the local manifold structure more closely approximates the tangent space .
To show the effects of noise on the estimators , we display in Tables 5 , 6 and 7 for each method the deviation of every estimate in the presence of noise as a proportion of the estimate obtained in the absence of noise . On the one hand , we note that global methods , k NNG in particular , are
0 5 10 15 20 25 30 350K2K4K6K8K10KEstimated IDQueryALOI 0 5 10 15 20 25 30 350K2K4K6K8K10KEstimated IDQueryMNIST 0 5 10 15 20 25 30 350K2K4K6K8K10KEstimated IDQueryANN_SIFT1BABDEFCG0K1K2K3K4K 0 5 10 15 20 25 30 35Abs . FrequencyEstimated IDALOI0K1K2K3K4K 0 5 10 15 20 25 30 35Abs . FrequencyEstimated IDMNIST0K1K2K3K4K 0 5 10 15 20 25 30 35Abs . FrequencyEstimated IDANN_SIFT1B35 Dataset m1 m2 m3 m7 m8 m9 m10a m10c m11 m12 d D IDMLE 8.07 10 3 2.67 3.56 4 2.49 2 12.29 12 12.39 20 7.39 10 24 14.05 2.49 2 20 12.48
11 5 6 3 72 20 11 25 3 20
Dataset m1 m2 m3 m7 m8 m9 m10a m10c m11 m12 d D IDMLE 9.04 10 2.88 3 3.86 4 2 1.96 13.72 12 14.47 20 8.20 10 16.66 24 1.99 2 20 15.46
11 5 6 3 72 20 11 25 3 20
Dataset m1 m2 m3 m7 m8 m9 m10a m10c m11 m12 d D IDMLE 10.07 10 2.43 3 30.83 4 8.67 2 44.17 12 20 21.77 21.46 10 7.98 24 32.16 2 20 22.83
11 5 6 3 72 20 11 25 3 20
IDMoM IDPWM IDGED 7.91 2.65 3.55 3.22 11.97 11.96 7.28 13.52 3.05 11.85
8.14 2.68 3.59 3.04 12.51 12.50 7.47 14.22 2.94 12.43
8.08 2.67 3.56 2.80 12.33 12.40 7.40 14.07 2.74 12.46
IDRVE MiNDml1 MiNDmli 8.95 3.00 4.00 2.00 13.00 13.50 8.00 15.35 2.00 14.00
7.79 2.60 3.49 3.12 11.79 11.79 7.16 13.32 2.97 11.67
9.50 2.94 3.88 2.00 13.49 15.03 8.50 17.69 2.01 16.79
CD Hein Takens 9.44 9.24 2.87 2.91 3.66 3.63 1.95 1.95 11.85 11.00 14.68 12.84 8.45 8.42 16.82 16.90 2.00 1.99 13.69 13.64
5.35 2.75 3.70 1.90 3.60 4.30 8.15 6.05 2.70 3.70 kNNG1 7.96 2.53 4.00 3.10 14.28 19.68 10.69 17.31 2.83 11.71 kNNG2 7.02 2.52 2.88 2.86 12.56 10.84 6.65 29.77 2.59 5.13
Table 3 : ID estimates for 1000 points .
IDMoM IDPWM IDGED 9.06 2.90 3.92 1.99 13.91 14.41 8.21 16.54 2.04 15.23
9.32 2.94 3.97 2.02 14.50 15.08 8.43 17.45 2.06 16.03
9.10 2.90 3.90 1.99 13.86 14.56 8.25 16.77 2.03 15.54
IDRVE MiNDml1 MiNDmli 9.00 3.00 4.00 2.00 14.00 15.00 8.00 17.00 2.00 16.00
8.92 2.85 3.85 1.95 13.69 14.18 8.08 16.28 2.00 15.00
9.61 2.96 3.92 1.99 12.91 15.95 8.86 18.50 1.99 17.74
CD Hein Takens 9.59 9.56 2.98 3.08 3.76 3.75 1.97 1.98 11.92 11.95 15.74 15.69 8.92 8.87 18.13 18.08 2.00 1.99 15.04 15.00
8.95 3.55 3.90 1.95 8.10 2.65 9.10 10.90 2.00 3.70 kNNG1 9.20 2.77 3.94 1.83 14.08 10.11 6.55 15.00 1.84 37.63 kNNG2 9.87 2.44 3.94 1.83 14.08 10.11 6.55 15.00 1.84 37.63
Table 4 : Dimensionality estimates for 10000 points .
IDMoM IDPWM IDGED 11.81 3.10 33.16 15.58 35.44 24.01 20.83 6.83 28.43 23.90
10.55 1.03 32.05 14.57 43.00 22.25 21.45 7.87 29.56 23.10
11.80 3.06 33.25 16.34 39.79 24.34 21.59 7.45 28.64 24.52
IDRVE MiNDml1 MiNDmli 2.78 11.88 0.00 3.51 25.00 33.25 0.00 15.90 60.71 35.65 23.98 17.00 25.00 20.92 11.76 6.88 0.00 28.50 23.93 19.69
1.56 36.49 23.47 34.17 115.49 9.97 22.12 14.76 47.74 16.52
CD 11.82 12.01 22.13 14.21 86.53 22.12 9.02 2.99 41.21 16.22
Hein Takens 12.10 23.15 22.34 9.60 85.99 22.62 8.07 3.75 40.50 16.27
38.55 18.31 41.03 10.26 25.93 167.92 64.29 74.31 10.00 13.51 kNNG1 62.17 16.97 35.79 44.81 93.68 157.17 338.17 177.73 195.65 84.45 kNNG2 64.74 32.79 35.79 44.81 93.68 157.17 338.17 177.73 195.65 84.45
Table 5 : Deviation of dimensionality estimates for 10000 manifold points with added noise ( p=001 )
Dataset m1 m2 m3 m7 m8 m9 m10a m10c m11 m12 d D IDMLE 10.18 10 3 2.43 30.57 4 8.67 2 44.24 12 21.77 20 21.46 10 24 8.04 32.16 2 20 22.83
11 5 6 3 72 20 11 25 3 20
IDMoM IDPWM IDGED 11.92 3.45 33.16 15.58 35.59 24.01 20.83 6.83 28.43 23.90
10.66 1.03 32.05 14.57 43.07 22.25 21.45 7.87 29.56 23.10
11.91 3.06 33.25 16.83 39.86 24.27 21.59 7.51 28.64 24.52
IDRVE MiNDml1 MiNDmli 2.78 12.00 3.51 0.00 25.00 33.25 0.00 15.90 60.71 35.72 17.33 23.91 25.00 20.79 6.94 11.76 0.00 28.50 23.93 19.37
1.87 37.16 23.47 34.17 116.42 10.22 21.78 14.49 46.73 16.18
CD 17.05 18.83 26.40 15.74 86.69 22.31 3.04 7.85 40.20 16.16
Hein Takens 12.20 22.82 22.07 11.62 86.16 22.74 7.96 3.53 39.00 16.33
63.69 9.86 42.31 7.69 46.30 132.08 48.35 59.17 37.50 33.78 kNNG1 341.09 7.94 31.47 38.25 9.52 15.73 25.65 18.80 255.43 174.25 kNNG2 324.72 4.51 31.47 38.25 9.52 15.73 25.65 18.80 255.43 174.25
Table 6 : Deviation of dimensionality estimates for 10000 manifold points with added noise ( p=004 )
Dataset m1 m2 m3 m7 m8 m9 m10a m10c m11 m12 d D IDMLE 10.18 10 2.43 3 30.83 4 2 8.67 44.17 12 21.77 20 21.46 10 8.04 24 31.66 2 20 22.83
11 5 6 3 72 20 11 25 3 20
IDMoM IDPWM IDGED 11.81 3.10 33.42 15.58 35.44 24.01 20.83 6.89 28.43 23.90
10.66 1.03 32.05 14.57 43.00 22.25 21.45 7.93 29.06 23.17
11.80 3.06 33.25 16.34 39.79 24.27 21.59 7.51 28.64 24.52
IDRVE MiNDml1 MiNDmli 2.78 11.88 0.00 3.51 25.00 33.25 15.90 0.00 59.64 35.57 17.00 23.98 25.00 20.79 11.76 6.88 0.00 28.50 23.93 19.69
1.77 37.16 22.96 34.17 115.72 9.66 21.22 14.43 46.73 16.52
CD 16.95 19.48 31.20 19.29 85.94 22.12 9.02 2.71 27.64 16.16
Hein Takens 12.10 23.49 22.34 15.15 85.65 22.68 8.18 3.42 39.00 16.27
35.75 18.31 35.90 18.46 11.11 100.00 39.56 30.73 10.00 6.76 kNNG1 37.61 24.19 35.03 4.37 11.93 907.22 34.35 610.20 3811.41 835.80 kNNG2 41.84 13.93 35.03 4.37 11.93 907.22 34.35 610.20 3811.41 835.80
Table 7 : Deviation of dimensionality estimates for 10000 manifold points with added noise ( p=016 )
PCA 11.00 3.00 5.30 3.00 24.00 20.00 10.00 24.00 3.00 20.00
PCA 11.00 3.00 5.05 3.00 24.00 20.00 10.00 24.00 3.00 20.00
PCA 22.73 33.33 60.40 66.67 95.21 31.75 10.00 4.17 35.00 26.00
PCA 23.18 33.33 60.40 66.67 95.21 31.75 10.00 4.17 35.00 26.00
PCA 22.73 33.33 60.40 66.67 95.21 31.75 10.00 4.17 35.00 26.00
36 ( a ) Illustration of the distribution of k nearest neighbor distances for k ∈ [ 1 , 1000 ] with respect to 7 points of interest .
( b ) Distribution of LID estimates based on k nearest neighbor sets for k ∈ [ 10 , 1000 ] with respect to 7 points of interest .
Figure 4 : Distribution of IDMLE estimates and distance values across neighborhoods around the points of interest . significantly affected by noise : their estimates diverge very quickly as noise is being introduced . On the other hand , the local estimators display more resistance to noise in the case of non linear manifolds ; among the local estimators , our EVT estimators tend to outperform the MiND variants .
We note that the additive noise considered in this experiment does not drastically impact the intrinsic dimensionality in the case of hypercubes . ( sets m10a , m10b and m10c ) . That explains why PCA appears resistant to noise for the sets m10a , m10b and m10c .
6.3 Real Data
Based on our experiments on synthetic data , we expect the performance of our proposed estimators to be largely in agreement with one another . Accordingly , for clarity of presentation , for the experimentation on real data , we show results only for the MLE estimator .
Fig 2 illustrates the distribution of LID estimates across reference points for all three data sets . The scatter plot for the ANN SIFT1B data set furthermore contains several points of interest annotated with their LID values , corresponding to objects of interest which we discuss later . First , we clearly observe differences in the location of the distribution of LID values among the three data sets ; for example , the mean value and standard deviation of the LID estimates for ALOI are considerably lower than those obtained for ANN SIFT1B . More specifically , we observe mean values of µALOI ≈ 2.2 , µMNIST ≈ 6.3 , and µANN SIFT1B ≈ 12.3 , with the corresponding standard deviations of σALOI ≈ 1.9 , σMNIST ≈ 2.7 , and σANN SIFT1B ≈ 30 It should be noted that the measured ID within the neighborhoods that were tested is far smaller than the dimension of the full feature spaces . By plotting the same data as histograms in Fig 3 , we can furthermore see that the individual distributions of LID values differ in kurtosis and skewness as well .
The most striking difference between the individual points of interest are the distances to their respective k nearest neighbors . Fig 4a displays for each point of interest the specific distribution of neighbor distances for all values of k between 1 and 1000 . Interestingly , the ID measured at the points of interest appears to be associated with other properties of the respective objects . For example , distribution of neighbor distances for objects with high corresponding dimensionality ( D , E and F ) indicate that these points are in some sense outliers . On the other hand , despite their distance distributions being quite dissimilar , the LID values measured at A , B , and C are nearly identical .
7 . CONCLUSION
Our experimental results on synthetic data show that the estimation of LID stabilizes for sample sizes on the order of 100 . However , for Theorem 2 to be applicable , one must set a sufficiently small threshold on the lower tail of the distribution , which may severely limit the number of data objects falling within the tail . Although there is a conflict between the accuracy of the estimator and the validity of the model , this conflict is resolved as the size of the dataset scales upward ; it is in precisely such situations where the applications of ID have the most impact .
Estimates of local ID constitute a measure of the complexity of data . Along with other indicators such as contrast [ 33 ] , LID could give researchers and practitioners more insight into the nature of their data , and therefore help them improve the efficiency and efficacy of their applications . As a tool for guiding learning processes , the proposed estimators could serve in many ways . Data collected during the retrieval processes could be automatically filtered out as noise , whenever they are associated with an unusually high ID value . In this way , the quality of query results may be enhanced as well .
The performance of content based retrieval systems is usually assessed in terms of the precision and recall of queries on a ground truth data set . However , in high dimensional settings it is often the case that some points are much less likely to appear in a query result than others . Unlike LID , conventional measures of complexity or performance do not account for this difficulty . LID has therefore the potential to aid in the design of fair benchmarks that truly reflect the power of retrieval systems , according to a sound , mathematicallygrounded procedure .
8 . ACKNOWLEDGMENTS
L . Amsaleg and T . Furon supported by French project Secular ANR 12 CORD 0014 . O . Chelly , M . E . Houle and K . Kawarabayashi supported by JST ERATO Kawarabayashi Project . M . E . Houle supported by JSPS Kakenhi Kiban ( A ) Research Grant 25240036 .
0 20000 40000 60000 80000 0 200 400 600 800 1000Neighbor DistanceNeighbor RankDistribution of Neighbor DistancesABCDEFG 0 5 10 15 20 25 30 35 200 400 600 800 1000Estimated IDNeighborhood SizeDistribution of Neighborhood IDABCDEFG37 9 . REFERENCES
[ 1 ] A . A . Balkema and L . de Haan . Residual Life Time at Great Age . The Annals of Probability , 2:792–804 , 1974 .
[ 2 ] N . Bingham , C . Goldie , and J . Teugels . Regular variation , volume 27 . Cambridge University Press , 1989 .
[ 3 ] N . Boujemaa , J . Fauqueur , M . Ferecatu , F . Fleuret ,
V . Gouet , B . LeSaux , and H . Sahbi . IKONA : Interactive Specific and Generic Image Retrieval . In MMCBIR , 2001 .
[ 4 ] C . Bouveyron , G . Celeux , and S . Girard . Intrinsic dimension estimation by maximum likelihood in isotropic probabilistic pca . Pattern Recogn . Lett . , 32 .
[ 5 ] J . Bruske and G . Sommer . Intrinsic dimensionality estimation with optimally topology preserving maps . PAMI , 20 .
[ 6 ] F . Camastra and A . Vinciarelli . Estimating the intrinsic dimension of data with a fractal based method . PAMI , 24 .
[ 7 ] S . Coles . An Introduction to Statistical Modeling of
Extreme Values . 2001 .
[ 8 ] J . Costa and A . Hero . Entropic graphs for manifold learning . In Asilomar Conf . on Signals , Sys . and Comput , pages 316–320 Vol.1 , 2003 .
[ 9 ] T . de Vries , S . Chawla , and M . E . Houle . Finding local anomalies in very high dimensional space . In ICDM , pages 128–137 , 2010 .
[ 10 ] R . A . Fisher and L . H . C . Tippett . Limiting Forms of the Frequency Distribution of the Largest or Smallest Member of a Sample . Math . Proc . Cambridge Phil . Soc . , 24:180–190 , 1928 .
[ 11 ] M . I . Fraga Alves , L . de Haan , and T . Lin . Estimation of the parameter controlling the speed of convergence in extreme value theory . Math . Methods of Stat . , 12 . [ 12 ] M . I . Fraga Alves , M . I . Gomes , and L . de Haan . A new class of semiparametric estimators of the second order parameter . Portugalia Mathematica , 60:193–213 , 2003 .
[ 13 ] B . V . Gnedenko . Sur la Distribution Limite du Terme
Maximum d’une S´erie Al´eatoire . Ann . Math . , 44:423–453 , 1943 .
[ 14 ] A . Gupta , R . Krauthgamer , and J . R . Lee . Bounded
Geometries , Fractals , and Low Distortion Embeddings . In FOCS , pages 534–543 , 2003 .
[ 15 ] M . Hein and J Y Audibert . Intrinsic dimensionality estimation of submanifolds in r d . In ICML , pages 289–296 , 2005 .
[ 16 ] B . M . Hill . A simple general approach to inference about the tail of a distribution . Ann . Stat . , 3(5):1163–1174 , 1975 .
[ 17 ] M . E . Houle . Dimensionality , Discriminability , Density & Distance Distributions . In ICDMW , pages 468–473 , 2013 .
[ 18 ] M . E . Houle . Inlierness , Outlierness , Hubness and
Discriminability : an Extreme Value Theoretic Foundation . Technical Report 2015 002E , NII , 2015 . [ 19 ] M . E . Houle , H . Kashima , and M . Nett . Generalized
Expansion Dimension . In ICDMW , pages 587–594 , 2012 .
[ 20 ] M . E . Houle , X . Ma , M . Nett , and V . Oria .
Dimensional Testing for Multi Step Similarity Search . In ICDM , pages 299–308 , 2012 .
[ 21 ] M . E . Houle , X . Ma , V . Oria , and J . Sun . Efficient algorithms for similarity search in axis aligned subspaces . In SISAP , pages 1–12 , 2014 .
[ 22 ] M . E . Houle and M . Nett . Rank based similarity search : Reducing the dimensional dependence . PAMI , 37(1):136–150 , 2015 .
[ 23 ] H . J´egou , R . Tavenard , M . Douze , and L . Amsaleg .
Searching in One Billion Vectors : Re rank with Source Coding . In ICASSP , pages 861–864 , 2011 .
[ 24 ] I . Jolliffe . Principal Component Analysis . 1986 . [ 25 ] D . R . Karger and M . Ruhl . Finding Nearest Neighbors in Growth Restricted Metrics . In STOC , pages 741–750 , 2002 .
[ 26 ] J . Karhunen and J . Joutsensalo . Representation and separation of signals using nonlinear PCA type learning . Neural Networks , 7(1):113–127 , 1994 .
[ 27 ] Y . LeCun , L . Bottou , Y . Bengio , and P . Haffner .
Gradient based Learning applied to Document Recognition . Proceedings of the IEEE , 86(11):2278–2324 , 1998 .
[ 28 ] J . Pickands , III . Statistical Inference Using Extreme
Order Statistics . Ann . Stat . , 3:119–131 , 1975 . [ 29 ] C . R . Rao . Linear statistical inference and its applications . 1973 .
[ 30 ] S . T . Roweis and L . K . Saul . Nonlinear Dimensionality
Reduction by Locally Linear Embedding . Science , 290(5500):2323–2326 , 2000 .
[ 31 ] A . Rozza , G . Lombardi , C . Ceruti , E . Casiraghi , and
P . Campadelli . Novel high intrinsic dimensionality estimators . Machine Learning Journal , 89(1 2):37–65 , 2012 .
[ 32 ] B . Sch¨olkopf , A . J . Smola , and K R M¨uller .
Nonlinear Component Analysis as a Kernel Eigenvalue Problem . Neural Computation , 10(5):1299–1319 , 1998 .
[ 33 ] U . Shaft and R . Ramakrishnan . Theory of nearest neighbors indexability . ACM Trans . Database Syst . , 31(3):814–838 , 2006 .
[ 34 ] F . Takens . On the numerical determination of the dimension of an attractor . 1985 .
[ 35 ] J . Tenenbaum , V . D . Silva , and J . Langford . A global geometric framework for non linear dimensionality reduction . Science , 290(5500):2319–2323 , 2000 .
[ 36 ] J . B . Tenenbaum , V . De Silva , and J . C . Langford . A global geometric framework for nonlinear dimensionality reduction . Science , 290(5500):2319–2323 , 2000 .
[ 37 ] J . Venna and S . Kaski . Local Multidimensional
Scaling . Neural Networks , 19(6–7):889–899 , 2006 .
[ 38 ] P . Verveer and R . Duin . An evaluation of intrinsic dimensionality estimators . PAMI , 17(1):81–86 , 1995 .
[ 39 ] J . von Br¨unken , M . E . Houle , and A . Zimek . Intrinsic
Dimensional Outlier Detection in High Dimensional Data . Technical Report 2015 003E , NII , 2015 .
38
