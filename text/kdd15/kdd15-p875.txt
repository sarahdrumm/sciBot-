Efficient PageRank Tracking in Evolving Networks
Naoto Ohsaka
The University of Tokyo , Japan JST , ERATO , Kawarabayashi
Large Graph Project ohsaka@issu tokyoacjp
Takanori Maehara
Shizuoka University , Japan JST , ERATO , Kawarabayashi
Large Graph Project maehara.takanori @shizuokaacjp
Ken ichi Kawarabayashi
National Institute of Informatics , Japan
JST , ERATO , Kawarabayashi
Large Graph Project k keniti@niiacjp
ABSTRACT Real world networks , such as the World Wide Web and online social networks , are very large and are evolving rapidly . Thus tracking personalized PageRank in such evolving networks is an important challenge in network analysis and graph mining .
In this paper , we propose an efficient online algorithm for tracking personalized PageRank in an evolving network . The proposed algorithm tracks personalized PageRank accurately ( ie , within a given accuracy > 0 ) . Moreover it can update the personalized PageRank scores in amortized O(1/ ) iterations for each graph modification . In addition , when m edges are randomly and sequentially inserted , the total number of iterations is expected to be O(log m/ ) . We evaluated our algorithm in real world networks .
In average case , for each edge insertion and deletion , our algorithm updated the personalized PageRank in 3µs in a web graph with 105M vertices and 3.7B edges , and 20ms in a social network with 42M vertices and 1.5B edges . By comparing existing state of the arts algorithms , our algorithm is 2–290 times faster with an equal accuracy .
Categories and Subject Descriptors H28 [ Database Applications ] : Data mining
1 .
INTRODUCTION
Recently , many real world networks , such as the World Wide Web ( WWW ) and online social networks have become increasingly very large and are evolving rapidly . The largest example is the WWW , which contains approximately 60T web pages , and approximately 600 thousand new pages are created every second ; see Table 1 for statistics about other evolving networks . Thus large graphs are rapidly evolving . In this sense , scalable algorithms that work only for static networks are insufficient for such evolving networks .
To analyze large evolving networks , we must use a scalable online algorithm that can track the real time evolution of the network . Developing such algorithms is one of the most important issues for all computer scientists . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee . Request permissions from Permissions@acmorg KDD’15 , August 10 13 , 2015 , Sydney , NSW , Australia . cfl 2015 ACM . ISBN 978 1 4503 3664 2/15/08 $1500 DOI : http://dxdoiorg/101145/27832582783297
Table 1 : Growth of real world large networks : Internet Live Stats1 , and Google Inside Search2 at November , 1st , 2014 .
Total Growth Name 60.0T +600K/sec Web pages Facebook users 1.30B +5/sec Google+ users 700M +19/sec Twitter users +3/sec
300M
One fundamental task for a large evolving network is the vertex importance tracking , which tracks the “ importance ” of vertices during network evolution . This task arises in many important applications .
• Consider a web search engine , such as Google , Bing , or Baidu . For a given search query , it reports a list of relevant web pages that is ordered in these importance [ 7 ] . Since the WWW is evolving , ie , many new pages are created and removed in each second , it must solve the vertex importance tracking problem on the WWW to reflect the recent status of the WWW .
• Consider web spammers who want to obtain a high ranking in web search results ; they usually create a link farm [ 16 ] . Importance based link farm detection methods have been proposed [ 5 , 17 ] and by exploiting temporal changes of the importance , link farms are effectively detected [ 10 , 11 , 12 , 28 ] . Therefore the vertex importance tracking can be adapted for this purpose . • Consider an online social network , such as Facebook , Google+ , or Twitter . People who have large importance scores are considered to be influencers ( eg , celebriIn particular , people whose importance ties ) [ 8 , 33 ] . scores are significantly increased from the previous time are considered “ new talents , ” who are attractive to many standard users in the online social network .
Here , we formulate the important vertex tracking problem mathematically . First , we model evolving networks as follows . At the initial time t = 0 , we have a simple , directed , unweighted graph ( we say a “ graph ” ) G(0 ) = ( V , E(0) ) , where V is the set of vertices and E(0 ) is the set of edges . For each time t = 1 , 2 , . . . , a graph G(t ) = ( V , E(t ) ) is obtained by inserting some edges to and removing some edges from G(t − 1 ) , ie , E(t ) = ( E(t − 1 ) \ ∆E−(t ) ) ∪ ∆E+(t ) , where ∆E+(t ) is the edges inserted at time t and ∆E−(t ) is the edges removed at time t . By this construction , we have
1http://wwwinternetlivestatscom/ 2http://wwwgooglecom/insidesearch/ howsearchworks/thestory/
875 a sequence of graphs {G(t ) : t = 0 , 1 , 2 , . . .} and we call this sequence an evolving network . 3
The importance of a vertex in a graph is typically measured by a personalized PageRank [ 19 ] ( or PageRank [ 26] ) , which are state of the art methods in web search engines ; see Section 2 for the definition of personalized PageRank . By using personalized PageRank , the vertex importance tracking problem can be stated as follows .
Problem 1 ( Tracking personalized PageRank in evolving networks ) We are given an initial graph G(0 ) , and for each time t = 1 , 2 , . . . , we are given information of edges ∆E+(t ) and ∆E−(t ) , which are inserted to or removed from G(t − 1 ) . The problem is to compute the personalized PageRank x(t ) of graph G(t ) for each time t = 1 , 2 , . . . sequentially .
The main challenge of this problem is as follows .
Suppose we have a solution x(t− 1 ) ; however , since a graph changes over time , we must update the previous solution x(t− 1 ) to update a new solution x(t ) for an updated graph . How can we perform this efficiently ?
This challenge seems quite practical , as pointed out above . In particular , a web search engine does have a solution for now , but it has to update a new solution for the next day or week . We are rather motivated by this issue . Indeed this problem has been studied by some researchers , see Section 3 for related work .
Here , we assume that we can access all data to date , including the information that which edges are modified . On the other hand , some researchers considered other settings . Bahmani et al . [ 3 ] considered that it is necessary to crawl the network to obtain knowledge about which part of the network is modified . They proposed a crawling strategy to reduce the estimation error of the personalized PageRank . Sarma et al . [ 27 ] considered that a graph is stored in a secondary storage . They proposed an algorithm that reads the edges as a stream , and works in a sublinear space . But our primal interest is different , as mentioned above , therefore we only focus on our setting . 1.1 Contributions
We propose an algorithm for tracking personalized PageRank in an evolving network , and make the following contributions .
• The proposed algorithm tracks personalized PageRank during network evolution with a specified accuracy ( ie , L∞ error ≤ ) . For each network modification , the algorithm updates personalized PageRank in amortized O(1/ ) iterations , which depends on only the number of inserted or removed edges rather than the total number of edges . Furthermore , if m edges are randomly and sequentially inserted , the total number of iterations is O(log m/ ) . Thus we can track all personalized PageRank changes with only an additional log m factor . These theoretical results outperform existing algorithms proposed by Chien et al . [ 9 ] and Bahmani et al . [ 2 ] both theoretically ( see Table 2 and Section 4.4 ) and experimentally ( see Section 52 ) Indeed it is 2–1,400 times
3We do not consider inserting or removing vertices . However the proposed algorithm can be adopted to these modifications .
Table 3 : List of symbols . symbol G(t ) E(t ) m(t ) i , j e = ( i , j ) di P ( t ) b α x(t ) description directed unweighted simple graph at t set of edges at t number of edges , m(t ) = |E(t)| vertices directed edge from i to j degree of vertex i , di = |{j ∈ V : ( i , j ) ∈ E}| transition matrix of G(t ) preference vector random jump probability , α = 0.85 personalized PageRank vector at t faster than the Chien et al . ’s algorithm and 2–290 times faster than Bahmani et al . ’s algorithm to achieve an equal accuracy . • We also compared the proposed algorithm with two naive power methods ; the power method computed from scratch and the power method with warm start ( ie , start from the previous solution)4 . The proposed algorithm outperforms these two naive power methods both theoretically ( see Table 2 and Section 4.4 ) and experimentally ( see Section 52 ) Indeed , these two methods require Ω(m ) time , which is much more timeconsuming . Through our experiments , the proposed method achieves almost the same accuracy as the power methods and runs several orders of magnitude faster than them . • The proposed algorithm scales to a network with 105M vertices and 3.7B edges , and for such a large network , a single edge insertion and/or deletion is performed in 3µs . Therefore , it can track an evolving network that is growing by 300K edges per second .
We experimentally observe that the average number of iterations is proportional to 1/m ( see Section 5.3 ) , and “ Many iterations case , ” which is more time consuming than the average , rarely and sporadically occurred .
The remainder of the paper is organized as follows .
To demonstrate how the proposed algorithm is useful , we tried to find emerging and hot pages for the real “ evolving ” Wikipedia hyperlink network . The proposed algorithm could detect the “ evolution ” of some pages ( see Section 55 ) In Section 2 , we describe personalized PageRank . In Section 3 , we review work related to tracking personalized PageRank in evolving networks . In Section 4 , we propose our main algorithm . In Section 5 , we present experimental results , and in Section 6 , we conclude the paper .
2 . PERSONALIZED PAGERANK
Before describing related work , we briefly describe person alized PageRank [ 19 ] , which is a main focus of this paper .
Let G = ( V , E ) be a simple directed unweighted graph , where V is the set of vertices , and E is the set of edges . Let n = |V | be the number of vertices and m = |E| be the number of edges . P = ( Pij ) is the transition matrix of G defined as follows .
Pij =
1/dj , 0 ,
( j , i ) ∈ E , otherwise .
( 1 )
4Therefore there is already an approximate solution , and from this , some iterations can find the final solution .
876 Table 2 : Comparison with existing methods . Here , m is the number of edges , d is the maximum degree , is the specified accuracy , and |S| is the parameter for aggregation/disaggregation method .
1 edge insertion/deletion m random edges insertion average O(m + d log m/ ) amortized O(d/ )
O(m2 log 1/ ) O(m2 log 1/ )
O(m|S|/ log(1/ ) ) average O(m + log m/ 2 ) O(m + 1/ 2 ) P{|x∗ space O(m ) O(m ) O(m ) O(m ) accuracy x∗ − x∞ ≤ x∗ − x∞ ≤ x∗ − x∞ ≤ depends on S i − xi| > } = O(1 )
Proposed Method
Power method from scratch
Power method with warm start Aggregation/Disaggregation [ 9 ]
Monte Carlo [ 2 ]
O(m log 1/ ) O(m log 1/ ) O(|S|/ log(1/ ) )
O(1/ 2 ) where di is the out degree of vertex i ∈ V , ie , di = |{j ∈ V : ( i , j ) ∈ E}| . Let b be a preference vector , which is a stochastic vector on V , ie , bi ≥ 0 for each i ∈ V and i∈V bi = 1 . The personalized PageRank ( PPR ) x is a solution of the following linear equation : x = αP x + ( 1 − α)b ,
( 2 ) where α ∈ ( 0 , 1 ) is a decay factor , which is typically set to α = 085 If bi = 1/n for all i ∈ V , the corresponding PPR x is referred to as the PageRank [ 26 ] . Note that solution x of Eq ( 2 ) uniquely exists for all P , b , and α ∈ ( 0 , 1 ) ( see [ 21 ] for more details ) .
Personalized PageRank has a “ random walk ” interpretation . Consider a random walk on G defined as follows : with probability α , it randomly follows its out going edge , and with probability 1 − α , it randomly jumps some vertex j with probability bj . Then the expected frequency of visiting vertex i by the random walker is equal to the PPR score xi of vertex i . This implies that if a vertex has a score , the vertex is considered to be important because it is frequently visited by the walkers .
The fundamental algorithm for computing PPR is power iteration [ 19 ] , which iterates Eq ( 2 ) until convergence . The convergence rate is O(αν ) , where ν is the number of iterations ; therefore the number of iterations is estimated as O(log(1/ ) ) for accuracy > 0 [ 24 ] .
Another fundamental algorithm is the Monte Carlo simulation [ 19 ] , which simulates the random walk , as described above . The complexity of this method is O(R ) where R is the number of Monte Carlo samples because the expected length of each random walk is a constant , 1/α [ 2 ] . The accuracy of this method can be evaluated using a probabilistic inequality , ie , the Hoeffding inequality [ 18 ] , which is expressed as follows :
P{|xi − x i | > } ≤ 2 exp(−cR 2 ) , ∗
( 3 ) where P denotes the probability , x∗ is the exact PPR , and c is a constant depending on α .
Personalized PageRank is an important measure in a network ; therefore , many other algorithms are proposed . To name a few , algorithms based on the Gauss–Seidel method [ 13 , 25 ] , the Gauss–Southwell method [ 6 , 32 ] , the Krylov subspace method [ 15 , 24 ] , power iteration with quadratic acceleration [ 20 ] , a combination of the Gauss–Southwell method and Monte Carlo simulation [ 23 ] , LU/QR factorization [ 14 ] have been proposed so far .
3 . RELATED WORK
Here , we review work related to tracking PPR in an evolving network . A simple way to track PPR in an evolving network is to compute the PPR of the current network by an existing static method for each change in the network . However , this method is very expensive when tracking rapidly evolving networks since it does not exploit “ evolving ” structures . For efficiency , we are interested in methods that require only “ sublinear ” time complexity ( in terms of m ) for each edge insertion and/or deletion . There are two methods that satisfy this requirement ; one is the approximate aggregation/disaggregation method [ 9 ] , and the other is the Monte Carlo method [ 2 ] . These methods are described in the following subsections . In addition , we compare these methods with the proposed method theoretically ( Section 4 ) and experimentally ( Section 5 ) .
Approximate aggregation/disaggregation method .
The aggregation/disaggregation method [ 29 ] focuses on the following observation .
When an edge ( i , j ) is inserted to and/or removed from a network at time t , only the PPR scores of the vertices “ around ” vertices i and j are updated .
Based on this observation , Chien et al . [ 9 ] proposed the approximate aggregation/disaggregation method . In that algorithm , when an edge is inserted or removed at time t , it takes a small subset S(t ) that contains the endpoints of the edge . Then , by “ contracting ” all other vertices V \ S(t ) to a single supernode s , a small network ¯G(t ) is obtained . The algorithm computes PPR ˜x(t ) on the small network ¯G(t ) and construct the approximate PPR in the original network G(t ) as follows : xi(t ) =
˜xi(t ) , xi(t − 1 ) , i ∈ S(t ) , i ∈ V \ S(t ) .
( 4 )
By definition , the algorithm only accesses the edges E(S(t ) ) that are adjacent to S(t ) . Therefore , it can update PPR scores in O(|E(S(t))|/ log(1/ ) ) time if power iteration is used to the small network .
The main disadvantage of this method is in accuracy . Initially , its accuracy depends on the choice of a subset S(t ) . Chien et al . [ 9 ] proposed a dissipation based method to take a subset S(t ) . Langville and Meyer [ 22 ] also discussed several methods for choosing S(t ) , but these methods do not seem to give us any theoretical guarantee . Furthermore , even if the approximation error at time t is small , approximation errors can accumulate over lengthy evolutions . Thus , for accurate PPR tracking using this method , it must compute the PPR scores periodically from scratch .
Langville and Meyer [ 22 ] discussed this accuracy issue and proposed an iterative aggregation method , which combines the approximate aggregation/disaggregation method and power iteration . Their method can be seen as an improved version of power iteration ; thus it converges to the exact PPR with reducing the number of power iterations significantly . However , it is still power iteration ; thus it requires O(m ) time for each modification , which is too expensive in an online setting .
877 Monte Carlo method .
As described in Section 2 , the Monte Carlo method is a fundamental approach for computing PPR . Bahmani et al . [ 2 ] proved that a simple idea improves tracking performance of the PPR computation greatly . Their algorithm was originally proposed to compute the ( non personalized ) PageRank ; however , it can be modified easily to compute PPR .
Their algorithm maintains R random walk segments explicitly . Then the PPR scores are approximated by taking the average of these segments with accuracy guaranteed by Eq ( 3 ) . When an edge is inserted or removed , their algorithm reconstructs all random walks related to this modification . More precisely , when an edge ( i , j ) is inserted to or removed from the current network , all random walk segments that contain vertex i are enumerated . Then it reconstructs these subsegments after the vertex i . In the worst case , their algorithm must reconstruct all segments for a modification ; thus it requires O(R ) time for a modification . This seems too expensive ; however , it is interesting that , when m edges are randomly and sequentially inserted , the total number M of the reconstructed segments is expected to E[M ] = O(R log m ) [ 2 ] .
The main disadvantage of this method is the trade off of the accuracy . If we want to compute a solution with accuracy > 0 , by ( 3 ) , R = Ω(1/ 2 ) samples are required . Furthermore , to accomplish an efficient update , it must store all R random walk segments explicitly ; therefore it requires O(R ) space . Thus it is even difficult to set ≤ 10−6 because Ω(1/ 2 ) space seems impractical . This means that it does not scale for large graphs if we need to compute accurate solutions with this method .
4 . TRACKING PERSONALIZED PAGERANK
Here , we propose an algorithm for tracking the PPR vector in an evolving network . The proposed method is an adaptation of the Gauss–Southwell method [ 30 , 31 ] , which was proposed to solve linear equations . Thus we first explain the Gauss–Southwell method for computing the PPR in a static network , which has been already adapted in [ 1 , 6 , 32 ] . We then propose our algorithm . We theoretically show that the proposed method can track the PPR vector very efficiently . Finally we compare our theoretical analysis with three existing methods ; namely the naive power methods , the approximate aggregation/disaggregation method and the Monte Carlo method . 4.1 Gauss–Southwell method for PPR
We first describe the Gauss–Southwell method for computing the PPR vector in a static network . This method is sometimes called the local algorithm or bookmark coloring algorithm [ 1 , 6 , 32 ] in the graph mining community .
The Gauss–Southwell method is an iterative algorithm that maintains the ν th solution x(ν ) and the corresponding residual as follows : r(ν ) = ( 1 − α)b − ( I − αP )x(ν ) .
( 5 ) The goal of the algorithm is to take r(ν ) → 0 because , in this case , the corresponding x(ν ) converges to the PPR . To find the ν th solution , the algorithm selects the largest component r(ν−1 ) in r(ν−1 ) . It then updates the solution and i for ν = 1 , 2 , . . . do
Algorithm 1 Gauss–Southwell algorithm for PPR . Here x(0 ) and r(0 ) are initial solution and the corresponding residual such that r(0 ) = ( 1 − α)b − ( I − αP )x(0 ) . 1 : procedure Gauss–Southwell(x(0 ) , r(0 ) ) 2 : 3 : 4 : 5 : 6 : 7 : 8 : 9 : end procedure end if Update x(ν ) and r(ν ) according to Eq ( 6 )
Pick the largest entry r(ν ) if |r(ν )
| < then i terminate end for i residual as x(ν ) = x(ν−1 ) + r(ν−1 ) r(ν ) = r(ν−1 ) − r(ν−1 ) i i ei , ei + αr(ν−1)P ei .
( 6 )
( 7 )
Since we have assumed that a graph is simple , the i th component of P ei is zero ; therefore the i th component of r(ν ) is zero . The Gauss–Southwell algorithm iterates this process until it terminates , as shown in Algorithm 1.5 | ≤ for all i ∈ V . Therefore the corresponding solution x(ν ) is an approximate PPR with approximation guarantee as follows :
When the algorithm terminates , we have |r(ν ) i x − x(ν)∞ ≤ ( I − αP )
−1∞r(ν)∞ =
1 − α
.
( 8 )
It is interesting that the algorithm terminates within a “ constant ” number ( depending on only and r(0 ) ) of iterations .
Proposition 2 ( well known ; cf . Lemma 2 in [ 1 ] ) For a given accuracy > 0 and an initial solution x(0 ) , the Gauss–Southwell algorithm terminates within r(0)/((1 − α ) ) iterations .
Proof . For the completeness , we give a proof here . We define the potential Φ(ν ) at the ν th iteration as follows . i∈V
Φ(ν ) = r(ν)1 =
|r(ν ) i
| .
( 9 )
The initial potential is Φ(0 ) = 1 − α , and for each iteration , it decreases by at least ( 1 − α ) : Φ(ν ) = Φ(ν−1 ) − ( 1 − α)r(ν−1 )
( 10 ) Since Φ(ν ) ≥ 0 , the algorithm terminates within r(0)1/(1− α ) iterations . In particular , if we start from x(0 ) = 0 , the algorithm terminates within 1/ iterations .
≤ Φ(ν−1 ) − ( 1 − α ) . i
The number of vertices accessed by the Gauss–Southwell method is at most the number of iterations of the algorithm ; thus we obtain the following corollary .
Corollary 3 . The number of vertices accessed by the
Gauss–Southwell method is O(1/ ) .
5For efficient implementation , as mentioned in [ 1 , 6 ] , we replace line 3 of Algorithm 1 with “ Pick some entry |r(ν ) | ≥ ” and use a queue data structure rather than a heap data structure ( the former requires O(1 ) but the latter requires O(log n ) for each iteration ) . An additional experiment showed that queue implementation and heap implementation require approximately equal number of iterations . i
878 1
3
2
5
4
6
Initial state . ( a ) residuals are All less than threshold .
1
3
2
5
4
6
( b ) Edge ( 2 , 5 ) is inserted . The residual on 2 exceeds ; thus our alg . starts .
1
3
2
5
4
6
( c ) Our alg . propagates the residual from 2 to its adjacent 3 and 5 .
1
3
2
5
4
6
( d ) Our alg . repeats the propagation until convergence .
Figure 1 : Example behavior of the proposed PPR tracking algorithm .
Thus we are only interested in the efficiency of the algorithm . Our main results are Propositions 5 and 6 .
Proposition 5 . The number of iterations for a modification is amortized O(1/ ) , ie , let T ( k ) be the total number of iterations for any k modifications . Then , limk→∞ T ( k)/k ≤ 2α/((1 − α ) ) .
Proposition 6 . Suppose that m edges are randomly and sequentially inserted into a network with n vertices and no edges . Then , the total number of iterations is expected to O(log m/ ) , ie , the expected number of iterations is 2α(1 + log m)/((1 − α ) ) .
In Proposition 5 , “ amortized ” cannot be omitted . In the worst case , when the residuals on all vertices are very close to threshold at the previous time , the number of iterations of the Gauss–Southwell method at this time requires O(n ) iterations due to a chain reaction . However , this proposition shows that , in an amortized sense , we can bound the number of iterations for each modification by a constant time .
Proposition 6 is similar to Theorem 4 in Bahmani et al . [ 2 ] for a random walk based algorithm . As seen in Section 4.1 , the Gauss–Southwell method can compute PPR in O(1/ ) time for a static network ; thus this proposition shows that only an additional log m factor is sufficient to track all PPR changes . Here , we prove Propositions 5 and 6 by potential analysis . Let Φ(t)(ν ) = r(t)(ν)1 be a potential at time t with ν iterations , and let Φ(t ) = Φ(t)(νend ) be the potential when the Gauss–Southwell method terminates at time t .
Algorithm 2 Proposed algorithm : 1 : procedure TrackingPPR 2 :
Let x(0 ) and r(0 ) be an initial solution and corre sponding residual .
3 : 4 :
5 : for t = 1 , 2 , . . . do
Compute x(t)(0 ) and r(t)(0 ) from x(t − 1 ) and r(t − 1 ) by using Eqs . ( 11 ) and ( 12 ) .
Apply Gauss–Southwell algorithm from x(t)(0 ) and r(t)(0 ) . end for
6 : 7 : end procedure 4.2 Tracking PPR in an evolving network
Here , we propose an algorithm for tracking PPR in an evolving network . The basic idea for efficient PPR tracking is derived from the following observation .
If modification of the network at time t is small , the current PPR x(t ) is close to the previous PPR x(t − 1 ) .
This implies that the PPR x(t − 1 ) at time t − 1 is a good initial solution for the PPR at time t . Suppose we have the ( approximate ) PPR x(t− 1 ) and the corresponding residual r(t−1 ) at t−1 . To compute the PPR x(t ) and the residual r(t ) at time t , we apply the Gauss– Southwell algorithm with initial solution from the previous PPR x(t − 1 ) and residual r(t − 1 ) . Since the network is modified at time t , we must compute the residual that corresponds to x(t)(0 ) as follows . x(t)(0 ) = x(t − 1 ) , r(t)(0 ) = ( 1 − α)b − ( I − αP ( t))x(t)(0 )
= r(t − 1 ) + α(P ( t ) − P ( t − 1))x(t − 1 ) .
( 12 ) We have assumed that P ( t ) and P ( t − 1 ) differ only a few rows ; thus we can compute Eqs . ( 11 ) and ( 12 ) efficiently . This is the proposed algorithm , summarized in Algorithm 2 . The typical behavior of the proposed algorithm is shown in Figure 1 ; when an edge is inserted ( or removed ) , the algorithm starts the “ propagation ” from the modified vertex until convergence . This shows the algorithm may access only “ local ” neighbors around the modified vertex . 4.3 Performance analysis
Here , we analyze the performance of the proposed algorithm . First , by construction , the algorithm has the following accuracy guarantee .
Proposition 4 . For each time t , the residual r(t ) satisfies r(t)∞ ≤ . Therefore x(t ) − x∗(t ) ≤ /(1 − α ) , where x∗(t ) is the exact PPR for G(t ) .
( 11 )
Proof of Proposition 5 . Suppose that some edges are inserted or removed at time t . Then , by Eq ( 12 ) , initial potential at time t is given by Φ(0)(t ) ≤ Φ(t − 1 ) + αP ( t)x(t − 1)1 + αP ( t − 1)x(t − 1)1
= Φ(t − 1 ) + 2α .
( 13 )
Thus the potential increases by at most 2α for each modification . Conversely , from Eq ( 10 ) , the potential decreases by at least ( 1− α ) for each iteration . Thus , if we have T ( k ) iterations during k modifications , we obtain the following :
0 ≤ Φ(t)(ν ) ≤ Φ(0 ) + 2αk − ( 1 − α ) T ( k ) .
( 14 )
Therefore , lim t→∞
T ( k ) k
≤
2α
( 1 − α )
.
( 15 )
It should be noted that Proposition 5 holds for any sequence of modifications . Thus , in an extreme case , if we
879 fully reconstruct the network from scratch for each time t , this proposition still holds . This implies that , in a realistic case , the estimation in Proposition 5 is too conservative .
Let us consider a realistic case . Suppose that an edge ( i , j ) is inserted at time t . Then the transition matrices P ( t ) and P ( t − 1 ) differ only in the i th column ; thus
( P ( t ) − P ( t − 1))x(t − 1 ) ≤ 2x(t − 1)i di(t )
.
( 16 )
This implies that , if an edge is inserted in a vertex with high degree and low PPR score , the increase of the potential is negligible . Here , we analyze a case in which edges arrive randomly using the estimation in Eq ( 16 ) . We use the following lemma , which was proved by Bahmani et al . [ 2 ] . ( To be self contained , we give a proof . )
Lemma 7 ( Lemma 3 in Bahmani et al . [ 2 ] ) Suppose that an edge arrives at time t in a random order over the edge set . Then x(t − 1)i di(t )
E[
] =
1 t
.
( 17 )
Proof . Since an edge is inserted randomly , the proba bility of selecting vertex i at time t is di(t)/t . Thus , x(t − 1)i di(t )
E[ i 1 t i
] =
= di(t ) x(t − 1)i t di(t ) x(t − 1)i =
1 t
.
( 18 ) m t=1
Proof of Proposition 6 . By Lemma 7 , we obtain the following :
E[(P ( t ) − P ( t − 1))x(t − 1 ) ] ≤ 2 t
.
( 19 )
Thus , for m random edge insertions , the potential is expected to increase at most by
≤ 2α(1 + log m )
2α t
( 20 )
Each iteration decreases the potential by at least ( 1 − α ) ; therefore , the expected number of iterations is at most 2α(1+ log m)/((1 − α ) ) . Note that Lemma 7 is itself useful . By its proof , this lemma holds when probability p(i ) affected by vertex i is proportional to d(i ) . Such a situation is considered in the Barab´asiAlbert model [ 4 ] , which is the most widely used model for evolving networks . This implies that , for a real large evolving network , the proposed algorithm can efficiently update the PPR , in terms of 1/|E(t)| ( ie , the number of iterations is actually proportional to 1/|E| ) . Therefore our algorithm is more suitable for a large network .
A similar analysis can be performed for edge deletion ; when a single edge is randomly removed , the potential difference is also estimated by Eq ( 16 ) . Thus the complexities for inserting and deleting a single edge are not so different . Therefore the proposed algorithm can handle both insertion and deletion queries efficiently .
Note that Bahmani et al . [ 2 ] verified this “ random insertion ” assumption holds in a real dataset . They observed that E[xi(t − 1)/di(t ) ] ≈ 0.81/t ( which is better than Lemma 7 ) is satisfied empirically in Twitter dataset . This implies that the proposed method will also work efficiently in a real world dataset . 4.4 Comparison with existing methods
Here , we theoretically compare the proposed method with one naive power method , and two existing methods described in Section 3 . Comparison with the power method .
The most naive method for tracking PPR in an evolving network is to apply the power method for each time stamp . If we use x(t)(0 ) = 0 to the initial guess of the power iteration for t th network , we call this method power method from scratch . To exploit the evolution of the network , adopting the PPR at the ( t − 1) th network to the initial guess of the power iteration for t th network is a natural idea . We call this method power method with warm start . Here , we compare these methods with our proposed method . Recall that the number of iterations required to converge in the power method is O(log r(0)/ ) , where r(0 ) is the residual for the initial guess x(0 ) . Since r(t)(0 ) = ( 1 − α)b for x(t)(0 ) = 0 , the power method from scratch requires O(m log(1−α)/ ) time . From ( 12 ) , the power iteration with warm start requires O(m log(αP ( t ) − P ( t − 1)/ ) ) time . Requiring Ω(m ) time is the main issue of the power iterations , even if only a few edges are modified . On the other hand , the Gauss–Southwell method only look at the vertices that are affected by the modification . Therefore , the proposed method , which adopts the Gauss–Southwell method , is more suitable for tracking an evolving network .
Comparison with approximate aggregation/disaggregation method .
The proposed method can be considered as an “ adaptive ” version of the approximate aggregation/disaggregation method [ 9 ] . When an edge is inserted or removed , the approximate aggregation/disaggregation method takes a subset S that contains vertices whose PPR scores are affected by the modification and computes the PPR vector in a small graph obtained by contracting the subset V \ S .
On the other hand , as seen in the typical behavior ( Figure 1 ) , the proposed method “ propagates ” the residual from a vertex that is affected by the modification . Thus , it only accesses local neighbors around the initial vertex , and the number of accessed vertices is bound by a constant ( cf . Corollary 3 ) . Thus the proposed algorithm may achieve the same accuracy for each modification more quickly , because so far we do not know how to choose the subset S in the approximate aggregation/disaggregation method so that the resulting PPR scores are still accurate .
A more important advantage of the proposed method over approximate aggregation/disaggregation methods is that the error of the proposed algorithm is theoretically O( ) for each time t , ie , errors do not accumulate as the evolution is tracked . However , for approximate aggregation/disaggregation methods , errors may accumulate ; consequently large errors may occur after many edge insertions and/or deletions . This implies that the proposed algorithm is more suitable for tracking evolution over a long period of time .
Comparison with Monte Carlo method .
The proposed method and the Monte Carlo method presented by Bahmani et al . [ 2 ] are based on completely dif
880 ferent strategies . However , the theoretical performance of both methods is very similar ( see Table 2 ) .
A significant difference between the proposed algorithm and these algorithms is the error dependency . As mentioned in Section 3 , to achieve an accuracy > 0 , a Monte Carlo method requires O(1/ 2 ) samples . In contrast , the proposed algorithm requires O(1/ ) iterations to achieve an accuracy > 0 . Therefore , to find a same accuracy solution , the proposed method is more efficient than their algorithm .
Moreover , their algorithm requires Ω(1/ 2 ) space to accomplish an efficient update . Therefore it is difficult to set ≤ 10−6 because Ω(1/ 2 ) space seems impractical . On the other hand , the space complexity of the proposed algorithm does not depend on the accuracy . Therefore the proposed algorithm is more suitable for large graphs in terms of an accurate computation which is important to track the small changes of the PPR . Such small changes are often important to detect initial behaviors of events .
5 . EXPERIMENTS
We conducted experiments with real world web graphs and social networks . The following is the overview of our experiments :
• Efficiency and scalability ( Section 5.2 ) : We evaluated the time and the number of iterations for a modification . We observe that the proposed algorithm is very efficient and requires very few iterations in average . We also observe that “ many iteration cases ” rarely occurred . • Accuracy ( Section 5.3 ) : We evaluated the accuracy of the proposed algorithm . We verify that the errors do not accumulate as the evolution is tracked . • Comparison with existing algorithms ( Section 5.4 ) : We compared the proposed algorithm with the power methods and the two existing algorithms by Chien et al . [ 9 ] and Bahmani et al . [ 2 ] , respectively . The proposed algorithm achieves almost the same accuracy as the power methods and runs several orders of magnitude faster than the power methods . Moreover , it is 2–290 times faster than the algorithms [ 2 , 9 ] . • Application : vertex importance tracking ( Section 5.5 ) : For demonstration of the algorithm , we perform the vertex importance tracking on the Wikipedia hyperlink network .
All experiments were conducted on an Intel Xeon E5 2690 2.90GHz CPU with 256GB memory running Ubuntu 1204 The proposed and compared algorithms were implemented in C++ and compiled with g++v4.6 with the O2 option . 5.1 Experimental settings
We used 15 directed graphs shown in Table 46 . These networks except Flickr and Wikipedia have no timestamps ; thus we randomly ordered the edges .
6twitter 2010 and uk 2007 05 datasets were obtained from Laboratory for Web Algorithmics http://lawdiunimi it/datasets.php , Flickr and Wikipedia datasets were obtained from The Koblenz Network Collection http:// konectuni koblenzde/networks/ , and all other datasets were obtained from Stanford Large Network Dataset Collection http://snapstanfordedu/data/
Table 4 : Datasets , update time and number of iterations of the proposed algorithm for a single edge insertion and deletion . Results are the average of 100K insertion and deletion .
Dataset
Insertion
Deletion
76.1 0.8 0.3 205.9
15.5 64.7 2.2 69.3
|E| |V | 403K 3.4M 14.6 1.7M 11M 288.4 3.8M 17M 2.7 75.1 317K 2.1M time[µs ] #iters time[µs ] #iters 79.2 amazon0601 as Skitter 1.0 0.3 cit Patents 210.2 com DBLP 37K 368K 1,269.3 1,328.3 1,298.5 1,328.1 email Enron 524.1 564.3 soc Epinions1 76K 509K 17.9 11.8 soc LiveJournal1 4.8M 68M 24.6 32.0 1.6M 31M soc Pokec 7.2 web Google 876K 5.1M 24.4 61.7 282K 2.3M web Stanford 16.7 2.2 2.4M 5.0M 589.6 wiki Talk 1.2 42M 1.5B 29,382.8 twitter 2010 0.0 uk 2007 05 2.3 95.3 Flickr 10.2 42.0 76.8 Wikipedia
518.3 564.4 18.9 7.6 34.0 17.1 7.5 22.6 15.8 61.1 2.3 160.3 0.7 9,555.8 3.8 0.0 16.2 66.3 85.0 46.0
105M 3.7B 2.3M 33M 1.9M 40M
We assumed that a personalized vector b has 100 nonzero elements , which were chosen uniformly at random . Here we set a = 0.85 and = 10−9 7 . 5.2 Efficiency and scalability
Tracking performance .
We first evaluated the scalability of the proposed method . Specifically , we measured the update time and the number of iterations for each edge insertion and deletion .
For inserting edges , we set an initial network G(0 ) as a network with all edges except the last 100,000 edges . We ran the Gauss–Southwell method for G(0 ) to obtain the initial PPR x(0 ) and the corresponding residual r(0 ) . We then sequentially inserted the last 100,000 edges ( one by one ) to the current network and computed the PPR using the proposed method . Similarly , for deleting edges , we set an initial network G(0 ) as the whole graph . We then sequentially removed the last 100,000 edges from the current network and computed the PPR using the proposed method .
The results are shown in Table 4 . For all datasets , except for email Enron and twitter 2010 datasets , the proposed algorithm updated the PPR within 1ms . In particular , for uk2007 05 dataset , which is the web graph of “ .uk ” domain , the proposed algorithm updated within 3µs . These imply that the proposed algorithm can track very large rapidly evolving networks . The times and the numbers of iterations for insertion and deletion are almost the same . Thus the proposed algorithm can handle both insertion and deletion efficiently ; this is theoretically claimed in Section 43 It should be noted that even if we insert or remove edges in the order of time scales rather than the random order ( ie Flickr and Wikipedia ) , the proposed algorithm efficiently updates the PPR . In Section 4.3 , we claimed that the proposed algorithm is efficient for large networks in terms of 1/|E(t)| ( Lemma 7 and the discussion therein ) . To verify this claim , we plotted the relation between the number of iterations and the graph size |E| in Figure 2 . This shows that the number of iterations is actually proportional to 1/|E| . Therefore this supports the theoretical result and implies that the proposed algorithm is more suitable for large networks .
7Though there are several ways to handle dangling nodes , for convenience sake , we do not make a change in the graph .
881 Figure 2 : Graph size |E| versus average number of iterations . Green line is for ∝ 1/|E| .
( a ) Transition of the number of iterations .
( b ) Number of iterations k versus the cumulative frequency .
Figure 3 : Number of iterations for each insertion .
Distribution of the number of iterations .
The above experiments show that the proposed algorithm requires very few number of iterations in average ; this coincides with our theoretical result ( Propositions 5 ) . However , since these results are from the average , we are also interested in the distribution of the number of iterations .
We selected eight datasets ( cit Patents , com DBLP , emailEnron , soc LiveJournal1 , web Google , web Stanford , Flickr , and Wikipedia ) and ran the proposed algorithm with the same setting as above . We omit the results for other datasets because we obtained similar results . We observed the number of iterations for edge insertion . Figure 3 shows this result ; ( a ) is for the number of iterations for each step , and ( b ) is for the cumulative distribution of the number of iterations . ( a ) shows that “ many iteration cases ” rarely and sporadically occurred , and these cases were very time consuming than the average case . For example , in Wikipedia dataset , the average number of iterations was 46.0 ; however the worst number of iterations was greater than 1,000,000 . This implies that the worst case was 20,000 times slower than the average case . However , ( b ) shows that such cases occurred very rarely .
Multiple edges insertion .
In practice , many edges are often inserted or removed simultaneously . Therefore we are interested in the performance in this situation . Proposition 5 shows that the number of iteration is at most some constant ( per edge ) for any modification ; however , we expect that if a modification is small , the corresponding update is also small .
To evaluate the actual relation between the performance and the size of modification , we conducted the following experiments . We selected only two datasets ( soc LiveJournal1 and web Google ) due to the limitations of space . We inserted 1 , 10 , 100 , 1,000 , or 10,000 edges simultaneously , and then updated the PPR by the proposed method . We evaluated the update time and the number of iterations per each edge . The results are shown in Table 5 . This shows that even if multiple edges were inserted , the performance per edge did not change much . We also conducted the exper
Table 5 : Update time and number of iterations of the proposed algorithm for a batch insertion ( ie , inserting many edges simultaneously ) . The results are the average per edge .
Dataset batch size 100 101 102 LiveJournal1 103 104 soc
Insertion
Dataset time[µs ] #iters 7.6 7.6 web7.6 Google 7.5 7.1
18.4 18.3 20.0 20.1 19.7
Insertion time[µs ] #iters 22.6 22.5 22.3 21.1 15.5
8.3 7.5 7.7 7.5 6.6
Table 6 : Comparison with “ Static ” and “ Tracking ” methods .
Dataset amazon0601 as Skitter cit Patents com DBLP email Enron soc Epinions1 soc LiveJournal1 soc Pokec web Google web Stanford wiki Talk twitter 2010 uk 2007 05 Flickr Wikipedia time[s ]
Static
#iter 0.75 3,314K 247K 0.13 0.31 81K 1.24 5,667K 838K 0.13 802K 0.16 20.25 26,383K 10.33 15,304K 0.91 2,844K 0.34 1,419K 0.50 586K
Tracking time[s ]
#iter 29.82 195,389K 9,915K 1,587.07 1,155K 31.24 87.71 360,373K 289.26 518,492K 171.07 335,298K 865.34 713,063K 586.48 869,903K 23.96 84,544K 20.22 95,990K 8,589K – 71,503.90 298,296K 1,945.17 249,518K 919.68 637,384K
1,373.93 290.92 37,558K > 100,000.00
14.37 6,812K 9.77 11,618K 10.37 15,237K iment for simultaneous edge deletion , and the results were almost the same to the simultaneous edge insertion . So we omit them here . These show that the performance of the algorithm depends on only the size of modification .
Comparison with static computation .
Proposition 6 shows that we can track all changes of the PPR with only an additional O(log m ) factor to compute the PPR in a static network . We experimentally verify this proposition .
We first computed the PPR for the whole network by the Gauss–Southwell method . We refer this “ Static . ” We then performed the PPR tracking : We set an initial network G(0 ) with non edges , and inserted all edges one by one and updated the PPR by the proposed method ( for each insertion ) . We refer this “ Tracking . ” We compared the total run time and the number of iterations of these two settings . The results are shown in Table 6 . These show that “ Tracking ” required only 20–100,000 times longer computation time and the number of iterations than “ Static . ” We emphasize that “ Tracking ” actually computed the PPR O(m ) times because there are O(m ) networks , say G(0 ) , G(1 ) , . . . , G(m ) . Thus this 20–100,000 factor is significantly smaller than O(m ) . 5.3 Accuracy
We also investigated estimation accuracy of the proposed method . First , we examined the transition of estimation accuracy . To make the figure easy to see , we only selected eight datasets ( cit Patents , com DBLP , email Enron , socLiveJournal1 , web Google , web Stanford , Flickr , and Wikipedia ) . Note that we observed similar results for other datasets . As in the previous experiment , we set an initial network G(0 ) as a network with all edges except the last 100,000 edges , and computed the PPR x(0 ) by the Gauss–Southwell method . We then performed edge insertion one by one , and measured
0.01 0.1 1 10 100 1000 100001051061071081091010Average number of iterationsNumber of edges00e+00020e+00540e+00560e+00580e+00510e+00612e+00614e+006 0 20000 40000 60000 80000 100000Number of iterationsQuerycit Patentscom DBLPemail Enronsoc LiveJournal1web Googleweb StanfordFlickrWikipedia 1 10 100 1000 10000 100000100101102103104105106107108Number of queries ( #iters >= k)kcit Patentscom DBLPemail Enronsoc LiveJournal1web Googleweb StanfordFlickrWikipedia882 Table 7 : Efficiency comparison of the algorithms . DNF means an algorithm did not finish in 10,000 seconds in average .
Dataset amazon0601 as Skitter cit Patents com DBLP email Enron soc Epinions1 soc LiveJournal1 soc Pokec web Google web Stanford wiki Talk twitter 2010 uk 2007 05 Flickr Wikipedia
This work ins[µs ] 14.6 288.4 2.7 75.1 1,269.3 524.1 17.9 24.6 7.2 16.7 589.6 29,382.8 2.3 95.3 76.8 del[µs ] 15.5 64.7 2.2 69.3 1,298.5 518.3 18.9 34.0 7.5 15.8 160.3 9,555.8 3.8 66.3 85.0
Chien et al . [ 9 ] ins[µs ] 189.0 12,491.8 206.2 387.8 2,841.8 2,231.2 4,203.4 1,213.2 320.2 1,872.3 29,693.5 DNF DNF 95,192.5 40,335.5 del[µs ] 190.8 11,636.4 193.3 369.6 2,673.2 2,117.5 3,957.9 1,126.7 307.8 1,751.2 26,914.6 DNF DNF 93,013.2 38,043.1
Bahmani et al . [ 2 ] ins[µs ] del[µs ] 366.9 373.2 146.3 246.3 49.6 48.8 1,137.0 1,209.1 4,952.8 4,417.3 3,841.5 3,782.9 4,034.7 5,140.1 2,565.8 2,263.2 443.9 443.5 654.8 635.9 1,321.3 1,328.8 DNF DNF DNF DNF 7,771.9 7,228.9 9,196.2 9,812.4
Warm start ins[µs ] 20,549.2 61,516.8 DNF 16,228.1 2,518.3 2,491.4 DNF DNF 80,994.2 20,855.5 68,587.2 DNF DNF DNF DNF del[µs ] 23,677.7 49,400.7 DNF 15,920.2 2,574.4 2,222.6 DNF DNF 73,518.0 21,432.8 55,802.5 DNF DNF DNF DNF
From scratch ins[µs ] DNF DNF DNF DNF 56,684.1 46,273.1 DNF DNF DNF DNF DNF DNF DNF DNF DNF del[µs ] DNF DNF DNF DNF 41,089.0 56,730.4 DNF DNF DNF DNF DNF DNF DNF DNF DNF
( a ) email Enron
( b ) soc Epinions1
Figure 4 : Accuracy comparison of the proposed algorithm , the power methods and the existing two algorithms .
( a ) Single edge was inserted for each time .
( b ) Multiple edges were inserted for each time on web Google dataset . All results are overlapped .
Figure 5 : Accuracy of the proposed algorithm . the average L1 error x∗(t ) − x(t)1/n . Here , x∗(t ) is the “ exact ” PPR for G(t ) , which was computed by the power method for G(t ) from scratch , and x(t ) is the PPR computed by our proposed method .
The results are shown in Figure 5 ( a ) . These show that the errors did not exceed the threshold ; this coincides with Proposition 4 . We also evaluated the errors in multipleedges insertion case ; the results are shown in Figure 5 ( b ) . As seen , the accuracy was not affected by the number of simultaneously inserted edges in a single update , and was affected by the total number of inserted edges so far .
5.4 Comparison with existing algorithms
We compared the proposed algorithm with the naive power methods and existing two algorithms , the approximate aggregation/disaggregation method proposed by Chien et al . [ 9 ] and the Monte Carlo method proposed by Bahmani et al . [ 2 ] . These algorithms are described in Section 3 and 4 .
Figure 6 : Accuracy versus the number of Monte Carlo samples for Bahmani et al . [ 2 ] . Dotted line is for ∝ 1/
√ R .
To compare the different algorithms , we controlled the parameters of these algorithms to achieve the same accuracy . We first considered the power methods from scratch and with warm start . These naive methods compute the PPR for each snapshot with the setting of = 10−6 and achieved the average L1 error of roughly 10−9 .
We next considered the approximate aggregation/disaggregation method . We decided that when an edge ( i , j ) is inserted or removed at time t , we take a subset S as a neighbor of j , ie , S = {k : ( j , k ) ∈ E} . Then we applied the power method to compute the PPR in a contracted network with the setting of = 10−9 . By this setting , this algorithm achieved the average L1 error of approximately 10−7 .
We then considered the Monte Carlo method . The algorithm has a parameter R , the number of Monte Carlo samples . We computed several values of R and evaluated the average L1 error of the algorithm . The results are shown in Figure 6 ; in order to achieve the accuracy > 0 , we must take R = Ω(1/ 2 ) samples , which coincides with Eq ( 3 ) . According to this figure , we take R = 8|V | samples , which achieved the average L1 error of around 10−7 .
Figure 4 shows the errors of the proposed algorithm , the two naive power methods , and the two existing algorithms with above settings , respectively . We first observe that the proposed algorithm is comparatively accurate to the naive power methods . Moreover , the proposed algorithm is more accurate than the existing algorithms [ 2,9 ] . However , due to the poor scalability accuracy tradeoff of these algorithms , it is hard to improve the accuracy for them . Thus we compared these algorithms in these settings .
We then compared the computational time of these three algorithms . The results are shown in Table 7 . For graphs with millions of edges , the power method from scratch did not finish in 10,000 seconds . The power method with warm
1e 0141e 0121e 0101e 0081e 0061e 004 0 20000 40000 60000 80000 100000Average L1 errorNumber of inserted edgesThis workChien et al.Bahmani et al.Warm startFrom scratch1e 0141e 0121e 0101e 0081e 0061e 004 0 20000 40000 60000 80000 100000Average L1 errorNumber of inserted edgesThis workChien et al.Bahmani et al.Warm startFrom scratch1e 0111e 0101e 0091e 0081e 0071e 006 0 20000 40000 60000 80000 100000Average L1 errorNumber of inserted edgescit Patentscom DBLPemail Enronsoc LiveJournal1web Googleweb StanfordFlickrWikipedia1e 0111e 0101e 0091e 0081e 0071e 006 0 20000 40000 60000 80000 100000Average L1 errorNumber of inserted edgesBatch size = 1Batch size = 10Batch size = 100Batch size = 10001e 0101e 0091e 0081e 0071e 0061e 0051e 004 0.1 1 10Average L1 errorR/|V|cit Patentscom DBLPemail Enronsoc LiveJournal1web Googleweb StanfordFlickrWikipediaO(R 1/2)883 [ 8 ] M . Cha , H . Haddadi , F . Benevenuto , and K . P . Gummadi .
Measuring user influence in twitter : The million follower fallacy . In ICWSM , pages 10–17 , 2010 .
[ 9 ] S . Chien , C . Dwork , R . Kumar , D . R . Simon , and
D . Sivakumar . Link evolution : Analysis and algorithms . Internet Math . , 1(3):277–304 , 2004 .
[ 10 ] Y . Chung , M . Toyoda , and M . Kitsuregawa . Detecting link hijacking by web spammers . In PAKDD , 2009 .
[ 11 ] Y . Chung , M . Toyoda , and M . Kitsuregawa . A study of link farm distribution and evolution using a time series of web snapshots . In AIRWeb , pages 9–16 , 2009 .
[ 12 ] Y . Chung , M . Toyoda , and M . Kitsuregawa . Identifying spam link generators for monitoring emerging web spam . In WICOW , pages 51–58 , 2010 .
[ 13 ] G . M . Del Corso , A . Gull´ı , and F . Romani . Fast pagerank computation via a sparse linear system . Internet Math . , 2(3):251–273 , 2005 .
[ 14 ] Y . Fujiwara , M . Nakatsuji , M . Onizuka , and
M . Kitsuregawa . Fast and exact top k search for random walk with restart . VLDB , 5(5):442–453 , 2012 .
[ 15 ] D . Gleich , L . Zhukov , and P . Berkhin . Fast parallel pagerank : A linear system approach . Yahoo! Research Technical Report YRL 2004 038 , 13:22 , 2004 .
[ 16 ] Z . Gy¨ongyi and H . Garcia Molina . Link spam alliances . In
VLDB , pages 517–528 , 2005 .
[ 17 ] Z . Gy¨ongyi , H . Garcia Molina , and J . Pedersen . Combating web spam with trustrank . In VLDB , pages 576–587 , 2004 . [ 18 ] W . Hoeffding . Probability inequalities for sums of bounded random variables . J . Amer . Statist . Assoc . , 58(301):13–30 , 1963 .
[ 19 ] G . Jeh and J . Widom . Scaling personalized web search . In
WWW , pages 271–279 , 2003 .
[ 20 ] S . D . Kamvar , T . H . Haveliwala , C . D . Manning , and G . H .
Golub . Extrapolation methods for accelerating pagerank computations . In WWW , pages 261–270 , 2003 .
[ 21 ] A . N . Langville and C . D . Meyer . Deeper inside pagerank .
Internet Math . , 1(3):335–380 , 2004 .
[ 22 ] A . N . Langville and C . D . Meyer . Updating markov chains with an eye on google ’s pagerank . SIAM J . Matrix Anal . Appl . , 27(4):968–987 , 2006 .
[ 23 ] P . Lofgren , S . Banerjee , A . Goel , and C . Seshadhri .
FAST PPR : Scaling personalized pagerank estimation for large graphs . In KDD , pages 1436–1445 , 2014 .
[ 24 ] T . Maehara , T . Akiba , Y . Iwata , and K . Kawarabayashi .
Computing personalized pagerank quickly by exploiting graph structures . VLDB , 7(12 ) , 2014 .
[ 25 ] F . McSherry . A uniform approach to accelerated pagerank computation . In WWW , pages 575–582 , 2005 .
[ 26 ] L . Page , S . Brin , R . Motwani , and T . Winograd . The pagerank citation ranking : Bringing order to the web . Technical report , Stanford InfoLab , 1999 .
[ 27 ] A . D . Sarma , S . Gollapudi , and R . Panigrahy . Estimating pagerank on graph streams . J . ACM , 58(3):13 , 2011 .
[ 28 ] G . Shen , B . Gao , T Y Liu , G . Feng , S . Song , and H . Li .
Detecting link spam using temporal information . In ICDM , pages 1049–1053 , 2006 .
[ 29 ] H . A . Simon and A . Ando . Aggregation of variables in dynamic systems . Econometrica , 29:114–138 , 1961 .
[ 30 ] R . V . Southwell . Relaxation Methods in Engineering
Science . Oxford University Press , 1940 .
[ 31 ] R . V . Southwell . Relaxation Methods in Theoretical
Physics . Oxford University Press , 1946 .
[ 32 ] D . A . Spielman and S H Teng . A local clustering algorithm for massive graphs and its application to nearly linear time graph partitioning . SIAM J . Comput . , 42(1):1–26 , 2013 .
[ 33 ] J . Weng , E . P . Lim , J . Jiang , and Q . He . TwitterRank : finding topic sensitive influential twitterers . In WSDM , pages 261–270 , 2010 .
Figure 7 : Application : vertex importance tracking . start becomes much slower for larger datasets because it requires Ω(m ) time . This table also shows that the proposed algorithm is up to 10,000 times faster than the power methods , 2–1,400 times faster than the aggregation/disaggregation method by Chien et al . [ 9 ] and 2–290 times faster than the Monte Carlo method by Bahmani et al . [ 2 ] ( except for asSkitter ) . Therefore the proposed algorithm practically outperforms all the existing algorithms . 5.5 Application : vertices importance tracking Finally , we demonstrated the proposed algorithm for an application of the vertices importance tracking problem . For this experiment , we obtained the hyperlink network of the English Wikipedia with edge arrival time from the Koblenz Network Collection.8 The network has 1.9M vertices and 40M dynamic edges whose timestamps are in 01/01/2001 00:00:00 to 12/31/2007 23:59:59 . We selected ten vertices , which have largest PPR scores in the final network , and tracked the changes of these scores . The results are shown in Figure 7 . We can observe several events from this figure ; for example , the article corresponding to the blue line , increased its score three times at 2005 , 2006 , and 2007 . Thus we can guess that there may be some events in these times . By drawing such a figure , we can detect and track emerging trends and hot topics from an evolving network .
6 . CONCLUSION
In this paper , we propose an algorithm for tracking personalized PageRank in an evolving network . The proposed algorithm is efficient in theory , and practically faster than the existing algorithms . For a web graph with 105M vertices and 3.7B edges , the algorithm can update the personalized PageRank in 3µs when an edge is inserted or removed . By comparing existing state of the arts algorithms , our algorithm is 2–290 times faster with an equal accuracy .
7 . REFERENCES [ 1 ] R . Andersen , F . Chung , and K . Lang . Local graph partitioning using pagerank vectors . In FOCS , pages 475–486 , 2006 .
[ 2 ] B . Bahmani , A . Chowdhury , and A . Goel . Fast incremental and personalized pagerank . VLDB , 4(3):173–184 , 2010 .
[ 3 ] B . Bahmani , R . Kumar , M . Mahdian , and E . Upfal .
Pagerank on an evolving graph . In KDD , 2012 .
[ 4 ] A L Barab´asi and R . Albert . Emergence of scaling in random networks . Science , 286(5439):509–512 , 1999 .
[ 5 ] A . A . Bencz´ur , K . Csalog´any , T . Sarl´os , and M . Uher .
SpamRank–fully automatic link spam detection work in progress . In AIRWeb , 2005 .
[ 6 ] P . Berkhin . Bookmark coloring approach to personalized pagerank computing . Internet Math . , 3(1):41–62 , 2006 .
[ 7 ] S . Brin and L . Page . The anatomy of a large scale hypertextual web search engine . Comput . Networks ISDN Syst . , 30(1):107–117 , 1998 .
8http://konectuni koblenzde/networks/
0 0.002 0.004 0.006 0.008 0.01 0.012200220032004200520062007 PPR scoreYear884
