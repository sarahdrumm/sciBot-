Proof Protocol for a Machine Learning Technique Making
Longitudinal Predictions in Dynamic Contexts
Kevin B . Pratt
Chief Scientist at ZZAlpha Ltd.1 kevinpratt@zzalphacom
This question might apply to behaviors of a live anti viral flu vaccine , to methods of a stock prices prediction engine , to robotic vehicles navigating in a city undergoing wartime destruction or to an ad placement “ optimization ” system for a multi national , multichannel , competitive consumer product . In a broader sense , how does one persuasively answer the skeptic ’s question of the value of big data analytics in the real and changing world where tomorrow will be different in unknowable ways and where humans are incapable of doing the data learning and analysis that is being conducted within the “ black box ” ?
In 2010 , we set out to prove that a machine learning technique which we were deploying could successfully , repeatedly find exploitable future stock market inefficiencies at scale . Our goal was to prove that technique ’s existence and efficacy beyond a reasonable doubt . Part of our task was defining a strong proof protocol . The proof must be strong because the existence and efficacy of such a technique contradicts the Efficient Market Hypothesis21 for which the Nobel Prize was awarded in 2013 .
The learning technique makes early morning recommendations to buy a small , fixed size set of stocks at a price equal to the opening price ( whatever it may be ) today and to sell those same stocks one week later at the opening price then . It uses input data as of market close the prior day . It thus must work with two great uncertainties ( latencies ) : it does not consider events occurring overnight when making its morning recommendation , and it does not revise recommendations , once made , in view of events in the coming week that affect the market before results of the recommendations ( sales ) are recorded . Our null hypothesis is that such a technique will produce nothing statistically significant in any market segment .
This article focuses on proof protocol and associated big data behavior visualizations applicable to dynamic contexts , rather than substantive results , which results are presented in another ( pending ) journal article .
2 . PROOF PROTOCOL Proof protocols and techniques for evaluating data mining validity in stationary contexts have been well developed . N fold cross validation with holdout test sets , confusion matrices , f measure weightings and Receiver Operating Curves ( ROC ) are widely applied and well understood [ 6 ] . Statistical dispersion measures for
                                                             1 The author is also Senior Analytics Scientist at Teradata Corp . The work described here has no association whatsoever with Teradata Corp . or the author ’s employment there . 2  “ In the 1960s , Eugene Fama demonstrated that stock price movements are impossible to predict in the short term… ” http://wwwnobelprizeorg/nobel_prizes/economicsciences/laureates/2013/fama factshtml  results visualizations the proof components . Components
ABSTRACT : We demonstrate a protocol for proving strongly that a black box machine learning technique robustly predicts the future in dynamic , indefinite contexts . We propose necessary components of the proof to support protocol and demonstrate evaluation of include contemporaneously verifiable discrete predictions , deterministic computability of longitudinal predictions , imposition of realistic costs and domain constraints , exposure to diverse contexts , to a priori statistically significant excess benefits relative benchmarks and Monte Carlo trials , insignificant decay of excess benefits , pathology detection and an extended real time trial “ in the wild . ” We apply the protocol to a big data machine learning technique deployed since 2011 that finds persistent , exploitable opportunities in many of 41 segments of US financial markets , the existence of which opportunities substantially contradict the Efficient Market Hypothesis . Categories and Subject Descriptors H34 [ Information Storage and Retrieval ] : Systems and Software Performance evaluation ( efficiency and effectiveness ) I26 [ Artificial Intelligence ] : Learning Concept learning I64 [ Simulation and Modeling ] : Model validation and Analysis
Keywords Experiment design , hypothesis testing , black box , big data , longitudinal study , dynamic context , simulation , pattern analysis , market forecast , prediction . 1 . INTRODUCTION “ We think it very interesting to teach machines to understand behaviors that humans cannot describe . But , how do we know they understood ? ”
How does one prove that a frequently erring method applied in a constantly evolving environment – is authentic , robust and valuable ? Especially where task of proof is aggravated because the method itself is evolving in response to the changing environment and lives hidden inside a “ proprietary black box ” ?
2049 measurement data , and goodness of fit to parametric descriptions have a long history . Evaluating big data techniques in dynamic contexts where no situation repeats requires new approaches [ 3,14 ] . Data distributions and object interconnectivity change constantly , and assumptions that distributions are “ normal ” under any parametric description are infrequently valid . Big data analytics feels like navigating midocean on a stormy night with unreliable instruments . Nonetheless , despite the constant changing contexts , we suggest that strict scientific method be applied to avoid squishy evaluation using “ pleasing anecdotes . ” While longitudinal experimentation is not new [ 24,26,27 ] and is applied in a variety of domains [ 1,2,8,12,19 ] , big data is causing reexamination of the methodologies [ 5,22,25 ] . Proving validity of prediction in longitudinal , constantly changing problem spaces suffers from three special challenges . i ) Data available today about prior inputs and prior outcomes may have been cleansed ( sometimes almost invisibly ) by hindsight . An obvious example is survivor bias during “ back testing ” in finance [ 9,13 ] . A less obvious example we observed is that published data such as stock trading data is silently revised by vendors in later publications when “ final numbers ” are received . ii ) More insidious is the hindsight technique of publishing only favorable results from a panel of “ equally wellconceived ” trials that had mixed results . This variety of hindsight filtering has been used intentionally in some stock scams , and sometimes naively in drug research where side effects in some cohorts are dismissed [ 7,17 ] . iii ) Longitudinal interim results can compound in dynamic feedback systems , whether as part of the contextual environment or as part of the prediction technique . What seems well behaved in early episodes may fluctuate or become brittle diverging or decaying over time . A well defined protocol for proving capability of machine learning in longitudinal , dynamic contexts supports user trust in today ’s black box , proprietary technologies [ 10,18,21,23 ] . It is also needed for valuation of such technologies for capital investment .
2.1 Components of Proof Protocol We suggest it is critical to define how success will be measured in advance of an experiment with a learning system , and how to assure the experiment is authentic and sufficient [ 20,22 ] .
211 Contemporaneously verifiable discrete predictions A prediction must be specific . The set of cohorts for which predictions will be made must be defined in advance . The deadline or trigger for each prediction must be established . In order to validate these predictions , all should be consolidated , unalterably archived and time stamped . In later evaluation of a learning technique , only those predictions meeting this criteria should be included , otherwise one risks “ revise as we go ” hindsight .
For the ZZAlpha machine learning technique , we defined ( in 2010 ) 41 market segments for which we would produce long and short recommendations before the market opened every day . We also defined 4 size sets for each segment . We aggregated these 320 daily predictions and submitted them to encryption and electronic timestamping by Digistamp [ 4 ] early each morning beginning in late 2011 . Digistamp is a trusted third party time stamp authority using an encryption algorithm that does not require deposit of documents with them . Rather a public certificate is provided for unlocking and verifying the date time of the document . A timestamp costs $0.40 and is commonly used for intellectual property protection .
The learning engine makes predictions for recommendations that are discrete and actionable : buy ( or sell ) a set of stocks at prescribed prices today .
212 Deterministic computability of repetitive longitudinal application of predictions In longitudinal studies with asynchronous triggering events , there is the issue of how to define and calculate the initialization of a fresh context given the resource impact of the sequence of prior decisions . In a stock investment context , that means determining how much cash is now available for purchases after closing prior investments . In an advertising campaign that might mean how to allocate remaining spend after detecting cohort responses to prior channel impressions . In a dynamic real life context , acting on every prediction causes resource adjustment , and there must be ongoing resource accounting with hard numbers .
For the machine learning evaluation , we created a strict cash flow trading performance evaluator ( detailed below ) based on a typical tax free retirement account where margin ( borrowing ) is prohibited . Thus , a sale must be closed at a defined price with dollar proceeds added to the “ pot ” before a following purchase can be made .
213 Imposition of realistic costs and context constraints during evaluation As part of the resource accounting identified above , there must be imposed realistic transaction costs and context constraints . For example , in a stock trading context , commissions are paid at the time of trade . Short trades cannot be made when brokerages do not have short shares to lend . Institutional investors may be prohibited by self imposed investment guidelines from trading low priced stocks . These costs and constraints need to be applied during the goforward accounting of prediction results . Identification of these costs and constraints typically requires consultation with industry subject matter experts and careful modeling . In an advertising campaign this might include lead times , placement premiums , and commission levels .
For the evaluation here , we applied trading commissions , price , liquidity and slippage constraints .
214 Exposure to diverse contexts To be deemed robust , a longitudinal technique must survive or thrive when exposed to the unknown where those unknowns may arise in the future . A general technique , for example a drug therapy , must apply successfully to diverse cohorts and cohort specific stressors , for example cohorts receiving other treatments . Cohorts must be defined in advance of the experiment . If future contextual events will be “ excused ” , those permissible excuses must also be defined in the proof protocol . For the learning technique experiment here , we identified 41 cohorts spanning diverse market segments , and allowed any trading day as an acceptable context . Impactful events such as the fall of gold prices , Chinese economy slowing , credit card frauds , oil fracking , and very low interest rates were some of the unexpected future unknowns to which the learning technique had to respond , for all cohorts .
2050 indexed learning method , after expenses , must exceed
215 Statistically significant excess benefits relative to a priori benchmarks and Monte Carlo trials In experiment design , the comparison benchmarks must be designated in advance [ 11 ] . They need to be appropriate to the cohort , and need to be measurable over the duration of the experiment . Thus , a traditional “ status quo ” strawman is never available in a dynamic context . There are often hindsight incentives to change to lesser benchmarks so that benefits can be announced from the method being attempted . In the stock domain , Exchange Traded funds to a sector often offer appropriate benchmarks . Stock indexes are less persuasive benchmarks because they are generally capitalization weighted and exclude the compounding benefits of dividends . Monte Carlo evaluation methods can apply in dynamic contexts , and can be applied to the varying cohorts [ 23 ] . Fundamentally each decision of the method being evaluated is replaced with a random selection from the decision set . Evaluation of Monte Carlo results provides both a distributional representation of outcomes , and a sense of whether the chosen dispersion method , such as standard deviation , is appropriate . Experiment design also requires advance specification of how improvement is to be measured and what value constitutes success . At ZZAlpha we selected an ETF or market index for comparison to each recommendation segment , and built a Monte Carlo trial option into the longitudinal performance evaluator . We chose two measures of success . With respect to a market benchmark , the machine the annualized returns of the benchmark by 200 basis points . With respect to the Monte Carlo trials , the machine learning method must provide results more than 3 standard deviations ( z score ) above the segment mean .
216 Insignificant decay of excess benefits In stationary contexts , over fitting causes brittleness that reduces accuracy when applied to the test sample . In longitudinal dynamic contexts a parallel problem can arise when the method is too strongly tuned to the training episodes [ 16 ] . Although one expects a learning method will vary in the success of its predictions as contexts change , the longer term results should not decay . Historic market prices fell 40 % or more in 2008 in most stock market segments , but rose again . Some segments have lower returns for several years .
The three years of certified recommendations here do not support evaluation of decay . The 10 year simulated recommendations using the machine learning technique in forward testing mode are the best available data to respond to decay concerns . We use a requirement of positive regression slope and specifically a more positive slope than that of benchmarks . 217 Controlled risk and absence of pathologies Black box techniques in dynamic contexts can exhibit pathological behaviors . In the market price prediction domain we needed to detect and evaluate three : Does the technique repeat or cycle prior predictions ? Do individual recommendations within a daily set have high risk ( large losses ) or are there extended episodes of underperformance ? Does the technique only detect opportunities of either price increase ( long ) or price decrease ( short ) ?
For evaluation of these possible pathologies , we used a quarterly periodicity visualization , a min max timeline envelope , z scores of both long and short recommendations , and descriptive statistics of the severity of below regression line risk .
To support the elements of the proof protocol , we prepared over 25 visualizations for each market segment , some of which are included below . These were to confirm the expected and expose the unexpected , to compare to benchmarks , and to support crosssegment interpretation and pathology identification .
218 Extended duration real time trial “ in the wild ” Simulations can inadvertently omit costs and constraints that occur when participating in dynamic contexts . We suggest it is critical to obtain some portion of ground truth to corroborate lab evaluation techniques , and to uncover experiment misconceptions .
We used daily a few of the machine learning recommendations in the most widely analyzed segment the stocks in the S&P 500 list to control actual trading over 15 months with a pot of about $500,000 . This parallels a limited clinical trial in the pharmaceutical domain or “ back road ” testing of driverless vehicles .
3 . MACHINE LEARNING TECHNIQUE We originated a machine learning technique and tuned that technique in 2009 2010 using portions of 2008 data . We began operating the machine learning technique in 2010 to make daily stock recommendations . The technique continues in autonomous operation today , situated on a large cloud platform . It has no tuning knobs .
To protect proprietary intellectual property , we hide the inner details of the technique . However , in overview , it ingests data , applies transforms , derives features , considers past market dynamics , applies common machine learning technique(s ) , and predicts stocks likely to rise or fall in price ( categorized within 41 market segments Table . 1 ) . 32
Inputs : The technique consumes 5 types of public data : i ) end ofday daily stock price and volume data , ii ) a large set of traditional stock fundamentals updated weekly , iii ) Securities and Exchange Commission ( SEC ) filings on initial public offerings , iv ) daily information on merger and acquisition announcements and v ) semiannual data needed to include or exclude stocks in lists of the 41 segments eg in the list of S&P 500 stocks or the Health sector or sub sector of Banks ( defined by SIC ) . All of the data consumed is available on the internet for less than $200 a year in subscription fees . It is also available more quickly from commercial suppliers such as CapitalIQ .
Outputs : The technique outputs daily newsletters on 320 recommendation portfolios , and most importantly a complete , consolidated is encrypted and contemporaneously certified by Digistamp before market open and then archived . The certifications began Nov . 2011 .
In 2011 we simulated the machine learning technique to produce 2005 2009 recommendations to confirm its robustness . That simulation involved feeding the machine learner the weekly                                                              3 This paper does not intend to emphasize the stock recommendation system , but rather to use such a system as an example to focus on the hard , ethically challenging and increasingly important question of “ how would one decide ” if such a system , and similar systems which must contend with unknown dynamic contexts , would be likely robust and effective .   recommendation list that
2051 fundamentals data as originally published at that historic week and providing historic information ( including delisted or renamed stocks ) of prices and volume ( which was split and dividend corrected ) as of 2011 . The machine learner then issued daily recommendations for the 320 recommendation portfolios day byday in a consolidated daily document . This five year simulation took over 9 months of computation .
The goal of the historic simulation ( combined with the subsequent go forward predictions ) was to help evaluate possible decay of results over the longer term , and to evaluate the resiliency of the machine learning technique in the extreme contexts of 2007 2008 .
Table 1 . Segments ( cohorts ) spanning the US market for predictions . All portfolios are produced for Long and Short positions , and in daily recommendation sizes of 2 , 5 , 10 and 20 stocks ( with a few exceptions ) .
We add several constraints to the evaluator : The “ pot ” of cash is initialized , typically set to $1 million . The pot is initially divided into n portions . On each day , one of the portions is available for purchases . After each day the portions are rebalanced to remain approximately equal . A minimum purchase block size is set to 10 . All transactions are “ limit order , ” which prevents price slippage during trades .
Some cash tracking details : If a stock appears in the new “ buy ” list on the day of “ sale ” , it is treated as held for n additional days and does not contribute to the cash portion on the originally expected day of “ sale ” . If for any reason a stock does not have an opening price on the day of purchase , the stock is not purchased and funds remain in cash . When a stock is not purchased , the cash allocated for that purchase remains unspent in the current portion and becomes again available n days later . The evaluator can also track tax gains and payments .
The evaluator also allows selection of a benchmark , such as the Dow Jones Industrial Index or an S&P500 tracking index fund ( for comparison of index fund long term tax treatment “ buy and hold ” ) .
The evaluator determines results of long positions . It does not evaluate short positions because i ) some stocks never or seldom allow shorting and we have not acquired that data , ii ) it is impossible to determine whether on a particular day a particular stock has short shares available from a particular brokerage for loan to a particular investor , and iii ) regulations sometimes only allow short on uptick . However the evaluator does support a computational concept of “ anti long ” . Given a list of stocks that are predicted to decline in price ( typically used in selecting shorts ) , it will indicate what results would be if that list had inadvertently been treated as long recommendations .
The recommendation performance evaluator operates in two modes :
4 . RECOMMENDATION PERFORMANCE EVALUATOR The evaluator is entirely separate from the machine learning technique . However , it is focused on evaluating the “ weekly rolling recommendations ” of the type made by the machine learning technique . The evaluator is cash based [ Fig 1 ] . It takes a list of k stocks to buy on a day and consumes the available portion of cash for that day to make equal dollar valued ( 1/k ) purchases of the stocks . After n trading days it sells those stocks and contributes the proceeds back to cash . It then repeats . Cash can never be negative . All purchases and sales are made at a price equivalent to the opening price that day . Commissions ( either fixed dollar or per share ) are applied at each transaction . the results of
Recommendation mode applies the certified ZZAlpha® portfolio recommendations ( for a specific portfolio in a market segment and of a specific size ) . Those results have been historically recorded about 3 days after sale of a position , showing the contemporaneous purchase price and the sale price and relative result ( without assuming a quantity of shares purchased ) . The purchase price is adjusted for dividends and splits to keep it consistent with the sale price . On a few occasions the record is incomplete , typically because of a data provider omission , because the stock price dropped below $3 or trading volume fell below 80,000 shares . Any missing records are treated as “ not bought ” with no gain or the recommendation mode . loss . Missing records are logged during the selection mode operates exactly
Random same as recommendation mode with two changes . First , instead of a recommendation stock , a stock is selected from the same portfolio market segment at random ( using the Java cryptographic random number generator ) . Second , the purchase and sale prices are obtained from current and archive listings of daily stock prices , adjusted for dividends and splits . On a few occasions the current price record is incomplete , typically because of a data provider omission , because the stock price dropped below $3 , because trading volume fell below 80,000 shares or because of delisting . Share price increases due to mergers and acquisitions are typically
2052 present in the records . Any missing records are treated as “ not bought ” with no gain or loss . There are more missing records in random selection mode than in recommendation mode , but seldom more than 10 missing records in a year . Missing records are logged during the random selection mode .
The effect of switching from recommendation mode to random selection mode is to substitute a random choice from the relevant market segment for a machine learning generated recommendation . Thus , operation of the performance evaluator over a period of time ( three years in this report ) in random selection mode is a Monte Carlo simulation trial , using exactly the same assumptions , constraints , costs and accounting as used in the recommendation mode evaluation . All Monte Carlo trials are repeated 1000 to generate reliable estimates . 5 . EVALUATION OF RESULTS PURSUANT TO THE PROOF PROTOCOL We applied the protocol to the machine learning technique's recommendations . We show visualizations for S1 Basic Industries , one of the market segments , for five recommendations each day . Contemporaneously verifiable discrete predictions The daily consolidated recommendations for the 320 set are unalterably time stamped by Digistamp prior to market open . Each recommendation set lists eg five stocks to buy at the opening price , which are to be held one week and sold .
Deterministic computability of repetitive longitudinal application of predictions We use the recommendation Performance Evaluator described in Fig 1 to obtain deterministic results .
Imposition of realistic costs and context constraints during evaluation In addition to the constraints expressed in the Performance Evaluator , we apply three additional constraints to assure to stocks which institutional investors can purchase under regulatory requirements : $3 recent minimum price , 80,000 daily share volume , and listed on the major US stock exchanges . the recommendations pertain that
Figure 2 . Annual results and benchmark for ten years , showing regression lines used for decay evaluation . Because the recommendation slope of 0.79 is less than the benchmark slope , this suggests the quality of recommendation is declining slightly .
Statistically significant excess benefits relative to a priori benchmarks and Monte Carlo trials We use z score to measure significance . Figures 3 , 4 and 5 show results for one of the segments Sector 1 Basic Materials . Both the long and anti long results have z scores greater than 3 , indicating great significance . In another article we detail that of the 41 recommendation sets of daily size 5 , 5 have z scores greater than 3 , 3 have z scores between 2 and 3 , and 9 have z scores between 1 and 2 . Anti longs show greater significance . Of the 41 portfolios , 23 exceed both their segment means and their benchmarks by 200 basis points ( annualized ) , and another 2 exceed their segment means and their benchmarks by 100 basis points ( annualized ) .
Exposure to diverse contexts The 41 market segments span the US equities market . Each segment suffers somewhat individual stressors , such as falling oil prices or semiconductor innovations . The three years of certified recommendation do not include an episode of price drop of over 10 % , and thus lack that diversity . The ten year simulation attempts to overcome that by including the 2007 2008 episode of price drop . Fig 2 shows the ten years of annual returns .
Figure 3 . Results compared to benchmarks : 1000 Monte Carlo trials and ETF . Monte Carlo mean and +/ standard deviation shown in black dotted lines . Machine learning recommendation results for long shown in blue line on right , for anti long in red line on left . Benchmark XLB results indicated by green dashed line .
2053 Select : Portfolio name , Long or anti long , Recommendation or Monte Carlo mode , Benchmark ( optional ) , Short term and long term tax rates ( optional ) . Initialize : Pot , Commission ( per trade or per share ) , Portfolio daily size k , Minimum block size , Dates of study period , ( and if Monte Carlo mode , number of trials ) . Begin : Divide cash pot into n portions ( equal to number of hold days = 5 ) . For each of n days , buy equal value quantities of k recommended ( or randomly selected ) stocks using that day ’s portion of the pot at opening price . Determine each purchase commission and apply before each purchase . If stock cannot be purchased , leave cash in that portion . Purchases must exceed minimum block size . ASSERT : remaining cash is near zero in each of the n portions . Continue until last date : sell stock at opening price and contribute to cash portion after paying commission . If the opening price is not available , treat as sold at original purchase price . Rebalance the portions by contributing 1/k of sale profits to each .
For each of the recommended stocks , buy equal value quantities of k recommended ( or randomly selected ) stocks using that day ’s portion of the pot at opening price ( omitting already held stocks ) . Determine each purchase commission and apply before each purchase . If a stock cannot be purchased ( because opening price is not available ) , leave cash in that portion . Purchases must exceed minimum block size .
On last date :
If is last day of year , determine tax owing or loss carryforward . If is Apr 1 , reduce total cash by tax owing before making day's purchases .
For each of the k stocks purchased n days prior , if stock currently held is in current day recommendations , continue to hold . Else ,
Calculate value of stocks held using purchase prices and add to cash . Report total value change . Report benchmark change .
If Monte Carlo mode and trials are not complete , reinitialize pot and Begin .
Figure 1 . Performance Evaluator pseudo code
Figure 4 . Comparison of results of each recommendation over 3 year test to benchmark ETF results ( for all week long periods ) . There is a long right tail on the portfolio recommendations in this Cauchy Lorentz distribution . The figure shows greater dispersion of results for the portfolio , but more positive area under the curve ( in the 3 years , 52 % of the recommended trades had positive profit ) .
Figure 5 . Cumulative results from recommendations in dark blue and benchmark XLB in thin black . This common timeline graph helps quickly identify correlations and anomalies .
Fig 6 shows persistence of results ( both high and low ) when comparing average results of individual stocks in the current year with the average results of the same stock in the prior 2 years . The positive correlation and regression slope suggest that a favorable context for a specific stock may repeat . The machine learning technique may be detecting that subtle context within the chaos of the dynamic market . However , this persistence was absent in other segments .
2054 Figure 6 . For year 2014 in this segment , , the average returns of a recommended stock in the prior two years are somewhat correlated ( Pearson r=0.24 ) with current year returns . The Loess spline regression emphasizes variations in the regression and indicates discontinuance of positive regression in the middle range , which is not shown by ordinary linear regression line .
Insignificant decay of excess benefits We identified two varieties of decay . As the number of daily recommendations increase , the recommendation effectiveness decreases : Annualized returns ( after commissions ) for sizes 2 , 5 , 10 and 20 declined from 23.8 for size 2 to 20.2 , 11.8 and 5.8 in the S1 Basic Materials segment . By changing the commission structure constraint from a fixed $8 per trade to $0.05 per share , the returns for size 20 were increased to 86 This is a pattern we saw often across the 41 market segments . Second , looking at annual return rates over 10 years ( 7 years of simulation and 3 years of certified results ) , we saw decay in results as shown in Fig 2 .
Controlled risk and absence of pathologies A machine learning technique in dynamic contexts will likely change its own behaviors . Visualizations from different perspectives are useful . In addition to traditional risk and drawdown statistics , we inspected three suspect behaviors . First , we evaluated the daily range of results to identify spikes or longer duration episodes of extreme results Fig 7 .
Figure 7 . Daily ranges of recommendation results ( 5 daily recommendations ) compared to benchmark . Note that more extreme events were positive , which contributes to rapid compounding of the rolling recommendation results .
We found that although greater mean returns occurred at higher capitalization ranges of the segment , Fig 8 , the greatest total profits derived from the lowest capitalization stocks Fig 9 . In Fig 10 , we saw that variation in returns dropped slowly as capitalization increased . Fig 11 indicated a slow change in capitalizations of selected stocks . We inspected recommendations for abnormal periodicity , Fig 12 and found one possible pathology . A few recommendations recurred 4 times on the same day in the 12 quarters of the 3 years , but no significant overall pattern appears in S1 Basic Materials . To further investigate the possible pathology , we looked at the frequencies of individual stock recommendations in the 3 years , which ranged from 90 to one . The existence of several high frequencies suggested some instances of clustered repetitions would not be abnormal . We also maintain extensive operational logs should the need arise to investigate pathologies .
We also investigated how the set of five daily recommendations fit within the rank of all segment members . We expected to see ranked recommendations correlated with ranked results . Use of rank correlation avoids difficulties of comparing absolute results from different contexts , where “ good ” can mean significantly different absolute results . Fig 13 show almost no rank correlation of the machine learning recommendations to ranked results . However we investigated the extremes in the corners of Fig 13 . The histogram in Fig 14 discloses a narrow spike of high result rank correlation to high recommendation rank . This exemplifies the “ needle in haystack ” that could be lost using ordinary descriptive statistics or down sampling of big data .
2055 Figure 8 . Mean returns by capitalization . Bin size = $1B . This example differs from many of other segments where positive returns were more commonly seen in the smaller cap stocks .
Figure 9 . Profit by capitalization . Low capitalization stocks heavily contribute ( because they are more often recommended ) to recommendation set returns for S1 Basic Materials .
Figure 11 . The recommendations included more large cap elements over time . The pink largest cap cluster at the top may indicate a single stock that either issued more shares and/or saw large price increases .
Figure 10 . Variation in returns by capitalization . The generally declining variation is as expected . It is one of many “ sanity checks ” on performance over the 3 years .
Figure 12 . Periodic repetition or recommendations across quarters . Small dots indicate 2x in 12 quarters . Largest dots indicate 4x in 12 quarters . The sequence of large dots at the top middle suggest a possible autocorrelation pathology for examination .
2056   Figure 14 . This figure repeats the right most column of Figure 13 as a histogram . It indicates that for the 5 highest ranked recommendations ( the only ones included ) , there is an unusually high correlation with the highest actual ranks , but most of the actual ranks are spread evenly across lower ranks . An optimal correlation would have higher right side bars descending to lower left side bars . This figure discloses an unexpected behavior that may indicate fragility . The center gap is an irrelevant artifact of aligning the right half of the visualization to account for varying sample set sizes . 6 . CONCLUSION We developed and applied this proof protocol to a black box machine learning technique that makes repeated longitudinal recommendations for stocks in the US market . We designed the protocol to support a thorough , trustworthy and persuasive evaluation of the machine learning technique , recognizing that the technique often received erroneous real time input data , that post recommendation events would materially affect recorded results and that it would need to operate in and be evaluated with respect to unknowable future contexts .
Effective application of a proof protocol requires subject matter expertise to assure appropriate context constraints and relevant benchmarks are applied . Because hindsight filtering is a primary experimental risk in longitudinal experiments , we suggest timestamp authenticity certification of every evaluated decision is a key minimal requirement . Extensive visualizations of the black box outputs , from many viewpoints help understanding of the performance . Cross comparison of visualizations among segments add important understanding of broader patterns .
Two proof concerns remain with respect to the machine learning recommendations discussed here : First , the contexts that arose during the three year certified experiment did not include an extended episode of falling prices , or “ crash . ” Second , there is some evidence of learning decay which needs continuing years of study .
It may never be possible to exhaustively evaluate a decisioning/recommending system built for dynamic contexts because of unforeseen factors . Yet , in business , defense , and space exploration final decisions must be made about design , expenditures and testing before deployment non the less . Decision makers need the best proof possible .
Finally , we hope insightful , skeptical critiques of the proof protocol will support its extension to other big data longitudinal prediction domains where contexts evolve in unknown ways .
Figure 13 . Rank correlation permits comparison across dynamic contexts . For each day , both predictions and results within a segment are ranked . An optimal correlation of result rank to recommendation rank would be a thin scatter running from lower left to upper right . Instead this figure shows almost no correlation , but concentrations in corners . This visualization applies in dynamic contexts where correlation of actual returns would be effectively unusable because a “ good ” prediction in an up market might be 5 % increase compared to a market wide 2 % increase and a “ good ” prediction in a down market might be a 5 % decrease compared to a market wide 8 % decrease . The horizontal and vertical gaps in the center of the figure are artifacts resulting from different sizes of the sample set on different days . To support inspection of the high ranks , both the right and top are aligned for the upper half of the samples . The regression line in the middle of the figure shows the very weak positive slope . The rank correlation patterns seen in the other 40 market segments can vary substantially this visualization . The recommendation engine is performing differently here than in other segments . from
Extended duration real time trial “ in the wild ” We used a pot of about $500,000 for about 15 months ( Oct 7 , 2013 Dec . 31 , 2014 ) to act upon machine learning recommendations ( and no others ) for stocks in the SP500 list , using a standard internet discount brokerage account . The actual ( cumulative ) , after commissions , exceeded the returns calculated by the Performance Evaluator for that segment and dates ( 34.1 % vs 222 % ) This both provides evidence confirming the ground truth of the Performance Evaluator software , and the reality that the machine learning technique's recommendations were actionable trades in the SP500 segment of the dynamic market . The small trial did not confirm viability at institutional scale . returns
2057 Note : The three year certified recommendations discussed here were deposited for public access with UCalIrvine machine learning repository . 7 . REFERENCES [ 1 ] Brooks , J . S . et al . 2006 . Testing Hypotheses for the Success of Different Conservation Strategies . Conservation Biology , 20 , 5 ( Oct . 2006 ) , 1528 1538 . DOI=101111/j15231739200600506
[ 2 ] Canadi , I . , Barford , P . and Sommers , J . 2012 . Revisiting broadband performance . In IMC '12 Proceedings of the 2012 ACM conference on Internet measurement conference . ACM New York , NY ( 2012 ) , 273 286 .
[ 3 ] Changa , R . M . , Kauffman , R . J . and Kwonc , Y . 2014 .
Understanding the paradigm shift to computational social science in the presence of big data . Decision Support Systems , 63 ( Jul . 2014 ) , 67 80 . DOI=101016/jdss201308008
[ 4 ] https://wwwdigistampcom/authority/evidence/ ( Feb . 2015 ) . [ 5 ] Driscoll , K . and Walker , S . 2014 . Big Data , Big Questions
Working Within a Black Box : Transparency in the Collection and Production of Big Twitter Data . Int'l J . of Communication , 8 ( 2014 ) .
[ 6 ] Han , J . and Kamber , M . 2011 . Data Mining : Concepts and
Techniques , 3d Ed . Morgan Kaufmann ( 2011 ) .
[ 7 ] Heid , I . M . et al . 2009 . Meta Analysis of the INSIG2
Association with Obesity Including 74,345 Individuals : Does Heterogeneity of Estimates Relate to Study Design ? ( 2009 ) PLoS Genet . 5 , 10 ( Oct . 2009 ) PMCID : PMC2757909 DOI=101371/journalpgen1000694
[ 8 ] Hine , D . 2007 . Change in a dynamic climate : a single longitudinal case study in a high technology industry . In Innovative Methodologies in Enterprise Research . Edward Elgar Pub . ( 2007 ) .
[ 9 ] Hirst , D . E . , Koonce , L . and Miller , J . 1999 . The Joint Effect of Management's Prior Forecast Accuracy and the Form of Its Financial Forecasts on Investor Judgment . J . Accounting Research , 37 , Studies on Credible Financial Reporting ( 1999 ) , 101 124 .
[ 10 ] Hu , R . , and Watt , S . M . 2014 . An Agent Based Financial Market Simulator for Evaluation of Algorithmic Trading Strategies . In 6th International Conference on Advances in System Simulation , ( 2014 ) , 221 227 .
[ 11 ] Kais , M . et al . 2006 . A Multi Sensor Acquisition Architecture and Real Time Reference for Sensor and Fusion Methods Benchmarking . Intelligent Vehicles Symposium , 2006 . IEEE ( Tokoyo ) , 418 423 . DOI=101109/IVS20061689664
[ 12 ] Knox , S . and Walker , D . 2010 . Empirical developments in the measurement of involvement , brand loyalty and their relationship in grocery markets . J . Strategic Marketing , ( May 2010 ) , 271 286 . DOI= 101080/0965254032000159072
[ 13 ] Krahnena , J . P . and Weber , M . 2001 . Generally accepted rating principles : A primer . J . Banking Finance 25 , 1 ( Jan . 2001 ) , 323 .
[ 14 ] Lebiere , C . , Gonzalez , C . and Warwick , W . 2009 . A
Comparative Approach to Understanding General Intelligence : Predicting Cognitive Performance in an Open ended Dynamic Task . In Proceedings of the Second Conference on Artificial General Intelligence . Atlantis Press ( 2009 ) . DOI= 102991/agi20092
[ 15 ] Li , Z . and Li , Z . 2015 . Optimal robust optimization approximation for chance constrained optimization problem . Computers & Chemical Engineering , 74 , 4 ( Mar . 2015 ) , 8999 .
[ 16 ] Lowenstein , R . 2000 . When Genius Failed : the Rise and Fall of Long Term Capital Management . Random House Trade Paperbacks , 95–97 . ISBN 978 0 375 75825 6 .
[ 17 ] Magnusson , D . and Bergman , L . R . 1990 . Data Quality in
Longitudinal Research . Cambridge U . Press ( 1990 ) .
[ 18 ] Olden , J . D . and Jackson , D . A . 2002 . Illuminating the “ black box ” : a randomization approach for understanding variable contributions in artificial neural networks . Ecological Modelling , 154 , 1–2 , ( Aug . 2002 ) , 135–150 .
[ 19 ] Terry McElrath , YM et al . 2011 . Effects of tobacco related media campaigns on smoking among 20–30 year old adults : longitudinal data from the USA . Tobacco Control ( 2011 ) . DOI=101136/tobaccocontrol 2011 050208
[ 20 ] Menon , A . , et al . 1999 . Antecedents and consequences of marketing strategy making : a model and a test . J . Marketing ( 1999 ) .
[ 21 ] Pavlou , P . A . and Sawy , O . A . 2011 . Understanding the Elusive Black Box of Dynamic Capabilities . Decision Sciences , 42 , 1 ( Feb . 2011 ) , 239 273 . DOI= 101111/j15405915201000287x
[ 22 ] Redline , S . , Dean , D . , and Sanders , M . H . 2013 . Entering the
Era of “ Big Data ” : Getting Our Metrics Right . Sleep 36 , 4 ( Apr 1 , 2013 ) , 465 469 . PMCID : PMC3612262 DOI= 105665/sleep2524
[ 23 ] Sen , K . , Viswanathan , M . and Agha , G . 2004 . Statistical Model
Checking of Black Box Probabilistic Systems . Computer Aided Verification : Lecture Notes in Computer Science , 3114 ( Springer Link 2004 ) , 202 215 .
[ 24 ] Sterman , J . D . 1987 . Testing Behavioral Simulation Models by Direct Experiment . Mgmt . Science , 33 , 12 ( Dec . 1987 ) , 15721592 . DOI=101287/mnsc33121572
[ 25 ] Varian , Hal R . 2014 . Big Data : New Tricks for Econometrics .
J . Economic Perspectives , 28 , 2 ( Spring 2014 ) , 3 .
[ 26 ] Warbrick Smith , J . et al . 2009 . Three hundred and fifty generations of extreme food specialisation : testing predictions of nutritional ecology . Entomologia Experimentalis et Applicata , 132 , 1 ( Jul . 2009 ) , 65 75 . DOI=101111/j15707458200900870
[ 27 ] Weiss , R . E . 2005 . Modeling Longitudinal Data : With 72
Figures . Springer ( 2005 ) .
2058
