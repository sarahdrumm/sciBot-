L∞ Error and Bandwidth Selection for Kernel Density
Estimates of Large Data
Yan Zheng , Jeff M . Phillips
School of Computing , University of Utah
Salt Lake City , UT , USA yanzheng@csutahedu , jeffp@csutahedu
ABSTRACT Kernel density estimates are a robust way to reconstruct a continuous distribution from a discrete point set . Typically their effectiveness is measured either in L1 or L2 error . In this paper we investigate the challenges in using L∞ ( or worst case ) error , a stronger measure than L1 or L2 . We present efficient solutions to two linked challenges : how to evaluate the L∞ error between two kernel density estimates and how to choose the bandwidth parameter for a kernel density estimate built on a subsample of a large data set . 1
1 .
INTRODUCTION
Kernel density estimates ( kdes ) are essential tools [ 33 , 31 , 11 , 12 ] for understanding a continuous distribution represented by a finite set of points . For instance , kdes are used in data mining amid uncertainty to provide an effective intermediate representation , which captures information about the noise in the underlying data [ 2 ] . They are also used in classification problems by constructing the class of conditional probability density functions that are used in a Bayesian classifier [ 25 ] . They have many applications in other areas , such as network outlier detection [ 8 ] , human motion tracking [ 6 ] , financial data modeling [ 3 ] and geometric inference [ 27 ] . Given a point set P ⊂ Rd and a kernel Kσ : Rd×Rd → R+ with bandwidth parameter σ , for any point x ∈ Rd , a kernel density estimate is defined as kdeP ( x ) = 1|P| p∈P Kσ(p , x ) . We focus on symmetric , shift invariant kernels which depend only on z = p − x and σ , then a kernel can be written as function Kσ(p , x ) = kσ(p − x ) = kσ(z ) . Intuitively , kdeP ( x ) smoothes the effect of each p ∈ P for the evaluation point x . For d = 1 this object can be used in place of an equi width histogram ; it removes the choice of how to shift the boundary of bins and thus kdes are more robust . Moreover , they generalize naturally to higher dimensions .
1Thanks to NSF Awards 1350888 , 1251019 , and 1443046 .
The brute force solution of evaluating a kernel density estimate requires O(|P| ) time , and is thus untenable as a data structure for large data sets . And a lot research has gone towards speeding up these queries [ 7 , 37 , 9 , 40 ] . One of the techniques [ 40 ] is to produce a coreset representation Q of the data which can be used as proxy for the true data P while guaranteeing approximation error . The size of Q depends only on the required error , not on any properties of P ; these go beyond just randomly sampling Q from P . Written concretely , given P , and some error parameter ε > 0 , the goal is to construct a point set Q to ensure
L∞(P , Q ) = err(P , Q ) = max x∈Rd
|kdeP ( x ) − kdeQ(x)| ≤ ε , or written err(P , σ , Q , ω ) if the bandwidths σ and ω for kdeP,σ and kdeQ,ω are under consideration . This line of work shows that an L∞ error measure , compared with L1 or L2 error , is a more natural way to assess various properties about kernel density estimates . This work ( like other work [ 7 , 37 , 9 ] ) assumes σ is given , and then implicitly also assumes ω = σ . In this paper , we will investigate choosing a bandwidth ω for kdeQ under L∞ error given P , σ , Q .
Thus , we empirically study two concrete problems : 1 . Given two point sets P , Q ⊂ Rd and a kernel K , esti mate err(P , Q ) .
2 . Given two point sets P , Q ⊂ Rd , a kernel K , and a bandwidth σ , estimate ω = arg minω err(P , σ , Q , ω ) .
It should be apparent that the first problem is a key subproblem for the second , but it is also quite interesting in its own right . We will observe that L∞ is a strictly stronger measure than L1 or L2 , yet can still be assessed . To the best of our knowledge , we provide the first rigorous empirical study of how to measure this L∞ error in practice in an efficient way , following theoretical investigations demonstrating it should be possible .
Bandwidth parameter is hugely important in the resulting kde , and hence , there have been a plethora of proposed approaches [ 33 , 31 , 11 , 12 , 24 , 34 , 17 , 28 , 4 , 32 , 18 , 29 , 14 , 30 , 21 , 36 , 16 , 23 , 39 , 10 , 20 , 19 ] to somehow automatically choose the “ correct ” value . These typically attempt to minimize the L2 [ 33 , 31 ] or L1 error [ 11 , 12 ] ( or less commonly other error measures [ 24 ] ) between kdeP and some unknown distribution µ that it is assumed P is randomly drawn from . Perhaps unsurprisingly , for such an abstract problem different methods produce wildly different results . In practice , many practitioners choose a bandwidth value in a ad hoc manner through visual inspection and domain knowledge .
1533 In this paper we argue that the choice of bandwidth should not be completely uniquely selected . Rather this value provides a choice of scale at which the data is inspected , and for some data sets there can be more than one correct choice depending on the goal . We demonstrate this on real and synthetic data in 1 and 2 dimensions . As an intuitive 1dimensional example , given temperature data collected from a weather station , there are very obvious modal trends at the scale of 1 day and at the scale of 1 year , and depending at which phenomenon one wishes to study , the bandwidth parameter should be chosen along the corresponding scale , so it is totally reasonable if we assume σ for kdeP is given . Via examinations of problem ( 2 ) , we observe that in some cases ( but not all ) , given P , Q , and σ , we can choose a new bandwidth ω ( with ω > σ ) so that err(P , σ , Q , ω ) is significantly smaller than the default err(P , σ , Q , σ ) . This corresponds with fine grained phenomenon disappearing with less data ( as |Q| < |P| ) , and has been prognosticated by theory about L2 [ 33 ] or L1 [ 11 ] error where the optimal bandwidth for kdeQ is a strictly shrinking function of |Q| . Yet , we urge more caution than this existing bandwidth theory indicates since we only observe this phenomenon in specific data sets with features present at different scales ( like the daily/yearly temperature data example in Section 23 ) Organization . Section 2 formalizes and further motivates the problem . Section 3 addresses problem ( 1 ) , and Section 4 problem ( 2 ) . Then Section 5 describes detailed experimental validations of our proposed approaches . Finally , Section 6 provides some concluding thoughts .
2 . BACKGROUND AND MOTIVATION
In addition to the symmetric , shift invariant properties of the kernels , it is convenient to enforce one of two other properties . A normalized kernel satisfies
Kσ(p , x)dx = 1 , x∈Rd so that the kernel and the kernel density estimate are probability distributions . A unit kernel satisfies
Kσ(x , x ) = 1 so that 0 ≤ Kσ(x , p ) ≤ 1 , which ensures that kdeP ( x ) ≤ 1 . Unlike with the normalized kernel , the changing of bandwidth does not affect the coefficient of kernel function , so Kσ(p , x ) = k(p − x/σ ) . σd(2π)d/2 exp(−p−x2/2σ2 ) , probably the most
Kσ(p , x ) = commonly used kernel , there are many other symmetric , shift invariant kernels such as
Although this paper focuses on the Gaussian kernel
1
σdcdd! exp(−x − p/σ ) ,
• Laplace Kernel : Kσ(p , x ) = 1
• Triangular Kernel : σdcd−1
Kσ(p , x ) =
• Epanechnikov Kernel : d
Kσ(p , x ) = d+2 2σdcd
• Ball Kernel : Kσ(p , x ) = { max{0 , 1 − x − p/σ} , max{0 , 1 − x − p2/σ2} , or
1
σdcd−1 if p − x ≤ σ ; ow 0} , d 2 where cd = rdπ Γ( d sphere . These are shown as normalized kernels , to make them unit kernels , the coefficient is simply set to 1 . is the volume of the unit d dimensional
2 +1 )
2.1 Unit Kernels or Normalized Kernels ?
Unit kernels are more natural to estimate the L∞ errors of kernel density estimates [ 26 , 38 ] since the range of values are in [ 0 , 1 ] . For normalized kernels as σ varies , the only bound in the range is [ 0,∞ ) .
Moreover , unit kernels , under a special case , correspond to the total variation distance of probability measures . In probability theory , the total variation distance for two probability measures P and Q on a sigma algebra F of subsets of sample space Ω is defined as :
δ(P , Q ) = sup A∈F
|P ( A ) − Q(A)| .
Terms P ( A ) , resp . Q(A ) , refer to the probability restricted to subset A . If we use F as the set of all balls of radius σ , so A is one such ball , then P ( A ) is the fraction of points of P falling in A . Hence P ( A ) can be viewed as the kdeP,σ(x ) under the ball kernel , where x is the center of ball A . When Q is the coreset of P , then Q(A ) is the fraction of points of Q falling in A , so it can be viewed as the kdeQ,σ(x ) under the ball kernel . In this sense , the total variance distance is the L∞ error , specifically err(P , Q ) where K is the ball kernel . The total variation distance also maps to other unit kernels if F can admit weighted subsets , not just subsets . However , normalized kernels are more useful in bandwidth selection . In this case , there is a finite value for σ ∈ ( 0,∞ ) which minimizes the L1 or L2 error between kdeP,σ and kdeQ,σ , whereas for unit kernels this is minimized for σ → 0 . But recall that unit and normalized kernels are only different in the scaling coefficient , so given one setting it is simple to convert to the other without changing the bandwidth . Hence we use both types of kernels in different scenarios : unit kernels for choosing the coresets , and normalized kernel for problem ( 1 ) and problem ( 2 ) . 2.2 Why Coresets ?
In the big data era , we are creating and accessing vastly more data than ever before . For example , mobile phones are consistently ( implicitly ) generating positional data along with various aspects of meta data including call duration and quality . To analyze or monitor the quality of signals or demand for this connections , we rarely need the entire data set , just an approximate version of it . A coreset can provide such a summary with accuracy guarantees , and by virtue of smaller size much more efficient and affordable access to it . More formally , a coreset of a point set P is a subset Q such that ( 1 ) one can perform a family of queries on Q instead of P and the returned results are guaranteed to have bounded errors , and ( 2 ) the size of Q is much smaller than P , often independent of the size of P and only depends on the guaranteed error on the queries . For this paper , we consider coresets which preserve properties about the kernel density estimate , namely that for any query point x that |kdeQ(x ) − kdeP ( x)| ≤ ε for some error parameter ε > 0 . The study of the worst case error was initiated by Phillips [ 26 ] , and similar results under the L2 error have been studied by Chen et al . [ 9 ] using an approach called kernel herding . Zheng etal [ 40 ] empirically improved these approaches to finding such a coreset in one and two dimensions , using methods based on random sampling , iteratively matching and halving of the data set , and Z order curves . For instance , the experiments in [ 40 ] show that in two dimension , a coreset of 10,000 points can be constructed in less than 5 seconds from a 160 million record data set with approximation ε = 001
1534 2.3 Why σ is given ?
Recall that problem ( 2 ) takes as given two point sets P and Q as well as a bandwidth σ associated with P , and then tries to find the best bandwidth ω for Q so that kdeP,σ is close to kdeQ,ω . This is different from how the “ bandwidth selection problem ” is typically posed [ 11 , 33 ] : a single point set Q is given with no bandwidth , and it is assumed that Q is drawn randomly from an unknown distribution .
We break from this formulation for two reasons . First , we often consider the point set Q chosen as a coreset from P , and this may not be randomly from P , as more intricate techniques [ 40 ] can obtain the same error with much smaller size sets Q . These non random samples break most modeling assumptions essential to the existing techniques .
Second , the choice of bandwidth may vary largely within the same data set , and these varied choices may each highlight a different aspect of the data . As an extended example consider temperature data ( here we treat a reading of 50 degrees as 50 data points at that time ) from a MesoWest weather station KSLC read every hour in all of 2012 . This results in 8760 total readings , illustrated in Figure 1 . For three bandwidth values of 3 , 72 , and 1440 , kdes are shown to represent daily , weekly , and yearly trends . All are useful representations of the data ; there is no “ one right bandwidth . ” Section 5 shows a 2 dimensional example of population densities where similarly there are several distinct reasonable choices of bandwidths .
Figure 1 : KDEs with different bandwidths showing daily , weekly and yearly temperature trends . Left shows the full year data , and right shows the one week data .
2.4 Why L∞ Error ? kdes are the L1 or L2 error , defined for p = {1 , 2} as
As mentioned the most common measures for comparing
Lp(P , Q ) = kdeP − kdeQp
= x∈Rd
|kdeP ( x ) − kdeQ(x)|p
1/p
.
Although this integral can be reasoned about , it is difficult to estimate precisely . Rather many techniques only evaluate at the points P and simply calculate
|kdeP ( p ) − kdeQ(p)|p1/p
.
1
|P| q∈P
These average over the domain or P ; hence if |kdeP ( x ) − kdeQ(x)| ≤ ε for all x , then Lp(P , Q ) is also at most ε . That “ for all ” bound is precisely what is guaranteed by L∞(P , Q ) , hence it is a stronger bound .
Another reason to study L∞ error is that it preserves the worst case error . This is particularly important when kdeP ( x ) values above a threshold trigger an alarm . For instance in tracking densities of tweets , too much activity in one location may indicate some event worth investigating . L1 or L2 error from a baseline may be small , but still have high error in one location either triggering a false alarm , or missing a real event . 2.5 Related Work on Bandwidth Selection
There is a vast literature on bandwidth selection under the L1 [ 11 , 12 ] or L2 [ 33 , 31 ] metric . In these settings Q is drawn , often at random , from an unknown continuous distribution µ ( but µ can be evaluated at any single point x ) . Then the goal is to choose ω to minimize µ− kdeQ,ω{1,2} . This can be conceptualized in two steps as µ−kdeµ,ω and kdeµ,ω − kdeQ,ω . The first step is minimized as ω → 0 and the second step as ω → ∞ . Together , there is a value ω{1,2} ∈ ( 0,∞ ) that minimizes the overall objective . tegrated Squared Errors(ISE ) ISE(ω ) = Error ( MISE ) M ISE(ω ) = EQ∼µ[
The most common error measure for ω under L2 are Inx∈Rd ( kdeQ,ω − µ)2dx and its expected value , the Mean Integrated Squared x∈Rd ( kdeQ,ω − µ)2dx ] .
As MISE is not mathematically tractable , often approximations such as the Asymptotic Mean Integrated Squared Error ( AMISE ) or others [ 33 , 34 ] are used . Cross validation techniques [ 17 , 28 , 4 , 32 , 29 , 14 ] are used to evaluate various parameters in these approximations . Alternatively , plug in methods [ 30 , 21 , 36 ] recursively build approximations to µ using kdeQ,ωi , and then refine the estimate of ωi+1 using kdeQ,ωi . Bayesian approaches [ 5 , 16 , 23 , 39 , 10 , 20 ] build on these models and select ω using MCMC approaches .
IAE(ω ) =
An alternative to these L2 approaches is using an L1 measure , like integrated absolute error ( IAE ) of kdeQ,ω is x∈Rd |kdeQ,ω − µ|dx , which has simple inter pretation of being the area between the two functions . Devroye and Gy¨orfi [ 11 ] describe several robustness advantages ( better tail behavior , transformation invariance ) to these approaches . Several of the approximation approaches from L2 can be extended to L1 [ 19 ] .
Perplexingly , however , the bandwidths generated by these methods can vary quite drastically! In this paper , we assume that some bandwidth is given to indicate the intended scale , and then we choose a bandwidth for a sparser point set . Hence the methods surveyed above are not directly comparable to our proposed approaches . We include the experiment results from some of the above methods to show that different approaches give quite different “ optimal ” bandwidth , which in another way shows us there are more than one correct bandwidth for some data sets .
3 . COMPUTING err(P , Q )
The goal of this section is to calculate err(P , Q ) = max x∈Rd
|kdeP ( x ) − kdeQ(x)| .
For notational convenience let G(x ) = |kdeP ( x)−kdeQ(x)| . We focus on the case where the kernel K is a unit Gaussian . Since even calculating maxx∈Rd kdeP ( x ) ( which is a special case of err(P , Q ) where Q is empty ) appears hard , and only constant factor approximations are known [ 1 , 27 ] , we will not calculate err(P , Q ) exactly . Unfortunately these approximation techniques [ 1 , 27 ] for maxx∈Rd kdeP ( x ) do not easily extend to estimating err(P , Q ) . They can focus on dense areas of P , since the maximum must occur there , but in
1535 err(P , Q ) , these dense areas may perfectly cancel out . These approaches are also much more involved than the strategies we will explore . 3.1 Approximation Strategy Towards estimating err(P , Q ) , which is optimized over all of Rd , our strategy is to generate a finite set X ⊂ Rd , and then return errX ( P , Q ) = maxx∈X G(x ) . Our goal in the generation of X is so that in practice our returned estimate errX ( P , Q ) is close to err(P , Q ) , but also so that under this process as |X| → ∞ then formally errX ( P , Q ) → err(P , Q ) . We say such a process converges . We formalize this in two steps . First we show G(x ) is Lipschitz continuous , hence a point ˆx ∈ Rd close to the point x∗ = arg maxx∈Rd G(x ) will also have error close to x∗ . Then given this fact , we show that our strategy will , for any radius r , as |X| → ∞ generate a point ˆx ∈ X so that x∗ − ˆx ≤ r . This will be aided by the following structural theorem on the location of x∗ , with proofs in 1 and 2 dimensions deferred to Appendix A . ( M is illustrated in Figure 2 . )
Theorem 1 . For Kσ a unit Gaussian kernel , and two point sets P , Q ∈ Rd , then x∗ = arg maxx∈Rd G(x ) must be in M , the Minkowski sum of a ball of radius σ and the convex hull of P ∪ Q .
Figure 2 : Illustration of the Minkowski sum of a ball of radius σ and convex hull of P ∪ Q .
We will not focus on proving theoretical bounds on the rate of convergence of these processes since they are quite data dependent , but will thoroughly empirically explore this rate in Section 5 . As |X| grows , the max error value in X will consistently approach some error value ( the same value for several provably converging approaches ) , and we can then have some confidence that as these processes plateau , they have successfully estimated err(P , Q ) . Our best pro|X| = 100 ) ; it is likely cess WCen6 converges quickly ( eg that the maximum error is approximately achieved in many locations . Now as a basis for formalizing these results we first show G(x ) is Lipschitz continuous . Recall a function f : Rd → R is Lipschitz continuous if there exists some constant β such that for any two points x , y ∈ Rd that |f ( x ) − f ( y)|/x − y ≤ β . This result follows from the Gaussian kernel ( as well as all other kernels mentioned in Section 1 except the Ball kernel ) also being Lipschitz continuous . Then since the function f ( x ) = kdeP ( x)−kdeQ(x ) is a finite weighted sum of Gaussian kernels , each of which is Lipschitz continuous , so is f ( x ) . Since taking absolute value does not affect Lipschitz continuity , the claim holds .
3.2 Generation of Evaluation Points
We now consider strategies to generate a set of points X so that errX ( P , Q ) is close to err(P , Q ) . Recall that M , the Minkowski sum of a ball of radius σ with the convex hull of P ∪ Q must contain the point x∗ which results in err(P , Q ) . In practice , it is typically easier to use B , the smallest axisaligned bounding box that contains M . For discussion we assume Q ⊂ P so P = P ∪ Q . Rand : Choose each point uniformly at random from B . close enough to x∗ , and this process converges . Orgp : Choose points uniformly at random from P .
Since x∗ ∈ M ⊂ B , eventually some point x ∈ X will be
This process does not converge since the maximum error point may not be in P . Yet Section 5 shows that this process converges to its limit very quickly . So many of the following proposed approaches will attempt to adapt this approach while still converging . Orgp+N : Choose points randomly from the original point set P then add Gaussian noise with bandwidth σ , where σ is the bandwidth of K .
Since the Gaussian has infinite support , points in X can be anywhere in Rd , and will eventually become close enough to x∗ . So this process converges . Grid : Place a uniform grid on B ( we assume each grid cell is a square ) and choose one point in each grid . For example in 2 dimension , if four evaluation points are needed , the grid would be 2 × 2 and if nine points are needed , it would be 3× 3 . So with this method , the number of evaluation points is a non prime integer . Since x∗ ∈ B , and eventually the grid cell radius is arbitrarily small , then some point x ∈ X is close enough to x∗ . Thus this process converges . Cen{E[m]} : Randomly select one point p1 from the original point set P and randomly choose m neighbor points of p1 within the distance of 3σ . m is chosen through a Exponential process with rate 1/E[m ] . Then we use the centroid of the selected neighbor points as the evaluation point . This method is inspired by [ 15 ] , which demonstrates interesting maximums of kdes at the centroids of the data points .
Since P is fixed , the centroid of any combination of points in P is also finite , and the set of these centroids may not include x∗ . So this process does not converge . We next modify it in a way so it does converge . WCen{E[m]} : Randomly select one point p1 from the original point set P and select the neighbor point pn ∈ P as candidate neighbor proportional to exp(− ||pn−p1||2 ) , where σ is the bandwidth for K . The smaller the distance between pn and p1 , the higher probability it will be the chosen . Repeat to choose m total points including p1 , where again m is from an Exponential process with rate 1/E[m ] . Now refine the m neighbor points so with probability 0.9 , it remains the original point pn ∈ P , with the remaining probability it is chosen randomly from a ball of radius σ centered at pn . Next , we assign each point a random weight in [ 0 , 1 ] so that all weights add to 1 . Then finally the evaluation point is the weighted centroid of these points .
2σ2
This method retains much of the effectiveness of Cen , but does converge . Without the 0.1 probability rule of being in a ball of radius σ around each point , this method can generate any points within the convex hull of P . That 0.1 probability allows it to expand to M , the Minkowski sum of the convex hull of P with a ball of radius σ . Since x∗ ∈ M , by Theorem 1 , this process converges .
 p11536 is a maximization over x ∈ Rd . There may in fact be more than one local minimum for ω in h(ω ) .
However , equipped with the WCen6 procedure to evaluate err(P , Q ) , we propose a relatively simple optimization algorithm . We can perform a golden section search over ω , using WCen6 to obtain a set X and evaluate errX ( P , σ , Q , ω ) . Such a search procedure requires a convex function for any sort of guarantees , and this property may not hold . However , we show next that h(ω ) has some restricted Lipschitz property , so that with random restarts it should be able to find reasonable local minimum . This is illustrated in Figure 4 , where the curve that is Lipschitz either has a large , relatively convex region around the global minimum , or has shallow local minimums . The other curve without a Lipschitz property has a very small convex region around the global minimum , and any search procedure will have a hard time finding it .
Figure 4 : Two curves , dark one is Lipschitz , dashed curve is not . 4.2 Lipschitz Properties of h
In general , however , h(ω ) is not Lipschitz in ω . But , we can show it is Lipschitz over a restricted domain , specifically when ω ≥ σ ≥ 1/A for some absolute constant A . Define y(ω , a ) = 1
2πω2 exp(−a2/(2ω2) ) .
Lemma 1 . For any ω ≥ σ ≥ 1/A , y(ω , a ) is β Lipschitz with respect to ω , with β = |a2 − 1/π|A3 .
Proof . By taking the first derivative of y(ω ) , we have dy(ω , a ) dω
And thusfifififi dy(ω , a ) dω
)ω
= ( a2 − 1 π fifififi = |a2 − 1/π|ω
≤ |a2 − 1/π|σ
−3 exp(−a2/(2ω2) ) .
−3 exp(−a2/(2ω2 ) ) −3 ≤ |a2 − 1/π|A3 .
So the absolute value of largest slope of function y(ω , a ) is β = |a2 − 1/π|A3 , thus y(ω , a ) is β Lipschitz continuous on ω .
Theorem 2 . For any ω ≥ σ ≥ 1/A , h(ω ) is β Lipschitz q∈Q |(x∗ − q)2 − 1/π|A3 with respect to ω , for β = 1|Q| where x∗ = arg maxx∈R2 |kdeP,σ(x ) − kdeQ,ω(x)| .
Proof . If kdeP,σ(x∗ ) ≥ kdeQ,ω(x∗ ) then h(ω ) = |kdeP,σ(x ∗ ∗ = kdeP,σ(x
)| 1
2πω2 exp
−(x∗ − q)2
2ω2
) − kdeQ,ω(x ∗ ) − 1 |Q| ) − 1 |Q| q∈Q q∈Q
∗ = kdeP,σ(x y(ω , ( x
∗ − q)) ) .
( a ) Original dataset .
( b ) Coreset .
Figure 3 : The example of necessary of larger bandwidth for coreset Q . The radius of the circle represents the chosen bandwidth .
Comb : Rand + Orgp : The combination of method Rand and Orgp , of which 20 % points generated from B and 80 % points generated from original points .
The 20 % of points from Rand guarantees convergence , but retain most empirical properties of Orgp . This was used before with little discussion [ 40 ] .
Section 5 describes extensive experiments on both synthetic and real data to evaluate these methods . The weighted centroid method WCen{E[m]} with large parameter ( eg E[m ] = 6 ) works very well for 1 and 2 dimensions , and also converges , so in general this technique is recommended . Although in some situations , it does not perform significantly better than other approaches like Rand+Orgp , which are simpler and also converge , so those may be a good option .
4 . BANDWIDTH SELECTION In this section we consider being given two point sets P , Q ⊂ R2 , a kernel K , and a bandwidth σ associated with P . We consider K as a normalized Gaussian kernel , and where Q is a coreset of P . The goal is to find another bandwidth ω to associate with Q so that err(P , σ , Q , ω ) is small . 4.1 Refining the Bandwidth for Coresets
In [ 40 ] , coresets are constructed assuming that kdeQ uses the same bandwidth σ as kdeP . Can we improve this relationship by using a different bandwidth ω for Q ? The theory for L1 or L2 error ( assuming Q is a random sample from P ) dictates that as |Q| decreases , the bandwidth ω should increase . This intuition holds under any error measure since with fewer data points , the kde should have less resolution . It also matches the L∞ theoretical error bounds described previously [ 26 ] . We first reinforce this with a simple 2 dimensional example . Consider point set P = ∪{P1 , P2 , P3 , P4} in Figure 3(a ) , the radius of the circle represents the bandwidth σ for P . Figure 3(b ) gives the coreset Q of P : Q = ∪{Q1 , Q2 , Q3 , Q4} , each Qi contains only one black point . Now suppose our evaluation point is point e . If we use the original bandwidth σ , kdeQ,σ(e ) = 0 with ball kernel , but if we use ω , which is the radius of larger circle centered at e , then kdeQ,ω(e ) > 0 , so the error is decreased . But , we don’t want ω too large , as it would reach the points in other Qi , which is not the case for σ in P , so the error would be increased again . Thus there seems to be a good choice for ω > σ .
But the situation of finding the ωopt that minimizes h(ω ) = err(P , σ , Q , ω ) is more complicated . For each ω , err(P , σ , Q , ω )
P1P2P3P4eQ1Q2Q3Q4e C2C1!opt!1537 Since h(ω ) is linear combination of |Q| functions of y(ω , a ) plus a constant and y(ω , a ) is Lipschitz continuous , based on the Lemma 1 , h(ω ) is Lipschitz continuous on ω . We can get the same result if kdeP,σ(x∗ ) ≤ kdeQ,ω(x∗ ) . In both directions , the first derivative of the function is bounded , so h(ω ) is bounded . 4.3 Random Golden Section Search
From the above properties , we design a search procedure that will be effective in finding the bandwidth ω minimizing err(P , σ , Q , ω ) . The random golden section search is based on the golden section search [ 22 ] , a technique to find extremum in a strictly unimodal function . To find a minimum , it successively narrows a range [ , r ] with known function values h( ) , h(m1 ) , h(m2 ) , and h(r ) with < m1 < m2 < r and with both h(m1 ) , h(m2 ) less than h( ) and h(r ) . If h(m1 ) < h(m2 ) the new search range is [ , m2 ] and otherwise it is [ m1 , r ] . In either case a new fourth point is chosen according to the golden ratio in such a way that the interval shrinks by a constant factor on each step .
However , h(ω ) in our case can be a multi modal function , thus golden section search is not guaranteed to work . We apply random restarts as follows . Starting with a range [ = σ , r = 10σ ] we choose one middle point at m = λσ for λ ∼ Unif(1 , 10 ) . If h(m ) > h(r ) we increase r by a factor 10 until it is ( eg r = 100σ ) . Then the second middle point is chosen using the golden ratio , and the search is run deterministically . We repeat with several random values λ .
5 . EXPERIMENTS
Here we run an extensive set of experiments to validate our techniques . We compare kdeP where P is in 1 and 2 dimensions with kernel density estimate under smaller coreset kdeQ for both synthetic and real data . To show our methods work well in large data sets , we use the large synthetic data set(0.5 million ) and real data set(1 million ) in 2 dimension . 5.1 Data Sets
We consider data sets that have different features at various scales , so that as more data is present using a smaller bandwidth more fine grain features are brought out , and a larger bandwidth only shows the coarse features . Our real data set in 1 dimension is the temperature data in Figure 1 , with default σ = 72 ( 3 days ) . We use parameter ε = 0.02 to generate a coreset Q with the Sort selection technique [ 40 ] . We can also simulate data with multi scale features . On a domain [ 0 , 1 ] we generate P recursively , starting with p1 = 0 and p2 = 1 . Next we consider the interval between [ p1 , p2 ] and insert two points at p3 = 2/5 and p4 = 3/5 . There are now 3 intervals [ p1 , p3 ] , [ p3 , p4 ] , and [ p4 , p2 ] . For each interval [ pi , pj ] we recursively insert 2 new points at pi + ( 2/5)·(pj −pi ) and at pi +(3/5)·(pj −pi ) , until |P| = 19684 . The kde of this data set with σ = 0.01 is shown in Figure 5(d ) , along with that of a coreset Q of size |Q| = 100 .
We construct the 2 dimensional synthetic data set in a similar way . The data is in [ 0 , 1]2 starting with four points p1 = ( 0 , 0 ) , p2 = ( 0 , 1 ) , p3 = ( 1 , 0 ) , p4 = ( 1 , 1 ) . We recurse on this rectangle by adding 4 new point in the middle m : the x coordinates are either at the 0.5 quantile or 0.8 quantile of the x coordinates , and same for new y coordinates . These 4 new points creates 9 smaller empty rectangles . We further recurse on each of these rectangles until |P| = 532900 . The kdeP with σ = 0.01 is shown in Figure 10(a ) . We use Grid matching [ 40 ] to generate a coreset Q with ε = 0.1 and size |Q| = 1040 . Under the original bandwidth σ , the kdeQ is shown in Figure 10(b ) ; due to a small bandwidth this kde has many more modes than the original , which motivates the larger bandwidth kde shown in Figure 10(c ) .
For real data with multiple scales in 2 dimension we consider OpenStreetMap data from the state of Iowa . Specifically , we use the longitude and latitude of all highway data points , then rescale so it lies in [ 0 , 1 ] × [ 0 , 1 ] . It was recognized in the early 1900s [ 35 ] that agricultural populations , such as Iowa , exhibited population densities at several scales . In experiment , we use the original data of size |P| = 1155102 with σ = 0.01 , and Q as a smaller coreset with ε = 0.1 and |Q| = 1128 . These are illustrated in Figure 11 . 5.2 Evaluating Point Generation for errX(P , Q )
To find the best evaluation point generation techniques , we compare the various ways to generate a set X to evaluate errX ( P , Q ) . The larger numbers are better , so we want to find point sets X so that errX ( P , Q ) is maximized with |X| small . As most of our methods are random , five evaluation point sets are generated for each method and the average errX ( P , Q ) is considered .
We start in 1 dimension , and investigate which parameter of the Cen and WCen methods work best . We will then compare the best in class against the remaining approaches . Recall the parameter E[m ] determines the expected number of points ( under a Exponential process ) chosen to take the centroid or weighted centroid of , respectively . We only show the test result with E[m ] from 2 to 7 , since the results are similar when E[m ] is larger than 7 , and the larger the parameter the slower ( and less desirable ) the process . The results are plotted in Figure 5 on the 1 dimensional synthetic data . Specifically , Figure 5(a ) shows the Cen method and Figure 5(b ) the WCen method . Both methods plateau , for some parameter setting , after around |X| = 100 , with WCen more robust to parameter choice . In particular WCen converges
( a ) Centroid methods
( b ) Weighted centroid meth
( c ) All methods
( d ) kdeP,σ and kdeQ,ω with original and best bandwidth
Figure 5 : Choosing best evaluating point generation techniques for 1 dimensional synthetic data .
1538 ( a ) Centroid methods
( b ) Weighted centroid meth
( a ) Centroid methods
( b ) Centroid methods
( c ) All methods
( d ) kdeP,σ and kdeQ,ω with original and best bandwidth Figure 6 : Choosing best evaluating point generation techniques for 1 dimensional real data . slightly faster but with not much pattern across the choice of parameter . We use Cen6 and WCen6 as representatives . We next compare these approaches directly against each other as well as Rand , Orgp , Orgp+N , Grid , and Comb in Figure 5(c ) . WCen6 appears the best in this experiment , but it has been selected as best WCen technique from random trials . The Rand and Grid techniques which also converge perform well , and are simpler to implement .
Similar results are seen on the real 1 dimensional data in Figure 6 . We can take best in class from Cen and WCen parameter choices , shown as Cen6 and WCen6 in Figure 6(a ) and Figure 6(b ) . These perform well and similar to the simpler Rand , Grid , and Orgp in Figure 6(c ) . Since Rand and Grid also converge , in 1 dimension we would recommend one of these simple methods .
For 2 dimensional data , the techniques perform a bit differently . We again start with Cen and WCen methods as shown in Figure 7 on real and synthetic data . The convergence results are not as good as in 1 dimension , as expected , and it takes roughly |X| = 10000 points to converge . All methods perform roughly the same for various parameter settings , so we use Cen6 and WCen6 as representatives . Comparing against all techniques in Figure 7(e ) , most techniques perform roughly the same relative to each other , and again WCen6 appears to be a good choice to use . The notable exceptions is that Grid and Rand perform worse in 2 d than in 1 d ; likely indicating that the data dependent approaches are more important in this setting . 5.3 Choosing New Bandwidth Evaluation
We now apply a random golden section search to find new bandwidth values for coresets on 1 dimensional and 2dimensional synthetic and real data . In all 10 random trials we always find the same local minimum , and report this value . We will see that a value ω > σ can often give better error results , both visually and empirically , by smoothing out the noise from the smaller coresets .
Figure 8 shows evaluation of errX ( P , σ , Q , ω ) for various ω values chosen while running the random golden section search on synthetic and real 1 dimensional data . In both
( c ) Weighted centroid meth ( d ) Weighted centroid meth
( e ) All methods
( f ) All methods
Figure 7 : Choosing the best evaluation set X for 2 dimensional synthetic ( left ) and real ( right ) data .
( a ) Synthetic data Figure 8 : ω∗ = arg minω errX ( P , σ , Q , ω ) in R1 .
( b ) Real data cases , setting ω = σ ( as ω = 0.01 and ω = 72 , respectively ) gives roughly twice as much error as using an omega roughly twice as large ( ω = 0.017 and ω = 142 , respectively ) .
We can see the same results in 2 dimensional data sets in Figure 9 . We observe in Figure 9(a ) on synthetic data that with the original ω = σ = 0.01 the error is roughly 3.6 , but by choosing ω = 0.013 that we can reduce the error to roughly 27 This is also shown visually in Figure 10 , where a small coreset Q is chosen to do kdeQ,σ ( Figure 10(b ) ) and the large scale pattern in kdeP,σ is replaced by many isolated points ; kdeQ,ω=0.013 ( Figure 10(c ) ) increases the bandwidth and the desired visual pattern re emerges . On real data , a similar pattern is seen in Figure 9(b ) . The original ω = σ = 0.01 has error roughly 3.0 , and an ω = 0.024 ( more than 2 times larger ) gives error about 11 This extra smoothing is illustrated in Figure 11 .
Thus we see that it is indeed useful to increase the bandwidth of kernel density estimates on a coreset , even though theoretical bounds already hold for using the same bandwidth . We show that doing so can decrease the error by
1539 ( a ) Synthetic data Figure 9 : ω∗ = arg minω expX ( P , σ , Q , ω ) in R2 .
( b ) Real data
( a ) Synthetic data
( b ) Real data
Figure 12 : Relations of L1 , L2 and L∞ error and ω
( a ) kdeP,σ=001
( b ) kdeQ,ω=001
( c ) kdeQ,ω=0013 Figure 10 : Visualization of KDEP and KDEQ for 2 dimensional synthetic data using different bandwidth .
( a ) kdeP,σ=001
( c ) kdeQ,ω=0024 Figure 11 : Visualization of KDEP and KDEQ for 2 dimensional real data using different bandwidth .
( b ) kdeQ,ω=001
We consider the following exemplars , among those surveyed in Section 2.5 : biased cross validation ( BCV ) , least squares cross validation ( LSCV ) , plug in ( PI ) , and smoothed crossvalidation ( SCV ) . We use the kernel smoothing R package ( ks ) , which was originally introduced by Duong in 2007 [ 13 ] and improved in 2014 . In the experiment , our data set is normalized and we assume data in each dimension is independent and share the same bandwidth ; so we use the largest value from the main diagonal of bandwidth matrix computed from the R package . For the 2 dimensional synthetic data set , we use the same coreset with |Q| = 1040 . The four exemplar methods , respectively , resulted in the following bandwidths ωBCV = 0.0085 , ωLSCV = 0.024 , ωP I = 0.0036 , and ωSCV = 00043 For the 2 dimensional real data set , with the coreset |Q| = 1128 , the bandwidth chosen by the four exemplar methods , respectively , are ωBCV = 0.0078 , ωLSCV = 0.0003 , ωP I = 0.0029 , ωSCV = 0004 The corresponding error trends compared to our method for these two data sets are shown in Figure 13 , where ωOP T denotes the optimal bandwidth from our method . Both of these figures show our method achieves the smallest error compared , and sometimes it is much ( a factor of 20 ) smaller . a factor of 2 or more . Since we consider ω = σ , and only decrease the error in the process , we can claim the same theoretical bounds for the new ω value . It is an open question of whether one can prove tighter coreset bounds by adapting the bandwidth value . 5.4 New Bandwidth for L1 and L2 Error
The above bandwidth selection method can be extended to minimizing the L1 and L2 errors . Differing from L∞ error , the L1 and L2 errors do not require finding a witness point of large error , but rather are the averaged over a region or , more commonly , the input points P . Figure 12 shows the L1 , L2 , and L∞ errors for 2 dimensional synthetic and real data ; other settings gave similar results . The results show that minimizing L∞ does not give significantly worse errors than minimizing L1 or L2 in our setting . For example , in Figure 12(a ) , we see that the choice of ω = 0.013 minimizes L∞ errors , ω = 0.014 gave a minimum L2 error of 0.608 and ω = 0.016 minimizes L1 error of 0450 Comparing instead to ω = 0.013 which provided the minimum L∞ error , then we get L2 error of 0.618 and L1 error of 0.476 ; both are within 1 % of the minimum solutions . 5.5 Comparing Bandwidth Selection Methods We compare against some traditional bandwidth selection methods for the 2 dimensional synthetic and real data using .
( a ) Synthetic data
( b ) Real data
Figure 13 : ω∗ = arg minω expX ( P , σ , Q , ω ) in R2 .
6 . CONCLUSION
This paper considers evaluating kernel density estimates under L∞ error , and how to use these criteria to select the bandwidth of a coreset . The L∞ error is stronger than the more traditional L1 or L2 error , it provides approximation guarantees for all points in the domain , and it aligns with recent theoretical results [ 26 ] of kernel range space . Thus it is worth rigorously investigating , and this paper presents the first such study .
We propose several methods to efficiently evaluate the L∞ error between two kernel density estimates and provide a convergence guarantee . The method Grid works well , and is very simple to implement in R1 . In R2 , methods that adapt more to the data perform much better , and our technique
1540 WCen is shown accurate and efficient on real and synthetic data . We then use these technique to select a new bandwidth value for coresets which can improve the error by a factor of 2 to 3 . We demonstrate this both visually and empirically on real and synthetic large data sets .
7 . REFERENCES [ 1 ] P . K . Agarwal , S . Har Peled , H . Kaplan , and
M . Sharir . Union of random minkowski sums and network vulnerability analysis . In SOCG , 2013 .
[ 2 ] C . C . Aggarwal . On density based transforms for uncertain data mining . In ICDE , 2007 .
[ 3 ] T . Bouezmarni and J . V . Rombouts . Nonparametric density estimation for multivariate bounded data . J . Statistical Planning and Inference , 140:139–152 , 2010 .
[ 4 ] A . W . Bowman . An alternative method of cross validation for the smoothing of density estimates . Biometrika , 71(2):353–360 , 1984 .
[ 5 ] M . J . Brewer . A bayesian model for local smoothing in kernel density estimation . Statistics and Computing , 10(4):299–309 , 2000 .
[ 6 ] T . Brox , B . Rosenhahn , D . Cremers , and H P Seidel .
Nonparametric density estimation with adaptive , anisotropic kernels for human motion tracking . In Human Motion–Understanding , Modeling , Capture and Animation , pages 152–165 . Springer , 2007 .
[ 7 ] P . B . Callahan and S . R . Kosaraju . Algorithms for dynamic closest pair and n body potential fields . In SODA , 1995 .
[ 8 ] Y . Cao , H . He , H . Man , and X . Shen . Integration of self organizing map ( SOM ) and kernel density estimation ( KDE ) for network intrusion detection . In SPIE Europe Security+ Defence , 2009 .
[ 9 ] Y . Chen , M . Welling , and A . Smola . Super samples from kernel hearding . In UAI , 2010 .
[ 10 ] M . S . de Lima and G . S . Atuncar . A bayesian method to estimate the optimal bandwidth for multivariate kernel estimator . Journal of Nonparametric Statistics , 23(1):137–148 , 2011 .
[ 11 ] L . Devroye and L . Gy¨orfi . Nonparametric Density
Estimation : The L1 View . Wiley , 1984 .
[ 12 ] L . Devroye and G . Lugosi . Combinatorial Methods in
Density Estimation . Springer Verlag , 2001 .
[ 13 ] T . Duong et al . ks : Kernel density estimation and kernel discriminant analysis for multivariate data in r . Journal of Statistical Software , 21(7):1–16 , 2007 . [ 14 ] T . Duong and M . L . Hazelton . Cross validation bandwidth matrices for multivariate kernel density estimation . Scandinavian J . of Stat . , 32:485–506 , 2005 .
[ 15 ] H . Edelsbrunner , B . T . Fasy , and G . Rote . Add isotropic Gaussian kernels at own risk : More and more resiliant modes in higher dimensions . SOCG , 2012 .
[ 16 ] A . Gangopadhyay and K . Cheung . Bayesian approach to choice of smoothing parameter in kernel density estimation . J . of Nonparam . Stat . , 14:655–664 , 2002 . [ 17 ] J . Habbema , J . Hermans , and K . van den Broek . A stepwise discrimination analysis program using density estimation . Proc . in Computational Statistics , 1974 .
[ 18 ] P . Hall , J . Marron , and B . U . Park . Smoothed cross validation . Prob . The . and Rel . Fields , 92:1–20 , 1992 .
[ 19 ] P . Hall and M . P . Wand . Minimizing L1 distance in nonparametric density estimation . Journal of Multivariate Analysis , 26(1):59–88 , 1988 .
[ 20 ] S . Hu , D . S . Poskitt , and X . Zhang . Bayesian adaptive bandwidth kernel density estimation of irregular multivariate distributions . CS&DA , 56:732–740 , 2012 . [ 21 ] M . Jones and S . Sheather . Using non stochastic terms to advantage in kernel based estimation of integrated squared density derivatives . Statistics & Probability Letters , 11:511–514 , 1991 .
[ 22 ] J . Kiefer . Sequential minimax search for a maximum .
Proc . Am . Mathematical Society , 4:502–506 , 1953 . [ 23 ] K . Kulasekera and W . Padgett . Bayes bandwidth selection in kernel density estimation with censored data . Nonparametric statistics , 18(2):129–143 , 2006 . [ 24 ] J . Marron and A . Tsybakov . Visual error criteria for qualitative smoothing . Journal of the American Statistical Association , 90(430):499–507 , 1995 . [ 25 ] A . P´erez , P . Larra˜naga , and I . Inza . Bayesian classifiers based on kernel density estimation : Flexible classifiers . Int . J . Approximate Reasoning , 50:341–362 , 2009 .
[ 26 ] J . M . Phillips . eps samples for kernels . SODA , 2013 . [ 27 ] J . M . Phillips , B . Wang , and Y . Zheng . Geometric inference on kernel density estimates . In SoCG , 2015 .
[ 28 ] M . Rudemo . Empirical choice of histograms and kernel density estimators . Scandin . J . of Stat . , 9:65–78 , 1982 .
[ 29 ] S . R . Sain , K . A . Baggerly , and D . W . Scott .
Cross validation of multivariate densities . J . American Statistical Association , 89:807–817 , 1994 .
[ 30 ] D . Scott , R . Tapia , and J . Thompson . Kernel density estimation revisited , . Nonlinear Analysis , Theory , Methods and Appplication , 1:339–372 , 1977 .
[ 31 ] D . W . Scott . Multivariate Density Estimation :
Theory , Practice , and Visualization . Wiley , 1992 .
[ 32 ] D . W . Scott and G . R . Terrell . Biased and unbiased cross validation in density estimation . J . ASA , 82:1131–1146 , 1987 .
[ 33 ] B . W . Silverman . Density Estimation for Statistics and Data Analysis . Chapman & Hall/CRC , 1986 .
[ 34 ] G . R . Terrell . Maximal smoothing principle in density estimation . J . ASA , 85:470–477 , 1990 .
[ 35 ] E . Ullman . A theory of location for cities . American
Journal of Sociology , pages 853–864 , 1941 .
[ 36 ] M . Wand and M . Jones . Multivariate plug in bandwidth selection . J . Comp . Stat , 9:97–116 , 1994 . [ 37 ] C . Yang , R . Duraiswami , and L . S . Davis . Efficient kernel machines using the improved fast gauss transform . In NIPS , 2004 .
[ 38 ] C . Yang , R . Duraiswami , N . A . Gumerov , and
L . Davis . Improved fast Gauss transform and efficient kernel density estimation . In ICCV , 2003 .
[ 39 ] X . Zhang , M . L . King , and R . J . Hyndman . A bayesian approach to bandwidth selection for multivariate kernel density estimation . CS&DA , 50:3009–3031 , 2006 .
[ 40 ] Y . Zheng , J . Jestes , J . M . Phillips , and F . Li . Quality and efficiency in kernel density estimates for large data . In SIGMOD , 2013 .
1541 APPENDIX A . STRUCTURAL PROOFS
To prove the weighted centroid method converges , we want to prove Theorem 1 in 1 and 2 dimension . For simplicity , we assume Q ⊂ P so P = P ∪ Q .
First we work on the weighted 1 dimensional data , and extend to 2 dimension using that the cross section of a 2dimensional Gaussian is still a 1 dimensional Gaussian . We focus on when P and Q use the same bandwidth σ , and a unit kernel Kσ . We start to examine two points in 1 dimension , and without loss of generality , we assume p1 = d and p2 = −d for d ≥ 0 , and that the coreset of P is Q = {p2} . We assign the weight for p1 as w1 and the weight for p2 as w2 . Plug in P , Q and the weight for each point , G(x ) = |kdeP ( x ) − kdeQ(x)| is expanded as following :
G(x ) = We assume w1 ≥ w2 , the largest error point must be closer to p1 . So we only need to discuss when x ≥ 0 , then 1 ( x−d)2
2σ2
2
2 fifififi 1 w1 exp , − ( x − d)2 − 1 2 w2 exp , − ( x+d)2 , so ≥ 1 − 1 w1 exp , − ( x − d)2
2σ2
2σ2
fifififi . w2 exp , − ( x + d)2 2 w1 exp,− . w2 exp , − ( x + d)2
G(x ) =
2σ2
2σ2
2
2σ2
1 2
Lemma 2 . For Kσ a unit Gaussian kernel , P = {p1 , p2} and Q = {p2} where p1 = d and p2 = −d , when x ≥ 0 , function G(x ) has only one local maximum , which is between d and d + σ and G(x ) is decreasing when x > d .
Proof . By taking the derivative of G(x ) , we can get
When 0 ≤ x < d , both 1 ( x−d)2
x−d
2σ2 ing .
When x = d ,
.
=
2σ2
σ2 dx
2σ2
2σ2 dG(x )
1 2 − 1 2
σ2 > 0 , thus dG(x )
x + d w2 exp , − ( x + d)2 x − d w1 exp , − ( x − d)2 x+d 2 w2 exp,− ( x+d)2 σ2 σ2 and − 1 2d w2 exp , − 2d2 σ2 ≥ 0 . x+d 2 w2 exp , − ( x+d)2 exp , − 2xd x−d 2 w1 exp , − ( x−d)2 and x+d dG(x ) w2 w1
1 2 dx
σ2
σ2
σ2
2σ2
2σ2
σ2
=
=
1
1 r(x ) =
Since both exp,− 2xd
σ2 r(x ) and thus dG(x ) dx
To understand x > d we examine the ratio function x−d are decreasing and positive , is decreasing when x > d .
When x = d + σ , the ratio function is exp(− 2σd + 2d2 r(d + σ ) = w2 w1
)
σ + 2d
σ
.
σ2
We can view the above equation as a function of variable d . r(d ) = w2 w1 exp(− 2σd + 2d2
σ2
)
σ + 2d
σ
, and take the derivative of r(d ) : dr(d ) dd
= − 4d(d + σ )
σ3 w2 w1 exp(− 2σd + 2d2
σ2
) ≤ 0 . dx > 0 , so G(x ) is always increas
2 w1 exp,−
x + d x − d
. dd ≤ 0 and thus r(d ) is a decreasing ≤ 1 when d = 0 ; thus dx ≤ 0 . With the above
With d ≥ 0 then dr(d ) function which attains maximum w2 w1 r(d ) ≤ 1 . So when x = d + σ , dG(x ) fact that dG(x ) is decreasing when x > d , there is only one point between d and d + σ making dG(x ) dx > 0 . There is only one maximum point of G(x ) between d and d + σ when x ≥ 0 . dx ≥ 0 when x = d and dG(x ) dx = 0 . Since when 0 ≤ x < d , dG(x ) dx
From Lemma 2 , we show that the evaluation point having largest error is between d and d + σ . Due to the symmetry of p1 and p2 , when w1 ≤ w2 , G(x ) gets its largest error between −d and −d − σ .
With the results on both sides , we now show the maximum value point of G(x ) can’t be outside σ distance of Conv(P ) .
Now we discuss the case for n points in 1 dimension .
Lemma 3 . For Kσ a unit Gaussian kernel , P has n points and |Q| = |P|/2 , arg maxx∈R1 G(x ) for 1 dimensional data is within σ distance of Conv(P ) .
Proof . Suppose n = 2k , P = {p1 , p2 , p3 , p4 , , p2k−1 , p2k} , choose any k points in Q . Then pair any point in Q with any point in P not in Q , so each point in P is in exactly one pair . For simplicity we set Q = {p1 , p3 , , p2k−1} and the pairs are {p1 , p2},{p3 , p4} , ,{p2k−1 , p2k} .
Suppose e1 = arg maxx∈R1 G(x ) is not within σ distance of Conv(P ) and p1 is the point closest to e1 . Based on Lemma 2 , for P has only two points , function G(x ) is decreasing as a point outside σ moves away from p1 . So if we choose another point e2 infinitesimally closer to p1 , and we set P1 = {p1 , p2} , Q1 = {p1} , GP1,Q1 ( e2 ) has larger value than GP1,Q1 ( e1 ) . Since p1 is the closest point in P , for any other set P2 = {p3 , p4} , Q2 = {p3} , e2 is closer to P2 than e1 is to P2 , hence GP2,Q2 ( e2 ) is also larger than GP2,Q2 ( e1 ) . The same result holds for all pairs {p2i−1 , p2i} , where i is from 1 to k . So G(e2 ) > G(e1 ) , which contradicts the assumption that e1 = arg maxx∈R1 G(x ) . So the largest error evaluation point should be within σ distance of Conv(P ) .
In 2 dimensions we show a similar result . We illustrate the Minkowski sum M of a set of points P with a ball of radius σ in Figure 2 .
Theorem 3 . For Kσ a unit Gaussian kernel , and two point sets P , Q ∈ R2 , |Q| = |P|/2 , arg maxx∈R2 G(x ) should be within the Minkowski sum M of a ball of radius σ and Conv(P ) .
Proof . Now we have n points in P ∈ R2 . Suppose the largest error position e1 = arg maxx∈R2 G(x ) ∈ M , then for some direction v no point in the convex hull of P is closer than σ to e1 after both are projected onto v . Then since any cross section of a Gaussian is a 1 dimensional Gaussian ( with reduced weight ) , we can now invoke the 1 dimensional result in Lemma 3 to show that e1 is not the largest error position along the direction v , thus e1 = arg maxx∈R2 G(x ) . So arg maxx∈R2 G(x ) should be within the Minkowski sum M of a ball of radius σ and Conv(P ) .
1542
