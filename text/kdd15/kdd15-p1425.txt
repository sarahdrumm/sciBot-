Dynamic Topic Modeling for Monitoring Market Competition from Online Text and Image Data
Hao Zhang
Carnegie Mellon University
Pittsburgh , PA , 15213 hao@cscmuedu
Gunhee Kim
Seoul , South Korea , 151 744
Seoul National University gunhee@snuackr
Eric P . Xing
Carnegie Mellon University
Pittsburgh , PA , 15213 epxing@cscmuedu
ABSTRACT We propose a dynamic topic model for monitoring temporal evolution of market competition by jointly leveraging tweets and their associated images . For a market of interest ( eg luxury goods ) , we aim at automatically detecting the latent topics ( eg bags , clothes , luxurious ) that are competitively shared by multiple brands ( eg Burberry , Prada , and Chanel ) , and tracking temporal evolution of the brandsâ€™ stakes over the shared topics . One of key applications of our work is social media monitoring that can provide companies with temporal summaries of highly overlapped or discriminative topics with their major competitors . We design our model to correctly address three major challenges : multiview representation of text and images , modeling of competitiveness of multiple brands over shared topics , and tracking their temporal evolution . As far as we know , no previous model can satisfy all the three challenges . For evaluation , we analyze about 10 millions of tweets and 8 millions of associated images of the 23 brands in the two categories of luxury and beer . Through experiments , we show that the proposed approach is more successful than other candidate methods for the topic modeling of competition . We also quantitatively demonstrate the generalization power of the proposed method for three prediction tasks . Categories and Subject Descriptors H28 [ Information Systems ] : Database Applicationsâ€” Data mining ; G.3 [ Probability and Statistics ] : Probabilistic Algorithms ; J.4 [ Computer Applications ] : Social and behavioral sciencesâ€”Economics Keywords Dynamic topic models ; Market competition ; Text and images
1 .
INTRODUCTION
The increasing pervasiveness of the Internet has lead to a wealth of consumer created data over a multitude of online platforms such as blogs , discussion forums , and social networking sites . Such contents are valuable for companies to listen in consumersâ€™ candidate opinions , and thus there have been many recent studies on online market intelligence [ 10 , 17 , 18 ] , whose goal is collecting and analyzing online information that is contributed by the general public toward companiesâ€™ products and services , and providing with pictures of ongoing brand performance in a set of given market conditions . The online market intelligence has been one of emerging fields in data mining research as market competition becomes fierce , and consumersâ€™ online reviews and evaluations are considered more trustworthy and spontaneous than other information described by vendors .
In this paper , we address the problem of modeling temporal evolution of market competition by jointly leveraging text data and their associated image data on the Web . More specifically , we study tweets and their linked images . Fig 1 illustrates the problem statement of this paper . For a specified competitive market ( eg luxury goods ) , multiple brands ( eg Burberry , Chanel , and Rolex ) compete one another to raise their stakes over shared values or topics , which include products related topics such as bags , clothes , and watch , or consumersâ€™ sentiments related topics such as luxurious , expensive . The objective of this research is to build an automatic system that crawls tweets , extract text and images from tweets , identify shared topics that multiple brands compete to possess one another , and track the evolution of brandsâ€™ proportional dominance over the topics .
Our approach focuses on the joint analysis of text and image data tagged with the names of competing brands , which have not been explored yet in the previous studies of online market intelligence . The joint interpretation of text and images is significant for several reasons . First , a large portion of tweets simply show images or links without any meaningful text in them . Hence , images play an important role for representing topics in this type of tweets . In our dataset , 70 % of tweets are attached with urls , and 28 % of tweets in the luxury category are with images . Second , many users prefer to use images to deliver their idea more clearly and broadly , and thus the topic detection with images reflects usersâ€™ intents better . The popularity of images can be seen in a simple statistics of our twitter dataset ; our luxury corpus contains more images than tweets ( eg 5.5 millions tweets with 6.6 millions of images ) . Third , the joint use of images with text also helps marketers interpret the discovered topics . Due to the short length of tweets ( ie 140 characters ) , marketers may need to see the associated images to understand key ideas of tweets easier and quicker . Finally , since
1425 Figure 1 : Problem statement . ( a ) Input is a large collection of tweets and their associated images that are retrieved by the names of competing brands in a market of interest . ( b ) As output we aim at identifying the topics that are shared by multiple brands , and track the evolution of topics and proportion of brands over the topics . the Internet is where users cannot physically interact one another about actual products or services , images may be essential for users to make conversation about customersâ€™ descriptions , experiences , and opinions toward the brands . From technical viewpoints , we propose a novel dynamic topic model to correctly address the following three major challenges : ( 1 ) multi view representation of text and images , ( 2 ) modeling of latent topics that are competitively shared by multiple brands , and ( 3 ) tracking temporal evolution of the topics . Some of existing work attain a subset of these challenges ( eg texts and images [ 4 , 7 ] and dynamic modeling [ 1 , 5] ) , but none of them satisfies all the challenges .
We evaluate our algorithm using newly collected dataset from Twitter from October 2014 to February 2015 . Our automatic crawler downloads all tweets tagged by brand names of interest , along with attached or linked images if available . Consequently , our dataset contains about 10 millions of original tweets and 8 millions of associated images of the 23 brands in the two categories of luxury and beer . The experiments demonstrate the superior performance of the proposed approach over other candidate methods , for dynamic topic modeling and three prediction tasks including prediction of the most associated brands , most likely created time , and competition trends for unseen tweets . Note that while we mainly deal with brands of the two categories , our approach is completely unsupervised and thus applicable , without any modification , to any categories once input sets of text and image streams are collected .
The foremost application of our work is social media monitoring , which assists marketers to summarize their fansâ€™ online tweets with sparse and salient topics of competition in an illustrative way . Especially , our algorithm can discover and visualize the temporal progression of what topics are highly overlapped or discriminative over other competitors . From our interaction with marketers , we observe that they are very curious to see and track what topics emerge and what pictures their fans ( re )tweet the most , but there is no such system yet . As another application , our method can be partly used for sentiment analysis [ 17 ] because the detected topics can be positive or negative . That is , multiple brands competes one another not only on positive topics ( eg multiple cosmetics brands compete on the health+beauty topic ) but also negative topics ( eg multiple beer brands compete on the drunk+driving topic ) . We do not perform in depth analysis on sentiment analysis because it is out of the scope , but at least marketers can observe their brandsâ€™ distribution on both positive and negative topics , which is also useful for market analysis . Although we mainly focus on the applications of brand competitions in a market , our problem formulation and approach are much broader and are applicable to other domains of competition , including tourism ( eg multiple cities compete to attract more international tourists ) , and politics ( eg multiple candidates contest to take leads on major issues to win an election ) , to name a few .
The main contributions of this paper are as follows . ( 1 ) To the best of our knowledge , our work is the first attempt so far to propose a principled topic model to discover the topics that are competitively shared between multiple brands , and track the temporal evolution of dominance of brands over topics by leveraging both text and image data . ( 2 ) We develop a new dynamic topic model for market competition that achieves three major challenges of our problem ; multiview representation of text and images , modeling of competitiveness of multiple entities over shared topics , and tracking their temporal evolution . As far as we know , no previous model can satisfy all the challenges . ( 3 ) With experiments on more than 10 millions of tweets with 8 millions of images for 23 competing brands , we show that the proposed algorithm is more successful for the topic modeling over other candidate methods . We also quantitatively demonstrate the generalization ability of the proposed method for three prediction tasks .
2 . RELATED WORK
Online Market Intelligence . One of most closely related line of work to ours is online market intelligence [ 17 ] , whose objective is , broadly speaking , to mine valuable information for companies from a wealth of consumer generated online data . Due to vast varieties of markets , brands , and information to mine , there have been many different directions to address the problem as follows . As one of early successful commercial solutions , the BrandPluse platform [ 10 ] monitors consumersâ€™ buzz phrases about brands , companies , or any emerging issues from public online data . In [ 15 ] , marketstructure perceptual maps are automatically created to show which brands are jointly discussed in consumersâ€™ forums especially for the two categories of market , which are sedan cars and diabetes drugs . The work of [ 24 ] focuses on extracting comparative relations from Amazon customer reviews , and visualize the comparative relation map ( eg Nokia N95 has a better camera than iPhone ) . The authors of [ 2 ] also leverage Amazon data to discover the relations between
#Style #PradaBlack Leather & Nylon Tessuto Saffiano Shoulder #Baghttp://dlvr.it/8WZKM2#Forsale #AuctionCoat from @ASOS , top from @FreePeople , jeans from Rag & Bone , boots from #ChristianLouboutin & bagfrom @Prada . What is the most beautifully designed perfume bottle ? Tell us on the blog here : http://smarturl.it/ie2fkaand win GucciThe latest crop of #ChanelPre Spring bags have arrived! See the full collection now : http://bit.ly/1z3PnKGPretty In Pink : From @Chanelto @nailsinc , the best petal hued make up launches this spring http://vogue.uk/8p6UOiDesigner Kate Spade , Invicta , Gucci& More Watches from $22 & Extra 20 % Off http://wwwdealspluscom/t/1zr85Ywatch+diamondrolex,watch,gold,dial,mens,datejust,ladies,steel,diamond,oyster,stainless,18kglasseschanel , giorgio , sunglasses , classic , glasses , reading , women's , #burberrygiftsbagsbag , leather , gucci , handbag , tote , clothing,shoulder,canvas , reading , women's , watch+diamondwatch , gold,whitedate,ladies,dialgift , rolex #deals_us,blue,vintage , bracelet , omega,glasseschanel , sunglasses,listen,green,funny,dark,xmas , womens , Armani , excellent , Havana . lacostebagsauthentic , leather,bag,shoes,gucci , handbag,prada,tote,deals,brown,wallettt +1TimelineChanelGucciPrada(a ) Input : Tweets and associated images of competing brands ( b ) Output : Temporal evolution of topics and brandsâ€™ proportion over the topics Topics ( text / visual words)Brands over topics1426 product sales and review scores of each product feature ( eg battery life , image quality , or memory for digital cameras ) . In [ 22 ] , a recommendation system on the blogosphere is developed to learn historical weblog posts of users , and predict which users the companies need to follow when they release new products . Our work has two distinctive features over existing research of this direction . First , we address an unexplored problem of detecting the latent topics that are competitively shared by multiple brands , and automatically tracking their temporal evolution . Second , we jointly leverage two complementary modalities , text and images , which have been rare in market intelligence research .
Topic Models for Econometrics . Lately , there have been significant efforts to develop generative topic models for modeling and prediction of economic behaviors of users on the Web . In [ 8 ] , a simple LDA model is applied to stock market data to detect the groups of companies that tend to move together . The work of [ 11 ] proposes a new dynamic topic model to predict the temporal changes of consumersâ€™ interests and purchasing probabilities over catalog items . In [ 13 ] , a geo topic model is developed to learn the latent topics of usersâ€™ interests from location log data , and recommend new location that are potentially interesting to users . Finally , [ 14 , 19 ] are examples of topic models that are applied to the tasks of opinion mining and sentiment analysis , in which they produce fine grained sentiment analysis from user reviews or weblog posts . Compared to previous research of this direction , our problem of modeling market competition of multiple brands is novel , and our model is also unique as an econometric topic model that jointly leverages online texts and images .
Dynamic and Multi view Topic Models . There has been a large body of work to develop dynamic topic models to analyze data streams [ 8 , 11 , 13 , 14 , 19 ] , and multi view topic models to discover the interactions between text and images in multimedia contests [ 4 , 7 , 9 , 21 ] . Compared to existing dynamic and multi view topic models , our approach is unique in the ability of directly modeling the competition of multiple entities ( eg brands ) over shared topic spaces . Since previous models cannot handle with the interactions between multiple entities , they are only applicable to the dataset of each brand separately . However , in this case , the detected topics can be different in each brand ; thus it is difficult to elicit shared topic spaces to model the competition .
3 . A DYNAMIC MODEL FOR MARKET
COMPETITION
We first discuss how to represent online documents and associated images , and then develop a generative model for market competition . 3.1 Representation of Text and Images Suppose that we are interested in a set of competing brands B = {1 , . . . ,BL} in the same market ( eg Chanel , Gucci , and Prada as luxury brands ) . We use Bl to denote a set of documents ( ie tweets ) that are downloaded by querying brand name l in the time range of [ 1 , T ] . We assume that each document d âˆˆ Bl consists of text and optionally URLs that are linked to images . That is , a tweet can be text only or associated with one or multiple images . Some tweets may be associated with multiple brand labels , if they are retrieved multiple times by different brand names . We use a
Ï†t k
Î¸d
Brand topic occupation matrix at time t ( âˆˆ RKÃ—L ) Î²t/Î³t Topic distributions over text/visual words at time t ( âˆˆ RKÃ—G / RKÃ—H ) . Document code of document d ( âˆˆ RK ) . zdn/ydm Word code of text/visual word n/m ( âˆˆ RK ) . wdn/vdm Occurrences of text/visual word n/m in document d . Brand code of brand b in document d ( âˆˆ RK ) . Indicator for each brand label b for document d . rdb gdb
Figure 2 : Plate diagram for the proposed topic model with a table of key random variables . vector gd âˆˆ RL to denote which brands are associated with document d .
For the text descriptor , we use the TF IDF weighted bag of words model [ 4 ] , where we build a dictionary of text vocabularies after removing words occurred fewer than 50 times . For image descriptor , we leverage ImageNet pre trained deep learning features with vector quantization . Specifically , we use Oxford VGG MatConvnet and utilize their pre trained model CNN 128 [ 20]1 . which a compact 128 dimensional descriptor for each image . Then , we construct H visual clusters by applying K means clustering to randomly sampled ( at max ) two millions of image descriptors . We assign the r nearest visual clusters to each image with the weights of an exponential function exp(âˆ’a2/2Ïƒ2 ) + , where a is the distance between the descriptor and the visual cluster , Ïƒ is a spatial scale , and is a small positive value to prevent zero denominator when normalization . Finally , each image is described by an H dimensional 1 normalized vector with only r nonzero weights . In our experiments , we set H = 1 , 024 , Ïƒ = 10 , and r = u0 which is the 0 norm of its corresponding text descriptor , so that text and image descriptors have the same number of nonzeros . As a result , we can represent every document and image as a vector . If we let U = {1 , . . . , G} and V = {1 , . . . , H} to denote sets of vocabularies for text and visual words respectively , each document d is represented by a pair of vector ( ud , vd ) , where ud = [ ud1,Â·Â·Â· , ud|N|]T where N is the index set of words in document d , and each udn(n âˆˆ N ) represents the number of appearances of word n . Likewise , vd = [ vd1,Â·Â·Â· , vd|M|]T where M is the index set of visual words . If a document has multiple associated images , vd is represented by a vector sum of image descriptors . For a document with no associated image , vd becomes a null vector and M is an empty set . 3.2 A Probabilistic Generative Process
Our model is designed based on our previous Sparse Topical Coding ( STC ) framework [ 26 ] , which is a topic model 1http://wwwrobotsoxacuk/âˆ¼vgg/software/deep eval/ .
ðœƒð‘‘ðœƒð‘‘ðœ‘ð‘˜ð‘¡+1ðœ‘ð‘˜ð‘¡ð›½ð‘˜ð‘¡+1ð›½ð‘˜ð‘¡ð›¾ð‘˜ð‘¡+1ð›¾ð‘˜ð‘¡ð‘¡=1:ð‘‡ð‘˜=1:ð¾ð‘Ÿð‘‘ð‘ð‘”ð‘‘ð‘ð‘Ÿð‘‘ð‘ð‘”ð‘‘ð‘ð‘§ð‘‘ð‘›ð‘¤ð‘‘ð‘›ð‘¦ð‘‘ð‘šð‘£ð‘‘ð‘šð‘§ð‘‘ð‘›ð‘¢ð‘‘ð‘›ð‘¦ð‘‘ð‘šð‘£ð‘‘ð‘šð‘‘=1:ð·ð‘‘=1:ð·1427 that can directly control the posterior sparsity . In our problem setting , each document and word is encouraged to be associated with only a small number of strong topics . Since we aim at analyzing the possibly complex interaction between multiple brands , in practice a few salient topical representation can make interpretation easier rather than letting every topic make a non zero contribution . In addition , the sparsity leads a more robust text/image representation since most of tweet documents are short and sparse in word spaces due to length limitation of 140 characters . Another practical advantage of the STC is that it supports simultaneous modeling of discrete and continuous variables such as image descriptors and brand associations .
However , our model significantly extends the STC in several aspects . First , we update the STC to be a dynamic model so that it handles the streams of tweets . Second , we extend to jointly leverage two complementary information modalities , text and associated images . Finally , we address an unexplored problem of detecting and tracking the topics that are competitively shared by multiple brands . All of them can be regarded as novel and nontrivial improvement of our method . Fig 2 shows the graphical model for the proposed generative process . Let Î² âˆˆ RKÃ—G and Î³ âˆˆ RKÃ—H be the matrices of K topic bases for each text and visual word respectively . That is , Î²k . indicates the k th text topic distribution over the vocabularies U . We also use Ï† âˆˆ RKÃ—L to denote the brand topic occupation matrix , which expresses the proportions of each brand over topics . We denote Î¸d âˆˆ RK as the document code , which is a latent topic distribution of document d . zdn âˆˆ RK and ydm âˆˆ RK are the text/visual word code respectively , which are latent topic representation of individual text word n and visual word m in document d .
Below we discuss in detail the generative process of our model , which is summarized in Table 1 .
Multi view STC model . For text content , we use the similar generative process with that of the original STC [ 26 ] . For each document d :
1 . Sample a document code Î¸d âˆ¼ prior p(Î¸ ) . 2 . For each observed word n âˆˆ N , ( a ) Sample a word code zdn âˆ¼ p(z|Î¸d ) . ( b ) Sample an observed word count udn âˆ¼ p(u|zdn , Î² ) .
In order to model documents with both text and images , we develop a multi view extension . Specifically , for each document d , we let its text part ud and its corresponding image part vd share the same document code Î¸d , as shown in Fig 2 . In addition , we assume the same generative process for visual words with the text counterpart . Consequently , we supplement the following step .
3 . For each observed visual word m âˆˆ M , ( a ) Sample a visual word code ydm âˆ¼ p(y|Î¸d ) . ( b ) Sample a visual word count vdm âˆ¼ p(v|ydm , Î³ ) . We now define the distributions used in the above process . Since each tweet is represented by a very sparse vector in a word space , the document code of a tweet is preferred to be sparse in a topic space in order to foreground the most salient topics and suppress noises . To achieve sparsity on Î¸ , we define the document code prior p(Î¸ ) as a Laplacian prior p(Î¸ ) âˆ exp(âˆ’Î»Î¸1 ) , which becomes a 1 regularizer in the negative log posterior . Similarly , to boost the topical sparsity of each word , we define the conditional distributions of word codes as the following composite distribution : p(zdn|Î¸d ) âˆ exp(âˆ’Î´uzdn âˆ’ Î¸d2 p(ydm|Î¸d ) âˆ exp(âˆ’Î´vydm âˆ’ Î¸d2
2 âˆ’ Ïuzdn1 ) 2 âˆ’ Ïvydm1 ) ,
( 1 ) which establishes a connection between the document code and word codes while encouraging sparsity on the word codes . For the last step of generating word counts , the STC recommends to use an exponential family distribution with the linear combination z dnÎ².n as a mean parameter to make optimization easier and the model applicable to rich forms of data . That is , Ep[u ] = z dnÎ².n + where Î².n denotes the n th column of Î² and is a small positive number for avoiding degenerated distributions . We choose to use a Gaussian distribution with the mean of z dnÎ².n , and apply the same idea to the visual word counts . Therefore , p(udn|zdn , Î² ) = N ( udn ; z dnÎ².n , Ïƒ2 uI ) p(vdm|ydm , Î³ ) = N ( vdm ; y dmÎ³.m , Ïƒ2 vI ) .
( 2 )
Dynamic extension .
In order to model the temporal evolution of topics , we let Î² and Î³ to change over time , based on the discrete dynamic topic model ( dDTM ) [ 5 ] . That is , we divide a corpus of documents into sequential groups per time slice t ( eg one week in our experiments ) , and assume that the documents in each group Dt are exchangeable . Then we evolve Î²t and Î³t from the ones in previous time slice t âˆ’ 1 by following the state space model with a Gaussian noise . Therefore , for each topic k , we use p(Î²t p(Î³t k.|Î²tâˆ’1 k.|Î³tâˆ’1 k . ) = N ( Î²tâˆ’1 k . ) = N ( Î³tâˆ’1 k . k .
, Ïƒ2 , Ïƒ2
Î²I )
Î³I ) .
( 3 )
Competition extension . We now extend the multi view dSTC to capture the competition between multiple brands over topics . We first define a brand topic occupation matrix Ï† âˆˆ RKÃ—L to represent the proportions of brands on latent topics . For each document d , we denote B âŠ† B as the index set of brands , and gd âˆˆ RB as an 1 normalized vector representing associated brand labels . For example , if tweet document d is retrieved by keywords {prada , chanel} , then B = {prada , chanel} and gd = [ gd1 gd2 ] , which are normalized values describing how strong the tweet is associated with the observed brands . One can use the same values ( eg gd1 = gd2 = 0.5 ) or proportional values according to relevance scores by the twitter search engine . For each b âˆˆ B and gdb , we use a latent brand code rdb âˆˆ RK as a representation of brand b in the topic space . We let rdb to be conditioned on the document code Î¸d , which governs the topic distributions of not only text/visual words but also brand labels .
There are two possible options of dynamics on the brandtopic occupation matrix Ï† . First , similarly to Î² and Î³ , we evolve Ï† to capture potential dynamics between brands and latent topics over the time . In this case , we use the state space model with a Gaussian noise , and thus Ï† has the same distribution of Eq ( 3 ) . Second , if we assume that the brand occupation over topics is stationary , we can sample Ï† from a uniform distribution . We take the first approach .
Finally , we can apply the same distributions to the generative process for the brands with the counterparts of text and visual words . That is , we use the composite distribution of Eq ( 1 ) for p(rdb|Î¸d ) , and the Gaussian distribution
1428 of Eq ( 2 ) for p(gdb|rdb , Ï† ) . In summary , p(rdb|Î¸d ) âˆ exp(âˆ’Î´brdb âˆ’ Î¸d2 b I ) p(gdb|rdb , Ï† ) = N ( gdb ; r k . ) = N ( Ï†tâˆ’1 p(Ï†t dbÏ†.b , Ïƒ2 , Ïƒ2 k.|Ï†tâˆ’1
Ï†I ) . k .
2 âˆ’ Ïbrdb1 )
( 4 )
4 . LEARNING AND INFERENCE
In this section , we describe the optimization for learning and inference of the proposed model . 4.1 MAP Formulation
The generative process of Fig 2 provides a joint probabil ity for a document d in each time slice t : p(Î¸ , z , u , y , v , r , g|Î² , Î³ , Ï† ) = p(Î¸ ) p(zn|Î¸)p(un|zn , Î² ) p(ym|Î³)p(vm|ym , Î³ ) nâˆˆN p(rb|Ï†)p(gb|rb , Ï† )
( 5 ) bâˆˆB mâˆˆM d , zt
If we add the superscript t to explicitly represent the time slice for each variable , we can denote the parameter set as follows : Î˜t = {Î¸t dn}nâˆˆN t , dm}mâˆˆM t d = {yt yt d denotes the word index set of document d in time slice t , and likewise for M t d . Although we skip the derivation due to space limitation , it is not difficult to show the negative log posterior for time slice t satisfies d=1 , where zt db}bâˆˆBt d = {zt , where N t d}Dt d , rt d = {rt d , yt and rt d and Bt d d d
âˆ’ log p(Î˜t , Î²t , Î³t , Ï†t|{ut d}Dt d , vt d=1 ) d=1|Î²t , Î³t , Ï†t ) . d}Dt
âˆ âˆ’ log{p(Î˜t,{ut d , gt d , gt d , vt In the above , Î»,{Î½i , Î´i , Ï€i}3 i=1 are hypeparameters , which
( 6 ) are chosen by cross validation in our experiments . 4.2 Parameter Estimation
We estimate the model parameters by minimizing the negative log posterior derived in previous section . Since Eq ( 6 ) is the one for the documents in a single time slice t , we accumulate the negative log posteriors of all time ranges , and seek for an optimal solution for the whole corpus of all time slices . Therefore , the final objective is derived as
Î»Î¸t d1
( 7 )
( Ï€1Î²t âˆ’ Î²tâˆ’12
2 + Ï€2Î³t âˆ’ Î³tâˆ’12
2 + Ï€3Ï†t âˆ’ Ï†tâˆ’12 2 )
T
Dt min
{Î˜t,Î²t,Î³t,Ï†t}T t=1 t=1 d=1 t=2
T T T T t=1 t=1
+
+
+
+ d=1
Dt Dt Dt d=1 nâˆˆN t d mâˆˆM t d t=1 d=1 bâˆˆBt d
( Î½1zt dn âˆ’ Î¸t d2
2 + Ï1zt dn1 + L(zt dn , Î²t ) )
( Î½2yt dm âˆ’ Î¸t d2
2 + Ï2yt dm1 + L(yt dm , Î³t ) )
( Î½3rt db âˆ’ Î¸t d2
2 + Ï3rt db1 + L(rt db , Ï†t ) ) d â‰¥ 0 , âˆ€d , t . zt st Î¸t k âˆˆ PU , Î³t Î²t dn , yt k âˆˆ PV , Ï†t dm , rt k âˆˆ PB , âˆ€k , t , db â‰¥ 0 , âˆ€d , n , m , b , t . where PU ,PV ,PB are the G âˆ’ 1 , H âˆ’ 1 and L âˆ’ 1 simplex , respectively ( ie For âˆ€k , t , each of Î²t k should k and Ï†t k , Î³t
For each time slice t : 1 . Draw a text topic matrix Î²t|Î²tâˆ’1 âˆ¼ N ( Î²tâˆ’1 , Ïƒ2 Î²I ) . 2 . Draw an image topic matrix Î³t|Î³tâˆ’1 âˆ¼ N ( Î³tâˆ’1 , Ïƒ2 Î³I ) . 3 . Draw a brand topic matrix with two options : ( i ) dyÏ†I ) , or ( ii ) independent namic Ï†t|Ï†tâˆ’1 âˆ¼ N ( Ï†tâˆ’1 , Ïƒ2 Ï†t âˆ¼ Unif ( 0 , 1 ) .
4 . For each document d = ( u , v ) in Dt , ( a ) Sample a document code Î¸d âˆ¼ prior p(Î¸ ) . ( b ) For each observed text word n âˆˆ N , i . Sample a word code zdn âˆ¼ p(zdn|Î¸d ) . ii . Sample a word count udn âˆ¼ p(u|zdn , Î² ) . ( c ) If M is not an empty set : i . For each observed visual word m âˆˆ M , A . Sample a visual word code ydm âˆ¼ p(ydm|Î¸d ) . B . Sample a visual word count vdm âˆ¼ p(v|ydm , Î³ ) .
( d ) For each observed brand b âˆˆ B , i . Sample a latent brand code rdb âˆ¼ p(rdb|Î¸d ) ii . Sample a brand association gdb âˆ¼ p(g|rdb , Ï† )
Table 1 : The generative process of the proposed model ( See text for details ) . sum to one ) . We denote L as the negative log loss of reconstruction for word counts and brand associations in Eq ( 2 ) : L(zt dn , Î²t ) = âˆ’log p(ut dn , Î²t ) = Î´1ut dnâˆ’zt dn|zt dnÎ².n
( 8 )
Thanks to the use of an exponential family distribution for generating word counts ( eg Gaussian distributions of Eq ( 2) ) , the loss function L is convex , and thus the optimization of Eq ( 7 ) is multi convex ( ie the optimization is convex over one parameter set when the others are fixed ) . Consequently , we can directly employ coordinate descent to solve the optimization problem .
Taking into consideration that tweet documents grow along with time , we propose two approaches for solving the above problem , namely online learning and smoothing . The two approaches are similar except that the online learning seeks for a local minimum in the current time slice , based on the data of one or several previous time slices , which can be more scalable for online monitoring of real world big data . On the other hand , the smoothing approach globally optimizes the objective over the data in all time slices , which is less scalable but yields more accurate fitness for data , and thus can be more suitable for batch analysis . 421 In the smoothing approach , we directly optimize the objective of Eq ( 7 ) . Note that every two adjacent time slices are only coupled by three parameters : Î² , Î³ and Ï† . Hence , if we fix these three parameters , the objective for each time slice is independent one another . Based on this idea , we alternate between the optimization for Î² , Î³ , Ï† and the one for the other variables using the coordinate descent algorithm : 1 . Fix all {Î²t , Î³t , Ï†t}T t=1 . We now decouple the optimization of every time slice t . Since documents can be assumed to be independent one another , we further decou
Smoothing Approach
1429 ple per document d . Therefore we solve
Î»Î¸t d1 dn âˆ’ Î¸t
( Î½1zt d2
2 + Ï1zt dn1 + L(zt dn , Î²t ) )
( 9 )
( Î½2yt dm âˆ’ Î¸t d2
2 + Ï2yt dm1 + L(yt dn , Î³t ) )
( Î½3rt db âˆ’ Î¸t d2
2 + Ï3rt db1 + L(rt db , Ï†t ) )
Î¸t d,zd,yd,rd nâˆˆN t d min mâˆˆM t d
+
+
+ bâˆˆBt st : Î¸t d d â‰¥ 0 ; zt dn , yt dn , rt db â‰¥ 0,âˆ€n .
Note that for every document d âˆˆ Dt , if Î¸d is fixed , zd , yd and rd are independent one another . Thus , we can use the coordinate descent to alternatingly optimize Î¸d and zd , yd , rd .
( a ) While fixing Î¸d , we solve each of zdn , ydm , rdb independently , all of which have close form solutions . Specifically , the solution for the kth element of zt jnâˆ’ Ï1 is zt where Ïƒ1 = 1 . dkâˆ’Î²t kn Î²t2 kn+Ïƒ1 dnk = max(0 , kn+Ïƒ1Î¸t j=k zt ut dnÎ²t dnj Î²t dn ) ,
2
( b ) While fixing zd , yd , and rd , we solve the following problem to update Î¸d :
Î»Î¸t d1 +
Î½1zt dn âˆ’ Î¸t d2
2
( 10 ) nâˆˆN t dm âˆ’ Î¸t d d2
2 +
Î½2yt
Î½3rt db âˆ’ Î¸t d2
2 bâˆˆBt d min Î¸t d
+ d mâˆˆM t d â‰¥ 0 stÎ¸t
The optimal Î¸ of this problem is the truncated average of zt db [ 26 ] . We drop the term including yt dm for the documents with no associated image . dm , rt dn , yt
2 . While fixing all parameters of {Î˜t}T t=1 , we optimize d t=2 d=1 nâˆˆN t
T Dt k âˆˆ P,âˆ€k , t
L(yt
L(rt mâˆˆM t bâˆˆBt k , Ï†t d d min
{Î²t,Î³t,Ï†t}T
T T t=2 t=1 dâˆˆDt
+
+ t=2 st Î²t dâˆˆDt k , Î³t
L(zt dn , Î²t ) +
Ï€1Î²t âˆ’ Î²tâˆ’12
2 t=2
Ï€2Î³t âˆ’ Î³tâˆ’12
2 dm , Î³t ) +
T
T T t=2 dm , Ï†t ) +
Ï€3Ï†t âˆ’ Ï†tâˆ’12
2 t=2
( 11 ) t=1 , {Ï†t}T
We can obtain the optimal of Eq ( 11 ) by separately solving {Î²t}T t=1 , {Î³t}T t=1 , because they are independent one another . When we solve each of them , for example of {Î²t}T t=1 , we utilize the coordinated descent and the projected gradient descent , in which we solve every Î²t one by one for each t . That is , at every iteration we fix all {Î²t}T t=1 \ Î²t , and use projected descent to solve Î²t . We iterate until convergence for every t .
422 Online Learning Approach Instead of directly optimizing the objective of Eq ( 7 ) , the online learning approach assumes that at every time t , we only observe a new set of data at t , and have learned model parameters from the data up to t âˆ’ 1 . This can be more practical in a real world scenario ; we may not always globally optimize using all the past data when we observed new data . Instead we would better seek for a local minimal that may be good enough to reflect the current state of market competition . Formally , we assume that at each t we only consider its p previous time slices to form an evolving chain . To make our discussion easier , we set p = 1 ; however , it is not difficult to derive the optimization solver for p > 1 . tâˆ’1 tâˆ’1
, Ë†Î³tâˆ’1 , and Ë†Ï†
Given the optimization algorithm for smoothing approach in previous section , online learning optimization is readily straightforward . At time slice t we assume that we are given the MAP solutions up to t âˆ’ 1 , which are denoted by tâˆ’1 Ë†Î² . We sample Î²t from the distribution p(Î²t| Ë†Î² ) as defined in Eq ( 3 ) . We do the same for Î³t and Ï†t as well . Once we have Î²t , Î³t , and Ï†t , as discussed in previous section , the objective of Eq ( 7 ) for each time slice is independent one another . Thus , we can directly apply the algorithm presented in previous section to solve the decoupled objective in every time slice one by one along with time . At the start of the optimization , we initialize all {Î²t}T t=1 using a uniform prior . t=1,{Ï†t}T t=1,{Î³t}T
5 . EXPERIMENTS
We evaluate our model from the following four aspects . First , we qualitatively and quantitatively evaluate the quality of learned text and visual topics ( Section 521 ) Second , we show how our model can simultaneously monitor topic evolution and market competition along with time , compared to some baseline methods ( Section 522 ) Third , we design three prediction methods based on our model , to show the generalization power of our model for unseen documents ( Section 53 ) Finally , we conduct internal comparisons and provide some analysis on our model ( Section 54 ) 5.1 Twitter Dataset for Multiple Brands
Fig 3 summarizes some statistics of our Twitter dataset for two groups of competing brands : Luxury and Beer . We query brand names using Twitter â€™s RESTs API without any filtering , in order to obtain usersâ€™ free and uncensored views on the brands . The data range from 10/20/2014 to 02/01/2015 , during which our crawler is scheduled to run once every week , 3 days per week to finish the weekly crawling job . After obtaining raw tweets , we use a publicly available tokenizer for Twitter [ 16 ] to extract text and valid URLs from each tweet , and eliminate illegal , non English characters , and stop words , while preserving emoticons , blocks of punctuation and twitter catchwords2 . In addition , our crawler traverses every legal URL , and downloads images located in the body of HTML pages . We exclude the images that have too small file sizes or unreasonable aspect ratios . We extract text and image descriptors as described in section 31 Note that our text and image descriptors for the same document have the same number of nonzero elements ( ie |N| = |M| ) . We then mean align the two descriptors by setting mean(u ) = mean(v ) . We standardize the text and image descriptor to avoid bias on any of them . For tweets with multiple images , we obtain the vector sum of all image descriptors , and standardize it .
2We follow [ 26 ] to use a standard list of 524 stop words .
1430 Figure 3 : Statistics of our newly collected twitter dataset on the timeline . We report the numbers of ( tweets , tweets with images , images ) from top to bottom . ( a ) The Beer corpus = ( 1,091,369 , 231,318 , 829,207 ) ( b ) The Luxury corpus = ( 5,511,887 , 935,903 , 6,606,125 ) .
Consequently , the Beer corpus involves 12 brands , yielding 1,101,192 raw tweets and 829,207 images . We build a dictionary of 12,488 text vocabulary words after removing words occurred fewer than 50 times . Finally , we obtain 1,091,369 valid tweet documents , out of which 231,318 tweets contain images as well . The Luxury corpus is much larger than the beer corpus , including 5,572,017 raw tweets and 6,606,125 images . Following the same preprocessing step , we obtain a dictionary of 36,023 words , and 5,511,887 tweet documents and 1,523,177 ones associated with images . 5.2 Model Evaluation
In this section , we evaluate the performance of topic detection and tracking of our model , and demonstrate its application to the market competition monitoring . 521 Topic Quality and Evolution We assess the quality of the learned topics by our model , compared to other commonly used topic models . Our goal here is to quantitatively show that ( 1 ) our model captures the common semantics shared in the tweet corpus textually and visually . ( 2 ) Our approach successfully tracks the topic evolution along with time .
While it is still an open problem how to quantitatively evaluate topic models , perplexity and held out likelihood have been popular measures to assess how well a topic model can be generalized to unseen documents . However , we do not use perplexity and held out likelihood , because they are not a proper metric in our evaluation for the two following reasons . First , the work of [ 6 ] performs a large scale experiment on the Amazon Mechanical Turk , and suggests that the perplexity and human judgment are often not correlated . Second , more importantly , our preliminary experiments reveal that they are not fair metrics for the comparison between the algorithms that use different distributions in the model . For example , our model shows a perplexity 10 times lower than other methods , because we model text/visual word counts using Gaussian , which always leads a higher per word likelihood than Multinomial distribution in LDA or Poisson regressor in STC .
Therefore , we quantitatively evaluate the coherence and validity of our learned topics by extending the Coherence Measure ( CM ) defined in [ 23 ] , which is inspired by human evaluation methods of [ 6 ] . Specifically , for every text topic , we select the top 10 words with the highest probabilities . Then , we ask 10 human annotators to judge whether the 10 words can be understood as a single specific topic . If not , the topic is labeled as ineffective . The annotators are further asked to scan every word and classify it as relevant or irrelevant to the topic . If more than a half of words are classified as relevant , then the topic is regarded as coherent . Similarly , for each visual topic , we use the same evaluation
VM ( Beer /Luxury ) CM ( Beer /Luxury ) dLDA
STC+dyn cdSTC+multi cdSTC+text
0.53 / 0.68 0.44 / 0.66 0.51 / 0.70
0.605 / 0.71
0.55 / 0.52 0.57 / 0.57 0.63 / 0.59 0.61 / 0.59
Table 2 : Average VM/CM comparison on text topics .
VM ( Beer /Luxury ) CM ( Beer /Luxury )
KMeans
LDA+multi cdSTC+multi
0.39 / 0.56 0.57 / 0.63 0.57 / 0.65
0.59 / 0.64 0.51 / 0.69 0.66 / 0.71
Table 3 : Average VM/CM comparison on visual topics . strategy : we first provide the labelers with the top 10 visual words of a visual topic , each of which is represented by top 10 nearest images . The labelers are asked to scan all 100 images globally to judge whether they illustrate a single specific topic . If yes , the visual topic is labeled as effective . Then the labelers classify the images in the each visual word as related or unrelated with the topic , and more than a half of images are classified as related , then the visual word is regarded as coherent with this topic . Based on the user study results , we define the validity measure ( VM ) and coherence measure ( CM ) , as two metrics of the topic quality :
V M =
# of valid topics
# of topics
, CM =
# of relevant words
# of words in valid topics
.
For experiments , we train the text only and multi view version of our model , cdSTC+text and cdSTC+multi , using the data of all time slices . We set the topic number to 50 . For the tests of text topics , we compare with two baselines : ( 1 ) dLDA [ 5 ] : dynamic LDA , and ( 2 ) STC+dyn [ 26 ] : the STC trained using the data up to t âˆ’ 1 time slice . For tests of visual topics , we compare our results with two baselines : ( 1 ) KMeans : a simple baseline of k means clustering . Specifically , we cluster the descriptor vectors of documents with images to 50 clusters , and regard each center as a topic , extract the nearest 10 images as an illustration of every center . ( 2 ) LDA+multi : A multi view LDA implemented based on [ 12 ] . Following [ 5 ] , we use the data at tâˆ’ 1 time slice for training . Table 2 and Table 3 show the average V M and CM results rated by the 10 human annotators . For text topics , our cdSTC+text achieves the best results on the V M measure , compared to the dLDA and STC+dyn models . For visual topics , our cdSTC+multi attains the highest score , which concludes that joint use of text and images help detect more human interpretable topics . 522 Monitoring Brand Competitions In this section , we demonstrate the application of our model to the market competition monitoring . Given social
1 2 3 4 5 6 7 8 9 10 11 12 13# of docs1# of docswith images# images351324(x103)(x104)(x104)2051525132051525Week(a)1 2 3 4 5 6 7 8# of docs1# of docswith images# images3505115205115224(x104)(x105)(x105)Week(b)1431 media data of multiple brands , our model can solve the following three tasks , from easy to difficult : ( 1 ) At one time slice , we monitor their occupations on latent topics . ( 2 ) Along the timeline , we monitor the trend of each brand â€™s occupation over the topics . ( 3 ) Along the timeline , we monitor the global competition trends between multiple brands .
Fig 4 illustrates the evolving chain of topic beauty on the luxury corpus in eight time slices from 2014 10 20 to 201412 15 . We also show the brand competition pie graphes and the trend curve of every brand occupation on the timeline . Our model successfully captures the topic dynamics ; the beauty topic gradually evolves with time , from makeup and lip to blackfriday , order and deals , and finally steps into winter , involving more health related words like skin care and hydra potection . The visual words also carry variations along with time , which are consistent with the text topics . The following eight pie graphes shows the competitions of the top seven brands on each time slice . We observe that ( 1 ) the dior dominates the beauty topic all the time and overwhelm the gucci , which is the largest brand that occupies almost a half of our whole corpus , and ( 2 ) other brands ( eg burberry , chanel , gucci ) show dynamic up and downs over the topics along with the time , which can be useful pieces of information for marketers . 5.3 Evaluation on Prediction
We further verify the generalization ability of the proposed model through three prediction tasks . The first two tasks are classification problems , which have been tested for evaluation in many topic model papers ( eg [ 4 , 7 , 25 , 26] ) . The third task helps marketers compare between interpolated trends and actual topic distribution side by side . 531 Prediction of Associated Brands The goal of the first prediction task is to estimate the most associated brand for a novel tweet . Although this task can be seen as a multi class classification problem where a plenty of other methods can be applied , we perform this task to prove the generalization power of our model on unseen data . For this prediction , we make two modifications to our model . First , we drop the terms related to brand competitions , which are not required for classification . Second , we develop a supervised extension to be applicable to classification problems . We use the document code as the input of a multi class max margin classifier , and jointly train the latent representations and multi class classifiers . The supervised dSTC ( sdSTC ) solves the following problem3 :
T f ( Î˜t,Mt , Dt ) + CR(Î˜t , Î·t ) + t=1 t=1
{Î˜t,Mt,Î·t}T min d â‰¥ 0 , âˆ€d , t . zt st Î¸t k âˆˆ PU , Î³t Î²t k âˆˆ PV , âˆ€k , t . dn , yt dm â‰¥ 0 , âˆ€d , n , m , t .
Î·t2
2
1 2
( 13 ) where Mt = {Î²t , Î³t} is a set of parameters , f ( Î˜t,Mt , Dt ) is the objective function for unsupervised dSTC in time slice t , and R is the multi class hinge loss :
R(Î˜t , Î·t ) =
1 |Dt| max y
( âˆ†(yd , y ) + Î· y Î¸t d âˆ’ Î· yd
Î¸t d ) ( 14 )
The above optimization problem can also be solved using the coordinated descent . Specifically , we alternate between
3Eq ( 13 ) excludes brand competition terms such as Ï† and r .
Dt d=1
Figure 5 : Comparison of accuracies of classification task ( I I ) between our methods sdSTC and sdSTC+multi and the baselines of dLDA , sLDA , and MedSTC .
Figure 6 : Comparison of accuracies of classification task ( I II ) between our methods sdSTC and sdSTC+multi and the baselines of LDA+dyn , sLDA+dyn , and MedSTC+dyn . optimizing between Î¸ and Î· . learn different Î·t for each time slice t .
It is worth noting that we
We design two experimental setups according to which data are used for training and test : ( 1 ) Task ( I I ) : we randomly divide the data in every time slice in [ 1 , t ] into two parts : 90 % for training and 10 % for test . ( 2 ) Task ( I II ) : we use the data in previous time slices [ 1 , tâˆ’ 1 ] for training , and use all the data at time t for test .
For quantitative comparison , we run the following algorithms : ( 1 ) sdSTC : our model trained using text data from all time slice . ( 2 ) sdSTC+multi : our full model with multiview extensions . ( 3 ) dLDA [ 5 ] : the dynamic LDA trained on all time slices , and then training a separate classier for each time slice . ( 4 ) MedSTC [ 26 ] : the MedSTC trained using text data from all time slices . ( 5 ) sLDA [ 25 ] : the supervised LDA trained using text from all time slices . ( 6 ) LDA+dyn [ 3 ] : the LDA trained using text data from time slice t âˆ’ 1 . ( 7 ) sLDA+dyn [ 25 ] , the supervised LDA trained using text data from time slice t âˆ’ 1 . ( 8 ) MedSTC+dyn [ 26 ] : the MedSTC trained using text data from time slice t âˆ’ 1 . Note that the baselines of ( 3)â€“(5 ) are used for task ( I I ) , while the baselines of ( 6)â€“(8 ) are for task ( I II ) .
Since the sLDA and sLDA+dyn are too slow to learn on millions of documents , we randomly partition the Beer and luxury corpus into 10 and 15 groups , respectively , and then apply the algorithm into 5 randomly chosen groups , and report the average performance .
In the task ( I I ) , the training and test data lie in the same ranges of time slices . We compare our methods sdSTC and sdSTC+multi with the baselines of dLDA , sLDA , and MedSTC . We separately acquire the accuracy in every time slice , and then report the average accuracy . Fig 5 shows that our model outperforms all the other baselines for the two corpora . The accuracy increase of our method is more significant when the the number of topics is smaller . It is mainly because we add sparse terms on both document and word codes , leading to a less noisy document representation for a small number of topics . In addition , our sdSTC+multi
( a ) Beer(b ) Luxury(a ) Beer(b ) Luxury1432 Figure 4 : The evolution of the topic beauty on the luxury corpus from 2014 10 20 to 2014 12 15 . ( a ) Text and visual words associated with the topic on the timeline . ( b ) Evolution of brand competition pie graphes at every time slice . ( c ) Variation of proportions of competing brands over the topic . using both text and images achieves slightly better accuracies than our text only sdSTC , which prove that text and images complement each other to detect better topics .
In the task ( I II ) , we compare our methods sdSTC and sdSTC+multi with the baselines of LDA+dyn , sLDA+dyn , and MedSTC+dyn . Different from the task ( I I ) , we train with the data up to time tâˆ’ 1 and perform prediction for the data at t . Fig 6 , show the results that our model achieves the best among all the methods , and the improvement of multi view model over text only model is significant , which indicates that image data is helpful to predict the future . 532 Temporal Localization The second prediction task is , given an unseen past document d = ( u , v , g ) , to predict to which time slice it is likely to belong . This is closely related to the timestamp prediction in the research of social media diffusion . Specially , we solve the following problem in dSTC : mâˆˆMd bâˆˆBd
( 15 ) p(gb|Î³t ) p(d|Mt ) , where t max p(d|Mt ) = nâˆˆNd p(un|Î²t ) p(vm|Î³t )
Figure 7 : Comparison of temporal localization accuracies between our methods dSTC+multi and cdSTC+multi and baselines dLDA+text , dLDA+multi , and dSTC+text . is the likelihood of document d given the parameters in time slice t . Similar to the task ( I I ) in the previous section , we randomly split the data of every time slice into 90 % for training and 10 % for localization test . We compare our methods dSTC+multi and cdSTC+multi ( ie with or without brand competition related terms ) with the three baselines . ( 1 ) dLDA+text : dynamic LDA with only text data , ( 2 ) dLDA+multi : multi view dynamic LDA using both text and image data , and ( 3 ) dSTC+text : dSTC with only text data . Fig 7 compares the average localization accuracies between our methods and baselines . We observe that our dSTC utilizing text , images and brands information achieve the best among all the methods . From a large accuracy rise from dLDA+multi to cdSTC+multi , we see that the explicit modeling of brand information helps improve the performance . 533 Prediction of Competition Trends In the last prediction task , we use our model to capture the market competition dynamics on the timeline . We evolve the brand competition matrix Ï† along with time , based on which we predict the future market competition trends using the past data . Specifically , we train our cdSTC model using the data in the range of [ 1 , tâˆ’ 1 ] , and then predict the brand competition at t . Since there is no groundtruth for the brand occupation over the topics , we approximate the groundtruth as follows . We manually select the most interpretable top
Figure 8 : The KL divergence D(prediction||groundtruth ) are ( bag , watch , perfume ) = ( 0.4019 0.2615 00739 ) ics , such as bag , watch , and perfume . For each topic , we collect all tweets at time slice t that contain the topic word denoted by S . Then , for each brand , we count the tweets of S that include the brand name in the text . Finally , we build an L dimensional normalized histogram , each bin of which implicitly indicates the proportion of the brand in the topic S . Fig 8 shows pie graphs comparing between the estimated Ï†t k by our method and the approximated groundtruth . In the caption , we also report the KL divergences for the three selected topics . Although it is hard to conclude that our prediction reflects well the actual proportions of brands over topics ( mainly due to lack of accurate groundtruth ) , it is interesting to see that our method can visualize brand competitions over topics in a principled way while no previous method has addressed so far . 5.4 Online Learning and Smoothing
To provide a deep understanding of our model , we empirically compare between online learning and smoothing beautymakeuplippinkglossglowcoloroptimumdrawplumperlipbeautymakeupcolorskinpinkglosseyedioraddictpalattediorbeautymencolognemakeupwomenperfumechanelcareeyebeautyhotcaremakeuplipeyepinkcolorglossmascarahealthybeautyorderskincarewinterdiornudenutritionmakeuphydrationsskindiorglasseseyewinterhydracollagenprotectionbeautyeyeglasst=1 ( 2014 10 22 ) t=2 ( 2014 10 30 ) t=3 ( 2014 11 06 ) t=4(2014 11 13 ) t=5(2014 11 20 ) t=6(2014 11 30 ) t=7(2014 12 08 ) t=8 ( 2014 12 15)dealshealthglow#salebody#diorskinclothesburberryBlackFriddayall in 1beautycaredior#Diorshowdesignerofferseyeflowchanelmascara(a)(b)(c)(a ) Beer(b ) LuxuryGroundtruthPredictionBagsPerfumeWatch1433 [ 2 ] N . Archak , A . Ghose , and P . G . Ipeirotis . Show me the
Money! Deriving the Pricing Power of Product Features by Mining Consumer Reviews . In KDD , 2007 .
[ 3 ] D . Blei , A . Ng , and M . Jordan . Latent Dirichlet Allocation .
JMLR , 3:993â€“1022 , 2003 .
[ 4 ] D . M . Blei and M . I . Jordan . Modeling Annotated Data . In
SIGIR , 2003 .
[ 5 ] D . M . Blei and J . D . Lafferty . Dynamic Topic Models . In
ICML , 2006 .
[ 6 ] J . Chang , J . L . Boyd graber , S . Gerrish , C . Wang , and
D . M . Blei . Reading Tea Leaves : How Humans Interpret Topic Models . In NIPS , 2009 .
[ 7 ] N . Chen , J . Zhu , F . Sun , and X . Eric P . Large Margin
Predictive Latent Subspace Learning for Multiview Data Analysis . IEEE PAMI , 34:2365â€“2378 , 2012 .
[ 8 ] G . Doyle and C . Elkan . Financial Topic Models . In NIPS
Workshop for Applications for Topic Models : Text and Beyond , 2009 .
[ 9 ] Y . Feng and M . Lapata . Topic Models for Image
Annotation and Text Illustration . In NAACL HLT , 2010 . [ 10 ] N . Glance , M . Hurst , K . Nigam , M . Siegler , R . Stockton , and T . Tomokiyo . Deriving Marketing Intelligence from Online Discussion . In KDD , 2005 .
[ 11 ] T . Iwata , S . Watanabe , T . Yamada , and N . Ueda . Topic
Tracking Model for Analyzing Consumer Purchase Behavior . In IJCAI , 2009 .
[ 12 ] G . Kim , C . Faloutsos , and M . Hebert . Unsupervised Modeling and Recognition of Object Categories with Combination of Visual Contents and Geometric Similarity Links . In ACM MIR , 2008 .
[ 13 ] T . Kurashima , T . Iwata , T . Hoshide , N . Takaya , and
K . Fujimura . Geo Topic Model : Joint Modeling of User â€™s Activity Area and Interests for Location Recommendation . In WSDM , 2013 .
[ 14 ] Q . Mei , X . Ling , M . Wondra , H . Su , and C . Zhai . Topic
Sentiment Mixture : Modeling Facets and Opinions in Weblogs . In WWW , 2007 .
[ 15 ] O . Netzer , R . Feldman , J . Goldenberg , and M . Fresko .
Mine Your Own Business : Market Structure Surveillance Through Text Mining . Marketing Science , 31(3):521â€“543 , 2012 .
[ 16 ] B . Oâ€™Connor , M . Krieger , and D . Ahn . TweetMotif :
Exploratory Search and Topic Summarization for Twitter . In ICWSM , 2010 .
[ 17 ] B . Pang and L . Lee . Opinion Mining and Sentiment
Analysis . Foundations and Trends in Information Retrieval , 2:1â€“135 , 2008 .
[ 18 ] J . F . Prescott and S . H . Miller . Proven Strategies in Competitive Intelligence : Lessons from the Trenches . Wiley , 2001 .
[ 19 ] I . Titov and R . McDonald . Modeling Online Reviews with
Multi grain Topic Models . In WWW , 2008 .
[ 20 ] A . Vedaldi and K . Lenc . MatConvNet â€“ Convolutional
Neural Networks for MATLAB . In CoRR , 2014 .
[ 21 ] Z . Wang , P . Cui , L . Xie , W . Zhu , Y . Rui , and S . Yang . Bilateral Correspondence Model for Words and Pictures Association in Multimedia rich Microblogs . ACM TOMM , 10:2365â€“2378 , 2014 .
[ 22 ] S . Wu , W . M . Rand , and L . Raschid . Recommendations in
Social Media for Brand Monitoring . In RecSys , 2011 .
[ 23 ] P . Xie and E . P . Xing . Integrating Document Clustering and Topic Modeling . In UAI , 2013 .
[ 24 ] K . Xu , S . S . Liao , J . Li , and Y . Song . Mining Comparative
Opinions from Customer Reviews for Competitive Intelligence . Decision Support Systems , 50:743â€”754 , 2011 .
[ 25 ] J . Zhu , A . Ahmed , and E . P . Xing . MedLDA : Maximum
Margin Supervised Topic Models . JMLR , 13:2237â€“Ë†aÂ´LË‡S2278 , 2012 .
[ 26 ] J . Zhu and E . P . Xing . Sparse Topical Coding . In UAI ,
2011 .
Figure 9 : Held out perplexity comparison between online learning and smoothing approach .
Figure 10 : Training time comparison between online learning and smoothing approach . approach . We split 5 % of data as a held out test set , and train the models using the other data from all time slices , including 1.04 and 5.23 millions of tweets for Beer and Luxury corpora with associated images . Fig 9 shows the perplexity comparison between both approaches . We observe that online learning approach achieves a slightly higher perplexity than smoothing approach4 , but both approaches does not show big difference with respect to the discovered topics and brand proportions . The training time of online learning approach is significantly shorter than that of smoothing approach , especially when the topic number is large . Therefore , online approach is more scalable on a large data set . Fig 10 shows the training time for both approaches . All experiments are performed in a single thread manner on a desktop with Intel Core I7 CPU and 32GB RAM . 6 . CONCLUSION
We have presented a dynamic topic model for monitoring temporal evolution of market competition from a large collection of tweets and their associated images . Our model is designed to successfully address three major challenges : multi view representation of text and images , competitiveness of multiple entities over shared topics , and tracking their temporal evolution . With experiments on a new twitter dataset consisting of about 10 millions of tweets and 8 millions of associated images , we showed that the proposed algorithm is more successful for the topic modeling and three prediction tasks over other candidate methods .
Acknowledgement . This work is supported by NSF Award IIS447676 . The authors thank NVIDIA for GPU donations .
7 . REFERENCES [ 1 ] A . Ahmed and E . P . Xing . Timeline : A Dynamic
Hierarchical Dirichlet Process Model for Recovering Birth/Death and Evolution of Topics in Text Stream . In UAI , 2010 .
4A lower perplexity means a better generalization performance .
Held out Perplexity on Beercorpus Held out Perplexity on Luxurycorpus Training time per time slice on Beercorpus Training time per time slice on Luxurycorpus 1434
