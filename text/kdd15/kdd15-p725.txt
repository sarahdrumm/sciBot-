Fast and Memory Efficient Significant Pattern Mining via
Permutation Testing
Felipe Llinares López
D BSSE , ETH Zürich felipe.llinares@ bsseethzch
Mahito Sugiyama ISIR , Osaka University mahito@arsanken
JST , PRESTO osaka uacjp
Laetitia Papaxanthos laetitia.papaxanthos@
D BSSE , ETH Zürich bsseethzch
Karsten M . Borgwardt
D BSSE , ETH Zürich karstenborgwardt@bsseethzch
ABSTRACT We present a novel algorithm for significant pattern mining , Westfall Young light . The target patterns are statistically significantly enriched in one of two classes of objects . Our method corrects for multiple hypothesis testing and correlations between patterns via the Westfall Young permutation procedure , which empirically estimates the null distribution of pattern frequencies in each class via permutations .
In our experiments , Westfall Young light dramatically outperforms the current state of the art approach , both in terms of runtime and memory efficiency on popular real world benchmark datasets for pattern mining . The key to this efficiency is that , unlike all existing methods , our algorithm does not need to solve the underlying frequent pattern mining problem anew for each permutation and does not need to store the occurrence list of all frequent patterns . WestfallYoung light opens the door to significant pattern mining on large datasets that previously involved prohibitive runtime or memory costs .
Our code is available from http://wwwbsseethzch/m lcb/research/machine learning/wylight.html
Categories and Subject Descriptors H28 [ Database Applications ] : Data mining
Keywords Significant pattern mining ; p value ; Multiple hypothesis testing ; Westfall Young permutation
1 .
INTRODUCTION
Frequent pattern mining is a fundamental problem in data mining , in particular in association rule mining [ 2 ] . In its most popular instance , frequent itemset mining , a user is given a database of transactions , each of which includes a set of items . In its original application [ 1 ] , each transaction represents the set of items purchased by a customer in a supermarket . A frequent itemset is then a set of items that co occur in different transactions more often than a predefined frequency threshold . Another prominent instance is frequent subgraph mining in graph databases [ 19 ] .
Significant pattern mining ( or discriminative pattern mining ) extends the classic problem of frequent pattern mining to two classes of transactions , such as cases and controls in clinical studies [ 8 ] . The interest is in finding those sets of items that occur statistically significantly more often in one class than in the other .
This problem is fundamentally important in many applications , ranging from marketing to health care . For instance , while frequent itemset mining tries to find products that are co bought by all customers , significant pattern mining tries to detect products that are co bought significantly more often by elderly customers than by younger customers . Similarly , frequent itemset mining may screen electronic health records for combinations of drugs that have frequently been administered to the same patients . In significant pattern mining , however , we may be interested in combinations of drugs that have been administered to patients who show severe side effects significantly more often than to patients who show no side effects [ 14 ] .
A critical problem in significant pattern mining is the multiple testing problem , which arises due to the fact that the number of patterns tested for association with class membership is often in the millions or billions . If not properly corrected for , significant pattern mining will retrieve a huge number of false positives ; that is , patterns deemed to be significantly associated with class membership by mistake . In most applications domains of data mining , from the natural sciences to the social sciences , these patterns are often explored further in a time consuming and cost consuming experimental validation . Therefore , a high number of a false positives is an enormous waste of resources in these fields .
In light of this multiple testing problem , current approaches to significant pattern mining face at least one of the following problems :
1 . Many methods for finding patterns that are associated with class membership do not correct for multiple testing at all ( [4 , 10 , 20 , 35 ] , see [ 36 ] for a comprehensive survey ) , even those that report a measure of statistical significance for this association [ 3 , 5 , 13 , 32 ] .
725 2 . Other methods have corrected for multiple testing using a naive Bonferroni correction , which leads to algorithms that are unable to retain statistical power if the whole search space is to be explored [ 27 , 29 , 28 ] . Therefore , those methods require that arbitrary limits be set on the maximum pattern size in order to keep the number of patterns being tested for association small .
3 . Recent approaches to correct for multiple testing in frequent itemset mining [ 24 , 18 , 22 ] , based on the seminal work by Tarone [ 23 ] , can work with arbitrary pattern sizes , but do not consider the dependence structure between patterns , which is very important in pattern mining due to the subset/superset relationship between patterns . Ignoring this dependence leads to a loss of statistical power ; that is , the ability to detect truly associated patterns .
4 . The single approach to pattern mining that corrects for multiple testing and takes pattern dependence into account [ 25 ] is expensive , either in terms of memory or runtime , and is limited in its applicability to larger datasets .
Our goal in this article is to present an approach to significant pattern mining that corrects for multiple testing , takes the dependence between patterns into account , and is efficient in terms of memory and runtime .
Below we will describe the multiple hypothesis testing problem in detail and discuss the current state of the art in significant pattern mining in Section 2 . Section 3 introduces our novel algorithm and the improvements it achieves . The experiments in Section 4 show that our method outperforms existing techniques in two instances of pattern mining ( itemset mining and subgraph mining ) across several datasets . Section 5 summarizes our key findings .
2 . BACKGROUND 2.1 Problem Statement
Let T = {t1 , t2 , t3 , , tn} be a set of transactions defined in a universe of m items . Each transaction ( or sample ) ti can be described by m binary features that indicate whether or not the corresponding items are present in the transaction . Thus , the transaction database T can be encoded as a n × m binary matrix T . Furthermore , each transaction is also tagged with a binary class label attribute C ∈ {c0 , c1} . In the following , we call pattern a set of items S = {l1 , l2 , . . . , lk} , lj ∈ {1 , . . . , m} of size k , and define the binary random variable G(S , ti ) = Ti,l1 ∧ Ti,l2 ∧ . . . ∧ Ti,lk such that G(S , ti ) = 1 if the pattern S is contained in transaction ti and G(S , ti ) = 0 otherwise . In other words , G(S , ti ) is simply an indicator binary variable that takes value 1 if pattern S is included in transaction ti and value 0 otherwise . For each pattern S , we evaluate G(S , ti ) for each transaction ti , i = 1 , . . . , n and build the following 2×2 contingency table : Variables G(S , t ) = 1 C = c1 xS − aS C = c0 n − n1 + aS − xS n − n1
Row totals aS
G(S , t ) = 0 n1 − aS n − xS
Col totals xS n1 n n denotes the total number of transactions , n1 the number of transactions with class label C = c1 ; xS is the number of transactions that include pattern S , ie the support of pattern S and , finally , aS is the number of transactions in class c1 that include pattern S ; that is , the support of pattern S among transactions of class c1 . Intuitively , our objective is to decide if the observed value of aS indicates that pattern S is over represented in one of the two classes , which would the occurrence of the pattern S within a transaction and the class labels dependent random variables . In the next section , we describe how to carry out that assessment in a statistically rigorous manner .
2.2 Statistical Association Testing In statistical association testing , the goal is to determine whether two random variables , such as G(S , t ) and C in our setting , are statistically dependent or associated . Statistical association testing is conservative , in that the null hypothesis of no association or independence between the two random variables is always assumed . Only when the data provides very strong evidence against that assumption will the null hypothesis be rejected and the random variables declared to be associated . When the two random variables to be tested for association are binary , as is the case with G(S , t ) and C , Fisher ’s exact test [ 11 ] is one of the most popular approaches . Fisher ’s exact test considers the margins ( xS , n1 , n ) of the 2× 2 conIt can be shown that , under tingency table to be fixed . the null hypothesis of independence between G(S , t ) and C , the cell count aS follows a hypergeometric distribution P ( .| xS , n1 , n ) : n1 a n − n1 xS − a
P ( aS = a| xS , n1 , n ) = n xS
In order to quantify the evidence provided by the data against the null hypothesis of independence , statistical association testing uses the concept of p values . A p value is defined as the probability of measuring an association that is at least as extreme as the one observed in the data when the null hypothesis of independence holds . Intuitively , the smaller the p value is , the less plausible the data appears to be with the null hypothesis of independence .
In the context of Fisher ’s exact test , extreme values of association refer directly to the likelihood of observing a given aS = u . In other words , all events more extreme than observing aS = u are observing cell counts aS = k that are less likely to occur under the hypergeometric distribution P ( .| xS , n1 , n ) . Mathematically , the p value is given by : P ( k | xS , n1 , n ) pS ( u ) = k | P ( k|xS ,n1,n)≤P ( u | xS ,n1,n )
Thus , pS ( u ) is the cumulative probability of all possible values of the cell count aS that are more unlikely than aS = u . In statistical association testing , an association between the two random variables is deemed significant when the p value is smaller than the significance threshold α , which must be fixed a priori . That is , a pattern will be declared significant if pS ( u ) ≤ α .
It can be shown that the probability of finding a false association that is , producing a false positive will be bounded above by α . Thus , if α is set to a low value , the discovered significant patterns are more likely to be true associations . On the other hand , using low values of α as significant thresholds causes many true associations to be missed as well . In other words , there is a trade off between Type I error ( probability of discovering a false association ) and Type II error ( probability of missing a true association )
726 that is controlled by α . The choice α = 0.05 is by far the most common , although it generally depends on the particular application . 2.3 Multiple Hypothesis Testing
If a large number d of statistical association tests at level α are performed as described in the previous section , the expected number of false positives will be approximately αd , which will lead to a large number of false discoveries . Therefore , rather than controlling the Type I error for each pattern individually , it is advisable to control the FWER ( familywise error rate ) , which is defined as the probability of producing at least one false positive , FWER = P ( FP > 0 ) , with FP denoting the number of false positives . To guarantee that FWER ≤ α , one can apply a multiple testing correction by changing the rule pS ( u ) ≤ α by pS ( u ) ≤ δ , where δ is said to be a corrected significance threshold . Ideally , the problem to be solved is :
∗
δ
= max{δ | FWER(δ ) ≤ α}
Note that it is desirable to maximize δ because larger values of the corrected significance threshold imply higher power to discover truly associated patterns , as discussed before . However , it is commonly not possible to evaluate FWER(δ ) in closed form . bon effectively 0 .
Thus , the most popular way to choose δ , the Bonferroni correction [ 7 ] , uses a much simpler , suboptimal scheme that substitutes FWER(δ ) by a loose upper bound FWER(δ ) ≤ δd , leading to δ∗ bon = α/d . Note , however , that : ( 1 ) the Bonferroni approximation FWER(δ ) ≤ δd assumes that all patterns are mutually independent , leading to an overly conservative procedure ; and ( 2 ) the Bonferroni correction is ineffective in our settings , as unless an arbitrary , restrictive limit is imposed on the maximum pattern size , d can be an extremely large number and δ∗ Permutation testing Alternatively , one of the most popular approaches for estimating FWER(δ ) is a permutation based resampling scheme proposed by Westfall and Young [ 30 ] . The idea is as follows : by randomly permuting the class labels of the transactions , one can generate a new , resampled transaction database for which no pattern S in the dataset is truly associated with the permuted class labels . Thus , in this new dataset , one can check if false positives have occurred by computing pmin = minS pS and checking whether pmin ≤ δ , in which case at least one false positive occurred ; that is , FP > 0 . If the whole procedure is repeated a sufficient number jp of times ( that is , jp = 103 or 104 ) , yielding a set { p(j ) j=1 , a good estimator of FWER(δ ) = P ( FP > 0 ) can be found as : min }jp where 1[• ] is an indicator function that takes value 1 if It is then possible its argument is true and 0 otherwise . to accurately estimate δ∗ as the α quantile of { p(j ) j=1 ; that is , choose δ∗ such that a proportion α of the values in { p(j ) j=1 are below δ∗ and the remaining values are above it . min }jp min }jp
Westfall Young permutation testing compensates for the dependence structure between patterns by directly estimating the joint null distribution of all test statistics , leading to largely increased statistical power with respect to a Bonferroni correction . The standard proof of FWER control for min ≤ δ p(j )
1 jp j=1
1 jp
FWER(δ ) =
Westfall Young permutation testing relies on a sufficient , yet not necessary , technical condition that is often hard to verify in practice ; the subset pivotality condition . Nonetheless , permutation based testing approaches are extensively used in practice and have produced numerous meaningful discoveries in such fields as computational biology [ 34 , 33 , 17 ] .
However , Westfall Young permutation testing can be extremely computationally demanding . Generating a single sample p(j ) min naively requires the enumeration of all patterns and the computation of all their corresponding p values , which is infeasible except in toy problems . More importantly , in order to obtain a reliable estimate of FWER(δ ) , a number of permutations in the order of jp = 103 or jp = 104 are required . Thus , a priori , applying permutation based testing to significant pattern mining is a challenging problem . Circumventing this computational limitation is our main goal in this article . 2.4 The FastWY Algorithm
To the best of our knowledge , the work of Terada et al . in [ 25 ] has been the only previous attempt to make permutation testing in significant pattern mining tractable . Those authors proposed FastWY , an algorithm that uses inherent properties of discrete test statistics and succeeds in reducing the computational burden that the WestfallYoung permutation based procedure entails . We introduce FastWY below , since we share the same problem setting and the key concept of the minimum attainable p value . FastWY will be used as a baseline in our experiments . The p value for a given 2× 2 contingency table in Fisher ’s exact test , or any other test statistic that assumes the margins xS , n1 and n fixed , is a function only of the cell count aS . Since 2 × 2 contingency tables are discrete objects , aS can only take a finite number of values ; that is , as ∈ [ [aS,min , aS,max ] ] with aS,min = max(0 , xS − ( n − n1 ) ) and aS,max = min(xS , n1 ) . Thus , there exists a minimum attainable p value Ψ(xS )1 strictly greater than 0 :
Ψ(xS ) = min{ pS ( u ) | aS,min ≤ u ≤ aS,max }
For Fisher ’s exact test , as the p value is a sum of positive terms , the minimum attainable p value Ψ(xS ) is reached when aS = aS,min or aS = aS,max and can be computed as a simple function of the pattern support xS . Related to the minimum attainable p value , we also introduce the set of testable patterns at significance level δ , IT ( δ ) = {S | Ψ(xS ) ≤ δ } . The word testable refers to the fact that , by definition of Ψ(xS ) , it is impossible for patterns not in IT ( δ ) to be significant at level δ .
In [ 24 ] , Terada et al . introduced a monotonically decreasing lower bound ˆΨ(xS ) on the true minimum attainable pvalue Ψ(xS ) :
ˆΨ(xS ) =
1ffi , n
Ψ(xS )
n1 < xS ≤ n
0 ≤ xS ≤ n1 , n1 and define ˆIT ( δ ) = {S | ˆΨ(xS ) ≤ δ } , which always satisfies IT ( δ ) ⊂ ˆIT ( δ ) . While this introduces some untestable patterns in the surrogate set ˆIT ( δ ) , one can rewrite ˆIT ( δ ) = {S | xS ≥ σ(δ)} with σ(δ ) = ˆΨ−1(δ ) and ˆΨ−1(δ ) welldefined due to monotonicity . This is an important observation , as it links retrieval of the sets ˆIT ( δ ) to an instance
1Ψ(xS ) also depends on the margins n1 and n but , since those are the same for all patterns we drop them to simplify the notation .
727 of frequent pattern mining , leading to a tractable scheme to enumerate testable patterns .
FastWY also exploits this concept . It is based on a decremental search scheme , which starts with the support σ = n1 . For each σ , a frequent pattern miner is first used as a black box to retrieve the set ˆIT ( σ ) . The p values pS are then computed for all S ∈ ˆIT ( σ ) and p min = min{ pS |S ∈ ˆIT ( σ)} is evaluated . If p min smaller and , therefore , p min > ˆΨ(σ ) , σ is decreased by one and the whole procedure is repeated until the condition p min ≤ ˆΨ(σ ) , no other pattern can make p min = pmin . Otherwise if p min ≤ ˆΨ(σ ) is satisfied .
If jp permutations are needed to empirically estimate the FWER , the procedure must be to repeated jp times . This includes the sequence of frequent mining problems needed to retrieve the sets ˆIT ( σ ) for each support value σ used throughout the decremental search . Given the usual range of values for jp , such an approach is as infeasible in practice as the original brute force approach .
Inspection of the code , which the authors kindly shared on their website , reveals that the actual implementation of the algorithm is different from the description in [ 25 ] . Indeed , to avoid repeating the whole frequent pattern mining process jp times , the authors resort to storing in memory the realizations of the variables G(S , ti ) for all S ∈ ˆIT ( σ ) , i = 1 , . . . , n and every value of σ explored during the decremental search . This decision corresponds to a drastic trade off between runtime and memory usage , leading to severe scalability limitations when the method is applied to even mid sized datasets , but it is an effective way to have acceptable runtime for small sized problems like the ones considered in [ 25 ] . min }jp exactly , even if only the ( cid:100)αjp smallest values in that set are actually involved in the computation of δ∗ as the α quantile of the set . This is problematic since ˆΨ−1(δ ) is a monotonically decreasing function , which means that the larger p(j ) min is , the smaller the minimum support for frequent pattern mining will be , requiring more enumeration runtime . Furthermore , if jp is large , there is a high probability that some of the p(j ) min will be rather large , creating a bottleneck in the algorithm . Nonetheless , a priori , it seems unclear how the ( cid:100)αjp smallest values in { p(j ) j=1 could be identified without first evaluating them all .
Moreover , the algorithm requires computing all { p(j ) min }jp j=1
In the next section , we propose a novel algorithm , WestfallYoung light , which overcomes all the scalability limitations of FastWY , leading to a large scale , permutation based significant pattern miner with FWER control .
3 . WESTFALL YOUNG LIGHT
In this section we present our contribution , the WestfallYoung light algorithm . Subsection 3.1 describes the method , starting from its pseudocode and then explaining the different steps in detail . Subsection 3.2 discusses the theoretical foundations of Westfall Young light , proving that it provides exactly the same solution as Westfall Young permutationtesting . Finally , in Subsection 3.3 we analyze all the improvements that Westfall Young light provides over the current state of the art method , FastWY . 3.1 The Algorithm : Westfall Young Light
The pseudocode of Westfall Young shown Algorithm 1 is composed of two parts : ( 1 ) the initialization part , function Westfall Young Light ; and ( 2 ) the core function Process
Algorithm 1 Westfall Young light for j = 1 , . . . , jp do permutations jp , and target FWER α
1 : Input : Transaction database T , class labels c , number of 2 : Output : Corrected significance threshold δ∗ 3 : function Westfall Young Light(α , jp , T , c ) 4 : 5 : 6 : 7 : 8 : 9 : c(j ) ← randperm(c ) min ← 1 p(j ) end for σ ← 1 , δ ← ˆΨ(σ ) ProcessNext(root , n )
10 :
Return α quantile of jp p(j ) min j=1
Compute p values pS ( u ) for all u ∈ [ aS,min , aS,max ] for j = 1 , . . . , jp do jp
Compute a(j)S min , pS ( a(j)S ) } min ← min{ p(j ) jp p(j ) end for min ≤ δ FWER(δ ) ← 1 p(j ) j=1 1 jp while FWER(δ ) > α do σ ← σ + 1 , δ ← ˆΨ(σ ) FWER(δ ) ← 1
11 : end function 12 : function ProcessNext(S , xS ) 13 : 14 : 15 : 16 : 17 : 18 : 19 : 20 : 21 : 22 : 23 : 24 : 25 : 26 : 27 : 28 : 29 : end function end while for S ∈ Children(S ) do
Compute xS if xS ≥ σ then
ProcessNext(S,xS ) end for end if jp j=1 1 min ≤ δ p(j )
Next , which processes each enumerated pattern and continues the enumeration recursively in a depth first manner .
Westfall Young Light function First , in Lines 4 7 , we precompute all jp permuted class label vectors c , which can be stored as a binary matrix of size n × jp . The set of minimum p values for each of the jp permutations , { p(j ) j=1 , is initialized to 1 ; that is , the maximum value a p value can take . Next , Lines 8 and 9 initialize the minimum support σ to 1 and compute the corresponding corrected significance threshold δ as the minimum attainable p value for patterns with support σ . Note that the choice σ = 0 is trivial , as the corresponding δ would be δ = 1 , deeming all patterns both testable and significant . min }jp
After initialization , we must start the pattern enumeration procedure . Patterns are enumerated as nodes of a tree such that children S of a pattern S have supports xS ≤ xS . This is a common assumption for classical pattern mining problems such as itemset , subgraph , or string mining [ 24 , 22 , 17 ] . Line 12 begins the enumeration process at the root of this tree by calling the ProcessNext function , which will continue enumerating patterns by exploring the tree with a depth first strategy . Note that if the pattern tree has no clear root , one can always define a dummy root as an empty itemset without loss of generality . ProcessNext function The ProcessNext function processes every enumerated pattern S , one at a time .
First , in Line 13 , we precompute all possible p values for a hypergeometric random variable with parameters xS , n1 and n . This precomputation technique is one runtime improve
728 ment in our algorithm . It is a consequence of the fact that , for fixed xS , n1 and n , the computational complexity of evaluating Fisher ’s exact test p values pS ( aS ) for a single value of aS or for all aS ∈ [ aS,min , aS,max ] is the same and equal to O(min{xS , n1} ) . This property can be readily checked from the definition of Fisher ’s exact test , since the main computational burden is evaluating the probability mass of the hypergeometric random variable with parameters xS , n1 and n , a computation that can be shared across all jp permutations . This reduces the computational complexity of this step from O(jp min{xS , n1} ) to O(min{xS , n1} ) , with jp ≈ 104 . Thus , this novel “ trick ” makes the time complexity of evaluating Fisher ’s exact test negligible . Such an optimization is only possible if all jp permutations are processed at the same time for each pattern S ; therefore , it is not feasible with the decremental scheme of FastWY . min if pjS < p(j )
In Lines 14 17 we compute the cell counts ajS for all permutations j = 1 , . . . , jp and fetch the corresponding p values pjS = pS ( a(j)S ) , updating p(j ) min if needed . We then also update the current estimate of the FWER at level δ in Line 18 . Next , between Lines 19 22 , we check if the current estimate of the FWER is too large ; that is , above the target FWER α . If it is , we must decrease the current threshold δ by increasing the minimum support σ until the empirical FWER is again below α . Note that every time σ is increased , δ decreases , which means that the empirical estimate of the FWER must be updated .
Finally , between Lines 23 28 , the pattern enumeration process continues ; the pattern tree is explored in a depthfirst manner along every child of pattern S that is frequent at the current support σ .
This combination of frequent pattern enumeration and adaptive threshold adjustment continues until all patterns for a certain minimum support σf have been enumerated . That is , σf is the minimum support when Westfall Young light finishes . Then , the original call to ProcessNext in Line 9 is completed and the algorithm terminates by returning the solution δ∗ as the α quantile of the set { p(j ) j=1 . 3.2 Correctness of Westfall Young Light min }jp
In this subsection , we will prove that Westfall Young light correctly obtains the optimal δ∗ based on the Westfall Young FWER estimator .
Theorem 1 . ( Correctness of Westfall Young Light ) The Westfall Young light algorithm returns the exact solution to δ∗ = max{δ | FWER(δ ) ≤ α} , where FWER(δ ) is the empirical Westfall Young FWER estimator FWER(δ ) = 1 jp jp min ≤ δ p(j ) The proof of Theorem 1 is based on exploiting the follow described in Section 23
1 j=1 ing properties of the FWER :
Proposition 1 . For a fixed δ , processing a new pattern S can never make the empirical FWER estimate decrease . Proof . Let M be the current set of patterns and S be a new pattern . We have min{ pR |R ∈ M ∪ {S}} ≤ min{pR | R ∈ M} and hence 1[ min{pR | R ∈ M} ≤ δ ] ≤ 1[ min{pR | R ∈ M ∪ {S}} ≤ δ ] .
Proposition 2 . FWER(δ ) with δ < ˆΨ(σ−1 ) can be evaluated exactly using only the p values of patterns with support xS ≥ σ . Proof . First note that xS ≥ σ ⇔ S ∈ ˆIT ( ˆΨ(σ) ) . Let min = min{ pS | S ∈ ˆIT ( ˆΨ(σ))} . If p p min > δ then pmin > δ , because pS ≥ ˆΨ(σ − 1 ) > δ ∀S ∈ IT ( ˆΨ(σ ) ) by definition of ˆIT ( • ) . Thus , 1[ pmin ≤ δ ] = 1[ p min ≤ δ ] .
Proof of Theorem 1 . Proposition 1 guarantees that every time FWER(δ ) > α in Line 20 of Algorithm 1 , we know that δ∗ < δ and must therefore decrease the current threshold δ . This is because even if the current estimate FWER(δ ) is still noisy due to the many remaining patterns not yet processed , we know that processing further patterns will only make FWER(δ ) larger .
Finally , Proposition 2 guarantees that , at convergence , Westfall Young light has all the information it needs to compute δ∗ . Let σf be the minimum support at convergence and δf = ˆΨ(σf ) the associated corrected significance threshold . Similarly , let δf,prev = ˆΨ(σf − 1 ) be the immediately preceding corrected significance threshold . By construction of the algorithm , FWER(δf ) ≤ α and FWER(δf,prev ) > α . Thus , δ∗ ∈ [ δf , δf,prev ) . By Proposition 2 , we can then only evaluate exactly FWER(δ ) for all δ < δf,prev , but since δ∗ < δf,prev , that is all we need . In fact , this implies that our algorithm does not waste time computing exactly the values of { p(j ) j=1 that are above δ∗ , translating into a much larger final minimum support σf for pattern enumeration . 3.3 Improvements over FastWY min }jp
Compared to the current state of the art FastWY , our proposal improves the following scalability aspects : ( I1 ) Instead of a decremental search strategy , it is based on an incremental search strategy , which recent studies have shown to be more efficient than decremental search [ 18 , 22 ] . The incremental search is implemented in the initialization of the support σ in Line 8 of Algorithm 1 and when σ is updated in Line 20 .
( I2 ) It computes the solution δ∗ enumerating the frequent patterns only once , instead of jp times . Moreover , it does so without any non scalable memory overheads such as storing G(S , ti ) for all S ∈ ˆIT ( σ ) , i = 1 , . . . , n . Indeed , Line 25 prunes the patterns according to the support xS . Non pruned patterns are processed using the function ProcessNext , which computes the jp pvalues corresponding to all jp permutations simultaneously . There is no longer any need to repeat pattern mining once per permutation or store occurrence lists G(S , ti ) in memory . ( I3 ) It does not need to compute the ( 1− α)jp largest values of { p(j ) j=1 exactly , which greatly increases the minimum support for pattern enumeration and thus reduces the runtime . It also decreases the number of cell counts that need to be evaluated , further reducing the overall runtime . This is a consequence of the incremental search strategy described in the previous point and the natural stopping criteria of the method via ending of the depth first enumeration recursion of function ProcessNext . min }jp
( I4 ) It uses an efficient scheme to share the computation of p values across permutations , reducing the corresponding runtime for that task by a factor of jp , with jp in the order of 104 . Therefore , the runtime for p value evaluation in Westfall Young light is negligible for all practical purposes . This is implemented via the precomputation of p values in Line 13 , which involves time complexity O(min{n1 , xS} ) independent of jp and neg
729 Figure 1 : Simulation results . Runtimes beyond 12 hours are extrapolated . Note that both the x and y axes have logarithmic scale . Data show means±SD in 10 trials . ligible storage complexity . Then , for each permutation j , we simply fetch the corresponding p value from the precomputed ones . In contrast , FastWY caches every p value computation in memory , creating an additional memory overhead .
In the next section , we empirically demonstrate that , as a consequence of improvements 1 4 , Westfall Young light has much better memory usage scaling and can be up to three orders of magnitude faster in real world data than FastWY .
4 . EXPERIMENTS
We evaluate our proposed method Westfall Young light on a wide range of synthetic and real world datasets in two representative data mining problems : significant itemset mining and significant subgraph mining , compared to the current state of the art FastWY .
4.1 Experimental Setup
Both Westfall Young light and FastWY were written in C/C++ . Since FastWY was implemented in Python by its authors , we reimplemented it in C/C++ for a fair comparison . This new implementation of FastWY used as a baseline is about two or three orders of magnitude faster and reduces the amount of RAM used by one or two orders of magnitude . As a frequent closed itemset miner , we chose LCM version 3 [ 26 ] for both Westfall Young light and FastWY . LCM has been shown to exhibit state of the art performance in a large number of datasets and won the FIMI’04 frequent itemset mining competition [ 12 ] . The code was compiled using Intel C++ compiler version 1401 and executed on a single 2.7 GHz Intel Xeon CPU with 256 GB of memory . In subgraph mining , we employed Gaston [ 19 ] as a frequent subgraph miner2 because it is reported to be one of the fastest algorithms [ 31 ] . The code was compiled with gcc 482 and run on a single 2.5 GHz Intel Xeon CPU with 256 GB of memory .
2Code available from http://wwwliacsnl/~snijssen/ga ston/iccs.html
4.2 Evaluation on Synthetic Data
First we analyze Westfall Young light on synthetic data to evaluate the improvement of overall runtime and memory usage compared to FastWY and the contribution of each improvement from ( I1 ) to ( I4 ) in Section 3 . Note that ( I1 ) and ( I3 ) contribute to the runtime efficiency and ( I2 ) , ( I3 ) , and ( I4 ) to the memory efficiency .
We randomly generated a set of transactions with n/n1 = 2 and m = 103 , where the size of each transaction is 50 , and we varied n from 103 to 105 . Then 10 true causal itemsets S1 , . . . ,S10 with |Si| = 5 were prepared , which were also randomly generated from the same domain of items , and each transaction with the label 1 contains one of those itemsets . Results are plotted in Figure 1 . The overall runtime in Figure 1a clearly shows that Westfall Young light is one to two orders of magnitude faster than FastWY , and the difference gets larger as n increases . To explore the reason for this difference , we solely ran LCM with the decremental or incremental scheme , omitting the computational effort for permutation testing that is shared between methods . In addition , to separate the improvement ( I1 ) and ( I3 ) , we also ran LCM with the decremental scheme by using the final minimum support obtained by our method . In our results , plotted in Figure 1b , the difference between a green line and a green dashed line shows the contribution of ( I3 ) , and the difference between a green dashed line and a blue line shows the contribution of ( I1 ) . Therefore , our improvement ( I1 ) of employing the incremental search strategy is crucial for the efficiency .
The peak memory usage of our method is larger than that of FastWY if n is small , but gets smaller once n exceeds 104 and is an order of magnitude smaller if n ≥ 5 · 104 ( Figure 1c ) . This comes from the aggregation of our three improvements ( I2 ) , ( I3 ) , and ( I4 ) , the contributions of which are demonstrated in Figures 1d , e , and f , respectively . Figure 1d shows the memory usage for a permutation matrix in our method ( green ) and storing all frequent patterns in FastWY ( blue ) . We can see that the amount of memory for storing frequent patterns finally exceeds that for a permutation matrix . This is due to the number of frequent patterns exponentially increasing with n , while the amount aLCM with incremental scheme with final minimum support of Westfall Young lightLCM with decremental scheme with worst case minimum support of FastWYLCM with decremental scheme with final minimum support of Westfall Young lightcdef1000500020000100000nExecution time ( s)10–110103105Runtime improvement of ( I2 ) and ( I4)nExecution time ( s)1102104106Overall runtime1000500020000100000Westfall Young lightFastWYbnMemory usage ( MB)100050002000010000010102103104105Overall memory usageWestfall Young lightFastWYnMemory usage ( MB)100050002000010000010–110103105Memory improvement of ( I2)Permutation matrixFrequent patternsnMemory usage ( MB)100050002000010000010–110103105Memory improvement ( I3)LCM with final minsup . ofWestfall Young light LCM with worst caseminsup . of FastWYnMemory usage ( MB)100050002000010000010–110103105Memory improvement ( I4)p value cache in FastWY730 Table 1 : Characteristics of itemset mining datasets . n and n1 are the number of transactions in total and in the minor class , respectively , m is the number of items and |t|/n the average transaction size . n/n1 is shown only for labeled datasets . y t r e p o r P n n/n1 m |t|/n e o T c a T c i T
958 2.89 18 6.93 s s e h C s d a t e n I m o o r h s u M r e c n a c t s a e r B r a t s b s m u P t c e n n o C w e i v b e W m B s l i a t e R
K 0 0 1 D 4 I 0 1 T
K 0 0 1 D 0 1 I 0 4 T s o p s m B
3196 − 75
37.00
3279 7.14 1554 12.00
8124 2.08 117 22.00
12773 11.31 1129 6.70
49046
− 7117 50.48
67557
− 129 43.00
77512
− 3340 4.62
88162
−
16470 10.31
100000
− 870 10.10
100000
− 942 39.61
515597
− 1657 6.53
Table 2 : Characteristics of subgraph mining datasets , where |V | and |E| denote the number of vertices and edges , respectively . y t r e p o r P n n/n1 max|V | max|E|
) R M
( C T P 584 3.23 181 181
) R F ( C T P 584 3.74 181 181
)
M M
( C T P 576 3.18 181 181
)
M F ( C T P 563 3.15 181 181
G A T U M
188 2.98 28 66
S E M Y Z N E 600 2.00 126 149
D & D
1 I C N
1 4 I C N
9 0 1 I C N
7 6 1 I C N
1178 2.42 5748 14267
4208 2.00 462 468
27965 17.23 462 468
4256 2.00 462 468
80581 8.38 482 478
0 2 2 I C N
900 3.10 239 255 of memory for storing a permutation matrix scales linearly . Moreover , the improvement ( I3 ) results in less memory usage in LCM ( shown in Figure 1e ) due to a larger minimum support , and ( I4 ) saves the extra amount of memory for the p value cache in FastWY in Figure 1f . Interestingly , the overall memory usage decreases if n = 2·105 . This is because the final minimum support rises , resulting in less memory usage in LCM as shown in Figure 1e . 4.3 Evaluation on Real Data
Itemset Mining Datasets : We used four labeled datasets : TicTacToe3 , Inetads4 , Mushroom , and Breast cancer . The first three are commonly studied datasets taken from the UCI repository and Breast cancer is described in [ 25 ] .
In addition , we used eight unlabeled datasets from the well known public benchmark datasets for frequent itemset mining [ 12 ] : Bmspos , BmsWebview , Retail , T10I4D100K , T40 I10D100K , Chess , Connect and Pumsb star . Since labels only affect the algorithm via n and n1 as far as finding the corrected significance threshold δ∗ is concerned , we considered two representative cases : n/n1 = 2 or 10 . Higher ratios n/n1 are usually more computationally demanding . This results in a total of 20 different cases to be tested . The main properties of each dataset are summarized in Table 1 . Subgraph Mining Datasets : We used 12 labeled graph datasets : four PTC ( Predictive Toxicology Challenge ) datasets5 , MUTAG , ENZYMES , D&D6 , and four NCI ( National Cancer Institute ) datasets7 , where ENZYMES and D&D are proteins and others are chemical compounds . These datasets are popular benchmarks and have been frequently used in previous studies ( eg [ 16 , 21] ) . Graph nodes are labeled in all datasets and edges are also labeled except for ENZYMES and D&D . In the four PTC datasets , graphs labeled as CE ,
3https://archiveicsuciedu/ml/datasets/Tic Tac T oe+Endgame 4https://archiveicsuciedu/ml/datasets/Internet +Advertisements 5http://wwwpredictive toxicologyorg/ptc/ 6MUTAG , ENZYMES , and D&D are obtained from http://mlcbistuebingenmpgde/Mitarbeiter/Nino/ Graphkernels/data.zip 7https://pubchemncbinlmnihgov/
SE , or P were treated as positive and those of NE or N as negative , the same setting as in [ 15 ] . The properties of these datasets are summarized in Table 2 .
Note that the number of nodes in subgraphs is bounded by 15 in NCI1 , NCI109 , and NCI220 , 10 in MUTAG , NCI41 , and NCI167 , and 8 in ENZYMES such that the comparison partner , FastWY , can finish in a reasonable time , allowing us to check its peak memory consumption . For example , in ENZYMES with the maximum subgraph size 10 , our method takes 3.6 hours while FastWY did not stop after two weeks . In D&D and the four PTC datasets , the size of subgraphs is unlimited .
Runtime and memory usage As the main result , we compare the runtime and memory usage of our method Westfall Young light and the comparison partner FastWY for all 20 itemset mining cases and 12 subgraph mining datasets . In both algorithms , the number jp of permutations is the only parameter and is set jp = 104 . We defer the discussion about the effect ofjp to the end of this section . The target FWER α is never to be treated as a parameter , but as a user requirement fixed a priori before seeing the data . Here we use α = 0.05 , which is the most standard choice across different scientific disciplines .
The results for significant itemset mining are summarized in Figure 2 , and those of significant subgraph mining are summarized in Figure 3 . Overall , our algorithm WestfallYoung Light tends to be two to three orders of magnitude faster than FastWY in itemset mining and one to two orders of magnitude faster in subgraph mining . The exact difference in runtime is dataset dependent , but there is a clear trend showing that , for large scale datasets , the runtime gap between FastWY and Westfall Young light increases sharply . Even more importantly , six out of 20 significant itemset datasets correspond to the datasets Chess , Pumsb star and Connect , for which FastWY crashed due to excessive memory requirements . Indeed , as far as memory usage is concerned , we see two different situations : ( 1 ) in 17 out of 32 datasets , both methods use approximately the same amount of memory ( up to the order of magnitude ) ; ( 2 ) in the others , the peak memory usage of FastWY is much larger than
731 ( a ) Runtime
( a ) Runtime
( b ) Peak memory usage
Figure 3 : Performance comparison between Westfall Young light and FastWY in subgraph mining with jp = 104 permutations . The memory usage due to Gaston is included .
( b ) Peak memory usage
Figure 2 : Performance comparison between Westfall Young light and FastWY in itemset mining with jp = 104 . Numbers attached to dataset names denote the class ratio n/n1 . that of Westfall Young light , in some cases soaring up to the point in which the algorithm simply breaks down . It should also be noted that for most of the 17 datasets in which both methods have a similar memory footprint , the frequent pattern miner ( LCM or Gaston ) is responsible for almost the entire memory usage , which means there is little room for improvement . In general , the trend for the memory usage gap is similar to that for the runtime gap : FastWY only works properly in small sized problems and shows poor scaling characteristics in larger datasets . With respect to memory usage , this is clearly due to the need to store G(S , ti ) for all S ∈ ˆIT ( σ ) , i = 1 , . . . , n . As a consequence , the peak memory overhead of FastWY scales with the number of frequent patterns enumerated by the algorithm , which tends to increase exponentially with the database size and density . In contrast , Westfall Young light has a much smaller memory overhead that does not increase with the number of frequent patterns ; it only increases linearly with the number of transactions n .
The actual memory usage of FastWY for the six out of 20 significant itemset mining datasets in which it crashed is actually only lower bounded in Figure 2b ; the numbers we plotted are a conservative lower bound on what the actual memory usage would have been , obtained simply by counting the number of testable patterns processed by Westfall
Young light and computing the amount of memory needed to store G(S , ti ) for all S ∈ ˆIT ( σ ) , i = 1 , . . . , n . This neglects : ( 1 ) all memory needed by LCM itself ; ( 2 ) the fact that FastWY produces many more testable patterns due to computing the entire set { p(j ) j=1 exactly . min }jp min }jp
Final minimum support for frequent pattern mining Since FastWY computes all values { p(j ) j=1 exactly , the final minimum support for frequent pattern mining is determined by maxj p(j ) min . In contrast , Westfall Young light has its final minimum support determined by the ( cid:100)αjp largest values only . The impact on the final support is shown quantitatively in Figure 4 . It can be seen from the figure that , in most datasets , the final minimum support of FastWY is considerably larger than that of Westfall Young light .
Most databases obey a power law distribution , which makes patterns with low supports much more abundant than those with large supports . This effect is particularly harmful for FastWY as the memory overhead needed to store G(S , ti ) for all S ∈ ˆIT ( σ ) , i = 1 , . . . , n scales with the number of frequent patterns at the final minimum support .
Statistical power of permutation testing Measuring the resulting FWER is a powerful proxy to compare the statistical power of several FWER controlling methods . Optimal schemes should achieve a FWER as close as possible to α without it ever being larger . If FWER α , the method is overly conservative , and will therefore have a higher probability of missing true positives . In this section ,
732 ( a ) Itemset mining .
( a ) BmsWebview ( n/n1 = 2 )
( b ) T40I10D100K ( n/n1 = 2 )
Figure 5 : Empirical FWER versus jp for two representative itemset mining databases .
( b ) Subgraph mining .
Figure 4 : Comparison of final minimum support between Westfall Young light and FastWY . Datasets for which FastWY crashed due to memory limitations were excluded . we compare the resulting FWER of Westfall Young permutation testing based methods to that of LAMP , δLAMP [ 24 ] , which is the first prominent method that controls the FWER in significant pattern mining . LAMP applies an improved Bonferroni correction that also exploits the concept of minimum attainable p value Ψ(xS ) , but does not correct for the dependence between patterns , thus making it a baseline to measure the statistical power of Westfall Young permutation testing approaches . Note that the resulting FWER of FastWY and Westfall Young light is identical since both use the same underlying statistical procedure .
As far as parameters are concerned , we must only deal with the number jp of permutations . Intuitively , the tradeoff involved when setting jp is clear : the larger jp , the more precise the estimation of δ∗ will be at the expense of increased runtime . To illustrate the effect of changes in the number jp of permutations , we executed Westfall Young light for 10 different values of jp between jp = 103 and jp = 104 in steps of ∆jp = 103 . For each pair ( dataset , jp ) we repeat the execution 100 times and show the median empirical FWER as a function of jp , along with the corresponding 5%–95 % confidence interval .
We depict the results for four sample datasets due to space considerations : two datasets in itemset mining ( BmsWebview and T40I10D100K ) in Figure 5 and two datasets in subgraph mining ( ENZYMES and NCI220 ) in Figure 6 . As the figure shows , the median FWER appears to be fairly stable to changes in jp and , more importantly , the spread of the empirical FWER saturates at about jp = 104 . Therefore , we believe jp = 104 to be the safest parameter choice . Using
( a ) ENZYMES .
( b ) NCI220 .
Figure 6 : Empirical FWER versus jp for two representative subgraph mining databases . jp = 103 might still lead to good performance while reducing the runtime by approximately one order of magnitude .
Finally , our results also clearly demonstrate that LAMP , while much more effective than a standard Bonferroni correction ( which would have a FWER extremely close to 0 ) , is still an overly conservative algorithm . It tends to yield an empirical FWER that oscillates between α/2 and α/100 depending on the dataset . Even just halving the target FWER can have drastic consequences and cause a large amount of significant patterns to be lost as a consequence of overcontrolling the FWER ; this point justifies the need to use permutation testing based approaches if optimal statistical power is required .
5 . CONCLUSIONS
In this paper , we have described a novel algorithm for mining statistically significant patterns , called Westfall Young light , which allows users to adjust the probability of having false discoveries . The algorithm estimates the null distribution of the test statistics via Westfall Young permutations , and succeeds in overcoming the massive computational cost of permutation testing in large databases by exploiting a set of computational tricks .
Empirically , our Westfall Young light algorithm drastically improves upon the state of the art . The runtime decreases by up to three orders of magnitude and the peak memory usage by up to two orders of magnitude in several itemset and subgraph mining benchmarks . We also show that the peak memory usage of Westfall Young light scales gently with the complexity of the database . In contrast , the peak memory usage of the state of the art algorithm soars as
733 the databases become large and dense , thus breaking down in large scale problems .
Several interesting challenges still remain to be addressed . In domains such as computational biology , there is increasing interest in less conservative statistical testing procedures that enjoy increased statistical power , such as FDR control [ 6 ] . Another critical problem is how to correct for confounders [ 9 ] . Those are predictive features that are correlated to both the target response and some of the patterns , artificially inflating the resulting p values . Extending the framework in either of those directions would represent a valuable contribution . 6 . ACKNOWLEDGMENTS
This work was funded in part by the Alfried Krupp von Bohlen und Halbach Stiftung ( KB ) , the SNSF Starting Grant ‘Significant Pattern Mining’ ( KB ) , a Grant in Aid for Scientific Research ( Research Activity Start up ) 26880013 ( MS ) and the Marie Curie Initial Training Network MLPM2012 , Grant No . 316861 ( FLL , KB ) . 7 . REFERENCES [ 1 ] C . C . Aggarwal and J . Han , editors . Frequent Pattern
Mining . Springer , 2014 .
[ 2 ] R . Agrawal , T . Imieli´nski , and A . Swami . Mining association rules between sets of items in large databases . In SIGMOD , pages 207–216 , 1993 .
[ 3 ] A . Arora , M . Sachan , and A . Bhattacharya . Mining statistically significant connected subgraphs in vertex labeled graphs . In SIGMOD , pages 1003–1014 , 2014 .
[ 4 ] M . Atzmueller and F . Puppe . SD Map – A fast algorithm for exhaustive subgroup discovery . In PKDD , volume 4213 of LNCS , pages 6–17 , 2006 .
[ 5 ] S . D . Bay and M . Pazzani . Detecting group differences :
Mining contrast sets . Data Mining and Knowledge Discovery , 5(3):213–246 , 2001 .
[ 6 ] Y . Benjamini and Y . Hochberg . Controlling the false discovery rate : A practical and powerful approach to multiple testing . Journal of the Royal Statistical Society . Series B , 57(1):289–300 , 1995 .
[ 7 ] C . E . Bonferroni . Teoria statistica delle classi e calcolo delle probabilit`a . Pubblicazioni del R Istituto Superiore di Scienze Economiche e Commerciali di Firenze , 8:3–62 , 1936 .
[ 8 ] G . Dong and J . Bailey . Contrast Data Mining : Concepts ,
Algorithms , and Applications . Chapman&Hall/CRC , 2012 . [ 9 ] M . P . Epstein , R . Duncan , Y . Jiang , K . N . Conneely , A . S .
Allen , and G . A . Satten . A permutation procedure to correct for confounders in case control studies , including tests of rare variation . Am J Hum Genet , 91:215–223 , 2012 . [ 10 ] W . Fan , K . Zhang , H . Cheng , J . Gao , X . Yan , J . Han , P . S . Yu , and O . Verscheure . Direct mining of discriminative and essential frequent patterns via model based search tree . In SIGKDD , pages 230–238 , 2008 .
[ 11 ] R . A . Fisher . On the Interpretation of χ2 from Contingency
Tables , and the Calculation of P . Journal of the Royal Statistical Society , 85(1):87–94 , 1922 .
[ 12 ] B . Goethals and M . J . Zaki . Frequent itemset mining dataset repository ( FIMI’04 ) . http://fimiuaacbe/data/ , 2004 .
[ 13 ] W . H¨am¨al¨ainen . StatApriori : an efficient algorithm for searching statistically significant association rules . Knowledge and Information Systems , 23(3):373–399 , 2010 .
[ 14 ] J . Hopstadius and G . N . Nor´en . Robust discovery of local patterns : Subsets and stratification in adverse drug reaction surveillance . In ACM SIGHIT , pages 265–274 , 2012 .
[ 15 ] X . Kong and P . S . Yu . Semi supervised feature selection for graph classification . In SIGKDD , pages 793–802 , 2010 .
[ 16 ] G . Li , M . Semerci , B . Yener , and M . J . Zaki . Effective graph classification based on topological and label attributes . Statistical Analysis and Data Mining , 5(4):265–283 , 2012 .
[ 17 ] F . Llinares L´opez , D . G . Grimm , D . A . Bodenham ,
U . Gieraths , M . Sugiyama , B . Rowan , and K . M . Borgwardt . Genome wide detection of intervals of genetic heterogeneity associated with complex traits . Bioinformatics , in press , 2015 .
[ 18 ] S . Minato , T . Uno , K . Tsuda , A . Terada , and J . Sese . A fast method of statistical assessment for combinatorial hypotheses based on frequent itemset enumeration . In ECMLPKDD , volume 8725 of LNCS , pages 422–436 , 2014 . [ 19 ] S . Nijssen and J . N . Kok . A quickstart in frequent structure mining can make a difference . In SIGKDD , pages 647–652 , 2004 .
[ 20 ] P . K . Novak , N . Lavraˇc , and G . I . Webb . Supervised descriptive rule discovery : A unifying survey of contrast set , emerging pattern and subgroup mining . JMLR , 10:377–403 , 2009 .
[ 21 ] N . Shervashidze , P . Schweitzer , E . J . van Leeuwen ,
K . Mehlhorn , and K . M . Borgwardt . Weisfeiler Lehman graph kernels . JMLR , 12:2359–2561 , 2011 .
[ 22 ] M . Sugiyama , F . Llinares L´opez , N . Kasenburg , and K . M .
Borgwardt . Significant subgraph mining with multiple testing correction . In SDM , pages 37–45 , 2015 .
[ 23 ] R . E . Tarone . A modified bonferroni method for discrete data . Biometrics , 46(2):515–522 , 1990 .
[ 24 ] A . Terada , M . Okada Hatakeyama , K . Tsuda , and J . Sese .
Statistical significance of combinatorial regulations . Proceedings of the National Academy of Sciences , 110(32):12996–13001 , 2013 .
[ 25 ] A . Terada , K . Tsuda , and J . Sese . Fast westfall young permutation procedure for combinatorial regulation discovery . In IEEE International Conference on Bioinformatics and Biomedicine , pages 153–158 , 2013 .
[ 26 ] T . Uno , T . Asai , Y . Uchida , and H . Arimura . An efficient algorithm for enumerating closed patterns in transaction databases . In Discovery Science , volume 3245 of LNCS , pages 16–31 , 2004 .
[ 27 ] G . Webb . Discovering significant rules . In L . Ungar ,
M . Craven , D . Gunopulos , and T . Eliassi Rad , editors , SIGKDD , pages 434 – 443 , New York , 2006 . The Association for Computing Machinery .
[ 28 ] G . Webb . Layered critical values : A powerful direct adjustment approach to discovering significant patterns . Machine Learning , 71(2 3):307–323 , 2008 .
[ 29 ] G . I . Webb . Discovering significant patterns . Machine
Learning , 68(1):1–33 , 2007 .
[ 30 ] P . Westfall and S . S . Young . Resampling Based Multiple
Testing : Examples and Methods for P Value Adjustment . Wiley , 1993 .
[ 31 ] M . W¨orlein , T . Meinl , I . Fischer , and M . Philippsen . A quantitative comparison of the subgraph miners MoFa , gSpan , FFSM , and Gaston . In PKDD , volume 3721 of LNCS , pages 392–403 . Springer , 2005 .
[ 32 ] X . Yan , H . Cheng , J . Han , and P . S . Yu . Mining significant graph patterns by leap search . In SIGMOD , pages 433–444 , 2008 .
[ 33 ] X . Zhang , S . Huang , F . Zou , and W . Wang . TEAM :
Efficient two locus epistasis tests in human genome wide association study . Bioinformatics , 26(12):i217–i227 , 2010 .
[ 34 ] X . Zhang , F . Zou , and W . Wang . FastANOVA : an efficient algorithm for genome wide association study . In SIGKDD , pages 821–829 , 2008 .
[ 35 ] A . Zimmermann , B . Bringmann , and U . R¨uckert . Fast , effective molecular feature mining by local optimization . In ECMLPKDD , volume 6323 of LNCS , pages 563–578 , 2010 . [ 36 ] A . Zimmermann and S . Nijssen . Supervised Pattern Mining and Applications to Classification , pages 425–442 . Springer , 2014 .
734
