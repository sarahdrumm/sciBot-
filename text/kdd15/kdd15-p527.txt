Scalable Blocking for Privacy Preserving Record Linkage
Alexandros Karakasidis Sch . of Science & Technology
Hellenic Open University akarakasidis@eapgr
Patras , Greece
Georgia Koloniari
Dept . of Applied Informatics
University of Macedonia Thessaloniki , Greece gkoloniari@uom.gr
Vassilios S . Verykios
Sch . of Science & Technology
Hellenic Open University
Patras , Greece verykios@eap.gr
ABSTRACT When dealing with sensitive and personal user data , the process of record linkage raises privacy issues . Thus , privacy preserving record linkage has emerged with the goal of identifying matching records across multiple data sources while preserving the privacy of the individuals they describe . The task is very resource demanding , considering the abundance of available data , which , in addition , are often dirty . Blocking techniques are deployed prior to matching to prune out unlikely to match candidate records so as to reduce processing time . However , when scaling to large datasets , such methods often result in quality loss . To this end , we propose Multi Sampling Transitive Closure for Encrypted Fields ( MS TCEF ) , a novel privacy preserving blocking technique based on the use of reference sets . Our new method effectively prunes records based on redundant assignments to blocks , providing better fault tolerance and maintaining result quality while scaling linearly with respect to the dataset size . We provide a theoretical analysis on the method ’s complexity and show how it outperforms state of the art privacy preserving blocking techniques with respect to both recall and processing cost .
Categories and Subject Descriptors H27 [ Database Administration ] : Security , integrity , and protection ; H28 [ Database Applications ] : Data mining
Keywords Private blocking , performance , reference sets
1 .
INTRODUCTION
With the data explosion we have been experiencing recently , the problem of linking records that represent the same real world entity across various and often heterogeneous sources has become increasingly important for a number of application areas such as data integration , business intelligence , web mining and recommendation systems . Known Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . Copyrights for components of this work owned by others than ACM must be honored . Abstracting with credit is permitted . To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee . Request permissions from Permissions@acmorg KDD '15 , August 10 13 , 2015 , Sydney , NSW , Australia . © 2015 ACM . ISBN 978 1 4503 3664 2/15/08…$1500 DOI : http://dxdoiorg/101145/27832582783290 also as data matching or entity resolution , the problem of record linkage , when dealing with personal or sensitive data such as data in social networks or medical research , invokes privacy concerns , opening up a new research domain , which is known as privacy preserving record linkage .
Let us consider a medical researcher who wants to investigate the possible side effects of a new medicine . The researcher needs access to data across different hospitals , doctors and pharmacies to identify patients that were prescribed the medicine and the symptoms or treatments these individuals may have received afterwards . However , revealing all such information about patients to the researcher compromises their privacy . Ideally , she would only need to know the symptoms exhibited by patients that were prescribed the medicine and no other information about them or other patients . Therefore , the goal of privacy preserving record linkage is to link records across multiple data sources without allowing the revelation of any other information to the sources or to any external party involved in the linkage process , besides the common ( linked ) records .
In both classical and privacy preserving record linkage , data sources are heterogeneous , holding different schemas and no common unique identifiers . Thus , linking methods exploit fields that are common to all data sources to identify common entities . In addition , data is usually dirty complicating the linkage process further . Exact matching may be inapplicable and approximate methods are deployed instead , using some form of distance or similarity function to determine when two records should be matched ( linked ) . As all possible record pairs are considered , linking requires quadratic distance computations , in which no traditional indexes may be used to accelerate the process since the data is dirty . Considering the large volumes of available data , record linkage is rendered a very resource demanding task . To deal with such scalability issues , blocking techniques were developed [ 22 ] . These methods group together records that are likely to match into blocks , by using some proximity criterion and perform comparisons only within each such block , thus reducing the required distance computations . In the same manner , blocking techniques can be deployed for privacy preserving record linkage with the additional requirement of preserving privacy , thus yielding privacy preserving blocking techniques .
As a result , we could say that a privacy preserving record linkage procedure consists of two steps . The privacy preserving blocking step , where coarse computations take place , and the privacy preserving matching step , where more elaborate matching occurs . Since even within blocks , matching
527 requires quadratic time , a meta blocking step may also be included . Such meta blocking techniques , which aim at further pruning candidate matching pairs within blocks , have been introduced for both the classical [ 18 ] and the privacy preserving version of record linkage [ 14 ] .
However , state of the art methods , when dealing with large datasets are unable to effectively speed up the matching process , without compromising privacy or result quality [ 22 ] . In particular , to maintain high recall , blocking techniques cannot effectively eliminate candidate matching pair records , thus , requiring more distance computations and increasing processing time . Similarly , ensuring high privacy often leads to poor performance .
In this paper , we propose a novel , highly scalable privacy preserving blocking technique : Multi Sampling Transitive Closure for Encrypted Fields ( MS TCEF ) . While MSTCEF may also be used as a standalone blocking method , we focus on its use in combination with a meta blocking technique [ 14 ] , to avoid the quadratic complexity of the matching phase . As we theoretically and experimentally show , our blocking technique leads to linear private record linkage and high quality in terms of matching accuracy without sacrificing privacy , as it relies on multiparty computation where no party gains access to all data .
The proposed blocking technique is based on the use of publicly available reference sets . Records are roughly grouped into blocks by comparing them against a reference set . Our idea is to introduce redundancy in the blocking process to improve both recall and fault tolerance , while keeping complexity as low as possible . In particular , using multiple reference set samples as a comparison basis , MS TCEF derives multiple assignments of records to blocks . As a result , recall is increased , and we deal with dirty data more effectively . A fast algorithm is used for deriving block assignments .
To further increase redundancy , and also deal with datasets that have few common attributes for matching and blocking , we use the transitive closure for encrypted fields ( TCEF ) method . The goal of TCEF is twofold . Firstly , it reduces the number of derived blocks by defining them over overlapping pairs of attributes rather than combinations of all the attributes . Secondly , it further increases fault tolerance and matching quality , as errors or missing values in attributes only influence a fraction of the derived block assignments .
To sum up , our contributions may be outlined as follows . ( 1 ) We propose MS TCEF , a novel , low complexity privacy preserving blocking technique , that exploits redundancy to provide accuracy and fault tolerance . ( 2 ) We theoretically prove that our blocking method scales linearly to the dataset size , and that combined with a metablocking method such as [ 14 ] , it renders the overall linkage process linear to the dataset size . ( 3 ) We experimentally explore MS TCEF ’s properties , provide insight on tuning its parameters , and show how it outperforms related state of the art methods [ 14 , 21 ] , both with respect to processing time and recall .
The rest of the paper is organized as follows . Section 2 references related work . In Section 3 , we formulate the problem we address , and briefly outline the privacy preserving matching approach we combine with our blocking technique . Section 4 details the proposed technique , while Section 5 includes our complexity and privacy analyses . Section 6 presents our empirical evaluation and Section 7 sums up with our conclusions and some thoughts for future work .
2 . RELATED WORK
Privacy preserving record linkage is first introduced in [ 6 ] , and privacy preserving blocking is presented in [ 1 ] , that proposes various levels of blocking in exchange of privacy , but falls short in terms of result quality . Recently , value generalization hierarchies [ 12 ] and differential privacy [ 13 ] were used in combination with homomorphic encryption . Both methods focus on categorical data , whereas MS TCEF is designed for alphanumeric data , and can easily deal with numerical data if an appropriate distance and matching function is deployed . For both numerical and alphanumeric data , in [ 15 ] , reference sets are used along with bigrams , bit vectors and homomorphic operations . However , despite the use of blocking , all methods involving homomorphic encryption remain computationally expensive . The Two Party Private Blocking ( TPPB ) method [ 21 ] avoids the use of a third party , and instead , trades off privacy for matching quality . It is also based on the use of a reference set , similarly to our approach , however as we experimentally show , TPPB scales quadratically to the dataset size making it inappropriate for large datasets . Privacy preserving metablocking is presented in [ 14 ] . The Sorted Neighborhood on Encrypted Fields ( SNEF ) approach [ 14 ] , is an extension of [ 11 ] , appropriate for dealing with encrypted data . We use SNEF as a metablocking method to reduce the complexity of the matching within the blocks generated by our blocking approach , so as to achieve a linear complexity for the overall linkage process . In terms of private matching , most methods rely on the use of a third party . In [ 5 ] , sources encode powersets of qgrams of their data and match them using one or more third parties . A less computationally intensive method [ 19 ] performs matching by means of embedding spaces and schema matching based on a global common schema . The method utilizes a metric space construction algorithm based on arbitrarily generated random strings . [ 3 ] is an extension of [ 19 ] , where the sample for the embedding is mined by one of the matching parties using a prefix tree , in such a way that differential privacy is satisfied . In [ 17 ] , the triangular inequality is exploited to assess similarity between actual and reference data , but the method suffers from low recall . We selected Bloom Bigrams [ 20 ] for matching , a method commonly used in private record linkage [ 8 , 14 , 21 ] , which inserts bigrams of alphanumeric data in a set of Bloom filters .
A survey classifying privacy preserving blocking and match ing methods along fifteen dimensions can be found in [ 22 ] . Blocking , indexing or searching techniques are also used in the context of the classical record linkage problem [ 11 , 16 , 18 ] . In this context , relative candidate keys , a category of dependencies for unreliable data , have been suggested for optimal matching and blocking by comparing a minimum number of attributes [ 10 ] . This approach may be viewed as complementary to MS TCEF for determining the best matching and blocking attributes . However , mining the matching dependencies as required by [ 10 ] violates privacy and therefore , we rely on privacy preserving techniques instead .
3 . PRELIMINARIES
In this section , we formulate the privacy preserving record linkage problem and briefly describe the privacy preserving meta blocking and the privacy preserving matching method that , along with the proposed blocking technique , is used to complete the overall linkage process .
528 3.1 Problem Formulation Without loss of generality , let us consider two data sources , respectively called Alice ( A ) and Bob ( B ) , with |r| records each . We denote as rA i the i th record of Alice and Bob , respectively . i and rB
Privacy preserving record linkage is the problem of identifying ( linking ) all pairs of rA and rB records that refer to the same real world entity , so that no more information is disclosed to either A , B or any third party involved in the process besides the identifiers of the linked rAs and rBs .
Let Alice ’s schema hold kA attributes , while Bob ’s kB , and let us assume that c of them are common between the two sources , where c ≤ kA , kB . None of the common attributes is a unique identifier that can be used to identify records . Therefore , a subset of m common attributes , called matching attributes , is used to determine when two records match , ie , when they refer to the same entity . To determine when two records match the respective attributes need to be compared . Considering that our data is often dirty , matching should rely on a similarity or distance function .
1 , rA
2 } and {rB
1 , rB 2 record matches Bob ’s rB
Example 31 Alice and Bob share a common subset of attributes ( a1 , a2 , a3 ) used as matching attributes . They hold 2 } respectively ( Table two records each , {rA 1 ) . Alice ’s rA 2 , but rB 2 has an error in its a1 field , where “ anne ” was inserted instead of “ ann ” . Let us consider a similarity function sim( ) → [ 01 ] and a threshold t > 0 . Given the records rA i with matching attributes ri.1 . . . ri.m for both Alice and Bob , we define the following matching function M → {0 , 1} : i and rB
M ( rA i , rB i ) = iff sim(rA 1 , 0 , otherwise . i .j , rB i .j ) ≥ t,∀j ∈ [ 1 , m ] i , rB i , rB i ) is a match . i ) = 1 , then the pair ( rA
If M ( rA Matching is a very expensive task . The required computations of the matching function are in the order of |r|2 . Thus , blocking is used to avoid the exhaustive comparisons applied during matching by quickly eliminating candidate pairs highly unlikely to match , while still preserving data privacy . The basic idea is to split the records of the data sources into groups called blocks . Given a set of l blocks , a mapping function bmap(ri ) → [ 1 . . . l ] assigns block indexes to records exploiting a subset of the common attributes , called blocking attributes , such that , if bmap(rA i ) , then the probability that M ( rA i ) = bmap(rB i ) = 1 is high . i , rB
Having agreed on the set of blocking attributes , each source first applies blocking separately on its plaintext data . In the next step , we use a third party , Carol . This is a common approach in privacy preserving data sharing [ 22 ] . Alice and Bob send Carol their blocks containing record identifiers to merge them . Carol may also be used to perform the matching process [ 19 , 20 ] . In the latter case , Alice and Bob send Carol encrypted data on which private matching is applied . 3.2 Private Meta Blocking and Matching
MS TCEF was developed both to function as a standalone blocking method and to be compatible with a privacy preserving meta blocking method [ 14 ] . In order to further explore the potential of privacy preserving meta blocking , since it is an area of limited research , we adopt the setup described in [ 14 ] , using SNEF for privacy preserving meta blocking and the Bloom Bigrams method for matching .
The Sorted Neighborhood on Encrypted Fields ( SNEF ) method [ 14 ] provides a modification to the original sorted neighborhood method [ 11 ] , so as to make it suitable for application on encrypted fields . It is deployed after blocking to further reduce the computational cost by avoiding all to all comparisons within each block . Each record is assigned a score using a ranking function that is based on the results of the blocking process . All records within a block are sorted based on this score . Next , a fixed size window slides over the resulting list of records , and the matching function is applied between all pairs of records that fall within the same window . The use of the sliding window is based on the intuition that , records which fall within the same window , thus having close values of the ranking function , are more likely to match .
We perform matching using SNEF applied on data encrypted with the Bloom bigrams method [ 20 ] , which is based on Bloom filters . These are probabilistic bit vectors of a particular length , mainly used for representing a set of values using multiple hash functions . In [ 20 ] , each matching attribute is encoded in a separate Bloom filter by extracting all of its substrings consisting of 2 adjacent characters ( bigrams ) . Each bigram is hashed by each of the hash functions and the corresponding bits are set . To determine whether two attributes rA i .j match , we define sim( ) as the Dice Coefficient measure [ 4 ] applied on their corresponding Bloom filters . Let bfA and bfB , the Bloom filters for the corresponding attributes , consisting of |bfA| and |bfB| bits equal to 1 respectively , and let |bfc| be the number of common bits equal to 1 between the two bit vectors . The Dice Coefficient score , simDC ( ) → [ 01 ] , is defined as : i .j ) = ( 2 · |bfc|)/(|bfA| + |bfB| ) . simDC ( rA i .j , rB i .j , rB
With Bloom filters , and as a result Bloom bigrams , there may be cases where the same bits are set for different bigram sets leading to false positives and falsely increasing the similarity when comparing the corresponding filters . However , this false positive ratio can be controlled by setting the filter size and the number of hash functions according to the input size , ie , the number of inserted bigrams .
4 . MULTI SAMPLING TRANSITIVE CLO
SURE FOR ENCRYPTED FIELDS
The basic idea behind blocking is to avoid the computationally expensive comparisons between candidate pairs of records that are unlikely to match . However , this pruning procedure that takes place during blocking , while achieving its goal for a more efficient matching process , often compromises the quality of the attained results . To this end , we propose a novel privacy preserving blocking technique called Multi Sampling Transitive Closure for Encrypted Fields ( MSTCEF ) , which , as we show , scales linearly to the dataset size , while preserving high matching quality .
In this section , we present in detail MS TCEF and all the basic components and algorithms that comprise it . 4.1 Reference Set based Blocking
Reference sets have been used in the context of private record linkage either in matching techniques [ 17 , 19 ] , or in blocking techniques [ 14 , 15 ] . They are either derived from publicly available databases [ 14 , 15 ] or synthetically generated [ 19 ] . These sets are used as an intermediate point of comparison exploiting the intuition that , if two objects are
529 Table 1 : Alice & Bob ’s data . a1 a2 a3 rA 1 rA 2 rB 1 rB 2 nicholas ann kevin anne smith cobb madrid london anderson warsaw london cobb
Table 2 : Reference set samples .
Id 1 2 3 4
S1 anthony lawrence victor zoe
S2 alex dorothy jonathan naomi
S3 alex john rhonda tristan
Table 3 : Classification results . Attribute Reference Value Distance rB 1 .a1 rB 1 .a2 rB 1 .a3 rB 2 .a1 rB 2 .a2 rB 2 .a3
S1.1 S2.1 S3.4 S1.1 S2.1 S3.2
6 6 5 4 4 4 similar to a third one , then , they are more likely to be similar to each other as well .
Initially , the data sources involved in the linkage process , ie , both Alice ( A ) and Bob ( B ) , agree on the parameters for generating identical reference sets . They also agree on the blocking and matching attributes that are going to be used . Let m be the number of matching and b the number of blocking attributes respectively , where b , m ≤ c and c is the number of common attributes between Alice and Bob . For presentation purposes and without harming the general case , we assume that b = m .
A reference set contains many different values referred to as reference values . When a reference set is used for blocking , the records of each data source are classified by associating the plaintext values of their blocking attributes with reference values . Each reference value determines a separate class . Each blocking attribute of each record in a data source is associated with its closest reference value , in a way that similar attributes are classified to the same class . The combination of a record ’s classes as derived by the blocking attributes determines the block that this record belongs to . As matching is later deployed only within each block , it is straightforward that to maintain high matching quality , block assignment should be as accurate as possible . However , using a single reference set does not always correctly map similar data values to corresponding reference values , especially since we often have to cope with dirty data . In general , reference values are not identical to data values and the distance metrics used for locating the most similar reference value to an attribute , often yield more than one result . Consequently , data records may be assigned to incorrect reference values , and therefore , to incorrect blocks .
To address the issues above , all techniques attempt to select the optimal reference values , either by using expensive clustering algorithms [ 14 , 15 ] or by random text generation for creating an embedding space [ 19 ] . Instead , we propose introducing redundancy into the intermediate phase . Rather than using a single reference set as our comparison basis , we propose using multiple reference sets . The idea is to simply repeat the blocking procedure with multiple , different S ( S > 1 ) reference set samples , so as to attain multiple block assignments . We argue that this way errors that occur with one assignment may be corrected by another , thus resulting in higher recall , as we experimentally demonstrate .
To generate each sample , we use a variant of the Durstenfeld shuffle [ 9 ] algorithm , which was originally developed for shuffling the contents of an array . Given an array of elements , the Durstenfeld shuffle algorithm generates a random permutation of this array by swapping the values between array positions . We have altered the Durstenfeld shuffle algorithm so as to swap only the first |RS| values of the array . These values comprise a reference set sample , ie , |RS| is the sample ’s size . We generate S such separate samples for each of the b blocking attributes used . We do not use the same reference sets for all blocking attributes so as to avoid frequency attacks .
Algorithm 1 : Fast Blocking Attribute Classification .
Input : RS : Reference Set Sample Values ba : List of values of a blocking attribute Output : C : Classes of blocking attribute values {c1c|ba|}
1 Sort ( RS ) ; 2 for i ← 1 to |ba| do pos ← BinarySearch ( ba(i ) , RS ) ; if pos − 1 > 0 then d1 ← EdDist ( ba(i ) , RS[pos − 1 ] ) ; d2 ← EdDist ( ba(i ) , RS[pos] ) ; if d1 < d2 then C[i ] ← pos − 1 ; else C[i ] ← pos ;
3
4
5
6
7 8 end
4.2 Record Classification
Alice and Bob use the samples of the reference sets , they have independently created , to classify their records based on similar reference values . The overall procedure is repeated for each of the reference set samples , leading to classifying each record to b· S classes that will result in multiple block assignments as was our goal .
To determine the most similar reference values of an attribute value so as to cope with dirty data , we use their edit distance as our proximity criterion . Since the classification process is repeated for each blocking attribute of each record and as many times as the reference samples used , a straightforward classification process would involve the computation of |r| · |RS| · b · S edit distances , rendering the use of many samples very expensive .
To this end , we propose a fast algorithm for classification described in Alg . 1 . Given the projection of a blocking attribute on the data source and a reference set sample , the algorithm returns a vector with the classes of all records based on the given blocking attribute . First , the reference set values are lexicographically sorted ( line 1 ) . Then , for each of the blocking attribute values , binary search is performed ( line 3 ) to locate the reference value with the most similar prefix . The chosen reference value and the one preceding it in the sorted reference sets are retrieved and their edit distances against the blocking attribute computed ( lines 4 5 ) . Finally , the index of the reference value with the smallest distance is selected and returned as the class for the given record ( lines 6 7 ) .
Example 41 Continuing with our example , Table 2 holds the reference set samples , where sample Si is used for attribute ai . For demonstration purposes and without harming the general case , we consider a single sample per blocking attribute . Alice and Bob separately classify their records using Alg . 1 . The classification of Bob ’s rB 1 .a1 field is illustrated
530 in Fig 1 . Initially , a reference set sample is retrieved and sorted . Then , a binary search determines where the data element ‘kevin’ is classified within the sorted reference set ( 1 ) . When search concludes ‘kevin’ is to be classified either with ‘anthony’ or with ‘lawrence’ ( 2 ) . The edit distance of ‘kevin’ with ‘anthony’ is 6 , and with ‘lawrence’ 7 . Thus , rB 1 .a1 is classified with ‘anthony’ ( 3 ) . The index of ‘anthony’ , S1.2 , is a class for rB 1 . The same procedure is followed for the rest of the blocking attributes yielding the class assignments shown in Table 3 . Alice also creates her own class assignments .
Figure 1 : Illustration of the classification .
Using binary search instead of sequentially comparing all reference values against each blocking attribute value reduces the method ’s complexity and allows us to use more and larger reference sets efficiently . As multiple classifications are derived , we experimentally show that the matching quality is high . The method seems especially vulnerable when errors occur in the start of a string value , but we can easily overcome this weakness if we also repeat the classification process by considering each string value from end to start , thus searching based on suffixes . Combining the two methods can even deal with dirtier data when necessary . The output of Alg . 1 is then used to fill a two column table . After each classification , |r| rows are added to the table , one for each record in the data source . In each row , the first column of the table maintains the class returned by the algorithm , and the second column maintains an alias for the corresponding record identifier that was classified to the specific class . Aliases are used instead of record ids to enhance the method ’s privacy . As each record is to be assigned to multiple blocks , using different aliases each time protects the data from any type of frequency analysis attacks . 4.3 Building Blocks with TCEF
We have assumed so far that b > 1 , ie , we use more than one blocking attributes . Using a single blocking attribute would require large reference sets , so as reasonably small blocks to be created . As matching is performed within each block , the goal of blocking techniques is to produce such reasonably small blocks . When b > 1 , the classes of the blocking attributes of each record need to be combined to derive the block it belongs to . However , combining all or many blocking attributes , makes the method vulnerable to record misclassifications , due to the fact that the data held by the data sources are dirty . An error in one attribute will affect the assignment of the record even if multiple samples are used as this value influences each classification .
To overcome this issue , we propose combining the blocking attributes in overlapping pairs to define our blocks . We call this approach Transitive Closure for Encrypted Fields ( TCEF ) , since record blocks are formed using different blocking attribute combinations at the source , without revealing any plaintext information . The main advantage of TCEF is that it offers the capacity of using numerous blocking attributes at a low time complexity . A similar approach has
Figure 2 : Actions taken by Carol . also been used in the classical version of the Sorted Neighborhood method [ 11 ] to improve matching accuracy . In our case , the basic idea is that , when b ≥ 3 , blocks are formed by considering blocking attributes in overlapping pairs instead of considering all possible combinations of the b attributes . Thus , blocks of different blocking attribute combinations are formed increasing the potential of two records being correctly placed in the same block , resulting in a technique that besides efficient is also more tolerant with respect to dirty data . Even if a record is misclassified for a specific blocking attribute pair , it may be classified correctly in the rest of the pairs . The block assignment is implemented by utilizing the table created during classification to define and populate the blocks .
Proposition 41 MS TCEF results in the creation of S· b · |RS|2 blocks at the most , when b ≥ 3 .
Proof . Using b ( b ≥ 3 ) blocking attributes in overlapping pairs , the following combinations occur : ( 1 , 2 ) , ( 2 , 3 ) , , ( b − 1 , b ) , ( b , 1 ) . This equals to b combinations . As such , b · |RS|2 blocks are built . Considering S separate samples for each block , this procedure is repeated S times resulting in S · b·|RS|2 blocks in total . As some blocks may be empty with no records assigned to them , S · b · |RS|2 is an upper bound for the number of blocks .
Example 42 Finally , in our example , based on Table 3 , Bob calculates the following non empty blocks , where a block label is represented by the idenitifiers of the corresponding reference values separated by an underscore : S1.1 S2.1 = {rB 1 } , S2.1 S3.2 = 2 } . Similarly , Alice creates her own {rB blocks .
1 , rB 2 } , S3.2 S1.1 = {rB
2 } , S2.1 S3.4 = {rB
1 } , S3.4 S2.1 = {rB
Up to this point , Alice and Bob each operate individually and do not exchange data . Next , a third party , Carol engages . The actions taken by Carol are illustrated in Fig 2 . Initially , Carol receives Alice and Bob ’s non empty blocks containing record id aliases ( 1 ) . Now , Carol holds blocks of records , which result from combining identical classes . For instance , as we may see in Fig 2(2 ) , there are three pairs of blocks . We refer to such blocks as corresponding . Carol merges Alice and Bob ’s corresponding blocks by calculating their union ( 2 ) . Any block that does not have a corresponding block in the other party ’s blocks is rejected . After forming the blocks , Carol requests the encrypted data records corresponding to all record ids that belong to blocks that contain records from both data sources . She can then apply any private matching method within these blocks to identify the matching records .
531 b
4.4 Using SNEF
When considering the case where MS TCEF is combined with meta blocking , and in particular with SNEF , then Carol is the one responsible for applying the meta blocking along with the privacy preserving matching process .
To apply SNEF , Carol needs to sort the record ids within each merged block based on SNEF ’s ranking function . In this case , the value of the function needs to be calculated during block assignment at each of the data sources for their corresponding records . Each time a record is put in a block , its score is calculated using the following ranking function : rf = fidi , where b is the number of blocking attributes , i=1 di is the edit distance between the blocking attribute and the respective reference value and fi is a weight factor . Weight factors appear in SNEF ’s ranking function to enhance the fuzziness of the results , making it more difficult for the party who performs matching to predict the individual distances that comprise a record ’s score . However , they may also be used for normalization , since the value space of each field might be different . These fi values are agreed in advance by the matching parties .
Thus , Carol receives along with the aliases of the record ids , their respective scores calculated by the ranking function , and by sliding a window of fixed size over the sorted records , she compares only those falling within the same window ( Fig 2(3) ) .
5 . THEORETICAL EVALUATION
We study the performance of MS TCEF in terms of complexity and privacy . For the complexity analysis in particular , we analyze not only our blocking technique , but in order to evaluate its influence in the overall linkage process , we also provide a similar analysis for matching and meta blocking , ie , SNEF , which is not included in [ 14 ] . 5.1 Complexity Evaluation Blocking Phase Complexity . We remind that b stands for the number of blocking attributes , |RS| the size of each of the S reference set samples , and MS TCEF considers pairs of blocking attributes . The first step in the blocking phase is generating S reference samples for each of the blocking attributes . To generate one sample , the modified Durstenfeld shuffle algorithm [ 9 ] requires |RS| permutations of the values held in the publicly available database , thus , the total number of required operations is : S · b · |RS|
( 1 )
Next , each source classifies its data . For a single sample for one blocking attribute , the reference set values are initially sorted , requiring O(|RS| · log|RS| ) operations , and a binary search is then performed for all |r| values of the blocking attribute in the data set , requiring O(|r|· log|RS| ) operations . Addressing the general case , with b blocking attributes and for S samples , the total number of required operations equals to O(S · b · ( |r| + |RS| ) · log|RS| ) . Given that |RS| |r| , the total number of operations for the classification at each source is :
O(S · b · |r| · log|RS| )
( 2 )
After classification , for all |r| records and for all S samples , each of the b overlapping pairs , which are formed by the composition of the blocking attributes , are combined to produce the final blocks . The total number of operations required is :
S · b · |r|
Finally , Carol engages to merge the corresponding blocks from the two sources . Based on Proposition 4.1 , merging forms S · b·|RS|2 blocks at the most , and all of them are examined . An equal number of union operations is performed and the complexity of this step is :
O(S · b · |RS|2 )
( 3 )
( 4 )
Theorem 51 The complexity of blocking with MS TCEF is O(S · b · |r| · log|RS| ) .
Proof . Based on Equations ( 1 ) to ( 4 ) , the total complexity of the blocking phase is O(S · b · ( |RS| + |r| · log|RS| + |r| + |RS|2) ) . Given that |RS| |r| , the total complexity for block creation renders to O(S · b · |r| · log|RS| ) .
Matching Phase Complexity . Matching normally requires |rA| · |rB| record comparisons , where |rA| and |rB| is the number of Alice and Bob ’s records , respectively . As we assumed for simplicity that |rA| = |rB| = |r| , this equals to |r|2 . Using the sorted neighborhood method , in the classical record linkage problem , reduces this cost to O(w · |r| ) [ 11 ] , where w is the size of the sliding window . Since we require privacy preserving matching , the SNEF [ 14 ] extension of the sorted neighborhood method is used instead .
Theorem 52 When a record falls within Z blocks , SNEF requires O(Z · w · |r| ) operations .
Proof . Alice and Bob together hold 2·|r| distinct records , which totals to 2·|r|· Z records for processing . Let L be the total number of blocks . Assuming that records are evenly distributed , each block holds on average ( 2·|r|·Z)/L records . Applying SNEF , within each block using a window of size w , as in sorted neighborhood [ 11 ] , results in O(w· ( 2·|r|· Z)/L ) comparisons within each block . For all L blocks , the number of operations required equals to O(Z · w · |r| ) .
Corollary 51 Applying SNEF with MS TCEF requires
O(S · b · w · |r| ) operations .
Proof . Each record falls within S· b blocks . Using Theo rem 5.2 with Z = S·b results in O(S·b·w·|r| ) operations .
Matching is the most expensive phase and as such , the product S · b · w should remain low , when tuning our approach . Overall Complexity . Considering the complexities evaluated , it is easy to see that the time required both for block generation and matching using SNEF is linearly dependent on the number of reference set samples S . Increasing its value may offer higher chances of matching at the expense of a higher execution time . However , the most important observation lies in the fact that all complexities are linear to |r| , assuming that |rA| |rB| . This means that the suggested blocking approach manages to lower the cost of private record linkage by one order of magnitude , from quadratic to linear .
Note that as we experimentally show , our assumption that |r| >> |RS| holds , as with an |RS| around 0.001 · |r| , we achieve both efficiency and high quality in our results .
Theorem 53 Execution time of applying MS TCEF with SNEF , given that Alice and Bob hold similarly sized databases , scales linearly to the size of the data sources .
532 Proof . From Theorem 5.1 and Corollary 5.1 , assuming |rA| = |rB| = |r| and |RS| |r| , we have : O(S · b · |r| · ( log|RS|+w) ) , which is linear to the data sources size |r| .
5.2 Leakage Discussion
A requirement of any part of the privacy preserving linkage process is that , the only additional knowledge gained by the involved parties regards the ids of the matched records . We based our private approach on the use of reference values and multi party computation where no party gains access to all data . We first focus solely on MS TCEF , and then , also investigate any privacy issues the combination with SNEF may raise .
We assume that all parties are semi honest without collusion . Alice , Bob and Carol execute their parts of the protocol exactly as specified with no deviations , but try to learn as much as they can about the data of the other parties from the information they gather . Such an assumption is usual in multi party protocols [ 15 ] . We also assume that all communications between participants are performed through a secure channel .
Let us first discuss how privacy is preserved between the two sources and what kind of information is leaked between them .
Information leaked to Alice and Bob . Data privacy may concern both the actual data records and the schema of the data . It is not uncommon for a data source to not want to reveal not only its actual contents , but their type as well . MS TCEF requires that the two sources reveal to each other only a subset of their schema , and specifically the blocking attributes . MS TCEF is actually designed to assist in reducing the number of blocking attributes required , and therefore the knowledge attained by the two parties . With the use of three attributes , the method can scale efficiently without compromising matching efficiency and quality later , by exploiting TCEF and multi sampling to derive multiple assignments for each data record .
At the data level , MS TCEF prevents information leakage between sources by not requiring the exchange of any data between them . Alice and Bob gain no information about each other ’s data . The two sources only need to agree on the reference sets source and then they can carry on the blocking process independently without any other communication between them to reveal the results of the blocking procedure . Therefore , with no direct access to each other ’s data , no source can perform a frequency analysis on the other one ’s data or assess its vocabulary .
Information leaked to Carol . We examine now what knowledge Carol , used as a third party , can infer based on the blocks of records she receives . We first consider that MS TCEF is used for blocking and a private method for matching without the use of SNEF .
Carol receives blocks of records identifiers from Alice and Bob in order to merge them . Immediately , Carol learns the number of blocks sent by Alice and Bob and the cardinality of each block . The contents of each block are populated by the intersection of the classes a record belongs to . Since the reference set samples used are not disclosed to Carol , she cannot infer their association with the blocks . Furthermore , since record aliases are used , she may not infer the cardinality of a record ’s occurrences within all blocks . As such , when MS TCEF is used as an independent blocking method , Carol does not gain any additional knowledge compared to the case where only a matching method , such as Bloom Bigrams [ 20 ] , is used without any blocking . Consequently , the use of MS TCEF does not cause additional information leakage .
Next , we study the case where MS TCEF is combined with SNEF . Now , besides the blocks of record identifiers , each record within such a block is accompanied by a score , which is used by SNEF . This score is a weighted linear combination calculated by SNEF ’s ranking function . Carol is not able to successfully factorize this number , since she knows neither the number of factors , nor the number of blocking attributes , nor the values of the weights used . Considering a marginal situation , where unitary weights are used , it still is computationally expensive for Carol to factorize the ranking function ’s linear combination score , since she does not know the number of factors that comprise it . Even if she somehow manages to learn that number , she would only get individual distances associated with alias record ids , rendering her unable to perform a frequency analysis attack , as she has access to neither the actual data nor the reference set samples .
6 . EXPERIMENTAL ANALYSIS
To prove the correctness of our theoretical analysis , we provide empirical evidence by means of extensive experimentation . We focus on the matching quality and time efficiency of our method and aim at discovering general rules , which may be used as guidelines for parameter tuning when our methodology is deployed . We also provide a comparison with state of the art approaches [ 14 , 21 ] .
Table 4 : Parameters of the experimental evaluation .
Parameter
Reference Set Size Number of Samples Sliding Window Size Voters Dataset Size Authors Dataset Size Blocking Attributes
Default Value
0.001 ∗ |r|
10 20
200K
206,990
3
Range
501,600
120 5160
50K800K
N/A 15
6.1 Experimental Setup
Using Java 7 , we built a simulator with all the protocols and algorithms we examine , only omitting data transmission , as we focus on processing time . All experiments are conducted on a dedicated Ubuntu 14.04 server powered by two AMD 6128 HE Opteron processors with 16 GB of RAM . We use two datasets , the voters dataset , originating from the North Carolina voters database1 and the authors dataset , retrieved from the DBLP++2 website , containing the author names of papers with exactly 3 authors , so as to have for the second dataset approximately the same size with the default size of the first dataset . We use the voters dataset to evaluate the behavior of our method , and the authors dataset as a control dataset for validating our results .
The attributes :
‘last name’ , ‘first name’ , ‘middle name’ , ‘city of residence’ and ‘precinct description’ were chosen as blocking and matching attributes for the voters dataset , and the ‘full name’ of each of the three authors for the authors
1Available at ftp://altncsbegov/data/ 2Available at http://dblpl3sde/dblp++php
533 ( a ) Attributes ( Vot . ) Time ( b ) Attributes ( Vot . ) Quality
( c ) RS Size ( Vot . ) Time
( d ) RS Size ( Vot . ) Quality
( e ) Samples ( Vot . ) Time
( f ) Samples ( Vot . ) Quality
( g ) Samples ( Auth . ) Time
( h ) Samples ( Auth . ) Quality
( i ) Window ( Vot . ) Time
( j ) Window ( Vot . ) Quality
( k ) TCEF Impact Time
( l ) TCEF Impact Quality
( m ) Classification Time
( n ) Classification Quality
( o ) Methods ( Vot . ) Time
( p ) Methods ( Vot . ) Quality
Figure 3 : Influence of ( a b ) blocking attributes , ( c d ) reference set size , ( e h ) number of samples , and ( ij ) window size , ( k l ) impact of TCEF , ( m n ) sequential vs binary search and ( o p ) comparison with other methods . dataset . The attributes chosen for the second dataset are much greater in length with respect to the first dataset , enabling us to study the performance of our method with data of different characteristics . We assume that the attributes chosen comprise a candidate key , and deduplicate each dataset using the respective attribute combination . After deduplication , we generated two databases from each dataset , so that 25 % of their records is common . We assume that the first generated database of each pair belongs to Alice , while the second one to Bob . Finally , as a source for reference set creation , we used the “ master ” table of Lahman ’s baseball database3 .
Since we are interested in linking low quality data , we corrupted Bob ’s common records using the German Record Linkage Center ’s data corrupter [ 2 ] . The corrupted records contain one error per attribute , as it is rather unusual for a word to contain more than one to two typographical er
3Available at http://wwwseanlahmancom rors [ 7 ] . With equal probability , the error may be either a character insertion , deletion or substitution .
The Bloom Bigrams [ 20 ] algorithm has been used as the matching method in all cases . As we discussed in Section 3 , in order to minimize the number of its false positives , we adjust the Bloom filter size according to the size of the input , ie , the number of bigrams we insert , which is analogous to the length of the matching attributes . As such , we use 900 bits for the voters dataset and 3,000 bits for the authors dataset which contains longer attributes , so as to retain the same false positive ratio for both datasets . In both cases , we use 4 hash functions . After extensive experimentation , we set up the matching threshold between two matching attributes in terms of dice coefficient , as used in [ 20 ] , to 04 To assess matching accuracy , we measure precision and recall . Precision is defined as the proportion of the number of relevant records retrieved to the total number of matched records , while recall as the ratio of the number of relevant records matched to the total number of relevant records in
0 100 200 300 400 500 600 700151020PPB+PPM Time(linear scale ) secNumber of Samples1 attr.2 attr.3 attr.4 attr.5 attr . 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1151020RecallNumber of Samples1 attr.2 attr.3 attr.4 attr.5 attr . 0 200 400 600 800 1000 1200 1400501002004008001600PPB+PPM Time(linear scale ) secReference Set Size50K 100K 200K 400K 800K 0.4 0.5 0.6 0.7 0.8 0.9 1501002004008001600RecallReference Set Size50K100K200K400K800K 0 100 200 300 400 500 600501002004008001600PPB+PPM Time(linear scale ) secReference Set Size151020 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1501002004008001600RecallReference Set Size151020 0 100 200 300 400 500 600 700 800 900 1000501002004008001600PPB+PPM Time(linear scale ) secReference Set Size151020 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1501002004008001600RecallReference Set Size151020 0 500 1000 1500 2000 2500 3000 3500 20 40 60 80 100 120 140 160PPB+PPM Time(linear scale ) secWindow Size50K100K200K400K800K 0.8 0.82 0.84 0.86 0.88 0.9 0.92 0.94 0.96 0.98 20 40 60 80 100 120 140 160RecallWindow Size50K100K200K400K800K 0 100 200 300 400 500 600 700 800 900 5 10 15 20PPB+PPM Time(linear scale ) secNumber of SamplesVoters TCEFVoters No TCEFAuthors TCEFAuthors No TCEF 0.4 0.5 0.6 0.7 0.8 0.9 1 5 10 15 20RecallNumber of SamplesVoters TCEFVoters No TCEFAuthors TCEFAuthors No TCEF 0 2000 4000 6000 8000 10000 12000 5 10 15 20PPB+PPM Time(linear scale ) secNumber of SamplesVoters MS TCEFVoters Seq.Authors MS TCEFAuthors Seq . 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 5 10 15 20RecallNumber of SamplesVoters MS TCEFVoters Seq.Authors MS TCEFAuthors Seq.100101102103104105106 200 400 600 800PPB+PPM Time(logarithmic scale ) secDataset Size ( Thousands)plainMS TCEFSNEFTPPB 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 200 400 600 800RecallDataset Size ( Thousands)plainMS TCEFSNEFTPPB534 the database . Efficiency is measured by the overall time required for the linkage process including both the blocking and matching phases , since the aim of blocking , regardless of the time it requires , is to speed up the matching process . As in all experiments , precision was above 0.98 for all cases , our figures only illustrate recall , for clarity .
Our parameters are summarized in Table 4 . In each experiment , the influence of some parameters is examined , while the rest maintain their default values . Each experiment is executed 5 times . For each measure , max and min values are removed and the computed average reported . In the figures , we denote with ‘Vot.’ , the experiments using the voters dataset , and with ‘Auth.’ , the ones using the authors dataset , respectively .
6.2 Parameter Tuning Number of blocking attributes . Our aim is to determine whether the lack of suitable blocking attributes may be compensated by increasing the number of samples . We consider 1 to 5 blocking attributes , while varying the number of samples . As Fig 3(a ) illustrates , execution time is linear to the number of blocking attributes confirming our theoretical results . Precision is above 0.99 in all cases , while recall increases with the number of blocking attributes , as more redundancy is inserted into our method ( Fig 3(b) ) . Increasing the number of samples improves the recall even for fewer blocking attributes . For instance , using 3 blocking attributes and 5 samples , results in higher recall compared to using 4 or 5 attributes , with 1 sample . We select 3 as the default number of blocking attributes to show how we can deal with a lack of common attributes .
Reference set size . In this experiment , we vary the size of each reference set sample from 50 to 1,600 values , while the dataset sizes range from 50,000 to 800,000 records . As the size of the reference set ( RS ) increases , the same occurs with the number of derived classes , resulting in a larger number of smaller blocks . Thus , less comparisons take place within each block reducing the overall processing time ( Fig 3(c) ) . With respect to quality , precision is around 0.99 for all cases . Recall is low for small RS sizes . It initially increases as the RS size increases , but deteriorates again for the largest sizes ( Fig 3(d) ) . For smaller sizes , a small number of classes and consequently blocks is created , with many records in each block . The problem in this case is that the size of the sliding window w does not suffice to include all candidate matches . Nevertheless , increasing the reference set size too much leads to many small blocks . As we deal with dirty data , for very small blocks , values with errors are very likely misclassified . A reasonable value , given our dataset size , is |RS| = 0.001 · dataset , since it provides a balanced tradeoff between execution time and matching quality .
Reference set samples . Next , we examine the number of reference set samples ( S ) with respect to different RS sizes . Figure 3(e ) shows that the increase in time induced by using larger number of samples is proportional to the number of S for all reference set sizes , verifying Theorem 53 Again , precision is not affected , but recall increases ( Fig 3(f) ) . Using a larger RS size combined with a reasonable number of samples allows us to reduce the time but still achieve a high recall , ie , above 0.9 for 10 samples of size equal to 200 . Using the authors dataset for the same configuration , we observe that it exhibits higher quality in its results ( Fig 3(h ) ) with a precision over 0.98 , but also requires more time ( Fig 3(g) ) . The upper bound on the quality for each dataset is given by the plain matching approach , as no blocking method can improve on that . The authors dataset contains a greater number of distinct values for each attribute than the number of values held in each attribute of the voters dataset , leading to more distance computations for the former but also better matching accuracy . The authors dataset reaches its maximum recall with less samples than the voters dataset , as it contains attributes of greater length and the probability of an error to cause misclassification is smaller .
Window size . Finally , we consider the effect of SNEF ’s sliding window w . Being a metablocking component , it influences the behavior of the linkage process , and while not our focus , we want to provide a first insight on tuning the parameters of the overall process . Thus , we study how scaling is affected by altering the size of w while using the default values of all other parameters . Execution time , depicted in Fig 3(i ) , is proportional to the window size w . Recall increases as the window size increases , since a record is compared with more records within the same window ( Fig 3(j) ) . A simple method for determining the appropriate value for w is starting with a small size and increasing it when matches are limited . However , in our case , even without an optimal window selection , recall is still high . For a fair tradeoff between execution time and recall , we select a value of 20 .
6.3 Methods Comparison Impact of TCEF . We examine the impact of using TCEF and its relation with the number of reference set samples . We employ both datasets . While using TCEF increases the processing cost ( Fig 3(k) ) , it also improves recall up to 45 % ( Fig 3(l) ) . For instance , for the voters dataset and 5 samples , the processing cost is increased by 107 secs , while recall is improved by 19 % . While the delay seems large , TCEF is still 6,696 secs faster than TPPB ( Fig 3(o) ) , with 5 % improved recall ( Fig 3(p) ) . Precision is not significantly affected as usual . Similarly , for 20 samples , recall is improved by 12 % for an additional delay of 417 secs . While the use of TCEF also improves the performance for the authors dataset ( improvement up to 10% ) , it shows a greater improvement in recall for the voters dataset ( up to 45% ) . The observed higher recall of the authors dataset is due to the dataset ’s inherent characteristics , and the influence of TCEF is limited compared to its influence on the voters dataset , which can be considered to contain the dirtier data . Thus , we conclude that its use is recommended especially with dirty data .
Classification . Next , we compare our fast classification algorithm that is based on binary search with a classification algorithm based on exhaustive sequential search to locate the closest reference value for each blocking attribute of each record . As expected , sequential search is much slower than binary search ( Fig 3(m) ) . The worst results are observed with sequential search for the authors dataset , since this set contains more discrete values than the voters set . While one expects sequential search to achieve better result quality , recall is actually improved when using binary search ( Fig 3(n) ) . The reason is that with sequential search , ties in edit distance may classify similar values to randomly different classes . On the other hand , lexicographical sorting classifies more accurately , except if errors occur in the beginning of
535 the alphanumeric value , but we can deal with such cases by sorting the values from end to beginning as well .
Comparison with other methods . Finally , using the voters dataset , we compare how MS TCEF scales against the simple all to all matching case ( plain ) , a method based on the original SNEF [ 14 ] , and a rival state of the art Two Party Private Blocking ( TPPB ) [ 21 ] method . In TPPB , one blocking attribute is used . Each data source creates a separate reference set , merges its data with it and lexicographically sorts the result . Data are assigned to clusters based on the reference value that precedes them in the sorted list . Clusters of similar reference values are merged , so as to contain at least k records in order to offer k anonymity . A subset of their reference values is exchanged between the two parties . It is then merged with their local reference values and finally sorted . Data blocks are formed from records that are assigned to the reference values which fall within a sliding window . To configure TPPB , we first used the default values k = 100 , |RS| = |D|/k·10 , ne = 50 % and w = 2 in [ 21 ] . Since we required a larger number of reference values , we employed a reference set consisting of 403,096 distinct last names originating from the Australian phonebook . However , experimenting with the 800K voters dataset exhausted the system ’s memory . Therefore , we chose to configure TPPB using the same size for the reference set as with MS TCEF , so as to also attain a more fair comparison . The ‘last name’ attribute was chosen as the blocking attribute . For configuring SNEF , we used our default values . Though SNEF deploys sequential search for record classification , scaling to 800K records also exhausted the system ’s memory , so instead , we deployed a faster SNEF with binary search .
As depicted in Fig 3(o ) , MS TCEF is faster than plain matching and TPPB . We used a logarithmic scale on the vertical axis to better display the escalation of each method . We did not include the measures for the plain case and the 800K dataset as the required processing time was several days long . TPPB and the plain approach scale quadratically when doubling the dataset size , while MS TCEF scales linearly , validating Theorem 53 The improved SNEF is faster , but its quality is far inferior compared to all other approaches . On the other hand , MS TCEF scores highly in terms of recall , outperforming TPPB ( Fig 3(p) ) . The slight drop in recall for the larger datasets can be improved if an appropriate window size is selected as shown in Fig 3(j ) .
7 . CONCLUSIONS AND FUTURE WORK We presented , MS TCEF ( Multi Sampling Transitive Closure for Encrypted Fields ) , a highly scalable privacy preserving blocking method that scales linearly to the dataset size . Based on the use of multiple reference set samples and multiple record to block assignments , MS TCEF exploits redundancy for achieving high result quality and effectively dealing with dirty data . As we experimentally showed , our approach outperforms existing state of the art methods by exhibiting high performance both in terms of matching accuracy and execution time .
Plans for future work include exploring the performance of MS TCEF combined with other privacy preserving matching methods , and the design of possible extensions , to address malicious adversaries .
8 . REFERENCES [ 1 ] A . Al Lawati , D . Lee , and P . McDaniel . Blocking aware private record linkage . In IQIS , 2005 .
[ 2 ] T . Bachteler and J . Reiher . A test data generator for evaluating record linkage methods . Technical report , German RLC Work . WP GRLC 2012 01 , 2012 . [ 3 ] L . Bonomi , L . Xiong , R . Chen , and B . C . Fung .
Frequent grams based embedding for privacy preserving record linkage . In ACM CIKM , 2012 .
[ 4 ] P . Christen . Data Matching . Data Centric Systems and Applications . Springer , 2012 .
[ 5 ] T . Churches and P . Christen . Some methods for blind folded record linkage . BMC Med . Inform . and Decision Making , 4(1):9 , 2004 .
[ 6 ] C . Clifton , M . Kantarcioglu , A . Doan , G . Schadow ,
J . Vaidya , A . Elmagarmid , and D . Suciu . Privacy preserving data integration and sharing . In DMKD , 2004 .
[ 7 ] F . J . Damerau . A technique for computer detection and correction of spelling errors . CACM , 7(3 ) , 1964 .
[ 8 ] E . A . Durham , M . Kantarcioglu , Y . Xue , C . T´oth ,
M . Kuzu , and B . Malin . Composite bloom filters for secure record linkage . TKDE , 26(12):2956–2968 , 2014 . [ 9 ] R . Durstenfeld . Algorithm 235 : Random permutation .
CACM , 7(7):420– , 1964 .
[ 10 ] W . Fan , X . Jia , J . Li , and S . Ma . Reasoning about record matching rules . PVLDB , 2(1 ) , 2009 .
[ 11 ] M . A . Hern´andez and S . J . Stolfo . Real world data is dirty : Data cleansing and the merge/purge problem . Data Mining and Knowl . Discovery , 2(1):9–37 , 1998 .
[ 12 ] A . Inan , M . Kantarcioglu , E . Bertino , and
M . Scannapieco . A hybrid approach to private record linkage . In IEEE ICDE , 2008 .
[ 13 ] A . Inan , M . Kantarcioglu , G . Ghinita , and E . Bertino .
Private record matching using differential privacy . In ACM EDBT , 2010 .
[ 14 ] A . Karakasidis and V . S . Verykios . A sorted neighborhood approach to multidimensional privacy preserving blocking . In IEEE ICDMW , 2012 .
[ 15 ] M . Kuzu , M . Kantarcioglu , A . Inan , E . Bertino ,
E . Durham , and B . Malin . Efficient privacy aware record integration . In ACM EDBT , 2013 .
[ 16 ] A . McCallum , K . Nigam , and L . H . Ungar . Efficient clustering of high dimensional data sets with appli cation to reference matching . In ACM KDD , 2000 .
[ 17 ] C . Pang , L . Gu , D . Hansen , and A . Maeder . Privacy Preserving Fuzzy Matching Using a Public Reference Table . In Intel . Patient Manag . , pages 71–89 . 2009 . [ 18 ] G . Papadakis , G . Papastefanatos , and G . Koutrika .
Supervised meta blocking . PVLDB , 7(14):1929–1940 , 2014 .
[ 19 ] M . Scannapieco , I . Figotin , E . Bertino , and A . K . Elmagarmid . Privacy preserving schema and data matching . In ACM SIGMOD , 2007 .
[ 20 ] R . Schnell , T . Bachteler , and J . Reiher . Privacy preserving record linkage using bloom filters . BMC Med . Inform . and Decision Making , 9(1):41+ , 2009 .
[ 21 ] D . Vatsalan , P . Christen , and V . S . Verykios . Efficient two party private blocking based on sorted nearest neighborhood clustering . In ACM CIKM , 2013 . [ 22 ] D . Vatsalan , P . Christen , and V . S . Verykios . A taxonomy of privacy preserving record linkage techniques . Inf . Systems , 38(6):946 – 969 , 2013 .
536
