LINKAGE : An Approach for Comprehensive Risk
Prediction for Care Management
Zhaonan Sun IBM Research
Yorktown Heights , NY zsun@usibmcom
Fei Wang
Department of Computer Science and Engineering University of Connecticut feiwang03@gmail.com
Jianying Hu IBM Research
Yorktown Heights , NY jyhu@usibmcom
ABSTRACT Comprehensive risk assessment lies in the core of enabling proactive healthcare delivery systems . In recent years , datadriven predictive modeling approaches have been increasingly recognized as promising techniques to help enhance healthcare quality and reduce cost . In this paper , we propose a data driven comprehensive risk prediction method , named Linkage , which can be used to jointly assess a set of associated risks in support of holistic care management . Our method can not only perform prediction but also discover the relationships among those risks . The advantages of the proposed model include : 1 ) It can leverage the relationship between risks and domains and achieve better risk prediction performance ; 2 ) It provides a data driven approach to understand relationship between risks ; 3 ) It leverages the information between risk prediction and risk association learning to regulate the improvement on both parts ; 4 ) It provides flexibility to incorporate domain knowledge in learning risk associations . We validate the effectiveness of the proposed model on synthetic data and a real world healthcare survey data set .
Categories and Subject Descriptors H.4 [ Information Systems Applications ] : Miscellaneous ; J.3 [ Life and Medical Sciences ] : Health , Medical information systems
General Terms Algorithms
Keywords Healthcare ; Comprehensive risk prediction ; Generalized linear model ; Generalized thresholding ; Covariance Matrix
1 .
INTRODUCTION
Healthcare is closely related to everyone ’s daily life . Because of the high complexity in healthcare industry , every year a huge amount of money is wasted . In recent years researchers from different areas went into the healthcare world with the hope of helping to reduce the cost and improve the quality of care delivery . Among all those emerging trends , data driven technologies have captured a lot of attentions due to the availability of more and more healthcare data . Data driven healthcare is at the center of the vision of learning health systems and holds great promise for transforming the current healthcare status . Until now the research of data driven healthcare is mainly in the clinical setting , ie , by analyzing medical data from clinical institutes such as Electronic Health Records ( EHR ) . However , for comprehensive care management , particularly among the older population , assessing risks in other domains such as daily tasks and social and behavioral activities are equally important , and can be critical for preventive care . Because of the complexity of patient health conditions , any specific health risk is usually associated with other related risks from these other domains . Those risks span medical , social , behavioral , mental , psychological aspects of the patients .
In this paper , we present the comprehensive risk prediction problem , wherein instead of just predicting a single risk , we will ( 1 ) jointly predict a set of associated risks ; ( 2 ) discover the hidden risk associations . There are many challenges in this process include the followings :
• Structure . The predicted risks are inter correlated . The correlation structure could be provided or unknown . How to incorporate ( if the risk association is provided ) or explore ( if the risk association is unknown ) the risk association structure in the learned predictive model ?
• Heterogeneity . The predicted risks could be heterogeneous , ie , they are with different types ( eg , disease onset risk is binary and the key vital indicators are usually continuous ) . How to construct predictive model for those mixed type risks ?
• Sparsity . Acquiring patients’ information is expensive , time consuming and sometimes even harmful to the patients ( eg , biopsy for cancer patients ) . How to leverage the sparse patient information into the model building process ?
• Noisiness . Due to the potential errors made by medical devices or humans , the information for the training patients could be noisy . How to construct a robust predictive model in this case ?
1145 We propose a general approach called Linkage ( LINKed tArgets reGrEssion ) , which models comprehensive risk prediction as a sparse optimization problem . Linkage builds a sparse linear predictor for every risk target , and assumes that the sparsity patterns on the coefficients of the linear predictors are similar for similar risks . Actually the sparsity pattern , ie , the nonzero elements of the linear predictor coefficients reflect the “ active ” features that really contribute to the predicted risk . Therefore our assumption is similar risks should have similar contributed features . We develop an efficient alternating optimization procedure to solve the problem and validate its effectiveness on both synthetic and real world data sets . It is worthwhile to highlight the following aspects of the proposed method :
• It can leverage the relationship among those associated risks and achieve better prediction performance . This alleviates the data sparsity problem and makes the maximal utilization of patient information .
• It provides a data driven approach to discover the association among risks . The basic assumption is that the similar risks will have similar sparsity patterns on the coefficients of their linear predictors .
• It has the flexibility to incorporate the prior knowledge on risk associations to enhance robustness .
• It allows the predicted risks to be heterogeneous , ie , of different types .
One thing that is worthy of mentioning here is that Linkage is closely related to Multi Task Learning ( MTL)[2 , 8 , 13 , 12 , 17 , 22 , 24 ] , which is a learning paradigm aiming at learning a problem together with other related problems at the same time , under a shared representation . To the best of our knowledge , most existing multi task learning methods assume the tasks are homogeneous , ie , of the same type . MTL methods also do joint predictions and learn task associations simultaneously , but they usually assume similar tasks will have similar coefficients for linear predictors , not similar sparse patterns [ 24 , 25 ] . This is a much stronger assumption and may miss some task associations . There is also work on graphical Lasso which made similar assumption as Linkage [ 9 ] , but the task association matrix is already given in their work . On the contrary , Linkage will learn the task association matrix from data .
The rest of this paper is organized as follows . Section 2 introduces the details of the proposed methodology . The empirical evaluations on both synthetic and real world data are introduced in Section 3 , followed by the conclusions in Section 4 .
2 . MODEL
2.1 Formation
Consider a problem of jointly predicting m risks for n observations ( or samples , patients ) . Let yj ∈ Rn×1 be the vector of the j th risk target , and Y = [ y1 , . . . , ym ] ∈ Rn×m be the target matrix . Assume there are d features . Let xi ∈ Rn×1 be the i th feature vector , and X = [ x1 , . . . , xd ] ∈ Rn×d be the feature matrix . In this section , we temporarily assume that both X and Y are completely observed . In EHR or healthcare related data sets , it is common that risk targets or features are incompletely observed . Different risks or features could be observed for different groups of samples , or they could partially share a group of samples . We will discuss an extension of our proposed framework in Section 2.3 to deal with incomplete observations .
For each risk target , we consider the following generalized linear model :
E ( yj|X ) = g−1 ( Xwj ) ,
( 1 ) where E ( · ) denotes expectation , g ( · ) is the link function , wj ∈ Rd×1 is the coefficient vector of target j , and Xwj is the linear predictor . We collectively denote W = [ w1 , . . . , wm ] ∈ Rd×m as the coefficient matrix . Each column of W contains the coefficients of one risk target , and each row contains the coefficients of one feature in the m targets .
The link function g ( · ) describes the relationship between the mean of target response yj and features X . Depending on the type of target response yj , there are many commonly used link functions . In this paper , we consider two types of risks , which are continuous risks and binary risks . Their corresponding choices of link function are discussed in Section 22
One of the two major goals of our proposed framework is to explore the hidden association between risk targets . In this paper , we assume that the risk association is revealed in the structure of the coefficient matrix W . In the literature of multi task learning , representations of target relatedness can be categorized into two types . Methods belong to the first type use the sparsity patterns of wj ’s to reflect target relatedness . Related targets are assumed to share the same group or similar groups of features [ 1 , 7 ] . Methods in the second type use the covariance matrix of W to characterize risk association[24 ] . In this paper , we blend the two different representations into a unified framework . Both the sparsity pattern and the covariance matrix of W are used to characterize risk associations . We follow Zhang and Yeung ( 2010 ) [ 24 ] and assume that the coefficient matrix W follows a Matrix Variate Normal ( MVN ) distribution [ 15 ] , ie ,
W ∼ M V N ( 0 , Γ , Ω ) .
( 2 )
In model ( 2 ) , the first term 0 is a d by m matrix of zeros . It represents the location of W . The second term Γ is a d by d matrix . It represents the row wise covariances of W . In this paper we set Γ = ǫ2I , where ǫ2 is unknown , and is transformed into a tuning parameter in the objective function ( 6 ) . By setting Γ to be a diagonal matrix , we assume that rows of W are independent with each other . In other words , coefficients of different features in the same target are not correlated . This assumption can be relaxed without adding too much complexity to the model . The third block of parameter Ω is a m by m symmetric positive definite matrix . Ω represents the column wise covariance of W . It is unknown and reflects risk association .
Estimating Ω when the dimension m is moderate or large is known to be a difficult problem [ 4 ] . In the field of healthcare informatics , domain knowledge about risk association is often available or partially available . In order to utilize available domain knowledge , we impose a prior distribution on Ω :
Ω ∼ IW ( αΩ0 , ν ) ,
( 3 ) where IW denotes the Inverse Wishart distribution , α and ν are two tuning parameters , and Ω0 ∈ Rm×m is a known
1146 symmetric positive definite matrix . Ω0 contains all prior knowledge about risk association . When domain knowledge on risk association is available , the prior distribution can leverage the information and help improve the estimation of Ω . When domain knowledge about risk association is not available , we set Ω0 to be δI , where δ is an arbitrary small value . In both cases , Ω0 is positive definite . Thus can help stabilize the proposed algorithm and enhance robustness .
Combining models ( 1 ) , ( 2 ) and ( 3 ) , the full likelihood of
W and Ω is expressed as follows : p ( W , Ω|X , Y , Ω0 ) ∝ p ( W |X , Y , Ω ) p ( Ω|Ω0 ) .
( 4 )
We use the Maximum Like Estimation ( MLE ) method to estimate the coefficient matrix W and risk association matrix Ω . As mentioned in the previous paragraph , the sparsity pattern of W also reflects risk association . To enforce sparsity of W , we add an additional l1 regularizer on W .
The structures of W and Ω are closely related . For instance , highly correlated risks may have similar groups of “ active ” features , and coefficients of two related risks may be similar . As far as we know , existing methods either impose regularizers on W [ 2 , 8 , 10 , 13 , 17 , 18 , 19 , 20 , 26 ] or on Ω[22 ] . The connection between the two parts has not been fully utilized . In this paper , we propose a novel regularizer , called Linkage regularizer , to link the two components . The Linkage regularizer is given as follows : pen ( γ , Ω , W ) = γXi6=j
|Ωij| kwi − sign ( Ωij ) wjk1 .
( 5 )
The notions |·| , k·k1 , and sign(· ) denote the absolute value , the l1 norm , and the sign function , respectively . Note that when Ω is known , the Linkage regularizer ( 5 ) reduces to the Graph guided Fussed Lasso regularizer in Chen et al . ( 2012)[9 ] . In this paper , both W and Ω are unknown and needed to be estimated .
The Linkage regularizer links the two components W and Ω and let them reciprocally leverage information from each other . To see the effect of Linkage on W , we re write ( 5 ) as follows : pen ( γ , Ω , W )ij =  
γ |Ωij| kwi − wjk1 γ |Ωij| kwi + wjk1 0 for Ωij > 0 , for Ωij < 0 , for Ωij = 0 .
When Ωij > 0 , risk targets i and j are positively correlated . The regularizer enforces the distance between wi and wj to be small . Consequently , if one element in wi is zero , its counterpart in wj is forced to be small . Together with the l1 regularizer , wi and wj tend to have similar sparsity patterns . When Ωij < 0 , risk targets i and j are negatively correlated . The regularizer enforces the distance between wi and wj to be large . When Ωij = 0 , targets i and j are not correlated . In this case no restriction is imposed on the distance between wi and wj . γ controls the strength of the regularization .
The effect of ( 5 ) on Ω is less straight forward . To make it clear , we temporarily treat wi and wj as scalars and consider them fixed . When wj and wj have the same sign , risks i and j are positively correlated , and consequently |wi − wj| ≤ |wi + wj| . In this case a negative Ωij gets larger penalty than a positive one , and Ωij is “ pushed ” towards the positive direction . In the reverse case where wi and wj have opposite signs , risks i and j are negatively correlated , and we have |wi − wj| > |wi + wj| . A positive Ωij gets larger penalty than a negative Ωij , and Ωij is pushed towards the negative direction . Moreover , together with the l1 regularizer discussed before , Linkage regularizer is able to enforce similar sparsity pattern for associated targets .
Combining all aforementioned models and regularizers , the proposed model solves the following optimization problem : min W,Ω st l ( Y , X , W ) + tr λ1 +pen ( γ1 , Ω , W ) + γ2 kW k1 , Ω 0 .
2
W ⊤W +
λ2 2
Ω0 Ω−1 +
λ3 2 log det ( Ω )
( 6 )
The first term l ( · ) denotes the loss function , which is derived from the negative log likelihood function of the generalized linear model ( 1 ) ; tr and det denote the trace and determinant of a matrix ; and λ1 , λ2 , λ3 , γ1 , and γ2 are tuning parameters .
2.2 Loss Function
The loss function term in ( 6 ) depends on the choice of link function in model ( 1 ) , which further depends on the types of risks . In this paper , we consider two types of risks : the continuous risk and the binary risk .
When the support of yij spans the whole real line , ie yij ∈ ( −∞ , ∞ ) , risk j belongs to the continuous type . In this case yij is assumed to follow a Gaussian distribution , and the corresponding link function is the identity function . The loss function can be written as follows : lij = l,yij , x(i ) , wj =
1
2 flflyij − x(i)wjflfl
2 2 ,
( 7 ) where x(i ) denotes the i th row of X , and k·k2 denote the l2 norm .
When yij only have two possible outcomes , ie yij ∈ {−1 , 1} , risk j belongs to the binary type . In this case , yij is assumed to follow a Bernoulli distribution . The corresponding loss function can be written as follows : lij = l,yij , x(i ) , wj = log,1 + exp,−yijx(i)wj .
Let L be a n by m matrix where the ( i , j) th element lij is defined either as in ( 7 ) or in ( 8 ) . Let 1m denote a mdimensional vector of all 1s . The loss function in ( 6 ) is defined as the summation of lij across all i ( observations ) and all j ( risks ) . The loss function can be expressed as follows :
( 8 ) l ( Y , X , W ) = 1⊤ n L1m = n m
Xi=1
Xj=1 l,yij , x(i ) , wj .
( 9 )
When all risk targets belong to the continuous type , we refer to the model as a continuous model ; when all risk targets belong to the binary type , we refer to the model as a binary model ; when both types of risk targets exist , the model is referred to as a mixed model .
2.3 Incomplete Observations
Incomplete observations are ubiquitous in healthcare data . Particularly when jointly predicting multiple risks , it is often expensive , or impossible to obtain all information from all samples/patients . In this paper , we deal with incomplete observations in risk targets ( Y ) and features ( X ) using different methods . Unobserved values in X are imputed in advance using off the shelf imputation methods , such as the K nearest neighbor method .
1147 Missing values in Y are excluded from the objective function ( 6 ) . Let S be a matrix of zeros and ones with the same dimension as Y . Particularly , Sij = 1 if Yij is observed , and Sij = 0 otherwise . Note that Y is involved in the objective function only through the loss function term . We replace L in ( 6 ) with ˜L = S ◦ L , where ◦ denotes Hadamard product . In this way , missing values have no contribution to the objective function , thus are excluded from the analysis .
Another way to view S is that it works as a weighting matrix . In the above example , all unobserved values get extremely low weights ( ie 0 ) , and all observed values have equal weights ( ie 1 ) . It is possible that elements of S take values other than 0 and 1 , and observed values could receive unequal weights . For example if a binary risk is highly unbalanced , a properly chosen S could mitigate the unbalanceness . How to chose a proper S is beyond the scope of this paper and we do not discuss the issue here . For simplicity of notation , we do not distinguish L and ˜L in the following discussion unless necessary .
2.4 Algorithm
Solving the optimization problem ( 6 ) is non trivial due to the following reasons : 1 ) the term log det ( Ω ) is concave in Ω , making the objective non convex ; 2 ) the Linkage regularizer involves the sign function and product terms of Ωij and wj . In this paper , we propose an iterative algorithm to solve the problem . Within each iteration , the two blocks W and Ω are updated alternatively . In the following discussion , we refer to the iteration as the global iteration .
Update W .
In the t th global iteration , we fix Ω = Ω(t−1 ) and update W . The optimization problem in this step is given as follows : min W l ( Y , XW ) +
λ1 2 trW ⊤W Ω−1 + pen ( γ1 , Ω , W ) + γ2 kW k1 .
( 10 )
When Ω is fixed , the Linkage regularizer reduces to the Graph guided Fussed Lasso regularizer . We follow Chen et al . ( 2012)[9 ] and use the Smoothing Proximal Gradient ( SPG ) method to solve the optimization problem ( 10 ) .
Reformulate the Linkage regularizer as maxkAk∞≤1CW ⊤ , Aff , where A is an auxiliary matrix , k·k∞ is the l∞ norm , and C is a m(m − 1) by m matrix with
C(i,j),k =  
γ1 |Ωij| −γ1sign ( Ωij ) |Ωij| 0 if k = i , if k = j , otherwise .
( 11 )
Using the technique from Nesterov ( 2005)[21 ] , a smooth approximation to the Linkage regularizer is constructed as follows :
DCW ⊤ , A∗E −
µ 2 kA∗k2
F ,
( 12 ) where µ > 0 is arbitrary small number , A∗ and gh ( · ) is the hard thresholding function defined as follows : ij = gh,CW ⊤ ij /µ , gh ( x ) =   x 1 −1 for − 1 ≤ x ≤ 1 , for x > 1 , for x < −1 .
It has been shown in Chen et al . ( 2012)[9 ] that the expression ( 12 ) is convex and smooth in W . The gradient of ( 12 ) wrt W is given by ( A∗)⊤ C . Replacing the Linkage regularizer with the smooth approximation ( 12 ) , we solve the following optimization problem : min W l ( Y , X , W ) +
λ1 2 trW ⊤W Ω−1 +DCW ⊤ , A∗E
−
µ 2 kA∗k2
F + γ2 kW k1 .
( 13 )
Next , using the proximal method , the optimization problem ( 13 ) is solved by iteratively solving the following problem : min W
1 2 kW − V k2
F +
γ2 η kW k1 ,
( 14 ) where
V = W ( k−1)−
1
η h∇W lY , X , W ( k−1 ) + λ1W ( k−1)Ω−1 + ( A∗)⊤ Ci .
To distinguish from the global iteration , we refer to this iteration as the inner SPG iteration . W ( k−1 ) is the solution obtained from the previous inner SPG iteration , and η is the step size . We use the ISTA with backtracking algorithm in Beck and Teboulle ( 2009)[3 ] to decide η .
Problem ( 14 ) becomes the Graphical Lasso problem [ 11 ] . The problem can be solved by applying the soft thresholding rule to each element of V . The solution is given as follows : vij − γ2 η vij + γ2 η 0 for vij ≥ γ2 η , for vij ≤ − γ2 η , otherwise .
( 15 ) w(t ) ij =  
To sum up , Algorithm 1 gives the steps to update W .
Algorithm 1 SPG algorithm to update W
Require : Ω = Ω(t−1 ) from last global iteration , data set X ,
Y , regularization parameters λ1 , γ1 , γ2 , and µ
1 : Initialize β(0 ) = W ( t−1 ) 2 : for k = 0 , 1 , 2 , . . . until convergence of β(k ) do 3 : 4 :
Formulate C according to ( 11 ) and calculate A∗ Compute ∇h W ( t−1 ) = ∇l W ( t−1)+ λ1W ( t−1)⊤ ( A∗)⊤ C Line search for step size η Compute V = β(k−1 ) − 1 Update β(k ) according to ( 15 )
η ∇hW ( t−1 )
5 : 6 :
7 : 8 : end for 9 : Update W ( t ) = β(k )
Ω−1 +
Update Ω .
In this step , we fix W and update Ω . The optimization problem in this step becomes the follows :
λ3 2 min
Ω st Ω 0 , log det ( Ω ) + tr.QΩ−1fi + pen ( γ1 , Ω , W ) ,
( 16 ) where Q = λ1
2 W ⊤W + λ2
2 Ω0 .
Problem ( 16 ) is closely related to the optimization problem in Bien and Tibshirani ( 2011)[5 ] . There are two differences between the two optimization problems . First , Bien and Tibshirani ( 2011 ) imposes a l1 regularizer on Ω , whereas in this paper we use the Linkage regularizer . Second , Q in
1148 [ 5 ] ( denoted as S ) denotes the sample covariance matrix . In ( 16 ) , Q is a combination of the covariance matrix of W and the prior matrix Ω0 . The second difference provides a flexible way to blend data driven approach and knowledge driven approach to discover risk association .
The first term in problem ( 16 ) is concave . We follow Bien and Tibshirani ( 2011)[5 ] and apply the Majorize Minimization ( MM ) method[16 ] iteratively to deal with the non convexity . To avoid confusion with later discussion , we refer to the iteration as the outer iteration . Let Ω(k−1 ) denote the solution of Ω obtained from the last outer iteration . At the k th outer iteration , the term log det ( Ω ) is majorized by its tangent at Ω(k−1 ) . Let Σ(0 ) = Ω(k−1 ) , we iteratively solve the following objective function :
λ3 2 min
Ω st Ω 0 . tr(Σ(0))−1Ω + tr,QΩ−1 + pen ( λ1 , Ω , W ) ,
( 17 )
Within the k th outer iteration , We use the proximal method and iteratively solve the the optimization problem ( 18 ) . To distinguish from the outer iteration , we refer to the iteration as the inner iteration . min
Σ
1 2 kΣ − U k2
F + pen γ1
, Σ , W , 2 ( Σ(0))−1 −Σ(i−1)−1
η
η λ3 where U = Σ(i−1)− 1
( 18 )
QΣ(i−1)−1 , k·kF denotes the Frobenius norm , η is the step size , and Σ(i−1 ) denotes the solution from the last inner iteration . The following theorems gives the solution for optimization problem ( 18 ) .
Theorem 1 . Let a and b be two vectors of the same dimension , γ and x be two scalars . The univariate Linkage regularizer pen ( x ; γ , a , b ) = γ |x| ka − sign ( x ) bk1 is continuous and convex in x . The solution x∗ for the problem minx
2 + pen ( x ; γ , a , b ) is given as follows :
2 kx − uk2 x∗ = sign ( u),|u| − γ ka − sign ( u ) bk1 + .
Proof . Reformulate the function pen ( x ; γ , a , b ) as fol
( 19 )
1 lows : pen ( x ; γ , a , b ) =  
γ ka − bk1 |x| γ ka + bk1 |x|
0 for x > 0 , for x < 0 , otherwise .
Note that c1 = ka − bk1 and c2 = ka + bk1 are two nonnegative constants . The function pen ( x ; γ , a , b ) resembles the absolute function with different slope on the negative and positive sides . It is easy to see that pen ( x ; γ , a , b ) is continuous and convex in x . An example of the function pen ( x ; γ , a , b ) is given in panel ( a ) of Figure 1 , in which c1 = 2 , c2 = 0.5 and γ = 05
Let ∂fx denote the sub gradient of fx wrt x . At the point x∗ , 0 ∈ ∂fx ( x∗ ) . When x > 0 , we have ∂f = x − u + c1γ = 0 ⇒ x = u − c1γ > 0 ⇒ u > c1γ ; When x < 0 , we have ∂fx = x − u − c2γ = 0 ⇒ x = u + c2γ < 0 ⇒ u < −c2γ ; and x = 0 otherwise . Therefore the solution can be written as follows :
( a )
( b )
Figure 1 : Plots of ( a ) the univariate Linkage regularizer , ( b ) the asymmetric soft thresholding function
Panel ( b ) of Figure 1 demonstrates an example of the function g(u ; γ , a , b ) . When u falls in the region between the two red dotted lines , a sparse solution is attained . The solution can be regarded as a soft thresholding rule with different thresholds on the positive and negative parts . The underlying rationale is that the relative scales of c1 and c2 reveal the information about whether two targets are more likely to be positively or negatively correlated . When c1 > c2 , two targets are more likely to be negatively correlated . The slope of the penalty function in the positive part is greater than that in the negative part , and the positive parts gets higher penalty . Similarly , when c1 < c2 , two targets are more likely to be positively correlated . In this case , the negative part gets higher penalty . Due to the asymmetric property of the solution ( 20 ) , we refer to it as the asymmetric soft thresholding rule .
Theorem 1 gives the solution for univariate Linkage regularizer . The solution for problem ( 18 ) is obtained by applying the asymmetric soft thresholding rule to all off diagonal elements of U in ( 18 ) . Algorithm 2 summarizes the steps to update Ω . The following Proposition gives the properties of the solution .
Proposition 2 . Let g(x ; γ , a , b ) denote the asymmetric thresholding rule described in ( 20 ) . Problem ( 18 ) can be solved by applying g(x ; γ , wi , wj ) element wise to Ωij for all i 6= j . Under the regularity condition ( 5 ) in Bickel and Levina ( 2007 ) [ 4 ] , the obtained solution is consistent with the true covariance matrix Ω.[23 ]
A sketch of proof for Proposition 2 is given in Appendix B . To sum up , the steps to update Ω are described in Algorithm 2 . In the global iteration , Algorithms 1 and 2 are applied alternatively until converge .
Preserve positive definiteness of Ω .
Note that the thresholding operator does not necessarily preserve positive definiteness even when applied only to offdiagonal elements[14 ] . To retain the positive definiteness of the solution , we bound the minimum eigenvalue of Ω by a small δ > 0 . If the minimum eigenvalue of the obtained solution from ( 18 ) is greater than or equal to δ , positive definiteness is preserved . If the minimum eigenvalue is less than δ , we solve the following problem : x∗ = g(u ; γ , a , b ) =   u − γ ka − bk1 u + γ ka + bk1 0 if u > γ ka − bk1 , if u < −γ ka + bk1 , otherwise . min ΣδI
1 2 kΣ − U k2
F + pen γ1
η
, Σ , W
( 21 )
The solution can be simplified into ( 19 ) .
( 20 )
We use the alternating direction method of multipliers ( ADMM ) method[6 ] to solve ( 21 ) . The algorithm is de
1149 Algorithm 2 Asymmetric thresholding rule to update Ω
Require : W ( t−1 ) from last iteration , regularization param eters λ1 , λ2 , λ3 , γ1 , and Q = λ1
2 W ( t)⊤
W ( t ) + λ2
2 Ω0
1 : Initialize Λ(0 ) = Ω(t−1 ) 2 : for k = 0 , 1 , 2 , . . . until convergence do 3 : 4 : 5 : 6 :
Set Σ(0 ) = Λ(k−1 ) for i = 0 , 1 , 2 , . . . until convergence do
Compute U according to ( 18 ) and line search for η Apply the asymmetric soft thresholding rule to offdiagonal elements of U and update Σ(i ) end for Update Λ(k ) = Σ(i )
7 : 8 : 9 : end for 10 : Update Ω(t ) = Λ(k ) scribed in Appendix Algorithm 3 . The choice of δ can be arbitrary , or can be calculated using the method in Appendix 2 of Bien and Tibshirani ( 2011)[5 ] .
3 . EXPERIMENTS
In this section , we evaluate the performance the Linkage method through synthetically generated data sets and real world examples . We compare our method with the baseline model in which each target is modeled and predicted individually .
3.1 Synthetic Dataset
We conducted simulation studies in three scenarios . In the first scenario , all risk targets are continuous ; in the second scenario , all targets are binary ; and in the third scenario , the targets contain both continuous and binary types . We evaluate the performance of the proposed framework through three aspects : ( i ) the performance of the predicted values ; ( ii ) the performance of estimated W ; and ( iii ) the performance of estimated target covariance matrix Ω .
Under each scenario , we generate 10 targets and 20 features . The covariance matrix Ω is set to be a block diagonal matrix with two blocks each containing 5 risks . W is generated according to ( 2 ) with Γ = I . A continuous risk yj is generated from the Normal distribution N ,Xwj , σ2 = 1 .
A binary risk yj is generated from the Bernoulli distribution with p = 1/ ( 1 + exp ( −Xwj) ) . Features are generated from the normal distribution N ( 0 , 1 ) . Ridge Regression and Logistic regression with l2 regularizer are used as the baseline models for continuous and binary targets , respectively . We use 10 fold cross validation to both Linkage and baseline model to select the tuning parameter(s ) . For each scenario , we repeat the experiment 20 times .
The relative change in the objective function ( 6 ) between two consecutive global iterations is used as the stopping criterion . We tested the convergence of the proposed algorithm under various settings . Based on our observation , the algorithm always converges and converges to the same point with different initial values . Figure 2 shows one example of the convergence of the algorithm . The x axis denote the iteration numbers , and the y axis denote the value of the objective function . In most cases , the algorithm converges within 5 global iterations .
Figure 2 : Example of convergence of Linkage algorithm
311 Performance of Predictions
For continuous targets , the average MSE of the predicted values across all targets , denoted as M SEp , is used to evaluate model performance . For binary risks , the average Area Under Curve ( AUC ) is used as the evaluation criterion . The mean of M SEp and AUC across 20 repeated experiments under the three scenario are presented in Table 1 . Their standard deviations are also reported ( in parenthesis ) . In all three scenarios , Linkage consistently outperform the baseline model . The improvements in scenarios 1 and 2 are greater than the improvements in scenario 3 . The reason is that in mixed models , selecting the step size η in the step of updating W is more challenging than continuous models or binary models . Therefore the improvement is mitigated .
312 Performance of Estimated W
Next we evaluate the performance of estimated W , denoted as ˆW . The average MSE of ˆW , denoted as M SEw , is used as the evaluation criterion . Average M SEw across 20 repeated experiments in the three scenarios , together with their standard deviations , are demonstrated in Table 1 . In all three scenarios , Linkage has better performance than the baseline model .
313 Performance of Estimated Ω
Next we evaluate the performance of estimated Ω , denoted as ˆΩ . Similar as in the previous paragraph , the average MSE of ˆΩ is used as the evaluation criterion . The average MSEs of ˆΩ across 20 repeated experiments in the three scenarios and their standard deviations are summarized in the last two rows of Table 1 . Linkage has lower M SEΩ than the baseline model in all three scenarios .
In the context of comprehensive risk prediction , risks usually form groups . We conduct another experiment to demonstrate that the estimated Ω from Linkage is able to preserve group structures . We generated 20 targets and 50 features with 1000 observations . Targets are randomly assigned to continuous or binary type . The 20 targets are separated into 5 groups . The first four groups contain 3 6 targets . The last target set to be an “ outlier ” target and is not correlated with any other targets . The heatmaps of ˆΩ and the true Ω are showed in Figure 3 , respectively . It is clear from Figure 3 that : ( i ) the Linkage can yield sparse estimation of Ω , making it desirable when there exist group structures among risks ; ( ii ) the block structure of Ω is successfully preserved except that a few “ off block ” elements are estimated as non zero ; ( iii ) the “ outlier ” target is separated from other groups of targets ; ( iv ) the color within each block is lighter in ˆΩ than that in Ω . This phenomenon is expected since Linkage results in element wise shrinkage estimate of Ω .
1150 Table 1 : Results from synthetic data
M SEp
AUC
M SEw
M SEΩ‘
Scenario 1
Scenario 2
Scenario 3
Linkage 0.9851 baseline
1.042
( 1.288 × 10−2 )
( 1.329 × 10−2 )
−−
−−
−−
−−
1.654 × 10−3 ( 5.091 × 10−5 ) 3.027 × 10−4 ( 8.104 × 10−4 )
1.691 × 10−3 ( 5.036 × 10−4 ) 1.216 × 10−3 ( 1.126 × 10−3 )
Linkage
−−
−− 0.979 baseline
−−
−− 0.842
( 9.302 × 10−3 ) 4.449 × 10−2 ( 2.425 × 10−2 ) 1.636 × 10−3 ( 1.075 × 10−3 )
( 1.710 × 10−2 ) 4.488 × 10−2 ( 2.437 × 10−2 ) 3.861 × 10−3 ( 2.715 × 10−3 )
Linkage
1.001 baseline
1.006
( 1.646 × 10−2 )
( 1.520 × 10−2 )
0.879
( 4.119 × 10−2 ) 4.484 × 10−2 ( 1.414 × 10−3 ) 1.684 × 10−3 ( 1.351 × 10−3 )
0.876
( 4.141 × 10−2 ) 4.526 × 10−2 ( 1.178 × 10−3 ) 4.459 × 10−2 ( 3.883 × 10−3 )
Targets Provided help to family friends or neighbors Gone to sport social or other kind of club Give help to others outside the household Depression Pessimism Sleep Irritability Appetite Concentration Activities of daily living index Instrumental activities of daily living index Mobility index Recall of words score Orientation to date Numeracy score
Group Social Social Social Mental Mental Mental Mental Mental Mental
Types Binary Binary Binary Binary Binary Binary Binary Binary Binary
Functional Continuous Functional Continuous Functional Continuous Cognitive Continuous Cognitive Cognitive
Binary Binary
Table 2 : Risk targets in easySHARE dataset health , behavior risk , healthcare , occupation and income . Due to limited space , detailed description features are not listed in this paper .
Both targets and features have missing values due to nonresponse and questionnaire filtering . The average missing value rates in targets and features are 6.9 % and 5.1 % , respectively . Missing values in features are imputed in advance using the K nearest neighbor method . The distance between sample i and sample j is defined as the averaged l2 distance of elements that are observed both in i and j . Let x(i ) and x(j ) denote the feature vectors of samples i and j . Let Oi and Oj be the sets of indices of observed elements in x(i ) and x(j ) , and Ci∩j denote the cardinality of Oi ∩ Oi . The distance between the two samples is calculated ( xik − xjk)2 . Unobserved values in by dij = 1 targets are handled using the method discussed in Section 23
Ci∩j Pk∈Oi∩Oi
322 Getting prior Ω0
In Section 2 , we discussed that prior knowledge on target association could be incorporated through Ω0 In the easySHARE dataset , our presumption is that targets from the same interview module are related with each other . For example in the functional module , the Activities of Daily Living ( ADL ) index , the Instrumental Activities of Daily Living index ( IADL ) , and the Mobility index all aim at assessing patients’ adequacies of performing basic daily functions . It is reasonable to assume that these three risk targets are correlated . On the other hand , relationship between targets from different modules is unclear . Based on the above argument , the prior matrix Ω0 is set to be a block diagonal matrix . Each block contains the risks in one interview
( a )
( b )
Figure 3 : Heatmaps of ( a ) ˆΩ ; and ( b ) the true Ω
3.2 easySHARE Data
321 Data description
In this section , we apply the Linkage framework to the easySHARE dataset , which is a simplified dataset from the Survey of Heath , Aging , and Retirement in Europe ( SHARE ) . SHARE includes multidisciplinary and cross national panel database on health , socio economic status , and social and family networks of more than 85,000 individuals from 20 European countries aged 50 or over . The easySHARE dataset is a simplified dataset adapted for comparability with the US Health and Retirement Study ( HRS ) . Four waves of interviews were conducted during 2004 2005 , 2006 2007 , 20082009 , and 2010 2011 , respectively , and are referred to as WAVE1 to WAVE4 interviews . In this paper , we used the WAVE1 and WAVE2 easySHARE data .
In total 20449 sample persons attended both WAVE1 and WAVE2 interviews . Multiple modules of interviews were conducted . The easySHARE data contains 105 variables from 13 interview modules . We extract risk targets from WAVE2 interview , and features from WAVE1 interview . Based on a literature review on comprehensive geriatric assessment , 15 variables are selected as risk targets . The 15 risks come from four interview modules : “ Social Support & Network ” , “ Mental Health ” , “ Functional Limitation Indices ” and “ Cognitive Function Indices ” . In the following discussion , we refer to these four modules as the Social , Mental , Functional and Cognitive modules . Among the 15 risks , 11 are binary , and 4 are continuous . Therefore , we build a mixed type Linkage model in this section . The description of the risks , their interview modules , as well as their types , are listed in Table 2 . Totally 75 features are constructed from 46 variables in WAVE1 interview . The features cover a wide range of assessments , including demographics , household composition , social support and network , physical health , mental
1151 module . The sample covariance matrix of yj ’s for j from the same module is used to fill the block .
323 Prediction Performance
In this section we evaluate the performance of the predicted values and compare Linkage with the baseline model . Same as in the simulation experiments , Ridge Regression and Logistic Regression with l2 regularizer are used as the baseline models . In Linkage , link functions for individual risks are chosen according to the types of the risks . We use 10 fold cross validation to select the tuning parameters . Let X k train be the training data in the k th fold , and test the testing data in the k th fold . Let ˆwk X k j be the estimated coefficient vector of the j th target in the k th fold . The predicted values in the the k th fold are calculated by ˆyk j for continuous targets , and train , Y k test and Y k test,j = X k test ˆwk
ˆyk test,j = sign
1
1+exp(−X k j ) test ˆwk
− 0.5 for binary targets .
Predicted values from the 10 folds are combined together . For notational simplicity , we denote the predicted value of the i th sample in the j th risk as ˆyij . The MSEs of predicted values are calculated for all continuous targets and are showed in Table 3 . The MSEs are decrease by 72 84 % , indicating that Linkage has significantly better performance than the baseline model on easySHARE data .
Table 3 : Mean Squared Errors of continuous targets in easySHARE data
Target ADL IADL Mobility Recall of words score
Linkage 0.0777 0.0787 0.1517 0.0924
Baseline 0.5143 0.4157 0.5901 0.3304
Two criteria are used to evaluate the prediction performances for binary targets . First , we evaluate the prediction accuracies from Linkage and the baseline model . The prediction accuracy of a target is defined to be the proportion of correctly predicted samples among all observed samples , ie accuj =
# of correctly predicted samples
# of observed samples
, and the overall prediction accuracy of a model is defined to be the average prediction accuracies across all binary targets . The overall prediction accuracy from Linkage and the baseline models are 0.8192 and 0.7513 , respectively . Linkage improves the baseline model by approximately 9 % . Prediction accuracies of individual targets are depicted in panel ( a ) of Figure 4 . Linkage outperforms the baseline model in 10 out of 11 targets . Furthermore , we plot the improvement of prediction accuracies in Linkage versus the prediction accuracies from the baseline model . The scatter plot is showed in panel ( b ) of Figure 4 . The plot demonstrates a negative relationship between the improvements and the prediction accuracies from the baseline model . Risks which have relatively low prediction accuracies from the baseline model ( eg sleep , pessimism ) achieve greater improvements . On the other hand , risks that already achieve high prediction accuracies from the baseline model ( eg appetite , orientation to date ) only receive minor improvements or do not improve upon the baseline model . linkage baseline
1 .0
0.9
0.8
0.7
0.6
0.5
0.4
0.3 y c a r u c c a n o i i t c d e r p club help fa mily help other depression pessimism
0.40
0.35
0.30
0.25
0.20
0.15
0.10
0.05
0.00
−0.05 t n e m e v o r p m i
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
1.1 prediction accuracy from baseline model sleep irritability appetite concentration date nu m eracy
( a )
( b )
Figure 4 : Prediction accuracies in easySHARE data
Figure 5 : ROC Curve of Combined Predictions
Second , we generate the Receiver Operating Characteristic ( ROC ) curves from Linkage and the baseline model , and we calculated their corresponding AUCs . To evaluate the overall performance of Linkage , we combine all binary risks together and generate the overall ROC curves . The curves are showed in Figure 5 . The overall AUCs from the baseline model and Linkage are 0.7578 and 0.8436 , respectively . Linkage gets approximately 9.5 % improvement from the baseline model . AUCs of individual risks are showed in Table 4 . Although the magnitude of improvements in AUCs are less significant than that in prediction accuracies , AUCs from Linkage are higher than the baseline model in all 11 binary targets . The improvements in AUC also demonstrate a negative relationship with AUCs from the baseline model . Together with the results in previous paragraph , the observations indicate that by leveraging the relationship between targets , the risk targets that have poor performance from baseline model can be significantly improved .
324 Risk Association
In this section we discuss the estimated risk association matrix Ω from Linkage . The estimated covariance matrix is denoted as ˆΩ . For fair comparison between risk targets , the covariance matrix is transformed to correlation matrix . Let ˆR denote the estimated correlation matrix , and ˆRij the ˆRij ranges correlation coefficient between risks i and j . from −1 to 1 . The sign of ˆRij represents the direction of represents the strength of the association . We perform a hierarchical clus risk association , and the magnitude of fififi
ˆRijfififi
1152 AUC
Target help family go to club help others depression pessimism sleep irritability appetite concentration orientation to date numeracy
Linkage 0.7531 0.7537 0.7395 0.7423 0.7403 0.7293 0.7025 0.7443 0.7381 0.8604 0.9401 baseline Linkage 0.8196 0.7497 0.7483 0.7930 0.7229 0.7375 0.7172 0.7406 0.8752 0.7383 0.7213 0.7272 0.6981 0.7706 0.9114 0.7420 0.8109 0.7324 0.9728 0.8562 0.9378 0.8965
Prediction Accuracy baseline 0.7459 0.7164 0.7100 0.7144 0.6816 0.3650 0.7647 0.9093 0.8122 0.9735 0.8717
Table 4 : Performance of predictions in binary targets tering on the 15 risks using 1 −fififi as the distance measure . The heatmap of ˆR is showed in Figure 6 . Both the rows and columns in Figure 6 are reordered from the hierarchical clustering . The row labels ( on the right ) are the risk names , and the column labels ( at the bottom ) gives the interview modules of the risks .
ˆRfififi
Figure 6 clearly indicates that targets form groups . Using half of the maximum distance as the cutoff , the 15 risks are separated into three groups . The first group ( on the bottom left of Figure 6 ) consists of three targets from the Cognitive module . The second group consists of three targets from the Functional module and one target ( appetite ) from the Mental module . The two groups are negatively correlated . Note that the direction of the between group correlation is related with the how the the risk targets are coded . In the easySHARE dataset , higher functional values indicate more difficulties with functional activities , whereas positive cognitive values indicate better cognitive abilities . Therefore , the negative correlation actually indicates that that functional and cognitive abilities are positively correlated . The remaining eight targets form the third group . In general , targets in the third groups are not correlated with each other . However , targets in the third group could be correlated with targets in the first two groups . For example , targets in the Mental module ( sleep , depression , irritability and concentration ) are correlated with appetite , which is also from the Mental module .
4 . CONCLUSION
In this paper , we propose a framework called Linkage for comprehensive risk prediction . Two goals , predicting multiple risks and learning the relationship between risk targets are done simultaneously . The newly proposed Linkage regularizer allows the coefficient matrix W and the target covariance matrix Ω to reciprocally leverage information from each other . Therefore , achieve better performance . Linkage is convex in both W and Ω when the other part is fixed . We developed an alternating method to solve the framework and prove that under certain regularity condition , the solution has good properties . We have conducted extensive evaluations on synthetic and real datasets . The results show that the proposed framework can improve the estimate of coefficient matrix and risk prediction , and at the same time has good performance in learning and understanding the relationship between risk targets .
Figure 6 : Heatmap of Learned Correlation Matrix ˆR
5 . REFERENCES
[ 1 ] R . K . Ando and T . Zhang . A framework for learning predictive structures from multiple tasks and unlabeled data . Journal of Machine Learning Research , 6:1817–1853 , Dec . 2005 .
[ 2 ] A . Argyriou , S . Cl´emen¸con , and R . Zhang . Learning the Graph of Relations Among Multiple Tasks . Rapport de recherche , Oct 2013 .
[ 3 ] A . Beck and M . Teboulle . A Fast Iterative
Shrinkage Thresholding Algorithm for Linear Inverse Problems . SIAM Journal on Imaging Sciences , 2(1):183–202 , Mar . 2009 .
[ 4 ] P . J . Bickel and E . Levina . Covariance regularization by thresholding . The Annals of Statistics , 36(6):2577–2604 , 2008 .
[ 5 ] J . Bien and R . J . Tibshirani . Sparse estimation of a covariance matrix . Biometrika , 98(4):807–820 , 2011 .
[ 6 ] C . S . Boyd , N . Parikh , E . Chu , B . Peleato ,
J . Eckstein , S . Boyd , N . Parikh , E . Chu , B . Peleato , and J . Eckstein . Distributed optimization and statistical learning via the alternating direction method of multipliers . Foundations and Trends in Machine Learning , pages 1–24 , 2011 .
[ 7 ] J . Chen , L . Tang , J . Liu , and J . Ye . A convex formulation for learning shared structures from multiple tasks . In Proceedings of the 26th Annual International Conference on Machine Learning ( ICML ) , pages 137–144 , Montreal , Quebec , Canada , June 2009 .
[ 8 ] J . Chen , J . Zhou , and J . Ye . Integrating low rank and group sparse structures for robust multi task learning .
1153 In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( KDD ) , pages 42–50 , 2011 .
[ 9 ] X . Chen , Q . Lin , S . Kim , J . G . Carbonell , and E . P .
Xing . Smoothing proximal gradient method for general structured sparse regression . The Annals of Applied Statistics , 6(2):719–752 , 2012 .
[ 10 ] T . Evgeniou and M . Pontil . Regularized multi task learning . In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( KDD ) , pages 109–117 , New York , NY , USA , 2004 .
[ 11 ] J . Friedman , T . Hastie , and R . Tibshirani . Sparse inverse covariance estimation with the graphical lasso . Biostatistics , 9(3):432–441 , 2008 .
[ 12 ] A . R . Goncalves , P . Das , S . Chatterjee , V . Sivakumar ,
F . J . V . Zuben , and A . Banerjee . Multi task sparse structure learning . In 23rd ACM International Conference on Information and Knowledge Management ( CIKM ) , pages 451–460 , Nov 2014 . [ 13 ] P . Gong , J . Ye , and C . Zhang . Robust multi task feature learning . In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( KDD ) , pages 895–903 , 2012 .
[ 14 ] D . Guillot and B . Rajaratnam . Functions preserving positive nefiniteness for sparse matrices . Transactions of the American Mathematical Society , 2012 .
[ 15 ] A . Gupta and D . Nagar . Matrix Variate Distributions .
Monographs and Surveys in Pure and Applied Mathematics . Chapman & Hall , 1999 .
[ 16 ] D . R . Hunter and R . Li . Variable selection using mm algorithms . Annals of Statistics , 33(4):1617–1642 , 2005 .
[ 17 ] L . Jacob , F . Bach , and J P Vert . Clustered multi task learning : A convex formulation . In D . Koller , D . Schuurmans , Y . Bengio , and L . Bottou , editors , Advances in Neural Information Processing Systems ( NIPS ) , pages 745–752 . Curran Associates , Inc . , 2008 .
[ 18 ] S . Kim and E . P . Xing . Tree guided group lasso for multi task regression with structured sparsity . In Proceedings of the 27th International Conference on Machine Learning ( ICML ) , pages 543–550 , Haifa , Israel , June 2010 .
[ 19 ] J . Liu , S . Ji , and J . Ye . Multi task feature learning via efficient l2 , 1 norm minimization . In J . Bilmes and A . Y . Ng , editors , The Conference on Uncertainty in Artificial Intelligence ( UAI ) , pages 339–348 . AUAI Press , 2009 .
[ 20 ] Y . Liu , A . Wu , D . Guo , K T Yao , and
C . Raghavendra . Weighted task regularization for multitask learning . In International Conference on Data Mining Workshops ( ICDMW ) , pages 399–406 , Dec 2013 .
[ 21 ] Y . Nesterov . Smooth minimization of non smooth functions . Mathematical Programming , 103(1):127–152 , May 2005 .
[ 22 ] P . Rai , A . Kumar , and H . D . III . Simultaneously leveraging output and task structures for multiple output regression . In Advances in Neural Information Processing Systems ( NIPS ) , pages 3194–3202 , 2012 .
[ 23 ] A . J . Rothman , E . Levina , and J . Zhu . Generalized thresholding of large covariance matrices . Journal of the American Statistical Association , 104(485):177–186 , 2009 .
[ 24 ] Y . Zhang and D . Yeung . A convex formulation for learning task relationships in multi task learning . In Proceedings of the Twenty Sixth Conference on Uncertainty in Artificial Intelligence ( UAI ) , pages 733–442 , July 2010 .
[ 25 ] Y . Zhang and D Y Yeung . A regularization approach to learning task relationships in multitask learning . ACM Transactions on Knowledge Discovery from Data ( TKDD ) , 8(3):12 , 2014 .
[ 26 ] J . Zhou , J . Chen , and J . Ye . Clustered multi task learning via alternating structure optimization . In J . Shawe Taylor , R . S . Zemel , P . L . Bartlett , F . C . N . Pereira , and K . Q . Weinberger , editors , Advances in Neural Information Processing Systems ( NIPS ) , pages 702–710 , 2011 .
APPENDIX
A . ADMM ALGORITHM
Algorithm 3 Alternating Direction Method of Multiplier
Require : δ , ρ , step size η . 1 : Initialize Θ(0 ) = Λ(k ) from Algorithm 2 and Z ( 0 ) = 0 2 : If the minimum eigenvalue of Θ(0 ) < δ , proceed : 3 : for i = 1 , 2 , . . . until converge do
4 : Diagonalize n,Σ(i−1) − η,Σ(0) −1 +ρΘ(i−1 ) − Z(i−1 ) ) /(1 + ρ ) = T DT ⊤ Σ(i ) = T DδT ⊤ , where Dδ = diag {max ( Djj , δ)}
−,Σ(i−1) −1
5 :
Q,Σ(i−1) −1
6 : Θ(i ) = Gnγ/ρ , Σ(i−1 ) + Z ( i−1)/ρ , Wo , where G de note element wise asymmetric soft thresholding
Z ( i ) = Z ( i−1 ) + ρΣ(i ) − Θ(i )
7 : 8 : end for 9 : Return Λ(k ) = Σ(i )
B . PROOF OF PROPOSITION 2
Without loss of generality , we assume that kwi − wjk1 is less than kwi + wjk1 . The asymmetric thresholding rule satisfies the following conditions for all x ∈ R :
( i ) |g(x ; γ , wi , wj)| ≤ |x| ( ii ) |g(x ; γ , wi , wj)| = 0 for |x| ≤ kwi − wjk1 ( iii ) |g(x ; γ , wi , wj ) − x| ≤ kwi + wjk1
Therefore , the thresholding rule ( 18 ) belongs to the class of generalized thresholding rules in Rothman , Levina and Zhu ( 2008)[23 ] . Based on Theorem 1 in [ 23 ] and Theorem 1 in Bickel and Levina ( 2008)[4 ] , the solution ˆΩ is consistent with Ω . For detailed proof , please refer to [ 23 ] and [ 4 ] .
1154
