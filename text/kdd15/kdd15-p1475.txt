Deep Model Based Transfer and Multi Task Learning for
Biological Image Analysis
Wenlu Zhang1 , Rongjian Li1 , Tao Zeng1 , Qian Sun2 , Sudhir Kumar3,4,5 , Jieping Ye6,7 and
Shuiwang Ji1
1Dept . of Computer Science , Old Dominion University , Norfolk , VA , 23529
2Dept . of Computer Science and Engineering , Arizona State University , Tempe , AZ 85287
3Institute for Genomics and Evolutionary Medicine , Temple University , Philadelphia , PA 19122
4Dept . of Biology , Temple University , Philadelphia , PA 19122
5Center of Excellence in Genomic Medicine Research , King Abdulaziz University , Jeddah , Saudi Arabia
6Dept . of Computational Medicine and Bioinformatics , University of Michigan , Ann Arbor , MI 48109 7Dept . of Electrical Engineering and Computer Science , University of Michigan , Ann Arbor , MI 48109
ABSTRACT A central theme in learning from image data is to develop appropriate image representations for the specific task at hand . Traditional methods used handcrafted local features combined with high level image representations to generate image level representations . Thus , a practical challenge is to determine what features are appropriate for specific tasks . For example , in the study of gene expression patterns in Drosophila melanogaster , texture features based on wavelets were particularly effective for determining the developmental stages from in situ hybridization ( ISH ) images . Such image representation is however not suitable for controlled vocabulary ( CV ) term annotation because each CV term is often associated with only a part of an image . Here , we developed problem independent feature extraction methods to generate hierarchical representations for ISH images . Our approach is based on the deep convolutional neural networks ( CNNs ) that can act on image pixels directly . To make the extracted features generic , the models were trained using a natural image set with millions of labeled examples . These models were transferred to the ISH image domain and used directly as feature extractors to compute image representations . Furthermore , we employed multi task learning method to fine tune the pre trained models with labeled ISH images , and also extracted features from the fine tuned models . Experimental results showed that feature representations computed by deep models based on transfer and multi task learning significantly outperformed other methods for annotating gene expression patterns at different stage ranges . We also demonstrated that the intermediate layers of deep models produced the best gene expression pattern representations .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications Data Mining General Terms Algorithms Keywords Deep learning ; transfer learning ; multi task learning ; image analysis ; bioinformatics
1 .
INTRODUCTION
A general consensus in image related research is that different recognition and learning tasks may require different image representations . Thus , a central challenge in learning from image data is to develop appropriate representations for the specific task at hand . Traditionally , a common practice is to hand tune features for specific tasks , which is time consuming and requires substantial domain knowledge . For example , in the study of gene expression patterns in Drosophila melanogaster , texture features based on wavelets , such as Gabor filters , were particularly effective for determining the developmental stages from in situ hybridization ( ISH ) images [ 24 ] . Such image representation , often referred to as “ global visual features ” , is not suitable for controlled vocabulary ( CV ) term annotation because each CV term is often associated with only a part of an image , thereby requiring an image representation of local visual features [ 8 , 27 ] . Current state of the art systems for CV term annotation first extracted local patches of an image and computed local features which are invariant to certain geometric transformations ( eg , scaling and translation ) . Each image was then represented as a bag of “ visual words ” , known as the “ bag of words ” representation [ 7 ] , or a set of “ sparse codes ” , known as the “ sparse coding ” representation [ 9 , 19 , 25 ] .
In addition to being problem dependent , a common property of traditional feature extraction methods is that they are “ shallow ” , because only one or two levels of feature extraction was applied , and the parameters for computing features are usually not trained using supervised algorithms .
1475 Given the complexity of patterns captured by biological images , these shallow models of feature extraction may not be sufficient . Therefore , it is desirable to develop a multi layer feature extractor , alleviating the tedious process of manual feature engineering and enhancing the representation power . In this work , we proposed to employ the deep learning methods to generate representations of ISH images . Deep learning models are a class of multi level systems that can act on the raw input images directly to compute increasingly high level representations . One particular type of deep learning models that have achieved practical success is the deep convolutional neural networks ( CNNs ) [ 13 ] . These models stack many layers of trainable convolutional filters and pooling operations on top of each other , thereby computing increasingly abstract representations of the inputs . Deep CNNs trained with millions of labeled natural images using supervised learning algorithms have led to dramatic performance improvement in natural image recognition and detection tasks [ 6 , 10 , 18 ] .
However , learning a deep CNN is usually associated with the estimation of millions of parameters , and this requires a large number of labeled image samples . This bottleneck currently prevents the application of CNNs to many biological problems due to the limited amount of labeled training data . To overcome this difficulty , we proposed to develop generic and problem independent feature extraction methods , which involves applying previously obtained knowledge to solve different but related problems . This is made possible by the initial success of transferring features among different natural image data sets [ 4 , 17 , 26 ] . These studies trained the models on the ImageNet data set that contains millions of labeled natural images with thousands of categories . The learned models were then applied to other image data sets for feature extraction , since layers of the deep models are expected to capture the intrinsic characteristics of visual objects .
In this article , we explored whether the transfer learning property of CNNs can be generalized to compute features for biological images . We proposed to transfer knowledge from natural images by training CNNs on the ImageNet data set . To take this idea one step further , we proposed to fine tune the trained model with labeled ISH images , and resumed training from already learned weights using multi task learning schemes . The two models were then both used as a feature extractors to compute image features from Drosophila gene expression pattern images . The resulting features were subsequently used to train and validate our machine learning method for annotating gene expression patterns . The overall pipeline of this work is given in Figure 1 .
Experimental results show that our approach of using CNNs outperformed the sparse coding methods [ 19 ] for annotating gene expression patterns at different stage ranges . In addition , our results indicated that the transfer and finetuning of knowledge by CNNs from natural images is very beneficial for producing high level representations of biological images . Furthermore , we showed that the intermediate layers of CNNs produced the best gene expression pattern representations . This is because the early layers encode very primitive image features that are not enough to capture gene expression patterns . Meanwhile , the later layers captured features that are specific to the training natural image set , and these features may not be relevant to gene expression pattern images .
2 . DEEP MODELS FOR TRANSFER LEARN
ING AND FEATURE EXTRACTION
Deep learning models are a class of methods that are capable of learning hierarchy of features from raw input images . Convolutional neural networks ( CNNs ) are a class of deep learning models that were designed to simulate the visual signal processing in central nervous systems [ 1 , 10 , 13 ] . These models usually consist of alternating combination of convolutional layers with trainable filters and local neighborhood pooling layers , resulting in a complex hierarchical representations of the inputs . CNNs are intrinsically capable of capturing highly nonlinear mappings between inputs and outputs . When trained with millions of labeled images , they have achieved superior performance on many image related tasks [ 10 , 13 , 18 ] .
A key challenge in applying CNNs to biological problems is that the available labeled training samples are very limited . To overcome this difficulty and develop a universal representation for biological image informatics , we proposed to employ transfer learning to transfer knowledge from labeled image data that are problem independent . The idea of transfer learning is to improve the performance of a task by applying knowledge acquired from different but related task with a lot of training samples . This approach of transfer learning has already yielded superior performance on natural image recognition tasks [ 4 , 14 , 17 , 23 , 26 ] .
In this work , we explored whether this transfer learning property of CNNs can be generalized to biological images . Specifically , the CNN model was trained on the ImageNet data containing millions of labeled natural images with thousands of categories and used directly as feature extractors to compute representations for ISH images . In this work , we applied the pre trained VGG model [ 18 ] that was trained on the ImageNet data to perform several computer vision tasks , such as localization , detection and classification . There are two pre trained models in [ 18 ] , which are “ 16 ” and “ 19 ” weight layers models . Since these two models generated similar performance on our ISH images , we used the “ 16 ” weight layers model in our experiment . The VGG architecture contains 36 layers . This network includes convolutional layers with fixed filter sizes and different numbers of feature maps . It also applied rectified non linearity , max pooling to different layers .
More details on various layers in the VGG weight layer model are given in Figure 2 . Since the output feature representations of layers before the third max pooling layer involve larger feature vectors , we used each Drosophila ISH image as input to the VGG model and extracted features from layers 17 , 21 , 24 , and 30 to reduce the computational cost . We then flattened all the feature maps and concatenated them into a single feature vector . For example , the number of feature maps in layer 21 is 512 , and the corresponding size of feature maps is 28 × 28 . Thus , the corresponding size of feature vector for this layer is 401,408 .
3 . DEEP MODELS FOR MULTI TASK
LEARNING
In addition to the transfer learning scheme described above , we also proposed a multi task learning strategy in which a CNN is first trained in the supervised mode using the ImageNet data and then fine tuned on the labeled ISH Drosophila images . This strategy is different from the pre trained model
1476 Natural Image
CNN
Biological
Image
Cross
Entropy
Loss
1000
ImageNet
Classes
Transfer
Predicted
Ground Truth parameters
Loss
Function
Loss
Function
Task 1
Embryonic foregut Embryonic foregut
Task 2
Embryonic brain Embryonic brain
Loss
Function
Task K
Ventral nerve cord Ventral nerve cord
Multi task Learning
CV terms
Figure 1 : Pipeline of deep models for transfer learning and multi task learning . The network was trained on the ImageNet data containing millions of labeled natural images with thousands of categories ( top row ) . The pre trained parameters are then transferred to the target domain of biological images . We first directly used the pre trained model to extract features from Drosophila gene expression pattern images . We then fine tuned the trained model with labeled ISH images . We then employed the fine tuned model to extract features to capture CV term specific discriminative information ( bottom row ) . we used above . To be specific , the pre trained model is designed to recognize objects in natural images while we studied the CV term annotation of Drosophila images instead . Although the leveraged knowledge from the source task could reflect some common characteristics shared in these two types of images such as corners or edges , extra efforts are also needed to capture the specific properties of ISH images . The Drosophila gene expression pattern images are organized into groups , and multiple CV term annotations are assigned to multiple images in the same group . This multi image multi label nature poses significant challenges to traditional image annotation methodologies . This is partially due to the fact that there are ambiguous multipleto multiple relationships between images and CV term annotations , since each group of images are associated with multiple CV term annotations .
In this paper , we proposed to use multi task learning strategy to overcome the above difficulty . To be specific , we first employed a CNN model that is pre trained on natural images to initialize the parameters of a deep network . Then , we fine tuned this network using multiple annotation term prediction tasks to obtain CV term specific discriminative representation . The pipeline of our method is illustrated in Figure 1 . We have a single pre trained network with the same inputs but with multiple outputs , each of which corresponds to a term annotation task . These outputs are fully connected to a hidden layer that they share . Because all outputs share a common layer , the internal representations learned by one task could be used by other tasks . Note that the back propagation is done in parallel on these outputs in the network . For each task , we used its individual loss function to measure the difference between outputs and the ground truth . In particular , we are given a training set of k tasks {Xi , yj i=1 , j = 1 , 2 , . . . , k , where Xi ∈ Rn denotes the i th training sample , m denotes the total number of training samples . Note that we used the same groups of samples for different tasks , which is a simplified version of traditional multi task learning . The output label yj i denotes the CV term annotation status of training sample , which is binary with the form i }m yj i = fl 1
0 if Xi is annotated with the j th CV term , otherwise .
To quantitatively measure the difference between the predicted annotation results and ground truth from human experts , we used a loss function in the following form : loss(y , ˆy ) = − m
Xi=1 where k
Xj=1yj i logf ( ˆyj i ) + ( 1 − yj i )log(1 − f ( ˆyj i ) ) ,
1 if
1+e−q 1 − 1
1+e−q q ≥ 0 if q < 0 , f ( q ) = fl i }m,k and y = {yj over different tasks , and ˆy = {yj i,j=1 denotes the ground truth label matrix i,j=1 is the output matrix i }m,k
1477 3@224×224
Convolution
Size 3 × 3
Stride 1 × 1
Convolution
Size 3 × 3
Stride 1 × 1
ReLU
ReLU
Convolution
Size 3 × 3
Stride 1 × 1
Convolution
Size 3 × 3
Stride 1 × 1
ReLU
ReLU
Max pooling
Size 2 × 2 Stride 2 × 2
Max pooling
Size 2 × 2 Stride 2 × 2
Convolution
Size 3 × 3
Stride 1 × 1
ReLU
Convolution
Size 3 × 3
Stride 1 × 1
ReLU
Convolution
Size 3 × 3
Stride 1 × 1
ReLU
Max pooling
Size 2 × 2
Stride 2 × 2
Convolution
Size 3 × 3
Stride 1 × 1
Convolution
Size 3 × 3
Stride 1 × 1
ReLU
ReLU
Convolution
Size 3 × 3
Stride 1 × 1
ReLU
Max pooling
Size 2 × 2
Stride 2 × 2
Convolution
Size 3 × 3
Stride 1 × 1
ReLU
Convolution
Size 3 × 3
Stride 1 × 1
ReLU
Convolution
Size 3 × 3
Stride 1 × 1
Max pooling
Size 2 × 2
Stride 2 × 2
Full
4096 × 1 × 1
ReLU
Full
4096 × 1 × 1
ReLU
Full
ReLU
4096 × 1 × 1
L5
L10
L17
L21
L24
L30
256@28×28
512@28×28
512@14×14
512@14×14
Input size
L17
L21
L24
L30
224×224
200704
401408
100352
100352
Figure 2 : Detailed architecture of the VGG model . “ Convolution ” , “ Max pooling ” and “ ReLU ” denote convolutional layer , max pooling layer and rectified linear unit function layer , respectively . This model consists of 36 layers . We extracted features from layers 17 , 21 , 24 , and 30 . of our network through feedforward propagation . Note that ˆyj i denotes the network output before the softmax activation function . This loss function is a special case of the cross entropy loss function by using sigmoid function to induce probability representation [ 2 , 3 ] . Note that our multi task loss function is the summation of multiple loss functions , and all of them are optimized simultaneously during training .
4 . BIOLOGICAL IMAGE ANALYSIS
The Drosophila melanogaster has been widely used as a model organism for the study of genetics and developmental biology . To determine the gene expression patterns during Drosophila embryogenesis , the Berkeley Drosophila Genome Project ( BDGP ) used high throughput RNA in situ hybridization ( ISH ) to generate a systematic gene expression image database [ 20 , 21 ] . In BDGP , each image captures the gene expression patterns of a single gene in an embryo . Each gene expression image is annotated with a collection of anatomical and developmental ontology terms using a CV term annotation to identify the characteristic structures in embryogenesis . This annotation work is now mainly carried out manually by human experts , which makes the whole process time consuming and costly . In addition , the number of available images is now increasing rapidly . Therefore , it is desirable to design an automatic and systematic annotation approach to increase the efficiency and accelerate biological discovery [ 5 , 8 , 11 , 12 , 15 , 16 ] .
Prior studies have employed machine learning and computer vision techniques to automate this task . Due to the effects of stochastic process in development , every embryo develops differently . In addition , the shape and position of the same embryonic part may vary from image to image .
Thus , how to handle local distortions on the images is crucial for building robust annotation methods . The seminal work in [ 28 ] employed the wavelet embryo features by using the wavelet transformation to project the original pixel based embryonic images onto a new feature domain . In subsequent work , local patches were first extracted from an image and local features which are invariant to certain geometric transformations ( eg , scaling and translation ) were then computed from each patch . Each image was then represented as a bag of “ visual words ” , known as the “ bag of words ” representation [ 7 ] , or a set of “ sparse codes ” , known as the “ sparse coding ” representation [ 19 , 25 ] . All prior methods used handcrafted local features combined with high level methods , such as the bag of words or sparse coding schemes , to obtain image representations . These methods can be viewed as two layer feature extractors . In this work , we proposed to employ the deep CNNs as a multi layer feature extractor to generate image representations for CV term annotation . We showed here that a universal feature extractor trained on problem independent data set can be used to compute feature representations for CV term annotation . Furthermore , the model trained on problem independent data set , such as the ImageNet data , can be fine tuned on labeled data from specific domains using the error back propagation algorithm . This will ensure that the knowledge transferred from problem independent images is adapted and tuned to capture domain specific features in biological images . Since generating manually annotated biological images is both timeconsuming and costly , the transfer of knowledge from other domains , such as the natural image world , is essential in achieving competitive performance .
1478 Table 1 : Statistics of the data set used in this work . The table shows the total number of images for each stage range and the numbers of positive samples for each term .
Stages
4 6 7 8 9 10 11 12 13 17
Number of images No . 1 No . 2 No . 3 No . 4 No . 5 No . 6 No . 7 No . 8 No . 9 No . 10
# of positive samples for each term
4173 1953 2153 7441 7564
953 782 899 2945 2572
438 741 787 2721 2169
1631 748 778 2056 2062
1270 723 744 1932 1753
1383 753 694 1847 1840
1351 668 496 1741 1699
351 510 559 1400 1273
568 340 452 1129 1261
582 165 350 767 891
500 209 264 1152 1061
5 . EXPERIMENTS
5.1 Experimental setup
In this study , we used the Drosophila ISH gene expression pattern images provided by the FlyExpress database [ 12 , 22 ] , which contains genome wide , standardized images from multiple sources , including the Berkeley Drosophila Genome Project ( BDGP ) . For each Drosophila embryo , a set of highresolution , two dimensional image series were taken from different views ( lateral , dorsal , and lateral dorsal and other intermediate views ) . These images were then subsequently standardized semi manually . In this study , we focused on the lateral view images only , since most of images in FlyExpress are in lateral view .
In the FlyExpress database , the embryogenesis of Drosophila has been divided into six discrete stage ranges ( stages 1 3 , 46 , 7 8 , 9 10 , 11 12 , and 13 17 ) . We used those images in the later 5 stage ranges in the CV term annotation , since only a very small number of keywords were used in the first stage range . One characteristic of these images is that a group of images from the same stage and same gene are assigned with the same set of keywords . Prior work in [ 19 ] has shown that image level annotation outperformed group level annotation using the BDGP images . In this work , we focused on the image level annotation only and used the same top 10 keywords that are most frequently annotated for each stage range as in [ 19 ] . The statistics of the numbers of images and most frequent 10 annotation terms for each stage range are given in Table 1 .
For CV term annotation , our image data set is highly imbalanced with much more negative samples than positive ones . For example , there are 7564 images in stages 13 17 , but only 891 of them are annotated the term “ dorsal prothoracic pharyngeal muscle ” . The commonly used classification algorithms might not work well for our specific problem , because they usually aimed to minimizing the overall error rate without paying special attention to the positive class . Prior work in [ 19 ] has shown that using under sampling with ensemble learning could produce better prediction performance . In particular , we selectively under sampled the majority class to obtain the same number of samples as the minority class and built a model for each sampling . This process was performed many times for each keyword to obtain a robust prediction . Following [ 19 ] , we employed classifier ensembles built on biased samples to train robust models for annotation . In order to further improve the performance , we produced the final prediction by using majority voting , since this sample scheme is one of the widely used methods for fusion of multiple classifiers . For comparison purpose , we also implemented the existing sparse coding image rep resentation method studied in [ 19 ] . The annotation performance was measured using accuracy , specificity , sensitivity and area under the ROC curve ( AUC ) for CV term annotation . For all of these measures , a higher value indicates better annotation performance . All classifiers used in this work are the ℓ2 norm regularized logistic regression .
5.2 Comparison of features extracted from dif ferent layers
The deep learning model consists of multiple layer of feature maps for representing the input images . With this hierarchical representation , a natural question is which layer has the most discriminative power to capture the characteristics of input images . When such networks were trained on natural image data set such as the ImageNet data , the features computed in lower layers usually correspond to local features of objects such as edges , corners or edge/color conjunctions . In contrast , the features encoded at higher layers mainly represent class specific information of the training data . Therefore , for the task of natural object recognition , the features extracted from higher layers usually yielded better discriminative power [ 26 ] .
In order to identify the most discriminative features for the gene expression pattern annotation tasks , we compared the features extracted from various layers of the VGG network . Specifically , we used the ISH images as inputs to the pre trained VGG network and extracted features from layers 17 , 21 , 24 , and 30 for each ISH image . These features were used for the annotation tasks , and the results are given in Figure 3 . We can observe that for all stage ranges , layer 21 features outperformed other features in terms of overall performance . Specifically , the discriminative power increases from layer 17 to layer 21 , and then drops afterwards as the depth of network increases . This indicates that gene expression features are best represented in the intermediate layers of CNN that was trained on natural image data set . One reasonable explanation about this observation is the lower layers compute very primitive image features that are not enough to capture gene expression patterns . Meanwhile , the higher layers captured features that are specific to the training natural image set , and these features may not be relevant for gene expression pattern images .
Then we proposed to use multi task learning strategy to fine tune the pre trained network with labeled ISH images . In order to show the gains through fine tuning on pre trained model , we extracted features from the same hidden layers that are used for the pre trained model . We reported the predictive performance achieved by features of different layers in the proposed fine tuned model in Figure 4 . It can be observed from the results that the predictive performance
1479 L17 L21 L24 L30
L17 L21 L24 L30 y c a r u c c A y t i c i f i c e p S
0.82
0.8
0.78
0.76
0.74
0.72
0.86
0.84
0.82
0.8
0.78
0.76
0.74
0.72
4−6
7−8
9−10 Stage
11−12
13−17
4−6
7−8
9−10 Stage
11−12
13−17
L17 L21 L24 L30
L17 L21 L24 L30 y t i v i t i s n e S
0.8
0.79
0.78
0.77
0.76
0.75
0.74
0.73
0.72
0.71
0.88
0.86
C U A
0.84
0.82
0.8
0.78
4−6
7−8
9−10 Stage
11−12
13−17
4−6
7−8
9−10 Stage
11−12
13−17
Figure 3 : Comparison of annotation performance achieved by features extracted from different layers of deep models for transfer learning over five stage ranges . “ Lx ” denotes the hidden layer from which the features were extracted .
L17 L21 L24 L30
L17 L21 L24 L30
0.84
0.82
0.8
0.78
0.76
0.74
0.72
0.9
0.88
0.86
0.84
0.82
0.8
0.78
0.76
0.74 y c a r u c c A y t i c i f i c e p S
4−6
7−8
9−10 Stage
11−12
13−17
4−6
7−8
9−10 Stage
11−12
13−17
L17 L21 L24 L30
L17 L21 L24 L30 y t i v i t i s n e S
0.82
0.8
0.78
0.76
0.74
0.72
0.88
0.86
C U A
0.84
0.82
0.8
4−6
7−8
9−10 Stage
11−12
13−17
4−6
7−8
9−10 Stage
11−12
13−17
Figure 4 : Comparison of annotation performance achieved by features extracted from different layers of the deep models for multi task learning over five stage ranges . “ Lx ” denotes the hidden layer from which the features were extracted .
1480 y c a r u c c A y t i c i f i c e p S
0.84
0.82
0.8
0.78
0.76
0.74
0.72
0.86 0.84 0.82 0.8 0.78 0.76 0.74 0.72 0.7 0.68
SC TL TL+MTL
4−6
7−8
9−10 Stage
11−12
13−17
SC TL TL+MTL
4−6
7−8
9−10 Stage
11−12
13−17 y t i v i t i s n e S
0.82
0.8
0.78
0.76
0.74
0.72
0.88
0.86
0.84
C U A
0.82
0.8
0.78
0.76
SC TL TL+MTL
4−6
7−8
9−10 Stage
11−12
13−17
SC TL TL+MTL
4−6
7−8
9−10 Stage
11−12
13−17
Figure 5 : Performance comparison of different methods . “ SC ” denotes sparse coding . “ TL ” and “ TL + MTL ” denote the performance achieved by transfer learning and multi task learning models , respectively . We only consider the features extracted from layer 21 of these two deep models . was generally higher on middle layers in the deep architecture . In particular , layer 21 outperformed other layers significantly . This result is consistent with the observation found on the pre trained model .
5.3 Comparison with prior methods
We also compared the performance achieved by different methods including sparse coding , transfer learning model and multi task learning . These results demonstrated that our deep model with multi task learning were able to accurately annotate gene expression images over all embryogenesis stage ranges . To compare our generic features with the domain specific features used in [ 19 ] , we compared the annotation performance of our deep learning features with that achieved by the domain specific sparse coding features . Deep learning models include transfer learning and multitask learning . In this experiment , we only considered the features extracted from layer 21 since they yielded the best performance among different layers . The performance of these three types of features averaged over all terms is given in Figure 5 and Table 2 . We can observe that the deep model for multi task learning features outperformed the sparse coding features and transfer learning features consistently and significantly in all cases . To examine the performance differences on individual anatomical terms , we showed the AUC values on each term in Figure 6 for different stage ranges . We can observe that our features extracted from layer 21 of the VGG networks for transfer learning and multi task learning outperformed the sparse coding features over all stage ranges for all terms consistently . These results demonstrated that our generic features of deep models were better at represent ing gene expression pattern images than the problem specific features based on sparse coding .
In Figure 7 , we provided a term by term and image byimage comparison between the results of the deep model for multi task learning and the sparse coding features for the 10 terms in stages 13 17 . The x axis corresponds to the 10 terms . The y axis corresponds to a subset of 50 images in stages 13 17 with the largest numbers of annotated terms . Overall , it is clear that the total number of green and blue entries is much more than the number of red and pink entries , indicating that , among all predictions disagreed by these two methods , the predictions by the multi task learning features were correct most of the time .
6 . CONCLUSIONS AND FUTURE WORK In this work , we proposed to employ the deep convolutional neural networks as a multi layer feature extractor to generate generic representations for ISH images . We used the deep convolutional neural network trained on large natural image set as feature extractors for ISH images . We first directly used the model trained on natural images as feature extractors . We then employed multi task classification methods to fine tune the pre trained model with labeled ISH images . Although the number of annotated ISH images is small , it nevertheless improved the pre trained model . We compared the performance of our generic approach with the problem specific methods . Results showed that our proposed approach significantly outperformed prior methods on ISH image annotation . We also showed that the intermediate layers of deep models produced the best gene expression pattern representations .
1481 Table 2 : Performance comparison in terms of accuracy , sensitivity , specificity , and AUC achieved by CNN models and Sparse Coding features for all stage ranges . “ TL+MTL ” and “ TL ” denote the features extracted from layer 21 of the deep model for multi task learning and transfer learning . “ SC ” denotes the performance of the sparse coding features .
Stage 4 6
Stage 7 8
Stage 9 10
Stage 11 12
Stage 13 17
Measures
Accuracy
Sensitivity
Specificity
AUC
Methods TL+MTL
TL SC
TL SC
TL+MTL
07938±00381 07521±00326 07217±00352 07825±00372 07405±00293 07321±00408 TL + MTL 08436±00376 07915±00247 07140±00389 TL + MTL 08493±00427 08344±00439 07687±00432
TL SC
TL SC
08216±00231 07837±00269 07401±00351 07829±00368 07515±00342 07190±00331 08581±00380 08160±00316 07605±00392 08565±00279 08401±00346 07834±00358
08318±00216 07929±00231 07549±00303 07721±00412 07876±00401 07468±00298 08422±00284 07983±00315 07629±00298 08695±00276 08508±00257 07921±00294
08128±00325 08094±00331 07659±00326 08026±00401 07905±00389 07576±00329 08527±00252 08342±00237 07749±00329 08776±00291 08702±00271 08061±00342
08327±00256 08205±00304 07681±00231 08185±00259 07964±00317 07328±00235 08716±00256 08517±00306 08005±00298 08824±00197 08746±00299 08105±00280
SC TL TL+MTL
Stage 4−6 c ellular bla m atern d ors al e cto sto d er m al d er m a nla g s u b s pro c e h p et e in statu n a s c e n di v e ntral e p ole c alic e cto d er cto d er m a nla g p o sterior e ell e in statu n m a nla g e in statu n a s c e n di a s c e n di y olk n u a nla g e in statu n n d o d er m a nla g a s c e n clei di e in statu n a s c e n di
SC TL TL+MTL
Stage 9−10
C U A
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
1
0.95
0.9
0.85
C U A
0.8
0.75
0.7
0.65
0.6 ordiu m
1
0.95
0.9
0.85
C U A
0.8
0.75
0.7
0.65
0.6
SC TL TL+MTL e m
SC TL TL+MTL
Stage 7−8 tru n k m h e a d m e s o d er m prim e s o d er m prim ordiu ordiu p o sterior e a nterior e n d o d er m prim n d o d er m a v e ntral e pro c e p h cto d er m prim alic e cto d er d ors al e cto hin d g ut a nla g p ole c d er m prim ell e v e ntral n erv e c nla g e ordiu m
P
2 m a nla g e ordiu m ord a nla g e
Stage 11−12 m
P
2
SC TL TL+MTL
1
0.95
0.9
0.85
C U A
0.8
0.75
0.7
0.65
0.6
1
0.95
0.9
0.85
C U A
0.8
0.75
0.7
0.65
0.6 m
P 4 brain prim ordiu m
P 2 tru n k m ordiu m ordiu m e m tru n k m e s o d er m prim p o sterior e h e a d m a nterior e n d o d er m prim e s o d er m prim n d o d er m prim pro c e p h alic e ordiu m ordiu m ordiu m
P 2 v e ntral n v e ntral e in clu siv erv e c ord prim cto d er m prim d ors al e cto fore g ut prim d er m prim ordiu e hin d g ut prim ordiu m ordiu m
P 3 ordiu m ordiu m ordiu m cto d er m prim p o sterior a nterior mid g ut prim mid g ut prim ordiu m ordiu m v e ntral n e s o d er m prim erv e c ord prim hin d g ut pro p d ors al e pid er er prim ordiu m ordiu m fore g ut prim mis prim ordiu m ordiu m h e a d m v e ntral e e s o d er m prim pid er mis prim ordiu m ordiu m m
Stage 13−17 e m e m e m e m e m e m v e ntral n bry o nic mid g ut erv e c bry o bry o nic brain ord nic hin d g ut bry o nic d bry o nic v e ors al e pid er ntral e pid er bry o nic/larv al m bry o nic h e a d e pid er ora cic p d ors al proth mis bry o nic fore g h ary n g e al m ut u s cle u s cle s y ste m mis mis
Figure 6 : Performance comparison of different methods for all stage ranges . “ SC ” , “ TL ” and “ TL + MTL ” denote sparse coding , transfer learning and multi task learning models , respectively .
1482 l(1)G0156 ( insitu35149 )
IP3K2 ( insitu34210 ) shg ( insitu18012 )
IP3K2 ( insitu34206 ) trn ( insitu56436 ) msk ( insitu4843 )
CG17032 ( insitu35193 ) mthl5 ( insitu31931 )
CG31619 ( insitu71241 )
Klc ( insitu29549 ) fra ( insitu38490 )
CG31619 ( insitu17598 ) l(2)05510 ( insitu56184 ) CG11444 ( insitu8346 ) CG31619 ( insitu17603 ) CG11444 ( insitu53408 )
Klc ( insitu29547 ) Mcm7 ( insitu60024 ) Mcm7 ( insitu60025 )
CG32626 ( insitu56335 ) CG1273 ( insitu25161 ) CG2837 ( insitu50086 ) pyd ( insitu23557 ) pyd ( insitu23556 )
CG11353 ( insitu15916 )
Hmgs ( insitu58483 ) Nhe2 ( insitu30903 )
CG3624 ( insitu17292 ) fng ( insitu24983 )
CG10737 ( insitu12281 ) CG12701 ( insitu22675 ) unc−115 ( insitu66432 ) knrl ( insitu14696 )
CG32556 ( insitu24929 ) CG6178 ( insitu26292 )
Spn4 ( insitu8834 )
HLHmbeta ( insitu16369 ) fng ( insitu24987 ) eEF1delta ( insitu49553 ) eIF−4E ( insitu77782 ) Wnt4 ( insitu31669 )
CG5428 ( insitu53262 ) CG3625 ( insitu47629 ) spi ( insitu43996 ) La ( insitu19864 )
Nhe2 ( insitu30915 )
CG32046 ( insitu34476 ) CG1749 ( insitu35346 ) CG2016 ( insitu32943 )
Hmgs ( insitu58475 ) e v e m m m m m m m e e e e e e d e b r y o n i c n t r a l n b r y b r y b r y b r y b r y b r y o o o o o o m i d g u t e r v e c o r d n i c b r a i n n i c h i n d g u t n i c d o r s a l e n i c v e n t r a l e n i c
/l a r v a l m n i c h e a d e p i d e r p i d e r m i s m i s u s c l e p i d e r s y m i s s t e m o r s a l m b r y o p r o t h o r a n i c f o r e g u t c i c p h a r y n g e a l m u s c l e
Figure 7 : Comparison of prediction results between the deep models for multi task learning and the sparse coding features for the 10 terms in stages 13 17 . The x axis shows the 10 terms . The y axis corresponds to a subset of 50 images in stages 13 17 with the largest numbers of annotated terms . The gene names and the FlyExpress image IDs in parentheses are displayed . The prediction results of different methods compared with the ground truth are distinguished by different colors . The white entries correspond to predictions agreed upon by these two methods , while non white entries were used to denote different types of disagreements . Specifically , the green and blue entries correspond to correct predictions by the multi task learning features but incorrect predictions by the sparse coding features . Green and blue indicate positive and negative samples , respectively , in the ground truth . Similarly , the red and pink entries correspond to incorrect predictions by the multi task learning features but correct predictions by the sparse coding features . Red and pink indicate positive and negative samples , respectively , in the ground truth .
In the current study , we focus on using deep models for CV annotation . There are many other biological image analysis tasks that require appropriate image representations such as developmental stage prediction . We will consider broader applications in the future . In this work , we considered a simplified version of the problem in which each term is associated with all images in the same group . We will extend our model to incorporate the image group information in the future .
7 . ACKNOWLEDGMENTS
( R01 LM010730 , HG002516 09 ) , and the NVIDIA Corporation with the donation of the Tesla K40 GPU .
8 . REFERENCES [ 1 ] Y . Bengio , A . Courville , and P . Vincent .
Representation learning : A review and new perspectives . IEEE Transactions on Pattern Analysis and Machine Intelligence , 35(8):1798–1828 , 2013 .
[ 2 ] C . Bishop . Neural Networks for Pattern Recognition .
Oxford University Press , 1995 .
[ 3 ] R . Collobert and J . Weston . A unified architecture for
This work was supported in part by research grants from NSF ( IIS 0953662 , DBI 1147134 and DBI 1350258 ) , NIH natural language processing : Deep neural networks with multitask learning . In Proceedings of the 25th
1483 international conference on Machine learning , pages 160–167 . ACM , 2008 . via sparse Bayesian factor models . PLoS Comput Biol , 7(7):e1002098 , 07 2011 .
[ 4 ] J . Donahue , Y . Jia , O . Vinyals , J . Hoffman , N . Zhang ,
[ 16 ] K . Puniyani , C . Faloutsos , and E . P . Xing . SPEX2 :
E . Tzeng , and T . Darrell . DeCAF : A deep convolutional activation feature for generic visual recognition . In Proceedings of the 31st International Conference on Machine Learning , pages 647–655 , 2014 .
[ 5 ] E . Frise , A . S . Hammonds , and S . E . Celniker . Systematic image driven analysis of the spatial Drosophila embryonic expression landscape . Molecular Systems Biology , 6:345 , 2010 .
[ 6 ] R . Girshick , J . Donahue , T . Darrell , and J . Malik .
Rich feature hierarchies for accurate object detection and semantic segmentation . In Proceedings of the 27th IEEE Conference on Computer Vision and Pattern Recognition , 2014 .
[ 7 ] S . Ji , Y X Li , Z H Zhou , S . Kumar , and J . Ye . A bag of words approach for Drosophila gene expression pattern annotation . BMC Bioinformatics , 10(1):119 , 2009 .
[ 8 ] S . Ji , L . Sun , R . Jin , S . Kumar , and J . Ye . Automated annotation of Drosophila gene expression patterns using a controlled vocabulary . Bioinformatics , 24(17):1881–1888 , 2008 .
[ 9 ] S . Ji , L . Yuan , Y X Li , Z H Zhou , S . Kumar , and J . Ye . Drosophila gene expression pattern annotation using sparse features and term term interactions . In Proceedings of the Fifteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 407–416 , 2009 .
[ 10 ] A . Krizhevsky , I . Sutskever , and G . Hinton . Imagenet classification with deep convolutional neural networks . In P . Bartlett , F . Pereira , C . Burges , L . Bottou , and K . Weinberger , editors , Advances in Neural Information Processing Systems 25 , pages 1106–1114 . 2012 .
[ 11 ] S . Kumar , K . Jayaraman , S . Panchanathan ,
R . Gurunathan , A . Marti Subirana , and S . J . Newfeld . BEST : a novel computational approach for comparing gene expression patterns from early stages of Drosophila melanogaster develeopment . Genetics , 169:2037–2047 , 2002 .
[ 12 ] S . Kumar , C . Konikoff , B . Van Emden , C . Busick , K . T . Davis , S . Ji , L W Wu , H . Ramos , T . Brody , S . Panchanathan , J . Ye , T . L . Karr , K . Gerold , M . McCutchan , and S . J . Newfeld . FlyExpress : visual mining of spatiotemporal patterns for genes and publications in Drosophila embryogenesis . Bioinformatics , 27(23):3319–3320 , 2011 .
[ 13 ] Y . LeCun , L . Bottou , Y . Bengio , and P . Haffner .
Gradient based learning applied to document recognition . Proceedings of the IEEE , 86(11):2278–2324 , November 1998 .
[ 14 ] M . Oquab , I . Laptev , L . Bottou , and J . Sivic .
Learning and transferring mid level image representations using convolutional neural networks . In Proceedings of the 27th IEEE Conference on Computer Vision and Pattern Recognition , 2014 . [ 15 ] I . Pruteanu Malinici , D . L . Mace , and U . Ohler .
Automatic annotation of spatial expression patterns automated concise extraction of spatial gene expression patterns from fly embryo ISH images . Bioinformatics , 26(12):i47–56 , 2010 .
[ 17 ] A . S . Razavian , H . Azizpour , J . Sullivan , and
S . Carlsson . Cnn features off the shelf : an astounding baseline for recognition . In Proceedings of the 27th IEEE Conference on Computer Vision and Pattern Recognition Workshops , 2014 .
[ 18 ] K . Simonyan and A . Zisserman . Very deep convolutional networks for large scale image recognition . arXiv preprint arXiv:1409.1556 , 2014 .
[ 19 ] Q . Sun , S . Muckatira , L . Yuan , S . Ji , S . Newfeld , S . Kumar , and J . Ye . Image level and group level models for Drosophila gene expression pattern annotation . BMC Bioinformatics , 14:350 , 2013 .
[ 20 ] P . Tomancak , A . Beaton , R . Weiszmann , E . Kwan ,
S . Shu , S . E . Lewis , S . Richards , M . Ashburner , V . Hartenstein , S . E . Celniker , and G . M . Rubin . Systematic determination of patterns of gene expression during Drosophila embryogenesis . Genome Biology , 3(12):research00881–008814 , 2002 .
[ 21 ] P . Tomancak , B . Berman , A . Beaton , R . Weiszmann , E . Kwan , V . Hartenstein , S . Celniker , and G . Rubin . Global analysis of patterns of gene expression during Drosophila embryogenesis . Genome Biology , 8(7):R145 , 2007 .
[ 22 ] B . Van Emden , H . Ramos , S . Panchanathan ,
S . Newfeld , and S . Kumar . Flyexpress : an image matching web tool for finding genes with overlapping patterns of expression in drosophila embryos . Tempe , AZ , 85287530 , 2006 .
[ 23 ] J . Yosinski , J . Clune , Y . Bengio , and H . Lipson . How transferable are features in deep neural networks ? In Advances in Neural Information Processing Systems , pages 3320–3328 , 2014 .
[ 24 ] L . Yuan , C . Pan , S . Ji , M . McCutchan , Z H Zhou ,
S . Newfeld , S . Kumar , and J . Ye . Automated annotation of developmental stages of Drosophila embryos in images containing spatial patterns of expression . Bioinformatics , 30(2):266–273 , 2014 .
[ 25 ] L . Yuan , A . Woodard , S . Ji , Y . Jiang , Z H Zhou ,
S . Kumar , and J . Ye . Learning sparse representations for fruit fly gene expression pattern image annotation and retrieval . BMC Bioinformatics , 13:107 , 2012 .
[ 26 ] M . D . Zeiler and R . Fergus . Visualizing and understanding convolutional networks . In Proceedings of the European Conference on Computer Vision , pages 818–833 . Springer , 2014 .
[ 27 ] W . Zhang , D . Feng , R . Li , A . Chernikov ,
N . Chrisochoides , C . Osgood , C . Konikoff , S . Newfeld , S . Kumar , and S . Ji . A mesh generation and machine learning framework for Drosophila gene expression pattern image analysis . BMC Bioinformatics , 14:372 , 2013 .
[ 28 ] J . Zhou and H . Peng . Automatic recognition and annotation of gene expression patterns of fly embryos . Bioinformatics , 23(5):589–596 , 2007 .
1484
