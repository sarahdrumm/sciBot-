On the Discovery of Evolving Truth
Yaliang Li1 , Qi Li1 , Jing Gao1 , Lu Su1 , Bo Zhao2 , Wei Fan3 , and Jiawei Han4
1SUNY Buffalo , Buffalo , NY USA 2LinkedIn , Mountain View , CA USA
3Baidu Big Data Lab , Sunnyvale , CA USA
4University of Illinois , Urbana , IL USA
{yaliangl,qli22,jing,lusu}@buffalo.edu , bozhaouiuc@gmailcom , fanwei03@baidu.com , hanj@illinois.edu
ABSTRACT In the era of big data , information regarding the same objects can be collected from increasingly more sources . Unfortunately , there usually exist conflicts among the information coming from different sources . To tackle this challenge , truth discovery , ie , to integrate multi source noisy information by estimating the reliability of each source , has emerged as a hot topic . In many real world applications , however , the information may come sequentially , and as a consequence , the truth of objects as well as the reliability of sources may be dynamically evolving . Existing truth discovery methods , unfortunately , cannot handle such scenarios . To address this problem , we investigate the temporal relations among both object truths and source reliability , and propose an incremental truth discovery framework that can dynamically update object truths and source weights upon the arrival of new data . Theoretical analysis is provided to show that the proposed method is guaranteed to converge at a fast rate . The experiments on three real world applications and a set of synthetic data demonstrate the advantages of the proposed method over state of the art truth discovery methods .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications—Data mining
Keywords Truth Discovery ; Source Reliability ; Dynamic Data
1 .
INTRODUCTION
Nowadays , with information explosion , it becomes much more convenient to collect information from multiple places . For example , to know the weather condition of a specific location , we can get the information from multiple weather services ; to get the up todate information about some stocks , multiple websites are recording the real time information ; to query the flight status , multiple agencies may provide such information . However , these collected information could conflict with each other .
To better utilize the collected multi source information , an important task is to resolve the conflicts among them , and output the trustworthy information . In the light of this challenge , truth discovery [ 3,5,7–9,14,17,21,23,25 ] is emerging as a promising paradigm that can help people identify trustworthy information from multiple noisy information sources , and it has been applied in various application domains [ 4 , 11 , 18 , 19 ] . In contrast to voting or averaging approaches that treat all sources equally , truth discovery methods estimate the source reliability and infer the trustworthy information simultaneously . As the source reliability degrees are usually unknown a priori , in truth discovery , source reliability estimation and trustworthy information inference are tightly combined by the following principle : If a source provides trustworthy information more often , it will be assigned a high reliability ; Meanwhile , if one piece of information is claimed by high quality sources , it will be regarded as trustworthy information .
Most of the existing truth discovery algorithms are proposed to work on static data . They can not handle the scenarios where the collected information comes sequentially , which happen in many real world applications . Consider the aforementioned applications : The weather condition , the stock information , and the flight status are collected in real time . These applications reveal the necessity to develop truth discovery methods for such scenarios .
In the scenarios that information is collected continuously , new challenges are brought by the nature of such applications . First , as data comes sequentially from multiple sources in a dynamic environment , we cannot afford to re run the batch algorithm at each timestamp . Instead , we need approaches that scan data once and conduct real time truth discovery to facilitate processing and storage on large scale data . Unfortunately , existing truth discovery approaches are not developed to handle dynamic streaming data . They work in batch processing manner , and cannot incrementally update truth and source reliability .
Second , unique characteristics of dynamic data are observed across various real world applications : 1 ) The true information of objects evolves over time , and for a specific object , the temporal smoothness exists among its information at different timestamps . 2 ) The observed source reliability changes over time , which is not consistent with the assumption held by existing approaches as they assume the source has unchanged reliability . In Section 2 , we will illustrate more details about these observations and discuss the difficulties they bring to truth discovery .
To tackle the aforementioned two challenges , in this paper , a new truth discovery method is developed for dynamic scenarios . We first propose an effective solution that can update truth and source reliability in an incremental fashion . Thus the proposed method can work in real time , and the data only needs to be visited once . To capture the unique characteristics of dynamic data , two factors ,
675 namely , smoothing factor and decay factor , are incorporated into the proposed approach to model the evolution of both truth and source reliability . Further , we give theoretical analysis of the proposed method , and show its fast rate of convergence .
To demonstrate the effectiveness and efficiency of the proposed method , we conduct a series of experiments on three real world applications and a set of synthetic datasets . By comparing with state of the art truth discovery methods , the improvement brought by the proposed method is justified . We also analyze the effect of the smoothing factor and decay factor , explain how these factors can capture the characteristics of dynamic data , and test the sensitivity of the proposed method with respect to these factors .
In summary , our contributions in this paper are : • Motivated by many real world applications , we study the truth discovery task under dynamic scenarios , in which the information is collected continuously , and evolution exists in both object truths and source reliability .
• We develop an incremental truth discovery method that can be applied in real time scenarios . Two more factors are incorporated into the proposed method to capture the characteristics of dynamic data .
• Theoretical analysis is presented to prove the convergence of the proposed method . The rate of convergence is also given . • We test the proposed method on three real world applications and several synthetic datasets , and the improvements on both performance accuracy and efficiency are demonstrated .
In the following sections , we first present the observations and challenges under dynamic scenarios in Section 2 . In Section 3 , after formally defining the task , the proposed solution is derived , and further the smoothing factor and decay factor are incorporated . In Section 4 , we give theoretical analysis on the proposed method . Section 5 shows the experiments we conduct to validate the effectiveness and efficiency of the proposed method . We discuss related work in Section 6 , and conclude the paper in Section 7 .
2 . OBSERVATIONS
As mentioned before , in this section , we explore and summarize common evolutionary patterns observed across various applications . Later in this paper we will present our solutions that are motivated by these patterns . In the following , we use three truth discovery tasks , ie , weather forecast , stock records , and flight status integration , to illustrate the effect of dynamic changes on truth discovery . These three datasets have been used before in truth discovery literature [ 7 , 8 , 26 ] as static data , but they all involve dynamically changing data . Specifically , we are interested in getting true answers for weather forecast of cities , real time recording of stocks , and status of flights by merging data that are continuously collected from multiple websites . More details and experimental results on these datasets can be found in Section 5 .
There are two major observations regarding the impact of dynamic data on truth discovery : 1 ) Truth is evolving but temporal smoothness is observed , and 2 ) source reliability changes , which differs from the assumption held by truth discovery approaches applied to static settings . Truth Evolution . In Figures 1 , we demonstrate the evolution of truths ( x axis denotes time , and y axis denotes the true value that we are interested in : temperature , market capitalization , or flight arrival time ) . Figure 1a shows the highest temperature of New York City during a period of two months , Figure 1b illustrates stock information over one month ( weekdays only ) , and Figure 1c is about the arrival time of a particular flight on each day of one month
( arrival time is translated into minutes from 12am , for example , 07 : 30am is translated into 450 mins ) . From these figures , we can observe that the value is constantly changing but the change within a small time window is smooth . For example , in temperature data , if today ’s highest temperature is 42F , it is more likely that the highest temperature tomorrow will not deviate much from 42F . Similar patterns can be observed on stock data . For flight data , as the scheduled arrival time for a flight is almost the same , for most of the days , the actual arrival time is around the arrival time of the previous ones ( temporal smoothness ) . Only a few exceptions are observed in which arrival time is quite different from the scheduled arrival time ( the peaks in Figure 1c ) . These exceptions will be discussed and analyzed in experiments ( Section 5 ) . Source Reliability Evolution . Truth discovery approaches can infer trustworthy information from conflicting multi source data as it takes source reliability into consideration . To estimate such source reliability , existing approaches make the assumption that each source ’s reliability is consistent over all the claims it makes . This assumption is made in [ 22 ] and further adopted by [ 3 , 5 , 7 , 8 , 14 , 23 , 25 ] , and it works well in static settings where all the claims are processed simultaneously . However , this assumption does not hold any more in dynamic environment . Figure 2 demonstrates how sources’ reliability changes over time on the three datasets we adopt . The reliability of a source is quantified by comparing the sources’ claims with the true values and measuring the closeness between them . An interesting observation is that source reliability fluctuates around a certain value , which may correspond to the underlying true source reliability . However , at different timestamps , the observed source reliability reflects the effect of both the underlying source reliability and some other factors that are “ local ” to each timestamp . For example , a source that usually provides accurate flight information may fail at certain timestamps when some unusual events happen . We call such factors as environment factors , which are different across time .
The above two observations demonstrate the characteristics of dynamic data that need to be considered in modeling truth discovery on such data . Moreover , as the information comes continuously , it requires that the computation process should be in real time . In the following section , we first propose an effective approach that can update truth and source reliability in real time , and then address the effect of temporal smoothness and environment factors in dynamic data .
3 . METHODOLOGY
We start with introducing some concepts in truth discovery , and then formally define the task . In the remaining part of this section , we first build an efficient algorithm for truth discovery with dynamic data , which provides an incremental scheme to guarantee high efficiency . Based on it , smoothing factor and decay factor are incorporated into the proposed method to capture the observations discussed above . 3.1 Problem Formulation
To describe the notations clearly , we group them as follows :
Input . Consider a set of objects O that we are interested in , and for each of them o ∈ O , related information can be collected from S sources at each timestamp t ∈ {1 , 2 , 3 , . . } Let vs o,t represent the information from the s th source about the object o at the t th timestamp . For convenience , let ’s denote all the information from source s at time t as X s o,t}o∈O . Further , the size of this set is denoted as cs t , that is , X s t = {vs t | . t = |X s
676 ( a ) Weather
( b ) Stock
( c ) Flight
Figure 1 : Truth Evolves over Time .
( a ) Weather
( b ) Stock
( c ) Flight
Figure 2 : The Observed Source Reliability Changes over Time . o,t be the aggregated result for object o at time t , and X ∗
Output . After collecting information from different sources , our goal is to aggregate these information and output trustworthy ones . Let ˆv∗ t = {ˆv∗ o,t}o∈O be the whole set of aggregated results at time t . Besides the aggregated results , truth discovery methods can also estimate sources’ reliability degrees . Let ws denote the weight ( reliability degree ) of the s th source , and W represent the whole set of source weights . As source weights are estimated based on their information errors ( difference ) compared with the aggregated results , here we introduce some notations about source errors . Let o,t indicate the error of the s th source made on object o at time es t , and es t contain the errors on all the objects for source s at time t . Further , we have some notations for accumulated errors : es 1:t denotes all the errors of source s from time 1 to time t , and e1:t contains such information for all the sources .
Table 1 summarizes the notations used in this paper .
Notation
Table 1 : Notations Definition o,t vs X s o,t t cs ˆv∗ t X ∗ t ws W es o,t es t es 1:t e1:t information for object o from source s at time t set of all the information from source s at time t number of claims provided by source s at time t the aggregated result for object o at time t set of aggregated results at time t weight of source s set of all the source weights error on object o made by source s at time t set of errors made by source s at time t errors of source s from time 1 to time t errors of all the sources from time 1 to time t
Task Definition . The studied task is formally defined as follows . For a set of objects we are interested in , at timestamp T , related information is collected from S sources . Our goal is to find the most trustworthy information ˆv∗ o,T for each object o by resolving the conflicts among information from different sources {vs s=1 . Meanwhile , to guarantee the efficiency , the proposed method should not re visit the information at previous timestamps t ∈ {1 , 2 , 3 , . . . , T − 1} . Besides the efficiency requirement , compared with tradition truth discovery tasks , the main difference of o,T}S the proposed one is that the temporal evolution patterns within both objects and sources are investigated . 3.2 Proposed Method
When applying the existing truth discovery methods on dynamic data , the key limitation is their efficiency . Most of them iteratively update estimated source reliability and the identified trustworthy information . Thus multiple visits of the whole dataset are required . In dynamic scenario , it becomes inefficient or even infeasible as the data comes continuously . In the light of this challenge , we first develop an efficient truth discovery method for dynamic data by exploring the equivalence between optimization based solution and maximum a posteriori estimation . Optimization Based Solution . At time T , we have all the information from the timestamp 1 to T . Based on the principles of truth discovery , we can consider the following optimization problem to infer both source reliability and trustworthy information : minW,{X ∗ t }T t=1
( 1 ) lt ,
LT =
TX o,t)2 − SX t=1 tX cs
SX Pcs s=1 t where lt is the loss function at time t , which is defined as follows : lt = θ ws o,t − ˆv ∗ ( vs cs t log(ws ) .
( 2 ) o=1 s=1 termPS s=1 ws o=1(vs o,t− ˆv∗
The motivation behind this loss function is following : 1 ) The first o,t)2 measures the weighted distance o,t and the aggregated result between the provided information vs ˆv∗ o,t . By minimizing this part , the aggregated result ˆv∗ o,t will be closer to the information from the sources with high weight ws . Meanwhile , if a source provides information that is far from the aggregated results , in order to minimize the total loss , it will be ast log(ws ) works as a constraint to prevent ws approaching 0 , which leads to the trivial optimum for the first term . 3 ) Parameter θ adjusts the trade off between these two terms above . signed a low weight . 2 ) The second termPS s=1 cs
The benefits of adopting this optimization based formulation are : 1 ) It encodes the idea of truth discovery . 2 ) It allows us to incorporate constraints and prior knowledge about source weights . 3 ) In
020406020406080TimestampHigh Temperature051015202452525526265x 1010TimestampMarket Capitalization051015202530450500550600650TimestampArrival Time0102030405060000200400600801TimestampSource Reliability Degree0510152000050101502TimestampSource Reliability Degree05101520253000102030405TimestampSource Reliability Degree677 the following , we will show that this formulation can be linked with MAP estimation which gives an efficient incremental solution . In this optimization problem ( Eq ( 1) ) , two sets of variables are involved , source weights W and aggregated results {X ∗ t=1 . To solve this problem , we adopt coordinate descent [ 1 ] , in which one set of variables are fixed in order to solve for the other set of variables . When source weights W are fixed , to infer aggregated results ˆv∗ o,t , we take the derivative of Eq ( 2 ) with respect to ˆv∗ o,t , and get the following formula : t }T
PS PS s=1 ws · vs s=1 ws
∗ ˆv o,t = o,t
.
( 3 )
According to this weighted combination strategy to compute the aggregated results , the information provided by high quality sources will play more important roles , which keeps consistent with the basic principle of truth discovery . However , to estimate the source weights W at time T , according to Eq ( 1 ) , we need to re visit all the information from the first timestamp to the current one , which leads to additional cost and makes the algorithm inefficient . Maximum A Posteriori Estimation . In order to improve the efficiency , we re examine the above object function from another perspective , and propose to learn source weights based on a probabilistic model . o,t = vs o,t − ˆv∗ o,t|ws ∼ N ( 0 ,
Recall that the error of the s th source on object o at time t is defined as es o,t . As the weight of source indicates the quality of its provided information , the errors of a specific source given its source weight ws can be assumed to follow a normal dis ) , where θ is the trade off parameter tribution : es If the source weight ws is high , the errors in the loss function . will be small , which is equivalent to the idea that the aggregated results should be close to the information from high quality sources . Next , we formally prove that the above optimization problem can be translated into an equivalent likelihood estimation task .
θws
1
THEOREM 31 Given the fixed aggregated results {X ∗ t }T t=1 ) , minimizing loss function LT in Eq ( 1 )
1 and es o,t|ws ∼ N ( 0 , is equivalent to maximizing likelihoodQS SY
θws p(es
1:T|ws ) = p(es t|ws ) = s=1 p(es
1:T|ws ) . tY SY cs
TY
PROOF . We first give the formulation of likelihood : p(es o,t|ws ) s=1 tY cs
( o=1
√ θws)e t=1 s=1 o=1 o,t)2 2
− θws(es Pcs o=1(es t
2
− θws
√ θws)cs t e o,t)2
.
( 4 ) t=1
TY ∝ TY TY t=1
= s=1
SY SY SY s=1
( s=1 t=1
To maximize Eq ( 4 ) , we can minimize its negative log likelihood , which is given as follows :
0@ TY 0@ 1 SX t=1
2
− log
TX
SY s=1 cs tY tX cs o=1
θws t=1 s=1 o=1
1A ∝ SX s=1 p(es o,t|ws ) o,t)2 − 1 ( es 2
1A . cs t log θ
SX s=1 t log(ws ) − 1 cs 2
( 5 ) Since the third term in Eq ( 5 ) is a constant , Eq ( 5 ) and Eq ( 1 ) are equivalent . p(ws|es
1:T ) ∝ p(es
T|ws)p(ws|es
According to Theorem 3.1 , the optimization problem in Eq ( 1 ) can be transferred into likelihood estimation . Thus the posterior distribution of ws after timestamp T can be linked with the distribution after timestamp T − 1 as follows : p(ws|es
1:T ) = p(ws|es ∝ p(ws , es ∝ p(es = p(es
T , es t , es t|ws , es T|ws)p(ws|es
1:(T−1 ) ) 1:(T−1 ) ) 1:(T−1))p(ws|es 1:(T−1) ) .
1:(T−1 ) )
( 6 ) Eq ( 6 ) gives an incremental way to estimate the source weight at time T : The weight can be updated based on the weight at time T − 1 , and it is not necessary to re visit the previous data . This dramatically improves the efficiency of the proposed method .
In order to incorporate prior knowledge , we use Maximum a posteriori ( MAP ) estimation to estimate source weight ws . We set the prior distribution for ws as Gamma distribution p(ws ) ∝ Gamma(α , β ) , which is equivalent to have an initial loss l0 = s=1 wsβ −PS PS s=1(α − 1 ) log(ws ) .
By combining the prior knowledge and Eq ( 6 ) , we get the pos terior distribution of ws after timestamp T : p(es t|ws ) t=1
TY 1:(T−1 ) ) ∝ p(ws ) TY Pcs √ o=1(es t θws)cs t e PT
− θws Pt Pcs 1:T ) ∼ Gamma(α +
−(β+ θ 2
( t=1
PT o=1(es t
− θws t=1 cs t
Pcs o=1(es t t=1 t=1 e e
2
2
2 o,t)2 o,t)2 o,t)2)ws .(7 ) PT t=1 cs t 2
∝ wα−1 s e
−βws
∝ wα−1 e s α−1+ = w s
−βws w PT s t=1 cs t
2
This indicates that : p(ws|es
PT
Pcs t
θ 2 weight ws after timestamp T is : o=1(es t=1
, β + o,t)2 ) . Thus the MAP estimation for source
2α − 2 +PT 2β + θPT Pcs t t=1 t=1 cs t o=1(es o,t)2 ws =
( 8 )
From Eq ( 8 ) , we can see that the estimated source weight is inversely proportional to the average difference between its provided information and the aggregated results . According to the basic principle of truth discovery , if a source provides information that is close to the aggregated results , its corresponding weight should be high . The above equation for source weight estimation follows this principle . Meanwhile , parameters from prior distribution α and β also exert their effect on the source weight estimation . t , where as
0 = 2α−2 and as
Let ’s denote the accumulated counts for the s th source by timestamp t as as t . Similarly , let t represent the accumulated error for the s th source by timestamp bs t , where bs o,t)2 . Thus , Eq ( 8 ) becomes :
0 = 2β , and bs t−1 +cs o=1(es t = as t = bs t t−1 + θPcs T−1 + θPcs as T−1 + cs T o=1(es
T bs
.
( 9 ) o,T )2 ws =
Following this equation , to update the source weights after timestamp T , we only need to count the information and calculate the errors that happen within the timestamp T . This guarantees that we do not need to re visit the data at previous timestamps . Algorithm Flow . So far , we have derived the incremental way to update source weights . The whole computation procedure for each timestamp is illustrated in Algorithm 1 . Similar to the existing truth discovery methods , the proposed method follows the
678 T }S
Algorithm 1 : Incremental Algorithm Flow Input : Information from S sources at timestamp T : {X s accumulated counts and errors at previous timestamp . Output : Aggregated information X ∗ T . 1 : Update the aggregated results ˆv∗ o,T according to Eq ( 3 ) based on the current 2 : Compute the count cs 3 : Update the weight of s th source according to Eq ( 9 ) , and meanwhile , update the 4 : return X ∗ T . accumulated counts and accumulated errors for next timestamp . estimation of source weights . o,T for each source .
T and error es s=1 , and the general principle to estimate source reliability and infer trustworthy information . The advantage of the proposed method is that it works in an incremental way and only scans the data once . Thus it has great efficiency and is suitable for dynamic scenario . The theoretical analysis in Section 4 will prove that the estimated source weights converge to the true source reliability . In the following , we will show that by slightly modifying this solution , the proposed method can capture the observations in Section 2 .
Smoothing Factor . In Section 2 , we show that the information of objects is evolving in a smooth way . To capture this observation , we add one smoothness constraint on the aggregated results . Thus , the loss function at time t becomes : o,t)2 − SX o,t − ˆv ∗ ( vs o=1 o,(t−1 ) − ˆv ∗ ∗ ( ˆv o,t ) , cs t log(ws ) s=1
( 10 ) lt = θ tX cs
SX X ws s=1 o∈O
+θλ
PS where λ is a parameter to control the effect of this smoothness constraint . This added term will not affect the source weight estimation step as the aggregated results are fixed in this step . For aggregation step , we can take the derivative of Eq ( 10 ) with respect to ˆv∗ o,t : s=1 wsvs
PS o,t + λˆv∗ s=1 ws + λ o,(t−1 )
∗ ˆv o,t =
.
( 11 ) Let ’s consider the aggregated results at previous timestamp t − 1 as a pseudo source for the corresponding objects at timestamp t . If we treat λ as the weight of this pseudo source , the aggregation is still in the form of weighted combination of all the sources . Based on this , we incorporate the smoothness assumption in a simple yet elegant way : at each time t , we treat the previous aggregated result ˆv∗ o,(t−1 ) as the information from a pseudo source , and the proposed algorithm remains the same .
One thing needs to set is the tuned parameter λ , which can be interpreted as “ the weight of the pseudo source ” . Big λ suggests that we should rely on this constraint more , while small λ can relax this constraint . We will study the effect of this parameter on both real world applications and synthetic data in Section 5 .
Decay Factor . Another observation in Section 2 is that the observed source reliability fluctuates around its true source weight , and it can be explained by the environment factor . In order to tackle this challenge in the proposed method , we introduce a decay factor γ for source weight estimation as follows :
2α − 2 +PT 2β + θPT Pcs t=1 γT−t · cs o=1 γT−t · ( es t=1 t t o,t)2 ws =
( 12 )
If we treat the true source weight as a “ global ” weight , this formula enables us to estimate a “ local ” one , which captures both the true source weight and the environment factor . The introduced decay factor γ is used to balance the true source weight and environ ment factor : If γ is close to 1 , the estimated local weight will be close to the true source weight ; While if it is smaller than 1 , the local weight captures the environment factor more . In experiments we will show that by incorporating this decay factor , the proposed method can model the balance between the true source weight and the environment factor . 4 . THEORETICAL ANALYSIS
In this section , we prove the estimated source weights given by the proposed method ( Eq ( 8 ) ) will converge to the true source weights , and further , the rate of convergence is as fast as o( 1√ ) . To prove this , we need the following lemma first .
T
LEMMA 41 Suppose for each T , there exists a W∗
T that mint=1 lt . Then the posterior distribution p(W|e1:t ) imizes LT = PT satisfies the asymptotic normality : T ))1/2(W − W∗
( ∇2LT ( W∗ where W ∼ p(W|e1:T ) .
T ) d−→ N ( 0 , 1 ) , as t → ∞ ,
( 13 )
PROOF . We use the Theorem 2.1 in [ 2 ] to prove the asymptotic normality . The theorem states that if a series of functions satisfies certain sufficient conditions , then the asymptotic normality stated in Lemma 4.1 holds . Therefore , we only need to prove that our functions satisfy those conditions . Recall that the posterior distributions p(W|e1:t ) are a series of probability density distributions with respect to t . Suppose that W∗ T is the local maximum of log p(W|e1:T ) . We need to prove the conditions as listed below :
P1 . ∇ log p(W|e1:T )|W∗ P2 . ΣT ≡ {−∇2 log p(W|e1:T )|W∗ C1 . “ Steepness ” : σ2
= 0 .
T
}−1 is positive definite . t → 0 , as t → ∞ , where σ2
T t is the largest
“ Smoothness ” : t ; δ ) = {|W − W∗ eigenvalue of Σt . for any > 0 , C2 . there exists an integer N and δ > 0 such that , for any t > N , and W ∈ H(W∗ t | < δ} , ∇2 log p(W|e1:t)|W satisfies I − A( ) ∇2 log p(W|e1:t)|W{∇2 log p(W|e1:t)|W∗ }−1 I + A( ) , where I is identity matrix and A( ) is positive semidefinite symmetric matrix whose largest eigenvalue goes to 0 as → 0 . the probability
R known , so we can treat θPcs
C3 . H(W∗ Proof of ( P1 ) and ( P2 ) . We first prove ( P1 ) and ( P2 ) . Here we assume that at each time t , the number of claims made by a source t is a positive constant . The truths v∗ o,t for t = 1 , . . . , T are also cs o,t−v∗ o,t)2 as a constant ( denoted t ) . As shown is Eq ( 7 ) and the definition of the loss functions t ;δ ) p(W|e1:t)dW → 1 as t → ∞ . for any δ > 0 ,
“ Concentration ” :
0=1(vs t t as qs ( Eq ( 2) ) : log p(W|e1:T ) = log p(W ) +
Therefore , log p(W|e1:T ) = −C ∗ LT , where C is a positive constant . Since there exists a W∗ lt . t=0 t=1
TX log p(e1:t|W ) ∝ − TX PT PT t=0 cs w∗
T that minimizes LT , we have : t − qs
]s = 0 . s,T t
TX
∇LT ( W∗
T )s = [
Further , t=0
∇2LT ( W∗
T ) = diag( t=0 cs w∗2 t s,T
)s , which is positive definite .
T )−1 = )s corresponds to ΣT in ( P2 ) , and therefore ( P1 ) and
Note that ∇2LT ( W∗ s,TPT w∗2 t=0 cs t diag( ( P2 ) are satisfied .
679 Proof of ( C1 ) . As T → ∞,PT
1 w∗2 w2 t=0 cs t , the number of claims made by a source , will go to infinity , which means all the eigenvalues of ∇2LT ( W∗ T )−1 will go to 0 . Thus the “ steepness ” condition ( C1 ) is satisfied . Proof of ( C2 ) . Since all elements in ∇2Lt(W ) is continuous with respect to ws,t , the “ smoothness ” assumption ( C2 ) for ∇2Lt(W ) is straightforward . To be more specific , if W ∈ H(W∗ t ; δ ) = {|W − W∗ 1−δ , where < 1 w∗2 δ = δ ) , it imw∗ w2 mediately implies ( C2 ) . Proof of ( C3 ) . Based on ( P1 ) and ( P2 ) , W∗ source , it can be calculate as t=0 qs t | < δ} , we have 1+δ < . Since ∇2Lt(W)∇2Lt(W∗
PT T exists and for each PT s,T is the mean of t ) , the posterior distribution . As T → is a constant , the → 0 , ∗ T )2 ] → 0 . Therefore , the “ con
. In fact , w∗ PT PT PT PT variance of the posterior distribution which means Ep(W|e1:T )[(W − W∗ centration ” assumption is proved . t=0 cs t will go to infinity . Since
Gamma(PT ∞,PT t=0 cs t t=0 qs t t=0 cs t t=0 qs t t )−1 = diag( t ,PT t=0 cs t t=0 qs t
1PT t=0 qs t=0 qs t s,t s,t s,t s s
As shown above , all the sufficient conditions for Theorem 2.1 in [ 2 ] are satisfied , so Lemma 4.1 holds .
Lemma 4.1 states the approximated distribution of the posterior p(W|e1:t ) . As T → ∞ , the asymptotic distribution implies that the estimation based on the posterior distribution can converge to the minimizer of the accumulated loss W∗ T . Next , we will show that the weight given by Eq ( 8 ) can converge to the true source weight W∗ at rate of o( 1√
) .
T
√
T ( ˆW − W∗
THEOREM 42 If for any t , lt = l , then as T → ∞ , ˆW , given by Eq ( 8 ) , converges to W∗ = arg minW E[l ] at ) d−→ N ( 0 , Σ ) , where Σ = ( ∇2E(l))−1V ar(∇l)(∇2E(l)−1 ) . √ PROOF . In order to prove Theorem 4.2 , we will first prove that T ( ˆW − Ep(W|e1:T ) ) → 0 , then prove that |Ep(W|e1:T )(W ) − T − W∗ ) d−→ T| = o( 1√ W∗ N ( 0 , Σ ) . The weight given by Eq ( 8 ) is the posterior mode of p(ws|es
) , and finally prove that
√ T ( W∗
( 14 )
1:t ) ,
T a Gamma distribution . Therefore ,
| ˆW − Ep(W|e1:T )(W)| =PS =PS s=1 | ˆws − Ep(ws|es 1:T )(ws)| Pcs 2β+θPT T ) = o( 1√
= Θ( 1 t o=1(es o,t)2 s=1 t=1
) ,
2
T
( 15 ) where Ep(ws|es
1:T )(ws ) is the posterior mean .
From Lemma 4.1 , we can take the expectation on the asymptotic distribution and get the following :
E{(∇2LT ( W∗
T ))1/2(W − W∗
T )} → 0 , which implies that
|Ep(W|e1:T ) − W∗
T| = o(1)|(∇2LT ( W∗
T ) )
−1/2| .
As shown in the proof for Lemma 4.1 , ∇2LT ( W∗ therefore |Ep(W|e1:T ) − W∗
T| = o( 1√
) .
T
T ) = Θ(T ) ,
Since W∗
PT
1 T
[ 16 ] ,
T = arg minW LT , t=0 lt , which converges it is also the minimizer to E(l ) by the law of of By the theorem of stochastic gradient delarge number . √ T ( W∗ d−→ N ( 0 , Σ ) , where Σ = T − W∗ ) cent ( ∇2E(l))−1V ar(∇l)(∇2E(l)−1 ) √ T ( ˆW − Ep(W|e1:T ) ) → 0 , So far , we prove the followings : √ √ T ( W∗ T ( Ep(W|e1:T ) − W∗ T − W∗ ) d−→ N ( 0 , Σ ) . T ) → 0 , and By Slutsky ’s theorem , we get
√ T ( ˆW − W∗ ) d−→ N ( 0 , Σ ) .
The theoretical analysis above demonstrates that the proposed method ( Eq ( 8 ) ) has not only the intuitive explanation , but also the theoretical guarantee : the estimated source weights converge to the true weights in a fast speed . We will confirm this claim experimentally in next section .
5 . EXPERIMENTS
In this section , we experimentally validate the proposed method for truth discovery under dynamic scenario from the following aspects : 1 ) On three real world datasets , the proposed method achieves better performance comparing with state of the art truth discovery algorithms , while the efficiency is significantly improved . We also show that the introduced smoothing and decay factors can capture the temporal relations among the evolving truths and source weights . 2 ) On synthetic data , the theoretical analysis is confirmed by the experiments . We further systematically study the effect of smoothing and decay factors , and the efficiency is tested on large scale dataset . 5.1 Experiment Setup Compared Methods . For the proposed method , we test the incremental version ( denoted as Dynamic Truth Discovery , DynaTD for short ) , incremental version with smoothing factor ( DynaTD+smoothing ) , incremental version with decay factor ( DynaTD+decay ) , and incremental version with both smoothing and decay factors ( denoted as DynaTD+all ) . By gradually adding more components into the proposed method , we can investigate the benefit of considering these factors and understand how it tackles the challenges .
For baseline methods , the following state of the art truth discovery methods are implemented : TruthFinder and AccuSim adopt pre defined rules to update both source reliability and aggregated results . Investment borrows the idea that a source “ invests ” its reliability on the information it provides . 3 Estimate further extends the scope by considering the difficulty of aggregating information for specific objects . GTM is a probabilistic graphical model based truth discovery method that is designed for continuous data , and CRH is a truth discovery framework that can incorporate various distance functions and work with heterogeneous data . CATD is a recent truth discovery approach that considers the confidence interval of the source reliability estimation .
Beyond the above truth discovery algorithms , we also implement two naive methods Mean and Median , which do not consider the source reliability and simply take the mean or median of all information as aggregated results .
Note that the baseline methods cannot take streaming data as input . Therefore , their settings are not exactly the same with the proposed methods . For all the baselines , since they cannot capture the temporal relations among evolving truths and source weights , they are deployed on the entire dataset in a batch way , and treat the same object at different timestamps as different objects . In contrast ,
680 Weather Dataset
Table 2 : Performance Comparison Stock Dataset
Flight Dataset
MAE 3.7722 3.7230 3.7646 3.7093 4.9903 4.9004 4.7463 3.9493 4.6310 4.7573 4.5369 4.8999 4.8804
RMSE 4.8655 4.8007 4.8627 4.7857 6.4982 6.5752 6.1749 5.1038 6.0178 6.6054 6.2482 6.8722 6.8350
Time(s ) MAE 0.9320 0.2659 0.1563 0.2801 0.9121 0.2450 0.1481 0.2849 0.9438 0.1167 0.1038 0.9133 0.8863 1.1480 0.6371 0.8398 0.8952 3.1769 0.8933 32.6739 0.8963 33.6196 0.9136 0.9750 31.9051 0.9096
RMSE 2.5582 0.7885 2.5498 0.7845 2.5357 2.6897 2.5365 2.6234 2.5527 2.6589 2.6595 2.6889 2.6908
Time(s ) MAE 6.5050 1.5226 6.2966 1.5320 6.3798 1.5260 6.2309 1.5210 0.5701 8.2575 7.8097 0.5919 7.6506 6.6506 9.1807 8.6980 8.6453 14.0154 8.9633 494.9164 7.5661 505.6787 6.7258 7.3751 224.3704 7.2561
RMSE 52.3547 45.9228 52.2029 45.8221 51.5801 58.2965 51.6956 58.1676 53.0601 62.6080 60.8732 60.1398 60.7468
Time(s ) 7.8491 7.6452 7.6444 7.6165 3.1808 3.2109 30.6503 38.9449 81.1288 598.6464 622.4706 25.5679 1186.7492
Method DynaTD
DynaTD+smoothing
DynaTD+decay DynaTD+All
Mean Median GTM CRH CATD
TruthFinder AccuSim Investment 3 Estimates the proposed methods work in an incremental fashion , and can deal with the evolving truths and source weights . To model the temporal relations among the true information , either the groundtruth or the aggregated results for past timestamps can be adopted . Performance Metrics . To evaluate the performance , we calculate the following metrics on aggregate results by comparing them with the groundtruth : Mean of Absolute Error ( MAE ) and Root of Mean Squared Error ( RMSE ) . MAE uses L1 norm distance that penalizes more on small errors , while RMSE adopts L2 norm distance that gives more penalty on big errors . In the results analysis , we will discuss their difference in more detail . For both metrics , lower value indicates better performance .
To assess the efficiency , we also report each method ’s running time . For each baseline , we implement it and set its parameters according to the original papers . All the methods are run on a machine with 16G RAM , Intel Core i7 processor . 5.2 Experiments on Real World Datasets Datasets . In order to evaluate the proposed method in real world applications , we adopt the aforementioned Weather Forecast dataset , Stock Record dataset , and Flight Status dataset as testbeds . Here we provide more details about these datasets . • Weather Forecast : We collect high temperature forecast information for 88 big US cities from HAM weather ( HAM)1 , Wunderground ( Wund)2 , and World Weather Online ( WWO)3 . The collection lasts for more than two months ( Oct . 7 , 2013 to Dec 17 , 2013 ) . In addition to the forecast information , real high temperature observations of each day are also collected for evaluation purpose .
• Stock Record : The stock data [ 8 ] contains information for 1000 stocks that are collected from 55 sources during each weekday of July 2011 . As we assume the information is continuous data type , Market property is adopted . The groundtruth information is provided by the authors .
• Flight Status : The flight data [ 8 ] extracts departure and arrival information for 1200 flights from 38 sources during every day in December 2011 . All the time information is translated into minutes ( for example , 07 : 30am is translated into 450 mins ) . The groundtruth information is also available from the authors .
1http://wwwhamweathercom 2http://wwwwundergroundcom 3http://wwwworldweatheronlinecom
Performance Comparison . Table 2 summarizes the results for all the methods on the three real datasets . In terms of aggregation accuracy , the proposed method achieves best performance on every dataset , and the improvement is promising : on Weather dataset , compared with the best baseline CRH , the proposed method ’s MAE decreases by 3 % while RMSE decreases by 6.23 % ; on Stock dataset , compared with the best baseline method CRH , MAE and RMSE of the proposed method decrease by 82.4 % and 70.1 % respectively ; while on Flight dataset , compared with the best baselines Investment and GTM , the proposed method ’s MAE reduces by 7.4 % and RMSE reduces by 114 % Among the baseline methods , Mean and Median simply aggregate the multi source information without considering source reliability , and thus they have the worst performance . TruthFinder , AccuSim , Investment and 3 Estimates take categorical data as input , so they are not able to handle continuous data type well . In this sense , GTM , CRH and CATD are more appropriate for these tasks . Our proposed method is based on the advantages of these existing methods , and by considering the time evolving truths and source weights in dynamic scenarios , it achieves the best performance .
In terms of efficiency , the running time of the proposed methods is close to Mean and Median , which can be viewed as the lower bound of running time for truth discovery algorithms . Compared with the most efficient truth discovery baseline method , the proposed method significantly reduces the running time : on Weather dataset , it runs two times faster than CRH ; on Stock dataset , it runs four times faster than GTM ; while on Flight dataset , it runs eight times faster than Investment . Most truth discovery methods require some iterations to converges , so it is time consuming . Among them , TruthFinder and AccuSim need to calculate the implication function , and 3 Estimates needs to estimate the difficulty of each object , so their running time increases dramatically .
To summarize , under the dynamic scenario , the proposed methods outperform all the baselines in terms of both accuracy and efficiency . Among the proposed methods , we can observe that by adding smoothing factor and decay factor , the performance is further improved . Next , we analyze the effect of these two factors .
The Effect of Smoothing Factor . Smoothing factor is used to deal with the evolving truths . Pseudo sources are created to model this , and the smoothing factor adjusts how much influence these pseudo sources will exert . Figure 3 shows the effect of different smoothing factors on the three real world datasets . For Weather dataset , a relative small smoothing factor leads to the best performance . For Stock dataset , the change range of stock information
681 on the collected data is small , so a big smoothing factor is suitable . The most interesting one is Flight dataset : the MAE and RMSE change in different ways as smoothing factor increases . Except some special cases , a flight will depart or arrive around the scheduled time , so a large smoothing factor will enforce the aggregated results to be close to the history , which can avoid large errors , which leads to the decreasing in RMSE as L2 norm gives more penalty on large errors . However , enforcing the results to be close to history information will bring some small errors , since the real departure and arrival time would not stay exactly the same . As a consequence , MAE increases as it penalizes more on small errors . From these results we can see that the introduced smoothing factor has the ability to capture the unique characteristics of real world applications . weights should be close to the global weights . However , for Stock application , the information changes in a fast speed , which means the effect of environment factors is big , so the improvement can be achieved when the decay factor is close to 0 and local weights are estimated . For Flight application , the environment is stable for most cases , but it may change rapidly , especially when flights delay due to unanticipated reasons , so a medium decay factor is more suitable . From these results , we observe that the introduced decay factor can handle the challenge of the fluctuated source weights .
( a ) Weather
( b ) Weather
( c ) Stock
( d ) Stock
( a ) Weather
( b ) Weather
( c ) Stock
( d ) Stock
( e ) Flight
( f ) Flight
Figure 3 : Performance wrt Smoothing Parameter λ
The Effect of Decay Factor . Under dynamic scenario , the errors might be introduced by environment factors instead of the source itself , so the observed source reliability fluctuates around its true weight . To tackle this challenge , decay factor is introduced to estimate local weights . We first examine how environment factors are captured by comparing the estimated weights without decay factor and with decay factor . Figure 4 reports the comparison on three real world datasets , in which blue dot lines show the estimated weights without considering decay factor while red line represents the estimated weights by considering decay factor . From these estimated weights , we can draw the conclusion that : without decay factor , the estimated source weight converges to its true weight ( we will further confirm this in the experiment on simulated data ) ; while by considering the decay factor , the estimated weights capture both the true weight trend and the local environment factors .
Next , we study the effect of decay factor . Figure 5 illustrates how the performance ( in terms of both MAE and RMSE ) changes with respect to decay factor . For Weather application , as the environment factor in it is relative stable and small , the performance reaches the best when the decay factor is close to 1 , that is , the local
( e ) Flight
( f ) Flight
Figure 5 : Performance wrt Decay Parameter γ
5.3 Experiments on Synthetic Datasets
The experiments on the real world datasets demonstrate that the proposed method can improve the accuracy while largely reduce the running time , and the effect of introduced factors are also justified . In this section , we design a series of experiments on synthetic datasets to confirm the theoretical analysis , study the relationship between the introduced factors and environment factors , and test the efficiency on large scale datasets . Weight Convergence Study . To confirm the theoretical convergence analysis of the proposed method , we simulate the behavior of ten sources with different reliability levels for 20 timestamps . We assign each source a true weight , and at each timestamp , we generate 2000 observations for each source according to its true weight , ) . Then the simulated data is fed where the errors follows N ( 0 , 1 w∗ into the proposed method to estimate source weights . Figure 6 illustrates the comparison , in which black lines show the true source weights and red dot lines represent the estimated weights by the proposed method . We observe that the estimated weights ( red dot line in figures ) quickly converge to the corresponding true weights ( black line in figures ) . This result experimentally confirms Theorem 4.2 in Section 4 . Smoothing Factor . As the smoothing factor is introduced to capture the information evolution , here we study how the change rate of information can affect the best choice for smoothing factor . s
0123453738394λMAE012345484955152λRMSE0246810020406081λMAE0246810115225λRMSE0246810636356464565λMAE024681040455055λRMSE00204060813753838539γMAE0020406081485494955505γRMSE0020406081091091509209250930935γMAE00204060812552555256γRMSE00204060816356464565655γMAE00204060815252553535γRMSE682 ( a ) Weather
( b ) Stock
( c ) Flight
Figure 4 : Source Weight Comparison
( a ) MAE
( b ) RMSE
Figure 8 : The effect of Decay Factor γ
Efficiency Study . We further test the efficiency of the proposed method DynaTD on large scale dataset . We simulate various scales of datasets by controlling the number of claims made by each source at a given timestamp . As Mean and Median do not estimate source reliability , they have the optimal efficiency comparing with truth discovery methods . Here we use their running time as references and compare the running time of the proposed method with them . From Figure 9 , the conclusion can be drawn that the proposed method ( black line ) has the nearly optimal efficiency compared with Mean ( blue line ) and Median ( red line ) , even on very large dataset . This improvement is brought by the fact that the proposed method incrementally updates the source weights and only scans the whole data once .
Figure 6 : Source Weight Convergence
For clarity and simplicity purpose , we simulate a linear evolution of the true information , and control the change rates by different slopes , where a small slope indicates a slow change rate and vice versa . The data is generated using the same method as described in the previous experiment . Figure 7 shows the best smoothing factor for different change rates , and the choice is made based on MAE and RMSE respectively . They both show that when information changes slowly , we can rely on history information more and thus big smoothing factor works better . In contrast , the smoothness constraint should be relaxed when the information changes quickly .
( a ) MAE Figure 7 : The effect of Smooth Factor λ
( b ) RMSE
Decay Factor . We also study the relation between decay factor and environment factors . The environment factors are simulated through adding Gaussian noise to the observations , and they can be tuned by changing the variance of the Gaussian distribution . A small variance indicates a stable environment and vice versa . As the environment would have the same impact on all sources , we add the same level of noise to all the sources at a given timestamp . Other settings are kept the same as the one in above convergence experiment . Figure 8 shows the best decay factor with respect to different levels of environment factor . When the environment factor is small , the local weight is almost the same as the true weight ( global one ) , so the decay factor should be close to 1 . On the other hand , when the environment factor is quite large , the local weight , which captures the environmental changes , should play a more important role , so the best decay factor will tend to be small .
Figure 9 : Running Time wrt Number of Claims
6 . RELATED WORK
Truth discovery is a hot topic for resolving conflicts among multi source noisy information . Its advantage is the estimation of source reliability : instead of treating all sources equally , these methods infer the reliability of each source and incorporate such estimated source reliability into the aggregation . The early stage truth discovery methods [ 5 , 8 , 14 , 21 ] iteratively update the estimated source reliability and the aggregated results according to some pre defined rules .
Recently , more truth discovery methods are proposed to fit various scenarios . As most existing methods take facts ( categorical data ) as input , to enlarge the scope of applications , GTM [ 24 ] is specially designed for continuous data , and CRH [ 7 ] is a framework that can plug in different types of distance function to capture the unique characteristic of different data types and conduct the es
02040608000200400600801TimestampSource Weight DynaTDDynaTD+Decay05101520000200400600801TimestampSource Weight DynaTDDynaTD+Decay051015202530000200400600801TimestampSource Weight DynaTDDynaTD+Decay05101520012TimestampSource WeightSource a True WeightEstimated Weight051015200020406TimestampSource WeightSource b True WeightEstimated Weight0510152000102TimestampSource WeightSource c True WeightEstimated Weight05101520002004006TimestampSource WeightSource d True WeightEstimated Weight0051152050010001500Truth Change RateBestλ0051152050010001500Truth Change RateBestλ00511520020406081Enviromental FactorBestγ00511520020406081Enviromental FactorBestγ0246810x 106010203040506070Number of ClaimsTime ( s ) DynaTDMedianMean683 timation jointly . Another direction of truth discovery is the source correlation analysis [ 3 , 17 ] , in which sources are not independent and they may copy from each other . To further improve the advantage of truth discovery , people are considering to estimate more information rather than a single reliability degree for each source , and they enrich the meaning of source reliability from different aspects [ 6 , 13 , 15 , 25 ] . Nowadays , truth discovery has been applied into several domains , including social and crowd sensing [ 18 , 19 ] , knowledge fusion [ 4 ] , online health communities [ 11 ] , etc .
There is a few work that shares some similarities with our work . In [ 10 ] , the authors consider the information for truth discovery can be collected continuously , but they study this scenario from the source perspective . That is , they dynamically choose information sources from a pool to retrieve information , while in our setting , the information sources are fixed . The proposed method in [ 12 ] takes into account the evolving true information of objects , and estimates the truths of current timestamp based on sources’ historical claims . However , in our setting , no historical data are kept due to space limit . In [ 20 ] , the authors propose a method to handle time varying truths , but it works in batch operation on categorical data . A singlepass truth discovery method [ 26 ] is proposed to fit streaming data , but it fails to consider the unique characteristics of dynamic data and the proposed method is designed for categorical data . To the best of our knowledge , we are the first to propose an efficient algorithm to capture the temporal relations among both information and source reliability for truth discovery . 7 . CONCLUSIONS
In this paper , we propose to discover truths from dynamic data , where the collected information comes sequentially , and both truths and the source reliability evolve over time . This is a challenging task since we have to come up with an efficient way to capture the temporal relations among the identified trustworthy information and source reliability . To address the efficiency issue , we propose an incremental method by studying the equivalence between optimization based solution and MAP estimation . Theoretical analysis shows that the proposed method can guarantee the convergence of the estimated source weights and the rate of convergence is as fast as o( 1√ ) . To capture the temporal relations among both the identified trustworthy information and source reliability , we further incorporate two more factors , smoothing factor λ and decay factor γ , into the proposed method . Experiments on both real world and synthetic datasets demonstrate that the proposed method has great efficiency while the integration performance is further improved by capturing those temporal relations . The effect of smoothing factor λ and decay factor γ are also studied under various settings . 8 . ACKNOWLEDGMENTS
T
The work was sponsored in part by the US Army Research Lab . under Cooperative Agreement No . W911NF 09 20053 ( NSCTA ) , National Science Foundation IIS 1319973 , IIS1017362 , IIS 1320617 , and IIS 1354329 , HDTRA1 10 1 0120 , and grant 1U54GM114838 awarded by NIGMS through funds provided by the trans NIH Big Data to Knowledge ( BD2K ) initiative ( wwwbd2knihgov ) , and MIAS , a DHS IDS Center for Multimodal Information Access and Synthesis at UIUC . The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies . 9 . REFERENCES [ 1 ] D . P . Bertsekas . Non linear Programming . Athena Scientific , 2 edition , 1999 .
[ 2 ] C F Chen . On asymptotic normality of limiting density functions with bayesian implications . Journal of the Royal Statistical Society . Series B ( Methodological ) , pages 540–546 , 1985 .
[ 3 ] X . L . Dong , L . Berti Equille , and D . Srivastava . Integrating conflicting data : The role of source dependence . PVLDB , 2(1):550–561 , 2009 .
[ 4 ] X . L . Dong , E . Gabrilovich , G . Heitz , W . Horn , N . Lao , K . Murphy , T . Strohmann , S . Sun , and W . Zhang . Knowledge vault : A web scale approach to probabilistic knowledge fusion . In Proc . of KDD , pages 601–610 , 2014 .
[ 5 ] A . Galland , S . Abiteboul , A . Marian , and P . Senellart . Corroborating information from disagreeing views . In Proc . of WSDM , pages 131–140 , 2010 .
[ 6 ] Q . Li , Y . Li , J . Gao , L . Su , B . Zhao , D . Murat , W . Fan , and J . Han . A confidence aware approach for truth discovery on long tail data . PVLDB , 8(4):425–436 , 2015 .
[ 7 ] Q . Li , Y . Li , J . Gao , B . Zhao , W . Fan , and J . Han . Resolving conflicts in heterogeneous data by truth discovery and source reliability estimation . In Proc . of SIGMOD , pages 1187–1198 , 2014 .
[ 8 ] X . Li , X . L . Dong , K . B . Lyons , W . Meng , and D . Srivastava . Truth finding on the deep web : Is the problem solved ? PVLDB , 6(2):97–108 , 2012 .
[ 9 ] Y . Li , J . Gao , C . Meng , Q . Li , L . Su , B . Zhao , W . Fan , and J . Han . A survey on truth discovery . arXiv preprint arXiv:1505.02463 , 2015 .
[ 10 ] X . Liu , X . L . Dong , B . C . Ooi , and D . Srivastava . Online data fusion .
PVLDB , 4(11):932–943 , 2011 .
[ 11 ] S . Mukherjee , G . Weikum , and C . Danescu Niculescu Mizil . People on drugs : credibility of user statements in health communities . In Proc . of KDD , pages 65–74 , 2014 .
[ 12 ] A . Pal , V . Rastogi , A . Machanavajjhala , and P . Bohannon .
Information integration over time in unreliable and uncertain environments . In Proc . of WWW , pages 789–798 , 2012 .
[ 13 ] J . Pasternack and D . Roth . Comprehensive trust metrics for information networks . In Army Science Conference , 2010 .
[ 14 ] J . Pasternack and D . Roth . Knowing what to believe ( when you already know something ) . In Proc . of COLING , pages 877–885 , 2010 .
[ 15 ] J . Pasternack and D . Roth . Latent credibility analysis . In Proc . of
WWW , pages 1009–1020 , 2013 .
[ 16 ] R . Pasupathy and S . Kim . The stochastic root finding problem : Overview , solutions , and open questions . ACM Transactions on Modeling and Computer Simulation ( TOMACS ) , 21(3):19 , 2011 .
[ 17 ] R . Pochampally , A . D . Sarma , X . L . Dong , A . Meliou , and
D . Srivastava . Fusing data with correlations . In Proc . of SIGMOD , pages 433–444 , 2014 .
[ 18 ] L . Su , Q . Li , S . Hu , S . Wang , J . Gao , H . Liu , T . Abdelzaher , J . Han , X . Liu , Y . Gao , and L . Kaplan . Generalized decision aggregation in distributed sensing systems . In Proc . of RTSS , pages 1–10 , 2014 .
[ 19 ] D . Wang , L . Kaplan , H . Le , and T . Abdelzaher . On truth discovery in social sensing : A maximum likelihood estimation approach . In Proc . of IPSN , pages 233–244 , 2012 .
[ 20 ] S . Wang , D . Wang , L . Su , L . Kaplan , and T . Abdelzaher . Towards cyber physical systems in social spaces : The data reliability challenge . In Proc . of RTSS , pages 74–85 , 2014 .
[ 21 ] X . Yin , J . Han , and P . S . Yu . Truth discovery with multiple conflicting information providers on the web . In Proc . of KDD , pages 1048–1052 , 2007 .
[ 22 ] X . Yin , J . Han , and P . S . Yu . Truth discovery with multiple conflicting information providers on the web . IEEE Transactions on Knowledge and Data Engineering , 20(6):796–808 , 2008 .
[ 23 ] X . Yin and W . Tan . Semi supervised truth discovery . In Proc . of
WWW , pages 217–226 , 2011 .
[ 24 ] B . Zhao and J . Han . A probabilistic model for estimating real valued truth from conflicting sources . In Proc . of QDB , 2012 .
[ 25 ] B . Zhao , B . I . P . Rubinstein , J . Gemmell , and J . Han . A bayesian approach to discovering truth from conflicting sources for data integration . PVLDB , 5(6):550–561 , 2012 .
[ 26 ] Z . Zhao , J . Cheng , and W . Ng . Truth discovery in data streams : A single pass probabilistic approach . In Proc . of CIKM , pages 1589–1598 , 2014 .
684
