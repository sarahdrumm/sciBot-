Optimal Kernel Group Transformation for Exploratory
Regression Analysis and Graphics
Pan Chao
Department of Statistics
Purdue University West Lafayette , IN
47907 2066 panc@purdue.edu
Qiming Huang
Department of Statistics
Purdue University West Lafayette , IN
47907 2066 huang260@purdue.edu
Michael Zhu
Department of Statistics
Purdue University West Lafayette , IN
47907 2066 yuzhu@purdue.edu
ABSTRACT The general goal of multivariate regression analysis is to infer about the relationship between a response variable Y and a predictor vector X . Many popularly used regression methods only focus on specific aspects of this relationship . Even though the conditional distribution P ( Y |X ) of Y given X fully characterizes the relationship between Y and X , estimation of the conditional density is challenging and becomes quickly infeasible when the dimension of X increases . In this article , we propose to use optimal group transformations as a general approach for exploring the relationship between Y and X . This approach can be considered an automatic procedure to identify the best characteristic of P ( Y |X ) under which the relationship between Y and X can be fully explored . The emphasis on using group transformations allows the approach to recover intrinsic group structures among the predictors . Furthermore , we develop kernel methods for estimating the optimal group transformations based on crosscovariance and conditional covariance operators . The statistical consistency of the estimates has been established . We refer to the proposed framework and approach as the Optimal Kernel Group Transformation ( OKGT ) method . Simulation study and real data applications show that the OKGT method is flexible and powerful for the general purpose of high dimensional regression analysis .
Categories and Subject Descriptors I26 [ ARTIFICIAL INTELLIGENCE ] : Learning
Keywords Optimal transformation ; kernel method ; conditional covariance operator ; eigen decomposition ; regression analysis
1 .
INTRODUCTION
Regression analysis is a statistical technique for studying the relationship between a response variable Y and a predictor vector X based on a sample of Y and X . The relationship between Y and X can be fully characterized by the conditional distribution of Y given X , which is denoted as P ( Y |X ) . Therefore , the general goal of regression analysis is to infer about P ( Y |X ) as much as possible with the given sample , which we refer to as the exploratory regression analysis . However , many commonly used regression methods only focus on some features of P ( Y |X ) instead of the full conditional distribution . For example , ordinary least squares regression analysis focuses on the conditional expectation E[Y |X ] , and quantile regression analysis targets the conditional median or other quantiles of the response . Regression methods that focus on particular features of P ( Y |X ) suffer from some limitations . Firstly , the majority of those methods such as linear regression relies on strong model assumptions , and departure from the model assumptions may render those methods ineffective . Secondly , focusing only on the feature of interest while neglecting other aspects of P ( Y |X ) may make the regression analysis inefficient . Thirdly , those methods cannot be used to fully explore and capture the dependence of Y on X in the conditional distribution P ( Y |X ) . For example , suppose Y = 2X1 +X2 , where ( X1 , X2 ) and are independent and has mean zero and variance one . Under this model , ordinary least squares regression analysis can only capture X1 , and the estimate of the coefficient of X1 is not efficient . There exists some effort to directly estimate the conditional distribution P ( Y |X ) using nonparametric methods , which is commonly referred to as conditional density estimation in the literature . Rosenblatt [ 10 ] introduced conditional density estimation in 1969 . For conditional density estimation , Fan et al . [ 5 ] proposed to use local polynomial regression in 1996 , and recently Sujiyama et al . ( 2010 ) [ 14 ] proposed to use least squares density ratio estimation . Conditional density estimation may be useful for some specific application , but generally it is not practical or feasible especially when X is multidimensional . It is known that density estimation is challenging when the dimensionality of X is higher than five , and conditional density estimation can be even more difficult . To ensure sufficient accuracy of the density estimator , an extremely large number of data points is required [ 13 ] . Even when the conditional density can be accurately estimated as a function of Y and X , the dependence of Y on X cannot be easily interpreted .
Another approach that can potentially overcome the limitations of the two types of approaches discussed above is to first apply transformation to Y and X and then study
905 the relationship between the transformed Y and X . Box and Cox ( 1964 ) proposed a family of power transformations ( called Box Cox transformations now ) and used them to transform the response Y so that after transformation , the assumptions of linear model , normality and homoscedascity become appropriate . Later on , Box Cox transformations were applied to both Y and X , and then regression analysis was conducted for the transformed response and predictor variables [ 11 ] . This extension can accommodate nonliear relationship between the transformed Y and X . Although Box Cox transformations work well in many applications , the power transformations can become too restrictive .
Breiman and Friedman ( 1985 ) [ 3 ] considered applying general non parametric transformations to Y and X and further developed the Alternate Conditional Expectation ( ACE ) algorithm to compute the optimal transformations . Let Y be the response and X = ( X1 , . . . , Xp ) be the predictors . Let g(Y ) , f1(X1 ) , . . . , fp(Xp ) be the transformations of Y and X1 , . . . , Xp , respectively . The optimal transformations are the solutions to the following minimization problem . min g∈L2(PY ),fj∈L2(PXj st
) e2 = E[{g(Y ) − p j=1 fj(Xj)}2 ] ,
E[g(Y ) ] = E[fj(Xj ) ] = 0 ; E[g2(Y ) ] = 1 , E[f 2 j ( Xj ) ] < ∞ . about Y and X will be lost . In many applications , predictors are naturally divided into different categories or groups , and they affect the response in groups . In such an application , optimal univariate transformations ignore the group information . In practice , sometimes , the group information is hidden . The optimal univariate transformation framework does not provide the capacity to recover the group structure of the predictors . The recovery of such group structures not only helps understand the dependence of Y on X but also leads to models with higher prediction power as will be shown later .
To overcome the limitation of the optimal univariate transformation framework , in this article , we propose a new framework called the optimal group transformation framework as a general approach for exploring the relationship between Y and X . The framework is described as follows . First , the predictors X1 , . . . , Xp are partitioned into d groups denoted as X1 , . . . , Xd . Then , let g(Y ) , f1(X1 ) , . . . , fd(Xd ) be the transformations of Y and X1 , . . . , Xd , respectively . The optimal group transformations are the solutions to the following minimization problem .
) e2 = E[{g(Y ) − d
=1 f(X)}2 ] ,
E{g(Y )} = E{f(X)} = 0 ; E{g2(Y )} = 1 , E{f 2
( X)} < ∞ .
( 2 )
( 1 ) min g∈L2(PY ),f∈L2(PX st
Here , PY and PXj denote the marginal distributions of Y and Xj , respectively , and L2(P ) denotes the class of square integrable functions under the measure P . The target func tion e2 can be interpreted as the mean square error of re gressing g(Y ) against fj(Xj ) ’s . Notice that in the regression , the transformations are applied to the predictors individually , and then the transformed response is regressed against the sum of the transformed predictors . We refer to such a framework as the optimal univariate transformation framework . Under some regularity conditions , Breiman and Friedman showed that the optimal transformations exist , and their estimates are asymptotically consistent . Burman [ 4 ] proposed to estimate the optimal transformations using B splines and showed that the resulting estimates are consistent .
The reason we believe optimal transformation can be an effective and efficient approach to investigating the relationship between Y and X is two fold . Firstly , compared to regression methods based on pre specified features of P ( Y |X ) , the optimal transformation approach does not need to prespecify a particular feature of P ( Y |X ) . As a result , finding the optimal transformations can be considered an automatic procedure to find the best feature under which the relationship between Y and X can be best explored . Secondly , compared with the conditional density estimation approach , finding optimal transformations essentially solves a regression problem , which is numerically less challenging and can lead to more interpretable results .
The optimal univariate transformation framework discussed above has one limitation , that is , it only applies transformation to individual variable . Optimal univariate transformations may be computationally easy to calculate , but from the view point of exploring the relationship between Y and X , it can become a disadvantage . When predictors interact with each other , optimal univariate transformations are not able to capture the interactions , and much information
Here , PY denotes the marginal distribution of Y and PX denotes the joint distribution of all variables in X . It is clear that the original problem ( 1 ) is a special case of the group version ( 2 ) with d = p . The other extreme case is when d = 1 in which the optimization problem ( 2 ) is equivalent to the maximum correlation problem [ 3 ] .
To solve ( 2 ) and calculate the optimal group transformations , we propose to use Reproducing Kernel Hilbert Space ( RKHS) based methods ( or kernel methods ) and use crosscovariance and conditional covariance operators developed for kernel methods [ 2 , 6 , 7 ] . The reason of choosing kernel methods over B splines is due to a number of advantages kernel methods provide for fitting multivariate nonparametric functions [ 8 ] . In addition , cross covariance operators and conditional covariance operators between RKHSs defined via the expectation and covariance of random variables characterize the distributions and conditional distributions of the involved random variables . By using conditional covariance operator , we can transform the original functional optimization problem to be a functional eigen problem , which can allow simple theoretical analysis and numerical solution .
Given a sample of Y and X , the functional eigen problem can further be reduced to a finite rank eigen problem , and the empirical cross covariance and conditional covariance operators can be estimated by Gram matrices calculated from the kernel functions and the data . Applying matrix eigen value and vector decomposition , we obtain the estimates of the optimal group transformations . Because our proposed approach uses kernel methods , we refer to it as the Optimal Kernel Group Transformation ( OKGT ) method .
In this article , we further show that the OKGT estimates are statistically consistent , that is , they converges to their population counterparts . When the group structure of the predictors are not given a priori , we further propose to apply the OKGT method to randomly generated partitions of the predictors , and then select the partitions that achieve
906 top performance in model fitting after transformation . The optimal kernel group transformations can also be used to generate graphics visualising the dependence of Y on X . Through simulation study and real data applications , we show that the OKGT method is flexible and powerful for exploring the relationship between Y and X . We believe the proposed framework , particularly the OKGT method , is a significant contribution to high dimensional regression .
The rest of the article is organized as follows . In Section 2 , we introduce various RKHSs , define cross covariance and conditional covariance operators , and convert the optimal group optimization problem to a functional eigen problem ; and we further derive the estimates of the optimal kernel group transformations . The theoretical properties of the estimates are given in Section 3 . The proofs of the theoretical properties are included in the Appendix . We report the experimental results based on simulation study and real data applications in Section 4 . Section 5 concludes this article .
2 . METHODS
In this section , we present the development of OKGT . First , we introduce RKHS and direct sum RKHS , and rewrite problem ( 2 ) based on those RKHSs . Then , we use covariance and conditional covariance operators on RKHSs to convert the optimal transformation problem to an eigen problem and obtain the optimal transformations at the population level . Lastly , we give an algorithm to obtain the estimates of the optimal transformations under a given sample . 2.1 Optimal Kernel Group Transformation
Let Y be the compact support of Y , and X the compact support of the th group of predictors X for = 1 , . . . , d . Let HY and HX denote the RKHSs with domains Y and X and kernels kY and kX , respectively . In this paper , it is always assumed that the kernels are positive and satisfy
EY [ kY ( Y , Y ) ] < ∞ and
EX [ kX ( X , X ) ] < ∞ .
( 3 ) As pointed out in [ 6 ] , the assumptions ( 3 ) guarantee that HY and HX are continuously included in L2(PY ) and L2(PX ) , respectively . We search for the optimal transformations of Y and X in HY and HX instead of the function space L2(P ) . Therefore , the original optimal group transformation problem ( 2 ) needs to be rewritten as follows . e2 = E[{g(Y ) − d f(X)}2 ] , min g∈HY ,f∈HX st
=1
E[g(Y ) ] = E[f(X ) ] = 0 ; E[g2(Y ) ] = 1 , E[f 2
( X ) ] < ∞ .
( 4 ) sum space of HX ’s , which is defined below . d
=1
H+X = ⊕d
=1HX := sponding kerneld
=1 kX .
It can be proved that H+X is also a RKHS with the corre f = f : f ∈ HX , = 1,··· , d
.
Therefore , ( 4 ) can be considered a minimization problem over HY and H+X subject to the same constraints . To solve ( 4 ) at the population level , one approach is to apply kernel basis expansion methods . In order to simplify and facilitate the theoretical analysis , we resort to covariance and conditional covariance operators and use them to convert the original problem ( 4 ) to an eigen problem . Suppose U and W are two random variables or vectors . Let HU and HW be two RKHSs associated with U and W , respectively . The cross covariance operators RW U : HU → HW is a mapping from HU to HW such that g , RW U fHW
= EW U [ (f ( U ) − EU [ f ( U )])(g(W ) − EW [ g(W )] ) ] = Cov ( f ( U ) , g(W ) ) .
( 5 ) holds for all f ∈ HU and g ∈ HW [ 2 , 6 ] . Riesz ’s representation theorem guarantees the existence and uniqueness of RW U and it is bounded . The cross covariance operator RW U contains all the information regarding the dependence of U and W that can be characterized by the functions in the RKHSs . If W is the same as U , HW becomes RW W ( or RU U ) , which is a positive self adjoint operator and called the covariance operator . For the optimal transformation problem ( 4 ) , HY is HW , and H+X is HU . And the the cross covariance operator RY X : H+X → HY can be defined as follows . g , RY XfHY = EY X
( g(Y ) − EY [ g(Y ) ] ) d f(X ) − EX[ d f(X ) ]
=1
=1
Following the definition of ( 5 ) , the operators RY X , RXXj and RXX can be similarly defined . Because H+X is a direct sum space of HX ’s , RY X and RXX can be decomposed In in terms of RY X and RXXj with , j = 1 , 2 , . . . , d . particular , for f and f ∈ HX and g ∈ HY , d d
=1 g , RY XfHY = f , RXXfff
HX = g , RY X fHY , d f
, RXXj fj ff
( 6 )
( 7 )
.
HX
=1 j=1
Similar to [ 3 ] , to ensure the existence of the optimal transformations , the following assumption needs to be imposed . constraints in ( 4 ) such that g(Y ) +d
Assumption 1 : The only set of functions satisfying the =1 f(X ) = 0 as are individually zero as only involvesd
The optimization problem ( 4 ) appears to search for the transformation g and the individual transformations f for = 1,··· , d . Due to the fact that the target function e2 =1 f(X ) , which is an additive sum of f ’s , ( 4 ) can indeed be solved equivalently in HY and the direct and
Due to the decompositions above , we can define the matrix representations for the additive cross covariance and covariance operators RY X and RXX as follows .
RY X =.RY X1 RY X2 
RX1X1 RX1X2 RX2X1 RX2X2
RXdX1 RXdX2
RXX = fi ,  .
··· RY Xd
··· RX1Xd ··· RX2Xd . . . ··· RXdXd
( 8 )
( 9 )
907 These matrix representations admit the usual matrix operations [ 12 ] , which will facilitate the estimation procedure for the operators in Section 22
To convert the optimal group transformation problem ( 4 ) to an eigen problem , we need to introduce and use another type of operators called the conditional covariance operator . Following [ 7 ] , the conditional covariance operator for W given U , which are equipped with the corresponding RKHSs as discussed earlier , is defined through the cross covariance and covariance operators as RW W|U := RW W − RW U R
Proposition 2 in [ 7 ] shows that for any g ∈ HW ,
−1 U U RUW . inf f∈HU g , RW W|U gHW = EW U| ( g(W ) − EW [ g(W ) ] ) − ( f ( U ) − EU [ f ( U )])|2 . ( 10 ) Again replacing HW and U with Y and HX + , we define the conditional covariance operator RY Y |X is defined as
RY Y |X := RY Y − RY X R
−1 XX RXY .
( 11 )
Similar to Proposition 2 in [ 7 ] , we have the following proposition .
Proposition 1 . For any g ∈ HY , g , RY Y |X gHY = inf f∈H+X
EY X| ( g(Y ) − EY [ g(Y ) ] ) − ( f ( X ) − EX [ f ( X)])|2 , where H+X is the direct sum RKHS defined in ( 21 )
( 12 )
Proposition 1 accomplishes a key step towards converting the optimization problem ( 4 ) to an equivalent eigen problem . To solve ( 4 ) , a two step approach can be taken . In the first step , the target function is minimized with respect to f ; and in the second step , the resulting target function is further minimized with respect to g . With the help of Proposition 1 , the second step becomes a eigen problem involving the conditional covariance operator RY Y |X . We state this result as a proposition below .
Proposition 2 . The optimization problem ( 4 ) is equiva lent to min g∈HY st g , RY Y |XgHY , g , RY Y gHY = 1 .
( 13 )
Plugging in the expression of RY Y |X in ( 11 ) , the minimization problem ( 13 ) becomes the following generalized eigen problem , max g∈HY st
XXRXY gHY , −1 g , RY X R g , RY Y gHY = 1 .
( 14 )
Further letting ϕ = R1/2
Y Y g , ( 14 ) can be rewritten as follows . max ϕ∈HY st
−1/2 Y Y RY XR
ϕ , R ||ϕ||2HY = 1 ,
−1 XXRXY R
Y Y ϕHY , −1/2
( 15 )
It is not difficult to see that the solution of ( 15 ) , denoted by −1/2 ϕ∗ , is a unit eigenfunction of R Y Y corresponding to the largest eigen value . Denote the largest
−1/2 Y Y RY XR
−1 XXRXY R eigen value as λ1 and the minimum of the target function in ( 13 ) as e2∗ . We have λ1 = 1 − e2∗ . After having obtained ϕ∗ , the optimal transformations of −1/2 −1 Y and X ’s are given by g∗ = R XXRXY g∗ . Y Y ϕ∗ and f∗ = R d is a function in H+X . Note that f∗ = f∗ Using the matrix representations ( 8 ) and ( 9 ) of RY X and RXX , we can obtain the individual optimal transforms f∗ for = 1 , . . . , d .
2 + ··· + f∗
1 + f∗
W V VW U R1/2
Remark It is proved in [ 2 ] that in general , a cross covariance operator RW U : HU → HW admits the following decompoU U , where VW U : HU → HW sition : RW U = R1/2 is a unique bounded operator such that VW U ≤ 1 and 1 . Based on the above decomposiVW U = QW VW U QU tion , the conditional covariance operator can be rewritten as RW W|U := RW W − R1/2 W W . In our case , we denote the counterpart of VY X by VY X : H+X → HY Then , we have
W W VW U VUW R1/2
VY XVXY = R
−1/2 Y Y RY XR
−1 XXRXY R
−1/2 Y Y
Clearly , VY XVXY is self adjoint . Assuming it is compact , the existence of optimal transformations g∗ and f∗ in RKHSs is guaranteed by the spectral theorem . We will show later in Section 3 that VY X plays an important role in deriving the theoretical results .
2.2 Estimation Method
In the previous subsection , we have shown that the optimal group transformations can be obtained by solving an eigen problem involving covariance and conditional covariance operators . In this subsection , we focus on estimating the optimal transformations based on a given sample . We first derive the empirical covariance and conditional covariance operators , and further use them to define the empirical eigen problem . With proper regularization , the empirical eigen problem can be solved to produce estimates of the optimal group transformations .
Let {yi , xi1 , . . . , xid}1≤i≤n
2 be an iid sample . We use the Let ˜kY ( · , yi ) = kY ( · , yi ) − n−1n cross covariance operator RY X as an example to show how kX ( · , xi ) − n−1n s=1 kX ( · , xs ) . We define HY and HX to to derive the empirical operators . s=1 kY ( · , ys ) and ˜kX = i=1 and {˜kX ( · , xi)}n be the spaces spanned by {˜kY ( · , yi)}n i=1 βi˜kY ( · , yi ) + g⊥ and f =n g =n i=1 , respectively . For any g ∈ HY and f ∈ HX , we can write ˜kX ( · , xi ) + f⊥ , HY and HX in HY and HX , respectively . This construcwhere g⊥ and f⊥ belong to the orthogonal complements of i=1 α i tion ensures the zero mean constrains in ( 4 ) . By using the reproducing property of RKHSs and Riesz representation theorem , the empirical operator corresponding to RY X , de
1QU : HU → R ( RU U ) and QW : HW → R ( RW W ) are two orthogonal projections . 2Here we use the group representation . Each xi denotes the ith observation for the th group of predictor variables . So it can be a scalar value or a vector depending on the prespecified group structure .
908 noted as R(n ) g,R(n ) f
Y X
Y X
, is given by
HY = ( cid:100)Cov(g(Y ) , f(X ) ) n n n n g , ˜kY ( · , yi )
1 n i=1
=
HY
= f , ˜kX ( · , xi )
HX
βi˜kY ( yi , yk)˜kX ( xj , xk)α j i=1 k=1 where ( cid:100)Cov(g(Y ) , f(X ) ) is the sample covariance of g(Y ) and f(X ) . Therefore , ( cid:100)Cov(g(Y ) , f(X ) ) is of finite rank
( 16 ) j=1 and can be represented by Gram matrices .
We define GY to be the Gram matrix of the kernel kY for Y as ( GY )ij = kY ( yi , yj ) . Let 1n = ( 1 , . . . , 1)T . The centered Gram matrix is In − 1 n
In − 1 n
KY =
1n1T n
1n1T n
GY
.
Similarly , we can derive the centered Gram matrix KX for X . he empirical operator in ( 16 ) admits the following matrix representation ,
= βT KY KX α . f
Y X
Therefore , the finite rank operator R(n ) by KYKX . Similarly , we estimate R(n )
HY
Y X can be estimated
Y Y and RXXk by
KY KY and KX KXj respectively . Using the operator matrix representation gives the following estimates of the additive operators as block matrices , g,R(n )

R(n )
XX =
KX1 KX1 KX1 KX2 KX2 KX1 KX2 KX2
KXd KX1 KXd KX2
··· KX1 KXd ··· KX2 KXd . . . ··· KXd KXd
 and
R(n )
Y X = ( KY KX1 , KY KX2 , . . . , KY KXd ) .
By using the estimates of the operators defined above , we can derive the empirical version of the OKGT eigen problem in ( 15 ) as
R(n ) max ϕ∈HY
ϕ ,
−1
− 1 2 R(n ) R(n )
Y X
R(n ) − 1
Y Y + nI
2 ϕ
*
XX + nI
Y Y + nI
R(n )
XY
( 17 )
HY st
||ϕ||2HY = 1 .
Note that a regularization term nI is needed above , which would enable matrix inversion and avoid trivial solution . A detailed discussion can be found in [ 1 ] ,
In the spirit of the decomposition of a cross covariance operator mentioned in the remark in Section 2.1 , we simplify the notations by defining
− 1 So the product of the matrices in ( 17 ) becomes V(n ) Y XV(n )
− 1 2 R(n )
R(n )
R(n )
V(n )
XX + nI
Y Y + nI
Y X =
2 .
Y X
XY in the following discussion .
Y XV(n )
XY corresponding to its largest eigen value . Then the empirical estimates of the optimal transformations are given by
Let ϕ∗ be the unit eigen vector of V(n ) −1/2ϕ −1 R(n ) XYg
R(n ) R(n ) Due to the additive structure , the estimate f∗ is in the form of a column stack of f∗
, = 1 , . . . , d , which are the estimates
XX + nI
Y Y + nI g f
=
=
∗
∗
∗
∗
,
. of the optimal transformations for individual groups . 2.3 Group Structure Detection and Graphics As discussed in the Introduction , we call the method developed in Sections 2.1 and 2.2 the Optimal Kernel Group Transformation ( OKGT ) . Note that different group partitions may yield different fitting results , which further lead to different model interpretation . Ideally , the underlying group structure is given before the OKGT method is applied . However , the underlying structure may be unknown in practice . Therefore , it is essential to have a procedure to detect a suitable group structure which can well approximate the underlying true structure and yield meaningful interpretations . An optimal procedure to find the true underlying group structure should take a number of factors into consideration , such as a proper definition of the discrepancy measure between two group structures and the selection of group size and group numbers . The development of such an optimal procedure is beyond the scope of the current paper . Instead , we propose an intuitive approach for group structure detection . In this paper , we propose to use the random partition method and use R2 = λ1 , the largest eigen value XY , as the criterion to identify a suitable group structure . We prefer a structure that maximizes R2 among all partitions and at the same time has small group sizes . A model with relatively small group sizes can alleviate the curse of dimensionality and enhance the interpretability of the fitting results . Due to this reason , we suggest that each group contains no more than four variables .
Y XV(n ) of V(n )
Once a proper group structure is detected and optimal transformations are found by applying the OKGT method , graphical tools can be used to explore the relationship between the variables . Two examples include the plot of transformed response against the original response , and the marginal plots of transformed response against each transformed group of predictors . When a certain group contains two variables , 3 D plots can be employed to visualize the relationship between the response , the transformed group of variables , and each of the variables in the group . We believe all these plots will provide more insights in revealing the relationships between the predictors and the response , resulting in meaningful interpretations . More illustrations on the aforementioned plots and graphs are given in the examples in Section 4 . 3 . THEORETICAL PROPERTIES
In this section , we show that the estimates of the optimal kernel group transformations produced by the OKGT method are consistent in L2 norm . The regularization parameter n is assumed to decay to zero and the main idea of the proof follows [ 6 ] .
First , the following theorem shows that the estimated eigen function corresponding to the largest eigen value in
909 ( 17 ) converges to its population version under the RKHS norm .
Theorem 1 . Assume VY X is compact . Let ϕ∗ be an V(n ) Y XV(n ) eigen function corresponding to the largest eigen value of XY . Then , as n → ∞ , there exists a sequence n → 0 such that , fififiϕ
∗
, ϕ
∗HY fififi P−→ 1 , where ϕ∗ is an eigen function corresponding to the largest eigen value of VY XVXY . is another group , and expect our algorithm to recover the interaction between X6 and X7 .
We use Laplace kernel for all the groups with a fixed bandwidth 05 The regularization parameter n for estimating the optimal transformations is set at 001 We generate one set of data with sample size 500 from model ( 18 ) and apply OKGT , which results in an R2 value equal to 0909 Figure 1 shows the univariate transformations for the variables X1 to X5 . Figure 2 shows the bivariate transformation for the variables X6 and X7 as a group .
Theorem 1 relies on the assumption that VY X is compact , which may not hold in general [ 6 ] . If VY X is not compact , the solution of optimal transformation may not exist in RKHSs . A sufficient condition for VY X being compact is given in [ 6 ] , which is restated here . Let ( X ,BX , µX ) and ( Y,BY , µY ) be two probability spaces . Let pXY , pX , and pY be the density functions . If then the operator VY X : HX → HY is Hilbert Schmidt , which implies the compactness of VY X . pXY ( x , y)2 pX(x)pY ( y ) dµX dµY < ∞ ,
The next theorem further establishes the consistency of the estimated optimal transformations in L2 norm .
Figure 1 : Optimal transformations of the variables from X1 to X5 in model ( 18 ) by applying OKGT .
Theorem 2 . Assume that ϕ∗ is in the range of RY Y , and VY X is compact . Then , as n → ∞ , there exists a sequence n → 0 such that ∗L2
P−→ 0 .
∗ − f
∗ − g
P−→ 0 and flflflf g
∗flflflL2
PX
PY
4 . EXPERIMENTS
In this section , we evaluate the effectiveness of the proposed OKGT method on both synthetic and real data sets . We use R2 as the performance measure for OKGT . In our first simulation example , we show the effectiveness of OKGT in recovering the true function structure from the data generated from a given model . In our second experiment , we demonstrate the gain by using a proper group structure for OKGT . We compare the performance of OKGT with different group structures on two real datasets , the SkillCraft1 Master data from UCI Machine Learning Repository3 and the glioblastoma multiforme data from the TCGA Data Coordinating Center4 . 4.1 Effectiveness on Synthetic Data
In this experiment , we apply OKGT on synthetic data simulated from a model with known group structure . We assume the following model , y = log ( 4 + sin(2πX1)+|X2| + X 2
3 + X 3
4 + X5
+X6 ∗ X7 + 0.1 )
( 18 ) where X6 and X7 form a bivariate group through their product . The predictor variables Xj , j = 1 , . . . , 7 , are independent and identically distributed as Unif(−1 , 1 ) . The error term is standard normal . We assume the true structure is known , that is X1 to X5 each is a single group and ( X6 , X7 ) 3http://archiveicsuciedu/ml/ 4tps://tcga datancinihgov/tcga/
Figure 2 : Optimal transformation of the grouped variables X6 and X7 in model ( 18 ) by applying OKGT . Top left : Top right : Smoothed contour plot with data points . Bottomleft : 2 D projection of X6 versus f6(X6 , X7 ) . Bottomright : 2 D projection of X7 versus f6(X6 , X7 ) .
3 D scatter plot .
From Figures 1 and 2 , we can see that OKGT successfully recovers all the function forms of the univariate variables from X1 to X5 . It also clearly reveals the interaction between X6 and X7 as f6(X6 , X7 ) = X6 ∗ X7 .
−10−05 0.0 0.5 10−08−06−04−02 0.0 0.2 0.4 0.6 08−10−05 0.0 0.5 1.0x6x7f(x6 , x7)llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllX6X7−050005−050005−06−04−0200020406llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−10−05000510−06−020206x6f(x6 , x7)llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−10−05000510−06−020206x7f(x6 , x7)910 We also apply the additive univariate transformation where each predictor forms its own group . The resulting R2 equals to 0.8368 , which is lower than that from the group structure mentioned above . Furthermore , the univariate transformations of X6 and X7 fail to capture their interaction . 4.2
Impact of Group Structure
In this experiment , we investigate the effect of different group structures on the goodness of fit for OKGT . It can be conjectured that a fully nonparametric model , ie d = 1 in OKGT , would not give a good fit in terms of R2 because of the limited sample size and complex function space where the algorithm searches a solution . Besides , applying a fully nonparametric model usually produces a result that is hard to interpret . On the other hand , imposing a fully additive structure , ie d = p , may be too restrictive and would cause excessive information loss . Thus , a compromise is needed to balance fitting efficiency and interpretability of the result .
The setting of the experiment is given as follows . We generate 500 iid observations from the model :
Y = ( 5 + sin ( X1X2)+|X3X4| + X X6
5 + ( X7 − X8)+ + 0.1 )2 .
( 19 )
X9
+
X10 + 0.1 where X1 through X10 are sampled from Unif(0 , 2 ) , the error is standard normal , and ( a)+ denotes the maximum of 0 and a . In each simulation , we apply OKGT under each of the following six group structures : 1 ) g(Y ) ← f ( X1 , X2,··· , X10 ) ; 2 ) g(Y ) ← f1(X1,··· , X5 ) + f2(X6,··· , X10) ) ; 3 ) g(Y ) ← f1(X1,··· , X4)+f2(X5 , . . . , X8)+f3(X9 , X10 ) ; 4 ) g(Y ) ← f1(X1 , X2 , X3)+f2(X4 , X5 , X6)+···+f4(X10 ) ; 5 ) g(Y ) ← f1(X1 , X2 ) + f2(X3 , X4 ) + ··· + f5(X9 , X10 ) ; 6 ) g(Y ) ← f1(X1 ) + f2(X2 ) + ··· + f10(X10 ) .
If a group structure is able to include interacting variables in the same group , we call it a correct group structure , otherwise it is called an incorrect group structure . If any further partition renders an existing group structure incorrect , we call the structure as the intrinsic group structure . Note that any combinations of groups from a correct group structure A will yield another correct group structure B , we call B as an inherited group structure from A . According to the definitions , the group structures 2 , 4 , 6 are incorrect and the group structures 1 , 3 and 5 are correct ones . Group structure 5 is the intrinsic group structure and group structures 1 and 3 are inherited from group structure 5 .
In simulation , the Laplace kernel is used with bandwidth being 0.5 , and the regularization parameter n is set to 001 By repeating this procedure 100 times , we obtain Figure 3 which shows the side by side boxplots of the R2 from the six group structures after applying OKGT .
From Figure 3 , we noticed that for OKGT with the correct group structures ( except for group structure 1 ) , the average R2 is larger than those with the incorrect group structures . Group structures 3 and 5 achieve the maximum average of R2 ≈ 0.94 , with correct structure specifications . While group 1 represents the most flexible structure which in theory can accommodate any model , its fitting result ( R2 ≈ 0.90 ) is not as good as that of other correct group structures . With incorrect group structure 6 , which assumes a fully additive structure , the fitting result of OKGT is the
Figure 3 : Boxplots of R2 for different number of groups when applying OKGT on the sample from model ( 19 ) under six different group structures . worst ( R2 ≈ 088 ) It is interesting to observe that though group structures 2 and 4 are incorrect , their average R2 ’s are even higher the that from group structure 1 .
This phenomenon demonstrates that , with limited observations , the grouping effect ( different R2 ) is the result of the interplay of three factors : 1 . group structure specification ; 2 . group size ; 3 . the number of groups . For group structure specification , it is expected that the fitting result from a correct group structure is generally better than that from an incorrect group structure ; For group size , estimating a single function which contains many variables suffers from the curse of dimensionality , thus the fitting efficiency is generally lower than estimating a single function with less variables . The number of groups determines the number of functions to be estimated . Estimating more functions will accumulate the losses on fitting efficiency .
Assume that all different group structures are correct , then there will be a trade off between group size and the number of groups on fitting efficiency . For example , when d = 1 , though function size is small , we need to estimate a function containing all variables and the estimation efficiency is low for large p due to the curse of dimensionality . It could be even worse than the case where the group structure is incorrect . For the fully additive structure d = p , though group size is small , we need to estimate p functions in total and fitting efficiency will decrease with the increase of the number of functions to be estimated . Therefore , a balance between the group size and the number of groups is preferable .
It can also been seen in Figure 3 that the result from group structure 3 is almost as good as that of the intrinsic structure ( group structure 5 ) . This indicates that under finite sample , a more flexible group structure can approximate the intrinsic structure without too much information loss measured by R2 . This justifies the generalization of optimal transformation by using a group structure . 4.3 Real Data Application
SkillCraft1 Master data
431 In this experiment , we apply OKGT on the SkillCraft1 Master data set . This is a video game telemetry data from real time strategy ( RTS ) games and was originally used in [ 15 ] to explore the development of expertise . The study of the development of RTS expertise is of interest because the
911 knowledge learned can be applied in other domains . The data was collected from 3395 Star Craft 2 players ranging over 7 levels of expertise from novices to full time professionals . The levels are coded by the leagues in which they compete , and are coded from 1 to 8 as ordinal data . For each player , a replay file recorded all the commands issued in the game and the data of game related variables are calculated from the replay file . Some game related variables include action per minute , number of unique hotkeys used per timestamp , and number right clicks on minimap per timestamp . Our goal is to find some interesting relationships between game related predictor variables and the league index as the response variable .
Before applying our method , we randomly select a subsample of size 500 . There are 19 predictor variables and we only use the 15 game related variables in this experiment . By using all of the 15 predictors and imposing a fully additive structure ( d = p ) , the resulting R2 is 08454 A single group structure ( d = 1 ) results in an R2 of 06666 The first eight plots in Figure 4 show the transformations of the response and the seven predictors with the large variance Var(f(X) ) . The last plot in the figure shows a scatter ( xi ) . The red lines are the loess smoothing curves . The last plot shows a fairly linear relationship between the transformed response and the sum of all transformed predictors , indicating an overall good fit using OKGT with the additive structure . plot ofg∗(yi ) and15 =1 f∗ before 30 milliseconds but decreases slowly after that . The overall decreasing pattern indicates that players with higher LeagueIndex generally have lower GapBetweenPACs . This transformation can be interpreted as follows . For different players with large GapBetweenPACs , their skills will not change so rapidly . Once they reach the level with GapBetweenPACs as small as 30 milliseconds , it will require a huge improvement in skills to achieve a small saving in time between PACs . Thus , LeagueIndex drops dramatically within that range .
The predictor variables can be partitioned into different groups according to different types of skill in gaming . Based on this observation , we partition the predictors into seven groups as follows . • APM • SelectByHotkeys AssignToHotkeys UniqueHotkeys • MinimapAttacks MinimapRightClicks • NumberOfPACs GapBetweenPACs • TotalMapExplored • WorkersMade UniqueUnitsMade ComplexUnitsMade • ComplexAbilitiesUsed
ActionLatency
ActionsInPAC
Applying OKGT with the group structure defined above , we can achieve an R2 of 0.8675 , which is higher than the R2 from fitting the fully additive structure . This improvement clearly supports the advantages provided by useful grouping . 432 TCGA glioblastoma multiforme data In this example , we consider modeling the survival time of patients with glioblastoma , which is the first cancer studied by The Cancer Genome Atlas ( TCGA ) . The dataset we use contains the expression levels of 12042 genes and the survival time ( length ) of 400 Glioblastoma patients . A smaller sample ( 206 patients ) was considered in [ 9 ] . We are interested in identifying important genes associated with patients’ survival time and in investigating their relationship , to improve our understanding of the underlying biology of gliomas .
Figure 4 : Application of OKGT on SkillCraft1 data . First eight figures are transformation of the response and seven variables presenting large fitted norm . The last figure is scatter plot of g∗(yi ) and 15 j=1 f∗
( xi ) by OKGT with all variables . The red curves are the loess smoothing applied on the transformations .
From Figure 4 , the transformations are highly nonlinear for both the response and the seven predictors . This graphics can provide some meaningful interpretations . For example , the transformation of the response variable LeagueIndex shows a S shaped pattern , indicating that the acquittance of skill is not linear . The improvement of skill from level 1 to 2 and from 7 to 8 are more significant than at the other levels . The transformation of APM shows a similar pattern as that of the response . Between 100 to 150 , APM is roughly independent of the skill levels , whereas in the lower and higher range some linear pattern is shown . The transformation of GapBetweenPACs shows an overall decreasing pattern . However , the curve drops dramatically
Figure 5 : Application of OKGT on TCGA glioblastoma data . Transformations of the response and the first eight variables after fitting 30 top ranked variables .
We first apply OKGT to the response and each gene as a predictor marginally and then rank all the genes according to their R2 values in descending order . We keep the top 30 genes with the largest R2 values . By imposing a fully
912 additive structure ( d = 30 ) on the 30 retained predictors and preforming OKGT , the resulting R2 is 08510 If a single group structure ( d = 1 ) is used , the R2 is 07142
To compare the effect of different group structures , we conduct the following experiment . The 30 retained gene predictors are randomly partitioned into a fixed number of groups of equal size . The number of groups ( d ) in this experiment is set at 15 , 10 , 5 , 3 , and 2 , which is corresponding to having 2 , 3 , 6 , 10 , and 15 predictors in each group . The variables in each group are randomly assigned . By applying OKGT under a random group structure using the original data , we can obtain an estimate of R2 . The boxplots in Figure 6 are based on 100 simulations at each fixed number of groups .
Figure 6 : Boxplots of R2 for different number of groups when applying OKGT on the TCGA glioblastoma data with top ranked 30 genes .
From Figure 6 , we notice that even with random grouping , OKGT can easily achieve a higher R2 compared with that with the fully additive structure d = p . This experiment further supports that grouping can be advantageous , due to the trade off on fitting efficiency between function complexity and group size .
5 . CONCLUSIONS
In this article , we have developed an effective kernel method called OKGT for achieving the general goal of multivariate regression analysis , which is to explore the relationship of a response variable Y and a predictor vector X . In simulation study and real data applications , the OKGT method outperforms the optimal univariate transformation method ( ie ACE ) as well as multivariate nonparametric regression . The reason for OKGT ’s excellent performance is because it can either take advantage of existing group structures of the predictor variables or it can be used to recover the hidden group structure . The use of cross covariance and conditional covariance operators and their empirical counterparts much simplifies both the theoretical and numerical analysis of the OKGT method , demonstrating their power for high dimensional data analysis .
There are three immediate directions to further improve the OKGT method . First , a more effective and efficient procedure is needed for the OKGT method to detect intrinsic group structure among the predictor variables . Second , after the optimal kernel group transformations are estimated , how to use graphics to reliably infer the relationship between
Y and X needs to be further studied . Third , when the dimension of X is high or extremely high , penalization will probably be needed to make the OKGT method stable and effective in exploring Y and X . We are currently working on all these directions , and hope to report the results in the near future .
6 . REFERENCES [ 1 ] F . Bach and M . Jordan . Kernel independent component analysis . The Journal of Machine Learning Research , 3:1–48 , 2003 .
[ 2 ] C . R . Baker . Joint measures and cross covariance operators . Transactions of the American Mathematical Society , 186:273–289 , 1973 .
[ 3 ] L . Breiman and J . H . Friedman . Estimating optimal transformations for multiple regression and correlation . Journal of the American Statistical Association , 80(391):580–598 , 1985 .
[ 4 ] P . Burman . Rates of convergence for the estimates of the optimal transformations of variables . Annals of Statistics , 19(2):702–723 , 1991 .
[ 5 ] J . Fan , Q . Yao , and H . Tong . Estimation of conditional densities and sensitivity measures in nonlinear dynamical systems . Biometrika , 83(1):189–206 , 1996 . [ 6 ] K . Fukumizu , F . R . Bach , and A . Gretton . Statistical consistency of kernel canonical correlation analysis . The Journal of Machine Learning Research , 8:361–383 , 2007 .
[ 7 ] K . Fukumizu , F . R . Bach , and M . I . Jordan . Kernel dimension reduction in regression . The Annals of Statistics , pages 1871–1905 , 2009 .
[ 8 ] T . Hofmann , B . Sch¨olkopf , and A . J . Smola . Kernel methods in machine learning . The Annals of Statistics , pages 1171–1220 , 2008 .
[ 9 ] R . McLendon , A . Friedman , D . Bigner , E . G .
Van Meir , D . J . Brat , G . M . Mastrogianakis , J . J . Olson , T . Mikkelsen , N . Lehman , K . Aldape , et al . Comprehensive genomic characterization defines human glioblastoma genes and core pathways . Nature , 455(7216):1061–1068 , 2008 .
[ 10 ] M . Rosenblatt . Conditional probability density and regression estimators . Multivariate Analysis II , 25:31 , 1969 .
[ 11 ] R . Sakia . The box cox transformation technique : a review . The Statistician , pages 169–178 , 1992 .
[ 12 ] Y . Sato . Theoretical Considerations for Multivariate
Functional Data Analysis . In Proceedings 59th ISI World Statistics Congress , number August , pages 25–30 , Hong Kong , 2013 .
[ 13 ] B . W . Silverman . Density estimation for statistics and data analysis , volume 26 . CRC press , 1986 .
[ 14 ] M . Sugiyama , I . Takeuchi , T . Suzuki , T . Kanamori , H . Hachiya , and D . Okanohara . Conditional density estimation via least squares density ratio estimation . In International Conference on Artificial Intelligence and Statistics , pages 781–788 , 2010 .
[ 15 ] J . J . Thompson , M . R . Blair , L . Chen , and A . J .
Henrey . Video game telemetry as a critical tool in the study of complex skill learning . PloS one , 8(9):e75129 , 2013 .
913 Proof . Let a = ( RY Y + nI )
R(n )
−1 , an =
−1
−1/2 , b = RY X , Y Y + nI
, bn = R(n )
Y X , and A∗ represents the adjoint op
−1/2
R(n )
H = ( RXX + nI )
XX + nI
Hn = erator of A . anbnHnb nan − abHb ∗
∗ a
≤ ( an − a)bnHnb n(an − a ) + 2(an − a)bnHnb ∗ na + ∗ a(bnHnb n − bHb ∗ ∗ S1 + S2 + S3
)a
From Lemma 1 to Lemma 3 , we have
S1 ≤ an − a2 bn2 Hn −1 n
−1/22 ·
−2 n n
= Op
= Op(
−5 n n
−1 )
( 21 )
S2 ≤ an − abn2 Hna −1/2 n
−1/2 · n · −1
−2 n n
= Op
−7/2 n n
−1/2 ( 22 )
= Op
S3 ≤ a2 bnHnb n − bHb ∗
∗
≤ a2 ( (bn − b)Hn(bn − b ) ∗ + 2bnHnb ∗ ) b(Hn − H)b
≤ a2,(bn − b)2 Hn + 2bnHnb + b2 Hn − H−1/2
∗ +
≤ O d2
−3 n n
( 23 )
Then Lemma 4 follows by combining ( 21 ) ( 23 ) . Lemma 5 . Assume VY X is compact . Then , as n → ∞ , for a sequence n → 0 , Y XV( ) flflflV( )
XY − VY XVXY flflfl P−→ 0 .
Lemma 4 proves the first part of ( 20 ) , and Lemma 5 proves the second part . With Lemma 4 and Lemma 5 , we can prove Theorem 1 similar to the proof of Lemma 10 in [ 6 ] . A.2 Proof of Theorem 2
ϕ∗ −1/2 Y Y ϕ∗ , we have R ∗ − g
Proof . Without loss of generality , we assume ϕ∗(n ) → R(n ) k in HY . As g∗ = ∗flflfl2 flflflR1/2 Y Y ( g g Y Yg∗ − ϕ∗flflfl2 flflflR1/2 R(n ) Similarly , as f∗ =
−1/2ϕ∗ flflflR1/2 Y Yg −1 R(n )
HY HY P−→ 0 follows the proof of and g∗ =
Theorem 2 in [ 6 ] .
The fact that
Y Y + nI flflfl2
∗ − ϕ
∗ − g
XX + nI
∗2
HY
=
=
L2
PY
∗
)
.
XYg∗ and f∗ = flflflf∗ − f∗flflflL2
.
PX
Y Y + nI
Lemma 2 .
Lemma 3 . flflflflR(n ) flflflflR(n ) flflflV(n ) Y XV(n )
XX + nI
Lemma 4 . When d = O(n1/4 ) , we have that for a se quence n → 0 as n → ∞ , XY − V( )
Y XV( )
XY
−2 n n
−1/2 ) .
−2 n n
−1/2 ) .
( RXX )
−1 RXY g∗ , the same result holds for
APPENDIX A . PROOFS
Due to the space limit , we summarize here our theoretical results , provide all lemmas that lead to our proofs of theorems , but only give proofs to important lemmas . All other proofs are omitted , but are available from the authors upon request . A.1 Proof of Theorem 1 operator V(n )
In order to prove Theorem 1 that the sample estimates converge in a RKHS , we need to show that the finite rank XY converges to its population counterpart
Y XV(n )
VY XVXY . By defining
V( )
XY =
Y XV( ) ( RY Y + nI )
− 1
2 RY X ( RXX + nI )
−1 RXY ( RY Y + nI )
− 1
2 ,
Theorem 1 can be proved through the following two consistency results : flflflV(n ) Y XV(n ) flflflV( )
Y XV( )
XY − V( ) Y XV( ) XY − VY XVXY
XY flflfl P−→ 0 , as n −→ ∞ , flflfl P−→ 0 , as n −→ ∞ .
( 20 )
Without further clarification , · should be understood as the operator norm . We state in the following some important consistency results which lead to Theorem 1 .
Similarly , we can obtain the following results .
Lemma 1 . As n → ∞ , Y X − RY X Y Y − RY Y XX − RXX flflflR(n )
With Lemma 1 and the fact that Y X − RY X
Y X − RY X
−1/2 ) . flflfl − RY X ≤
Y X
, we have that
−1/2 ) .
−1/2 ) , −1/2 ) ,
Y X
= Op(n
= Op(dn
= Op(d2n flflflHS flflflHS flflflHS flflflR(n ) flflflR(n ) flflflR(n ) flflflR(n ) flflflHS flflfl ≤flflflR(n ) flflfl = RY X + Op(dn flflflR(n ) flflflR(n ) flflfl = RY Y + Op(n flflfl = RXX + Op(d2n flflflR(n ) −1/2 − ( RY Y + nI ) −1 −,(RXX + nI )
−1/2 ) ,
−1/2
XX
Y Y
−1/2 ) . flflflfl = Op( −1 flflflfl = Op(d2 flflfl = Op(d2
−7/2 n n
−1/2 ) .
914
