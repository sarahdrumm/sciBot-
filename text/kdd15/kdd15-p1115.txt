Turn Waste into Wealth : On Simultaneous Clustering and
Cleaning over Dirty Data
Shaoxu Song
Chunping Li
Xiaoquan Zhang
KLiss , MoE ; TNList ; School of Software , Tsinghua University , China
{sxsong , cli , zxq8984}@tsinghuaeducn
ABSTRACT Dirty data commonly exist . Simply discarding a large number of inaccurate points ( as noises ) could greatly affect clustering results . We argue that dirty data can be repaired and utilized as strong supports in clustering . To this end , we study a novel problem of clustering and repairing over dirty data at the same time . Referring to the minimum change principle in data repairing , the objective is to find a minimum modification of inaccurate points such that the large amount of dirty data can enhance the clustering . We show that the problem can be formulated as an integer linear programming ( ilp ) problem . Efficient approximation is then devised by a linear programming ( lp ) relaxation . In particular , we illustrate that an optimal solution of the lp problem can be directly obtained without calling a solver . A quadratic time approximation algorithm is developed based on the aforesaid lp solution . We further advance the algorithm to linear time cost , where a trade off between effectiveness and efficiency is enabled . Empirical results demonstrate that both the clustering and cleaning accuracies can be improved by our approach of repairing and utilizing the dirty data in clustering .
Categories and Subject Descriptors I53 [ Clustering ] : Algorithms
Keywords Data repairing ; data cleaning
1 .
INTRODUCTION
Density based clustering can successfully identify noises ( see a survey in [ 16] ) . However , rather than a small proportion of noise points , real data are often dirty with a large number of inaccurate points [ 10 ] . For instance , a ( very ) large portion of GPS data are inaccurate , especially in the indoor environment with weak signals . According to our experiments ( in Section 6 ) , 139 out of 818 ( about 17 % ) GPS readings are inaccurate . Similar examples of inaccurate data include RFID sensor readings [ 4 ] or multimedia data [ 11 ] .
The large amount of noise points are simply discarded , if we directly apply the existing density based clustering approaches , eg , the well known dbscan [ 7 ] . With too much information loss , clustering results could be dramatically affected ( see examples below ) .
Instead of discarding dirty data , we argue that the large amount of dirty data can be repaired and utilized as strong supports in clustering . To the best of our knowledge , this is the first study on performing data cleaning and clustering at the same time .
A natural idea is to first clean the dirty data before clustering . Existing constraint based cleaning techniques can be applied , eg , repairing with functional dependencies ( fd ) [ 3 ] ( see [ 8 ] for a survey ) . According to our empirical study ( in Figure 11 ) , the clustering accuracy can be improved by applying first the fd based repairing , when the fd constraints are available in the considered dataset . However , for some other datasets with simple schema , such as GPS data , no such ( fd ) integrity constraints could be declared ( and thus the constraint based repairing is not applicable ) .
Besides repairing mentored by integrity constraints , we propose to repair the dirty data under the guidance of density information , inspired by the successful identification of noisy data in density based clustering . The idea is to simultaneously repair dirty data wrt the density of data during the clustering process , rather than separately repairing ( ahead ) wrt integrity constraints . By interchangeably taking the advantages of density based clustering and repairing , both the clustering and repairing tasks benefit ( with accuracy improvement as shown in the experiments in Section 6 ) .
Following the minimum change principle in data repairing [ 24 ] , ie , the change made in repairing is expected to be as small as possible , the objective of simultaneous repairing and clustering is to find a minimum repair of data such that all the data can be clustered ( utilized ) . The rationale of minimum change is that systems or humans always try to minimize mistakes in practice . That is , dirty values , such as inaccurate GPS readings or typos in text , are often not very far from true values . Referring to this objective , we formalize the problem as an optimization problem , namely Density based Optimal Repairing and Clustering ( dorc ) .
Example 1 . We illustrate a motivation example of GPS data in Figure 1 . Consider GPS readings in two nearby buildings , denoted by blue and white points in Figure 1(a ) , respectively . C1 denotes more precise points ( high density , in a building with good GPS signal ) , whereas C2 , for a
1115 Figure 1 : Clustering with repairing over dirty data
. building with weaker GPS signal , contains inaccurate points ( low density , that drop far away from the building and need concentrating repairing ) . The density based clustering such as dbscan either returns a cluster C1 ( with high density ( with low density parameters ) . White requirements ) or C1 points corresponding to the second physical building cannot form a separate cluster ( either directly ignored as noisy ) . The masdata in C1 or merged with blue ones in C1 sive points identified as noises by dbscan in Figure 1(a ) are strong evidence to concentrate C2 . ( See more example results in Figure 6 in Section 6.1 )
.
In this study , we propose to repair the inaccurate data points during the clustering process . For example , as shown in Figure 1(b ) , an arrow ( a → b ) denotes that a point is repaired from location a to location b . Since points are concentrated ( with higher density ) after repairing , two clusters are successfully formed in Figure 1(c ) .
The problem of clustering with repairing , however , is nontrivial . Simply repairing noise points to the closest clusters is not sufficient , eg , repairing all the noise points to C1 in Figure 1 does not help in identifying the second cluster C2 . Indeed , it should be considered that dirty points may possibly form clusters with repairing ( ie , C2 ) .
It is notable that our proposed dorc techniques are complementary to the existing constraint based repairing ( when constraints are available ) . As illustrated in experiments ( Figure 12 in Section 6.3 ) , by combining our dorc approach with the existing fd based repairing , both the clustering and repairing accuracies are further improved . Compared to existing constraint based repairing , the major advantages of dorc are in two aspects : ( 1 ) it does not require any external knowledge of integrity constraints or rules ; ( 2 ) instead , it explores the density information embedded inside the data , which is not considered in the preliminary constraint based methods . In this sense , our formulation favors errors with significant distortion on density , while minor errors without altering density might not be handled .
Our major contributions in this paper are summarized as : ( 1 ) We formalize the dorc problem of simultaneous clustering and repairing . In particular , no additional parameters are introduced for dorc besides the density and distance requirements η and ε for clustering .
( 2 ) We formulate dorc as an ilp problem ( in Section 3 ) , investigate its lp relaxation and , most importantly , show that an optimal solution to the lp problem can be directly obtained without calling a solver .
( 3 ) We devise aquadratic time approximation algorithm qdorc upon the lp solution ( in Section 4 ) .
( 4 ) We advance the algorithm to linear time complexity ( in Section 5 ) . The more efficient ldorc supports a tradeoff between effectiveness and efficiency , via a group distance threshold τ .
Table 1 : Notations
Symbol Description
|P| the number of data points p ∈ P n m |L| the number of leader points p ∈ L distance of data points , δ : P × P → R δ distance threshold , Eps
ε
+ 0
η hij λ Δ(λ ) C ( pj ) cj τ density threshold , MinPts ε neighborhood of two points pi to pj repair of data points , λ : P → P cost of a repair λ set of ε neighbors of point pj count of neighbors in location pj after repairing group distance threshold
( 5 ) We report an extensive experimental study on both real and synthetic datasets ( in Section 6 ) . The results demonstrate that both the clustering and repairing accuracies are improved by our proposed dorc approaches .
Table 1 lists the frequently used notations in this paper .
Proofs of all lemmas and propositions can be found in [ 1 ] .
2 . PROBLEM STATEMENT Clustering . Consider a set of data points P . Let δ : P × P → R + 0 be a distance function , satisfying nonnegativity δ(pi , pj ) ≥ 0 , identity of indiscernibles δ(pi , pj ) = 0 iff pi = pj , symmetry δ(pi , pj ) = δ(pj , pi ) , where pi , pj ∈ P . Two points pi , pj ∈ P are said to be in ε neighborhood , if δ(pi , pj ) ≤ ε . We denote C(pi ) ={ pj ∈ P |δ ( pi , pj ) ≤ ε} the set of ε neighbors of pi , where pi ∈ C(pi ) as well . Definition 1 ( Core points , Border points , Noise points ) . Given a distance threshold ε ( Eps ) and a density threshold η ( called MinPts ) , a point pi with |C(pi)| ≥η is considered as a core point . A border point has |C(pi)| < η but is in εneighborhood of some core point . All the other points , which are neither core points nor border points ( in ε neighborhood of some core point ) , are noise points .
Repairing . A repair over a set of points is a mapping λ : P → P . We denote λ(pi ) the location of point pi after repairing . The ε neighbors of λ(pi ) after repairing is Cλ(pi ) = {pj ∈ P |δ ( λ(pi ) , λ(pj ) ) ≤ ε}
Following the minimum change principle in data cleaning that we prefer a repair close to the input [ 24 ] , the repairing cost Δ(λ ) is defined as n .
Δ(λ ) = w ( pi , λ(pi) ) ,
( 1 ) i=1 where w ( pi , λ(pi ) ) is the cost of repairing a point pi to the new location λ(pi ) . For instance , a count cost [ 15 ] , with w ( pi , λ(pi ) ) = 1 iff pi = λ(pi ) , considers the number of data points that are modified as the repairing cost . Alternatively , the distance of point locations before and after repairing could also be considered [ 3 ] , with w ( pi , λ(pi ) ) = δ(pi , λ(pi) ) .
DORC Problem . As mentioned in the introduction , by simply relaxing parameters in dbscan , the diffusion of nearby
1116 After repairing , there may exist multiple points pi being repaired to the location of a point pj . The new εneighborhood count of location pj is cj = |{pi ∈ P | δ(λ(pi ) , pj ) ≤ ε}| = n . n . n . hjkxik , xij + i=1 k=1 i=1
( 3 ) where hjk = 1 denotes that locations pj and pk are in εneighborhood ; otherwise , hjk = 0 . That is , cj counts the total number of points located in pj and all of its ε neighbors pk , after repairing .
Let yj = 1 denote that location pj has ε neighbor count no less than η , ie , core location ; otherwise , yj = 0 . It follows
≥ yj ≥ cj − η + 1
, cj η
( 4 ) where n = |P| is the total number of points . It specifies that yj = 1 iff cj ≥ η ; and yj = 0 iff cj < η . n
The repairing should ensure eliminating all noise points . In other words , a point is either a core point or a border point ( which is a neighbor of a core point ) . More precisely , for any location pj with at least one point retained after repairing , ie , xkj = 1 for some k , it is required that either this point or one of its neighbors belongs to core points ( with ε neighborhood size no less than η ) . We have n . i=1 yj + yihij ≥ 1 n n . k=1 xkj .
( 5 )
In other words , for any j with xkj = 1 for some k , it requires either yj = 1 or some other yi = 1 such that pi is in εneighborhood with pj ( hij = 1 ) .
Given the constraints in formulas ( 2 ) , ( 4 ) and ( 5 ) , the dorc problem is formulated as the following ilp problem . minimize subject to n . j=1 n . n . i=1 wijxij j=1 xij = 1 , cj − ηyj ≥ 0 , yjn − cj ≥ 1 − η , yihij − 1 n yj + xij , yj ∈ {0 , 1} n . i=1
1 ≤ i ≤ n 1 ≤ j ≤ n 1 ≤ j ≤ n 1 ≤ j ≤ n 1 ≤ j ≤ n
( 6 ) n . xkj ≥ 0 , 1 ≤ i ≤ n , k=1
Existing ilp solvers can be directly applied to compute the optimal solutions . It returns not only a repair xij but also a set of core pointsλ ( pi ) = pj with yj = 1 after repairing .
Proposition 1 . The optimal solution xilp , yilp of ilp forms an optimal repair λilp with the minimum repairing cost n . n . i=1 j=1
Δ(λilp ) = wijxilp ij ,
Figure 2 : Example of repairing clusters may force them to combined , eg , in Figure 1(a ) , C2 ( white points ) is either ignored as noises by C1 or merged as C1’ ( by relaxing the density parameter ) . We propose to utilize the dirty ( noise ) points for clustering by repairing . That is , the noise points are repaired and thus clustered as either core points or border points . In other words , for each repaired λ(pi ) , either itself or one of its ε neighbors has a εneighborhood size greater than MinPts η . Since a cluster is uniquely determined by its core points [ 7 ] , the repairing process with identification of core points ( in the repair results ) outputs the clustering results as well . Problem 1 . Given a set of data points P , a distance threshold ε and a density threshold η , the Density based Optimal Repairing and Clustering ( dorc ) problem is to find a repair λ ( ie , a mapping λ : P → P ) such that
( 1 ) the repairing cost Δ(λ ) is minimized , and ( 2 ) for each repaired λ(pi ) , either |Cλ(pi)| ≥ η ( core points ) , or |Cλ(pj)| ≥ η for some pj with δ(λ(pi ) , λ(pj ) ) ≤ ε .
It is worth noting that multiple points may be repaired to the same “ physical ” location , having λ(pi ) =λ ( pj ) .
Example 2 . Consider a clustering density requirement η = 3 . As shown in Figure 2(a ) , point p1 , whose |C ( p1)| = |{p1 , p2 , p4}| = 3 , is a core point . Points p2 and p4 in εneighborhood of p1 are border points . Point p3 , not in εneighborhood of any point , is considered as a noise point .
Figure 2(b ) illustrates a possible repair λ , where p3 is moved to the location of p2 , ie,λ ( p3 ) = p2 . Point p2 with λ(p2 ) = p2 remains unchanged ( and similarly for p1 , p4 ) . There are two points in the location of p2 after repairing ( denoted by red and black concentric circles ) . We have Cλ(p2 ) = Cλ(p3 ) = {p1 , p2 , p3} . That is , points p2 and p3 upgrade to core points by repairing .
3 .
ILP FORMULATION
In this section , we illustrate how to formulate the dorc problem as an ilp problem , and thus existing ilp solvers can be directly applied ( a built in advantage ) . Consider variable xij , 0 ≤ xij ≤ 1 . Let xij = 1 denote that point pi is repaired to location pj after repairing , ie , λ(pi ) = pj ; otherwise , xij = 0 . Obviously , a point can only be repaired to one location , having n . xij = 1 .
( 2 ) where λilp(pi ) = pj iff xilp ij = 1 , 1 ≤ i ≤ n , 1 ≤ j ≤ n . j=1
The weight wij for xij is defined as the corresponding cost of repairing pi to pj , wij = w ( pi , pj ) .
Example 3 ( Example 2 continued ) . Consider again the example of 4 points in Figure 2 with clustering density requirement η = 3 . We show how the repair λ , with λ(p3 ) = p2
1117 Figure 3 : Example of repairing with LP solution as the probability of a point pj being core point in clustering . In each step , we consider a point pj ∈ P with the largest ylp j in Line 4 . In order to become a core point , point pj needs at least ( 1 − yj)η additional ε neighbors . We heuristically consider the noise point pi with the minimum wij ( in Line 7 ) and conduct the repairing of pi to pj , ie , assign xij = 1 . If there are no sufficient noise points retained for repairing , ie , the remaining noise points cannot form at least η neighbors of the point pj , we repair all the remaining noise points to their closest non noise points in Line 13 .
Algorithm 1 : qdorc(P , h , ε , η ) Data : A set of data points P with neighborhood h , distance threshold ε and density threshold η
Result : A set of core locations with yj = 1 in y and the corresponding repairing xij in x
1 N := N ( h ) wrt h ; 2 x , y := xlp , ylp the lp solution wrt h in Lemma 3 ; 3 while N = ∅ do let pj ∈ P with the maximum yj and yj < 1 ; if |N| ≥ ( 1 − yj)η then
// noise points exist repeat let pi ∈ N with the minimum wij ; xii := 0 , xij := 1 and N := N \ {pi} ; until ( 1 − yj)η times ; yj := 1 and N := N \ ( {pj} ∪ {pk | hjk = 1} ) ; for each pi ∈ N do
// no sufficient noises retain let pk ∈ P \ N with the minimum wik ; xii := 0 , xik := 1 and N := N \ {pi} ; else
14 15 return y , x in Figure 2(b ) , corresponds to the feasible solution x to the ilp , where x11 = x22 = x44 = 1 and x32 = 1 .
It follows y2 = 1 according to formula ( 4 ) .
For the location of p2 , we have c2 = 2 + 1 = 3 by formula In other ( 3 ) . words , all the points in the location of p2 after repairing are core points , ie , p2 and p3 as indicated in Example 2 .
For the location of p3 , since there is no point retained k=1 xk3 = 0 , formula ( 5 ) is satisafter repairing , having fied . Therefore , the solution corresponding to the repair λ satisfies all the constraints in formula ( 6 ) and is a feasible solution of ilp . fi4
We can show that the dorc problem is always solvable .
Proposition 2 . For η < n , a feasible solution to the ilp problem always exists .
Proof . By simply repairing all the points to a single location say p1 , we have xi1 = 1 and y1 = 1 , ie , all the points become core points locating in p1 after repairing .
4 . QUADRATIC TIME APPROXIMATION Another advantage of modeling dorc as ilp is the possible lp relaxation for efficiently computing near optimal solutions . In this section , we first indicate that an optimal lp solution can be directly derived without calling a solver . An approximation algorithm is then built upon the lp solution . 4.1 LP Solution without Calling a Solver The ilp problem can be relaxed as a lp problem by changing the integer constraints in formula ( 6 ) to 0 ≤ xij ≤ 1 , 0 ≤ yj ≤ 1 . We show below that an optimal solution to the lp problem can be directly obtained without calling a solver .
Lemma 3 . There always exists an optimal solution xlp , ylp for the lp problem , where xlp ii = 1 , xlp ij = 0 ,
' ylp j = i = 1 , . . . , n fin i = j , i = 1 , . . . , n , j = 1 , . . . , n fin k=1 hjk ≥ η k=1 hjk < η k=1 hjk if if
η
1 .n
( 7 ) j = 1 , . . . , n
4.2 Clustering&Repairing with LP Solution
As mentioned , a cluster is uniquely determined by its core point [ 7 ] . The clustering and repairing process is thus to round the lp solution , ie , to determine xij ∈ {0 , 1} , yj ∈ {0 , 1} according to the aforesaid lp solution ( xlp , ylp ) .
Before introducing the algorithm , let us first rephrase core , border and noise points in Definition 1 in terms of neighborhood h .
Definition 2 ( Core points , Border points , Noise points ) . For a set of data points P and the corresponding neighborhood h , core points C(h ) , border points B(h ) , and noise fin points N ( h ) wrt h are fin j=1 hij ) ≥ η} , j=1 hij ) < η , hik = 1 , pk ∈ C(h)} ,
C(h ) = {pi ∈ P | ( B(h ) = {pi ∈ P | ( N ( h ) = P \ ( C(h ) ∪ B(h) ) . Algorithm 1 ( qdorc ) presents an approximation to dorc . We consider all the noise points N ( h ) wrt h , denoted by N in Line 1 . Note that ylp in the lp solution can be interpreted j
4
5
6
7
8
9
10
11
12
13
The returned result x corresponds to a repair λqdorc such that λqdorc(pi ) = pj for xij = 1 , 1 ≤ i ≤ n , 1 ≤ j ≤ n .
Example 4 . Consider an example of 4 points in Figure 3(a ) with clustering density requirement η = 3 . Each edge , eg , ( p1 , p2 ) , denotes that two points are in ε neighborhood . Point p3 is identified as a noise point , referring to Definition 2 wrt neighborhoods , having N = {p3} . According to the formulas in Lemma 3 , we derive an lp solution , with y1 = 1 , y2 = 2/3 , y3 = 1/3 , y4 = 2/3 .
Line 3 in Algorithm 1 selects a point p2 with the maximum y2 = 2/3 < 1 . To make p2 a core point , ie , y2 = 1 , we need at least one additional noise point , to repair to the location of p2 . Line 7 selects p3 ∈ N , and repair it to the location of p2 by setting x32 = 1 . Since no noise point retains in N , the algorithm terminates and returns a solution with x11 = x22 = x44 = 1 , x32 = 1 , y1 = y2 = 1 ( all the others are equal to 0 ) . It corresponds to a repair λ with λ(p3 ) = p2 ( and 3 other unchanged points )
1118 Figure 4 : Example of leaders and followers and a set of core points {λ(p1 ) , λ(p3 ) , λ(p3)} ( located in the core locations of p1 , p2 ) . Proposition 4 . Algorithm 1 ( qdorc ) runs in O(n2 ) time , returns a feasible solution to the ilp problem , where n = |P| .
5 . LINEAR TIME APPROXIMATION
It is worth noting that capturing the ε neighborhood h for qdorc is costly ( in O(n2 ) time ) . Following the same line of performing efficient density based clustering [ 23 ] , we present an algorithm for efficiently estimating the neighborhood by partitioning data points into groups , and perform dorc over the estimate neighborhood ( in linear time ) . 5.1 Estimating Neighborhood via Grouping Let τ be a distance threshold in grouping , 0 ≤ τ ≤ ε 2 . Each group consists of a leader data point p and a set of follower data points , denoted by follower(p ) . Each follower should have distance to its leader no greater than the group distance threshold τ , ie,δ ( p , pi ) ≤ τ,∀pi ∈ follower(p ) . Lines 2 8 in Algorithm 2 present the grouping procedure of generating leaders and followers . Let L denote the set of leaders . Each point p ∈ P is either assigned as a follower of some existing leader pl ∈ L ( in Line 8 if δ(pl , p ) ≤ τ ) , or created as a new leader ( in Line 5 ) .
We introduce approximate ε neighborhoods , hL for estimating h . As presented in Line 11 in Algorithm 2 , for two leaders pk , pl ∈ L , we assign h L := 1 if δ(pk , pl ) ≤ ε − 2τ rather than δ(pk , pl ) ≤ ε for hkl := 1 . For any pi ∈ follower(pk ) , pj ∈ follower(pl ) , as illustrated in Figure 4 , we use the neighborhood of leaders to approximate that of followers , by assigning h L ij = h L kl . Thereby , to count the numij for any follower pj ∈ follower(pl ) , ber of neighbors wrt h L it is equivalent to “ count ” the corresponding leaders . Lemma 5 . The neighbor count wrt h L ij of a point pj has kl n . m . h L ij = kl|follower(pk)| , h L i=1 k=1 where pi ∈ follower(pk ) , pj ∈ follower(pl ) , and m = |L| is the number of leaders .
When calling qdorc(P , hL , ε , η ) in Line 12 in Algorithm 2 ldorc , the noise points N = N ( hL ) in Line 1 and thelp solution x , y in Line 2 in Algorithm 1 qdorc can be computed by counting the aforesaid leaders ( in O(mn ) time ) .
For heuristically choosing repair candidates wrt weight w in Lines 7 and 13 of Algorithm 1 qdorc , we employ again the leaders . Similar to approximating the neighborhood of followers by that of leaders , we replace Line 7 of Algorithm 1 by choosing ap i ∈ follower(pk ) such that the weight wkl of leaders is minimized , where pj ∈ follower(pl ) , k = l . Example 5 . Consider the example of 4 points in Figure 5(a ) . Suppose that the points are processed in an order of p1 , p2 , p3 , p4 during grouping . Two groups are generated with leaders p1 and p3 . The followers are follower(p1 ) =
Algorithm 2 : ldorc(P , τ , ε , η ) Data : Data point set P , group distance threshold τ , cluster distance threshold ε , density threshold η
Result : A set of core locations with yj = 1 in y and the corresponding repairing xij in x
1 L := ∅ ; 2 for each point p ∈ P do find the first leader pl ∈ L st δ(pl , p ) ≤ τ ; if pl does not exist then
L = Lff{p} ; follower(p ) := {p} ;
// create a new leader
// for neighborhood estimation
3
4
5
6
7
10
11 ff{p} ; else
8 follower(pl ) := follower(pl ) 9 for each leader pair pk , pl ∈ L do if δ(pk , pl ) ≤ ε − 2τ then h L kl := 1 ;
12 return qdorc(P , hL , ε , η )
Figure 5 : Example of repairing with grouping i=1 h L
21 = h L
12 = h L fin
{p1 , p2} and follower(p3 ) = {p3 , p4} . Suppose that δ(p1 , p3 ) > ε − 2τ . The estimate neighborhoods are h L 34 = h L 43 = 1 ( all the others are equal to 0 ) . Algorithm 2 then calls qdorc(P , hL , ε , η ) in Line 12 to process the repairing . According to Lemma 5 , the number of neighbors wrt hL of a follower , say p2 , can be calculated by counting the sizes ( number of followers ) of all leaders with distance to p1 ( leader of p2 ) inε − 2τ , ie , i2 = |follower(p1)| = 2 . The same neighbor count is calculated for the other points . Given a clustering density threshold η = 4 and the aforesaid neighbor counts , all the 4 points are identified as noise points wrt hL , ie , N ( hL ) ={ p1 , p2 , p3 , p4} referring to Definition 2 . The lp solution has y1 = y2 = y3 = y4 = 2/4 . Suppose that p2 is first considered in Line 4 ( in Algorithm 1 ) when calling qdorc(P , hL , ε , η ) . Referring to y2 = 2/4 , it has to repair at least two noise points in order to make itself a core point . As introduced in the paragraph before Example 5 , we consider a point ( say p4 ) infollower ( p3 ) to repair , since p3 ( is the only leader that ) has the minimum weight w31 to the leader p1 of p2 . As illustrated in Figure 5(b ) , p4 is repaired to the location of p2 , having x42 = 1 and λ(p4 ) = p2 . The algorithm carries on by repairing p3 to the location of p2 , λ(p3 ) = p2 , in order to makey 2 = 1 . Since p2 upgrades to a core point , its neighbor p1 is removed from N as well . We have N = ∅ and the algorithm terminates .
Proposition 6 . Algorithm 2 ( ldorc ) runs in O(mn ) time , where n = |P| and m = |L| . 5.2 Performance Analysis hL contains false negatives , ie , when a h L ij = 0 indicates that the neighborhood does not exist between pi and pj
1119 ( the result is negative ) , but it is in fact present ( hij = 1 ) . Nevertheless , hL will never lead to false positives , where h L ij = 1 but hij = 0 .
Lemma 7 . For the neighborhood estimation for h , it always has h L ij ≤ hij .
In other words , the neighborhood hL is a subset of h . The returned result wrt hL must also be a feasible solution of ilp wrt h . Correctness of Algorithm 2 is guaranteed . Indeed , N ( hL ) considered for repairing in ldorc is a superset of N ( h ) for the original qdorc .
Lemma 8 . For the noise points wrt the neighborhood , we have N ( hL ) ⊇ N ( h ) .
We show that the number of leaders m is bounded by a constant wrt τ , given a finite domain of data instances ( ie , with a bounded maximum distance of two points ) . Proposition 9 . The number of leaders m = |L| is bounded τ + 1)2 , where δmax is the maximum distance by m < ( δmax between two points , and τ is the group distance threshold .
Combining Propositions 6 and 9 , Algorithm 2 ( ldorc ) τ + 1)2n ) time , ie , a linear time algorithm runs in O(( δmax in the number of data points n . When given a finite space of data instances , the maximum distance of two points δmax is a constant . However , for an infinite space , δmax could be arbitrarily large .
Trade off between Effectiveness and Efficiency Indeed , the parameter τ of group distance threshold provides a trade off between efficiency and effectiveness .
First , the larger ( closer to δmax ) the group distance threshold τ , the smaller the bound of m ( number of leaders ) is , ie , lower algorithm time cost . However , for an extremely large τ , eg , the largest τ = ε 2 , no neighborhood between leaders exist given ε − 2τ = 0 . That is , only the neighborhoods between the points in the same group can be identified in hL . Such a weak estimation leads to inaccurate computation of y in the lp solution , and thus the corresponding repairing might not be reliable ( ie , lower repairing and clustering accuracy as illustrated in Figure 10 of experiments ) .
On the other hand , for a small τ , eg,τ = 0 , it leads to single size groups , where each point corresponds to the leader of a group but without any other follower . In this sense , ldorc is exactly qdorc without grouping . Since the lp solution wrt accurate neighborhood is utilized , the algorithm effectiveness is high in this case . However , the corresponding time cost increases as grouping takes no effect .
6 . EXPERIMENTS
Experimental evaluation answers the following questions : ( 1 ) By utilizing dirty data , can it form more accurate clusters ? In Figure 6 , we illustrate an example of artificial data points on how the dirty points affect the clustering and how the simultaneous clustering and repairing work . Figures 7 , 8 and 11 compare quantitatively the clustering accuracy of our dorc to the existing dbscan ( directly discarding all noisy data ) on both synthetic and real data sets .
( 2 ) By simultaneous repairing and clustering , in practice is the repairing accuracy improved compared with the existing data repairing approaches ? Figures 9 and 12 compare our proposed dorc with the existing data repairing methods over real data sets .
( 3 ) How do the approaches scale ? In Figure 10 , we show that it is possible to trade effectiveness for efficiency in ldorc via the group distance threshold τ . Figure 13 reports the scalability over a large scale data set with up to 400k data points .
Our programs are implemented in Java and all experiments were performed on a PC with Intel(R ) Core(TM ) i72600 3.40GHz CPU and 8 GB memory .
Clustering Accuracy . To evaluate the clustering accuracy , we employ the purity measure [ 18 ] . It evaluates the most frequent class label of data points in each cluster , fi purity = 1 n i maxj |clusteri ∩ classj| , that is , counting the maximum number of data points in each cluster i corresponding to a class j . The higher the measure is , the better the clustering accuracy is .
Repairing Accuracy . The repair accuracy evaluates how close the repaired result λ(pi ) is compared to the true location truth(pi ) . We employ the repair error measure , rootmean square error ( RMS ) [ 13 ] , fin error =
1 n i=1 δ(truth(pi ) , λ(pi))2 .
The lower the repair error is , the more accurate the repair is ( closer to the ground truth ) . 6.1 Artificial Data Set
Example Results . To study the exact ilp and approximate dorc solutions , we draw a small synthetic dataset as shown in Figure 6 . For the clean data in Figure 6(a ) , there are two classes , C1 and C2 , with 164 data points . We introduce up to 34 artificial dirty points ( by moving points in particular areas to random locations in a certain radius ) . Clustering approaches are performed over the data with dirty points . Figure 6(b ) presents the clustering results by dbscan over the dirty data ( without repairing ) . Three clusters present and a number of 17 black points are identified as noises , ie , not belonging to any cluster . Figure 6(c ) reports the results by our proposed dorc of simultaneous repairing and clustering over the same dirty dataset . As illustrated in Figure 6(c ) , dorc can successfully repair the dirty points and return two clusters similar to the two classes in ground truth in Figure 6(a ) . These results verify our intuition that a large number of dirty points may greatly affect the clustering results ( Figures 6(a ) vs . 6(b) ) , while the clustering with repairing can address the variance introduced by dirty data ( Figures 6(a ) vs . 6(c) ) . The results illustrate that our proposal is not limited to splitting cluster ( in Figure 1 ) , but may also return merged clusters that are erroneously split by dbscan in Figure 6(b ) .
Quantitive Results . Figure 7 delivers the accuracies of clustering and repairing under various dirty rates over the synthetic dataset . A dirty rate 0.21 denotes that 21 % points are modified as dirty data . Besides dbscan , the results of another density based clustering , optics [ 2 ] , are also reported . As shown in Figure 7(a ) , it is not surprising that the clustering accuracy of all approaches drops with the increase of
1120 ( a ) Clusters in ground truth
( b ) Clusters in dirty data without repairing
( c ) Clusters in dirty data with repairing
C1 C2
C1 C2 C3 Noise
C1 C2
Figure 6 : Clusters in ( a ) synthetic clean data , ( b ) dirty data without repairing , ( c ) dirty data with repairing y t i r u p g n i r e t s u C l
1 0.98 0.96 0.94 0.92 0.9 0.88 0.86 0.84 0.82 0.8
( a ) ε=15 , η=6
DBSCAN ILP QDORC LDORC(τ/ε=1/5 ) LDORC(τ/ε=1/10 ) OPTICS
0.03 0.06 0.09 0.12 0.15 0.18 0.21
Dirty rate r o r r e g n i r i a p e R
5.5 5 4.5 4 3.5 3 2.5 2 1.5 1 0.5
( b ) ε=15 , η=6
ILP QDORC LDORC(τ/ε=1/5 ) LDORC(τ/ε=1/10 )
0.03 0.06 0.09 0.12 0.15 0.18 0.21
Dirty rate
I
M N
1
0.9
0.8
0.7
0.6
0.5
( c ) ε=15 , η=6
DBSCAN ILP QDORC LDORC(τ/ε=1/5 ) LDORC(τ/ε=1/10 ) OPTICS
0.03 0.06 0.09 0.12 0.15 0.18 0.21
Dirty rate
Figure 7 : ( a ) Clustering accuracy , ( b ) repairing accuracy , and ( c ) NMI accuracy , over synthetic data dirty rate . Most importantly , our dorc approaches can significantly improve the accuracy of clustering compared to dbscan , especially when the dirty rate is large . Moreover , the clustering purity of the approximation algorithm qdorc is comparable to that of the exact method ilp ( we fail to obtain ilp results in dirty rates greater than 0.18 owing to the extremely high time costs ) .
Figure 7(b ) reports repairing error . The exact method ilp has lower repairing error . The result verifies the rationale of considering the minimum cost repairs , following the minimum change principle in data repairing [ 24 ] . Nevertheless , the approximate qdorc approach is comparable with ilp , especially when the dirty rate is large . The reason behind is that with an extremely large amount of dirty data , the repairing ( even the exact one ) might not be precise .
6.2 Real GPS Data Set
For the real dataset , we collect1 818 GPS reading points in three nearby buildings , which correspond to three classes . Owing to the weak signal inside buildings , a large amount of GPS readings are inaccurate ( 139 points outside the buildings ) . We manually alter these ( originally embedded rather than randomly introduced ) dirty data points , ie , label the true building for each inaccurate point as the ground truth . Clustering approaches are conducted over the dirty dataset , where the truth class of each dirty point is labeled . We use Euclidean distance as the distance function δ on GPS points . Note that the GPS data are continuously collected in a time period . Filtering techniques can be applied to clean the noisy data in such a time space correlated time series [ 14 ] , eg , by the widely used Median Filter ( mf ) [ 22 ] . The main idea of mf is to go through the GPS readings one by one in the time series , repairing each point with the me
1Since existing datasets for evaluating clustering are not labeled for dirty data , we need to collect and manually label the ground truth of dirty data points ( with a great manual effort ) . dian of ( temporally ) neighboring points . Therefore , instead of simultaneous repairing and clustering , mf+dbscan first applies mf to clean the GPS data , and then performs the existing dbscan clustering over the mf pre processed data . Figures 8 and 9 present the clustering and repairing accuracy results with various dirty rates , density thresholds η and distance thresholds ε ( parameters η and ε are inherited from the density based clustering dbscan , see [ 7 ] for a discussion on determining such parameters ) . The results are generally similar to that on the synthetic data in Figure 7 . In addition , by applying mf , the clustering accuracy of dbscan is slightly improved . It verifies the motivation of this study , ie , utilizing the dirty data can enhance clustering .
With various clustering requirements of ε and η in Figures 8 and 9 , our proposed qdorc always shows aclear improvement . In particular , the clustering accuracy improvement by qdorc is more significant than that of mf+dbscan in Figures 8(b ) and 8(c ) . The result is not surprising given the significantly lower repairing error of qdorc compared to mf in the corresponding Figures 9(b ) and 9(c ) .
Figure 10 verifies the analysis at the end of Section 5 that τ in ldorc provides a trade off in efficiency and effectiveness . As shown , a large τ = 1 ε shows high efficiency ( low time 2 cost ) in Figure 10(c ) , while its clustering accuracy in Figure 10(a ) is low and the repairing error in Figure 10(b ) is high . On the other hand , a small τ leads to high time cost but lower repairing error and better clustering purity . When τ = 0 , the result of ldorc is exact the same as that of qdorc without grouping . The corresponding time cost ( of τ = 0 ) may be a bit higher since ldorc has extra cost on grouping . 6.3 Restaurant Data Set
Restaurant2 is a collection of 864 restaurant records that contains 112 duplicates and is widely used for record matching [ 21 ] . Each group of duplicates can be interpreted as a
2http://wwwcsutexasedu/users/ml/riddle/datahtml
1121 1 y t i r u p g n i r e t s u C l
0.96
0.92
0.88
0.84
0.8
0.02
( a ) ε=8E 5 , η=8
DBSCAN MF+DBSCAN QDORC MF+QDORC OPTICS
0.06
0.1
Dirty rate
0.14
0.18
1
0.98
0.96
0.94
0.92
0.9 y t i r u p g n i r e t s u C l
0.88
5E 5
( b ) η=8
DBSCAN MF+DBSCAN QDORC MF+QDORC
8E 5 Distance threshold ε
11E 5
14E 5
1 y t i r u p g n i r e t s u C l
0.98
0.96
0.94
0.92
0.9
5
6
( c ) ε=8E 5
DBSCAN MF+DBSCAN QDORC MF+QDORC
7
8
9
10 11 12 13 14
Density threshold η ( MinPts )
Figure 8 : Clustering accuracy under various ( a ) dirty rates , ( b ) ε , and ( c)η , over GPS data
( a ) ε=8E 5 , η=8
0.014
0.012
0.01
0.008
0.006
0.004 r o r r e g n i r i a p e R
0.002
0.02
0.06
0.1
Dirty rate
MF QDORC MF+QDORC
0.14
0.18
0.01
0.009
0.008
0.007
0.006
0.005 r o r r e g n i r i a p e R
0.004
5E 5
( b ) η=8
MF QDORC MF+QDORC
8E 5 Distance threshold ε
11E 5
14E 5 r o r r e g n i r i a p e R
0.01
0.009
0.008
0.007
0.006
0.005
0.004
5
6
( c ) ε=8E 5
MF QDORC MF+QDORC
7
8
9 10 11 12 13 14
Density threshold η ( MinPts )
Figure 9 : Repairing accuracy under various ( a ) dirty rates , ( b ) ε , and ( c)η , over GPS data y t i r u p g n i r e t s u C l
1 0.99 0.98 0.97 0.96 0.95 0.94 0.93 0.92
0
( a ) ε=8E 5 , η=8
QDORC LDORC
1/64 1/4 Group distance threshold τ/ε
1/16
1/6
1/2 r o r r e g n i r i a p e R
0.008 0.0075 0.007 0.0065 0.006 0.0055 0.005 0.0045 0.004
0
( b ) ε=8E 5 , η=8
( c ) ε=8E 5 , η=8
200
180
) s m
( t s o c
160
140
QDORC LDORC
1/64 Group distance threshold τ/ε
1/16
1/6
1/4
1/2
120 e m T i
100
80
0
QDORC LDORC
1/64 1/4 Group distance threshold τ/ε
1/16
1/6
1/2
Figure 10 : Trade off in LDORC via group distance threshold τ over GPS data class . A record includes four attributes , name , address , city and type . Edit distance [ 19 ] is employed as the distance function δ . Since the data are originally clean , following the same line of evaluating data repairing performance [ 3 ] , we introduce dirty values by randomly replacing values ( could be any data instead of one particular area ) in the data set , with various dirty rates . Different from filtering numerical values on GPS data set , integrity constraints ( such as fds name,address → city ) in databases are employed to clean the dirty data [ 3 ] . It is the reason why we employ this data set , where both the existing clustering and repairing techniques can be applied ( as rational baselines ) .
The results in Figures 11 and 12 are generally similar to Figures 8 and 9 over GPS data . Figure 11 shows that by applying fd based repairing first , the clustering accuracy is improved by fd+dbscan . Our proposed qdorc achieves significantly higher clustering and repairing accuracies . Similar results on various η and ε are also observed .
Most importantly , by combining our qdorc with the existing fd constraint based repairing , ie , fd+qdorc , both the clustering purity and repairing error are further improved compared to the fd(+dbscan ) approach . The results demonstrate that the proposed dorc is complementary to the existing constraint based repairing , by directly applying qdorc over the data repaired by fd .
6.4 Foursquare Data Set
The experiment on Foursquare dataset focuses on scalability over large data sizes , up to 400k check in data points . Since this large scale data set is not pre labeled , we mainly observe the time cost .
First , as shown in Figure 13(a ) , while the number of noise points increases as the data size , the number of leaders keeps low . It verifies the result in Proposition 9 that the number of leaders m is bounded . Consequently , the corresponding time cost of ldorc increases linearly , in Figure 13(b ) . The result verifies the linear time complexity of the ldorc algorithm in Proposition 6 . Finally , Figure 13(c ) reports a result similar to Figure 10(c ) over GPS data that with the increase of τ , the efficiency is improved .
6.5 UCI Data Set
Finally , in order to show that the proposed approach improves the clustering accuracy on real data , we report experiments on two labeled publicly available benchmark data , Iris and Ecoli , from UCI3 . Moreover , a state of the art evaluation measure , normalized mutual information ( NMI ) [ 20 ] , is employed for clustering validation . As shown in Figure 14 , similar results are generally observed , ie , our qdorc still
3http://archiveicsuciedu/ml/datasetshtml
1122 0.95
0.9
0.85
0.8
0.75 y t i r u p g n i r e t s u C l
0.7
0.1
0.14
( a ) ε=0.22 , η=16
DBSCAN FD+DBSCAN QDORC FD+QDORC
0.95
0.9
0.85
0.8
0.75 y t i r u p g n i r e t s u C l
0.18
0.22
Dirty rate
0.26
0.3
0.7
0.14
( b ) η=16
DBSCAN FD+DBSCAN QDORC FD+QDORC
0.18
0.22
0.26
0.30
0.50
Distance threshold ε y t i r u p g n i r e t s u C l
0.95
0.9
0.85
0.8
0.75
0.7
( c ) ε=0.22
DBSCAN FD+DBSCAN QDORC FD+QDORC
14 16 18 20 22 24 26 28 30 32
Density threshold η ( MinPts )
Figure 11 : Clustering accuracy under various ( a ) dirty rates , ( b ) ε , and ( c)η , over Restaurant r o r r e g n i r i a p e R
0.3 0.29 0.28 0.27 0.26 0.25 0.24 0.23 0.22 0.21 0.2
0.1
0.14
( a ) ε=0.22 , η=16
FD QDORC FD+QDORC
0.26
0.3
0.18
0.22
Dirty rate
0.28
0.27
0.26
0.25
0.24 r o r r e g n i r i a p e R
0.23
0.14
( b ) η=16
FD QDORC FD+QDORC
0.18
0.22
0.26
0.30
0.50
Distance threshold ε r o r r e g n i r i a p e R
0.28
0.27
0.26
0.25
0.24
0.23
( c ) ε=0.22
FD QDORC FD+QDORC
14 16 18 20 22 24 26 28 30 32
Density threshold η ( MinPts )
Figure 12 : Repairing accuracy under various ( a ) dirty rates , ( b ) ε , and ( c)η , over Restaurant s t i n o p f o
#
70000 60000 50000 40000 30000 20000 10000 0
( a ) ε=0.06 η=500 τ=0.006
# of noises # of leaders
100k
200k Data size
300k
400k
) s ( t s o c e m T i
3500 3000 2500 2000 1500 1000 500 0
( b ) ε=0.06 , η=500 , τ=0.006
( c ) ε=0.06 , η=500 , data size = 100k
QDORC LDORC
100k
200k Data size
300k
400k
) s ( t s o c e m T i
140 120 100 80 60 40 20 0 1/30
QDORC LDORC
1/18
1/12
Group distance threshold τ/ε
1/6
Figure 13 : Scalability over Foursquare data
1
0.9
0.8
0.7
0.6
I
M N
0.5
0.02
( a ) GPS
DBSCAN MF+DBSCAN QDORC MF+QDORC OPTICS
0.06
0.1
Dirty rate
0.14
0.18
I
M N
1
0.8
0.6
0.4
0.2
0
( b ) UCI
DBSCAN OPTICS QDORC
Iris
Ecoli
Dataset
Figure 14 : NMI clustering accuracy on real datasets shows much higher ( NMI ) accuracy ( compared to dbscan and optics ) .
7 . RELATED WORK putation [ 6 ] of dbscan has been devised . In this study , we also follow the settings of Eps ε and MinPts η . optics [ 2 ] produces a cluster ordering of data points with various Eps levels , and maintains a sorting of core points . In contrast , we rank non core points in this paper , according to their likelihood of being core points after repairing . denclue [ 11 ] generalizes the notation of density clusters by introducing the concept of influence functions . The influence function models the influence of a point to its neighbors , eg , by square wave functions or Gaussian functions . fdbscan [ 17 ] considers the extension over fuzzy/uncertain data points . Rather than pruning outliers , Gupta and Ghosh [ 9 ] identify dense regions of subsets of points . Xiong et al . [ 25 ] study clustering anomaly , where clusters are formed by a ( very ) small portion of points in a large data set . Again , all these studies focus on identifying noise/non noise points , while the large amount of dirty data ( identified as noises ) are still not employed to form clusters . In contrast , our proposal considers cleaning and clustering with dirty data .
Clustering . Density based clustering has proved to be useful in various applications ( see [ 16 ] for a survey ) . Given a distance threshold Eps ε and a minimum requirement of neighborhood MinPts η , dbscan [ 7 ] determines the density region of core points as well as the corresponding border points . Efficient implementation [ 23 ] and incremental com
While outlier detection ( see [ 12 ] for a survey ) identifies dirty points , data repairing further modifies the points for correction . Indeed , our proposal incorporates density based dbscan ( that also identifies noises and could be regarded as an outlier detection method ) . In this sense , data repairing and outlier detection are complementary .
1123 Repairing . Besides the minimum modification model [ 24 ] , which is also adopted in our study , a deletion model [ 5 ] is often considered . The deletion model finds the minimum removal of dirty data . In this sense , dbscan is also a deletionbased cleaning technique that removes dirty data . As discussed , simply ignoring the large amount of dirty data as noises by the existing clustering approaches is not rational and may affect greatly the clustering results .
In addition to the widely used fd constraints , other types of data quality rules are employed ( see [ 8 ] for a review ) . Again , as discussed in the introduction , our study considers only the density information embedded in data rather than the external knowledge of constraints or rules . Most importantly , following the same line of combining with fd based repairing ( in Section 6.3 ) , our proposal is complementary to these state of the art techniques , whenever extra information such as master data or constraint rules are available .
8 . CONCLUSION
Preliminary density based clustering can successfully identify noisy data but without cleaning them . On the other side , existing constraint based repairing relies on external constraint knowledge without utilizing the density information embedded inside the data . In this paper , inspired by the aforesaid victory and defeat , we study a novel problem of clustering and repairing dirty data at the same time . To the best of our knowledge , this is the first study on enhancing clustering by repairing and utilizing dirty data . With the happy marriage of clustering and repairing advantages , both the clustering and repairing accuracies are significantly improved as presented in the experimental evaluation .
To tackle the dorc problem of simultaneous clustering and repairing , our major technique contributions include : ( 1 ) the formulation of dorc as an ilp problem ; ( 2 ) an optimal solution to the corresponding lp relaxation that can be directly obtained without calling lp solvers ; ( 3 ) a quadratic time approximation algorithm devised upon the lp solution ; and ( 4 ) a linear time improvement via grouping data points . In particular , the linear time algorithm provides a trade off between effectiveness and efficiency . Acknowledgement This work is supported in part by the Tsinghua University Initiative Scientific Research Program ; China NSFC under Grants 61202008 , 91218302 and 61370055 ; National Grand Fundamental Research 973 Program of China under Grant 2012 CB316200 ; Huawei Innovation Research Program .
9 . REFERENCES [ 1 ] Full Version . http://isethsstsinghuaeducn/sxsong/doc/ccpdf
[ 2 ] M . Ankerst , M . M . Breunig , H P Kriegel , and
J . Sander . Optics : Ordering points to identify the clustering structure . In SIGMOD Conference , pages 49–60 , 1999 .
[ 3 ] P . Bohannon , M . Flaster , W . Fan , and R . Rastogi . A cost based model and effective heuristic for repairing constraints by value modification . In SIGMOD Conference , pages 143–154 , 2005 .
[ 4 ] H . Chen , W S Ku , H . Wang , and M T Sun .
Leveraging spatio temporal redundancy for rfid data cleansing . In SIGMOD Conference , pages 51–62 , 2010 .
[ 5 ] J . Chomicki and J . Marcinkowski . Minimal change integrity maintenance using tuple deletions . Inf . Comput . , 197(1 2):90–121 , 2005 .
[ 6 ] M . Ester , H P Kriegel , J . Sander , M . Wimmer , and
X . Xu . Incremental clustering for mining in a data warehousing environment . In VLDB , pages 323–333 , 1998 .
[ 7 ] M . Ester , H P Kriegel , J . Sander , and X . Xu . A density based algorithm for discovering clusters in large spatial databases with noise . In KDD , pages 226–231 , 1996 .
[ 8 ] W . Fan . Dependencies revisited for improving data quality . In PODS , pages 159–170 , 2008 .
[ 9 ] G . Gupta and J . Ghosh . Robust one class clustering using hybrid global and local search . In ICML , pages 273–280 , 2005 .
[ 10 ] M . A . Hern´andez and S . J . Stolfo . Real world data is dirty : Data cleansing and the merge/purge problem . Data Min . Knowl . Discov . , 2(1):9–37 , 1998 .
[ 11 ] A . Hinneburg and D . A . Keim . An efficient approach to clustering in large multimedia databases with noise . In KDD , pages 58–65 , 1998 .
[ 12 ] V . J . Hodge and J . Austin . A survey of outlier detection methodologies . Artif . Intell . Rev . , 22(2):85–126 , 2004 .
[ 13 ] S . R . Jeffery , M . N . Garofalakis , and M . J . Franklin .
Adaptive cleaning for rfid data streams . In VLDB , pages 163–174 , 2006 .
[ 14 ] K . H . Ji and T . A . Herring . A method for detecting transient signals in gps position time series : smoothing and principal component analysis . Geophysical Journal International , 193(1):171–186 , 2013 .
[ 15 ] S . Kolahi and L . V . S . Lakshmanan . On approximating optimum repairs for functional dependency violations . In ICDT , pages 53–62 , 2009 .
[ 16 ] H P Kriegel , P . Kr¨oger , J . Sander , and A . Zimek .
Density based clustering . Wiley Interdisc . Rew . : Data Mining and Knowledge Discovery , 1(3):231–240 , 2011 . [ 17 ] H P Kriegel and M . Pfeifle . Density based clustering of uncertain data . In KDD , pages 672–677 , 2005 .
[ 18 ] C . D . Manning , P . Raghavan , and H . Sch¨utze .
Introduction to information retrieval . Cambridge University Press , 2008 .
[ 19 ] G . Navarro . A guided tour to approximate string matching . ACM Comput . Surv . , 33(1):31–88 , 2001 . [ 20 ] X . V . Nguyen , J . Epps , and J . Bailey . Information theoretic measures for clusterings comparison : Variants , properties , normalization and correction for chance . Journal of Machine Learning Research , 11:2837–2854 , 2010 .
[ 21 ] P . D . Ravikumar and W . W . Cohen . A hierarchical graphical model for record linkage . In UAI , pages 454–461 , 2004 .
[ 22 ] J . W . Tukey . Exploratory data analysis . 1977 . [ 23 ] P . Viswanath and V . Suresh Babu . Rough dbscan : A fast hybrid density based clustering method for large data sets . Pattern Recognition Letters , 30(16):1477–1488 , 2009 .
[ 24 ] J . Wijsen . Database repairing using updates . ACM
Trans . Database Syst . , 30(3):722–768 , 2005 .
[ 25 ] Y . Xiong , Y . Zhu , P . S . Yu , and J . Pei . Towards cohesive anomaly mining . In AAAI , 2013 .
1124
