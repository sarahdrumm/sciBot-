Facets : Fast Comprehensive Mining of Coevolving
High order Time Series
Yongjie Cai
The Graduate Center , CUNY ycai@gradcentercunyedu
Hanghang Tong
Arizona State University hanghangtong@asuedu
Wei Fan
Big Data Labs Baidu USA fanwei03@baidu.com
Ping Ji
The Graduate Center , CUNY pji@jjaycunyedu
ABSTRACT Mining time series data has been a very active research area in the past decade , exactly because of its prevalence in many high impact applications , ranging from environmental monitoring , intelligent transportation systems , computer network forensics , to smart buildings and many more . It has posed many fascinating research questions . Among others , three prominent challenges shared by a variety of real applications are ( a ) high order ; ( b ) contextual constraints and ( c ) temporal smoothness . The state of the art mining algorithms are rich in addressing each of these challenges , but relatively short of comprehensiveness in attacking the coexistence of multiple or even all of these three challenges .
In this paper , we propose a comprehensive method , Facets , to simultaneously model all these three challenges . We formulate it as an optimization problem from a dynamic graphical model perspective . The key idea is to use tensor factorization to address multi aspect challenges , and perform careful regularizations to attack both contextual and temporal challenges . Based on that , we propose an effective and scalable algorithm to solve the problem . Our experimental evaluations on three real datasets demonstrate that our method ( 1 ) outperforms its competitors in two common data mining tasks ( imputation and prediction ) ; and ( 2 ) enjoys a linear scalability wrt the length of time series .
Categories and Subject Descriptors H28 [ Database Management ] : Database Applications— Data mining
Keywords A network of time series ; tensor factorization 1 .
INTRODUCTION
Mining time series data has been a very active research area in the past decade , exactly because of its prevalence in
Qing He
University at Buffalo , SUNY qinghe@buffalo.edu
Figure 1 : An illustrative example of high order time series with rich contextual networks . ( Best viewed in color ) . many high impact applications , ranging from environmental monitoring , intelligent transportation systems , computer network forensics , to smart buildings and many more . It has posed many fascinating research questions . We identify and summarize three prominent challenges as follows :
C1 . High order1 . Multiple time series arising from real applications are often collected from multiple locations with multiple types ( See Fig 1(a ) for an example ) . Yet , many classic time series analysis tools , such as Kalman filtering , often fall short in modeling such multiaspect , high order time series data .
C2 . Contextual constraints . Many real time series data is accompanied by contextual information ( eg , the sensor network in Fig 1(b) ) . How to effectively leverage such contextual information remains an open question for time series data mining .
C3 . Temporal smoothness . This refers to the correlation among the adjacent observations along the temporal dimension ( eg , the smooth curves in Fig 1(b)(c) ) . Despite its key importance for some data mining tasks ( eg , imputation and prediction ) , temporal smoothness is often ignored in certain time series mining algorithms ( eg , the standard matrix/tensor decomposition ) , which have been increasingly attracting attention in the recent years .
1aka multivariate in statistics . timesensortype010002000300040005000600014161820222426timetemperature0100020003000400050006000−2−101234timeNormalized Readings TemperatureLightHumidityVoltage 0.8 0.4 0.4 0.7 0.4 0.6 ( b)asensor timesliceintemperaturedimension(c)atype timeslicefromonesensor(a)79 An example of time series that exposes these three challenges is illustrated in Fig 1 . The time series data are generated from a set of sensors deployed in a smart building . Each sensor generates multiple time series by measuring certain types of room condition ( eg , temperature , humidity , etc . ) over time . As shown in Fig 1(a ) , the time series data can be viewed as a cube , in which sensor , measure type , and time step are represented in each dimension , respectively ( high order ) . As shown in Fig 1(b ) , by selecting one measurement type ( eg , temperature in the figure ) , we obtain a sensor time slice that consists of multiple temperature time series from the sensor network . The weight labeled on the network edge indicates the similarity between connected nodes . We use the same color to represent time series and its corresponding network node . It is clear that the time series in the sensor time slice are connected with each other by the underlying sensor network ( contextual constraints ) . Similarly , if we extract a type time slice from one sensor , we can also find the time series of multiple types are essentially connected by the type network shown in Fig 1(c ) . Specifically , temperature , light , voltage have similar daily patterns , while humidity shows a trend inverse to those of others . Finally , we can easily observe temporal smoothness from time series figures .
The state of the art mining algorithms are rich in dealing with each of the aforementioned challenges , but relatively short of comprehensiveness in overcoming the coexistence of multiple or all of the challenges . For example , dynaMMo [ 17 ] applies linear dynamic systems after interpolation to learn the temporal dynamics and improve the accuracy of estimating the missing values . Dynamic Tensor Analysis ( DTA ) provides a compact summary for high order and high dimensional data [ 25 ] . Note that neither dynaMMo ( as well as its high order generalization [ 23 ] ) nor DTA encodes the contextual constraints ( eg , the sensor network in a smart building ) . On the other hand , the dynamic contextual matrix factorization ( DCMF ) algorithm [ 5 ] encodes both contextual information and temporal dynamics , but falls short in modeling the high order of time series data .
In this paper , we propose a method of fast comprehensive mining of coevolving high order time series ( Facets ) . It formulates high order time series as tensors and adopts the tensor decomposition model to find the latent factors of time series data . By encoding the contextual constraints , Facets finds similar latent factors from similar time series . It further encodes temporal smoothness with multilinear dynamic systems .
The main contributions of this paper can be summarized as follows .
• Problem Definition . To the best of our knowledge , we are the first to collectively address C1 C3 on mining high order time series data .
• Algorithms and Analysis . We propose a comprehensive model to solve all of the three challenges , and propose an effective and scalable algorithm that naturally fits in the tasks of imputation and prediction ; and analyze its effectiveness and complexity .
• Empirical Evaluations . The experimental evaluations on real datasets demonstrate that our method ( 1 ) outperforms its competitors in two common data mining tasks ( ie , imputation and prediction ) ; and ( 2 )
Table 1 : Symbols and Definitions .
Aj , A.j Ai . A a , b , fi ⊗
X Xt W S Z B U V
Symbol Definition and Description A , tensors ( calligraphic style ) matrices ( bold upper case ) A , the element at the ith row and the jth colAij umn of matrix A the jth column of matrix A the ith row of matrix A transpose of matrix A column vectors ( bold lower case ) element wise multiplication kronecker product tensor product time series tensor with all time steps time series tensor at tth time step indicator tensor contextual matrix set {S(1 ) , , S(M )} time series latent variable transition variable coefficient latent variable contextual latent matrix set {V(1 ) , , V(M )} order of Xt M length of time series T dimensions of X or W on mode m Nm Lm dimensions of latent variables on mode m vectorization of tensor X vec(X ) mat(X ) matricization of tensor X mode n matricization of tensor X X(n ) enjoys a linear scalability wrt the length of time series .
The rest of this paper is organized as follows : In Section 2 , we introduce the notations and formally define the problem . Then we briefly introduce the background knowledge in Section 3 . We present the proposed solution and its analysis in Section 4 , and provide the experimental results in Section 5 . The related work is reviewed in Section 6 , followed by the conclusions in Section 7 .
2 . PROBLEM DEFINITION
Table 1 lists the main symbols we use throughout this paper . Besides the standard notations , we use an ( M + 1)order2 tensor X ∈ RN1××NM ×T to denote time series , where Nm(1 ≤ m ≤ M ) is the dimensionality of the mth mode and the last mode of the time series tensor represents the temporal mode with T dimensions ( ie , the time series has T time steps ) . We can also rewrite the time series tensor as a sequence of M order tensors X1,X2,XT , where each Xt ∈ RN1××NM ( 1 ≤ t ≤ T ) denotes the observed data at tth time step . We use an indicator tensor W ∈ RN1××NM ×T to indicate whether a single entry in X is observed or missing . Specifically , Wn1nM t = 0 if Xn1nM t is missing , otherwise Wn1nM t = 1 . Besides , we have a set of contextual matrices S = {S(1 ) , S(2 ) , , S(M )} , where each S(m ) ∈ RNm×Nm ( 1 ≤ m ≤ M ) represents the contextual network of X ’s mth mode and each entry of S(m ) 2In this paper , order and mode are interchangeable .
80 indicates the correlations between the corresponding two dimensions in the mth mode .
With the above notations , we generalize the concept of a Network of Time Series , which was first introduced in [ 5 ] , and formally define A Network of High order Time Series ( Net HiTs ) as follows :
Definition 21 A Network of High order Time Series ( NetHiTs ) . A Network of High order Time Series is defined as a quadruplet R = X ,W,S , ζ , where X ∈ RN1××NM ×T is a partially observed ( M + 1) order time series tensor ; W is an indicator tensor in the same size with X ( W ∈ RN1××NM ×T ) , T in both tensors represents the dimensionality of the time mode ; S contains a set of contextual matrices , which represent the correlations between any two dimensions in each mode of X ; and ζ is a one to one mapping function , which maps each dimension of the time series X to a node in S .
Accordingly , the problem of time series missing value re covery and prediction can be defined as follows :
Problem 21 Net HiTs Missing Value Recovery . Given : a network of high order time series R = X ,W,S , ζ ; Recover : its missing parts indicated by the indicator tensor W .
Problem 22 Net HiTs Prediction . Given : a network of high order time series R = X ,W,S , ζ , and the time step t to predict ; Predict : t time steps after X .
3 . PRELIMINARIES
In this section , we briefly introduce some definitions and lemmas in multilinear algebra ( aka tensor algebra or multilinear analysis ) from tensor related literatures [ 13 , 23 ] .
Definition 31 Vectorization . The vectorization of an M order tensor X ∈ RN1××NM vec(X ) is obtained by iterating elements of X . vec(X ) ∈ RN1NM .
The ordering of the elements does not matter as long as it is consistent . In this paper , we follow Matlab linear indexing with multidimensional arrays to index the elements . Specifically , the jth element of vec(X ) is given by vec(X )j =
Xn1,,nM , where j = 1 +M k=1(nk − 1)k−1 m=1 Nm .
Definition 32 Matricization . Let the ordered sets R = {r1 , , rL} and C = {c1 , , cM−L} be a partitioning of the modes {1,2,,M} , the matricization of a tensor X ∈ RN1××NM can be specified by X(R×C:N1××NM ) ∈ RJ×K with J = n∈C The indices in R and C are mapped to the rows and the = i=1 Nri , and k = columns , respectively . Specifically,,X(R×C:N1××NM ) xn1n2nM , where j = 1 +L 1 +M−L m=1 ( ncm − 1)m−1 l=1(nrl − 1)l−1
Nn and K = i=1 Nci n∈R
Nn . jk
Given a tensor X ∈ RN1××NM ×L1××LM , we partition the first half of modes {N1 , , NM} as rows and the second half {L1 , , LM} as columns . The element of the matricization mat(X ) is given by mat(X )ij = Xn1,,nM ,l1,,lM , m=1 Lm . k=1(lk− k=1(nk−1)k−1 m=1 Nm , and j = 1+M where i = 1+M 1)k−1 A special case is mode n matricizing , which happens when RN1××NM , the mode n matricizing X(n ) ∈ RNn× R is a singleton . For example , given an M order tensor X ∈ i=n Ni , ie , R = {n},C = {1 , 2 , n − 1 , n + 1 , , M} . Definition 33 Product or Contracted Product . Given two tensors U ∈ RN1××NM ×L1××LM and Z ∈ RL1××LM , the product or the contracted product X = UZ Un1,,nM ,l1,,lMZl1,,lM , is given by Xn1,,nM =
X ∈ RN1××NM . Definition 34 Tensor Factorization . Given a tensor U ∈ RN1××NM ×L1××LM , the factorization of U is to decompose it into M factor matrices {U(m ) ∈ RNm×Lm}M . It can also be written as mat(U ) = U(M ) ⊗ U(M−1 ) ⊗ ⊗ U(1 ) , where ⊗ denotes the Kronecker product . m=1 , so that Un1,,nM ,l1,,lM = M m=1 U(m ) l1,,lM nmlm
With the above definitions , we can easily prove the fol lowing two lemmas . Lemma 31 Given two tensors U ∈ RN1××NM ×L1××LM and Z ∈ RL1××LM , let X = U Z , then vec(X ) = mat(U ) vec(Z ) . If U is factorizable with matrices {U(m)}M m=1 , then vec(X ) = [ U(M ) ⊗ U(M−1 ) ⊗ ⊗ U(1 ) ] vec(Z ) . Lemma 32 Given two tensors U ∈ RN1××NM ×L1××LM , Z ∈ RL1××LM , and mat(U ) = U(M ) ⊗ U(M−1 ) ⊗ ⊗ U(1 )
X = U Z ⇔
X(n ) = U(n)Z(n)(U(M ) ⊗ ⊗ U(n+1 ) ⊗ U(n−1 ) ⊗ ⊗ U(1 ) )
In addition , we introduce a lemma of matrix normal distribution [ 9 ] and the definition of tensor normal distribution [ 23 ] :
Lemma 33 Matrix Normal Distribution . Given a matrix X ∈ RN×P , X follows the matrix normal distribution MN ( M , U , V ) if and only if vec(X ) ∼ N ( vec(M ) , V ⊗ U ) , where M ∈ RN×P , U ∈ RN×N , V ∈ RP×P .
Definition 35 Tensor Normal Distribution . Given a tensor X ∈ RN1××NM , X follows tensor normal distribution N ( U,D ) if vec(X ) ∼ N ( vec(U ) , mat(D) ) , where U ∈ RN1××NM and D ∈ RN1××NM ×N1××NM .
4 . PROPOSED APPROACH : FACETS
In this section , we present our proposed Facets for fast comprehensive mining of coevolving high order time series . We give the formal formulation of the model and then provide the detailed algorithm to learn the model . 4.1 Our Optimization Formulation
In order to collectively address all the three challenges outlined in the introduction , we present a regularization optimization formulation from a dynamic graphical model perspective . Step 1 Addressing both C1 and C2 . Facets adopts the tensor decomposition model to find the latent factors for the input high order ( C1 ) time series data . It further encodes the contextual information ( C2 ) to encourage the similar time series to share similar latent factors .
81 Figure 2 : Step 1 Addressing both C1 and C2
Formally , let X1 , X2 , XT be an M order time series , and S = {S(1 ) , S(2 ) , , S(M )} be a set of contextual matrices , where each S(m ) represents the contextual network of X ’s mth mode , we define the conditional distribution of Xt to be a multilinear Gaussian distribution with the covariance D and the mean as the product of two latent factors U and Zt , shown in Eq ( 1 ) . U is further factorized into M factor matrices U(1 ) , , U(M−1 ) , U(M ) , shown in Eq ( 2 ) . Each U(m ) indicates the time series similarity among the dimensions in the mth mode . Then , we define the conditional distribution of the jth column in the mth contextual matrix S(m ) as a linear Gaussian distribution with the covariance Ξ(m ) and the mean as the product of the latent factors U(m ) and V(m ) , presented in Eq ( 3 ) . In Eq ( 4 ) , we define zero mean spherical Gaussian priors on V(m ) , with entries in S(m ) scaled to [ −1 , 1 ] . j j
Xt|Zt,U ∼ N ( U Zt,D ) , mat(U ) = U(M ) ⊗ U(M−1 ) ⊗ ⊗ U(1 ) ,
, U(m ) ∼ N
, Ξ(m )
,
U(m)V(m ) j
S(m ) j
|V(m ) j j ∼ N ( 0 , Γ(m) ) , V(m ) j = 1 , , Nm;m = 1 , , M .
( 1 )
( 2 )
( 3 )
( 4 )
Fig 2 illustrates this step in the case of M = 3 , where we omit V(m ) for clarity . Step 2 Addressing C3 . Facets encodes temporal smoothness ( C3 ) with multilinear dynamical systems [ 23 ] . Specifically , we define the conditional distribution of the latent factor Zt as a multilinear Gaussian distribution with a multilinear transition tensor B and the covariance O , shown in Eq ( 6 ) . B is also defined as factorizable , formulated in Eq ( 7 ) . As defined in Eq ( 5 ) , the initial state of Z1 is generated based on the tensor normal distribution with the mean Z0 and the covariance O0 . Z1 ∼ N ( Z0,O0 ) ,
( 5 )
Zt|Zt−1 ∼ N ( B Zt−1,O ) , mat(B ) = B(M ) ⊗ B(M−1 ) ⊗ ⊗ B(1 ) .
( 6 )
( 7 )
In addition , since missing values in time series exist in many applications , we modify Eq ( 1 ) as follows :
X ∗ t |Zt,U ∼ N ( U∗ Zt,D∗
( 8 ) where X ∗ t represents observed entries of Xt ( ie , the corresponding entries in Wt equal 1. ) , which is a subset of Xt . U∗,D∗ are the subsets of U and D , corresponding to the entries of X ∗ t .
) , argmax
U ,Z,{V(m)},θ p(X ,S,U,Z,{V(m)} ) = argmax
U ,Z,{V(m)},θ p(Z1 )
T t=1 p(Xt|Zt,U )
× M
Nm m=1 j=1 p(V(m ) j
)
Nm j=1
M m=1 p(S(m ) j
|V(m ) j
, U(m ) ) ,
( 9 )
T t=2 p(Zt|Zt−1 )
C3 . temporal smoothness
Figure 3 : Graphical Model of Facets . The blue solid box is a plate representation of M instances , of which only a single instance is shown explicitly . The dashed rectangle indicates that V(m ) and S(m ) can be ignored in the model .
Our goal is to estimate the model parameters θ = {B,Z0 , O0,O,D,{Ξ(m)},{Γ(m)}} and find the latent factors U,Z , {V(m)}M m=1 that maximize the following joint distribution :
C1 . tensor time series where we omit the model parameters in the equation .
C2 . contextual information
The complete graphical model of Facets is shown in Fig 3 . The dashed rectangle means that V(m ) and S(m ) can be ignored if the contextual matrix S(m ) is unavailable or inapplicable . In this model , we include a contextual weight vector λ ∈ RM to control the contributions of contextual matrices . The contextual matrix of the mth mode is ignored if λm is set to zero .
In order to accommodate sparse time series datasets with our model , we simplify the covariances by assuming that the transition noise and the observation noise are independent and identically distributed ( iid ) , and the covariances } are iid Thus , the covariances of latent variable {V(m ) Ξ(m),D,O,O0 and Γ(m ) are reduced to ( ξ(m))2 , σ2 O , σ2 0 and σ2
V m , respectively .
R , σ2 j
4.2 Proposed Optimization Algorithm It is difficult to find the global optimal solution of Eq ( 9 ) due to three couplings in the model : a ) the latent factors U and Z jointly determine X , and U(m ) and V(m ) jointly determine S(m ) ; b ) both the parameters and the latent factors are unknown ; c ) U is determined by U(1 ) , U(2) , , and U(M ) jointly , and so is B . Hence , we aim to find its local optimum instead following the expectation maximization ( E M ) algorithm . Specifically , Facets employs the following steps to address the aforementioned difficulties : a ) it regards U as a model parameter ; b ) Facets searches a local optimal solution by updating the parameters θ and the expectations of Z and V(m ) alternatively ; c ) Facets iteratively updates U(m ) and B(m ) when keeping other factors of U and B fixed .
ZtXtU(1)U(3)U(2)N1N2N3N1L1N2L2N3L3L1L2L3S(1)S(3)S(2)N1N3N2N1N3N2Z1Z2Zt 1ZtZt+1X1X2Xt 1XtXt+1MU(m)S(m)V(m)82 Inferring latent factors Z and {V(m)} . With the fixed parameters , we first perform the vectorizations and matricizations in Eq ( 10 ) . With Xt and Zt reshaped into vectors , we apply the forward and backward algorithms to obtain the expectations of the latent factors as in the DCMF algorithm [ 5 ] . The details are presented in Appendix A . where F = ( B(M ) ⊗ ⊗ B(m+1 ) ⊗ B(m−1)⊗ B(1) ) , Zt,(m ) denotes mode m matricizing of Zt and I/I denotes the identity tensor/matrix . Then , we can replace p(Zt|Zt−1 ) with p(Zt,(m)|Zt−1,(m ) ) in Eq ( 11 ) and obtain the derivative wrt B(m ) . By setting the derivative to zero , we get a new estimation of B(m ) :
( B(m))new = C2/C1 ,
T T t=2 n=m Ln n=m Ln j=1 t=2 j=1
C1 =
C2 =
E[(Zt−1,(m)Fj)(Zt−1,(m)Fj )
] ,
E[(Zt,(m))j(Zt−1,(m)Fj )
] ,
( 14 )
,
( 11 ) where E[(Zt−1,(m)Fj)(Zt−1,(m)Fj ) ] and E[(Zt,(m))j(Zt−1,(m)Fj ) ] are calculated from the expectations and the covariances of the latent factors . Specifically ,
E[(Zt,(m))j(Zt−1,(m)Fj ) ] = cov((Zt,(m))j , ( Zt−1,(m)Fj ) jE[Zt−1,(m ) ] +E[(Zt,(m))j]F ( 15 )
E[(Zt−1,(m)Fj)(Zt−1,(m)Fj ) ] = cov(Zt−1,(m)Fj , ( Zt−1,(m)Fj )
)
,
+E[(Zt−1,(m))]FjF jE[Zt−1,(m ) ]
( 16 ) Each entry apq of cov((Zt,(m))j , ( Zt−1,(m)Fj ) ) and each entry bpq of cov(Zt−1,(m)Fj , ( Zt−1,(m)Fj ) ) are obtained by :
.
)
] − E[vec(Z1)]E[vec(Z1 )
]fi , apq = bpq =
E[vec(Zt ) vec(Zt )
] i k
Fkj cov(Zt,(m ) , Zt−1,(m))pjqk ,
( 17 )
FkjFij cov(Zt−1,(m ) , Zt−1,(m))piqk ,
( 18 ) k vec(Z1 ) ∼ N ( vec(Z0 ) , mat(O0 ) ) , vec(Zt)| vec(Zt−1 ) ∼ N ( mat(B ) vec(Zt−1 ) , mat(O ) ) , vec(Xt)| vec(Zt ) ∼ N ( mat(U ) vec(Zt ) , mat(D ) ) .
( 10 )
Updating non multilinear parameters . In this step , we obtain new estimations of the non multilinear parameters to maximize the expectation of the log likelihood defined in Eq ( 11 ) .
Q(θ , θold ) = EZ,{V(m)}|θold ln p(X ,S,Z,{V(m)}|θ )
θnew = argmax
Q(θ , θold ) ,
θ where θ = {U,B,Z0 , σ2 S,Z,{V(m)}|θ ) is defined in Eq ( 9 ) .
O , σ2
0 , σ2
R,{(ξ(m))2},{σ2
Vm}} , and p(X ,
With the expectations obtained in the latent factor inferring step , each non multilinear model parameter can be obtained by taking the derivative of Q(θ , θold ) wrt that parameter and setting the derivative to zero . The parameters are updated as follows :
( σ2
Vm )new =
1
NmLm vec(Z new
0
) = E[vec(Z1) ] ,
( σ2
0)new =
( σ2
O)new = tr(E[V(m )
Nm tr.E[vec(Z1 ) vec(Z1 )
( V(m )
] ) , j=1
) j j tr m=1 Lm
T ] − T t=2 t=2
− mat(B )
E[vec(Zt−1 ) vec(Zt )
E[vec(Zt ) vec(Zt−1 )
] mat(B )
E[vec(Zt−1 ) vec(Zt−1 )
] mat(B )
,
( (ξ(m))2)new =
( σ2
R)new = jU(m)E[V(m ) j
]
( U(m ) ) j j j
)
)
V(m )
( S(m )
( V(m ) j − 2(S(m ) )
S(m ) T .vec(X ∗ ∗E.vec(Zt ) vec(Zt ) ∗E[vec(Zt)]fi . mat(U )
1 i vec(Wt)i t=1
, fi ( mat(U ) vec(X ∗ t ) ∗ t )
)
( 12 )
Updating multilinear operators . Though the new estimations of mat(U ) and mat(B ) can be derived in the same way with updating non multilinear parameters , it cannot determine the factors of U and B . We apply the following steps to obtain a closed form solution for B(m ) and U(m ) , by keeping other factors fixed .
Based on Lemma 31 33 , we can get
Zt|Zt−1 ∼ N,B Zt−1 , σ2 OI
⇔ vec(Zt)| vec(Zt−1 ) ∼ N,mat(B ) vec(Zt−1 ) , σ2 OI) , ⇔ Zt,(m)|Zt−1,(m ) ∼ MN
B(m)Zt−1,(m))F , I , σ2
( 13 )
OI )
1 t=2 m=1 Lm
+ mat(B )
1M ( T − 1)M T T
Nm
U(m)E T
+ tr,mat(U )
1 N 2 m
+ tr j=1 t=1 t=2
−2 vec(X ∗ t ) where cov(Zt,(m ) , Zt−1,(m ) ) is calculated as follows : 1 ) revert the inferred cov(vec(Zt ) , vec(Zt−1 ) ) to the tensor form ; 2 ) permute the order of the mode from [ 1 , 2 , , 2M ] to [ m , 1 , , m − 1 , m + 1 , , M , m + M , 1 + M , , m − 1 + M , m + 1 + M , , 2M ] ; 3 ) reshape the reordered covariance tensor by keeping the first and ( M +1)th mode fixed and concatenating data from the 2nd mode to the M th mode into one mode , and data from the ( M + 2)th mode to the ( 2M )th mode into another mode , and get the four order cov(Zt,(m) ) , Zt−1,(m) ) . We perform similar operations to construct E[(Zt,(m) ) ] from E[vec(Zt) ] , cov(Zt,(m ) , Zt,(m ) ) from cov(vec(Zt ) , vec(Zt) ) . We can also perform similar transformations for U(m ) shown in Eq ( 19 ) . We only include the observed data in the learning process and ignore the missing entries of time series data indicated by ( Wt,(m) ) . Consequently , we update each row U(m ) i . with Eq ( 20 ) .
Xt|Zt,U ∼ N,U Zt , σ2 RI⇔ Xt,(m)|Zt,(m ) ∼ MN
( 19 )
,
U(m)Zt,(m))G , I , σ2
RI ) where G = ( U(M ) ⊗ ⊗ U(m+1 ) ⊗ U(m−1 ) ⊗ U(1) ) , and Xt,(m ) denotes mode m matricizing of Xt .
,
( U(m ) i .
)new =
λmA11/(ξ(m))2 + ( 1 − λm)A12/σ2 λmA21/(ξ(m))2 + ( 1 − λm)A22/σ2
R
R
,
( 20 )
83 j=1
Nm Nm T T j=1 t=1
A11 =
A21 =
A12 =
A22 =
S(m ) ij
E[V(m ) j
]
,
( V(m ) j
)
] , j
E[V(m ) n=m Nn n=m Ln j=1
( Wt,(m))ij(Xt,(m))ijE[(Zt,(m)Gj )
] ,
( Wt,(m))ijE[Zt,(m)Gj(Zt,(m)Gj )
] , t=1 j=1 where λm represents the contextual weight of mode m and E[Zt,(m)Gj(Zt,(m)Gj ) ] can be obtained similarly to E[(Zt−1,(m ) Fj)(Zt−1,(m)Fj ) ] .
The overall algorithm . Putting everything together , we have the overall algorithm ( Facets ) summarized in Algorithm 1 to get a local optimal solution of Eq ( 9 ) . Given a network of high order time series ( NetHiTs ) R , the dimensions of latent factors L ∈ RM , and the contextual weight vector λ ∈ RM , our algorithm aims to find the latent factors U,Z,V and other model parameters in θ .
The Facets algorithm first randomly initializes the model parameters θ ( step 1 ) and obtains the mode number and the dimensionality of Xt ’s each mode ( step 2 ) . Then it performs matricizations of X ,W that are repeatedly used in the EM steps . It also calculates the expectations of V(m ) before the first iteration ( step 3 10 ) . Then the algorithm alternatively updates the parameters and the latent factors until the convergence . In each repetition , it conducts M iterations to update each B(m ) and U(m ) , while keeping other B(n ) and U(n)(n = m ) fixed ( step 13 21 ) . Specifically , it iteratively infers the expectations and the covariances of vectorized Z , including E[vec(Zt) ] , E[(vec(Zt ) vec(Zt) ] , E[(vec(Zt ) vec(Zt−1) ] , cov(vec(Zt ) , vec(Zt) ) , and cov(vec(Zt ) , vec(Zt−1 ) ) ( step 14 ) . Afterwards , it updates the model parameters ( step 15 17 ) . Before the next iteration , it updates the expectations related to V(m ) if λm > 0 ( step 18 20 ) .
Our Facets algorithm converges to a local optima of Eq ( 9 ) , as it strictly follows the EM algorithm procedure .
Time complexity . The time complexity of our proposed algorithm is summarized in Lemma 41 Lemma 41 Given a network of high order time series R that consists of X ∈ RN1×,,×NM ×T ,W ∈ RN1×,,×NM ×T , and S = {S(1 ) , S(2 ) , , S(M )} , where each S(m ) ∈ RNm×Nm ( 1 ≤ m ≤ M ) , the contextual weight λ ∈ RM and the hidden latent dimensions L ∈ RM , the time complexity of the Facets algorithm is upper bounded by m
L2 mN 2 m ) )
O(#iterations · ( l2nT + where l =M m=1 Lm , n =M m=1 Nm .
Proof . Omitted for Brevity .
Algorithm 1 : Facets Input : Net HiTs R = X ,W,S , ζ , dimension of latent factors L weight of contextual information λ E[Z ] , E[V ]
Output : θ = {U,B,Z0 , σ2
O , σ2
0 , σ2
R,{(ξ(m))2},{σ2
Vm}} ,
1 Initialize θ ; 2 M ← Xt ’s mode ; N1 , N2 , , NM ← dimensions of Xt ; 3 Matricize X ,W along time mode to obtain X , W ; 4 for m =1:M do Matricize each Xt,Wt to obtain Xt,(m ) , Wt,(m ) ; 5 if λm > 0 then
6
7
8
9 for j=1:Nm do Infer E[v(m ) j
] , E[v(m ) j
( v(m ) j
) ] end end
10 11 end 12 repeat 13 for m = 1:M do
14
15
16
17
18
19
20
R and if
0 , σ2
Infer the expectations and covariances of vectorized latent factors ; With Eq ( 12 ) , update Z0 , σ2 O , σ2 λm > 0 , update ( ξ(m))2 , σ2 Vm ; Reshape cov(vec(Zt ) , vec(Zt−1) ) , cov(vec(Zt ) , vec(Zt ) ) and E[vec(Zt ) ] to cov(Zt,(m ) , Z E[Zt,(m) ] , respectively ; Update B(m ) and U(m ) by Eq ( 14 ) ( 20 ) ; if λm > 0 then t−1,(m) ) , cov(Zt,(m ) , Z t,(m ) ) and update the expectations related to V(m ) ; end end
21 22 until convergence ;
4.3 Data Mining Applications
Our proposed Facets algorithm captures temporal smooth ness and contextual correlations with observed time series data and input contextual constraints . It naturally fits in the tasks of imputation and forecasting . Other data mining applications where our algorithm can be conveniently applied include denoising , anomaly detection and time series clustering . We omit the details for the limited space . A1 Imputation . With the output of the Facets algorithm , the vectorized version of the reconstructed time series ˆX can be obtained by vec( ˆXt ) = mat(U )E[vec(Zt) ] . The missing values of Xt can be inferred from the corresponding entries of vec( ˆXt ) . A2 Prediction . Given the output parameter θ and the inferred ˆZt , we can predict ˆZt+1 as B ˆZt , and ˆXt+1 as U ˆZt+1 . In addition , once the real data Xt+1 is arrived at ( t + 1)th time step , Facets will update ˆZt+1 according to the observed Xt+1 to improve the accuracy of predicting the latter time series . Note that Facets can naturally deal with the missing values in the newly arriving time series data .
In practice , the length ( T ) of the time series is often orders of magnitude larger than the number of the time series ( n ) . Hence , the actual running time of Facets is dominated by the term related to the length of the time series T , which is linear in T .
5 . EXPERIMENTAL RESULTS
In this section , we present the empirical evaluations on three real datasets , which are designed to answer the following two questions :
84 • Effectiveness : how accurate is the proposed Facets algorithm in terms of imputation and prediction of time series ?
• Efficiency : how does the proposed Facets algorithm scale wrt the size of the input time series ?
5.1 Experimental Setup A . Baseline Methods . We compare our method with the following state of the art algorithms : Probabilistic Matrix Factorization ( PMF ) [ 21 ] , Social Recommendation ( SoRec ) [ 19 ] , SmoothPMF and SmoothSoRec . SmoothPMF and SmoothSoRec are improved algorithms of PMF and SoRec , which encode temporal smoothness by adding regularizations on consecutive time series latent factors [ 5 ] . We also compare with DCMF , which is a special case of our Facets algorithm where M = 1 . B . Datasets . We use the following three real datasets in our experiments .
SST Dataset The Sea Surface Temperature ( SST ) dataset consists of hourly temperature measurements from a 5 by 6 grid of sea surface located from 5◦N , 180◦W to 5◦S , 110◦W [ 3 ] . The measurement started from April 26 , 1994 at 7:00 pm to July 19 , 1994 at 3:00 am , a total of 2000 time steps . Since no contextual network data is available in this dataset , we perform matriciztions along each mode and construct a contextual matrix based on the cosine similarity of each pair of time series in that mode . In the experimental evaluations , we set the dimensions of latent factors as L = [ 3 , 3 ] for this dataset .
Motes Dataset The Motes dataset consists of 4 data types collected from 54 sensors deployed at the Intel Berkeley Research Lab over a month [ 1 ] . The data types include three room conditions ( ie , temperature , humidity , light ) and the sensor voltage level . Considering that the running time of some baselines is quite slow , we evaluate all the algorithms only on the first day measurement data with a total of 2880 time steps . We construct the contextual sensor network in the same way as in [ 5 ] and ignore the contextual information in the type mode for simplicity . By default , we set the dimensions of latent factors as L = [ 15 , 3 ] in the evaluations .
SPMD Dataset The Safety Pilot Model Deployment ( SPMD ) dataset was collected to understand the potential safety benefits of connected vehicle safety technologies [ 2 ] . It consists of multimodal and multidimensional traffic data mostly within the test site of Ann Arbor , Michigan . The available one day sample dataset contains trajectory traces of monitored vehicles collected on April 11 , 2013 , including a total of 369 trips and 124 vehicles ranging from passenger cars , trucks to buses . The trip duration ranges from seconds to hours with an average around 20 mins . We select the trips lasting over 20 mins , which result in a total of 52 trips . Then , we set the departure time of each trip as the first time step and sample two dimensional location coordinates every one second with a duration of 20 minutes , resulting in a total of 1200 time steps . Theoretically , if we have sufficient trips , we can learn the contextual matrix for trips based on drivers’ behaviors . With only one day data available in our evaluations , we define the trip contextual matrix based on the cosine similarities among trip trajectories . Since the longitude and the latitude are not strongly correlated practically , we ignore the contextual constraint in the location mode . In the evaluations , we set the dimensions of latent factors as L = [ 30 , 2 ] by default .
C . Evaluation Metrics . To evaluate the effectiveness , we perform the vectorization of the observed time series tensors and the estimated time series tensors . Then , we calculate the root mean squared error ( RMSE ) [ 5 ] between them . 5.2 Sensitivity Results
We perform the parametric studies wrt the two hyperparameters in our algorithm the dimensions of latent factors L and the contextual weight λ . Our results shown in Fig 4 indicate that the performance of our algorithm is robust in a large range of both parameters .
( a ) Impact of L .
( b ) Impact of λ . Figure 4 : Parameter sensitivity results .
Fig 4(a ) shows the impact of L on the training parts and the test parts of the Motes dataset . We keep the latent dimension of the second mode fixed in 3 . As we can see , the RMSEs of our method fluctuate within a range of less than 0.01 between [ 15 , 3 ] and [ 50 , 3 ] . Fig 4(b ) shows the impact of λ on the Motes dataset . We vary the the contextual weight from zero to one . If λ equals zero , the contextual information is ignored . If λ equals one , only the contextual information is included to learn U . As we can see , the RMSEs of our method fluctuate within a range of less than 0.05 in general cases ( ie , λ > 0 ) ; and in the meanwhile , the RMSEs significantly increase if we ignore the contextual information ( ie , λ = 0 ) , especially when time series data is very sparse , which indicates the importance of modeling the contextual network information . 5.3 Effectiveness Results
To evaluate the effectiveness of the algorithms in the task of missing value recovery , we incrementally generate training sets and test sets with an increasing amount of test data ( 10 % , 50 % , ) within time series . As a result , the subsequent test set always contains the previous test data .
Fig 5 shows the imputation results on the three datasets . We can clearly see that our Facets significantly outperforms the others , especially when the percentage of the missing values is large . Fig 6 presents our Facets ’s imputation results of a trip instance in the SPMD dataset . The x axis and the y axis denote the normalized latitude and longitude , respectively . In each subfigure , the blue curve with crosses represents the training data with missing values and the red curve denotes the recovered trace by the Facets algorithm . As we can see , Facets achieves good approximations with a few training data ( 90 % 10% ) . Even with only 1 % training data ( ie , 6 pairs of x y coordinates ) , our algorithm also achieves good performance shown in Fig 6(d ) .
Fig 7 shows the prediction results . In the evaluations of the SST dataset and the Motes dataset , we use the first k time steps of time series as training data , where k = T ∗ ( 1 − prediction% ) . For the SPMD dataset , the most common scenario is to predict the trace of a target trip based
[ 3,3][ 5,3][10,3][15,3][20,3][30,3][50,3]00501015020250303504LRMSE test RMSEtraining RMSE 001040608 1020406081121416λRMSE 10 % missing50 % missing90 % missing99 % missing99.9 % missing99.95 % missing85 ( a ) SST
( b ) Motes
( c ) SPMD
Figure 5 : Effectiveness of imputation . The lower the better .
( a ) 90 % training data
( b ) 50 % training data
( c ) 10 % training data
( d ) 1 % training data
Figure 6 : Facets ’s imputation results of one trip instance in SPMD Dataset . on some completed trips in history . Therefore , we randomly select prediction % of the trips and hide their last prediction % of time steps as test data . As we can see from Fig 7 , our Facets algorithm is quite robust for different prediction ratios , ranging from 10 % to 40 % . Our Facets algorithm and its special case , DCMF , achieve much higher accuracy than others . Fig 8 demonstrates Facets ’s prediction results on two sensor instances from the Motes dataset . 5.4 Efficiency Results
We test the scalability of the Facets algorithm on a number of subsets of the Motes dataset and the SPMD dataset . As Fig 9(a ) shows , our proposed Facets algorithm scales linearly wrt the sequence length T , which is consistent with our complexity analysis in section 4 . Fig 9(b ) demonstrates that the running time of Facets is close to linear wrt the aggregated dimensions of time series n .
( a ) Motes Dataset
( b ) SPMD Dataset
Figure 9 : Scalability .
We also compare the running time of our Facets algorithm with other baselines . Fig 10(a ) presents the running time of the imputation experiments on the SST dataset . Combining with the results of Fig 5(a ) , we can see that Facets and DCMF yield superior effectiveness while spend much less time in comparison with PMF , SoRec , SmoothPMF and SmoothSoRec . We can get similar observations from Fig 10(b ) , which presents the running time of the prediction experiments on the SPMD dataset .
( a ) Imputation on SST
( b ) Prediction on SPMD
Figure 10 : Efficiency
6 . RELATED WORK
There are plenty of work in mining time series data [ 12 ] , including representation [ 24 , 22 ] , classification [ 30 , 8 , 11 ] , outlier detection [ 16 ] , etc . Li et al . proposed DynaMMo for mining coevolving time series with missing values based on linear dynamic systems [ 17 ] . Matsubara et al . developed AutoPlait , which combined a multi level chain model and a cost model to find typical patterns and meaningful segments in multiple time series [ 20 ] . Shieh et al . proposed a multi resolution symbolic representation to index time series datasets for fast exact search and approximate search [ 24 ] . Tensor decompositions have been successfully applied in many domains including signal processing , computer vision , neuroscience and data mining [ 14 ] . Most of tensor compositions are based on tucker decomposition [ 27 ] , canonical/parrallelfactors ( CP ) decomposition [ 10 , 6 ] , and multilinear PCA [ 18 ] .
10 50 90 99 99999950051152253354missing value%RMSE PMFSoRecSmoothPMFSmoothSoRecDCMFFacets 10 50 90 99 99999950051152253missing value%RMSE PMFSoRecSmoothPMFSmoothSoRecDCMFFacets 10 50 90 99 99999950020406081missing value%RMSE PMFSoRecSmoothPMFSmoothSoRecDCMFFacets−12−1−08−06−04−04−020020406081Normalized LatitudeNormalized Longitude 90 % MeasuredRecovered−12−1−08−06−04−04−020020406081Normalized LatitudeNormalized Longitude 50 % MeasuredRecovered−12−1−08−06−04−04−020020406081Normalized LatitudeNormalized Longitude 10 % MeasuredRecovered−12−1−08−06−04−04−020020406081Normalized LatitudeNormalized Longitude 1 % MeasuredRecovered 0 2160 4320 6480 86401080001020304050Trunning time(min ) L=[1,3]L=[5,3]L=[10,3]L=[15,3 ] 40 48 56 64 72 80 88 9610420406080100120140160nrunning time(s ) L=[5,2]L=[10,2]L=[15,2]L=[20,2 ] 10 50 90 99 99999950020406081121416missing value%Running Time ( h ) PMFSoRecSmoothPMFSmoothSoRecDCMFFacets10203040005115225335prediction%Running Time ( h ) PMFSoRecSmoothPMFSmoothSoRecDCMFFacets86 ( a ) SST
( b ) Motes
( c ) SPMD
Figure 7 : Effectiveness of prediction . The lower the better .
( a ) Temperature
( b ) Humidity
( c ) Light
( d ) Voltage
Figure 8 : Facets ’s prediction for Sensor 30 and Sensor 54 in Motes Dataset .
Sun et al . formally introduced tensors in time evolving settings and proposed dynamic and streaming tensor analysis , which learned a latent , low dimensional core tensor and a set of projection matrices to summarize large tensor sequences and detect patterns [ 25 ] . Xiong et al . extended twodimensional collaborative filter problems into three order tensor space after introducing the time factor [ 28 ] . They proposed a bayesian probabilistic tensor factorization method to compute CP decompositions to get the latent factors in each mode . Rogers et al . presented multilinear dynamical systems to model tensor time series based on linear dynamic systems [ 23 ] . All of these models ignore contextual information embedded in time series data . Bahadori et al . proposed a neat and flexible framework , under which either spatial or temporal information can be realized/modeled , respectively [ 4 ] . Yet , how to simultaneously model these two aspects was not answered which is exactly one major advantage of our work .
If time series tensor is degraded to time series matrix , our work is also related to matrix factorization , which is widely applied in the recommendation systems . In the recommendation problems , the matrix factorization methods find low rank latent factors to represent users and items , which can be fit into the user item rating matrix and make rating prediction [ 21 , 19 , 15 , 26 , 29 ] . To improve the prediction accuracy , some side information , such as the user social network and/or the item item similarity are included in the probabilistic factor analysis [ 19 , 29 ] . Recently , dynamic matrix factorization methods have been proposed to capture the evolving user preferences [ 7 , 26 ] . For example , Sun et al . applied a dynamic state space model on probabilistic matrix factorization to track the temporal dynamics of the user latent factor[26 ] or model the changes of user preferences on the user item adoption problem [ 7 ] .
7 . CONCLUSION
In this paper , we have proposed a comprehensive method , Facets , to address three prominent challenges in mining a network of high order time series data : ( a ) high order ; ( b ) contextual constraints ; and ( c ) temporal smoothness . Based on tensor factorization and multilinear dynamic systems , for the first time , our algorithm effectively and efficiently solve all of the three challenges at the same time . The experimental evaluations on three real datasets demonstrated the effectiveness and the scalability of our algorithm . For future work , we are interested in applying our Facets method to other time series mining tasks , such as anomaly detection ; and finding alternative to the EM algorithm , such as approximate inference and sampling .
8 . ACKNOWLEDGMENT
This material is supported by the National Science Foundation under the grant number IIS1017415 and CNS 0904901 , by the Army Research Laboratory under Cooperative Agreement Number W911NF 09 2 0053 , by National Institutes of Health under the grant number R01LM011986 , Region II University Transportation Center under the project number 49997 32 25 and 49997 33 25 .
The content of the information in this document does not necessarily reflect the position or the policy of the Government , and no official endorsement should be inferred . The US Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on .
102030400051152253prediction%RMSE PMFSoRecSmoothPMFSmoothSoRecDCMFFacets1020304000204060811214prediction%RMSE PMFSoRecSmoothPMFSmoothSoRecDCMFFacets1020304000050101502025prediction%RMSE PMFSoRecSmoothPMFSmoothSoRecDCMFFacets50100150200250300350−15−1−05005115225Predicted Time StepNormalized Temperature MeasuredPredictedSensor 54Sensor 3050100150200250300350−25−2−15−1−05005115Predicted Time StepNormalized Humidity MeasuredPredictedSensor 30Sensor 5450100150200250300350−101234Predicted Time StepNormalized Light MeasuredPredictedSensor 54Sensor 3050100150200250300350−15−1−050051152Predicted Time StepNormalized Voltage MeasuredPredictedSensor 30Sensor 5487 9 . REFERENCES
[ 1 ] Motes dataset . http://dbcsailmitedu/labdata/labdatahtml
[ 2 ] Safty pilot model deployment . https://wwwits rde net/showds?dataEnvironmentNumber=10014 .
[ 3 ] Tropical atmosphere ocean project . http :
//wwwpmelnoaagov/tao/data_deliv/delivhtml
[ 4 ] M . T . Bahadori , Q . R . Yu , and Y . Liu . Fast multivariate spatio temporal analysis via low rank tensor learning . In NIPS , 2014 .
[ 5 ] Y . Cai , H . Tong , W . Fan , and P . Ji . Fast mining of a network of coevolving time series . In SDM , 2015 . [ 6 ] J . Carroll and J J Chang . Analysis of individual differences in multidimensional scaling via an n way generalization of ‘eckart young’ decomposition . Psychometrika , 35(3):283–319 , 1970 .
[ 7 ] F . C . T . Chua , R . J . Oentaryo , and E P Lim .
Modeling temporal adoptions using dynamic matrix factorization . In ICDM , 2013 .
[ 8 ] J . Gao , B . Ding , W . Fan , J . Han , and P . S . Yu .
Classifying data streams with skewed class distributions and concept drifts . IEEE Internet Computing , 12(6):37–49 , 2008 .
[ 9 ] A . K . Gupta and D . K . Nagar . Matrix Variate
Distributions . CRC Press , 1999 .
[ 10 ] R . A . Harshman . Foundations of the PARAFAC procedure : Models and conditions for an ‘explanatory’ multi modal factor analysis . UCLA Working Papers in Phonetics , 16(1):84 , 1970 .
[ 11 ] B . Hu , Y . Chen , J . Zakaria , L . Ulanova , and E . Keogh . Classification of multi dimensional streaming time series by weighting each classifier ’s track record . In ICDM , 2013 .
[ 12 ] E . Keogh . Tutorial : Machine learning in time series databases ( and everything is a time series! ) . In AAAI , 2011 .
[ 13 ] T . G . Kolda . Multilinear operators for higher order decompositions . 2006 .
[ 14 ] T . G . Kolda and B . W . Bader . Tensor decompositions and applications . SIAM review , 51(3):455–500 , 2009 .
[ 15 ] Y . Koren , R . Bell , and C . Volinsky . Matrix factorization techniques for recommender systems . Computer , 42(8):30–37 , 2009 .
[ 16 ] J G Lee , J . Han , and X . Li . Trajectory outlier detection : A partition and detect framework . In ICDE , 2008 .
[ 17 ] L . Li , J . McCann , N . S . Pollard , and C . Faloutsos .
Dynammo : Mining and summarization of coevolving sequences with missing values . In KDD , 2009 .
[ 18 ] H . Lu , K . N . Plataniotis , and A . N . Venetsanopoulos .
Multilinear principal component analysis of tensor objects for recognition . In ICPR , 2006 .
[ 19 ] H . Ma , H . Yang , M . R . Lyu , and I . King . Sorec : social recommendation using probabilistic matrix factorization . In CIKM , 2008 .
[ 20 ] Y . Matsubara , Y . Sakurai , and C . Faloutsos .
Autoplait : Automatic mining of co evolving time sequences . In SIGMOD , 2014 .
[ 21 ] A . Mnih and R . Salakhutdinov . Probabilistic matrix factorization . In NIPS , 2007 .
[ 22 ] S . Papadimitriou , J . Sun , and C . Faloutsos . Streaming pattern discovery in multiple time series . In VLDB , 2005 .
[ 23 ] M . Rogers , L . Li , and S . J . Russell . Multilinear dynamical systems for tensor time series . In NIPS , 2013 .
[ 24 ] J . Shieh and E . Keogh . isax : indexing and mining terabyte sized time series . In KDD , 2008 .
[ 25 ] J . Sun , D . Tao , and C . Faloutsos . Beyond streams and graphs : Dynamic tensor analysis . In KDD , 2006 .
[ 26 ] J . Z . Sun , K . R . Varshney , and K . Subbian . Dynamic matrix factorization : A state space approach . In ICASSP , 2012 .
[ 27 ] L . Tucker . Some mathematical notes on three mode factor analysis . Psychometrika , 31(3):279–311 , 1966 .
[ 28 ] L . Xiong , X . Chen , T K Huang , J . G . Schneider , and J . G . Carbonell . Temporal collaborative filtering with bayesian probabilistic tensor factorization . In SDM , 2010 .
[ 29 ] Y . Yao , H . Tong , G . Yan , F . Xu , X . Zhang ,
B . Szymanski , and J . Lu . Dual regularized one class collaborative filtering . In CIKM , 2014 .
[ 30 ] L . Ye and E . Keogh . Time series shapelets : a new primitive for data mining . In KDD , 2009 .
APPENDIX A . Details of inferring latent factors
In Step 6 9 and Step 18 20 of Algorithm 1 , the expecta tions of V(m ) j are calculated with the follows equations : j j j j j j
.
,
)
( ν ( m )
( U(m ) )
( V(m ) )
U(m ) + ( ξ(m))2σ j ] = Υ + ν ( m )
S(m ) , Υ = , E[V(m )
ν ( m ) j = Υ(U(m ) ) E[V(m ) ] = ν ( m ) ( 21 ) In Step 14 , Algorithm 1 infers the expectations and the covariances of latent factors Zt , which is difficult in the tensor spaces . Instead , Facets performs the vectorizations and matricizations with Eq ( 10 ) , which reduces to find the expectations and covariances of vec(Zt ) . For clarity , in the following equations , we write vec(Xt ) as xt , vec(Zt ) as zt and formulate vec(Wt ) as Wt , ( t = 1 , , T ) . We use U , B to denote matricization results of mat(U ) , mat(B ) , respectively .
First we only count on the observed time series data . We use ot to denote the indices of the observed entries of xt and x∗ t and Ht are defined as follows : ot = {i|Wit > 0 , i = 1 , , n} , x Let p(zt|x1 , , xt ) = N ( zt|µt , Ψt ) and p(zt|x1 , , xT ) = N ( zt| ˆµt , ˆΨt ) , Step 14 of Algorithm 1 is calculated with the following equations :
∗ t = xt(ot , : ) , Ht = U(ot , : ) .
( 22 )
−1
−2 Vm
0I − K1H1 ,
+ σ2 t + σ2 OI , Kt = Pt−1H t(HtPt−1H RI ) t − HtBµt−1 ) , Ψt = ( I − KtHt)Pt−1 , ∗
−1 ,
1 − H1z0 ) , Ψ1 = σ2 ∗
RI )
−1 ,
0H1H
K1 = σ2
1 + σ2
1(σ2 0H µ1 = z0 + K1(x Pt−1 = BΨt−1B µt = Bµt−1 + Kt(x ˆµt = µt + Jt( ˆµt+1 − Bµt ) , ˆΨt = Ψt + Jt( ˆΨt+1 − Pt)J E[zt ] = ˆµt , cov(zt , zt−1 ) = ˆΨtJ E[ztz t−1 ] = ˆΨtJ t−1 , E[ztz t−1 + ˆµt ˆµ t , Jt = ΨtB
−1 ,
( Pt ) t−1 , cov(zt , zt ) = ˆΨt , t ] = ˆΨt + ˆµt ˆµ t . ( 23 )
88
