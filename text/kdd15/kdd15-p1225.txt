Towards Interactive Construction of Topical Hierarchy : A
Recursive Tensor Decomposition Approach
Chi Wang† , Xueqing Liu‡ , Yanglei Song‡ , Jiawei Han‡ ‡University of Illinois at Urbana Champaign . Urbana , IL , 61801
†Microsoft Research . Redmond , WA 98007 chiw@microsoft.com , {xliu93 , ysong44 , hanj}@illinois.edu
,
,
ABSTRACT Automatic construction of user desired topical hierarchies over large volumes of text data is a highly desirable but challenging task . This study proposes to give users freedom to construct topical hierarchies via interactive operations such as expanding a branch and merging several branches . Existing hierarchical topic modeling techniques are inadequate for this purpose because ( 1 ) they cannot consistently preserve the topics when the hierarchy structure is modified ; and ( 2 ) the slow inference prevents swift response to user requests . In this study , we propose a novel method , called STROD , that allows efficient and consistent modification of topic hierarchies , based on a recursive generative model and a scalable tensor decomposition inference algorithm with theoretical performance guarantee . Empirical evaluation shows that STROD reduces the runtime of construction by several orders of magnitude , while generating consistent and quality hierarchies .
Categories and Subject Descriptors I.7 [ Computing Methodologies ] : Document and Text Processing ; H28 [ Database Applications ] : Data Mining
Keywords Topic Modeling , Ontology Learning , Interactive Data Exploration , Tensor Decomposition
1 .
INTRODUCTION
Constructing a topic hierarchy for large text collection , such as business documents , news articles , social media messages , and research publications , is helpful for information workers , data analysts and researchers to summarize and navigate them in multiple granularity efficiently . While existing hierarchical topic models can be used to produce such hierarchies as an exploration tool , they still require human curation ( eg , modify the structure and label the topics ) to meet the quality requirement for reliable exploitation . The manual work for curation is very expensive . This work focuses on helping with the structure modification task .
The nature of this task is interactive and iterative . On one hand , people use a topic model to explore a dataset when the topics are unknown a priori . Thus it is hard to determine the best shape of the hierarchy upfront . On the other hand , as they see the results ( inferred topics even with imperfect structure ) , people have ideas about a more desirable structure , eg , one topic should be expanded , or multiple topics should be merged . Then they may want to modify part of the hierarchy but preserve other parts that already look good to be labeled . Some modification , such as expanding a topic , is again exploratory and needs help from the machine . It takes multiple iterations of human investigation and algorithm run to finish the construction .
To enable interactive construction of the topic hierarchy , ie , allowing users to modify the structure on the go , the system needs to satisfy two conditions : efficiency and consistency . Efficiency is necessary for users to see results quickly and react before they lose the context . Consistency is necessary for confusion free modification , and has two fold meanings : when people want to modify certain parts of the hierarchy , the remaining parts should be preserved after each run ( single run consistency ) ; and a system should output undifferentiated results given identical input in multiple runs ( multi run consistency ) .
Limitation of prior work . Most existing hierarchical topic modeling techniques [ 10 , 17 , 20 , 14 , 2 ] are based on the extensions of latent Dirichlet allocation ( LDA ) , and are not designed for interactive construction of the hierarchy . First , the inference algorithms for these models are expensive , demanding hundreds or thousands of passes of data . Second , an inference algorithm generates one hierarchy for one corpus in one run of the algorithm . Running the inference algorithm with a slightly modified hierarchy structure does not guarantee preservation of topics in untouched branches . Rerunning the inference algorithm with the same input may result in very different results . Therefore , both single run and multi run consistency conditions are violated if we use them for interaction .
Our solution . We consider a strategy of top down , progressive construction of a topical hierarchy , instead of inferring a complex hierarchical model all at once . Thus the construction can be done via a series of interactive operations , such as expanding a topic , collapsing a topic , merging topics and removing topics . Efficient and consistent algorithms can then be designed for each operation . Users can see the results after each operation , and decide what opera
1225 tion to take next . This strategy has several benefits : users can easily control the complexity of the hierarchy ; users can see intermediate results and curate the hierarchy in early stages ; and it is easier for curators to focus on one simple operation a time .
To support these interactive operations in an efficient and consistent manner , we resort to moment based inference . Simply put , moment based inference compresses the original data by collecting important statistics from the documents , eg , term co occurrences , and uses these statistics to infer topics . For one advantage , the inference based on the compressed information avoids the expensive , numerous passes of the data . For another advantage , the compression reduces randomness in the data by aggregation . With careful choice of the statistics and the inference method , we can uncover the topics with theoretical guarantee . Modifications to the hierarchy can be supported by manipulating the moments . We establish a top down hierarchy construction framework STROD based on these ideas . To the best of our knowledge , it is the first framework towards interactive topical hierarchy construction . The following summarizes our main contributions : • We propose a new hierarchical topic model such that the modification operations mentioned above can be achieved by several atomic operators to the model .
• We develop a scalable tensor based recursive orthogonal decomposition ( STROD ) method for efficient and consistent construction .
• Our experiments demonstrate that our method is several orders of magnitude more efficient than the alternatives , while generating consistent , quality topic hierarchy that is comprehensible to users .
2 . RELATED WORK
Statistical topic modeling techniques model a document as a mixture of multiple topics , while every topic is modeled as a distribution over terms . Two important models are probabilistic latent semantic analysis ( PLSA ) [ 13 ] and its Bayesian extension latent Dirichlet allocation ( LDA ) [ 5 ] . They model the generative processes of each term from each document in a corpus , and then infer the unknown distributions that best explain the observed documents .
Hierarchical topic models follow the same generative spirit . Instead of having a pool of flat topics , these models assume an internal hierarchical structure of the topics . Different models use different generative processes to simulate this hierarchical structure , such as nested Chinese Restaurant Process [ 10 ] , Pachinko Allocation [ 17 ] , hierarchical Pachinko Allocation [ 20 ] , recursive Chinese Restaurant Process [ 14 ] , and nested Chinese Restaurant Franchise [ 2 ] . When these models are applied to constructing a topical hierarchy , the entire hierarchy is inferred all at once from the corpus .
The main inference methods for these topic models can be divided into two categories : MCMC sampling [ 11 ] and variational inference [ 5 ] . They are essentially approximation of the Maximum Likelihood ( ML ) principle ( including its Bayesian version maximum a posterior ) : Find the best parameters that maximize the joint probability specified by a model . There has been a substantial amount of work on speeding up LDA inference , eg , by leveraging sparsity [ 22 , 30 , 16 ] and parallelization [ 21 , 24 , 31 ] , or online learning mechanism [ 1 , 12 , 8 ] . Few of these ideas have been adopted by the hierarchical topic model studies .
These inference methods have no theoretical guarantee of convergence within a bounded number of iterations , and are nondeterministic either due to the sampling or the random initialization . Recently , a new inference method for LDA has been proposed based on a method of moments , rather than ML . It is found to have provable error bound and convergence properties in theory [ 3 ] .
All of the hierarchical topic models follow the bag ofwords assumption , while some other extensions of LDA have been developed to model sequential n grams to achieve better interpretability [ 26 , 29 , 18 ] . No one has integrated them in a hierarchical topic model . The efficiency and consistency issues will become more challenging in an integrated model . A practical approach is to decouple the topic modeling part and the phrase mining part . Blei and Lafferty [ 4 ] have proposed to use a statistical test to find topical phrases , which is time consuming . A much less expensive heuristic is studied in recent work [ 6 ] and shown to be effective .
There are a few alternative approaches to constructing a topical hierarchy . Pujara and Skomoroch [ 23 ] proposed to first run LDA on the entire corpus , and then split the corpus heuristically according to the results and run LDA on each split corpus individually . CATHY [ 28 ] is a recursive topical phrase mining framework for short , content representative text . It also decouples phrase mining and topic discovery for efficiency purpose . Though it is not designed for generic text , it bears some similarity with this work such as topdown recursion and compression of documents .
After the hierarchy is constructed from a corpus , people can label these topics and derive topic distributions for each document [ 25 ] . Those are not the subject of this paper . Broadly speaking , this work is also related to : hierarchical clustering of documents [ 9 ] , queries [ 19 ] , keywords [ 28 ] etc . ; and ontology learning [ 15 ] , which mines subsumption ( ‘is a’ ) relationships from text .
3 . PROBLEM FORMULATION
Given a corpus , our goal is to construct a user desired topical hierarchy , ie , a tree of topics , where each child topic is about a more specific theme within the parent topic .
For easy interaction , the topics need to be visualized in user friendly forms . Unigrams are often ambiguous , especially across fine grained topics [ 27 ] . We choose to enhance the topic representation with ranked phrases . The ranking should reflect both their popularity and discriminating power for a topic . For example , the top ranked phrases for the database topic can be : “ database systems ” , “ query processing ” , “ concurrency control ” . . . . A phrase can appear in multiple topics , though it will have various ranks in them .
Formally , the input data are a corpus of D documents . The d th document can be segmented into a sequence of ld tokens . All the unique tokens in this corpus are indexed using a vocabulary of V terms . And wd,j ∈ [ V ] , j = 1 , . . . , ld represents the index of the j th token in document d . A topic t is defined by a probability distribution over terms φt ∈ ∆V −1 , and an ordered list of phrases Pt = {Pt,1 , Pt,2 , . . .} , where Pt,i is the phrase ranked at i th position for topic t . A topical hierarchy is defined as a tree T in which each node is a topic . Every non leaf topic t has Ct child topics . We assume Ct is bounded by a small number K , such as 10 , because the topical hierarchy is intended for human to
1226 0 expand collapse
1
1
4 o → 1 → 1
3
0
2
0o o → 2
5
2
6 split
0
3
1
2
1 merge
1
0
2
8
1
7
3
5
7
0
3
0
2
4
4
3
0
0
1
2 remove
1
4
3
0
1
4
4
3
2
5 move
5 unaffected operated created
Figure 1 : Examples of 6 user operations . A topic can be indexed by a integer ( in the circle ) , or by the path from root efficiently browse the subtopics for each topic . The number K is named the width of the tree T .
A topical hierarchy for a corpus is constructed via a series of user operations . An operation transforms one topic hierarchy T to another T . A top down construction framework supports the following user operations . 1 . Expand – for an arbitrary topic t in T , grow a subtree rooted at t .
2 . Collapse – for an arbitrary topic t in T , remove all its descendant topics . delete these topics .
3 . Split – for an arbitrary topic t in T , split it into k topics . 4 . Remove – for an arbitrary set of topics t1 , . . . , tn in T , 5 . Merge – for an arbitrary set of topics t1 , . . . , tn in T , merge these topics as a new topic , whose parent is the least common ancestor of them , and whose children are the union of the children of all merged topics .
6 . Move – for an arbitrary topic t in T , move the subtree rooted at t to be under a different parent topic t .
Figure 1 demonstrates these operations . In these operations , only a few topics are affected , so users can consistently modify the hierarchy and control the change .
For convenience , we index a topic using the top down path from root to this topic . The root topic is indexed as o . Every non root topic t is recursively indexed by πt → χt , where πt is the path index of its parent topic , and χt ∈ [ Ct ] is the index of t among its siblings . For example , topic 2 in the ‘merge’ example of Figure 1 is indexed as o → 2 , and topic 3 in the same tree is indexed as o → 1 → 1 . The level ht of a topic t is defined to be its distance to the root . So root topic is in level 0 , and topic o → 1 → 1 is in level 2 . The height H of a tree is defined to be the maximal level over all the topics in the tree . Clearly , the total number T of topics is upper bounded by KH+1−1
.
K−1
4 . THE STROD FRAMEWORK
We develop a Scalable Tensor Recursive Orthogonal Decomposition ( STROD ) framework for interactive topical hierarchy construction . In Section 4.1 , we propose a new hierarchical topic model , and introduce how the user operations can be achieved by atomic manipulations to the model . In Section 4.2 , we present our tensor based algorithms supporting these operations . Section 4.3 introduces topical phrase mining and ranking based on the inferred model parameters .
4.1 Hierarchical Topic Modeling
Generative hierarchical topic modeling assumes the documents are generated from a latent variable model , and then infers the model parameters from observed documents to recover the topics . Distinct from prior work , we do not infer a hierarchy for one corpus only once . Instead , we allow users to perform consistent modification to the hierarchy . Therefore , we need a model that is convenient for manipulation and supports all the user operations introduced in Section 3 . We first introduce our generative model when the hierarchy structure is fixed , and then discuss atomic operators to manipulate the model structure . 411 Latent Dirichlet Allocation with Topic Tree In this subsection we assume the topic hierarchy structure is fixed . Its height is H , and there are τ leaf nodes and T −τ non leaf nodes . For ease of explanation we assume all leaf nodes are on the level of H . Every leaf topic node t(Ct = 0 ) has a multinomial distribution φt = p(w = ·|t ) over terms . Every document d paired with a non leaf node t(Ct > 0 ) has a multinomial distribution θd,t = p(w = ·|d , t ) over t ’s child topics : t → 1 through t → Ct . θd,t represents the content bias of document d towards t ’s subtopics . For the ‘merge’ example in Figure 1 , before merge , there are 3 non leaf topics : o , o → 1 and o → 2 . So a document d is associated with 3 multinomial distributions over topics : θd,o over its 2 children , θd,o→1 over its 3 children , and θd,o→2 over its 2 children . Each multinomial distribution θd,t is generated from a Dirichlet prior ( αt→1 , . . . , αt→Ct ) . αt→z represents the corpus’ bias towards z th child of topic t , and αt =Ct z=1 αt→z . To generate a token wd,j , we first sample a path from d,j → ··· → zH the root to a leaf node o → z1 d,j . The nodes along the path are sampled one by one , starting from the root . Each time one child zi d,j is selected from all children of o → z1 d,j , from the multinomial . When a leaf node is reached , the token is θd,o→z1 generated from its multinomial distribution φo→z1 d,j → ··· → zi−1 d,j→···→zi−1 d,j → z2 d,j
The whole generative process is : d,j→z2 d,j→···→zH d,j
1 . For each leaf node t in T , generate its distribution over terms φt ∼ Dir(β ) ;
2 . For each document d ∈ [ D ] :
( a ) For each non leaf node t in T , draw a multinomial dis tribution over its subtopics : θd,t ∼ Dir(αt→1 , . . . , αt→Ct ) ;
.
1227 Table 1 : Notations used in our model
Symbol Description the number of unique terms in the corpus
D the number of documents in the corpus V H the height of the topical hierarchy T τ Ct ld πt χt ∈ [ Cπt ] wd,j ∈ [ V ] zi d,j φt αt θd,t the total number of topics in the hierarchy the number of leaf topics in the hierarchy the number of child topics of topic t the length ( number of tokens ) of document d the parent topic of topic t the index of topic t among its siblings the j th token in the document d the child index of the topic at level i for wd,j topic t ’s multinomial distribution over terms the Dirichlet hyperparameter of topic t document d ’s distribution over t ’s child topics
θd,t
T − τ
α z1 d,j z2 d,j
··· zH d,j wd,j ld D
β
φt
τ
Figure 2 : Latent Dirichlet Allocation with Topic Tree
( b ) For each token index j ∈ [ ld ] of document d : i . i ← 0 ; ii . While o → z1 A . i ← i + 1 ; B . Draw subtopic zi d,j → ··· → zi iii . Generate token wd,j ∼ M ulti(φo→z1 d,j ∼ M ulti(θd,o→z1 d,j→···→zi−1 d,j ) . d,j→···→zi d,j d,j is not a leaf node :
Its graphical representation is Figure 2 . Table 1 collects the notations .
For every non leaf topic node , we can derive a term distribution by marginalizing their children ’s term distributions :
Ct
Ct
αt→z
αt
φt→z
φt = p(w = ·|t ) = p(t → z|t)p(w = ·|t → z ) = z=1 z=1
( 1 ) So in our model , the term distribution φt for an internal node in the topic hierarchy can be calculated as a mixture of its children ’s term distributions . The Dirichlet prior α determines the mixing weight . When the structure T is fixed , we need to infer its parameters φ(T ) and α(T ) from a given corpus . When the height of the hierarchy H = 1 , our model reduces to the flat LDA model . 412 Model Structure Manipulation The main advantage of this model is that it can be con sistently manipulated to accommodate user operations .
Proposition 1 . The following atomic manipulation operators are sufficient in order to compose all the user operations introduced in Section 3 : • EXP(t , k ) . Discover k subtopics of a leaf topic t .
• MER(t1 , t2 ) . Merge two topics t1 and t2 into a new topic t3 under their least common ancestor t .
• MOV(t1 , t2 ) . Move the subtree rooted at topic t1 to be under t2 .
The following are examples about how to use these manipulation operators to compose the user operations in Figure 1 . • ‘Collapse’ – applying MER(o , o → 1 ) three times . • ‘Split’ – EXP(o → 2 , 2 ) followed by MER(o , o → 2 ) . • ‘Remove’ – MER(o → 2 , o → 2 → 1 ) followed by MER(o , o →
2 ) .
Implementation of these atomic operators needs to follow the consistency requirement . 1 . Single run consistency – suppose the topical hierarchy T1 is altered into T2 after a user operation , certain nodes are not affected . For example , in the ‘merge’ operation in Figure 1 , node 0,1,2,3,5,7 are not touched . The consistency condition requires that , if we restart step 2 (b ) whenever we reach an affected node in step 2 (b) ii , T1 and T2 are equivalent generative models , ie , generate the same documents in expectation . By this definition , we have the following proposition .
Proposition 2 . A single run altering T1 into T2 is consistent if and only if i ) for each unaffected leaf node t , αt(T1 ) = αt(T2 ) , φt(T1 ) = φt(T2 ) ; and ii ) for each in ternal node t , αt ( T2 ) =Ct z=1 αt→z(T2 ) .
2 . Multi run consistency – with identical input across multiple runs , one operator should output nearly identical ( undifferentiated to human ) α and φ .
) ;
Section 4.2 presents a moment based method to compute these operators efficiently and consistently . 4.2 Moment based Operation
In statistics , the ξ th order population moment of a ranIn our dom variable is the expectation of its ξ th power . problem , the random variable is a token wd,j in a document d . The ξ th population moment is the expected cooccurrence of terms in ξ token positions . They are related to the model parameters α and φ . The method of moments collects empirical moments from the corpus , and estimate α and φ by fitting the empirical moments with theoretical moments . As a computational advantage , it only relies on the term co occurrence statistics . The statistics contain important information compressed from the full data , and require only a few scans of the data to collect .
To compute our three atomic operators , we generalize the notion of population moments . We consider the population moments conditioned on a topic t . The first order conditional moment E1(t ) is a vector in RV . Component x is the expectation of 1w=x given that w is drawn from topic t ’s descendant .
E1(t ) = p(w = ·|t , α ) = φt =
αt→z
αt
φt→z
( 2 )
The second order moment E2(t ) ∈ RV ×V is a V × V tensor ( hence , a matrix ) , storing the expectation of the cooccurrences of two terms w1 and w2 given that they are
Ct z=1
1228 both drawn from topic t ’s descendants . Integrating over the document topic distribution θ , we have : E2(t ) = p(w1 = · , w2 = ·|t , t , α )
( 3 )
αt→z1 αt→z2 αt(αt + 1 )
φt→z1 ⊗ φt→z2 +
αt→z(αt→z + 1 )
αt(αt + 1 )
⊗2 t→z
φ
= z1=z2
Ct z=1
The operator ⊗ denotes an outer product between tensors : if A ∈ Rs1×···×sp , and B ∈ Rsp+1×···×sp+q , then A⊗B is a tensor in Rs1×···×sp+q , and [ A⊗B]i1ip+q = Ai1ip Bip+1ip+q . Likewise , we can derive the third order moment E3(t ) ∈ RV ×V ×V ( a V × V × V tensor ) as the expectation of cooccurrences of three terms w1 , w2 and w3 given that they are all drawn from topic t ’s descendants : E3(t ) = p(w1 = · , w2 = · , w3 = ·|t , t , t , α )
+
=
φt→z1 ⊗ φt→z2 ⊗ φt→z3 z1=z2=z3=z1
αt→z1 αt→z2 αt→z3 αt(αt + 1)(αt + 2 ) αt→z1 αt→z2 ( αt→z1 + 1 )
+φt→z1 ⊗ φt→z2 ⊗ φt→z1 + φt→z2 ⊗ φt→z1 ⊗ φt→z1 ) Ct
αt→z(αt→z + 1)(αt→z + 2 )
αt(αt + 1)(αt + 2 ) z1=z2
( φt→z1 ⊗ φt→z1 ⊗ φt→z2
φ⊗3 t→z
+ z=1
αt(αt + 1)(αt + 2 )
Equations ( 2)–(4 ) characterize the theoretical conditional moments for topic t using model parameters associated with t ’s children . The empirical conditional moments can be estimated from data and parameters of t ’s ancestors .
For topic t , we estimate the empirical ‘topical’ count of
( 4 ) term x in document d as : cd,x(t ) = cd,xp(t|x ) = cd,x(πt )
Cπt
αtφt,x z=1 απt→zφπt→z,x
( 5 )
Recall that πt is t ’s parent . cd,x(t ) can be recursively computed through cd,x(πt ) and the boundary is cd,x(o ) = cd,x , ie , the total counts of term x in document d .
Then we can estimate empirical conditional moments us ing these empirical topical counts :
1
E1(t ) =
D D where ld(t ) =V
E2(t ) = d=1 d=1 cd(t ) ld(t )
1 ld(t)(ld(t ) − 1 )
[ cd(t ) ⊗ cd(t ) − diag(cd(t) ) ]
( 6 ) x=1 ci,x(t ) . These enable fast estimation of empirical moments by passing data once . gorithm that returns ( αt→z , φt→z ) , z ∈ [ k ] , withk
The following three subsections discuss the computation of the three atomic operators EXP , MER and MOV with the method of moments . 421 EXP Operator EXP(t , k ) should find k subtopics under topic t , without changing any existing model parameters . So we need an alz=1 αt→z = αt . By recursion , we note that only αo needs to be set by a user . It controls the degree of topical purity of documents . When αo → ∞ , each document is only about one leaf topic . In Equations ( 2)– ( 4 ) , we replace the left hand side with the empirical conditional moments estimated from the data . The right hand
We employ the method of moments .
2
Ct z=1
Ct z=1 side is theoretical moments with αt→z , φt→z , z ∈ [ k ] as unknown variables . Solving these equations yields a solution of the acquired model parameters . The following theorem by Anandkumar et al . [ 3 ] suggests that we only need to use up to 3rd order moments to find the solution .
Theorem 1 . Assume M2 and M3 are defined as : k z=1 k z=1
M2 =
⊗2 z , M3 =
λzv
⊗3 z
λzv
( 7 ) where λz > 0 , vz ’s are linearly independent , and vz = 1 . When M2 and M3 are given , vz and λz in Equation ( 7 ) can be uniquely solved in polynomial time .
To write Equations ( 2)–(4 ) in this form , we define : M2(t ) = ( αt + 1)E2(t ) − αtE1(t ) U1(t ) = E2(t ) ⊗ E1(t ) ,
⊗2
U2(t ) = Ω(U1(t ) , 1 , 3 , 2 ) , U3(t ) = Ω(U1(t ) , 2 , 3 , 1 )
M3(t ) =
( αt + 1)(αt + 2 )
E3(t ) + α2 t E
⊗3 1
2 − αt(αt + 1 )
[ U1(t ) + U2(t ) + U3(t ) ]
( 8 )
( 9 )
( 10 ) where Ω(A , a , b , c ) permutes the modes of tensor A , such that Ω(A , a , b , c)i1,i2,i3 = Aia,ib,ic . It follows that :
M2(t ) =
αt→z
αt
⊗2 t→z , M3(t ) =
φ
αt→z
αt
⊗3 t→z
φ
So they fit Equation ( 7 ) nicely , and intuitively . If we decompose M2(t ) and M3(t ) , the z th component is determined by the child ’s term distribution φt→z , and its weight is αt→z , which is equal to p(t → z|t ) . αt M2(t ) is a dense V × V matrix , and M3(t ) is a dense V × V ×V tensor . Direct application of the tensor decomposition algorithm in [ 3 ] is challenging due to the creation of these huge dense tensors . Therefore , we design a more scalable algorithm . The idea is to bypass the creation of M2(t ) and M3(t ) and utilize the sparsity and decoupled decomposition of the moments . We go over Algorithm 1 to explain it . the data .
Line 1.1 collects the empirical moments with one scan of Lines 1.2 to 1.6 project the large tensor M3 ∈ RV ×V ×V into a smaller tensor T ∈ Rk×k×k . T is not only of smaller T = k ⊗3 . vz , z ∈ [ k ] are orthonormal vectors in size , but also can be decomposed into an orthogonal form :
Rk . This is assured by the whitening matrix W calculated in Line 1.5 , which satisfies W T M2W = I . This part contains two major tricks : z=1(cid:102)λzvz
1 . When calculating W , the straightforward computation requires spectral decomposition of M2 . We avoid explicit creation of M2 , but achieve the equivalent spectral decomposition . We first perform spectral decomposition for E2(t ) = U Σ1U T , where U ∈ RV ×k is the matrix of k eigenvectors , and Σ1 ∈ Rk×k is the diagonal eigenvalue matrix . The k column vectors of U form an orthonormal basis of the column space of E2(t ) . E1(t ) ’s representation in this basis is M1 = U T E1(t ) . According to Equation ( 8 ) , M2 can now be written as :
M2 = U [ (αt + 1)Σ1 − αtM1 ⊗ M1]U T
1229 2 = ( αt + 1)Σ1 − αtM1 ⊗ M1 , as M
So a second spectral decomposition can be performed on M 2 = UΣUT . Then we have U UΣ(U U)T as M2 ’s spectral decomposition . The space requirement is reduced from V 2 to m = E2(t)0 V 2 , because only term pairs ever cooccurring in one document contribute to non zero elements of E2(t ) . The time for spectral decomposition is reduced from O(V 2K ) to O(mK ) .
T = M3(t)(W , W , W ) using explicit M3(t ) and W requires
2 . The straightforward computation of the tensor product O(V 3 ) space and O(V 3K +Lˆl2 ) time , where ˆl is the maximal document length . We decouple M3(t ) as a summation of multiple tensors , such that the product between each tensor and W is in a decomposable form : either ( v ⊗ v ⊗ v)(W , W , W ) or ( v ⊗ B)(W , W , W ) , which can be computed as easily as ( W T v)⊗3 or ( W T v ) ⊗ ( W T BW ) .
T =
2
− αt(αt + 1 ) 1 D
E3(t ) =
( αt + 1)(αt + 2 )
2
E3(t)(W , W , W ) + α2 t ( W T E1(t ) )
⊗3
[ (U1 + U2 + U3)(W , W , W ) ] [ A1 − A2 − Ω(A2 , 2 , 1 , 3 ) − Ω(A2 , 2 , 3 , 1 ) + 2A3 ]
( 11 ) d=1
D D V d=1
D
A1(W , W , W ) =
A2(W , W , W ) =
A3(W , W , W ) =
U1(W , W , W ) = sd(t)(W T cd(t ) )
⊗3 sd(t)(W T cd(t ) ) ⊗ W T diag(cd(t))W sd(t)cd,x(t)(W T x )
⊗3 x=1 d=1 1
( αt + 1 )
[ I + αt(W T E1(t ) )
⊗2 ] ⊗ W T E1(t )
1 ld(t)[ld(t)−1][ld(t)−2 ] and W T where sd(t ) = x is the x th column of W T . U2(W , W , W ) and U3(W , W , W ) can be obtained by permuting U1(W , W , W ) ’s modes . The renovated procedure needs only one pass of data in O(LK 2 ) time .
Lines 1.7 to 1.14 perform orthogonal decomposition of T ( (cid:102)λz , vz ) are found one by one . To find one such pair , the algo via a power iteration method . The orthonormal eigenpairs rithm randomly starts with a unit norm vector v , runs power iteration ( Line 1.11 ) for n times , and records the candidate eigenpair . This process further repeats by N times , starting from various unit norm vectors . Line 1.12 picks the eigenpair with the largest eigenvalue . After an eigenpair is found , the tensor T is deflated by the found component ( Line 1.14 ) , eigenpair . After all the k orthonormal eigenpairs ( (cid:102)λz , vz ) are and the same power iteration is applied to it to find the next found , they can be used to uniquely determine the k target components ( αt→z , φt→z ) ( Line 113 )
Algorithm 1 : EXP(t , k )
1.1 Compute E1(t ) and E2(t ) according to Equation ( 6 ) ; 1.2 Find k largest orthonormal eigenpairs ( σz , µz ) , z ∈ [ k ] of E2 ; // U = [ µ1 , . . . , µk ] , Σ1 = diag(σ1 , . . . , σk ) 1.3 M1 = U E1(t ) ; 1.4 Compute spectral decomposition for
2 = ( αt + 1)Σ1 − αtM1 ⊗ M1 = UΣUT ; 1 2 ;
1.6 Compute T = M3(t)(W , W , W ) according to Equation ( 11 ) ;
2 , ( W T )+ = XΣ
1Σ− 1
M
1.5 X = U U , W = M 1.7 for z ∈ [ k ] do λ∗ ← 0 ; for outIter ∈ [ N ] do
1.9
1.8
// the largest eigenvalue so far
1.10
1.11
1.12
1.13
1.14
// N , n are a small constants v ← a random unit norm vector ; for innerIter ∈ [ n ] do v ← T ( I,v,v ) ||T ( I,v,v)|| ; if T ( v , v , v ) > λ∗ then ( λ∗ , v∗ ) ← ( T ( v , v , v),v ) ; T ← T − λ∗v∗ ⊗ v∗ ⊗ v∗ ; ( W T )+v∗ ;
( λ∗)2 , φt→z = αt→z
αt→z = αt
αt
1
// deflation
1.15 Compute cd(t → z ) , z ∈ [ k ] according to Equation ( 5 ) ;
( λz = αt→z αt iteration step of Line 1.11 converges in a quadratic rate .
, vz = φt→z ) with high probability . The power
It satisfies single run consistency . Multi run consistency is guaranteed if the empirical moments are close to theoretical moments . We empirically evaluate it in Section 5 . The overall time complexity for EXP is O(LK 2 + Km + N nK 4 ) , which can be regarded linear to the data size since N and n can be as small constants as 10 to 30 , and K is a small number like 10 to 50 due to our assumption of humanmanageable tree width . It requires only three scans of data . 422 MER Operator
Algorithm 2 : MER(t1 , t2 )
2.1 Find the least common ancestor t of t1 and t2 ; 2.2 t ← t1 ; 2.3 while t = t and πt = t do
2.4
2.5
2.6 t ← πt ; c(t ) ← c(t ) − c(t1 ) ; αt ← αt − αt1
2.7 t ← t2 ; 2.8 for t = t and πt = t do t ← πt ; c(t ) ← c(t ) − c(t2 ) ; αt ← αt − αt2
2.9
2.10
2.11
2.12 if t = t1 or t = t2 then t3 ← t ; 2.13 else 2.14
Create topic node t3 with parent t ; c(t3 ) ← c(t1 ) + c(t2 ) ; αt3 ← αt1 + αt2 ;
2.15
2.16
2.17 for z ∈ [ Ct1 ] do π(t1 → z ) ← t3 ; 2.18 for z ∈ [ Ct2 ] do π(t2 → z ) ← t3 ; 2.19 Remove t1 and t2 from T , and add t3 to T ;
Line 1.15 computes the empirical topical counts for the k inferred child topics . It requires one scan of the data .
The decomposition by Algorithm 1 is fast and unique with sufficient data .
Theorem 2 . Assume M2 and M3 are defined as in Equation ( 7 ) , λz > 0 , and vz ’s are linearly independent with unit norm , then Algorithm 1 finds exactly the same set of
To merge two topics t1 and t2 , we need to find their least common ancestor t ( Line 2.1 ) , subtract the topical counts c(t ) and the Dirichlet prior αt for any other topic t in the path between t1 and t2 ( Lines 22–211 ) , and then create a new node t3 to sum up the topical counts and Dirichlet prior of t1 and t2 ( Lines 214–216 ) with one exception : when t1 is t2 ’s direct ancestor or direct descendant , we can just use
1230 t as the merged topic node ( Line 212 ) We then move the children of t1 and t2 to be under the merged topic node ( Lines 217–218 ) Last , we remove t1 and t2 and add t3 to the topical hierarchy ( Line 219 ) The complexity for MER is O(LH ) .
423 MOV Operator
Algorithm 3 : MOV(t1 , t2 )
3.1 t ← πt1 ; 3.2 while t = o do c(t ) ← c(t ) − c(t1 ) ; αt ← αt − αt1 ; t ← πt ;
3.6 t ← t2 ; 3.7 while t = o do
3.3
3.4
3.5
3.8
3.9
3.10 c(t ) ← c(t ) + c(t1 ) ; αt ← αt + αt1 ; t ← πt ;
3.11 Set πt1 ← t2 ;
To move the subtree rooted at t1 to be under t2 , we first subtract topical counts and Dirichlet prior from every ancestor of t1 ( Lines 31–35 ) , and then add them to every ancestor of t2 , including t2 itself ( Lines 36–310 ) Finally , we set the parent of t1 to be t2 . The complexity for MOV is O(LH ) .
The implementation of MER and MOV using Algorithms 2 and 3 satisfy both multi run and single run consistency requirement . 4.3 Phrase Mining and Ranking
After the term distribution in each topic is inferred , we can then mine and rank topical phrases within each topic . The phrase mining and ranking in STROD adapt CATHY [ 27 ] to generic text . Here we briefly present the process .
In this work , a phrase is defined as a frequent consecutive sequence of terms of arbitrary lengths . To filter out incomplete phrases ( eg , ‘vector machine’ instead of ‘support vector machine’ ) and frequently co occurred terms that do not make up a meaningful phrase ( eg , ‘often use’ ) , we use a statistical test to select quality phrases [ 7 ] , and record the count cd,P of each phrase P in each document d .
After the phrases of mixed lengths are mined , they are ranked with regard to the representativeness of each topic in the hierarchy , based on two factors : popularity and discriminativeness . A phrase is popular for a topic if it appears frequently in documents containing that topic ( eg , ‘information retrieval’ has better popularity than ‘cross language information retrieval’ in the Information Retrieval topic ) . A phrase is discriminative of a topic if it is frequent only in the documents about that topic but not in those about other topics ( eg , ‘query processing’ is more discriminative than ‘query’ in the database topic ) .
We use the topical term distributions inferred from our model to estimate the ‘topical count’ cd,P ( t ) of each phrase P in each document d , in a similar way as we estimate the topical count of terms in Equation ( 5 ) : cd,P ( t ) = cd,P ( πt)p(t|P , πt ) = cd,P ( πt )
Cπt
αt x∈P φt,x z=1 απt→z x∈P φπt→z,x
1 D
D
Let the conditional probability p(P|t ) be the probability of “ randomly choose a document and a phrase that is about topic t , the phrase is P . ” It can be estimated as p(P|t ) = cd,P ( t ) P cd,P ( t ) . The popularity of a phrase P in a topic t can be quantified by p(P|t ) . The discriminativeness can be measured by the log ratio between the probability p(P|t ) conditioned on topic t and the probability p(P|πt ) conditioned on its parent topic πt : log p(P|t ) p(P|πt ) . d=1
A good ranking function to combine these two factors is their product : rt(P ) = p(P|t ) log p(P|t ) p(P|πt )
( 12 ) which has an information theoretic sense : the pointwise KLdivergence between the two probabilities [ 27 ] . Finally , we use rt(P ) to rank phrases in topic t in the descending order .
5 . EXPERIMENTS
In this section we first introduce the datasets and the methods used for comparison , and then describe our evaluation on efficiency , consistency , and quality .
Datasets . Our performance study is on four datasets : • DBLP title : A set of titles of recently published papers in DBLP ( wwwdblporg ) The set has 1.9M titles , 152K unique terms , and 11M tokens .
• CS abstract : A dataset of computer science paper abstracts from Arnetminer ( wwwarnetminerorg ) The set has 529K papers , 186K unique terms , and 39M tokens . • TREC AP news : A TREC news dataset ( 1998 ) . It has 106K full articles , 170K unique terms , and 19M tokens . • Pubmed abstract : A dataset of life sciences and biomedical topic . We crawled 1.5M abstracts from Jan . 2012 to Sep . 2013 on Pubmed ( wwwncbinlmnihgov/pubmed ) The dataset has 98K unique terms and 169M tokens .
We remove English stopwords from all the documents . Methods for comparison . We mainly evaluate EXP because it dominates the runtime , and its consistency in realworld data is subject to empirical evaluation . We compare the following topical hierarchy construction methods . • hPAM – parametric hierarchical topic model . The hierarchical Pachinko Allocation Model [ 20 ] is a state of the art parametric hierarchical topic modeling approach . hPAM outputs a specified number of supertopics and subtopics , as well as the associations between them . • nCRP – nonparametric hierarchical topic model . We choose nCRP to represent this category for its relative efficiency . It outputs a tree with a specified height . The number of topics cannot be set exactly . We tune its hyperparameter to generate an approximately identical number of topics as other methods . • splitLDA – recursively applying LDA , as discussed in Section 2 . This heuristic method is more efficienct than the above two methods . We implement splitLDA on top of an efficient single machine LDA inference algorithm [ 30 ] . • CATHY – recursively clustering term co occurrence networks . CATHY [ 27 ] uses a term co occurrence network to compress short documents and performs topic discovery through an EM algorithm .
1231 slower than STROD due to many rounds of EM iterations . splitLDA and hPAM rely on Gibbs sampling , and the former is faster because it recursively performs LDA , and considers fewer dependencies in sampling . nCRP is two orders of magnitude slower .
We then conduct analytical study of the runtime growth with respect to different factors . Figures 4a–4c show the runtime varying with the number of tokens , the tree height and the tree width . We can see that the runtime of STROD grows slowly , and it has the best performance in all occasions . The margin of our method over others grows quickly when the scale increases . In Figure 4b , we exclude hPAM because it is designed for H = 2 . We exclude nCRP from all these experiments because it takes too long time to finish ( >90 hours with 600K tokens ) .
Figure 4d shows the performance in comparison with the variations of STROD . Both RTOD and RTOD2 fail to finish when the vocabulary size grows beyond 1K , because the third order moment tensor M3(t ) requires O(V 3 ) space to create . RTOD3 also has limited scalability because the second order moment tensor M2(t ) ∈ RV ×V is dense . STROD scales up easily by avoiding explicit creation of these tensors . 5.2 Consistency
The second evaluation assesses the multi run consistency of different algorithms . For each dataset , we sample 10,000 documents and run each algorithm 10 times and measure the variance among the 10 runs for the same method as follows . Each pair of algorithm runs generate the same number of topics , but their correspondence is generally unknown ( STROD makes an exception with its ability to obtain a unique order of subtopics according to learned α ) . For example , the topic o → 1 in the first run may be close to o → 3 in the second run . We measure the KL divergence between all pairs of topical term distributions between the two runs , build a bipartite graph using the negative KL divergence as the edge weight , and then use a maximum matching algorithm to determine the best correspondence ( top down recursively ) . Then we average the KL divergence between matched pairs as the difference between the two algorithm runs . Finally , we average the difference between all 10 × 9 = 90 ordered pairs of algorithm runs as the final variance . We exclude nCRP in this section , since even the number of topics is not a constant after each run .
Table 2 summarizes the results : STROD has lowest variance in all the three datasets . The other three methods based on Gibbs sampling have variance larger than 1 in all datasets , which implies that the topics generated across multiple algorithm runs are considerably different .
We also evaluate the variance of STROD when we vary the number of outer and inner iterations N and n . As shown in Figure 5 , the variance of STROD quickly diminishes when the number of outer and inner iterations grow to 10 . This validates the theoretical analysis of their fast convergence . In conclusion , STROD achieves consistent performance with small runtime . It is stable and robust to be used as a hierarchy construction method for large text collections . 5.3 Interpretability
The final evaluation assesses the interpretability of the constructed topical hierarchy , via human judgment . We evaluate hierarchies constructed from DBLP titles and TREC AP news . For simplicity , we set the number of subtopics to
Figure 3 : Total runtime on each dataset , H = 2 , Ct = 5
( a ) Runtime wrt corpus size ( # tokens )
( b ) Runtime wrt tree height
( c ) Runtime wrt # children for each topic
( d ) STROD vs its variations ( All except STROD fail to scale beyond 50K vocabulary size )
Figure 4 : Runtime with varying scale
• STROD and its variations RTOD , RTOD2 , RTOD3 – recursively applying our EXP operator to expand the tree . We implement several variations to analyze our scalability improvement techniques : ( i ) RTOD : recursive tensor orthogonal decomposition without scalability improvement [ 3 ] ; ( ii ) RTOD2 : RTOD plus the efficient computation of whitening matrix by avoiding creation of M2 ; ( iii ) RTOD3 : RTOD plus the efficient computation of tensor product by avoiding creation of M3 ; and ( iv ) STROD : Algorithm 1 with the full scale up technique .
5.1 Efficiency
The first evaluation assesses the efficiency of different algorithms when constructing a topical hierarchy with the same depth and width .
Figure 3 shows the overall runtime in these datasets . STROD is several orders of magnitude faster than the existing methods . On the largest dataset it reduces the runtime from one or more days to 18 minutes in total . CATHY is the second best method in short documents such as titles and abstracts because it compresses the documents into term co occurrence networks . But it is still more than 100 times
246810x 1060200040006000800010000number of tokensruntime ( seconds ) hPAMsplitLDACATHYSTROD12340500100015002000tree heightruntime ( seconds ) splitLDACATHYSTROD2345678910050010001500200025003000number of children of each topicruntime ( seconds ) hPAMsplitLDACATHYSTROD102103104105100101102103vocabulary size ( in log scale)runtime ( seconds in log scale ) RTOD/RTOD2RTOD3STROD1232 Table 2 : The variance of multiple algorithm runs in each dataset
Method
DBLP title CS abstract TREC AP news hPAM splitLDA CATHY STROD
5.578 3.393 17.34 0.6114
5.715 1.600 1.956
5.890 1.578 1.418
0.0001384
0.004522
( a ) varying N when n = 30
( b ) varying n when N = 30
Figure 5 : The variance and runtime of STROD when varying # outer and inner iterations N and n ( CS abstract ) be 5 for all topics . For hPAM , we post process them to obtain the 5 strongest subtopics for each topic . For all the methods we use the same phrase mining and ranking procedure to enhance the interpretability . We do not include nCRP in this study because hPAM has been shown to have superior performance of it [ 20 ] .
In order to evaluate the coherence of the hierarchy , we use an Topic Intrusion ( TI ) task which were proposed in [ 27 ] : Evaluators are shown a parent topic t and X candidate child topics . X − 1 of the child topics are actual children of t in the generated hierarchy , and the remaining child topic is not . Each topic is represented by its top 5 ranked phrases . Evaluators are asked to select the intruder child topic , or to indicate that they are unable to make a choice .
For this study we set X = 4 . 160 Topic Intrusion questions are randomly generated . We then calculate the agreement of the human choices with the actual hierarchical structure constructed by the various methods . We consider a higher match between a given hierarchy and human judgment to imply a higher quality hierarchy . For each method , we report the F 1 measure of the answers matched consistently by three human judgers with CS background .
Figure 6 summarizes the results . STROD is the best performing method in both datasets . This suggests that the quality of the hierarchy is not compromised by the strong efficiency and consistency of STROD . As the tree goes deeper , splitLDA degrades in quality due to inclusion of irrelevant portion of each document . Compared to splitLDA , STROD does not assign a document entirely to a topic . In addition , STROD has a theoretically guaranteed inference method for expansion , which may also account for the superior quality . A subset of the hierarchy constructed from CS abstract by ‘Expand’ is presented in Figure 7 . For each non root node , we show the top ranked phrases . Node o → 1 is about ‘data’ , while its children involve database , data mining and bioinformatics . The lower the level is , the more specific the topic is , and the more multigrams emerge ahead of unigrams in general . This initial hierarchy helps users quickly see the main topics without going through all the documents . They can then use other operators to make small changes to the hierarchy to confidently and continuously refine the quality .
Figure 6 : Topic intrustion study
6 . DISCUSSIONS
In this work , we tackle the efficiency and consistency challenge of interactive topical hierarchy construction from largescale text data . We design a novel moment based framework to build the hierarchy recursively . Our framework divides the construction task into simpler operations in which users can be interactively involved . To support these operations , we design a new model for topical hierarchy which can be learned recursively . For consistent inference , we extend a theoretically guaranteed tensor orthogonal decomposition technique to this model . Utilizing the special structure of the tensor in our task , we scale up the algorithm significantly . By evaluating our approach on a variety of datasets , we demonstrate a prominent computational advantage . Our algorithm generates consistent and quality topic hierarchy 100 1000 times faster than the state of the art , and the margin grows when the corpus size increases .
This invention opens up numerous possibilities for future work . On the application side , it is foundation for building new systems to support explorative generation of textual data catalogs . Existing choice is either fully manual or fully automatic . The former is high quality but labor expensive , and the latter is the opposite . By adding interaction capability to automated methods , there is hope to reduce human effort and meanwhile allow users to have quality control . On the methodology side , the advantage of STROD can be further fulfilled by parallelization and adaptation to dynamic text collections .
7 . ACKNOWLEDGMENTS
Research was sponsored in part by the US Army Research
Lab . under Cooperative Agreement No . W911NF 09 2 0053 ( NSCTA ) , National Science Foundation IIS 1017362 , IIS 1320617 , and IIS1354329 , HDTRA1 10 1 0120 , and grant 1U54GM114838 awarded by NIGMS through funds provided by the trans NIH Big Data to Knowledge ( BD2K ) initiative ( wwwbd2knihgov ) , and MIAS , a DHS IDS Center for Multimodal Information Access and Synthesis at UIUC .
8 . REFERENCES
[ 1 ] A . Ahmed , Q . Ho , C . H . Teo , J . Eisenstein , E . P . Xing , and
A . J . Smola . Online inference for the infinite topic cluster model : Storylines from streaming text . In AISTATS , 2011 .
[ 2 ] A . Ahmed , L . Hong , and A . Smola . Nested chinese restaurant franchise process : Applications to user tracking and document modeling . In ICML , 2013 .
[ 3 ] A . Anandkumar , R . Ge , D . Hsu , S . M . Kakade , and
M . Telgarsky . Tensor decompositions for learning latent variable models . Journal of Machine Learning Research , 15:2773–2832 , 2014 .
[ 4 ] D . M . Blei and J . D . Lafferty . Visualizing Topics with
Multi Word Expressions . arXiv:0907.1013 , 2009 .
[ 5 ] D . M . Blei , A . Y . Ng , and M . I . Jordan . Latent dirichlet allocation . Journal of Machine Learning Research , 3:993–1022 , 2003 .
010203000204avg KL−divergence# outer iterations N 01020302040secondsvarianceruntime0102030000501015avg KL−divergence# inner iterations n 010203020304050secondsvarianceruntime1233 Figure 7 : Sample of hierarchy generated by STROD ( two phrases only differing in plural/single forms are shown only once )
[ 6 ] M . Danilevsky , C . Wang , N . Desai , J . Guo , and J . Han .
[ 19 ] X . Liu , Y . Song , S . Liu , and H . Wang . Automatic taxonomy
Automatic construction and ranking of topical keyphrases on collections of short documents . In SDM , 2014 . construction from keywords . In KDD , 2012 .
[ 20 ] D . Mimno , W . Li , and A . McCallum . Mixtures of hierarchical
[ 7 ] A . El Kishky , Y . Song , C . Wang , C . R . Vossand , and J . Han . topics with pachinko allocation . In ICML , 2007 .
Scalable topical phrase mining from text corpora . VLDB , 2015 .
[ 8 ] J . Foulds , L . Boyles , C . DuBois , P . Smyth , and M . Welling . Stochastic collapsed variational bayesian inference for latent dirichlet allocation . In KDD , 2013 .
[ 9 ] B . C . Fung , K . Wang , and M . Ester . Hierarchical document clustering using frequent itemsets . In SDM , 2003 .
[ 21 ] D . Newman , A . Asuncion , P . Smyth , and M . Welling .
Distributed algorithms for topic models . The Journal of Machine Learning Research , 10:1801–1828 , 2009 .
[ 22 ] I . Porteous , D . Newman , A . Ihler , A . Asuncion , P . Smyth , and
M . Welling . Fast collapsed gibbs sampling for latent dirichlet allocation . In KDD , 2008 .
[ 10 ] T . Griffiths , M . Jordan , J . Tenenbaum , and D . M . Blei .
[ 23 ] J . Pujara and P . Skomoroch . Large scale hierarchical topic
Hierarchical topic models and the nested chinese restaurant process . NIPS , 2004 . models . In NIPS Workshop on Big Learning , 2012 .
[ 24 ] A . Smola and S . Narayanamurthy . An architecture for parallel
[ 11 ] T . L . Griffiths and M . Steyvers . Finding scientific topics . Proceedings of the National academy of Sciences of the United States of America , 101(Suppl 1):5228–5235 , 2004 . [ 12 ] M . Hoffman , D . Blei , C . Wang , and J . Paisley . Stochastic variational inference . Journal of Machine Learning Research , 14:1303–1347 , 2013 .
[ 13 ] T . Hofmann . Unsupervised learning by probabilistic latent semantic analysis . Machine Learning , 42(1 2):177–196 , Jan . 2001 .
[ 14 ] J . H . Kim , D . Kim , S . Kim , and A . Oh . Modeling topic hierarchies with the recursive chinese restaurant process . In CIKM , 2012 .
[ 15 ] D . Lawrie and W . B . Croft . Discovering and comparing topic hierarchies . In Proc . RIAO , 2000 .
[ 16 ] A . Li , A . Ahmed , S . Ravi , and A . J . Smola . Reducing the sampling complexity of topic models . In KDD , 2014 .
[ 17 ] W . Li and A . McCallum . Pachinko allocation : Dag structured mixture models of topic correlations . In ICML , 2006 . topic models . Proceedings of the VLDB Endowment , 3(1 2):703–710 , 2010 .
[ 25 ] D . Sontag and D . Roy . Complexity of inference in latent dirichlet allocation . In NIPS , pages 1008–1016 , 2011 .
[ 26 ] H . M . Wallach . Topic modeling : beyond bag of words . In
ICML , 2006 .
[ 27 ] C . Wang , M . Danilevsky , N . Desai , Y . Zhang , P . Nguyen ,
T . Taula , and J . Han . A phrase mining framework for recursive construction of a topical hierarchy . In KDD , 2013 .
[ 28 ] C . Wang , X . Yu , Y . Li , C . Zhai , and J . Han . Content coverage maximization on word networks for hierarchical topic summarization . In CIKM , 2013 .
[ 29 ] X . Wang , A . McCallum , and X . Wei . Topical n grams : Phrase and topic discovery , with an application to information retrieval . In ICDM , 2007 .
[ 30 ] L . Yao , D . Mimno , and A . McCallum . Efficient methods for topic model inference on streaming document collections . In KDD , 2009 .
[ 18 ] R . V . Lindsey , W . P . Headden , III , and M . J . Stipicevic . A
[ 31 ] K . Zhai , J . Boyd Graber , N . Asadi , and M . L . Alkhouja . Mr . phrase discovering topic model using hierarchical pitman yor processes . In EMNLP CoNLL , 2012 . lda : A flexible large scale topic modeling package using variational inference in mapreduce . In WWW , 2012 . ooÅ1data,datamining,datasets,sets,structures,datastructures,database,analysis,applicationsoÅ2problem,algorithm,time,realtime,set,number,show,solution,graphoÅ3method,based,proposed,experimental results,features,results,oÅ1Å1skyline , nearestneighbor,locality sensitivehashing lsh,olap , declusteringoÅ1Å2queries , cache , expression,disk , geneexpression , genes,caching , accesses , localityoÅ1Å3data , datamining , datasets , datastructures , stream,datacollected , datasets,oÅ1Å1Å1skyline , locality sensitivehashing lsh , multidimensionalscaling , skylinepoints,oÅ1Å1Å2olap , udfs , rolap , lsh , udf , magnetictape , tpc , rknn , sdss , materialized , molapoÅ1Å2Å1geneexpression , genes , proteins , genome , expression , dna , samples , biologicaloÅ1Å2Å2cache , locality , cachemisses , prefetching , caching , heap , accesses , cachecoherenceoÅ1Å3Å1datamining , streams , datastreams , data , datasets , rawdata , dataprocessingoÅ1Å3Å2olap , datacube , multidimensionaldata , datawarehouse , olap queries,lineanalyticalprocessing olap , dataaggregation , datawarehousing , cube , olap systemsoÅ2Å1graph , edges , planar , wileyperiodicals , vertices , graphbased , directed , shortestpath,conjecture , trees , acyclicoÅ2Å2geneticalgorithmga , gene ticalgorithm , antcolony , part icleswarm , optimization psogreedy algorithm , steiner treeoÅ2Å1Å1graphs , planar graphs , directed graphs , bipartitegraphs , randomgraphs , undirectedgraphs , generalgraphs , subgraphs , combin , regular graphs , cayley , classgraphs , bipartiteoÅ2Å1Å2edges , vertices , spanning tree , shortestpath , digraph , number vertices , number edgesoÅ2Å2Å1steiner tree , nonnegativematrix factorization nmf , spanning tree , phylogenetictreeoÅ2Å2Å2bp decoding , crossover mutation , emo algorithms , shuffled , antcolony , colonyoÅ3Å1features , learning , class ification , objects , face , info rmation , based , training,oÅ3Å2images , regions , segment ation , imageprocessing , color,shape , motion , scene , noise,oÅ3Å1Å1classifier , recognition , classification , supportvector machinesvm , featureselection,oÅ3Å1Å2documents , text , retrieval , indexing , information retrieval , words , corpusoÅ3Å1Å3face , facerecognition , faceface , illumination , objectrecognition , facial , eyesoÅ3Å2Å1color , color images , color texture , video compression , skin , illumination , histogramoÅ3Å2Å2segmentation , imagesegmentation , segmented , contour , motion , video sequencesooÅ1data,datamining,datasets,sets,structures,datastructures,database,analysis,applicationsoÅ2problem,algorithm,time,realtime,set,number,show,solution,graphoÅ3method,based,proposed,experimental results,features,results,oÅ1Å1skyline , nearestneighbor,locality sensitivehashing lsh,olap , declusteringoÅ1Å2queries , cache , expression,disk , geneexpression , genes,caching , accesses , localityoÅ1Å3data , datamining , datasets , datastructures , stream,datacollected , datasets,oÅ1Å1Å1skyline , locality sensitivehashing lsh , multidimensionalscaling , skylinepoints,oÅ1Å1Å2olap , udfs , rolap , lsh , udf , magnetictape , tpc , rknn , sdss , materialized , molapoÅ1Å2Å1geneexpression , genes , proteins , genome , expression , dna , samples , biologicaloÅ1Å2Å2cache , locality , cachemisses , prefetching , caching , heap , accesses , cachecoherenceoÅ1Å3Å1datamining , streams , datastreams , data , datasets , rawdata , dataprocessingoÅ1Å3Å2olap , datacube , multidimensionaldata , datawarehouse , olap queries,lineanalyticalprocessing olap , dataaggregation , datawarehousing , cube , olap systemsoÅ2Å1graph , edges , planar , wileyperiodicals , vertices , graphbased , directed , shortestpath,conjecture , trees , acyclicoÅ2Å2geneticalgorithmga , gene ticalgorithm , antcolony , part icleswarm , optimization psogreedy algorithm , steiner treeoÅ2Å1Å1graphs , planar graphs , directed graphs , bipartitegraphs , randomgraphs , undirectedgraphs , generalgraphs , subgraphs , combin , regular graphs , cayley , classgraphs , bipartiteoÅ2Å1Å2edges , vertices , spanning tree , shortestpath , digraph , number vertices , number edgesoÅ2Å2Å1steiner tree , nonnegativematrix factorization nmf , spanning tree , phylogenetictreeoÅ2Å2Å2bp decoding , crossover mutation , emo algorithms , shuffled , antcolony , colonyoÅ3Å1features , learning , class ification , objects , face , info rmation , based , training,oÅ3Å2images , regions , segment ation , imageprocessing , color,shape , motion , scene , noise,oÅ3Å1Å1classifier , recognition , classification , supportvector machinesvm , featureselection,oÅ3Å1Å2documents , text , retrieval , indexing , information retrieval , words , corpusoÅ3Å1Å3face , facerecognition , faceface , illumination , objectrecognition , facial , eyesoÅ3Å2Å1color , color images , color texture , video compression , skin , illumination , histogramoÅ3Å2Å2segmentation , imagesegmentation , segmented , contour , motion , video sequencesooÅ1data,datamining,datasets,sets,structures,datastructures,database,analysis,applicationsoÅ2problem,algorithm,time,realtime,set,number,show,solution,graphoÅ3method,based,proposed,experimental results,features,results,oÅ1Å1skyline , nearestneighbor,locality sensitivehashing lsh,olap , declusteringoÅ1Å2queries , cache , expression,disk , geneexpression , genes,caching , accesses , localityoÅ1Å3data , datamining , datasets , datastructures , stream,datacollected , datasets,oÅ1Å1Å1skyline , locality sensitivehashing lsh , multidimensionalscaling , skylinepoints,oÅ1Å1Å2olap , udfs , rolap , lsh , udf , magnetictape , tpc , rknn , sdss , materialized , molapoÅ1Å2Å1geneexpression , genes , proteins , genome , expression , dna , samples , biologicaloÅ1Å2Å2cache , locality , cachemisses , prefetching , caching , heap , accesses , cachecoherenceoÅ1Å3Å1datamining , streams , datastreams , data , datasets , rawdata , dataprocessingoÅ1Å3Å2olap , datacube , multidimensionaldata , datawarehouse , olap queries,lineanalyticalprocessing olap , dataaggregation , datawarehousing , cube , olap systemsoÅ2Å1graph , edges , planar , wileyperiodicals , vertices , graphbased , directed , shortestpath,conjecture , trees , acyclicoÅ2Å2geneticalgorithmga , gene ticalgorithm , antcolony , part icleswarm , optimization psogreedy algorithm , steiner treeoÅ2Å1Å1graphs , planar graphs , directed graphs , bipartitegraphs , randomgraphs , undirectedgraphs , generalgraphs , subgraphs , combin , regular graphs , cayley , classgraphs , bipartiteoÅ2Å1Å2edges , vertices , spanning tree , shortestpath , digraph , number vertices , number edgesoÅ2Å2Å1steiner tree , nonnegativematrix factorization nmf , spanning tree , phylogenetictreeoÅ2Å2Å2bp decoding , crossover mutation , emo algorithms , shuffled , antcolony , colonyoÅ3Å1features , learning , class ification , objects , face , info rmation , based , training,oÅ3Å2images , regions , segment ation , imageprocessing , color,shape , motion , scene , noise,oÅ3Å1Å1classifier , recognition , classification , supportvector machinesvm , featureselection,oÅ3Å1Å2documents , text , retrieval , indexing , information retrieval , words , corpusoÅ3Å1Å3face , facerecognition , faceface , illumination , objectrecognition , facial , eyesoÅ3Å2Å1color , color images , color texture , video compression , skin , illumination , histogramoÅ3Å2Å2segmentation , imagesegmentation , segmented , contour , motion , video sequences→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→1234
