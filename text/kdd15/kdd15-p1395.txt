Entity Matching across Heterogeneous Sources
Yang Yang† , Yizhou Sun‡ , Jie Tang†(cid:63 ) , Bo Ma , and Juanzi Li† †Department of Computer Science and Technology , Tsinghua University
Tsinghua National Laboratory for Information Science and Technology ( TNList )
‡Department of Computer Science , Northeastern University Department of Computer Science , Carnegie Mellon University
{sherlockbourne , mabodx}@gmail.com , {jietang , lijuanzi}@tsinghuaeducn , yzsun@csneuedu
ABSTRACT Given an entity in a source domain , finding its matched entities from another ( target ) domain is an important task in many applications . Traditionally , the problem was usually addressed by first extracting major keywords corresponding to the source entity and then query relevant entities from the target domain using those keywords . However , the method would inevitably fails if the two domains have less or no overlapping in the content . An extreme case is that the source domain is in English and the target domain is in Chinese .
In this paper , we formalize the problem as entity matching across heterogeneous sources and propose a probabilistic topic model to solve the problem . The model integrates the topic extraction and entity matching , two core subtasks for dealing with the problem , into a unified model . Specifically , for handling the text disjointing problem , we use a cross sampling process in our model to extract topics with terms coming from all the sources , and leverage existing matching relations through latent topic layers instead of at text layers . Benefit from the proposed model , we can not only find the matched documents for a query entity , but also explain why these documents are related by showing the common topics they share . Our experiments in two real world applications show that the proposed model can extensively improve the matching performance ( +19.8 % and +7.1 % in two applications respectively ) compared with several alternative methods .
Categories and Subject Descriptors H28 [ Database Applications ] : Data Mining ; H33 [ Information Search and Retrieval ] : Retrieval Models
Keywords Heterogeneous sources ; Cross lingual matching ; Topic model
1 .
INTRODUCTION
With the rapid growth of the Web , including online digital libraries , online social and information networks , and E commerce systems , the Web provides abundant information to describe entities from different sources . Given an entity in a source domain , finding its matched entities from another ( target ) domain is an important task in many applications . For example , a patent expert may be interested in finding related patents in a patent database for a product ; a user may be interested in finding all the related Chinese Wiki pages for a particular English Wiki page ; and a doctor may be interested in finding all related drugs for a specific disease . Similar search problems can be found in many other applications . The problem can be generalized as an entity matching problem across corpora from heterogeneous sources . In other words , given an entity ( eg , product ) in one source , the goal is to find related entities ( eg , patents ) from a different source . Despite many studies on entity matching tasks [ 23 , 22 , 3 , 13 , 23 ] . Different from traditional search tasks , one key challenge of such problem is that different sources of corpora may use rather different languages or terminologies even when describing the same topic . For example , the terms used to express the same topic about Siri , are quite different in Wikipedia and patents . As Figure 1 ( a ) shows , the Siri Wiki article uses more daily expressions ( eg , “ voice control , ” “ personal assistant , ” “ iPhone , ” etc . ) to describe Siri , in order to make it easier to understand by everyone . However , more professional and technical terms are used in patents ( eg , “ information retrieval , ” “ heuristic modules , ” “ computer readable medium , ” etc ) The descriptions of two related entities from different sources can be very dissimilar in terms of their text similarity , and thus the traditional text based search can no longer solve the problem . In addition , for each relevant entity , it would be interesting to know on which topic the target entity is relevant to the source entity . For example , as shown in Figure 1 ( a ) , the patent “ Method for improving voice recognition ” is talking about “ voice control ” and its relevance probability to the source Wiki article on this topic is 0.83 , while the relevance probability of the second patent is 0.54 but on topic “ ranking ” .
One possible solution is to map two entities into the same latent topic space . Intuitively , two entities are relevant to each other if they refer to the same topic , eg , a Wiki article and a patent article should be relevant if they are both talking about the topic of Siri . A topic in such case should contain terms from heterogeneous sources . For example , the topic of Siri should contain both the general terms in Wiki and the special terms in the related patents . If we can extract hidden topics from heterogeneous sources , we will be able to infer the relevance score between two entities . However , for most topic modeling methods , such as PLSA [ 11 ] and LDA [ 4 ] , they do not deal with the issue of heterogeneous sources and are not able to generate topics with terms from different sources , since these terms seldom appear in the same entities .
In this paper , we propose a novel probabilistic model , CrossSource Topic ( CST ) model , to solve the entity matching problem
1395 Figure 1 : ( a ) An example of the entity between Wiki articles and patents . The rectangle on the left side represents the Wiki article which gives a general description of Siri . The rectangles on the right side denote patents reporting related technologies to Siri . Titles and high frequency phrases in the entities are shown in the rectangles . Links between the Wiki article and patents indicate their matching relations , with the topic relevance probability presented . ( b ) Product patent matching performance of LDA based methods and the proposed model ( CST ) . ( c ) Cross lingual matching performance of a method not considering latent topics , a LDA based method , and a CST based method . for a two source case , which integrates the topic extraction and entity matching into a unified model . We first ask the users to give a small portion of labels indicating the matching between entities from heterogeneous sources . Then we model both the hidden topics and the entity matching in a unified framework , where a topic contains terms from heterogeneous sources and the entity matching is determined by the topic distributions of the two entities . By using this model , we can not only find the matched entities for a query entity , but also explain why these entities are related by showing the common topics they share . It turns out that our model can successfully overcome the little text overlap problem across heterogeneous corpus sources , by modeling a topic with terms coming from all the sources and utilizing the matching labels for entities across different sources . A mean field variational inference [ 28 , 12 ] method is used to learn the model , which can be used to infer the matching relation between entities with no labels .
We evaluate the CST model in two real scenarios : 1 ) given a Wiki article describing a specific product , searching patents in the online patent database USPTO1 that are related to the same product ; 2 ) given an English Wikipedia article , searching the corresponding article from the Chinese Wiki knowledge base . Figure 1 ( b) (c ) show the experimental results in each scenario respectively , from which we can see that the proposed model extensively improve the performance ( averagely +19.8 % and +7.1 % in two real scenarios respectively ) .
In all , our contributions of this paper are summarized in the fol lowing .
• We identify and formalize a new problem called entity matching across heterogeneous sources , which is important and useful in this age of plentiful online open sources from different domains . To the best of our knowledge , no previous work has extensively studied this problem .
1http://wwwusptogov/
• We propose a novel and powerful probabilistic model , CrossSource Topic ( CST ) model , to solve the entity matching problem for a two source case , which integrates the topic extraction and matching into a unified model .
• We design an efficient variational inference based learning algorithm to learn the model and enable it scale to large scale data sets .
• We have demonstrated the power of our new method using two real world applications , compared with the state of theart baselines .
Organization Section 2 formulates the problem . Section 3 explains our proposed model , describes the algorithm for model learning , and introduces the applications of the model . Section 4 introduces our experiment that validates the effectiveness of our methodology , including its setup , baseline methods and results . Section 5 reviews some related work , and finally , Section 6 concludes this work .
2 . PROBLEM DEFINITION
In this section , we present related definitions and formulate the problem . We first give the formal definition of heterogeneous source corpus . Generally , a heterogeneous source corpus contains the descriptions of entities from multiple sources . However , to make the definition and the description of the proposed model clear , we use a dual source corpus as an instance in all related definitions . We leave the source extension as future work .
DEFINITION 1 . Dual Source Corpus . A dual source corpus C is a set of text collections {C1 , C2} from two sources with vocabulary Vt = {wt Nt} ( t ∈ {1 , 2} ) , where Ct = {dt Dt} is a collection of entities ( each entity is represented by a document describing it ) from source t , Dt is the number of entities in Ct , and Nt is the total number of words in
2 , , wt
2 , , dt
1 , wt
1 , dt
( a ) An example of the matching Wiki articles and patents(c ) Cross lingual matching(b ) Product patent matchingUniversal interface for retrieval of information in a computer systemrank candidatedescriptorsranking modulesearch enginerelevant areaobjectSource 2 : PatentsMethod for improvingvoice recognitionheuristic algorithmsspeech recognition distribution systemdata sourcetext to speechSiri ( Software)intelligent personal assistantknowledge navigatornatural language user interfaceiOSiPhoneiPadiPodvoice controlCydiaSource 1 : Siri's Wiki pageApple serverTopic : voice controlVoice menusystemmediagraphical user interfacesynchronizecustomizedprocessorhost devicedatabase083054Topic : ranking0.11Topic : processChallenge 1 : Less content overlapping between two sources;Challenge 2 : How to model the topic level relevance probability.1396 Vt . Following the common assumption of bag of words representation , each entity dt i in Ct can be represented as a bag of words {wt } , where N t i is the number of words in the entity dt i . i2 , , wt i1 , wt i
N t i
Given a dual source corpus , we can extract cross source topics , which contain terms from different sources :
DEFINITION 2 . Cross Source Topic . A cross source topic ϕ contains multiple multinomial distributions over words from different sources . For example , a 2 source topic contains two word distributions P1(w|ϕ ) and P2(w|ϕ ) , where Pt(w|ϕ ) defines the probability of a word w from source t ( t ∈ {1 , 2} ) appearing in this topic . Thus words with highest probabilities associated with each topic would suggest the semantics represented by the topic . pt(w|ϕ ) = 1 ( t ∈ {1 , 2} ) for any
Notice that we have w∈Vt cross source topic ϕ .
Next , we use a matching relation matrix to represent the correla tions between entities from different sources .
DEFINITION 3 . Matching Relation Matrix . A matching relation matrix L represents the matching status between entities in a dual source corpus C . If d1 j is matched , li,j = 1 , otherwise li,j = −1 . li,j = ? denotes that the value is missing and needs to be inferred . i and d2
Since entities from different sources may share few terms , the known values in the matching relation matrix are important guidance to extract the cross source topics and infer the missing values in the matrix . We can finally define the main problem addressed in this paper :
PROBLEM 1 . Entity Matching across Heterogeneous Given a heterogeneous source corpus C , and a The goal of cross source entity
Sources . matching relation matrix L . matching is to determine the missing values in L .
For example , we have a dual source corpus from Wikipedia and USPTO , the cross source entity matching problem is : given a Wiki article describing a specific product , finding patents from USPTO which report the technologies related with the product . As an example , given a Wiki article describing Siri , one of the matched patent could be the one claims on the technology about “ universal interface for retrieval of information , ” which is highly relevant to Siri .
Another example is cross lingual Wiki article matching . Given an English Wiki article , the task aims to find a Chinese Wiki article that reports the same content . Compared with cross lingual information retrieval problems , which mostly incorporate bilingual dictionaries , however , our problem is more general . Instead of using dictionaries , we focus on utilizing known relations to help extract cross source topics and infer unknown relations .
3 . CROSS SOURCE TOPIC MODEL
Modeling cross source matching entities is a challenging task . Intuitively , two entities are relevant if they refer to the same topic , and topic extraction will help us infer the connection between entities . However , due to the different terminologies used in different domains , word distributions of corpora from two sources may be quite different . In this situation , traditional topic modeling technologies would fail to identify the same topic from two sources but separate the topic into two or more , as shown in our Siri example
Figure 2 : Plate representation of the Cross Source Topic model . Modeling part for entities in source 2 has a symmetrical structure as source 1 . For simplicity , the modeling part for the entities in source 2 is omitted .
Table 1 : Notations in the CST model .
SYMBOL DESCRIPTION
ϕ1,z , ϕ2,z multinomial distribution over terminologies specific to
K D
D1 , D2 ld,d wd,j zd,j cd,j
θd
λd
α , β
γ ρ e1 , e2 the number of topics the total number entities the number of entities in source 1 and source 2 the value in the matching relation matrix , denotes whether d is matched with d the jth attribute ( word ) in entity d the topic assigned to attribute wd,j the latent variable assigned to zd,j , the value of cd,j can be d or the index of matched entities with d multinomial distribution over topics specific to entity d topic z in source 1 and 2 multinomial distribution over latent variables c specific to entity d Dirichlet priors to multinomial distributions θ and λ global regression parameter a function provides binary probabilities used to generate ld,d two constant values used to determine β ( weights of the prior for cross sampling ) in Figure 1 . In this paper , we propose a new semi supervised probabilistic model called Cross Source Topic ( CST ) model to capture the cross source topics and perform entity matching from different sources simultaneously . 3.1 Model Overview
Framework . The basic assumption of the proposed model is that , for entities from different sources , their matching relations and hidden topics are influenced by each other . Matching entities are similar in hidden space of topics , though the topics have different representations ( eg , word distributions ) in different sources , and vice versa , entities that are similar in hidden space of topics tend to be matched . Thus the basic idea here is to leverage the known matching relations to help the extraction of hidden topics , and use the extracted topics to infer the unknown relations .
Figure 2 shows the plate representation of the proposed semisupervised model . For simplicity , we omit the modeling part for the words in source 2 as it is the same as source 1 . Table 1 summarizes the notations used in the CST model .
In order to avoid pairwise relation modeling , before we use CST to model the generation of given entities and the generation of matching relations , we first process a candidate filtering . For the entities that have no chance to be matched with each other , CST will not model the relation generation for them . For example , given
1397 Input : a dual source corpus C , a matching relation matrix L , and hyper parameters α and β foreach entity d do
Generate θd ∼ Dir(α ) ; end % cross sampling based entity generation foreach d in each source t do
Set β according to Ld ; Generate λd ∼ Dir(β ) ; for n = 1 to Nd do
Generate cd,n ∼ Mult(λd ) , cd,n can be d or the index of matched entities with d ; Draw a topic zd,n ∼ Mult(θcd,n ) from the topic distribution of the entity c ; Draw a word wd,n ∼ Mult(ϕt,zd,n ) from zd,n specific word distribution ; end end % matching relation generation foreach ( d , d ) with possible links do Generate ld,d ∼ ρ(·|zd , zd , γ ) ; end
Algorithm 1 : Generative process for the CST model a Wiki article describing a product ( eg , iPhone ) , we only consider patents belonging to the company which creates this product ( eg , Apple ) in relation generation part . More general method to filter candidates is left as future work .
Cross Sampling . We then introduce an important concept in the CST model : cross sampling , which allows CST to leverage known relations and extract cross source topics . The idea of crosssampling is : when generating topics for an entity d , the sampling process is not only based on the topic distribution of d , but also the topic distributions of all the matching entities of d . The intuition behind the idea is that the matched entities are similar in hidden space of topics . For example , a user would like to edit a Chinese Wikipedia article about “ Barack Obama . ” Before he starts , he may take a look at what topics the corresponding English Wikipedia article contains , and finds out that the article contains Obama ’s early career as a Chicago community organizer . Thus he will edit the Chinese Wikipedia article to present Obama ’s experience as a community organizer but in different words . This process of crosssampling allows us to bridge the topics in entities from different sources and model the cross source topics .
By cross sampling , the CST model utilizes the known matching relations and makes the matching entities to have similar topic distributions . Similar ideas are proposed in some other models [ 8 , 10 , 17 ] . However , these unsupervised methods can hardly infer unknown relations in a unified model . As we will introduce later , CST employs a semi supervised learning algorithm to infer unknown relations . Another kind of linked topic models [ 7 , 20 , 21 ] are able to infer missing links between entities . However , they do not consider the direct effect of known links on hidden topics , and CST employs cross sampling to model a more explicit and high order dependency between matching entities . The more sufficient utilization of known relations makes the CST model more suitable for heterogeneous source corpses than traditional topic models ( experiments show that the CST model outperforms RTM , a traditional linked topic model , by 40.9 % on average ) .
Figure 3 : An intermediate step of cross sampling . There is a matching relation between d1 from source 1 and d2 from source 2 . Latent topics for a word w1 from d1 is sampled based on both d1 and d2 .
3.2 Generative Process
Formally , the generative process is described in Algorithm 1 . It consists of two parts : ( 1 ) cross sampling based entity generation and ( 2 ) matching relation generation .
Cross Sampling Based Entity Generation . Here , we introduce the entity generation in detail . First , for each entity d in source 1 , we sample its topic distribution θd : θd ∼ Dir(α ) . Next , for each word w in d , we choose a topic z : z ∼ Mult(θc ) , where c could be d itself or one of d ’s matching entities . We sample c according to c ∼ Mult(λd ) , where λd indicates how likely an entity matched with d ( including d itself ) will be sampled . λd is sampled according to λd ∼Dir(βd ) , βd is a |D| dimensional vector , where |D| is the total number of entities , and we define βd as follows : we set βd,d = e1 , where e1 is a constant value denotes the weight of the prior to sample d ’s topics from its own topic distribution θd ; for an entity d matched with d , we set θd,d = e2 , where e2 is another constant value represents the weight of the prior to sample topics from one of d ’s matching entities ; for other entities we set the corresponding values in β to 0 .
Figure 3 gives an example , in which we have three entities d1 , d2 , and d3 ; d1 is from source 1 ; d2 and d3 are from source 2 ; the only matching relation exists between d1 and d2 . Thus we set βd1 = ( e1 , e2 , 0 ) . From Figure 3 , we can see that d1 is only assigned with topics z1 and z2 in last step . However , in this step , as there is a matching relation between d1 and d2 , the word w1 from d1 can still be assigned with topics from d2 ( z3 and z4 ) , which bridges the latent topic space between linked entities .
With above definition , there is no chance to sample an entity d ’s topics from entities not matching with d . If d has no matching relations , each z is sampled according to its own entity ’s topic distribution θd . Thus the generation of d is the same with LDA [ 4 ] . Finally the word w is sampled according to the word distribution of topic z in source 1 : w ∼ Mult(ϕ1,z ) . As different terminologies are used to represent the same topic in different sources , we separate the word distribution of a topic z into ϕ1,z and ϕ2,z . We use source 1 as an example above and the documents in source 2 are generated in the same way .
Matching Relation Generation . In this step , each matching relation ld,d is modeled as a binary variable . As entities with similar topic distributions tend to be matched with a higher probability , it is natural to model the probability of a matching relation as a func
Source 1Source 2Topics062038053036010073027047043001Word…1398 d=1
+
+
D D − D − D d=1 d=1 n=1
Nd Nd Nd K n=1 n=1
Nd n=1
D d=1 d=1
D Nd n=1 d∈R(d ) d1,d2
L(ϑ , τ , η , ) =
Eq[ln P ( ld1,d2|zd1 , zd2 , γ ) ] +
Eq[ln P ( cd,n|λd ) ] +
Eq[ln P ( λd|β ) ]
D d=1
Eq[ln P ( θd|α ) ]
Eq[ln P ( wd,n|zd,n , ϕ ) ]
Eq[ln P ( zd,n|θ , cd,n ) ] +
Eq[ln qz(zd,n ) ] − D Eq[ln qθ(θd,i ) ] − D d=1
Eq[ln qc(cd,n ) ]
Eq[ln qλ(λd,d ) ] d=1 i=1 d=1
( 4 ) where d1 and d2 stratify ld1,d2 = ? . We then need to compute each item in Eq 4 . We focus on the first item as others , which are expected values of the log of a single probability component under the Dirichlet or the multinomial , can be expanded similar with LDA model . The first term is :
Nd1 Eq[ln P ( ld1,d2|zd1 , zd2 , γ ) ] = Eq[γ(zd1 ◦ zd2 ) ]
Nd2
=γ( n=1 φd1,n
◦
Nd1 n=1 φd2,n
)
Nd2
( 5 )
We then take the derivatives with respect to each variational parameter . We use η as an example . We first collect all of the terms associated with η and get :
L[η ] =
( 2 )
D d=1
( c∈R(d )
− ( log Γ(
ηd,i ) − i∈R(d ) c∈R(d ) log Γ(ηd,c ) )
( Nd × d,c + βd,c − ηd,c)(Ψ(ηd,c ) − Ψ(
ηd,i ) ) i∈R(d ) tion ρ of topic distributions . There are many possibilities for the function ρ . In this paper , we consider the following form
ρ(ld,d = 1|zd , zd , γ ) ∝ exp[γT ( ˜zd ◦ ˜zd ) ]
( 1 ) of each topic in d , ˜zd,k = Nd where the ◦ notation denotes the Hadamard product ( ( ˜zd ◦ ˜zd )k = ˜zd,k× ˜zd,k ) , ˜zd is a K dimension vector indicating the appearance j=1 1(zd,j = k ) . The function ρ is parameterized by coefficients γ . We define the function as an exponential one thus when zd and zd are close , with large weighted Hadamard product , the probability increases exponentially .
A similar regression method is used in Relational Topic Model ( RTM ) [ 7 ] . The difference between RTM and CST is , RTM can hardly deal with the entities from multiple sources while CST bridges multiple sourced entities by learning how likely they will be influenced by each other ( λ ) . Also , by cross sampling , CST models a high order dependency between matching entities and utilize the known relations more sufficiently .
As a conclusion , cross sampling based entity generation allows CST to leverage the known relations to help extract hidden crosssource topics . The matching relation generation uses extracted topics to infer the relations between entities in a latent space . 3.3 Model Learning
According to the model description above , the likelihood of the observed data in the CST model is given as
( zd1
,zd2 d1,d2
P ( w , L|α , β , γ , ϕ ) =
×
{ D ×
θ d
[ P ( θd|α)P ( λd|β )
λd
P ( ld1,d2|zd1 , zd2 , γ ) )
Nd
( P ( cd,j|λd ) j=1 cd,j
( P ( zd,j|θ , cd,j ) × P ( wd,j|zd,j , ϕ)))]dλd}dθ where w is a set of observed words in given corpus , L is the matching relation matrix , d1 and d2 are two entities with a labeled ld,d ( ld1,d2 =? ) , and Nd is the number of words in entity d .
We employ MAP estimation to learn the parameters of the CST model . However , the exact posterior inference is intractable and we appeal to approximate inference methods . In this work , we employ the mean field variational inference [ 28 , 12 ] . Generally , we define four variational parameters and aim to maximize the evidence lower bound ( ELBO ) [ 30 ] . Specifically , We define ϑ and as variational multinomial parameters . We also define τ and η as variational Dirichlet parameters . The approximate posterior is then defined as
Q(z , θ , λ , c|ϑ , τ , η , ) =
D
Nd qθ(θd|τd)qλ(λd|ηd ) qz(zd,n|ϑd,n)qc(cd,n| d )
( 3 ) d=1 n=1
We aim to minimize the Kullback Leibler ( KL ) divergence between the variational distribution and the true posterior , which is equivalent to maximizing the evidence lower bound ( ELBO ) [ 30 ] . The complete equation of ELBO is shown below . zd,j
We then take the derivative with respect to η
∂L[η ] ∂ηd,c
= ( Nd × d,c + βd,c − ηd,c)(Ψ(ηd,c ) − Ψ(
ηd,i ) ) i∈R(d )
The derivations of other variational parameters could be obtained similarly . We then set the derivations to zero , and find :
ηd,c = βd,c + Nd × d,c
Nd n=1
τd,k = αk + i∈R(d )
ϑd,n,k
( exp{ K
γk d,n,c ∝ exp{Ψ(ηd,c ) − Ψ(
ϑd,n,k ∝ d∈{R(d),d}
+ Ψ(τd,k ) − Ψ( i=1 ϑd,i,k Nd Nd d=d τd,j)} d,n,d × ϕt,k,v )
ηd,i)}
Nd
( 6 )
( 7 )
( 8 )
( 9 ) j=1 where t is the source of entity d , v is the n th word of d , and R(d ) is a set of entities matched with d . Intuitively , Eq 9 utilizes the
1399 known relations to update ϑ . The first summation in this equation is related with cross sampling and the second one is based on the regression part of CST . These updates above are performed iteratively until convergence , since they depend on each other .
We then fit the model by maximizing the resulting ELBO with respect to the model parameters ϕ and γ . In source t , given a topic k and a term v , the update for ϕt,k,v is :
Dt
Nd d=1 n=1
ϕt,k,v ∝
ϑd,n,k1(wt d,n = v )
( 10 )
The derivate with respect to γ takes a convenient form . To solve this problem , we add a 2 norm regularizer , which penalizes the objective function with the term ζ||λ||2 , where ζ is a free parameter . We then have :
2
γk = d,d 1 d,d ld,d [ (Υd − Υd ) ◦ ( Υd − Υd )]k
( 11 )
Nd where d and d are two entities with a labeled ld,d ( ld,d =? ) , and . Both the above update and Eq 9 utilize Υd,k = Nd known relations . n=1 ϑd,n,k
With all update equations above , we employ the variational expectation maximization algorithm to learn the model , which yields the following iterations : ( See Algorithm 2 for details . ) E step : optimize the ELBO with respect to the variational parameters {ϑ , τ , η , } . Update these variational parameters according to Eqs . 6 9 . M step : maximize the resulting ELBO with respect to the model parameters {ϕ , γ} . Update the model parameters according to Eqs . 10 11 .
Inferring Matching Relations . We finally detect the matching entities from different sources . Given a dual source corpus and a matching relation matrix with missing values , we use the learning algorithm from Section 3.3 to estimate the model ’s parameters by optimizing the ELBO for the observed data : words from the corpus and known relations in the matching relation matrix . After that , given two entities d and d with an unknown relation ( ld,d =? ) , we use the fitted model ’s variational parameters to approximate the predictive probability :
P ( ld,d|wd , wd ) ≈ Eq[p(ld,d|zd , zd ) ]
( 12 )
The right hand of Eq ( 12 ) is an expectation of ρ ( defined in Eq 1 ) with respect to the approximation posterior ( Eq 3 ) . Intuitively , the approximated predictive probability indicates that CST considers the content information and infers the matching relations between entities in hidden space of topics . Also , CST can be plugged into other detection frameworks ( eg , random walk [ 15 ] or factor graphs [ 14 ] ) easily , to further leverage structural information . Details and two examples will be described in the next section .
4 . EXPERIMENTS
We evaluate our proposed model with two experiments . All datasets and codes used in this work are publicly available2 .
Input : a dual source corpus C , a matching relation matrix L , and hyper parameters α and β Initialize {ϑ , τ , η , , ϕ , γ} randomly ; repeat
% E Step : optimize the ELBO ; foreach d in each source t do for c = 0 to 1 do
Update ηd,c according to Eq 6 ;
Update τd,k according to Eq 7 ; end for k = 1 to K do end for n = 1 to Nd do for c = 0 to 1 do end for k = 1 to K do end end
Update d,n,c according to Eq 8 ;
Update ϑd,n,k according to Eq 9 ; end % M Step : maximize the resulting ELBO ; foreach topic k in each source t do foreach term v do
Update ϕ according to Eq 10 ; end Update γk according to Eq 11 ; end until Convergence ;
Algorithm 2 : Variational EM for model learning .
4.1 Tasks and Data Sets
We validate the proposed model in two real scenarios : productpatent matching and cross lingual matching . We describe the details of each task below . Product patent matching . In this task , given a Wiki article describing a specific product , we aim to find relevant patents , eg , a Wiki article and a patent should be relevant if they are both talking about the topic of Siri . We collect 13,085 Wiki articles and 15,000 patents from Wikipedia and USPTO respectively . For some Wiki article that describes a product , we use it as a query to find patents related with the same product . One Wiki article may be matched with more than one patent , eg , a Wiki article describing iPhone corresponds to patents that claim on touch screen , camera , soft keyboard , etc We sample 233 Wiki articles as queries and find 1,060 matching relations in total . We randomly choose 30 % of the matching relations as known . The remaining relations are regarded as unknown and need to be inferred .
The ground truth data , which consists of 1,060 matching relations , is labeled by four human annotators . For each of 233 Wiki articles as queries , each annotator reads all patents belonging to the same company with the corresponding product in the query . Some online systems and materials are referred when filtering the candidates and labeling the data ( eg , PatentMiner [ 25]3 , news related with companies’ lawsuit , official documents of the products , etc ) To see more details of how we label the data , please refer to our public web page2 . We say a Wiki article is matched with a patent when four annotators all agree . Based on this work , we have deployed a product patent matching function to PatentMiner . We are
2http://arnetminer.org/document match/
3A public patent search and analysis system : http://pminer.org
1400 Table 2 : Performance of product patent matching task . P@3 Method R@20 MRR 0.053 0.046 CS + LDA 0.111 0.429 0.233 RW + LDA 0.111 0.171 0.141 0.501 0.667 0.668 0.333 0.667 0.457 0.683
P@20 MAP 0.109 0.083 0.123 0.117 0.416 0.233 0.341 0.167 0.250 0.445
R@3 0.011 0.033 0.057 0.200 0.171
RW + CST
RTM
CST
Precision
Table 3 : Performance of cross lingual matching task . Method F2 Measure Title Only SVM S LFG
F1 Measure
1.000 0.957 0.661 0.652 0.682
Recall 0.410 0.563 0.820 0.805 0.849
LFG + LDA LFG + CST
0.581 0.709 0.732 0.721 0.757
0.465 0.613 0.782 0.769 0.809 collecting user feedbacks to create a bigger evaluation data set for future work . Cross lingual matching . In this task , given an English Wiki article , we aim to find a Chinese article , which reports the same content , from a Chinese Wiki knowledge base . We use the same data set with [ 29 ] . The data set is collected as follows : we first randomly select an English article A with a cross lingual link to a Chinese article B from Wikipedia . We then use the B ’s title to find another Chinese article C with the same title in Baidu Baike4 . As A is cross lingually linked with B in Wikipedia , and B has the same main idea with C ( normally a Wiki article uses its main idea as the title ) . It is reasonable to say there is a cross lingual matching relation between A and C .
The data set consists of totally 2,000 English articles from Wikipedia , and 2,000 Chinese articles from Baidu Baike . Each English article corresponds to one Chinese article . We conduct 3 fold cross validation on the evaluation data set . 4.2 Evaluation Evaluation metrics . In the first experiment , for each Wiki article , we rank all patents according to the probability predicted by the proposed model and alternative methods . We evaluate all the methods in terms of P@3 ( Precision for the top 3 ranking results ) , P@20 , MAP ( Mean Average Precision ) , R@3 ( Recall for the top 3 results ) , R@20 , and MRR ( Mean Reciprocal Rank ) .
In the second experiment , to keep consistence with [ 29 ] , we consider cross lingual matching as a two class classification problem : given an English Wiki article and a Chinese Wiki article , we label this pair of two entities as “ matched ” or “ not matched ” . We compare all baselines in terms of Precision ( Prec. ) , Recall ( Rec. ) , F1 Measure ( F1 ) , and F2 Measure ( F2 ) . Comparison methods . For the first experiment , we compare the following methods for product patent matching : • Content Similarity based on LDA ( CS + LDA ) : It calculates the similarity between a Wiki article and a patent based on their topic distributions calculated by LDA . Specifically , we use pd1 and pd2 to represent the topic distribution of a Wiki article and a patent respectively . The similarity score is defined based on the Cosine similarity between pd1 and pd2
• Random Walk based on LDA ( RW + LDA ) : It ranks candidates by combining the extracted topics into a random walk with restart algorithm [ 27 ] . Specifically , it creates a graph containing Wiki articles and patents as nodes . And it links a Wiki article u to a patent v with a weight
Sim(u,v ) w Sim(u,w )
Wu,v =
0 if Sim(u , v ) ≥ µ otherwise
( 14 ) where µ is a threshold value defined manually , and Sim(u , v ) is the Cosine similarity between u and v . Thus there is a bigger chance for a Wiki article node to reach a more similar patent node . It employs LDA to calculate the topic distributions . Besides the textural contents of entities , this framework also considers the structural information . We create a link from one patent node to another if the former one cites the latter one . We also create a link from one Wiki article nodes to another if they have a hyperlink in Wikipedia . The weights of these links are defined as a constant value ( in practice , we define all of them as 1 ) . Finally , the transition probability from u to v can be defined as
Qu,v = ( 1 − a )
+ a1(v = s )
( 15 )
Wu,v x Wu,w where s is the start node , a is the restart probability .
• Relational Topic Model ( RTM ) : It employs the RTM , which is generally used to model the links between entities , proposed by Blei et al . [ 7 ] . In our problem , this method regards there is a link between two matching entities . We use Blei ’s implementation of RTM5 . • Random Walk based on CST ( RW + CST ) : The difference between this method and RW + LDA is , instead of using Sim(u , v ) to define the weight of links from a Wiki node to a patent node , it uses P ( lu,v ) ( see Section 3.3 for details ) calculated by CST . • CST : It is our proposed model . We first use the training set to learn the model . Then we use the fitted model to detect unknown relations . We set K = 50 , α = 50/K , e1 = 4 , and e2 = 1 in both this method and RW + CST .
All methods use entities in the training set to fit the model . Methods related to RTM or CST utilize known matching relations as guidance , while LDA is unable to leverage this information . Random walk based methods further consider structural information ( citations in the patent database and hyperlinks in Wikipedia ) . for cross lingual matching :
For the second experiment , we compare the following methods • Title Only : This method first translates the title of Chinese articles into English by Google Translation API 6 , then matches the translated titles with English articles . Two articles are considered as equivalent ones if they have strictly the same English titles .
• SVM S : It is a classifier proposed by Sorg et al .
[ 24 ] to find cross lingual links between English Wikipedia and German Wikipedia . The authors define several graph based and text based features . Here we train a SVM with their features on evaluation data set . For SVM , we choose LIBSVM [ 6 ] . • LFG : It is the method proposed by Wang et al . [ 29 ] , which is based on a factor graph model and mainly considers the structural information to solve the problem of cross lingual matching .
Sim(d1 , d2 ) = pd1 · pd2
||pd1|| × ||pd2||
( 13 )
4A Chinese Wiki knoledge base : http://baikebaiducom/
5http://wwwcsprincetonedu/~blei/topicmodelinghtml 6https://developersgooglecom/translate/?hl=zhcn
1401 ( a ) Number of topics K
( b ) Ratio
( c ) Precision
( d ) Convergence analysis
Figure 4 : Parameter analysis . ( a ) Performance of the CST model by varying the number of topics K ; ( b ) Performance of the CST model by varying the ratio of e1 to e2 ; ( c ) Performance of the CST model is stable when varying the precision of β ; ( d ) Convergence analysis of the CST model . Y axises in all figures denote the MAP value of the CST model in product patent matching experiment and the F1 score of the CST model in cross lingual matching experiment .
• LFG + LDA : It adds a feature , which captures the content similarity between articles , to the feature function of LFG . It uses Sim(u , v ) ( see Eq 14 ) as the feature value . • LFG + CST : LFG mainly considers structural information . We enhance it by bringing in content information ( hidden topics extracted by CST ) . The difference between this method and LFG + LDA is that , instead of using Sim(u , v ) to define the newly added feature , it uses P ( lu,v ) calculated by CST . We compare this method with LFG to see if content information can help in this problem . We compare it with Title Only and SVM S to show the power of utilizing corss lingual topics extracted by CST . We also compare it with LFG + LDA to show the effectiveness of the CST model compared with a traditional topic model . Here we keep values of K , α , and e2 the same with the first task , and set e1 = 2 . We will give the intuitive explanation why we change e1 latter .
4.3 Quantitative Results
Product patent matching . Table 2 lists the performance of product patent matching problem using different methods . We first compare CST with two unsupervised methods , CS + LDA and RW + LDA . With the help of known relations as guidance , we can see CST clearly outperforms these two methods ( +724% 755 % in terms of MAP ) . We then compare CST with RTM , which also utilizes the known relations as guidance . With the help of the crosssampling , CST can better extract cross source topics . Thus it can better detect the matching relations ( +74.9 % in terms of MRR ) . To our surprise , when employing the CST model , combining content and structural information hurts the performance ( RW + CST drops 23.4 % in terms of MAP ) . By a careful investigation , we find that a Wiki article normally has lots of hyperlinks to other articles ( 56.4 out links in average ) . Much noise is contained in these links and hurts the performance . However , the structural information does help for top results ( +14.5 % in terms of R@3 ) .
Cross lingual matching . Table 3 shows the performance of crosslingual matching problem . Title Only and SVM S employ the translated terminologies and perform well in terms of Prec . However , without capturing the hidden topics of entities , the translation can not be performed precisely . Thus these methods miss a number of matching relations between entities , which hurts the Recall .
LFG focuses on utilizing structural information . We enhance this method by bringing in hidden topics extracted by LDA and CST respectively . From the table , we see that LFG + CST improves the performance . It outperforms all baselines in terms of Recall , F1 , and F2 ( eg , averagely +15.2 % in terms of F2 ) . In fact , crosslingual topics can hardly be extracted due to the low co occurrence of English and Chinese terminologies . Without a precise cross lingual topic extraction , LFG + LDA performs worse than LFG , which indicates the incorrect topics will hurt the performance . By studying some cross lingual topics found by the CST model , we find that the top Chinese and English terminologies in the same topic are very relevant . Some Chinese terminologies are translated results of English ones .
Topics analysis . How many topics are enough for the productpatent matching problem and cross lingual matching problem ? We perform an analysis by varying the number of topics in the CST model . Figure 4(a ) shows its performance with number of topics K varied . We can see that , the performance improves by increasing K when K is small ( < 50 ) . After that , the trend becomes stable . Ratio analysis . We study how the ratio of e1 to e2 influence the performance . We fix e2 as 1 and vary e1 . Figure 4(b ) shows the trend of the performance following the changes of the ratio in both two problems . In the product patent matching problem , the value of MAP reaches largest when e1 : e2 = 4 . And in the crosslingual problem , F1 reaches the maximum value when e1 : e2 = 2 , corresponding to a larger prior probability of cross sampling .
Intuitively , compared with cross lingual matched articles , patents and Wiki articles with matching relations are more dissimilar in hidden space of topics : patents focus on specific technologies , while Wiki articles describe general descriptions of products ( eg , histories , sales , etc ) And cross lingual matched articles report the same objects . Thus the prior of cross sampling in crosslingual matching problem should be larger ( smaller e1 , larger e2 ) . It indicates that the hyper parameters of CST can be determined intuitively : if the matching entities in a specific problem assumed to be more similar in topics , we can give a smaller value to e1 : e2 , otherwise we should set e1 : e2 a larger value . Precision analysis . We further investigate how the precision [ 19 ] of β , which indicates the confidence in the prior , influence the performance . We vary the precision from 1 to 450 . As Figure 4(c ) shows , the CST model ’s matching performance is not sensitive to the precision of β .
Convergence analysis . We finally investigate the convergence of the CST model . Figure 4(d ) shows the convergence analysis of the CST model on product patent matching problem and cross lingual matching problem . We see the CST model converges within 100 iterations ion both two tasks .
4.4 Qualitative Results
In this section , we demonstrate some examples generated from our experiments to show the effectiveness of the CST model .
2040608010000102030405060708#topicsPerformance ( MAP / F1 ) Prodct−patentCross−lingual12345678902030405060708e1 : e2Performance ( MAP / F1 ) Prodct−patentCross−lingual5010015020025030035040045003040506070809precisionPerformance ( MAP / F1 ) Product−patentCross−lingual2040608010012014016018000102030405060708#iterationsPerformance ( MAP / F1 ) Prodct−patentCross−lingual1402 Dg Dg of companies , we train the CST model by these companies’ patents and Wiki articles describing the companies’ products . And for a company g , we define its topic distribution as P ( z|g ) = d=1 θd,z , where Dg is the set of entities relevant to g . Here we use Apple and Samsung as an example . Table 4 lists the top three topics related with both Apple and Samsung . We also represent each topic ’s top words from both Wiki articles and patents . We see that terminologies related with technologies are more likely to appear in patents ( eg , recognition , range , etc ) And most terms closer to our lives and applications are from Wikipedia ( eg , video , iPad , etc ) d=1 1
“ Gravity Sensing ” and “ Touchscreen ” are both highly related with the products of Apple and Samsung ( eg , smart phones , iPad , etc. ) , which indicates through the label information between patents and products , the CST model can identify the topics bridging products and related technologies . Moreover , “ Application Icons ” is also discovered by CST . As we know , one of the Apple patents been violated by Samsung7 , is the design patent 305 : Rounded square icons on interface , which is related to this topic . It indicates that the results of CST may be helpful to infer the competitive relationships between companies .
5 . RELATED WORK
Cross source matching . We first review some related work on cross source matching problem . Wang et al . [ 29 ] study the crosslingual knowledge linking problem . They aim to link Chinese and English Wiki articles which report on the same content . However , the model they proposed , called LFG , only considers the structural information . In this paper , we utilize our proposed model to bring in content information to LFG . We conduct a similar experiment with Wang et al . and the result shows that the performance is significantly improved . Mimno et al . [ 18 ] has studied a similar problem . They propose a polylingual topic model to discover topics aligned across multiple languages . Tang et al . [ 26 ] propose a method called Cross domain Topic Learning . Their goal , which is to recommend cross domain collaborations , is different from ours . More importantly , their method separates topic extraction and link prediction into two models . Our model integrates topic modeling and entity matching into a unified model .
Besides , Barnard et al . [ 2 ] propose an approach for modeling segmented images with associated text simultaneously . However , their approach do not integrate entity matching and topic modeling into a uniform framework .
Topic modeling . It is natural to apply topic modeling ( eg , LDA [ 4 ] and PLSA [ 11 ] ) on a collection of documents , and use the derived topic distribution to represent each document . The basic mechanism behind these models is to exploit co occurrence patterns of words in documents to find K semantically meaningful topics and best describe the given corpus . However , both PLSA and LDA treat documents in a given corpus independently .
To deal with the pairwise information of documents , Cohn and Hoffman [ 10 ] build an extension to the PLSA model , which is called PHITS . A similar model called mixed membership model is developed by Erosheva et al . [ 9 ] . Mei et al . [ 17 ] add a regularization constraint on a prior knowledge that some pairs of documents should be similar , to the traditional topic models . Dietz et al . [ 8 , 16 ] also propose similar methods . These approaches regard links as input data , whereas in our work , the proposed model is able to infer the unknown relations between documents from different sources .
7http://wwwbusinessinsidercom/apple versus samsung 2012 8
Figure 5 : Examples of the correlations between topics , patents , and Wiki articles in the CST model . θ , the probability of a topic give an entity , is represented on each black solid edge . And the weight on each red dotted edge denotes the likelihood of a matching relation . The titles of topics are hand labeled . And for each topic , we separate the terminologies used in patents ( the upper part of each topic box ) and the terminologies used in Wiki articles ( the lower part of each topic box ) . We remove some edges whose probabilities are negligible .
Table 4 : Examples of topics highly relevant to both Apple and Samsung found by the CST model . Top terminologies from each source are showed . The titles of topics are hand labeled .
Title
Gravity Sensing
Touchscreen
Application Icons
Top Patent Terms rotational , gravity , interface , sharing , frame , layer recognition , point , digital , touch , sensitivity , image interface , range , drives , icon , industrial , pixel
Top Wiki Terms gravity , iPhone , layer , video , version , menu screen , touch , iPad , os , unlock , press icon , player , software , touch , screen , application
Product patent matching . Figure 5 shows a part of the matching results of “ Macbook Pro ” Wiki article . We select 3 topics extracted by the CST model and display them with top words in both two sources . We also represent the probability of a specific topic z given an entity d ( θz,d ) , and the matching probability of two entities in the form of edges . As we can see from the figure , a patent mostly focus on one topic , a specific technology . And a Wiki article generally describe a number of features of a product . Thus Wiki articles have more diverse topic distributions .
When predicting a matching relation for two entities , the regression part of the CST mode is able to distinguish relevant topics from others . As the figure shows , the CST mode successfully detects the Macbook Pro is matched with “ Wide touchpad on a portable computer ” and “ Display that emits circularly polarized light ” respectively . Each of the two patents is associated with a topic relevant to Macbook Pro .
Apple vs . Samsung . The CST model will be helpful to find the patents that a company uses to protect her products , by detecting the matching relations between products and patents . CST is also able to infer the inner connections between companies . Given a set
Wide touchpad on a portable computerDisplay that emits circularly polarized light!Macbook ProTouchpadElectronic Visual Display061041038074Touchscreen023069087Integrated touch screen077PatentsTopicsProductsfingerdragbuttondevicepresssensortouchpadpositionportableoperate0043002100150013001300380034002900260025osscreenservicerecognitiondigitalipadpresspointtouchunlock0029002800230023001900270025002200190018crystalvisuallcdelectronictelevisionvisualmonitorelectronicbacklightdisplay0036003100250025001600410033001700140012Macbook Pro041038038087fingerdragbuttondevicepress00380034002900260025ipadpresspointtouchunlock00270025002200190018visualmonitorelectronicbacklightdisplay00410033001700140012Wide touchpad on a portable computerDisplay that emits circularly polarized lightTouchpadElectronic Visual Display061074Touchscreen023069Integrated touch screen077sensortouchpadpositionportableoperate00430021001500130013osscreenservicerecognitiondigital00290028002300230019crystalvisuallcdelectronictelevision003600310025002500161403 To integrate supervised information , Blei et al . propose a method called Relational Topic Model ( RTM ) [ 7 ] , which models the links of each pair of documents as a binary random variable that is conditioned on their contents . Nallapati et al . [ 20 ] propose a model combines the ideas of LDA and Mixed Membership Block Stochastic Models [ 1 ] and allows modeling arbitrary link structure . They also propose another model [ 21 ] , which assumes the link structure is a bipartite graph and combines the LDA and PLSA into a single graphical model . Blei et al . [ 5 ] introduce supervised LDA and use it to predict ratings for movies . All these models introduced above depend on the co occurrence of terms , whereas multiple source documents ( entities ) share few terms . Thus they can hardly deal with corpus from different sources . By cross sampling , our proposed model is able to extract cross source topics and infer matching documents ( entities ) from different sources . 6 . CONCLUSION
In this paper , we propose an approach to solve the problem of entity matching across heterogeneous sources . The model we proposed is named as the Cross Source Topic model , which integrates the topic extraction and entity matching into a unified framework . A semi supervised learning algorithm is proposed to learn the model . We validate the model on two real scenarios . The experimental results demonstrate that the proposed model can extensively improve the performance compared with baseline methods ( +19.8 % and +7.1 % in two scenarios respectively ) . The proposed model mainly considers the text content information of entities . Meanwhile , the model is easy to be plugged into other frameworks which can leverage both the content and structural information together . We give two examples to show how the proposed model can be plugged into a random walk framework and a factor graph respectively .
Acknowledgements . The work is supported by the National High tech R&D Program ( No . 2014AA015103 ) , National Basic Research Program of China ( No . 2014CB340506 , No . 2012CB316006 ) , Natural Science Foundation of China ( No . 61222212 ) , National Social Science Foundation of China ( No . 13&ZD190 ) , NSF CAREER Award ( No . 1453800 ) , NSFC ANR ( No . 61261130588 ) , and a research fund supported by Huawei Inc . 7 . REFERENCES [ 1 ] E . M . Airoldi , D . M . Blei , S . E . Fienberg , and E . P . Xing .
Mixed membership stochastic blockmodels . JMLR , 9:1981–2014 , 2008 .
[ 2 ] K . Barnard , P . Duygulu , D . Forsyth , N . De Freitas , D . Blei , and M . Jordan . Matching words and pictures . JMLR , 3:1107–1135 , 2003 .
[ 3 ] K . Bellare , S . Iyengar , A . G . Parameswaran , and V . Rastogi .
Active sampling for entity matching . In KDD’12 , pages 1131–1139 . ACM , 2012 .
[ 4 ] D . Blei , A . Ng , and M . Jordan . Latent dirichlet allocation .
JMLR , 3:993–1022 , 2003 .
[ 5 ] D . M . Blei and J . D . McAuliffe . Supervised topic models . arXiv preprint arXiv:1003.0783 , 2010 .
[ 6 ] C C Chang and C J Lin . LIBSVM : A library for support vector machines . ACM TIST , 2:27:1–27:27 , 2011 .
[ 7 ] J . Chang and D . Blei . Relational topic models for document networks . In AIS’09 , pages 81–88 , 2009 .
[ 8 ] L . Dietz , S . Bickel , and T . Scheffer . Unsupervised prediction of citation influences . In ICML’07 , pages 233–240 , 2007 .
[ 9 ] E . Erosheva , S . Fienberg , and J . Lafferty . Mixed membership models of scientific publications . PNAS’04 , 101(Suppl 1):5220–5227 , 2004 .
[ 10 ] D . C . T . Hofmann . The missing link a probabilistic model of document content and hypertext connectivity . NIPS’00 , 13:430 , 2000 .
[ 11 ] T . Hofmann . Probabilistic latent semantic indexing . In
SIGIR’99 , pages 50–57 , 1999 .
[ 12 ] M . Jordan , Z . Ghahramani , T . Jaakkola , and L . Saul . An introduction to variational methods for graphical models . Machine learning , 37(2):183–233 , 1999 .
[ 13 ] J . Li , J . Tang , Y . Li , and Q . Luo . Rimom : A dynamic multistrategy ontology alignment framework . TKDE’09 , 21(8):1218–1232 , 2009 .
[ 14 ] H A Loeliger . An introduction to factor graphs . Signal
Processing Magazine , IEEE , 21(1):28–41 , 2004 . [ 15 ] L . Lovász . Random walks on graphs : A survey .
Combinatorics , 2(1):1–46 , 1993 .
[ 16 ] A . McCallum , A . Corrada Emmanuel , and X . Wang . Topic and role discovery in social networks . Computer Science Department Faculty Publication Series , page 3 , 2005 .
[ 17 ] Q . Mei , D . Cai , D . Zhang , and C . Zhai . Topic modeling with network regularization . In WWW’08 , pages 101–110 , 2008 . [ 18 ] D . Mimno , H . M . Wallach , J . Naradowsky , D . A . Smith , and
A . McCallum . Polylingual topic models . In EMNLP’09 , pages 880–889 , 2009 .
[ 19 ] T . Minka . Estimating a dirichlet distribution . Technical report , MIT , 2000 .
[ 20 ] R . Nallapati , A . Ahmed , E . Xing , and W . Cohen . Joint latent topic models for text and citations . In KDD’08 , pages 542–550 , 2008 .
[ 21 ] R . Nallapati and W . Cohen . Link plsa lda : A new unsupervised model for topics and influence of blogs . In ICWSM’08 , 2008 .
[ 22 ] D . Rinser , D . Lange , and F . Naumann . Cross lingual entity matching and infobox alignment in wikipedia . Information Systems , 38(6):887–907 , 2013 .
[ 23 ] W . Shen , J . Wang , P . Luo , and M . Wang . Linking named entities in tweets with knowledge base via user interest modeling . In KDD’13 , pages 68–76 , 2013 .
[ 24 ] P . Sorg and P . Cimiano . Enriching the crosslingual link structure of wikipedia a classification based approach . In AAAI’08 Workshop on Wikipedia and Artifical Intelligence , pages 49–54 , 2008 .
[ 25 ] J . Tang , B . Wang , Y . Yang , P . Hu , Y . Zhao , X . Yan , B . Gao ,
M . Huang , P . Xu , W . Li , and A . K . Usadi . Patentminer : Topic driven patent analysis and mining . In KDD’12 , pages 1366–1375 , 2012 .
[ 26 ] J . Tang , S . Wu , J . Sun , and H . Su . Cross domain collaboration recommendation . In KDD’12 , pages 1285–1293 , 2012 .
[ 27 ] H . Tong , C . Faloutsos , and J . Pan . Fast random walk with restart and its applications . In ICDM’06 , pages 613–622 , 2006 .
[ 28 ] M . Wainwright and M . Jordan . Graphical models , exponential families , and variational inference . Foundations and Trends in Machine Learning , 1(1 2):1–305 , 2008 .
[ 29 ] Z . Wang , J . Li , Z . Wang , and J . Tang . Cross lingual knowledge linking across wiki knowledge bases . In WWW’12 , pages 459–468 , 2012 .
[ 30 ] J . Winn . Variational message passing and its applications . Unpublished doctoral dissertation , Cambridge University , 2003 .
1404
