Differentially Private High Dimensional Data Publication via Sampling Based Inference
Rui Chen∗
Samsung Research America , USA ruichen1@samsungcom
Yu Zhang
Hong Kong Baptist University , Hong Kong yuzhang@comphkbueduhk
Qian Xiao
National University of Singapore , Singapore xiaoqian@nusedusg
Jianliang Xu
Hong Kong Baptist University , Hong Kong xujl@comphkbueduhk
ABSTRACT Releasing high dimensional data enables a wide spectrum of data mining tasks . Yet , individual privacy has been a major obstacle to data sharing . In this paper , we consider the problem of releasing high dimensional data with differential privacy guarantees . We propose a novel solution to preserve the joint distribution of a high dimensional dataset . We first develop a robust sampling based framework to systematically explore the dependencies among all attributes and subsequently build a dependency graph . This framework is coupled with a generic threshold mechanism to significantly improve accuracy . We then identify a set of marginal tables from the dependency graph to approximate the joint distribution based on the solid inference foundation of the junction tree algorithm while minimizing the resultant error . We prove that selecting the optimal marginals with the goal of minimizing error is NP hard and , thus , design an approximation algorithm using an integer programming relaxation and the constrained concave convex procedure . Extensive experiments on real datasets demonstrate that our solution substantially outperforms the state of the art competitors .
Categories and Subject Descriptors K41 [ COMPUTERS AND SOCIETY ] : Privacy
Keywords Differential privacy , high dimensional data , joint distribution , dependency graph , junction tree algorithm
1 .
INTRODUCTION
With the rapid development of computing technologies , high dimensional data has become prevalent in many appli∗Part of this work was done while the author was with Hong Kong Baptist University cation domains . Releasing such data has been a prerequisite for many data mining tasks . However , individual privacy has been a major public concern in data sharing . In this paper , we consider the problem of releasing high dimensional data under differential privacy [ 6 ] , a privacy notion widely advocated due to its strong privacy guarantee . There have been a series of techniques proposed for differentially private lowdimensional data publication [ 1,2,6,9,12,17,18,24,25,27,30 ] . Unfortunately , when applied to high dimensional data , all these techniques suffer from the curse of dimensionality , that is , they cannot achieve either reasonable scalability or desirable utility [ 21,29 ] . Special treatment is needed to overcome the challenges incurred by high dimensionality .
In addressing the challenges , one of the most promising ideas is to decompose high dimensional data into a set of low dimensional marginal tables C , along with an inference mechanism that infers the joint data distribution from C . This has been the rationale of the very recent study [ 29 ] in which PrivBayes is proposed to learn a set of low dimensional conditional probabilities via a Bayesian network and approximate the joint distribution by the chain rule for Bayesian networks . While making substantial progress , it has two major limitations due to the constraint of differential privacy . First , one has to minimize probes over the underlying dataset ; otherwise the result of a probe becomes largely inaccurate . This implies that one often has to compromise more sophisticated algorithms that require excessive access to the data . For this reason , PrivBayes employs a simple greedy algorithm to learn a Bayesian network , whose performance is sensitive to the randomly selected initial attribute , and limits the size of each attribute ’s parent set to be identical . Second , with the increasing number of attributes , the privacy budget used for determining each attribute ’s parent set decreases quickly , making the learned conditional probabilities unreliable . As a result , PrivBayes is still not able to adequately capture the characteristics of the underlying data in order to maximize data utility .
In practice , the independence properties exist in many high dimensional datasets , which is the rationale behind probabilistic graphical models [ 15 ] . It is beneficial to systematically explore such ( conditional ) independences among attributes . However , conducting such an exploration under differential privacy requires non trivial efforts . The key technical challenge is how to reliably learn all attributes’ pairwise correlations using a limited privacy budget . For a dataset with d attributes , existing techniques require to divide the
129 given privacy budget into,d
2 portions , each being used for a pair of attributes . There is no doubt that none of these techniques can learn the correlations with reasonable accuracy . We address this challenge by proposing a sampling based testing framework and a generic threshold mechanism . This design allows us to learn the pairwise correlations without splitting the privacy budget in proportion to,d enjoy the privacy budget amplification due to sampling .
2 and further
With the learned correlations , how to develop an inference model to estimate the joint distribution with minimum error remains a second challenge . We decompose this task into two steps . First , we build a solid statistical inference foundation by employing the well established junction tree algorithm . Unfortunately , the junction tree algorithm does not take into consideration the differential privacy constraint . Directly injecting noise into the marginals of the cliques returned by the junction tree algorithm usually leads to excessive noise . We then formulate an optimization problem for finding the optimal marginals with the minimum error and solve it by the constrained concave convex procedure . Contributions . Our key contribution is a novel samplingbased solution for publishing high dimensional data under differential privacy , which features a solid statistical inference foundation . More specifically , we make the following contributions :
First , we design a sampling based testing framework to systematically explore pairwise dependencies while satisfying differential privacy . This framework is made possible by a generic threshold mechanism , which is an extended version of the sparse vector technique [ 10 ] and the threshold query technique [ 16 ] . This threshold mechanism allows to use multiple thresholds and may substantially improve the accuracy of many algorithms .
Second , we propose to apply the junction tree algorithm to establish an inference mechanism for inferring the joint data distribution . How to generate differentially private marginals to feed the inference mechanism with minimum error is vital to the final accuracy . We prove that this optimization problem is NP hard and propose an approximation algorithm using an integer programming relaxation and the constrained concave convex procedure .
Third , we extend the mutual consistency technique in [ 21 ] to the general case where the noisy marginals are of different sizes and attributes may not be binary , and propose a simple yet effective thresholding strategy to mitigate the systematic bias due to rounding negative noisy counts to 0 . We also show how to efficiently generate synthetic datasets from the noisy marginals using the junction tree representation .
We conduct extensive experiments over five standard real datasets for two different analysis tasks . We show that our solution not only significantly outperforms PrivBayes [ 29 ] , the state of the art technique for releasing high dimensional data , but also achieves comparable , and sometimes better , accuracy to that of PriView [ 21 ] , the state of the art technique tailored to generating k way marginals over highdimensional data . Moreover , our solution works for both binary and non binary data while PriView only works for binary data1 .
1To handle non binary data , PriView requires adapting existing covering design algorithms to the case where different views may have different sizes [ 21 ] . To our best knowledge , there does not exist such an algorithm .
2 . RELATED WORK
Most of the existing works on differentially private data publishing focus on low dimensional data ( eg , marginal tables or histograms ) . Xiao et al . [ 24 ] propose to apply the wavelet transformation to an input histogram and add noise to wavelet coefficients . Hay et al . [ 12 ] exploit the consistency constraints that should hold over the noisy output to improve accuracy . These two methods are special cases of a more general matrix mechanism [ 17 ] , which calculates the answers to a set of queries from another set of properly selected queries called query strategy . Yaroslavtsev et al . [ 26 ] further explore the idea of using a query strategy by adding non uniform noise . Yuan et al . [ 27 ] introduce the low rank mechanism for answering batch linear counting queries based on low rank matrix approximation . Hardt et al . [ 9 ] present an algorithm based on multiplicative weights and the exponential mechanism . In another line of research , Xu et al . [ 25 ] propose to group a histogram ’s adjacent bins with close counts to trade for smaller Laplace noise . A similar idea is proposed by Acs et al . [ 1 ] . They design a hierarchical bisection algorithm to identify a good grouping scheme . Zhang et al . [ 30 ] indicate that global clustering achieves better utility than local grouping . Li et al . [ 18 ] propose to answer range queries by taking into consideration both the underlying data and the query set .
In general , the above methods cannot overcome the inherent challenges due to the curse of dimensionality . In spite of its wide applications , differentially private high dimensional data publication has been relatively rarely studied . Barak et al . [ 2 ] show how to construct a synthetic database to preserve all low dimensional marginals by adding noise to the Fourier domain . The problem in [ 2 ] is equivalent to publishing OLAP cubes , which is studied by Ding et al . [ 5 ] . They first compute a subset of cuboids and then generate the remaining cuboids from this subset . The main limitation of these two approaches is their exponential complexity in the dimensionality of the domain . Mohammed et al . [ 20 ] introduce probabilistic generalization to overcome the curse of dimensionality . However , with the increasing dimensionality , the benefit of generalization diminishes rapidly . Cormode et al . [ 4 ] solely consider the scalability aspect of the problem . They design a statistical process to compute a private summary without materializing the entire contingency table . Very recently , Qardaji et al . [ 21 ] study how to generate accurate k way marginals for a binary dataset . They propose PriView that uses covering design to select a set of low dimensional marginals called views and then generates k way marginals based on maximum entropy optimization . The work closest to ours is PrivBayes [ 29 ] , which iteratively learns the parent sets of the attributes in a Bayesian network by applying the exponential mechanism with a surrogate function for mutual information . Compared with PrivBayes , our solution features a systematic exploration of attribute correlations and a series of new generic techniques , which together achieve substantially better performance . The connection between probabilistic inference and differential privacy is also studied in [ 23 ] .
3 . PROBLEM FORMULATION 3.1 Problem Statement
In this paper , we consider the following problem : Given a dataset D with d attributes ( either numerical or categori
130 cal ) A = {A1 , A2 , · · · , Ad} , we want to generate a synthetic dataset that accurately preserves the joint distribution of the tuples in D while satisfying differential privacy .
We denote the value domain of an attribute Ai by Ωi and its size ( ie , the number of distinct values in Ωi ) by |Ωi|2 . Therefore , the entire output domain is defined by Ω1 × Ω2 × · · · × Ωd , whose size is |Ω1| × |Ω2| × · · · × |Ωd| . We focus on the case where |Ω1| × |Ω2| × · · · × |Ωd| is too large to be handled by the existing low dimensional data publishing techniques . We assume that the size of the dataset D ( ie , the number of tuples ) , denoted by |D| , is known . This is a common assumption under our definition of neighboring databases [ 6 ] . Note that the dataset D does not have to be a relational table . It can also be , for example , a set valued dataset in which all attributes are binary . Differential Privacy . Differential privacy is built on the notion of indistinguishability of two neighboring databases . We consider two databases D and D′ to be neighbors if D can be obtained from D′ by changing the value of exactly one tuple . Intuitively , differential privacy guarantees that any computational result from D and D′ will be statistically indistinguishable . A formal definition is given below .
Definition 1 . ( ǫ Differential Privacy [ 6 ] ) A randomized alif for any two gorithm M satisfies ǫ differential privacy , neighboring databases D and D′ , and for any O ⊆ Range(M ) ,
P [ M(D ) ∈ O ] ≤ exp(ǫ ) · P [ M(D′ ) ∈ O ] , where the probability is taken over M ’s randomness .
Differential privacy can be achieved by the Laplace mechanism [ 6 ] , which injects properly calibrated Laplace noise into a function ’s output to mask the impact of any single tuple . The maximal impact of a tuple to the output of a function f is called its sensitivity . For any two neighboring databases D and D′ , the sensitivity of f : D → Rd is defined as ∆f = maxD,D′ kf ( D ) − f ( D′)k1 .
Theorem 1 .
[ 6 ] For any function f : D → Rd , the mech anism M ,
M ( D ) = f ( D ) +Lap1 ∆f gives ǫ differential privacy , where Lapi , ∆f
ǫ , . . . , Lapd ∆f ǫ * ǫ are iid Laplace variables with scale parameter ∆f ǫ . 3.2 Junction Tree Algorithm
The key to overcoming the curse of dimensionality in our problem is to factorize the joint probability distribution into modular components based on conditional independences that exist in many real world datasets . Probabilistic graphical models are an elegant tool for identifying such a modular structure . Markov networks are the most widely used graphical model based on undirected graphs . In our problem , we record the independences in terms of a dependency graph , which is essentially a Markov network in that the nodes represent the attributes in a dataset , and the edges correspond to the dependencies between the attributes [ 15 ] .
The junction tree algorithm is a standard method to parameterize a Markov network so that the joint distribution and marginal distributions can be readily calculated . The 2Continuous attributes can be discretized to fit into our solution .
( a ) A sample dependency graph
A4
A4
( b ) A junction tree
Figure 1 : A dependency graph and its junction tree general idea is to represent the joint distribution P ( A ) of all attributes A = {A1 , A2 , · · · , Ad} in a dataset as a function of the marginals of a set of maximal cliques and their separators . Let Ci be a clique in the junction tree T , and Sij = Ci ∩ Cj be the separator between cliques Ci and Cj . The joint distribution can be calculated as follows :
P ( A ) = QCi∈T P ( Ci ) QSij ∈T P ( Sij )
,
( 1 ) where P ( Ci ) and P ( Sij ) are the marginal distributions of the cliques and the separators , respectively . This establishes the theoretical foundation of exactly inferring the joint distribution from a junction tree .
A junction tree can be constructed in two steps . The first step is triangulation . An undirected graph is said to be triangulated if and only if there is an edge between any two non successive vertices in every cycle . A triangulated graph is guaranteed to have a junction tree . Then we can construct the junction tree in one of two basic ways : ( 1 ) based on variable elimination and ( 2 ) based on direct graph manipulation . In either way , the resultant junction tree satisfies the running intersection property : for each pair of vertices u and v , all vertices on the path between u and v contain the intersection u ∩ v .
Example 1 . Figure 1(a ) gives the dependency graph G of a sample dataset with attributes A = {A1 , A2 , · · · , A6} . Since the cycle A2 − A3 − A5 − A4 − A2 has non successive vertices that are not connected , we add the edge ( A3 , A4 ) to make G triangulated , as illustrated by the dashed line . Note that the triangulation procedure may not be unique . For example , adding the edge ( A2 , A5 ) also makes G triangulated . A junction tree of the triangulated graph is given in Figure 1(b ) , where cliques are represented by oval nodes and separators by rectangle nodes . Similarly , the junction tree of a triangulated graph is not unique .
4 . OUR SOLUTION
Broadly , our solution first systematically explores pairwise dependencies to construct the dependency graph and then differentially privately infers the joint distribution by applying the junction tree algorithm . More specifically , our solution is composed of the following four steps .
1 . Build the dependency graph . The first step is to learn the pairwise correlations of all attributes under a sampling based testing framework , from which the dependency graph is generated .
2 . Form attribute clusters . We apply the junction tree algorithm to the dependency graph in order to
131 generate the set of cliques to form the inference foundation and further identify a collection of attribute clusters to derive all the cliques’ noisy marginals with the minimum error .
3 . Generate noisy marginals . For each attribute cluster , we generate a differentially private marginal table and enforce consistency constraints over all such marginals .
4 . Produce a synthetic dataset . We make use of the noisy marginal tables and the inference model to efficiently generate a synthetic dataset .
Since only the first and third steps require access to the input dataset , we divide the total privacy budget ǫ into two portions with ǫ1 being used for the first step and ǫ2 for the third step . In the following , we will show that the first and third steps are ǫ1 and ǫ2 differentially private , respectively . Hence , by the composition property [ 6 ] , our solution satisfies ǫ differential privacy as a whole , where ǫ = ǫ1 + ǫ2 . 4.1 Constructing the Dependency Graph
To build the dependency graph , we propose a samplingbased dependency testing framework . The theoretical foundation of our framework is derived from log linear models [ 3 ] that have been extensively studied in the statistical literature . The saturated log linear model states that the joint distribution can be modeled as a summation of effects whose dimensionality ranges from 0 up to d ( ie , the correlations of pairs of attributes , triples , quadruples , and so forth ) . Yet , recent studies [ 13 ] have shown the diminishing return of maintaining higher order correlations , suggesting that pairwise correlations are most important to approximate the joint distribution . This result justifies why our framework focuses on learning pairwise correlations .
In the literature , the correlation between two attributes can be measured by several metrics , such as chi squared test χ2 , mean square contingency , Cramer ’s V φc and mutual information I , among others . In this paper , we choose mutual information due not only to its small sensitivity but also to its capability of capturing both linear and non linear correlations . Given two attributes Ak and Al , the mutual information I(Ak , Al ) is defined as
I(Ak , Al ) =
|Ωk|Xi=1
|Ωl|Xj=1 pij log pij pi·p·j
, where pij is the fraction of the tuples whose Ak = i ( ie , the ith value in Ωk ) and Al = j , pi· =Pj pij and p·j =Pi pij .
As suggested by [ 14 ] , we can consider that Ak and Al are independent if I(Ak , Al ) ≤ θkl for some small θkl > 0 . Yet , how to set a reasonable θkl is unknown . In this paper , we perform a more formal analysis on the choice of θkl by establishing the connection between mutual information and Cramer ’s V φc . Mutual information can be well approximated as I(Ak , Al ) ≈ min(|Ωk|−1,|Ωl|−1)φ2 . Since φc has a wellinterpreted level of dependency ( eg , φc = 0.2 stands for weak dependency ) , we design θkl = σ min(|Ωk| − 1 , |Ωl| − 1 ) , where σ is a parameter controlled by the φc value representing the desired level of dependency ( eg , φc = 02 )
2 c ns =

Algorithm 1 Generate Dependency Graph Input : Dataset D with attributes A = {A1 , A2 , · · · , Ad} Input : Privacy parameter ǫ1 Output : Dependency graph G
1 : Initialize G = ( V , E ) with V = {A1 , A2 , · · · , Ad} and
E = ∅ ;
2 : Calculate sampling rate β ; 3 : Generate Ds by sampling tuples in D with rate β ; 4 : ǫa = ln(eǫ1 − 1 + β ) − ln β ;
5 : η = Lap 2∆I ǫa ;
6 : for each attribute pair ( Ak , Al ) do 7 : eI(Ak , Al ) = I(Ak , Al ) + Lap 2∆I ǫa ; eθkl = θkl + η ; if eI(Ak , Al ) ≥eθkl then
8 : 9 : 10 : 11 : return G ;
Add edge ( Ak , Al ) in G ; size . This observation is vital to allow us to enjoy the sampling property of differential privacy [ 19 ] .
Theorem 2 .
[ 19 ] Let M be an ǫ differentially private algorithm and Mβ be another algorithm that first independently samples each tuple in its input dataset with probability β and then applies M to the sampled dataset . Mβ satisfies ln(1 + β(eǫ − 1)) differential privacy .
In Line 2 , we calculate the sampling rate β by first determining a good sample size ns . Our intuition is to select the sample size that minimizes the ratio of noise magnitude to the range of mutual information I on the sampled dataset , which is equivalent to identifying the best signal to noise ratio [ 29 ] . To quantify this size , we need to know the amplified privacy parameter ǫa due to sampling and the sensitivity of |D| .
Next we give the sensitivity of I .
I . From Theorem 2 , we have ǫa = lneǫ1 − 1 + ns ∆I = where n is the sampled dataset size . n − 1 n + 1 n − 1
|D|−ln ns
, if any attr . is binary
Theorem 3 .
, otherwise log n + n + 1 n − 1 n − 1
[ 29 ] n n
1 n log log
2 n log
+
2 n
Note that ∆I is insensitive to attributes’ domain sizes . It only cares whether an attribute is binary . In Algorithm 1 , if all attributes of the input dataset are binary , we use ∆I = n log n + n−1 1 2 + n log n+1 n−1 . The reason of this choice will be made clear soon . Thus , the best sample size can be calculated as n−1 ; otherwise we use ∆I = 2 n log n+1 n log n n−1 argmin n argmin n
1 n log n + n−1 lneǫ1 − 1 + n n log n+1 2 + n−1 lneǫ1 − 1 + n n log n n−1 |D| − ln n n log n+1 |D| − ln n n−1
2
|D|
|D|
,if all attr . are binary
,otherwise
We present our differentially private sampling based framework in Algorithm 1 . Previous work [ 13 ] has indicated that the sample size required for learning correlations for a specified degree of accuracy can be independent of the dataset
In Line 3 , we sample the tuples in D by including each tuple with probability β = ns |D| . Then we learn the pairwise correlations between all attributes over the sampled dataset Ds . A simple attempt is to split the privacy parameter ǫa where 1 < n ≤ |D| stands for all possible sample sizes .
132 into ,d
2 portions , each being used for a pair of attributes .
However , when d is relatively large , this simple scheme can barely obtain reliable results . In addressing this problem , we generalize the threshold query technique [ 16 ] , which is derived from the sparse vector technique [ 10 ] . The key difference is that in our problem , for different pairs of attributes Ak and Al , I(Ak , Al ) needs to be compared with different thresholds θkl = σ min(|Ωk| − 1 , |Ωl| − 1 ) , while in [ 16 ] all queries are compared with the same threshold . We call this generalized mechanism the threshold mechanism . The general intuition is that if what one wants to learn from a sequence of queries is a sequence of yes or no answers with respect to some threshold values ( ie , whether a query answer can pass the threshold ) , instead of the noisy values of the queries , there is no need to split the privacy parameter in proportion to the number of queries . Intuitively , this is possible if the single tuple difference changes all yes or no answers in the same way with respect to the thresholds . The threshold mechanism is instantiated in Lines 5 10 . We will give its formal privacy guarantee later .
|Ds| log |Ds|+1
|Ds| log |Ds|+1
In Line 5 , we draw a random Laplace variable η with scale 2∆I to add randomness to all thresholds , where ∆I is ǫa the maximum sensitivity of all I(Ak , Al ) . That is , if there is at least an attribute Ai in D with |Ωi| > 2 , we should 2 + |Ds|−1 use ∆I = 2 |Ds|−1 , where |Ds| is 3 . This is a must to establish the privathe actual size of Ds cy guarantee of the threshold mechanism . In Lines 6 10 , for each pair of attributes Ak and Al , we calculate the noisy ver ) . Here the noise scale does not depend on the number of queries , which is the key benefit of employing the thresh sion of I(Ak , Al ) , denoted byeI(Ak , Al ) , by adding Lap( 2∆I old mechanism . In Line 9 , eI(Ak , Al ) is compared with the noisy thresholdeθkl = θkl + η . If eI(Ak , Al ) ≥eθkl , it indicates that Ak and Al are correlated , and we record this correlation by adding an edge between them in the dependency graph . Now we formally prove the privacy guarantee of Algorith
ǫa
Once v<i is fixed , vi = 1 iff Ii + Lap( 2∆I ) ≥ θi + η , where Ii ǫa and θi are the mutual information and threshold being used for generating vi . Let Hi(η ) be the probability that vi = 1 on D when the threshold is θi + η and H ′ i(η ) be that on D′ . Let λ = 2∆I ǫa
. We have
Hi(η ) = P ( vi = 1|v<i ) = P ( Lap(λ ) + Ii − θi ≥ η | v<i )
2λ exp− |y−µ|
λ . We can rewrite Hi(η ) as
η f ( y|Ii − θi , λ)dy . After making the substitution
Let f ( y|µ , λ ) = 1
Hi(η ) =R ∞ u = y − ∆I , we get
Hi(η ) =Z ∞
η−∆I f ( u|Ii − θi − ∆I , λ)du
By definition of sensitivity , Hi(η ) ≤R ∞ i−θi , λ)du = i(η − ∆I ) . Recall that η is a random Laplace variable with 2 )p(η = x − ∆I ) ,
H ′ scale 2∆I ǫa where p(· ) is the probability density function . We have
. It holds that p(η = x ) ≤ exp( ǫa
η−∆I f ( u|I ′ p(η = x ) Yi:ai=1
Hi(x)dx
Hi(x)dx
H ′ i(x − ∆I)dx
Yi:ai=1
≤ exp(
≤ exp(
= exp(
= exp(
−∞
−∞
ǫa 2
P ( vi = 1|v<i ) =Z ∞ )Z ∞ )Z ∞ )Z ∞ ) Yi:ai=1 p(η = x − ∆I ) Yi:ai=1 p(η = x − ∆I ) Yi:ai=1 p(η = x ) Yi:ai=1
P ′(vi = 1|v<i )
ǫa 2
ǫa 2
ǫa 2
−∞
−∞
H ′ i(x)dx
( 3 )
Similarly , we can prove that
Yi:ai=0
P ( vi = 0|v<i ) ≤ exp(
ǫa 2
) Yi:ai=0
P ′(vi = 0|v<i )
( 4 ) m 1 , including that of the threshold mechanism .
Using Formulae 2 , 3 and 4 , we derive
Theorem 4 . Algorithm 1 is ǫ1 differentially private .
P ( v = a ) P ′(v = a )
≤ exp(
ǫa 2
) · exp(
ǫa 2
) = exp(ǫa )
( 5 )
Proof . Let M be the version of Algorithm 1 without sampling ( ie , without Lines 2 3 ) and Ms be Algorithm 1 . We first prove that M is ǫa differentially private and then make use of Theorem 2 to show that Ms is ǫ1 differentially private . Essentially , M outputs a vector v = [ v1 , · · · , vw ] , where vi takes a binary value to indicate whether the cor responding pair of attributes is correlated , and w = ,d 2 .
Let P ( v = a ) and P ′(v = a ) be the probabilities that a ∈ {0 , 1}w is produced by two neighboring databases D and D′ , respectively . By definition , we want to prove that for all D and D′ and for all output vectors a of M , P ( v=a ) P ′(v=a ) ≤ exp(ǫα ) . Let v<i denote the answers to the first ( i − 1 ) elements in v and ai ∈ {0 , 1} be an answer .
P ( v = a ) P ′(v = a )
= Qw Qw = Yi:ai=1 i=1 P ( vi = ai|v<i ) i=1 P ′(vi = ai|v<i ) P ( vi = 1|v<i ) P ′(vi = 1|v<i )
· Yi:ai=0
P ( vi = 0|v<i ) P ′(vi = 0|v<i )
( 2 )
3Note that |Ds| may not equal ns due to the Bernoulli sampling process ( Line 3 ) required by Theorem 2 .
That is , M is ǫa differentially private . Due to Theorem 2 , Ms achieves ǫ1 differential privacy because ln ( 1 + β(eǫa − 1 ) ) = ln1 + β( eǫ1 − 1
β
+ 1 − 1 ) = ǫ1 .
This concludes the proof .
We would like to stress that in general , to make the threshold mechanism work , all queries do not need to have the same sensitivity as long as we add noise to the queries and thresholds based on the maximum sensitivity of all queries . This explains why we have to use the larger ∆I if there is an attribute that is not binary . The randomness added to all threshold values must come from the same Laplace variable ; otherwise we cannot establish the above proof .
For ease of discussion , in the following sections , we assume that the dependency graph is connected . Otherwise , we simply process each connected component separately . 4.2 Forming Attribute Clusters
With the learned dependency graph , we can feed it into the junction tree algorithm introduced in Section 3.2 to obtain the set of cliques and separators . A simple idea is to
133 directly generate noisy marginals based on these cliques so that we can infer the joint distribution . There is no need to generate marginals for the separators as they can always be derived from the cliques . However , the junction tree algorithm selects the cliques irrespective of the differential privacy constraint . In fact , the number of marginals is directly related to the accuracy of the estimated joint distribution . Our insight is that we can properly merge the cliques into larger and fewer clusters from which we can derive the joint distribution with less noise .
Consider merging cliques C1 , · · · , Ck into a cluster CL , which contains the set of attributes in C1 , · · · , Ck . Let the set of attributes be {A1 , · · · , Al} , ǫ2 be the privacy budget for generating the noisy marginals and the total number of marginals be m . Clearly , we can generate the cliques’ marginals from CL ’s marginal with the total noise variance )2 is the noise k × 2( 2m ǫ2 variance added to each entry . We illustrate the benefit of merging cliques by the following example .
)2 × |Ω1| × · · · × |Ωl| , where 2( 2m ǫ2
Example 2 . Continue with Example 1 . Assume that the domain sizes of A1 , A2 , A3 , A4 , A5 , A6 are 2 , 2 , 3 , 4 , 3 and 2 , respectively . If we directly add Laplace noise to the marginals of the cliques A1A2 , A2A3A4 , A3A4A5 and A4A6 , the total variance of the marginals is 8960 . Alternatively , ǫ2 2 we can merge A1A2 and A2A3A4 into A1A2A3A4 , merge A3A4A5 and A4A6 into A3A4A5A6 , add Laplace noise to A1A2A3A4 and A3A4A5A6 , and derive the cliques’ marginals from A1A2A3A4 and A3A4A5A6 . The total variance of the cliques’ marginals is 7680 . Finally , it is also possible to ǫ2 2 merge A1A2 and A4A6 into A1A2A4A6 and merge A2A3A4 and A3A4A5 into A2A3A4A5 , and achieve a total variance 6656
.
ǫ2 2
There are two important observations from Example 2 : ( 1 ) If we properly merge the cliques and then derive their marginals from the merged clusters , we are able to reduce the magnitude of noise ; ( 2 ) the merging procedure does not have to depend on the junction tree structure ( eg , merging A1A2 and A4A6 , which are not adjacent in the junction tree , leads to an even smaller variance ) . We formally define the OptimalMerging problem as follows .
Definition 2 . ( OptimalMerging ) Given a set of attributes A = {A1 , · · · , Ad} and the set of cliques C = {C1 , · · · , Ck} derived from A , merge the cliques into the set of clusters CL = {CL1 , · · · , CLm} such that : ( 1 ) The total noise vari|Ωj| is minimum wrt all possible m values , ( 2 ) each clique is in exactly one cluster , and ( 3 ) the clusters contain all cliques . ance of the cliques’ marginalsPm
|CLi|QAj ∈CLi
8m2 ǫ2 2 i=1
While it has been clear that finding the optimal merging scheme may substantially improve the accuracy of the estimated joint distribution , the OptimalMerging problem is , unfortunately , NP hard to solve .
Theorem 5 . The OptimalMerging problem is NP hard .
Proof . We establish the NP hardness of our problem by the reduction from 3 PARTITION [ 7 ] : Given a multiset S of n = 3m positive integers {I1 , I2 , · · · , In} such that ∀i ∈ {1 , 2 , · · · , n} , B partitioned into m disjoint subsets S1 , S2 , · · · , Sm such that
2 and Pi Ii = mB , can S be
4 < Ii < B
Ii = B and ∪jSj = S ?
∀j PIi∈Sj
Algorithm 2 Form Attribute Clusters Input : Dependency graph G Input : Attribute set A = {A1 , A2 , · · · , Ad} Output : Junction cliques C Output : Separators S Output : Attribute clusters CL
1 : ( C , S ) ← JunctionTreeAlgorithm(G ) ; 2 : for i = |C| to 1 do 3 : 4 :
CLi ← IdentifyOptimalMerging(C , i ) ; Calculate the total noise variance V ar(CLi ) of the cliques’ marginals derived from CLi ;
5 : CL = argminCLi 6 : return C , S , and CL ;
( V ar(CLi) ) ;
10B
10B
Since B
4 < Ii < B to minimize Pm
2 , each Si must contain exactly three integers from S . We now show a polynomial time reduction from 3 PARTITION to OptimalMerging . Given an instance of 3 PARTITION with n positive integers {I1 , I2 , · · · , In} and a bound B , we construct the corresponding instance of OptimalMerging as follows . We transform the input integers to 3 PARTITION to {m10I1 , m10I2 , · · · , m10In } as the input to OptimalMerging , where ∀i ∈ {1 , 2 , · · · , n} , m 4 < m10Ii < m 2 . We restrict OptimalMerging to just consider merging n non overlapping cliques into m clusters . Given a fixed m , since 8m2 is a constant , OptimalMerging aims ǫ2 2 m10Ij , which we call the objective function . Now ask whether one can obtain a value of the objective function at most 3m10B+2 . It is easy to see that if there is a 3 PARTITION , putting all integers in a subset into a cluster gives at most m · 3m · m10B for the objective function . Conversely , suppose that there is a merging scheme such that the objective function value is at most m · 3m · m10B . We can map the clusters to the subsets in 3 PARTITION . Note that no subset can add to more than B , since otherwise the product in that cluster would be at least m10B+10 , which is already more than the objective function value m · 3m · m10B . Since each Ii is in the range of ( B in 3 PARTITION have size exactly 3 . This establishes the proof .
2 ) andPi Ii = mB , it follows that all subsets i=1 |Si|Qm
10Ij ∈Si
4 , B
Considering the hardness of OptimalMerging , we design an approximation algorithm using an integer programming relaxation and the constrained concave convex procedure . Algorithm 2 gives the pseudocode of our solution . For each possible number m of clusters , we identify the corresponding optimal merging scheme by the IdentifyOptimalMerging procedure , which will be discussed next in detail .
We first introduce the notations for modeling the OptimalMerging problem for a given number m of clusters . For simplicity of notation , we slightly abuse some notations we used before . Suppose there are a total of d attributes and the junction tree contains n cliques . The size of the ith attribute is denoted by ci . We define the occurrence matrix O = [ oi,j]d×n where oi,j equals 1 when the ith attribute is contained in the jth clique and 0 otherwise . Let pi be the product of the attributes’ domain sizes in the ith clique Ci . We define zi,k ∈ {0 , 1} as the indicator of the ith clique in the kth cluster , ie , zi,k = 1 implies that the ith clique Ci has been merged into the kth cluster . The objective function to find an optimal merging scheme for a given number
134 m of clusters is formulated as min Z,r st zi,k i Pn i=1 zi,k j=1 zj,k oi,j −1
Pn i i=1 p i=1 c mXk=1 Qn Qd zi,k ∈ {0 , 1} , ∀i , k ln mXk=1 nXi=1 zi,k = 1 , ∀i kzi − zjk2
2 ≥ r , ∀i 6= j zi,k ≥ 1 , ∀k
 − λr
( 6 ) where Z is an n × m matrix with zi,k as its ( i , k)th element , zi denotes the ith column of Z , λ is a positive constant to balance the trade off between the two terms , and k · k2 denotes the ℓ2 norm of a vector . Note that the first term in the objective function is to minimize the logarithm of the total noise variance derived from the m marginals ( see Definition 2 ) , the first two constraints guarantee that each clique is merged into exactly one cluster , and the last constraint ensures that each cluster contains at least one clique . Moreover , the third constraint enforces the assignments of any two clusters to be different , and we expect that the difference between the assignments captured by r is as large as possible , leading to the term ‘−λr’ in the objective function . Since the factor 8m2 is a constant for each term in the ǫ2 2 summation of the logarithmic function , it is omitted from the objective function .
We introduce m new variables {tk}m k=1 which upper bound each term in the summation of the first term in the objective function of Problem ( 6 ) via the exponential function , ie , Qn
≤ exp{tk} , which implies that i=1 p zi,k i Pn i=1 zi,k Pn j=1 zj,k oi,j −1
Then we can reformulate Problem ( 6 ) as
Qd i=1 c i ln nXi=1 min Z,t,r st zi,k ln pi ≤ tk+ exp{tk}! − λr zi,k ∈ {0 , 1} , ∀i , k zi,k = 1 , ∀i r − kzi − zjk2
2 ≤ 0 , ∀i 6= j zi,k ≥ 1 , ∀k zi,k!+ nXi=1 ln mXk=1 mXk=1 nXi=1 nXi=1 + ln nXi=1 zi,k ln pi − tk − dXi=1 nXj=1 zi,k! ≤ 0 , ∀k zj,koi,j − 1! ln ci
( 7 ) where t = [ t1 , . . . , tm]T . Problem ( 7 ) is an NP hard integer programming problem . Here we relax the first constraint of Problem ( 7 ) to zi,k ≥ 0 , ∀i , k , that is , zi,k is relaxed to lie in [ 0 , 1 ] , instead of belonging to the binary set {0 , 1} . After the relaxation , zi,k can be viewed as the probability of the ith clique belonging to the kth cluster . We call this relaxed Problem ( 7 ) .
Relaxed Problem ( 7 ) is a non convex problem since the third and fifth constraints are non convex . Here we use the constrained concave convex procedure ( CCCP ) [ 28 ] to solve it . The CCCP method requires that the non convex constraints can be formulated as the difference of two convex functions . To guarantee that relaxed Problem ( 7 ) can be solved by the CCCP method , we reformulate the third and fifth non convex constraints as
− kzi − zjk2 2
≤ 0
} zi,k ln pi − tk −
Convex
{z
|
Convex r|{z} nXi=1 | − − ln nXi=1 {z |
Convex zi,k!! } dXi=1 nXj=1 {z
Convex
≤ 0 zj,koi,j − 1! ln ci }
Through the reformulation , we can see that the requirement of the CCCP method is satisfied , making it capable of solving relaxed Problem ( 7 ) . The CCCP method is an iterative method . In each iteration , the CCCP method first replaces the concave parts in the objective function and constraints with their first order Taylor expansions based on the solution in the previous iteration and then solves the resulting convex subproblem . Specifically , for relaxed Problem ( 7 ) , the lth iteration of the CCCP method solves the following problem as ln mXk=1 mXk=1 nXi=1 nXi=1 + ln nXi=1 r − 2(zi − zj)T p(l−1 ) i,j + q(l−1 ) i,j ≤ 0 , ∀i 6= j zi,k ≥ 1 , ∀k zi,k ln pi − tk − dXi=1 nXj=1 i,k ! +Pn Pn z(l−1 ) zj,koi,j − 1! ln ci i=1(zi,k − z(l−1 ) i,k
)
≤ 0 , ∀k ( 8 ) i=1 z(l−1 ) i,k i j i,j i,k
− z(l−1 )
, and q(l−1 ) i,j = z(l−1 ) where z(l−1 ) is the solution in the ( l − 1)th iteration of the CCCP method , p(l−1 ) i,j = kp(l−1 ) k2 2 . Note that the last two terms in the third and fifth constraints of Problem ( 8 ) compose the first order Taylor expansion of the corresponding convex parts at Z(l−1 ) , the solution in the previous iteration of the CCCP method . Problem ( 8 ) is obviously convex since both the objective function and constraints are convex , and we can use some solver such as the CVX [ 8 ] to solve it . Moreover , the CCCP method can guarantee to converge to a local optimum of relaxed Problem ( 7 ) by solving Problem ( 8 ) in each iteration . dXi=1 nXj=1 zj,koi,j − 1! ln ci . min Z,t,r exp{tk}! − λr st zi,k ≥ 0 , ∀i , k zi,k = 1 , ∀i
135 Recall that the above solution gives the probabilities of a clique belonging to different clusters . We thus assign a clique to the cluster with the largest probability . We calculate the total noise variance for this merging scheme . We iterate the IdentifyOptimalMerging procedure for all possible numbers of clusters from 1 to |C| , and return the scheme with the minimal total noise variance . 4.3 Generating Noisy Marginals
Given the clusters identified by Algorithm 2 , we use the Laplace mechanism to generate their corresponding noisy marginal tables . Let the number of clusters be m . For each cluster ’s marginal table , we add Laplace noise Lap( 2m ) to ǫ2 each entry ’s count . Therefore , the noisy marginals satisfy ǫ2 differential privacy . In our problem , mutual consistency among different noisy marginals is critical because for any separator Sij = Ci ∩ Cj we need to guarantee that the noisy marginal of Sij constructed from Ci is identical to that constructed from Cj so as to obtain consistent inference . We extend the post processing technique in [ 21 ] to the general case where the noisy marginals are of different sizes and attributes may not be binary .
Consider a set of clusters CL1 , · · · , CLl . Let A = CL1 ∩ CL2 ∩· · ·∩CLl 6= ∅ . We use TCLi to denote CLi ’s marginal , TCLi [ A ] to denote A ’s marginal constructed from CLi and TCLi [ A ] ≡ TCLj [ A ] to denote that the two marginals are identical . We want to ensure TCL1 [ A ] ≡ · · · ≡ TCLl [ A ] . We achieve this goal in two steps . The first step is to derive the best approximation of A ’s marginal table . Let a be a possible value in A ’s domain and TA(a ) be the count of a in A ’s marginal . Since each TCLi ( a ) is an independent observation of TA(a ) , we use inverse variance weighting [ 11 ] to give the approximation of TA(a ) that minimizes the variance of the weighted average as follows :
TA(a ) = Pl i=1 TCLi ( a)/σ2 i
,
Pi 1/σ2 i i =QAj ∈(CLi\A ) |Ωj| is proportional to the variance where σ2 of TCLi [ A](a ) . The second step is to update all TCLi ’s to be consistent with TA . The general idea is to distribute the difference between TA and TCLi [ A ] to all entries in TCLi with A = a , that is
TCLi ( e ) ← TCLi ( e ) +
TA(a ) − TCLi ( a )
QAj ∈(CLi\A ) |Ωj|
, where e is the entries with their A = a .
To make all marginals consistent , we need to perform a sequence of mutual consistency steps . The order of these steps is critical , otherwise previously consistent results may be invalidated by subsequent steps . As suggested in [ 21 ] , this problem can be avoided by enforcing a partial order under the subset relation on all non empty intersections of some subset of the clusters and following a topological order over these intersections . For space reasons , we refer the interested reader to [ 21 ] for more details .
In addition to mutual consistency , we propose a simple yet effective thresholding strategy to mitigate the systematic bias due to rounding negative noisy counts to 0 . We select a positive integer threshold such that the noisy counts above the threshold sum up to a number N that is closest to |D| . We then normalize all noisy counts above the threshold by multiplying |D|/N and set all noisy counts below the thresh
Table 1 : Dataset statistics Attr .
Type
Data Size
Number
Dataset
AOL
Binary Binary
Retail BR2000 Non binary Non binary Adult TPC E Non binary
619,418 88,162 38,000 45,222 40,000
45 50 14 15 24
Domain
Size 245 250 ≈ 232 ≈ 252 ≈ 277 old to 0 . The rationale behind this strategy is that we want to filter out the bias introduced by the positive Laplace noise added to low true counts . 4.4 Producing Synthetic Datasets
With the junction tree and the noisy marginals , we can calculate the joint distribution by Equation 1 . However , directly sampling a synthetic dataset from the joint distribution is computationally prohibitive . To this end , we provide an efficient way to generate a synthetic dataset by a series of local computations . We start by randomly choosing an initial clique from the junction tree and sampling its attributes from its marginal distribution , and then continuously sample other attributes in the cliques adjacent to the cliques whose attributes have been fully sampled from their conditional distribution . A clique is adjacent to another clique if they share a common separator . We terminate this process when all attributes have been sampled . It is easy to verify that all these probability distributions are available from the noisy marginals of the cliques and that the joint distribution of the synthetic dataset will be identical to that calculated from Equation 1 .
5 . EXPERIMENTAL EVALUATION
In this section , we demonstrate the performance of our solution ( referred to as JTree ) by comparing with two state ofthe art techniques , namely PrivBayes [ 29 ] and PriView [ 21 ] . Moreover , for SVM classification , we also compare with PrivateSVM [ 22 ] , a method specialized for SVM classification . We make use of five standard real datasets ( both binary and non binary ) in our experiments . For binary datasets , we deliberately choose the ones with larger domain sizes to test the performance of JTree . We use AOL , the real dataset with the largest domain size used in [ 21 ] , and another benchmark frequent itemset mining dataset Retail4 . AOL is a search log dataset that includes users’ search keywords and is preprocessed to contain 45 binary attributes [ 21 ] . Retail is a retail market basket dataset , where each record consists of the distinct items purchased in a shopping visit . We preprocess Retail to include 50 binary attributes ( for the reason of reproducibility , we choose the top 50 most frequent items as the binary attributes ) . For non binary datasets , we use the same datasets used in [ 29 ] . BR2000 contains the demographics information collected from Brazil in 2000 . Adult contains census data from the 1994 US census . TPC E contains information of “ Trade ” , “ Security ” , “ Security status ” and “ Trade type ” tables in the TPC E benchmark . We summarize the statistics of the datasets in Table 1 . 5.1 Evaluation Methodology
We consider the same analysis tasks in [ 29 ] , namely kway marginals and SVM classification . Since PriView on4Retail is available at http://fimiuaacbe/data/
136 ly works for binary datasets and cannot generate synthetic datasets for SVM classification , for binary datasets we only report the results on k way marginals . We follow the same evaluation scheme used in PriView : We use privacy budget ǫ ∈ {0.1 , 1.0} and generate 200 random k way marginals for each k ∈ {4 , 6 , 8} . We then plot the average L2 error , which is normalized by the data size .
For non binary datasets , when k is relatively large ( eg , 4 , 6 or 8 ) , a k way marginal is normally very sparse , and therefore the evaluation scheme used by PriView may be significantly biased . As such , we choose to follow the same methodology used in PrivBayes . We generate all 2 way and 3 way marginals and report the average total variation distance between the original datasets and the noisy datasets . In addition , we test the classification results with SVM classifiers . Due to space limitation , we only report the results on Adult , which is the most widely used benchmark dataset for classification analysis . We train SVM classifiers on Adult to predict whether an individual ( 1 ) is a male , ( 2 ) holds a postsecondary degree , ( 3 ) has salary > 50K per year , and ( 4 ) has never married . We evaluate each classification task with privacy budget ǫ ∈ {0.2 , 0.5 , 0.8 , 10} Each task uses 80 % of the tuples in Adult as the training set and the remaining 20 % for prediction . As in PrivBayes , we also employ the misclassification rate as the performance metric .
For PriView , we report the results based on its full version with the ripple non negativity technique . For PrivBayes , as suggested in [ 29 ] , we allocate half of the privacy budget on the construction of the Bayesian network and the other half on generating conditional distributions . We set θ = 4 ( for θusefulness ) to select the appropriate value of the degree of a Bayesian network , as recommended in [ 29 ] . For JTree , when ǫ = 0.1 , we allocate ǫ1 = 0.05 on junction tree construction and ǫ2 = 0.05 on marginal generation . For all other ǫ values , we allocate ǫ1 = 0.1 on junction tree construction and the rest on marginal generation . All experimental results we report below are the average of ten runs .
5.2 Results on Binary Datasets
In the first set of experiments , we compare the performance of the three solutions on the binary datasets under different privacy budgets , as presented in Figure 2 . It can be seen that the accuracy of JTree is substantially better than that of PrivBayes in most cases . Note that the Y axis is in log scale . Compared with PriView , JTree also achieves comparable accuracy . The superiority of JTree is more observable when ǫ is small ( eg , ǫ = 01 ) For the only case where JTree is less accurate than PriView , the L2 error of JTree is already very small . Furthermore , we deem that this represents an acceptable trade off as JTree is a generic framework that publishes synthetic datasets for different analysis tasks , while PriView is the state of the art technique specifically tailored for k way marginals . In addition , JTree can work seamlessly on non binary datasets , which is important for many real world applications .
5.3 Results on Non Binary Datasets k Way Marginals . In the second set of experiments , we study the average total variation distance of PrivBayes and JTree for varying ǫ values on non binary datasets and present the results in Figure 3 . Recall that PriView cannot process non binary datasets , and therefore it is not reported here . As can be observed , JTree substantially outperform
1
0.1
JTree PriView PrivBayes
1
0.1
JTree PriView PrivBayes r o r r e 2 L
0.0
1 r o r r e 2 L
0.0
1
0.0
0
1
0.0
0
1
4
6 k way marginals
8
4
6 k way marginals
8
( a ) Retail , ǫ = 0.1
( b ) Retail , ǫ = 1.0
1
0.1
JTree PriView PrivBayes
1
0.1
JTree PriView PrivBayes r o r r e 2 L
0.0
1 r o r r e 2 L
0.0
1
0.0
0
1
0.0
0
1
4
6 k way marginals
8
4
6 k way marginals
8
( c ) AOL , ǫ = 0.1
( d ) AOL , ǫ = 1.0
Figure 2 : L2 error of k way marginals on binary datasets s PrivBayes in almost all cases . In some cases , the total variation distance of JTree is just half of that of PrivBayes . The only case that PrivBayes performs better than JTree is ǫ = 0.2 on TPC E , but even in this case , the accuracy of JTree is still close to that of PrivBayes . In the last set of experiments , we SVM Classification . compare JTree , PrivBayes and PrivateSVM for SVM classification . Due to space limitation , we only report the results on Adult , the benchmark dataset for classification . Figure 4 shows the misclassification rate of each method under different ǫ values . The misclassification rate of the original dataset ( denoted as Non Private ) stands for the best performance we can achieve . We can observe that JTree consistently outperforms PrivBayes on Adult . Compared with PrivateSVM that is specialized for SVM classification , JTree also achieves comparable performance . We interpret the slight performance degradation as the reasonable price for providing a generic data publishing solution .
6 . CONCLUSION
Publishing high dimensional data with differential privacy guarantees is one of the most fundamental and challenging problems . In this paper , we have proposed a novel samplingbased inference framework to preserve the joint distribution of high dimensional data under differential privacy . This framework features a sophisticated systematic exploration of pairwise attribute dependencies and establishes the solid inference foundation based on the junction tree algorithm , along with a series of new techniques . Extensive experiments on real benchmark datasets demonstrate that our solution substantially outperforms the state of the art competitors .
7 . ACKNOWLEDGMENTS
We would like to thank all reviewers for their valuable suggestions . This work was supported by RGC/GRF Grants HKBU 211512 and 12200114 and NSFC 61305071 .
8 . REFERENCES [ 1 ] G . Acs , C . Castelluccia , and R . Chen . Differentially private histogram publishing through lossy compression . In ICDM , 2012 .
137 average variation distance average variation distance svm misclassification rates svm misclassification rates
0.3
0.2
0.1
0.0
0.3
0.2
0.1
0.0
0.3
0.2
0.1
0.0
JTree PrivBayes
JTree PrivBayes
0.4
0.3
0.2
0.1
0.0
0.2
0.5 privacy budget e
0.8
1
0.2
0.5 privacy budget e
0.8
( a ) BR2000 , 2 way
( b ) BR2000 , 3 way average variation distance average variation distance
JTree PrivBayes
JTree PrivBayes
0.4
0.3
0.2
0.1
0.0
0.2
0.5 privacy budget e
0.8
1
0.2
0.5 privacy budget e
0.8
( c ) Adult , 2 way
( d ) Adult , 3 way average variation distance average variation distance
JTree PrivBayes
JTree PrivBayes
0.4
0.3
0.2
0.1
0.0
0.2
0.5 privacy budget e
0.8
1
0.2
0.5 privacy budget e
0.8
1
1
1
( e ) TPC E , 2 way
( f ) TPC E , 3 way
Figure 3 : Total variation distance of k way marginals on non binary datasets
[ 2 ] B . Barak , C . Dwork , S . Kale , F . McSherry , and K . Talwar .
Privacy , accuracy , and consistency too : a holistic solution to contingency table release . In PODS , 2007 .
[ 3 ] Y . M . Bishop , S . E . Fienberg , and P . W . Paul . Discrete multivariate analysis . MIT Press , 1975 .
[ 4 ] G . Cormode , C . M . Procopiuc , D . Srivastava , and T . T . L .
Tran . Differential private summaries for sparse data . In ICDT , 2012 .
[ 5 ] B . Ding , M . Winslett , J . Han , and Z . Li . Differentially private data cubes : optimizing noise sources and consistency . In SIGMOD , 2011 .
[ 6 ] C . Dwork , F . McSherry , K . Nissim , and A . Smith .
Calibrating noise to sensitivity in private data analysis . In TCC , 2006 .
[ 7 ] M . R . Garey and D . S . Johnson . Computers and
Intractability : a guide to the theory of NP Completeness . W . H . Freeman , 1979 .
[ 8 ] M . Grant and S . Boyd . CVX : Matlab software for disciplined convex programming , version 21 http://cvxr.com/cvx , 2014 .
[ 9 ] M . Hardt , K . Ligett , and F . McSherry . A simple and practical algorithm for differentially private data release . In NIPS , 2012 .
[ 10 ] M . Hardt and G . N . Rothblum . A multiplicative weights mechanism for privacy preserving data analysis . In FOCS , 2010 .
[ 11 ] J . Hartung , G . Knapp , and B . K . Sinha . Statistical meta analysis with applications . John Wiley & Sons , 2008 . [ 12 ] M . Hay , V . Rastogi , G . Miklau , and D . Suciu . Boosting the accuracy of differentially private histograms through consistency . PVLDB , 3(1):1021–1032 , 2010 .
[ 13 ] I . F . Ilyas , W . Markl , P . Haas , P . Brown , and
40 %
35 %
30 %
25 %
20 %
15 %
30 %
25 %
20 %
15 %
JTree PrivBayes PrivateSVM Non Private
0.2
0.5 privacy budget e
0.8
( a ) Adult , Y=gender svm misclassification rates
JTree PrivBayes PrivateSVM Non Private
0.2
0.5 privacy budget e
0.8
60 %
40 %
20 %
0 %
30 %
25 %
20 %
15 %
10 %
1
1
JTree PrivBayes PrivateSVM Non Private
0.2
0.5 privacy budget e
0.8
1
( b ) Adult , Y=education svm misclassification rates
JTree PrivBayes PrivateSVM Non Private
0.2
0.5 privacy budget e
0.8
1
( c ) Adult , Y=salary
( d ) Adult , Y=marital
Figure 4 : SVM misclassification rates on non binary datasets
[ 14 ] G . D . Kleiter . The posterior probability of Bayes nets with strong dependences . Soft Computing , 3:162–173 , 1999 .
[ 15 ] D . Koller and N . Friedman . Probabilistic graphical models : principles and techniques . MIT Press , 2009 .
[ 16 ] J . Lee and C . Clifton . Top k frequent itemsets via differentially private FP trees . In SIGKDD , 2014 .
[ 17 ] C . Li , M . Hay , G . Miklau , and A . McGregor . Optimizing linear counting queries under differential privacy . In PODS , 2010 .
[ 18 ] C . Li , M . Hay , G . Miklau , and Y . Wang . A data and workload aware query answering algorithm for range queries under differential privacy . PVLDB , 7(5):341–352 , 2014 .
[ 19 ] N . Li , W . H . Qardaji , and D . Su . On sampling , anonymization , and differential privacy or , k anonymization meets differentail privacy . In ASIACCS , 2012 .
[ 20 ] N . Mohammed , R . Chen , B . C . M . Fung , and P . S . Yu .
Differentially private data release for data mining . In SIGKDD , 2011 .
[ 21 ] W . Qardaji , W . Yang , and N . Li . Priview : practical differentially private release of marginal contingency tables . In SIGMOD , 2014 .
[ 22 ] B . Rubinstein , P . L . Bartlett , L . Huang , and N . Taft . Learning in a large function space : privacy preserving mechanisms for SVM learning . Journal of Privacy and Confidentiality , 4(1):65–100 , 2012 .
[ 23 ] O . Williams and F . McSherry . Probabilistic inference and differential privacy . In NIPS , 2010 .
[ 24 ] X . Xiao , G . Wang , and J . Gehrke . Differential privacy via wavelet transform . In ICDE , 2010 .
[ 25 ] J . Xu , Z . Zhang , X . Xiao , and G . Yu . Differentially private histogram publicaiton . In ICDE , 2012 .
[ 26 ] G . Yaroslavtsev , G . Cormode , C . M . Procopiuc , and
D . Srivastava . Accurate and efficient private release of datacubes and contingency tables . In ICDE , 2013 .
[ 27 ] G . Yuan , Z . Zhang , M . Winslett , X . Xiao , Y . Yang , and Z . Hao . Low rank mechanism : optimizing batch queries under differential privacy . PVLDB , 5(11):1352–1363 , 2012 .
[ 28 ] A . Yuille and A . Rangarajan . The concave convex procedure . Neural Computation , 15(4):915–936 , 2003 .
[ 29 ] J . Zhang , G . Cormode , C . M . Procopiuc , D . Srivastava , and
X . Xiao . Privbayes : Private data release via bayesian networks . In SIGMOD , 2014 .
A . Aboulnaga . CORDS : Automatic discovery of correlations and soft functional dependencies . In SIGMOD , 2004 .
[ 30 ] X . Zhang , R . Chen , J . Xu , X . Meng , and Y . Xie . Towards accurate histogram publication under differential privacy . In SDM , 2014 .
138
