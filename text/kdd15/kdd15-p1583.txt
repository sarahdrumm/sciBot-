Co Clustering based Dual Prediction for
Cargo Pricing Optimization
Yada Zhu IBM Research
Yorktown Heights , NY 10598 yzhu@usibmcom
Hongxia Yang
Yahoo! Inc
Sunnyvale , California 94089 hongxia@yahoo inc.com
Jingrui He
Arizona State University
Tempe , AZ 85281 jingruihe@asuedu
ABSTRACT This paper targets the problem of cargo pricing optimization in the air cargo business . Given the features associated with a pair of origination and destination , how can we simultaneously predict both the optimal price for the bid stage and the outcome of the transaction ( win rate ) in the decision stage ? In addition , it is often the case that the matrix representing pairs of originations and destinations has a block structure , ie , the originations and destinations can be coclustered such that the predictive models are similar within the same co cluster , and exhibit significant variation among different co clusters . How can we uncover the co clusters of originations and destinations while constructing the dual predictive models for the two stages ?
We take the first step at addressing these problems . In particular , we propose a probabilistic framework to simultaneously construct dual predictive models and uncover the co clusters of originations and destinations . It maximizes the conditional probability of observing the responses from both the quotation stage and the decision stage , given the features and the co clusters . By introducing an auxiliary distribution based on the co clustering assumption , such conditional probability can be converted into an objective function . To minimize the objective function , we propose the COCOA algorithm , which will generate both the suite of predictive models for all the pairs of originations and destinations , as well as the co clusters consisting of similar pairs . Experimental results on both synthetic data and real data from cargo price bidding demonstrate the effectiveness and efficiency of the proposed algorithm .
Categories and Subject Descriptors I52 [ Pattern Recognition ] : Design Methodology|classifier design and evaluation ; I53 [ Pattern Recognition ] : clustering|algorithm ; I54 [ Pattern Recognition ] : Applications c⃝ 2015 ACM . ISBN 978 1 4503 3664 2/15/08 $1500
DOI : http://dxdoiorg/101145/27832582783337
General Terms Algorithms ; Performance
Keywords Co clustering ; dual predictive models
1 .
INTRODUCTION
Revenue management in the air cargo business is a fast growing field . It usually consists of two stages : the bidding stage where the vendor provides a bidding price with respect to a pair of origination and destination , or OD pair , and the decision stage where the customer makes a decision whether to accept this price or not . Compared to other industries such as passenger airlines or hotels , this field is more challenging in multiple respects due to the specific characteristics of cargo inventory , cargo business , and cargo booking behavior . This renders traditional yield management models ineffective or inefficient , thus necessitates the development of new models .
Figure 1 : Transaction size of different OD pairs .
Here we focus on the following three major challenges . First , as illustrated in Figure 1 , the number of transactions varies significantly among different OD pairs . For those pairs whose transaction volume is small , the resulting predictive model tends to be inaccurate due to the lack of training data . Second , most existing techniques construct predictive models for the two stages separately , thus prevent key information to be shared by these models . Third , the originations and destinations can be naturally co clustered such that the underlying predictive models are similar within each co cluster . This property has not been exploited for improv
1583 ing model performance and gaining insights into different OD pairs .
To address these challenges , in this paper , we propose a novel probabilistic framework to simultaneously construct dual predictive models for each OD pair , while uncovering the co clusters of originations and destinations . It is based on the conditional probability of observing the two types of responses from the two stages , given the features with respect to the OD pair , and the mappings for co clustering . We approximate this probability using an auxiliary distribution that satisfies the co clustering assumption , and develop a special case of the framework based on generalized linear models . Furthermore , we propose an effective and efficient algorithm named COCOA for solving the resulting optimization problem , whose computational complexity is linear with respect to the total number of OD pairs . Finally , we evaluate the performance of COCOA from various aspects using both synthetic data and real data on cargo price optimization .
To the best of our knowledge , we are the first to tackle the problem of cargo price optimization from a holistic perspective . In other words , we encapsulate multiple correlated models into a single probabilistic framework . The main advantage is that it allows key information to be shared among the different models . For example , the co clusters provide regularization for the dual predictive models , and accurate dual predictive models in turn will improve the performance of co clustering . Notice that for the OD pairs with a small transaction volume , such regularization helps alleviate the problem of small training set size .
The main contributions of this paper can be summarized as follows .
1 . A novel probabilistic framework for cargo price optimization , which leverages the intrinsic co clusters of originations and destinations to construct dual predictive models for all pairs of originations and destinations ;
2 . An effective and efficient algorithm named COCOA for solving the resulting optimization problem ;
3 . Experimental results on both synthetic and real data sets demonstrating the performance of COCOA .
The rest of the paper is organized as follows . In Section 2 , we briefly review the related work . The probabilistic framework for simultaneous dual prediction and co clustering is proposed in Section 3 , where we also present the COCOA algorithm . Section 4 shows promising results on both the synthetic data as well as a real data set for air cargo price optimization . Finally , we conclude in Section 5 .
2 . RELATED WORK
In this section , we review the related work on multi label learning , multi way clustering , and cargo pricing optimization . 2.1 Multi Label Learning
Multi label learning studies the problem where each example is associated with a set of labels [ 34 ] . One key issue is to exploit correlations or dependencies among multiple labels . According to [ 39 ] , existing strategies for label correlation exploitation can be grouped into three categories : first order , second order , and high order approaches .
First order methods assume that labels are independent , and multi label learning problem can be transformed into a number of independent binary classification problems , eg , ML kNN [ 40 ] . Second order approaches consider the pairwise relations between labels . Then the multi label learning problem is transformed into the label ranking problem which aims at properly ranking every relevant irrelevant label pair for each training instance , eg , Rank SVM [ 10 ] . Various methods have been proposed for high order label correlation learning . For example , LEAD [ 39 ] employed Bayesian network to encode the conditional dependencies of the labels as well as the feature set , with the feature set as the common parent of all labels . The LS ML algorithm was proposed for multi label learning to extract common subspace shared among multiple labels [ 18 ] . A hypergraph spectral learning formulation was proposed for multi label classification to exploit the correlation information among different labels using hypergraph [ 31 ] . TRAM [ 20 ] studied the problem of transductive multi label learning by utilizing the information from both labeled and unlabeled data . LIFT [ 38 ] constructed features specific to each label by conducting clustering analysis on its positive and negative instances , and then performed training and testing by querying the clustering results . MAHR [ 15 ] aimed to discover the label relationship via a boosting approach with a hypothesis reuse mechanism . A generic empirical risk minimization ( ERM ) framework was proposed for large scale multi label learning [ 36 ] . A theoretical analysis on multi label consistency was proposed in [ 12 ] . The authors proved a necessary and sufficient condition for the consistency of multi label learning based on surrogate loss functions . Another related work is MLLOC [ 16 ] , which assumed that the label correlation may be shared by only a subset of instances rather than all the instances .
Our problem setting for cargo pricing optimization is different from multi label learning . This can be seen from the fact that the two types of responses are obtained based on different sets of inputs : for the bidding stage , the inputs include the features for each OD pair ; and for the decision stage , in addition to these features , the inputs also include the estimated price from the predictive model of the bidding stage . Furthermore , in our proposed framework , we leverage the intrinsic co clusters of originations and destinations to facilitate information sharing , which is particularly beneficial for those OD pairs with a small transaction volume .
2.2 Multi Way Clustering
Different from traditional clustering techniques [ 17 ] , which are designed to group objects so as to maximize within cluster similarity and between cluster dissimilarity , for sparse relational data , co clustering or bi clustering methods [ 24 ] aim at simultaneously cluster objects of each type . These methods typically produce groupings of better quality by leveraging clusters of other types in the similarity measure [ 14 ] . The information theoretic co clustering method [ 8 ] is among the first to address this problem , which monotonically increases the preserved mutual information by intertwining the row and column clustering . Follow up work includes the generative model for evolutionary heterogeneous clusters in dynamic networks [ 32 ] , the evolutionary co clustering method proposed in [ 19 ] , the MMRC model proposed in [ 23 ] , the minimum Bregman information principle proposed in [ 1 ] , the general binary clustering model and its variations proposed in [ 21 ] , minimum sum squared residue co clustering
1584 proposed in [ 6 ] , etc . The proposed technique is also related to our previous work in [ 41 ] , where we studied the co clustering of multiple time series that fit into a matrix .
Co clustering has been generalized to handle more than two object types , ie , multi way clustering . Examples include Consistent Bipartite Graph Co partitioning ( CBGC ) [ 11 ] , which aims at collectively clustering star shaped relationships among different types of objects ; spectral relational clustering [ 22 ] , which iteratively embeds each type of objects into low dimensional spaces and benefits from the interactions among the hidden structures of different types ; collective matrix factorization [ 29 ] , which assumes shared parameters among factors when an entity participates in multiple relations ; etc . Furthermore , researchers have proposed various techniques to automatically determine the number of clusters for each object type , such as cross associations [ 3 ] , AutoPart [ 2 ] , PaCK [ 14 ] , etc .
The major difference between existing methods for multiway clustering and our proposed work is as follows . Here the inputs of co clustering are the dual predictive models for both the bidding stage and the decision stage , and our goal is to jointly infer the mappings for co clustering as well as the dual predictive models . 2.3 Pricing Optimization for Revenue Man agement
The air cargo industry has substantially grown over the past decades , driving the need of a structured environment with the explicit goal of maximizing revenues by offering optimized bidding prices . Air cargo companies use bidding price to accept/reject incoming bookings : if the rate of the booking is lower than the bidding price value then the booking is rejected , otherwise it is accepted . Bidding price controls are revenue based and have the advantages of being simple , having a natural interpretation as the marginal value of a given resource , and have a very good revenue performance [ 33 , 5 ] .
Optimal bidding price methods were introduced [ 30 ] , and extended by [ 28 ] , and [ 35 ] . [ 4 ] addressed the issue of origindestination specific demand , which is common for low fare passengers cargo , versus the itinerary specific demand , widely used in the passenger models . They extend three popular models to incorporate origin destination demand and introduce a routing algorithm tailored towards the special structure of the flight networks and their objectives . The simulation results report the superiority of an extended probabilistic model over a first come first serve policy applied to cargo revenue management . [ 26 ] proposed a dual ascent scheme to solve the Lagrangian of the probabilistic model used in [ 4 ] . Our proposed framework is significantly different from existing techniques due to the fact that it addresses the problem of pricing optimization from a holistic perspective . In particular , it bridges the bidding stage and the decision stage by jointly learning the dual predictive models , and it leverages the intrinsic co clusters of originations and destinations to enable information sharing among different OD pairs . Therefore , it is able to improve the performance of both predictive models , which eventually lead to increased revenue .
3 . THE PROPOSED FRAMEWORK
In this section , we propose a probabilistic framework to simultaneously construct dual predictive models , and uncover the co clusters of originations and destinations , followed by the introduction of the COCOA algorithm for solving the optimization problem .
3.1 Notation Suppose that there are O originations and D destinations . Let xi;j;k 2 Rd denote the features extracted with respect to the ith origination and the jth destination for the kth transaction , i = 1 ; : : : ; O , j = 1 ; : : : ; D , k = 1 ; : : : ; Ki;j , where Ki;j denotes the number of transactions with respect to the ith origination and the jth destination . For xi;j;k , we aim to 2 R+ , which denotes predict two types of responses : y(1 ) i;j;k 2 f0 ; 1g , the quoted price in the bidding stage , and y(2 ) i;j;k which denotes the outcome of the transaction ( successful or not ) in the decision stage . Notice that our problem setting is different from traditional multi label learning [ 37 ] in the sense that the estimated ^y(1 ) i;j;k is a function of xi;j;k , and the estimated ^y(2 ) i;j;k ; whereas multi label learning assumes that the prediction of different labels is based on the same set of inputs . Furthermore , in our proposed probabilistic framework , we exploit the 2 dimensional structure formed by combinations of O originations and D destinations in order to gain deeper understanding of the groupings of originations and destinations , which in turn help improve the predictive models . i;j;k is a function of both xi;j;k and ^y(1 )
3.2 Probabilistic Framework i;j;k and y(2 ) i;j;k . Let fi(1 ) i;j
We propose to use generalized linear models to predict 2 Rd and both types of responses y(1 ) 2 Rd+1 denote the vectors of unknown parameters in fi(2 ) i;j the two models respectively . For example , we could use the ,1(x identity link function g for the prediction of real valued y(1 ) i;j;k , which leads to the linear regression model ; and the logit link function g
′ i;j;kfi(1 ) i;j ) = x
′ i;j;kfi(1 )
′ ,1([x i;j;k ; ^y(1 ) for the prediction of binary y(2 )
= i;j i;j;k]fi(2 ) i;j ) i;j;k ,
1
′ 1+exp(,[x i;j;k;^y
( 1 ) i;j;k]fi
( 2 ) i;j ) which leads to the logistic regression model . Notice that for each stage s = 1 ; 2 , we can construct an O . D array B(s ) such that B(s ) i;j , the element in the ith row and the jth column , is set to fi(s ) i;j . We assume that the rows and columns of B(s ) can be re arranged such that B(s ) has a block structure where the elements within the same block are similar to each other , and the elements across different blocks are dissimilar . This is equivalent to co clustering the O originations to R row clusters , and the D destinations to C column clusters . Furthermore , in pricing optimization , it is usually the case that the block structure is shared across the two stages . Let u1 ; : : : ; uO and v1 ; : : : ; vD denote the O originations and D destinations respectively ; Let ( R denote the mapping from each origination ( u1 ; : : : ; uO ) to one of the R row clusters ( ^u1 ; : : : ; ^uR ) , and ( C denote the mapping from each destination ( v1 ; : : : ; vD ) to one of the C column clusters ( ^v1 ; : : : ; ^vC ) . Following [ 41 , 9 ] , given ( R and ( C such that ( R(ui ) = ^ur and ( C ( vj ) = ^vc , we assume that the probability p(s)(fi(s ) ; ui ; vjj(R ; ( C ) of having fi(s ) for the combination of the ith origination and the jth destination can be approximated with an auxiliary distribution q(s)(fi(s ) ; ui ; vjj(R ; ( C ) with the following property .
1585 p(s)(fi(s ) ; ui ; vjj(R ; ( C ) = p(s)(fi(s ) q(s)(fi(s ) ; ui ; vjj(R ; ( C ) : = ( s ) i;j j(R ; ( C ) i;j p(^ur ; ^vc)p(s)(uij^ur)p(s)(vjj^vc)p(s)(fi(s)jui)p(s)(fi(s)jvj )
( 1 ) i;j is a normalization parameter such that q(s)( ) is a where ( s ) valid probability distribution . According to Equation 1 , the auxiliary distribution q(s)(fi(s ) ; ui ; vjj(R ; ( C ) reflects the coclustering assumption , and it can be decomposed into five non constant terms , all of which are derived from p(s)(fi(s ) ; ui ; vj ) : p(^ur ; ^vc ) , which is the joint probability of the rth row cluster and the cth column cluster ; p(s)(uij^ur ) ( p(s)(vjj^vc) ) , which is the probability of having the ith origination ui ( the jth destination vj ) in the rth row cluster ( the cth column cluster ) ; and p(s)(fi(s)jui ) ( p(s)(fi(s)jvj) ) , which is the probability of having fi(s ) given the ith origination ( the jth destination ) . Notice that p(1)(^ur ; ^vc ) = p(2)(^ur ; ^vc ) due to the assumption that the block structure is shared by B(1 ) and B(2 ) . Therefore , we omit the superscript of this term for brevity . Lemma 3.1 in [ 41 ] shows that certain probabilities derived from p(s)( ) are preserved in the auxiliary distribution q(s)( ) , including : q(^ur; : ) = p(^ur;: ) ; q(^v:;c ) = p(^v:;c ) ; q(^ur; : ; ^v:;c ) = p(^ur; : ; ^v:;c ) q(s)(ui;:j^ur; : ) = p(s)(ui;:j^ur;: ) ; q(s)(v:;jj^v:;c ) = p(s)(v:;jj^v:;c ) ( 2 )
Based on the above discussion , the conditional probability i;j;k , and i;j given the features xi;j;k , and of observing the data ( the two types of responses y(s ) the vectors of parameters fi(s ) the mappings ( R , ( C ) can be expressed as follows .
3.3 Objective Function
In this subsection , we specify the probabilities used in Equation 3 , which originate from the application of pricing optimization . i;j
)2 i;j
2( i;j;k exp(y
22 0
( 2 ) i;j )
( 2 ) i;j )
1+exp([x
( s ) 2( i
( 1 ) i;j;k]fi
( 1 ) i;j;k]fi
,x
∥y(1 )
′ i;j;kfi(1 ) and p(s)(fi(s ) i;j
′ ( 2 ) i;j;k[x i;j;k;y ′ i;j;k;y jui ) / exp( , 1 ∥fi(s ) ffi(s ) jvj ) / exp( , 1 ffi(s ) g and Ep(s)jvj log p(y(1 ) i;j;k dicting y(2 ) jxi;j;k ; y(1 ) and p(s)(fi(s ) i;j tributions , ie , p(s)(fi(s ) i;j
For predicting y(1 ) i;j;k , we use linear regression model , and ∥2 . For prei;j ) = , 1 jxi;j;k ; fi(1 ) i;j;k , we use logistic regression model , and log p(y(2 ) i;j;k i;j;k ; fi(2 ) . For p(s)(fi(s ) i;j ) = i;j jvj ) , we assume that they follow Gaussian dis,Ep(s)jui ∥fi(s ) , Ep(s)jvj ffi(s ) g∥2 ) , g are the expectation i;j with respect to p(s)( ) for the ith origination and the where Ep(s)jui of fi(s ) jth destination respectively , ( s ) are two positive constants . Furthermore , it is straight forward to see that the MLE estimate of both expectations can be expressed as origination wise and destination wise average , ie , Ep(s)jui fi(s ) i ; :
Before specifying probabilities p(s)(uij^ur ) and p(s)(vjj^vc ) , we first compute the expectations of the vector fi(s ) with respect to each row/column cluster based on q(s)( ) . First of all , the conditional distribution of the vector fi(s ) given the rth row cluster can be expressed as follows . g fi(s ) i;j , Ep(s)jvj and ( s ) ffi(s )
∑
∑ i fi(s ) i;j . j fi(s )
: = 1 D
: = 1 O
( s ) j
)2 i;j i;j i;j i j i;j i;j
:;j
∑
∑ c
( R(ui)=^ur ( C ( vj )=^vc q(s)(fi(s ) ; ui ; vj ) i;j p(^vcj^ur)p(s)(uij^ur)p(s)(vjj^vc ) ( s ) p(s)(^ur )
∑ ∑ q(s)(fi(s)j^ur ) = q(fi(s ) ; ^ur ) q(^ur )
=
= c
( R(ui)=^ur ( C ( vj )=^vc
p(s)(fi(s)jui)p(s)(fi(s)jvj ) jui ) ffi(s ) i;j g∥2 ) , ffi(s ) i;j g
∑
∑
∑ c
∑ r log p(y(s ) i;j;k ; fi(s ) f i;j
= jxi;j;k ; ( R ; ( C ) j(R ; ( C ) log p(s)(fi(s ) i;j
∑ [ s i;j;k log p(y(1 ) i;j;k ∑ flog p(y(1 ) ∑ ∑ log ( s )
( R(ui)=^ur r;c
( k i;j
∑ ∑ ∑ ∑ ∑ ∑ i;j;k s r
+
+
+
+
] jxi;j;k ; fi(1 ) jxi;j;k ; fi(1 ) i;j ) + log p(y(2 ) i;j;k i;j ) + log p(y(2 ) i;j;k jxi;j;k ; y(1 ) jxi;j;k ; y(1 ) g i;j;k ; fi(2 ) i;j ) i;j )g i;j;k ; fi(2 ) where we repeatedly applied Equation 2 to replace the probabilities derived from q(s)( ) with those derived from p(s)( ) . As discussed before , both p(s)(fi(s)jui ) and p(s)(fi(s)jvj ) follow Gaussian distributions . Therefore , the expectation of fi(s ) given ^ur can be derived as follows . i;j p(^ur ; ^vc ) log p(s)(uij^ur)p(s)(fi(s ) i;j jui )
^fi
( s ) r ; :
: = Eq(s)j^ur ffi(s)g = p(^vcj^ur )
p(s)(uij^ur)p(vjj^vc ) 2
( R(ui)=^ur ( C ( vj )=^vc j fi(s ) i ; : + 2 2 i + 2 j i fi(s )
:;j log p(s)(vjj^vc)p(s)(fi(s ) jvj ) )
( 3 ) i;j c
( C ( ui)=^ur i;j;k and y(2 )
In Equation 3 , the overall probability is approximated by the sum of five terms : the first two terms come from the generalized linear models to predict y(1 ) i;j;k ; the third term is fixed given ( R and ( C ; the fourth term measures the probability of having fi(s ) i;j for the ith origination of the rth row cluster ; and the last term measures the probability of having fi(s ) i;j for the jth destination of the cth column cluster . Finally , both the vectors of parameters fi(s ) i;j and the mappings ( R , ( C can be obtained by maximizing Equation 3 with specific choices of the probabilities .
Similarly , the expectation of fi(s ) given ^vc can be derived as follows .
^fi
( s ) :;c
: = Eq(s)j^vc ffi(s)g = p(^urj^vc )
p(s)(uij^ur)p(vjj^vc ) 2
( R(ui)=^ur ( C ( vj )=^vc j fi(s ) i ; : + 2 2 i + 2 j i fi(s )
:;j
( 4 )
( 5 )
1586 Using these expectations , p(s)(uij^ur ) and p(s)(vjj^vc ) can be specified as follows . p(s)(uij^ur ) / exp( , 1 ( (s ) p(s)(vij^vc ) / exp( , 1 ( (s ) r )2 c )2
∥fi(s ) i ; :
, ^fi
( s ) r ; :
∥2 )
∥fi(s )
:;j
, ^fi
( s ) :;c
∥2 )
( 6 )
Based on the above specified probabilities , we have the following objective function , which is the negative log probability of observing the data . Minimizing the objective function with respect to fi(s ) i;j ; ( R ; ( C will lead to the dual predictive models with respect to the bidding price and the outcome of the transaction , as well as the co clustering of origination/destination pairs .
∑ ∑ i;j;k f∥y(1 ) i;j;k
, x ′ i;j;kfi(1 ) i;j
∥2 i;j ; ( R ; ( C ) = f ( fi(s ) ∑ , ff1y(2 ) i;j;k([x fff2 ∑ +
∑ ∑ s r
+ ff4
′ i;j;k ; y(1 )
′ i;j;k]fi(2 ) i;j;k ; y(1 ) i;j ) + ff1 log(1 + exp([x [ ∥fi(s ) ∥fi(s ) i;j )g i;j;k]fi(2 ) , fi(s ) ∥2 ]
, ^fi i;j i ; :
( s ) r ; : i ; :
6 :
( R(ui)=^ur
[ ∥fi(s )
:;j
, ^fi
( s ) :;c
∥2 + ff5
∥fi(s ) i;j
, fi(s )
:;j
∥2]g
∑ j
∥2 + ff3 ∑ update both p(s)(uij^ur ) and p(s)(vjj^vc ) based on the current expectations ^fi r ; : and ^fi
( s ) :;c .
( s )
Algorithm 1 COCOA Algorithm i;j;k , R , C , ff1 , ff2 , ff3 , ff4 , ff5 , n1 , n2
Input : xi;j;k , y(s ) Output : fi(s ) 1 : Initialize fi(s ) i;j , ( R , ( C
:;j to be 0 vectors , s = 1 ; 2 , i = i ; : and fi(s ) 1 ; : : : ; O , j = 1 ; : : : ; D ; 2 : Randomly initialize ( R and ( C ; 3 : Initialize p(s)(uij^ur ) = j(R(ui)=^urj and p(s)(vjj^vc ) = j(R(vj )=^vcj , s = 1 ; 2 , i = 1 ; : : : ; O , j = 1 ; : : : ; D , r = 1 ; : : : ; R , c = 1 ; : : : ; C ;
1
1
4 : for t = 1 to n1 do for fi(1 ) 5 : i;j
Solve by minimizing i ; : i;j
, fi(s )
,ff1y(2 ) i;j;kfi(1)i;j∥2 + ff3∥fi(s ) ′ ∑ x i = 1 ; : : : ; O , j = 1 ; : : : ; D ; fi(2 ) Solve for i;j ′ i;j;k]fi(2 ) i;j;k ; y(1 ) i;j;k([x i;j )g + ff3∥fi(s ) ′ ∑ i;j;k ; y(1 ) ∥2 , i = 1 ; : : : ; O , j = 1 ; : : : ; D ; i;j and fi(s ) exp([x fi(s ) :;j i;j;k]fi(2 ) i ; : = 1 j fi(s ) i;j k
O
1 ; 2 , i = 1 ; : : : ; O , j = 1 ; : : : ; D ; for t
= 1 to n2 do
′
7 : Update fi(s )
∑ ∥2 + ff5∥fi(s ) k i;j
∥y(1 ) , fi(s ) i;j;k
:;j
, ∥2 , by minimizing i;j ) + ff1 log(1 + ∥2 + ff5∥fi(s ) , ∑
, fi(s ) i;j i ; :
:;j = 1
D i fi(s ) i;j , s =
( s ) r ; : using Equation 4 , r = 1 ; : : : ; R ; ( s ) :;c using Equation 5 , c = 1 ; : : : ; C ;
Compute ^fi ∑ Compute ^fi for i = 1 to O do Let ^r arg minr Update ( R(ui ) ^u^r ; ∑ end for for j = 1 to D do Let ^c arg minc Update ( C ( vj ) ^v^c ;
2 s=1
2 s=1
∥fi(s ) i ; :
, ^fi
∥2 ;
( s ) r ; :
∥fi(s )
:;j
, ^fi
∥2 ;
( s ) :;c end for Update p(s)(uij^ur ) and p(s)(vjj^vc ) using Equation 6 ; end for
20 : 21 : end for
3.5 Discussions
In this subsection , we discuss the proposed COCOA algo rithm from various aspects .
First of all , in COCOA , fi(s ) i;j is obtained via regularized risk minimization . In the first iteration of the outer loop , Steps 5 and 6 are reduced to ridge regression and L2 regularized logistic regression respectively . In the following iterations , both are regularized by the origination wise and destination wise average . Notice that in the training stage , for the prediction of y(2 ) i;j;k , an alternative choice is to use ′ ′ i;j;kfi(1 ) In other words , i;j;k ; x [ x i;j to replace the we could use the estimated price x real price y(1 ) i;j;k . In this way , the ridge regression model in Step 5 of COCOA for predicting y(1 ) i;j;k will have an additional ′ ′ i;j;kfi(1 ) i;j;k([x i;j ) + ff1 log(1 + i;j;k ; x regularizer i;j )g . The benefit of using the esti exp([x mated price instead of the real price is that it provides a
,ff1y(2 ) i;j ]fi(2 )
′ ′ i;j;kfi(1 ) i;j;k ; x i;j ] instead of [ x i;j;k ] . ′ i;j;kfi(1 )
′ i;j;k ; y(1 ) i;j ]fi(2 )
∑ k
8 : 9 :
10 : 11 : 12 : 13 : 14 : 15 : 16 : 17 : 18 : 19 : c
( 2(vj )=^vc i where ff1 ; : : : ; ff5 are constants that depend on 0 , ( s ) ( s ) r , and ( s ) 3.4 COCOA Algorithm
. c i
( 7 )
, ( s ) j
,
To minimize the objective function in Equation 7 , we propose the following algorithm named COCOA based on block coordinate descent , which is described in Algorithm 1 . i;j;k and y(2 )
The COCOA algorithm works as follows . It takes as input the features xi;j;k , two types of responses y(1 ) i;j;k , the total number of row clusters R , the total number of column clusters C , parameters ff1 ; : : : ; ff5 , and numbers of iteration steps n1 , n2 . Then it proceeds by alternating the optimization with respect to fi(s ) i;j and ( R , ( C . Finally , it outputs both the vectors fi(s ) i;j , as well as the two mappings ( R and ( C that generate the co clustering of O originations and D destinations .
To be specific , in Steps 1 and 2 , we initialize both fi(s ) i ; : and fi(s ) :;j to be 0 vector , and initialize both ( R and ( C by randomly assigning each origination/destination to one of the row/column clusters . Then we initialize both p(s)(uij^ur ) and p(s)(vjj^vc ) assuming uniform distribution among the originations/destinations within the same row/column cluster . Next we repeat the following steps n1 times until convergence . In Steps 5 and 6 , we solve for fi(s ) i;j via regularized risk minimization . Notice that here we have two regularization terms using fi(s ) :;j respectively . In Step 7 , we update both fi(s ) :;j using origination wise and destinationwise average . Steps 8 to 20 form an inner loop for updating the mappings ( R and ( C . In the inner loop , we first compute the expectations of fi(s ) within each row/column cluster in Steps 9 and 10 . Then we update mapping ( R by assigning each origination to its closest row cluster ( Steps 11 to 14 ) , and mapping ( C by assigning each destination to its closest column cluster ( Steps 15 to 18 ) . Finally , we i ; : and fi(s ) i ; : and fi(s )
1587 more accurate model for the test stage , since during the test stage , the real price will be unknown . However , it comes with the cost of coupling the estimation of fi(1 ) i;j and fi(2 ) i;j , which might affect the convergence of the iterative algorithm . Investigating effective and efficient algorithms for coupled parameters is one of our future research directions . Second , the computational complexity of COCOA is shown in the following lemma .
∑
Lemma 1 . The time complexity of COCOA is O[n1 ( d2:376+ i;j Ki;j + n2(ORd + DCd + OD)) ] , and the space com d2 plexity is O[d3 + ODd ] . different number of \originations" and \destinations" ( ranges from 3 to 5 ) . The specific vectors for ith row , jth column y(s ) i;j ; : ( s = 1 for the linear model and s = 2 for the generalized linear model ) are generated as following .
1 . For each row and column cluster , generate cluster spe cific mean ^fi
( s ) r ; : and ^fi
( s ) :;c ;
2 . Given row cluster mean ^fi
( s ) R , generate row vector fi(s ) tributions ;
( s ) r ; : and standard deviation i ; : through Gaussian dis i;j Ki;j , including both ridge regression using Coppersmith
∑
Proof . The time complexity can be proven based on the fact that the computational cost of COCOA is dominated by regularized risk minimization in Steps 5 and 6 , as well as the inner loop between Steps 8 and 20 . The computational complexity of regularized risk minimization is d2:376 + d2 Winograd algorithm [ 7 ] for matrix inversion , and logistic regression using coordinate ascent , conjugate gradient ascent , quasi Newton method , or iterative scaling [ 25 ] . Within the inner loop , the time complexity of Steps 9 and 10 is O(OD ) , Steps 11 to 14 is O(ORd ) , Steps 15 to 18 is O(DCd ) , and Step 19 is O(Od + Dd ) . On the other hand , the space complexity includes both the space requirement for d . d matrix inversion , the storage of all the vectors of parameters fi(s ) i;j , the probabilities p(s)(uij^ur ) and p(s)(vjj^vc ) , as well as the various ex(s ) pectations fi(s ) :;c , where i = 1 ; : : : ; O , j = 1 ; : : : ; D , r = 1 ; : : : ; R , c = 1 ; : : : ; C.' r ; : , and ^fi i ; : , fi(s ) j ; : , ^fi
( s )
Lemma 1 implies that the time complexity of COCOA scales linearly with respect to the number of originationdestination pairs , and the total number of transactions across all origination destination pairs . It also depends on both n1 and n2 . As we will show in the next section , empirically the number of iterations required for the inner loop n2 and the outer loop n1 to converge typically does not exceed 8 . 4 . EXPERIMENTAL RESULTS
In this section , we demonstrate the effectiveness of the proposed COCOA algorithm both on synthetic and real data . To our best knowledge , COCOA is the first algorithm for co clustering based dual prediction framework and we compare its performance with an advanced hierarchical clustering and prediction methodology , hglm , which is currently adopted by a worldwide cargo company . Methodology details are described in 421 Different from COCOA , hglm performs co clustering and prediction separately . In addition , this two step method tends to introduce extra variances by using outputs from the first step model as inputs for the second model . Also , there is no adaptive feedback process to improve the performance for both models . COCOA can potentially tackle these challenges by integrating a regularized linear submodel for the bid price prediction and a generalized linear submodel for the win rate prediction in a consistent framework . 4.1 Synthetic Data
In this subsection , we first test the performance of COCOA on a synthetic data set which mimics the real world problem . The synthetic data consists of 3 row clusters and 3 column clusters . Each row and column cluster consists of
3 . Given column cluster mean ^fi
C , generate column vector fi(s ) tion ( s ) sian distributions ;
( s ) :;c and standard devia:;j through Gaus
4 . fi(s ) i;j is finalized through the weighted average of fi(s ) and fi(s )
:;j using respective standard deviations ; i ; :
5 . For each OD pair , generate input features xi;j ; : and y(1 ) i;j ; : and y(2 ) i;j ; : are then produced through linear model ( to mimic the price distribution ) and a generalized linear model ( to mimic the winning probability distribution ) through fi(s ) i;j and xi;j ; : with perturbation error 001 i ; : and fi(s )
We set R ; C = 3 , fi(s )
:;j to be 0 vectors and randomly generate cluster members as starting points . We use 5 fold cross validation to choose ff . After the algorithm converges , the estimated winning probability for each OD pair is shown in Figure 3(b ) . The red color stands for higher winning probability given all other conditions the same and the blue color denotes relatively lower wining probability . We also use the solid lines to illustrate the clustering boundaries . As we can see , OD pairs in similar colors are grouped together , which implies COCOA recovers all the clusters . The winning probability for the OD pairs before co clustering is also shown in Figure 3(a ) for comparison . 4.2 Worldwide Cargo Company Challenge
( s ) r ; : and ^fi
In this subsection , we test the performance of COCOA on a real cargo pricing optimization problem . We select 20 originations and 20 destinations with relatively high volume of transactions . Among the resulting 400 OD pairs , about 25 % of them have less than 20 transactions . Such OD pairs are excluded from training fi , which is estimated based on ( s ) its cluster membership ’s average ^fi :;c . Each transaction is accompanied with historical bidding prices and bidding stages ( win or loss ) and several other features , including number of cargo pieces , cargo weight , cargo volume , lead time and customer size , etc . Based on domain knowledge and the initial study , we set R ; C = 3 . Similarly to the synthetic study , we initialize fi(s ) :;j as 0 , randomly generate cluster members , and use the 5 fold cross validation to choose ff . However , we tested with other settings and found that our algorithm is insensitive to these starting values . To make a fair comparison , hglm is also given the same number of row/column clusters R=C . i ; : and fi(s )
In the following sub sections , we describe in detail the evaluation of COCOA in terms of the co clustering results , predictive likelihood , improvement of revenue , and convergence rate .
1588 Figure 2 : COCOA performance on real data : the three row clusters present clear different in bidding price which can be interpretated by the difference in three important features , ie , lead time , customer size and cargo volume
421 The Introduction to hglm We first introduce an advanced Hierarchical clustering and prediction methodology framework currently adopted by a worldwide cargo company . Notice that this problem has a natural hierarchical structure , eg , different transactions are grouped to different OD pairs . The first step of the framework is to cluster OD pairs based on the win rate effects directly coming from the OD pairs and use the following Hierarchical Logistic Regression Model [ 27 ] to estimate such effect . logitfE(yijk)g = Xijkfiij + Zijuij + ϵijk ;
( 8 ) where yijk is the kth cargo price bidding stage ( win or loss ) for the ith origination and the jth destination ; Xijk be the corresponding fixed effects which include bidding specific variables and customer market information . Let Zij stand for the random effects coming from the OD ( origination and destination ) pair ( i ; j ) . fiij is the coefficient for the fixed effects and uij is OD pair ( i ; j ) effect estimation . In a hierarchical model , observations are grouped into clusters ( eg , origination destination in this cargo price bidding problem ) , and the distribution of an observation is determined not only by the common structure among all clusters but also by the specific structure of the cluster where this observation belongs to . So the random effect component , different for different clusters , is introduced into the model . In the second step , the cargo company would co cluster the OD pairs based on the homogeneous effects for the win rates that are estimated through Model ( 8 ) or uij . Each cell of the matrix is the random effect estimation of a specific OD pair . The basic idea of co clustering consists in making permutations of objects and variables in order to draw a correspondence structure ( eg , pattern recognition ) for the most similar effects . The density for each block is given by [ 13 ] :
}
( uij , kl)2
;
( 9 )
1√
{ , 1
22 kl fkl(uij ; ff ) = exp
22 kl where uij is OD pair effect for ORIG=i and DEST=j and ff = ( kl ; 2 kl ) is the cluster specific mean and variance .
However , this two step method may introduce extra variances from estimating the first framework and using the outputs as inputs for the second modeling . Also , there is no adaptive feedback process to improve the performance for both models .
This two step method has been very successful in helping the cargo company to develop an automatic optimized pricing machinery to increase revenue . However , this two step method does not bridge the information sharing and connection among the two modeling . 422 Co clustering Result Figure 4 shows the win probability prediction before and after co clustering by COCOA algorithm . Red color stands for higher win probability given all other conditions the same , and the solid lines demonstrate the clustering boundaries . After co clustering , the OD pairs are rearranged and OD pairs in similar colors are grouped together . The three row clusters generated from the 20 originations show clear geographical pattern . To be specific , airports , such as AMS , BUD , CGN and DUS from European cities are in the same row cluster ; airports , such as ATL , IAH , JFK and ORD from US cities , and airports , such as BOM , NRT and PVG from Asian and pacific area belong to the other two row clusters , respectively . In addition , the three row clusters
1589 ( a ) OD pair latent winning probability before coclustering
( a ) Before clustering
( b ) OD pair latent winning probability after coclustering
Figure 3 : Performance on synthetic data set : COCOA recovers all the clusters . present strong difference in average bidding price . For example , the average bidding price corresponding to airports ATL , AMS and BOM are about 4e4 , 2e4 and 5:5e4 unit , respectively . These three airports are examples from each of the three row clusters , respectively . The price distinction can be explained by the difference in three critical features , ie , lead time , customer size , and cargo volume . As shown in Figure 2 , the cargo volume of AMS is about half of that of ATL and BOM . The customer size of BOM is relatively large compared to ATL and AMS . This matches our intuition that large customer size and cargo volume lead to high bidding price and vice versa . The three column clusters present similar patterns .
423 Predictive Likelihood Figure 5 presents the comparison between COCOA and hglm based on the real data set in terms of total log likelihood of price and win probability . In Figure 5 , the x axis is the fraction of transactions per OD pair used for training , and y axis is the total log likelihood of price and win probability prediction normalized by test sample size . For each training sample fraction , we repeat the experiment for 20 times and report the mean and standard deviation of the normalized log likelihood as an error bar plot . Figure 5 shows that COCOA obtains not only larger mean log likeli
( b ) After clustering
Figure 4 : COCOA performance on real data : co clustering results on the winning probability prediction of selected OD pairs . hood value but also smaller variation of log likelihood value than hglm over all training sample fractions . COCOA outperforms hglm because it leverages the block structure in the prediction stages of both price and win probability . 424 Weighted Prediction Objectives In reality , the objective function can be extended to include a weight ( fl ) on the price prediction and win probability prediction to reflect one ’s preference , ie fll( )(1 ) + ( 1 , fl)l( )(2 ) . The impact of fl on the price optimization is presented in Figure 6 , where the x axis is a range of fl values and the y axis is the total log likelihood of price and win probability on the test data . The log likelihood value obtains the maximum at fl = 0:2 which corresponds to a relatively high weight on the win probability . On the other hand , the variation of the log likelihood increases significantly at very small or large fl values , such as 0.1 and 09 This is because the prediction of undominated objective results in large variation . These findings can help practitioners select appropriate fl values . The total expected revenue which is given by the summation of predictive price multiple by predictive win probability of all the transaction on the test data is compared with actual revenue . As shown in Figure 7 , COCOA can improve revenue significantly , although the variation of revenue prediction increases given relatively large fl values . This again demonstrates the superer performance of COCOA .
1590 Figure 5 : Performance on real data set : COCOA consistently outperforms hglm .
Figure 7 : Performance on real data set : COCOA leads to higher expected revenue .
Figure 6 : Effect of the weight on the two prediction objective functions wrt log likelihood .
Figure 8 : Convergence study of COCOA : the clustering objective function converges typically within 8 iterations
425 Convergence Study We evaluate the time complexity of COCOA based on the inner loop . We set n2 as a sufficiently large number and assume the inner loop converges if the co clustering membership does not change . At the first iteration of the outer loop , we report the co clustering objective function value of the inner loop at each iteration . As shown in Figure 8 , the inner loop converges quicky , which is typically less than 8 iterations .
5 . CONCLUSIONS
In this paper , we studied the problem of cargo pricing optimization , and proposed a probabilistic framework to maximize the conditional probability of observing two types of responses from the two stages ( the bidding stage and the decision stage ) given the features for each OD pair and the mappings for co clustering . Compared with existing work , the main advantage of the proposed framework is three fold . First of all , it allows information sharing among all the OD pairs , which significantly boosts the performance on the OD pairs with a small transaction volume . Second , it bridges the two stages by jointly learning the dual predictive models . Finally , it leverages the intrinsic co clusters of originations and destinations to improve the model performance . Furthermore , we instantiated the framework with both an auxiliary distribution designed based on the co clustering as sumption , and generalized linear models for the two types of responses . We also proposed an iterative algorithm named COCOA for solving the resulting optimization problem in an effective and efficient manner . The performance of COCOA is demonstrated on both synthetic and real data sets .
6 . REFERENCES [ 1 ] A . Banerjee , I . S . Dhillon , J . Ghosh , S . Merugu , and
D . S . Modha . A generalized maximum entropy approach to bregman co clustering and matrix approximation . Journal of Machine Learning Research , 8:1919{1986 , 2007 .
[ 2 ] D . Chakrabarti . Autopart : Parameter free graph partitioning and outlier detection . In PKDD , pages 112{124 , 2004 .
[ 3 ] D . Chakrabarti , S . Papadimitriou , D . S . Modha , and
C . Faloutsos . Fully automatic cross associations . In KDD , pages 79{88 , 2004 .
[ 4 ] V . CHen , D . Guenther , and E . Johnson . Routing considerations in airline yield management . In PRoceedings 5th International Conference of the Decision Science Institute , 1999 .
[ 5 ] W . Chiang , C . Chen , and X . Xu . An overview of research on revenue management : current issues and future research . International Journal of Revenue Management , pages 97{128 , 2006 .
1591 [ 6 ] H . Cho , I . S . Dhillon , Y . Guan , and S . Sra . Minimum
[ 24 ] S . C . Madeira and A . L . Oliveira . Biclustering sum squared residue co clustering of gene expression data . In SDM , pages 114{125 , 2004 .
[ 7 ] D . Coppersmith and S . Winograd . Matrix multiplication via arithmetic progressions . J . Symb . Comput . , 9(3):251{280 , 1990 .
[ 8 ] I . S . Dhillon , S . Mallela , and D . S . Modha .
Information theoretic co clustering . In Proceedings of the 9th ACM international Conference on Knowledge Discovery and Data Mining , pages 89{98 , 2003 .
[ 9 ] I . S . Dhillon , S . Mallela , and D . S . Modha .
Information theoretic co clustering . In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , Washington , DC , USA , August 24 27 , 2003 , pages 89{98 , 2003 .
[ 10 ] A . Elisseeff and J . Weston . A kernel method for multi labelled classification . In NIPS , pages 681{687 , 2001 .
[ 11 ] B . Gao , T Y Liu , X . Zheng , Q . Cheng , and W Y
Ma . Consistent bipartite graph co partitioning for star structured high order heterogeneous data co clustering . In KDD , pages 41{50 , 2005 .
[ 12 ] W . Gao and Z H Zhou . On the consistency of multi label learning . Artif . Intell . ( AI ) , pages 22{44 , 2013 .
[ 13 ] G . Govaert and M . Nadif . Clustering with block mixture models . Pattern Recognition , 36(2):463{473 , 2003 .
[ 14 ] J . He , H . Tong , S . Papadimitriou , T . Eliassi Rad ,
C . Faloutsos , and J . Carbonell . Pack : Scalable parameter free clustering on k partite graphs . In SDM Workshop on Link Analysis , Counterterrorism and Security , 2009 .
[ 15 ] S J Huang , Y . Yu , and Z H Zhou . Multi label hypothesis reuse . In KDD , pages 525{533 , 2012 .
[ 16 ] S J Huang and Z H Zhou . Multi label learning by exploiting label correlations locally . In AAAI , pages 1{7 , 2012 . algorithms for biological data analysis : a survey . IEEE Transactions on Computational Biology and Bioinformatics , pages 24{45 , 2004 .
[ 25 ] T . P . Minka . A comparison of numerical optimizers for logistic regression . Technical report , Microsoft Research , 2007 .
[ 26 ] B . Rao . A convex programming model for cargo revenue mix optimization . Internal Report , Sabre Holdings , 2000 .
[ 27 ] L . Ronnegard , X . Shen , and M . Alam . hglm : A package for fitting hierarchical generalized linear models . The R Journal , 2(2):20{28 , 2010 .
[ 28 ] R . Simpson . Using network flow techniques to find shadow prices for market and seat inventory control . MIT Flight Transportation Laboratory Memorandum M89 1 , Cambridge , MA , 1989 .
[ 29 ] A . P . Singh and G . J . Gordon . Relational learning via collective matrix factorization . In KDD , pages 650{658 , 2008 .
[ 30 ] B . Smith and C . Penn . Analysis of alternative origin destination control strategies . AGIFORS Annual Symposium Proceedings , 28 .
[ 31 ] L . Sun , S . Ji , and J . Ye . Hypergraph spectral learning for multi label classification . In KDD , pages 668{676 , 2008 .
[ 32 ] Y . Sun , J . Tang , J . Han , C . Chen , and M . Gupta .
Co evolution of multi typed objects in dynamic star networks . IEEE Trans . on Knowledge and Data Engineering , 99:1{14 , 2013 .
[ 33 ] K . Talluri and G . Van Ryzin . The Theory and
Practice of Revenue Management , International Series in Operations Research and Management Science . Boston/Dordrecht/London : Kluwer Academic Publishers , 2004 .
[ 34 ] G . Tsoumakas and I . Katakis . Multi label classification : An overview . International Journal of Data Warehousing and Mining , 3(3):1{13 , 2007 .
[ 17 ] A . K . Jain and R . C . Dubes . Algorithms for Clustering
[ 35 ] E . Williamson . Airline network seat control . PhD
Data . Prentice Hall , Inc . , Upper Saddle River , NJ , USA , 1988 .
Thesis , MIT , Cambridge , MA , 1992 .
[ 36 ] H F Yu , P . Jain , P . Kar , and I . S . Dhillon .
[ 18 ] S . Ji , L . Tang , S . Yu , and J . Ye . Extracting shared subspace for multi label classification . In KDD , pages 381{389 , 2008 .
[ 19 ] S . Ji , W . Zhang , and J . Liu . A sparsity inducing formulation for evolutionary co clustering . In KDD , pages 334{342 , 2012 .
[ 20 ] X . Kong , M . K . Ng , and Z H Zhou . Transductive multilabel learning via label set propagation . IEEE Trans . Knowl . Data Eng . ( TKDE ) , pages 704{719 , 2013 .
[ 21 ] T . Li . A general model for clustering binary data . In
KDD , pages 188{197 , 2005 .
[ 22 ] B . Long , Z . M . Zhang , X . Wu , and P . S . Yu . Spectral clustering for multi type relational data . In ICML , pages 585{592 , 2006 .
Large scale multi label learning with missing labels . In ICML , pages 593{601 , 2014 .
[ 37 ] M . Zhang and Z . Zhou . A review on multi label learning algorithms . IEEE Trans . Knowl . Data Eng . , 26(8):1819{1837 , 2014 .
[ 38 ] M L Zhang . Lift : Multi label learning with label specific features . In IJCAI , pages 1609{1614 , 2011 .
[ 39 ] M L Zhang and K . Zhang . Multi label learning by exploiting label dependency . In KDD , pages 999{1008 , 2010 .
[ 40 ] M L Zhang and Z H Zhou . Ml knn : A lazy learning approach to multi label learning . Pattern Recognition , pages 2038{2048 , 2007 .
[ 41 ] Y . Zhu and J . He . Co clustering structural temporal
[ 23 ] B . Long , Z . M . Zhang , and P . S . Yu . A probabilistic data with applications to semiconductor framework for relational clustering . In KDD , pages 470{479 , 2007 . manufacturing . In ICDM , 2014 .
1592
